file_path,api_count,code
setup.py,0,"b'from setuptools import setup, Extension, find_packages\n\ninstall_requires = [\n    ""cpprb>=8.1.1"",\n    ""setuptools>=41.0.0"",\n    ""numpy>=1.16.0"",\n    ""joblib"",\n    ""scipy""\n]\n\nextras_require = {\n    ""tf"": [""tensorflow==2.0.0""],\n    ""tf_gpu"": [""tensorflow-gpu==2.0.0""],\n    ""examples"": [""gym[atari]"", ""opencv-python""],\n    ""test"": [""coveralls"", ""gym[atari]"", ""matplotlib"", ""gast==0.2.2""]\n}\n\nsetup(\n    name=""tf2rl"",\n    version=""0.1.13"",\n    description=""Deep Reinforcement Learning for TensorFlow2.0"",\n    url=""https://github.com/keiohta/tf2rl"",\n    author=""Kei Ohta"",\n    author_email=""dev.ohtakei@gmail.com"",\n    license=""MIT"",\n    packages=find_packages("".""),\n    install_requires=install_requires,\n    extras_require=extras_require)\n'"
examples/run_apex_ddpg.py,1,"b'import gym\n\nfrom tf2rl.algos.apex import apex_argument, run\nfrom tf2rl.algos.ddpg import DDPG\nfrom tf2rl.misc.target_update_ops import update_target_variables\n\n\nif __name__ == \'__main__\':\n    parser = apex_argument()\n    parser.add_argument(\'--env-name\', type=str,\n                        default=""Pendulum-v0"")\n    parser = DDPG.get_argument(parser)\n    args = parser.parse_args()\n\n    # Prepare env and policy function\n    def env_fn():\n        return gym.make(args.env_name)\n\n    def policy_fn(env, name, memory_capacity=int(1e6),\n                  gpu=-1, noise_level=0.3):\n        return DDPG(\n            state_shape=env.observation_space.shape,\n            action_dim=env.action_space.high.size,\n            max_action=env.action_space.high[0],\n            gpu=gpu,\n            name=name,\n            sigma=noise_level,\n            batch_size=100,\n            lr_actor=0.001,\n            lr_critic=0.001,\n            actor_units=[400, 300],\n            critic_units=[400, 300],\n            memory_capacity=memory_capacity)\n\n    def get_weights_fn(policy):\n        # TODO: Check if following needed\n        import tensorflow as tf\n        with tf.device(policy.device):\n            return [policy.actor.weights,\n                    policy.critic.weights,\n                    policy.critic_target.weights]\n\n    def set_weights_fn(policy, weights):\n        actor_weights, critic_weights, critic_target_weights = weights\n        update_target_variables(\n            policy.actor.weights, actor_weights, tau=1.)\n        update_target_variables(\n            policy.critic.weights, critic_weights, tau=1.)\n        update_target_variables(\n            policy.critic_target.weights, critic_target_weights, tau=1.)\n\n    run(args, env_fn, policy_fn, get_weights_fn, set_weights_fn)\n'"
examples/run_apex_dqn.py,1,"b'import argparse\nimport numpy as np\nimport gym\nimport tensorflow as tf\n\nfrom tf2rl.algos.apex import apex_argument, run\nfrom tf2rl.algos.dqn import DQN\nfrom tf2rl.misc.target_update_ops import update_target_variables\nfrom tf2rl.networks.atari_model import AtariQFunc\n\n\nif __name__ == \'__main__\':\n    parser = apex_argument()\n    parser = DQN.get_argument(parser)\n    parser.add_argument(\'--atari\', action=\'store_true\')\n    parser.add_argument(\'--env-name\', type=str,\n                        default=""SpaceInvadersNoFrameskip-v4"")\n    args = parser.parse_args()\n\n    if args.atari:\n        env_name = args.env_name\n        n_warmup = 50000\n        target_replace_interval = 10000\n        batch_size = 32\n        optimizer = tf.keras.optimizers.Adam(\n            learning_rate=0.0000625, epsilon=1.5e-4)\n        epsilon_decay_rate = int(1e6)\n        QFunc = AtariQFunc\n    else:\n        env_name = ""CartPole-v0""\n        n_warmup = 500\n        target_replace_interval = 300\n        batch_size = 32\n        optimizer = None\n        epsilon_decay_rate = int(1e3)\n        QFunc = None\n\n    # Prepare env and policy function\n    def env_fn():\n        return gym.make(env_name)\n\n    def policy_fn(env, name, memory_capacity=int(1e6),\n                  gpu=-1, noise_level=0.3):\n        return DQN(\n            name=name,\n            enable_double_dqn=args.enable_double_dqn,\n            enable_dueling_dqn=args.enable_dueling_dqn,\n            enable_noisy_dqn=args.enable_noisy_dqn,\n            enable_categorical_dqn=args.enable_categorical_dqn,\n            state_shape=env.observation_space.shape,\n            action_dim=env.action_space.n,\n            n_warmup=n_warmup,\n            target_replace_interval=target_replace_interval,\n            batch_size=batch_size,\n            memory_capacity=memory_capacity,\n            discount=0.99,\n            epsilon=1.,\n            epsilon_min=0.1,\n            epsilon_decay_step=epsilon_decay_rate,\n            optimizer=optimizer,\n            update_interval=4,\n            q_func=QFunc,\n            gpu=gpu)\n\n    def get_weights_fn(policy):\n        return [policy.q_func.weights,\n                policy.q_func_target.weights]\n\n    def set_weights_fn(policy, weights):\n        q_func_weights, qfunc_target_weights = weights\n        update_target_variables(\n            policy.q_func.weights, q_func_weights, tau=1.)\n        update_target_variables(\n            policy.q_func_target.weights, qfunc_target_weights, tau=1.)\n\n    run(args, env_fn, policy_fn, get_weights_fn, set_weights_fn)\n'"
examples/run_bi_res_ddpg.py,0,"b'import gym\n\nfrom tf2rl.algos.bi_res_ddpg import BiResDDPG\nfrom tf2rl.experiments.trainer import Trainer\n\n\nif __name__ == \'__main__\':\n    parser = Trainer.get_argument()\n    parser = BiResDDPG.get_argument(parser)\n    parser.add_argument(\'--env-name\', type=str, default=""Pendulum-v0"")\n    parser.set_defaults(batch_size=100)\n    parser.set_defaults(n_warmup=10000)\n    args = parser.parse_args()\n\n    env = gym.make(args.env_name)\n    test_env = gym.make(args.env_name)\n    policy = BiResDDPG(\n        state_shape=env.observation_space.shape,\n        action_dim=env.action_space.high.size,\n        gpu=args.gpu,\n        eta=args.eta,\n        memory_capacity=args.memory_capacity,\n        max_action=env.action_space.high[0],\n        batch_size=args.batch_size,\n        n_warmup=args.n_warmup)\n    trainer = Trainer(policy, env, args, test_env=test_env)\n    trainer()\n'"
examples/run_ddpg.py,0,"b'import gym\n\nfrom tf2rl.algos.ddpg import DDPG\nfrom tf2rl.experiments.trainer import Trainer\n\n\nif __name__ == \'__main__\':\n    parser = Trainer.get_argument()\n    parser = DDPG.get_argument(parser)\n    parser.add_argument(\'--env-name\', type=str, default=""Pendulum-v0"")\n    parser.set_defaults(batch_size=100)\n    parser.set_defaults(n_warmup=10000)\n    args = parser.parse_args()\n\n    env = gym.make(args.env_name)\n    test_env = gym.make(args.env_name)\n    policy = DDPG(\n        state_shape=env.observation_space.shape,\n        action_dim=env.action_space.high.size,\n        gpu=args.gpu,\n        memory_capacity=args.memory_capacity,\n        max_action=env.action_space.high[0],\n        batch_size=args.batch_size,\n        n_warmup=args.n_warmup)\n    trainer = Trainer(policy, env, args, test_env=test_env)\n    trainer()\n'"
examples/run_dqn.py,0,"b'import gym\n\nfrom tf2rl.algos.dqn import DQN\nfrom tf2rl.experiments.trainer import Trainer\n\n\nif __name__ == \'__main__\':\n    parser = Trainer.get_argument()\n    parser = DQN.get_argument(parser)\n    parser.set_defaults(test_interval=2000)\n    parser.set_defaults(max_steps=100000)\n    parser.set_defaults(gpu=-1)\n    parser.set_defaults(n_warmup=500)\n    parser.set_defaults(batch_size=32)\n    parser.set_defaults(memory_capacity=int(1e4))\n    args = parser.parse_args()\n\n    env = gym.make(""CartPole-v0"")\n    test_env = gym.make(""CartPole-v0"")\n    policy = DQN(\n        enable_double_dqn=args.enable_double_dqn,\n        enable_dueling_dqn=args.enable_dueling_dqn,\n        enable_noisy_dqn=args.enable_noisy_dqn,\n        enable_categorical_dqn=args.enable_categorical_dqn,\n        state_shape=env.observation_space.shape,\n        action_dim=env.action_space.n,\n        target_replace_interval=300,\n        discount=0.99,\n        gpu=args.gpu,\n        memory_capacity=args.memory_capacity,\n        batch_size=args.batch_size,\n        n_warmup=args.n_warmup)\n    trainer = Trainer(policy, env, args, test_env=test_env)\n    trainer()\n'"
examples/run_dqn_atari.py,1,"b'import gym\n\nimport tensorflow as tf\n\nfrom tf2rl.algos.dqn import DQN\nfrom tf2rl.envs.atari_wrapper import wrap_dqn\nfrom tf2rl.experiments.trainer import Trainer\nfrom tf2rl.networks.atari_model import AtariQFunc as QFunc\n\n\nif __name__ == \'__main__\':\n    parser = Trainer.get_argument()\n    parser = DQN.get_argument(parser)\n    parser.add_argument(\'--env-name\', type=str,\n                        default=""SpaceInvadersNoFrameskip-v4"")\n    parser.set_defaults(episode_max_steps=108000)\n    parser.set_defaults(test_interval=10000)\n    parser.set_defaults(max_steps=int(1e9))\n    parser.set_defaults(save_model_interval=500000)\n    parser.set_defaults(gpu=0)\n    parser.set_defaults(show_test_images=True)\n    parser.set_defaults(memory_capacity=int(1e6))\n    args = parser.parse_args()\n\n    env = wrap_dqn(gym.make(args.env_name))\n    test_env = wrap_dqn(gym.make(args.env_name), reward_clipping=False)\n    # Following parameters are equivalent to DeepMind DQN paper\n    # https://www.nature.com/articles/nature14236\n    optimizer = tf.keras.optimizers.Adam(\n        learning_rate=0.0000625, epsilon=1.5e-4)  # This value is from Rainbow\n    policy = DQN(\n        enable_double_dqn=args.enable_double_dqn,\n        enable_dueling_dqn=args.enable_dueling_dqn,\n        enable_noisy_dqn=args.enable_noisy_dqn,\n        enable_categorical_dqn=args.enable_categorical_dqn,\n        state_shape=env.observation_space.shape,\n        action_dim=env.action_space.n,\n        n_warmup=50000,\n        target_replace_interval=10000,\n        batch_size=32,\n        memory_capacity=args.memory_capacity,\n        discount=0.99,\n        epsilon=1.,\n        epsilon_min=0.1,\n        epsilon_decay_step=int(1e6),\n        optimizer=optimizer,\n        update_interval=4,\n        q_func=QFunc,\n        gpu=args.gpu)\n    trainer = Trainer(policy, env, args, test_env=test_env)\n    trainer()\n'"
examples/run_gaifo_ddpg.py,0,"b'import gym\n\nfrom tf2rl.algos.ddpg import DDPG\nfrom tf2rl.algos.gaifo import GAIfO\nfrom tf2rl.experiments.irl_trainer import IRLTrainer\nfrom tf2rl.experiments.utils import restore_latest_n_traj\n\n\nif __name__ == \'__main__\':\n    parser = IRLTrainer.get_argument()\n    parser = GAIfO.get_argument(parser)\n    parser.add_argument(\'--env-name\', type=str, default=""Pendulum-v0"")\n    args = parser.parse_args()\n\n    if args.expert_path_dir is None:\n        print(""Plaese generate demonstrations first"")\n        print(""python examples/run_sac.py --env-name~Pendulum-v0 --save-test-path --test-interval=50000"")\n        exit()\n\n    units = [400, 300]\n\n    env = gym.make(args.env_name)\n    test_env = gym.make(args.env_name)\n    policy = DDPG(\n        state_shape=env.observation_space.shape,\n        action_dim=env.action_space.high.size,\n        max_action=env.action_space.high[0],\n        gpu=args.gpu,\n        actor_units=units,\n        critic_units=units,\n        n_warmup=10000,\n        batch_size=100)\n    irl = GAIfO(\n        state_shape=env.observation_space.shape,\n        units=units,\n        enable_sn=args.enable_sn,\n        batch_size=32,\n        gpu=args.gpu)\n    expert_trajs = restore_latest_n_traj(\n        args.expert_path_dir, n_path=20, max_steps=1000)\n    trainer = IRLTrainer(policy, env, args, irl, expert_trajs[""obses""],\n                         expert_trajs[""next_obses""], expert_trajs[""acts""], test_env)\n    trainer()\n'"
examples/run_gail_ddpg.py,0,"b'import gym\n\nfrom tf2rl.algos.ddpg import DDPG\nfrom tf2rl.algos.gail import GAIL\nfrom tf2rl.experiments.irl_trainer import IRLTrainer\nfrom tf2rl.experiments.utils import restore_latest_n_traj\n\n\nif __name__ == \'__main__\':\n    parser = IRLTrainer.get_argument()\n    parser = GAIL.get_argument(parser)\n    parser.add_argument(\'--env-name\', type=str, default=""Pendulum-v0"")\n    args = parser.parse_args()\n\n    if args.expert_path_dir is None:\n        print(""Plaese generate demonstrations first"")\n        print(""python examples/run_sac.py --env-name=Pendulum-v0 --save-test-path --test-interval=50000"")\n        exit()\n\n    units = [400, 300]\n\n    env = gym.make(args.env_name)\n    test_env = gym.make(args.env_name)\n    policy = DDPG(\n        state_shape=env.observation_space.shape,\n        action_dim=env.action_space.high.size,\n        max_action=env.action_space.high[0],\n        gpu=args.gpu,\n        actor_units=units,\n        critic_units=units,\n        n_warmup=10000,\n        batch_size=100)\n    irl = GAIL(\n        state_shape=env.observation_space.shape,\n        action_dim=env.action_space.high.size,\n        units=units,\n        enable_sn=args.enable_sn,\n        batch_size=32,\n        gpu=args.gpu)\n    expert_trajs = restore_latest_n_traj(\n        args.expert_path_dir, n_path=20, max_steps=1000)\n    trainer = IRLTrainer(policy, env, args, irl, expert_trajs[""obses""],\n                         expert_trajs[""next_obses""], expert_trajs[""acts""], test_env)\n    trainer()\n'"
examples/run_mpc.py,0,"b'import numpy as np\nimport gym\n\nfrom tf2rl.experiments.mpc_trainer import MPCTrainer, RandomPolicy\n\n\ndef reward_fn_pendulum(obses, next_obses, acts):\n    assert obses.ndim == next_obses.ndim == acts.ndim == 2\n    assert obses.shape[0] == next_obses.shape[0] == acts.shape[0]\n    acts = np.squeeze(acts)\n    thetas = np.arctan2(obses[:, 1], obses[:, 0])\n    theta_dots = obses[:, 2]\n\n    def angle_normalize(x):\n        return (((x+np.pi) % (2*np.pi)) - np.pi)\n\n    acts = np.clip(acts, -2, 2)\n    assert thetas.shape == theta_dots.shape == acts.shape\n    costs = angle_normalize(thetas)**2 + .1*theta_dots**2 + .001*(acts**2)\n    return -costs\n\n\nif __name__ == ""__main__"":\n    parser = MPCTrainer.get_argument()\n    parser.set_defaults(episode_max_steps=200)\n    args = parser.parse_args()\n\n    env = gym.make(""Pendulum-v0"")\n    test_env = gym.make(""Pendulum-v0"")\n\n    policy = RandomPolicy(\n        max_action=env.action_space.high[0],\n        act_dim=env.action_space.high.size)\n\n    trainer = MPCTrainer(policy, env, args, reward_fn=reward_fn_pendulum, test_env=test_env)\n    trainer()\n'"
examples/run_ppo.py,0,"b'import gym\n\nfrom tf2rl.algos.ppo import PPO\nfrom tf2rl.experiments.on_policy_trainer import OnPolicyTrainer\nfrom tf2rl.envs.utils import is_discrete, get_act_dim\n\n\nif __name__ == \'__main__\':\n    parser = OnPolicyTrainer.get_argument()\n    parser = PPO.get_argument(parser)\n    parser.add_argument(\'--env-name\', type=str,\n                        default=""Pendulum-v0"")\n    parser.set_defaults(test_interval=20480)\n    parser.set_defaults(save_summary_interval=20480)\n    parser.set_defaults(max_steps=int(2e6))\n    parser.set_defaults(horizon=2048)\n    parser.set_defaults(batch_size=64)\n    parser.set_defaults(gpu=-1)\n    args = parser.parse_args()\n\n    env = gym.make(args.env_name)\n    test_env = gym.make(args.env_name)\n\n    policy = PPO(\n        state_shape=env.observation_space.shape,\n        action_dim=get_act_dim(env.action_space),\n        is_discrete=is_discrete(env.action_space),\n        max_action=None if is_discrete(\n            env.action_space) else env.action_space.high[0],\n        batch_size=args.batch_size,\n        actor_units=(64, 64),\n        critic_units=(64, 64),\n        n_epoch=10,\n        lr_actor=3e-4,\n        lr_critic=3e-4,\n        hidden_activation_actor=""tanh"",\n        hidden_activation_critic=""tanh"",\n        discount=0.99,\n        lam=0.95,\n        entropy_coef=0.,\n        horizon=args.horizon,\n        normalize_adv=args.normalize_adv,\n        enable_gae=args.enable_gae,\n        gpu=args.gpu)\n    trainer = OnPolicyTrainer(policy, env, args, test_env=test_env)\n    trainer()\n'"
examples/run_ppo_atari.py,0,"b'import gym\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.algos.ppo import PPO\nfrom tf2rl.envs.atari_wrapper import wrap_dqn\nfrom tf2rl.experiments.on_policy_trainer import OnPolicyTrainer\nfrom tf2rl.networks.atari_model import AtariCategoricalActorCritic\n\n\nif __name__ == \'__main__\':\n    parser = OnPolicyTrainer.get_argument()\n    parser = PPO.get_argument(parser)\n    parser.add_argument(\'--env-name\', type=str,\n                        default=""PongNoFrameskip-v4"")\n    parser.set_defaults(episode_max_steps=108000)\n    parser.set_defaults(horizon=1024)\n    parser.set_defaults(test_interval=204800)\n    parser.set_defaults(max_steps=int(1e9))\n    parser.set_defaults(save_model_interval=2048000)\n    parser.set_defaults(gpu=0)\n    parser.set_defaults(show_test_images=True)\n    args = parser.parse_args()\n\n    env = wrap_dqn(gym.make(args.env_name))\n    test_env = wrap_dqn(gym.make(args.env_name), reward_clipping=False)\n\n    state_shape = env.observation_space.shape\n    action_dim = env.action_space.n\n\n    actor_critic = AtariCategoricalActorCritic(\n        state_shape=state_shape, action_dim=action_dim)\n\n    policy = PPO(\n        state_shape=state_shape,\n        action_dim=action_dim,\n        is_discrete=True,\n        actor_critic=actor_critic,\n        batch_size=args.batch_size,\n        n_epoch=3,\n        lr_actor=2.5e-4,\n        lr_critic=2.5e-4,\n        discount=0.99,\n        lam=0.95,\n        horizon=args.horizon,\n        normalize_adv=args.normalize_adv,\n        enable_gae=args.enable_gae,\n        gpu=args.gpu)\n    trainer = OnPolicyTrainer(policy, env, args, test_env=test_env)\n    trainer()\n'"
examples/run_sac.py,0,"b'import gym\n\nfrom tf2rl.algos.sac import SAC\nfrom tf2rl.experiments.trainer import Trainer\n\n\nif __name__ == \'__main__\':\n    parser = Trainer.get_argument()\n    parser = SAC.get_argument(parser)\n    parser.add_argument(\'--env-name\', type=str, default=""Pendulum-v0"")\n    parser.set_defaults(batch_size=100)\n    parser.set_defaults(n_warmup=10000)\n    parser.set_defaults(max_steps=3e6)\n    args = parser.parse_args()\n\n    env = gym.make(args.env_name)\n    test_env = gym.make(args.env_name)\n    policy = SAC(\n        state_shape=env.observation_space.shape,\n        action_dim=env.action_space.high.size,\n        gpu=args.gpu,\n        memory_capacity=args.memory_capacity,\n        max_action=env.action_space.high[0],\n        batch_size=args.batch_size,\n        n_warmup=args.n_warmup,\n        alpha=args.alpha,\n        auto_alpha=args.auto_alpha)\n    trainer = Trainer(policy, env, args, test_env=test_env)\n    trainer()\n'"
examples/run_sac_discrete.py,0,"b'import gym\n\nfrom tf2rl.algos.sac_discrete import SACDiscrete\nfrom tf2rl.experiments.trainer import Trainer\nfrom tf2rl.envs.utils import is_atari_env\nfrom tf2rl.envs.atari_wrapper import wrap_dqn\nfrom tf2rl.networks.atari_model import AtariQFunc, AtariCategoricalActor\n\n\nif __name__ == \'__main__\':\n    parser = Trainer.get_argument()\n    parser = SACDiscrete.get_argument(parser)\n    parser.set_defaults(test_interval=2000)\n    parser.set_defaults(max_steps=100000)\n    parser.set_defaults(gpu=-1)\n    parser.set_defaults(n_warmup=500)\n    parser.set_defaults(batch_size=32)\n    parser.set_defaults(memory_capacity=int(1e4))\n    parser.add_argument(\'--env-name\', type=str,\n                        default=""SpaceInvadersNoFrameskip-v4"")\n    args = parser.parse_args()\n\n    env = gym.make(args.env_name)\n    test_env = gym.make(args.env_name)\n\n    if is_atari_env(env):\n        # Parameters come from Appendix.B in original paper.\n        # See https://arxiv.org/abs/1910.07207\n        parser.set_defaults(episode_max_steps=108000)\n        parser.set_defaults(test_interval=int(1e5))\n        parser.set_defaults(show_test_images=True)\n        parser.set_defaults(max_steps=int(1e9))\n        parser.set_defaults(target_update_interval=8000)\n        parser.set_defaults(n_warmup=int(2e4))\n        args = parser.parse_args()\n        if args.gpu == -1:\n            print(""Are you sure you\'re trying to solve Atari without GPU?"")\n\n        env = wrap_dqn(env, wrap_ndarray=True)\n        test_env = wrap_dqn(test_env, wrap_ndarray=True, reward_clipping=False)\n        policy = SACDiscrete(\n            state_shape=env.observation_space.shape,\n            action_dim=env.action_space.n,\n            discount=0.99,\n            critic_fn=AtariQFunc,\n            actor_fn=AtariCategoricalActor,\n            lr=3e-4,\n            memory_capacity=args.memory_capacity,\n            batch_size=64,\n            n_warmup=args.n_warmup,\n            update_interval=4,\n            target_update_interval=args.target_update_interval,\n            auto_alpha=args.auto_alpha,\n            gpu=args.gpu)\n    else:\n        policy = SACDiscrete(\n            state_shape=env.observation_space.shape,\n            action_dim=env.action_space.n,\n            discount=0.99,\n            memory_capacity=args.memory_capacity,\n            batch_size=args.batch_size,\n            n_warmup=args.n_warmup,\n            target_update_interval=args.target_update_interval,\n            auto_alpha=args.auto_alpha,\n            gpu=args.gpu)\n    trainer = Trainer(policy, env, args, test_env=test_env)\n    trainer()\n'"
examples/run_td3.py,0,"b'import gym\n\nfrom tf2rl.algos.td3 import TD3\nfrom tf2rl.experiments.trainer import Trainer\n\n\nif __name__ == \'__main__\':\n    parser = Trainer.get_argument()\n    parser = TD3.get_argument(parser)\n    parser.add_argument(\'--env-name\', type=str, default=""Pendulum-v0"")\n    parser.set_defaults(batch_size=100)\n    parser.set_defaults(n_warmup=10000)\n    args = parser.parse_args()\n\n    env = gym.make(args.env_name)\n    test_env = gym.make(args.env_name)\n    policy = TD3(\n        state_shape=env.observation_space.shape,\n        action_dim=env.action_space.high.size,\n        gpu=args.gpu,\n        memory_capacity=args.memory_capacity,\n        max_action=env.action_space.high[0],\n        batch_size=args.batch_size,\n        n_warmup=args.n_warmup)\n    trainer = Trainer(policy, env, args, test_env=test_env)\n    trainer()\n'"
examples/run_vail_ddpg.py,0,"b'import gym\n\nfrom tf2rl.algos.ddpg import DDPG\nfrom tf2rl.algos.vail import VAIL\nfrom tf2rl.experiments.irl_trainer import IRLTrainer\nfrom tf2rl.experiments.utils import restore_latest_n_traj\n\n\nif __name__ == \'__main__\':\n    parser = IRLTrainer.get_argument()\n    parser = VAIL.get_argument(parser)\n    parser.add_argument(\'--env-name\', type=str, default=""Pendulum-v0"")\n    args = parser.parse_args()\n\n    if args.expert_path_dir is None:\n        print(""Plaese generate demonstrations first"")\n        print(""python examples/run_sac.py --env-name=Pendulum-v0 --save-test-path --test-interval=50000"")\n        exit()\n\n    units = [400, 300]\n\n    env = gym.make(args.env_name)\n    test_env = gym.make(args.env_name)\n    policy = DDPG(\n        state_shape=env.observation_space.shape,\n        action_dim=env.action_space.high.size,\n        max_action=env.action_space.high[0],\n        gpu=args.gpu,\n        actor_units=units,\n        critic_units=units,\n        n_warmup=10000,\n        batch_size=100)\n    irl = VAIL(\n        state_shape=env.observation_space.shape,\n        action_dim=env.action_space.high.size,\n        units=units,\n        enable_sn=args.enable_sn,\n        batch_size=32,\n        gpu=args.gpu)\n    expert_trajs = restore_latest_n_traj(\n        args.expert_path_dir, n_path=20, max_steps=1000)\n    trainer = IRLTrainer(policy, env, args, irl, expert_trajs[""obses""],\n                         expert_trajs[""next_obses""], expert_trajs[""acts""], test_env)\n    trainer()\n'"
examples/run_vpg.py,0,"b'import gym\n\nfrom tf2rl.algos.vpg import VPG\nfrom tf2rl.experiments.on_policy_trainer import OnPolicyTrainer\nfrom tf2rl.envs.utils import is_discrete, get_act_dim\n\n\nif __name__ == \'__main__\':\n    parser = OnPolicyTrainer.get_argument()\n    parser = VPG.get_argument(parser)\n    parser.add_argument(\'--env-name\', type=str,\n                        default=""Pendulum-v0"")\n    parser.set_defaults(test_interval=10240)\n    parser.set_defaults(max_steps=int(1e7))\n    parser.set_defaults(horizon=512)\n    parser.set_defaults(batch_size=32)\n    parser.set_defaults(gpu=-1)\n    args = parser.parse_args()\n\n    env = gym.make(args.env_name)\n    test_env = gym.make(args.env_name)\n    policy = VPG(\n        state_shape=env.observation_space.shape,\n        action_dim=get_act_dim(env.action_space),\n        is_discrete=is_discrete(env.action_space),\n        max_action=None if is_discrete(\n            env.action_space) else env.action_space.high[0],\n        batch_size=args.batch_size,\n        actor_units=[32, 32],\n        critic_units=[32, 32],\n        discount=0.9,\n        horizon=args.horizon,\n        fix_std=True,\n        normalize_adv=args.normalize_adv,\n        enable_gae=args.enable_gae,\n        gpu=args.gpu)\n    trainer = OnPolicyTrainer(policy, env, args, test_env=test_env)\n    trainer()\n'"
tests/__init__.py,0,"b""import sys\n\nsys.path.append('tf2rl')\n"""
tf2rl/__init__.py,0,b''
tests/algos/__init__.py,0,b'from .common import CommonAlgos'
tests/algos/common.py,0,"b'import unittest\nimport numpy as np\nimport gym\n\n\nclass CommonAlgos(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # TODO: Remove dependencies to gym\n        cls.discrete_env = gym.make(""CartPole-v0"")\n        cls.continuous_env = gym.make(""Pendulum-v0"")\n        cls.batch_size = 32\n        cls.agent = None\n\n\nclass CommonOffPolAlgos(CommonAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.env = None\n        cls.action_dim = None\n        cls.is_discrete = True\n\n    def test_get_action(self):\n        if self.agent is None:\n            return\n        # Single input\n        state = self.env.reset().astype(np.float32)\n        action_train = self.agent.get_action(state, test=False)\n        action_test = self.agent.get_action(state, test=True)\n        if self.is_discrete:\n            self.assertTrue(type(action_train) ==\n                            np.int64 or type(action_train) == int)\n            self.assertTrue(type(action_test) ==\n                            np.int64 or type(action_test) == int)\n        else:\n            self.assertEqual(action_train.shape[0], self.action_dim)\n            self.assertEqual(action_test.shape[0], self.action_dim)\n\n        # Multiple inputs\n        states = np.zeros(\n            shape=(self.batch_size, state.shape[0]), dtype=np.float32)\n        actions_train = self.agent.get_action(states, test=False)\n        actions_test = self.agent.get_action(states, test=True)\n\n        if self.is_discrete:\n            self.assertEqual(\n                actions_train.shape, (self.batch_size,))\n            self.assertEqual(\n                actions_test.shape, (self.batch_size,))\n        else:\n            self.assertEqual(\n                actions_train.shape, (self.batch_size, self.action_dim))\n            self.assertEqual(\n                actions_test.shape, (self.batch_size, self.action_dim))\n\n    def test_get_action_greedy(self):\n        if self.agent is None:\n            return\n        # Multiple inputs\n        states = np.zeros(\n            shape=(self.batch_size, self.env.reset().astype(np.float32).shape[0]), dtype=np.float32)\n        actions_train = self.agent.get_action(states, test=False)\n        actions_test = self.agent.get_action(states, test=True)\n\n        # All actions should be same if `test=True`, and not same if `test=False`\n        if self.is_discrete:\n            self.assertEqual(np.prod(np.unique(actions_test).shape), 1)\n            self.assertGreater(np.prod(np.unique(actions_train).shape), 1)\n        else:\n            self.assertEqual(np.prod(np.all(actions_test == actions_test[0, :], axis=0)), 1)\n            self.assertEqual(np.prod(np.all(actions_train == actions_train[0, :], axis=0)), 0)\n\n    def test_train(self):\n        if self.agent is None:\n            return\n        rewards = np.zeros(shape=(self.batch_size, 1), dtype=np.float32)\n        dones = np.zeros(shape=(self.batch_size, 1), dtype=np.float32)\n        obses = np.zeros(\n            shape=(self.batch_size,)+self.env.observation_space.shape,\n            dtype=np.float32)\n        acts = np.zeros(\n            shape=(self.batch_size, self.action_dim,),\n            dtype=np.float32)\n        self.agent.train(\n            obses, acts, obses, rewards, dones)\n\n    def test_compute_td_error(self):\n        if self.agent is None:\n            return\n        rewards = np.zeros(shape=(self.batch_size, 1), dtype=np.float32)\n        dones = np.zeros(shape=(self.batch_size, 1), dtype=np.float32)\n        obses = np.zeros(\n            shape=(self.batch_size,)+self.env.observation_space.shape,\n            dtype=np.float32)\n        acts = np.zeros(\n            shape=(self.batch_size, self.continuous_env.action_space.low.size,),\n            dtype=np.float32)\n        self.agent.compute_td_error(\n            states=obses, actions=acts, next_states=obses,\n            rewards=rewards, dones=dones)\n\n\nclass CommonOffPolContinuousAlgos(CommonOffPolAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.env = cls.continuous_env\n        cls.action_dim = cls.continuous_env.action_space.low.size\n        cls.is_discrete = False\n\n\nclass CommonOffPolDiscreteAlgos(CommonOffPolAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.env = cls.discrete_env\n        cls.action_dim = 1\n        cls.is_discrete = True\n\n\nclass CommonOnPolActorCritic(CommonAlgos):\n    def test_get_action(self):\n        if self.agent is None:\n            return\n        # Single input\n        state = self.env.reset().astype(np.float32)\n        action_train, logp_train = self.agent.get_action(state, test=False)\n        action_test, logp_test = self.agent.get_action(state, test=True)\n        if self.is_discrete:\n            self.assertTrue(type(action_train) ==\n                            np.int64 or type(action_train) == int)\n            self.assertTrue(type(action_test) ==\n                            np.int64 or type(action_test) == int)\n        else:\n            self.assertEqual(action_train.shape[0], self.action_dim)\n            self.assertEqual(action_test.shape[0], self.action_dim)\n        self.assertEqual(logp_train.shape[0], 1)\n        self.assertEqual(logp_test.shape[0], 1)\n\n        # Multiple inputs\n        states = np.zeros(\n            shape=(self.batch_size, state.shape[0]), dtype=np.float32)\n        actions_train, logps_train = self.agent.get_action(states, test=False)\n        actions_test, logps_test = self.agent.get_action(states, test=True)\n        if self.is_discrete:\n            self.assertEqual(\n                actions_train.shape, (self.batch_size,))\n            self.assertEqual(\n                actions_test.shape, (self.batch_size,))\n        else:\n            self.assertEqual(\n                actions_train.shape, (self.batch_size, self.action_dim))\n            self.assertEqual(\n                actions_test.shape, (self.batch_size, self.action_dim))\n        self.assertEqual(logps_train.shape, (self.batch_size,))\n        self.assertEqual(logps_test.shape, (self.batch_size,))\n\n    def test_train(self):\n        if self.agent is None:\n            return\n        state = self.env.reset().astype(np.float32)\n        obses = np.zeros(\n            shape=(self.batch_size,)+state.shape,\n            dtype=np.float32)\n        acts = np.zeros(\n            shape=(self.batch_size, self.action_dim),\n            dtype=np.int32 if self.is_discrete else np.float32)\n        advs = np.ones(\n            shape=(self.batch_size, 1),\n            dtype=np.float32)\n        logps = np.ones_like(advs)\n        returns = np.zeros(\n            shape=(self.batch_size, 1),\n            dtype=np.float32)\n\n        self.agent.train(obses, acts, advs, logps, returns)\n\n\nclass CommonOnPolActorCriticContinuousAlgos(CommonOnPolActorCritic):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.env = cls.continuous_env\n        cls.action_dim = cls.continuous_env.action_space.low.size\n        cls.is_discrete = False\n\n\nclass CommonOnPolActorCriticDiscreteAlgos(CommonOnPolActorCritic):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.env = cls.discrete_env\n        cls.action_dim = 1\n        cls.is_discrete = True\n\n\nclass CommonIRLAlgos(CommonAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.irl_discrete = None\n        cls.irl_continuous = None\n\n    def test_inference_discrete(self):\n        if self.irl_discrete is None:\n            return\n        state = np.zeros(\n            shape=(self.discrete_env.observation_space.low.size,),\n            dtype=np.float32)\n        action = np.zeros(\n            shape=(self.discrete_env.action_space.n,),\n            dtype=np.float32)\n        action[self.discrete_env.action_space.sample()] = 1.\n        self.irl_discrete.inference(state, action, state)\n\n    def test_inference_continuous(self):\n        if self.irl_continuous is None:\n            return\n        state = np.zeros(\n            shape=(self.continuous_env.observation_space.low.size,),\n            dtype=np.float32)\n        action = np.zeros(\n            shape=(self.continuous_env.action_space.low.size,),\n            dtype=np.float32)\n        self.irl_continuous.inference(state, action, state)\n\n    def test_train_discrete(self):\n        if self.irl_discrete is None:\n            return\n        states = np.zeros(\n            shape=(self.batch_size, self.discrete_env.observation_space.low.size),\n            dtype=np.float32)\n        actions = np.zeros(\n            shape=(self.batch_size, self.discrete_env.action_space.n),\n            dtype=np.float32)\n        self.irl_discrete.train(\n            agent_states=states,\n            agent_acts=actions,\n            agent_next_states=states,\n            expert_states=states,\n            expert_acts=actions,\n            expert_next_states=states)\n\n    def test_train_continuous(self):\n        if self.irl_continuous is None:\n            return\n        states = np.zeros(\n            shape=(self.batch_size, self.continuous_env.observation_space.low.size),\n            dtype=np.float32)\n        actions = np.zeros(\n            shape=(self.batch_size, self.continuous_env.action_space.low.size),\n            dtype=np.float32)\n        self.irl_continuous.train(\n            agent_states=states,\n            agent_acts=actions,\n            agent_next_states=states,\n            expert_states=states,\n            expert_acts=actions,\n            expert_next_states=states)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/algos/test_apex.py,0,"b'# Hard to test each function, so just execute shortly\n\nimport unittest\nimport gym\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.algos.apex import apex_argument, run\nfrom tf2rl.misc.target_update_ops import update_target_variables\n\n\nclass TestApeX(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.parser = apex_argument()\n        cls.parser.set_defaults(n_training=10)\n        cls.parser.set_defaults(param_update_freq=1)\n        cls.parser.set_defaults(test_freq=10)\n        cls.parser.set_defaults(n_explorer=2)\n        cls.parser.set_defaults(n_env=4)\n        cls.parser.set_defaults(local_buffer_size=64)\n\n    def test_run_discrete(self):\n        from tf2rl.algos.dqn import DQN\n        parser = DQN.get_argument(self.parser)\n        parser.set_defaults(n_warmup=1)\n        args, _ = parser.parse_known_args()\n\n        def env_fn():\n            return gym.make(""CartPole-v0"")\n\n        def policy_fn(env, name, memory_capacity=int(1e6), gpu=-1, *args, **kwargs):\n            return DQN(\n                name=name,\n                state_shape=env.observation_space.shape,\n                action_dim=env.action_space.n,\n                n_warmup=500,\n                target_replace_interval=300,\n                batch_size=32,\n                memory_capacity=memory_capacity,\n                discount=0.99,\n                gpu=-1)\n\n        def get_weights_fn(policy):\n            return [policy.q_func.weights,\n                    policy.q_func_target.weights]\n\n        def set_weights_fn(policy, weights):\n            q_func_weights, qfunc_target_weights = weights\n            update_target_variables(\n                policy.q_func.weights, q_func_weights, tau=1.)\n            update_target_variables(\n                policy.q_func_target.weights, qfunc_target_weights, tau=1.)\n\n        run(args, env_fn, policy_fn, get_weights_fn, set_weights_fn)\n\n    def test_run_continuous(self):\n        from tf2rl.algos.ddpg import DDPG\n        parser = DDPG.get_argument(self.parser)\n        parser.set_defaults(n_warmup=1)\n        args, _ = parser.parse_known_args()\n\n        def env_fn():\n            return gym.make(\'Pendulum-v0\')\n\n        def policy_fn(env, name, memory_capacity=int(1e6), gpu=-1, *args, **kwargs):\n            return DDPG(\n                state_shape=env.observation_space.shape,\n                action_dim=env.action_space.high.size,\n                n_warmup=500,\n                gpu=-1)\n\n        def get_weights_fn(policy):\n            return [policy.actor.weights,\n                    policy.critic.weights,\n                    policy.critic_target.weights]\n\n        def set_weights_fn(policy, weights):\n            actor_weights, critic_weights, critic_target_weights = weights\n            update_target_variables(\n                policy.actor.weights, actor_weights, tau=1.)\n            update_target_variables(\n                policy.critic.weights, critic_weights, tau=1.)\n            update_target_variables(\n                policy.critic_target.weights, critic_target_weights, tau=1.)\n\n        run(args, env_fn, policy_fn, get_weights_fn, set_weights_fn)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/algos/test_bi_res_ddpg.py,0,"b""import unittest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.algos.bi_res_ddpg import BiResDDPG\nfrom tests.algos.common import CommonOffPolContinuousAlgos\n\n\nclass TestBiResDDPG(CommonOffPolContinuousAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.agent = BiResDDPG(\n            state_shape=cls.continuous_env.observation_space.shape,\n            action_dim=cls.continuous_env.action_space.low.size,\n            batch_size=cls.batch_size,\n            gpu=-1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/algos/test_ddpg.py,0,"b""import unittest\n\nfrom tf2rl.algos.ddpg import DDPG\nfrom tests.algos.common import CommonOffPolContinuousAlgos\n\n\nclass TestDDPG(CommonOffPolContinuousAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.agent = DDPG(\n            state_shape=cls.continuous_env.observation_space.shape,\n            action_dim=cls.continuous_env.action_space.low.size,\n            batch_size=cls.batch_size,\n            sigma=0.5,  # Make noise bigger to easier to test\n            gpu=-1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/algos/test_dqn.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom tf2rl.algos.dqn import DQN\nfrom tests.algos.common import CommonOffPolDiscreteAlgos\n\n\nclass TestDQN(CommonOffPolDiscreteAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.agent = DQN(\n            state_shape=cls.discrete_env.observation_space.shape,\n            action_dim=cls.discrete_env.action_space.n,\n            batch_size=cls.batch_size,\n            epsilon=1.,\n            gpu=-1)\n\n\nclass TestDuelingDoubleDQN(CommonOffPolDiscreteAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.agent = DQN(\n            state_shape=cls.discrete_env.observation_space.shape,\n            action_dim=cls.discrete_env.action_space.n,\n            batch_size=cls.batch_size,\n            enable_double_dqn=True,\n            enable_dueling_dqn=True,\n            epsilon=1.,\n            gpu=-1)\n\n\nclass TestNoisyDQN(CommonOffPolDiscreteAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.agent = DQN(\n            state_shape=cls.discrete_env.observation_space.shape,\n            action_dim=cls.discrete_env.action_space.n,\n            batch_size=cls.batch_size,\n            enable_noisy_dqn=True,\n            epsilon=1.,\n            gpu=-1)\n\n    def get_actions(self, states, test):\n        if test:\n            return self.agent.q_func(states).numpy()\n        else:\n            return np.array([self.agent.q_func(np.expand_dims(state, axis=0)).numpy()[0] for state in states])\n\n    def test_get_action_greedy(self):\n        states = np.zeros(\n            shape=(self.batch_size, self.env.reset().astype(np.float32).shape[0]), dtype=np.float32)\n        q_values = np.array([self.agent.q_func(np.expand_dims(state, axis=0)).numpy()[0] for state in states])\n        self.assertEqual(np.prod(np.all(q_values == q_values[0, :], axis=0)), 0)\n\n\nclass TestCategoricalDQN(CommonOffPolDiscreteAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.agent = DQN(\n            state_shape=cls.discrete_env.observation_space.shape,\n            action_dim=cls.discrete_env.action_space.n,\n            batch_size=cls.batch_size,\n            enable_categorical_dqn=True,\n            enable_dueling_dqn=True,\n            epsilon=1.,\n            gpu=-1)\n\n\nclass TestCategoricalDuelingDQN(CommonOffPolDiscreteAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.agent = DQN(\n            state_shape=cls.discrete_env.observation_space.shape,\n            action_dim=cls.discrete_env.action_space.n,\n            batch_size=cls.batch_size,\n            enable_categorical_dqn=True,\n            epsilon=1.,\n            gpu=-1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/algos/test_gaifo.py,0,"b""import unittest\n\nfrom tf2rl.algos.gaifo import GAIfO\nfrom tests.algos.common import CommonIRLAlgos\n\n\nclass TestGAIfO(CommonIRLAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.irl_discrete = GAIfO(\n            state_shape=cls.discrete_env.observation_space.shape,\n            gpu=-1)\n        cls.irl_continuous = GAIfO(\n            state_shape=cls.continuous_env.observation_space.shape,\n            gpu=-1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/algos/test_gail.py,0,"b""import unittest\n\nfrom tf2rl.algos.gail import GAIL\nfrom tests.algos.common import CommonIRLAlgos\n\n\nclass TestGAIL(CommonIRLAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.irl_discrete = GAIL(\n            state_shape=cls.discrete_env.observation_space.shape,\n            action_dim=cls.discrete_env.action_space.n,\n            gpu=-1)\n        cls.irl_continuous = GAIL(\n            state_shape=cls.continuous_env.observation_space.shape,\n            action_dim=cls.continuous_env.action_space.low.size,\n            gpu=-1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/algos/test_ppo.py,0,"b""import unittest\nimport gym\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.algos.ppo import PPO\nfrom tests.algos.common import CommonOnPolActorCriticContinuousAlgos, CommonOnPolActorCriticDiscreteAlgos\n\n\nclass TestContinuousVPG(CommonOnPolActorCriticContinuousAlgos):\n    @classmethod\n    def setUpClass(cls):\n        cls.agent = PPO(\n            state_shape=cls.continuous_env.observation_space.shape,\n            action_dim=cls.continuous_env.action_space.low.size,\n            is_discrete=False,\n            gpu=-1)\n\n\nclass TestDiscreteVPG(CommonOnPolActorCriticDiscreteAlgos):\n    @classmethod\n    def setUpClass(cls):\n        cls.agent = PPO(\n            state_shape=cls.discrete_env.observation_space.shape,\n            action_dim=cls.discrete_env.action_space.n,\n            is_discrete=True,\n            gpu=-1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/algos/test_sac.py,0,"b""import unittest\n\nfrom tf2rl.algos.sac import SAC\nfrom tests.algos.common import CommonOffPolContinuousAlgos\n\n\nclass TestSAC(CommonOffPolContinuousAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.agent = SAC(\n            state_shape=cls.continuous_env.observation_space.shape,\n            action_dim=cls.continuous_env.action_space.low.size,\n            batch_size=cls.batch_size,\n            gpu=-1)\n\n\nclass TestSACAutoAlpha(CommonOffPolContinuousAlgos):\n    # TODO: Skip duplicated tests called in TestSAC\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.agent = SAC(\n            state_shape=cls.continuous_env.observation_space.shape,\n            action_dim=cls.continuous_env.action_space.low.size,\n            batch_size=cls.batch_size,\n            auto_alpha=True,\n            gpu=-1)\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/algos/test_sac_discrete.py,0,"b""import unittest\n\nfrom tf2rl.algos.sac_discrete import SACDiscrete\nfrom tests.algos.common import CommonOffPolDiscreteAlgos\n\n\nclass TestSACDiscrete(CommonOffPolDiscreteAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.agent = SACDiscrete(\n            state_shape=cls.discrete_env.observation_space.shape,\n            action_dim=cls.discrete_env.action_space.n,\n            batch_size=cls.batch_size,\n            gpu=-1)\n\n\nclass TestSACDiscreteAutoAlpha(CommonOffPolDiscreteAlgos):\n    # TODO: Skip duplicated tests called in TestSACDiscrete\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.agent = SACDiscrete(\n            state_shape=cls.discrete_env.observation_space.shape,\n            action_dim=cls.discrete_env.action_space.n,\n            batch_size=cls.batch_size,\n            auto_alpha=True,\n            gpu=-1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/algos/test_td3.py,0,"b""import unittest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.algos.td3 import TD3\nfrom tests.algos.common import CommonOffPolContinuousAlgos\n\n\nclass TestTD3(CommonOffPolContinuousAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.agent = TD3(\n            state_shape=cls.continuous_env.observation_space.shape,\n            action_dim=cls.continuous_env.action_space.low.size,\n            batch_size=cls.batch_size,\n            gpu=-1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/algos/test_vail.py,0,"b""import unittest\nimport numpy as np\n\nfrom tf2rl.algos.vail import VAIL\nfrom tests.algos.common import CommonIRLAlgos\n\n\nclass TestVAIL(CommonIRLAlgos):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.irl_discrete = VAIL(\n            state_shape=cls.discrete_env.observation_space.shape,\n            action_dim=cls.discrete_env.action_space.n,\n            gpu=-1)\n        cls.irl_continuous = VAIL(\n            state_shape=cls.continuous_env.observation_space.shape,\n            action_dim=cls.continuous_env.action_space.low.size,\n            gpu=-1)\n\n    def test__compute_kl_latent(self):\n        means = np.zeros(\n            shape=(self.batch_size, self.continuous_env.action_space.high.size))\n        log_stds = np.zeros_like(means)\n        results = self.irl_continuous._compute_kl_latent(\n            means=means, log_stds=log_stds)\n        np.testing.assert_array_equal(results, np.zeros_like(results))\n\n        means = np.random.rand(\n            self.batch_size, self.continuous_env.action_space.high.size)\n        log_stds = np.random.rand(\n            self.batch_size, self.continuous_env.action_space.high.size)\n        results = self.irl_continuous._compute_kl_latent(\n            means=means, log_stds=log_stds)\n        np.testing.assert_equal(np.any(np.not_equal(\n            results, np.zeros_like(results))), True)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/algos/test_vpg.py,0,"b""import unittest\nimport gym\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.algos.vpg import VPG\nfrom tests.algos.common import CommonOnPolActorCriticContinuousAlgos, CommonOnPolActorCriticDiscreteAlgos\n\n\nclass TestContinuousVPG(CommonOnPolActorCriticContinuousAlgos):\n    @classmethod\n    def setUpClass(cls):\n        cls.agent = VPG(\n            state_shape=cls.continuous_env.observation_space.shape,\n            action_dim=cls.continuous_env.action_space.low.size,\n            is_discrete=False,\n            gpu=-1)\n\n\nclass TestDiscreteVPG(CommonOnPolActorCriticDiscreteAlgos):\n    @classmethod\n    def setUpClass(cls):\n        cls.agent = VPG(\n            state_shape=cls.discrete_env.observation_space.shape,\n            action_dim=cls.discrete_env.action_space.n,\n            is_discrete=True,\n            gpu=-1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/distributions/__init__.py,0,b''
tests/distributions/common.py,0,b'import unittest\n\n\nclass CommonDist(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.dim = 3\n        cls.batch_size = 4\n        cls.dist = None\n\n    def _skip_test_in_parent(self):\n        if self.dist is None:\n            return\n        else:\n            raise NotImplementedError\n\n    def test_kl(self):\n        self._skip_test_in_parent()\n\n    def test_log_likelihood(self):\n        self._skip_test_in_parent()\n\n    def test_ent(self):\n        self._skip_test_in_parent()\n\n    def test_sample(self):\n        self._skip_test_in_parent()\n'
tests/distributions/test_categorical.py,0,"b'import unittest\nimport numpy as np\n\nfrom tf2rl.distributions.categorical import Categorical\nfrom tests.distributions.common import CommonDist\n\n\nclass TestCategorical(CommonDist):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.dist = Categorical(dim=cls.dim)\n        prob = np.zeros(shape=(cls.dim), dtype=np.float32)\n        prob[0] = 1.\n        cls.param = {\n            ""prob"": np.tile(prob, (1, 1))}  # [1, dim]\n        cls.params = {\n            ""prob"": np.tile(prob, (cls.batch_size, 1))}  # [batch_size, dim]\n\n    def test_kl(self):\n        # KL of same distribution should be zero\n        np.testing.assert_array_equal(\n            self.dist.kl(self.param, self.param),\n            np.zeros(shape=(1,)))\n        np.testing.assert_array_equal(\n            self.dist.kl(self.params, self.params),\n            np.zeros(shape=(self.batch_size,)))\n\n        # Add tests with not same distribution\n\n    def test_log_likelihood(self):\n        pass\n\n    def test_ent(self):\n        pass\n\n    def test_sample(self):\n        samples = self.dist.sample(self.param)\n        self.assertEqual(samples.shape, (1, 1))\n        samples = self.dist.sample(self.params)\n        self.assertEqual(samples.shape, (self.batch_size, 1))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/distributions/test_diagonal_gaussian.py,0,"b'import unittest\nimport numpy as np\n\nfrom tf2rl.distributions.diagonal_gaussian import DiagonalGaussian\nfrom tests.distributions.common import CommonDist\n\n\nclass TestDiagonalGaussian(CommonDist):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.dist = DiagonalGaussian(dim=cls.dim)\n        cls.param = param = {\n            ""mean"": np.zeros(shape=(1, cls.dim), dtype=np.float32),\n            ""log_std"": np.ones(shape=(1, cls.dim), dtype=np.float32)*np.log(1.)}\n        cls.params = {\n            ""mean"": np.zeros(shape=(cls.batch_size, cls.dim), dtype=np.float32),\n            ""log_std"": np.ones(shape=(cls.batch_size, cls.dim), dtype=np.float32)*np.log(1.)}\n\n    def test_kl(self):\n        # KL of same distribution should be zero\n        np.testing.assert_array_equal(\n            self.dist.kl(self.param, self.param),\n            np.zeros(shape=(1,)))\n        np.testing.assert_array_equal(\n            self.dist.kl(self.params, self.params),\n            np.zeros(shape=(self.batch_size,)))\n\n        # Add tests with not same distribution\n\n    def test_log_likelihood(self):\n        pass\n\n    def test_ent(self):\n        pass\n\n    def test_sample(self):\n        samples = self.dist.sample(self.param)\n        self.assertEqual(samples.shape, (1, self.dim))\n        samples = self.dist.sample(self.params)\n        self.assertEqual(samples.shape, (self.batch_size, self.dim))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/envs/__init__.py,0,b''
tests/envs/test_atari_wrapper.py,0,"b'import unittest\n\nimport numpy as np\nimport gym\n\nfrom tf2rl.envs.atari_wrapper import wrap_dqn\n\n\nclass TestAtariWrapper(unittest.TestCase):\n    def test_wrap_dqn(self):\n        env = wrap_dqn(gym.make(""SpaceInvadersNoFrameskip-v4""), wrap_ndarray=True)\n\n        obs = env.reset()\n        self.assertEqual(type(obs), np.ndarray)\n        self.assertEqual(obs.shape, (84, 84, 4))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/envs/test_multi_thread_env.py,4,"b'import unittest\nimport gym\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.envs.multi_thread_env import MultiThreadEnv\n\n\nclass TestMultiThreadEnv(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.batch_size = 64\n        cls.thread_pool = 4\n        cls.max_episode_steps = 1000\n        def env_fn(): return gym.make(""Pendulum-v0"")\n        cls.continuous_sample_env = env_fn()\n        cls.continuous_envs = MultiThreadEnv(\n            env_fn=env_fn,\n            batch_size=cls.batch_size,\n            max_episode_steps=cls.max_episode_steps)\n\n        def env_fn(): return gym.make(""CartPole-v0"")\n        cls.discrete_sample_env = env_fn()\n        cls.discrete_envs = MultiThreadEnv(\n            env_fn=env_fn,\n            batch_size=cls.batch_size,\n            max_episode_steps=cls.max_episode_steps)\n\n    def test_py_reset(self):\n        obses = self.continuous_envs.py_reset()\n        self.assertEqual(self.batch_size, obses.shape[0])\n        self.assertEqual(\n            self.continuous_sample_env.observation_space.low.size, obses.shape[1])\n        obses = self.discrete_envs.py_reset()\n        self.assertEqual(self.batch_size, obses.shape[0])\n        self.assertEqual(\n            self.discrete_sample_env.observation_space.low.size, obses.shape[1])\n\n    def test_step(self):\n        # Test with continuous envs\n        actions = tf.convert_to_tensor([\n            self.continuous_sample_env.action_space.sample()\n            for _ in range(self.batch_size)])\n\n        self.continuous_envs.py_reset()\n        obses, rewards, dones, _ = self.continuous_envs.step(actions)\n        self.assertEqual(self.batch_size, obses.shape[0])\n        self.assertEqual(self.batch_size, rewards.shape[0])\n        self.assertEqual(self.batch_size, dones.shape[0])\n\n        # Test with discrete envs\n        actions = tf.convert_to_tensor([\n            self.discrete_sample_env.action_space.sample()\n            for _ in range(self.batch_size)])\n        obses, rewards, dones, _ = self.discrete_envs.step(actions)\n        self.assertEqual(self.batch_size, obses.shape[0])\n        self.assertEqual(self.batch_size, rewards.shape[0])\n        self.assertEqual(self.batch_size, dones.shape[0])\n\n\nif __name__ == \'__main__\':\n    config = tf.ConfigProto(allow_soft_placement=True)\n    tf.enable_eager_execution(config=config)\n    unittest.main()\n'"
tests/envs/test_utils.py,0,"b'import unittest\nimport gym\n\nfrom tf2rl.envs.utils import is_discrete\n\n\nclass TestUtils(unittest.TestCase):\n    def test_is_discrete(self):\n        discrete_space = gym.make(\'CartPole-v0\').action_space\n        continuous_space = gym.make(\'Pendulum-v0\').action_space\n        self.assertTrue(is_discrete(discrete_space))\n        self.assertFalse(is_discrete(continuous_space))\n\n    # def test_is_mujoco_env(self):\n    #     try:\n    #         from tf2rl.envs.utils import is_mujoco_env\n    #         self.assertTrue(is_mujoco_env(\n    #             gym.make(""HalfCheetah-v2"")))\n    #         self.assertFalse(is_mujoco_env(\n    #             gym.make(""Pendulum-v0"")))\n    #     except ModuleNotFoundError:\n    #         print(""mujoco_py not found"")\n    #\n    # def test_is_atari_env(self):\n    #     self.assertTrue(is_atari_env(\n    #         gym.make(""SpaceInvadersNoFrameskip-v4"")))\n    #     self.assertFalse(is_atari_env(\n    #         gym.make(""Pendulum-v0"")))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/experiments/__init__.py,0,b''
tests/experiments/test_utils.py,0,"b'import unittest\n\nimport os\nimport numpy as np\nimport gym\n\nfrom tf2rl.misc.get_replay_buffer import get_replay_buffer\nfrom tf2rl.experiments.utils import save_path, restore_latest_n_traj\nfrom tf2rl.algos.dqn import DQN\n\n\nclass TestUtils(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.env = gym.make(""CartPole-v0"")\n        policy = DQN(\n            state_shape=cls.env.observation_space.shape,\n            action_dim=cls.env.action_space.n,\n            memory_capacity=2**4)\n        cls.replay_buffer = get_replay_buffer(\n            policy, cls.env)\n        cls.output_dir = os.path.join(\n            os.path.dirname(__file__),\n            ""tests"")\n        if not os.path.isdir(cls.output_dir):\n            os.makedirs(cls.output_dir)\n\n    def test_save_path(self):\n        n_store_episodes = 10\n        obs = np.ones(shape=self.env.observation_space.shape, dtype=np.float32)\n        for epi in range(n_store_episodes):\n            for i in range(self.replay_buffer.get_buffer_size()):\n                self.replay_buffer.add(\n                    obs=obs*i, act=i, rew=0., next_obs=obs*(i+1), done=False)\n            save_path(\n                self.replay_buffer.sample(\n                    self.replay_buffer.get_buffer_size()),\n                os.path.join(self.output_dir,\n                             ""step_0_epi_{}_return_0.0.pkl"").format(epi))\n        data = restore_latest_n_traj(self.output_dir)\n        self.assertEqual(data[""obses""].shape[0],\n                         (self.replay_buffer.get_buffer_size() - 1) * n_store_episodes)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/misc/__init__.py,0,b''
tests/misc/test_get_replay_buffer.py,0,"b'import unittest\n\nimport os\nimport numpy as np\nimport gym\n\nfrom cpprb import ReplayBuffer\nfrom cpprb import PrioritizedReplayBuffer\n\nfrom tf2rl.misc.get_replay_buffer import get_replay_buffer\nfrom tf2rl.algos.policy_base import OnPolicyAgent, OffPolicyAgent\n\n\nclass TestGetReplayBuffer(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.batch_size = 32\n        cls.memory_capacity = 32\n        cls.on_policy_agent = OnPolicyAgent(\n            name=""OnPolicyAgent"",\n            batch_size=cls.batch_size)\n        cls.off_policy_agent = OffPolicyAgent(\n            name=""OffPolicyAgent"",\n            memory_capacity=cls.memory_capacity)\n        cls.discrete_env = gym.make(""CartPole-v0"")\n        cls.continuous_env = gym.make(""Pendulum-v0"")\n\n    def test_get_replay_buffer(self):\n        # Replay Buffer\n        rb = get_replay_buffer(\n            self.on_policy_agent, self.discrete_env)\n        self.assertTrue(isinstance(rb, ReplayBuffer))\n\n        rb = get_replay_buffer(\n            self.off_policy_agent, self.discrete_env)\n        self.assertTrue(isinstance(rb, ReplayBuffer))\n\n        # Prioritized Replay Buffer\n        rb = get_replay_buffer(\n            self.off_policy_agent, self.discrete_env,\n            use_prioritized_rb=True)\n        self.assertTrue(isinstance(rb, PrioritizedReplayBuffer))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/misc/test_huber_loss.py,0,"b'import unittest\nimport numpy as np\n\nfrom tf2rl.misc.huber_loss import huber_loss\n\n\nclass TestHuberLoss(unittest.TestCase):\n    def test_huber_loss(self):\n        """"""Test of huber loss\n        huber_loss() allows two types of inputs:\n        - `y_target` and `y_pred`\n        - `diff`\n        """"""\n        # [1, 1] -> [0.5, 0.5]\n        loss = huber_loss(np.array([1., 1.]), delta=1.)\n        np.testing.assert_array_equal(\n            np.array([0.5, 0.5]),\n            loss.numpy())\n\n        # [0,0] and [10, 10] -> [9.5, 9.5]\n        loss = huber_loss(np.array([10., 10.]), delta=1.)\n        np.testing.assert_array_equal(\n            np.array([9.5, 9.5]),\n            loss.numpy())\n\n        # [0,0] and [-1, -2] -> [0.5, 1.5]\n        loss = huber_loss(np.array([-1., -2.]), delta=1.)\n        np.testing.assert_array_equal(\n            np.array([0.5, 1.5]),\n            loss.numpy())\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/misc/test_utils.py,0,"b""import unittest\nimport numpy as np\n\nfrom tf2rl.misc.discount_cumsum import discount_cumsum\n\n\nclass TestUtils(unittest.TestCase):\n    def test_discount_cumsum(self):\n        x = np.array([1., 1., 1.])\n        discount = 0.99\n        expected = [\n            1. + 1.*discount**1 + 1.*discount**2,\n            1. + 1.*discount**1,\n            1.]\n        results = discount_cumsum(x, discount)\n        np.testing.assert_array_equal(results, expected)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/networks/test_atari_model.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom tf2rl.networks.atari_model import AtariQFunc, AtariCategoricalActor\n\n\nclass TestNoisyDense(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls) -> None:\n        cls.inputs = np.zeros(shape=(32, 84, 84, 4), dtype=np.uint8)\n        cls.output_dim = 18\n\n    def test_atariqfunc(self):\n        qfunc = AtariQFunc(state_shape=self.inputs.shape[1:],\n                           action_dim=self.output_dim)\n        qfunc(self.inputs)\n\n    def test_ataricatgoricalactor(self):\n        actor = AtariCategoricalActor(\n            state_shape=self.inputs.shape[1:],\n            action_dim=self.output_dim)\n        actor(self.inputs)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/networks/test_noisy_dense.py,0,"b""import unittest\n\nfrom tf2rl.networks.noisy_dense import NoisyDense\nfrom tests.networks.utils import layer_test\n\n\nclass TestNoisyDense(unittest.TestCase):\n    def test_sn_dense(self):\n        layer_test(\n            NoisyDense, kwargs={'units': 3}, input_shape=(3, 2),\n            custom_objects={'NoisyDense': NoisyDense})\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/networks/test_spectral_norm_dense.py,0,"b""import unittest\n\nfrom tf2rl.networks.spectral_norm_dense import SNDense\nfrom tests.networks.utils import layer_test\n\n\nclass TestSNDense(unittest.TestCase):\n    def test_sn_dense(self):\n        layer_test(\n            SNDense, kwargs={'units': 3}, input_shape=(3, 2),\n            custom_objects={'SNDense': SNDense})\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/networks/utils.py,0,"b'""""""\nCopyright 2019 The TensorFlow Authors.  All rights reserved.\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      ""License"" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      ""Licensor"" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      ""Legal Entity"" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      ""control"" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      ""You"" (or ""Your"") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      ""Source"" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      ""Object"" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      ""Work"" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      ""Derivative Works"" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      ""Contribution"" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, ""submitted""\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as ""Not a Contribution.""\n\n      ""Contributor"" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a ""NOTICE"" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an ""AS IS"" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets ""[]""\n      replaced with your own identifying information. (Don\'t include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same ""printed page"" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2017, The TensorFlow Authors.\n\n   Licensed under the Apache License, Version 2.0 (the ""License"");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an ""AS IS"" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n""""""\nimport numpy as np\n\nfrom tensorflow.python import keras\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.training.rmsprop import RMSPropOptimizer\nfrom tensorflow.python.util import tf_inspect\n\n\ndef layer_test(layer_cls, kwargs=None, input_shape=None, input_dtype=None,\n               input_data=None, expected_output=None,\n               expected_output_dtype=None, custom_objects=None):\n    """"""Test routine for a layer with a single input and single output.\n\n    Arguments:\n      layer_cls: Layer class object.\n      kwargs: Optional dictionary of keyword arguments for instantiating the\n        layer.\n      input_shape: Input shape tuple.\n      input_dtype: Data type of the input data.\n      input_data: Numpy array of input data.\n      expected_output: Shape tuple for the expected shape of the output.\n      expected_output_dtype: Data type expected for the output.\n\n    Returns:\n      The output data (Numpy array) returned by the layer, for additional\n      checks to be done by the calling code.\n    """"""\n    if input_data is None:\n        assert input_shape\n        if not input_dtype:\n            input_dtype = \'float32\'\n        input_data_shape = list(input_shape)\n        for i, e in enumerate(input_data_shape):\n            if e is None:\n                input_data_shape[i] = np.random.randint(1, 4)\n        input_data = 10 * np.random.random(input_data_shape)\n        if input_dtype[:5] == \'float\':\n            input_data -= 0.5\n        input_data = input_data.astype(input_dtype)\n    elif input_shape is None:\n        input_shape = input_data.shape\n    if input_dtype is None:\n        input_dtype = input_data.dtype\n    if expected_output_dtype is None:\n        expected_output_dtype = input_dtype\n\n    # instantiation\n    kwargs = kwargs or {}\n    layer = layer_cls(**kwargs)\n\n    # test get_weights , set_weights at layer level\n    weights = layer.get_weights()\n    layer.set_weights(weights)\n\n    # test and instantiation from weights\n    if \'weights\' in tf_inspect.getargspec(layer_cls.__init__):\n        kwargs[\'weights\'] = weights\n        layer = layer_cls(**kwargs)\n\n    # test in functional API\n    x = keras.layers.Input(shape=input_shape[1:], dtype=input_dtype)\n    y = layer(x)\n    if keras.backend.dtype(y) != expected_output_dtype:\n        raise AssertionError(\'When testing layer %s, for input %s, found output \'\n                             \'dtype=%s but expected to find %s.\\nFull kwargs: %s\' %\n                             (layer_cls.__name__,\n                              x,\n                              keras.backend.dtype(y),\n                              expected_output_dtype,\n                              kwargs))\n    # check shape inference\n    model = keras.models.Model(x, y)\n    expected_output_shape = tuple(\n        layer.compute_output_shape(\n            tensor_shape.TensorShape(input_shape)).as_list())\n    actual_output = model.predict(input_data)\n    actual_output_shape = actual_output.shape\n    for expected_dim, actual_dim in zip(expected_output_shape,\n                                        actual_output_shape):\n        if expected_dim is not None:\n            if expected_dim != actual_dim:\n                raise AssertionError(\n                    \'When testing layer %s, for input %s, found output_shape=\'\n                    \'%s but expected to find %s.\\nFull kwargs: %s\' %\n                    (layer_cls.__name__,\n                     x,\n                     actual_output_shape,\n                     expected_output_shape,\n                     kwargs))\n    if expected_output is not None:\n        np.testing.assert_allclose(actual_output, expected_output, rtol=1e-3)\n\n    return None\n\n    # test serialization, weight setting at model level\n    model_config = model.get_config()\n    recovered_model = keras.models.Model.from_config(\n        model_config, custom_objects=custom_objects)\n    if model.weights:\n        weights = model.get_weights()\n        recovered_model.set_weights(weights)\n        output = recovered_model.predict(input_data)\n        np.testing.assert_allclose(output, actual_output, rtol=1e-3)\n\n    # test training mode (e.g. useful for dropout tests)\n    model.compile(RMSPropOptimizer(0.01), \'mse\')\n    model.train_on_batch(input_data, actual_output)\n\n    # test as first layer in Sequential API\n    layer_config = layer.get_config()\n    layer_config[\'batch_input_shape\'] = input_shape\n    layer = layer.__class__.from_config(layer_config)\n\n    model = keras.models.Sequential()\n    model.add(layer)\n    actual_output = model.predict(input_data)\n    actual_output_shape = actual_output.shape\n    for expected_dim, actual_dim in zip(expected_output_shape,\n                                        actual_output_shape):\n        if expected_dim is not None:\n            if expected_dim != actual_dim:\n                raise AssertionError(\n                    \'When testing layer %s, for input %s, found output_shape=\'\n                    \'%s but expected to find %s.\\nFull kwargs: %s\' %\n                    (layer_cls.__name__,\n                     x,\n                     actual_output_shape,\n                     expected_output_shape,\n                     kwargs))\n    if expected_output is not None:\n        np.testing.assert_allclose(actual_output, expected_output, rtol=1e-3)\n\n    # test serialization, weight setting at model level\n    model_config = model.get_config()\n    recovered_model = keras.models.Sequential.from_config(\n        model_config, custom_objects=custom_objects)\n    if model.weights:\n        weights = model.get_weights()\n        recovered_model.set_weights(weights)\n        output = recovered_model.predict(input_data)\n        np.testing.assert_allclose(output, actual_output, rtol=1e-3)\n\n    # for further checks in the caller function\n    return actual_output\n'"
tests/policies/__init__.py,0,b''
tests/policies/common.py,0,"b'import unittest\n\nfrom tests.algos.common import CommonAlgos\n\n\nclass CommonModel(CommonAlgos):\n    def _test_call(\n            self, inputs, expected_action_shapes,\n            expected_log_prob_shapes, policy=None):\n        """"""Check shape of ouputs\n        """"""\n        policy = policy if policy is not None else self.policy\n        # Probabilistic sampling\n        actions, log_probs, param = policy(inputs, test=False)\n        self.assertEqual(actions.shape, expected_action_shapes)\n        self.assertEqual(log_probs.shape, expected_log_prob_shapes)\n        # Greedy sampling\n        actions, log_probs, param = policy(inputs, test=True)\n        self.assertEqual(actions.shape, expected_action_shapes)\n        self.assertEqual(log_probs.shape, expected_log_prob_shapes)\n\n    def _test_compute_log_probs(\n            self, states, actions, expected_shapes, policy=None):\n        policy = policy if policy is not None else self.policy\n        log_probs = policy.compute_log_probs(states, actions)\n        self.assertEqual(log_probs.shape, expected_shapes)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/policies/test_categorical_actor.py,0,"b""import unittest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.policies.categorical_actor import CategoricalActor\nfrom tests.policies.common import CommonModel\n\n\nclass TestCategoricalActor(CommonModel):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.policy = CategoricalActor(\n            state_shape=cls.discrete_env.observation_space.shape,\n            action_dim=cls.discrete_env.action_space.n,\n            units=[4, 4])\n\n    def test_call(self):\n        # Single input\n        state = np.random.rand(\n            1, self.discrete_env.observation_space.low.size)\n        self._test_call(\n            inputs=state,\n            expected_action_shapes=(1,),\n            expected_log_prob_shapes=(1,))\n        # Multiple inputs\n        states = np.random.rand(\n            self.batch_size, self.discrete_env.observation_space.low.size)\n        self._test_call(\n            inputs=states,\n            expected_action_shapes=(self.batch_size,),\n            expected_log_prob_shapes=(self.batch_size,))\n\n    def test_compute_log_probs(self):\n        # Single input\n        state = np.random.rand(\n            1, self.discrete_env.observation_space.low.size)\n        action = np.random.randint(\n            self.discrete_env.action_space.n, size=1)\n        self._test_compute_log_probs(\n            states=state,\n            actions=action,\n            expected_shapes=(1,))\n        # Multiple inputs\n        states = np.random.rand(\n            self.batch_size, self.discrete_env.observation_space.low.size)\n        actions = np.random.randint(\n            self.discrete_env.action_space.n, size=self.batch_size)\n        self._test_compute_log_probs(\n            states=states,\n            actions=actions,\n            expected_shapes=(self.batch_size,))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/policies/test_gaussian_actor.py,0,"b'import unittest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.policies.gaussian_actor import GaussianActor\nfrom tests.policies.common import CommonModel\n\n\nclass TestGaussianActor(CommonModel):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.policy = GaussianActor(\n            state_shape=cls.continuous_env.observation_space.shape,\n            action_dim=cls.continuous_env.action_space.low.size,\n            max_action=1.,\n            units=[4, 4])\n        cls.const_std = 0.1\n        cls.policy_fixed_sigma = GaussianActor(\n            state_shape=cls.continuous_env.observation_space.shape,\n            action_dim=cls.continuous_env.action_space.low.size,\n            max_action=1.,\n            units=[4, 4],\n            fix_std=True,\n            const_std=cls.const_std)\n\n    def test_call(self):\n        """"""Not fix sigma""""""\n        # Single input\n        state = np.random.rand(\n            1, self.continuous_env.observation_space.low.size)\n        self._test_call(\n            state,\n            (1, self.continuous_env.action_space.low.size),\n            (1,))\n        # Multiple inputs\n        states = np.random.rand(\n            self.batch_size, self.continuous_env.observation_space.low.size)\n        self._test_call(\n            states,\n            (self.batch_size, self.continuous_env.action_space.low.size),\n            (self.batch_size,))\n\n        """"""Fix sigma""""""\n        state = np.random.rand(\n            1, self.continuous_env.observation_space.low.size)\n        self._test_call(\n            state,\n            (1, self.continuous_env.action_space.low.size),\n            (1,),\n            policy=self.policy_fixed_sigma)\n        # Multiple inputs\n        states = np.random.rand(\n            self.batch_size, self.continuous_env.observation_space.low.size)\n        self._test_call(\n            states,\n            (self.batch_size, self.continuous_env.action_space.low.size),\n            (self.batch_size,),\n            policy=self.policy_fixed_sigma)\n\n    def test_compute_log_probs(self):\n        """"""Not fix sigma""""""\n        # Single input\n        state = np.random.rand(\n            1, self.continuous_env.observation_space.low.size).astype(np.float32)\n        action = np.random.rand(\n            1, self.continuous_env.action_space.low.size).astype(np.float32)\n        self._test_compute_log_probs(\n            state, action, (1,))\n        # Multiple inputs\n        states = np.random.rand(\n            self.batch_size,\n            self.continuous_env.observation_space.low.size).astype(np.float32)\n        actions = np.random.rand(\n            self.batch_size,\n            self.continuous_env.action_space.low.size).astype(np.float32)\n        self._test_compute_log_probs(\n            states, actions, (self.batch_size,))\n\n        """"""Fix sigma""""""\n        # Single input\n        state = np.random.rand(\n            1, self.continuous_env.observation_space.low.size).astype(np.float32)\n        action = np.random.rand(\n            1, self.continuous_env.action_space.low.size).astype(np.float32)\n        self._test_compute_log_probs(\n            state, action, (1,),\n            policy=self.policy_fixed_sigma)\n        # Multiple inputs\n        states = np.random.rand(\n            self.batch_size,\n            self.continuous_env.observation_space.low.size).astype(np.float32)\n        actions = np.random.rand(\n            self.batch_size,\n            self.continuous_env.action_space.low.size).astype(np.float32)\n        self._test_compute_log_probs(\n            states, actions, (self.batch_size,),\n            policy=self.policy_fixed_sigma)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tf2rl/algos/__init__.py,0,b''
tf2rl/algos/apex.py,11,"b'import time\nimport numpy as np\nimport argparse\nimport logging\nimport multiprocessing\nfrom multiprocessing import Process, Queue, Value, Event, Lock\nfrom multiprocessing.managers import SyncManager\n\nfrom cpprb import ReplayBuffer, PrioritizedReplayBuffer\n\nfrom tf2rl.envs.multi_thread_env import MultiThreadEnv\nfrom tf2rl.misc.prepare_output_dir import prepare_output_dir\nfrom tf2rl.misc.get_replay_buffer import get_default_rb_dict\nfrom tf2rl.misc.initialize_logger import initialize_logger\n\n\ndef import_tf():\n    import tensorflow as tf\n    if tf.config.experimental.list_physical_devices(\'GPU\'):\n        for cur_device in tf.config.experimental.list_physical_devices(""GPU""):\n            print(cur_device)\n            tf.config.experimental.set_memory_growth(cur_device, enable=True)\n    return tf\n\n\ndef explorer(global_rb, queue, trained_steps, is_training_done,\n             lock, env_fn, policy_fn, set_weights_fn, noise_level,\n             n_env=64, n_thread=4, buffer_size=1024, episode_max_steps=1000, gpu=0):\n    """"""\n    Collect transitions and store them to prioritized replay buffer.\n\n    :param global_rb (multiprocessing.managers.AutoProxy[PrioritizedReplayBuffer]):\n        Prioritized replay buffer sharing with multiple explorers and only one learner.\n        This object is shared over processes, so it must be locked when trying to\n        operate something with `lock` object.\n    :param queue (multiprocessing.Queue):\n        A FIFO shared with the `learner` and `evaluator` to get the latest network weights.\n        This is process safe, so you don\'t need to lock process when use this.\n    :param trained_steps (multiprocessing.Value):\n        Number of steps to apply gradients.\n    :param is_training_done (multiprocessing.Event):\n        multiprocessing.Event object to share the status of training.\n    :param lock (multiprocessing.Lock):\n        multiprocessing.Lock to lock other processes.\n    :param env_fn (function):\n        Method object to generate an environment.\n    :param policy_fn (function):\n        Method object to generate an explorer.\n    :param set_weights_fn (function):\n        Method object to set network weights gotten from queue.\n    :param noise_level (float):\n        Noise level for exploration. For epsilon-greedy policy like DQN variants,\n        this will be epsilon, and if DDPG variants this will be variance for Normal distribution.\n    :param n_env (int):\n        Number of environments to distribute. If this is set to be more than 1,\n        `MultiThreadEnv` will be used.\n    :param n_thread (int):\n        Number of thread used in `MultiThreadEnv`.\n    :param buffer_size (int):\n        Size of local buffer. If this is filled with transitions, add them to `global_rb`\n    :param episode_max_steps (int):\n        Maximum number of steps of an episode.\n    :param gpu (int):\n        GPU id. If this is set to -1, then this process uses only CPU.\n    """"""\n    import_tf()\n    logger = logging.getLogger(""tf2rl"")\n\n    if n_env > 1:\n        envs = MultiThreadEnv(\n            env_fn=env_fn, batch_size=n_env, thread_pool=n_thread,\n            max_episode_steps=episode_max_steps)\n        env = envs._sample_env\n    else:\n        env = env_fn()\n\n    policy = policy_fn(\n        env=env, name=""Explorer"",\n        memory_capacity=global_rb.get_buffer_size(),\n        noise_level=noise_level, gpu=gpu)\n\n    kwargs = get_default_rb_dict(buffer_size, env)\n    if n_env > 1:\n        kwargs[""env_dict""][""priorities""] = {}\n    local_rb = ReplayBuffer(**kwargs)\n    local_idx = np.arange(buffer_size)\n\n    if n_env == 1:\n        s = env.reset()\n        episode_steps = 0\n        total_reward = 0.\n        total_rewards = []\n\n    start = time.time()\n    n_sample, n_sample_old = 0, 0\n\n    while not is_training_done.is_set():\n        if n_env == 1:\n            n_sample += 1\n            episode_steps += 1\n            a = policy.get_action(s)\n            s_, r, done, _ = env.step(a)\n            done_flag = done\n            if episode_steps == env._max_episode_steps:\n                done_flag = False\n            total_reward += r\n            local_rb.add(obs=s, act=a, rew=r, next_obs=s_, done=done_flag)\n\n            s = s_\n            if done or episode_steps == episode_max_steps:\n                s = env.reset()\n                total_rewards.append(total_reward)\n                total_reward = 0\n                episode_steps = 0\n        else:\n            n_sample += n_env\n            obses = envs.py_observation()\n            actions = policy.get_action(obses, tensor=True)\n            next_obses, rewards, dones, _ = envs.step(actions)\n            td_errors = policy.compute_td_error(\n                states=obses, actions=actions, next_states=next_obses,\n                rewards=rewards, dones=dones)\n            local_rb.add(obs=obses, act=actions, next_obs=next_obses,\n                         rew=rewards, done=dones,\n                         priorities=np.abs(td_errors+1e-6))\n\n        # Periodically copy weights of explorer\n        if not queue.empty():\n            set_weights_fn(policy, queue.get())\n\n        # Add collected experiences to global replay buffer\n        if local_rb.get_stored_size() == buffer_size:\n            samples = local_rb._encode_sample(local_idx)\n            if n_env > 1:\n                priorities = np.squeeze(samples[""priorities""])\n            else:\n                td_errors = policy.compute_td_error(\n                    states=samples[""obs""], actions=samples[""act""],\n                    next_states=samples[""next_obs""], rewards=samples[""rew""],\n                    dones=samples[""done""])\n                priorities = np.abs(np.squeeze(td_errors)) + 1e-6\n            lock.acquire()\n            global_rb.add(\n                obs=samples[""obs""], act=samples[""act""], rew=samples[""rew""],\n                next_obs=samples[""next_obs""], done=samples[""done""],\n                priorities=priorities)\n            lock.release()\n            local_rb.clear()\n\n            msg = ""Grad: {0: 6d}\\t"".format(trained_steps.value)\n            msg += ""Samples: {0: 7d}\\t"".format(n_sample)\n            msg += ""TDErr: {0:.5f}\\t"".format(np.average(priorities))\n            if n_env == 1:\n                ave_rew = 0 if len(total_rewards) == 0 else \\\n                    sum(total_rewards) / len(total_rewards)\n                msg += ""AveEpiRew: {0:.3f}\\t"".format(ave_rew)\n                total_rewards = []\n            msg += ""FPS: {0:.2f}"".format(\n                (n_sample - n_sample_old) / (time.time() - start))\n            logger.info(msg)\n\n            start = time.time()\n            n_sample_old = n_sample\n\n\ndef learner(global_rb, trained_steps, is_training_done,\n            lock, env, policy_fn, get_weights_fn,\n            n_training, update_freq, evaluation_freq, gpu, queues):\n    """"""\n    Update network weights using samples collected by explorers.\n\n    :param global_rb (multiprocessing.managers.AutoProxy[PrioritizedReplayBuffer]):\n        Prioritized replay buffer sharing with multiple explorers and only one learner.\n        This object is shared over processes, so it must be locked when trying to\n        operate something with `lock` object.\n    :param trained_steps (multiprocessing.Value):\n        Number of steps to apply gradients.\n    :param is_training_done (multiprocessing.Event):\n        multiprocessing.Event object to share the status of training.\n    :param lock (multiprocessing.Lock):\n        multiprocessing.Lock to lock other processes.\n    :param env (Gym environment):\n        Environment object.\n    :param policy_fn (function):\n        Method object to generate an explorer.\n    :param get_weights_fn (function):\n        Method object to get network weights and put them to queue.\n    :param n_training (int):\n        Maximum number of times to apply gradients. If number of applying gradients\n        is over this value, training will be done by setting `is_training_done` to `True`\n    :param update_freq (int):\n        Frequency to update parameters, i.e., put network parameters to `queues`\n    :param evaluation_freq (int):\n        Frequency to call `evaluator`.\n    :param gpu (int):\n        GPU id. If this is set to -1, then this process uses only CPU.\n    :param queues (List):\n        List of Queues shared with explorers to send latest network parameters.\n    """"""\n    tf = import_tf()\n    logger = logging.getLogger(""tf2rl"")\n\n    policy = policy_fn(env, ""Learner"", global_rb.get_buffer_size(), gpu=gpu)\n\n    output_dir = prepare_output_dir(\n        args=None, user_specified_dir=""./results"", suffix=""learner"")\n    writer = tf.summary.create_file_writer(output_dir)\n    writer.set_as_default()\n\n    # Wait until explorers collect transitions\n    while not is_training_done.is_set() and global_rb.get_stored_size() < policy.n_warmup:\n        continue\n\n    start_time = time.time()\n    while not is_training_done.is_set():\n        trained_steps.value += 1\n        tf.summary.experimental.set_step(trained_steps.value)\n        lock.acquire()\n        samples = global_rb.sample(policy.batch_size)\n        lock.release()\n        td_errors = policy.train(\n            samples[""obs""], samples[""act""], samples[""next_obs""],\n            samples[""rew""], samples[""done""], samples[""weights""])\n        writer.flush()\n        lock.acquire()\n        global_rb.update_priorities(\n            samples[""indexes""], np.abs(td_errors)+1e-6)\n        lock.release()\n\n        # Put updated weights to queue\n        if trained_steps.value % update_freq == 0:\n            weights = get_weights_fn(policy)\n            for i in range(len(queues) - 1):\n                queues[i].put(weights)\n            fps = update_freq / (time.time() - start_time)\n            tf.summary.scalar(name=""apex/fps"", data=fps)\n            logger.info(""Update weights. {0:.2f} FPS for GRAD. Learned {1:.2f} steps"".format(\n                fps, trained_steps.value))\n            start_time = time.time()\n\n        # Periodically do evaluation\n        if trained_steps.value % evaluation_freq == 0:\n            queues[-1].put(get_weights_fn(policy))\n            queues[-1].put(trained_steps.value)\n\n        if trained_steps.value >= n_training:\n            is_training_done.set()\n\n\ndef evaluator(is_training_done, env, policy_fn, set_weights_fn, queue, gpu,\n              save_model_interval=int(1e6), n_evaluation=10, episode_max_steps=1000,\n              show_test_progress=False):\n    """"""\n    Evaluate trained network weights periodically.\n\n    :param is_training_done (multiprocessing.Event):\n        multiprocessing.Event object to share the status of training.\n    :param env (Gym environment):\n        Environment object.\n    :param policy_fn (function):\n        Method object to generate an explorer.\n    :param set_weights_fn (function):\n        Method object to set network weights gotten from queue.\n    :param queue (multiprocessing.Queue):\n        A FIFO shared with the learner to get the latest network weights.\n        This is process safe, so you don\'t need to lock process when use this.\n    :param gpu (int):\n        GPU id. If this is set to -1, then this process uses only CPU.\n    :param save_model_interval (int):\n        Interval to save model.\n    :param n_evaluation (int):\n        Number of episodes to evaluate.\n    :param episode_max_steps (int):\n        Maximum number of steps of an episode.\n    :param show_test_progress (bool):\n        If true, `render` will be called to visualize evaluation process.\n    """"""\n    tf = import_tf()\n    logger = logging.getLogger(""tf2rl"")\n\n    output_dir = prepare_output_dir(\n        args=None, user_specified_dir=""./results"", suffix=""evaluator"")\n    writer = tf.summary.create_file_writer(\n        output_dir, filename_suffix=""_evaluation"")\n    writer.set_as_default()\n\n    policy = policy_fn(env, ""Learner"", gpu=gpu)\n    model_save_threshold = save_model_interval\n\n    checkpoint = tf.train.Checkpoint(policy=policy)\n    checkpoint_manager = tf.train.CheckpointManager(\n        checkpoint, directory=output_dir, max_to_keep=10)\n\n    while not is_training_done.is_set():\n        n_evaluated_episode = 0\n        # Wait until a new weights comes\n        if queue.empty():\n            continue\n        else:\n            set_weights_fn(policy, queue.get())\n            trained_steps = queue.get()\n            tf.summary.experimental.set_step(trained_steps)\n            avg_test_return = 0.\n            for _ in range(n_evaluation):\n                n_evaluated_episode += 1\n                episode_return = 0.\n                obs = env.reset()\n                done = False\n                for _ in range(episode_max_steps):\n                    action = policy.get_action(obs, test=True)\n                    next_obs, reward, done, _ = env.step(action)\n                    if show_test_progress:\n                        env.render()\n                    episode_return += reward\n                    obs = next_obs\n                    if done:\n                        break\n                avg_test_return += episode_return\n                # Break if a new weights comes\n                if not queue.empty():\n                    break\n            avg_test_return /= n_evaluated_episode\n            logger.info(""Evaluation: {} over {} run"".format(\n                avg_test_return, n_evaluated_episode))\n            tf.summary.scalar(\n                name=""apex/average_test_return"", data=avg_test_return)\n            writer.flush()\n            if trained_steps > model_save_threshold:\n                model_save_threshold += save_model_interval\n                checkpoint_manager.save()\n    checkpoint_manager.save()\n\n\ndef apex_argument(parser=None):\n    if parser is None:\n        parser = argparse.ArgumentParser(conflict_handler=\'resolve\')\n    parser.add_argument(\'--n-training\', type=int, default=1e7,\n                        help=\'number of times to apply batch update\')\n    parser.add_argument(\'--episode-max-steps\', type=int, default=int(1e3),\n                        help=\'Maximum steps in an episode\')\n    parser.add_argument(\'--param-update-freq\', type=int, default=1e2,\n                        help=\'frequency to update parameter\')\n    parser.add_argument(\'--n-explorer\', type=int, default=None,\n                        help=\'number of explorers to distribute. if None, use maximum number\')\n    parser.add_argument(\'--replay-buffer-size\', type=int, default=1e6,\n                        help=\'size of replay buffer\')\n    parser.add_argument(\'--local-buffer-size\', type=int, default=1e4,\n                        help=\'size of local replay buffer for explorer\')\n    parser.add_argument(\'--gpu-explorer\', type=int, default=0)\n    parser.add_argument(\'--gpu-learner\', type=int, default=0)\n    parser.add_argument(\'--gpu-evaluator\', type=int, default=0)\n    # Test setting\n    parser.add_argument(\'--test-freq\', type=int, default=1e3,\n                        help=\'Frequency to evaluate policy\')\n    parser.add_argument(\'--save-model-interval\', type=int, default=int(1e4),\n                        help=\'Interval to save model\')\n    # Multi Env setting\n    parser.add_argument(\'--n-env\', type=int, default=1,\n                        help=\'Number of environments\')\n    parser.add_argument(\'--n-thread\', type=int, default=4,\n                        help=\'Number of thread pool\')\n    # Others\n    parser.add_argument(\'--logging-level\', choices=[\'DEBUG\', \'INFO\', \'WARNING\'],\n                        default=\'INFO\', help=\'Logging level\')\n    return parser\n\n\ndef prepare_experiment(env, args):\n    # Manager to share PER between a learner and explorers\n    SyncManager.register(\'PrioritizedReplayBuffer\',\n                         PrioritizedReplayBuffer)\n    manager = SyncManager()\n    manager.start()\n\n    kwargs = get_default_rb_dict(args.replay_buffer_size, env)\n    kwargs[""check_for_update""] = True\n    global_rb = manager.PrioritizedReplayBuffer(**kwargs)\n\n    # queues to share network parameters between a learner and explorers\n    n_queue = 1 if args.n_env > 1 else args.n_explorer\n    n_queue += 1  # for evaluation\n    queues = [manager.Queue() for _ in range(n_queue)]\n\n    # Event object to share training status. if event is set True, all exolorers stop sampling transitions\n    is_training_done = Event()\n\n    # Lock\n    lock = manager.Lock()\n\n    # Shared memory objects to count number of samples and applied gradients\n    trained_steps = Value(\'i\', 0)\n\n    return global_rb, queues, is_training_done, lock, trained_steps\n\n\ndef run(args, env_fn, policy_fn, get_weights_fn, set_weights_fn):\n    initialize_logger(\n        logging_level=logging.getLevelName(args.logging_level))\n\n    if args.n_env > 1:\n        args.n_explorer = 1\n    elif args.n_explorer is None:\n        args.n_explorer = multiprocessing.cpu_count() - 1\n    assert args.n_explorer > 0, ""[error] number of explorers must be positive integer""\n\n    env = env_fn()\n\n    global_rb, queues, is_training_done, lock, trained_steps = \\\n        prepare_experiment(env, args)\n\n    noise = 0.3\n    tasks = []\n\n    # Add explorers\n    if args.n_env > 1:\n        tasks.append(Process(\n            target=explorer,\n            args=[global_rb, queues[0], trained_steps, is_training_done,\n                  lock, env_fn, policy_fn, set_weights_fn, noise,\n                  args.n_env, args.n_thread, args.local_buffer_size,\n                  args.episode_max_steps, args.gpu_explorer]))\n    else:\n        for i in range(args.n_explorer):\n            tasks.append(Process(\n                target=explorer,\n                args=[global_rb, queues[i], trained_steps, is_training_done,\n                      lock, env_fn, policy_fn, set_weights_fn, noise,\n                      args.n_env, args.n_thread, args.local_buffer_size,\n                      args.episode_max_steps, args.gpu_explorer]))\n\n    # Add learner\n    tasks.append(Process(\n        target=learner,\n        args=[global_rb, trained_steps, is_training_done,\n              lock, env_fn(), policy_fn, get_weights_fn,\n              args.n_training, args.param_update_freq,\n              args.test_freq, args.gpu_learner, queues]))\n\n    # Add evaluator\n    tasks.append(Process(\n        target=evaluator,\n        args=[is_training_done, env_fn(), policy_fn, set_weights_fn,\n              queues[-1], args.gpu_evaluator, args.save_model_interval]))\n\n    for task in tasks:\n        task.start()\n    for task in tasks:\n        task.join()\n'"
tf2rl/algos/bi_res_ddpg.py,14,"b'""""""\nThis is an implementation of Bi-Res-DDPG\nhttps://arxiv.org/abs/1905.01072\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.algos.ddpg import DDPG\nfrom tf2rl.misc.target_update_ops import update_target_variables\n\n\nclass BiResDDPG(DDPG):\n    def __init__(self, eta=0.05, name=""BiResDDPG"", **kwargs):\n        kwargs[""name""] = name\n        super().__init__(**kwargs)\n        self._eta = eta\n\n    @tf.function\n    def _train_body(self, states, actions, next_states, rewards, dones, weights):\n        with tf.device(self.device):\n            with tf.GradientTape() as tape:\n                td_errors1, td_errors2 = self._compute_td_error_body(\n                    states, actions, next_states, rewards, dones)\n                critic_loss = tf.reduce_mean(\n                    tf.square(td_errors1) * weights * 0.5 +\n                    tf.square(td_errors2) * weights * self.discount * self._eta * 0.5)\n\n            critic_grad = tape.gradient(\n                critic_loss, self.critic.trainable_variables)\n            self.critic_optimizer.apply_gradients(\n                zip(critic_grad, self.critic.trainable_variables))\n\n            with tf.GradientTape() as tape:\n                next_action = self.actor(states)\n                actor_loss = -tf.reduce_mean(self.critic([states, next_action]))\n\n            actor_grad = tape.gradient(\n                actor_loss, self.actor.trainable_variables)\n            self.actor_optimizer.apply_gradients(\n                zip(actor_grad, self.actor.trainable_variables))\n\n            # Update target networks\n            update_target_variables(\n                self.critic_target.weights, self.critic.weights, self.tau)\n            update_target_variables(\n                self.actor_target.weights, self.actor.weights, self.tau)\n\n            return actor_loss, critic_loss, tf.abs(td_errors1) + tf.abs(td_errors2)\n\n    def compute_td_error(self, states, actions, next_states, rewards, dones):\n        td_error1, td_error2 = self._compute_td_error_body(\n            states, actions, next_states, rewards, dones)\n        return np.squeeze(np.abs(td_error1.numpy()) + np.abs(td_error2.numpy()))\n\n    @tf.function\n    def _compute_td_error_body(self, states, actions, next_states, rewards, dones):\n        with tf.device(self.device):\n            not_dones = 1. - dones\n            # Compute standard TD error\n            target_Q = self.critic_target(\n                [next_states, self.actor_target(next_states)])\n            target_Q = rewards + (not_dones * self.discount * target_Q)\n            target_Q = tf.stop_gradient(target_Q)\n            current_Q = self.critic([states, actions])\n            td_errors1 = target_Q - current_Q\n            # Compute residual TD error\n            next_actions = tf.stop_gradient(self.actor(next_states))\n            target_Q = self.critic([next_states, next_actions])\n            target_Q = rewards + (not_dones * self.discount * target_Q)\n            current_Q = tf.stop_gradient(self.critic_target([states, actions]))\n            td_errors2 = target_Q - current_Q\n        return td_errors1, td_errors2\n\n    @staticmethod\n    def get_argument(parser=None):\n        parser = DDPG.get_argument(parser)\n        parser.add_argument(\'--eta\', type=float, default=0.05)\n        return parser\n'"
tf2rl/algos/ddpg.py,36,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\n\nfrom tf2rl.algos.policy_base import OffPolicyAgent\nfrom tf2rl.misc.target_update_ops import update_target_variables\nfrom tf2rl.misc.huber_loss import huber_loss\n\n\nclass Actor(tf.keras.Model):\n    def __init__(self, state_shape, action_dim, max_action, units=[400, 300], name=""Actor""):\n        super().__init__(name=name)\n\n        self.l1 = Dense(units[0], name=""L1"")\n        self.l2 = Dense(units[1], name=""L2"")\n        self.l3 = Dense(action_dim, name=""L3"")\n\n        self.max_action = max_action\n\n        with tf.device(""/cpu:0""):\n            self(tf.constant(np.zeros(shape=(1,)+state_shape, dtype=np.float32)))\n\n    def call(self, inputs):\n        features = tf.nn.relu(self.l1(inputs))\n        features = tf.nn.relu(self.l2(features))\n        features = self.l3(features)\n        action = self.max_action * tf.nn.tanh(features)\n        return action\n\n\nclass Critic(tf.keras.Model):\n    def __init__(self, state_shape, action_dim, units=[400, 300], name=""Critic""):\n        super().__init__(name=name)\n\n        self.l1 = Dense(units[0], name=""L1"")\n        self.l2 = Dense(units[1], name=""L2"")\n        self.l3 = Dense(1, name=""L3"")\n\n        dummy_state = tf.constant(\n            np.zeros(shape=(1,)+state_shape, dtype=np.float32))\n        dummy_action = tf.constant(\n            np.zeros(shape=[1, action_dim], dtype=np.float32))\n        with tf.device(""/cpu:0""):\n            self([dummy_state, dummy_action])\n\n    def call(self, inputs):\n        states, actions = inputs\n        features = tf.concat([states, actions], axis=1)\n        features = tf.nn.relu(self.l1(features))\n        features = tf.nn.relu(self.l2(features))\n        features = self.l3(features)\n        return features\n\n\nclass DDPG(OffPolicyAgent):\n    def __init__(\n            self,\n            state_shape,\n            action_dim,\n            name=""DDPG"",\n            max_action=1.,\n            lr_actor=0.001,\n            lr_critic=0.001,\n            actor_units=[400, 300],\n            critic_units=[400, 300],\n            sigma=0.1,\n            tau=0.005,\n            n_warmup=int(1e4),\n            memory_capacity=int(1e6),\n            **kwargs):\n        super().__init__(name=name, memory_capacity=memory_capacity, n_warmup=n_warmup, **kwargs)\n\n        # Define and initialize Actor network\n        self.actor = Actor(state_shape, action_dim, max_action, actor_units)\n        self.actor_target = Actor(\n            state_shape, action_dim, max_action, actor_units)\n        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_actor)\n        update_target_variables(self.actor_target.weights,\n                                self.actor.weights, tau=1.)\n\n        # Define and initialize Critic network\n        self.critic = Critic(state_shape, action_dim, critic_units)\n        self.critic_target = Critic(state_shape, action_dim, critic_units)\n        self.critic_optimizer = tf.keras.optimizers.Adam(\n            learning_rate=lr_critic)\n        update_target_variables(\n            self.critic_target.weights, self.critic.weights, tau=1.)\n\n        # Set hyperparameters\n        self.sigma = sigma\n        self.tau = tau\n\n    def get_action(self, state, test=False, tensor=False):\n        is_single_state = len(state.shape) == 1\n        if not tensor:\n            assert isinstance(state, np.ndarray)\n        state = np.expand_dims(state, axis=0).astype(\n            np.float32) if is_single_state else state\n        action = self._get_action_body(\n            tf.constant(state), self.sigma * (1. - test),\n            tf.constant(self.actor.max_action, dtype=tf.float32))\n        if tensor:\n            return action\n        else:\n            return action.numpy()[0] if is_single_state else action.numpy()\n\n    @tf.function\n    def _get_action_body(self, state, sigma, max_action):\n        with tf.device(self.device):\n            action = self.actor(state)\n            action += tf.random.normal(shape=action.shape,\n                                       mean=0., stddev=sigma, dtype=tf.float32)\n            return tf.clip_by_value(action, -max_action, max_action)\n\n    def train(self, states, actions, next_states, rewards, done, weights=None):\n        if weights is None:\n            weights = np.ones_like(rewards)\n        actor_loss, critic_loss, td_errors = self._train_body(\n            states, actions, next_states, rewards, done, weights)\n\n        if actor_loss is not None:\n            tf.summary.scalar(name=self.policy_name+""/actor_loss"",\n                              data=actor_loss)\n        tf.summary.scalar(name=self.policy_name+""/critic_loss"",\n                          data=critic_loss)\n\n        return td_errors\n\n    @tf.function\n    def _train_body(self, states, actions, next_states, rewards, done, weights):\n        with tf.device(self.device):\n            with tf.GradientTape() as tape:\n                td_errors = self._compute_td_error_body(\n                    states, actions, next_states, rewards, done)\n                critic_loss = tf.reduce_mean(\n                    huber_loss(td_errors, delta=self.max_grad) * weights)\n\n            critic_grad = tape.gradient(\n                critic_loss, self.critic.trainable_variables)\n            self.critic_optimizer.apply_gradients(\n                zip(critic_grad, self.critic.trainable_variables))\n\n            with tf.GradientTape() as tape:\n                next_action = self.actor(states)\n                actor_loss = -tf.reduce_mean(self.critic([states, next_action]))\n\n            actor_grad = tape.gradient(\n                actor_loss, self.actor.trainable_variables)\n            self.actor_optimizer.apply_gradients(\n                zip(actor_grad, self.actor.trainable_variables))\n\n            # Update target networks\n            update_target_variables(\n                self.critic_target.weights, self.critic.weights, self.tau)\n            update_target_variables(\n                self.actor_target.weights, self.actor.weights, self.tau)\n\n            return actor_loss, critic_loss, td_errors\n\n    def compute_td_error(self, states, actions, next_states, rewards, dones):\n        if isinstance(actions, tf.Tensor):\n            rewards = tf.expand_dims(rewards, axis=1)\n            dones = tf.expand_dims(dones, 1)\n        td_errors = self._compute_td_error_body(\n            states, actions, next_states, rewards, dones)\n        return np.abs(np.ravel(td_errors.numpy()))\n\n    @tf.function\n    def _compute_td_error_body(self, states, actions, next_states, rewards, dones):\n        with tf.device(self.device):\n            not_dones = 1. - dones\n            target_Q = self.critic_target(\n                [next_states, self.actor_target(next_states)])\n            target_Q = rewards + (not_dones * self.discount * target_Q)\n            target_Q = tf.stop_gradient(target_Q)\n            current_Q = self.critic([states, actions])\n            td_errors = target_Q - current_Q\n        return td_errors\n'"
tf2rl/algos/dqn.py,98,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\n\nfrom tf2rl.algos.policy_base import OffPolicyAgent\nfrom tf2rl.networks.noisy_dense import NoisyDense\nfrom tf2rl.envs.atari_wrapper import LazyFrames\nfrom tf2rl.misc.target_update_ops import update_target_variables\nfrom tf2rl.misc.huber_loss import huber_loss\n\n\nclass QFunc(tf.keras.Model):\n    def __init__(self, state_shape, action_dim, units=[32, 32],\n                 name=""QFunc"", enable_dueling_dqn=False,\n                 enable_noisy_dqn=False, enable_categorical_dqn=False,\n                 n_atoms=51):\n        super().__init__(name=name)\n        self._enable_dueling_dqn = enable_dueling_dqn\n        self._enable_noisy_dqn = enable_noisy_dqn\n        self._enable_categorical_dqn = enable_categorical_dqn\n        if enable_categorical_dqn:\n            self._action_dim = action_dim\n            self._n_atoms = n_atoms\n            action_dim = (action_dim + int(enable_dueling_dqn)) * n_atoms\n\n        DenseLayer = NoisyDense if enable_noisy_dqn else Dense\n\n        self.l1 = DenseLayer(units[0], name=""L1"", activation=""relu"")\n        self.l2 = DenseLayer(units[1], name=""L2"", activation=""relu"")\n        self.l3 = DenseLayer(action_dim, name=""L3"", activation=""linear"")\n\n        if enable_dueling_dqn and not enable_categorical_dqn:\n            self.l4 = DenseLayer(1, name=""L3"", activation=""linear"")\n\n        with tf.device(""/cpu:0""):\n            self(inputs=tf.constant(np.zeros(shape=(1,)+state_shape,\n                                             dtype=np.float32)))\n\n    def call(self, inputs):\n        features = self.l1(inputs)\n        features = self.l2(features)\n        if self._enable_categorical_dqn:\n            features = self.l3(features)\n            if self._enable_dueling_dqn:\n                features = tf.reshape(\n                    features, (-1, self._action_dim+1, self._n_atoms))  # [batch_size, action_dim, n_atoms]\n                v_values = tf.reshape(\n                    features[:, 0], (-1, 1, self._n_atoms))\n                advantages = tf.reshape(\n                    features[:, 1:], [-1, self._action_dim, self._n_atoms])\n                features = v_values + (advantages - tf.expand_dims(\n                    tf.reduce_mean(advantages, axis=1), axis=1))\n            else:\n                features = tf.reshape(\n                    features, (-1, self._action_dim, self._n_atoms))  # [batch_size, action_dim, n_atoms]\n            # [batch_size, action_dim, n_atoms]\n            q_dist = tf.keras.activations.softmax(features, axis=2)\n            return tf.clip_by_value(q_dist, 1e-8, 1.0-1e-8)\n        else:\n            if self._enable_dueling_dqn:\n                advantages = self.l3(features)\n                v_values = self.l4(features)\n                q_values = v_values + \\\n                    (advantages - tf.reduce_mean(advantages, axis=1, keepdims=True))\n            else:\n                q_values = self.l3(features)\n            return q_values\n\n\nclass DQN(OffPolicyAgent):\n    def __init__(\n            self,\n            state_shape,\n            action_dim,\n            q_func=None,\n            name=""DQN"",\n            lr=0.001,\n            units=[32, 32],\n            epsilon=0.1,\n            epsilon_min=None,\n            epsilon_decay_step=int(1e6),\n            n_warmup=int(1e4),\n            target_replace_interval=int(5e3),\n            memory_capacity=int(1e6),\n            optimizer=None,\n            enable_double_dqn=False,\n            enable_dueling_dqn=False,\n            enable_noisy_dqn=False,\n            enable_categorical_dqn=False,\n            **kwargs):\n        super().__init__(name=name, memory_capacity=memory_capacity, n_warmup=n_warmup, **kwargs)\n\n        q_func = q_func if q_func is not None else QFunc\n        # Define and initialize Q-function network\n        kwargs_dqn = {\n            ""state_shape"": state_shape,\n            ""action_dim"": action_dim,\n            ""units"": units,\n            ""enable_dueling_dqn"": enable_dueling_dqn,\n            ""enable_noisy_dqn"": enable_noisy_dqn,\n            ""enable_categorical_dqn"": enable_categorical_dqn}\n        self.q_func = q_func(**kwargs_dqn)\n        self.q_func_target = q_func(**kwargs_dqn)\n        self.q_func_optimizer = optimizer if optimizer is not None else \\\n            tf.keras.optimizers.Adam(learning_rate=lr)\n        update_target_variables(self.q_func_target.weights,\n                                self.q_func.weights, tau=1.)\n\n        self._action_dim = action_dim\n        # This is used to check if input state to `get_action` is multiple (batch) or single\n        self._state_ndim = np.array(state_shape).shape[0]\n\n        # Distributional DQN\n        if enable_categorical_dqn:\n            self._v_max, self._v_min = 10., -10.\n            self._delta_z = (self._v_max - self._v_min) / \\\n                (self.q_func._n_atoms - 1)\n            self._z_list = tf.constant(\n                [self._v_min + i *\n                    self._delta_z for i in range(self.q_func._n_atoms)],\n                dtype=tf.float32)\n            self._z_list_broadcasted = tf.tile(\n                tf.reshape(self._z_list, [1, self.q_func._n_atoms]),\n                tf.constant([self._action_dim, 1]))\n\n        # Set hyper-parameters\n        if epsilon_min is not None and not enable_noisy_dqn:\n            assert epsilon > epsilon_min\n            self.epsilon_min = epsilon_min\n            self.epsilon_decay_rate = (\n                epsilon - epsilon_min) / epsilon_decay_step\n            self.epsilon = max(epsilon - self.epsilon_decay_rate * self.n_warmup,\n                               self.epsilon_min)\n        else:\n            epsilon = epsilon if not enable_noisy_dqn else 0.\n            self.epsilon = epsilon\n            self.epsilon_min = epsilon\n            self.epsilon_decay_rate = 0.\n        self.target_replace_interval = target_replace_interval\n        self.n_update = 0\n\n        # DQN variants\n        self._enable_double_dqn = enable_double_dqn\n        self._enable_noisy_dqn = enable_noisy_dqn\n        self._enable_categorical_dqn = enable_categorical_dqn\n\n    def get_action(self, state, test=False, tensor=False):\n        if isinstance(state, LazyFrames):\n            state = np.array(state)\n        if not tensor:\n            assert isinstance(state, np.ndarray)\n        is_single_input = state.ndim == self._state_ndim\n\n        if not test and np.random.rand() < self.epsilon:\n            if is_single_input:\n                action = np.random.randint(self._action_dim)\n            else:\n                action = np.array([np.random.randint(self._action_dim)\n                                   for _ in range(state.shape[0])], dtype=np.int64)\n            if tensor:\n                return tf.convert_to_tensor(action)\n            else:\n                return action\n\n        state = np.expand_dims(state, axis=0).astype(\n            np.float32) if is_single_input else state\n        if self._enable_categorical_dqn:\n            action = self._get_action_body_distributional(tf.constant(state))\n        else:\n            action = self._get_action_body(tf.constant(state))\n        if tensor:\n            return action\n        else:\n            if is_single_input:\n                return action.numpy()[0]\n            else:\n                return action.numpy()\n\n    @tf.function\n    def _get_action_body(self, state):\n        q_values = self.q_func(state)\n        return tf.argmax(q_values, axis=1)\n\n    @tf.function\n    def _get_action_body_distributional(self, state):\n        action_probs = self.q_func(state)\n        return tf.argmax(\n            tf.reduce_sum(action_probs * self._z_list_broadcasted, axis=2),\n            axis=1)\n\n    def train(self, states, actions, next_states, rewards, done, weights=None):\n        if weights is None:\n            weights = np.ones_like(rewards)\n        td_errors, q_func_loss = self._train_body(\n            states, actions, next_states, rewards, done, weights)\n\n        tf.summary.scalar(name=self.policy_name +\n                          ""/q_func_Loss"", data=q_func_loss)\n\n        # TODO: Remove following by using tf.global_step\n        self.n_update += 1\n        # Update target networks\n        if self.n_update % self.target_replace_interval == 0:\n            update_target_variables(\n                self.q_func_target.weights, self.q_func.weights, tau=1.)\n\n        # Update exploration rate\n        self.epsilon = max(self.epsilon - self.epsilon_decay_rate * self.update_interval,\n                           self.epsilon_min)\n        tf.summary.scalar(name=self.policy_name+""/epsilon"", data=self.epsilon)\n\n        return td_errors\n\n    @tf.function\n    def _train_body(self, states, actions, next_states, rewards, done, weights):\n        with tf.device(self.device):\n            with tf.GradientTape() as tape:\n                if self._enable_categorical_dqn:\n                    td_errors = self._compute_td_error_body_distributional(\n                        states, actions, next_states, rewards, done)\n                    q_func_loss = tf.reduce_mean(\n                        huber_loss(tf.negative(td_errors),\n                                   delta=self.max_grad) * weights)\n                else:\n                    td_errors = self._compute_td_error_body(\n                        states, actions, next_states, rewards, done)\n                    q_func_loss = tf.reduce_mean(\n                        huber_loss(td_errors,\n                                   delta=self.max_grad) * weights)\n\n            q_func_grad = tape.gradient(\n                q_func_loss, self.q_func.trainable_variables)\n            self.q_func_optimizer.apply_gradients(\n                zip(q_func_grad, self.q_func.trainable_variables))\n\n            return td_errors, q_func_loss\n\n    def compute_td_error(self, states, actions, next_states, rewards, dones):\n        # TODO: fix this ugly conversion\n        if isinstance(actions, tf.Tensor):\n            actions = tf.expand_dims(actions, axis=1)\n            rewards = tf.expand_dims(rewards, axis=1)\n            dones = tf.expand_dims(dones, 1)\n        if self._enable_categorical_dqn:\n            return self._compute_td_error_body_distributional(\n                states, actions, next_states, rewards, dones)\n        else:\n            return self._compute_td_error_body(\n                states, actions, next_states, rewards, dones)\n\n    @tf.function\n    def _compute_td_error_body(self, states, actions, next_states, rewards, dones):\n        # TODO: Clean code\n        batch_size = states.shape[0]\n        not_dones = 1. - tf.cast(dones, dtype=tf.float32)\n        actions = tf.cast(actions, dtype=tf.int32)\n        with tf.device(self.device):\n            indices = tf.concat(\n                values=[tf.expand_dims(tf.range(batch_size), axis=1),\n                        actions], axis=1)\n            current_Q = tf.expand_dims(\n                tf.gather_nd(self.q_func(states), indices), axis=1)\n\n            if self._enable_double_dqn:\n                max_q_indexes = tf.argmax(self.q_func(next_states),\n                                          axis=1, output_type=tf.int32)\n                # TODO: Reuse predefined `indices`\n                indices = tf.concat(\n                    values=[tf.expand_dims(tf.range(batch_size), axis=1),\n                            tf.expand_dims(max_q_indexes, axis=1)], axis=1)\n                target_Q = tf.expand_dims(\n                    tf.gather_nd(self.q_func_target(next_states), indices), axis=1)\n                target_Q = rewards + not_dones * self.discount * target_Q\n            else:\n                target_Q = rewards + not_dones * self.discount * tf.reduce_max(\n                    self.q_func_target(next_states), keepdims=True, axis=1)\n            target_Q = tf.stop_gradient(target_Q)\n            td_errors = current_Q - target_Q\n        return td_errors\n\n    @tf.function\n    def _compute_td_error_body_distributional(self, states, actions, next_states, rewards, done):\n        actions = tf.cast(actions, dtype=tf.int32)\n        with tf.device(self.device):\n            rewards = tf.tile(\n                tf.reshape(rewards, [-1, 1]),\n                tf.constant([1, self.q_func._n_atoms]))  # [batch_size, n_atoms]\n            not_done = 1.0 - tf.tile(\n                tf.reshape(done, [-1, 1]),\n                tf.constant([1, self.q_func._n_atoms]))  # [batch_size, n_atoms]\n            discounts = tf.cast(\n                tf.reshape(self.discount, [-1, 1]), tf.float32)\n            z = tf.reshape(\n                self._z_list, [1, self.q_func._n_atoms])  # [1, n_atoms]\n            z = rewards + not_done * discounts * z  # [batch_size, n_atoms]\n            # [batch_size, n_atoms]\n            z = tf.clip_by_value(z, self._v_min, self._v_max)\n            b = (z - self._v_min) / self._delta_z  # [batch_size, n_atoms]\n\n            index_help = tf.expand_dims(\n                tf.tile(\n                    tf.reshape(tf.range(self.batch_size), [-1, 1]),\n                    tf.constant([1, self.q_func._n_atoms])),\n                -1)  # [batch_size, n_atoms, 1]\n            u, l = tf.math.ceil(b), tf.math.floor(b)  # [batch_size, n_atoms]\n            u_id = tf.concat(\n                [index_help, tf.expand_dims(tf.cast(u, tf.int32), -1)],\n                axis=2)  # [batch_size, n_atoms, 2]\n            l_id = tf.concat(\n                [index_help, tf.expand_dims(tf.cast(l, tf.int32), -1)],\n                axis=2)  # [batch_size, n_atoms, 2]\n\n            target_Q_next_dist = self.q_func_target(\n                next_states)  # [batch_size, n_action, n_atoms]\n            if self._enable_double_dqn:\n                # TODO: Check this implementation is correct\n                target_Q_next_dist = tf.gather_nd(\n                    target_Q_next_dist,\n                    tf.concat(\n                        [tf.reshape(tf.range(self.batch_size), [-1, 1]),\n                         tf.reshape(actions, [-1, 1])],\n                        axis=1))\n            else:\n                target_Q_next_sum = tf.reduce_sum(\n                    target_Q_next_dist * self._z_list_broadcasted, axis=2)  # [batch_size, n_action]\n                actions_by_target_Q = tf.cast(\n                    tf.argmax(target_Q_next_sum, axis=1),\n                    tf.int32)  # [batch_size,]\n                target_Q_next_dist = tf.gather_nd(\n                    target_Q_next_dist,\n                    tf.concat(\n                        [tf.reshape(tf.range(self.batch_size), [-1, 1]),\n                         tf.reshape(actions_by_target_Q, [-1, 1])],\n                        axis=1))  # [batch_size, n_atoms]\n\n            action_indices = tf.concat(\n                values=[tf.expand_dims(tf.range(self.batch_size), axis=1),\n                        actions], axis=1)\n            current_Q_dist = tf.gather_nd(\n                self.q_func(states), action_indices)  # [batch_size, n_atoms]\n\n            td_errors = tf.reduce_sum(\n                target_Q_next_dist * (u - b) * tf.math.log(\n                    tf.gather_nd(current_Q_dist, l_id)) +\n                target_Q_next_dist * (b - l) * tf.math.log(\n                    tf.gather_nd(current_Q_dist, u_id)),\n                axis=1)\n\n        return td_errors\n\n    @staticmethod\n    def get_argument(parser=None):\n        parser = OffPolicyAgent.get_argument(parser)\n        parser.add_argument(\'--enable-double-dqn\', action=\'store_true\')\n        parser.add_argument(\'--enable-dueling-dqn\', action=\'store_true\')\n        parser.add_argument(\'--enable-categorical-dqn\', action=\'store_true\')\n        parser.add_argument(\'--enable-noisy-dqn\', action=\'store_true\')\n        return parser\n'"
tf2rl/algos/gaifo.py,17,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\n\nfrom tf2rl.algos.policy_base import IRLPolicy\nfrom tf2rl.algos.gail import GAIL, Discriminator as DiscriminatorGAIL\nfrom tf2rl.networks.spectral_norm_dense import SNDense\n\n\nclass Discriminator(DiscriminatorGAIL):\n    def __init__(self, state_shape, units=[32, 32],\n                 enable_sn=False, output_activation=""sigmoid"",\n                 name=""Discriminator""):\n        tf.keras.Model.__init__(self, name=name)\n\n        DenseClass = SNDense if enable_sn else Dense\n        self.l1 = DenseClass(units[0], name=""L1"", activation=""relu"")\n        self.l2 = DenseClass(units[1], name=""L2"", activation=""relu"")\n        self.l3 = DenseClass(1, name=""L3"", activation=output_activation)\n\n        dummy_state = tf.constant(\n            np.zeros(shape=(1,) + state_shape, dtype=np.float32))\n        dummy_next_state = tf.constant(\n            np.zeros(shape=(1,) + state_shape, dtype=np.float32))\n        with tf.device(""/cpu:0""):\n            self([dummy_state, dummy_next_state])\n\n\nclass GAIfO(GAIL):\n    def __init__(\n            self,\n            state_shape,\n            units=[32, 32],\n            lr=0.001,\n            enable_sn=False,\n            name=""GAIfO"",\n            **kwargs):\n        IRLPolicy.__init__(self, name=name, n_training=1, **kwargs)\n        self.disc = Discriminator(\n            state_shape=state_shape,\n            units=units, enable_sn=enable_sn)\n        self.optimizer = tf.keras.optimizers.Adam(\n            learning_rate=lr, beta_1=0.5)\n\n    def train(self, agent_states, agent_next_states,\n              expert_states, expert_next_states, **kwargs):\n        loss, accuracy, js_divergence = self._train_body(\n            agent_states, agent_next_states, expert_states, expert_next_states)\n        tf.summary.scalar(name=self.policy_name+""/DiscriminatorLoss"", data=loss)\n        tf.summary.scalar(name=self.policy_name+""/Accuracy"", data=accuracy)\n        tf.summary.scalar(name=self.policy_name+""/JSdivergence"", data=js_divergence)\n\n    @tf.function\n    def _train_body(self, agent_states, agent_next_states, expert_states, expert_next_states):\n        epsilon = 1e-8\n        with tf.device(self.device):\n            with tf.GradientTape() as tape:\n                real_logits = self.disc([expert_states, expert_next_states])\n                fake_logits = self.disc([agent_states, agent_next_states])\n                loss = -(tf.reduce_mean(tf.math.log(real_logits + epsilon)) +\n                         tf.reduce_mean(tf.math.log(1. - fake_logits + epsilon)))\n            grads = tape.gradient(loss, self.disc.trainable_variables)\n            self.optimizer.apply_gradients(\n                zip(grads, self.disc.trainable_variables))\n\n        accuracy = \\\n            tf.reduce_mean(tf.cast(real_logits >= 0.5, tf.float32)) / 2. + \\\n            tf.reduce_mean(tf.cast(fake_logits < 0.5, tf.float32)) / 2.\n        js_divergence = self._compute_js_divergence(\n            fake_logits, real_logits)\n        return loss, accuracy, js_divergence\n\n    def inference(self, states, actions, next_states):\n        assert states.shape == next_states.shape\n        if states.ndim == 1:\n            states = np.expand_dims(states, axis=0)\n            next_states = np.expand_dims(next_states, axis=0)\n        return self._inference_body(states, next_states)\n\n    @tf.function\n    def _inference_body(self, states, next_states):\n        with tf.device(self.device):\n            return self.disc.compute_reward([states, next_states])\n'"
tf2rl/algos/gail.py,21,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\n\nfrom tf2rl.algos.policy_base import IRLPolicy\nfrom tf2rl.networks.spectral_norm_dense import SNDense\n\n\nclass Discriminator(tf.keras.Model):\n    def __init__(self, state_shape, action_dim, units=[32, 32],\n                 enable_sn=False, output_activation=""sigmoid"",\n                 name=""Discriminator""):\n        super().__init__(name=name)\n\n        DenseClass = SNDense if enable_sn else Dense\n        self.l1 = DenseClass(units[0], name=""L1"", activation=""relu"")\n        self.l2 = DenseClass(units[1], name=""L2"", activation=""relu"")\n        self.l3 = DenseClass(1, name=""L3"", activation=output_activation)\n\n        dummy_state = tf.constant(\n            np.zeros(shape=(1,)+state_shape, dtype=np.float32))\n        dummy_action = tf.constant(\n            np.zeros(shape=[1, action_dim], dtype=np.float32))\n        with tf.device(""/cpu:0""):\n            self([dummy_state, dummy_action])\n\n    def call(self, inputs):\n        features = tf.concat(inputs, axis=1)\n        features = self.l1(features)\n        features = self.l2(features)\n        return self.l3(features)\n\n    def compute_reward(self, inputs):\n        return tf.math.log(self(inputs) + 1e-8)\n\n\nclass GAIL(IRLPolicy):\n    def __init__(\n            self,\n            state_shape,\n            action_dim,\n            units=[32, 32],\n            lr=0.001,\n            enable_sn=False,\n            name=""GAIL"",\n            **kwargs):\n        super().__init__(name=name, n_training=1, **kwargs)\n        self.disc = Discriminator(\n            state_shape=state_shape, action_dim=action_dim,\n            units=units, enable_sn=enable_sn)\n        self.optimizer = tf.keras.optimizers.Adam(\n            learning_rate=lr, beta_1=0.5)\n\n    def train(self, agent_states, agent_acts,\n              expert_states, expert_acts, **kwargs):\n        loss, accuracy, js_divergence = self._train_body(\n            agent_states, agent_acts, expert_states, expert_acts)\n        tf.summary.scalar(name=self.policy_name+""/DiscriminatorLoss"", data=loss)\n        tf.summary.scalar(name=self.policy_name+""/Accuracy"", data=accuracy)\n        tf.summary.scalar(name=self.policy_name+""/JSdivergence"", data=js_divergence)\n\n    def _compute_js_divergence(self, fake_logits, real_logits):\n        m = (fake_logits + real_logits) / 2.\n        return tf.reduce_mean((\n            fake_logits * tf.math.log(fake_logits / m + 1e-8) + real_logits * tf.math.log(real_logits / m + 1e-8)) / 2.)\n\n    @tf.function\n    def _train_body(self, agent_states, agent_acts, expert_states, expert_acts):\n        epsilon = 1e-8\n        with tf.device(self.device):\n            with tf.GradientTape() as tape:\n                real_logits = self.disc([expert_states, expert_acts])\n                fake_logits = self.disc([agent_states, agent_acts])\n                loss = -(tf.reduce_mean(tf.math.log(real_logits + epsilon)) +\n                         tf.reduce_mean(tf.math.log(1. - fake_logits + epsilon)))\n            grads = tape.gradient(loss, self.disc.trainable_variables)\n            self.optimizer.apply_gradients(\n                zip(grads, self.disc.trainable_variables))\n\n        accuracy = \\\n            tf.reduce_mean(tf.cast(real_logits >= 0.5, tf.float32)) / 2. + \\\n            tf.reduce_mean(tf.cast(fake_logits < 0.5, tf.float32)) / 2.\n        js_divergence = self._compute_js_divergence(\n            fake_logits, real_logits)\n        return loss, accuracy, js_divergence\n\n    def inference(self, states, actions, next_states):\n        if states.ndim == actions.ndim == 1:\n            states = np.expand_dims(states, axis=0)\n            actions = np.expand_dims(actions, axis=0)\n        return self._inference_body(states, actions)\n\n    @tf.function\n    def _inference_body(self, states, actions):\n        with tf.device(self.device):\n            return self.disc.compute_reward([states, actions])\n\n    @staticmethod\n    def get_argument(parser=None):\n        import argparse\n        if parser is None:\n            parser = argparse.ArgumentParser(conflict_handler=\'resolve\')\n        parser.add_argument(\'--enable-sn\', action=\'store_true\')\n        return parser\n'"
tf2rl/algos/policy_base.py,1,"b'import numpy as np\nimport tensorflow as tf\n\n\nclass Policy(tf.keras.Model):\n    def __init__(\n            self,\n            name,\n            memory_capacity,\n            update_interval=1,\n            batch_size=256,\n            discount=0.99,\n            n_warmup=0,\n            max_grad=10.,\n            n_epoch=1,\n            gpu=0):\n        super().__init__()\n        self.policy_name = name\n        self.update_interval = update_interval\n        self.batch_size = batch_size\n        self.discount = discount\n        self.n_warmup = n_warmup\n        self.n_epoch = n_epoch\n        self.max_grad = max_grad\n        self.memory_capacity = memory_capacity\n        self.device = ""/gpu:{}"".format(gpu) if gpu >= 0 else ""/cpu:0""\n\n    def get_action(self, observation, test=False):\n        raise NotImplementedError\n\n    @staticmethod\n    def get_argument(parser=None):\n        import argparse\n        if parser is None:\n            parser = argparse.ArgumentParser(conflict_handler=\'resolve\')\n        parser.add_argument(\'--n-warmup\', type=int, default=int(1e4))\n        parser.add_argument(\'--batch-size\', type=int, default=32)\n        parser.add_argument(\'--gpu\', type=int, default=0,\n                            help=\'GPU id\')\n        return parser\n\n\nclass OnPolicyAgent(Policy):\n    """"""\n    Base class for on-policy agents\n    """"""\n\n    def __init__(\n            self,\n            horizon=2048,\n            lam=0.95,\n            enable_gae=True,\n            normalize_adv=True,\n            entropy_coef=0.01,\n            vfunc_coef=1.,\n            **kwargs):\n        self.horizon = horizon\n        self.lam = lam\n        self.enable_gae = enable_gae\n        self.normalize_adv = normalize_adv\n        self.entropy_coef = entropy_coef\n        self.vfunc_coef = vfunc_coef\n        kwargs[""n_warmup""] = 0\n        kwargs[""memory_capacity""] = self.horizon\n        super().__init__(**kwargs)\n        assert self.horizon % self.batch_size == 0, \\\n            ""Horizon should be divisible by batch size""\n\n    @staticmethod\n    def get_argument(parser=None):\n        parser = Policy.get_argument(parser)\n        parser.add_argument(\'--horizon\', type=int, default=2048)\n        parser.add_argument(\'--normalize-adv\', action=\'store_true\')\n        parser.add_argument(\'--enable-gae\', action=\'store_true\')\n        return parser\n\n\nclass OffPolicyAgent(Policy):\n    """"""\n    Base class for off-policy agents\n    """"""\n\n    def __init__(\n            self,\n            memory_capacity,\n            **kwargs):\n        super().__init__(memory_capacity=memory_capacity, **kwargs)\n\n    @staticmethod\n    def get_argument(parser=None):\n        parser = Policy.get_argument(parser)\n        parser.add_argument(\'--memory-capacity\', type=int, default=int(1e6))\n        return parser\n\n\nclass IRLPolicy(Policy):\n    def __init__(\n            self,\n            n_training=1,\n            memory_capacity=0,\n            **kwargs):\n        self.n_training = n_training\n        super().__init__(memory_capacity=memory_capacity, **kwargs)\n'"
tf2rl/algos/ppo.py,33,"b'import numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.algos.vpg import VPG\n\n\nclass PPO(VPG):\n    def __init__(\n            self,\n            clip=True,\n            clip_ratio=0.2,\n            name=""PPO"",\n            **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.clip = clip\n        self.clip_ratio = clip_ratio\n\n    def train(self, states, actions, advantages, logp_olds, returns):\n        # Train actor and critic\n        if self.actor_critic is not None:\n            actor_loss, critic_loss, logp_news, ratio, ent = \\\n                self._train_actor_critic_body(\n                    states, actions, advantages, logp_olds, returns)\n        else:\n            actor_loss, logp_news, ratio, ent = self._train_actor_body(\n                states, actions, advantages, logp_olds)\n            critic_loss = self._train_critic_body(states, returns)\n        # Visualize results in TensorBoard\n        tf.summary.scalar(name=self.policy_name+""/actor_loss"",\n                          data=actor_loss)\n        tf.summary.scalar(name=self.policy_name+""/logp_max"",\n                          data=np.max(logp_news))\n        tf.summary.scalar(name=self.policy_name+""/logp_min"",\n                          data=np.min(logp_news))\n        tf.summary.scalar(name=self.policy_name+""/logp_mean"",\n                          data=np.mean(logp_news))\n        tf.summary.scalar(name=self.policy_name+""/adv_max"",\n                          data=np.max(advantages))\n        tf.summary.scalar(name=self.policy_name+""/adv_min"",\n                          data=np.min(advantages))\n        tf.summary.scalar(name=self.policy_name+""/kl"",\n                          data=tf.reduce_mean(logp_olds - logp_news))\n        tf.summary.scalar(name=self.policy_name+""/ent"",\n                          data=ent)\n        tf.summary.scalar(name=self.policy_name+""/ratio"",\n                          data=tf.reduce_mean(ratio))\n        tf.summary.scalar(name=self.policy_name+""/critic_loss"",\n                          data=critic_loss)\n        return actor_loss, critic_loss\n\n    @tf.function\n    def _train_actor_critic_body(\n            self, states, actions, advantages, logp_olds, returns):\n        with tf.device(self.device):\n            with tf.GradientTape() as tape:\n                _, _, current_V = self.actor_critic(states)\n                ent = tf.reduce_mean(\n                    self.actor_critic.compute_entropy(states))\n                # Train actor\n                if self.clip:\n                    logp_news = self.actor_critic.compute_log_probs(\n                        states, actions)\n                    ratio = tf.math.exp(\n                        logp_news - tf.squeeze(logp_olds))\n                    min_adv = tf.clip_by_value(\n                        ratio,\n                        1.0 - self.clip_ratio,\n                        1.0 + self.clip_ratio) * tf.squeeze(advantages)\n                    actor_loss = -tf.reduce_mean(tf.minimum(\n                        ratio * tf.squeeze(advantages),\n                        min_adv))\n                    actor_loss -= self.entropy_coef * ent\n                else:\n                    raise NotImplementedError\n                # Train critic\n                td_errors = tf.squeeze(returns) - current_V\n                critic_loss = tf.reduce_mean(0.5 * tf.square(td_errors))\n                total_loss = actor_loss + self.vfunc_coef * critic_loss\n\n            grads = tape.gradient(\n                total_loss, self.actor_critic.trainable_variables)\n            self.actor_critic_optimizer.apply_gradients(\n                zip(grads, self.actor_critic.trainable_variables))\n\n        return actor_loss, critic_loss, logp_news, ratio, ent\n\n    @tf.function\n    def _train_actor_body(self, states, actions, advantages, logp_olds):\n        with tf.device(self.device):\n            # Update actor\n            with tf.GradientTape() as tape:\n                ent = tf.reduce_mean(\n                    self.actor.compute_entropy(states))\n                if self.clip:\n                    logp_news = self.actor.compute_log_probs(\n                        states, actions)\n                    ratio = tf.math.exp(logp_news - tf.squeeze(logp_olds))\n                    min_adv = tf.clip_by_value(\n                        ratio,\n                        1.0 - self.clip_ratio,\n                        1.0 + self.clip_ratio) * tf.squeeze(advantages)\n                    actor_loss = -tf.reduce_mean(tf.minimum(\n                        ratio * tf.squeeze(advantages),\n                        min_adv))\n                    actor_loss -= self.entropy_coef * ent\n                else:\n                    raise NotImplementedError\n            actor_grad = tape.gradient(\n                actor_loss, self.actor.trainable_variables)\n            self.actor_optimizer.apply_gradients(\n                zip(actor_grad, self.actor.trainable_variables))\n\n        return actor_loss, logp_news, ratio, ent\n'"
tf2rl/algos/sac.py,51,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\n\nfrom tf2rl.policies.gaussian_actor import GaussianActor\nfrom tf2rl.algos.policy_base import OffPolicyAgent\nfrom tf2rl.misc.target_update_ops import update_target_variables\nfrom tf2rl.misc.huber_loss import huber_loss\n\n\nclass CriticV(tf.keras.Model):\n    def __init__(self, state_shape, critic_units=[256, 256], name=\'vf\'):\n        super().__init__(name=name)\n\n        self.l1 = Dense(critic_units[0], name=""L1"", activation=\'relu\')\n        self.l2 = Dense(critic_units[1], name=""L2"", activation=\'relu\')\n        self.l3 = Dense(1, name=""L3"", activation=\'linear\')\n\n        dummy_state = tf.constant(\n            np.zeros(shape=(1,)+state_shape, dtype=np.float32))\n        self(dummy_state)\n\n    def call(self, states):\n        features = self.l1(states)\n        features = self.l2(features)\n        values = self.l3(features)\n\n        return tf.squeeze(values, axis=1, name=""values"")\n\n\nclass CriticQ(tf.keras.Model):\n    def __init__(self, state_shape, action_dim, critic_units=[256, 256], name=\'qf\'):\n        super().__init__(name=name)\n\n        self.l1 = Dense(critic_units[0], name=""L1"", activation=\'relu\')\n        self.l2 = Dense(critic_units[1], name=""L2"", activation=\'relu\')\n        self.l3 = Dense(1, name=""L2"", activation=\'linear\')\n\n        dummy_state = tf.constant(\n            np.zeros(shape=(1,)+state_shape, dtype=np.float32))\n        dummy_action = tf.constant(\n            np.zeros(shape=[1, action_dim], dtype=np.float32))\n        self([dummy_state, dummy_action])\n\n    def call(self, inputs):\n        [states, actions] = inputs\n        features = tf.concat([states, actions], axis=1)\n        features = self.l1(features)\n        features = self.l2(features)\n        values = self.l3(features)\n\n        return tf.squeeze(values, axis=1)\n\n\nclass SAC(OffPolicyAgent):\n    def __init__(\n            self,\n            state_shape,\n            action_dim,\n            name=""SAC"",\n            max_action=1.,\n            lr=3e-4,\n            actor_units=[256, 256],\n            critic_units=[256, 256],\n            tau=0.005,\n            alpha=.2,\n            auto_alpha=False,\n            n_warmup=int(1e4),\n            memory_capacity=int(1e6),\n            **kwargs):\n        super().__init__(\n            name=name, memory_capacity=memory_capacity,\n            n_warmup=n_warmup, **kwargs)\n\n        self._setup_actor(state_shape, action_dim, actor_units, lr, max_action)\n        self._setup_critic_v(state_shape, critic_units, lr)\n        self._setup_critic_q(state_shape, action_dim, critic_units, lr)\n\n        # Set hyper-parameters\n        self.tau = tau\n        self.auto_alpha = auto_alpha\n        self.state_ndim = len(state_shape)\n        if auto_alpha:\n            self.log_alpha = tf.Variable(0., dtype=tf.float32)\n            self.alpha = tf.Variable(0., dtype=tf.float32)\n            self.alpha.assign(tf.exp(self.log_alpha))\n            self.target_alpha = -action_dim\n            self.alpha_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n        else:\n            self.alpha = alpha\n\n    def _setup_actor(self, state_shape, action_dim, actor_units, lr, max_action=1.):\n        self.actor = GaussianActor(\n            state_shape, action_dim, max_action, squash=True,\n            units=actor_units)\n        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n\n    def _setup_critic_q(self, state_shape, action_dim, critic_units, lr):\n        self.qf1 = CriticQ(state_shape, action_dim, critic_units, name=""qf1"")\n        self.qf2 = CriticQ(state_shape, action_dim, critic_units, name=""qf2"")\n        self.qf1_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n        self.qf2_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n\n    def _setup_critic_v(self, state_shape, critic_units, lr):\n        self.vf = CriticV(state_shape, critic_units)\n        self.vf_target = CriticV(state_shape, critic_units)\n        update_target_variables(self.vf_target.weights,\n                                self.vf.weights, tau=1.)\n        self.vf_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n\n    def get_action(self, state, test=False):\n        assert isinstance(state, np.ndarray)\n        is_single_state = len(state.shape) == self.state_ndim\n\n        state = np.expand_dims(state, axis=0).astype(\n            np.float32) if is_single_state else state\n        action = self._get_action_body(tf.constant(state), test)\n\n        return action.numpy()[0] if is_single_state else action\n\n    @tf.function\n    def _get_action_body(self, state, test):\n        return self.actor(state, test)[0]\n\n    def train(self, states, actions, next_states, rewards, dones, weights=None):\n        # TODO: Replace `done` with `dones`\n        if weights is None:\n            weights = np.ones_like(rewards)\n\n        td_errors, actor_loss, vf_loss, qf_loss, logp_min, logp_max, logp_mean = \\\n            self._train_body(states, actions, next_states,\n                             rewards, dones, weights)\n\n        tf.summary.scalar(name=self.policy_name+""/actor_loss"", data=actor_loss)\n        tf.summary.scalar(name=self.policy_name+""/critic_V_loss"", data=vf_loss)\n        tf.summary.scalar(name=self.policy_name+""/critic_Q_loss"", data=qf_loss)\n        tf.summary.scalar(name=self.policy_name+""/logp_min"", data=logp_min)\n        tf.summary.scalar(name=self.policy_name+""/logp_max"", data=logp_max)\n        tf.summary.scalar(name=self.policy_name+""/logp_mean"", data=logp_mean)\n        if self.auto_alpha:\n            tf.summary.scalar(name=self.policy_name + ""/log_ent"", data=self.log_alpha)\n            tf.summary.scalar(name=self.policy_name+""/logp_mean+target"", data=logp_mean+self.target_alpha)\n        tf.summary.scalar(name=self.policy_name+""/ent"", data=self.alpha)\n\n        return td_errors\n\n    @tf.function\n    def _train_body(self, states, actions, next_states, rewards, dones, weights):\n        with tf.device(self.device):\n            if tf.rank(rewards) == 2:\n                rewards = tf.squeeze(rewards, axis=1)\n            not_dones = 1. - tf.cast(dones, dtype=tf.float32)\n\n            with tf.GradientTape(persistent=True) as tape:\n                # Compute loss of critic Q\n                current_q1 = self.qf1([states, actions])\n                current_q2 = self.qf2([states, actions])\n                vf_next_target = self.vf_target(next_states)\n\n                target_q = tf.stop_gradient(\n                    rewards + not_dones * self.discount * vf_next_target)\n\n                td_loss_q1 = tf.reduce_mean(huber_loss(\n                    target_q - current_q1, delta=self.max_grad) * weights)\n                td_loss_q2 = tf.reduce_mean(huber_loss(\n                    target_q - current_q2, delta=self.max_grad) * weights)  # Eq.(7)\n\n                # Compute loss of critic V\n                current_v = self.vf(states)\n\n                sample_actions, logp, _ = self.actor(states)  # Resample actions to update V\n                current_q1 = self.qf1([states, sample_actions])\n                current_q2 = self.qf2([states, sample_actions])\n                current_min_q = tf.minimum(current_q1, current_q2)\n\n                target_v = tf.stop_gradient(\n                    current_min_q - self.alpha * logp)\n                td_errors = target_v - current_v\n                td_loss_v = tf.reduce_mean(\n                    huber_loss(td_errors, delta=self.max_grad) * weights)  # Eq.(5)\n\n                # Compute loss of policy\n                policy_loss = tf.reduce_mean(\n                    (self.alpha * logp - current_min_q) * weights)  # Eq.(12)\n\n                # Compute loss of temperature parameter for entropy\n                if self.auto_alpha:\n                    alpha_loss = -tf.reduce_mean(\n                        (self.log_alpha * tf.stop_gradient(logp + self.target_alpha)))\n\n            q1_grad = tape.gradient(td_loss_q1, self.qf1.trainable_variables)\n            self.qf1_optimizer.apply_gradients(\n                zip(q1_grad, self.qf1.trainable_variables))\n            q2_grad = tape.gradient(td_loss_q2, self.qf2.trainable_variables)\n            self.qf2_optimizer.apply_gradients(\n                zip(q2_grad, self.qf2.trainable_variables))\n\n            vf_grad = tape.gradient(td_loss_v, self.vf.trainable_variables)\n            self.vf_optimizer.apply_gradients(\n                zip(vf_grad, self.vf.trainable_variables))\n            update_target_variables(\n                self.vf_target.weights, self.vf.weights, self.tau)\n\n            actor_grad = tape.gradient(\n                policy_loss, self.actor.trainable_variables)\n            self.actor_optimizer.apply_gradients(\n                zip(actor_grad, self.actor.trainable_variables))\n\n            if self.auto_alpha:\n                alpha_grad = tape.gradient(alpha_loss, [self.log_alpha])\n                self.alpha_optimizer.apply_gradients(\n                    zip(alpha_grad, [self.log_alpha]))\n                self.alpha.assign(tf.exp(self.log_alpha))\n\n            del tape\n\n        return td_errors, policy_loss, td_loss_v, td_loss_q1, tf.reduce_min(logp), tf.reduce_max(logp), tf.reduce_mean(logp)\n\n    def compute_td_error(self, states, actions, next_states, rewards, dones):\n        if isinstance(actions, tf.Tensor):\n            rewards = tf.expand_dims(rewards, axis=1)\n            dones = tf.expand_dims(dones, 1)\n        td_errors = self._compute_td_error_body(\n            states, actions, next_states, rewards, dones)\n        return td_errors.numpy()\n\n    @tf.function\n    def _compute_td_error_body(self, states, actions, next_states, rewards, dones):\n        with tf.device(self.device):\n            not_dones = 1. - tf.cast(dones, dtype=tf.float32)\n\n            # Compute TD errors for Q-value func\n            current_q1 = self.qf1([states, actions])\n            vf_next_target = self.vf_target(next_states)\n\n            target_q = tf.stop_gradient(\n                rewards + not_dones * self.discount * vf_next_target)\n\n            td_errors_q1 = target_q - current_q1\n\n        return td_errors_q1\n\n    @staticmethod\n    def get_argument(parser=None):\n        parser = OffPolicyAgent.get_argument(parser)\n        parser.add_argument(\'--alpha\', type=float, default=0.2)\n        parser.add_argument(\'--auto-alpha\', action=""store_true"")\n        return parser\n'"
tf2rl/algos/sac_discrete.py,55,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\n\nfrom tf2rl.algos.sac import SAC\nfrom tf2rl.misc.huber_loss import huber_loss\nfrom tf2rl.misc.target_update_ops import update_target_variables\nfrom tf2rl.policies.categorical_actor import CategoricalActor\n\n\nclass CriticQ(tf.keras.Model):\n    """"""\n    Compared with original (continuous) version of SAC, the output of Q-function moves\n        from Q: S x A -> R\n        to   Q: S -> R^|A|\n    """"""\n\n    def __init__(self, state_shape, action_dim, critic_units=[256, 256], name=\'qf\'):\n        super().__init__(name=name)\n\n        self.l1 = Dense(critic_units[0], name=""L1"", activation=\'relu\')\n        self.l2 = Dense(critic_units[1], name=""L2"", activation=\'relu\')\n        self.l3 = Dense(action_dim, name=""L2"", activation=\'linear\')\n\n        dummy_state = tf.constant(\n            np.zeros(shape=(1,) + state_shape, dtype=np.float32))\n        self(dummy_state)\n\n    def call(self, states):\n        features = self.l1(states)\n        features = self.l2(features)\n        values = self.l3(features)\n\n        return values\n\n\nclass SACDiscrete(SAC):\n    def __init__(\n            self,\n            state_shape,\n            action_dim,\n            *args,\n            actor_fn=None,\n            critic_fn=None,\n            target_update_interval=None,\n            **kwargs):\n        kwargs[""name""] = ""SAC_discrete""\n        self.actor_fn = actor_fn if actor_fn is not None else CategoricalActor\n        self.critic_fn = critic_fn if critic_fn is not None else CriticQ\n        self.target_hard_update = target_update_interval is not None\n        self.target_update_interval = target_update_interval\n        self.n_training = tf.Variable(0, dtype=tf.int32)\n        super().__init__(state_shape, action_dim, *args, **kwargs)\n        if self.auto_alpha:\n            # Referring author\'s implementation of original paper:\n            # https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/blob/master/agents/actor_critic_agents/SAC_Discrete.pyhttps://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/blob/master/agents/actor_critic_agents/SAC_Discrete.py\n            self.target_alpha = -np.log((1.0 / action_dim)) * 0.98\n\n    def _setup_actor(self, state_shape, action_dim, actor_units, lr, max_action=1.):\n        # The output of actor is categorical distribution\n        self.actor = self.actor_fn(\n            state_shape, action_dim, actor_units)\n        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n\n    def _setup_critic_q(self, state_shape, action_dim, critic_units, lr):\n        self.qf1 = self.critic_fn(state_shape, action_dim, critic_units, name=""qf1"")\n        self.qf2 = self.critic_fn(state_shape, action_dim, critic_units, name=""qf2"")\n        self.qf1_target = self.critic_fn(state_shape, action_dim, critic_units, name=""qf1_target"")\n        self.qf2_target = self.critic_fn(state_shape, action_dim, critic_units, name=""qf2_target"")\n        update_target_variables(self.qf1_target.weights,\n                                self.qf1.weights, tau=1.)\n        update_target_variables(self.qf2_target.weights,\n                                self.qf2.weights, tau=1.)\n        self.qf1_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n        self.qf2_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n\n    def _setup_critic_v(self, *args, **kwargs):\n        """"""\n        Do not need state-value function because it can be directly computed from Q-function.\n        See Eq.(10) in the paper.\n        """"""\n        pass\n\n    def train(self, states, actions, next_states, rewards, dones, weights=None):\n        if weights is None:\n            weights = np.ones_like(rewards)\n\n        td_errors, actor_loss, mean_ent, logp_min, logp_max, logp_mean = \\\n            self._train_body(states, actions, next_states,\n                             rewards, dones, weights)\n\n        tf.summary.scalar(name=self.policy_name + ""/actor_loss"", data=actor_loss)\n        tf.summary.scalar(name=self.policy_name + ""/critic_loss"", data=td_errors)\n        tf.summary.scalar(name=self.policy_name + ""/mean_ent"", data=mean_ent)\n        tf.summary.scalar(name=self.policy_name + ""/logp_min"", data=logp_min)\n        tf.summary.scalar(name=self.policy_name + ""/logp_max"", data=logp_max)\n        if self.auto_alpha:\n            tf.summary.scalar(name=self.policy_name + ""/log_ent"", data=self.log_alpha)\n            tf.summary.scalar(name=self.policy_name+""/logp_mean+target"", data=logp_mean+self.target_alpha)\n        tf.summary.scalar(name=self.policy_name + ""/ent"", data=self.alpha)\n\n    @tf.function\n    def _train_body(self, states, actions, next_states, rewards, dones, weights):\n        with tf.device(self.device):\n            batch_size = states.shape[0]\n            not_dones = 1. - tf.cast(dones, dtype=tf.float32)\n            actions = tf.cast(actions, dtype=tf.int32)\n\n            indices = tf.concat(\n                values=[tf.expand_dims(tf.range(batch_size), axis=1),\n                        actions], axis=1)\n\n            with tf.GradientTape(persistent=True) as tape:\n                # Compute critic loss\n                _, _, next_action_param = self.actor(next_states)\n                next_action_prob = next_action_param[""prob""]\n                next_action_logp = tf.math.log(next_action_prob + 1e-8)\n                next_q = tf.minimum(\n                    self.qf1_target(next_states), self.qf2_target(next_states))\n\n                # Compute state value function V by directly computes expectation\n                target_q = tf.expand_dims(tf.einsum(\n                    \'ij,ij->i\', next_action_prob, next_q - self.alpha * next_action_logp), axis=1)  # Eq.(10)\n                target_q = tf.stop_gradient(\n                    rewards + not_dones * self.discount * target_q)\n\n                current_q1 = self.qf1(states)\n                current_q2 = self.qf2(states)\n\n                td_loss1 = tf.reduce_mean(huber_loss(\n                    target_q - tf.expand_dims(tf.gather_nd(current_q1, indices), axis=1),\n                    delta=self.max_grad) * weights)\n                td_loss2 = tf.reduce_mean(huber_loss(\n                    target_q - tf.expand_dims(tf.gather_nd(current_q2, indices), axis=1),\n                    delta=self.max_grad) * weights)  # Eq.(7)\n\n                # Compute actor loss\n                _, _, current_action_param = self.actor(states)\n                current_action_prob = current_action_param[""prob""]\n                current_action_logp = tf.math.log(current_action_prob + 1e-8)\n\n                policy_loss = tf.reduce_mean(\n                    tf.einsum(\'ij,ij->i\', current_action_prob,\n                              self.alpha * current_action_logp - tf.stop_gradient(\n                                  tf.minimum(current_q1, current_q2))) * weights)  # Eq.(12)\n                mean_ent = tf.reduce_mean(\n                    tf.einsum(\'ij,ij->i\', current_action_prob, current_action_logp)) * (-1)\n\n                if self.auto_alpha:\n                    alpha_loss = -tf.reduce_mean(\n                        (self.log_alpha * tf.stop_gradient(current_action_logp + self.target_alpha)))\n\n            q1_grad = tape.gradient(td_loss1, self.qf1.trainable_variables)\n            self.qf1_optimizer.apply_gradients(\n                zip(q1_grad, self.qf1.trainable_variables))\n            q2_grad = tape.gradient(td_loss2, self.qf2.trainable_variables)\n            self.qf2_optimizer.apply_gradients(\n                zip(q2_grad, self.qf2.trainable_variables))\n\n            if self.target_hard_update:\n                if self.n_training % self.target_update_interval == 0:\n                    update_target_variables(self.qf1_target.weights,\n                                            self.qf1.weights, tau=1.)\n                    update_target_variables(self.qf2_target.weights,\n                                            self.qf2.weights, tau=1.)\n            else:\n                update_target_variables(self.qf1_target.weights,\n                                        self.qf1.weights, tau=self.tau)\n                update_target_variables(self.qf2_target.weights,\n                                        self.qf2.weights, tau=self.tau)\n\n            actor_grad = tape.gradient(\n                policy_loss, self.actor.trainable_variables)\n            self.actor_optimizer.apply_gradients(\n                zip(actor_grad, self.actor.trainable_variables))\n\n            if self.auto_alpha:\n                alpha_grad = tape.gradient(alpha_loss, [self.log_alpha])\n                self.alpha_optimizer.apply_gradients(\n                    zip(alpha_grad, [self.log_alpha]))\n                self.alpha.assign(tf.exp(self.log_alpha))\n\n        return (td_loss1 + td_loss2) / 2., policy_loss, mean_ent, \\\n            tf.reduce_min(current_action_logp), tf.reduce_max(current_action_logp), \\\n            tf.reduce_mean(current_action_logp)\n\n    def compute_td_error(self, states, actions, next_states, rewards, dones):\n        td_errors_q1, td_errors_q2 = self._compute_td_error_body(\n            states, actions, next_states, rewards, dones)\n        return np.squeeze(\n            np.abs(td_errors_q1.numpy()) +\n            np.abs(td_errors_q2.numpy()))\n\n    @tf.function\n    def _compute_td_error_body(self, states, actions, next_states, rewards, dones):\n        with tf.device(self.device):\n            batch_size = states.shape[0]\n            not_dones = 1. - tf.cast(dones, dtype=tf.float32)\n            actions = tf.cast(actions, dtype=tf.int32)\n\n            indices = tf.concat(\n                values=[tf.expand_dims(tf.range(batch_size), axis=1),\n                        actions], axis=1)\n\n            _, _, next_action_param = self.actor(next_states)\n            next_action_prob = next_action_param[""prob""]\n            next_action_logp = tf.math.log(next_action_prob + 1e-8)\n            next_q = tf.minimum(\n                self.qf1_target(next_states), self.qf2_target(next_states))\n\n            target_q = tf.expand_dims(tf.einsum(\n                \'ij,ij->i\', next_action_prob, next_q - self.alpha * next_action_logp), axis=1)  # Eq.(10)\n            target_q = tf.stop_gradient(\n                rewards + not_dones * self.discount * target_q)\n\n            current_q1 = self.qf1(states)\n            current_q2 = self.qf2(states)\n\n            td_errors_q1 = target_q - tf.expand_dims(\n                tf.gather_nd(current_q1, indices), axis=1)\n            td_errors_q2 = target_q - tf.expand_dims(\n                tf.gather_nd(current_q2, indices), axis=1)  # Eq.(7)\n\n        return td_errors_q1, td_errors_q2\n\n    @staticmethod\n    def get_argument(parser=None):\n        parser = SAC.get_argument(parser)\n        parser.add_argument(\'--target-update-interval\', type=int, default=None)\n        return parser\n'"
tf2rl/algos/td3.py,29,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\n\nfrom tf2rl.algos.ddpg import DDPG, Actor\nfrom tf2rl.misc.target_update_ops import update_target_variables\nfrom tf2rl.misc.huber_loss import huber_loss\n\n\nclass Critic(tf.keras.Model):\n    def __init__(self, state_shape, action_dim, units=[400, 300], name=""Critic""):\n        super().__init__(name=name)\n\n        self.l1 = Dense(units[0], name=""L1"")\n        self.l2 = Dense(units[1], name=""L2"")\n        self.l3 = Dense(1, name=""L3"")\n\n        self.l4 = Dense(units[0], name=""L4"")\n        self.l5 = Dense(units[1], name=""L5"")\n        self.l6 = Dense(1, name=""L6"")\n\n        dummy_state = tf.constant(\n            np.zeros(shape=(1,)+state_shape, dtype=np.float32))\n        dummy_action = tf.constant(\n            np.zeros(shape=[1, action_dim], dtype=np.float32))\n        with tf.device(""/cpu:0""):\n            self([dummy_state, dummy_action])\n\n    def call(self, inputs):\n        states, actions = inputs\n        xu = tf.concat([states, actions], axis=1)\n\n        x1 = tf.nn.relu(self.l1(xu))\n        x1 = tf.nn.relu(self.l2(x1))\n        x1 = self.l3(x1)\n\n        x2 = tf.nn.relu(self.l4(xu))\n        x2 = tf.nn.relu(self.l5(x2))\n        x2 = self.l6(x2)\n\n        return x1, x2\n\n\nclass TD3(DDPG):\n    def __init__(\n            self,\n            state_shape,\n            action_dim,\n            name=""TD3"",\n            actor_update_freq=2,\n            policy_noise=0.2,\n            noise_clip=0.5,\n            actor_units=[400, 300],\n            critic_units=[400, 300],\n            lr_critic=0.001,\n            **kwargs):\n        super().__init__(name=name, state_shape=state_shape, action_dim=action_dim,\n                         actor_units=actor_units, critic_units=critic_units,\n                         lr_critic=lr_critic, **kwargs)\n\n        self.critic = Critic(state_shape, action_dim, critic_units)\n        self.critic_target = Critic(state_shape, action_dim, critic_units)\n        update_target_variables(\n            self.critic_target.weights, self.critic.weights, tau=1.)\n        self.critic_optimizer = tf.keras.optimizers.Adam(\n            learning_rate=lr_critic)\n\n        self._policy_noise = policy_noise\n        self._noise_clip = noise_clip\n\n        self._actor_update_freq = actor_update_freq\n        self._it = tf.Variable(0, dtype=tf.int32)\n\n    @tf.function\n    def _train_body(self, states, actions, next_states, rewards, done, weights):\n        with tf.device(self.device):\n            with tf.GradientTape() as tape:\n                td_error1, td_error2 = self._compute_td_error_body(\n                    states, actions, next_states, rewards, done)\n                critic_loss = tf.reduce_mean(huber_loss(td_error1, delta=self.max_grad) * weights) + \\\n                              tf.reduce_mean(huber_loss(td_error2, delta=self.max_grad) * weights)\n\n            critic_grad = tape.gradient(\n                critic_loss, self.critic.trainable_variables)\n            self.critic_optimizer.apply_gradients(\n                zip(critic_grad, self.critic.trainable_variables))\n\n            self._it.assign_add(1)\n            with tf.GradientTape() as tape:\n                next_actions = self.actor(states)\n                actor_loss = - \\\n                    tf.reduce_mean(self.critic([states, next_actions]))\n\n            remainder = tf.math.mod(self._it, self._actor_update_freq)\n            def optimize_actor():\n                actor_grad = tape.gradient(\n                    actor_loss, self.actor.trainable_variables)\n                return self.actor_optimizer.apply_gradients(\n                    zip(actor_grad, self.actor.trainable_variables))\n\n            tf.cond(pred=tf.equal(remainder, 0), true_fn=optimize_actor, false_fn=tf.no_op)\n            # Update target networks\n            update_target_variables(\n                self.critic_target.weights, self.critic.weights, self.tau)\n            update_target_variables(\n                self.actor_target.weights, self.actor.weights, self.tau)\n\n            return actor_loss, critic_loss, tf.abs(td_error1) + tf.abs(td_error2)\n\n    def compute_td_error(self, states, actions, next_states, rewards, dones):\n        td_errors1, td_errors2 = self._compute_td_error_body(\n            states, actions, next_states, rewards, dones)\n        return np.squeeze(np.abs(td_errors1.numpy()) + np.abs(td_errors2.numpy()))\n\n    @tf.function\n    def _compute_td_error_body(self, states, actions, next_states, rewards, dones):\n        with tf.device(self.device):\n            not_dones = 1. - dones\n\n            # Get noisy action\n            next_action = self.actor_target(next_states)\n            noise = tf.cast(tf.clip_by_value(\n                tf.random.normal(shape=tf.shape(next_action),\n                                 stddev=self._policy_noise),\n                -self._noise_clip, self._noise_clip), tf.float32)\n            next_action = tf.clip_by_value(\n                next_action + noise, -self.actor_target.max_action, self.actor_target.max_action)\n\n            target_Q1, target_Q2 = self.critic_target(\n                [next_states, next_action])\n            target_Q = tf.minimum(target_Q1, target_Q2)\n            target_Q = rewards + (not_dones * self.discount * target_Q)\n            target_Q = tf.stop_gradient(target_Q)\n            current_Q1, current_Q2 = self.critic([states, actions])\n\n        return target_Q - current_Q1, target_Q - current_Q2\n'"
tf2rl/algos/vail.py,32,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\n\nfrom tf2rl.algos.gail import GAIL\nfrom tf2rl.algos.policy_base import IRLPolicy\nfrom tf2rl.networks.spectral_norm_dense import SNDense\n\n\nclass Discriminator(tf.keras.Model):\n    LOG_SIG_CAP_MAX = 2  # np.e**2 = 7.389\n    LOG_SIG_CAP_MIN = -20  # np.e**-10 = 4.540e-05\n    EPS = 1e-6\n\n    def __init__(self, state_shape, action_dim, units=[32, 32],\n                 n_latent_unit=32, enable_sn=False, name=""Discriminator""):\n        super().__init__(name=name)\n\n        DenseClass = SNDense if enable_sn else Dense\n        self.l1 = DenseClass(units[0], name=""L1"", activation=""relu"")\n        self.l2 = DenseClass(units[1], name=""L2"", activation=""relu"")\n        self.l_mean = DenseClass(n_latent_unit, name=""L_mean"", activation=""linear"")\n        self.l_logstd = DenseClass(n_latent_unit, name=""L_std"", activation=""linear"")\n        self.l3 = DenseClass(1, name=""L3"", activation=""sigmoid"")\n\n        dummy_state = tf.constant(\n            np.zeros(shape=(1,)+state_shape, dtype=np.float32))\n        dummy_action = tf.constant(\n            np.zeros(shape=[1, action_dim], dtype=np.float32))\n        with tf.device(""/cpu:0""):\n            self([dummy_state, dummy_action])\n\n    def call(self, inputs):\n        # Encoder\n        features = tf.concat(inputs, axis=1)\n        features = self.l1(features)\n        features = self.l2(features)\n        means = self.l_mean(features)\n        logstds = self.l_logstd(features)\n        logstds = tf.clip_by_value(\n            logstds, self.LOG_SIG_CAP_MIN, self.LOG_SIG_CAP_MAX)\n        latents = means + tf.random.normal(shape=means.shape) * tf.math.exp(logstds)\n        # Binary classifier\n        out = self.l3(latents)\n        return out, means, logstds\n\n    def compute_reward(self, inputs):\n        features = tf.concat(inputs, axis=1)\n        features = self.l1(features)\n        features = self.l2(features)\n        means = self.l_mean(features)\n        return tf.math.log(self.l3(means) + 1e-8)\n\n\nclass VAIL(GAIL):\n    def __init__(\n            self,\n            state_shape,\n            action_dim,\n            units=[32, 32],\n            n_latent_unit=32,\n            lr=5e-5,\n            kl_target=0.5,\n            reg_param=0.,\n            enable_sn=False,\n            enable_gp=False,\n            name=""VAIL"",\n            **kwargs):\n        """"""\n        :param enable_sn (bool): If true, add spectral normalization in Dense layer\n        :param enable_gp (bool): If true, add gradient penalty to loss function\n        """"""\n        IRLPolicy.__init__(\n            self, name=name, n_training=10, **kwargs)\n        self.disc = Discriminator(\n            state_shape=state_shape, action_dim=action_dim,\n            units=units, n_latent_unit=n_latent_unit,\n            enable_sn=enable_sn)\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n        self._kl_target = kl_target\n        self._reg_param = tf.Variable(reg_param, dtype=tf.float32)\n        self._step_reg_param = tf.constant(1e-5, dtype=tf.float32)\n        self._enable_gp = enable_gp\n\n    def train(self, agent_states, agent_acts,\n              expert_states, expert_acts, **kwargs):\n        loss, accuracy, real_kl, fake_kl, js_divergence = self._train_body(\n            agent_states, agent_acts, expert_states, expert_acts)\n        tf.summary.scalar(name=self.policy_name+""/DiscriminatorLoss"", data=loss)\n        tf.summary.scalar(name=self.policy_name+""/Accuracy"", data=accuracy)\n        tf.summary.scalar(name=self.policy_name+""/RegParam"", data=self._reg_param)\n        tf.summary.scalar(name=self.policy_name+""/RealLatentKL"", data=real_kl)\n        tf.summary.scalar(name=self.policy_name+""/FakeLatentKL"", data=fake_kl)\n        tf.summary.scalar(name=self.policy_name+""/JSdivergence"", data=js_divergence)\n\n    @tf.function\n    def _compute_kl_latent(self, means, log_stds):\n        """"""\n        Compute KL divergence of latent spaces over standard Normal\n        distribution to compute loss in eq.5.  The formulation of\n        KL divergence between two normal distributions is as follows:\n            ln(\\sigma_2 / \\sigma_1) + {(\\mu_1 - \\mu_2)^2 + \\sigma_1^2 - \\sigma_2^2} / (2 * \\sigma_2^2)\n        Since the target distribution is standard Normal distributions,\n        we can assume `\\sigma_2 = 1` and `mean_2 = 0`.\n        So, the resulting equation can be computed as:\n            ln(1 / \\sigma_1) + (\\mu_1^2 + \\sigma_1^2 - 1) / 2\n        """"""\n        return tf.reduce_sum(\n            -log_stds + (tf.square(means) + tf.square(tf.exp(log_stds)) - 1.) / 2.,\n                axis=-1)\n\n    @tf.function\n    def _train_body(self, agent_states, agent_acts, expert_states, expert_acts):\n        epsilon = 1e-8\n        with tf.device(self.device):\n            with tf.GradientTape() as tape:\n                # Compute discriminator loss\n                real_logits, real_means, real_logstds = self.disc(\n                    [expert_states, expert_acts])\n                fake_logits, fake_means, fake_logstds = self.disc(\n                    [agent_states, agent_acts])\n                disc_loss = -(tf.reduce_mean(tf.math.log(real_logits + epsilon)) +\n                              tf.reduce_mean(tf.math.log(1. - fake_logits + epsilon)))\n                # Compute KL loss\n                real_kl = tf.reduce_mean(\n                    self._compute_kl_latent(real_means, real_logstds))\n                fake_kl = tf.reduce_mean(\n                    self._compute_kl_latent(fake_means, fake_logstds))\n                kl_loss = 0.5 * (real_kl - self._kl_target +\n                                 fake_kl - self._kl_target)\n                loss = disc_loss + self._reg_param * kl_loss\n                # Gradient penalty\n                if self._enable_gp:\n                    raise NotImplementedError\n\n            grads = tape.gradient(loss, self.disc.trainable_variables)\n            self.optimizer.apply_gradients(\n                zip(grads, self.disc.trainable_variables))\n\n        # Update reguralizer parameter \\beta in eq.(9)\n        self._reg_param.assign(tf.maximum(\n            tf.constant(0., dtype=tf.float32),\n            self._reg_param + self._step_reg_param * (kl_loss - self._kl_target)))\n\n        accuracy = \\\n            tf.reduce_mean(tf.cast(real_logits >= 0.5, tf.float32)) / 2. + \\\n            tf.reduce_mean(tf.cast(fake_logits < 0.5, tf.float32)) / 2.\n        js_divergence = self._compute_js_divergence(\n            fake_logits, real_logits)\n        return loss, accuracy, real_kl, fake_kl, js_divergence\n'"
tf2rl/algos/vpg.py,28,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\n\nfrom tf2rl.algos.policy_base import OnPolicyAgent\nfrom tf2rl.policies.gaussian_actor import GaussianActor\nfrom tf2rl.policies.categorical_actor import CategoricalActor\nfrom tf2rl.envs.atari_wrapper import LazyFrames\n\n\nclass CriticV(tf.keras.Model):\n    def __init__(self, state_shape, units, name=\'critic_v\', hidden_activation=\'relu\'):\n        super().__init__(name=name)\n\n        self.l1 = Dense(units[0], name=""L1"", activation=hidden_activation)\n        self.l2 = Dense(units[1], name=""L2"", activation=hidden_activation)\n        self.l3 = Dense(1, name=""L2"", activation=\'linear\')\n\n        with tf.device(\'/cpu:0\'):\n            self(tf.constant(\n                np.zeros(shape=(1,)+state_shape, dtype=np.float32)))\n\n    def call(self, inputs):\n        features = self.l1(inputs)\n        features = self.l2(features)\n        values = self.l3(features)\n\n        return tf.squeeze(values, axis=1)\n\n\nclass VPG(OnPolicyAgent):\n    def __init__(\n            self,\n            state_shape,\n            action_dim,\n            is_discrete,\n            actor=None,\n            critic=None,\n            actor_critic=None,\n            max_action=1.,\n            actor_units=[256, 256],\n            critic_units=[256, 256],\n            lr_actor=1e-3,\n            lr_critic=3e-3,\n            fix_std=False,\n            const_std=0.3,\n            hidden_activation_actor=""relu"",\n            hidden_activation_critic=""relu"",\n            name=""VPG"",\n            **kwargs):\n        super().__init__(name=name, **kwargs)\n        self._is_discrete = is_discrete\n\n        # TODO: clean codes\n        if actor_critic is not None:\n            self.actor_critic = actor_critic\n            self.actor_critic_optimizer = tf.keras.optimizers.Adam(\n                learning_rate=lr_actor)\n            self.actor = None\n            self.critic = None\n        else:\n            self.actor_critic = None\n            if actor is None:\n                if is_discrete:\n                    self.actor = CategoricalActor(\n                        state_shape, action_dim, actor_units)\n                else:\n                    self.actor = GaussianActor(\n                        state_shape, action_dim, max_action, actor_units,\n                        hidden_activation=hidden_activation_actor,\n                        fix_std=fix_std, const_std=const_std, state_independent_std=True)\n            else:\n                self.actor = actor\n            if critic is None:\n                self.critic = CriticV(state_shape, critic_units,\n                                      hidden_activation=hidden_activation_critic)\n            else:\n                self.critic = critic\n            self.actor_optimizer = tf.keras.optimizers.Adam(\n                learning_rate=lr_actor)\n            self.critic_optimizer = tf.keras.optimizers.Adam(\n                learning_rate=lr_critic)\n\n        # This is used to check if input state to `get_action` is multiple (batch) or single\n        self._state_ndim = np.array(state_shape).shape[0]\n\n    def get_action(self, state, test=False):\n        if isinstance(state, LazyFrames):\n            state = np.array(state)\n        assert isinstance(state, np.ndarray), \\\n            ""Input instance should be np.ndarray, not {}"".format(type(state))\n\n        is_single_input = state.ndim == self._state_ndim\n        if is_single_input:\n            state = np.expand_dims(state, axis=0).astype(np.float32)\n        action, logp, _ = self._get_action_body(state, test)\n\n        if is_single_input:\n            return action.numpy()[0], logp.numpy()\n        else:\n            return action.numpy(), logp.numpy()\n\n    def get_action_and_val(self, state, test=False):\n        if isinstance(state, LazyFrames):\n            state = np.array(state)\n        is_single_input = state.ndim == self._state_ndim\n        if is_single_input:\n            state = np.expand_dims(state, axis=0).astype(np.float32)\n\n        action, logp, v = self._get_action_logp_v_body(state, test)\n\n        if is_single_input:\n            v = v[0]\n            action = action[0]\n\n        return action.numpy(), logp.numpy(), v.numpy()\n\n    @tf.function\n    def _get_action_logp_v_body(self, state, test):\n        if self.actor_critic:\n            return self.actor_critic(state, test)\n        else:\n            action, logp, _ = self.actor(state, test)\n            v = self.critic(state)\n            return action, logp, v\n\n    @tf.function\n    def _get_action_body(self, state, test):\n        if self.actor_critic is not None:\n            action, logp, param = self.actor_critic(state, test)\n            return action, logp, param\n        else:\n            return self.actor(state, test)\n\n    def train(self, states, actions, advantages, logp_olds, returns):\n        # Train actor and critic\n        actor_loss, logp_news = self._train_actor_body(\n            states, actions, advantages, logp_olds)\n        critic_loss = self._train_critic_body(states, returns)\n        # Visualize results in TensorBoard\n        tf.summary.scalar(name=self.policy_name+""/actor_loss"",\n                          data=actor_loss)\n        tf.summary.scalar(name=self.policy_name+""/logp_max"",\n                          data=np.max(logp_news))\n        tf.summary.scalar(name=self.policy_name+""/logp_min"",\n                          data=np.min(logp_news))\n        tf.summary.scalar(name=self.policy_name+""/logp_mean"",\n                          data=np.mean(logp_news))\n        tf.summary.scalar(name=self.policy_name+""/adv_max"",\n                          data=np.max(advantages))\n        tf.summary.scalar(name=self.policy_name+""/adv_min"",\n                          data=np.min(advantages))\n        tf.summary.scalar(name=self.policy_name+""/kl"",\n                          data=tf.reduce_mean(logp_olds - logp_news))\n        tf.summary.scalar(name=self.policy_name +\n                          ""/critic_loss"", data=critic_loss)\n        return actor_loss, critic_loss\n\n    @tf.function\n    def _train_actor_body(self, states, actions, advantages, logp_olds):\n        with tf.device(self.device):\n            with tf.GradientTape() as tape:\n                log_probs = self.actor.compute_log_probs(states, actions)\n                weights = tf.stop_gradient(tf.squeeze(advantages))\n                # + lambda * entropy\n                actor_loss = tf.reduce_mean(-log_probs * weights)\n            actor_grads = tape.gradient(\n                actor_loss, self.actor.trainable_variables)\n            self.actor_optimizer.apply_gradients(\n                zip(actor_grads, self.actor.trainable_variables))\n\n        return actor_loss, log_probs\n\n    @tf.function\n    def _train_critic_body(self, states, returns):\n        with tf.device(self.device):\n            # Train baseline\n            with tf.GradientTape() as tape:\n                current_V = self.critic(states)\n                td_errors = tf.squeeze(returns) - current_V\n                critic_loss = tf.reduce_mean(0.5 * tf.square(td_errors))\n            critic_grad = tape.gradient(\n                critic_loss, self.critic.trainable_variables)\n            self.critic_optimizer.apply_gradients(\n                zip(critic_grad, self.critic.trainable_variables))\n\n        return critic_loss\n'"
tf2rl/distributions/__init__.py,0,b''
tf2rl/distributions/base.py,0,"b'class Distribution(object):\n    def __init__(self, dim):\n        self._dim = dim\n        self._tiny = 1e-8\n\n    @property\n    def dim(self):\n        raise self._dim\n\n    def kl(self, old_dist, new_dist):\n        """"""\n        Compute the KL divergence of two distributions\n        """"""\n        raise NotImplementedError\n\n    def likelihood_ratio(self, x, old_dist, new_dist):\n        raise NotImplementedError\n\n    def entropy(self, dist):\n        raise NotImplementedError\n\n    def log_likelihood_sym(self, x, dist):\n        raise NotImplementedError\n\n    def log_likelihood(self, xs, dist):\n        raise NotImplementedError\n'"
tf2rl/distributions/categorical.py,9,"b'import numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.distributions.base import Distribution\n\n\nclass Categorical(Distribution):\n    def kl(self, old_param, new_param):\n        """"""\n        Compute the KL divergence of two Categorical distribution as:\n            p_1 * (\\log p_1  - \\log p_2)\n        """"""\n        old_prob, new_prob = old_param[""prob""], new_param[""prob""]\n        return tf.reduce_sum(\n            old_prob * (tf.math.log(old_prob + self._tiny) - tf.math.log(new_prob + self._tiny)))\n\n    def likelihood_ratio(self, x, old_param, new_param):\n        old_prob, new_prob = old_param[""prob""], new_param[""prob""]\n        return (tf.reduce_sum(new_prob * x) + self._tiny) / (tf.reduce_sum(old_prob * x) + self._tiny)\n\n    def log_likelihood(self, x, param):\n        """"""\n        Compute log likelihood as:\n            \\log \\sum(p_i * x_i)\n\n        :param x (tf.Tensor or np.ndarray): Values to compute log likelihood\n        :param param (Dict): Dictionary that contains probabilities of outputs\n        :return (tf.Tensor): Log probabilities\n        """"""\n        probs = param[""prob""]\n        assert probs.shape == x.shape, \\\n            ""Different shape inputted. You might have forgotten to convert `x` to one-hot vector.""\n        return tf.math.log(tf.reduce_sum(probs * x, axis=1) + self._tiny)\n\n    def sample(self, param):\n        probs = param[""prob""]\n        # NOTE: input to `tf.random.categorical` is log probabilities\n        # For more details, see https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random/categorical\n        # [probs.shape[0], 1]\n        return tf.random.categorical(tf.math.log(probs), 1)\n\n    def entropy(self, param):\n        probs = param[""prob""]\n        return -tf.reduce_sum(probs * tf.math.log(probs + self._tiny), axis=1)\n'"
tf2rl/distributions/diagonal_gaussian.py,13,"b'import numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.distributions.base import Distribution\n\n\nclass DiagonalGaussian(Distribution):\n    def __init__(self, dim):\n        self._dim = dim\n\n    @property\n    def dim(self):\n        return self._dim\n\n    def kl(self, old_param, new_param):\n        """"""\n        Compute KL divergence of two distributions as:\n            {(\\mu_1 - \\mu_2)^2 + \\sigma_1^2 - \\sigma_2^2} / (2 * \\sigma_2^2) + ln(\\sigma_2 / \\sigma_1)\n\n        :param old_param (Dict):\n            Gaussian distribution to compare with that contains\n            means: (batch_size * output_dim)\n            std: (batch_size * output_dim)\n        :param new_param (Dict): Same contents with old_param\n        """"""\n        old_means, old_log_stds = old_param[""mean""], old_param[""log_std""]\n        new_means, new_log_stds = new_param[""mean""], new_param[""log_std""]\n        old_std = tf.math.exp(old_log_stds)\n        new_std = tf.math.exp(new_log_stds)\n\n        numerator = tf.math.square(old_means - new_means) \\\n            + tf.math.square(old_std) - tf.math.square(new_std)\n        denominator = 2 * tf.math.square(new_std) + 1e-8\n        return tf.math.reduce_sum(numerator / denominator + new_log_stds - old_log_stds)\n\n    def likelihood_ratio(self, x, old_param, new_param):\n        llh_new = self.log_likelihood(x, new_param)\n        llh_old = self.log_likelihood(x, old_param)\n        return tf.math.exp(llh_new - llh_old)\n\n    def log_likelihood(self, x, param):\n        """"""\n        Compute log likelihood as:\n            TODO: write equation\n        """"""\n        means = param[""mean""]\n        log_stds = param[""log_std""]\n        assert means.shape == log_stds.shape\n        zs = (x - means) / tf.exp(log_stds)\n        return - tf.reduce_sum(log_stds, axis=-1) \\\n               - 0.5 * tf.reduce_sum(tf.square(zs), axis=-1) \\\n               - 0.5 * self.dim * tf.math.log(2 * np.pi)\n\n    def sample(self, param):\n        means = param[""mean""]\n        log_stds = param[""log_std""]\n        # reparameterization\n        return means + tf.random.normal(shape=means.shape) * tf.math.exp(log_stds)\n\n    def entropy(self, param):\n        log_stds = param[""log_std""]\n        return tf.reduce_sum(log_stds + tf.math.log(tf.math.sqrt(2 * np.pi * np.e)), axis=-1)\n'"
tf2rl/envs/__init__.py,0,b''
tf2rl/envs/atari_wrapper.py,0,"b'""""""\nThe MIT License\n\nCopyright (c) 2017 OpenAI (http://openai.com)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n""""""\n\nimport os\nfrom collections import deque\n\nimport numpy as np\nimport cv2\nfrom gym import spaces\nimport gym\n\nos.environ.setdefault(\'PATH\', \'\')\ncv2.ocl.setUseOpenCL(False)\n\n\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        """"""\n        Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == \'NOOP\'\n\n    def reset(self, **kwargs):\n        """"""\n        Do no-op action for a number of steps in [1, noop_max].\n        """"""\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(\n                1, self.noop_max + 1)  # pylint: disable=E1101\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n            if done:\n                obs = self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""\n        Take action on reset for environments that are fixed until firing.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""\n        Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so it\'s important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        """"""\n        Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """"""\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        """"""\n        Return only every `skip`-th frame\n        """"""\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros(\n            (2,)+env.observation_space.shape, dtype=np.uint8)\n        self._skip = skip\n\n    def step(self, action):\n        """"""\n        Repeat action, sum reward, and max over last observations.\n        """"""\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2:\n                self._obs_buffer[0] = obs\n            if i == self._skip - 1:\n                self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn\'t matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        """"""Bin reward to {+1, 0, -1} by its sign.""""""\n        return np.sign(reward)\n\n\nclass WarpFrame(gym.ObservationWrapper):\n    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n        """"""\n        Warp frames to 84x84 as done in the Nature paper and later work.\n        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\n        observation should be warped.\n        """"""\n        super().__init__(env)\n        self._width = width\n        self._height = height\n        self._grayscale = grayscale\n        self._key = dict_space_key\n        if self._grayscale:\n            num_colors = 1\n        else:\n            num_colors = 3\n\n        new_space = gym.spaces.Box(\n            low=0,\n            high=255,\n            shape=(self._height, self._width, num_colors),\n            dtype=np.uint8,\n        )\n        if self._key is None:\n            original_space = self.observation_space\n            self.observation_space = new_space\n        else:\n            original_space = self.observation_space.spaces[self._key]\n            self.observation_space.spaces[self._key] = new_space\n        assert original_space.dtype == np.uint8 and len(\n            original_space.shape) == 3\n\n    def observation(self, obs):\n        if self._key is None:\n            frame = obs\n        else:\n            frame = obs[self._key]\n\n        if self._grayscale:\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        frame = cv2.resize(\n            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\n        )\n        if self._grayscale:\n            frame = np.expand_dims(frame, -1)\n\n        if self._key is None:\n            obs = frame\n        else:\n            obs = obs.copy()\n            obs[self._key] = frame\n        return obs\n\n\nclass ProcessFrame84(gym.ObservationWrapper):\n    def __init__(self, env=None):\n        super(ProcessFrame84, self).__init__(env)\n        self.observation_space = spaces.Box(\n            low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n\n    def observation(self, obs):\n        return ProcessFrame84.process(obs)\n\n    @staticmethod\n    def process(frame):\n        if frame.size == 210 * 160 * 3:\n            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n        elif frame.size == 250 * 160 * 3:\n            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n        else:\n            assert False, ""Unknown resolution.""\n        img = img[:, :, 0] * 0.299 + img[:, :, 1] * \\\n            0.587 + img[:, :, 2] * 0.114\n        resized_screen = cv2.resize(\n            img, (84, 110), interpolation=cv2.INTER_AREA)\n        x_t = resized_screen[18:102, :]\n        x_t = np.reshape(x_t, [84, 84, 1])\n        return x_t.astype(np.uint8)\n\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        """"""\n        Stack k last frames.\n        Returns lazy array, which is much more memory efficient.\n        See also baselines.common.atari_wrappers.LazyFrames\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = spaces.Box(\n            low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)),\n            dtype=env.observation_space.dtype)\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return self._get_ob()\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return self._get_ob(), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\n\nclass ScaledFloatFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        gym.ObservationWrapper.__init__(self, env)\n        self.observation_space = gym.spaces.Box(\n            low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n\n    def observation(self, observation):\n        # careful! This undoes the memory optimization, use\n        # with smaller replay buffers only.\n        return np.array(observation).astype(np.float32) / 255.0\n\n\nclass LazyFrames(object):\n    def __init__(self, frames):\n        """"""\n        This object ensures that common frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN\'s 1M frames replay\n        buffers.\n        This object should only be converted to numpy array before being passed to the model.\n        You\'d not believe how complex the previous solution was.""""""\n        self._frames = frames\n        self._out = None\n\n    def _force(self):\n        if self._out is None:\n            self._out = np.concatenate(self._frames, axis=-1)\n            self._frames = None\n        return self._out\n\n    def __array__(self, dtype=None):\n        out = self._force()\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self._force())\n\n    def __getitem__(self, i):\n        return self._force()[..., i]\n\n\nclass NdarrayFrames(gym.Wrapper):\n    def __init__(self, env):\n        gym.Wrapper.__init__(self, env)\n\n    def reset(self):\n        obs = self.env.reset()\n        assert isinstance(obs, LazyFrames)\n        return np.array(obs)\n\n    def step(self, action):\n        next_obs, rew, done, env_info = self.env.step(action)\n        assert isinstance(next_obs, LazyFrames)\n        return np.array(next_obs), rew, done, env_info\n\n\ndef make_atari(env_id, max_episode_steps=None):\n    env = gym.make(env_id)\n    assert \'NoFrameskip\' in env.spec.id\n    env = NoopResetEnv(env, noop_max=30)\n    env = MaxAndSkipEnv(env, skip=4)\n    return env\n\n\ndef wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n    """"""\n    Configure environment for DeepMind-style Atari.\n    """"""\n    if episode_life:\n        env = EpisodicLifeEnv(env)\n    if \'FIRE\' in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env = WarpFrame(env)\n    if scale:\n        env = ScaledFloatFrame(env)\n    if clip_rewards:\n        env = ClipRewardEnv(env)\n    if frame_stack:\n        env = FrameStack(env, 4)\n    return env\n\n\n# forked from https://github.com/Shmuma/ptan/blob/master/ptan/common/wrappers.py\ndef wrap_dqn(env, stack_frames=4, episodic_life=True,\n             reward_clipping=True, wrap_ndarray=False):\n    """"""\n    Apply a common set of wrappers for Atari games.\n    """"""\n    assert \'NoFrameskip\' in env.spec.id\n    if episodic_life:\n        env = EpisodicLifeEnv(env)\n    env = NoopResetEnv(env, noop_max=30)\n    env = MaxAndSkipEnv(env, skip=4)\n    if \'FIRE\' in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env = ProcessFrame84(env)\n    env = FrameStack(env, stack_frames)\n    if reward_clipping:\n        env = ClipRewardEnv(env)\n    if wrap_ndarray:\n        env = NdarrayFrames(env)\n    return env\n'"
tf2rl/envs/env_utils.py,0,"b'from gym.spaces import Discrete, Box\n\n\ndef get_act_dim(env):\n    if isinstance(env.action_space, Discrete):\n        return 1  # env.action_space.n\n    elif isinstance(env.action_space, Box):\n        return env.action_space.low.size\n    else:\n        raise NotImplementedError\n'"
tf2rl/envs/multi_thread_env.py,6,"b'import threading\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass MultiThreadEnv(object):\n    """"""\n    This contains multiple environments.\n    When step() is called, all of them forward one-step.\n\n    This serve tensorflow operators to manipulate multiple environments.\n    """"""\n\n    def __init__(self, env_fn, batch_size, thread_pool=4, max_episode_steps=1000):\n        """"""\n        :param env_fn (function): function to make environments.\n        :param batch_size (int): batch_size\n        ;param thread_pool (int): thread pool size\n        """"""\n        assert batch_size % thread_pool == 0\n\n        self.batch_size = batch_size\n        self.thread_pool = thread_pool\n        self.batch_thread = batch_size // thread_pool\n        self.envs = [env_fn() for _ in range(batch_size)]\n\n        # collects environment information\n        sample_env = env_fn()\n        sample_obs = sample_env.reset()\n        self._sample_env = sample_env\n        self.observation_shape = sample_obs.shape\n        # episode time limit\n        self.max_episode_steps = max_episode_steps\n        if hasattr(sample_env.spec, ""max_episode_steps""):\n            self.max_episode_steps = sample_env.spec.max_episode_steps\n            print(""Use max steps of env {} instead of specified value {}"".format(\n                sample_env.spec.max_episode_steps, max_episode_steps))\n\n        self.list_obs = [None] * self.batch_size\n        self.list_rewards = [None] * self.batch_size\n        self.list_done = [None] * self.batch_size\n        self.list_steps = [0] * self.batch_size\n\n        self.py_reset()\n\n    @property\n    def original_env(self):\n        return self._sample_env\n\n    def step(self, actions, name=None):\n        """"""take 1-step in all environments.\n\n        :param tf.Tensor action: float32[batch_size, dim_action]\n        :param name: Operator\xe5\x90\x8d\n        :rtype: (tf.Tensor, tf.Tensor, tf.Tensor)\n        :return:\n           (obs, reward, done)\n          obs = [batch_size, dim_obs]\n          reward = [batch_size]\n          done = [batch_size]\n          reach_limit = [batch_size] : whether each environment reached time limit or not.\n        """"""\n        assert isinstance(actions, tf.Tensor)\n        # with tf.variable_scope(name, default_name=""MultiStep""):\n        obs, reward, done = tf.py_function(\n            func=self.py_step,\n            inp=[actions],\n            Tout=[tf.float32, tf.float32, tf.float32],\n            name=""py_step"")\n        obs.set_shape((self.batch_size,) + self.observation_shape)\n        reward.set_shape((self.batch_size,))\n        done.set_shape((self.batch_size,))\n\n        return obs, reward, done, None\n\n    def py_step(self, actions):\n        """"""\n        :param np.array actions: np.float32 [batch_size, dim_action]\n        :return:\n          (obs, reward, done)\n\n          obs = [batch_size, C, H, W]\n          reward = [batch_size]\n          done = [batch_size]\n        """"""\n        def _process(offset):\n            for idx_env in range(offset, offset+self.batch_thread):\n                new_obs, reward, done, _ = self.envs[idx_env].step(\n                    actions[idx_env].numpy())\n                self.list_obs[idx_env] = new_obs\n                self.list_rewards[idx_env] = reward\n                self.list_done[idx_env] = done\n                self.list_steps[idx_env] += 1\n\n        threads = []\n        for i in range(self.thread_pool):\n            thread = threading.Thread(\n                target=_process, args=[i*self.batch_thread])\n            thread.start()\n            threads.append(thread)\n\n        for t in threads:\n            t.join()\n\n        for i in range(self.batch_size):\n            if self.list_steps[i] == self.max_episode_steps:\n                self.list_done[i] = False\n\n        obs = np.stack(self.list_obs, axis=0)\n        reward = np.stack(self.list_rewards, axis=0).astype(np.float32)\n        done = np.stack(self.list_done, axis=0).astype(np.float32)\n\n        # TODO reset from multiple threads\n        for i in range(self.batch_size):\n            if self.list_done[i] or self.list_steps[i] == self.max_episode_steps:\n                self.list_obs[i] = self.envs[i].reset()\n                self.list_steps[i] = 0\n\n        return obs, reward, done\n\n    def py_observation(self):\n        obs = np.stack(self.list_obs, axis=0).astype(np.float32)\n        return obs\n\n    def py_reset(self):\n        for idx_env, env in enumerate(self.envs):\n            obs = env.reset()\n            # TODO: Allow flexible data type\n            self.list_obs[idx_env] = obs.astype(np.float32)\n\n        return np.stack(self.list_obs, axis=0)\n\n    @property\n    def max_action(self):\n        return float(self._sample_env.action_space.high[0])\n\n    @property\n    def min_action(self):\n        return float(self._sample_env.action_space.low[0])\n\n    @property\n    def state_dim(self):\n        return self._sample_env.observation_space.shape[0]\n'"
tf2rl/envs/normalizer.py,0,"b'""""""\nMIT License\n\nCopyright (c) 2017 Preferred Networks, Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n""""""\n# Modified by keiohta\n\nimport numpy as np\n\n\nclass EmpiricalNormalizer:\n    """"""Normalize mean and variance of values based on emprical values.\n    Args:\n        shape (int or tuple of int): Shape of input values except batch axis.\n        batch_axis (int): Batch axis.\n        eps (float): Small value for stability.\n        dtype (dtype): Dtype of input values.\n        until (int or None): If this arg is specified, the link learns input\n            values until the sum of batch sizes exceeds it.\n    """"""\n\n    def __init__(self, shape, batch_axis=0, eps=1e-2, dtype=np.float32,\n                 until=None, clip_threshold=None):\n        dtype = np.dtype(dtype)\n        self.batch_axis = batch_axis\n        self.eps = dtype.type(eps)\n        self.until = until\n        self.clip_threshold = clip_threshold\n        self._mean = np.expand_dims(np.zeros(shape, dtype=dtype), batch_axis)\n        self._var = np.expand_dims(np.ones(shape, dtype=dtype), batch_axis)\n        self.count = 0\n\n        # cache\n        self._cached_std_inverse = None\n\n    @property\n    def mean(self):\n        return np.squeeze(self._mean, self.batch_axis).copy()\n\n    @property\n    def std(self):\n        return np.sqrt(np.squeeze(self._var, self.batch_axis))\n\n    @property\n    def _std_inverse(self):\n        if self._cached_std_inverse is None:\n            self._cached_std_inverse = (self._var + self.eps) ** -0.5\n\n        return self._cached_std_inverse\n\n    def experience(self, x):\n        """"""Learn input values without computing the output values of them""""""\n\n        if self.until is not None and self.count >= self.until:\n            return\n\n        count_x = x.shape[self.batch_axis]\n        if count_x == 0:\n            return\n\n        self.count += count_x\n        rate = x.dtype.type(count_x / self.count)\n\n        mean_x = np.mean(x, axis=self.batch_axis, keepdims=True)\n        var_x = np.var(x, axis=self.batch_axis, keepdims=True)\n        delta_mean = mean_x - self._mean\n        self._mean += rate * delta_mean\n        self._var += rate * (\n            var_x - self._var\n            + delta_mean * (mean_x - self._mean)\n        )\n\n        # clear cache\n        self._cached_std_inverse = None\n\n    def __call__(self, x, update=True):\n        """"""Normalize mean and variance of values based on emprical values.\n        Args:\n            x (ndarray or Variable): Input values\n            update (bool): Flag to learn the input values\n        Returns:\n            ndarray or Variable: Normalized output values\n        """"""\n        if self.count == 0:\n            return x\n\n        mean = np.broadcast_to(self._mean, x.shape)\n        std_inv = np.broadcast_to(self._std_inverse, x.shape)\n\n        if update:\n            self.experience(x)\n\n        normalized = (x - mean) * std_inv\n        if self.clip_threshold is not None:\n            normalized = np.clip(\n                normalized, -self.clip_threshold, self.clip_threshold)\n        return normalized\n\n    def inverse(self, y):\n        mean = np.broadcast_to(self._mean, y.shape)\n        std = np.broadcast_to(np.sqrt(self._var + self.eps), y.shape)\n        return y * std + mean\n'"
tf2rl/envs/utils.py,0,"b'import gym\nfrom gym.spaces import Discrete, Box\n\n\ndef is_discrete(space):\n    if isinstance(space, Discrete):\n        return True\n    elif isinstance(space, Box):\n        return False\n    else:\n        raise NotImplementedError\n\n\ndef get_act_dim(action_space):\n    if isinstance(action_space, Discrete):\n        return action_space.n\n    elif isinstance(action_space, Box):\n        return action_space.low.size\n    else:\n        raise NotImplementedError\n\n\ndef is_mujoco_env(env):\n    from gym.envs import mujoco\n    if not hasattr(env, ""env""):\n        return False\n    return gym.envs.mujoco.mujoco_env.MujocoEnv in env.env.__class__.__bases__\n\n\ndef is_atari_env(env):\n    from gym.envs import atari\n    if not hasattr(env, ""env""):\n        return False\n    return gym.envs.atari.atari_env.AtariEnv == env.env.__class__\n'"
tf2rl/experiments/__init__.py,0,b''
tf2rl/experiments/irl_trainer.py,7,"b'import time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf2rl.misc.get_replay_buffer import get_replay_buffer\nfrom tf2rl.experiments.trainer import Trainer\n\n\nclass IRLTrainer(Trainer):\n    def __init__(\n            self,\n            policy,\n            env,\n            args,\n            irl,\n            expert_obs,\n            expert_next_obs,\n            expert_act,\n            test_env=None):\n        self._irl = irl\n        args.dir_suffix = self._irl.policy_name + args.dir_suffix\n        super().__init__(policy, env, args, test_env)\n        # TODO: Add assertion to check dimention of expert demos and current policy, env is the same\n        self._expert_obs = expert_obs\n        self._expert_next_obs = expert_next_obs\n        self._expert_act = expert_act\n        # Minus one to get next obs\n        self._random_range = range(expert_obs.shape[0])\n\n    def __call__(self):\n        total_steps = 0\n        tf.summary.experimental.set_step(total_steps)\n        episode_steps = 0\n        episode_return = 0\n        episode_start_time = time.perf_counter()\n        n_episode = 0\n\n        replay_buffer = get_replay_buffer(\n            self._policy, self._env, self._use_prioritized_rb,\n            self._use_nstep_rb, self._n_step)\n\n        obs = self._env.reset()\n\n        while total_steps < self._max_steps:\n            while total_steps < self._max_steps:\n                if total_steps < self._policy.n_warmup:\n                    action = self._env.action_space.sample()\n                else:\n                    action = self._policy.get_action(obs)\n\n                next_obs, reward, done, _ = self._env.step(action)\n                if self._show_progress:\n                    self._env.render()\n                episode_steps += 1\n                episode_return += reward\n                total_steps += 1\n                tf.summary.experimental.set_step(total_steps)\n\n                done_flag = done\n                if hasattr(self._env, ""_max_episode_steps"") and \\\n                        episode_steps == self._env._max_episode_steps:\n                    done_flag = False\n                replay_buffer.add(obs=obs, act=action,\n                                  next_obs=next_obs, rew=reward, done=done_flag)\n                obs = next_obs\n\n                if done or episode_steps == self._episode_max_steps:\n                    obs = self._env.reset()\n\n                    n_episode += 1\n                    fps = episode_steps / (time.perf_counter() - episode_start_time)\n                    self.logger.info(""Total Epi: {0: 5} Steps: {1: 7} Episode Steps: {2: 5} Return: {3: 5.4f} FPS: {4:5.2f}"".format(\n                        n_episode, int(total_steps), episode_steps, episode_return, fps))\n                    tf.summary.scalar(\n                        name=""Common/training_return"", data=episode_return)\n\n                    episode_steps = 0\n                    episode_return = 0\n                    episode_start_time = time.perf_counter()\n\n                if total_steps < self._policy.n_warmup:\n                    continue\n\n                if total_steps % self._policy.update_interval == 0:\n                    samples = replay_buffer.sample(self._policy.batch_size)\n                    # Train policy\n                    rew = self._irl.inference(samples[""obs""], samples[""act""], samples[""next_obs""])\n                    with tf.summary.record_if(total_steps % self._save_summary_interval == 0):\n                        self._policy.train(\n                            samples[""obs""], samples[""act""], samples[""next_obs""],\n                            rew, samples[""done""],\n                            None if not self._use_prioritized_rb else samples[""weights""])\n                        if self._use_prioritized_rb:\n                            td_error = self._policy.compute_td_error(\n                                samples[""obs""], samples[""act""], samples[""next_obs""],\n                                rew, samples[""done""])\n                            replay_buffer.update_priorities(\n                                samples[""indexes""], np.abs(td_error) + 1e-6)\n\n                        # Train IRL\n                        for _ in range(self._irl.n_training):\n                            samples = replay_buffer.sample(self._irl.batch_size)\n                            # Do not allow duplication!!!\n                            indices = np.random.choice(\n                                self._random_range, self._irl.batch_size, replace=False)\n                            self._irl.train(\n                                agent_states=samples[""obs""],\n                                agent_acts=samples[""act""],\n                                agent_next_states=samples[""next_obs""],\n                                expert_states=self._expert_obs[indices],\n                                expert_acts=self._expert_act[indices],\n                                expert_next_states=self._expert_next_obs[indices])\n\n                if total_steps % self._test_interval == 0:\n                    avg_test_return = self.evaluate_policy(total_steps)\n                    self.logger.info(""Evaluation Total Steps: {0: 7} Average Reward {1: 5.4f} over {2: 2} episodes"".format(\n                        total_steps, avg_test_return, self._test_episodes))\n                    tf.summary.scalar(\n                        name=""Common/average_test_return"", data=avg_test_return)\n                    tf.summary.scalar(\n                        name=""Common/fps"", data=fps)\n                    self.writer.flush()\n\n                if total_steps % self._save_model_interval == 0:\n                    self.checkpoint_manager.save()\n\n            tf.summary.flush()\n\n    @staticmethod\n    def get_argument(parser=None):\n        parser = Trainer.get_argument(parser)\n        parser.add_argument(\'--expert-path-dir\', default=None,\n                            help=\'Path to directory that contains expert trajectories\')\n        return parser\n'"
tf2rl/experiments/mpc_trainer.py,15,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\nfrom cpprb import ReplayBuffer\n\nfrom tf2rl.experiments.trainer import Trainer\nfrom tf2rl.misc.get_replay_buffer import get_space_size\n\n\nclass MLP(tf.keras.Model):\n    def __init__(self, input_dim, output_dim, units=[32, 32],\n                 name=""MLP"", gpu=0):\n        self.device = ""/gpu:{}"".format(gpu) if gpu >= 0 else ""/cpu:0""\n        super().__init__(name=name)\n\n        self.l1 = Dense(units[0], name=""L1"", activation=""relu"")\n        self.l2 = Dense(units[1], name=""L2"", activation=""relu"")\n        self.l3 = Dense(output_dim, name=""L3"", activation=""linear"")\n\n        with tf.device(self.device):\n            self(tf.constant(np.zeros(shape=(1, input_dim), dtype=np.float32)))\n\n    @tf.function\n    def call(self, inputs):\n        features = self.l1(inputs)\n        features = self.l2(features)\n        return self.l3(features)\n\n    def predict(self, inputs):\n        assert isinstance(inputs, np.ndarray)\n        if inputs.ndim == 1:\n            inputs = np.expand_dims(inputs, axis=0)\n\n        with tf.device(self.device):\n            outputs = self.call(inputs)\n\n        if inputs.shape[0] == 1:\n            return outputs.numpy()[0]\n        else:\n            return outputs.numpy()\n\n\nclass RandomPolicy:\n    def __init__(self, max_action, act_dim):\n        self._max_action = max_action\n        self._act_dim = act_dim\n        self.policy_name = ""RandomPolicy""\n\n    def get_action(self):\n        return np.random.uniform(\n            low=-self._max_action,\n            high=self._max_action,\n            size=self._act_dim)\n\n    def get_actions(self, batch_size):\n        return np.random.uniform(\n            low=-self._max_action,\n            high=self._max_action,\n            size=(batch_size, self._act_dim))\n\n\nclass MPCTrainer(Trainer):\n    def __init__(\n            self,\n            policy,\n            env,\n            args,\n            reward_fn,\n            buffer_size=int(1e6),\n            lr=0.001,\n            **kwargs):\n        super().__init__(policy, env, args, **kwargs)\n\n        # Prepare buffer that stores transitions (s, a, s\')\n        rb_dict = {\n            ""size"": buffer_size,\n            ""default_dtype"": np.float32,\n            ""env_dict"": {\n                ""obs"": {\n                    ""shape"": get_space_size(self._env.observation_space)},\n                ""next_obs"": {\n                    ""shape"": get_space_size(self._env.observation_space)},\n                ""act"": {\n                    ""shape"": get_space_size(self._env.action_space)}}}\n        self.dynamics_buffer = ReplayBuffer(**rb_dict)\n\n        # Dynamics model\n        obs_dim = self._env.observation_space.high.size\n        act_dim = self._env.action_space.high.size\n        self._dynamics_model = MLP(\n            input_dim=obs_dim + act_dim,\n            output_dim=obs_dim,\n            gpu=args.gpu)\n        self._optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n\n        # Reward function\n        self._reward_fn = reward_fn\n\n    def _set_check_point(self, model_dir):\n        # Save and restore model\n        if isinstance(self._policy, tf.keras.Model):\n            super()._set_check_point(model_dir)\n\n    def __call__(self):\n        total_steps = 0\n        tf.summary.experimental.set_step(total_steps)\n        # Gather dataset of random trajectories\n        self.logger.info(""Ramdomly collect {} samples..."".format(self._n_random_rollout * self._episode_max_steps))\n        self.collect_sample_randomly()\n\n        for i in range(self._max_iter):\n            # Train dynamics f(s, a) according to eq.(2)\n            self.fit_dynamics(i)\n\n            total_rew = 0.\n\n            # Collect new sample\n            obs = self._env.reset()\n            for _ in range(self._episode_max_steps):\n                total_steps += 1\n                act = self._mpc(obs)\n                next_obs, rew, done, _ = self._env.step(act)\n                self.dynamics_buffer.add(\n                    obs=obs, act=act, next_obs=next_obs)\n                total_rew += rew\n                if done:\n                    break\n                obs = next_obs\n            tf.summary.experimental.set_step(total_steps)\n            tf.summary.scalar(""mpc/total_rew"", total_rew)\n            self.logger.info(""iter={0: 3d} total_rew: {1:4.4f}"".format(i, total_rew))\n\n    def _mpc(self, obs):\n        init_actions = self._policy.get_actions(\n            batch_size=self._n_sample)\n        total_rewards = np.zeros(shape=(self._n_sample,))\n        obses = np.tile(obs, (self._n_sample, 1))\n\n        for i in range(self._horizon):\n            if i == 0:\n                acts = init_actions\n            else:\n                acts = self._policy.get_actions(\n                    batch_size=self._n_sample)\n            assert obses.shape[0] == acts.shape[0]\n            obs_diffs = self._dynamics_model.predict(\n                np.concatenate([obses, acts], axis=1))\n            assert obses.shape == obs_diffs.shape\n            next_obses = obses + obs_diffs\n            rewards = self._reward_fn(obses, next_obses, acts)\n            assert rewards.shape == total_rewards.shape\n            total_rewards += rewards\n            obses = next_obses\n\n        idx = np.argmax(total_rewards)\n        return init_actions[idx]\n\n    def _set_from_args(self, args):\n        super()._set_from_args(args)\n        self._max_iter = args.max_iter\n        self._horizon = args.horizon\n        self._n_sample = args.n_sample\n        self._n_random_rollout = args.n_random_rollout\n        self._batch_size = args.batch_size\n\n    def collect_sample_randomly(self):\n        for _ in range(self._n_random_rollout):\n            obs = self._env.reset()\n            for _ in range(self._episode_max_steps):\n                act = np.random.uniform(\n                    low=self._env.action_space.low,\n                    high=self._env.action_space.high,\n                    size=self._env.action_space.shape[0])\n                next_obs, _, done, _ = self._env.step(act)\n                self.dynamics_buffer.add(\n                    obs=obs, act=act, next_obs=next_obs)\n                obs = next_obs\n                if done:\n                    break\n\n    @tf.function\n    def _fit_dynamics_body(self, inputs, labels):\n        with tf.GradientTape() as tape:\n            predicts = self._dynamics_model(inputs)\n            loss = tf.reduce_mean(0.5 * tf.square(labels - predicts))\n        grads = tape.gradient(\n            loss, self._dynamics_model.trainable_variables)\n        self._optimizer.apply_gradients(\n            zip(grads, self._dynamics_model.trainable_variables))\n        return loss\n\n    def fit_dynamics(self, n_iter, n_epoch=1):\n        samples = self.dynamics_buffer.sample(\n            self.dynamics_buffer.get_stored_size())\n        inputs = np.concatenate([samples[""obs""], samples[""act""]], axis=1)\n        labels = samples[""next_obs""] - samples[""obs""]\n        dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n        dataset = dataset.batch(self._batch_size)\n        dataset = dataset.shuffle(buffer_size=1000)\n        dataset = dataset.repeat(n_epoch)\n        for batch, (x, y) in enumerate(dataset):\n            loss = self._fit_dynamics_body(x, y)\n            self.logger.debug(""batch: {} loss: {:2.6f}"".format(batch, loss))\n        tf.summary.scalar(""mpc/model_loss"", loss)\n        self.logger.info(""iter={0: 3d} loss: {1:2.8f}"".format(n_iter, loss))\n\n    @staticmethod\n    def get_argument(parser=None):\n        parser = Trainer.get_argument(parser)\n        parser.add_argument(\'--gpu\', type=int, default=0,\n                            help=\'GPU id\')\n        parser.add_argument(""--max-iter"", type=int, default=100)\n        parser.add_argument(""--horizon"", type=int, default=20)\n        parser.add_argument(""--n-sample"", type=int, default=1000)\n        parser.add_argument(""--n-random-rollout"", type=int, default=1000)\n        parser.add_argument(""--batch-size"", type=int, default=512)\n        return parser\n'"
tf2rl/experiments/on_policy_trainer.py,12,"b'import os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom cpprb import ReplayBuffer\n\nfrom tf2rl.experiments.trainer import Trainer\nfrom tf2rl.experiments.utils import save_path, frames_to_gif\nfrom tf2rl.misc.get_replay_buffer import get_replay_buffer, get_default_rb_dict\nfrom tf2rl.misc.discount_cumsum import discount_cumsum\nfrom tf2rl.envs.utils import is_discrete\n\n\nclass OnPolicyTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def __call__(self):\n        # Prepare buffer\n        self.replay_buffer = get_replay_buffer(\n            self._policy, self._env)\n        kwargs_local_buf = get_default_rb_dict(\n            size=self._policy.horizon, env=self._env)\n        kwargs_local_buf[""env_dict""][""logp""] = {}\n        kwargs_local_buf[""env_dict""][""val""] = {}\n        if is_discrete(self._env.action_space):\n            kwargs_local_buf[""env_dict""][""act""][""dtype""] = np.int32\n        self.local_buffer = ReplayBuffer(**kwargs_local_buf)\n\n        episode_steps = 0\n        episode_return = 0\n        episode_start_time = time.time()\n        total_steps = np.array(0, dtype=np.int32)\n        n_epoisode = 0\n        obs = self._env.reset()\n\n        tf.summary.experimental.set_step(total_steps)\n        while total_steps < self._max_steps:\n            # Collect samples\n            for _ in range(self._policy.horizon):\n                if self._normalize_obs:\n                    obs = self._obs_normalizer(obs, update=False)\n                act, logp, val = self._policy.get_action_and_val(obs)\n                next_obs, reward, done, _ = self._env.step(act)\n                if self._show_progress:\n                    self._env.render()\n\n                episode_steps += 1\n                total_steps += 1\n                episode_return += reward\n\n                done_flag = done\n                if hasattr(self._env, ""_max_episode_steps"") and \\\n                        episode_steps == self._env._max_episode_steps:\n                    done_flag = False\n                self.local_buffer.add(\n                    obs=obs, act=act, next_obs=next_obs,\n                    rew=reward, done=done_flag, logp=logp, val=val)\n                obs = next_obs\n\n                if done or episode_steps == self._episode_max_steps:\n                    tf.summary.experimental.set_step(total_steps)\n                    self.finish_horizon()\n                    obs = self._env.reset()\n                    n_epoisode += 1\n                    fps = episode_steps / (time.time() - episode_start_time)\n                    self.logger.info(\n                        ""Total Epi: {0: 5} Steps: {1: 7} Episode Steps: {2: 5} Return: {3: 5.4f} FPS: {4:5.2f}"".format(\n                            n_epoisode, int(total_steps), episode_steps, episode_return, fps))\n                    tf.summary.scalar(name=""Common/training_return"", data=episode_return)\n                    tf.summary.scalar(name=""Common/fps"", data=fps)\n                    episode_steps = 0\n                    episode_return = 0\n                    episode_start_time = time.time()\n\n                if total_steps % self._test_interval == 0:\n                    avg_test_return = self.evaluate_policy(total_steps)\n                    self.logger.info(""Evaluation Total Steps: {0: 7} Average Reward {1: 5.4f} over {2: 2} episodes"".format(\n                        total_steps, avg_test_return, self._test_episodes))\n                    tf.summary.scalar(\n                        name=""Common/average_test_return"", data=avg_test_return)\n                    self.writer.flush()\n\n                if total_steps % self._save_model_interval == 0:\n                    self.checkpoint_manager.save()\n\n            self.finish_horizon(last_val=val)\n\n            tf.summary.experimental.set_step(total_steps)\n\n            # Train actor critic\n            if self._policy.normalize_adv:\n                samples = self.replay_buffer._encode_sample(np.arange(self._policy.horizon))\n                mean_adv = np.mean(samples[""adv""])\n                std_adv = np.std(samples[""adv""])\n                # Update normalizer\n                if self._normalize_obs:\n                    self._obs_normalizer.experience(samples[""obs""])\n            with tf.summary.record_if(total_steps % self._save_summary_interval == 0):\n                for _ in range(self._policy.n_epoch):\n                    samples = self.replay_buffer._encode_sample(\n                        np.random.permutation(self._policy.horizon))\n                    if self._normalize_obs:\n                        samples[""obs""] = self._obs_normalizer(samples[""obs""], update=False)\n                    if self._policy.normalize_adv:\n                        adv = (samples[""adv""] - mean_adv) / (std_adv + 1e-8)\n                    else:\n                        adv = samples[""adv""]\n                    for idx in range(int(self._policy.horizon / self._policy.batch_size)):\n                        target = slice(idx * self._policy.batch_size,\n                                       (idx + 1) * self._policy.batch_size)\n                        self._policy.train(\n                            states=samples[""obs""][target],\n                            actions=samples[""act""][target],\n                            advantages=adv[target],\n                            logp_olds=samples[""logp""][target],\n                            returns=samples[""ret""][target])\n\n        tf.summary.flush()\n\n    def finish_horizon(self, last_val=0):\n        samples = self.local_buffer._encode_sample(\n            np.arange(self.local_buffer.get_stored_size()))\n        rews = np.append(samples[""rew""], last_val)\n        vals = np.append(samples[""val""], last_val)\n\n        # GAE-Lambda advantage calculation\n        deltas = rews[:-1] + self._policy.discount * vals[1:] - vals[:-1]\n        if self._policy.enable_gae:\n            advs = discount_cumsum(\n                deltas, self._policy.discount * self._policy.lam)\n        else:\n            advs = deltas\n\n        # Rewards-to-go, to be targets for the value function\n        rets = discount_cumsum(rews, self._policy.discount)[:-1]\n        self.replay_buffer.add(\n            obs=samples[""obs""], act=samples[""act""], done=samples[""done""],\n            ret=rets, adv=advs, logp=np.squeeze(samples[""logp""]))\n        self.local_buffer.clear()\n\n    def evaluate_policy(self, total_steps):\n        avg_test_return = 0.\n        if self._save_test_path:\n            replay_buffer = get_replay_buffer(\n                self._policy, self._test_env, size=self._episode_max_steps)\n        for i in range(self._test_episodes):\n            episode_return = 0.\n            frames = []\n            obs = self._test_env.reset()\n            for _ in range(self._episode_max_steps):\n                if self._normalize_obs:\n                    obs = self._obs_normalizer(obs, update=False)\n                act, _ = self._policy.get_action(obs, test=True)\n                act = act if not hasattr(self._env.action_space, ""high"") else \\\n                    np.clip(act, self._env.action_space.low, self._env.action_space.high)\n                next_obs, reward, done, _ = self._test_env.step(act)\n                if self._save_test_path:\n                    replay_buffer.add(\n                        obs=obs, act=act, next_obs=next_obs,\n                        rew=reward, done=done)\n\n                if self._save_test_movie:\n                    frames.append(self._test_env.render(mode=\'rgb_array\'))\n                elif self._show_test_progress:\n                    self._test_env.render()\n                episode_return += reward\n                obs = next_obs\n                if done:\n                    break\n            prefix = ""step_{0:08d}_epi_{1:02d}_return_{2:010.4f}"".format(\n                total_steps, i, episode_return)\n            if self._save_test_path:\n                save_path(replay_buffer.sample(self._episode_max_steps),\n                          os.path.join(self._output_dir, prefix + "".pkl""))\n                replay_buffer.clear()\n            if self._save_test_movie:\n                frames_to_gif(frames, prefix, self._output_dir)\n            avg_test_return += episode_return\n        if self._show_test_images:\n            images = tf.cast(\n                tf.expand_dims(np.array(obs).transpose(2, 0, 1), axis=3),\n                tf.uint8)\n            tf.summary.image(\'train/input_img\', images, )\n        return avg_test_return / self._test_episodes\n'"
tf2rl/experiments/trainer.py,20,"b'import os\nimport time\nimport logging\nimport argparse\n\nimport numpy as np\nimport tensorflow as tf\nfrom gym.spaces import Box\n\nfrom tf2rl.experiments.utils import save_path, frames_to_gif\nfrom tf2rl.misc.get_replay_buffer import get_replay_buffer\nfrom tf2rl.misc.prepare_output_dir import prepare_output_dir\nfrom tf2rl.misc.initialize_logger import initialize_logger\nfrom tf2rl.envs.normalizer import EmpiricalNormalizer\n\n\nif tf.config.experimental.list_physical_devices(\'GPU\'):\n    for cur_device in tf.config.experimental.list_physical_devices(""GPU""):\n        print(cur_device)\n        tf.config.experimental.set_memory_growth(cur_device, enable=True)\n\n\nclass Trainer:\n    def __init__(\n            self,\n            policy,\n            env,\n            args,\n            test_env=None):\n        self._set_from_args(args)\n        self._policy = policy\n        self._env = env\n        self._test_env = self._env if test_env is None else test_env\n        if self._normalize_obs:\n            assert isinstance(env.observation_space, Box)\n            self._obs_normalizer = EmpiricalNormalizer(\n                shape=env.observation_space.shape)\n\n        # prepare log directory\n        self._output_dir = prepare_output_dir(\n            args=args, user_specified_dir=self._logdir,\n            suffix=""{}_{}"".format(self._policy.policy_name, args.dir_suffix))\n        self.logger = initialize_logger(\n            logging_level=logging.getLevelName(args.logging_level),\n            output_dir=self._output_dir)\n\n        if args.evaluate:\n            assert args.model_dir is not None\n        self._set_check_point(args.model_dir)\n\n        # prepare TensorBoard output\n        self.writer = tf.summary.create_file_writer(self._output_dir)\n        self.writer.set_as_default()\n\n    def _set_check_point(self, model_dir):\n        # Save and restore model\n        self._checkpoint = tf.train.Checkpoint(policy=self._policy)\n        self.checkpoint_manager = tf.train.CheckpointManager(\n            self._checkpoint, directory=self._output_dir, max_to_keep=5)\n\n        if model_dir is not None:\n            assert os.path.isdir(model_dir)\n            self._latest_path_ckpt = tf.train.latest_checkpoint(model_dir)\n            self._checkpoint.restore(self._latest_path_ckpt)\n            self.logger.info(""Restored {}"".format(self._latest_path_ckpt))\n\n    def __call__(self):\n        total_steps = 0\n        tf.summary.experimental.set_step(total_steps)\n        episode_steps = 0\n        episode_return = 0\n        episode_start_time = time.perf_counter()\n        n_episode = 0\n\n        replay_buffer = get_replay_buffer(\n            self._policy, self._env, self._use_prioritized_rb,\n            self._use_nstep_rb, self._n_step)\n\n        obs = self._env.reset()\n\n        while total_steps < self._max_steps:\n            if total_steps < self._policy.n_warmup:\n                action = self._env.action_space.sample()\n            else:\n                action = self._policy.get_action(obs)\n\n            next_obs, reward, done, _ = self._env.step(action)\n            if self._show_progress:\n                self._env.render()\n            episode_steps += 1\n            episode_return += reward\n            total_steps += 1\n            tf.summary.experimental.set_step(total_steps)\n\n            done_flag = done\n            if hasattr(self._env, ""_max_episode_steps"") and \\\n                    episode_steps == self._env._max_episode_steps:\n                done_flag = False\n            replay_buffer.add(obs=obs, act=action,\n                              next_obs=next_obs, rew=reward, done=done_flag)\n            obs = next_obs\n\n            if done or episode_steps == self._episode_max_steps:\n                obs = self._env.reset()\n\n                n_episode += 1\n                fps = episode_steps / (time.perf_counter() - episode_start_time)\n                self.logger.info(""Total Epi: {0: 5} Steps: {1: 7} Episode Steps: {2: 5} Return: {3: 5.4f} FPS: {4:5.2f}"".format(\n                    n_episode, total_steps, episode_steps, episode_return, fps))\n                tf.summary.scalar(\n                    name=""Common/training_return"", data=episode_return)\n\n                episode_steps = 0\n                episode_return = 0\n                episode_start_time = time.perf_counter()\n\n            if total_steps < self._policy.n_warmup:\n                continue\n\n            if total_steps % self._policy.update_interval == 0:\n                samples = replay_buffer.sample(self._policy.batch_size)\n                with tf.summary.record_if(total_steps % self._save_summary_interval == 0):\n                    self._policy.train(\n                        samples[""obs""], samples[""act""], samples[""next_obs""],\n                        samples[""rew""], np.array(samples[""done""], dtype=np.float32),\n                        None if not self._use_prioritized_rb else samples[""weights""])\n                if self._use_prioritized_rb:\n                    td_error = self._policy.compute_td_error(\n                        samples[""obs""], samples[""act""], samples[""next_obs""],\n                        samples[""rew""], np.array(samples[""done""], dtype=np.float32))\n                    replay_buffer.update_priorities(\n                        samples[""indexes""], np.abs(td_error) + 1e-6)\n\n            if total_steps % self._test_interval == 0:\n                avg_test_return = self.evaluate_policy(total_steps)\n                self.logger.info(""Evaluation Total Steps: {0: 7} Average Reward {1: 5.4f} over {2: 2} episodes"".format(\n                    total_steps, avg_test_return, self._test_episodes))\n                tf.summary.scalar(\n                    name=""Common/average_test_return"", data=avg_test_return)\n                tf.summary.scalar(name=""Common/fps"", data=fps)\n                self.writer.flush()\n\n            if total_steps % self._save_model_interval == 0:\n                self.checkpoint_manager.save()\n\n        tf.summary.flush()\n\n    def evaluate_policy_continuously(self):\n        """"""\n        Periodically search the latest checkpoint, and keep evaluating with the latest model until user kills process.\n        """"""\n        if self._model_dir is None:\n            self.logger.error(""Please specify model directory by passing command line argument `--model-dir`"")\n            exit(-1)\n\n        self.evaluate_policy(total_steps=0)\n        while True:\n            latest_path_ckpt = tf.train.latest_checkpoint(self._model_dir)\n            if self._latest_path_ckpt != latest_path_ckpt:\n                self._latest_path_ckpt = latest_path_ckpt\n                self._checkpoint.restore(self._latest_path_ckpt)\n                self.logger.info(""Restored {}"".format(self._latest_path_ckpt))\n            self.evaluate_policy(total_steps=0)\n\n    def evaluate_policy(self, total_steps):\n        tf.summary.experimental.set_step(total_steps)\n        if self._normalize_obs:\n            self._test_env.normalizer.set_params(\n                *self._env.normalizer.get_params())\n        avg_test_return = 0.\n        if self._save_test_path:\n            replay_buffer = get_replay_buffer(\n                self._policy, self._test_env, size=self._episode_max_steps)\n        for i in range(self._test_episodes):\n            episode_return = 0.\n            frames = []\n            obs = self._test_env.reset()\n            for _ in range(self._episode_max_steps):\n                action = self._policy.get_action(obs, test=True)\n                next_obs, reward, done, _ = self._test_env.step(action)\n                if self._save_test_path:\n                    replay_buffer.add(obs=obs, act=action,\n                                      next_obs=next_obs, rew=reward, done=done)\n\n                if self._save_test_movie:\n                    frames.append(self._test_env.render(mode=\'rgb_array\'))\n                elif self._show_test_progress:\n                    self._test_env.render()\n                episode_return += reward\n                obs = next_obs\n                if done:\n                    break\n            prefix = ""step_{0:08d}_epi_{1:02d}_return_{2:010.4f}"".format(\n                total_steps, i, episode_return)\n            if self._save_test_path:\n                save_path(replay_buffer._encode_sample(np.arange(self._episode_max_steps)),\n                          os.path.join(self._output_dir, prefix + "".pkl""))\n                replay_buffer.clear()\n            if self._save_test_movie:\n                frames_to_gif(frames, prefix, self._output_dir)\n            avg_test_return += episode_return\n        if self._show_test_images:\n            images = tf.cast(\n                tf.expand_dims(np.array(obs).transpose(2, 0, 1), axis=3),\n                tf.uint8)\n            tf.summary.image(\'train/input_img\', images,)\n        return avg_test_return / self._test_episodes\n\n    def _set_from_args(self, args):\n        # experiment settings\n        self._max_steps = args.max_steps\n        self._episode_max_steps = args.episode_max_steps \\\n            if args.episode_max_steps is not None \\\n            else args.max_steps\n        self._n_experiments = args.n_experiments\n        self._show_progress = args.show_progress\n        self._save_model_interval = args.save_model_interval\n        self._save_summary_interval = args.save_summary_interval\n        self._normalize_obs = args.normalize_obs\n        self._logdir = args.logdir\n        self._model_dir = args.model_dir\n        # replay buffer\n        self._use_prioritized_rb = args.use_prioritized_rb\n        self._use_nstep_rb = args.use_nstep_rb\n        self._n_step = args.n_step\n        # test settings\n        self._test_interval = args.test_interval\n        self._show_test_progress = args.show_test_progress\n        self._test_episodes = args.test_episodes\n        self._save_test_path = args.save_test_path\n        self._save_test_movie = args.save_test_movie\n        self._show_test_images = args.show_test_images\n\n    @staticmethod\n    def get_argument(parser=None):\n        if parser is None:\n            parser = argparse.ArgumentParser(conflict_handler=\'resolve\')\n        # experiment settings\n        parser.add_argument(\'--max-steps\', type=int, default=int(1e6),\n                            help=\'Maximum number steps to interact with env.\')\n        parser.add_argument(\'--episode-max-steps\', type=int, default=int(1e3),\n                            help=\'Maximum steps in an episode\')\n        parser.add_argument(\'--n-experiments\', type=int, default=1,\n                            help=\'Number of experiments\')\n        parser.add_argument(\'--show-progress\', action=\'store_true\',\n                            help=\'Call `render` in training process\')\n        parser.add_argument(\'--save-model-interval\', type=int, default=int(1e4),\n                            help=\'Interval to save model\')\n        parser.add_argument(\'--save-summary-interval\', type=int, default=int(1e3),\n                            help=\'Interval to save summary\')\n        parser.add_argument(\'--model-dir\', type=str, default=None,\n                            help=\'Directory to restore model\')\n        parser.add_argument(\'--dir-suffix\', type=str, default=\'\',\n                            help=\'Suffix for directory that contains results\')\n        parser.add_argument(\'--normalize-obs\', action=\'store_true\',\n                            help=\'Normalize observation\')\n        parser.add_argument(\'--logdir\', type=str, default=\'results\',\n                            help=\'Output directory\')\n        # test settings\n        parser.add_argument(\'--evaluate\', action=\'store_true\',\n                            help=\'Evaluate trained model\')\n        parser.add_argument(\'--test-interval\', type=int, default=int(1e4),\n                            help=\'Interval to evaluate trained model\')\n        parser.add_argument(\'--show-test-progress\', action=\'store_true\',\n                            help=\'Call `render` in evaluation process\')\n        parser.add_argument(\'--test-episodes\', type=int, default=5,\n                            help=\'Number of episodes to evaluate at once\')\n        parser.add_argument(\'--save-test-path\', action=\'store_true\',\n                            help=\'Save trajectories of evaluation\')\n        parser.add_argument(\'--show-test-images\', action=\'store_true\',\n                            help=\'Show input images to neural networks when an episode finishes\')\n        parser.add_argument(\'--save-test-movie\', action=\'store_true\',\n                            help=\'Save rendering results\')\n        # replay buffer\n        parser.add_argument(\'--use-prioritized-rb\', action=\'store_true\',\n                            help=\'Flag to use prioritized experience replay\')\n        parser.add_argument(\'--use-nstep-rb\', action=\'store_true\',\n                            help=\'Flag to use nstep experience replay\')\n        parser.add_argument(\'--n-step\', type=int, default=4,\n                            help=\'Number of steps to look over\')\n        # others\n        parser.add_argument(\'--logging-level\', choices=[\'DEBUG\', \'INFO\', \'WARNING\'],\n                            default=\'INFO\', help=\'Logging level\')\n        return parser\n'"
tf2rl/experiments/utils.py,0,"b'import os\nimport random\n\nimport numpy as np\nimport joblib\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nimport tensorflow as tf\n\n\ndef save_path(samples, filename):\n    joblib.dump(samples, filename, compress=3)\n\n\ndef restore_latest_n_traj(dirname, n_path=10, max_steps=None):\n    assert os.path.isdir(dirname)\n    filenames = get_filenames(dirname, n_path)\n    return load_trajectories(filenames, None)\n\n\ndef get_filenames(dirname, n_path=None):\n    import re\n    itr_reg = re.compile(\n        r""step_(?P<step>[0-9]+)_epi_(?P<episodes>[0-9]+)_return_(-?)(?P<return_u>[0-9]+).(?P<return_l>[0-9]+).pkl"")\n\n    itr_files = []\n    for _, filename in enumerate(os.listdir(dirname)):\n        m = itr_reg.match(filename)\n        if m:\n            itr_count = m.group(\'step\')\n            itr_files.append((itr_count, filename))\n\n    n_path = n_path if n_path is not None else len(itr_files)\n    itr_files = sorted(itr_files, key=lambda x: int(\n        x[0]), reverse=True)[:n_path]\n    filenames = []\n    for itr_file_and_count in itr_files:\n        filenames.append(os.path.join(dirname, itr_file_and_count[1]))\n    return filenames\n\n\ndef load_trajectories(filenames, max_steps=None):\n    assert len(filenames) > 0\n    paths = []\n    for filename in filenames:\n        paths.append(joblib.load(filename))\n\n    def get_obs_and_act(path):\n        obses = path[\'obs\'][:-1]\n        next_obses = path[\'obs\'][1:]\n        actions = path[\'act\'][:-1]\n        if max_steps is not None:\n            return obses[:max_steps], next_obses[:max_steps], actions[:max_steps-1]\n        else:\n            return obses, next_obses, actions\n\n    for i, path in enumerate(paths):\n        if i == 0:\n            obses, next_obses, acts = get_obs_and_act(path)\n        else:\n            obs, next_obs, act = get_obs_and_act(path)\n            obses = np.vstack((obs, obses))\n            next_obses = np.vstack((next_obs, next_obses))\n            acts = np.vstack((act, acts))\n    return {\'obses\': obses, \'next_obses\': next_obses, \'acts\': acts}\n\n\ndef frames_to_gif(frames, prefix, save_dir, interval=50, fps=30):\n    """"""\n    Convert frames to gif file\n    """"""\n    assert len(frames) > 0\n    plt.figure(figsize=(frames[0].shape[1] / 72.,\n                        frames[0].shape[0] / 72.), dpi=72)\n    patch = plt.imshow(frames[0])\n    plt.axis(\'off\')\n\n    def animate(i):\n        patch.set_data(frames[i])\n\n    # TODO: interval should be 1000 / fps ?\n    anim = animation.FuncAnimation(\n        plt.gcf(), animate, frames=len(frames), interval=interval)\n    output_path = ""{}/{}.gif"".format(save_dir, prefix)\n    anim.save(output_path, writer=\'imagemagick\', fps=fps)\n'"
tf2rl/misc/__init__.py,0,b''
tf2rl/misc/discount_cumsum.py,1,"b'from scipy.signal import lfilter\n\n\ndef discount_cumsum(x, discount):\n    """"""\n    Forked from rllab for computing discounted cumulative sums of vectors.\n\n    :param x (np.ndarray or tf.Tensor)\n        vector of [x0, x1, x2]\n    :return output:\n        [x0 + discount * x1 + discount^2 * x2,  \n         x1 + discount * x2,\n         x2]\n    """"""\n    return lfilter(\n        b=[1],\n        a=[1, float(-discount)],\n        x=x[::-1],\n        axis=0)[::-1]\n'"
tf2rl/misc/get_replay_buffer.py,0,"b'import numpy as np\nfrom gym.spaces.box import Box\nfrom gym.spaces.discrete import Discrete\nfrom gym.spaces.dict import Dict\n\nfrom cpprb import ReplayBuffer, PrioritizedReplayBuffer\n\nfrom tf2rl.algos.policy_base import OffPolicyAgent\nfrom tf2rl.envs.utils import is_discrete\n\n\ndef get_space_size(space):\n    if isinstance(space, Box):\n        return space.shape\n    elif isinstance(space, Discrete):\n        return [1, ]  # space.n\n    else:\n        raise NotImplementedError(""Assuming to use Box or Discrete, not {}"".format(type(space)))\n\n\ndef get_default_rb_dict(size, env):\n    return {\n        ""size"": size,\n        ""default_dtype"": np.float32,\n        ""env_dict"": {\n            ""obs"": {\n                ""shape"": get_space_size(env.observation_space)},\n            ""next_obs"": {\n                ""shape"": get_space_size(env.observation_space)},\n            ""act"": {\n                ""shape"": get_space_size(env.action_space)},\n            ""rew"": {},\n            ""done"": {}}}\n\n\ndef get_replay_buffer(policy, env, use_prioritized_rb=False,\n                      use_nstep_rb=False, n_step=1, size=None):\n    if policy is None or env is None:\n        return None\n\n    obs_shape = get_space_size(env.observation_space)\n    kwargs = get_default_rb_dict(policy.memory_capacity, env)\n\n    if size is not None:\n        kwargs[""size""] = size\n\n    # on-policy policy\n    if not issubclass(type(policy), OffPolicyAgent):\n        kwargs[""size""] = policy.horizon\n        kwargs[""env_dict""].pop(""next_obs"")\n        kwargs[""env_dict""].pop(""rew"")\n        # TODO: Remove done. Currently cannot remove because of cpprb implementation\n        # kwargs[""env_dict""].pop(""done"")\n        kwargs[""env_dict""][""logp""] = {}\n        kwargs[""env_dict""][""ret""] = {}\n        kwargs[""env_dict""][""adv""] = {}\n        if is_discrete(env.action_space):\n            kwargs[""env_dict""][""act""][""dtype""] = np.int32\n        return ReplayBuffer(**kwargs)\n\n    # N-step prioritized\n    if use_prioritized_rb and use_nstep_rb:\n        kwargs[""Nstep""] = {""size"": n_step,\n                           ""gamma"": policy.discount,\n                           ""rew"": ""rew"",\n                           ""next"": ""next_obs""}\n        return PrioritizedReplayBuffer(**kwargs)\n\n    if len(obs_shape) == 3:\n        kwargs[""env_dict""][""obs""][""dtype""] = np.ubyte\n        kwargs[""env_dict""][""next_obs""][""dtype""] = np.ubyte\n\n    # prioritized\n    if use_prioritized_rb:\n        return PrioritizedReplayBuffer(**kwargs)\n\n    # N-step\n    if use_nstep_rb:\n        kwargs[""Nstep""] = {""size"": n_step,\n                           ""gamma"": policy.discount,\n                           ""rew"": ""rew"",\n                           ""next"": ""next_obs""}\n        return ReplayBuffer(**kwargs)\n\n    return ReplayBuffer(**kwargs)\n'"
tf2rl/misc/huber_loss.py,7,"b'import numpy as np\nimport tensorflow as tf\n\n\ndef huber_loss(x, delta=1.):\n    """"""\n    Compute the huber loss.\n    https://en.wikipedia.org/wiki/Huber_loss\n\n    :param x (np.ndarray or tf.Tensor): Values to compute the huber loss.\n    :param delta (float): Positive floating point value. Represents the\n                          maximum possible gradient magnitude.\n    :return (tf.Tensor): The huber loss.\n    """"""\n    delta = tf.ones_like(x) * delta\n    less_than_max = 0.5 * tf.square(x)\n    greater_than_max = delta * (tf.abs(x) - 0.5 * delta)\n    return tf.where(\n        tf.abs(x) <= delta,\n        x=less_than_max,\n        y=greater_than_max)\n'"
tf2rl/misc/initialize_logger.py,0,"b'import os\nimport logging\nfrom logging import getLogger, StreamHandler, FileHandler, Formatter\nimport datetime\n\n\ndef initialize_logger(logging_level=logging.INFO, output_dir=""results/"", filename=None, save_log=True):\n    logger = getLogger(""tf2rl"")\n    logger.setLevel(logging_level)\n\n    handler_format = Formatter(\n        \'%(asctime)s.%(msecs)03d [%(levelname)s] (%(filename)s:%(lineno)s) %(message)s\',\n        ""%H:%M:%S"")  # If you want to show date, add %Y-%m-%d\n    stream_handler = StreamHandler()\n    stream_handler.setLevel(logging_level)\n    stream_handler.setFormatter(handler_format)\n\n    if save_log:\n        if filename is not None:\n            filename = filename\n        else:\n            if not os.path.exists(output_dir):\n                os.mkdir(output_dir)\n            filename = os.path.join(\n                output_dir, datetime.datetime.now().strftime(""%Y%m%dT%H%M%S.%f"") + \'.log\')\n\n        file_handler = FileHandler(filename, \'a\')\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(handler_format)\n\n    if len(logger.handlers) == 0:\n        logger.addHandler(stream_handler)\n        if save_log:\n            logger.addHandler(file_handler)\n    else:\n        # Overwrite logging setting\n        logger.handlers[0] = stream_handler\n        if save_log:\n            logger.handlers[1] = file_handler\n\n    logger.propagate = False\n\n    return logger\n'"
tf2rl/misc/normalizer.py,10,"b'import numpy as np\nimport tensorflow as tf\n\n\nclass Normalizer:\n    """"""\n    Normalize input data online. This is based on following:\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm\n    """"""\n\n    def __init__(self, mean_only=False):\n        self._mean_only = mean_only\n        self._n = tf.Variable(0, dtype=tf.float32)\n        self._mean = tf.Variable(0, dtype=tf.float32)\n        self._mean_diff = tf.Variable(0, dtype=tf.float32)\n        if not self._mean_only:\n            self._var = tf.Variable(0, dtype=tf.float32)\n\n    @tf.function\n    def observe(self, x):\n        """"""\n        Compute next mean and std\n\n        :param x (float): Input data\n        """"""\n        self._n.assign_add(1)\n        numerator = x - self._mean\n        self._mean.assign_add((x - self._mean) / self._n)\n        self._mean_diff.assign_add(numerator * (x - self._mean))\n        if not self._mean_only:\n            self._var = tf.clip_by_value(\n                tf.math.divide_no_nan(self._mean_diff, self._n), 1e-2, 1e+2)\n\n    @tf.function\n    def normalize(self, x):\n        std = tf.math.sqrt(self._var)\n        return tf.math.divide_no_nan(x - self._mean, std)\n\n\nclass NormalizerNumpy:\n    def __init__(self):\n        self._n = 0\n        self._mean = 0\n        self._mean_diff = 0\n        self._var = 0\n\n    def observe(self, x):\n        self._n += 1\n        numerator = x - self._mean\n        self._mean += (x - self._mean) / self._n\n        self._mean_diff += numerator * (x - self._mean)\n        self._var = self._mean_diff / self._n\n\n    def normalize(self, x, update=False):\n        if update:\n            self.observe(x)\n        return (x - self._mean) / (np.sqrt(self._var) + 1e-8)\n\n    def get_params(self):\n        return self._n, self._mean, self._mean_diff, self._var\n\n    def set_params(self, n, mean, mean_diff, var):\n        self._n = n\n        self._mean = mean\n        self._mean_diff = mean_diff\n        self._var = var\n'"
tf2rl/misc/periodic_ops.py,10,"b'# Copyright 2018 The trfl Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or    implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""\nPeriodic execution ops.\n\nIt is very common in Reinforcement Learning for certain ops to only need to be\nexecuted periodically, for example: once every N agent steps. The ops below\nsupport this common use-case by wrapping a subgraph as a periodic op that only\nactually executes the underlying computation once every N evaluations of the op,\nbehaving as a no-op in all other calls.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef periodically(body, period, name=""periodically""):\n    """"""\n    Periodically performs a tensorflow op.\n\n    The body tensorflow op will be executed every `period` times the periodically\n    op is executed. More specifically, with `n` the number of times the op has\n    been executed, the body will be executed when `n` is a non zero positive\n    multiple of `period` (i.e. there exist an integer `k > 0` such that\n    `k * period == n`).\n\n    If `period` is 0 or `None`, it would not perform any op and would return a\n    `tf.no_op()`.\n\n    :param body (callable): callable that returns the tensorflow op to be performed every time\n        an internal counter is divisible by the period. The op must have no\n        output (for example, a tf.group()).\n    :param period (int): inverse frequency with which to perform the op.\n    :param name (str): name of the variable_scope.\n    :raise TypeError: if body is not a callable.\n    :raise ValueError: if period is negative.\n    :return: An op that periodically performs the specified op.\n    """"""\n    if not callable(body):\n        raise TypeError(""body must be callable."")\n\n    if period is None or period == 0:\n        return tf.no_op()\n\n    if period < 0:\n        raise ValueError(""period cannot be less than 0."")\n\n    if period == 1:\n        return body()\n\n    with tf.variable_scope(None, default_name=name):\n        counter = tf.get_variable(\n            ""counter"",\n            shape=[],\n            dtype=tf.int64,\n            trainable=False,\n            initializer=tf.constant_initializer(period, dtype=tf.int64))\n\n        def _wrapped_body():\n            with tf.control_dependencies([body()]):\n                # Done the deed, resets the counter.\n                return counter.assign(1)\n\n        update = tf.cond(\n            tf.equal(counter, period), _wrapped_body, lambda: counter.assign_add(1))\n\n    return update\n'"
tf2rl/misc/prepare_output_dir.py,0,"b'""""""\nMIT License\n\nCopyright (c) 2017 Preferred Networks, Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n""""""\n\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\nfrom future import standard_library\nstandard_library.install_aliases()  # NOQA\n\nimport argparse\nimport datetime\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\n\n\ndef is_return_code_zero(args):\n    """"""\n    Return true if the given command\'s return code is zero.\n    All the messages to stdout or stderr are suppressed.\n    forked from https://github.com/chainer/chainerrl/blob/master/chainerrl/misc/is_return_code_zero.py\n    """"""\n    with open(os.devnull, \'wb\') as FNULL:\n        try:\n            subprocess.check_call(args, stdout=FNULL, stderr=FNULL)\n        except subprocess.CalledProcessError:\n            # The given command returned an error\n            return False\n        except OSError:\n            # The given command was not found\n            return False\n        return True\n\n\ndef is_under_git_control():\n    """"""\n    Return true iff the current directory is under git control.\n    """"""\n    return is_return_code_zero([\'git\', \'rev-parse\'])\n\n\ndef prepare_output_dir(args, user_specified_dir=None, argv=None,\n                       time_format=\'%Y%m%dT%H%M%S.%f\', suffix=""""):\n    """"""\n    Prepare a directory for outputting training results.\n    An output directory, which ends with the current datetime string,\n    is created. Then the following infomation is saved into the directory:\n        args.txt: command line arguments\n        command.txt: command itself\n        environ.txt: environmental variables\n    Additionally, if the current directory is under git control, the following\n    information is saved:\n        git-head.txt: result of `git rev-parse HEAD`\n        git-status.txt: result of `git status`\n        git-log.txt: result of `git log`\n        git-diff.txt: result of `git diff`\n\n    :param args (dict or argparse.Namespace): Arguments to save\n    :param user_specified_dir (str or None): If str is specified, the output\n        directory is created under that path. If not specified, it is\n        created as a new temporary directory instead.\n    :param argv (list or None): The list of command line arguments passed to a\n        script. If not specified, sys.argv is used instead.\n    :param time_format (str): Format used to represent the current datetime. The\n    :param default format is the basic format of ISO 8601.\n    :return: Path of the output directory created by this function (str).\n    """"""\n    if suffix is not """":\n        suffix = ""_"" + suffix\n    time_str = datetime.datetime.now().strftime(time_format) + suffix\n    if user_specified_dir is not None:\n        if os.path.exists(user_specified_dir):\n            if not os.path.isdir(user_specified_dir):\n                raise RuntimeError(\n                    \'{} is not a directory\'.format(user_specified_dir))\n        outdir = os.path.join(user_specified_dir, time_str)\n        if os.path.exists(outdir):\n            raise RuntimeError(\'{} exists\'.format(outdir))\n        else:\n            os.makedirs(outdir)\n    else:\n        outdir = tempfile.mkdtemp(prefix=time_str)\n\n    # Save all the arguments\n    with open(os.path.join(outdir, \'args.txt\'), \'w\') as f:\n        if isinstance(args, argparse.Namespace):\n            args = vars(args)\n        f.write(json.dumps(args))\n\n    # Save all the environment variables\n    with open(os.path.join(outdir, \'environ.txt\'), \'w\') as f:\n        f.write(json.dumps(dict(os.environ)))\n\n    # Save the command\n    with open(os.path.join(outdir, \'command.txt\'), \'w\') as f:\n        f.write(\' \'.join(sys.argv))\n\n    if is_under_git_control():\n        # Save `git rev-parse HEAD` (SHA of the current commit)\n        with open(os.path.join(outdir, \'git-head.txt\'), \'wb\') as f:\n            f.write(subprocess.check_output(\'git rev-parse HEAD\'.split()))\n\n        # Save `git status`\n        with open(os.path.join(outdir, \'git-status.txt\'), \'wb\') as f:\n            f.write(subprocess.check_output(\'git status\'.split()))\n\n        # Save `git log`\n        with open(os.path.join(outdir, \'git-log.txt\'), \'wb\') as f:\n            f.write(subprocess.check_output(\'git log\'.split()))\n\n        # Save `git diff`\n        with open(os.path.join(outdir, \'git-diff.txt\'), \'wb\') as f:\n            f.write(subprocess.check_output(\'git diff\'.split()))\n\n    return outdir\n'"
tf2rl/misc/target_update_ops.py,5,"b'# Copyright 2018 The trfl Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or    implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""\nTensorflow ops for updating target networks.\n\nTensorflow ops that are used to update a target network from a source network.\nThis is used in agents such as DQN or DPG, which use a target network that\nchanges more slowly than the online network, in order to improve stability.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tf2rl.misc import periodic_ops\n\n\ndef update_target_variables(target_variables,\n                            source_variables,\n                            tau=1.0,\n                            use_locking=False,\n                            name=""update_target_variables""):\n    """"""\n    Returns an op to update a list of target variables from source variables.\n\n    The update rule is:\n    `target_variable = (1 - tau) * target_variable + tau * source_variable`.\n\n    :param target_variables: a list of the variables to be updated.\n    :param source_variables: a list of the variables used for the update.\n    :param tau: weight used to gate the update. The permitted range is 0 < tau <= 1,\n        with small tau representing an incremental update, and tau == 1\n        representing a full update (that is, a straight copy).\n    :param use_locking: use `tf.Variable.assign`\'s locking option when assigning\n        source variable values to target variables.\n    :param name: sets the `name_scope` for this op.\n    :raise TypeError: when tau is not a Python float\n    :raise ValueError: when tau is out of range, or the source and target variables\n        have different numbers or shapes.\n    :return: An op that executes all the variable updates.\n    """"""\n    if not isinstance(tau, float):\n        raise TypeError(""Tau has wrong type (should be float) {}"".format(tau))\n    if not 0.0 < tau <= 1.0:\n        raise ValueError(""Invalid parameter tau {}"".format(tau))\n    if len(target_variables) != len(source_variables):\n        raise ValueError(""Number of target variables {} is not the same as ""\n                         ""number of source variables {}"".format(\n                             len(target_variables), len(source_variables)))\n\n    same_shape = all(trg.get_shape() == src.get_shape()\n                     for trg, src in zip(target_variables, source_variables))\n    if not same_shape:\n        raise ValueError(""Target variables don\'t have the same shape as source ""\n                         ""variables."")\n\n    def update_op(target_variable, source_variable, tau):\n        if tau == 1.0:\n            return target_variable.assign(source_variable, use_locking)\n        else:\n            return target_variable.assign(\n                tau * source_variable + (1.0 - tau) * target_variable, use_locking)\n\n    # with tf.name_scope(name, values=target_variables + source_variables):\n    update_ops = [update_op(target_var, source_var, tau)\n                  for target_var, source_var\n                  in zip(target_variables, source_variables)]\n    return tf.group(name=""update_all_variables"", *update_ops)\n\n\ndef periodic_target_update(target_variables,\n                           source_variables,\n                           update_period,\n                           tau=1.0,\n                           use_locking=False,\n                           name=""periodic_target_update""):\n    """"""\n    Returns an op to periodically update a list of target variables.\n\n    The `update_target_variables` op is executed every `update_period`\n    executions of the `periodic_target_update` op.\n\n    The update rule is:\n    `target_variable = (1 - tau) * target_variable + tau * source_variable`.\n\n    :param target_variables: a list of the variables to be updated.\n    :param source_variables: a list of the variables used for the update.\n    :param update_period: inverse frequency with which to apply the update.\n    :param tau: weight used to gate the update. The permitted range is 0 < tau <= 1,\n        with small tau representing an incremental update, and tau == 1\n        representing a full update (that is, a straight copy).\n    :param use_locking: use `tf.variable.Assign`\'s locking option when assigning\n        source variable values to target variables.\n    :param name: sets the `name_scope` for this op.\n    :return: An op that periodically updates `target_variables` with `source_variables`.\n    """"""\n\n    def update_op():\n        return update_target_variables(\n            target_variables, source_variables, tau, use_locking)\n\n    with tf.name_scope(name, values=target_variables + source_variables):\n        return periodic_ops.periodically(update_op, update_period)\n'"
tf2rl/networks/__init__.py,0,b''
tf2rl/networks/atari_model.py,19,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten\n\nfrom tf2rl.networks.noisy_dense import NoisyDense\nfrom tf2rl.policies.categorical_actor import CategoricalActorCritic, CategoricalActor\nfrom tf2rl.distributions.categorical import Categorical\n\n\nclass AtariBaseModel(tf.keras.Model):\n    def __init__(self, name, enable_noisy_dqn=False):\n        super().__init__(name=name)\n\n        DenseLayer = NoisyDense if enable_noisy_dqn else Dense\n\n        self.conv1 = Conv2D(32, kernel_size=(8, 8), strides=(4, 4),\n                            padding=\'valid\', activation=\'relu\')\n        self.conv2 = Conv2D(64, kernel_size=(4, 4), strides=(2, 2),\n                            padding=\'valid\', activation=\'relu\')\n        self.conv3 = Conv2D(64, kernel_size=(3, 3), strides=(1, 1),\n                            padding=\'valid\', activation=\'relu\')\n        self.flat = Flatten()\n        self.fc1 = DenseLayer(512, activation=\'relu\')\n\n    def call(self, inputs):\n        # TODO: This type conversion seems to be bottle neck\n        features = tf.divide(tf.cast(inputs, tf.float32),\n                             tf.constant(255.))\n        features = self.conv1(features)\n        features = self.conv2(features)\n        features = self.conv3(features)\n        features = self.flat(features)\n        features = self.fc1(features)\n        return features\n\n\nclass AtariQFunc(AtariBaseModel):\n    def __init__(self, state_shape, action_dim, units=None,\n                 name=""QFunc"", enable_dueling_dqn=False,\n                 enable_noisy_dqn=False, enable_categorical_dqn=False,\n                 n_atoms=51):\n        self._enable_dueling_dqn = enable_dueling_dqn\n        self._enable_noisy_dqn = enable_noisy_dqn\n        self._enable_categorical_dqn = enable_categorical_dqn\n        if enable_categorical_dqn:\n            self._action_dim = action_dim\n            self._n_atoms = n_atoms\n            action_dim = (action_dim + int(enable_dueling_dqn)) * n_atoms\n\n        # Build base layers\n        super().__init__(name, enable_noisy_dqn)\n        DenseLayer = NoisyDense if enable_noisy_dqn else Dense\n\n        self.fc2 = DenseLayer(action_dim, activation=\'linear\')\n\n        if self._enable_dueling_dqn and not enable_categorical_dqn:\n            self.fc3 = DenseLayer(1, activation=\'linear\')\n\n        input_shape = (1,) + state_shape\n        with tf.device(""/cpu:0""):\n            self(inputs=tf.constant(np.zeros(shape=input_shape,\n                                             dtype=np.float32)))\n\n    def call(self, inputs):\n        # Extract features on base layers\n        features = super().call(inputs)\n\n        if self._enable_categorical_dqn:\n            features = self.fc2(features)\n            if self._enable_dueling_dqn:\n                features = tf.reshape(\n                    features, (-1, self._action_dim+1, self._n_atoms))  # [batch_size, action_dim, n_atoms]\n                v_values = tf.reshape(\n                    features[:, 0], (-1, 1, self._n_atoms))\n                advantages = tf.reshape(\n                    features[:, 1:], [-1, self._action_dim, self._n_atoms])\n                features = v_values + (advantages - tf.expand_dims(\n                    tf.reduce_mean(advantages, axis=1), axis=1))\n            else:\n                features = tf.reshape(\n                    features, (-1, self._action_dim, self._n_atoms))  # [batch_size, action_dim, n_atoms]\n            # [batch_size, action_dim, n_atoms]\n            q_dist = tf.keras.activations.softmax(features, axis=2)\n            return tf.clip_by_value(q_dist, 1e-8, 1.0-1e-8)\n        else:\n            if self._enable_dueling_dqn:\n                advantages = self.fc2(features)\n                v_values = self.fc3(features)\n                q_values = v_values + \\\n                    (advantages - tf.reduce_mean(advantages, axis=1, keepdims=True))\n            else:\n                q_values = self.fc2(features)\n            return q_values\n\n\nclass AtariCategoricalActor(CategoricalActor, AtariBaseModel):\n    def __init__(self, state_shape, action_dim, units=None,\n                 name=""AtariCategoricalActor""):\n        self.dist = Categorical(dim=action_dim)\n        self.action_dim = action_dim\n\n        # Build base layers\n        AtariBaseModel.__init__(self, name, state_shape)\n\n        # Build top layer\n        self.prob = Dense(action_dim, activation=\'softmax\')\n\n        self(tf.constant(\n            np.zeros(shape=(1,)+state_shape, dtype=np.uint8)))\n\n    def _compute_feature(self, states):\n        # Extract features on base layers\n        return AtariBaseModel.call(self, states)\n\n\nclass AtariCategoricalActorCritic(CategoricalActorCritic):\n    def __init__(self, state_shape, action_dim, units=None,\n                 name=""AtariCategoricalActorCritic""):\n        tf.keras.Model.__init__(self, name=name)\n        self.dist = Categorical(dim=action_dim)\n        self.action_dim = action_dim\n\n        self.conv1 = Conv2D(32, kernel_size=(8, 8), strides=(4, 4),\n                            padding=\'valid\', activation=\'relu\')\n        self.conv2 = Conv2D(64, kernel_size=(4, 4), strides=(2, 2),\n                            padding=\'valid\', activation=\'relu\')\n        self.conv3 = Conv2D(64, kernel_size=(3, 3), strides=(1, 1),\n                            padding=\'valid\', activation=\'relu\')\n        self.flat = Flatten()\n        self.fc1 = Dense(512, activation=\'relu\')\n        self.prob = Dense(action_dim, activation=\'softmax\')\n        self.v = Dense(1, activation=""linear"")\n\n        self(tf.constant(\n            np.zeros(shape=(1,)+state_shape, dtype=np.float32)))\n\n    def _compute_feature(self, states):\n        features = tf.divide(tf.cast(states, tf.float32),\n                             tf.constant(255.))\n        features = self.conv1(features)\n        features = self.conv2(features)\n        features = self.conv3(features)\n        features = self.flat(features)\n        features = self.fc1(features)\n        return features\n'"
tf2rl/networks/noisy_dense.py,15,"b""# This is an implementation of noisy linear layer defined in following paper:\n# Noisy Networks for Exploration, https://arxiv.org/abs/1706.10295\n# Forked from https://github.com/LuEE-C/Noisy-A3C-Keras/blob/master/NoisyDense.py\n# Fixed a bug by Kei Ohta\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import backend as K\n\n\nclass NoisyDense(tf.keras.layers.Layer):\n    def __init__(\n            self,\n            units,\n            sigma_init=0.017,\n            activation=None,\n            use_bias=True,\n            kernel_initializer='glorot_uniform',\n            bias_initializer='zeros',\n            kernel_regularizer=None,\n            bias_regularizer=None,\n            activity_regularizer=None,\n            kernel_constraint=None,\n            bias_constraint=None,\n            trainable=True,\n            **kwargs):\n        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n        super().__init__(**kwargs)\n        self.units = int(units)\n        self.sigma_init = sigma_init\n        self.activation = tf.keras.activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n        self.activity_regularizer = tf.keras.regularizers.get(\n            activity_regularizer)\n        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n        self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n        self.trainable = trainable\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        self.input_dim = input_shape[-1]\n        self.kernel_shape = tf.constant((self.input_dim, self.units))\n        self.bias_shape = tf.constant((self.units,))\n\n        self.kernel = self.add_weight(\n            shape=[self.input_dim, self.units],\n            initializer=tf.keras.initializers.Orthogonal(),\n            name='kernel',\n            dtype=tf.float32,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n            trainable=self.trainable)\n\n        self.sigma_kernel = self.add_weight(\n            shape=(self.input_dim, self.units),\n            initializer=tf.keras.initializers.Constant(value=self.sigma_init),\n            name='sigma_kernel',\n            trainable=self.trainable)\n\n        if self.use_bias:\n            self.bias = self.add_weight(\n                shape=(self.units,),\n                initializer=self.bias_initializer,\n                name='bias',\n                regularizer=self.bias_regularizer,\n                constraint=self.bias_constraint,\n                trainable=self.trainable)\n\n            self.sigma_bias = self.add_weight(\n                shape=(self.units,),\n                initializer=tf.keras.initializers.Constant(\n                    value=self.sigma_init),\n                name='sigma_bias',\n                trainable=self.trainable)\n\n        else:\n            self.bias = None\n            self.epsilon_bias = None\n\n        super().build(input_shape)\n\n    def call(self, inputs):\n        # Implement Eq.(9)\n        perturbed_kernel = self.kernel + \\\n            self.sigma_kernel * K.random_uniform(shape=self.kernel_shape)\n        outputs = K.dot(inputs, perturbed_kernel)\n        if self.use_bias:\n            perturbed_bias = self.bias + \\\n                self.sigma_bias * K.random_uniform(shape=self.bias_shape)\n            outputs = K.bias_add(outputs, perturbed_bias)\n        if self.activation is not None:\n            outputs = self.activation(outputs)\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.units)\n"""
tf2rl/networks/spectral_norm_dense.py,7,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import common_shapes\nfrom tensorflow.python.ops import gen_math_ops\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import nn\n\n\nclass SNDense(Dense):\n    def __init__(\n            self,\n            units,\n            trainable=True,\n            u_kernel_initializer=tf.keras.initializers.TruncatedNormal,\n            **kwargs):\n        super().__init__(units, **kwargs)\n        self.trainable = trainable\n        self.u_kernel_initializer = u_kernel_initializer\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        super().build(input_shape)\n        self.u_kernel = self.add_weight(\n            shape=(1, self.units),\n            initializer=self.u_kernel_initializer(),\n            name=\'u_kernel\',\n            trainable=False)\n\n    def compute_spectral_norm(self):\n        u_hat = self.u_kernel\n\n        def l2_norm(val, eps=1e-12):\n            return val / (tf.reduce_sum(val ** 2) ** 0.5 + eps)\n\n        v_ = tf.matmul(u_hat, tf.transpose(self.kernel))\n        v_hat = l2_norm(v_)\n        u_ = tf.matmul(v_hat, self.kernel)\n        u_hat = l2_norm(u_)\n\n        sigma = tf.matmul(tf.matmul(v_hat, self.kernel), tf.transpose(u_hat))\n        w_norm = self.kernel / sigma\n        with tf.control_dependencies([self.u_kernel.assign(u_hat)]):\n            w_norm = tf.reshape(w_norm, self.kernel.shape.as_list())\n        return w_norm\n\n    def call(self, inputs):\n        w = self.compute_spectral_norm()\n        inputs = ops.convert_to_tensor(inputs, dtype=self.dtype)\n        rank = common_shapes.rank(inputs)\n        if rank > 2:\n            # Broadcasting is required for the inputs.\n            outputs = standard_ops.tensordot(inputs, w, [[rank - 1], [0]])\n            # Reshape the output back to the original ndim of the input.\n            if not context.executing_eagerly():\n                shape = inputs.get_shape().as_list()\n                output_shape = shape[:-1] + [self.units]\n                outputs.set_shape(output_shape)\n        else:\n            outputs = gen_math_ops.mat_mul(inputs, w)\n        if self.use_bias:\n            outputs = nn.bias_add(outputs, self.bias)\n        if self.activation is not None:\n            return self.activation(outputs)  # pylint: disable=not-callable\n        return outputs\n\n    def get_config(self):\n        config = {\n            ""u_kernel_initializer"": self.u_kernel_initializer,\n            ""trainable"": self.trainable}\n        base_config = super(SNDense, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
tf2rl/policies/__init__.py,0,b''
tf2rl/policies/categorical_actor.py,22,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\n\nfrom tf2rl.distributions.categorical import Categorical\n\n\nclass CategoricalActor(tf.keras.Model):\n    def __init__(self, state_shape, action_dim, units=[256, 256],\n                 name=""CategoricalActor""):\n        super().__init__(name=name)\n        self.dist = Categorical(dim=action_dim)\n        self.action_dim = action_dim\n\n        self.l1 = Dense(units[0], activation=\'relu\')\n        self.l2 = Dense(units[1], activation=\'relu\')\n        self.prob = Dense(action_dim, activation=\'softmax\')\n\n        self(tf.constant(\n            np.zeros(shape=(1,)+state_shape, dtype=np.float32)))\n\n    def _compute_feature(self, states):\n        features = self.l1(states)\n        return self.l2(features)\n\n    def _compute_dist(self, states):\n        """"""\n        Compute categorical distribution\n\n        :param states (np.ndarray or tf.Tensor): Inputs to neural network.\n            NN outputs probabilities of K classes\n        :return: Categorical distribution\n        """"""\n        features = self._compute_feature(states)\n        probs = self.prob(features)\n        return {""prob"": probs}\n\n    def call(self, states, test=False):\n        """"""\n        Compute actions and log probability of the selected action\n\n        :return action (tf.Tensors): Tensor of actions\n        :return log_probs (tf.Tensor): Tensors of log probabilities of selected actions\n        """"""\n        param = self._compute_dist(states)\n        if test:\n            action = tf.math.argmax(param[""prob""], axis=1)  # (size,)\n        else:\n            action = tf.squeeze(self.dist.sample(param), axis=1)  # (size,)\n        log_prob = self.dist.log_likelihood(\n            tf.one_hot(indices=action, depth=self.action_dim), param)\n\n        return action, log_prob, param\n\n    def compute_entropy(self, states):\n        param = self._compute_dist(states)\n        return self.dist.entropy(param)\n\n    def compute_log_probs(self, states, actions):\n        """"""Compute log probabilities of inputted actions\n\n        :param states (tf.Tensor): Tensors of inputs to NN\n        :param actions (tf.Tensor): Tensors of NOT one-hot vector.\n            They will be converted to one-hot vector inside this function.\n        """"""\n        param = self._compute_dist(states)\n        actions = tf.one_hot(\n            indices=tf.squeeze(actions),\n            depth=self.action_dim)\n        param[""prob""] = tf.cond(\n            tf.math.greater(tf.rank(actions), tf.rank(param[""prob""])),\n            lambda: tf.expand_dims(param[""prob""], axis=0),\n            lambda: param[""prob""])\n        actions = tf.cond(\n            tf.math.greater(tf.rank(param[""prob""]), tf.rank(actions)),\n            lambda: tf.expand_dims(actions, axis=0),\n            lambda: actions)\n        log_prob = self.dist.log_likelihood(actions, param)\n        return log_prob\n\n\nclass CategoricalActorCritic(CategoricalActor):\n    def __init__(self, *args, **kwargs):\n        tf.keras.Model.__init__(self)\n        self.v = Dense(1, activation=""linear"")\n        super().__init__(*args, **kwargs)\n\n    def call(self, states, test=False):\n        features = self._compute_feature(states)\n        probs = self.prob(features)\n        param = {""prob"": probs}\n        if test:\n            action = tf.math.argmax(param[""prob""], axis=1)  # (size,)\n        else:\n            action = tf.squeeze(self.dist.sample(param), axis=1)  # (size,)\n\n        log_prob = self.dist.log_likelihood(\n            tf.one_hot(indices=action, depth=self.action_dim), param)\n        v = self.v(features)\n\n        return action, log_prob, v\n'"
tf2rl/policies/gaussian_actor.py,14,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\n\nfrom tf2rl.distributions.diagonal_gaussian import DiagonalGaussian\n\n\nclass GaussianActor(tf.keras.Model):\n    LOG_SIG_CAP_MAX = 2  # np.e**2 = 7.389\n    LOG_SIG_CAP_MIN = -20  # np.e**-10 = 4.540e-05\n    EPS = 1e-6\n\n    def __init__(self, state_shape, action_dim, max_action,\n                 units=[256, 256], hidden_activation=""relu"",\n                 fix_std=False, const_std=0.1,\n                 state_independent_std=False,\n                 squash=False, name=\'GaussianPolicy\'):\n        super().__init__(name=name)\n        self.dist = DiagonalGaussian(dim=action_dim)\n        self._fix_std = fix_std\n        self._const_std = const_std\n        self._max_action = max_action\n        self._squash = squash\n        self._state_independent_std = state_independent_std\n\n        self.l1 = Dense(units[0], name=""L1"", activation=hidden_activation)\n        self.l2 = Dense(units[1], name=""L2"", activation=hidden_activation)\n        self.out_mean = Dense(action_dim, name=""L_mean"")\n        if not self._fix_std:\n            if self._state_independent_std:\n                self.out_log_std = tf.Variable(\n                    initial_value=-0.5*np.ones(action_dim, dtype=np.float32),\n                    dtype=tf.float32, name=""logstd"")\n            else:\n                self.out_log_std = Dense(\n                    action_dim, name=""L_sigma"")\n\n        self(tf.constant(\n            np.zeros(shape=(1,)+state_shape, dtype=np.float32)))\n\n    def _compute_dist(self, states):\n        """"""\n        Compute multivariate normal distribution\n\n        :param states (np.ndarray or tf.Tensor): Inputs to neural network.\n            NN outputs mean and standard deviation to compute the distribution\n        :return (Dict): Multivariate normal distribution\n        """"""\n        features = self.l1(states)\n        features = self.l2(features)\n        mean = self.out_mean(features)\n        if self._fix_std:\n            log_std = tf.ones_like(mean) * tf.math.log(self._const_std)\n        else:\n            if self._state_independent_std:\n                log_std = tf.tile(\n                    input=tf.expand_dims(self.out_log_std, axis=0),\n                    multiples=[mean.shape[0], 1])\n            else:\n                log_std = self.out_log_std(features)\n                log_std = tf.clip_by_value(\n                    log_std, self.LOG_SIG_CAP_MIN, self.LOG_SIG_CAP_MAX)\n\n        return {""mean"": mean, ""log_std"": log_std}\n\n    def call(self, states, test=False):\n        """"""\n        Compute actions and log probabilities of the selected action\n        """"""\n        param = self._compute_dist(states)\n        if test:\n            raw_actions = param[""mean""]\n        else:\n            raw_actions = self.dist.sample(param)\n        logp_pis = self.dist.log_likelihood(raw_actions, param)\n\n        actions = raw_actions\n\n        if self._squash:\n            actions = tf.tanh(raw_actions)\n            logp_pis = self._squash_correction(logp_pis, actions)\n\n        return actions * self._max_action, logp_pis, param\n\n    def compute_log_probs(self, states, actions):\n        actions /= self._max_action\n        param = self._compute_dist(states)\n        logp_pis = self.dist.log_likelihood(actions, param)\n        if self._squash:\n            logp_pis = self._squash_correction(logp_pis, actions)\n        return logp_pis\n\n    def compute_entropy(self, states):\n        param = self._compute_dist(states)\n        return self.dist.entropy(param)\n\n    def _squash_correction(self, logp_pis, actions):\n        # assert_op = tf.Assert(tf.less_equal(tf.reduce_max(actions), 1.), [actions])\n        # To avoid evil machine precision error, strictly clip 1-pi**2 to [0,1] range.\n        # with tf.control_dependencies([assert_op]):\n        diff = tf.reduce_sum(\n            tf.math.log(1. - actions ** 2 + self.EPS), axis=1)\n        return logp_pis - diff\n'"
tf2rl/tools/__init__.py,0,b''
tf2rl/tools/vae.py,35,"b'import numpy as np\nimport tensorflow as tf\n\n\nclass VAE(tf.keras.Model):\n    def __init__(self, latent_dim, inference_net, generative_net):\n        super(VAE, self).__init__()\n        self.latent_dim = latent_dim\n        self.optimizer = tf.keras.optimizers.Adam(1e-4)\n        self.inference_net = inference_net\n        self.generative_net = generative_net\n\n    @tf.function\n    def sample(self, eps=None):\n        if eps is None:\n            eps = tf.random.normal(shape=(100, self.latent_dim))\n        return self.decode(eps, apply_sigmoid=True)\n\n    def encode(self, x):\n        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n        return mean, logvar\n\n    def reparameterize(self, mean, logvar):\n        eps = tf.random.normal(shape=mean.shape)\n        return eps * tf.exp(logvar * .5) + mean\n\n    def decode(self, z, apply_sigmoid=False):\n        logits = self.generative_net(z)\n        if apply_sigmoid:\n            probs = tf.sigmoid(logits)\n            return probs\n\n        return logits\n\n    @tf.function\n    def compute_loss(self, x):\n        mean, logvar = self.encode(x)\n        z = self.reparameterize(mean, logvar)\n        x_logit = self.decode(z)\n\n        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n        logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n        logpz = log_normal_pdf(z, 0., 0.)\n        logqz_x = log_normal_pdf(z, mean, logvar)\n        return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n\n    @tf.function\n    def compute_apply_gradients(self, x):\n        with tf.GradientTape() as tape:\n            loss = self.compute_loss(x)\n        gradients = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n\n\ndef log_normal_pdf(sample, mean, logvar, raxis=1):\n    log2pi = tf.math.log(2. * np.pi)\n    return tf.reduce_sum(\n        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n        axis=raxis)\n\n\nif __name__ == ""__main__"":\n    import time\n    import glob\n    import matplotlib.pyplot as plt\n    import PIL\n    import imageio\n\n    from IPython import display\n\n    (train_images, _), (test_images, _) = tf.keras.datasets.fashion_mnist.load_data()\n    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype(\'float32\')\n    test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype(\'float32\')\n\n    # Normalizing the images to the range of [0., 1.]\n    train_images /= 255.\n    test_images /= 255.\n\n    TRAIN_BUF = 60000\n    BATCH_SIZE = 100\n\n    TEST_BUF = 10000\n\n    train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(TRAIN_BUF).batch(BATCH_SIZE)\n    test_dataset = tf.data.Dataset.from_tensor_slices(test_images).shuffle(TEST_BUF).batch(BATCH_SIZE)\n\n    epochs = 100\n    latent_dim = 50\n    num_examples_to_generate = 16\n\n    inference_net = tf.keras.Sequential(\n        [\n            tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n            tf.keras.layers.Conv2D(\n                filters=32, kernel_size=3, strides=(2, 2), activation=\'relu\'),\n            tf.keras.layers.Conv2D(\n                filters=64, kernel_size=3, strides=(2, 2), activation=\'relu\'),\n            tf.keras.layers.Flatten(),\n            # No activation\n            tf.keras.layers.Dense(latent_dim + latent_dim),\n        ]\n    )\n\n    generative_net = tf.keras.Sequential(\n        [\n            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n            tf.keras.layers.Dense(units=7 * 7 * 32, activation=tf.nn.relu),\n            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n            tf.keras.layers.Conv2DTranspose(\n                filters=64,\n                kernel_size=3,\n                strides=(2, 2),\n                padding=""SAME"",\n                activation=\'relu\'),\n            tf.keras.layers.Conv2DTranspose(\n                filters=32,\n                kernel_size=3,\n                strides=(2, 2),\n                padding=""SAME"",\n                activation=\'relu\'),\n            # No activation\n            tf.keras.layers.Conv2DTranspose(\n                filters=1, kernel_size=3, strides=(1, 1), padding=""SAME""),\n        ]\n    )\n\n    # keeping the random vector constant for generation (prediction) so\n    # it will be easier to see the improvement.\n    random_vector_for_generation = tf.random.normal(\n        shape=[num_examples_to_generate, latent_dim])\n    model = VAE(latent_dim, inference_net, generative_net)\n\n\n    def generate_and_save_images(model, epoch, test_input):\n        predictions = model.sample(test_input)\n        plt.close()\n        fig = plt.figure(figsize=(4, 4))\n\n        for i in range(predictions.shape[0]):\n            plt.subplot(4, 4, i + 1)\n            plt.imshow(predictions[i, :, :, 0], cmap=\'gray\')\n            plt.axis(\'off\')\n\n        plt.savefig(\'image_at_epoch_{:04d}.png\'.format(epoch))\n\n\n    generate_and_save_images(model, 0, random_vector_for_generation)\n\n    for epoch in range(1, epochs + 1):\n        start_time = time.time()\n        for train_x in train_dataset:\n            model.compute_apply_gradients(train_x)\n        end_time = time.time()\n\n        if epoch % 1 == 0:\n            loss = tf.keras.metrics.Mean()\n            for test_x in test_dataset:\n                loss(model.compute_loss(test_x))\n            elbo = -loss.result()\n            print(\'Epoch: {}, Test set ELBO: {}, \'\n                  \'time elapse for current epoch {}\'.format(epoch,\n                                                            elbo,\n                                                            end_time - start_time))\n            generate_and_save_images(\n                model, epoch, random_vector_for_generation)\n\n\n    def display_image(epoch_no):\n        return PIL.Image.open(\'image_at_epoch_{:04d}.png\'.format(epoch_no))\n\n\n    anim_file = \'vae.gif\'\n\n    with imageio.get_writer(anim_file, mode=\'I\') as writer:\n        filenames = glob.glob(\'image*.png\')\n        filenames = sorted(filenames)\n        last = -1\n        for i, filename in enumerate(filenames):\n            frame = 2 * (i ** 0.5)\n            if round(frame) > round(last):\n                last = frame\n            else:\n                continue\n            image = imageio.imread(filename)\n            writer.append_data(image)\n        image = imageio.imread(filename)\n        writer.append_data(image)\n'"
