file_path,api_count,code
demo_server.py,0,"b'import argparse\nimport falcon\nfrom hparams import hparams, hparams_debug_string\nimport os\nfrom synthesizer import Synthesizer\n\n\nhtml_body = \'\'\'<html><title>Demo</title>\n<style>\nbody {padding: 16px; font-family: sans-serif; font-size: 14px; color: #444}\ninput {font-size: 14px; padding: 8px 12px; outline: none; border: 1px solid #ddd}\ninput:focus {box-shadow: 0 1px 2px rgba(0,0,0,.15)}\np {padding: 12px}\nbutton {background: #28d; padding: 9px 14px; margin-left: 8px; border: none; outline: none;\n        color: #fff; font-size: 14px; border-radius: 4px; cursor: pointer;}\nbutton:hover {box-shadow: 0 1px 2px rgba(0,0,0,.15); opacity: 0.9;}\nbutton:active {background: #29f;}\nbutton[disabled] {opacity: 0.4; cursor: default}\n</style>\n<body>\n<form>\n  <input id=""text"" type=""text"" size=""40"" placeholder=""Enter Text"">\n  <button id=""button"" name=""synthesize"">Speak</button>\n</form>\n<p id=""message""></p>\n<audio id=""audio"" controls autoplay hidden></audio>\n<script>\nfunction q(selector) {return document.querySelector(selector)}\nq(\'#text\').focus()\nq(\'#button\').addEventListener(\'click\', function(e) {\n  text = q(\'#text\').value.trim()\n  if (text) {\n    q(\'#message\').textContent = \'Synthesizing...\'\n    q(\'#button\').disabled = true\n    q(\'#audio\').hidden = true\n    synthesize(text)\n  }\n  e.preventDefault()\n  return false\n})\nfunction synthesize(text) {\n  fetch(\'/synthesize?text=\' + encodeURIComponent(text), {cache: \'no-cache\'})\n    .then(function(res) {\n      if (!res.ok) throw Error(res.statusText)\n      return res.blob()\n    }).then(function(blob) {\n      q(\'#message\').textContent = \'\'\n      q(\'#button\').disabled = false\n      q(\'#audio\').src = URL.createObjectURL(blob)\n      q(\'#audio\').hidden = false\n    }).catch(function(err) {\n      q(\'#message\').textContent = \'Error: \' + err.message\n      q(\'#button\').disabled = false\n    })\n}\n</script></body></html>\n\'\'\'\n\n\nclass UIResource:\n  def on_get(self, req, res):\n    res.content_type = \'text/html\'\n    res.body = html_body\n\n\nclass SynthesisResource:\n  def on_get(self, req, res):\n    if not req.params.get(\'text\'):\n      raise falcon.HTTPBadRequest()\n    res.data = synthesizer.synthesize(req.params.get(\'text\'))\n    res.content_type = \'audio/wav\'\n\n\nsynthesizer = Synthesizer()\napi = falcon.API()\napi.add_route(\'/synthesize\', SynthesisResource())\napi.add_route(\'/\', UIResource())\n\n\nif __name__ == \'__main__\':\n  from wsgiref import simple_server\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--checkpoint\', required=True, help=\'Full path to model checkpoint\')\n  parser.add_argument(\'--port\', type=int, default=9000)\n  parser.add_argument(\'--hparams\', default=\'\',\n    help=\'Hyperparameter overrides as a comma-separated list of name=value pairs\')\n  args = parser.parse_args()\n  os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n  hparams.parse(args.hparams)\n  print(hparams_debug_string())\n  synthesizer.load(args.checkpoint)\n  print(\'Serving on port %d\' % args.port)\n  simple_server.make_server(\'0.0.0.0\', args.port, api).serve_forever()\nelse:\n  synthesizer.load(os.environ[\'CHECKPOINT\'])\n'"
eval.py,0,"b""import argparse\nimport os\nimport re\nfrom hparams import hparams, hparams_debug_string\nfrom synthesizer import Synthesizer\n\n\nsentences = [\n  # From July 8, 2017 New York Times:\n  'Scientists at the CERN laboratory say they have discovered a new particle.',\n  'There\xe2\x80\x99s a way to measure the acute emotional intelligence that has never gone out of style.',\n  'President Trump met with other leaders at the Group of 20 conference.',\n  'The Senate\\'s bill to repeal and replace the Affordable Care Act is now imperiled.',\n  # From Google's Tacotron example page:\n  'Generative adversarial network or variational auto-encoder.',\n  'The buses aren\\'t the problem, they actually provide a solution.',\n  'Does the quick brown fox jump over the lazy dog?',\n  'Talib Kweli confirmed to AllHipHop that he will be releasing an album in the next year.',\n]\n\n\ndef get_output_base_path(checkpoint_path):\n  base_dir = os.path.dirname(checkpoint_path)\n  m = re.compile(r'.*?\\.ckpt\\-([0-9]+)').match(checkpoint_path)\n  name = 'eval-%d' % int(m.group(1)) if m else 'eval'\n  return os.path.join(base_dir, name)\n\n\ndef run_eval(args):\n  print(hparams_debug_string())\n  synth = Synthesizer()\n  synth.load(args.checkpoint)\n  base_path = get_output_base_path(args.checkpoint)\n  for i, text in enumerate(sentences):\n    path = '%s-%d.wav' % (base_path, i)\n    print('Synthesizing: %s' % path)\n    with open(path, 'wb') as f:\n      f.write(synth.synthesize(text))\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--checkpoint', required=True, help='Path to model checkpoint')\n  parser.add_argument('--hparams', default='',\n    help='Hyperparameter overrides as a comma-separated list of name=value pairs')\n  args = parser.parse_args()\n  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n  hparams.parse(args.hparams)\n  run_eval(args)\n\n\nif __name__ == '__main__':\n  main()\n"""
hparams.py,1,"b'import tensorflow as tf\n\n\n# Default hyperparameters:\nhparams = tf.contrib.training.HParams(\n  # Comma-separated list of cleaners to run on text prior to training and eval. For non-English\n  # text, you may want to use ""basic_cleaners"" or ""transliteration_cleaners"" See TRAINING_DATA.md.\n  cleaners=\'english_cleaners\',\n\n  # Audio:\n  num_mels=80,\n  num_freq=1025,\n  sample_rate=20000,\n  frame_length_ms=50,\n  frame_shift_ms=12.5,\n  preemphasis=0.97,\n  min_level_db=-100,\n  ref_level_db=20,\n\n  # Model:\n  outputs_per_step=5,\n  embed_depth=256,\n  prenet_depths=[256, 128],\n  encoder_depth=256,\n  postnet_depth=256,\n  attention_depth=256,\n  decoder_depth=256,\n\n  # Training:\n  batch_size=32,\n  adam_beta1=0.9,\n  adam_beta2=0.999,\n  initial_learning_rate=0.002,\n  decay_learning_rate=True,\n  use_cmudict=False,  # Use CMUDict during training to learn pronunciation of ARPAbet phonemes\n\n  # Eval:\n  max_iters=200,\n  griffin_lim_iters=60,\n  power=1.5,              # Power to raise magnitudes to prior to Griffin-Lim\n)\n\n\ndef hparams_debug_string():\n  values = hparams.values()\n  hp = [\'  %s: %s\' % (name, values[name]) for name in sorted(values)]\n  return \'Hyperparameters:\\n\' + \'\\n\'.join(hp)\n'"
preprocess.py,0,"b'import argparse\nimport os\nfrom multiprocessing import cpu_count\nfrom tqdm import tqdm\nfrom datasets import blizzard, ljspeech\nfrom hparams import hparams\n\n\ndef preprocess_blizzard(args):\n  in_dir = os.path.join(args.base_dir, \'Blizzard2012\')\n  out_dir = os.path.join(args.base_dir, args.output)\n  os.makedirs(out_dir, exist_ok=True)\n  metadata = blizzard.build_from_path(in_dir, out_dir, args.num_workers, tqdm=tqdm)\n  write_metadata(metadata, out_dir)\n\n\ndef preprocess_ljspeech(args):\n  in_dir = os.path.join(args.base_dir, \'LJSpeech-1.1\')\n  out_dir = os.path.join(args.base_dir, args.output)\n  os.makedirs(out_dir, exist_ok=True)\n  metadata = ljspeech.build_from_path(in_dir, out_dir, args.num_workers, tqdm=tqdm)\n  write_metadata(metadata, out_dir)\n\n\ndef write_metadata(metadata, out_dir):\n  with open(os.path.join(out_dir, \'train.txt\'), \'w\', encoding=\'utf-8\') as f:\n    for m in metadata:\n      f.write(\'|\'.join([str(x) for x in m]) + \'\\n\')\n  frames = sum([m[2] for m in metadata])\n  hours = frames * hparams.frame_shift_ms / (3600 * 1000)\n  print(\'Wrote %d utterances, %d frames (%.2f hours)\' % (len(metadata), frames, hours))\n  print(\'Max input length:  %d\' % max(len(m[3]) for m in metadata))\n  print(\'Max output length: %d\' % max(m[2] for m in metadata))\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--base_dir\', default=os.path.expanduser(\'~/tacotron\'))\n  parser.add_argument(\'--output\', default=\'training\')\n  parser.add_argument(\'--dataset\', required=True, choices=[\'blizzard\', \'ljspeech\'])\n  parser.add_argument(\'--num_workers\', type=int, default=cpu_count())\n  args = parser.parse_args()\n  if args.dataset == \'blizzard\':\n    preprocess_blizzard(args)\n  elif args.dataset == \'ljspeech\':\n    preprocess_ljspeech(args)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
synthesizer.py,6,"b""import io\nimport numpy as np\nimport tensorflow as tf\nfrom hparams import hparams\nfrom librosa import effects\nfrom models import create_model\nfrom text import text_to_sequence\nfrom util import audio\n\n\nclass Synthesizer:\n  def load(self, checkpoint_path, model_name='tacotron'):\n    print('Constructing model: %s' % model_name)\n    inputs = tf.placeholder(tf.int32, [1, None], 'inputs')\n    input_lengths = tf.placeholder(tf.int32, [1], 'input_lengths')\n    with tf.variable_scope('model') as scope:\n      self.model = create_model(model_name, hparams)\n      self.model.initialize(inputs, input_lengths)\n      self.wav_output = audio.inv_spectrogram_tensorflow(self.model.linear_outputs[0])\n\n    print('Loading checkpoint: %s' % checkpoint_path)\n    self.session = tf.Session()\n    self.session.run(tf.global_variables_initializer())\n    saver = tf.train.Saver()\n    saver.restore(self.session, checkpoint_path)\n\n\n  def synthesize(self, text):\n    cleaner_names = [x.strip() for x in hparams.cleaners.split(',')]\n    seq = text_to_sequence(text, cleaner_names)\n    feed_dict = {\n      self.model.inputs: [np.asarray(seq, dtype=np.int32)],\n      self.model.input_lengths: np.asarray([len(seq)], dtype=np.int32)\n    }\n    wav = self.session.run(self.wav_output, feed_dict=feed_dict)\n    wav = audio.inv_preemphasis(wav)\n    wav = wav[:audio.find_endpoint(wav)]\n    out = io.BytesIO()\n    audio.save_wav(wav, out)\n    return out.getvalue()\n"""
train.py,21,"b""import argparse\nfrom datetime import datetime\nimport math\nimport os\nimport subprocess\nimport time\nimport tensorflow as tf\nimport traceback\n\nfrom datasets.datafeeder import DataFeeder\nfrom hparams import hparams, hparams_debug_string\nfrom models import create_model\nfrom text import sequence_to_text\nfrom util import audio, infolog, plot, ValueWindow\nlog = infolog.log\n\n\ndef get_git_commit():\n  subprocess.check_output(['git', 'diff-index', '--quiet', 'HEAD'])   # Verify client is clean\n  commit = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()[:10]\n  log('Git commit: %s' % commit)\n  return commit\n\n\ndef add_stats(model):\n  with tf.variable_scope('stats') as scope:\n    tf.summary.histogram('linear_outputs', model.linear_outputs)\n    tf.summary.histogram('linear_targets', model.linear_targets)\n    tf.summary.histogram('mel_outputs', model.mel_outputs)\n    tf.summary.histogram('mel_targets', model.mel_targets)\n    tf.summary.scalar('loss_mel', model.mel_loss)\n    tf.summary.scalar('loss_linear', model.linear_loss)\n    tf.summary.scalar('learning_rate', model.learning_rate)\n    tf.summary.scalar('loss', model.loss)\n    gradient_norms = [tf.norm(grad) for grad in model.gradients]\n    tf.summary.histogram('gradient_norm', gradient_norms)\n    tf.summary.scalar('max_gradient_norm', tf.reduce_max(gradient_norms))\n    return tf.summary.merge_all()\n\n\ndef time_string():\n  return datetime.now().strftime('%Y-%m-%d %H:%M')\n\n\ndef train(log_dir, args):\n  commit = get_git_commit() if args.git else 'None'\n  checkpoint_path = os.path.join(log_dir, 'model.ckpt')\n  input_path = os.path.join(args.base_dir, args.input)\n  log('Checkpoint path: %s' % checkpoint_path)\n  log('Loading training data from: %s' % input_path)\n  log('Using model: %s' % args.model)\n  log(hparams_debug_string())\n\n  # Set up DataFeeder:\n  coord = tf.train.Coordinator()\n  with tf.variable_scope('datafeeder') as scope:\n    feeder = DataFeeder(coord, input_path, hparams)\n\n  # Set up model:\n  global_step = tf.Variable(0, name='global_step', trainable=False)\n  with tf.variable_scope('model') as scope:\n    model = create_model(args.model, hparams)\n    model.initialize(feeder.inputs, feeder.input_lengths, feeder.mel_targets, feeder.linear_targets)\n    model.add_loss()\n    model.add_optimizer(global_step)\n    stats = add_stats(model)\n\n  # Bookkeeping:\n  step = 0\n  time_window = ValueWindow(100)\n  loss_window = ValueWindow(100)\n  saver = tf.train.Saver(max_to_keep=5, keep_checkpoint_every_n_hours=2)\n\n  # Train!\n  with tf.Session() as sess:\n    try:\n      summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n      sess.run(tf.global_variables_initializer())\n\n      if args.restore_step:\n        # Restore from a checkpoint if the user requested it.\n        restore_path = '%s-%d' % (checkpoint_path, args.restore_step)\n        saver.restore(sess, restore_path)\n        log('Resuming from checkpoint: %s at commit: %s' % (restore_path, commit), slack=True)\n      else:\n        log('Starting new training run at commit: %s' % commit, slack=True)\n\n      feeder.start_in_session(sess)\n\n      while not coord.should_stop():\n        start_time = time.time()\n        step, loss, opt = sess.run([global_step, model.loss, model.optimize])\n        time_window.append(time.time() - start_time)\n        loss_window.append(loss)\n        message = 'Step %-7d [%.03f sec/step, loss=%.05f, avg_loss=%.05f]' % (\n          step, time_window.average, loss, loss_window.average)\n        log(message, slack=(step % args.checkpoint_interval == 0))\n\n        if loss > 100 or math.isnan(loss):\n          log('Loss exploded to %.05f at step %d!' % (loss, step), slack=True)\n          raise Exception('Loss Exploded')\n\n        if step % args.summary_interval == 0:\n          log('Writing summary at step: %d' % step)\n          summary_writer.add_summary(sess.run(stats), step)\n\n        if step % args.checkpoint_interval == 0:\n          log('Saving checkpoint to: %s-%d' % (checkpoint_path, step))\n          saver.save(sess, checkpoint_path, global_step=step)\n          log('Saving audio and alignment...')\n          input_seq, spectrogram, alignment = sess.run([\n            model.inputs[0], model.linear_outputs[0], model.alignments[0]])\n          waveform = audio.inv_spectrogram(spectrogram.T)\n          audio.save_wav(waveform, os.path.join(log_dir, 'step-%d-audio.wav' % step))\n          plot.plot_alignment(alignment, os.path.join(log_dir, 'step-%d-align.png' % step),\n            info='%s, %s, %s, step=%d, loss=%.5f' % (args.model, commit, time_string(), step, loss))\n          log('Input: %s' % sequence_to_text(input_seq))\n\n    except Exception as e:\n      log('Exiting due to exception: %s' % e, slack=True)\n      traceback.print_exc()\n      coord.request_stop(e)\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--base_dir', default=os.path.expanduser('~/tacotron'))\n  parser.add_argument('--input', default='training/train.txt')\n  parser.add_argument('--model', default='tacotron')\n  parser.add_argument('--name', help='Name of the run. Used for logging. Defaults to model name.')\n  parser.add_argument('--hparams', default='',\n    help='Hyperparameter overrides as a comma-separated list of name=value pairs')\n  parser.add_argument('--restore_step', type=int, help='Global step to restore from checkpoint.')\n  parser.add_argument('--summary_interval', type=int, default=100,\n    help='Steps between running summary ops.')\n  parser.add_argument('--checkpoint_interval', type=int, default=1000,\n    help='Steps between writing checkpoints.')\n  parser.add_argument('--slack_url', help='Slack webhook URL to get periodic reports.')\n  parser.add_argument('--tf_log_level', type=int, default=1, help='Tensorflow C++ log level.')\n  parser.add_argument('--git', action='store_true', help='If set, verify that the client is clean.')\n  args = parser.parse_args()\n  os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(args.tf_log_level)\n  run_name = args.name or args.model\n  log_dir = os.path.join(args.base_dir, 'logs-%s' % run_name)\n  os.makedirs(log_dir, exist_ok=True)\n  infolog.init(os.path.join(log_dir, 'train.log'), run_name, args.slack_url)\n  hparams.parse(args.hparams)\n  train(log_dir, args)\n\n\nif __name__ == '__main__':\n  main()\n"""
datasets/__init__.py,0,b''
datasets/blizzard.py,0,"b'from concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport numpy as np\nimport os\nfrom hparams import hparams\nfrom util import audio\n\n\n_max_out_length = 700\n_end_buffer = 0.05\n_min_confidence = 90\n\n# Note: ""A Tramp Abroad"" & ""The Man That Corrupted Hadleyburg"" are higher quality than the others.\nbooks = [\n  \'ATrampAbroad\',\n  \'TheManThatCorruptedHadleyburg\',\n  # \'LifeOnTheMississippi\',\n  # \'TheAdventuresOfTomSawyer\',\n]\n\ndef build_from_path(in_dir, out_dir, num_workers=1, tqdm=lambda x: x):\n  executor = ProcessPoolExecutor(max_workers=num_workers)\n  futures = []\n  index = 1\n  for book in books:\n    with open(os.path.join(in_dir, book, \'sentence_index.txt\')) as f:\n      for line in f:\n        parts = line.strip().split(\'\\t\')\n        if line[0] is not \'#\' and len(parts) == 8 and float(parts[3]) > _min_confidence:\n          wav_path = os.path.join(in_dir, book, \'wav\', \'%s.wav\' % parts[0])\n          labels_path = os.path.join(in_dir, book, \'lab\', \'%s.lab\' % parts[0])\n          text = parts[5]\n          task = partial(_process_utterance, out_dir, index, wav_path, labels_path, text)\n          futures.append(executor.submit(task))\n          index += 1\n  results = [future.result() for future in tqdm(futures)]\n  return [r for r in results if r is not None]\n\n\ndef _process_utterance(out_dir, index, wav_path, labels_path, text):\n  # Load the wav file and trim silence from the ends:\n  wav = audio.load_wav(wav_path)\n  start_offset, end_offset = _parse_labels(labels_path)\n  start = int(start_offset * hparams.sample_rate)\n  end = int(end_offset * hparams.sample_rate) if end_offset is not None else -1\n  wav = wav[start:end]\n  max_samples = _max_out_length * hparams.frame_shift_ms / 1000 * hparams.sample_rate\n  if len(wav) > max_samples:\n    return None\n  spectrogram = audio.spectrogram(wav).astype(np.float32)\n  n_frames = spectrogram.shape[1]\n  mel_spectrogram = audio.melspectrogram(wav).astype(np.float32)\n  spectrogram_filename = \'blizzard-spec-%05d.npy\' % index\n  mel_filename = \'blizzard-mel-%05d.npy\' % index\n  np.save(os.path.join(out_dir, spectrogram_filename), spectrogram.T, allow_pickle=False)\n  np.save(os.path.join(out_dir, mel_filename), mel_spectrogram.T, allow_pickle=False)\n  return (spectrogram_filename, mel_filename, n_frames, text)\n\n\ndef _parse_labels(path):\n  labels = []\n  with open(os.path.join(path)) as f:\n    for line in f:\n      parts = line.strip().split(\' \')\n      if len(parts) >= 3:\n        labels.append((float(parts[0]), \' \'.join(parts[2:])))\n  start = 0\n  end = None\n  if labels[0][1] == \'sil\':\n    start = labels[0][0]\n  if labels[-1][1] == \'sil\':\n    end = labels[-2][0] + _end_buffer\n  return (start, end)\n'"
datasets/datafeeder.py,5,"b""import numpy as np\nimport os\nimport random\nimport tensorflow as tf\nimport threading\nimport time\nimport traceback\nfrom text import cmudict, text_to_sequence\nfrom util.infolog import log\n\n\n_batches_per_group = 32\n_p_cmudict = 0.5\n_pad = 0\n\n\nclass DataFeeder(threading.Thread):\n  '''Feeds batches of data into a queue on a background thread.'''\n\n  def __init__(self, coordinator, metadata_filename, hparams):\n    super(DataFeeder, self).__init__()\n    self._coord = coordinator\n    self._hparams = hparams\n    self._cleaner_names = [x.strip() for x in hparams.cleaners.split(',')]\n    self._offset = 0\n\n    # Load metadata:\n    self._datadir = os.path.dirname(metadata_filename)\n    with open(metadata_filename, encoding='utf-8') as f:\n      self._metadata = [line.strip().split('|') for line in f]\n      hours = sum((int(x[2]) for x in self._metadata)) * hparams.frame_shift_ms / (3600 * 1000)\n      log('Loaded metadata for %d examples (%.2f hours)' % (len(self._metadata), hours))\n\n    # Create placeholders for inputs and targets. Don't specify batch size because we want to\n    # be able to feed different sized batches at eval time.\n    self._placeholders = [\n      tf.placeholder(tf.int32, [None, None], 'inputs'),\n      tf.placeholder(tf.int32, [None], 'input_lengths'),\n      tf.placeholder(tf.float32, [None, None, hparams.num_mels], 'mel_targets'),\n      tf.placeholder(tf.float32, [None, None, hparams.num_freq], 'linear_targets')\n    ]\n\n    # Create queue for buffering data:\n    queue = tf.FIFOQueue(8, [tf.int32, tf.int32, tf.float32, tf.float32], name='input_queue')\n    self._enqueue_op = queue.enqueue(self._placeholders)\n    self.inputs, self.input_lengths, self.mel_targets, self.linear_targets = queue.dequeue()\n    self.inputs.set_shape(self._placeholders[0].shape)\n    self.input_lengths.set_shape(self._placeholders[1].shape)\n    self.mel_targets.set_shape(self._placeholders[2].shape)\n    self.linear_targets.set_shape(self._placeholders[3].shape)\n\n    # Load CMUDict: If enabled, this will randomly substitute some words in the training data with\n    # their ARPABet equivalents, which will allow you to also pass ARPABet to the model for\n    # synthesis (useful for proper nouns, etc.)\n    if hparams.use_cmudict:\n      cmudict_path = os.path.join(self._datadir, 'cmudict-0.7b')\n      if not os.path.isfile(cmudict_path):\n        raise Exception('If use_cmudict=True, you must download ' +\n          'http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b to %s'  % cmudict_path)\n      self._cmudict = cmudict.CMUDict(cmudict_path, keep_ambiguous=False)\n      log('Loaded CMUDict with %d unambiguous entries' % len(self._cmudict))\n    else:\n      self._cmudict = None\n\n\n  def start_in_session(self, session):\n    self._session = session\n    self.start()\n\n\n  def run(self):\n    try:\n      while not self._coord.should_stop():\n        self._enqueue_next_group()\n    except Exception as e:\n      traceback.print_exc()\n      self._coord.request_stop(e)\n\n\n  def _enqueue_next_group(self):\n    start = time.time()\n\n    # Read a group of examples:\n    n = self._hparams.batch_size\n    r = self._hparams.outputs_per_step\n    examples = [self._get_next_example() for i in range(n * _batches_per_group)]\n\n    # Bucket examples based on similar output sequence length for efficiency:\n    examples.sort(key=lambda x: x[-1])\n    batches = [examples[i:i+n] for i in range(0, len(examples), n)]\n    random.shuffle(batches)\n\n    log('Generated %d batches of size %d in %.03f sec' % (len(batches), n, time.time() - start))\n    for batch in batches:\n      feed_dict = dict(zip(self._placeholders, _prepare_batch(batch, r)))\n      self._session.run(self._enqueue_op, feed_dict=feed_dict)\n\n\n  def _get_next_example(self):\n    '''Loads a single example (input, mel_target, linear_target, cost) from disk'''\n    if self._offset >= len(self._metadata):\n      self._offset = 0\n      random.shuffle(self._metadata)\n    meta = self._metadata[self._offset]\n    self._offset += 1\n\n    text = meta[3]\n    if self._cmudict and random.random() < _p_cmudict:\n      text = ' '.join([self._maybe_get_arpabet(word) for word in text.split(' ')])\n\n    input_data = np.asarray(text_to_sequence(text, self._cleaner_names), dtype=np.int32)\n    linear_target = np.load(os.path.join(self._datadir, meta[0]))\n    mel_target = np.load(os.path.join(self._datadir, meta[1]))\n    return (input_data, mel_target, linear_target, len(linear_target))\n\n\n  def _maybe_get_arpabet(self, word):\n    arpabet = self._cmudict.lookup(word)\n    return '{%s}' % arpabet[0] if arpabet is not None and random.random() < 0.5 else word\n\n\ndef _prepare_batch(batch, outputs_per_step):\n  random.shuffle(batch)\n  inputs = _prepare_inputs([x[0] for x in batch])\n  input_lengths = np.asarray([len(x[0]) for x in batch], dtype=np.int32)\n  mel_targets = _prepare_targets([x[1] for x in batch], outputs_per_step)\n  linear_targets = _prepare_targets([x[2] for x in batch], outputs_per_step)\n  return (inputs, input_lengths, mel_targets, linear_targets)\n\n\ndef _prepare_inputs(inputs):\n  max_len = max((len(x) for x in inputs))\n  return np.stack([_pad_input(x, max_len) for x in inputs])\n\n\ndef _prepare_targets(targets, alignment):\n  max_len = max((len(t) for t in targets)) + 1\n  return np.stack([_pad_target(t, _round_up(max_len, alignment)) for t in targets])\n\n\ndef _pad_input(x, length):\n  return np.pad(x, (0, length - x.shape[0]), mode='constant', constant_values=_pad)\n\n\ndef _pad_target(t, length):\n  return np.pad(t, [(0, length - t.shape[0]), (0,0)], mode='constant', constant_values=_pad)\n\n\ndef _round_up(x, multiple):\n  remainder = x % multiple\n  return x if remainder == 0 else x + multiple - remainder\n"""
datasets/ljspeech.py,0,"b""from concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport numpy as np\nimport os\nfrom util import audio\n\n\ndef build_from_path(in_dir, out_dir, num_workers=1, tqdm=lambda x: x):\n  '''Preprocesses the LJ Speech dataset from a given input path into a given output directory.\n\n    Args:\n      in_dir: The directory where you have downloaded the LJ Speech dataset\n      out_dir: The directory to write the output into\n      num_workers: Optional number of worker processes to parallelize across\n      tqdm: You can optionally pass tqdm to get a nice progress bar\n\n    Returns:\n      A list of tuples describing the training examples. This should be written to train.txt\n  '''\n\n  # We use ProcessPoolExecutor to parallelize across processes. This is just an optimization and you\n  # can omit it and just call _process_utterance on each input if you want.\n  executor = ProcessPoolExecutor(max_workers=num_workers)\n  futures = []\n  index = 1\n  with open(os.path.join(in_dir, 'metadata.csv'), encoding='utf-8') as f:\n    for line in f:\n      parts = line.strip().split('|')\n      wav_path = os.path.join(in_dir, 'wavs', '%s.wav' % parts[0])\n      text = parts[2]\n      futures.append(executor.submit(partial(_process_utterance, out_dir, index, wav_path, text)))\n      index += 1\n  return [future.result() for future in tqdm(futures)]\n\n\ndef _process_utterance(out_dir, index, wav_path, text):\n  '''Preprocesses a single utterance audio/text pair.\n\n  This writes the mel and linear scale spectrograms to disk and returns a tuple to write\n  to the train.txt file.\n\n  Args:\n    out_dir: The directory to write the spectrograms into\n    index: The numeric index to use in the spectrogram filenames.\n    wav_path: Path to the audio file containing the speech input\n    text: The text spoken in the input audio file\n\n  Returns:\n    A (spectrogram_filename, mel_filename, n_frames, text) tuple to write to train.txt\n  '''\n\n  # Load the audio to a numpy array:\n  wav = audio.load_wav(wav_path)\n\n  # Compute the linear-scale spectrogram from the wav:\n  spectrogram = audio.spectrogram(wav).astype(np.float32)\n  n_frames = spectrogram.shape[1]\n\n  # Compute a mel-scale spectrogram from the wav:\n  mel_spectrogram = audio.melspectrogram(wav).astype(np.float32)\n\n  # Write the spectrograms to disk:\n  spectrogram_filename = 'ljspeech-spec-%05d.npy' % index\n  mel_filename = 'ljspeech-mel-%05d.npy' % index\n  np.save(os.path.join(out_dir, spectrogram_filename), spectrogram.T, allow_pickle=False)\n  np.save(os.path.join(out_dir, mel_filename), mel_spectrogram.T, allow_pickle=False)\n\n  # Return a tuple describing this training example:\n  return (spectrogram_filename, mel_filename, n_frames, text)\n"""
models/__init__.py,0,"b""from .tacotron import Tacotron\n\n\ndef create_model(name, hparams):\n  if name == 'tacotron':\n    return Tacotron(hparams)\n  else:\n    raise Exception('Unknown model: ' + name)\n"""
models/helpers.py,17,"b""import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.seq2seq import Helper\n\n\n# Adapted from tf.contrib.seq2seq.GreedyEmbeddingHelper\nclass TacoTestHelper(Helper):\n  def __init__(self, batch_size, output_dim, r):\n    with tf.name_scope('TacoTestHelper'):\n      self._batch_size = batch_size\n      self._output_dim = output_dim\n      self._end_token = tf.tile([0.0], [output_dim * r])\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def sample_ids_shape(self):\n    return tf.TensorShape([])\n\n  @property\n  def sample_ids_dtype(self):\n    return np.int32\n\n  def initialize(self, name=None):\n    return (tf.tile([False], [self._batch_size]), _go_frames(self._batch_size, self._output_dim))\n\n  def sample(self, time, outputs, state, name=None):\n    return tf.tile([0], [self._batch_size])  # Return all 0; we ignore them\n\n  def next_inputs(self, time, outputs, state, sample_ids, name=None):\n    '''Stop on EOS. Otherwise, pass the last output as the next input and pass through state.'''\n    with tf.name_scope('TacoTestHelper'):\n      finished = tf.reduce_all(tf.equal(outputs, self._end_token), axis=1)\n      # Feed last output frame as next input. outputs is [N, output_dim * r]\n      next_inputs = outputs[:, -self._output_dim:]\n      return (finished, next_inputs, state)\n\n\nclass TacoTrainingHelper(Helper):\n  def __init__(self, inputs, targets, output_dim, r):\n    # inputs is [N, T_in], targets is [N, T_out, D]\n    with tf.name_scope('TacoTrainingHelper'):\n      self._batch_size = tf.shape(inputs)[0]\n      self._output_dim = output_dim\n\n      # Feed every r-th target frame as input\n      self._targets = targets[:, r-1::r, :]\n\n      # Use full length for every target because we don't want to mask the padding frames\n      num_steps = tf.shape(self._targets)[1]\n      self._lengths = tf.tile([num_steps], [self._batch_size])\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def sample_ids_shape(self):\n    return tf.TensorShape([])\n\n  @property\n  def sample_ids_dtype(self):\n    return np.int32\n\n  def initialize(self, name=None):\n    return (tf.tile([False], [self._batch_size]), _go_frames(self._batch_size, self._output_dim))\n\n  def sample(self, time, outputs, state, name=None):\n    return tf.tile([0], [self._batch_size])  # Return all 0; we ignore them\n\n  def next_inputs(self, time, outputs, state, sample_ids, name=None):\n    with tf.name_scope(name or 'TacoTrainingHelper'):\n      finished = (time + 1 >= self._lengths)\n      next_inputs = self._targets[:, time, :]\n      return (finished, next_inputs, state)\n\n\ndef _go_frames(batch_size, output_dim):\n  '''Returns all-zero <GO> frames for a given batch size and output dimension'''\n  return tf.tile([[0.0]], [batch_size, output_dim])\n\n"""
models/modules.py,22,"b""import tensorflow as tf\nfrom tensorflow.contrib.rnn import GRUCell\n\n\ndef prenet(inputs, is_training, layer_sizes, scope=None):\n  x = inputs\n  drop_rate = 0.5 if is_training else 0.0\n  with tf.variable_scope(scope or 'prenet'):\n    for i, size in enumerate(layer_sizes):\n      dense = tf.layers.dense(x, units=size, activation=tf.nn.relu, name='dense_%d' % (i+1))\n      x = tf.layers.dropout(dense, rate=drop_rate, training=is_training, name='dropout_%d' % (i+1))\n  return x\n\n\ndef encoder_cbhg(inputs, input_lengths, is_training, depth):\n  input_channels = inputs.get_shape()[2]\n  return cbhg(\n    inputs,\n    input_lengths,\n    is_training,\n    scope='encoder_cbhg',\n    K=16,\n    projections=[128, input_channels],\n    depth=depth)\n\n\ndef post_cbhg(inputs, input_dim, is_training, depth):\n  return cbhg(\n    inputs,\n    None,\n    is_training,\n    scope='post_cbhg',\n    K=8,\n    projections=[256, input_dim],\n    depth=depth)\n\n\ndef cbhg(inputs, input_lengths, is_training, scope, K, projections, depth):\n  with tf.variable_scope(scope):\n    with tf.variable_scope('conv_bank'):\n      # Convolution bank: concatenate on the last axis to stack channels from all convolutions\n      conv_outputs = tf.concat(\n        [conv1d(inputs, k, 128, tf.nn.relu, is_training, 'conv1d_%d' % k) for k in range(1, K+1)],\n        axis=-1\n      )\n\n    # Maxpooling:\n    maxpool_output = tf.layers.max_pooling1d(\n      conv_outputs,\n      pool_size=2,\n      strides=1,\n      padding='same')\n\n    # Two projection layers:\n    proj1_output = conv1d(maxpool_output, 3, projections[0], tf.nn.relu, is_training, 'proj_1')\n    proj2_output = conv1d(proj1_output, 3, projections[1], None, is_training, 'proj_2')\n\n    # Residual connection:\n    highway_input = proj2_output + inputs\n\n    half_depth = depth // 2\n    assert half_depth*2 == depth, 'encoder and postnet depths must be even.'\n\n    # Handle dimensionality mismatch:\n    if highway_input.shape[2] != half_depth:\n      highway_input = tf.layers.dense(highway_input, half_depth)\n\n    # 4-layer HighwayNet:\n    for i in range(4):\n      highway_input = highwaynet(highway_input, 'highway_%d' % (i+1), half_depth)\n    rnn_input = highway_input\n\n    # Bidirectional RNN\n    outputs, states = tf.nn.bidirectional_dynamic_rnn(\n      GRUCell(half_depth),\n      GRUCell(half_depth),\n      rnn_input,\n      sequence_length=input_lengths,\n      dtype=tf.float32)\n    return tf.concat(outputs, axis=2)  # Concat forward and backward\n\n\ndef highwaynet(inputs, scope, depth):\n  with tf.variable_scope(scope):\n    H = tf.layers.dense(\n      inputs,\n      units=depth,\n      activation=tf.nn.relu,\n      name='H')\n    T = tf.layers.dense(\n      inputs,\n      units=depth,\n      activation=tf.nn.sigmoid,\n      name='T',\n      bias_initializer=tf.constant_initializer(-1.0))\n    return H * T + inputs * (1.0 - T)\n\n\ndef conv1d(inputs, kernel_size, channels, activation, is_training, scope):\n  with tf.variable_scope(scope):\n    conv1d_output = tf.layers.conv1d(\n      inputs,\n      filters=channels,\n      kernel_size=kernel_size,\n      activation=activation,\n      padding='same')\n    return tf.layers.batch_normalization(conv1d_output, training=is_training)\n"""
models/rnn_wrappers.py,1,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import RNNCell\nfrom .modules import prenet\n\n\nclass DecoderPrenetWrapper(RNNCell):\n  \'\'\'Runs RNN inputs through a prenet before sending them to the cell.\'\'\'\n  def __init__(self, cell, is_training, layer_sizes):\n    super(DecoderPrenetWrapper, self).__init__()\n    self._cell = cell\n    self._is_training = is_training\n    self._layer_sizes = layer_sizes\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  @property\n  def output_size(self):\n    return self._cell.output_size\n\n  def call(self, inputs, state):\n    prenet_out = prenet(inputs, self._is_training, self._layer_sizes, scope=\'decoder_prenet\')\n    return self._cell(prenet_out, state)\n\n  def zero_state(self, batch_size, dtype):\n    return self._cell.zero_state(batch_size, dtype)\n\n\n\nclass ConcatOutputAndAttentionWrapper(RNNCell):\n  \'\'\'Concatenates RNN cell output with the attention context vector.\n\n  This is expected to wrap a cell wrapped with an AttentionWrapper constructed with\n  attention_layer_size=None and output_attention=False. Such a cell\'s state will include an\n  ""attention"" field that is the context vector.\n  \'\'\'\n  def __init__(self, cell):\n    super(ConcatOutputAndAttentionWrapper, self).__init__()\n    self._cell = cell\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  @property\n  def output_size(self):\n    return self._cell.output_size + self._cell.state_size.attention\n\n  def call(self, inputs, state):\n    output, res_state = self._cell(inputs, state)\n    return tf.concat([output, res_state.attention], axis=-1), res_state\n\n  def zero_state(self, batch_size, dtype):\n    return self._cell.zero_state(batch_size, dtype)\n'"
models/tacotron.py,22,"b'import tensorflow as tf\nfrom tensorflow.contrib.rnn import GRUCell, MultiRNNCell, OutputProjectionWrapper, ResidualWrapper\nfrom tensorflow.contrib.seq2seq import BasicDecoder, BahdanauAttention, AttentionWrapper\nfrom text.symbols import symbols\nfrom util.infolog import log\nfrom .helpers import TacoTestHelper, TacoTrainingHelper\nfrom .modules import encoder_cbhg, post_cbhg, prenet\nfrom .rnn_wrappers import DecoderPrenetWrapper, ConcatOutputAndAttentionWrapper\n\n\n\nclass Tacotron():\n  def __init__(self, hparams):\n    self._hparams = hparams\n\n\n  def initialize(self, inputs, input_lengths, mel_targets=None, linear_targets=None):\n    \'\'\'Initializes the model for inference.\n\n    Sets ""mel_outputs"", ""linear_outputs"", and ""alignments"" fields.\n\n    Args:\n      inputs: int32 Tensor with shape [N, T_in] where N is batch size, T_in is number of\n        steps in the input time series, and values are character IDs\n      input_lengths: int32 Tensor with shape [N] where N is batch size and values are the lengths\n        of each sequence in inputs.\n      mel_targets: float32 Tensor with shape [N, T_out, M] where N is batch size, T_out is number\n        of steps in the output time series, M is num_mels, and values are entries in the mel\n        spectrogram. Only needed for training.\n      linear_targets: float32 Tensor with shape [N, T_out, F] where N is batch_size, T_out is number\n        of steps in the output time series, F is num_freq, and values are entries in the linear\n        spectrogram. Only needed for training.\n    \'\'\'\n    with tf.variable_scope(\'inference\') as scope:\n      is_training = linear_targets is not None\n      batch_size = tf.shape(inputs)[0]\n      hp = self._hparams\n\n      # Embeddings\n      embedding_table = tf.get_variable(\n        \'embedding\', [len(symbols), hp.embed_depth], dtype=tf.float32,\n        initializer=tf.truncated_normal_initializer(stddev=0.5))\n      embedded_inputs = tf.nn.embedding_lookup(embedding_table, inputs)          # [N, T_in, embed_depth=256]\n\n      # Encoder\n      prenet_outputs = prenet(embedded_inputs, is_training, hp.prenet_depths)    # [N, T_in, prenet_depths[-1]=128]\n      encoder_outputs = encoder_cbhg(prenet_outputs, input_lengths, is_training, # [N, T_in, encoder_depth=256]\n                                     hp.encoder_depth)\n\n      # Attention\n      attention_cell = AttentionWrapper(\n        GRUCell(hp.attention_depth),\n        BahdanauAttention(hp.attention_depth, encoder_outputs),\n        alignment_history=True,\n        output_attention=False)                                                  # [N, T_in, attention_depth=256]\n      \n      # Apply prenet before concatenation in AttentionWrapper.\n      attention_cell = DecoderPrenetWrapper(attention_cell, is_training, hp.prenet_depths)\n\n      # Concatenate attention context vector and RNN cell output into a 2*attention_depth=512D vector.\n      concat_cell = ConcatOutputAndAttentionWrapper(attention_cell)              # [N, T_in, 2*attention_depth=512]\n\n      # Decoder (layers specified bottom to top):\n      decoder_cell = MultiRNNCell([\n          OutputProjectionWrapper(concat_cell, hp.decoder_depth),\n          ResidualWrapper(GRUCell(hp.decoder_depth)),\n          ResidualWrapper(GRUCell(hp.decoder_depth))\n        ], state_is_tuple=True)                                                  # [N, T_in, decoder_depth=256]\n\n      # Project onto r mel spectrograms (predict r outputs at each RNN step):\n      output_cell = OutputProjectionWrapper(decoder_cell, hp.num_mels * hp.outputs_per_step)\n      decoder_init_state = output_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n\n      if is_training:\n        helper = TacoTrainingHelper(inputs, mel_targets, hp.num_mels, hp.outputs_per_step)\n      else:\n        helper = TacoTestHelper(batch_size, hp.num_mels, hp.outputs_per_step)\n\n      (decoder_outputs, _), final_decoder_state, _ = tf.contrib.seq2seq.dynamic_decode(\n        BasicDecoder(output_cell, helper, decoder_init_state),\n        maximum_iterations=hp.max_iters)                                         # [N, T_out/r, M*r]\n\n      # Reshape outputs to be one output per entry\n      mel_outputs = tf.reshape(decoder_outputs, [batch_size, -1, hp.num_mels])   # [N, T_out, M]\n\n      # Add post-processing CBHG:\n      post_outputs = post_cbhg(mel_outputs, hp.num_mels, is_training,            # [N, T_out, postnet_depth=256]\n                               hp.postnet_depth)\n      linear_outputs = tf.layers.dense(post_outputs, hp.num_freq)                # [N, T_out, F]\n\n      # Grab alignments from the final decoder state:\n      alignments = tf.transpose(final_decoder_state[0].alignment_history.stack(), [1, 2, 0])\n\n      self.inputs = inputs\n      self.input_lengths = input_lengths\n      self.mel_outputs = mel_outputs\n      self.linear_outputs = linear_outputs\n      self.alignments = alignments\n      self.mel_targets = mel_targets\n      self.linear_targets = linear_targets\n      log(\'Initialized Tacotron model. Dimensions: \')\n      log(\'  embedding:               %d\' % embedded_inputs.shape[-1])\n      log(\'  prenet out:              %d\' % prenet_outputs.shape[-1])\n      log(\'  encoder out:             %d\' % encoder_outputs.shape[-1])\n      log(\'  attention out:           %d\' % attention_cell.output_size)\n      log(\'  concat attn & out:       %d\' % concat_cell.output_size)\n      log(\'  decoder cell out:        %d\' % decoder_cell.output_size)\n      log(\'  decoder out (%d frames):  %d\' % (hp.outputs_per_step, decoder_outputs.shape[-1]))\n      log(\'  decoder out (1 frame):   %d\' % mel_outputs.shape[-1])\n      log(\'  postnet out:             %d\' % post_outputs.shape[-1])\n      log(\'  linear out:              %d\' % linear_outputs.shape[-1])\n\n\n  def add_loss(self):\n    \'\'\'Adds loss to the model. Sets ""loss"" field. initialize must have been called.\'\'\'\n    with tf.variable_scope(\'loss\') as scope:\n      hp = self._hparams\n      self.mel_loss = tf.reduce_mean(tf.abs(self.mel_targets - self.mel_outputs))\n      l1 = tf.abs(self.linear_targets - self.linear_outputs)\n      # Prioritize loss for frequencies under 3000 Hz.\n      n_priority_freq = int(3000 / (hp.sample_rate * 0.5) * hp.num_freq)\n      self.linear_loss = 0.5 * tf.reduce_mean(l1) + 0.5 * tf.reduce_mean(l1[:,:,0:n_priority_freq])\n      self.loss = self.mel_loss + self.linear_loss\n\n\n  def add_optimizer(self, global_step):\n    \'\'\'Adds optimizer. Sets ""gradients"" and ""optimize"" fields. add_loss must have been called.\n\n    Args:\n      global_step: int32 scalar Tensor representing current global step in training\n    \'\'\'\n    with tf.variable_scope(\'optimizer\') as scope:\n      hp = self._hparams\n      if hp.decay_learning_rate:\n        self.learning_rate = _learning_rate_decay(hp.initial_learning_rate, global_step)\n      else:\n        self.learning_rate = tf.convert_to_tensor(hp.initial_learning_rate)\n      optimizer = tf.train.AdamOptimizer(self.learning_rate, hp.adam_beta1, hp.adam_beta2)\n      gradients, variables = zip(*optimizer.compute_gradients(self.loss))\n      self.gradients = gradients\n      clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n\n      # Add dependency on UPDATE_OPS; otherwise batchnorm won\'t work correctly. See:\n      # https://github.com/tensorflow/tensorflow/issues/1122\n      with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        self.optimize = optimizer.apply_gradients(zip(clipped_gradients, variables),\n          global_step=global_step)\n\n\ndef _learning_rate_decay(init_lr, global_step):\n  # Noam scheme from tensor2tensor:\n  warmup_steps = 4000.0\n  step = tf.cast(global_step + 1, dtype=tf.float32)\n  return init_lr * warmup_steps**0.5 * tf.minimum(step * warmup_steps**-1.5, step**-0.5)\n'"
tests/__init__.py,0,b''
tests/cmudict_test.py,0,"b'import io\nfrom text import cmudict\n\n\ntest_data = \'\'\'\n;;; # CMUdict  --  Major Version: 0.07\n)PAREN  P ER EH N\n\'TIS  T IH Z\nADVERSE  AE0 D V ER1 S\nADVERSE(1)  AE1 D V ER2 S\nADVERSE(2)  AE2 D V ER1 S\nADVERSELY  AE0 D V ER1 S L IY0\nADVERSITY  AE0 D V ER1 S IH0 T IY2\nBARBERSHOP  B AA1 R B ER0 SH AA2 P\nYOU\'LL  Y UW1 L\n\'\'\'\n\n\ndef test_cmudict():\n  c = cmudict.CMUDict(io.StringIO(test_data))\n  assert len(c) == 6\n  assert len(cmudict.valid_symbols) == 84\n  assert c.lookup(\'ADVERSITY\') == [\'AE0 D V ER1 S IH0 T IY2\']\n  assert c.lookup(\'BarberShop\') == [\'B AA1 R B ER0 SH AA2 P\']\n  assert c.lookup(""You\'ll"") == [\'Y UW1 L\']\n  assert c.lookup(""\'tis"") == [\'T IH Z\']\n  assert c.lookup(\'adverse\') == [\n    \'AE0 D V ER1 S\',\n    \'AE1 D V ER2 S\',\n    \'AE2 D V ER1 S\',\n  ]\n  assert c.lookup(\'\') == None\n  assert c.lookup(\'foo\') == None\n  assert c.lookup(\')paren\') == None\n\n\ndef test_cmudict_no_keep_ambiguous():\n  c = cmudict.CMUDict(io.StringIO(test_data), keep_ambiguous=False)\n  assert len(c) == 5\n  assert c.lookup(\'adversity\') == [\'AE0 D V ER1 S IH0 T IY2\']\n  assert c.lookup(\'adverse\') == None\n'"
tests/numbers_test.py,0,"b""from text.numbers import normalize_numbers\n\n\ndef test_normalize_numbers():\n  assert normalize_numbers('1') == 'one'\n  assert normalize_numbers('15') == 'fifteen'\n  assert normalize_numbers('24') == 'twenty-four'\n  assert normalize_numbers('100') == 'one hundred'\n  assert normalize_numbers('101') == 'one hundred one'\n  assert normalize_numbers('456') == 'four hundred fifty-six'\n  assert normalize_numbers('1000') == 'one thousand'\n  assert normalize_numbers('1800') == 'eighteen hundred'\n  assert normalize_numbers('2,000') == 'two thousand'\n  assert normalize_numbers('3000') == 'three thousand'\n  assert normalize_numbers('18000') == 'eighteen thousand'\n  assert normalize_numbers('24,000') == 'twenty-four thousand'\n  assert normalize_numbers('124,001') == 'one hundred twenty-four thousand one'\n  assert normalize_numbers('6.4 sec') == 'six point four sec'\n\n\ndef test_normalize_ordinals():\n  assert normalize_numbers('1st') == 'first'\n  assert normalize_numbers('2nd') == 'second'\n  assert normalize_numbers('9th') == 'ninth'\n  assert normalize_numbers('243rd place') == 'two hundred and forty-third place'\n\n\ndef test_normalize_dates():\n  assert normalize_numbers('1400') == 'fourteen hundred'\n  assert normalize_numbers('1901') == 'nineteen oh one'\n  assert normalize_numbers('1999') == 'nineteen ninety-nine'\n  assert normalize_numbers('2000') == 'two thousand'\n  assert normalize_numbers('2004') == 'two thousand four'\n  assert normalize_numbers('2010') == 'twenty ten'\n  assert normalize_numbers('2012') == 'twenty twelve'\n  assert normalize_numbers('2025') == 'twenty twenty-five'\n  assert normalize_numbers('September 11, 2001') == 'September eleven, two thousand one'\n  assert normalize_numbers('July 26, 1984.') == 'July twenty-six, nineteen eighty-four.'\n\n\ndef test_normalize_money():\n  assert normalize_numbers('$0.00') == 'zero dollars'\n  assert normalize_numbers('$1') == 'one dollar'\n  assert normalize_numbers('$10') == 'ten dollars'\n  assert normalize_numbers('$.01') == 'one cent'\n  assert normalize_numbers('$0.25') == 'twenty-five cents'\n  assert normalize_numbers('$5.00') == 'five dollars'\n  assert normalize_numbers('$5.01') == 'five dollars, one cent'\n  assert normalize_numbers('$135.99.') == 'one hundred thirty-five dollars, ninety-nine cents.'\n  assert normalize_numbers('$40,000') == 'forty thousand dollars'\n  assert normalize_numbers('for \xc2\xa32500!') == 'for twenty-five hundred pounds!'\n"""
tests/text_test.py,0,"b'from text import cleaners, symbols, text_to_sequence, sequence_to_text\nfrom unidecode import unidecode\n\n\ndef test_symbols():\n  assert len(symbols) >= 3\n  assert symbols[0] == \'_\'\n  assert symbols[1] == \'~\'\n\n\ndef test_text_to_sequence():\n  assert text_to_sequence(\'\', []) == [1]\n  assert text_to_sequence(\'Hi!\', []) == [9, 36, 54, 1]\n  assert text_to_sequence(\'""A""_B\', []) == [2, 3, 1]\n  assert text_to_sequence(\'A {AW1 S} B\', []) == [2, 64, 83, 132, 64, 3, 1]\n  assert text_to_sequence(\'Hi\', [\'lowercase\']) == [35, 36, 1]\n  assert text_to_sequence(\'A {AW1 S}  B\', [\'english_cleaners\']) == [28, 64, 83, 132, 64, 29, 1]\n\n\ndef test_sequence_to_text():\n  assert sequence_to_text([]) == \'\'\n  assert sequence_to_text([1]) == \'~\'\n  assert sequence_to_text([9, 36, 54, 1]) == \'Hi!~\'\n  assert sequence_to_text([2, 64, 83, 132, 64, 3]) == \'A {AW1 S} B\'\n\n\ndef test_collapse_whitespace():\n  assert cleaners.collapse_whitespace(\'\') == \'\'\n  assert cleaners.collapse_whitespace(\'  \') == \' \'\n  assert cleaners.collapse_whitespace(\'x\') == \'x\'\n  assert cleaners.collapse_whitespace(\' x.  y,  \\tz\') == \' x. y, z\'\n\n\ndef test_convert_to_ascii():\n  assert cleaners.convert_to_ascii(""raison d\'\xc3\xaatre"") == ""raison d\'etre""\n  assert cleaners.convert_to_ascii(\'gr\xc3\xbc\xc3\x9f gott\') == \'gruss gott\'\n  assert cleaners.convert_to_ascii(\'\xec\x95\x88\xeb\x85\x95\') == \'annyeong\'\n  assert cleaners.convert_to_ascii(\'\xd0\x97\xd0\xb4\xd1\x80\xd0\xb0\xd0\xb2\xd1\x81\xd1\x82\xd0\xb2\xd1\x83\xd0\xb9\xd1\x82\xd0\xb5\') == \'Zdravstvuite\'\n\n\ndef test_lowercase():\n  assert cleaners.lowercase(\'Happy Birthday!\') == \'happy birthday!\'\n  assert cleaners.lowercase(\'CAF\xc3\x89\') == \'caf\xc3\xa9\'\n\n\ndef test_expand_abbreviations():\n  assert cleaners.expand_abbreviations(\'mr. and mrs. smith\') == \'mister and misess smith\'\n\n\ndef test_expand_numbers():\n  assert cleaners.expand_numbers(\'3 apples and 44 pears\') == \'three apples and forty-four pears\'\n  assert cleaners.expand_numbers(\'$3.50 for gas.\') == \'three dollars, fifty cents for gas.\'\n\n\ndef test_cleaner_pipelines():\n  text = \'Mr. M\xc3\xbcller ate  2 Apples\'\n  assert cleaners.english_cleaners(text) == \'mister muller ate two apples\'\n  assert cleaners.transliteration_cleaners(text) == \'mr. muller ate 2 apples\'\n  assert cleaners.basic_cleaners(text) == \'mr. m\xc3\xbcller ate 2 apples\'\n\n'"
text/__init__.py,0,"b'import re\nfrom text import cleaners\nfrom text.symbols import symbols\n\n\n# Mappings from symbol to numeric ID and vice versa:\n_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n\n# Regular expression matching text enclosed in curly braces:\n_curly_re = re.compile(r\'(.*?)\\{(.+?)\\}(.*)\')\n\n\ndef text_to_sequence(text, cleaner_names):\n  \'\'\'Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n\n    The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n    in it. For example, ""Turn left on {HH AW1 S S T AH0 N} Street.""\n\n    Args:\n      text: string to convert to a sequence\n      cleaner_names: names of the cleaner functions to run the text through\n\n    Returns:\n      List of integers corresponding to the symbols in the text\n  \'\'\'\n  sequence = []\n\n  # Check for curly braces and treat their contents as ARPAbet:\n  while len(text):\n    m = _curly_re.match(text)\n    if not m:\n      sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n      break\n    sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n    sequence += _arpabet_to_sequence(m.group(2))\n    text = m.group(3)\n\n  # Append EOS token\n  sequence.append(_symbol_to_id[\'~\'])\n  return sequence\n\n\ndef sequence_to_text(sequence):\n  \'\'\'Converts a sequence of IDs back to a string\'\'\'\n  result = \'\'\n  for symbol_id in sequence:\n    if symbol_id in _id_to_symbol:\n      s = _id_to_symbol[symbol_id]\n      # Enclose ARPAbet back in curly braces:\n      if len(s) > 1 and s[0] == \'@\':\n        s = \'{%s}\' % s[1:]\n      result += s\n  return result.replace(\'}{\', \' \')\n\n\ndef _clean_text(text, cleaner_names):\n  for name in cleaner_names:\n    cleaner = getattr(cleaners, name)\n    if not cleaner:\n      raise Exception(\'Unknown cleaner: %s\' % name)\n    text = cleaner(text)\n  return text\n\n\ndef _symbols_to_sequence(symbols):\n  return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]\n\n\ndef _arpabet_to_sequence(text):\n  return _symbols_to_sequence([\'@\' + s for s in text.split()])\n\n\ndef _should_keep_symbol(s):\n  return s in _symbol_to_id and s is not \'_\' and s is not \'~\'\n'"
text/cleaners.py,0,"b'\'\'\'\nCleaners are transformations that run over the input text at both training and eval time.\n\nCleaners can be selected by passing a comma-delimited list of cleaner names as the ""cleaners""\nhyperparameter. Some cleaners are English-specific. You\'ll typically want to use:\n  1. ""english_cleaners"" for English text\n  2. ""transliteration_cleaners"" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n  3. ""basic_cleaners"" if you do not want to transliterate (in this case, you should also update\n     the symbols in symbols.py to match your data).\n\'\'\'\n\nimport re\nfrom unidecode import unidecode\nfrom .numbers import normalize_numbers\n\n\n# Regular expression matching whitespace:\n_whitespace_re = re.compile(r\'\\s+\')\n\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [(re.compile(\'\\\\b%s\\\\.\' % x[0], re.IGNORECASE), x[1]) for x in [\n  (\'mrs\', \'misess\'),\n  (\'mr\', \'mister\'),\n  (\'dr\', \'doctor\'),\n  (\'st\', \'saint\'),\n  (\'co\', \'company\'),\n  (\'jr\', \'junior\'),\n  (\'maj\', \'major\'),\n  (\'gen\', \'general\'),\n  (\'drs\', \'doctors\'),\n  (\'rev\', \'reverend\'),\n  (\'lt\', \'lieutenant\'),\n  (\'hon\', \'honorable\'),\n  (\'sgt\', \'sergeant\'),\n  (\'capt\', \'captain\'),\n  (\'esq\', \'esquire\'),\n  (\'ltd\', \'limited\'),\n  (\'col\', \'colonel\'),\n  (\'ft\', \'fort\'),\n]]\n\n\ndef expand_abbreviations(text):\n  for regex, replacement in _abbreviations:\n    text = re.sub(regex, replacement, text)\n  return text\n\n\ndef expand_numbers(text):\n  return normalize_numbers(text)\n\n\ndef lowercase(text):\n  return text.lower()\n\n\ndef collapse_whitespace(text):\n  return re.sub(_whitespace_re, \' \', text)\n\n\ndef convert_to_ascii(text):\n  return unidecode(text)\n\n\ndef basic_cleaners(text):\n  \'\'\'Basic pipeline that lowercases and collapses whitespace without transliteration.\'\'\'\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef transliteration_cleaners(text):\n  \'\'\'Pipeline for non-English text that transliterates to ASCII.\'\'\'\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef english_cleaners(text):\n  \'\'\'Pipeline for English text, including number and abbreviation expansion.\'\'\'\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = expand_numbers(text)\n  text = expand_abbreviations(text)\n  text = collapse_whitespace(text)\n  return text\n'"
text/cmudict.py,0,"b'import re\n\n\nvalid_symbols = [\n  \'AA\', \'AA0\', \'AA1\', \'AA2\', \'AE\', \'AE0\', \'AE1\', \'AE2\', \'AH\', \'AH0\', \'AH1\', \'AH2\',\n  \'AO\', \'AO0\', \'AO1\', \'AO2\', \'AW\', \'AW0\', \'AW1\', \'AW2\', \'AY\', \'AY0\', \'AY1\', \'AY2\',\n  \'B\', \'CH\', \'D\', \'DH\', \'EH\', \'EH0\', \'EH1\', \'EH2\', \'ER\', \'ER0\', \'ER1\', \'ER2\', \'EY\',\n  \'EY0\', \'EY1\', \'EY2\', \'F\', \'G\', \'HH\', \'IH\', \'IH0\', \'IH1\', \'IH2\', \'IY\', \'IY0\', \'IY1\',\n  \'IY2\', \'JH\', \'K\', \'L\', \'M\', \'N\', \'NG\', \'OW\', \'OW0\', \'OW1\', \'OW2\', \'OY\', \'OY0\',\n  \'OY1\', \'OY2\', \'P\', \'R\', \'S\', \'SH\', \'T\', \'TH\', \'UH\', \'UH0\', \'UH1\', \'UH2\', \'UW\',\n  \'UW0\', \'UW1\', \'UW2\', \'V\', \'W\', \'Y\', \'Z\', \'ZH\'\n]\n\n_valid_symbol_set = set(valid_symbols)\n\n\nclass CMUDict:\n  \'\'\'Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\'\'\'\n  def __init__(self, file_or_path, keep_ambiguous=True):\n    if isinstance(file_or_path, str):\n      with open(file_or_path, encoding=\'latin-1\') as f:\n        entries = _parse_cmudict(f)\n    else:\n      entries = _parse_cmudict(file_or_path)\n    if not keep_ambiguous:\n      entries = {word: pron for word, pron in entries.items() if len(pron) == 1}\n    self._entries = entries\n\n\n  def __len__(self):\n    return len(self._entries)\n\n\n  def lookup(self, word):\n    \'\'\'Returns list of ARPAbet pronunciations of the given word.\'\'\'\n    return self._entries.get(word.upper())\n\n\n\n_alt_re = re.compile(r\'\\([0-9]+\\)\')\n\n\ndef _parse_cmudict(file):\n  cmudict = {}\n  for line in file:\n    if len(line) and (line[0] >= \'A\' and line[0] <= \'Z\' or line[0] == ""\'""):\n      parts = line.split(\'  \')\n      word = re.sub(_alt_re, \'\', parts[0])\n      pronunciation = _get_pronunciation(parts[1])\n      if pronunciation:\n        if word in cmudict:\n          cmudict[word].append(pronunciation)\n        else:\n          cmudict[word] = [pronunciation]\n  return cmudict\n\n\ndef _get_pronunciation(s):\n  parts = s.strip().split(\' \')\n  for part in parts:\n    if part not in _valid_symbol_set:\n      return None\n  return \' \'.join(parts)\n'"
text/numbers.py,0,"b""import inflect\nimport re\n\n\n_inflect = inflect.engine()\n_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n_pounds_re = re.compile(r'\xc2\xa3([0-9\\,]*[0-9]+)')\n_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n_ordinal_re = re.compile(r'[0-9]+(st|nd|rd|th)')\n_number_re = re.compile(r'[0-9]+')\n\n\ndef _remove_commas(m):\n  return m.group(1).replace(',', '')\n\n\ndef _expand_decimal_point(m):\n  return m.group(1).replace('.', ' point ')\n\n\ndef _expand_dollars(m):\n  match = m.group(1)\n  parts = match.split('.')\n  if len(parts) > 2:\n    return match + ' dollars'  # Unexpected format\n  dollars = int(parts[0]) if parts[0] else 0\n  cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n  if dollars and cents:\n    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n    cent_unit = 'cent' if cents == 1 else 'cents'\n    return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n  elif dollars:\n    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n    return '%s %s' % (dollars, dollar_unit)\n  elif cents:\n    cent_unit = 'cent' if cents == 1 else 'cents'\n    return '%s %s' % (cents, cent_unit)\n  else:\n    return 'zero dollars'\n\n\ndef _expand_ordinal(m):\n  return _inflect.number_to_words(m.group(0))\n\n\ndef _expand_number(m):\n  num = int(m.group(0))\n  if num > 1000 and num < 3000:\n    if num == 2000:\n      return 'two thousand'\n    elif num > 2000 and num < 2010:\n      return 'two thousand ' + _inflect.number_to_words(num % 100)\n    elif num % 100 == 0:\n      return _inflect.number_to_words(num // 100) + ' hundred'\n    else:\n      return _inflect.number_to_words(num, andword='', zero='oh', group=2).replace(', ', ' ')\n  else:\n    return _inflect.number_to_words(num, andword='')\n\n\ndef normalize_numbers(text):\n  text = re.sub(_comma_number_re, _remove_commas, text)\n  text = re.sub(_pounds_re, r'\\1 pounds', text)\n  text = re.sub(_dollars_re, _expand_dollars, text)\n  text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n  text = re.sub(_ordinal_re, _expand_ordinal, text)\n  text = re.sub(_number_re, _expand_number, text)\n  return text\n"""
text/symbols.py,0,"b'\'\'\'\nDefines the set of symbols used in text input to the model.\n\nThe default is a set of ASCII characters that works well for English or text that has been run\nthrough Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details.\n\'\'\'\nfrom text import cmudict\n\n_pad        = \'_\'\n_eos        = \'~\'\n_characters = \'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\\'(),-.:;? \'\n\n# Prepend ""@"" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n_arpabet = [\'@\' + s for s in cmudict.valid_symbols]\n\n# Export all symbols:\nsymbols = [_pad, _eos] + list(_characters) + _arpabet\n'"
util/__init__.py,0,"b'class ValueWindow():\n  def __init__(self, window_size=100):\n    self._window_size = window_size\n    self._values = []\n\n  def append(self, x):\n    self._values = self._values[-(self._window_size - 1):] + [x]\n\n  @property\n  def sum(self):\n    return sum(self._values)\n\n  @property\n  def count(self):\n    return len(self._values)\n\n  @property\n  def average(self):\n    return self.sum / max(1, self.count)\n\n  def reset(self):\n    self._values = []\n'"
util/audio.py,10,"b""import librosa\nimport librosa.filters\nimport math\nimport numpy as np\nimport tensorflow as tf\nimport scipy\nfrom hparams import hparams\n\n\ndef load_wav(path):\n  return librosa.core.load(path, sr=hparams.sample_rate)[0]\n\n\ndef save_wav(wav, path):\n  wav *= 32767 / max(0.01, np.max(np.abs(wav)))\n  scipy.io.wavfile.write(path, hparams.sample_rate, wav.astype(np.int16))\n\n\ndef preemphasis(x):\n  return scipy.signal.lfilter([1, -hparams.preemphasis], [1], x)\n\n\ndef inv_preemphasis(x):\n  return scipy.signal.lfilter([1], [1, -hparams.preemphasis], x)\n\n\ndef spectrogram(y):\n  D = _stft(preemphasis(y))\n  S = _amp_to_db(np.abs(D)) - hparams.ref_level_db\n  return _normalize(S)\n\n\ndef inv_spectrogram(spectrogram):\n  '''Converts spectrogram to waveform using librosa'''\n  S = _db_to_amp(_denormalize(spectrogram) + hparams.ref_level_db)  # Convert back to linear\n  return inv_preemphasis(_griffin_lim(S ** hparams.power))          # Reconstruct phase\n\n\ndef inv_spectrogram_tensorflow(spectrogram):\n  '''Builds computational graph to convert spectrogram to waveform using TensorFlow.\n\n  Unlike inv_spectrogram, this does NOT invert the preemphasis. The caller should call\n  inv_preemphasis on the output after running the graph.\n  '''\n  S = _db_to_amp_tensorflow(_denormalize_tensorflow(spectrogram) + hparams.ref_level_db)\n  return _griffin_lim_tensorflow(tf.pow(S, hparams.power))\n\n\ndef melspectrogram(y):\n  D = _stft(preemphasis(y))\n  S = _amp_to_db(_linear_to_mel(np.abs(D))) - hparams.ref_level_db\n  return _normalize(S)\n\n\ndef find_endpoint(wav, threshold_db=-40, min_silence_sec=0.8):\n  window_length = int(hparams.sample_rate * min_silence_sec)\n  hop_length = int(window_length / 4)\n  threshold = _db_to_amp(threshold_db)\n  for x in range(hop_length, len(wav) - window_length, hop_length):\n    if np.max(wav[x:x+window_length]) < threshold:\n      return x + hop_length\n  return len(wav)\n\n\ndef _griffin_lim(S):\n  '''librosa implementation of Griffin-Lim\n  Based on https://github.com/librosa/librosa/issues/434\n  '''\n  angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n  S_complex = np.abs(S).astype(np.complex)\n  y = _istft(S_complex * angles)\n  for i in range(hparams.griffin_lim_iters):\n    angles = np.exp(1j * np.angle(_stft(y)))\n    y = _istft(S_complex * angles)\n  return y\n\n\ndef _griffin_lim_tensorflow(S):\n  '''TensorFlow implementation of Griffin-Lim\n  Based on https://github.com/Kyubyong/tensorflow-exercises/blob/master/Audio_Processing.ipynb\n  '''\n  with tf.variable_scope('griffinlim'):\n    # TensorFlow's stft and istft operate on a batch of spectrograms; create batch of size 1\n    S = tf.expand_dims(S, 0)\n    S_complex = tf.identity(tf.cast(S, dtype=tf.complex64))\n    y = _istft_tensorflow(S_complex)\n    for i in range(hparams.griffin_lim_iters):\n      est = _stft_tensorflow(y)\n      angles = est / tf.cast(tf.maximum(1e-8, tf.abs(est)), tf.complex64)\n      y = _istft_tensorflow(S_complex * angles)\n    return tf.squeeze(y, 0)\n\n\ndef _stft(y):\n  n_fft, hop_length, win_length = _stft_parameters()\n  return librosa.stft(y=y, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n\n\ndef _istft(y):\n  _, hop_length, win_length = _stft_parameters()\n  return librosa.istft(y, hop_length=hop_length, win_length=win_length)\n\n\ndef _stft_tensorflow(signals):\n  n_fft, hop_length, win_length = _stft_parameters()\n  return tf.contrib.signal.stft(signals, win_length, hop_length, n_fft, pad_end=False)\n\n\ndef _istft_tensorflow(stfts):\n  n_fft, hop_length, win_length = _stft_parameters()\n  return tf.contrib.signal.inverse_stft(stfts, win_length, hop_length, n_fft)\n\n\ndef _stft_parameters():\n  n_fft = (hparams.num_freq - 1) * 2\n  hop_length = int(hparams.frame_shift_ms / 1000 * hparams.sample_rate)\n  win_length = int(hparams.frame_length_ms / 1000 * hparams.sample_rate)\n  return n_fft, hop_length, win_length\n\n\n# Conversions:\n\n_mel_basis = None\n\ndef _linear_to_mel(spectrogram):\n  global _mel_basis\n  if _mel_basis is None:\n    _mel_basis = _build_mel_basis()\n  return np.dot(_mel_basis, spectrogram)\n\ndef _build_mel_basis():\n  n_fft = (hparams.num_freq - 1) * 2\n  return librosa.filters.mel(hparams.sample_rate, n_fft, n_mels=hparams.num_mels)\n\ndef _amp_to_db(x):\n  return 20 * np.log10(np.maximum(1e-5, x))\n\ndef _db_to_amp(x):\n  return np.power(10.0, x * 0.05)\n\ndef _db_to_amp_tensorflow(x):\n  return tf.pow(tf.ones(tf.shape(x)) * 10.0, x * 0.05)\n\ndef _normalize(S):\n  return np.clip((S - hparams.min_level_db) / -hparams.min_level_db, 0, 1)\n\ndef _denormalize(S):\n  return (np.clip(S, 0, 1) * -hparams.min_level_db) + hparams.min_level_db\n\ndef _denormalize_tensorflow(S):\n  return (tf.clip_by_value(S, 0, 1) * -hparams.min_level_db) + hparams.min_level_db\n"""
util/infolog.py,0,"b'import atexit\nfrom datetime import datetime\nimport json\nfrom threading import Thread\nfrom urllib.request import Request, urlopen\n\n\n_format = \'%Y-%m-%d %H:%M:%S.%f\'\n_file = None\n_run_name = None\n_slack_url = None\n\n\ndef init(filename, run_name, slack_url=None):\n  global _file, _run_name, _slack_url\n  _close_logfile()\n  _file = open(filename, \'a\', encoding=""utf-8"")\n  _file.write(\'\\n-----------------------------------------------------------------\\n\')\n  _file.write(\'Starting new training run\\n\')\n  _file.write(\'-----------------------------------------------------------------\\n\')\n  _run_name = run_name\n  _slack_url = slack_url\n\n\ndef log(msg, slack=False):\n  print(msg)\n  if _file is not None:\n    _file.write(\'[%s]  %s\\n\' % (datetime.now().strftime(_format)[:-3], msg))\n  if slack and _slack_url is not None:\n    Thread(target=_send_slack, args=(msg,)).start()\n\n\ndef _close_logfile():\n  global _file\n  if _file is not None:\n    _file.close()\n    _file = None\n\n\ndef _send_slack(msg):\n  req = Request(_slack_url)\n  req.add_header(\'Content-Type\', \'application/json\')\n  urlopen(req, json.dumps({\n    \'username\': \'tacotron\',\n    \'icon_emoji\': \':taco:\',\n    \'text\': \'*%s*: %s\' % (_run_name, msg)\n  }).encode())\n\n\natexit.register(_close_logfile)\n'"
util/plot.py,0,"b""import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\n\ndef plot_alignment(alignment, path, info=None):\n  fig, ax = plt.subplots()\n  im = ax.imshow(\n    alignment,\n    aspect='auto',\n    origin='lower',\n    interpolation='none')\n  fig.colorbar(im, ax=ax)\n  xlabel = 'Decoder timestep'\n  if info is not None:\n    xlabel += '\\n\\n' + info\n  plt.xlabel(xlabel)\n  plt.ylabel('Encoder timestep')\n  plt.tight_layout()\n  plt.savefig(path, format='png')\n"""
