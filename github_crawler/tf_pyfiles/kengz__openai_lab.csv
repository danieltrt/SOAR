file_path,api_count,code
main.py,0,"b""from rl.experiment import run\nfrom rl.util import args\n\nif __name__ == '__main__':\n    run(args.experiment, **vars(args))\n"""
setup.py,0,"b'import os\nimport sys\nfrom setuptools import setup\nfrom setuptools.command.test import test as TestCommand\n\n\n# explicitly config\ntest_args = [\n    \'-n 4\',\n    \'--cov-report=term\',\n    \'--cov-report=html\',\n    \'--cov=rl\',\n    \'test\'\n]\n\n\nclass PyTest(TestCommand):\n    user_options = [(\'pytest-args=\', \'a\', ""Arguments to pass to py.test"")]\n\n    def initialize_options(self):\n        TestCommand.initialize_options(self)\n        self.pytest_args = test_args\n\n    def run_tests(self):\n        # import here, cause outside the eggs aren\'t loaded\n        import pytest\n        errno = pytest.main(self.pytest_args)\n        sys.exit(errno)\n\n\n# Utility function to read the README file.\ndef read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname)).read()\n\n\n# the setup\nsetup(\n    name=\'openai_lab\',\n    version=\'1.0.0\',\n    description=\'An experimentation system for Reinforcement Learning using OpenAI and Keras\',\n    long_description=read(\'README.md\'),\n    keywords=\'openai gym\',\n    url=\'https://github.com/kengz/openai_lab\',\n    author=\'kengz,lgraesser\',\n    author_email=\'kengzwl@gmail.com\',\n    license=\'MIT\',\n    packages=[],\n    zip_safe=False,\n    include_package_data=True,\n    install_requires=[],\n    dependency_links=[],\n    extras_require={\n        \'dev\': [],\n        \'docs\': [],\n        \'testing\': []\n    },\n    classifiers=[],\n    tests_require=[\'pytest\', \'pytest-cov\'],\n    test_suite=\'test\',\n    cmdclass={\'test\': PyTest}\n)\n'"
rl/__init__.py,0,"b'# curse of python pathing, hack to solve rel import\nimport glob\nimport sys\nfrom os import path\nfrom os.path import dirname, basename, isfile\nfile_path = path.normpath(path.join(path.dirname(__file__)))\nsys.path.insert(0, file_path)\n\n# another py curse, expose to prevent \'agent.<agent>\' call\npattern = ""/*.py""\nmodules = glob.glob(dirname(__file__) + pattern)\n__all__ = [basename(f)[:-3] for f in modules if isfile(f)]\n'"
rl/analytics.py,0,"b'import json\nimport numpy as np\nimport pandas as pd\nimport platform\nimport warnings\nfrom os import environ\nfrom rl.util import *\n\nwarnings.filterwarnings(""ignore"", module=""matplotlib"")\n\nif platform.system() == \'Darwin\':\n    MPL_BACKEND = \'agg\' if args.param_selection else \'macosx\'\nelse:\n    MPL_BACKEND = \'TkAgg\'\n\nSTATS_COLS = [\n    \'best_session_epi\',\n    \'best_session_id\',\n    \'best_session_mean_rewards\',\n    \'best_session_stability\',\n    \'fitness_score\',\n    \'mean_rewards_per_epi_stats_mean\',\n    \'mean_rewards_stats_mean\',\n    \'mean_rewards_stats_max\',\n    \'epi_stats_mean\',\n    \'epi_stats_min\',\n    \'solved_ratio_of_sessions\',\n    \'max_total_rewards_stats_mean\',\n    \'trial_id\',\n]\n\nEXPERIMENT_DATA_Y_COLS = [\n    \'fitness_score\',\n    \'mean_rewards_stats_max\',\n    \'max_total_rewards_stats_mean\',\n    \'epi_stats_min\',\n]\n\n\n# import matplotlib scoped to the class for gc in multiprocessing\ndef scoped_mpl_import():\n    import matplotlib\n    matplotlib.rcParams[\'backend\'] = MPL_BACKEND\n\n    import matplotlib.pyplot as plt\n    plt.rcParams[\'toolbar\'] = \'None\'  # mute matplotlib toolbar\n\n    import seaborn as sns\n    sns.set(style=""whitegrid"", color_codes=True, font_scale=1.0,\n            rc={\'lines.linewidth\': 1.0,\n                \'backend\': matplotlib.rcParams[\'backend\']})\n    palette = sns.color_palette(""Blues_d"")\n    palette.reverse()\n    sns.set_palette(palette)\n\n    return (matplotlib, plt, sns)\n\n\nclass Grapher(object):\n\n    \'\'\'\n    Grapher object that belongs to a Session\n    to draw graphs from its data\n    \'\'\'\n\n    def __init__(self, session):\n        if environ.get(\'CI\'):\n            return\n        (_mpl, self.plt, _sns) = scoped_mpl_import()\n        self.session = session\n        self.graph_filename = self.session.graph_filename\n        self.subgraphs = {}\n        self.figure = self.plt.figure(facecolor=\'white\', figsize=(8, 9))\n        self.figure.suptitle(wrap_text(self.session.session_id))\n        self.init_figure()\n\n    def init_figure(self):\n        # graph 1\n        ax1 = self.figure.add_subplot(\n            311,\n            frame_on=False,\n            title=""\\n\\ntotal rewards per episode"",\n            ylabel=\'total rewards\')\n        p1, = ax1.plot([], [])\n        self.subgraphs[\'total rewards\'] = (ax1, p1)\n\n        ax1e = ax1.twinx()\n        ax1e.set_ylabel(\'exploration rate\').set_color(\'r\')\n        ax1e.set_frame_on(False)\n        ax1e.grid(False)\n        p1e, = ax1e.plot([], [], \'r\')\n        self.subgraphs[\'e\'] = (ax1e, p1e)\n\n        # graph 2\n        ax2 = self.figure.add_subplot(\n            312,\n            frame_on=False,\n            title=\'mean rewards over last 100 episodes\',\n            ylabel=\'mean rewards\')\n        p2, = ax2.plot([], [], \'g\')\n        self.subgraphs[\'mean rewards\'] = (ax2, p2)\n\n        # graph 3\n        ax3 = self.figure.add_subplot(\n            313,\n            frame_on=False,\n            title=\'loss over time, episode\',\n            ylabel=\'loss\')\n        p3, = ax3.plot([], [])\n        self.subgraphs[\'loss\'] = (ax3, p3)\n\n        self.plt.tight_layout()  # auto-fix spacing\n        self.plt.ion()  # for live plot\n\n    def plot(self):\n        \'\'\'do live plotting\'\'\'\n        if environ.get(\'CI\'):\n            return\n        sys_vars = self.session.sys_vars\n        ax1, p1 = self.subgraphs[\'total rewards\']\n        p1.set_ydata(sys_vars[\'total_rewards_history\'])\n        p1.set_xdata(np.arange(len(p1.get_ydata())))\n        ax1.relim()\n        ax1.autoscale_view(tight=True, scalex=True, scaley=True)\n\n        ax1e, p1e = self.subgraphs[\'e\']\n        p1e.set_ydata(sys_vars[\'explore_history\'])\n        p1e.set_xdata(np.arange(len(p1e.get_ydata())))\n        ax1e.relim()\n        ax1e.autoscale_view(tight=True, scalex=True, scaley=True)\n\n        ax2, p2 = self.subgraphs[\'mean rewards\']\n        p2.set_ydata(sys_vars[\'mean_rewards_history\'])\n        p2.set_xdata(np.arange(len(p2.get_ydata())))\n        ax2.relim()\n        ax2.autoscale_view(tight=True, scalex=True, scaley=True)\n\n        ax3, p3 = self.subgraphs[\'loss\']\n        p3.set_ydata(sys_vars[\'loss\'])\n        p3.set_xdata(np.arange(len(p3.get_ydata())))\n        ax3.relim()\n        ax3.autoscale_view(tight=True, scalex=True, scaley=True)\n\n        self.plt.draw()\n        self.plt.pause(0.01)\n        self.save()\n        import gc\n        gc.collect()\n\n    def save(self):\n        \'\'\'save graph to filename\'\'\'\n        self.figure.savefig(self.graph_filename)\n\n    def clear(self):\n        if environ.get(\'CI\'):\n            return\n        self.plt.close()\n        del_self_attr(self)\n\n\ndef calc_stability(sys_vars):\n    \'\'\'\n    calculate the stability of a session using its sys_vars\n    when problem is unsolved (unbounded), use 1 sigma 95% of max\n    stability 1 = perfectly stable\n    0.5 = half-ish unstable\n    0 = totally unstable, cannot yield solution\n    \'\'\'\n    total_r_history = sys_vars[\'total_rewards_history\']\n    if sys_vars[\'SOLVED_MEAN_REWARD\'] is None:\n        min_rewards = min(total_r_history)\n        max_rewards = max(total_r_history)\n        rewards_gap = max_rewards - min_rewards\n        r_threshold = max_rewards - (0.10 * rewards_gap)\n    else:\n        r_threshold = sys_vars[\'SOLVED_MEAN_REWARD\']\n    # find index i.e. epi of first solved\n    first_solved_epi = next(\n        (idx for idx, total_r in enumerate(total_r_history)\n            if total_r > r_threshold), None)\n    last_epi = sys_vars[\'epi\']\n    stable_epi_count = len([\n        total_r for total_r in total_r_history if total_r > r_threshold])\n\n    if (first_solved_epi is None) or (last_epi == first_solved_epi):\n        mastery_gap = np.inf\n    else:  # get max if mastery_gap is smaller (faster) than needed - perfect\n        mastery_gap = last_epi - first_solved_epi\n    stability = stable_epi_count / mastery_gap\n    return stability\n\n\ndef fitness_score(stats):\n    \'\'\'\n    calculate the fitness score (see doc Metrics for more)\n    1. solution rewards\n    2. solving speed: /epi\n    3. stability\n    4. consistency\n    5. granularity\n    6. amplification of good results\n    7. distinguishability\n    \'\'\'\n    mean_rewards_per_epi = stats[\'mean_rewards_per_epi_stats\'][\'mean\']\n    stability = stats[\'stability_stats\'][\'mean\']\n    consistency = stats[\'solved_ratio_of_sessions\']\n    amplifier = (1+stability)*((1+consistency)**2)\n    distinguisher = amplifier ** np.sign(mean_rewards_per_epi)\n    fitness = mean_rewards_per_epi * distinguisher\n    return fitness\n\n\ndef ideal_fitness_score(problem):\n    \'\'\'\n    calculate the ideal fitness_score with perfect solved ratio\n    for hyperparameter optimization to select\n    \'\'\'\n    if problem[\'SOLVED_MEAN_REWARD\'] is None:\n        return np.inf  # for unsolved environments\n    solved_mean_reward = problem[\'SOLVED_MEAN_REWARD\']\n    max_episodes = problem[\'MAX_EPISODES\']\n    solved_epi_speedup = 3\n    ideal_epi = max_episodes / solved_epi_speedup\n    ideal_mean_rewards_per_epi = solved_mean_reward / ideal_epi\n    ideal_stability = 1\n    ideal_consistency = 1\n    amplifier = (1+ideal_stability)*((1+ideal_consistency)**2)\n    distinguisher = amplifier ** np.sign(ideal_mean_rewards_per_epi)\n    ideal_fitness = ideal_mean_rewards_per_epi * distinguisher\n    return ideal_fitness\n\n\ndef basic_stats(array):\n    \'\'\'generate the basic stats for a numerical array\'\'\'\n    if not len(array):\n        return None\n    return {\n        \'min\': np.min(array).astype(float),\n        \'max\': np.max(array).astype(float),\n        \'mean\': np.mean(array).astype(float),\n        \'std\': np.std(array).astype(float),\n    }\n\n\ndef compose_data(trial):\n    \'\'\'\n    compose raw data from an trial object\n    into useful summary and full metrics for analysis\n    \'\'\'\n    sys_vars_array = trial.data[\'sys_vars_array\']\n\n    # collect all data from sys_vars_array\n    solved_sys_vars_array = list(filter(\n        lambda sv: sv[\'solved\'], sys_vars_array))\n    errored_array = list(map(\n        lambda sv: sv[\'errored\'], sys_vars_array))\n    mean_rewards_array = np.array(list(map(\n        lambda sv: sv[\'mean_rewards\'], sys_vars_array)))\n    max_total_rewards_array = np.array(list(map(\n        lambda sv: np.max(sv[\'total_rewards_history\']), sys_vars_array)))\n    epi_array = np.array(list(map(lambda sv: sv[\'epi\'], sys_vars_array)))\n    mean_rewards_per_epi_array = np.divide(mean_rewards_array, epi_array + 1)\n    stability_array = list(map(calc_stability, sys_vars_array))\n    t_array = np.array(list(map(lambda sv: sv[\'t\'], sys_vars_array)))\n    time_taken_array = np.array(list(map(\n        lambda sv: timestamp_elapse_to_seconds(sv[\'time_taken\']),\n        sys_vars_array)))\n    solved_epi_array = np.array(list(map(\n        lambda sv: sv[\'epi\'], solved_sys_vars_array)))\n    solved_t_array = np.array(list(map(\n        lambda sv: sv[\'t\'], solved_sys_vars_array)))\n    solved_time_taken_array = np.array(list(map(\n        lambda sv: timestamp_elapse_to_seconds(sv[\'time_taken\']),\n        solved_sys_vars_array)))\n    best_idx = list(mean_rewards_per_epi_array).index(\n        max(mean_rewards_per_epi_array))\n    best_session_id = \'{}_s{}\'.format(trial.data[\'trial_id\'], best_idx)\n\n    # compose sys_vars stats\n    stats = {\n        \'best_session_epi\': epi_array.tolist()[best_idx],\n        \'best_session_id\': best_session_id,\n        \'best_session_mean_rewards\': mean_rewards_array[best_idx],\n        \'best_session_stability\': stability_array[best_idx],\n        \'errored\': any(errored_array),\n        \'epi_stats\': basic_stats(epi_array),\n        \'max_total_rewards_stats\': basic_stats(max_total_rewards_array),\n        \'mean_rewards_stats\': basic_stats(mean_rewards_array),\n        \'mean_rewards_per_epi_stats\': basic_stats(\n            mean_rewards_per_epi_array),\n        \'num_of_sessions\': len(sys_vars_array),\n        \'solved_epi_stats\': basic_stats(solved_epi_array),\n        \'solved_num_of_sessions\': len(solved_sys_vars_array),\n        \'solved_ratio_of_sessions\': float(len(\n            solved_sys_vars_array)) / trial.times,\n        \'solved_t_stats\': basic_stats(solved_t_array),\n        \'solved_time_taken_stats\': basic_stats(solved_time_taken_array),\n        \'stability_stats\': basic_stats(stability_array),\n        \'t_stats\': basic_stats(t_array),\n        \'time_taken_stats\': basic_stats(time_taken_array),\n    }\n    stats.update({\n        \'fitness_score\': fitness_score(stats)\n    })\n\n    # summary metrics picked from stats\n    metrics = {\n        \'best_session_epi\': stats[\'best_session_epi\'],\n        \'best_session_id\': stats[\'best_session_id\'],\n        \'best_session_mean_rewards\': stats[\'best_session_mean_rewards\'],\n        \'best_session_stability\': stats[\'best_session_stability\'],\n        \'fitness_score\': stats[\'fitness_score\'],\n        \'mean_rewards_per_epi_stats_mean\': stats[\n            \'mean_rewards_per_epi_stats\'][\'mean\'],\n        \'solved_ratio_of_sessions\': stats[\'solved_ratio_of_sessions\'],\n        \'t_stats_mean\': stats[\'t_stats\'][\'mean\'],\n    }\n\n    # param variables for independent vars of trials\n    default_param = trial.experiment_spec[\'param\']\n    param_variables = {\n        pv: default_param[pv] for\n        pv in trial.param_variables if pv in default_param}\n\n    trial.data[\'metrics\'].update(metrics)\n    trial.data[\'param_variables\'] = param_variables\n    trial.data[\'stats\'] = stats\n    return trial.data\n\n\n# order a unique df categorical data for plotting\ndef order_category(uniq_df):\n    uniq_list = list(uniq_df)\n    try:\n        uniq_dict = {k: json.loads(k) for k in uniq_list}\n        sorted_pair = sorted(uniq_dict.items(), key=lambda x: x[1])\n        return [pair[0] for pair in sorted_pair]\n    except (json.JSONDecodeError, TypeError):\n        return list(sorted(uniq_list))\n\n\n# plot the experiment data from data_df\n# X are columns with name starting with \'variable_\'\n# Y cols are defined below\ndef plot_experiment(data_df, trial_id):\n    if len(data_df) < 2:  # no multi selection\n        return\n    (_mpl, _plt, sns) = scoped_mpl_import()\n    experiment_id = parse_experiment_id(trial_id)\n    hue = \'solved_ratio_of_sessions\'\n    data_df = data_df.sort_values(hue)\n    fitness_hue = \'fitness_score_bin\'\n    data_df[fitness_hue] = pd.cut(data_df[\'fitness_score\'], bins=5)\n    X_cols = list(filter(lambda c: c.startswith(\'variable_\'), data_df.columns))\n    col_size = len(X_cols)\n    row_size = len(EXPERIMENT_DATA_Y_COLS)\n    groups = data_df.groupby(hue)\n\n    # for main grid plot\n    sns_only = True\n    big_fig, axes = sns.plt.subplots(\n        row_size, col_size, figsize=(col_size*4, row_size*3),\n        sharex=\'col\', sharey=\'row\')\n    for ix, x in enumerate(X_cols):\n        for iy, y in enumerate(EXPERIMENT_DATA_Y_COLS):\n            big_ax = axes[iy] if col_size == 1 else axes[iy][ix]\n            uniq_df = data_df[x].unique()\n            if (data_df[x].dtype.name == \'category\' or\n                    len(uniq_df) <= 5):\n                order = order_category(uniq_df)\n                sns.swarmplot(\n                    data=data_df, x=x, y=y, hue=hue, size=3,\n                    order=order, ax=big_ax)\n            else:\n                sns_only = False\n                big_ax.margins(0.05)\n                big_ax.xaxis.grid(False)\n                for _, group in groups:\n                    big_ax.plot(group[x], group[y], label=hue,\n                                marker=\'o\', ms=3, linestyle=\'\')\n                    big_ax.set_xlabel(x)\n                    big_ax.set_ylabel(y)\n\n            big_ax.legend_ = None  # set common legend below\n            # label only left and bottom axes\n            if iy != row_size - 1:\n                big_ax.set_xlabel(\'\')\n            if ix != 0:\n                big_ax.set_ylabel(\'\')\n\n    big_fig.tight_layout()\n    big_fig.suptitle(wrap_text(experiment_id))\n    legend_labels = None if sns_only else sorted(data_df[hue].unique())\n    legend_ms = 0.5 if sns_only else 1\n    legend = sns.plt.legend(title=\'solved_ratio_of_sessions\',\n                            labels=legend_labels, markerscale=legend_ms,\n                            fontsize=10, loc=\'center right\',\n                            bbox_to_anchor=(1.1+col_size*0.1, row_size+0.1))\n    legend.get_title().set_fontsize(\'10\')\n    big_fig.subplots_adjust(top=0.96, right=0.9)\n\n    filename = \'./data/{0}/{0}_analysis.png\'.format(\n        experiment_id)\n    big_fig.savefig(filename)\n    big_fig.clear()\n\n    # use numerical, since contour only makes sense for ordered azes\n    numerics = [\'int16\', \'int32\', \'int64\', \'float16\', \'float32\', \'float64\']\n    numeric_X_cols = list(\n        filter(lambda x: data_df[x].dtype in numerics, X_cols))\n    with sns.axes_style(\'white\', {\'axes.linewidth\': 0.2}):\n        g = sns.pairplot(\n            data_df, vars=numeric_X_cols, hue=fitness_hue,\n            size=3, aspect=1, plot_kws={\'s\': 50, \'alpha\': 0.5})\n        g.fig.suptitle(wrap_text(experiment_id))\n        g = g.add_legend()\n        filename = \'./data/{0}/{0}_analysis_correlation.png\'.format(\n            experiment_id)\n        g.savefig(filename)\n        g.fig.clear()\n\n    sns.plt.close()\n\n\ndef analyze_data(experiment_data_or_experiment_id):\n    \'\'\'\n    get all the data from all trials.run()\n    or read from all data files matching the prefix of trial_id\n    e.g. usage without running:\n    experiment_id = \'DevCartPole-v0_DQN_LinearMemoryWithForgetting_BoltzmannPolicy_2017-01-15_142810\'\n    analyze_data(experiment_id)\n    \'\'\'\n    if isinstance(experiment_data_or_experiment_id, str):\n        experiment_data = load_data_array_from_experiment_id(\n            experiment_data_or_experiment_id)\n    else:\n        experiment_data = experiment_data_or_experiment_id\n\n    stats_array, param_variables_array = [], []\n    for data in experiment_data:\n        stats = flatten_dict(data[\'stats\'])\n        stats.update({\'trial_id\': data[\'trial_id\']})\n        param_variables = flat_cast_dict(data[\'param_variables\'])\n        if stats[\'errored\']:  # remove errored trials\n            continue\n        stats_array.append(stats)\n        param_variables_array.append(param_variables)\n\n    raw_stats_df = pd.DataFrame.from_dict(stats_array)\n    stats_df = raw_stats_df[STATS_COLS]\n\n    param_variables_df = pd.DataFrame.from_dict(param_variables_array)\n    param_variables_df.columns = [\n        \'variable_\'+c for c in param_variables_df.columns]\n\n    data_df = pd.concat([stats_df, param_variables_df], axis=1)\n    for c in data_df.columns:\n        if data_df[c].dtype == object:  # guard\n            data_df[c] = data_df[c].astype(\'category\')\n\n    data_df.sort_values(\n        [\'fitness_score\'], ascending=False, inplace=True)\n    data_df.reset_index(drop=True, inplace=True)\n\n    trial_id = experiment_data[0][\'trial_id\']\n    save_experiment_data(data_df, trial_id)\n    plot_experiment(data_df, trial_id)\n    return data_df\n'"
rl/experiment.py,0,"b'# The trial logic and analysis\nRAND_SEED = 42\nimport numpy as np\nnp.random.seed(RAND_SEED)\nnp.seterr(all=\'raise\')\nimport copy\nimport gym\nimport traceback\nfrom os import environ, makedirs\nfrom rl.util import *\nfrom rl.agent import *\nfrom rl.analytics import *\nfrom rl.hyperoptimizer import *\nfrom rl.memory import *\nfrom rl.optimizer import *\nfrom rl.policy import *\nfrom rl.preprocessor import *\n\nGREF = globals()\n\n# the keys and their defaults need to be implemented by a sys_var\n# the constants (capitalized) are problem configs,\n# set in asset/problems.json\nREQUIRED_SYS_KEYS = {\n    \'RENDER\': None,\n    \'GYM_ENV_NAME\': None,\n    \'SOLVED_MEAN_REWARD\': None,\n    \'MAX_EPISODES\': None,\n    \'REWARD_MEAN_LEN\': None,\n    \'epi\': 0,\n    \'t\': 0,\n    \'done\': False,\n    \'loss\': [],\n    \'total_rewards_history\': [],\n    \'explore_history\': [],\n    \'mean_rewards_history\': [],\n    \'mean_rewards\': 0,\n    \'total_rewards\': 0,\n    \'solved\': False,\n    \'errored\': False,\n}\n\n\nclass Session(object):\n\n    \'\'\'\n    The base unit of an Trial\n    An Trial for a config on repeat for k time\n    will run k Sessions, each with identical experiment_spec\n    for a problem, Agent, Memory, Policy, param.\n    Handles its own data, plots and saves its own graphs\n    Serialized by the parent trial_id with its session_id\n    \'\'\'\n\n    def __init__(self, trial, session_num=0, num_of_sessions=1, **kwargs):\n        from keras import backend as K\n        self.K = K\n\n        self.trial = trial\n        self.session_num = session_num\n        self.num_of_sessions = num_of_sessions\n        self.session_id = self.trial.trial_id + \\\n            \'_s\' + str(self.session_num)\n        log_session_delimiter(self, \'Init\')\n\n        self.experiment_spec = self.trial.experiment_spec\n        self.problem = self.experiment_spec[\'problem\']\n        self.Agent = get_module(GREF, self.experiment_spec[\'Agent\'])\n        self.Memory = get_module(GREF, self.experiment_spec[\'Memory\'])\n        self.Optimizer = get_module(\n            GREF, self.experiment_spec[\'Optimizer\'])\n        self.Policy = get_module(GREF, self.experiment_spec[\'Policy\'])\n        self.PreProcessor = get_module(\n            GREF, self.experiment_spec[\'PreProcessor\'])\n        self.param = self.experiment_spec[\'param\']\n        # init all things, so a session can only be ran once\n        self.sys_vars = self.init_sys_vars()\n        self.env = gym.make(self.sys_vars[\'GYM_ENV_NAME\'])\n        self.preprocessor = self.PreProcessor(**self.param)\n        self.env_spec = self.set_env_spec()\n        self.agent = self.Agent(self.env_spec, **self.param)\n        self.memory = self.Memory(self.env_spec, **self.param)\n        self.optimizer = self.Optimizer(**self.param)\n        self.policy = self.Policy(self.env_spec, **self.param)\n        self.agent.compile(\n            self.memory, self.optimizer, self.policy, self.preprocessor)\n\n        # data file and graph\n        self.base_filename = \'./data/{}/{}\'.format(\n            self.trial.experiment_id, self.session_id)\n        self.graph_filename = self.base_filename + \'.png\'\n\n        # for plotting\n        self.grapher = Grapher(self)\n\n    def init_sys_vars(self):\n        \'\'\'\n        init the sys vars for a problem by reading from\n        asset/problems.json, then reset the other sys vars\n        on reset will add vars (lower cases, see REQUIRED_SYS_KEYS)\n        \'\'\'\n        sys_vars = PROBLEMS[self.problem]\n        if args.max_epis >= 0:\n            sys_vars[\'MAX_EPISODES\'] = args.max_epis\n        sys_vars[\'RENDER\'] = (not args.param_selection and args.render)\n        if environ.get(\'CI\'):\n            sys_vars[\'RENDER\'] = False\n        self.sys_vars = sys_vars\n        self.reset_sys_vars()\n        return self.sys_vars\n\n    def reset_sys_vars(self):\n        \'\'\'reset and check RL system vars (lower case)\n        before each new session\'\'\'\n        for k in REQUIRED_SYS_KEYS:\n            if k.islower():\n                self.sys_vars[k] = copy.copy(REQUIRED_SYS_KEYS.get(k))\n        self.check_sys_vars()\n        return self.sys_vars\n\n    def check_sys_vars(self):\n        \'\'\'ensure the requried RL system vars are specified\'\'\'\n        sys_keys = self.sys_vars.keys()\n        assert all(k in sys_keys for k in REQUIRED_SYS_KEYS), \\\n            \'sys_vars do not have all REQUIRED_SYS_KEYS\'.format()\n\n    def set_env_spec(self):\n        \'\'\'Helper: return the env specs: dims, actions, reward range\'\'\'\n        env = self.env\n        state_dim = env.observation_space.shape[0]\n        if (len(env.observation_space.shape) > 1):\n            state_dim = env.observation_space.shape\n        if env.action_space.__class__.__name__ == \'Box\':  # continuous\n            action_dim = env.action_space.shape[0]\n            actions = \'continuous\'\n            action_low = env.action_space.low\n            action_high = env.action_space.high\n        else:\n            action_dim = env.action_space.n\n            actions = list(range(env.action_space.n))\n            action_low = 0\n            action_high = 1\n\n        env_spec = {\n            \'problem\': PROBLEMS[self.problem],\n            \'state_dim\': state_dim,\n            \'state_bound_low\': env.observation_space.low,\n            \'state_bound_high\': env.observation_space.high,\n            \'action_dim\': action_dim,\n            \'actions\': actions,\n            \'action_bound_low\': action_low,\n            \'action_bound_high\': action_high,\n            \'reward_range\': env.reward_range,\n            \'timestep_limit\': env.spec.tags.get(\n                \'wrapper_config.TimeLimit.max_episode_steps\')\n        }\n        self.env_spec = self.preprocessor.preprocess_env_spec(\n            env_spec)  # preprocess\n        return self.env_spec\n\n    def debug_agent_info(self):\n        logger.debug(\n            ""Agent info: {}"".format(\n                format_obj_dict(\n                    self.agent, [\'lr\', \'n_epoch\'])))\n        logger.debug(\n            ""Memory info: size: {}"".format(self.agent.memory.size()))\n        logger.debug(\n            ""Optimizer info: {}"".format(\n                format_obj_dict(self.agent.optimizer, [])))\n        logger.debug(\n            ""Policy info: {}"".format(\n                format_obj_dict(self.agent.policy, [\'e\', \'tau\'])))\n        logger.debug(\n            ""PreProcessor info: {}"".format(\n                format_obj_dict(self.agent.preprocessor, [])))\n\n    def check_end(self):\n        \'\'\'check if session ends (if is last episode)\n        do ending steps\'\'\'\n        sys_vars = self.sys_vars\n\n        logger.debug(\n            ""RL Sys info: {}"".format(\n                format_obj_dict(\n                    sys_vars, [\'epi\', \'t\', \'total_rewards\', \'mean_rewards\'])))\n        logger.debug(\'{:->30}\'.format(\'\'))\n\n        if (sys_vars[\'solved\'] or\n                (sys_vars[\'epi\'] == sys_vars[\'MAX_EPISODES\'] - 1)):\n            logger.info(\n                \'Problem solved? {}\\nAt episode: {}\\nsession_id: {}\'.format(\n                    sys_vars[\'solved\'], sys_vars[\'epi\'],\n                    self.session_id))\n            self.env.close()\n\n    def update_history(self):\n        \'\'\'\n        update the data per episode end\n        \'\'\'\n        sys_vars = self.sys_vars\n        sys_vars[\'total_rewards_history\'].append(sys_vars[\'total_rewards\'])\n        sys_vars[\'explore_history\'].append(\n            getattr(self.policy, \'e\', 0) or getattr(self.policy, \'tau\', 0))\n        avg_len = sys_vars[\'REWARD_MEAN_LEN\']\n        # Calculating mean_reward over last 100 episodes\n        # cast away from np for json serializable (dumb python)\n        mean_rewards = float(\n            np.mean(sys_vars[\'total_rewards_history\'][-avg_len:]))\n        solved = 0 if (sys_vars[\'SOLVED_MEAN_REWARD\'] is None) else (\n            mean_rewards >= sys_vars[\'SOLVED_MEAN_REWARD\'])\n        sys_vars[\'mean_rewards\'] = mean_rewards\n        sys_vars[\'mean_rewards_history\'].append(mean_rewards)\n        sys_vars[\'solved\'] = solved\n\n        self.grapher.plot()\n        self.check_end()\n        return sys_vars\n\n    def run_episode(self):\n        \'\'\'run an episode, return sys_vars\'\'\'\n        sys_vars, env, agent = self.sys_vars, self.env, self.agent\n        sys_vars[\'total_rewards\'] = 0\n        state = env.reset()\n        processed_state = agent.preprocessor.reset_state(state)\n        agent.memory.reset_state(processed_state)\n        self.debug_agent_info()\n\n        for t in range(agent.env_spec[\'timestep_limit\']):\n            sys_vars[\'t\'] = t  # update sys_vars t\n            if sys_vars.get(\'RENDER\'):\n                env.render()\n\n            processed_state = agent.preprocessor.preprocess_state()\n            action = agent.select_action(processed_state)\n            next_state, reward, done, _info = env.step(action)\n            processed_exp = agent.preprocessor.preprocess_memory(\n                action, reward, next_state, done)\n            if processed_exp is not None:\n                agent.memory.add_exp(*processed_exp)\n\n            sys_vars[\'done\'] = done\n            agent.update(sys_vars)\n            if agent.to_train(sys_vars):\n                agent.train(sys_vars)\n            sys_vars[\'total_rewards\'] += reward\n            if done:\n                break\n        self.update_history()\n        return sys_vars\n\n    def clear(self):\n        self.grapher.clear()\n        if self.K.backend() == \'tensorflow\':\n            self.K.clear_session()  # manual gc to fix TF issue 3388\n        del_self_attr(self)\n\n    def run(self):\n        \'\'\'run a session of agent\'\'\'\n        log_session_delimiter(self, \'Run\')\n        logger.info(\n            \'Experiment Trial Spec: {}\'.format(to_json(self.experiment_spec)))\n        sys_vars = self.sys_vars\n        sys_vars[\'time_start\'] = timestamp()\n        for epi in range(sys_vars[\'MAX_EPISODES\']):\n            sys_vars[\'epi\'] = epi  # update sys_vars epi\n            try:\n                self.run_episode()\n            except Exception:\n                logger.error(\n                    \'Error in trial, terminating \'\n                    \'further session from {}\'.format(self.session_id))\n                traceback.print_exc(file=sys.stdout)\n                sys_vars[\'errored\'] = True\n                break\n            if sys_vars[\'solved\']:\n                break\n\n        sys_vars[\'time_end\'] = timestamp()\n        sys_vars[\'time_taken\'] = timestamp_elapse(\n            sys_vars[\'time_start\'], sys_vars[\'time_end\'])\n\n        log_session_delimiter(self, \'End\')\n        self.clear()\n        return sys_vars\n\n\nclass Trial(object):\n\n    \'\'\'\n    An Trial for a config on repeat for k time\n    will run k Sessions, each with identical experiment_spec\n    for a problem, Agent, Memory, Policy, param.\n    Will spawn as many Sessions for repetition\n    Handles all the data from sessions\n    to provide an trial-level summary for a experiment_spec\n    Its trial_id is serialized by\n    problem, Agent, Memory, Policy and timestamp\n    Data Requirements:\n    JSON, single file, quick and useful summary,\n    replottable data, rerunnable specs\n    Keys:\n    all below X array of hyper param selection:\n    - trial_id\n    - metrics\n        - <metrics>\n        - time_start\n        - time_end\n        - time_taken\n    - experiment_spec (so we can plug in directly again to rerun)\n    - stats\n    - sys_vars_array\n    \'\'\'\n\n    def __init__(self, experiment_spec, times=1,\n                 trial_num=0, num_of_trials=1,\n                 run_timestamp=timestamp(),\n                 experiment_id_override=None,\n                 **kwargs):\n        self.experiment_spec = experiment_spec\n        self.experiment_name = self.experiment_spec.get(\'experiment_name\')\n        self.times = times\n        self.trial_num = trial_num\n        self.num_of_trials = num_of_trials\n        self.run_timestamp = run_timestamp\n        self.experiment_id = experiment_id_override or \'{}-{}\'.format(\n            self.experiment_name, self.run_timestamp)\n        self.trial_id = self.experiment_id + \'_t\' + str(self.trial_num)\n        log_trial_delimiter(self, \'Init\')\n\n        param_range = EXPERIMENT_SPECS.get(\n            self.experiment_name).get(\'param_range\')\n        self.param_variables = list(\n            param_range.keys()) if param_range else []\n        self.experiment_spec.pop(\'param_range\', None)  # single exp, del range\n        self.data = None\n\n        # data file\n        self.base_dir = \'./data/{}\'.format(self.experiment_id)\n        makedirs(self.base_dir, exist_ok=True)\n        self.base_filename = \'./data/{}/{}\'.format(\n            self.experiment_id, self.trial_id)\n        self.data_filename = self.base_filename + \'.json\'\n\n    def save(self):\n        \'\'\'save the entire trial data grid from inside run()\'\'\'\n        with open(self.data_filename, \'w\') as f:\n            f.write(to_json(self.data))\n        logger.info(\n            \'Session complete, data saved to {}\'.format(self.data_filename))\n\n    def is_completed(self, s=None):\n        \'\'\'check if the trial is already completed, if so dont run\'\'\'\n        # guard for resume loading, already init to None\n        self.data = self.data or load_data_from_trial_id(self.trial_id)\n\n        if self.data is None:  # if no data, confirmed not complete\n            return False\n        else:  # has data, check if the latest session is the last\n            if s is None:  # used for when reading from data\n                s = len(self.data[\'sys_vars_array\']) - 1\n            failed = (0 < s and s < self.times) and (\n                self.data[\'stats\'][\'solved_ratio_of_sessions\'] == 0.)\n            if failed:\n                logger.info(\n                    \'Failed trial, terminating sessions for {}\'.format(\n                        self.trial_id))\n            return failed\n\n    def clear(self):\n        del_self_attr(self)\n\n    def run(self):\n        \'\'\'\n        helper: run a trial for Session\n        a number of times times given a experiment_spec from gym_specs\n        \'\'\'\n        if self.is_completed():\n            log_trial_delimiter(self, \'Already completed\')\n        else:\n            log_trial_delimiter(self, \'Run\')\n            self.keras_session = configure_hardware(RAND_SEED)\n            time_start = timestamp()\n            sys_vars_array = [] if (self.data is None) else self.data[\n                \'sys_vars_array\']\n            # skip session if already has its data\n            s_start = len(sys_vars_array)\n            for s in range(s_start, self.times):\n                sess = Session(\n                    trial=self, session_num=s, num_of_sessions=self.times)\n                sys_vars = sess.run()\n                lean_sys_vars = copy.deepcopy(sys_vars)\n                lean_sys_vars.pop(\'loss\', None)\n                sys_vars_array.append(lean_sys_vars)\n                time_taken = timestamp_elapse(time_start, timestamp())\n\n                self.data = {  # trial data\n                    \'trial_id\': self.trial_id,\n                    \'metrics\': {\n                        \'time_taken\': time_taken,\n                    },\n                    \'experiment_spec\': self.experiment_spec,\n                    \'stats\': None,\n                    \'sys_vars_array\': sys_vars_array,\n                }\n                compose_data(self)\n                self.save()  # progressive update, write per session done\n                del sess\n                import gc\n                gc.collect()\n\n                if self.is_completed(s):\n                    break\n\n        log_trial_delimiter(self, \'End\')\n        compose_data(self)  # final update, for resume mode\n        self.save()\n        trial_data = copy.deepcopy(self.data)\n        self.clear()\n        return trial_data\n\n\ndef analyze_experiment(trial_or_experiment_id):\n    \'\'\'plot from a saved data by init sessions for each sys_vars\'\'\'\n    experiment_data = load_data_array_from_experiment_id(\n        trial_or_experiment_id)\n    return analyze_data(experiment_data)\n\n\ndef run(name_id_spec, times=1, param_selection=False, **kwargs):\n    \'\'\'\n    primary method:\n    specify:\n    - experiment_name(str) or experiment_spec(Dict): run new trial,\n    - experiment_id(str): rerun any incomplete trials from the experiment\n    - trial_id(str): rerun trial from data\n    This runs all trials, specified by the obtained experiment_spec\n    for a specified number of sessions per trial\n    Multiple trials are ran if param_selection=True\n    \'\'\'\n    experiment_kwargs = {\n        \'experiment_spec\': None,\n        \'experiment_id_override\': None,\n        \'times\': times\n    }\n    # set experiment_spec based on input\n    if isinstance(name_id_spec, str):\n        # rerun an incomplete experiment by experiment_id\n        if parse_experiment_id(name_id_spec):\n            experiment_id = parse_experiment_id(name_id_spec)\n            logger.info(\n                \'Rerun an incomplete experiment by id {}\'.format(\n                    experiment_id))\n            experiment_kwargs[\'experiment_id_override\'] = experiment_id\n            experiment_spec = EXPERIMENT_SPECS.get(\n                parse_experiment_name(name_id_spec))\n        else:  # run a new experiment by name\n            experiment_name = parse_experiment_name(name_id_spec)\n            logger.info(\n                \'Run a new experiment by name {}\'.format(experiment_name))\n            experiment_spec = EXPERIMENT_SPECS.get(experiment_name)\n    else:  # run a new experiment by spec\n        logger.info(\'Run a new experiment by spec\')\n        experiment_spec = name_id_spec\n    experiment_kwargs[\'experiment_spec\'] = copy.deepcopy(experiment_spec)\n\n    # compose grid and run param selection\n    if param_selection:\n        experiment_kwargs.update(kwargs)\n        Hopt = get_module(GREF, experiment_spec[\'HyperOptimizer\'])\n        hopt = Hopt(Trial, **experiment_kwargs)\n        experiment_data = hopt.run()\n    else:\n        trial = Trial(**experiment_kwargs)\n        trial_data = trial.run()\n        experiment_data = [trial_data]\n\n    return analyze_data(experiment_data)\n'"
rl/util.py,4,"b'import argparse\nimport collections\nimport inspect\nimport json\nimport logging\nimport multiprocessing as mp\nimport numpy as np\nimport re\nimport sys\nimport zipfile\nfrom datetime import datetime, timedelta\nfrom os import path, listdir, environ, getpid\nfrom textwrap import wrap\n\nPARALLEL_PROCESS_NUM = mp.cpu_count()\nTIMESTAMP_REGEX = r\'(\\d{4}_\\d{2}_\\d{2}_\\d{6})\'\nSPEC_PATH = path.join(path.dirname(__file__), \'spec\')\nCOMPONENT_LOCKS = json.loads(\n    open(path.join(SPEC_PATH, \'component_locks.json\')).read())\nLOCK_HEAD_REST_SIG = {\n    # signature list of [head, rest] in component lock\n    \'mutex\': [[0, 0], [1, 1]],\n    \'subset\': [[0, 0], [1, 0], [1, 1]],\n}\n\n\n# parse_args to add flag\nparser = argparse.ArgumentParser(description=\'Set flags for functions\')\nparser.add_argument(""-b"", ""--blind"",\n                    help=""dont render graphics"",\n                    action=""store_const"",\n                    dest=""render"",\n                    const=False,\n                    default=True)\nparser.add_argument(""-d"", ""--debug"",\n                    help=""activate debug log"",\n                    action=""store_const"",\n                    dest=""loglevel"",\n                    const=logging.DEBUG,\n                    default=logging.INFO)\nparser.add_argument(""-e"", ""--experiment"",\n                    help=""specify experiment to run"",\n                    action=""store"",\n                    type=str,\n                    nargs=\'?\',\n                    dest=""experiment"",\n                    default=""dev_dqn"")\nparser.add_argument(""-p"", ""--param_selection"",\n                    help=""run parameter selection if present"",\n                    action=""store_true"",\n                    dest=""param_selection"",\n                    default=False)\nparser.add_argument(""-q"", ""--quiet"",\n                    help=""change log to warning level"",\n                    action=""store_const"",\n                    dest=""loglevel"",\n                    const=logging.WARNING,\n                    default=logging.INFO)\nparser.add_argument(""-t"", ""--times"",\n                    help=""number of times session is run"",\n                    action=""store"",\n                    nargs=\'?\',\n                    type=int,\n                    dest=""times"",\n                    default=1)\nparser.add_argument(""-x"", ""--max_episodes"",\n                    help=""manually set environment max episodes"",\n                    action=""store"",\n                    nargs=\'?\',\n                    type=int,\n                    dest=""max_epis"",\n                    default=-1)\nargs = parser.parse_args([]) if environ.get(\'CI\') else parser.parse_args()\n\n# Goddam python logger\nlogger = logging.getLogger(__name__)\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setFormatter(\n    logging.Formatter(\'[%(asctime)s] %(levelname)s: %(message)s\'))\nlogger.setLevel(args.loglevel)\nlogger.addHandler(handler)\nlogger.propagate = False\nenviron[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # mute tf warnings on optimized setup\n\n\ndef check_equal(iterator):\n    \'\'\'check if list contains all the same elements\'\'\'\n    iterator = iter(iterator)\n    try:\n        first = next(iterator)\n    except StopIteration:\n        return True\n    return all(first == rest for rest in iterator)\n\n\ndef check_lock(lock_name, lock, experiment_spec):\n    \'\'\'\n    refer to rl/spec/component_locks.json\n    check a spec\'s component lock using binary signatures\n    e.g. head = problem (discrete)\n    rest = [Agent, Policy] (to be discrete too)\n    first check if rest all has the same signature, i.e. same set\n    then check pair [bin_head, bin_rest] in valid_lock_sig_list\n    as specified by the lock\'s type\n    \'\'\'\n    lock_type = lock[\'type\']\n    valid_lock_sig_list = LOCK_HEAD_REST_SIG[lock_type]\n    lock_head = lock[\'head\']\n    bin_head = (experiment_spec[lock_head] in lock[lock_head])\n    bin_rest_list = []\n    for k, v_list in lock.items():\n        if k in experiment_spec and k != lock_head:\n            bin_rest_list.append(experiment_spec[k] in v_list)\n    # rest must all have the same signature\n    rest_equal = check_equal(bin_rest_list)\n    if not rest_equal:\n        logger.warn(\n            \'All components need to be of the same set, \'\n            \'check component lock ""{}"" and your spec ""{}""\'.format(\n                lock_name, experiment_spec[\'experiment_name\']))\n\n    bin_rest = bin_rest_list[0]\n    lock_sig = [bin_head, bin_rest]\n    lock_valid = lock_sig in valid_lock_sig_list\n    if not lock_valid:\n        logger.warn(\n            \'Component lock violated: ""{}"", spec: ""{}""\'.format(\n                lock_name, experiment_spec[\'experiment_name\']))\n    return lock_valid\n\n\ndef check_component_locks(experiment_spec):\n    \'\'\'\n    check the spec components for all locks\n    to ensure no lock is violated\n    refer to rl/spec/component_locks.json\n    \'\'\'\n    for lock_name, lock in COMPONENT_LOCKS.items():\n        check_lock(lock_name, lock, experiment_spec)\n    return\n\n\n# import and safeguard the PROBLEMS, EXPERIMENT_SPECS with checks\ndef import_guard_asset():\n    PROBLEMS = json.loads(open(path.join(SPEC_PATH, \'problems.json\')).read())\n    EXPERIMENT_SPECS = {}\n    spec_files = [spec_json for spec_json in listdir(\n        SPEC_PATH) if spec_json.endswith(\'experiment_specs.json\')]\n    for filename in spec_files:\n        specs = json.loads(open(path.join(SPEC_PATH, filename)).read())\n        EXPERIMENT_SPECS.update(specs)\n\n    REQUIRED_PROBLEM_KEYS = [\n        \'GYM_ENV_NAME\', \'SOLVED_MEAN_REWARD\',\n        \'MAX_EPISODES\', \'REWARD_MEAN_LEN\']\n    REQUIRED_SPEC_KEYS = [\n        \'problem\', \'Agent\', \'HyperOptimizer\',\n        \'Memory\', \'Optimizer\', \'Policy\', \'PreProcessor\', \'param\']\n\n    for problem_name, problem in PROBLEMS.items():\n        assert all(k in problem for k in REQUIRED_PROBLEM_KEYS), \\\n            \'{} needs all REQUIRED_PROBLEM_KEYS\'.format(\n            problem_name)\n\n    for experiment_name, spec in EXPERIMENT_SPECS.items():\n        assert all(k in spec for k in REQUIRED_SPEC_KEYS), \\\n            \'{} needs all REQUIRED_SPEC_KEYS\'.format(experiment_name)\n        EXPERIMENT_SPECS[experiment_name][\'experiment_name\'] = experiment_name\n        check_component_locks(spec)  # check component_locks.json\n        if \'param_range\' not in EXPERIMENT_SPECS[experiment_name]:\n            continue\n\n        param_range = EXPERIMENT_SPECS[experiment_name][\'param_range\']\n        for param_key, param_val in param_range.items():\n            if isinstance(param_val, list):\n                param_range[param_key] = sorted(param_val)\n            elif isinstance(param_val, dict):\n                pass\n            else:\n                assert False, \\\n                    \'param_range value must be list or dict: {}.{}:{}\'.format(\n                        experiment_name, param_key, param_val)\n\n        EXPERIMENT_SPECS[experiment_name][\'param_range\'] = param_range\n    return PROBLEMS, EXPERIMENT_SPECS\n\nPROBLEMS, EXPERIMENT_SPECS = import_guard_asset()\n\n\ndef log_self(subject):\n    max_info_len = 300\n    info = \'{}, param: {}\'.format(\n        subject.__class__.__name__,\n        to_json(subject.__dict__))\n    trunc_info = (\n        info[:max_info_len] + \'...\' if len(info) > max_info_len else info)\n    logger.debug(trunc_info)\n\n\ndef wrap_text(text):\n    return \'\\n\'.join(wrap(text, 60))\n\n\ndef make_line(line=\'-\'):\n    if environ.get(\'CI\'):\n        return\n    columns = 80\n    line_str = line*int(columns)\n    return line_str\n\n\ndef log_delimiter(msg, line=\'-\'):\n    delim_msg = \'\'\'\\n{0}\\n{1}\\n{0}\\n\\n\'\'\'.format(\n        make_line(line), msg)\n    logger.info(delim_msg)\n\n\ndef log_trial_delimiter(trial, action):\n    log_delimiter(\'{} Trial #{}/{} on PID {}:\\n{}\'.format(\n        action, trial.trial_num, trial.num_of_trials,\n        getpid(), trial.trial_id), \'=\')\n\n\ndef log_session_delimiter(sess, action):\n    log_delimiter(\n        \'{} Session #{}/{} of Trial #{}/{} on PID {}:\\n{}\'.format(\n            action, sess.session_num, sess.num_of_sessions,\n            sess.trial.trial_num, sess.trial.num_of_trials,\n            getpid(), sess.session_id))\n\n\ndef timestamp():\n    \'\'\'timestamp used for filename\'\'\'\n    timestamp_str = \'{:%Y_%m_%d_%H%M%S}\'.format(datetime.now())\n    assert re.search(TIMESTAMP_REGEX, timestamp_str)\n    return timestamp_str\n\n\ndef timestamp_elapse(s1, s2):\n    \'\'\'calculate the time elapsed between timestamps from s1 to s2\'\'\'\n    FMT = \'%Y_%m_%d_%H%M%S\'\n    delta_t = datetime.strptime(s2, FMT) - datetime.strptime(s1, FMT)\n    return str(delta_t)\n\n\ndef timestamp_elapse_to_seconds(s1):\n    a = datetime.strptime(s1, \'%H:%M:%S\')\n    secs = timedelta(hours=a.hour, minutes=a.minute, seconds=a.second).seconds\n    return secs\n\n\n# own custom sorted json serializer, cuz python\ndef to_json(o, level=0):\n    INDENT = 2\n    SPACE = "" ""\n    NEWLINE = ""\\n""\n    ret = """"\n    if isinstance(o, dict):\n        ret += ""{"" + NEWLINE\n        comma = """"\n        for k in sorted(o.keys()):\n            v = o[k]\n            ret += comma\n            comma = "",\\n""\n            ret += SPACE * INDENT * (level+1)\n            ret += \'""\' + str(k) + \'"":\' + SPACE\n            ret += to_json(v, level + 1)\n\n        ret += NEWLINE + SPACE * INDENT * level + ""}""\n    elif isinstance(o, str):\n        ret += \'""\' + o + \'""\'\n    elif isinstance(o, list) or isinstance(o, tuple):\n        ret += ""["" + "","".join([to_json(e, level+1) for e in o]) + ""]""\n    elif isinstance(o, bool):\n        ret += ""true"" if o else ""false""\n    elif isinstance(o, int):\n        ret += str(o)\n    elif isinstance(o, float):\n        ret += \'%.7g\' % o\n    elif isinstance(o, np.ndarray) and np.issubdtype(o.dtype, np.integer):\n        ret += ""["" + \',\'.join(map(str, o.flatten().tolist())) + ""]""\n    elif isinstance(o, np.ndarray) and np.issubdtype(o.dtype, np.inexact):\n        ret += ""["" + \\\n            \',\'.join(map(lambda x: \'%.7g\' % x, o.flatten().tolist())) + ""]""\n    elif o is None:\n        ret += \'null\'\n    elif hasattr(o, \'__class__\'):\n        ret += \'""\' + o.__class__.__name__ + \'""\'\n    else:\n        raise TypeError(\n            ""Unknown type \'%s\' for json serialization"" % str(type(o)))\n    return ret\n\n\n# format object and its properties into printable dict\ndef format_obj_dict(obj, keys):\n    if isinstance(obj, dict):\n        return to_json(\n            {k: obj.get(k) for k in keys if obj.get(k) is not None})\n    else:\n        return to_json(\n            {k: getattr(obj, k, None) for k in keys\n             if getattr(obj, k, None) is not None})\n\n\n# cast dict to have flat values (int, float, str)\ndef flat_cast_dict(d):\n    for k in d:\n        v = d[k]\n        if not isinstance(v, (int, float)):\n            d[k] = str(v)\n    return d\n\n\ndef flatten_dict(d, parent_key=\'\', sep=\'_\'):\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n\ndef get_module(GREF, dot_path):\n    # get module from globals() by string dot_path\n    path_arr = dot_path.split(\'.\')\n    # base level from globals\n    mod = GREF.get(path_arr.pop(0))\n    for deeper_path in path_arr:\n        mod = getattr(mod, deeper_path)\n    return mod\n\n\ndef import_package_files(globals_, locals_, __file__):\n    \'\'\'\n    Dynamically import all the public attributes of the python modules in this\n    file\'s directory (the package directory) and return a list of their names.\n    \'\'\'\n    exports = []\n    # globals_, locals_ = globals(), locals()\n    package_path = path.dirname(__file__)\n    package_name = path.basename(package_path)\n\n    for filename in listdir(package_path):\n        modulename, ext = path.splitext(filename)\n        if modulename[0] != \'_\' and ext in (\'.py\', \'.pyw\'):\n            subpackage = \'{}.{}\'.format(\n                package_name, modulename)  # pkg relative\n            module = __import__(subpackage, globals_, locals_, [modulename])\n            modict = module.__dict__\n            names = (modict[\'__all__\'] if \'__all__\' in modict else\n                     [name for name in\n                      modict if inspect.isclass(modict[name])])  # all public\n            exports.extend(names)\n            globals_.update((name, modict[name]) for name in names)\n\n    return exports\n\n\ndef clean_id_str(id_str):\n    return id_str.split(\'/\').pop().split(\'.\').pop(0)\n\n\ndef parse_trial_id(id_str):\n    c_id_str = clean_id_str(id_str)\n    if re.search(TIMESTAMP_REGEX, c_id_str):\n        name_time_trial = re.split(TIMESTAMP_REGEX, c_id_str)\n        if len(name_time_trial) == 3:\n            return c_id_str\n        else:\n            return None\n    else:\n        return None\n\n\ndef parse_experiment_id(id_str):\n    c_id_str = clean_id_str(id_str)\n    if re.search(TIMESTAMP_REGEX, c_id_str):\n        name_time_trial = re.split(TIMESTAMP_REGEX, c_id_str)\n        name_time_trial.pop()\n        experiment_id = \'\'.join(name_time_trial)\n        return experiment_id\n    else:\n        return None\n\n\ndef parse_experiment_name(id_str):\n    c_id_str = clean_id_str(id_str)\n    experiment_id = parse_experiment_id(c_id_str)\n    if experiment_id is None:\n        experiment_name = c_id_str\n    else:\n        experiment_name = re.sub(TIMESTAMP_REGEX, \'\', experiment_id).strip(\'-\')\n    assert experiment_name in EXPERIMENT_SPECS, \\\n        \'{} not in EXPERIMENT_SPECS\'.format(experiment_name)\n    return experiment_name\n\n\ndef load_data_from_trial_id(id_str):\n    experiment_id = parse_experiment_id(id_str)\n    trial_id = parse_trial_id(id_str)\n    data_filename = \'./data/{}/{}.json\'.format(experiment_id, trial_id)\n    try:\n        data = json.loads(open(data_filename).read())\n    except (FileNotFoundError, json.JSONDecodeError):\n        data = None\n    return data\n\n\ndef load_data_array_from_experiment_id(id_str):\n    # to load all ./data files for a series of trials\n    experiment_id = parse_experiment_id(id_str)\n    data_path = \'./data/{}\'.format(experiment_id)\n    trial_id_array = [\n        f for f in listdir(data_path)\n        if (path.isfile(path.join(data_path, f)) and\n            f.startswith(experiment_id) and\n            f.endswith(\'.json\'))\n    ]\n    return list(filter(None, [load_data_from_trial_id(trial_id)\n                              for trial_id in trial_id_array]))\n\n\ndef save_experiment_data(data_df, trial_id):\n    experiment_id = parse_experiment_id(trial_id)\n    filedir = \'./data/{0}\'.format(experiment_id)\n    filename = \'{0}_analysis_data.csv\'.format(experiment_id)\n    filepath = \'{}/{}\'.format(filedir, filename)\n    data_df.round(6).to_csv(filepath, index=False)\n\n    # zip the csv and best trial json for upload to PR\n    zipfile.ZipFile(filepath+\'.zip\', mode=\'w\').write(\n        filepath, arcname=filename)\n    trial_filename = data_df.loc[0, \'trial_id\'] + \'.json\'\n    trial_filepath = \'{}/{}\'.format(filedir, trial_filename)\n    zipfile.ZipFile(trial_filepath+\'.zip\', mode=\'w\').write(\n        trial_filepath, arcname=trial_filename)\n\n    logger.info(\n        \'experiment data saved to {}\'.format(filepath))\n\n\ndef configure_hardware(RAND_SEED):\n    \'\'\'configure rand seed, GPU\'\'\'\n    from keras import backend as K\n    if K.backend() == \'tensorflow\':\n        K.tf.set_random_seed(RAND_SEED)\n    else:\n        K.theano.tensor.shared_randomstreams.RandomStreams(seed=RAND_SEED)\n\n    if K.backend() != \'tensorflow\':\n        # GPU config for tf only\n        return\n\n    process_num = PARALLEL_PROCESS_NUM if args.param_selection else 1\n    tf = K.tf\n    gpu_options = tf.GPUOptions(\n        allow_growth=True,\n        per_process_gpu_memory_fraction=1./float(process_num))\n    config = tf.ConfigProto(\n        gpu_options=gpu_options,\n        allow_soft_placement=True)\n    sess = tf.Session(config=config)\n    K.set_session(sess)\n    return sess\n\n\ndef debug_mem_usage():\n    import psutil\n    from mem_top import mem_top\n    pid = getpid()\n    logger.debug(\n        \'MEM USAGE for PID {}, MEM_INFO: {}\\n{}\'.format(\n            pid, psutil.Process().memory_info(), mem_top()))\n\n\ndef del_self_attr(subject):\n    self_attrs = list(subject.__dict__.keys())\n    for attr in self_attrs:\n        delattr(subject, attr)\n    import gc\n    gc.collect()\n\n\n# clone a keras model without file I/O\ndef clone_model(model, custom_objects=None):\n    from keras.models import model_from_config\n    custom_objects = custom_objects or {}\n    config = {\n        \'class_name\': model.__class__.__name__,\n        \'config\': model.get_config(),\n    }\n    clone = model_from_config(config, custom_objects=custom_objects)\n    clone.set_weights(model.get_weights())\n    return clone\n\n\n# clone a keras optimizer without file I/O\ndef clone_optimizer(optimizer):\n    from keras.optimizers import optimizer_from_config\n    if isinstance(optimizer, str):\n        return get(optimizer)\n    params = dict([(k, v) for k, v in optimizer.get_config().items()])\n    config = {\n        \'class_name\': optimizer.__class__.__name__,\n        \'config\': params,\n    }\n    clone = optimizer_from_config(config)\n    return clone\n'"
test/__init__.py,0,b''
test/conftest.py,0,"b""import pytest\nimport rl\nfrom os import environ\n\nenviron['CI'] = environ.get('CI') or 'true'\n\n\ndef pytest_runtest_setup(item):\n    for problem in rl.util.PROBLEMS:\n        if problem == 'TestPassCartPole-v0':\n            pass\n        else:\n            rl.util.PROBLEMS[problem]['MAX_EPISODES'] = 3\n"""
test/test_atari.py,0,"b'import unittest\nimport pytest\nfrom os import environ\nfrom rl.experiment import run\nfrom . import conftest\nimport pandas as pd\n\n\nclass AtariTest(unittest.TestCase):\n\n    @unittest.skipIf(environ.get(\'CI\'), ""Delay CI test until dev stable"")\n    @classmethod\n    def test_breakout_dqn(cls):\n        data_df = run(\'breakout_dqn\')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @unittest.skipIf(environ.get(\'CI\'), ""Delay CI test until dev stable"")\n    @classmethod\n    def test_breakout_double_dqn(cls):\n        data_df = run(\'breakout_double_dqn\')\n        assert isinstance(data_df, pd.DataFrame)\n'"
test/test_box2d.py,0,"b""import unittest\nimport pytest\nfrom os import environ\nfrom rl.experiment import run\nfrom . import conftest\nimport pandas as pd\n\n\nclass Box2DTest(unittest.TestCase):\n\n    @classmethod\n    def test_lunar_dqn(cls):\n        data_df = run('lunar_dqn')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_lunar_double_dqn(cls):\n        data_df = run('lunar_double_dqn')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_lunar_freeze(cls):\n        data_df = run('lunar_freeze')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_walker_ddpg_linearnoise(cls):\n        data_df = run('walker_ddpg_linearnoise')\n        assert isinstance(data_df, pd.DataFrame)\n"""
test/test_classic.py,0,"b""import unittest\nimport pytest\nfrom os import environ\nfrom rl.experiment import run\nfrom . import conftest\nimport pandas as pd\n\n\nclass ClassicTest(unittest.TestCase):\n\n    @classmethod\n    def test_quickstart_dqn(cls):\n        data_df = run('quickstart_dqn')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_dqn_epsilon(cls):\n        data_df = run('dqn_epsilon')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_dqn(cls):\n        data_df = run('dqn')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_dqn_per(cls):\n        data_df = run('dqn_per')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_double_dqn(cls):\n        data_df = run('double_dqn')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_sarsa(cls):\n        data_df = run('sarsa')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_exp_sarsa(cls):\n        data_df = run('exp_sarsa')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_offpol_sarsa(cls):\n        data_df = run('offpol_sarsa')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_cartpole_ac_argmax(cls):\n        data_df = run('cartpole_ac_argmax')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_dqn_v1(cls):\n        data_df = run('dqn_v1')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_acrobot(cls):\n        data_df = run('acrobot')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_pendulum_ddpg_linearnoise(cls):\n        data_df = run('pendulum_ddpg_linearnoise')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_mountain_dqn(cls):\n        data_df = run('mountain_dqn')\n        assert isinstance(data_df, pd.DataFrame)\n"""
test/test_dev.py,0,"b'import unittest\nimport pytest\nfrom os import environ\nfrom rl.experiment import run\nfrom . import conftest\nimport pandas as pd\n\n\nclass DevTest(unittest.TestCase):\n\n    @classmethod\n    def test_clean_import(cls):\n        print(dir())\n        assert \'keras\' not in dir(\n        ), \'keras import should be contained within classes\'\n        assert \'matplotlib\' not in dir(\n        ), \'matplotlib import should be contained within classes\'\n\n    @classmethod\n    def test_gym_tour(cls):\n        data_df = run(\'dummy\')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @classmethod\n    def test_q_table(cls):\n        data_df = run(\'q_table\')\n        assert isinstance(data_df, pd.DataFrame)\n\n    @unittest.skipIf(environ.get(\'CI\'),\n                     ""Causing build to crash since it\'s unstable."")\n    @classmethod\n    def test_dqn_pass(cls):\n        data_df = run(\'test_dqn_pass\')\n        max_total_rewards = data_df[\'max_total_rewards_stats_mean\'][0]\n        print(max_total_rewards)\n        assert max_total_rewards > 50, \'dqn failed to hit max_total_rewards\'\n\n    # TODO cant run searches with these shits together, will hang everything wtf\n    # @classmethod\n    # def test_dqn_grid_search(cls):\n    #     data_df = run(\'test_dqn_grid_search\', param_selection=True)\n    #     assert isinstance(data_df, pd.DataFrame)\n\n    # TODO cant run searches with these shits together, will hang everything wtf\n    # @classmethod\n    # def test_dqn_random_search(cls):\n    #     data_df = run(\'test_dqn_random_search\', param_selection=True)\n    #     assert isinstance(data_df, pd.DataFrame)\n'"
rl/agent/__init__.py,0,"b""from rl.util import import_package_files\n\n__all__ = ['__all__'] + import_package_files(globals(), locals(), __file__)\n"""
rl/agent/actor_critic.py,0,"b'import numpy as np\nfrom rl.agent.dqn import DQN\nfrom rl.util import logger\n\n\nclass ActorCritic(DQN):\n\n    \'\'\'\n    Actor Critic algorithm. The actor\'s policy\n    is adjusted in the direction that will lead to\n    better actions, guided by the critic\n    Implementation adapted from\n    http://www.rage.net/~greg/2016-07-05-ActorCritic-with-OpenAI-Gym.html\n\n    Assumes one of the policies in actor_critic.py are being used\n    \'\'\'\n\n    def __init__(self, env_spec,\n                 train_per_n_new_exp=1,\n                 gamma=0.95, lr=0.1,\n                 epi_change_lr=None,\n                 batch_size=16, n_epoch=5, hidden_layers=None,\n                 hidden_layers_activation=\'sigmoid\',\n                 output_layer_activation=\'linear\',\n                 auto_architecture=False,\n                 num_hidden_layers=3,\n                 first_hidden_layer_size=256,\n                 num_initial_channels=16,\n                 **kwargs):  # absorb generic param without breaking\n        # import only when needed to contain side-effects\n        from keras.layers.core import Dense\n        from keras.models import Sequential, load_model\n        self.Dense = Dense\n        self.Sequential = Sequential\n        self.load_model = load_model\n\n        super(ActorCritic, self).__init__(env_spec,\n                                          train_per_n_new_exp,\n                                          gamma, lr,\n                                          epi_change_lr,\n                                          batch_size, n_epoch, hidden_layers,\n                                          hidden_layers_activation,\n                                          output_layer_activation,\n                                          auto_architecture,\n                                          num_hidden_layers,\n                                          first_hidden_layer_size,\n                                          num_initial_channels,\n                                          **kwargs)\n\n    def build_model(self):\n        self.build_actor()\n        self.build_critic()\n        logger.info(""Actor and critic models built"")\n\n    def build_actor(self):\n        actor = self.Sequential()\n        super(ActorCritic, self).build_hidden_layers(actor)\n        actor.add(self.Dense(self.env_spec[\'action_dim\'],\n                             init=\'lecun_uniform\',\n                             activation=self.output_layer_activation))\n        logger.info(""Actor summary"")\n        actor.summary()\n        self.actor = actor\n\n    def build_critic(self):\n        critic = self.Sequential()\n        super(ActorCritic, self).build_hidden_layers(critic)\n        critic.add(self.Dense(1,\n                              init=\'lecun_uniform\',\n                              activation=self.output_layer_activation))\n        logger.info(""Critic summary"")\n        critic.summary()\n        self.critic = critic\n\n    def compile_model(self):\n        self.actor.compile(\n            loss=\'mse\',\n            optimizer=self.optimizer.keras_optimizer)\n        self.critic.compile(\n            loss=\'mse\',\n            optimizer=self.optimizer.keras_optimizer)\n        logger.info(""Actor and critic compiled"")\n\n    def recompile_model(self, sys_vars):\n        \'\'\'\n        Option to change model optimizer settings\n        Currently only used for changing the learning rate\n        Compiling does not affect the model weights\n        \'\'\'\n        if self.epi_change_lr is not None:\n            if (sys_vars[\'epi\'] == self.epi_change_lr and\n                    sys_vars[\'t\'] == 0):\n                self.lr = self.lr / 10.0\n                self.optimizer.change_optim_param(**{\'lr\': self.lr})\n                self.actor.compile(\n                    loss=\'mse\',\n                    optimizer=self.optimizer.keras_optimizer)\n                self.critic.compile(\n                    loss=\'mse\',\n                    optimizer=self.optimizer.keras_optimizer)\n                logger.info(\n                    \'Actor and critic models recompiled with new settings: \'\n                    \'Learning rate: {}\'.format(self.lr))\n\n    def train_critic(self, minibatch):\n        Q_vals = np.clip(self.critic.predict(minibatch[\'states\']),\n                         -self.clip_val, self.clip_val)\n        Q_next_vals = np.clip(self.critic.predict(minibatch[\'next_states\']),\n                              -self.clip_val, self.clip_val)\n        Q_targets = minibatch[\'rewards\'] + self.gamma * \\\n            (1 - minibatch[\'terminals\']) * Q_next_vals.squeeze()\n        Q_targets = np.expand_dims(Q_targets, axis=1)\n\n        actor_delta = Q_next_vals - Q_vals\n        loss = self.critic.train_on_batch(minibatch[\'states\'], Q_targets)\n\n        # update memory, needed for PER\n        errors = abs(np.sum(Q_vals - Q_targets, axis=1))\n        # Q size is only 1, from critic\n        assert Q_targets.shape == (self.batch_size, 1)\n        assert errors.shape == (self.batch_size, )\n        self.memory.update(errors)\n        return loss, actor_delta\n\n    def train_actor(self, minibatch, actor_delta):\n        old_vals = self.actor.predict(minibatch[\'states\'])\n        if self.env_spec[\'actions\'] == \'continuous\':\n            A_targets = np.zeros(\n                (actor_delta.shape[0], self.env_spec[\'action_dim\']))\n            for j in range(A_targets.shape[1]):\n                A_targets[:, j] = actor_delta.squeeze()\n        else:\n            A_targets = minibatch[\'actions\'] * actor_delta + \\\n                (1 - minibatch[\'actions\']) * old_vals\n\n        loss = self.actor.train_on_batch(minibatch[\'states\'], A_targets)\n        return loss\n\n    def train_an_epoch(self):\n        minibatch = self.memory.rand_minibatch(self.batch_size)\n        critic_loss, actor_delta = self.train_critic(minibatch)\n        actor_loss = self.train_actor(minibatch, actor_delta)\n        return critic_loss + actor_loss\n'"
rl/agent/base_agent.py,0,"b""from rl.util import logger\n\n\nclass Agent(object):\n\n    '''\n    The base class of Agent, with the core methods\n    '''\n\n    def __init__(self, env_spec,\n                 **kwargs):  # absorb generic param without breaking\n        self.env_spec = env_spec\n\n    def compile(self, memory, optimizer, policy, preprocessor):\n        # set 2 way references\n        self.memory = memory\n        self.optimizer = optimizer\n        self.policy = policy\n        self.preprocessor = preprocessor\n        # back references\n        setattr(memory, 'agent', self)\n        setattr(optimizer, 'agent', self)\n        setattr(policy, 'agent', self)\n        setattr(preprocessor, 'agent', self)\n        self.compile_model()\n        logger.info(\n            'Compiled:\\nAgent, Memory, Optimizer, Policy, '\n            'Preprocessor:\\n{}'.format(\n                ', '.join([comp.__class__.__name__ for comp in\n                           [self, memory, optimizer, policy, preprocessor]])\n            ))\n\n    def build_model(self):\n        raise NotImplementedError()\n\n    def compile_model(self):\n        raise NotImplementedError()\n\n    def select_action(self, state):\n        self.policy.select_action(state)\n        raise NotImplementedError()\n\n    def update(self, sys_vars):\n        '''Agent update apart from training the Q function'''\n        self.policy.update(sys_vars)\n        raise NotImplementedError()\n\n    def to_train(self, sys_vars):\n        raise NotImplementedError()\n\n    def train(self, sys_vars):\n        raise NotImplementedError()\n"""
rl/agent/conv_dqn.py,0,"b""import math\nfrom rl.agent.dqn import DQN\n\n\nclass ConvDQN(DQN):\n\n    def __init__(self, *args, **kwargs):\n        from keras.layers.core import Dense, Flatten\n        from keras.layers.convolutional import Convolution2D\n        from keras import backend as K\n        if K.backend() == 'theano':\n            K.set_image_dim_ordering('tf')\n        self.Dense = Dense\n        self.Flatten = Flatten\n        self.Convolution2D = Convolution2D\n\n        self.kernel = 4\n        self.stride = (2, 2)\n        super(ConvDQN, self).__init__(*args, **kwargs)\n\n    def build_hidden_layers(self, model):\n        '''\n        build the hidden layers into model using parameter self.hidden_layers\n        Auto architecture infers the size of the hidden layers from the number\n        of channels in the first hidden layer and number of layers\n        With each successive layer the number of channels is doubled\n        Kernel size is fixed at 4, and stride at (2, 2)\n        No new layers are added if the cols or rows have dim <= 5\n        Enables hyperparameter optimization over network architecture\n        '''\n        if self.auto_architecture:\n            num_channels = self.num_initial_channels\n            cols = self.env_spec['state_dim'][0]\n            rows = self.env_spec['state_dim'][1]\n            # input layer\n            model.add(\n                self.Convolution2D(\n                    num_channels,\n                    self.kernel,\n                    self.kernel,\n                    subsample=self.stride,\n                    input_shape=self.env_spec['state_dim'],\n                    activation=self.hidden_layers_activation,\n                    # border_mode='same',\n                    init='lecun_uniform'))\n\n            for i in range(1, self.num_hidden_layers):\n                num_channels *= 2\n                cols = math.ceil(\n                    math.floor(cols - self.kernel - 1) / self.stride[0]) + 1\n                rows = math.ceil(\n                    math.floor(rows - self.kernel - 1) / self.stride[1]) + 1\n                if cols > 5 and rows > 5:\n                    model.add(\n                        self.Convolution2D(\n                            num_channels,\n                            self.kernel,\n                            self.kernel,\n                            subsample=self.stride,\n                            activation=self.hidden_layers_activation,\n                            # border_mode='same',\n                            init='lecun_uniform'))\n                else:\n                    # stop addition of too many layers\n                    # and from breakage by cols, rows growing to 0\n                    break\n\n        else:\n            model.add(\n                self.Convolution2D(\n                    self.hidden_layers[0][0],\n                    self.hidden_layers[0][1],\n                    self.hidden_layers[0][2],\n                    subsample=self.hidden_layers[0][3],\n                    input_shape=self.env_spec['state_dim'],\n                    activation=self.hidden_layers_activation,\n                    # border_mode='same',\n                    init='lecun_uniform'))\n\n            if (len(self.hidden_layers) > 1):\n                for i in range(1, len(self.hidden_layers)):\n                    model.add(\n                        self.Convolution2D(\n                            self.hidden_layers[i][0],\n                            self.hidden_layers[i][1],\n                            self.hidden_layers[i][2],\n                            subsample=self.hidden_layers[i][3],\n                            activation=self.hidden_layers_activation,\n                            # border_mode='same',\n                            init='lecun_uniform'))\n\n        model.add(self.Flatten())\n        model.add(self.Dense(256,\n                             init='lecun_uniform',\n                             activation=self.hidden_layers_activation))\n\n        return model\n"""
rl/agent/ddpg.py,15,"b'import numpy as np\nfrom rl.agent.dqn import DQN\nfrom rl.util import logger, clone_model\n\n\nclass Actor(DQN):\n    \'\'\'\n    Actor of DDPG, with its network and target network\n    input is states, output is action\n    very similar to DQN\n    \'\'\'\n\n    def __init__(self, *args, tau=0.001, **kwargs):\n        from keras import backend as K\n        self.K = K\n        self.tf = self.K.tf\n        self.sess = self.K.get_session()\n        self.tau = tau\n        super(Actor, self).__init__(*args, **kwargs)\n\n    def build_model(self):\n        self.model = super(Actor, self).build_model()\n        self.target_model = clone_model(self.model)\n\n        self.actor_states = self.model.inputs[0]\n        self.out = self.model.output\n        self.scaled_out = self.tf.multiply(\n            self.out, self.env_spec[\'action_bound_high\'])\n        self.network_params = self.model.trainable_weights\n\n        self.target_actor_states = self.target_model.inputs[0]\n        self.target_out = self.target_model.output\n        self.target_scaled_out = self.tf.multiply(\n            self.target_out, self.env_spec[\'action_bound_high\'])\n        self.target_network_params = self.target_model.trainable_weights\n\n        # Op for updating target network\n        self.update_target_network_op = []\n        for i, t_w in enumerate(self.target_network_params):\n            op = t_w.assign(\n                self.tf.multiply(\n                    self.tau, self.network_params[i]\n                ) + self.tf.multiply(1. - self.tau, t_w))\n            self.update_target_network_op.append(op)\n\n        # will be fed as self.action_gradient: critic_grads\n        self.action_gradient = self.tf.placeholder(\n            self.tf.float32, [None, self.env_spec[\'action_dim\']])\n\n        # actor model gradient op, to be fed from critic\n        self.actor_gradients = self.tf.gradients(\n            self.scaled_out, self.network_params, -self.action_gradient)\n\n        # Optimization op\n        self.optimize = self.tf.train.AdamOptimizer(self.lr).apply_gradients(\n            zip(self.actor_gradients, self.network_params))\n        return self.model\n\n    def compile_model(self):\n        pass\n\n    def recompile_model(self, sys_vars):\n        pass\n\n    def update(self, sys_vars):\n        self.sess.run(self.update_target_network_op)\n\n    def predict(self, states):\n        return self.sess.run(self.scaled_out, feed_dict={\n            self.actor_states: states\n        })\n\n    def target_predict(self, next_states):\n        return self.sess.run(self.target_scaled_out, feed_dict={\n            self.target_actor_states: next_states\n        })\n\n    def train_tf(self, states, critic_action_gradient):\n        return self.sess.run(self.optimize, feed_dict={\n            self.actor_states: states,\n            self.action_gradient: critic_action_gradient\n        })\n\n\nclass Critic(DQN):\n\n    \'\'\'\n    Critic of DDPG, with its network and target network\n    input is states and actions, output is Q value\n    the action is from Actor\n    \'\'\'\n\n    def __init__(self, *args, tau=0.001, critic_lr=0.001, **kwargs):\n        from keras.layers import Dense, Merge\n        from keras import backend as K\n        self.Dense = Dense\n        self.Merge = Merge\n        self.K = K\n        self.tf = self.K.tf\n        self.sess = self.K.get_session()\n        self.tau = tau\n        self.critic_lr = critic_lr  # suggestion: 10 x actor_lr\n        super(Critic, self).__init__(*args, **kwargs)\n\n    def build_critic_models(self):\n        state_branch = self.Sequential()\n        state_branch.add(self.Dense(\n            self.hidden_layers[0],\n            input_shape=(self.env_spec[\'state_dim\'],),\n            activation=self.hidden_layers_activation,\n            init=\'lecun_uniform\'))\n\n        action_branch = self.Sequential()\n        action_branch.add(self.Dense(\n            self.hidden_layers[0],\n            input_shape=(self.env_spec[\'action_dim\'],),\n            activation=self.hidden_layers_activation,\n            init=\'lecun_uniform\'))\n\n        input_layer = self.Merge([state_branch, action_branch], mode=\'concat\')\n\n        model = self.Sequential()\n        model.add(input_layer)\n\n        if (len(self.hidden_layers) > 1):\n            for i in range(1, len(self.hidden_layers)):\n                model.add(self.Dense(\n                    self.hidden_layers[i],\n                    init=\'lecun_uniform\',\n                    activation=self.hidden_layers_activation))\n\n        model.add(self.Dense(1,\n                             init=\'lecun_uniform\',\n                             activation=\'linear\'))  # fixed\n        logger.info(\'Critic model summary\')\n        model.summary()\n        self.model = model\n\n        logger.info(""Model built"")\n        return self.model\n\n    def build_model(self):\n        self.model = self.build_critic_models()\n        self.target_model = clone_model(self.model)\n\n        self.critic_states = self.model.inputs[0]\n        self.critic_actions = self.model.inputs[1]\n        self.out = self.model.output\n        self.network_params = self.model.trainable_weights\n\n        self.target_critic_states = self.target_model.inputs[0]\n        self.target_critic_actions = self.target_model.inputs[1]\n        self.target_out = self.target_model.output\n        self.target_network_params = self.target_model.trainable_weights\n\n        # Op for updating target network\n        self.update_target_network_op = []\n        for i, t_w in enumerate(self.target_network_params):\n            op = t_w.assign(\n                self.tf.multiply(\n                    self.tau, self.network_params[i]\n                ) + self.tf.multiply(1. - self.tau, t_w))\n            self.update_target_network_op.append(op)\n\n        # custom loss and optimization Op\n        self.y = self.tf.placeholder(self.tf.float32, [None, 1])\n        self.loss = self.tf.losses.mean_squared_error(self.y, self.out)\n        self.optimize = self.tf.train.AdamOptimizer(\n            self.critic_lr).minimize(self.loss)\n\n        self.action_gradient = self.tf.gradients(self.out, self.critic_actions)\n        return self.model\n\n    def update(self, sys_vars):\n        self.sess.run(self.update_target_network_op)\n\n    def get_action_gradient(self, states, actions):\n        return self.sess.run(self.action_gradient, feed_dict={\n            self.critic_states: states,\n            self.critic_actions: actions\n        })[0]\n\n    # def predict(self, inputs, action):\n    #     return self.sess.run(self.out, feed_dict={\n    #         self.critic_states: inputs,\n    #         self.critic_actions: action\n    #     })\n\n    def target_predict(self, next_states, mu_prime):\n        return self.sess.run(self.target_out, feed_dict={\n            self.target_critic_states: next_states,\n            self.target_critic_actions: mu_prime\n        })\n\n    def train_tf(self, states, actions, y):\n        return self.sess.run([self.out, self.optimize, self.loss], feed_dict={\n            self.critic_states: states,\n            self.critic_actions: actions,\n            self.y: y\n        })\n\n\nclass DDPG(DQN):\n\n    \'\'\'\n    DDPG Algorithm, from https://arxiv.org/abs/1509.02971\n    has Actor, Critic, and each has its own target network\n    Implementation referred from https://github.com/pemami4911/deep-rl\n    \'\'\'\n\n    def __init__(self, *args, **kwargs):\n        # import only when needed to contain side-effects\n        from keras import backend as K\n        self.K = K\n        self.sess = self.K.get_session()\n        self.actor = Actor(*args, **kwargs)\n        self.critic = Critic(*args, **kwargs)\n        self.sess.run(self.K.tf.global_variables_initializer())\n        super(DDPG, self).__init__(*args, **kwargs)\n\n    def build_model(self):\n        pass\n\n    def compile_model(self):\n        pass\n\n    def recompile_model(self, sys_vars):\n        pass\n\n    def select_action(self, state):\n        return self.policy.select_action(state)\n\n    def update(self, sys_vars):\n        # Update target networks\n        self.actor.update(sys_vars)\n        self.critic.update(sys_vars)\n        self.policy.update(sys_vars)\n        self.update_n_epoch(sys_vars)\n\n    def train_an_epoch(self):\n        minibatch = self.memory.rand_minibatch(self.batch_size)\n\n        # train critic\n        mu_prime = self.actor.target_predict(minibatch[\'next_states\'])\n        q_val = self.critic.target_predict(minibatch[\'states\'], mu_prime)\n        q_prime = self.critic.target_predict(\n            minibatch[\'next_states\'], mu_prime)\n        # reshape for element-wise multiplication\n        # to feed into network, y shape needs to be (?, 1)\n        y = minibatch[\'rewards\'] + self.gamma * \\\n            (1 - minibatch[\'terminals\']) * np.reshape(q_prime, (-1))\n        y = np.reshape(y, (-1, 1))\n\n        # update memory, needed for PER\n        errors = abs(np.sum(q_val - y, axis=1))\n        # Q size is only 1, from critic\n        assert y.shape == (self.batch_size, 1)\n        assert errors.shape == (self.batch_size, )\n        self.memory.update(errors)\n\n        _, _, critic_loss = self.critic.train_tf(\n            minibatch[\'states\'], minibatch[\'actions\'], y)\n\n        # train actor\n        # Update the actor policy using the sampled gradient\n        actions = self.actor.predict(minibatch[\'states\'])\n        critic_action_gradient = self.critic.get_action_gradient(\n            minibatch[\'states\'], actions)\n        # currently cant be gotten\n        _actorloss = self.actor.train_tf(\n            minibatch[\'states\'], critic_action_gradient)\n\n        loss = critic_loss\n        return loss\n'"
rl/agent/deep_exp_sarsa.py,0,"b""import numpy as np\nfrom rl.agent.deep_sarsa import DeepSarsa\n\n\nclass DeepExpectedSarsa(DeepSarsa):\n\n    '''\n    Deep Expected Sarsa agent.\n    On policy, with updates after each experience\n    Policy = epsilonGreedyPolicy\n    '''\n\n    def compute_Q_states(self, minibatch):\n        (Q_states, Q_next_states, _max) = super(\n            DeepExpectedSarsa, self).compute_Q_states(minibatch)\n\n        curr_e = self.policy.e\n        curr_e_per_a = curr_e / self.env_spec['action_dim']\n\n        Q_next_states_max = np.amax(Q_next_states, axis=1)\n        Q_next_states_selected = (1 - curr_e) * Q_next_states_max + \\\n            np.sum(Q_next_states * curr_e_per_a, axis=1)\n        return (Q_states, Q_next_states, Q_next_states_selected)\n"""
rl/agent/deep_sarsa.py,0,"b""import numpy as np\nfrom rl.agent.dqn import DQN\n\n\nclass DeepSarsa(DQN):\n\n    '''\n    Deep Sarsa agent.\n    On policy, with updates after each experience\n    Policy = epsilonGreedyPolicy\n    '''\n\n    def __init__(self, *args, **kwargs):\n        super(DeepSarsa, self).__init__(*args, **kwargs)\n        self.train_per_n_new_exp = 1\n        self.batch_size = 1\n        self.n_epoch = 1\n        self.final_n_epoch = 1\n\n    def compute_Q_states(self, minibatch):\n        (Q_states, Q_next_states, _max) = super(\n            DeepSarsa, self).compute_Q_states(minibatch)\n        next_action = self.select_action(minibatch['next_states'][0])\n        Q_next_states_selected = Q_next_states[:, next_action]\n        return (Q_states, Q_next_states, Q_next_states_selected)\n\n    def train_an_epoch(self):\n        minibatch = self.memory.pop()\n        (Q_states, _next, Q_next_states_selected\n         ) = self.compute_Q_states(minibatch)\n        Q_targets = self.compute_Q_targets(\n            minibatch, Q_states, Q_next_states_selected)\n        loss = self.model.train_on_batch(minibatch['states'], Q_targets)\n\n        errors = abs(np.sum(Q_states - Q_targets, axis=1))\n        assert Q_targets.shape == (\n            self.batch_size, self.env_spec['action_dim'])\n        assert errors.shape == (self.batch_size, )\n        self.memory.update(errors)\n        return loss\n"""
rl/agent/double_conv_dqn.py,0,"b""from rl.agent.conv_dqn import ConvDQN\nfrom rl.agent.double_dqn import DoubleDQN\n\n\nclass DoubleConvDQN(DoubleDQN, ConvDQN):\n\n    '''\n    The base class of double convolutional DQNs\n    extended from DoubleDQN and ConvDQN\n    multiple inheritance will use the method from the first class\n    if multiple ones exists\n    '''\n\n    def build_hidden_layers(self, model):\n        ConvDQN.build_hidden_layers(self, model)\n"""
rl/agent/double_dqn.py,0,"b'import numpy as np\nfrom rl.agent.dqn import DQN\nfrom rl.util import logger, clone_model, clone_optimizer\n\n\nclass DoubleDQN(DQN):\n\n    \'\'\'\n    The base class of double DQNs\n    \'\'\'\n\n    def build_model(self):\n        super(DoubleDQN, self).build_model()\n\n        model_2 = clone_model(self.model)\n        logger.info(""Model 2 summary"")\n        model_2.summary()\n        self.model_2 = model_2\n\n        logger.info(""Models 1 and 2 built"")\n        return self.model, self.model_2\n\n    def compile_model(self):\n        self.optimizer.keras_optimizer_2 = clone_optimizer(\n            self.optimizer.keras_optimizer)\n        self.model.compile(\n            loss=\'mse\',\n            optimizer=self.optimizer.keras_optimizer)\n        self.model_2.compile(\n            loss=\'mse\',\n            optimizer=self.optimizer.keras_optimizer_2)\n        logger.info(""Models 1 and 2 compiled"")\n\n    def switch_models(self):\n         # Switch model 1 and model 2, also the optimizers\n        temp = self.model\n        self.model = self.model_2\n        self.model_2 = temp\n\n        temp_optimizer = self.optimizer.keras_optimizer\n        self.optimizer.keras_optimizer = self.optimizer.keras_optimizer_2\n        self.optimizer.keras_optimizer_2 = temp_optimizer\n\n    # def recompile_model(self, sys_vars):\n    #     \'\'\'rotate and recompile both models\'\'\'\n    #     # TODO fix this, double recompile breaks solving power\n    #     if self.epi_change_lr is not None:\n    #         self.switch_models()  # to model_2\n    #         super(DoubleDQN, self).recompile_model(sys_vars)\n    #         self.switch_models()  # back to model\n    #         super(DoubleDQN, self).recompile_model(sys_vars)\n    #     return self.model\n\n    def compute_Q_states(self, minibatch):\n        (Q_states, Q_next_states_select, _max) = super(\n            DoubleDQN, self).compute_Q_states(minibatch)\n        # Different from (single) dqn: Select max using model 2\n        Q_next_states_max_ind = np.argmax(Q_next_states_select, axis=1)\n        # same as dqn again, but use Q_next_states_max_ind above\n        Q_next_states = np.clip(\n            self.model_2.predict(minibatch[\'next_states\']),\n            -self.clip_val, self.clip_val)\n        rows = np.arange(Q_next_states_max_ind.shape[0])\n        Q_next_states_max = Q_next_states[rows, Q_next_states_max_ind]\n\n        return (Q_states, Q_next_states, Q_next_states_max)\n\n    def train_an_epoch(self):\n        self.switch_models()\n        return super(DoubleDQN, self).train_an_epoch()\n'"
rl/agent/dqn.py,0,"b'import numpy as np\nfrom rl.agent.base_agent import Agent\nfrom rl.util import logger, log_self\n\n\nclass DQN(Agent):\n\n    \'\'\'\n    The base class of DQNs, with the core methods\n    The simplest deep Q network,\n    with epsilon-greedy method and\n    Bellman equation for value, using neural net.\n    \'\'\'\n\n    def __init__(self, env_spec,\n                 train_per_n_new_exp=1,\n                 gamma=0.95, lr=0.1,\n                 epi_change_lr=None,\n                 batch_size=16, n_epoch=5, hidden_layers=None,\n                 hidden_layers_activation=\'sigmoid\',\n                 output_layer_activation=\'linear\',\n                 auto_architecture=False,\n                 num_hidden_layers=3,\n                 first_hidden_layer_size=256,\n                 num_initial_channels=16,\n                 **kwargs):  # absorb generic param without breaking\n        # import only when needed to contain side-effects\n        from keras.layers.core import Dense\n        from keras.models import Sequential, load_model\n        self.Dense = Dense\n        self.Sequential = Sequential\n        self.load_model = load_model\n\n        super(DQN, self).__init__(env_spec)\n\n        self.train_per_n_new_exp = train_per_n_new_exp\n        self.gamma = gamma\n        self.lr = lr\n        self.epi_change_lr = epi_change_lr\n        self.batch_size = batch_size\n        self.n_epoch = 1\n        self.final_n_epoch = n_epoch\n        self.hidden_layers = hidden_layers or [4]\n        self.hidden_layers_activation = hidden_layers_activation\n        self.output_layer_activation = output_layer_activation\n        self.clip_val = 10000\n        self.auto_architecture = auto_architecture\n        self.num_hidden_layers = num_hidden_layers\n        self.first_hidden_layer_size = first_hidden_layer_size\n        self.num_initial_channels = num_initial_channels\n        log_self(self)\n        self.build_model()\n\n    def build_hidden_layers(self, model):\n        \'\'\'\n        build the hidden layers into model using parameter self.hidden_layers\n        \'\'\'\n\n        # Auto architecture infers the size of the hidden layers from the size\n        # of the first layer. Each successive hidden layer is half the size of the\n        # previous layer\n        # Enables hyperparameter optimization over network architecture\n        if self.auto_architecture:\n            curr_layer_size = self.first_hidden_layer_size\n            model.add(self.Dense(curr_layer_size,\n                                 input_shape=(self.env_spec[\'state_dim\'],),\n                                 activation=self.hidden_layers_activation,\n                                 init=\'lecun_uniform\'))\n\n            curr_layer_size = int(curr_layer_size / 2)\n            for i in range(1, self.num_hidden_layers):\n                model.add(self.Dense(curr_layer_size,\n                                     init=\'lecun_uniform\',\n                                     activation=self.hidden_layers_activation))\n                curr_layer_size = int(curr_layer_size / 2)\n\n        else:\n            model.add(self.Dense(self.hidden_layers[0],\n                                 input_shape=(self.env_spec[\'state_dim\'],),\n                                 activation=self.hidden_layers_activation,\n                                 init=\'lecun_uniform\'))\n            # inner hidden layer: no specification of input shape\n            if (len(self.hidden_layers) > 1):\n                for i in range(1, len(self.hidden_layers)):\n                    model.add(self.Dense(\n                        self.hidden_layers[i],\n                        init=\'lecun_uniform\',\n                        activation=self.hidden_layers_activation))\n\n        return model\n\n    def build_model(self):\n        model = self.Sequential()\n        self.build_hidden_layers(model)\n        model.add(self.Dense(self.env_spec[\'action_dim\'],\n                             init=\'lecun_uniform\',\n                             activation=self.output_layer_activation))\n        logger.info(""Model summary"")\n        model.summary()\n        self.model = model\n\n        logger.info(""Model built"")\n        return self.model\n\n    def compile_model(self):\n        self.model.compile(\n            loss=\'mse\',\n            optimizer=self.optimizer.keras_optimizer)\n        logger.info(""Model compiled"")\n\n    def recompile_model(self, sys_vars):\n        \'\'\'\n        Option to change model optimizer settings\n        Currently only used for changing the learning rate\n        Compiling does not affect the model weights\n        \'\'\'\n        if self.epi_change_lr is not None:\n            if (sys_vars[\'epi\'] == self.epi_change_lr and\n                    sys_vars[\'t\'] == 0):\n                self.lr = self.lr / 10.0\n                self.optimizer.change_optim_param(**{\'lr\': self.lr})\n                self.model.compile(\n                    loss=\'mse\',\n                    optimizer=self.optimizer.keras_optimizer)\n                logger.info(\'Model recompiled with new settings: \'\n                            \'Learning rate: {}\'.format(self.lr))\n        return self.model\n\n    def update_n_epoch(self, sys_vars):\n        \'\'\'\n        Increase epochs at the beginning of each session,\n        for training for later episodes,\n        once it has more experience\n        Best so far, increment num epochs every 2 up to a max of 5\n        \'\'\'\n        if (self.n_epoch < self.final_n_epoch and\n                sys_vars[\'t\'] == 0 and\n                sys_vars[\'epi\'] % 2 == 0):\n            self.n_epoch += 1\n        return self.n_epoch\n\n    def select_action(self, state):\n        \'\'\'epsilon-greedy method\'\'\'\n        return self.policy.select_action(state)\n\n    def update(self, sys_vars):\n        \'\'\'\n        Agent update apart from training the Q function\n        \'\'\'\n        self.policy.update(sys_vars)\n        self.update_n_epoch(sys_vars)\n        self.recompile_model(sys_vars)\n\n    def to_train(self, sys_vars):\n        \'\'\'\n        return boolean condition if agent should train\n        get n NEW experiences before training model\n        \'\'\'\n        t = sys_vars[\'t\']\n        done = sys_vars[\'done\']\n        timestep_limit = self.env_spec[\'timestep_limit\']\n        return (t > 0) and bool(\n            t % self.train_per_n_new_exp == 0 or\n            t == (timestep_limit-1) or\n            done)\n\n    def compute_Q_states(self, minibatch):\n        # note the computed values below are batched in array\n        Q_states = np.clip(self.model.predict(minibatch[\'states\']),\n                           -self.clip_val, self.clip_val)\n        Q_next_states = np.clip(self.model.predict(minibatch[\'next_states\']),\n                                -self.clip_val, self.clip_val)\n        Q_next_states_max = np.amax(Q_next_states, axis=1)\n        return (Q_states, Q_next_states, Q_next_states_max)\n\n    def compute_Q_targets(self, minibatch, Q_states, Q_next_states_max):\n        # make future reward 0 if exp is terminal\n        Q_targets_a = minibatch[\'rewards\'] + self.gamma * \\\n            (1 - minibatch[\'terminals\']) * Q_next_states_max\n        # set batch Q_targets of a as above, the rest as is\n        # minibatch[\'actions\'] is one-hot encoded\n        Q_targets = minibatch[\'actions\'] * Q_targets_a[:, np.newaxis] + \\\n            (1 - minibatch[\'actions\']) * Q_states\n        return Q_targets\n\n    def train_an_epoch(self):\n        minibatch = self.memory.rand_minibatch(self.batch_size)\n\n        (Q_states, _states, Q_next_states_max) = self.compute_Q_states(\n            minibatch)\n        Q_targets = self.compute_Q_targets(\n            minibatch, Q_states, Q_next_states_max)\n        loss = self.model.train_on_batch(minibatch[\'states\'], Q_targets)\n\n        errors = abs(np.sum(Q_states - Q_targets, axis=1))\n        assert Q_targets.shape == (\n            self.batch_size, self.env_spec[\'action_dim\'])\n        assert errors.shape == (self.batch_size, )\n        self.memory.update(errors)\n        return loss\n\n    def train(self, sys_vars):\n        \'\'\'\n        Training is for the Q function (NN) only\n        otherwise (e.g. policy) see self.update()\n        step 1,2,3,4 of algo.\n        \'\'\'\n        loss_total = 0\n        for _epoch in range(self.n_epoch):\n            loss = self.train_an_epoch()\n            loss_total += loss\n        avg_loss = loss_total / self.n_epoch\n        sys_vars[\'loss\'].append(avg_loss)\n        return avg_loss\n\n    def save(self, model_path, global_step=None):\n        logger.info(\'Saving model checkpoint\')\n        self.model.save_weights(model_path)\n\n    def restore(self, model_path):\n        self.model.load_weights(model_path, by_name=False)\n'"
rl/agent/freeze_dqn.py,0,"b'import numpy as np\nfrom rl.agent.double_dqn import DoubleDQN\nfrom rl.agent.dqn import DQN\nfrom rl.util import logger, clone_model\n\n\nclass FreezeDQN(DoubleDQN):\n\n    \'\'\'\n    Extends DQN agent to freeze target Q network\n    and periodically update them to the weights of the\n    exploration model\n    Avoids oscillations and breaks correlation\n    between Q-network and target\n    http://www0.cs.ucl.ac.uk/staff/d.silver/web/Resources_files/deep_rl.pdf\n    Exploration model periodically cloned into target Q network\n    \'\'\'\n\n    def compute_Q_states(self, minibatch):\n        Q_states = np.clip(self.model.predict(minibatch[\'states\']),\n                           -self.clip_val, self.clip_val)\n        Q_next_states = np.clip(self.model_2.predict(minibatch[\'next_states\']),\n                                -self.clip_val, self.clip_val)\n        Q_next_states_max = np.amax(Q_next_states, axis=1)\n        return (Q_states, Q_next_states, Q_next_states_max)\n\n    def train_an_epoch(self):\n        # Should call DQN to train an epoch, not DoubleDQN\n        return DQN.train_an_epoch(self)\n\n    def update_target_model(self):\n        # Also, loading logic seems off\n        self.model_2 = clone_model(self.model)\n        logger.debug(""Updated target model weights"")\n\n    def update(self, sys_vars):\n        \'\'\'\n        Agent update apart from training the Q function\n        \'\'\'\n        done = sys_vars[\'done\']\n        timestep_check = sys_vars[\'t\'] == (self.env_spec[\'timestep_limit\'] - 1)\n        if done or timestep_check:\n            self.update_target_model()\n        super(FreezeDQN, self).update(sys_vars)\n'"
rl/agent/offpol_sarsa.py,0,"b""import numpy as np\nfrom rl.agent.dqn import DQN\n\n\nclass OffPolicySarsa(DQN):\n\n    '''\n    Deep Sarsa agent.\n    Off policy. Reduces to Q learning when eval_e = 0\n    Evaluation policy = epsilonGreedyPolicy, eval_e = 0.05\n    Experience generating policy = Boltzmann or\n    EpsilonGreedy with annealing\n    '''\n\n    def __init__(self, *args, **kwargs):\n        super(OffPolicySarsa, self).__init__(*args, **kwargs)\n        self.eval_e = 0.05\n\n    def compute_Q_states(self, minibatch):\n        (Q_states, Q_next_states, _max) = super(\n            OffPolicySarsa, self).compute_Q_states(minibatch)\n\n        e_per_action = self.eval_e / self.env_spec['action_dim']\n\n        Q_next_states_max = np.amax(Q_next_states, axis=1)\n        Q_next_states_selected = (1 - self.eval_e) * Q_next_states_max + \\\n            np.sum(Q_next_states * e_per_action, axis=1)\n        return (Q_states, None, Q_next_states_selected)\n"""
rl/agent/q_table.py,0,"b'import numpy as np\nfrom rl.agent.base_agent import Agent\n\n\nclass Dummy(Agent):\n\n    \'\'\'\n    A dummy agent that does random actions, for demo\n    \'\'\'\n\n    def select_action(self, state):\n        \'\'\'epsilon-greedy method\'\'\'\n        action = np.random.choice(self.env_spec[\'actions\'])\n        return action\n\n    def update(self, sys_vars):\n        return\n\n    def to_train(self, sys_vars):\n        return True\n\n    def train(self, sys_vars):\n        return\n\n    def build_model(self):\n        return\n\n    def compile_model(self):\n        return\n\n\nclass QTable(Agent):\n\n    \'\'\'\n    The simplest Q learner - a table,\n    with epsilon-greedy method and\n    Bellman equation for value.\n    \'\'\'\n\n    def __init__(self, env_spec,\n                 resolution=10,\n                 gamma=0.95, lr=0.1,\n                 init_e=1.0, final_e=0.1, exploration_anneal_episodes=1000,\n                 **kwargs):  # absorb generic param without breaking\n        super(QTable, self).__init__(env_spec)\n        self.resolution = resolution\n        self.gamma = gamma\n        self.lr = lr\n        self.init_e = init_e\n        self.final_e = final_e\n        self.e = self.init_e\n        self.exploration_anneal_episodes = exploration_anneal_episodes\n        self.build_model()\n\n    def build_model(self):\n        \'\'\'\n        init the 2D qtable by\n        bijecting the state space into pixelated, flattened vector\n        multiplied with\n        list of possible discrete actions\n        \'\'\'\n        self.pixelate_state_space(self.resolution)\n        flat_state_size = self.resolution ** self.env_spec[\'state_dim\']\n        self.qtable = np.random.uniform(\n            low=-1, high=1,\n            size=(flat_state_size, self.env_spec[\'action_dim\']))\n        return self.qtable\n\n    def compile_model(self):\n        return\n\n    def pixelate_state_space(self, resolution=10):\n        \'\'\'chunk up the state space hypercube to specified resolution\'\'\'\n        state_bounds = np.transpose(\n            [self.env_spec[\'state_bound_low\'],\n             self.env_spec[\'state_bound_high\']])\n        self.state_pixels = [np.linspace(*sb, num=resolution+1)\n                             for sb in state_bounds]\n        return self.state_pixels\n\n    def flatten_state(self, state):\n        \'\'\'\n        collapse a hyperdim state by binning into state_pixels\n        then flattening the pixel_state into 1-dim bijection\n        \'\'\'\n        val_space_pairs = list(zip(state, self.state_pixels))\n        pixel_state = [np.digitize(*val_space)\n                       for val_space in val_space_pairs]  # binning\n        flat_state = int("""".join([str(ps) for ps in pixel_state]))\n        return flat_state\n\n    def select_action(self, state):\n        \'\'\'epsilon-greedy method\'\'\'\n        if self.e > np.random.rand():\n            action = np.random.choice(self.env_spec[\'actions\'])\n        else:\n            flat_state = self.flatten_state(state)\n            action = np.argmax(self.qtable[flat_state, :])\n        return action\n\n    def update_e(self):\n        \'\'\'strategy to update epsilon\'\'\'\n        self.e = max(self.e -\n                     (self.init_e - self.final_e) /\n                     float(self.exploration_anneal_episodes),\n                     self.final_e)\n        return self.e\n\n    def update(self, sys_vars):\n        self.update_e()\n\n    def to_train(self, sys_vars):\n        return True\n\n    def train(self, sys_vars):\n        \'\'\'\n        run the basic bellman equation update\n        \'\'\'\n        last_exp = self.memory.pop()\n        state = last_exp[\'states\'][0]\n        flat_state = self.flatten_state(state)\n        next_state = last_exp[\'next_states\'][0]\n        next_flat_state = self.flatten_state(next_state)\n        action = np.argmax(last_exp[\'actions\'][0])  # from one-hot\n        reward = last_exp[\'rewards\'][0]\n        Q_state_action = self.qtable[flat_state, action]\n        Q_next_state = self.qtable[next_flat_state, :]\n        Q_next_state_max = np.amax(Q_next_state)\n        loss = (reward + self.gamma * Q_next_state_max - Q_state_action)\n        sys_vars[\'loss\'].append(loss)\n\n        self.qtable[flat_state, action] = Q_state_action + \\\n            self.lr * loss\n        return self.qtable\n'"
rl/hyperoptimizer/__init__.py,0,"b""from rl.util import import_package_files\n\n__all__ = ['__all__'] + import_package_files(globals(), locals(), __file__)\n"""
rl/hyperoptimizer/base_hyperoptimizer.py,0,"b""import copy\nimport multiprocessing as mp\nimport os\nimport time\nfrom collections import OrderedDict\nfrom rl.util import logger, timestamp, PARALLEL_PROCESS_NUM, debug_mem_usage\n\n\nclass HyperOptimizer(object):\n\n    '''\n    The base class of hyperparam optimizer, with core methods\n    read about it on the documentation\n    input: Trial (and some specs), param space P (as standardized specs)\n    Algo:\n    1. search the next p in P using its internal search algo,\n    add to its internal `param_search_list`\n    2. run a (slow) function Trial(p) = score (inside trial data)\n    3. update search using feedback score\n    4. repeat till max steps or fitness condition met\n\n    it will be ran by the experiment as:\n    hyperopt = HyperOptimizer(Trial, **experiment_kwargs)\n    experiment_data = hyperopt.run()\n    '''\n\n    def __init__(self, Trial, **kwargs):\n        self.Trial = Trial\n        self.REQUIRED_ARGS = [\n            'experiment_spec',\n            'experiment_id_override',\n            'times'\n        ]\n        self.PARALLEL_PROCESS_NUM = PARALLEL_PROCESS_NUM\n        self.free_cpu = self.PARALLEL_PROCESS_NUM  # for parallel run\n        logger.info('Initialize {}'.format(self.__class__.__name__))\n        self.set_keys(**kwargs)\n        self.init_search()\n\n    def set_keys(self, **kwargs):\n        assert all(k in kwargs for k in self.REQUIRED_ARGS), \\\n            'kwargs do not have all REQUIRED_ARGS'\n        for k in kwargs:\n            setattr(self, k, kwargs[k])\n\n        self.experiment_name = self.experiment_spec.get('experiment_name')\n        self.run_timestamp = timestamp()\n        self.experiment_id = self.experiment_id_override or '{}-{}'.format(\n            self.experiment_name, self.run_timestamp)\n        self.experiment_data = []\n        self.param_search_list = []\n        # the index of next param to try in param_search_list\n        self.next_trial_num = len(self.param_search_list)\n\n        self.default_param = self.experiment_spec['param']\n        unordered_param_range = self.experiment_spec['param_range']\n        # import ordering for param_range for search serialization\n        self.param_range = OrderedDict(sorted(unordered_param_range.items()))\n        self.param_range_keys = sorted(self.param_range.keys())\n\n    def compose_experiment_spec(self, param):\n        new_experiment_spec = copy.deepcopy(self.experiment_spec)\n        new_experiment_spec.pop('param_range', None)\n        new_experiment_spec.update({\n            'param': param,\n        })\n        return new_experiment_spec\n\n    def init_search(self):\n        '''initialize the search algo and the search space'''\n        raise NotImplementedError()\n\n    def search(self):\n        '''\n        algo step 1, search and return the next p for Trial(p),\n        Its only job is to append to (or modify)\n        its internal self.param_search_list using its search logic\n        It may refer to self.experiment_data as search memory\n        and whatever new pointer or special memory implemented by a HyperOptimizer class\n        '''\n        raise NotImplementedError()\n\n    def next_param(self):\n        '''retrieve trial_num and param, advance the class next_trial_num'''\n        assert self.next_trial_num < len(self.param_search_list), \\\n            'param_search_list expansion cannot keep up with next_trial_num'\n        trial_num = self.next_trial_num\n        param = self.param_search_list[self.next_trial_num]\n        self.next_trial_num = self.next_trial_num + 1\n        return (trial_num, param)\n\n    def run_trial(self, trial_num, param):\n        '''\n        algo step 2, construct and run Trial with the next param\n        args trial_num, param must be provided externally,\n        otherwise they will not progress within mp.process\n        '''\n        experiment_spec = self.compose_experiment_spec(param)\n        trial = self.Trial(\n            experiment_spec, trial_num=trial_num,\n            times=self.times,\n            num_of_trials=self.num_of_trials,\n            run_timestamp=self.run_timestamp,\n            experiment_id_override=self.experiment_id_override)\n        trial_data = trial.run()\n        del trial\n        import gc\n        gc.collect()\n        debug_mem_usage()\n        return trial_data\n\n    # retrieve the trial_num, param, fitness_score from trial_data\n    @classmethod\n    def get_fitness(cls, trial_data):\n        trial_id = trial_data['trial_id']\n        trial_num = trial_id.split('_').pop()\n        param = trial_data['experiment_spec']['param']\n        metrics = trial_data['metrics']\n        fitness_score = metrics['fitness_score']\n        return trial_num, param, fitness_score\n\n    def update_search(self):\n        '''algo step 3, update search algo using self.experiment_data'''\n        raise NotImplementedError()\n\n    def to_terminate(self):\n        '''algo step 4, terminate when at max steps or fitness condition met'''\n        raise NotImplementedError()\n\n    # handler task after a search is complete from multiprocessing pool\n    def post_search(self, trial_data):\n        self.experiment_data.append(trial_data)\n        self.update_search()\n        self.free_cpu += 1\n\n    @classmethod\n    def pool_init(self):\n        # you can never be too safe in multiprocessing gc\n        import gc\n        gc.collect()\n\n    @classmethod\n    def raise_error(cls, e):\n        logger.error('Pool worker throws Exception')\n        print(e.__cause__)\n        time.sleep(1)\n        os._exit(1)\n\n    def run(self):\n        '''\n        top level method to run the entire hyperoptimizer\n        will gather and compose experiment_data, then return it\n        '''\n        logger.info('Run {}'.format(self.__class__.__name__))\n        # crucial maxtasksperchild to free up memory by respawning worker\n        pool = mp.Pool(self.PARALLEL_PROCESS_NUM,\n                       initializer=self.pool_init, maxtasksperchild=1)\n        while (not self.to_terminate()):\n            if self.free_cpu > 0:\n                self.free_cpu -= 1  # update\n                self.search()  # add to self.param_search_list\n                trial_num, param = self.next_param()\n                pool.apply_async(\n                    self.run_trial, (trial_num, param),\n                    callback=self.post_search, error_callback=self.raise_error)\n            else:\n                pass  # keep looping till free_cpu available\n            time.sleep(0.02)  # prevent cpu overwork from while loop\n        pool.close()\n        pool.join()\n        return self.experiment_data\n"""
rl/hyperoptimizer/grid_search.py,0,"b""import copy\nimport itertools\nfrom rl.hyperoptimizer.line_search import LineSearch\n\n\nclass GridSearch(LineSearch):\n\n    def init_search(self):\n        '''\n        convert a dict of param ranges into\n        a list of cartesian products of param_range\n        e.g. {'a': [1,2], 'b': [3]} into\n        [{'a': 1, 'b': 3}, {'a': 2, 'b': 3}]\n        note that this is order-preserving, as required by design\n        '''\n        range_vals = self.param_range.values()\n        for vals in itertools.product(*range_vals):\n            param = copy.deepcopy(self.default_param)\n            param.update(dict(zip(self.param_range_keys, vals)))\n            self.param_search_list.append(param)\n        self.num_of_trials = len(self.param_search_list)\n"""
rl/hyperoptimizer/line_search.py,0,"b""import copy\nfrom rl.hyperoptimizer.base_hyperoptimizer import HyperOptimizer\n\n\nclass LineSearch(HyperOptimizer):\n\n    def init_search(self):\n        '''\n        convert a dict of param ranges into\n        a list parameter settings corresponding\n        to a line search of the param range\n        for each param\n        All other parameters set to default vals\n        note that this is order-preserving, as required by design\n        '''\n        for key in self.param_range_keys:\n            vals = self.param_range[key]\n            for val in vals:\n                param = copy.deepcopy(self.default_param)\n                param[key] = val\n                self.param_search_list.append(param)\n        self.num_of_trials = len(self.param_search_list)\n\n    def search(self):\n        '''no action needed here for exhaustive trials'''\n        return\n\n    def update_search(self):\n        '''no action needed here for exhaustive trials'''\n        return\n\n    def to_terminate(self):\n        return not (self.next_trial_num < len(self.param_search_list))\n"""
rl/hyperoptimizer/random_search.py,0,"b""import json\nimport numpy as np\nfrom rl.analytics import ideal_fitness_score\nfrom rl.hyperoptimizer.base_hyperoptimizer import HyperOptimizer\nfrom rl.util import PROBLEMS, to_json, logger\n\n\nclass RandomSearch(HyperOptimizer):\n\n    '''\n    Random Search by sampling on hysphere around a search path\n    algo:\n    1. init x a random position in space\n    2. until termination (max_eval or fitness, e.g. solved all), do:\n        2.1 sample new pos some radius away: next_x = x + r\n        2.2 if f(next_x) > f(x) then set x = next_x\n\n    Extra search memory units:\n    - search_path\n    - best_point\n\n    save for experiment resume, search_history:\n    - search_path\n    - best_point\n    - param_search_list\n    '''\n\n    # # calculate the constant radius needed to traverse unit cube\n    # def cube_traversal_radius(self):\n    #     traversal_diameter = 1/np.power(self.max_evals,\n    #                                     1/self.search_dim)\n    #     traversal_radius = traversal_diameter/2\n    #     return traversal_radius\n\n    def decay_search_radius(self):\n        '''\n        start of half cube for diameter (0.25 radius) then decay\n        at 100 searches, will shrink to 1/10 of initial radius 0.025\n        clip to prevent going too small (0.01)\n        '''\n        min_radius = 0.01\n        linear_decay_rate = self.next_trial_num/10./self.PARALLEL_PROCESS_NUM\n        self.search_radius = np.clip(\n            self.init_search_radius / linear_decay_rate,\n            min_radius, self.init_search_radius)\n\n    @classmethod\n    def sample_hypersphere(cls, dim, r=1):\n        '''Marsaglia algo for sampling uniformly on a hypersphere'''\n        v = np.random.randn(dim)\n        v = v * r / np.linalg.norm(v)\n        return v\n\n    def sample_cube(self):\n        return np.random.rand(self.search_dim)\n\n    def sample_r(self):\n        return self.sample_hypersphere(\n            self.search_dim, self.search_radius)\n\n    # biject [0, 1] to [x_min, x_max]\n    def biject_continuous(self, norm_val, x_min, x_max):\n        return np.around(norm_val*(x_max - x_min) + x_min, self.precision)\n\n    # biject [0, 1] to x_list = [a, b, c, ...] by binning\n    def biject_discrete(self, norm_val, x_list):\n        list_len = len(x_list)\n        inds = np.arange(list_len)\n        cont_val = self.biject_continuous(norm_val, 0, list_len)\n        ind = np.digitize(cont_val, inds) - 1\n        return x_list[ind]\n\n    # biject one dimension: [0, 1] to a param_range val\n    def biject_dim(self, norm_val, dim_spec):\n        if isinstance(dim_spec, list):  # discrete\n            return self.biject_discrete(norm_val, dim_spec)\n        else:  # cont\n            return self.biject_continuous(\n                norm_val, dim_spec['min'], dim_spec['max'])\n        return\n\n    # biject a vector on unit cube into a param in param_space\n    def biject_param(self, v):\n        param = {}\n        for i, param_key in enumerate(self.param_range_keys):\n            dim_spec = self.param_range[param_key]\n            param[param_key] = self.biject_dim(v[i], dim_spec)\n        return param\n\n    def init_search(self):\n        '''\n        Initialize the random search internal variables\n        '''\n        self.max_evals = self.experiment_spec['param']['max_evals']\n        self.num_of_trials = self.max_evals\n        self.search_dim = len(self.param_range_keys)\n        self.precision = 4  # decimal roundoff biject_continuous\n        self.search_radius = self.init_search_radius = 0.5\n        self.search_count = 0  # number of times search() has ran\n        self.search_exhausted = False\n        self.search_path = []\n        self.best_point = {\n            'trial_num': None,\n            'param': None,\n            'x': self.sample_cube(),\n            'fitness_score': float('-inf'),\n        }\n        problem = PROBLEMS.get(self.experiment_spec['problem'])\n        self.ideal_fitness_score = ideal_fitness_score(problem)\n        logger.info(\n            'ideal_fitness_scrore: {}'.format(self.ideal_fitness_score))\n\n        self.filename = './data/{}/random_search_history.json'.format(\n            self.experiment_id)\n        if self.experiment_id_override is not None:\n            self.load()  # resume\n\n    def search(self):\n        '''\n        algo step 2.1 sample new pos some radius away: next_x = x + r\n        update search_path and param_search_list\n        '''\n        self.search_count += 1\n        if self.next_trial_num < len(self.search_path):  # resuming\n            next_x = self.search_path[self.next_trial_num]\n            next_param = self.param_search_list[self.next_trial_num]\n        else:\n            next_x = np.clip(self.best_point['x'] + self.sample_r(), 0., 1.)\n            # check if too close to previously searched x\n            distances = [np.linalg.norm(next_x - old_x)\n                         for old_x in self.search_path]\n            distances = np.around(distances, self.precision)\n\n            if self.search_count > (10 * self.max_evals):\n                self.search_exhausted = True  # exhausted search space\n                next_param = self.biject_param(next_x)\n                self.search_path.append(next_x)\n                self.param_search_list.append(next_param)\n            elif len(distances) > 0 and np.amin(distances) == 0:\n                self.search()\n            else:\n                next_param = self.biject_param(next_x)\n                self.search_path.append(next_x)\n                self.param_search_list.append(next_param)\n\n    def update_search(self):\n        '''\n        algo step 2.2 if f(next_x) > f(x) then set x = next_x\n        invoked right after the latest run_trial()\n        update self.best_point\n        '''\n        if (self.next_trial_num < self.PARALLEL_PROCESS_NUM or\n                self.next_trial_num < len(self.search_path)):\n            # yet to have history or still resuming from history\n            return\n        assert len(self.experiment_data) > 0, \\\n            'self.experiment_data must not be empty for update_search'\n\n        self.decay_search_radius()\n\n        x = self.search_path[-1]\n        trial_data = self.experiment_data[-1]\n        trial_num, param, fitness_score = self.get_fitness(trial_data)\n        if fitness_score > self.best_point['fitness_score']:\n            self.best_point = {\n                'trial_num': trial_num,\n                'param': param,\n                'x': x,\n                'fitness_score': fitness_score,\n            }\n        self.save()\n\n    def save(self):\n        search_history = {\n            'search_path': self.search_path,\n            'search_count': self.search_count,\n            'best_point': self.best_point,\n            'param_search_list': self.param_search_list,\n        }\n        with open(self.filename, 'w') as f:\n            f.write(to_json(search_history))\n        logger.info(\n            'Save search history to {}'.format(self.filename))\n        return\n\n    def load(self):\n        try:\n            search_history = json.loads(open(self.filename).read())\n            self.search_path = search_history['search_path']\n            self.best_point = search_history['best_point']\n            self.param_search_list = search_history['param_search_list']\n            logger.info('Load search history from {}'.format(self.filename))\n        except (FileNotFoundError, json.JSONDecodeError):\n            logger.info(\n                'Fail to load search history from {}'.format(self.filename))\n            return None\n\n    def satisfy_fitness(self):\n        '''\n        break on the first strong solution\n        '''\n        best_fitness_score = self.best_point['fitness_score']\n        if self.next_trial_num < self.PARALLEL_PROCESS_NUM:\n            return False\n        elif best_fitness_score > self.ideal_fitness_score:\n            logger.info(\n                'fitness_score {} > ideal_fitness_score {}, '\n                'could terminate early'.format(\n                    best_fitness_score, self.ideal_fitness_score))\n            # return True\n            # TODO fix ideal_fitness_score\n            return False\n        else:\n            return False\n\n    def to_terminate(self):\n        return (self.search_exhausted or\n                self.next_trial_num >= self.max_evals or\n                self.satisfy_fitness())\n"""
rl/memory/__init__.py,0,"b""from rl.util import import_package_files\n\n__all__ = ['__all__'] + import_package_files(globals(), locals(), __file__)\n"""
rl/memory/base_memory.py,0,"b""class Memory(object):\n\n    '''\n    The base class of Memory, with the core methods\n    '''\n\n    def __init__(self, env_spec, **kwargs):  # absorb generic param without breaking\n        '''Construct externally, and set at Agent.compile()'''\n        self.env_spec = env_spec\n        self.agent = None\n        self.state = None\n\n    def reset_state(self, init_state):\n        '''reset the state of LinearMemory per episode env.reset()'''\n        self.state = init_state\n\n    def add_exp(self, action, reward, next_state, terminal):\n        '''add an experience'''\n        raise NotImplementedError()\n\n    def get_exp(self, inds):\n        '''get a batch of experiences by indices'''\n        raise NotImplementedError()\n\n    def pop(self):\n        '''get the last experience (batched like get_exp()'''\n        raise NotImplementedError()\n\n    def size(self):\n        '''get a batch of experiences by indices'''\n        raise NotImplementedError()\n\n    def rand_minibatch(self, size):\n        '''get a batch of experiences by indices'''\n        raise NotImplementedError()\n\n    def update(self, updates):\n        '''update elements of the memory as requires'''\n        raise NotImplementedError()\n"""
rl/memory/linear.py,0,"b""import numpy as np\nfrom rl.memory.base_memory import Memory\nfrom rl.util import log_self\nfrom scipy.stats import halfnorm\n\n\nclass LinearMemory(Memory):\n\n    '''\n    The replay memory used for random minibatch training\n    '''\n\n    # absorb generic param without breaking\n    def __init__(self, env_spec, **kwargs):\n        super(LinearMemory, self).__init__(env_spec)\n        self.exp_keys = [\n            'states', 'actions', 'rewards', 'next_states', 'terminals']\n        self.exp = {k: [] for k in self.exp_keys}\n        log_self(self)\n\n    def encode_action(self, action):\n        '''encode action based on continuous/discrete before adding'''\n        if self.agent.env_spec['actions'] == 'continuous':\n            return action\n        else:  # do one-hot encoding\n            action_arr = np.zeros(self.agent.env_spec['action_dim'])\n            action_arr[action] = 1\n            return action_arr\n\n    def add_exp(self, action, reward, next_state, terminal):\n        '''\n        after the env.step(a) that returns s', r,\n        using the previously stored state for the s,\n        form an experience tuple <s, a, r, s'>\n        '''\n        self.exp['states'].append(self.state)\n        self.exp['actions'].append(self.encode_action(action))\n        self.exp['rewards'].append(reward)\n        self.exp['next_states'].append(next_state)\n        self.exp['terminals'].append(int(terminal))\n        self.state = next_state\n\n    def _get_exp(self, exp_name, inds):\n        return np.array([self.exp[exp_name][i] for i in inds])\n\n    def get_exp(self, inds):\n        return {k: self._get_exp(k, inds) for k in self.exp_keys}\n\n    def pop(self):\n        '''convenient method to get exp at [last_ind]'''\n        assert self.size() > 0, 'memory is empty, cannot pop'\n        return self.get_exp([self.size() - 1])\n\n    def size(self):\n        return len(self.exp['rewards'])\n\n    def rand_minibatch(self, size):\n        '''plain random sampling'''\n        memory_size = self.size()\n        rand_inds = np.random.randint(memory_size, size=size)\n        minibatch = self.get_exp(rand_inds)\n        return minibatch\n\n    def update(self, updates):\n        pass\n\n\nclass LinearMemoryWithForgetting(LinearMemory):\n\n    '''\n    Linear memory with uniform sampling, retaining last 50k experiences\n    '''\n\n    def __init__(self, env_spec, max_mem_len=50000,\n                 **kwargs):  # absorb generic param without breaking\n        super(LinearMemoryWithForgetting, self).__init__(env_spec)\n        self.max_mem_len = max_mem_len\n\n    def trim_exp(self):\n        '''The forgetting mechanism'''\n        if (self.size() > self.max_mem_len):\n            for k in self.exp_keys:\n                del self.exp[k][0]\n\n    def add_exp(self, action, reward, next_state, terminal):\n        '''\n        add exp as usual, but preserve only the recent episodes\n        '''\n        super(LinearMemoryWithForgetting, self).add_exp(\n            action, reward, next_state, terminal)\n        self.trim_exp()\n\n\nclass LeftTailMemory(LinearMemory):\n\n    '''\n    Memory with sampling via a left-tail distribution\n    '''\n\n    def rand_minibatch(self, size):\n        '''\n        get a minibatch of random exp for training\n        use simple memory decay, i.e. sample with a left tail\n        distribution to draw more from latest memory\n        then append with the most recent, untrained experience\n        '''\n        memory_size = self.size()\n        new_exp_size = self.agent.train_per_n_new_exp\n        if memory_size <= size or memory_size <= new_exp_size:\n            inds = np.random.randint(memory_size, size=size)\n        else:\n            new_memory_ind = max(0, memory_size - new_exp_size)\n            old_memory_ind = max(0, new_memory_ind - 1)\n            latest_inds = np.arange(new_memory_ind, memory_size)\n            random_batch_size = size - new_exp_size\n            rand_inds = (old_memory_ind - halfnorm.rvs(\n                size=random_batch_size,\n                scale=float(old_memory_ind)*0.80).astype(int))\n            inds = np.concatenate([rand_inds, latest_inds]).clip(0)\n        minibatch = self.get_exp(inds)\n        return minibatch\n"""
rl/memory/prioritized_exp_replay.py,0,"b""import numpy as np\nfrom rl.memory.linear import LinearMemoryWithForgetting\n\n\nclass PrioritizedExperienceReplay(LinearMemoryWithForgetting):\n\n    '''\n    Replay memory with random sampling weighted by the absolute\n    size of the value function error\n\n    Adapted from https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n    memory unit\n    '''\n\n    def __init__(self, env_spec, max_mem_len=None, e=0.01, alpha=0.6,\n                 **kwargs):\n        if max_mem_len is None:  # auto calculate mem len\n            max_timestep = env_spec['timestep_limit']\n            max_epis = env_spec['problem']['MAX_EPISODES']\n            memory_epi = np.ceil(max_epis / 3.).astype(int)\n            max_mem_len = max(10**6, max_timestep * memory_epi)\n        super(PrioritizedExperienceReplay, self).__init__(\n            env_spec, max_mem_len)\n        self.exp_keys.append('error')\n        self.exp = {k: [] for k in self.exp_keys}  # reinit with added mem key\n        # Prevents experiences with error of 0 from being replayed\n        self.e = e\n        # Controls how spiked the distribution is. alpha = 0 means uniform\n        self.alpha = alpha\n        self.curr_data_inds = None\n        self.curr_tree_inds = None\n        self.prio_tree = SumTree(self.max_mem_len)\n        self.head = 0\n\n    def get_priority(self, error):\n        # add min_priority to prevent root of negative = complex\n        p = (error + self.e) ** self.alpha\n        assert np.isfinite(p)\n        return p\n\n    def add_exp(self, action, reward, next_state, terminal):\n        '''Round robin memory updating'''\n        # init error to reward first, update later\n        error = abs(reward)\n        p = self.get_priority(error)\n\n        if self.size() < self.max_mem_len:  # add as usual\n            super(PrioritizedExperienceReplay, self).add_exp(\n                action, reward, next_state, terminal)\n            self.exp['error'].append(error)\n        else:  # replace round robin\n            self.exp['states'][self.head] = self.state\n            self.exp['actions'][self.head] = self.encode_action(action)\n            self.exp['rewards'][self.head] = reward\n            self.exp['next_states'][self.head] = next_state\n            self.exp['terminals'][self.head] = int(terminal)\n            self.exp['error'][self.head] = error\n            self.state = next_state\n\n        self.head += 1\n        if self.head >= self.max_mem_len:\n            self.head = 0  # reset for round robin\n\n        self.prio_tree.add(p)\n\n        assert self.head == self.prio_tree.head, 'prio_tree head is wrong'\n\n    def rand_minibatch(self, size):\n        '''random sampling weighted by priority'''\n        self.curr_tree_inds, self.curr_data_inds = self.select_prio_inds(size)\n        minibatch = self.get_exp(self.curr_data_inds)\n        return minibatch\n\n    def select_prio_inds(self, size):\n        tree_inds = []\n        data_inds = []\n        segment = self.prio_tree.total() / size\n\n        for i in range(size):\n            a = segment * i\n            b = segment * (i + 1)\n\n            s = np.random.uniform(a, b)\n            t_idx, d_idx = self.prio_tree.get(s)\n            tree_inds.append(t_idx)\n            data_inds.append(d_idx)\n\n        return tree_inds, data_inds\n\n    def update(self, updates):\n        for i, u in enumerate(updates):\n            t_idx = self.curr_tree_inds[i]\n            d_idx = self.curr_data_inds[i]\n            p = self.get_priority(u)\n            self.prio_tree.update(t_idx, p)\n            self.exp['error'][d_idx] = u\n\n\nclass SumTree(object):\n\n    '''\n    Adapted from  https://github.com/jaara/AI-blog/blob/master/SumTree.py\n    See https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/\n    for a good introduction to PER\n    '''\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.tree = np.zeros(2*capacity - 1)\n        self.head = 0\n\n    def _propagate(self, idx, change):\n        parent = (idx - 1) // 2\n        self.tree[parent] += change\n        if parent != 0:\n            self._propagate(parent, change)\n\n    def _retrieve(self, idx, s):\n        left = 2 * idx + 1\n        right = left + 1\n\n        if left >= len(self.tree):\n            return idx\n\n        if s <= self.tree[left]:\n            return self._retrieve(left, s)\n        else:\n            return self._retrieve(right, s-self.tree[left])\n\n    def total(self):\n        return self.tree[0]\n\n    def add(self, p):\n        idx = self.head + self.capacity - 1\n        self.update(idx, p)\n        self.head += 1\n        if self.head >= self.capacity:\n            self.head = 0\n\n    def update(self, idx, p):\n        change = p - self.tree[idx]\n        self.tree[idx] = p\n        self._propagate(idx, change)\n\n    def get(self, s):\n        idx = self._retrieve(0, s)\n        data_idx = idx - self.capacity + 1\n        return idx, data_idx\n"""
rl/memory/ranked.py,0,"b'import numpy as np\nfrom rl.memory.linear import LinearMemory\nfrom rl.util import log_self\nimport math\n\n\nclass HighLowMemory(LinearMemory):\n\n    \'\'\'\n    Memory divided into two: good and bad experiences\n    As with RankedMemory experiences are grouped episodically\n    Episodes with a total reward > threshold are assigned to good memory\n    The threshold is recomputed every n episodes and\n    episodes are reassigned accordingly.\n    Memories are sampled from good experiences with a self.prob_high\n    Memories are sampled from bad experiences with a 1 - self.prob_high\n    Experiences are sampled from a maximum of 3 randomly selected episodes,\n    per minibatch for each of the high and low memories\n    TODO improvement: do a more natural continuous range to sort high low\n    by self.epi_memory.sort(key=lambda epi_exp: epi_exp[\'total_rewards\'])\n    \'\'\'\n\n    # absorb generic param without breaking\n    def __init__(self, env_spec, **kwargs):\n        super(HighLowMemory, self).__init__(env_spec)\n        # use the old self.exp as buffer, remember to clear\n        self.last_exp = self.exp\n        self.epi_memory_high = []\n        self.epi_memory_low = []\n        self.max_reward = -math.inf\n        self.min_reward = math.inf\n        # 1st  5 epis goes into bad half, recompute every 5 epis\n        self.threshold = math.inf\n        self.threshold_history = []\n        self.epi_num = 0\n        self.prob_high = 0.66\n        self.num_epis_to_sample = 3\n        self.max_epis_in_mem = 15\n        self.recompute_freq = 10\n        log_self(self)\n\n    def reassign_episodes(self):\n        new_high, new_low = []\n\n        for mem in (self.epi_memory_high, self.epi_memory_low):\n            for epi_exp in mem:\n                if (epi_exp[\'total_rewards\'] > self.threshold):\n                    new_high.append(epi_exp)\n                else:\n                    new_low.append(epi_exp)\n\n        self.epi_memory_high = new_high\n        self.epi_memory_low = new_low\n\n    def compute_threshold(self):\n        self.threshold_history.append([self.threshold,\n                                       self.max_reward,\n                                       self.min_reward])\n        if (len(self.threshold_history) > 1):\n            # Scaled because this threshold seems too severe based on trial\n            # runs\n            self.threshold =  \\\n                max(self.threshold,\n                    (self.max_reward + self.min_reward) / 2.0 * 0.75)\n        else:\n            self.threshold = (self.max_reward + self.min_reward) / 2.0 * 0.75\n        self.reassign_episodes()\n        self.max_reward = -math.inf\n        self.min_reward = math.inf\n\n    def add_exp(self, action, reward, next_state, terminal):\n        super(HighLowMemory, self).add_exp(\n            action, reward, next_state, terminal)\n        if terminal:\n            epi_exp = {\n                \'exp\': self.exp,\n                \'total_rewards\': np.sum(self.exp[\'rewards\']),\n                \'epi_num\': self.epi_num\n            }\n            if (epi_exp[\'total_rewards\'] <= self.threshold):\n                self.epi_memory_low.append(epi_exp)\n            else:\n                self.epi_memory_high.append(epi_exp)\n            if (self.epi_num > 0 and self.epi_num % self.recompute_freq == 0):\n                self.compute_threshold()\n            if (epi_exp[\'total_rewards\'] > self.max_reward):\n                self.max_reward = epi_exp[\'total_rewards\']\n            if (epi_exp[\'total_rewards\'] < self.min_reward):\n                self.min_reward = epi_exp[\'total_rewards\']\n            self.last_exp = self.exp\n            self.exp = {k: [] for k in self.exp_keys}\n            self.epi_num += 1\n            # print(""THRESHOLD HISTORY"")\n            # print(self.threshold_history)\n            # print(""HIGH MEM"")\n            # for epi in self.epi_memory_high:\n            #     print(str(epi[\'total_rewards\'])+ "" ,"", end="" "")\n            # print()\n            # print(""LOW MEM"")\n            # for epi in self.epi_memory_low:\n            #     print(str(epi[\'total_rewards\'] )+ "" ,"", end="" "")\n            # print()\n\n    def pop(self):\n        \'\'\'convenient method to get exp at [last_ind]\'\'\'\n        buffer_exp = self.exp  # store for restore later\n        self.exp = self.last_exp\n        res = super(HighLowMemory, self).pop()\n        self.exp = buffer_exp\n        return res\n\n    def rand_minibatch(self, size):\n        # base case, early exit\n        high_samples = np.int(np.ceil(size * self.prob_high))\n        low_samples = size - high_samples\n\n        if (len(self.epi_memory_high) == 0 and\n                len(self.epi_memory_low) == 0):\n            return super(HighLowMemory, self).rand_minibatch(size)\n\n        if (len(self.epi_memory_high) == 0):\n            high_samples = 0\n            low_samples = size\n\n        high_samples_per_epi = np.int(\n            np.ceil(high_samples / self.num_epis_to_sample))\n        low_samples_per_epi = np.int(\n            np.ceil(low_samples / self.num_epis_to_sample))\n\n        buffer_exp = self.exp\n        minibatch_as_list = []\n        if high_samples > 0:\n            for _i in range(4):\n                idx = np.random.randint(0, len(self.epi_memory_high))\n                epi_exp = self.epi_memory_high[idx][\'exp\']\n                self.exp = epi_exp\n                epi_minibatch = super(HighLowMemory, self).rand_minibatch(\n                    high_samples_per_epi)\n                minibatch_as_list.append(epi_minibatch)\n\n        if low_samples > 0:\n            for _i in range(4):\n                idx = np.random.randint(0, len(self.epi_memory_low))\n                epi_exp = self.epi_memory_low[idx][\'exp\']\n                self.exp = epi_exp\n                epi_minibatch = super(HighLowMemory, self).rand_minibatch(\n                    low_samples_per_epi)\n                minibatch_as_list.append(epi_minibatch)\n\n        # set buffer back to original\n        self.exp = buffer_exp\n\n        # merge all minibatches from best_epi_memory into a minibatch\n        minibatch = {}\n        for k in self.exp_keys:\n            k_exp = np.concatenate(\n                [epi_exp[k] for epi_exp in minibatch_as_list]\n            )[-size:]\n            minibatch[k] = k_exp\n        assert len(\n            minibatch[\'rewards\']) == size, \'minibatch has the wrong size\'\n\n        return minibatch\n\n    def update(self, updates):\n        pass\n\n\nclass HighLowMemoryWithForgetting(HighLowMemory):\n\n    \'\'\'\n    Like HighLowMemory but also has forgetting capability\n    Controlled by max_epis_in_mem param\n    \'\'\'\n\n    # absorb generic param without breaking\n    def __init__(self, env_spec, **kwargs):\n        super(HighLowMemoryWithForgetting, self).__init__(env_spec)\n        self.max_epis_in_mem = 250\n        log_self(self)\n\n    def reassign_episodes(self):\n        new_high, new_low = []\n\n        for mem in (self.epi_memory_high, self.epi_memory_low):\n            for epi_exp in mem:\n                if (self.epi_num - epi_exp[\'epi_num\'] <= self.max_epis_in_mem):\n                    if (epi_exp[\'total_rewards\'] > self.threshold):\n                        new_high.append(epi_exp)\n                    else:\n                        new_low.append(epi_exp)\n\n        self.epi_memory_high = new_high\n        self.epi_memory_low = new_low\n'"
rl/optimizer/__init__.py,0,"b""from rl.util import import_package_files\n\n__all__ = ['__all__'] + import_package_files(globals(), locals(), __file__)\n"""
rl/optimizer/adam.py,0,"b""from rl.optimizer.base_optimizer import Optimizer\n\n\nclass AdamOptimizer(Optimizer):\n\n    '''\n    Adam optimizer\n    Potential param:\n        lr (learning rate)\n        beta_1\n        beta_2\n        epsilon\n        decay\n        Suggested to leave at default param with the expected of lr\n    '''\n\n    def __init__(self, **kwargs):\n        from keras.optimizers import Adam\n        self.Adam = Adam\n\n        self.optim_param_keys = ['lr', 'beta_1', 'beta_2', 'epsilon', 'decay']\n        super(AdamOptimizer, self).__init__(**kwargs)\n\n    def init_optimizer(self):\n        self.keras_optimizer = self.Adam(**self.optim_param)\n"""
rl/optimizer/base_optimizer.py,0,"b'from rl.util import log_self, logger\n\n\nclass Optimizer(object):\n\n    \'\'\'\n    The base class of Optimizer, with the core methods\n    \'\'\'\n\n    def __init__(self, **kwargs):\n        \'\'\'Construct externally, and set at Agent.compile()\'\'\'\n        self.agent = None\n        self.keras_optimizer = None\n        self.optim_param = {}\n        self.update_optim_param(**kwargs)\n        self.init_optimizer()\n        log_self(self)\n\n    def update_optim_param(self, **kwargs):\n        o_param = {\n            k: kwargs.get(k) for k in self.optim_param_keys\n            if kwargs.get(k) is not None}\n        self.optim_param.update(o_param)\n\n    def init_optimizer(self):\n        raise NotImplementedError()\n\n    def change_optim_param(self, **new_param):\n        self.update_optim_param(**new_param)\n        self.init_optimizer()\n        logger.info(""Optimizer param changed"")\n        log_self(self)\n'"
rl/optimizer/rmsprop.py,0,"b""from rl.optimizer.base_optimizer import Optimizer\n\n\nclass RMSpropOptimizer(Optimizer):\n\n    '''\n    RMS prop\n    Potential param:\n        lr (learning rate)\n        rho\n        decay\n        epsilon\n    '''\n\n    def __init__(self, **kwargs):\n        from keras.optimizers import RMSprop\n        self.RMSprop = RMSprop\n\n        self.optim_param_keys = ['lr', 'rho', 'decay', 'epsilon']\n        super(RMSpropOptimizer, self).__init__(**kwargs)\n\n    def init_optimizer(self):\n        self.keras_optimizer = self.RMSprop(**self.optim_param)\n"""
rl/optimizer/sgd.py,0,"b""from rl.optimizer.base_optimizer import Optimizer\n\n\nclass SGDOptimizer(Optimizer):\n\n    '''\n    Stochastic gradient descent\n    Potential param:\n        lr (learning rate)\n        momentum\n        decay\n        nesterov\n    '''\n\n    def __init__(self, **kwargs):\n        from keras.optimizers import SGD\n        self.SGD = SGD\n\n        self.optim_param_keys = ['lr', 'momentum', 'decay', 'nesterov']\n        super(SGDOptimizer, self).__init__(**kwargs)\n\n    def init_optimizer(self):\n        self.keras_optimizer = self.SGD(**self.optim_param)\n"""
rl/policy/__init__.py,0,"b""from rl.util import import_package_files\n\n__all__ = ['__all__'] + import_package_files(globals(), locals(), __file__)\n"""
rl/policy/actor_critic.py,0,"b""import numpy as np\nfrom rl.policy.base_policy import Policy\nfrom rl.util import log_self\n\n\nclass ArgmaxPolicy(Policy):\n\n    '''\n    The argmax policy for actor critic agents\n    Agent takes the action with the highest\n    action score\n    '''\n\n    def __init__(self, env_spec,\n                 **kwargs):  # absorb generic param without breaking\n        super(ArgmaxPolicy, self).__init__(env_spec)\n        log_self(self)\n\n    def select_action(self, state):\n        agent = self.agent\n        state = np.expand_dims(state, axis=0)\n        A_score = agent.actor.predict(state)[0]  # extract from batch predict\n        assert A_score.ndim == 1\n        action = np.argmax(A_score)\n        return action\n\n    def update(self, sys_vars):\n        pass\n\n\nclass SoftmaxPolicy(Policy):\n\n    '''\n    The softmax policy for actor critic agents\n    Action is drawn from the prob dist generated\n    by softmax(acion_scores)\n    '''\n\n    def __init__(self, env_spec,\n                 **kwargs):  # absorb generic param without breaking\n        super(SoftmaxPolicy, self).__init__(env_spec)\n        self.clip_val = 500.\n        log_self(self)\n\n    def select_action(self, state):\n        agent = self.agent\n        state = np.expand_dims(state, axis=0)\n        A_score = agent.actor.predict(state)[0]  # extract from batch predict\n        assert A_score.ndim == 1\n        A_score = A_score.astype('float64')  # fix precision overflow\n        exp_values = np.exp(\n            np.clip(A_score, -self.clip_val, self.clip_val))\n        assert np.isfinite(exp_values).all()\n        probs = np.array(exp_values / np.sum(exp_values))\n        probs /= probs.sum()  # renormalize to prevent floating pt error\n        action = np.random.choice(agent.env_spec['actions'], p=probs)\n        return action\n\n    def update(self, sys_vars):\n        pass\n\n\nclass GaussianPolicy(Policy):\n\n    '''\n    Continuous policy for actor critic models\n    Output of the actor network is the mean action\n    along each dimension. Action chosen is the mean\n    plus some noise parameterized by the variance\n    '''\n\n    def __init__(self, env_spec,\n                 variance=1.0,\n                 **kwargs):  # absorb generic param without breaking\n        super(GaussianPolicy, self).__init__(env_spec)\n        self.variance = variance\n        log_self(self)\n\n    def select_action(self, state):\n        agent = self.agent\n        state = np.expand_dims(state, axis=0)\n        a_mean = agent.actor.predict(state)[0]  # extract from batch predict\n        action = a_mean + np.random.normal(\n            loc=0.0, scale=self.variance, size=a_mean.shape)\n        action = np.clip(action,\n                         self.env_spec['action_bound_low'],\n                         self.env_spec['action_bound_high'])\n        return action\n\n    def update(self, sys_vars):\n        pass\n\n\nclass BoundedPolicy(Policy):\n\n    '''\n    The bounded policy for actor critic agents\n    and continous, bounded policy spaces\n    Action bounded above and below by\n    - action_bound, + action_bound\n    '''\n\n    def __init__(self, env_spec,\n                 **kwargs):  # absorb generic param without breaking\n        super(BoundedPolicy, self).__init__(env_spec)\n        self.action_bound = env_spec['action_bound_high']\n        assert env_spec['action_bound_high'] == -env_spec['action_bound_low']\n        log_self(self)\n\n    def select_action(self, state):\n        agent = self.agent\n        state = np.expand_dims(state, axis=0)\n        A_score = agent.actor.predict(state)[0]  # extract from batch predict\n        action = np.tanh(A_score) * self.action_bound\n        return action\n\n    def update(self, sys_vars):\n        pass\n"""
rl/policy/base_policy.py,0,"b""class Policy(object):\n\n    '''\n    The base class of Policy, with the core methods\n    '''\n\n    def __init__(self, env_spec,\n                 **kwargs):  # absorb generic param without breaking\n        '''Construct externally, and set at Agent.compile()'''\n        self.env_spec = env_spec\n        self.agent = None\n\n    def select_action(self, state):\n        raise NotImplementedError()\n\n    def update(self, sys_vars):\n        raise NotImplementedError()\n"""
rl/policy/boltzmann.py,0,"b""import numpy as np\nfrom rl.policy.base_policy import Policy\nfrom rl.util import log_self\n\n\nclass BoltzmannPolicy(Policy):\n\n    '''\n    The Boltzmann policy, where prob dist for selection\n    p = exp(Q/tau) / sum(Q[a]/tau)\n    '''\n\n    def __init__(self, env_spec,\n                 init_tau=5., final_tau=0.5, exploration_anneal_episodes=20,\n                 **kwargs):  # absorb generic param without breaking\n        super(BoltzmannPolicy, self).__init__(env_spec)\n        self.init_tau = init_tau\n        self.final_tau = final_tau\n        self.tau = self.init_tau\n        self.exploration_anneal_episodes = exploration_anneal_episodes\n        self.clip_val = 500.\n        log_self(self)\n\n    def select_action(self, state):\n        agent = self.agent\n        state = np.expand_dims(state, axis=0)\n        Q_state = agent.model.predict(state)[0]  # extract from batch predict\n        assert Q_state.ndim == 1\n        Q_state = Q_state.astype('float64')  # fix precision overflow\n        exp_values = np.exp(\n            np.clip(Q_state / self.tau, -self.clip_val, self.clip_val))\n        assert np.isfinite(exp_values).all()\n        probs = np.array(exp_values / np.sum(exp_values))\n        probs /= probs.sum()  # renormalize to prevent floating pt error\n        action = np.random.choice(agent.env_spec['actions'], p=probs)\n        return action\n\n    def update(self, sys_vars):\n        '''strategy to update tau in agent'''\n        epi = sys_vars['epi']\n        rise = self.final_tau - self.init_tau\n        slope = rise / float(self.exploration_anneal_episodes)\n        self.tau = max(slope * epi + self.init_tau, self.final_tau)\n        return self.tau\n\n\nclass DoubleDQNBoltzmannPolicy(BoltzmannPolicy):\n\n    '''\n    Same as the Boltzmann policy but for a Double DQN agent\n    '''\n\n    def __init__(self, env_spec,\n                 init_tau=5., final_tau=0.5, exploration_anneal_episodes=20,\n                 **kwargs):  # absorb generic param without breaking\n        super(DoubleDQNBoltzmannPolicy, self).__init__(\n            env_spec, init_tau, final_tau,\n            exploration_anneal_episodes)\n\n    def select_action(self, state):\n        agent = self.agent\n        state = np.expand_dims(state, axis=0)\n        # extract from batch predict\n        Q_state1 = agent.model.predict(state)[0]\n        Q_state2 = agent.model_2.predict(state)[0]\n        Q_state = Q_state1 + Q_state2\n        assert Q_state.ndim == 1\n        Q_state = Q_state.astype('float64')  # fix precision overflow\n        exp_values = np.exp(\n            np.clip(Q_state / self.tau, -self.clip_val, self.clip_val))\n        assert np.isfinite(exp_values).all()\n        probs = np.array(exp_values / np.sum(exp_values))\n        probs /= probs.sum()  # renormalize to prevent floating pt error\n        action = np.random.choice(agent.env_spec['actions'], p=probs)\n        return action\n"""
rl/policy/epsilon_greedy.py,0,"b""import numpy as np\nfrom rl.policy.base_policy import Policy\nfrom rl.util import log_self\n\n\nclass EpsilonGreedyPolicy(Policy):\n\n    '''\n    The Epsilon-greedy policy\n    '''\n\n    def __init__(self, env_spec,\n                 init_e=1.0, final_e=0.1, exploration_anneal_episodes=30,\n                 **kwargs):  # absorb generic param without breaking\n        super(EpsilonGreedyPolicy, self).__init__(env_spec)\n        self.init_e = init_e\n        self.final_e = final_e\n        self.e = self.init_e\n        self.exploration_anneal_episodes = exploration_anneal_episodes\n        log_self(self)\n\n    def select_action(self, state):\n        '''epsilon-greedy method'''\n        agent = self.agent\n        if self.e > np.random.rand():\n            action = np.random.choice(agent.env_spec['actions'])\n        else:\n            state = np.expand_dims(state, axis=0)\n            # extract from batch predict\n            Q_state = agent.model.predict(state)[0]\n            assert Q_state.ndim == 1\n            action = np.argmax(Q_state)\n        return action\n\n    def update(self, sys_vars):\n        '''strategy to update epsilon in agent'''\n        epi = sys_vars['epi']\n        rise = self.final_e - self.init_e\n        slope = rise / float(self.exploration_anneal_episodes)\n        self.e = max(slope * epi + self.init_e, self.final_e)\n        return self.e\n\n\nclass DoubleDQNEpsilonGreedyPolicy(EpsilonGreedyPolicy):\n\n    '''\n    Policy to accompany double dqn agents\n    When actions are not random this policy\n    selects actions by symming the outputs from\n    each of the two Q-state approximators\n    before taking the max of the result\n    '''\n\n    def __init__(self, env_spec,\n                 init_e=1.0, final_e=0.1, exploration_anneal_episodes=30,\n                 **kwargs):  # absorb generic param without breaking\n        super(DoubleDQNEpsilonGreedyPolicy, self).__init__(\n            env_spec, init_e, final_e,\n            exploration_anneal_episodes)\n\n    def select_action(self, state):\n        '''epsilon-greedy method'''\n        agent = self.agent\n        if self.e > np.random.rand():\n            action = np.random.choice(agent.env_spec['actions'])\n        else:\n            state = np.expand_dims(state, axis=0)\n            # extract from batch predict\n            Q_state1 = agent.model.predict(state)[0]\n            Q_state2 = agent.model_2.predict(state)[0]\n            Q_state = Q_state1 + Q_state2\n            assert Q_state.ndim == 1\n            action = np.argmax(Q_state)\n        return action\n\n\nclass DecayingEpsilonGreedyPolicy(EpsilonGreedyPolicy):\n\n    '''\n    Inspired by alvacarce's solution to mountain car\n    https://gym.openai.com/evaluations/eval_t3GN2Xb0R5KpyjkJUGsLw\n    '''\n\n    def __init__(self, env_spec,\n                 init_e=1.0, final_e=0.1, exploration_anneal_episodes=30,\n                 **kwargs):  # absorb generic param without breaking\n        super(DecayingEpsilonGreedyPolicy, self).__init__(\n            env_spec, init_e, final_e, exploration_anneal_episodes)\n        self.e_decay = 0.9997\n\n    def update(self, sys_vars):\n        _epi = sys_vars['epi']\n        if self.e > self.final_e:\n            self.e = self.e * self.e_decay\n        return self.e\n\n\nclass OscillatingEpsilonGreedyPolicy(EpsilonGreedyPolicy):\n\n    '''\n    The epsilon-greedy policy with oscillating epsilon\n    periodically agent.e will drop to a fraction of\n    the current exploration rate\n    '''\n\n    def update(self, sys_vars):\n        '''strategy to update epsilon in agent'''\n        super(OscillatingEpsilonGreedyPolicy, self).update(\n            sys_vars)\n        epi = sys_vars['epi']\n        if not (epi % 3) and epi > 15:\n            # drop to 1/3 of the current exploration rate\n            self.e = max(self.e/3., self.final_e)\n        return self.e\n\n\nclass TargetedEpsilonGreedyPolicy(EpsilonGreedyPolicy):\n\n    '''\n    switch between active and inactive exploration cycles by\n    partial mean rewards and its distance to the target mean rewards\n    '''\n\n    def update(self, sys_vars):\n        '''strategy to update epsilon in agent'''\n        epi = sys_vars['epi']\n        assert sys_vars['SOLVED_MEAN_REWARD'] is not None, \\\n            'this policy needs an explicit target SOLVED_MEAN_REWARD'\n        SOLVED_MEAN_REWARD = sys_vars['SOLVED_MEAN_REWARD']\n        REWARD_MEAN_LEN = sys_vars['REWARD_MEAN_LEN']\n        PARTIAL_MEAN_LEN = int(REWARD_MEAN_LEN * 0.20)\n        if epi < 1:  # corner case when no total_rewards_history to avg\n            return\n        # the partial mean for projection the entire mean\n        partial_mean_reward = np.mean(\n            sys_vars['total_rewards_history'][-PARTIAL_MEAN_LEN:])\n        # difference to target, and its ratio (1 if denominator is 0)\n        min_reward = np.amin(sys_vars['total_rewards_history'])\n        projection_gap = SOLVED_MEAN_REWARD - partial_mean_reward\n        worst_gap = SOLVED_MEAN_REWARD - min_reward\n        gap_ratio = projection_gap / worst_gap\n        envelope = self.init_e + (self.final_e - self.init_e) / 2. * \\\n            (float(epi)/float(self.exploration_anneal_episodes))\n        pessimistic_gap_ratio = envelope * min(2 * gap_ratio, 1)\n        # if is in odd cycle, and diff is still big, actively explore\n        active_exploration_cycle = not bool(\n            int(epi/PARTIAL_MEAN_LEN) % 2) and (\n            projection_gap > abs(SOLVED_MEAN_REWARD * 0.05))\n        self.e = max(pessimistic_gap_ratio * self.init_e, self.final_e)\n\n        if not active_exploration_cycle:\n            self.e = max(self.e/2., self.final_e)\n        return self.e\n"""
rl/policy/noise.py,0,"b""import numpy as np\nfrom rl.util import log_self\nfrom rl.policy.base_policy import Policy\nfrom rl.policy.epsilon_greedy import EpsilonGreedyPolicy\n\n\nclass NoNoisePolicy(Policy):\n\n    '''\n    The base class for noise policy for DDPG\n    default is no noise\n    '''\n\n    def __init__(self, env_spec,\n                 **kwargs):  # absorb generic param without breaking\n        super(NoNoisePolicy, self).__init__(env_spec)\n        log_self(self)\n\n    def sample(self):\n        '''implement noise here, default is none'''\n        assert 'actions' in self.env_spec\n        return 0\n\n    def select_action(self, state):\n        agent = self.agent\n        state = np.expand_dims(state, axis=0)\n        if self.env_spec['actions'] == 'continuous':\n            action = agent.actor.predict(state)[0] + self.sample()\n            action = np.clip(action,\n                             self.env_spec['action_bound_low'],\n                             self.env_spec['action_bound_high'])\n        else:\n            Q_state = agent.actor.predict(state)[0]\n            assert Q_state.ndim == 1\n            action = np.argmax(Q_state)\n        return action\n\n    def update(self, sys_vars):\n        pass\n\n\nclass LinearNoisePolicy(NoNoisePolicy):\n\n    '''\n    policy with linearly decaying noise (1. / (1. + self.epi))\n    '''\n\n    def __init__(self, env_spec, exploration_anneal_episodes=20,\n                 **kwargs):  # absorb generic param without breaking\n        super(LinearNoisePolicy, self).__init__(env_spec)\n        self.exploration_anneal_episodes = exploration_anneal_episodes\n        self.n_step = 0  # init\n        log_self(self)\n\n    def sample(self):\n        noise = (1. / (1. + self.n_step))\n        return noise\n\n    def update(self, sys_vars):\n        epi = sys_vars['epi']\n        if epi >= self.exploration_anneal_episodes:\n            self.n_step = np.inf  # noise divide to zero\n        else:\n            self.n_step = sys_vars['epi']\n\n\nclass EpsilonGreedyNoisePolicy(EpsilonGreedyPolicy, NoNoisePolicy):\n\n    '''\n    akin to epsilon greedy decay,\n    but return random sample instead\n    '''\n\n    def sample(self):\n        if self.e > np.random.rand():\n            noise = np.random.uniform(\n                0.5 * self.env_spec['action_bound_low'],\n                0.5 * self.env_spec['action_bound_high'])\n        else:\n            noise = 0\n        return noise\n\n    def select_action(self, state):\n        return NoNoisePolicy.select_action(self, state)\n\n\nclass AnnealedGaussianPolicy(LinearNoisePolicy):\n\n    '''\n    Base class of random noise policy for DDPG\n    Adopted from\n    https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n    '''\n\n    def __init__(self, env_spec, exploration_anneal_episodes,\n                 mu, sigma, sigma_min,\n                 **kwargs):  # absorb generic param without breaking\n        super(AnnealedGaussianPolicy, self).__init__(\n            env_spec, exploration_anneal_episodes)\n        self.size = env_spec['action_dim']\n        self.mu = mu\n        self.sigma = sigma\n\n        if sigma_min is not None:\n            self.m = -(sigma - sigma_min) / self.exploration_anneal_episodes\n            self.c = sigma\n            self.sigma_min = sigma_min\n        else:\n            self.m = 0.\n            self.c = sigma\n            self.sigma_min = sigma\n\n    @property\n    def current_sigma(self):\n        sigma = max(self.sigma_min, self.m * self.n_step + self.c)\n        return sigma\n\n\nclass GaussianWhiteNoisePolicy(AnnealedGaussianPolicy):\n\n    def __init__(self, env_spec, exploration_anneal_episodes=20,\n                 mu=0., sigma=.3, sigma_min=None,\n                 **kwargs):  # absorb generic param without breaking\n        super(GaussianWhiteNoisePolicy, self).__init__(\n            env_spec, exploration_anneal_episodes,\n            mu, sigma, sigma_min)\n\n    def sample(self):\n        sample = np.random.normal(self.mu, self.current_sigma, self.size)\n        return sample\n\n\nclass OUNoisePolicy(AnnealedGaussianPolicy):\n\n    '''\n    Based on\n    http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n    '''\n\n    def __init__(self, env_spec, exploration_anneal_episodes=20,\n                 theta=.15, mu=0., sigma=.3, dt=1e-2, x0=None, sigma_min=None,\n                 **kwargs):  # absorb generic param without breaking\n        super(OUNoisePolicy, self).__init__(\n            env_spec, exploration_anneal_episodes,\n            mu, sigma, sigma_min,\n            **kwargs)\n        self.theta = theta\n        self.mu = mu\n        self.dt = dt\n        self.x0 = x0\n        self.reset_states()\n\n    def reset_states(self):\n        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)\n\n    def sample(self):\n        x = self.x_prev + self.theta * \\\n            (self.mu - self.x_prev) * self.dt + self.current_sigma * \\\n            np.sqrt(self.dt) * np.random.normal(size=self.size)\n        self.x_prev = x\n        return x\n"""
rl/preprocessor/__init__.py,0,"b""from rl.util import import_package_files\n\n__all__ = ['__all__'] + import_package_files(globals(), locals(), __file__)\n"""
rl/preprocessor/atari.py,0,"b""import numpy as np\nimport scipy as sp\nfrom rl.preprocessor.base_preprocessor import PreProcessor\n\n\n# Util functions for state preprocessing\n\ndef resize_image(im):\n    return sp.misc.imresize(im, (110, 84))\n\n\ndef crop_image(im):\n    return im[-84:, :]\n\n\ndef process_image_atari(im):\n    '''\n    Image preprocessing from the paper\n    Playing Atari with Deep Reinforcement Learning, 2013\n    Takes an RGB image and converts it to grayscale,\n    downsizes to 110 x 84\n    and crops to square 84 x 84, taking bottomost rows of image\n    '''\n    im_gray = np.dot(im[..., :3], [0.299, 0.587, 0.114])\n    im_resized = resize_image(im_gray)\n    im_cropped = crop_image(im_resized)\n    return im_cropped\n\n\nclass Atari(PreProcessor):\n\n    '''\n    Convert images to greyscale, downsize, crop, then stack 4 states\n    NOTE: Image order is cols * rows * channels to match openai gym format\n    Input to model is rows * cols * channels (== states)\n    '''\n\n    def __init__(self, **kwargs):  # absorb generic param without breaking):\n        super(Atari, self).__init__()\n\n    def preprocess_state(self):\n        processed_state_queue = (\n            process_image_atari(self.state),\n            process_image_atari(self.previous_state),\n            process_image_atari(self.pre_previous_state),\n            process_image_atari(self.pre_pre_previous_state))\n        processed_state = np.stack(processed_state_queue, axis=-1)\n        return processed_state\n\n    def preprocess_memory(self, action, reward, next_state, done):\n        self.add_raw_exp(action, reward, next_state, done)\n        if (self.exp_queue_size() < self.MAX_QUEUE_SIZE):  # insufficient queue\n            return\n        (_state, action, reward, next_state, done) = self.exp_queue[-1]\n        processed_next_state_queue = (\n            process_image_atari(self.exp_queue[-1][3]),\n            process_image_atari(self.exp_queue[-2][3]),\n            process_image_atari(self.exp_queue[-3][3]),\n            process_image_atari(self.exp_queue[-4][3]))\n        processed_state = self.preprocess_state()\n        processed_next_state = np.stack(processed_next_state_queue, axis=-1)\n        self.debug_state(processed_state, processed_next_state)\n        processed_exp = (action, reward, processed_next_state, done)\n        return processed_exp\n"""
rl/preprocessor/base_preprocessor.py,0,"b'import numpy as np\nfrom rl.util import logger, log_self\n\n\ndef create_dummy_states(state):\n    state_shape = state.shape\n    previous_state = np.zeros(state_shape)\n    pre_previous_state = np.zeros(state_shape)\n    pre_pre_previous_state = np.zeros(state_shape)\n    if (previous_state.ndim == 1):\n        previous_state = np.zeros([state_shape[0]])\n        pre_previous_state = np.zeros([state_shape[0]])\n        pre_pre_previous_state = np.zeros([state_shape[0]])\n    return (previous_state, pre_previous_state, pre_pre_previous_state)\n\n\nclass PreProcessor(object):\n\n    \'\'\'\n    The Base class for state preprocessing\n    \'\'\'\n\n    def __init__(self, max_queue_size=4, **kwargs):\n        \'\'\'Construct externally, and set at Agent.compile()\'\'\'\n        self.agent = None\n        self.state = None\n        self.exp_queue = []\n        self.MAX_QUEUE_SIZE = max_queue_size\n        self.never_debugged = True\n        log_self(self)\n\n    def reset_state(self, init_state):\n        \'\'\'reset the state of LinearMemory per episode env.reset()\'\'\'\n        self.state = np.array(init_state)  # cast into np for safety\n        (previous_state, pre_previous_state,\n            pre_pre_previous_state) = create_dummy_states(self.state)\n        self.previous_state = previous_state\n        self.pre_previous_state = pre_previous_state\n        self.pre_pre_previous_state = pre_pre_previous_state\n        return self.preprocess_state()\n\n    def exp_queue_size(self):\n        return len(self.exp_queue)\n\n    def debug_state(self, processed_state, processed_next_state):\n        if self.never_debugged:\n            logger.debug(""State shape: {}"".format(processed_state.shape))\n            logger.debug(\n                ""Next state shape: {}"".format(processed_next_state.shape))\n            self.never_debugged = False\n\n    def preprocess_env_spec(self, env_spec):\n        \'\'\'helper to tweak env_spec according to preprocessor\'\'\'\n        class_name = self.__class__.__name__\n        if class_name is \'StackStates\':\n            env_spec[\'state_dim\'] = env_spec[\'state_dim\'] * 2\n        elif class_name is \'Atari\':\n            env_spec[\'state_dim\'] = (84, 84, 4)\n        return env_spec\n\n    def preprocess_state(self):\n        raise NotImplementedError()\n\n    def advance_state(self, next_state):\n        self.pre_pre_previous_state = self.pre_previous_state\n        self.pre_previous_state = self.previous_state\n        self.previous_state = self.state\n        self.state = next_state\n\n    def add_raw_exp(self, action, reward, next_state, done):\n        \'\'\'\n        Buffer currently set to hold only last 4 experiences\n        Amount needed for Atari games preprocessing\n        \'\'\'\n        self.exp_queue.append([self.state, action, reward, next_state, done])\n        if (self.exp_queue_size() > self.MAX_QUEUE_SIZE):\n            del self.exp_queue[0]\n        self.advance_state(next_state)\n\n    def preprocess_memory(self, action, reward, next_state, done):\n        raise NotImplementedError()\n'"
rl/preprocessor/linear.py,0,"b""import numpy as np\nfrom rl.preprocessor.base_preprocessor import PreProcessor\n\n\nclass NoPreProcessor(PreProcessor):\n\n    '''\n    Default class, no preprocessing\n    '''\n\n    def __init__(self, **kwargs):  # absorb generic param without breaking):\n        super(NoPreProcessor, self).__init__()\n\n    def preprocess_state(self):\n        return self.state\n\n    def preprocess_memory(self, action, reward, next_state, done):\n        '''No state processing'''\n        self.add_raw_exp(action, reward, next_state, done)\n        (_state, action, reward, next_state, done) = self.exp_queue[-1]\n        processed_exp = (action, reward, next_state, done)\n        return processed_exp\n\n\nclass StackStates(PreProcessor):\n\n    '''\n    Current and last state are concatenated to form input to model\n    '''\n\n    def __init__(self, **kwargs):  # absorb generic param without breaking):\n        super(StackStates, self).__init__(max_queue_size=2)\n\n    def preprocess_state(self):\n        processed_state = np.concatenate([self.previous_state, self.state])\n        return processed_state\n\n    def preprocess_memory(self, action, reward, next_state, done):\n        '''Concatenate: previous + current states'''\n        self.add_raw_exp(action, reward, next_state, done)\n        if (self.exp_queue_size() < self.MAX_QUEUE_SIZE):  # insufficient queue\n            return\n        (state, action, reward, next_state, done) = self.exp_queue[-1]\n        processed_state = self.preprocess_state()\n        processed_next_state = np.concatenate([state, next_state])\n        self.debug_state(processed_state, processed_next_state)\n        processed_exp = (action, reward, processed_next_state, done)\n        return processed_exp\n\n\nclass DiffStates(PreProcessor):\n\n    '''\n    Different between current and last state is input to model\n    '''\n\n    def __init__(self, **kwargs):  # absorb generic param without breaking):\n        super(DiffStates, self).__init__(max_queue_size=2)\n\n    def preprocess_state(self):\n        processed_state = self.state - self.previous_state\n        return processed_state\n\n    def preprocess_memory(self, action, reward, next_state, done):\n        '''Change in state, curr_state - last_state'''\n        self.add_raw_exp(action, reward, next_state, done)\n        if (self.exp_queue_size() < self.MAX_QUEUE_SIZE):  # insufficient queue\n            return\n        (state, action, reward, next_state, done) = self.exp_queue[-1]\n        processed_state = self.preprocess_state()\n        processed_next_state = next_state - state\n        self.debug_state(processed_state, processed_next_state)\n        processed_exp = (action, reward, processed_next_state, done)\n        return processed_exp\n"""
