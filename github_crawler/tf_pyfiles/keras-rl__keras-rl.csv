file_path,api_count,code
setup.py,0,"b""from setuptools import setup\nfrom setuptools import find_packages\n\n\nsetup(name='keras-rl',\n      version='0.4.2',\n      description='Deep Reinforcement Learning for Keras',\n      author='Matthias Plappert',\n      author_email='matthiasplappert@me.com',\n      url='https://github.com/keras-rl/keras-rl',\n      license='MIT',\n      install_requires=['keras>=2.0.7'],\n      extras_require={\n          'gym': ['gym'],\n      },\n      packages=find_packages())\n"""
docs/autogen.py,0,"b'# -*- coding: utf-8 -*-\n"""""" This code and the entire documentation setup was adopted from the Keras repository:\nhttps://github.com/fchollet/keras/blob/master/docs/autogen.py\n""""""\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport re\nimport inspect\nimport os\nimport shutil\nimport sys\nif sys.version[0] == \'2\':\n    reload(sys)\n    sys.setdefaultencoding(\'utf8\')\n\nimport rl\nimport rl.core\nimport rl.processors\nimport rl.agents\n\n\nEXCLUDE = {\n    \n}\n\nPAGES = [\n    {\n        \'page\': \'core.md\',\n        \'all_module_classes\': [rl.core],\n    },\n    {\n        \'page\': \'processors.md\',\n        \'all_module_classes\': [rl.processors],\n    },\n    {\n        \'page\': \'agents/overview.md\',\n        \'functions\': [\n            rl.core.Agent.fit,\n            rl.core.Agent.test,\n            rl.core.Agent.compile,\n            rl.core.Agent.get_config,\n            rl.core.Agent.reset_states,\n            rl.core.Agent.load_weights,\n            rl.core.Agent.save_weights,\n        ],\n    },\n    {\n        \'page\': \'agents/dqn.md\',\n        \'classes\': [rl.agents.DQNAgent],\n    },\n    {\n        \'page\': \'agents/naf.md\',\n        \'classes\': [rl.agents.NAFAgent],\n    },\n    {\n        \'page\': \'agents/ddpg.md\',\n        \'classes\': [rl.agents.DDPGAgent],\n    },\n    {\n        \'page\': \'agents/sarsa.md\',\n        \'classes\': [rl.agents.SARSAAgent],\n    },\n    {\n        \'page\': \'agents/cem.md\',\n        \'classes\': [rl.agents.CEMAgent],\n    },\n]\n\n\nROOT_MODULE_NAME = \'rl.\'\n\n\ndef get_earliest_class_that_defined_member(member, cls):\n    ancestors = get_classes_ancestors([cls])\n    result = None\n    for ancestor in ancestors:\n        if member in dir(ancestor):\n            result = ancestor\n    if not result:\n        return cls\n    return result\n\n\ndef get_classes_ancestors(classes):\n    ancestors = []\n    for cls in classes:\n        ancestors += cls.__bases__\n    filtered_ancestors = []\n    for ancestor in ancestors:\n        if ancestor.__name__ in [\'object\']:\n            continue\n        filtered_ancestors.append(ancestor)\n    if filtered_ancestors:\n        return filtered_ancestors + get_classes_ancestors(filtered_ancestors)\n    else:\n        return filtered_ancestors\n\n\ndef get_function_signature(function, method=True):\n    signature = getattr(function, \'_legacy_support_signature\', None)\n    if signature is None:\n        signature = inspect.getargspec(function)\n    defaults = signature.defaults\n    if method:\n        args = signature.args[1:]\n    else:\n        args = signature.args\n    if defaults:\n        kwargs = zip(args[-len(defaults):], defaults)\n        args = args[:-len(defaults)]\n    else:\n        kwargs = []\n    st = \'%s.%s(\' % (function.__module__, function.__name__)\n    for a in args:\n        st += str(a) + \', \'\n    for a, v in kwargs:\n        if isinstance(v, str):\n            v = \'\\\'\' + v + \'\\\'\'\n        st += str(a) + \'=\' + str(v) + \', \'\n    if kwargs or args:\n        return st[:-2] + \')\'\n    else:\n        return st + \')\'\n\n\ndef get_class_signature(cls):\n    try:\n        class_signature = get_function_signature(cls.__init__)\n        class_signature = class_signature.replace(\'__init__\', cls.__name__)\n    except:\n        # in case the class inherits from object and does not\n        # define __init__\n        class_signature = cls.__module__ + \'.\' + cls.__name__ + \'()\'\n    return class_signature\n\n\ndef class_to_source_link(cls):\n    module_name = cls.__module__\n    assert module_name.startswith(ROOT_MODULE_NAME)\n    path = module_name.replace(\'.\', \'/\')\n    path += \'.py\'\n    line = inspect.getsourcelines(cls)[-1]\n    link = \'https://github.com/keras-rl/keras-rl/blob/master/\' + path + \'#L\' + str(line)\n    return \'[[source]](\' + link + \')\'\n\n\ndef function_to_source_link(fn):\n    module_name = fn.__module__\n    assert module_name.startswith(ROOT_MODULE_NAME)\n    path = module_name.replace(\'.\', \'/\')\n    path += \'.py\'\n    line = inspect.getsourcelines(fn)[-1]\n    link = \'https://github.com/keras-rl/keras-rl/blob/master/\' + path + \'#L\' + str(line)\n    return \'[[source]](\' + link + \')\'\n\n\ndef code_snippet(snippet):\n    result = \'```python\\n\'\n    result += snippet + \'\\n\'\n    result += \'```\\n\'\n    return result\n\n\ndef process_class_docstring(docstring):\n    docstring = re.sub(r\'\\n    # (.*)\\n\',\n                       r\'\\n    __\\1__\\n\\n\',\n                       docstring)\n\n    docstring = re.sub(r\'    ([^\\s\\\\]+) \\((.*)\\n\',\n                       r\'    - __\\1__ (\\2\\n\',\n                       docstring)\n\n    docstring = docstring.replace(\'    \' * 5, \'\\t\\t\')\n    docstring = docstring.replace(\'    \' * 3, \'\\t\')\n    docstring = docstring.replace(\'    \', \'\')\n    return docstring\n\n\ndef process_function_docstring(docstring):\n    docstring = re.sub(r\'\\n    # (.*)\\n\',\n                       r\'\\n    __\\1__\\n\\n\',\n                       docstring)\n    docstring = re.sub(r\'\\n        # (.*)\\n\',\n                       r\'\\n        __\\1__\\n\\n\',\n                       docstring)\n\n    docstring = re.sub(r\'    ([^\\s\\\\]+) \\((.*)\\n\',\n                       r\'    - __\\1__ (\\2\\n\',\n                       docstring)\n\n    docstring = docstring.replace(\'    \' * 6, \'\\t\\t\')\n    docstring = docstring.replace(\'    \' * 4, \'\\t\')\n    docstring = docstring.replace(\'    \', \'\')\n    return docstring\n\nprint(\'Cleaning up existing sources directory.\')\nif os.path.exists(\'sources\'):\n    shutil.rmtree(\'sources\')\n\nprint(\'Populating sources directory with templates.\')\nfor subdir, dirs, fnames in os.walk(\'templates\'):\n    for fname in fnames:\n        new_subdir = subdir.replace(\'templates\', \'sources\')\n        if not os.path.exists(new_subdir):\n            os.makedirs(new_subdir)\n        if fname[-3:] == \'.md\':\n            fpath = os.path.join(subdir, fname)\n            new_fpath = fpath.replace(\'templates\', \'sources\')\n            shutil.copy(fpath, new_fpath)\n\n# Take care of index page.\nreadme = open(\'../README.md\').read()\nindex = open(\'templates/index.md\').read()\nindex = index.replace(\'{{autogenerated}}\', readme[readme.find(\'##\'):])\nf = open(\'sources/index.md\', \'w\')\nf.write(index)\nf.close()\n\nprint(\'Starting autogeneration.\')\nfor page_data in PAGES:\n    blocks = []\n    classes = page_data.get(\'classes\', [])\n    for module in page_data.get(\'all_module_classes\', []):\n        module_classes = []\n        for name in dir(module):\n            if name[0] == \'_\' or name in EXCLUDE:\n                continue\n            module_member = getattr(module, name)\n            if inspect.isclass(module_member):\n                cls = module_member\n                if cls.__module__ == module.__name__:\n                    if cls not in module_classes:\n                        module_classes.append(cls)\n        module_classes.sort(key=lambda x: id(x))\n        classes += module_classes\n\n    for cls in classes:\n        subblocks = []\n        signature = get_class_signature(cls)\n        subblocks.append(\'<span style=""float:right;"">\' + class_to_source_link(cls) + \'</span>\')\n        subblocks.append(\'### \' + cls.__name__ + \'\\n\')\n        subblocks.append(code_snippet(signature))\n        docstring = cls.__doc__\n        if docstring:\n            subblocks.append(process_class_docstring(docstring))\n        blocks.append(\'\\n\'.join(subblocks))\n\n    functions = page_data.get(\'functions\', [])\n    for module in page_data.get(\'all_module_functions\', []):\n        module_functions = []\n        for name in dir(module):\n            if name[0] == \'_\' or name in EXCLUDE:\n                continue\n            module_member = getattr(module, name)\n            if inspect.isfunction(module_member):\n                function = module_member\n                if module.__name__ in function.__module__:\n                    if function not in module_functions:\n                        module_functions.append(function)\n        module_functions.sort(key=lambda x: id(x))\n        functions += module_functions\n\n    for function in functions:\n        subblocks = []\n        signature = get_function_signature(function, method=False)\n        signature = signature.replace(function.__module__ + \'.\', \'\')\n        subblocks.append(\'<span style=""float:right;"">\' + function_to_source_link(function) + \'</span>\')\n        subblocks.append(\'### \' + function.__name__ + \'\\n\')\n        subblocks.append(code_snippet(signature))\n        docstring = function.__doc__\n        if docstring:\n            subblocks.append(process_function_docstring(docstring))\n        blocks.append(\'\\n\\n\'.join(subblocks))\n\n    if not blocks:\n        raise RuntimeError(\'Found no content for page \' +\n                           page_data[\'page\'])\n\n    mkdown = \'\\n----\\n\\n\'.join(blocks)\n    # save module page.\n    # Either insert content into existing page,\n    # or create page otherwise\n    page_name = page_data[\'page\']\n    path = os.path.join(\'sources\', page_name)\n    if os.path.exists(path):\n        template = open(path).read()\n        assert \'{{autogenerated}}\' in template, (\'Template found for \' + path +\n                                                 \' but missing {{autogenerated}} tag.\')\n        mkdown = template.replace(\'{{autogenerated}}\', mkdown)\n        print(\'...inserting autogenerated content into template:\', path)\n    else:\n        print(\'...creating new page with autogenerated content:\', path)\n    subdir = os.path.dirname(path)\n    if not os.path.exists(subdir):\n        os.makedirs(subdir)\n    open(path, \'w\').write(mkdown)\n'"
examples/cem_cartpole.py,0,"b""import numpy as np\nimport gym\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\n\nfrom rl.agents.cem import CEMAgent\nfrom rl.memory import EpisodeParameterMemory\n\nENV_NAME = 'CartPole-v0'\n\n\n# Get the environment and extract the number of actions.\nenv = gym.make(ENV_NAME)\nnp.random.seed(123)\nenv.seed(123)\n\nnb_actions = env.action_space.n\nobs_dim = env.observation_space.shape[0]\n\n# Option 1 : Simple model\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('softmax'))\n\n# Option 2: deep network\n# model = Sequential()\n# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n# model.add(Dense(16))\n# model.add(Activation('relu'))\n# model.add(Dense(16))\n# model.add(Activation('relu'))\n# model.add(Dense(16))\n# model.add(Activation('relu'))\n# model.add(Dense(nb_actions))\n# model.add(Activation('softmax'))\n\n\nprint(model.summary())\n\n\n# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n# even the metrics!\nmemory = EpisodeParameterMemory(limit=1000, window_length=1)\n\ncem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\ncem.compile()\n\n# Okay, now it's time to learn something! We visualize the training here for show, but this\n# slows down training quite a lot. You can always safely abort the training prematurely using\n# Ctrl + C.\ncem.fit(env, nb_steps=100000, visualize=False, verbose=2)\n\n# After training is done, we save the best weights.\ncem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)\n\n# Finally, evaluate our algorithm for 5 episodes.\ncem.test(env, nb_episodes=5, visualize=True)\n"""
examples/ddpg_mujoco.py,0,"b""import numpy as np\n\nimport gym\nfrom gym import wrappers\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation, Flatten, Input, Concatenate\nfrom keras.optimizers import Adam\n\nfrom rl.processors import WhiteningNormalizerProcessor\nfrom rl.agents import DDPGAgent\nfrom rl.memory import SequentialMemory\nfrom rl.random import OrnsteinUhlenbeckProcess\n\n\nclass MujocoProcessor(WhiteningNormalizerProcessor):\n    def process_action(self, action):\n        return np.clip(action, -1., 1.)\n\n\nENV_NAME = 'HalfCheetah-v2'\n\n\n# Get the environment and extract the number of actions.\nenv = gym.make(ENV_NAME)\nenv = wrappers.Monitor(env, '/tmp/{}'.format(ENV_NAME), force=True)\nnp.random.seed(123)\nenv.seed(123)\nassert len(env.action_space.shape) == 1\nnb_actions = env.action_space.shape[0]\n\n# Next, we build a very simple model.\nactor = Sequential()\nactor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nactor.add(Dense(400))\nactor.add(Activation('relu'))\nactor.add(Dense(300))\nactor.add(Activation('relu'))\nactor.add(Dense(nb_actions))\nactor.add(Activation('tanh'))\nprint(actor.summary())\n\naction_input = Input(shape=(nb_actions,), name='action_input')\nobservation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\nflattened_observation = Flatten()(observation_input)\nx = Dense(400)(flattened_observation)\nx = Activation('relu')(x)\nx = Concatenate()([x, action_input])\nx = Dense(300)(x)\nx = Activation('relu')(x)\nx = Dense(1)(x)\nx = Activation('linear')(x)\ncritic = Model(inputs=[action_input, observation_input], outputs=x)\nprint(critic.summary())\n\n# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n# even the metrics!\nmemory = SequentialMemory(limit=100000, window_length=1)\nrandom_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=.15, mu=0., sigma=.1)\nagent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n                  memory=memory, nb_steps_warmup_critic=1000, nb_steps_warmup_actor=1000,\n                  random_process=random_process, gamma=.99, target_model_update=1e-3,\n                  processor=MujocoProcessor())\nagent.compile([Adam(lr=1e-4), Adam(lr=1e-3)], metrics=['mae'])\n\n# Okay, now it's time to learn something! We visualize the training here for show, but this\n# slows down training quite a lot. You can always safely abort the training prematurely using\n# Ctrl + C.\nagent.fit(env, nb_steps=1000000, visualize=False, verbose=1)\n\n# After training is done, we save the final weights.\nagent.save_weights('ddpg_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n\n# Finally, evaluate our algorithm for 5 episodes.\nagent.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=200)\n"""
examples/ddpg_pendulum.py,0,"b""import numpy as np\nimport gym\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation, Flatten, Input, Concatenate\nfrom keras.optimizers import Adam\n\nfrom rl.agents import DDPGAgent\nfrom rl.memory import SequentialMemory\nfrom rl.random import OrnsteinUhlenbeckProcess\n\n\nENV_NAME = 'Pendulum-v0'\n\n\n# Get the environment and extract the number of actions.\nenv = gym.make(ENV_NAME)\nnp.random.seed(123)\nenv.seed(123)\nassert len(env.action_space.shape) == 1\nnb_actions = env.action_space.shape[0]\n\n# Next, we build a very simple model.\nactor = Sequential()\nactor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nactor.add(Dense(16))\nactor.add(Activation('relu'))\nactor.add(Dense(16))\nactor.add(Activation('relu'))\nactor.add(Dense(16))\nactor.add(Activation('relu'))\nactor.add(Dense(nb_actions))\nactor.add(Activation('linear'))\nprint(actor.summary())\n\naction_input = Input(shape=(nb_actions,), name='action_input')\nobservation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\nflattened_observation = Flatten()(observation_input)\nx = Concatenate()([action_input, flattened_observation])\nx = Dense(32)(x)\nx = Activation('relu')(x)\nx = Dense(32)(x)\nx = Activation('relu')(x)\nx = Dense(32)(x)\nx = Activation('relu')(x)\nx = Dense(1)(x)\nx = Activation('linear')(x)\ncritic = Model(inputs=[action_input, observation_input], outputs=x)\nprint(critic.summary())\n\n# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n# even the metrics!\nmemory = SequentialMemory(limit=100000, window_length=1)\nrandom_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=.15, mu=0., sigma=.3)\nagent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n                  memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n                  random_process=random_process, gamma=.99, target_model_update=1e-3)\nagent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])\n\n# Okay, now it's time to learn something! We visualize the training here for show, but this\n# slows down training quite a lot. You can always safely abort the training prematurely using\n# Ctrl + C.\nagent.fit(env, nb_steps=50000, visualize=True, verbose=1, nb_max_episode_steps=200)\n\n# After training is done, we save the final weights.\nagent.save_weights('ddpg_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n\n# Finally, evaluate our algorithm for 5 episodes.\nagent.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=200)\n"""
examples/dqn_atari.py,0,"b""from __future__ import division\nimport argparse\n\nfrom PIL import Image\nimport numpy as np\nimport gym\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\nfrom keras.optimizers import Adam\nimport keras.backend as K\n\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\nfrom rl.memory import SequentialMemory\nfrom rl.core import Processor\nfrom rl.callbacks import FileLogger, ModelIntervalCheckpoint\n\n\nINPUT_SHAPE = (84, 84)\nWINDOW_LENGTH = 4\n\n\nclass AtariProcessor(Processor):\n    def process_observation(self, observation):\n        assert observation.ndim == 3  # (height, width, channel)\n        img = Image.fromarray(observation)\n        img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n        processed_observation = np.array(img)\n        assert processed_observation.shape == INPUT_SHAPE\n        return processed_observation.astype('uint8')  # saves storage in experience memory\n\n    def process_state_batch(self, batch):\n        # We could perform this processing step in `process_observation`. In this case, however,\n        # we would need to store a `float32` array instead, which is 4x more memory intensive than\n        # an `uint8` array. This matters if we store 1M observations.\n        processed_batch = batch.astype('float32') / 255.\n        return processed_batch\n\n    def process_reward(self, reward):\n        return np.clip(reward, -1., 1.)\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--mode', choices=['train', 'test'], default='train')\nparser.add_argument('--env-name', type=str, default='BreakoutDeterministic-v4')\nparser.add_argument('--weights', type=str, default=None)\nargs = parser.parse_args()\n\n# Get the environment and extract the number of actions.\nenv = gym.make(args.env_name)\nnp.random.seed(123)\nenv.seed(123)\nnb_actions = env.action_space.n\n\n# Next, we build our model. We use the same model that was described by Mnih et al. (2015).\ninput_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\nmodel = Sequential()\nif K.image_dim_ordering() == 'tf':\n    # (width, height, channels)\n    model.add(Permute((2, 3, 1), input_shape=input_shape))\nelif K.image_dim_ordering() == 'th':\n    # (channels, width, height)\n    model.add(Permute((1, 2, 3), input_shape=input_shape))\nelse:\n    raise RuntimeError('Unknown image_dim_ordering.')\nmodel.add(Convolution2D(32, (8, 8), strides=(4, 4)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(64, (4, 4), strides=(2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(64, (3, 3), strides=(1, 1)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))\nprint(model.summary())\n\n# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n# even the metrics!\nmemory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\nprocessor = AtariProcessor()\n\n# Select a policy. We use eps-greedy action selection, which means that a random action is selected\n# with probability eps. We anneal eps from 1.0 to 0.1 over the course of 1M steps. This is done so that\n# the agent initially explores the environment (high eps) and then gradually sticks to what it knows\n# (low eps). We also set a dedicated eps value that is used during testing. Note that we set it to 0.05\n# so that the agent still performs some random actions. This ensures that the agent cannot get stuck.\npolicy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n                              nb_steps=1000000)\n\n# The trade-off between exploration and exploitation is difficult and an on-going research topic.\n# If you want, you can experiment with the parameters or use a different policy. Another popular one\n# is Boltzmann-style exploration:\n# policy = BoltzmannQPolicy(tau=1.)\n# Feel free to give it a try!\n\ndqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n               train_interval=4, delta_clip=1.)\ndqn.compile(Adam(lr=.00025), metrics=['mae'])\n\nif args.mode == 'train':\n    # Okay, now it's time to learn something! We capture the interrupt exception so that training\n    # can be prematurely aborted. Notice that now you can use the built-in Keras callbacks!\n    weights_filename = 'dqn_{}_weights.h5f'.format(args.env_name)\n    checkpoint_weights_filename = 'dqn_' + args.env_name + '_weights_{step}.h5f'\n    log_filename = 'dqn_{}_log.json'.format(args.env_name)\n    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n    callbacks += [FileLogger(log_filename, interval=100)]\n    dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000)\n\n    # After training is done, we save the final weights one more time.\n    dqn.save_weights(weights_filename, overwrite=True)\n\n    # Finally, evaluate our algorithm for 10 episodes.\n    dqn.test(env, nb_episodes=10, visualize=False)\nelif args.mode == 'test':\n    weights_filename = 'dqn_{}_weights.h5f'.format(args.env_name)\n    if args.weights:\n        weights_filename = args.weights\n    dqn.load_weights(weights_filename)\n    dqn.test(env, nb_episodes=10, visualize=True)\n"""
examples/dqn_cartpole.py,0,"b""import numpy as np\nimport gym\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\n\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import BoltzmannQPolicy\nfrom rl.memory import SequentialMemory\n\n\nENV_NAME = 'CartPole-v0'\n\n\n# Get the environment and extract the number of actions.\nenv = gym.make(ENV_NAME)\nnp.random.seed(123)\nenv.seed(123)\nnb_actions = env.action_space.n\n\n# Next, we build a very simple model.\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))\nprint(model.summary())\n\n# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n# even the metrics!\nmemory = SequentialMemory(limit=50000, window_length=1)\npolicy = BoltzmannQPolicy()\ndqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n               target_model_update=1e-2, policy=policy)\ndqn.compile(Adam(lr=1e-3), metrics=['mae'])\n\n# Okay, now it's time to learn something! We visualize the training here for show, but this\n# slows down training quite a lot. You can always safely abort the training prematurely using\n# Ctrl + C.\ndqn.fit(env, nb_steps=50000, visualize=True, verbose=2)\n\n# After training is done, we save the final weights.\ndqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n\n# Finally, evaluate our algorithm for 5 episodes.\ndqn.test(env, nb_episodes=5, visualize=True)\n"""
examples/duel_dqn_cartpole.py,0,"b""import numpy as np\nimport gym\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\n\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import BoltzmannQPolicy\nfrom rl.memory import SequentialMemory\n\n\nENV_NAME = 'CartPole-v0'\n\n\n# Get the environment and extract the number of actions.\nenv = gym.make(ENV_NAME)\nnp.random.seed(123)\nenv.seed(123)\nnb_actions = env.action_space.n\n\n# Next, we build a very simple model regardless of the dueling architecture\n# if you enable dueling network in DQN , DQN will build a dueling network base on your model automatically\n# Also, you can build a dueling network by yourself and turn off the dueling network in DQN.\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions, activation='linear'))\nprint(model.summary())\n\n# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n# even the metrics!\nmemory = SequentialMemory(limit=50000, window_length=1)\npolicy = BoltzmannQPolicy()\n# enable the dueling network\n# you can specify the dueling_type to one of {'avg','max','naive'}\ndqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n               enable_dueling_network=True, dueling_type='avg', target_model_update=1e-2, policy=policy)\ndqn.compile(Adam(lr=1e-3), metrics=['mae'])\n\n# Okay, now it's time to learn something! We visualize the training here for show, but this\n# slows down training quite a lot. You can always safely abort the training prematurely using\n# Ctrl + C.\ndqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n\n# After training is done, we save the final weights.\ndqn.save_weights('duel_dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n\n# Finally, evaluate our algorithm for 5 episodes.\ndqn.test(env, nb_episodes=5, visualize=False)\n"""
examples/naf_pendulum.py,0,"b""import numpy as np\nimport gym\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation, Flatten, Input, Concatenate\nfrom keras.optimizers import Adam\n\nfrom rl.agents import NAFAgent\nfrom rl.memory import SequentialMemory\nfrom rl.random import OrnsteinUhlenbeckProcess\nfrom rl.core import Processor\n\nclass PendulumProcessor(Processor):\n    def process_reward(self, reward):\n        # The magnitude of the reward can be important. Since each step yields a relatively\n        # high reward, we reduce the magnitude by two orders.\n        return reward / 100.\n\n\nENV_NAME = 'Pendulum-v0'\n\n\n# Get the environment and extract the number of actions.\nenv = gym.make(ENV_NAME)\nnp.random.seed(123)\nenv.seed(123)\nassert len(env.action_space.shape) == 1\nnb_actions = env.action_space.shape[0]\n\n# Build all necessary models: V, mu, and L networks.\nV_model = Sequential()\nV_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nV_model.add(Dense(16))\nV_model.add(Activation('relu'))\nV_model.add(Dense(16))\nV_model.add(Activation('relu'))\nV_model.add(Dense(16))\nV_model.add(Activation('relu'))\nV_model.add(Dense(1))\nV_model.add(Activation('linear'))\nprint(V_model.summary())\n\nmu_model = Sequential()\nmu_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nmu_model.add(Dense(16))\nmu_model.add(Activation('relu'))\nmu_model.add(Dense(16))\nmu_model.add(Activation('relu'))\nmu_model.add(Dense(16))\nmu_model.add(Activation('relu'))\nmu_model.add(Dense(nb_actions))\nmu_model.add(Activation('linear'))\nprint(mu_model.summary())\n\naction_input = Input(shape=(nb_actions,), name='action_input')\nobservation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\nx = Concatenate()([action_input, Flatten()(observation_input)])\nx = Dense(32)(x)\nx = Activation('relu')(x)\nx = Dense(32)(x)\nx = Activation('relu')(x)\nx = Dense(32)(x)\nx = Activation('relu')(x)\nx = Dense(((nb_actions * nb_actions + nb_actions) // 2))(x)\nx = Activation('linear')(x)\nL_model = Model(inputs=[action_input, observation_input], outputs=x)\nprint(L_model.summary())\n\n# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n# even the metrics!\nprocessor = PendulumProcessor()\nmemory = SequentialMemory(limit=100000, window_length=1)\nrandom_process = OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.3, size=nb_actions)\nagent = NAFAgent(nb_actions=nb_actions, V_model=V_model, L_model=L_model, mu_model=mu_model,\n                 memory=memory, nb_steps_warmup=100, random_process=random_process,\n                 gamma=.99, target_model_update=1e-3, processor=processor)\nagent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])\n\n# Okay, now it's time to learn something! We visualize the training here for show, but this\n# slows down training quite a lot. You can always safely abort the training prematurely using\n# Ctrl + C.\nagent.fit(env, nb_steps=50000, visualize=True, verbose=1, nb_max_episode_steps=200)\n\n# After training is done, we save the final weights.\nagent.save_weights('cdqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n\n# Finally, evaluate our algorithm for 5 episodes.\nagent.test(env, nb_episodes=10, visualize=True, nb_max_episode_steps=200)\n"""
examples/sarsa_cartpole.py,0,"b""import numpy as np\nimport gym\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\n\nfrom rl.agents import SARSAAgent\nfrom rl.policy import BoltzmannQPolicy\n\n\nENV_NAME = 'CartPole-v0'\n\n# Get the environment and extract the number of actions.\nenv = gym.make(ENV_NAME)\nnp.random.seed(123)\nenv.seed(123)\nnb_actions = env.action_space.n\n\n# Next, we build a very simple model.\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))\nprint(model.summary())\n\n# SARSA does not require a memory.\npolicy = BoltzmannQPolicy()\nsarsa = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=10, policy=policy)\nsarsa.compile(Adam(lr=1e-3), metrics=['mae'])\n\n# Okay, now it's time to learn something! We visualize the training here for show, but this\n# slows down training quite a lot. You can always safely abort the training prematurely using\n# Ctrl + C.\nsarsa.fit(env, nb_steps=50000, visualize=False, verbose=2)\n\n# After training is done, we save the final weights.\nsarsa.save_weights('sarsa_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n\n# Finally, evaluate our algorithm for 5 episodes.\nsarsa.test(env, nb_episodes=5, visualize=True)\n"""
examples/visualize_log.py,0,"b'import argparse\nimport json\n\nimport matplotlib.pyplot as plt\n\n\ndef visualize_log(filename, figsize=None, output=None):\n    with open(filename, \'r\') as f:\n        data = json.load(f)\n    if \'episode\' not in data:\n        raise ValueError(\'Log file ""{}"" does not contain the ""episode"" key.\'.format(filename))\n    episodes = data[\'episode\']\n\n    # Get value keys. The x axis is shared and is the number of episodes.\n    keys = sorted(list(set(data.keys()).difference(set([\'episode\']))))\n\n    if figsize is None:\n        figsize = (15., 5. * len(keys))\n    f, axarr = plt.subplots(len(keys), sharex=True, figsize=figsize)\n    for idx, key in enumerate(keys):\n        axarr[idx].plot(episodes, data[key])\n        axarr[idx].set_ylabel(key)\n    plt.xlabel(\'episodes\')\n    plt.tight_layout()\n    if output is None:\n        plt.show()\n    else:\n        plt.savefig(output)\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'filename\', type=str, help=\'The filename of the JSON log generated during training.\')\nparser.add_argument(\'--output\', type=str, default=None, help=\'The output file. If not specified, the log will only be displayed.\')\nparser.add_argument(\'--figsize\', nargs=2, type=float, default=None, help=\'The size of the figure in `width height` format specified in points.\')\nargs = parser.parse_args()\n\n# You can use visualize_log to easily view the stats that were recorded during training. Simply\n# provide the filename of the `FileLogger` that was used in `FileLogger`.\nvisualize_log(args.filename, output=args.output, figsize=args.figsize)\n'"
rl/__init__.py,0,b''
rl/callbacks.py,0,"b'from __future__ import division\nfrom __future__ import print_function\nimport warnings\nimport timeit\nimport json\nfrom tempfile import mkdtemp\n\nimport numpy as np\nimport wandb\n\nfrom keras import __version__ as KERAS_VERSION\nfrom keras.callbacks import Callback as KerasCallback, CallbackList as KerasCallbackList\nfrom keras.utils.generic_utils import Progbar\n\n\nclass Callback(KerasCallback):\n    def _set_env(self, env):\n        self.env = env\n\n    def on_episode_begin(self, episode, logs={}):\n        """"""Called at beginning of each episode""""""\n        pass\n\n    def on_episode_end(self, episode, logs={}):\n        """"""Called at end of each episode""""""\n        pass\n\n    def on_step_begin(self, step, logs={}):\n        """"""Called at beginning of each step""""""\n        pass\n\n    def on_step_end(self, step, logs={}):\n        """"""Called at end of each step""""""\n        pass\n\n    def on_action_begin(self, action, logs={}):\n        """"""Called at beginning of each action""""""\n        pass\n\n    def on_action_end(self, action, logs={}):\n        """"""Called at end of each action""""""\n        pass\n\n\nclass CallbackList(KerasCallbackList):\n    def _set_env(self, env):\n        """""" Set environment for each callback in callbackList """"""\n        for callback in self.callbacks:\n            if callable(getattr(callback, \'_set_env\', None)):\n                callback._set_env(env)\n\n    def on_episode_begin(self, episode, logs={}):\n        """""" Called at beginning of each episode for each callback in callbackList""""""\n        for callback in self.callbacks:\n            # Check if callback supports the more appropriate `on_episode_begin` callback.\n            # If not, fall back to `on_epoch_begin` to be compatible with built-in Keras callbacks.\n            if callable(getattr(callback, \'on_episode_begin\', None)):\n                callback.on_episode_begin(episode, logs=logs)\n            else:\n                callback.on_epoch_begin(episode, logs=logs)\n\n    def on_episode_end(self, episode, logs={}):\n        """""" Called at end of each episode for each callback in callbackList""""""\n        for callback in self.callbacks:\n            # Check if callback supports the more appropriate `on_episode_end` callback.\n            # If not, fall back to `on_epoch_end` to be compatible with built-in Keras callbacks.\n            if callable(getattr(callback, \'on_episode_end\', None)):\n                callback.on_episode_end(episode, logs=logs)\n            else:\n                callback.on_epoch_end(episode, logs=logs)\n\n    def on_step_begin(self, step, logs={}):\n        """""" Called at beginning of each step for each callback in callbackList""""""\n        for callback in self.callbacks:\n            # Check if callback supports the more appropriate `on_step_begin` callback.\n            # If not, fall back to `on_batch_begin` to be compatible with built-in Keras callbacks.\n            if callable(getattr(callback, \'on_step_begin\', None)):\n                callback.on_step_begin(step, logs=logs)\n            else:\n                callback.on_batch_begin(step, logs=logs)\n\n    def on_step_end(self, step, logs={}):\n        """""" Called at end of each step for each callback in callbackList""""""\n        for callback in self.callbacks:\n            # Check if callback supports the more appropriate `on_step_end` callback.\n            # If not, fall back to `on_batch_end` to be compatible with built-in Keras callbacks.\n            if callable(getattr(callback, \'on_step_end\', None)):\n                callback.on_step_end(step, logs=logs)\n            else:\n                callback.on_batch_end(step, logs=logs)\n\n    def on_action_begin(self, action, logs={}):\n        """""" Called at beginning of each action for each callback in callbackList""""""\n        for callback in self.callbacks:\n            if callable(getattr(callback, \'on_action_begin\', None)):\n                callback.on_action_begin(action, logs=logs)\n\n    def on_action_end(self, action, logs={}):\n        """""" Called at end of each action for each callback in callbackList""""""\n        for callback in self.callbacks:\n            if callable(getattr(callback, \'on_action_end\', None)):\n                callback.on_action_end(action, logs=logs)\n\n\nclass TestLogger(Callback):\n    """""" Logger Class for Test """"""\n\n    def on_train_begin(self, logs):\n        """""" Print logs at beginning of training""""""\n        print(\'Testing for {} episodes ...\'.format(self.params[\'nb_episodes\']))\n\n    def on_episode_end(self, episode, logs):\n        """""" Print logs at end of each episode """"""\n        template = \'Episode {0}: reward: {1:.3f}, steps: {2}\'\n        variables = [\n            episode + 1,\n            logs[\'episode_reward\'],\n            logs[\'nb_steps\'],\n        ]\n        print(template.format(*variables))\n\n\nclass TrainEpisodeLogger(Callback):\n    def __init__(self):\n        # Some algorithms compute multiple episodes at once since they are multi-threaded.\n        # We therefore use a dictionary that is indexed by the episode to separate episodes\n        # from each other.\n        self.episode_start = {}\n        self.observations = {}\n        self.rewards = {}\n        self.actions = {}\n        self.metrics = {}\n        self.step = 0\n\n    def on_train_begin(self, logs):\n        """""" Print training values at beginning of training """"""\n        self.train_start = timeit.default_timer()\n        self.metrics_names = self.model.metrics_names\n        print(\'Training for {} steps ...\'.format(self.params[\'nb_steps\']))\n\n    def on_train_end(self, logs):\n        """""" Print training time at end of training """"""\n        duration = timeit.default_timer() - self.train_start\n        print(\'done, took {:.3f} seconds\'.format(duration))\n\n    def on_episode_begin(self, episode, logs):\n        """""" Reset environment variables at beginning of each episode """"""\n        self.episode_start[episode] = timeit.default_timer()\n        self.observations[episode] = []\n        self.rewards[episode] = []\n        self.actions[episode] = []\n        self.metrics[episode] = []\n\n    def on_episode_end(self, episode, logs):\n        """""" Compute and print training statistics of the episode when done """"""\n        duration = timeit.default_timer() - self.episode_start[episode]\n        episode_steps = len(self.observations[episode])\n\n        # Format all metrics.\n        metrics = np.array(self.metrics[episode])\n        metrics_template = \'\'\n        metrics_variables = []\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\'error\')\n            for idx, name in enumerate(self.metrics_names):\n                if idx > 0:\n                    metrics_template += \', \'\n                try:\n                    value = np.nanmean(metrics[:, idx])\n                    metrics_template += \'{}: {:f}\'\n                except Warning:\n                    value = \'--\'\n                    metrics_template += \'{}: {}\'\n                metrics_variables += [name, value]\n        metrics_text = metrics_template.format(*metrics_variables)\n\n        nb_step_digits = str(\n            int(np.ceil(np.log10(self.params[\'nb_steps\']))) + 1)\n        template = \'{step: \' + nb_step_digits + \\\n            \'d}/{nb_steps}: episode: {episode}, duration: {duration:.3f}s, episode steps: {episode_steps}, steps per second: {sps:.0f}, episode reward: {episode_reward:.3f}, mean reward: {reward_mean:.3f} [{reward_min:.3f}, {reward_max:.3f}], mean action: {action_mean:.3f} [{action_min:.3f}, {action_max:.3f}], mean observation: {obs_mean:.3f} [{obs_min:.3f}, {obs_max:.3f}], {metrics}\'\n        variables = {\n            \'step\': self.step,\n            \'nb_steps\': self.params[\'nb_steps\'],\n            \'episode\': episode + 1,\n            \'duration\': duration,\n            \'episode_steps\': episode_steps,\n            \'sps\': float(episode_steps) / duration,\n            \'episode_reward\': np.sum(self.rewards[episode]),\n            \'reward_mean\': np.mean(self.rewards[episode]),\n            \'reward_min\': np.min(self.rewards[episode]),\n            \'reward_max\': np.max(self.rewards[episode]),\n            \'action_mean\': np.mean(self.actions[episode]),\n            \'action_min\': np.min(self.actions[episode]),\n            \'action_max\': np.max(self.actions[episode]),\n            \'obs_mean\': np.mean(self.observations[episode]),\n            \'obs_min\': np.min(self.observations[episode]),\n            \'obs_max\': np.max(self.observations[episode]),\n            \'metrics\': metrics_text,\n        }\n        print(template.format(**variables))\n\n        # Free up resources.\n        del self.episode_start[episode]\n        del self.observations[episode]\n        del self.rewards[episode]\n        del self.actions[episode]\n        del self.metrics[episode]\n\n    def on_step_end(self, step, logs):\n        """""" Update statistics of episode after each step """"""\n        episode = logs[\'episode\']\n        self.observations[episode].append(logs[\'observation\'])\n        self.rewards[episode].append(logs[\'reward\'])\n        self.actions[episode].append(logs[\'action\'])\n        self.metrics[episode].append(logs[\'metrics\'])\n        self.step += 1\n\n\nclass TrainIntervalLogger(Callback):\n    def __init__(self, interval=10000):\n        self.interval = interval\n        self.step = 0\n        self.reset()\n\n    def reset(self):\n        """""" Reset statistics """"""\n        self.interval_start = timeit.default_timer()\n        self.progbar = Progbar(target=self.interval)\n        self.metrics = []\n        self.infos = []\n        self.info_names = None\n        self.episode_rewards = []\n\n    def on_train_begin(self, logs):\n        """""" Initialize training statistics at beginning of training """"""\n        self.train_start = timeit.default_timer()\n        self.metrics_names = self.model.metrics_names\n        print(\'Training for {} steps ...\'.format(self.params[\'nb_steps\']))\n\n    def on_train_end(self, logs):\n        """""" Print training duration at end of training """"""\n        duration = timeit.default_timer() - self.train_start\n        print(\'done, took {:.3f} seconds\'.format(duration))\n\n    def on_step_begin(self, step, logs):\n        """""" Print metrics if interval is over """"""\n        if self.step % self.interval == 0:\n            if len(self.episode_rewards) > 0:\n                metrics = np.array(self.metrics)\n                assert metrics.shape == (\n                    self.interval, len(self.metrics_names))\n                formatted_metrics = \'\'\n                if not np.isnan(metrics).all():  # not all values are means\n                    means = np.nanmean(self.metrics, axis=0)\n                    assert means.shape == (len(self.metrics_names),)\n                    for name, mean in zip(self.metrics_names, means):\n                        formatted_metrics += \' - {}: {:.3f}\'.format(name, mean)\n\n                formatted_infos = \'\'\n                if len(self.infos) > 0:\n                    infos = np.array(self.infos)\n                    if not np.isnan(infos).all():  # not all values are means\n                        means = np.nanmean(self.infos, axis=0)\n                        assert means.shape == (len(self.info_names),)\n                        for name, mean in zip(self.info_names, means):\n                            formatted_infos += \' - {}: {:.3f}\'.format(\n                                name, mean)\n                print(\'{} episodes - episode_reward: {:.3f} [{:.3f}, {:.3f}]{}{}\'.format(len(self.episode_rewards), np.mean(\n                    self.episode_rewards), np.min(self.episode_rewards), np.max(self.episode_rewards), formatted_metrics, formatted_infos))\n                print(\'\')\n            self.reset()\n            print(\'Interval {} ({} steps performed)\'.format(\n                self.step // self.interval + 1, self.step))\n\n    def on_step_end(self, step, logs):\n        """""" Update progression bar at the end of each step """"""\n        if self.info_names is None:\n            self.info_names = logs[\'info\'].keys()\n        values = [(\'reward\', logs[\'reward\'])]\n        if KERAS_VERSION > \'2.1.3\':\n            self.progbar.update((self.step % self.interval) + 1, values=values)\n        else:\n            self.progbar.update((self.step % self.interval) +\n                                1, values=values, force=True)\n        self.step += 1\n        self.metrics.append(logs[\'metrics\'])\n        if len(self.info_names) > 0:\n            self.infos.append([logs[\'info\'][k] for k in self.info_names])\n\n    def on_episode_end(self, episode, logs):\n        """""" Update reward value at the end of each episode """"""\n        self.episode_rewards.append(logs[\'episode_reward\'])\n\n\nclass FileLogger(Callback):\n    def __init__(self, filepath, interval=None):\n        self.filepath = filepath\n        self.interval = interval\n\n        # Some algorithms compute multiple episodes at once since they are multi-threaded.\n        # We therefore use a dict that maps from episode to metrics array.\n        self.metrics = {}\n        self.starts = {}\n        self.data = {}\n\n    def on_train_begin(self, logs):\n        """""" Initialize model metrics before training """"""\n        self.metrics_names = self.model.metrics_names\n\n    def on_train_end(self, logs):\n        """""" Save model at the end of training """"""\n        self.save_data()\n\n    def on_episode_begin(self, episode, logs):\n        """""" Initialize metrics at the beginning of each episode """"""\n        assert episode not in self.metrics\n        assert episode not in self.starts\n        self.metrics[episode] = []\n        self.starts[episode] = timeit.default_timer()\n\n    def on_episode_end(self, episode, logs):\n        """""" Compute and print metrics at the end of each episode """"""\n        duration = timeit.default_timer() - self.starts[episode]\n\n        metrics = self.metrics[episode]\n        if np.isnan(metrics).all():\n            mean_metrics = np.array([np.nan for _ in self.metrics_names])\n        else:\n            mean_metrics = np.nanmean(metrics, axis=0)\n        assert len(mean_metrics) == len(self.metrics_names)\n\n        data = list(zip(self.metrics_names, mean_metrics))\n        data += list(logs.items())\n        data += [(\'episode\', episode), (\'duration\', duration)]\n        for key, value in data:\n            if key not in self.data:\n                self.data[key] = []\n            self.data[key].append(value)\n\n        if self.interval is not None and episode % self.interval == 0:\n            self.save_data()\n\n        # Clean up.\n        del self.metrics[episode]\n        del self.starts[episode]\n\n    def on_step_end(self, step, logs):\n        """""" Append metric at the end of each step """"""\n        self.metrics[logs[\'episode\']].append(logs[\'metrics\'])\n\n    def save_data(self):\n        """""" Save metrics in a json file """"""\n        if len(self.data.keys()) == 0:\n            return\n\n        # Sort everything by episode.\n        assert \'episode\' in self.data\n        sorted_indexes = np.argsort(self.data[\'episode\'])\n        sorted_data = {}\n        for key, values in self.data.items():\n            assert len(self.data[key]) == len(sorted_indexes)\n            # We convert to np.array() and then to list to convert from np datatypes to native datatypes.\n            # This is necessary because json.dump cannot handle np.float32, for example.\n            sorted_data[key] = np.array(\n                [self.data[key][idx] for idx in sorted_indexes]).tolist()\n\n        # Overwrite already open file. We can simply seek to the beginning since the file will\n        # grow strictly monotonously.\n        with open(self.filepath, \'w\') as f:\n            json.dump(sorted_data, f)\n\n\nclass Visualizer(Callback):\n    def on_action_end(self, action, logs):\n        """""" Render environment at the end of each action """"""\n        self.env.render(mode=\'human\')\n\n\nclass ModelIntervalCheckpoint(Callback):\n    def __init__(self, filepath, interval, verbose=0):\n        super(ModelIntervalCheckpoint, self).__init__()\n        self.filepath = filepath\n        self.interval = interval\n        self.verbose = verbose\n        self.total_steps = 0\n\n    def on_step_end(self, step, logs={}):\n        """""" Save weights at interval steps during training """"""\n        self.total_steps += 1\n        if self.total_steps % self.interval != 0:\n            # Nothing to do.\n            return\n\n        filepath = self.filepath.format(step=self.total_steps, **logs)\n        if self.verbose > 0:\n            print(\'Step {}: saving model to {}\'.format(\n                self.total_steps, filepath))\n        self.model.save_weights(filepath, overwrite=True)\n\n\nclass WandbLogger(Callback):\n    """""" Similar to TrainEpisodeLogger, but sends data to Weights & Biases to be visualized """"""\n\n    def __init__(self, **kwargs):\n        kwargs = {\n            \'project\': \'keras-rl\',\n            \'anonymous\': \'allow\',\n            **kwargs\n        }\n        wandb.init(**kwargs)\n        self.episode_start = {}\n        self.observations = {}\n        self.rewards = {}\n        self.actions = {}\n        self.metrics = {}\n        self.step = 0\n\n    def on_train_begin(self, logs):\n        self.train_start = timeit.default_timer()\n        self.metrics_names = self.model.metrics_names\n        wandb.config.update({\n            \'params\': self.params,\n            \'env\': self.env.__dict__,\n            \'env.env\': self.env.env.__dict__,\n            \'env.env.spec\': self.env.env.spec.__dict__,\n            \'agent\': self.model.__dict__\n        })\n\n    def on_episode_begin(self, episode, logs):\n        """""" Reset environment variables at beginning of each episode """"""\n        self.episode_start[episode] = timeit.default_timer()\n        self.observations[episode] = []\n        self.rewards[episode] = []\n        self.actions[episode] = []\n        self.metrics[episode] = []\n\n    def on_episode_end(self, episode, logs):\n        """""" Compute and log training statistics of the episode when done """"""\n        duration = timeit.default_timer() - self.episode_start[episode]\n        episode_steps = len(self.observations[episode])\n\n        metrics = np.array(self.metrics[episode])\n        metrics_dict = {}\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\'error\')\n            for idx, name in enumerate(self.metrics_names):\n                try:\n                    metrics_dict[name] = np.nanmean(metrics[:, idx])\n                except Warning:\n                    metrics_dict[name] = float(\'nan\')\n\n        wandb.log({\n            \'step\': self.step,\n            \'episode\': episode + 1,\n            \'duration\': duration,\n            \'episode_steps\': episode_steps,\n            \'sps\': float(episode_steps) / duration,\n            \'episode_reward\': np.sum(self.rewards[episode]),\n            \'reward_mean\': np.mean(self.rewards[episode]),\n            \'reward_min\': np.min(self.rewards[episode]),\n            \'reward_max\': np.max(self.rewards[episode]),\n            \'action_mean\': np.mean(self.actions[episode]),\n            \'action_min\': np.min(self.actions[episode]),\n            \'action_max\': np.max(self.actions[episode]),\n            \'obs_mean\': np.mean(self.observations[episode]),\n            \'obs_min\': np.min(self.observations[episode]),\n            \'obs_max\': np.max(self.observations[episode]),\n            **metrics_dict\n        })\n\n        # Free up resources.\n        del self.episode_start[episode]\n        del self.observations[episode]\n        del self.rewards[episode]\n        del self.actions[episode]\n        del self.metrics[episode]\n\n    def on_step_end(self, step, logs):\n        """""" Update statistics of episode after each step """"""\n        episode = logs[\'episode\']\n        self.observations[episode].append(logs[\'observation\'])\n        self.rewards[episode].append(logs[\'reward\'])\n        self.actions[episode].append(logs[\'action\'])\n        self.metrics[episode].append(logs[\'metrics\'])\n        self.step += 1\n'"
rl/core.py,0,"b'# -*- coding: utf-8 -*-\nimport warnings\nfrom copy import deepcopy\n\nimport numpy as np\nfrom keras.callbacks import History\n\nfrom rl.callbacks import (\n    CallbackList,\n    TestLogger,\n    TrainEpisodeLogger,\n    TrainIntervalLogger,\n    Visualizer,\n    WandbLogger\n)\n\n\nclass Agent(object):\n    """"""Abstract base class for all implemented agents.\n\n    Each agent interacts with the environment (as defined by the `Env` class) by first observing the\n    state of the environment. Based on this observation the agent changes the environment by performing\n    an action.\n\n    Do not use this abstract base class directly but instead use one of the concrete agents implemented.\n    Each agent realizes a reinforcement learning algorithm. Since all agents conform to the same\n    interface, you can use them interchangeably.\n\n    To implement your own agent, you have to implement the following methods:\n\n    - `forward`\n    - `backward`\n    - `compile`\n    - `load_weights`\n    - `save_weights`\n    - `layers`\n\n    # Arguments\n        processor (`Processor` instance): See [Processor](#processor) for details.\n    """"""\n\n    def __init__(self, processor=None):\n        self.processor = processor\n        self.training = False\n        self.step = 0\n\n    def get_config(self):\n        """"""Configuration of the agent for serialization.\n\n        # Returns\n            Dictionnary with agent configuration\n        """"""\n        return {}\n\n    def fit(self, env, nb_steps, action_repetition=1, callbacks=None, verbose=1,\n            visualize=False, nb_max_start_steps=0, start_step_policy=None, log_interval=10000,\n            nb_max_episode_steps=None):\n        """"""Trains the agent on the given environment.\n\n        # Arguments\n            env: (`Env` instance): Environment that the agent interacts with. See [Env](#env) for details.\n            nb_steps (integer): Number of training steps to be performed.\n            action_repetition (integer): Number of times the agent repeats the same action without\n                observing the environment again. Setting this to a value > 1 can be useful\n                if a single action only has a very small effect on the environment.\n            callbacks (list of `keras.callbacks.Callback` or `rl.callbacks.Callback` instances):\n                List of callbacks to apply during training. See [callbacks](/callbacks) for details.\n            verbose (integer): 0 for no logging, 1 for interval logging (compare `log_interval`), 2 for episode logging\n            visualize (boolean): If `True`, the environment is visualized during training. However,\n                this is likely going to slow down training significantly and is thus intended to be\n                a debugging instrument.\n            nb_max_start_steps (integer): Number of maximum steps that the agent performs at the beginning\n                of each episode using `start_step_policy`. Notice that this is an upper limit since\n                the exact number of steps to be performed is sampled uniformly from [0, max_start_steps]\n                at the beginning of each episode.\n            start_step_policy (`lambda observation: action`): The policy\n                to follow if `nb_max_start_steps` > 0. If set to `None`, a random action is performed.\n            log_interval (integer): If `verbose` = 1, the number of steps that are considered to be an interval.\n            nb_max_episode_steps (integer): Number of steps per episode that the agent performs before\n                automatically resetting the environment. Set to `None` if each episode should run\n                (potentially indefinitely) until the environment signals a terminal state.\n\n        # Returns\n            A `keras.callbacks.History` instance that recorded the entire training process.\n        """"""\n        if not self.compiled:\n            raise RuntimeError(\n                \'Your tried to fit your agent but it hasn\\\'t been compiled yet. Please call `compile()` before `fit()`.\')\n        if action_repetition < 1:\n            raise ValueError(\n                \'action_repetition must be >= 1, is {}\'.format(action_repetition))\n\n        self.training = True\n\n        callbacks = [] if not callbacks else callbacks[:]\n\n        if verbose == 1:\n            callbacks += [TrainIntervalLogger(interval=log_interval)]\n        elif verbose > 1:\n            callbacks += [TrainEpisodeLogger()]\n        if visualize:\n            callbacks += [Visualizer()]\n        history = History()\n        callbacks += [history]\n        callbacks = CallbackList(callbacks)\n        if hasattr(callbacks, \'set_model\'):\n            callbacks.set_model(self)\n        else:\n            callbacks._set_model(self)\n        callbacks._set_env(env)\n        params = {\n            \'nb_steps\': nb_steps,\n        }\n        if hasattr(callbacks, \'set_params\'):\n            callbacks.set_params(params)\n        else:\n            callbacks._set_params(params)\n        self._on_train_begin()\n        callbacks.on_train_begin()\n\n        episode = np.int16(0)\n        self.step = np.int16(0)\n        observation = None\n        episode_reward = None\n        episode_step = None\n        did_abort = False\n        try:\n            while self.step < nb_steps:\n                if observation is None:  # start of a new episode\n                    callbacks.on_episode_begin(episode)\n                    episode_step = np.int16(0)\n                    episode_reward = np.float32(0)\n\n                    # Obtain the initial observation by resetting the environment.\n                    self.reset_states()\n                    observation = deepcopy(env.reset())\n                    if self.processor is not None:\n                        observation = self.processor.process_observation(\n                            observation)\n                    assert observation is not None\n\n                    # Perform random starts at beginning of episode and do not record them into the experience.\n                    # This slightly changes the start position between games.\n                    nb_random_start_steps = 0 if nb_max_start_steps == 0 else np.random.randint(\n                        nb_max_start_steps)\n                    for _ in range(nb_random_start_steps):\n                        if start_step_policy is None:\n                            action = env.action_space.sample()\n                        else:\n                            action = start_step_policy(observation)\n                        if self.processor is not None:\n                            action = self.processor.process_action(action)\n                        callbacks.on_action_begin(action)\n                        observation, reward, done, info = env.step(action)\n                        observation = deepcopy(observation)\n                        if self.processor is not None:\n                            observation, reward, done, info = self.processor.process_step(\n                                observation, reward, done, info)\n                        callbacks.on_action_end(action)\n                        if done:\n                            warnings.warn(\'Env ended before {} random steps could be performed at the start. You should probably lower the `nb_max_start_steps` parameter.\'.format(\n                                nb_random_start_steps))\n                            observation = deepcopy(env.reset())\n                            if self.processor is not None:\n                                observation = self.processor.process_observation(\n                                    observation)\n                            break\n\n                # At this point, we expect to be fully initialized.\n                assert episode_reward is not None\n                assert episode_step is not None\n                assert observation is not None\n\n                # Run a single step.\n                callbacks.on_step_begin(episode_step)\n                # This is were all of the work happens. We first perceive and compute the action\n                # (forward step) and then use the reward to improve (backward step).\n                action = self.forward(observation)\n                if self.processor is not None:\n                    action = self.processor.process_action(action)\n                reward = np.float32(0)\n                accumulated_info = {}\n                done = False\n                for _ in range(action_repetition):\n                    callbacks.on_action_begin(action)\n                    observation, r, done, info = env.step(action)\n                    observation = deepcopy(observation)\n                    if self.processor is not None:\n                        observation, r, done, info = self.processor.process_step(\n                            observation, r, done, info)\n                    for key, value in info.items():\n                        if not np.isreal(value):\n                            continue\n                        if key not in accumulated_info:\n                            accumulated_info[key] = np.zeros_like(value)\n                        accumulated_info[key] += value\n                    callbacks.on_action_end(action)\n                    reward += r\n                    if done:\n                        break\n                if nb_max_episode_steps and episode_step >= nb_max_episode_steps - 1:\n                    # Force a terminal state.\n                    done = True\n                metrics = self.backward(reward, terminal=done)\n                episode_reward += reward\n\n                step_logs = {\n                    \'action\': action,\n                    \'observation\': observation,\n                    \'reward\': reward,\n                    \'metrics\': metrics,\n                    \'episode\': episode,\n                    \'info\': accumulated_info,\n                }\n                callbacks.on_step_end(episode_step, step_logs)\n                episode_step += 1\n                self.step += 1\n\n                if done:\n                    # We are in a terminal state but the agent hasn\'t yet seen it. We therefore\n                    # perform one more forward-backward call and simply ignore the action before\n                    # resetting the environment. We need to pass in `terminal=False` here since\n                    # the *next* state, that is the state of the newly reset environment, is\n                    # always non-terminal by convention.\n                    self.forward(observation)\n                    self.backward(0., terminal=False)\n\n                    # This episode is finished, report and reset.\n                    episode_logs = {\n                        \'episode_reward\': episode_reward,\n                        \'nb_episode_steps\': episode_step,\n                        \'nb_steps\': self.step,\n                    }\n                    callbacks.on_episode_end(episode, episode_logs)\n\n                    episode += 1\n                    observation = None\n                    episode_step = None\n                    episode_reward = None\n        except KeyboardInterrupt:\n            # We catch keyboard interrupts here so that training can be be safely aborted.\n            # This is so common that we\'ve built this right into this function, which ensures that\n            # the `on_train_end` method is properly called.\n            did_abort = True\n        callbacks.on_train_end(logs={\'did_abort\': did_abort})\n        self._on_train_end()\n\n        return history\n\n    def test(self, env, nb_episodes=1, action_repetition=1, callbacks=None, visualize=True,\n             nb_max_episode_steps=None, nb_max_start_steps=0, start_step_policy=None, verbose=1):\n        """"""Callback that is called before training begins.\n\n        # Arguments\n            env: (`Env` instance): Environment that the agent interacts with. See [Env](#env) for details.\n            nb_episodes (integer): Number of episodes to perform.\n            action_repetition (integer): Number of times the agent repeats the same action without\n                observing the environment again. Setting this to a value > 1 can be useful\n                if a single action only has a very small effect on the environment.\n            callbacks (list of `keras.callbacks.Callback` or `rl.callbacks.Callback` instances):\n                List of callbacks to apply during training. See [callbacks](/callbacks) for details.\n            verbose (integer): 0 for no logging, 1 for interval logging (compare `log_interval`), 2 for episode logging\n            visualize (boolean): If `True`, the environment is visualized during training. However,\n                this is likely going to slow down training significantly and is thus intended to be\n                a debugging instrument.\n            nb_max_start_steps (integer): Number of maximum steps that the agent performs at the beginning\n                of each episode using `start_step_policy`. Notice that this is an upper limit since\n                the exact number of steps to be performed is sampled uniformly from [0, max_start_steps]\n                at the beginning of each episode.\n            start_step_policy (`lambda observation: action`): The policy\n                to follow if `nb_max_start_steps` > 0. If set to `None`, a random action is performed.\n            log_interval (integer): If `verbose` = 1, the number of steps that are considered to be an interval.\n            nb_max_episode_steps (integer): Number of steps per episode that the agent performs before\n                automatically resetting the environment. Set to `None` if each episode should run\n                (potentially indefinitely) until the environment signals a terminal state.\n\n        # Returns\n            A `keras.callbacks.History` instance that recorded the entire training process.\n        """"""\n        if not self.compiled:\n            raise RuntimeError(\n                \'Your tried to test your agent but it hasn\\\'t been compiled yet. Please call `compile()` before `test()`.\')\n        if action_repetition < 1:\n            raise ValueError(\n                \'action_repetition must be >= 1, is {}\'.format(action_repetition))\n\n        self.training = False\n        self.step = 0\n\n        callbacks = [] if not callbacks else callbacks[:]\n\n        if verbose >= 1:\n            callbacks += [TestLogger()]\n        if visualize:\n            callbacks += [Visualizer()]\n        history = History()\n        callbacks += [history]\n        callbacks = CallbackList(callbacks)\n        if hasattr(callbacks, \'set_model\'):\n            callbacks.set_model(self)\n        else:\n            callbacks._set_model(self)\n        callbacks._set_env(env)\n        params = {\n            \'nb_episodes\': nb_episodes,\n        }\n        if hasattr(callbacks, \'set_params\'):\n            callbacks.set_params(params)\n        else:\n            callbacks._set_params(params)\n\n        self._on_test_begin()\n        callbacks.on_train_begin()\n        for episode in range(nb_episodes):\n            callbacks.on_episode_begin(episode)\n            episode_reward = 0.\n            episode_step = 0\n\n            # Obtain the initial observation by resetting the environment.\n            self.reset_states()\n            observation = deepcopy(env.reset())\n            if self.processor is not None:\n                observation = self.processor.process_observation(observation)\n            assert observation is not None\n\n            # Perform random starts at beginning of episode and do not record them into the experience.\n            # This slightly changes the start position between games.\n            nb_random_start_steps = 0 if nb_max_start_steps == 0 else np.random.randint(\n                nb_max_start_steps)\n            for _ in range(nb_random_start_steps):\n                if start_step_policy is None:\n                    action = env.action_space.sample()\n                else:\n                    action = start_step_policy(observation)\n                if self.processor is not None:\n                    action = self.processor.process_action(action)\n                callbacks.on_action_begin(action)\n                observation, r, done, info = env.step(action)\n                observation = deepcopy(observation)\n                if self.processor is not None:\n                    observation, r, done, info = self.processor.process_step(\n                        observation, r, done, info)\n                callbacks.on_action_end(action)\n                if done:\n                    warnings.warn(\'Env ended before {} random steps could be performed at the start. You should probably lower the `nb_max_start_steps` parameter.\'.format(\n                        nb_random_start_steps))\n                    observation = deepcopy(env.reset())\n                    if self.processor is not None:\n                        observation = self.processor.process_observation(\n                            observation)\n                    break\n\n            # Run the episode until we\'re done.\n            done = False\n            while not done:\n                callbacks.on_step_begin(episode_step)\n\n                action = self.forward(observation)\n                if self.processor is not None:\n                    action = self.processor.process_action(action)\n                reward = 0.\n                accumulated_info = {}\n                for _ in range(action_repetition):\n                    callbacks.on_action_begin(action)\n                    observation, r, d, info = env.step(action)\n                    observation = deepcopy(observation)\n                    if self.processor is not None:\n                        observation, r, d, info = self.processor.process_step(\n                            observation, r, d, info)\n                    callbacks.on_action_end(action)\n                    reward += r\n                    for key, value in info.items():\n                        if not np.isreal(value):\n                            continue\n                        if key not in accumulated_info:\n                            accumulated_info[key] = np.zeros_like(value)\n                        accumulated_info[key] += value\n                    if d:\n                        done = True\n                        break\n                if nb_max_episode_steps and episode_step >= nb_max_episode_steps - 1:\n                    done = True\n                self.backward(reward, terminal=done)\n                episode_reward += reward\n\n                step_logs = {\n                    \'action\': action,\n                    \'observation\': observation,\n                    \'reward\': reward,\n                    \'episode\': episode,\n                    \'info\': accumulated_info,\n                }\n                callbacks.on_step_end(episode_step, step_logs)\n                episode_step += 1\n                self.step += 1\n\n            # We are in a terminal state but the agent hasn\'t yet seen it. We therefore\n            # perform one more forward-backward call and simply ignore the action before\n            # resetting the environment. We need to pass in `terminal=False` here since\n            # the *next* state, that is the state of the newly reset environment, is\n            # always non-terminal by convention.\n            self.forward(observation)\n            self.backward(0., terminal=False)\n\n            # Report end of episode.\n            episode_logs = {\n                \'episode_reward\': episode_reward,\n                \'nb_steps\': episode_step,\n            }\n            callbacks.on_episode_end(episode, episode_logs)\n        callbacks.on_train_end()\n        self._on_test_end()\n\n        return history\n\n    def reset_states(self):\n        """"""Resets all internally kept states after an episode is completed.\n        """"""\n        pass\n\n    def forward(self, observation):\n        """"""Takes the an observation from the environment and returns the action to be taken next.\n        If the policy is implemented by a neural network, this corresponds to a forward (inference) pass.\n\n        # Argument\n            observation (object): The current observation from the environment.\n\n        # Returns\n            The next action to be executed in the environment.\n        """"""\n        raise NotImplementedError()\n\n    def backward(self, reward, terminal):\n        """"""Updates the agent after having executed the action returned by `forward`.\n        If the policy is implemented by a neural network, this corresponds to a weight update using back-prop.\n\n        # Argument\n            reward (float): The observed reward after executing the action returned by `forward`.\n            terminal (boolean): `True` if the new state of the environment is terminal.\n\n        # Returns\n            List of metrics values\n        """"""\n        raise NotImplementedError()\n\n    def compile(self, optimizer, metrics=[]):\n        """"""Compiles an agent and the underlaying models to be used for training and testing.\n\n        # Arguments\n            optimizer (`keras.optimizers.Optimizer` instance): The optimizer to be used during training.\n            metrics (list of functions `lambda y_true, y_pred: metric`): The metrics to run during training.\n        """"""\n        raise NotImplementedError()\n\n    def load_weights(self, filepath):\n        """"""Loads the weights of an agent from an HDF5 file.\n\n        # Arguments\n            filepath (str): The path to the HDF5 file.\n        """"""\n        raise NotImplementedError()\n\n    def save_weights(self, filepath, overwrite=False):\n        """"""Saves the weights of an agent as an HDF5 file.\n\n        # Arguments\n            filepath (str): The path to where the weights should be saved.\n            overwrite (boolean): If `False` and `filepath` already exists, raises an error.\n        """"""\n        raise NotImplementedError()\n\n    @property\n    def layers(self):\n        """"""Returns all layers of the underlying model(s).\n\n        If the concrete implementation uses multiple internal models,\n        this method returns them in a concatenated list.\n\n        # Returns\n            A list of the model\'s layers\n        """"""\n        raise NotImplementedError()\n\n    @property\n    def metrics_names(self):\n        """"""The human-readable names of the agent\'s metrics. Must return as many names as there\n        are metrics (see also `compile`).\n\n        # Returns\n            A list of metric\'s names (string)\n        """"""\n        return []\n\n    def _on_train_begin(self):\n        """"""Callback that is called before training begins.""\n        """"""\n        pass\n\n    def _on_train_end(self):\n        """"""Callback that is called after training ends.""\n        """"""\n        pass\n\n    def _on_test_begin(self):\n        """"""Callback that is called before testing begins.""\n        """"""\n        pass\n\n    def _on_test_end(self):\n        """"""Callback that is called after testing ends.""\n        """"""\n        pass\n\n\nclass Processor(object):\n    """"""Abstract base class for implementing processors.\n\n    A processor acts as a coupling mechanism between an `Agent` and its `Env`. This can\n    be necessary if your agent has different requirements with respect to the form of the\n    observations, actions, and rewards of the environment. By implementing a custom processor,\n    you can effectively translate between the two without having to change the underlaying\n    implementation of the agent or environment.\n\n    Do not use this abstract base class directly but instead use one of the concrete implementations\n    or write your own.\n    """"""\n\n    def process_step(self, observation, reward, done, info):\n        """"""Processes an entire step by applying the processor to the observation, reward, and info arguments.\n\n        # Arguments\n            observation (object): An observation as obtained by the environment.\n            reward (float): A reward as obtained by the environment.\n            done (boolean): `True` if the environment is in a terminal state, `False` otherwise.\n            info (dict): The debug info dictionary as obtained by the environment.\n\n        # Returns\n            The tupel (observation, reward, done, reward) with with all elements after being processed.\n        """"""\n        observation = self.process_observation(observation)\n        reward = self.process_reward(reward)\n        info = self.process_info(info)\n        return observation, reward, done, info\n\n    def process_observation(self, observation):\n        """"""Processes the observation as obtained from the environment for use in an agent and\n        returns it.\n\n        # Arguments\n            observation (object): An observation as obtained by the environment\n\n        # Returns\n            Observation obtained by the environment processed\n        """"""\n        return observation\n\n    def process_reward(self, reward):\n        """"""Processes the reward as obtained from the environment for use in an agent and\n        returns it.\n\n        # Arguments\n            reward (float): A reward as obtained by the environment\n\n        # Returns\n            Reward obtained by the environment processed\n        """"""\n        return reward\n\n    def process_info(self, info):\n        """"""Processes the info as obtained from the environment for use in an agent and\n        returns it.\n\n        # Arguments\n            info (dict): An info as obtained by the environment\n\n        # Returns\n            Info obtained by the environment processed\n        """"""\n        return info\n\n    def process_action(self, action):\n        """"""Processes an action predicted by an agent but before execution in an environment.\n\n        # Arguments\n            action (int): Action given to the environment\n\n        #\xc2\xa0Returns\n            Processed action given to the environment\n        """"""\n        return action\n\n    def process_state_batch(self, batch):\n        """"""Processes an entire batch of states and returns it.\n\n        # Arguments\n            batch (list): List of states\n\n        # Returns\n            Processed list of states\n        """"""\n        return batch\n\n    @property\n    def metrics(self):\n        """"""The metrics of the processor, which will be reported during training.\n\n        # Returns\n            List of `lambda y_true, y_pred: metric` functions.\n        """"""\n        return []\n\n    @property\n    def metrics_names(self):\n        """"""The human-readable names of the agent\'s metrics. Must return as many names as there\n        are metrics (see also `compile`).\n        """"""\n        return []\n\n\n# Note: the API of the `Env` and `Space` classes are taken from the OpenAI Gym implementation.\n# https://github.com/openai/gym/blob/master/gym/core.py\n\n\nclass Env(object):\n    """"""The abstract environment class that is used by all agents. This class has the exact\n    same API that OpenAI Gym uses so that integrating with it is trivial. In contrast to the\n    OpenAI Gym implementation, this class only defines the abstract methods without any actual\n    implementation.\n\n    To implement your own environment, you need to define the following methods:\n\n    - `step`\n    - `reset`\n    -\xc2\xa0`render`\n    -\xc2\xa0`close`\n\n    Refer to the [Gym documentation](https://gym.openai.com/docs/#environments).\n    """"""\n    reward_range = (-np.inf, np.inf)\n    action_space = None\n    observation_space = None\n\n    def step(self, action):\n        """"""Run one timestep of the environment\'s dynamics.\n        Accepts an action and returns a tuple (observation, reward, done, info).\n\n        # Arguments\n            action (object): An action provided by the environment.\n\n        # Returns\n            observation (object): Agent\'s observation of the current environment.\n            reward (float) : Amount of reward returned after previous action.\n            done (boolean): Whether the episode has ended, in which case further step() calls will return undefined results.\n            info (dict): Contains auxiliary diagnostic information (helpful for debugging, and sometimes learning).\n        """"""\n        raise NotImplementedError()\n\n    def reset(self):\n        """"""\n        Resets the state of the environment and returns an initial observation.\n\n        # Returns\n            observation (object): The initial observation of the space. Initial reward is assumed to be 0.\n        """"""\n        raise NotImplementedError()\n\n    def render(self, mode=\'human\', close=False):\n        """"""Renders the environment.\n        The set of supported modes varies per environment. (And some\n        environments do not support rendering at all.)\n\n        # Arguments\n            mode (str): The mode to render with.\n            close (bool): Close all open renderings.\n        """"""\n        raise NotImplementedError()\n\n    def close(self):\n        """"""Override in your subclass to perform any necessary cleanup.\n        Environments will automatically close() themselves when\n        garbage collected or when the program exits.\n        """"""\n        raise NotImplementedError()\n\n    def seed(self, seed=None):\n        """"""Sets the seed for this env\'s random number generator(s).\n\n        # Returns\n            Returns the list of seeds used in this env\'s random number generators\n        """"""\n        raise NotImplementedError()\n\n    def configure(self, *args, **kwargs):\n        """"""Provides runtime configuration to the environment.\n        This configuration should consist of data that tells your\n        environment how to run (such as an address of a remote server,\n        or path to your ImageNet data). It should not affect the\n        semantics of the environment.\n        """"""\n        raise NotImplementedError()\n\n    def __del__(self):\n        self.close()\n\n    def __str__(self):\n        return \'<{} instance>\'.format(type(self).__name__)\n\n\nclass Space(object):\n    """"""Abstract model for a space that is used for the state and action spaces. This class has the\n    exact same API that OpenAI Gym uses so that integrating with it is trivial.\n\n    Please refer to [Gym Documentation](https://gym.openai.com/docs/#spaces)\n    """"""\n\n    def sample(self, seed=None):\n        """"""Uniformly randomly sample a random element of this space.\n        """"""\n        raise NotImplementedError()\n\n    def contains(self, x):\n        """"""Return boolean specifying if x is a valid member of this space\n        """"""\n        raise NotImplementedError()\n'"
rl/memory.py,0,"b'from __future__ import absolute_import\nfrom collections import deque, namedtuple\nimport warnings\nimport random\n\nimport numpy as np\n\n\n# This is to be understood as a transition: Given `state0`, performing `action`\n# yields `reward` and results in `state1`, which might be `terminal`.\nExperience = namedtuple(\'Experience\', \'state0, action, reward, state1, terminal1\')\n\n\ndef sample_batch_indexes(low, high, size):\n    """"""Return a sample of (size) unique elements between low and high\n\n        # Argument\n            low (int): The minimum value for our samples\n            high (int): The maximum value for our samples\n            size (int): The number of samples to pick\n\n        # Returns\n            A list of samples of length size, with values between low and high\n        """"""\n    if high - low >= size:\n        # We have enough data. Draw without replacement, that is each index is unique in the\n        # batch. We cannot use `np.random.choice` here because it is horribly inefficient as\n        # the memory grows. See https://github.com/numpy/numpy/issues/2764 for a discussion.\n        # `random.sample` does the same thing (drawing without replacement) and is way faster.\n        try:\n            r = xrange(low, high)\n        except NameError:\n            r = range(low, high)\n        batch_idxs = random.sample(r, size)\n    else:\n        # Not enough data. Help ourselves with sampling from the range, but the same index\n        # can occur multiple times. This is not good and should be avoided by picking a\n        # large enough warm-up phase.\n        warnings.warn(\'Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\')\n        batch_idxs = np.random.random_integers(low, high - 1, size=size)\n    assert len(batch_idxs) == size\n    return batch_idxs\n\n\nclass RingBuffer(object):\n    def __init__(self, maxlen):\n        self.maxlen = maxlen\n        self.data = deque(maxlen=maxlen)\n\n    def __len__(self):\n        return self.length()\n\n    def __getitem__(self, idx):\n        """"""Return element of buffer at specific index\n\n        # Argument\n            idx (int): Index wanted\n\n        # Returns\n            The element of buffer at given index\n        """"""\n        if idx < 0 or idx >= self.length():\n            raise KeyError()\n        return self.data[idx]\n\n    def append(self, v):\n        """"""Append an element to the buffer\n\n        # Argument\n            v (object): Element to append\n        """"""\n        self.data.append(v)\n\n    def length(self):\n        """"""Return the length of Deque\n\n        # Argument\n            None\n\n        # Returns\n            The lenght of deque element\n        """"""\n        return len(self.data)\n\ndef zeroed_observation(observation):\n    """"""Return an array of zeros with same shape as given observation\n\n    # Argument\n        observation (list): List of observation\n    \n    # Return\n        A np.ndarray of zeros with observation.shape\n    """"""\n    if hasattr(observation, \'shape\'):\n        return np.zeros(observation.shape)\n    elif hasattr(observation, \'__iter__\'):\n        out = []\n        for x in observation:\n            out.append(zeroed_observation(x))\n        return out\n    else:\n        return 0.\n\n\nclass Memory(object):\n    def __init__(self, window_length, ignore_episode_boundaries=False):\n        self.window_length = window_length\n        self.ignore_episode_boundaries = ignore_episode_boundaries\n\n        self.recent_observations = deque(maxlen=window_length)\n        self.recent_terminals = deque(maxlen=window_length)\n\n    def sample(self, batch_size, batch_idxs=None):\n        raise NotImplementedError()\n\n    def append(self, observation, action, reward, terminal, training=True):\n        self.recent_observations.append(observation)\n        self.recent_terminals.append(terminal)\n\n    def get_recent_state(self, current_observation):\n        """"""Return list of last observations\n\n        # Argument\n            current_observation (object): Last observation\n\n        # Returns\n            A list of the last observations\n        """"""\n        # This code is slightly complicated by the fact that subsequent observations might be\n        # from different episodes. We ensure that an experience never spans multiple episodes.\n        # This is probably not that important in practice but it seems cleaner.\n        state = [current_observation]\n        idx = len(self.recent_observations) - 1\n        for offset in range(0, self.window_length - 1):\n            current_idx = idx - offset\n            current_terminal = self.recent_terminals[current_idx - 1] if current_idx - 1 >= 0 else False\n            if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n                # The previously handled observation was terminal, don\'t add the current one.\n                # Otherwise we would leak into a different episode.\n                break\n            state.insert(0, self.recent_observations[current_idx])\n        while len(state) < self.window_length:\n            state.insert(0, zeroed_observation(state[0]))\n        return state\n\n    def get_config(self):\n        """"""Return configuration (window_length, ignore_episode_boundaries) for Memory\n        \n        # Return\n            A dict with keys window_length and ignore_episode_boundaries\n        """"""\n        config = {\n            \'window_length\': self.window_length,\n            \'ignore_episode_boundaries\': self.ignore_episode_boundaries,\n        }\n        return config\n\nclass SequentialMemory(Memory):\n    def __init__(self, limit, **kwargs):\n        super(SequentialMemory, self).__init__(**kwargs)\n        \n        self.limit = limit\n\n        # Do not use deque to implement the memory. This data structure may seem convenient but\n        # it is way too slow on random access. Instead, we use our own ring buffer implementation.\n        self.actions = RingBuffer(limit)\n        self.rewards = RingBuffer(limit)\n        self.terminals = RingBuffer(limit)\n        self.observations = RingBuffer(limit)\n\n    def sample(self, batch_size, batch_idxs=None):\n        """"""Return a randomized batch of experiences\n\n        # Argument\n            batch_size (int): Size of the all batch\n            batch_idxs (int): Indexes to extract\n        # Returns\n            A list of experiences randomly selected\n        """"""\n        # It is not possible to tell whether the first state in the memory is terminal, because it\n        # would require access to the ""terminal"" flag associated to the previous state. As a result\n        # we will never return this first state (only using `self.terminals[0]` to know whether the\n        # second state is terminal).\n        # In addition we need enough entries to fill the desired window length.\n        assert self.nb_entries >= self.window_length + 2, \'not enough entries in the memory\'\n\n        if batch_idxs is None:\n            # Draw random indexes such that we have enough entries before each index to fill the\n            # desired window length.\n            batch_idxs = sample_batch_indexes(\n                self.window_length, self.nb_entries - 1, size=batch_size)\n        batch_idxs = np.array(batch_idxs) + 1\n        assert np.min(batch_idxs) >= self.window_length + 1\n        assert np.max(batch_idxs) < self.nb_entries\n        assert len(batch_idxs) == batch_size\n\n        # Create experiences\n        experiences = []\n        for idx in batch_idxs:\n            terminal0 = self.terminals[idx - 2]\n            while terminal0:\n                # Skip this transition because the environment was reset here. Select a new, random\n                # transition and use this instead. This may cause the batch to contain the same\n                # transition twice.\n                idx = sample_batch_indexes(self.window_length + 1, self.nb_entries, size=1)[0]\n                terminal0 = self.terminals[idx - 2]\n            assert self.window_length + 1 <= idx < self.nb_entries\n\n            # This code is slightly complicated by the fact that subsequent observations might be\n            # from different episodes. We ensure that an experience never spans multiple episodes.\n            # This is probably not that important in practice but it seems cleaner.\n            state0 = [self.observations[idx - 1]]\n            for offset in range(0, self.window_length - 1):\n                current_idx = idx - 2 - offset\n                assert current_idx >= 1\n                current_terminal = self.terminals[current_idx - 1]\n                if current_terminal and not self.ignore_episode_boundaries:\n                    # The previously handled observation was terminal, don\'t add the current one.\n                    # Otherwise we would leak into a different episode.\n                    break\n                state0.insert(0, self.observations[current_idx])\n            while len(state0) < self.window_length:\n                state0.insert(0, zeroed_observation(state0[0]))\n            action = self.actions[idx - 1]\n            reward = self.rewards[idx - 1]\n            terminal1 = self.terminals[idx - 1]\n\n            # Okay, now we need to create the follow-up state. This is state0 shifted on timestep\n            # to the right. Again, we need to be careful to not include an observation from the next\n            # episode if the last state is terminal.\n            state1 = [np.copy(x) for x in state0[1:]]\n            state1.append(self.observations[idx])\n\n            assert len(state0) == self.window_length\n            assert len(state1) == len(state0)\n            experiences.append(Experience(state0=state0, action=action, reward=reward,\n                                          state1=state1, terminal1=terminal1))\n        assert len(experiences) == batch_size\n        return experiences\n\n    def append(self, observation, action, reward, terminal, training=True):\n        """"""Append an observation to the memory\n\n        # Argument\n            observation (dict): Observation returned by environment\n            action (int): Action taken to obtain this observation\n            reward (float): Reward obtained by taking this action\n            terminal (boolean): Is the state terminal\n        """""" \n        super(SequentialMemory, self).append(observation, action, reward, terminal, training=training)\n        \n        # This needs to be understood as follows: in `observation`, take `action`, obtain `reward`\n        # and weather the next state is `terminal` or not.\n        if training:\n            self.observations.append(observation)\n            self.actions.append(action)\n            self.rewards.append(reward)\n            self.terminals.append(terminal)\n\n    @property\n    def nb_entries(self):\n        """"""Return number of observations\n\n        # Returns\n            Number of observations\n        """"""\n        return len(self.observations)\n\n    def get_config(self):\n        """"""Return configurations of SequentialMemory\n\n        # Returns\n            Dict of config\n        """"""\n        config = super(SequentialMemory, self).get_config()\n        config[\'limit\'] = self.limit\n        return config\n\n\nclass EpisodeParameterMemory(Memory):\n    def __init__(self, limit, **kwargs):\n        super(EpisodeParameterMemory, self).__init__(**kwargs)\n        self.limit = limit\n\n        self.params = RingBuffer(limit)\n        self.intermediate_rewards = []\n        self.total_rewards = RingBuffer(limit)\n\n    def sample(self, batch_size, batch_idxs=None):\n        """"""Return a randomized batch of params and rewards\n\n        # Argument\n            batch_size (int): Size of the all batch\n            batch_idxs (int): Indexes to extract\n        # Returns\n            A list of params randomly selected and a list of associated rewards\n        """"""\n        if batch_idxs is None:\n            batch_idxs = sample_batch_indexes(0, self.nb_entries, size=batch_size)\n        assert len(batch_idxs) == batch_size\n\n        batch_params = []\n        batch_total_rewards = []\n        for idx in batch_idxs:\n            batch_params.append(self.params[idx])\n            batch_total_rewards.append(self.total_rewards[idx])\n        return batch_params, batch_total_rewards\n\n    def append(self, observation, action, reward, terminal, training=True):\n        """"""Append a reward to the memory\n\n        # Argument\n            observation (dict): Observation returned by environment\n            action (int): Action taken to obtain this observation\n            reward (float): Reward obtained by taking this action\n            terminal (boolean): Is the state terminal\n        """"""\n        super(EpisodeParameterMemory, self).append(observation, action, reward, terminal, training=training)\n        if training:\n            self.intermediate_rewards.append(reward)\n\n    def finalize_episode(self, params):\n        """"""Closes the current episode, sums up rewards and stores the parameters\n\n        # Argument\n            params (object): Parameters associated with the episode to be stored and then retrieved back in sample()\n        """"""\n        total_reward = sum(self.intermediate_rewards)\n        self.total_rewards.append(total_reward)\n        self.params.append(params)\n        self.intermediate_rewards = []\n\n    @property\n    def nb_entries(self):\n        """"""Return number of episode rewards\n\n        # Returns\n            Number of episode rewards\n        """"""\n        return len(self.total_rewards)\n\n    def get_config(self):\n        """"""Return configurations of SequentialMemory\n\n        # Returns\n            Dict of config\n        """"""\n        config = super(SequentialMemory, self).get_config()\n        config[\'limit\'] = self.limit\n        return config\n'"
rl/policy.py,0,"b'from __future__ import division\nimport numpy as np\n\nfrom rl.util import *\n\n\nclass Policy(object):\n    """"""Abstract base class for all implemented policies.\n\n    Each policy helps with selection of action to take on an environment.\n\n    Do not use this abstract base class directly but instead use one of the concrete policies implemented.\n    To implement your own policy, you have to implement the following methods:\n\n    - `select_action`\n\n    # Arguments\n        agent (rl.core.Agent): Agent used\n    """"""\n    def _set_agent(self, agent):\n        self.agent = agent\n\n    @property\n    def metrics_names(self):\n        return []\n\n    @property\n    def metrics(self):\n        return []\n\n    def select_action(self, **kwargs):\n        raise NotImplementedError()\n\n    def get_config(self):\n        """"""Return configuration of the policy\n\n        # Returns\n            Configuration as dict\n        """"""\n        return {}\n\n\nclass LinearAnnealedPolicy(Policy):\n    """"""Implement the linear annealing policy\n\n    Linear Annealing Policy computes a current threshold value and\n    transfers it to an inner policy which chooses the action. The threshold\n    value is following a linear function decreasing over time.""""""\n    def __init__(self, inner_policy, attr, value_max, value_min, value_test, nb_steps):\n        if not hasattr(inner_policy, attr):\n            raise ValueError(\'Policy does not have attribute ""{}"".\'.format(attr))\n\n        super(LinearAnnealedPolicy, self).__init__()\n\n        self.inner_policy = inner_policy\n        self.attr = attr\n        self.value_max = value_max\n        self.value_min = value_min\n        self.value_test = value_test\n        self.nb_steps = nb_steps\n\n    def get_current_value(self):\n        """"""Return current annealing value\n\n        # Returns\n            Value to use in annealing\n        """"""\n        if self.agent.training:\n            # Linear annealed: f(x) = ax + b.\n            a = -float(self.value_max - self.value_min) / float(self.nb_steps)\n            b = float(self.value_max)\n            value = max(self.value_min, a * float(self.agent.step) + b)\n        else:\n            value = self.value_test\n        return value\n\n    def select_action(self, **kwargs):\n        """"""Choose an action to perform\n\n        # Returns\n            Action to take (int)\n        """"""\n        setattr(self.inner_policy, self.attr, self.get_current_value())\n        return self.inner_policy.select_action(**kwargs)\n\n    @property\n    def metrics_names(self):\n        """"""Return names of metrics\n\n        # Returns\n            List of metric names\n        """"""\n        return [\'mean_{}\'.format(self.attr)]\n\n    @property\n    def metrics(self):\n        """"""Return metrics values\n\n        # Returns\n            List of metric values\n        """"""\n\n        return [getattr(self.inner_policy, self.attr)]\n\n    def get_config(self):\n        """"""Return configurations of LinearAnnealedPolicy\n\n        # Returns\n            Dict of config\n        """"""\n        config = super(LinearAnnealedPolicy, self).get_config()\n        config[\'attr\'] = self.attr\n        config[\'value_max\'] = self.value_max\n        config[\'value_min\'] = self.value_min\n        config[\'value_test\'] = self.value_test\n        config[\'nb_steps\'] = self.nb_steps\n        config[\'inner_policy\'] = get_object_config(self.inner_policy)\n        return config\n\nclass SoftmaxPolicy(Policy):\n    """""" Implement softmax policy for multinimial distribution\n\n    Simple Policy\n\n    - takes action according to the pobability distribution\n\n    """"""\n    def select_action(self, nb_actions, probs):\n        """"""Return the selected action\n\n        # Arguments\n            probs (np.ndarray) : Probabilty for each action\n\n        # Returns\n            action\n\n        """"""\n        action = np.random.choice(range(nb_actions), p=probs)\n        return action\n\nclass EpsGreedyQPolicy(Policy):\n    """"""Implement the epsilon greedy policy\n\n    Eps Greedy policy either:\n\n    - takes a random action with probability epsilon\n    - takes current best action with prob (1 - epsilon)\n    """"""\n    def __init__(self, eps=.1):\n        super(EpsGreedyQPolicy, self).__init__()\n        self.eps = eps\n\n    def select_action(self, q_values):\n        """"""Return the selected action\n\n        # Arguments\n            q_values (np.ndarray): List of the estimations of Q for each action\n\n        # Returns\n            Selection action\n        """"""\n        assert q_values.ndim == 1\n        nb_actions = q_values.shape[0]\n\n        if np.random.uniform() < self.eps:\n            action = np.random.randint(0, nb_actions)\n        else:\n            action = np.argmax(q_values)\n        return action\n\n    def get_config(self):\n        """"""Return configurations of EpsGreedyQPolicy\n\n        # Returns\n            Dict of config\n        """"""\n        config = super(EpsGreedyQPolicy, self).get_config()\n        config[\'eps\'] = self.eps\n        return config\n\n\nclass GreedyQPolicy(Policy):\n    """"""Implement the greedy policy\n\n    Greedy policy returns the current best action according to q_values\n    """"""\n    def select_action(self, q_values):\n        """"""Return the selected action\n\n        # Arguments\n            q_values (np.ndarray): List of the estimations of Q for each action\n\n        # Returns\n            Selection action\n        """"""\n        assert q_values.ndim == 1\n        action = np.argmax(q_values)\n        return action\n\n\nclass BoltzmannQPolicy(Policy):\n    """"""Implement the Boltzmann Q Policy\n\n    Boltzmann Q Policy builds a probability law on q values and returns\n    an action selected randomly according to this law.\n    """"""\n    def __init__(self, tau=1., clip=(-500., 500.)):\n        super(BoltzmannQPolicy, self).__init__()\n        self.tau = tau\n        self.clip = clip\n\n    def select_action(self, q_values):\n        """"""Return the selected action\n\n        # Arguments\n            q_values (np.ndarray): List of the estimations of Q for each action\n\n        # Returns\n            Selection action\n        """"""\n        assert q_values.ndim == 1\n        q_values = q_values.astype(\'float64\')\n        nb_actions = q_values.shape[0]\n\n        exp_values = np.exp(np.clip(q_values / self.tau, self.clip[0], self.clip[1]))\n        probs = exp_values / np.sum(exp_values)\n        action = np.random.choice(range(nb_actions), p=probs)\n        return action\n\n    def get_config(self):\n        """"""Return configurations of BoltzmannQPolicy\n\n        # Returns\n            Dict of config\n        """"""\n        config = super(BoltzmannQPolicy, self).get_config()\n        config[\'tau\'] = self.tau\n        config[\'clip\'] = self.clip\n        return config\n\n\nclass MaxBoltzmannQPolicy(Policy):\n    """"""\n    A combination of the eps-greedy and Boltzman q-policy.\n\n    Wiering, M.: Explorations in Efficient Reinforcement Learning.\n    PhD thesis, University of Amsterdam, Amsterdam (1999)\n\n    https://pure.uva.nl/ws/files/3153478/8461_UBA003000033.pdf\n    """"""\n    def __init__(self, eps=.1, tau=1., clip=(-500., 500.)):\n        super(MaxBoltzmannQPolicy, self).__init__()\n        self.eps = eps\n        self.tau = tau\n        self.clip = clip\n\n    def select_action(self, q_values):\n        """"""Return the selected action\n        The selected action follows the BoltzmannQPolicy with probability epsilon\n        or return the Greedy Policy with probability (1 - epsilon)\n\n        # Arguments\n            q_values (np.ndarray): List of the estimations of Q for each action\n\n        # Returns\n            Selection action\n        """"""\n        assert q_values.ndim == 1\n        q_values = q_values.astype(\'float64\')\n        nb_actions = q_values.shape[0]\n\n        if np.random.uniform() < self.eps:\n            exp_values = np.exp(np.clip(q_values / self.tau, self.clip[0], self.clip[1]))\n            probs = exp_values / np.sum(exp_values)\n            action = np.random.choice(range(nb_actions), p=probs)\n        else:\n            action = np.argmax(q_values)\n        return action\n\n    def get_config(self):\n        """"""Return configurations of MaxBoltzmannQPolicy\n\n        # Returns\n            Dict of config\n        """"""\n        config = super(MaxBoltzmannQPolicy, self).get_config()\n        config[\'eps\'] = self.eps\n        config[\'tau\'] = self.tau\n        config[\'clip\'] = self.clip\n        return config\n\n\nclass BoltzmannGumbelQPolicy(Policy):\n    """"""Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n    based on the paper Boltzmann Exploration Done Right\n    (https://arxiv.org/pdf/1705.10257.pdf).\n\n    BGE is invariant with respect to the mean of the rewards but not their\n    variance. The parameter C, which defaults to 1, can be used to correct for\n    this, and should be set to the least upper bound on the standard deviation\n    of the rewards.\n\n    BGE is only available for training, not testing. For testing purposes, you\n    can achieve approximately the same result as BGE after training for N steps\n    on K actions with parameter C by using the BoltzmannQPolicy and setting\n    tau = C/sqrt(N/K).""""""\n\n    def __init__(self, C=1.0):\n        assert C > 0, ""BoltzmannGumbelQPolicy C parameter must be > 0, not "" + repr(C)\n        super(BoltzmannGumbelQPolicy, self).__init__()\n        self.C = C\n        self.action_counts = None\n\n    def select_action(self, q_values):\n        """"""Return the selected action\n\n        # Arguments\n            q_values (np.ndarray): List of the estimations of Q for each action\n\n        # Returns\n            Selection action\n        """"""\n        # We can\'t use BGE during testing, since we don\'t have access to the\n        # action_counts at the end of training.\n        assert self.agent.training, ""BoltzmannGumbelQPolicy should only be used for training, not testing""\n\n        assert q_values.ndim == 1, q_values.ndim\n        q_values = q_values.astype(\'float64\')\n\n        # If we are starting training, we should reset the action_counts.\n        # Otherwise, action_counts should already be initialized, since we\n        # always do so when we begin training.\n        if self.agent.step == 0:\n            self.action_counts = np.ones(q_values.shape)\n        assert self.action_counts is not None, self.agent.step\n        assert self.action_counts.shape == q_values.shape, (self.action_counts.shape, q_values.shape)\n\n        beta = self.C/np.sqrt(self.action_counts)\n        Z = np.random.gumbel(size=q_values.shape)\n\n        perturbation = beta * Z\n        perturbed_q_values = q_values + perturbation\n        action = np.argmax(perturbed_q_values)\n\n        self.action_counts[action] += 1\n        return action\n\n    def get_config(self):\n        """"""Return configurations of BoltzmannGumbelQPolicy\n\n        # Returns\n            Dict of config\n        """"""\n        config = super(BoltzmannGumbelQPolicy, self).get_config()\n        config[\'C\'] = self.C\n        return config\n'"
rl/processors.py,0,"b'import numpy as np\n\nfrom rl.core import Processor\nfrom rl.util import WhiteningNormalizer\n\n\nclass MultiInputProcessor(Processor):\n    """"""Converts observations from an environment with multiple observations for use in a neural network\n    policy.\n\n    In some cases, you have environments that return multiple different observations per timestep \n    (in a robotics context, for example, a camera may be used to view the scene and a joint encoder may\n    be used to report the angles for each joint). Usually, this can be handled by a policy that has\n    multiple inputs, one for each modality. However, observations are returned by the environment\n    in the form of a tuple `[(modality1_t, modality2_t, ..., modalityn_t) for t in T]` but the neural network\n    expects them in per-modality batches like so: `[[modality1_1, ..., modality1_T], ..., [[modalityn_1, ..., modalityn_T]]`.\n    This processor converts observations appropriate for this use case.\n\n    # Arguments\n        nb_inputs (integer): The number of inputs, that is different modalities, to be used.\n            Your neural network that you use for the policy must have a corresponding number of\n            inputs.\n    """"""\n    def __init__(self, nb_inputs):\n        self.nb_inputs = nb_inputs\n\n    def process_state_batch(self, state_batch):\n        input_batches = [[] for x in range(self.nb_inputs)]\n        for state in state_batch:\n            processed_state = [[] for x in range(self.nb_inputs)]\n            for observation in state:\n                assert len(observation) == self.nb_inputs\n                for o, s in zip(observation, processed_state):\n                    s.append(o)\n            for idx, s in enumerate(processed_state):\n                input_batches[idx].append(s)\n        return [np.array(x) for x in input_batches]\n\n\nclass WhiteningNormalizerProcessor(Processor):\n    """"""Normalizes the observations to have zero mean and standard deviation of one,\n    i.e. it applies whitening to the inputs.\n\n    This typically helps significantly with learning, especially if different dimensions are\n    on different scales. However, it complicates training in the sense that you will have to store\n    these weights alongside the policy if you intend to load it later. It is the responsibility of\n    the user to do so.\n    """"""\n    def __init__(self):\n        self.normalizer = None\n\n    def process_state_batch(self, batch):\n        if self.normalizer is None:\n            self.normalizer = WhiteningNormalizer(shape=batch.shape[1:], dtype=batch.dtype)\n        self.normalizer.update(batch)\n        return self.normalizer.normalize(batch)\n'"
rl/random.py,0,"b'from __future__ import division\nimport numpy as np\n\n\nclass RandomProcess(object):\n    def reset_states(self):\n        pass\n\n\nclass AnnealedGaussianProcess(RandomProcess):\n    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n        self.mu = mu\n        self.sigma = sigma\n        self.n_steps = 0\n\n        if sigma_min is not None:\n            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n            self.c = sigma\n            self.sigma_min = sigma_min\n        else:\n            self.m = 0.\n            self.c = sigma\n            self.sigma_min = sigma\n\n    @property\n    def current_sigma(self):\n        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n        return sigma\n\n\nclass GaussianWhiteNoiseProcess(AnnealedGaussianProcess):\n    def __init__(self, mu=0., sigma=1., sigma_min=None, n_steps_annealing=1000, size=1):\n        super(GaussianWhiteNoiseProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n        self.size = size\n\n    def sample(self):\n        sample = np.random.normal(self.mu, self.current_sigma, self.size)\n        self.n_steps += 1\n        return sample\n\n# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\nclass OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, size=1, sigma_min=None, n_steps_annealing=1000):\n        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n        self.theta = theta\n        self.mu = mu\n        self.dt = dt\n        self.size = size\n        self.reset_states()\n\n    def sample(self):\n        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n        self.x_prev = x\n        self.n_steps += 1\n        return x\n\n    def reset_states(self):\n        self.x_prev = np.random.normal(self.mu,self.current_sigma,self.size)\n'"
rl/util.py,2,"b'import numpy as np\n\nfrom keras.models import model_from_config, Sequential, Model, model_from_config\nimport keras.optimizers as optimizers\nimport keras.backend as K\n\n\ndef clone_model(model, custom_objects={}):\n    # Requires Keras 1.0.7 since get_config has breaking changes.\n    config = {\n        \'class_name\': model.__class__.__name__,\n        \'config\': model.get_config(),\n    }\n    clone = model_from_config(config, custom_objects=custom_objects)\n    clone.set_weights(model.get_weights())\n    return clone\n\n\ndef clone_optimizer(optimizer):\n    if type(optimizer) is str:\n        return optimizers.get(optimizer)\n    # Requires Keras 1.0.7 since get_config has breaking changes.\n    params = dict([(k, v) for k, v in optimizer.get_config().items()])\n    config = {\n        \'class_name\': optimizer.__class__.__name__,\n        \'config\': params,\n    }\n    if hasattr(optimizers, \'optimizer_from_config\'):\n        # COMPATIBILITY: Keras < 2.0\n        clone = optimizers.optimizer_from_config(config)\n    else:\n        clone = optimizers.deserialize(config)\n    return clone\n\n\ndef get_soft_target_model_updates(target, source, tau):\n    target_weights = target.trainable_weights + sum([l.non_trainable_weights for l in target.layers], [])\n    source_weights = source.trainable_weights + sum([l.non_trainable_weights for l in source.layers], [])\n    assert len(target_weights) == len(source_weights)\n\n    # Create updates.\n    updates = []\n    for tw, sw in zip(target_weights, source_weights):\n        updates.append((tw, tau * sw + (1. - tau) * tw))\n    return updates\n\n\ndef get_object_config(o):\n    if o is None:\n        return None\n\n    config = {\n        \'class_name\': o.__class__.__name__,\n        \'config\': o.get_config()\n    }\n    return config\n\n\ndef huber_loss(y_true, y_pred, clip_value):\n    # Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n    # https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n    # for details.\n    assert clip_value > 0.\n\n    x = y_true - y_pred\n    if np.isinf(clip_value):\n        # Spacial case for infinity since Tensorflow does have problems\n        # if we compare `K.abs(x) < np.inf`.\n        return .5 * K.square(x)\n\n    condition = K.abs(x) < clip_value\n    squared_loss = .5 * K.square(x)\n    linear_loss = clip_value * (K.abs(x) - .5 * clip_value)\n    if K.backend() == \'tensorflow\':\n        import tensorflow as tf\n        if hasattr(tf, \'select\'):\n            return tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n        else:\n            return tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n    elif K.backend() == \'theano\':\n        from theano import tensor as T\n        return T.switch(condition, squared_loss, linear_loss)\n    else:\n        raise RuntimeError(\'Unknown backend ""{}"".\'.format(K.backend()))\n\n\nclass AdditionalUpdatesOptimizer(optimizers.Optimizer):\n    def __init__(self, optimizer, additional_updates):\n        super(AdditionalUpdatesOptimizer, self).__init__()\n        self.optimizer = optimizer\n        self.additional_updates = additional_updates\n\n    def get_updates(self, params, loss):\n        updates = self.optimizer.get_updates(params=params, loss=loss)\n        updates += self.additional_updates\n        self.updates = updates\n        return self.updates\n\n    def get_config(self):\n        return self.optimizer.get_config()\n\n\n# Based on https://github.com/openai/baselines/blob/master/baselines/common/mpi_running_mean_std.py\nclass WhiteningNormalizer(object):\n    def __init__(self, shape, eps=1e-2, dtype=np.float64):\n        self.eps = eps\n        self.shape = shape\n        self.dtype = dtype\n\n        self._sum = np.zeros(shape, dtype=dtype)\n        self._sumsq = np.zeros(shape, dtype=dtype)\n        self._count = 0\n\n        self.mean = np.zeros(shape, dtype=dtype)\n        self.std = np.ones(shape, dtype=dtype)\n\n    def normalize(self, x):\n        return (x - self.mean) / self.std\n\n    def denormalize(self, x):\n        return self.std * x + self.mean\n\n    def update(self, x):\n        if x.ndim == len(self.shape):\n            x = x.reshape(-1, *self.shape)\n        assert x.shape[1:] == self.shape\n\n        self._count += x.shape[0]\n        self._sum += np.sum(x, axis=0)\n        self._sumsq += np.sum(np.square(x), axis=0)\n\n        self.mean = self._sum / float(self._count)\n        self.std = np.sqrt(np.maximum(np.square(self.eps), self._sumsq / float(self._count) - np.square(self.mean)))\n'"
tests/__init__.py,0,b''
utils/__init__.py,0,b''
rl/agents/__init__.py,0,"b'from __future__ import absolute_import\nfrom .dqn import DQNAgent, NAFAgent, ContinuousDQNAgent\nfrom .ddpg import DDPGAgent\nfrom .cem import CEMAgent\nfrom .sarsa import SarsaAgent, SARSAAgent\n'"
rl/agents/cem.py,0,"b'from __future__ import division\nfrom collections import deque\nfrom copy import deepcopy\n\nimport numpy as np\nimport keras.backend as K\nfrom keras.models import Model\n\nfrom rl.core import Agent\nfrom rl.util import *\n\nclass CEMAgent(Agent):\n    """"""Write me\n    """"""\n    def __init__(self, model, nb_actions, memory, batch_size=50, nb_steps_warmup=1000,\n                 train_interval=50, elite_frac=0.05, memory_interval=1, theta_init=None,\n                 noise_decay_const=0.0, noise_ampl=0.0, **kwargs):\n        super(CEMAgent, self).__init__(**kwargs)\n\n        # Parameters.\n        self.nb_actions = nb_actions\n        self.batch_size = batch_size\n        self.elite_frac = elite_frac\n        self.num_best = int(self.batch_size * self.elite_frac)\n        self.nb_steps_warmup = nb_steps_warmup\n        self.train_interval = train_interval\n        self.memory_interval = memory_interval\n        \n        # if using noisy CEM, the minimum standard deviation will be ampl * exp (- decay_const * step )\n        self.noise_decay_const = noise_decay_const\n        self.noise_ampl = noise_ampl\n                \n        # default initial mean & cov, override this by passing an theta_init argument\n        self.init_mean = 0.0\n        self.init_stdev = 1.0\n        \n        # Related objects.\n        self.memory = memory\n        self.model = model\n        self.shapes = [w.shape for w in model.get_weights()]\n        self.sizes = [w.size for w in model.get_weights()]\n        self.num_weights = sum(self.sizes)\n        \n        # store the best result seen during training, as a tuple (reward, flat_weights)\n        self.best_seen = (-np.inf, np.zeros(self.num_weights))\n\n        self.theta = np.zeros(self.num_weights*2)\n        self.update_theta(theta_init)\n\n        # State.\n        self.episode = 0\n        self.compiled = False\n        self.reset_states()\n\n    def compile(self):\n        self.model.compile(optimizer=\'sgd\', loss=\'mse\')\n        self.compiled = True\n\n    def load_weights(self, filepath):\n        self.model.load_weights(filepath)\n\n    def save_weights(self, filepath, overwrite=False):\n        self.model.save_weights(filepath, overwrite=overwrite)\n\n    def get_weights_flat(self,weights):\n        weights_flat = np.zeros(self.num_weights)\n\n        pos = 0\n        for i_layer, size in enumerate(self.sizes):\n            weights_flat[pos:pos+size] = weights[i_layer].flatten()\n            pos += size\n        return weights_flat\n        \n    def get_weights_list(self,weights_flat):\n        weights = []\n        pos = 0\n        for i_layer, size in enumerate(self.sizes):\n            arr = weights_flat[pos:pos+size].reshape(self.shapes[i_layer])\n            weights.append(arr)\n            pos += size\n        return weights          \n\n    def reset_states(self):\n        self.recent_observation = None\n        self.recent_action = None\n\n    def select_action(self, state, stochastic=False):\n        batch = np.array([state])\n        if self.processor is not None:\n            batch = self.processor.process_state_batch(batch)\n\n        action = self.model.predict_on_batch(batch).flatten()\n        if stochastic or self.training:\n            return np.random.choice(np.arange(self.nb_actions), p=np.exp(action) / np.sum(np.exp(action)))\n        return np.argmax(action)\n    \n    def update_theta(self,theta):\n        if (theta is not None):\n            assert theta.shape == self.theta.shape, ""Invalid theta, shape is {0} but should be {1}"".format(theta.shape,self.theta.shape)\n            assert (not np.isnan(theta).any()), ""Invalid theta, NaN encountered""\n            assert (theta[self.num_weights:] >= 0.).all(), ""Invalid theta, standard deviations must be nonnegative""            \n            self.theta = theta\n        else:\n            means = np.ones(self.num_weights) * self.init_mean\n            stdevs = np.ones(self.num_weights) * self.init_stdev\n            self.theta = np.hstack((means,stdevs))\n\n    def choose_weights(self):\n        mean = self.theta[:self.num_weights]\n        std = self.theta[self.num_weights:]\n        weights_flat = std * np.random.randn(self.num_weights) + mean\n\n        sampled_weights = self.get_weights_list(weights_flat)\n        self.model.set_weights(sampled_weights)\n\n    def forward(self, observation):\n        # Select an action.\n        state = self.memory.get_recent_state(observation)\n        action = self.select_action(state)\n\n        # Book-keeping.\n        self.recent_observation = observation\n        self.recent_action = action\n\n        return action\n\n    @property\n    def layers(self):\n        return self.model.layers[:]\n         \n    def backward(self, reward, terminal):\n        # Store most recent experience in memory.\n        if self.step % self.memory_interval == 0:\n            self.memory.append(self.recent_observation, self.recent_action, reward, terminal,\n                               training=self.training)\n\n        metrics = [np.nan for _ in self.metrics_names]\n        if not self.training:\n            # We\'re done here. No need to update the experience memory since we only use the working\n            # memory to obtain the state over the most recent observations.\n            return metrics\n\n        if terminal:\n            params = self.get_weights_flat(self.model.get_weights())\n            self.memory.finalize_episode(params)\n\n            if self.step > self.nb_steps_warmup and self.episode % self.train_interval == 0:\n                params, reward_totals = self.memory.sample(self.batch_size)\n                best_idx = np.argsort(np.array(reward_totals))[-self.num_best:]\n                best = np.vstack([params[i] for i in best_idx])\n\n                if reward_totals[best_idx[-1]] > self.best_seen[0]:\n                    self.best_seen = (reward_totals[best_idx[-1]], params[best_idx[-1]])\n                    \n                metrics = [np.mean(np.array(reward_totals)[best_idx])]\n                if self.processor is not None:\n                    metrics += self.processor.metrics\n                min_std = self.noise_ampl * np.exp(-self.step * self.noise_decay_const)\n                \n                mean = np.mean(best, axis=0)\n                std = np.std(best, axis=0) + min_std\n                new_theta = np.hstack((mean, std))\n                self.update_theta(new_theta)\n            self.choose_weights()\n            self.episode += 1\n        return metrics\n\n    def _on_train_end(self):\n        self.model.set_weights(self.get_weights_list(self.best_seen[1]))\n\n    @property\n    def metrics_names(self):\n        names = [\'mean_best_reward\']\n        if self.processor is not None:\n            names += self.processor.metrics_names[:]\n        return names\n'"
rl/agents/ddpg.py,0,"b'from __future__ import division\nfrom collections import deque\nimport os\nimport warnings\n\nimport numpy as np\nimport keras.backend as K\nimport keras.optimizers as optimizers\n\nfrom rl.core import Agent\nfrom rl.random import OrnsteinUhlenbeckProcess\nfrom rl.util import *\n\n\ndef mean_q(y_true, y_pred):\n    return K.mean(K.max(y_pred, axis=-1))\n\n\n# Deep DPG as described by Lillicrap et al. (2015)\n# http://arxiv.org/pdf/1509.02971v2.pdf\n# http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.646.4324&rep=rep1&type=pdf\nclass DDPGAgent(Agent):\n    """"""Write me\n    """"""\n    def __init__(self, nb_actions, actor, critic, critic_action_input, memory,\n                 gamma=.99, batch_size=32, nb_steps_warmup_critic=1000, nb_steps_warmup_actor=1000,\n                 train_interval=1, memory_interval=1, delta_range=None, delta_clip=np.inf,\n                 random_process=None, custom_model_objects={}, target_model_update=.001, **kwargs):\n        if hasattr(actor.output, \'__len__\') and len(actor.output) > 1:\n            raise ValueError(\'Actor ""{}"" has more than one output. DDPG expects an actor that has a single output.\'.format(actor))\n        if hasattr(critic.output, \'__len__\') and len(critic.output) > 1:\n            raise ValueError(\'Critic ""{}"" has more than one output. DDPG expects a critic that has a single output.\'.format(critic))\n        if critic_action_input not in critic.input:\n            raise ValueError(\'Critic ""{}"" does not have designated action input ""{}"".\'.format(critic, critic_action_input))\n        if not hasattr(critic.input, \'__len__\') or len(critic.input) < 2:\n            raise ValueError(\'Critic ""{}"" does not have enough inputs. The critic must have at exactly two inputs, one for the action and one for the observation.\'.format(critic))\n\n        super(DDPGAgent, self).__init__(**kwargs)\n\n        # Soft vs hard target model updates.\n        if target_model_update < 0:\n            raise ValueError(\'`target_model_update` must be >= 0.\')\n        elif target_model_update >= 1:\n            # Hard update every `target_model_update` steps.\n            target_model_update = int(target_model_update)\n        else:\n            # Soft update with `(1 - target_model_update) * old + target_model_update * new`.\n            target_model_update = float(target_model_update)\n\n        if delta_range is not None:\n            warnings.warn(\'`delta_range` is deprecated. Please use `delta_clip` instead, which takes a single scalar. For now we\\\'re falling back to `delta_range[1] = {}`\'.format(delta_range[1]))\n            delta_clip = delta_range[1]\n\n        # Parameters.\n        self.nb_actions = nb_actions\n        self.nb_steps_warmup_actor = nb_steps_warmup_actor\n        self.nb_steps_warmup_critic = nb_steps_warmup_critic\n        self.random_process = random_process\n        self.delta_clip = delta_clip\n        self.gamma = gamma\n        self.target_model_update = target_model_update\n        self.batch_size = batch_size\n        self.train_interval = train_interval\n        self.memory_interval = memory_interval\n        self.custom_model_objects = custom_model_objects\n\n        # Related objects.\n        self.actor = actor\n        self.critic = critic\n        self.critic_action_input = critic_action_input\n        self.critic_action_input_idx = self.critic.input.index(critic_action_input)\n        self.memory = memory\n\n        # State.\n        self.compiled = False\n        self.reset_states()\n\n    @property\n    def uses_learning_phase(self):\n        return self.actor.uses_learning_phase or self.critic.uses_learning_phase\n\n    def compile(self, optimizer, metrics=[]):\n        metrics += [mean_q]\n\n        if type(optimizer) in (list, tuple):\n            if len(optimizer) != 2:\n                raise ValueError(\'More than two optimizers provided. Please only provide a maximum of two optimizers, the first one for the actor and the second one for the critic.\')\n            actor_optimizer, critic_optimizer = optimizer\n        else:\n            actor_optimizer = optimizer\n            critic_optimizer = clone_optimizer(optimizer)\n        if type(actor_optimizer) is str:\n            actor_optimizer = optimizers.get(actor_optimizer)\n        if type(critic_optimizer) is str:\n            critic_optimizer = optimizers.get(critic_optimizer)\n        assert actor_optimizer != critic_optimizer\n\n        if len(metrics) == 2 and hasattr(metrics[0], \'__len__\') and hasattr(metrics[1], \'__len__\'):\n            actor_metrics, critic_metrics = metrics\n        else:\n            actor_metrics = critic_metrics = metrics\n\n        def clipped_error(y_true, y_pred):\n            return K.mean(huber_loss(y_true, y_pred, self.delta_clip), axis=-1)\n\n        # Compile target networks. We only use them in feed-forward mode, hence we can pass any\n        # optimizer and loss since we never use it anyway.\n        self.target_actor = clone_model(self.actor, self.custom_model_objects)\n        self.target_actor.compile(optimizer=\'sgd\', loss=\'mse\')\n        self.target_critic = clone_model(self.critic, self.custom_model_objects)\n        self.target_critic.compile(optimizer=\'sgd\', loss=\'mse\')\n\n        # We also compile the actor. We never optimize the actor using Keras but instead compute\n        # the policy gradient ourselves. However, we need the actor in feed-forward mode, hence\n        # we also compile it with any optimzer and\n        self.actor.compile(optimizer=\'sgd\', loss=\'mse\')\n\n        # Compile the critic.\n        if self.target_model_update < 1.:\n            # We use the `AdditionalUpdatesOptimizer` to efficiently soft-update the target model.\n            critic_updates = get_soft_target_model_updates(self.target_critic, self.critic, self.target_model_update)\n            critic_optimizer = AdditionalUpdatesOptimizer(critic_optimizer, critic_updates)\n        self.critic.compile(optimizer=critic_optimizer, loss=clipped_error, metrics=critic_metrics)\n\n        # Combine actor and critic so that we can get the policy gradient.\n        # Assuming critic\'s state inputs are the same as actor\'s.\n        combined_inputs = []\n        state_inputs = []\n        for i in self.critic.input:\n            if i == self.critic_action_input:\n                combined_inputs.append([])\n            else:\n                combined_inputs.append(i)\n                state_inputs.append(i)\n        combined_inputs[self.critic_action_input_idx] = self.actor(state_inputs)\n\n        combined_output = self.critic(combined_inputs)\n\n        updates = actor_optimizer.get_updates(\n            params=self.actor.trainable_weights, loss=-K.mean(combined_output))\n        if self.target_model_update < 1.:\n            # Include soft target model updates.\n            updates += get_soft_target_model_updates(self.target_actor, self.actor, self.target_model_update)\n        updates += self.actor.updates  # include other updates of the actor, e.g. for BN\n\n        # Finally, combine it all into a callable function.\n        if K.backend() == \'tensorflow\':\n            self.actor_train_fn = K.function(state_inputs + [K.learning_phase()],\n                                             [self.actor(state_inputs)], updates=updates)\n        else:\n            if self.uses_learning_phase:\n                state_inputs += [K.learning_phase()]\n            self.actor_train_fn = K.function(state_inputs, [self.actor(state_inputs)], updates=updates)\n        self.actor_optimizer = actor_optimizer\n\n        self.compiled = True\n\n    def load_weights(self, filepath):\n        filename, extension = os.path.splitext(filepath)\n        actor_filepath = filename + \'_actor\' + extension\n        critic_filepath = filename + \'_critic\' + extension\n        self.actor.load_weights(actor_filepath)\n        self.critic.load_weights(critic_filepath)\n        self.update_target_models_hard()\n\n    def save_weights(self, filepath, overwrite=False):\n        filename, extension = os.path.splitext(filepath)\n        actor_filepath = filename + \'_actor\' + extension\n        critic_filepath = filename + \'_critic\' + extension\n        self.actor.save_weights(actor_filepath, overwrite=overwrite)\n        self.critic.save_weights(critic_filepath, overwrite=overwrite)\n\n    def update_target_models_hard(self):\n        self.target_critic.set_weights(self.critic.get_weights())\n        self.target_actor.set_weights(self.actor.get_weights())\n\n    # TODO: implement pickle\n\n    def reset_states(self):\n        if self.random_process is not None:\n            self.random_process.reset_states()\n        self.recent_action = None\n        self.recent_observation = None\n        if self.compiled:\n            self.actor.reset_states()\n            self.critic.reset_states()\n            self.target_actor.reset_states()\n            self.target_critic.reset_states()\n\n    def process_state_batch(self, batch):\n        batch = np.array(batch)\n        if self.processor is None:\n            return batch\n        return self.processor.process_state_batch(batch)\n\n    def select_action(self, state):\n        batch = self.process_state_batch([state])\n        action = self.actor.predict_on_batch(batch).flatten()\n        assert action.shape == (self.nb_actions,)\n\n        # Apply noise, if a random process is set.\n        if self.training and self.random_process is not None:\n            noise = self.random_process.sample()\n            assert noise.shape == action.shape\n            action += noise\n\n        return action\n\n    def forward(self, observation):\n        # Select an action.\n        state = self.memory.get_recent_state(observation)\n        action = self.select_action(state)  # TODO: move this into policy\n\n        # Book-keeping.\n        self.recent_observation = observation\n        self.recent_action = action\n\n        return action\n\n    @property\n    def layers(self):\n        return self.actor.layers[:] + self.critic.layers[:]\n\n    @property\n    def metrics_names(self):\n        names = self.critic.metrics_names[:]\n        if self.processor is not None:\n            names += self.processor.metrics_names[:]\n        return names\n\n    def backward(self, reward, terminal=False):\n        # Store most recent experience in memory.\n        if self.step % self.memory_interval == 0:\n            self.memory.append(self.recent_observation, self.recent_action, reward, terminal,\n                               training=self.training)\n\n        metrics = [np.nan for _ in self.metrics_names]\n        if not self.training:\n            # We\'re done here. No need to update the experience memory since we only use the working\n            # memory to obtain the state over the most recent observations.\n            return metrics\n\n        # Train the network on a single stochastic batch.\n        can_train_either = self.step > self.nb_steps_warmup_critic or self.step > self.nb_steps_warmup_actor\n        if can_train_either and self.step % self.train_interval == 0:\n            experiences = self.memory.sample(self.batch_size)\n            assert len(experiences) == self.batch_size\n\n            # Start by extracting the necessary parameters (we use a vectorized implementation).\n            state0_batch = []\n            reward_batch = []\n            action_batch = []\n            terminal1_batch = []\n            state1_batch = []\n            for e in experiences:\n                state0_batch.append(e.state0)\n                state1_batch.append(e.state1)\n                reward_batch.append(e.reward)\n                action_batch.append(e.action)\n                terminal1_batch.append(0. if e.terminal1 else 1.)\n\n            # Prepare and validate parameters.\n            state0_batch = self.process_state_batch(state0_batch)\n            state1_batch = self.process_state_batch(state1_batch)\n            terminal1_batch = np.array(terminal1_batch)\n            reward_batch = np.array(reward_batch)\n            action_batch = np.array(action_batch)\n            assert reward_batch.shape == (self.batch_size,)\n            assert terminal1_batch.shape == reward_batch.shape\n            assert action_batch.shape == (self.batch_size, self.nb_actions)\n\n            # Update critic, if warm up is over.\n            if self.step > self.nb_steps_warmup_critic:\n                target_actions = self.target_actor.predict_on_batch(state1_batch)\n                assert target_actions.shape == (self.batch_size, self.nb_actions)\n                if len(self.critic.inputs) >= 3:\n                    state1_batch_with_action = state1_batch[:]\n                else:\n                    state1_batch_with_action = [state1_batch]\n                state1_batch_with_action.insert(self.critic_action_input_idx, target_actions)\n                target_q_values = self.target_critic.predict_on_batch(state1_batch_with_action).flatten()\n                assert target_q_values.shape == (self.batch_size,)\n\n                # Compute r_t + gamma * max_a Q(s_t+1, a) and update the target ys accordingly,\n                # but only for the affected output units (as given by action_batch).\n                discounted_reward_batch = self.gamma * target_q_values\n                discounted_reward_batch *= terminal1_batch\n                assert discounted_reward_batch.shape == reward_batch.shape\n                targets = (reward_batch + discounted_reward_batch).reshape(self.batch_size, 1)\n\n                # Perform a single batch update on the critic network.\n                if len(self.critic.inputs) >= 3:\n                    state0_batch_with_action = state0_batch[:]\n                else:\n                    state0_batch_with_action = [state0_batch]\n                state0_batch_with_action.insert(self.critic_action_input_idx, action_batch)\n                metrics = self.critic.train_on_batch(state0_batch_with_action, targets)\n                if self.processor is not None:\n                    metrics += self.processor.metrics\n\n            # Update actor, if warm up is over.\n            if self.step > self.nb_steps_warmup_actor:\n                # TODO: implement metrics for actor\n                if len(self.actor.inputs) >= 2:\n                    inputs = state0_batch[:]\n                else:\n                    inputs = [state0_batch]\n                if self.uses_learning_phase:\n                    inputs += [self.training]\n                action_values = self.actor_train_fn(inputs)[0]\n                assert action_values.shape == (self.batch_size, self.nb_actions)\n\n        if self.target_model_update >= 1 and self.step % self.target_model_update == 0:\n            self.update_target_models_hard()\n\n        return metrics\n'"
rl/agents/dqn.py,13,"b'from __future__ import division\nimport warnings\n\nimport keras.backend as K\nfrom keras.models import Model\nfrom keras.layers import Lambda, Input, Layer, Dense\n\nfrom rl.core import Agent\nfrom rl.policy import EpsGreedyQPolicy, GreedyQPolicy\nfrom rl.util import *\n\n\ndef mean_q(y_true, y_pred):\n    return K.mean(K.max(y_pred, axis=-1))\n\n\nclass AbstractDQNAgent(Agent):\n    """"""Write me\n    """"""\n    def __init__(self, nb_actions, memory, gamma=.99, batch_size=32, nb_steps_warmup=1000,\n                 train_interval=1, memory_interval=1, target_model_update=10000,\n                 delta_range=None, delta_clip=np.inf, custom_model_objects={}, **kwargs):\n        super(AbstractDQNAgent, self).__init__(**kwargs)\n\n        # Soft vs hard target model updates.\n        if target_model_update < 0:\n            raise ValueError(\'`target_model_update` must be >= 0.\')\n        elif target_model_update >= 1:\n            # Hard update every `target_model_update` steps.\n            target_model_update = int(target_model_update)\n        else:\n            # Soft update with `(1 - target_model_update) * old + target_model_update * new`.\n            target_model_update = float(target_model_update)\n\n        if delta_range is not None:\n            warnings.warn(\'`delta_range` is deprecated. Please use `delta_clip` instead, which takes a single scalar. For now we\\\'re falling back to `delta_range[1] = {}`\'.format(delta_range[1]))\n            delta_clip = delta_range[1]\n\n        # Parameters.\n        self.nb_actions = nb_actions\n        self.gamma = gamma\n        self.batch_size = batch_size\n        self.nb_steps_warmup = nb_steps_warmup\n        self.train_interval = train_interval\n        self.memory_interval = memory_interval\n        self.target_model_update = target_model_update\n        self.delta_clip = delta_clip\n        self.custom_model_objects = custom_model_objects\n\n        # Related objects.\n        self.memory = memory\n\n        # State.\n        self.compiled = False\n\n    def process_state_batch(self, batch):\n        batch = np.array(batch)\n        if self.processor is None:\n            return batch\n        return self.processor.process_state_batch(batch)\n\n    def compute_batch_q_values(self, state_batch):\n        batch = self.process_state_batch(state_batch)\n        q_values = self.model.predict_on_batch(batch)\n        assert q_values.shape == (len(state_batch), self.nb_actions)\n        return q_values\n\n    def compute_q_values(self, state):\n        q_values = self.compute_batch_q_values([state]).flatten()\n        assert q_values.shape == (self.nb_actions,)\n        return q_values\n\n    def get_config(self):\n        return {\n            \'nb_actions\': self.nb_actions,\n            \'gamma\': self.gamma,\n            \'batch_size\': self.batch_size,\n            \'nb_steps_warmup\': self.nb_steps_warmup,\n            \'train_interval\': self.train_interval,\n            \'memory_interval\': self.memory_interval,\n            \'target_model_update\': self.target_model_update,\n            \'delta_clip\': self.delta_clip,\n            \'memory\': get_object_config(self.memory),\n        }\n\n# An implementation of the DQN agent as described in Mnih (2013) and Mnih (2015).\n# http://arxiv.org/pdf/1312.5602.pdf\n# http://arxiv.org/abs/1509.06461\nclass DQNAgent(AbstractDQNAgent):\n    """"""\n    # Arguments\n        model__: A Keras model.\n        policy__: A Keras-rl policy that are defined in [policy](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py).\n        test_policy__: A Keras-rl policy.\n        enable_double_dqn__: A boolean which enable target network as a second network proposed by van Hasselt et al. to decrease overfitting.\n        enable_dueling_dqn__: A boolean which enable dueling architecture proposed by Mnih et al.\n        dueling_type__: If `enable_dueling_dqn` is set to `True`, a type of dueling architecture must be chosen which calculate Q(s,a) from V(s) and A(s,a) differently. Note that `avg` is recommanded in the [paper](https://arxiv.org/abs/1511.06581).\n            `avg`: Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-Avg_a(A(s,a;theta)))\n            `max`: Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-max_a(A(s,a;theta)))\n            `naive`: Q(s,a;theta) = V(s;theta) + A(s,a;theta)\n\n    """"""\n    def __init__(self, model, policy=None, test_policy=None, enable_double_dqn=False, enable_dueling_network=False,\n                 dueling_type=\'avg\', *args, **kwargs):\n        super(DQNAgent, self).__init__(*args, **kwargs)\n\n        # Validate (important) input.\n        if hasattr(model.output, \'__len__\') and len(model.output) > 1:\n            raise ValueError(\'Model ""{}"" has more than one output. DQN expects a model that has a single output.\'.format(model))\n        if model.output._keras_shape != (None, self.nb_actions):\n            raise ValueError(\'Model output ""{}"" has invalid shape. DQN expects a model that has one dimension for each action, in this case {}.\'.format(model.output, self.nb_actions))\n\n        # Parameters.\n        self.enable_double_dqn = enable_double_dqn\n        self.enable_dueling_network = enable_dueling_network\n        self.dueling_type = dueling_type\n        if self.enable_dueling_network:\n            # get the second last layer of the model, abandon the last layer\n            layer = model.layers[-2]\n            nb_action = model.output._keras_shape[-1]\n            # layer y has a shape (nb_action+1,)\n            # y[:,0] represents V(s;theta)\n            # y[:,1:] represents A(s,a;theta)\n            y = Dense(nb_action + 1, activation=\'linear\')(layer.output)\n            # caculate the Q(s,a;theta)\n            # dueling_type == \'avg\'\n            # Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-Avg_a(A(s,a;theta)))\n            # dueling_type == \'max\'\n            # Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-max_a(A(s,a;theta)))\n            # dueling_type == \'naive\'\n            # Q(s,a;theta) = V(s;theta) + A(s,a;theta)\n            if self.dueling_type == \'avg\':\n                outputlayer = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - K.mean(a[:, 1:], axis=1, keepdims=True), output_shape=(nb_action,))(y)\n            elif self.dueling_type == \'max\':\n                outputlayer = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - K.max(a[:, 1:], axis=1, keepdims=True), output_shape=(nb_action,))(y)\n            elif self.dueling_type == \'naive\':\n                outputlayer = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:], output_shape=(nb_action,))(y)\n            else:\n                assert False, ""dueling_type must be one of {\'avg\',\'max\',\'naive\'}""\n\n            model = Model(inputs=model.input, outputs=outputlayer)\n\n        # Related objects.\n        self.model = model\n        if policy is None:\n            policy = EpsGreedyQPolicy()\n        if test_policy is None:\n            test_policy = GreedyQPolicy()\n        self.policy = policy\n        self.test_policy = test_policy\n\n        # State.\n        self.reset_states()\n\n    def get_config(self):\n        config = super(DQNAgent, self).get_config()\n        config[\'enable_double_dqn\'] = self.enable_double_dqn\n        config[\'dueling_type\'] = self.dueling_type\n        config[\'enable_dueling_network\'] = self.enable_dueling_network\n        config[\'model\'] = get_object_config(self.model)\n        config[\'policy\'] = get_object_config(self.policy)\n        config[\'test_policy\'] = get_object_config(self.test_policy)\n        if self.compiled:\n            config[\'target_model\'] = get_object_config(self.target_model)\n        return config\n\n    def compile(self, optimizer, metrics=[]):\n        metrics += [mean_q]  # register default metrics\n\n        # We never train the target model, hence we can set the optimizer and loss arbitrarily.\n        self.target_model = clone_model(self.model, self.custom_model_objects)\n        self.target_model.compile(optimizer=\'sgd\', loss=\'mse\')\n        self.model.compile(optimizer=\'sgd\', loss=\'mse\')\n\n        # Compile model.\n        if self.target_model_update < 1.:\n            # We use the `AdditionalUpdatesOptimizer` to efficiently soft-update the target model.\n            updates = get_soft_target_model_updates(self.target_model, self.model, self.target_model_update)\n            optimizer = AdditionalUpdatesOptimizer(optimizer, updates)\n\n        def clipped_masked_error(args):\n            y_true, y_pred, mask = args\n            loss = huber_loss(y_true, y_pred, self.delta_clip)\n            loss *= mask  # apply element-wise mask\n            return K.sum(loss, axis=-1)\n\n        # Create trainable model. The problem is that we need to mask the output since we only\n        # ever want to update the Q values for a certain action. The way we achieve this is by\n        # using a custom Lambda layer that computes the loss. This gives us the necessary flexibility\n        # to mask out certain parameters by passing in multiple inputs to the Lambda layer.\n        y_pred = self.model.output\n        y_true = Input(name=\'y_true\', shape=(self.nb_actions,))\n        mask = Input(name=\'mask\', shape=(self.nb_actions,))\n        loss_out = Lambda(clipped_masked_error, output_shape=(1,), name=\'loss\')([y_true, y_pred, mask])\n        ins = [self.model.input] if type(self.model.input) is not list else self.model.input\n        trainable_model = Model(inputs=ins + [y_true, mask], outputs=[loss_out, y_pred])\n        assert len(trainable_model.output_names) == 2\n        combined_metrics = {trainable_model.output_names[1]: metrics}\n        losses = [\n            lambda y_true, y_pred: y_pred,  # loss is computed in Lambda layer\n            lambda y_true, y_pred: K.zeros_like(y_pred),  # we only include this for the metrics\n        ]\n        trainable_model.compile(optimizer=optimizer, loss=losses, metrics=combined_metrics)\n        self.trainable_model = trainable_model\n\n        self.compiled = True\n\n    def load_weights(self, filepath):\n        self.model.load_weights(filepath)\n        self.update_target_model_hard()\n\n    def save_weights(self, filepath, overwrite=False):\n        self.model.save_weights(filepath, overwrite=overwrite)\n\n    def reset_states(self):\n        self.recent_action = None\n        self.recent_observation = None\n        if self.compiled:\n            self.model.reset_states()\n            self.target_model.reset_states()\n\n    def update_target_model_hard(self):\n        self.target_model.set_weights(self.model.get_weights())\n\n    def forward(self, observation):\n        # Select an action.\n        state = self.memory.get_recent_state(observation)\n        q_values = self.compute_q_values(state)\n        if self.training:\n            action = self.policy.select_action(q_values=q_values)\n        else:\n            action = self.test_policy.select_action(q_values=q_values)\n\n        # Book-keeping.\n        self.recent_observation = observation\n        self.recent_action = action\n\n        return action\n\n    def backward(self, reward, terminal):\n        # Store most recent experience in memory.\n        if self.step % self.memory_interval == 0:\n            self.memory.append(self.recent_observation, self.recent_action, reward, terminal,\n                               training=self.training)\n\n        metrics = [np.nan for _ in self.metrics_names]\n        if not self.training:\n            # We\'re done here. No need to update the experience memory since we only use the working\n            # memory to obtain the state over the most recent observations.\n            return metrics\n\n        # Train the network on a single stochastic batch.\n        if self.step > self.nb_steps_warmup and self.step % self.train_interval == 0:\n            experiences = self.memory.sample(self.batch_size)\n            assert len(experiences) == self.batch_size\n\n            # Start by extracting the necessary parameters (we use a vectorized implementation).\n            state0_batch = []\n            reward_batch = []\n            action_batch = []\n            terminal1_batch = []\n            state1_batch = []\n            for e in experiences:\n                state0_batch.append(e.state0)\n                state1_batch.append(e.state1)\n                reward_batch.append(e.reward)\n                action_batch.append(e.action)\n                terminal1_batch.append(0. if e.terminal1 else 1.)\n\n            # Prepare and validate parameters.\n            state0_batch = self.process_state_batch(state0_batch)\n            state1_batch = self.process_state_batch(state1_batch)\n            terminal1_batch = np.array(terminal1_batch)\n            reward_batch = np.array(reward_batch)\n            assert reward_batch.shape == (self.batch_size,)\n            assert terminal1_batch.shape == reward_batch.shape\n            assert len(action_batch) == len(reward_batch)\n\n            # Compute Q values for mini-batch update.\n            if self.enable_double_dqn:\n                # According to the paper ""Deep Reinforcement Learning with Double Q-learning""\n                # (van Hasselt et al., 2015), in Double DQN, the online network predicts the actions\n                # while the target network is used to estimate the Q value.\n                q_values = self.model.predict_on_batch(state1_batch)\n                assert q_values.shape == (self.batch_size, self.nb_actions)\n                actions = np.argmax(q_values, axis=1)\n                assert actions.shape == (self.batch_size,)\n\n                # Now, estimate Q values using the target network but select the values with the\n                # highest Q value wrt to the online model (as computed above).\n                target_q_values = self.target_model.predict_on_batch(state1_batch)\n                assert target_q_values.shape == (self.batch_size, self.nb_actions)\n                q_batch = target_q_values[range(self.batch_size), actions]\n            else:\n                # Compute the q_values given state1, and extract the maximum for each sample in the batch.\n                # We perform this prediction on the target_model instead of the model for reasons\n                # outlined in Mnih (2015). In short: it makes the algorithm more stable.\n                target_q_values = self.target_model.predict_on_batch(state1_batch)\n                assert target_q_values.shape == (self.batch_size, self.nb_actions)\n                q_batch = np.max(target_q_values, axis=1).flatten()\n            assert q_batch.shape == (self.batch_size,)\n\n            targets = np.zeros((self.batch_size, self.nb_actions))\n            dummy_targets = np.zeros((self.batch_size,))\n            masks = np.zeros((self.batch_size, self.nb_actions))\n\n            # Compute r_t + gamma * max_a Q(s_t+1, a) and update the target targets accordingly,\n            # but only for the affected output units (as given by action_batch).\n            discounted_reward_batch = self.gamma * q_batch\n            # Set discounted reward to zero for all states that were terminal.\n            discounted_reward_batch *= terminal1_batch\n            assert discounted_reward_batch.shape == reward_batch.shape\n            Rs = reward_batch + discounted_reward_batch\n            for idx, (target, mask, R, action) in enumerate(zip(targets, masks, Rs, action_batch)):\n                target[action] = R  # update action with estimated accumulated reward\n                dummy_targets[idx] = R\n                mask[action] = 1.  # enable loss for this specific action\n            targets = np.array(targets).astype(\'float32\')\n            masks = np.array(masks).astype(\'float32\')\n\n            # Finally, perform a single update on the entire batch. We use a dummy target since\n            # the actual loss is computed in a Lambda layer that needs more complex input. However,\n            # it is still useful to know the actual target to compute metrics properly.\n            ins = [state0_batch] if type(self.model.input) is not list else state0_batch\n            metrics = self.trainable_model.train_on_batch(ins + [targets, masks], [dummy_targets, targets])\n            metrics = [metric for idx, metric in enumerate(metrics) if idx not in (1, 2)]  # throw away individual losses\n            metrics += self.policy.metrics\n            if self.processor is not None:\n                metrics += self.processor.metrics\n\n        if self.target_model_update >= 1 and self.step % self.target_model_update == 0:\n            self.update_target_model_hard()\n\n        return metrics\n\n    @property\n    def layers(self):\n        return self.model.layers[:]\n\n    @property\n    def metrics_names(self):\n        # Throw away individual losses and replace output name since this is hidden from the user.\n        assert len(self.trainable_model.output_names) == 2\n        dummy_output_name = self.trainable_model.output_names[1]\n        model_metrics = [name for idx, name in enumerate(self.trainable_model.metrics_names) if idx not in (1, 2)]\n        model_metrics = [name.replace(dummy_output_name + \'_\', \'\') for name in model_metrics]\n\n        names = model_metrics + self.policy.metrics_names[:]\n        if self.processor is not None:\n            names += self.processor.metrics_names[:]\n        return names\n\n    @property\n    def policy(self):\n        return self.__policy\n\n    @policy.setter\n    def policy(self, policy):\n        self.__policy = policy\n        self.__policy._set_agent(self)\n\n    @property\n    def test_policy(self):\n        return self.__test_policy\n\n    @test_policy.setter\n    def test_policy(self, policy):\n        self.__test_policy = policy\n        self.__test_policy._set_agent(self)\n\n\nclass NAFLayer(Layer):\n    """"""Write me\n    """"""\n    def __init__(self, nb_actions, mode=\'full\', **kwargs):\n        if mode not in (\'full\', \'diag\'):\n            raise RuntimeError(\'Unknown mode ""{}"" in NAFLayer.\'.format(self.mode))\n\n        self.nb_actions = nb_actions\n        self.mode = mode\n        super(NAFLayer, self).__init__(**kwargs)\n\n    def call(self, x, mask=None):\n        # TODO: validate input shape\n\n        assert (len(x) == 3)\n        L_flat = x[0]\n        mu = x[1]\n        a = x[2]\n\n        if self.mode == \'full\':\n            # Create L and L^T matrix, which we use to construct the positive-definite matrix P.\n            L = None\n            LT = None\n            if K.backend() == \'theano\':\n                import theano.tensor as T\n                import theano\n\n                def fn(x, L_acc, LT_acc):\n                    x_ = K.zeros((self.nb_actions, self.nb_actions))\n                    x_ = T.set_subtensor(x_[np.tril_indices(self.nb_actions)], x)\n                    diag = K.exp(T.diag(x_)) + K.epsilon()\n                    x_ = T.set_subtensor(x_[np.diag_indices(self.nb_actions)], diag)\n                    return x_, x_.T\n\n                outputs_info = [\n                    K.zeros((self.nb_actions, self.nb_actions)),\n                    K.zeros((self.nb_actions, self.nb_actions)),\n                ]\n                results, _ = theano.scan(fn=fn, sequences=L_flat, outputs_info=outputs_info)\n                L, LT = results\n            elif K.backend() == \'tensorflow\':\n                import tensorflow as tf\n\n                # Number of elements in a triangular matrix.\n                nb_elems = (self.nb_actions * self.nb_actions + self.nb_actions) // 2\n\n                # Create mask for the diagonal elements in L_flat. This is used to exponentiate\n                # only the diagonal elements, which is done before gathering.\n                diag_indeces = [0]\n                for row in range(1, self.nb_actions):\n                    diag_indeces.append(diag_indeces[-1] + (row + 1))\n                diag_mask = np.zeros(1 + nb_elems)  # +1 for the leading zero\n                diag_mask[np.array(diag_indeces) + 1] = 1\n                diag_mask = K.variable(diag_mask)\n\n                # Add leading zero element to each element in the L_flat. We use this zero\n                # element when gathering L_flat into a lower triangular matrix L.\n                nb_rows = tf.shape(L_flat)[0]\n                zeros = tf.expand_dims(tf.tile(K.zeros((1,)), [nb_rows]), 1)\n                try:\n                    # Old TF behavior.\n                    L_flat = tf.concat(1, [zeros, L_flat])\n                except (TypeError, ValueError):\n                    # New TF behavior\n                    L_flat = tf.concat([zeros, L_flat], 1)\n\n                # Create mask that can be used to gather elements from L_flat and put them\n                # into a lower triangular matrix.\n                tril_mask = np.zeros((self.nb_actions, self.nb_actions), dtype=\'int32\')\n                tril_mask[np.tril_indices(self.nb_actions)] = range(1, nb_elems + 1)\n\n                # Finally, process each element of the batch.\n                init = [\n                    K.zeros((self.nb_actions, self.nb_actions)),\n                    K.zeros((self.nb_actions, self.nb_actions)),\n                ]\n\n                def fn(a, x):\n                    # Exponentiate everything. This is much easier than only exponentiating\n                    # the diagonal elements, and, usually, the action space is relatively low.\n                    x_ = K.exp(x) + K.epsilon()\n                    # Only keep the diagonal elements.\n                    x_ *= diag_mask\n                    # Add the original, non-diagonal elements.\n                    x_ += x * (1. - diag_mask)\n                    # Finally, gather everything into a lower triangular matrix.\n                    L_ = tf.gather(x_, tril_mask)\n                    return [L_, tf.transpose(L_)]\n\n                tmp = tf.scan(fn, L_flat, initializer=init)\n                if isinstance(tmp, (list, tuple)):\n                    # TensorFlow 0.10 now returns a tuple of tensors.\n                    L, LT = tmp\n                else:\n                    # Old TensorFlow < 0.10 returns a shared tensor.\n                    L = tmp[:, 0, :, :]\n                    LT = tmp[:, 1, :, :]\n            else:\n                raise RuntimeError(\'Unknown Keras backend ""{}"".\'.format(K.backend()))\n            assert L is not None\n            assert LT is not None\n            P = K.batch_dot(L, LT)\n        elif self.mode == \'diag\':\n            if K.backend() == \'theano\':\n                import theano.tensor as T\n                import theano\n\n                def fn(x, P_acc):\n                    x_ = K.zeros((self.nb_actions, self.nb_actions))\n                    x_ = T.set_subtensor(x_[np.diag_indices(self.nb_actions)], x)\n                    return x_\n\n                outputs_info = [\n                    K.zeros((self.nb_actions, self.nb_actions)),\n                ]\n                P, _ = theano.scan(fn=fn, sequences=L_flat, outputs_info=outputs_info)\n            elif K.backend() == \'tensorflow\':\n                import tensorflow as tf\n\n                # Create mask that can be used to gather elements from L_flat and put them\n                # into a diagonal matrix.\n                diag_mask = np.zeros((self.nb_actions, self.nb_actions), dtype=\'int32\')\n                diag_mask[np.diag_indices(self.nb_actions)] = range(1, self.nb_actions + 1)\n\n                # Add leading zero element to each element in the L_flat. We use this zero\n                # element when gathering L_flat into a lower triangular matrix L.\n                nb_rows = tf.shape(L_flat)[0]\n                zeros = tf.expand_dims(tf.tile(K.zeros((1,)), [nb_rows]), 1)\n                try:\n                    # Old TF behavior.\n                    L_flat = tf.concat(1, [zeros, L_flat])\n                except (TypeError, ValueError):\n                    # New TF behavior\n                    L_flat = tf.concat([zeros, L_flat], 1)\n\n                # Finally, process each element of the batch.\n                def fn(a, x):\n                    x_ = tf.gather(x, diag_mask)\n                    return x_\n\n                P = tf.scan(fn, L_flat, initializer=K.zeros((self.nb_actions, self.nb_actions)))\n            else:\n                raise RuntimeError(\'Unknown Keras backend ""{}"".\'.format(K.backend()))\n        assert P is not None\n        assert K.ndim(P) == 3\n\n        # Combine a, mu and P into a scalar (over the batches). What we compute here is\n        # -.5 * (a - mu)^T * P * (a - mu), where * denotes the dot-product. Unfortunately\n        # TensorFlow handles vector * P slightly suboptimal, hence we convert the vectors to\n        # 1xd/dx1 matrices and finally flatten the resulting 1x1 matrix into a scalar. All\n        # operations happen over the batch size, which is dimension 0.\n        prod = K.batch_dot(K.expand_dims(a - mu, 1), P)\n        prod = K.batch_dot(prod, K.expand_dims(a - mu, -1))\n        A = -.5 * K.batch_flatten(prod)\n        assert K.ndim(A) == 2\n        return A\n\n    def get_output_shape_for(self, input_shape):\n        return self.compute_output_shape(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        if len(input_shape) != 3:\n            raise RuntimeError(""Expects 3 inputs: L, mu, a"")\n        for i, shape in enumerate(input_shape):\n            if len(shape) != 2:\n                raise RuntimeError(""Input {} has {} dimensions but should have 2"".format(i, len(shape)))\n        assert self.mode in (\'full\',\'diag\')\n        if self.mode == \'full\':\n            expected_elements = (self.nb_actions * self.nb_actions + self.nb_actions) // 2\n        elif self.mode == \'diag\':\n            expected_elements = self.nb_actions\n        else:\n            expected_elements = None\n        assert expected_elements is not None\n        if input_shape[0][1] != expected_elements:\n            raise RuntimeError(""Input 0 (L) should have {} elements but has {}"".format(input_shape[0][1]))\n        if input_shape[1][1] != self.nb_actions:\n            raise RuntimeError(\n                ""Input 1 (mu) should have {} elements but has {}"".format(self.nb_actions, input_shape[1][1]))\n        if input_shape[2][1] != self.nb_actions:\n            raise RuntimeError(\n                ""Input 2 (action) should have {} elements but has {}"".format(self.nb_actions, input_shape[1][1]))\n        return input_shape[0][0], 1\n\n\nclass NAFAgent(AbstractDQNAgent):\n    """"""Write me\n    """"""\n    def __init__(self, V_model, L_model, mu_model, random_process=None,\n                 covariance_mode=\'full\', *args, **kwargs):\n        super(NAFAgent, self).__init__(*args, **kwargs)\n\n        # TODO: Validate (important) input.\n\n        # Parameters.\n        self.random_process = random_process\n        self.covariance_mode = covariance_mode\n\n        # Related objects.\n        self.V_model = V_model\n        self.L_model = L_model\n        self.mu_model = mu_model\n\n        # State.\n        self.reset_states()\n\n    def update_target_model_hard(self):\n        self.target_V_model.set_weights(self.V_model.get_weights())\n\n    def load_weights(self, filepath):\n        self.combined_model.load_weights(filepath)  # updates V, L and mu model since the weights are shared\n        self.update_target_model_hard()\n\n    def save_weights(self, filepath, overwrite=False):\n        self.combined_model.save_weights(filepath, overwrite=overwrite)\n\n    def reset_states(self):\n        if self.random_process is not None:\n            self.random_process.reset_states()\n        self.recent_action = None\n        self.recent_observation = None\n        if self.compiled:\n            self.combined_model.reset_states()\n            self.target_V_model.reset_states()\n\n    def compile(self, optimizer, metrics=[]):\n        metrics += [mean_q]  # register default metrics\n\n        # Create target V model. We don\'t need targets for mu or L.\n        self.target_V_model = clone_model(self.V_model, self.custom_model_objects)\n        self.target_V_model.compile(optimizer=\'sgd\', loss=\'mse\')\n\n        # Build combined model.\n        a_in = Input(shape=(self.nb_actions,), name=\'action_input\')\n        if type(self.V_model.input) is list:\n            observation_shapes = [i._keras_shape[1:] for i in self.V_model.input]\n        else:\n            observation_shapes = [self.V_model.input._keras_shape[1:]]\n        os_in = [Input(shape=shape, name=\'observation_input_{}\'.format(idx)) for idx, shape in enumerate(observation_shapes)]\n        L_out = self.L_model([a_in] + os_in)\n        V_out = self.V_model(os_in)\n\n        mu_out = self.mu_model(os_in)\n        A_out = NAFLayer(self.nb_actions, mode=self.covariance_mode)([L_out, mu_out, a_in])\n        combined_out = Lambda(lambda x: x[0]+x[1], output_shape=lambda x: x[0])([A_out, V_out])\n        combined = Model(inputs=[a_in] + os_in, outputs=[combined_out])\n        # Compile combined model.\n        if self.target_model_update < 1.:\n            # We use the `AdditionalUpdatesOptimizer` to efficiently soft-update the target model.\n            updates = get_soft_target_model_updates(self.target_V_model, self.V_model, self.target_model_update)\n            optimizer = AdditionalUpdatesOptimizer(optimizer, updates)\n\n        def clipped_error(y_true, y_pred):\n            return K.mean(huber_loss(y_true, y_pred, self.delta_clip), axis=-1)\n\n        combined.compile(loss=clipped_error, optimizer=optimizer, metrics=metrics)\n        self.combined_model = combined\n\n        self.compiled = True\n\n    def select_action(self, state):\n        batch = self.process_state_batch([state])\n        action = self.mu_model.predict_on_batch(batch).flatten()\n        assert action.shape == (self.nb_actions,)\n\n        # Apply noise, if a random process is set.\n        if self.training and self.random_process is not None:\n            noise = self.random_process.sample()\n            assert noise.shape == action.shape\n            action += noise\n\n        return action\n\n    def forward(self, observation):\n        # Select an action.\n        state = self.memory.get_recent_state(observation)\n        action = self.select_action(state)\n\n        # Book-keeping.\n        self.recent_observation = observation\n        self.recent_action = action\n\n        return action\n\n    def backward(self, reward, terminal):\n        # Store most recent experience in memory.\n        if self.step % self.memory_interval == 0:\n            self.memory.append(self.recent_observation, self.recent_action, reward, terminal,\n                               training=self.training)\n\n        metrics = [np.nan for _ in self.metrics_names]\n        if not self.training:\n            # We\'re done here. No need to update the experience memory since we only use the working\n            # memory to obtain the state over the most recent observations.\n            return metrics\n\n        # Train the network on a single stochastic batch.\n        if self.step > self.nb_steps_warmup and self.step % self.train_interval == 0:\n            experiences = self.memory.sample(self.batch_size)\n            assert len(experiences) == self.batch_size\n\n            # Start by extracting the necessary parameters (we use a vectorized implementation).\n            state0_batch = []\n            reward_batch = []\n            action_batch = []\n            terminal1_batch = []\n            state1_batch = []\n            for e in experiences:\n                state0_batch.append(e.state0)\n                state1_batch.append(e.state1)\n                reward_batch.append(e.reward)\n                action_batch.append(e.action)\n                terminal1_batch.append(0. if e.terminal1 else 1.)\n\n            # Prepare and validate parameters.\n            state0_batch = self.process_state_batch(state0_batch)\n            state1_batch = self.process_state_batch(state1_batch)\n            terminal1_batch = np.array(terminal1_batch)\n            reward_batch = np.array(reward_batch)\n            action_batch = np.array(action_batch)\n            assert reward_batch.shape == (self.batch_size,)\n            assert terminal1_batch.shape == reward_batch.shape\n            assert action_batch.shape == (self.batch_size, self.nb_actions)\n\n            # Compute Q values for mini-batch update.\n            q_batch = self.target_V_model.predict_on_batch(state1_batch).flatten()\n            assert q_batch.shape == (self.batch_size,)\n\n            # Compute discounted reward.\n            discounted_reward_batch = self.gamma * q_batch\n            # Set discounted reward to zero for all states that were terminal.\n            discounted_reward_batch *= terminal1_batch\n            assert discounted_reward_batch.shape == reward_batch.shape\n            Rs = reward_batch + discounted_reward_batch\n            assert Rs.shape == (self.batch_size,)\n\n            # Finally, perform a single update on the entire batch.\n            if len(self.combined_model.input) == 2:\n                metrics = self.combined_model.train_on_batch([action_batch, state0_batch], Rs)\n            else:\n                metrics = self.combined_model.train_on_batch([action_batch] + state0_batch, Rs)\n            if self.processor is not None:\n                metrics += self.processor.metrics\n\n        if self.target_model_update >= 1 and self.step % self.target_model_update == 0:\n            self.update_target_model_hard()\n\n        return metrics\n\n    @property\n    def layers(self):\n        return self.combined_model.layers[:]\n\n    def get_config(self):\n        config = super(NAFAgent, self).get_config()\n        config[\'V_model\'] = get_object_config(self.V_model)\n        config[\'mu_model\'] = get_object_config(self.mu_model)\n        config[\'L_model\'] = get_object_config(self.L_model)\n        if self.compiled:\n            config[\'target_V_model\'] = get_object_config(self.target_V_model)\n        return config\n\n    @property\n    def metrics_names(self):\n        names = self.combined_model.metrics_names[:]\n        if self.processor is not None:\n            names += self.processor.metrics_names[:]\n        return names\n\n\n# Aliases\nContinuousDQNAgent = NAFAgent\n'"
rl/agents/sarsa.py,0,"b'import collections\n\nimport numpy as np\n\nfrom keras.callbacks import History\nfrom keras.models import Model\nfrom keras.layers import Input, Lambda\nimport keras.backend as K\n\nfrom rl.core import Agent\nfrom rl.agents.dqn import mean_q\nfrom rl.util import huber_loss\nfrom rl.policy import EpsGreedyQPolicy, GreedyQPolicy\nfrom rl.util import get_object_config\n\n\nclass SARSAAgent(Agent):\n    """"""Write me\n    """"""\n    def __init__(self, model, nb_actions, policy=None, test_policy=None, gamma=.99, nb_steps_warmup=10,\n                 train_interval=1, delta_clip=np.inf, *args, **kwargs):\n        super(SarsaAgent, self).__init__(*args, **kwargs)\n\n        # Do not use defaults in constructor because that would mean that each instance shares the same\n        # policy.\n        if policy is None:\n            policy = EpsGreedyQPolicy()\n        if test_policy is None:\n            test_policy = GreedyQPolicy()\n\n        self.model = model\n        self.nb_actions = nb_actions\n        self.policy = policy\n        self.test_policy = test_policy\n        self.gamma = gamma\n        self.nb_steps_warmup = nb_steps_warmup\n        self.train_interval = train_interval\n\n        self.delta_clip = delta_clip\n        self.compiled = False\n        self.actions = None\n        self.observations = None\n        self.rewards = None\n\n    def compute_batch_q_values(self, state_batch):\n        batch = self.process_state_batch(state_batch)\n        q_values = self.model.predict_on_batch(batch)\n        assert q_values.shape == (len(state_batch), self.nb_actions)\n        return q_values\n\n    def compute_q_values(self, state):\n        q_values = self.compute_batch_q_values([state]).flatten()\n        assert q_values.shape == (self.nb_actions,)\n        return q_values\n\n    def process_state_batch(self, batch):\n        batch = np.array(batch)\n        if self.processor is None:\n            return batch\n        return self.processor.process_state_batch(batch)\n\n    def get_config(self):\n        config = super(SarsaAgent, self).get_config()\n        config[\'nb_actions\'] = self.nb_actions\n        config[\'gamma\'] = self.gamma\n        config[\'nb_steps_warmup\'] = self.nb_steps_warmup\n        config[\'train_interval\'] = self.train_interval\n        config[\'delta_clip\'] = self.delta_clip\n        config[\'model\'] = get_object_config(self.model)\n        config[\'policy\'] = get_object_config(self.policy)\n        config[\'test_policy\'] = get_object_config(self.test_policy)\n        return config\n\n    def compile(self, optimizer, metrics=[]):\n        metrics += [mean_q]  # register default metrics\n\n        def clipped_masked_error(args):\n            y_true, y_pred, mask = args\n            loss = huber_loss(y_true, y_pred, self.delta_clip)\n            loss *= mask  # apply element-wise mask\n            return K.sum(loss, axis=-1)\n\n        # Create trainable model. The problem is that we need to mask the output since we only\n        # ever want to update the Q values for a certain action. The way we achieve this is by\n        # using a custom Lambda layer that computes the loss. This gives us the necessary flexibility\n        # to mask out certain parameters by passing in multiple inputs to the Lambda layer.\n        y_pred = self.model.output\n        y_true = Input(name=\'y_true\', shape=(self.nb_actions,))\n        mask = Input(name=\'mask\', shape=(self.nb_actions,))\n        loss_out = Lambda(clipped_masked_error, output_shape=(1,), name=\'loss\')([y_pred, y_true, mask])\n        ins = [self.model.input] if type(self.model.input) is not list else self.model.input\n        trainable_model = Model(inputs=ins + [y_true, mask], outputs=[loss_out, y_pred])\n        assert len(trainable_model.output_names) == 2\n        combined_metrics = {trainable_model.output_names[1]: metrics}\n        losses = [\n            lambda y_true, y_pred: y_pred,  # loss is computed in Lambda layer\n            lambda y_true, y_pred: K.zeros_like(y_pred),  # we only include this for the metrics\n        ]\n        trainable_model.compile(optimizer=optimizer, loss=losses, metrics=combined_metrics)\n        self.trainable_model = trainable_model\n\n        self.compiled = True\n\n    def load_weights(self, filepath):\n        self.model.load_weights(filepath)\n\n    def save_weights(self, filepath, overwrite=False):\n        self.model.save_weights(filepath, overwrite=overwrite)\n\n    def reset_states(self):\n        self.actions = collections.deque(maxlen=2)\n        self.observations = collections.deque(maxlen=2)\n        self.rewards = collections.deque(maxlen=2)\n        if self.compiled:\n            self.model.reset_states()\n\n    def forward(self, observation):\n        # Select an action.\n        q_values = self.compute_q_values([observation])\n        if self.training:\n            action = self.policy.select_action(q_values=q_values)\n        else:\n            action = self.test_policy.select_action(q_values=q_values)\n\n        # Book-keeping.\n        self.observations.append(observation)\n        self.actions.append(action)\n\n        return action\n\n    def backward(self, reward, terminal):\n        metrics = [np.nan for _ in self.metrics_names]\n        if not self.training:\n            # We\'re done here. No need to update the experience memory since we only use the working\n            # memory to obtain the state over the most recent observations.\n            return metrics\n\n        # Train the network on a single stochastic batch.\n        if self.step > self.nb_steps_warmup and self.step % self.train_interval == 0:\n            # Start by extracting the necessary parameters (we use a vectorized implementation).\n            self.rewards.append(reward)\n            if len(self.observations) < 2:\n                return metrics  # not enough data yet\n\n            state0_batch = [self.observations[0]]\n            reward_batch = [self.rewards[0]]\n            action_batch = [self.actions[0]]\n            terminal1_batch = [0.] if terminal else [1.]\n            state1_batch = [self.observations[1]]\n            action1_batch = [self.actions[1]]\n\n            # Prepare and validate parameters.\n            state0_batch = self.process_state_batch(state0_batch)\n            state1_batch = self.process_state_batch(state1_batch)\n            terminal1_batch = np.array(terminal1_batch)\n            reward_batch = np.array(reward_batch)\n            assert reward_batch.shape == (1,)\n            assert terminal1_batch.shape == reward_batch.shape\n            assert len(action_batch) == len(reward_batch)\n\n            batch = self.process_state_batch(state1_batch)\n            q_values = self.compute_q_values(batch)\n            q_values = q_values.reshape((1, self.nb_actions))\n\n            q_batch = q_values[0, action1_batch]\n\n            assert q_batch.shape == (1,)\n            targets = np.zeros((1, self.nb_actions))\n            dummy_targets = np.zeros((1,))\n            masks = np.zeros((1, self.nb_actions))\n\n            # Compute r_t + gamma * Q(s_t+1, a_t+1)\n            discounted_reward_batch = self.gamma * q_batch\n            # Set discounted reward to zero for all states that were terminal.\n            discounted_reward_batch *= terminal1_batch\n            assert discounted_reward_batch.shape == reward_batch.shape\n            Rs = reward_batch + discounted_reward_batch\n            for idx, (target, mask, R, action) in enumerate(zip(targets, masks, Rs, action_batch)):\n                target[action] = R  # update action with estimated accumulated reward\n                dummy_targets[idx] = R\n                mask[action] = 1.  # enable loss for this specific action\n            targets = np.array(targets).astype(\'float32\')\n            masks = np.array(masks).astype(\'float32\')\n\n            # Finally, perform a single update on the entire batch. We use a dummy target since\n            # the actual loss is computed in a Lambda layer that needs more complex input. However,\n            # it is still useful to know the actual target to compute metrics properly.\n            state0_batch = state0_batch.reshape((1,) + state0_batch.shape)\n            ins = [state0_batch] if type(self.model.input) is not list else state0_batch\n            metrics = self.trainable_model.train_on_batch(ins + [targets, masks], [dummy_targets, targets])\n            metrics = [metric for idx, metric in enumerate(metrics) if idx not in (1, 2)]  # throw away individual losses\n            metrics += self.policy.metrics\n            if self.processor is not None:\n                metrics += self.processor.metrics\n        return metrics\n\n    @property\n    def layers(self):\n        return self.model.layers[:]\n\n    @property\n    def metrics_names(self):\n        # Throw away individual losses and replace output name since this is hidden from the user.\n        assert len(self.trainable_model.output_names) == 2\n        dummy_output_name = self.trainable_model.output_names[1]\n        model_metrics = [name for idx, name in enumerate(self.trainable_model.metrics_names) if idx not in (1, 2)]\n        model_metrics = [name.replace(dummy_output_name + \'_\', \'\') for name in model_metrics]\n\n        names = model_metrics + self.policy.metrics_names[:]\n        if self.processor is not None:\n            names += self.processor.metrics_names[:]\n        return names\n\n    @property\n    def policy(self):\n        return self.__policy\n\n    @policy.setter\n    def policy(self, policy):\n        self.__policy = policy\n        self.__policy._set_agent(self)\n\n    @property\n    def test_policy(self):\n        return self.__test_policy\n\n    @test_policy.setter\n    def test_policy(self, policy):\n        self.__test_policy = policy\n        self.__test_policy._set_agent(self)\n\n# Aliases\nSarsaAgent = SARSAAgent\n'"
rl/common/__init__.py,0,b'from .misc_util import *\n'
rl/common/cmd_util.py,0,"b'# Inspired from OpenAI Baselines\nimport gym\nfrom rl.common.vec_env.subproc_env_vec import SubprocVecEnv\nfrom rl.common import set_global_seeds\n\n\ndef make_gym_env(env_id, num_env=2, seed=123, wrapper_kwargs=None, start_index=0):\n    """"""\n    Create a wrapped, SubprocVecEnv for Gym Environments.\n    """"""\n    if wrapper_kwargs is None:\n        wrapper_kwargs = {}\n\n    def make_env(rank): # pylint: disable=C0111\n        def _thunk():\n            env = gym.make(env_id)\n            env.seed(seed + rank)\n            return env\n        return _thunk\n\n    set_global_seeds(seed)\n    return SubprocVecEnv([make_env(i + start_index) for i in range(num_env)])\n'"
rl/common/misc_util.py,0,b'# Inspired from OpenAI Baselines\n\nimport gym\nimport numpy as np\nimport random\n\ndef set_global_seeds(i):\n    np.random.seed(i)\n    random.seed(i)\n'
rl/common/tile_images.py,0,"b'import numpy as np\n\ndef tile_images(img_nhwc):\n    """"""\n    Tile N images into one big PxQ image\n    (P,Q) are chosen to be as close as possible, and if N\n    is square, then P=Q.\n    input: img_nhwc, list or array of images, ndim=4 once turned into array\n        n = batch index, h = height, w = width, c = channel\n    returns:\n        bigim_HWc, ndarray with ndim=3\n    """"""\n    img_nhwc = np.asarray(img_nhwc)\n    N, h, w, c = img_nhwc.shape\n    H = int(np.ceil(np.sqrt(N)))\n    W = int(np.ceil(float(N)/H))\n    img_nhwc = np.array(list(img_nhwc) + [img_nhwc[0]*0 for _ in range(N, H*W)])\n    img_HWhwc = img_nhwc.reshape(H, W, h, w, c)\n    img_HhWwc = img_HWhwc.transpose(0, 2, 1, 3, 4)\n    img_Hh_Ww_c = img_HhWwc.reshape(H*h, W*w, c)\n    return img_Hh_Ww_c\n'"
tests/integration/test_continuous.py,0,"b""import random\n\nimport numpy as np\nimport gym\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation, Flatten, Input, Concatenate\nfrom keras.optimizers import Adam\n\nfrom rl.agents import NAFAgent, DDPGAgent\nfrom rl.random import OrnsteinUhlenbeckProcess\nfrom rl.memory import SequentialMemory\n\n\ndef test_cdqn():\n    # TODO: replace this with a simpler environment where we can actually test if it finds a solution\n    env = gym.make('Pendulum-v0')\n    np.random.seed(123)\n    env.seed(123)\n    random.seed(123)\n    nb_actions = env.action_space.shape[0]\n\n    V_model = Sequential()\n    V_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n    V_model.add(Dense(16))\n    V_model.add(Activation('relu'))\n    V_model.add(Dense(1))\n\n    mu_model = Sequential()\n    mu_model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n    mu_model.add(Dense(16))\n    mu_model.add(Activation('relu'))\n    mu_model.add(Dense(nb_actions))\n    \n    action_input = Input(shape=(nb_actions,), name='action_input')\n    observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n    x = Concatenate()([action_input, Flatten()(observation_input)])\n    x = Dense(16)(x)\n    x = Activation('relu')(x)\n    x = Dense(((nb_actions * nb_actions + nb_actions) // 2))(x)\n    L_model = Model(inputs=[action_input, observation_input], outputs=x)\n\n    memory = SequentialMemory(limit=1000, window_length=1)\n    random_process = OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.3, size=nb_actions)\n    agent = NAFAgent(nb_actions=nb_actions, V_model=V_model, L_model=L_model, mu_model=mu_model,\n                     memory=memory, nb_steps_warmup=50, random_process=random_process,\n                     gamma=.99, target_model_update=1e-3)\n    agent.compile(Adam(lr=1e-3))\n\n    agent.fit(env, nb_steps=400, visualize=False, verbose=0, nb_max_episode_steps=100)\n    h = agent.test(env, nb_episodes=2, visualize=False, nb_max_episode_steps=100)\n    # TODO: evaluate history\n\n\ndef test_ddpg():\n    # TODO: replace this with a simpler environment where we can actually test if it finds a solution\n    env = gym.make('Pendulum-v0')\n    np.random.seed(123)\n    env.seed(123)\n    random.seed(123)\n    nb_actions = env.action_space.shape[0]\n\n    actor = Sequential()\n    actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n    actor.add(Dense(16))\n    actor.add(Activation('relu'))\n    actor.add(Dense(nb_actions))\n    actor.add(Activation('linear'))\n\n    action_input = Input(shape=(nb_actions,), name='action_input')\n    observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n    flattened_observation = Flatten()(observation_input)\n    x = Concatenate()([action_input, flattened_observation])\n    x = Dense(16)(x)\n    x = Activation('relu')(x)\n    x = Dense(1)(x)\n    x = Activation('linear')(x)\n    critic = Model(inputs=[action_input, observation_input], outputs=x)\n    \n    memory = SequentialMemory(limit=1000, window_length=1)\n    random_process = OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.3)\n    agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n                      memory=memory, nb_steps_warmup_critic=50, nb_steps_warmup_actor=50,\n                      random_process=random_process, gamma=.99, target_model_update=1e-3)\n    agent.compile([Adam(lr=1e-3), Adam(lr=1e-3)])\n\n    agent.fit(env, nb_steps=400, visualize=False, verbose=0, nb_max_episode_steps=100)\n    h = agent.test(env, nb_episodes=2, visualize=False, nb_max_episode_steps=100)\n    # TODO: evaluate history\n"""
tests/integration/test_discrete.py,0,"b""import random\n\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import Adam\nfrom rl.agents import DQNAgent, CEMAgent, SARSAAgent\nfrom rl.policy import EpsGreedyQPolicy\nfrom rl.memory import SequentialMemory, EpisodeParameterMemory\nfrom utils.gym.envs import TwoRoundDeterministicRewardEnv\n\n\ndef test_dqn():\n    env = TwoRoundDeterministicRewardEnv()\n    np.random.seed(123)\n    env.seed(123)\n    random.seed(123)\n    nb_actions = env.action_space.n\n\n    # Next, we build a very simple model.\n    model = Sequential()\n    model.add(Dense(16, input_shape=(1,)))\n    model.add(Activation('relu'))\n    model.add(Dense(nb_actions))\n    model.add(Activation('linear'))\n\n    memory = SequentialMemory(limit=1000, window_length=1)\n    policy = EpsGreedyQPolicy(eps=.1)\n    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=50,\n                   target_model_update=1e-1, policy=policy, enable_double_dqn=False)\n    dqn.compile(Adam(lr=1e-3))\n\n    dqn.fit(env, nb_steps=2000, visualize=False, verbose=0)\n    policy.eps = 0.\n    h = dqn.test(env, nb_episodes=20, visualize=False)\n    assert_allclose(np.mean(h.history['episode_reward']), 3.)\n\n\ndef test_double_dqn():\n    env = TwoRoundDeterministicRewardEnv()\n    np.random.seed(123)\n    env.seed(123)\n    random.seed(123)\n    nb_actions = env.action_space.n\n\n    # Next, we build a very simple model.\n    model = Sequential()\n    model.add(Dense(16, input_shape=(1,)))\n    model.add(Activation('relu'))\n    model.add(Dense(nb_actions))\n    model.add(Activation('linear'))\n\n    memory = SequentialMemory(limit=1000, window_length=1)\n    policy = EpsGreedyQPolicy(eps=.1)\n    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=50,\n                   target_model_update=1e-1, policy=policy, enable_double_dqn=True)\n    dqn.compile(Adam(lr=1e-3))\n\n    dqn.fit(env, nb_steps=2000, visualize=False, verbose=0)\n    policy.eps = 0.\n    h = dqn.test(env, nb_episodes=20, visualize=False)\n    assert_allclose(np.mean(h.history['episode_reward']), 3.)\n\n\ndef test_cem():\n    env = TwoRoundDeterministicRewardEnv()\n    np.random.seed(123)\n    env.seed(123)\n    random.seed(123)\n    nb_actions = env.action_space.n\n\n    # Next, we build a very simple model.\n    model = Sequential()\n    model.add(Dense(16, input_shape=(1,)))\n    model.add(Activation('relu'))\n    model.add(Dense(nb_actions))\n    model.add(Activation('linear'))\n\n    memory = EpisodeParameterMemory(limit=1000, window_length=1)\n    dqn = CEMAgent(model=model, nb_actions=nb_actions, memory=memory)\n    dqn.compile()\n\n    dqn.fit(env, nb_steps=2000, visualize=False, verbose=1)\n    h = dqn.test(env, nb_episodes=20, visualize=False)\n    assert_allclose(np.mean(h.history['episode_reward']), 3.)\n\n\ndef test_duel_dqn():\n    env = TwoRoundDeterministicRewardEnv()\n    np.random.seed(123)\n    env.seed(123)\n    random.seed(123)\n    nb_actions = env.action_space.n\n\n    # Next, we build a very simple model.\n    model = Sequential()\n    model.add(Dense(16, input_shape=(1,)))\n    model.add(Activation('relu'))\n    model.add(Dense(nb_actions, activation='linear'))\n\n    memory = SequentialMemory(limit=1000, window_length=1)\n    policy = EpsGreedyQPolicy(eps=.1)\n    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=50,\n                   target_model_update=1e-1, policy=policy, enable_double_dqn=False, enable_dueling_network=True)\n    dqn.compile(Adam(lr=1e-3))\n\n    dqn.fit(env, nb_steps=2000, visualize=False, verbose=0)\n    policy.eps = 0.\n    h = dqn.test(env, nb_episodes=20, visualize=False)\n    assert_allclose(np.mean(h.history['episode_reward']), 3.)\n\n\ndef test_sarsa():\n    env = TwoRoundDeterministicRewardEnv()\n    np.random.seed(123)\n    env.seed(123)\n    random.seed(123)\n    nb_actions = env.action_space.n\n\n    # Next, we build a very simple model.\n    model = Sequential()\n    model.add(Dense(16, input_shape=(1,)))\n    model.add(Activation('relu'))\n    model.add(Dense(nb_actions, activation='linear'))\n\n    policy = EpsGreedyQPolicy(eps=.1)\n    sarsa = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=50, policy=policy)\n    sarsa.compile(Adam(lr=1e-3))\n\n    sarsa.fit(env, nb_steps=20000, visualize=False, verbose=0)\n    policy.eps = 0.\n    h = sarsa.test(env, nb_episodes=20, visualize=False)\n    assert_allclose(np.mean(h.history['episode_reward']), 3.)\n"""
tests/rl/__init__.py,0,b''
tests/rl/test_core.py,0,"b""from __future__ import division\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom rl.memory import SequentialMemory\nfrom rl.core import Agent, Env, Processor\n\n\nclass TestEnv(Env):\n    def __init__(self):\n        super(TestEnv, self).__init__()\n\n    def step(self, action):\n        self.state += 1\n        done = self.state >= 6\n        reward = float(self.state) / 10.\n        return np.array(self.state), reward, done, {}\n\n    def reset(self):\n        self.state = 1\n        return np.array(self.state)\n\n    def seed(self, seed=None):\n        pass\n\n    def configure(self, *args, **kwargs):\n        pass\n\n\nclass TestAgent(Agent):\n    def __init__(self, memory, **kwargs):\n        super(TestAgent, self).__init__(**kwargs)\n        self.memory = memory\n\n    def forward(self, observation):\n        action = observation\n        self.recent_action = action\n        self.recent_observation = observation\n        return action\n\n    def backward(self, reward, terminal):\n        metrics = [np.nan for _ in self.metrics_names]\n        self.memory.append(self.recent_observation, self.recent_action, reward, terminal)\n        return metrics\n\n    def compile(self):\n        self.compiled = True\n\n\ndef test_fit_observations():\n    memory = SequentialMemory(100, window_length=2, ignore_episode_boundaries=False)\n    agent = TestAgent(memory)\n    env = TestEnv()\n    agent.compile()\n    agent.fit(env, 20, verbose=0)\n\n    # Inspect memory to see if observations are correct.\n    experiencies = memory.sample(batch_size=6, batch_idxs=range(2, 8))\n\n    assert experiencies[0].reward == .4\n    assert experiencies[0].action == 3\n    assert_allclose(experiencies[0].state0, np.array([2, 3]))\n    assert_allclose(experiencies[0].state1, np.array([3, 4]))\n    assert experiencies[0].terminal1 is False\n\n    assert experiencies[1].reward == .5\n    assert experiencies[1].action == 4\n    assert_allclose(experiencies[1].state0, np.array([3, 4]))\n    assert_allclose(experiencies[1].state1, np.array([4, 5]))\n    assert experiencies[1].terminal1 is False\n\n    assert experiencies[2].reward == .6\n    assert experiencies[2].action == 5\n    assert_allclose(experiencies[2].state0, np.array([4, 5]))\n    assert_allclose(experiencies[2].state1, np.array([5, 6]))\n    assert experiencies[2].terminal1 is True\n\n    # Experience 3 has been re-sampled since since state0 would be terminal in which case we\n    # cannot really have a meaningful transition because the environment gets reset. We thus\n    # just ensure that state0 is not terminal.\n    assert not np.all(experiencies[3].state0 == np.array([5, 6]))\n\n    assert experiencies[4].reward == .2\n    assert experiencies[4].action == 1\n    assert_allclose(experiencies[4].state0, np.array([0, 1]))\n    assert_allclose(experiencies[4].state1, np.array([1, 2]))\n    assert experiencies[4].terminal1 is False\n\n    assert experiencies[5].reward == .3\n    assert experiencies[5].action == 2\n    assert_allclose(experiencies[5].state0, np.array([1, 2]))\n    assert_allclose(experiencies[5].state1, np.array([2, 3]))\n    assert experiencies[5].terminal1 is False\n\n\ndef test_copy_observations():\n    methods = [\n        'fit',\n        'test',\n    ]\n\n    for method in methods:\n        original_observations = []\n\n        class LocalEnv(Env):\n            def __init__(self):\n                super(LocalEnv, self).__init__()\n\n            def step(self, action):\n                self.state += 1\n                done = self.state >= 6\n                reward = float(self.state) / 10.\n                obs = np.array(self.state)\n                original_observations.append(obs)\n                return obs, reward, done, {}\n\n            def reset(self):\n                self.state = 1\n                return np.array(self.state)\n\n            def seed(self, seed=None):\n                pass\n\n            def configure(self, *args, **kwargs):\n                pass\n\n        # Slight abuse of the processor for test purposes.\n        observations = []\n\n        class LocalProcessor(Processor):\n            def process_step(self, observation, reward, done, info):\n                observations.append(observation)\n                return observation, reward, done, info\n\n        processor = LocalProcessor()\n        memory = SequentialMemory(100, window_length=1)\n        agent = TestAgent(memory, processor=processor)\n        env = LocalEnv()\n        agent.compile()\n        getattr(agent, method)(env, 20, verbose=0, visualize=False)\n\n        assert len(observations) == len(original_observations)\n        assert_allclose(np.array(observations), np.array(original_observations))\n        assert np.all([o is not o_ for o, o_ in zip(original_observations, observations)])\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/rl/test_memory.py,0,"b""from __future__ import division\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom rl.memory import SequentialMemory, RingBuffer\n\n\ndef test_ring_buffer():\n    def assert_elements(b, ref):\n        assert len(b) == len(ref)\n        for idx in range(b.maxlen):\n            if idx >= len(ref):\n                with pytest.raises(KeyError):\n                    b[idx]\n            else:\n                assert b[idx] == ref[idx]\n\n    b = RingBuffer(5)\n\n    # Fill buffer.\n    assert_elements(b, [])\n    b.append(1)\n    assert_elements(b, [1])\n    b.append(2)\n    assert_elements(b, [1, 2])\n    b.append(3)\n    assert_elements(b, [1, 2, 3])\n    b.append(4)\n    assert_elements(b, [1, 2, 3, 4])\n    b.append(5)\n    assert_elements(b, [1, 2, 3, 4, 5])\n\n    # Add couple more items with buffer at limit.\n    b.append(6)\n    assert_elements(b, [2, 3, 4, 5, 6])\n    b.append(7)\n    assert_elements(b, [3, 4, 5, 6, 7])\n    b.append(8)\n    assert_elements(b, [4, 5, 6, 7, 8])\n\n\ndef test_get_recent_state_with_episode_boundaries():\n    memory = SequentialMemory(3, window_length=2, ignore_episode_boundaries=False)\n    obs_size = (3, 4)\n\n    obs0 = np.random.random(obs_size)\n    terminal0 = False\n\n    obs1 = np.random.random(obs_size)\n    terminal1 = False\n\n    obs2 = np.random.random(obs_size)\n    terminal2 = False\n\n    obs3 = np.random.random(obs_size)\n    terminal3 = True\n\n    obs4 = np.random.random(obs_size)\n    terminal4 = False\n\n    obs5 = np.random.random(obs_size)\n    terminal5 = True\n\n    obs6 = np.random.random(obs_size)\n    terminal6 = False\n\n    state = np.array(memory.get_recent_state(obs0))\n    assert state.shape == (2,) + obs_size\n    assert np.allclose(state[0], 0.)\n    assert np.all(state[1] == obs0)\n\n    # memory.append takes the current observation, the reward after taking an action and if\n    # the *new* observation is terminal, thus `obs0` and `terminal1` is correct.\n    memory.append(obs0, 0, 0., terminal1)\n    state = np.array(memory.get_recent_state(obs1))\n    assert state.shape == (2,) + obs_size\n    assert np.all(state[0] == obs0)\n    assert np.all(state[1] == obs1)\n\n    memory.append(obs1, 0, 0., terminal2)\n    state = np.array(memory.get_recent_state(obs2))\n    assert state.shape == (2,) + obs_size\n    assert np.all(state[0] == obs1)\n    assert np.all(state[1] == obs2)\n\n    memory.append(obs2, 0, 0., terminal3)\n    state = np.array(memory.get_recent_state(obs3))\n    assert state.shape == (2,) + obs_size\n    assert np.all(state[0] == obs2)\n    assert np.all(state[1] == obs3)\n\n    memory.append(obs3, 0, 0., terminal4)\n    state = np.array(memory.get_recent_state(obs4))\n    assert state.shape == (2,) + obs_size\n    assert np.all(state[0] == np.zeros(obs_size))\n    assert np.all(state[1] == obs4)\n\n    memory.append(obs4, 0, 0., terminal5)\n    state = np.array(memory.get_recent_state(obs5))\n    assert state.shape == (2,) + obs_size\n    assert np.all(state[0] == obs4)\n    assert np.all(state[1] == obs5)\n\n    memory.append(obs5, 0, 0., terminal6)\n    state = np.array(memory.get_recent_state(obs6))\n    assert state.shape == (2,) + obs_size\n    assert np.all(state[0] == np.zeros(obs_size))\n    assert np.all(state[1] == obs6)\n\n\ndef test_training_flag():\n    obs_size = (3, 4)\n\n    obs0 = np.random.random(obs_size)\n    terminal0 = False\n\n    obs1 = np.random.random(obs_size)\n    terminal1 = True\n\n    obs2 = np.random.random(obs_size)\n    terminal2 = False\n\n    for training in (True, False):\n        memory = SequentialMemory(3, window_length=2)\n\n        state = np.array(memory.get_recent_state(obs0))\n        assert state.shape == (2,) + obs_size\n        assert np.allclose(state[0], 0.)\n        assert np.all(state[1] == obs0)\n        assert memory.nb_entries == 0\n\n        memory.append(obs0, 0, 0., terminal1, training=training)\n        state = np.array(memory.get_recent_state(obs1))\n        assert state.shape == (2,) + obs_size\n        assert np.all(state[0] == obs0)\n        assert np.all(state[1] == obs1)\n        if training:\n            assert memory.nb_entries == 1\n        else:\n            assert memory.nb_entries == 0\n\n        memory.append(obs1, 0, 0., terminal2, training=training)\n        state = np.array(memory.get_recent_state(obs2))\n        assert state.shape == (2,) + obs_size\n        assert np.allclose(state[0], 0.)\n        assert np.all(state[1] == obs2)\n        if training:\n            assert memory.nb_entries == 2\n        else:\n            assert memory.nb_entries == 0\n\n\ndef test_get_recent_state_without_episode_boundaries():\n    memory = SequentialMemory(3, window_length=2, ignore_episode_boundaries=True)\n    obs_size = (3, 4)\n\n    obs0 = np.random.random(obs_size)\n    terminal0 = False\n\n    obs1 = np.random.random(obs_size)\n    terminal1 = False\n\n    obs2 = np.random.random(obs_size)\n    terminal2 = False\n\n    obs3 = np.random.random(obs_size)\n    terminal3 = True\n\n    obs4 = np.random.random(obs_size)\n    terminal4 = False\n\n    obs5 = np.random.random(obs_size)\n    terminal5 = True\n\n    obs6 = np.random.random(obs_size)\n    terminal6 = False\n\n    state = np.array(memory.get_recent_state(obs0))\n    assert state.shape == (2,) + obs_size\n    assert np.allclose(state[0], 0.)\n    assert np.all(state[1] == obs0)\n\n    # memory.append takes the current observation, the reward after taking an action and if\n    # the *new* observation is terminal, thus `obs0` and `terminal1` is correct.\n    memory.append(obs0, 0, 0., terminal1)\n    state = np.array(memory.get_recent_state(obs1))\n    assert state.shape == (2,) + obs_size\n    assert np.all(state[0] == obs0)\n    assert np.all(state[1] == obs1)\n\n    memory.append(obs1, 0, 0., terminal2)\n    state = np.array(memory.get_recent_state(obs2))\n    assert state.shape == (2,) + obs_size\n    assert np.all(state[0] == obs1)\n    assert np.all(state[1] == obs2)\n\n    memory.append(obs2, 0, 0., terminal3)\n    state = np.array(memory.get_recent_state(obs3))\n    assert state.shape == (2,) + obs_size\n    assert np.all(state[0] == obs2)\n    assert np.all(state[1] == obs3)\n\n    memory.append(obs3, 0, 0., terminal4)\n    state = np.array(memory.get_recent_state(obs4))\n    assert state.shape == (2,) + obs_size\n    assert np.all(state[0] == obs3)\n    assert np.all(state[1] == obs4)\n\n    memory.append(obs4, 0, 0., terminal5)\n    state = np.array(memory.get_recent_state(obs5))\n    assert state.shape == (2,) + obs_size\n    assert np.all(state[0] == obs4)\n    assert np.all(state[1] == obs5)\n\n    memory.append(obs5, 0, 0., terminal6)\n    state = np.array(memory.get_recent_state(obs6))\n    assert state.shape == (2,) + obs_size\n    assert np.all(state[0] == obs5)\n    assert np.all(state[1] == obs6)\n\n\ndef test_sampling():\n    memory = SequentialMemory(100, window_length=2, ignore_episode_boundaries=False)\n    obs_size = (3, 4)\n    actions = range(5)\n\n    obs0 = np.random.random(obs_size)\n    terminal0 = False\n    action0 = np.random.choice(actions)\n    reward0 = np.random.random()\n\n    obs1 = np.random.random(obs_size)\n    terminal1 = False\n    action1 = np.random.choice(actions)\n    reward1 = np.random.random()\n\n    obs2 = np.random.random(obs_size)\n    terminal2 = False\n    action2 = np.random.choice(actions)\n    reward2 = np.random.random()\n\n    obs3 = np.random.random(obs_size)\n    terminal3 = True\n    action3 = np.random.choice(actions)\n    reward3 = np.random.random()\n\n    obs4 = np.random.random(obs_size)\n    terminal4 = False\n    action4 = np.random.choice(actions)\n    reward4 = np.random.random()\n\n    obs5 = np.random.random(obs_size)\n    terminal5 = False\n    action5 = np.random.choice(actions)\n    reward5 = np.random.random()\n\n    obs6 = np.random.random(obs_size)\n    terminal6 = False\n    action6 = np.random.choice(actions)\n    reward6 = np.random.random()\n\n    # memory.append takes the current observation, the reward after taking an action and if\n    # the *new* observation is terminal, thus `obs0` and `terminal1` is correct.\n    memory.append(obs0, action0, reward0, terminal1)\n    memory.append(obs1, action1, reward1, terminal2)\n    memory.append(obs2, action2, reward2, terminal3)\n    memory.append(obs3, action3, reward3, terminal4)\n    memory.append(obs4, action4, reward4, terminal5)\n    memory.append(obs5, action5, reward5, terminal6)\n    assert memory.nb_entries == 6\n\n    experiences = memory.sample(batch_size=3, batch_idxs=[2, 3, 4])\n    assert len(experiences) == 3\n\n    assert_allclose(experiences[0].state0, np.array([obs1, obs2]))\n    assert_allclose(experiences[0].state1, np.array([obs2, obs3]))\n    assert experiences[0].action == action2\n    assert experiences[0].reward == reward2\n    assert experiences[0].terminal1 is True\n\n    # Next experience has been re-sampled since since state0 would be terminal in which case we\n    # cannot really have a meaningful transition because the environment gets reset. We thus\n    # just ensure that state0 is not terminal.\n    assert not np.all(experiences[1].state0 == np.array([obs2, obs3]))\n\n    assert_allclose(experiences[2].state0, np.array([np.zeros(obs_size), obs4]))\n    assert_allclose(experiences[2].state1, np.array([obs4, obs5]))\n    assert experiences[2].action == action4\n    assert experiences[2].reward == reward4\n    assert experiences[2].terminal1 is False\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/rl/test_util.py,0,"b""from __future__ import division\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Concatenate\nfrom keras.optimizers import SGD\nimport keras.backend as K\n\nfrom rl.util import clone_optimizer, clone_model, huber_loss, WhiteningNormalizer\n\n\ndef test_clone_sequential_model():\n    seq = Sequential()\n    seq.add(Dense(8, input_shape=(3,)))\n    seq.compile(optimizer='sgd', loss='mse')\n\n    clone = clone_model(seq)\n    clone.compile(optimizer='sgd', loss='mse')\n\n    ins = np.random.random((4, 3))\n    y_pred_seq = seq.predict_on_batch(ins)\n    y_pred_clone = clone.predict_on_batch(ins)\n    assert y_pred_seq.shape == y_pred_clone.shape\n    assert_allclose(y_pred_seq, y_pred_clone)\n\n\ndef test_clone_graph_model():\n    in1 = Input(shape=(2,))\n    in2 = Input(shape=(3,))\n    x = Dense(8)(Concatenate()([in1, in2]))\n    graph = Model([in1, in2], x)\n    graph.compile(optimizer='sgd', loss='mse')\n\n    clone = clone_model(graph)\n    clone.compile(optimizer='sgd', loss='mse')\n\n    ins = [np.random.random((4, 2)), np.random.random((4, 3))]\n    y_pred_graph = graph.predict_on_batch(ins)\n    y_pred_clone = clone.predict_on_batch(ins)\n    assert y_pred_graph.shape == y_pred_clone.shape\n    assert_allclose(y_pred_graph, y_pred_clone)\n\n\ndef test_clone_optimizer():\n    lr, momentum, clipnorm, clipvalue = np.random.random(size=4)\n    optimizer = SGD(lr=lr, momentum=momentum, clipnorm=clipnorm, clipvalue=clipvalue)\n    clone = clone_optimizer(optimizer)\n\n    assert isinstance(clone, SGD)\n    assert K.get_value(optimizer.lr) == K.get_value(clone.lr)\n    assert K.get_value(optimizer.momentum) == K.get_value(clone.momentum)\n    assert optimizer.clipnorm == clone.clipnorm\n    assert optimizer.clipvalue == clone.clipvalue\n\n\ndef test_clone_optimizer_from_string():\n    clone = clone_optimizer('sgd')\n    assert isinstance(clone, SGD)\n\n\ndef test_huber_loss():\n    a = np.array([1.,  1.5, 2., 4.])\n    b = np.array([1.5, 1.,  4., 2.])\n    assert_allclose(K.eval(huber_loss(a, b, 1.)), np.array([.125, .125, 1.5, 1.5]))\n    assert_allclose(K.eval(huber_loss(a, b, 3.)), np.array([.125, .125, 2., 2.]))\n    assert_allclose(K.eval(huber_loss(a, b, np.inf)), np.array([.125, .125, 2., 2.]))\n\n\ndef test_whitening_normalizer():\n    x = np.random.normal(loc=.2, scale=2., size=(1000, 5))\n    normalizer = WhiteningNormalizer(shape=(5,))\n    normalizer.update(x[:500])\n    normalizer.update(x[500:])\n\n    assert_allclose(normalizer.mean, np.mean(x, axis=0))\n    assert_allclose(normalizer.std, np.std(x, axis=0))\n    \n    x_norm = normalizer.normalize(x)\n    assert_allclose(np.mean(x_norm, axis=0), np.zeros(5, dtype=normalizer.dtype), atol=1e-5)\n    assert_allclose(np.std(x_norm, axis=0), np.ones(5, dtype=normalizer.dtype), atol=1e-5)\n\n    x_denorm = normalizer.denormalize(x_norm)\n    assert_allclose(x_denorm, x)\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/rl/util.py,0,"b'import numpy as np\nimport random\n\nfrom rl.core import Env\n\n\nclass MultiInputTestEnv(Env):\n    def __init__(self, observation_shape):\n        self.observation_shape = observation_shape\n\n    def step(self, action):\n        return self._get_obs(), random.choice([0, 1]), random.choice([True, False]), {}\n\n    def reset(self):\n        return self._get_obs()\n\n    def _get_obs(self):\n        if type(self.observation_shape) is list:\n            return [np.random.random(s) for s in self.observation_shape]\n        else:\n            return np.random.random(self.observation_shape)\n\n    def __del__(self):\n        pass\n'"
utils/gym/__init__.py,0,b''
utils/gym/prng.py,0,"b'import numpy\n\nnp_random = numpy.random.RandomState()\n\n\ndef seed(seed=None):\n    """"""Seed the common numpy.random.RandomState used in spaces\n\n    CF\n    https://github.com/openai/gym/commit/58e6aa95e5af2c738557431f812abb81c505a7cf#commitcomment-17669277\n    for some details about why we seed the spaces separately from the\n    envs, but tl;dr is that it\'s pretty uncommon for them to be used\n    within an actual algorithm, and the code becomes simpler to just\n    use this common numpy.random.RandomState.\n    """"""\n    np_random.seed(seed)\n\n# This numpy.random.RandomState gets used in all spaces for their\n# \'sample\' method. It\'s not really expected that people will be using\n# these in their algorithms.\nseed(0)\n'"
rl/common/vec_env/__init__.py,0,"b'# Inspired from VecEnv from OpenAI Baselines\n\nclass VecEnv(object):\n    """"""\n    An abstract asynchronous, vectorized environment.\n    """"""\n    def __init__(self, num_envs, observation_space, action_space):\n        self.num_envs = num_envs\n        self.observation_space = observation_space\n        self.action_space = action_space\n\n    def reset(self):\n        """"""\n        Reset all the environments and return an array of\n        observations, or a tuple of observation arrays.\n        If step_async is still doing work, that work will\n        be cancelled and step_wait() should not be called\n        until step_async() is invoked again.\n        """"""\n        pass\n\n    def step_async(self, actions):\n        """"""\n        Tell all the environments to start taking a step\n        with the given actions.\n        Call step_wait() to get the results of the step.\n        You should not call this if a step_async run is\n        already pending.\n        """"""\n        raise NotImplementedError()\n\n    def step_wait(self):\n        """"""\n        Wait for the step taken with step_async().\n        Returns (obs, rews, dones, infos):\n         - obs: an array of observations, or a tuple of\n                arrays of observations.\n         - rews: an array of rewards\n         - dones: an array of ""episode done"" booleans\n         - infos: a sequence of info objects\n        """"""\n        raise NotImplementedError()\n\n    def close(self):\n        """"""\n        Clean up the environments\' resources.\n        """"""\n        raise NotImplementedError()\n\n    def step(self, actions):\n        self.step_async(actions)\n        return self.step_wait()\n\n    def render(self, mode=\'human\'):\n        logger.warn(\'Render not defined for %s\' % self)\n\n    def seed(self, i):\n        raise NotImplementedError()\n\n    @property\n    def unwrapped(self):\n        if isinstance(self, VecEnvWrapper):\n            return self.venv.unwrapped\n        else:\n            return self\n\nclass CloudpickleWrapper(object):\n    """"""\n    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n    """"""\n    def __init__(self, x):\n        self.x = x\n\n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.x)\n\n    def __setstate__(self, ob):\n        import pickle\n        self.x = pickle.loads(ob)\n'"
rl/common/vec_env/subproc_env_vec.py,0,"b'# Inspired from OpenAI Baselines\n\nimport numpy as np\nfrom multiprocessing import Process, Pipe\nfrom rl.common.vec_env import VecEnv, CloudpickleWrapper\nfrom rl.common.tile_images import tile_images\n\ndef worker(remote, parent_remote, env_fn_wrapper):\n    parent_remote.close()\n    env = env_fn_wrapper.x()\n    while True:\n        cmd, data = remote.recv()\n        if cmd == \'step\':\n            ob, reward, done, info = env.step(data)\n            if done:\n                ob = env.reset()\n            remote.send((ob, reward, done, info))\n        elif cmd == \'reset\':\n            ob = env.reset()\n            remote.send(ob)\n        elif cmd == \'render\':\n            remote.send(env.render(mode=\'rgb_array\'))\n        elif cmd == \'close\':\n            remote.close()\n            break\n        elif cmd == \'get_spaces\':\n            remote.send((env.observation_space, env.action_space))\n        elif cmd == \'seed\':\n            val = env.seed(data)\n            remote.send(val)\n        else:\n            raise NotImplementedError\n\n\nclass SubprocVecEnv(VecEnv):\n    def __init__(self, env_fns, spaces=None):\n        """"""\n        envs: list of gym environments to run in subprocesses\n        """"""\n        self.waiting = False\n        self.closed = False\n        nenvs = len(env_fns)\n        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n                   for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n        for p in self.ps:\n            p.daemon = True # if the main process crashes, we should not cause things to hang\n            p.start()\n        for remote in self.work_remotes:\n            remote.close()\n\n        self.remotes[0].send((\'get_spaces\', None))\n        observation_space, action_space = self.remotes[0].recv()\n        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n\n    def step_async(self, actions):\n        for remote, action in zip(self.remotes, actions):\n            remote.send((\'step\', action))\n        self.waiting = True\n\n    def step_wait(self):\n        results = [remote.recv() for remote in self.remotes]\n        self.waiting = False\n        obs, rews, dones, infos = zip(*results)\n        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n\n    def reset(self):\n        for remote in self.remotes:\n            remote.send((\'reset\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def reset_task(self):\n        for remote in self.remotes:\n            remote.send((\'reset_task\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def close(self):\n        if self.closed:\n            return\n        if self.waiting:\n            for remote in self.remotes:            \n                remote.recv()\n        for remote in self.remotes:\n            remote.send((\'close\', None))\n        for p in self.ps:\n            p.join()\n        self.closed = True\n\n    def render(self, mode=\'human\'):\n        raise NotImplementedError(\'Render is not implemented for Synchronous Environment\')\n\n    def seed(self, i):\n        rank = i\n        for remote in self.remotes:\n            remote.send((\'seed\', rank))\n            rank += 1\n'"
tests/rl/agents/__init__.py,0,b''
tests/rl/agents/test_cem.py,0,"b'from __future__ import division\nfrom __future__ import absolute_import\n\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Flatten, Concatenate\n\nfrom rl.agents.cem import CEMAgent\nfrom rl.memory import EpisodeParameterMemory\nfrom rl.processors import MultiInputProcessor\n\nfrom ..util import MultiInputTestEnv\n\n\ndef test_single_cem_input():\n    model = Sequential()\n    model.add(Flatten(input_shape=(2, 3)))\n    model.add(Dense(2))\n\n    memory = EpisodeParameterMemory(limit=10, window_length=2)\n    agent = CEMAgent(model, memory=memory, nb_actions=2, nb_steps_warmup=5, batch_size=4, train_interval=50)\n    agent.compile()\n    agent.fit(MultiInputTestEnv((3,)), nb_steps=100)\n\n\ndef test_multi_cem_input():\n    input1 = Input(shape=(2, 3))\n    input2 = Input(shape=(2, 4))\n    x = Concatenate()([input1, input2])\n    x = Flatten()(x)\n    x = Dense(2)(x)\n    model = Model(inputs=[input1, input2], outputs=x)\n\n    memory = EpisodeParameterMemory(limit=10, window_length=2)\n    processor = MultiInputProcessor(nb_inputs=2)\n    agent = CEMAgent(model, memory=memory, nb_actions=2, nb_steps_warmup=5, batch_size=4,\n                     processor=processor, train_interval=50)\n    agent.compile()\n    agent.fit(MultiInputTestEnv([(3,), (4,)]), nb_steps=100)\n'"
tests/rl/agents/test_ddpg.py,0,"b""from __future__ import division\nfrom __future__ import absolute_import\n\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Flatten, Concatenate\n\nfrom rl.agents.ddpg import DDPGAgent\nfrom rl.memory import SequentialMemory\nfrom rl.processors import MultiInputProcessor\n\nfrom ..util import MultiInputTestEnv\n\n\ndef test_single_ddpg_input():\n    nb_actions = 2\n\n    actor = Sequential()\n    actor.add(Flatten(input_shape=(2, 3)))\n    actor.add(Dense(nb_actions))\n\n    action_input = Input(shape=(nb_actions,), name='action_input')\n    observation_input = Input(shape=(2, 3), name='observation_input')\n    x = Concatenate()([action_input, Flatten()(observation_input)])\n    x = Dense(1)(x)\n    critic = Model(inputs=[action_input, observation_input], outputs=x)\n\n    memory = SequentialMemory(limit=10, window_length=2)\n    agent = DDPGAgent(actor=actor, critic=critic, critic_action_input=action_input, memory=memory,\n                      nb_actions=2, nb_steps_warmup_critic=5, nb_steps_warmup_actor=5, batch_size=4)\n    agent.compile('sgd')\n    agent.fit(MultiInputTestEnv((3,)), nb_steps=10)\n\n\ndef test_multi_ddpg_input():\n    nb_actions = 2\n\n    actor_observation_input1 = Input(shape=(2, 3), name='actor_observation_input1')\n    actor_observation_input2 = Input(shape=(2, 4), name='actor_observation_input2')\n    actor = Sequential()\n    x = Concatenate()([actor_observation_input1, actor_observation_input2])\n    x = Flatten()(x)\n    x = Dense(nb_actions)(x)\n    actor = Model(inputs=[actor_observation_input1, actor_observation_input2], outputs=x)\n\n    action_input = Input(shape=(nb_actions,), name='action_input')\n    critic_observation_input1 = Input(shape=(2, 3), name='critic_observation_input1')\n    critic_observation_input2 = Input(shape=(2, 4), name='critic_observation_input2')\n    x = Concatenate()([critic_observation_input1, critic_observation_input2])\n    x = Concatenate()([action_input, Flatten()(x)])\n    x = Dense(1)(x)\n    critic = Model(inputs=[action_input, critic_observation_input1, critic_observation_input2], outputs=x)\n\n    processor = MultiInputProcessor(nb_inputs=2)\n    memory = SequentialMemory(limit=10, window_length=2)\n    agent = DDPGAgent(actor=actor, critic=critic, critic_action_input=action_input, memory=memory,\n                      nb_actions=2, nb_steps_warmup_critic=5, nb_steps_warmup_actor=5, batch_size=4,\n                      processor=processor)\n    agent.compile('sgd')\n    agent.fit(MultiInputTestEnv([(3,), (4,)]), nb_steps=10)\n"""
tests/rl/agents/test_dqn.py,0,"b""from __future__ import division\nfrom __future__ import absolute_import\n\nimport pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Flatten, Concatenate\n\nfrom rl.agents.dqn import NAFLayer, DQNAgent, NAFAgent\nfrom rl.memory import SequentialMemory\nfrom rl.processors import MultiInputProcessor\n\nfrom ..util import MultiInputTestEnv\n\n\ndef test_single_dqn_input():\n    model = Sequential()\n    model.add(Flatten(input_shape=(2, 3)))\n    model.add(Dense(2))\n\n    memory = SequentialMemory(limit=10, window_length=2)\n    for double_dqn in (True, False):\n        agent = DQNAgent(model, memory=memory, nb_actions=2, nb_steps_warmup=5, batch_size=4,\n                         enable_double_dqn=double_dqn)\n        agent.compile('sgd')\n        agent.fit(MultiInputTestEnv((3,)), nb_steps=10)\n\n\ndef test_multi_dqn_input():\n    input1 = Input(shape=(2, 3))\n    input2 = Input(shape=(2, 4))\n    x = Concatenate()([input1, input2])\n    x = Flatten()(x)\n    x = Dense(2)(x)\n    model = Model(inputs=[input1, input2], outputs=x)\n\n    memory = SequentialMemory(limit=10, window_length=2)\n    processor = MultiInputProcessor(nb_inputs=2)\n    for double_dqn in (True, False):\n        agent = DQNAgent(model, memory=memory, nb_actions=2, nb_steps_warmup=5, batch_size=4,\n                         processor=processor, enable_double_dqn=double_dqn)\n        agent.compile('sgd')\n        agent.fit(MultiInputTestEnv([(3,), (4,)]), nb_steps=10)\n\n\ndef test_single_continuous_dqn_input():\n    nb_actions = 2\n\n    V_model = Sequential()\n    V_model.add(Flatten(input_shape=(2, 3)))\n    V_model.add(Dense(1))\n\n    mu_model = Sequential()\n    mu_model.add(Flatten(input_shape=(2, 3)))\n    mu_model.add(Dense(nb_actions))\n\n    L_input = Input(shape=(2, 3))\n    L_input_action = Input(shape=(nb_actions,))\n    x = Concatenate()([Flatten()(L_input), L_input_action])\n    x = Dense(((nb_actions * nb_actions + nb_actions) // 2))(x)\n    L_model = Model(inputs=[L_input_action, L_input], outputs=x)\n\n    memory = SequentialMemory(limit=10, window_length=2)\n    agent = NAFAgent(nb_actions=nb_actions, V_model=V_model, L_model=L_model, mu_model=mu_model,\n                     memory=memory, nb_steps_warmup=5, batch_size=4)\n    agent.compile('sgd')\n    agent.fit(MultiInputTestEnv((3,)), nb_steps=10)\n\n\ndef test_multi_continuous_dqn_input():\n    nb_actions = 2\n\n    V_input1 = Input(shape=(2, 3))\n    V_input2 = Input(shape=(2, 4))\n    x = Concatenate()([V_input1, V_input2])\n    x = Flatten()(x)\n    x = Dense(1)(x)\n    V_model = Model(inputs=[V_input1, V_input2], outputs=x)\n\n    mu_input1 = Input(shape=(2, 3))\n    mu_input2 = Input(shape=(2, 4))\n    x = Concatenate()([mu_input1, mu_input2])\n    x = Flatten()(x)\n    x = Dense(nb_actions)(x)\n    mu_model = Model(inputs=[mu_input1, mu_input2], outputs=x)\n\n    L_input1 = Input(shape=(2, 3))\n    L_input2 = Input(shape=(2, 4))\n    L_input_action = Input(shape=(nb_actions,))\n    x = Concatenate()([L_input1, L_input2])\n    x = Concatenate()([Flatten()(x), L_input_action])\n    x = Dense(((nb_actions * nb_actions + nb_actions) // 2))(x)\n    L_model = Model(inputs=[L_input_action, L_input1, L_input2], outputs=x)\n\n    memory = SequentialMemory(limit=10, window_length=2)\n    processor = MultiInputProcessor(nb_inputs=2)\n    agent = NAFAgent(nb_actions=nb_actions, V_model=V_model, L_model=L_model, mu_model=mu_model,\n                     memory=memory, nb_steps_warmup=5, batch_size=4, processor=processor)\n    agent.compile('sgd')\n    agent.fit(MultiInputTestEnv([(3,), (4,)]), nb_steps=10)\n\n\ndef test_naf_layer_full():\n    batch_size = 2\n    for nb_actions in (1, 3):\n        # Construct single model with NAF as the only layer, hence it is fully deterministic\n        # since no weights are used, which would be randomly initialized.\n        L_flat_input = Input(shape=((nb_actions * nb_actions + nb_actions) // 2,))\n        mu_input = Input(shape=(nb_actions,))\n        action_input = Input(shape=(nb_actions,))\n        x = NAFLayer(nb_actions, mode='full')([L_flat_input, mu_input, action_input])\n        model = Model(inputs=[L_flat_input, mu_input, action_input], outputs=x)\n        model.compile(loss='mse', optimizer='sgd')\n        \n        # Create random test data.\n        L_flat = np.random.random((batch_size, (nb_actions * nb_actions + nb_actions) // 2)).astype('float32')\n        mu = np.random.random((batch_size, nb_actions)).astype('float32')\n        action = np.random.random((batch_size, nb_actions)).astype('float32')\n\n        # Perform reference computations in numpy since these are much easier to verify.\n        L = np.zeros((batch_size, nb_actions, nb_actions)).astype('float32')\n        LT = np.copy(L)\n        for l, l_T, l_flat in zip(L, LT, L_flat):\n            l[np.tril_indices(nb_actions)] = l_flat\n            l[np.diag_indices(nb_actions)] = np.exp(l[np.diag_indices(nb_actions)])\n            l_T[:, :] = l.T\n        P = np.array([np.dot(l, l_T) for l, l_T in zip(L, LT)]).astype('float32')\n        A_ref = np.array([np.dot(np.dot(a - m, p), a - m) for a, m, p in zip(action, mu, P)]).astype('float32')\n        A_ref *= -.5\n\n        # Finally, compute the output of the net, which should be identical to the previously\n        # computed reference.\n        A_net = model.predict([L_flat, mu, action]).flatten()\n        assert_allclose(A_net, A_ref, rtol=1e-5)\n\n\ndef test_naf_layer_diag():\n    batch_size = 2\n    for nb_actions in (1, 3):\n        # Construct single model with NAF as the only layer, hence it is fully deterministic\n        # since no weights are used, which would be randomly initialized.\n        L_flat_input = Input(shape=(nb_actions,))\n        mu_input = Input(shape=(nb_actions,))\n        action_input = Input(shape=(nb_actions,))\n        x = NAFLayer(nb_actions, mode='diag')([L_flat_input, mu_input, action_input])\n        model = Model(inputs=[L_flat_input, mu_input, action_input], outputs=x)\n        model.compile(loss='mse', optimizer='sgd')\n        \n        # Create random test data.\n        L_flat = np.random.random((batch_size, nb_actions)).astype('float32')\n        mu = np.random.random((batch_size, nb_actions)).astype('float32')\n        action = np.random.random((batch_size, nb_actions)).astype('float32')\n\n        # Perform reference computations in numpy since these are much easier to verify.\n        P = np.zeros((batch_size, nb_actions, nb_actions)).astype('float32')\n        for p, l_flat in zip(P, L_flat):\n            p[np.diag_indices(nb_actions)] = l_flat\n        print(P, L_flat)\n        A_ref = np.array([np.dot(np.dot(a - m, p), a - m) for a, m, p in zip(action, mu, P)]).astype('float32')\n        A_ref *= -.5\n\n        # Finally, compute the output of the net, which should be identical to the previously\n        # computed reference.\n        A_net = model.predict([L_flat, mu, action]).flatten()\n        assert_allclose(A_net, A_ref, rtol=1e-5)\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
utils/gym/envs/__init__.py,0,b'from .twoRoundDeterministicRewardEnv import TwoRoundDeterministicRewardEnv\n'
utils/gym/envs/twoRoundDeterministicRewardEnv.py,0,"b'import gym\nfrom ..spaces import Discrete\n\n\nclass TwoRoundDeterministicRewardEnv(gym.Env):\n    def __init__(self):\n        self.action_space = Discrete(2)\n        self.observation_space = Discrete(3)\n        self.reset()\n\n    def step(self, action):\n        rewards = [[0, 3], [1, 2]]\n\n        assert self.action_space.contains(action)\n\n        if self.firstAction is None:\n            self.firstAction = action\n            reward = 0\n            done = False\n        else:\n            reward = rewards[self.firstAction][action]\n            done = True\n\n        return self.get_obs(), reward, done, {}\n\n    def get_obs(self):\n        if self.firstAction is None:\n            return 2\n        else:\n            return self.firstAction\n\n    def reset(self):\n        self.firstAction = None\n        return self.get_obs()\n'"
utils/gym/spaces/__init__.py,0,b'from .discrete import Discrete\n'
utils/gym/spaces/discrete.py,0,"b'import numpy as np\n\nimport gym\nfrom .. import prng\n\n\nclass Discrete(gym.Space):\n    """"""\n    {0,1,...,n-1}\n\n    Example usage:\n    self.observation_space = spaces.Discrete(2)\n    """"""\n    def __init__(self, n):\n        self.n = n\n\n    def sample(self):\n        return prng.np_random.randint(self.n)\n\n    def contains(self, x):\n        if isinstance(x, int):\n            as_int = x\n        elif isinstance(x, (np.generic, np.ndarray)) and (x.dtype.kind in np.typecodes[\'AllInteger\'] and x.shape == ()):\n            as_int = int(x)\n        else:\n            return False\n        return as_int >= 0 and as_int < self.n\n\n    @property\n    def shape(self):\n        return (self.n,)\n\n    def __repr__(self):\n        return ""Discrete(%d)"" % self.n\n\n    def __eq__(self, other):\n        return self.n == other.n\n'"
