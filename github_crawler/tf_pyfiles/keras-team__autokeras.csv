file_path,api_count,code
setup.py,0,"b'from distutils.core import setup\nfrom pathlib import Path\n\nfrom setuptools import find_packages\n\nthis_file = Path(__file__).resolve()\nreadme = this_file.parent / \'README.md\'\n\nsetup(\n    name=\'autokeras\',\n    version=\'1.0.3\',\n    description=\'AutoML for deep learning\',\n    package_data={\'\': [\'README.md\']},\n    long_description=readme.read_text(encoding=\'utf-8\'),\n    long_description_content_type=\'text/markdown\',\n    author=\'Data Analytics at Texas A&M (DATA) Lab, Keras Team\',\n    author_email=\'jhfjhfj1@gmail.com\',\n    url=\'http://autokeras.com\',\n    download_url=\'https://github.com/keras-team/autokeras/archive/1.0.3.tar.gz\',\n    keywords=[\'AutoML\', \'Keras\'],\n    install_requires=[\n        \'packaging\',\n        \'keras-tuner>=1.0.1\',\n        \'tensorflow>=2.2.0\',\n        \'scikit-learn\',\n        \'numpy\',\n        \'pandas\',\n    ],\n    extras_require={\n        \'tests\': [\'pytest>=4.4.0\',\n                  \'flake8\',\n                  \'isort\',\n                  \'pytest-xdist\',\n                  \'pytest-cov\',\n                  \'coverage\',\n                  \'typeguard>=2,<2.10.0\',\n                  \'typedapi>=0.2,<0.3\'\n                  ],\n    },\n    classifiers=[\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Education"",\n        ""Intended Audience :: Science/Research"",\n        \'License :: OSI Approved :: MIT License\',\n        ""Programming Language :: Python :: 3.5"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Topic :: Scientific/Engineering :: Mathematics"",\n        ""Topic :: Software Development :: Libraries :: Python Modules"",\n        ""Topic :: Software Development :: Libraries"",\n    ],\n    license=""MIT"",\n    packages=find_packages(exclude=(\'tests\',)),\n)\n'"
autokeras/__init__.py,0,"b""from autokeras.auto_model import AutoModel\nfrom autokeras.blocks import CategoricalToNumerical\nfrom autokeras.blocks import ClassificationHead\nfrom autokeras.blocks import ConvBlock\nfrom autokeras.blocks import DenseBlock\nfrom autokeras.blocks import Embedding\nfrom autokeras.blocks import Flatten\nfrom autokeras.blocks import ImageAugmentation\nfrom autokeras.blocks import ImageBlock\nfrom autokeras.blocks import Merge\nfrom autokeras.blocks import Normalization\nfrom autokeras.blocks import RegressionHead\nfrom autokeras.blocks import ResNetBlock\nfrom autokeras.blocks import RNNBlock\nfrom autokeras.blocks import SpatialReduction\nfrom autokeras.blocks import StructuredDataBlock\nfrom autokeras.blocks import TemporalReduction\nfrom autokeras.blocks import TextBlock\nfrom autokeras.blocks import TextToIntSequence\nfrom autokeras.blocks import TextToNgramVector\nfrom autokeras.blocks import XceptionBlock\nfrom autokeras.engine.block import Block\nfrom autokeras.engine.head import Head\nfrom autokeras.engine.node import Node\nfrom autokeras.keras_layers import CUSTOM_OBJECTS\nfrom autokeras.nodes import ImageInput\nfrom autokeras.nodes import Input\nfrom autokeras.nodes import StructuredDataInput\nfrom autokeras.nodes import TextInput\nfrom autokeras.nodes import TimeseriesInput\nfrom autokeras.tasks import ImageClassifier\nfrom autokeras.tasks import ImageRegressor\nfrom autokeras.tasks import StructuredDataClassifier\nfrom autokeras.tasks import StructuredDataRegressor\nfrom autokeras.tasks import TextClassifier\nfrom autokeras.tasks import TextRegressor\nfrom autokeras.tasks import TimeseriesForecaster\nfrom autokeras.utils.utils import check_tf_version\n\n__version__ = '1.0.3'\ncheck_tf_version()\n"""
autokeras/auto_model.py,15,"b'from pathlib import Path\nfrom typing import List\nfrom typing import Optional\nfrom typing import Type\nfrom typing import Union\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\nfrom autokeras import blocks\nfrom autokeras import graph as graph_module\nfrom autokeras import nodes as input_module\nfrom autokeras import tuners\nfrom autokeras.engine import head as head_module\nfrom autokeras.engine import node as node_module\nfrom autokeras.engine import preprocessor\nfrom autokeras.engine import tuner\nfrom autokeras.nodes import Input\nfrom autokeras.utils import data_utils\n\nTUNER_CLASSES = {\n    \'bayesian\': tuners.BayesianOptimization,\n    \'random\': tuners.RandomSearch,\n    \'hyperband\': tuners.Hyperband,\n    \'greedy\': tuners.Greedy,\n}\n\n\ndef get_tuner_class(tuner):\n    if isinstance(tuner, str) and tuner in TUNER_CLASSES:\n        return TUNER_CLASSES.get(tuner)\n    else:\n        raise ValueError(\'The value {tuner} passed for argument tuner is invalid, \'\n                         \'expected one of ""greedy"", ""random"", ""hyperband"", \'\n                         \'""bayesian"".\'.format(tuner=tuner))\n\n\nclass AutoModel(object):\n    """""" A Model defined by inputs and outputs.\n    AutoModel combines a HyperModel and a Tuner to tune the HyperModel.\n    The user can use it in a similar way to a Keras model since it\n    also has `fit()` and  `predict()` methods.\n\n    The AutoModel has two use cases. In the first case, the user only specifies the\n    input nodes and output heads of the AutoModel. The AutoModel infers the rest part\n    of the model. In the second case, user can specify the high-level architecture of\n    the AutoModel by connecting the Blocks with the functional API, which is the same\n    as the Keras [functional API](https://www.tensorflow.org/guide/keras/functional).\n\n    # Example\n    ```python\n        # The user only specifies the input nodes and output heads.\n        import autokeras as ak\n        ak.AutoModel(\n            inputs=[ak.ImageInput(), ak.TextInput()],\n            outputs=[ak.ClassificationHead(), ak.RegressionHead()]\n        )\n    ```\n    ```python\n        # The user specifies the high-level architecture.\n        import autokeras as ak\n        image_input = ak.ImageInput()\n        image_output = ak.ImageBlock()(image_input)\n        text_input = ak.TextInput()\n        text_output = ak.TextBlock()(text_input)\n        output = ak.Merge()([image_output, text_output])\n        classification_output = ak.ClassificationHead()(output)\n        regression_output = ak.RegressionHead()(output)\n        ak.AutoModel(\n            inputs=[image_input, text_input],\n            outputs=[classification_output, regression_output]\n        )\n    ```\n\n    # Arguments\n        inputs: A list of Node instances.\n            The input node(s) of the AutoModel.\n        outputs: A list of Node or Head instances.\n            The output node(s) or head(s) of the AutoModel.\n        preprocessors: An instance or list of `Preprocessor` objects corresponding to\n            each AutoModel input, to preprocess a `tf.data.Dataset` before passing it\n            to the model. Defaults to None (no external preprocessing).\n        project_name: String. The name of the AutoModel. Defaults to \'auto_model\'.\n        max_trials: Int. The maximum number of different Keras Models to try.\n            The search may finish before reaching the max_trials. Defaults to 100.\n        directory: String. The path to a directory for storing the search outputs.\n            Defaults to None, which would create a folder with the name of the\n            AutoModel in the current directory.\n        objective: String. Name of model metric to minimize\n            or maximize, e.g. \'val_accuracy\'. Defaults to \'val_loss\'.\n        tuner: String or subclass of AutoTuner. If use string, it should be one of\n            \'greedy\', \'bayesian\', \'hyperband\' or \'random\'. It can also be a subclass\n            of AutoTuner. Defaults to \'greedy\'.\n        overwrite: Boolean. Defaults to `False`. If `False`, reloads an existing\n            project of the same name if one is found. Otherwise, overwrites the\n            project.\n        seed: Int. Random seed.\n        **kwargs: Any arguments supported by kerastuner.Tuner.\n    """"""\n\n    def __init__(self,\n                 inputs: Union[Input, List[Input]],\n                 outputs: Union[head_module.Head, node_module.Node, list],\n                 preprocessors: Optional[\n                     Union[preprocessor.Preprocessor,\n                           List[preprocessor.Preprocessor]]] = None,\n                 project_name: str = \'auto_model\',\n                 max_trials: int = 100,\n                 directory: Union[str, Path, None] = None,\n                 objective: str = \'val_loss\',\n                 tuner: Union[str, Type[tuner.AutoTuner]] = \'greedy\',\n                 overwrite: bool = False,\n                 seed: Optional[int] = None,\n                 **kwargs):\n        self.inputs = nest.flatten(inputs)\n        self.outputs = nest.flatten(outputs)\n        self.seed = seed\n        if seed:\n            np.random.seed(seed)\n            tf.random.set_seed(seed)\n        # TODO: Support passing a tuner instance.\n        # Initialize the hyper_graph.\n        graph = self._build_graph()\n        if isinstance(tuner, str):\n            tuner = get_tuner_class(tuner)\n        self.tuner = tuner(\n            hypermodel=graph,\n            preprocessors=preprocessors,\n            overwrite=overwrite,\n            objective=objective,\n            max_trials=max_trials,\n            directory=directory,\n            seed=self.seed,\n            project_name=project_name,\n            **kwargs)\n        # Used by tuner to decide whether to use validation set for final fit.\n        self._split_dataset = False\n        self._heads = [output_node.in_blocks[0] for output_node in self.outputs]\n        self._input_adapters = [input_node.get_adapter()\n                                for input_node in self.inputs]\n        self._output_adapters = [head.get_adapter()\n                                 for head in self._heads]\n\n    @property\n    def overwrite(self):\n        return self.tuner.overwrite\n\n    @property\n    def objective(self):\n        return self.tuner.objective\n\n    @property\n    def max_trials(self):\n        return self.tuner.max_trials\n\n    @property\n    def directory(self):\n        return self.tuner.directory\n\n    @property\n    def project_name(self):\n        return self.tuner.project_name\n\n    @property\n    def preprocessors(self):\n        return self.tuner.preprocessors\n\n    def _assemble(self):\n        """"""Assemble the Blocks based on the input output nodes.""""""\n        inputs = nest.flatten(self.inputs)\n        outputs = nest.flatten(self.outputs)\n\n        middle_nodes = []\n        for input_node in inputs:\n            if isinstance(input_node, input_module.TextInput):\n                middle_nodes.append(blocks.TextBlock()(input_node))\n            if isinstance(input_node, input_module.ImageInput):\n                middle_nodes.append(blocks.ImageBlock()(input_node))\n            if isinstance(input_node, input_module.StructuredDataInput):\n                middle_nodes.append(blocks.StructuredDataBlock()(input_node))\n            if isinstance(input_node, input_module.TimeseriesInput):\n                middle_nodes.append(blocks.TimeseriesBlock()(input_node))\n\n        # Merge the middle nodes.\n        if len(middle_nodes) > 1:\n            output_node = blocks.Merge()(middle_nodes)\n        else:\n            output_node = middle_nodes[0]\n\n        outputs = nest.flatten([output_blocks(output_node)\n                                for output_blocks in outputs])\n        return graph_module.Graph(inputs=inputs, outputs=outputs)\n\n    def _build_graph(self):\n        # Using functional API.\n        if all([isinstance(output, node_module.Node) for output in self.outputs]):\n            graph = graph_module.Graph(inputs=self.inputs, outputs=self.outputs)\n        # Using input/output API.\n        elif all([isinstance(output, head_module.Head) for output in self.outputs]):\n            graph = self._assemble()\n            self.outputs = graph.outputs\n\n        return graph\n\n    def fit(self,\n            x=None,\n            y=None,\n            batch_size=32,\n            epochs=None,\n            callbacks=None,\n            validation_split=0.2,\n            validation_data=None,\n            **kwargs):\n        """"""Search for the best model and hyperparameters for the AutoModel.\n\n        It will search for the best model based on the performances on\n        validation data.\n\n        # Arguments\n            x: numpy.ndarray or tensorflow.Dataset. Training data x.\n            y: numpy.ndarray or tensorflow.Dataset. Training data y.\n            batch_size: Int. Number of samples per gradient update. Defaults to 32.\n            epochs: Int. The number of epochs to train each model during the search.\n                If unspecified, by default we train for a maximum of 1000 epochs,\n                but we stop training if the validation loss stops improving for 10\n                epochs (unless you specified an EarlyStopping callback as part of\n                the callbacks argument, in which case the EarlyStopping callback you\n                specified will determine early stopping).\n            callbacks: List of Keras callbacks to apply during training and\n                validation.\n            validation_split: Float between 0 and 1. Defaults to 0.2.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling. This argument is\n                not supported when `x` is a dataset.\n                The best model found would be fit on the entire dataset including the\n                validation data.\n            validation_data: Data on which to evaluate the loss and any model metrics\n                at the end of each epoch. The model will not be trained on this data.\n                `validation_data` will override `validation_split`. The type of the\n                validation data should be the same as the training data.\n                The best model found would be fit on the training dataset without the\n                validation data.\n            **kwargs: Any arguments supported by keras.Model.fit.\n        """"""\n        for adapter in self._input_adapters + self._output_adapters:\n            adapter.batch_size = batch_size\n        dataset, validation_data = self._prepare_data(\n            x=x,\n            y=y,\n            validation_data=validation_data,\n            validation_split=validation_split)\n\n        self.tuner.search(x=dataset,\n                          epochs=epochs,\n                          callbacks=callbacks,\n                          validation_data=validation_data,\n                          fit_on_val_data=self._split_dataset,\n                          **kwargs)\n\n    @staticmethod\n    def _adapt(sources, fit, hms, adapters):\n        sources = nest.flatten(sources)\n        adapted = []\n        for source, hm, adapter in zip(sources, hms, adapters):\n            if fit:\n                source = adapter.fit_transform(source)\n                hm.config_from_adapter(adapter)\n            else:\n                source = adapter.transform(source)\n            adapted.append(source)\n        if len(adapted) == 1:\n            return adapted[0]\n        return tf.data.Dataset.zip(tuple(adapted))\n\n    def _process_xy(self, x, y, fit=False, validation=False, predict=False):\n        """"""Convert x, y to tf.data.Dataset.\n\n        # Arguments\n            x: Any type allowed by the corresponding input node.\n            y: Any type allowed by the corresponding head.\n            fit: Boolean. Whether to fit the type converter with the provided data.\n            validation: Boolean. Whether it is validation data or not.\n            predict: Boolean. True means the data doesn\'t contain y.\n\n        # Returns\n            A tf.data.Dataset containing both x and y.\n        """"""\n        self._check_data_format(x, y, validation=validation, predict=predict)\n        if isinstance(x, tf.data.Dataset):\n            dataset = x\n            if not predict:\n                y = dataset.map(lambda a, b: b)\n                y = [y.map(lambda *a: nest.flatten(a)[index])\n                     for index in range(len(self.outputs))]\n                x = dataset.map(lambda a, b: a)\n            x = [x.map(lambda *a: nest.flatten(a)[index])\n                 for index in range(len(self.inputs))]\n\n        x = self._adapt(x, fit, self.inputs, self._input_adapters)\n        if not predict:\n            y = self._adapt(y, fit, self._heads, self._output_adapters)\n\n        if not predict:\n            return tf.data.Dataset.zip((x, y))\n\n        if len(self.inputs) == 1:\n            return x\n\n        return x.map(lambda *x: (x, ))\n\n    def _check_data_format(self, x, y, validation=False, predict=False):\n        """"""Check if the dataset has the same number of IOs with the model.""""""\n        if validation:\n            in_val = \' in validation_data\'\n        else:\n            in_val = \'\'\n\n        if isinstance(x, tf.data.Dataset) and y is not None:\n            raise ValueError(\'Expect y is None when x is \'\n                             \'tf.data.Dataset{in_val}.\'.format(in_val=in_val))\n\n        if isinstance(x, tf.data.Dataset):\n            if not predict:\n                x_shapes, y_shapes = data_utils.dataset_shape(x)\n                x_shapes = nest.flatten(x_shapes)\n                y_shapes = nest.flatten(y_shapes)\n            else:\n                x_shapes = nest.flatten(data_utils.dataset_shape(x))\n        else:\n            x_shapes = [a.shape for a in nest.flatten(x)]\n            if not predict:\n                y_shapes = [a.shape for a in nest.flatten(y)]\n\n        if len(x_shapes) != len(self.inputs):\n            raise ValueError(\n                \'Expect x{in_val} to have {input_num} arrays, \'\n                \'but got {data_num}\'.format(\n                    in_val=in_val,\n                    input_num=len(self.inputs),\n                    data_num=len(x_shapes)))\n        if not predict and len(y_shapes) != len(self.outputs):\n            raise ValueError(\n                \'Expect y{in_val} to have {output_num} arrays, \'\n                \'but got {data_num}\'.format(\n                    in_val=in_val,\n                    output_num=len(self.outputs),\n                    data_num=len(y_shapes)))\n\n    def _prepare_data(self, x, y, validation_data, validation_split):\n        """"""Convert the data to tf.data.Dataset.""""""\n        # Check validation information.\n        if not validation_data and not validation_split:\n            raise ValueError(\'Either validation_data or validation_split \'\n                             \'should be provided.\')\n        # TODO: Handle other types of input, zip dataset, tensor, dict.\n        # Prepare the dataset.\n        self._check_data_format(x, y)\n        dataset = self._process_xy(x, y, fit=True)\n        if validation_data:\n            self._split_dataset = False\n            if isinstance(validation_data, tf.data.Dataset):\n                x_val = validation_data\n                y_val = None\n            else:\n                x_val, y_val = validation_data\n            validation_data = self._process_xy(x_val, y_val, validation=True)\n        # Split the data with validation_split.\n        if validation_data is None and validation_split:\n            self._split_dataset = True\n            dataset, validation_data = data_utils.split_dataset(\n                dataset,\n                validation_split)\n        return dataset, validation_data\n\n    def _get_x(self, dataset):\n        """"""Remove y from the tf.data.Dataset if exists.""""""\n        shapes = data_utils.dataset_shape(dataset)\n        # Only one or less element in the first level.\n        if len(shapes) <= 1:\n            return dataset.map(lambda *x: x[0])\n        # The first level has more than 1 element.\n        # The nest has 2 levels.\n        for shape in shapes:\n            if isinstance(shape, tuple):\n                return dataset.map(lambda x, y: x)\n        # The nest has one level.\n        # It matches the single IO case.\n        if len(shapes) == 2 and len(self.inputs) == 1 and len(self.outputs) == 1:\n            return dataset.map(lambda x, y: x)\n        return dataset\n\n    def predict(self, x, **kwargs):\n        """"""Predict the output for a given testing data.\n\n        # Arguments\n            x: Any allowed types according to the input node. Testing data.\n            **kwargs: Any arguments supported by keras.Model.predict.\n\n        # Returns\n            A list of numpy.ndarray objects or a single numpy.ndarray.\n            The predicted results.\n        """"""\n        if isinstance(x, tf.data.Dataset):\n            x = self._get_x(x)\n            dataset = self._process_xy(x, None, predict=True)\n        else:\n            dataset = self._adapt(x, False, self.inputs, self._input_adapters)\n        model = self.tuner.get_best_model()\n        y = model.predict(dataset, **kwargs)\n        y = self._postprocess(y)\n        if isinstance(y, list) and len(y) == 1:\n            y = y[0]\n        return y\n\n    def _postprocess(self, y):\n        y = nest.flatten(y)\n        new_y = []\n        for temp_y, adapter in zip(y, self._output_adapters):\n            temp_y = adapter.postprocess(temp_y)\n            new_y.append(temp_y)\n        return new_y\n\n    def evaluate(self, x, y=None, **kwargs):\n        """"""Evaluate the best model for the given data.\n\n        # Arguments\n            x: Any allowed types according to the input node. Testing data.\n            y: Any allowed types according to the head. Testing targets.\n                Defaults to None.\n            **kwargs: Any arguments supported by keras.Model.evaluate.\n\n        # Returns\n            Scalar test loss (if the model has a single output and no metrics) or\n            list of scalars (if the model has multiple outputs and/or metrics).\n            The attribute model.metrics_names will give you the display labels for\n            the scalar outputs.\n        """"""\n        dataset = self._process_xy(x, y, False)\n        return self.tuner.get_best_model().evaluate(x=dataset, **kwargs)\n\n    def export_model(self):\n        """"""Export the best Keras Model.\n\n        # Returns\n            tf.keras.Model instance. The best model found during the search, loaded\n            with trained weights.\n        """"""\n        return self.tuner.get_best_model()\n'"
autokeras/encoders.py,2,"b'import numpy as np\nimport tensorflow as tf\n\nfrom autokeras.engine import encoder as encoder_module\n\n\nclass OneHotEncoder(encoder_module.Encoder):\n    """"""OneHotEncoder to encode and decode the labels.\n\n    This class provides ways to transform data\'s classification label into vector.\n\n    # Arguments\n        num_classes: The number of classes in the classification problem.\n    """"""\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._label_to_vec = {}\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\'label_to_vec\': self._label_to_vec})\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        obj = super().from_config(config)\n        obj._label_to_vec = config[\'label_to_vec\']\n\n    def fit(self, data):\n        """"""Create mapping from label to vector, and vector to label.\n\n        # Arguments\n            data: list or numpy.ndarray. The original labels.\n        """"""\n        data = np.array(data).flatten()\n        self._labels = set(data)\n        if not self.num_classes:\n            self.num_classes = len(self._labels)\n        if self.num_classes < len(self._labels):\n            raise ValueError(\'More classes in data than specified.\')\n        for index, label in enumerate(self._labels):\n            vec = np.array([0] * self.num_classes)\n            vec[index] = 1\n            self._label_to_vec[label] = vec\n            self._int_to_label[index] = label\n\n    def encode(self, data):\n        """"""Get vector for every element in the data array.\n\n        # Arguments\n            data: list or numpy.ndarray. The original labels.\n\n        # Returns\n            numpy.ndarray. The one-hot encoded labels.\n        """"""\n        data = np.array(data)\n        if len(data.shape) > 1:\n            data = data.flatten()\n        return np.array(list(map(lambda x: self._label_to_vec[x], data)))\n\n    def decode(self, data):\n        """"""Get label for every element in data.\n\n        # Arguments\n            data: numpy.ndarray. The output probabilities of the classification head.\n\n        # Returns\n            numpy.ndarray. The original labels.\n        """"""\n        return np.array(list(map(lambda x: self._int_to_label[x],\n                                 np.argmax(np.array(data), axis=1)))).reshape(-1, 1)\n\n\nclass LabelEncoder(encoder_module.Encoder):\n    """"""An encoder to encode the labels to integers.\n\n    # Arguments\n        num_classes: Int. The number of classes. Defaults to None.\n    """"""\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._label_to_int = {}\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\'label_to_int\': self._label_to_int})\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        obj = super().from_config(config)\n        obj._label_to_int = config[\'label_to_int\']\n\n    def fit(self, data):\n        """"""Fit the encoder with all the labels.\n\n        # Arguments\n            data: numpy.ndarray. The original labels.\n        """"""\n        data = np.array(data).flatten()\n        self._labels = set(data)\n        if not self.num_classes:\n            self.num_classes = len(self._labels)\n        if self.num_classes < len(self._labels):\n            raise ValueError(\'More classes in data than specified.\')\n        for index, label in enumerate(self._labels):\n            self._int_to_label[index] = label\n            self._label_to_int[label] = index\n\n    def transform(self, x):\n        return self._label_to_int[x]\n\n    def encode(self, data):\n        """"""Encode the original labels.\n\n        # Arguments\n            data: numpy.ndarray. The original labels.\n\n        # Returns\n            numpy.ndarray with shape (n, 1). The encoded labels.\n        """"""\n        data = np.array(data)\n        if len(data.shape) > 1:\n            data = data.flatten()\n        return np.array(list(map(lambda x: self._label_to_int[x],\n                                 data))).reshape(-1, 1)\n\n    def decode(self, data):\n        """"""Get label for every element in data.\n\n        # Arguments\n            data: numpy.ndarray. The output probabilities of the classification head.\n\n        # Returns\n            numpy.ndarray. The original labels.\n        """"""\n        return np.array(list(map(lambda x: self._int_to_label[int(round(x[0]))],\n                                 np.array(data)))).reshape(-1, 1)\n\n\ndef serialize(encoder):\n    return tf.keras.utils.serialize_keras_object(encoder)\n\n\ndef deserialize(config, custom_objects=None):\n    return tf.keras.utils.deserialize_keras_object(\n        config,\n        module_objects=globals(),\n        custom_objects=custom_objects,\n        printable_module_name=\'encoder\')\n'"
autokeras/graph.py,3,"b'import kerastuner\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\nfrom autokeras import blocks as blocks_module\nfrom autokeras import nodes as nodes_module\nfrom autokeras.engine import head as head_module\nfrom autokeras.engine import serializable\nfrom autokeras.utils import utils\n\n\ndef feature_encoding_input(block):\n    """"""Fetch the column_types and column_names.\n\n    The values are fetched for FeatureEncoding from StructuredDataInput.\n    """"""\n    if not isinstance(block.inputs[0], nodes_module.StructuredDataInput):\n        raise TypeError(\'FeatureEncoding block can only be used \'\n                        \'with StructuredDataInput.\')\n    block.column_types = block.inputs[0].column_types\n    block.column_names = block.inputs[0].column_names\n\n\n# Compile the graph.\nCOMPILE_FUNCTIONS = {\n    blocks_module.StructuredDataBlock: [feature_encoding_input],\n    blocks_module.CategoricalToNumerical: [feature_encoding_input],\n}\n\n\ndef load_graph(filepath, custom_objects=None):\n    if custom_objects is None:\n        custom_objects = {}\n    with tf.keras.utils.custom_object_scope(custom_objects):\n        return Graph.from_config(utils.load_json(filepath))\n\n\nclass Graph(kerastuner.HyperModel, serializable.Serializable):\n    """"""A graph consists of connected Blocks, or Heads.\n\n    # Arguments\n        inputs: A list of input node(s) for the Graph.\n        outputs: A list of output node(s) for the Graph.\n        override_hps: A list of HyperParameters. The predefined HyperParameters that\n            will override the space of the Hyperparameters defined in the Hypermodels\n            with the same names.\n    """"""\n\n    def __init__(self, inputs=None, outputs=None, override_hps=None):\n        super().__init__()\n        self.inputs = nest.flatten(inputs)\n        self.outputs = nest.flatten(outputs)\n        self._node_to_id = {}\n        self._nodes = []\n        self.blocks = []\n        self._block_to_id = {}\n        if inputs and outputs:\n            self._build_network()\n        self.override_hps = override_hps or []\n\n    def compile(self):\n        """"""Share the information between blocks.""""""\n        for block in self.blocks:\n            for func in COMPILE_FUNCTIONS.get(block.__class__, []):\n                func(block)\n\n    def _register_hps(self, hp):\n        """"""Register the override HyperParameters for current HyperParameters.""""""\n        for single_hp in self.override_hps:\n            name = single_hp.name\n            if name not in hp.values:\n                hp._register(single_hp)\n                hp.values[name] = single_hp.default\n\n    def _build_network(self):\n        self._node_to_id = {}\n\n        # Recursively find all the interested nodes.\n        for input_node in self.inputs:\n            self._search_network(input_node, self.outputs, set(), set())\n        self._nodes = sorted(list(self._node_to_id.keys()),\n                             key=lambda x: self._node_to_id[x])\n\n        for node in (self.inputs + self.outputs):\n            if node not in self._node_to_id:\n                raise ValueError(\'Inputs and outputs not connected.\')\n\n        # Find the blocks.\n        blocks = []\n        for input_node in self._nodes:\n            for block in input_node.out_blocks:\n                if any([output_node in self._node_to_id\n                        for output_node in block.outputs]) and block not in blocks:\n                    blocks.append(block)\n\n        # Check if all the inputs of the blocks are set as inputs.\n        for block in blocks:\n            for input_node in block.inputs:\n                if input_node not in self._node_to_id:\n                    raise ValueError(\'A required input is missing for HyperModel \'\n                                     \'{name}.\'.format(name=block.name))\n\n        # Calculate the in degree of all the nodes\n        in_degree = [0] * len(self._nodes)\n        for node_id, node in enumerate(self._nodes):\n            in_degree[node_id] = len([\n                block for block in node.in_blocks if block in blocks])\n\n        # Add the blocks in topological order.\n        self.blocks = []\n        self._block_to_id = {}\n        while len(blocks) != 0:\n            new_added = []\n\n            # Collect blocks with in degree 0.\n            for block in blocks:\n                if any([in_degree[self._node_to_id[node]]\n                        for node in block.inputs]):\n                    continue\n                new_added.append(block)\n\n            # Remove the collected blocks from blocks.\n            for block in new_added:\n                blocks.remove(block)\n\n            for block in new_added:\n                # Add the collected blocks to the Graph.\n                self._add_block(block)\n\n                # Decrease the in degree of the output nodes.\n                for output_node in block.outputs:\n                    if output_node not in self._node_to_id:\n                        continue\n                    output_node_id = self._node_to_id[output_node]\n                    in_degree[output_node_id] -= 1\n\n    def _search_network(self, input_node, outputs, in_stack_nodes,\n                        visited_nodes):\n        visited_nodes.add(input_node)\n        in_stack_nodes.add(input_node)\n\n        outputs_reached = False\n        if input_node in outputs:\n            outputs_reached = True\n\n        for block in input_node.out_blocks:\n            for output_node in block.outputs:\n                if output_node in in_stack_nodes:\n                    raise ValueError(\'The network has a cycle.\')\n                if output_node not in visited_nodes:\n                    self._search_network(output_node, outputs, in_stack_nodes,\n                                         visited_nodes)\n                if output_node in self._node_to_id.keys():\n                    outputs_reached = True\n\n        if outputs_reached:\n            self._add_node(input_node)\n\n        in_stack_nodes.remove(input_node)\n\n    def _add_block(self, block):\n        if block not in self.blocks:\n            block_id = len(self.blocks)\n            self._block_to_id[block] = block_id\n            self.blocks.append(block)\n\n    def _add_node(self, input_node):\n        if input_node not in self._node_to_id:\n            self._node_to_id[input_node] = len(self._node_to_id)\n\n    def _get_block(self, name):\n        for block in self.blocks:\n            if block.name == name:\n                return block\n        raise ValueError(\'Cannot find block named {name}.\'.format(name=name))\n\n    def get_config(self):\n        blocks = [blocks_module.serialize(block) for block in self.blocks]\n        nodes = {str(self._node_to_id[node]): nodes_module.serialize(node)\n                 for node in self.inputs}\n        override_hps = [kerastuner.engine.hyperparameters.serialize(hp)\n                        for hp in self.override_hps]\n        block_inputs = {\n            str(block_id): [self._node_to_id[node]\n                            for node in block.inputs]\n            for block_id, block in enumerate(self.blocks)}\n        block_outputs = {\n            str(block_id): [self._node_to_id[node]\n                            for node in block.outputs]\n            for block_id, block in enumerate(self.blocks)}\n\n        outputs = [self._node_to_id[node] for node in self.outputs]\n\n        return {\n            \'override_hps\': override_hps,  # List [serialized].\n            \'blocks\': blocks,  # Dict {id: serialized}.\n            \'nodes\': nodes,  # Dict {id: serialized}.\n            \'outputs\': outputs,  # List of node_ids.\n            \'block_inputs\': block_inputs,  # Dict {id: List of node_ids}.\n            \'block_outputs\': block_outputs,  # Dict {id: List of node_ids}.\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        blocks = [blocks_module.deserialize(block) for block in config[\'blocks\']]\n        nodes = {int(node_id): nodes_module.deserialize(node)\n                 for node_id, node in config[\'nodes\'].items()}\n        override_hps = [kerastuner.engine.hyperparameters.deserialize(config)\n                        for config in config[\'override_hps\']]\n\n        inputs = [nodes[node_id] for node_id in nodes]\n        for block_id, block in enumerate(blocks):\n            input_nodes = [nodes[node_id]\n                           for node_id in config[\'block_inputs\'][str(block_id)]]\n            output_nodes = nest.flatten(block(input_nodes))\n            for output_node, node_id in zip(\n                    output_nodes, config[\'block_outputs\'][str(block_id)]):\n                nodes[node_id] = output_node\n\n        outputs = [nodes[node_id] for node_id in config[\'outputs\']]\n        return cls(inputs=inputs, outputs=outputs, override_hps=override_hps)\n\n    def build(self, hp):\n        """"""Build the HyperModel into a Keras Model.""""""\n        tf.keras.backend.clear_session()\n        self._register_hps(hp)\n        self.compile()\n        real_nodes = {}\n        for input_node in self.inputs:\n            node_id = self._node_to_id[input_node]\n            real_nodes[node_id] = input_node.build()\n        for block in self.blocks:\n            temp_inputs = [real_nodes[self._node_to_id[input_node]]\n                           for input_node in block.inputs]\n            outputs = block.build(hp, inputs=temp_inputs)\n            outputs = nest.flatten(outputs)\n            for output_node, real_output_node in zip(block.outputs, outputs):\n                real_nodes[self._node_to_id[output_node]] = real_output_node\n        model = tf.keras.Model(\n            [real_nodes[self._node_to_id[input_node]] for input_node in\n             self.inputs],\n            [real_nodes[self._node_to_id[output_node]] for output_node in\n             self.outputs])\n\n        return self._compile_keras_model(hp, model)\n\n    def _get_metrics(self):\n        metrics = {}\n        for output_node in self.outputs:\n            block = output_node.in_blocks[0]\n            if isinstance(block, head_module.Head):\n                metrics[block.name] = block.metrics\n        return metrics\n\n    def _get_loss(self):\n        loss = {}\n        for output_node in self.outputs:\n            block = output_node.in_blocks[0]\n            if isinstance(block, head_module.Head):\n                loss[block.name] = block.loss\n        return loss\n\n    def _compile_keras_model(self, hp, model):\n        # Specify hyperparameters from compile(...)\n        optimizer = hp.Choice(\'optimizer\',\n                              [\'adam\', \'adadelta\', \'sgd\'],\n                              default=\'adam\')\n\n        model.compile(optimizer=optimizer,\n                      metrics=self._get_metrics(),\n                      loss=self._get_loss())\n\n        return model\n\n    def save(self, filepath):\n        utils.save_json(filepath, self.get_config())\n'"
autokeras/keras_layers.py,8,"b'import inspect\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.python.keras.layers.preprocessing import index_lookup\nfrom tensorflow.python.util import nest\n\nCombinerPreprocessingLayer = inspect.getmro(preprocessing.Normalization)[1]\nCombiner = inspect.getmro(preprocessing.Normalization()._combiner.__class__)[1]\n\nINT = \'int\'\nNONE = \'none\'\nONE_HOT = \'one-hot\'\n\n\nclass MultiColumnCategoricalEncoding(preprocessing.PreprocessingLayer):\n    """"""Encode the categorical features to numerical features.\n\n    # Arguments\n        encoding: A list of strings, which has the same number of elements as the\n            columns in the structured data. Each of the strings specifies the\n            encoding method used for the corresponding column. Use \'int\' for\n            categorical columns and \'none\' for numerical columns.\n    """"""\n\n    # TODO: Support one-hot encoding.\n    # TODO: Support frequency encoding.\n\n    def __init__(self, encoding, **kwargs):\n        super().__init__(**kwargs)\n        self.encoding = encoding\n        self.encoding_layers = []\n        for encoding in self.encoding:\n            if encoding == NONE:\n                self.encoding_layers.append(None)\n            elif encoding == INT:\n                self.encoding_layers.append(index_lookup.IndexLookup())\n            elif encoding == ONE_HOT:\n                self.encoding_layers.append(None)\n\n    def build(self, input_shape):\n        for encoding_layer in self.encoding_layers:\n            if encoding_layer is not None:\n                encoding_layer.build(tf.TensorShape([1]))\n\n    def call(self, inputs):\n        input_nodes = nest.flatten(inputs)[0]\n        split_inputs = tf.split(input_nodes, [1] * len(self.encoding), axis=-1)\n        output_nodes = []\n        for input_node, encoding_layer in zip(split_inputs, self.encoding_layers):\n            if encoding_layer is None:\n                number = tf.strings.to_number(input_node, tf.float32)\n                # Replace NaN with 0.\n                imputed = tf.where(tf.math.is_nan(number),\n                                   tf.zeros_like(number),\n                                   number)\n                output_nodes.append(imputed)\n            else:\n                output_nodes.append(tf.cast(encoding_layer(input_node), tf.float32))\n        if len(output_nodes) == 1:\n            return output_nodes[0]\n        return tf.keras.layers.Concatenate()(output_nodes)\n\n    def adapt(self, data):\n        for index, encoding_layer in enumerate(self.encoding_layers):\n            if encoding_layer is None:\n                continue\n            data_column = data.map(lambda x: tf.slice(x, [0, index], [-1, 1]))\n            encoding_layer.adapt(data_column)\n\n    def get_config(self):\n        config = {\n            \'encoding\': self.encoding,\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nCUSTOM_OBJECTS = {\n    \'MultiColumnCategoricalEncoding\': MultiColumnCategoricalEncoding,\n    \'IndexLookup\': index_lookup.IndexLookup,\n}\n'"
autokeras/nodes.py,9,"b'import tensorflow as tf\n\nfrom autokeras import adapters\nfrom autokeras.engine import io_hypermodel\nfrom autokeras.engine import node as node_module\n\n\ndef serialize(obj):\n    return tf.keras.utils.serialize_keras_object(obj)\n\n\ndef deserialize(config, custom_objects=None):\n    return tf.keras.utils.deserialize_keras_object(\n        config,\n        module_objects=globals(),\n        custom_objects=custom_objects,\n        printable_module_name=\'nodes\')\n\n\nclass Input(node_module.Node, io_hypermodel.IOHyperModel):\n    """"""Input node for tensor data.\n\n    The data should be numpy.ndarray or tf.data.Dataset.\n    """"""\n\n    def build(self):\n        return tf.keras.Input(shape=self.shape, dtype=tf.float32)\n\n    def get_adapter(self):\n        return adapters.InputAdapter()\n\n    def config_from_adapter(self, adapter):\n        self.shape = adapter.shape\n\n\nclass ImageInput(Input):\n    """"""Input node for image data.\n\n    The input data should be numpy.ndarray or tf.data.Dataset. The shape of the data\n    should be should be (samples, width, height) or\n    (samples, width, height, channels).\n    """"""\n\n    def get_adapter(self):\n        return adapters.ImageInputAdapter()\n\n\nclass TextInput(Input):\n    """"""Input node for text data.\n\n    The input data should be numpy.ndarray or tf.data.Dataset. The data should be\n    one-dimensional. Each element in the data should be a string which is a full\n    sentence.\n    """"""\n\n    def build(self):\n        return tf.keras.Input(shape=self.shape, dtype=tf.string)\n\n    def get_adapter(self):\n        return adapters.TextInputAdapter()\n\n\nclass StructuredDataInput(Input):\n    """"""Input node for structured data.\n\n    The input data should be numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n    The data should be two-dimensional with numerical or categorical values.\n\n    # Arguments\n        column_names: A list of strings specifying the names of the columns. The\n            length of the list should be equal to the number of columns of the data.\n            Defaults to None. If None, it will be obtained from the header of the csv\n            file or the pandas.DataFrame.\n        column_types: Dict. The keys are the column names. The values should either\n            be \'numerical\' or \'categorical\', indicating the type of that column.\n            Defaults to None. If not None, the column_names need to be specified.\n            If None, it will be inferred from the data. A column will be judged as\n            categorical if the number of different values is less than 5% of the\n            number of instances.\n    """"""\n\n    def __init__(self, column_names=None, column_types=None, **kwargs):\n        super().__init__(**kwargs)\n        self.column_names = column_names\n        self.column_types = column_types\n\n    def build(self):\n        return tf.keras.Input(shape=self.shape, dtype=tf.string)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'column_names\': self.column_names,\n            \'column_types\': self.column_types,\n        })\n        return config\n\n    def get_adapter(self):\n        return adapters.StructuredDataInputAdapter(\n            self.column_names,\n            self.column_types\n        )\n\n    def config_from_adapter(self, adapter):\n        super().config_from_adapter(adapter)\n        self.column_names = adapter.column_names\n        self.column_types = adapter.column_types\n\n\nclass TimeseriesInput(Input):\n    """"""Input node for timeseries data.\n\n    # Arguments\n        lookback: Int. The range of history steps to consider for each prediction.\n            For example, if lookback=n, the data in the range of [i - n, i - 1]\n            is used to predict the value of step i. If unspecified, it will be tuned\n            automatically.\n        column_names: A list of strings specifying the names of the columns. The\n            length of the list should be equal to the number of columns of the data.\n            Defaults to None. If None, it will be obtained from the header of the csv\n            file or the pandas.DataFrame.\n        column_types: Dict. The keys are the column names. The values should either\n            be \'numerical\' or \'categorical\', indicating the type of that column.\n            Defaults to None. If not None, the column_names need to be specified.\n            If None, it will be inferred from the data. A column will be judged as\n            categorical if the number of different values is less than 5% of the\n            number of instances.\n    """"""\n\n    def __init__(self,\n                 lookback=None,\n                 column_names=None,\n                 column_types=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.lookback = lookback\n        self.column_names = column_names\n        self.column_types = column_types\n\n    def build(self):\n        if len(self.shape) == 1:\n            self.shape = (self.lookback, self.shape[0],)\n        return tf.keras.Input(shape=self.shape, dtype=tf.float32)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'lookback\': self.lookback,\n            \'column_names\': self.column_names,\n            \'column_types\': self.column_types\n        })\n        return config\n\n    def get_adapter(self):\n        return adapters.TimeseriesInputAdapter(lookback=self.lookback,\n                                               column_names=self.column_names,\n                                               column_types=self.column_types)\n\n    def config_from_adapter(self, adapter):\n        super().config_from_adapter(adapter)\n        self.lookback = adapter.lookback\n        self.column_names = adapter.column_names\n        self.column_types = adapter.column_types\n'"
docker/pre_commit.py,0,"b'from subprocess import CalledProcessError\nfrom subprocess import check_call\n\n\ndef check_bash_call(string):\n    check_call([""bash"", ""-c"", string])\n\n\ndef _run_format_and_flake8():\n    files_changed = False\n\n    try:\n        check_bash_call(""isort -sl -rc -c"")\n    except CalledProcessError:\n        check_bash_call(""isort -y -sl -rc"")\n        files_changed = True\n\n    try:\n        check_bash_call(""autopep8 --exclude=\\""./docs/py/*.py\\"" -r -i ."")\n    except CalledProcessError as error:\n        if error.returncode == 2:\n            files_changed = True\n        else:\n            # there was another type of error\n            raise\n\n    if files_changed:\n        print(""Some files have changed."")\n        print(""Please do git add and git commit again"")\n    else:\n        print(""No formatting needed."")\n\n    print(""Running flake8."")\n    check_bash_call(""flake8"")\n    print(""Done"")\n\n    if files_changed:\n        exit(1)\n\n\ndef run_format_and_flake8():\n    try:\n        _run_format_and_flake8()\n    except CalledProcessError as error:\n        print(""Pre-commit returned exit code"", error.returncode)\n        exit(error.returncode)\n\n\nif __name__ == ""__main__"":\n    run_format_and_flake8()\n'"
docs/autogen.py,0,"b'import os\nimport pathlib\nimport shutil\n\nimport keras_autodoc\nimport tutobooks\n\nPAGES = {\n    \'image_classifier.md\': [\n        \'autokeras.ImageClassifier\',\n        \'autokeras.ImageClassifier.fit\',\n        \'autokeras.ImageClassifier.predict\',\n        \'autokeras.ImageClassifier.evaluate\',\n        \'autokeras.ImageClassifier.export_model\',\n    ],\n    \'image_regressor.md\': [\n        \'autokeras.ImageRegressor\',\n        \'autokeras.ImageRegressor.fit\',\n        \'autokeras.ImageRegressor.predict\',\n        \'autokeras.ImageRegressor.evaluate\',\n        \'autokeras.ImageRegressor.export_model\',\n    ],\n    \'text_classifier.md\': [\n        \'autokeras.TextClassifier\',\n        \'autokeras.TextClassifier.fit\',\n        \'autokeras.TextClassifier.predict\',\n        \'autokeras.TextClassifier.evaluate\',\n        \'autokeras.TextClassifier.export_model\',\n    ],\n    \'text_regressor.md\': [\n        \'autokeras.TextRegressor\',\n        \'autokeras.TextRegressor.fit\',\n        \'autokeras.TextRegressor.predict\',\n        \'autokeras.TextRegressor.evaluate\',\n        \'autokeras.TextRegressor.export_model\',\n    ],\n    \'structured_data_classifier.md\': [\n        \'autokeras.StructuredDataClassifier\',\n        \'autokeras.StructuredDataClassifier.fit\',\n        \'autokeras.StructuredDataClassifier.predict\',\n        \'autokeras.StructuredDataClassifier.evaluate\',\n        \'autokeras.StructuredDataClassifier.export_model\',\n    ],\n    \'structured_data_regressor.md\': [\n        \'autokeras.StructuredDataRegressor\',\n        \'autokeras.StructuredDataRegressor.fit\',\n        \'autokeras.StructuredDataRegressor.predict\',\n        \'autokeras.StructuredDataRegressor.evaluate\',\n        \'autokeras.StructuredDataRegressor.export_model\',\n    ],\n    \'auto_model.md\': [\n        \'autokeras.AutoModel\',\n        \'autokeras.AutoModel.fit\',\n        \'autokeras.AutoModel.predict\',\n        \'autokeras.AutoModel.evaluate\',\n        \'autokeras.AutoModel.export_model\',\n    ],\n    \'base.md\': [\n        \'autokeras.Node\',\n        \'autokeras.Block\',\n        \'autokeras.Block.build\',\n        \'autokeras.Head\',\n    ],\n    \'node.md\': [\n        \'autokeras.ImageInput\',\n        \'autokeras.Input\',\n        \'autokeras.StructuredDataInput\',\n        \'autokeras.TextInput\',\n    ],\n    \'block.md\': [\n        \'autokeras.ConvBlock\',\n        \'autokeras.DenseBlock\',\n        \'autokeras.Embedding\',\n        \'autokeras.Merge\',\n        \'autokeras.ResNetBlock\',\n        \'autokeras.RNNBlock\',\n        \'autokeras.SpatialReduction\',\n        \'autokeras.TemporalReduction\',\n        \'autokeras.XceptionBlock\',\n        \'autokeras.ImageBlock\',\n        \'autokeras.StructuredDataBlock\',\n        \'autokeras.TextBlock\',\n        \'autokeras.ImageAugmentation\',\n        \'autokeras.Normalization\',\n        \'autokeras.TextToIntSequence\',\n        \'autokeras.TextToNgramVector\',\n        \'autokeras.CategoricalToNumerical\',\n        \'autokeras.ClassificationHead\',\n        \'autokeras.RegressionHead\',\n    ],\n}\n\n\naliases_needed = [\n    \'tensorflow.keras.callbacks.Callback\',\n    \'tensorflow.keras.losses.Loss\',\n    \'tensorflow.keras.metrics.Metric\',\n    \'tensorflow.data.Dataset\'\n]\n\n\nROOT = \'http://autokeras.com/\'\n\nautokeras_dir = pathlib.Path(__file__).resolve().parents[1]\n\n\ndef py_to_nb_md(dest_dir):\n    for file_path in os.listdir(\'py/\'):\n        dir_path = \'py\'\n        file_name = file_path\n        py_path = os.path.join(dir_path, file_path)\n        file_name_no_ext = os.path.splitext(file_name)[0]\n        ext = os.path.splitext(file_name)[1]\n\n        if ext != \'.py\':\n            continue\n\n        nb_path = os.path.join(\'ipynb\',  file_name_no_ext + \'.ipynb\')\n        md_path = os.path.join(dest_dir, \'tutorial\', file_name_no_ext + \'.md\')\n\n        tutobooks.py_to_md(py_path, nb_path, md_path, \'templates/img\')\n\n        github_repo_dir = \'keras-team/autokeras/tree/master/docs/\'\n        with open(md_path, \'r\') as md_file:\n            button_lines = [\n                \':material-link: \'\n                ""[**View in Colab**](https://colab.research.google.com/github/""\n                + github_repo_dir\n                + ""ipynb/""\n                + file_name_no_ext + "".ipynb""\n                + "")   &nbsp; &nbsp;""\n                # + \'<span class=""k-dot"">\xe2\x80\xa2</span>\'\n                + \':octicons-octoface: \'\n                ""[**GitHub source**](https://github.com/"" + github_repo_dir + ""py/""\n                + file_name_no_ext + "".py)"",\n                ""\\n"",\n            ]\n            md_content = \'\'.join(button_lines) + \'\\n\' + md_file.read()\n\n        with open(md_path, \'w\') as md_file:\n            md_file.write(md_content)\n\n\ndef generate(dest_dir):\n    template_dir = autokeras_dir / \'docs\' / \'templates\'\n    doc_generator = keras_autodoc.DocumentationGenerator(\n        PAGES,\n        \'https://github.com/keras-team/autokeras/blob/master\',\n        template_dir,\n        autokeras_dir / \'examples\',\n        extra_aliases=aliases_needed,\n    )\n    doc_generator.generate(dest_dir)\n    readme = (autokeras_dir / \'README.md\').read_text()\n    index = (template_dir / \'index.md\').read_text()\n    index = index.replace(\'{{autogenerated}}\', readme[readme.find(\'##\'):])\n    (dest_dir / \'index.md\').write_text(index, encoding=\'utf-8\')\n    shutil.copyfile(autokeras_dir / \'.github\' / \'CONTRIBUTING.md\',\n                    dest_dir / \'contributing.md\')\n\n    py_to_nb_md(dest_dir)\n\n\nif __name__ == \'__main__\':\n    generate(autokeras_dir / \'docs\' / \'sources\')\n'"
docs/tutobooks.py,0,"b'""""""Keras tutobooks implementation.\n\nA tutobook is a tutorial available simultaneously as a notebook,\nas a Python script, and as a nicely rendered webpage.\n\nIts source-of-truth (for manual edition and version control) is\nits Python script form, but you can also create one by starting\nfrom a notebook and converting it with the command `nb2py`.\n\nText cells are stored in markdown-formatted comment blocks.\nthe first line (starting with "" * 3) may optionally contain a special\nannotation, one of:\n\n- invisible: do not render this block.\n- shell: execute this block while prefixing each line with `!`.\n\nThe script form should start with a header with the following fields:\nTitle:\nAuthor: (could be `Authors`: as well, and may contain markdown links)\nDate created: (date in yyyy/mm/dd format)\nLast modified: (date in yyyy/mm/dd format)\nDescription: (one-line text description)\n\n## How to add a new code example to Keras.io\n\nYou would typically start from an existing notebook.\n\nSave it to disk (let\'s say as `path_to_your_nb.ipynb`).\n`cd` to the `keras-io/scripts/` directory.\n\nThen run:\n\n```\npython tutobooks nb2py path_to_your_nb.ipynb ../examples/your_example.py\n```\n\nThis will create the file `examples/your_example.py`. Open it,\nfill in the headers, and generally edit it so that it looks nice.\n\nNOTE THAT THE CONVERSION SCRIPT MAY MAKE MISTAKES IN ITS ATTEMPTS\nTO SHORTEN LINES. MAKE SURE TO PROOFREAD THE GENERATED .py IN FULL.\nOr alternatively, make sure to keep your lines reasonably-sized (<90 char)\nto start with, so that the script won\'t have to shorten them.\n\nYou can then preview what it looks like when converted back again\nto ipynb by running:\n\n```\npython tutobooks py2nb ../examples/your_example.py preview.ipynb\n```\n\nNOTE THAT THIS COMMAND WILL ERROR OUT IF ANY CELLS TAKES TOO LONG\nTO EXECUTE. In that case, make your code lighter/faster.\nRemember that examples are meant to demonstrate workflows, not\ntrain state-of-the-art models. They should\nstay very lightweight.\n\nOpen the generated `preview.ipynb` and make sure it looks like what\nyou expect. If not, keep editing `your_example.py` until it does.\n\nFinally, submit a PR adding `examples/your_example.py`.\n""""""\nimport json\nimport os\nimport random\nimport shutil\nimport sys\nfrom pathlib import Path\n\nTIMEOUT = 60 * 60\nMAX_LOC = 300\n\n\ndef nb_to_py(nb_path, py_path):\n    f = open(nb_path)\n    content = f.read()\n    f.close()\n    nb = json.loads(content)\n    py = \'""""""\\n\'\n    py += ""Title: FILLME\\n""\n    py += ""Author: FILLME\\n""\n    py += ""Date created: FILLME\\n""\n    py += ""Last modified: FILLME\\n""\n    py += ""Description: FILLME\\n""\n    py += \'""""""\\n\'\n    for cell in nb[""cells""]:\n        if cell[""cell_type""] == ""code"":\n            # Is it a shell cell?\n            if (cell[""source""] and\n                    cell[""source""][0] and\n                    cell[""source""][0][0] == ""!""):\n                # It\'s a shell cell\n                py += \'""""""shell\\n\'\n                py += """".join(cell[""source""]) + ""\\n""\n                py += \'""""""\\n\\n\'\n            else:\n                # It\'s a Python cell\n                py += """".join(cell[""source""]) + ""\\n\\n""\n        elif cell[""cell_type""] == ""markdown"":\n            py += \'""""""\\n\'\n            py += """".join(cell[""source""]) + ""\\n""\n            py += \'""""""\\n\\n\'\n    # Save file\n    f = open(py_path, ""w"")\n    f.write(py)\n    f.close()\n    # Format file with Black\n    os.system(""black "" + py_path)\n    # Shorten lines\n    py = open(py_path).read()\n    try:\n        py = _shorten_lines(py)\n    finally:\n        f = open(py_path, ""w"")\n        f.write(py)\n        f.close()\n\n\ndef py_to_nb(py_path, nb_path, fill_outputs=True):\n    f = open(py_path)\n    py = f.read()\n    f.close()\n    # validate(py)\n\n    # header, _, py, tag = _get_next_script_element(py)\n    # attributes = _parse_header(header)\n    cells = []\n    loc = 0\n    # Write first header cell\n    # header_cell = {\n    # ""cell_type"": ""markdown"",\n    # ""source"": [\n    # ""# "" + attributes[""title""] + ""\\n"",\n    # ""\\n"",\n    # ""**"" + attributes[""auth_field""] + "":** "" + attributes[""author""] + ""<br>\\n"",\n    # ""**Date created:** "" + attributes[""date_created""] + ""<br>\\n"",\n    # ""**Last modified:** "" + attributes[""last_modified""] + ""<br>\\n"",\n    # ""**Description:** "" + attributes[""description""],\n    # ],\n    # ""metadata"": {""colab_type"": ""text""},\n    # }\n    # cells.append(header_cell)\n    while py:\n        e, cell_type, py, tag = _get_next_script_element(py)\n        lines = e.split(""\\n"")\n\n        if all(l == """" for l in lines):\n            continue\n\n        if lines and not lines[0]:\n            lines = lines[1:]\n        source = [l + ""\\n"" for l in lines]\n        # Drop last newline char\n        if source and not source[-1].strip():\n            source = source[:-1]\n        if tag == ""shell"":\n            source = [""!"" + l for l in source]\n            cell_type = ""code""\n        if tag != ""invisible"" and source:\n            cell = {""cell_type"": cell_type, ""source"": source}\n            if cell_type == ""code"":\n                cell[""outputs""] = []\n                cell[""metadata""] = {""colab_type"": ""code""}\n                cell[""execution_count""] = 0\n                loc += _count_locs(source)\n            else:\n                cell[""metadata""] = {""colab_type"": ""text""}\n            cells.append(cell)\n    notebook = {}\n    for key in NB_BASE.keys():\n        notebook[key] = NB_BASE[key]\n    notebook[""metadata""][""colab""][""name""] = str(py_path).split(""/"")[-1][:-3]\n    notebook[""cells""] = cells\n    if loc > MAX_LOC:\n        raise ValueError(\n            \'Found %d lines of code, but expected fewer than %d\'\n            % (loc, MAX_LOC))\n\n    f = open(nb_path, ""w"")\n    f.write(json.dumps(notebook, indent=1, sort_keys=True))\n    f.close()\n    if fill_outputs:\n        print(""Generating ipynb"")\n        parent_dir = Path(nb_path).parent\n        current_files = os.listdir(parent_dir)\n        try:\n            os.system(\n                ""jupyter nbconvert --to notebook --execute --debug ""\n                + str(nb_path)\n                + "" --inplace""\n                + "" --ExecutePreprocessor.timeout=""\n                + str(TIMEOUT)\n            )\n        finally:\n            new_files = os.listdir(parent_dir)\n            for fname in new_files:\n                if fname not in current_files:\n                    fpath = parent_dir / fname\n                    if os.path.isdir(fpath):\n                        print(""Removing created folder:"", fname)\n                        shutil.rmtree(fpath)\n                    else:\n                        print(""Removing created file:"", fname)\n                        os.remove(fpath)\n\n\ndef nb_to_md(nb_path, md_path, img_dir, working_dir=None):\n    img_exts = (""png"", ""jpg"", ""jpeg"")\n    # Assumes an already populated notebook.\n    assert str(md_path).endswith("".md"")\n    current_dir = os.getcwd()\n    original_img_dir = str(img_dir)\n    if original_img_dir.endswith(""/""):\n        original_img_dir = original_img_dir[:-1]\n    img_dir = os.path.abspath(img_dir)\n    nb_path = os.path.abspath(nb_path)\n    nb_fname = str(nb_path).split(""/"")[-1]\n\n    del_working_dir = False\n    if working_dir is None:\n        del_working_dir = True\n        working_dir = ""tmp_"" + str(random.randint(1e6, 1e7))\n    if not os.path.exists(working_dir):\n        os.makedirs(working_dir)\n    print(""Using working_dir:"", working_dir)\n\n    os.chdir(working_dir)\n    shutil.copyfile(nb_path, nb_fname)\n\n    md_name = str(md_path).split(""/"")[-1][:-3]\n    target_md = md_name + "".md""\n    img_dir = Path(img_dir) / md_name\n    if not os.path.exists(img_dir):\n        os.makedirs(img_dir)\n\n    os.system(\n        # ""jupyter nbconvert --to markdown --execute --debug ""\n        ""jupyter nbconvert --to markdown ""\n        + nb_fname\n        + "" --output ""\n        + target_md\n        # + "" --ExecutePreprocessor.timeout=""\n        # + str(TIMEOUT)\n    )\n    tmp_img_dir = md_name + ""_files""\n    if os.path.exists(tmp_img_dir):\n        for fname in os.listdir(tmp_img_dir):\n            if fname.endswith(img_exts):\n                src = Path(tmp_img_dir) / fname\n                target = Path(img_dir) / fname\n                print(""copy"", src, ""to"", target)\n                shutil.copyfile(src, target)\n    os.chdir(current_dir)\n    md_content = open(Path(working_dir) / (md_name + "".md"")).read()\n    for ext in img_exts:\n        md_content = md_content.replace(\n            ""!["" + ext + ""]("" + md_name + ""_files"",\n            ""!["" + ext + ""]("" + original_img_dir + ""/"" + md_name,\n        )\n    md_content = _make_output_code_blocks(md_content)\n    open(md_path, ""w"").write(md_content)\n    if del_working_dir:\n        shutil.rmtree(working_dir)\n\n\ndef py_to_md(py_path, nb_path, md_path, img_dir, working_dir=None):\n    py_to_nb(py_path, nb_path, fill_outputs=False)\n    nb_to_md(nb_path, md_path, img_dir, working_dir=working_dir)\n\n\ndef validate(py):\n    """"""Validate the format of a tutobook script.\n\n    Specifically:\n        - validate headers\n        - validate style with black\n    """"""\n    lines = py.split(""\\n"")\n    if not lines[0].startswith(\'""""""\'):\n        raise ValueError(\'Missing `""""""`-fenced header at top of script.\')\n    if not lines[1].startswith(""Title: ""):\n        raise ValueError(""Missing `Title:` field."")\n    if not lines[2].startswith(""Author: "") and not lines[2].startswith(""Authors: ""):\n        raise ValueError(""Missing `Author:` field."")\n    if not lines[3].startswith(""Date created: ""):\n        raise ValueError(""Missing `Date created:` field."")\n    if not lines[4].startswith(""Last modified: ""):\n        raise ValueError(""Missing `Last modified:` field."")\n    if not lines[5].startswith(""Description: ""):\n        raise ValueError(""Missing `Description:` field."")\n    description = lines[5][len(""Description: ""):]\n    if not description:\n        raise ValueError(""Missing `Description:` field content."")\n    if not description[0] == description[0].upper():\n        raise ValueError(""Description field content must be capitalized."")\n    if not description[-1] == ""."":\n        raise ValueError(""Description field content must end with a period."")\n    if len(description) > 100:\n        raise ValueError(""Description field content must be less than 100 chars."")\n    for i, line in enumerate(lines):\n        if line.startswith(\'""""""\') and line.endswith(\'""""""\') and len(line) > 3:\n            raise ValueError(\n                \'Do not use single line `""""""`-fenced comments. \'\n                ""Encountered at line %d"" % (i,)\n            )\n    for i, line in enumerate(lines):\n        if line.endswith("" ""):\n            raise ValueError(\n                ""Found trailing space on line %d; line: `%s`"" % (i, line))\n    # Validate style with black\n    fpath = ""/tmp/"" + str(random.randint(1e6, 1e7)) + "".py""\n    f = open(fpath, ""w"")\n    pre_formatting = ""\\n"".join(lines)\n    f.write(pre_formatting)\n    f.close()\n    os.system(""black "" + fpath)\n    f = open(fpath)\n    formatted = f.read()\n    f.close()\n    os.remove(fpath)\n    if formatted != pre_formatting:\n        raise ValueError(\n            ""You python file did not follow `black` conventions. ""\n            ""Run `black your_file.py` to autoformat it.""\n        )\n\n\ndef _count_locs(lines):\n    loc = 0\n    string_open = False\n    for line in lines:\n        line = line.strip()\n        if not line or line.startswith(\'#\'):\n            continue\n        if not string_open:\n            if not line.startswith(\'""""""\'):\n                loc += 1\n            else:\n                if not line.endswith(\'""""""\'):\n                    string_open = True\n        else:\n            if line.startswith(\'""""""\'):\n                string_open = False\n    return loc\n\n\ndef _shorten_lines(py):\n    max_len = 90\n    lines = []\n    for line in py.split(""\\n""):\n        if len(line) <= max_len:\n            lines.append(line)\n            continue\n        i = 0\n        while len(line) > max_len:\n            line = line.lstrip()\n            if "" "" not in line[1:]:\n                lines.append(line)\n                break\n            else:\n                short_line = line[:max_len]\n                line = line[max_len:]\n                if "" "" in short_line:\n                    reversed_short_line = short_line[::-1]\n                    index = reversed_short_line.find("" "") + 1\n                    line = short_line[-index:] + line\n                    short_line = short_line[:-index]\n\n                lines.append(short_line.lstrip())\n            i += 1\n            if i > 10:\n                raise\n        lines.append(line.lstrip())\n    return ""\\n"".join(lines)\n\n\ndef _get_next_script_element(py):\n    lines = py.split(""\\n"")\n    assert lines\n    elines = []\n    i = 0\n    tag = None\n    if lines[0].startswith(\'""""""\'):\n        assert len(lines) >= 2\n        etype = ""markdown""\n        if len(lines[0]) > 3:\n            tag = lines[0][3:]\n            if tag not in [""shell"", ""invisible""]:\n                raise ValueError(""Found unknown cell tag:"", tag)\n        lines = lines[1:]\n    else:\n        etype = ""code""\n\n    for i, line in enumerate(lines):\n        if line.startswith(\'""""""\'):\n            break\n        else:\n            elines.append(line)\n\n    if etype == ""markdown"":\n        py = ""\\n"".join(lines[i + 1:])\n    else:\n        py = ""\\n"".join(lines[i:])\n    e = ""\\n"".join(elines)\n\n    return e, etype, py, tag\n\n\ndef _parse_header(header):\n    lines = header.split(""\\n"")\n    title = lines[0][len(""Title: ""):]\n    author_line = lines[1]\n    if author_line.startswith(""Authors""):\n        author = author_line[len(""Authors: ""):]\n        auth_field = ""Authors""\n    else:\n        author = author_line[len(""Author: ""):]\n        auth_field = ""Author""\n    date_created = lines[2][len(""Date created: ""):]\n    last_modified = lines[3][len(""Last modified: ""):]\n    description = lines[4][len(""Description: ""):]\n    return {\n        ""title"": title,\n        ""author"": author,\n        ""auth_field"": auth_field,\n        ""date_created"": date_created,\n        ""last_modified"": last_modified,\n        ""description"": description,\n    }\n\n\ndef _make_output_code_blocks(md):\n    lines = md.split(""\\n"")\n    output_lines = []\n    final_lines = []\n    is_inside_backticks = False\n\n    def is_output_line(line, prev_line, output_lines):\n        if line.startswith(""    "") and len(line) >= 5:\n            if output_lines or (lines[i - 1].strip() == """" and line.strip()):\n                return True\n        return False\n\n    def flush(output_lines, final_lines):\n        final_lines.append(\'<div class=""k-default-codeblock"">\')\n        final_lines.append(""```"")\n        if len(output_lines) == 1:\n            line = output_lines[0]\n            final_lines.append(line[4:])\n        else:\n            for line in output_lines:\n                final_lines.append(line[4:])\n        final_lines.append(""```"")\n        final_lines.append(""</div>"")\n\n    for i, line in enumerate(lines):\n        if line.startswith(""```""):\n            is_inside_backticks = not is_inside_backticks\n            final_lines.append(line)\n            continue\n\n        if is_inside_backticks:\n            final_lines.append(line)\n            continue\n\n        if i > 0 and is_output_line(line, lines[-1], output_lines):\n            output_lines.append(line)\n        elif not line:\n            if output_lines:\n                if output_lines[-1]:\n                    output_lines.append(line)\n            else:\n                final_lines.append(line)\n        else:\n            if output_lines:\n                flush(output_lines, final_lines)\n                output_lines = []\n            final_lines.append(line)\n    if output_lines:\n        flush(output_lines, final_lines)\n    return ""\\n"".join(final_lines)\n\n\nNB_BASE = {\n    ""metadata"": {\n        ""colab"": {\n            ""collapsed_sections"": [],\n            ""name"": """",  # FILL ME\n            ""private_outputs"": False,\n            ""provenance"": [],\n            ""toc_visible"": True,\n        },\n        ""kernelspec"": {\n            ""display_name"": ""Python 3"",\n            ""language"": ""python"",\n            ""name"": ""python3"",\n        },\n        ""language_info"": {\n            ""codemirror_mode"": {""name"": ""ipython"", ""version"": 3},\n            ""file_extension"": "".py"",\n            ""mimetype"": ""text/x-python"",\n            ""name"": ""python"",\n            ""nbconvert_exporter"": ""python"",\n            ""pygments_lexer"": ""ipython3"",\n            ""version"": ""3.7.0"",\n        },\n    },\n    ""nbformat"": 4,\n    ""nbformat_minor"": 0,\n}\n\n\nif __name__ == ""__main__"":\n    cmd = sys.argv[1]\n    if cmd not in {""nb2py"", ""py2nb""}:\n        raise ValueError(\n            ""Specify a command: either ""\n            ""`nb2py source_filename.ipynb target_filename.py` or ""\n            ""`py2nb source_filename.py target_file name.ipynb""\n        )\n    if len(sys.argv) < 4:\n        raise ValueError(""Specify a source filename and a target filename"")\n    source = sys.argv[2]\n    target = sys.argv[3]\n\n    if cmd == ""py2nb"":\n        if not source.endswith("".py""):\n            raise ValueError(\n                ""The source filename should be a Python file. Got:"", source\n            )\n        if not target.endswith("".ipynb""):\n            raise ValueError(\n                ""The target filename should be a notebook file. Got:"", target\n            )\n        py_to_nb(source, target)\n    if cmd == ""nb2py"":\n        if not source.endswith("".ipynb""):\n            raise ValueError(\n                ""The source filename should be a notebook file. Got:"", source\n            )\n        if not target.endswith("".py""):\n            raise ValueError(\n                ""The target filename should be a Python file. Got:"", target\n            )\n        nb_to_py(source, target)\n'"
examples/imdb.py,2,"b'""""""\nSearch for a good model for the\n[IMDB](\nhttps://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) dataset.\n""""""\nimport numpy as np\nimport tensorflow as tf\n\nimport autokeras as ak\n\n\ndef imdb_raw():\n    max_features = 20000\n    index_offset = 3  # word index offset\n\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(\n        num_words=max_features,\n        index_from=index_offset)\n    x_train = x_train\n    y_train = y_train.reshape(-1, 1)\n    x_test = x_test\n    y_test = y_test.reshape(-1, 1)\n\n    word_to_id = tf.keras.datasets.imdb.get_word_index()\n    word_to_id = {k: (v + index_offset) for k, v in word_to_id.items()}\n    word_to_id[""<PAD>""] = 0\n    word_to_id[""<START>""] = 1\n    word_to_id[""<UNK>""] = 2\n\n    id_to_word = {value: key for key, value in word_to_id.items()}\n    x_train = list(map(lambda sentence: \' \'.join(\n        id_to_word[i] for i in sentence), x_train))\n    x_test = list(map(lambda sentence: \' \'.join(\n        id_to_word[i] for i in sentence), x_test))\n    x_train = np.array(x_train, dtype=np.str)\n    x_test = np.array(x_test, dtype=np.str)\n    return (x_train, y_train), (x_test, y_test)\n\n\n# Prepare the data.\n(x_train, y_train), (x_test, y_test) = imdb_raw()\nprint(x_train.shape)  # (25000,)\nprint(y_train.shape)  # (25000, 1)\nprint(x_train[0][:50])  # <START> this film was just brilliant casting <UNK>\n\n# Initialize the TextClassifier\nclf = ak.TextClassifier(max_trials=3)\n# Search for the best model.\nclf.fit(x_train, y_train, epochs=2)\n# Evaluate on the testing data.\nprint(\'Accuracy: {accuracy}\'.format(accuracy=clf.evaluate(x_test, y_test)))\n'"
examples/mnist.py,0,"b'""""""\nSearch for a good model for the\n[MNIST](https://keras.io/datasets/#mnist-database-of-handwritten-digits) dataset.\n""""""\n\nfrom tensorflow.keras.datasets import mnist\n\nimport autokeras as ak\n\n# Prepare the dataset.\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint(x_train.shape)  # (60000, 28, 28)\nprint(y_train.shape)  # (60000,)\nprint(y_train[:3])  # array([7, 2, 1], dtype=uint8)\n\n# Initialize the ImageClassifier.\nclf = ak.ImageClassifier(max_trials=3)\n# Search for the best model.\nclf.fit(x_train, y_train, epochs=10)\n# Evaluate on the testing data.\nprint(\'Accuracy: {accuracy}\'.format(\n    accuracy=clf.evaluate(x_test, y_test)))\n'"
examples/titanic.py,0,"b'""""""\nSearch for a good model for the [Titanic](https://www.kaggle.com/c/titanic) dataset.\nFirst, you need to download the titanic dataset file\n[train.csv](\nhttps://raw.githubusercontent.com/keras-team/autokeras/master/tests/\nfixtures/titanic/train.csv\n)\nand\n[eval.csv](\nhttps://raw.githubusercontent.com/keras-team/autokeras/master/tests/\nfixtures/titanic/eval.csv\n).\nSecond, replace `PATH_TO/train.csv` and `PATH_TO/eval.csv` in the following example\nwith the real path to those two files.\nThen, you can run the code.\n""""""\n\nimport autokeras as ak\n\n# Initialize the classifier.\nclf = ak.StructuredDataClassifier(max_trials=30)\n# x is the path to the csv file. y is the column name of the column to predict.\nclf.fit(x=\'PATH_TO/train.csv\', y=\'survived\')\n# Evaluate the accuracy of the found model.\nprint(\'Accuracy: {accuracy}\'.format(\n    accuracy=clf.evaluate(x=\'PATH_TO/eval.csv\', y=\'survived\')))\n'"
tests/__init__.py,0,b''
tests/utils.py,7,"b'import kerastuner\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport autokeras as ak\n\nSEED = 5\nCOLUMN_NAMES_FROM_NUMPY = [\n    \'bool_\',\n    \'num_to_cat_\',\n    \'float_\',\n    \'int_\',\n    \'morethan_32_\',\n    \'col1_morethan_100_\',\n    \'col2_morethan_100_\',\n    \'col3_morethan_100_\']\nCOLUMN_TYPES_FROM_NUMPY = {\n    \'bool_\': \'categorical\',\n    \'num_to_cat_\': \'categorical\',\n    \'float_\': \'numerical\',\n    \'int_\': \'numerical\',\n    \'morethan_32_\': \'categorical\',\n    \'col1_morethan_100_\': \'categorical\',\n    \'col2_morethan_100_\': \'categorical\',\n    \'col3_morethan_100_\': \'categorical\'}\nCOLUMN_NAMES_FROM_CSV = [\n    \'sex\',\n    \'age\',\n    \'n_siblings_spouses\',\n    \'parch\',\n    \'fare\',\n    \'class\',\n    \'deck\',\n    \'embark_town\',\n    \'alone\']\nLESS_COLUMN_NAMES_FROM_CSV = [\n    \'age\',\n    \'n_siblings_spouses\',\n    \'parch\',\n    \'fare\',\n    \'class\',\n    \'deck\',\n    \'embark_town\',\n    \'alone\']\nCOLUMN_TYPES_FROM_CSV = {\n    \'sex\': \'categorical\',\n    \'age\': \'numerical\',\n    \'n_siblings_spouses\': \'categorical\',\n    \'parch\': \'categorical\',\n    \'fare\': \'numerical\',\n    \'class\': \'categorical\',\n    \'deck\': \'categorical\',\n    \'embark_town\': \'categorical\',\n    \'alone\': \'categorical\'}\nFALSE_COLUMN_TYPES_FROM_CSV = {\n    \'sex\': \'cat\',\n    \'age\': \'num\',\n    \'n_siblings_spouses\': \'cat\',\n    \'parch\': \'categorical\',\n    \'fare\': \'numerical\',\n    \'class\': \'categorical\',\n    \'deck\': \'categorical\',\n    \'embark_town\': \'categorical\',\n    \'alone\': \'categorical\'}\nPARTIAL_COLUMN_TYPES_FROM_CSV = {\n    \'fare\': \'categorical\',\n    \'class\': \'categorical\',\n    \'deck\': \'categorical\',\n    \'embark_town\': \'categorical\',\n    \'alone\': \'categorical\'}\nTRAIN_FILE_PATH = r\'tests/fixtures/titanic/train.csv\'\nTEST_FILE_PATH = r\'tests/fixtures/titanic/eval.csv\'\n\n\ndef generate_structured_data(num_instances=500, dtype=\'np\'):\n    # generate high_level dataset\n    num_feature = 8\n    num_nan = 100\n    # 12 classes\n    career = [\'doctor\', \'nurse\', \'driver\', \'chef\', \'teacher\', \'writer\',\n              \'actress\', \'engineer\', \'lawyer\', \'realtor\', \'agent\', \'pilot\']\n    # 15 classes\n    states = [\'CA\', \'FL\', \'GA\', \'IL\', \'MD\',\n              \'MA\', \'MI\', \'MN\', \'NJ\', \'NY\',\n              \'NC\', \'PA\', \'TX\', \'UT\', \'VA\']\n    # 13 classes\n    years = [\'first\', \'second\', \'third\', \'fourth\', \'fifth\',\n             \'sixth\', \'seventh\', \'eighth\', \'ninth\', \'tenth\',\n             \'eleventh\', \'twelfth\', \'thirteenth\']\n    # 10 classes\n    color = [\'red\', \'orange\', \'yellow\', \'green\', \'blue\',\n             \'purple\', \'beige\', \'pink\', \'silver\', \'gold\']\n    # 3 classes\n    size = [\'S\', \'M\', \'L\']\n    boolean = [\'True\', \'False\']\n    career_states = []  # 180 classes\n    career_years = []  # 156 classes\n    career_color = []  # 120 classes\n    career_size = []  # 36 classes\n    for c in career:\n        for s in states:\n            career_states.append(c+\'_\'+s)\n        for y in years:\n            career_years.append(c+\'_\'+y)\n        for r in color:\n            career_color.append(c+\'_\'+r)\n        for g in size:\n            career_size.append(c+\'_\'+g)\n\n    np.random.seed(0)\n    col_bool = np.random.choice(boolean, num_instances).reshape(num_instances, 1)\n    col_num_to_cat = np.random.randint(\n        20, 41, size=num_instances).reshape(num_instances, 1)\n    col_float = 100*np.random.random(num_instances,).reshape(num_instances, 1)\n    col_int = np.random.randint(\n        2000, 4000, size=num_instances).reshape(num_instances, 1)\n    col_morethan_32 = np.random.choice(\n        career_size, num_instances).reshape(num_instances, 1)\n    col1_morethan_100 = np.random.choice(career_states,\n                                         num_instances).reshape(num_instances, 1)\n    col2_morethan_100 = np.random.choice(career_years,\n                                         num_instances).reshape(num_instances, 1)\n    col3_morethan_100 = np.random.choice(career_color,\n                                         num_instances).reshape(num_instances, 1)\n    data = np.concatenate((col_bool, col_num_to_cat, col_float, col_int,\n                           col_morethan_32, col1_morethan_100, col2_morethan_100,\n                           col3_morethan_100), axis=1)\n    # generate np.nan data\n    for i in range(num_nan):\n        row = np.random.randint(0, num_instances)\n        col = np.random.randint(0, num_feature)\n        data[row][col] = np.nan\n    if dtype == \'np\':\n        return data\n    if dtype == \'dataset\':\n        return tf.data.Dataset.from_tensor_slices(data)\n\n\ndef dataframe_numpy():\n    x = pd.read_csv(TRAIN_FILE_PATH)\n    y = x.pop(\'survived\').to_numpy()\n    val_x = pd.read_csv(TEST_FILE_PATH)\n    val_y = val_x.pop(\'survived\').to_numpy()\n    return (x, y), (val_x, val_y)\n\n\ndef dataframe_dataframe():\n    x = pd.read_csv(TRAIN_FILE_PATH)\n    y = pd.DataFrame(x.pop(\'survived\'))\n    val_x = pd.read_csv(TEST_FILE_PATH)\n    val_y = pd.DataFrame(val_x.pop(\'survived\'))\n    return (x, y), (val_x, val_y)\n\n\ndef dataframe_series():\n    x = pd.read_csv(TRAIN_FILE_PATH)\n    y = x.pop(\'survived\')\n    val_x = pd.read_csv(TEST_FILE_PATH)\n    val_y = val_x.pop(\'survived\')\n    return (x, y), (val_x, val_y)\n\n\ndef csv_test(target):\n    x_test = pd.read_csv(TEST_FILE_PATH)\n    if target == \'regression\':\n        x_test = x_test.drop(\'fare\', axis=1)\n    else:\n        x_test = x_test.drop(\'survived\', axis=1)\n    return x_test\n\n\ndef generate_data(num_instances=100, shape=(32, 32, 3), dtype=\'np\'):\n    np.random.seed(SEED)\n    data = np.random.rand(*((num_instances,) + shape))\n    if data.dtype == np.float64:\n        data = data.astype(np.float32)\n    if dtype == \'np\':\n        return data\n    if dtype == \'dataset\':\n        return tf.data.Dataset.from_tensor_slices(data)\n\n\ndef generate_one_hot_labels(num_instances=100, num_classes=10, dtype=\'np\'):\n    np.random.seed(SEED)\n    labels = np.random.randint(num_classes, size=num_instances)\n    data = tf.keras.utils.to_categorical(labels)\n    if dtype == \'np\':\n        return data\n    if dtype == \'dataset\':\n        return tf.data.Dataset.from_tensor_slices(data).batch(32)\n\n\ndef fit_predict_with_graph(inputs, outputs, x, y):\n    model = ak.graph.HyperBuiltGraphHyperModel(\n        inputs, outputs).build(kerastuner.HyperParameters())\n    model.fit(x, y,\n              epochs=1,\n              batch_size=100,\n              verbose=False,\n              validation_split=0.2)\n    return model.predict(x)\n\n\ndef do_nothing(*args, **kwargs):\n    pass\n\n\ndef imdb_raw(num_instances=100):\n    index_offset = 3  # word index offset\n\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(\n        num_words=1000,\n        index_from=index_offset)\n    x_train = x_train[:num_instances]\n    y_train = y_train[:num_instances].reshape(-1, 1)\n    x_test = x_test[:num_instances]\n    y_test = y_test[:num_instances].reshape(-1, 1)\n\n    word_to_id = tf.keras.datasets.imdb.get_word_index()\n    word_to_id = {k: (v + index_offset) for k, v in word_to_id.items()}\n    word_to_id[""<PAD>""] = 0\n    word_to_id[""<START>""] = 1\n    word_to_id[""<UNK>""] = 2\n\n    id_to_word = {value: key for key, value in word_to_id.items()}\n    x_train = list(map(lambda sentence: \' \'.join(\n        id_to_word[i] for i in sentence), x_train))\n    x_test = list(map(lambda sentence: \' \'.join(\n        id_to_word[i] for i in sentence), x_test))\n    x_train = np.array(x_train, dtype=np.str)\n    x_test = np.array(x_test, dtype=np.str)\n    return (x_train, y_train), (x_test, y_test)\n\n\ndef build_graph():\n    tf.keras.backend.clear_session()\n    image_input = ak.ImageInput(shape=(32, 32, 3))\n    merged_outputs = ak.ImageBlock()(image_input)\n    head = ak.ClassificationHead(num_classes=10)\n    head.output_shape = (10,)\n    classification_outputs = head(merged_outputs)\n    return ak.graph.Graph(\n        inputs=image_input,\n        outputs=classification_outputs)\n\n\ndef config_tests(obj, excluded_keys=None):\n    if excluded_keys is None:\n        excluded_keys = []\n    config_keys = obj.get_config().keys()\n    for key in config_keys:\n        assert key in obj.__dict__\n    for key in obj.__dict__:\n        if key not in excluded_keys:\n            assert key in config_keys\n'"
autokeras/adapters/__init__.py,0,b'from autokeras.adapters.input_adapter import CATEGORICAL\nfrom autokeras.adapters.input_adapter import NUMERICAL\nfrom autokeras.adapters.input_adapter import ImageInputAdapter\nfrom autokeras.adapters.input_adapter import InputAdapter\nfrom autokeras.adapters.input_adapter import StructuredDataInputAdapter\nfrom autokeras.adapters.input_adapter import TextInputAdapter\nfrom autokeras.adapters.input_adapter import TimeseriesInputAdapter\nfrom autokeras.adapters.output_adapter import ClassificationHeadAdapter\nfrom autokeras.adapters.output_adapter import RegressionHeadAdapter\nfrom autokeras.adapters.output_adapter import SegmentationHeadAdapter\n'
autokeras/adapters/input_adapter.py,14,"b'import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\nfrom autokeras.engine import adapter as adapter_module\nfrom autokeras.utils import data_utils\n\nCATEGORICAL = \'categorical\'\nNUMERICAL = \'numerical\'\n\n\nclass InputAdapter(adapter_module.Adapter):\n\n    def check(self, x):\n        """"""Record any information needed by transform.""""""\n        if not isinstance(x, (np.ndarray, tf.data.Dataset)):\n            raise TypeError(\'Expect the data to Input to be numpy.ndarray or \'\n                            \'tf.data.Dataset, but got {type}.\'.format(type=type(x)))\n        if isinstance(x, np.ndarray) and not np.issubdtype(x.dtype, np.number):\n            raise TypeError(\'Expect the data to Input to be numerical, but got \'\n                            \'{type}.\'.format(type=x.dtype))\n\n\nclass ImageInputAdapter(adapter_module.Adapter):\n\n    def check(self, x):\n        """"""Record any information needed by transform.""""""\n        if not isinstance(x, (np.ndarray, tf.data.Dataset)):\n            raise TypeError(\'Expect the data to ImageInput to be numpy.ndarray or \'\n                            \'tf.data.Dataset, but got {type}.\'.format(type=type(x)))\n        if isinstance(x, np.ndarray) and x.ndim not in [3, 4]:\n            raise ValueError(\'Expect the data to ImageInput to have 3 or 4 \'\n                             \'dimensions, but got input shape {shape} with {ndim} \'\n                             \'dimensions\'.format(shape=x.shape, ndim=x.ndim))\n        if isinstance(x, np.ndarray) and not np.issubdtype(x.dtype, np.number):\n            raise TypeError(\'Expect the data to ImageInput to be numerical, but got \'\n                            \'{type}.\'.format(type=x.dtype))\n\n    def convert_to_dataset(self, x):\n        if isinstance(x, np.ndarray):\n            # TODO: expand the dims after converting to Dataset.\n            if x.ndim == 3:\n                x = np.expand_dims(x, axis=3)\n            x = x.astype(np.float32)\n        return super().convert_to_dataset(x)\n\n\nclass TextInputAdapter(adapter_module.Adapter):\n\n    def check(self, x):\n        """"""Record any information needed by transform.""""""\n        if not isinstance(x, (np.ndarray, tf.data.Dataset)):\n            raise TypeError(\'Expect the data to TextInput to be numpy.ndarray or \'\n                            \'tf.data.Dataset, but got {type}.\'.format(type=type(x)))\n\n        if isinstance(x, np.ndarray) and x.ndim != 1:\n            raise ValueError(\'Expect the data to TextInput to have 1 dimension, but \'\n                             \'got input shape {shape} with {ndim} dimensions\'.format(\n                                 shape=x.shape,\n                                 ndim=x.ndim))\n        if isinstance(x, np.ndarray) and not np.issubdtype(x.dtype, np.character):\n            raise TypeError(\'Expect the data to TextInput to be strings, but got \'\n                            \'{type}.\'.format(type=x.dtype))\n\n    def convert_to_dataset(self, x):\n        x = super().convert_to_dataset(x)\n        shape = data_utils.dataset_shape(x)\n        if len(shape) == 1:\n            x = x.map(lambda a: tf.reshape(a, [-1, 1]))\n        return x\n\n\nclass StructuredDataInputAdapter(adapter_module.Adapter):\n\n    def __init__(self, column_names=None, column_types=None, **kwargs):\n        super().__init__(**kwargs)\n        self.column_names = column_names\n        self.column_types = column_types\n        # Variables for inferring column types.\n        self.count_nan = None\n        self.count_numerical = None\n        self.count_categorical = None\n        self.count_unique_numerical = []\n        self.num_col = None\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'count_nan\': self.count_nan,\n            \'count_numerical\': self.count_numerical,\n            \'count_categorical\': self.count_categorical,\n            \'count_unique_numerical\': self.count_unique_numerical,\n            \'num_col\': self.num_col\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        obj = super().from_config(config)\n        obj.count_nan = config[\'count_nan\']\n        obj.count_numerical = config[\'count_numerical\']\n        obj.count_categorical = config[\'count_categorical\']\n        obj.count_unique_numerical = config[\'count_unique_numerical\']\n        obj.num_col = config[\'num_col\']\n\n    def check(self, x):\n        if not isinstance(x, (pd.DataFrame, np.ndarray, tf.data.Dataset)):\n            raise TypeError(\'Unsupported type {type} for \'\n                            \'{name}.\'.format(type=type(x),\n                                             name=self.__class__.__name__))\n\n        # Extract column_names from pd.DataFrame.\n        if isinstance(x, pd.DataFrame) and self.column_names is None:\n            self.column_names = list(x.columns)\n            # column_types is provided by user\n            if self.column_types:\n                for column_name in self.column_types:\n                    if column_name not in self.column_names:\n                        raise ValueError(\'Column_names and column_types are \'\n                                         \'mismatched. Cannot find column name \'\n                                         \'{name} in the data.\'.format(\n                                             name=column_name))\n\n        if self.column_names is None:\n            if self.column_types:\n                raise ValueError(\'Column names must be specified.\')\n\n    def convert_to_dataset(self, x):\n        if isinstance(x, pd.DataFrame):\n            # Convert x, y, validation_data to tf.Dataset.\n            x = x.values.astype(np.unicode)\n        if isinstance(x, np.ndarray):\n            x = x.astype(np.unicode)\n        return super().convert_to_dataset(x)\n\n    def fit(self, dataset):\n        super().fit(dataset)\n        for x in dataset:\n            self.update(x)\n        self.infer_column_types()\n\n    def update(self, x):\n        # Calculate the statistics.\n        x = nest.flatten(x)[0].numpy()\n        for instance in x:\n            self._update_instance(instance)\n\n    def _update_instance(self, x):\n        if self.num_col is None:\n            self.num_col = len(x)\n            self.count_nan = np.zeros(self.num_col)\n            self.count_numerical = np.zeros(self.num_col)\n            self.count_categorical = np.zeros(self.num_col)\n            for i in range(len(x)):\n                self.count_unique_numerical.append({})\n        for i in range(self.num_col):\n            x[i] = x[i].decode(\'utf-8\')\n            if x[i] == \'nan\':\n                self.count_nan[i] += 1\n            elif x[i] == \'True\':\n                self.count_categorical[i] += 1\n            elif x[i] == \'False\':\n                self.count_categorical[i] += 1\n            else:\n                try:\n                    tmp_num = float(x[i])\n                    self.count_numerical[i] += 1\n                    if tmp_num not in self.count_unique_numerical[i]:\n                        self.count_unique_numerical[i][tmp_num] = 1\n                    else:\n                        self.count_unique_numerical[i][tmp_num] += 1\n                except ValueError:\n                    self.count_categorical[i] += 1\n\n    def infer_column_types(self):\n        column_types = {}\n\n        if self.column_names is None:\n            # Generate column names.\n            self.column_names = [index for index in range(self.num_col)]\n        # Check if column_names has the correct length.\n        elif len(self.column_names) != self.num_col:\n            raise ValueError(\'Expect column_names to have length {expect} \'\n                             \'but got {actual}.\'.format(\n                                 expect=self.num_col,\n                                 actual=len(self.column_names)))\n\n        for i in range(self.num_col):\n            if self.count_categorical[i] > 0:\n                column_types[self.column_names[i]] = CATEGORICAL\n            elif len(self.count_unique_numerical[i])/self.count_numerical[i] < 0.05:\n                column_types[self.column_names[i]] = CATEGORICAL\n            else:\n                column_types[self.column_names[i]] = NUMERICAL\n        # Partial column_types is provided.\n        if self.column_types is None:\n            self.column_types = {}\n        for key, value in column_types.items():\n            if key not in self.column_types:\n                self.column_types[key] = value\n\n\nclass TimeseriesInputAdapter(adapter_module.Adapter):\n\n    def __init__(self,\n                 lookback=None,\n                 column_names=None,\n                 column_types=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.lookback = lookback\n        self.column_names = column_names\n        self.column_types = column_types\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'lookback\': self.lookback,\n            \'column_names\': self.column_names,\n            \'column_types\': self.column_types\n        })\n        return config\n\n    def check(self, x):\n        """"""Record any information needed by transform.""""""\n        if not isinstance(x, (pd.DataFrame, np.ndarray, tf.data.Dataset)):\n            raise TypeError(\'Expect the data in TimeseriesInput to be numpy.ndarray\'\n                            \' or tf.data.Dataset or pd.DataFrame, but got {type}.\'.\n                            format(type=type(x)))\n\n        if isinstance(x, np.ndarray) and x.ndim != 2:\n            raise ValueError(\'Expect the data in TimeseriesInput to have 2 dimension\'\n                             \', but got input shape {shape} with {ndim} \'\n                             \'dimensions\'.format(\n                                 shape=x.shape,\n                                 ndim=x.ndim))\n\n        # Extract column_names from pd.DataFrame.\n        if isinstance(x, pd.DataFrame) and self.column_names is None:\n            self.column_names = list(x.columns)\n            # column_types is provided by user\n            if self.column_types:\n                for column_name in self.column_types:\n                    if column_name not in self.column_names:\n                        raise ValueError(\'Column_names and column_types are \'\n                                         \'mismatched. Cannot find column name \'\n                                         \'{name} in the data.\'.format(\n                                             name=column_name))\n\n        # Generate column_names.\n        if self.column_names is None:\n            if self.column_types:\n                raise ValueError(\'Column names must be specified.\')\n            self.column_names = [index for index in range(x.shape[1])]\n\n        # Check if column_names has the correct length.\n        if len(self.column_names) != x.shape[1]:\n            raise ValueError(\'Expect column_names to have length {expect} \'\n                             \'but got {actual}.\'.format(\n                                 expect=x.shape[1],\n                                 actual=len(self.column_names)))\n\n    def convert_to_dataset(self, x):\n        if isinstance(x, pd.DataFrame):\n            # Convert x, y, validation_data to tf.Dataset.\n            x = x.values.astype(np.float32)\n        if isinstance(x, np.ndarray):\n            x = x.astype(np.float32)\n        x = tf.data.Dataset.from_tensor_slices(x)\n        x = x.window(self.lookback, shift=1, drop_remainder=True)\n        final_data = []\n        for window in x:\n            final_data.append([elems.numpy() for elems in window])\n        final_data = tf.data.Dataset.from_tensor_slices(final_data)\n        return super().convert_to_dataset(final_data)\n'"
autokeras/adapters/output_adapter.py,4,"b'import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom autokeras import encoders\nfrom autokeras.engine import adapter as adapter_module\nfrom autokeras.utils import data_utils\n\n\nclass HeadAdapter(adapter_module.Adapter):\n\n    def __init__(self, name, **kwargs):\n        super().__init__(**kwargs)\n        self.name = name\n\n    def check(self, dataset):\n        supported_types = (tf.data.Dataset, np.ndarray, pd.DataFrame, pd.Series)\n        if not isinstance(dataset, supported_types):\n            raise TypeError(\'Expect the target data of {name} to be tf.data.Dataset,\'\n                            \' np.ndarray, pd.DataFrame or pd.Series, but got {type}.\'\n                            .format(name=self.name, type=type(dataset)))\n\n    def convert_to_dataset(self, dataset):\n        if isinstance(dataset, np.ndarray):\n            if len(dataset.shape) == 1:\n                dataset = dataset.reshape(-1, 1)\n        if isinstance(dataset, pd.DataFrame):\n            dataset = dataset.values\n        if isinstance(dataset, pd.Series):\n            dataset = dataset.values.reshape(-1, 1)\n        return super().convert_to_dataset(dataset)\n\n    def postprocess(self, y):\n        """"""Postprocess the output of the Keras Model.""""""\n        return y\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'name\': self.name,\n        })\n        return config\n\n\nclass ClassificationHeadAdapter(HeadAdapter):\n\n    def __init__(self,\n                 num_classes=None,\n                 multi_label=False,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.num_classes = num_classes\n        self.label_encoder = None\n        self.multi_label = multi_label\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'encoder\': encoders.serialize(self.label_encoder),\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        obj = super().from_config(config)\n        obj.label_encoder = encoders.deserialize(config[\'encoder\'])\n\n    def fit_before_convert(self, dataset):\n        """"""Fit the encoder.""""""\n        # If in tf.data.Dataset, must be encoded already.\n        if isinstance(dataset, tf.data.Dataset):\n            return\n\n        # Convert the data to np.ndarray.\n        if isinstance(dataset, pd.DataFrame):\n            dataset = dataset.values\n        if isinstance(dataset, pd.Series):\n            dataset = dataset.values.reshape(-1, 1)\n\n        # If encoded.\n        # TODO: support raw string labels for multi-label.\n        if len(dataset.flatten()) != len(dataset):\n            if self.num_classes:\n                self._check_data_shape(dataset.shape[1:])\n            return\n\n        # Fit encoder.\n        labels = set(dataset.flatten())\n        if len(labels) < 2:\n            raise ValueError(\'Expect the target data for {name} to have \'\n                             \'at least 2 classes, but got {num_classes}.\'\n                             .format(name=self.name, num_classes=self.num_classes))\n        if len(labels) == 2 and not self.multi_label:\n            self.label_encoder = encoders.LabelEncoder()\n        else:\n            self.label_encoder = encoders.OneHotEncoder()\n        self.label_encoder.fit(dataset)\n\n    def convert_to_dataset(self, dataset):\n        if self.label_encoder:\n            dataset = self.label_encoder.encode(dataset)\n        return super().convert_to_dataset(dataset)\n\n    def fit(self, dataset):\n        super().fit(dataset)\n        shape = tuple(data_utils.dataset_shape(dataset).as_list()[1:])\n        # Infer the num_classes.\n        if not self.num_classes:\n            # Single column with 0s and 1s.\n            if shape == (1,):\n                self.num_classes = 2\n            else:\n                self.num_classes = shape[0]\n            return\n\n        # Compute expected shape from num_classes.\n        if self.num_classes == 2 and not self.multi_label:\n            expected = (1,)\n        else:\n            expected = (self.num_classes,)\n\n        # Check shape equals expected shape.\n        if shape != expected:\n            raise ValueError(\'Expect the target data for {name} to have \'\n                             \'shape {expected}, but got {actual}.\'\n                             .format(name=self.name, expected=expected,\n                                     actual=shape))\n\n    def postprocess(self, y):\n        if self.multi_label:\n            y[y < 0.5] = 0\n            y[y > 0.5] = 1\n        if self.label_encoder:\n            y = self.label_encoder.decode(y)\n        return y\n\n\nclass RegressionHeadAdapter(HeadAdapter):\n    pass\n\n\nclass SegmentationHeadAdapter(ClassificationHeadAdapter):\n    pass\n'"
autokeras/blocks/__init__.py,2,"b""import tensorflow as tf\n\nfrom autokeras.blocks.basic import ConvBlock\nfrom autokeras.blocks.basic import DenseBlock\nfrom autokeras.blocks.basic import Embedding\nfrom autokeras.blocks.basic import ResNetBlock\nfrom autokeras.blocks.basic import RNNBlock\nfrom autokeras.blocks.basic import XceptionBlock\nfrom autokeras.blocks.heads import ClassificationHead\nfrom autokeras.blocks.heads import RegressionHead\nfrom autokeras.blocks.heads import SegmentationHead\nfrom autokeras.blocks.preprocessing import CategoricalToNumerical\nfrom autokeras.blocks.preprocessing import ImageAugmentation\nfrom autokeras.blocks.preprocessing import Normalization\nfrom autokeras.blocks.preprocessing import TextToIntSequence\nfrom autokeras.blocks.preprocessing import TextToNgramVector\nfrom autokeras.blocks.reduction import Flatten\nfrom autokeras.blocks.reduction import Merge\nfrom autokeras.blocks.reduction import SpatialReduction\nfrom autokeras.blocks.reduction import TemporalReduction\nfrom autokeras.blocks.wrapper import ImageBlock\nfrom autokeras.blocks.wrapper import StructuredDataBlock\nfrom autokeras.blocks.wrapper import TextBlock\nfrom autokeras.blocks.wrapper import TimeseriesBlock\n\n\ndef serialize(obj):\n    return tf.keras.utils.serialize_keras_object(obj)\n\n\ndef deserialize(config, custom_objects=None):\n    return tf.keras.utils.deserialize_keras_object(\n        config,\n        module_objects=globals(),\n        custom_objects=custom_objects,\n        printable_module_name='hypermodels')\n"""
autokeras/blocks/basic.py,0,"b'from typing import Optional\n\nfrom kerastuner.applications import resnet\nfrom kerastuner.applications import xception\nfrom tensorflow.keras import layers\nfrom tensorflow.python.util import nest\n\nfrom autokeras.blocks import reduction\nfrom autokeras.engine import block as block_module\nfrom autokeras.utils import layer_utils\nfrom autokeras.utils import utils\n\n\ndef set_hp_value(hp, name, value):\n    full_name = hp._get_name(name)\n    hp.values[full_name] = value or hp.values[full_name]\n\n\nclass DenseBlock(block_module.Block):\n    """"""Block for Dense layers.\n\n    # Arguments\n        num_layers: Int. The number of Dense layers in the block.\n            If left unspecified, it will be tuned automatically.\n        use_bn: Boolean. Whether to use BatchNormalization layers.\n            If left unspecified, it will be tuned automatically.\n        dropout_rate: Float. The dropout rate for the layers.\n            If left unspecified, it will be tuned automatically.\n    """"""\n\n    def __init__(self,\n                 num_layers: Optional[int] = None,\n                 use_batchnorm: Optional[bool] = None,\n                 dropout_rate: Optional[float] = None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.num_layers = num_layers\n        self.use_batchnorm = use_batchnorm\n        self.dropout_rate = dropout_rate\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'num_layers\': self.num_layers,\n            \'use_batchnorm\': self.use_batchnorm,\n            \'dropout_rate\': self.dropout_rate})\n        return config\n\n    def build(self, hp, inputs=None):\n        inputs = nest.flatten(inputs)\n        utils.validate_num_inputs(inputs, 1)\n        input_node = inputs[0]\n        output_node = input_node\n        output_node = reduction.Flatten().build(hp, output_node)\n\n        num_layers = self.num_layers or hp.Choice(\'num_layers\', [1, 2, 3], default=2)\n        use_batchnorm = self.use_batchnorm\n        if use_batchnorm is None:\n            use_batchnorm = hp.Boolean(\'use_batchnorm\', default=False)\n        if self.dropout_rate is not None:\n            dropout_rate = self.dropout_rate\n        else:\n            dropout_rate = hp.Choice(\'dropout_rate\', [0.0, 0.25, 0.5], default=0)\n\n        for i in range(num_layers):\n            units = hp.Choice(\n                \'units_{i}\'.format(i=i),\n                [16, 32, 64, 128, 256, 512, 1024],\n                default=32)\n            output_node = layers.Dense(units)(output_node)\n            if use_batchnorm:\n                output_node = layers.BatchNormalization()(output_node)\n            output_node = layers.ReLU()(output_node)\n            if dropout_rate > 0:\n                output_node = layers.Dropout(dropout_rate)(output_node)\n        return output_node\n\n\nclass RNNBlock(block_module.Block):\n    """"""An RNN Block.\n\n    # Arguments\n        return_sequences: Boolean. Whether to return the last output in the\n            output sequence, or the full sequence. Defaults to False.\n        bidirectional: Boolean. Bidirectional RNN. If left unspecified, it will be\n            tuned automatically.\n        num_layers: Int. The number of layers in RNN. If left unspecified, it will\n            be tuned automatically.\n        layer_type: String. \'gru\' or \'lstm\'. If left unspecified, it will be tuned\n            automatically.\n    """"""\n\n    def __init__(self,\n                 return_sequences: bool = False,\n                 bidirectional: Optional[bool] = None,\n                 num_layers: Optional[int] = None,\n                 layer_type: Optional[int] = None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.return_sequences = return_sequences\n        self.bidirectional = bidirectional\n        self.num_layers = num_layers\n        self.layer_type = layer_type\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'return_sequences\': self.return_sequences,\n            \'bidirectional\': self.bidirectional,\n            \'num_layers\': self.num_layers,\n            \'layer_type\': self.layer_type})\n        return config\n\n    def build(self, hp, inputs=None):\n        inputs = nest.flatten(inputs)\n        utils.validate_num_inputs(inputs, 1)\n        input_node = inputs[0]\n        shape = input_node.shape.as_list()\n        if len(shape) != 3:\n            raise ValueError(\n                \'Expect the input tensor to have \'\n                \'at least 3 dimensions for rnn models, \'\n                \'but got {shape}\'.format(shape=input_node.shape))\n\n        feature_size = shape[-1]\n        output_node = input_node\n\n        bidirectional = self.bidirectional\n        if bidirectional is None:\n            bidirectional = hp.Boolean(\'bidirectional\', default=True)\n        layer_type = self.layer_type or hp.Choice(\'layer_type\',\n                                                  [\'gru\', \'lstm\'],\n                                                  default=\'lstm\')\n        num_layers = self.num_layers or hp.Choice(\'num_layers\',\n                                                  [1, 2, 3],\n                                                  default=2)\n        rnn_layers = {\n            \'gru\': layers.GRU,\n            \'lstm\': layers.LSTM\n        }\n        in_layer = rnn_layers[layer_type]\n        for i in range(num_layers):\n            return_sequences = True\n            if i == num_layers - 1:\n                return_sequences = self.return_sequences\n            if bidirectional:\n                output_node = layers.Bidirectional(\n                    in_layer(feature_size,\n                             return_sequences=return_sequences))(output_node)\n            else:\n                output_node = in_layer(\n                    feature_size,\n                    return_sequences=return_sequences)(output_node)\n        return output_node\n\n\nclass ConvBlock(block_module.Block):\n    """"""Block for vanilla ConvNets.\n\n    # Arguments\n        kernel_size: Int. If left unspecified, it will be tuned automatically.\n        num_blocks: Int. The number of conv blocks, each of which may contain\n            convolutional, max pooling, dropout, and activation. If left unspecified,\n            it will be tuned automatically.\n        num_layers: Int. The number of convolutional layers in each block. If left\n            unspecified, it will be tuned automatically.\n        max_pooling: Boolean. Whether to use max pooling layer in each block. If left\n            unspecified, it will be tuned automatically.\n        separable: Boolean. Whether to use separable conv layers.\n            If left unspecified, it will be tuned automatically.\n        dropout_rate: Float. Between 0 and 1. The dropout rate for after the\n            convolutional layers. If left unspecified, it will be tuned\n            automatically.\n    """"""\n\n    def __init__(self,\n                 kernel_size: Optional[int] = None,\n                 num_blocks: Optional[int] = None,\n                 num_layers: Optional[int] = None,\n                 max_pooling: Optional[bool] = None,\n                 separable: Optional[bool] = None,\n                 dropout_rate: Optional[float] = None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.kernel_size = kernel_size\n        self.num_blocks = num_blocks\n        self.num_layers = num_layers\n        self.max_pooling = max_pooling\n        self.separable = separable\n        self.dropout_rate = dropout_rate\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'kernel_size\': self.kernel_size,\n            \'num_blocks\': self.num_blocks,\n            \'num_layers\': self.num_layers,\n            \'max_pooling\': self.max_pooling,\n            \'separable\': self.separable,\n            \'dropout_rate\': self.dropout_rate})\n        return config\n\n    def build(self, hp, inputs=None):\n        inputs = nest.flatten(inputs)\n        utils.validate_num_inputs(inputs, 1)\n        input_node = inputs[0]\n        output_node = input_node\n\n        kernel_size = self.kernel_size or hp.Choice(\'kernel_size\',\n                                                    [3, 5, 7],\n                                                    default=3)\n        num_blocks = self.num_blocks or hp.Choice(\'num_blocks\',\n                                                  [1, 2, 3],\n                                                  default=2)\n        num_layers = self.num_layers or hp.Choice(\'num_layers\',\n                                                  [1, 2],\n                                                  default=2)\n        separable = self.separable\n        if separable is None:\n            separable = hp.Boolean(\'separable\', default=False)\n\n        if separable:\n            conv = layer_utils.get_sep_conv(input_node.shape)\n        else:\n            conv = layer_utils.get_conv(input_node.shape)\n\n        max_pooling = self.max_pooling\n        if max_pooling is None:\n            max_pooling = hp.Boolean(\'max_pooling\', default=True)\n        pool = layer_utils.get_max_pooling(input_node.shape)\n\n        if self.dropout_rate is not None:\n            dropout_rate = self.dropout_rate\n        else:\n            dropout_rate = hp.Choice(\'dropout_rate\', [0.0, 0.25, 0.5], default=0)\n\n        for i in range(num_blocks):\n            for j in range(num_layers):\n                output_node = conv(\n                    hp.Choice(\'filters_{i}_{j}\'.format(i=i, j=j),\n                              [16, 32, 64, 128, 256, 512],\n                              default=32),\n                    kernel_size,\n                    padding=self._get_padding(kernel_size, output_node),\n                    activation=\'relu\')(output_node)\n            if max_pooling:\n                output_node = pool(\n                    kernel_size - 1,\n                    padding=self._get_padding(kernel_size - 1,\n                                              output_node))(output_node)\n            if dropout_rate > 0:\n                output_node = layers.Dropout(dropout_rate)(output_node)\n        return output_node\n\n    @staticmethod\n    def _get_padding(kernel_size, output_node):\n        if all([kernel_size * 2 <= length\n                for length in output_node.shape[1:-1]]):\n            return \'valid\'\n        return \'same\'\n\n\nclass ResNetBlock(resnet.HyperResNet, block_module.Block):\n    """"""Block for ResNet.\n\n    # Arguments\n        version: String. \'v1\', \'v2\' or \'next\'. The type of ResNet to use.\n            If left unspecified, it will be tuned automatically.\n        pooling: String. \'avg\', \'max\'. The type of pooling layer to use.\n            If left unspecified, it will be tuned automatically.\n    """"""\n\n    def __init__(self,\n                 version: Optional[str] = None,\n                 pooling: Optional[str] = None,\n                 **kwargs):\n        if \'include_top\' in kwargs:\n            raise ValueError(\n                \'Argument ""include_top"" is not supported in ResNetBlock.\')\n        if \'input_shape\' in kwargs:\n            raise ValueError(\n                \'Argument ""input_shape"" is not supported in ResNetBlock.\')\n        super().__init__(include_top=False, input_shape=(10,), **kwargs)\n        self.version = version\n        self.pooling = pooling\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'classes\': self.classes,\n            \'version\': self.version,\n            \'pooling\': self.pooling})\n        return config\n\n    def build(self, hp, inputs=None):\n        self.input_tensor = nest.flatten(inputs)[0]\n        self.input_shape = None\n\n        hp.Choice(\'version\', [\'v1\', \'v2\', \'next\'], default=\'v2\')\n        hp.Choice(\'pooling\', [\'avg\', \'max\'], default=\'avg\')\n\n        set_hp_value(hp, \'version\', self.version)\n        set_hp_value(hp, \'pooling\', self.pooling)\n\n        model = super().build(hp)\n        return model.outputs\n\n\nclass XceptionBlock(xception.HyperXception, block_module.Block):\n    """"""XceptionBlock.\n\n    An Xception structure, used for specifying your model with specific datasets.\n\n    The original Xception architecture is from https://arxiv.org/abs/1610.02357.\n    The data first goes through the entry flow, then through the middle flow which\n    is repeated eight times, and finally through the exit flow.\n\n    This XceptionBlock returns a similar architecture as Xception except without\n    the last (optional) fully connected layer(s) and logistic regression.\n    The size of this architecture could be decided by `HyperParameters`, to get an\n    architecture with a half, an identical, or a double size of the original one.\n\n    # Arguments\n        activation: String. \'selu\' or \'relu\'. If left unspecified, it will be tuned\n            automatically.\n        initial_strides: Int. If left unspecified, it will be tuned automatically.\n        num_residual_blocks: Int. If left unspecified, it will be tuned\n            automatically.\n        pooling: String. \'ave\', \'flatten\', or \'max\'. If left unspecified, it will be\n            tuned automatically.\n    """"""\n\n    def __init__(self,\n                 activation: Optional[str] = None,\n                 initial_strides: Optional[int] = None,\n                 num_residual_blocks: Optional[int] = None,\n                 pooling: Optional[str] = None,\n                 **kwargs):\n        if \'include_top\' in kwargs:\n            raise ValueError(\n                \'Argument ""include_top"" is not supported in XceptionBlock.\')\n        if \'input_shape\' in kwargs:\n            raise ValueError(\n                \'Argument ""input_shape"" is not supported in XceptionBlock.\')\n        super().__init__(include_top=False, input_shape=(10,), **kwargs)\n        self.activation = activation\n        self.initial_strides = initial_strides\n        self.num_residual_blocks = num_residual_blocks\n        self.pooling = pooling\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'classes\': self.classes,\n            \'activation\': self.activation,\n            \'initial_strides\': self.initial_strides,\n            \'num_residual_blocks\': self.num_residual_blocks,\n            \'pooling\': self.pooling})\n        return config\n\n    def build(self, hp, inputs=None):\n        self.input_tensor = nest.flatten(inputs)[0]\n        self.input_shape = None\n\n        hp.Choice(\'activation\', [\'relu\', \'selu\'])\n        hp.Choice(\'initial_strides\', [2])\n        hp.Int(\'num_residual_blocks\', 2, 8, default=4)\n        hp.Choice(\'pooling\', [\'avg\', \'flatten\', \'max\'])\n\n        set_hp_value(hp, \'activation\', self.activation)\n        set_hp_value(hp, \'initial_strides\', self.initial_strides)\n        set_hp_value(hp, \'num_residual_blocks\', self.num_residual_blocks)\n        set_hp_value(hp, \'pooling\', self.pooling)\n\n        model = super().build(hp)\n        return model.outputs\n\n\nclass Embedding(block_module.Block):\n    """"""Word embedding block for sequences.\n\n    The input should be tokenized sequences with the same length, where each element\n    of a sequence should be the index of the word.\n\n    # Arguments\n        max_features: Int. Size of the vocabulary. Must be set if not using\n            TextToIntSequence before this block. Defaults to 20001.\n        pretraining: String. \'random\' (use random weights instead any pretrained\n            model), \'glove\', \'fasttext\' or \'word2vec\'. Use pretrained word embedding.\n            If left unspecified, it will be tuned automatically.\n        embedding_dim: Int. If left unspecified, it will be tuned automatically.\n        dropout_rate: Float. The dropout rate for after the Embedding layer.\n            If left unspecified, it will be tuned automatically.\n    """"""\n\n    def __init__(self,\n                 max_features: int = 20001,\n                 pretraining: Optional[str] = None,\n                 embedding_dim: Optional[int] = None,\n                 dropout_rate: Optional[float] = None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.max_features = max_features\n        self.pretraining = pretraining\n        self.embedding_dim = embedding_dim\n        self.dropout_rate = dropout_rate\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'max_features\': self.max_features,\n            \'pretraining\': self.pretraining,\n            \'embedding_dim\': self.embedding_dim,\n            \'dropout_rate\': self.dropout_rate})\n        return config\n\n    def build(self, hp, inputs=None):\n        input_node = nest.flatten(inputs)[0]\n        # TODO: support more pretrained embedding layers.\n        # glove, fasttext, and word2vec\n        pretraining = self.pretraining or hp.Choice(\n            \'pretraining\',\n            [\'random\', \'glove\', \'fasttext\', \'word2vec\', \'none\'],\n            default=\'none\')\n        embedding_dim = self.embedding_dim or hp.Choice(\n            \'embedding_dim\',\n            [32, 64, 128, 256, 512],\n            default=128)\n        if pretraining != \'none\':\n            # TODO: load from pretrained weights\n            layer = layers.Embedding(\n                input_dim=self.max_features,\n                output_dim=embedding_dim,\n                input_length=input_node.shape[1])\n            # trainable=False,\n            # weights=[embedding_matrix])\n        else:\n            layer = layers.Embedding(\n                input_dim=self.max_features,\n                output_dim=embedding_dim)\n            # input_length=input_node.shape[1],\n            # trainable=True)\n        output_node = layer(input_node)\n        if self.dropout_rate is not None:\n            dropout_rate = self.dropout_rate\n        else:\n            dropout_rate = hp.Choice(\'dropout_rate\', [0.0, 0.25, 0.5], default=0.25)\n        if dropout_rate > 0:\n            output_node = layers.Dropout(dropout_rate)(output_node)\n        return output_node\n'"
autokeras/blocks/heads.py,4,"b'from typing import Optional\n\nimport tensorflow as tf\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow.python.util import nest\n\nfrom autokeras import adapters\nfrom autokeras.blocks import reduction\nfrom autokeras.engine import head as head_module\nfrom autokeras.utils import types\nfrom autokeras.utils import utils\n\n\nclass ClassificationHead(head_module.Head):\n    """"""Classification Dense layers.\n\n    Use sigmoid and binary crossentropy for binary classification and multi-label\n    classification. Use softmax and categorical crossentropy for multi-class\n    (more than 2) classification. Use Accuracy as metrics by default.\n\n    The targets passing to the head would have to be tf.data.Dataset, np.ndarray,\n    pd.DataFrame or pd.Series. It can be raw labels, one-hot encoded if more than two\n    classes, or binary encoded for binary classification.\n\n    The raw labels will be encoded to one column if two classes were found,\n    or one-hot encoded if more than two classes were found.\n\n    # Arguments\n        num_classes: Int. Defaults to None. If None, it will be inferred from the\n            data.\n        multi_label: Boolean. Defaults to False.\n        loss: A Keras loss function. Defaults to use `binary_crossentropy` or\n            `categorical_crossentropy` based on the number of classes.\n        metrics: A list of Keras metrics. Defaults to use \'accuracy\'.\n        dropout_rate: Float. The dropout rate for the layers.\n            If left unspecified, it will be tuned automatically.\n    """"""\n\n    def __init__(self,\n                 num_classes: Optional[int] = None,\n                 multi_label: bool = False,\n                 loss: Optional[types.LossType] = None,\n                 metrics: Optional[types.MetricsType] = None,\n                 dropout_rate: Optional[float] = None,\n                 **kwargs):\n        self.num_classes = num_classes\n        self.multi_label = multi_label\n        self.dropout_rate = dropout_rate\n        if metrics is None:\n            metrics = [\'accuracy\']\n        if loss is None:\n            loss = self.infer_loss()\n        super().__init__(loss=loss,\n                         metrics=metrics,\n                         **kwargs)\n\n    def infer_loss(self):\n        if not self.num_classes:\n            return None\n        if self.num_classes == 2 or self.multi_label:\n            return losses.BinaryCrossentropy()\n        return losses.CategoricalCrossentropy()\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'num_classes\': self.num_classes,\n            \'multi_label\': self.multi_label,\n            \'dropout_rate\': self.dropout_rate})\n        return config\n\n    def build(self, hp, inputs=None):\n        inputs = nest.flatten(inputs)\n        utils.validate_num_inputs(inputs, 1)\n        input_node = inputs[0]\n        output_node = input_node\n\n        # Reduce the tensor to a vector.\n        if len(output_node.shape) > 2:\n            output_node = reduction.SpatialReduction().build(hp, output_node)\n\n        if self.dropout_rate is not None:\n            dropout_rate = self.dropout_rate\n        else:\n            dropout_rate = hp.Choice(\'dropout_rate\', [0.0, 0.25, 0.5], default=0)\n\n        if dropout_rate > 0:\n            output_node = layers.Dropout(dropout_rate)(output_node)\n        output_node = layers.Dense(self.output_shape[-1])(output_node)\n        if isinstance(self.loss, tf.keras.losses.BinaryCrossentropy):\n            output_node = layers.Activation(activations.sigmoid,\n                                            name=self.name)(output_node)\n        else:\n            output_node = layers.Softmax(name=self.name)(output_node)\n        return output_node\n\n    def get_adapter(self):\n        return adapters.ClassificationHeadAdapter(\n            name=self.name, multi_label=self.multi_label)\n\n    def config_from_adapter(self, adapter):\n        super().config_from_adapter(adapter)\n        self.num_classes = adapter.num_classes\n        self.loss = self.infer_loss()\n\n\nclass RegressionHead(head_module.Head):\n    """"""Regression Dense layers.\n\n    The targets passing to the head would have to be tf.data.Dataset, np.ndarray,\n    pd.DataFrame or pd.Series. It can be single-column or multi-column. The\n    values should all be numerical.\n\n    # Arguments\n        output_dim: Int. The number of output dimensions. Defaults to None.\n            If None, it will be inferred from the data.\n        multi_label: Boolean. Defaults to False.\n        loss: A Keras loss function. Defaults to use `mean_squared_error`.\n        metrics: A list of Keras metrics. Defaults to use `mean_squared_error`.\n        dropout_rate: Float. The dropout rate for the layers.\n            If left unspecified, it will be tuned automatically.\n    """"""\n\n    def __init__(self,\n                 output_dim: Optional[int] = None,\n                 loss: types.LossType = \'mean_squared_error\',\n                 metrics: Optional[types.MetricsType] = None,\n                 dropout_rate: Optional[float] = None,\n                 **kwargs):\n        if metrics is None:\n            metrics = [\'mean_squared_error\']\n        super().__init__(loss=loss,\n                         metrics=metrics,\n                         **kwargs)\n        self.output_dim = output_dim\n        self.dropout_rate = dropout_rate\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'output_dim\': self.output_dim,\n            \'dropout_rate\': self.dropout_rate})\n        return config\n\n    def build(self, hp, inputs=None):\n        if self.output_dim and self.output_shape[-1] != self.output_dim:\n            raise ValueError(\n                \'The data doesn\\\'t match the output_dim. \'\n                \'Expecting {} but got {}\'.format(self.output_dim,\n                                                 self.output_shape[-1]))\n        inputs = nest.flatten(inputs)\n        utils.validate_num_inputs(inputs, 1)\n        input_node = inputs[0]\n        output_node = input_node\n\n        dropout_rate = self.dropout_rate or hp.Choice(\'dropout_rate\',\n                                                      [0.0, 0.25, 0.5],\n                                                      default=0)\n\n        if dropout_rate > 0:\n            output_node = layers.Dropout(dropout_rate)(output_node)\n        output_node = reduction.Flatten().build(hp, output_node)\n        output_node = layers.Dense(self.output_shape[-1],\n                                   name=self.name)(output_node)\n        return output_node\n\n    def get_adapter(self):\n        return adapters.RegressionHeadAdapter(name=self.name)\n\n\nclass SegmentationHead(ClassificationHead):\n    """"""Segmentation layers.\n\n    Use sigmoid and binary crossentropy for binary element segmentation.\n    Use softmax and categorical crossentropy for multi-class\n    (more than 2) segmentation. Use Accuracy as metrics by default.\n\n    The targets passing to the head would have to be tf.data.Dataset, np.ndarray,\n    pd.DataFrame or pd.Series. It can be raw labels, one-hot encoded if more than two\n    classes, or binary encoded for binary element segmentation.\n\n    The raw labels will be encoded to 0s and 1s if two classes were found, or\n    one-hot encoded if more than two classes were found.\n    One pixel only corresponds to one label.\n\n    # Arguments\n        num_classes: Int. Defaults to None. If None, it will be inferred from the\n            data.\n        loss: A Keras loss function. Defaults to use `binary_crossentropy` or\n            `categorical_crossentropy` based on the number of classes.\n        metrics: A list of Keras metrics. Defaults to use \'accuracy\'.\n        dropout_rate: Float. The dropout rate for the layers.\n            If left unspecified, it will be tuned automatically.\n    """"""\n\n    def __init__(self,\n                 num_classes: Optional[int] = None,\n                 loss: Optional[types.LossType] = None,\n                 metrics: Optional[types.MetricsType] = None,\n                 dropout_rate: Optional[float] = None,\n                 **kwargs):\n        super().__init__(loss=loss,\n                         metrics=metrics,\n                         num_classes=num_classes,\n                         dropout_rate=dropout_rate,\n                         **kwargs)\n\n    def build(self, hp, inputs):\n        return inputs\n\n    def get_adapter(self):\n        return adapters.SegmentationHeadAdapter(name=self.name)\n'"
autokeras/blocks/preprocessing.py,0,"b'from typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.python.util import nest\n\nfrom autokeras import adapters\nfrom autokeras import keras_layers\nfrom autokeras.engine import block as block_module\n\n\nclass Normalization(block_module.Block):\n    """""" Perform basic image transformation and augmentation.\n\n    # Arguments\n        axis: Integer or tuple of integers, the axis or axes that should be\n            normalized (typically the features axis). We will normalize each element\n            in the specified axis. The default is \'-1\' (the innermost axis); 0 (the\n            batch axis) is not allowed.\n    """"""\n\n    def __init__(self, axis: int = -1, **kwargs):\n        super().__init__(**kwargs)\n        self.axis = axis\n\n    def build(self, hp, inputs=None):\n        input_node = nest.flatten(inputs)[0]\n        return preprocessing.Normalization(axis=self.axis)(input_node)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\'axis\': self.axis})\n        return config\n\n\nclass TextToIntSequence(block_module.Block):\n    """"""Convert raw texts to sequences of word indices.\n\n    # Arguments\n        output_sequence_length: Int. The maximum length of a sentence. If\n            unspecified, it would be tuned automatically.\n        max_tokens: Int. The maximum size of the vocabulary. Defaults to 20000.\n    """"""\n\n    def __init__(self,\n                 output_sequence_length: Optional[int] = None,\n                 max_tokens: int = 20000,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.output_sequence_length = output_sequence_length\n        self.max_tokens = max_tokens\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'output_sequence_length\': self.output_sequence_length,\n            \'max_tokens\': self.max_tokens,\n        })\n        return config\n\n    def build(self, hp, inputs=None):\n        input_node = nest.flatten(inputs)[0]\n        if self.output_sequence_length is not None:\n            output_sequence_length = self.output_sequence_length\n        else:\n            output_sequence_length = hp.Choice(\'output_sequence_length\',\n                                               [64, 128, 256, 512], default=64)\n        output_node = preprocessing.TextVectorization(\n            max_tokens=self.max_tokens,\n            output_mode=\'int\',\n            output_sequence_length=output_sequence_length)(input_node)\n        return output_node\n\n\nclass TextToNgramVector(block_module.Block):\n    """"""Convert raw texts to n-gram vectors.\n\n    # Arguments\n        max_tokens: Int. The maximum size of the vocabulary. Defaults to 20000.\n        ngrams: Int or tuple of ints. Passing an integer will create ngrams up to\n            that integer, and passing a tuple of integers will create ngrams for the\n            specified values in the tuple. If left unspecified, it will be tuned\n            automatically.\n    """"""\n\n    def __init__(self,\n                 max_tokens: int = 20000,\n                 ngrams: Union[int, Tuple[int], None] = None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.max_tokens = max_tokens\n        self.ngrams = ngrams\n\n    def build(self, hp, inputs=None):\n        input_node = nest.flatten(inputs)[0]\n        if self.ngrams is not None:\n            ngrams = self.ngrams\n        else:\n            ngrams = hp.Int(\'ngrams\', min_value=1, max_value=2, default=2)\n        return preprocessing.TextVectorization(\n            max_tokens=self.max_tokens,\n            ngrams=ngrams,\n            output_mode=\'tf-idf\')(input_node)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'max_tokens\': self.max_tokens,\n            \'ngrams\': self.ngrams,\n        })\n        return config\n\n\nclass ImageAugmentation(block_module.Block):\n    """"""Collection of various image augmentation methods.\n\n    # Arguments\n        translation_factor: A positive float represented as fraction value, or a\n            tuple of 2 representing fraction for translation vertically and\n            horizontally.  For instance, `translation_factor=0.2` result in a random\n            translation factor within 20% of the width and height. Defaults to 0.5.\n        vertical_flip: Boolean. Whether to flip the image vertically.\n            If left unspecified, it will be tuned automatically.\n        horizontal_flip: Boolean. Whether to flip the image horizontally.\n            If left unspecified, it will be tuned automatically.\n        rotation_factor: Float. A positive float represented as fraction of 2pi\n            upper bound for rotating clockwise and counter-clockwise. When\n            represented as a single float, lower = upper.  Defaults to 0.5.\n        zoom_factor: A positive float represented as fraction value, or a tuple of 2\n            representing fraction for zooming vertically and horizontally. For\n            instance, `zoom_factor=0.2` result in a random zoom factor from 80% to\n            120%. Defaults to 0.5.\n        contrast_factor: A positive float represented as fraction of value, or a\n            tuple of size 2 representing lower and upper bound. When represented as a\n            single float, lower = upper. The contrast factor will be randomly picked\n            between [1.0 - lower, 1.0 + upper]. Defaults to 0.5.\n    """"""\n\n    def __init__(self,\n                 translation_factor=0.5,\n                 vertical_flip=None,\n                 horizontal_flip=None,\n                 rotation_factor=0.5,\n                 zoom_factor=0.5,\n                 contrast_factor=0.5,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.translation_factor = translation_factor\n        self.horizontal_flip = horizontal_flip\n        self.vertical_flip = vertical_flip\n        self.rotation_factor = rotation_factor\n        self.zoom_factor = zoom_factor\n        self.contrast_factor = contrast_factor\n\n    @staticmethod\n    def _get_fraction_value(value):\n        if isinstance(value, tuple):\n            return value\n        return value, value\n\n    def build(self, hp, inputs=None):\n        input_node = nest.flatten(inputs)[0]\n        output_node = input_node\n\n        if self.translation_factor != 0 and self.translation_factor != (0, 0):\n            height_factor, width_factor = self._get_fraction_value(\n                self.translation_factor)\n            output_node = preprocessing.RandomTranslation(\n                height_factor, width_factor)(output_node)\n\n        horizontal_flip = self.horizontal_flip\n        if horizontal_flip is None:\n            horizontal_flip = hp.Boolean(\'horizontal_flip\', default=True)\n        vertical_flip = self.vertical_flip\n        if self.vertical_flip is None:\n            vertical_flip = hp.Boolean(\'vertical_flip\', default=True)\n        if not horizontal_flip and not vertical_flip:\n            flip_mode = \'\'\n        elif horizontal_flip and vertical_flip:\n            flip_mode = \'horizontal_and_vertical\'\n        elif horizontal_flip and not vertical_flip:\n            flip_mode = \'horizontal\'\n        elif not horizontal_flip and vertical_flip:\n            flip_mode = \'vertical\'\n        if flip_mode != \'\':\n            output_node = preprocessing.RandomFlip(\n                mode=flip_mode)(output_node)\n\n        if self.rotation_factor != 0:\n            output_node = preprocessing.RandomRotation(\n                self.rotation_factor)(output_node)\n\n        if self.zoom_factor != 0 and self.zoom_factor != (0, 0):\n            height_factor, width_factor = self._get_fraction_value(\n                self.zoom_factor)\n            # TODO: Add back RandomZoom when it is ready.\n            # output_node = preprocessing.RandomZoom(\n            # height_factor, width_factor)(output_node)\n\n        if self.contrast_factor != 0 and self.contrast_factor != (0, 0):\n            output_node = preprocessing.RandomContrast(\n                self.contrast_factor)(output_node)\n\n        return output_node\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'translation_factor\': self.translation_factor,\n            \'horizontal_flip\': self.horizontal_flip,\n            \'vertical_flip\': self.vertical_flip,\n            \'rotation_factor\': self.rotation_factor,\n            \'zoom_factor\': self.zoom_factor,\n            \'contrast_factor\': self.contrast_factor,\n        })\n        return config\n\n\nclass CategoricalToNumerical(block_module.Block):\n    """"""Encode the categorical features to numerical features.""""""\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.column_types = None\n        self.column_names = None\n\n    def build(self, hp, inputs=None):\n        input_node = nest.flatten(inputs)[0]\n        encoding = []\n        for column_name in self.column_names:\n            column_type = self.column_types[column_name]\n            if column_type == adapters.CATEGORICAL:\n                # TODO: Search to use one-hot or int.\n                encoding.append(keras_layers.INT)\n            else:\n                encoding.append(keras_layers.NONE)\n        return keras_layers.MultiColumnCategoricalEncoding(encoding)(input_node)\n\n    @classmethod\n    def from_config(cls, config):\n        column_types = config.pop(\'column_types\')\n        column_names = config.pop(\'column_names\')\n        instance = cls(**config)\n        instance.column_types = column_types\n        instance.column_names = column_names\n        return instance\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\'column_types\': self.column_types,\n                       \'column_names\': self.column_names})\n        return config\n'"
autokeras/blocks/reduction.py,3,"b'from typing import Optional\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.python.util import nest\n\nfrom autokeras.engine import block as block_module\nfrom autokeras.utils import layer_utils\nfrom autokeras.utils import utils\n\n\ndef shape_compatible(shape1, shape2):\n    if len(shape1) != len(shape2):\n        return False\n    # TODO: If they can be the same after passing through any layer,\n    #  they are compatible. e.g. (32, 32, 3), (16, 16, 2) are compatible\n    return shape1[:-1] == shape2[:-1]\n\n\nclass Merge(block_module.Block):\n    """"""Merge block to merge multiple nodes into one.\n\n    # Arguments\n        merge_type: String. \'add\' or \'concatenate\'. If left unspecified, it will be\n            tuned automatically.\n    """"""\n\n    def __init__(self, merge_type: Optional[str] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.merge_type = merge_type\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\'merge_type\': self.merge_type})\n        return config\n\n    def build(self, hp, inputs=None):\n        inputs = nest.flatten(inputs)\n        if len(inputs) == 1:\n            return inputs\n\n        merge_type = self.merge_type or hp.Choice(\'merge_type\',\n                                                  [\'add\', \'concatenate\'],\n                                                  default=\'add\')\n\n        if not all([shape_compatible(input_node.shape, inputs[0].shape) for\n                    input_node in inputs]):\n            new_inputs = []\n            for input_node in inputs:\n                new_inputs.append(Flatten().build(hp, input_node))\n            inputs = new_inputs\n\n        # TODO: Even inputs have different shape[-1], they can still be Add(\n        #  ) after another layer. Check if the inputs are all of the same\n        #  shape\n        if all([input_node.shape == inputs[0].shape for input_node in inputs]):\n            if merge_type == \'add\':\n                return layers.Add(inputs)\n\n        return layers.Concatenate()(inputs)\n\n\nclass Flatten(block_module.Block):\n    """"""Flatten the input tensor with Keras Flatten layer.""""""\n\n    def build(self, hp, inputs=None):\n        inputs = nest.flatten(inputs)\n        utils.validate_num_inputs(inputs, 1)\n        input_node = inputs[0]\n        if len(input_node.shape) > 2:\n            return layers.Flatten()(input_node)\n        return input_node\n\n\nclass SpatialReduction(block_module.Block):\n    """"""Reduce the dimension of a spatial tensor, e.g. image, to a vector.\n\n    # Arguments\n        reduction_type: String. \'flatten\', \'global_max\' or \'global_avg\'.\n            If left unspecified, it will be tuned automatically.\n    """"""\n\n    def __init__(self, reduction_type: Optional[str] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.reduction_type = reduction_type\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\'reduction_type\': self.reduction_type})\n        return config\n\n    def build(self, hp, inputs=None):\n        inputs = nest.flatten(inputs)\n        utils.validate_num_inputs(inputs, 1)\n        input_node = inputs[0]\n        output_node = input_node\n\n        # No need to reduce.\n        if len(output_node.shape) <= 2:\n            return output_node\n\n        reduction_type = self.reduction_type or hp.Choice(\'reduction_type\',\n                                                          [\'flatten\',\n                                                           \'global_max\',\n                                                           \'global_avg\'],\n                                                          default=\'global_avg\')\n        if reduction_type == \'flatten\':\n            output_node = Flatten().build(hp, output_node)\n        elif reduction_type == \'global_max\':\n            output_node = layer_utils.get_global_max_pooling(\n                output_node.shape)()(output_node)\n        elif reduction_type == \'global_avg\':\n            output_node = layer_utils.get_global_average_pooling(\n                output_node.shape)()(output_node)\n        return output_node\n\n\nclass TemporalReduction(block_module.Block):\n    """"""Reduce the dimension of a temporal tensor, e.g. output of RNN, to a vector.\n\n    # Arguments\n        reduction_type: String. \'flatten\', \'global_max\' or \'global_avg\'. If left\n            unspecified, it will be tuned automatically.\n    """"""\n\n    def __init__(self, reduction_type: Optional[str] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.reduction_type = reduction_type\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\'reduction_type\': self.reduction_type})\n        return config\n\n    def build(self, hp, inputs=None):\n        inputs = nest.flatten(inputs)\n        utils.validate_num_inputs(inputs, 1)\n        input_node = inputs[0]\n        output_node = input_node\n\n        # No need to reduce.\n        if len(output_node.shape) <= 2:\n            return output_node\n\n        reduction_type = self.reduction_type or hp.Choice(\'reduction_type\',\n                                                          [\'flatten\',\n                                                           \'global_max\',\n                                                           \'global_avg\'],\n                                                          default=\'global_avg\')\n\n        if reduction_type == \'flatten\':\n            output_node = Flatten().build(hp, output_node)\n        elif reduction_type == \'global_max\':\n            output_node = tf.math.reduce_max(output_node, axis=-2)\n        elif reduction_type == \'global_avg\':\n            output_node = tf.math.reduce_mean(output_node, axis=-2)\n        elif reduction_type == \'global_min\':\n            output_node = tf.math.reduce_min(output_node, axis=-2)\n\n        return output_node\n'"
autokeras/blocks/wrapper.py,0,"b'from tensorflow.python.util import nest\n\nfrom autokeras.blocks import basic\nfrom autokeras.blocks import preprocessing\nfrom autokeras.blocks import reduction\nfrom autokeras.engine import block as block_module\n\n\nclass ImageBlock(block_module.Block):\n    """"""Block for image data.\n\n    The image blocks is a block choosing from ResNetBlock, XceptionBlock, ConvBlock,\n    which is controlled by a hyperparameter, \'block_type\'.\n\n    # Arguments\n        block_type: String. \'resnet\', \'xception\', \'vanilla\'. The type of Block\n            to use. If unspecified, it will be tuned automatically.\n        normalize: Boolean. Whether to channel-wise normalize the images.\n            If unspecified, it will be tuned automatically.\n        augment: Boolean. Whether to do image augmentation. If unspecified,\n            it will be tuned automatically.\n    """"""\n\n    def __init__(self,\n                 block_type=None,\n                 normalize=None,\n                 augment=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.block_type = block_type\n        self.normalize = normalize\n        self.augment = augment\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\'block_type\': self.block_type,\n                       \'normalize\': self.normalize,\n                       \'augment\': self.augment})\n        return config\n\n    def build(self, hp, inputs=None):\n        input_node = nest.flatten(inputs)[0]\n        output_node = input_node\n\n        block_type = self.block_type or hp.Choice(\'block_type\',\n                                                  [\'resnet\', \'xception\', \'vanilla\'],\n                                                  default=\'vanilla\')\n\n        normalize = self.normalize\n        if normalize is None:\n            normalize = hp.Boolean(\'normalize\', default=False)\n        augment = self.augment\n        if augment is None:\n            augment = hp.Boolean(\'augment\', default=False)\n        if normalize:\n            output_node = preprocessing.Normalization().build(hp, output_node)\n        if augment:\n            output_node = preprocessing.ImageAugmentation().build(hp, output_node)\n        if block_type == \'resnet\':\n            output_node = basic.ResNetBlock().build(hp, output_node)\n        elif block_type == \'xception\':\n            output_node = basic.XceptionBlock().build(hp, output_node)\n        elif block_type == \'vanilla\':\n            output_node = basic.ConvBlock().build(hp, output_node)\n        return output_node\n\n\nclass TextBlock(block_module.Block):\n    """"""Block for text data.\n\n    # Arguments\n        max_tokens: Int. The maximum size of the vocabulary.\n            If left unspecified, it will be tuned automatically.\n        vectorizer: String. \'sequence\' or \'ngram\'. If it is \'sequence\',\n            TextToIntSequence will be used. If it is \'ngram\', TextToNgramVector will\n            be used. If unspecified, it will be tuned automatically.\n        pretraining: String. \'random\' (use random weights instead any pretrained\n            model), \'glove\', \'fasttext\' or \'word2vec\'. Use pretrained word embedding.\n            If left unspecified, it will be tuned automatically.\n    """"""\n\n    def __init__(self,\n                 max_tokens=None,\n                 vectorizer=None,\n                 pretraining=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.max_tokens = max_tokens\n        self.vectorizer = vectorizer\n        self.pretraining = pretraining\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'max_tokens\': self.max_tokens,\n            \'vectorizer\': self.vectorizer,\n            \'pretraining\': self.pretraining})\n        return config\n\n    def build(self, hp, inputs=None):\n        input_node = nest.flatten(inputs)[0]\n        output_node = input_node\n        vectorizer = self.vectorizer or hp.Choice(\'vectorizer\',\n                                                  [\'sequence\', \'ngram\'],\n                                                  default=\'sequence\')\n        max_tokens = self.max_tokens or hp.Choice(\'max_tokens\',\n                                                  [500, 5000, 20000],\n                                                  default=5000)\n        if vectorizer == \'ngram\':\n            output_node = preprocessing.TextToNgramVector(\n                max_tokens=max_tokens).build(hp, output_node)\n            output_node = basic.DenseBlock().build(hp, output_node)\n        else:\n            output_node = preprocessing.TextToIntSequence(\n                max_tokens=max_tokens).build(hp, output_node)\n            output_node = basic.Embedding(\n                max_features=max_tokens + 1,\n                pretraining=self.pretraining).build(hp, output_node)\n            output_node = basic.ConvBlock().build(hp, output_node)\n            output_node = reduction.SpatialReduction().build(hp, output_node)\n            output_node = basic.DenseBlock().build(hp, output_node)\n        return output_node\n\n\nclass StructuredDataBlock(block_module.Block):\n    """"""Block for structured data.\n\n    # Arguments\n        categorical_encoding: Boolean. Whether to use the CategoricalToNumerical to\n            encode the categorical features to numerical features. Defaults to True.\n            If specified as None, it will be tuned automatically.\n        seed: Int. Random seed.\n    """"""\n\n    def __init__(self,\n                 categorical_encoding=True,\n                 seed=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.categorical_encoding = categorical_encoding\n        self.seed = seed\n        self.column_types = None\n        self.column_names = None\n\n    @classmethod\n    def from_config(cls, config):\n        column_types = config.pop(\'column_types\')\n        column_names = config.pop(\'column_names\')\n        instance = cls(**config)\n        instance.column_types = column_types\n        instance.column_names = column_names\n        return instance\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\'categorical_encoding\': self.categorical_encoding,\n                       \'seed\': self.seed,\n                       \'column_types\': self.column_types,\n                       \'column_names\': self.column_names})\n        return config\n\n    def build_categorical_encoding(self, hp, input_node):\n        output_node = input_node\n        categorical_encoding = self.categorical_encoding\n        if categorical_encoding is None:\n            categorical_encoding = hp.Choice(\'categorical_encoding\',\n                                             [True, False],\n                                             default=True)\n        if categorical_encoding:\n            block = preprocessing.CategoricalToNumerical()\n            block.column_types = self.column_types\n            block.column_names = self.column_names\n            output_node = block.build(hp, output_node)\n        return output_node\n\n    def build_body(self, hp, input_node):\n        output_node = basic.DenseBlock().build(hp, input_node)\n        return output_node\n\n    def build(self, hp, inputs=None):\n        input_node = nest.flatten(inputs)[0]\n        output_node = self.build_categorical_encoding(hp, input_node)\n        output_node = self.build_body(hp, output_node)\n        return output_node\n\n\nclass TimeseriesBlock(block_module.Block):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def get_config(self):\n        config = super().get_config()\n        return config\n\n    def build(self, hp, inputs=None):\n        input_node = nest.flatten(inputs)[0]\n        output_node = input_node\n        output_node = basic.RNNBlock().build(hp, output_node)\n        return output_node\n\n\nclass GeneralBlock(block_module.Block):\n    """"""A general neural network block when the input type is unknown.\n\n    When the input type is unknown. The GeneralBlock would search in a large space\n    for a good model.\n\n    # Arguments\n        name: String.\n    """"""\n\n    def build(self, hp, inputs=None):\n        raise NotImplementedError\n'"
autokeras/engine/__init__.py,0,b''
autokeras/engine/adapter.py,8,"b'import numpy as np\nimport tensorflow as tf\n\nfrom autokeras.engine import serializable\nfrom autokeras.utils import data_utils\n\n\nclass Adapter(serializable.Serializable):\n    """"""Adpat the input and output format for Keras Model.\n\n    Adapter is used by the input nodes and the heads of the hypermodel. It analyzes\n    the training data to get useful information, e.g., the shape of the data, which\n    is required for building the Keras Model. It also converts the dataset to\n    tf.data.Dataset format.\n\n    # Arguments\n        shape: Tuple of int. The input or output shape of the hypermodel.\n        batch_size: Int. Number of samples per gradient update. Defaults to 32.\n    """"""\n\n    def __init__(self, shape=None, batch_size=32):\n        self.shape = shape\n        self.batch_size = batch_size\n\n    def check(self, dataset):\n        """"""Check if the dataset is valid for the input node.\n\n        # Arguments\n            dataset: Usually numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                The dataset to be checked.\n\n        # Returns\n            Boolean. Whether the dataset is valid for the input node.\n        """"""\n        return True\n\n    def convert_to_dataset(self, dataset):\n        """"""Convert supported formats of datasets to tf.data.Dataset.\n\n        # Arguments\n            dataset: Usually numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                The dataset to be converted.\n\n        # Returns\n            tf.data.Dataset. The converted dataset.\n        """"""\n        if isinstance(dataset, np.ndarray):\n            dataset = tf.data.Dataset.from_tensor_slices(dataset)\n        return data_utils.batch_dataset(dataset, self.batch_size)\n\n    def fit(self, dataset):\n        """"""Analyze the dataset and record useful information.\n\n        # Arguments\n            dataset: tf.data.Dataset.\n        """"""\n        self._record_dataset_shape(dataset)\n\n    def fit_before_convert(self, dataset):\n        """"""Analyze the dataset before converting to tf.data.Dataset.\n\n        # Arguments\n            dataset: Usually numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n        """"""\n        pass\n\n    def fit_transform(self, dataset):\n        self.check(dataset)\n        self.fit_before_convert(dataset)\n        dataset = self.convert_to_dataset(dataset)\n        self.fit(dataset)\n        return dataset\n\n    def _record_dataset_shape(self, dataset):\n        self.shape = data_utils.dataset_shape(dataset)[1:]\n\n    def transform(self, dataset):\n        """"""Transform the input dataset to tf.data.Dataset.\n\n        # Arguments\n            dataset: Usually numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                The dataset to be transformed.\n\n        # Returns\n            tf.data.Dataset. The converted dataset.\n        """"""\n        self.check(dataset)\n        return self.convert_to_dataset(dataset)\n\n    def get_config(self):\n        return {\'shape\': self.shape}\n'"
autokeras/engine/block.py,1,"b'import kerastuner\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\nfrom autokeras.engine import node as node_module\nfrom autokeras.engine import serializable\nfrom autokeras.utils import utils\n\n\nclass Block(kerastuner.HyperModel, serializable.Serializable):\n    """"""The base class for different Block.\n\n    The Block can be connected together to build the search space\n    for an AutoModel. Notably, many args in the __init__ function are defaults to\n    be a tunable variable when not specified by the user.\n\n    # Arguments\n        name: String. The name of the block. If unspecified, it will be set\n            automatically with the class name.\n    """"""\n\n    def __init__(self, name: str = None, **kwargs):\n        if not name:\n            prefix = self.__class__.__name__\n            name = prefix + \'_\' + str(tf.keras.backend.get_uid(prefix))\n            name = utils.to_snake_case(name)\n        super().__init__(name=name, **kwargs)\n        self.inputs = None\n        self.outputs = None\n        self._num_output_node = 1\n\n    def _build_wrapper(self, hp, *args, **kwargs):\n        with hp.name_scope(self.name):\n            return super()._build_wrapper(hp, *args, **kwargs)\n\n    def __call__(self, inputs):\n        """"""Functional API.\n\n        # Arguments\n            inputs: A list of input node(s) or a single input node for the block.\n\n        # Returns\n            list: A list of output node(s) of the Block.\n        """"""\n        self.inputs = nest.flatten(inputs)\n        for input_node in self.inputs:\n            if not isinstance(input_node, node_module.Node):\n                raise TypeError(\'Expect the inputs to layer {name} to be \'\n                                \'a Node, but got {type}.\'.format(\n                                    name=self.name,\n                                    type=type(input_node)))\n            input_node.add_out_block(self)\n        self.outputs = []\n        for _ in range(self._num_output_node):\n            output_node = node_module.Node()\n            output_node.add_in_block(self)\n            self.outputs.append(output_node)\n        return self.outputs\n\n    def build(self, hp, inputs=None):\n        """"""Build the Block into a real Keras Model.\n\n        The subclasses should override this function and return the output node.\n\n        # Arguments\n            hp: HyperParameters. The hyperparameters for building the model.\n            inputs: A list of input node(s).\n        """"""\n        return super().build(hp)\n\n    def get_config(self):\n        """"""Get the configuration of the preprocessor.\n\n        # Returns\n            A dictionary of configurations of the preprocessor.\n        """"""\n        return {\'name\': self.name, \'tunable\': self.tunable}\n'"
autokeras/engine/encoder.py,0,"b'from autokeras.engine import serializable\n\n\nclass Encoder(serializable.Serializable):\n    """"""Base class for encoders of the prediction targets.\n\n    # Arguments\n        num_classes: Int. The number of classes. Defaults to None.\n    """"""\n\n    def __init__(self, num_classes=None):\n        self.num_classes = num_classes\n        self._labels = None\n        self._int_to_label = {}\n\n    def fit_with_labels(self, data):\n        """"""Fit the encoder with all the labels.\n\n        # Arguments\n            data: numpy.ndarray. The original labels.\n        """"""\n        raise NotImplementedError\n\n    def encode(self, data):\n        """"""Encode the original labels.\n\n        # Arguments\n            data: numpy.ndarray. The original labels.\n\n        # Returns\n            numpy.ndarray. The encoded labels.\n        """"""\n        raise NotImplementedError\n\n    def decode(self, data):\n        """"""Decode the encoded labels to original labels.\n\n        # Arguments\n            data: numpy.ndarray. The encoded labels.\n\n        # Returns\n            numpy.ndarray. The original labels.\n        """"""\n        raise NotImplementedError\n\n    def get_config(self):\n        return {\n            \'num_classes\': self.num_classes,\n            \'labels\': self._labels,\n            \'int_to_label\': self._int_to_label,\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        obj = super().from_config(config)\n        obj._labels = config[\'labels\']\n        obj._int_to_label = config[\'int_to_label\']\n'"
autokeras/engine/head.py,4,"b'import tensorflow as tf\n\nfrom autokeras.engine import block as block_module\nfrom autokeras.engine import io_hypermodel\n\n\ndef serialize_metrics(metrics):\n    serialized = []\n    for metric in metrics:\n        if isinstance(metric, str):\n            serialized.append([metric])\n        else:\n            serialized.append(tf.keras.metrics.serialize(metric))\n    return serialized\n\n\ndef deserialize_metrics(metrics):\n    deserialized = []\n    for metric in metrics:\n        if isinstance(metric, list):\n            deserialized.append(metric[0])\n        else:\n            deserialized.append(tf.keras.metrics.deserialize(metric))\n    return deserialized\n\n\ndef serialize_loss(loss):\n    if isinstance(loss, str):\n        return [loss]\n    return tf.keras.losses.serialize(loss)\n\n\ndef deserialize_loss(loss):\n    if isinstance(loss, list):\n        return loss[0]\n    return tf.keras.losses.deserialize(loss)\n\n\nclass Head(block_module.Block, io_hypermodel.IOHyperModel):\n    """"""Base class for the heads, e.g. classification, regression.\n\n    # Arguments\n        loss: A Keras loss function. Defaults to None. If None, the loss will be\n            inferred from the AutoModel.\n        metrics: A list of Keras metrics. Defaults to None. If None, the metrics will\n            be inferred from the AutoModel.\n        output_shape: Tuple of int(s). Defaults to None. If None, the output shape\n            will be inferred from the AutoModel.\n    """"""\n\n    def __init__(self, loss=None, metrics=None, output_shape=None, **kwargs):\n        super().__init__(**kwargs)\n        self.output_shape = output_shape\n        self.loss = loss\n        if metrics is None:\n            metrics = []\n        self.metrics = metrics\n        # Mark if the head should directly output the input tensor.\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \'loss\': serialize_loss(self.loss),\n            \'metrics\': serialize_metrics(self.metrics),\n            \'output_shape\': self.output_shape\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        config[\'loss\'] = deserialize_loss(config[\'loss\'])\n        config[\'metrics\'] = deserialize_metrics(config[\'metrics\'])\n        return super().from_config(config)\n\n    def build(self, hp, inputs=None):\n        raise NotImplementedError\n\n    def config_from_adapter(self, adapter):\n        self.output_shape = adapter.shape\n'"
autokeras/engine/io_hypermodel.py,0,"b'class IOHyperModel(object):\n    """"""A mixin class connecting the input nodes and heads with the adapters.\n\n    This class is extended by the input nodes and the heads. The AutoModel calls the\n    functions to get the corresponding adapters and pass the information back to the\n    input nodes and heads.\n    """"""\n\n    def get_adapter(self):\n        """"""Get the corresponding adapter.\n\n        # Returns\n            An instance of a subclass of autokeras.engine.Adapter.\n        """"""\n        raise NotImplementedError\n\n    def config_from_adapter(self, adapter):\n        """"""Load the learned information on dataset from the adapter.\n\n        # Arguments\n            adapter: An instance of a subclass of autokeras.engine.Adapter.\n        """"""\n        raise NotImplementedError\n'"
autokeras/engine/node.py,0,"b'from autokeras.engine import serializable\n\n\nclass Node(serializable.Serializable):\n    """"""The nodes in a network connecting the blocks.""""""\n\n    def __init__(self, shape=None):\n        super().__init__()\n        self.in_blocks = []\n        self.out_blocks = []\n        self.shape = shape\n\n    def add_in_block(self, hypermodel):\n        self.in_blocks.append(hypermodel)\n\n    def add_out_block(self, hypermodel):\n        self.out_blocks.append(hypermodel)\n\n    def build(self):\n        raise NotImplementedError\n\n    def get_config(self):\n        return {\'shape\': self.shape}\n'"
autokeras/engine/preprocessor.py,4,"b'import kerastuner\n\nfrom autokeras.engine import serializable\n\n\nclass Preprocessor(kerastuner.HyperModel, serializable.Serializable):\n    """"""Input data preprocessor search space.\n\n    This class defines the search space for input data preprocessor. A\n    preprocessor transforms the dataset using `tf.data` operations.\n    """"""\n\n    def build(self, hp, x):\n        """"""Build the `tf.data` input preprocessor.\n\n        # Arguments\n            hp: `HyperParameters` instance. The hyperparameters for building the\n                model.\n            x: `tf.data.Dataset` instance. The input data for preprocessing.\n\n        # Returns\n            `tf.data.Dataset`. The preprocessed data to pass to the model.\n        """"""\n        raise NotImplementedError\n'"
autokeras/engine/serializable.py,0,"b'class Serializable(object):\n    """"""Serializable from and to JSON with same mechanism as Keras Layer.""""""\n\n    def get_config(self):\n        """"""Returns the current config of this object.\n\n        # Returns\n            Dictionary.\n        """"""\n        raise NotImplementedError\n\n    @classmethod\n    def from_config(cls, config):\n        """"""Build an instance from the config of this object.\n\n        # Arguments\n            config: Dict. The config of the object.\n        """"""\n        return cls(**config)\n'"
autokeras/engine/tuner.py,2,"b'import copy\nimport os\n\nimport kerastuner\nimport tensorflow as tf\nfrom tensorflow.keras import callbacks as tf_callbacks\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.python.util import nest\n\nfrom autokeras.utils import utils\n\n\nclass AutoTuner(kerastuner.engine.tuner.Tuner):\n    """"""A Tuner class based on KerasTuner for AutoKeras.\n\n    Different from KerasTuner\'s Tuner class. AutoTuner\'s not only tunes the\n    Hypermodel which can be directly built into a Keras model, but also the\n    preprocessors. Therefore, a HyperGraph stores the overall search space containing\n    both the Preprocessors and Hypermodel. For every trial, the HyperGraph build the\n    PreprocessGraph and KerasGraph with the provided HyperParameters.\n\n    The AutoTuner uses EarlyStopping for acceleration during the search and fully\n    train the model with full epochs and with both training and validation data.\n    The fully trained model is the best model to be used by AutoModel.\n\n    # Arguments\n        preprocessors: An instance or list of `Preprocessor` objects corresponding to\n            each AutoModel input, to preprocess a `tf.data.Dataset` before passing it\n            to the model. Defaults to None (no external preprocessing).\n        **kwargs: The args supported by KerasTuner.\n    """"""\n\n    def __init__(self, preprocessors=None, **kwargs):\n        super().__init__(**kwargs)\n        self.preprocessors = nest.flatten(preprocessors)\n        self._finished = False\n        # Save or load the HyperModel.\n        self.hypermodel.hypermodel.save(os.path.join(self.project_dir, \'graph\'))\n\n    # Override the function to prevent building the model during initialization.\n    def _populate_initial_space(self):\n        pass\n\n    def get_best_model(self):\n        model = super().get_best_models()[0]\n        model.load_weights(self.best_model_path)\n        return model\n\n    def _on_train_begin(self, model, hp, x, *args, **kwargs):\n        """"""Adapt the preprocessing layers and tune the fit arguments.""""""\n        self.adapt(model, x)\n\n    @staticmethod\n    def adapt(model, dataset):\n        """"""Adapt the preprocessing layers in the model.""""""\n        # Currently, only support using the original dataset to adapt all the\n        # preprocessing layers before the first non-preprocessing layer.\n        # TODO: Use PreprocessingStage for preprocessing layers adapt.\n        # TODO: Use Keras Tuner for preprocessing layers adapt.\n        x = dataset.map(lambda x, y: x)\n\n        def get_output_layer(tensor):\n            tensor = nest.flatten(tensor)[0]\n            for layer in model.layers:\n                if isinstance(layer, tf.keras.layers.InputLayer):\n                    continue\n                input_node = nest.flatten(layer.input)[0]\n                if input_node is tensor:\n                    return layer\n            return None\n\n        for index, input_node in enumerate(nest.flatten(model.input)):\n            temp_x = x.map(lambda *args: nest.flatten(args)[index])\n            layer = get_output_layer(input_node)\n            while isinstance(layer, preprocessing.PreprocessingLayer):\n                layer.adapt(temp_x)\n                layer = get_output_layer(layer.output)\n        return model\n\n    def search(self,\n               epochs=None,\n               callbacks=None,\n               fit_on_val_data=False,\n               **fit_kwargs):\n        """"""Search for the best HyperParameters.\n\n        If there is not early-stopping in the callbacks, the early-stopping callback\n        is injected to accelerate the search process. At the end of the search, the\n        best model will be fully trained with the specified number of epochs.\n\n        # Arguments\n            callbacks: A list of callback functions. Defaults to None.\n            fit_on_val_data: Boolean. Use the training set and validation set for the\n                final fit of the best model.\n        """"""\n        if self._finished:\n            return\n\n        if callbacks is None:\n            callbacks = []\n\n        # Insert early-stopping for adaptive number of epochs.\n        if epochs is None:\n            epochs = 1000\n            if not utils.contain_instance(callbacks, tf_callbacks.EarlyStopping):\n                callbacks.append(tf_callbacks.EarlyStopping(patience=10))\n\n        # Insert early-stopping for acceleration.\n        acceleration = False\n        new_callbacks = self._deepcopy_callbacks(callbacks)\n        if not utils.contain_instance(callbacks, tf_callbacks.EarlyStopping):\n            acceleration = True\n            new_callbacks.append(tf_callbacks.EarlyStopping(patience=10))\n\n        super().search(epochs=epochs, callbacks=new_callbacks, **fit_kwargs)\n\n        # Fully train the best model with original callbacks.\n        if acceleration or fit_on_val_data:\n            copied_fit_kwargs = copy.copy(fit_kwargs)\n            if fit_on_val_data:\n                # Concatenate training and validation data.\n                copied_fit_kwargs[\'x\'] = copied_fit_kwargs[\'x\'].concatenate(\n                    fit_kwargs[\'validation_data\'])\n                copied_fit_kwargs.pop(\'validation_data\')\n                # Remove early-stopping since no validation data.\n                if utils.contain_instance(callbacks, tf_callbacks.EarlyStopping):\n                    copied_fit_kwargs[\'callbacks\'] = [\n                        copy.deepcopy(callbacks)\n                        for callback in callbacks\n                        if not isinstance(callback, tf_callbacks.EarlyStopping)]\n                    # Use best trial number of epochs.\n                    copied_fit_kwargs[\'epochs\'] = self._get_best_trial_epochs()\n            model = self.final_fit(**copied_fit_kwargs)\n        else:\n            model = self.get_best_models()[0]\n\n        model.save_weights(self.best_model_path)\n        self._finished = True\n\n    def _get_best_trial_epochs(self):\n        best_trial = self.oracle.get_best_trials(1)[0]\n        return len(best_trial.metrics.metrics[\'val_loss\']._observations)\n\n    def final_fit(self, x=None, **fit_kwargs):\n        best_trial = self.oracle.get_best_trials(1)[0]\n        best_hp = best_trial.hyperparameters\n        model = self.hypermodel.build(best_hp)\n        self.adapt(model, x)\n        model.fit(x, **fit_kwargs)\n        return model\n\n    @property\n    def best_model_path(self):\n        return os.path.join(self.project_dir, \'best_model\')\n\n    @property\n    def objective(self):\n        return self.tuner.objective\n\n    @property\n    def max_trials(self):\n        return self.oracle.max_trials\n'"
autokeras/tasks/__init__.py,0,b'from autokeras.tasks.image import ImageClassifier\nfrom autokeras.tasks.image import ImageRegressor\nfrom autokeras.tasks.structured_data import StructuredDataClassifier\nfrom autokeras.tasks.structured_data import StructuredDataRegressor\nfrom autokeras.tasks.text import TextClassifier\nfrom autokeras.tasks.text import TextRegressor\nfrom autokeras.tasks.time_series_forecaster import TimeseriesForecaster\n'
autokeras/tasks/image.py,5,"b'from pathlib import Path\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport tensorflow as tf\n\nfrom autokeras import auto_model\nfrom autokeras import blocks\nfrom autokeras import nodes as input_module\nfrom autokeras.tuners import greedy\nfrom autokeras.tuners import task_specific\nfrom autokeras.utils import types\n\n\nclass SupervisedImagePipeline(auto_model.AutoModel):\n\n    def __init__(self, outputs, **kwargs):\n        super().__init__(inputs=input_module.ImageInput(),\n                         outputs=outputs,\n                         **kwargs)\n\n\nclass ImageClassifier(SupervisedImagePipeline):\n    """"""AutoKeras image classification class.\n\n    # Arguments\n        num_classes: Int. Defaults to None. If None, it will be inferred from the\n            data.\n        multi_label: Boolean. Defaults to False.\n        loss: A Keras loss function. Defaults to use \'binary_crossentropy\' or\n            \'categorical_crossentropy\' based on the number of classes.\n        metrics: A list of Keras metrics. Defaults to use \'accuracy\'.\n        project_name: String. The name of the AutoModel.\n            Defaults to \'image_classifier\'.\n        max_trials: Int. The maximum number of different Keras Models to try.\n            The search may finish before reaching the max_trials. Defaults to 100.\n        directory: String. The path to a directory for storing the search outputs.\n            Defaults to None, which would create a folder with the name of the\n            AutoModel in the current directory.\n        objective: String. Name of model metric to minimize\n            or maximize, e.g. \'val_accuracy\'. Defaults to \'val_loss\'.\n        overwrite: Boolean. Defaults to `True`. If `False`, reloads an existing\n            project of the same name if one is found. Otherwise, overwrites the\n            project.\n        seed: Int. Random seed.\n        **kwargs: Any arguments supported by AutoModel.\n    """"""\n\n    def __init__(self,\n                 num_classes: Optional[int] = None,\n                 multi_label: bool = False,\n                 loss: types.LossType = None,\n                 metrics: Optional[types.MetricsType] = None,\n                 project_name: str = \'image_classifier\',\n                 max_trials: int = 100,\n                 directory: Union[str, Path, None] = None,\n                 objective: str = \'val_loss\',\n                 overwrite: bool = True,\n                 seed: Optional[int] = None,\n                 **kwargs):\n        super().__init__(\n            outputs=blocks.ClassificationHead(num_classes=num_classes,\n                                              multi_label=multi_label,\n                                              loss=loss,\n                                              metrics=metrics),\n            max_trials=max_trials,\n            directory=directory,\n            project_name=project_name,\n            objective=objective,\n            tuner=task_specific.ImageClassifierTuner,\n            overwrite=overwrite,\n            seed=seed,\n            **kwargs)\n\n    def fit(self,\n            x: Optional[types.DatasetType] = None,\n            y: Optional[types.DatasetType] = None,\n            epochs: Optional[int] = None,\n            callbacks: Optional[List[tf.keras.callbacks.Callback]] = None,\n            validation_split: Optional[float] = 0.2,\n            validation_data: Union[\n                tf.data.Dataset,\n                Tuple[types.DatasetType, types.DatasetType],\n                None] = None,\n            **kwargs):\n        """"""Search for the best model and hyperparameters for the AutoModel.\n\n        It will search for the best model based on the performances on\n        validation data.\n\n        # Arguments\n            x: numpy.ndarray or tensorflow.Dataset. Training data x. The shape of\n                the data should be (samples, width, height)\n                or (samples, width, height, channels).\n            y: numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw\n                labels, one-hot encoded if more than two classes, or binary encoded\n                for binary classification.\n            epochs: Int. The number of epochs to train each model during the search.\n                If unspecified, by default we train for a maximum of 1000 epochs,\n                but we stop training if the validation loss stops improving for 10\n                epochs (unless you specified an EarlyStopping callback as part of\n                the callbacks argument, in which case the EarlyStopping callback you\n                specified will determine early stopping).\n            callbacks: List of Keras callbacks to apply during training and\n                validation.\n            validation_split: Float between 0 and 1. Defaults to 0.2.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling. This argument is\n                not supported when `x` is a dataset.\n                The best model found would be fit on the entire dataset including the\n                validation data.\n            validation_data: Data on which to evaluate the loss and any model metrics\n                at the end of each epoch. The model will not be trained on this data.\n                `validation_data` will override `validation_split`. The type of the\n                validation data should be the same as the training data.\n                The best model found would be fit on the training dataset without the\n                validation data.\n            **kwargs: Any arguments supported by keras.Model.fit.\n        """"""\n        super().fit(x=x,\n                    y=y,\n                    epochs=epochs,\n                    callbacks=callbacks,\n                    validation_split=validation_split,\n                    validation_data=validation_data,\n                    **kwargs)\n\n\nclass ImageRegressor(SupervisedImagePipeline):\n    """"""AutoKeras image regression class.\n\n    # Arguments\n        output_dim: Int. The number of output dimensions. Defaults to None.\n            If None, it will be inferred from the data.\n        loss: A Keras loss function. Defaults to use \'mean_squared_error\'.\n        metrics: A list of Keras metrics. Defaults to use \'mean_squared_error\'.\n        project_name: String. The name of the AutoModel.\n            Defaults to \'image_regressor\'.\n        max_trials: Int. The maximum number of different Keras Models to try.\n            The search may finish before reaching the max_trials. Defaults to 100.\n        directory: String. The path to a directory for storing the search outputs.\n            Defaults to None, which would create a folder with the name of the\n            AutoModel in the current directory.\n        objective: String. Name of model metric to minimize\n            or maximize, e.g. \'val_accuracy\'. Defaults to \'val_loss\'.\n        overwrite: Boolean. Defaults to `True`. If `False`, reloads an existing\n            project of the same name if one is found. Otherwise, overwrites the\n            project.\n        seed: Int. Random seed.\n        **kwargs: Any arguments supported by AutoModel.\n    """"""\n\n    def __init__(self,\n                 output_dim: Optional[int] = None,\n                 loss: types.LossType = \'mean_squared_error\',\n                 metrics: Optional[types.MetricsType] = None,\n                 project_name: str = \'image_regressor\',\n                 max_trials: int = 100,\n                 directory: Union[str, Path, None] = None,\n                 objective: str = \'val_loss\',\n                 overwrite: bool = True,\n                 seed: Optional[int] = None,\n                 **kwargs):\n        super().__init__(\n            outputs=blocks.RegressionHead(output_dim=output_dim,\n                                          loss=loss,\n                                          metrics=metrics),\n            max_trials=max_trials,\n            directory=directory,\n            project_name=project_name,\n            objective=objective,\n            tuner=greedy.Greedy,\n            overwrite=overwrite,\n            seed=seed,\n            **kwargs)\n\n    def fit(\n            self,\n            x: Optional[types.DatasetType] = None,\n            y: Optional[types.DatasetType] = None,\n            epochs: Optional[int] = None,\n            callbacks: Optional[List[tf.keras.callbacks.Callback]] = None,\n            validation_split: Optional[float] = 0.2,\n            validation_data: Union[types.DatasetType,\n                                   Tuple[types.DatasetType], None] = None,\n            **kwargs):\n        """"""Search for the best model and hyperparameters for the AutoModel.\n\n        It will search for the best model based on the performances on\n        validation data.\n\n        # Arguments\n            x: numpy.ndarray or tensorflow.Dataset. Training data x. The shape of\n                the data should be (samples, width, height) or\n                (samples, width, height, channels).\n            y: numpy.ndarray or tensorflow.Dataset. Training data y. The targets\n                passing to the head would have to be tf.data.Dataset, np.ndarray,\n                pd.DataFrame or pd.Series. It can be single-column or multi-column.\n                The values should all be numerical.\n            epochs: Int. The number of epochs to train each model during the search.\n                If unspecified, by default we train for a maximum of 1000 epochs,\n                but we stop training if the validation loss stops improving for 10\n                epochs (unless you specified an EarlyStopping callback as part of\n                the callbacks argument, in which case the EarlyStopping callback you\n                specified will determine early stopping).\n            callbacks: List of Keras callbacks to apply during training and\n                validation.\n            validation_split: Float between 0 and 1. Defaults to 0.2.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling. This argument is\n                not supported when `x` is a dataset.\n                The best model found would be fit on the entire dataset including the\n                validation data.\n            validation_data: Data on which to evaluate the loss and any model metrics\n                at the end of each epoch. The model will not be trained on this data.\n                `validation_data` will override `validation_split`. The type of the\n                validation data should be the same as the training data.\n                The best model found would be fit on the training dataset without the\n                validation data.\n            **kwargs: Any arguments supported by keras.Model.fit.\n        """"""\n        super().fit(x=x,\n                    y=y,\n                    epochs=epochs,\n                    callbacks=callbacks,\n                    validation_split=validation_split,\n                    validation_data=validation_data,\n                    **kwargs)\n\n\nclass ImageSegmenter(SupervisedImagePipeline):\n    """"""AutoKeras image segmentation class.\n    # Arguments\n        num_classes: Int. Defaults to None. If None, it will be inferred from the\n            data.\n        loss: A Keras loss function. Defaults to use \'binary_crossentropy\' or\n            \'categorical_crossentropy\' based on the number of classes.\n        metrics: A list of metrics used to measure the accuracy of the model,\n            default to \'accuracy\'.\n        project_name: String. The name of the AutoModel.\n            Defaults to \'image_segmenter\'.\n        max_trials: Int. The maximum number of different Keras Models to try.\n            The search may finish before reaching the max_trials. Defaults to 100.\n        directory: String. The path to a directory for storing the search outputs.\n            Defaults to None, which would create a folder with the name of the\n            AutoModel in the current directory.\n        objective: String. Name of model metric to minimize\n            or maximize, e.g. \'val_accuracy\'. Defaults to \'val_loss\'.\n        overwrite: Boolean. Defaults to `True`. If `False`, reloads an existing\n            project of the same name if one is found. Otherwise, overwrites the\n            project.\n        seed: Int. Random seed.\n        **kwargs: Any arguments supported by AutoModel.\n    """"""\n\n    def __init__(self,\n                 num_classes: Optional[int] = None,\n                 loss: types.LossType = None,\n                 metrics: Optional[types.MetricsType] = None,\n                 project_name: str = \'image_classifier\',\n                 max_trials: int = 100,\n                 directory: Union[str, Path, None] = None,\n                 objective: str = \'val_loss\',\n                 overwrite: bool = True,\n                 seed: Optional[int] = None,\n                 **kwargs):\n        super().__init__(\n            outputs=blocks.SegmentationHead(num_classes=num_classes,\n                                            loss=loss,\n                                            metrics=metrics),\n            max_trials=max_trials,\n            directory=directory,\n            project_name=project_name,\n            objective=objective,\n            tuner=greedy.Greedy,\n            overwrite=overwrite,\n            seed=seed,\n            **kwargs)\n\n    def fit(\n            self,\n            x: Optional[types.DatasetType] = None,\n            y: Optional[types.DatasetType] = None,\n            epochs: Optional[int] = None,\n            callbacks: Optional[List[tf.keras.callbacks.Callback]] = None,\n            validation_split: Optional[float] = 0.2,\n            validation_data: Union[types.DatasetType,\n                                   Tuple[types.DatasetType], None] = None,\n            **kwargs):\n        """"""Search for the best model and hyperparameters for the AutoModel.\n\n        It will search for the best model based on the performances on\n        validation data.\n\n        # Arguments\n            x: numpy.ndarray or tensorflow.Dataset. Training image dataset x.\n                The shape of the data should be (samples, width, height) or\n                (samples, width, height, channels).\n            y: numpy.ndarray or tensorflow.Dataset. Training image data set y.\n                It should be a tensor and the height and width should be the same\n                as x. Each element in the tensor is the label of the corresponding\n                pixel.\n            epochs: Int. The number of epochs to train each model during the search.\n                If unspecified, by default we train for a maximum of 1000 epochs,\n                but we stop training if the validation loss stops improving for 10\n                epochs (unless you specified an EarlyStopping callback as part of\n                the callbacks argument, in which case the EarlyStopping callback you\n                specified will determine early stopping).\n            callbacks: List of Keras callbacks to apply during training and\n                validation.\n            validation_split: Float between 0 and 1. Defaults to 0.2.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling. This argument is\n                not supported when `x` is a dataset.\n                The best model found would be fit on the entire dataset including the\n                validation data.\n            validation_data: Data on which to evaluate the loss and any model metrics\n                at the end of each epoch. The model will not be trained on this data.\n                `validation_data` will override `validation_split`. The type of the\n                validation data should be the same as the training data.\n                The best model found would be fit on the training dataset without the\n                validation data.\n            **kwargs: Any arguments supported by keras.Model.fit.\n        """"""\n        super().fit(x=x,\n                    y=y,\n                    epochs=epochs,\n                    callbacks=callbacks,\n                    validation_split=validation_split,\n                    validation_data=validation_data,\n                    **kwargs)\n'"
autokeras/tasks/structured_data.py,0,"b'import pathlib\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Union\n\nimport pandas as pd\n\nfrom autokeras import auto_model\nfrom autokeras import blocks\nfrom autokeras import nodes as input_module\nfrom autokeras.tasks.structured_data_mixin import StructuredDataMixin\nfrom autokeras.tuners import greedy\nfrom autokeras.utils import types\n\n\nclass SupervisedStructuredDataPipeline(StructuredDataMixin, auto_model.AutoModel):\n\n    def __init__(self, outputs, column_names, column_types, **kwargs):\n        inputs = input_module.StructuredDataInput()\n        inputs.column_types = column_types\n        inputs.column_names = column_names\n        self.check(column_names, column_types)\n        super().__init__(inputs=inputs,\n                         outputs=outputs,\n                         **kwargs)\n        self._target_col_name = None\n\n    @staticmethod\n    def _read_from_csv(x, y):\n        df = pd.read_csv(x)\n        target = df.pop(y).to_numpy()\n        return df, target\n\n    def fit(self,\n            x=None,\n            y=None,\n            epochs=None,\n            callbacks=None,\n            validation_split=0.2,\n            validation_data=None,\n            **kwargs):\n        """"""Search for the best model and hyperparameters for the task.\n\n        # Arguments\n            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                Training data x. If the data is from a csv file, it should be a\n                string specifying the path of the csv file of the training data.\n            y: String, numpy.ndarray, or tensorflow.Dataset. Training data y.\n                If the data is from a csv file, it should be a string, which is the\n                name of the target column. Otherwise, it can be single-column or\n                multi-column. The values should all be numerical.\n            epochs: Int. The number of epochs to train each model during the search.\n                If unspecified, we would use epochs equal to 1000 and early stopping\n                with patience equal to 30.\n            callbacks: List of Keras callbacks to apply during training and\n                validation.\n            validation_split: Float between 0 and 1. Defaults to 0.2.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling. This argument is\n                not supported when `x` is a dataset.\n                The best model found would be fit on the entire dataset including the\n                validation data.\n            validation_data: Data on which to evaluate the loss and any model metrics\n                at the end of each epoch. The model will not be trained on this data.\n                `validation_data` will override `validation_split`. The type of the\n                validation data should be the same as the training data.\n                The best model found would be fit on the training dataset without the\n                validation data.\n            **kwargs: Any arguments supported by keras.Model.fit.\n        """"""\n        # x is file path of training data\n        if isinstance(x, str):\n            self._target_col_name = y\n            x, y = self._read_from_csv(x, y)\n        if validation_data:\n            x_val, y_val = validation_data\n            if isinstance(x_val, str):\n                validation_data = self._read_from_csv(x_val, y_val)\n\n        super().fit(x=x,\n                    y=y,\n                    epochs=epochs,\n                    callbacks=callbacks,\n                    validation_split=validation_split,\n                    validation_data=validation_data,\n                    **kwargs)\n\n    def predict(self, x, batch_size=32, **kwargs):\n        """"""Predict the output for a given testing data.\n\n        # Arguments\n            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                Testing data x. If the data is from a csv file, it should be a\n                string specifying the path of the csv file of the testing data.\n            batch_size: Int. Defaults to 32.\n            **kwargs: Any arguments supported by keras.Model.predict.\n\n        # Returns\n            A list of numpy.ndarray objects or a single numpy.ndarray.\n            The predicted results.\n        """"""\n        x = self.read_for_predict(x)\n\n        return super().predict(x=x,\n                               batch_size=batch_size,\n                               **kwargs)\n\n    def evaluate(self, x, y=None, batch_size=32, **kwargs):\n        """"""Evaluate the best model for the given data.\n\n        # Arguments\n            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                Testing data x. If the data is from a csv file, it should be a\n                string specifying the path of the csv file of the testing data.\n            y: String, numpy.ndarray, or tensorflow.Dataset. Testing data y.\n                If the data is from a csv file, it should be a string corresponding\n                to the label column.\n            batch_size: Int. Defaults to 32.\n            **kwargs: Any arguments supported by keras.Model.evaluate.\n\n        # Returns\n            Scalar test loss (if the model has a single output and no metrics) or\n            list of scalars (if the model has multiple outputs and/or metrics).\n            The attribute model.metrics_names will give you the display labels for\n            the scalar outputs.\n        """"""\n        if isinstance(x, str):\n            x, y = self._read_from_csv(x, y)\n        return super().evaluate(x=x,\n                                y=y,\n                                batch_size=batch_size,\n                                **kwargs)\n\n\nclass StructuredDataClassifier(SupervisedStructuredDataPipeline):\n    """"""AutoKeras structured data classification class.\n\n    # Arguments\n        column_names: A list of strings specifying the names of the columns. The\n            length of the list should be equal to the number of columns of the data\n            excluding the target column. Defaults to None. If None, it will obtained\n            from the header of the csv file or the pandas.DataFrame.\n        column_types: Dict. The keys are the column names. The values should either\n            be \'numerical\' or \'categorical\', indicating the type of that column.\n            Defaults to None. If not None, the column_names need to be specified.\n            If None, it will be inferred from the data.\n        num_classes: Int. Defaults to None. If None, it will be inferred from the\n            data.\n        multi_label: Boolean. Defaults to False.\n        loss: A Keras loss function. Defaults to use \'binary_crossentropy\' or\n            \'categorical_crossentropy\' based on the number of classes.\n        metrics: A list of Keras metrics. Defaults to use \'accuracy\'.\n        project_name: String. The name of the AutoModel. Defaults to\n            \'structured_data_classifier\'.\n        max_trials: Int. The maximum number of different Keras Models to try.\n            The search may finish before reaching the max_trials. Defaults to 100.\n        directory: String. The path to a directory for storing the search outputs.\n            Defaults to None, which would create a folder with the name of the\n            AutoModel in the current directory.\n        objective: String. Name of model metric to minimize\n            or maximize. Defaults to \'val_accuracy\'.\n        overwrite: Boolean. Defaults to `True`. If `False`, reloads an existing\n            project of the same name if one is found. Otherwise, overwrites the\n            project.\n        seed: Int. Random seed.\n        **kwargs: Any arguments supported by AutoModel.\n    """"""\n\n    def __init__(self,\n                 column_names=None,\n                 column_types=None,\n                 num_classes=None,\n                 multi_label=False,\n                 loss=None,\n                 metrics=None,\n                 project_name=\'structured_data_classifier\',\n                 max_trials=100,\n                 directory=None,\n                 objective=\'val_accuracy\',\n                 overwrite=True,\n                 seed=None,\n                 **kwargs):\n        super().__init__(\n            outputs=blocks.ClassificationHead(num_classes=num_classes,\n                                              multi_label=multi_label,\n                                              loss=loss,\n                                              metrics=metrics),\n            column_names=column_names,\n            column_types=column_types,\n            max_trials=max_trials,\n            directory=directory,\n            project_name=project_name,\n            objective=objective,\n            tuner=greedy.Greedy,\n            overwrite=overwrite,\n            seed=seed,\n            **kwargs)\n\n    def fit(self,\n            x=None,\n            y=None,\n            epochs=None,\n            callbacks=None,\n            validation_split=0.2,\n            validation_data=None,\n            **kwargs):\n        """"""Search for the best model and hyperparameters for the task.\n\n        # Arguments\n            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                Training data x. If the data is from a csv file, it should be a\n                string specifying the path of the csv file of the training data.\n            y: String, numpy.ndarray, or tensorflow.Dataset. Training data y.\n                If the data is from a csv file, it should be a string, which is the\n                name of the target column. Otherwise, It can be raw labels, one-hot\n                encoded if more than two classes, or binary encoded for binary\n                classification.\n            epochs: Int. The number of epochs to train each model during the search.\n                If unspecified, we would use epochs equal to 1000 and early stopping\n                with patience equal to 30.\n            callbacks: List of Keras callbacks to apply during training and\n                validation.\n            validation_split: Float between 0 and 1. Defaults to 0.2.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling. This argument is\n                not supported when `x` is a dataset.\n            validation_data: Data on which to evaluate the loss and any model metrics\n                at the end of each epoch. The model will not be trained on this data.\n                `validation_data` will override `validation_split`. The type of the\n                validation data should be the same as the training data.\n            **kwargs: Any arguments supported by keras.Model.fit.\n        """"""\n        super().fit(x=x,\n                    y=y,\n                    epochs=epochs,\n                    callbacks=callbacks,\n                    validation_split=validation_split,\n                    validation_data=validation_data,\n                    **kwargs)\n\n\nclass StructuredDataRegressor(SupervisedStructuredDataPipeline):\n    """"""AutoKeras structured data regression class.\n\n    # Arguments\n        column_names: A list of strings specifying the names of the columns. The\n            length of the list should be equal to the number of columns of the data\n            excluding the target column. Defaults to None. If None, it will obtained\n            from the header of the csv file or the pandas.DataFrame.\n        column_types: Dict. The keys are the column names. The values should either\n            be \'numerical\' or \'categorical\', indicating the type of that column.\n            Defaults to None. If not None, the column_names need to be specified.\n            If None, it will be inferred from the data.\n        output_dim: Int. The number of output dimensions. Defaults to None.\n            If None, it will be inferred from the data.\n        loss: A Keras loss function. Defaults to use \'mean_squared_error\'.\n        metrics: A list of Keras metrics. Defaults to use \'mean_squared_error\'.\n        project_name: String. The name of the AutoModel. Defaults to\n            \'structured_data_regressor\'.\n        max_trials: Int. The maximum number of different Keras Models to try.\n            The search may finish before reaching the max_trials. Defaults to 100.\n        directory: String. The path to a directory for storing the search outputs.\n            Defaults to None, which would create a folder with the name of the\n            AutoModel in the current directory.\n        objective: String. Name of model metric to minimize\n            or maximize, e.g. \'val_accuracy\'. Defaults to \'val_loss\'.\n        overwrite: Boolean. Defaults to `True`. If `False`, reloads an existing\n            project of the same name if one is found. Otherwise, overwrites the\n            project.\n        seed: Int. Random seed.\n        **kwargs: Any arguments supported by AutoModel.\n    """"""\n\n    def __init__(self,\n                 column_names: Optional[List[str]] = None,\n                 column_types: Optional[Dict[str, str]] = None,\n                 output_dim: Optional[int] = None,\n                 loss: types.LossType = \'mean_squared_error\',\n                 metrics: Optional[types.MetricsType] = None,\n                 project_name: str = \'structured_data_regressor\',\n                 max_trials: int = 100,\n                 directory: Union[str, pathlib.Path, None] = None,\n                 objective: str = \'val_loss\',\n                 overwrite: bool = True,\n                 seed: Optional[int] = None,\n                 **kwargs):\n        super().__init__(\n            outputs=blocks.RegressionHead(output_dim=output_dim,\n                                          loss=loss,\n                                          metrics=metrics),\n            column_names=column_names,\n            column_types=column_types,\n            max_trials=max_trials,\n            directory=directory,\n            project_name=project_name,\n            objective=objective,\n            tuner=greedy.Greedy,\n            overwrite=overwrite,\n            seed=seed,\n            **kwargs)\n'"
autokeras/tasks/structured_data_mixin.py,0,"b'import pandas as pd\n\n\nclass StructuredDataMixin(object):\n\n    def check(self, column_names, column_types):\n        if column_types:\n            for column_type in column_types.values():\n                if column_type not in [\'categorical\', \'numerical\']:\n                    raise ValueError(\n                        \'Column_types should be either ""categorical"" \'\n                        \'or ""numerical"", but got {name}\'.format(name=column_type))\n        if column_names and column_types:\n            for column_name in column_types:\n                if column_name not in column_names:\n                    raise ValueError(\'Column_names and column_types are \'\n                                     \'mismatched. Cannot find column name \'\n                                     \'{name} in the data.\'.format(name=column_name))\n\n    def read_for_predict(self, x):\n        if isinstance(x, str):\n            x = pd.read_csv(x)\n            if self._target_col_name in x:\n                x.pop(self._target_col_name)\n        return x\n'"
autokeras/tasks/text.py,3,"b'import pathlib\nfrom typing import Optional\nfrom typing import Union\n\nfrom autokeras import auto_model\nfrom autokeras import blocks\nfrom autokeras import nodes as input_module\nfrom autokeras.tuners import greedy\nfrom autokeras.tuners import task_specific\nfrom autokeras.utils import types\n\n\nclass SupervisedTextPipeline(auto_model.AutoModel):\n\n    def __init__(self, outputs, **kwargs):\n        super().__init__(inputs=input_module.TextInput(),\n                         outputs=outputs,\n                         **kwargs)\n\n\nclass TextClassifier(SupervisedTextPipeline):\n    """"""AutoKeras text classification class.\n\n    # Arguments\n        num_classes: Int. Defaults to None. If None, it will be inferred from the\n            data.\n        multi_label: Boolean. Defaults to False.\n        loss: A Keras loss function. Defaults to use \'binary_crossentropy\' or\n            \'categorical_crossentropy\' based on the number of classes.\n        metrics: A list of Keras metrics. Defaults to use \'accuracy\'.\n        project_name: String. The name of the AutoModel.\n            Defaults to \'text_classifier\'.\n        max_trials: Int. The maximum number of different Keras Models to try.\n            The search may finish before reaching the max_trials. Defaults to 100.\n        directory: String. The path to a directory for storing the search outputs.\n            Defaults to None, which would create a folder with the name of the\n            AutoModel in the current directory.\n        objective: String. Name of model metric to minimize\n            or maximize, e.g. \'val_accuracy\'. Defaults to \'val_loss\'.\n        overwrite: Boolean. Defaults to `True`. If `False`, reloads an existing\n            project of the same name if one is found. Otherwise, overwrites the\n            project.\n        seed: Int. Random seed.\n        **kwargs: Any arguments supported by AutoModel.\n    """"""\n\n    def __init__(self,\n                 num_classes: Optional[int] = None,\n                 multi_label: bool = False,\n                 loss: types.LossType = None,\n                 metrics: Optional[types.MetricsType] = None,\n                 project_name: str = \'text_classifier\',\n                 max_trials: int = 100,\n                 directory: Union[str, pathlib.Path, None] = None,\n                 objective: str = \'val_loss\',\n                 overwrite: bool = True,\n                 seed: Optional[int] = None,\n                 **kwargs):\n        super().__init__(\n            outputs=blocks.ClassificationHead(num_classes=num_classes,\n                                              multi_label=multi_label,\n                                              loss=loss,\n                                              metrics=metrics),\n            max_trials=max_trials,\n            directory=directory,\n            project_name=project_name,\n            objective=objective,\n            tuner=task_specific.TextClassifierTuner,\n            overwrite=overwrite,\n            seed=seed,\n            **kwargs)\n\n    def fit(self,\n            x=None,\n            y=None,\n            epochs=None,\n            callbacks=None,\n            validation_split=0.2,\n            validation_data=None,\n            **kwargs):\n        """"""Search for the best model and hyperparameters for the AutoModel.\n\n        It will search for the best model based on the performances on\n        validation data.\n\n        # Arguments\n            x: numpy.ndarray or tensorflow.Dataset. Training data x. The input data\n                should be numpy.ndarray or tf.data.Dataset. The data should be one\n                dimensional. Each element in the data should be a string which is a\n                full sentence.\n            y: numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw\n                labels, one-hot encoded if more than two classes, or binary encoded\n                for binary classification.\n            epochs: Int. The number of epochs to train each model during the search.\n                If unspecified, by default we train for a maximum of 1000 epochs,\n                but we stop training if the validation loss stops improving for 10\n                epochs (unless you specified an EarlyStopping callback as part of\n                the callbacks argument, in which case the EarlyStopping callback you\n                specified will determine early stopping).\n            callbacks: List of Keras callbacks to apply during training and\n                validation.\n            validation_split: Float between 0 and 1. Defaults to 0.2.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling. This argument is\n                not supported when `x` is a dataset.\n                The best model found would be fit on the entire dataset including the\n                validation data.\n            validation_data: Data on which to evaluate the loss and any model metrics\n                at the end of each epoch. The model will not be trained on this data.\n                `validation_data` will override `validation_split`. The type of the\n                validation data should be the same as the training data.\n                The best model found would be fit on the training dataset without the\n                validation data.\n            **kwargs: Any arguments supported by keras.Model.fit.\n        """"""\n        super().fit(x=x,\n                    y=y,\n                    epochs=epochs,\n                    callbacks=callbacks,\n                    validation_split=validation_split,\n                    validation_data=validation_data,\n                    **kwargs)\n\n\nclass TextRegressor(SupervisedTextPipeline):\n    """"""AutoKeras text regression class.\n\n    # Arguments\n        output_dim: Int. The number of output dimensions. Defaults to None.\n            If None, it will be inferred from the data.\n        loss: A Keras loss function. Defaults to use \'mean_squared_error\'.\n        metrics: A list of Keras metrics. Defaults to use \'mean_squared_error\'.\n        project_name: String. The name of the AutoModel.\n            Defaults to \'text_regressor\'.\n        max_trials: Int. The maximum number of different Keras Models to try.\n            The search may finish before reaching the max_trials. Defaults to 100.\n        directory: String. The path to a directory for storing the search outputs.\n            Defaults to None, which would create a folder with the name of the\n            AutoModel in the current directory.\n        objective: String. Name of model metric to minimize\n            or maximize, e.g. \'val_accuracy\'. Defaults to \'val_loss\'.\n        overwrite: Boolean. Defaults to `True`. If `False`, reloads an existing\n            project of the same name if one is found. Otherwise, overwrites the\n            project.\n        seed: Int. Random seed.\n        **kwargs: Any arguments supported by AutoModel.\n    """"""\n\n    def __init__(self,\n                 output_dim=None,\n                 loss=\'mean_squared_error\',\n                 metrics=None,\n                 project_name=\'text_regressor\',\n                 max_trials=100,\n                 directory=None,\n                 objective=\'val_loss\',\n                 overwrite=True,\n                 seed=None,\n                 **kwargs):\n        super().__init__(\n            outputs=blocks.RegressionHead(output_dim=output_dim,\n                                          loss=loss,\n                                          metrics=metrics),\n            max_trials=max_trials,\n            directory=directory,\n            project_name=project_name,\n            objective=objective,\n            tuner=greedy.Greedy,\n            overwrite=overwrite,\n            seed=seed,\n            **kwargs)\n\n    def fit(self,\n            x=None,\n            y=None,\n            epochs=None,\n            callbacks=None,\n            validation_split=0.2,\n            validation_data=None,\n            **kwargs):\n        """"""Search for the best model and hyperparameters for the AutoModel.\n\n        It will search for the best model based on the performances on\n        validation data.\n\n        # Arguments\n            x: numpy.ndarray or tensorflow.Dataset. Training data x. The input data\n                should be numpy.ndarray or tf.data.Dataset. The data should be one\n                dimensional. Each element in the data should be a string which is a\n                full sentence.\n            y: numpy.ndarray or tensorflow.Dataset. Training data y. The targets\n                passing to the head would have to be tf.data.Dataset, np.ndarray,\n                pd.DataFrame or pd.Series. It can be single-column or multi-column.\n                The values should all be numerical.\n            epochs: Int. The number of epochs to train each model during the search.\n                If unspecified, by default we train for a maximum of 1000 epochs,\n                but we stop training if the validation loss stops improving for 10\n                epochs (unless you specified an EarlyStopping callback as part of\n                the callbacks argument, in which case the EarlyStopping callback you\n                specified will determine early stopping).\n            callbacks: List of Keras callbacks to apply during training and\n                validation.\n            validation_split: Float between 0 and 1. Defaults to 0.2.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling. This argument is\n                not supported when `x` is a dataset.\n                The best model found would be fit on the entire dataset including the\n                validation data.\n            validation_data: Data on which to evaluate the loss and any model metrics\n                at the end of each epoch. The model will not be trained on this data.\n                `validation_data` will override `validation_split`. The type of the\n                validation data should be the same as the training data.\n                The best model found would be fit on the training dataset without the\n                validation data.\n            **kwargs: Any arguments supported by keras.Model.fit.\n        """"""\n        super().fit(x=x,\n                    y=y,\n                    epochs=epochs,\n                    callbacks=callbacks,\n                    validation_split=validation_split,\n                    validation_data=validation_data,\n                    **kwargs)\n'"
autokeras/tasks/time_series_forecaster.py,0,"b'import pandas as pd\n\nfrom autokeras import auto_model\nfrom autokeras import blocks\nfrom autokeras import nodes as input_module\nfrom autokeras.tasks.structured_data_mixin import StructuredDataMixin\nfrom autokeras.tuners import greedy\n\n\nclass SupervisedTimeseriesDataPipeline(StructuredDataMixin, auto_model.AutoModel):\n\n    def __init__(self,\n                 outputs,\n                 column_names=None,\n                 column_types=None,\n                 lookback=None,\n                 predict_from=1,\n                 predict_until=None,\n                 **kwargs):\n        inputs = input_module.TimeseriesInput(lookback=lookback,\n                                              column_names=column_names,\n                                              column_types=column_types)\n        self.check(column_names, column_types)\n        super().__init__(inputs=inputs,\n                         outputs=outputs,\n                         **kwargs)\n        self.predict_from = predict_from\n        self.predict_until = predict_until\n        self._target_col_name = None\n        self.train_len = 0\n\n    @staticmethod\n    def _read_from_csv(x, y):\n        df = pd.read_csv(x)\n        target = df.pop(y).dropna().to_numpy()\n        return df, target\n\n    def fit(self,\n            x=None,\n            y=None,\n            epochs=None,\n            callbacks=None,\n            validation_split=0.2,\n            validation_data=None,\n            **kwargs):\n        # x is file path of training data\n        if isinstance(x, str):\n            self._target_col_name = y\n            x, y = self._read_from_csv(x, y)\n        if validation_data:\n            x_val, y_val = validation_data\n            if isinstance(x_val, str):\n                validation_data = self._read_from_csv(x_val, y_val)\n\n        self.train_len = len(y)\n\n        super().fit(x=x[:self.train_len],\n                    y=y[self.lookback-1:],\n                    epochs=epochs,\n                    callbacks=callbacks,\n                    validation_split=validation_split,\n                    validation_data=validation_data,\n                    **kwargs)\n\n    def predict(self, x, batch_size=32, **kwargs):\n        x = self.read_for_predict(x)\n        y_pred = super().predict(x=x,\n                                 batch_size=batch_size,\n                                 **kwargs)\n        lower_bound = self.train_len + self.predict_from\n        if self.predict_until is None:\n            self.predict_until = len(y_pred)\n        upper_bound = min(self.train_len + self.predict_until + 1, len(y_pred))\n        return y_pred[lower_bound:upper_bound]\n\n    def evaluate(self, x, y=None, batch_size=32, **kwargs):\n        """"""Evaluate the best model for the given data.\n\n        # Arguments\n            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                Testing data x. If the data is from a csv file, it should be a\n                string specifying the path of the csv file of the testing data.\n            y: String, numpy.ndarray, or tensorflow.Dataset. Testing data y.\n                If the data is from a csv file, it should be a string corresponding\n                to the label column.\n            batch_size: Int. Defaults to 32.\n            **kwargs: Any arguments supported by keras.Model.evaluate.\n\n        # Returns\n            Scalar test loss (if the model has a single output and no metrics) or\n            list of scalars (if the model has multiple outputs and/or metrics).\n            The attribute model.metrics_names will give you the display labels for\n            the scalar outputs.\n        """"""\n        if isinstance(x, str):\n            x, y = self._read_from_csv(x, y)\n        return super().evaluate(x=x[:len(y)],\n                                y=y[self.lookback-1:],\n                                batch_size=batch_size,\n                                **kwargs)\n\n\nclass TimeseriesForecaster(SupervisedTimeseriesDataPipeline):\n    """"""AutoKeras time series data forecast class.\n\n    # Arguments\n        column_names: A list of strings specifying the names of the columns. The\n            length of the list should be equal to the number of columns of the data.\n            Defaults to None. If None, it will be obtained from the header of the csv\n            file or the pandas.DataFrame.\n        column_types: Dict. The keys are the column names. The values should either\n            be \'numerical\' or \'categorical\', indicating the type of that column.\n            Defaults to None. If not None, the column_names need to be specified.\n            If None, it will be inferred from the data.\n        lookback: Int. The range of history steps to consider for each prediction.\n            For example, if lookback=n, the data in the range of [i - n, i - 1]\n            is used to predict the value of step i. If unspecified, it will be tuned\n            automatically.\n        predict_from: Int. The starting point of the forecast for each sample (in\n            number of steps) after the last time step in the input. If N is the last\n            step in the input, then the first step of the predicted output will be\n            N + predict_from. Defaults to 1 (which corresponds to starting the\n            forecast immediately after the last step in the input).\n        predict_until: Int. The end point of the forecast for each sample (in number\n            of steps) after the last time step in the input. If N is the last step in\n            the input, then the last step of the predicted output will be\n            N + predict_until. If unspecified, it will predict till end of dataset.\n            Defaults to None.\n        loss: A Keras loss function. Defaults to use \'mean_squared_error\'.\n        metrics: A list of Keras metrics. Defaults to use \'mean_squared_error\'.\n        project_name: String. The name of the AutoModel. Defaults to\n            \'time_series_forecaster\'.\n        max_trials: Int. The maximum number of different Keras Models to try.\n            The search may finish before reaching the max_trials. Defaults to 100.\n        directory: String. The path to a directory for storing the search outputs.\n            Defaults to None, which would create a folder with the name of the\n            AutoModel in the current directory.\n        objective: String. Name of model metric to minimize\n            or maximize, e.g. \'val_accuracy\'. Defaults to \'val_loss\'.\n        overwrite: Boolean. Defaults to `True`. If `False`, reloads an existing\n            project of the same name if one is found. Otherwise, overwrites the\n            project.\n        seed: Int. Random seed.\n        **kwargs: Any arguments supported by AutoModel.\n    """"""\n\n    def __init__(self,\n                 output_dim=None,\n                 column_names=None,\n                 column_types=None,\n                 lookback=None,\n                 predict_from=1,\n                 predict_until=None,\n                 loss=\'mean_squared_error\',\n                 metrics=None,\n                 project_name=\'time_series_forecaster\',\n                 max_trials=100,\n                 directory=None,\n                 objective=\'val_loss\',\n                 overwrite=True,\n                 seed=None,\n                 **kwargs):\n        super().__init__(outputs=blocks.RegressionHead(output_dim=output_dim,\n                                                       loss=loss,\n                                                       metrics=metrics),\n                         column_names=column_names,\n                         column_types=column_types,\n                         lookback=lookback,\n                         predict_from=predict_from,\n                         predict_until=predict_until,\n                         project_name=project_name,\n                         max_trials=max_trials,\n                         directory=directory,\n                         objective=objective,\n                         tuner=greedy.Greedy,\n                         overwrite=overwrite,\n                         seed=seed,\n                         **kwargs)\n        self.lookback = lookback\n        self.predict_from = predict_from\n        self.predict_until = predict_until\n\n    def fit(self,\n            x=None,\n            y=None,\n            validation_split=0.2,\n            validation_data=None,\n            **kwargs):\n        """"""Search for the best model and hyperparameters for the task.\n\n        # Arguments\n            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                Training data x. If the data is from a csv file, it should be a\n                string specifying the path of the csv file of the training data.\n            y: String, a list of string(s), numpy.ndarray, pandas.DataFrame or\n                tensorflow.Dataset. Training data y.\n                If the data is from a csv file, it should be a list of string(s)\n                specifying the name(s) of the column(s) need to be forecasted.\n                If it is multivariate forecasting, y should be a list of more than\n                one column names. If it is univariate forecasting, y should be a\n                string or a list of one string.\n            validation_split: Float between 0 and 1. Defaults to 0.2.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling. This argument is\n                not supported when `x` is a dataset.\n                The best model found would be fit on the entire dataset including the\n                validation data.\n            validation_data: Data on which to evaluate the loss and any model metrics\n                at the end of each epoch. The model will not be trained on this data.\n                `validation_data` will override `validation_split`. The type of the\n                validation data should be the same as the training data.\n                The best model found would be fit on the training dataset without the\n                validation data.\n            **kwargs: Any arguments supported by keras.Model.fit.\n        """"""\n        super().fit(x=x,\n                    y=y,\n                    validation_split=validation_split,\n                    validation_data=validation_data,\n                    **kwargs)\n\n    def predict(self, x=None, batch_size=32, **kwargs):\n        """"""Predict the output for a given testing data.\n\n        # Arguments\n            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                Testing data x. If the data is from a csv file, it should be a\n                string specifying the path of the csv file of the testing data.\n            batch_size: Int. Defaults to 32.\n            **kwargs: Any arguments supported by keras.Model.predict.\n\n        # Returns\n            A list of numpy.ndarray objects or a single numpy.ndarray.\n            The predicted results.\n        """"""\n        return super().predict(x=x, batch_size=batch_size, **kwargs)\n\n    def fit_and_predict(self,\n                        x=None,\n                        y=None,\n                        validation_split=0.2,\n                        validation_data=None,\n                        batch_size=32,\n                        **kwargs):\n        """"""Search for the best model and then predict for remaining data points.\n\n        # Arguments\n            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                Training data x. If the data is from a csv file, it should be a\n                string specifying the path of the csv file of the training data.\n            y: String, a list of string(s), numpy.ndarray, pandas.DataFrame or\n                tensorflow.Dataset. Training data y.\n                If the data is from a csv file, it should be a list of string(s)\n                specifying the name(s) of the column(s) need to be forecasted.\n                If it is multivariate forecasting, y should be a list of more than\n                one column names. If it is univariate forecasting, y should be a\n                string or a list of one string.\n            validation_split: Float between 0 and 1. Defaults to 0.2.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling. This argument is\n                not supported when `x` is a dataset.\n                The best model found would be fit on the entire dataset including the\n                validation data.\n            validation_data: Data on which to evaluate the loss and any model metrics\n                at the end of each epoch. The model will not be trained on this data.\n                `validation_data` will override `validation_split`. The type of the\n                validation data should be the same as the training data.\n                The best model found would be fit on the training dataset without the\n                validation data.\n            batch_size: Int. Defaults to 32.\n            **kwargs: Any arguments supported by keras.Model.fit.\n        """"""\n        self.fit(x=x,\n                 y=y,\n                 validation_split=validation_split,\n                 validation_data=validation_data,\n                 **kwargs)\n\n        return self.predict(x=x, batch_size=batch_size)\n\n\nclass TimeseriesClassifier(SupervisedTimeseriesDataPipeline):\n    """"""""AutoKeras time series data classification class.\n\n    # Arguments\n        column_names: A list of strings specifying the names of the columns. The\n            length of the list should be equal to the number of columns of the data.\n            Defaults to None. If None, it will be obtained from the header of the csv\n            file or the pandas.DataFrame.\n        column_types: Dict. The keys are the column names. The values should either\n            be \'numerical\' or \'categorical\', indicating the type of that column.\n            Defaults to None. If not None, the column_names need to be specified.\n            If None, it will be inferred from the data.\n        lookback: Int. The range of history steps to consider for each prediction.\n            For example, if lookback=n, the data in the range of [i - n, i - 1]\n            is used to predict the value of step i. If unspecified, it will be tuned\n            automatically.\n        predict_from: Int. The starting point of the forecast for each sample (in\n            number of steps) after the last time step in the input. If N is the last\n            step in the input, then the first step of the predicted output will be\n            N + predict_from. Defaults to 1 (which corresponds to starting the\n            forecast immediately after the last step in the input).\n        predict_until: Int. The end point of the forecast for each sample (in number\n            of steps) after the last time step in the input. If N is the last step in\n            the input, then the last step of the predicted output will be\n            N + predict_until. If unspecified, it will predict till end of dataset.\n            Defaults to None.\n        loss: A Keras loss function. Defaults to use \'mean_squared_error\'.\n        metrics: A list of Keras metrics. Defaults to use \'mean_squared_error\'.\n        project_name: String. The name of the AutoModel. Defaults to\n            \'time_series_forecaster\'.\n        max_trials: Int. The maximum number of different Keras Models to try.\n            The search may finish before reaching the max_trials. Defaults to 100.\n        directory: String. The path to a directory for storing the search outputs.\n            Defaults to None, which would create a folder with the name of the\n            AutoModel in the current directory.\n        objective: String. Name of model metric to minimize\n            or maximize, e.g. \'val_accuracy\'. Defaults to \'val_loss\'.\n        overwrite: Boolean. Defaults to `True`. If `False`, reloads an existing\n            project of the same name if one is found. Otherwise, overwrites the\n            project.\n        seed: Int. Random seed.\n        **kwargs: Any arguments supported by AutoModel.\n    """"""\n\n    def __init__(self,\n                 output_dim=None,\n                 column_names=None,\n                 column_types=None,\n                 lookback=None,\n                 predict_from=1,\n                 predict_until=None,\n                 loss=\'mean_squared_error\',\n                 metrics=None,\n                 project_name=\'time_series_classifier\',\n                 max_trials=100,\n                 directory=None,\n                 objective=\'val_loss\',\n                 overwrite=True,\n                 seed=None,\n                 **kwargs):\n        raise NotImplementedError\n\n    def fit(self,\n            x=None,\n            y=None,\n            validation_split=0.2,\n            validation_data=None,\n            **kwargs):\n        """"""Search for the best model and hyperparameters for the task.\n\n        # Arguments\n            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                Training data x. If the data is from a csv file, it should be a\n                string specifying the path of the csv file of the training data.\n            y: String, a list of string(s), numpy.ndarray, pandas.DataFrame or\n                tensorflow.Dataset. Training data y.\n                If the data is from a csv file, it should be a list of string(s)\n                specifying the name(s) of the column(s) need to be forecasted.\n                If it is multivariate forecasting, y should be a list of more than\n                one column names. If it is univariate forecasting, y should be a\n                string or a list of one string.\n            validation_split: Float between 0 and 1. Defaults to 0.2.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling. This argument is\n                not supported when `x` is a dataset.\n                The best model found would be fit on the entire dataset including the\n                validation data.\n            validation_data: Data on which to evaluate the loss and any model metrics\n                at the end of each epoch. The model will not be trained on this data.\n                `validation_data` will override `validation_split`. The type of the\n                validation data should be the same as the training data.\n                The best model found would be fit on the training dataset without the\n                validation data.\n            **kwargs: Any arguments supported by keras.Model.fit.\n        """"""\n        raise NotImplementedError\n\n    def predict(self, x=None, batch_size=32, **kwargs):\n        """"""Predict the output for a given testing data.\n\n        # Arguments\n            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                Testing data x, it should also contain the training data used as,\n                subsequent predictions depend on them. If the data is from a csv\n                file, it should be a string specifying the path of the csv file\n                of the testing data.\n            batch_size: Int. Defaults to 32.\n            **kwargs: Any arguments supported by keras.Model.predict.\n\n        # Returns\n            A list of numpy.ndarray objects or a single numpy.ndarray.\n            The predicted results.\n        """"""\n        raise NotImplementedError\n\n    def fit_and_predict(self,\n                        x=None,\n                        y=None,\n                        validation_split=0.2,\n                        validation_data=None,\n                        batch_size=32,\n                        **kwargs):\n        """"""Search for the best model and then predict for remaining data points.\n\n        # Arguments\n            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.\n                Training and Test data x. If the data is from a csv file, it\n                should be a string specifying the path of the csv file of the\n                training data.\n            y: String, a list of string(s), numpy.ndarray, pandas.DataFrame or\n                tensorflow.Dataset. Training data y.\n                If the data is from a csv file, it should be a list of string(s)\n                specifying the name(s) of the column(s) need to be forecasted.\n                If it is multivariate forecasting, y should be a list of more than\n                one column names. If it is univariate forecasting, y should be a\n                string or a list of one string.\n            validation_split: Float between 0 and 1. Defaults to 0.2.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling. This argument is\n                not supported when `x` is a dataset.\n                The best model found would be fit on the entire dataset including the\n                validation data.\n            validation_data: Data on which to evaluate the loss and any model metrics\n                at the end of each epoch. The model will not be trained on this data.\n                `validation_data` will override `validation_split`. The type of the\n                validation data should be the same as the training data.\n                The best model found would be fit on the training dataset without the\n                validation data.\n            batch_size: Int. Defaults to 32.\n            **kwargs: Any arguments supported by keras.Model.fit.\n        """"""\n        raise NotImplementedError\n'"
autokeras/tuners/__init__.py,0,b'from autokeras.tuners.bayesian_optimization import BayesianOptimization\nfrom autokeras.tuners.greedy import Greedy\nfrom autokeras.tuners.hyperband import Hyperband\nfrom autokeras.tuners.random_search import RandomSearch\nfrom autokeras.tuners.task_specific import ImageClassifierTuner\n'
autokeras/tuners/bayesian_optimization.py,0,"b'import kerastuner\n\nfrom autokeras.engine import tuner as tuner_module\n\n\nclass BayesianOptimization(kerastuner.BayesianOptimization, tuner_module.AutoTuner):\n    """"""KerasTuner BayesianOptimization with preprocessing layer tuning.""""""\n    pass\n'"
autokeras/tuners/greedy.py,0,"b'import copy\nimport random\n\nimport kerastuner\nimport numpy as np\n\nfrom autokeras import blocks\nfrom autokeras.engine import tuner as tuner_module\n\n\nclass GreedyOracle(kerastuner.Oracle):\n    """"""An oracle combining random search and greedy algorithm.\n\n    It groups the HyperParameters into several categories, namely, HyperGraph,\n    Preprocessor, Architecture, and Optimization. The oracle tunes each group\n    separately using random search. In each trial, it use a greedy strategy to\n    generate new values for one of the categories of HyperParameters and use the best\n    trial so far for the rest of the HyperParameters values.\n\n    # Arguments\n        initial_hps: A list of dictionaries in the form of\n            {HyperParameter name (String): HyperParameter value}.\n            Each dictionary is one set of HyperParameters, which are used as the\n            initial trials for the search. Defaults to None.\n        seed: Int. Random seed.\n    """"""\n\n    HYPER = \'HYPER\'\n    PREPROCESS = \'PREPROCESS\'\n    OPT = \'OPT\'\n    ARCH = \'ARCH\'\n    STAGES = [HYPER, PREPROCESS, OPT, ARCH]\n\n    @staticmethod\n    def next_stage(stage):\n        stages = GreedyOracle.STAGES\n        return stages[(stages.index(stage) + 1) % len(stages)]\n\n    def __init__(self,\n                 hypermodel,\n                 initial_hps=None,\n                 seed=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.initial_hps = initial_hps or []\n        self._tried_initial_hps = [False] * len(self.initial_hps)\n        self.hypermodel = hypermodel\n        # Sets of HyperParameter names.\n        self._hp_names = {\n            GreedyOracle.HYPER: set(),\n            GreedyOracle.PREPROCESS: set(),\n            GreedyOracle.OPT: set(),\n            GreedyOracle.ARCH: set(),\n        }\n        # The quota used to tune each category of hps.\n        self.seed = seed or random.randint(1, 1e4)\n        # Incremented at every call to `populate_space`.\n        self._seed_state = self.seed\n        self._tried_so_far = set()\n        self._max_collisions = 5\n\n    def update_space(self, hyperparameters):\n        # Get the block names.\n        self.hypermodel.build(hyperparameters)\n\n        # Add the new Hyperparameters to different categories.\n        ref_names = {hp.name for hp in self.hyperparameters.space}\n        for hp in hyperparameters.space:\n            if hp.name not in ref_names:\n                hp_type = None\n                if any([hp.name.startswith(block.name)\n                        for block in self.hypermodel.blocks if isinstance(block, (\n                            blocks.ImageBlock,\n                            blocks.TextBlock,\n                            blocks.StructuredDataBlock,\n                        ))]):\n                    hp_type = GreedyOracle.HYPER\n                elif any([hp.name.startswith(block.name)\n                          for block in self.hypermodel.blocks if isinstance(block, (\n                              blocks.TextToIntSequence,\n                              blocks.TextToNgramVector,\n                              blocks.Normalization,\n                              blocks.ImageAugmentation,\n                              blocks.CategoricalToNumerical\n                          ))]):\n                    hp_type = GreedyOracle.PREPROCESS\n                elif any([hp.name.startswith(block.name)\n                          for block in self.hypermodel.blocks if isinstance(block, (\n                              blocks.Embedding,\n                              blocks.ConvBlock,\n                              blocks.RNNBlock,\n                              blocks.DenseBlock,\n                              blocks.ResNetBlock,\n                              blocks.XceptionBlock,\n                              blocks.Merge,\n                              blocks.Flatten,\n                              blocks.SpatialReduction,\n                              blocks.TemporalReduction,\n                              blocks.ClassificationHead,\n                              blocks.RegressionHead,\n                          ))]):\n                    hp_type = GreedyOracle.ARCH\n                else:\n                    hp_type = GreedyOracle.OPT\n                self._hp_names[hp_type].add(hp.name)\n\n        super().update_space(hyperparameters)\n\n    def _generate_stage(self):\n        probabilities = np.array([pow(len(value), 2)\n                                  for value in self._hp_names.values()])\n        sum_p = np.sum(probabilities)\n        if sum_p == 0:\n            probabilities = np.array([1] * len(probabilities))\n            sum_p = np.sum(probabilities)\n        probabilities = probabilities / sum_p\n        return np.random.choice(list(self._hp_names.keys()), p=probabilities)\n\n    def _next_initial_hps(self):\n        for index, hps in enumerate(self.initial_hps):\n            if not self._tried_initial_hps[index]:\n                self._tried_initial_hps[index] = True\n                return hps\n\n    def _populate_space(self, trial_id):\n        if not all(self._tried_initial_hps):\n            values = self._next_initial_hps()\n            hp = self.get_space()\n            # while not all initial_hps are registered in hp.\n            while len(set(values.keys()) - set(hp.values.keys())) != 0:\n                hp.values = copy.copy(values)\n                self.hypermodel.build(hp)\n                self.update_space(hp)\n                hp = self.get_space()\n            return {\'status\': kerastuner.engine.trial.TrialStatus.RUNNING,\n                    \'values\': values}\n\n        stage = self._generate_stage()\n        for _ in range(len(GreedyOracle.STAGES)):\n            values = self._generate_stage_values(stage)\n            # Reached max collisions.\n            if values is None:\n                # Try next stage.\n                stage = GreedyOracle.next_stage(stage)\n                continue\n            # Values found.\n            return {\'status\': kerastuner.engine.trial.TrialStatus.RUNNING,\n                    \'values\': values}\n        # All stages reached max collisions.\n        return {\'status\': kerastuner.engine.trial.TrialStatus.STOPPED,\n                \'values\': None}\n\n    def _generate_stage_values(self, stage):\n        best_trials = self.get_best_trials()\n        if best_trials:\n            best_values = best_trials[0].hyperparameters.values\n        else:\n            best_values = self.hyperparameters.values\n        collisions = 0\n        while True:\n            # Generate new values for the current stage.\n            values = {}\n            for p in self.hyperparameters.space:\n                if p.name in self._hp_names[stage]:\n                    values[p.name] = p.random_sample(self._seed_state)\n                    self._seed_state += 1\n            values = {**best_values, **values}\n            # Keep trying until the set of values is unique,\n            # or until we exit due to too many collisions.\n            values_hash = self._compute_values_hash(values)\n            if values_hash not in self._tried_so_far:\n                self._tried_so_far.add(values_hash)\n                break\n            collisions += 1\n            if collisions > self._max_collisions:\n                # Reached max collisions. No value to return.\n                return None\n        return values\n\n\nclass Greedy(tuner_module.AutoTuner):\n\n    def __init__(self,\n                 hypermodel,\n                 objective,\n                 max_trials,\n                 initial_hps=None,\n                 seed=None,\n                 hyperparameters=None,\n                 tune_new_entries=True,\n                 allow_new_entries=True,\n                 **kwargs):\n        self.seed = seed\n        oracle = GreedyOracle(\n            hypermodel=hypermodel,\n            objective=objective,\n            max_trials=max_trials,\n            initial_hps=initial_hps,\n            seed=seed,\n            hyperparameters=hyperparameters,\n            tune_new_entries=tune_new_entries,\n            allow_new_entries=allow_new_entries)\n        super().__init__(\n            hypermodel=hypermodel,\n            oracle=oracle,\n            **kwargs)\n'"
autokeras/tuners/hyperband.py,0,"b'import kerastuner\n\nfrom autokeras.engine import tuner as tuner_module\n\n\nclass Hyperband(kerastuner.Hyperband, tuner_module.AutoTuner):\n    """"""KerasTuner Hyperband with preprocessing layer tuning.""""""\n    pass\n'"
autokeras/tuners/random_search.py,0,"b'import kerastuner\n\nfrom autokeras.engine import tuner as tuner_module\n\n\nclass RandomSearch(kerastuner.RandomSearch, tuner_module.AutoTuner):\n    """"""KerasTuner RandomSearch with preprocessing layer tuning.""""""\n    pass\n'"
autokeras/tuners/task_specific.py,0,"b""from autokeras.tuners import greedy\n\nIMAGE_CLASSIFIER = [{\n    'image_block_1/block_type': 'vanilla',\n    'image_block_1/normalize': True,\n    'image_block_1/augment': False,\n    'image_block_1/conv_block_1/kernel_size': 3,\n    'image_block_1/conv_block_1/num_blocks': 1,\n    'image_block_1/conv_block_1/num_layers': 2,\n    'image_block_1/conv_block_1/max_pooling': True,\n    'image_block_1/conv_block_1/separable': False,\n    'image_block_1/conv_block_1/dropout_rate': 0.25,\n    'image_block_1/conv_block_1/filters_0_0': 32,\n    'image_block_1/conv_block_1/filters_0_1': 64,\n    'classification_head_1/spatial_reduction_1/reduction_type': 'flatten',\n    'classification_head_1/dropout_rate': 0.5,\n    'optimizer': 'adam'\n}, {\n    'image_block_1/block_type': 'resnet',\n    'image_block_1/normalize': True,\n    'image_block_1/augment': True,\n    'image_block_1/image_augmentation_1/horizontal_flip': True,\n    'image_block_1/image_augmentation_1/vertical_flip': True,\n    'image_block_1/res_net_block_1/version': 'v2',\n    'image_block_1/res_net_block_1/pooling': 'avg',\n    'image_block_1/res_net_block_1/conv3_depth': 4,\n    'image_block_1/res_net_block_1/conv4_depth': 6,\n    'classification_head_1/dropout_rate': 0,\n    'optimizer': 'adam'\n}]\n\nTEXT_CLASSIFIER = [{\n    'text_block_1/vectorizer': 'sequence',\n    'classification_head_1/dropout_rate': 0,\n    'optimizer': 'adam',\n    'text_block_1/max_tokens': 5000,\n    'text_block_1/conv_block_1/separable': False,\n    'text_block_1/text_to_int_sequence_1/output_sequence_length': 512,\n    'text_block_1/embedding_1/pretraining': 'none',\n    'text_block_1/embedding_1/embedding_dim': 64,\n    'text_block_1/embedding_1/dropout_rate': 0.25,\n    'text_block_1/conv_block_1/kernel_size': 5,\n    'text_block_1/conv_block_1/num_blocks': 1,\n    'text_block_1/conv_block_1/num_layers': 1,\n    'text_block_1/conv_block_1/max_pooling': False,\n    'text_block_1/conv_block_1/dropout_rate': 0,\n    'text_block_1/conv_block_1/filters_0_0': 256,\n    'text_block_1/spatial_reduction_1/reduction_type': 'global_max',\n    'text_block_1/dense_block_1/num_layers': 1,\n    'text_block_1/dense_block_1/use_batchnorm': False,\n    'text_block_1/dense_block_1/dropout_rate': 0.5,\n    'text_block_1/dense_block_1/units_0': 256,\n}]\n\n\nclass ImageClassifierTuner(greedy.Greedy):\n    def __init__(self, **kwargs):\n        super().__init__(\n            initial_hps=IMAGE_CLASSIFIER,\n            **kwargs)\n\n\nclass TextClassifierTuner(greedy.Greedy):\n    def __init__(self, **kwargs):\n        super().__init__(\n            initial_hps=TEXT_CLASSIFIER,\n            **kwargs)\n"""
autokeras/utils/__init__.py,0,b''
autokeras/utils/data_utils.py,3,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\n\ndef batched(dataset):\n    shape = nest.flatten(dataset_shape(dataset))[0]\n    return len(shape) > 0 and shape[0] is None\n\n\ndef batch_dataset(dataset, batch_size):\n    if batched(dataset):\n        return dataset\n    return dataset.batch(batch_size)\n\n\ndef split_dataset(dataset, validation_split):\n    """"""Split dataset into training and validation.\n\n    # Arguments\n        dataset: tf.data.Dataset. The entire dataset to be split.\n        validation_split: Float. The split ratio for the validation set.\n\n    # Raises\n        ValueError: If the dataset provided is too small to be split.\n\n    # Returns\n        A tuple of two tf.data.Dataset. The training set and the validation set.\n    """"""\n    num_instances = dataset.reduce(np.int64(0), lambda x, _: x + 1).numpy()\n    if num_instances < 2:\n        raise ValueError(\'The dataset should at least contain 2 \'\n                         \'batches to be split.\')\n    validation_set_size = min(\n        max(int(num_instances * validation_split), 1),\n        num_instances - 1)\n    train_set_size = num_instances - validation_set_size\n    train_dataset = dataset.take(train_set_size)\n    validation_dataset = dataset.skip(train_set_size)\n    return train_dataset, validation_dataset\n\n\ndef dataset_shape(dataset):\n    return tf.compat.v1.data.get_output_shapes(dataset)\n\n\ndef is_label(y):\n    """"""Check if the targets are one-hot encoded or plain labels.\n\n    # Arguments\n        y: numpy.ndarray. The targets.\n\n    # Returns\n        Boolean. Whether the targets are plain label, not encoded.\n    """"""\n    return len(y.flatten()) == len(y)\n'"
autokeras/utils/layer_utils.py,18,"b'import tensorflow as tf\n\n\ndef get_global_average_pooling(shape):\n    return [tf.keras.layers.GlobalAveragePooling1D,\n            tf.keras.layers.GlobalAveragePooling2D,\n            tf.keras.layers.GlobalAveragePooling3D][len(shape) - 3]\n\n\ndef get_global_max_pooling(shape):\n    return [tf.keras.layers.GlobalMaxPool1D,\n            tf.keras.layers.GlobalMaxPool2D,\n            tf.keras.layers.GlobalMaxPool3D][len(shape) - 3]\n\n\ndef get_max_pooling(shape):\n    return [tf.keras.layers.MaxPool1D,\n            tf.keras.layers.MaxPool2D,\n            tf.keras.layers.MaxPool3D][len(shape) - 3]\n\n\ndef get_conv(shape):\n    return [tf.keras.layers.Conv1D,\n            tf.keras.layers.Conv2D,\n            tf.keras.layers.Conv3D][len(shape) - 3]\n\n\ndef get_sep_conv(shape):\n    return [tf.keras.layers.SeparableConv1D,\n            tf.keras.layers.SeparableConv2D,\n            tf.keras.layers.Conv3D][len(shape) - 3]\n\n\ndef get_dropout(shape):\n    return [tf.keras.layers.SpatialDropout1D,\n            tf.keras.layers.SpatialDropout2D,\n            tf.keras.layers.SpatialDropout3D][len(shape) - 3]\n'"
autokeras/utils/types.py,1,"b'from typing import Callable\nfrom typing import Dict\nfrom typing import List\nfrom typing import Union\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.losses import Loss\nfrom tensorflow.keras.metrics import Metric\n\nDatasetType = Union[np.ndarray, tf.data.Dataset]\nLossType = Union[str, Callable, Loss]\nAcceptableMetric = Union[str, Callable, Metric]\nMetricsType = Union[List[AcceptableMetric],\n                    List[List[AcceptableMetric]],\n                    Dict[str, AcceptableMetric]]\n'"
autokeras/utils/utils.py,5,"b'import json\nimport re\n\nimport tensorflow as tf\nfrom packaging.version import parse\nfrom tensorflow.python.util import nest\n\n\ndef validate_num_inputs(inputs, num):\n    inputs = nest.flatten(inputs)\n    if not len(inputs) == num:\n        raise ValueError(\'Expected {num} elements in the inputs list \'\n                         \'but received {len} inputs.\'.format(num=num,\n                                                             len=len(inputs)))\n\n\ndef get_name_scope():\n    with tf.name_scope(\'a\') as scope:\n        name_scope = scope[:-2]\n    return name_scope\n\n\ndef to_snake_case(name):\n    intermediate = re.sub(\'(.)([A-Z][a-z0-9]+)\', r\'\\1_\\2\', name)\n    insecure = re.sub(\'([a-z])([A-Z])\', r\'\\1_\\2\', intermediate).lower()\n    # If the class is private the name starts with ""_"" which is not secure\n    # for creating scopes. We prefix the name with ""private"" in this case.\n    if insecure[0] != \'_\':\n        return insecure\n    return \'private\' + insecure\n\n\ndef to_type_key(dictionary, convert_func):\n    """"""Convert the keys of a dictionary to a different type.\n\n    # Arguments\n        dictionary: Dictionary. The dictionary to be converted.\n        convert_func: Function. The function to convert a key.\n    """"""\n    return {convert_func(key): value\n            for key, value in dictionary.items()}\n\n\ndef check_tf_version() -> None:\n    if parse(tf.__version__) < parse(\'2.1.0\'):\n        raise ImportError(\n            \'The Tensorflow package version needs to be at least v2.1.0 \\n\'\n            \'for AutoKeras to run. Currently, your TensorFlow version is \\n\'\n            \'v{version}. Please upgrade with \\n\'\n            \'`$ pip install --upgrade tensorflow` -> GPU version \\n\'\n            \'or \\n\'\n            \'`$ pip install --upgrade tensorflow-cpu` -> CPU version. \\n\'\n            \'You can use `pip freeze` to check afterwards that everything is \'\n            \'ok.\'.format(version=tf.__version__)\n        )\n\n\ndef save_json(path, obj):\n    obj = json.dumps(obj)\n    with tf.io.gfile.GFile(path, \'w\') as f:\n        f.write(obj)\n\n\ndef load_json(path):\n    with tf.io.gfile.GFile(path, \'r\') as f:\n        obj = f.read()\n    return json.loads(obj)\n\n\ndef contain_instance(instance_list, instance_type):\n    return any([isinstance(instance, instance_type)\n                for instance in instance_list])\n'"
docs/py/customized.py,2,"b'""""""shell\npip install autokeras\n""""""\n\n""""""\nIn this tutorial, we show how to customize your search space with\n[AutoModel](/auto_model/#automodel-class) and how to implement your own block as search space.\nThis API is mainly for advanced users who already know what their model should look like.\n\n## Customized Search Space\nFirst, let us see how we can build the following neural network using the building blocks in AutoKeras.\n\n<div class=""mermaid"">\ngraph LR\n    id1(ImageInput) --> id2(Normalization)\n    id2 --> id3(Image Augmentation)\n    id3 --> id4(Convolutional)\n    id3 --> id5(ResNet V2)\n    id4 --> id6(Merge)\n    id5 --> id6\n    id6 --> id7(Classification Head)\n</div>\n\nWe can make use of the [AutoModel](/auto_model/#automodel-class) API in AutoKeras to implemented as follows.\nThe usage is the same as the [Keras functional API](https://www.tensorflow.org/guide/keras/functional).\nSince this is just a demo, we use small amount of `max_trials` and `epochs`.\n""""""\n\nimport autokeras as ak\n\ninput_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node1 = ak.ConvBlock()(output_node)\noutput_node2 = ak.ResNetBlock(version=\'v2\')(output_node)\noutput_node = ak.Merge()([output_node1, output_node2])\noutput_node = ak.ClassificationHead()(output_node)\n\nauto_model = ak.AutoModel(\n    inputs=input_node, \n    outputs=output_node,\n    max_trials=1)\n\n""""""\nWhild building the model, the blocks used need to follow this topology:\n`Preprocessor` -> `Block` -> `Head`. `Normalization` and `ImageAugmentation` are `Preprocessor`s.\n`ClassificationHead` is `Head`. The rest are `Block`s.\n\nIn the code above, we use `ak.ResNetBlock(version=\'v2\')` to specify the version of ResNet to use.\nThere are many other arguments to specify for each building block.\nFor most of the arguments, if not specified, they would be tuned automatically.\nPlease refer to the documentation links at the bottom of the page for more details.\n\nThen, we prepare some data to run the model.\n""""""\n\nfrom tensorflow.keras.datasets import mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint(x_train.shape) # (60000, 28, 28)\nprint(y_train.shape) # (60000,)\nprint(y_train[:3]) # array([7, 2, 1], dtype=uint8)\n\n# Feed the AutoModel with training data.\nauto_model.fit(x_train[:100], y_train[:100], epochs=1)\n# Predict with the best model.\npredicted_y = auto_model.predict(x_test)\n# Evaluate the best model with testing data.\nprint(auto_model.evaluate(x_test, y_test))\n\n""""""\nFor multiple input nodes and multiple heads search space, you can refer to [this section](/tutorial/multi/#customized-search-space).\n\n## Validation Data\nIf you would like to provide your own validation data or change the ratio of the validation data, please refer to\nthe Validation Data section of the tutorials of\n[Image Classification](/tutorial/image_classification/#validation-data),\n[Text Classification](/tutorial/text_classification/#validation-data),\n[Structured Data Classification](/tutorial/structured_data_classification/#validation-data),\n[Multi-task and Multiple Validation](/tutorial/multi/#validation-data).\n\n## Data Format\nYou can refer to the documentation of\n[ImageInput](/node/#imageinput-class),\n[StructuredDataInput](/node/#structureddatainput-class),\n[TextInput](/node/#textinput-class),\n[RegressionHead](/head/#regressionhead-class),\n[ClassificationHead](/head/#classificationhead-class),\nfor the format of different types of data.\nYou can also refer to the Data Format section of the tutorials of\n[Image Classification](/tutorial/image_classification/#data-format),\n[Text Classification](/tutorial/text_classification/#data-format),\n[Structured Data Classification](/tutorial/structured_data_classification/#data-format).\n\n## Implement New Block\n\nYou can extend the [Block](/base/#block-class) \nclass to implement your own building blocks and use it with \n[AutoModel](/auto_model/#automodel-class).\n\nThe first step is to learn how to write a build function for [KerasTuner](https://keras-team.github.io/keras-tuner/#usage-the-basics).\nYou need to override the [build function](/base/#build-method) of the block.\nThe following example shows how to implement a single Dense layer block whose number of neurons is tunable.\n""""""\n\nimport autokeras as ak\nimport tensorflow as tf\n\nclass SingleDenseLayerBlock(ak.Block):\n    \n    def build(self, hp, inputs=None):\n        # Get the input_node from inputs.\n        input_node = tf.python.util.nest.flatten(inputs)[0]\n        layer = tf.keras.layers.Dense(\n            hp.Int(\'num_units\', min_value=32, max_value=512, step=32))\n        output_node = layer(input_node)\n        return output_node\n\n""""""\nYou can connect it with other blocks and build it into an\n[AutoModel](/auto_model/#automodel-class).\n""""""\n\n# Build the AutoModel\ninput_node = ak.Input()\noutput_node = SingleDenseLayerBlock()(input_node)\noutput_node = ak.RegressionHead()(output_node)\nauto_model = ak.AutoModel(input_node, output_node, max_trials=1)\n# Prepare Data\nnum_instances = 100\nx_train = np.random.rand(num_instances, 20).astype(np.float32)\ny_train = np.random.rand(num_instances, 1).astype(np.float32)\nx_test = np.random.rand(num_instances, 20).astype(np.float32)\ny_test = np.random.rand(num_instances, 1).astype(np.float32)\n# Train the model\nauto_model.fit(x_train, y_train, epochs=1)\nprint(auto_model.evaluate(x_test, y_test))\n\n""""""\n## Reference\n\n[AutoModel](/auto_model/#automodel-class)\n\n**Nodes**:\n[ImageInput](/node/#imageinput-class),\n[Input](/node/#input-class),\n[StructuredDataInput](/node/#structureddatainput-class),\n[TextInput](/node/#textinput-class).\n\n**Preprocessors**:\n[FeatureEngineering](/preprocessor/#featureengineering-class),\n[ImageAugmentation](/preprocessor/#imageaugmentation-class),\n[LightGBM](/preprocessor/#lightgbm-class),\n[Normalization](/preprocessor/#normalization-class),\n[TextToIntSequence](/preprocessor/#texttointsequence-class),\n[TextToNgramVector](/preprocessor/#texttongramvector-class).\n\n**Blocks**:\n[ConvBlock](/block/#convblock-class),\n[DenseBlock](/block/#denseblock-class),\n[Embedding](/block/#embedding-class),\n[Merge](/block/#merge-class),\n[ResNetBlock](/block/#resnetblock-class),\n[RNNBlock](/block/#rnnblock-class),\n[SpatialReduction](/block/#spatialreduction-class),\n[TemporalReduction](/block/#temporalreduction-class),\n[XceptionBlock](/block/#xceptionblock-class),\n[ImageBlock](/block/#imageblock-class),\n[StructuredDataBlock](/block/#structureddatablock-class),\n[TextBlock](/block/#textblock-class).\n""""""\n'"
docs/py/export.py,2,"b'""""""\nYou can easily export your model the best model found by AutoKeras as a Keras Model.\n\nThe following example uses [ImageClassifier](/image_classifier) as an example.\nAll the tasks and the [AutoModel](/auto_model/#automodel-class) has this\n[export_model](/auto_model/#export_model-method) function.\n\n""""""\n\n""""""shell\n!pip install tensorflow==2.1.0\n!pip install autokeras\n""""""\n\nimport tensorflow as tf\n\nprint(tf.__version__)\n\nfrom tensorflow.keras.datasets import mnist\nimport tensorflow as tf\nimport autokeras as ak\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Initialize the image classifier.\nclf = ak.ImageClassifier(max_trials=1)  # Try only 1 model.(Increase accordingly)\n# Feed the image classifier with training data.\nclf.fit(x_train, y_train, epochs=1)  # Change no of epochs to improve the model\n# Export as a Keras Model.\nmodel = clf.export_model()\n\nprint(type(model))  # <class \'tensorflow.python.keras.engine.training.Model\'>\n\ntry:\n    model.save(""model_autokeras"", save_format=""tf"")\nexcept:\n    model.save(""model_autokeras.h5"")\n\nfrom tensorflow.keras.models import load_model\n\nloaded_model = load_model(""model_autokeras"", custom_objects=ak.CUSTOM_OBJECTS)\n\npredicted_y = loaded_model.predict(tf.expand_dims(x_test, -1))\nprint(predicted_y)\n'"
docs/py/image_classification.py,3,"b'""""""shell\npip install autokeras\n""""""\n\n""""""\n## A Simple Example\nThe first step is to prepare your data. Here we use the MNIST dataset as an example\n""""""\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.python.keras.utils.data_utils import Sequence\nimport autokeras as ak\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint(x_train.shape)  # (60000, 28, 28)\nprint(y_train.shape)  # (60000,)\nprint(y_train[:3])  # array([7, 2, 1], dtype=uint8)\n\n""""""\nThe second step is to run the ImageClassifier.\nIt is recommended have more trials for more complicated datasets.\nThis is just a quick demo of MNIST, so we set max_trials to 1.\n""""""\n\n# Initialize the image classifier.\nclf = ak.ImageClassifier(max_trials=1)\n# Feed the image classifier with training data.\nclf.fit(x_train, y_train, epochs=10)\n\n\n# Predict with the best model.\npredicted_y = clf.predict(x_test)\nprint(predicted_y)\n\n\n# Evaluate the best model with testing data.\nprint(clf.evaluate(x_test, y_test))\n\n""""""\n## Validation Data\nBy default, AutoKeras use the last 20% of training data as validation data. As shown in\nthe example below, you can use validation_split to specify the percentage.\n""""""\n\nclf.fit(\n    x_train,\n    y_train,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=10,\n)\n\n""""""\nYou can also use your own validation set instead of splitting it from the training data\nwith validation_data.\n""""""\n\nsplit = 50000\nx_val = x_train[split:]\ny_val = y_train[split:]\nx_train = x_train[:split]\ny_train = y_train[:split]\nclf.fit(\n    x_train,\n    y_train,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    epochs=10,\n)\n\n""""""\n## Customized Search Space\nFor advanced users, you may customize your search space by using AutoModel instead of\nImageClassifier. You can configure the ImageBlock for some high-level configurations,\ne.g., block_type for the type of neural network to search, normalize for whether to do\ndata normalization, augment for whether to do data augmentation. You can also do not\nspecify these arguments, which would leave the different choices to be tuned\nautomatically. See the following example for detail.\n""""""\n\ninput_node = ak.ImageInput()\noutput_node = ak.ImageBlock(\n    # Only search ResNet architectures.\n    block_type=""resnet"",\n    # Normalize the dataset.\n    normalize=True,\n    # Do not do data augmentation.\n    augment=False,\n)(input_node)\noutput_node = ak.ClassificationHead()(output_node)\nclf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=1)\nclf.fit(x_train, y_train, epochs=10)\n\n""""""\nThe usage of AutoModel is similar to the functional API of Keras. Basically, you are\nbuilding a graph, whose edges are blocks and the nodes are intermediate outputs of\nblocks. To add an edge from input_node to output_node with output_node =\nak.[some_block]([block_args])(input_node).\n\nYou can even also use more fine grained blocks to customize the search space even\nfurther. See the following example.\n""""""\n\ninput_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node = ak.ImageAugmentation(horizontal_flip=False)(output_node)\noutput_node = ak.ResNetBlock(version=""v2"")(output_node)\noutput_node = ak.ClassificationHead()(output_node)\nclf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=1)\nclf.fit(x_train, y_train, epochs=10)\n\n""""""\n## Data Format\nThe AutoKeras ImageClassifier is quite flexible for the data format.\n\nFor the image, it accepts data formats both with and without the channel dimension. The\nimages in the MNIST dataset do not have the channel dimension. Each image is a matrix\nwith shape (28, 28). AutoKeras also accepts images of three dimensions with the channel\ndimension at last, e.g., (32, 32, 3), (28, 28, 1).\n\nFor the classification labels, AutoKeras accepts both plain labels, i.e. strings or\nintegers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s.\n\nSo if you prepare your data in the following way, the ImageClassifier should still work.\n""""""\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Reshape the images to have the channel dimension.\nx_train = x_train.reshape(x_train.shape + (1,))\nx_test = x_test.reshape(x_test.shape + (1,))\n\n# One-hot encode the labels.\neye = np.eye(10)\ny_train = eye[y_train]\ny_test = eye[y_test]\n\nprint(x_train.shape)  # (60000, 28, 28, 1)\nprint(y_train.shape)  # (60000, 10)\nprint(y_train[:3])\n# array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n#        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n#        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n\n""""""\nWe also support using tf.data.Dataset format for the training data. In this case, the\nimages would have to be 3-dimentional. The labels have to be one-hot encoded for\nmulti-class classification to be wrapped into tensorflow Dataset.\n""""""\n\ntrain_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,)))\ntest_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,)))\n\nclf = ak.ImageClassifier(max_trials=1)\n# Feed the tensorflow Dataset to the classifier.\nclf.fit(train_set, epochs=10)\n# Predict with the best model.\npredicted_y = clf.predict(test_set)\n# Evaluate the best model with testing data.\nprint(clf.evaluate(test_set))\n\n""""""\n## Reference\n[ImageClassifier](/image_classifier),\n[AutoModel](/auto_model/#automodel-class),\n[ImageBlock](/block/#imageblock-class),\n[Normalization](/preprocessor/#normalization-class),\n[ImageAugmentation](/preprocessor/#image-augmentation-class),\n[ResNetBlock](/block/#resnetblock-class),\n[ImageInput](/node/#imageinput-class),\n[ClassificationHead](/head/#classificationhead-class).\n""""""\n'"
docs/py/image_regression.py,3,"b'""""""\nRegression tasks estimate a numeric variable, such as the price of a house or voter\nturnout.\n\nThis example is adapted from a\n[notebook](https://gist.github.com/mapmeld/98d1e9839f2d1f9c4ee197953661ed07) which\nestimates a person\'s age from their image, trained on the\n[IMDB-WIKI](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/) photographs of famous\npeople.\n\nFirst, prepare your image data in a numpy.ndarray or tensorflow.Dataset format. Each\nimage must have the same shape, meaning each has the same width, height, and color\nchannels as other images in the set.\n""""""\n\n""""""\n### Connect your Google Drive for Data\n""""""\n\n\nimport os\nfrom datetime import datetime\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom google.colab import drive\nfrom PIL import Image\nfrom scipy.io import loadmat\nfrom tensorflow.python.keras.utils.data_utils import Sequence\nimport autokeras as ak\ndrive.mount(""/content/drive"")\n\n""""""\n### Install AutoKeras and TensorFlow\n\nDownload the master branch to your Google Drive for this tutorial. In general, you can\nuse *pip install autokeras* .\n""""""\n\n""""""shell\n!pip install  -v ""/content/drive/My Drive/AutoKeras-dev/autokeras-master.zip""\n!pip uninstall keras-tuner\n!pip install\ngit+git://github.com/keras-team/keras-tuner.git@d2d69cba21a0b482a85ce2a38893e2322e139c01\n""""""\n\n""""""shell\n!pip install tensorflow==2.2.0\n""""""\n\n""""""\n###**Import IMDB Celeb images and metadata**\n""""""\n\n""""""shell\n!mkdir ./drive/My\\ Drive/mlin/celebs\n""""""\n\n""""""shell\n! wget -O ./drive/My\\ Drive/mlin/celebs/imdb_0.tar\nhttps://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/imdb_0.tar\n""""""\n\n""""""shell\n! cd ./drive/My\\ Drive/mlin/celebs && tar -xf imdb_0.tar\n! rm ./drive/My\\ Drive/mlin/celebs/imdb_0.tar\n""""""\n\n""""""\nUncomment and run the below cell if you need to re-run the cells again and above don\'t\nneed to install everything from the beginning.\n""""""\n\n# ! cd ./drive/My\\ Drive/mlin/celebs.\n\n""""""shell\n! ls ./drive/My\\ Drive/mlin/celebs/imdb/\n""""""\n\n""""""shell\n! wget https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/imdb_meta.tar\n! tar -xf imdb_meta.tar\n! rm imdb_meta.tar\n""""""\n\n""""""\n###**Converting from MATLAB date to actual Date-of-Birth**\n""""""\n\n\ndef datenum_to_datetime(datenum):\n    """"""\n    Convert Matlab datenum into Python datetime.\n    """"""\n    days = datenum % 1\n    hours = days % 1 * 24\n    minutes = hours % 1 * 60\n    seconds = minutes % 1 * 60\n    try:\n        return (\n            datetime.fromordinal(int(datenum))\n            + timedelta(days=int(days))\n            + timedelta(hours=int(hours))\n            + timedelta(minutes=int(minutes))\n            + timedelta(seconds=round(seconds))\n            - timedelta(days=366)\n        )\n    except:\n        return datenum_to_datetime(700000)\n\n\nprint(datenum_to_datetime(734963))\n\n""""""\n### **Opening MatLab file to Pandas DataFrame**\n""""""\n\n\nx = loadmat(""imdb/imdb.mat"")\n\n\nmdata = x[""imdb""]  # variable in mat file\nmdtype = mdata.dtype  # dtypes of structures are ""unsized objects""\nndata = {n: mdata[n][0, 0] for n in mdtype.names}\ncolumns = [n for n, v in ndata.items()]\n\nrows = []\nfor col in range(0, 10):\n    values = list(ndata.items())[col]\n    for num, val in enumerate(values[1][0], start=0):\n        if col == 0:\n            rows.append([])\n        if num > 0:\n            if columns[col] == ""dob"":\n                rows[num].append(datenum_to_datetime(int(val)))\n            elif columns[col] == ""photo_taken"":\n                rows[num].append(datetime(year=int(val), month=6, day=30))\n            else:\n                rows[num].append(val)\n\ndt = map(lambda row: np.array(row), np.array(rows[1:]))\n\ndf = pd.DataFrame(data=dt, index=range(0, len(rows) - 1), columns=columns)\nprint(df.head())\n\nprint(columns)\nprint(df[""full_path""])\n\n""""""\n### **Calculating age at time photo was taken**\n""""""\n\ndf[""age""] = (df[""photo_taken""] - df[""dob""]).astype(""int"") / 31558102e9\nprint(df[""age""])\n\n""""""\n### **Creating dataset**\n\n\n* We sample 200 of the images which were included in this first download.\n* Images are resized to 128x128 to standardize shape and conserve memory\n* RGB images are converted to grayscale to standardize shape\n* Ages are converted to ints\n\n\n""""""\n\n\ndef df2numpy(train_set):\n    images = []\n    for img_path in train_set[""full_path""]:\n        img = (\n            Image.open(""./drive/My Drive/mlin/celebs/imdb/"" + img_path[0])\n            .resize((128, 128))\n            .convert(""L"")\n        )\n        images.append(np.asarray(img, dtype=""int32""))\n\n    image_inputs = np.array(images)\n\n    ages = train_set[""age""].astype(""int"").to_numpy()\n    return image_inputs, ages\n\n\ntrain_set = df[df[""full_path""] < ""02""].sample(200)\ntrain_imgs, train_ages = df2numpy(train_set)\n\ntest_set = df[df[""full_path""] < ""02""].sample(100)\ntest_imgs, test_ages = df2numpy(test_set)\n\n""""""\n### **Training using AutoKeras**\n""""""\n\n\n# Initialize the image regressor\nreg = ak.ImageRegressor(max_trials=15)  # AutoKeras tries 15 different models.\n\n# Find the best model for the given training data\nreg.fit(train_imgs, train_ages)\n\n# Predict with the chosen model:\n# predict_y = reg.predict(test_images)  # Uncomment if required\n\n# Evaluate the chosen model with testing data\nprint(reg.evaluate(test_images, test_ages))\n\n""""""\n### **Validation Data**\n\nBy default, AutoKeras use the last 20% of training data as validation data. As shown in\nthe example below, you can use validation_split to specify the percentage.\n""""""\n\nreg.fit(\n    train_imgs,\n    train_ages,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=3,\n)\n\n""""""\nYou can also use your own validation set instead of splitting it from the training data\nwith validation_data.\n""""""\n\nsplit = 460000\nx_val = train_imgs[split:]\ny_val = train_ages[split:]\nx_train = train_imgs[:split]\ny_train = train_ages[:split]\nreg.fit(\n    x_train,\n    y_train,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    epochs=3,\n)\n\n""""""\n### **Customized Search Space**\n\nFor advanced users, you may customize your search space by using AutoModel instead of\nImageRegressor. You can configure the ImageBlock for some high-level configurations,\ne.g., block_type for the type of neural network to search, normalize for whether to do\ndata normalization, augment for whether to do data augmentation. You can also choose not\nto specify these arguments, which would leave the different choices to be tuned\nautomatically. See the following example for detail.\n""""""\n\n\ninput_node = ak.ImageInput()\noutput_node = ak.ImageBlock(\n    # Only search ResNet architectures.\n    block_type=""resnet"",\n    # Normalize the dataset.\n    normalize=True,\n    # Do not do data augmentation.\n    augment=False,\n)(input_node)\noutput_node = ak.RegressionHead()(output_node)\nreg = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10)\nreg.fit(x_train, y_train, epochs=3)\n\n""""""\nThe usage of AutoModel is similar to the functional API of Keras. Basically, you are\nbuilding a graph, whose edges are blocks and the nodes are intermediate outputs of\nblocks. To add an edge from input_node to output_node with output_node =\nak.some_block(input_node).\nYou can even also use more fine grained blocks to customize the search space even\nfurther. See the following example.\n""""""\n\n\ninput_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node = ak.ImageAugmentation(translation_factor=0.3)(output_node)\noutput_node = ak.ResNetBlock(version=""v2"")(output_node)\noutput_node = ak.RegressionHead()(output_node)\nclf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10)\nclf.fit(x_train, y_train, epochs=3)\n\n""""""\n### **Data Format**\n""""""\n\n""""""\nThe AutoKeras ImageClassifier is quite flexible for the data format.\n\nFor the image, it accepts data formats both with and without the channel dimension. The\nimages in the IMDB-Wiki dataset do not have a channel dimension. Each image is a matrix\nwith shape (128, 128). AutoKeras also accepts images with a channel dimension at last,\ne.g., (32, 32, 3), (28, 28, 1).\n\nFor the classification labels, AutoKeras accepts both plain labels, i.e. strings or\nintegers, and one-hot encoded labels, i.e. vectors of 0s and 1s.\n\nSo if you prepare your data in the following way, the ImageClassifier should still work.\n""""""\n\n# Reshape the images to have the channel dimension.\ntrain_imgs = train_imgs.reshape(train_imgs.shape + (1,))\ntest_imgs = test_imgs.reshape(test_imgs.shape + (1,))\n\nprint(train_imgs.shape)  # (200, 128, 128, 1)\nprint(test_imgs.shape)  # (100, 128, 128, 1)\nprint(train_ages[:3])\n\n""""""\nWe also support using tf.data.Dataset format for the training data. In this case, the\nimages would have to be 3-dimentional. The labels have to be one-hot encoded for\nmulti-class classification to be wrapped into tensorflow Dataset.\n""""""\n\n\ntrain_set = tf.data.Dataset.from_tensor_slices(((train_imgs,), (train_ages,)))\ntest_set = tf.data.Dataset.from_tensor_slices(((test_imgs,), (test_ages,)))\n\nreg = ak.ImageRegressor(max_trials=15)\n# Feed the tensorflow Dataset to the classifier.\nreg.fit(train_set)\n# Predict with the best model.\npredicted_y = clf.predict(test_set)\n# Evaluate the best model with testing data.\nprint(clf.evaluate(test_set))\n\n""""""\n## References\n\n[Main Reference\nNotebook](https://gist.github.com/mapmeld/98d1e9839f2d1f9c4ee197953661ed07),\n[Dataset](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/),\n[ImageRegressor](/image_regressor),\n[ResNetBlock](/block/#resnetblock-class),\n[ImageInput](/node/#imageinput-class),\n[AutoModel](/auto_model/#automodel-class),\n[ImageBlock](/block/#imageblock-class),\n[Normalization](/preprocessor/#normalization-class),\n[ImageAugmentation](/preprocessor/#image-augmentation-class),\n[RegressionHead](/head/#regressionhead-class).\n\n""""""\n'"
docs/py/multi.py,0,"b'""""""shell\npip install autokeras\n""""""\n\n""""""\nIn this tutorial we are making use of the \n[AutoModel](/auto_model/#automodel-class)\n API to show how to handle multi-modal data and multi-task.\n\n## What is multi-modal?\n\nMulti-model data means each data instance has multiple forms of information. For example, a photo can be saved as a image. Besides the image, it may also have when and where it was taken as its attributes, which can be represented as structured data. \n\n## What is multi-task?\n\nMulti-task here we refer to we want to predict multiple targets with the same input features. For example, we not only want to classify an image according to its content, but we also want to regress its quality as a float number between 0 and 1.\n\nThe following diagram shows an example of multi-modal and multi-task neural network model.\n\n<div class=""mermaid"">\ngraph TD\n    id1(ImageInput) --> id3(Some Neural Network Model)\n    id2(StructuredDataInput) --> id3\n    id3 --> id4(ClassificationHead)\n    id3 --> id5(RegressionHead)\n</div>\n\nIt has two inputs the images and the structured data. Each image is associated with a set of attributes in the structured data. From these data, we are trying to predict the classification label and the regression value at the same time.\n\n## Data Preparation\n\nTo illustrate our idea, we generate some random image and structured data as the multi-modal data.\n""""""\n\nimport numpy as np\n\nnum_instances = 100\n# Generate image data.\nimage_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)\n# Generate structured data.\nstructured_data = np.random.rand(num_instances, 20).astype(np.float32)\n\n""""""\nWe also generate some multi-task targets for classification and regression.\n""""""\n\n# Generate regression targets.\nregression_target = np.random.rand(num_instances, 1).astype(np.float32)\n# Generate classification labels of five classes.\nclassification_target = np.random.randint(5, size=num_instances)\n\n""""""\n## Build and Train the Model\nThen we initialize the multi-modal and multi-task model with \n[AutoModel](/auto_model/#automodel-class).\nSince this is just a demo, we use small amount of `max_trials` and `epochs`.\n""""""\n\nimport autokeras as ak\n# Initialize the multi with multiple inputs and outputs.\nmodel = ak.AutoModel(\n    inputs=[ak.ImageInput(), ak.StructuredDataInput()],\n    outputs=[\n        ak.RegressionHead(metrics=[\'mae\']),\n        ak.ClassificationHead(loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\n    ],\n    max_trials=2)\n# Fit the model with prepared data.\nmodel.fit(\n    [image_data, structured_data],\n    [regression_target, classification_target],\n    epochs=3)\n\n""""""\n## Validation Data\nBy default, AutoKeras use the last 20% of training data as validation data.\nAs shown in the example below, you can use `validation_split` to specify the percentage.\n""""""\n\nmodel.fit(\n    [image_data, structured_data],\n    [regression_target, classification_target],\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=2)\n\n""""""\nYou can also use your own validation set\ninstead of splitting it from the training data with `validation_data`.\n""""""\n\nsplit = 20\n\nimage_val = image_data[split:]\nstructured_val = structured_data[split:]\nregression_val = regression_target[split:]\nclassification_val = classification_target[split:]\n\nimage_data = image_data[:split]\nstructured_data = structured_data[:split]\nregression_target = regression_target[:split]\nclassification_target = classification_target[:split]\n\nmodel.fit(\n    [image_data, structured_data],\n    [regression_target, classification_target],\n    # Use your own validation set.\n    validation_data=(\n        [image_val, structured_val],\n        [regression_val, classification_val]),\n    epochs=2)\n\n""""""\n## Customized Search Space\nYou can customize your search space.\nThe following figure shows the search space we want to define.\n\n<div class=""mermaid"">\ngraph LR\n    id1(ImageInput) --> id2(Normalization)\n    id2 --> id3(Image Augmentation)\n    id3 --> id4(Convolutional)\n    id3 --> id5(ResNet V2)\n    id4 --> id6(Merge)\n    id5 --> id6\n    id7(StructuredDataInput) --> id8(CategoricalToNumerical)\n    id8 --> id9(DenseBlock)\n    id6 --> id10(Merge)\n    id9 --> id10\n    id10 --> id11(Classification Head)\n    id10 --> id12(Regression Head)\n</div>\n""""""\n\nimport autokeras as ak\n\ninput_node1 = ak.ImageInput()\noutput_node = ak.Normalization()(input_node1)\noutput_node = ak.ImageAugmentation()(output_node)\noutput_node1 = ak.ConvBlock()(output_node)\noutput_node2 = ak.ResNetBlock(version=\'v2\')(output_node)\noutput_node1 = ak.Merge()([output_node1, output_node2])\n\ninput_node2 = ak.StructuredDataInput()\noutput_node = ak.CategoricalToNumerical()(input_node2)\noutput_node2 = ak.DenseBlock()(output_node)\n\noutput_node = ak.Merge()([output_node1, output_node2])\noutput_node1 = ak.ClassificationHead()(output_node)\noutput_node2 = ak.RegressionHead()(output_node)\n\nauto_model = ak.AutoModel(\n    inputs=[input_node1, input_node2], \n    outputs=[output_node1, output_node2],\n    max_trials=2)\n\nimage_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)\nstructured_data = np.random.rand(num_instances, 20).astype(np.float32)\nregression_target = np.random.rand(num_instances, 1).astype(np.float32)\nclassification_target = np.random.randint(5, size=num_instances)\n\nauto_model.fit(\n    [image_data, structured_data],\n    [classification_target, regression_target],\n    batch_size=32,\n    epochs=3)\n\n""""""\n## Data Format\nYou can refer to the documentation of\n[ImageInput](/node/#imageinput-class),\n[StructuredDataInput](/node/#structureddatainput-class),\n[TextInput](/node/#textinput-class),\n[RegressionHead](/head/#regressionhead-class),\n[ClassificationHead](/head/#classificationhead-class),\nfor the format of different types of data.\nYou can also refer to the Data Format section of the tutorials of\n[Image Classification](/tutorial/image_classification/#data-format),\n[Text Classification](/tutorial/text_classification/#data-format),\n[Structured Data Classification](/tutorial/structured_data_classification/#data-format).\n\n\n## Reference\n[AutoModel](/auto_model/#automodel-class),\n[ImageInput](/node/#imageinput-class),\n[StructuredDataInput](/node/#structureddatainput-class),\n[DenseBlock](/block/#denseblock-class),\n[RegressionHead](/head/#regressionhead-class),\n[ClassificationHead](/head/#classificationhead-class),\n[CategoricalToNumerical](/preprocessor/#categoricaltonumerical-class).\n""""""\n'"
docs/py/structured_data_classification.py,6,"b'""""""shell\npip install autokeras\n""""""\n\n""""""\n## A Simple Example\nThe first step is to prepare your data. Here we use the [Titanic\ndataset](https://www.kaggle.com/c/titanic) as an example. You can download the CSV\nfiles [here](https://github.com/keras-team/autokeras/tree/master/tests/fixtures/titanic).\n\nThe second step is to run the\n[StructuredDataClassifier](/structured_data_classifier).\nReplace all the `/path/to` with the path to the csv files.\n""""""\n\nimport tensorflow as tf\nimport autokeras as ak\n\nTRAIN_DATA_URL = ""https://storage.googleapis.com/tf-datasets/titanic/train.csv""\nTEST_DATA_URL = ""https://storage.googleapis.com/tf-datasets/titanic/eval.csv""\n\ntrain_file_path = tf.keras.utils.get_file(""train.csv"", TRAIN_DATA_URL)\ntest_file_path = tf.keras.utils.get_file(""eval.csv"", TEST_DATA_URL)\n\n# Initialize the structured data classifier.\nclf = ak.StructuredDataClassifier(max_trials=3) # It tries 10 different models.\n# Feed the structured data classifier with training data.\nclf.fit(\n    # The path to the train.csv file.\n    train_file_path,\n    # The name of the label column.\n    \'survived\',\n    epochs=10)\n# Predict with the best model.\npredicted_y = clf.predict(test_file_path)\n# Evaluate the best model with testing data.\nprint(clf.evaluate(test_file_path, \'survived\'))\n\n""""""\n## Data Format\nThe AutoKeras StructuredDataClassifier is quite flexible for the data format.\n\nThe example above shows how to use the CSV files directly. Besides CSV files, it also\nsupports numpy.ndarray, pandas.DataFrame or [tf.data.Dataset](\nhttps://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable). The data should be\ntwo-dimensional with numerical or categorical values.\n\nFor the classification labels,\nAutoKeras accepts both plain labels, i.e.  strings or integers, and one-hot encoded\nencoded labels, i.e. vectors of 0s and 1s.\nThe labels can be numpy.ndarray, pandas.DataFrame, or pandas.Series.\n\nThe following examples show how the data can be prepared with numpy.ndarray,\npandas.DataFrame, and tensorflow.data.Dataset.\n""""""\n\nimport pandas as pd\nimport numpy as np\n# x_train as pandas.DataFrame, y_train as pandas.Series\nx_train = pd.read_csv(train_file_path)\nprint(type(x_train)) # pandas.DataFrame\ny_train = x_train.pop(\'survived\')\nprint(type(y_train)) # pandas.Series\n\n# You can also use pandas.DataFrame for y_train.\ny_train = pd.DataFrame(y_train)\nprint(type(y_train)) # pandas.DataFrame\n\n# You can also use numpy.ndarray for x_train and y_train.\nx_train = x_train.to_numpy().astype(np.unicode)\ny_train = y_train.to_numpy()\nprint(type(x_train)) # numpy.ndarray\nprint(type(y_train)) # numpy.ndarray\n\n# Preparing testing data.\nx_test = pd.read_csv(test_file_path)\ny_test = x_test.pop(\'survived\')\n\n# It tries 10 different models.\nclf = ak.StructuredDataClassifier(max_trials=3)\n# Feed the structured data classifier with training data.\nclf.fit(x_train, y_train, epochs=10)\n# Predict with the best model.\npredicted_y = clf.predict(x_test)\n# Evaluate the best model with testing data.\nprint(clf.evaluate(x_test, y_test))\n\n""""""\nThe following code shows how to convert numpy.ndarray to tf.data.Dataset.\nNotably, the labels have to be one-hot encoded for multi-class\nclassification to be wrapped into tensorflow Dataset.\nSince the Titanic dataset is binary\nclassification, it should not be one-hot encoded.\n""""""\n\ntrain_set = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntest_set = tf.data.Dataset.from_tensor_slices((x_test.to_numpy().astype(np.unicode), y_test))\n\nclf = ak.StructuredDataClassifier(max_trials=3)\n# Feed the tensorflow Dataset to the classifier.\nclf.fit(train_set, epochs=10)\n# Predict with the best model.\npredicted_y = clf.predict(test_set)\n# Evaluate the best model with testing data.\nprint(clf.evaluate(test_set))\n\n""""""\nYou can also specify the column names and types for the data as follows.\nThe `column_names` is optional if the training data already have the column names, e.g.\npandas.DataFrame, CSV file.\nAny column, whose type is not specified will be inferred from the training data.\n""""""\n\n# Initialize the structured data classifier.\nclf = ak.StructuredDataClassifier(\n    column_names=[\n        \'sex\',\n        \'age\',\n        \'n_siblings_spouses\',\n        \'parch\',\n        \'fare\',\n        \'class\',\n        \'deck\',\n        \'embark_town\',\n        \'alone\'],\n    column_types={\'sex\': \'categorical\', \'fare\': \'numerical\'},\n    max_trials=10, # It tries 10 different models.\n)\n\n\n""""""\n## Validation Data\nBy default, AutoKeras use the last 20% of training data as validation data.\nAs shown in the example below, you can use `validation_split` to specify the percentage.\n""""""\n\nclf.fit(x_train,\n        y_train,\n        # Split the training data and use the last 15% as validation data.\n        validation_split=0.15,\n        epochs=10)\n\n""""""\nYou can also use your own validation set\ninstead of splitting it from the training data with `validation_data`.\n""""""\n\nsplit = 500\nx_val = x_train[split:]\ny_val = y_train[split:]\nx_train = x_train[:split]\ny_train = y_train[:split]\nclf.fit(x_train,\n        y_train,\n        # Use your own validation set.\n        validation_data=(x_val, y_val),\n        epochs=10)\n\n""""""\n## Customized Search Space\nFor advanced users, you may customize your search space by using\n[AutoModel](/auto_model/#automodel-class) instead of\n[StructuredDataClassifier](/structured_data_classifier). You can configure the\n[StructuredDataBlock](/block/#structureddatablock-class) for some high-level\nconfigurations, e.g., `categorical_encoding` for whether to use the\n[CategoricalToNumerical](/preprocessor/#categoricaltonumerical-class). You can also do not specify these\narguments, which would leave the different choices to be tuned automatically. See\nthe following example for detail.\n""""""\n\nimport autokeras as ak\n\ninput_node = ak.StructuredDataInput()\noutput_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)\noutput_node = ak.ClassificationHead()(output_node)\nclf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=3)\nclf.fit(x_train, y_train, epochs=10)\n\n""""""\nThe usage of [AutoModel](/auto_model/#automodel-class) is similar to the\n[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.\nBasically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks.\nTo add an edge from `input_node` to `output_node` with\n`output_node = ak.[some_block]([block_args])(input_node)`.\n\nYou can even also use more fine grained blocks to customize the search space even\nfurther. See the following example.\n""""""\n\nimport autokeras as ak\n\ninput_node = ak.StructuredDataInput()\noutput_node = ak.CategoricalToNumerical()(input_node)\noutput_node = ak.DenseBlock()(output_node)\noutput_node = ak.ClassificationHead()(output_node)\nclf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=3)\nclf.fit(x_train, y_train, epochs=10)\n\n\n""""""\n## Reference\n[StructuredDataClassifier](/structured_data_classifier),\n[AutoModel](/auto_model/#automodel-class),\n[StructuredDataClassifier](/structured_data_classifier),\n[StructuredDataBlock](/block/#structureddatablock-class),\n[DenseBlock](/block/#denseblock-class),\n[StructuredDataInput](/node/#structureddatainput-class),\n[ClassificationHead](/head/#classificationhead-class),\n[CategoricalToNumerical](/preprocessor/#categoricaltonumerical-class).\n""""""\n'"
docs/py/structured_data_regression.py,1,"b'""""""shell\npip install autokeras\n""""""\n\n""""""\n## A Simple Example with Auto MPG Data Set\n\nDownload [Auto MPG Data Set](https://archive.ics.uci.edu/ml/datasets/auto+mpg):\n""""""\n\nimport tensorflow as tf\nimport pandas as pd\ncolumn_names = [\'MPG\',\'Cylinders\',\'Displacement\',\'Horsepower\',\'Weight\',\n                \'Acceleration\', \'Model Year\', \'Origin\']\ndataset_path = ""http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data""\nraw_dataset = pd.read_csv(dataset_path, names=column_names,\n                      na_values = ""?"", comment=\'\\t\',\n                      sep="" "", skipinitialspace=True)\n\ndataset = raw_dataset.copy()\ndataset = dataset.dropna()\ndataset.tail()\n\n""""""\nMake all but the last feature (\'Origin\') numerical.\n""""""\n\ncolumn_names.remove(\'MPG\')\ndata_cols =column_names \ndata_type = (len(data_cols)-1) * [\'numerical\'] + [\'categorical\']\ndata_type = dict(zip(data_cols, data_type))\n\ntrain_dataset = dataset.sample(frac=0.8,random_state=0)\ntest_dataset = dataset.drop(train_dataset.index)\ntrain_dataset.describe()\n\nimport autokeras as ak\n\nregressor = ak.StructuredDataRegressor(max_trials=100, column_names=data_cols, column_types=data_type)\nregressor.fit(x=train_dataset.drop(columns=[\'MPG\']), y=train_dataset[\'MPG\'])\n# Evaluate the accuracy of the found model.\nprint(\'Accuracy: {accuracy}\'.format(\n    accuracy=regressor.evaluate(x=test_dataset.drop(columns=[\'MPG\']), y=test_dataset[\'MPG\'])))\n\n""""""\nAccuracy: [9.906872749328613, 9.715665]\n""""""\n\nmodel = regressor.export_model()\ntf.keras.utils.plot_model(model, show_shapes=True, expand_nested=True)\n\n""""""\n![Network Topology found by autokeras](Reg_Network.png)\n""""""\n\n'"
docs/py/text_classification.py,3,"b'""""""shell\npip install autokeras\n""""""\n\n""""""\n## A Simple Example\nThe first step is to prepare your data. Here we use the [IMDB\ndataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) as\nan example.\n""""""\n\nimport numpy as np\nfrom tensorflow.keras.datasets import imdb\n\n# Load the integer sequence the IMDB dataset with Keras.\nindex_offset = 3  # word index offset\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000,\n                                                      index_from=index_offset)\ny_train = y_train.reshape(-1, 1)\ny_test = y_test.reshape(-1, 1)\n# Prepare the dictionary of index to word.\nword_to_id = imdb.get_word_index()\nword_to_id = {k: (v + index_offset) for k, v in word_to_id.items()}\nword_to_id[""<PAD>""] = 0\nword_to_id[""<START>""] = 1\nword_to_id[""<UNK>""] = 2\nid_to_word = {value: key for key, value in word_to_id.items()}\n# Convert the word indices to words.\nx_train = list(map(lambda sentence: \' \'.join(\n    id_to_word[i] for i in sentence), x_train))\nx_test = list(map(lambda sentence: \' \'.join(\n    id_to_word[i] for i in sentence), x_test))\nx_train = np.array(x_train, dtype=np.str)\nx_test = np.array(x_test, dtype=np.str)\nprint(x_train.shape)  # (25000,)\nprint(y_train.shape)  # (25000, 1)\nprint(x_train[0][:50])  # <START> this film was just brilliant casting <UNK>\n\n""""""\nThe second step is to run the [TextClassifier](/text_classifier).\n""""""\n\nimport autokeras as ak\n\n# Initialize the text classifier.\nclf = ak.TextClassifier(max_trials=1) # It tries 10 different models.\n# Feed the text classifier with training data.\nclf.fit(x_train, y_train, epochs=2)\n# Predict with the best model.\npredicted_y = clf.predict(x_test)\n# Evaluate the best model with testing data.\nprint(clf.evaluate(x_test, y_test))\n\n\n""""""\n## Validation Data\nBy default, AutoKeras use the last 20% of training data as validation data.\nAs shown in the example below, you can use `validation_split` to specify the percentage.\n""""""\n\nclf.fit(x_train,\n        y_train,\n        # Split the training data and use the last 15% as validation data.\n        validation_split=0.15)\n\n""""""\nYou can also use your own validation set\ninstead of splitting it from the training data with `validation_data`.\n""""""\n\nsplit = 5000\nx_val = x_train[split:]\ny_val = y_train[split:]\nx_train = x_train[:split]\ny_train = y_train[:split]\nclf.fit(x_train,\n        y_train,\n        epochs=2,\n        # Use your own validation set.\n        validation_data=(x_val, y_val))\n\n""""""\n## Customized Search Space\nFor advanced users, you may customize your search space by using\n[AutoModel](/auto_model/#automodel-class) instead of\n[TextClassifier](/text_classifier). You can configure the\n[TextBlock](/block/#textblock-class) for some high-level configurations, e.g., `vectorizer`\nfor the type of text vectorization method to use.  You can use \'sequence\', which uses\n[TextToInteSequence](/preprocessor/#texttointsequence-class) to convert the words to\nintegers and use [Embedding](/block/#embedding-class) for embedding the\ninteger sequences, or you can use \'ngram\', which uses\n[TextToNgramVector](/preprocessor/#texttongramvector-class) to vectorize the\nsentences.  You can also do not specify these arguments, which would leave the\ndifferent choices to be tuned automatically.  See the following example for detail.\n""""""\n\nimport autokeras as ak\n\ninput_node = ak.TextInput()\noutput_node = ak.TextBlock(vectorizer=\'ngram\')(input_node)\noutput_node = ak.ClassificationHead()(output_node)\nclf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=1)\nclf.fit(x_train, y_train, epochs=2)\n\n""""""\nThe usage of [AutoModel](/auto_model/#automodel-class) is similar to the\n[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.\nBasically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks.\nTo add an edge from `input_node` to `output_node` with\n`output_node = ak.[some_block]([block_args])(input_node)`.\n\nYou can even also use more fine grained blocks to customize the search space even\nfurther. See the following example.\n""""""\n\nimport autokeras as ak\n\ninput_node = ak.TextInput()\noutput_node = ak.TextToIntSequence()(input_node)\noutput_node = ak.Embedding()(output_node)\n# Use separable Conv layers in Keras.\noutput_node = ak.ConvBlock(separable=True)(output_node)\noutput_node = ak.ClassificationHead()(output_node)\nclf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=1)\nclf.fit(x_train, y_train, epochs=2)\n\n""""""\n## Data Format\nThe AutoKeras TextClassifier is quite flexible for the data format.\n\nFor the text, the input data should be one-dimensional \nFor the classification labels, AutoKeras accepts both plain labels, i.e. strings or\nintegers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s.\n\nWe also support using [tf.data.Dataset](\nhttps://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable) format for\nthe training data.\nThe labels have to be one-hot encoded for multi-class\nclassification to be wrapped into tensorflow Dataset.\nSince the IMDB dataset is binary classification, it should not be one-hot encoded.\n""""""\n\nimport tensorflow as tf\ntrain_set = tf.data.Dataset.from_tensor_slices(((x_train, ), (y_train, ))).batch(32)\ntest_set = tf.data.Dataset.from_tensor_slices(((x_test, ), (y_test, ))).batch(32)\n\nclf = ak.TextClassifier(max_trials=3)\n# Feed the tensorflow Dataset to the classifier.\nclf.fit(train_set, epochs=2)\n# Predict with the best model.\npredicted_y = clf.predict(test_set)\n# Evaluate the best model with testing data.\nprint(clf.evaluate(test_set))\n\n""""""\n## Reference\n[TextClassifier](/text_classifier),\n[AutoModel](/auto_model/#automodel-class),\n[TextBlock](/block/#textblock-class),\n[TextToInteSequence](/preprocessor/#texttointsequence-class),\n[Embedding](/block/#embedding-class),\n[TextToNgramVector](/preprocessor/#texttongramvector-class),\n[ConvBlock](/block/#convblock-class),\n[TextInput](/node/#textinput-class),\n[ClassificationHead](/head/#classificationhead-class).\n""""""\n'"
docs/py/text_regression.py,0,"b'""""""shell\npip install autokeras\n""""""\n\n""""""\n## Social Media Articles Example\n\nRegression tasks estimate a numeric variable, such as the price of a house\nor a person\'s age.\n\nThis example estimates the view counts for an article on social media platforms,\ntrained on a\n[News Popularity](https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms)\ndataset collected from 2015-2016.\n\nFirst, prepare your text data in a `numpy.ndarray` or `tensorflow.Dataset`\nformat.\n""""""\n\nimport pandas as pd\nimport numpy as np\n\n# converting from other formats (such as pandas) to numpy\ntrain_df = pd.read_csv(""./News_Final.csv"")\n\ntext_inputs = df.Title.to_numpy(dtype=""str"")\nmedia_success_outputs = df.Facebook.to_numpy(dtype=""int"")\n\n""""""\nNext, initialize and train the [TextRegressor](/text_regressor).\n""""""\n\nimport autokeras as ak\n\n# Initialize the text regressor\nreg = ak.TextRegressor(max_trials=15) # AutoKeras tries 15 different models.\n\n# Find the best model for the given training data\nreg.fit(text_inputs, media_success_outputs)\n\n# Predict with the chosen model:\npredict_y = reg.predict(predict_x)\n\n""""""\nIf your text source has a larger vocabulary (number of distinct words), you may\nneed to create a custom pipeline in AutoKeras to increase the `max_tokens`\nparameter.\n""""""\n\ntext_input = (df.Title + "" "" + df.Headline).to_numpy(dtype=""str"")\n\n# text input and tokenization\ninput_node = ak.TextInput()\noutput_node = ak.TextToIntSequence(max_tokens=20000)(input_node)\n\n# regression output\noutput_node = ak.RegressionHead()(output_node)\n\n# initialize AutoKeras and find the best model\nreg = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=15)\nreg.fit(text_input, media_success_output)\n\n""""""\nMeasure the accuracy of the regressor on an independent test set:\n""""""\n\nprint(reg.evaluate(test_text, test_responses))\n'"
tests/autokeras/__init__.py,0,b''
tests/autokeras/auto_model_test.py,15,"b""from unittest import mock\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport autokeras as ak\nfrom tests import utils\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_evaluate(tuner_fn, tmp_path):\n    x_train = np.random.rand(100, 32)\n    y_train = np.random.rand(100, 1)\n\n    input_node = ak.Input()\n    output_node = input_node\n    output_node = ak.DenseBlock()(output_node)\n    output_node = ak.RegressionHead()(output_node)\n\n    auto_model = ak.AutoModel(input_node,\n                              output_node,\n                              directory=tmp_path,\n                              max_trials=1)\n    auto_model.fit(x_train, y_train, epochs=1, validation_data=(x_train, y_train))\n    auto_model.evaluate(x_train, y_train)\n    assert tuner_fn.called\n\n\ndef get_single_io_auto_model(tmp_path):\n    return ak.AutoModel(ak.ImageInput(),\n                        ak.RegressionHead(),\n                        directory=tmp_path,\n                        max_trials=2)\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_auto_model_predict(tuner_fn, tmp_path):\n    x_train = np.random.rand(100, 32, 32, 3)\n    y_train = np.random.rand(100, 1)\n\n    auto_model = get_single_io_auto_model(tmp_path)\n    auto_model.fit(x_train, y_train, epochs=2, validation_split=0.2)\n    auto_model.predict(x_train)\n    assert tuner_fn.called\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_final_fit_concat(tuner_fn, tmp_path):\n    tuner = tuner_fn.return_value.return_value\n\n    x_train = np.random.rand(100, 32, 32, 3)\n    y_train = np.random.rand(100, 1)\n\n    auto_model = get_single_io_auto_model(tmp_path)\n    auto_model.fit(x_train, y_train, epochs=2, validation_split=0.2)\n    assert auto_model._split_dataset\n    assert tuner.search.call_args_list[0][1]['fit_on_val_data']\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_final_fit_not_concat(tuner_fn, tmp_path):\n    tuner = tuner_fn.return_value.return_value\n\n    x_train = np.random.rand(100, 32, 32, 3)\n    y_train = np.random.rand(100, 1)\n\n    auto_model = get_single_io_auto_model(tmp_path)\n    auto_model.fit(x_train, y_train, epochs=2, validation_data=(x_train, y_train))\n    assert not auto_model._split_dataset\n    assert not tuner.search.call_args_list[0][1]['fit_on_val_data']\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_overwrite(tuner_fn, tmp_path):\n    tuner_class = tuner_fn.return_value\n\n    x_train = np.random.rand(100, 32, 32, 3)\n    y_train = np.random.rand(100, 1)\n\n    auto_model = get_single_io_auto_model(tmp_path)\n    auto_model.fit(x_train, y_train, epochs=2, validation_data=(x_train, y_train))\n    assert not tuner_class.call_args_list[0][1]['overwrite']\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_export_model(tuner_fn, tmp_path):\n    tuner_class = tuner_fn.return_value\n    tuner = tuner_class.return_value\n\n    x_train = np.random.rand(100, 32, 32, 3)\n    y_train = np.random.rand(100, 1)\n\n    auto_model = get_single_io_auto_model(tmp_path)\n    auto_model.fit(x_train, y_train, epochs=2, validation_data=(x_train, y_train))\n    auto_model.export_model()\n    assert tuner.get_best_model.called\n\n\ndef get_multi_io_auto_model(tmp_path):\n    return ak.AutoModel([ak.ImageInput(), ak.ImageInput()],\n                        [ak.RegressionHead(), ak.RegressionHead()],\n                        directory=tmp_path,\n                        max_trials=2,\n                        overwrite=False)\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_multi_io_with_tf_dataset(tuner_fn, tmp_path):\n    auto_model = get_multi_io_auto_model(tmp_path)\n    x1 = utils.generate_data()\n    y1 = utils.generate_data(shape=(1,))\n    dataset = tf.data.Dataset.from_tensor_slices(((x1, x1), (y1, y1)))\n    auto_model.fit(dataset, epochs=2)\n\n    for adapter in auto_model._input_adapters + auto_model._output_adapters:\n        assert adapter.shape is not None\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_single_nested_dataset(tuner_fn, tmp_path):\n    auto_model = ak.AutoModel(ak.ImageInput(),\n                              ak.RegressionHead(),\n                              directory=tmp_path,\n                              max_trials=2,\n                              overwrite=False)\n    x1 = utils.generate_data()\n    y1 = utils.generate_data(shape=(1,))\n    dataset = tf.data.Dataset.from_tensor_slices(((x1,), y1))\n    auto_model.fit(dataset, epochs=2)\n\n    for adapter in auto_model._input_adapters + auto_model._output_adapters:\n        assert adapter.shape is not None\n\n\ndef dataset_error(x, y, validation_data, message, tmp_path):\n    auto_model = get_multi_io_auto_model(tmp_path)\n    with pytest.raises(ValueError) as info:\n        auto_model.fit(x, y, epochs=2, validation_data=validation_data)\n    assert message in str(info.value)\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_data_io_consistency_input(tuner_fn, tmp_path):\n    x1 = utils.generate_data()\n    y1 = utils.generate_data(shape=(1,))\n    dataset = tf.data.Dataset.from_tensor_slices(((x1,), (y1, y1)))\n    dataset_error(dataset, None, dataset, 'Expect x to have', tmp_path)\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_data_io_consistency_output(tuner_fn, tmp_path):\n    x1 = utils.generate_data()\n    y1 = utils.generate_data(shape=(1,))\n    dataset = tf.data.Dataset.from_tensor_slices(((x1, x1), (y1,)))\n    dataset_error(dataset, None, dataset, 'Expect y to have', tmp_path)\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_data_io_consistency_validation(tuner_fn, tmp_path):\n    x1 = utils.generate_data()\n    y1 = utils.generate_data(shape=(1,))\n    dataset = tf.data.Dataset.from_tensor_slices(((x1, x1), (y1, y1)))\n    val_dataset = tf.data.Dataset.from_tensor_slices(((x1,), (y1, y1)))\n    dataset_error(dataset, None, val_dataset,\n                  'Expect x in validation_data to have', tmp_path)\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_dataset_and_y(tuner_fn, tmp_path):\n    x1 = utils.generate_data()\n    y1 = utils.generate_data(shape=(1,))\n    x = tf.data.Dataset.from_tensor_slices((x1, x1))\n    y = tf.data.Dataset.from_tensor_slices((y1, y1))\n    val_dataset = tf.data.Dataset.from_tensor_slices(((x1,), (y1, y1)))\n    dataset_error(x, y, val_dataset,\n                  'Expect y is None', tmp_path)\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_multi_input_predict(tuner_fn, tmp_path):\n    auto_model = get_multi_io_auto_model(tmp_path)\n    x1 = utils.generate_data()\n    y1 = utils.generate_data(shape=(1,))\n    dataset = tf.data.Dataset.from_tensor_slices(((x1, x1), (y1, y1)))\n    auto_model.fit(dataset, None, epochs=2, validation_data=dataset)\n\n    dataset2 = tf.data.Dataset.from_tensor_slices(((x1, x1),))\n    auto_model.predict(dataset2)\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_multi_input_predict2(tuner_fn, tmp_path):\n    auto_model = get_multi_io_auto_model(tmp_path)\n    x1 = utils.generate_data()\n    y1 = utils.generate_data(shape=(1,))\n    dataset = tf.data.Dataset.from_tensor_slices(((x1, x1), (y1, y1)))\n    auto_model.fit(dataset, None, epochs=2, validation_data=dataset)\n\n    dataset2 = tf.data.Dataset.from_tensor_slices((x1, x1))\n    auto_model.predict(dataset2)\n\n\n@mock.patch('autokeras.auto_model.get_tuner_class')\ndef test_single_input_predict(tuner_fn, tmp_path):\n    auto_model = get_single_io_auto_model(tmp_path)\n    x1 = utils.generate_data()\n    y1 = utils.generate_data(shape=(1,))\n    dataset = tf.data.Dataset.from_tensor_slices((x1, y1))\n    auto_model.fit(dataset, None, epochs=2, validation_data=dataset)\n\n    dataset2 = tf.data.Dataset.from_tensor_slices((x1, y1))\n    auto_model.predict(dataset2)\n"""
tests/autokeras/encoders_test.py,0,b''
tests/autokeras/graph_test.py,0,"b""import os\n\nimport kerastuner\nimport pytest\nfrom kerastuner.engine import hyperparameters as hp_module\n\nimport autokeras as ak\nfrom autokeras import graph as graph_module\n\n\ndef test_set_hp():\n    input_node = ak.Input((32,))\n    output_node = input_node\n    output_node = ak.DenseBlock()(output_node)\n    head = ak.RegressionHead()\n    head.output_shape = (1,)\n    output_node = head(output_node)\n\n    graph = graph_module.Graph(\n        inputs=input_node,\n        outputs=output_node,\n        override_hps=[hp_module.Choice('dense_block_1/num_layers', [6], default=6)])\n    hp = kerastuner.HyperParameters()\n    graph.build(hp)\n\n    for single_hp in hp.space:\n        if single_hp.name == 'dense_block_1/num_layers':\n            assert len(single_hp.values) == 1\n            assert single_hp.values[0] == 6\n            return\n    assert False\n\n\ndef test_input_output_disconnect():\n    input_node1 = ak.Input()\n    output_node = input_node1\n    _ = ak.DenseBlock()(output_node)\n\n    input_node = ak.Input()\n    output_node = input_node\n    output_node = ak.DenseBlock()(output_node)\n    output_node = ak.RegressionHead()(output_node)\n\n    with pytest.raises(ValueError) as info:\n        graph_module.Graph(inputs=input_node1, outputs=output_node)\n    assert 'Inputs and outputs not connected.' in str(info.value)\n\n\ndef test_hyper_graph_cycle():\n    input_node1 = ak.Input()\n    input_node2 = ak.Input()\n    output_node1 = ak.DenseBlock()(input_node1)\n    output_node2 = ak.DenseBlock()(input_node2)\n    output_node = ak.Merge()([output_node1, output_node2])\n    head = ak.RegressionHead()\n    output_node = head(output_node)\n    head.outputs = output_node1\n\n    with pytest.raises(ValueError) as info:\n        graph_module.Graph(inputs=[input_node1, input_node2],\n                           outputs=output_node)\n    assert 'The network has a cycle.' in str(info.value)\n\n\ndef test_input_missing():\n    input_node1 = ak.Input()\n    input_node2 = ak.Input()\n    output_node1 = ak.DenseBlock()(input_node1)\n    output_node2 = ak.DenseBlock()(input_node2)\n    output_node = ak.Merge()([output_node1, output_node2])\n    output_node = ak.RegressionHead()(output_node)\n\n    with pytest.raises(ValueError) as info:\n        graph_module.Graph(inputs=input_node1,\n                           outputs=output_node)\n    assert 'A required input is missing for HyperModel' in str(info.value)\n\n\ndef test_graph_basics():\n    input_node = ak.Input(shape=(30,))\n    output_node = input_node\n    output_node = ak.DenseBlock()(output_node)\n    output_node = ak.RegressionHead(output_shape=(1,))(output_node)\n\n    model = graph_module.Graph(\n        inputs=input_node,\n        outputs=output_node).build(kerastuner.HyperParameters())\n    assert model.input_shape == (None, 30)\n    assert model.output_shape == (None, 1)\n\n\ndef test_graph_save_load(tmp_path):\n    input1 = ak.Input()\n    input2 = ak.Input()\n    output1 = ak.DenseBlock()(input1)\n    output2 = ak.ConvBlock()(input2)\n    output = ak.Merge()([output1, output2])\n    output1 = ak.RegressionHead()(output)\n    output2 = ak.ClassificationHead()(output)\n\n    graph = graph_module.Graph(\n        inputs=[input1, input2],\n        outputs=[output1, output2],\n        override_hps=[hp_module.Choice('dense_block_1/num_layers', [6], default=6)])\n    config = graph.get_config()\n    graph = graph_module.Graph.from_config(config)\n\n    assert len(graph.inputs) == 2\n    assert len(graph.outputs) == 2\n    assert isinstance(graph.inputs[0].out_blocks[0], ak.DenseBlock)\n    assert isinstance(graph.inputs[1].out_blocks[0], ak.ConvBlock)\n    assert isinstance(graph.override_hps[0], hp_module.Choice)\n\n\ndef test_merge():\n    input_node1 = ak.Input(shape=(30,))\n    input_node2 = ak.Input(shape=(40,))\n    output_node1 = ak.DenseBlock()(input_node1)\n    output_node2 = ak.DenseBlock()(input_node2)\n    output_node = ak.Merge()([output_node1, output_node2])\n    output_node = ak.RegressionHead(output_shape=(1,))(output_node)\n\n    model = graph_module.Graph(\n        inputs=[input_node1, input_node2],\n        outputs=output_node).build(kerastuner.HyperParameters())\n    assert model.input_shape == [(None, 30), (None, 40)]\n    assert model.output_shape == (None, 1)\n\n\ndef test_save_custom_metrics_loss(tmp_path):\n    def custom_metric(y_pred, y_true):\n        return 1\n\n    def custom_loss(y_pred, y_true):\n        return y_pred - y_true\n\n    head = ak.ClassificationHead(\n        loss=custom_loss, metrics=['accuracy', custom_metric])\n    input_node = ak.Input()\n    output_node = head(input_node)\n    graph = graph_module.Graph(input_node, output_node)\n    path = os.path.join(tmp_path, 'graph')\n    graph.save(path)\n    new_graph = graph_module.load_graph(\n        path, custom_objects={\n            'custom_metric': custom_metric,\n            'custom_loss': custom_loss,\n        })\n    assert new_graph.blocks[0].metrics[1](0, 0) == 1\n    assert new_graph.blocks[0].loss(3, 2) == 1\n"""
tests/autokeras/keras_layers_test.py,14,"b""import os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom autokeras import keras_layers as layer_module\n\n\ndef get_data():\n    train = np.array([['a', 'ab', 2.1], ['b', 'bc', 1.0], ['a', 'bc', 'nan']])\n    test = np.array([['a', 'ab', 2.1], ['x', 'bc', 1.0], ['a', 'bc', 'nan']])\n    y = np.random.rand(3, 1)\n    return train, test, y\n\n\ndef test_multi_column_categorical_encoding(tmp_path):\n    x_train, x_test, y_train = get_data()\n    input_node = tf.keras.Input(shape=(3,), dtype=tf.string)\n    layer = layer_module.MultiColumnCategoricalEncoding([\n        layer_module.INT,\n        layer_module.INT,\n        layer_module.NONE,\n    ])\n    hidden_node = layer(input_node)\n    output_node = tf.keras.layers.Dense(1, activation='sigmoid')(hidden_node)\n    model = tf.keras.Model(input_node, output_node)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    tf.data.Dataset.zip((\n        (tf.data.Dataset.from_tensor_slices(x_train).batch(32),),\n        (tf.data.Dataset.from_tensor_slices(np.random.rand(3, 1)).batch(32),),\n    ))\n    layer.adapt(tf.data.Dataset.from_tensor_slices(x_train).batch(32))\n\n    model.fit(x_train, y_train, epochs=1)\n\n    model2 = tf.keras.Model(input_node, hidden_node)\n    result = model2.predict(x_train)\n    assert result[0][0] == result[2][0]\n    assert result[0][0] != result[1][0]\n    assert result[0][1] != result[1][1]\n    assert result[0][1] != result[2][1]\n    assert result[2][2] == 0\n\n    output = model2.predict(x_test)\n    assert output.dtype == np.dtype('float32')\n\n\ndef build_model():\n    input_node = tf.keras.Input(shape=(3,), dtype=tf.string)\n    layer = layer_module.MultiColumnCategoricalEncoding(encoding=[\n        layer_module.INT, layer_module.INT, layer_module.NONE])\n    output_node = layer(input_node)\n    output_node = tf.keras.layers.Dense(1)(output_node)\n    return tf.keras.Model(input_node, output_node), layer\n\n\ndef test_model_save(tmp_path):\n    x_train, x_test, y_train = get_data()\n    model, layer = build_model()\n    layer.adapt(tf.data.Dataset.from_tensor_slices(x_train).batch(32))\n    model.compile(optimizer='adam', loss='mse')\n    model.fit(x_train, y_train, epochs=1, verbose=False)\n\n    model.save(os.path.join(tmp_path, 'model'))\n    model2 = tf.keras.models.load_model(os.path.join(tmp_path, 'model'),\n                                        custom_objects=layer_module.CUSTOM_OBJECTS)\n\n    assert np.array_equal(model.predict(x_train), model2.predict(x_train))\n\n\ndef test_weight_save(tmp_path):\n    x_train, x_test, y_train = get_data()\n    model, layer = build_model()\n    layer.adapt(tf.data.Dataset.from_tensor_slices(x_train).batch(32))\n    model.compile(optimizer='adam', loss='mse')\n    model.fit(x_train, y_train, epochs=1, verbose=False)\n    model.save_weights(os.path.join(tmp_path, 'checkpoint'))\n\n    model2, _ = build_model()\n    model2.load_weights(os.path.join(tmp_path, 'checkpoint'))\n\n    assert np.array_equal(model.predict(x_train), model2.predict(x_train))\n"""
tests/autokeras/typed_api_test.py,0,"b'from typedapi import ensure_api_is_typed\n\nimport autokeras\n\nHELP_MESSAGE = (\n    ""You can also take a look at this issue:\\n""\n    ""https://github.com/keras-team/autokeras/issues/918""\n)\n\n\n# TODO: add types and remove all elements from\n# the exception list.\nEXCEPTION_LIST = [\n    autokeras.Head,\n    autokeras.Node,\n    autokeras.CategoricalToNumerical,\n    autokeras.Flatten,\n    autokeras.ImageAugmentation,\n    autokeras.ImageBlock,\n    autokeras.StructuredDataBlock,\n    autokeras.TextBlock,\n    autokeras.TextToNgramVector,\n    autokeras.ImageInput,\n    autokeras.Input,\n    autokeras.StructuredDataInput,\n    autokeras.TimeseriesInput,\n    autokeras.TextInput,\n    autokeras.ImageClassifier,\n    autokeras.ImageRegressor,\n    autokeras.StructuredDataClassifier,\n    autokeras.TextRegressor,\n    autokeras.TimeseriesForecaster,\n]\n\n\ndef test_api_surface_is_typed():\n    ensure_api_is_typed(\n        [autokeras], EXCEPTION_LIST, init_only=True, additional_message=HELP_MESSAGE,\n    )\n'"
tests/integration_tests/__init__.py,0,b''
tests/integration_tests/functional_api_test.py,0,"b""from tensorflow.python.keras.datasets import mnist\n\nimport autokeras as ak\nfrom tests import utils\n\n\ndef test_functional_api(tmp_path):\n    # Prepare the data.\n    num_instances = 80\n    (image_x, train_y), (test_x, test_y) = mnist.load_data()\n    (text_x, train_y), (test_x, test_y) = utils.imdb_raw()\n    (structured_data_x, train_y), (test_x, test_y) = utils.dataframe_numpy()\n\n    image_x = image_x[:num_instances]\n    text_x = text_x[:num_instances]\n    structured_data_x = structured_data_x[:num_instances]\n    classification_y = utils.generate_one_hot_labels(num_instances=num_instances,\n                                                     num_classes=3)\n    regression_y = utils.generate_data(num_instances=num_instances, shape=(1,))\n\n    # Build model and train.\n    image_input = ak.ImageInput()\n    output = ak.Normalization()(image_input)\n    output = ak.ImageAugmentation()(output)\n    outputs1 = ak.ResNetBlock(version='next')(output)\n    outputs2 = ak.XceptionBlock()(output)\n    image_output = ak.Merge()((outputs1, outputs2))\n\n    structured_data_input = ak.StructuredDataInput()\n    structured_data_output = ak.CategoricalToNumerical()(structured_data_input)\n    structured_data_output = ak.DenseBlock()(structured_data_output)\n\n    text_input = ak.TextInput()\n    outputs1 = ak.TextToIntSequence()(text_input)\n    outputs1 = ak.Embedding()(outputs1)\n    outputs1 = ak.ConvBlock(separable=True)(outputs1)\n    outputs1 = ak.SpatialReduction()(outputs1)\n    outputs2 = ak.TextToNgramVector()(text_input)\n    outputs2 = ak.DenseBlock()(outputs2)\n    text_output = ak.Merge()((\n        outputs1,\n        outputs2\n    ))\n\n    merged_outputs = ak.Merge()((\n        structured_data_output,\n        image_output,\n        text_output\n    ))\n\n    regression_outputs = ak.RegressionHead()(merged_outputs)\n    classification_outputs = ak.ClassificationHead()(merged_outputs)\n    automodel = ak.AutoModel(\n        inputs=[\n            image_input,\n            text_input,\n            structured_data_input\n        ],\n        directory=tmp_path,\n        outputs=[\n            regression_outputs,\n            classification_outputs\n        ],\n        max_trials=2,\n        seed=utils.SEED)\n\n    automodel.fit(\n        (\n            image_x,\n            text_x,\n            structured_data_x\n        ),\n        (\n            regression_y,\n            classification_y\n        ),\n        validation_split=0.2,\n        epochs=1)\n"""
tests/integration_tests/io_api_test.py,0,"b""from tensorflow.python.keras.datasets import mnist\n\nimport autokeras as ak\nfrom tests import utils\n\n\ndef test_io_api(tmp_path):\n    num_instances = 100\n    (image_x, train_y), (test_x, test_y) = mnist.load_data()\n    (text_x, train_y), (test_x, test_y) = utils.imdb_raw(\n        num_instances=num_instances)\n\n    image_x = image_x[:num_instances]\n    text_x = text_x[:num_instances]\n    structured_data_x = utils.generate_structured_data(num_instances=num_instances)\n    classification_y = utils.generate_one_hot_labels(num_instances=num_instances,\n                                                     num_classes=3)\n    regression_y = utils.generate_data(num_instances=num_instances, shape=(1,))\n\n    # Build model and train.\n    automodel = ak.AutoModel(\n        inputs=[\n            ak.ImageInput(),\n            ak.TextInput(),\n            ak.StructuredDataInput()\n        ],\n        outputs=[ak.RegressionHead(metrics=['mae']),\n                 ak.ClassificationHead(loss='categorical_crossentropy',\n                                       metrics=['accuracy'])],\n        directory=tmp_path,\n        max_trials=2,\n        seed=utils.SEED)\n    automodel.fit([\n        image_x,\n        text_x,\n        structured_data_x\n    ],\n        [regression_y, classification_y],\n        epochs=1,\n        validation_split=0.2)\n"""
tests/integration_tests/task_api_test.py,2,"b""import tensorflow as tf\n\nimport autokeras as ak\nfrom tests import utils\n\n\ndef test_image_classifier(tmp_path):\n    train_x = utils.generate_data(num_instances=320, shape=(32, 32, 3))\n    train_y = utils.generate_one_hot_labels(num_instances=320, num_classes=10)\n    clf = ak.ImageClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED)\n    clf.fit(train_x, train_y, epochs=1, validation_split=0.2)\n    keras_model = clf.export_model()\n    clf.evaluate(train_x, train_y)\n    assert clf.predict(train_x).shape == (len(train_x), 10)\n    assert isinstance(keras_model, tf.keras.Model)\n\n\ndef test_image_regressor(tmp_path):\n    train_x = utils.generate_data(num_instances=320, shape=(32, 32, 3))\n    train_y = utils.generate_data(num_instances=320, shape=(1,))\n    clf = ak.ImageRegressor(directory=tmp_path, max_trials=2, seed=utils.SEED)\n    clf.fit(train_x, train_y, epochs=1, validation_split=0.2)\n    clf.export_model()\n    assert clf.predict(train_x).shape == (len(train_x), 1)\n\n\ndef test_text_classifier(tmp_path):\n    (train_x, train_y), (test_x, test_y) = utils.imdb_raw()\n    clf = ak.TextClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED,\n                            metrics=['accuracy'], objective='accuracy')\n    clf.fit(train_x, train_y, epochs=2, validation_data=(test_x, test_y))\n    clf.export_model()\n    assert clf.predict(test_x).shape == (len(test_x), 1)\n    assert clf.tuner._get_best_trial_epochs() == 2\n\n\ndef test_text_regressor(tmp_path):\n    (train_x, train_y), (test_x, test_y) = utils.imdb_raw()\n    train_y = utils.generate_data(num_instances=train_y.shape[0], shape=(1,))\n    test_y = utils.generate_data(num_instances=test_y.shape[0], shape=(1,))\n    clf = ak.TextRegressor(directory=tmp_path, max_trials=2, seed=utils.SEED)\n    clf.fit(train_x, train_y, epochs=1, validation_data=(test_x, test_y))\n    clf.export_model()\n    assert clf.predict(test_x).shape == (len(test_x), 1)\n\n\ndef test_structured_data_from_numpy_regressor(tmp_path):\n    num_data = 500\n    num_train = 400\n    data = utils.generate_structured_data(num_data)\n    x_train, x_test = data[:num_train], data[num_train:]\n    y = utils.generate_data(num_instances=num_data, shape=(1,))\n    y_train, y_test = y[:num_train], y[num_train:]\n    clf = ak.StructuredDataRegressor(directory=tmp_path,\n                                     max_trials=2,\n                                     seed=utils.SEED)\n    clf.fit(x_train, y_train, epochs=11, validation_data=(x_train, y_train))\n    clf.export_model()\n    assert clf.predict(x_test).shape == (len(y_test), 1)\n\n\ndef test_structured_data_from_numpy_classifier(tmp_path):\n    num_data = 500\n    num_train = 400\n    data = utils.generate_structured_data(num_data)\n    x_train, x_test = data[:num_train], data[num_train:]\n    y = utils.generate_one_hot_labels(num_instances=num_data, num_classes=3)\n    y_train, y_test = y[:num_train], y[num_train:]\n    clf = ak.StructuredDataClassifier(directory=tmp_path,\n                                      max_trials=1,\n                                      seed=utils.SEED)\n    clf.fit(x_train, y_train, epochs=2, validation_data=(x_train, y_train))\n    clf.export_model()\n    assert clf.predict(x_test).shape == (len(y_test), 3)\n\n\ndef test_timeseries_forecaster(tmp_path):\n    lookback = 2\n    predict_from = 1\n    predict_until = 10\n    train_x = utils.generate_data(num_instances=100, shape=(32,))\n    train_y = utils.generate_data(num_instances=80, shape=(1,))\n    clf = ak.TimeseriesForecaster(lookback=lookback,\n                                  directory=tmp_path,\n                                  predict_from=predict_from,\n                                  predict_until=predict_until,\n                                  max_trials=2,\n                                  seed=utils.SEED)\n    clf.fit(train_x, train_y, epochs=1, validation_split=0.2)\n    keras_model = clf.export_model()\n    clf.evaluate(train_x, train_y)\n    assert clf.predict(train_x).shape == (predict_until - predict_from + 1, 1)\n    assert clf.fit_and_predict(train_x,\n                               train_y,\n                               epochs=1,\n                               validation_split=0.2).shape == (predict_until -\n                                                               predict_from + 1, 1)\n    assert isinstance(keras_model, tf.keras.Model)\n"""
tests/autokeras/adapters/__init__.py,0,b''
tests/autokeras/adapters/input_adapter_test.py,9,"b""import copy\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom autokeras.adapters import input_adapter\nfrom autokeras.utils import data_utils\nfrom tests import utils\n\n\ndef test_structured_data_input_col_type_without_name():\n    num_data = 500\n    train_x = utils.generate_structured_data(num_data)\n    with pytest.raises(ValueError) as info:\n        adapter = input_adapter.StructuredDataInputAdapter(\n            column_types=utils.COLUMN_TYPES_FROM_NUMPY)\n        adapter.transform(train_x)\n    assert str(info.value) == 'Column names must be specified.'\n\n\ndef test_structured_data_input_less_col_name():\n    (x, _), _1 = utils.dataframe_numpy()\n    with pytest.raises(ValueError) as info:\n        adapter = input_adapter.StructuredDataInputAdapter(\n            column_names=utils.LESS_COLUMN_NAMES_FROM_CSV)\n        adapter.fit_transform(x)\n    assert 'Expect column_names to have length' in str(info.value)\n\n\ndef test_structured_data_input_name_type_mismatch():\n    (x, _), _1 = utils.dataframe_dataframe()\n    column_types = copy.copy(utils.COLUMN_TYPES_FROM_CSV)\n    column_types['age_'] = column_types.pop('age')\n    with pytest.raises(ValueError) as info:\n        adapter = input_adapter.StructuredDataInputAdapter(\n            column_types=column_types)\n        adapter.transform(x)\n    assert 'Column_names and column_types are mismatched.' in str(info.value)\n\n\ndef test_structured_data_input_unsupported_type():\n    x = 'unknown'\n    with pytest.raises(TypeError) as info:\n        adapter = input_adapter.StructuredDataInputAdapter(\n            column_names=utils.COLUMN_TYPES_FROM_NUMPY,\n            column_types=utils.COLUMN_TYPES_FROM_NUMPY)\n        adapter.transform(x)\n    assert 'Unsupported type' in str(info.value)\n\n\ndef test_structured_data_input_transform():\n    (x, _), _1 = utils.dataframe_dataframe()\n    adapter = input_adapter.StructuredDataInputAdapter()\n    adapter.fit_transform(x)\n    assert adapter.column_names[0] == 'sex'\n    assert adapter.column_types == utils.COLUMN_TYPES_FROM_CSV\n\n\ndef test_structured_data_input_dataset():\n    (x, _), _1 = utils.dataframe_dataframe()\n    x = tf.data.Dataset.from_tensor_slices(x.to_numpy().astype(np.unicode))\n    adapter = input_adapter.StructuredDataInputAdapter()\n    x = adapter.fit_transform(x)\n    assert isinstance(x, tf.data.Dataset)\n\n\ndef test_partial_column_types():\n    adapter = input_adapter.StructuredDataInputAdapter(\n        column_names=utils.COLUMN_NAMES_FROM_CSV,\n        column_types=utils.PARTIAL_COLUMN_TYPES_FROM_CSV)\n    (x, y), (val_x, val_y) = utils.dataframe_numpy()\n    dataset = x.values.astype(np.unicode)\n    adapter.transform(dataset)\n    assert adapter.column_types['fare'] == 'categorical'\n\n\ndef test_image_input():\n    x = utils.generate_data()\n    adapter = input_adapter.ImageInputAdapter()\n    assert isinstance(adapter.transform(x), tf.data.Dataset)\n\n\ndef test_image_input_with_three_dim():\n    x = utils.generate_data(shape=(32, 32))\n    adapter = input_adapter.ImageInputAdapter()\n    x = adapter.transform(x)\n    assert isinstance(x, tf.data.Dataset)\n    for a in x:\n        assert a.shape[1:] == (32, 32, 1)\n        break\n\n\ndef test_image_input_with_illegal_dim():\n    x = utils.generate_data(shape=(32,))\n    adapter = input_adapter.ImageInputAdapter()\n    with pytest.raises(ValueError) as info:\n        x = adapter.transform(x)\n    assert 'Expect the data to ImageInput to have 3' in str(info.value)\n\n\ndef test_image_input_unsupported_type():\n    x = 'unknown'\n    adapter = input_adapter.ImageInputAdapter()\n    with pytest.raises(TypeError) as info:\n        x = adapter.transform(x)\n    assert 'Expect the data to ImageInput to be numpy' in str(info.value)\n\n\ndef test_image_input_numerical():\n    x = np.array([[['unknown']]])\n    adapter = input_adapter.ImageInputAdapter()\n    with pytest.raises(TypeError) as info:\n        x = adapter.transform(x)\n    assert 'Expect the data to ImageInput to be numerical' in str(info.value)\n\n\ndef test_input_type_error():\n    x = 'unknown'\n    adapter = input_adapter.InputAdapter()\n    with pytest.raises(TypeError) as info:\n        adapter.check(x)\n        x = adapter.transform(x)\n    assert 'Expect the data to Input to be numpy' in str(info.value)\n\n\ndef test_input_numerical():\n    x = np.array([[['unknown']]])\n    adapter = input_adapter.InputAdapter()\n    with pytest.raises(TypeError) as info:\n        adapter.check(x)\n        x = adapter.transform(x)\n    assert 'Expect the data to Input to be numerical' in str(info.value)\n\n\ndef test_text_dataset():\n    x = tf.data.Dataset.from_tensor_slices(np.array([\n        'a b c',\n        'b b c',\n    ]))\n    adapter = input_adapter.TextInputAdapter()\n    x = adapter.transform(x)\n    assert data_utils.dataset_shape(x).as_list() == [None, 1]\n    assert isinstance(x, tf.data.Dataset)\n\n\ndef test_text_dataset_batch():\n    x = tf.data.Dataset.from_tensor_slices(np.array([\n        'a b c',\n        'b b c',\n    ])).batch(32)\n    adapter = input_adapter.TextInputAdapter()\n    x = adapter.transform(x)\n    assert data_utils.dataset_shape(x).as_list() == [None, 1]\n    assert isinstance(x, tf.data.Dataset)\n\n\ndef test_text_np():\n    x = np.array([\n        'a b c',\n        'b b c',\n    ])\n    adapter = input_adapter.TextInputAdapter()\n    x = adapter.transform(x)\n    assert data_utils.dataset_shape(x).as_list() == [None, 1]\n    assert isinstance(x, tf.data.Dataset)\n\n\ndef test_text_input_type_error():\n    x = 'unknown'\n    adapter = input_adapter.TextInputAdapter()\n    with pytest.raises(TypeError) as info:\n        x = adapter.transform(x)\n    assert 'Expect the data to TextInput to be numpy' in str(info.value)\n\n\ndef test_text_input_with_illegal_dim():\n    x = utils.generate_data(shape=(32,))\n    adapter = input_adapter.TextInputAdapter()\n    with pytest.raises(ValueError) as info:\n        x = adapter.transform(x)\n    assert 'Expect the data to TextInput to have 1' in str(info.value)\n\n\ndef test_text_string():\n    x = np.array([1, 2])\n    adapter = input_adapter.TextInputAdapter()\n    with pytest.raises(TypeError) as info:\n        x = adapter.transform(x)\n    assert 'Expect the data to TextInput to be strings' in str(info.value)\n\n\ndef test_time_series_input_type_error():\n    x = 'unknown'\n    adapter = input_adapter.TimeseriesInputAdapter(2)\n    with pytest.raises(TypeError) as info:\n        x = adapter.transform(x)\n    assert 'Expect the data in TimeseriesInput to be numpy' in str(info.value)\n\n\ndef test_time_series_input_with_illegal_dim():\n    x = utils.generate_data(shape=(32, 32))\n    adapter = input_adapter.TimeseriesInputAdapter(2)\n    with pytest.raises(ValueError) as info:\n        x = adapter.transform(x)\n    assert 'Expect the data in TimeseriesInput to have 2' in str(info.value)\n\n\ndef test_time_series_input_col_type_without_name():\n    num_data = 500\n    train_x = utils.generate_structured_data(num_data)\n    with pytest.raises(ValueError) as info:\n        adapter = input_adapter.TimeseriesInputAdapter(\n            lookback=2,\n            column_types=utils.COLUMN_TYPES_FROM_NUMPY)\n        adapter.transform(train_x)\n    assert str(info.value) == 'Column names must be specified.'\n\n\ndef test_time_series_input_less_col_name():\n    (x, _), _1 = utils.dataframe_numpy()\n    with pytest.raises(ValueError) as info:\n        adapter = input_adapter.TimeseriesInputAdapter(\n            lookback=2,\n            column_names=utils.LESS_COLUMN_NAMES_FROM_CSV)\n        adapter.transform(x)\n    assert 'Expect column_names to have length' in str(info.value)\n\n\ndef test_time_series_input_name_type_mismatch():\n    (x, _), _1 = utils.dataframe_dataframe()\n    column_types = copy.copy(utils.COLUMN_TYPES_FROM_CSV)\n    column_types['age_'] = column_types.pop('age')\n    with pytest.raises(ValueError) as info:\n        adapter = input_adapter.TimeseriesInputAdapter(\n            lookback=2,\n            column_types=column_types)\n        adapter.transform(x)\n    assert 'Column_names and column_types are mismatched.' in str(info.value)\n\n\ndef test_time_series_input_transform():\n    x = utils.generate_data(shape=(32,))\n    adapter = input_adapter.TimeseriesInputAdapter(2)\n    x = adapter.transform(x)\n    for row in x.as_numpy_iterator():\n        assert row.ndim == 3\n"""
tests/autokeras/adapters/output_adapter_test.py,5,"b""import numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom autokeras.adapters import output_adapter\nfrom tests import utils\n\n\ndef test_y_is_pd_series():\n    (x, y), (val_x, val_y) = utils.dataframe_series()\n    adapter = output_adapter.ClassificationHeadAdapter(name='a')\n    adapter.fit_transform(y)\n    assert isinstance(adapter.transform(y), tf.data.Dataset)\n\n\ndef test_unsupported_types():\n    y = 1\n    adapter = output_adapter.ClassificationHeadAdapter(name='a')\n    with pytest.raises(TypeError) as info:\n        adapter.check(y)\n    assert 'Expect the target data' in str(info.value)\n\n\ndef test_one_class():\n    y = np.array(['a', 'a', 'a'])\n    adapter = output_adapter.ClassificationHeadAdapter(name='a')\n    with pytest.raises(ValueError) as info:\n        adapter.fit_before_convert(y)\n    assert 'Expect the target data' in str(info.value)\n\n\ndef test_infer_num_classes():\n    y = utils.generate_one_hot_labels(dtype='dataset')\n    adapter = output_adapter.ClassificationHeadAdapter(name='a')\n    y = adapter.fit(y)\n    assert adapter.num_classes == 10\n\n\ndef test_infer_two_classes():\n    y = tf.data.Dataset.from_tensor_slices(np.random.rand(10, 1)).batch(32)\n    adapter = output_adapter.ClassificationHeadAdapter(name='a')\n    y = adapter.fit(y)\n    assert adapter.num_classes == 2\n\n\ndef test_check_data_shape():\n    y = tf.data.Dataset.from_tensor_slices(np.random.rand(10, 5)).batch(32)\n    adapter = output_adapter.ClassificationHeadAdapter(name='a', num_classes=5)\n    adapter.fit(y)\n\n\ndef test_check_data_shape_two_classes():\n    y = tf.data.Dataset.from_tensor_slices(np.random.rand(10, 1)).batch(32)\n    adapter = output_adapter.ClassificationHeadAdapter(name='a', num_classes=2)\n    adapter.fit(y)\n\n\ndef test_check_data_shape_error():\n    y = tf.data.Dataset.from_tensor_slices(np.random.rand(10, 3)).batch(32)\n    adapter = output_adapter.ClassificationHeadAdapter(name='a', num_classes=5)\n    with pytest.raises(ValueError) as info:\n        adapter.fit(y)\n    assert 'Expect the target data for a to have shape' in str(info.value)\n\n\ndef test_multi_label_two_classes():\n    y = np.random.rand(10, 2)\n    adapter = output_adapter.ClassificationHeadAdapter(name='a', multi_label=True)\n    adapter.fit_transform(y)\n    assert adapter.label_encoder is None\n\n\ndef test_multi_label_postprocessing():\n    y = np.random.rand(10, 3)\n    adapter = output_adapter.ClassificationHeadAdapter(name='a', multi_label=True)\n    adapter.fit_transform(y)\n    y = adapter.postprocess(y)\n    assert set(y.flatten().tolist()) == set([1, 0])\n"""
tests/autokeras/blocks/__init__.py,0,b''
tests/autokeras/blocks/basic_test.py,6,"b'import pytest\nimport tensorflow as tf\n\nfrom autokeras.blocks import basic\nfrom tests.autokeras.blocks import utils\n\n\ndef test_type_error_for_call():\n    block = basic.ConvBlock()\n    with pytest.raises(TypeError) as info:\n        block(block)\n    assert \'Expect the inputs to layer\' in str(info.value)\n\n\ndef test_resnet_block():\n    utils.block_basic_exam(\n        basic.ResNetBlock(),\n        tf.keras.Input(shape=(32, 32, 3), dtype=tf.float32),\n        [\'version\', \'pooling\'],\n    )\n\n\ndef test_resnet_invalid_kwargs():\n    with pytest.raises(ValueError) as info:\n        basic.ResNetBlock(include_top=True)\n    assert \'Argument ""include_top"" is not\' in str(info.value)\n\n\ndef test_resnet_invalid_kwargs2():\n    with pytest.raises(ValueError) as info:\n        basic.ResNetBlock(input_shape=(10,))\n    assert \'Argument ""input_shape"" is not\' in str(info.value)\n\n\ndef test_xception_block():\n    utils.block_basic_exam(\n        basic.XceptionBlock(),\n        tf.keras.Input(shape=(32, 32, 3), dtype=tf.float32),\n        [\n            \'activation\',\n            \'initial_strides\',\n            \'num_residual_blocks\',\n            \'pooling\',\n        ])\n\n\ndef test_xception_invalid_kwargs():\n    with pytest.raises(ValueError) as info:\n        basic.XceptionBlock(include_top=True)\n    assert \'Argument ""include_top"" is not\' in str(info.value)\n\n\ndef test_xception_invalid_kwargs2():\n    with pytest.raises(ValueError) as info:\n        basic.XceptionBlock(input_shape=(10,))\n    assert \'Argument ""input_shape"" is not\' in str(info.value)\n\n\ndef test_conv_block():\n    utils.block_basic_exam(\n        basic.ConvBlock(),\n        tf.keras.Input(shape=(32, 32, 3), dtype=tf.float32),\n        [\n            \'kernel_size\',\n            \'num_blocks\',\n            \'separable\',\n        ])\n\n\ndef test_rnn_block():\n    utils.block_basic_exam(\n        basic.RNNBlock(),\n        tf.keras.Input(shape=(32, 10), dtype=tf.float32),\n        [\n            \'bidirectional\',\n            \'layer_type\',\n            \'num_layers\',\n        ])\n\n\ndef test_dense_block():\n    utils.block_basic_exam(\n        basic.DenseBlock(),\n        tf.keras.Input(shape=(32,), dtype=tf.float32),\n        [\n            \'num_layers\',\n            \'use_batchnorm\',\n        ])\n\n\ndef test_embedding_block():\n    utils.block_basic_exam(\n        basic.Embedding(),\n        tf.keras.Input(shape=(32,), dtype=tf.float32),\n        [\n            \'pretraining\',\n            \'embedding_dim\',\n        ])\n'"
tests/autokeras/blocks/heads_test.py,2,"b""import kerastuner\nimport numpy as np\nimport tensorflow as tf\n\nimport autokeras as ak\nfrom autokeras import blocks\nfrom autokeras import nodes as input_module\nfrom autokeras.blocks import heads as head_module\n\n\ndef test_two_classes():\n    y = np.array(['a', 'a', 'a', 'b'])\n    head = head_module.ClassificationHead(name='a')\n    adapter = head.get_adapter()\n    adapter.fit_transform(y)\n    head.config_from_adapter(adapter)\n    head.output_shape = (1,)\n    head.build(kerastuner.HyperParameters(), input_module.Input(shape=(32,)).build())\n    assert head.loss.name == 'binary_crossentropy'\n\n\ndef test_three_classes():\n    y = np.array(['a', 'a', 'c', 'b'])\n    head = head_module.ClassificationHead(name='a')\n    adapter = head.get_adapter()\n    adapter.fit_transform(y)\n    head.config_from_adapter(adapter)\n    assert head.loss.name == 'categorical_crossentropy'\n\n\ndef test_multi_label_loss():\n    head = head_module.ClassificationHead(name='a', multi_label=True, num_classes=8)\n    head.output_shape = (8,)\n    input_node = tf.keras.Input(shape=(5,))\n    output_node = head.build(kerastuner.HyperParameters(), input_node)\n    model = tf.keras.Model(input_node, output_node)\n    assert model.layers[-1].activation.__name__ == 'sigmoid'\n    assert head.loss.name == 'binary_crossentropy'\n\n\ndef test_segmentation():\n    y = np.array(['a', 'a', 'c', 'b'])\n    head = head_module.SegmentationHead(name='a')\n    adapter = head.get_adapter()\n    adapter.fit_transform(y)\n    head.config_from_adapter(adapter)\n    input_shape = (64, 64, 21)\n    hp = kerastuner.HyperParameters()\n    head = blocks.deserialize(blocks.serialize(head))\n    head.build(hp, ak.Input(shape=input_shape).build())\n"""
tests/autokeras/blocks/preprocessing_test.py,4,"b""import tensorflow as tf\n\nfrom autokeras.blocks import preprocessing\nfrom tests.autokeras.blocks import utils\n\n\ndef test_image_augmentation():\n    utils.block_basic_exam(\n        preprocessing.ImageAugmentation(),\n        tf.keras.Input(shape=(32, 32, 3), dtype=tf.float32),\n        ['vertical_flip', 'horizontal_flip'],\n    )\n\n\ndef test_text_to_ngram_vector():\n    utils.block_basic_exam(\n        preprocessing.TextToNgramVector(),\n        tf.keras.Input(shape=(1,), dtype=tf.string),\n        ['ngrams'],\n    )\n\n\ndef test_text_to_int_sequence():\n    utils.block_basic_exam(\n        preprocessing.TextToIntSequence(),\n        tf.keras.Input(shape=(1,), dtype=tf.string),\n        ['output_sequence_length'],\n    )\n\n\ndef test_categorical_to_numerical():\n    block = preprocessing.CategoricalToNumerical()\n    block.column_names = ['a']\n    block.column_types = {'a': 'num'}\n    utils.block_basic_exam(\n        block,\n        tf.keras.Input(shape=(1,), dtype=tf.string),\n        [],\n    )\n"""
tests/autokeras/blocks/reduction_test.py,4,"b""import tensorflow as tf\n\nfrom autokeras.blocks import reduction\nfrom tests.autokeras.blocks import utils\n\n\ndef test_merge():\n    utils.block_basic_exam(\n        reduction.Merge(),\n        [\n            tf.keras.Input(shape=(32,), dtype=tf.float32),\n            tf.keras.Input(shape=(4, 8), dtype=tf.float32),\n        ],\n        ['merge_type'],\n    )\n\n\ndef test_temporal_reduction():\n    utils.block_basic_exam(\n        reduction.TemporalReduction(),\n        tf.keras.Input(shape=(32, 10), dtype=tf.float32),\n        ['reduction_type'],\n    )\n\n\ndef test_spatial_reduction():\n    utils.block_basic_exam(\n        reduction.SpatialReduction(),\n        tf.keras.Input(shape=(32, 32, 3), dtype=tf.float32),\n        ['reduction_type'],\n    )\n"""
tests/autokeras/blocks/utils.py,0,"b""import kerastuner\n\nfrom autokeras import blocks\nfrom tests import utils\n\n\ndef name_in_hps(hp_name, hp):\n    return any([hp_name in name for name in hp.values])\n\n\ndef block_basic_exam(block, inputs, hp_names):\n    hp = kerastuner.HyperParameters()\n    block = blocks.deserialize(blocks.serialize(block))\n    utils.config_tests(block, excluded_keys=[\n        'inputs',\n        'outputs',\n        'build',\n        '_build',\n        'input_tensor',\n        'input_shape',\n        'include_top',\n        '_num_output_node'])\n    outputs = block.build(hp, inputs)\n\n    for hp_name in hp_names:\n        assert name_in_hps(hp_name, hp)\n\n    return outputs\n"""
tests/autokeras/blocks/wrapper_test.py,6,"b""import tensorflow as tf\nfrom tensorflow.python.util import nest\n\nfrom autokeras import adapters\nfrom autokeras.blocks import wrapper\nfrom tests.autokeras.blocks import utils\n\n\ndef test_image_block():\n    utils.block_basic_exam(\n        wrapper.ImageBlock(normalize=None, augment=None),\n        tf.keras.Input(shape=(32, 32, 3), dtype=tf.float32),\n        [\n            'block_type',\n            'normalize',\n            'augment',\n        ],\n    )\n\n\ndef test_text_block():\n    utils.block_basic_exam(\n        wrapper.TextBlock(),\n        tf.keras.Input(shape=(1,), dtype=tf.string),\n        ['vectorizer'],\n    )\n\n\ndef test_structured_data_block():\n    block = wrapper.StructuredDataBlock()\n    block.column_names = ['0', '1']\n    block.column_types = {\n        '0': adapters.NUMERICAL,\n        '1': adapters.NUMERICAL,\n    }\n    outputs = utils.block_basic_exam(\n        block,\n        tf.keras.Input(shape=(2,), dtype=tf.string),\n        [],\n    )\n    assert isinstance(nest.flatten(outputs)[0], tf.Tensor)\n\n\ndef test_timeseries_block():\n    block = wrapper.TimeseriesBlock()\n    block.column_names = ['0', '1']\n    block.column_types = {\n        '0': adapters.NUMERICAL,\n        '1': adapters.NUMERICAL,\n    }\n    outputs = utils.block_basic_exam(\n        block,\n        tf.keras.Input(shape=(32, 2), dtype=tf.float32),\n        [],\n    )\n    assert isinstance(nest.flatten(outputs)[0], tf.Tensor)\n"""
tests/autokeras/engine/__init__.py,0,b''
tests/autokeras/engine/block_test.py,0,b''
tests/autokeras/engine/node_test.py,2,"b'import tensorflow as tf\n\nimport autokeras as ak\nfrom autokeras import nodes\n\n\ndef test_time_series_input_node():\n    # TODO. Change test once TimeSeriesBlock is added.\n    node = ak.TimeseriesInput(shape=(32,), lookback=2)\n    output = node.build()\n    assert isinstance(output, tf.Tensor)\n\n    node = nodes.deserialize(nodes.serialize(node))\n    output = node.build()\n    assert isinstance(output, tf.Tensor)\n'"
tests/autokeras/engine/tuner_test.py,7,"b""from unittest import mock\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nfrom autokeras.engine import tuner as tuner_module\nfrom autokeras.tuners import greedy\nfrom tests import utils\n\n\n@mock.patch('kerastuner.engine.base_tuner.BaseTuner.search')\n@mock.patch('autokeras.engine.tuner.AutoTuner.final_fit')\ndef test_add_early_stopping(fit_fn, base_tuner_search, tmp_path):\n    graph = utils.build_graph()\n    tuner = tuner_module.AutoTuner(\n        oracle=greedy.GreedyOracle(graph, objective='val_loss'),\n        hypermodel=graph,\n        directory=tmp_path)\n\n    tuner.search(x=None, epochs=10)\n\n    callbacks = base_tuner_search.call_args_list[0][1]['callbacks']\n    assert any([isinstance(callback, tf.keras.callbacks.EarlyStopping)\n                for callback in callbacks])\n\n\n@mock.patch('kerastuner.engine.base_tuner.BaseTuner.search')\n@mock.patch('autokeras.engine.tuner.AutoTuner.final_fit')\ndef test_overwrite_search(fit_fn, base_tuner_search, tmp_path):\n    graph = utils.build_graph()\n    tuner = tuner_module.AutoTuner(\n        oracle=greedy.GreedyOracle(graph, objective='val_loss'),\n        hypermodel=graph,\n        directory=tmp_path)\n\n    tuner.search(epochs=10)\n\n    assert tuner._finished\n\n\n@mock.patch('kerastuner.engine.base_tuner.BaseTuner.search')\n@mock.patch('autokeras.engine.tuner.AutoTuner.final_fit')\n@mock.patch('autokeras.engine.tuner.AutoTuner._get_best_trial_epochs')\ndef test_no_epochs(best_epochs, fit_fn, base_tuner_search, tmp_path):\n    best_epochs.return_value = 2\n    graph = utils.build_graph()\n    tuner = tuner_module.AutoTuner(\n        oracle=greedy.GreedyOracle(graph, objective='val_loss'),\n        hypermodel=graph,\n        directory=tmp_path)\n\n    tuner.search(x=mock.Mock(), epochs=None, fit_on_val_data=True,\n                 validation_data=mock.Mock())\n\n    callbacks = fit_fn.call_args_list[0][1]['callbacks']\n    assert not any([isinstance(callback, tf.keras.callbacks.EarlyStopping)\n                    for callback in callbacks])\n\n\ndef test_preprocessing_adapt():\n    class MockLayer(preprocessing.TextVectorization):\n        def adapt(self, *args, **kwargs):\n            super().adapt(*args, **kwargs)\n            self.is_called = True\n\n    (x_train, y_train), (x_test, y_test) = utils.imdb_raw()\n    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n    layer1 = MockLayer(\n        max_tokens=5000,\n        output_mode='int',\n        output_sequence_length=40)\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n    model.add(layer1)\n    model.add(tf.keras.layers.Embedding(50001, 10))\n    model.add(tf.keras.layers.Dense(1))\n\n    tuner_module.AutoTuner.adapt(model, dataset)\n\n    assert layer1.is_called\n"""
tests/autokeras/tasks/__init__.py,0,b''
tests/autokeras/tasks/image_test.py,0,"b""from unittest import mock\n\nfrom autokeras.tasks import image\nfrom tests import utils\n\n\n@mock.patch('autokeras.auto_model.AutoModel.__init__')\ndef test_image_classifier(auto_model, tmp_path):\n    image.ImageClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED)\n    assert auto_model.called\n\n\n@mock.patch('autokeras.auto_model.AutoModel.__init__')\ndef test_image_regressor(auto_model, tmp_path):\n    image.ImageRegressor(directory=tmp_path, max_trials=2, seed=utils.SEED)\n    assert auto_model.called\n\n\n@mock.patch('autokeras.auto_model.AutoModel.__init__')\ndef test_image_segmenter(auto_model, tmp_path):\n    image.ImageSegmenter(directory=tmp_path, max_trials=2, seed=utils.SEED)\n    assert auto_model.called\n"""
tests/autokeras/tasks/structure_data_test.py,0,"b'from unittest import mock\n\nimport numpy as np\nimport pandas\nimport pytest\n\nfrom autokeras.tasks import structured_data\nfrom tests import utils\n\n\ndef test_structured_data_unknown_str_in_col_type(tmp_path):\n    with pytest.raises(ValueError) as info:\n        structured_data.StructuredDataClassifier(\n            column_types=utils.FALSE_COLUMN_TYPES_FROM_CSV,\n            directory=tmp_path,\n            max_trials=1,\n            seed=utils.SEED)\n    assert \'Column_types should be either ""categorical""\' in str(info.value)\n\n\ndef test_structured_data_col_name_type_mismatch(tmp_path):\n    with pytest.raises(ValueError) as info:\n        structured_data.StructuredDataClassifier(\n            column_names=utils.COLUMN_NAMES_FROM_NUMPY,\n            column_types=utils.COLUMN_TYPES_FROM_CSV,\n            directory=tmp_path,\n            max_trials=1,\n            seed=utils.SEED)\n    assert \'Column_names and column_types are mismatched.\' in str(info.value)\n\n\n@mock.patch(\'autokeras.auto_model.AutoModel.fit\')\n@mock.patch(\'autokeras.auto_model.AutoModel.__init__\')\ndef test_structured_classifier(init, fit, tmp_path):\n    num_data = 500\n    train_x = utils.generate_structured_data(num_data)\n    train_y = utils.generate_one_hot_labels(num_instances=num_data, num_classes=3)\n\n    clf = structured_data.StructuredDataClassifier(\n        column_names=utils.COLUMN_NAMES_FROM_NUMPY,\n        directory=tmp_path,\n        max_trials=1,\n        seed=utils.SEED)\n    clf.fit(train_x, train_y, epochs=2, validation_data=(train_x, train_y))\n\n    assert init.called\n    assert fit.called\n\n\n@mock.patch(\'autokeras.auto_model.AutoModel.fit\')\n@mock.patch(\'autokeras.auto_model.AutoModel.__init__\')\ndef test_structured_regressor(init, fit, tmp_path):\n    num_data = 500\n    train_x = utils.generate_structured_data(num_data)\n    train_y = utils.generate_data(num_instances=100, shape=(1,))\n\n    clf = structured_data.StructuredDataRegressor(\n        column_names=utils.COLUMN_NAMES_FROM_NUMPY,\n        directory=tmp_path,\n        max_trials=1,\n        seed=utils.SEED)\n    clf.fit(train_x, train_y, epochs=2, validation_data=(train_x, train_y))\n\n    assert init.called\n    assert fit.called\n\n\n@mock.patch(\'autokeras.auto_model.AutoModel.fit\')\n@mock.patch(\'autokeras.auto_model.AutoModel.__init__\')\ndef test_structured_data_classifier_from_csv(init, fit, tmp_path):\n    clf = structured_data.StructuredDataClassifier(\n        directory=tmp_path,\n        max_trials=1,\n        seed=utils.SEED)\n\n    clf.fit(x=utils.TRAIN_FILE_PATH, y=\'survived\', epochs=2,\n            validation_data=(utils.TEST_FILE_PATH, \'survived\'))\n\n    assert init.called\n    _, kwargs = fit.call_args_list[0]\n    assert isinstance(kwargs[\'x\'], pandas.DataFrame)\n    assert isinstance(kwargs[\'y\'], np.ndarray)\n'"
tests/autokeras/tasks/text_test.py,0,"b""from unittest import mock\n\nfrom autokeras.tasks import text\nfrom tests import utils\n\n\n@mock.patch('autokeras.auto_model.AutoModel.__init__')\ndef test_text_classifier(auto_model, tmp_path):\n    text.TextClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED)\n    assert auto_model.called\n\n\n@mock.patch('autokeras.auto_model.AutoModel.__init__')\ndef test_text_regressor(auto_model, tmp_path):\n    text.TextRegressor(directory=tmp_path, max_trials=2, seed=utils.SEED)\n    assert auto_model.called\n"""
tests/autokeras/tuners/__init__.py,0,b''
tests/autokeras/tuners/greedy_test.py,0,"b""from unittest import mock\n\nimport kerastuner\n\nfrom autokeras.tuners import greedy\nfrom tests import utils\n\n\ndef test_random_oracle_state():\n    graph = utils.build_graph()\n    oracle = greedy.GreedyOracle(\n        hypermodel=graph,\n        objective='val_loss',\n    )\n    oracle.hypermodel = graph\n    oracle.set_state(oracle.get_state())\n    assert oracle.hypermodel is graph\n\n\n@mock.patch('autokeras.tuners.greedy.GreedyOracle.get_best_trials')\ndef test_random_oracle(fn):\n    graph = utils.build_graph()\n    oracle = greedy.GreedyOracle(\n        hypermodel=graph,\n        objective='val_loss',\n    )\n    oracle.hypermodel = graph\n    trial = mock.Mock()\n    hp = kerastuner.HyperParameters()\n    trial.hyperparameters = hp\n    fn.return_value = [trial]\n\n    oracle.update_space(hp)\n    for i in range(2000):\n        oracle._populate_space(str(i))\n\n    assert 'optimizer' in oracle._hp_names[greedy.GreedyOracle.OPT]\n    assert 'classification_head_1/dropout_rate' in oracle._hp_names[\n        greedy.GreedyOracle.ARCH]\n    assert 'image_block_1/block_type' in oracle._hp_names[\n        greedy.GreedyOracle.HYPER]\n"""
tests/autokeras/tuners/hyperband_test.py,0,b''
tests/autokeras/tuners/random_search_test.py,0,b''
tests/autokeras/tuners/task_specific_test.py,4,"b""import copy\n\nimport kerastuner\nimport tensorflow as tf\n\nimport autokeras as ak\nfrom autokeras import graph as graph_module\nfrom autokeras.tuners import greedy\nfrom autokeras.tuners import task_specific\n\n\ndef check_initial_hp(initial_hp, graph):\n    hp = kerastuner.HyperParameters()\n    for i in range(3):\n        hp.values = copy.copy(initial_hp)\n        graph.build(hp)\n    assert len(set(initial_hp.keys()) - set(hp._hps.keys())) == 0\n\n\ndef test_image_classifier_tuner0():\n    tf.keras.backend.clear_session()\n    input_node = ak.ImageInput(shape=(32, 32, 3))\n    output_node = ak.ImageBlock()(input_node)\n    output_node = ak.ClassificationHead(\n        loss='categorical_crossentropy',\n        output_shape=(10,))(output_node)\n    graph = graph_module.Graph(input_node, output_node)\n    check_initial_hp(task_specific.IMAGE_CLASSIFIER[0], graph)\n\n\ndef test_image_classifier_tuner1():\n    tf.keras.backend.clear_session()\n    input_node = ak.ImageInput(shape=(32, 32, 3))\n    output_node = ak.ImageBlock()(input_node)\n    output_node = ak.ClassificationHead(\n        loss='categorical_crossentropy',\n        output_shape=(10,))(output_node)\n    graph = graph_module.Graph(input_node, output_node)\n    check_initial_hp(task_specific.IMAGE_CLASSIFIER[1], graph)\n\n\ndef test_text_classifier_tuner0():\n    tf.keras.backend.clear_session()\n    input_node = ak.TextInput(shape=(1,))\n    output_node = ak.TextBlock()(input_node)\n    output_node = ak.ClassificationHead(\n        loss='categorical_crossentropy',\n        output_shape=(10,))(output_node)\n    graph = graph_module.Graph(input_node, output_node)\n    check_initial_hp(task_specific.TEXT_CLASSIFIER[0], graph)\n\n\ndef test_image_classifier_oracle():\n    tf.keras.backend.clear_session()\n    input_node = ak.ImageInput(shape=(32, 32, 3))\n    output_node = ak.ImageBlock()(input_node)\n    output_node = ak.ClassificationHead(\n        loss='categorical_crossentropy',\n        output_shape=(10,))(output_node)\n    graph = graph_module.Graph(input_node, output_node)\n    oracle = greedy.GreedyOracle(\n        hypermodel=graph,\n        initial_hps=task_specific.IMAGE_CLASSIFIER,\n        objective='val_loss')\n    oracle._populate_space('0')\n    hp = oracle.get_space()\n    hp.values = task_specific.IMAGE_CLASSIFIER[0]\n    assert len(set(\n        task_specific.IMAGE_CLASSIFIER[0].keys()\n    ) - set(\n        oracle.get_space().values.keys())) == 0\n    oracle._populate_space('1')\n    assert len(set(\n        task_specific.IMAGE_CLASSIFIER[1].keys()\n    ) - set(\n        oracle.get_space().values.keys())) == 0\n"""
tests/autokeras/utils/__init__.py,0,b''
tests/autokeras/utils/utils_test.py,0,b''
