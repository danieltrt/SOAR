file_path,api_count,code
convert_to_tf_keras.py,4,"b'import os\r\nimport sys\r\n\r\nlist_conversions = [(\'import keras.\', \'import tensorflow.keras.\'),\r\n                    (\'import keras \', \'from tensorflow import keras \'),\r\n                    (\'import keras\\n\', \'from tensorflow import keras\\n\'),\r\n                    (\'from keras.\', \'from tensorflow.keras.\'),\r\n                    (\'from keras \', \'from tensorflow.keras \')]\r\n\r\n\r\ndef replace_imports_in_text(string, revert):\r\n    if revert:\r\n        list_imports_to_change = [x[::-1] for x in list_conversions]\r\n    else:\r\n        list_imports_to_change = list_conversions\r\n\r\n    text_updated = string\r\n    for old_str, new_str in list_imports_to_change:\r\n        text_updated = text_updated.replace(old_str, new_str)\r\n    return text_updated\r\n\r\n\r\ndef replace_imports_in_file(file_path, revert):\r\n    if not file_path.endswith(\'.py\'):\r\n        return False\r\n    if os.path.abspath(file_path) == os.path.abspath(__file__):\r\n        return False\r\n    with open(file_path, \'r\') as f:\r\n        text = f.read()\r\n\r\n    text_updated = replace_imports_in_text(text, revert)\r\n\r\n    with open(file_path, \'w+\') as f:\r\n        f.write(text_updated)\r\n\r\n    return text_updated != text\r\n\r\n\r\ndef convert_codebase(revert):\r\n    nb_of_files_changed = 0\r\n    keras_dir = os.path.dirname(os.path.abspath(__file__))\r\n    for root, dirs, files in os.walk(keras_dir):\r\n        for name in files:\r\n            if replace_imports_in_file(os.path.join(root, name), revert):\r\n                nb_of_files_changed += 1\r\n    print(\'Changed imports in \' + str(nb_of_files_changed) + \' files.\')\r\n    print(\'Those files were found in the directory \' + keras_dir)\r\n\r\n\r\ndef convert_to_tf_keras():\r\n    """"""Convert the codebase to tf.keras""""""\r\n    convert_codebase(False)\r\n\r\n\r\ndef convert_to_keras_team_keras():\r\n    """"""Convert the codebase from tf.keras to keras-team/keras""""""\r\n    convert_codebase(True)\r\n\r\n\r\ndef test_replace_imports():\r\n    python_code = """"""\r\n    import keras\r\n    from keras import backend as K\r\n    import os\r\n    import keras_contrib\r\n    import keras_contrib.layers as lay\r\n    import keras.layers\r\n    from keras.layers import Dense\r\n\r\n    if K.backend() == \'tensorflow\':\r\n        import tensorflow as tf\r\n        function = tf.max\r\n    """"""\r\n\r\n    expected_code = """"""\r\n    from tensorflow import keras\r\n    from tensorflow.keras import backend as K\r\n    import os\r\n    import keras_contrib\r\n    import keras_contrib.layers as lay\r\n    import tensorflow.keras.layers\r\n    from tensorflow.keras.layers import Dense\r\n\r\n    if K.backend() == \'tensorflow\':\r\n        import tensorflow as tf\r\n        function = tf.max\r\n    """"""\r\n\r\n    code_with_replacement = replace_imports_in_text(python_code, False)\r\n    assert expected_code == code_with_replacement\r\n    assert python_code == replace_imports_in_text(code_with_replacement, True)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    if \'--revert\' in sys.argv:\r\n        convert_to_keras_team_keras()\r\n    else:\r\n        convert_to_tf_keras()\r\n'"
setup.py,0,"b""from setuptools import setup\nfrom setuptools import find_packages\nimport os\n\n\nif os.environ.get('USE_TF_KERAS', None) == '1':\n    name = 'tf_keras_contrib'\n    install_requires = []\nelse:\n    name = 'keras_contrib'\n    install_requires = ['keras']\n\nsetup(name=name,\n      version='2.0.8',\n      description='Keras Deep Learning for Python, Community Contributions',\n      author='Fariz Rahman',\n      author_email='farizrahman4u@gmail.com',\n      url='https://github.com/farizrahman4u/keras-contrib',\n      license='MIT',\n      install_requires=install_requires,\n      extras_require={\n          'h5py': ['h5py'],\n          'visualize': ['pydot>=1.2.0'],\n          'tests': ['pytest',\n                    'pytest-pep8',\n                    'pytest-xdist',\n                    'pytest-cov'],\n      },\n      classifiers=[\n          'Development Status :: 3 - Alpha',\n          'Intended Audience :: Developers',\n          'Intended Audience :: Education',\n          'Intended Audience :: Science/Research',\n          'License :: OSI Approved :: MIT License',\n          'Programming Language :: Python :: 2',\n          'Programming Language :: Python :: 2.7',\n          'Programming Language :: Python :: 3',\n          'Programming Language :: Python :: 3.6',\n          'Topic :: Software Development :: Libraries',\n          'Topic :: Software Development :: Libraries :: Python Modules'\n      ],\n      packages=find_packages())\n"""
examples/cifar10_clr.py,0,"b'\'\'\'Train a simple deep CNN on the CIFAR10 small images dataset using\na triangular cyclic learning rate (CLR) policy.\nIt gets to 75% validation accuracy in 15 epochs, and 79% after 40 epochs;\ncompare to 25 and 50 epochs respectively without CLR.\n\'\'\'\n\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nimport keras\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras_contrib.callbacks import CyclicLR\n\nimport os\n\nbatch_size = 100\nepochs = 50\nnum_classes = 10\ndata_augmentation = True\nnum_predictions = 20\nsave_dir = os.path.join(os.getcwd(), \'saved_models\')\nmodel_name = \'keras_cifar10_trained_model.h5\'\n\n# The data, split between train and test sets:\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint(\'x_train shape:\', x_train.shape)\nprint(x_train.shape[0], \'train samples\')\nprint(x_test.shape[0], \'test samples\')\n\n# Convert class vectors to binary class matrices.\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding=\'same\',\n                 input_shape=x_train.shape[1:]))\nmodel.add(Activation(\'relu\'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation(\'relu\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), padding=\'same\'))\nmodel.add(Activation(\'relu\'))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation(\'relu\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation(\'relu\'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation(\'softmax\'))\n\n# initiate RMSprop optimizer\nopt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n\n# initiate CyclicLR LR scheduler\nclr = CyclicLR(\n    base_lr=0.0001,\n    max_lr=0.0005,\n    step_size=2000,\n    mode=\'triangular\')\n\n\n# Let\'s train the model using RMSprop\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=opt,\n              metrics=[\'accuracy\'])\n\nx_train = x_train.astype(\'float32\')\nx_test = x_test.astype(\'float32\')\nx_train /= 255\nx_test /= 255\n\nif not data_augmentation:\n    print(\'Not using data augmentation.\')\n    model.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),\n              callbacks=[clr],\n              shuffle=True)\nelse:\n    print(\'Using real-time data augmentation.\')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n        rotation_range=0,\n        # randomly rotate images in the range (degrees, 0 to 180)\n        # randomly shift images horizontally (fraction of total width)\n        width_shift_range=0.1,\n        # randomly shift images vertically (fraction of total height)\n        height_shift_range=0.1,\n        shear_range=0.,  # set range for random shear\n        zoom_range=0.,  # set range for random zoom\n        channel_shift_range=0.,  # set range for random channel shifts\n        # set mode for filling points outside the input boundaries\n        fill_mode=\'nearest\',\n        cval=0.,  # value used for fill_mode = ""constant""\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False,  # randomly flip images\n        # set rescaling factor (applied before any other transformation)\n        rescale=None,\n        # set function that will be applied on each input\n        preprocessing_function=None,\n        # image data format, either ""channels_first"" or ""channels_last""\n        data_format=None,\n        # fraction of images reserved for validation (strictly between 0 and 1)\n        validation_split=0.0)\n\n    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(x_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n\n    model.fit_generator(datagen.flow(x_train, y_train,\n                                     batch_size=batch_size),\n                        epochs=epochs,\n                        validation_data=(x_test, y_test),\n                        callbacks=[clr],\n                        workers=4)\n\n# Save model and weights\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint(\'Saved trained model at %s \' % model_path)\n\n# Score trained model.\nscores = model.evaluate(x_test, y_test, verbose=1)\nprint(\'Test loss:\', scores[0])\nprint(\'Test accuracy:\', scores[1])\n'"
examples/cifar10_densenet.py,0,"b""'''\nTrains a DenseNet-40-12 model on the CIFAR-10 Dataset.\n\nGets a 94.84% accuracy score after 100 epochs.\n'''\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\n\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.datasets import cifar10\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import np_utils\nfrom keras_contrib.applications import DenseNet\n\nbatch_size = 64\nnb_classes = 10\nepochs = 100\n\nimg_rows, img_cols = 32, 32\nimg_channels = 3\n\n# Parameters for the DenseNet model builder\nif K.image_data_format() == 'channels_first':\n    img_dim = (img_channels, img_rows, img_cols)\nelse:\n    img_dim = (img_rows, img_cols, img_channels)\ndepth = 40\nnb_dense_block = 3\ngrowth_rate = 12\nnb_filter = 16\ndropout_rate = 0.0  # 0.0 for data augmentation\n\n# Create the model (without loading weights)\nmodel = DenseNet(depth=depth, nb_dense_block=nb_dense_block,\n                 growth_rate=growth_rate, nb_filter=nb_filter,\n                 dropout_rate=dropout_rate,\n                 input_shape=img_dim,\n                 weights=None)\nprint('Model created')\n\nmodel.summary()\n\noptimizer = Adam(lr=1e-3)  # Using Adam instead of SGD to speed up training\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\nprint('Finished compiling')\n\n(trainX, trainY), (testX, testY) = cifar10.load_data()\n\ntrainX = trainX.astype('float32')\ntestX = testX.astype('float32')\n\ntrainX /= 255.\ntestX /= 255.\n\nY_train = np_utils.to_categorical(trainY, nb_classes)\nY_test = np_utils.to_categorical(testY, nb_classes)\n\ngenerator = ImageDataGenerator(rotation_range=15,\n                               width_shift_range=5. / 32,\n                               height_shift_range=5. / 32)\n\ngenerator.fit(trainX, seed=0)\n\nweights_file = 'DenseNet-40-12-CIFAR-10.h5'\n\nlr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1),\n                               cooldown=0, patience=10, min_lr=0.5e-6)\nearly_stopper = EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=20)\nmodel_checkpoint = ModelCheckpoint(weights_file, monitor='val_acc',\n                                   save_best_only=True,\n                                   save_weights_only=True, mode='auto')\n\ncallbacks = [lr_reducer, early_stopper, model_checkpoint]\n\nmodel.fit_generator(generator.flow(trainX, Y_train, batch_size=batch_size),\n                    steps_per_epoch=len(trainX) // batch_size,\n                    epochs=epochs,\n                    callbacks=callbacks,\n                    validation_data=(testX, Y_test),\n                    verbose=2)\n\nscores = model.evaluate(testX, Y_test, batch_size=batch_size)\nprint('Test loss : ', scores[0])\nprint('Test accuracy : ', scores[1])\n"""
examples/cifar10_nasnet.py,0,"b'""""""\nAdapted from keras example cifar10_cnn.py\nTrain NASNet-CIFAR on the CIFAR10 small images dataset.\n""""""\nfrom __future__ import print_function\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.callbacks import CSVLogger\nfrom keras.optimizers import Adam\nfrom keras_contrib.applications.nasnet import NASNetCIFAR, preprocess_input\n\nimport numpy as np\n\n\nweights_file = \'NASNet-CIFAR-10.h5\'\nlr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.5),\n                               cooldown=0,\n                               patience=5,\n                               min_lr=0.5e-5)\ncsv_logger = CSVLogger(\'NASNet-CIFAR-10.csv\')\nmodel_checkpoint = ModelCheckpoint(weights_file,\n                                   monitor=\'val_predictions_acc\',\n                                   save_best_only=True,\n                                   save_weights_only=True, mode=\'max\')\n\nbatch_size = 128\nnb_classes = 10\nnb_epoch = 600\ndata_augmentation = True\n\n# input image dimensions\nimg_rows, img_cols = 32, 32\n# The CIFAR10 images are RGB.\nimg_channels = 3\n\n# The data, shuffled and split between train and test sets:\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n\n# Convert class vectors to binary class matrices.\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\nX_train = X_train.astype(\'float32\')\nX_test = X_test.astype(\'float32\')\n\n# preprocess input\nX_train = preprocess_input(X_train)\nX_test = preprocess_input(X_test)\n\n# For training, the auxilary branch must be used to correctly train NASNet\nmodel = NASNetCIFAR((img_rows, img_cols, img_channels), use_auxilary_branch=True)\nmodel.summary()\n\noptimizer = Adam(lr=1e-3, clipnorm=5)\nmodel.compile(loss=[\'categorical_crossentropy\', \'categorical_crossentropy\'],\n              optimizer=optimizer, metrics=[\'accuracy\'], loss_weights=[1.0, 0.4])\n\n# model.load_weights(\'NASNet-CIFAR-10.h5\', by_name=True)\n\nif not data_augmentation:\n    print(\'Not using data augmentation.\')\n    model.fit(X_train, [Y_train, Y_train],\n              batch_size=batch_size,\n              epochs=nb_epoch,\n              validation_data=(X_test, [Y_test, Y_test]),\n              shuffle=True,\n              verbose=2,\n              callbacks=[lr_reducer, csv_logger, model_checkpoint])\nelse:\n    print(\'Using real-time data augmentation.\')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for featurewise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(X_train)\n\n    # wrap the ImageDataGenerator to yield\n    # two label batches [y, y] for each input batch X\n    # When training a NASNet model, we have to use its auxilary training head\n    # Therefore the model is technically a 1 input - 2 output model, and requires\n    # the label to be duplicated for the auxilary head\n    def image_data_generator_wrapper(image_datagenerator, batch_size):\n        iterator = datagen.flow(X_train, Y_train, batch_size=batch_size)\n\n        while True:\n            X, y = next(iterator)  # get the next batch\n            yield X, [y, y]  # duplicate the labels for each batch\n\n    # Fit the model on the batches generated by datagen.flow().\n    model.fit_generator(image_data_generator_wrapper(datagen, batch_size),\n                        steps_per_epoch=X_train.shape[0] // batch_size,\n                        validation_data=(X_test, [Y_test, Y_test]),\n                        epochs=nb_epoch, verbose=2,\n                        callbacks=[lr_reducer, csv_logger, model_checkpoint])\n\nscores = model.evaluate(X_test, [Y_test, Y_test], batch_size=batch_size)\nfor score, metric_name in zip(scores, model.metrics_names):\n    print(""%s : %0.4f"" % (metric_name, score))\n'"
examples/cifar10_resnet.py,0,"b'""""""\nAdapted from keras example cifar10_cnn.py and github.com/raghakot/keras-resnet\nTrain ResNet-18 on the CIFAR10 small images dataset.\n\nGPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python cifar10.py\n""""""\nfrom __future__ import print_function\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.callbacks import CSVLogger\nfrom keras.callbacks import EarlyStopping\nfrom keras_contrib.applications.resnet import ResNet18\n\nimport numpy as np\n\n\nweights_file = \'ResNet18v2-CIFAR-10.h5\'\nlr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0,\n                               patience=5, min_lr=0.5e-6)\nearly_stopper = EarlyStopping(min_delta=0.001, patience=10)\ncsv_logger = CSVLogger(\'ResNet18v2-CIFAR-10.csv\')\nmodel_checkpoint = ModelCheckpoint(weights_file, monitor=\'val_acc\', save_best_only=True,\n                                   save_weights_only=True, mode=\'auto\')\n\nbatch_size = 32\nnb_classes = 10\nnb_epoch = 200\ndata_augmentation = True\n\n# input image dimensions\nimg_rows, img_cols = 32, 32\n# The CIFAR10 images are RGB.\nimg_channels = 3\n\n# The data, shuffled and split between train and test sets:\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n\n# Convert class vectors to binary class matrices.\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\nX_train = X_train.astype(\'float32\')\nX_test = X_test.astype(\'float32\')\n\n# subtract mean and normalize\nmean_image = np.mean(X_train, axis=0)\nX_train -= mean_image\nX_test -= mean_image\nX_train /= 128.\nX_test /= 128.\n\nmodel = ResNet18((img_rows, img_cols, img_channels), nb_classes)\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=\'adam\',\n              metrics=[\'accuracy\'])\n\nif not data_augmentation:\n    print(\'Not using data augmentation.\')\n    model.fit(X_train, Y_train,\n              batch_size=batch_size,\n              nb_epoch=nb_epoch,\n              validation_data=(X_test, Y_test),\n              shuffle=True,\n              callbacks=[lr_reducer, early_stopper, csv_logger, model_checkpoint])\nelse:\n    print(\'Using real-time data augmentation.\')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally\n        height_shift_range=0.1,  # randomly shift images vertically\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for featurewise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(X_train)\n\n    callbacks = [lr_reducer, early_stopper, csv_logger, model_checkpoint]\n    # Fit the model on the batches generated by datagen.flow().\n    model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n                        steps_per_epoch=X_train.shape[0] // batch_size,\n                        validation_data=(X_test, Y_test),\n                        epochs=nb_epoch, verbose=2,\n                        callbacks=callbacks)\n\nscores = model.evaluate(X_test, Y_test, batch_size=batch_size)\nprint(\'Test loss : \', scores[0])\nprint(\'Test accuracy : \', scores[1])\n'"
examples/cifar10_ror.py,0,"b""'''\nTrains a Residual-of-Residual Network (WRN-40-2) model on the CIFAR-10 Dataset.\n\nGets a 94.53% accuracy score after 150 epochs.\n'''\n\nimport keras.callbacks as callbacks\nimport keras.utils.np_utils as kutils\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\n\nfrom keras_contrib.applications import ResidualOfResidual\n\nbatch_size = 64\nepochs = 150\nimg_rows, img_cols = 32, 32\n\n(trainX, trainY), (testX, testY) = cifar10.load_data()\n\ntrainX = trainX.astype('float32')\ntestX = testX.astype('float32')\n\ntrainX /= 255\ntestX /= 255\n\ntempY = testY\ntrainY = kutils.to_categorical(trainY)\ntestY = kutils.to_categorical(testY)\n\ngenerator = ImageDataGenerator(rotation_range=15,\n                               width_shift_range=5. / 32,\n                               height_shift_range=5. / 32)\n\ngenerator.fit(trainX, seed=0)\n\nmodel = ResidualOfResidual(depth=40, width=2, dropout_rate=0.0, weights=None)\n\noptimizer = Adam(lr=1e-3)\n\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\nprint('Finished compiling')\n\ncheckpoint = callbacks.ModelCheckpoint('weights/RoR-WRN-40-2-Weights.h5',\n                                       monitor='val_acc',\n                                       save_best_only=True,\n                                       save_weights_only=True)\nmodel.fit_generator(generator.flow(trainX, trainY, batch_size=batch_size),\n                    steps_per_epoch=len(trainX) // batch_size,\n                    epochs=epochs,\n                    callbacks=[checkpoint],\n                    validation_data=(testX, testY),\n                    verbose=2)\n\nscores = model.evaluate(testX, testY, batch_size)\nprint('Test loss : ', scores[0])\nprint('Test accuracy : ', scores[1])\n"""
examples/cifar10_wide_resnet.py,0,"b""'''\nTrains a WRN-28-8 model on the CIFAR-10 Dataset.\n\nPerformance is slightly less than the paper, since\nthey use WRN-28-10 model (95.83%).\n\nGets a 95.54% accuracy score after 300 epochs.\n'''\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nfrom keras.datasets import cifar10\nimport keras.callbacks as callbacks\nimport keras.utils.np_utils as kutils\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom keras_contrib.applications.wide_resnet import WideResidualNetwork\n\nbatch_size = 64\nepochs = 300\nimg_rows, img_cols = 32, 32\n\n(trainX, trainY), (testX, testY) = cifar10.load_data()\n\ntrainX = trainX.astype('float32')\ntrainX /= 255.0\ntestX = testX.astype('float32')\ntestX /= 255.0\n\ntempY = testY\ntrainY = kutils.to_categorical(trainY)\ntestY = kutils.to_categorical(testY)\n\ngenerator = ImageDataGenerator(rotation_range=10,\n                               width_shift_range=5. / 32,\n                               height_shift_range=5. / 32,\n                               horizontal_flip=True)\n\ngenerator.fit(trainX, seed=0, augment=True)\n\n# We will be training the model, therefore no need to load weights\nmodel = WideResidualNetwork(depth=28, width=8, dropout_rate=0.0, weights=None)\n\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\nprint('Finished compiling')\nmodel_checkpoint = callbacks.ModelCheckpoint('WRN-28-8 Weights.h5',\n                                             monitor='val_acc',\n                                             save_best_only=True,\n                                             save_weights_only=True)\nmodel.fit_generator(generator.flow(trainX, trainY, batch_size=batch_size),\n                    steps_per_epoch=len(trainX) // batch_size,\n                    epochs=epochs,\n                    callbacks=[model_checkpoint],\n                    validation_data=(testX, testY))\n\nscores = model.evaluate(testX, testY, batch_size)\nprint('Test loss : %0.5f' % (scores[0]))\nprint('Test accuracy = %0.5f' % (scores[1]))\n"""
examples/conll2000_chunking_crf.py,0,"b'""""""Train CRF and BiLSTM-CRF on CONLL2000 chunking data,\nsimilar to https://arxiv.org/pdf/1508.01991v1.pdf.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy\nfrom collections import Counter\n\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Bidirectional, LSTM\nfrom keras_contrib.layers import CRF\nfrom keras_contrib.losses import crf_loss\nfrom keras_contrib.metrics import crf_viterbi_accuracy\nfrom keras_contrib.datasets import conll2000\n\nEPOCHS = 10\nEMBED_DIM = 200\nBiRNN_UNITS = 200\n\n\ndef classification_report(y_true, y_pred, labels):\n    \'\'\'Similar to the one in sklearn.metrics,\n    reports per classs recall, precision and F1 score\'\'\'\n    y_true = numpy.asarray(y_true).ravel()\n    y_pred = numpy.asarray(y_pred).ravel()\n    corrects = Counter(yt for yt, yp in zip(y_true, y_pred) if yt == yp)\n    y_true_counts = Counter(y_true)\n    y_pred_counts = Counter(y_pred)\n    report = ((lab,  # label\n               corrects[i] / max(1, y_true_counts[i]),  # recall\n               corrects[i] / max(1, y_pred_counts[i]),  # precision\n               y_true_counts[i]  # support\n               ) for i, lab in enumerate(labels))\n    report = [(l, r, p, 2 * r * p / max(1e-9, r + p), s) for l, r, p, s in report]\n\n    print(\'{:<15}{:>10}{:>10}{:>10}{:>10}\\n\'.format(\'\',\n                                                    \'recall\',\n                                                    \'precision\',\n                                                    \'f1-score\',\n                                                    \'support\'))\n    formatter = \'{:<15}{:>10.2f}{:>10.2f}{:>10.2f}{:>10d}\'.format\n    for r in report:\n        print(formatter(*r))\n    print(\'\')\n    report2 = list(zip(*[(r * s, p * s, f1 * s) for l, r, p, f1, s in report]))\n    N = len(y_true)\n    print(formatter(\'avg / total\',\n                    sum(report2[0]) / N,\n                    sum(report2[1]) / N,\n                    sum(report2[2]) / N, N) + \'\\n\')\n\n\n# ------\n# Data\n# -----\n\n# conll200 has two different targets, here will only use\n# IBO like chunking as an example\ntrain, test, voc = conll2000.load_data()\n(train_x, _, train_y) = train\n(test_x, _, test_y) = test\n(vocab, _, class_labels) = voc\n\n# --------------\n# 1. Regular CRF\n# --------------\n\nprint(\'==== training CRF ====\')\n\nmodel = Sequential()\nmodel.add(Embedding(len(vocab), EMBED_DIM, mask_zero=True))  # Random embedding\ncrf = CRF(len(class_labels), sparse_target=True)\nmodel.add(crf)\nmodel.summary()\n\n# The default `crf_loss` for `learn_mode=\'join\'` is negative log likelihood.\nmodel.compile(\'adam\', loss=crf_loss, metrics=[crf_viterbi_accuracy])\nmodel.fit(train_x, train_y, epochs=EPOCHS, validation_data=[test_x, test_y])\n\ntest_y_pred = model.predict(test_x).argmax(-1)[test_x > 0]\ntest_y_true = test_y[test_x > 0]\n\nprint(\'\\n---- Result of CRF ----\\n\')\nclassification_report(test_y_true, test_y_pred, class_labels)\n\n# -------------\n# 2. BiLSTM-CRF\n# -------------\n\nprint(\'==== training BiLSTM-CRF ====\')\n\nmodel = Sequential()\nmodel.add(Embedding(len(vocab), EMBED_DIM, mask_zero=True))  # Random embedding\nmodel.add(Bidirectional(LSTM(BiRNN_UNITS // 2, return_sequences=True)))\ncrf = CRF(len(class_labels), sparse_target=True)\nmodel.add(crf)\nmodel.summary()\n\nmodel.compile(\'adam\', loss=crf_loss, metrics=[crf_viterbi_accuracy])\nmodel.fit(train_x, train_y, epochs=EPOCHS, validation_data=[test_x, test_y])\n\ntest_y_pred = model.predict(test_x).argmax(-1)[test_x > 0]\ntest_y_true = test_y[test_x > 0]\n\nprint(\'\\n---- Result of BiLSTM-CRF ----\\n\')\nclassification_report(test_y_true, test_y_pred, class_labels)\n'"
examples/improved_wgan.py,0,"b'""""""An implementation of the improved WGAN described in https://arxiv.org/abs/1704.00028\r\n\r\nThe improved WGAN has a term in the loss function which penalizes the network if its\r\ngradient norm moves away from 1. This is included because the Earth Mover (EM) distance\r\nused in WGANs is only easy to calculate for 1-Lipschitz functions (i.e. functions where\r\nthe gradient norm has a constant upper bound of 1).\r\n\r\nThe original WGAN paper enforced this by clipping weights to very small values\r\n[-0.01, 0.01]. However, this drastically reduced network capacity. Penalizing the\r\ngradient norm is more natural, but this requires second-order gradients. These are not\r\nsupported for some tensorflow ops (particularly MaxPool and AveragePool) in the current\r\nrelease (1.0.x), but they are supported in the current nightly builds\r\n(1.1.0-rc1 and higher).\r\n\r\nTo avoid this, this model uses strided convolutions instead of Average/Maxpooling for\r\ndownsampling. If you wish to use pooling operations in your discriminator, please ensure\r\nyou update Tensorflow to 1.1.0-rc1 or higher. I haven\'t tested this with Theano at all.\r\n\r\nThe model saves images using pillow. If you don\'t have pillow, either install it or\r\nremove the calls to generate_images.\r\n""""""\r\nimport argparse\r\nimport os\r\nimport numpy as np\r\nfrom keras.models import Model, Sequential\r\nfrom keras.layers import Input, Dense, Reshape, Flatten\r\nfrom keras.layers.merge import _Merge\r\nfrom keras.layers.convolutional import Convolution2D, Conv2DTranspose\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.layers.advanced_activations import LeakyReLU\r\nfrom keras.optimizers import Adam\r\nfrom keras.datasets import mnist\r\nfrom keras import backend as K\r\nfrom functools import partial\r\n\r\ntry:\r\n    from PIL import Image\r\nexcept ImportError:\r\n    print(\'This script depends on pillow! \'\r\n          \'Please install it (e.g. with pip install pillow)\')\r\n    exit()\r\n\r\nBATCH_SIZE = 64\r\n# The training ratio is the number of discriminator updates\r\n# per generator update. The paper uses 5.\r\nTRAINING_RATIO = 5\r\nGRADIENT_PENALTY_WEIGHT = 10  # As per the paper\r\n\r\n\r\ndef wasserstein_loss(y_true, y_pred):\r\n    """"""Calculates the Wasserstein loss for a sample batch.\r\n\r\n    The Wasserstein loss function is very simple to calculate. In a standard GAN, the\r\n    discriminator has a sigmoid output, representing the probability that samples are\r\n    real or generated. In Wasserstein GANs, however, the output is linear with no\r\n    activation function! Instead of being constrained to [0, 1], the discriminator wants\r\n    to make the distance between its output for real and generated samples as\r\n    large as possible.\r\n\r\n    The most natural way to achieve this is to label generated samples -1 and real\r\n    samples 1, instead of the 0 and 1 used in normal GANs, so that multiplying the\r\n    outputs by the labels will give you the loss immediately.\r\n\r\n    Note that the nature of this loss means that it can be (and frequently will be)\r\n    less than 0.""""""\r\n    return K.mean(y_true * y_pred)\r\n\r\n\r\ndef gradient_penalty_loss(y_true, y_pred, averaged_samples,\r\n                          gradient_penalty_weight):\r\n    """"""Calculates the gradient penalty loss for a batch of ""averaged"" samples.\r\n\r\n    In Improved WGANs, the 1-Lipschitz constraint is enforced by adding a term to the\r\n    loss function that penalizes the network if the gradient norm moves away from 1.\r\n    However, it is impossible to evaluate this function at all points in the input\r\n    space. The compromise used in the paper is to choose random points on the lines\r\n    between real and generated samples, and check the gradients at these points. Note\r\n    that it is the gradient w.r.t. the input averaged samples, not the weights of the\r\n    discriminator, that we\'re penalizing!\r\n\r\n    In order to evaluate the gradients, we must first run samples through the generator\r\n    and evaluate the loss. Then we get the gradients of the discriminator w.r.t. the\r\n    input averaged samples. The l2 norm and penalty can then be calculated for this\r\n    gradient.\r\n\r\n    Note that this loss function requires the original averaged samples as input, but\r\n    Keras only supports passing y_true and y_pred to loss functions. To get around this,\r\n    we make a partial() of the function with the averaged_samples argument, and use that\r\n    for model training.""""""\r\n    # first get the gradients:\r\n    #   assuming: - that y_pred has dimensions (batch_size, 1)\r\n    #             - averaged_samples has dimensions (batch_size, nbr_features)\r\n    # gradients afterwards has dimension (batch_size, nbr_features), basically\r\n    # a list of nbr_features-dimensional gradient vectors\r\n    gradients = K.gradients(y_pred, averaged_samples)[0]\r\n    # compute the euclidean norm by squaring ...\r\n    gradients_sqr = K.square(gradients)\r\n    #   ... summing over the rows ...\r\n    gradients_sqr_sum = K.sum(gradients_sqr,\r\n                              axis=np.arange(1, len(gradients_sqr.shape)))\r\n    #   ... and sqrt\r\n    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\r\n    # compute lambda * (1 - ||grad||)^2 still for each single sample\r\n    gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\r\n    # return the mean as loss over all the batch samples\r\n    return K.mean(gradient_penalty)\r\n\r\n\r\ndef make_generator():\r\n    """"""Creates a generator model that takes a 100-dimensional noise vector as a ""seed"",\r\n    and outputs images of size 28x28x1.""""""\r\n    model = Sequential()\r\n    model.add(Dense(1024, input_dim=100))\r\n    model.add(LeakyReLU())\r\n    model.add(Dense(128 * 7 * 7))\r\n    model.add(BatchNormalization())\r\n    model.add(LeakyReLU())\r\n    if K.image_data_format() == \'channels_first\':\r\n        model.add(Reshape((128, 7, 7), input_shape=(128 * 7 * 7,)))\r\n        bn_axis = 1\r\n    else:\r\n        model.add(Reshape((7, 7, 128), input_shape=(128 * 7 * 7,)))\r\n        bn_axis = -1\r\n    model.add(Conv2DTranspose(128, (5, 5), strides=2, padding=\'same\'))\r\n    model.add(BatchNormalization(axis=bn_axis))\r\n    model.add(LeakyReLU())\r\n    model.add(Convolution2D(64, (5, 5), padding=\'same\'))\r\n    model.add(BatchNormalization(axis=bn_axis))\r\n    model.add(LeakyReLU())\r\n    model.add(Conv2DTranspose(64, (5, 5), strides=2, padding=\'same\'))\r\n    model.add(BatchNormalization(axis=bn_axis))\r\n    model.add(LeakyReLU())\r\n    # Because we normalized training inputs to lie in the range [-1, 1],\r\n    # the tanh function should be used for the output of the generator to ensure\r\n    # its output also lies in this range.\r\n    model.add(Convolution2D(1, (5, 5), padding=\'same\', activation=\'tanh\'))\r\n    return model\r\n\r\n\r\ndef make_discriminator():\r\n    """"""Creates a discriminator model that takes an image as input and outputs a single\r\n    value, representing whether the input is real or generated. Unlike normal GANs, the\r\n    output is not sigmoid and does not represent a probability! Instead, the output\r\n    should be as large and negative as possible for generated inputs and as large and\r\n    positive as possible for real inputs.\r\n\r\n    Note that the improved WGAN paper suggests that BatchNormalization should not be\r\n    used in the discriminator.""""""\r\n    model = Sequential()\r\n    if K.image_data_format() == \'channels_first\':\r\n        model.add(Convolution2D(64, (5, 5), padding=\'same\', input_shape=(1, 28, 28)))\r\n    else:\r\n        model.add(Convolution2D(64, (5, 5), padding=\'same\', input_shape=(28, 28, 1)))\r\n    model.add(LeakyReLU())\r\n    model.add(Convolution2D(128, (5, 5), kernel_initializer=\'he_normal\',\r\n                            strides=[2, 2]))\r\n    model.add(LeakyReLU())\r\n    model.add(Convolution2D(128, (5, 5), kernel_initializer=\'he_normal\', padding=\'same\',\r\n                            strides=[2, 2]))\r\n    model.add(LeakyReLU())\r\n    model.add(Flatten())\r\n    model.add(Dense(1024, kernel_initializer=\'he_normal\'))\r\n    model.add(LeakyReLU())\r\n    model.add(Dense(1, kernel_initializer=\'he_normal\'))\r\n    return model\r\n\r\n\r\ndef tile_images(image_stack):\r\n    """"""Given a stacked tensor of images, reshapes them into a horizontal tiling for\r\n    display.""""""\r\n    assert len(image_stack.shape) == 3\r\n    image_list = [image_stack[i, :, :] for i in range(image_stack.shape[0])]\r\n    tiled_images = np.concatenate(image_list, axis=1)\r\n    return tiled_images\r\n\r\n\r\nclass RandomWeightedAverage(_Merge):\r\n    """"""Takes a randomly-weighted average of two tensors. In geometric terms, this\r\n    outputs a random point on the line between each pair of input points.\r\n\r\n    Inheriting from _Merge is a little messy but it was the quickest solution I could\r\n    think of. Improvements appreciated.""""""\r\n\r\n    def _merge_function(self, inputs):\r\n        weights = K.random_uniform((BATCH_SIZE, 1, 1, 1))\r\n        return (weights * inputs[0]) + ((1 - weights) * inputs[1])\r\n\r\n\r\ndef generate_images(generator_model, output_dir, epoch):\r\n    """"""Feeds random seeds into the generator and tiles and saves the output to a PNG\r\n    file.""""""\r\n    test_image_stack = generator_model.predict(np.random.rand(10, 100))\r\n    test_image_stack = (test_image_stack * 127.5) + 127.5\r\n    test_image_stack = np.squeeze(np.round(test_image_stack).astype(np.uint8))\r\n    tiled_output = tile_images(test_image_stack)\r\n    tiled_output = Image.fromarray(tiled_output, mode=\'L\')  # L specifies greyscale\r\n    outfile = os.path.join(output_dir, \'epoch_{}.png\'.format(epoch))\r\n    tiled_output.save(outfile)\r\n\r\n\r\nparser = argparse.ArgumentParser(description=""Improved Wasserstein GAN ""\r\n                                             ""implementation for Keras."")\r\nparser.add_argument(""--output_dir"", ""-o"", required=True,\r\n                    help=""Directory to output generated files to"")\r\nargs = parser.parse_args()\r\n\r\n# First we load the image data, reshape it and normalize it to the range [-1, 1]\r\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\nX_train = np.concatenate((X_train, X_test), axis=0)\r\nif K.image_data_format() == \'channels_first\':\r\n    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1], X_train.shape[2]))\r\nelse:\r\n    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\r\nX_train = (X_train.astype(np.float32) - 127.5) / 127.5\r\n\r\n# Now we initialize the generator and discriminator.\r\ngenerator = make_generator()\r\ndiscriminator = make_discriminator()\r\n\r\n# The generator_model is used when we want to train the generator layers.\r\n# As such, we ensure that the discriminator layers are not trainable.\r\n# Note that once we compile this model, updating .trainable will have no effect within\r\n# it. As such, it won\'t cause problems if we later set discriminator.trainable = True\r\n# for the discriminator_model, as long as we compile the generator_model first.\r\nfor layer in discriminator.layers:\r\n    layer.trainable = False\r\ndiscriminator.trainable = False\r\ngenerator_input = Input(shape=(100,))\r\ngenerator_layers = generator(generator_input)\r\ndiscriminator_layers_for_generator = discriminator(generator_layers)\r\ngenerator_model = Model(inputs=[generator_input],\r\n                        outputs=[discriminator_layers_for_generator])\r\n# We use the Adam paramaters from Gulrajani et al.\r\ngenerator_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),\r\n                        loss=wasserstein_loss)\r\n\r\n# Now that the generator_model is compiled, we can make the discriminator\r\n# layers trainable.\r\nfor layer in discriminator.layers:\r\n    layer.trainable = True\r\nfor layer in generator.layers:\r\n    layer.trainable = False\r\ndiscriminator.trainable = True\r\ngenerator.trainable = False\r\n\r\n# The discriminator_model is more complex. It takes both real image samples and random\r\n# noise seeds as input. The noise seed is run through the generator model to get\r\n# generated images. Both real and generated images are then run through the\r\n# discriminator. Although we could concatenate the real and generated images into a\r\n# single tensor, we don\'t (see model compilation for why).\r\nreal_samples = Input(shape=X_train.shape[1:])\r\ngenerator_input_for_discriminator = Input(shape=(100,))\r\ngenerated_samples_for_discriminator = generator(generator_input_for_discriminator)\r\ndiscriminator_output_from_generator = discriminator(generated_samples_for_discriminator)\r\ndiscriminator_output_from_real_samples = discriminator(real_samples)\r\n\r\n# We also need to generate weighted-averages of real and generated samples,\r\n# to use for the gradient norm penalty.\r\naveraged_samples = RandomWeightedAverage()([real_samples,\r\n                                            generated_samples_for_discriminator])\r\n# We then run these samples through the discriminator as well. Note that we never\r\n# really use the discriminator output for these samples - we\'re only running them to\r\n# get the gradient norm for the gradient penalty loss.\r\naveraged_samples_out = discriminator(averaged_samples)\r\n\r\n# The gradient penalty loss function requires the input averaged samples to get\r\n# gradients. However, Keras loss functions can only have two arguments, y_true and\r\n# y_pred. We get around this by making a partial() of the function with the averaged\r\n# samples here.\r\npartial_gp_loss = partial(gradient_penalty_loss,\r\n                          averaged_samples=averaged_samples,\r\n                          gradient_penalty_weight=GRADIENT_PENALTY_WEIGHT)\r\n# Functions need names or Keras will throw an error\r\npartial_gp_loss.__name__ = \'gradient_penalty\'\r\n\r\n# Keras requires that inputs and outputs have the same number of samples. This is why\r\n# we didn\'t concatenate the real samples and generated samples before passing them to\r\n# the discriminator: If we had, it would create an output with 2 * BATCH_SIZE samples,\r\n# while the output of the ""averaged"" samples for gradient penalty\r\n# would have only BATCH_SIZE samples.\r\n\r\n# If we don\'t concatenate the real and generated samples, however, we get three\r\n# outputs: One of the generated samples, one of the real samples, and one of the\r\n# averaged samples, all of size BATCH_SIZE. This works neatly!\r\ndiscriminator_model = Model(inputs=[real_samples,\r\n                                    generator_input_for_discriminator],\r\n                            outputs=[discriminator_output_from_real_samples,\r\n                                     discriminator_output_from_generator,\r\n                                     averaged_samples_out])\r\n# We use the Adam paramaters from Gulrajani et al. We use the Wasserstein loss for both\r\n# the real and generated samples, and the gradient penalty loss for the averaged samples\r\ndiscriminator_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),\r\n                            loss=[wasserstein_loss,\r\n                                  wasserstein_loss,\r\n                                  partial_gp_loss])\r\n# We make three label vectors for training. positive_y is the label vector for real\r\n# samples, with value 1. negative_y is the label vector for generated samples, with\r\n# value -1. The dummy_y vector is passed to the gradient_penalty loss function and\r\n# is not used.\r\npositive_y = np.ones((BATCH_SIZE, 1), dtype=np.float32)\r\nnegative_y = -positive_y\r\ndummy_y = np.zeros((BATCH_SIZE, 1), dtype=np.float32)\r\n\r\nfor epoch in range(100):\r\n    np.random.shuffle(X_train)\r\n    print(""Epoch: "", epoch)\r\n    print(""Number of batches: "", int(X_train.shape[0] // BATCH_SIZE))\r\n    discriminator_loss = []\r\n    generator_loss = []\r\n    minibatches_size = BATCH_SIZE * TRAINING_RATIO\r\n    for i in range(int(X_train.shape[0] // (BATCH_SIZE * TRAINING_RATIO))):\r\n        discriminator_minibatches = X_train[i * minibatches_size:\r\n                                            (i + 1) * minibatches_size]\r\n        for j in range(TRAINING_RATIO):\r\n            image_batch = discriminator_minibatches[j * BATCH_SIZE:\r\n                                                    (j + 1) * BATCH_SIZE]\r\n            noise = np.random.rand(BATCH_SIZE, 100).astype(np.float32)\r\n            discriminator_loss.append(discriminator_model.train_on_batch(\r\n                [image_batch, noise],\r\n                [positive_y, negative_y, dummy_y]))\r\n        generator_loss.append(generator_model.train_on_batch(np.random.rand(BATCH_SIZE,\r\n                                                                            100),\r\n                                                             positive_y))\r\n    # Still needs some code to display losses from the generator and discriminator,\r\n    # progress bars, etc.\r\n    generate_images(generator, args.output_dir, epoch)\r\n'"
examples/jaccard_loss.py,0,"b'import keras\nfrom keras import backend as K\nfrom keras_contrib.losses.jaccard import jaccard_distance\nimport numpy as np\n\n# Test and plot\ny_pred = np.array([np.arange(-10, 10 + 0.1, 0.1)]).T\ny_true = np.zeros(y_pred.shape)\nname = \'jaccard_distance_loss\'\ntry:\n    loss = jaccard_distance_loss(\n        K.variable(y_true), K.variable(y_pred)\n    ).eval(session=K.get_session())\nexcept Exception as e:\n    print(""error plotting"", name, e)\nelse:\n    plt.title(name)\n    plt.plot(y_pred, loss)\n    plt.show()\n\nprint(""TYPE                 |Almost_right |half right |all_wrong"")\ny_true = np.array([[0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1., 0.]])\ny_pred = np.array([[0, 0, 0.9, 0], [0, 0, 0.1, 0], [1, 1, 0.1, 1.]])\n\nr = jaccard_distance(\n    K.variable(y_true),\n    K.variable(y_pred),\n).eval(session=K.get_session())\nprint(\'jaccard_distance_loss\', r)\nassert r[0] < r[1]\nassert r[1] < r[2]\n\nr = keras.losses.binary_crossentropy(\n    K.variable(y_true),\n    K.variable(y_pred),\n).eval(session=K.get_session())\nprint(\'binary_crossentropy\', r)\nprint(\'binary_crossentropy_scaled\', r / r.max())\nassert r[0] < r[1]\nassert r[1] < r[2]\n\n""""""\nTYPE                 |Almost_right |half right |all_wrong\njaccard_distance_loss [ 0.09900928  0.89108944  3.75000238]\nbinary_crossentropy [  0.02634021   0.57564634  12.53243446]\nbinary_crossentropy_scaled [ 0.00210176  0.04593252  1.        ]\n""""""\n'"
keras_contrib/__init__.py,0,"b""from __future__ import absolute_import\nfrom . import backend\nfrom . import datasets\nfrom . import layers\nfrom . import preprocessing\nfrom . import utils\nfrom . import wrappers\nfrom . import callbacks\nfrom . import constraints\nfrom . import initializers\nfrom . import metrics\nfrom . import losses\nfrom . import optimizers\nfrom . import regularizers\n\n__version__ = '0.0.2'\n"""
tests/conftest.py,0,"b'import pytest\r\nfrom keras import backend as K\r\n\r\n\r\n@pytest.fixture(autouse=True)\r\ndef clear_session_after_test():\r\n    """"""Test wrapper to clean up after TensorFlow and CNTK tests.\r\n\r\n    This wrapper runs for all the tests in the keras test suite.\r\n    """"""\r\n    yield\r\n    if K.backend() == \'tensorflow\' or K.backend() == \'cntk\':\r\n        K.clear_session()\r\n'"
keras_contrib/activations/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .squash import squash\n'
keras_contrib/activations/squash.py,0,"b'from keras import backend as K\n\n\ndef squash(x, axis=-1):\n    """"""\n    Squash activation function (generally used in Capsule layers).\n    """"""\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n    return scale * x\n'"
keras_contrib/applications/__init__.py,0,"b'from .densenet import DenseNet\r\nfrom .resnet import ResNet, ResNet18, ResNet34, ResNet50, ResNet101, ResNet152\r\nfrom .wide_resnet import WideResidualNetwork\r\nfrom .nasnet import NASNet, NASNetLarge, NASNetMobile\r\n'"
keras_contrib/applications/densenet.py,0,"b'# -*- coding: utf-8 -*-\n\'\'\'DenseNet and DenseNet-FCN models for Keras.\n\nDenseNet is a network architecture where each layer is directly connected\nto every other layer in a feed-forward fashion (within each dense block).\nFor each layer, the feature maps of all preceding layers are treated as\nseparate inputs whereas its own feature maps are passed on as inputs to\nall subsequent layers. This connectivity pattern yields state-of-the-art\naccuracies on CIFAR10/100 (with or without data augmentation) and SVHN.\nOn the large scale ILSVRC 2012 (ImageNet) dataset, DenseNet achieves a\nsimilar accuracy as ResNet, but using less than half the amount of\nparameters and roughly half the number of FLOPs.\n\nDenseNets support any input image size of 32x32 or greater, and are thus\nsuited for CIFAR-10 or CIFAR-100 datasets. There are two types of DenseNets,\none suited for smaller images (DenseNet) and one suited for ImageNet,\ncalled DenseNetImageNet. They are differentiated by the strided convolution\nand pooling operations prior to the initial dense block.\n\nThe following table describes the size and accuracy of DenseNetImageNet models\non the ImageNet dataset (single crop), for which weights are provided:\n------------------------------------------------------------------------------------\n    Model type      | ImageNet Acc (Top 1)  |  ImageNet Acc (Top 5) |  Params (M)  |\n------------------------------------------------------------------------------------\n|   DenseNet-121    |    25.02 %            |        7.71 %         |     8.0      |\n|   DenseNet-169    |    23.80 %            |        6.85 %         |     14.3     |\n|   DenseNet-201    |    22.58 %            |        6.34 %         |     20.2     |\n|   DenseNet-161    |    22.20 %            |         -   %         |     28.9     |\n------------------------------------------------------------------------------------\n\nDenseNets can be extended to image segmentation tasks as described in the\npaper ""The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for\nSemantic Segmentation"". Here, the dense blocks are arranged and concatenated\nwith long skip connections for state of the art performance on the CamVid dataset.\n\n# Reference\n- [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)\n- [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic\n   Segmentation](https://arxiv.org/pdf/1611.09326.pdf)\n\nThis implementation is based on the following reference code:\n - https://github.com/gpleiss/efficient_densenet_pytorch\n - https://github.com/liuzhuang13/DenseNet\n\n\'\'\'\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Activation\nfrom keras.layers import Reshape\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import UpSampling2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import AveragePooling2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import Input\nfrom keras.layers import concatenate\nfrom keras.layers import BatchNormalization\nfrom keras.regularizers import l2\nfrom keras.utils.layer_utils import convert_all_kernels_in_model\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras_applications.imagenet_utils import _obtain_input_shape\nfrom keras.applications.imagenet_utils import preprocess_input as _preprocess_input\nimport keras.backend as K\n\nfrom keras_contrib.layers import SubPixelUpscaling\n\nDENSENET_121_WEIGHTS_PATH = (r\'https://github.com/titu1994/DenseNet/releases/download\'\n                             r\'/v3.0/DenseNet-BC-121-32.h5\')\nDENSENET_161_WEIGHTS_PATH = (r\'https://github.com/titu1994/DenseNet/releases/download\'\n                             r\'/v3.0/DenseNet-BC-161-48.h5\')\nDENSENET_169_WEIGHTS_PATH = (r\'https://github.com/titu1994/DenseNet/releases/download\'\n                             r\'/v3.0/DenseNet-BC-169-32.h5\')\nDENSENET_121_WEIGHTS_PATH_NO_TOP = (r\'https://github.com/titu1994/DenseNet/releases/\'\n                                    r\'download/v3.0/DenseNet-BC-121-32-no-top.h5\')\nDENSENET_161_WEIGHTS_PATH_NO_TOP = (r\'https://github.com/titu1994/DenseNet/releases/\'\n                                    r\'download/v3.0/DenseNet-BC-161-48-no-top.h5\')\nDENSENET_169_WEIGHTS_PATH_NO_TOP = (r\'https://github.com/titu1994/DenseNet/releases/\'\n                                    r\'download/v3.0/DenseNet-BC-169-32-no-top.h5\')\n\n\ndef preprocess_input(x, data_format=None):\n    """"""Preprocesses a tensor encoding a batch of images.\n\n    # Arguments\n        x: input Numpy tensor, 4D.\n        data_format: data format of the image tensor.\n\n    # Returns\n        Preprocessed tensor.\n    """"""\n    x = _preprocess_input(x, data_format=data_format)\n    x *= 0.017  # scale values\n    return x\n\n\ndef DenseNet(input_shape=None,\n             depth=40,\n             nb_dense_block=3,\n             growth_rate=12,\n             nb_filter=-1,\n             nb_layers_per_block=-1,\n             bottleneck=False,\n             reduction=0.0,\n             dropout_rate=0.0,\n             weight_decay=1e-4,\n             subsample_initial_block=False,\n             include_top=True,\n             weights=None,\n             input_tensor=None,\n             pooling=None,\n             classes=10,\n             activation=\'softmax\',\n             transition_pooling=\'avg\'):\n    \'\'\'Instantiate the DenseNet architecture.\n\n    The model and the weights are compatible with both\n    TensorFlow and Theano. The dimension ordering\n    convention used by the model is the one\n    specified in your Keras config file.\n\n    # Arguments\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(224, 224, 3)` (with `channels_last` dim ordering)\n            or `(3, 224, 224)` (with `channels_first` dim ordering).\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 8.\n            E.g. `(224, 224, 3)` would be one valid value.\n        depth: number or layers in the DenseNet\n        nb_dense_block: number of dense blocks to add to end\n        growth_rate: number of filters to add per dense block\n        nb_filter: initial number of filters. -1 indicates initial\n            number of filters will default to 2 * growth_rate\n        nb_layers_per_block: number of layers in each dense block.\n            Can be a -1, positive integer or a list.\n            If -1, calculates nb_layer_per_block from the network depth.\n            If positive integer, a set number of layers per dense block.\n            If list, nb_layer is used as provided. Note that list size must\n            be nb_dense_block\n        bottleneck: flag to add bottleneck blocks in between dense blocks\n        reduction: reduction factor of transition blocks.\n            Note : reduction value is inverted to compute compression.\n        dropout_rate: dropout rate\n        weight_decay: weight decay rate\n        subsample_initial_block: Changes model type to suit different datasets.\n            Should be set to True for ImageNet, and False for CIFAR datasets.\n            When set to True, the initial convolution will be strided and\n            adds a MaxPooling2D before the initial dense block.\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        weights: one of `None` (random initialization) or\n            \'imagenet\' (pre-training on ImageNet)..\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model\n                will be the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a\n                2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n        activation: Type of activation at the top layer. Can be one of\n            \'softmax\' or \'sigmoid\'. Note that if sigmoid is used,\n             classes must be 1.\n        transition_pooling: `avg` for avg pooling (default), `max` for max pooling,\n            None for no pooling during scale transition blocks. Please note that this\n            default differs from the DenseNetFCN paper in accordance with the DenseNet\n            paper.\n\n    # Returns\n        A Keras model instance.\n\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n    \'\'\'\n\n    if weights not in {\'imagenet\', None}:\n        raise ValueError(\'The `weights` argument should be either \'\n                         \'`None` (random initialization) or `imagenet` \'\n                         \'(pre-training on ImageNet).\')\n\n    if weights == \'imagenet\' and include_top and classes != 1000:\n        raise ValueError(\'If using `weights` as ImageNet with `include_top` \'\n                         \'as true, `classes` should be 1000\')\n\n    if activation not in [\'softmax\', \'sigmoid\']:\n        raise ValueError(\'activation must be one of ""softmax"" or ""sigmoid""\')\n\n    if activation == \'sigmoid\' and classes != 1:\n        raise ValueError(\'sigmoid activation can only be used when classes = 1\')\n\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=32,\n                                      min_size=8,\n                                      data_format=K.image_data_format(),\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    x = __create_dense_net(classes, img_input, include_top, depth, nb_dense_block,\n                           growth_rate, nb_filter, nb_layers_per_block, bottleneck,\n                           reduction, dropout_rate, weight_decay,\n                           subsample_initial_block, pooling, activation,\n                           transition_pooling)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name=\'densenet\')\n\n    # load weights\n    if weights == \'imagenet\':\n        weights_loaded = False\n\n        if ((depth == 121) and (nb_dense_block == 4) and (growth_rate == 32) and\n                (nb_filter == 64) and (bottleneck is True) and (reduction == 0.5) and\n                subsample_initial_block):\n            if include_top:\n                weights_path = get_file(\'DenseNet-BC-121-32.h5\',\n                                        DENSENET_121_WEIGHTS_PATH,\n                                        cache_subdir=\'models\',\n                                        md5_hash=\'a439dd41aa672aef6daba4ee1fd54abd\')\n            else:\n                weights_path = get_file(\'DenseNet-BC-121-32-no-top.h5\',\n                                        DENSENET_121_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\',\n                                        md5_hash=\'55e62a6358af8a0af0eedf399b5aea99\')\n            model.load_weights(weights_path, by_name=True)\n            weights_loaded = True\n\n        if ((depth == 161) and (nb_dense_block == 4) and (growth_rate == 48) and\n                (nb_filter == 96) and (bottleneck is True) and (reduction == 0.5) and\n                subsample_initial_block):\n            if include_top:\n                weights_path = get_file(\'DenseNet-BC-161-48.h5\',\n                                        DENSENET_161_WEIGHTS_PATH,\n                                        cache_subdir=\'models\',\n                                        md5_hash=\'6c326cf4fbdb57d31eff04333a23fcca\')\n            else:\n                weights_path = get_file(\'DenseNet-BC-161-48-no-top.h5\',\n                                        DENSENET_161_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\',\n                                        md5_hash=\'1a9476b79f6b7673acaa2769e6427b92\')\n            model.load_weights(weights_path, by_name=True)\n            weights_loaded = True\n\n        if ((depth == 169) and (nb_dense_block == 4) and (growth_rate == 32) and\n                (nb_filter == 64) and (bottleneck is True) and (reduction == 0.5) and\n                subsample_initial_block):\n            if include_top:\n                weights_path = get_file(\'DenseNet-BC-169-32.h5\',\n                                        DENSENET_169_WEIGHTS_PATH,\n                                        cache_subdir=\'models\',\n                                        md5_hash=\'914869c361303d2e39dec640b4e606a6\')\n            else:\n                weights_path = get_file(\'DenseNet-BC-169-32-no-top.h5\',\n                                        DENSENET_169_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir=\'models\',\n                                        md5_hash=\'89c19e8276cfd10585d5fadc1df6859e\')\n            model.load_weights(weights_path, by_name=True)\n            weights_loaded = True\n\n        if weights_loaded:\n            if K.backend() == \'theano\':\n                convert_all_kernels_in_model(model)\n\n            if ((K.image_data_format() == \'channels_first\') and\n                    (K.backend() == \'tensorflow\')):\n                warnings.warn(\'You are using the TensorFlow backend, yet you \'\n                              \'are using the Theano \'\n                              \'image data format convention \'\n                              \'(`image_data_format=""channels_first""`). \'\n                              \'For best performance, set \'\n                              \'`image_data_format=""channels_last""` in \'\n                              \'your Keras config \'\n                              \'at ~/.keras/keras.json.\')\n\n            print(""Weights for the model were loaded successfully"")\n\n    return model\n\n\ndef DenseNetFCN(input_shape, nb_dense_block=5, growth_rate=16, nb_layers_per_block=4,\n                reduction=0.0, dropout_rate=0.0, weight_decay=1E-4,\n                init_conv_filters=48, include_top=True, weights=None, input_tensor=None,\n                classes=1, activation=\'softmax\', upsampling_conv=128,\n                upsampling_type=\'deconv\', early_transition=False,\n                transition_pooling=\'max\', initial_kernel_size=(3, 3)):\n    \'\'\'Instantiate the DenseNet FCN architecture.\n        Note that when using TensorFlow,\n        for best performance you should set\n        `image_data_format=\'channels_last\'` in your Keras config\n        at ~/.keras/keras.json.\n        # Arguments\n            nb_dense_block: number of dense blocks to add to end (generally = 3)\n            growth_rate: number of filters to add per dense block\n            nb_layers_per_block: number of layers in each dense block.\n                Can be a positive integer or a list.\n                If positive integer, a set number of layers per dense block.\n                If list, nb_layer is used as provided. Note that list size must\n                be (nb_dense_block + 1)\n            reduction: reduction factor of transition blocks.\n                Note : reduction value is inverted to compute compression.\n            dropout_rate: dropout rate\n            weight_decay: weight decay factor\n            init_conv_filters: number of layers in the initial convolution layer\n            include_top: whether to include the fully-connected\n                layer at the top of the network.\n            weights: one of `None` (random initialization) or\n                \'cifar10\' (pre-training on CIFAR-10)..\n            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n                to use as image input for the model.\n            input_shape: optional shape tuple, only to be specified\n                if `include_top` is False (otherwise the input shape\n                has to be `(32, 32, 3)` (with `channels_last` dim ordering)\n                or `(3, 32, 32)` (with `channels_first` dim ordering).\n                It should have exactly 3 inputs channels,\n                and width and height should be no smaller than 8.\n                E.g. `(200, 200, 3)` would be one valid value.\n            classes: optional number of classes to classify images\n                into, only to be specified if `include_top` is True, and\n                if no `weights` argument is specified.\n            activation: Type of activation at the top layer. Can be one of \'softmax\'\n                or \'sigmoid\'. Note that if sigmoid is used, classes must be 1.\n            upsampling_conv: number of convolutional layers in upsampling via subpixel\n                convolution\n            upsampling_type: Can be one of \'deconv\', \'upsampling\' and\n                \'subpixel\'. Defines type of upsampling algorithm used.\n            batchsize: Fixed batch size. This is a temporary requirement for\n                computation of output shape in the case of Deconvolution2D layers.\n                Parameter will be removed in next iteration of Keras, which infers\n                output shape of deconvolution layers automatically.\n            early_transition: Start with an extra initial transition down and end with\n                an extra transition up to reduce the network size.\n            initial_kernel_size: The first Conv2D kernel might vary in size based on the\n                application, this parameter makes it configurable.\n\n        # Returns\n            A Keras model instance.\n    \'\'\'\n\n    if weights not in {None}:\n        raise ValueError(\'The `weights` argument should be \'\n                         \'`None` (random initialization) as no \'\n                         \'model weights are provided.\')\n\n    upsampling_type = upsampling_type.lower()\n\n    if upsampling_type not in [\'upsampling\', \'deconv\', \'subpixel\']:\n        raise ValueError(\'Parameter ""upsampling_type"" must be one of ""upsampling"", \'\n                         \'""deconv"" or ""subpixel"".\')\n\n    if input_shape is None:\n        raise ValueError(\'For fully convolutional models, \'\n                         \'input shape must be supplied.\')\n\n    if type(nb_layers_per_block) is not list and nb_dense_block < 1:\n        raise ValueError(\'Number of dense layers per block must be greater than 1. \'\n                         \'Argument value was %d.\' % nb_layers_per_block)\n\n    if activation not in [\'softmax\', \'sigmoid\']:\n        raise ValueError(\'activation must be one of ""softmax"" or ""sigmoid""\')\n\n    if activation == \'sigmoid\' and classes != 1:\n        raise ValueError(\'sigmoid activation can only be used when classes = 1\')\n\n    # Determine proper input shape\n    min_size = 2 ** nb_dense_block\n\n    if K.image_data_format() == \'channels_first\':\n        if input_shape is not None:\n            if ((input_shape[1] is not None and input_shape[1] < min_size) or\n                    (input_shape[2] is not None and input_shape[2] < min_size)):\n                raise ValueError(\'Input size must be at least \' +\n                                 str(min_size) + \'x\' + str(min_size) +\n                                 \', got `input_shape=\' + str(input_shape) + \'`\')\n        else:\n            input_shape = (classes, None, None)\n    else:\n        if input_shape is not None:\n            if ((input_shape[0] is not None and input_shape[0] < min_size) or\n                    (input_shape[1] is not None and input_shape[1] < min_size)):\n                raise ValueError(\'Input size must be at least \' +\n                                 str(min_size) + \'x\' + str(min_size) +\n                                 \', got `input_shape=\' + str(input_shape) + \'`\')\n        else:\n            input_shape = (None, None, classes)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    x = __create_fcn_dense_net(classes, img_input, include_top, nb_dense_block,\n                               growth_rate, reduction, dropout_rate, weight_decay,\n                               nb_layers_per_block, upsampling_conv, upsampling_type,\n                               init_conv_filters, input_shape, activation,\n                               early_transition, transition_pooling,\n                               initial_kernel_size)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name=\'fcn-densenet\')\n\n    return model\n\n\ndef DenseNetImageNet121(input_shape=None,\n                        bottleneck=True,\n                        reduction=0.5,\n                        dropout_rate=0.0,\n                        weight_decay=1e-4,\n                        include_top=True,\n                        weights=\'imagenet\',\n                        input_tensor=None,\n                        pooling=None,\n                        classes=1000,\n                        activation=\'softmax\'):\n    return DenseNet(input_shape, depth=121, nb_dense_block=4, growth_rate=32,\n                    nb_filter=64, nb_layers_per_block=[6, 12, 24, 16],\n                    bottleneck=bottleneck, reduction=reduction,\n                    dropout_rate=dropout_rate, weight_decay=weight_decay,\n                    subsample_initial_block=True, include_top=include_top,\n                    weights=weights, input_tensor=input_tensor,\n                    pooling=pooling, classes=classes, activation=activation)\n\n\ndef DenseNetImageNet169(input_shape=None,\n                        bottleneck=True,\n                        reduction=0.5,\n                        dropout_rate=0.0,\n                        weight_decay=1e-4,\n                        include_top=True,\n                        weights=\'imagenet\',\n                        input_tensor=None,\n                        pooling=None,\n                        classes=1000,\n                        activation=\'softmax\'):\n    return DenseNet(input_shape, depth=169, nb_dense_block=4, growth_rate=32,\n                    nb_filter=64, nb_layers_per_block=[6, 12, 32, 32],\n                    bottleneck=bottleneck, reduction=reduction,\n                    dropout_rate=dropout_rate, weight_decay=weight_decay,\n                    subsample_initial_block=True, include_top=include_top,\n                    weights=weights, input_tensor=input_tensor,\n                    pooling=pooling, classes=classes, activation=activation)\n\n\ndef DenseNetImageNet201(input_shape=None,\n                        bottleneck=True,\n                        reduction=0.5,\n                        dropout_rate=0.0,\n                        weight_decay=1e-4,\n                        include_top=True,\n                        weights=None,\n                        input_tensor=None,\n                        pooling=None,\n                        classes=1000,\n                        activation=\'softmax\'):\n    return DenseNet(input_shape, depth=201, nb_dense_block=4, growth_rate=32,\n                    nb_filter=64, nb_layers_per_block=[6, 12, 48, 32],\n                    bottleneck=bottleneck, reduction=reduction,\n                    dropout_rate=dropout_rate, weight_decay=weight_decay,\n                    subsample_initial_block=True, include_top=include_top,\n                    weights=weights, input_tensor=input_tensor,\n                    pooling=pooling, classes=classes, activation=activation)\n\n\ndef DenseNetImageNet264(input_shape=None,\n                        bottleneck=True,\n                        reduction=0.5,\n                        dropout_rate=0.0,\n                        weight_decay=1e-4,\n                        include_top=True,\n                        weights=None,\n                        input_tensor=None,\n                        pooling=None,\n                        classes=1000,\n                        activation=\'softmax\'):\n    return DenseNet(input_shape, depth=264, nb_dense_block=4, growth_rate=32,\n                    nb_filter=64, nb_layers_per_block=[6, 12, 64, 48],\n                    bottleneck=bottleneck, reduction=reduction,\n                    dropout_rate=dropout_rate, weight_decay=weight_decay,\n                    subsample_initial_block=True, include_top=include_top,\n                    weights=weights, input_tensor=input_tensor,\n                    pooling=pooling, classes=classes, activation=activation)\n\n\ndef DenseNetImageNet161(input_shape=None,\n                        bottleneck=True,\n                        reduction=0.5,\n                        dropout_rate=0.0,\n                        weight_decay=1e-4,\n                        include_top=True,\n                        weights=\'imagenet\',\n                        input_tensor=None,\n                        pooling=None,\n                        classes=1000,\n                        activation=\'softmax\'):\n    return DenseNet(input_shape, depth=161, nb_dense_block=4, growth_rate=48,\n                    nb_filter=96, nb_layers_per_block=[6, 12, 36, 24],\n                    bottleneck=bottleneck, reduction=reduction,\n                    dropout_rate=dropout_rate, weight_decay=weight_decay,\n                    subsample_initial_block=True, include_top=include_top,\n                    weights=weights, input_tensor=input_tensor,\n                    pooling=pooling, classes=classes, activation=activation)\n\n\ndef name_or_none(prefix, name):\n    return prefix + name if (prefix is not None and name is not None) else None\n\n\ndef __conv_block(ip, nb_filter, bottleneck=False, dropout_rate=None,\n                 weight_decay=1e-4, block_prefix=None):\n    \'\'\'\n    Adds a convolution layer (with batch normalization and relu),\n    and optionally a bottleneck layer.\n\n    # Arguments\n        ip: Input tensor\n        nb_filter: integer, the dimensionality of the output space\n            (i.e. the number output of filters in the convolution)\n        bottleneck: if True, adds a bottleneck convolution block\n        dropout_rate: dropout rate\n        weight_decay: weight decay factor\n        block_prefix: str, for unique layer naming\n\n     # Input shape\n        4D tensor with shape:\n        `(samples, channels, rows, cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, rows, cols, channels)` if data_format=\'channels_last\'.\n\n    # Output shape\n        4D tensor with shape:\n        `(samples, filters, new_rows, new_cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, new_rows, new_cols, filters)` if data_format=\'channels_last\'.\n        `rows` and `cols` values might have changed due to stride.\n\n    # Returns\n        output tensor of block\n    \'\'\'\n    with K.name_scope(\'ConvBlock\'):\n        concat_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5,\n                               name=name_or_none(block_prefix, \'_bn\'))(ip)\n        x = Activation(\'relu\')(x)\n\n        if bottleneck:\n            inter_channel = nb_filter * 4\n\n            x = Conv2D(inter_channel, (1, 1), kernel_initializer=\'he_normal\',\n                       padding=\'same\', use_bias=False,\n                       kernel_regularizer=l2(weight_decay),\n                       name=name_or_none(block_prefix, \'_bottleneck_conv2D\'))(x)\n            x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5,\n                                   name=name_or_none(block_prefix, \'_bottleneck_bn\'))(x)\n            x = Activation(\'relu\')(x)\n\n        x = Conv2D(nb_filter, (3, 3), kernel_initializer=\'he_normal\', padding=\'same\',\n                   use_bias=False, name=name_or_none(block_prefix, \'_conv2D\'))(x)\n        if dropout_rate:\n            x = Dropout(dropout_rate)(x)\n\n    return x\n\n\ndef __dense_block(x, nb_layers, nb_filter, growth_rate, bottleneck=False,\n                  dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True,\n                  return_concat_list=False, block_prefix=None):\n    \'\'\'\n    Build a dense_block where the output of each conv_block is fed\n    to subsequent ones\n\n    # Arguments\n        x: input keras tensor\n        nb_layers: the number of conv_blocks to append to the model\n        nb_filter: integer, the dimensionality of the output space\n            (i.e. the number output of filters in the convolution)\n        growth_rate: growth rate of the dense block\n        bottleneck: if True, adds a bottleneck convolution block to\n            each conv_block\n        dropout_rate: dropout rate\n        weight_decay: weight decay factor\n        grow_nb_filters: if True, allows number of filters to grow\n        return_concat_list: set to True to return the list of\n            feature maps along with the actual output\n        block_prefix: str, for block unique naming\n\n    # Return\n        If return_concat_list is True, returns a list of the output\n        keras tensor, the number of filters and a list of all the\n        dense blocks added to the keras tensor\n\n        If return_concat_list is False, returns a list of the output\n        keras tensor and the number of filters\n    \'\'\'\n    with K.name_scope(\'DenseBlock\'):\n        concat_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n        x_list = [x]\n\n        for i in range(nb_layers):\n            cb = __conv_block(x, growth_rate, bottleneck, dropout_rate, weight_decay,\n                              block_prefix=name_or_none(block_prefix, \'_%i\' % i))\n            x_list.append(cb)\n\n            x = concatenate([x, cb], axis=concat_axis)\n\n            if grow_nb_filters:\n                nb_filter += growth_rate\n\n        if return_concat_list:\n            return x, nb_filter, x_list\n        else:\n            return x, nb_filter\n\n\ndef __transition_block(ip, nb_filter, compression=1.0, weight_decay=1e-4,\n                       block_prefix=None, transition_pooling=\'max\'):\n    \'\'\'\n    Adds a pointwise convolution layer (with batch normalization and relu),\n    and an average pooling layer. The number of output convolution filters\n    can be reduced by appropriately reducing the compression parameter.\n\n    # Arguments\n        ip: input keras tensor\n        nb_filter: integer, the dimensionality of the output space\n            (i.e. the number output of filters in the convolution)\n        compression: calculated as 1 - reduction. Reduces the number\n            of feature maps in the transition block.\n        weight_decay: weight decay factor\n        block_prefix: str, for block unique naming\n\n    # Input shape\n        4D tensor with shape:\n        `(samples, channels, rows, cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, rows, cols, channels)` if data_format=\'channels_last\'.\n\n    # Output shape\n        4D tensor with shape:\n        `(samples, nb_filter * compression, rows / 2, cols / 2)`\n        if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, rows / 2, cols / 2, nb_filter * compression)`\n        if data_format=\'channels_last\'.\n\n    # Returns\n        a keras tensor\n    \'\'\'\n    with K.name_scope(\'Transition\'):\n        concat_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5,\n                               name=name_or_none(block_prefix, \'_bn\'))(ip)\n        x = Activation(\'relu\')(x)\n        x = Conv2D(int(nb_filter * compression), (1, 1), kernel_initializer=\'he_normal\',\n                   padding=\'same\', use_bias=False, kernel_regularizer=l2(weight_decay),\n                   name=name_or_none(block_prefix, \'_conv2D\'))(x)\n        if transition_pooling == \'avg\':\n            x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n        elif transition_pooling == \'max\':\n            x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n        return x\n\n\ndef __transition_up_block(ip, nb_filters, type=\'deconv\', weight_decay=1E-4,\n                          block_prefix=None):\n    \'\'\'Adds an upsampling block. Upsampling operation relies on the the type parameter.\n\n    # Arguments\n        ip: input keras tensor\n        nb_filters: integer, the dimensionality of the output space\n            (i.e. the number output of filters in the convolution)\n        type: can be \'upsampling\', \'subpixel\', \'deconv\'. Determines\n            type of upsampling performed\n        weight_decay: weight decay factor\n        block_prefix: str, for block unique naming\n\n    # Input shape\n        4D tensor with shape:\n        `(samples, channels, rows, cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, rows, cols, channels)` if data_format=\'channels_last\'.\n\n    # Output shape\n        4D tensor with shape:\n        `(samples, nb_filter, rows * 2, cols * 2)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, rows * 2, cols * 2, nb_filter)` if data_format=\'channels_last\'.\n\n    # Returns\n        a keras tensor\n    \'\'\'\n    with K.name_scope(\'TransitionUp\'):\n\n        if type == \'upsampling\':\n            x = UpSampling2D(name=name_or_none(block_prefix, \'_upsampling\'))(ip)\n        elif type == \'subpixel\':\n            x = Conv2D(nb_filters, (3, 3), activation=\'relu\', padding=\'same\',\n                       kernel_regularizer=l2(weight_decay), use_bias=False,\n                       kernel_initializer=\'he_normal\',\n                       name=name_or_none(block_prefix, \'_conv2D\'))(ip)\n            x = SubPixelUpscaling(scale_factor=2,\n                                  name=name_or_none(block_prefix, \'_subpixel\'))(x)\n            x = Conv2D(nb_filters, (3, 3), activation=\'relu\', padding=\'same\',\n                       kernel_regularizer=l2(weight_decay), use_bias=False,\n                       kernel_initializer=\'he_normal\',\n                       name=name_or_none(block_prefix, \'_conv2D\'))(x)\n        else:\n            x = Conv2DTranspose(nb_filters, (3, 3), activation=\'relu\', padding=\'same\',\n                                strides=(2, 2), kernel_initializer=\'he_normal\',\n                                kernel_regularizer=l2(weight_decay),\n                                name=name_or_none(block_prefix, \'_conv2DT\'))(ip)\n        return x\n\n\ndef __create_dense_net(nb_classes, img_input, include_top, depth=40, nb_dense_block=3,\n                       growth_rate=12, nb_filter=-1, nb_layers_per_block=-1,\n                       bottleneck=False, reduction=0.0, dropout_rate=None,\n                       weight_decay=1e-4, subsample_initial_block=False, pooling=None,\n                       activation=\'softmax\', transition_pooling=\'avg\'):\n    \'\'\' Build the DenseNet model\n\n    # Arguments\n        nb_classes: number of classes\n        img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n        include_top: flag to include the final Dense layer\n        depth: number or layers\n        nb_dense_block: number of dense blocks to add to end (generally = 3)\n        growth_rate: number of filters to add per dense block\n        nb_filter: initial number of filters. Default -1 indicates initial number\n            of filters is 2 * growth_rate\n        nb_layers_per_block: number of layers in each dense block.\n                Can be a -1, positive integer or a list.\n                If -1, calculates nb_layer_per_block from the depth of the network.\n                If positive integer, a set number of layers per dense block.\n                If list, nb_layer is used as provided. Note that list size must\n                be (nb_dense_block + 1)\n        bottleneck: add bottleneck blocks\n        reduction: reduction factor of transition blocks. Note : reduction value is\n            inverted to compute compression\n        dropout_rate: dropout rate\n        weight_decay: weight decay rate\n        subsample_initial_block: Changes model type to suit different datasets.\n            Should be set to True for ImageNet, and False for CIFAR datasets.\n            When set to True, the initial convolution will be strided and\n            adds a MaxPooling2D before the initial dense block.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model\n                will be the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a\n                2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        activation: Type of activation at the top layer. Can be one of \'softmax\' or\n            \'sigmoid\'. Note that if sigmoid is used, classes must be 1.\n        transition_pooling: `avg` for avg pooling (default), `max` for max pooling,\n            None for no pooling during scale transition blocks. Please note that this\n            default differs from the DenseNetFCN paper in accordance with the DenseNet\n            paper.\n\n    # Returns\n        a keras tensor\n\n    # Raises\n        ValueError: in case of invalid argument for `reduction`\n            or `nb_dense_block`\n    \'\'\'\n    with K.name_scope(\'DenseNet\'):\n        concat_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n        if reduction != 0.0:\n            if not (reduction <= 1.0 and reduction > 0.0):\n                raise ValueError(\'`reduction` value must lie between 0.0 and 1.0\')\n\n        # layers in each dense block\n        if type(nb_layers_per_block) is list or type(nb_layers_per_block) is tuple:\n            nb_layers = list(nb_layers_per_block)  # Convert tuple to list\n\n            if len(nb_layers) != nb_dense_block:\n                raise ValueError(\'If `nb_dense_block` is a list, its length must match \'\n                                 \'the number of layers provided by `nb_layers`.\')\n\n            final_nb_layer = nb_layers[-1]\n            nb_layers = nb_layers[:-1]\n        else:\n            if nb_layers_per_block == -1:\n                assert (depth - 4) % 3 == 0, (\'Depth must be 3 N + 4 \'\n                                              \'if nb_layers_per_block == -1\')\n                count = int((depth - 4) / 3)\n\n                if bottleneck:\n                    count = count // 2\n\n                nb_layers = [count for _ in range(nb_dense_block)]\n                final_nb_layer = count\n            else:\n                final_nb_layer = nb_layers_per_block\n                nb_layers = [nb_layers_per_block] * nb_dense_block\n\n        # compute initial nb_filter if -1, else accept users initial nb_filter\n        if nb_filter <= 0:\n            nb_filter = 2 * growth_rate\n\n        # compute compression factor\n        compression = 1.0 - reduction\n\n        # Initial convolution\n        if subsample_initial_block:\n            initial_kernel = (7, 7)\n            initial_strides = (2, 2)\n        else:\n            initial_kernel = (3, 3)\n            initial_strides = (1, 1)\n\n        x = Conv2D(nb_filter, initial_kernel, kernel_initializer=\'he_normal\',\n                   padding=\'same\', name=\'initial_conv2D\', strides=initial_strides,\n                   use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n\n        if subsample_initial_block:\n            x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5,\n                                   name=\'initial_bn\')(x)\n            x = Activation(\'relu\')(x)\n            x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n\n        # Add dense blocks\n        for block_idx in range(nb_dense_block - 1):\n            x, nb_filter = __dense_block(x, nb_layers[block_idx], nb_filter,\n                                         growth_rate, bottleneck=bottleneck,\n                                         dropout_rate=dropout_rate,\n                                         weight_decay=weight_decay,\n                                         block_prefix=\'dense_%i\' % block_idx)\n            # add transition_block\n            x = __transition_block(x, nb_filter, compression=compression,\n                                   weight_decay=weight_decay,\n                                   block_prefix=\'tr_%i\' % block_idx,\n                                   transition_pooling=transition_pooling)\n            nb_filter = int(nb_filter * compression)\n\n        # The last dense_block does not have a transition_block\n        x, nb_filter = __dense_block(x, final_nb_layer, nb_filter, growth_rate,\n                                     bottleneck=bottleneck, dropout_rate=dropout_rate,\n                                     weight_decay=weight_decay,\n                                     block_prefix=\'dense_%i\' % (nb_dense_block - 1))\n\n        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5, name=\'final_bn\')(x)\n        x = Activation(\'relu\')(x)\n\n        if include_top:\n            if pooling == \'avg\':\n                x = GlobalAveragePooling2D()(x)\n            elif pooling == \'max\':\n                x = GlobalMaxPooling2D()(x)\n            x = Dense(nb_classes, activation=activation)(x)\n        else:\n            if pooling == \'avg\':\n                x = GlobalAveragePooling2D()(x)\n            elif pooling == \'max\':\n                x = GlobalMaxPooling2D()(x)\n\n        return x\n\n\ndef __create_fcn_dense_net(nb_classes, img_input, include_top, nb_dense_block=5,\n                           growth_rate=12, reduction=0.0, dropout_rate=None,\n                           weight_decay=1e-4, nb_layers_per_block=4,\n                           nb_upsampling_conv=128, upsampling_type=\'deconv\',\n                           init_conv_filters=48, input_shape=None, activation=\'softmax\',\n                           early_transition=False, transition_pooling=\'max\',\n                           initial_kernel_size=(3, 3)):\n    \'\'\' Build the DenseNet-FCN model\n\n    # Arguments\n        nb_classes: number of classes\n        img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n        include_top: flag to include the final Dense layer\n        nb_dense_block: number of dense blocks to add to end (generally = 3)\n        growth_rate: number of filters to add per dense block\n        reduction: reduction factor of transition blocks. Note : reduction value\n            is inverted to compute compression\n        dropout_rate: dropout rate\n        weight_decay: weight decay\n        nb_layers_per_block: number of layers in each dense block.\n            Can be a positive integer or a list.\n            If positive integer, a set number of layers per dense block.\n            If list, nb_layer is used as provided. Note that list size must\n            be (nb_dense_block + 1)\n        nb_upsampling_conv: number of convolutional layers in upsampling via subpixel\n            convolution\n        upsampling_type: Can be one of \'upsampling\', \'deconv\' and \'subpixel\'. Defines\n            type of upsampling algorithm used.\n        input_shape: Only used for shape inference in fully convolutional networks.\n        activation: Type of activation at the top layer. Can be one of \'softmax\' or\n            \'sigmoid\'. Note that if sigmoid is used, classes must be 1.\n        early_transition: Start with an extra initial transition down and end with an\n            extra transition up to reduce the network size.\n        transition_pooling: \'max\' for max pooling (default), \'avg\' for average pooling,\n            None for no pooling. Please note that this default differs from the DenseNet\n            paper in accordance with the DenseNetFCN paper.\n        initial_kernel_size: The first Conv2D kernel might vary in size based on the\n            application, this parameter makes it configurable.\n\n    # Returns\n        a keras tensor\n\n    # Raises\n        ValueError: in case of invalid argument for `reduction`,\n            `nb_dense_block` or `nb_upsampling_conv`.\n    \'\'\'\n    with K.name_scope(\'DenseNetFCN\'):\n        concat_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n        if concat_axis == 1:  # channels_first dim ordering\n            _, rows, cols = input_shape\n        else:\n            rows, cols, _ = input_shape\n\n        if reduction != 0.0:\n            if not (reduction <= 1.0 and reduction > 0.0):\n                raise ValueError(\'`reduction` value must lie between 0.0 and 1.0\')\n\n        # check if upsampling_conv has minimum number of filters minimum\n        # is set to 12, as at least 3 color channels are needed for correct upsampling\n        if not (nb_upsampling_conv > 12 and nb_upsampling_conv % 4 == 0):\n            raise ValueError(\'Parameter `nb_upsampling_conv` number of channels must \'\n                             \'be a positive number divisible by 4 and greater than 12\')\n\n        # layers in each dense block\n        if type(nb_layers_per_block) is list or type(nb_layers_per_block) is tuple:\n            nb_layers = list(nb_layers_per_block)  # Convert tuple to list\n\n            if len(nb_layers) != (nb_dense_block + 1):\n                raise ValueError(\'If `nb_dense_block` is a list, its length must be \'\n                                 \'(`nb_dense_block` + 1)\')\n\n            bottleneck_nb_layers = nb_layers[-1]\n            rev_layers = nb_layers[::-1]\n            nb_layers.extend(rev_layers[1:])\n        else:\n            bottleneck_nb_layers = nb_layers_per_block\n            nb_layers = [nb_layers_per_block] * (2 * nb_dense_block + 1)\n\n        # compute compression factor\n        compression = 1.0 - reduction\n\n        # Initial convolution\n        x = Conv2D(init_conv_filters, initial_kernel_size,\n                   kernel_initializer=\'he_normal\', padding=\'same\',\n                   name=\'initial_conv2D\', use_bias=False,\n                   kernel_regularizer=l2(weight_decay))(img_input)\n        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5, name=\'initial_bn\')(x)\n        x = Activation(\'relu\')(x)\n\n        nb_filter = init_conv_filters\n\n        skip_list = []\n\n        if early_transition:\n            x = __transition_block(x, nb_filter, compression=compression,\n                                   weight_decay=weight_decay, block_prefix=\'tr_early\',\n                                   transition_pooling=transition_pooling)\n\n        # Add dense blocks and transition down block\n        for block_idx in range(nb_dense_block):\n            x, nb_filter = __dense_block(x, nb_layers[block_idx], nb_filter,\n                                         growth_rate, dropout_rate=dropout_rate,\n                                         weight_decay=weight_decay,\n                                         block_prefix=\'dense_%i\' % block_idx)\n\n            # Skip connection\n            skip_list.append(x)\n\n            # add transition_block\n            x = __transition_block(x, nb_filter, compression=compression,\n                                   weight_decay=weight_decay,\n                                   block_prefix=\'tr_%i\' % block_idx,\n                                   transition_pooling=transition_pooling)\n\n            # this is calculated inside transition_down_block\n            nb_filter = int(nb_filter * compression)\n\n        # The last dense_block does not have a transition_down_block\n        # return the concatenated feature maps without the concatenation of the input\n        block_prefix = \'dense_%i\' % nb_dense_block\n        _, nb_filter, concat_list = __dense_block(x, bottleneck_nb_layers, nb_filter,\n                                                  growth_rate,\n                                                  dropout_rate=dropout_rate,\n                                                  weight_decay=weight_decay,\n                                                  return_concat_list=True,\n                                                  block_prefix=block_prefix)\n\n        skip_list = skip_list[::-1]  # reverse the skip list\n\n        # Add dense blocks and transition up block\n        for block_idx in range(nb_dense_block):\n            n_filters_keep = growth_rate * nb_layers[nb_dense_block + block_idx]\n\n            # upsampling block must upsample only the feature maps (concat_list[1:]),\n            # not the concatenation of the input with the feature maps (concat_list[0].\n            l = concatenate(concat_list[1:], axis=concat_axis)\n\n            t = __transition_up_block(l, nb_filters=n_filters_keep,\n                                      type=upsampling_type, weight_decay=weight_decay,\n                                      block_prefix=\'tr_up_%i\' % block_idx)\n\n            # concatenate the skip connection with the transition block\n            x = concatenate([t, skip_list[block_idx]], axis=concat_axis)\n\n            # Dont allow the feature map size to grow in upsampling dense blocks\n            block_layer_index = nb_dense_block + 1 + block_idx\n            block_prefix = \'dense_%i\' % (block_layer_index)\n            x_up, nb_filter, concat_list = __dense_block(x,\n                                                         nb_layers[block_layer_index],\n                                                         nb_filter=growth_rate,\n                                                         growth_rate=growth_rate,\n                                                         dropout_rate=dropout_rate,\n                                                         weight_decay=weight_decay,\n                                                         return_concat_list=True,\n                                                         grow_nb_filters=False,\n                                                         block_prefix=block_prefix)\n\n        if early_transition:\n            x_up = __transition_up_block(x_up, nb_filters=nb_filter,\n                                         type=upsampling_type,\n                                         weight_decay=weight_decay,\n                                         block_prefix=\'tr_up_early\')\n        if include_top:\n            x = Conv2D(nb_classes, (1, 1), activation=\'linear\', padding=\'same\',\n                       use_bias=False)(x_up)\n\n            if K.image_data_format() == \'channels_first\':\n                channel, row, col = input_shape\n            else:\n                row, col, channel = input_shape\n\n            x = Reshape((row * col, nb_classes))(x)\n            x = Activation(activation)(x)\n            x = Reshape((row, col, nb_classes))(x)\n        else:\n            x = x_up\n\n        return x\n'"
keras_contrib/applications/nasnet.py,0,"b'""""""Collection of NASNet models\n\nThe reference paper:\n - [Learning Transferable Architectures for Scalable Image Recognition]\n    (https://arxiv.org/abs/1707.07012)\n\nThe reference implementation:\n1. TF Slim\n - https://github.com/tensorflow/models/blob/master/research/slim/nets/\n   nasnet/nasnet.py\n2. TensorNets\n - https://github.com/taehoonlee/tensornets/blob/master/tensornets/nasnets.py\n3. Weights\n - https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet\n""""""\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Activation\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import AveragePooling2D\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers import Conv2D\nfrom keras.layers import SeparableConv2D\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import Cropping2D\nfrom keras.layers import concatenate\nfrom keras.layers import add\nfrom keras.regularizers import l2\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras_applications.imagenet_utils import _obtain_input_shape\nfrom keras import backend as K\n\n_BN_DECAY = 0.9997\n_BN_EPSILON = 1e-3\n\nNASNET_MOBILE_WEIGHT_PATH = (\n    ""https://github.com/titu1994/Keras-NASNet/""\n    ""releases/download/v1.0/NASNet-mobile.h5"")\nNASNET_MOBILE_WEIGHT_PATH_NO_TOP = (\n    ""https://github.com/titu1994/Keras-NASNet/""\n    ""releases/download/v1.0/NASNet-mobile-no-top.h5"")\nNASNET_MOBILE_WEIGHT_PATH_WITH_AUXULARY = (\n    ""https://github.com/titu1994/Keras-NASNet/""\n    ""releases/download/v1.0/NASNet-auxiliary-mobile.h5"")\nNASNET_MOBILE_WEIGHT_PATH_WITH_AUXULARY_NO_TOP = (\n    ""https://github.com/titu1994/Keras-NASNet/""\n    ""releases/download/v1.0/NASNet-auxiliary-mobile-no-top.h5"")\nNASNET_LARGE_WEIGHT_PATH = (\n    ""https://github.com/titu1994/Keras-NASNet/releases/download/v1.1/NASNet-large.h5"")\nNASNET_LARGE_WEIGHT_PATH_NO_TOP = (\n    ""https://github.com/titu1994/Keras-NASNet/""\n    ""releases/download/v1.1/NASNet-large-no-top.h5"")\nNASNET_LARGE_WEIGHT_PATH_WITH_auxiliary = (\n    ""https://github.com/titu1994/Keras-NASNet/""\n    ""releases/download/v1.1/NASNet-auxiliary-large.h5"")\nNASNET_LARGE_WEIGHT_PATH_WITH_auxiliary_NO_TOP = (\n    ""https://github.com/titu1994/Keras-NASNet/""\n    ""releases/download/v1.1/NASNet-auxiliary-large-no-top.h5"")\n\n\ndef NASNet(input_shape=None,\n           penultimate_filters=4032,\n           nb_blocks=6,\n           stem_filters=96,\n           initial_reduction=True,\n           skip_reduction_layer_input=True,\n           use_auxiliary_branch=False,\n           filters_multiplier=2,\n           dropout=0.5,\n           weight_decay=5e-5,\n           include_top=True,\n           weights=None,\n           input_tensor=None,\n           pooling=None,\n           classes=1000,\n           default_size=None,\n           activation=\'softmax\'):\n    """"""Instantiates a NASNet architecture.\n    Note that only TensorFlow is supported for now,\n    therefore it only works with the data format\n    `image_data_format=\'channels_last\'` in your Keras config\n    at `~/.keras/keras.json`.\n\n    # Arguments\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(331, 331, 3)` for NASNetLarge or\n            `(224, 224, 3)` for NASNetMobile\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 32.\n            E.g. `(224, 224, 3)` would be one valid value.\n        penultimate_filters: number of filters in the penultimate layer.\n            NASNet models use the notation `NASNet (N @ P)`, where:\n                -   N is the number of blocks\n                -   P is the number of penultimate filters\n        nb_blocks: number of repeated blocks of the NASNet model.\n            NASNet models use the notation `NASNet (N @ P)`, where:\n                -   N is the number of blocks\n                -   P is the number of penultimate filters\n        stem_filters: number of filters in the initial stem block\n        initial_reduction: Whether to perform the reduction step at the beginning\n            end of the network. Set to `True` for CIFAR models.\n        skip_reduction_layer_input: Determines whether to skip the reduction layers\n            when calculating the previous layer to connect to.\n        use_auxiliary_branch: Whether to use the auxiliary branch during\n            training or evaluation.\n        filters_multiplier: controls the width of the network.\n            - If `filters_multiplier` < 1.0, proportionally decreases the number\n                of filters in each layer.\n            - If `filters_multiplier` > 1.0, proportionally increases the number\n                of filters in each layer.\n            - If `filters_multiplier` = 1, default number of filters from the paper\n                 are used at each layer.\n        dropout: dropout rate\n        weight_decay: l2 regularization weight\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        weights: `None` (random initialization) or\n            `imagenet` (ImageNet weights)\n        input_tensor: optional Keras tensor (i.e. output of\n            `layers.Input()`)\n            to use as image input for the model.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model\n                will be the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a\n                2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n        default_size: specifies the default image size of the model\n        activation: Type of activation at the top layer.\n            Can be one of \'softmax\' or \'sigmoid\'.\n    # Returns\n        A Keras model instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n        RuntimeError: If attempting to run this model with a\n            backend that does not support separable convolutions.\n    """"""\n    if K.backend() != \'tensorflow\':\n        raise RuntimeError(\'Only Tensorflow backend is currently supported, \'\n                           \'as other backends do not support \'\n                           \'separable convolution.\')\n\n    if weights not in {\'imagenet\', None}:\n        raise ValueError(\'The `weights` argument should be either \'\n                         \'`None` (random initialization) or `imagenet` \'\n                         \'(pre-training on ImageNet).\')\n\n    if weights == \'imagenet\' and include_top and classes != 1000:\n        raise ValueError(\'If using `weights` as ImageNet with `include_top` \'\n                         \'as true, `classes` should be 1000\')\n\n    if default_size is None:\n        default_size = 331\n\n    # Determine proper input shape and default size.\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=default_size,\n                                      min_size=32,\n                                      data_format=K.image_data_format(),\n                                      require_flatten=include_top or weights)\n\n    if K.image_data_format() != \'channels_last\':\n        warnings.warn(\'The NASNet family of models is only available \'\n                      \'for the input data format ""channels_last"" \'\n                      \'(width, height, channels). \'\n                      \'However your settings specify the default \'\n                      \'data format ""channels_first"" (channels, width, height).\'\n                      \' You should set `image_data_format=""channels_last""` \'\n                      \'in your Keras config located at ~/.keras/keras.json. \'\n                      \'The model being returned right now will expect inputs \'\n                      \'to follow the ""channels_last"" data format.\')\n        K.set_image_data_format(\'channels_last\')\n        old_data_format = \'channels_first\'\n    else:\n        old_data_format = None\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    assert penultimate_filters % 24 == 0, ""`penultimate_filters` needs to be "" \\\n                                          ""divisible by 24.""\n\n    channel_dim = 1 if K.image_data_format() == \'channels_first\' else -1\n    filters = penultimate_filters // 24\n\n    if initial_reduction:\n        x = Conv2D(stem_filters, (3, 3), strides=(2, 2), padding=\'valid\',\n                   use_bias=False, name=\'stem_conv1\', kernel_initializer=\'he_normal\',\n                   kernel_regularizer=l2(weight_decay))(img_input)\n    else:\n        x = Conv2D(stem_filters, (3, 3), strides=(1, 1), padding=\'same\', use_bias=False,\n                   name=\'stem_conv1\', kernel_initializer=\'he_normal\',\n                   kernel_regularizer=l2(weight_decay))(img_input)\n\n    x = BatchNormalization(axis=channel_dim, momentum=_BN_DECAY, epsilon=_BN_EPSILON,\n                           name=\'stem_bn1\')(x)\n\n    p = None\n    if initial_reduction:  # imagenet / mobile mode\n        x, p = _reduction_A(x, p, filters // (filters_multiplier ** 2), weight_decay,\n                            id=\'stem_1\')\n        x, p = _reduction_A(x, p, filters // filters_multiplier, weight_decay,\n                            id=\'stem_2\')\n\n    for i in range(nb_blocks):\n        x, p = _normal_A(x, p, filters, weight_decay, id=\'%d\' % i)\n\n    x, p0 = _reduction_A(x, p, filters * filters_multiplier, weight_decay,\n                         id=\'reduce_%d\' % nb_blocks)\n\n    p = p0 if not skip_reduction_layer_input else p\n\n    for i in range(nb_blocks):\n        x, p = _normal_A(x, p, filters * filters_multiplier, weight_decay,\n                         id=\'%d\' % (nb_blocks + i + 1))\n\n    auxiliary_x = None\n    if not initial_reduction:  # imagenet / mobile mode\n        if use_auxiliary_branch:\n            auxiliary_x = _add_auxiliary_head(x, classes, weight_decay, pooling,\n                                              include_top, activation)\n\n    x, p0 = _reduction_A(x, p, filters * filters_multiplier ** 2, weight_decay,\n                         id=\'reduce_%d\' % (2 * nb_blocks))\n\n    if initial_reduction:  # CIFAR mode\n        if use_auxiliary_branch:\n            auxiliary_x = _add_auxiliary_head(x, classes, weight_decay, pooling,\n                                              include_top, activation)\n\n    p = p0 if not skip_reduction_layer_input else p\n\n    for i in range(nb_blocks):\n        x, p = _normal_A(x, p, filters * filters_multiplier ** 2, weight_decay,\n                         id=\'%d\' % (2 * nb_blocks + i + 1))\n\n    x = Activation(\'relu\')(x)\n\n    if include_top:\n        x = GlobalAveragePooling2D()(x)\n        x = Dropout(dropout)(x)\n        x = Dense(classes, activation=activation,\n                  kernel_regularizer=l2(weight_decay), name=\'predictions\')(x)\n    else:\n        if pooling == \'avg\':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling == \'max\':\n            x = GlobalMaxPooling2D()(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n\n    # Create model.\n    if use_auxiliary_branch:\n        model = Model(inputs, [x, auxiliary_x], name=\'NASNet_with_auxiliary\')\n    else:\n        model = Model(inputs, x, name=\'NASNet\')\n\n    # load weights\n    if weights == \'imagenet\':\n        if default_size == 224:  # mobile version\n            if include_top:\n                if use_auxiliary_branch:\n                    weight_path = NASNET_MOBILE_WEIGHT_PATH_WITH_AUXULARY\n                    model_name = \'nasnet_mobile_with_aux.h5\'\n                else:\n                    weight_path = NASNET_MOBILE_WEIGHT_PATH\n                    model_name = \'nasnet_mobile.h5\'\n            else:\n                if use_auxiliary_branch:\n                    weight_path = NASNET_MOBILE_WEIGHT_PATH_WITH_AUXULARY_NO_TOP\n                    model_name = \'nasnet_mobile_with_aux_no_top.h5\'\n                else:\n                    weight_path = NASNET_MOBILE_WEIGHT_PATH_NO_TOP\n                    model_name = \'nasnet_mobile_no_top.h5\'\n\n            weights_file = get_file(model_name, weight_path, cache_subdir=\'models\')\n            model.load_weights(weights_file, by_name=True)\n\n        elif default_size == 331:  # large version\n            if include_top:\n                if use_auxiliary_branch:\n                    weight_path = NASNET_LARGE_WEIGHT_PATH_WITH_auxiliary\n                    model_name = \'nasnet_large_with_aux.h5\'\n                else:\n                    weight_path = NASNET_LARGE_WEIGHT_PATH\n                    model_name = \'nasnet_large.h5\'\n            else:\n                if use_auxiliary_branch:\n                    weight_path = NASNET_LARGE_WEIGHT_PATH_WITH_auxiliary_NO_TOP\n                    model_name = \'nasnet_large_with_aux_no_top.h5\'\n                else:\n                    weight_path = NASNET_LARGE_WEIGHT_PATH_NO_TOP\n                    model_name = \'nasnet_large_no_top.h5\'\n\n            weights_file = get_file(model_name, weight_path, cache_subdir=\'models\')\n            model.load_weights(weights_file, by_name=True)\n\n        else:\n            raise ValueError(\'ImageNet weights can only be loaded on NASNetLarge \'\n                             \'or NASNetMobile\')\n\n    if old_data_format:\n        K.set_image_data_format(old_data_format)\n\n    return model\n\n\ndef NASNetLarge(input_shape=(331, 331, 3),\n                dropout=0.5,\n                weight_decay=5e-5,\n                use_auxiliary_branch=False,\n                include_top=True,\n                weights=\'imagenet\',\n                input_tensor=None,\n                pooling=None,\n                classes=1000,\n                activation=\'softmax\'):\n    """"""Instantiates a NASNet architecture in ImageNet mode.\n    Note that only TensorFlow is supported for now,\n    therefore it only works with the data format\n    `image_data_format=\'channels_last\'` in your Keras config\n    at `~/.keras/keras.json`.\n\n    # Arguments\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(331, 331, 3)` for NASNetLarge.\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 32.\n            E.g. `(224, 224, 3)` would be one valid value.\n        use_auxiliary_branch: Whether to use the auxiliary branch during\n            training or evaluation.\n        dropout: dropout rate\n        weight_decay: l2 regularization weight\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        weights: `None` (random initialization) or\n            `imagenet` (ImageNet weights)\n        input_tensor: optional Keras tensor (i.e. output of\n            `layers.Input()`)\n            to use as image input for the model.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model\n                will be the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a\n                2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n        default_size: specifies the default image size of the model\n         activation: Type of activation at the top layer.\n             Can be one of \'softmax\' or \'sigmoid\'.\n    # Returns\n        A Keras model instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n        RuntimeError: If attempting to run this model with a\n            backend that does not support separable convolutions.\n    """"""\n    global _BN_DECAY, _BN_EPSILON\n    _BN_DECAY = 0.9997\n    _BN_EPSILON = 1e-3\n\n    return NASNet(input_shape,\n                  penultimate_filters=4032,\n                  nb_blocks=6,\n                  stem_filters=96,\n                  initial_reduction=True,\n                  skip_reduction_layer_input=True,\n                  use_auxiliary_branch=use_auxiliary_branch,\n                  filters_multiplier=2,\n                  dropout=dropout,\n                  weight_decay=weight_decay,\n                  include_top=include_top,\n                  weights=weights,\n                  input_tensor=input_tensor,\n                  pooling=pooling,\n                  classes=classes,\n                  default_size=331,\n                  activation=activation)\n\n\ndef NASNetMobile(input_shape=(224, 224, 3),\n                 dropout=0.5,\n                 weight_decay=4e-5,\n                 use_auxiliary_branch=False,\n                 include_top=True,\n                 weights=\'imagenet\',\n                 input_tensor=None,\n                 pooling=None,\n                 classes=1000,\n                 activation=\'softmax\'):\n    """"""Instantiates a NASNet architecture in Mobile ImageNet mode.\n    Note that only TensorFlow is supported for now,\n    therefore it only works with the data format\n    `image_data_format=\'channels_last\'` in your Keras config\n    at `~/.keras/keras.json`.\n\n    # Arguments\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(224, 224, 3)` for NASNetMobile\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 32.\n            E.g. `(224, 224, 3)` would be one valid value.\n        use_auxiliary_branch: Whether to use the auxiliary branch during\n            training or evaluation.\n        dropout: dropout rate\n        weight_decay: l2 regularization weight\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        weights: `None` (random initialization) or\n            `imagenet` (ImageNet weights)\n        input_tensor: optional Keras tensor (i.e. output of\n            `layers.Input()`)\n            to use as image input for the model.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model\n                will be the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a\n                2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n        default_size: specifies the default image size of the model\n         activation: Type of activation at the top layer.\n             Can be one of \'softmax\' or \'sigmoid\'.\n    # Returns\n        A Keras model instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n        RuntimeError: If attempting to run this model with a\n            backend that does not support separable convolutions.\n    """"""\n    global _BN_DECAY, _BN_EPSILON\n    _BN_DECAY = 0.9997\n    _BN_EPSILON = 1e-3\n\n    return NASNet(input_shape,\n                  penultimate_filters=1056,\n                  nb_blocks=4,\n                  stem_filters=32,\n                  initial_reduction=True,\n                  skip_reduction_layer_input=False,\n                  use_auxiliary_branch=use_auxiliary_branch,\n                  filters_multiplier=2,\n                  dropout=dropout,\n                  weight_decay=weight_decay,\n                  include_top=include_top,\n                  weights=weights,\n                  input_tensor=input_tensor,\n                  pooling=pooling,\n                  classes=classes,\n                  default_size=224)\n\n\ndef NASNetCIFAR(input_shape=(32, 32, 3),\n                dropout=0.0,\n                weight_decay=5e-4,\n                use_auxiliary_branch=False,\n                include_top=True,\n                weights=None,\n                input_tensor=None,\n                pooling=None,\n                classes=10,\n                activation=\'softmax\'):\n    """"""Instantiates a NASNet architecture in CIFAR mode.\n    Note that only TensorFlow is supported for now,\n    therefore it only works with the data format\n    `image_data_format=\'channels_last\'` in your Keras config\n    at `~/.keras/keras.json`.\n\n    # Arguments\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(32, 32, 3)` for NASNetMobile\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 32.\n            E.g. `(32, 32, 3)` would be one valid value.\n        use_auxiliary_branch: Whether to use the auxiliary branch during\n            training or evaluation.\n        dropout: dropout rate\n        weight_decay: l2 regularization weight\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        weights: `None` (random initialization) or\n            `imagenet` (ImageNet weights)\n        input_tensor: optional Keras tensor (i.e. output of\n            `layers.Input()`)\n            to use as image input for the model.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model\n                will be the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a\n                2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n        default_size: specifies the default image size of the model\n         activation: Type of activation at the top layer.\n             Can be one of \'softmax\' or \'sigmoid\'.\n    # Returns\n        A Keras model instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n        RuntimeError: If attempting to run this model with a\n            backend that does not support separable convolutions.\n    """"""\n    global _BN_DECAY, _BN_EPSILON\n    _BN_DECAY = 0.9\n    _BN_EPSILON = 1e-5\n\n    return NASNet(input_shape,\n                  penultimate_filters=768,\n                  nb_blocks=6,\n                  stem_filters=32,\n                  initial_reduction=False,\n                  skip_reduction_layer_input=False,\n                  use_auxiliary_branch=use_auxiliary_branch,\n                  filters_multiplier=2,\n                  dropout=dropout,\n                  weight_decay=weight_decay,\n                  include_top=include_top,\n                  weights=weights,\n                  input_tensor=input_tensor,\n                  pooling=pooling,\n                  classes=classes,\n                  default_size=224,\n                  activation=activation)\n\n\ndef _separable_conv_block(ip, filters, kernel_size=(3, 3), strides=(1, 1),\n                          weight_decay=5e-5, id=None):\n    \'\'\'Adds 2 blocks of [relu-separable conv-batchnorm]\n\n    # Arguments:\n        ip: input tensor\n        filters: number of output filters per layer\n        kernel_size: kernel size of separable convolutions\n        strides: strided convolution for downsampling\n        weight_decay: l2 regularization weight\n        id: string id\n\n    # Returns:\n        a Keras tensor\n    \'\'\'\n    channel_dim = 1 if K.image_data_format() == \'channels_first\' else -1\n\n    with K.name_scope(\'separable_conv_block_%s\' % id):\n        x = Activation(\'relu\')(ip)\n        x = SeparableConv2D(filters, kernel_size, strides=strides,\n                            name=\'separable_conv_1_%s\' % id, padding=\'same\',\n                            use_bias=False, kernel_initializer=\'he_normal\',\n                            kernel_regularizer=l2(weight_decay))(x)\n        x = BatchNormalization(axis=channel_dim, momentum=_BN_DECAY,\n                               epsilon=_BN_EPSILON,\n                               name=""separable_conv_1_bn_%s"" % id)(x)\n        x = Activation(\'relu\')(x)\n        x = SeparableConv2D(filters, kernel_size, name=\'separable_conv_2_%s\' % id,\n                            padding=\'same\', use_bias=False,\n                            kernel_initializer=\'he_normal\',\n                            kernel_regularizer=l2(weight_decay))(x)\n        x = BatchNormalization(axis=channel_dim, momentum=_BN_DECAY,\n                               epsilon=_BN_EPSILON,\n                               name=""separable_conv_2_bn_%s"" % id)(x)\n    return x\n\n\ndef _adjust_block(p, ip, filters, weight_decay=5e-5, id=None):\n    \'\'\'\n    Adjusts the input `p` to match the shape of the `input`\n    or situations where the output number of filters needs to\n    be changed\n\n    # Arguments:\n        p: input tensor which needs to be modified\n        ip: input tensor whose shape needs to be matched\n        filters: number of output filters to be matched\n        weight_decay: l2 regularization weight\n        id: string id\n\n    # Returns:\n        an adjusted Keras tensor\n    \'\'\'\n    channel_dim = 1 if K.image_data_format() == \'channels_first\' else -1\n    img_dim = 2 if K.image_data_format() == \'channels_first\' else -2\n\n    with K.name_scope(\'adjust_block\'):\n        if p is None:\n            p = ip\n\n        elif p._keras_shape[img_dim] != ip._keras_shape[img_dim]:\n            with K.name_scope(\'adjust_reduction_block_%s\' % id):\n                p = Activation(\'relu\', name=\'adjust_relu_1_%s\' % id)(p)\n\n                p1 = AveragePooling2D((1, 1), strides=(2, 2), padding=\'valid\',\n                                      name=\'adjust_avg_pool_1_%s\' % id)(p)\n                p1 = Conv2D(filters // 2, (1, 1), padding=\'same\', use_bias=False,\n                            kernel_regularizer=l2(weight_decay),\n                            name=\'adjust_conv_1_%s\' % id,\n                            kernel_initializer=\'he_normal\')(p1)\n\n                p2 = ZeroPadding2D(padding=((0, 1), (0, 1)))(p)\n                p2 = Cropping2D(cropping=((1, 0), (1, 0)))(p2)\n                p2 = AveragePooling2D((1, 1), strides=(2, 2), padding=\'valid\',\n                                      name=\'adjust_avg_pool_2_%s\' % id)(p2)\n                p2 = Conv2D(filters // 2, (1, 1), padding=\'same\', use_bias=False,\n                            kernel_regularizer=l2(weight_decay),\n                            name=\'adjust_conv_2_%s\' % id,\n                            kernel_initializer=\'he_normal\')(p2)\n\n                p = concatenate([p1, p2], axis=channel_dim)\n                p = BatchNormalization(axis=channel_dim, momentum=_BN_DECAY,\n                                       epsilon=_BN_EPSILON,\n                                       name=\'adjust_bn_%s\' % id)(p)\n\n        elif p._keras_shape[channel_dim] != filters:\n            with K.name_scope(\'adjust_projection_block_%s\' % id):\n                p = Activation(\'relu\')(p)\n                p = Conv2D(filters, (1, 1), strides=(1, 1), padding=\'same\',\n                           name=\'adjust_conv_projection_%s\' % id, use_bias=False,\n                           kernel_regularizer=l2(weight_decay),\n                           kernel_initializer=\'he_normal\')(p)\n                p = BatchNormalization(axis=channel_dim, momentum=_BN_DECAY,\n                                       epsilon=_BN_EPSILON,\n                                       name=\'adjust_bn_%s\' % id)(p)\n    return p\n\n\ndef _normal_A(ip, p, filters, weight_decay=5e-5, id=None):\n    \'\'\'Adds a Normal cell for NASNet-A (Fig. 4 in the paper)\n\n    # Arguments:\n        ip: input tensor `x`\n        p: input tensor `p`\n        filters: number of output filters\n        weight_decay: l2 regularization weight\n        id: string id\n\n    # Returns:\n        a Keras tensor\n    \'\'\'\n    channel_dim = 1 if K.image_data_format() == \'channels_first\' else -1\n\n    with K.name_scope(\'normal_A_block_%s\' % id):\n        p = _adjust_block(p, ip, filters, weight_decay, id)\n\n        h = Activation(\'relu\')(ip)\n        h = Conv2D(filters, (1, 1), strides=(1, 1), padding=\'same\',\n                   name=\'normal_conv_1_%s\' % id, use_bias=False,\n                   kernel_initializer=\'he_normal\',\n                   kernel_regularizer=l2(weight_decay))(h)\n        h = BatchNormalization(axis=channel_dim, momentum=_BN_DECAY,\n                               epsilon=_BN_EPSILON, name=\'normal_bn_1_%s\' % id)(h)\n\n        with K.name_scope(\'block_1\'):\n            x1_1 = _separable_conv_block(h, filters, kernel_size=(5, 5),\n                                         weight_decay=weight_decay,\n                                         id=\'normal_left1_%s\' % id)\n            x1_2 = _separable_conv_block(p, filters, weight_decay=weight_decay,\n                                         id=\'normal_right1_%s\' % id)\n            x1 = add([x1_1, x1_2], name=\'normal_add_1_%s\' % id)\n\n        with K.name_scope(\'block_2\'):\n            x2_1 = _separable_conv_block(p, filters, (5, 5), weight_decay=weight_decay,\n                                         id=\'normal_left2_%s\' % id)\n            x2_2 = _separable_conv_block(p, filters, (3, 3), weight_decay=weight_decay,\n                                         id=\'normal_right2_%s\' % id)\n            x2 = add([x2_1, x2_2], name=\'normal_add_2_%s\' % id)\n\n        with K.name_scope(\'block_3\'):\n            x3 = AveragePooling2D((3, 3), strides=(1, 1), padding=\'same\',\n                                  name=\'normal_left3_%s\' % id)(h)\n            x3 = add([x3, p], name=\'normal_add_3_%s\' % id)\n\n        with K.name_scope(\'block_4\'):\n            x4_1 = AveragePooling2D((3, 3), strides=(1, 1), padding=\'same\',\n                                    name=\'normal_left4_%s\' % id)(p)\n            x4_2 = AveragePooling2D((3, 3), strides=(1, 1), padding=\'same\',\n                                    name=\'normal_right4_%s\' % id)(p)\n            x4 = add([x4_1, x4_2], name=\'normal_add_4_%s\' % id)\n\n        with K.name_scope(\'block_5\'):\n            x5 = _separable_conv_block(h, filters, weight_decay=weight_decay,\n                                       id=\'normal_left5_%s\' % id)\n            x5 = add([x5, h], name=\'normal_add_5_%s\' % id)\n\n        x = concatenate([p, x1, x2, x3, x4, x5], axis=channel_dim,\n                        name=\'normal_concat_%s\' % id)\n    return x, ip\n\n\ndef _reduction_A(ip, p, filters, weight_decay=5e-5, id=None):\n    \'\'\'Adds a Reduction cell for NASNet-A (Fig. 4 in the paper)\n\n    # Arguments:\n        ip: input tensor `x`\n        p: input tensor `p`\n        filters: number of output filters\n        weight_decay: l2 regularization weight\n        id: string id\n\n    # Returns:\n        a Keras tensor\n    \'\'\'\n    """"""""""""\n    channel_dim = 1 if K.image_data_format() == \'channels_first\' else -1\n\n    with K.name_scope(\'reduction_A_block_%s\' % id):\n        p = _adjust_block(p, ip, filters, weight_decay, id)\n\n        h = Activation(\'relu\')(ip)\n        h = Conv2D(filters, (1, 1), strides=(1, 1), padding=\'same\',\n                   name=\'reduction_conv_1_%s\' % id, use_bias=False,\n                   kernel_initializer=\'he_normal\',\n                   kernel_regularizer=l2(weight_decay))(h)\n        h = BatchNormalization(axis=channel_dim, momentum=_BN_DECAY,\n                               epsilon=_BN_EPSILON,\n                               name=\'reduction_bn_1_%s\' % id)(h)\n\n        with K.name_scope(\'block_1\'):\n            x1_1 = _separable_conv_block(h, filters, (5, 5), strides=(2, 2),\n                                         weight_decay=weight_decay,\n                                         id=\'reduction_left1_%s\' % id)\n            x1_2 = _separable_conv_block(p, filters, (7, 7), strides=(2, 2),\n                                         weight_decay=weight_decay,\n                                         id=\'reduction_1_%s\' % id)\n            x1 = add([x1_1, x1_2], name=\'reduction_add_1_%s\' % id)\n\n        with K.name_scope(\'block_2\'):\n            x2_1 = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\',\n                                name=\'reduction_left2_%s\' % id)(h)\n            x2_2 = _separable_conv_block(p, filters, (7, 7), strides=(2, 2),\n                                         weight_decay=weight_decay,\n                                         id=\'reduction_right2_%s\' % id)\n            x2 = add([x2_1, x2_2], name=\'reduction_add_2_%s\' % id)\n\n        with K.name_scope(\'block_3\'):\n            x3_1 = AveragePooling2D((3, 3), strides=(2, 2), padding=\'same\',\n                                    name=\'reduction_left3_%s\' % id)(h)\n            x3_2 = _separable_conv_block(p, filters, (5, 5), strides=(2, 2),\n                                         weight_decay=weight_decay,\n                                         id=\'reduction_right3_%s\' % id)\n            x3 = add([x3_1, x3_2], name=\'reduction_add3_%s\' % id)\n\n        with K.name_scope(\'block_4\'):\n            x4 = AveragePooling2D((3, 3), strides=(1, 1), padding=\'same\',\n                                  name=\'reduction_left4_%s\' % id)(x1)\n            x4 = add([x2, x4])\n\n        with K.name_scope(\'block_5\'):\n            x5_1 = _separable_conv_block(x1, filters, (3, 3),\n                                         weight_decay=weight_decay,\n                                         id=\'reduction_left4_%s\' % id)\n            x5_2 = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\',\n                                name=\'reduction_right5_%s\' % id)(h)\n            x5 = add([x5_1, x5_2], name=\'reduction_add4_%s\' % id)\n\n        x = concatenate([x2, x3, x4, x5], axis=channel_dim,\n                        name=\'reduction_concat_%s\' % id)\n        return x, ip\n\n\ndef _add_auxiliary_head(x, classes, weight_decay, pooling, include_top, activation):\n    \'\'\'Adds an auxiliary head for training the model\n\n    From section A.7 ""Training of ImageNet models"" of the paper, all NASNet models are\n    trained using an auxiliary classifier around 2/3 of the depth of the network, with\n    a loss weight of 0.4\n\n    # Arguments\n        x: input tensor\n        classes: number of output classes\n        weight_decay: l2 regularization weight\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model\n                will be the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a\n                2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        activation: Type of activation at the top layer.\n            Can be one of \'softmax\' or \'sigmoid\'.\n\n    # Returns\n        a keras Tensor\n    \'\'\'\n    img_height = 1 if K.image_data_format() == \'channels_last\' else 2\n    img_width = 2 if K.image_data_format() == \'channels_last\' else 3\n    channel_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n    with K.name_scope(\'auxiliary_branch\'):\n        auxiliary_x = Activation(\'relu\')(x)\n        auxiliary_x = AveragePooling2D((5, 5), strides=(3, 3), padding=\'valid\',\n                                       name=\'aux_pool\')(auxiliary_x)\n        auxiliary_x = Conv2D(128, (1, 1), padding=\'same\', use_bias=False,\n                             name=\'aux_conv_projection\', kernel_initializer=\'he_normal\',\n                             kernel_regularizer=l2(weight_decay))(auxiliary_x)\n        auxiliary_x = BatchNormalization(axis=channel_axis, momentum=_BN_DECAY,\n                                         epsilon=_BN_EPSILON,\n                                         name=\'aux_bn_projection\')(auxiliary_x)\n        auxiliary_x = Activation(\'relu\')(auxiliary_x)\n\n        auxiliary_x = Conv2D(768, (auxiliary_x._keras_shape[img_height],\n                                   auxiliary_x._keras_shape[img_width]),\n                             padding=\'valid\', use_bias=False,\n                             kernel_initializer=\'he_normal\',\n                             kernel_regularizer=l2(weight_decay),\n                             name=\'aux_conv_reduction\')(auxiliary_x)\n        auxiliary_x = BatchNormalization(axis=channel_axis, momentum=_BN_DECAY,\n                                         epsilon=_BN_EPSILON,\n                                         name=\'aux_bn_reduction\')(auxiliary_x)\n        auxiliary_x = Activation(\'relu\')(auxiliary_x)\n\n        if include_top:\n            auxiliary_x = Flatten()(auxiliary_x)\n            auxiliary_x = Dense(classes, activation=activation,\n                                kernel_regularizer=l2(weight_decay),\n                                name=\'aux_predictions\')(auxiliary_x)\n        else:\n            if pooling == \'avg\':\n                auxiliary_x = GlobalAveragePooling2D()(auxiliary_x)\n            elif pooling == \'max\':\n                auxiliary_x = GlobalMaxPooling2D()(auxiliary_x)\n\n    return auxiliary_x\n'"
keras_contrib/applications/resnet.py,0,"b'""""""ResNet v1, v2, and segmentation models for Keras.\n\n# Reference\n\n- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n- [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)\n\nReference material for extended functionality:\n\n- [ResNeXt](https://arxiv.org/abs/1611.05431) for Tiny ImageNet support.\n- [Dilated Residual Networks](https://arxiv.org/pdf/1705.09914) for segmentation support\n- [Deep Residual Learning for Instrument Segmentation in\n   Robotic Surgery](https://arxiv.org/abs/1703.08580)\n  for segmentation support.\n\nImplementation Adapted from: github.com/raghakot/keras-resnet\n""""""  # pylint: disable=E501\nfrom __future__ import division\n\nimport six\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Activation\nfrom keras.layers import Reshape\nfrom keras.layers import Dense\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import Dropout\nfrom keras.layers.merge import add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.regularizers import l2\nfrom keras import backend as K\nfrom keras_applications.imagenet_utils import _obtain_input_shape\n\n\ndef _bn_relu(x, bn_name=None, relu_name=None):\n    """"""Helper to build a BN -> relu block\n    """"""\n    norm = BatchNormalization(axis=CHANNEL_AXIS, name=bn_name)(x)\n    return Activation(""relu"", name=relu_name)(norm)\n\n\ndef _conv_bn_relu(**conv_params):\n    """"""Helper to build a conv -> BN -> relu residual unit activation function.\n       This is the original ResNet v1 scheme in https://arxiv.org/abs/1512.03385\n    """"""\n    filters = conv_params[""filters""]\n    kernel_size = conv_params[""kernel_size""]\n    strides = conv_params.setdefault(""strides"", (1, 1))\n    dilation_rate = conv_params.setdefault(""dilation_rate"", (1, 1))\n    conv_name = conv_params.setdefault(""conv_name"", None)\n    bn_name = conv_params.setdefault(""bn_name"", None)\n    relu_name = conv_params.setdefault(""relu_name"", None)\n    kernel_initializer = conv_params.setdefault(""kernel_initializer"", ""he_normal"")\n    padding = conv_params.setdefault(""padding"", ""same"")\n    kernel_regularizer = conv_params.setdefault(""kernel_regularizer"", l2(1.e-4))\n\n    def f(x):\n        x = Conv2D(filters=filters, kernel_size=kernel_size,\n                   strides=strides, padding=padding,\n                   dilation_rate=dilation_rate,\n                   kernel_initializer=kernel_initializer,\n                   kernel_regularizer=kernel_regularizer,\n                   name=conv_name)(x)\n        return _bn_relu(x, bn_name=bn_name, relu_name=relu_name)\n\n    return f\n\n\ndef _bn_relu_conv(**conv_params):\n    """"""Helper to build a BN -> relu -> conv residual unit with full pre-activation\n    function. This is the ResNet v2 scheme proposed in\n    http://arxiv.org/pdf/1603.05027v2.pdf\n    """"""\n    filters = conv_params[""filters""]\n    kernel_size = conv_params[""kernel_size""]\n    strides = conv_params.setdefault(""strides"", (1, 1))\n    dilation_rate = conv_params.setdefault(""dilation_rate"", (1, 1))\n    conv_name = conv_params.setdefault(""conv_name"", None)\n    bn_name = conv_params.setdefault(""bn_name"", None)\n    relu_name = conv_params.setdefault(""relu_name"", None)\n    kernel_initializer = conv_params.setdefault(""kernel_initializer"", ""he_normal"")\n    padding = conv_params.setdefault(""padding"", ""same"")\n    kernel_regularizer = conv_params.setdefault(""kernel_regularizer"", l2(1.e-4))\n\n    def f(x):\n        activation = _bn_relu(x, bn_name=bn_name, relu_name=relu_name)\n        return Conv2D(filters=filters, kernel_size=kernel_size,\n                      strides=strides, padding=padding,\n                      dilation_rate=dilation_rate,\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name=conv_name)(activation)\n\n    return f\n\n\ndef _shortcut(input_feature, residual, conv_name_base=None, bn_name_base=None):\n    """"""Adds a shortcut between input and residual block and merges them with ""sum""\n    """"""\n    # Expand channels of shortcut to match residual.\n    # Stride appropriately to match residual (width, height)\n    # Should be int if network architecture is correctly configured.\n    input_shape = K.int_shape(input_feature)\n    residual_shape = K.int_shape(residual)\n    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n\n    shortcut = input_feature\n    # 1 X 1 conv if shape is different. Else identity.\n    if stride_width > 1 or stride_height > 1 or not equal_channels:\n        print(\'reshaping via a convolution...\')\n        if conv_name_base is not None:\n            conv_name_base = conv_name_base + \'1\'\n        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n                          kernel_size=(1, 1),\n                          strides=(stride_width, stride_height),\n                          padding=""valid"",\n                          kernel_initializer=""he_normal"",\n                          kernel_regularizer=l2(0.0001),\n                          name=conv_name_base)(input_feature)\n        if bn_name_base is not None:\n            bn_name_base = bn_name_base + \'1\'\n        shortcut = BatchNormalization(axis=CHANNEL_AXIS,\n                                      name=bn_name_base)(shortcut)\n\n    return add([shortcut, residual])\n\n\ndef _residual_block(block_function, filters, blocks, stage,\n                    transition_strides=None, transition_dilation_rates=None,\n                    dilation_rates=None, is_first_layer=False, dropout=None,\n                    residual_unit=_bn_relu_conv):\n    """"""Builds a residual block with repeating bottleneck blocks.\n\n       stage: integer, current stage label, used for generating layer names\n       blocks: number of blocks \'a\',\'b\'..., current block label, used for generating\n            layer names\n       transition_strides: a list of tuples for the strides of each transition\n       transition_dilation_rates: a list of tuples for the dilation rate of each\n            transition\n    """"""\n    if transition_dilation_rates is None:\n        transition_dilation_rates = [(1, 1)] * blocks\n    if transition_strides is None:\n        transition_strides = [(1, 1)] * blocks\n    if dilation_rates is None:\n        dilation_rates = [1] * blocks\n\n    def f(x):\n        for i in range(blocks):\n            is_first_block = is_first_layer and i == 0\n            x = block_function(filters=filters, stage=stage, block=i,\n                               transition_strides=transition_strides[i],\n                               dilation_rate=dilation_rates[i],\n                               is_first_block_of_first_layer=is_first_block,\n                               dropout=dropout,\n                               residual_unit=residual_unit)(x)\n        return x\n\n    return f\n\n\ndef _block_name_base(stage, block):\n    """"""Get the convolution name base and batch normalization name base defined by\n    stage and block.\n\n    If there are less than 26 blocks they will be labeled \'a\', \'b\', \'c\' to match the\n    paper and keras and beyond 26 blocks they will simply be numbered.\n    """"""\n    if block < 27:\n        block = \'%c\' % (block + 97)  # 97 is the ascii number for lowercase \'a\'\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n    return conv_name_base, bn_name_base\n\n\ndef basic_block(filters, stage, block, transition_strides=(1, 1),\n                dilation_rate=(1, 1), is_first_block_of_first_layer=False, dropout=None,\n                residual_unit=_bn_relu_conv):\n    """"""Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n    """"""\n    def f(input_features):\n        conv_name_base, bn_name_base = _block_name_base(stage, block)\n        if is_first_block_of_first_layer:\n            # don\'t repeat bn->relu since we just did bn->relu->maxpool\n            x = Conv2D(filters=filters, kernel_size=(3, 3),\n                       strides=transition_strides,\n                       dilation_rate=dilation_rate,\n                       padding=""same"",\n                       kernel_initializer=""he_normal"",\n                       kernel_regularizer=l2(1e-4),\n                       name=conv_name_base + \'2a\')(input_features)\n        else:\n            x = residual_unit(filters=filters, kernel_size=(3, 3),\n                              strides=transition_strides,\n                              dilation_rate=dilation_rate,\n                              conv_name_base=conv_name_base + \'2a\',\n                              bn_name_base=bn_name_base + \'2a\')(input_features)\n\n        if dropout is not None:\n            x = Dropout(dropout)(x)\n\n        x = residual_unit(filters=filters, kernel_size=(3, 3),\n                          conv_name_base=conv_name_base + \'2b\',\n                          bn_name_base=bn_name_base + \'2b\')(x)\n\n        return _shortcut(input_features, x)\n\n    return f\n\n\ndef bottleneck(filters, stage, block, transition_strides=(1, 1),\n               dilation_rate=(1, 1), is_first_block_of_first_layer=False, dropout=None,\n               residual_unit=_bn_relu_conv):\n    """"""Bottleneck architecture for > 34 layer resnet.\n    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n\n    Returns:\n        A final conv layer of filters * 4\n    """"""\n    def f(input_feature):\n        conv_name_base, bn_name_base = _block_name_base(stage, block)\n        if is_first_block_of_first_layer:\n            # don\'t repeat bn->relu since we just did bn->relu->maxpool\n            x = Conv2D(filters=filters, kernel_size=(1, 1),\n                       strides=transition_strides,\n                       dilation_rate=dilation_rate,\n                       padding=""same"",\n                       kernel_initializer=""he_normal"",\n                       kernel_regularizer=l2(1e-4),\n                       name=conv_name_base + \'2a\')(input_feature)\n        else:\n            x = residual_unit(filters=filters, kernel_size=(1, 1),\n                              strides=transition_strides,\n                              dilation_rate=dilation_rate,\n                              conv_name_base=conv_name_base + \'2a\',\n                              bn_name_base=bn_name_base + \'2a\')(input_feature)\n\n        if dropout is not None:\n            x = Dropout(dropout)(x)\n\n        x = residual_unit(filters=filters, kernel_size=(3, 3),\n                          conv_name_base=conv_name_base + \'2b\',\n                          bn_name_base=bn_name_base + \'2b\')(x)\n\n        if dropout is not None:\n            x = Dropout(dropout)(x)\n\n        x = residual_unit(filters=filters * 4, kernel_size=(1, 1),\n                          conv_name_base=conv_name_base + \'2c\',\n                          bn_name_base=bn_name_base + \'2c\')(x)\n\n        return _shortcut(input_feature, x)\n\n    return f\n\n\ndef _handle_dim_ordering():\n    global ROW_AXIS\n    global COL_AXIS\n    global CHANNEL_AXIS\n    if K.image_data_format() == \'channels_last\':\n        ROW_AXIS = 1\n        COL_AXIS = 2\n        CHANNEL_AXIS = 3\n    else:\n        CHANNEL_AXIS = 1\n        ROW_AXIS = 2\n        COL_AXIS = 3\n\n\ndef _string_to_function(identifier):\n    if isinstance(identifier, six.string_types):\n        res = globals().get(identifier)\n        if not res:\n            raise ValueError(\'Invalid {}\'.format(identifier))\n        return res\n    return identifier\n\n\ndef ResNet(input_shape=None, classes=10, block=\'bottleneck\', residual_unit=\'v2\',\n           repetitions=None, initial_filters=64, activation=\'softmax\', include_top=True,\n           input_tensor=None, dropout=None, transition_dilation_rate=(1, 1),\n           initial_strides=(2, 2), initial_kernel_size=(7, 7), initial_pooling=\'max\',\n           final_pooling=None, top=\'classification\'):\n    """"""Builds a custom ResNet like architecture. Defaults to ResNet50 v2.\n\n    Args:\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(224, 224, 3)` (with `channels_last` dim ordering)\n            or `(3, 224, 224)` (with `channels_first` dim ordering).\n            It should have exactly 3 dimensions,\n            and width and height should be no smaller than 8.\n            E.g. `(224, 224, 3)` would be one valid value.\n        classes: The number of outputs at final softmax layer\n        block: The block function to use. This is either `\'basic\'` or `\'bottleneck\'`.\n            The original paper used `basic` for layers < 50.\n        repetitions: Number of repetitions of various block units.\n            At each block unit, the number of filters are doubled and the input size\n            is halved. Default of None implies the ResNet50v2 values of [3, 4, 6, 3].\n        residual_unit: the basic residual unit, \'v1\' for conv bn relu, \'v2\' for bn relu\n            conv. See [Identity Mappings in\n            Deep Residual Networks](https://arxiv.org/abs/1603.05027)\n            for details.\n        dropout: None for no dropout, otherwise rate of dropout from 0 to 1.\n            Based on [Wide Residual Networks.(https://arxiv.org/pdf/1605.07146) paper.\n        transition_dilation_rate: Dilation rate for transition layers. For semantic\n            segmentation of images use a dilation rate of (2, 2).\n        initial_strides: Stride of the very first residual unit and MaxPooling2D call,\n            with default (2, 2), set to (1, 1) for small images like cifar.\n        initial_kernel_size: kernel size of the very first convolution, (7, 7) for\n            imagenet and (3, 3) for small image datasets like tiny imagenet and cifar.\n            See [ResNeXt](https://arxiv.org/abs/1611.05431) paper for details.\n        initial_pooling: Determine if there will be an initial pooling layer,\n            \'max\' for imagenet and None for small image datasets.\n            See [ResNeXt](https://arxiv.org/abs/1611.05431) paper for details.\n        final_pooling: Optional pooling mode for feature extraction at the final\n            model layer when `include_top` is `False`.\n            - `None` means that the output of the model\n                will be the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a\n                2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        top: Defines final layers to evaluate based on a specific problem type. Options\n            are \'classification\' for ImageNet style problems, \'segmentation\' for\n            problems like the Pascal VOC dataset, and None to exclude these layers\n            entirely.\n\n    Returns:\n        The keras `Model`.\n    """"""\n    if activation not in [\'softmax\', \'sigmoid\', None]:\n        raise ValueError(\'activation must be one of ""softmax"", ""sigmoid"", or None\')\n    if activation == \'sigmoid\' and classes != 1:\n        raise ValueError(\'sigmoid activation can only be used when classes = 1\')\n    if repetitions is None:\n        repetitions = [3, 4, 6, 3]\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=32,\n                                      min_size=8,\n                                      data_format=K.image_data_format(),\n                                      require_flatten=include_top)\n    _handle_dim_ordering()\n    if len(input_shape) != 3:\n        raise Exception(""Input shape should be a tuple (nb_channels, nb_rows, nb_cols)"")\n\n    if block == \'basic\':\n        block_fn = basic_block\n    elif block == \'bottleneck\':\n        block_fn = bottleneck\n    elif isinstance(block, six.string_types):\n        block_fn = _string_to_function(block)\n    else:\n        block_fn = block\n\n    if residual_unit == \'v2\':\n        residual_unit = _bn_relu_conv\n    elif residual_unit == \'v1\':\n        residual_unit = _conv_bn_relu\n    elif isinstance(residual_unit, six.string_types):\n        residual_unit = _string_to_function(residual_unit)\n    else:\n        residual_unit = residual_unit\n\n    # Permute dimension order if necessary\n    if K.image_data_format() == \'channels_first\':\n        input_shape = (input_shape[1], input_shape[2], input_shape[0])\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=32,\n                                      min_size=8,\n                                      data_format=K.image_data_format(),\n                                      require_flatten=include_top)\n\n    img_input = Input(shape=input_shape, tensor=input_tensor)\n    x = _conv_bn_relu(filters=initial_filters, kernel_size=initial_kernel_size,\n                      strides=initial_strides)(img_input)\n    if initial_pooling == \'max\':\n        x = MaxPooling2D(pool_size=(3, 3), strides=initial_strides, padding=""same"")(x)\n\n    block = x\n    filters = initial_filters\n    for i, r in enumerate(repetitions):\n        transition_dilation_rates = [transition_dilation_rate] * r\n        transition_strides = [(1, 1)] * r\n        if transition_dilation_rate == (1, 1):\n            transition_strides[0] = (2, 2)\n        block = _residual_block(block_fn, filters=filters,\n                                stage=i, blocks=r,\n                                is_first_layer=(i == 0),\n                                dropout=dropout,\n                                transition_dilation_rates=transition_dilation_rates,\n                                transition_strides=transition_strides,\n                                residual_unit=residual_unit)(block)\n        filters *= 2\n\n    # Last activation\n    x = _bn_relu(block)\n\n    # Classifier block\n    if include_top and top is \'classification\':\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(units=classes, activation=activation,\n                  kernel_initializer=""he_normal"")(x)\n    elif include_top and top is \'segmentation\':\n        x = Conv2D(classes, (1, 1), activation=\'linear\', padding=\'same\')(x)\n\n        if K.image_data_format() == \'channels_first\':\n            channel, row, col = input_shape\n        else:\n            row, col, channel = input_shape\n\n        x = Reshape((row * col, classes))(x)\n        x = Activation(activation)(x)\n        x = Reshape((row, col, classes))(x)\n    elif final_pooling == \'avg\':\n        x = GlobalAveragePooling2D()(x)\n    elif final_pooling == \'max\':\n        x = GlobalMaxPooling2D()(x)\n\n    model = Model(inputs=img_input, outputs=x)\n    return model\n\n\ndef ResNet18(input_shape, classes):\n    """"""ResNet with 18 layers and v2 residual units\n    """"""\n    return ResNet(input_shape, classes, basic_block, repetitions=[2, 2, 2, 2])\n\n\ndef ResNet34(input_shape, classes):\n    """"""ResNet with 34 layers and v2 residual units\n    """"""\n    return ResNet(input_shape, classes, basic_block, repetitions=[3, 4, 6, 3])\n\n\ndef ResNet50(input_shape, classes):\n    """"""ResNet with 50 layers and v2 residual units\n    """"""\n    return ResNet(input_shape, classes, bottleneck, repetitions=[3, 4, 6, 3])\n\n\ndef ResNet101(input_shape, classes):\n    """"""ResNet with 101 layers and v2 residual units\n    """"""\n    return ResNet(input_shape, classes, bottleneck, repetitions=[3, 4, 23, 3])\n\n\ndef ResNet152(input_shape, classes):\n    """"""ResNet with 152 layers and v2 residual units\n    """"""\n    return ResNet(input_shape, classes, bottleneck, repetitions=[3, 8, 36, 3])\n'"
keras_contrib/applications/wide_resnet.py,0,"b'# -*- coding: utf-8 -*-\n""""""Wide Residual Network models for Keras.\n\n# Reference\n\n- [Wide Residual Networks](https://arxiv.org/abs/1605.07146)\n\n""""""\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.pooling import MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Input, Conv2D\nfrom keras.layers.merge import add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils.layer_utils import convert_all_kernels_in_model\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras_applications.imagenet_utils import _obtain_input_shape\nimport keras.backend as K\n\nTH_WEIGHTS_PATH = (\'https://github.com/titu1994/Wide-Residual-Networks/\'\n                   \'releases/download/v1.2/wrn_28_8_th_kernels_th_dim_ordering.h5\')\nTF_WEIGHTS_PATH = (\'https://github.com/titu1994/Wide-Residual-Networks/\'\n                   \'releases/download/v1.2/wrn_28_8_tf_kernels_tf_dim_ordering.h5\')\nTH_WEIGHTS_PATH_NO_TOP = (\'https://github.com/titu1994/Wide-Residual-Networks/releases/\'\n                          \'download/v1.2/wrn_28_8_th_kernels_th_dim_ordering_no_top.h5\')\nTF_WEIGHTS_PATH_NO_TOP = (\'https://github.com/titu1994/Wide-Residual-Networks/releases/\'\n                          \'download/v1.2/wrn_28_8_tf_kernels_tf_dim_ordering_no_top.h5\')\n\n\ndef WideResidualNetwork(depth=28, width=8, dropout_rate=0.0,\n                        include_top=True, weights=\'cifar10\',\n                        input_tensor=None, input_shape=None,\n                        classes=10, activation=\'softmax\'):\n    """"""Instantiate the Wide Residual Network architecture,\n        optionally loading weights pre-trained\n        on CIFAR-10. Note that when using TensorFlow,\n        for best performance you should set\n        `image_dim_ordering=""tf""` in your Keras config\n        at ~/.keras/keras.json.\n\n        The model and the weights are compatible with both\n        TensorFlow and Theano. The dimension ordering\n        convention used by the model is the one\n        specified in your Keras config file.\n\n        # Arguments\n            depth: number or layers in the DenseNet\n            width: multiplier to the ResNet width (number of filters)\n            dropout_rate: dropout rate\n            include_top: whether to include the fully-connected\n                layer at the top of the network.\n            weights: one of `None` (random initialization) or\n                ""cifar10"" (pre-training on CIFAR-10)..\n            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n                to use as image input for the model.\n            input_shape: optional shape tuple, only to be specified\n                if `include_top` is False (otherwise the input shape\n                has to be `(32, 32, 3)` (with `tf` dim ordering)\n                or `(3, 32, 32)` (with `th` dim ordering).\n                It should have exactly 3 inputs channels,\n                and width and height should be no smaller than 8.\n                E.g. `(200, 200, 3)` would be one valid value.\n            classes: optional number of classes to classify images\n                into, only to be specified if `include_top` is True, and\n                if no `weights` argument is specified.\n\n        # Returns\n            A Keras model instance.\n        """"""\n\n    if weights not in {\'cifar10\', None}:\n        raise ValueError(\'The `weights` argument should be either \'\n                         \'`None` (random initialization) or `cifar10` \'\n                         \'(pre-training on CIFAR-10).\')\n\n    if weights == \'cifar10\' and include_top and classes != 10:\n        raise ValueError(\'If using `weights` as CIFAR 10 with `include_top`\'\n                         \' as true, `classes` should be 10\')\n\n    if (depth - 4) % 6 != 0:\n        raise ValueError(\'Depth of the network must be such that (depth - 4)\'\n                         \'should be divisible by 6.\')\n\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=32,\n                                      min_size=8,\n                                      data_format=K.image_dim_ordering(),\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    x = __create_wide_residual_network(classes, img_input, include_top, depth, width,\n                                       dropout_rate, activation)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name=\'wide-resnet\')\n\n    # load weights\n    if weights == \'cifar10\':\n        if (depth == 28) and (width == 8) and (dropout_rate == 0.0):\n            # Default parameters match. Weights for this model exist:\n\n            if K.image_dim_ordering() == \'th\':\n                if include_top:\n                    h5_file = \'wide_resnet_28_8_th_dim_ordering_th_kernels.h5\'\n                    weights_path = get_file(h5_file,\n                                            TH_WEIGHTS_PATH,\n                                            cache_subdir=\'models\')\n                else:\n                    h5_file = \'wide_resnet_28_8_th_dim_ordering_th_kernels_no_top.h5\'\n                    weights_path = get_file(h5_file,\n                                            TH_WEIGHTS_PATH_NO_TOP,\n                                            cache_subdir=\'models\')\n\n                model.load_weights(weights_path)\n\n                if K.backend() == \'tensorflow\':\n                    warnings.warn(\'You are using the TensorFlow backend, yet you \'\n                                  \'are using the Theano \'\n                                  \'image dimension ordering convention \'\n                                  \'(`image_dim_ordering=""th""`). \'\n                                  \'For best performance, set \'\n                                  \'`image_dim_ordering=""tf""` in \'\n                                  \'your Keras config \'\n                                  \'at ~/.keras/keras.json.\')\n                    convert_all_kernels_in_model(model)\n            else:\n                if include_top:\n                    h5_file = \'wide_resnet_28_8_tf_dim_ordering_tf_kernels.h5\'\n                    weights_path = get_file(h5_file,\n                                            TF_WEIGHTS_PATH,\n                                            cache_subdir=\'models\')\n                else:\n                    h5_file = \'wide_resnet_28_8_tf_dim_ordering_tf_kernels_no_top.h5\'\n                    weights_path = get_file(h5_file,\n                                            TF_WEIGHTS_PATH_NO_TOP,\n                                            cache_subdir=\'models\')\n\n                model.load_weights(weights_path)\n\n                if K.backend() == \'theano\':\n                    convert_all_kernels_in_model(model)\n\n    return model\n\n\ndef __conv1_block(input):\n    x = Conv2D(16, (3, 3), padding=\'same\')(input)\n\n    channel_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation(\'relu\')(x)\n    return x\n\n\ndef __conv2_block(input, k=1, dropout=0.0):\n    init = input\n\n    channel_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n    # Check if input number of filters is same as 16 * k, else create\n    # convolution2d for this input\n    if K.image_data_format() == \'channels_first\':\n        if init._keras_shape[1] != 16 * k:\n            init = Conv2D(16 * k, (1, 1), activation=\'linear\', padding=\'same\')(init)\n    else:\n        if init._keras_shape[-1] != 16 * k:\n            init = Conv2D(16 * k, (1, 1), activation=\'linear\', padding=\'same\')(init)\n\n    x = Conv2D(16 * k, (3, 3), padding=\'same\')(input)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation(\'relu\')(x)\n\n    if dropout > 0.0:\n        x = Dropout(dropout)(x)\n\n    x = Conv2D(16 * k, (3, 3), padding=\'same\')(x)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation(\'relu\')(x)\n\n    m = add([init, x])\n    return m\n\n\ndef __conv3_block(input, k=1, dropout=0.0):\n    init = input\n\n    channel_axis = 1 if K.image_data_format() == \'channels_first\' else -1\n\n    # Check if input number of filters is same as 32 * k, else\n    # create convolution2d for this input\n    if K.image_data_format() == \'channels_first\':\n        if init._keras_shape[1] != 32 * k:\n            init = Conv2D(32 * k, (1, 1), activation=\'linear\', padding=\'same\')(init)\n    else:\n        if init._keras_shape[-1] != 32 * k:\n            init = Conv2D(32 * k, (1, 1), activation=\'linear\', padding=\'same\')(init)\n\n    x = Conv2D(32 * k, (3, 3), padding=\'same\')(input)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation(\'relu\')(x)\n\n    if dropout > 0.0:\n        x = Dropout(dropout)(x)\n\n    x = Conv2D(32 * k, (3, 3), padding=\'same\')(x)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation(\'relu\')(x)\n\n    m = add([init, x])\n    return m\n\n\ndef ___conv4_block(input, k=1, dropout=0.0):\n    init = input\n\n    channel_axis = 1 if K.image_dim_ordering() == \'th\' else -1\n\n    # Check if input number of filters is same as 64 * k, else\n    # create convolution2d for this input\n    if K.image_dim_ordering() == \'th\':\n        if init._keras_shape[1] != 64 * k:\n            init = Conv2D(64 * k, (1, 1), activation=\'linear\', padding=\'same\')(init)\n    else:\n        if init._keras_shape[-1] != 64 * k:\n            init = Conv2D(64 * k, (1, 1), activation=\'linear\', padding=\'same\')(init)\n\n    x = Conv2D(64 * k, (3, 3), padding=\'same\')(input)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation(\'relu\')(x)\n\n    if dropout > 0.0:\n        x = Dropout(dropout)(x)\n\n    x = Conv2D(64 * k, (3, 3), padding=\'same\')(x)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation(\'relu\')(x)\n\n    m = add([init, x])\n    return m\n\n\ndef __create_wide_residual_network(nb_classes, img_input, include_top, depth=28,\n                                   width=8, dropout=0.0, activation=\'softmax\'):\n    \'\'\' Creates a Wide Residual Network with specified parameters\n\n    Args:\n        nb_classes: Number of output classes\n        img_input: Input tensor or layer\n        include_top: Flag to include the last dense layer\n        depth: Depth of the network. Compute N = (n - 4) / 6.\n               For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n               For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n               For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n        width: Width of the network.\n        dropout: Adds dropout if value is greater than 0.0\n\n    Returns:a Keras Model\n    \'\'\'\n\n    N = (depth - 4) // 6\n\n    x = __conv1_block(img_input)\n    nb_conv = 4\n\n    for i in range(N):\n        x = __conv2_block(x, width, dropout)\n        nb_conv += 2\n\n    x = MaxPooling2D((2, 2))(x)\n\n    for i in range(N):\n        x = __conv3_block(x, width, dropout)\n        nb_conv += 2\n\n    x = MaxPooling2D((2, 2))(x)\n\n    for i in range(N):\n        x = ___conv4_block(x, width, dropout)\n        nb_conv += 2\n\n    if include_top:\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(nb_classes, activation=activation)(x)\n\n    return x\n'"
keras_contrib/backend/__init__.py,0,"b""from keras import backend as K\n\n# We import all keras backend functions here,\n# so that files in this repo can import both\n# core and contrib backend functions with a\n# single import statement.\n\nif K.backend() == 'theano':\n    from .theano_backend import *\nelif K.backend() == 'tensorflow':\n    from .tensorflow_backend import *\nelif K.backend() == 'cntk':\n    from .cntk_backend import *\n"""
keras_contrib/backend/cntk_backend.py,0,"b""from keras.backend import cntk_backend as KCN\n\n\ndef moments(x, axes, shift=None, keep_dims=False):\n    ''' Calculates and returns the mean and variance of the input '''\n    mean, variant = KCN._moments(x, axes=axes, shift=shift, keep_dims=keep_dims)\n    return mean, variant\n"""
keras_contrib/backend/numpy_backend.py,0,"b""import numpy as np\r\nfrom keras import backend as K\r\n\r\n\r\ndef extract_image_patches(X, ksizes, strides,\r\n                          padding='valid',\r\n                          data_format='channels_first'):\r\n    raise NotImplementedError\r\n\r\n\r\ndef depth_to_space(input, scale, data_format=None):\r\n    raise NotImplementedError\r\n\r\n\r\ndef moments(x, axes, shift=None, keep_dims=False):\r\n    mean_batch = np.mean(x, axis=tuple(axes), keepdims=keep_dims)\r\n    var_batch = np.var(x, axis=tuple(axes), keepdims=keep_dims)\r\n    return mean_batch, var_batch\r\n"""
keras_contrib/backend/tensorflow_backend.py,9,"b'import tensorflow as tf\n\ntry:\n    from tensorflow.python.ops import ctc_ops as ctc\nexcept ImportError:\n    import tensorflow.contrib.ctc as ctc\nimport keras.backend as K\n\npy_all = all\n\n\ndef _preprocess_conv2d_input(x, data_format):\n    """"""Transpose and cast the input before the conv2d.\n\n    # Arguments\n        x: input tensor.\n        data_format: string, `""channels_last""` or `""channels_first""`.\n\n    # Returns\n        A tensor.\n    """"""\n    if K.dtype(x) == \'float64\':\n        x = tf.cast(x, \'float32\')\n    if data_format == \'channels_first\':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, rows, cols)\n        # TF input shape: (samples, rows, cols, input_depth)\n        x = tf.transpose(x, (0, 2, 3, 1))\n    return x\n\n\ndef _postprocess_conv2d_output(x, data_format):\n    """"""Transpose and cast the output from conv2d if needed.\n\n    # Arguments\n        x: A tensor.\n        data_format: string, `""channels_last""` or `""channels_first""`.\n\n    # Returns\n        A tensor.\n    """"""\n\n    if data_format == \'channels_first\':\n        x = tf.transpose(x, (0, 3, 1, 2))\n\n    if K.floatx() == \'float64\':\n        x = tf.cast(x, \'float64\')\n    return x\n\n\ndef _preprocess_padding(padding):\n    """"""Convert keras\' padding to tensorflow\'s padding.\n\n    # Arguments\n        padding: string, `""same""` or `""valid""`.\n\n    # Returns\n        a string, `""SAME""` or `""VALID""`.\n\n    # Raises\n        ValueError: if `padding` is invalid.\n    """"""\n    if padding == \'same\':\n        padding = \'SAME\'\n    elif padding == \'valid\':\n        padding = \'VALID\'\n    else:\n        raise ValueError(\'Invalid padding:\', padding)\n    return padding\n\n\ndef conv2d(x, kernel, strides=(1, 1), padding=\'valid\', data_format=\'channels_first\',\n           image_shape=None, filter_shape=None):\n    """"""2D convolution.\n\n    # Arguments\n        x: Input tensor\n        kernel: kernel tensor.\n        strides: strides tuple.\n        padding: string, ""same"" or ""valid"".\n        data_format: \'channels_first\' or \'channels_last\'.\n            Whether to use Theano or TensorFlow dimension\n            ordering in inputs/kernels/ouputs.\n        image_shape: Optional, the input tensor shape\n        filter_shape: Optional, the kernel shape.\n\n    # Returns\n        x convolved with the kernel.\n\n    # Raises\n        Exception: In case of invalid border mode or data format.\n    """"""\n    return K.conv2d(x, kernel, strides, padding, data_format)\n\n\ndef extract_image_patches(x, ksizes, ssizes, padding=\'same\',\n                          data_format=\'channels_last\'):\n    """"""Extract the patches from an image.\n\n    # Arguments\n        x: The input image\n        ksizes: 2-d tuple with the kernel size\n        ssizes: 2-d tuple with the strides size\n        padding: \'same\' or \'valid\'\n        data_format: \'channels_last\' or \'channels_first\'\n\n    # Returns\n        The (k_w,k_h) patches extracted\n        TF ==> (batch_size,w,h,k_w,k_h,c)\n        TH ==> (batch_size,w,h,c,k_w,k_h)\n    """"""\n    kernel = [1, ksizes[0], ksizes[1], 1]\n    strides = [1, ssizes[0], ssizes[1], 1]\n    padding = _preprocess_padding(padding)\n    if data_format == \'channels_first\':\n        x = K.permute_dimensions(x, (0, 2, 3, 1))\n    bs_i, w_i, h_i, ch_i = K.int_shape(x)\n    patches = tf.extract_image_patches(x, kernel, strides, [1, 1, 1, 1],\n                                       padding)\n    # Reshaping to fit Theano\n    bs, w, h, ch = K.int_shape(patches)\n    reshaped = tf.reshape(patches, [-1, w, h, tf.floordiv(ch, ch_i), ch_i])\n    final_shape = [-1, w, h, ch_i, ksizes[0], ksizes[1]]\n    patches = tf.reshape(tf.transpose(reshaped, [0, 1, 2, 4, 3]), final_shape)\n    if data_format == \'channels_last\':\n        patches = K.permute_dimensions(patches, [0, 1, 2, 4, 5, 3])\n    return patches\n\n\ndef depth_to_space(input, scale, data_format=None):\n    """""" Uses phase shift algorithm to convert channels/depth for spatial resolution.\n\n    # Arguments\n        input: Input tensor\n        scale: n `int` that is `>= 2`. The size of the spatial block.\n        data_format: \'channels_first\' or \'channels_last\'.\n            Whether to use Theano or TensorFlow dimension\n            ordering in inputs/kernels/ouputs.\n\n    # Returns\n        TODO (PR welcome): Filling this section.\n    """"""\n    if data_format is None:\n        data_format = K.image_data_format()\n    data_format = data_format.lower()\n    input = _preprocess_conv2d_input(input, data_format)\n    out = tf.depth_to_space(input, scale)\n    out = _postprocess_conv2d_output(out, data_format)\n    return out\n\n\ndef moments(x, axes, shift=None, keep_dims=False):\n    \'\'\' Wrapper over tensorflow backend call \'\'\'\n\n    return tf.nn.moments(x, axes, shift=shift, keep_dims=keep_dims)\n'"
keras_contrib/backend/theano_backend.py,0,"b'from theano import tensor as T\nfrom theano.sandbox.neighbours import images2neibs\n\ntry:\n    import theano.sparse as th_sparse_module\nexcept ImportError:\n    th_sparse_module = None\ntry:\n    from theano.tensor.nnet.nnet import softsign as T_softsign\nexcept ImportError:\n    from theano.sandbox.softsign import softsign as T_softsign\nfrom keras.backend import theano_backend as KTH\nfrom keras.backend.common import image_data_format\nfrom keras.backend.theano_backend import _preprocess_conv2d_input\nfrom keras.backend.theano_backend import _postprocess_conv2d_output\n\npy_all = all\n\n\ndef conv2d(x, kernel, strides=(1, 1), padding=\'valid\', data_format=\'channels_first\',\n           image_shape=None, filter_shape=None):\n    \'\'\'\n    padding: string, ""same"" or ""valid"".\n    \'\'\'\n    if data_format not in {\'channels_first\', \'channels_last\'}:\n        raise Exception(\'Unknown data_format \' + str(data_format))\n\n    if data_format == \'channels_last\':\n        # TF uses the last dimension as channel dimension,\n        # instead of the 2nd one.\n        # TH input shape: (samples, input_depth, rows, cols)\n        # TF input shape: (samples, rows, cols, input_depth)\n        # TH kernel shape: (depth, input_depth, rows, cols)\n        # TF kernel shape: (rows, cols, input_depth, depth)\n        x = x.dimshuffle((0, 3, 1, 2))\n        kernel = kernel.dimshuffle((3, 2, 0, 1))\n        if image_shape:\n            image_shape = (image_shape[0], image_shape[3],\n                           image_shape[1], image_shape[2])\n        if filter_shape:\n            filter_shape = (filter_shape[3], filter_shape[2],\n                            filter_shape[0], filter_shape[1])\n\n    if padding == \'same\':\n        th_padding = \'half\'\n        np_kernel = kernel.eval()\n    elif padding == \'valid\':\n        th_padding = \'valid\'\n    else:\n        raise Exception(\'Border mode not supported: \' + str(padding))\n\n    # Theano might not accept long type\n    def int_or_none(value):\n        try:\n            return int(value)\n        except TypeError:\n            return None\n\n    if image_shape is not None:\n        image_shape = tuple(int_or_none(v) for v in image_shape)\n\n    if filter_shape is not None:\n        filter_shape = tuple(int_or_none(v) for v in filter_shape)\n\n    conv_out = T.nnet.conv2d(x, kernel,\n                             border_mode=th_padding,\n                             subsample=strides,\n                             input_shape=image_shape,\n                             filter_shape=filter_shape)\n\n    if padding == \'same\':\n        if np_kernel.shape[2] % 2 == 0:\n            end = (x.shape[2] + strides[0] - 1) // strides[0]\n            conv_out = conv_out[:, :, :end, :]\n        if np_kernel.shape[3] % 2 == 0:\n            end = (x.shape[3] + strides[1] - 1) // strides[1]\n            conv_out = conv_out[:, :, :, :end]\n\n    if data_format == \'channels_last\':\n        conv_out = conv_out.dimshuffle((0, 2, 3, 1))\n    return conv_out\n\n\ndef extract_image_patches(X, ksizes, strides,\n                          padding=\'valid\',\n                          data_format=\'channels_first\'):\n    \'\'\'\n    Extract the patches from an image\n    Parameters\n    ----------\n    X : The input image\n    ksizes : 2-d tuple with the kernel size\n    strides : 2-d tuple with the strides size\n    padding : \'same\' or \'valid\'\n    data_format : \'channels_last\' or \'channels_first\'\n    Returns\n    -------\n    The (k_w,k_h) patches extracted\n    TF ==> (batch_size,w,h,k_w,k_h,c)\n    TH ==> (batch_size,w,h,c,k_w,k_h)\n    \'\'\'\n    patch_size = ksizes[1]\n    if padding == \'same\':\n        padding = \'ignore_borders\'\n    if data_format == \'channels_last\':\n        X = KTH.permute_dimensions(X, [0, 3, 1, 2])\n    # Thanks to https://github.com/awentzonline for the help!\n    batch, c, w, h = KTH.shape(X)\n    xs = KTH.shape(X)\n    num_rows = 1 + (xs[-2] - patch_size) // strides[1]\n    num_cols = 1 + (xs[-1] - patch_size) // strides[1]\n    num_channels = xs[-3]\n    patches = images2neibs(X, ksizes, strides, padding)\n    # Theano is sorting by channel\n    new_shape = (batch, num_channels, num_rows * num_cols, patch_size, patch_size)\n    patches = KTH.reshape(patches, new_shape)\n    patches = KTH.permute_dimensions(patches, (0, 2, 1, 3, 4))\n    # arrange in a 2d-grid (rows, cols, channels, px, py)\n    new_shape = (batch, num_rows, num_cols, num_channels, patch_size, patch_size)\n    patches = KTH.reshape(patches, new_shape)\n    if data_format == \'channels_last\':\n        patches = KTH.permute_dimensions(patches, [0, 1, 2, 4, 5, 3])\n    return patches\n\n\ndef depth_to_space(input, scale, data_format=None):\n    """"""Uses phase shift algorithm to convert\n    channels/depth for spatial resolution\n    """"""\n    if data_format is None:\n        data_format = image_data_format()\n    data_format = data_format.lower()\n    input = _preprocess_conv2d_input(input, data_format)\n\n    b, k, row, col = input.shape\n    out_channels = k // (scale ** 2)\n    x = T.reshape(input, (b, scale, scale, out_channels, row, col))\n    x = T.transpose(x, (0, 3, 4, 1, 5, 2))\n    out = T.reshape(x, (b, out_channels, row * scale, col * scale))\n\n    out = _postprocess_conv2d_output(out, input, None, None, None, data_format)\n    return out\n\n\ndef moments(x, axes, shift=None, keep_dims=False):\n    \'\'\' Calculates and returns the mean and variance of the input \'\'\'\n\n    mean_batch = KTH.mean(x, axis=axes, keepdims=keep_dims)\n    var_batch = KTH.var(x, axis=axes, keepdims=keep_dims)\n\n    return mean_batch, var_batch\n'"
keras_contrib/callbacks/__init__.py,0,"b'from .snapshot import SnapshotCallbackBuilder, SnapshotModelCheckpoint\nfrom .dead_relu_detector import DeadReluDetector\nfrom .cyclical_learning_rate import CyclicLR\nfrom .tensorboard import TensorBoardGrouped\n'"
keras_contrib/callbacks/cyclical_learning_rate.py,0,"b'from keras.callbacks import Callback\nfrom keras import backend as K\nimport numpy as np\n\n\nclass CyclicLR(Callback):\n    """"""This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency.\n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore\n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default \'triangular\'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in \'exp_range\' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where\n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored\n        scale_mode: {\'cycle\', \'iterations\'}.\n            Defines whether scale_fn is evaluated on\n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is \'cycle\'.\n\n    The amplitude of the cycle can be scaled on a per-iteration or\n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    ""triangular"":\n        A basic triangular cycle w/ no amplitude scaling.\n    ""triangular2"":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    ""exp_range"":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n        cycle iteration.\n    For more detail, please see paper.\n\n    # Example for CIFAR-10 w/ batch size 100:\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode=\'triangular\')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n\n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode=\'cycle\')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n\n    # References\n\n      - [Cyclical Learning Rates for Training Neural Networks](\n      https://arxiv.org/abs/1506.01186)\n    """"""\n\n    def __init__(\n            self,\n            base_lr=0.001,\n            max_lr=0.006,\n            step_size=2000.,\n            mode=\'triangular\',\n            gamma=1.,\n            scale_fn=None,\n            scale_mode=\'cycle\'):\n        super(CyclicLR, self).__init__()\n\n        if mode not in [\'triangular\', \'triangular2\',\n                        \'exp_range\']:\n            raise KeyError(""mode must be one of \'triangular\', ""\n                           ""\'triangular2\', or \'exp_range\'"")\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn is None:\n            if self.mode == \'triangular\':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'triangular2\':\n                self.scale_fn = lambda x: 1 / (2.**(x - 1))\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'exp_range\':\n                self.scale_fn = lambda x: gamma ** x\n                self.scale_mode = \'iterations\'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        """"""Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        """"""\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n\n    def clr(self):\n        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n        if self.scale_mode == \'cycle\':\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_batch_end(self, epoch, logs=None):\n\n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n        self.history.setdefault(\n            \'lr\', []).append(\n            K.get_value(\n                self.model.optimizer.lr))\n        self.history.setdefault(\'iterations\', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs[\'lr\'] = K.get_value(self.model.optimizer.lr)\n'"
keras_contrib/callbacks/dead_relu_detector.py,0,"b'import numpy as np\n\nfrom keras.callbacks import Callback\nfrom keras import backend as K\n\n\nclass DeadReluDetector(Callback):\n    """"""Reports the number of dead ReLUs after each training epoch\n    ReLU is considered to be dead if it did not fire once for entire training set\n\n    # Arguments\n        x_train: Training dataset to check whether or not neurons fire\n        verbose: verbosity mode\n            True means that even a single dead neuron triggers a warning message\n            False means that only significant number of dead neurons (10% or more)\n            triggers a warning message\n    """"""\n\n    def __init__(self, x_train, verbose=False):\n        super(DeadReluDetector, self).__init__()\n        self.x_train = x_train\n        self.verbose = verbose\n        self.dead_neurons_share_threshold = 0.1\n\n    @staticmethod\n    def is_relu_layer(layer):\n        # Should work for all layers with relu\n        # activation. Tested for Dense and Conv2D\n        return layer.get_config().get(\'activation\', None) == \'relu\'\n\n    def get_relu_activations(self):\n        model_input = self.model.input\n        is_multi_input = isinstance(model_input, list)\n        if not is_multi_input:\n            model_input = [model_input]\n\n        funcs = {}\n        for index, layer in enumerate(self.model.layers):\n            if not layer.get_weights():\n                continue\n            funcs[index] = K.function(model_input\n                                      + [K.learning_phase()], [layer.output])\n\n        if is_multi_input:\n            list_inputs = []\n            list_inputs.extend(self.x_train)\n            list_inputs.append(1.)\n        else:\n            list_inputs = [self.x_train, 1.]\n\n        layer_outputs = {}\n        for index, func in funcs.items():\n            layer_outputs[index] = func(list_inputs)[0]\n\n        for layer_index, layer_activations in layer_outputs.items():\n            if self.is_relu_layer(self.model.layers[layer_index]):\n                layer_name = self.model.layers[layer_index].name\n                # layer_weight is a list [W] (+ [b])\n                layer_weight = self.model.layers[layer_index].get_weights()\n\n                # with kernel and bias, the weights are saved as a list [W, b].\n                # If only weights, it is [W]\n                if type(layer_weight) is not list:\n                    raise ValueError(""\'Layer_weight\' should be a list, ""\n                                     ""but was {}"".format(type(layer_weight)))\n\n                # there are no weights for current layer; skip it\n                # this is only legitimate if layer is ""Activation""\n                if len(layer_weight) == 0:\n                    continue\n\n                layer_weight_shape = np.shape(layer_weight[0])\n                yield [layer_index,\n                       layer_activations,\n                       layer_name,\n                       layer_weight_shape]\n\n    def on_epoch_end(self, epoch, logs={}):\n        for relu_activation in self.get_relu_activations():\n            layer_index = relu_activation[0]\n            activation_values = relu_activation[1]\n            layer_name = relu_activation[2]\n            layer_weight_shape = relu_activation[3]\n\n            shape_act = activation_values.shape\n\n            weight_len = len(layer_weight_shape)\n            act_len = len(shape_act)\n\n            # should work for both Conv and Flat\n            if K.image_data_format() == \'channels_last\':\n                # features in last axis\n                axis_filter = -1\n            else:\n                # features before the convolution axis, for weight_\n                # len the input and output have to be subtracted\n                axis_filter = -1 - (weight_len - 2)\n\n            total_featuremaps = shape_act[axis_filter]\n\n            axis = []\n            for i in range(act_len):\n                if (i != axis_filter) and (i != (len(shape_act) + axis_filter)):\n                    axis.append(i)\n            axis = tuple(axis)\n\n            dead_neurons = np.sum(np.sum(activation_values, axis=axis) == 0)\n\n            dead_neurons_share = float(dead_neurons) / float(total_featuremaps)\n            if ((self.verbose and dead_neurons > 0)\n                    or dead_neurons_share >= self.dead_neurons_share_threshold):\n                str_warning = (\'Layer {} (#{}) has {} \'\n                               \'dead neurons ({:.2%})!\').format(layer_name,\n                                                                layer_index,\n                                                                dead_neurons,\n                                                                dead_neurons_share)\n                print(str_warning)\n'"
keras_contrib/callbacks/snapshot.py,0,"b'from __future__ import absolute_import\nfrom __future__ import print_function\n\nimport os\n\nimport numpy as np\n\nfrom keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler\n\ntry:\n    import requests\nexcept ImportError:\n    requests = None\n\n\nclass SnapshotModelCheckpoint(Callback):\n    """"""Callback that saves the snapshot weights of the model.\n\n    Saves the model weights on certain epochs (which can be considered the\n    snapshot of the model at that epoch).\n\n    Should be used with the cosine annealing learning rate schedule to save\n    the weight just before learning rate is sharply increased.\n\n    # Arguments:\n        nb_epochs: total number of epochs that the model will be trained for.\n        nb_snapshots: number of times the weights of the model will be saved.\n        fn_prefix: prefix for the filename of the weights.\n    """"""\n\n    def __init__(self, nb_epochs, nb_snapshots, fn_prefix=\'Model\'):\n        super(SnapshotModelCheckpoint, self).__init__()\n\n        self.check = nb_epochs // nb_snapshots\n        self.fn_prefix = fn_prefix\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch != 0 and (epoch + 1) % self.check == 0:\n            filepath = self.fn_prefix + \'-%d.h5\' % ((epoch + 1) // self.check)\n            self.model.save_weights(filepath, overwrite=True)\n            # print(""Saved snapshot at weights/%s_%d.h5"" % (self.fn_prefix, epoch))\n\n\nclass SnapshotCallbackBuilder:\n    """"""Callback builder for snapshot ensemble training of a model.\n    From the paper ""Snapshot Ensembles: Train 1, Get M For Free"" (\n    https://openreview.net/pdf?id=BJYwwY9ll)\n\n    Creates a list of callbacks, which are provided when training a model\n    so as to save the model weights at certain epochs, and then sharply\n    increase the learning rate.\n    """"""\n\n    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n        """"""\n        Initialize a snapshot callback builder.\n\n        # Arguments:\n            nb_epochs: total number of epochs that the model will be trained for.\n            nb_snapshots: number of times the weights of the model will be saved.\n            init_lr: initial learning rate\n        """"""\n        self.T = nb_epochs\n        self.M = nb_snapshots\n        self.alpha_zero = init_lr\n\n    def get_callbacks(self, model_prefix=\'Model\'):\n        """"""\n        Creates a list of callbacks that can be used during training to create a\n        snapshot ensemble of the model.\n\n        Args:\n            model_prefix: prefix for the filename of the weights.\n\n        Returns: list of 3 callbacks [ModelCheckpoint, LearningRateScheduler,\n                 SnapshotModelCheckpoint] which can be provided to the \'fit\' function\n        """"""\n        if not os.path.exists(\'weights/\'):\n            os.makedirs(\'weights/\')\n\n        callback_list = [ModelCheckpoint(\'weights/%s-Best.h5\' % model_prefix,\n                                         monitor=\'val_acc\',\n                                         save_best_only=True, save_weights_only=True),\n                         LearningRateScheduler(schedule=self._cosine_anneal_schedule),\n                         SnapshotModelCheckpoint(self.T,\n                                                 self.M,\n                                                 fn_prefix=\'weights/%s\' % model_prefix)]\n\n        return callback_list\n\n    def _cosine_anneal_schedule(self, t):\n        cos_inner = np.pi * (t % (self.T // self.M))\n        cos_inner /= self.T // self.M\n        cos_out = np.cos(cos_inner) + 1\n        return float(self.alpha_zero / 2 * cos_out)\n'"
keras_contrib/callbacks/tensorboard.py,2,"b'from keras.callbacks import TensorBoard\nimport numpy as np\nimport os\n\n\nclass TensorBoardGrouped(TensorBoard):\n    """"""TensorBoard basic visualizations.\n\n    [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard)\n    is a visualization tool provided with TensorFlow.\n\n    This callback is a subclass of `keras.callbacks.TensorBoard`.\n    The only difference is that the training and validation logs are\n    grouped and written to the same plot.\n\n    It\'s a drop-in replacement for the keras callback.\n    The arguments are the same.\n    """"""\n\n    def __init__(self, log_dir=\'./logs\', *args, **kwargs):\n        self.base_log_dir = log_dir\n        self.train_log_dir = os.path.join(log_dir, \'train\')\n        self.val_log_dir = os.path.join(log_dir, \'val\')\n        super(TensorBoardGrouped, self).__init__(self.train_log_dir,\n                                                 *args,\n                                                 **kwargs)\n\n    def set_model(self, model):\n        super(TensorBoardGrouped, self).set_model(model)\n        import tensorflow as tf\n        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n\n    def _write_logs(self, logs, index):\n        import tensorflow as tf\n        for name, value in logs.items():\n            if name in [\'batch\', \'size\']:\n                continue\n            if name.startswith(\'val_\'):\n                writer = self.val_writer\n                name = name[4:]  # remove val_\n            else:\n                writer = self.writer\n            summary = tf.Summary()\n            summary_value = summary.value.add()\n            if isinstance(value, np.ndarray):\n                summary_value.simple_value = value.item()\n            else:\n                summary_value.simple_value = value\n            summary_value.tag = name\n            writer.add_summary(summary, index)\n        self.writer.flush()\n        self.val_writer.flush()\n\n    def on_train_end(self, _):\n        self.writer.close()\n        self.val_writer.flush()\n'"
keras_contrib/constraints/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .clip import Clip\n\n# Aliases.\n\nclip = Clip\n'
keras_contrib/constraints/clip.py,0,"b'from __future__ import absolute_import\nfrom keras import backend as K\nfrom keras.constraints import Constraint\n\n\nclass Clip(Constraint):\n    """"""Clips weights to [-c, c].\n\n    # Arguments\n        c: Clipping parameter.\n    """"""\n\n    def __init__(self, c=0.01):\n        self.c = c\n\n    def __call__(self, p):\n        return K.clip(p, -self.c, self.c)\n\n    def get_config(self):\n        return {\'name\': self.__class__.__name__,\n                \'c\': self.c}\n'"
keras_contrib/datasets/__init__.py,0,b''
keras_contrib/datasets/coco.py,0,"b'#!/usr/bin/env python\n# coding=utf-8\n""""""\nThis is a script for downloading and converting the microsoft coco dataset\nfrom mscoco.org. This can be run as an independent executable to download\nthe dataset or be imported by scripts used for larger experiments.\n""""""\nfrom __future__ import division, print_function, unicode_literals\nimport os\nimport errno\nimport zipfile\nimport json\nfrom sacred import Experiment, Ingredient\nimport numpy as np\nfrom PIL import Image\nfrom keras.utils import get_file\nfrom keras.utils.generic_utils import Progbar\nfrom pycocotools.coco import COCO\n\n\ndef palette():\n    max_cid = max(ids()) + 1\n    return [(cid, cid, cid) for cid in range(max_cid)]\n\n\ndef cids_to_ids_map():\n    return {cid: idx for idx, cid in enumerate(ids())}\n\n\ndef ids():\n    return [0,\n            1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17,\n            18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36,\n            37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53,\n            54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73,\n            74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n\n\ndef id_to_palette_map():\n    return {idx: color for idx, color in enumerate(palette())}\n    # return {0: (0, 0, 0), idx: (idx, idx, idx)\n    # for idx, _ in enumerate(categories())}\n\n\ndef cid_to_palette_map():\n    return {ids()[idx]: color for idx, color in enumerate(palette())}\n\n\ndef palette_to_id_map():\n    return {color: ids()[idx] for idx, color in enumerate(palette())}\n    # return {(0, 0, 0): 0, (idx, idx, idx): idx\n    # for idx, _ in enumerate(categories())}\n\n\ndef class_weight(image_segmentation_stats_file=None,\n                 weighting_algorithm=\'total_pixels_p_complement\'):\n    # weights = defaultdict(lambda: 1.5)\n    if image_segmentation_stats_file is None:\n        weights = {i: 1.5 for i in ids()}\n        weights[0] = 0.5\n        return weights\n    else:\n        with open(image_segmentation_stats_file, \'r\') as fjson:\n            stats = json.loads(fjson)\n            return stats[weighting_algorithm]\n\n\ndef mask_to_palette_map(cid):\n    mapper = id_to_palette_map()\n    return {0: mapper[0], 255: mapper[cid]}\n\n\ndef categories():  # 80 classes\n    return [\'background\',  # class zero\n            \'person\', \'bicycle\', \'car\', \'motorcycle\',\n            \'airplane\', \'bus\', \'train\',\n            \'truck\', \'boat\', \'traffic light\',\n            \'fire hydrant\', \'stop sign\', \'parking meter\', \'bench\', \'bird\',\n            \'cat\', \'dog\', \'horse\', \'sheep\', \'cow\',\n            \'elephant\', \'bear\', \'zebra\', \'giraffe\',\n            \'backpack\', \'umbrella\', \'handbag\', \'tie\', \'suitcase\', \'frisbee\',\n            \'skis\', \'snowboard\', \'sports ball\', \'kite\',\n            \'baseball bat\', \'baseball glove\', \'skateboard\',\n            \'surfboard\', \'tennis racket\', \'bottle\',\n            \'wine glass\', \'cup\', \'fork\', \'knife\',\n            \'spoon\', \'bowl\', \'banana\', \'apple\', \'sandwich\', \'orange\',\n            \'broccoli\', \'carrot\', \'hot dog\', \'pizza\',\n            \'donut\', \'cake\', \'chair\', \'couch\', \'potted plant\', \'bed\',\n            \'dining table\', \'toilet\', \'tv\', \'laptop\',\n            \'mouse\', \'remote\', \'keyboard\', \'cell phone\', \'microwave\', \'oven\',\n            \'toaster\', \'sink\', \'refrigerator\', \'book\',\n            \'clock\', \'vase\', \'scissors\', \'teddy bear\', \'hair drier\', \'toothbrush\']\n\n\ndef id_to_category(category_id):\n    return {cid: categories()[idx] for idx, cid in enumerate(ids())}[category_id]\n\n\ndef category_to_cid_map():\n    return {category: ids()[idx] for idx, category in enumerate(categories())}\n\n\ndef mkdir_p(path):\n    # http://stackoverflow.com/questions/600268/mkdir-p-functionality-in-python\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\n\n# ============== Ingredient 2: dataset =======================\ndata_coco = Experiment(""dataset"")\n\n\n@data_coco.config\ndef coco_config():\n    # TODO(ahundt) add md5 sums for each file\n    verbose = 1\n    coco_api = \'https://github.com/pdollar/coco/\'\n    dataset_root = os.path.join(os.path.expanduser(\'~\'), \'datasets\')\n    dataset_path = os.path.join(dataset_root, \'coco\')\n    urls = [\n        \'coco2014/train2014.zip\',\n        \'coco2014/val2014.zip\',\n        \'coco2014/test2014.zip\',\n        \'coco2015/test2015.zip\',\n        \'annotations-1-0-3/instances_train-val2014.zip\',\n        \'annotations-1-0-3/person_keypoints_trainval2014.zip\',\n        \'annotations-1-0-4/image_info_test2014.zip\',\n        \'annotations-1-0-4/image_info_test2015.zip\',\n        \'annotations-1-0-3/captions_train-val2014.zip\'\n    ]\n    base_url = \'http://msvocds.blob.core.windows.net/\'\n    urls = [base_url + x for x in urls]\n    data_prefixes = [\n        \'train2014\',\n        \'val2014\',\n        \'test2014\',\n        \'test2015\',\n    ]\n    image_filenames = [prefix + \'.zip\' for prefix in data_prefixes]\n    annotation_filenames = [\n        \'instances_train-val2014.zip\',  # training AND validation info\n        \'image_info_test2014.zip\',  # basic info like download links + category\n        \'image_info_test2015.zip\',  # basic info like download links + category\n        \'person_keypoints_trainval2014.zip\',  # elbows, head, wrist etc\n        \'captions_train-val2014.zip\',  # descriptions of images\n    ]\n    md5s = [\n        \'0da8c0bd3d6becc4dcb32757491aca88\',  # train2014.zip\n        \'a3d79f5ed8d289b7a7554ce06a5782b3\',  # val2014.zip\n        \'04127eef689ceac55e3a572c2c92f264\',  # test2014.zip\n        \'65562e58af7d695cc47356951578c041\',  # test2015.zip\n        \'59582776b8dd745d649cd249ada5acf7\',  # instances_train-val2014.zip\n        \'926b9df843c698817ee62e0e049e3753\',  # person_keypoints_trainval2014.zip\n        \'f3366b66dc90d8ae0764806c95e43c86\',  # image_info_test2014.zip\n        \'8a5ad1a903b7896df7f8b34833b61757\',  # image_info_test2015.zip\n        \'5750999c8c964077e3c81581170be65b\'   # captions_train-val2014.zip\n    ]\n    filenames = image_filenames + annotation_filenames\n    seg_mask_path = os.path.join(dataset_path, \'seg_mask\')\n    annotation_json = [\n        \'annotations/instances_train2014.json\',\n        \'annotations/instances_val2014.json\'\n    ]\n    annotation_paths = [os.path.join(dataset_path, postfix)\n                        for postfix in annotation_json]\n    # only first two data prefixes contain segmentation masks\n    seg_mask_image_paths = [os.path.join(dataset_path, prefix)\n                            for prefix in data_prefixes[0:1]]\n    seg_mask_output_paths = [os.path.join(seg_mask_path, prefix)\n                             for prefix in data_prefixes[0:1]]\n    seg_mask_extensions = [\'.npy\' for prefix in data_prefixes[0:1]]\n    image_dirs = [os.path.join(dataset_path, prefix) for prefix in data_prefixes]\n    image_extensions = [\'.jpg\' for prefix in data_prefixes]\n    voc_imageset_txt_paths = [os.path.join(dataset_path,\n                                           \'annotations\', prefix + \'.txt\')\n                              for prefix in data_prefixes]\n\n\n@data_coco.capture\ndef coco_files(dataset_path, filenames, dataset_root, urls, md5s, annotation_paths):\n    print(dataset_path)\n    print(dataset_root)\n    print(urls)\n    print(filenames)\n    print(md5s)\n    print(annotation_paths)\n    return [os.path.join(dataset_path, file) for file in filenames]\n\n\n@data_coco.command\ndef print_coco_files(dataset_path, filenames, dataset_root,\n                     urls, md5s, annotation_paths):\n    coco_files(dataset_path, filenames, dataset_root, urls, md5s, annotation_paths)\n\n\n@data_coco.command\ndef coco_download(dataset_path, filenames, dataset_root,\n                  urls, md5s, annotation_paths):\n    zip_paths = coco_files(dataset_path, filenames, dataset_root,\n                           urls, md5s, annotation_paths)\n    for url, filename, md5 in zip(urls, filenames, md5s):\n        path = get_file(filename, url, md5_hash=md5,\n                        extract=True, cache_subdir=dataset_path)\n        # TODO(ahundt) check if it is already extracted, don\'t re-extract. see\n        # https://github.com/fchollet/keras/issues/5861\n        zip_file = zipfile.ZipFile(path, \'r\')\n        zip_file.extractall(path=dataset_path)\n        zip_file.close()\n\n\n@data_coco.command\ndef coco_json_to_segmentation(seg_mask_output_paths,\n                              annotation_paths, seg_mask_image_paths, verbose):\n    for (seg_mask_path, annFile, image_path) in zip(\n            seg_mask_output_paths, annotation_paths, seg_mask_image_paths):\n        print(\'Loading COCO Annotations File: \', annFile)\n        print(\'Segmentation Mask Output Folder: \', seg_mask_path)\n        print(\'Source Image Folder: \', image_path)\n        print(\'\\n\'\n              \'WARNING: Each pixel can have multiple classes! That means\'\n              \'class data overlaps. Also, single objects can be outlined\'\n              \'multiple times because they were labeled by different people!\'\n              \'In other words, even a single object may be segmented twice.\'\n              \'This means the .png files are missing entire objects.\\n\\n\'\n              \'Use of categorical one-hot encoded .npy files is recommended,\'\n              \'but .npy files also have limitations, because the .npy files\'\n              \'only have one label per pixel for each class,\'\n              \'and currently take the union of multiple human class labels.\'\n              \'Improving how your data is handled will improve your results\'\n              \'so remember to consider that limitation. There is still\'\n              \'an opportunity to improve how this training data is handled &\'\n              \'integrated with your training scripts and utilities...\')\n        coco = COCO(annFile)\n\n        print(\'Converting Annotations to Segmentation Masks...\')\n        mkdir_p(seg_mask_path)\n        total_imgs = len(coco.imgToAnns.keys())\n        progbar = Progbar(total_imgs + len(coco.getImgIds()), verbose=verbose)\n        # \'annotations\' was previously \'instances\' in an old version\n        for img_num in range(total_imgs):\n            # Both [0]\'s are used to extract the element from a list\n            img = coco.loadImgs(\n                coco.imgToAnns[coco.imgToAnns.keys()[img_num]][0][\'image_id\'])[0]\n            h = img[\'height\']\n            w = img[\'width\']\n            name = img[\'file_name\']\n            root_name = name[:-4]\n            filename = os.path.join(seg_mask_path, root_name + "".png"")\n            file_exists = os.path.exists(filename)\n            if file_exists:\n                progbar.update(img_num, [(\'file_fraction_already_exists\', 1)])\n                continue\n            else:\n                progbar.update(img_num, [(\'file_fraction_already_exists\', 0)])\n                print(filename)\n\n            MASK = np.zeros((h, w), dtype=np.uint8)\n            np.where(MASK > 0)\n            for ann in coco.imgToAnns[coco.imgToAnns.keys()[img_num]]:\n                mask = coco.annToMask(ann)\n                idxs = np.where(mask > 0)\n                MASK[idxs] = ann[\'category_id\']\n\n            im = Image.fromarray(MASK)\n            im.save(filename)\n\n        print(\'\\nConverting Annotations to one hot encoded\'\n              \'categorical .npy Segmentation Masks...\')\n        img_ids = coco.getImgIds()\n        use_original_dims = True  # not target_shape\n        for idx, img_id in enumerate(img_ids):\n            img = coco.loadImgs(img_id)[0]\n            name = img[\'file_name\']\n            root_name = name[:-4]\n            filename = os.path.join(seg_mask_path, root_name + "".npy"")\n            file_exists = os.path.exists(filename)\n            if file_exists:\n                progbar.add(1, [(\'file_fraction_already_exists\', 1)])\n                continue\n            else:\n                progbar.add(1, [(\'file_fraction_already_exists\', 0)])\n\n            if use_original_dims:\n                target_shape = (img[\'height\'], img[\'width\'], max(ids()) + 1)\n            ann_ids = coco.getAnnIds(imgIds=img[\'id\'], iscrowd=None)\n            anns = coco.loadAnns(ann_ids)\n            mask_one_hot = np.zeros(target_shape, dtype=np.uint8)\n            mask_one_hot[:, :, 0] = 1  # every pixel begins as background\n            # mask_one_hot = cv2.resize(mask_one_hot,\n            #                           target_shape[:2],\n            #                           interpolation=cv2.INTER_NEAREST)\n\n            for ann in anns:\n                mask_partial = coco.annToMask(ann)\n                # mask_partial = cv2.resize(mask_partial,\n                #                           (target_shape[1], target_shape[0]),\n                #                           interpolation=cv2.INTER_NEAREST)\n                # # width and height match\n                # assert mask_one_hot.shape[:2] == mask_partial.shape[:2]\n                #    print(\'another shape:\',\n                #          mask_one_hot[mask_partial > 0].shape)\n                mask_one_hot[mask_partial > 0, ann[\'category_id\']] = 1\n                mask_one_hot[mask_partial > 0, 0] = 0\n\n            np.save(filename, mask_one_hot)\n\n\n@data_coco.command\ndef coco_to_pascal_voc_imageset_txt(voc_imageset_txt_paths, image_dirs,\n                                    image_extensions):\n    # os.environ[""CUDA_VISIBLE_DEVICES""] = \'1\'\n    # Get some image/annotation pairs for example\n    for imgset_path, img_dir, t_ext in zip(\n            voc_imageset_txt_paths, image_dirs, image_extensions):\n        with open(imgset_path, \'w\') as txtfile:\n            [txtfile.write(os.path.splitext(os.path.basename(file))[0] + \'\\n\')\n             for file in os.listdir(img_dir) if file.endswith(t_ext)]\n\n\n@data_coco.command\ndef coco_image_segmentation_stats(seg_mask_output_paths, annotation_paths,\n                                  seg_mask_image_paths, verbose):\n    for (seg_mask_path, annFile, image_path) in zip(\n            seg_mask_output_paths, annotation_paths, seg_mask_image_paths):\n        print(\'Loading COCO Annotations File: \', annFile)\n        print(\'Segmentation Mask Output Folder: \', seg_mask_path)\n        print(\'Source Image Folder: \', image_path)\n        stats_json = os.path.join(seg_mask_path,\n                                  \'image_segmentation_class_stats.json\')\n        print(\'Image stats will be saved to:\', stats_json)\n        cat_csv = os.path.join(seg_mask_path,\n                               \'class_counts_over_sum_category_counts.csv\')\n        print(\'Category weights will be saved to:\', cat_csv)\n        coco = COCO(annFile)\n        print(\'Annotation file info:\')\n        coco.info()\n        print(\'category ids, not including 0 for background:\')\n        print(coco.getCatIds())\n        # display COCO categories and supercategories\n        cats = coco.loadCats(coco.getCatIds())\n        nms = [cat[\'name\'] for cat in cats]\n        print(\'categories: \\n\\n\', \' \'.join(nms))\n\n        nms = set([cat[\'supercategory\'] for cat in cats])\n        print(\'supercategories: \\n\', \' \'.join(nms))\n        img_ids = coco.getImgIds()\n        use_original_dims = True  # not target_shape\n        max_ids = max(ids()) + 1  # add background category\n        # 0 indicates no category (not even background) for counting bins\n        max_bin_count = max_ids + 1\n        bin_count = np.zeros(max_bin_count)\n        total_pixels = 0\n\n        print(\'Calculating image segmentation stats...\')\n        progbar = Progbar(len(img_ids), verbose=verbose)\n        i = 0\n        for idx, img_id in enumerate(img_ids):\n            img = coco.loadImgs(img_id)[0]\n            i += 1\n            progbar.update(i)\n            ann_ids = coco.getAnnIds(imgIds=img[\'id\'], iscrowd=None)\n            anns = coco.loadAnns(ann_ids)\n            target_shape = (img[\'height\'], img[\'width\'], max_ids)\n            # print(\'\\ntarget_shape:\', target_shape)\n            mask_one_hot = np.zeros(target_shape, dtype=np.uint8)\n\n            # Note to only count background pixels once, we define a temporary\n            # null class of 0, and shift all class category ids up by 1\n            mask_one_hot[:, :, 0] = 1  # every pixel begins as background\n\n            for ann in anns:\n                mask_partial = coco.annToMask(ann)\n                above_zero = mask_partial > 0\n                mask_one_hot[above_zero, ann[\'category_id\']] = ann[\'category_id\'] + 1\n                mask_one_hot[above_zero, 0] = 0\n\n            # print( mask_one_hot)\n            # print(\'initial bin_count shape:\', np.shape(bin_count))\n            # flat_mask_one_hot = mask_one_hot.flatten()\n            bincount_result = np.bincount(mask_one_hot.flatten())\n            # print(\'bincount_result TYPE:\', type(bincount_result))\n            # np.array(np.ndarray.flatten(np.bincount(np.ndarray.\n            # flatten(np.array(mask_one_hot)).astype(int))).resize(max_bin_count))\n            # print(\'bincount_result:\', bincount_result)\n            # print(\'bincount_result_shape\', np.shape(bincount_result))\n            length = int(np.shape(bincount_result)[0])\n            zeros_to_add = max_bin_count - length\n            z = np.zeros(zeros_to_add)\n            # print(\'zeros_to_add TYPE:\', type(zeros_to_add))\n            # this is a workaround because for some strange reason the\n            # output type of bincount couldn\'t interact with other numpy arrays\n            bincount_result_long = bincount_result.tolist() + z.tolist()\n            # bincount_result = bincount_result.resize(max_bin_count)\n            # print(\'bincount_result2:\', bincount_result_long)\n            # print(\'bincount_result2_shape\',bincount_result_long)\n            bin_count = bin_count + np.array(bincount_result_long)\n            total_pixels += (img[\'height\'] * img[\'width\'])\n\n        print(\'Final Tally:\')\n        # shift categories back down by 1\n        bin_count = bin_count[1:]\n        category_ids = range(bin_count.size)\n        sum_category_counts = np.sum(bin_count)\n\n        # sum will be =1 as a pixel can be in multiple categories\n        category_counts_over_sum_category_counts = \\\n            np.true_divide(bin_count.astype(np.float64), sum_category_counts)\n        np.savetxt(cat_csv, category_counts_over_sum_category_counts)\n\n        # sum will be >1 as a pixel can be in multiple categories\n        category_counts_over_total_pixels = \\\n            np.true_divide(bin_count.astype(np.float64), total_pixels)\n\n        # less common categories have more weight, sum = 1\n        category_counts_p_complement = \\\n            [1 - x if x > 0.0 else 0.0\n             for x in category_counts_over_sum_category_counts]\n\n        # less common categories have more weight, sum > 1\n        total_pixels_p_complement = \\\n            [1 - x if x > 0.0 else 0.0\n             for x in category_counts_over_total_pixels]\n\n        print(bin_count)\n        stat_dict = {\n            \'total_pixels\': total_pixels,\n            \'category_counts\': dict(zip(category_ids, bin_count)),\n            \'sum_category_counts\': sum_category_counts,\n            \'category_counts_over_sum_category_counts\':\n                dict(zip(category_ids,\n                         category_counts_over_sum_category_counts)),\n            \'category_counts_over_total_pixels\':\n                dict(zip(category_ids, category_counts_over_total_pixels)),\n            \'category_counts_p_complement\':\n                dict(zip(category_ids, category_counts_p_complement)),\n            \'total_pixels_p_complement\':\n                dict(zip(category_ids, total_pixels_p_complement)),\n            \'ids\': ids(),\n            \'categories\': categories()\n        }\n        print(stat_dict)\n        with open(stats_json, \'w\') as fjson:\n            json.dump(stat_dict, fjson, ensure_ascii=False)\n\n\n@data_coco.command\ndef coco_setup(dataset_root, dataset_path, data_prefixes,\n               filenames, urls, md5s, annotation_paths,\n               image_dirs, seg_mask_output_paths, verbose,\n               image_extensions):\n    # download the dataset\n    coco_download(dataset_path, filenames, dataset_root,\n                  urls, md5s, annotation_paths)\n    # convert the relevant files to a more useful format\n    coco_json_to_segmentation(seg_mask_output_paths, annotation_paths)\n    coco_to_pascal_voc_imageset_txt(voc_imageset_txt_paths, image_dirs,\n                                    image_extensions)\n\n\n@data_coco.automain\ndef main(dataset_root, dataset_path, data_prefixes,\n         filenames, urls, md5s, annotation_paths,\n         image_dirs, seg_mask_output_paths):\n    coco_config()\n    coco_setup(data_prefixes, dataset_path, filenames, dataset_root, urls,\n               md5s, annotation_paths, image_dirs,\n               seg_mask_output_paths)\n'"
keras_contrib/datasets/conll2000.py,0,"b""from __future__ import print_function\nimport numpy\nfrom keras.utils.data_utils import get_file\nfrom zipfile import ZipFile\nfrom collections import Counter\nfrom keras.preprocessing.sequence import pad_sequences\n\n\ndef load_data(path='conll2000.zip', min_freq=2):\n    path = get_file(path,\n                    origin='https://raw.githubusercontent.com/nltk'\n                           '/nltk_data/gh-pages/packages/corpora/conll2000.zip')\n    print(path)\n    archive = ZipFile(path, 'r')\n    train = _parse_data(archive.open('conll2000/train.txt'))\n    test = _parse_data(archive.open('conll2000/test.txt'))\n    archive.close()\n\n    word_counts = Counter(row[0].lower() for sample in train for row in sample)\n    vocab = ['<pad>', '<unk>']\n    vocab += [w for w, f in iter(word_counts.items()) if f >= min_freq]\n    # in alphabetic order\n    pos_tags = sorted(list(set(row[1] for sample in train + test for row in sample)))\n    # in alphabetic order\n    chunk_tags = sorted(list(set(row[2] for sample in train + test for row in sample)))\n\n    train = _process_data(train, vocab, pos_tags, chunk_tags)\n    test = _process_data(test, vocab, pos_tags, chunk_tags)\n    return train, test, (vocab, pos_tags, chunk_tags)\n\n\ndef _parse_data(fh):\n    string = fh.read()\n    data = []\n    for sample in string.decode().strip().split('\\n\\n'):\n        data.append([row.split() for row in sample.split('\\n')])\n    fh.close()\n    return data\n\n\ndef _process_data(data, vocab, pos_tags, chunk_tags, maxlen=None, onehot=False):\n    if maxlen is None:\n        maxlen = max(len(s) for s in data)\n    word2idx = dict((w, i) for i, w in enumerate(vocab))\n    # set to <unk> (index 1) if not in vocab\n    x = [[word2idx.get(w[0].lower(), 1) for w in s] for s in data]\n\n    y_pos = [[pos_tags.index(w[1]) for w in s] for s in data]\n    y_chunk = [[chunk_tags.index(w[2]) for w in s] for s in data]\n\n    x = pad_sequences(x, maxlen)  # left padding\n\n    # lef padded with -1. Indeed, any integer works as it will be masked\n    y_pos = pad_sequences(y_pos, maxlen, value=-1)\n    y_chunk = pad_sequences(y_chunk, maxlen, value=-1)\n\n    if onehot:\n        y_pos = numpy.eye(len(pos_tags), dtype='float32')[y]\n        y_chunk = numpy.eye(len(chunk_tags), dtype='float32')[y]\n    else:\n        y_pos = numpy.expand_dims(y_pos, 2)\n        y_chunk = numpy.expand_dims(y_chunk, 2)\n    return x, y_pos, y_chunk\n"""
keras_contrib/datasets/pascal_voc.py,0,"b'#!/usr/bin/env python\n# coding=utf-8\n""""""\nThis is a script for downloading and converting the pascal voc 2012 dataset\nand the berkeley extended version.\n\n# original PASCAL VOC 2012\n# 2 GB\n# http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n\n# berkeley augmented Pascal VOC\n# 1.3 GB\n# http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz\n\nThis can be run as an independent executable to download\nthe dataset or be imported by scripts used for larger experiments.\n\nIf you aren\'t sure run this to do a full download + conversion setup of the dataset:\n   ./data_pascal_voc.py pascal_voc_setup\n""""""  # pylint: disable=E501\nfrom __future__ import division, print_function, unicode_literals\nimport os\nimport shutil\nimport errno\nfrom sacred import Ingredient, Experiment\nfrom keras.utils import get_file\nimport skimage.io as io\n\n\n# ============== Ingredient 2: dataset =======================\ndata_pascal_voc = Experiment(""dataset"")\n\n\ndef mkdir_p(path):\n    # http://stackoverflow.com/questions/600268/mkdir-p-functionality-in-python\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\n\ndef pascal_segmentation_lut():\n    """"""Return look-up table with number and correspondng class names\n    for PASCAL VOC segmentation dataset. Two special classes are: 0 -\n    background and 255 - ambigious region. All others are numerated from\n    1 to 20.\n\n    Returns\n    -------\n    classes_lut : dict\n        look-up table with number and correspondng class names\n    """"""\n\n    class_names = [\'background\', \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                   \'bottle\', \'bus\', \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\',\n                   \'dog\', \'horse\', \'motorbike\', \'person\', \'potted-plant\',\n                   \'sheep\', \'sofa\', \'train\', \'tv/monitor\', \'ambigious\']\n\n    enumerated_array = enumerate(class_names[:-1])\n\n    classes_lut = list(enumerated_array)\n\n    # Add a special class representing ambigious regions\n    # which has index 255.\n    classes_lut.append((255, class_names[-1]))\n\n    classes_lut = dict(classes_lut)\n\n    return classes_lut\n\n\ndef get_pascal_segmentation_images_lists_txts(pascal_root):\n    """"""Return full paths to files in PASCAL VOC with train and val image name lists.\n    This function returns full paths to files which contain names of images\n    and respective annotations for the segmentation in PASCAL VOC.\n\n    Parameters\n    ----------\n    pascal_root : string\n        Full path to the root of PASCAL VOC dataset.\n\n    Returns\n    -------\n    full_filenames_txts : [string, string, string]\n        Array that contains paths for train/val/trainval txts with images names.\n    """"""\n\n    segmentation_relative_folder = \'ImageSets/Segmentation\'\n\n    segmentation_folder = os.path.join(pascal_root, segmentation_relative_folder)\n\n    pascal_train_list_filename = os.path.join(segmentation_folder, \'train.txt\')\n\n    pascal_validation_list_filename = os.path.join(segmentation_folder, \'val.txt\')\n\n    pascal_trainval_list_filename = os.path.join(segmentation_folder, \'trainval.txt\')\n\n    return [\n        pascal_train_list_filename,\n        pascal_validation_list_filename,\n        pascal_trainval_list_filename\n    ]\n\n\ndef readlines_with_strip(filename):\n    """"""Reads lines from specified file with whitespaced removed on both sides.\n    The function reads each line in the specified file and applies string.strip()\n    function to each line which results in removing all whitespaces on both ends\n    of each string. Also removes the newline symbol which is usually present\n    after the lines wre read using readlines() function.\n\n    Parameters\n    ----------\n    filename : string\n        Full path to the root of PASCAL VOC dataset.\n\n    Returns\n    -------\n    clean_lines : array of strings\n        Strings that were read from the file and cleaned up.\n    """"""\n\n    # Get raw filnames from the file\n    with open(filename, \'r\') as f:\n        lines = f.readlines()\n\n    # Clean filenames from whitespaces and newline symbols\n    return map(lambda x: x.strip(), lines)\n\n\ndef readlines_with_strip_array_version(filenames_array):\n    """"""The function that is similar to readlines_with_strip() but for filenames array.\n    Applies readlines_with_strip() to each filename in the array.\n\n    Parameters\n    ----------\n    filenames_array : array of strings\n        Array of strings. Each specifies a path to a file.\n\n    Returns\n    -------\n    clean_lines : array of (array of strings)\n        Strings that were read from the file and cleaned up.\n    """"""\n\n    return map(readlines_with_strip, filenames_array)\n\n\ndef add_full_path_and_extention_to_filenames(filenames_array, full_path, extention):\n    """"""Concatenates full path to the left of the image and file extention to the right.\n    The function accepts array of filenames without fullpath and extention like \'cat\'\n    and adds specified full path and extetion to each of the filenames in the array like\n    \'full/path/to/somewhere/cat.jpg.\n    Parameters\n    ----------\n    filenames_array : array of strings\n        Array of strings representing filenames\n    full_path : string\n        Full path string to be added on the left to each filename\n    extention : string\n        Extention string to be added on the right to each filename\n    Returns\n    -------\n    full_filenames : array of strings\n        updated array with filenames\n    """"""\n    return map(lambda x: os.path.join(full_path, x) + \'.\' + extention, filenames_array)\n\n\ndef add_full_path_and_extention_to_filenames_array_version(filenames_array_array,\n                                                           full_path,\n                                                           extention):\n    """"""Array version of the add_full_path_and_extention_to_filenames() function.\n    Applies add_full_path_and_extention_to_filenames() to each element of array.\n    Parameters\n    ----------\n    filenames_array_array : array of array of strings\n        Array of strings representing filenames\n    full_path : string\n        Full path string to be added on the left to each filename\n    extention : string\n        Extention string to be added on the right to each filename\n    Returns\n    -------\n    full_filenames : array of array of strings\n        updated array of array with filenames\n    """"""\n    return map(lambda x: add_full_path_and_extention_to_filenames(x,\n                                                                  full_path,\n                                                                  extention),\n               filenames_array_array)\n\n\ndef get_pascal_segmentation_image_annotation_filenames_pairs(pascal_root):\n    """"""Return (image, annotation) filenames pairs from PASCAL VOC segmentation dataset.\n    Returns three dimensional array where first dimension represents the type\n    of the dataset: train, val or trainval in the respective order. Second\n    dimension represents the a pair of images in that belongs to a particular\n    dataset. And third one is responsible for the first or second element in the\n    dataset.\n    Parameters\n    ----------\n    pascal_root : string\n        Path to the PASCAL VOC dataset root that is usually named \'VOC2012\'\n        after being extracted from tar file.\n    Returns\n    -------\n    image_annotation_filename_pairs :\n        Array with filename pairs.\n    """"""\n\n    pascal_relative_images_folder = \'JPEGImages\'\n    pascal_relative_class_annotations_folder = \'SegmentationClass\'\n\n    images_extention = \'jpg\'\n    annotations_extention = \'png\'\n\n    pascal_images_folder = os.path.join(\n        pascal_root, pascal_relative_images_folder)\n    pascal_class_annotations_folder = os.path.join(\n        pascal_root, pascal_relative_class_annotations_folder)\n\n    pascal_images_lists_txts = get_pascal_segmentation_images_lists_txts(\n        pascal_root)\n\n    pascal_image_names = readlines_with_strip_array_version(\n        pascal_images_lists_txts)\n\n    images_full_names = add_full_path_and_extention_to_filenames_array_version(\n        pascal_image_names,\n        pascal_images_folder,\n        images_extention,\n    )\n\n    annotations_full_names = add_full_path_and_extention_to_filenames_array_version(\n        pascal_image_names,\n        pascal_class_annotations_folder,\n        annotations_extention,\n    )\n\n    # Combine so that we have [(images full filenames, annotation full names), .. ]\n    # where each element in the array represent train, val, trainval sets.\n    # Overall, we have 3 elements in the array.\n    temp = zip(images_full_names, annotations_full_names)\n\n    # Now we should combine the elements of images full filenames annotation full names\n    # so that we have pairs of respective image plus annotation\n    # [[(pair_1), (pair_1), ..], [(pair_1), (pair_2), ..] ..]\n    # Overall, we have 3 elements -- representing train/val/trainval datasets\n    image_annotation_filename_pairs = map(lambda x: zip(*x), temp)\n\n    return image_annotation_filename_pairs\n\n\n@data_pascal_voc.command\ndef convert_pascal_berkeley_augmented_mat_annotations_to_png(\n        pascal_berkeley_augmented_root):\n    """""" Creates a new folder in the root folder of the dataset with annotations stored\n    in .png. The function accepts a full path to the root of Berkeley augmented Pascal\n    VOC segmentation dataset and converts annotations that are stored in .mat files to\n    .png files. It creates a new folder dataset/cls_png where all the converted files\n    will be located. If this directory already exists the function does nothing. The\n    Berkley augmented dataset can be downloaded from here:\n    http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz\n\n    Parameters\n    ----------\n    pascal_berkeley_augmented_root : string\n        Full path to the root of augmented Berkley PASCAL VOC dataset.\n\n    """"""  # pylint: disable=E501\n\n    import scipy.io\n\n    def read_class_annotation_array_from_berkeley_mat(mat_filename, key=\'GTcls\'):\n\n        # Mat to png conversion for\n        # http://www.cs.berkeley.edu/~bharath2/codes/SBD/download.html\n        # \'GTcls\' key is for class segmentation\n        # \'GTinst\' key is for instance segmentation\n        # Credit:\n        # https://github.com/martinkersner/train-DeepLab/blob/master/utils.py\n\n        mat = scipy.io.loadmat(mat_filename, mat_dtype=True,\n                               squeeze_me=True, struct_as_record=False)\n        return mat[key].Segmentation\n\n    mat_file_extension_string = \'.mat\'\n    png_file_extension_string = \'.png\'\n    relative_path_to_annotation_mat_files = \'dataset/cls\'\n    relative_path_to_annotation_png_files = \'dataset/cls_png\'\n\n    mat_file_extension_string_length = len(mat_file_extension_string)\n\n    annotation_mat_files_fullpath = os.path.join(pascal_berkeley_augmented_root,\n                                                 relative_path_to_annotation_mat_files)\n\n    annotation_png_save_fullpath = os.path.join(pascal_berkeley_augmented_root,\n                                                relative_path_to_annotation_png_files)\n\n    # Create the folder where all the converted png files will be placed\n    # If the folder already exists, do nothing\n    if not os.path.exists(annotation_png_save_fullpath):\n        os.makedirs(annotation_png_save_fullpath)\n    else:\n        return\n\n    mat_files_names = os.listdir(annotation_mat_files_fullpath)\n\n    for current_mat_file_name in mat_files_names:\n\n        current_file_name_without_extention = current_mat_file_name[\n            :-mat_file_extension_string_length]\n\n        current_mat_file_full_path = os.path.join(annotation_mat_files_fullpath,\n                                                  current_mat_file_name)\n\n        current_png_file_full_path_to_be_saved = os.path.join(\n            annotation_png_save_fullpath,\n            current_file_name_without_extention,\n        )\n\n        current_png_file_full_path_to_be_saved += png_file_extension_string\n\n        annotation_array = read_class_annotation_array_from_berkeley_mat(\n            current_mat_file_full_path)\n\n        # TODO: hide \'low-contrast\' image warning during saving.\n        io.imsave(current_png_file_full_path_to_be_saved, annotation_array)\n\n\ndef get_pascal_berkeley_augmented_segmentation_images_lists_txts(pascal_berkeley_root):\n    """"""Return full paths to files in PASCAL Berkley augmented VOC with train and\n    val image name lists. This function returns full paths to files which contain names\n    of images and respective annotations for the segmentation in PASCAL VOC.\n\n    Parameters\n    ----------\n    pascal_berkeley_root : string\n        Full path to the root of PASCAL VOC Berkley augmented dataset.\n\n    Returns\n    -------\n    full_filenames_txts : [string, string]\n        Array that contains paths for train/val txts with images names.\n    """"""\n\n    segmentation_relative_folder = \'dataset\'\n\n    segmentation_folder = os.path.join(pascal_berkeley_root,\n                                       segmentation_relative_folder)\n\n    # TODO: add function that will joing both train.txt and val.txt into\n    # trainval.txt\n    pascal_train_list_filename = os.path.join(segmentation_folder,\n                                              \'train.txt\')\n\n    pascal_validation_list_filename = os.path.join(segmentation_folder,\n                                                   \'val.txt\')\n\n    return [\n        pascal_train_list_filename,\n        pascal_validation_list_filename\n    ]\n\n\ndef get_pascal_berkeley_augmented_segmentation_image_annotation_filenames_pairs(\n        pascal_berkeley_root):\n    """"""Return (image, annotation) filenames pairs from PASCAL Berkeley VOC segmentation\n    dataset. Returns three dimensional array where first dimension represents the type\n    of the dataset: train, val in the respective order. Second\n    dimension represents the a pair of images in that belongs to a particular\n    dataset. And third one is responsible for the first or second element in the\n    dataset.\n    Parameters\n    ----------\n    pascal_berkeley_root : string\n        Path to the PASCAL Berkeley VOC dataset root that is usually named\n        \'benchmark_RELEASE\' after being extracted from tar file.\n    Returns\n    -------\n    image_annotation_filename_pairs :\n        Array with filename pairs.\n    """"""\n\n    pascal_relative_images_folder = \'dataset/img\'\n    pascal_relative_class_annotations_folder = \'dataset/cls_png\'\n\n    images_extention = \'jpg\'\n    annotations_extention = \'png\'\n\n    pascal_images_folder = os.path.join(\n        pascal_berkeley_root, pascal_relative_images_folder)\n    pascal_class_annotations_folder = os.path.join(\n        pascal_berkeley_root, pascal_relative_class_annotations_folder)\n\n    pascal_images_lists_txts = (\n        get_pascal_berkeley_augmented_segmentation_images_lists_txts(\n            pascal_berkeley_root))\n\n    pascal_image_names = readlines_with_strip_array_version(\n        pascal_images_lists_txts)\n\n    images_full_names = add_full_path_and_extention_to_filenames_array_version(\n        pascal_image_names,\n        pascal_images_folder,\n        images_extention,\n    )\n\n    annotations_full_names = add_full_path_and_extention_to_filenames_array_version(\n        pascal_image_names,\n        pascal_class_annotations_folder,\n        annotations_extention,\n    )\n\n    # Combine so that we have [(images full filenames, annotation full names), .. ]\n    # where each element in the array represent train, val, trainval sets.\n    # Overall, we have 3 elements in the array.\n    temp = zip(images_full_names, annotations_full_names)\n\n    # Now we should combine the elements of images full filenames annotation full names\n    # so that we have pairs of respective image plus annotation\n    # [[(pair_1), (pair_1), ..], [(pair_1), (pair_2), ..] ..]\n    # Overall, we have 3 elements -- representing train/val/trainval datasets\n    image_annotation_filename_pairs = map(lambda x: zip(*x), temp)\n\n    return image_annotation_filename_pairs\n\n\ndef get_pascal_berkeley_augmented_selected_image_annotation_filenames_pairs(\n        pascal_berkeley_root,\n        selected_names,\n):\n    """"""Returns (image, annotation) filenames pairs from PASCAL Berkeley VOC segmentation\n    dataset for selected names. The function accepts the selected file names from PASCAL\n    Berkeley VOC segmentation dataset and returns image, annotation pairs with fullpath\n    and extention for those names.\n\n    Parameters\n    ----------\n    pascal_berkeley_root : string\n        Path to the PASCAL Berkeley VOC dataset root that is usually named\n        \'benchmark_RELEASE\' after being extracted from tar file.\n    selected_names : array of strings\n        Selected filenames from PASCAL VOC Berkeley that can be read from txt files that\n        come with dataset.\n    Returns\n    -------\n    image_annotation_pairs :\n        Array with filename pairs with fullnames.\n    """"""\n    pascal_relative_images_folder = \'dataset/img\'\n    pascal_relative_class_annotations_folder = \'dataset/cls_png\'\n\n    images_extention = \'jpg\'\n    annotations_extention = \'png\'\n\n    pascal_images_folder = os.path.join(\n        pascal_berkeley_root, pascal_relative_images_folder)\n    pascal_class_annotations_folder = os.path.join(\n        pascal_berkeley_root, pascal_relative_class_annotations_folder)\n\n    images_full_names = add_full_path_and_extention_to_filenames(\n        selected_names,\n        pascal_images_folder,\n        images_extention,\n    )\n\n    annotations_full_names = add_full_path_and_extention_to_filenames(\n        selected_names,\n        pascal_class_annotations_folder,\n        annotations_extention,\n    )\n\n    image_annotation_pairs = zip(images_full_names,\n                                 annotations_full_names)\n\n    return image_annotation_pairs\n\n\ndef get_pascal_selected_image_annotation_filenames_pairs(pascal_root, selected_names):\n    """"""Returns (image, annotation) filenames pairs from PASCAL VOC segmentation dataset\n    for selected names. The function accepts the selected file names from PASCAL VOC\n    segmentation dataset and returns image, annotation pairs with fullpath and extention\n    for those names.\n\n    Parameters\n    ----------\n    pascal_root : string\n        Path to the PASCAL VOC dataset root that is usually named \'VOC2012\'\n        after being extracted from tar file.\n    selected_names : array of strings\n        Selected filenames from PASCAL VOC that can be read from txt files that\n        come with dataset.\n    Returns\n    -------\n    image_annotation_pairs :\n        Array with filename pairs with fullnames.\n    """"""\n    pascal_relative_images_folder = \'JPEGImages\'\n    pascal_relative_class_annotations_folder = \'SegmentationClass\'\n\n    images_extention = \'jpg\'\n    annotations_extention = \'png\'\n\n    pascal_images_folder = os.path.join(\n        pascal_root, pascal_relative_images_folder)\n    pascal_class_annotations_folder = os.path.join(\n        pascal_root, pascal_relative_class_annotations_folder)\n\n    images_full_names = add_full_path_and_extention_to_filenames(selected_names,\n                                                                 pascal_images_folder,\n                                                                 images_extention)\n\n    annotations_full_names = add_full_path_and_extention_to_filenames(\n        selected_names,\n        pascal_class_annotations_folder,\n        annotations_extention,\n    )\n\n    image_annotation_pairs = zip(images_full_names,\n                                 annotations_full_names)\n\n    return image_annotation_pairs\n\n\ndef get_augmented_pascal_image_annotation_filename_pairs(pascal_root,\n                                                         pascal_berkeley_root,\n                                                         mode=2):\n    """"""Returns image/annotation filenames pairs train/val splits from combined Pascal\n    VOC. Returns two arrays with train and validation split respectively that has\n    image full filename/ annotation full filename pairs in each of the that were derived\n    from PASCAL and PASCAL Berkeley Augmented dataset. The Berkley augmented dataset\n    can be downloaded from here:\n    http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz\n    Consider running convert_pascal_berkeley_augmented_mat_annotations_to_png() after\n    extraction.\n\n    The PASCAL VOC dataset can be downloaded from here:\n    http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n    Consider specifying root full names for both of them as arguments for this function\n    after extracting them.\n    The function has three type of train/val splits(credit matconvnet-fcn):\n\n        Let BT, BV, PT, PV, and PX be the Berkeley training and validation\n        sets and PASCAL segmentation challenge training, validation, and\n        test sets. Let T, V, X the final trainig, validation, and test\n        sets.\n\n        Mode 1::\n              V = PV (same validation set as PASCAL)\n\n        Mode 2:: (default))\n              V = PV \\ BT (PASCAL val set that is not a Berkeley training\n              image)\n\n        Mode 3::\n              V = PV \\ (BV + BT)\n\n        In all cases:\n\n              S = PT + PV + BT + BV\n              X = PX  (the test set is uncahgend)\n              T = (S \\ V) \\ X (the rest is training material)\n    Parameters\n    ----------\n    pascal_root : string\n        Path to the PASCAL VOC dataset root that is usually named \'VOC2012\'\n        after being extracted from tar file.\n    pascal_berkeley_root : string\n        Path to the PASCAL Berkeley VOC dataset root that is usually named\n        \'benchmark_RELEASE\' after being extracted from tar file.\n    mode: int\n        The type of train/val data split. Read the function main description for more\n        info.\n    Returns\n    -------\n    image_annotation_pairs : Array with filename pairs with fullnames.\n        [[(str, str), .. , (str, str)][(str, str), .., (str, str)]]\n    """"""  # pylint: disable=E501\n    pascal_txts = get_pascal_segmentation_images_lists_txts(\n        pascal_root=pascal_root)\n    berkeley_txts = get_pascal_berkeley_augmented_segmentation_images_lists_txts(\n        pascal_berkeley_root=pascal_berkeley_root)\n\n    pascal_name_lists = readlines_with_strip_array_version(pascal_txts)\n    berkeley_name_lists = readlines_with_strip_array_version(berkeley_txts)\n\n    pascal_train_name_set, pascal_val_name_set, _ = map(\n        lambda x: set(x), pascal_name_lists)\n    berkeley_train_name_set, berkeley_val_name_set = map(\n        lambda x: set(x), berkeley_name_lists)\n\n    all_berkeley = berkeley_train_name_set | berkeley_val_name_set\n    all_pascal = pascal_train_name_set | pascal_val_name_set\n\n    everything = all_berkeley | all_pascal\n\n    # Extract the validation subset based on selected mode\n    if mode == 1:\n        # 1449 validation images, 10582 training images\n        validation = pascal_val_name_set\n\n    if mode == 2:\n        # 904 validatioin images, 11127 training images\n        validation = pascal_val_name_set - berkeley_train_name_set\n\n    if mode == 3:\n        # 346 validation images, 11685 training images\n        validation = pascal_val_name_set - all_berkeley\n\n    # The rest of the dataset is for training\n    train = everything - validation\n\n    # Get the part that can be extracted from berkeley\n    train_from_berkeley = train & all_berkeley\n\n    # The rest of the data will be loaded from pascal\n    train_from_pascal = train - train_from_berkeley\n\n    train_from_berkeley_image_annotation_pairs = (\n        get_pascal_berkeley_augmented_selected_image_annotation_filenames_pairs(\n            pascal_berkeley_root,\n            list(train_from_berkeley)))\n\n    train_from_pascal_image_annotation_pairs = \\\n        get_pascal_selected_image_annotation_filenames_pairs(pascal_root,\n                                                             list(train_from_pascal))\n\n    overall_train_image_annotation_filename_pairs = \\\n        list(train_from_berkeley_image_annotation_pairs) + \\\n        list(train_from_pascal_image_annotation_pairs)\n\n    overall_val_image_annotation_filename_pairs = \\\n        get_pascal_selected_image_annotation_filenames_pairs(pascal_root,\n                                                             validation)\n\n    return (overall_train_image_annotation_filename_pairs,\n            overall_val_image_annotation_filename_pairs)\n\n\ndef pascal_filename_pairs_to_imageset_txt(voc_imageset_txt_path, filename_pairs,\n                                          image_extension=\'.jpg\'):\n    with open(voc_imageset_txt_path, \'w\') as txtfile:\n        [txtfile.write(os.path.splitext(os.path.basename(file1))[0] + \'\\n\')\n         for file1, file2 in filename_pairs if file1.endswith(image_extension)]\n\n\ndef pascal_combine_annotation_files(filename_pairs, output_annotations_path):\n    mkdir_p(output_annotations_path)\n    for img_path, gt_path in filename_pairs:\n        shutil.copy2(gt_path, output_annotations_path)\n\n\n@data_pascal_voc.config\ndef voc_config():\n    # TODO(ahundt) add md5 sums for each file\n    verbose = True\n    dataset_root = os.path.join(os.path.expanduser(""~""), \'.keras\', \'datasets\')\n    dataset_path = dataset_root + \'/VOC2012\'\n    # sys.path.append(""tf-image-segmentation/"")\n    # os.environ[""CUDA_VISIBLE_DEVICES""] = \'1\'\n    # based on https://github.com/martinkersner/train-DeepLab\n\n    # original PASCAL VOC 2012\n    # wget\n    # http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n    # # 2 GB\n    pascal_root = dataset_path + \'/VOCdevkit/VOC2012\'\n\n    # berkeley augmented Pascal VOC\n    # wget\n    # http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz\n    # # 1.3 GB\n\n    # Pascal Context\n    # http://www.cs.stanford.edu/~roozbeh/pascal-context/\n    # http://www.cs.stanford.edu/~roozbeh/pascal-context/trainval.tar.gz\n    pascal_berkeley_root = dataset_path + \'/benchmark_RELEASE\'\n    urls = [\n        \'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\',\n        \'http://www.eecs.berkeley.edu/Research/Projects/\'\n        \'CS/vision/grouping/semantic_contours/benchmark.tgz\',\n        \'http://www.cs.stanford.edu/~roozbeh/pascal-context/trainval.tar.gz\',\n        \'http://www.cs.stanford.edu/~roozbeh/pascal-context/33_context_labels.tar.gz\',\n        \'http://www.cs.stanford.edu/~roozbeh/pascal-context/59_context_labels.tar.gz\',\n        \'http://www.cs.stanford.edu/~roozbeh/pascal-context/33_labels.txt\',\n        \'http://www.cs.stanford.edu/~roozbeh/pascal-context/59_labels.txt\'\n    ]\n    filenames = [\'VOCtrainval_11-May-2012.tar\',\n                 \'benchmark.tgz\',\n                 \'trainval.tar.gz\',\n                 \'33_context_labels.tar.gz\',\n                 \'59_context_labels.tar.gz\',\n                 \'33_labels.txt\',\n                 \'59_labels.txt\'\n                 ]\n\n    md5s = [\'6cd6e144f989b92b3379bac3b3de84fd\',\n            \'82b4d87ceb2ed10f6038a1cba92111cb\',\n            \'df034edb2c12aa7d33b42b20bb1796e3\',\n            \'180101cfc01c71867b6686207f071eb9\',\n            \'f85d450010762a0e1080304286ce30ed\',\n            \'8840f5439b471aecf991ac6448b826e6\',\n            \'993901f2d930cc038c406845f08fa082\']\n\n    combined_imageset_train_txt = dataset_path + \'/combined_imageset_train.txt\'\n    combined_imageset_val_txt = dataset_path + \'/combined_imageset_val.txt\'\n    combined_annotations_path = dataset_path + \'/combined_annotations\'\n\n    # see get_augmented_pascal_image_annotation_filename_pairs()\n    voc_data_subset_mode = 2\n\n\n@data_pascal_voc.capture\ndef pascal_voc_files(dataset_path, filenames, dataset_root, urls, md5s):\n    print(dataset_path)\n    print(dataset_root)\n    print(urls)\n    print(filenames)\n    print(md5s)\n    return [dataset_path + filename for filename in filenames]\n\n\n@data_pascal_voc.command\ndef pascal_voc_download(dataset_path, filenames, dataset_root, urls, md5s):\n    zip_paths = pascal_voc_files(\n        dataset_path, filenames, dataset_root, urls, md5s)\n    for url, filename, md5 in zip(urls, filenames, md5s):\n        path = get_file(filename, url, md5_hash=md5,\n                        extract=True, cache_subdir=dataset_path)\n\n\n@data_pascal_voc.command\ndef pascal_voc_berkeley_combined(dataset_path,\n                                 pascal_root,\n                                 pascal_berkeley_root,\n                                 voc_data_subset_mode,\n                                 combined_imageset_train_txt,\n                                 combined_imageset_val_txt,\n                                 combined_annotations_path):\n    # Returns a list of (image, annotation)\n    # filename pairs (filename.jpg, filename.png)\n    overall_train_image_annotation_filename_pairs, \\\n        overall_val_image_annotation_filename_pairs = \\\n        get_augmented_pascal_image_annotation_filename_pairs(\n            pascal_root=pascal_root,\n            pascal_berkeley_root=pascal_berkeley_root,\n            mode=voc_data_subset_mode)\n    # combine the annotation files into one folder\n    pascal_combine_annotation_files(\n        list(overall_train_image_annotation_filename_pairs) +\n        list(overall_val_image_annotation_filename_pairs),\n        combined_annotations_path)\n    # generate the train imageset txt\n    pascal_filename_pairs_to_imageset_txt(\n        combined_imageset_train_txt,\n        overall_train_image_annotation_filename_pairs\n    )\n    # generate the val imageset txt\n    pascal_filename_pairs_to_imageset_txt(\n        combined_imageset_val_txt,\n        overall_val_image_annotation_filename_pairs\n    )\n\n\n@data_pascal_voc.command\ndef pascal_voc_setup(filenames, dataset_path, pascal_root,\n                     pascal_berkeley_root, dataset_root,\n                     voc_data_subset_mode,\n                     urls, md5s,\n                     combined_imageset_train_txt,\n                     combined_imageset_val_txt,\n                     combined_annotations_path):\n    # download the dataset\n    pascal_voc_download(dataset_path, filenames,\n                        dataset_root, urls, md5s)\n    # convert the relevant files to a more useful format\n    convert_pascal_berkeley_augmented_mat_annotations_to_png(\n        pascal_berkeley_root)\n    pascal_voc_berkeley_combined(dataset_path,\n                                 pascal_root,\n                                 pascal_berkeley_root,\n                                 voc_data_subset_mode,\n                                 combined_imageset_train_txt,\n                                 combined_imageset_val_txt,\n                                 combined_annotations_path)\n\n\n@data_pascal_voc.automain\ndef main(filenames, dataset_path, pascal_root,\n         pascal_berkeley_root, dataset_root,\n         voc_data_subset_mode,\n         urls, md5s,\n         combined_imageset_train_txt,\n         combined_imageset_val_txt,\n         combined_annotations_path):\n    voc_config()\n    pascal_voc_setup(filenames, dataset_path, pascal_root,\n                     pascal_berkeley_root, dataset_root,\n                     voc_data_subset_mode,\n                     urls, md5s,\n                     combined_imageset_train_txt,\n                     combined_imageset_val_txt,\n                     combined_annotations_path)\n'"
keras_contrib/initializers/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .convaware import ConvolutionAware\n'
keras_contrib/initializers/convaware.py,0,"b'from __future__ import absolute_import\nimport numpy as np\nfrom keras import backend as K\nfrom keras.initializers import Initializer, Orthogonal\n\n\nclass ConvolutionAware(Initializer):\n    """"""\n    Initializer that generates orthogonal convolution filters in the fourier\n    space. If this initializer is passed a shape that is not 3D or 4D,\n    orthogonal initialization will be used.\n    # Arguments\n        eps_std: Standard deviation for the random normal noise used to break\n        symmetry in the inverse fourier transform.\n        seed: A Python integer. Used to seed the random generator.\n    # References\n        Armen Aghajanyan, https://arxiv.org/abs/1702.06295\n    """"""\n\n    def __init__(self, eps_std=0.05, seed=None):\n        self.eps_std = eps_std\n        self.seed = seed\n        self.orthogonal = Orthogonal()\n\n    def __call__(self, shape):\n        rank = len(shape)\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n\n        fan_in, fan_out = _compute_fans(shape, K.image_data_format())\n        variance = 2 / fan_in\n\n        if rank == 3:\n            row, stack_size, filters_size = shape\n\n            transpose_dimensions = (2, 1, 0)\n            kernel_shape = (row,)\n            correct_ifft = lambda shape, s=[None]: np.fft.irfft(shape, s[0])\n            correct_fft = np.fft.rfft\n\n        elif rank == 4:\n            row, column, stack_size, filters_size = shape\n\n            transpose_dimensions = (2, 3, 0, 1)\n            kernel_shape = (row, column)\n            correct_ifft = np.fft.irfft2\n            correct_fft = np.fft.rfft2\n\n        elif rank == 5:\n            x, y, z, stack_size, filters_size = shape\n\n            transpose_dimensions = (3, 4, 0, 1, 2)\n            kernel_shape = (x, y, z)\n            correct_fft = np.fft.rfftn\n            correct_ifft = np.fft.irfftn\n        else:\n            return K.variable(self.orthogonal(shape), dtype=K.floatx())\n\n        kernel_fourier_shape = correct_fft(np.zeros(kernel_shape)).shape\n\n        init = []\n        for i in range(filters_size):\n            basis = self._create_basis(\n                stack_size, np.prod(kernel_fourier_shape))\n            basis = basis.reshape((stack_size,) + kernel_fourier_shape)\n\n            filters = [correct_ifft(x, kernel_shape) +\n                       np.random.normal(0, self.eps_std, kernel_shape) for\n                       x in basis]\n\n            init.append(filters)\n\n        # Format of array is now: filters, stack, row, column\n        init = np.array(init)\n        init = self._scale_filters(init, variance)\n        return init.transpose(transpose_dimensions)\n\n    def _create_basis(self, filters, size):\n        if size == 1:\n            return np.random.normal(0.0, self.eps_std, (filters, size))\n\n        nbb = filters // size + 1\n        li = []\n        for i in range(nbb):\n            a = np.random.normal(0.0, 1.0, (size, size))\n            a = self._symmetrize(a)\n            u, _, v = np.linalg.svd(a)\n            li.extend(u.T.tolist())\n        p = np.array(li[:filters], dtype=K.floatx())\n        return p\n\n    def _symmetrize(self, a):\n        return a + a.T - np.diag(a.diagonal())\n\n    def _scale_filters(self, filters, variance):\n        c_var = np.var(filters)\n        p = np.sqrt(variance / c_var)\n        return filters * p\n\n    def get_config(self):\n        return {\n            \'eps_std\': self.eps_std,\n            \'seed\': self.seed\n        }\n\n\ndef _compute_fans(shape, data_format=\'channels_last\'):\n    """"""Computes the number of input and output units for a weight shape.\n\n    # Arguments\n        shape: Integer shape tuple.\n        data_format: Image data format to use for convolution kernels.\n            Note that all kernels in Keras are standardized on the\n            `channels_last` ordering (even when inputs are set\n            to `channels_first`).\n\n    # Returns\n        A tuple of scalars, `(fan_in, fan_out)`.\n\n    # Raises\n        ValueError: in case of invalid `data_format` argument.\n    """"""\n    if len(shape) == 2:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    elif len(shape) in {3, 4, 5}:\n        # Assuming convolution kernels (1D, 2D or 3D).\n        # TH kernel shape: (depth, input_depth, ...)\n        # TF kernel shape: (..., input_depth, depth)\n        if data_format == \'channels_first\':\n            receptive_field_size = np.prod(shape[2:])\n            fan_in = shape[1] * receptive_field_size\n            fan_out = shape[0] * receptive_field_size\n        elif data_format == \'channels_last\':\n            receptive_field_size = np.prod(shape[:-2])\n            fan_in = shape[-2] * receptive_field_size\n            fan_out = shape[-1] * receptive_field_size\n        else:\n            raise ValueError(\'Invalid data_format: \' + data_format)\n    else:\n        # No specific assumptions.\n        fan_in = np.sqrt(np.prod(shape))\n        fan_out = np.sqrt(np.prod(shape))\n    return fan_in, fan_out\n'"
keras_contrib/layers/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .advanced_activations.pelu import PELU\nfrom .advanced_activations.srelu import SReLU\nfrom .advanced_activations.swish import Swish\nfrom .advanced_activations.sinerelu import SineReLU\n\nfrom .convolutional.cosineconvolution2d import CosineConv2D\nfrom .convolutional.cosineconvolution2d import CosineConvolution2D\nfrom .convolutional.subpixelupscaling import SubPixelUpscaling\n\nfrom .core import CosineDense\n\nfrom .crf import CRF\n\nfrom .capsule import Capsule\n\nfrom .normalization.instancenormalization import InstanceNormalization\nfrom .normalization.groupnormalization import GroupNormalization\n'
keras_contrib/layers/capsule.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\nfrom keras import backend as K\nfrom keras import activations\nfrom keras import regularizers\nfrom keras import initializers\nfrom keras import constraints\nfrom keras.layers import Layer\nfrom keras_contrib.utils.test_utils import to_tuple\n\n\nclass Capsule(Layer):\n    """"""Capsule Layer implementation in Keras\n\n       This implementation is based on Dynamic Routing of Capsules,\n       Geoffrey Hinton et. al.\n\n       The Capsule Layer is a Neural Network Layer which helps\n       modeling relationships in image and sequential data better\n       than just CNNs or RNNs. It achieves this by understanding\n       the spatial relationships between objects (in images)\n       or words (in text) by encoding additional information\n       about the image or text, such as angle of rotation,\n       thickness and brightness, relative proportions etc.\n       This layer can be used instead of pooling layers to\n       lower dimensions and still capture important information\n       about the relationships and structures within the data.\n       A normal pooling layer would lose a lot of\n       this information.\n\n       This layer can be used on the output of any layer\n       which has a 3-D output (including batch_size). For example,\n       in image classification, it can be used on the output of a\n       Conv2D layer for Computer Vision applications. Also,\n       it can be used on the output of a GRU or LSTM Layer\n       (Bidirectional or Unidirectional) for NLP applications.\n\n       The default activation function is \'linear\'. But, this layer\n       is generally used with the \'squash\' activation function\n       (recommended). To use the squash activation function, do :\n\n       from keras_contrib.activations import squash\n\n       capsule = Capsule(num_capsule=10,\n                         dim_capsule=10,\n                         routings=3,\n                         share_weights=True,\n                         activation=squash)\n\n       # Example usage :\n           1). COMPUTER VISION\n\n           input_image = Input(shape=(None, None, 3))\n\n           conv_2d = Conv2D(64,\n                            (3, 3),\n                            activation=\'relu\')(input_image)\n\n           capsule = Capsule(num_capsule=10,\n                             dim_capsule=16,\n                             routings=3,\n                             activation=\'relu\',\n                             share_weights=True)(conv_2d)\n\n           2). NLP\n\n           maxlen = 72\n           max_features = 120000\n           input_text = Input(shape=(maxlen,))\n\n           embedding = Embedding(max_features,\n                                 embed_size,\n                                 weights=[embedding_matrix],\n                                 trainable=False)(input_text)\n\n           bi_gru = Bidirectional(GRU(64,\n                                      return_seqeunces=True))(embedding)\n\n           capsule = Capsule(num_capsule=5,\n                             dim_capsule=5,\n                             routings=4,\n                             activation=\'sigmoid\',\n                             share_weights=True)(bi_gru)\n\n       # Arguments\n           num_capsule : Number of Capsules (int)\n           dim_capsules : Dimensions of the vector output of each Capsule (int)\n           routings : Number of dynamic routings in the Capsule Layer (int)\n           share_weights : Whether to share weights between Capsules or not\n           (boolean)\n           activation : Activation function for the Capsules\n           regularizer : Regularizer for the weights of the Capsules\n           initializer : Initializer for the weights of the Caspules\n           constraint : Constraint for the weights of the Capsules\n\n       # Input shape\n            3D tensor with shape:\n            (batch_size, input_num_capsule, input_dim_capsule)\n            [any 3-D Tensor with the first dimension as batch_size]\n\n       # Output shape\n            3D tensor with shape:\n            (batch_size, num_capsule, dim_capsule)\n\n       # References\n        - [Dynamic-Routing-Between-Capsules]\n          (https://arxiv.org/pdf/1710.09829.pdf)\n        - [Keras-Examples-CIFAR10-CNN-Capsule]""""""\n\n    def __init__(self,\n                 num_capsule,\n                 dim_capsule,\n                 routings=3,\n                 share_weights=True,\n                 initializer=\'glorot_uniform\',\n                 activation=None,\n                 regularizer=None,\n                 constraint=None,\n                 **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.share_weights = share_weights\n\n        self.activation = activations.get(activation)\n        self.regularizer = regularizers.get(regularizer)\n        self.initializer = initializers.get(initializer)\n        self.constraint = constraints.get(constraint)\n\n    def build(self, input_shape):\n        input_shape = to_tuple(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name=\'capsule_kernel\',\n                                     shape=(1,\n                                            input_dim_capsule,\n                                            self.num_capsule *\n                                            self.dim_capsule),\n                                     initializer=self.initializer,\n                                     regularizer=self.regularizer,\n                                     constraint=self.constraint,\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name=\'capsule_kernel\',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule *\n                                            self.dim_capsule),\n                                     initializer=self.initializer,\n                                     regularizer=self.regularizer,\n                                     constraint=self.constraint,\n                                     trainable=True)\n\n        self.build = True\n\n    def call(self, inputs):\n        if self.share_weights:\n            u_hat_vectors = K.conv1d(inputs, self.W)\n        else:\n            u_hat_vectors = K.local_conv1d(inputs, self.W, [1], [1])\n\n        # u_hat_vectors : The spatially transformed input vectors (with local_conv_1d)\n\n        batch_size = K.shape(inputs)[0]\n        input_num_capsule = K.shape(inputs)[1]\n        u_hat_vectors = K.reshape(u_hat_vectors, (batch_size,\n                                                  input_num_capsule,\n                                                  self.num_capsule,\n                                                  self.dim_capsule))\n\n        u_hat_vectors = K.permute_dimensions(u_hat_vectors, (0, 2, 1, 3))\n        routing_weights = K.zeros_like(u_hat_vectors[:, :, :, 0])\n\n        for i in range(self.routings):\n            capsule_weights = K.softmax(routing_weights, 1)\n            outputs = K.batch_dot(capsule_weights, u_hat_vectors, [2, 2])\n            if K.ndim(outputs) == 4:\n                outputs = K.sum(outputs, axis=1)\n            if i < self.routings - 1:\n                outputs = K.l2_normalize(outputs, -1)\n                routing_weights = K.batch_dot(outputs, u_hat_vectors, [2, 3])\n                if K.ndim(routing_weights) == 4:\n                    routing_weights = K.sum(routing_weights, axis=1)\n\n        return self.activation(outputs)\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)\n\n    def get_config(self):\n        config = {\'num_capsule\': self.num_capsule,\n                  \'dim_capsule\': self.dim_capsule,\n                  \'routings\': self.routings,\n                  \'share_weights\': self.share_weights,\n                  \'activation\': activations.serialize(self.activation),\n                  \'regularizer\': regularizers.serialize(self.regularizer),\n                  \'initializer\': initializers.serialize(self.initializer),\n                  \'constraint\': constraints.serialize(self.constraint)}\n\n        base_config = super(Capsule, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
keras_contrib/layers/core.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nfrom keras import backend as K\nfrom keras import activations\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.layers import InputSpec\nfrom keras.layers import Layer\nfrom keras_contrib.utils.test_utils import to_tuple\n\n\nclass CosineDense(Layer):\n    """"""A cosine normalized densely-connected NN layer\n\n    # Example\n\n    ```python\n        # as first layer in a sequential model:\n        model = Sequential()\n        model.add(CosineDense(32, input_dim=16))\n        # now the model will take as input arrays of shape (*, 16)\n        # and output arrays of shape (*, 32)\n\n        # this is equivalent to the above:\n        model = Sequential()\n        model.add(CosineDense(32, input_shape=(16,)))\n\n        # after the first layer, you don\'t need to specify\n        # the size of the input anymore:\n        model.add(CosineDense(32))\n\n        # Note that a regular Dense layer may work better as the final layer\n    ```\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        init: name of initialization function for the weights of the layer\n            (see [initializers](https://keras.io/initializers)),\n            or alternatively, Theano function to use for weights\n            initialization. This parameter is only relevant\n            if you don\'t pass a `weights` argument.\n        activation: name of activation function to use\n            (see [activations](https://keras.io/activations)),\n            or alternatively, elementwise Python function.\n            If you don\'t specify anything, no activation is applied\n            (ie. ""linear"" activation: a(x) = x).\n        weights: list of Numpy arrays to set as initial weights.\n            The list should have 2 elements, of shape `(input_dim, units)`\n            and (units,) for weights and biases respectively.\n        kernel_regularizer: instance of [WeightRegularizer](\n            https://keras.io/regularizers)\n            (eg. L1 or L2 regularization), applied to the main weights matrix.\n        bias_regularizer: instance of [WeightRegularizer](\n            https://keras.io/regularizers), applied to the bias.\n        activity_regularizer: instance of [ActivityRegularizer](\n            https://keras.io/regularizers), applied to the network output.\n        kernel_constraint: instance of the [constraints](\n            https://keras.io/constraints/) module\n            (eg. maxnorm, nonneg), applied to the main weights matrix.\n        bias_constraint: instance of the [constraints](\n            https://keras.io/constraints/) module, applied to the bias.\n        use_bias: whether to include a bias\n            (i.e. make the layer affine rather than linear).\n        input_dim: dimensionality of the input (integer). This argument\n            (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n\n    # Input shape\n        nD tensor with shape: `(nb_samples, ..., input_dim)`.\n        The most common situation would be\n        a 2D input with shape `(nb_samples, input_dim)`.\n\n    # Output shape\n        nD tensor with shape: `(nb_samples, ..., units)`.\n        For instance, for a 2D input with shape `(nb_samples, input_dim)`,\n        the output would have shape `(nb_samples, units)`.\n\n    # References\n        - [Cosine Normalization: Using Cosine Similarity Instead\n           of Dot Product in Neural Networks](https://arxiv.org/pdf/1702.05870.pdf)\n    """"""\n\n    def __init__(self, units, kernel_initializer=\'glorot_uniform\',\n                 activation=None, weights=None,\n                 kernel_regularizer=None, bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None, bias_constraint=None,\n                 use_bias=True, **kwargs):\n        if \'input_shape\' not in kwargs and \'input_dim\' in kwargs:\n            kwargs[\'input_shape\'] = (kwargs.pop(\'input_dim\'),)\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.activation = activations.get(activation)\n        self.units = units\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.use_bias = use_bias\n        self.initial_weights = weights\n        super(CosineDense, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        input_shape = to_tuple(input_shape)\n        ndim = len(input_shape)\n        assert ndim >= 2\n        input_dim = input_shape[-1]\n        self.input_dim = input_dim\n        self.input_spec = [InputSpec(dtype=K.floatx(),\n                                     ndim=ndim)]\n\n        self.kernel = self.add_weight(shape=(input_dim, self.units),\n                                      initializer=self.kernel_initializer,\n                                      name=\'{}_W\'.format(self.name),\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        initializer=\'zero\',\n                                        name=\'{}_b\'.format(self.name),\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n        self.built = True\n\n    def call(self, x, mask=None):\n        if self.use_bias:\n            b, xb = self.bias, 1.\n        else:\n            b, xb = 0., 0.\n\n        xnorm = K.sqrt(K.sum(K.square(x), axis=-1, keepdims=True)\n                       + xb\n                       + K.epsilon())\n        Wnorm = K.sqrt(K.sum(K.square(self.kernel), axis=0)\n                       + K.square(b)\n                       + K.epsilon())\n\n        xWnorm = (xnorm * Wnorm)\n\n        output = K.dot(x, self.kernel) / xWnorm\n        if self.use_bias:\n            output += (self.bias / xWnorm)\n        return self.activation(output)\n\n    def compute_output_shape(self, input_shape):\n        assert input_shape\n        assert len(input_shape) >= 2\n        assert input_shape[-1]\n        output_shape = list(input_shape)\n        output_shape[-1] = self.units\n        return tuple(output_shape)\n\n    def get_config(self):\n        config = {\n            \'units\': self.units,\n            \'kernel_initializer\': initializers.serialize(self.kernel_initializer),\n            \'activation\': activations.serialize(self.activation),\n            \'kernel_regularizer\': regularizers.serialize(self.kernel_regularizer),\n            \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n            \'activity_regularizer\':\n                regularizers.serialize(self.activity_regularizer),\n            \'kernel_constraint\': constraints.serialize(self.kernel_constraint),\n            \'bias_constraint\': constraints.serialize(self.bias_constraint),\n            \'use_bias\': self.use_bias\n        }\n        base_config = super(CosineDense, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
keras_contrib/layers/crf.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\n\nimport warnings\n\nfrom keras import backend as K\nfrom keras import activations\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.layers import Layer\nfrom keras.layers import InputSpec\n\nfrom keras_contrib.losses import crf_loss\nfrom keras_contrib.metrics import crf_marginal_accuracy\nfrom keras_contrib.metrics import crf_viterbi_accuracy\nfrom keras_contrib.utils.test_utils import to_tuple\n\n\nclass CRF(Layer):\n    """"""An implementation of linear chain conditional random field (CRF).\n\n    An linear chain CRF is defined to maximize the following likelihood function:\n\n    $$ L(W, U, b; y_1, ..., y_n) := \\frac{1}{Z}\n    \\sum_{y_1, ..., y_n} \\exp(-a_1\' y_1 - a_n\' y_n\n        - \\sum_{k=1^n}((f(x_k\' W + b) y_k) + y_1\' U y_2)), $$\n\n    where:\n        $Z$: normalization constant\n        $x_k, y_k$:  inputs and outputs\n\n    This implementation has two modes for optimization:\n    1. (`join mode`) optimized by maximizing join likelihood,\n    which is optimal in theory of statistics.\n       Note that in this case, CRF must be the output/last layer.\n    2. (`marginal mode`) return marginal probabilities on each time\n    step and optimized via composition\n       likelihood (product of marginal likelihood), i.e.,\n       using `categorical_crossentropy` loss.\n       Note that in this case, CRF can be either the last layer or an\n       intermediate layer (though not explored).\n\n    For prediction (test phrase), one can choose either Viterbi\n    best path (class indices) or marginal\n    probabilities if probabilities are needed.\n    However, if one chooses *join mode* for training,\n    Viterbi output is typically better than marginal output,\n    but the marginal output will still perform\n    reasonably close, while if *marginal mode* is used for training,\n    marginal output usually performs\n    much better. The default behavior and `metrics.crf_accuracy`\n    is set according to this observation.\n\n    In addition, this implementation supports masking and accepts either\n    onehot or sparse target.\n\n    If you open a issue or a pull request about CRF, please\n    add \'cc @lzfelix\' to notify Luiz Felix.\n\n\n    # Examples\n\n    ```python\n        from keras_contrib.layers import CRF\n        from keras_contrib.losses import crf_loss\n        from keras_contrib.metrics import crf_viterbi_accuracy\n\n        model = Sequential()\n        model.add(Embedding(3001, 300, mask_zero=True)(X)\n\n        # use learn_mode = \'join\', test_mode = \'viterbi\',\n        # sparse_target = True (label indice output)\n        crf = CRF(10, sparse_target=True)\n        model.add(crf)\n\n        # crf_accuracy is default to Viterbi acc if using join-mode (default).\n        # One can add crf.marginal_acc if interested, but may slow down learning\n        model.compile(\'adam\', loss=crf_loss, metrics=[crf_viterbi_accuracy])\n\n        # y must be label indices (with shape 1 at dim 3) here,\n        # since `sparse_target=True`\n        model.fit(x, y)\n\n        # prediction give onehot representation of Viterbi best path\n        y_hat = model.predict(x_test)\n    ```\n\n    The following snippet shows how to load a persisted\n    model that uses the CRF layer:\n\n    ```python\n        from keras.models import load_model\n        from keras_contrib.losses import import crf_loss\n        from keras_contrib.metrics import crf_viterbi_accuracy\n\n        custom_objects={\'CRF\': CRF,\n                        \'crf_loss\': crf_loss,\n                        \'crf_viterbi_accuracy\': crf_viterbi_accuracy}\n\n        loaded_model = load_model(\'<path_to_model>\',\n                                  custom_objects=custom_objects)\n    ```\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        learn_mode: Either \'join\' or \'marginal\'.\n            The former train the model by maximizing join likelihood while the latter\n            maximize the product of marginal likelihood over all time steps.\n            One should use `losses.crf_nll` for \'join\' mode\n            and `losses.categorical_crossentropy` or\n            `losses.sparse_categorical_crossentropy` for\n            `marginal` mode.  For convenience, simply\n            use `losses.crf_loss`, which will decide the proper loss as described.\n        test_mode: Either \'viterbi\' or \'marginal\'.\n            The former is recommended and as default when `learn_mode = \'join\'` and\n            gives one-hot representation of the best path at test (prediction) time,\n            while the latter is recommended and chosen as default\n            when `learn_mode = \'marginal\'`,\n            which produces marginal probabilities for each time step.\n            For evaluating metrics, one should\n            use `metrics.crf_viterbi_accuracy` for \'viterbi\' mode and\n            \'metrics.crf_marginal_accuracy\' for \'marginal\' mode, or\n            simply use `metrics.crf_accuracy` for\n            both which automatically decides it as described.\n            One can also use both for evaluation at training.\n        sparse_target: Boolean (default False) indicating\n            if provided labels are one-hot or\n            indices (with shape 1 at dim 3).\n        use_boundary: Boolean (default True) indicating if trainable\n            start-end chain energies\n            should be added to model.\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        chain_initializer: Initializer for the `chain_kernel` weights matrix,\n            used for the CRF chain energy.\n            (see [initializers](../initializers.md)).\n        boundary_initializer: Initializer for the `left_boundary`,\n            \'right_boundary\' weights vectors,\n            used for the start/left and end/right boundary energy.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. ""linear"" activation: `a(x) = x`).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        chain_regularizer: Regularizer function applied to\n            the `chain_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        boundary_regularizer: Regularizer function applied to\n            the \'left_boundary\', \'right_boundary\' weight vectors\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        chain_constraint: Constraint function applied to\n            the `chain_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        boundary_constraint: Constraint function applied to\n            the `left_boundary`, `right_boundary` weights vectors\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        unroll: Boolean (default False). If True, the network will be\n            unrolled, else a symbolic loop will be used.\n            Unrolling can speed-up a RNN, although it tends\n            to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n\n    # Input shape\n        3D tensor with shape `(nb_samples, timesteps, input_dim)`.\n\n    # Output shape\n        3D tensor with shape `(nb_samples, timesteps, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    """"""\n\n    def __init__(self, units,\n                 learn_mode=\'join\',\n                 test_mode=None,\n                 sparse_target=False,\n                 use_boundary=True,\n                 use_bias=True,\n                 activation=\'linear\',\n                 kernel_initializer=\'glorot_uniform\',\n                 chain_initializer=\'orthogonal\',\n                 bias_initializer=\'zeros\',\n                 boundary_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 chain_regularizer=None,\n                 boundary_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 chain_constraint=None,\n                 boundary_constraint=None,\n                 bias_constraint=None,\n                 input_dim=None,\n                 unroll=False,\n                 **kwargs):\n        super(CRF, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.units = units\n        self.learn_mode = learn_mode\n        assert self.learn_mode in [\'join\', \'marginal\']\n        self.test_mode = test_mode\n        if self.test_mode is None:\n            self.test_mode = \'viterbi\' if self.learn_mode == \'join\' else \'marginal\'\n        else:\n            assert self.test_mode in [\'viterbi\', \'marginal\']\n        self.sparse_target = sparse_target\n        self.use_boundary = use_boundary\n        self.use_bias = use_bias\n\n        self.activation = activations.get(activation)\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.chain_initializer = initializers.get(chain_initializer)\n        self.boundary_initializer = initializers.get(boundary_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.chain_regularizer = regularizers.get(chain_regularizer)\n        self.boundary_regularizer = regularizers.get(boundary_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.chain_constraint = constraints.get(chain_constraint)\n        self.boundary_constraint = constraints.get(boundary_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.unroll = unroll\n\n    def build(self, input_shape):\n        input_shape = to_tuple(input_shape)\n        self.input_spec = [InputSpec(shape=input_shape)]\n        self.input_dim = input_shape[-1]\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units),\n                                      name=\'kernel\',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.chain_kernel = self.add_weight(shape=(self.units, self.units),\n                                            name=\'chain_kernel\',\n                                            initializer=self.chain_initializer,\n                                            regularizer=self.chain_regularizer,\n                                            constraint=self.chain_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        name=\'bias\',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = 0\n\n        if self.use_boundary:\n            self.left_boundary = self.add_weight(shape=(self.units,),\n                                                 name=\'left_boundary\',\n                                                 initializer=self.boundary_initializer,\n                                                 regularizer=self.boundary_regularizer,\n                                                 constraint=self.boundary_constraint)\n            self.right_boundary = self.add_weight(shape=(self.units,),\n                                                  name=\'right_boundary\',\n                                                  initializer=self.boundary_initializer,\n                                                  regularizer=self.boundary_regularizer,\n                                                  constraint=self.boundary_constraint)\n        self.built = True\n\n    def call(self, X, mask=None):\n        if mask is not None:\n            assert K.ndim(mask) == 2, \'Input mask to CRF must have dim 2 if not None\'\n\n        if self.test_mode == \'viterbi\':\n            test_output = self.viterbi_decoding(X, mask)\n        else:\n            test_output = self.get_marginal_prob(X, mask)\n\n        self.uses_learning_phase = True\n        if self.learn_mode == \'join\':\n            train_output = K.zeros_like(K.dot(X, self.kernel))\n            out = K.in_train_phase(train_output, test_output)\n        else:\n            if self.test_mode == \'viterbi\':\n                train_output = self.get_marginal_prob(X, mask)\n                out = K.in_train_phase(train_output, test_output)\n            else:\n                out = test_output\n        return out\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[:2] + (self.units,)\n\n    def compute_mask(self, input, mask=None):\n        if mask is not None and self.learn_mode == \'join\':\n            return K.any(mask, axis=1)\n        return mask\n\n    def get_config(self):\n        config = {\n            \'units\': self.units,\n            \'learn_mode\': self.learn_mode,\n            \'test_mode\': self.test_mode,\n            \'use_boundary\': self.use_boundary,\n            \'use_bias\': self.use_bias,\n            \'sparse_target\': self.sparse_target,\n            \'kernel_initializer\': initializers.serialize(self.kernel_initializer),\n            \'chain_initializer\': initializers.serialize(self.chain_initializer),\n            \'boundary_initializer\': initializers.serialize(\n                self.boundary_initializer),\n            \'bias_initializer\': initializers.serialize(self.bias_initializer),\n            \'activation\': activations.serialize(self.activation),\n            \'kernel_regularizer\': regularizers.serialize(self.kernel_regularizer),\n            \'chain_regularizer\': regularizers.serialize(self.chain_regularizer),\n            \'boundary_regularizer\': regularizers.serialize(\n                self.boundary_regularizer),\n            \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n            \'kernel_constraint\': constraints.serialize(self.kernel_constraint),\n            \'chain_constraint\': constraints.serialize(self.chain_constraint),\n            \'boundary_constraint\': constraints.serialize(self.boundary_constraint),\n            \'bias_constraint\': constraints.serialize(self.bias_constraint),\n            \'input_dim\': self.input_dim,\n            \'unroll\': self.unroll}\n        base_config = super(CRF, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @property\n    def loss_function(self):\n        warnings.warn(\'CRF.loss_function is deprecated \'\n                      \'and it might be removed in the future. Please \'\n                      \'use losses.crf_loss instead.\')\n        return crf_loss\n\n    @property\n    def accuracy(self):\n        warnings.warn(\'CRF.accuracy is deprecated and it \'\n                      \'might be removed in the future. Please \'\n                      \'use metrics.crf_accuracy\')\n        if self.test_mode == \'viterbi\':\n            return crf_viterbi_accuracy\n        else:\n            return crf_marginal_accuracy\n\n    @property\n    def viterbi_acc(self):\n        warnings.warn(\'CRF.viterbi_acc is deprecated and it might \'\n                      \'be removed in the future. Please \'\n                      \'use metrics.viterbi_acc instead.\')\n        return crf_viterbi_accuracy\n\n    @property\n    def marginal_acc(self):\n        warnings.warn(\'CRF.moarginal_acc is deprecated and it \'\n                      \'might be removed in the future. Please \'\n                      \'use metrics.marginal_acc instead.\')\n        return crf_marginal_accuracy\n\n    @staticmethod\n    def softmaxNd(x, axis=-1):\n        m = K.max(x, axis=axis, keepdims=True)\n        exp_x = K.exp(x - m)\n        prob_x = exp_x / K.sum(exp_x, axis=axis, keepdims=True)\n        return prob_x\n\n    @staticmethod\n    def shift_left(x, offset=1):\n        assert offset > 0\n        return K.concatenate([x[:, offset:], K.zeros_like(x[:, :offset])], axis=1)\n\n    @staticmethod\n    def shift_right(x, offset=1):\n        assert offset > 0\n        return K.concatenate([K.zeros_like(x[:, :offset]), x[:, :-offset]], axis=1)\n\n    def add_boundary_energy(self, energy, mask, start, end):\n        start = K.expand_dims(K.expand_dims(start, 0), 0)\n        end = K.expand_dims(K.expand_dims(end, 0), 0)\n        if mask is None:\n            energy = K.concatenate([energy[:, :1, :] + start, energy[:, 1:, :]],\n                                   axis=1)\n            energy = K.concatenate([energy[:, :-1, :], energy[:, -1:, :] + end],\n                                   axis=1)\n        else:\n            mask = K.expand_dims(K.cast(mask, K.floatx()))\n            start_mask = K.cast(K.greater(mask, self.shift_right(mask)), K.floatx())\n            end_mask = K.cast(K.greater(self.shift_left(mask), mask), K.floatx())\n            energy = energy + start_mask * start\n            energy = energy + end_mask * end\n        return energy\n\n    def get_log_normalization_constant(self, input_energy, mask, **kwargs):\n        """"""Compute logarithm of the normalization constant Z, where\n        Z = sum exp(-E) -> logZ = log sum exp(-E) =: -nlogZ\n        """"""\n        # should have logZ[:, i] == logZ[:, j] for any i, j\n        logZ = self.recursion(input_energy, mask, return_sequences=False, **kwargs)\n        return logZ[:, 0]\n\n    def get_energy(self, y_true, input_energy, mask):\n        """"""Energy = a1\' y1 + u1\' y1 + y1\' U y2 + u2\' y2 + y2\' U y3 + u3\' y3 + an\' y3\n        """"""\n        input_energy = K.sum(input_energy * y_true, 2)  # (B, T)\n        # (B, T-1)\n        chain_energy = K.sum(K.dot(y_true[:, :-1, :],\n                                   self.chain_kernel) * y_true[:, 1:, :], 2)\n\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            # (B, T-1), mask[:,:-1]*mask[:,1:] makes it work with any padding\n            chain_mask = mask[:, :-1] * mask[:, 1:]\n            input_energy = input_energy * mask\n            chain_energy = chain_energy * chain_mask\n        total_energy = K.sum(input_energy, -1) + K.sum(chain_energy, -1)  # (B, )\n\n        return total_energy\n\n    def get_negative_log_likelihood(self, y_true, X, mask):\n        """"""Compute the loss, i.e., negative log likelihood (normalize by number of time steps)\n           likelihood = 1/Z * exp(-E) ->  neg_log_like = - log(1/Z * exp(-E)) = logZ + E\n        """"""\n        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n        if self.use_boundary:\n            input_energy = self.add_boundary_energy(input_energy, mask,\n                                                    self.left_boundary,\n                                                    self.right_boundary)\n        energy = self.get_energy(y_true, input_energy, mask)\n        logZ = self.get_log_normalization_constant(input_energy, mask,\n                                                   input_length=K.int_shape(X)[1])\n        nloglik = logZ + energy\n        if mask is not None:\n            nloglik = nloglik / K.sum(K.cast(mask, K.floatx()), 1)\n        else:\n            nloglik = nloglik / K.cast(K.shape(X)[1], K.floatx())\n        return nloglik\n\n    def step(self, input_energy_t, states, return_logZ=True):\n        # not in the following  `prev_target_val` has shape = (B, F)\n        # where B = batch_size, F = output feature dim\n        # Note: `i` is of float32, due to the behavior of `K.rnn`\n        prev_target_val, i, chain_energy = states[:3]\n        t = K.cast(i[0, 0], dtype=\'int32\')\n        if len(states) > 3:\n            if K.backend() == \'theano\':\n                m = states[3][:, t:(t + 2)]\n            else:\n                m = K.slice(states[3], [0, t], [-1, 2])\n            input_energy_t = input_energy_t * K.expand_dims(m[:, 0])\n            # (1, F, F)*(B, 1, 1) -> (B, F, F)\n            chain_energy = chain_energy * K.expand_dims(\n                K.expand_dims(m[:, 0] * m[:, 1]))\n        if return_logZ:\n            # shapes: (1, B, F) + (B, F, 1) -> (B, F, F)\n            energy = chain_energy + K.expand_dims(input_energy_t - prev_target_val, 2)\n            new_target_val = K.logsumexp(-energy, 1)  # shapes: (B, F)\n            return new_target_val, [new_target_val, i + 1]\n        else:\n            energy = chain_energy + K.expand_dims(input_energy_t + prev_target_val, 2)\n            min_energy = K.min(energy, 1)\n            # cast for tf-version `K.rnn\n            argmin_table = K.cast(K.argmin(energy, 1), K.floatx())\n            return argmin_table, [min_energy, i + 1]\n\n    def recursion(self, input_energy, mask=None, go_backwards=False,\n                  return_sequences=True, return_logZ=True, input_length=None):\n        """"""Forward (alpha) or backward (beta) recursion\n\n        If `return_logZ = True`, compute the logZ, the normalization constant:\n\n        \\[ Z = \\sum_{y1, y2, y3} exp(-E) # energy\n          = \\sum_{y1, y2, y3} exp(-(u1\' y1 + y1\' W y2 + u2\' y2 + y2\' W y3 + u3\' y3))\n          = sum_{y2, y3} (exp(-(u2\' y2 + y2\' W y3 + u3\' y3))\n          sum_{y1} exp(-(u1\' y1\' + y1\' W y2))) \\]\n\n        Denote:\n            \\[ S(y2) := sum_{y1} exp(-(u1\' y1 + y1\' W y2)), \\]\n            \\[ Z = sum_{y2, y3} exp(log S(y2) - (u2\' y2 + y2\' W y3 + u3\' y3)) \\]\n            \\[ logS(y2) = log S(y2) = log_sum_exp(-(u1\' y1\' + y1\' W y2)) \\]\n        Note that:\n              yi\'s are one-hot vectors\n              u1, u3: boundary energies have been merged\n\n        If `return_logZ = False`, compute the Viterbi\'s best path lookup table.\n        """"""\n        chain_energy = self.chain_kernel\n        # shape=(1, F, F): F=num of output features. 1st F is for t-1, 2nd F for t\n        chain_energy = K.expand_dims(chain_energy, 0)\n        # shape=(B, F), dtype=float32\n        prev_target_val = K.zeros_like(input_energy[:, 0, :])\n\n        if go_backwards:\n            input_energy = K.reverse(input_energy, 1)\n            if mask is not None:\n                mask = K.reverse(mask, 1)\n\n        initial_states = [prev_target_val, K.zeros_like(prev_target_val[:, :1])]\n        constants = [chain_energy]\n\n        if mask is not None:\n            mask2 = K.cast(K.concatenate([mask, K.zeros_like(mask[:, :1])], axis=1),\n                           K.floatx())\n            constants.append(mask2)\n\n        def _step(input_energy_i, states):\n            return self.step(input_energy_i, states, return_logZ)\n\n        target_val_last, target_val_seq, _ = K.rnn(_step, input_energy,\n                                                   initial_states,\n                                                   constants=constants,\n                                                   input_length=input_length,\n                                                   unroll=self.unroll)\n\n        if return_sequences:\n            if go_backwards:\n                target_val_seq = K.reverse(target_val_seq, 1)\n            return target_val_seq\n        else:\n            return target_val_last\n\n    def forward_recursion(self, input_energy, **kwargs):\n        return self.recursion(input_energy, **kwargs)\n\n    def backward_recursion(self, input_energy, **kwargs):\n        return self.recursion(input_energy, go_backwards=True, **kwargs)\n\n    def get_marginal_prob(self, X, mask=None):\n        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n        if self.use_boundary:\n            input_energy = self.add_boundary_energy(input_energy, mask,\n                                                    self.left_boundary,\n                                                    self.right_boundary)\n        input_length = K.int_shape(X)[1]\n        alpha = self.forward_recursion(input_energy, mask=mask,\n                                       input_length=input_length)\n        beta = self.backward_recursion(input_energy, mask=mask,\n                                       input_length=input_length)\n        if mask is not None:\n            input_energy = input_energy * K.expand_dims(K.cast(mask, K.floatx()))\n        margin = -(self.shift_right(alpha) + input_energy + self.shift_left(beta))\n        return self.softmaxNd(margin)\n\n    def viterbi_decoding(self, X, mask=None):\n        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n        if self.use_boundary:\n            input_energy = self.add_boundary_energy(\n                input_energy, mask, self.left_boundary, self.right_boundary)\n\n        argmin_tables = self.recursion(input_energy, mask, return_logZ=False)\n        argmin_tables = K.cast(argmin_tables, \'int32\')\n\n        # backward to find best path, `initial_best_idx` can be any,\n        # as all elements in the last argmin_table are the same\n        argmin_tables = K.reverse(argmin_tables, 1)\n        # matrix instead of vector is required by tf `K.rnn`\n        initial_best_idx = [K.expand_dims(argmin_tables[:, 0, 0])]\n        if K.backend() == \'theano\':\n            from theano import tensor as T\n            initial_best_idx = [T.unbroadcast(initial_best_idx[0], 1)]\n\n        def gather_each_row(params, indices):\n            n = K.shape(indices)[0]\n            if K.backend() == \'theano\':\n                from theano import tensor as T\n                return params[T.arange(n), indices]\n            elif K.backend() == \'tensorflow\':\n                import tensorflow as tf\n                indices = K.transpose(K.stack([tf.range(n), indices]))\n                return tf.gather_nd(params, indices)\n            else:\n                raise NotImplementedError\n\n        def find_path(argmin_table, best_idx):\n            next_best_idx = gather_each_row(argmin_table, best_idx[0][:, 0])\n            next_best_idx = K.expand_dims(next_best_idx)\n            if K.backend() == \'theano\':\n                from theano import tensor as T\n                next_best_idx = T.unbroadcast(next_best_idx, 1)\n            return next_best_idx, [next_best_idx]\n\n        _, best_paths, _ = K.rnn(find_path, argmin_tables, initial_best_idx,\n                                 input_length=K.int_shape(X)[1], unroll=self.unroll)\n        best_paths = K.reverse(best_paths, 1)\n        best_paths = K.squeeze(best_paths, 2)\n\n        return K.one_hot(best_paths, self.units)\n'"
keras_contrib/losses/__init__.py,0,"b'from .dssim import DSSIMObjective\nfrom .jaccard import jaccard_distance\nfrom .crf_losses import crf_loss, crf_nll\n'"
keras_contrib/losses/crf_losses.py,0,"b'from keras import backend as K\nfrom keras.losses import categorical_crossentropy\nfrom keras.losses import sparse_categorical_crossentropy\n\n\ndef crf_nll(y_true, y_pred):\n    """"""The negative log-likelihood for linear chain Conditional Random Field (CRF).\n\n    This loss function is only used when the `layers.CRF` layer\n    is trained in the ""join"" mode.\n\n    # Arguments\n        y_true: tensor with true targets.\n        y_pred: tensor with predicted targets.\n\n    # Returns\n        A scalar representing corresponding to the negative log-likelihood.\n\n    # Raises\n        TypeError: If CRF is not the last layer.\n\n    # About GitHub\n        If you open an issue or a pull request about CRF, please\n        add `cc @lzfelix` to notify Luiz Felix.\n    """"""\n\n    crf, idx = y_pred._keras_history[:2]\n    if crf._outbound_nodes:\n        raise TypeError(\'When learn_model=""join"", CRF must be the last layer.\')\n    if crf.sparse_target:\n        y_true = K.one_hot(K.cast(y_true[:, :, 0], \'int32\'), crf.units)\n    X = crf._inbound_nodes[idx].input_tensors[0]\n    mask = crf._inbound_nodes[idx].input_masks[0]\n    nloglik = crf.get_negative_log_likelihood(y_true, X, mask)\n    return nloglik\n\n\ndef crf_loss(y_true, y_pred):\n    """"""General CRF loss function depending on the learning mode.\n\n    # Arguments\n        y_true: tensor with true targets.\n        y_pred: tensor with predicted targets.\n\n    # Returns\n        If the CRF layer is being trained in the join mode, returns the negative\n        log-likelihood. Otherwise returns the categorical crossentropy implemented\n        by the underlying Keras backend.\n\n    # About GitHub\n        If you open an issue or a pull request about CRF, please\n        add `cc @lzfelix` to notify Luiz Felix.\n    """"""\n    crf, idx = y_pred._keras_history[:2]\n    if crf.learn_mode == \'join\':\n        return crf_nll(y_true, y_pred)\n    else:\n        if crf.sparse_target:\n            return sparse_categorical_crossentropy(y_true, y_pred)\n        else:\n            return categorical_crossentropy(y_true, y_pred)\n'"
keras_contrib/losses/dssim.py,0,"b'from __future__ import absolute_import\nimport keras_contrib.backend as KC\nfrom keras import backend as K\n\n\nclass DSSIMObjective:\n    """"""Difference of Structural Similarity (DSSIM loss function).\n    Clipped between 0 and 0.5\n\n    Note : You should add a regularization term like a l2 loss in addition to this one.\n    Note : In theano, the `kernel_size` must be a factor of the output size. So 3 could\n           not be the `kernel_size` for an output of 32.\n\n    # Arguments\n        k1: Parameter of the SSIM (default 0.01)\n        k2: Parameter of the SSIM (default 0.03)\n        kernel_size: Size of the sliding window (default 3)\n        max_value: Max value of the output (default 1.0)\n    """"""\n\n    def __init__(self, k1=0.01, k2=0.03, kernel_size=3, max_value=1.0):\n        self.__name__ = \'DSSIMObjective\'\n        self.kernel_size = kernel_size\n        self.k1 = k1\n        self.k2 = k2\n        self.max_value = max_value\n        self.c1 = (self.k1 * self.max_value) ** 2\n        self.c2 = (self.k2 * self.max_value) ** 2\n        self.dim_ordering = K.image_data_format()\n        self.backend = K.backend()\n\n    def __int_shape(self, x):\n        return K.int_shape(x) if self.backend == \'tensorflow\' else K.shape(x)\n\n    def __call__(self, y_true, y_pred):\n        # There are additional parameters for this function\n        # Note: some of the \'modes\' for edge behavior do not yet have a\n        # gradient definition in the Theano tree\n        #   and cannot be used for learning\n\n        kernel = [self.kernel_size, self.kernel_size]\n        y_true = K.reshape(y_true, [-1] + list(self.__int_shape(y_pred)[1:]))\n        y_pred = K.reshape(y_pred, [-1] + list(self.__int_shape(y_pred)[1:]))\n\n        patches_pred = KC.extract_image_patches(y_pred, kernel, kernel, \'valid\',\n                                                self.dim_ordering)\n        patches_true = KC.extract_image_patches(y_true, kernel, kernel, \'valid\',\n                                                self.dim_ordering)\n\n        # Reshape to get the var in the cells\n        bs, w, h, c1, c2, c3 = self.__int_shape(patches_pred)\n        patches_pred = K.reshape(patches_pred, [-1, w, h, c1 * c2 * c3])\n        patches_true = K.reshape(patches_true, [-1, w, h, c1 * c2 * c3])\n        # Get mean\n        u_true = K.mean(patches_true, axis=-1)\n        u_pred = K.mean(patches_pred, axis=-1)\n        # Get variance\n        var_true = K.var(patches_true, axis=-1)\n        var_pred = K.var(patches_pred, axis=-1)\n        # Get std dev\n        covar_true_pred = K.mean(patches_true * patches_pred, axis=-1) - u_true * u_pred\n\n        ssim = (2 * u_true * u_pred + self.c1) * (2 * covar_true_pred + self.c2)\n        denom = ((K.square(u_true)\n                  + K.square(u_pred)\n                  + self.c1) * (var_pred + var_true + self.c2))\n        ssim /= denom  # no need for clipping, c1 and c2 make the denom non-zero\n        return K.mean((1.0 - ssim) / 2.0)\n'"
keras_contrib/losses/jaccard.py,0,"b'from keras import backend as K\n\n\ndef jaccard_distance(y_true, y_pred, smooth=100):\n    """"""Jaccard distance for semantic segmentation.\n\n    Also known as the intersection-over-union loss.\n\n    This loss is useful when you have unbalanced numbers of pixels within an image\n    because it gives all classes equal weight. However, it is not the defacto\n    standard for image segmentation.\n\n    For example, assume you are trying to predict if\n    each pixel is cat, dog, or background.\n    You have 80% background pixels, 10% dog, and 10% cat.\n    If the model predicts 100% background\n    should it be be 80% right (as with categorical cross entropy)\n    or 30% (with this loss)?\n\n    The loss has been modified to have a smooth gradient as it converges on zero.\n    This has been shifted so it converges on 0 and is smoothed to avoid exploding\n    or disappearing gradient.\n\n    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n\n    # Arguments\n        y_true: The ground truth tensor.\n        y_pred: The predicted tensor\n        smooth: Smoothing factor. Default is 100.\n\n    # Returns\n        The Jaccard distance between the two tensors.\n\n    # References\n        - [What is a good evaluation measure for semantic segmentation?](\n           http://www.bmva.org/bmvc/2013/Papers/paper0032/paper0032.pdf)\n\n    """"""\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n    return (1 - jac) * smooth\n'"
keras_contrib/metrics/__init__.py,0,"b'from .crf_accuracies import crf_accuracy, crf_marginal_accuracy\nfrom .crf_accuracies import crf_viterbi_accuracy\n'"
keras_contrib/metrics/crf_accuracies.py,0,"b'from keras import backend as K\n\n\ndef _get_accuracy(y_true, y_pred, mask, sparse_target=False):\n    y_pred = K.argmax(y_pred, -1)\n    if sparse_target:\n        y_true = K.cast(y_true[:, :, 0], K.dtype(y_pred))\n    else:\n        y_true = K.argmax(y_true, -1)\n    judge = K.cast(K.equal(y_pred, y_true), K.floatx())\n    if mask is None:\n        return K.mean(judge)\n    else:\n        mask = K.cast(mask, K.floatx())\n        return K.sum(judge * mask) / K.sum(mask)\n\n\ndef crf_viterbi_accuracy(y_true, y_pred):\n    \'\'\'Use Viterbi algorithm to get best path, and compute its accuracy.\n    `y_pred` must be an output from CRF.\'\'\'\n    crf, idx = y_pred._keras_history[:2]\n    X = crf._inbound_nodes[idx].input_tensors[0]\n    mask = crf._inbound_nodes[idx].input_masks[0]\n    y_pred = crf.viterbi_decoding(X, mask)\n    return _get_accuracy(y_true, y_pred, mask, crf.sparse_target)\n\n\ndef crf_marginal_accuracy(y_true, y_pred):\n    \'\'\'Use time-wise marginal argmax as prediction.\n    `y_pred` must be an output from CRF with `learn_mode=""marginal""`.\'\'\'\n    crf, idx = y_pred._keras_history[:2]\n    X = crf._inbound_nodes[idx].input_tensors[0]\n    mask = crf._inbound_nodes[idx].input_masks[0]\n    y_pred = crf.get_marginal_prob(X, mask)\n    return _get_accuracy(y_true, y_pred, mask, crf.sparse_target)\n\n\ndef crf_accuracy(y_true, y_pred):\n    \'\'\'Ge default accuracy based on CRF `test_mode`.\'\'\'\n    crf, idx = y_pred._keras_history[:2]\n    if crf.test_mode == \'viterbi\':\n        return crf_viterbi_accuracy(y_true, y_pred)\n    else:\n        return crf_marginal_accuracy(y_true, y_pred)\n'"
keras_contrib/optimizers/__init__.py,0,b'from .ftml import FTML\nfrom .padam import Padam\nfrom .yogi import Yogi\nfrom .lars import LARS\n\n# aliases\nftml = FTML\nlars = LARS\n'
keras_contrib/optimizers/ftml.py,0,"b'from __future__ import absolute_import\nfrom keras.optimizers import Optimizer\nfrom keras import backend as K\n\n\nclass FTML(Optimizer):\n    """"""FTML optimizer.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 0.5.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor.\n        decay: float >= 0. Learning rate decay over each update.\n\n    # References\n        - [FTML - Follow the Moving Leader in Deep Learning](\n        http://www.cse.ust.hk/~szhengac/papers/icml17.pdf)\n    """"""\n\n    def __init__(self, lr=0.0025, beta_1=0.6, beta_2=0.999,\n                 epsilon=1e-8, decay=0., **kwargs):\n        super(FTML, self).__init__(**kwargs)\n        self.__dict__.update(locals())\n        self.iterations = K.variable(0)\n        self.lr = K.variable(lr)\n        self.beta_1 = K.variable(beta_1)\n        self.beta_2 = K.variable(beta_2)\n        self.decay = K.variable(decay)\n        self.epsilon = epsilon\n        self.inital_decay = decay\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.inital_decay > 0:\n            lr *= (1. / (1. + self.decay * self.iterations))\n\n        t = self.iterations + 1\n\n        lr_t = lr / (1. - K.pow(self.beta_1, t))\n\n        shapes = [K.int_shape(p) for p in params]\n        zs = [K.zeros(shape) for shape in shapes]\n        vs = [K.zeros(shape) for shape in shapes]\n        ds = [K.zeros(shape) for shape in shapes]\n        self.weights = [self.iterations] + zs + vs + ds\n\n        for p, g, z, v, d in zip(params, grads, zs, vs, ds):\n            v_t = self.beta_2 * v + (1. - self.beta_2) * K.square(g)\n            d_t = (K.sqrt(v_t / (1. - K.pow(self.beta_2, t)))\n                   + self.epsilon) / lr_t\n            sigma_t = d_t - self.beta_1 * d\n            z_t = self.beta_1 * z + (1. - self.beta_1) * g - sigma_t * p\n\n            p_t = - z_t / d_t\n\n            self.updates.append(K.update(z, z_t))\n            self.updates.append(K.update(v, v_t))\n            self.updates.append(K.update(d, d_t))\n\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, \'constraint\', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {\'lr\': float(K.get_value(self.lr)),\n                  \'beta_1\': float(K.get_value(self.beta_1)),\n                  \'beta_2\': float(K.get_value(self.beta_2)),\n                  \'decay\': float(K.get_value(self.decay)),\n                  \'epsilon\': self.epsilon}\n        base_config = super(FTML, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
keras_contrib/optimizers/lars.py,0,"b'from keras import backend as K\nfrom keras.optimizers import Optimizer\n\n\nclass LARS(Optimizer):\n    """"""Layer-wise Adaptive Rate Scaling for large batch training.\n    Introduced by ""Large Batch Training of Convolutional Networks"" by Y. You,\n    I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n    Implements the LARS learning rate scheme presented in the paper above. This\n    optimizer is useful when scaling the batch size to up to 32K without\n    significant performance degradation. It is recommended to use the optimizer\n    in conjunction with:\n        - Gradual learning rate warm-up\n        - Linear learning rate scaling\n        - Poly rule learning rate decay\n    Note, LARS scaling is currently only enabled for dense tensors.\n\n    Args:\n        lr: A `Tensor` or floating point value. The base learning rate.\n        momentum: A floating point value. Momentum hyperparameter.\n        weight_decay: A floating point value. Weight decay hyperparameter.\n        eeta: LARS coefficient as used in the paper. Dfault set to LARS\n            coefficient from the paper. (eeta / weight_decay) determines the\n            highest scaling factor in LARS.\n        epsilon: Optional epsilon parameter to be set in models that have very\n            small gradients. Default set to 0.0.\n        nesterov: when set to True, nesterov momentum will be enabled\n    """"""\n\n    def __init__(self,\n                 lr,\n                 momentum=0.9,\n                 weight_decay=0.0001,\n                 eeta=0.001,\n                 epsilon=0.0,\n                 nesterov=False,\n                 **kwargs):\n\n        if momentum < 0.0:\n            raise ValueError(""momentum should be positive: %s"" % momentum)\n        if weight_decay < 0.0:\n            raise ValueError(""weight_decay is not positive: %s"" % weight_decay)\n        super(LARS, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype=\'int64\', name=\'iterations\')\n            self.lr = K.variable(lr, name=\'lr\')\n            self.momentum = K.variable(momentum, name=\'momentum\')\n            self.weight_decay = K.variable(weight_decay, name=\'weight_decay\')\n            self.eeta = K.variable(eeta, name=\'eeta\')\n        self.epsilon = epsilon\n        self.nesterov = nesterov\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        weights = self.get_weights()\n        self.updates = [K.update_add(self.iterations, 1)]\n        scaled_lr = self.lr\n        w_norm = K.sqrt(K.sum([K.sum(K.square(weight))\n                               for weight in weights]))\n        g_norm = K.sqrt(K.sum([K.sum(K.square(grad))\n                               for grad in grads]))\n        scaled_lr = K.switch(K.greater(w_norm * g_norm, K.zeros([1])),\n                             K.expand_dims((self.eeta * w_norm /\n                                            (g_norm + self.weight_decay * w_norm +\n                                             self.epsilon)) * self.lr),\n                             K.ones([1]) * self.lr)\n        if K.backend() == \'theano\':\n            scaled_lr = scaled_lr[0]  # otherwise theano raise broadcasting error\n        # momentum\n        moments = [K.zeros(K.int_shape(param), dtype=K.dtype(param))\n                   for param in params]\n        self.weights = [self.iterations] + moments\n        for param, grad, moment in zip(params, grads, moments):\n            v0 = (moment * self.momentum)\n            v1 = scaled_lr * grad  # velocity\n            veloc = v0 - v1\n            self.updates.append(K.update(moment, veloc))\n\n            if self.nesterov:\n                new_param = param + (veloc * self.momentum) - v1\n            else:\n                new_param = param + veloc\n\n            # Apply constraints.\n            if getattr(param, \'constraint\', None) is not None:\n                new_param = param.constraint(new_param)\n\n            self.updates.append(K.update(param, new_param))\n        return self.updates\n\n    def get_config(self):\n        config = {\'lr\': float(K.get_value(self.lr)),\n                  \'momentum\': float(K.get_value(self.momentum)),\n                  \'weight_decay\': float(K.get_value(self.weight_decay)),\n                  \'epsilon\': self.epsilon,\n                  \'eeta\': float(K.get_value(self.eeta)),\n                  \'nesterov\': self.nesterov}\n        base_config = super(LARS, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
keras_contrib/optimizers/padam.py,0,"b'from keras import backend as K\nfrom keras.optimizers import Optimizer\n\n\nclass Padam(Optimizer):\n    """"""Partially adaptive momentum estimation optimizer.\n\n    # Arguments\n        lr: float >= 0. Learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 1.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n        decay: float >= 0. Learning rate decay over each update.\n        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n            algorithm from the paper ""On the Convergence of Adam and\n            Beyond"".\n        partial: float, 0 <= partial <= 0.5 . Parameter controlling partial\n            momentum adaption. For `partial=0`, this optimizer behaves like SGD,\n            for `partial=0.5` it behaves like AMSGrad.\n\n    # References\n        - [Closing the Generalization Gap of Adaptive Gradient Methods\n        in Training Deep Neural Networks](https://arxiv.org/pdf/1806.06763.pdf)\n\n    """"""\n\n    def __init__(self, lr=1e-1, beta_1=0.9, beta_2=0.999,\n                 epsilon=1e-8, decay=0., amsgrad=False, partial=1. / 8., **kwargs):\n        if partial < 0 or partial > 0.5:\n            raise ValueError(\n                ""Padam: \'partial\' must be a positive float with a maximum ""\n                ""value of `0.5`, since higher values will cause divergence ""\n                ""during training.""\n            )\n        super(Padam, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype=\'int64\', name=\'iterations\')\n            self.lr = K.variable(lr, name=\'lr\')\n            self.beta_1 = K.variable(beta_1, name=\'beta_1\')\n            self.beta_2 = K.variable(beta_2, name=\'beta_2\')\n            self.decay = K.variable(decay, name=\'decay\')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.partial = partial\n        self.initial_decay = decay\n        self.amsgrad = amsgrad\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                      K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n                     (1. - K.pow(self.beta_1, t)))\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        else:\n            vhats = [K.zeros(1) for _ in params]\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                denom = (K.sqrt(vhat_t) + self.epsilon)\n                self.updates.append(K.update(vhat, vhat_t))\n            else:\n                denom = (K.sqrt(v_t) + self.epsilon)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n\n            # Partial momentum adaption.\n            new_p = p - (lr_t * (m_t / (denom ** (self.partial * 2))))\n\n            # Apply constraints.\n            if getattr(p, \'constraint\', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {\'lr\': float(K.get_value(self.lr)),\n                  \'beta_1\': float(K.get_value(self.beta_1)),\n                  \'beta_2\': float(K.get_value(self.beta_2)),\n                  \'decay\': float(K.get_value(self.decay)),\n                  \'epsilon\': self.epsilon,\n                  \'amsgrad\': self.amsgrad,\n                  \'partial\': self.partial}\n        base_config = super(Padam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
keras_contrib/optimizers/yogi.py,0,"b'from keras import backend as K\nfrom keras.optimizers import Optimizer\n\n\nclass Yogi(Optimizer):\n    """"""Yogi optimizer.\n    Yogi is a variation of Adam that controls the increase in effective\n    learning rate, which (according to the paper) leads to even better\n    performance than Adam with similar theoretical guarantees on convergence.\n    Default parameters follow those provided in the original paper, Tab.1\n    # Arguments\n        lr: float >= 0. Learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 1.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n        decay: float >= 0. Learning rate decay over each update.\n    # References\n        - [Adaptive Methods for Nonconvex Optimization](\n           https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization)\n\n    If you open an issue or a pull request about the Yogi optimizer,\n    please add \'cc @MarcoAndreaBuchmann\' to notify him.\n    """"""\n\n    def __init__(self, lr=0.01, beta_1=0.9, beta_2=0.999,\n                 epsilon=1e-3, decay=0., **kwargs):\n        super(Yogi, self).__init__(**kwargs)\n        if beta_1 <= 0 or beta_1 >= 1:\n            raise ValueError(""beta_1 has to be in ]0, 1["")\n        if beta_2 <= 0 or beta_2 >= 1:\n            raise ValueError(""beta_2 has to be in ]0, 1["")\n\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype=\'int64\', name=\'iterations\')\n            self.lr = K.variable(lr, name=\'lr\')\n            self.beta_1 = K.variable(beta_1, name=\'beta_1\')\n            self.beta_2 = K.variable(beta_2, name=\'beta_2\')\n            self.decay = K.variable(decay, name=\'decay\')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        if epsilon <= 0:\n            raise ValueError(""epsilon has to be larger than 0"")\n        self.epsilon = epsilon\n        self.initial_decay = decay\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                      K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n                     (1. - K.pow(self.beta_1, t)))\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vhats = [K.zeros(1) for _ in params]\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n            g2 = K.square(g)\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = v - (1. - self.beta_2) * K.sign(v - g2) * g2\n            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, \'constraint\', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {\'lr\': float(K.get_value(self.lr)),\n                  \'beta_1\': float(K.get_value(self.beta_1)),\n                  \'beta_2\': float(K.get_value(self.beta_2)),\n                  \'decay\': float(K.get_value(self.decay)),\n                  \'epsilon\': self.epsilon}\n        base_config = super(Yogi, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
keras_contrib/preprocessing/__init__.py,0,b''
keras_contrib/regularizers/__init__.py,0,b'\nfrom __future__ import absolute_import\n'
keras_contrib/tests/__init__.py,0,b''
keras_contrib/tests/activations.py,0,"b""import numpy as np\n\nfrom keras import backend as K\n\n\ndef get_standard_values():\n    '''\n    These are just a set of floats used for testing the activation\n    functions, and are useful in multiple tests.\n    '''\n    return np.array([[0, 0.1, 0.5, 0.9, 1.0]], dtype=K.floatx())\n\n\ndef validate_activation(activation):\n    activation(get_standard_values())\n"""
keras_contrib/tests/metrics.py,0,"b'import numpy as np\n\nfrom keras import backend as K\n\nall_metrics = []\nall_sparse_metrics = []\n\n\ndef validate_metric(metric):\n    y_a = K.variable(np.random.random((6, 7)))\n    y_b = K.variable(np.random.random((6, 7)))\n    output = metric(y_a, y_b)\n    assert K.eval(output).shape == ()\n'"
keras_contrib/tests/optimizers.py,0,"b""from __future__ import print_function\nimport numpy as np\n\nfrom keras_contrib.utils import test_utils\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.utils import to_categorical\n\n\ndef get_test_data():\n    np.random.seed(1337)\n    (x_train, y_train), _ = test_utils.get_test_data(num_train=1000,\n                                                     num_test=200,\n                                                     input_shape=(10,),\n                                                     classification=True,\n                                                     num_classes=2)\n    y_train = to_categorical(y_train)\n    return x_train, y_train\n\n\ndef get_model(input_dim, num_hidden, output_dim):\n    model = Sequential()\n    model.add(Dense(num_hidden, input_shape=(input_dim,)))\n    model.add(Activation('relu'))\n    model.add(Dense(output_dim))\n    model.add(Activation('softmax'))\n    return model\n\n\ndef _test_optimizer(optimizer, target=0.75):\n    x_train, y_train = get_test_data()\n    model = get_model(x_train.shape[1], 10, y_train.shape[1])\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n    history = model.fit(x_train, y_train, epochs=2, batch_size=16, verbose=0)\n    assert history.history['acc'][-1] >= target\n    config = optimizers.serialize(optimizer)\n    custom_objects = {optimizer.__class__.__name__: optimizer.__class__}\n    optim = optimizers.deserialize(config, custom_objects)\n    new_config = optimizers.serialize(optim)\n    assert config == new_config\n"""
keras_contrib/tests/regularizers.py,0,"b""import numpy as np\nfrom keras.datasets import mnist\nfrom keras.layers import Activation\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\n\nnp.random.seed(1337)\n\nnb_classes = 10\nbatch_size = 128\nnb_epoch = 5\nweighted_class = 9\nstandard_weight = 1\nhigh_weight = 5\nmax_train_samples = 5000\nmax_test_samples = 1000\n\n\ndef get_data():\n    # the data, shuffled and split between tran and test sets\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n    X_train = X_train.reshape(60000, 784)[:max_train_samples]\n    X_test = X_test.reshape(10000, 784)[:max_test_samples]\n    X_train = X_train.astype('float32') / 255\n    X_test = X_test.astype('float32') / 255\n\n    # convert class vectors to binary class matrices\n    y_train = y_train[:max_train_samples]\n    y_test = y_test[:max_test_samples]\n    Y_train = np_utils.to_categorical(y_train, nb_classes)\n    Y_test = np_utils.to_categorical(y_test, nb_classes)\n    test_ids = np.where(y_test == np.array(weighted_class))[0]\n\n    return (X_train, Y_train), (X_test, Y_test), test_ids\n\n\ndef validate_regularizer(weight_reg=None, activity_reg=None):\n    model = Sequential()\n    model.add(Dense(50, input_shape=(784,)))\n    model.add(Activation('relu'))\n    model.add(Dense(10, W_regularizer=weight_reg,\n                    activity_regularizer=activity_reg))\n    model.add(Activation('softmax'))\n    return model\n"""
keras_contrib/utils/__init__.py,0,b''
keras_contrib/utils/conv_utils.py,1,"b'import keras.backend as K\r\n\r\n\r\ndef conv_output_length(input_length, filter_size,\r\n                       padding, stride, dilation=1):\r\n    """"""Determines output length of a convolution given input length.\r\n\r\n    Copy of the function of keras-team/keras because it\'s not in the public API\r\n    So we can\'t use the function in keras-team/keras to test tf.keras\r\n\r\n    # Arguments\r\n        input_length: integer.\r\n        filter_size: integer.\r\n        padding: one of `""same""`, `""valid""`, `""full""`.\r\n        stride: integer.\r\n        dilation: dilation rate, integer.\r\n\r\n    # Returns\r\n        The output length (integer).\r\n    """"""\r\n    if input_length is None:\r\n        return None\r\n    assert padding in {\'same\', \'valid\', \'full\', \'causal\'}\r\n    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\r\n    if padding == \'same\':\r\n        output_length = input_length\r\n    elif padding == \'valid\':\r\n        output_length = input_length - dilated_filter_size + 1\r\n    elif padding == \'causal\':\r\n        output_length = input_length\r\n    elif padding == \'full\':\r\n        output_length = input_length + dilated_filter_size - 1\r\n    return (output_length + stride - 1) // stride\r\n\r\n\r\ndef normalize_data_format(value):\r\n    """"""Checks that the value correspond to a valid data format.\r\n\r\n    Copy of the function in keras-team/keras because it\'s not public API.\r\n\r\n    # Arguments\r\n        value: String or None. `\'channels_first\'` or `\'channels_last\'`.\r\n\r\n    # Returns\r\n        A string, either `\'channels_first\'` or `\'channels_last\'`\r\n\r\n    # Example\r\n    ```python\r\n        >>> from keras import backend as K\r\n        >>> K.normalize_data_format(None)\r\n        \'channels_first\'\r\n        >>> K.normalize_data_format(\'channels_last\')\r\n        \'channels_last\'\r\n    ```\r\n\r\n    # Raises\r\n        ValueError: if `value` or the global `data_format` invalid.\r\n    """"""\r\n    if value is None:\r\n        value = K.image_data_format()\r\n    data_format = value.lower()\r\n    if data_format not in {\'channels_first\', \'channels_last\'}:\r\n        raise ValueError(\'The `data_format` argument must be one of \'\r\n                         \'""channels_first"", ""channels_last"". Received: \' +\r\n                         str(value))\r\n    return data_format\r\n'"
keras_contrib/utils/save_load_utils.py,0,"b'import warnings\n\nimport h5py\nimport keras.backend as K\nfrom keras import optimizers\nfrom keras.engine import saving\n\n\ndef save_all_weights(model, filepath, include_optimizer=True):\n    """"""\n    Save model weights and optimizer weights but not configuration to a HDF5 file.\n    Functionally between `save` and `save_weights`.\n\n    The HDF5 file contains:\n        - the model\'s weights\n        - the model\'s optimizer\'s state (if any)\n    If you have a complicated model or set of models that do not serialize\n    to JSON correctly, use this method.\n    # Arguments\n        model: Keras model instance to be saved.\n        filepath: String, path where to save the model.\n        include_optimizer: If True, save optimizer\'s state together.\n    # Raises\n        ImportError: if h5py is not available.\n    """"""\n    if h5py is None:\n        raise ImportError(\'`save_all_weights` requires h5py.\')\n\n    with h5py.File(filepath, \'w\') as f:\n        model_weights_group = f.create_group(\'model_weights\')\n        model_layers = model.layers\n        saving.save_weights_to_hdf5_group(model_weights_group, model_layers)\n\n        if include_optimizer and hasattr(model, \'optimizer\') and model.optimizer:\n            if isinstance(model.optimizer, optimizers.TFOptimizer):\n                warnings.warn(\n                    \'TensorFlow optimizers do not \'\n                    \'make it possible to access \'\n                    \'optimizer attributes or optimizer state \'\n                    \'after instantiation. \'\n                    \'As a result, we cannot save the optimizer \'\n                    \'as part of the model save file.\'\n                    \'You will have to compile your model again after loading it. \'\n                    \'Prefer using a Keras optimizer instead \'\n                    \'(see keras.io/optimizers).\')\n            else:\n                # Save optimizer weights.\n                symbolic_weights = getattr(model.optimizer, \'weights\')\n                if symbolic_weights:\n                    optimizer_weights_group = f.create_group(\'optimizer_weights\')\n                    weight_values = K.batch_get_value(symbolic_weights)\n                    weight_names = []\n                    for i, (w, val) in enumerate(zip(symbolic_weights, weight_values)):\n                        # Default values of symbolic_weights is /variable for theano\n                        if K.backend() == \'theano\':\n                            if hasattr(w, \'name\') and w.name != ""/variable"":\n                                name = str(w.name)\n                            else:\n                                name = \'param_\' + str(i)\n                        else:\n                            if hasattr(w, \'name\') and w.name:\n                                name = str(w.name)\n                            else:\n                                name = \'param_\' + str(i)\n                        weight_names.append(name.encode(\'utf8\'))\n                    optimizer_weights_group.attrs[\'weight_names\'] = weight_names\n                    for name, val in zip(weight_names, weight_values):\n                        param_dset = optimizer_weights_group.create_dataset(\n                            name,\n                            val.shape,\n                            dtype=val.dtype)\n                        if not val.shape:\n                            # scalar\n                            param_dset[()] = val\n                        else:\n                            param_dset[:] = val\n\n\ndef load_all_weights(model, filepath, include_optimizer=True):\n    """"""Loads the weights of a model saved via `save_all_weights`.\n    If model has been compiled, optionally load its optimizer\'s weights.\n    # Arguments\n        model: instantiated model with architecture matching the saved model.\n            Compile the model beforehand if you want to load optimizer weights.\n        filepath: String, path to the saved model.\n    # Returns\n        None. The model will have its weights updated.\n    # Raises\n        ImportError: if h5py is not available.\n        ValueError: In case of an invalid savefile.\n    """"""\n    if h5py is None:\n        raise ImportError(\'`load_all_weights` requires h5py.\')\n\n    with h5py.File(filepath, mode=\'r\') as f:\n        # set weights\n        saving.load_weights_from_hdf5_group(f[\'model_weights\'], model.layers)\n        # Set optimizer weights.\n        if (include_optimizer\n                and \'optimizer_weights\' in f and hasattr(model, \'optimizer\')\n                and model.optimizer):\n            optimizer_weights_group = f[\'optimizer_weights\']\n            optimizer_weight_names = [n.decode(\'utf8\') for n in\n                                      optimizer_weights_group.attrs[\'weight_names\']]\n            optimizer_weight_values = [optimizer_weights_group[n] for n in\n                                       optimizer_weight_names]\n            model.optimizer.set_weights(optimizer_weight_values)\n'"
keras_contrib/utils/test_utils.py,5,"b'""""""Utilities related to Keras unit tests.""""""\nimport sys\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport inspect\n\nimport keras\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras import backend as K\n\n\ndef get_test_data(num_train=1000, num_test=500, input_shape=(10,),\n                  output_shape=(2,),\n                  classification=True, num_classes=2):\n    """"""Generates test data to train a model on.\n\n    classification=True overrides output_shape\n    (i.e. output_shape is set to (1,)) and the output\n    consists in integers in [0, num_class-1].\n\n    Otherwise: float output with shape output_shape.\n    """"""\n    samples = num_train + num_test\n    if classification:\n        y = np.random.randint(0, num_classes, size=(samples,))\n        X = np.zeros((samples,) + input_shape)\n        for i in range(samples):\n            X[i] = np.random.normal(loc=y[i], scale=0.7, size=input_shape)\n    else:\n        y_loc = np.random.random((samples,))\n        X = np.zeros((samples,) + input_shape)\n        y = np.zeros((samples,) + output_shape)\n        for i in range(samples):\n            X[i] = np.random.normal(loc=y_loc[i], scale=0.7, size=input_shape)\n            y[i] = np.random.normal(loc=y_loc[i], scale=0.7, size=output_shape)\n\n    return (X[:num_train], y[:num_train]), (X[num_train:], y[num_train:])\n\n\ndef layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n               input_data=None, expected_output=None,\n               expected_output_dtype=None, fixed_batch_size=False):\n    """"""Test routine for a layer with a single input tensor\n    and single output tensor.\n\n    Copy of the function in keras-team/keras because it\'s not in the public API.\n    If we use the one from keras-team/keras it won\'t work with tf.keras.\n    """"""\n    # generate input data\n    if input_data is None:\n        assert input_shape\n        if not input_dtype:\n            input_dtype = K.floatx()\n        input_data_shape = list(input_shape)\n        for i, e in enumerate(input_data_shape):\n            if e is None:\n                input_data_shape[i] = np.random.randint(1, 4)\n        input_data = (10 * np.random.random(input_data_shape))\n        input_data = input_data.astype(input_dtype)\n    else:\n        if input_shape is None:\n            input_shape = input_data.shape\n        if input_dtype is None:\n            input_dtype = input_data.dtype\n    if expected_output_dtype is None:\n        expected_output_dtype = input_dtype\n\n    # instantiation\n    layer = layer_cls(**kwargs)\n\n    # test get_weights , set_weights at layer level\n    weights = layer.get_weights()\n    layer.set_weights(weights)\n\n    expected_output_shape = layer.compute_output_shape(input_shape)\n\n    # test in functional API\n    if fixed_batch_size:\n        x = Input(batch_shape=input_shape, dtype=input_dtype)\n    else:\n        x = Input(shape=input_shape[1:], dtype=input_dtype)\n    y = layer(x)\n    assert K.dtype(y) == expected_output_dtype\n\n    # check with the functional API\n    model = Model(x, y)\n\n    actual_output = model.predict(input_data)\n    actual_output_shape = actual_output.shape\n    for expected_dim, actual_dim in zip(expected_output_shape,\n                                        actual_output_shape):\n        if expected_dim is not None:\n            assert expected_dim == actual_dim\n\n    if expected_output is not None:\n        assert_allclose(actual_output, expected_output, rtol=1e-3)\n\n    # test serialization, weight setting at model level\n    model_config = model.get_config()\n    custom_objects = {layer.__class__.__name__: layer.__class__}\n    recovered_model = model.__class__.from_config(model_config, custom_objects)\n    if model.weights:\n        weights = model.get_weights()\n        recovered_model.set_weights(weights)\n        _output = recovered_model.predict(input_data)\n        assert_allclose(_output, actual_output, rtol=1e-3)\n\n    # test training mode (e.g. useful when the layer has a\n    # different behavior at training and testing time).\n    if has_arg(layer.call, \'training\'):\n        model.compile(\'rmsprop\', \'mse\')\n        model.train_on_batch(input_data, actual_output)\n\n    # test instantiation from layer config\n    layer_config = layer.get_config()\n    layer_config[\'batch_input_shape\'] = input_shape\n    layer = layer.__class__.from_config(layer_config)\n\n    # for further checks in the caller function\n    return actual_output\n\n\ndef has_arg(fn, name, accept_all=False):\n    """"""Checks if a callable accepts a given keyword argument.\n\n    For Python 2, checks if there is an argument with the given name.\n\n    For Python 3, checks if there is an argument with the given name, and\n    also whether this argument can be called with a keyword (i.e. if it is\n    not a positional-only argument).\n\n    This function is a copy of the one in keras-team/keras because it\'s not\n    in the public API.\n\n    # Arguments\n        fn: Callable to inspect.\n        name: Check if `fn` can be called with `name` as a keyword argument.\n        accept_all: What to return if there is no parameter called `name`\n                    but the function accepts a `**kwargs` argument.\n\n    # Returns\n        bool, whether `fn` accepts a `name` keyword argument.\n    """"""\n    if sys.version_info < (3,):\n        arg_spec = inspect.getargspec(fn)\n        if accept_all and arg_spec.keywords is not None:\n            return True\n        return name in arg_spec.args\n    elif sys.version_info < (3, 3):\n        arg_spec = inspect.getfullargspec(fn)\n        if accept_all and arg_spec.varkw is not None:\n            return True\n        return (name in arg_spec.args or\n                name in arg_spec.kwonlyargs)\n    else:\n        signature = inspect.signature(fn)\n        parameter = signature.parameters.get(name)\n        if parameter is None:\n            if accept_all:\n                for param in signature.parameters.values():\n                    if param.kind == inspect.Parameter.VAR_KEYWORD:\n                        return True\n            return False\n        return (parameter.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD,\n                                   inspect.Parameter.KEYWORD_ONLY))\n\n\ndef to_list(x, allow_tuple=False):\n    if isinstance(x, list):\n        return x\n    if allow_tuple and isinstance(x, tuple):\n        return list(x)\n    return [x]\n\n\ndef unpack_singleton(x):\n    if len(x) == 1:\n        return x[0]\n    return x\n\n\nif keras.__name__ == \'keras\':\n    is_tf_keras = False\nelif keras.__name__ == \'tensorflow.keras\':\n    is_tf_keras = True\nelse:\n    raise KeyError(\'Cannot detect if using keras or tf.keras.\')\n\n\ndef to_tuple(shape):\n    """"""This functions is here to fix an inconsistency between keras and tf.keras.\n\n    In tf.keras, the input_shape argument is an tuple with `Dimensions` objects.\n    In keras, the input_shape is a simple tuple of ints or `None`.\n\n    We\'ll work with tuples of ints or `None` to be consistent\n    with keras-team/keras. So we must apply this function to\n    all input_shapes of the build methods in custom layers.\n    """"""\n    if is_tf_keras:\n        import tensorflow as tf\n        return tuple(tf.TensorShape(shape).as_list())\n    else:\n        return shape\n'"
keras_contrib/wrappers/__init__.py,0,b''
tests/keras_contrib/constraints_test.py,0,"b""import pytest\nimport numpy as np\n\nfrom keras import backend as K\nfrom keras_contrib import constraints\n\n\ntest_values = [0.1, 0.5, 3, 8, 1e-7]\nnp.random.seed(3537)\nexample_array = np.random.random((100, 100)) * 100. - 50.\nexample_array[0, 0] = 0.  # 0 could possibly cause trouble\n\n\ndef test_clip():\n    clip_instance = constraints.clip()\n    clipped = clip_instance(K.variable(example_array))\n    assert(np.max(np.abs(K.eval(clipped))) <= K.cast_to_floatx(0.01))\n    clip_instance = constraints.clip(0.1)\n    clipped = clip_instance(K.variable(example_array))\n    assert(np.max(np.abs(K.eval(clipped))) <= K.cast_to_floatx(0.1))\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/keras_contrib/initializers_test.py,0,"b""from keras import backend as K\nfrom keras_contrib import backend as KC\nfrom keras_contrib import initializers\nimport pytest\nimport numpy as np\n\n\n# 2D tensor test fixture\nFC_SHAPE = (100, 100)\n\n# 4D convolution in th order. This shape has the same effective shape as\n# FC_SHAPE\nCONV_SHAPE = (25, 25, 2, 2)\n\n# The equivalent shape of both test fixtures\nSHAPE = (100, 100)\n\n\ndef _runner(init, shape, target_mean=None, target_std=None,\n            target_max=None, target_min=None, upper_bound=None, lower_bound=None):\n    variable = init(shape)\n    if not isinstance(variable, np.ndarray):\n        output = K.get_value(variable)\n    else:\n        output = variable\n\n    lim = 1e-2\n    if target_std is not None:\n        assert abs(output.std() - target_std) < lim\n    if target_mean is not None:\n        assert abs(output.mean() - target_mean) < lim\n    if target_max is not None:\n        assert abs(output.max() - target_max) < lim\n    if target_min is not None:\n        assert abs(output.min() - target_min) < lim\n    if upper_bound is not None:\n        assert output.max() < upper_bound\n    if lower_bound is not None:\n        assert output.min() > lower_bound\n\n\n'''\n# Example :\n\n@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_uniform(tensor_shape):\n    _runner(initializations.uniform, tensor_shape, target_mean=0.,\n            target_max=0.05, target_min=-0.05)\n\n'''\n\n\n@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_cai(tensor_shape):\n    # upper and lower bounds are proved in original paper\n    _runner(initializers.ConvolutionAware(), tensor_shape,\n            upper_bound=1, lower_bound=-1)\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/tooling/test_codeowners.py,0,"b""import os\r\nimport pytest\r\nfrom github import Github\r\ntry:\r\n    import pathlib\r\nexcept ImportError:\r\n    import pathlib2 as pathlib\r\n\r\npath_to_keras_contrib = pathlib.Path(__file__).resolve().parents[2]\r\npath_to_codeowners = path_to_keras_contrib / 'CODEOWNERS'\r\n\r\nauthenticated = True\r\ntry:\r\n    github_client = Github(os.environ['GITHUB_TOKEN'])\r\nexcept KeyError:\r\n    try:\r\n        github_client = Github(os.environ['GITHUB_USER'],\r\n                               os.environ['GITHUB_PASSWORD'])\r\n    except KeyError:\r\n        authenticated = False\r\n\r\n\r\ndef parse_codeowners():\r\n    map_path_owner = []\r\n    for line in open(path_to_codeowners, 'r'):\r\n        line = line.strip()\r\n        if line.startswith('#') or line == '':\r\n            continue\r\n        x = line.split(' ')\r\n        path = path_to_keras_contrib / x[0]\r\n        owner = x[-1]\r\n        map_path_owner.append((path, owner))\r\n    return map_path_owner\r\n\r\n\r\ndef test_codeowners_file_exist():\r\n    for path, _ in parse_codeowners():\r\n        assert path.exists()\r\n\r\n\r\n@pytest.mark.skipif(not authenticated,\r\n                    reason='It should be possible to run the test without'\r\n                           'authentication, but we might get our request refused'\r\n                           'by github. To be deterministic, we\\'ll disable it.')\r\ndef test_codeowners_user_exist():\r\n    for _, user in parse_codeowners():\r\n        assert user[0] == '@'\r\n        assert github_client.get_user(user[1:])\r\n\r\n\r\ndirectories_to_test = [\r\n    'examples',\r\n    'keras_contrib/activations',\r\n    'keras_contrib/applications',\r\n    'keras_contrib/callbacks',\r\n    'keras_contrib/constraints',\r\n    'keras_contrib/datasets',\r\n    'keras_contrib/initializers',\r\n    'keras_contrib/layers',\r\n    'keras_contrib/losses',\r\n    'keras_contrib/metrics',\r\n    'keras_contrib/optimizers',\r\n    'keras_contrib/preprocessing',\r\n    'keras_contrib/regularizers',\r\n    'keras_contrib/wrappers'\r\n]\r\ndirectories_to_test = [path_to_keras_contrib / x for x in directories_to_test]\r\n\r\n# TODO: remove those files or find them owners.\r\nexclude = [\r\n    'examples/cifar10_clr.py',\r\n    'examples/cifar10_densenet.py',\r\n    'examples/cifar10_nasnet.py',\r\n    'examples/cifar10_resnet.py',\r\n    'examples/cifar10_ror.py',\r\n    'examples/cifar10_wide_resnet.py',\r\n    'examples/conll2000_chunking_crf.py',\r\n    'examples/improved_wgan.py',\r\n    'examples/jaccard_loss.py',\r\n    'keras_contrib/callbacks/cyclical_learning_rate.py',\r\n    'keras_contrib/callbacks/dead_relu_detector.py',\r\n    'keras_contrib/applications/resnet.py',\r\n    'keras_contrib/constraints/clip.py',\r\n    'keras_contrib/datasets/coco.py',\r\n    'keras_contrib/datasets/conll2000.py',\r\n    'keras_contrib/datasets/pascal_voc.py',\r\n    'keras_contrib/initializers/convaware.py',\r\n    'keras_contrib/losses/crf_losses.py',\r\n    'keras_contrib/losses/dssim.py',\r\n    'keras_contrib/losses/jaccard.py',\r\n    'keras_contrib/layers/advanced_activations/pelu.py',\r\n    'keras_contrib/layers/advanced_activations/srelu.py',\r\n    'keras_contrib/layers/convolutional/cosineconvolution2d.py',\r\n    'keras_contrib/layers/core.py',\r\n    'keras_contrib/layers/crf.py',\r\n    'keras_contrib/layers/normalization/instancenormalization.py',\r\n    'keras_contrib/optimizers/ftml.py',\r\n    'keras_contrib/optimizers/lars.py',\r\n    'keras_contrib/metrics/crf_accuracies.py',\r\n]\r\nexclude = [path_to_keras_contrib / x for x in exclude]\r\n\r\n\r\n@pytest.mark.parametrize('directory', directories_to_test)\r\ndef test_all_files_have_owners(directory):\r\n    files_with_owners = [x[0] for x in parse_codeowners()]\r\n    for root, dirs, files in os.walk(directory):\r\n        for name in files:\r\n            file_path = pathlib.Path(root) / name\r\n            if file_path.suffix != '.py':\r\n                continue\r\n            if file_path.name == '__init__.py':\r\n                continue\r\n            if file_path in exclude:\r\n                continue\r\n            assert file_path in files_with_owners\r\n\r\n\r\nif __name__ == '__main__':\r\n    pytest.main([__file__])\r\n"""
tests/tooling/test_doc_auto_generation.py,0,"b'from markdown import markdown\nfrom docs import autogen\nimport pytest\n\ntest_doc1 = {\n    \'doc\': """"""Base class for recurrent layers.\n\n    # Arguments\n        cell: A RNN cell instance. A RNN cell is a class that has:\n            - a `call(input_at_t, states_at_t)` method, returning\n                `(output_at_t, states_at_t_plus_1)`. The call method of the\n                cell can also take the optional argument `constants`, see\n                section ""Note on passing external constants"" below.\n            - a `state_size` attribute. This can be a single integer\n                (single state) in which case it is\n                the size of the recurrent state\n                (which should be the same as the size of the cell output).\n                This can also be a list/tuple of integers\n                (one size per state). In this case, the first entry\n                (`state_size[0]`) should be the same as\n                the size of the cell output.\n            It is also possible for `cell` to be a list of RNN cell instances,\n            in which cases the cells get stacked on after the other in the RNN,\n            implementing an efficient stacked RNN.\n        return_sequences: Boolean. Whether to return the last output\n            in the output sequence, or the full sequence.\n        return_state: Boolean. Whether to return the last state\n            in addition to the output.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards and return the\n            reversed sequence.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively,\n            the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        input_length: Length of input sequences, to be specified\n            when it is constant.\n            This argument is required if you are going to connect\n            `Flatten` then `Dense` layers upstream\n            (without it, the shape of the dense outputs cannot be computed).\n            Note that if the recurrent layer is not the first layer\n            in your model, you would need to specify the input length\n            at the level of the first layer\n            (e.g. via the `input_shape` argument)\n\n    # Input shape\n        3D tensor with shape `(batch_size, timesteps, input_dim)`.\n\n    # Output shape\n        - if `return_state`: a list of tensors. The first tensor is\n            the output. The remaining tensors are the last states,\n            each with shape `(batch_size, units)`.\n        - if `return_sequences`: 3D tensor with shape\n            `(batch_size, timesteps, units)`.\n        - else, 2D tensor with shape `(batch_size, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    # Note on using statefulness in RNNs\n        You can set RNN layers to be \'stateful\', which means that the states\n        computed for the samples in one batch will be reused as initial states\n        for the samples in the next batch. This assumes a one-to-one mapping\n        between samples in different successive batches.\n\n        To enable statefulness:\n            - specify `stateful=True` in the layer constructor.\n            - specify a fixed batch size for your model, by passing\n                if sequential model:\n                  `batch_input_shape=(...)` to the first layer in your model.\n                else for functional model with 1 or more Input layers:\n                  `batch_shape=(...)` to all the first layers in your model.\n                This is the expected shape of your inputs\n                *including the batch size*.\n                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n            - specify `shuffle=False` when calling fit().\n\n        To reset the states of your model, call `.reset_states()` on either\n        a specific layer, or on your entire model.\n\n    # Note on specifying the initial state of RNNs\n    Note: that\n        One: You can specify the initial state of RNN layers symbolically by\n            calling them with the keyword argument `initial_state`.\n        Two: The value of `initial_state` should be a tensor or list of\n            tensors representing\n            the initial state of the RNN layer.\n        You can specify the initial state of RNN layers numerically by:\n        One: calling `reset_states`\n            - With the keyword argument `states`.\n                - The value of\n            `states` should be a numpy array or\n            list of numpy arrays representing\n        the initial state of the RNN layer.\n\n    # Note on passing external constants to RNNs\n        You can pass ""external"" constants to the cell using the `constants`\n        keyword: argument of `RNN.__call__` (as well as `RNN.call`) method.\n        This: requires that the `cell.call` method accepts the same keyword argument\n        `constants`. Such constants can be used to condition the cell\n        transformation on additional static inputs (not changing over time),\n        a.k.a. an attention mechanism.\n\n    # Examples\n\n    ```python\n        # First, let\'s define a RNN Cell, as a layer subclass.\n\n        class MinimalRNNCell(keras.layers.Layer):\n\n            def __init__(self, units, **kwargs):\n                self.units = units\n                self.state_size = units\n                super(MinimalRNNCell, self).__init__(**kwargs)\n\n            def build(self, input_shape):\n                self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                              initializer=\'uniform\',\n                                              name=\'kernel\')\n                self.recurrent_kernel = self.add_weight(\n                    shape=(self.units, self.units),\n                    initializer=\'uniform\',\n                    name=\'recurrent_kernel\')\n                self.built = True\n\n            def call(self, inputs, states):\n                prev_output = states[0]\n                h = K.dot(inputs, self.kernel)\n                output = h + K.dot(prev_output, self.recurrent_kernel)\n                return output, [output]\n\n        # Let\'s use this cell in a RNN layer:\n\n        cell = MinimalRNNCell(32)\n        x = keras.Input((None, 5))\n        layer = RNN(cell)\n        y = layer(x)\n\n        # Here\'s how to use the cell to build a stacked RNN:\n\n        cells = [MinimalRNNCell(32), MinimalRNNCell(64)]\n        x = keras.Input((None, 5))\n        layer = RNN(cells)\n        y = layer(x)\n    ```\n    """""",\n    \'result\': \'\'\'Base class for recurrent layers.\n\n__Arguments__\n\n- __cell__: A RNN cell instance. A RNN cell is a class that has:\n    - a `call(input_at_t, states_at_t)` method, returning\n        `(output_at_t, states_at_t_plus_1)`. The call method of the\n        cell can also take the optional argument `constants`, see\n        section ""Note on passing external constants"" below.\n    - a `state_size` attribute. This can be a single integer\n        (single state) in which case it is\n        the size of the recurrent state\n        (which should be the same as the size of the cell output).\n        This can also be a list/tuple of integers\n        (one size per state). In this case, the first entry\n        (`state_size[0]`) should be the same as\n        the size of the cell output.\n\n    It is also possible for `cell` to be a list of RNN cell instances,\n    in which cases the cells get stacked on after the other in the RNN,\n    implementing an efficient stacked RNN.\n\n- __return_sequences__: Boolean. Whether to return the last output\n    in the output sequence, or the full sequence.\n- __return_state__: Boolean. Whether to return the last state\n    in addition to the output.\n- __go_backwards__: Boolean (default False).\n    If True, process the input sequence backwards and return the\n    reversed sequence.\n- __stateful__: Boolean (default False). If True, the last state\n    for each sample at index i in a batch will be used as initial\n    state for the sample of index i in the following batch.\n- __unroll__: Boolean (default False).\n    If True, the network will be unrolled,\n    else a symbolic loop will be used.\n    Unrolling can speed-up a RNN,\n    although it tends to be more memory-intensive.\n    Unrolling is only suitable for short sequences.\n- __input_dim__: dimensionality of the input (integer).\n    This argument (or alternatively,\n    the keyword argument `input_shape`)\n    is required when using this layer as the first layer in a model.\n- __input_length__: Length of input sequences, to be specified\n    when it is constant.\n    This argument is required if you are going to connect\n    `Flatten` then `Dense` layers upstream\n    (without it, the shape of the dense outputs cannot be computed).\n    Note that if the recurrent layer is not the first layer\n    in your model, you would need to specify the input length\n    at the level of the first layer\n    (e.g. via the `input_shape` argument)\n\n__Input shape__\n\n3D tensor with shape `(batch_size, timesteps, input_dim)`.\n\n__Output shape__\n\n- if `return_state`: a list of tensors. The first tensor is\n    the output. The remaining tensors are the last states,\n    each with shape `(batch_size, units)`.\n- if `return_sequences`: 3D tensor with shape\n    `(batch_size, timesteps, units)`.\n- else, 2D tensor with shape `(batch_size, units)`.\n\n__Masking__\n\nThis layer supports masking for input data with a variable number\nof timesteps. To introduce masks to your data,\nuse an [Embedding](embeddings.md) layer with the `mask_zero` parameter\nset to `True`.\n\n__Note on using statefulness in RNNs__\n\nYou can set RNN layers to be \'stateful\', which means that the states\ncomputed for the samples in one batch will be reused as initial states\nfor the samples in the next batch. This assumes a one-to-one mapping\nbetween samples in different successive batches.\n\nTo enable statefulness:\n- specify `stateful=True` in the layer constructor.\n- specify a fixed batch size for your model, by passing\nif sequential model:\n`batch_input_shape=(...)` to the first layer in your model.\nelse for functional model with 1 or more Input layers:\n`batch_shape=(...)` to all the first layers in your model.\nThis is the expected shape of your inputs\n*including the batch size*.\nIt should be a tuple of integers, e.g. `(32, 10, 100)`.\n- specify `shuffle=False` when calling fit().\n\nTo reset the states of your model, call `.reset_states()` on either\na specific layer, or on your entire model.\n\n__Note on specifying the initial state of RNNs__\n\nNote: that\n- __One__: You can specify the initial state of RNN layers symbolically by\n    calling them with the keyword argument `initial_state`.\n- __Two__: The value of `initial_state` should be a tensor or list of\n    tensors representing\n    the initial state of the RNN layer.\n\nYou can specify the initial state of RNN layers numerically by:\n\n- __One__: calling `reset_states`\n    - With the keyword argument `states`.\n        - The value of\n\n    `states` should be a numpy array or\n    list of numpy arrays representing\n\nthe initial state of the RNN layer.\n\n__Note on passing external constants to RNNs__\n\nYou can pass ""external"" constants to the cell using the `constants`\n- __keyword__: argument of `RNN.__call__` (as well as `RNN.call`) method.\n- __This__: requires that the `cell.call` method accepts the same keyword argument\n\n`constants`. Such constants can be used to condition the cell\ntransformation on additional static inputs (not changing over time),\na.k.a. an attention mechanism.\n\n__Examples__\n\n\n```python\n# First, let\'s define a RNN Cell, as a layer subclass.\n\nclass MinimalRNNCell(keras.layers.Layer):\n\n    def __init__(self, units, **kwargs):\n        self.units = units\n        self.state_size = units\n        super(MinimalRNNCell, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n                                      initializer=\'uniform\',\n                                      name=\'kernel\')\n        self.recurrent_kernel = self.add_weight(\n            shape=(self.units, self.units),\n            initializer=\'uniform\',\n            name=\'recurrent_kernel\')\n        self.built = True\n\n    def call(self, inputs, states):\n        prev_output = states[0]\n        h = K.dot(inputs, self.kernel)\n        output = h + K.dot(prev_output, self.recurrent_kernel)\n        return output, [output]\n\n# Let\'s use this cell in a RNN layer:\n\ncell = MinimalRNNCell(32)\nx = keras.Input((None, 5))\nlayer = RNN(cell)\ny = layer(x)\n\n# Here\'s how to use the cell to build a stacked RNN:\n\ncells = [MinimalRNNCell(32), MinimalRNNCell(64)]\nx = keras.Input((None, 5))\nlayer = RNN(cells)\ny = layer(x)\n```\n\'\'\'}\n\n\ndef test_doc_lists():\n    docstring = autogen.process_docstring(test_doc1[\'doc\'])\n    assert markdown(docstring) == markdown(test_doc1[\'result\'])\n\n\ndummy_docstring = """"""Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n\n    When attempting to multiply a nD tensor\n    with a nD tensor, it reproduces the Theano behavior.\n    (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`)\n\n    # Examples\n    ```python\n        # Theano-like behavior example\n        >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)\n        >>> y = K.ones((4, 3, 5))\n        >>> xy = K.dot(x, y)\n        >>> K.int_shape(xy)\n        (2, 4, 5)\n    ```\n\n    # Numpy implementation\n    ```python\n        def dot(x, y):\n            return dot(x, y)\n    ```\n    """"""\n\n\ndef test_doc_multiple_sections_code():\n    """""" Checks that we can have code blocks in multiple sections.""""""\n    generated = autogen.process_docstring(dummy_docstring)\n    assert \'# Theano-like behavior example\' in generated\n    assert \'def dot(x, y):\' in generated\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/tooling/test_documentation.py,0,"b'import importlib\nimport inspect\nimport re\nimport sys\nfrom itertools import compress\n\nimport pytest\n\nmodules = [\'keras_contrib.layers\',\n           \'keras_contrib\',\n           \'keras_contrib.backend.tensorflow_backend\',\n           \'keras_contrib.wrappers\',\n           \'keras_contrib.utils\',\n           \'keras_contrib.callbacks\',\n           \'keras_contrib.activations\',\n           \'keras_contrib.losses\',\n           \'keras_contrib.optimizers\']\naccepted_name = [\'from_config\']\naccepted_module = []\n\n# Functions or classes with less than \'MIN_CODE_SIZE\' lines can be ignored\nMIN_CODE_SIZE = 10\n\n\ndef handle_class(name, member):\n    if is_accepted(name, member):\n        return\n\n    if member.__doc__ is None and not member_too_small(member):\n        raise ValueError(""{} class doesn\'t have any documentation"".format(name),\n                         member.__module__, inspect.getmodule(member).__file__)\n    for n, met in inspect.getmembers(member):\n        if inspect.ismethod(met):\n            handle_method(n, met)\n\n\ndef handle_function(name, member):\n    if is_accepted(name, member) or member_too_small(member):\n        # We don\'t need to check this one.\n        return\n    doc = member.__doc__\n    if doc is None:\n        raise ValueError(""{} function doesn\'t have any documentation"".format(name),\n                         member.__module__, inspect.getmodule(member).__file__)\n\n    args = list(inspect.signature(member).parameters.keys())\n    assert_args_presence(args, doc, member, name)\n    assert_function_style(name, member, doc, args)\n    assert_doc_style(name, member, doc)\n\n\ndef assert_doc_style(name, member, doc):\n    lines = doc.split(""\\n"")\n    first_line = lines[0]\n    if len(first_line.strip()) == 0:\n        raise ValueError(\n            ""{} the documentation should be on the first line."".format(name),\n            member.__module__)\n    if first_line.strip()[-1] != \'.\':\n        raise ValueError(""{} first line should end with a \'.\'"".format(name),\n                         member.__module__)\n\n\ndef assert_function_style(name, member, doc, args):\n    code = inspect.getsource(member)\n    has_return = re.findall(r""\\s*return \\S+"", code, re.MULTILINE)\n    if has_return and ""# Returns"" not in doc:\n        innerfunction = [inspect.getsource(x) for x in member.__code__.co_consts if\n                         inspect.iscode(x)]\n        return_in_sub = [ret for code_inner in innerfunction for ret in\n                         re.findall(r""\\s*return \\S+"", code_inner, re.MULTILINE)]\n        if len(return_in_sub) < len(has_return):\n            raise ValueError(""{} needs a \'# Returns\' section"".format(name),\n                             member.__module__)\n\n    has_raise = re.findall(r""^\\s*raise \\S+"", code, re.MULTILINE)\n    if has_raise and ""# Raises"" not in doc:\n        innerfunction = [inspect.getsource(x) for x in member.__code__.co_consts if\n                         inspect.iscode(x)]\n        raise_in_sub = [ret for code_inner in innerfunction for ret in\n                        re.findall(r""\\s*raise \\S+"", code_inner, re.MULTILINE)]\n        if len(raise_in_sub) < len(has_raise):\n            raise ValueError(""{} needs a \'# Raises\' section"".format(name),\n                             member.__module__)\n\n    if len(args) > 0 and ""# Arguments"" not in doc:\n        raise ValueError(""{} needs a \'# Arguments\' section"".format(name),\n                         member.__module__)\n\n    assert_blank_before(name, member, doc, [\'# Arguments\', \'# Raises\', \'# Returns\'])\n\n\ndef assert_blank_before(name, member, doc, keywords):\n    doc_lines = [x.strip() for x in doc.split(\'\\n\')]\n    for keyword in keywords:\n        if keyword in doc_lines:\n            index = doc_lines.index(keyword)\n            if doc_lines[index - 1] != \'\':\n                raise ValueError(\n                    ""{} \'{}\' should have a blank line above."".format(name, keyword),\n                    member.__module__)\n\n\ndef is_accepted(name, member):\n    if \'keras\' not in str(member.__module__):\n        return True\n    return name in accepted_name or member.__module__ in accepted_module\n\n\ndef member_too_small(member):\n    code = inspect.getsource(member).split(\'\\n\')\n    return len(code) < MIN_CODE_SIZE\n\n\ndef assert_args_presence(args, doc, member, name):\n    args_not_in_doc = [arg not in doc for arg in args]\n    if any(args_not_in_doc):\n        raise ValueError(\n            ""{} {} arguments are not present in documentation "".format(name, list(\n                compress(args, args_not_in_doc))), member.__module__)\n    words = doc.replace(\'*\', \'\').split()\n    # Check arguments styling\n    styles = [arg + "":"" not in words for arg in args]\n    if any(styles):\n        raise ValueError(\n            ""{} {} are not style properly \'argument\': documentation"".format(\n                name,\n                list(compress(args, styles))),\n            member.__module__)\n\n    # Check arguments order\n    indexes = [words.index(arg + "":"") for arg in args]\n    if indexes != sorted(indexes):\n        raise ValueError(\n            ""{} arguments order is different from the documentation"".format(name),\n            member.__module__)\n\n\ndef handle_method(name, member):\n    if name in accepted_name or member.__module__ in accepted_module:\n        return\n    handle_function(name, member)\n\n\ndef handle_module(mod):\n    for name, mem in inspect.getmembers(mod):\n        if inspect.isclass(mem):\n            handle_class(name, mem)\n        elif inspect.isfunction(mem):\n            handle_function(name, mem)\n        elif \'keras\' in name and inspect.ismodule(mem):\n            # Only test keras\' modules\n            handle_module(mem)\n\n\n@pytest.mark.skipif(sys.version_info < (3, 3), reason=""requires python3.3"")\n@pytest.mark.parametrize(\'module\', modules)\ndef test_doc(module):\n    mod = importlib.import_module(module)\n    handle_module(mod)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
keras_contrib/layers/advanced_activations/__init__.py,0,b''
keras_contrib/layers/advanced_activations/pelu.py,0,"b'from keras.layers import Layer, InputSpec\r\nfrom keras import initializers, regularizers, constraints\r\nimport keras.backend as K\r\nfrom keras_contrib.utils.test_utils import to_tuple\r\n\r\n\r\nclass PELU(Layer):\r\n    """"""Parametric Exponential Linear Unit.\r\n\r\n    It follows:\r\n    `f(x) = alphas * (exp(x / betas) - 1) for x < 0`,\r\n    `f(x) = (alphas / betas) * x for x >= 0`,\r\n    where `alphas` & `betas` are learned arrays with the same shape as x.\r\n\r\n    # Input shape\r\n        Arbitrary. Use the keyword argument `input_shape`\r\n        (tuple of integers, does not include the samples axis)\r\n        when using this layer as the first layer in a model.\r\n\r\n    # Output shape\r\n        Same shape as the input.\r\n\r\n    # Arguments\r\n        alphas_initializer: initialization function for the alpha variable weights.\r\n        betas_initializer: initialization function for the beta variable weights.\r\n        weights: initial weights, as a list of a single Numpy array.\r\n        shared_axes: the axes along which to share learnable\r\n            parameters for the activation function.\r\n            For example, if the incoming feature maps\r\n            are from a 2D convolution\r\n            with output shape `(batch, height, width, channels)`,\r\n            and you wish to share parameters across space\r\n            so that each filter only has one set of parameters,\r\n            set `shared_axes=[1, 2]`.\r\n\r\n    # References\r\n        - [Parametric exponential linear unit for deep convolutional neural networks](\r\n           https://arxiv.org/abs/1605.09332v3)\r\n    """"""\r\n\r\n    def __init__(self, alpha_initializer=\'ones\',\r\n                 alpha_regularizer=None,\r\n                 alpha_constraint=None,\r\n                 beta_initializer=\'ones\',\r\n                 beta_regularizer=None,\r\n                 beta_constraint=None,\r\n                 shared_axes=None,\r\n                 **kwargs):\r\n        super(PELU, self).__init__(**kwargs)\r\n        self.supports_masking = True\r\n        self.alpha_initializer = initializers.get(alpha_initializer)\r\n        self.alpha_regularizer = regularizers.get(alpha_regularizer)\r\n        self.alpha_constraint = constraints.get(alpha_constraint)\r\n        self.beta_initializer = initializers.get(beta_initializer)\r\n        self.beta_regularizer = regularizers.get(beta_regularizer)\r\n        self.beta_constraint = constraints.get(beta_constraint)\r\n        if shared_axes is None:\r\n            self.shared_axes = None\r\n        elif not isinstance(shared_axes, (list, tuple)):\r\n            self.shared_axes = [shared_axes]\r\n        else:\r\n            self.shared_axes = list(shared_axes)\r\n\r\n    def build(self, input_shape):\r\n        input_shape = to_tuple(input_shape)\r\n        param_shape = list(input_shape[1:])\r\n        self.param_broadcast = [False] * len(param_shape)\r\n        if self.shared_axes is not None:\r\n            for i in self.shared_axes:\r\n                param_shape[i - 1] = 1\r\n                self.param_broadcast[i - 1] = True\r\n\r\n        param_shape = tuple(param_shape)\r\n        # Initialised as ones to emulate the default ELU\r\n        self.alpha = self.add_weight(shape=param_shape,\r\n                                     name=\'alpha\',\r\n                                     initializer=self.alpha_initializer,\r\n                                     regularizer=self.alpha_regularizer,\r\n                                     constraint=self.alpha_constraint)\r\n        self.beta = self.add_weight(shape=param_shape,\r\n                                    name=\'beta\',\r\n                                    initializer=self.beta_initializer,\r\n                                    regularizer=self.beta_regularizer,\r\n                                    constraint=self.beta_constraint)\r\n\r\n        # Set input spec\r\n        axes = {}\r\n        if self.shared_axes:\r\n            for i in range(1, len(input_shape)):\r\n                if i not in self.shared_axes:\r\n                    axes[i] = input_shape[i]\r\n        self.input_spec = InputSpec(ndim=len(input_shape), axes=axes)\r\n        self.built = True\r\n\r\n    def call(self, x, mask=None):\r\n        if K.backend() == \'theano\':\r\n            pos = K.relu(x) * (K.pattern_broadcast(self.alpha, self.param_broadcast) /\r\n                               K.pattern_broadcast(self.beta, self.param_broadcast))\r\n            neg = (K.pattern_broadcast(self.alpha, self.param_broadcast) *\r\n                   (K.exp((-K.relu(-x))\r\n                          / K.pattern_broadcast(self.beta, self.param_broadcast)) - 1))\r\n        else:\r\n            pos = K.relu(x) * self.alpha / self.beta\r\n            neg = self.alpha * (K.exp((-K.relu(-x)) / self.beta) - 1)\r\n        return neg + pos\r\n\r\n    def get_config(self):\r\n        config = {\r\n            \'alpha_initializer\': initializers.serialize(self.alpha_initializer),\r\n            \'alpha_regularizer\': regularizers.serialize(self.alpha_regularizer),\r\n            \'alpha_constraint\': constraints.serialize(self.alpha_constraint),\r\n            \'beta_initializer\': initializers.serialize(self.beta_initializer),\r\n            \'beta_regularizer\': regularizers.serialize(self.beta_regularizer),\r\n            \'beta_constraint\': constraints.serialize(self.beta_constraint),\r\n            \'shared_axes\': self.shared_axes\r\n        }\r\n        base_config = super(PELU, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape\r\n'"
keras_contrib/layers/advanced_activations/sinerelu.py,0,"b'import keras.backend as K\r\nfrom keras.layers import Layer\r\n\r\n\r\nclass SineReLU(Layer):\r\n    """"""Sine Rectified Linear Unit to generate oscilations.\r\n\r\n    It allows an oscilation in the gradients when the weights are negative.\r\n    The oscilation can be controlled with a parameter, which makes it be close\r\n    or equal to zero. The functional is diferentiable at any point due to\r\n    its derivative.\r\n    For instance, at 0, the derivative of \'sin(0) - cos(0)\'\r\n    is \'cos(0) + sin(0)\' which is 1.\r\n\r\n    # Input shape\r\n        Arbitrary. Use the keyword argument `input_shape`\r\n        (tuple of integers, does not include the samples axis)\r\n        when using this layer as the first layer in a model.\r\n\r\n    # Output shape\r\n        Same shape as the input.\r\n\r\n    # Arguments\r\n        epsilon: float. Hyper-parameter used to control the amplitude of the\r\n            sinusoidal wave when weights are negative.\r\n            The default value, 0.0025, since it works better for CNN layers and\r\n            those are the most used layers nowadays.\r\n            When using Dense Networks, try something around 0.006.\r\n\r\n    # References:\r\n        - [SineReLU: An Alternative to the ReLU Activation Function](\r\n           https://medium.com/@wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d).\r\n\r\n        This function was\r\n        first introduced at the Codemotion Amsterdam 2018 and then at\r\n        the DevDays, in Vilnius, Lithuania.\r\n        It has been extensively tested with Deep Nets, CNNs,\r\n        LSTMs, Residual Nets and GANs, based\r\n        on the MNIST, Kaggle Toxicity and IMDB datasets.\r\n\r\n    # Performance:\r\n\r\n        - Fashion MNIST\r\n          * Mean of 6 runs per Activation Function\r\n            * Fully Connection Network\r\n              - SineReLU: loss mean -> 0.3522; accuracy mean -> 89.18;\r\n                  mean of std loss -> 0.08375204467435822\r\n              - LeakyReLU: loss mean-> 0.3553; accuracy mean -> 88.98;\r\n              mean of std loss -> 0.0831161868455245\r\n              - ReLU: loss mean -> 0.3519; accuracy mean -> 88.84;\r\n              mean of std loss -> 0.08358816501301362\r\n            * Convolutional Neural Network\r\n              - SineReLU: loss mean -> 0.2180; accuracy mean -> 92.49;\r\n              mean of std loss -> 0.0781155784858847\r\n              - LeakyReLU: loss mean -> 0.2205; accuracy mean -> 92.37;\r\n              mean of std loss -> 0.09273670474788205\r\n              - ReLU: loss mean -> 0.2144; accuracy mean -> 92.45;\r\n              mean of std loss -> 0.09396114585977\r\n        - MNIST\r\n          * Mean of 6 runs per Activation Function\r\n            * Fully Connection Network\r\n              - SineReLU: loss mean -> 0.0623; accuracy mean -> 98.53;\r\n              mean of std loss -> 0.06012015231824904\r\n              - LeakyReLU: loss mean-> 0.0623; accuracy mean -> 98.50;\r\n              mean of std loss -> 0.06052147632835356\r\n              - ReLU: loss mean -> 0.0605; accuracy mean -> 98.49;\r\n              mean of std loss -> 0.059599885665016096\r\n            * Convolutional Neural Network\r\n              - SineReLU: loss mean -> 0.0198; accuracy mean -> 99.51;\r\n              mean of std loss -> 0.0425338329550847\r\n              - LeakyReLU: loss mean -> 0.0216; accuracy mean -> 99.40;\r\n              mean of std loss -> 0.04834468835196667\r\n              - ReLU: loss mean -> 0.0185; accuracy mean -> 99.49;\r\n              mean of std loss -> 0.05503719489690131\r\n\r\n    # Jupyter Notebooks\r\n        - https://github.com/ekholabs/DLinK/blob/master/notebooks/keras\r\n\r\n    # Examples\r\n        The Advanced Activation function SineReLU have to be imported from the\r\n        keras_contrib.layers package.\r\n\r\n        To see full source-code of this architecture and other examples,\r\n        please follow this link: https://github.com/ekholabs/DLinK\r\n\r\n        ```python\r\n            model = Sequential()\r\n            model.add(Dense(128, input_shape = (784,)))\r\n            model.add(SineReLU())\r\n            model.add(Dropout(0.2))\r\n\r\n            model.add(Dense(256))\r\n            model.add(SineReLU())\r\n            model.add(Dropout(0.3))\r\n\r\n            model.add(Dense(1024))\r\n            model.add(SineReLU())\r\n            model.add(Dropout(0.5))\r\n\r\n            model.add(Dense(10, activation = \'softmax\'))\r\n        ```\r\n    """"""\r\n\r\n    def __init__(self, epsilon=0.0025, **kwargs):\r\n        super(SineReLU, self).__init__(**kwargs)\r\n        self.supports_masking = True\r\n        self.epsilon = K.cast_to_floatx(epsilon)\r\n\r\n    def call(self, Z):\r\n        m = self.epsilon * (K.sin(Z) - K.cos(Z))\r\n        A = K.maximum(m, Z)\r\n        return A\r\n\r\n    def get_config(self):\r\n        config = {\'epsilon\': float(self.epsilon)}\r\n        base_config = super(SineReLU, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape\r\n'"
keras_contrib/layers/advanced_activations/srelu.py,0,"b'from keras.layers import Layer, InputSpec\r\nfrom keras import initializers\r\nimport keras.backend as K\r\nfrom keras_contrib.utils.test_utils import to_tuple\r\n\r\n\r\nclass SReLU(Layer):\r\n    """"""S-shaped Rectified Linear Unit.\r\n\r\n    It follows:\r\n    `f(x) = t^r + a^r(x - t^r) for x >= t^r`,\r\n    `f(x) = x for t^r > x > t^l`,\r\n    `f(x) = t^l + a^l(x - t^l) for x <= t^l`.\r\n\r\n    # Input shape\r\n        Arbitrary. Use the keyword argument `input_shape`\r\n        (tuple of integers, does not include the samples axis)\r\n        when using this layer as the first layer in a model.\r\n\r\n    # Output shape\r\n        Same shape as the input.\r\n\r\n    # Arguments\r\n        t_left_initializer: initializer function for the left part intercept\r\n        a_left_initializer: initializer function for the left part slope\r\n        t_right_initializer: initializer function for the right part intercept\r\n        a_right_initializer: initializer function for the right part slope\r\n        shared_axes: the axes along which to share learnable\r\n            parameters for the activation function.\r\n            For example, if the incoming feature maps\r\n            are from a 2D convolution\r\n            with output shape `(batch, height, width, channels)`,\r\n            and you wish to share parameters across space\r\n            so that each filter only has one set of parameters,\r\n            set `shared_axes=[1, 2]`.\r\n\r\n    # References\r\n        - [Deep Learning with S-shaped Rectified Linear Activation Units](\r\n           http://arxiv.org/abs/1512.07030)\r\n    """"""\r\n\r\n    def __init__(self, t_left_initializer=\'zeros\',\r\n                 a_left_initializer=initializers.RandomUniform(minval=0, maxval=1),\r\n                 t_right_initializer=initializers.RandomUniform(minval=0, maxval=5),\r\n                 a_right_initializer=\'ones\',\r\n                 shared_axes=None,\r\n                 **kwargs):\r\n        super(SReLU, self).__init__(**kwargs)\r\n        self.supports_masking = True\r\n        self.t_left_initializer = initializers.get(t_left_initializer)\r\n        self.a_left_initializer = initializers.get(a_left_initializer)\r\n        self.t_right_initializer = initializers.get(t_right_initializer)\r\n        self.a_right_initializer = initializers.get(a_right_initializer)\r\n        if shared_axes is None:\r\n            self.shared_axes = None\r\n        elif not isinstance(shared_axes, (list, tuple)):\r\n            self.shared_axes = [shared_axes]\r\n        else:\r\n            self.shared_axes = list(shared_axes)\r\n\r\n    def build(self, input_shape):\r\n        input_shape = to_tuple(input_shape)\r\n        param_shape = list(input_shape[1:])\r\n        self.param_broadcast = [False] * len(param_shape)\r\n        if self.shared_axes is not None:\r\n            for i in self.shared_axes:\r\n                param_shape[i - 1] = 1\r\n                self.param_broadcast[i - 1] = True\r\n\r\n        param_shape = tuple(param_shape)\r\n\r\n        self.t_left = self.add_weight(shape=param_shape,\r\n                                      name=\'t_left\',\r\n                                      initializer=self.t_left_initializer)\r\n\r\n        self.a_left = self.add_weight(shape=param_shape,\r\n                                      name=\'a_left\',\r\n                                      initializer=self.a_left_initializer)\r\n\r\n        self.t_right = self.add_weight(shape=param_shape,\r\n                                       name=\'t_right\',\r\n                                       initializer=self.t_right_initializer)\r\n\r\n        self.a_right = self.add_weight(shape=param_shape,\r\n                                       name=\'a_right\',\r\n                                       initializer=self.a_right_initializer)\r\n\r\n        # Set input spec\r\n        axes = {}\r\n        if self.shared_axes:\r\n            for i in range(1, len(input_shape)):\r\n                if i not in self.shared_axes:\r\n                    axes[i] = input_shape[i]\r\n        self.input_spec = InputSpec(ndim=len(input_shape), axes=axes)\r\n        self.built = True\r\n\r\n    def call(self, x, mask=None):\r\n        # ensure the the right part is always to the right of the left\r\n        t_right_actual = self.t_left + K.abs(self.t_right)\r\n\r\n        if K.backend() == \'theano\':\r\n            t_left = K.pattern_broadcast(self.t_left, self.param_broadcast)\r\n            a_left = K.pattern_broadcast(self.a_left, self.param_broadcast)\r\n            a_right = K.pattern_broadcast(self.a_right, self.param_broadcast)\r\n            t_right_actual = K.pattern_broadcast(t_right_actual,\r\n                                                 self.param_broadcast)\r\n        else:\r\n            t_left = self.t_left\r\n            a_left = self.a_left\r\n            a_right = self.a_right\r\n\r\n        y_left_and_center = t_left + K.relu(x - t_left,\r\n                                            a_left,\r\n                                            t_right_actual - t_left)\r\n        y_right = K.relu(x - t_right_actual) * a_right\r\n        return y_left_and_center + y_right\r\n\r\n    def get_config(self):\r\n        config = {\r\n            \'t_left_initializer\': self.t_left_initializer,\r\n            \'a_left_initializer\': self.a_left_initializer,\r\n            \'t_right_initializer\': self.t_right_initializer,\r\n            \'a_right_initializer\': self.a_right_initializer,\r\n            \'shared_axes\': self.shared_axes\r\n        }\r\n        base_config = super(SReLU, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape\r\n'"
keras_contrib/layers/advanced_activations/swish.py,0,"b'from keras import backend as K\r\nfrom keras.layers import Layer\r\n\r\n\r\nclass Swish(Layer):\r\n    """""" Swish (Ramachandranet al., 2017)\r\n\r\n    # Input shape\r\n        Arbitrary. Use the keyword argument `input_shape`\r\n        (tuple of integers, does not include the samples axis)\r\n        when using this layer as the first layer in a model.\r\n\r\n    # Output shape\r\n        Same shape as the input.\r\n\r\n    # Arguments\r\n        beta: float >= 0. Scaling factor\r\n            if set to 1 and trainable set to False (default),\r\n            Swish equals the SiLU activation (Elfwing et al., 2017)\r\n        trainable: whether to learn the scaling factor during training or not\r\n\r\n    # References\r\n        - [Searching for Activation Functions](https://arxiv.org/abs/1710.05941)\r\n        - [Sigmoid-weighted linear units for neural network function\r\n           approximation in reinforcement learning](https://arxiv.org/abs/1702.03118)\r\n    """"""\r\n\r\n    def __init__(self, beta=1.0, trainable=False, **kwargs):\r\n        super(Swish, self).__init__(**kwargs)\r\n        self.supports_masking = True\r\n        self.beta = beta\r\n        self.trainable = trainable\r\n\r\n    def build(self, input_shape):\r\n        self.scaling_factor = K.variable(self.beta,\r\n                                         dtype=K.floatx(),\r\n                                         name=\'scaling_factor\')\r\n        if self.trainable:\r\n            self._trainable_weights.append(self.scaling_factor)\r\n        super(Swish, self).build(input_shape)\r\n\r\n    def call(self, inputs, mask=None):\r\n        return inputs * K.sigmoid(self.scaling_factor * inputs)\r\n\r\n    def get_config(self):\r\n        config = {\'beta\': self.get_weights()[0] if self.trainable else self.beta,\r\n                  \'trainable\': self.trainable}\r\n        base_config = super(Swish, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape\r\n'"
keras_contrib/layers/convolutional/__init__.py,0,b''
keras_contrib/layers/convolutional/cosineconvolution2d.py,0,"b'# -*- coding: utf-8 -*-\r\nfrom __future__ import absolute_import\r\nfrom functools import partial\r\n\r\nfrom keras import backend as K\r\nfrom keras_contrib import backend as KC\r\nfrom keras import activations\r\nfrom keras import initializers\r\nfrom keras import regularizers\r\nfrom keras import constraints\r\nfrom keras.layers import Layer\r\nfrom keras.layers import InputSpec\r\nfrom keras_contrib.utils.conv_utils import conv_output_length\r\nfrom keras_contrib.utils.conv_utils import normalize_data_format\r\nfrom keras_contrib.utils.test_utils import to_tuple\r\nimport numpy as np\r\n\r\n\r\nclass CosineConvolution2D(Layer):\r\n    """"""Cosine Normalized Convolution operator for filtering\r\n    windows of two-dimensional inputs.\r\n\r\n    # Examples\r\n\r\n    ```python\r\n        # apply a 3x3 convolution with 64 output filters on a 256x256 image:\r\n        model = Sequential()\r\n        model.add(CosineConvolution2D(64, 3, 3,\r\n                                padding=\'same\',\r\n                                input_shape=(3, 256, 256)))\r\n        # now model.output_shape == (None, 64, 256, 256)\r\n\r\n        # add a 3x3 convolution on top, with 32 output filters:\r\n        model.add(CosineConvolution2D(32, 3, 3, padding=\'same\'))\r\n        # now model.output_shape == (None, 32, 256, 256)\r\n    ```\r\n\r\n    # Arguments\r\n        filters: Number of convolution filters to use.\r\n        kernel_size: kernel_size: An integer or tuple/list of\r\n            2 integers, specifying the\r\n            dimensions of the convolution window.\r\n        init: name of initialization function for the weights of the layer\r\n            (see [initializers](https://keras.io/initializers)), or alternatively,\r\n            Theano function to use for weights initialization.\r\n            This parameter is only relevant if you don\'t pass\r\n            a `weights` argument.\r\n        activation: name of activation function to use\r\n            (see [activations](https://keras.io/activations)),\r\n            or alternatively, elementwise Theano function.\r\n            If you don\'t specify anything, no activation is applied\r\n            (ie. ""linear"" activation: a(x) = x).\r\n        weights: list of numpy arrays to set as initial weights.\r\n        padding: \'valid\', \'same\' or \'full\'\r\n            (\'full\' requires the Theano backend).\r\n        strides: tuple of length 2. Factor by which to strides output.\r\n            Also called strides elsewhere.\r\n        kernel_regularizer: instance of [WeightRegularizer](\r\n            https://keras.io/regularizers)\r\n            (eg. L1 or L2 regularization), applied to the main weights matrix.\r\n        bias_regularizer: instance of [WeightRegularizer](\r\n            https://keras.io/regularizers), applied to the use_bias.\r\n        activity_regularizer: instance of [ActivityRegularizer](\r\n            https://keras.io/regularizers), applied to the network output.\r\n        kernel_constraint: instance of the [constraints](\r\n            https://keras.io/constraints) module\r\n            (eg. maxnorm, nonneg), applied to the main weights matrix.\r\n        bias_constraint: instance of the [constraints](\r\n            https://keras.io/constraints) module, applied to the use_bias.\r\n        data_format: \'channels_first\' or \'channels_last\'.\r\n            In \'channels_first\' mode, the channels dimension\r\n            (the depth) is at index 1, in \'channels_last\' mode is it at index 3.\r\n            It defaults to the `image_data_format` value found in your\r\n            Keras config file at `~/.keras/keras.json`.\r\n            If you never set it, then it will be `\'channels_last\'`.\r\n        use_bias: whether to include a use_bias\r\n            (i.e. make the layer affine rather than linear).\r\n\r\n    # Input shape\r\n        4D tensor with shape:\r\n        `(samples, channels, rows, cols)` if data_format=\'channels_first\'\r\n        or 4D tensor with shape:\r\n        `(samples, rows, cols, channels)` if data_format=\'channels_last\'.\r\n\r\n    # Output shape\r\n        4D tensor with shape:\r\n        `(samples, filters, nekernel_rows, nekernel_cols)`\r\n        if data_format=\'channels_first\'\r\n        or 4D tensor with shape:\r\n        `(samples, nekernel_rows, nekernel_cols, filters)`\r\n        if data_format=\'channels_last\'.\r\n        `rows` and `cols` values might have changed due to padding.\r\n\r\n\r\n    # References\r\n        - [Cosine Normalization: Using Cosine Similarity Instead\r\n           of Dot Product in Neural Networks](https://arxiv.org/pdf/1702.05870.pdf)\r\n    """"""\r\n\r\n    def __init__(self, filters, kernel_size,\r\n                 kernel_initializer=\'glorot_uniform\', activation=None, weights=None,\r\n                 padding=\'valid\', strides=(1, 1), data_format=None,\r\n                 kernel_regularizer=None, bias_regularizer=None,\r\n                 activity_regularizer=None,\r\n                 kernel_constraint=None, bias_constraint=None,\r\n                 use_bias=True, **kwargs):\r\n        if data_format is None:\r\n            data_format = K.image_data_format()\r\n        if padding not in {\'valid\', \'same\', \'full\'}:\r\n            raise ValueError(\'Invalid border mode for CosineConvolution2D:\', padding)\r\n        self.filters = filters\r\n        self.kernel_size = kernel_size\r\n        self.nb_row, self.nb_col = self.kernel_size\r\n        self.kernel_initializer = initializers.get(kernel_initializer)\r\n        self.activation = activations.get(activation)\r\n        self.padding = padding\r\n        self.strides = tuple(strides)\r\n        self.data_format = normalize_data_format(data_format)\r\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\r\n        self.bias_regularizer = regularizers.get(bias_regularizer)\r\n        self.activity_regularizer = regularizers.get(activity_regularizer)\r\n\r\n        self.kernel_constraint = constraints.get(kernel_constraint)\r\n        self.bias_constraint = constraints.get(bias_constraint)\r\n\r\n        self.use_bias = use_bias\r\n        self.input_spec = [InputSpec(ndim=4)]\r\n        self.initial_weights = weights\r\n        super(CosineConvolution2D, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        input_shape = to_tuple(input_shape)\r\n        if self.data_format == \'channels_first\':\r\n            stack_size = input_shape[1]\r\n            self.kernel_shape = (self.filters, stack_size, self.nb_row, self.nb_col)\r\n            self.kernel_norm_shape = (1, stack_size, self.nb_row, self.nb_col)\r\n        elif self.data_format == \'channels_last\':\r\n            stack_size = input_shape[3]\r\n            self.kernel_shape = (self.nb_row, self.nb_col, stack_size, self.filters)\r\n            self.kernel_norm_shape = (self.nb_row, self.nb_col, stack_size, 1)\r\n        else:\r\n            raise ValueError(\'Invalid data_format:\', self.data_format)\r\n        self.W = self.add_weight(shape=self.kernel_shape,\r\n                                 initializer=partial(self.kernel_initializer),\r\n                                 name=\'{}_W\'.format(self.name),\r\n                                 regularizer=self.kernel_regularizer,\r\n                                 constraint=self.kernel_constraint)\r\n\r\n        kernel_norm_name = \'{}_kernel_norm\'.format(self.name)\r\n        self.kernel_norm = K.variable(np.ones(self.kernel_norm_shape),\r\n                                      name=kernel_norm_name)\r\n\r\n        if self.use_bias:\r\n            self.b = self.add_weight(shape=(self.filters,),\r\n                                     initializer=\'zero\',\r\n                                     name=\'{}_b\'.format(self.name),\r\n                                     regularizer=self.bias_regularizer,\r\n                                     constraint=self.bias_constraint)\r\n        else:\r\n            self.b = None\r\n\r\n        if self.initial_weights is not None:\r\n            self.set_weights(self.initial_weights)\r\n            del self.initial_weights\r\n        self.built = True\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        if self.data_format == \'channels_first\':\r\n            rows = input_shape[2]\r\n            cols = input_shape[3]\r\n        elif self.data_format == \'channels_last\':\r\n            rows = input_shape[1]\r\n            cols = input_shape[2]\r\n        else:\r\n            raise ValueError(\'Invalid data_format:\', self.data_format)\r\n\r\n        rows = conv_output_length(rows, self.nb_row,\r\n                                  self.padding, self.strides[0])\r\n        cols = conv_output_length(cols, self.nb_col,\r\n                                  self.padding, self.strides[1])\r\n\r\n        if self.data_format == \'channels_first\':\r\n            return input_shape[0], self.filters, rows, cols\r\n        elif self.data_format == \'channels_last\':\r\n            return input_shape[0], rows, cols, self.filters\r\n\r\n    def call(self, x, mask=None):\r\n        b, xb = 0., 0.\r\n        if self.data_format == \'channels_first\':\r\n            kernel_sum_axes = [1, 2, 3]\r\n            if self.use_bias:\r\n                b = K.reshape(self.b, (self.filters, 1, 1, 1))\r\n                xb = 1.\r\n        elif self.data_format == \'channels_last\':\r\n            kernel_sum_axes = [0, 1, 2]\r\n            if self.use_bias:\r\n                b = K.reshape(self.b, (1, 1, 1, self.filters))\r\n                xb = 1.\r\n\r\n        tmp = K.sum(K.square(self.W), axis=kernel_sum_axes, keepdims=True)\r\n        Wnorm = K.sqrt(tmp + K.square(b) + K.epsilon())\r\n\r\n        tmp = KC.conv2d(K.square(x), self.kernel_norm, strides=self.strides,\r\n                        padding=self.padding,\r\n                        data_format=self.data_format,\r\n                        filter_shape=self.kernel_norm_shape)\r\n        xnorm = K.sqrt(tmp + xb + K.epsilon())\r\n\r\n        W = self.W / Wnorm\r\n\r\n        output = KC.conv2d(x, W, strides=self.strides,\r\n                           padding=self.padding,\r\n                           data_format=self.data_format,\r\n                           filter_shape=self.kernel_shape)\r\n\r\n        if K.backend() == \'theano\':\r\n            xnorm = K.pattern_broadcast(xnorm, [False, True, False, False])\r\n\r\n        output /= xnorm\r\n\r\n        if self.use_bias:\r\n            b /= Wnorm\r\n            if self.data_format == \'channels_first\':\r\n                b = K.reshape(b, (1, self.filters, 1, 1))\r\n            elif self.data_format == \'channels_last\':\r\n                b = K.reshape(b, (1, 1, 1, self.filters))\r\n            else:\r\n                raise ValueError(\'Invalid data_format:\', self.data_format)\r\n            b /= xnorm\r\n            output += b\r\n        output = self.activation(output)\r\n        return output\r\n\r\n    def get_config(self):\r\n        config = {\r\n            \'filters\': self.filters,\r\n            \'kernel_size\': self.kernel_size,\r\n            \'kernel_initializer\': initializers.serialize(self.kernel_initializer),\r\n            \'activation\': activations.serialize(self.activation),\r\n            \'padding\': self.padding,\r\n            \'strides\': self.strides,\r\n            \'data_format\': self.data_format,\r\n            \'kernel_regularizer\': regularizers.serialize(self.kernel_regularizer),\r\n            \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\r\n            \'activity_regularizer\':\r\n                regularizers.serialize(self.activity_regularizer),\r\n            \'kernel_constraint\': constraints.serialize(self.kernel_constraint),\r\n            \'bias_constraint\': constraints.serialize(self.bias_constraint),\r\n            \'use_bias\': self.use_bias}\r\n        base_config = super(CosineConvolution2D, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\n\r\nCosineConv2D = CosineConvolution2D\r\n'"
keras_contrib/layers/convolutional/subpixelupscaling.py,0,"b'# -*- coding: utf-8 -*-\r\nfrom __future__ import absolute_import\r\n\r\nfrom keras.layers import Layer\r\n\r\nfrom keras_contrib import backend as KC\r\nfrom keras_contrib.utils.conv_utils import normalize_data_format\r\n\r\n\r\nclass SubPixelUpscaling(Layer):\r\n    """""" Sub-pixel convolutional upscaling layer.\r\n\r\n    This layer requires a Convolution2D prior to it,\r\n    having output filters computed according to\r\n    the formula :\r\n\r\n        filters = k * (scale_factor * scale_factor)\r\n        where k = a user defined number of filters (generally larger than 32)\r\n              scale_factor = the upscaling factor (generally 2)\r\n\r\n    This layer performs the depth to space operation on\r\n    the convolution filters, and returns a\r\n    tensor with the size as defined below.\r\n\r\n    # Example :\r\n    ```python\r\n        # A standard subpixel upscaling block\r\n        x = Convolution2D(256, 3, 3, padding=\'same\', activation=\'relu\')(...)\r\n        u = SubPixelUpscaling(scale_factor=2)(x)\r\n\r\n        # Optional\r\n        x = Convolution2D(256, 3, 3, padding=\'same\', activation=\'relu\')(u)\r\n    ```\r\n\r\n    In practice, it is useful to have a second convolution layer after the\r\n    SubPixelUpscaling layer to speed up the learning process.\r\n\r\n    However, if you are stacking multiple\r\n    SubPixelUpscaling blocks, it may increase\r\n    the number of parameters greatly, so the\r\n    Convolution layer after SubPixelUpscaling\r\n    layer can be removed.\r\n\r\n    # Arguments\r\n        scale_factor: Upscaling factor.\r\n        data_format: Can be None, \'channels_first\' or \'channels_last\'.\r\n\r\n    # Input shape\r\n        4D tensor with shape:\r\n        `(samples, k * (scale_factor * scale_factor) channels, rows, cols)`\r\n        if data_format=\'channels_first\'\r\n        or 4D tensor with shape:\r\n        `(samples, rows, cols, k * (scale_factor * scale_factor) channels)`\r\n        if data_format=\'channels_last\'.\r\n\r\n    # Output shape\r\n        4D tensor with shape:\r\n        `(samples, k channels, rows * scale_factor, cols * scale_factor))`\r\n        if data_format=\'channels_first\'\r\n        or 4D tensor with shape:\r\n        `(samples, rows * scale_factor, cols * scale_factor, k channels)`\r\n        if data_format=\'channels_last\'.\r\n\r\n    # References\r\n        - [Real-Time Single Image and Video Super-Resolution Using an\r\n           Efficient Sub-Pixel Convolutional Neural Network](\r\n           https://arxiv.org/abs/1609.05158)\r\n    """"""\r\n\r\n    def __init__(self, scale_factor=2, data_format=None, **kwargs):\r\n        super(SubPixelUpscaling, self).__init__(**kwargs)\r\n\r\n        self.scale_factor = scale_factor\r\n        self.data_format = normalize_data_format(data_format)\r\n\r\n    def build(self, input_shape):\r\n        pass\r\n\r\n    def call(self, x, mask=None):\r\n        y = KC.depth_to_space(x, self.scale_factor, self.data_format)\r\n        return y\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        if self.data_format == \'channels_first\':\r\n            b, k, r, c = input_shape\r\n            new_k = k // (self.scale_factor ** 2)\r\n            new_r = r * self.scale_factor\r\n            new_c = c * self.scale_factor\r\n            return b, new_k, new_r, new_c\r\n        else:\r\n            b, r, c, k = input_shape\r\n            new_r = r * self.scale_factor\r\n            new_c = c * self.scale_factor\r\n            new_k = k // (self.scale_factor ** 2)\r\n            return b, new_r, new_c, new_k\r\n\r\n    def get_config(self):\r\n        config = {\'scale_factor\': self.scale_factor,\r\n                  \'data_format\': self.data_format}\r\n        base_config = super(SubPixelUpscaling, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n'"
keras_contrib/layers/normalization/__init__.py,0,b''
keras_contrib/layers/normalization/groupnormalization.py,0,"b'from keras.layers import Layer, InputSpec\r\nfrom keras import initializers, regularizers, constraints\r\nfrom keras import backend as K\r\nfrom keras_contrib import backend as KC\r\n\r\n\r\nclass GroupNormalization(Layer):\r\n    """"""Group normalization layer.\r\n\r\n    Group Normalization divides the channels into groups and computes\r\n    within each group\r\n    the mean and variance for normalization.\r\n    Group Normalization\'s computation is independent\r\n     of batch sizes, and its accuracy is stable in a wide range of batch sizes.\r\n\r\n    Relation to Layer Normalization:\r\n    If the number of groups is set to 1, then this operation becomes identical to\r\n    Layer Normalization.\r\n\r\n    Relation to Instance Normalization:\r\n    If the number of groups is set to the\r\n    input dimension (number of groups is equal\r\n    to number of channels), then this operation becomes\r\n    identical to Instance Normalization.\r\n\r\n    # Arguments\r\n        groups: Integer, the number of groups for Group Normalization.\r\n            Can be in the range [1, N] where N is the input dimension.\r\n            The input dimension must be divisible by the number of groups.\r\n        axis: Integer, the axis that should be normalized\r\n            (typically the features axis).\r\n            For instance, after a `Conv2D` layer with\r\n            `data_format=""channels_first""`,\r\n            set `axis=1` in `BatchNormalization`.\r\n        epsilon: Small float added to variance to avoid dividing by zero.\r\n        center: If True, add offset of `beta` to normalized tensor.\r\n            If False, `beta` is ignored.\r\n        scale: If True, multiply by `gamma`.\r\n            If False, `gamma` is not used.\r\n            When the next layer is linear (also e.g. `nn.relu`),\r\n            this can be disabled since the scaling\r\n            will be done by the next layer.\r\n        beta_initializer: Initializer for the beta weight.\r\n        gamma_initializer: Initializer for the gamma weight.\r\n        beta_regularizer: Optional regularizer for the beta weight.\r\n        gamma_regularizer: Optional regularizer for the gamma weight.\r\n        beta_constraint: Optional constraint for the beta weight.\r\n        gamma_constraint: Optional constraint for the gamma weight.\r\n\r\n    # Input shape\r\n        Arbitrary. Use the keyword argument `input_shape`\r\n        (tuple of integers, does not include the samples axis)\r\n        when using this layer as the first layer in a model.\r\n\r\n    # Output shape\r\n        Same shape as input.\r\n\r\n    # References\r\n        - [Group Normalization](https://arxiv.org/abs/1803.08494)\r\n    """"""\r\n\r\n    def __init__(self,\r\n                 groups=32,\r\n                 axis=-1,\r\n                 epsilon=1e-5,\r\n                 center=True,\r\n                 scale=True,\r\n                 beta_initializer=\'zeros\',\r\n                 gamma_initializer=\'ones\',\r\n                 beta_regularizer=None,\r\n                 gamma_regularizer=None,\r\n                 beta_constraint=None,\r\n                 gamma_constraint=None,\r\n                 **kwargs):\r\n        super(GroupNormalization, self).__init__(**kwargs)\r\n        self.supports_masking = True\r\n        self.groups = groups\r\n        self.axis = axis\r\n        self.epsilon = epsilon\r\n        self.center = center\r\n        self.scale = scale\r\n        self.beta_initializer = initializers.get(beta_initializer)\r\n        self.gamma_initializer = initializers.get(gamma_initializer)\r\n        self.beta_regularizer = regularizers.get(beta_regularizer)\r\n        self.gamma_regularizer = regularizers.get(gamma_regularizer)\r\n        self.beta_constraint = constraints.get(beta_constraint)\r\n        self.gamma_constraint = constraints.get(gamma_constraint)\r\n\r\n    def build(self, input_shape):\r\n        dim = input_shape[self.axis]\r\n\r\n        if dim is None:\r\n            raise ValueError(\'Axis \' + str(self.axis) + \' of \'\r\n                             \'input tensor should have a defined dimension \'\r\n                             \'but the layer received an input with shape \' +\r\n                             str(input_shape) + \'.\')\r\n\r\n        if dim < self.groups:\r\n            raise ValueError(\'Number of groups (\' + str(self.groups) + \') cannot be \'\r\n                             \'more than the number of channels (\' +\r\n                             str(dim) + \').\')\r\n\r\n        if dim % self.groups != 0:\r\n            raise ValueError(\'Number of groups (\' + str(self.groups) + \') must be a \'\r\n                             \'multiple of the number of channels (\' +\r\n                             str(dim) + \').\')\r\n\r\n        self.input_spec = InputSpec(ndim=len(input_shape),\r\n                                    axes={self.axis: dim})\r\n        shape = (dim,)\r\n\r\n        if self.scale:\r\n            self.gamma = self.add_weight(shape=shape,\r\n                                         name=\'gamma\',\r\n                                         initializer=self.gamma_initializer,\r\n                                         regularizer=self.gamma_regularizer,\r\n                                         constraint=self.gamma_constraint)\r\n        else:\r\n            self.gamma = None\r\n        if self.center:\r\n            self.beta = self.add_weight(shape=shape,\r\n                                        name=\'beta\',\r\n                                        initializer=self.beta_initializer,\r\n                                        regularizer=self.beta_regularizer,\r\n                                        constraint=self.beta_constraint)\r\n        else:\r\n            self.beta = None\r\n        self.built = True\r\n\r\n    def call(self, inputs, **kwargs):\r\n        input_shape = K.int_shape(inputs)\r\n        tensor_input_shape = K.shape(inputs)\r\n\r\n        # Prepare broadcasting shape.\r\n        reduction_axes = list(range(len(input_shape)))\r\n        del reduction_axes[self.axis]\r\n        broadcast_shape = [1] * len(input_shape)\r\n        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\r\n        broadcast_shape.insert(1, self.groups)\r\n\r\n        reshape_group_shape = K.shape(inputs)\r\n        group_axes = [reshape_group_shape[i] for i in range(len(input_shape))]\r\n        group_axes[self.axis] = input_shape[self.axis] // self.groups\r\n        group_axes.insert(1, self.groups)\r\n\r\n        # reshape inputs to new group shape\r\n        group_shape = [group_axes[0], self.groups] + group_axes[2:]\r\n        group_shape = K.stack(group_shape)\r\n        inputs = K.reshape(inputs, group_shape)\r\n\r\n        group_reduction_axes = list(range(len(group_axes)))\r\n        mean, variance = KC.moments(inputs, group_reduction_axes[2:],\r\n                                    keep_dims=True)\r\n        inputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\r\n\r\n        # prepare broadcast shape\r\n        inputs = K.reshape(inputs, group_shape)\r\n\r\n        outputs = inputs\r\n\r\n        # In this case we must explicitly broadcast all parameters.\r\n        if self.scale:\r\n            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\r\n            outputs = outputs * broadcast_gamma\r\n\r\n        if self.center:\r\n            broadcast_beta = K.reshape(self.beta, broadcast_shape)\r\n            outputs = outputs + broadcast_beta\r\n\r\n        # finally we reshape the output back to the input shape\r\n        outputs = K.reshape(outputs, tensor_input_shape)\r\n\r\n        return outputs\r\n\r\n    def get_config(self):\r\n        config = {\r\n            \'groups\': self.groups,\r\n            \'axis\': self.axis,\r\n            \'epsilon\': self.epsilon,\r\n            \'center\': self.center,\r\n            \'scale\': self.scale,\r\n            \'beta_initializer\': initializers.serialize(self.beta_initializer),\r\n            \'gamma_initializer\': initializers.serialize(self.gamma_initializer),\r\n            \'beta_regularizer\': regularizers.serialize(self.beta_regularizer),\r\n            \'gamma_regularizer\': regularizers.serialize(self.gamma_regularizer),\r\n            \'beta_constraint\': constraints.serialize(self.beta_constraint),\r\n            \'gamma_constraint\': constraints.serialize(self.gamma_constraint)\r\n        }\r\n        base_config = super(GroupNormalization, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape\r\n'"
keras_contrib/layers/normalization/instancenormalization.py,0,"b'from keras.layers import Layer, InputSpec\r\nfrom keras import initializers, regularizers, constraints\r\nfrom keras import backend as K\r\n\r\n\r\nclass InstanceNormalization(Layer):\r\n    """"""Instance normalization layer.\r\n\r\n    Normalize the activations of the previous layer at each step,\r\n    i.e. applies a transformation that maintains the mean activation\r\n    close to 0 and the activation standard deviation close to 1.\r\n\r\n    # Arguments\r\n        axis: Integer, the axis that should be normalized\r\n            (typically the features axis).\r\n            For instance, after a `Conv2D` layer with\r\n            `data_format=""channels_first""`,\r\n            set `axis=1` in `InstanceNormalization`.\r\n            Setting `axis=None` will normalize all values in each\r\n            instance of the batch.\r\n            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\r\n        epsilon: Small float added to variance to avoid dividing by zero.\r\n        center: If True, add offset of `beta` to normalized tensor.\r\n            If False, `beta` is ignored.\r\n        scale: If True, multiply by `gamma`.\r\n            If False, `gamma` is not used.\r\n            When the next layer is linear (also e.g. `nn.relu`),\r\n            this can be disabled since the scaling\r\n            will be done by the next layer.\r\n        beta_initializer: Initializer for the beta weight.\r\n        gamma_initializer: Initializer for the gamma weight.\r\n        beta_regularizer: Optional regularizer for the beta weight.\r\n        gamma_regularizer: Optional regularizer for the gamma weight.\r\n        beta_constraint: Optional constraint for the beta weight.\r\n        gamma_constraint: Optional constraint for the gamma weight.\r\n\r\n    # Input shape\r\n        Arbitrary. Use the keyword argument `input_shape`\r\n        (tuple of integers, does not include the samples axis)\r\n        when using this layer as the first layer in a Sequential model.\r\n\r\n    # Output shape\r\n        Same shape as input.\r\n\r\n    # References\r\n        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\r\n        - [Instance Normalization: The Missing Ingredient for Fast Stylization](\r\n        https://arxiv.org/abs/1607.08022)\r\n    """"""\r\n    def __init__(self,\r\n                 axis=None,\r\n                 epsilon=1e-3,\r\n                 center=True,\r\n                 scale=True,\r\n                 beta_initializer=\'zeros\',\r\n                 gamma_initializer=\'ones\',\r\n                 beta_regularizer=None,\r\n                 gamma_regularizer=None,\r\n                 beta_constraint=None,\r\n                 gamma_constraint=None,\r\n                 **kwargs):\r\n        super(InstanceNormalization, self).__init__(**kwargs)\r\n        self.supports_masking = True\r\n        self.axis = axis\r\n        self.epsilon = epsilon\r\n        self.center = center\r\n        self.scale = scale\r\n        self.beta_initializer = initializers.get(beta_initializer)\r\n        self.gamma_initializer = initializers.get(gamma_initializer)\r\n        self.beta_regularizer = regularizers.get(beta_regularizer)\r\n        self.gamma_regularizer = regularizers.get(gamma_regularizer)\r\n        self.beta_constraint = constraints.get(beta_constraint)\r\n        self.gamma_constraint = constraints.get(gamma_constraint)\r\n\r\n    def build(self, input_shape):\r\n        ndim = len(input_shape)\r\n        if self.axis == 0:\r\n            raise ValueError(\'Axis cannot be zero\')\r\n\r\n        if (self.axis is not None) and (ndim == 2):\r\n            raise ValueError(\'Cannot specify axis for rank 1 tensor\')\r\n\r\n        self.input_spec = InputSpec(ndim=ndim)\r\n\r\n        if self.axis is None:\r\n            shape = (1,)\r\n        else:\r\n            shape = (input_shape[self.axis],)\r\n\r\n        if self.scale:\r\n            self.gamma = self.add_weight(shape=shape,\r\n                                         name=\'gamma\',\r\n                                         initializer=self.gamma_initializer,\r\n                                         regularizer=self.gamma_regularizer,\r\n                                         constraint=self.gamma_constraint)\r\n        else:\r\n            self.gamma = None\r\n        if self.center:\r\n            self.beta = self.add_weight(shape=shape,\r\n                                        name=\'beta\',\r\n                                        initializer=self.beta_initializer,\r\n                                        regularizer=self.beta_regularizer,\r\n                                        constraint=self.beta_constraint)\r\n        else:\r\n            self.beta = None\r\n        self.built = True\r\n\r\n    def call(self, inputs, training=None):\r\n        input_shape = K.int_shape(inputs)\r\n        reduction_axes = list(range(0, len(input_shape)))\r\n\r\n        if self.axis is not None:\r\n            del reduction_axes[self.axis]\r\n\r\n        del reduction_axes[0]\r\n\r\n        mean = K.mean(inputs, reduction_axes, keepdims=True)\r\n        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\r\n        normed = (inputs - mean) / stddev\r\n\r\n        broadcast_shape = [1] * len(input_shape)\r\n        if self.axis is not None:\r\n            broadcast_shape[self.axis] = input_shape[self.axis]\r\n\r\n        if self.scale:\r\n            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\r\n            normed = normed * broadcast_gamma\r\n        if self.center:\r\n            broadcast_beta = K.reshape(self.beta, broadcast_shape)\r\n            normed = normed + broadcast_beta\r\n        return normed\r\n\r\n    def get_config(self):\r\n        config = {\r\n            \'axis\': self.axis,\r\n            \'epsilon\': self.epsilon,\r\n            \'center\': self.center,\r\n            \'scale\': self.scale,\r\n            \'beta_initializer\': initializers.serialize(self.beta_initializer),\r\n            \'gamma_initializer\': initializers.serialize(self.gamma_initializer),\r\n            \'beta_regularizer\': regularizers.serialize(self.beta_regularizer),\r\n            \'gamma_regularizer\': regularizers.serialize(self.gamma_regularizer),\r\n            \'beta_constraint\': constraints.serialize(self.beta_constraint),\r\n            \'gamma_constraint\': constraints.serialize(self.gamma_constraint)\r\n        }\r\n        base_config = super(InstanceNormalization, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n'"
tests/keras_contrib/activations/test_squash.py,0,"b'from keras_contrib import activations\nimport keras.backend as K\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\n\ndef get_standard_values():\n    """"""A set of floats used for testing squash.\n    """"""\n    return np.array([[0, 0.1, 0.5, 0.9, 1.0]], dtype=K.floatx())\n\n\ndef test_squash_valid():\n    """"""Test using a reference implementation of squash.\n    """"""\n    def squash(x, axis=-1):\n        s_squared_norm = np.sum(np.square(x), axis) + 1e-7\n        scale = np.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n        return scale * x\n\n    x = K.placeholder(ndim=2)\n    f = K.function([x], [activations.squash(x)])\n    test_values = get_standard_values()\n\n    result = f([test_values])[0]\n    expected = squash(test_values)\n    assert_allclose(result, expected, rtol=1e-05)\n\n\ntest_squash_valid()\n'"
tests/keras_contrib/backend/backend_test.py,5,"b""import pytest\nfrom numpy.testing import assert_allclose\nimport numpy as np\n\nfrom keras import backend as K\nfrom keras.backend import theano_backend as KTH\nfrom keras.backend import tensorflow_backend as KTF\nimport keras_contrib.backend.theano_backend as KCTH\nimport keras_contrib.backend.tensorflow_backend as KCTF\nimport keras_contrib.backend.numpy_backend as KCNP\nfrom keras_contrib import backend as KC\n\n\ndef check_dtype(var, dtype):\n    if K._BACKEND == 'theano':\n        assert var.dtype == dtype\n    else:\n        assert var.dtype.name == '%s_ref' % dtype\n\n\ndef check_single_tensor_operation(function_name, input_shape, **kwargs):\n    val = np.random.random(input_shape) - 0.5\n    xth = KTH.variable(val)\n    xtf = KTF.variable(val)\n\n    zth = KTH.eval(getattr(KCTH, function_name)(xth, **kwargs))\n    ztf = KTF.eval(getattr(KCTF, function_name)(xtf, **kwargs))\n\n    assert zth.shape == ztf.shape\n    assert_allclose(zth, ztf, atol=1e-05)\n\n\ndef check_two_tensor_operation(function_name, x_input_shape,\n                               y_input_shape, **kwargs):\n    xval = np.random.random(x_input_shape) - 0.5\n\n    xth = KTH.variable(xval)\n    xtf = KTF.variable(xval)\n\n    yval = np.random.random(y_input_shape) - 0.5\n\n    yth = KTH.variable(yval)\n    ytf = KTF.variable(yval)\n\n    zth = KTH.eval(getattr(KCTH, function_name)(xth, yth, **kwargs))\n    ztf = KTF.eval(getattr(KCTF, function_name)(xtf, ytf, **kwargs))\n\n    assert zth.shape == ztf.shape\n    assert_allclose(zth, ztf, atol=1e-05)\n\n\ndef check_composed_tensor_operations(first_function_name, first_function_args,\n                                     second_function_name, second_function_args,\n                                     input_shape):\n    ''' Creates a random tensor t0 with shape input_shape and compute\n                 t1 = first_function_name(t0, **first_function_args)\n                 t2 = second_function_name(t1, **second_function_args)\n        with both Theano and TensorFlow backends and ensures the answers match.\n    '''\n    val = np.random.random(input_shape) - 0.5\n    xth = KTH.variable(val)\n    xtf = KTF.variable(val)\n\n    yth = getattr(KCTH, first_function_name)(xth, **first_function_args)\n    ytf = getattr(KCTF, first_function_name)(xtf, **first_function_args)\n\n    zth = KTH.eval(getattr(KCTH, second_function_name)(yth, **second_function_args))\n    ztf = KTF.eval(getattr(KCTF, second_function_name)(ytf, **second_function_args))\n\n    assert zth.shape == ztf.shape\n    assert_allclose(zth, ztf, atol=1e-05)\n\n\nclass TestBackend(object):\n\n    @pytest.mark.skipif(K.backend() != 'tensorflow',\n                        reason='No need to run the tests twice.')\n    @pytest.mark.parametrize('input_shape', [(1, 3, 40, 40), (1, 3, 10, 10)])\n    @pytest.mark.parametrize('kernel_shape', [2, 5])\n    def test_extract(self, input_shape, kernel_shape):\n        xval = np.random.random(input_shape)\n        kernel = [kernel_shape, kernel_shape]\n        strides = [kernel_shape, kernel_shape]\n        xth = KTH.variable(xval)\n        xtf = KTF.variable(xval)\n        ztf = KTF.eval(KCTF.extract_image_patches(xtf, kernel, strides,\n                                                  data_format='channels_first',\n                                                  padding='valid'))\n        zth = KTH.eval(KCTH.extract_image_patches(xth, kernel, strides,\n                                                  data_format='channels_first',\n                                                  padding='valid'))\n        assert zth.shape == ztf.shape\n        assert_allclose(zth, ztf, atol=1e-02)\n\n    @pytest.mark.skipif(K.backend() != 'tensorflow',\n                        reason='No need to run the tests twice.')\n    @pytest.mark.parametrize('input_shape', [(1, 40, 40, 3), (1, 10, 10, 3)])\n    @pytest.mark.parametrize('kernel_shape', [2, 5])\n    def test_extract2(self, input_shape, kernel_shape):\n\n        xval = np.random.random(input_shape)\n\n        kernel = [kernel_shape, kernel_shape]\n        strides = [kernel_shape, kernel_shape]\n        xth = KTH.variable(xval)\n        xtf = KTF.variable(xval)\n        ztf = KTF.eval(KCTF.extract_image_patches(xtf, kernel, strides,\n                                                  data_format='channels_last',\n                                                  padding='same'))\n        zth = KTH.eval(KCTH.extract_image_patches(xth, kernel, strides,\n                                                  data_format='channels_last',\n                                                  padding='same'))\n        assert zth.shape == ztf.shape\n        assert_allclose(zth, ztf, atol=1e-02)\n\n    @pytest.mark.skipif(K.backend() != 'tensorflow',\n                        reason='No need to run the tests twice.')\n    @pytest.mark.parametrize('batch_size', [1, 2, 3])\n    @pytest.mark.parametrize('scale', [2, 3])\n    @pytest.mark.parametrize('channels', [1, 2, 3])\n    @pytest.mark.parametrize('rows', [1, 2, 3])\n    @pytest.mark.parametrize('cols', [1, 2, 3])\n    def test_depth_to_space(self, batch_size, scale, channels, rows, cols):\n        if K.image_data_format() == 'channels_first':\n            arr = np.arange(batch_size * channels * scale * scale * rows * cols)\\\n                .reshape((batch_size, channels * scale * scale, rows, cols))\n        elif K.image_data_format() == 'channels_last':\n            arr = np.arange(batch_size * rows * cols * scale * scale * channels) \\\n                .reshape((batch_size, rows, cols, channels * scale * scale))\n\n        arr_tf = KTF.variable(arr)\n        arr_th = KTH.variable(arr)\n\n        if K.image_data_format() == 'channels_first':\n            expected = arr.reshape((batch_size, scale, scale, channels, rows, cols))\\\n                .transpose((0, 3, 4, 1, 5, 2))\\\n                .reshape((batch_size, channels, rows * scale, cols * scale))\n        elif K.image_data_format() == 'channels_last':\n            expected = arr.reshape((batch_size, rows, cols, scale, scale, channels))\\\n                .transpose((0, 1, 3, 2, 4, 5))\\\n                .reshape((batch_size, rows * scale, cols * scale, channels))\n\n        tf_ans = KTF.eval(KCTF.depth_to_space(arr_tf, scale))\n        th_ans = KTH.eval(KCTH.depth_to_space(arr_th, scale))\n\n        assert tf_ans.shape == expected.shape\n        assert th_ans.shape == expected.shape\n        assert_allclose(expected, tf_ans, atol=1e-05)\n        assert_allclose(expected, th_ans, atol=1e-05)\n\n    @pytest.mark.parametrize('keep_dims', [True, False])\n    def test_moments(self, keep_dims):\n        input_shape = (10, 10, 10, 10)\n        x_0 = np.zeros(input_shape)\n        x_1 = np.ones(input_shape)\n        x_random = np.random.random(input_shape)\n\n        th_axes = [0, 2, 3]\n        tf_axes = [0, 1, 2]\n\n        for ip in [x_0, x_1, x_random]:\n            for axes in [th_axes, tf_axes]:\n                K_mean, K_var = KC.moments(K.variable(ip), axes, keep_dims=keep_dims)\n                np_mean, np_var = KCNP.moments(ip, axes, keep_dims=keep_dims)\n\n                K_mean_val = K.eval(K_mean)\n                K_var_val = K.eval(K_var)\n\n                # absolute tolerance needed when working with zeros\n                assert_allclose(K_mean_val, np_mean, rtol=1e-4, atol=1e-10)\n                assert_allclose(K_var_val, np_var, rtol=1e-4, atol=1e-10)\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/keras_contrib/callbacks/cyclical_learning_rate_test.py,0,"b""import pytest\nimport numpy as np\nfrom keras_contrib import callbacks\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom numpy.testing import assert_allclose\n\n\ndef build_model():\n    model = Sequential([\n        Dense(2, activation='relu', input_shape=(2,)),\n        Dense(1, activation='sigmoid')\n    ])\n    return model\n\n\ndef cycle(i):\n    return np.floor(1 + i / (2 * 2000))\n\n\ndef x(i):\n    return np.abs(i / 2000. - 2 * cycle(i) + 1)\n\n\ndef test_cyclic_lr_triangular_1():\n    X = np.random.rand(4000, 2)\n    y = np.random.rand(4000).reshape(-1, 1)\n\n    clr = callbacks.CyclicLR()\n\n    model = build_model()\n    model.compile(\n        optimizer='sgd',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    model.fit(X, y, batch_size=1, epochs=1, verbose=0, callbacks=[clr])\n\n    r = np.concatenate([\n        np.linspace(0.001, 0.006, num=2001)[1:],\n        np.linspace(0.006, 0.001, num=2001)[1:]\n    ])\n\n    assert_allclose(clr.history['lr'], r)\n\n\ndef test_cyclic_lr_triangular_2():\n    X = np.random.rand(4000, 2)\n    y = np.random.rand(4000).reshape(-1, 1)\n\n    clr = callbacks.CyclicLR(mode='triangular2')\n\n    model = build_model()\n    model.compile(\n        optimizer='sgd',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    model.fit(X, y, batch_size=1, epochs=2, verbose=0, callbacks=[clr])\n\n    r = np.concatenate([\n        np.linspace(0.001, 0.006, num=2001)[1:],\n        np.linspace(0.006, 0.001, num=2001)[1:],\n        np.linspace(0.001, 0.0035, num=2001)[1:],\n        np.linspace(0.0035, 0.001, num=2001)[1:],\n    ])\n\n    assert_allclose(clr.history['lr'], r)\n\n\ndef test_cyclic_lr_exp_range():\n    X = np.random.rand(4000, 2)\n    y = np.random.rand(4000).reshape(-1, 1)\n\n    clr = callbacks.CyclicLR(mode='exp_range', gamma=0.9996)\n\n    model = build_model()\n    model.compile(\n        optimizer='sgd',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    model.fit(X, y, batch_size=1, epochs=2, verbose=0, callbacks=[clr])\n\n    exp_range = []\n\n    def scale_fn(i):\n        return 0.001 + (0.006 - 0.001) * np.maximum(0, (1 - x(i))) * (0.9996 ** i)\n\n    for i in range(8000):\n        exp_range.append(scale_fn(i + 1))\n\n    assert_allclose(clr.history['lr'], np.array(exp_range))\n\n\ndef test_cyclic_lr_custom_fn_test():\n    X = np.random.rand(4000, 2)\n    y = np.random.rand(4000).reshape(-1, 1)\n\n    def clr_fn(x):\n        return 1 / (5 ** (x * 0.0001))\n\n    clr = callbacks.CyclicLR(scale_fn=clr_fn, scale_mode='iterations')\n\n    model = build_model()\n    model.compile(\n        optimizer='sgd',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    model.fit(X, y, batch_size=1, epochs=2, verbose=0, callbacks=[clr])\n\n    custom_range = []\n\n    def scale_fn(i):\n        c = 0.006 - 0.001\n        return 0.001 + c * np.maximum(0, (1 - x(i))) * 1 / (5 ** (i * 0.0001))\n\n    for i in range(8000):\n        custom_range.append(scale_fn(i + 1))\n\n    assert_allclose(clr.history['lr'], np.array(custom_range))\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/keras_contrib/callbacks/dead_relu_detector_test.py,0,"b'import pytest\nimport numpy as np\nimport sys\n\nif sys.version_info > (3, 0):\n    from io import StringIO\nelse:\n    from StringIO import StringIO\n\nfrom keras_contrib import callbacks\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Conv2D, Flatten, Activation\nfrom keras import backend as K\n\nn_out = 11\n# with 1 neuron dead, 1/11 is just below the threshold of 10% with verbose = False\n\n\ndef check_print(do_train, expected_warnings, nr_dead=None, perc_dead=None):\n    """"""\n    Receive stdout to check if correct warning message is delivered\n    :param nr_dead: int\n    :param perc_dead: float, 10% should be written as 0.1\n    """"""\n\n    saved_stdout = sys.stdout\n\n    out = StringIO()\n    out.flush()\n    sys.stdout = out    # overwrite current stdout\n\n    do_train()\n\n    # get prints, can be something like: ""Layer\n    # dense (#0) has 2 dead neurons (20.00%)!""\n    stdoutput = out.getvalue().strip()\n    str_to_count = ""dead neurons""\n    count = stdoutput.count(str_to_count)\n\n    sys.stdout = saved_stdout   # restore stdout\n    out.close()\n\n    assert expected_warnings == count\n    if expected_warnings and (nr_dead is not None):\n        str_to_check = \'has {} dead\'.format(nr_dead)\n        assert str_to_check in stdoutput, \'""{}"" not in ""{}""\'.format(str_to_check,\n                                                                    stdoutput)\n    if expected_warnings and (perc_dead is not None):\n        str_to_check = \'neurons ({:.2%})!\'.format(perc_dead)\n        assert str_to_check in stdoutput, \'""{}"" not in ""{}""\'.format(str_to_check,\n                                                                    stdoutput)\n\n\ndef test_DeadDeadReluDetector():\n    n_samples = 9\n\n    input_shape = (n_samples, 3, 4)  # 4 input features\n    shape_out = (n_samples, 3, n_out)  # 11 output features\n    shape_weights = (4, n_out)\n\n    # ignore batch size\n    input_shape_dense = tuple(input_shape[1:])\n\n    def do_test(weights, expected_warnings, verbose, nr_dead=None, perc_dead=None):\n\n        def do_train():\n            dataset = np.ones(input_shape)    # data to be fed as training\n            model = Sequential()\n            model.add(Dense(n_out, activation=\'relu\', input_shape=input_shape_dense,\n                            use_bias=False, weights=[weights], name=\'dense\'))\n            model.compile(optimizer=\'sgd\', loss=\'categorical_crossentropy\')\n            model.fit(\n                dataset,\n                np.ones(shape_out),\n                batch_size=1,\n                epochs=1,\n                callbacks=[callbacks.DeadReluDetector(dataset, verbose=verbose)],\n                verbose=False\n            )\n\n        check_print(do_train, expected_warnings, nr_dead, perc_dead)\n\n    # weights that correspond to NN with 1/11 neurons dead\n    weights_1_dead = np.ones(shape_weights)\n    # weights that correspond to NN with 2/11 neurons dead\n    weights_2_dead = np.ones(shape_weights)\n    # weights that correspond to all neurons dead\n    weights_all_dead = np.zeros(shape_weights)\n\n    weights_1_dead[:, 0] = 0\n    weights_2_dead[:, 0:2] = 0\n\n    do_test(weights_1_dead, verbose=True,\n            expected_warnings=1, nr_dead=1, perc_dead=1. / n_out)\n    do_test(weights_1_dead, verbose=False, expected_warnings=0)\n    do_test(weights_2_dead, verbose=True,\n            expected_warnings=1, nr_dead=2, perc_dead=2. / n_out)\n    # do_test(weights_all_dead, verbose=True, expected_warnings=1,\n    # nr_dead=n_out, perc_dead=1.)\n\n\ndef test_DeadDeadReluDetector_bias():\n    n_samples = 9\n\n    input_shape = (n_samples, 4)  # 4 input features\n    shape_weights = (4, n_out)\n    shape_bias = (n_out, )\n    shape_out = (n_samples, n_out)  # 11 output features\n\n    # ignore batch size\n    input_shape_dense = tuple(input_shape[1:])\n\n    def do_test(weights, bias, expected_warnings, verbose,\n                nr_dead=None, perc_dead=None):\n\n        def do_train():\n            dataset = np.ones(input_shape)  # data to be fed as training\n            model = Sequential()\n            model.add(Dense(n_out, activation=\'relu\', input_shape=input_shape_dense,\n                            use_bias=True, weights=[weights, bias], name=\'dense\'))\n            model.compile(optimizer=\'sgd\', loss=\'categorical_crossentropy\')\n            model.fit(\n                dataset,\n                np.ones(shape_out),\n                batch_size=1,\n                epochs=1,\n                callbacks=[callbacks.DeadReluDetector(dataset, verbose=verbose)],\n                verbose=False\n            )\n\n        check_print(do_train, expected_warnings, nr_dead, perc_dead)\n\n    # weights that correspond to NN with 1/11 neurons dead\n    weights_1_dead = np.ones(shape_weights)\n    # weights that correspond to NN with 2/11 neurons dead\n    weights_2_dead = np.ones(shape_weights)\n    # weights that correspond to all neurons dead\n    weights_all_dead = np.zeros(shape_weights)\n\n    weights_1_dead[:, 0] = 0\n    weights_2_dead[:, 0:2] = 0\n\n    bias = np.zeros(shape_bias)\n\n    do_test(weights_1_dead, bias, verbose=True, expected_warnings=1,\n            nr_dead=1, perc_dead=1. / n_out)\n    do_test(weights_1_dead, bias, verbose=False, expected_warnings=0)\n    do_test(weights_2_dead, bias, verbose=True, expected_warnings=1,\n            nr_dead=2, perc_dead=2. / n_out)\n    # do_test(weights_all_dead, bias, verbose=True,\n    # expected_warnings=1, nr_dead=n_out, perc_dead=1.)\n\n\ndef test_DeadDeadReluDetector_conv():\n    n_samples = 9\n\n    # (5, 5) kernel, 4 input featuremaps and 11 output featuremaps\n    if K.image_data_format() == \'channels_last\':\n        input_shape = (n_samples, 5, 5, 4)\n    else:\n        input_shape = (n_samples, 4, 5, 5)\n\n    # ignore batch size\n    input_shape_conv = tuple(input_shape[1:])\n    shape_weights = (5, 5, 4, n_out)\n    shape_out = (n_samples, n_out)\n\n    def do_test(weights_bias, expected_warnings, verbose,\n                nr_dead=None, perc_dead=None):\n        """"""\n        :param perc_dead: as float, 10% should be written as 0.1\n        """"""\n\n        def do_train():\n            dataset = np.ones(input_shape)  # data to be fed as training\n            model = Sequential()\n            model.add(Conv2D(n_out, (5, 5), activation=\'relu\',\n                             input_shape=input_shape_conv,\n                             use_bias=True, weights=weights_bias, name=\'conv\'))\n            model.add(Flatten())  # to handle Theano\'s categorical crossentropy\n            model.compile(optimizer=\'sgd\', loss=\'categorical_crossentropy\')\n            model.fit(\n                dataset,\n                np.ones(shape_out),\n                batch_size=1,\n                epochs=1,\n                callbacks=[callbacks.DeadReluDetector(dataset, verbose=verbose)],\n                verbose=False\n            )\n\n        check_print(do_train, expected_warnings, nr_dead, perc_dead)\n\n    # weights that correspond to NN with 1/11 neurons dead\n    weights_1_dead = np.ones(shape_weights)\n    weights_1_dead[..., 0] = 0\n    # weights that correspond to NN with 2/11 neurons dead\n    weights_2_dead = np.ones(shape_weights)\n    weights_2_dead[..., 0:2] = 0\n    # weights that correspond to NN with all neurons dead\n    weights_all_dead = np.zeros(shape_weights)\n\n    bias = np.zeros((11, ))\n\n    weights_bias_1_dead = [weights_1_dead, bias]\n    weights_bias_2_dead = [weights_2_dead, bias]\n    weights_bias_all_dead = [weights_all_dead, bias]\n\n    do_test(weights_bias_1_dead, verbose=True, expected_warnings=1,\n            nr_dead=1, perc_dead=1. / n_out)\n    do_test(weights_bias_1_dead, verbose=False, expected_warnings=0)\n    do_test(weights_bias_2_dead, verbose=True, expected_warnings=1,\n            nr_dead=2, perc_dead=2. / n_out)\n    # do_test(weights_bias_all_dead, verbose=True, expected_warnings=1,\n    # nr_dead=n_out, perc_dead=1.)\n\n\ndef test_DeadDeadReluDetector_activation():\n    """"""\n    Tests that using ""Activation"" layer does not throw error\n    """"""\n    input_data = Input(shape=(1,))\n    output_data = Activation(\'relu\')(input_data)\n    model = Model(input_data, output_data)\n    model.compile(optimizer=\'adadelta\', loss=\'binary_crossentropy\')\n    model.fit(\n        np.array([[1]]),\n        np.array([[1]]),\n        epochs=1,\n        validation_data=(np.array([[1]]), np.array([[1]])),\n        callbacks=[callbacks.DeadReluDetector(np.array([[1]]))]\n    )\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/keras_contrib/callbacks/tensorboard_test.py,0,"b""import pytest\nimport numpy as np\nimport os\nimport shutil\nfrom keras.utils import to_categorical\nfrom keras.layers import Layer, Input, Dense, Dropout, BatchNormalization\nfrom keras_contrib.utils.test_utils import to_list, unpack_singleton\nfrom keras_contrib.utils.test_utils import get_test_data\nfrom keras import Model\nfrom keras import backend as K\nfrom keras_contrib.callbacks import TensorBoardGrouped\n\ninput_dim = 2\nnum_hidden = 4\nnum_classes = 2\nbatch_size = 5\ntrain_samples = 20\ntest_samples = 20\n\n\ndef data_generator(x, y, batch_size):\n    x = to_list(x)\n    y = to_list(y)\n    max_batch_index = len(x[0]) // batch_size\n    i = 0\n    while 1:\n        x_batch = [array[i * batch_size: (i + 1) * batch_size] for array in x]\n        x_batch = unpack_singleton(x_batch)\n\n        y_batch = [array[i * batch_size: (i + 1) * batch_size] for array in y]\n        y_batch = unpack_singleton(y_batch)\n        yield x_batch, y_batch\n        i += 1\n        i = i % max_batch_index\n\n\n# Changing the default arguments of get_test_data.\ndef get_data_callbacks(num_train=train_samples,\n                       num_test=test_samples,\n                       input_shape=(input_dim,),\n                       classification=True,\n                       num_classes=num_classes):\n    return get_test_data(num_train=num_train,\n                         num_test=num_test,\n                         input_shape=input_shape,\n                         classification=classification,\n                         num_classes=num_classes)\n\n\ndef test_TensorBoard(tmpdir):\n    np.random.seed(np.random.randint(1, 1e7))\n    filepath = str(tmpdir / 'logs')\n\n    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n    y_test = to_categorical(y_test)\n    y_train = to_categorical(y_train)\n\n    class DummyStatefulMetric(Layer):\n\n        def __init__(self, name='dummy_stateful_metric', **kwargs):\n            super(DummyStatefulMetric, self).__init__(name=name, **kwargs)\n            self.stateful = True\n            self.state = K.variable(value=0, dtype='int32')\n\n        def reset_states(self):\n            pass\n\n        def __call__(self, y_true, y_pred):\n            return self.state\n\n    inp = Input((input_dim,))\n    hidden = Dense(num_hidden, activation='relu')(inp)\n    hidden = Dropout(0.1)(hidden)\n    hidden = BatchNormalization()(hidden)\n    output = Dense(num_classes, activation='softmax')(hidden)\n    model = Model(inputs=inp, outputs=output)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy', DummyStatefulMetric()])\n\n    # we must generate new callbacks for each test, as they aren't stateless\n    def callbacks_factory(histogram_freq):\n        return [TensorBoardGrouped(log_dir=filepath,\n                                   histogram_freq=histogram_freq,\n                                   write_images=True, write_grads=True,\n                                   batch_size=5)]\n\n    # fit without validation data\n    model.fit(X_train, y_train, batch_size=batch_size,\n              callbacks=callbacks_factory(histogram_freq=0),\n              epochs=3)\n\n    # fit with validation data and accuracy\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test),\n              callbacks=callbacks_factory(histogram_freq=0), epochs=2)\n\n    # fit generator without validation data\n    train_generator = data_generator(X_train, y_train, batch_size)\n    model.fit_generator(train_generator, len(X_train), epochs=2,\n                        callbacks=callbacks_factory(histogram_freq=0))\n\n    # fit generator with validation data and accuracy\n    train_generator = data_generator(X_train, y_train, batch_size)\n    model.fit_generator(train_generator, len(X_train), epochs=2,\n                        validation_data=(X_test, y_test),\n                        callbacks=callbacks_factory(histogram_freq=1))\n\n    assert os.path.isdir(filepath)\n    shutil.rmtree(filepath)\n    assert not tmpdir.listdir()\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/keras_contrib/datasets/test_datasets.py,0,"b""from __future__ import print_function\nimport pytest\nimport time\nimport random\nfrom keras_contrib import datasets\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/keras_contrib/layers/test_capsule.py,0,"b""import pytest\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom keras_contrib.utils.test_utils import layer_test\nfrom keras_contrib.utils.test_utils import is_tf_keras\nfrom keras import backend as K\nfrom keras_contrib.layers import capsule\nfrom keras.models import Sequential\n\n\n@pytest.mark.parametrize('num_capsule', [10, 20])\n@pytest.mark.parametrize('dim_capsule', [10, 20])\n@pytest.mark.parametrize('routings', [3, 4])\n@pytest.mark.parametrize('share_weights', [True, False])\n@pytest.mark.parametrize('activation', ['sigmoid', 'relu'])\ndef test_capsule(num_capsule,\n                 dim_capsule,\n                 routings,\n                 share_weights,\n                 activation):\n\n    # TODO: removed this once the issue #25546 in the Tensorflow repo is fixed.\n    if is_tf_keras and not share_weights:\n        return\n\n    num_samples = 100\n    num_rows = 256\n    num_cols = 256\n\n    kwargs = {'num_capsule': num_capsule,\n              'dim_capsule': dim_capsule,\n              'routings': routings,\n              'share_weights': share_weights,\n              'activation': activation}\n\n    layer_test(capsule.Capsule,\n               kwargs=kwargs,\n               input_shape=(num_samples, num_rows, num_cols))\n\n\ndef test_capsule_correctness():\n    X = np.random.random((1, 1, 1))\n\n    model = Sequential()\n    model.add(capsule.Capsule(1, 1, 1, True, activation='sigmoid'))\n\n    model.compile(loss='mse', optimizer='rmsprop')\n    init_out = model.predict(X)  # mock predict call to initialize weights\n    model.set_weights([np.zeros((1, 1, 1))])\n    out = model.predict(X)\n    assert_allclose(out, np.zeros((1, 1, 1), dtype=K.floatx()) + 0.5, atol=1e-5)\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/keras_contrib/layers/test_core.py,0,"b""import pytest\nimport numpy as np\n\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.models import Sequential\nfrom keras import backend as K\nfrom keras_contrib.layers import core\nfrom keras_contrib.utils.test_utils import layer_test\nfrom numpy.testing import assert_allclose\n\n\n@pytest.mark.parametrize('input_shape', [(3, 2),\n                                         (3, 4, 2),\n                                         (None, None, 2),\n                                         (3, 4, 5, 2)])\ndef test_cosinedense(input_shape):\n\n    layer_test(core.CosineDense,\n               kwargs={'units': 3},\n               input_shape=input_shape)\n\n\ndef test_cosinedense_reg_constraint():\n    layer_test(core.CosineDense,\n               kwargs={'units': 3,\n                       'kernel_regularizer': regularizers.l2(0.01),\n                       'bias_regularizer': regularizers.l1(0.01),\n                       'activity_regularizer': regularizers.l2(0.01),\n                       'kernel_constraint': constraints.MaxNorm(1),\n                       'bias_constraint': constraints.MaxNorm(1)},\n               input_shape=(3, 2))\n\n\ndef test_cosinedense_correctness():\n    X = np.random.randn(1, 20)\n    model = Sequential()\n    model.add(core.CosineDense(1, use_bias=True, input_shape=(20,)))\n    model.compile(loss='mse', optimizer='rmsprop')\n    W = model.get_weights()\n    W[0] = X.T\n    W[1] = np.asarray([1.])\n    model.set_weights(W)\n    out = model.predict(X)\n    assert_allclose(out, np.ones((1, 1), dtype=K.floatx()), atol=1e-5)\n\n    X = np.random.randn(1, 20)\n    model = Sequential()\n    model.add(core.CosineDense(1, use_bias=False, input_shape=(20,)))\n    model.compile(loss='mse', optimizer='rmsprop')\n    W = model.get_weights()\n    W[0] = -2 * X.T\n    model.set_weights(W)\n    out = model.predict(X)\n    assert_allclose(out, -np.ones((1, 1), dtype=K.floatx()), atol=1e-5)\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/keras_contrib/layers/test_crf.py,0,"b""import pytest\nimport numpy as np\nimport os\nfrom numpy.testing import assert_allclose\n\nfrom keras.layers import Embedding\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras_contrib.losses import crf_loss\nfrom keras_contrib.metrics import crf_accuracy\nfrom keras_contrib.metrics import crf_marginal_accuracy\nfrom keras_contrib.metrics import crf_viterbi_accuracy\nfrom keras_contrib.layers import CRF\nfrom keras_contrib.utils.test_utils import is_tf_keras\n\nnb_samples, timesteps, embedding_dim, output_dim = 2, 10, 4, 5\nembedding_num = 12\n\nMODEL_PERSISTENCE_PATH = './test_saving_crf_model.h5'\n\n\n@pytest.mark.xfail(is_tf_keras,\n                   reason='TODO: fix it. Using K.tf which is bad.',\n                   strict=True)\ndef test_CRF():\n    # data\n    x = np.random.randint(1, embedding_num, nb_samples * timesteps)\n    x = x.reshape((nb_samples, timesteps))\n    x[0, -4:] = 0  # right padding\n    x[1, :5] = 0  # left padding\n    y = np.random.randint(0, output_dim, nb_samples * timesteps)\n    y = y.reshape((nb_samples, timesteps))\n    y_onehot = np.eye(output_dim)[y]\n    y = np.expand_dims(y, 2)  # .astype('float32')\n\n    # test with no masking, onehot, fix length\n    model = Sequential()\n    model.add(Embedding(embedding_num, embedding_dim, input_length=timesteps))\n    crf = CRF(output_dim)\n    model.add(crf)\n    model.compile(optimizer='rmsprop', loss=crf_loss)\n    model.fit(x, y_onehot, epochs=1, batch_size=10)\n    model.save(MODEL_PERSISTENCE_PATH)\n    load_model(MODEL_PERSISTENCE_PATH,\n               custom_objects={'CRF': CRF,\n                               'crf_loss': crf_loss,\n                               'crf_viterbi_accuracy': crf_viterbi_accuracy})\n\n    # test with masking, sparse target, dynamic length;\n    # test crf_viterbi_accuracy, crf_marginal_accuracy\n\n    model = Sequential()\n    model.add(Embedding(embedding_num, embedding_dim, mask_zero=True))\n    crf = CRF(output_dim, sparse_target=True)\n    model.add(crf)\n    model.compile(optimizer='rmsprop', loss=crf_loss,\n                  metrics=[crf_viterbi_accuracy, crf_marginal_accuracy])\n    model.fit(x, y, epochs=1, batch_size=10)\n\n    # check mask\n    y_pred = model.predict(x).argmax(-1)\n    assert (y_pred[0, -4:] == 0).all()  # right padding\n    assert (y_pred[1, :5] == 0).all()  # left padding\n\n    # test viterbi_acc\n    _, v_acc, _ = model.evaluate(x, y)\n    np_acc = (y_pred[x > 0] == y[:, :, 0][x > 0]).astype('float32').mean()\n    print(v_acc, np_acc)\n    assert np.abs(v_acc - np_acc) < 1e-4\n\n    # test config\n    model.get_config()\n\n    # test marginal learn mode, fix length\n\n    model = Sequential()\n    model.add(Embedding(embedding_num, embedding_dim, input_length=timesteps,\n                        mask_zero=True))\n    crf = CRF(output_dim, learn_mode='marginal', unroll=True)\n    model.add(crf)\n    model.compile(optimizer='rmsprop', loss=crf_loss)\n    model.fit(x, y_onehot, epochs=1, batch_size=10)\n\n    # check mask (marginal output)\n    y_pred = model.predict(x)\n    assert_allclose(y_pred[0, -4:], 1. / output_dim, atol=1e-6)\n    assert_allclose(y_pred[1, :5], 1. / output_dim, atol=1e-6)\n\n    # test marginal learn mode, but with Viterbi test_mode\n    model = Sequential()\n    model.add(Embedding(embedding_num, embedding_dim, input_length=timesteps,\n                        mask_zero=True))\n    crf = CRF(output_dim, learn_mode='marginal', test_mode='viterbi')\n    model.add(crf)\n    model.compile(optimizer='rmsprop', loss=crf_loss, metrics=[crf_accuracy])\n    model.fit(x, y_onehot, epochs=1, batch_size=10)\n\n    y_pred = model.predict(x)\n\n    # check y_pred is onehot vector (output from 'viterbi' test mode)\n    assert_allclose(np.eye(output_dim)[y_pred.argmax(-1)], y_pred, atol=1e-6)\n\n    try:\n        os.remove(MODEL_PERSISTENCE_PATH)\n    except OSError:\n        pass\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/keras_contrib/losses/dssim_test.py,0,"b""import pytest\nimport numpy as np\nfrom keras_contrib.utils.test_utils import is_tf_keras\nfrom numpy.testing import assert_allclose\nfrom keras.layers import Conv2D\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\n\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras import backend as K\nfrom keras_contrib.losses import DSSIMObjective\n\nallobj = []\n\n\ndef test_objective_shapes_3d():\n    y_a = K.variable(np.random.random((5, 6, 7)))\n    y_b = K.variable(np.random.random((5, 6, 7)))\n    for obj in allobj:\n        objective_output = obj(y_a, y_b)\n        assert K.eval(objective_output).shape == (5, 6)\n\n\ndef test_objective_shapes_2d():\n    y_a = K.variable(np.random.random((6, 7)))\n    y_b = K.variable(np.random.random((6, 7)))\n    for obj in allobj:\n        objective_output = obj(y_a, y_b)\n        assert K.eval(objective_output).shape == (6,)\n\n\ndef test_cce_one_hot():\n    y_a = K.variable(np.random.randint(0, 7, (5, 6)))\n    y_b = K.variable(np.random.random((5, 6, 7)))\n    objective_output = sparse_categorical_crossentropy(y_a, y_b)\n    assert K.eval(objective_output).shape == (5, 6)\n\n    y_a = K.variable(np.random.randint(0, 7, (6,)))\n    y_b = K.variable(np.random.random((6, 7)))\n    assert K.eval(sparse_categorical_crossentropy(y_a, y_b)).shape == (6,)\n\n\ndef test_DSSIM_channels_last():\n    prev_data = K.image_data_format()\n    K.set_image_data_format('channels_last')\n    for input_dim, kernel_size in zip([32, 33], [2, 3]):\n        input_shape = [input_dim, input_dim, 3]\n        X = np.random.random_sample(4 * input_dim * input_dim * 3)\n        X = X.reshape([4] + input_shape)\n        y = np.random.random_sample(4 * input_dim * input_dim * 3)\n        y = y.reshape([4] + input_shape)\n\n        model = Sequential()\n        model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape,\n                         activation='relu'))\n        model.add(Conv2D(3, (3, 3), padding='same', input_shape=input_shape,\n                         activation='relu'))\n        adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n        model.compile(loss=DSSIMObjective(kernel_size=kernel_size),\n                      metrics=['mse'],\n                      optimizer=adam)\n        model.fit(X, y, batch_size=2, epochs=1, shuffle='batch')\n\n        # Test same\n        x1 = K.constant(X, 'float32')\n        x2 = K.constant(X, 'float32')\n        dssim = DSSIMObjective(kernel_size=kernel_size)\n        assert_allclose(0.0, K.eval(dssim(x1, x2)), atol=1e-4)\n\n        # Test opposite\n        x1 = K.zeros([4] + input_shape)\n        x2 = K.ones([4] + input_shape)\n        dssim = DSSIMObjective(kernel_size=kernel_size)\n        assert_allclose(0.5, K.eval(dssim(x1, x2)), atol=1e-4)\n\n    K.set_image_data_format(prev_data)\n\n\n@pytest.mark.xfail(is_tf_keras,\n                   reason='TODO fix this.',\n                   strict=True)\ndef test_DSSIM_channels_first():\n    prev_data = K.image_data_format()\n    K.set_image_data_format('channels_first')\n    for input_dim, kernel_size in zip([32, 33], [2, 3]):\n        input_shape = [3, input_dim, input_dim]\n        X = np.random.random_sample(4 * input_dim * input_dim * 3)\n        X = X.reshape([4] + input_shape)\n        y = np.random.random_sample(4 * input_dim * input_dim * 3)\n        y = y.reshape([4] + input_shape)\n\n        model = Sequential()\n        model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape,\n                         activation='relu'))\n        model.add(Conv2D(3, (3, 3), padding='same', input_shape=input_shape,\n                         activation='relu'))\n        adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n        model.compile(loss=DSSIMObjective(kernel_size=kernel_size), metrics=['mse'],\n                      optimizer=adam)\n        model.fit(X, y, batch_size=2, epochs=1, shuffle='batch')\n\n        # Test same\n        x1 = K.constant(X, 'float32')\n        x2 = K.constant(X, 'float32')\n        dssim = DSSIMObjective(kernel_size=kernel_size)\n        assert_allclose(0.0, K.eval(dssim(x1, x2)), atol=1e-4)\n\n        # Test opposite\n        x1 = K.zeros([4] + input_shape)\n        x2 = K.ones([4] + input_shape)\n        dssim = DSSIMObjective(kernel_size=kernel_size)\n        assert_allclose(0.5, K.eval(dssim(x1, x2)), atol=1e-4)\n\n    K.set_image_data_format(prev_data)\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/keras_contrib/losses/jaccard_test.py,0,"b""import pytest\n\nfrom keras_contrib.losses import jaccard_distance\nfrom keras_contrib.utils.test_utils import is_tf_keras\nfrom keras import backend as K\nimport numpy as np\n\n\n@pytest.mark.xfail(is_tf_keras,\n                   reason='TODO fix this.',\n                   strict=True)\ndef test_jaccard_distance():\n    # all_right, almost_right, half_right, all_wrong\n    y_true = np.array([[0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0],\n                       [0, 0, 1., 0.]])\n    y_pred = np.array([[0, 0, 1, 0], [0, 0, 0.9, 0], [0, 0, 0.1, 0],\n                       [1, 1, 0.1, 1.]])\n\n    r = jaccard_distance(\n        K.variable(y_true),\n        K.variable(y_pred), )\n    if K.is_keras_tensor(r):\n        assert K.int_shape(r) == (4, )\n\n    all_right, almost_right, half_right, all_wrong = K.eval(r)\n    assert all_right == 0, 'should converge on zero'\n    assert all_right < almost_right\n    assert almost_right < half_right\n    assert half_right < all_wrong\n\n\ndef test_jaccard_distance_shapes_3d():\n    y_a = K.variable(np.random.random((5, 6, 7)))\n    y_b = K.variable(np.random.random((5, 6, 7)))\n    objective_output = jaccard_distance(y_a, y_b)\n    assert K.eval(objective_output).shape == (5, 6)\n\n\ndef test_jaccard_distance_shapes_2d():\n    y_a = K.variable(np.random.random((6, 7)))\n    y_b = K.variable(np.random.random((6, 7)))\n    objective_output = jaccard_distance(y_a, y_b)\n    assert K.eval(objective_output).shape == (6, )\n"""
tests/keras_contrib/optimizers/ftml_test.py,0,"b""from __future__ import print_function\nimport pytest\nfrom keras_contrib.utils.test_utils import is_tf_keras\nfrom keras_contrib.tests import optimizers\nfrom keras_contrib.optimizers import ftml\n\n\n@pytest.mark.xfail(is_tf_keras,\n                   reason='TODO fix this.',\n                   strict=True)\ndef test_ftml():\n    optimizers._test_optimizer(ftml())\n    optimizers._test_optimizer(ftml(lr=0.003, beta_1=0.8,\n                                    beta_2=0.9, epsilon=1e-5,\n                                    decay=1e-3))\n"""
tests/keras_contrib/optimizers/lars_test.py,0,"b'from __future__ import print_function\nimport numpy as np\nfrom keras_contrib.tests import optimizers\nfrom keras_contrib.optimizers import lars\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n\ndef test_base_lars():\n    optimizers._test_optimizer(lars(0.01))\n\n\ndef test_nesterov_lars():\n    optimizers._test_optimizer(lars(0.01, nesterov=True))\n'"
tests/keras_contrib/optimizers/padam_test.py,0,b'from __future__ import print_function\nfrom keras_contrib.tests import optimizers\nfrom keras_contrib.optimizers import Padam\n\n\ndef test_padam():\n    optimizers._test_optimizer(Padam())\n    optimizers._test_optimizer(Padam(decay=1e-3))\n'
tests/keras_contrib/optimizers/yogi_test.py,0,"b""from __future__ import print_function\nimport pytest\nfrom keras_contrib.tests import optimizers\nfrom keras_contrib.optimizers import Yogi\nfrom keras_contrib.utils.test_utils import is_tf_keras\n\n\ndef test_yogi():\n    optimizers._test_optimizer(Yogi())\n    optimizers._test_optimizer(Yogi(beta_1=0.9, beta_2=0.9))\n    optimizers._test_optimizer(Yogi(beta_1=0.9, beta_2=0.99))\n    optimizers._test_optimizer(Yogi(beta_1=0.9, beta_2=0.999))\n\n\n@pytest.mark.skipif(is_tf_keras,\n                    reason='Sometimes fail. It is random.',\n                    strict=True)\ndef test_yogi_change_lr():\n    optimizers._test_optimizer(Yogi(beta_1=0.9, beta_2=0.999, lr=0.001))\n"""
tests/keras_contrib/utils/save_load_utils_test.py,0,"b""import pytest\nimport os\nfrom keras import backend as K\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom numpy.testing import assert_allclose\n\nfrom keras_contrib.utils.save_load_utils import save_all_weights, load_all_weights\n\n\n@pytest.mark.skipif(K.backend() != 'tensorflow',\n                    reason='save_all_weights and load_all_weights only '\n                           'supported on TensorFlow')\ndef test_save_and_load_all_weights():\n    '''\n    Test save_all_weights and load_all_weights.\n    Save and load optimizer and model weights but not configuration.\n    '''\n\n    def make_model():\n        _x = Input((10,))\n        _y = Dense(10)(_x)\n        _m = Model(_x, _y)\n        _m.compile('adam', 'mean_squared_error')\n        _m._make_train_function()\n        return _m\n\n    # make a model\n    m1 = make_model()\n    # set weights\n    w1 = m1.layers[1].kernel  # dense layer\n    w1value = K.get_value(w1)\n    w1value[0, 0:4] = [1, 3, 3, 7]\n    K.set_value(w1, w1value)\n    # set optimizer weights\n    ow1 = m1.optimizer.weights[3]  # momentum weights\n    ow1value = K.get_value(ow1)\n    ow1value[0, 0:3] = [4, 2, 0]\n    K.set_value(ow1, ow1value)\n    # save all weights\n    save_all_weights(m1, 'model.h5')\n    # new model\n    m2 = make_model()\n    # load all weights\n    load_all_weights(m2, 'model.h5')\n    # check weights\n    assert_allclose(K.get_value(m2.layers[1].kernel)[0, 0:4], [1, 3, 3, 7])\n    # check optimizer weights\n    assert_allclose(K.get_value(m2.optimizer.weights[3])[0, 0:3], [4, 2, 0])\n    os.remove('model.h5')\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/keras_contrib/layers/advanced_activations/test_pelu.py,0,"b""import pytest\r\nfrom keras_contrib.utils.test_utils import layer_test\r\nfrom keras_contrib.layers import PELU\r\n\r\n\r\n@pytest.mark.parametrize('kwargs', [{}, {'shared_axes': 1}])\r\ndef test_pelu(kwargs):\r\n    layer_test(PELU, kwargs=kwargs,\r\n               input_shape=(2, 3, 4))\r\n\r\n\r\nif __name__ == '__main__':\r\n    pytest.main([__file__])\r\n"""
tests/keras_contrib/layers/advanced_activations/test_sinerelu.py,0,"b""import pytest\r\nfrom keras_contrib.utils.test_utils import layer_test\r\nfrom keras_contrib.layers import SineReLU\r\n\r\n\r\n@pytest.mark.parametrize('epsilon', [0.0025, 0.0035, 0.0045])\r\ndef test_sine_relu(epsilon):\r\n    layer_test(SineReLU, kwargs={'epsilon': epsilon}, input_shape=(2, 3, 4))\r\n\r\n\r\nif __name__ == '__main__':\r\n    pytest.main([__file__])\r\n"""
tests/keras_contrib/layers/advanced_activations/test_srelu.py,0,"b""import pytest\r\nfrom keras_contrib.utils.test_utils import layer_test\r\nfrom keras_contrib.layers import SReLU\r\n\r\n\r\n@pytest.mark.parametrize('kwargs', [{}, {'shared_axes': 1}])\r\ndef test_srelu(kwargs):\r\n    layer_test(SReLU, kwargs=kwargs, input_shape=(2, 3, 4))\r\n\r\n\r\nif __name__ == '__main__':\r\n    pytest.main([__file__])\r\n"""
tests/keras_contrib/layers/advanced_activations/test_swish.py,0,"b""import pytest\r\nfrom keras_contrib.utils.test_utils import layer_test\r\nfrom keras_contrib.layers import Swish\r\n\r\n\r\n@pytest.mark.parametrize('trainable', [False, True])\r\ndef test_swish(trainable):\r\n    layer_test(Swish, kwargs={'beta': 1.0, 'trainable': trainable},\r\n               input_shape=(2, 3, 4))\r\n\r\n\r\nif __name__ == '__main__':\r\n    pytest.main([__file__])\r\n"""
tests/keras_contrib/layers/convolutional/test_cosineconvolution2d.py,0,"b""import numpy as np\r\nimport pytest\r\nfrom keras import backend as K\r\nfrom keras.models import Sequential\r\nfrom numpy.testing import assert_allclose\r\n\r\nfrom keras_contrib.utils.test_utils import layer_test\r\nfrom keras_contrib.layers import CosineConvolution2D\r\n\r\n# TensorFlow does not support full convolution.\r\nif K.backend() == 'theano':\r\n    _convolution_border_modes = ['valid', 'same']\r\n    data_format = 'channels_first'\r\nelse:\r\n    _convolution_border_modes = ['valid', 'same']\r\n    data_format = 'channels_last'\r\n\r\n\r\n@pytest.mark.parametrize('border_mode', _convolution_border_modes)\r\n@pytest.mark.parametrize('subsample', [(1, 1), (2, 2)])\r\n@pytest.mark.parametrize('use_bias_mode', [True, False])\r\n@pytest.mark.parametrize('use_regularizer', [True, False])\r\ndef test_cosineconvolution_2d(border_mode,\r\n                              subsample,\r\n                              use_bias_mode,\r\n                              use_regularizer):\r\n    num_samples = 2\r\n    num_filter = 2\r\n    stack_size = 3\r\n    num_row = 10\r\n    num_col = 6\r\n\r\n    if border_mode == 'same' and subsample != (1, 1):\r\n        return\r\n\r\n    kwargs = {'filters': num_filter,\r\n              'kernel_size': (3, 3),\r\n              'padding': border_mode,\r\n              'strides': subsample,\r\n              'use_bias': use_bias_mode,\r\n              'data_format': data_format}\r\n    if use_regularizer:\r\n        kwargs.update({'kernel_regularizer': 'l2',\r\n                       'bias_regularizer': 'l2',\r\n                       'activity_regularizer': 'l2'})\r\n\r\n    layer_test(CosineConvolution2D,\r\n               kwargs=kwargs,\r\n               input_shape=(num_samples, num_row, num_col, stack_size))\r\n\r\n\r\ndef test_cosineconvolution_2d_correctness():\r\n    if data_format == 'channels_first':\r\n        X = np.random.randn(1, 3, 5, 5)\r\n        input_dim = (3, 5, 5)\r\n        W0 = X[:, :, ::-1, ::-1]\r\n    elif data_format == 'channels_last':\r\n        X = np.random.randn(1, 5, 5, 3)\r\n        input_dim = (5, 5, 3)\r\n        W0 = X[0, :, :, :, None]\r\n\r\n    model = Sequential()\r\n    model.add(CosineConvolution2D(1, (5, 5), use_bias=True,\r\n                                  input_shape=input_dim,\r\n                                  data_format=data_format))\r\n    model.compile(loss='mse', optimizer='rmsprop')\r\n    W = model.get_weights()\r\n    W[0] = W0\r\n    W[1] = np.asarray([1.])\r\n    model.set_weights(W)\r\n    out = model.predict(X)\r\n    assert_allclose(out, np.ones((1, 1, 1, 1), dtype=K.floatx()), atol=1e-5)\r\n\r\n    model = Sequential()\r\n    model.add(CosineConvolution2D(1, (5, 5),\r\n                                  use_bias=False,\r\n                                  input_shape=input_dim,\r\n                                  data_format=data_format))\r\n    model.compile(loss='mse', optimizer='rmsprop')\r\n    W = model.get_weights()\r\n    W[0] = -2 * W0\r\n    model.set_weights(W)\r\n    out = model.predict(X)\r\n    assert_allclose(out, -np.ones((1, 1, 1, 1), dtype=K.floatx()), atol=1e-5)\r\n\r\n\r\nif __name__ == '__main__':\r\n    pytest.main([__file__])\r\n"""
tests/keras_contrib/layers/convolutional/test_subpixelupscaling.py,0,"b""import numpy as np\r\nimport pytest\r\nfrom keras import backend as K\r\n\r\nfrom keras_contrib import backend as KC\r\nfrom keras_contrib.layers import SubPixelUpscaling\r\nfrom keras_contrib.utils.test_utils import layer_test\r\n\r\n# TensorFlow does not support full convolution.\r\nif K.backend() == 'theano':\r\n    _convolution_border_modes = ['valid', 'same']\r\n    data_format = 'channels_first'\r\nelse:\r\n    _convolution_border_modes = ['valid', 'same']\r\n    data_format = 'channels_last'\r\n\r\n\r\n@pytest.mark.parametrize('scale_factor', [2, 3, 4])\r\ndef test_sub_pixel_upscaling(scale_factor):\r\n    num_samples = 2\r\n    num_row = 16\r\n    num_col = 16\r\n    input_dtype = K.floatx()\r\n\r\n    nb_channels = 4 * (scale_factor ** 2)\r\n    input_data = np.random.random((num_samples, nb_channels, num_row, num_col))\r\n    input_data = input_data.astype(input_dtype)\r\n\r\n    if K.image_data_format() == 'channels_last':\r\n        input_data = input_data.transpose((0, 2, 3, 1))\r\n\r\n    input_tensor = K.variable(input_data)\r\n    expected_output = K.eval(KC.depth_to_space(input_tensor,\r\n                                               scale=scale_factor))\r\n\r\n    layer_test(SubPixelUpscaling,\r\n               kwargs={'scale_factor': scale_factor},\r\n               input_data=input_data,\r\n               expected_output=expected_output,\r\n               expected_output_dtype=K.floatx())\r\n\r\n\r\nif __name__ == '__main__':\r\n    pytest.main([__file__])\r\n"""
tests/keras_contrib/layers/normalization/test_groupnormalization.py,0,"b""import numpy as np\r\nimport pytest\r\nfrom keras import backend as K\r\nfrom keras import regularizers\r\nfrom keras.layers import Input\r\nfrom keras.models import Sequential, Model\r\nfrom numpy.testing import assert_allclose\r\n\r\nfrom keras_contrib.layers import GroupNormalization\r\nfrom keras_contrib.utils.test_utils import layer_test\r\n\r\ninput_1 = np.arange(10)\r\ninput_2 = np.zeros(10)\r\ninput_3 = np.ones(10)\r\ninput_shapes = [np.ones((10, 10)), np.ones((10, 10, 10))]\r\n\r\n\r\ndef test_basic_groupnorm():\r\n    layer_test(GroupNormalization,\r\n               kwargs={'groups': 2,\r\n                       'epsilon': 0.1,\r\n                       'gamma_regularizer': regularizers.l2(0.01),\r\n                       'beta_regularizer': regularizers.l2(0.01)},\r\n               input_shape=(3, 4, 2))\r\n    layer_test(GroupNormalization,\r\n               kwargs={'groups': 2,\r\n                       'epsilon': 0.1,\r\n                       'axis': 1},\r\n               input_shape=(3, 4, 2))\r\n    layer_test(GroupNormalization,\r\n               kwargs={'groups': 2,\r\n                       'gamma_initializer': 'ones',\r\n                       'beta_initializer': 'ones'},\r\n               input_shape=(3, 4, 2, 4))\r\n    if K.backend() != 'theano':\r\n        layer_test(GroupNormalization,\r\n                   kwargs={'groups': 2,\r\n                           'axis': 1,\r\n                           'scale': False,\r\n                           'center': False},\r\n                   input_shape=(3, 4, 2, 4))\r\n\r\n\r\ndef test_groupnorm_correctness_1d():\r\n    model = Sequential()\r\n    norm = GroupNormalization(input_shape=(10,), groups=2)\r\n    model.add(norm)\r\n    model.compile(loss='mse', optimizer='rmsprop')\r\n\r\n    # centered on 5.0, variance 10.0\r\n    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10))\r\n    model.fit(x, x, epochs=5, verbose=0)\r\n    out = model.predict(x)\r\n    out -= K.eval(norm.beta)\r\n    out /= K.eval(norm.gamma)\r\n\r\n    assert_allclose(out.mean(), 0.0, atol=1e-1)\r\n    assert_allclose(out.std(), 1.0, atol=1e-1)\r\n\r\n\r\ndef test_groupnorm_correctness_2d():\r\n    model = Sequential()\r\n    norm = GroupNormalization(axis=1, input_shape=(10, 6), groups=2)\r\n    model.add(norm)\r\n    model.compile(loss='mse', optimizer='rmsprop')\r\n\r\n    # centered on 5.0, variance 10.0\r\n    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10, 6))\r\n    model.fit(x, x, epochs=5, verbose=0)\r\n    out = model.predict(x)\r\n    out -= np.reshape(K.eval(norm.beta), (1, 10, 1))\r\n    out /= np.reshape(K.eval(norm.gamma), (1, 10, 1))\r\n\r\n    assert_allclose(out.mean(axis=(0, 2)), 0.0, atol=1.1e-1)\r\n    assert_allclose(out.std(axis=(0, 2)), 1.0, atol=1.1e-1)\r\n\r\n\r\ndef test_groupnorm_correctness_2d_different_groups():\r\n    norm1 = GroupNormalization(axis=1, input_shape=(10, 6), groups=2)\r\n    norm2 = GroupNormalization(axis=1, input_shape=(10, 6), groups=1)\r\n    norm3 = GroupNormalization(axis=1, input_shape=(10, 6), groups=10)\r\n\r\n    model = Sequential()\r\n    model.add(norm1)\r\n    model.compile(loss='mse', optimizer='rmsprop')\r\n\r\n    # centered on 5.0, variance 10.0\r\n    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10, 6))\r\n    model.fit(x, x, epochs=5, verbose=0)\r\n    out = model.predict(x)\r\n    out -= np.reshape(K.eval(norm1.beta), (1, 10, 1))\r\n    out /= np.reshape(K.eval(norm1.gamma), (1, 10, 1))\r\n\r\n    assert_allclose(out.mean(axis=(0, 2)), 0.0, atol=1.1e-1)\r\n    assert_allclose(out.std(axis=(0, 2)), 1.0, atol=1.1e-1)\r\n\r\n    model = Sequential()\r\n    model.add(norm2)\r\n    model.compile(loss='mse', optimizer='rmsprop')\r\n\r\n    # centered on 5.0, variance 10.0\r\n    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10, 6))\r\n    model.fit(x, x, epochs=5, verbose=0)\r\n    out = model.predict(x)\r\n    out -= np.reshape(K.eval(norm2.beta), (1, 10, 1))\r\n    out /= np.reshape(K.eval(norm2.gamma), (1, 10, 1))\r\n\r\n    assert_allclose(out.mean(axis=(0, 2)), 0.0, atol=1.1e-1)\r\n    assert_allclose(out.std(axis=(0, 2)), 1.0, atol=1.1e-1)\r\n\r\n    model = Sequential()\r\n    model.add(norm3)\r\n    model.compile(loss='mse', optimizer='rmsprop')\r\n\r\n    # centered on 5.0, variance 10.0\r\n    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10, 6))\r\n    model.fit(x, x, epochs=5, verbose=0)\r\n    out = model.predict(x)\r\n    out -= np.reshape(K.eval(norm3.beta), (1, 10, 1))\r\n    out /= np.reshape(K.eval(norm3.gamma), (1, 10, 1))\r\n\r\n    assert_allclose(out.mean(axis=(0, 2)), 0.0, atol=1.1e-1)\r\n    assert_allclose(out.std(axis=(0, 2)), 1.0, atol=1.1e-1)\r\n\r\n\r\ndef test_groupnorm_mode_twice():\r\n    # This is a regression test for issue #4881 with the old\r\n    # batch normalization functions in the Theano backend.\r\n    model = Sequential()\r\n    model.add(GroupNormalization(input_shape=(10, 5, 5),\r\n                                 axis=1,\r\n                                 groups=2))\r\n    model.add(GroupNormalization(input_shape=(10, 5, 5),\r\n                                 axis=1,\r\n                                 groups=2))\r\n    model.compile(loss='mse', optimizer='sgd')\r\n\r\n    x = np.random.normal(loc=5.0, scale=10.0, size=(20, 10, 5, 5))\r\n    model.fit(x, x, epochs=1, verbose=0)\r\n    model.predict(x)\r\n\r\n\r\ndef test_groupnorm_convnet():\r\n    model = Sequential()\r\n    norm = GroupNormalization(axis=1,\r\n                              input_shape=(3, 4, 4),\r\n                              groups=3)\r\n    model.add(norm)\r\n    model.compile(loss='mse', optimizer='sgd')\r\n\r\n    # centered on 5.0, variance 10.0\r\n    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))\r\n    model.fit(x, x, epochs=4, verbose=0)\r\n    out = model.predict(x)\r\n    out -= np.reshape(K.eval(norm.beta), (1, 3, 1, 1))\r\n    out /= np.reshape(K.eval(norm.gamma), (1, 3, 1, 1))\r\n\r\n    assert_allclose(np.mean(out, axis=(0, 2, 3)), 0.0, atol=1e-1)\r\n    assert_allclose(np.std(out, axis=(0, 2, 3)), 1.0, atol=1e-1)\r\n\r\n\r\n@pytest.mark.skipif((K.backend() == 'theano'),\r\n                    reason='Bug with theano backend')\r\ndef test_groupnorm_convnet_no_center_no_scale():\r\n    model = Sequential()\r\n    norm = GroupNormalization(axis=-1, center=False, scale=False,\r\n                              input_shape=(3, 4, 4), groups=2)\r\n    model.add(norm)\r\n    model.compile(loss='mse', optimizer='sgd')\r\n\r\n    # centered on 5.0, variance 10.0\r\n    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))\r\n    model.fit(x, x, epochs=4, verbose=0)\r\n    out = model.predict(x)\r\n\r\n    assert_allclose(np.mean(out, axis=(0, 2, 3)), 0.0, atol=1e-1)\r\n    assert_allclose(np.std(out, axis=(0, 2, 3)), 1.0, atol=1e-1)\r\n\r\n\r\ndef test_shared_groupnorm():\r\n    '''Test that a GN layer can be shared\r\n    across different data streams.\r\n    '''\r\n    # Test single layer reuse\r\n    bn = GroupNormalization(input_shape=(10,), groups=2)\r\n    x1 = Input(shape=(10,))\r\n    bn(x1)\r\n\r\n    x2 = Input(shape=(10,))\r\n    y2 = bn(x2)\r\n\r\n    x = np.random.normal(loc=5.0, scale=10.0, size=(2, 10))\r\n    model = Model(x2, y2)\r\n    assert len(model.updates) == 0\r\n    model.compile('sgd', 'mse')\r\n    model.train_on_batch(x, x)\r\n\r\n    # Test model-level reuse\r\n    x3 = Input(shape=(10,))\r\n    y3 = model(x3)\r\n    new_model = Model(x3, y3)\r\n    assert len(model.updates) == 0\r\n    new_model.compile('sgd', 'mse')\r\n    new_model.train_on_batch(x, x)\r\n\r\n\r\ndef test_that_trainable_disables_updates():\r\n    val_a = np.random.random((10, 4))\r\n    val_out = np.random.random((10, 4))\r\n\r\n    a = Input(shape=(4,))\r\n    layer = GroupNormalization(input_shape=(4,), groups=2)\r\n    b = layer(a)\r\n    model = Model(a, b)\r\n\r\n    model.trainable = False\r\n    assert len(model.updates) == 0\r\n\r\n    model.compile('sgd', 'mse')\r\n    assert len(model.updates) == 0\r\n\r\n    x1 = model.predict(val_a)\r\n    model.train_on_batch(val_a, val_out)\r\n    x2 = model.predict(val_a)\r\n    assert_allclose(x1, x2, atol=1e-7)\r\n\r\n    model.trainable = True\r\n    model.compile('sgd', 'mse')\r\n    assert len(model.updates) == 0\r\n\r\n    model.train_on_batch(val_a, val_out)\r\n    x2 = model.predict(val_a)\r\n    assert np.abs(np.sum(x1 - x2)) > 1e-5\r\n\r\n    layer.trainable = False\r\n    model.compile('sgd', 'mse')\r\n    assert len(model.updates) == 0\r\n\r\n    x1 = model.predict(val_a)\r\n    model.train_on_batch(val_a, val_out)\r\n    x2 = model.predict(val_a)\r\n    assert_allclose(x1, x2, atol=1e-7)\r\n\r\n\r\nif __name__ == '__main__':\r\n    pytest.main([__file__])\r\n"""
tests/keras_contrib/layers/normalization/test_instancenormalization.py,0,"b""import numpy as np\r\nimport pytest\r\nfrom keras import backend as K\r\nfrom keras.layers import Input\r\nfrom keras.models import Sequential, Model\r\nfrom numpy.testing import assert_allclose\r\n\r\nfrom keras_contrib.layers import InstanceNormalization\r\nfrom keras_contrib.utils.test_utils import layer_test\r\n\r\ninput_1 = np.arange(10)\r\ninput_2 = np.zeros(10)\r\ninput_3 = np.ones(10)\r\ninput_shapes = [np.ones((10, 10)), np.ones((10, 10, 10))]\r\n\r\n\r\ndef basic_instancenorm_test():\r\n    from keras import regularizers\r\n    layer_test(InstanceNormalization,\r\n               kwargs={'epsilon': 0.1,\r\n                       'gamma_regularizer': regularizers.l2(0.01),\r\n                       'beta_regularizer': regularizers.l2(0.01)},\r\n               input_shape=(3, 4, 2))\r\n    layer_test(InstanceNormalization,\r\n               kwargs={'gamma_initializer': 'ones',\r\n                       'beta_initializer': 'ones'},\r\n               input_shape=(3, 4, 2))\r\n    layer_test(InstanceNormalization,\r\n               kwargs={'scale': False, 'center': False},\r\n               input_shape=(3, 3))\r\n\r\n\r\n@pytest.mark.parametrize('input_shape,axis', [((10, 1), -1),\r\n                                              ((10,), None)])\r\ndef test_instancenorm_correctness_rank2(input_shape, axis):\r\n    model = Sequential()\r\n    norm = InstanceNormalization(input_shape=input_shape, axis=axis)\r\n    model.add(norm)\r\n    model.compile(loss='mse', optimizer='sgd')\r\n\r\n    # centered on 5.0, variance 10.0\r\n    x = np.random.normal(loc=5.0, scale=10.0, size=(1000,) + input_shape)\r\n    model.fit(x, x, epochs=4, verbose=0)\r\n    out = model.predict(x)\r\n    out -= K.eval(norm.beta)\r\n    out /= K.eval(norm.gamma)\r\n\r\n    assert_allclose(out.mean(), 0.0, atol=1e-1)\r\n    assert_allclose(out.std(), 1.0, atol=1e-1)\r\n\r\n\r\ndef test_instancenorm_training_argument():\r\n    bn1 = InstanceNormalization(input_shape=(10,))\r\n    x1 = Input(shape=(10,))\r\n    y1 = bn1(x1, training=True)\r\n\r\n    model1 = Model(x1, y1)\r\n    np.random.seed(123)\r\n    x = np.random.normal(loc=5.0, scale=10.0, size=(20, 10))\r\n    output_a = model1.predict(x)\r\n\r\n    model1.compile(loss='mse', optimizer='rmsprop')\r\n    model1.fit(x, x, epochs=1, verbose=0)\r\n    output_b = model1.predict(x)\r\n    assert np.abs(np.sum(output_a - output_b)) > 0.1\r\n    assert_allclose(output_b.mean(), 0.0, atol=1e-1)\r\n    assert_allclose(output_b.std(), 1.0, atol=1e-1)\r\n\r\n    bn2 = InstanceNormalization(input_shape=(10,))\r\n    x2 = Input(shape=(10,))\r\n    bn2(x2, training=False)\r\n\r\n\r\ndef test_instancenorm_convnet():\r\n    model = Sequential()\r\n    norm = InstanceNormalization(axis=1, input_shape=(3, 4, 4))\r\n    model.add(norm)\r\n    model.compile(loss='mse', optimizer='sgd')\r\n\r\n    # centered on 5.0, variance 10.0\r\n    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))\r\n    model.fit(x, x, epochs=4, verbose=0)\r\n    out = model.predict(x)\r\n    out -= np.reshape(K.eval(norm.beta), (1, 3, 1, 1))\r\n    out /= np.reshape(K.eval(norm.gamma), (1, 3, 1, 1))\r\n\r\n    assert_allclose(np.mean(out, axis=(0, 2, 3)), 0.0, atol=1e-1)\r\n    assert_allclose(np.std(out, axis=(0, 2, 3)), 1.0, atol=1e-1)\r\n\r\n\r\ndef test_shared_instancenorm():\r\n    '''Test that a IN layer can be shared\r\n    across different data streams.\r\n    '''\r\n    # Test single layer reuse\r\n    bn = InstanceNormalization(input_shape=(10,))\r\n    x1 = Input(shape=(10,))\r\n    bn(x1)\r\n\r\n    x2 = Input(shape=(10,))\r\n    y2 = bn(x2)\r\n\r\n    x = np.random.normal(loc=5.0, scale=10.0, size=(2, 10))\r\n    model = Model(x2, y2)\r\n    model.compile('sgd', 'mse')\r\n    model.train_on_batch(x, x)\r\n\r\n    # Test model-level reuse\r\n    x3 = Input(shape=(10,))\r\n    y3 = model(x3)\r\n    new_model = Model(x3, y3)\r\n    new_model.compile('sgd', 'mse')\r\n    new_model.train_on_batch(x, x)\r\n\r\n\r\ndef test_instancenorm_perinstancecorrectness():\r\n    model = Sequential()\r\n    norm = InstanceNormalization(input_shape=(10,))\r\n    model.add(norm)\r\n    model.compile(loss='mse', optimizer='sgd')\r\n\r\n    # bimodal distribution\r\n    z = np.random.normal(loc=5.0, scale=10.0, size=(2, 10))\r\n    y = np.random.normal(loc=-5.0, scale=17.0, size=(2, 10))\r\n    x = np.append(z, y)\r\n    x = np.reshape(x, (4, 10))\r\n    model.fit(x, x, epochs=4, batch_size=4, verbose=1)\r\n    out = model.predict(x)\r\n    out -= K.eval(norm.beta)\r\n    out /= K.eval(norm.gamma)\r\n\r\n    # verify that each instance in the batch is individually normalized\r\n    for i in range(4):\r\n        instance = out[i]\r\n        assert_allclose(instance.mean(), 0.0, atol=1e-1)\r\n        assert_allclose(instance.std(), 1.0, atol=1e-1)\r\n\r\n    # if each instance is normalized, so should the batch\r\n    assert_allclose(out.mean(), 0.0, atol=1e-1)\r\n    assert_allclose(out.std(), 1.0, atol=1e-1)\r\n\r\n\r\ndef test_instancenorm_perchannel_correctness():\r\n\r\n    # have each channel with a different average and std\r\n    x = np.random.normal(loc=5.0, scale=2.0, size=(10, 1, 4, 4))\r\n    y = np.random.normal(loc=10.0, scale=3.0, size=(10, 1, 4, 4))\r\n    z = np.random.normal(loc=-5.0, scale=5.0, size=(10, 1, 4, 4))\r\n\r\n    batch = np.append(x, y, axis=1)\r\n    batch = np.append(batch, z, axis=1)\r\n\r\n    # this model does not provide a normalization axis\r\n    model = Sequential()\r\n    norm = InstanceNormalization(axis=None,\r\n                                 input_shape=(3, 4, 4),\r\n                                 center=False,\r\n                                 scale=False)\r\n    model.add(norm)\r\n    model.compile(loss='mse', optimizer='sgd')\r\n    model.fit(batch, batch, epochs=4, verbose=0)\r\n    out = model.predict(batch)\r\n\r\n    # values will not be normalized per-channel\r\n    for instance in range(10):\r\n        for channel in range(3):\r\n            activations = out[instance, channel]\r\n            assert abs(activations.mean()) > 1e-2\r\n            assert abs(activations.std() - 1.0) > 1e-6\r\n\r\n        # but values are still normalized per-instance\r\n        activations = out[instance]\r\n        assert_allclose(activations.mean(), 0.0, atol=1e-1)\r\n        assert_allclose(activations.std(), 1.0, atol=1e-1)\r\n\r\n    # this model sets the channel as a normalization axis\r\n    model = Sequential()\r\n    norm = InstanceNormalization(axis=1,\r\n                                 input_shape=(3, 4, 4),\r\n                                 center=False,\r\n                                 scale=False)\r\n    model.add(norm)\r\n    model.compile(loss='mse', optimizer='sgd')\r\n\r\n    model.fit(batch, batch, epochs=4, verbose=0)\r\n    out = model.predict(batch)\r\n\r\n    # values are now normalized per-channel\r\n    for instance in range(10):\r\n        for channel in range(3):\r\n            activations = out[instance, channel]\r\n            assert_allclose(activations.mean(), 0.0, atol=1e-1)\r\n            assert_allclose(activations.std(), 1.0, atol=1e-1)\r\n\r\n\r\nif __name__ == '__main__':\r\n    pytest.main([__file__])\r\n"""
