file_path,api_count,code
app.py,0,"b'import os\nimport re\nimport math\nimport sys\nimport shutil\nimport json\nimport traceback\nimport PIL.Image as PilImage\nimport threading\nimport tkinter as tk\nfrom tkinter import messagebox\nfrom tkinter import ttk\nfrom tkinter import filedialog\nfrom constants import *\nfrom config import ModelConfig, OUTPUT_SHAPE1_MAP, NETWORK_MAP, DataAugmentationEntity, PretreatmentEntity, CORE_VERSION\nfrom make_dataset import DataSets\nfrom predict_testing import Predict\nfrom trains import Trains\nfrom category import category_extract, SIMPLE_CATEGORY_MODEL\nfrom gui.utils import LayoutGUI\nfrom gui.data_augmentation import DataAugmentationDialog\nfrom gui.pretreatment import PretreatmentDialog\n\n\nclass Wizard:\n\n    job: threading.Thread\n    current_task: Trains = None\n    is_task_running: bool = False\n    data_augmentation_entity = DataAugmentationEntity()\n    pretreatment_entity = PretreatmentEntity()\n    extract_regex = "".*?(?=_)""\n    label_from = LabelFrom.FileName\n\n    def __init__(self, parent: tk.Tk):\n        self.layout = {\n            \'global\': {\n                \'start\': {\'x\': 15, \'y\': 20},\n                \'space\': {\'x\': 15, \'y\': 25},\n                \'tiny_space\': {\'x\': 5, \'y\': 10}\n            }\n        }\n        self.parent = parent\n        self.parent.iconbitmap(Wizard.resource_path(""resource/icon.ico""))\n        self.current_project: str = """"\n        self.project_root_path = ""./projects""\n        if not os.path.exists(self.project_root_path):\n            os.makedirs(self.project_root_path)\n        self.parent.title(\'Image Classification Wizard Tool based on Deep Learning\')\n        self.parent.resizable(width=False, height=False)\n        self.window_width = 815\n        self.window_height = 700\n        self.layout_utils = LayoutGUI(self.layout, self.window_width)\n        screenwidth = self.parent.winfo_screenwidth()\n        screenheight = self.parent.winfo_screenheight()\n        size = \'%dx%d+%d+%d\' % (\n            self.window_width,\n            self.window_height,\n            (screenwidth - self.window_width) / 2,\n            (screenheight - self.window_height) / 2\n        )\n\n        self.parent.bind(\'<Button-1>\', lambda x: self.blank_click(x))\n\n        # ============================= Menu 1 =====================================\n        self.menubar = tk.Menu(self.parent)\n        self.data_menu = tk.Menu(self.menubar, tearoff=False)\n        self.help_menu = tk.Menu(self.menubar, tearoff=False)\n        self.system_menu = tk.Menu(self.menubar, tearoff=False)\n        self.edit_var = tk.DoubleVar()\n        self.memory_usage_menu = tk.Menu(self.menubar, tearoff=False)\n        self.memory_usage_menu.add_radiobutton(label=""50%"", variable=self.edit_var, value=0.5)\n        self.memory_usage_menu.add_radiobutton(label=""60%"", variable=self.edit_var, value=0.6)\n        self.memory_usage_menu.add_radiobutton(label=""70%"", variable=self.edit_var, value=0.7)\n        self.memory_usage_menu.add_radiobutton(label=""80%"", variable=self.edit_var, value=0.8)\n\n        self.menubar.add_cascade(label=""System"", menu=self.system_menu)\n        self.system_menu.add_cascade(label=""Memory Usage"", menu=self.memory_usage_menu)\n\n        self.data_menu.add_command(label=""Data Augmentation"", command=lambda: self.popup_data_augmentation())\n        self.data_menu.add_command(label=""Pretreatment"", command=lambda: self.popup_pretreatment())\n        self.data_menu.add_separator()\n        self.data_menu.add_command(label=""Clear Dataset"", command=lambda: self.clear_dataset())\n        self.menubar.add_cascade(label=""Data"", menu=self.data_menu)\n\n        self.help_menu.add_command(label=""About"", command=lambda: self.popup_about())\n        self.menubar.add_cascade(label=""Help"", menu=self.help_menu)\n\n        self.parent.config(menu=self.menubar)\n\n        # ============================= Group 1 =====================================\n        self.label_frame_source = ttk.Labelframe(self.parent, text=\'Sample Source\')\n        self.label_frame_source.place(\n            x=self.layout[\'global\'][\'start\'][\'x\'],\n            y=self.layout[\'global\'][\'start\'][\'y\'],\n            width=790,\n            height=150\n        )\n\n        # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\xba\x90\xe8\xb7\xaf\xe5\xbe\x84 - \xe6\xa0\x87\xe7\xad\xbe\n        self.dataset_train_path_text = ttk.Label(self.parent, text=\'Training Path\', anchor=tk.W)\n        self.layout_utils.inside_widget(\n            src=self.dataset_train_path_text,\n            target=self.label_frame_source,\n            width=90,\n            height=20\n        )\n\n        # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\xba\x90\xe8\xb7\xaf\xe5\xbe\x84 - \xe8\xbe\x93\xe5\x85\xa5\xe6\x8e\xa7\xe4\xbb\xb6\n        self.source_train_path_listbox = tk.Listbox(self.parent, font=(\'\xe5\xbe\xae\xe8\xbd\xaf\xe9\x9b\x85\xe9\xbb\x91\', 9))\n        self.layout_utils.next_to_widget(\n            src=self.source_train_path_listbox,\n            target=self.dataset_train_path_text,\n            width=600,\n            height=50,\n            tiny_space=True\n        )\n        self.source_train_path_listbox.bind(\n            sequence=""<Delete>"",\n            func=lambda x: self.listbox_delete_item_callback(x, self.source_train_path_listbox)\n        )\n        self.listbox_scrollbar(self.source_train_path_listbox)\n\n        # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\xba\x90\xe8\xb7\xaf\xe5\xbe\x84 - \xe6\x8c\x89\xe9\x92\xae\n        self.btn_browse_train = ttk.Button(\n            self.parent, text=\'Browse\', command=lambda: self.browse_dataset(DatasetType.Directory, RunMode.Trains)\n        )\n        self.layout_utils.next_to_widget(\n            src=self.btn_browse_train,\n            target=self.source_train_path_listbox,\n            width=60,\n            height=24,\n            tiny_space=True\n        )\n\n        # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe6\xba\x90\xe8\xb7\xaf\xe5\xbe\x84 - \xe6\xa0\x87\xe7\xad\xbe\n        label_edge = self.layout_utils.object_edge_info(self.dataset_train_path_text)\n        widget_edge = self.layout_utils.object_edge_info(self.source_train_path_listbox)\n        self.dataset_validation_path_text = ttk.Label(self.parent, text=\'Validation Path\', anchor=tk.W)\n        self.dataset_validation_path_text.place(\n            x=label_edge[\'x\'],\n            y=widget_edge[\'edge_y\'] + self.layout[\'global\'][\'space\'][\'y\'] / 2,\n            width=90,\n            height=20\n        )\n\n        # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe6\xba\x90\xe8\xb7\xaf\xe5\xbe\x84 - \xe8\xbe\x93\xe5\x85\xa5\xe6\x8e\xa7\xe4\xbb\xb6\n        self.source_validation_path_listbox = tk.Listbox(self.parent, font=(\'\xe5\xbe\xae\xe8\xbd\xaf\xe9\x9b\x85\xe9\xbb\x91\', 9))\n        self.layout_utils.next_to_widget(\n            src=self.source_validation_path_listbox,\n            target=self.dataset_validation_path_text,\n            width=600,\n            height=50,\n            tiny_space=True\n        )\n        self.source_validation_path_listbox.bind(\n            sequence=""<Delete>"",\n            func=lambda x: self.listbox_delete_item_callback(x, self.source_validation_path_listbox)\n        )\n        self.listbox_scrollbar(self.source_validation_path_listbox)\n\n        # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\xba\x90\xe8\xb7\xaf\xe5\xbe\x84 - \xe6\x8c\x89\xe9\x92\xae\n        self.btn_browse_validation = ttk.Button(\n            self.parent, text=\'Browse\', command=lambda: self.browse_dataset(DatasetType.Directory, RunMode.Validation)\n        )\n        self.layout_utils.next_to_widget(\n            src=self.btn_browse_validation,\n            target=self.source_validation_path_listbox,\n            width=60,\n            height=24,\n            tiny_space=True\n        )\n\n        # ============================= Group 2 =====================================\n        self.label_frame_neu = ttk.Labelframe(self.parent, text=\'Neural Network\')\n        self.layout_utils.below_widget(\n            src=self.label_frame_neu,\n            target=self.label_frame_source,\n            width=790,\n            height=120,\n            tiny_space=False\n        )\n\n        # \xe6\x9c\x80\xe5\xa4\xa7\xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe7\x9b\xae - \xe6\xa0\x87\xe7\xad\xbe\n        self.label_num_text = ttk.Label(self.parent, text=\'Label Num\', anchor=tk.W)\n        self.layout_utils.inside_widget(\n            src=self.label_num_text,\n            target=self.label_frame_neu,\n            width=65,\n            height=20,\n        )\n\n        # \xe6\x9c\x80\xe5\xa4\xa7\xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe7\x9b\xae - \xe6\xbb\x9a\xe5\x8a\xa8\xe6\xa1\x86\n        self.label_num_spin = ttk.Spinbox(self.parent, from_=1, to=12)\n        self.label_num_spin.set(1)\n        self.layout_utils.next_to_widget(\n            src=self.label_num_spin,\n            target=self.label_num_text,\n            width=50,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe5\x9b\xbe\xe5\x83\x8f\xe9\x80\x9a\xe9\x81\x93 - \xe6\xa0\x87\xe7\xad\xbe\n        self.channel_text = ttk.Label(self.parent, text=\'Channel\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.channel_text,\n            target=self.label_num_spin,\n            width=50,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe5\x9b\xbe\xe5\x83\x8f\xe9\x80\x9a\xe9\x81\x93 - \xe4\xb8\x8b\xe6\x8b\x89\xe6\xa1\x86\n        self.comb_channel = ttk.Combobox(self.parent, values=(3, 1), state=\'readonly\')\n        self.comb_channel.current(1)\n        self.layout_utils.next_to_widget(\n            src=self.comb_channel,\n            target=self.channel_text,\n            width=38,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82 - \xe6\xa0\x87\xe7\xad\xbe\n        self.neu_cnn_text = ttk.Label(self.parent, text=\'CNN Layer\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.neu_cnn_text,\n            target=self.comb_channel,\n            width=65,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82 - \xe4\xb8\x8b\xe6\x8b\x89\xe6\xa1\x86\n        self.comb_neu_cnn = ttk.Combobox(self.parent, values=[_.name for _ in CNNNetwork], state=\'readonly\')\n        self.comb_neu_cnn.current(0)\n        self.layout_utils.next_to_widget(\n            src=self.comb_neu_cnn,\n            target=self.neu_cnn_text,\n            width=80,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe5\xbe\xaa\xe7\x8e\xaf\xe5\xb1\x82 - \xe6\xa0\x87\xe7\xad\xbe\n        self.neu_recurrent_text = ttk.Label(self.parent, text=\'Recurrent Layer\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.neu_recurrent_text,\n            target=self.comb_neu_cnn,\n            width=95,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe5\xbe\xaa\xe7\x8e\xaf\xe5\xb1\x82 - \xe4\xb8\x8b\xe6\x8b\x89\xe6\xa1\x86\n        self.comb_recurrent = ttk.Combobox(self.parent, values=[_.name for _ in RecurrentNetwork], state=\'readonly\')\n        self.comb_recurrent.current(1)\n        self.layout_utils.next_to_widget(\n            src=self.comb_recurrent,\n            target=self.neu_recurrent_text,\n            width=112,\n            height=20,\n            tiny_space=True\n        )\n        self.comb_recurrent.bind(""<<ComboboxSelected>>"", lambda x: self.auto_loss(x))\n\n        # \xe5\xbe\xaa\xe7\x8e\xaf\xe5\xb1\x82\xe5\x8d\x95\xe5\x85\x83\xe6\x95\xb0 - \xe6\xa0\x87\xe7\xad\xbe\n        self.units_num_text = ttk.Label(self.parent, text=\'UnitsNum\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.units_num_text,\n            target=self.comb_recurrent,\n            width=60,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe5\xbe\xaa\xe7\x8e\xaf\xe5\xb1\x82\xe5\x8d\x95\xe5\x85\x83\xe6\x95\xb0 - \xe4\xb8\x8b\xe6\x8b\x89\xe6\xa1\x86\n        self.units_num_spin = ttk.Spinbox(self.parent, from_=16, to=512, increment=16, wrap=True)\n        self.units_num_spin.set(64)\n        self.layout_utils.next_to_widget(\n            src=self.units_num_spin,\n            target=self.units_num_text,\n            width=55,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0 - \xe6\xa0\x87\xe7\xad\xbe\n        self.loss_func_text = ttk.Label(self.parent, text=\'Loss Function\', anchor=tk.W)\n        self.layout_utils.below_widget(\n            src=self.loss_func_text,\n            target=self.label_num_text,\n            width=85,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0 - \xe4\xb8\x8b\xe6\x8b\x89\xe6\xa1\x86\n        self.comb_loss = ttk.Combobox(self.parent, values=[_.name for _ in LossFunction], state=\'readonly\')\n        self.comb_loss.current(1)\n        self.layout_utils.next_to_widget(\n            src=self.comb_loss,\n            target=self.loss_func_text,\n            width=101,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8 - \xe6\xa0\x87\xe7\xad\xbe\n        self.optimizer_text = ttk.Label(self.parent, text=\'Optimizer\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.optimizer_text,\n            target=self.comb_loss,\n            width=60,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8 - \xe4\xb8\x8b\xe6\x8b\x89\xe6\xa1\x86\n        self.comb_optimizer = ttk.Combobox(self.parent, values=[_.name for _ in Optimizer], state=\'readonly\')\n        self.comb_optimizer.current(0)\n        self.layout_utils.next_to_widget(\n            src=self.comb_optimizer,\n            target=self.optimizer_text,\n            width=88,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87 - \xe6\xa0\x87\xe7\xad\xbe\n        self.learning_rate_text = ttk.Label(self.parent, text=\'Learning Rate\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.learning_rate_text,\n            target=self.comb_optimizer,\n            width=85,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87 - \xe6\xbb\x9a\xe5\x8a\xa8\xe6\xa1\x86\n        self.learning_rate_spin = ttk.Spinbox(self.parent, from_=0.00001, to=0.1, increment=\'0.0001\')\n        self.learning_rate_spin.set(0.001)\n        self.layout_utils.next_to_widget(\n            src=self.learning_rate_spin,\n            target=self.learning_rate_text,\n            width=67,\n            height=20,\n            tiny_space=True\n        )\n\n        # Resize - \xe6\xa0\x87\xe7\xad\xbe\n        self.resize_text = ttk.Label(self.parent, text=\'Resize\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.resize_text,\n            target=self.learning_rate_spin,\n            width=36,\n            height=20,\n            tiny_space=False\n        )\n\n        # Resize - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.resize_val = tk.StringVar()\n        self.resize_val.set(\'[150, 50]\')\n        self.resize_entry = ttk.Entry(self.parent, textvariable=self.resize_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.resize_entry,\n            target=self.resize_text,\n            width=60,\n            height=20,\n            tiny_space=True\n        )\n\n        # Size - \xe6\xa0\x87\xe7\xad\xbe\n        self.size_text = ttk.Label(self.parent, text=\'Size\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.size_text,\n            target=self.resize_entry,\n            width=30,\n            height=20,\n            tiny_space=False\n        )\n\n        # Size - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.size_val = tk.StringVar()\n        self.size_val.set(\'[-1, -1]\')\n        self.size_entry = ttk.Entry(self.parent, textvariable=self.size_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.size_entry,\n            target=self.size_text,\n            width=60,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe7\xb1\xbb\xe5\x88\xab - \xe6\xa0\x87\xe7\xad\xbe\n        self.category_text = ttk.Label(self.parent, text=\'Category\', anchor=tk.W)\n        self.layout_utils.below_widget(\n            src=self.category_text,\n            target=self.loss_func_text,\n            width=72,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe7\xb1\xbb\xe5\x88\xab - \xe4\xb8\x8b\xe6\x8b\x89\xe6\xa1\x86\n        self.comb_category = ttk.Combobox(self.parent, values=(\n            \'CUSTOMIZED\',\n            \'NUMERIC\',\n            \'ALPHANUMERIC\',\n            \'ALPHANUMERIC_LOWER\',\n            \'ALPHANUMERIC_UPPER\',\n            \'ALPHABET_LOWER\',\n            \'ALPHABET_UPPER\',\n            \'ALPHABET\',\n            \'ARITHMETIC\',\n            \'FLOAT\',\n            \'CHS_3500\',\n            \'ALPHANUMERIC_CHS_3500_LOWER\',\n            \'DOCUMENT_OCR\'\n        ), state=\'readonly\')\n        self.comb_category.current(1)\n        self.comb_category.bind(""<<ComboboxSelected>>"", lambda x: self.comb_category_callback(x))\n        self.layout_utils.next_to_widget(\n            src=self.comb_category,\n            target=self.category_text,\n            width=225,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe7\xb1\xbb\xe5\x88\xab - \xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.category_val = tk.StringVar()\n        self.category_val.set(\'\')\n        self.category_entry = ttk.Entry(self.parent, textvariable=self.category_val, justify=tk.LEFT, state=tk.DISABLED)\n        self.layout_utils.next_to_widget(\n            src=self.category_entry,\n            target=self.comb_category,\n            width=440,\n            height=20,\n            tiny_space=False\n        )\n\n        # ============================= Group 3 =====================================\n        self.label_frame_train = ttk.Labelframe(self.parent, text=\'Training Configuration\')\n        self.layout_utils.below_widget(\n            src=self.label_frame_train,\n            target=self.label_frame_neu,\n            width=790,\n            height=60,\n            tiny_space=True\n        )\n\n        # \xe4\xbb\xbb\xe5\x8a\xa1\xe5\xae\x8c\xe6\x88\x90\xe6\xa0\x87\xe5\x87\x86 - \xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 - \xe6\xa0\x87\xe7\xad\xbe\n        self.end_acc_text = ttk.Label(self.parent, text=\'End Accuracy\', anchor=tk.W)\n        self.layout_utils.inside_widget(\n            src=self.end_acc_text,\n            target=self.label_frame_train,\n            width=85,\n            height=20,\n        )\n\n        # \xe4\xbb\xbb\xe5\x8a\xa1\xe5\xae\x8c\xe6\x88\x90\xe6\xa0\x87\xe5\x87\x86 - \xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.end_acc_val = tk.DoubleVar()\n        self.end_acc_val.set(0.95)\n        self.end_acc_entry = ttk.Entry(self.parent, textvariable=self.end_acc_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.end_acc_entry,\n            target=self.end_acc_text,\n            width=56,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe4\xbb\xbb\xe5\x8a\xa1\xe5\xae\x8c\xe6\x88\x90\xe6\xa0\x87\xe5\x87\x86 - \xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1 - \xe6\xa0\x87\xe7\xad\xbe\n        self.end_cost_text = ttk.Label(self.parent, text=\'End Cost\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.end_cost_text,\n            target=self.end_acc_entry,\n            width=60,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe4\xbb\xbb\xe5\x8a\xa1\xe5\xae\x8c\xe6\x88\x90\xe6\xa0\x87\xe5\x87\x86 - \xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1 - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.end_cost_val = tk.DoubleVar()\n        self.end_cost_val.set(0.5)\n        self.end_cost_entry = ttk.Entry(self.parent, textvariable=self.end_cost_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.end_cost_entry,\n            target=self.end_cost_text,\n            width=58,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe4\xbb\xbb\xe5\x8a\xa1\xe5\xae\x8c\xe6\x88\x90\xe6\xa0\x87\xe5\x87\x86 - \xe5\xbe\xaa\xe7\x8e\xaf\xe8\xbd\xae\xe6\xac\xa1 - \xe6\xa0\x87\xe7\xad\xbe\n        self.end_epochs_text = ttk.Label(self.parent, text=\'End Epochs\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.end_epochs_text,\n            target=self.end_cost_entry,\n            width=72,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe4\xbb\xbb\xe5\x8a\xa1\xe5\xae\x8c\xe6\x88\x90\xe6\xa0\x87\xe5\x87\x86 - \xe5\xbe\xaa\xe7\x8e\xaf\xe8\xbd\xae\xe6\xac\xa1 - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.end_epochs_spin = ttk.Spinbox(self.parent, from_=0, to=10000)\n        self.end_epochs_spin.set(2)\n        self.layout_utils.next_to_widget(\n            src=self.end_epochs_spin,\n            target=self.end_epochs_text,\n            width=50,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe8\xae\xad\xe7\xbb\x83\xe6\x89\xb9\xe6\xac\xa1\xe5\xa4\xa7\xe5\xb0\x8f - \xe6\xa0\x87\xe7\xad\xbe\n        self.batch_size_text = ttk.Label(self.parent, text=\'Train BatchSize\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.batch_size_text,\n            target=self.end_epochs_spin,\n            width=90,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe8\xae\xad\xe7\xbb\x83\xe6\x89\xb9\xe6\xac\xa1\xe5\xa4\xa7\xe5\xb0\x8f - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.batch_size_val = tk.IntVar()\n        self.batch_size_val.set(64)\n        self.batch_size_entry = ttk.Entry(self.parent, textvariable=self.batch_size_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.batch_size_entry,\n            target=self.batch_size_text,\n            width=40,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe9\xaa\x8c\xe8\xaf\x81\xe6\x89\xb9\xe6\xac\xa1\xe5\xa4\xa7\xe5\xb0\x8f - \xe6\xa0\x87\xe7\xad\xbe\n        self.validation_batch_size_text = ttk.Label(self.parent, text=\'Validation BatchSize\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.validation_batch_size_text,\n            target=self.batch_size_entry,\n            width=120,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe9\xaa\x8c\xe8\xaf\x81\xe6\x89\xb9\xe6\xac\xa1\xe5\xa4\xa7\xe5\xb0\x8f - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.validation_batch_size_val = tk.IntVar()\n        self.validation_batch_size_val.set(300)\n        self.validation_batch_size_entry = ttk.Entry(self.parent, textvariable=self.validation_batch_size_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.validation_batch_size_entry,\n            target=self.validation_batch_size_text,\n            width=40,\n            height=20,\n            tiny_space=True\n        )\n\n        # ============================= Group 5 =====================================\n        self.label_frame_project = ttk.Labelframe(self.parent, text=\'Project Configuration\')\n        self.layout_utils.below_widget(\n            src=self.label_frame_project,\n            target=self.label_frame_train,\n            width=790,\n            height=60,\n            tiny_space=True\n        )\n\n        # \xe9\xa1\xb9\xe7\x9b\xae\xe5\x90\x8d - \xe6\xa0\x87\xe7\xad\xbe\n        self.project_name_text = ttk.Label(self.parent, text=\'Project Name\', anchor=tk.W)\n        self.layout_utils.inside_widget(\n            src=self.project_name_text,\n            target=self.label_frame_project,\n            width=90,\n            height=20\n        )\n\n        # \xe9\xa1\xb9\xe7\x9b\xae\xe5\x90\x8d - \xe4\xb8\x8b\xe6\x8b\x89\xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.comb_project_name = ttk.Combobox(self.parent)\n        self.layout_utils.next_to_widget(\n            src=self.comb_project_name,\n            target=self.project_name_text,\n            width=430,\n            height=20,\n            tiny_space=True\n        )\n        self.comb_project_name.bind(\n            sequence=""<Return>"",\n            func=lambda x: self.project_name_fill_callback(x)\n        )\n        self.comb_project_name.bind(\n            sequence=""<Button-1>"",\n            func=lambda x: self.fetch_projects()\n        )\n        self.comb_project_name.bind(""<<ComboboxSelected>>"", lambda x: self.read_conf(x))\n\n        # \xe4\xbf\x9d\xe5\xad\x98\xe9\x85\x8d\xe7\xbd\xae - \xe6\x8c\x89\xe9\x92\xae\n        self.btn_save_conf = ttk.Button(\n            self.parent, text=\'Save Configuration\', command=lambda: self.save_conf()\n        )\n        self.layout_utils.next_to_widget(\n            src=self.btn_save_conf,\n            target=self.comb_project_name,\n            width=130,\n            height=24,\n            tiny_space=False,\n            offset_y=-2\n        )\n\n        # \xe5\x88\xa0\xe9\x99\xa4\xe9\xa1\xb9\xe7\x9b\xae - \xe6\x8c\x89\xe9\x92\xae\n        self.btn_delete = ttk.Button(\n            self.parent, text=\'Delete\', command=lambda: self.delete_project()\n        )\n        self.layout_utils.next_to_widget(\n            src=self.btn_delete,\n            target=self.btn_save_conf,\n            width=80,\n            height=24,\n            tiny_space=False,\n        )\n\n        # ============================= Group 6 =====================================\n        self.label_frame_dataset = ttk.Labelframe(\n            self.parent, text=\'Sample Dataset\'\n        )\n        self.layout_utils.below_widget(\n            src=self.label_frame_dataset,\n            target=self.label_frame_project,\n            width=790,\n            height=170,\n            tiny_space=True\n        )\n\n        # \xe9\x99\x84\xe5\x8a\xa0\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86 - \xe6\x8c\x89\xe9\x92\xae\n        self.btn_attach_dataset = ttk.Button(\n            self.parent,\n            text=\'Attach Dataset\',\n            command=lambda: self.attach_dataset()\n        )\n        self.layout_utils.inside_widget(\n            src=self.btn_attach_dataset,\n            target=self.label_frame_dataset,\n            width=120,\n            height=24,\n        )\n\n        # \xe9\x99\x84\xe5\x8a\xa0\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86 - \xe6\x98\xbe\xe7\xa4\xba\xe6\xa1\x86\n        self.attach_dataset_val = tk.StringVar()\n        self.attach_dataset_val.set(\'\')\n        self.attach_dataset_entry = ttk.Entry(\n            self.parent, textvariable=self.attach_dataset_val, justify=tk.LEFT, state=tk.DISABLED\n        )\n        self.layout_utils.next_to_widget(\n            src=self.attach_dataset_entry,\n            target=self.btn_attach_dataset,\n            width=420,\n            height=24,\n            tiny_space=True\n        )\n\n        # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe6\x95\xb0\xe7\x9b\xae - \xe6\xa0\x87\xe7\xad\xbe\n        self.validation_num_text = ttk.Label(self.parent, text=\'Validation Set Num\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.validation_num_text,\n            target=self.attach_dataset_entry,\n            width=120,\n            height=20,\n            tiny_space=False,\n            offset_y=2\n        )\n\n        # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe6\x95\xb0\xe7\x9b\xae - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.validation_num_val = tk.IntVar()\n        self.validation_num_val.set(300)\n        self.validation_num_entry = ttk.Entry(self.parent, textvariable=self.validation_num_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.validation_num_entry,\n            target=self.validation_num_text,\n            width=71,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe8\xb7\xaf\xe5\xbe\x84 - \xe6\xa0\x87\xe7\xad\xbe\n        self.dataset_train_path_text = ttk.Label(self.parent, text=\'Training Dataset\', anchor=tk.W)\n        self.layout_utils.below_widget(\n            src=self.dataset_train_path_text,\n            target=self.btn_attach_dataset,\n            width=100,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe8\xb7\xaf\xe5\xbe\x84 - \xe5\x88\x97\xe8\xa1\xa8\xe6\xa1\x86\n        self.dataset_train_listbox = tk.Listbox(self.parent, font=(\'\xe5\xbe\xae\xe8\xbd\xaf\xe9\x9b\x85\xe9\xbb\x91\', 9))\n        self.layout_utils.next_to_widget(\n            src=self.dataset_train_listbox,\n            target=self.dataset_train_path_text,\n            width=640,\n            height=36,\n            tiny_space=False\n        )\n        self.dataset_train_listbox.bind(\n            sequence=""<Delete>"",\n            func=lambda x: self.listbox_delete_item_callback(x, self.dataset_train_listbox)\n        )\n        self.listbox_scrollbar(self.dataset_train_listbox)\n\n        # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe8\xb7\xaf\xe5\xbe\x84 - \xe6\xa0\x87\xe7\xad\xbe\n        label_edge = self.layout_utils.object_edge_info(self.dataset_train_path_text)\n        widget_edge = self.layout_utils.object_edge_info(self.dataset_train_listbox)\n        self.dataset_validation_path_text = ttk.Label(self.parent, text=\'Validation Dataset\', anchor=tk.W)\n        self.dataset_validation_path_text.place(\n            x=label_edge[\'x\'],\n            y=widget_edge[\'edge_y\'] + self.layout[\'global\'][\'space\'][\'y\'] / 2,\n            width=100,\n            height=20\n        )\n\n        # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe8\xb7\xaf\xe5\xbe\x84 - \xe4\xb8\x8b\xe6\x8b\x89\xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.dataset_validation_listbox = tk.Listbox(self.parent, font=(\'\xe5\xbe\xae\xe8\xbd\xaf\xe9\x9b\x85\xe9\xbb\x91\', 9))\n        self.layout_utils.next_to_widget(\n            src=self.dataset_validation_listbox,\n            target=self.dataset_validation_path_text,\n            width=640,\n            height=36,\n            tiny_space=False\n        )\n        self.dataset_validation_listbox.bind(\n            sequence=""<Delete>"",\n            func=lambda x: self.listbox_delete_item_callback(x, self.dataset_validation_listbox)\n        )\n        self.listbox_scrollbar(self.dataset_validation_listbox)\n\n        self.sample_map = {\n            DatasetType.Directory: {\n                RunMode.Trains: self.source_train_path_listbox,\n                RunMode.Validation: self.source_validation_path_listbox\n            },\n            DatasetType.TFRecords: {\n                RunMode.Trains: self.dataset_train_listbox,\n                RunMode.Validation: self.dataset_validation_listbox\n            }\n        }\n\n        # \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83 - \xe6\x8c\x89\xe9\x92\xae\n        self.btn_training = ttk.Button(self.parent, text=\'Start Training\', command=lambda: self.start_training())\n        self.layout_utils.widget_from_right(\n            src=self.btn_training,\n            target=self.label_frame_dataset,\n            width=120,\n            height=24,\n            tiny_space=True\n        )\n\n        # \xe7\xbb\x88\xe6\xad\xa2\xe8\xae\xad\xe7\xbb\x83 - \xe6\x8c\x89\xe9\x92\xae\n        self.btn_stop = ttk.Button(self.parent, text=\'Stop\', command=lambda: self.stop_training())\n        self.button_state(self.btn_stop, tk.DISABLED)\n        self.layout_utils.before_widget(\n            src=self.btn_stop,\n            target=self.btn_training,\n            width=60,\n            height=24,\n            tiny_space=True\n        )\n\n        # \xe7\xbc\x96\xe8\xaf\x91\xe6\xa8\xa1\xe5\x9e\x8b - \xe6\x8c\x89\xe9\x92\xae\n        self.btn_compile = ttk.Button(self.parent, text=\'Compile\', command=lambda: self.compile())\n        self.layout_utils.before_widget(\n            src=self.btn_compile,\n            target=self.btn_stop,\n            width=80,\n            height=24,\n            tiny_space=True\n        )\n\n        # \xe6\x89\x93\xe5\x8c\x85\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86 - \xe6\x8c\x89\xe9\x92\xae\n        self.btn_make_dataset = ttk.Button(self.parent, text=\'Make Dataset\', command=lambda: self.make_dataset())\n        self.layout_utils.before_widget(\n            src=self.btn_make_dataset,\n            target=self.btn_compile,\n            width=120,\n            height=24,\n            tiny_space=True\n        )\n\n        # \xe6\xb8\x85\xe9\x99\xa4\xe8\xae\xad\xe7\xbb\x83\xe8\xae\xb0\xe5\xbd\x95 - \xe6\x8c\x89\xe9\x92\xae\n        self.btn_reset_history = ttk.Button(\n            self.parent, text=\'Reset History\', command=lambda: self.reset_history()\n        )\n        self.layout_utils.before_widget(\n            src=self.btn_reset_history,\n            target=self.btn_make_dataset,\n            width=120,\n            height=24,\n            tiny_space=True\n        )\n\n        # \xe9\xa2\x84\xe6\xb5\x8b - \xe6\x8c\x89\xe9\x92\xae\n        self.btn_testing = ttk.Button(\n            self.parent, text=\'Testing\', command=lambda: self.testing_model()\n        )\n        self.layout_utils.before_widget(\n            src=self.btn_testing,\n            target=self.btn_reset_history,\n            width=80,\n            height=24,\n            tiny_space=True\n        )\n\n        self.parent.geometry(size)\n\n    @staticmethod\n    def threading_exec(func, *args) -> threading.Thread:\n        th = threading.Thread(target=func, args=args)\n        th.setDaemon(True)\n        th.start()\n        return th\n\n    def popup_data_augmentation(self):\n        if not self.current_project:\n            messagebox.showerror(\n                ""Error!"", ""Please set the project name first.""\n            )\n            return\n        data_augmentation = DataAugmentationDialog()\n        data_augmentation.read_conf(self.data_augmentation_entity)\n\n    def popup_pretreatment(self):\n        if not self.current_project:\n            messagebox.showerror(\n                ""Error!"", ""Please set the project name first.""\n            )\n            return\n        pretreatment = PretreatmentDialog()\n        pretreatment.read_conf(self.pretreatment_entity)\n\n    @staticmethod\n    def listbox_scrollbar(listbox: tk.Listbox):\n        y_scrollbar = tk.Scrollbar(\n            listbox, command=listbox.yview\n        )\n        y_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n        listbox.config(yscrollcommand=y_scrollbar.set)\n\n    def blank_click(self, event):\n        if self.current_project != self.comb_project_name.get():\n            self.project_name_fill_callback(event)\n\n    def project_name_fill_callback(self, event):\n        suffix = \'-{}-{}-H{}-{}-C{}\'.format(\n            self.comb_neu_cnn.get(),\n            self.comb_recurrent.get(),\n            self.units_num_spin.get(),\n            self.comb_loss.get(),\n            self.comb_channel.get(),\n        )\n        current_project_name = self.comb_project_name.get()\n        if len(current_project_name) > 0 and current_project_name not in self.project_names:\n            self.extract_regex = "".*?(?=_)""\n            self.label_from = LabelFrom.FileName\n            self.sample_map[DatasetType.Directory][RunMode.Trains].delete(0, tk.END)\n            self.sample_map[DatasetType.Directory][RunMode.Validation].delete(0, tk.END)\n            self.category_val.set("""")\n            if not current_project_name.endswith(suffix):\n                self.comb_project_name.insert(tk.END, suffix)\n            self.current_project = self.comb_project_name.get()\n            self.update_dataset_files_path(mode=RunMode.Trains)\n            self.update_dataset_files_path(mode=RunMode.Validation)\n\n    @property\n    def project_path(self):\n        if not self.current_project:\n            return None\n        project_path = ""{}/{}"".format(self.project_root_path, self.current_project)\n        if not os.path.exists(project_path):\n            os.makedirs(project_path)\n        return project_path\n\n    def update_dataset_files_path(self, mode: RunMode):\n        dataset_name = ""dataset/{}.0.tfrecords"".format(mode.value)\n        dataset_path = os.path.join(self.project_path, dataset_name)\n        dataset_path = dataset_path.replace(""\\\\"", \'/\')\n        self.sample_map[DatasetType.TFRecords][mode].delete(0, tk.END)\n        self.sample_map[DatasetType.TFRecords][mode].insert(tk.END, dataset_path)\n        self.save_conf()\n\n    def attach_dataset(self):\n        if self.is_task_running:\n            messagebox.showerror(\n                ""Error!"", ""Please terminate the current training first or wait for the training to end.""\n            )\n            return\n        if not self.current_project:\n            messagebox.showerror(\n                ""Error!"", ""Please set the project name first.""\n            )\n            return\n        filename = filedialog.askdirectory()\n        if not filename:\n            return\n        model_conf = ModelConfig(self.current_project)\n\n        if not self.check_dataset(model_conf):\n            return\n\n        self.attach_dataset_val.set(filename)\n        self.sample_map[DatasetType.Directory][RunMode.Trains].insert(tk.END, filename)\n        self.button_state(self.btn_attach_dataset, tk.DISABLED)\n\n        for mode in [RunMode.Trains, RunMode.Validation]:\n            attached_dataset_name = model_conf.dataset_increasing_name(mode)\n            attached_dataset_name = ""dataset/{}"".format(attached_dataset_name)\n            attached_dataset_path = os.path.join(self.project_path, attached_dataset_name)\n            attached_dataset_path = attached_dataset_path.replace(""\\\\"", \'/\')\n            if mode == RunMode.Validation and self.validation_num_val.get() == 0:\n                continue\n            self.sample_map[DatasetType.TFRecords][mode].insert(tk.END, attached_dataset_path)\n        self.save_conf()\n        model_conf = ModelConfig(self.current_project)\n        self.threading_exec(\n            lambda: DataSets(model_conf).make_dataset(\n                trains_path=filename,\n                is_add=True,\n                callback=lambda: self.button_state(self.btn_attach_dataset, tk.NORMAL),\n                msg=lambda x: tk.messagebox.showinfo(\'Attach Dataset Status\', x)\n            )\n        )\n        pass\n\n    @staticmethod\n    def button_state(btn: ttk.Button, state: str):\n        btn[\'state\'] = state\n\n    def delete_project(self):\n        if not self.current_project:\n            messagebox.showerror(\n                ""Error!"", ""Please select a project to delete.""\n            )\n            return\n        if self.is_task_running:\n            messagebox.showerror(\n                ""Error!"", ""Please terminate the current training first or wait for the training to end.""\n            )\n            return\n        project_path = ""./projects/{}"".format(self.current_project)\n        try:\n            shutil.rmtree(project_path)\n        except Exception as e:\n            messagebox.showerror(\n                ""Error!"", json.dumps(e.args, ensure_ascii=False)\n            )\n        messagebox.showinfo(\n            ""Error!"", ""Delete successful!""\n        )\n        self.comb_project_name.delete(0, tk.END)\n\n    def reset_history(self):\n        if not self.current_project:\n            messagebox.showerror(\n                ""Error!"", ""Please select a project first.""\n            )\n            return\n        if self.is_task_running:\n            messagebox.showerror(\n                ""Error!"", ""Please terminate the current training first or wait for the training to end.""\n            )\n            return\n        project_history_path = ""./projects/{}/model"".format(self.current_project)\n        try:\n            shutil.rmtree(project_history_path)\n        except Exception as e:\n            messagebox.showerror(\n                ""Error!"", json.dumps(e.args, ensure_ascii=False)\n            )\n        messagebox.showinfo(\n            ""Error!"", ""Delete history successful!""\n        )\n\n    def testing_model(self):\n        filename = filedialog.askdirectory()\n        if not filename:\n            return\n        filename = filename.replace(""\\\\"", ""/"")\n        predict = Predict(project_name=self.current_project)\n        predict.testing(image_dir=filename, limit=self.validation_batch_size)\n\n    def clear_dataset(self):\n        if not self.current_project:\n            messagebox.showerror(\n                ""Error!"", ""Please select a project first.""\n            )\n            return\n        if self.is_task_running:\n            messagebox.showerror(\n                ""Error!"", ""Please terminate the current training first or wait for the training to end.""\n            )\n            return\n        project_history_path = ""./projects/{}/dataset"".format(self.current_project)\n        try:\n            shutil.rmtree(project_history_path)\n            self.dataset_train_listbox.delete(1, tk.END)\n            self.dataset_validation_listbox.delete(1, tk.END)\n        except Exception as e:\n            messagebox.showerror(\n                ""Error!"", json.dumps(e.args, ensure_ascii=False)\n            )\n        messagebox.showinfo(\n            ""Error!"", ""Clear dataset successful!""\n        )\n\n    @staticmethod\n    def popup_about():\n        messagebox.showinfo(""About"", ""Image Classification Wizard Tool based on Deep Learning 1.0 CORE_VERSION({})\\n\\nAuthor\'s mailbox: kerlomz@gmail.com\\n\\nQQ Group: 857149419"".format(CORE_VERSION))\n\n    def auto_loss(self, event):\n        if self.comb_recurrent.get() == \'NoRecurrent\':\n            self.comb_loss.set(""CrossEntropy"")\n\n    @staticmethod\n    def get_param(src: dict, key, default=None):\n        result = src.get(key)\n        return result if result else default\n\n    def read_conf(self, event):\n        selected = self.comb_project_name.get()\n        self.current_project = selected\n        model_conf = ModelConfig(selected)\n        self.edit_var.set(model_conf.memory_usage)\n        self.size_val.set(""[{}, {}]"".format(model_conf.image_width, model_conf.image_height))\n        self.resize_val.set(json.dumps(model_conf.resize))\n        self.source_train_path_listbox.delete(0, tk.END)\n        self.source_validation_path_listbox.delete(0, tk.END)\n        self.dataset_validation_listbox.delete(0, tk.END)\n        self.dataset_train_listbox.delete(0, tk.END)\n        for source_train in self.get_param(model_conf.trains_path, DatasetType.Directory, default=[]):\n            self.source_train_path_listbox.insert(tk.END, source_train)\n        for source_validation in self.get_param(model_conf.validation_path, DatasetType.Directory, default=[]):\n            self.source_validation_path_listbox.insert(tk.END, source_validation)\n        self.label_num_spin.set(model_conf.max_label_num)\n        self.comb_channel.set(model_conf.image_channel)\n        self.comb_neu_cnn.set(model_conf.neu_cnn_param)\n        self.comb_recurrent.set(model_conf.neu_recurrent_param)\n        self.units_num_spin.set(model_conf.units_num)\n        self.comb_loss.set(model_conf.loss_func_param)\n\n        self.extract_regex = model_conf.extract_regex\n        self.label_from = model_conf.label_from\n\n        if isinstance(model_conf.category_param, list):\n            self.category_entry[\'state\'] = tk.NORMAL\n            self.comb_category.set(\'CUSTOMIZED\')\n            self.category_val.set(json.dumps(model_conf.category_param, ensure_ascii=False))\n        else:\n            self.category_val.set("""")\n            self.category_entry[\'state\'] = tk.DISABLED\n            self.comb_category.set(model_conf.category_param)\n\n        self.comb_optimizer.set(model_conf.neu_optimizer_param)\n        self.learning_rate_spin.set(model_conf.trains_learning_rate)\n        self.end_acc_val.set(model_conf.trains_end_acc)\n        self.end_cost_val.set(model_conf.trains_end_cost)\n        self.end_epochs_spin.set(model_conf.trains_end_epochs)\n        self.batch_size_val.set(model_conf.batch_size)\n        self.validation_batch_size_val.set(model_conf.validation_batch_size)\n        self.validation_num_val.set(model_conf.validation_set_num)\n\n        self.data_augmentation_entity.binaryzation = model_conf.da_binaryzation\n        self.data_augmentation_entity.median_blur = model_conf.da_median_blur\n        self.data_augmentation_entity.gaussian_blur = model_conf.da_gaussian_blur\n        self.data_augmentation_entity.equalize_hist = model_conf.da_equalize_hist\n        self.data_augmentation_entity.laplace = model_conf.da_laplace\n        self.data_augmentation_entity.warp_perspective = model_conf.da_warp_perspective\n        self.data_augmentation_entity.rotate = model_conf.da_rotate\n        self.data_augmentation_entity.sp_noise = model_conf.da_sp_noise\n        self.data_augmentation_entity.brightness = model_conf.da_brightness\n        self.data_augmentation_entity.hue = model_conf.da_hue\n        self.data_augmentation_entity.saturation = model_conf.da_saturation\n        self.data_augmentation_entity.gamma = model_conf.da_gamma\n        self.data_augmentation_entity.channel_swap = model_conf.da_channel_swap\n        self.data_augmentation_entity.random_blank = model_conf.da_random_blank\n        self.data_augmentation_entity.random_transition = model_conf.da_random_transition\n        self.data_augmentation_entity.random_captcha = model_conf.da_random_captcha\n\n        self.pretreatment_entity.binaryzation = model_conf.pre_binaryzation\n        self.pretreatment_entity.replace_transparent = model_conf.pre_replace_transparent\n        self.pretreatment_entity.horizontal_stitching = model_conf.pre_horizontal_stitching\n        self.pretreatment_entity.concat_frames = model_conf.pre_concat_frames\n        self.pretreatment_entity.blend_frames = model_conf.pre_blend_frames\n\n        for dataset_validation in self.get_param(model_conf.validation_path, DatasetType.TFRecords, default=[]):\n            self.dataset_validation_listbox.insert(tk.END, dataset_validation)\n        for dataset_train in self.get_param(model_conf.trains_path, DatasetType.TFRecords, default=[]):\n            self.dataset_train_listbox.insert(tk.END, dataset_train)\n        return model_conf\n\n    @property\n    def validation_batch_size(self):\n        # if self.dataset_validation_listbox.size() > 1:\n        return self.validation_batch_size_val.get()\n        # else:\n        #     return min(self.validation_batch_size_val.get(), self.validation_num_val.get())\n\n    @property\n    def device_usage(self):\n        return self.edit_var.get()\n\n    def save_conf(self):\n        if not self.current_project:\n            messagebox.showerror(\n                ""Error!"", ""Please set the project name first.""\n            )\n            return\n        model_conf = ModelConfig(\n            project_name=self.current_project,\n            MemoryUsage=self.device_usage,\n            CNNNetwork=self.neu_cnn,\n            RecurrentNetwork=self.neu_recurrent,\n            UnitsNum=self.units_num_spin.get(),\n            Optimizer=self.optimizer,\n            LossFunction=self.loss_func,\n            Decoder=self.comb_loss.get(),\n            ModelName=self.current_project,\n            ModelField=ModelField.Image.value,\n            ModelScene=ModelScene.Classification.value,\n            Category=self.category,\n            Resize=self.resize,\n            ImageChannel=self.comb_channel.get(),\n            ImageWidth=self.image_width,\n            ImageHeight=self.image_height,\n            MaxLabelNum=self.label_num_spin.get(),\n            AutoPadding=True,\n            ReplaceTransparent=False,\n            HorizontalStitching=False,\n            OutputSplit=\'\',\n            LabelFrom=self.label_from.value,\n            ExtractRegex=self.extract_regex,\n            LabelSplit=\'\',\n            DatasetTrainsPath=self.dataset_value(\n                dataset_type=DatasetType.TFRecords, mode=RunMode.Trains\n            ),\n            DatasetValidationPath=self.dataset_value(\n                dataset_type=DatasetType.TFRecords, mode=RunMode.Validation\n            ),\n            SourceTrainPath=self.dataset_value(\n                dataset_type=DatasetType.Directory, mode=RunMode.Trains\n            ),\n            SourceValidationPath=self.dataset_value(\n                dataset_type=DatasetType.Directory, mode=RunMode.Validation\n            ),\n            ValidationSetNum=self.validation_num_val.get(),\n            SavedSteps=100,\n            ValidationSteps=500,\n            EndAcc=self.end_acc_val.get(),\n            EndCost=self.end_cost_val.get(),\n            EndEpochs=self.end_epochs_spin.get(),\n            BatchSize=self.batch_size_val.get(),\n            ValidationBatchSize=self.validation_batch_size,\n            LearningRate=self.learning_rate_spin.get(),\n            DA_Binaryzation=self.data_augmentation_entity.binaryzation,\n            DA_MedianBlur=self.data_augmentation_entity.median_blur,\n            DA_GaussianBlur=self.data_augmentation_entity.gaussian_blur,\n            DA_EqualizeHist=self.data_augmentation_entity.equalize_hist,\n            DA_Laplace=self.data_augmentation_entity.laplace,\n            DA_WarpPerspective=self.data_augmentation_entity.warp_perspective,\n            DA_Rotate=self.data_augmentation_entity.rotate,\n            DA_PepperNoise=self.data_augmentation_entity.sp_noise,\n            DA_Brightness=self.data_augmentation_entity.brightness,\n            DA_Saturation=self.data_augmentation_entity.saturation,\n            DA_Hue=self.data_augmentation_entity.hue,\n            DA_Gamma=self.data_augmentation_entity.gamma,\n            DA_ChannelSwap=self.data_augmentation_entity.channel_swap,\n            DA_RandomBlank=self.data_augmentation_entity.random_blank,\n            DA_RandomTransition=self.data_augmentation_entity.random_transition,\n            DA_RandomCaptcha=self.data_augmentation_entity.random_captcha,\n            Pre_Binaryzation=self.pretreatment_entity.binaryzation,\n            Pre_ReplaceTransparent=self.pretreatment_entity.replace_transparent,\n            Pre_HorizontalStitching=self.pretreatment_entity.horizontal_stitching,\n            Pre_ConcatFrames=self.pretreatment_entity.concat_frames,\n            Pre_BlendFrames=self.pretreatment_entity.blend_frames,\n        )\n        model_conf.update()\n        return model_conf\n\n    def make_dataset(self):\n        if not self.current_project:\n            messagebox.showerror(\n                ""Error!"", ""Please set the project name first.""\n            )\n            return\n        if self.is_task_running:\n            messagebox.showerror(\n                ""Error!"", ""Please terminate the current training first or wait for the training to end.""\n            )\n            return\n        self.save_conf()\n        self.button_state(self.btn_make_dataset, tk.DISABLED)\n        model_conf = ModelConfig(self.current_project)\n        train_path = self.dataset_value(DatasetType.Directory, RunMode.Trains)\n        validation_path = self.dataset_value(DatasetType.Directory, RunMode.Validation)\n        if len(train_path) < 1:\n            messagebox.showerror(\n                ""Error!"", ""{} Sample set has not been added."".format(RunMode.Trains.value)\n            )\n            self.button_state(self.btn_make_dataset, tk.NORMAL)\n            return\n        self.threading_exec(\n            lambda: DataSets(model_conf).make_dataset(\n                trains_path=train_path,\n                validation_path=validation_path,\n                is_add=False,\n                callback=lambda: self.button_state(self.btn_make_dataset, tk.NORMAL),\n                msg=lambda x: tk.messagebox.showinfo(\'Make Dataset Status\', x)\n            )\n        )\n\n    @property\n    def size(self):\n        return self.json_filter(self.size_val.get(), int)\n\n    @property\n    def image_height(self):\n        return self.size[1]\n\n    @property\n    def image_width(self):\n        return self.size[0]\n\n    @property\n    def resize(self):\n        return self.json_filter(self.resize_val.get(), int)\n\n    @property\n    def neu_cnn(self):\n        return self.comb_neu_cnn.get()\n\n    @property\n    def neu_recurrent(self):\n        return self.comb_recurrent.get()\n\n    @property\n    def loss_func(self):\n        return self.comb_loss.get()\n\n    @property\n    def optimizer(self):\n        return self.comb_optimizer.get()\n\n    @staticmethod\n    def json_filter(content, item_type):\n        if not content:\n            messagebox.showerror(\n                ""Error!"", ""To select a customized category, you must specify the category set manually.""\n            )\n            return None\n        try:\n            content = json.loads(content)\n        except ValueError as e:\n            messagebox.showerror(\n                ""Error!"", ""Input must be of type JSON.""\n            )\n            return None\n        content = [item_type(i) for i in content]\n        return content\n\n    @property\n    def category(self):\n        comb_selected = self.comb_category.get()\n        if not comb_selected:\n            messagebox.showerror(\n                ""Error!"", ""Please select built-in category or custom category first""\n            )\n            return None\n        if comb_selected == \'CUSTOMIZED\':\n            category_value = self.category_entry.get()\n            category_value = category_value.replace(""\'"", \'""\') if ""\'"" in category_value else category_value\n            category_value = self.json_filter(category_value, str)\n        else:\n            category_value = comb_selected\n        return category_value\n\n    def dataset_value(self, dataset_type: DatasetType, mode: RunMode):\n        listbox = self.sample_map[dataset_type][mode]\n        value = list(listbox.get(0, listbox.size() - 1))\n        return value\n\n    def compile_task(self):\n        if not self.current_project:\n            messagebox.showerror(\n                ""Error!"", ""Please set the project name first.""\n            )\n            return\n        model_conf = ModelConfig(project_name=self.current_project)\n        if not os.path.exists(model_conf.model_root_path):\n            messagebox.showerror(\n                ""Error"", ""Model storage folder does not exist.""\n            )\n            return\n        if len(os.listdir(model_conf.model_root_path)) < 3:\n            messagebox.showerror(\n                ""Error"", ""There is no training model record, please train before compiling.""\n            )\n            return\n        try:\n            if not self.current_task:\n                self.current_task = Trains(model_conf)\n\n            self.current_task.compile_graph(0)\n            status = \'Compile completed\'\n        except Exception as e:\n            messagebox.showerror(\n                e.__class__.__name__, json.dumps(e.args, ensure_ascii=False)\n            )\n            status = \'Compile failure\'\n        tk.messagebox.showinfo(\'Compile Status\', status)\n\n    def compile(self):\n        self.job = self.threading_exec(\n            lambda: self.compile_task()\n        )\n\n    def training_task(self):\n        model_conf = ModelConfig(project_name=self.current_project)\n\n        self.current_task = Trains(model_conf)\n        try:\n            self.button_state(self.btn_training, tk.DISABLED)\n            self.button_state(self.btn_stop, tk.NORMAL)\n            self.is_task_running = True\n            self.current_task.train_process()\n            status = \'Training completed\'\n        except Exception as e:\n            traceback.print_exc()\n            messagebox.showerror(\n                e.__class__.__name__, json.dumps(e.args, ensure_ascii=False)\n            )\n            status = \'Training failure\'\n        self.button_state(self.btn_training, tk.NORMAL)\n        self.button_state(self.btn_stop, tk.DISABLED)\n        self.is_task_running = False\n        tk.messagebox.showinfo(\'Training Status\', status)\n\n    @staticmethod\n    def check_dataset(model_conf):\n        trains_path = model_conf.trains_path[DatasetType.TFRecords]\n        validation_path = model_conf.validation_path[DatasetType.TFRecords]\n        if not trains_path or not validation_path:\n            messagebox.showerror(\n                ""Error!"", ""Training set or validation set not defined.""\n            )\n            return False\n        for tp in trains_path:\n            if not os.path.exists(tp):\n                messagebox.showerror(\n                    ""Error!"", ""Training set path does not exist, please make dataset first""\n                )\n                return False\n        for vp in validation_path:\n            if not os.path.exists(vp):\n                messagebox.showerror(\n                    ""Error!"", ""Validation set path does not exist, please make dataset first""\n                )\n                return False\n        return True\n\n    def start_training(self):\n        if not self.check_resize():\n            return\n        if not self.current_project:\n            messagebox.showerror(\n                ""Error!"", ""Please set the project name first.""\n            )\n            return\n        model_conf = self.save_conf()\n        if not self.check_dataset(model_conf):\n            return\n        self.job = self.threading_exec(\n            lambda: self.training_task()\n        )\n\n    def stop_training(self):\n        self.current_task.stop_flag = True\n\n    @property\n    def project_names(self):\n        return [i.name for i in os.scandir(self.project_root_path) if i.is_dir()]\n\n    def fetch_projects(self):\n        self.comb_project_name[\'values\'] = self.project_names\n\n    def browse_dataset(self, dataset_type: DatasetType, mode: RunMode):\n        if not self.current_project:\n            messagebox.showerror(\n                ""Error!"", ""Please define the project name first.""\n            )\n            return\n        filename = filedialog.askdirectory()\n        if not filename:\n            return\n        is_sub = False\n        for i, item in enumerate(os.scandir(filename)):\n            if item.is_dir():\n                path = item.path.replace(""\\\\"", ""/"")\n                if self.sample_map[dataset_type][mode].size() == 0:\n                    self.fetch_sample([path])\n                self.sample_map[dataset_type][mode].insert(tk.END, path)\n                if i > 0:\n                    continue\n                is_sub = True\n            else:\n                break\n        if not is_sub:\n            filename = filename.replace(""\\\\"", ""/"")\n            if self.sample_map[dataset_type][mode].size() == 0:\n                self.fetch_sample([filename])\n            self.sample_map[dataset_type][mode].insert(tk.END, filename)\n\n    @staticmethod\n    def closest_category(category):\n        category = set(category)\n        category_group = dict()\n        for key in SIMPLE_CATEGORY_MODEL.keys():\n            category_set = set(category_extract(key))\n            if category <= category_set:\n                category_group[key] = len(category_set) - len(category)\n        if not category_group:\n            return None\n        min_index = min(category_group.values())\n        for k, v in category_group.items():\n            if v == min_index:\n                return k\n\n    def fetch_sample(self, dataset_path):\n        file_names = os.listdir(dataset_path[0])[0:100]\n        category = list()\n        len_label = -1\n\n        for file_name in file_names:\n            if ""_"" in file_name:\n                label = file_name.split(""_"")[0]\n                label = [i for i in label]\n                len_label = len(label)\n                category.extend(label)\n\n        category_pram = self.closest_category(category)\n        if not category_pram:\n            return\n        self.comb_category.set(category_pram)\n        size = PilImage.open(os.path.join(dataset_path[0], file_names[0])).size\n        self.size_val.set(json.dumps(size))\n        self.resize_val.set(json.dumps(size))\n        self.label_num_spin.set(len_label)\n\n    def listbox_delete_item_callback(self, event, listbox: tk.Listbox):\n        i = listbox.curselection()[0]\n        listbox.delete(i)\n        self.save_conf()\n\n    def comb_category_callback(self, event):\n        comb_selected = self.comb_category.get()\n        if comb_selected == \'CUSTOMIZED\':\n            self.category_entry[\'state\'] = tk.NORMAL\n        else:\n            self.category_entry.delete(0, tk.END)\n            self.category_entry[\'state\'] = tk.DISABLED\n\n    def check_resize(self):\n        if self.loss_func == \'CTC\':\n            return True\n        param = OUTPUT_SHAPE1_MAP[NETWORK_MAP[self.neu_cnn]]\n        shape1w = math.ceil(1.0*self.resize[0]/param[0])\n        shape1h = math.ceil(1.0*self.resize[1]/param[0])\n        input_s1 = shape1w * shape1h * param[1]\n        label_num = int(self.label_num_spin.get())\n        if input_s1 % label_num != 0:\n            messagebox.showerror(\n                ""Error!"", ""Shape[1] = {} must divide the label_num = {}."".format(input_s1, label_num)\n            )\n            return False\n        return True\n\n    @staticmethod\n    def resource_path(relative_path):\n        try:\n            # PyInstaller creates a temp folder and stores path in _MEIPASS\n            base_path = sys._MEIPASS\n        except AttributeError:\n            base_path = os.path.abspath(""."")\n        return os.path.join(base_path, relative_path)\n\n\nif __name__ == \'__main__\':\n    root = tk.Tk()\n    app = Wizard(root)\n    root.mainloop()\n\n'"
category.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nfrom exception import *\nfrom constants import SimpleCharset\n\n# TODO \xe9\x9c\x80\xe8\xa6\x81\xe5\x8d\x95\xe7\x8b\xac\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6:\nSPACE_INDEX = 0\nSPACE_TOKEN = [\'\']\nNUMBER = [\'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\']\nALPHA_UPPER = [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\', \'G\', \'H\', \'I\', \'J\', \'K\', \'L\', \'M\', \'N\', \'O\', \'P\', \'Q\', \'R\', \'S\', \'T\', \'U\',\n               \'V\', \'W\', \'X\', \'Y\', \'Z\']\nALPHA_LOWER = [\'a\', \'b\', \'c\', \'d\', \'e\', \'f\', \'g\', \'h\', \'i\', \'j\', \'k\', \'l\', \'m\', \'n\', \'o\', \'p\', \'q\', \'r\', \'s\', \'t\', \'u\',\n               \'v\', \'w\', \'x\', \'y\', \'z\']\nARITHMETIC_SYMBOL = [\'(\', \')\', \'+\', \'-\', \'\xc3\x97\', \'\xc3\xb7\', \'=\']\nCHINESE_3500 = [\n    \'\xe4\xb8\x80\', \'\xe4\xb9\x99\', \'\xe4\xba\x8c\', \'\xe5\x8d\x81\', \'\xe4\xb8\x81\', \'\xe5\x8e\x82\', \'\xe4\xb8\x83\', \'\xe5\x8d\x9c\', \'\xe4\xba\xba\', \'\xe5\x85\xa5\', \'\xe5\x85\xab\', \'\xe4\xb9\x9d\', \'\xe5\x87\xa0\', \'\xe5\x84\xbf\', \'\xe4\xba\x86\', \'\xe5\x8a\x9b\', \'\xe4\xb9\x83\', \'\xe5\x88\x80\', \'\xe5\x8f\x88\', \'\xe4\xb8\x89\',\n    \'\xe4\xba\x8e\', \'\xe5\xb9\xb2\', \'\xe4\xba\x8f\', \'\xe5\xa3\xab\', \'\xe5\xb7\xa5\', \'\xe5\x9c\x9f\', \'\xe6\x89\x8d\', \'\xe5\xaf\xb8\', \'\xe4\xb8\x8b\', \'\xe5\xa4\xa7\', \'\xe4\xb8\x88\', \'\xe4\xb8\x8e\', \'\xe4\xb8\x87\', \'\xe4\xb8\x8a\', \'\xe5\xb0\x8f\', \'\xe5\x8f\xa3\', \'\xe5\xb7\xbe\', \'\xe5\xb1\xb1\', \'\xe5\x8d\x83\', \'\xe4\xb9\x9e\',\n    \'\xe5\xb7\x9d\', \'\xe4\xba\xbf\', \'\xe4\xb8\xaa\', \'\xe5\x8b\xba\', \'\xe4\xb9\x85\', \'\xe5\x87\xa1\', \'\xe5\x8f\x8a\', \'\xe5\xa4\x95\', \'\xe4\xb8\xb8\', \'\xe4\xb9\x88\', \'\xe5\xb9\xbf\', \'\xe4\xba\xa1\', \'\xe9\x97\xa8\', \'\xe4\xb9\x89\', \'\xe4\xb9\x8b\', \'\xe5\xb0\xb8\', \'\xe5\xbc\x93\', \'\xe5\xb7\xb1\', \'\xe5\xb7\xb2\', \'\xe5\xad\x90\',\n    \'\xe5\x8d\xab\', \'\xe4\xb9\x9f\', \'\xe5\xa5\xb3\', \'\xe9\xa3\x9e\', \'\xe5\x88\x83\', \'\xe4\xb9\xa0\', \'\xe5\x8f\x89\', \'\xe9\xa9\xac\', \'\xe4\xb9\xa1\', \'\xe4\xb8\xb0\', \'\xe7\x8e\x8b\', \'\xe4\xba\x95\', \'\xe5\xbc\x80\', \'\xe5\xa4\xab\', \'\xe5\xa4\xa9\', \'\xe6\x97\xa0\', \'\xe5\x85\x83\', \'\xe4\xb8\x93\', \'\xe4\xba\x91\', \'\xe6\x89\x8e\',\n    \'\xe8\x89\xba\', \'\xe6\x9c\xa8\', \'\xe4\xba\x94\', \'\xe6\x94\xaf\', \'\xe5\x8e\x85\', \'\xe4\xb8\x8d\', \'\xe5\xa4\xaa\', \'\xe7\x8a\xac\', \'\xe5\x8c\xba\', \'\xe5\x8e\x86\', \'\xe5\xb0\xa4\', \'\xe5\x8f\x8b\', \'\xe5\x8c\xb9\', \'\xe8\xbd\xa6\', \'\xe5\xb7\xa8\', \'\xe7\x89\x99\', \'\xe5\xb1\xaf\', \'\xe6\xaf\x94\', \'\xe4\xba\x92\', \'\xe5\x88\x87\',\n    \'\xe7\x93\xa6\', \'\xe6\xad\xa2\', \'\xe5\xb0\x91\', \'\xe6\x97\xa5\', \'\xe4\xb8\xad\', \'\xe5\x86\x88\', \'\xe8\xb4\x9d\', \'\xe5\x86\x85\', \'\xe6\xb0\xb4\', \'\xe8\xa7\x81\', \'\xe5\x8d\x88\', \'\xe7\x89\x9b\', \'\xe6\x89\x8b\', \'\xe6\xaf\x9b\', \'\xe6\xb0\x94\', \'\xe5\x8d\x87\', \'\xe9\x95\xbf\', \'\xe4\xbb\x81\', \'\xe4\xbb\x80\', \'\xe7\x89\x87\',\n    \'\xe4\xbb\x86\', \'\xe5\x8c\x96\', \'\xe4\xbb\x87\', \'\xe5\xb8\x81\', \'\xe4\xbb\x8d\', \'\xe4\xbb\x85\', \'\xe6\x96\xa4\', \'\xe7\x88\xaa\', \'\xe5\x8f\x8d\', \'\xe4\xbb\x8b\', \'\xe7\x88\xb6\', \'\xe4\xbb\x8e\', \'\xe4\xbb\x8a\', \'\xe5\x87\xb6\', \'\xe5\x88\x86\', \'\xe4\xb9\x8f\', \'\xe5\x85\xac\', \'\xe4\xbb\x93\', \'\xe6\x9c\x88\', \'\xe6\xb0\x8f\',\n    \'\xe5\x8b\xbf\', \'\xe6\xac\xa0\', \'\xe9\xa3\x8e\', \'\xe4\xb8\xb9\', \'\xe5\x8c\x80\', \'\xe4\xb9\x8c\', \'\xe5\x87\xa4\', \'\xe5\x8b\xbe\', \'\xe6\x96\x87\', \'\xe5\x85\xad\', \'\xe6\x96\xb9\', \'\xe7\x81\xab\', \'\xe4\xb8\xba\', \'\xe6\x96\x97\', \'\xe5\xbf\x86\', \'\xe8\xae\xa2\', \'\xe8\xae\xa1\', \'\xe6\x88\xb7\', \'\xe8\xae\xa4\', \'\xe5\xbf\x83\',\n    \'\xe5\xb0\xba\', \'\xe5\xbc\x95\', \'\xe4\xb8\x91\', \'\xe5\xb7\xb4\', \'\xe5\xad\x94\', \'\xe9\x98\x9f\', \'\xe5\x8a\x9e\', \'\xe4\xbb\xa5\', \'\xe5\x85\x81\', \'\xe4\xba\x88\', \'\xe5\x8a\x9d\', \'\xe5\x8f\x8c\', \'\xe4\xb9\xa6\', \'\xe5\xb9\xbb\', \'\xe7\x8e\x89\', \'\xe5\x88\x8a\', \'\xe7\xa4\xba\', \'\xe6\x9c\xab\', \'\xe6\x9c\xaa\', \'\xe5\x87\xbb\',\n    \'\xe6\x89\x93\', \'\xe5\xb7\xa7\', \'\xe6\xad\xa3\', \'\xe6\x89\x91\', \'\xe6\x89\x92\', \'\xe5\x8a\x9f\', \'\xe6\x89\x94\', \'\xe5\x8e\xbb\', \'\xe7\x94\x98\', \'\xe4\xb8\x96\', \'\xe5\x8f\xa4\', \'\xe8\x8a\x82\', \'\xe6\x9c\xac\', \'\xe6\x9c\xaf\', \'\xe5\x8f\xaf\', \'\xe4\xb8\x99\', \'\xe5\xb7\xa6\', \'\xe5\x8e\x89\', \'\xe5\x8f\xb3\', \'\xe7\x9f\xb3\',\n    \'\xe5\xb8\x83\', \'\xe9\xbe\x99\', \'\xe5\xb9\xb3\', \'\xe7\x81\xad\', \'\xe8\xbd\xa7\', \'\xe4\xb8\x9c\', \'\xe5\x8d\xa1\', \'\xe5\x8c\x97\', \'\xe5\x8d\xa0\', \'\xe4\xb8\x9a\', \'\xe6\x97\xa7\', \'\xe5\xb8\x85\', \'\xe5\xbd\x92\', \'\xe4\xb8\x94\', \'\xe6\x97\xa6\', \'\xe7\x9b\xae\', \'\xe5\x8f\xb6\', \'\xe7\x94\xb2\', \'\xe7\x94\xb3\', \'\xe5\x8f\xae\',\n    \'\xe7\x94\xb5\', \'\xe5\x8f\xb7\', \'\xe7\x94\xb0\', \'\xe7\x94\xb1\', \'\xe5\x8f\xb2\', \'\xe5\x8f\xaa\', \'\xe5\xa4\xae\', \'\xe5\x85\x84\', \'\xe5\x8f\xbc\', \'\xe5\x8f\xab\', \'\xe5\x8f\xa6\', \'\xe5\x8f\xa8\', \'\xe5\x8f\xb9\', \'\xe5\x9b\x9b\', \'\xe7\x94\x9f\', \'\xe5\xa4\xb1\', \'\xe7\xa6\xbe\', \'\xe4\xb8\x98\', \'\xe4\xbb\x98\', \'\xe4\xbb\x97\',\n    \'\xe4\xbb\xa3\', \'\xe4\xbb\x99\', \'\xe4\xbb\xac\', \'\xe4\xbb\xaa\', \'\xe7\x99\xbd\', \'\xe4\xbb\x94\', \'\xe4\xbb\x96\', \'\xe6\x96\xa5\', \'\xe7\x93\x9c\', \'\xe4\xb9\x8e\', \'\xe4\xb8\x9b\', \'\xe4\xbb\xa4\', \'\xe7\x94\xa8\', \'\xe7\x94\xa9\', \'\xe5\x8d\xb0\', \'\xe4\xb9\x90\', \'\xe5\x8f\xa5\', \'\xe5\x8c\x86\', \'\xe5\x86\x8c\', \'\xe7\x8a\xaf\',\n    \'\xe5\xa4\x96\', \'\xe5\xa4\x84\', \'\xe5\x86\xac\', \'\xe9\xb8\x9f\', \'\xe5\x8a\xa1\', \'\xe5\x8c\x85\', \'\xe9\xa5\xa5\', \'\xe4\xb8\xbb\', \'\xe5\xb8\x82\', \'\xe7\xab\x8b\', \'\xe9\x97\xaa\', \'\xe5\x85\xb0\', \'\xe5\x8d\x8a\', \'\xe6\xb1\x81\', \'\xe6\xb1\x87\', \'\xe5\xa4\xb4\', \'\xe6\xb1\x89\', \'\xe5\xae\x81\', \'\xe7\xa9\xb4\', \'\xe5\xae\x83\',\n    \'\xe8\xae\xa8\', \'\xe5\x86\x99\', \'\xe8\xae\xa9\', \'\xe7\xa4\xbc\', \'\xe8\xae\xad\', \'\xe5\xbf\x85\', \'\xe8\xae\xae\', \'\xe8\xae\xaf\', \'\xe8\xae\xb0\', \'\xe6\xb0\xb8\', \'\xe5\x8f\xb8\', \'\xe5\xb0\xbc\', \'\xe6\xb0\x91\', \'\xe5\x87\xba\', \'\xe8\xbe\xbd\', \'\xe5\xa5\xb6\', \'\xe5\xa5\xb4\', \'\xe5\x8a\xa0\', \'\xe5\x8f\xac\', \'\xe7\x9a\xae\',\n    \'\xe8\xbe\xb9\', \'\xe5\x8f\x91\', \'\xe5\xad\x95\', \'\xe5\x9c\xa3\', \'\xe5\xaf\xb9\', \'\xe5\x8f\xb0\', \'\xe7\x9f\x9b\', \'\xe7\xba\xa0\', \'\xe6\xaf\x8d\', \'\xe5\xb9\xbc\', \'\xe4\xb8\x9d\', \'\xe5\xbc\x8f\', \'\xe5\x88\x91\', \'\xe5\x8a\xa8\', \'\xe6\x89\x9b\', \'\xe5\xaf\xba\', \'\xe5\x90\x89\', \'\xe6\x89\xa3\', \'\xe8\x80\x83\', \'\xe6\x89\x98\',\n    \'\xe8\x80\x81\', \'\xe6\x89\xa7\', \'\xe5\xb7\xa9\', \'\xe5\x9c\xbe\', \'\xe6\x89\xa9\', \'\xe6\x89\xab\', \'\xe5\x9c\xb0\', \'\xe6\x89\xac\', \'\xe5\x9c\xba\', \'\xe8\x80\xb3\', \'\xe5\x85\xb1\', \'\xe8\x8a\x92\', \'\xe4\xba\x9a\', \'\xe8\x8a\x9d\', \'\xe6\x9c\xbd\', \'\xe6\x9c\xb4\', \'\xe6\x9c\xba\', \'\xe6\x9d\x83\', \'\xe8\xbf\x87\', \'\xe8\x87\xa3\',\n    \'\xe5\x86\x8d\', \'\xe5\x8d\x8f\', \'\xe8\xa5\xbf\', \'\xe5\x8e\x8b\', \'\xe5\x8e\x8c\', \'\xe5\x9c\xa8\', \'\xe6\x9c\x89\', \'\xe7\x99\xbe\', \'\xe5\xad\x98\', \'\xe8\x80\x8c\', \'\xe9\xa1\xb5\', \'\xe5\x8c\xa0\', \'\xe5\xa4\xb8\', \'\xe5\xa4\xba\', \'\xe7\x81\xb0\', \'\xe8\xbe\xbe\', \'\xe5\x88\x97\', \'\xe6\xad\xbb\', \'\xe6\x88\x90\', \'\xe5\xa4\xb9\',\n    \'\xe8\xbd\xa8\', \'\xe9\x82\xaa\', \'\xe5\x88\x92\', \'\xe8\xbf\x88\', \'\xe6\xaf\x95\', \'\xe8\x87\xb3\', \'\xe6\xad\xa4\', \'\xe8\xb4\x9e\', \'\xe5\xb8\x88\', \'\xe5\xb0\x98\', \'\xe5\xb0\x96\', \'\xe5\x8a\xa3\', \'\xe5\x85\x89\', \'\xe5\xbd\x93\', \'\xe6\x97\xa9\', \'\xe5\x90\x90\', \'\xe5\x90\x93\', \'\xe8\x99\xab\', \'\xe6\x9b\xb2\', \'\xe5\x9b\xa2\',\n    \'\xe5\x90\x8c\', \'\xe5\x90\x8a\', \'\xe5\x90\x83\', \'\xe5\x9b\xa0\', \'\xe5\x90\xb8\', \'\xe5\x90\x97\', \'\xe5\xb1\xbf\', \'\xe5\xb8\x86\', \'\xe5\xb2\x81\', \'\xe5\x9b\x9e\', \'\xe5\xb2\x82\', \'\xe5\x88\x9a\', \'\xe5\x88\x99\', \'\xe8\x82\x89\', \'\xe7\xbd\x91\', \'\xe5\xb9\xb4\', \'\xe6\x9c\xb1\', \'\xe5\x85\x88\', \'\xe4\xb8\xa2\', \'\xe8\x88\x8c\',\n    \'\xe7\xab\xb9\', \'\xe8\xbf\x81\', \'\xe4\xb9\x94\', \'\xe4\xbc\x9f\', \'\xe4\xbc\xa0\', \'\xe4\xb9\x92\', \'\xe4\xb9\x93\', \'\xe4\xbc\x91\', \'\xe4\xbc\x8d\', \'\xe4\xbc\x8f\', \'\xe4\xbc\x98\', \'\xe4\xbc\x90\', \'\xe5\xbb\xb6\', \'\xe4\xbb\xb6\', \'\xe4\xbb\xbb\', \'\xe4\xbc\xa4\', \'\xe4\xbb\xb7\', \'\xe4\xbb\xbd\', \'\xe5\x8d\x8e\', \'\xe4\xbb\xb0\',\n    \'\xe4\xbb\xbf\', \'\xe4\xbc\x99\', \'\xe4\xbc\xaa\', \'\xe8\x87\xaa\', \'\xe8\xa1\x80\', \'\xe5\x90\x91\', \'\xe4\xbc\xbc\', \'\xe5\x90\x8e\', \'\xe8\xa1\x8c\', \'\xe8\x88\x9f\', \'\xe5\x85\xa8\', \'\xe4\xbc\x9a\', \'\xe6\x9d\x80\', \'\xe5\x90\x88\', \'\xe5\x85\x86\', \'\xe4\xbc\x81\', \'\xe4\xbc\x97\', \'\xe7\x88\xb7\', \'\xe4\xbc\x9e\', \'\xe5\x88\x9b\',\n    \'\xe8\x82\x8c\', \'\xe6\x9c\xb5\', \'\xe6\x9d\x82\', \'\xe5\x8d\xb1\', \'\xe6\x97\xac\', \'\xe6\x97\xa8\', \'\xe8\xb4\x9f\', \'\xe5\x90\x84\', \'\xe5\x90\x8d\', \'\xe5\xa4\x9a\', \'\xe4\xba\x89\', \'\xe8\x89\xb2\', \'\xe5\xa3\xae\', \'\xe5\x86\xb2\', \'\xe5\x86\xb0\', \'\xe5\xba\x84\', \'\xe5\xba\x86\', \'\xe4\xba\xa6\', \'\xe5\x88\x98\', \'\xe9\xbd\x90\',\n    \'\xe4\xba\xa4\', \'\xe6\xac\xa1\', \'\xe8\xa1\xa3\', \'\xe4\xba\xa7\', \'\xe5\x86\xb3\', \'\xe5\x85\x85\', \'\xe5\xa6\x84\', \'\xe9\x97\xad\', \'\xe9\x97\xae\', \'\xe9\x97\xaf\', \'\xe7\xbe\x8a\', \'\xe5\xb9\xb6\', \'\xe5\x85\xb3\', \'\xe7\xb1\xb3\', \'\xe7\x81\xaf\', \'\xe5\xb7\x9e\', \'\xe6\xb1\x97\', \'\xe6\xb1\xa1\', \'\xe6\xb1\x9f\', \'\xe6\xb1\xa0\',\n    \'\xe6\xb1\xa4\', \'\xe5\xbf\x99\', \'\xe5\x85\xb4\', \'\xe5\xae\x87\', \'\xe5\xae\x88\', \'\xe5\xae\x85\', \'\xe5\xad\x97\', \'\xe5\xae\x89\', \'\xe8\xae\xb2\', \'\xe5\x86\x9b\', \'\xe8\xae\xb8\', \'\xe8\xae\xba\', \'\xe5\x86\x9c\', \'\xe8\xae\xbd\', \'\xe8\xae\xbe\', \'\xe8\xae\xbf\', \'\xe5\xaf\xbb\', \'\xe9\x82\xa3\', \'\xe8\xbf\x85\', \'\xe5\xb0\xbd\',\n    \'\xe5\xaf\xbc\', \'\xe5\xbc\x82\', \'\xe5\xad\x99\', \'\xe9\x98\xb5\', \'\xe9\x98\xb3\', \'\xe6\x94\xb6\', \'\xe9\x98\xb6\', \'\xe9\x98\xb4\', \'\xe9\x98\xb2\', \'\xe5\xa5\xb8\', \'\xe5\xa6\x82\', \'\xe5\xa6\x87\', \'\xe5\xa5\xbd\', \'\xe5\xa5\xb9\', \'\xe5\xa6\x88\', \'\xe6\x88\x8f\', \'\xe7\xbe\xbd\', \'\xe8\xa7\x82\', \'\xe6\xac\xa2\', \'\xe4\xb9\xb0\',\n    \'\xe7\xba\xa2\', \'\xe7\xba\xa4\', \'\xe7\xba\xa7\', \'\xe7\xba\xa6\', \'\xe7\xba\xaa\', \'\xe9\xa9\xb0\', \'\xe5\xb7\xa1\', \'\xe5\xaf\xbf\', \'\xe5\xbc\x84\', \'\xe9\xba\xa6\', \'\xe5\xbd\xa2\', \'\xe8\xbf\x9b\', \'\xe6\x88\x92\', \'\xe5\x90\x9e\', \'\xe8\xbf\x9c\', \'\xe8\xbf\x9d\', \'\xe8\xbf\x90\', \'\xe6\x89\xb6\', \'\xe6\x8a\x9a\', \'\xe5\x9d\x9b\',\n    \'\xe6\x8a\x80\', \'\xe5\x9d\x8f\', \'\xe6\x89\xb0\', \'\xe6\x8b\x92\', \'\xe6\x89\xbe\', \'\xe6\x89\xb9\', \'\xe6\x89\xaf\', \'\xe5\x9d\x80\', \'\xe8\xb5\xb0\', \'\xe6\x8a\x84\', \'\xe5\x9d\x9d\', \'\xe8\xb4\xa1\', \'\xe6\x94\xbb\', \'\xe8\xb5\xa4\', \'\xe6\x8a\x98\', \'\xe6\x8a\x93\', \'\xe6\x89\xae\', \'\xe6\x8a\xa2\', \'\xe5\xad\x9d\', \'\xe5\x9d\x87\',\n    \'\xe6\x8a\x9b\', \'\xe6\x8a\x95\', \'\xe5\x9d\x9f\', \'\xe6\x8a\x97\', \'\xe5\x9d\x91\', \'\xe5\x9d\x8a\', \'\xe6\x8a\x96\', \'\xe6\x8a\xa4\', \'\xe5\xa3\xb3\', \'\xe5\xbf\x97\', \'\xe6\x89\xad\', \'\xe5\x9d\x97\', \'\xe5\xa3\xb0\', \'\xe6\x8a\x8a\', \'\xe6\x8a\xa5\', \'\xe5\x8d\xb4\', \'\xe5\x8a\xab\', \'\xe8\x8a\xbd\', \'\xe8\x8a\xb1\', \'\xe8\x8a\xb9\',\n    \'\xe8\x8a\xac\', \'\xe8\x8b\x8d\', \'\xe8\x8a\xb3\', \'\xe4\xb8\xa5\', \'\xe8\x8a\xa6\', \'\xe5\x8a\xb3\', \'\xe5\x85\x8b\', \'\xe8\x8b\x8f\', \'\xe6\x9d\x86\', \'\xe6\x9d\xa0\', \'\xe6\x9d\x9c\', \'\xe6\x9d\x90\', \'\xe6\x9d\x91\', \'\xe6\x9d\x8f\', \'\xe6\x9e\x81\', \'\xe6\x9d\x8e\', \'\xe6\x9d\xa8\', \'\xe6\xb1\x82\', \'\xe6\x9b\xb4\', \'\xe6\x9d\x9f\',\n    \'\xe8\xb1\x86\', \'\xe4\xb8\xa4\', \'\xe4\xb8\xbd\', \'\xe5\x8c\xbb\', \'\xe8\xbe\xb0\', \'\xe5\x8a\xb1\', \'\xe5\x90\xa6\', \'\xe8\xbf\x98\', \'\xe6\xad\xbc\', \'\xe6\x9d\xa5\', \'\xe8\xbf\x9e\', \'\xe6\xad\xa5\', \'\xe5\x9d\x9a\', \'\xe6\x97\xb1\', \'\xe7\x9b\xaf\', \'\xe5\x91\x88\', \'\xe6\x97\xb6\', \'\xe5\x90\xb4\', \'\xe5\x8a\xa9\', \'\xe5\x8e\xbf\',\n    \'\xe9\x87\x8c\', \'\xe5\x91\x86\', \'\xe5\x9b\xad\', \'\xe6\x97\xb7\', \'\xe5\x9b\xb4\', \'\xe5\x91\x80\', \'\xe5\x90\xa8\', \'\xe8\xb6\xb3\', \'\xe9\x82\xae\', \'\xe7\x94\xb7\', \'\xe5\x9b\xb0\', \'\xe5\x90\xb5\', \'\xe4\xb8\xb2\', \'\xe5\x91\x98\', \'\xe5\x90\xac\', \'\xe5\x90\xa9\', \'\xe5\x90\xb9\', \'\xe5\x91\x9c\', \'\xe5\x90\xa7\', \'\xe5\x90\xbc\',\n    \'\xe5\x88\xab\', \'\xe5\xb2\x97\', \'\xe5\xb8\x90\', \'\xe8\xb4\xa2\', \'\xe9\x92\x88\', \'\xe9\x92\x89\', \'\xe5\x91\x8a\', \'\xe6\x88\x91\', \'\xe4\xb9\xb1\', \'\xe5\x88\xa9\', \'\xe7\xa7\x83\', \'\xe7\xa7\x80\', \'\xe7\xa7\x81\', \'\xe6\xaf\x8f\', \'\xe5\x85\xb5\', \'\xe4\xbc\xb0\', \'\xe4\xbd\x93\', \'\xe4\xbd\x95\', \'\xe4\xbd\x86\', \'\xe4\xbc\xb8\',\n    \'\xe4\xbd\x9c\', \'\xe4\xbc\xaf\', \'\xe4\xbc\xb6\', \'\xe4\xbd\xa3\', \'\xe4\xbd\x8e\', \'\xe4\xbd\xa0\', \'\xe4\xbd\x8f\', \'\xe4\xbd\x8d\', \'\xe4\xbc\xb4\', \'\xe8\xba\xab\', \'\xe7\x9a\x82\', \'\xe4\xbd\x9b\', \'\xe8\xbf\x91\', \'\xe5\xbd\xbb\', \'\xe5\xbd\xb9\', \'\xe8\xbf\x94\', \'\xe4\xbd\x99\', \'\xe5\xb8\x8c\', \'\xe5\x9d\x90\', \'\xe8\xb0\xb7\',\n    \'\xe5\xa6\xa5\', \'\xe5\x90\xab\', \'\xe9\x82\xbb\', \'\xe5\xb2\x94\', \'\xe8\x82\x9d\', \'\xe8\x82\x9a\', \'\xe8\x82\xa0\', \'\xe9\xbe\x9f\', \'\xe5\x85\x8d\', \'\xe7\x8b\x82\', \'\xe7\x8a\xb9\', \'\xe8\xa7\x92\', \'\xe5\x88\xa0\', \'\xe6\x9d\xa1\', \'\xe5\x8d\xb5\', \'\xe5\xb2\x9b\', \'\xe8\xbf\x8e\', \'\xe9\xa5\xad\', \'\xe9\xa5\xae\', \'\xe7\xb3\xbb\',\n    \'\xe8\xa8\x80\', \'\xe5\x86\xbb\', \'\xe7\x8a\xb6\', \'\xe4\xba\xa9\', \'\xe5\x86\xb5\', \'\xe5\xba\x8a\', \'\xe5\xba\x93\', \'\xe7\x96\x97\', \'\xe5\xba\x94\', \'\xe5\x86\xb7\', \'\xe8\xbf\x99\', \'\xe5\xba\x8f\', \'\xe8\xbe\x9b\', \'\xe5\xbc\x83\', \'\xe5\x86\xb6\', \'\xe5\xbf\x98\', \'\xe9\x97\xb2\', \'\xe9\x97\xb4\', \'\xe9\x97\xb7\', \'\xe5\x88\xa4\',\n    \'\xe7\x81\xb6\', \'\xe7\x81\xbf\', \'\xe5\xbc\x9f\', \'\xe6\xb1\xaa\', \'\xe6\xb2\x99\', \'\xe6\xb1\xbd\', \'\xe6\xb2\x83\', \'\xe6\xb3\x9b\', \'\xe6\xb2\x9f\', \'\xe6\xb2\xa1\', \'\xe6\xb2\x88\', \'\xe6\xb2\x89\', \'\xe6\x80\x80\', \'\xe5\xbf\xa7\', \'\xe5\xbf\xab\', \'\xe5\xae\x8c\', \'\xe5\xae\x8b\', \'\xe5\xae\x8f\', \'\xe7\x89\xa2\', \'\xe7\xa9\xb6\',\n    \'\xe7\xa9\xb7\', \'\xe7\x81\xbe\', \'\xe8\x89\xaf\', \'\xe8\xaf\x81\', \'\xe5\x90\xaf\', \'\xe8\xaf\x84\', \'\xe8\xa1\xa5\', \'\xe5\x88\x9d\', \'\xe7\xa4\xbe\', \'\xe8\xaf\x86\', \'\xe8\xaf\x89\', \'\xe8\xaf\x8a\', \'\xe8\xaf\x8d\', \'\xe8\xaf\x91\', \'\xe5\x90\x9b\', \'\xe7\x81\xb5\', \'\xe5\x8d\xb3\', \'\xe5\xb1\x82\', \'\xe5\xb0\xbf\', \'\xe5\xb0\xbe\',\n    \'\xe8\xbf\x9f\', \'\xe5\xb1\x80\', \'\xe6\x94\xb9\', \'\xe5\xbc\xa0\', \'\xe5\xbf\x8c\', \'\xe9\x99\x85\', \'\xe9\x99\x86\', \'\xe9\x98\xbf\', \'\xe9\x99\x88\', \'\xe9\x98\xbb\', \'\xe9\x99\x84\', \'\xe5\xa6\x99\', \'\xe5\xa6\x96\', \'\xe5\xa6\xa8\', \'\xe5\x8a\xaa\', \'\xe5\xbf\x8d\', \'\xe5\x8a\xb2\', \'\xe9\xb8\xa1\', \'\xe9\xa9\xb1\', \'\xe7\xba\xaf\',\n    \'\xe7\xba\xb1\', \'\xe7\xba\xb3\', \'\xe7\xba\xb2\', \'\xe9\xa9\xb3\', \'\xe7\xba\xb5\', \'\xe7\xba\xb7\', \'\xe7\xba\xb8\', \'\xe7\xba\xb9\', \'\xe7\xba\xba\', \'\xe9\xa9\xb4\', \'\xe7\xba\xbd\', \'\xe5\xa5\x89\', \'\xe7\x8e\xa9\', \'\xe7\x8e\xaf\', \'\xe6\xad\xa6\', \'\xe9\x9d\x92\', \'\xe8\xb4\xa3\', \'\xe7\x8e\xb0\', \'\xe8\xa1\xa8\', \'\xe8\xa7\x84\',\n    \'\xe6\x8a\xb9\', \'\xe6\x8b\xa2\', \'\xe6\x8b\x94\', \'\xe6\x8b\xa3\', \'\xe6\x8b\x85\', \'\xe5\x9d\xa6\', \'\xe6\x8a\xbc\', \'\xe6\x8a\xbd\', \'\xe6\x8b\x90\', \'\xe6\x8b\x96\', \'\xe6\x8b\x8d\', \'\xe8\x80\x85\', \'\xe9\xa1\xb6\', \'\xe6\x8b\x86\', \'\xe6\x8b\xa5\', \'\xe6\x8a\xb5\', \'\xe6\x8b\x98\', \'\xe5\x8a\xbf\', \'\xe6\x8a\xb1\', \'\xe5\x9e\x83\',\n    \'\xe6\x8b\x89\', \'\xe6\x8b\xa6\', \'\xe6\x8b\x8c\', \'\xe5\xb9\xb8\', \'\xe6\x8b\x9b\', \'\xe5\x9d\xa1\', \'\xe6\x8a\xab\', \'\xe6\x8b\xa8\', \'\xe6\x8b\xa9\', \'\xe6\x8a\xac\', \'\xe5\x85\xb6\', \'\xe5\x8f\x96\', \'\xe8\x8b\xa6\', \'\xe8\x8b\xa5\', \'\xe8\x8c\x82\', \'\xe8\x8b\xb9\', \'\xe8\x8b\x97\', \'\xe8\x8b\xb1\', \'\xe8\x8c\x83\', \'\xe7\x9b\xb4\',\n    \'\xe8\x8c\x84\', \'\xe8\x8c\x8e\', \'\xe8\x8c\x85\', \'\xe6\x9e\x97\', \'\xe6\x9e\x9d\', \'\xe6\x9d\xaf\', \'\xe6\x9f\x9c\', \'\xe6\x9e\x90\', \'\xe6\x9d\xbf\', \'\xe6\x9d\xbe\', \'\xe6\x9e\xaa\', \'\xe6\x9e\x84\', \'\xe6\x9d\xb0\', \'\xe8\xbf\xb0\', \'\xe6\x9e\x95\', \'\xe4\xb8\xa7\', \'\xe6\x88\x96\', \'\xe7\x94\xbb\', \'\xe5\x8d\xa7\', \'\xe4\xba\x8b\',\n    \'\xe5\x88\xba\', \'\xe6\x9e\xa3\', \'\xe9\x9b\xa8\', \'\xe5\x8d\x96\', \'\xe7\x9f\xbf\', \'\xe7\xa0\x81\', \'\xe5\x8e\x95\', \'\xe5\xa5\x94\', \'\xe5\xa5\x87\', \'\xe5\xa5\x8b\', \'\xe6\x80\x81\', \'\xe6\xac\xa7\', \'\xe5\x9e\x84\', \'\xe5\xa6\xbb\', \'\xe8\xbd\xb0\', \'\xe9\xa1\xb7\', \'\xe8\xbd\xac\', \'\xe6\x96\xa9\', \'\xe8\xbd\xae\', \'\xe8\xbd\xaf\',\n    \'\xe5\x88\xb0\', \'\xe9\x9d\x9e\', \'\xe5\x8f\x94\', \'\xe8\x82\xaf\', \'\xe9\xbd\xbf\', \'\xe4\xba\x9b\', \'\xe8\x99\x8e\', \'\xe8\x99\x8f\', \'\xe8\x82\xbe\', \'\xe8\xb4\xa4\', \'\xe5\xb0\x9a\', \'\xe6\x97\xba\', \'\xe5\x85\xb7\', \'\xe6\x9e\x9c\', \'\xe5\x91\xb3\', \'\xe6\x98\x86\', \'\xe5\x9b\xbd\', \'\xe6\x98\x8c\', \'\xe7\x95\x85\', \'\xe6\x98\x8e\',\n    \'\xe6\x98\x93\', \'\xe6\x98\x82\', \'\xe5\x85\xb8\', \'\xe5\x9b\xba\', \'\xe5\xbf\xa0\', \'\xe5\x92\x90\', \'\xe5\x91\xbc\', \'\xe9\xb8\xa3\', \'\xe5\x92\x8f\', \'\xe5\x91\xa2\', \'\xe5\xb2\xb8\', \'\xe5\xb2\xa9\', \'\xe5\xb8\x96\', \'\xe7\xbd\x97\', \'\xe5\xb8\x9c\', \'\xe5\xb2\xad\', \'\xe5\x87\xaf\', \'\xe8\xb4\xa5\', \'\xe8\xb4\xa9\', \'\xe8\xb4\xad\',\n    \'\xe5\x9b\xbe\', \'\xe9\x92\x93\', \'\xe5\x88\xb6\', \'\xe7\x9f\xa5\', \'\xe5\x9e\x82\', \'\xe7\x89\xa7\', \'\xe7\x89\xa9\', \'\xe4\xb9\x96\', \'\xe5\x88\xae\', \'\xe7\xa7\x86\', \'\xe5\x92\x8c\', \'\xe5\xad\xa3\', \'\xe5\xa7\x94\', \'\xe4\xbd\xb3\', \'\xe4\xbe\x8d\', \'\xe4\xbe\x9b\', \'\xe4\xbd\xbf\', \'\xe4\xbe\x8b\', \'\xe7\x89\x88\', \'\xe4\xbe\x84\',\n    \'\xe4\xbe\xa6\', \'\xe4\xbe\xa7\', \'\xe5\x87\xad\', \'\xe4\xbe\xa8\', \'\xe4\xbd\xa9\', \'\xe8\xb4\xa7\', \'\xe4\xbe\x9d\', \'\xe7\x9a\x84\', \'\xe8\xbf\xab\', \'\xe8\xb4\xa8\', \'\xe6\xac\xa3\', \'\xe5\xbe\x81\', \'\xe5\xbe\x80\', \'\xe7\x88\xac\', \'\xe5\xbd\xbc\', \'\xe5\xbe\x84\', \'\xe6\x89\x80\', \'\xe8\x88\x8d\', \'\xe9\x87\x91\', \'\xe5\x91\xbd\',\n    \'\xe6\x96\xa7\', \'\xe7\x88\xb8\', \'\xe9\x87\x87\', \'\xe5\x8f\x97\', \'\xe4\xb9\xb3\', \'\xe8\xb4\xaa\', \'\xe5\xbf\xb5\', \'\xe8\xb4\xab\', \'\xe8\x82\xa4\', \'\xe8\x82\xba\', \'\xe8\x82\xa2\', \'\xe8\x82\xbf\', \'\xe8\x83\x80\', \'\xe6\x9c\x8b\', \'\xe8\x82\xa1\', \'\xe8\x82\xa5\', \'\xe6\x9c\x8d\', \'\xe8\x83\x81\', \'\xe5\x91\xa8\', \'\xe6\x98\x8f\',\n    \'\xe9\xb1\xbc\', \'\xe5\x85\x94\', \'\xe7\x8b\x90\', \'\xe5\xbf\xbd\', \'\xe7\x8b\x97\', \'\xe5\xa4\x87\', \'\xe9\xa5\xb0\', \'\xe9\xa5\xb1\', \'\xe9\xa5\xb2\', \'\xe5\x8f\x98\', \'\xe4\xba\xac\', \'\xe4\xba\xab\', \'\xe5\xba\x97\', \'\xe5\xa4\x9c\', \'\xe5\xba\x99\', \'\xe5\xba\x9c\', \'\xe5\xba\x95\', \'\xe5\x89\x82\', \'\xe9\x83\x8a\', \'\xe5\xba\x9f\',\n    \'\xe5\x87\x80\', \'\xe7\x9b\xb2\', \'\xe6\x94\xbe\', \'\xe5\x88\xbb\', \'\xe8\x82\xb2\', \'\xe9\x97\xb8\', \'\xe9\x97\xb9\', \'\xe9\x83\x91\', \'\xe5\x88\xb8\', \'\xe5\x8d\xb7\', \'\xe5\x8d\x95\', \'\xe7\x82\x92\', \'\xe7\x82\x8a\', \'\xe7\x82\x95\', \'\xe7\x82\x8e\', \'\xe7\x82\x89\', \'\xe6\xb2\xab\', \'\xe6\xb5\x85\', \'\xe6\xb3\x95\', \'\xe6\xb3\x84\',\n    \'\xe6\xb2\xb3\', \'\xe6\xb2\xbe\', \'\xe6\xb3\xaa\', \'\xe6\xb2\xb9\', \'\xe6\xb3\x8a\', \'\xe6\xb2\xbf\', \'\xe6\xb3\xa1\', \'\xe6\xb3\xa8\', \'\xe6\xb3\xbb\', \'\xe6\xb3\xb3\', \'\xe6\xb3\xa5\', \'\xe6\xb2\xb8\', \'\xe6\xb3\xa2\', \'\xe6\xb3\xbc\', \'\xe6\xb3\xbd\', \'\xe6\xb2\xbb\', \'\xe6\x80\x96\', \'\xe6\x80\xa7\', \'\xe6\x80\x95\', \'\xe6\x80\x9c\',\n    \'\xe6\x80\xaa\', \'\xe5\xad\xa6\', \'\xe5\xae\x9d\', \'\xe5\xae\x97\', \'\xe5\xae\x9a\', \'\xe5\xae\x9c\', \'\xe5\xae\xa1\', \'\xe5\xae\x99\', \'\xe5\xae\x98\', \'\xe7\xa9\xba\', \'\xe5\xb8\x98\', \'\xe5\xae\x9e\', \'\xe8\xaf\x95\', \'\xe9\x83\x8e\', \'\xe8\xaf\x97\', \'\xe8\x82\xa9\', \'\xe6\x88\xbf\', \'\xe8\xaf\x9a\', \'\xe8\xa1\xac\', \'\xe8\xa1\xab\',\n    \'\xe8\xa7\x86\', \'\xe8\xaf\x9d\', \'\xe8\xaf\x9e\', \'\xe8\xaf\xa2\', \'\xe8\xaf\xa5\', \'\xe8\xaf\xa6\', \'\xe5\xbb\xba\', \'\xe8\x82\x83\', \'\xe5\xbd\x95\', \'\xe9\x9a\xb6\', \'\xe5\xb1\x85\', \'\xe5\xb1\x8a\', \'\xe5\x88\xb7\', \'\xe5\xb1\x88\', \'\xe5\xbc\xa6\', \'\xe6\x89\xbf\', \'\xe5\xad\x9f\', \'\xe5\xad\xa4\', \'\xe9\x99\x95\', \'\xe9\x99\x8d\',\n    \'\xe9\x99\x90\', \'\xe5\xa6\xb9\', \'\xe5\xa7\x91\', \'\xe5\xa7\x90\', \'\xe5\xa7\x93\', \'\xe5\xa7\x8b\', \'\xe9\xa9\xbe\', \'\xe5\x8f\x82\', \'\xe8\x89\xb0\', \'\xe7\xba\xbf\', \'\xe7\xbb\x83\', \'\xe7\xbb\x84\', \'\xe7\xbb\x86\', \'\xe9\xa9\xb6\', \'\xe7\xbb\x87\', \'\xe7\xbb\x88\', \'\xe9\xa9\xbb\', \'\xe9\xa9\xbc\', \'\xe7\xbb\x8d\', \'\xe7\xbb\x8f\',\n    \'\xe8\xb4\xaf\', \'\xe5\xa5\x8f\', \'\xe6\x98\xa5\', \'\xe5\xb8\xae\', \'\xe7\x8f\x8d\', \'\xe7\x8e\xbb\', \'\xe6\xaf\x92\', \'\xe5\x9e\x8b\', \'\xe6\x8c\x82\', \'\xe5\xb0\x81\', \'\xe6\x8c\x81\', \'\xe9\xa1\xb9\', \'\xe5\x9e\xae\', \'\xe6\x8c\x8e\', \'\xe5\x9f\x8e\', \'\xe6\x8c\xa0\', \'\xe6\x94\xbf\', \'\xe8\xb5\xb4\', \'\xe8\xb5\xb5\', \'\xe6\x8c\xa1\',\n    \'\xe6\x8c\xba\', \'\xe6\x8b\xac\', \'\xe6\x8b\xb4\', \'\xe6\x8b\xbe\', \'\xe6\x8c\x91\', \'\xe6\x8c\x87\', \'\xe5\x9e\xab\', \'\xe6\x8c\xa3\', \'\xe6\x8c\xa4\', \'\xe6\x8b\xbc\', \'\xe6\x8c\x96\', \'\xe6\x8c\x89\', \'\xe6\x8c\xa5\', \'\xe6\x8c\xaa\', \'\xe6\x9f\x90\', \'\xe7\x94\x9a\', \'\xe9\x9d\xa9\', \'\xe8\x8d\x90\', \'\xe5\xb7\xb7\', \'\xe5\xb8\xa6\',\n    \'\xe8\x8d\x89\', \'\xe8\x8c\xa7\', \'\xe8\x8c\xb6\', \'\xe8\x8d\x92\', \'\xe8\x8c\xab\', \'\xe8\x8d\xa1\', \'\xe8\x8d\xa3\', \'\xe6\x95\x85\', \'\xe8\x83\xa1\', \'\xe5\x8d\x97\', \'\xe8\x8d\xaf\', \'\xe6\xa0\x87\', \'\xe6\x9e\xaf\', \'\xe6\x9f\x84\', \'\xe6\xa0\x8b\', \'\xe7\x9b\xb8\', \'\xe6\x9f\xa5\', \'\xe6\x9f\x8f\', \'\xe6\x9f\xb3\', \'\xe6\x9f\xb1\',\n    \'\xe6\x9f\xbf\', \'\xe6\xa0\x8f\', \'\xe6\xa0\x91\', \'\xe8\xa6\x81\', \'\xe5\x92\xb8\', \'\xe5\xa8\x81\', \'\xe6\xad\xaa\', \'\xe7\xa0\x94\', \'\xe7\xa0\x96\', \'\xe5\x8e\x98\', \'\xe5\x8e\x9a\', \'\xe7\xa0\x8c\', \'\xe7\xa0\x8d\', \'\xe9\x9d\xa2\', \'\xe8\x80\x90\', \'\xe8\x80\x8d\', \'\xe7\x89\xb5\', \'\xe6\xae\x8b\', \'\xe6\xae\x83\', \'\xe8\xbd\xbb\',\n    \'\xe9\xb8\xa6\', \'\xe7\x9a\x86\', \'\xe8\x83\x8c\', \'\xe6\x88\x98\', \'\xe7\x82\xb9\', \'\xe4\xb8\xb4\', \'\xe8\xa7\x88\', \'\xe7\xab\x96\', \'\xe7\x9c\x81\', \'\xe5\x89\x8a\', \'\xe5\xb0\x9d\', \'\xe6\x98\xaf\', \'\xe7\x9b\xbc\', \'\xe7\x9c\xa8\', \'\xe5\x93\x84\', \'\xe6\x98\xbe\', \'\xe5\x93\x91\', \'\xe5\x86\x92\', \'\xe6\x98\xa0\', \'\xe6\x98\x9f\',\n    \'\xe6\x98\xa8\', \'\xe7\x95\x8f\', \'\xe8\xb6\xb4\', \'\xe8\x83\x83\', \'\xe8\xb4\xb5\', \'\xe7\x95\x8c\', \'\xe8\x99\xb9\', \'\xe8\x99\xbe\', \'\xe8\x9a\x81\', \'\xe6\x80\x9d\', \'\xe8\x9a\x82\', \'\xe8\x99\xbd\', \'\xe5\x93\x81\', \'\xe5\x92\xbd\', \'\xe9\xaa\x82\', \'\xe5\x93\x97\', \'\xe5\x92\xb1\', \'\xe5\x93\x8d\', \'\xe5\x93\x88\', \'\xe5\x92\xac\',\n    \'\xe5\x92\xb3\', \'\xe5\x93\xaa\', \'\xe7\x82\xad\', \'\xe5\xb3\xa1\', \'\xe7\xbd\x9a\', \'\xe8\xb4\xb1\', \'\xe8\xb4\xb4\', \'\xe9\xaa\xa8\', \'\xe9\x92\x9e\', \'\xe9\x92\x9f\', \'\xe9\x92\xa2\', \'\xe9\x92\xa5\', \'\xe9\x92\xa9\', \'\xe5\x8d\xb8\', \'\xe7\xbc\xb8\', \'\xe6\x8b\x9c\', \'\xe7\x9c\x8b\', \'\xe7\x9f\xa9\', \'\xe6\x80\x8e\', \'\xe7\x89\xb2\',\n    \'\xe9\x80\x89\', \'\xe9\x80\x82\', \'\xe7\xa7\x92\', \'\xe9\xa6\x99\', \'\xe7\xa7\x8d\', \'\xe7\xa7\x8b\', \'\xe7\xa7\x91\', \'\xe9\x87\x8d\', \'\xe5\xa4\x8d\', \'\xe7\xab\xbf\', \'\xe6\xae\xb5\', \'\xe4\xbe\xbf\', \'\xe4\xbf\xa9\', \'\xe8\xb4\xb7\', \'\xe9\xa1\xba\', \'\xe4\xbf\xae\', \'\xe4\xbf\x9d\', \'\xe4\xbf\x83\', \'\xe4\xbe\xae\', \'\xe4\xbf\xad\',\n    \'\xe4\xbf\x97\', \'\xe4\xbf\x98\', \'\xe4\xbf\xa1\', \'\xe7\x9a\x87\', \'\xe6\xb3\x89\', \'\xe9\xac\xbc\', \'\xe4\xbe\xb5\', \'\xe8\xbf\xbd\', \'\xe4\xbf\x8a\', \'\xe7\x9b\xbe\', \'\xe5\xbe\x85\', \'\xe5\xbe\x8b\', \'\xe5\xbe\x88\', \'\xe9\xa1\xbb\', \'\xe5\x8f\x99\', \'\xe5\x89\x91\', \'\xe9\x80\x83\', \'\xe9\xa3\x9f\', \'\xe7\x9b\x86\', \'\xe8\x83\x86\',\n    \'\xe8\x83\x9c\', \'\xe8\x83\x9e\', \'\xe8\x83\x96\', \'\xe8\x84\x89\', \'\xe5\x8b\x89\', \'\xe7\x8b\xad\', \'\xe7\x8b\xae\', \'\xe7\x8b\xac\', \'\xe7\x8b\xa1\', \'\xe7\x8b\xb1\', \'\xe7\x8b\xa0\', \'\xe8\xb4\xb8\', \'\xe6\x80\xa8\', \'\xe6\x80\xa5\', \'\xe9\xa5\xb6\', \'\xe8\x9a\x80\', \'\xe9\xa5\xba\', \'\xe9\xa5\xbc\', \'\xe5\xbc\xaf\', \'\xe5\xb0\x86\',\n    \'\xe5\xa5\x96\', \'\xe5\x93\x80\', \'\xe4\xba\xad\', \'\xe4\xba\xae\', \'\xe5\xba\xa6\', \'\xe8\xbf\xb9\', \'\xe5\xba\xad\', \'\xe7\x96\xae\', \'\xe7\x96\xaf\', \'\xe7\x96\xab\', \'\xe7\x96\xa4\', \'\xe5\xa7\xbf\', \'\xe4\xba\xb2\', \'\xe9\x9f\xb3\', \'\xe5\xb8\x9d\', \'\xe6\x96\xbd\', \'\xe9\x97\xbb\', \'\xe9\x98\x80\', \'\xe9\x98\x81\', \'\xe5\xb7\xae\',\n    \'\xe5\x85\xbb\', \'\xe7\xbe\x8e\', \'\xe5\xa7\x9c\', \'\xe5\x8f\x9b\', \'\xe9\x80\x81\', \'\xe7\xb1\xbb\', \'\xe8\xbf\xb7\', \'\xe5\x89\x8d\', \'\xe9\xa6\x96\', \'\xe9\x80\x86\', \'\xe6\x80\xbb\', \'\xe7\x82\xbc\', \'\xe7\x82\xb8\', \'\xe7\x82\xae\', \'\xe7\x83\x82\', \'\xe5\x89\x83\', \'\xe6\xb4\x81\', \'\xe6\xb4\xaa\', \'\xe6\xb4\x92\', \'\xe6\xb5\x87\',\n    \'\xe6\xb5\x8a\', \'\xe6\xb4\x9e\', \'\xe6\xb5\x8b\', \'\xe6\xb4\x97\', \'\xe6\xb4\xbb\', \'\xe6\xb4\xbe\', \'\xe6\xb4\xbd\', \'\xe6\x9f\x93\', \'\xe6\xb5\x8e\', \'\xe6\xb4\x8b\', \'\xe6\xb4\xb2\', \'\xe6\xb5\x91\', \'\xe6\xb5\x93\', \'\xe6\xb4\xa5\', \'\xe6\x81\x92\', \'\xe6\x81\xa2\', \'\xe6\x81\xb0\', \'\xe6\x81\xbc\', \'\xe6\x81\xa8\', \'\xe4\xb8\xbe\',\n    \'\xe8\xa7\x89\', \'\xe5\xae\xa3\', \'\xe5\xae\xa4\', \'\xe5\xae\xab\', \'\xe5\xae\xaa\', \'\xe7\xaa\x81\', \'\xe7\xa9\xbf\', \'\xe7\xaa\x83\', \'\xe5\xae\xa2\', \'\xe5\x86\xa0\', \'\xe8\xaf\xad\', \'\xe6\x89\x81\', \'\xe8\xa2\x84\', \'\xe7\xa5\x96\', \'\xe7\xa5\x9e\', \'\xe7\xa5\x9d\', \'\xe8\xaf\xaf\', \'\xe8\xaf\xb1\', \'\xe8\xaf\xb4\', \'\xe8\xaf\xb5\',\n    \'\xe5\x9e\xa6\', \'\xe9\x80\x80\', \'\xe6\x97\xa2\', \'\xe5\xb1\x8b\', \'\xe6\x98\xbc\', \'\xe8\xb4\xb9\', \'\xe9\x99\xa1\', \'\xe7\x9c\x89\', \'\xe5\xad\xa9\', \'\xe9\x99\xa4\', \'\xe9\x99\xa9\', \'\xe9\x99\xa2\', \'\xe5\xa8\x83\', \'\xe5\xa7\xa5\', \'\xe5\xa7\xa8\', \'\xe5\xa7\xbb\', \'\xe5\xa8\x87\', \'\xe6\x80\x92\', \'\xe6\x9e\xb6\', \'\xe8\xb4\xba\',\n    \'\xe7\x9b\x88\', \'\xe5\x8b\x87\', \'\xe6\x80\xa0\', \'\xe6\x9f\x94\', \'\xe5\x9e\x92\', \'\xe7\xbb\x91\', \'\xe7\xbb\x92\', \'\xe7\xbb\x93\', \'\xe7\xbb\x95\', \'\xe9\xaa\x84\', \'\xe7\xbb\x98\', \'\xe7\xbb\x99\', \'\xe7\xbb\x9c\', \'\xe9\xaa\x86\', \'\xe7\xbb\x9d\', \'\xe7\xbb\x9e\', \'\xe7\xbb\x9f\', \'\xe8\x80\x95\', \'\xe8\x80\x97\', \'\xe8\x89\xb3\',\n    \'\xe6\xb3\xb0\', \'\xe7\x8f\xa0\', \'\xe7\x8f\xad\', \'\xe7\xb4\xa0\', \'\xe8\x9a\x95\', \'\xe9\xa1\xbd\', \'\xe7\x9b\x8f\', \'\xe5\x8c\xaa\', \'\xe6\x8d\x9e\', \'\xe6\xa0\xbd\', \'\xe6\x8d\x95\', \'\xe6\x8c\xaf\', \'\xe8\xbd\xbd\', \'\xe8\xb5\xb6\', \'\xe8\xb5\xb7\', \'\xe7\x9b\x90\', \'\xe6\x8d\x8e\', \'\xe6\x8d\x8f\', \'\xe5\x9f\x8b\', \'\xe6\x8d\x89\',\n    \'\xe6\x8d\x86\', \'\xe6\x8d\x90\', \'\xe6\x8d\x9f\', \'\xe9\x83\xbd\', \'\xe5\x93\xb2\', \'\xe9\x80\x9d\', \'\xe6\x8d\xa1\', \'\xe6\x8d\xa2\', \'\xe6\x8c\xbd\', \'\xe7\x83\xad\', \'\xe6\x81\x90\', \'\xe5\xa3\xb6\', \'\xe6\x8c\xa8\', \'\xe8\x80\xbb\', \'\xe8\x80\xbd\', \'\xe6\x81\xad\', \'\xe8\x8e\xb2\', \'\xe8\x8e\xab\', \'\xe8\x8d\xb7\', \'\xe8\x8e\xb7\',\n    \'\xe6\x99\x8b\', \'\xe6\x81\xb6\', \'\xe7\x9c\x9f\', \'\xe6\xa1\x86\', \'\xe6\xa1\x82\', \'\xe6\xa1\xa3\', \'\xe6\xa1\x90\', \'\xe6\xa0\xaa\', \'\xe6\xa1\xa5\', \'\xe6\xa1\x83\', \'\xe6\xa0\xbc\', \'\xe6\xa0\xa1\', \'\xe6\xa0\xb8\', \'\xe6\xa0\xb7\', \'\xe6\xa0\xb9\', \'\xe7\xb4\xa2\', \'\xe5\x93\xa5\', \'\xe9\x80\x9f\', \'\xe9\x80\x97\', \'\xe6\xa0\x97\',\n    \'\xe9\x85\x8d\', \'\xe7\xbf\x85\', \'\xe8\xbe\xb1\', \'\xe5\x94\x87\', \'\xe5\xa4\x8f\', \'\xe7\xa1\x80\', \'\xe7\xa0\xb4\', \'\xe5\x8e\x9f\', \'\xe5\xa5\x97\', \'\xe9\x80\x90\', \'\xe7\x83\x88\', \'\xe6\xae\x8a\', \'\xe9\xa1\xbe\', \'\xe8\xbd\xbf\', \'\xe8\xbe\x83\', \'\xe9\xa1\xbf\', \'\xe6\xaf\x99\', \'\xe8\x87\xb4\', \'\xe6\x9f\xb4\', \'\xe6\xa1\x8c\',\n    \'\xe8\x99\x91\', \'\xe7\x9b\x91\', \'\xe7\xb4\xa7\', \'\xe5\x85\x9a\', \'\xe6\x99\x92\', \'\xe7\x9c\xa0\', \'\xe6\x99\x93\', \'\xe9\xb8\xad\', \'\xe6\x99\x83\', \'\xe6\x99\x8c\', \'\xe6\x99\x95\', \'\xe8\x9a\x8a\', \'\xe5\x93\xa8\', \'\xe5\x93\xad\', \'\xe6\x81\xa9\', \'\xe5\x94\xa4\', \'\xe5\x95\x8a\', \'\xe5\x94\x89\', \'\xe7\xbd\xa2\', \'\xe5\xb3\xb0\',\n    \'\xe5\x9c\x86\', \'\xe8\xb4\xbc\', \'\xe8\xb4\xbf\', \'\xe9\x92\xb1\', \'\xe9\x92\xb3\', \'\xe9\x92\xbb\', \'\xe9\x93\x81\', \'\xe9\x93\x83\', \'\xe9\x93\x85\', \'\xe7\xbc\xba\', \'\xe6\xb0\xa7\', \'\xe7\x89\xb9\', \'\xe7\x89\xba\', \'\xe9\x80\xa0\', \'\xe4\xb9\x98\', \'\xe6\x95\x8c\', \'\xe7\xa7\xa4\', \'\xe7\xa7\x9f\', \'\xe7\xa7\xaf\', \'\xe7\xa7\xa7\',\n    \'\xe7\xa7\xa9\', \'\xe7\xa7\xb0\', \'\xe7\xa7\x98\', \'\xe9\x80\x8f\', \'\xe7\xac\x94\', \'\xe7\xac\x91\', \'\xe7\xac\x8b\', \'\xe5\x80\xba\', \'\xe5\x80\x9f\', \'\xe5\x80\xbc\', \'\xe5\x80\x9a\', \'\xe5\x80\xbe\', \'\xe5\x80\x92\', \'\xe5\x80\x98\', \'\xe4\xbf\xb1\', \'\xe5\x80\xa1\', \'\xe5\x80\x99\', \'\xe4\xbf\xaf\', \'\xe5\x80\x8d\', \'\xe5\x80\xa6\',\n    \'\xe5\x81\xa5\', \'\xe8\x87\xad\', \'\xe5\xb0\x84\', \'\xe8\xba\xac\', \'\xe6\x81\xaf\', \'\xe5\xbe\x92\', \'\xe5\xbe\x90\', \'\xe8\x88\xb0\', \'\xe8\x88\xb1\', \'\xe8\x88\xac\', \'\xe8\x88\xaa\', \'\xe9\x80\x94\', \'\xe6\x8b\xbf\', \'\xe7\x88\xb9\', \'\xe7\x88\xb1\', \'\xe9\xa2\x82\', \'\xe7\xbf\x81\', \'\xe8\x84\x86\', \'\xe8\x84\x82\', \'\xe8\x83\xb8\',\n    \'\xe8\x83\xb3\', \'\xe8\x84\x8f\', \'\xe8\x83\xb6\', \'\xe8\x84\x91\', \'\xe7\x8b\xb8\', \'\xe7\x8b\xbc\', \'\xe9\x80\xa2\', \'\xe7\x95\x99\', \'\xe7\x9a\xb1\', \'\xe9\xa5\xbf\', \'\xe6\x81\x8b\', \'\xe6\xa1\xa8\', \'\xe6\xb5\x86\', \'\xe8\xa1\xb0\', \'\xe9\xab\x98\', \'\xe5\xb8\xad\', \'\xe5\x87\x86\', \'\xe5\xba\xa7\', \'\xe8\x84\x8a\', \'\xe7\x97\x87\',\n    \'\xe7\x97\x85\', \'\xe7\x96\xbe\', \'\xe7\x96\xbc\', \'\xe7\x96\xb2\', \'\xe6\x95\x88\', \'\xe7\xa6\xbb\', \'\xe5\x94\x90\', \'\xe8\xb5\x84\', \'\xe5\x87\x89\', \'\xe7\xab\x99\', \'\xe5\x89\x96\', \'\xe7\xab\x9e\', \'\xe9\x83\xa8\', \'\xe6\x97\x81\', \'\xe6\x97\x85\', \'\xe7\x95\x9c\', \'\xe9\x98\x85\', \'\xe7\xbe\x9e\', \'\xe7\x93\xb6\', \'\xe6\x8b\xb3\',\n    \'\xe7\xb2\x89\', \'\xe6\x96\x99\', \'\xe7\x9b\x8a\', \'\xe5\x85\xbc\', \'\xe7\x83\xa4\', \'\xe7\x83\x98\', \'\xe7\x83\xa6\', \'\xe7\x83\xa7\', \'\xe7\x83\x9b\', \'\xe7\x83\x9f\', \'\xe9\x80\x92\', \'\xe6\xb6\x9b\', \'\xe6\xb5\x99\', \'\xe6\xb6\x9d\', \'\xe9\x85\x92\', \'\xe6\xb6\x89\', \'\xe6\xb6\x88\', \'\xe6\xb5\xa9\', \'\xe6\xb5\xb7\', \'\xe6\xb6\x82\',\n    \'\xe6\xb5\xb4\', \'\xe6\xb5\xae\', \'\xe6\xb5\x81\', \'\xe6\xb6\xa6\', \'\xe6\xb5\xaa\', \'\xe6\xb5\xb8\', \'\xe6\xb6\xa8\', \'\xe7\x83\xab\', \'\xe6\xb6\x8c\', \'\xe6\x82\x9f\', \'\xe6\x82\x84\', \'\xe6\x82\x94\', \'\xe6\x82\xa6\', \'\xe5\xae\xb3\', \'\xe5\xae\xbd\', \'\xe5\xae\xb6\', \'\xe5\xae\xb5\', \'\xe5\xae\xb4\', \'\xe5\xae\xbe\', \'\xe7\xaa\x84\',\n    \'\xe5\xae\xb9\', \'\xe5\xae\xb0\', \'\xe6\xa1\x88\', \'\xe8\xaf\xb7\', \'\xe6\x9c\x97\', \'\xe8\xaf\xb8\', \'\xe8\xaf\xbb\', \'\xe6\x89\x87\', \'\xe8\xa2\x9c\', \'\xe8\xa2\x96\', \'\xe8\xa2\x8d\', \'\xe8\xa2\xab\', \'\xe7\xa5\xa5\', \'\xe8\xaf\xbe\', \'\xe8\xb0\x81\', \'\xe8\xb0\x83\', \'\xe5\x86\xa4\', \'\xe8\xb0\x85\', \'\xe8\xb0\x88\', \'\xe8\xb0\x8a\',\n    \'\xe5\x89\xa5\', \'\xe6\x81\xb3\', \'\xe5\xb1\x95\', \'\xe5\x89\xa7\', \'\xe5\xb1\x91\', \'\xe5\xbc\xb1\', \'\xe9\x99\xb5\', \'\xe9\x99\xb6\', \'\xe9\x99\xb7\', \'\xe9\x99\xaa\', \'\xe5\xa8\xb1\', \'\xe5\xa8\x98\', \'\xe9\x80\x9a\', \'\xe8\x83\xbd\', \'\xe9\x9a\xbe\', \'\xe9\xa2\x84\', \'\xe6\xa1\x91\', \'\xe7\xbb\xa2\', \'\xe7\xbb\xa3\', \'\xe9\xaa\x8c\',\n    \'\xe7\xbb\xa7\', \'\xe7\x90\x83\', \'\xe7\x90\x86\', \'\xe6\x8d\xa7\', \'\xe5\xa0\xb5\', \'\xe6\x8f\x8f\', \'\xe5\x9f\x9f\', \'\xe6\x8e\xa9\', \'\xe6\x8d\xb7\', \'\xe6\x8e\x92\', \'\xe6\x8e\x89\', \'\xe5\xa0\x86\', \'\xe6\x8e\xa8\', \'\xe6\x8e\x80\', \'\xe6\x8e\x88\', \'\xe6\x95\x99\', \'\xe6\x8e\x8f\', \'\xe6\x8e\xa0\', \'\xe5\x9f\xb9\', \'\xe6\x8e\xa5\',\n    \'\xe6\x8e\xa7\', \'\xe6\x8e\xa2\', \'\xe6\x8d\xae\', \'\xe6\x8e\x98\', \'\xe8\x81\x8c\', \'\xe5\x9f\xba\', \'\xe8\x91\x97\', \'\xe5\x8b\x92\', \'\xe9\xbb\x84\', \'\xe8\x90\x8c\', \'\xe8\x90\x9d\', \'\xe8\x8f\x8c\', \'\xe8\x8f\x9c\', \'\xe8\x90\x84\', \'\xe8\x8f\x8a\', \'\xe8\x90\x8d\', \'\xe8\x8f\xa0\', \'\xe8\x90\xa5\', \'\xe6\xa2\xb0\', \'\xe6\xa2\xa6\',\n    \'\xe6\xa2\xa2\', \'\xe6\xa2\x85\', \'\xe6\xa3\x80\', \'\xe6\xa2\xb3\', \'\xe6\xa2\xaf\', \'\xe6\xa1\xb6\', \'\xe6\x95\x91\', \'\xe5\x89\xaf\', \'\xe7\xa5\xa8\', \'\xe6\x88\x9a\', \'\xe7\x88\xbd\', \'\xe8\x81\x8b\', \'\xe8\xa2\xad\', \'\xe7\x9b\x9b\', \'\xe9\x9b\xaa\', \'\xe8\xbe\x85\', \'\xe8\xbe\x86\', \'\xe8\x99\x9a\', \'\xe9\x9b\x80\', \'\xe5\xa0\x82\',\n    \'\xe5\xb8\xb8\', \'\xe5\x8c\x99\', \'\xe6\x99\xa8\', \'\xe7\x9d\x81\', \'\xe7\x9c\xaf\', \'\xe7\x9c\xbc\', \'\xe6\x82\xac\', \'\xe9\x87\x8e\', \'\xe5\x95\xa6\', \'\xe6\x99\x9a\', \'\xe5\x95\x84\', \'\xe8\xb7\x9d\', \'\xe8\xb7\x83\', \'\xe7\x95\xa5\', \'\xe8\x9b\x87\', \'\xe7\xb4\xaf\', \'\xe5\x94\xb1\', \'\xe6\x82\xa3\', \'\xe5\x94\xaf\', \'\xe5\xb4\x96\',\n    \'\xe5\xb4\xad\', \'\xe5\xb4\x87\', \'\xe5\x9c\x88\', \'\xe9\x93\x9c\', \'\xe9\x93\xb2\', \'\xe9\x93\xb6\', \'\xe7\x94\x9c\', \'\xe6\xa2\xa8\', \'\xe7\x8a\x81\', \'\xe7\xa7\xbb\', \'\xe7\xac\xa8\', \'\xe7\xac\xbc\', \'\xe7\xac\x9b\', \'\xe7\xac\xa6\', \'\xe7\xac\xac\', \'\xe6\x95\x8f\', \'\xe5\x81\x9a\', \'\xe8\xa2\x8b\', \'\xe6\x82\xa0\', \'\xe5\x81\xbf\',\n    \'\xe5\x81\xb6\', \'\xe5\x81\xb7\', \'\xe6\x82\xa8\', \'\xe5\x94\xae\', \'\xe5\x81\x9c\', \'\xe5\x81\x8f\', \'\xe5\x81\x87\', \'\xe5\xbe\x97\', \'\xe8\xa1\x94\', \'\xe7\x9b\x98\', \'\xe8\x88\xb9\', \'\xe6\x96\x9c\', \'\xe7\x9b\x92\', \'\xe9\xb8\xbd\', \'\xe6\x82\x89\', \'\xe6\xac\xb2\', \'\xe5\xbd\xa9\', \'\xe9\xa2\x86\', \'\xe8\x84\x9a\', \'\xe8\x84\x96\',\n    \'\xe8\x84\xb8\', \'\xe8\x84\xb1\', \'\xe8\xb1\xa1\', \'\xe5\xa4\x9f\', \'\xe7\x8c\x9c\', \'\xe7\x8c\xaa\', \'\xe7\x8c\x8e\', \'\xe7\x8c\xab\', \'\xe7\x8c\x9b\', \'\xe9\xa6\x85\', \'\xe9\xa6\x86\', \'\xe5\x87\x91\', \'\xe5\x87\x8f\', \'\xe6\xaf\xab\', \'\xe9\xba\xbb\', \'\xe7\x97\x92\', \'\xe7\x97\x95\', \'\xe5\xbb\x8a\', \'\xe5\xba\xb7\', \'\xe5\xba\xb8\',\n    \'\xe9\xb9\xbf\', \'\xe7\x9b\x97\', \'\xe7\xab\xa0\', \'\xe7\xab\x9f\', \'\xe5\x95\x86\', \'\xe6\x97\x8f\', \'\xe6\x97\x8b\', \'\xe6\x9c\x9b\', \'\xe7\x8e\x87\', \'\xe7\x9d\x80\', \'\xe7\x9b\x96\', \'\xe7\xb2\x98\', \'\xe7\xb2\x97\', \'\xe7\xb2\x92\', \'\xe6\x96\xad\', \'\xe5\x89\xaa\', \'\xe5\x85\xbd\', \'\xe6\xb8\x85\', \'\xe6\xb7\xbb\', \'\xe6\xb7\x8b\',\n    \'\xe6\xb7\xb9\', \'\xe6\xb8\xa0\', \'\xe6\xb8\x90\', \'\xe6\xb7\xb7\', \'\xe6\xb8\x94\', \'\xe6\xb7\x98\', \'\xe6\xb6\xb2\', \'\xe6\xb7\xa1\', \'\xe6\xb7\xb1\', \'\xe5\xa9\x86\', \'\xe6\xa2\x81\', \'\xe6\xb8\x97\', \'\xe6\x83\x85\', \'\xe6\x83\x9c\', \'\xe6\x83\xad\', \'\xe6\x82\xbc\', \'\xe6\x83\xa7\', \'\xe6\x83\x95\', \'\xe6\x83\x8a\', \'\xe6\x83\xa8\',\n    \'\xe6\x83\xaf\', \'\xe5\xaf\x87\', \'\xe5\xaf\x84\', \'\xe5\xae\xbf\', \'\xe7\xaa\x91\', \'\xe5\xaf\x86\', \'\xe8\xb0\x8b\', \'\xe8\xb0\x8e\', \'\xe7\xa5\xb8\', \'\xe8\xb0\x9c\', \'\xe9\x80\xae\', \'\xe6\x95\xa2\', \'\xe5\xb1\xa0\', \'\xe5\xbc\xb9\', \'\xe9\x9a\x8f\', \'\xe8\x9b\x8b\', \'\xe9\x9a\x86\', \'\xe9\x9a\x90\', \'\xe5\xa9\x9a\', \'\xe5\xa9\xb6\',\n    \'\xe9\xa2\x88\', \'\xe7\xbb\xa9\', \'\xe7\xbb\xaa\', \'\xe7\xbb\xad\', \'\xe9\xaa\x91\', \'\xe7\xbb\xb3\', \'\xe7\xbb\xb4\', \'\xe7\xbb\xb5\', \'\xe7\xbb\xb8\', \'\xe7\xbb\xbf\', \'\xe7\x90\xb4\', \'\xe6\x96\x91\', \'\xe6\x9b\xbf\', \'\xe6\xac\xbe\', \'\xe5\xa0\xaa\', \'\xe6\x90\xad\', \'\xe5\xa1\x94\', \'\xe8\xb6\x8a\', \'\xe8\xb6\x81\', \'\xe8\xb6\x8b\',\n    \'\xe8\xb6\x85\', \'\xe6\x8f\x90\', \'\xe5\xa0\xa4\', \'\xe5\x8d\x9a\', \'\xe6\x8f\xad\', \'\xe5\x96\x9c\', \'\xe6\x8f\x92\', \'\xe6\x8f\xaa\', \'\xe6\x90\x9c\', \'\xe7\x85\xae\', \'\xe6\x8f\xb4\', \'\xe8\xa3\x81\', \'\xe6\x90\x81\', \'\xe6\x90\x82\', \'\xe6\x90\x85\', \'\xe6\x8f\xa1\', \'\xe6\x8f\x89\', \'\xe6\x96\xaf\', \'\xe6\x9c\x9f\', \'\xe6\xac\xba\',\n    \'\xe8\x81\x94\', \'\xe6\x95\xa3\', \'\xe6\x83\xb9\', \'\xe8\x91\xac\', \'\xe8\x91\x9b\', \'\xe8\x91\xa3\', \'\xe8\x91\xa1\', \'\xe6\x95\xac\', \'\xe8\x91\xb1\', \'\xe8\x90\xbd\', \'\xe6\x9c\x9d\', \'\xe8\xbe\x9c\', \'\xe8\x91\xb5\', \'\xe6\xa3\x92\', \'\xe6\xa3\x8b\', \'\xe6\xa4\x8d\', \'\xe6\xa3\xae\', \'\xe6\xa4\x85\', \'\xe6\xa4\x92\', \'\xe6\xa3\xb5\',\n    \'\xe6\xa3\x8d\', \'\xe6\xa3\x89\', \'\xe6\xa3\x9a\', \'\xe6\xa3\x95\', \'\xe6\x83\xa0\', \'\xe6\x83\x91\', \'\xe9\x80\xbc\', \'\xe5\x8e\xa8\', \'\xe5\x8e\xa6\', \'\xe7\xa1\xac\', \'\xe7\xa1\xae\', \'\xe9\x9b\x81\', \'\xe6\xae\x96\', \'\xe8\xa3\x82\', \'\xe9\x9b\x84\', \'\xe6\x9a\x82\', \'\xe9\x9b\x85\', \'\xe8\xbe\x88\', \'\xe6\x82\xb2\', \'\xe7\xb4\xab\',\n    \'\xe8\xbe\x89\', \'\xe6\x95\x9e\', \'\xe8\xb5\x8f\', \'\xe6\x8e\x8c\', \'\xe6\x99\xb4\', \'\xe6\x9a\x91\', \'\xe6\x9c\x80\', \'\xe9\x87\x8f\', \'\xe5\x96\xb7\', \'\xe6\x99\xb6\', \'\xe5\x96\x87\', \'\xe9\x81\x87\', \'\xe5\x96\x8a\', \'\xe6\x99\xaf\', \'\xe8\xb7\xb5\', \'\xe8\xb7\x8c\', \'\xe8\xb7\x91\', \'\xe9\x81\x97\', \'\xe8\x9b\x99\', \'\xe8\x9b\x9b\',\n    \'\xe8\x9c\x93\', \'\xe5\x96\x9d\', \'\xe5\x96\x82\', \'\xe5\x96\x98\', \'\xe5\x96\x89\', \'\xe5\xb9\x85\', \'\xe5\xb8\xbd\', \'\xe8\xb5\x8c\', \'\xe8\xb5\x94\', \'\xe9\xbb\x91\', \'\xe9\x93\xb8\', \'\xe9\x93\xba\', \'\xe9\x93\xbe\', \'\xe9\x94\x80\', \'\xe9\x94\x81\', \'\xe9\x94\x84\', \'\xe9\x94\x85\', \'\xe9\x94\x88\', \'\xe9\x94\x8b\', \'\xe9\x94\x90\',\n    \'\xe7\x9f\xad\', \'\xe6\x99\xba\', \'\xe6\xaf\xaf\', \'\xe9\xb9\x85\', \'\xe5\x89\xa9\', \'\xe7\xa8\x8d\', \'\xe7\xa8\x8b\', \'\xe7\xa8\x80\', \'\xe7\xa8\x8e\', \'\xe7\xad\x90\', \'\xe7\xad\x89\', \'\xe7\xad\x91\', \'\xe7\xad\x96\', \'\xe7\xad\x9b\', \'\xe7\xad\x92\', \'\xe7\xad\x94\', \'\xe7\xad\x8b\', \'\xe7\xad\x9d\', \'\xe5\x82\xb2\', \'\xe5\x82\x85\',\n    \'\xe7\x89\x8c\', \'\xe5\xa0\xa1\', \'\xe9\x9b\x86\', \'\xe7\x84\xa6\', \'\xe5\x82\x8d\', \'\xe5\x82\xa8\', \'\xe5\xa5\xa5\', \'\xe8\xa1\x97\', \'\xe6\x83\xa9\', \'\xe5\xbe\xa1\', \'\xe5\xbe\xaa\', \'\xe8\x89\x87\', \'\xe8\x88\x92\', \'\xe7\x95\xaa\', \'\xe9\x87\x8a\', \'\xe7\xa6\xbd\', \'\xe8\x85\x8a\', \'\xe8\x84\xbe\', \'\xe8\x85\x94\', \'\xe9\xb2\x81\',\n    \'\xe7\x8c\xbe\', \'\xe7\x8c\xb4\', \'\xe7\x84\xb6\', \'\xe9\xa6\x8b\', \'\xe8\xa3\x85\', \'\xe8\x9b\xae\', \'\xe5\xb0\xb1\', \'\xe7\x97\x9b\', \'\xe7\xab\xa5\', \'\xe9\x98\x94\', \'\xe5\x96\x84\', \'\xe7\xbe\xa1\', \'\xe6\x99\xae\', \'\xe7\xb2\xaa\', \'\xe5\xb0\x8a\', \'\xe9\x81\x93\', \'\xe6\x9b\xbe\', \'\xe7\x84\xb0\', \'\xe6\xb8\xaf\', \'\xe6\xb9\x96\',\n    \'\xe6\xb8\xa3\', \'\xe6\xb9\xbf\', \'\xe6\xb8\xa9\', \'\xe6\xb8\xb4\', \'\xe6\xbb\x91\', \'\xe6\xb9\xbe\', \'\xe6\xb8\xa1\', \'\xe6\xb8\xb8\', \'\xe6\xbb\x8b\', \'\xe6\xba\x89\', \'\xe6\x84\xa4\', \'\xe6\x85\x8c\', \'\xe6\x83\xb0\', \'\xe6\x84\xa7\', \'\xe6\x84\x89\', \'\xe6\x85\xa8\', \'\xe5\x89\xb2\', \'\xe5\xaf\x92\', \'\xe5\xaf\x8c\', \'\xe7\xaa\x9c\',\n    \'\xe7\xaa\x9d\', \'\xe7\xaa\x97\', \'\xe9\x81\x8d\', \'\xe8\xa3\x95\', \'\xe8\xa3\xa4\', \'\xe8\xa3\x99\', \'\xe8\xb0\xa2\', \'\xe8\xb0\xa3\', \'\xe8\xb0\xa6\', \'\xe5\xb1\x9e\', \'\xe5\xb1\xa1\', \'\xe5\xbc\xba\', \'\xe7\xb2\xa5\', \'\xe7\x96\x8f\', \'\xe9\x9a\x94\', \'\xe9\x9a\x99\', \'\xe7\xb5\xae\', \'\xe5\xab\x82\', \'\xe7\x99\xbb\', \'\xe7\xbc\x8e\',\n    \'\xe7\xbc\x93\', \'\xe7\xbc\x96\', \'\xe9\xaa\x97\', \'\xe7\xbc\x98\', \'\xe7\x91\x9e\', \'\xe9\xad\x82\', \'\xe8\x82\x86\', \'\xe6\x91\x84\', \'\xe6\x91\xb8\', \'\xe5\xa1\xab\', \'\xe6\x90\x8f\', \'\xe5\xa1\x8c\', \'\xe9\xbc\x93\', \'\xe6\x91\x86\', \'\xe6\x90\xba\', \'\xe6\x90\xac\', \'\xe6\x91\x87\', \'\xe6\x90\x9e\', \'\xe5\xa1\x98\', \'\xe6\x91\x8a\',\n    \'\xe8\x92\x9c\', \'\xe5\x8b\xa4\', \'\xe9\xb9\x8a\', \'\xe8\x93\x9d\', \'\xe5\xa2\x93\', \'\xe5\xb9\x95\', \'\xe8\x93\xac\', \'\xe8\x93\x84\', \'\xe8\x92\x99\', \'\xe8\x92\xb8\', \'\xe7\x8c\xae\', \'\xe7\xa6\x81\', \'\xe6\xa5\x9a\', \'\xe6\x83\xb3\', \'\xe6\xa7\x90\', \'\xe6\xa6\x86\', \'\xe6\xa5\xbc\', \'\xe6\xa6\x82\', \'\xe8\xb5\x96\', \'\xe9\x85\xac\',\n    \'\xe6\x84\x9f\', \'\xe7\xa2\x8d\', \'\xe7\xa2\x91\', \'\xe7\xa2\x8e\', \'\xe7\xa2\xb0\', \'\xe7\xa2\x97\', \'\xe7\xa2\x8c\', \'\xe9\x9b\xb7\', \'\xe9\x9b\xb6\', \'\xe9\x9b\xbe\', \'\xe9\x9b\xb9\', \'\xe8\xbe\x93\', \'\xe7\x9d\xa3\', \'\xe9\xbe\x84\', \'\xe9\x89\xb4\', \'\xe7\x9d\x9b\', \'\xe7\x9d\xa1\', \'\xe7\x9d\xac\', \'\xe9\x84\x99\', \'\xe6\x84\x9a\',\n    \'\xe6\x9a\x96\', \'\xe7\x9b\x9f\', \'\xe6\xad\x87\', \'\xe6\x9a\x97\', \'\xe7\x85\xa7\', \'\xe8\xb7\xa8\', \'\xe8\xb7\xb3\', \'\xe8\xb7\xaa\', \'\xe8\xb7\xaf\', \'\xe8\xb7\x9f\', \'\xe9\x81\xa3\', \'\xe8\x9b\xbe\', \'\xe8\x9c\x82\', \'\xe5\x97\x93\', \'\xe7\xbd\xae\', \'\xe7\xbd\xaa\', \'\xe7\xbd\xa9\', \'\xe9\x94\x99\', \'\xe9\x94\xa1\', \'\xe9\x94\xa3\',\n    \'\xe9\x94\xa4\', \'\xe9\x94\xa6\', \'\xe9\x94\xae\', \'\xe9\x94\xaf\', \'\xe7\x9f\xae\', \'\xe8\xbe\x9e\', \'\xe7\xa8\xa0\', \'\xe6\x84\x81\', \'\xe7\xad\xb9\', \'\xe7\xad\xbe\', \'\xe7\xae\x80\', \'\xe6\xaf\x81\', \'\xe8\x88\x85\', \'\xe9\xbc\xa0\', \'\xe5\x82\xac\', \'\xe5\x82\xbb\', \'\xe5\x83\x8f\', \'\xe8\xba\xb2\', \'\xe5\xbe\xae\', \'\xe6\x84\x88\',\n    \'\xe9\x81\xa5\', \'\xe8\x85\xb0\', \'\xe8\x85\xa5\', \'\xe8\x85\xb9\', \'\xe8\x85\xbe\', \'\xe8\x85\xbf\', \'\xe8\xa7\xa6\', \'\xe8\xa7\xa3\', \'\xe9\x85\xb1\', \'\xe7\x97\xb0\', \'\xe5\xbb\x89\', \'\xe6\x96\xb0\', \'\xe9\x9f\xb5\', \'\xe6\x84\x8f\', \'\xe7\xb2\xae\', \'\xe6\x95\xb0\', \'\xe7\x85\x8e\', \'\xe5\xa1\x91\', \'\xe6\x85\x88\', \'\xe7\x85\xa4\',\n    \'\xe7\x85\x8c\', \'\xe6\xbb\xa1\', \'\xe6\xbc\xa0\', \'\xe6\xba\x90\', \'\xe6\xbb\xa4\', \'\xe6\xbb\xa5\', \'\xe6\xbb\x94\', \'\xe6\xba\xaa\', \'\xe6\xba\x9c\', \'\xe6\xbb\x9a\', \'\xe6\xbb\xa8\', \'\xe7\xb2\xb1\', \'\xe6\xbb\xa9\', \'\xe6\x85\x8e\', \'\xe8\xaa\x89\', \'\xe5\xa1\x9e\', \'\xe8\xb0\xa8\', \'\xe7\xa6\x8f\', \'\xe7\xbe\xa4\', \'\xe6\xae\xbf\',\n    \'\xe8\xbe\x9f\', \'\xe9\x9a\x9c\', \'\xe5\xab\x8c\', \'\xe5\xab\x81\', \'\xe5\x8f\xa0\', \'\xe7\xbc\x9d\', \'\xe7\xbc\xa0\', \'\xe9\x9d\x99\', \'\xe7\xa2\xa7\', \'\xe7\x92\x83\', \'\xe5\xa2\x99\', \'\xe6\x92\x87\', \'\xe5\x98\x89\', \'\xe6\x91\xa7\', \'\xe6\x88\xaa\', \'\xe8\xaa\x93\', \'\xe5\xa2\x83\', \'\xe6\x91\x98\', \'\xe6\x91\x94\', \'\xe8\x81\x9a\',\n    \'\xe8\x94\xbd\', \'\xe6\x85\x95\', \'\xe6\x9a\xae\', \'\xe8\x94\x91\', \'\xe6\xa8\xa1\', \'\xe6\xa6\xb4\', \'\xe6\xa6\x9c\', \'\xe6\xa6\xa8\', \'\xe6\xad\x8c\', \'\xe9\x81\xad\', \'\xe9\x85\xb7\', \'\xe9\x85\xbf\', \'\xe9\x85\xb8\', \'\xe7\xa3\x81\', \'\xe6\x84\xbf\', \'\xe9\x9c\x80\', \'\xe5\xbc\x8a\', \'\xe8\xa3\xb3\', \'\xe9\xa2\x97\', \'\xe5\x97\xbd\',\n    \'\xe8\x9c\xbb\', \'\xe8\x9c\xa1\', \'\xe8\x9d\x87\', \'\xe8\x9c\x98\', \'\xe8\xb5\x9a\', \'\xe9\x94\xb9\', \'\xe9\x94\xbb\', \'\xe8\x88\x9e\', \'\xe7\xa8\xb3\', \'\xe7\xae\x97\', \'\xe7\xae\xa9\', \'\xe7\xae\xa1\', \'\xe5\x83\x9a\', \'\xe9\xbc\xbb\', \'\xe9\xad\x84\', \'\xe8\xb2\x8c\', \'\xe8\x86\x9c\', \'\xe8\x86\x8a\', \'\xe8\x86\x80\', \'\xe9\xb2\x9c\',\n    \'\xe7\x96\x91\', \'\xe9\xa6\x92\', \'\xe8\xa3\xb9\', \'\xe6\x95\xb2\', \'\xe8\xb1\xaa\', \'\xe8\x86\x8f\', \'\xe9\x81\xae\', \'\xe8\x85\x90\', \'\xe7\x98\xa6\', \'\xe8\xbe\xa3\', \'\xe7\xab\xad\', \'\xe7\xab\xaf\', \'\xe6\x97\x97\', \'\xe7\xb2\xbe\', \'\xe6\xad\x89\', \'\xe7\x86\x84\', \'\xe7\x86\x94\', \'\xe6\xbc\x86\', \'\xe6\xbc\x82\', \'\xe6\xbc\xab\',\n    \'\xe6\xbb\xb4\', \'\xe6\xbc\x94\', \'\xe6\xbc\x8f\', \'\xe6\x85\xa2\', \'\xe5\xaf\xa8\', \'\xe8\xb5\x9b\', \'\xe5\xaf\x9f\', \'\xe8\x9c\x9c\', \'\xe8\xb0\xb1\', \'\xe5\xab\xa9\', \'\xe7\xbf\xa0\', \'\xe7\x86\x8a\', \'\xe5\x87\xb3\', \'\xe9\xaa\xa1\', \'\xe7\xbc\xa9\', \'\xe6\x85\xa7\', \'\xe6\x92\x95\', \'\xe6\x92\x92\', \'\xe8\xb6\xa3\', \'\xe8\xb6\x9f\',\n    \'\xe6\x92\x91\', \'\xe6\x92\xad\', \'\xe6\x92\x9e\', \'\xe6\x92\xa4\', \'\xe5\xa2\x9e\', \'\xe8\x81\xaa\', \'\xe9\x9e\x8b\', \'\xe8\x95\x89\', \'\xe8\x94\xac\', \'\xe6\xa8\xaa\', \'\xe6\xa7\xbd\', \'\xe6\xa8\xb1\', \'\xe6\xa9\xa1\', \'\xe9\xa3\x98\', \'\xe9\x86\x8b\', \'\xe9\x86\x89\', \'\xe9\x9c\x87\', \'\xe9\x9c\x89\', \'\xe7\x9e\x92\', \'\xe9\xa2\x98\',\n    \'\xe6\x9a\xb4\', \'\xe7\x9e\x8e\', \'\xe5\xbd\xb1\', \'\xe8\xb8\xa2\', \'\xe8\xb8\x8f\', \'\xe8\xb8\xa9\', \'\xe8\xb8\xaa\', \'\xe8\x9d\xb6\', \'\xe8\x9d\xb4\', \'\xe5\x98\xb1\', \'\xe5\xa2\xa8\', \'\xe9\x95\x87\', \'\xe9\x9d\xa0\', \'\xe7\xa8\xbb\', \'\xe9\xbb\x8e\', \'\xe7\xa8\xbf\', \'\xe7\xa8\xbc\', \'\xe7\xae\xb1\', \'\xe7\xae\xad\', \'\xe7\xaf\x87\',\n    \'\xe5\x83\xb5\', \'\xe8\xba\xba\', \'\xe5\x83\xbb\', \'\xe5\xbe\xb7\', \'\xe8\x89\x98\', \'\xe8\x86\x9d\', \'\xe8\x86\x9b\', \'\xe7\x86\x9f\', \'\xe6\x91\xa9\', \'\xe9\xa2\x9c\', \'\xe6\xaf\x85\', \'\xe7\xb3\x8a\', \'\xe9\x81\xb5\', \'\xe6\xbd\x9c\', \'\xe6\xbd\xae\', \'\xe6\x87\x82\', \'\xe9\xa2\x9d\', \'\xe6\x85\xb0\', \'\xe5\x8a\x88\', \'\xe6\x93\x8d\',\n    \'\xe7\x87\x95\', \'\xe8\x96\xaf\', \'\xe8\x96\xaa\', \'\xe8\x96\x84\', \'\xe9\xa2\xa0\', \'\xe6\xa9\x98\', \'\xe6\x95\xb4\', \'\xe8\x9e\x8d\', \'\xe9\x86\x92\', \'\xe9\xa4\x90\', \'\xe5\x98\xb4\', \'\xe8\xb9\x84\', \'\xe5\x99\xa8\', \'\xe8\xb5\xa0\', \'\xe9\xbb\x98\', \'\xe9\x95\x9c\', \'\xe8\xb5\x9e\', \'\xe7\xaf\xae\', \'\xe9\x82\x80\', \'\xe8\xa1\xa1\',\n    \'\xe8\x86\xa8\', \'\xe9\x9b\x95\', \'\xe7\xa3\xa8\', \'\xe5\x87\x9d\', \'\xe8\xbe\xa8\', \'\xe8\xbe\xa9\', \'\xe7\xb3\x96\', \'\xe7\xb3\x95\', \'\xe7\x87\x83\', \'\xe6\xbe\xa1\', \'\xe6\xbf\x80\', \'\xe6\x87\x92\', \'\xe5\xa3\x81\', \'\xe9\x81\xbf\', \'\xe7\xbc\xb4\', \'\xe6\x88\xb4\', \'\xe6\x93\xa6\', \'\xe9\x9e\xa0\', \'\xe8\x97\x8f\', \'\xe9\x9c\x9c\',\n    \'\xe9\x9c\x9e\', \'\xe7\x9e\xa7\', \'\xe8\xb9\x88\', \'\xe8\x9e\xba\', \'\xe7\xa9\x97\', \'\xe7\xb9\x81\', \'\xe8\xbe\xab\', \'\xe8\xb5\xa2\', \'\xe7\xb3\x9f\', \'\xe7\xb3\xa0\', \'\xe7\x87\xa5\', \'\xe8\x87\x82\', \'\xe7\xbf\xbc\', \'\xe9\xaa\xa4\', \'\xe9\x9e\xad\', \'\xe8\xa6\x86\', \'\xe8\xb9\xa6\', \'\xe9\x95\xb0\', \'\xe7\xbf\xbb\', \'\xe9\xb9\xb0\',\n    \'\xe8\xad\xa6\', \'\xe6\x94\x80\', \'\xe8\xb9\xb2\', \'\xe9\xa2\xa4\', \'\xe7\x93\xa3\', \'\xe7\x88\x86\', \'\xe7\x96\x86\', \'\xe5\xa3\xa4\', \'\xe8\x80\x80\', \'\xe8\xba\x81\', \'\xe5\x9a\xbc\', \'\xe5\x9a\xb7\', \'\xe7\xb1\x8d\', \'\xe9\xad\x94\', \'\xe7\x81\x8c\', \'\xe8\xa0\xa2\', \'\xe9\x9c\xb8\', \'\xe9\x9c\xb2\', \'\xe5\x9b\x8a\', \'\xe7\xbd\x90\',\n    \'\xe5\x8c\x95\', \'\xe5\x88\x81\', \'\xe4\xb8\x90\', \'\xe6\xad\xb9\', \'\xe6\x88\x88\', \'\xe5\xa4\xad\', \'\xe4\xbb\x91\', \'\xe8\xae\xa5\', \'\xe5\x86\x97\', \'\xe9\x82\x93\', \'\xe8\x89\xbe\', \'\xe5\xa4\xaf\', \'\xe5\x87\xb8\', \'\xe5\x8d\xa2\', \'\xe5\x8f\xad\', \'\xe5\x8f\xbd\', \'\xe7\x9a\xbf\', \'\xe5\x87\xb9\', \'\xe5\x9b\x9a\', \'\xe7\x9f\xa2\',\n    \'\xe4\xb9\x8d\', \'\xe5\xb0\x94\', \'\xe5\x86\xaf\', \'\xe7\x8e\x84\', \'\xe9\x82\xa6\', \'\xe8\xbf\x82\', \'\xe9\x82\xa2\', \'\xe8\x8a\x8b\', \'\xe8\x8a\x8d\', \'\xe5\x90\x8f\', \'\xe5\xa4\xb7\', \'\xe5\x90\x81\', \'\xe5\x90\x95\', \'\xe5\x90\x86\', \'\xe5\xb1\xb9\', \'\xe5\xbb\xb7\', \'\xe8\xbf\x84\', \'\xe8\x87\xbc\', \'\xe4\xbb\xb2\', \'\xe4\xbc\xa6\',\n    \'\xe4\xbc\x8a\', \'\xe8\x82\x8b\', \'\xe6\x97\xad\', \'\xe5\x8c\x88\', \'\xe5\x87\xab\', \'\xe5\xa6\x86\', \'\xe4\xba\xa5\', \'\xe6\xb1\x9b\', \'\xe8\xae\xb3\', \'\xe8\xae\xb6\', \'\xe8\xae\xb9\', \'\xe8\xae\xbc\', \'\xe8\xaf\x80\', \'\xe5\xbc\x9b\', \'\xe9\x98\xb1\', \'\xe9\xa9\xae\', \'\xe9\xa9\xaf\', \'\xe7\xba\xab\', \'\xe7\x8e\x96\', \'\xe7\x8e\x9b\',\n    \'\xe9\x9f\xa7\', \'\xe6\x8a\xa0\', \'\xe6\x89\xbc\', \'\xe6\xb1\x9e\', \'\xe6\x89\xb3\', \'\xe6\x8a\xa1\', \'\xe5\x9d\x8e\', \'\xe5\x9d\x9e\', \'\xe6\x8a\x91\', \'\xe6\x8b\x9f\', \'\xe6\x8a\x92\', \'\xe8\x8a\x99\', \'\xe8\x8a\x9c\', \'\xe8\x8b\x87\', \'\xe8\x8a\xa5\', \'\xe8\x8a\xaf\', \'\xe8\x8a\xad\', \'\xe6\x9d\x96\', \'\xe6\x9d\x89\', \'\xe5\xb7\xab\',\n    \'\xe6\x9d\x88\', \'\xe7\x94\xab\', \'\xe5\x8c\xa3\', \'\xe8\xbd\xa9\', \'\xe5\x8d\xa4\', \'\xe8\x82\x96\', \'\xe5\x90\xb1\', \'\xe5\x90\xa0\', \'\xe5\x91\x95\', \'\xe5\x91\x90\', \'\xe5\x90\x9f\', \'\xe5\x91\x9b\', \'\xe5\x90\xbb\', \'\xe5\x90\xad\', \'\xe9\x82\x91\', \'\xe5\x9b\xa4\', \'\xe5\x90\xae\', \'\xe5\xb2\x96\', \'\xe7\x89\xa1\', \'\xe4\xbd\x91\',\n    \'\xe4\xbd\x83\', \'\xe4\xbc\xba\', \'\xe5\x9b\xb1\', \'\xe8\x82\x9b\', \'\xe8\x82\x98\', \'\xe7\x94\xb8\', \'\xe7\x8b\x88\', \'\xe9\xb8\xa0\', \'\xe5\xbd\xa4\', \'\xe7\x81\xb8\', \'\xe5\x88\xa8\', \'\xe5\xba\x87\', \'\xe5\x90\x9d\', \'\xe5\xba\x90\', \'\xe9\x97\xb0\', \'\xe5\x85\x91\', \'\xe7\x81\xbc\', \'\xe6\xb2\x90\', \'\xe6\xb2\x9b\', \'\xe6\xb1\xb0\',\n    \'\xe6\xb2\xa5\', \'\xe6\xb2\xa6\', \'\xe6\xb1\xb9\', \'\xe6\xb2\xa7\', \'\xe6\xb2\xaa\', \'\xe5\xbf\xb1\', \'\xe8\xaf\x85\', \'\xe8\xaf\x88\', \'\xe7\xbd\x95\', \'\xe5\xb1\x81\', \'\xe5\x9d\xa0\', \'\xe5\xa6\x93\', \'\xe5\xa7\x8a\', \'\xe5\xa6\x92\', \'\xe7\xba\xac\', \'\xe7\x8e\xab\', \'\xe5\x8d\xa6\', \'\xe5\x9d\xb7\', \'\xe5\x9d\xaf\', \'\xe6\x8b\x93\',\n    \'\xe5\x9d\xaa\', \'\xe5\x9d\xa4\', \'\xe6\x8b\x84\', \'\xe6\x8b\xa7\', \'\xe6\x8b\x82\', \'\xe6\x8b\x99\', \'\xe6\x8b\x87\', \'\xe6\x8b\x97\', \'\xe8\x8c\x89\', \'\xe6\x98\x94\', \'\xe8\x8b\x9b\', \'\xe8\x8b\xab\', \'\xe8\x8b\x9f\', \'\xe8\x8b\x9e\', \'\xe8\x8c\x81\', \'\xe8\x8b\x94\', \'\xe6\x9e\x89\', \'\xe6\x9e\xa2\', \'\xe6\x9e\x9a\', \'\xe6\x9e\xab\',\n    \'\xe6\x9d\xad\', \'\xe9\x83\x81\', \'\xe7\x9f\xbe\', \'\xe5\xa5\x88\', \'\xe5\xa5\x84\', \'\xe6\xae\xb4\', \'\xe6\xad\xa7\', \'\xe5\x8d\x93\', \'\xe6\x98\x99\', \'\xe5\x93\x8e\', \'\xe5\x92\x95\', \'\xe5\x91\xb5\', \'\xe5\x92\x99\', \'\xe5\x91\xbb\', \'\xe5\x92\x92\', \'\xe5\x92\x86\', \'\xe5\x92\x96\', \'\xe5\xb8\x95\', \'\xe8\xb4\xa6\', \'\xe8\xb4\xac\',\n    \'\xe8\xb4\xae\', \'\xe6\xb0\x9b\', \'\xe7\xa7\x89\', \'\xe5\xb2\xb3\', \'\xe4\xbe\xa0\', \'\xe4\xbe\xa5\', \'\xe4\xbe\xa3\', \'\xe4\xbe\x88\', \'\xe5\x8d\x91\', \'\xe5\x88\xbd\', \'\xe5\x88\xb9\', \'\xe8\x82\xb4\', \'\xe8\xa7\x85\', \'\xe5\xbf\xbf\', \'\xe7\x93\xae\', \'\xe8\x82\xae\', \'\xe8\x82\xaa\', \'\xe7\x8b\x9e\', \'\xe5\xba\x9e\', \'\xe7\x96\x9f\',\n    \'\xe7\x96\x99\', \'\xe7\x96\x9a\', \'\xe5\x8d\x92\', \'\xe6\xb0\x93\', \'\xe7\x82\xac\', \'\xe6\xb2\xbd\', \'\xe6\xb2\xae\', \'\xe6\xb3\xa3\', \'\xe6\xb3\x9e\', \'\xe6\xb3\x8c\', \'\xe6\xb2\xbc\', \'\xe6\x80\x94\', \'\xe6\x80\xaf\', \'\xe5\xae\xa0\', \'\xe5\xae\x9b\', \'\xe8\xa1\xa9\', \'\xe7\xa5\x88\', \'\xe8\xaf\xa1\', \'\xe5\xb8\x9a\', \'\xe5\xb1\x89\',\n    \'\xe5\xbc\xa7\', \'\xe5\xbc\xa5\', \'\xe9\x99\x8b\', \'\xe9\x99\x8c\', \'\xe5\x87\xbd\', \'\xe5\xa7\x86\', \'\xe8\x99\xb1\', \'\xe5\x8f\x81\', \'\xe7\xbb\x85\', \'\xe9\xa9\xb9\', \'\xe7\xbb\x8a\', \'\xe7\xbb\x8e\', \'\xe5\xa5\x91\', \'\xe8\xb4\xb0\', \'\xe7\x8e\xb7\', \'\xe7\x8e\xb2\', \'\xe7\x8f\x8a\', \'\xe6\x8b\xad\', \'\xe6\x8b\xb7\', \'\xe6\x8b\xb1\',\n    \'\xe6\x8c\x9f\', \'\xe5\x9e\xa2\', \'\xe5\x9e\x9b\', \'\xe6\x8b\xaf\', \'\xe8\x8d\x86\', \'\xe8\x8c\xb8\', \'\xe8\x8c\xac\', \'\xe8\x8d\x9a\', \'\xe8\x8c\xb5\', \'\xe8\x8c\xb4\', \'\xe8\x8d\x9e\', \'\xe8\x8d\xa0\', \'\xe8\x8d\xa4\', \'\xe8\x8d\xa7\', \'\xe8\x8d\x94\', \'\xe6\xa0\x88\', \'\xe6\x9f\x91\', \'\xe6\xa0\x85\', \'\xe6\x9f\xa0\', \'\xe6\x9e\xb7\',\n    \'\xe5\x8b\x83\', \'\xe6\x9f\xac\', \'\xe7\xa0\x82\', \'\xe6\xb3\xb5\', \'\xe7\xa0\x9a\', \'\xe9\xb8\xa5\', \'\xe8\xbd\xb4\', \'\xe9\x9f\xad\', \'\xe8\x99\x90\', \'\xe6\x98\xa7\', \'\xe7\x9b\xb9\', \'\xe5\x92\xa7\', \'\xe6\x98\xb5\', \'\xe6\x98\xad\', \'\xe7\x9b\x85\', \'\xe5\x8b\x8b\', \'\xe5\x93\x86\', \'\xe5\x92\xaa\', \'\xe5\x93\x9f\', \'\xe5\xb9\xbd\',\n    \'\xe9\x92\x99\', \'\xe9\x92\x9d\', \'\xe9\x92\xa0\', \'\xe9\x92\xa6\', \'\xe9\x92\xa7\', \'\xe9\x92\xae\', \'\xe6\xaf\xa1\', \'\xe6\xb0\xa2\', \'\xe7\xa7\x95\', \'\xe4\xbf\x8f\', \'\xe4\xbf\x84\', \'\xe4\xbf\x90\', \'\xe4\xbe\xaf\', \'\xe5\xbe\x8a\', \'\xe8\xa1\x8d\', \'\xe8\x83\x9a\', \'\xe8\x83\xa7\', \'\xe8\x83\x8e\', \'\xe7\x8b\xb0\', \'\xe9\xa5\xb5\',\n    \'\xe5\xb3\xa6\', \'\xe5\xa5\x95\', \'\xe5\x92\xa8\', \'\xe9\xa3\x92\', \'\xe9\x97\xba\', \'\xe9\x97\xbd\', \'\xe7\xb1\xbd\', \'\xe5\xa8\x84\', \'\xe7\x83\x81\', \'\xe7\x82\xab\', \'\xe6\xb4\xbc\', \'\xe6\x9f\x92\', \'\xe6\xb6\x8e\', \'\xe6\xb4\x9b\', \'\xe6\x81\x83\', \'\xe6\x81\x8d\', \'\xe6\x81\xac\', \'\xe6\x81\xa4\', \'\xe5\xae\xa6\', \'\xe8\xaf\xab\',\n    \'\xe8\xaf\xac\', \'\xe7\xa5\xa0\', \'\xe8\xaf\xb2\', \'\xe5\xb1\x8f\', \'\xe5\xb1\x8e\', \'\xe9\x80\x8a\', \'\xe9\x99\xa8\', \'\xe5\xa7\x9a\', \'\xe5\xa8\x9c\', \'\xe8\x9a\xa4\', \'\xe9\xaa\x87\', \'\xe8\x80\x98\', \'\xe8\x80\x99\', \'\xe7\xa7\xa6\', \'\xe5\x8c\xbf\', \'\xe5\x9f\x82\', \'\xe6\x8d\x82\', \'\xe6\x8d\x8d\', \'\xe8\xa2\x81\', \'\xe6\x8d\x8c\',\n    \'\xe6\x8c\xab\', \'\xe6\x8c\x9a\', \'\xe6\x8d\xa3\', \'\xe6\x8d\x85\', \'\xe5\x9f\x83\', \'\xe8\x80\xbf\', \'\xe8\x81\x82\', \'\xe8\x8d\xb8\', \'\xe8\x8e\xbd\', \'\xe8\x8e\xb1\', \'\xe8\x8e\x89\', \'\xe8\x8e\xb9\', \'\xe8\x8e\xba\', \'\xe6\xa2\x86\', \'\xe6\xa0\x96\', \'\xe6\xa1\xa6\', \'\xe6\xa0\x93\', \'\xe6\xa1\x85\', \'\xe6\xa1\xa9\', \'\xe8\xb4\xbe\',\n    \'\xe9\x85\x8c\', \'\xe7\xa0\xb8\', \'\xe7\xa0\xb0\', \'\xe7\xa0\xbe\', \'\xe6\xae\x89\', \'\xe9\x80\x9e\', \'\xe5\x93\xae\', \'\xe5\x94\xa0\', \'\xe5\x93\xba\', \'\xe5\x89\x94\', \'\xe8\x9a\x8c\', \'\xe8\x9a\x9c\', \'\xe7\x95\x94\', \'\xe8\x9a\xa3\', \'\xe8\x9a\xaa\', \'\xe8\x9a\x93\', \'\xe5\x93\xa9\', \'\xe5\x9c\x83\', \'\xe9\xb8\xaf\', \'\xe5\x94\x81\',\n    \'\xe5\x93\xbc\', \'\xe5\x94\x86\', \'\xe5\xb3\xad\', \'\xe5\x94\xa7\', \'\xe5\xb3\xbb\', \'\xe8\xb5\x82\', \'\xe8\xb5\x83\', \'\xe9\x92\xbe\', \'\xe9\x93\x86\', \'\xe6\xb0\xa8\', \'\xe7\xa7\xab\', \'\xe7\xac\x86\', \'\xe4\xbf\xba\', \'\xe8\xb5\x81\', \'\xe5\x80\x94\', \'\xe6\xae\xb7\', \'\xe8\x80\xb8\', \'\xe8\x88\x80\', \'\xe8\xb1\xba\', \'\xe8\xb1\xb9\',\n    \'\xe9\xa2\x81\', \'\xe8\x83\xaf\', \'\xe8\x83\xb0\', \'\xe8\x84\x90\', \'\xe8\x84\x93\', \'\xe9\x80\x9b\', \'\xe5\x8d\xbf\', \'\xe9\xb8\xb5\', \'\xe9\xb8\xb3\', \'\xe9\xa6\x81\', \'\xe5\x87\x8c\', \'\xe5\x87\x84\', \'\xe8\xa1\xb7\', \'\xe9\x83\xad\', \'\xe6\x96\x8b\', \'\xe7\x96\xb9\', \'\xe7\xb4\x8a\', \'\xe7\x93\xb7\', \'\xe7\xbe\x94\', \'\xe7\x83\x99\',\n    \'\xe6\xb5\xa6\', \'\xe6\xb6\xa1\', \'\xe6\xb6\xa3\', \'\xe6\xb6\xa4\', \'\xe6\xb6\xa7\', \'\xe6\xb6\x95\', \'\xe6\xb6\xa9\', \'\xe6\x82\x8d\', \'\xe6\x82\xaf\', \'\xe7\xaa\x8d\', \'\xe8\xaf\xba\', \'\xe8\xaf\xbd\', \'\xe8\xa2\x92\', \'\xe8\xb0\x86\', \'\xe7\xa5\x9f\', \'\xe6\x81\x95\', \'\xe5\xa8\xa9\', \'\xe9\xaa\x8f\', \'\xe7\x90\x90\', \'\xe9\xba\xb8\',\n    \'\xe7\x90\x89\', \'\xe7\x90\x85\', \'\xe6\x8e\xaa\', \'\xe6\x8d\xba\', \'\xe6\x8d\xb6\', \'\xe8\xb5\xa6\', \'\xe5\x9f\xa0\', \'\xe6\x8d\xbb\', \'\xe6\x8e\x90\', \'\xe6\x8e\x82\', \'\xe6\x8e\x96\', \'\xe6\x8e\xb7\', \'\xe6\x8e\xb8\', \'\xe6\x8e\xba\', \'\xe5\x8b\x98\', \'\xe8\x81\x8a\', \'\xe5\xa8\xb6\', \'\xe8\x8f\xb1\', \'\xe8\x8f\xb2\', \'\xe8\x90\x8e\',\n    \'\xe8\x8f\xa9\', \'\xe8\x90\xa4\', \'\xe4\xb9\xbe\', \'\xe8\x90\xa7\', \'\xe8\x90\xa8\', \'\xe8\x8f\x87\', \'\xe5\xbd\xac\', \'\xe6\xa2\x97\', \'\xe6\xa2\xa7\', \'\xe6\xa2\xad\', \'\xe6\x9b\xb9\', \'\xe9\x85\x9d\', \'\xe9\x85\x97\', \'\xe5\x8e\xa2\', \'\xe7\xa1\x85\', \'\xe7\xa1\x95\', \'\xe5\xa5\xa2\', \'\xe7\x9b\x94\', \'\xe5\x8c\xbe\', \'\xe9\xa2\x85\',\n    \'\xe5\xbd\xaa\', \'\xe7\x9c\xb6\', \'\xe6\x99\xa4\', \'\xe6\x9b\xbc\', \'\xe6\x99\xa6\', \'\xe5\x86\x95\', \'\xe5\x95\xa1\', \'\xe7\x95\xa6\', \'\xe8\xb6\xbe\', \'\xe5\x95\x83\', \'\xe8\x9b\x86\', \'\xe8\x9a\xaf\', \'\xe8\x9b\x89\', \'\xe8\x9b\x80\', \'\xe5\x94\xac\', \'\xe5\x94\xbe\', \'\xe5\x95\xa4\', \'\xe5\x95\xa5\', \'\xe5\x95\xb8\', \'\xe5\xb4\x8e\',\n    \'\xe9\x80\xbb\', \'\xe5\xb4\x94\', \'\xe5\xb4\xa9\', \'\xe5\xa9\xb4\', \'\xe8\xb5\x8a\', \'\xe9\x93\x90\', \'\xe9\x93\x9b\', \'\xe9\x93\x9d\', \'\xe9\x93\xa1\', \'\xe9\x93\xa3\', \'\xe9\x93\xad\', \'\xe7\x9f\xab\', \'\xe7\xa7\xb8\', \'\xe7\xa7\xbd\', \'\xe7\xac\x99\', \'\xe7\xac\xa4\', \'\xe5\x81\x8e\', \'\xe5\x82\x80\', \'\xe8\xba\xaf\', \'\xe5\x85\x9c\',\n    \'\xe8\xa1\x85\', \'\xe5\xbe\x98\', \'\xe5\xbe\x99\', \'\xe8\x88\xb6\', \'\xe8\x88\xb7\', \'\xe8\x88\xb5\', \'\xe6\x95\x9b\', \'\xe7\xbf\x8e\', \'\xe8\x84\xaf\', \'\xe9\x80\xb8\', \'\xe5\x87\xb0\', \'\xe7\x8c\x96\', \'\xe7\xa5\xad\', \'\xe7\x83\xb9\', \'\xe5\xba\xb6\', \'\xe5\xba\xb5\', \'\xe7\x97\x8a\', \'\xe9\x98\x8e\', \'\xe9\x98\x90\', \'\xe7\x9c\xb7\',\n    \'\xe7\x84\x8a\', \'\xe7\x84\x95\', \'\xe9\xb8\xbf\', \'\xe6\xb6\xaf\', \'\xe6\xb7\x91\', \'\xe6\xb7\x8c\', \'\xe6\xb7\xae\', \'\xe6\xb7\x86\', \'\xe6\xb8\x8a\', \'\xe6\xb7\xab\', \'\xe6\xb7\xb3\', \'\xe6\xb7\xa4\', \'\xe6\xb7\x80\', \'\xe6\xb6\xae\', \'\xe6\xb6\xb5\', \'\xe6\x83\xa6\', \'\xe6\x82\xb4\', \'\xe6\x83\x8b\', \'\xe5\xaf\x82\', \'\xe7\xaa\x92\',\n    \'\xe8\xb0\x8d\', \'\xe8\xb0\x90\', \'\xe8\xa3\x86\', \'\xe8\xa2\xb1\', \'\xe7\xa5\xb7\', \'\xe8\xb0\x92\', \'\xe8\xb0\x93\', \'\xe8\xb0\x9a\', \'\xe5\xb0\x89\', \'\xe5\xa0\x95\', \'\xe9\x9a\x85\', \'\xe5\xa9\x89\', \'\xe9\xa2\x87\', \'\xe7\xbb\xb0\', \'\xe7\xbb\xb7\', \'\xe7\xbb\xbc\', \'\xe7\xbb\xbd\', \'\xe7\xbc\x80\', \'\xe5\xb7\xa2\', \'\xe7\x90\xb3\',\n    \'\xe7\x90\xa2\', \'\xe7\x90\xbc\', \'\xe6\x8f\x8d\', \'\xe5\xa0\xb0\', \'\xe6\x8f\xa9\', \'\xe6\x8f\xbd\', \'\xe6\x8f\x96\', \'\xe5\xbd\xad\', \'\xe6\x8f\xa3\', \'\xe6\x90\x80\', \'\xe6\x90\x93\', \'\xe5\xa3\xb9\', \'\xe6\x90\x94\', \'\xe8\x91\xab\', \'\xe5\x8b\x9f\', \'\xe8\x92\x8b\', \'\xe8\x92\x82\', \'\xe9\x9f\xa9\', \'\xe6\xa3\xb1\', \'\xe6\xa4\xb0\',\n    \'\xe7\x84\x9a\', \'\xe6\xa4\x8e\', \'\xe6\xa3\xba\', \'\xe6\xa6\x94\', \'\xe6\xa4\xad\', \'\xe7\xb2\x9f\', \'\xe6\xa3\x98\', \'\xe9\x85\xa3\', \'\xe9\x85\xa5\', \'\xe7\xa1\x9d\', \'\xe7\xa1\xab\', \'\xe9\xa2\x8a\', \'\xe9\x9b\xb3\', \'\xe7\xbf\x98\', \'\xe5\x87\xbf\', \'\xe6\xa3\xa0\', \'\xe6\x99\xb0\', \'\xe9\xbc\x8e\', \'\xe5\x96\xb3\', \'\xe9\x81\x8f\',\n    \'\xe6\x99\xbe\', \'\xe7\x95\xb4\', \'\xe8\xb7\x8b\', \'\xe8\xb7\x9b\', \'\xe8\x9b\x94\', \'\xe8\x9c\x92\', \'\xe8\x9b\xa4\', \'\xe9\xb9\x83\', \'\xe5\x96\xbb\', \'\xe5\x95\xbc\', \'\xe5\x96\xa7\', \'\xe5\xb5\x8c\', \'\xe8\xb5\x8b\', \'\xe8\xb5\x8e\', \'\xe8\xb5\x90\', \'\xe9\x94\x89\', \'\xe9\x94\x8c\', \'\xe7\x94\xa5\', \'\xe6\x8e\xb0\', \'\xe6\xb0\xae\',\n    \'\xe6\xb0\xaf\', \'\xe9\xbb\x8d\', \'\xe7\xad\x8f\', \'\xe7\x89\x8d\', \'\xe7\xb2\xa4\', \'\xe9\x80\xbe\', \'\xe8\x85\x8c\', \'\xe8\x85\x8b\', \'\xe8\x85\x95\', \'\xe7\x8c\xa9\', \'\xe7\x8c\xac\', \'\xe6\x83\xab\', \'\xe6\x95\xa6\', \'\xe7\x97\x98\', \'\xe7\x97\xa2\', \'\xe7\x97\xaa\', \'\xe7\xab\xa3\', \'\xe7\xbf\x94\', \'\xe5\xa5\xa0\', \'\xe9\x81\x82\',\n    \'\xe7\x84\x99\', \'\xe6\xbb\x9e\', \'\xe6\xb9\x98\', \'\xe6\xb8\xa4\', \'\xe6\xb8\xba\', \'\xe6\xba\x83\', \'\xe6\xba\x85\', \'\xe6\xb9\x83\', \'\xe6\x84\x95\', \'\xe6\x83\xb6\', \'\xe5\xaf\x93\', \'\xe7\xaa\x96\', \'\xe7\xaa\x98\', \'\xe9\x9b\x87\', \'\xe8\xb0\xa4\', \'\xe7\x8a\x80\', \'\xe9\x9a\x98\', \'\xe5\xaa\x92\', \'\xe5\xaa\x9a\', \'\xe5\xa9\xbf\',\n    \'\xe7\xbc\x85\', \'\xe7\xbc\x86\', \'\xe7\xbc\x94\', \'\xe7\xbc\x95\', \'\xe9\xaa\x9a\', \'\xe7\x91\x9f\', \'\xe9\xb9\x89\', \'\xe7\x91\xb0\', \'\xe6\x90\xaa\', \'\xe8\x81\x98\', \'\xe6\x96\x9f\', \'\xe9\x9d\xb4\', \'\xe9\x9d\xb6\', \'\xe8\x93\x96\', \'\xe8\x92\xbf\', \'\xe8\x92\xb2\', \'\xe8\x93\x89\', \'\xe6\xa5\x94\', \'\xe6\xa4\xbf\', \'\xe6\xa5\xb7\',\n    \'\xe6\xa6\x84\', \'\xe6\xa5\x9e\', \'\xe6\xa5\xa3\', \'\xe9\x85\xaa\', \'\xe7\xa2\x98\', \'\xe7\xa1\xbc\', \'\xe7\xa2\x89\', \'\xe8\xbe\x90\', \'\xe8\xbe\x91\', \'\xe9\xa2\x91\', \'\xe7\x9d\xb9\', \'\xe7\x9d\xa6\', \'\xe7\x9e\x84\', \'\xe5\x97\x9c\', \'\xe5\x97\xa6\', \'\xe6\x9a\x87\', \'\xe7\x95\xb8\', \'\xe8\xb7\xb7\', \'\xe8\xb7\xba\', \'\xe8\x9c\x88\',\n    \'\xe8\x9c\x97\', \'\xe8\x9c\x95\', \'\xe8\x9b\xb9\', \'\xe5\x97\x85\', \'\xe5\x97\xa1\', \'\xe5\x97\xa4\', \'\xe7\xbd\xb2\', \'\xe8\x9c\x80\', \'\xe5\xb9\x8c\', \'\xe9\x94\x9a\', \'\xe9\x94\xa5\', \'\xe9\x94\xa8\', \'\xe9\x94\xad\', \'\xe9\x94\xb0\', \'\xe7\xa8\x9a\', \'\xe9\xa2\x93\', \'\xe7\xad\xb7\', \'\xe9\xad\x81\', \'\xe8\xa1\x99\', \'\xe8\x85\xbb\',\n    \'\xe8\x85\xae\', \'\xe8\x85\xba\', \'\xe9\xb9\x8f\', \'\xe8\x82\x84\', \'\xe7\x8c\xbf\', \'\xe9\xa2\x96\', \'\xe7\x85\x9e\', \'\xe9\x9b\x8f\', \'\xe9\xa6\x8d\', \'\xe9\xa6\x8f\', \'\xe7\xa6\x80\', \'\xe7\x97\xb9\', \'\xe5\xbb\x93\', \'\xe7\x97\xb4\', \'\xe9\x9d\x96\', \'\xe8\xaa\x8a\', \'\xe6\xbc\x93\', \'\xe6\xba\xa2\', \'\xe6\xba\xaf\', \'\xe6\xba\xb6\',\n    \'\xe6\xbb\x93\', \'\xe6\xba\xba\', \'\xe5\xaf\x9e\', \'\xe7\xaa\xa5\', \'\xe7\xaa\x9f\', \'\xe5\xaf\x9d\', \'\xe8\xa4\x82\', \'\xe8\xa3\xb8\', \'\xe8\xb0\xac\', \'\xe5\xaa\xb3\', \'\xe5\xab\x89\', \'\xe7\xbc\x9a\', \'\xe7\xbc\xa4\', \'\xe5\x89\xbf\', \'\xe8\xb5\x98\', \'\xe7\x86\xac\', \'\xe8\xb5\xab\', \'\xe8\x94\xab\', \'\xe6\x91\xb9\', \'\xe8\x94\x93\',\n    \'\xe8\x94\x97\', \'\xe8\x94\xbc\', \'\xe7\x86\x99\', \'\xe8\x94\x9a\', \'\xe5\x85\xa2\', \'\xe6\xa6\x9b\', \'\xe6\xa6\x95\', \'\xe9\x85\xb5\', \'\xe7\xa2\x9f\', \'\xe7\xa2\xb4\', \'\xe7\xa2\xb1\', \'\xe7\xa2\xb3\', \'\xe8\xbe\x95\', \'\xe8\xbe\x96\', \'\xe9\x9b\x8c\', \'\xe5\xa2\x85\', \'\xe5\x98\x81\', \'\xe8\xb8\x8a\', \'\xe8\x9d\x89\', \'\xe5\x98\x80\',\n    \'\xe5\xb9\x94\', \'\xe9\x95\x80\', \'\xe8\x88\x94\', \'\xe7\x86\x8f\', \'\xe7\xae\x8d\', \'\xe7\xae\x95\', \'\xe7\xae\xab\', \'\xe8\x88\x86\', \'\xe5\x83\xa7\', \'\xe5\xad\xb5\', \'\xe7\x98\xa9\', \'\xe7\x98\x9f\', \'\xe5\xbd\xb0\', \'\xe7\xb2\xb9\', \'\xe6\xbc\xb1\', \'\xe6\xbc\xa9\', \'\xe6\xbc\xbe\', \'\xe6\x85\xb7\', \'\xe5\xaf\xa1\', \'\xe5\xaf\xa5\',\n    \'\xe8\xb0\xad\', \'\xe8\xa4\x90\', \'\xe8\xa4\xaa\', \'\xe9\x9a\xa7\', \'\xe5\xab\xa1\', \'\xe7\xbc\xa8\', \'\xe6\x92\xb5\', \'\xe6\x92\xa9\', \'\xe6\x92\xae\', \'\xe6\x92\xac\', \'\xe6\x93\x92\', \'\xe5\xa2\xa9\', \'\xe6\x92\xb0\', \'\xe9\x9e\x8d\', \'\xe8\x95\x8a\', \'\xe8\x95\xb4\', \'\xe6\xa8\x8a\', \'\xe6\xa8\x9f\', \'\xe6\xa9\x84\', \'\xe6\x95\xb7\',\n    \'\xe8\xb1\x8c\', \'\xe9\x86\x87\', \'\xe7\xa3\x95\', \'\xe7\xa3\x85\', \'\xe7\xa2\xbe\', \'\xe6\x86\x8b\', \'\xe5\x98\xb6\', \'\xe5\x98\xb2\', \'\xe5\x98\xb9\', \'\xe8\x9d\xa0\', \'\xe8\x9d\x8e\', \'\xe8\x9d\x8c\', \'\xe8\x9d\x97\', \'\xe8\x9d\x99\', \'\xe5\x98\xbf\', \'\xe5\xb9\xa2\', \'\xe9\x95\x8a\', \'\xe9\x95\x90\', \'\xe7\xa8\xbd\', \'\xe7\xaf\x93\',\n    \'\xe8\x86\x98\', \'\xe9\xb2\xa4\', \'\xe9\xb2\xab\', \'\xe8\xa4\x92\', \'\xe7\x98\xaa\', \'\xe7\x98\xa4\', \'\xe7\x98\xab\', \'\xe5\x87\x9b\', \'\xe6\xbe\x8e\', \'\xe6\xbd\xad\', \'\xe6\xbd\xa6\', \'\xe6\xbe\xb3\', \'\xe6\xbd\x98\', \'\xe6\xbe\x88\', \'\xe6\xbe\x9c\', \'\xe6\xbe\x84\', \'\xe6\x86\x94\', \'\xe6\x87\x8a\', \'\xe6\x86\x8e\', \'\xe7\xbf\xa9\',\n    \'\xe8\xa4\xa5\', \'\xe8\xb0\xb4\', \'\xe9\xb9\xa4\', \'\xe6\x86\xa8\', \'\xe5\xb1\xa5\', \'\xe5\xac\x89\', \'\xe8\xb1\xab\', \'\xe7\xbc\xad\', \'\xe6\x92\xbc\', \'\xe6\x93\x82\', \'\xe6\x93\x85\', \'\xe8\x95\xbe\', \'\xe8\x96\x9b\', \'\xe8\x96\x87\', \'\xe6\x93\x8e\', \'\xe7\xbf\xb0\', \'\xe5\x99\xa9\', \'\xe6\xa9\xb1\', \'\xe6\xa9\x99\', \'\xe7\x93\xa2\',\n    \'\xe8\x9f\xa5\', \'\xe9\x9c\x8d\', \'\xe9\x9c\x8e\', \'\xe8\xbe\x99\', \'\xe5\x86\x80\', \'\xe8\xb8\xb1\', \'\xe8\xb9\x82\', \'\xe8\x9f\x86\', \'\xe8\x9e\x83\', \'\xe8\x9e\x9f\', \'\xe5\x99\xaa\', \'\xe9\xb9\xa6\', \'\xe9\xbb\x94\', \'\xe7\xa9\x86\', \'\xe7\xaf\xa1\', \'\xe7\xaf\xb7\', \'\xe7\xaf\x99\', \'\xe7\xaf\xb1\', \'\xe5\x84\x92\', \'\xe8\x86\xb3\',\n    \'\xe9\xb2\xb8\', \'\xe7\x98\xbe\', \'\xe7\x98\xb8\', \'\xe7\xb3\x99\', \'\xe7\x87\x8e\', \'\xe6\xbf\x92\', \'\xe6\x86\xbe\', \'\xe6\x87\x88\', \'\xe7\xaa\xbf\', \'\xe7\xbc\xb0\', \'\xe5\xa3\x95\', \'\xe8\x97\x90\', \'\xe6\xaa\xac\', \'\xe6\xaa\x90\', \'\xe6\xaa\xa9\', \'\xe6\xaa\x80\', \'\xe7\xa4\x81\', \'\xe7\xa3\xb7\', \'\xe4\xba\x86\', \'\xe7\x9e\xac\',\n    \'\xe7\x9e\xb3\', \'\xe7\x9e\xaa\', \'\xe6\x9b\x99\', \'\xe8\xb9\x8b\', \'\xe8\x9f\x8b\', \'\xe8\x9f\x80\', \'\xe5\x9a\x8e\', \'\xe8\xb5\xa1\', \'\xe9\x95\xa3\', \'\xe9\xad\x8f\', \'\xe7\xb0\x87\', \'\xe5\x84\xa1\', \'\xe5\xbe\xbd\', \'\xe7\x88\xb5\', \'\xe6\x9c\xa6\', \'\xe8\x87\x8a\', \'\xe9\xb3\x84\', \'\xe7\xb3\x9c\', \'\xe7\x99\x8c\', \'\xe6\x87\xa6\',\n    \'\xe8\xb1\x81\', \'\xe8\x87\x80\', \'\xe8\x97\x95\', \'\xe8\x97\xa4\', \'\xe7\x9e\xbb\', \'\xe5\x9a\xa3\', \'\xe9\xb3\x8d\', \'\xe7\x99\x9e\', \'\xe7\x80\x91\', \'\xe8\xa5\x9f\', \'\xe7\x92\xa7\', \'\xe6\x88\xb3\', \'\xe6\x94\x92\', \'\xe5\xad\xbd\', \'\xe8\x98\x91\', \'\xe8\x97\xbb\', \'\xe9\xb3\x96\', \'\xe8\xb9\xad\', \'\xe8\xb9\xac\', \'\xe7\xb0\xb8\',\n    \'\xe7\xb0\xbf\', \'\xe8\x9f\xb9\', \'\xe9\x9d\xa1\', \'\xe7\x99\xa3\', \'\xe7\xbe\xb9\', \'\xe9\xac\x93\', \'\xe6\x94\x98\', \'\xe8\xa0\x95\', \'\xe5\xb7\x8d\', \'\xe9\xb3\x9e\', \'\xe7\xb3\xaf\', \'\xe8\xad\xac\', \'\xe9\x9c\xb9\', \'\xe8\xba\x8f\', \'\xe9\xab\x93\', \'\xe8\x98\xb8\', \'\xe9\x95\xb6\', \'\xe7\x93\xa4\', \'\xe7\x9f\x97\', \'\xe5\x9c\xb3\',\n    \'\xe7\x8f\x8f\', \'\xe8\x95\x99\', \'\xe6\x97\xbb\', \'\xe6\xb6\x85\', \'\xe6\x94\xb8\', \'\xe5\x98\x9b\', \'\xe9\x86\xaa\', \'\xe7\xbc\xaa\', \'\xe5\x99\x97\', \'\xe7\x9e\xa8\', \'\xe9\x9d\xb3\', \'\xe5\xb8\xb7\', \'\xe5\xbe\xa8\',\n]\n\nDOCUMENT_SYMBOLS = [\n    \'!\', \'""\', \'#\', \'$\', \'%\', \'&\', ""\'"", \'(\', \')\', \'*\', \'+\', \',\', \'-\', \'.\', \'/\', \':\', \';\', \'<\', \'=\', \'>\', \'?\', \'@\', \'[\',\n    \']\', \'^\', \'_\', \'`\', \'{\', \'|\', \'}\', \'~\', \'\xc2\xb0\', \'\xc2\xb1\', \'\xc2\xb7\', \'\xc3\x97\', \'\xc3\xa0\', \'\xc3\xa9\', \'\xc3\xb7\', \'\xc3\xbc\', \'\xce\xb1\', \'\xce\xb2\', \'\xd0\x9e\', \'\xd0\x9f\', \'\xd0\xa0\', \'\xe2\x80\x93\', \'\xe2\x80\x94\',\n    \'\xe2\x80\x95\', \'\xe2\x80\x98\', \'\xe2\x80\x99\', \'\xe2\x80\x9c\', \'\xe2\x80\x9d\', \'\xe2\x80\xa6\', \'\xe2\x80\xb0\', \'\xe2\x80\xb2\', \'\xe2\x80\xbb\', \'\xe2\x84\x83\', \'\xe2\x85\xa0\', \'\xe2\x85\xa1\', \'\xe2\x85\xa2\', \'\xe2\x85\xa3\', \'\xe2\x86\x92\', \'\xe2\x86\x93\', \'\xe2\x88\x88\', \'\xe2\x88\x9a\', \'\xe2\x88\xa9\', \'\xe2\x88\xb5\', \'\xe2\x88\xb6\', \'\xe2\x89\xa0\',\n    \'\xe2\x89\xa4\', \'\xe2\x89\xa5\', \'\xe2\x91\xa0\', \'\xe2\x91\xa1\', \'\xe2\x91\xa2\', \'\xe2\x91\xa3\', \'\xe2\x91\xa4\', \'\xe2\x91\xa5\', \'\xe2\x91\xa6\', \'\xe2\x91\xa7\', \'\xe2\x91\xa8\', \'\xe2\x91\xa9\', \'\xe2\x91\xb4\', \'\xe2\x91\xb5\', \'\xe2\x91\xb6\', \'\xe2\x91\xbe\', \'\xe2\x91\xbf\', \'\xe2\x92\x80\', \'\xe2\x92\x81\', \'\xe2\x92\x82\', \'\xe2\x92\x83\',\n    \'\xe2\x92\x84\', \'\xe2\x92\x85\', \'\xe2\x92\x86\', \'\xe2\x92\x88\', \'\xe2\x92\x89\', \'\xe2\x92\x8a\', \'\xe2\x94\x80\', \'\xe2\x94\x81\', \'\xe2\x94\x82\', \'\xe2\x94\x8c\', \'\xe2\x94\x90\', \'\xe2\x95\xb1\', \'\xe2\x96\xa0\', \'\xe2\x96\xa1\', \'\xe2\x96\xb2\', \'\xe2\x96\xb3\', \'\xe2\x97\x86\', \'\xe2\x97\x87\', \'\xe2\x97\x8b\', \'\xe2\x97\x8e\', \'\xe2\x97\x8f\',\n    \'\xe2\x98\x85\', \'\xe2\x98\x86\', \'\xe3\x80\x81\', \'\xe3\x80\x82\', \'\xe3\x80\x87\', \'\xe3\x80\x88\', \'\xe3\x80\x89\', \'\xe3\x80\x8a\', \'\xe3\x80\x8b\', \'\xe3\x80\x8c\', \'\xe3\x80\x8d\', \'\xe3\x80\x8e\', \'\xe3\x80\x8f\', \'\xe3\x80\x90\', \'\xe3\x80\x91\', \'\xe3\x80\x94\', \'\xe3\x80\x95\',  \'\xef\xb8\xb0\', \'\xef\xb9\x90\',\n    \'\xef\xb9\x91\', \'\xef\xb9\x92\', \'\xef\xb9\x94\', \'\xef\xb9\x96\', \'\xef\xbc\x88\', \'\xef\xbc\x89\', \'\xef\xbc\x8b\', \'\xef\xbc\x8c\', \'\xef\xbc\x8e\', \'\xef\xbd\x9e\', \'\xef\xbf\xa5\'\n]\n\nDOCUMENT_CHS = [\n    \'\xe4\xb8\x80\', \'\xe4\xb8\x81\', \'\xe4\xb8\x83\', \'\xe4\xb8\x87\', \'\xe4\xb8\x88\', \'\xe4\xb8\x89\', \'\xe4\xb8\x8a\', \'\xe4\xb8\x8b\', \'\xe4\xb8\x8d\', \'\xe4\xb8\x8e\', \'\xe4\xb8\x90\', \'\xe4\xb8\x91\', \'\xe4\xb8\x93\', \'\xe4\xb8\x94\', \'\xe4\xb8\x95\', \'\xe4\xb8\x96\', \'\xe4\xb8\x98\', \'\xe4\xb8\x99\', \'\xe4\xb8\x9a\', \'\xe4\xb8\x9b\',\n    \'\xe4\xb8\x9c\', \'\xe4\xb8\x9d\', \'\xe4\xb8\x9e\', \'\xe4\xb8\xa2\', \'\xe4\xb8\xa4\', \'\xe4\xb8\xa5\', \'\xe4\xb8\xa7\', \'\xe4\xb8\xaa\', \'\xe4\xb8\xab\', \'\xe4\xb8\xad\', \'\xe4\xb8\xb0\', \'\xe4\xb8\xb2\', \'\xe4\xb8\xb4\', \'\xe4\xb8\xb8\', \'\xe4\xb8\xb9\', \'\xe4\xb8\xba\', \'\xe4\xb8\xbb\', \'\xe4\xb8\xbd\', \'\xe4\xb8\xbe\', \'\xe4\xb9\x82\',\n    \'\xe4\xb9\x83\', \'\xe4\xb9\x85\', \'\xe4\xb9\x88\', \'\xe4\xb9\x89\', \'\xe4\xb9\x8b\', \'\xe4\xb9\x8c\', \'\xe4\xb9\x8d\', \'\xe4\xb9\x8e\', \'\xe4\xb9\x8f\', \'\xe4\xb9\x90\', \'\xe4\xb9\x92\', \'\xe4\xb9\x93\', \'\xe4\xb9\x94\', \'\xe4\xb9\x96\', \'\xe4\xb9\x98\', \'\xe4\xb9\x99\', \'\xe4\xb9\x9d\', \'\xe4\xb9\x9e\', \'\xe4\xb9\x9f\', \'\xe4\xb9\xa0\',\n    \'\xe4\xb9\xa1\', \'\xe4\xb9\xa6\', \'\xe4\xb9\xa9\', \'\xe4\xb9\xb0\', \'\xe4\xb9\xb1\', \'\xe4\xb9\xb3\', \'\xe4\xb9\xbe\', \'\xe4\xba\x86\', \'\xe4\xba\x88\', \'\xe4\xba\x89\', \'\xe4\xba\x8b\', \'\xe4\xba\x8c\', \'\xe4\xba\x8e\', \'\xe4\xba\x8f\', \'\xe4\xba\x91\', \'\xe4\xba\x92\', \'\xe4\xba\x93\', \'\xe4\xba\x94\', \'\xe4\xba\x95\', \'\xe4\xba\x98\',\n    \'\xe4\xba\x9a\', \'\xe4\xba\x9b\', \'\xe4\xba\x9f\', \'\xe4\xba\xa1\', \'\xe4\xba\xa2\', \'\xe4\xba\xa4\', \'\xe4\xba\xa5\', \'\xe4\xba\xa6\', \'\xe4\xba\xa7\', \'\xe4\xba\xa8\', \'\xe4\xba\xa9\', \'\xe4\xba\xab\', \'\xe4\xba\xac\', \'\xe4\xba\xad\', \'\xe4\xba\xae\', \'\xe4\xba\xb2\', \'\xe4\xba\xb3\', \'\xe4\xba\xb5\', \'\xe4\xba\xb6\', \'\xe4\xba\xb9\',\n    \'\xe4\xba\xba\', \'\xe4\xba\xbf\', \'\xe4\xbb\x80\', \'\xe4\xbb\x81\', \'\xe4\xbb\x83\', \'\xe4\xbb\x84\', \'\xe4\xbb\x85\', \'\xe4\xbb\x86\', \'\xe4\xbb\x87\', \'\xe4\xbb\x8a\', \'\xe4\xbb\x8b\', \'\xe4\xbb\x8d\', \'\xe4\xbb\x8e\', \'\xe4\xbb\x91\', \'\xe4\xbb\x93\', \'\xe4\xbb\x94\', \'\xe4\xbb\x95\', \'\xe4\xbb\x96\', \'\xe4\xbb\x97\', \'\xe4\xbb\x98\',\n    \'\xe4\xbb\x99\', \'\xe4\xbb\x9e\', \'\xe4\xbb\xa1\', \'\xe4\xbb\xa3\', \'\xe4\xbb\xa4\', \'\xe4\xbb\xa5\', \'\xe4\xbb\xaa\', \'\xe4\xbb\xab\', \'\xe4\xbb\xac\', \'\xe4\xbb\xb0\', \'\xe4\xbb\xb2\', \'\xe4\xbb\xb6\', \'\xe4\xbb\xb7\', \'\xe4\xbb\xbb\', \'\xe4\xbb\xbd\', \'\xe4\xbb\xbf\', \'\xe4\xbc\x81\', \'\xe4\xbc\x89\', \'\xe4\xbc\x8a\', \'\xe4\xbc\x8b\',\n    \'\xe4\xbc\x8d\', \'\xe4\xbc\x8e\', \'\xe4\xbc\x8f\', \'\xe4\xbc\x90\', \'\xe4\xbc\x91\', \'\xe4\xbc\x97\', \'\xe4\xbc\x98\', \'\xe4\xbc\x99\', \'\xe4\xbc\x9a\', \'\xe4\xbc\x9b\', \'\xe4\xbc\x9e\', \'\xe4\xbc\x9f\', \'\xe4\xbc\xa0\', \'\xe4\xbc\xa4\', \'\xe4\xbc\xa6\', \'\xe4\xbc\xa7\', \'\xe4\xbc\xaa\', \'\xe4\xbc\xab\', \'\xe4\xbc\xaf\', \'\xe4\xbc\xb0\',\n    \'\xe4\xbc\xb4\', \'\xe4\xbc\xb6\', \'\xe4\xbc\xb8\', \'\xe4\xbc\xba\', \'\xe4\xbc\xbb\', \'\xe4\xbc\xbc\', \'\xe4\xbc\xbd\', \'\xe4\xbd\x83\', \'\xe4\xbd\x86\', \'\xe4\xbd\x88\', \'\xe4\xbd\x8d\', \'\xe4\xbd\x8e\', \'\xe4\xbd\x8f\', \'\xe4\xbd\x90\', \'\xe4\xbd\x91\', \'\xe4\xbd\x93\', \'\xe4\xbd\x95\', \'\xe4\xbd\x97\', \'\xe4\xbd\x98\', \'\xe4\xbd\x99\',\n    \'\xe4\xbd\x9a\', \'\xe4\xbd\x9b\', \'\xe4\xbd\x9c\', \'\xe4\xbd\x9d\', \'\xe4\xbd\x9e\', \'\xe4\xbd\x9f\', \'\xe4\xbd\xa0\', \'\xe4\xbd\xa3\', \'\xe4\xbd\xa4\', \'\xe4\xbd\xa9\', \'\xe4\xbd\xac\', \'\xe4\xbd\xaf\', \'\xe4\xbd\xb0\', \'\xe4\xbd\xb3\', \'\xe4\xbd\xb6\', \'\xe4\xbd\xbb\', \'\xe4\xbd\xbc\', \'\xe4\xbd\xbf\', \'\xe4\xbe\x83\', \'\xe4\xbe\x84\',\n    \'\xe4\xbe\x88\', \'\xe4\xbe\x8b\', \'\xe4\xbe\x8d\', \'\xe4\xbe\x8f\', \'\xe4\xbe\x91\', \'\xe4\xbe\x94\', \'\xe4\xbe\x97\', \'\xe4\xbe\x9b\', \'\xe4\xbe\x9d\', \'\xe4\xbe\xa0\', \'\xe4\xbe\xa3\', \'\xe4\xbe\xa5\', \'\xe4\xbe\xa6\', \'\xe4\xbe\xa7\', \'\xe4\xbe\xa8\', \'\xe4\xbe\xa9\', \'\xe4\xbe\xaa\', \'\xe4\xbe\xac\', \'\xe4\xbe\xae\', \'\xe4\xbe\xaf\',\n    \'\xe4\xbe\xb5\', \'\xe4\xbe\xbf\', \'\xe4\xbf\x83\', \'\xe4\xbf\x84\', \'\xe4\xbf\x85\', \'\xe4\xbf\x8a\', \'\xe4\xbf\x8e\', \'\xe4\xbf\x8f\', \'\xe4\xbf\x90\', \'\xe4\xbf\x91\', \'\xe4\xbf\x97\', \'\xe4\xbf\x98\', \'\xe4\xbf\x9a\', \'\xe4\xbf\x9b\', \'\xe4\xbf\x9d\', \'\xe4\xbf\x9e\', \'\xe4\xbf\x9f\', \'\xe4\xbf\xa1\', \'\xe4\xbf\xa6\', \'\xe4\xbf\xa8\',\n    \'\xe4\xbf\xa9\', \'\xe4\xbf\xaa\', \'\xe4\xbf\xad\', \'\xe4\xbf\xae\', \'\xe4\xbf\xaf\', \'\xe4\xbf\xb1\', \'\xe4\xbf\xb3\', \'\xe4\xbf\xb8\', \'\xe4\xbf\xba\', \'\xe4\xbf\xbe\', \'\xe5\x80\x8d\', \'\xe5\x80\x8f\', \'\xe5\x80\x92\', \'\xe5\x80\x94\', \'\xe5\x80\x98\', \'\xe5\x80\x99\', \'\xe5\x80\x9a\', \'\xe5\x80\x9c\', \'\xe5\x80\x9f\', \'\xe5\x80\xa1\',\n    \'\xe5\x80\xa6\', \'\xe5\x80\xa8\', \'\xe5\x80\xa9\', \'\xe5\x80\xaa\', \'\xe5\x80\xac\', \'\xe5\x80\xad\', \'\xe5\x80\xba\', \'\xe5\x80\xbc\', \'\xe5\x80\xbe\', \'\xe5\x81\x83\', \'\xe5\x81\x87\', \'\xe5\x81\x88\', \'\xe5\x81\x8c\', \'\xe5\x81\x8e\', \'\xe5\x81\x8f\', \'\xe5\x81\x95\', \'\xe5\x81\x9a\', \'\xe5\x81\x9c\', \'\xe5\x81\xa5\', \'\xe5\x81\xb6\',\n    \'\xe5\x82\x95\', \'\xe5\x82\xa3\', \'\xe5\x82\xa5\', \'\xe5\x82\xa8\', \'\xe5\x82\xa9\', \'\xe5\x82\xac\', \'\xe5\x82\xb2\', \'\xe5\x82\xbb\', \'\xe5\x83\x87\', \'\xe5\x83\x8a\', \'\xe5\x83\x8f\', \'\xe5\x83\x96\', \'\xe5\x83\x9a\', \'\xe5\x83\xa6\', \'\xe5\x83\xa7\', \'\xe5\x83\xad\', \'\xe5\x83\xae\', \'\xe5\x83\xb0\', \'\xe5\x83\xb3\', \'\xe5\x83\xb5\',\n    \'\xe5\x83\xb9\', \'\xe5\x83\xbb\', \'\xe5\x84\x86\', \'\xe5\x84\x87\', \'\xe5\x84\x8b\', \'\xe5\x84\x92\', \'\xe5\x84\x99\', \'\xe5\x84\xa1\', \'\xe5\x84\xa3\', \'\xe5\x84\xbf\', \'\xe5\x85\x80\', \'\xe5\x85\x81\', \'\xe5\x85\x83\', \'\xe5\x85\x84\', \'\xe5\x85\x85\', \'\xe5\x85\x86\', \'\xe5\x85\x88\', \'\xe5\x85\x89\', \'\xe5\x85\x8b\', \'\xe5\x85\x8d\',\n    \'\xe5\x85\x91\', \'\xe5\x85\x92\', \'\xe5\x85\x94\', \'\xe5\x85\x95\', \'\xe5\x85\x96\', \'\xe5\x85\x9a\', \'\xe5\x85\x9c\', \'\xe5\x85\xa2\', \'\xe5\x85\xa5\', \'\xe5\x85\xa8\', \'\xe5\x85\xab\', \'\xe5\x85\xac\', \'\xe5\x85\xad\', \'\xe5\x85\xae\', \'\xe5\x85\xb0\', \'\xe5\x85\xb1\', \'\xe5\x85\xb3\', \'\xe5\x85\xb4\', \'\xe5\x85\xb5\', \'\xe5\x85\xb6\',\n    \'\xe5\x85\xb7\', \'\xe5\x85\xb8\', \'\xe5\x85\xb9\', \'\xe5\x85\xbb\', \'\xe5\x85\xbc\', \'\xe5\x85\xbd\', \'\xe5\x86\x80\', \'\xe5\x86\x81\', \'\xe5\x86\x85\', \'\xe5\x86\x88\', \'\xe5\x86\x89\', \'\xe5\x86\x8c\', \'\xe5\x86\x8d\', \'\xe5\x86\x8f\', \'\xe5\x86\x91\', \'\xe5\x86\x92\', \'\xe5\x86\x95\', \'\xe5\x86\x97\', \'\xe5\x86\x99\', \'\xe5\x86\x9b\',\n    \'\xe5\x86\x9c\', \'\xe5\x86\xa0\', \'\xe5\x86\xa2\', \'\xe5\x86\xa4\', \'\xe5\x86\xa5\', \'\xe5\x86\xac\', \'\xe5\x86\xaf\', \'\xe5\x86\xb0\', \'\xe5\x86\xb2\', \'\xe5\x86\xb3\', \'\xe5\x86\xb5\', \'\xe5\x86\xb6\', \'\xe5\x86\xb7\', \'\xe5\x86\xbb\', \'\xe5\x86\xbd\', \'\xe5\x87\x80\', \'\xe5\x87\x84\', \'\xe5\x87\x86\', \'\xe5\x87\x87\', \'\xe5\x87\x89\',\n    \'\xe5\x87\x8b\', \'\xe5\x87\x8c\', \'\xe5\x87\x8f\', \'\xe5\x87\x91\', \'\xe5\x87\x9b\', \'\xe5\x87\x9d\', \'\xe5\x87\xa0\', \'\xe5\x87\xa1\', \'\xe5\x87\xa4\', \'\xe5\x87\xab\', \'\xe5\x87\xad\', \'\xe5\x87\xaf\', \'\xe5\x87\xb0\', \'\xe5\x87\xb3\', \'\xe5\x87\xb6\', \'\xe5\x87\xb8\', \'\xe5\x87\xb9\', \'\xe5\x87\xba\', \'\xe5\x87\xbb\', \'\xe5\x87\xbd\',\n    \'\xe5\x87\xbf\', \'\xe5\x88\x80\', \'\xe5\x88\x81\', \'\xe5\x88\x83\', \'\xe5\x88\x86\', \'\xe5\x88\x87\', \'\xe5\x88\x88\', \'\xe5\x88\x8a\', \'\xe5\x88\x8d\', \'\xe5\x88\x8e\', \'\xe5\x88\x91\', \'\xe5\x88\x92\', \'\xe5\x88\x93\', \'\xe5\x88\x96\', \'\xe5\x88\x97\', \'\xe5\x88\x98\', \'\xe5\x88\x99\', \'\xe5\x88\x9a\', \'\xe5\x88\x9b\', \'\xe5\x88\x9d\',\n    \'\xe5\x88\xa0\', \'\xe5\x88\xa4\', \'\xe5\x88\xa8\', \'\xe5\x88\xa9\', \'\xe5\x88\xab\', \'\xe5\x88\xad\', \'\xe5\x88\xae\', \'\xe5\x88\xb0\', \'\xe5\x88\xb3\', \'\xe5\x88\xb6\', \'\xe5\x88\xb7\', \'\xe5\x88\xb8\', \'\xe5\x88\xb9\', \'\xe5\x88\xba\', \'\xe5\x88\xbb\', \'\xe5\x88\xbd\', \'\xe5\x89\x81\', \'\xe5\x89\x82\', \'\xe5\x89\x83\', \'\xe5\x89\x8a\',\n    \'\xe5\x89\x8c\', \'\xe5\x89\x8d\', \'\xe5\x89\x8e\', \'\xe5\x89\x90\', \'\xe5\x89\x91\', \'\xe5\x89\x94\', \'\xe5\x89\x96\', \'\xe5\x89\x9c\', \'\xe5\x89\xa5\', \'\xe5\x89\xa7\', \'\xe5\x89\xa9\', \'\xe5\x89\xaa\', \'\xe5\x89\xaf\', \'\xe5\x89\xb2\', \'\xe5\x89\xbd\', \'\xe5\x89\xbf\', \'\xe5\x8a\x88\', \'\xe5\x8a\x93\', \'\xe5\x8a\x99\', \'\xe5\x8a\x9b\',\n    \'\xe5\x8a\x9d\', \'\xe5\x8a\x9e\', \'\xe5\x8a\x9f\', \'\xe5\x8a\xa0\', \'\xe5\x8a\xa1\', \'\xe5\x8a\xa3\', \'\xe5\x8a\xa8\', \'\xe5\x8a\xa9\', \'\xe5\x8a\xaa\', \'\xe5\x8a\xab\', \'\xe5\x8a\xac\', \'\xe5\x8a\xad\', \'\xe5\x8a\xb1\', \'\xe5\x8a\xb2\', \'\xe5\x8a\xb3\', \'\xe5\x8a\xbe\', \'\xe5\x8a\xbf\', \'\xe5\x8b\x83\', \'\xe5\x8b\x87\', \'\xe5\x8b\x89\',\n    \'\xe5\x8b\x8b\', \'\xe5\x8b\x90\', \'\xe5\x8b\x92\', \'\xe5\x8b\x96\', \'\xe5\x8b\x98\', \'\xe5\x8b\x9f\', \'\xe5\x8b\xa4\', \'\xe5\x8b\xba\', \'\xe5\x8b\xbe\', \'\xe5\x8b\xbf\', \'\xe5\x8c\x80\', \'\xe5\x8c\x85\', \'\xe5\x8c\x86\', \'\xe5\x8c\x88\', \'\xe5\x8c\x8d\', \'\xe5\x8c\x8f\', \'\xe5\x8c\x90\', \'\xe5\x8c\x95\', \'\xe5\x8c\x96\', \'\xe5\x8c\x97\',\n    \'\xe5\x8c\x99\', \'\xe5\x8c\x9d\', \'\xe5\x8c\xa0\', \'\xe5\x8c\xa1\', \'\xe5\x8c\xa3\', \'\xe5\x8c\xaa\', \'\xe5\x8c\xae\', \'\xe5\x8c\xb3\', \'\xe5\x8c\xb9\', \'\xe5\x8c\xba\', \'\xe5\x8c\xbb\', \'\xe5\x8c\xbe\', \'\xe5\x8c\xbf\', \'\xe5\x8d\x81\', \'\xe5\x8d\x83\', \'\xe5\x8d\x85\', \'\xe5\x8d\x87\', \'\xe5\x8d\x88\', \'\xe5\x8d\x89\', \'\xe5\x8d\x8a\',\n    \'\xe5\x8d\x8e\', \'\xe5\x8d\x8f\', \'\xe5\x8d\x91\', \'\xe5\x8d\x92\', \'\xe5\x8d\x93\', \'\xe5\x8d\x95\', \'\xe5\x8d\x96\', \'\xe5\x8d\x97\', \'\xe5\x8d\x9a\', \'\xe5\x8d\x9c\', \'\xe5\x8d\x9e\', \'\xe5\x8d\xa0\', \'\xe5\x8d\xa1\', \'\xe5\x8d\xa2\', \'\xe5\x8d\xa4\', \'\xe5\x8d\xa6\', \'\xe5\x8d\xa7\', \'\xe5\x8d\xab\', \'\xe5\x8d\xac\', \'\xe5\x8d\xae\',\n    \'\xe5\x8d\xaf\', \'\xe5\x8d\xb0\', \'\xe5\x8d\xb1\', \'\xe5\x8d\xb3\', \'\xe5\x8d\xb4\', \'\xe5\x8d\xb5\', \'\xe5\x8d\xb7\', \'\xe5\x8d\xb8\', \'\xe5\x8d\xba\', \'\xe5\x8d\xbf\', \'\xe5\x8e\x82\', \'\xe5\x8e\x84\', \'\xe5\x8e\x85\', \'\xe5\x8e\x86\', \'\xe5\x8e\x89\', \'\xe5\x8e\x8b\', \'\xe5\x8e\x8c\', \'\xe5\x8e\x8d\', \'\xe5\x8e\x95\', \'\xe5\x8e\x98\',\n    \'\xe5\x8e\x9a\', \'\xe5\x8e\x9d\', \'\xe5\x8e\x9f\', \'\xe5\x8e\xa2\', \'\xe5\x8e\xa5\', \'\xe5\x8e\xa6\', \'\xe5\x8e\xa8\', \'\xe5\x8e\xa9\', \'\xe5\x8e\xae\', \'\xe5\x8e\xbb\', \'\xe5\x8e\xbf\', \'\xe5\x8f\x82\', \'\xe5\x8f\x88\', \'\xe5\x8f\x89\', \'\xe5\x8f\x8a\', \'\xe5\x8f\x8b\', \'\xe5\x8f\x8c\', \'\xe5\x8f\x8d\', \'\xe5\x8f\x91\', \'\xe5\x8f\x94\',\n    \'\xe5\x8f\x96\', \'\xe5\x8f\x97\', \'\xe5\x8f\x98\', \'\xe5\x8f\x99\', \'\xe5\x8f\x9b\', \'\xe5\x8f\x9f\', \'\xe5\x8f\xa0\', \'\xe5\x8f\xa1\', \'\xe5\x8f\xa3\', \'\xe5\x8f\xa4\', \'\xe5\x8f\xa5\', \'\xe5\x8f\xa6\', \'\xe5\x8f\xa8\', \'\xe5\x8f\xa9\', \'\xe5\x8f\xaa\', \'\xe5\x8f\xab\', \'\xe5\x8f\xac\', \'\xe5\x8f\xad\', \'\xe5\x8f\xae\', \'\xe5\x8f\xaf\',\n    \'\xe5\x8f\xb0\', \'\xe5\x8f\xb1\', \'\xe5\x8f\xb2\', \'\xe5\x8f\xb3\', \'\xe5\x8f\xb5\', \'\xe5\x8f\xb6\', \'\xe5\x8f\xb7\', \'\xe5\x8f\xb8\', \'\xe5\x8f\xb9\', \'\xe5\x8f\xbc\', \'\xe5\x8f\xbd\', \'\xe5\x90\x81\', \'\xe5\x90\x83\', \'\xe5\x90\x84\', \'\xe5\x90\x86\', \'\xe5\x90\x88\', \'\xe5\x90\x89\', \'\xe5\x90\x8a\', \'\xe5\x90\x8c\', \'\xe5\x90\x8d\',\n    \'\xe5\x90\x8e\', \'\xe5\x90\x8f\', \'\xe5\x90\x90\', \'\xe5\x90\x91\', \'\xe5\x90\x92\', \'\xe5\x90\x93\', \'\xe5\x90\x95\', \'\xe5\x90\x97\', \'\xe5\x90\x9b\', \'\xe5\x90\x9d\', \'\xe5\x90\x9e\', \'\xe5\x90\x9f\', \'\xe5\x90\xa0\', \'\xe5\x90\xa6\', \'\xe5\x90\xa7\', \'\xe5\x90\xa8\', \'\xe5\x90\xa9\', \'\xe5\x90\xab\', \'\xe5\x90\xac\', \'\xe5\x90\xad\',\n    \'\xe5\x90\xae\', \'\xe5\x90\xaf\', \'\xe5\x90\xb1\', \'\xe5\x90\xb4\', \'\xe5\x90\xb5\', \'\xe5\x90\xb8\', \'\xe5\x90\xb9\', \'\xe5\x90\xbb\', \'\xe5\x90\xbc\', \'\xe5\x90\xbe\', \'\xe5\x91\x80\', \'\xe5\x91\x83\', \'\xe5\x91\x86\', \'\xe5\x91\x88\', \'\xe5\x91\x8a\', \'\xe5\x91\x90\', \'\xe5\x91\x93\', \'\xe5\x91\x95\', \'\xe5\x91\x97\', \'\xe5\x91\x98\',\n    \'\xe5\x91\x9b\', \'\xe5\x91\x9c\', \'\xe5\x91\xa2\', \'\xe5\x91\xa4\', \'\xe5\x91\xa6\', \'\xe5\x91\xa8\', \'\xe5\x91\xb1\', \'\xe5\x91\xb3\', \'\xe5\x91\xb5\', \'\xe5\x91\xb6\', \'\xe5\x91\xb7\', \'\xe5\x91\xbb\', \'\xe5\x91\xbc\', \'\xe5\x91\xbd\', \'\xe5\x92\x80\', \'\xe5\x92\x82\', \'\xe5\x92\x84\', \'\xe5\x92\x86\', \'\xe5\x92\x8b\', \'\xe5\x92\x8c\',\n    \'\xe5\x92\x8e\', \'\xe5\x92\x8f\', \'\xe5\x92\x90\', \'\xe5\x92\x92\', \'\xe5\x92\x94\', \'\xe5\x92\x95\', \'\xe5\x92\x96\', \'\xe5\x92\x99\', \'\xe5\x92\x9a\', \'\xe5\x92\x9b\', \'\xe5\x92\xa3\', \'\xe5\x92\xa4\', \'\xe5\x92\xa6\', \'\xe5\x92\xa7\', \'\xe5\x92\xa8\', \'\xe5\x92\xaa\', \'\xe5\x92\xab\', \'\xe5\x92\xac\', \'\xe5\x92\xaf\', \'\xe5\x92\xb1\',\n    \'\xe5\x92\xb3\', \'\xe5\x92\xb8\', \'\xe5\x92\xbb\', \'\xe5\x92\xbd\', \'\xe5\x92\xbf\', \'\xe5\x93\x80\', \'\xe5\x93\x81\', \'\xe5\x93\x82\', \'\xe5\x93\x84\', \'\xe5\x93\x86\', \'\xe5\x93\x87\', \'\xe5\x93\x88\', \'\xe5\x93\x89\', \'\xe5\x93\x8c\', \'\xe5\x93\x8d\', \'\xe5\x93\x8e\', \'\xe5\x93\x8f\', \'\xe5\x93\x91\', \'\xe5\x93\x97\', \'\xe5\x93\x99\',\n    \'\xe5\x93\x9d\', \'\xe5\x93\x9f\', \'\xe5\x93\xa5\', \'\xe5\x93\xa6\', \'\xe5\x93\xa7\', \'\xe5\x93\xa8\', \'\xe5\x93\xa9\', \'\xe5\x93\xaa\', \'\xe5\x93\xad\', \'\xe5\x93\xae\', \'\xe5\x93\xb2\', \'\xe5\x93\xba\', \'\xe5\x93\xbc\', \'\xe5\x93\xbd\', \'\xe5\x94\x81\', \'\xe5\x94\x86\', \'\xe5\x94\x87\', \'\xe5\x94\x89\', \'\xe5\x94\x8f\', \'\xe5\x94\x90\',\n    \'\xe5\x94\x91\', \'\xe5\x94\x94\', \'\xe5\x94\xa0\', \'\xe5\x94\xa2\', \'\xe5\x94\xa4\', \'\xe5\x94\xa7\', \'\xe5\x94\xac\', \'\xe5\x94\xae\', \'\xe5\x94\xaf\', \'\xe5\x94\xb1\', \'\xe5\x94\xb3\', \'\xe5\x94\xb6\', \'\xe5\x94\xbe\', \'\xe5\x94\xbf\', \'\xe5\x95\x81\', \'\xe5\x95\x83\', \'\xe5\x95\x84\', \'\xe5\x95\x86\', \'\xe5\x95\x8a\', \'\xe5\x95\x90\',\n    \'\xe5\x95\x95\', \'\xe5\x95\x96\', \'\xe5\x95\x9c\', \'\xe5\x95\xa1\', \'\xe5\x95\xa4\', \'\xe5\x95\xa5\', \'\xe5\x95\xa6\', \'\xe5\x95\xa7\', \'\xe5\x95\xaa\', \'\xe5\x95\xac\', \'\xe5\x95\xae\', \'\xe5\x95\xb0\', \'\xe5\x95\xb1\', \'\xe5\x95\xb6\', \'\xe5\x95\xb8\', \'\xe5\x95\xbb\', \'\xe5\x95\xbc\', \'\xe5\x95\xbe\', \'\xe5\x96\x80\', \'\xe5\x96\x81\',\n    \'\xe5\x96\x82\', \'\xe5\x96\x83\', \'\xe5\x96\x84\', \'\xe5\x96\x86\', \'\xe5\x96\x87\', \'\xe5\x96\x89\', \'\xe5\x96\x8a\', \'\xe5\x96\x8b\', \'\xe5\x96\x8f\', \'\xe5\x96\x91\', \'\xe5\x96\x94\', \'\xe5\x96\x98\', \'\xe5\x96\x99\', \'\xe5\x96\x9c\', \'\xe5\x96\x9d\', \'\xe5\x96\x9f\', \'\xe5\x96\xa7\', \'\xe5\x96\xb1\', \'\xe5\x96\xb3\', \'\xe5\x96\xb7\',\n    \'\xe5\x96\xb9\', \'\xe5\x96\xbb\', \'\xe5\x96\xbd\', \'\xe5\x96\xbe\', \'\xe5\x97\x84\', \'\xe5\x97\x85\', \'\xe5\x97\x8c\', \'\xe5\x97\x91\', \'\xe5\x97\x92\', \'\xe5\x97\x93\', \'\xe5\x97\x94\', \'\xe5\x97\x96\', \'\xe5\x97\x9b\', \'\xe5\x97\x9c\', \'\xe5\x97\x9d\', \'\xe5\x97\x9f\', \'\xe5\x97\xa1\', \'\xe5\x97\xa3\', \'\xe5\x97\xa4\', \'\xe5\x97\xa5\',\n    \'\xe5\x97\xa6\', \'\xe5\x97\xa8\', \'\xe5\x97\xaa\', \'\xe5\x97\xab\', \'\xe5\x97\xaf\', \'\xe5\x97\xb2\', \'\xe5\x97\xb3\', \'\xe5\x97\xb7\', \'\xe5\x97\xbd\', \'\xe5\x97\xbe\', \'\xe5\x98\x80\', \'\xe5\x98\x88\', \'\xe5\x98\x89\', \'\xe5\x98\x8c\', \'\xe5\x98\x8e\', \'\xe5\x98\x98\', \'\xe5\x98\x9b\', \'\xe5\x98\x9f\', \'\xe5\x98\xa4\', \'\xe5\x98\xad\',\n    \'\xe5\x98\xb1\', \'\xe5\x98\xb2\', \'\xe5\x98\xb4\', \'\xe5\x98\xb6\', \'\xe5\x98\xb9\', \'\xe5\x98\xbb\', \'\xe5\x98\xbf\', \'\xe5\x99\x89\', \'\xe5\x99\x8c\', \'\xe5\x99\x8e\', \'\xe5\x99\x94\', \'\xe5\x99\x97\', \'\xe5\x99\x99\', \'\xe5\x99\x9c\', \'\xe5\x99\xa2\', \'\xe5\x99\xa4\', \'\xe5\x99\xa8\', \'\xe5\x99\xa9\', \'\xe5\x99\xaa\', \'\xe5\x99\xab\',\n    \'\xe5\x99\xac\', \'\xe5\x99\xad\', \'\xe5\x99\xb1\', \'\xe5\x99\xb6\', \'\xe5\x99\xbb\', \'\xe5\x9a\x85\', \'\xe5\x9a\x8b\', \'\xe5\x9a\x8e\', \'\xe5\x9a\x8f\', \'\xe5\x9a\x93\', \'\xe5\x9a\xa3\', \'\xe5\x9a\xac\', \'\xe5\x9a\xad\', \'\xe5\x9a\xb0\', \'\xe5\x9a\xb7\', \'\xe5\x9a\xbc\', \'\xe5\x9b\x8a\', \'\xe5\x9b\x90\', \'\xe5\x9b\x94\', \'\xe5\x9b\x9a\',\n    \'\xe5\x9b\x9b\', \'\xe5\x9b\x9e\', \'\xe5\x9b\xa0\', \'\xe5\x9b\xa2\', \'\xe5\x9b\xa4\', \'\xe5\x9b\xab\', \'\xe5\x9b\xad\', \'\xe5\x9b\xb0\', \'\xe5\x9b\xb1\', \'\xe5\x9b\xb4\', \'\xe5\x9b\xb5\', \'\xe5\x9b\xb9\', \'\xe5\x9b\xba\', \'\xe5\x9b\xbd\', \'\xe5\x9b\xbe\', \'\xe5\x9b\xbf\', \'\xe5\x9c\x83\', \'\xe5\x9c\x84\', \'\xe5\x9c\x86\', \'\xe5\x9c\x88\',\n    \'\xe5\x9c\x89\', \'\xe5\x9c\x9c\', \'\xe5\x9c\x9f\', \'\xe5\x9c\xa3\', \'\xe5\x9c\xa8\', \'\xe5\x9c\xa9\', \'\xe5\x9c\xad\', \'\xe5\x9c\xaf\', \'\xe5\x9c\xb0\', \'\xe5\x9c\xb3\', \'\xe5\x9c\xb9\', \'\xe5\x9c\xba\', \'\xe5\x9c\xbe\', \'\xe5\x9d\x80\', \'\xe5\x9d\x82\', \'\xe5\x9d\x87\', \'\xe5\x9d\x8a\', \'\xe5\x9d\x8c\', \'\xe5\x9d\x8d\', \'\xe5\x9d\x8e\',\n    \'\xe5\x9d\x8f\', \'\xe5\x9d\x90\', \'\xe5\x9d\x91\', \'\xe5\x9d\x97\', \'\xe5\x9d\x9a\', \'\xe5\x9d\x9b\', \'\xe5\x9d\x9d\', \'\xe5\x9d\x9e\', \'\xe5\x9d\x9f\', \'\xe5\x9d\xa0\', \'\xe5\x9d\xa1\', \'\xe5\x9d\xa4\', \'\xe5\x9d\xa6\', \'\xe5\x9d\xa8\', \'\xe5\x9d\xaa\', \'\xe5\x9d\xad\', \'\xe5\x9d\xaf\', \'\xe5\x9d\xb3\', \'\xe5\x9d\xb7\', \'\xe5\x9d\xbb\',\n    \'\xe5\x9d\xbc\', \'\xe5\x9e\x82\', \'\xe5\x9e\x83\', \'\xe5\x9e\x84\', \'\xe5\x9e\x8b\', \'\xe5\x9e\x92\', \'\xe5\x9e\x93\', \'\xe5\x9e\x9b\', \'\xe5\x9e\x9d\', \'\xe5\x9e\xa0\', \'\xe5\x9e\xa2\', \'\xe5\x9e\xa3\', \'\xe5\x9e\xa6\', \'\xe5\x9e\xa9\', \'\xe5\x9e\xab\', \'\xe5\x9e\xae\', \'\xe5\x9f\x83\', \'\xe5\x9f\x8b\', \'\xe5\x9f\x8e\', \'\xe5\x9f\x92\',\n    \'\xe5\x9f\x94\', \'\xe5\x9f\x9f\', \'\xe5\x9f\xa0\', \'\xe5\x9f\xa4\', \'\xe5\x9f\xb6\', \'\xe5\x9f\xb9\', \'\xe5\x9f\xba\', \'\xe5\x9f\xbd\', \'\xe5\xa0\x82\', \'\xe5\xa0\x83\', \'\xe5\xa0\x86\', \'\xe5\xa0\x91\', \'\xe5\xa0\x95\', \'\xe5\xa0\x99\', \'\xe5\xa0\xa1\', \'\xe5\xa0\xa4\', \'\xe5\xa0\xaa\', \'\xe5\xa0\xb0\', \'\xe5\xa0\xb5\', \'\xe5\xa1\x8c\',\n    \'\xe5\xa1\x91\', \'\xe5\xa1\x94\', \'\xe5\xa1\x98\', \'\xe5\xa1\x9e\', \'\xe5\xa1\xab\', \'\xe5\xa1\xbe\', \'\xe5\xa2\x80\', \'\xe5\xa2\x83\', \'\xe5\xa2\x85\', \'\xe5\xa2\x89\', \'\xe5\xa2\x93\', \'\xe5\xa2\x99\', \'\xe5\xa2\x9e\', \'\xe5\xa2\x9f\', \'\xe5\xa2\xa0\', \'\xe5\xa2\xa8\', \'\xe5\xa2\xa9\', \'\xe5\xa3\x81\', \'\xe5\xa3\x85\', \'\xe5\xa3\x91\',\n    \'\xe5\xa3\x95\', \'\xe5\xa3\x96\', \'\xe5\xa3\xa4\', \'\xe5\xa3\xab\', \'\xe5\xa3\xac\', \'\xe5\xa3\xae\', \'\xe5\xa3\xb0\', \'\xe5\xa3\xb3\', \'\xe5\xa3\xb6\', \'\xe5\xa3\xb9\', \'\xe5\xa4\x84\', \'\xe5\xa4\x87\', \'\xe5\xa4\x8d\', \'\xe5\xa4\x8f\', \'\xe5\xa4\x94\', \'\xe5\xa4\x95\', \'\xe5\xa4\x96\', \'\xe5\xa4\x99\', \'\xe5\xa4\x9a\', \'\xe5\xa4\x9c\',\n    \'\xe5\xa4\x9f\', \'\xe5\xa4\xa1\', \'\xe5\xa4\xa4\', \'\xe5\xa4\xa5\', \'\xe5\xa4\xa7\', \'\xe5\xa4\xa9\', \'\xe5\xa4\xaa\', \'\xe5\xa4\xab\', \'\xe5\xa4\xad\', \'\xe5\xa4\xae\', \'\xe5\xa4\xaf\', \'\xe5\xa4\xb1\', \'\xe5\xa4\xb4\', \'\xe5\xa4\xb7\', \'\xe5\xa4\xb8\', \'\xe5\xa4\xb9\', \'\xe5\xa4\xba\', \'\xe5\xa5\x81\', \'\xe5\xa5\x82\', \'\xe5\xa5\x84\',\n    \'\xe5\xa5\x87\', \'\xe5\xa5\x88\', \'\xe5\xa5\x89\', \'\xe5\xa5\x8b\', \'\xe5\xa5\x8e\', \'\xe5\xa5\x8f\', \'\xe5\xa5\x91\', \'\xe5\xa5\x94\', \'\xe5\xa5\x95\', \'\xe5\xa5\x96\', \'\xe5\xa5\x97\', \'\xe5\xa5\x98\', \'\xe5\xa5\x9a\', \'\xe5\xa5\xa0\', \'\xe5\xa5\xa1\', \'\xe5\xa5\xa2\', \'\xe5\xa5\xa5\', \'\xe5\xa5\xad\', \'\xe5\xa5\xb3\', \'\xe5\xa5\xb4\',\n    \'\xe5\xa5\xb6\', \'\xe5\xa5\xb8\', \'\xe5\xa5\xb9\', \'\xe5\xa5\xbd\', \'\xe5\xa6\x81\', \'\xe5\xa6\x82\', \'\xe5\xa6\x83\', \'\xe5\xa6\x84\', \'\xe5\xa6\x86\', \'\xe5\xa6\x87\', \'\xe5\xa6\x88\', \'\xe5\xa6\x8a\', \'\xe5\xa6\x8d\', \'\xe5\xa6\x92\', \'\xe5\xa6\x93\', \'\xe5\xa6\x96\', \'\xe5\xa6\x97\', \'\xe5\xa6\x99\', \'\xe5\xa6\x9e\', \'\xe5\xa6\xa4\',\n    \'\xe5\xa6\xa5\', \'\xe5\xa6\xa8\', \'\xe5\xa6\xa9\', \'\xe5\xa6\xaa\', \'\xe5\xa6\xab\', \'\xe5\xa6\xae\', \'\xe5\xa6\xb2\', \'\xe5\xa6\xb3\', \'\xe5\xa6\xb9\', \'\xe5\xa6\xbb\', \'\xe5\xa6\xbe\', \'\xe5\xa7\x81\', \'\xe5\xa7\x86\', \'\xe5\xa7\x8a\', \'\xe5\xa7\x8b\', \'\xe5\xa7\x90\', \'\xe5\xa7\x91\', \'\xe5\xa7\x92\', \'\xe5\xa7\x93\', \'\xe5\xa7\x94\',\n    \'\xe5\xa7\x97\', \'\xe5\xa7\x9a\', \'\xe5\xa7\x9c\', \'\xe5\xa7\x9d\', \'\xe5\xa7\x9e\', \'\xe5\xa7\xa3\', \'\xe5\xa7\xa5\', \'\xe5\xa7\xa8\', \'\xe5\xa7\xac\', \'\xe5\xa7\xae\', \'\xe5\xa7\xb9\', \'\xe5\xa7\xbb\', \'\xe5\xa7\xbf\', \'\xe5\xa8\x80\', \'\xe5\xa8\x81\', \'\xe5\xa8\x83\', \'\xe5\xa8\x84\', \'\xe5\xa8\x85\', \'\xe5\xa8\x86\', \'\xe5\xa8\x87\',\n    \'\xe5\xa8\x88\', \'\xe5\xa8\x89\', \'\xe5\xa8\x91\', \'\xe5\xa8\x93\', \'\xe5\xa8\x98\', \'\xe5\xa8\x9c\', \'\xe5\xa8\x9f\', \'\xe5\xa8\xa0\', \'\xe5\xa8\xa1\', \'\xe5\xa8\xa3\', \'\xe5\xa8\xa5\', \'\xe5\xa8\xa9\', \'\xe5\xa8\xb1\', \'\xe5\xa8\xb2\', \'\xe5\xa8\xb4\', \'\xe5\xa8\xb6\', \'\xe5\xa8\xbc\', \'\xe5\xa9\x80\', \'\xe5\xa9\x86\', \'\xe5\xa9\x89\',\n    \'\xe5\xa9\x8a\', \'\xe5\xa9\x95\', \'\xe5\xa9\x9a\', \'\xe5\xa9\xa2\', \'\xe5\xa9\xa7\', \'\xe5\xa9\xaa\', \'\xe5\xa9\xb4\', \'\xe5\xa9\xb6\', \'\xe5\xa9\xb7\', \'\xe5\xa9\xba\', \'\xe5\xa9\xbf\', \'\xe5\xaa\x92\', \'\xe5\xaa\x9a\', \'\xe5\xaa\x9b\', \'\xe5\xaa\xa2\', \'\xe5\xaa\xaa\', \'\xe5\xaa\xb2\', \'\xe5\xaa\xb3\', \'\xe5\xaa\xb5\', \'\xe5\xaa\xb8\',\n    \'\xe5\xaa\xbe\', \'\xe5\xab\x81\', \'\xe5\xab\x82\', \'\xe5\xab\x84\', \'\xe5\xab\x89\', \'\xe5\xab\x8c\', \'\xe5\xab\x94\', \'\xe5\xab\x96\', \'\xe5\xab\x9a\', \'\xe5\xab\x9c\', \'\xe5\xab\xa1\', \'\xe5\xab\xa3\', \'\xe5\xab\xa6\', \'\xe5\xab\xa9\', \'\xe5\xab\xaa\', \'\xe5\xab\xb1\', \'\xe5\xac\x83\', \'\xe5\xac\x89\', \'\xe5\xac\x96\', \'\xe5\xac\x97\',\n    \'\xe5\xac\x9b\', \'\xe5\xac\xb4\', \'\xe5\xad\x80\', \'\xe5\xad\x90\', \'\xe5\xad\x91\', \'\xe5\xad\x94\', \'\xe5\xad\x95\', \'\xe5\xad\x97\', \'\xe5\xad\x98\', \'\xe5\xad\x99\', \'\xe5\xad\x9a\', \'\xe5\xad\x9b\', \'\xe5\xad\x9c\', \'\xe5\xad\x9d\', \'\xe5\xad\x9f\', \'\xe5\xad\xa2\', \'\xe5\xad\xa3\', \'\xe5\xad\xa4\', \'\xe5\xad\xa5\', \'\xe5\xad\xa6\',\n    \'\xe5\xad\xa9\', \'\xe5\xad\xaa\', \'\xe5\xad\xb0\', \'\xe5\xad\xb1\', \'\xe5\xad\xb3\', \'\xe5\xad\xb5\', \'\xe5\xad\xba\', \'\xe5\xad\xbd\', \'\xe5\xae\x81\', \'\xe5\xae\x83\', \'\xe5\xae\x85\', \'\xe5\xae\x87\', \'\xe5\xae\x88\', \'\xe5\xae\x89\', \'\xe5\xae\x8b\', \'\xe5\xae\x8c\', \'\xe5\xae\x8f\', \'\xe5\xae\x93\', \'\xe5\xae\x95\', \'\xe5\xae\x97\',\n    \'\xe5\xae\x98\', \'\xe5\xae\x99\', \'\xe5\xae\x9a\', \'\xe5\xae\x9b\', \'\xe5\xae\x9c\', \'\xe5\xae\x9d\', \'\xe5\xae\x9e\', \'\xe5\xae\xa0\', \'\xe5\xae\xa1\', \'\xe5\xae\xa2\', \'\xe5\xae\xa3\', \'\xe5\xae\xa4\', \'\xe5\xae\xa5\', \'\xe5\xae\xa6\', \'\xe5\xae\xaa\', \'\xe5\xae\xab\', \'\xe5\xae\xb0\', \'\xe5\xae\xb3\', \'\xe5\xae\xb4\', \'\xe5\xae\xb5\',\n    \'\xe5\xae\xb6\', \'\xe5\xae\xb8\', \'\xe5\xae\xb9\', \'\xe5\xae\xbd\', \'\xe5\xae\xbe\', \'\xe5\xae\xbf\', \'\xe5\xaf\x82\', \'\xe5\xaf\x84\', \'\xe5\xaf\x85\', \'\xe5\xaf\x86\', \'\xe5\xaf\x87\', \'\xe5\xaf\x8c\', \'\xe5\xaf\x90\', \'\xe5\xaf\x92\', \'\xe5\xaf\x93\', \'\xe5\xaf\x96\', \'\xe5\xaf\x98\', \'\xe5\xaf\x9d\', \'\xe5\xaf\x9e\', \'\xe5\xaf\x9f\',\n    \'\xe5\xaf\xa1\', \'\xe5\xaf\xa4\', \'\xe5\xaf\xa5\', \'\xe5\xaf\xa8\', \'\xe5\xaf\xb0\', \'\xe5\xaf\xb8\', \'\xe5\xaf\xb9\', \'\xe5\xaf\xba\', \'\xe5\xaf\xbb\', \'\xe5\xaf\xbc\', \'\xe5\xaf\xbf\', \'\xe5\xb0\x81\', \'\xe5\xb0\x84\', \'\xe5\xb0\x86\', \'\xe5\xb0\x89\', \'\xe5\xb0\x8a\', \'\xe5\xb0\x8f\', \'\xe5\xb0\x91\', \'\xe5\xb0\x94\', \'\xe5\xb0\x96\',\n    \'\xe5\xb0\x98\', \'\xe5\xb0\x9a\', \'\xe5\xb0\x9d\', \'\xe5\xb0\xa4\', \'\xe5\xb0\xa7\', \'\xe5\xb0\xac\', \'\xe5\xb0\xb1\', \'\xe5\xb0\xb4\', \'\xe5\xb0\xb8\', \'\xe5\xb0\xb9\', \'\xe5\xb0\xba\', \'\xe5\xb0\xbb\', \'\xe5\xb0\xbc\', \'\xe5\xb0\xbd\', \'\xe5\xb0\xbe\', \'\xe5\xb0\xbf\', \'\xe5\xb1\x80\', \'\xe5\xb1\x81\', \'\xe5\xb1\x82\', \'\xe5\xb1\x85\',\n    \'\xe5\xb1\x88\', \'\xe5\xb1\x89\', \'\xe5\xb1\x8a\', \'\xe5\xb1\x8b\', \'\xe5\xb1\x8e\', \'\xe5\xb1\x8f\', \'\xe5\xb1\x91\', \'\xe5\xb1\x95\', \'\xe5\xb1\x9e\', \'\xe5\xb1\xa0\', \'\xe5\xb1\xa1\', \'\xe5\xb1\xa3\', \'\xe5\xb1\xa5\', \'\xe5\xb1\xa6\', \'\xe5\xb1\xaf\', \'\xe5\xb1\xb1\', \'\xe5\xb1\xb9\', \'\xe5\xb1\xbf\', \'\xe5\xb2\x81\', \'\xe5\xb2\x82\',\n    \'\xe5\xb2\x8c\', \'\xe5\xb2\x90\', \'\xe5\xb2\x91\', \'\xe5\xb2\x94\', \'\xe5\xb2\x96\', \'\xe5\xb2\x97\', \'\xe5\xb2\x9a\', \'\xe5\xb2\x9b\', \'\xe5\xb2\xa9\', \'\xe5\xb2\xab\', \'\xe5\xb2\xac\', \'\xe5\xb2\xad\', \'\xe5\xb2\xb1\', \'\xe5\xb2\xb3\', \'\xe5\xb2\xb7\', \'\xe5\xb2\xb8\', \'\xe5\xb2\xbf\', \'\xe5\xb3\x84\', \'\xe5\xb3\x87\', \'\xe5\xb3\x8b\',\n    \'\xe5\xb3\x92\', \'\xe5\xb3\x99\', \'\xe5\xb3\xa1\', \'\xe5\xb3\xa3\', \'\xe5\xb3\xa4\', \'\xe5\xb3\xa5\', \'\xe5\xb3\xa6\', \'\xe5\xb3\xa8\', \'\xe5\xb3\xaa\', \'\xe5\xb3\xad\', \'\xe5\xb3\xb0\', \'\xe5\xb3\xbb\', \'\xe5\xb4\x82\', \'\xe5\xb4\x83\', \'\xe5\xb4\x86\', \'\xe5\xb4\x87\', \'\xe5\xb4\x8e\', \'\xe5\xb4\x94\', \'\xe5\xb4\x96\', \'\xe5\xb4\x9a\',\n    \'\xe5\xb4\x9b\', \'\xe5\xb4\xa7\', \'\xe5\xb4\xa9\', \'\xe5\xb4\xad\', \'\xe5\xb4\xb4\', \'\xe5\xb5\x8b\', \'\xe5\xb5\x8c\', \'\xe5\xb5\x98\', \'\xe5\xb5\xa9\', \'\xe5\xb5\xac\', \'\xe5\xb5\xaf\', \'\xe5\xb6\x82\', \'\xe5\xb6\x93\', \'\xe5\xb6\x99\', \'\xe5\xb6\xb2\', \'\xe5\xb6\xb6\', \'\xe5\xb6\xb7\', \'\xe5\xb7\x82\', \'\xe5\xb7\x85\', \'\xe5\xb7\x89\',\n    \'\xe5\xb7\x8d\', \'\xe5\xb7\x9d\', \'\xe5\xb7\x9e\', \'\xe5\xb7\xa1\', \'\xe5\xb7\xa2\', \'\xe5\xb7\xa5\', \'\xe5\xb7\xa6\', \'\xe5\xb7\xa7\', \'\xe5\xb7\xa8\', \'\xe5\xb7\xa9\', \'\xe5\xb7\xab\', \'\xe5\xb7\xae\', \'\xe5\xb7\xb1\', \'\xe5\xb7\xb2\', \'\xe5\xb7\xb3\', \'\xe5\xb7\xb4\', \'\xe5\xb7\xb7\', \'\xe5\xb7\xbd\', \'\xe5\xb7\xbe\', \'\xe5\xb7\xbf\',\n    \'\xe5\xb8\x81\', \'\xe5\xb8\x82\', \'\xe5\xb8\x83\', \'\xe5\xb8\x85\', \'\xe5\xb8\x86\', \'\xe5\xb8\x87\', \'\xe5\xb8\x88\', \'\xe5\xb8\x8c\', \'\xe5\xb8\x8f\', \'\xe5\xb8\x90\', \'\xe5\xb8\x91\', \'\xe5\xb8\x94\', \'\xe5\xb8\x95\', \'\xe5\xb8\x96\', \'\xe5\xb8\x98\', \'\xe5\xb8\x9a\', \'\xe5\xb8\x9b\', \'\xe5\xb8\x9c\', \'\xe5\xb8\x9d\', \'\xe5\xb8\xa6\',\n    \'\xe5\xb8\xa7\', \'\xe5\xb8\xa8\', \'\xe5\xb8\xad\', \'\xe5\xb8\xae\', \'\xe5\xb8\xb7\', \'\xe5\xb8\xb8\', \'\xe5\xb8\xbc\', \'\xe5\xb8\xbd\', \'\xe5\xb9\x82\', \'\xe5\xb9\x84\', \'\xe5\xb9\x85\', \'\xe5\xb9\x8c\', \'\xe5\xb9\x94\', \'\xe5\xb9\x95\', \'\xe5\xb9\x9b\', \'\xe5\xb9\x9e\', \'\xe5\xb9\xa1\', \'\xe5\xb9\xa2\', \'\xe5\xb9\xa4\', \'\xe5\xb9\xb2\',\n    \'\xe5\xb9\xb3\', \'\xe5\xb9\xb4\', \'\xe5\xb9\xb6\', \'\xe5\xb9\xb8\', \'\xe5\xb9\xba\', \'\xe5\xb9\xbb\', \'\xe5\xb9\xbc\', \'\xe5\xb9\xbd\', \'\xe5\xb9\xbf\', \'\xe5\xba\x84\', \'\xe5\xba\x86\', \'\xe5\xba\x87\', \'\xe5\xba\x8a\', \'\xe5\xba\x8f\', \'\xe5\xba\x90\', \'\xe5\xba\x91\', \'\xe5\xba\x93\', \'\xe5\xba\x94\', \'\xe5\xba\x95\', \'\xe5\xba\x96\',\n    \'\xe5\xba\x97\', \'\xe5\xba\x99\', \'\xe5\xba\x9a\', \'\xe5\xba\x9c\', \'\xe5\xba\x9e\', \'\xe5\xba\x9f\', \'\xe5\xba\xa0\', \'\xe5\xba\xa5\', \'\xe5\xba\xa6\', \'\xe5\xba\xa7\', \'\xe5\xba\xad\', \'\xe5\xba\xb3\', \'\xe5\xba\xb5\', \'\xe5\xba\xb6\', \'\xe5\xba\xb7\', \'\xe5\xba\xb8\', \'\xe5\xba\xbe\', \'\xe5\xbb\x89\', \'\xe5\xbb\x8a\', \'\xe5\xbb\x93\',\n    \'\xe5\xbb\x96\', \'\xe5\xbb\x9b\', \'\xe5\xbb\xa8\', \'\xe5\xbb\xaa\', \'\xe5\xbb\xb6\', \'\xe5\xbb\xb7\', \'\xe5\xbb\xba\', \'\xe5\xbb\xbf\', \'\xe5\xbc\x80\', \'\xe5\xbc\x81\', \'\xe5\xbc\x82\', \'\xe5\xbc\x83\', \'\xe5\xbc\x84\', \'\xe5\xbc\x88\', \'\xe5\xbc\x8a\', \'\xe5\xbc\x8b\', \'\xe5\xbc\x8f\', \'\xe5\xbc\x91\', \'\xe5\xbc\x92\', \'\xe5\xbc\x93\',\n    \'\xe5\xbc\x95\', \'\xe5\xbc\x97\', \'\xe5\xbc\x98\', \'\xe5\xbc\x9b\', \'\xe5\xbc\x9f\', \'\xe5\xbc\xa0\', \'\xe5\xbc\xa2\', \'\xe5\xbc\xa5\', \'\xe5\xbc\xa6\', \'\xe5\xbc\xa7\', \'\xe5\xbc\xa9\', \'\xe5\xbc\xad\', \'\xe5\xbc\xaf\', \'\xe5\xbc\xb1\', \'\xe5\xbc\xb5\', \'\xe5\xbc\xb9\', \'\xe5\xbc\xba\', \'\xe5\xbc\xbc\', \'\xe5\xbd\x80\', \'\xe5\xbd\x8a\',\n    \'\xe5\xbd\x92\', \'\xe5\xbd\x93\', \'\xe5\xbd\x95\', \'\xe5\xbd\x97\', \'\xe5\xbd\x98\', \'\xe5\xbd\x9d\', \'\xe5\xbd\xa2\', \'\xe5\xbd\xa4\', \'\xe5\xbd\xa6\', \'\xe5\xbd\xa9\', \'\xe5\xbd\xaa\', \'\xe5\xbd\xac\', \'\xe5\xbd\xad\', \'\xe5\xbd\xb0\', \'\xe5\xbd\xb1\', \'\xe5\xbd\xb7\', \'\xe5\xbd\xb9\', \'\xe5\xbd\xbb\', \'\xe5\xbd\xbc\', \'\xe5\xbe\x80\',\n    \'\xe5\xbe\x81\', \'\xe5\xbe\x82\', \'\xe5\xbe\x84\', \'\xe5\xbe\x85\', \'\xe5\xbe\x87\', \'\xe5\xbe\x88\', \'\xe5\xbe\x89\', \'\xe5\xbe\x8a\', \'\xe5\xbe\x8b\', \'\xe5\xbe\x8c\', \'\xe5\xbe\x90\', \'\xe5\xbe\x92\', \'\xe5\xbe\x95\', \'\xe5\xbe\x97\', \'\xe5\xbe\x98\', \'\xe5\xbe\x99\', \'\xe5\xbe\x9c\', \'\xe5\xbe\xa1\', \'\xe5\xbe\xa8\', \'\xe5\xbe\xaa\',\n    \'\xe5\xbe\xad\', \'\xe5\xbe\xae\', \'\xe5\xbe\xb3\', \'\xe5\xbe\xb5\', \'\xe5\xbe\xb7\', \'\xe5\xbe\xbc\', \'\xe5\xbe\xbd\', \'\xe5\xbf\x83\', \'\xe5\xbf\x85\', \'\xe5\xbf\x86\', \'\xe5\xbf\x8c\', \'\xe5\xbf\x8d\', \'\xe5\xbf\x8f\', \'\xe5\xbf\x90\', \'\xe5\xbf\x91\', \'\xe5\xbf\x92\', \'\xe5\xbf\x96\', \'\xe5\xbf\x97\', \'\xe5\xbf\x98\', \'\xe5\xbf\x99\',\n    \'\xe5\xbf\x9d\', \'\xe5\xbf\xa0\', \'\xe5\xbf\xa1\', \'\xe5\xbf\xa4\', \'\xe5\xbf\xa7\', \'\xe5\xbf\xaa\', \'\xe5\xbf\xab\', \'\xe5\xbf\xb1\', \'\xe5\xbf\xb5\', \'\xe5\xbf\xbb\', \'\xe5\xbf\xbd\', \'\xe5\xbf\xbf\', \'\xe6\x80\x80\', \'\xe6\x80\x81\', \'\xe6\x80\x82\', \'\xe6\x80\x85\', \'\xe6\x80\x86\', \'\xe6\x80\x8d\', \'\xe6\x80\x8e\', \'\xe6\x80\x8f\',\n    \'\xe6\x80\x92\', \'\xe6\x80\x94\', \'\xe6\x80\x95\', \'\xe6\x80\x96\', \'\xe6\x80\x99\', \'\xe6\x80\x9b\', \'\xe6\x80\x9c\', \'\xe6\x80\x9d\', \'\xe6\x80\xa0\', \'\xe6\x80\xa1\', \'\xe6\x80\xa5\', \'\xe6\x80\xa6\', \'\xe6\x80\xa7\', \'\xe6\x80\xa8\', \'\xe6\x80\xaa\', \'\xe6\x80\xab\', \'\xe6\x80\xaf\', \'\xe6\x80\xb5\', \'\xe6\x80\xbb\', \'\xe6\x80\xbc\',\n    \'\xe6\x80\xbf\', \'\xe6\x81\x82\', \'\xe6\x81\x83\', \'\xe6\x81\x8b\', \'\xe6\x81\x8d\', \'\xe6\x81\x90\', \'\xe6\x81\x92\', \'\xe6\x81\x95\', \'\xe6\x81\x99\', \'\xe6\x81\x9a\', \'\xe6\x81\x9d\', \'\xe6\x81\xa2\', \'\xe6\x81\xa3\', \'\xe6\x81\xa4\', \'\xe6\x81\xa8\', \'\xe6\x81\xa9\', \'\xe6\x81\xaa\', \'\xe6\x81\xab\', \'\xe6\x81\xac\', \'\xe6\x81\xad\',\n    \'\xe6\x81\xaf\', \'\xe6\x81\xb0\', \'\xe6\x81\xb3\', \'\xe6\x81\xb6\', \'\xe6\x81\xb8\', \'\xe6\x81\xb9\', \'\xe6\x81\xba\', \'\xe6\x81\xbb\', \'\xe6\x81\xbc\', \'\xe6\x81\xbd\', \'\xe6\x81\xbf\', \'\xe6\x82\x84\', \'\xe6\x82\x89\', \'\xe6\x82\x8c\', \'\xe6\x82\x8d\', \'\xe6\x82\x92\', \'\xe6\x82\x94\', \'\xe6\x82\x96\', \'\xe6\x82\x9a\', \'\xe6\x82\x9d\',\n    \'\xe6\x82\x9e\', \'\xe6\x82\x9f\', \'\xe6\x82\xa0\', \'\xe6\x82\xa3\', \'\xe6\x82\xa6\', \'\xe6\x82\xa8\', \'\xe6\x82\xab\', \'\xe6\x82\xac\', \'\xe6\x82\xad\', \'\xe6\x82\xaf\', \'\xe6\x82\xb2\', \'\xe6\x82\xb4\', \'\xe6\x82\xb8\', \'\xe6\x82\xbb\', \'\xe6\x82\xbc\', \'\xe6\x83\x85\', \'\xe6\x83\x86\', \'\xe6\x83\x87\', \'\xe6\x83\x8a\', \'\xe6\x83\x8b\',\n    \'\xe6\x83\x91\', \'\xe6\x83\x95\', \'\xe6\x83\x98\', \'\xe6\x83\x9a\', \'\xe6\x83\x9c\', \'\xe6\x83\x9f\', \'\xe6\x83\xa0\', \'\xe6\x83\xa6\', \'\xe6\x83\xa7\', \'\xe6\x83\xa8\', \'\xe6\x83\xa9\', \'\xe6\x83\xab\', \'\xe6\x83\xac\', \'\xe6\x83\xad\', \'\xe6\x83\xae\', \'\xe6\x83\xaf\', \'\xe6\x83\xb0\', \'\xe6\x83\xb3\', \'\xe6\x83\xb4\', \'\xe6\x83\xb6\',\n    \'\xe6\x83\xb9\', \'\xe6\x83\xba\', \'\xe6\x84\x80\', \'\xe6\x84\x81\', \'\xe6\x84\x86\', \'\xe6\x84\x88\', \'\xe6\x84\x89\', \'\xe6\x84\x8d\', \'\xe6\x84\x8e\', \'\xe6\x84\x8f\', \'\xe6\x84\x95\', \'\xe6\x84\x9a\', \'\xe6\x84\x9b\', \'\xe6\x84\x9f\', \'\xe6\x84\xa0\', \'\xe6\x84\xa3\', \'\xe6\x84\xa4\', \'\xe6\x84\xa6\', \'\xe6\x84\xa7\', \'\xe6\x84\xab\',\n    \'\xe6\x84\xac\', \'\xe6\x84\xbf\', \'\xe6\x85\x88\', \'\xe6\x85\x8c\', \'\xe6\x85\x8e\', \'\xe6\x85\x91\', \'\xe6\x85\x95\', \'\xe6\x85\x99\', \'\xe6\x85\x9d\', \'\xe6\x85\xa2\', \'\xe6\x85\xa7\', \'\xe6\x85\xa8\', \'\xe6\x85\xb0\', \'\xe6\x85\xb5\', \'\xe6\x85\xb7\', \'\xe6\x86\x8b\', \'\xe6\x86\x8e\', \'\xe6\x86\x94\', \'\xe6\x86\xa7\', \'\xe6\x86\xa8\',\n    \'\xe6\x86\xa9\', \'\xe6\x86\xac\', \'\xe6\x86\xbe\', \'\xe6\x87\x82\', \'\xe6\x87\x88\', \'\xe6\x87\x8a\', \'\xe6\x87\x8b\', \'\xe6\x87\x91\', \'\xe6\x87\x92\', \'\xe6\x87\x9c\', \'\xe6\x87\xa6\', \'\xe6\x87\xb5\', \'\xe6\x87\xbf\', \'\xe6\x88\x86\', \'\xe6\x88\x88\', \'\xe6\x88\x8a\', \'\xe6\x88\x8c\', \'\xe6\x88\x8d\', \'\xe6\x88\x8e\', \'\xe6\x88\x8f\',\n    \'\xe6\x88\x90\', \'\xe6\x88\x91\', \'\xe6\x88\x92\', \'\xe6\x88\x95\', \'\xe6\x88\x96\', \'\xe6\x88\x97\', \'\xe6\x88\x98\', \'\xe6\x88\x9a\', \'\xe6\x88\x9b\', \'\xe6\x88\x9f\', \'\xe6\x88\xa2\', \'\xe6\x88\xaa\', \'\xe6\x88\xae\', \'\xe6\x88\xb3\', \'\xe6\x88\xb4\', \'\xe6\x88\xb7\', \'\xe6\x88\xbe\', \'\xe6\x88\xbf\', \'\xe6\x89\x80\', \'\xe6\x89\x81\',\n    \'\xe6\x89\x83\', \'\xe6\x89\x87\', \'\xe6\x89\x88\', \'\xe6\x89\x89\', \'\xe6\x89\x8b\', \'\xe6\x89\x8c\', \'\xe6\x89\x8d\', \'\xe6\x89\x8e\', \'\xe6\x89\x91\', \'\xe6\x89\x92\', \'\xe6\x89\x93\', \'\xe6\x89\x94\', \'\xe6\x89\x98\', \'\xe6\x89\x9b\', \'\xe6\x89\x9e\', \'\xe6\x89\xa2\', \'\xe6\x89\xa3\', \'\xe6\x89\xa7\', \'\xe6\x89\xa9\', \'\xe6\x89\xaa\',\n    \'\xe6\x89\xab\', \'\xe6\x89\xac\', \'\xe6\x89\xad\', \'\xe6\x89\xae\', \'\xe6\x89\xaf\', \'\xe6\x89\xb0\', \'\xe6\x89\xb3\', \'\xe6\x89\xb6\', \'\xe6\x89\xb9\', \'\xe6\x89\xbc\', \'\xe6\x89\xbe\', \'\xe6\x89\xbf\', \'\xe6\x8a\x80\', \'\xe6\x8a\x84\', \'\xe6\x8a\x89\', \'\xe6\x8a\x8a\', \'\xe6\x8a\x91\', \'\xe6\x8a\x92\', \'\xe6\x8a\x93\', \'\xe6\x8a\x95\',\n    \'\xe6\x8a\x96\', \'\xe6\x8a\x97\', \'\xe6\x8a\x98\', \'\xe6\x8a\x9a\', \'\xe6\x8a\x9b\', \'\xe6\x8a\x9f\', \'\xe6\x8a\xa0\', \'\xe6\x8a\xa1\', \'\xe6\x8a\xa2\', \'\xe6\x8a\xa4\', \'\xe6\x8a\xa5\', \'\xe6\x8a\xa8\', \'\xe6\x8a\xab\', \'\xe6\x8a\xac\', \'\xe6\x8a\xb1\', \'\xe6\x8a\xb5\', \'\xe6\x8a\xb9\', \'\xe6\x8a\xbc\', \'\xe6\x8a\xbd\', \'\xe6\x8a\xbf\',\n    \'\xe6\x8b\x82\', \'\xe6\x8b\x84\', \'\xe6\x8b\x85\', \'\xe6\x8b\x86\', \'\xe6\x8b\x87\', \'\xe6\x8b\x88\', \'\xe6\x8b\x89\', \'\xe6\x8b\x8a\', \'\xe6\x8b\x8c\', \'\xe6\x8b\x8d\', \'\xe6\x8b\x8e\', \'\xe6\x8b\x90\', \'\xe6\x8b\x92\', \'\xe6\x8b\x93\', \'\xe6\x8b\x94\', \'\xe6\x8b\x96\', \'\xe6\x8b\x97\', \'\xe6\x8b\x98\', \'\xe6\x8b\x99\', \'\xe6\x8b\x9a\',\n    \'\xe6\x8b\x9b\', \'\xe6\x8b\x9c\', \'\xe6\x8b\x9f\', \'\xe6\x8b\xa2\', \'\xe6\x8b\xa3\', \'\xe6\x8b\xa5\', \'\xe6\x8b\xa6\', \'\xe6\x8b\xa7\', \'\xe6\x8b\xa8\', \'\xe6\x8b\xa9\', \'\xe6\x8b\xac\', \'\xe6\x8b\xad\', \'\xe6\x8b\xae\', \'\xe6\x8b\xaf\', \'\xe6\x8b\xb1\', \'\xe6\x8b\xb3\', \'\xe6\x8b\xb4\', \'\xe6\x8b\xb7\', \'\xe6\x8b\xbc\', \'\xe6\x8b\xbd\',\n    \'\xe6\x8b\xbe\', \'\xe6\x8b\xbf\', \'\xe6\x8c\x81\', \'\xe6\x8c\x82\', \'\xe6\x8c\x87\', \'\xe6\x8c\x88\', \'\xe6\x8c\x89\', \'\xe6\x8c\x8e\', \'\xe6\x8c\x91\', \'\xe6\x8c\x96\', \'\xe6\x8c\x9a\', \'\xe6\x8c\x9b\', \'\xe6\x8c\x9d\', \'\xe6\x8c\x9e\', \'\xe6\x8c\x9f\', \'\xe6\x8c\xa0\', \'\xe6\x8c\xa1\', \'\xe6\x8c\xa3\', \'\xe6\x8c\xa4\', \'\xe6\x8c\xa5\',\n    \'\xe6\x8c\xa8\', \'\xe6\x8c\xaa\', \'\xe6\x8c\xab\', \'\xe6\x8c\xaf\', \'\xe6\x8c\xb9\', \'\xe6\x8c\xba\', \'\xe6\x8c\xbd\', \'\xe6\x8d\x82\', \'\xe6\x8d\x85\', \'\xe6\x8d\x86\', \'\xe6\x8d\x89\', \'\xe6\x8d\x8b\', \'\xe6\x8d\x8d\', \'\xe6\x8d\x8e\', \'\xe6\x8d\x8f\', \'\xe6\x8d\x90\', \'\xe6\x8d\x95\', \'\xe6\x8d\x9e\', \'\xe6\x8d\x9f\', \'\xe6\x8d\xa1\',\n    \'\xe6\x8d\xa2\', \'\xe6\x8d\xa3\', \'\xe6\x8d\xa7\', \'\xe6\x8d\xad\', \'\xe6\x8d\xae\', \'\xe6\x8d\xb1\', \'\xe6\x8d\xb6\', \'\xe6\x8d\xb7\', \'\xe6\x8d\xba\', \'\xe6\x8d\xbb\', \'\xe6\x8d\xbd\', \'\xe6\x8e\x80\', \'\xe6\x8e\x82\', \'\xe6\x8e\x87\', \'\xe6\x8e\x88\', \'\xe6\x8e\x89\', \'\xe6\x8e\x8c\', \'\xe6\x8e\x8f\', \'\xe6\x8e\x90\', \'\xe6\x8e\x92\',\n    \'\xe6\x8e\x96\', \'\xe6\x8e\x98\', \'\xe6\x8e\xa0\', \'\xe6\x8e\xa2\', \'\xe6\x8e\xa3\', \'\xe6\x8e\xa5\', \'\xe6\x8e\xa7\', \'\xe6\x8e\xa8\', \'\xe6\x8e\xa9\', \'\xe6\x8e\xaa\', \'\xe6\x8e\xac\', \'\xe6\x8e\xb0\', \'\xe6\x8e\xb3\', \'\xe6\x8e\xb7\', \'\xe6\x8e\xba\', \'\xe6\x8e\xbc\', \'\xe6\x8e\xbe\', \'\xe6\x8f\x84\', \'\xe6\x8f\x86\', \'\xe6\x8f\x89\',\n    \'\xe6\x8f\x8d\', \'\xe6\x8f\x8f\', \'\xe6\x8f\x90\', \'\xe6\x8f\x92\', \'\xe6\x8f\x96\', \'\xe6\x8f\x9c\', \'\xe6\x8f\xa1\', \'\xe6\x8f\xa3\', \'\xe6\x8f\xa9\', \'\xe6\x8f\xaa\', \'\xe6\x8f\xad\', \'\xe6\x8f\xb4\', \'\xe6\x8f\xb6\', \'\xe6\x8f\xbd\', \'\xe6\x90\x80\', \'\xe6\x90\x81\', \'\xe6\x90\x82\', \'\xe6\x90\x85\', \'\xe6\x90\x8f\', \'\xe6\x90\x90\',\n    \'\xe6\x90\x92\', \'\xe6\x90\x93\', \'\xe6\x90\x94\', \'\xe6\x90\x9c\', \'\xe6\x90\x9e\', \'\xe6\x90\xa0\', \'\xe6\x90\xa2\', \'\xe6\x90\xa4\', \'\xe6\x90\xaa\', \'\xe6\x90\xac\', \'\xe6\x90\xad\', \'\xe6\x90\xb4\', \'\xe6\x90\xba\', \'\xe6\x90\xbd\', \'\xe6\x91\x81\', \'\xe6\x91\x84\', \'\xe6\x91\x86\', \'\xe6\x91\x87\', \'\xe6\x91\x88\', \'\xe6\x91\x8a\',\n    \'\xe6\x91\x92\', \'\xe6\x91\x94\', \'\xe6\x91\x98\', \'\xe6\x91\x9e\', \'\xe6\x91\xa7\', \'\xe6\x91\xa9\', \'\xe6\x91\xb8\', \'\xe6\x91\xb9\', \'\xe6\x92\x82\', \'\xe6\x92\x87\', \'\xe6\x92\x91\', \'\xe6\x92\x92\', \'\xe6\x92\x95\', \'\xe6\x92\x9d\', \'\xe6\x92\x9e\', \'\xe6\x92\xa4\', \'\xe6\x92\xa9\', \'\xe6\x92\xac\', \'\xe6\x92\xad\', \'\xe6\x92\xae\',\n    \'\xe6\x92\xb0\', \'\xe6\x92\xb5\', \'\xe6\x92\xb7\', \'\xe6\x92\xba\', \'\xe6\x92\xbc\', \'\xe6\x93\x82\', \'\xe6\x93\x85\', \'\xe6\x93\x8d\', \'\xe6\x93\x8e\', \'\xe6\x93\x92\', \'\xe6\x93\x98\', \'\xe6\x93\x9e\', \'\xe6\x93\xa2\', \'\xe6\x93\xa6\', \'\xe6\x94\x80\', \'\xe6\x94\x92\', \'\xe6\x94\x98\', \'\xe6\x94\xa5\', \'\xe6\x94\xab\', \'\xe6\x94\xaf\',\n    \'\xe6\x94\xb6\', \'\xe6\x94\xb8\', \'\xe6\x94\xb9\', \'\xe6\x94\xbb\', \'\xe6\x94\xbe\', \'\xe6\x94\xbf\', \'\xe6\x95\x85\', \'\xe6\x95\x88\', \'\xe6\x95\x8c\', \'\xe6\x95\x8f\', \'\xe6\x95\x91\', \'\xe6\x95\x95\', \'\xe6\x95\x96\', \'\xe6\x95\x99\', \'\xe6\x95\x9b\', \'\xe6\x95\x9d\', \'\xe6\x95\x9e\', \'\xe6\x95\xa2\', \'\xe6\x95\xa3\', \'\xe6\x95\xa6\',\n    \'\xe6\x95\xac\', \'\xe6\x95\xb0\', \'\xe6\x95\xb2\', \'\xe6\x95\xb4\', \'\xe6\x95\xb7\', \'\xe6\x96\x84\', \'\xe6\x96\x87\', \'\xe6\x96\x8b\', \'\xe6\x96\x8c\', \'\xe6\x96\x90\', \'\xe6\x96\x91\', \'\xe6\x96\x93\', \'\xe6\x96\x97\', \'\xe6\x96\x99\', \'\xe6\x96\x9b\', \'\xe6\x96\x9c\', \'\xe6\x96\x9f\', \'\xe6\x96\xa1\', \'\xe6\x96\xa4\', \'\xe6\x96\xa5\',\n    \'\xe6\x96\xa7\', \'\xe6\x96\xa9\', \'\xe6\x96\xab\', \'\xe6\x96\xad\', \'\xe6\x96\xaf\', \'\xe6\x96\xb0\', \'\xe6\x96\xb2\', \'\xe6\x96\xb9\', \'\xe6\x96\xbc\', \'\xe6\x96\xbd\', \'\xe6\x97\x81\', \'\xe6\x97\x83\', \'\xe6\x97\x84\', \'\xe6\x97\x85\', \'\xe6\x97\x8b\', \'\xe6\x97\x8c\', \'\xe6\x97\x8e\', \'\xe6\x97\x8f\', \'\xe6\x97\x92\', \'\xe6\x97\x96\',\n    \'\xe6\x97\x97\', \'\xe6\x97\x98\', \'\xe6\x97\xa0\', \'\xe6\x97\xa2\', \'\xe6\x97\xa5\', \'\xe6\x97\xa6\', \'\xe6\x97\xa7\', \'\xe6\x97\xa8\', \'\xe6\x97\xa9\', \'\xe6\x97\xac\', \'\xe6\x97\xad\', \'\xe6\x97\xb1\', \'\xe6\x97\xb3\', \'\xe6\x97\xb6\', \'\xe6\x97\xb7\', \'\xe6\x97\xba\', \'\xe6\x97\xbb\', \'\xe6\x98\x80\', \'\xe6\x98\x82\', \'\xe6\x98\x83\',\n    \'\xe6\x98\x86\', \'\xe6\x98\x87\', \'\xe6\x98\x8a\', \'\xe6\x98\x8c\', \'\xe6\x98\x8e\', \'\xe6\x98\x8f\', \'\xe6\x98\x93\', \'\xe6\x98\x94\', \'\xe6\x98\x95\', \'\xe6\x98\x99\', \'\xe6\x98\x9f\', \'\xe6\x98\xa0\', \'\xe6\x98\xa5\', \'\xe6\x98\xa7\', \'\xe6\x98\xa8\', \'\xe6\x98\xad\', \'\xe6\x98\xaf\', \'\xe6\x98\xb1\', \'\xe6\x98\xb4\', \'\xe6\x98\xb5\',\n    \'\xe6\x98\xbc\', \'\xe6\x98\xbe\', \'\xe6\x99\x81\', \'\xe6\x99\x83\', \'\xe6\x99\x8b\', \'\xe6\x99\x8c\', \'\xe6\x99\x8f\', \'\xe6\x99\x92\', \'\xe6\x99\x93\', \'\xe6\x99\x94\', \'\xe6\x99\x95\', \'\xe6\x99\x96\', \'\xe6\x99\x97\', \'\xe6\x99\x9a\', \'\xe6\x99\x9e\', \'\xe6\x99\x9f\', \'\xe6\x99\xa1\', \'\xe6\x99\xa4\', \'\xe6\x99\xa6\', \'\xe6\x99\xa8\',\n    \'\xe6\x99\xae\', \'\xe6\x99\xaf\', \'\xe6\x99\xb0\', \'\xe6\x99\xb4\', \'\xe6\x99\xb6\', \'\xe6\x99\xba\', \'\xe6\x99\xbe\', \'\xe6\x9a\x82\', \'\xe6\x9a\x84\', \'\xe6\x9a\x87\', \'\xe6\x9a\x91\', \'\xe6\x9a\x96\', \'\xe6\x9a\x97\', \'\xe6\x9a\xa7\', \'\xe6\x9a\xa8\', \'\xe6\x9a\xae\', \'\xe6\x9a\xb2\', \'\xe6\x9a\xb4\', \'\xe6\x9a\xb9\', \'\xe6\x9a\xbe\',\n    \'\xe6\x9b\x99\', \'\xe6\x9b\x9b\', \'\xe6\x9b\x9c\', \'\xe6\x9b\x9d\', \'\xe6\x9b\xa6\', \'\xe6\x9b\xa9\', \'\xe6\x9b\xaa\', \'\xe6\x9b\xb0\', \'\xe6\x9b\xb2\', \'\xe6\x9b\xb3\', \'\xe6\x9b\xb4\', \'\xe6\x9b\xb7\', \'\xe6\x9b\xb8\', \'\xe6\x9b\xb9\', \'\xe6\x9b\xbc\', \'\xe6\x9b\xbe\', \'\xe6\x9b\xbf\', \'\xe6\x9c\x80\', \'\xe6\x9c\x88\', \'\xe6\x9c\x89\',\n    \'\xe6\x9c\x8b\', \'\xe6\x9c\x8d\', \'\xe6\x9c\x90\', \'\xe6\x9c\x94\', \'\xe6\x9c\x95\', \'\xe6\x9c\x97\', \'\xe6\x9c\x9b\', \'\xe6\x9c\x9d\', \'\xe6\x9c\x9f\', \'\xe6\x9c\xa6\', \'\xe6\x9c\xa8\', \'\xe6\x9c\xaa\', \'\xe6\x9c\xab\', \'\xe6\x9c\xac\', \'\xe6\x9c\xad\', \'\xe6\x9c\xaf\', \'\xe6\x9c\xb1\', \'\xe6\x9c\xb4\', \'\xe6\x9c\xb5\', \'\xe6\x9c\xba\',\n    \'\xe6\x9c\xbd\', \'\xe6\x9d\x80\', \'\xe6\x9d\x82\', \'\xe6\x9d\x83\', \'\xe6\x9d\x85\', \'\xe6\x9d\x86\', \'\xe6\x9d\x89\', \'\xe6\x9d\x8c\', \'\xe6\x9d\x8e\', \'\xe6\x9d\x8f\', \'\xe6\x9d\x90\', \'\xe6\x9d\x91\', \'\xe6\x9d\x93\', \'\xe6\x9d\x96\', \'\xe6\x9d\x9c\', \'\xe6\x9d\x9e\', \'\xe6\x9d\x9f\', \'\xe6\x9d\xa0\', \'\xe6\x9d\xa1\', \'\xe6\x9d\xa5\',\n    \'\xe6\x9d\xa8\', \'\xe6\x9d\xaa\', \'\xe6\x9d\xad\', \'\xe6\x9d\xaf\', \'\xe6\x9d\xb0\', \'\xe6\x9d\xb2\', \'\xe6\x9d\xb3\', \'\xe6\x9d\xb5\', \'\xe6\x9d\xb7\', \'\xe6\x9d\xbc\', \'\xe6\x9d\xbe\', \'\xe6\x9d\xbf\', \'\xe6\x9e\x81\', \'\xe6\x9e\x84\', \'\xe6\x9e\x87\', \'\xe6\x9e\x89\', \'\xe6\x9e\x8b\', \'\xe6\x9e\x90\', \'\xe6\x9e\x95\', \'\xe6\x9e\x97\',\n    \'\xe6\x9e\x9a\', \'\xe6\x9e\x9c\', \'\xe6\x9e\x9d\', \'\xe6\x9e\x9e\', \'\xe6\x9e\xa2\', \'\xe6\x9e\xa3\', \'\xe6\x9e\xa5\', \'\xe6\x9e\xaa\', \'\xe6\x9e\xab\', \'\xe6\x9e\xad\', \'\xe6\x9e\xaf\', \'\xe6\x9e\xb0\', \'\xe6\x9e\xb3\', \'\xe6\x9e\xb5\', \'\xe6\x9e\xb6\', \'\xe6\x9e\xb7\', \'\xe6\x9e\xb8\', \'\xe6\x9f\x84\', \'\xe6\x9f\x88\', \'\xe6\x9f\x8f\',\n    \'\xe6\x9f\x90\', \'\xe6\x9f\x91\', \'\xe6\x9f\x93\', \'\xe6\x9f\x94\', \'\xe6\x9f\x98\', \'\xe6\x9f\x9a\', \'\xe6\x9f\x9c\', \'\xe6\x9f\x9e\', \'\xe6\x9f\xa0\', \'\xe6\x9f\xa2\', \'\xe6\x9f\xa5\', \'\xe6\x9f\xa9\', \'\xe6\x9f\xac\', \'\xe6\x9f\xaf\', \'\xe6\x9f\xb0\', \'\xe6\x9f\xb1\', \'\xe6\x9f\xb3\', \'\xe6\x9f\xb4\', \'\xe6\x9f\xbf\', \'\xe6\xa0\x80\',\n    \'\xe6\xa0\x85\', \'\xe6\xa0\x87\', \'\xe6\xa0\x88\', \'\xe6\xa0\x89\', \'\xe6\xa0\x8b\', \'\xe6\xa0\x8e\', \'\xe6\xa0\x8f\', \'\xe6\xa0\x91\', \'\xe6\xa0\x93\', \'\xe6\xa0\x96\', \'\xe6\xa0\x97\', \'\xe6\xa0\xa1\', \'\xe6\xa0\xa9\', \'\xe6\xa0\xaa\', \'\xe6\xa0\xb7\', \'\xe6\xa0\xb8\', \'\xe6\xa0\xb9\', \'\xe6\xa0\xbc\', \'\xe6\xa0\xbd\', \'\xe6\xa0\xbe\',\n    \'\xe6\xa1\x80\', \'\xe6\xa1\x82\', \'\xe6\xa1\x83\', \'\xe6\xa1\x85\', \'\xe6\xa1\x86\', \'\xe6\xa1\x88\', \'\xe6\xa1\x8c\', \'\xe6\xa1\x8e\', \'\xe6\xa1\x90\', \'\xe6\xa1\x91\', \'\xe6\xa1\x93\', \'\xe6\xa1\x94\', \'\xe6\xa1\x9e\', \'\xe6\xa1\xa0\', \'\xe6\xa1\xa1\', \'\xe6\xa1\xa2\', \'\xe6\xa1\xa3\', \'\xe6\xa1\xa5\', \'\xe6\xa1\xa6\', \'\xe6\xa1\xa7\',\n    \'\xe6\xa1\xa8\', \'\xe6\xa1\xa9\', \'\xe6\xa1\xb6\', \'\xe6\xa2\x81\', \'\xe6\xa2\x83\', \'\xe6\xa2\x85\', \'\xe6\xa2\x86\', \'\xe6\xa2\x8f\', \'\xe6\xa2\x93\', \'\xe6\xa2\x97\', \'\xe6\xa2\xa2\', \'\xe6\xa2\xa6\', \'\xe6\xa2\xa7\', \'\xe6\xa2\xa8\', \'\xe6\xa2\xad\', \'\xe6\xa2\xaf\', \'\xe6\xa2\xb0\', \'\xe6\xa2\xb3\', \'\xe6\xa2\xb5\', \'\xe6\xa3\x80\',\n    \'\xe6\xa3\x82\', \'\xe6\xa3\x89\', \'\xe6\xa3\x8b\', \'\xe6\xa3\x8d\', \'\xe6\xa3\x92\', \'\xe6\xa3\x93\', \'\xe6\xa3\x95\', \'\xe6\xa3\x98\', \'\xe6\xa3\x9a\', \'\xe6\xa3\xa0\', \'\xe6\xa3\xa3\', \'\xe6\xa3\xae\', \'\xe6\xa3\xb0\', \'\xe6\xa3\xb1\', \'\xe6\xa3\xb5\', \'\xe6\xa3\xb9\', \'\xe6\xa3\xba\', \'\xe6\xa4\x80\', \'\xe6\xa4\x81\', \'\xe6\xa4\x85\',\n    \'\xe6\xa4\x8d\', \'\xe6\xa4\x8e\', \'\xe6\xa4\x90\', \'\xe6\xa4\x92\', \'\xe6\xa4\x9f\', \'\xe6\xa4\xad\', \'\xe6\xa4\xb0\', \'\xe6\xa4\xb9\', \'\xe6\xa4\xbd\', \'\xe6\xa4\xbf\', \'\xe6\xa5\x82\', \'\xe6\xa5\x94\', \'\xe6\xa5\x97\', \'\xe6\xa5\x9a\', \'\xe6\xa5\x9e\', \'\xe6\xa5\xa0\', \'\xe6\xa5\xa3\', \'\xe6\xa5\xab\', \'\xe6\xa5\xae\', \'\xe6\xa5\xaf\',\n    \'\xe6\xa5\xb7\', \'\xe6\xa5\xb8\', \'\xe6\xa5\xb9\', \'\xe6\xa5\xbc\', \'\xe6\xa6\x82\', \'\xe6\xa6\x84\', \'\xe6\xa6\x86\', \'\xe6\xa6\x87\', \'\xe6\xa6\x88\', \'\xe6\xa6\x94\', \'\xe6\xa6\x95\', \'\xe6\xa6\x9b\', \'\xe6\xa6\x9c\', \'\xe6\xa6\xa7\', \'\xe6\xa6\xa8\', \'\xe6\xa6\xab\', \'\xe6\xa6\xad\', \'\xe6\xa6\xb1\', \'\xe6\xa6\xb4\', \'\xe6\xa6\xb7\',\n    \'\xe6\xa6\xbb\', \'\xe6\xa6\xbc\', \'\xe6\xa7\x81\', \'\xe6\xa7\x83\', \'\xe6\xa7\x8a\', \'\xe6\xa7\x8c\', \'\xe6\xa7\x90\', \'\xe6\xa7\x9b\', \'\xe6\xa7\x9f\', \'\xe6\xa7\xbd\', \'\xe6\xa7\xbf\', \'\xe6\xa8\x8a\', \'\xe6\xa8\x97\', \'\xe6\xa8\x9f\', \'\xe6\xa8\xa1\', \'\xe6\xa8\xaa\', \'\xe6\xa8\xad\', \'\xe6\xa8\xaf\', \'\xe6\xa8\xb1\', \'\xe6\xa8\xb5\',\n    \'\xe6\xa8\xbd\', \'\xe6\xa8\xbe\', \'\xe6\xa9\x84\', \'\xe6\xa9\x87\', \'\xe6\xa9\x90\', \'\xe6\xa9\x98\', \'\xe6\xa9\x99\', \'\xe6\xa9\xa1\', \'\xe6\xa9\xaa\', \'\xe6\xa9\xb1\', \'\xe6\xa9\xb9\', \'\xe6\xaa\x80\', \'\xe6\xaa\x84\', \'\xe6\xaa\x87\', \'\xe6\xaa\x90\', \'\xe6\xaa\xa0\', \'\xe6\xaa\xac\', \'\xe6\xac\x83\', \'\xe6\xac\xa0\', \'\xe6\xac\xa1\',\n    \'\xe6\xac\xa2\', \'\xe6\xac\xa3\', \'\xe6\xac\xa4\', \'\xe6\xac\xa7\', \'\xe6\xac\xb2\', \'\xe6\xac\xb7\', \'\xe6\xac\xba\', \'\xe6\xac\xbb\', \'\xe6\xac\xbe\', \'\xe6\xad\x83\', \'\xe6\xad\x86\', \'\xe6\xad\x87\', \'\xe6\xad\x89\', \'\xe6\xad\x8c\', \'\xe6\xad\x94\', \'\xe6\xad\x98\', \'\xe6\xad\x99\', \'\xe6\xad\xa2\', \'\xe6\xad\xa3\', \'\xe6\xad\xa4\',\n    \'\xe6\xad\xa5\', \'\xe6\xad\xa6\', \'\xe6\xad\xa7\', \'\xe6\xad\xaa\', \'\xe6\xad\xb9\', \'\xe6\xad\xbb\', \'\xe6\xad\xbc\', \'\xe6\xae\x81\', \'\xe6\xae\x82\', \'\xe6\xae\x83\', \'\xe6\xae\x84\', \'\xe6\xae\x86\', \'\xe6\xae\x87\', \'\xe6\xae\x89\', \'\xe6\xae\x8a\', \'\xe6\xae\x8b\', \'\xe6\xae\x92\', \'\xe6\xae\x93\', \'\xe6\xae\x96\', \'\xe6\xae\x9a\',\n    \'\xe6\xae\x9b\', \'\xe6\xae\xa1\', \'\xe6\xae\xaa\', \'\xe6\xae\xb4\', \'\xe6\xae\xb5\', \'\xe6\xae\xb7\', \'\xe6\xae\xbd\', \'\xe6\xae\xbf\', \'\xe6\xaf\x81\', \'\xe6\xaf\x82\', \'\xe6\xaf\x85\', \'\xe6\xaf\x8b\', \'\xe6\xaf\x8d\', \'\xe6\xaf\x8f\', \'\xe6\xaf\x90\', \'\xe6\xaf\x92\', \'\xe6\xaf\x93\', \'\xe6\xaf\x94\', \'\xe6\xaf\x95\', \'\xe6\xaf\x97\',\n    \'\xe6\xaf\x99\', \'\xe6\xaf\x9b\', \'\xe6\xaf\xa1\', \'\xe6\xaf\xab\', \'\xe6\xaf\xaf\', \'\xe6\xaf\xb6\', \'\xe6\xb0\x85\', \'\xe6\xb0\x86\', \'\xe6\xb0\x87\', \'\xe6\xb0\x8f\', \'\xe6\xb0\x90\', \'\xe6\xb0\x91\', \'\xe6\xb0\x93\', \'\xe6\xb0\x94\', \'\xe6\xb0\x96\', \'\xe6\xb0\x9b\', \'\xe6\xb0\x9f\', \'\xe6\xb0\xa1\', \'\xe6\xb0\xa2\', \'\xe6\xb0\xa4\',\n    \'\xe6\xb0\xa6\', \'\xe6\xb0\xa7\', \'\xe6\xb0\xa8\', \'\xe6\xb0\xa9\', \'\xe6\xb0\xae\', \'\xe6\xb0\xaf\', \'\xe6\xb0\xb0\', \'\xe6\xb0\xb2\', \'\xe6\xb0\xb4\', \'\xe6\xb0\xb8\', \'\xe6\xb1\x80\', \'\xe6\xb1\x81\', \'\xe6\xb1\x82\', \'\xe6\xb1\x87\', \'\xe6\xb1\x89\', \'\xe6\xb1\x90\', \'\xe6\xb1\x95\', \'\xe6\xb1\x97\', \'\xe6\xb1\x9b\', \'\xe6\xb1\x9c\',\n    \'\xe6\xb1\x9d\', \'\xe6\xb1\x9e\', \'\xe6\xb1\x9f\', \'\xe6\xb1\xa0\', \'\xe6\xb1\xa1\', \'\xe6\xb1\xa4\', \'\xe6\xb1\xa7\', \'\xe6\xb1\xa8\', \'\xe6\xb1\xa9\', \'\xe6\xb1\xaa\', \'\xe6\xb1\xad\', \'\xe6\xb1\xb0\', \'\xe6\xb1\xb2\', \'\xe6\xb1\xb4\', \'\xe6\xb1\xb6\', \'\xe6\xb1\xb9\', \'\xe6\xb1\xbd\', \'\xe6\xb1\xbe\', \'\xe6\xb2\x81\', \'\xe6\xb2\x82\',\n    \'\xe6\xb2\x83\', \'\xe6\xb2\x85\', \'\xe6\xb2\x87\', \'\xe6\xb2\x88\', \'\xe6\xb2\x89\', \'\xe6\xb2\x8c\', \'\xe6\xb2\x90\', \'\xe6\xb2\x93\', \'\xe6\xb2\x94\', \'\xe6\xb2\x95\', \'\xe6\xb2\x99\', \'\xe6\xb2\x9b\', \'\xe6\xb2\x9f\', \'\xe6\xb2\xa1\', \'\xe6\xb2\xa3\', \'\xe6\xb2\xa5\', \'\xe6\xb2\xa6\', \'\xe6\xb2\xa7\', \'\xe6\xb2\xaa\', \'\xe6\xb2\xab\',\n    \'\xe6\xb2\xac\', \'\xe6\xb2\xae\', \'\xe6\xb2\xb1\', \'\xe6\xb2\xb3\', \'\xe6\xb2\xb8\', \'\xe6\xb2\xb9\', \'\xe6\xb2\xbb\', \'\xe6\xb2\xbc\', \'\xe6\xb2\xbd\', \'\xe6\xb2\xbe\', \'\xe6\xb2\xbf\', \'\xe6\xb3\x84\', \'\xe6\xb3\x85\', \'\xe6\xb3\x89\', \'\xe6\xb3\x8a\', \'\xe6\xb3\x8c\', \'\xe6\xb3\x93\', \'\xe6\xb3\x95\', \'\xe6\xb3\x97\', \'\xe6\xb3\x9b\',\n    \'\xe6\xb3\x9e\', \'\xe6\xb3\xa0\', \'\xe6\xb3\xa1\', \'\xe6\xb3\xa2\', \'\xe6\xb3\xa3\', \'\xe6\xb3\xa5\', \'\xe6\xb3\xa8\', \'\xe6\xb3\xaa\', \'\xe6\xb3\xab\', \'\xe6\xb3\xae\', \'\xe6\xb3\xaf\', \'\xe6\xb3\xb0\', \'\xe6\xb3\xb1\', \'\xe6\xb3\xb3\', \'\xe6\xb3\xb5\', \'\xe6\xb3\xb7\', \'\xe6\xb3\xb8\', \'\xe6\xb3\xbb\', \'\xe6\xb3\xbc\', \'\xe6\xb3\xbd\',\n    \'\xe6\xb3\xbe\', \'\xe6\xb4\x81\', \'\xe6\xb4\x8b\', \'\xe6\xb4\x92\', \'\xe6\xb4\x97\', \'\xe6\xb4\x99\', \'\xe6\xb4\x9b\', \'\xe6\xb4\x9e\', \'\xe6\xb4\x9f\', \'\xe6\xb4\xa5\', \'\xe6\xb4\xaa\', \'\xe6\xb4\xab\', \'\xe6\xb4\xae\', \'\xe6\xb4\xb1\', \'\xe6\xb4\xb2\', \'\xe6\xb4\xb9\', \'\xe6\xb4\xba\', \'\xe6\xb4\xbb\', \'\xe6\xb4\xbc\', \'\xe6\xb4\xbd\',\n    \'\xe6\xb4\xbe\', \'\xe6\xb5\x81\', \'\xe6\xb5\x83\', \'\xe6\xb5\x85\', \'\xe6\xb5\x86\', \'\xe6\xb5\x87\', \'\xe6\xb5\x8a\', \'\xe6\xb5\x8b\', \'\xe6\xb5\x8d\', \'\xe6\xb5\x8e\', \'\xe6\xb5\x8f\', \'\xe6\xb5\x90\', \'\xe6\xb5\x91\', \'\xe6\xb5\x92\', \'\xe6\xb5\x93\', \'\xe6\xb5\x94\', \'\xe6\xb5\x99\', \'\xe6\xb5\x9a\', \'\xe6\xb5\x9c\', \'\xe6\xb5\x9e\',\n    \'\xe6\xb5\xa3\', \'\xe6\xb5\xa6\', \'\xe6\xb5\xa9\', \'\xe6\xb5\xaa\', \'\xe6\xb5\xae\', \'\xe6\xb5\xb4\', \'\xe6\xb5\xb7\', \'\xe6\xb5\xb8\', \'\xe6\xb5\xbc\', \'\xe6\xb5\xbf\', \'\xe6\xb6\x82\', \'\xe6\xb6\x85\', \'\xe6\xb6\x88\', \'\xe6\xb6\x89\', \'\xe6\xb6\x8c\', \'\xe6\xb6\x8e\', \'\xe6\xb6\x92\', \'\xe6\xb6\x93\', \'\xe6\xb6\x94\', \'\xe6\xb6\x95\',\n    \'\xe6\xb6\x9b\', \'\xe6\xb6\x9d\', \'\xe6\xb6\x9f\', \'\xe6\xb6\xa1\', \'\xe6\xb6\xa3\', \'\xe6\xb6\xa4\', \'\xe6\xb6\xa6\', \'\xe6\xb6\xa7\', \'\xe6\xb6\xa8\', \'\xe6\xb6\xa9\', \'\xe6\xb6\xaa\', \'\xe6\xb6\xae\', \'\xe6\xb6\xaf\', \'\xe6\xb6\xb2\', \'\xe6\xb6\xb5\', \'\xe6\xb6\xb8\', \'\xe6\xb6\xbf\', \'\xe6\xb7\x80\', \'\xe6\xb7\x84\', \'\xe6\xb7\x85\',\n    \'\xe6\xb7\x86\', \'\xe6\xb7\x87\', \'\xe6\xb7\x8b\', \'\xe6\xb7\x8c\', \'\xe6\xb7\x91\', \'\xe6\xb7\x96\', \'\xe6\xb7\x98\', \'\xe6\xb7\x99\', \'\xe6\xb7\x9e\', \'\xe6\xb7\xa1\', \'\xe6\xb7\xa4\', \'\xe6\xb7\xab\', \'\xe6\xb7\xac\', \'\xe6\xb7\xae\', \'\xe6\xb7\xb1\', \'\xe6\xb7\xb3\', \'\xe6\xb7\xb7\', \'\xe6\xb7\xb9\', \'\xe6\xb7\xbb\', \'\xe6\xb7\xbc\',\n    \'\xe6\xb8\x85\', \'\xe6\xb8\x8a\', \'\xe6\xb8\x8d\', \'\xe6\xb8\x8e\', \'\xe6\xb8\x90\', \'\xe6\xb8\x91\', \'\xe6\xb8\x94\', \'\xe6\xb8\x96\', \'\xe6\xb8\x97\', \'\xe6\xb8\x9a\', \'\xe6\xb8\x9d\', \'\xe6\xb8\xa0\', \'\xe6\xb8\xa1\', \'\xe6\xb8\xa3\', \'\xe6\xb8\xa4\', \'\xe6\xb8\xa5\', \'\xe6\xb8\xa9\', \'\xe6\xb8\xad\', \'\xe6\xb8\xaf\', \'\xe6\xb8\xb2\',\n    \'\xe6\xb8\xb4\', \'\xe6\xb8\xb8\', \'\xe6\xb8\xba\', \'\xe6\xb9\x83\', \'\xe6\xb9\x84\', \'\xe6\xb9\x8d\', \'\xe6\xb9\x8e\', \'\xe6\xb9\x96\', \'\xe6\xb9\x98\', \'\xe6\xb9\x9b\', \'\xe6\xb9\x9f\', \'\xe6\xb9\xab\', \'\xe6\xb9\xae\', \'\xe6\xb9\xbe\', \'\xe6\xb9\xbf\', \'\xe6\xba\x83\', \'\xe6\xba\x85\', \'\xe6\xba\x89\', \'\xe6\xba\x8a\', \'\xe6\xba\x8d\',\n    \'\xe6\xba\x90\', \'\xe6\xba\x9c\', \'\xe6\xba\x9f\', \'\xe6\xba\xa2\', \'\xe6\xba\xa5\', \'\xe6\xba\xa7\', \'\xe6\xba\xaa\', \'\xe6\xba\xaf\', \'\xe6\xba\xb2\', \'\xe6\xba\xb4\', \'\xe6\xba\xb6\', \'\xe6\xba\xb7\', \'\xe6\xba\xba\', \'\xe6\xbb\x81\', \'\xe6\xbb\x82\', \'\xe6\xbb\x87\', \'\xe6\xbb\x88\', \'\xe6\xbb\x8b\', \'\xe6\xbb\x91\', \'\xe6\xbb\x93\',\n    \'\xe6\xbb\x94\', \'\xe6\xbb\x95\', \'\xe6\xbb\x9a\', \'\xe6\xbb\x9e\', \'\xe6\xbb\xa1\', \'\xe6\xbb\xa4\', \'\xe6\xbb\xa5\', \'\xe6\xbb\xa6\', \'\xe6\xbb\xa8\', \'\xe6\xbb\xa9\', \'\xe6\xbb\xb4\', \'\xe6\xbc\x82\', \'\xe6\xbc\x86\', \'\xe6\xbc\x89\', \'\xe6\xbc\x8f\', \'\xe6\xbc\x93\', \'\xe6\xbc\x94\', \'\xe6\xbc\x95\', \'\xe6\xbc\xa0\', \'\xe6\xbc\xa6\',\n    \'\xe6\xbc\xa9\', \'\xe6\xbc\xaa\', \'\xe6\xbc\xab\', \'\xe6\xbc\xaf\', \'\xe6\xbc\xb1\', \'\xe6\xbc\xb3\', \'\xe6\xbc\xbe\', \'\xe6\xbd\x87\', \'\xe6\xbd\x8d\', \'\xe6\xbd\x8f\', \'\xe6\xbd\x98\', \'\xe6\xbd\x9c\', \'\xe6\xbd\x9e\', \'\xe6\xbd\xa2\', \'\xe6\xbd\xa6\', \'\xe6\xbd\xad\', \'\xe6\xbd\xae\', \'\xe6\xbd\xb4\', \'\xe6\xbd\xb8\', \'\xe6\xbd\xba\',\n    \'\xe6\xbd\xbc\', \'\xe6\xbe\x84\', \'\xe6\xbe\x88\', \'\xe6\xbe\x8d\', \'\xe6\xbe\x8e\', \'\xe6\xbe\x99\', \'\xe6\xbe\x9c\', \'\xe6\xbe\x9d\', \'\xe6\xbe\xa1\', \'\xe6\xbe\xa7\', \'\xe6\xbe\xb3\', \'\xe6\xbe\xb9\', \'\xe6\xbf\x80\', \'\xe6\xbf\x82\', \'\xe6\xbf\x91\', \'\xe6\xbf\x92\', \'\xe6\xbf\x9e\', \'\xe6\xbf\xa0\', \'\xe6\xbf\xa1\', \'\xe6\xbf\xa9\',\n    \'\xe6\xbf\xae\', \'\xe6\xbf\xaf\', \'\xe7\x80\x91\', \'\xe7\x80\x9a\', \'\xe7\x80\x9b\', \'\xe7\x80\xb9\', \'\xe7\x81\x8a\', \'\xe7\x81\x8c\', \'\xe7\x81\x9e\', \'\xe7\x81\xab\', \'\xe7\x81\xad\', \'\xe7\x81\xaf\', \'\xe7\x81\xb0\', \'\xe7\x81\xb5\', \'\xe7\x81\xb6\', \'\xe7\x81\xb8\', \'\xe7\x81\xbc\', \'\xe7\x81\xbe\', \'\xe7\x81\xbf\', \'\xe7\x82\x80\',\n    \'\xe7\x82\x89\', \'\xe7\x82\x8a\', \'\xe7\x82\x8e\', \'\xe7\x82\x92\', \'\xe7\x82\x94\', \'\xe7\x82\x95\', \'\xe7\x82\x96\', \'\xe7\x82\x99\', \'\xe7\x82\x9c\', \'\xe7\x82\x9f\', \'\xe7\x82\xab\', \'\xe7\x82\xac\', \'\xe7\x82\xad\', \'\xe7\x82\xae\', \'\xe7\x82\xaf\', \'\xe7\x82\xb3\', \'\xe7\x82\xb7\', \'\xe7\x82\xb8\', \'\xe7\x82\xb9\', \'\xe7\x82\xbc\',\n    \'\xe7\x82\xbd\', \'\xe7\x83\x81\', \'\xe7\x83\x82\', \'\xe7\x83\x88\', \'\xe7\x83\x98\', \'\xe7\x83\x99\', \'\xe7\x83\x9b\', \'\xe7\x83\x9c\', \'\xe7\x83\x9d\', \'\xe7\x83\x9f\', \'\xe7\x83\xa4\', \'\xe7\x83\xa6\', \'\xe7\x83\xa7\', \'\xe7\x83\xa8\', \'\xe7\x83\xa9\', \'\xe7\x83\xab\', \'\xe7\x83\xac\', \'\xe7\x83\xad\', \'\xe7\x83\xaf\', \'\xe7\x83\xb7\',\n    \'\xe7\x83\xb9\', \'\xe7\x83\xbd\', \'\xe7\x84\x89\', \'\xe7\x84\x8a\', \'\xe7\x84\x92\', \'\xe7\x84\x95\', \'\xe7\x84\x96\', \'\xe7\x84\x98\', \'\xe7\x84\x99\', \'\xe7\x84\x9a\', \'\xe7\x84\x9c\', \'\xe7\x84\xa6\', \'\xe7\x84\xaf\', \'\xe7\x84\xb0\', \'\xe7\x84\xb1\', \'\xe7\x84\xb6\', \'\xe7\x84\xbb\', \'\xe7\x85\x85\', \'\xe7\x85\x8a\', \'\xe7\x85\x8c\',\n    \'\xe7\x85\x8e\', \'\xe7\x85\x9c\', \'\xe7\x85\x9e\', \'\xe7\x85\xa4\', \'\xe7\x85\xa6\', \'\xe7\x85\xa7\', \'\xe7\x85\xa8\', \'\xe7\x85\xae\', \'\xe7\x85\xb2\', \'\xe7\x85\xbd\', \'\xe7\x86\x84\', \'\xe7\x86\x8a\', \'\xe7\x86\x8f\', \'\xe7\x86\x94\', \'\xe7\x86\x99\', \'\xe7\x86\x9b\', \'\xe7\x86\x9f\', \'\xe7\x86\xa0\', \'\xe7\x86\xa8\', \'\xe7\x86\xac\',\n    \'\xe7\x86\xb9\', \'\xe7\x87\x83\', \'\xe7\x87\x8e\', \'\xe7\x87\x94\', \'\xe7\x87\x95\', \'\xe7\x87\xa0\', \'\xe7\x87\xa5\', \'\xe7\x87\xa7\', \'\xe7\x87\xae\', \'\xe7\x87\xb9\', \'\xe7\x88\x86\', \'\xe7\x88\x87\', \'\xe7\x88\xa8\', \'\xe7\x88\xaa\', \'\xe7\x88\xac\', \'\xe7\x88\xb0\', \'\xe7\x88\xb1\', \'\xe7\x88\xb2\', \'\xe7\x88\xb5\', \'\xe7\x88\xb6\',\n    \'\xe7\x88\xb7\', \'\xe7\x88\xb8\', \'\xe7\x88\xb9\', \'\xe7\x88\xbb\', \'\xe7\x88\xbd\', \'\xe7\x89\x82\', \'\xe7\x89\x87\', \'\xe7\x89\x88\', \'\xe7\x89\x8c\', \'\xe7\x89\x8d\', \'\xe7\x89\x92\', \'\xe7\x89\x96\', \'\xe7\x89\x99\', \'\xe7\x89\x9b\', \'\xe7\x89\x9d\', \'\xe7\x89\x9f\', \'\xe7\x89\xa1\', \'\xe7\x89\xa2\', \'\xe7\x89\xa6\', \'\xe7\x89\xa7\',\n    \'\xe7\x89\xa9\', \'\xe7\x89\xaf\', \'\xe7\x89\xb2\', \'\xe7\x89\xb5\', \'\xe7\x89\xb9\', \'\xe7\x89\xba\', \'\xe7\x8a\x80\', \'\xe7\x8a\x81\', \'\xe7\x8a\x8a\', \'\xe7\x8a\x8d\', \'\xe7\x8a\x92\', \'\xe7\x8a\xa8\', \'\xe7\x8a\xac\', \'\xe7\x8a\xaf\', \'\xe7\x8a\xb4\', \'\xe7\x8a\xb6\', \'\xe7\x8a\xb7\', \'\xe7\x8a\xb9\', \'\xe7\x8b\x81\', \'\xe7\x8b\x82\',\n    \'\xe7\x8b\x83\', \'\xe7\x8b\x84\', \'\xe7\x8b\x88\', \'\xe7\x8b\x8e\', \'\xe7\x8b\x90\', \'\xe7\x8b\x92\', \'\xe7\x8b\x97\', \'\xe7\x8b\x99\', \'\xe7\x8b\x9d\', \'\xe7\x8b\x9e\', \'\xe7\x8b\xa0\', \'\xe7\x8b\xa1\', \'\xe7\x8b\xa9\', \'\xe7\x8b\xac\', \'\xe7\x8b\xad\', \'\xe7\x8b\xae\', \'\xe7\x8b\xb0\', \'\xe7\x8b\xb1\', \'\xe7\x8b\xb2\', \'\xe7\x8b\xb8\',\n    \'\xe7\x8b\xbb\', \'\xe7\x8b\xbc\', \'\xe7\x8c\x83\', \'\xe7\x8c\x8a\', \'\xe7\x8c\x8e\', \'\xe7\x8c\x95\', \'\xe7\x8c\x96\', \'\xe7\x8c\x97\', \'\xe7\x8c\x9b\', \'\xe7\x8c\x9c\', \'\xe7\x8c\x9d\', \'\xe7\x8c\xa2\', \'\xe7\x8c\xa5\', \'\xe7\x8c\xa9\', \'\xe7\x8c\xaa\', \'\xe7\x8c\xab\', \'\xe7\x8c\xac\', \'\xe7\x8c\xae\', \'\xe7\x8c\xb1\', \'\xe7\x8c\xb4\',\n    \'\xe7\x8c\xb7\', \'\xe7\x8c\xbe\', \'\xe7\x8c\xbf\', \'\xe7\x8d\x90\', \'\xe7\x8d\x97\', \'\xe7\x8d\x98\', \'\xe7\x8d\xa0\', \'\xe7\x8d\xac\', \'\xe7\x8d\xad\', \'\xe7\x8d\xbe\', \'\xe7\x8e\x83\', \'\xe7\x8e\x84\', \'\xe7\x8e\x87\', \'\xe7\x8e\x89\', \'\xe7\x8e\x8a\', \'\xe7\x8e\x8b\', \'\xe7\x8e\x91\', \'\xe7\x8e\x95\', \'\xe7\x8e\x9b\', \'\xe7\x8e\xa6\',\n    \'\xe7\x8e\xa9\', \'\xe7\x8e\xab\', \'\xe7\x8e\xae\', \'\xe7\x8e\xaf\', \'\xe7\x8e\xb0\', \'\xe7\x8e\xb2\', \'\xe7\x8e\xb3\', \'\xe7\x8e\xb7\', \'\xe7\x8e\xba\', \'\xe7\x8e\xbb\', \'\xe7\x8f\x80\', \'\xe7\x8f\x82\', \'\xe7\x8f\x85\', \'\xe7\x8f\x88\', \'\xe7\x8f\x8a\', \'\xe7\x8f\x8d\', \'\xe7\x8f\x8f\', \'\xe7\x8f\x90\', \'\xe7\x8f\x91\', \'\xe7\x8f\x9e\',\n    \'\xe7\x8f\xa0\', \'\xe7\x8f\xa5\', \'\xe7\x8f\xa9\', \'\xe7\x8f\xaa\', \'\xe7\x8f\xad\', \'\xe7\x8f\xae\', \'\xe7\x8f\xb0\', \'\xe7\x90\x83\', \'\xe7\x90\x85\', \'\xe7\x90\x86\', \'\xe7\x90\x89\', \'\xe7\x90\x8f\', \'\xe7\x90\x90\', \'\xe7\x90\x96\', \'\xe7\x90\x9b\', \'\xe7\x90\xa2\', \'\xe7\x90\xa5\', \'\xe7\x90\xa6\', \'\xe7\x90\xa8\', \'\xe7\x90\xaa\',\n    \'\xe7\x90\xae\', \'\xe7\x90\xb0\', \'\xe7\x90\xb3\', \'\xe7\x90\xb4\', \'\xe7\x90\xb5\', \'\xe7\x90\xb6\', \'\xe7\x90\xbc\', \'\xe7\x91\x81\', \'\xe7\x91\x95\', \'\xe7\x91\x99\', \'\xe7\x91\x9a\', \'\xe7\x91\x9b\', \'\xe7\x91\x9c\', \'\xe7\x91\x9e\', \'\xe7\x91\x9f\', \'\xe7\x91\xb0\', \'\xe7\x91\xb6\', \'\xe7\x91\xbe\', \'\xe7\x92\x80\', \'\xe7\x92\x83\',\n    \'\xe7\x92\x86\', \'\xe7\x92\x87\', \'\xe7\x92\x8b\', \'\xe7\x92\x90\', \'\xe7\x92\x9c\', \'\xe7\x92\x9e\', \'\xe7\x92\x9f\', \'\xe7\x92\xa7\', \'\xe7\x92\xa8\', \'\xe7\x93\x92\', \'\xe7\x93\x9c\', \'\xe7\x93\xa0\', \'\xe7\x93\xa2\', \'\xe7\x93\xa3\', \'\xe7\x93\xa6\', \'\xe7\x93\xae\', \'\xe7\x93\xaf\', \'\xe7\x93\xb4\', \'\xe7\x93\xb6\', \'\xe7\x93\xb7\',\n    \'\xe7\x93\xbb\', \'\xe7\x94\x84\', \'\xe7\x94\x91\', \'\xe7\x94\x98\', \'\xe7\x94\x99\', \'\xe7\x94\x9a\', \'\xe7\x94\x9c\', \'\xe7\x94\x9f\', \'\xe7\x94\xa3\', \'\xe7\x94\xa5\', \'\xe7\x94\xa8\', \'\xe7\x94\xa9\', \'\xe7\x94\xab\', \'\xe7\x94\xac\', \'\xe7\x94\xad\', \'\xe7\x94\xb0\', \'\xe7\x94\xb1\', \'\xe7\x94\xb2\', \'\xe7\x94\xb3\', \'\xe7\x94\xb5\',\n    \'\xe7\x94\xb7\', \'\xe7\x94\xb8\', \'\xe7\x94\xba\', \'\xe7\x94\xbb\', \'\xe7\x94\xbe\', \'\xe7\x95\x80\', \'\xe7\x95\x85\', \'\xe7\x95\x8c\', \'\xe7\x95\x8e\', \'\xe7\x95\x8f\', \'\xe7\x95\x91\', \'\xe7\x95\x94\', \'\xe7\x95\x99\', \'\xe7\x95\x9c\', \'\xe7\x95\xa4\', \'\xe7\x95\xa5\', \'\xe7\x95\xa6\', \'\xe7\x95\xaa\', \'\xe7\x95\xb2\', \'\xe7\x95\xb4\',\n    \'\xe7\x95\xb8\', \'\xe7\x95\xbc\', \'\xe7\x95\xbf\', \'\xe7\x96\x83\', \'\xe7\x96\x86\', \'\xe7\x96\x8b\', \'\xe7\x96\x8f\', \'\xe7\x96\x91\', \'\xe7\x96\x97\', \'\xe7\x96\x99\', \'\xe7\x96\x9a\', \'\xe7\x96\x9d\', \'\xe7\x96\x9f\', \'\xe7\x96\xa1\', \'\xe7\x96\xa3\', \'\xe7\x96\xa4\', \'\xe7\x96\xa5\', \'\xe7\x96\xab\', \'\xe7\x96\xae\', \'\xe7\x96\xaf\',\n    \'\xe7\x96\xb1\', \'\xe7\x96\xb2\', \'\xe7\x96\xb4\', \'\xe7\x96\xb5\', \'\xe7\x96\xb8\', \'\xe7\x96\xb9\', \'\xe7\x96\xbc\', \'\xe7\x96\xbd\', \'\xe7\x96\xbe\', \'\xe7\x97\x82\', \'\xe7\x97\x85\', \'\xe7\x97\x87\', \'\xe7\x97\x88\', \'\xe7\x97\x89\', \'\xe7\x97\x8a\', \'\xe7\x97\x8d\', \'\xe7\x97\x92\', \'\xe7\x97\x94\', \'\xe7\x97\x95\', \'\xe7\x97\x98\',\n    \'\xe7\x97\x9b\', \'\xe7\x97\x9e\', \'\xe7\x97\xa2\', \'\xe7\x97\xa3\', \'\xe7\x97\xa4\', \'\xe7\x97\xa7\', \'\xe7\x97\xaa\', \'\xe7\x97\xab\', \'\xe7\x97\xb0\', \'\xe7\x97\xb4\', \'\xe7\x97\xb9\', \'\xe7\x97\xbc\', \'\xe7\x97\xbf\', \'\xe7\x98\x80\', \'\xe7\x98\x81\', \'\xe7\x98\x90\', \'\xe7\x98\x97\', \'\xe7\x98\x98\', \'\xe7\x98\x99\', \'\xe7\x98\x9f\',\n    \'\xe7\x98\xa0\', \'\xe7\x98\xa2\', \'\xe7\x98\xa4\', \'\xe7\x98\xa5\', \'\xe7\x98\xa6\', \'\xe7\x98\xa9\', \'\xe7\x98\xaa\', \'\xe7\x98\xab\', \'\xe7\x98\xb3\', \'\xe7\x98\xb4\', \'\xe7\x98\xb5\', \'\xe7\x98\xb8\', \'\xe7\x98\xbe\', \'\xe7\x99\x80\', \'\xe7\x99\x8c\', \'\xe7\x99\x94\', \'\xe7\x99\x96\', \'\xe7\x99\x9c\', \'\xe7\x99\x9e\', \'\xe7\x99\xa3\',\n    \'\xe7\x99\xab\', \'\xe7\x99\xb8\', \'\xe7\x99\xbb\', \'\xe7\x99\xbd\', \'\xe7\x99\xbe\', \'\xe7\x9a\x81\', \'\xe7\x9a\x82\', \'\xe7\x9a\x84\', \'\xe7\x9a\x86\', \'\xe7\x9a\x87\', \'\xe7\x9a\x88\', \'\xe7\x9a\x8b\', \'\xe7\x9a\x8e\', \'\xe7\x9a\x91\', \'\xe7\x9a\x93\', \'\xe7\x9a\x96\', \'\xe7\x9a\x99\', \'\xe7\x9a\xa4\', \'\xe7\x9a\xae\', \'\xe7\x9a\xb1\',\n    \'\xe7\x9a\xb4\', \'\xe7\x9a\xbf\', \'\xe7\x9b\x82\', \'\xe7\x9b\x85\', \'\xe7\x9b\x86\', \'\xe7\x9b\x88\', \'\xe7\x9b\x8a\', \'\xe7\x9b\x8d\', \'\xe7\x9b\x8e\', \'\xe7\x9b\x8f\', \'\xe7\x9b\x90\', \'\xe7\x9b\x91\', \'\xe7\x9b\x92\', \'\xe7\x9b\x94\', \'\xe7\x9b\x96\', \'\xe7\x9b\x97\', \'\xe7\x9b\x98\', \'\xe7\x9b\x9b\', \'\xe7\x9b\x9f\', \'\xe7\x9b\xa5\',\n    \'\xe7\x9b\xae\', \'\xe7\x9b\xaf\', \'\xe7\x9b\xb1\', \'\xe7\x9b\xb2\', \'\xe7\x9b\xb4\', \'\xe7\x9b\xb8\', \'\xe7\x9b\xb9\', \'\xe7\x9b\xbc\', \'\xe7\x9b\xbe\', \'\xe7\x9c\x81\', \'\xe7\x9c\x87\', \'\xe7\x9c\x88\', \'\xe7\x9c\x89\', \'\xe7\x9c\x8b\', \'\xe7\x9c\x99\', \'\xe7\x9c\x9b\', \'\xe7\x9c\x9f\', \'\xe7\x9c\xa0\', \'\xe7\x9c\xa2\', \'\xe7\x9c\xa6\',\n    \'\xe7\x9c\xa8\', \'\xe7\x9c\xa9\', \'\xe7\x9c\xaf\', \'\xe7\x9c\xb6\', \'\xe7\x9c\xb7\', \'\xe7\x9c\xb8\', \'\xe7\x9c\xba\', \'\xe7\x9c\xbc\', \'\xe7\x9d\x80\', \'\xe7\x9d\x81\', \'\xe7\x9d\x87\', \'\xe7\x9d\x90\', \'\xe7\x9d\x91\', \'\xe7\x9d\x92\', \'\xe7\x9d\x9a\', \'\xe7\x9d\x9b\', \'\xe7\x9d\xa1\', \'\xe7\x9d\xa2\', \'\xe7\x9d\xa3\', \'\xe7\x9d\xa6\',\n    \'\xe7\x9d\xa8\', \'\xe7\x9d\xaa\', \'\xe7\x9d\xab\', \'\xe7\x9d\xac\', \'\xe7\x9d\xb9\', \'\xe7\x9d\xbd\', \'\xe7\x9d\xbe\', \'\xe7\x9d\xbf\', \'\xe7\x9e\x80\', \'\xe7\x9e\x84\', \'\xe7\x9e\x85\', \'\xe7\x9e\x8b\', \'\xe7\x9e\x8c\', \'\xe7\x9e\x8e\', \'\xe7\x9e\x91\', \'\xe7\x9e\x92\', \'\xe7\x9e\x9f\', \'\xe7\x9e\xa0\', \'\xe7\x9e\xa5\', \'\xe7\x9e\xa7\',\n    \'\xe7\x9e\xa9\', \'\xe7\x9e\xaa\', \'\xe7\x9e\xac\', \'\xe7\x9e\xad\', \'\xe7\x9e\xb0\', \'\xe7\x9e\xb3\', \'\xe7\x9e\xbb\', \'\xe7\x9e\xbd\', \'\xe7\x9e\xbe\', \'\xe7\x9e\xbf\', \'\xe7\x9f\x97\', \'\xe7\x9f\x9b\', \'\xe7\x9f\x9c\', \'\xe7\x9f\xa2\', \'\xe7\x9f\xa3\', \'\xe7\x9f\xa5\', \'\xe7\x9f\xa9\', \'\xe7\x9f\xab\', \'\xe7\x9f\xad\', \'\xe7\x9f\xae\',\n    \'\xe7\x9f\xb3\', \'\xe7\x9f\xb6\', \'\xe7\x9f\xbd\', \'\xe7\x9f\xbe\', \'\xe7\x9f\xbf\', \'\xe7\xa0\x80\', \'\xe7\xa0\x81\', \'\xe7\xa0\x82\', \'\xe7\xa0\x8c\', \'\xe7\xa0\x8d\', \'\xe7\xa0\x92\', \'\xe7\xa0\x94\', \'\xe7\xa0\x96\', \'\xe7\xa0\x9a\', \'\xe7\xa0\x9d\', \'\xe7\xa0\xa3\', \'\xe7\xa0\xa5\', \'\xe7\xa0\xa7\', \'\xe7\xa0\xad\', \'\xe7\xa0\xb0\',\n    \'\xe7\xa0\xb4\', \'\xe7\xa0\xb7\', \'\xe7\xa0\xb8\', \'\xe7\xa0\xba\', \'\xe7\xa0\xbe\', \'\xe7\xa1\x80\', \'\xe7\xa1\x85\', \'\xe7\xa1\x92\', \'\xe7\xa1\x95\', \'\xe7\xa1\x9d\', \'\xe7\xa1\xab\', \'\xe7\xa1\xac\', \'\xe7\xa1\xae\', \'\xe7\xa1\xbc\', \'\xe7\xa2\x81\', \'\xe7\xa2\x89\', \'\xe7\xa2\x8c\', \'\xe7\xa2\x8d\', \'\xe7\xa2\x8e\', \'\xe7\xa2\x91\',\n    \'\xe7\xa2\x93\', \'\xe7\xa2\x97\', \'\xe7\xa2\x98\', \'\xe7\xa2\x9b\', \'\xe7\xa2\x9f\', \'\xe7\xa2\xa3\', \'\xe7\xa2\xa7\', \'\xe7\xa2\xb0\', \'\xe7\xa2\xb1\', \'\xe7\xa2\xb3\', \'\xe7\xa2\xb4\', \'\xe7\xa2\xbe\', \'\xe7\xa3\x81\', \'\xe7\xa3\x85\', \'\xe7\xa3\x8a\', \'\xe7\xa3\x8b\', \'\xe7\xa3\x90\', \'\xe7\xa3\x94\', \'\xe7\xa3\x95\', \'\xe7\xa3\x9b\',\n    \'\xe7\xa3\xa8\', \'\xe7\xa3\xac\', \'\xe7\xa3\xb7\', \'\xe7\xa3\xba\', \'\xe7\xa4\x81\', \'\xe7\xa4\xb4\', \'\xe7\xa4\xb6\', \'\xe7\xa4\xba\', \'\xe7\xa4\xbb\', \'\xe7\xa4\xbc\', \'\xe7\xa4\xbe\', \'\xe7\xa5\x80\', \'\xe7\xa5\x81\', \'\xe7\xa5\x87\', \'\xe7\xa5\x88\', \'\xe7\xa5\x89\', \'\xe7\xa5\x8a\', \'\xe7\xa5\x8e\', \'\xe7\xa5\x90\', \'\xe7\xa5\x93\',\n    \'\xe7\xa5\x96\', \'\xe7\xa5\x97\', \'\xe7\xa5\x9a\', \'\xe7\xa5\x9b\', \'\xe7\xa5\x9c\', \'\xe7\xa5\x9d\', \'\xe7\xa5\x9e\', \'\xe7\xa5\x9f\', \'\xe7\xa5\xa0\', \'\xe7\xa5\xa5\', \'\xe7\xa5\xa7\', \'\xe7\xa5\xa8\', \'\xe7\xa5\xad\', \'\xe7\xa5\xaf\', \'\xe7\xa5\xb7\', \'\xe7\xa5\xb8\', \'\xe7\xa5\xba\', \'\xe7\xa6\x80\', \'\xe7\xa6\x81\', \'\xe7\xa6\x84\',\n    \'\xe7\xa6\x85\', \'\xe7\xa6\x8f\', \'\xe7\xa6\xa7\', \'\xe7\xa6\xa8\', \'\xe7\xa6\xb3\', \'\xe7\xa6\xb9\', \'\xe7\xa6\xba\', \'\xe7\xa6\xbb\', \'\xe7\xa6\xbd\', \'\xe7\xa6\xbe\', \'\xe7\xa7\x80\', \'\xe7\xa7\x81\', \'\xe7\xa7\x83\', \'\xe7\xa7\x89\', \'\xe7\xa7\x8b\', \'\xe7\xa7\x8d\', \'\xe7\xa7\x8f\', \'\xe7\xa7\x91\', \'\xe7\xa7\x92\', \'\xe7\xa7\x98\',\n    \'\xe7\xa7\x9f\', \'\xe7\xa7\xa4\', \'\xe7\xa7\xa6\', \'\xe7\xa7\xa7\', \'\xe7\xa7\xa9\', \'\xe7\xa7\xaf\', \'\xe7\xa7\xb0\', \'\xe7\xa7\xb8\', \'\xe7\xa7\xbb\', \'\xe7\xa7\xbd\', \'\xe7\xa8\x80\', \'\xe7\xa8\x8b\', \'\xe7\xa8\x8d\', \'\xe7\xa8\x8e\', \'\xe7\xa8\x94\', \'\xe7\xa8\x97\', \'\xe7\xa8\x9a\', \'\xe7\xa8\x9e\', \'\xe7\xa8\xa0\', \'\xe7\xa8\xa3\',\n    \'\xe7\xa8\xb3\', \'\xe7\xa8\xb7\', \'\xe7\xa8\xbb\', \'\xe7\xa8\xbc\', \'\xe7\xa8\xbd\', \'\xe7\xa8\xbf\', \'\xe7\xa9\x86\', \'\xe7\xa9\x91\', \'\xe7\xa9\x97\', \'\xe7\xa9\xb0\', \'\xe7\xa9\xb4\', \'\xe7\xa9\xb6\', \'\xe7\xa9\xb7\', \'\xe7\xa9\xb9\', \'\xe7\xa9\xba\', \'\xe7\xa9\xbf\', \'\xe7\xaa\x81\', \'\xe7\xaa\x83\', \'\xe7\xaa\x84\', \'\xe7\xaa\x88\',\n    \'\xe7\xaa\x8b\', \'\xe7\xaa\x8d\', \'\xe7\xaa\x8e\', \'\xe7\xaa\x91\', \'\xe7\xaa\x92\', \'\xe7\xaa\x95\', \'\xe7\xaa\x96\', \'\xe7\xaa\x97\', \'\xe7\xaa\x98\', \'\xe7\xaa\x9c\', \'\xe7\xaa\x9d\', \'\xe7\xaa\x9f\', \'\xe7\xaa\xa0\', \'\xe7\xaa\xa3\', \'\xe7\xaa\xa5\', \'\xe7\xaa\xa6\', \'\xe7\xaa\xac\', \'\xe7\xaa\xad\', \'\xe7\xaa\xb3\', \'\xe7\xaa\xb4\',\n    \'\xe7\xaa\xbf\', \'\xe7\xab\x8b\', \'\xe7\xab\x96\', \'\xe7\xab\x99\', \'\xe7\xab\x9e\', \'\xe7\xab\x9f\', \'\xe7\xab\xa0\', \'\xe7\xab\xa3\', \'\xe7\xab\xa5\', \'\xe7\xab\xad\', \'\xe7\xab\xaf\', \'\xe7\xab\xb9\', \'\xe7\xab\xba\', \'\xe7\xab\xbd\', \'\xe7\xab\xbf\', \'\xe7\xac\x83\', \'\xe7\xac\x84\', \'\xe7\xac\x86\', \'\xe7\xac\x88\', \'\xe7\xac\x8b\',\n    \'\xe7\xac\x91\', \'\xe7\xac\x94\', \'\xe7\xac\x99\', \'\xe7\xac\x9b\', \'\xe7\xac\x9e\', \'\xe7\xac\xa0\', \'\xe7\xac\xa4\', \'\xe7\xac\xa5\', \'\xe7\xac\xa6\', \'\xe7\xac\xa8\', \'\xe7\xac\xab\', \'\xe7\xac\xac\', \'\xe7\xac\xae\', \'\xe7\xac\xba\', \'\xe7\xac\xbc\', \'\xe7\xad\x89\', \'\xe7\xad\x8b\', \'\xe7\xad\x8f\', \'\xe7\xad\x90\', \'\xe7\xad\x91\',\n    \'\xe7\xad\x92\', \'\xe7\xad\x94\', \'\xe7\xad\x96\', \'\xe7\xad\x9b\', \'\xe7\xad\x9d\', \'\xe7\xad\xa0\', \'\xe7\xad\xae\', \'\xe7\xad\xb0\', \'\xe7\xad\xb1\', \'\xe7\xad\xb4\', \'\xe7\xad\xb5\', \'\xe7\xad\xb7\', \'\xe7\xad\xb9\', \'\xe7\xad\xbe\', \'\xe7\xae\x80\', \'\xe7\xae\x8d\', \'\xe7\xae\x94\', \'\xe7\xae\x95\', \'\xe7\xae\x97\', \'\xe7\xae\xa1\',\n    \'\xe7\xae\xa6\', \'\xe7\xae\xa7\', \'\xe7\xae\xa9\', \'\xe7\xae\xab\', \'\xe7\xae\xad\', \'\xe7\xae\xb1\', \'\xe7\xae\xb4\', \'\xe7\xae\xb8\', \'\xe7\xaf\x81\', \'\xe7\xaf\x83\', \'\xe7\xaf\x86\', \'\xe7\xaf\x87\', \'\xe7\xaf\x91\', \'\xe7\xaf\x93\', \'\xe7\xaf\x99\', \'\xe7\xaf\x9d\', \'\xe7\xaf\xa1\', \'\xe7\xaf\xae\', \'\xe7\xaf\xb1\', \'\xe7\xaf\xb7\',\n    \'\xe7\xaf\xbe\', \'\xe7\xb0\x87\', \'\xe7\xb0\x8b\', \'\xe7\xb0\x8c\', \'\xe7\xb0\x8f\', \'\xe7\xb0\xa7\', \'\xe7\xb0\xaa\', \'\xe7\xb0\xb8\', \'\xe7\xb0\xbf\', \'\xe7\xb1\x81\', \'\xe7\xb1\x8d\', \'\xe7\xb1\xb3\', \'\xe7\xb1\xbb\', \'\xe7\xb1\xbd\', \'\xe7\xb2\x89\', \'\xe7\xb2\x91\', \'\xe7\xb2\x92\', \'\xe7\xb2\x95\', \'\xe7\xb2\x97\', \'\xe7\xb2\x98\',\n    \'\xe7\xb2\x9c\', \'\xe7\xb2\x9d\', \'\xe7\xb2\x9f\', \'\xe7\xb2\xa4\', \'\xe7\xb2\xa5\', \'\xe7\xb2\xaa\', \'\xe7\xb2\xae\', \'\xe7\xb2\xb1\', \'\xe7\xb2\xb2\', \'\xe7\xb2\xb3\', \'\xe7\xb2\xb9\', \'\xe7\xb2\xbc\', \'\xe7\xb2\xbd\', \'\xe7\xb2\xbe\', \'\xe7\xb3\x81\', \'\xe7\xb3\x85\', \'\xe7\xb3\x8a\', \'\xe7\xb3\x8c\', \'\xe7\xb3\x8d\', \'\xe7\xb3\x95\',\n    \'\xe7\xb3\x96\', \'\xe7\xb3\x97\', \'\xe7\xb3\x99\', \'\xe7\xb3\x9c\', \'\xe7\xb3\x9f\', \'\xe7\xb3\xa0\', \'\xe7\xb3\xaf\', \'\xe7\xb3\xbb\', \'\xe7\xb4\x8a\', \'\xe7\xb4\xa0\', \'\xe7\xb4\xa2\', \'\xe7\xb4\xa7\', \'\xe7\xb4\xab\', \'\xe7\xb4\xac\', \'\xe7\xb4\xaf\', \'\xe7\xb5\x9c\', \'\xe7\xb5\xae\', \'\xe7\xb5\xb7\', \'\xe7\xb6\xa6\', \'\xe7\xb7\x83\',\n    \'\xe7\xb8\xa0\', \'\xe7\xb8\xa2\', \'\xe7\xb8\xaf\', \'\xe7\xb8\xbb\', \'\xe7\xb9\x81\', \'\xe7\xb9\x87\', \'\xe7\xb9\xbb\', \'\xe7\xba\x82\', \'\xe7\xba\x94\', \'\xe7\xba\xa0\', \'\xe7\xba\xa1\', \'\xe7\xba\xa2\', \'\xe7\xba\xa3\', \'\xe7\xba\xa4\', \'\xe7\xba\xa5\', \'\xe7\xba\xa6\', \'\xe7\xba\xa7\', \'\xe7\xba\xa8\', \'\xe7\xba\xaa\', \'\xe7\xba\xab\',\n    \'\xe7\xba\xac\', \'\xe7\xba\xad\', \'\xe7\xba\xae\', \'\xe7\xba\xaf\', \'\xe7\xba\xb0\', \'\xe7\xba\xb1\', \'\xe7\xba\xb2\', \'\xe7\xba\xb3\', \'\xe7\xba\xb5\', \'\xe7\xba\xb6\', \'\xe7\xba\xb7\', \'\xe7\xba\xb8\', \'\xe7\xba\xb9\', \'\xe7\xba\xba\', \'\xe7\xba\xbb\', \'\xe7\xba\xbd\', \'\xe7\xba\xbe\', \'\xe7\xba\xbf\', \'\xe7\xbb\x80\', \'\xe7\xbb\x81\',\n    \'\xe7\xbb\x83\', \'\xe7\xbb\x84\', \'\xe7\xbb\x85\', \'\xe7\xbb\x86\', \'\xe7\xbb\x87\', \'\xe7\xbb\x88\', \'\xe7\xbb\x8a\', \'\xe7\xbb\x8c\', \'\xe7\xbb\x8d\', \'\xe7\xbb\x8e\', \'\xe7\xbb\x8f\', \'\xe7\xbb\x90\', \'\xe7\xbb\x91\', \'\xe7\xbb\x92\', \'\xe7\xbb\x93\', \'\xe7\xbb\x94\', \'\xe7\xbb\x95\', \'\xe7\xbb\x96\', \'\xe7\xbb\x98\', \'\xe7\xbb\x99\',\n    \'\xe7\xbb\x9a\', \'\xe7\xbb\x9b\', \'\xe7\xbb\x9c\', \'\xe7\xbb\x9d\', \'\xe7\xbb\x9e\', \'\xe7\xbb\x9f\', \'\xe7\xbb\xa0\', \'\xe7\xbb\xa1\', \'\xe7\xbb\xa2\', \'\xe7\xbb\xa3\', \'\xe7\xbb\xa5\', \'\xe7\xbb\xa6\', \'\xe7\xbb\xa7\', \'\xe7\xbb\xa8\', \'\xe7\xbb\xa9\', \'\xe7\xbb\xaa\', \'\xe7\xbb\xab\', \'\xe7\xbb\xad\', \'\xe7\xbb\xae\', \'\xe7\xbb\xaf\',\n    \'\xe7\xbb\xb0\', \'\xe7\xbb\xb3\', \'\xe7\xbb\xb4\', \'\xe7\xbb\xb5\', \'\xe7\xbb\xb6\', \'\xe7\xbb\xb7\', \'\xe7\xbb\xb8\', \'\xe7\xbb\xba\', \'\xe7\xbb\xbb\', \'\xe7\xbb\xbc\', \'\xe7\xbb\xbd\', \'\xe7\xbb\xbe\', \'\xe7\xbb\xbf\', \'\xe7\xbc\x80\', \'\xe7\xbc\x81\', \'\xe7\xbc\x84\', \'\xe7\xbc\x85\', \'\xe7\xbc\x86\', \'\xe7\xbc\x87\', \'\xe7\xbc\x88\',\n    \'\xe7\xbc\x89\', \'\xe7\xbc\x8e\', \'\xe7\xbc\x91\', \'\xe7\xbc\x92\', \'\xe7\xbc\x93\', \'\xe7\xbc\x94\', \'\xe7\xbc\x95\', \'\xe7\xbc\x96\', \'\xe7\xbc\x97\', \'\xe7\xbc\x98\', \'\xe7\xbc\x99\', \'\xe7\xbc\x9a\', \'\xe7\xbc\x9c\', \'\xe7\xbc\x9d\', \'\xe7\xbc\x9e\', \'\xe7\xbc\x9f\', \'\xe7\xbc\xa0\', \'\xe7\xbc\xa2\', \'\xe7\xbc\xa4\', \'\xe7\xbc\xa5\',\n    \'\xe7\xbc\xa7\', \'\xe7\xbc\xa8\', \'\xe7\xbc\xa9\', \'\xe7\xbc\xaa\', \'\xe7\xbc\xad\', \'\xe7\xbc\xae\', \'\xe7\xbc\xaf\', \'\xe7\xbc\xb0\', \'\xe7\xbc\xb1\', \'\xe7\xbc\xb3\', \'\xe7\xbc\xb4\', \'\xe7\xbc\xb5\', \'\xe7\xbc\xb6\', \'\xe7\xbc\xb8\', \'\xe7\xbc\xba\', \'\xe7\xbd\x82\', \'\xe7\xbd\x83\', \'\xe7\xbd\x84\', \'\xe7\xbd\x85\', \'\xe7\xbd\x90\',\n    \'\xe7\xbd\x91\', \'\xe7\xbd\x94\', \'\xe7\xbd\x95\', \'\xe7\xbd\x97\', \'\xe7\xbd\x98\', \'\xe7\xbd\x9a\', \'\xe7\xbd\xa1\', \'\xe7\xbd\xa2\', \'\xe7\xbd\xa5\', \'\xe7\xbd\xa9\', \'\xe7\xbd\xaa\', \'\xe7\xbd\xae\', \'\xe7\xbd\xb2\', \'\xe7\xbd\xb4\', \'\xe7\xbd\xb9\', \'\xe7\xbe\x81\', \'\xe7\xbe\x8a\', \'\xe7\xbe\x8c\', \'\xe7\xbe\x8e\', \'\xe7\xbe\x91\',\n    \'\xe7\xbe\x94\', \'\xe7\xbe\x96\', \'\xe7\xbe\x9a\', \'\xe7\xbe\x9e\', \'\xe7\xbe\x9f\', \'\xe7\xbe\xa1\', \'\xe7\xbe\xa4\', \'\xe7\xbe\xaf\', \'\xe7\xbe\xb2\', \'\xe7\xbe\xb8\', \'\xe7\xbe\xb9\', \'\xe7\xbe\xbd\', \'\xe7\xbe\xbf\', \'\xe7\xbf\x81\', \'\xe7\xbf\x85\', \'\xe7\xbf\x8a\', \'\xe7\xbf\x8c\', \'\xe7\xbf\x8e\', \'\xe7\xbf\x94\', \'\xe7\xbf\x95\',\n    \'\xe7\xbf\x98\', \'\xe7\xbf\x9f\', \'\xe7\xbf\xa0\', \'\xe7\xbf\xa1\', \'\xe7\xbf\xa6\', \'\xe7\xbf\xa9\', \'\xe7\xbf\xae\', \'\xe7\xbf\xb0\', \'\xe7\xbf\xb1\', \'\xe7\xbf\xb3\', \'\xe7\xbf\xbb\', \'\xe7\xbf\xbc\', \'\xe8\x80\x80\', \'\xe8\x80\x81\', \'\xe8\x80\x83\', \'\xe8\x80\x84\', \'\xe8\x80\x85\', \'\xe8\x80\x86\', \'\xe8\x80\x8b\', \'\xe8\x80\x8c\',\n    \'\xe8\x80\x8d\', \'\xe8\x80\x8e\', \'\xe8\x80\x90\', \'\xe8\x80\x95\', \'\xe8\x80\x97\', \'\xe8\x80\x98\', \'\xe8\x80\x99\', \'\xe8\x80\x9c\', \'\xe8\x80\xa6\', \'\xe8\x80\xa8\', \'\xe8\x80\xb3\', \'\xe8\x80\xb6\', \'\xe8\x80\xb8\', \'\xe8\x80\xbb\', \'\xe8\x80\xbd\', \'\xe8\x80\xbf\', \'\xe8\x81\x82\', \'\xe8\x81\x86\', \'\xe8\x81\x8a\', \'\xe8\x81\x8b\',\n    \'\xe8\x81\x8c\', \'\xe8\x81\x92\', \'\xe8\x81\x94\', \'\xe8\x81\x98\', \'\xe8\x81\x9a\', \'\xe8\x81\xa9\', \'\xe8\x81\xaa\', \'\xe8\x82\x83\', \'\xe8\x82\x84\', \'\xe8\x82\x86\', \'\xe8\x82\x87\', \'\xe8\x82\x89\', \'\xe8\x82\x8b\', \'\xe8\x82\x8c\', \'\xe8\x82\x93\', \'\xe8\x82\x96\', \'\xe8\x82\x98\', \'\xe8\x82\x9a\', \'\xe8\x82\x9b\', \'\xe8\x82\x9d\',\n    \'\xe8\x82\xa0\', \'\xe8\x82\xa1\', \'\xe8\x82\xa2\', \'\xe8\x82\xa3\', \'\xe8\x82\xa4\', \'\xe8\x82\xa5\', \'\xe8\x82\xa9\', \'\xe8\x82\xaa\', \'\xe8\x82\xae\', \'\xe8\x82\xaf\', \'\xe8\x82\xb1\', \'\xe8\x82\xb2\', \'\xe8\x82\xb4\', \'\xe8\x82\xb8\', \'\xe8\x82\xba\', \'\xe8\x82\xbd\', \'\xe8\x82\xbe\', \'\xe8\x82\xbf\', \'\xe8\x83\x80\', \'\xe8\x83\x81\',\n    \'\xe8\x83\x83\', \'\xe8\x83\x84\', \'\xe8\x83\x86\', \'\xe8\x83\x8c\', \'\xe8\x83\x8e\', \'\xe8\x83\x96\', \'\xe8\x83\x99\', \'\xe8\x83\x9a\', \'\xe8\x83\x9c\', \'\xe8\x83\x9d\', \'\xe8\x83\x9e\', \'\xe8\x83\xa1\', \'\xe8\x83\xa4\', \'\xe8\x83\xa5\', \'\xe8\x83\xa7\', \'\xe8\x83\xaa\', \'\xe8\x83\xab\', \'\xe8\x83\xad\', \'\xe8\x83\xaf\', \'\xe8\x83\xb0\',\n    \'\xe8\x83\xb1\', \'\xe8\x83\xb3\', \'\xe8\x83\xb6\', \'\xe8\x83\xb8\', \'\xe8\x83\xba\', \'\xe8\x83\xbd\', \'\xe8\x83\xbe\', \'\xe8\x84\x81\', \'\xe8\x84\x82\', \'\xe8\x84\x86\', \'\xe8\x84\x89\', \'\xe8\x84\x8a\', \'\xe8\x84\x8d\', \'\xe8\x84\x8f\', \'\xe8\x84\x90\', \'\xe8\x84\x91\', \'\xe8\x84\x93\', \'\xe8\x84\x94\', \'\xe8\x84\x96\', \'\xe8\x84\x9a\',\n    \'\xe8\x84\xaf\', \'\xe8\x84\xb1\', \'\xe8\x84\xb2\', \'\xe8\x84\xb8\', \'\xe8\x84\xbe\', \'\xe8\x85\x86\', \'\xe8\x85\x88\', \'\xe8\x85\x89\', \'\xe8\x85\x8a\', \'\xe8\x85\x8b\', \'\xe8\x85\x8c\', \'\xe8\x85\x90\', \'\xe8\x85\x91\', \'\xe8\x85\x93\', \'\xe8\x85\x94\', \'\xe8\x85\x95\', \'\xe8\x85\xa5\', \'\xe8\x85\xa9\', \'\xe8\x85\xad\', \'\xe8\x85\xae\',\n    \'\xe8\x85\xb0\', \'\xe8\x85\xb1\', \'\xe8\x85\xb4\', \'\xe8\x85\xb9\', \'\xe8\x85\xba\', \'\xe8\x85\xbb\', \'\xe8\x85\xbc\', \'\xe8\x85\xbe\', \'\xe8\x85\xbf\', \'\xe8\x86\x80\', \'\xe8\x86\x88\', \'\xe8\x86\x8a\', \'\xe8\x86\x8f\', \'\xe8\x86\x91\', \'\xe8\x86\x98\', \'\xe8\x86\x9b\', \'\xe8\x86\x9c\', \'\xe8\x86\x9d\', \'\xe8\x86\xa6\', \'\xe8\x86\xa8\',\n    \'\xe8\x86\xb3\', \'\xe8\x86\xba\', \'\xe8\x86\xbb\', \'\xe8\x87\x80\', \'\xe8\x87\x82\', \'\xe8\x87\x83\', \'\xe8\x87\x86\', \'\xe8\x87\x8a\', \'\xe8\x87\x9b\', \'\xe8\x87\xa3\', \'\xe8\x87\xa7\', \'\xe8\x87\xaa\', \'\xe8\x87\xac\', \'\xe8\x87\xad\', \'\xe8\x87\xb1\', \'\xe8\x87\xb3\', \'\xe8\x87\xb4\', \'\xe8\x87\xbb\', \'\xe8\x87\xbc\', \'\xe8\x87\xbe\',\n    \'\xe8\x88\x80\', \'\xe8\x88\x81\', \'\xe8\x88\x82\', \'\xe8\x88\x84\', \'\xe8\x88\x85\', \'\xe8\x88\x86\', \'\xe8\x88\x8c\', \'\xe8\x88\x8d\', \'\xe8\x88\x90\', \'\xe8\x88\x92\', \'\xe8\x88\x94\', \'\xe8\x88\x9b\', \'\xe8\x88\x9c\', \'\xe8\x88\x9e\', \'\xe8\x88\x9f\', \'\xe8\x88\xa1\', \'\xe8\x88\xaa\', \'\xe8\x88\xab\', \'\xe8\x88\xac\', \'\xe8\x88\xb0\',\n    \'\xe8\x88\xb1\', \'\xe8\x88\xb5\', \'\xe8\x88\xb6\', \'\xe8\x88\xb7\', \'\xe8\x88\xb9\', \'\xe8\x89\x87\', \'\xe8\x89\x98\', \'\xe8\x89\xae\', \'\xe8\x89\xaf\', \'\xe8\x89\xb0\', \'\xe8\x89\xb2\', \'\xe8\x89\xb3\', \'\xe8\x89\xba\', \'\xe8\x89\xbe\', \'\xe8\x8a\x82\', \'\xe8\x8a\x88\', \'\xe8\x8a\x8b\', \'\xe8\x8a\x8d\', \'\xe8\x8a\x8e\', \'\xe8\x8a\x92\',\n    \'\xe8\x8a\x98\', \'\xe8\x8a\x99\', \'\xe8\x8a\x9c\', \'\xe8\x8a\x9d\', \'\xe8\x8a\xa1\', \'\xe8\x8a\xa5\', \'\xe8\x8a\xa6\', \'\xe8\x8a\xaa\', \'\xe8\x8a\xab\', \'\xe8\x8a\xac\', \'\xe8\x8a\xad\', \'\xe8\x8a\xae\', \'\xe8\x8a\xaf\', \'\xe8\x8a\xb1\', \'\xe8\x8a\xb3\', \'\xe8\x8a\xb7\', \'\xe8\x8a\xb8\', \'\xe8\x8a\xb9\', \'\xe8\x8a\xbd\', \'\xe8\x8b\x84\',\n    \'\xe8\x8b\x87\', \'\xe8\x8b\x8b\', \'\xe8\x8b\x8c\', \'\xe8\x8b\x8d\', \'\xe8\x8b\x8e\', \'\xe8\x8b\x8f\', \'\xe8\x8b\x91\', \'\xe8\x8b\x92\', \'\xe8\x8b\x93\', \'\xe8\x8b\x94\', \'\xe8\x8b\x95\', \'\xe8\x8b\x97\', \'\xe8\x8b\x9b\', \'\xe8\x8b\x9e\', \'\xe8\x8b\x9f\', \'\xe8\x8b\xa1\', \'\xe8\x8b\xa3\', \'\xe8\x8b\xa5\', \'\xe8\x8b\xa6\', \'\xe8\x8b\xab\',\n    \'\xe8\x8b\xaf\', \'\xe8\x8b\xb1\', \'\xe8\x8b\xb4\', \'\xe8\x8b\xb7\', \'\xe8\x8b\xb9\', \'\xe8\x8b\xbb\', \'\xe8\x8c\x80\', \'\xe8\x8c\x81\', \'\xe8\x8c\x82\', \'\xe8\x8c\x83\', \'\xe8\x8c\x84\', \'\xe8\x8c\x85\', \'\xe8\x8c\x86\', \'\xe8\x8c\x89\', \'\xe8\x8c\x8e\', \'\xe8\x8c\x8f\', \'\xe8\x8c\x94\', \'\xe8\x8c\x95\', \'\xe8\x8c\x97\', \'\xe8\x8c\x9c\',\n    \'\xe8\x8c\xa7\', \'\xe8\x8c\xa8\', \'\xe8\x8c\xab\', \'\xe8\x8c\xac\', \'\xe8\x8c\xad\', \'\xe8\x8c\xaf\', \'\xe8\x8c\xb1\', \'\xe8\x8c\xb4\', \'\xe8\x8c\xb5\', \'\xe8\x8c\xb6\', \'\xe8\x8c\xb8\', \'\xe8\x8c\xb9\', \'\xe8\x8d\x80\', \'\xe8\x8d\x83\', \'\xe8\x8d\x86\', \'\xe8\x8d\x89\', \'\xe8\x8d\x8f\', \'\xe8\x8d\x90\', \'\xe8\x8d\x92\', \'\xe8\x8d\x94\',\n    \'\xe8\x8d\x9a\', \'\xe8\x8d\x9c\', \'\xe8\x8d\x9e\', \'\xe8\x8d\x9f\', \'\xe8\x8d\xa0\', \'\xe8\x8d\xa1\', \'\xe8\x8d\xa3\', \'\xe8\x8d\xa4\', \'\xe8\x8d\xa5\', \'\xe8\x8d\xa6\', \'\xe8\x8d\xa7\', \'\xe8\x8d\xaa\', \'\xe8\x8d\xab\', \'\xe8\x8d\xaf\', \'\xe8\x8d\xb7\', \'\xe8\x8d\xbb\', \'\xe8\x8d\xbc\', \'\xe8\x8e\x85\', \'\xe8\x8e\x86\', \'\xe8\x8e\x89\',\n    \'\xe8\x8e\x8e\', \'\xe8\x8e\x92\', \'\xe8\x8e\x93\', \'\xe8\x8e\x98\', \'\xe8\x8e\x9c\', \'\xe8\x8e\x9e\', \'\xe8\x8e\xa0\', \'\xe8\x8e\xa8\', \'\xe8\x8e\xa9\', \'\xe8\x8e\xab\', \'\xe8\x8e\xb1\', \'\xe8\x8e\xb2\', \'\xe8\x8e\xb4\', \'\xe8\x8e\xb7\', \'\xe8\x8e\xb9\', \'\xe8\x8e\xba\', \'\xe8\x8e\xbd\', \'\xe8\x8f\x81\', \'\xe8\x8f\x85\', \'\xe8\x8f\x87\',\n    \'\xe8\x8f\x8a\', \'\xe8\x8f\x8c\', \'\xe8\x8f\x8f\', \'\xe8\x8f\x91\', \'\xe8\x8f\x9c\', \'\xe8\x8f\x9f\', \'\xe8\x8f\xa0\', \'\xe8\x8f\xa1\', \'\xe8\x8f\xa9\', \'\xe8\x8f\xb1\', \'\xe8\x8f\xb2\', \'\xe8\x8f\xbd\', \'\xe8\x90\x83\', \'\xe8\x90\x84\', \'\xe8\x90\x8c\', \'\xe8\x90\x8d\', \'\xe8\x90\x8e\', \'\xe8\x90\x9d\', \'\xe8\x90\xa4\', \'\xe8\x90\xa5\',\n    \'\xe8\x90\xa6\', \'\xe8\x90\xa7\', \'\xe8\x90\xa8\', \'\xe8\x90\xb8\', \'\xe8\x90\xbd\', \'\xe8\x91\x86\', \'\xe8\x91\x97\', \'\xe8\x91\x9b\', \'\xe8\x91\xa1\', \'\xe8\x91\xa3\', \'\xe8\x91\xa9\', \'\xe8\x91\xab\', \'\xe8\x91\xac\', \'\xe8\x91\xad\', \'\xe8\x91\xb1\', \'\xe8\x91\xb3\', \'\xe8\x91\xb4\', \'\xe8\x91\xb5\', \'\xe8\x91\xba\', \'\xe8\x92\x82\',\n    \'\xe8\x92\x89\', \'\xe8\x92\x8b\', \'\xe8\x92\x8d\', \'\xe8\x92\x97\', \'\xe8\x92\x99\', \'\xe8\x92\x9c\', \'\xe8\x92\xaf\', \'\xe8\x92\xb2\', \'\xe8\x92\xb8\', \'\xe8\x92\xba\', \'\xe8\x92\xbd\', \'\xe8\x92\xbf\', \'\xe8\x93\x84\', \'\xe8\x93\x89\', \'\xe8\x93\x8d\', \'\xe8\x93\x90\', \'\xe8\x93\x93\', \'\xe8\x93\x96\', \'\xe8\x93\x9d\', \'\xe8\x93\x9f\',\n    \'\xe8\x93\xa6\', \'\xe8\x93\xac\', \'\xe8\x93\xba\', \'\xe8\x93\xbc\', \'\xe8\x93\xbf\', \'\xe8\x94\x91\', \'\xe8\x94\x93\', \'\xe8\x94\x97\', \'\xe8\x94\x9a\', \'\xe8\x94\x9f\', \'\xe8\x94\xa1\', \'\xe8\x94\xab\', \'\xe8\x94\xac\', \'\xe8\x94\xb7\', \'\xe8\x94\xba\', \'\xe8\x94\xbc\', \'\xe8\x94\xbd\', \'\xe8\x95\x83\', \'\xe8\x95\x89\', \'\xe8\x95\x8a\',\n    \'\xe8\x95\x99\', \'\xe8\x95\xa4\', \'\xe8\x95\xa8\', \'\xe8\x95\xb2\', \'\xe8\x95\xb4\', \'\xe8\x95\xbe\', \'\xe8\x96\x84\', \'\xe8\x96\x87\', \'\xe8\x96\x8f\', \'\xe8\x96\x9b\', \'\xe8\x96\x9c\', \'\xe8\x96\xa4\', \'\xe8\x96\xa8\', \'\xe8\x96\xaa\', \'\xe8\x96\xae\', \'\xe8\x96\xaf\', \'\xe8\x96\xb0\', \'\xe8\x97\x81\', \'\xe8\x97\x89\', \'\xe8\x97\x8f\',\n    \'\xe8\x97\x90\', \'\xe8\x97\x93\', \'\xe8\x97\x95\', \'\xe8\x97\x9c\', \'\xe8\x97\xa4\', \'\xe8\x97\xa9\', \'\xe8\x97\xbb\', \'\xe8\x97\xbf\', \'\xe8\x98\x91\', \'\xe8\x98\xa7\', \'\xe8\x98\xb8\', \'\xe8\x99\x8e\', \'\xe8\x99\x8f\', \'\xe8\x99\x90\', \'\xe8\x99\x91\', \'\xe8\x99\x94\', \'\xe8\x99\x9a\', \'\xe8\x99\x9e\', \'\xe8\x99\xa2\', \'\xe8\x99\xab\',\n    \'\xe8\x99\xac\', \'\xe8\x99\xae\', \'\xe8\x99\xb1\', \'\xe8\x99\xb9\', \'\xe8\x99\xba\', \'\xe8\x99\xbd\', \'\xe8\x99\xbe\', \'\xe8\x9a\x80\', \'\xe8\x9a\x81\', \'\xe8\x9a\x82\', \'\xe8\x9a\x8a\', \'\xe8\x9a\x8c\', \'\xe8\x9a\x93\', \'\xe8\x9a\x95\', \'\xe8\x9a\x9d\', \'\xe8\x9a\xa1\', \'\xe8\x9a\xa3\', \'\xe8\x9a\xa4\', \'\xe8\x9a\xa9\', \'\xe8\x9a\xaf\',\n    \'\xe8\x9a\xb0\', \'\xe8\x9b\x80\', \'\xe8\x9b\x86\', \'\xe8\x9b\x87\', \'\xe8\x9b\x8a\', \'\xe8\x9b\x8b\', \'\xe8\x9b\x8e\', \'\xe8\x9b\x90\', \'\xe8\x9b\x94\', \'\xe8\x9b\x99\', \'\xe8\x9b\x9b\', \'\xe8\x9b\x9f\', \'\xe8\x9b\xa4\', \'\xe8\x9b\xa9\', \'\xe8\x9b\xad\', \'\xe8\x9b\xae\', \'\xe8\x9b\xb0\', \'\xe8\x9b\xb2\', \'\xe8\x9b\xb9\', \'\xe8\x9b\xbe\',\n    \'\xe8\x9c\x80\', \'\xe8\x9c\x82\', \'\xe8\x9c\x83\', \'\xe8\x9c\x87\', \'\xe8\x9c\x88\', \'\xe8\x9c\x8d\', \'\xe8\x9c\x92\', \'\xe8\x9c\x93\', \'\xe8\x9c\x94\', \'\xe8\x9c\x95\', \'\xe8\x9c\x97\', \'\xe8\x9c\x98\', \'\xe8\x9c\x9a\', \'\xe8\x9c\x9c\', \'\xe8\x9c\xa1\', \'\xe8\x9c\xa5\', \'\xe8\x9c\xb4\', \'\xe8\x9c\xb7\', \'\xe8\x9c\xbb\', \'\xe8\x9c\xbf\',\n    \'\xe8\x9d\x84\', \'\xe8\x9d\x87\', \'\xe8\x9d\x89\', \'\xe8\x9d\x8e\', \'\xe8\x9d\x97\', \'\xe8\x9d\x99\', \'\xe8\x9d\xa0\', \'\xe8\x9d\xae\', \'\xe8\x9d\xb4\', \'\xe8\x9d\xb6\', \'\xe8\x9d\xbc\', \'\xe8\x9e\x82\', \'\xe8\x9e\x83\', \'\xe8\x9e\x8d\', \'\xe8\x9e\xa8\', \'\xe8\x9e\xab\', \'\xe8\x9e\xad\', \'\xe8\x9e\xb3\', \'\xe8\x9e\xba\', \'\xe8\x9e\xbe\',\n    \'\xe8\x9f\x86\', \'\xe8\x9f\x8b\', \'\xe8\x9f\x92\', \'\xe8\x9f\x9c\', \'\xe8\x9f\xa0\', \'\xe8\x9f\xad\', \'\xe8\x9f\xb9\', \'\xe8\x9f\xbe\', \'\xe8\xa0\x95\', \'\xe8\xa0\xa1\', \'\xe8\xa0\xa2\', \'\xe8\xa0\xb9\', \'\xe8\xa1\x80\', \'\xe8\xa1\x85\', \'\xe8\xa1\x8c\', \'\xe8\xa1\x8d\', \'\xe8\xa1\x94\', \'\xe8\xa1\x97\', \'\xe8\xa1\x99\', \'\xe8\xa1\xa1\',\n    \'\xe8\xa1\xa2\', \'\xe8\xa1\xa3\', \'\xe8\xa1\xa5\', \'\xe8\xa1\xa8\', \'\xe8\xa1\xa9\', \'\xe8\xa1\xab\', \'\xe8\xa1\xac\', \'\xe8\xa1\xae\', \'\xe8\xa1\xb0\', \'\xe8\xa1\xb2\', \'\xe8\xa1\xb7\', \'\xe8\xa1\xbd\', \'\xe8\xa1\xbe\', \'\xe8\xa1\xbf\', \'\xe8\xa2\x81\', \'\xe8\xa2\x82\', \'\xe8\xa2\x84\', \'\xe8\xa2\x85\', \'\xe8\xa2\x88\', \'\xe8\xa2\x8b\',\n    \'\xe8\xa2\x8d\', \'\xe8\xa2\x92\', \'\xe8\xa2\x96\', \'\xe8\xa2\x9c\', \'\xe8\xa2\xa4\', \'\xe8\xa2\xab\', \'\xe8\xa2\xad\', \'\xe8\xa2\xb1\', \'\xe8\xa2\xb4\', \'\xe8\xa3\x80\', \'\xe8\xa3\x81\', \'\xe8\xa3\x82\', \'\xe8\xa3\x85\', \'\xe8\xa3\x86\', \'\xe8\xa3\x94\', \'\xe8\xa3\x95\', \'\xe8\xa3\x98\', \'\xe8\xa3\x99\', \'\xe8\xa3\x9f\', \'\xe8\xa3\xa4\',\n    \'\xe8\xa3\xa8\', \'\xe8\xa3\xb0\', \'\xe8\xa3\xb1\', \'\xe8\xa3\xb3\', \'\xe8\xa3\xb4\', \'\xe8\xa3\xb8\', \'\xe8\xa3\xb9\', \'\xe8\xa3\xbe\', \'\xe8\xa4\x82\', \'\xe8\xa4\x8a\', \'\xe8\xa4\x90\', \'\xe8\xa4\x92\', \'\xe8\xa4\x93\', \'\xe8\xa4\x9a\', \'\xe8\xa4\x9b\', \'\xe8\xa4\xa5\', \'\xe8\xa4\xaa\', \'\xe8\xa4\xab\', \'\xe8\xa4\xb4\', \'\xe8\xa4\xb6\',\n    \'\xe8\xa5\x81\', \'\xe8\xa5\x84\', \'\xe8\xa5\x86\', \'\xe8\xa5\x9c\', \'\xe8\xa5\x9f\', \'\xe8\xa5\xa6\', \'\xe8\xa5\xbf\', \'\xe8\xa6\x81\', \'\xe8\xa6\x83\', \'\xe8\xa6\x86\', \'\xe8\xa7\x81\', \'\xe8\xa7\x82\', \'\xe8\xa7\x84\', \'\xe8\xa7\x85\', \'\xe8\xa7\x86\', \'\xe8\xa7\x87\', \'\xe8\xa7\x88\', \'\xe8\xa7\x89\', \'\xe8\xa7\x8a\', \'\xe8\xa7\x8c\',\n    \'\xe8\xa7\x8e\', \'\xe8\xa7\x90\', \'\xe8\xa7\x91\', \'\xe8\xa7\x92\', \'\xe8\xa7\x96\', \'\xe8\xa7\x9a\', \'\xe8\xa7\x9c\', \'\xe8\xa7\x9e\', \'\xe8\xa7\xa3\', \'\xe8\xa7\xa5\', \'\xe8\xa7\xa6\', \'\xe8\xa7\xb3\', \'\xe8\xa7\xbd\', \'\xe8\xa8\x80\', \'\xe8\xa8\x87\', \'\xe8\xa8\xbe\', \'\xe8\xa9\x88\', \'\xe8\xa9\xb9\', \'\xe8\xaa\x89\', \'\xe8\xaa\x93\',\n    \'\xe8\xad\x84\', \'\xe8\xad\xa6\', \'\xe8\xad\xac\', \'\xe8\xae\x99\', \'\xe8\xae\xa1\', \'\xe8\xae\xa2\', \'\xe8\xae\xa3\', \'\xe8\xae\xa4\', \'\xe8\xae\xa5\', \'\xe8\xae\xa6\', \'\xe8\xae\xa7\', \'\xe8\xae\xa8\', \'\xe8\xae\xa9\', \'\xe8\xae\xaa\', \'\xe8\xae\xab\', \'\xe8\xae\xad\', \'\xe8\xae\xae\', \'\xe8\xae\xaf\', \'\xe8\xae\xb0\', \'\xe8\xae\xb2\',\n    \'\xe8\xae\xb3\', \'\xe8\xae\xb4\', \'\xe8\xae\xb5\', \'\xe8\xae\xb6\', \'\xe8\xae\xb7\', \'\xe8\xae\xb8\', \'\xe8\xae\xb9\', \'\xe8\xae\xba\', \'\xe8\xae\xbc\', \'\xe8\xae\xbd\', \'\xe8\xae\xbe\', \'\xe8\xae\xbf\', \'\xe8\xaf\x80\', \'\xe8\xaf\x81\', \'\xe8\xaf\x82\', \'\xe8\xaf\x83\', \'\xe8\xaf\x84\', \'\xe8\xaf\x85\', \'\xe8\xaf\x86\', \'\xe8\xaf\x88\',\n    \'\xe8\xaf\x89\', \'\xe8\xaf\x8a\', \'\xe8\xaf\x8b\', \'\xe8\xaf\x8d\', \'\xe8\xaf\x8e\', \'\xe8\xaf\x8f\', \'\xe8\xaf\x91\', \'\xe8\xaf\x92\', \'\xe8\xaf\x95\', \'\xe8\xaf\x97\', \'\xe8\xaf\x98\', \'\xe8\xaf\x99\', \'\xe8\xaf\x9a\', \'\xe8\xaf\x9b\', \'\xe8\xaf\x9d\', \'\xe8\xaf\x9e\', \'\xe8\xaf\x9f\', \'\xe8\xaf\xa0\', \'\xe8\xaf\xa1\', \'\xe8\xaf\xa2\',\n    \'\xe8\xaf\xa3\', \'\xe8\xaf\xa4\', \'\xe8\xaf\xa5\', \'\xe8\xaf\xa6\', \'\xe8\xaf\xa7\', \'\xe8\xaf\xa8\', \'\xe8\xaf\xa9\', \'\xe8\xaf\xab\', \'\xe8\xaf\xac\', \'\xe8\xaf\xad\', \'\xe8\xaf\xae\', \'\xe8\xaf\xaf\', \'\xe8\xaf\xb0\', \'\xe8\xaf\xb1\', \'\xe8\xaf\xb2\', \'\xe8\xaf\xb3\', \'\xe8\xaf\xb4\', \'\xe8\xaf\xb5\', \'\xe8\xaf\xb7\', \'\xe8\xaf\xb8\',\n    \'\xe8\xaf\xba\', \'\xe8\xaf\xbb\', \'\xe8\xaf\xbd\', \'\xe8\xaf\xbe\', \'\xe8\xaf\xbf\', \'\xe8\xb0\x80\', \'\xe8\xb0\x81\', \'\xe8\xb0\x83\', \'\xe8\xb0\x84\', \'\xe8\xb0\x85\', \'\xe8\xb0\x86\', \'\xe8\xb0\x87\', \'\xe8\xb0\x88\', \'\xe8\xb0\x8a\', \'\xe8\xb0\x8b\', \'\xe8\xb0\x8d\', \'\xe8\xb0\x8e\', \'\xe8\xb0\x8f\', \'\xe8\xb0\x90\', \'\xe8\xb0\x91\',\n    \'\xe8\xb0\x92\', \'\xe8\xb0\x93\', \'\xe8\xb0\x94\', \'\xe8\xb0\x95\', \'\xe8\xb0\x97\', \'\xe8\xb0\x99\', \'\xe8\xb0\x9a\', \'\xe8\xb0\x9b\', \'\xe8\xb0\x9c\', \'\xe8\xb0\x9f\', \'\xe8\xb0\xa2\', \'\xe8\xb0\xa3\', \'\xe8\xb0\xa4\', \'\xe8\xb0\xa5\', \'\xe8\xb0\xa6\', \'\xe8\xb0\xa7\', \'\xe8\xb0\xa8\', \'\xe8\xb0\xa9\', \'\xe8\xb0\xaa\', \'\xe8\xb0\xac\',\n    \'\xe8\xb0\xad\', \'\xe8\xb0\xae\', \'\xe8\xb0\xaf\', \'\xe8\xb0\xb1\', \'\xe8\xb0\xb2\', \'\xe8\xb0\xb3\', \'\xe8\xb0\xb4\', \'\xe8\xb0\xb6\', \'\xe8\xb0\xb7\', \'\xe8\xb0\xb8\', \'\xe8\xb1\x81\', \'\xe8\xb1\x86\', \'\xe8\xb1\x89\', \'\xe8\xb1\x8c\', \'\xe8\xb1\x95\', \'\xe8\xb1\x9a\', \'\xe8\xb1\xa1\', \'\xe8\xb1\xa2\', \'\xe8\xb1\xa8\', \'\xe8\xb1\xaa\',\n    \'\xe8\xb1\xab\', \'\xe8\xb1\xad\', \'\xe8\xb1\xb3\', \'\xe8\xb1\xb8\', \'\xe8\xb1\xb9\', \'\xe8\xb1\xba\', \'\xe8\xb2\x82\', \'\xe8\xb2\x89\', \'\xe8\xb2\x8a\', \'\xe8\xb2\x8b\', \'\xe8\xb2\x8c\', \'\xe8\xb2\x94\', \'\xe8\xb4\x87\', \'\xe8\xb4\x9d\', \'\xe8\xb4\x9e\', \'\xe8\xb4\x9f\', \'\xe8\xb4\xa1\', \'\xe8\xb4\xa2\', \'\xe8\xb4\xa3\', \'\xe8\xb4\xa4\',\n    \'\xe8\xb4\xa5\', \'\xe8\xb4\xa6\', \'\xe8\xb4\xa7\', \'\xe8\xb4\xa8\', \'\xe8\xb4\xa9\', \'\xe8\xb4\xaa\', \'\xe8\xb4\xab\', \'\xe8\xb4\xac\', \'\xe8\xb4\xad\', \'\xe8\xb4\xae\', \'\xe8\xb4\xaf\', \'\xe8\xb4\xb0\', \'\xe8\xb4\xb1\', \'\xe8\xb4\xb2\', \'\xe8\xb4\xb3\', \'\xe8\xb4\xb4\', \'\xe8\xb4\xb5\', \'\xe8\xb4\xb7\', \'\xe8\xb4\xb8\', \'\xe8\xb4\xb9\',\n    \'\xe8\xb4\xba\', \'\xe8\xb4\xbb\', \'\xe8\xb4\xbc\', \'\xe8\xb4\xbd\', \'\xe8\xb4\xbe\', \'\xe8\xb4\xbf\', \'\xe8\xb5\x80\', \'\xe8\xb5\x81\', \'\xe8\xb5\x82\', \'\xe8\xb5\x83\', \'\xe8\xb5\x84\', \'\xe8\xb5\x85\', \'\xe8\xb5\x87\', \'\xe8\xb5\x88\', \'\xe8\xb5\x89\', \'\xe8\xb5\x8a\', \'\xe8\xb5\x8b\', \'\xe8\xb5\x8c\', \'\xe8\xb5\x8d\', \'\xe8\xb5\x8e\',\n    \'\xe8\xb5\x8f\', \'\xe8\xb5\x90\', \'\xe8\xb5\x94\', \'\xe8\xb5\x96\', \'\xe8\xb5\x98\', \'\xe8\xb5\x9a\', \'\xe8\xb5\x9b\', \'\xe8\xb5\x9c\', \'\xe8\xb5\x9d\', \'\xe8\xb5\x9e\', \'\xe8\xb5\x9f\', \'\xe8\xb5\xa0\', \'\xe8\xb5\xa1\', \'\xe8\xb5\xa2\', \'\xe8\xb5\xa3\', \'\xe8\xb5\xa4\', \'\xe8\xb5\xa6\', \'\xe8\xb5\xa7\', \'\xe8\xb5\xaa\', \'\xe8\xb5\xab\',\n    \'\xe8\xb5\xad\', \'\xe8\xb5\xb0\', \'\xe8\xb5\xb3\', \'\xe8\xb5\xb4\', \'\xe8\xb5\xb5\', \'\xe8\xb5\xb6\', \'\xe8\xb5\xb7\', \'\xe8\xb6\x81\', \'\xe8\xb6\x84\', \'\xe8\xb6\x85\', \'\xe8\xb6\x8a\', \'\xe8\xb6\x8b\', \'\xe8\xb6\x9f\', \'\xe8\xb6\xa3\', \'\xe8\xb6\xb1\', \'\xe8\xb6\xb3\', \'\xe8\xb6\xb4\', \'\xe8\xb6\xb5\', \'\xe8\xb6\xb8\', \'\xe8\xb6\xb9\',\n    \'\xe8\xb6\xba\', \'\xe8\xb6\xbe\', \'\xe8\xb7\x82\', \'\xe8\xb7\x83\', \'\xe8\xb7\x84\', \'\xe8\xb7\x86\', \'\xe8\xb7\x8b\', \'\xe8\xb7\x8c\', \'\xe8\xb7\x91\', \'\xe8\xb7\x96\', \'\xe8\xb7\x9a\', \'\xe8\xb7\x9b\', \'\xe8\xb7\x9d\', \'\xe8\xb7\x9f\', \'\xe8\xb7\xa3\', \'\xe8\xb7\xa4\', \'\xe8\xb7\xa8\', \'\xe8\xb7\xaa\', \'\xe8\xb7\xac\', \'\xe8\xb7\xaf\',\n    \'\xe8\xb7\xb3\', \'\xe8\xb7\xb5\', \'\xe8\xb7\xb7\', \'\xe8\xb7\xb8\', \'\xe8\xb7\xb9\', \'\xe8\xb7\xba\', \'\xe8\xb7\xbb\', \'\xe8\xb7\xbd\', \'\xe8\xb8\x89\', \'\xe8\xb8\x8a\', \'\xe8\xb8\x8c\', \'\xe8\xb8\x8f\', \'\xe8\xb8\x94\', \'\xe8\xb8\x9d\', \'\xe8\xb8\x9e\', \'\xe8\xb8\x9f\', \'\xe8\xb8\xa2\', \'\xe8\xb8\xa3\', \'\xe8\xb8\xa7\', \'\xe8\xb8\xa9\',\n    \'\xe8\xb8\xaa\', \'\xe8\xb8\xaf\', \'\xe8\xb8\xb0\', \'\xe8\xb8\xb1\', \'\xe8\xb8\xb5\', \'\xe8\xb8\xb9\', \'\xe8\xb8\xbd\', \'\xe8\xb9\x80\', \'\xe8\xb9\x82\', \'\xe8\xb9\x84\', \'\xe8\xb9\x87\', \'\xe8\xb9\x88\', \'\xe8\xb9\x89\', \'\xe8\xb9\x8a\', \'\xe8\xb9\x8b\', \'\xe8\xb9\x91\', \'\xe8\xb9\x92\', \'\xe8\xb9\x99\', \'\xe8\xb9\xa6\', \'\xe8\xb9\xa9\',\n    \'\xe8\xb9\xac\', \'\xe8\xb9\xad\', \'\xe8\xb9\xb0\', \'\xe8\xb9\xb2\', \'\xe8\xb9\xb4\', \'\xe8\xb9\xb6\', \'\xe8\xb9\xbb\', \'\xe8\xb9\xbf\', \'\xe8\xba\x81\', \'\xe8\xba\x85\', \'\xe8\xba\x87\', \'\xe8\xba\x8f\', \'\xe8\xba\x9e\', \'\xe8\xba\xab\', \'\xe8\xba\xac\', \'\xe8\xba\xaf\', \'\xe8\xba\xb2\', \'\xe8\xba\xba\', \'\xe8\xbd\x98\', \'\xe8\xbd\xa6\',\n    \'\xe8\xbd\xa7\', \'\xe8\xbd\xa8\', \'\xe8\xbd\xa9\', \'\xe8\xbd\xab\', \'\xe8\xbd\xac\', \'\xe8\xbd\xae\', \'\xe8\xbd\xaf\', \'\xe8\xbd\xb0\', \'\xe8\xbd\xb2\', \'\xe8\xbd\xb4\', \'\xe8\xbd\xb5\', \'\xe8\xbd\xb6\', \'\xe8\xbd\xb8\', \'\xe8\xbd\xba\', \'\xe8\xbd\xbb\', \'\xe8\xbd\xbc\', \'\xe8\xbd\xbd\', \'\xe8\xbd\xbf\', \'\xe8\xbe\x82\', \'\xe8\xbe\x83\',\n    \'\xe8\xbe\x84\', \'\xe8\xbe\x85\', \'\xe8\xbe\x86\', \'\xe8\xbe\x87\', \'\xe8\xbe\x88\', \'\xe8\xbe\x89\', \'\xe8\xbe\x8d\', \'\xe8\xbe\x8e\', \'\xe8\xbe\x90\', \'\xe8\xbe\x91\', \'\xe8\xbe\x93\', \'\xe8\xbe\x94\', \'\xe8\xbe\x95\', \'\xe8\xbe\x96\', \'\xe8\xbe\x97\', \'\xe8\xbe\x98\', \'\xe8\xbe\x99\', \'\xe8\xbe\x9b\', \'\xe8\xbe\x9c\', \'\xe8\xbe\x9e\',\n    \'\xe8\xbe\x9f\', \'\xe8\xbe\xa3\', \'\xe8\xbe\xa8\', \'\xe8\xbe\xa9\', \'\xe8\xbe\xab\', \'\xe8\xbe\xb0\', \'\xe8\xbe\xb1\', \'\xe8\xbe\xb9\', \'\xe8\xbe\xbd\', \'\xe8\xbe\xbe\', \'\xe8\xbf\x81\', \'\xe8\xbf\x82\', \'\xe8\xbf\x84\', \'\xe8\xbf\x85\', \'\xe8\xbf\x87\', \'\xe8\xbf\x88\', \'\xe8\xbf\x8e\', \'\xe8\xbf\x90\', \'\xe8\xbf\x91\', \'\xe8\xbf\x93\',\n    \'\xe8\xbf\x94\', \'\xe8\xbf\x95\', \'\xe8\xbf\x98\', \'\xe8\xbf\x99\', \'\xe8\xbf\x9b\', \'\xe8\xbf\x9c\', \'\xe8\xbf\x9d\', \'\xe8\xbf\x9e\', \'\xe8\xbf\x9f\', \'\xe8\xbf\xa2\', \'\xe8\xbf\xa4\', \'\xe8\xbf\xa5\', \'\xe8\xbf\xa6\', \'\xe8\xbf\xa8\', \'\xe8\xbf\xa9\', \'\xe8\xbf\xaa\', \'\xe8\xbf\xab\', \'\xe8\xbf\xad\', \'\xe8\xbf\xb0\', \'\xe8\xbf\xb7\',\n    \'\xe8\xbf\xb8\', \'\xe8\xbf\xb9\', \'\xe8\xbf\xbd\', \'\xe9\x80\x80\', \'\xe9\x80\x81\', \'\xe9\x80\x82\', \'\xe9\x80\x83\', \'\xe9\x80\x85\', \'\xe9\x80\x86\', \'\xe9\x80\x89\', \'\xe9\x80\x8a\', \'\xe9\x80\x8b\', \'\xe9\x80\x8d\', \'\xe9\x80\x8f\', \'\xe9\x80\x90\', \'\xe9\x80\x91\', \'\xe9\x80\x92\', \'\xe9\x80\x94\', \'\xe9\x80\x96\', \'\xe9\x80\x97\',\n    \'\xe9\x80\x9a\', \'\xe9\x80\x9b\', \'\xe9\x80\x9d\', \'\xe9\x80\x9e\', \'\xe9\x80\x9f\', \'\xe9\x80\xa0\', \'\xe9\x80\xa1\', \'\xe9\x80\xa2\', \'\xe9\x80\xa6\', \'\xe9\x80\xae\', \'\xe9\x80\xb5\', \'\xe9\x80\xb6\', \'\xe9\x80\xb8\', \'\xe9\x80\xbb\', \'\xe9\x80\xbc\', \'\xe9\x80\xbe\', \'\xe9\x81\x81\', \'\xe9\x81\x82\', \'\xe9\x81\x87\', \'\xe9\x81\x8d\',\n    \'\xe9\x81\x8f\', \'\xe9\x81\x90\', \'\xe9\x81\x91\', \'\xe9\x81\x92\', \'\xe9\x81\x93\', \'\xe9\x81\x97\', \'\xe9\x81\x98\', \'\xe9\x81\x9b\', \'\xe9\x81\xa2\', \'\xe9\x81\xa3\', \'\xe9\x81\xa5\', \'\xe9\x81\xa8\', \'\xe9\x81\xab\', \'\xe9\x81\xad\', \'\xe9\x81\xae\', \'\xe9\x81\xb4\', \'\xe9\x81\xb5\', \'\xe9\x81\xb6\', \'\xe9\x81\xb9\', \'\xe9\x81\xbd\',\n    \'\xe9\x81\xbf\', \'\xe9\x82\x80\', \'\xe9\x82\x82\', \'\xe9\x82\x83\', \'\xe9\x82\x88\', \'\xe9\x82\x8b\', \'\xe9\x82\x91\', \'\xe9\x82\x93\', \'\xe9\x82\x95\', \'\xe9\x82\x98\', \'\xe9\x82\x9b\', \'\xe9\x82\xa0\', \'\xe9\x82\xa2\', \'\xe9\x82\xa3\', \'\xe9\x82\xa6\', \'\xe9\x82\xaa\', \'\xe9\x82\xac\', \'\xe9\x82\xae\', \'\xe9\x82\xaf\', \'\xe9\x82\xb0\',\n    \'\xe9\x82\xb1\', \'\xe9\x82\xb3\', \'\xe9\x82\xb4\', \'\xe9\x82\xb5\', \'\xe9\x82\xb8\', \'\xe9\x82\xb9\', \'\xe9\x82\xba\', \'\xe9\x82\xbb\', \'\xe9\x82\xbd\', \'\xe9\x82\xbe\', \'\xe9\x83\x81\', \'\xe9\x83\x84\', \'\xe9\x83\x85\', \'\xe9\x83\x87\', \'\xe9\x83\x88\', \'\xe9\x83\x8a\', \'\xe9\x83\x8e\', \'\xe9\x83\x8f\', \'\xe9\x83\x90\', \'\xe9\x83\x91\',\n    \'\xe9\x83\x93\', \'\xe9\x83\x95\', \'\xe9\x83\x9c\', \'\xe9\x83\x9d\', \'\xe9\x83\xa1\', \'\xe9\x83\xa2\', \'\xe9\x83\xa6\', \'\xe9\x83\xa7\', \'\xe9\x83\xa8\', \'\xe9\x83\xaa\', \'\xe9\x83\xab\', \'\xe9\x83\xad\', \'\xe9\x83\xaf\', \'\xe9\x83\xb4\', \'\xe9\x83\xb8\', \'\xe9\x83\xbd\', \'\xe9\x83\xbe\', \'\xe9\x83\xbf\', \'\xe9\x84\x82\', \'\xe9\x84\x84\',\n    \'\xe9\x84\x8f\', \'\xe9\x84\x97\', \'\xe9\x84\x99\', \'\xe9\x84\x9c\', \'\xe9\x84\xa0\', \'\xe9\x84\xa2\', \'\xe9\x84\xa3\', \'\xe9\x84\xb1\', \'\xe9\x84\xb3\', \'\xe9\x85\x86\', \'\xe9\x85\x87\', \'\xe9\x85\x89\', \'\xe9\x85\x8a\', \'\xe9\x85\x8b\', \'\xe9\x85\x8c\', \'\xe9\x85\x8d\', \'\xe9\x85\x8e\', \'\xe9\x85\x92\', \'\xe9\x85\x97\', \'\xe9\x85\x9a\',\n    \'\xe9\x85\x9d\', \'\xe9\x85\x9e\', \'\xe9\x85\xa1\', \'\xe9\x85\xa2\', \'\xe9\x85\xa3\', \'\xe9\x85\xa4\', \'\xe9\x85\xa5\', \'\xe9\x85\xa9\', \'\xe9\x85\xaa\', \'\xe9\x85\xac\', \'\xe9\x85\xae\', \'\xe9\x85\xaf\', \'\xe9\x85\xb0\', \'\xe9\x85\xb1\', \'\xe9\x85\xb5\', \'\xe9\x85\xb6\', \'\xe9\x85\xb7\', \'\xe9\x85\xb8\', \'\xe9\x85\xb9\', \'\xe9\x85\xba\',\n    \'\xe9\x85\xbf\', \'\xe9\x86\x87\', \'\xe9\x86\x89\', \'\xe9\x86\x8b\', \'\xe9\x86\x8d\', \'\xe9\x86\x92\', \'\xe9\x86\x9a\', \'\xe9\x86\x9b\', \'\xe9\x86\xa2\', \'\xe9\x86\xa6\', \'\xe9\x86\xaa\', \'\xe9\x86\xae\', \'\xe9\x86\xb3\', \'\xe9\x86\xb4\', \'\xe9\x86\xb5\', \'\xe9\x86\xba\', \'\xe9\x87\x82\', \'\xe9\x87\x87\', \'\xe9\x87\x89\', \'\xe9\x87\x8a\',\n    \'\xe9\x87\x8c\', \'\xe9\x87\x8d\', \'\xe9\x87\x8e\', \'\xe9\x87\x8f\', \'\xe9\x87\x90\', \'\xe9\x87\x91\', \'\xe9\x87\x9c\', \'\xe9\x88\x87\', \'\xe9\x89\x8f\', \'\xe9\x89\xb4\', \'\xe9\x8a\xae\', \'\xe9\x8f\x96\', \'\xe9\x90\x98\', \'\xe9\x91\x99\', \'\xe9\x91\xab\', \'\xe9\x92\x88\', \'\xe9\x92\x89\', \'\xe9\x92\x8a\', \'\xe9\x92\x8e\', \'\xe9\x92\x8f\',\n    \'\xe9\x92\x92\', \'\xe9\x92\x93\', \'\xe9\x92\x97\', \'\xe9\x92\x99\', \'\xe9\x92\x9a\', \'\xe9\x92\x9b\', \'\xe9\x92\x9c\', \'\xe9\x92\x9d\', \'\xe9\x92\x9e\', \'\xe9\x92\x9f\', \'\xe9\x92\xa0\', \'\xe9\x92\xa1\', \'\xe9\x92\xa2\', \'\xe9\x92\xa4\', \'\xe9\x92\xa5\', \'\xe9\x92\xa6\', \'\xe9\x92\xa7\', \'\xe9\x92\xa8\', \'\xe9\x92\xa9\', \'\xe9\x92\xae\',\n    \'\xe9\x92\xaf\', \'\xe9\x92\xb0\', \'\xe9\x92\xb1\', \'\xe9\x92\xb2\', \'\xe9\x92\xb3\', \'\xe9\x92\xb4\', \'\xe9\x92\xb5\', \'\xe9\x92\xb9\', \'\xe9\x92\xba\', \'\xe9\x92\xbb\', \'\xe9\x92\xbc\', \'\xe9\x92\xbe\', \'\xe9\x92\xbf\', \'\xe9\x93\x80\', \'\xe9\x93\x81\', \'\xe9\x93\x82\', \'\xe9\x93\x83\', \'\xe9\x93\x84\', \'\xe9\x93\x85\', \'\xe9\x93\x86\',\n    \'\xe9\x93\x89\', \'\xe9\x93\x8e\', \'\xe9\x93\x90\', \'\xe9\x93\x99\', \'\xe9\x93\x9a\', \'\xe9\x93\x9b\', \'\xe9\x93\x9c\', \'\xe9\x93\x9d\', \'\xe9\x93\xa0\', \'\xe9\x93\xa2\', \'\xe9\x93\xa3\', \'\xe9\x93\xa4\', \'\xe9\x93\xa8\', \'\xe9\x93\xa9\', \'\xe9\x93\xac\', \'\xe9\x93\xad\', \'\xe9\x93\xae\', \'\xe9\x93\xb2\', \'\xe9\x93\xb3\', \'\xe9\x93\xb5\',\n    \'\xe9\x93\xb6\', \'\xe9\x93\xb8\', \'\xe9\x93\xba\', \'\xe9\x93\xbe\', \'\xe9\x93\xbf\', \'\xe9\x94\x80\', \'\xe9\x94\x81\', \'\xe9\x94\x82\', \'\xe9\x94\x84\', \'\xe9\x94\x85\', \'\xe9\x94\x88\', \'\xe9\x94\x89\', \'\xe9\x94\x8b\', \'\xe9\x94\x8c\', \'\xe9\x94\x8f\', \'\xe9\x94\x90\', \'\xe9\x94\x91\', \'\xe9\x94\x97\', \'\xe9\x94\x99\', \'\xe9\x94\x9a\',\n    \'\xe9\x94\xa1\', \'\xe9\x94\xa2\', \'\xe9\x94\xa3\', \'\xe9\x94\xa4\', \'\xe9\x94\xa5\', \'\xe9\x94\xa6\', \'\xe9\x94\xad\', \'\xe9\x94\xae\', \'\xe9\x94\xaf\', \'\xe9\x94\xb0\', \'\xe9\x94\xb1\', \'\xe9\x94\xb2\', \'\xe9\x94\xb5\', \'\xe9\x94\xb7\', \'\xe9\x94\xb9\', \'\xe9\x94\xba\', \'\xe9\x94\xbb\', \'\xe9\x94\xbd\', \'\xe9\x94\xbe\', \'\xe9\x95\x80\',\n    \'\xe9\x95\x81\', \'\xe9\x95\x82\', \'\xe9\x95\x87\', \'\xe9\x95\x89\', \'\xe9\x95\x8a\', \'\xe9\x95\x8c\', \'\xe9\x95\x8d\', \'\xe9\x95\x90\', \'\xe9\x95\x91\', \'\xe9\x95\x92\', \'\xe9\x95\x95\', \'\xe9\x95\x96\', \'\xe9\x95\x9b\', \'\xe9\x95\x9c\', \'\xe9\x95\x9d\', \'\xe9\x95\x9e\', \'\xe9\x95\xa3\', \'\xe9\x95\xaa\', \'\xe9\x95\xac\', \'\xe9\x95\xad\',\n    \'\xe9\x95\xaf\', \'\xe9\x95\xb0\', \'\xe9\x95\xb3\', \'\xe9\x95\xb5\', \'\xe9\x95\xb6\', \'\xe9\x95\xbf\', \'\xe9\x96\x8b\', \'\xe9\x96\x93\', \'\xe9\x97\x87\', \'\xe9\x97\x9f\', \'\xe9\x97\xa8\', \'\xe9\x97\xaa\', \'\xe9\x97\xab\', \'\xe9\x97\xad\', \'\xe9\x97\xae\', \'\xe9\x97\xaf\', \'\xe9\x97\xb0\', \'\xe9\x97\xb1\', \'\xe9\x97\xb2\', \'\xe9\x97\xb3\',\n    \'\xe9\x97\xb4\', \'\xe9\x97\xb5\', \'\xe9\x97\xb7\', \'\xe9\x97\xb8\', \'\xe9\x97\xb9\', \'\xe9\x97\xba\', \'\xe9\x97\xbb\', \'\xe9\x97\xbc\', \'\xe9\x97\xbd\', \'\xe9\x97\xbe\', \'\xe9\x98\x80\', \'\xe9\x98\x81\', \'\xe9\x98\x82\', \'\xe9\x98\x83\', \'\xe9\x98\x85\', \'\xe9\x98\x86\', \'\xe9\x98\x88\', \'\xe9\x98\x89\', \'\xe9\x98\x8a\', \'\xe9\x98\x8d\',\n    \'\xe9\x98\x8e\', \'\xe9\x98\x8f\', \'\xe9\x98\x90\', \'\xe9\x98\x91\', \'\xe9\x98\x94\', \'\xe9\x98\x95\', \'\xe9\x98\x96\', \'\xe9\x98\x97\', \'\xe9\x98\x99\', \'\xe9\x98\x9a\', \'\xe9\x98\x9c\', \'\xe9\x98\x9f\', \'\xe9\x98\xa1\', \'\xe9\x98\xaa\', \'\xe9\x98\xae\', \'\xe9\x98\xb1\', \'\xe9\x98\xb2\', \'\xe9\x98\xb3\', \'\xe9\x98\xb4\', \'\xe9\x98\xb5\',\n    \'\xe9\x98\xb6\', \'\xe9\x98\xbb\', \'\xe9\x98\xbf\', \'\xe9\x99\x80\', \'\xe9\x99\x82\', \'\xe9\x99\x84\', \'\xe9\x99\x85\', \'\xe9\x99\x86\', \'\xe9\x99\x87\', \'\xe9\x99\x88\', \'\xe9\x99\x89\', \'\xe9\x99\x8b\', \'\xe9\x99\x8c\', \'\xe9\x99\x8d\', \'\xe9\x99\x90\', \'\xe9\x99\x95\', \'\xe9\x99\x9b\', \'\xe9\x99\x9f\', \'\xe9\x99\xa1\', \'\xe9\x99\xa2\',\n    \'\xe9\x99\xa4\', \'\xe9\x99\xa8\', \'\xe9\x99\xa9\', \'\xe9\x99\xaa\', \'\xe9\x99\xac\', \'\xe9\x99\xb2\', \'\xe9\x99\xb5\', \'\xe9\x99\xb6\', \'\xe9\x99\xb7\', \'\xe9\x9a\x85\', \'\xe9\x9a\x86\', \'\xe9\x9a\x8b\', \'\xe9\x9a\x8d\', \'\xe9\x9a\x8f\', \'\xe9\x9a\x90\', \'\xe9\x9a\x94\', \'\xe9\x9a\x97\', \'\xe9\x9a\x98\', \'\xe9\x9a\x99\', \'\xe9\x9a\x9c\',\n    \'\xe9\x9a\xa7\', \'\xe9\x9a\xb0\', \'\xe9\x9a\xb3\', \'\xe9\x9a\xb6\', \'\xe9\x9a\xbc\', \'\xe9\x9a\xbd\', \'\xe9\x9a\xbe\', \'\xe9\x9b\x80\', \'\xe9\x9b\x81\', \'\xe9\x9b\x84\', \'\xe9\x9b\x85\', \'\xe9\x9b\x86\', \'\xe9\x9b\x87\', \'\xe9\x9b\x89\', \'\xe9\x9b\x8c\', \'\xe9\x9b\x8d\', \'\xe9\x9b\x8e\', \'\xe9\x9b\x8f\', \'\xe9\x9b\x92\', \'\xe9\x9b\x95\',\n    \'\xe9\x9b\xa0\', \'\xe9\x9b\xa8\', \'\xe9\x9b\xa9\', \'\xe9\x9b\xaa\', \'\xe9\x9b\xaf\', \'\xe9\x9b\xb3\', \'\xe9\x9b\xb6\', \'\xe9\x9b\xb7\', \'\xe9\x9b\xb9\', \'\xe9\x9b\xbe\', \'\xe9\x9c\x80\', \'\xe9\x9c\x81\', \'\xe9\x9c\x84\', \'\xe9\x9c\x86\', \'\xe9\x9c\x87\', \'\xe9\x9c\x89\', \'\xe9\x9c\x8d\', \'\xe9\x9c\x8e\', \'\xe9\x9c\x93\', \'\xe9\x9c\x96\',\n    \'\xe9\x9c\x9c\', \'\xe9\x9c\x9e\', \'\xe9\x9c\xa3\', \'\xe9\x9c\xad\', \'\xe9\x9c\xb2\', \'\xe9\x9c\xb8\', \'\xe9\x9c\xb9\', \'\xe9\x9c\xbe\', \'\xe9\x9d\x92\', \'\xe9\x9d\x93\', \'\xe9\x9d\x96\', \'\xe9\x9d\x99\', \'\xe9\x9d\x9b\', \'\xe9\x9d\x9e\', \'\xe9\x9d\xa0\', \'\xe9\x9d\xa1\', \'\xe9\x9d\xa2\', \'\xe9\x9d\xa5\', \'\xe9\x9d\xa9\', \'\xe9\x9d\xb3\',\n    \'\xe9\x9d\xb4\', \'\xe9\x9d\xb6\', \'\xe9\x9e\x85\', \'\xe9\x9e\x8b\', \'\xe9\x9e\x8d\', \'\xe9\x9e\x91\', \'\xe9\x9e\x98\', \'\xe9\x9e\x9a\', \'\xe9\x9e\xa0\', \'\xe9\x9e\xa3\', \'\xe9\x9e\xab\', \'\xe9\x9e\xad\', \'\xe9\x9e\xae\', \'\xe9\x9f\x82\', \'\xe9\x9f\xa6\', \'\xe9\x9f\xa7\', \'\xe9\x9f\xa9\', \'\xe9\x9f\xaa\', \'\xe9\x9f\xac\', \'\xe9\x9f\xad\',\n    \'\xe9\x9f\xb3\', \'\xe9\x9f\xb5\', \'\xe9\x9f\xb6\', \'\xe9\xa0\x89\', \'\xe9\xa0\xab\', \'\xe9\xa1\x92\', \'\xe9\xa1\xb5\', \'\xe9\xa1\xb6\', \'\xe9\xa1\xb7\', \'\xe9\xa1\xb9\', \'\xe9\xa1\xba\', \'\xe9\xa1\xbb\', \'\xe9\xa1\xbc\', \'\xe9\xa1\xbd\', \'\xe9\xa1\xbe\', \'\xe9\xa1\xbf\', \'\xe9\xa2\x80\', \'\xe9\xa2\x81\', \'\xe9\xa2\x82\', \'\xe9\xa2\x84\',\n    \'\xe9\xa2\x85\', \'\xe9\xa2\x86\', \'\xe9\xa2\x87\', \'\xe9\xa2\x88\', \'\xe9\xa2\x89\', \'\xe9\xa2\x8a\', \'\xe9\xa2\x8c\', \'\xe9\xa2\x8d\', \'\xe9\xa2\x90\', \'\xe9\xa2\x91\', \'\xe9\xa2\x93\', \'\xe9\xa2\x94\', \'\xe9\xa2\x96\', \'\xe9\xa2\x97\', \'\xe9\xa2\x98\', \'\xe9\xa2\x9a\', \'\xe9\xa2\x9b\', \'\xe9\xa2\x9c\', \'\xe9\xa2\x9d\', \'\xe9\xa2\xa0\',\n    \'\xe9\xa2\xa1\', \'\xe9\xa2\xa4\', \'\xe9\xa2\xa6\', \'\xe9\xa2\xa7\', \'\xe9\xa3\x8e\', \'\xe9\xa3\x92\', \'\xe9\xa3\x93\', \'\xe9\xa3\x95\', \'\xe9\xa3\x98\', \'\xe9\xa3\x99\', \'\xe9\xa3\x9a\', \'\xe9\xa3\x9e\', \'\xe9\xa3\x9f\', \'\xe9\xa3\xa7\', \'\xe9\xa3\xa8\', \'\xe9\xa4\x8d\', \'\xe9\xa4\x90\', \'\xe9\xa4\xae\', \'\xe9\xa5\x95\', \'\xe9\xa5\x9f\',\n    \'\xe9\xa5\xa5\', \'\xe9\xa5\xa6\', \'\xe9\xa5\xaa\', \'\xe9\xa5\xac\', \'\xe9\xa5\xad\', \'\xe9\xa5\xae\', \'\xe9\xa5\xaf\', \'\xe9\xa5\xb0\', \'\xe9\xa5\xb1\', \'\xe9\xa5\xb2\', \'\xe9\xa5\xb4\', \'\xe9\xa5\xb5\', \'\xe9\xa5\xb6\', \'\xe9\xa5\xb7\', \'\xe9\xa5\xba\', \'\xe9\xa5\xbc\', \'\xe9\xa5\xbd\', \'\xe9\xa5\xbf\', \'\xe9\xa6\x80\', \'\xe9\xa6\x81\',\n    \'\xe9\xa6\x85\', \'\xe9\xa6\x86\', \'\xe9\xa6\x88\', \'\xe9\xa6\x8b\', \'\xe9\xa6\x8d\', \'\xe9\xa6\x8e\', \'\xe9\xa6\x8f\', \'\xe9\xa6\x90\', \'\xe9\xa6\x92\', \'\xe9\xa6\x93\', \'\xe9\xa6\x94\', \'\xe9\xa6\x95\', \'\xe9\xa6\x96\', \'\xe9\xa6\x99\', \'\xe9\xa6\xa5\', \'\xe9\xa6\xa8\', \'\xe9\xa7\x86\', \'\xe9\xa7\xb9\', \'\xe9\xa8\xa0\', \'\xe9\xa8\xb3\',\n    \'\xe9\xa9\xa9\', \'\xe9\xa9\xac\', \'\xe9\xa9\xad\', \'\xe9\xa9\xae\', \'\xe9\xa9\xaf\', \'\xe9\xa9\xb0\', \'\xe9\xa9\xb1\', \'\xe9\xa9\xb3\', \'\xe9\xa9\xb4\', \'\xe9\xa9\xb5\', \'\xe9\xa9\xb6\', \'\xe9\xa9\xb7\', \'\xe9\xa9\xb8\', \'\xe9\xa9\xb9\', \'\xe9\xa9\xba\', \'\xe9\xa9\xbb\', \'\xe9\xa9\xbc\', \'\xe9\xa9\xbd\', \'\xe9\xa9\xbe\', \'\xe9\xa9\xbf\',\n    \'\xe9\xaa\x80\', \'\xe9\xaa\x81\', \'\xe9\xaa\x82\', \'\xe9\xaa\x83\', \'\xe9\xaa\x84\', \'\xe9\xaa\x85\', \'\xe9\xaa\x86\', \'\xe9\xaa\x87\', \'\xe9\xaa\x88\', \'\xe9\xaa\x8a\', \'\xe9\xaa\x8b\', \'\xe9\xaa\x8c\', \'\xe9\xaa\x8d\', \'\xe9\xaa\x8f\', \'\xe9\xaa\x90\', \'\xe9\xaa\x91\', \'\xe9\xaa\x96\', \'\xe9\xaa\x97\', \'\xe9\xaa\x98\', \'\xe9\xaa\x9a\',\n    \'\xe9\xaa\x9b\', \'\xe9\xaa\x9c\', \'\xe9\xaa\x9e\', \'\xe9\xaa\xa0\', \'\xe9\xaa\xa1\', \'\xe9\xaa\xa4\', \'\xe9\xaa\xa5\', \'\xe9\xaa\xa7\', \'\xe9\xaa\xa8\', \'\xe9\xaa\xb0\', \'\xe9\xaa\xb6\', \'\xe9\xaa\xb7\', \'\xe9\xaa\xb8\', \'\xe9\xaa\xbc\', \'\xe9\xab\x80\', \'\xe9\xab\x83\', \'\xe9\xab\x85\', \'\xe9\xab\x8b\', \'\xe9\xab\x91\', \'\xe9\xab\x93\',\n    \'\xe9\xab\x98\', \'\xe9\xab\xa1\', \'\xe9\xab\xa6\', \'\xe9\xab\xab\', \'\xe9\xab\xad\', \'\xe9\xab\xaf\', \'\xe9\xab\xbb\', \'\xe9\xac\x83\', \'\xe9\xac\x93\', \'\xe9\xac\x9f\', \'\xe9\xac\xa3\', \'\xe9\xac\xb2\', \'\xe9\xac\xbb\', \'\xe9\xac\xbc\', \'\xe9\xad\x81\', \'\xe9\xad\x82\', \'\xe9\xad\x84\', \'\xe9\xad\x85\', \'\xe9\xad\x87\', \'\xe9\xad\x89\',\n    \'\xe9\xad\x8b\', \'\xe9\xad\x8d\', \'\xe9\xad\x8f\', \'\xe9\xad\x91\', \'\xe9\xad\x94\', \'\xe9\xad\xad\', \'\xe9\xae\xbc\', \'\xe9\xb1\xbc\', \'\xe9\xb1\xbf\', \'\xe9\xb2\x81\', \'\xe9\xb2\x87\', \'\xe9\xb2\x8b\', \'\xe9\xb2\x8d\', \'\xe9\xb2\x90\', \'\xe9\xb2\x91\', \'\xe9\xb2\x94\', \'\xe9\xb2\x9b\', \'\xe9\xb2\x9c\', \'\xe9\xb2\xa0\', \'\xe9\xb2\xa4\',\n    \'\xe9\xb2\xa7\', \'\xe9\xb2\xa8\', \'\xe9\xb2\xb0\', \'\xe9\xb2\xb2\', \'\xe9\xb2\xb8\', \'\xe9\xb3\x83\', \'\xe9\xb3\x84\', \'\xe9\xb3\x8c\', \'\xe9\xb3\x8d\', \'\xe9\xb3\x8f\', \'\xe9\xb3\x96\', \'\xe9\xb3\x9c\', \'\xe9\xb3\x9d\', \'\xe9\xb3\x9e\', \'\xe9\xb4\x88\', \'\xe9\xb7\xaa\', \'\xe9\xb8\x9f\', \'\xe9\xb8\xa0\', \'\xe9\xb8\xa1\', \'\xe9\xb8\xa2\',\n    \'\xe9\xb8\xa3\', \'\xe9\xb8\xa5\', \'\xe9\xb8\xa6\', \'\xe9\xb8\xa8\', \'\xe9\xb8\xa9\', \'\xe9\xb8\xac\', \'\xe9\xb8\xad\', \'\xe9\xb8\xae\', \'\xe9\xb8\xaf\', \'\xe9\xb8\xb1\', \'\xe9\xb8\xb3\', \'\xe9\xb8\xb5\', \'\xe9\xb8\xb7\', \'\xe9\xb8\xbd\', \'\xe9\xb8\xbe\', \'\xe9\xb8\xbf\', \'\xe9\xb9\x82\', \'\xe9\xb9\x83\', \'\xe9\xb9\x84\', \'\xe9\xb9\x85\',\n    \'\xe9\xb9\x86\', \'\xe9\xb9\x89\', \'\xe9\xb9\x8a\', \'\xe9\xb9\x8f\', \'\xe9\xb9\x91\', \'\xe9\xb9\x96\', \'\xe9\xb9\x97\', \'\xe9\xb9\x98\', \'\xe9\xb9\x9a\', \'\xe9\xb9\x9c\', \'\xe9\xb9\x9e\', \'\xe9\xb9\xa3\', \'\xe9\xb9\xa4\', \'\xe9\xb9\xa6\', \'\xe9\xb9\xab\', \'\xe9\xb9\xad\', \'\xe9\xb9\xb0\', \'\xe9\xb9\xb3\', \'\xe9\xb9\xbf\', \'\xe9\xba\x82\',\n    \'\xe9\xba\x83\', \'\xe9\xba\x8b\', \'\xe9\xba\x92\', \'\xe9\xba\x93\', \'\xe9\xba\x9d\', \'\xe9\xba\x9f\', \'\xe9\xba\xa4\', \'\xe9\xba\xa6\', \'\xe9\xba\xbb\', \'\xe9\xba\xbd\', \'\xe9\xba\xbe\', \'\xe9\xbb\x84\', \'\xe9\xbb\x8d\', \'\xe9\xbb\x8e\', \'\xe9\xbb\x8f\', \'\xe9\xbb\x91\', \'\xe9\xbb\x94\', \'\xe9\xbb\x98\', \'\xe9\xbb\x99\', \'\xe9\xbb\x9b\',\n    \'\xe9\xbb\x9c\', \'\xe9\xbb\x9d\', \'\xe9\xbb\x9f\', \'\xe9\xbb\xa0\', \'\xe9\xbb\xa1\', \'\xe9\xbb\xa5\', \'\xe9\xbb\xaf\', \'\xe9\xbb\xbe\', \'\xe9\xbc\x8b\', \'\xe9\xbc\x8d\', \'\xe9\xbc\x8e\', \'\xe9\xbc\x93\', \'\xe9\xbc\xa0\', \'\xe9\xbc\xbb\', \'\xe9\xbc\xbe\', \'\xe9\xbd\x90\', \'\xe9\xbd\xae\', \'\xe9\xbd\xbf\', \'\xe9\xbe\x81\', \'\xe9\xbe\x82\',\n    \'\xe9\xbe\x84\', \'\xe9\xbe\x88\', \'\xe9\xbe\x8a\', \'\xe9\xbe\x8b\', \'\xe9\xbe\x8c\', \'\xe9\xbe\x8f\', \'\xe9\xbe\x99\', \'\xe9\xbe\x9a\', \'\xe9\xbe\x9b\', \'\xe9\xbe\x9f\', \'\xe5\x81\xb7\', \'\xe5\x81\xbb\', \'\xe5\x81\xbf\', \'\xe5\x82\x80\', \'\xe5\x82\x85\', \'\xe5\x82\x88\', \'\xe5\x82\x8d\', \'\xe5\x82\x92\'\n]\n\nFULL_ANGLE_MAP = {\n    \'\xef\xbd\x81\': \'a\', \'\xef\xbd\x82\': \'b\', \'\xef\xbd\x83\': \'c\', \'\xef\xbd\x84\': \'d\', \'\xef\xbd\x85\': \'e\', \'\xef\xbd\x86\': \'f\', \'\xef\xbd\x87\': \'g\', \'\xef\xbd\x88\': \'h\', \'\xef\xbd\x89\': \'i\',\n    \'\xef\xbd\x8a\': \'j\', \'\xef\xbd\x8b\': \'k\', \'\xef\xbd\x8c\': \'l\', \'\xef\xbd\x8d\': \'m\', \'\xef\xbd\x8e\': \'n\', \'\xef\xbd\x8f\': \'o\', \'\xef\xbd\x90\': \'p\', \'\xef\xbd\x91\': \'q\', \'\xef\xbd\x92\': \'r\',\n    \'\xef\xbd\x93\': \'s\', \'\xef\xbd\x94\': \'t\', \'\xef\xbd\x95\': \'u\', \'\xef\xbd\x96\': \'v\', \'\xef\xbd\x97\': \'w\', \'\xef\xbd\x98\': \'x\', \'\xef\xbd\x99\': \'y\', \'\xef\xbd\x9a\': \'z\', \'\xef\xbc\xa1\': \'A\',\n    \'\xef\xbc\xa2\': \'B\', \'\xef\xbc\xa3\': \'C\', \'\xef\xbc\xa4\': \'D\', \'\xef\xbc\xa5\': \'E\', \'\xef\xbc\xa6\': \'F\', \'\xef\xbc\xa7\': \'G\', \'\xef\xbc\xa8\': \'H\', \'\xef\xbc\xa9\': \'I\', \'\xef\xbc\xaa\': \'J\',\n    \'\xef\xbc\xab\': \'K\', \'\xef\xbc\xac\': \'L\', \'\xef\xbc\xad\': \'M\', \'\xef\xbc\xae\': \'N\', \'\xef\xbc\xaf\': \'O\', \'\xef\xbc\xb0\': \'P\', \'\xef\xbc\xb1\': \'Q\', \'\xef\xbc\xb2\': \'R\', \'\xef\xbc\xb3\': \'S\',\n    \'\xef\xbc\xb4\': \'T\', \'\xef\xbc\xb5\': \'U\', \'\xef\xbc\xb6\': \'V\', \'\xef\xbc\xb7\': \'W\', \'\xef\xbc\xb8\': \'X\', \'\xef\xbc\xb9\': \'Y\', \'\xef\xbc\xba\': \'Z\', \'\xef\xbc\x91\': \'1\', \'\xef\xbc\x92\': \'2\',\n    \'\xef\xbc\x93\': \'3\', \'\xef\xbc\x94\': \'4\', \'\xef\xbc\x95\': \'5\', \'\xef\xbc\x96\': \'6\', \'\xef\xbc\x97\': \'7\', \'\xef\xbc\x98\': \'8\', \'\xef\xbc\x99\': \'9\', \'\xef\xbc\x90\': \'0\', \'\xef\xbc\x9f\': \'?\',\n    \'\xef\xbc\x81\': \'!\', \'\xef\xbc\x9a\': \':\', \'\xef\xbc\x9b\': \':\'\n}\n\nFLOAT_SYMBOL = [\'.\']\n\nSIMPLE_CATEGORY_MODEL = dict(\n    NUMERIC=NUMBER,\n    ALPHANUMERIC=NUMBER + ALPHA_LOWER + ALPHA_UPPER,\n    ALPHANUMERIC_LOWER=NUMBER + ALPHA_LOWER,\n    ALPHANUMERIC_UPPER=NUMBER + ALPHA_UPPER,\n    ALPHABET_LOWER=ALPHA_LOWER,\n    ALPHABET_UPPER=ALPHA_UPPER,\n    ALPHABET=ALPHA_LOWER + ALPHA_UPPER,\n    ARITHMETIC=NUMBER + ARITHMETIC_SYMBOL,\n    FLOAT=NUMBER + FLOAT_SYMBOL,\n    CHS_3500=CHINESE_3500,\n    ALPHANUMERIC_CHS_3500_LOWER=NUMBER + ALPHA_LOWER + CHINESE_3500,\n    DOCUMENT_OCR=NUMBER + ALPHA_LOWER + ALPHA_UPPER + DOCUMENT_SYMBOLS + DOCUMENT_CHS\n)\n\n\ndef category_extract(param):\n    if isinstance(param, list):\n        return param\n    if isinstance(param, SimpleCharset):\n        param = param.value\n    if isinstance(param, str):\n        if param in SIMPLE_CATEGORY_MODEL.keys():\n            return SIMPLE_CATEGORY_MODEL.get(param)\n        exception(\n            ""Category set configuration error, customized category set should be list type"",\n            ConfigException.CATEGORY_INCORRECT\n        )\n\n\ndef encode_maps(source):\n    return {category: i for i, category in enumerate(source, 0)}\n'"
config.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\n\nimport os\nimport json\nimport platform\nimport re\nimport yaml\nfrom category import *\nfrom constants import *\nfrom exception import exception, ConfigException\n\n# Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n# If you have a GPU, you shouldn\'t care about AVX support.\n# Just disables the warning, doesn\'t enable AVX/FMA\n# os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nPLATFORM = platform.system()\n# PATH_SPLIT = ""\\\\"" if PLATFORM == ""Windows"" else ""/""\nPATH_SPLIT = ""/""\nMODEL_CONFIG_NAME = ""model.yaml""\nIGNORE_FILES = [\'.DS_Store\']\n\nCORE_VERSION = \'20200530\'\n\nNETWORK_MAP = {\n    \'CNNX\': CNNNetwork.CNNX,\n    \'CNN5\': CNNNetwork.CNN5,\n    \'ResNetTiny\': CNNNetwork.ResNetTiny,\n    \'ResNet50\': CNNNetwork.ResNet50,\n    \'DenseNet\': CNNNetwork.DenseNet,\n    \'MobileNetV2\': CNNNetwork.MobileNetV2,\n    \'LSTM\': RecurrentNetwork.LSTM,\n    \'BiLSTM\': RecurrentNetwork.BiLSTM,\n    \'GRU\': RecurrentNetwork.GRU,\n    \'BiGRU\': RecurrentNetwork.BiGRU,\n    \'LSTMcuDNN\': RecurrentNetwork.LSTMcuDNN,\n    \'BiLSTMcuDNN\': RecurrentNetwork.BiLSTMcuDNN,\n    \'GRUcuDNN\': RecurrentNetwork.GRUcuDNN,\n    \'NoRecurrent\': RecurrentNetwork.NoRecurrent\n}\n\nBUILT_IN_CATEGORY_MAP = {\n    \'NUMERIC\': SimpleCharset.NUMERIC,\n    \'ALPHANUMERIC\': SimpleCharset.ALPHANUMERIC,\n    \'ALPHANUMERIC_LOWER\': SimpleCharset.ALPHANUMERIC_LOWER,\n    \'ALPHANUMERIC_UPPER\': SimpleCharset.ALPHANUMERIC_UPPER,\n    \'ALPHABET_LOWER\': SimpleCharset.ALPHABET_LOWER,\n    \'ALPHABET_UPPER\': SimpleCharset.ALPHABET_UPPER,\n    \'ALPHABET\': SimpleCharset.ALPHABET,\n    \'ARITHMETIC\': SimpleCharset.ARITHMETIC,\n    \'FLOAT\': SimpleCharset.FLOAT,\n    \'CHS_3500\': SimpleCharset.CHS_3500,\n    \'ALPHANUMERIC_CHS_3500_LOWER\': SimpleCharset.ALPHANUMERIC_CHS_3500_LOWER,\n}\n\nOPTIMIZER_MAP = {\n    \'RAdam\': Optimizer.RAdam,\n    \'Adam\': Optimizer.Adam,\n    \'AdaBound\': Optimizer.AdaBound,\n    \'Momentum\': Optimizer.Momentum,\n    \'SGD\': Optimizer.SGD,\n    \'AdaGrad\': Optimizer.AdaGrad,\n    \'RMSProp\': Optimizer.RMSProp\n}\n\nMODEL_SCENE_MAP = {\n    \'Classification\': ModelScene.Classification\n}\n\nLOSS_FUNC_MAP = {\n    \'CTC\': LossFunction.CTC,\n    \'CrossEntropy\': LossFunction.CrossEntropy\n}\n\nCOMPILE_MODEL_MAP = {\n    ModelType.PB: "".pb"",\n    ModelType.ONNX: "".onnx"",\n    ModelType.TFLITE: "".tflite""\n}\n\nRESIZE_MAP = {\n    LossFunction.CTC: lambda x, y: [None, y],\n    LossFunction.CrossEntropy: lambda x, y: [x, y]\n}\n\nLABEL_FROM_MAP = {\n    \'XML\': LabelFrom.XML,\n    \'LMDB\': LabelFrom.LMDB,\n    \'FileName\': LabelFrom.FileName,\n    \'TXT\': LabelFrom.TXT\n}\n\nEXCEPT_FORMAT_MAP = {\n    ModelField.Image: \'png\',\n    ModelField.Text: \'csv\'\n}\n\nMODEL_FIELD_MAP = {\n    \'Image\': ModelField.Image,\n    \'Text\': ModelField.Text\n}\n\nOUTPUT_SHAPE1_MAP = {\n    CNNNetwork.CNN5: [16, 64],\n    CNNNetwork.CNNX: [8, 64],\n    CNNNetwork.ResNetTiny: [16, 1024],\n    CNNNetwork.ResNet50: [16, 2048],\n    CNNNetwork.DenseNet: [32, 2048],\n    CNNNetwork.MobileNetV2: [32, 1200]\n}\n\n\nclass DataAugmentationEntity:\n    binaryzation: object = -1\n    median_blur: int = -1\n    gaussian_blur: int = -1\n    equalize_hist: bool = False\n    laplace: bool = False\n    warp_perspective: bool = False\n    rotate: int = -1\n    sp_noise: float = -1.0\n    brightness: bool = False\n    saturation: bool = False\n    hue: bool = False\n    gamma: bool = False\n    channel_swap: bool = False\n    random_blank: int = -1\n    random_transition: int = -1\n    random_captcha: dict = {""Enable"": False, ""FontPath"": """"}\n\n\nclass PretreatmentEntity:\n    binaryzation: object = -1\n    concat_frames: object = -1\n    blend_frames: object = -1\n    replace_transparent: bool = True\n    horizontal_stitching: bool = False\n\n\nclass ModelConfig:\n    """"""MODEL""""""\n    model_root: dict\n    model_name: str\n    model_tag: str\n    model_field_param: str\n    model_scene_param: str\n\n    """"""SYSTEM""""""\n    system_root: dict\n    memory_usage: float\n    save_model: str\n    save_checkpoint: str\n\n    """"""FIELD PARAM - IMAGE""""""\n    field_root: dict\n    category_param: list or str\n    image_channel: int\n    image_width: int\n    image_height: int\n    resize: list\n    max_label_num: int\n    auto_padding: bool\n    output_split: str\n\n    """"""NEURAL NETWORK""""""\n    neu_network_root: dict\n    neu_cnn_param: str\n    neu_recurrent_param: str\n    units_num: int\n    neu_optimizer_param: str\n    output_layer: dict\n    loss_func_param: str\n    decoder: str\n\n    """"""LABEL""""""\n    label_root: dict\n    label_from_param: str\n    extract_regex: str\n    label_split: str\n\n    """"""PATH""""""\n    trains_root: dict\n    dataset_path_root: dict\n    source_path_root: dict\n    trains_path: dict = {DatasetType.TFRecords: [], DatasetType.Directory: []}\n    validation_path: dict = {DatasetType.TFRecords: [], DatasetType.Directory: []}\n    dataset_map = {\n        RunMode.Trains: trains_path,\n        RunMode.Validation: validation_path\n    }\n    validation_set_num: int\n\n    """"""TRAINS""""""\n    trains_save_steps: int\n    trains_validation_steps: int\n    trains_end_acc: float\n    trains_end_cost: float\n    trains_end_epochs: int\n    trains_learning_rate: float\n    batch_size: int\n    validation_batch_size: int\n\n    """"""DATA AUGMENTATION""""""\n    data_augmentation_root: dict\n    da_binaryzation: list\n    da_median_blur: int\n    da_gaussian_blur: int\n    da_equalize_hist: bool\n    da_laplace: bool\n    da_rotate: int\n    da_warp_perspective: bool\n    da_sp_noise: float\n    da_brightness: bool\n    da_saturation: bool\n    da_hue: bool\n    da_gamma: bool\n    da_channel_swap: bool\n    da_random_blank: int\n    da_random_transition: int\n    da_random_captcha: dict = {""Enable"": False, ""FontPath"": """"}\n\n    """"""PRETREATMENT""""""\n    pretreatment_root: dict\n    pre_binaryzation: int\n    pre_replace_transparent: bool\n    pre_horizontal_stitching: bool\n    pre_concat_frames: object\n    pre_blend_frames: object\n\n    """"""COMPILE_MODEL""""""\n    compile_model_path: str\n\n    def __init__(self, project_name, project_path=None, is_dev=True, **argv):\n        self.is_dev = is_dev\n        self.project_path = project_path if project_path else ""./projects/{}"".format(project_name)\n        self.output_path = os.path.join(self.project_path, \'out\')\n        self.compile_conf_path = os.path.join(self.output_path, \'model\')\n        self.compile_conf_path = os.path.join(self.compile_conf_path, ""{}_model.yaml"".format(project_name))\n        self.model_root_path = os.path.join(self.project_path, \'model\')\n        self.model_conf_path = os.path.join(self.project_path, MODEL_CONFIG_NAME)\n        self.dataset_root_path = os.path.join(self.project_path, \'dataset\')\n        self.checkpoint_tag = \'checkpoint\'\n\n        if not os.path.exists(self.project_path):\n            os.makedirs(self.project_path)\n\n        if not os.path.exists(self.model_root_path):\n            os.makedirs(self.model_root_path)\n\n        if not os.path.exists(self.output_path):\n            os.makedirs(self.output_path)\n\n        if not os.path.exists(self.dataset_root_path):\n            os.makedirs(self.dataset_root_path)\n\n        if len(argv) > 0:\n            self.new(**argv)\n        else:\n            self.read_conf()\n\n    def read_conf(self):\n        """"""MODEL""""""\n        self.model_root = self.conf[\'Model\']\n        self.model_name = self.model_root.get(\'ModelName\')\n        self.model_tag = \'{model_name}.model\'.format(model_name=self.model_name)\n\n        self.model_field_param = self.model_root.get(\'ModelField\')\n        self.model_scene_param = self.model_root.get(\'ModelScene\')\n\n        """"""SYSTEM""""""\n        self.system_root = self.conf[\'System\']\n        self.memory_usage = self.system_root.get(\'MemoryUsage\')\n        self.model_version = self.system_root.get(""Version"")\n        self.save_model = os.path.join(self.model_root_path, self.model_tag)\n        self.save_checkpoint = os.path.join(self.model_root_path, self.checkpoint_tag)\n\n        """"""FIELD PARAM - IMAGE""""""\n        self.field_root = self.conf[\'FieldParam\']\n        self.category_param = self.field_root.get(\'Category\')\n\n        self.image_channel = self.field_root.get(\'ImageChannel\')\n        self.image_width = self.field_root.get(\'ImageWidth\')\n        self.image_height = self.field_root.get(\'ImageHeight\')\n        self.resize = self.field_root.get(\'Resize\')\n        self.max_label_num = self.field_root.get(\'MaxLabelNum\')\n        self.auto_padding = self.field_root.get(\'AutoPadding\')\n        self.output_split = self.field_root.get(\'OutputSplit\')\n\n        """"""NEURAL NETWORK""""""\n        self.neu_network_root = self.conf[\'NeuralNet\']\n        self.neu_cnn_param = self.neu_network_root.get(\'CNNNetwork\')\n\n        self.neu_recurrent_param = self.neu_network_root.get(\'RecurrentNetwork\')\n        self.neu_recurrent_param = self.neu_recurrent_param if self.neu_recurrent_param else \'NoRecurrent\'\n\n        self.units_num = self.neu_network_root.get(\'UnitsNum\')\n        self.neu_optimizer_param = self.neu_network_root.get(\'Optimizer\')\n        self.neu_optimizer_param = self.neu_optimizer_param if self.neu_optimizer_param else \'RAdam\'\n\n        self.output_layer = self.neu_network_root.get(\'OutputLayer\')\n        self.loss_func_param = self.output_layer.get(\'LossFunction\')\n\n        self.decoder = self.output_layer.get(\'Decoder\')\n\n        """"""LABEL""""""\n        self.label_root = self.conf.get(\'Label\')\n        self.label_from_param = self.label_root.get(\'LabelFrom\')\n        self.extract_regex = self.label_root.get(\'ExtractRegex\')\n        self.extract_regex = self.extract_regex if self.extract_regex else "".*?(?=_)""\n        self.label_split = self.label_root.get(\'LabelSplit\')\n\n        """"""PATH""""""\n        self.trains_root = self.conf[\'Trains\']\n\n        self.dataset_path_root = self.trains_root.get(\'DatasetPath\')\n        self.trains_path[DatasetType.TFRecords]: list = self.dataset_path_root.get(\'Training\')\n        self.validation_path[DatasetType.TFRecords]: list = self.dataset_path_root.get(\'Validation\')\n\n        self.source_path_root = self.trains_root.get(\'SourcePath\')\n        self.trains_path[DatasetType.Directory]: list = self.source_path_root.get(\'Training\')\n        self.validation_path[DatasetType.Directory]: list = self.source_path_root.get(\'Validation\')\n\n        self.validation_set_num: int = self.trains_root.get(\'ValidationSetNum\')\n        # self.validation_set_num = self.validation_set_num if self.validation_set_num else 500\n\n        """"""TRAINS""""""\n        self.trains_save_steps = self.trains_root.get(\'SavedSteps\')\n        self.trains_validation_steps = self.trains_root.get(\'ValidationSteps\')\n        self.trains_end_acc = self.trains_root.get(\'EndAcc\')\n        self.trains_end_cost = self.trains_root.get(\'EndCost\')\n        self.trains_end_cost = self.trains_end_cost if self.trains_end_cost else 1\n        self.trains_end_epochs = self.trains_root.get(\'EndEpochs\')\n        self.trains_end_epochs = self.trains_end_epochs if self.trains_end_epochs else 2\n        self.trains_learning_rate = self.trains_root.get(\'LearningRate\')\n        self.batch_size = self.trains_root.get(\'BatchSize\')\n        self.batch_size = self.batch_size if self.batch_size else 64\n        self.validation_batch_size = self.trains_root.get(\'ValidationBatchSize\')\n        self.validation_batch_size = self.validation_batch_size if self.validation_batch_size else 300\n\n        """"""DATA AUGMENTATION""""""\n        self.data_augmentation_root = self.conf[\'DataAugmentation\']\n        self.da_binaryzation = self.data_augmentation_root.get(\'Binaryzation\')\n        self.da_median_blur = self.data_augmentation_root.get(\'MedianBlur\')\n        self.da_gaussian_blur = self.data_augmentation_root.get(\'GaussianBlur\')\n        self.da_equalize_hist = self.data_augmentation_root.get(\'EqualizeHist\')\n        self.da_laplace = self.data_augmentation_root.get(\'Laplace\')\n        self.da_rotate = self.data_augmentation_root.get(\'Rotate\')\n        self.da_warp_perspective = self.data_augmentation_root.get(\'WarpPerspective\')\n        self.da_sp_noise = self.data_augmentation_root.get(\'PepperNoise\')\n        self.da_brightness = self.data_augmentation_root.get(\'Brightness\')\n        self.da_saturation = self.data_augmentation_root.get(\'Saturation\')\n        self.da_hue = self.data_augmentation_root.get(\'Hue\')\n        self.da_gamma = self.data_augmentation_root.get(\'Gamma\')\n        self.da_channel_swap = self.data_augmentation_root.get(\'ChannelSwap\')\n        self.da_random_blank = self.data_augmentation_root.get(\'RandomBlank\')\n        self.da_random_transition = self.data_augmentation_root.get(\'RandomTransition\')\n        self.da_random_captcha = self.data_augmentation_root.get(\'RandomCaptcha\')\n        if not self.da_random_captcha:\n            self.da_random_captcha = {""Enable"": False, ""FontPath"": """"}\n\n        """"""PRETREATMENT""""""\n        self.pretreatment_root = self.conf[\'Pretreatment\']\n        self.pre_binaryzation = self.pretreatment_root.get(\'Binaryzation\')\n        self.pre_replace_transparent = self.pretreatment_root.get(""ReplaceTransparent"")\n        self.pre_horizontal_stitching = self.pretreatment_root.get(""HorizontalStitching"")\n        self.pre_concat_frames = self.pretreatment_root.get(\'ConcatFrames\')\n        self.pre_blend_frames = self.pretreatment_root.get(\'BlendFrames\')\n\n        """"""COMPILE_MODEL""""""\n        self.compile_model_path = os.path.join(self.output_path, \'graph\')\n        self.compile_model_path = self.compile_model_path.replace(""\\\\"", ""/"")\n        self.check_field()\n\n    @property\n    def model_field(self) -> ModelField:\n        return ModelConfig.param_convert(\n            source=self.model_field_param,\n            param_map=MODEL_FIELD_MAP,\n            text=""Current model field ({model_field}) is not supported"".format(model_field=self.model_field_param),\n            code=ConfigException.MODEL_FIELD_NOT_SUPPORTED\n        )\n\n    @property\n    def model_scene(self) -> ModelScene:\n        return ModelConfig.param_convert(\n            source=self.model_scene_param,\n            param_map=MODEL_SCENE_MAP,\n            text=""Current model scene ({model_scene}) is not supported"".format(model_scene=self.model_scene_param),\n            code=ConfigException.MODEL_SCENE_NOT_SUPPORTED\n        )\n\n    @property\n    def neu_cnn(self) -> CNNNetwork:\n        return ModelConfig.param_convert(\n            source=self.neu_cnn_param,\n            param_map=NETWORK_MAP,\n            text=""This cnn layer ({param}) is not supported at this time."".format(param=self.neu_cnn_param),\n            code=ConfigException.NETWORK_NOT_SUPPORTED\n        )\n\n    @property\n    def neu_recurrent(self) -> RecurrentNetwork:\n        return ModelConfig.param_convert(\n            source=self.neu_recurrent_param,\n            param_map=NETWORK_MAP,\n            text=""Current recurrent layer ({recurrent}) is not supported"".format(recurrent=self.neu_recurrent_param),\n            code=ConfigException.NETWORK_NOT_SUPPORTED\n        )\n\n    @property\n    def neu_optimizer(self) -> Optimizer:\n        return ModelConfig.param_convert(\n            source=self.neu_optimizer_param,\n            param_map=OPTIMIZER_MAP,\n            text=""This optimizer ({param}) is not supported at this time."".format(param=self.neu_optimizer_param),\n            code=ConfigException.NETWORK_NOT_SUPPORTED\n        )\n\n    @property\n    def loss_func(self) -> LossFunction:\n        return ModelConfig.param_convert(\n            source=self.loss_func_param,\n            param_map=LOSS_FUNC_MAP,\n            text=""This type of loss function ({loss}) is not supported at this time."".format(loss=self.loss_func_param),\n            code=ConfigException.LOSS_FUNC_NOT_SUPPORTED,\n        )\n\n    @property\n    def label_from(self) -> LabelFrom:\n        return ModelConfig.param_convert(\n            source=self.label_from_param,\n            param_map=LABEL_FROM_MAP,\n            text=""This type of label from ({lf}) is not supported at this time."".format(lf=self.label_from_param),\n            code=ConfigException.ERROR_LABEL_FROM,\n        )\n\n    @property\n    def category(self) -> list:\n        category_value = category_extract(self.category_param)\n        return SPACE_TOKEN + category_value\n\n    @property\n    def category_num(self) -> int:\n        return len(self.category)\n\n    @staticmethod\n    def param_convert(source, param_map: dict, text, code, default=None):\n        if source is None:\n            return default\n        if source not in param_map.keys():\n            exception(text, code)\n        return param_map[source]\n\n    def check_field(self):\n\n        if not os.path.exists(self.model_conf_path):\n            exception(\n                \'Configuration File ""{}"" No Found. \'\n                \'If it is used for the first time, please copy one according to model.template as {}\'.format(\n                    MODEL_CONFIG_NAME,\n                    MODEL_CONFIG_NAME\n                ), ConfigException.MODEL_CONFIG_PATH_NOT_EXIST\n            )\n        if not os.path.exists(self.model_root_path):\n            os.makedirs(self.model_root_path)\n\n        model_file = ModelConfig.checkpoint(self.model_name, self.model_root_path)\n        checkpoint = \'model_checkpoint_path: {}\\nall_model_checkpoint_paths: {}\'.format(model_file, model_file)\n        with open(self.save_checkpoint, \'w\') as f:\n            f.write(checkpoint)\n\n    @staticmethod\n    def checkpoint(_name, _path):\n        file_list = os.listdir(_path)\n        checkpoint_group = [\n            \'""{}""\'.format(i.split("".meta"")[0]) for i in file_list if\n            _name + "".model"" in i and i.endswith(\'.meta\')\n        ]\n        if not checkpoint_group:\n            return None\n        checkpoint_step = [int(re.search(\'(?<=model-).*?(?="")\', i).group()) for i in checkpoint_group]\n        return checkpoint_group[checkpoint_step.index(max(checkpoint_step))]\n\n    @property\n    def conf(self) -> dict:\n        with open(self.model_conf_path if self.is_dev else self.compile_conf_path, \'r\', encoding=""utf-8"") as sys_fp:\n            sys_stream = sys_fp.read()\n            return yaml.load(sys_stream, Loader=yaml.SafeLoader)\n\n    @staticmethod\n    def list_param(params, intent=6):\n        if params is None:\n            params = []\n        if isinstance(params, str):\n            params = [params]\n        result = """".join([""\\n{}- "".format(\' \' * intent) + i for i in params])\n        return result\n\n    @staticmethod\n    def dict_param(params: dict, intent=6):\n        if params is None:\n            params = {}\n        result = """".join([""\\n{} "".format(\' \' * intent) + ""{}: {}"".format(k, v) for k, v in params.items()])\n        return result\n\n    @staticmethod\n    def val_filter(val):\n        if isinstance(val, str) and len(val) == 1:\n            val = ""\'{}\'"".format(val)\n        elif val is None:\n            val = \'null\'\n        return val\n\n    def update(self, model_conf_path=None, model_name=None):\n        with open(""model.template"", encoding=""utf8"") as f:\n            base_config = """".join(f.readlines())\n            model = base_config.format(\n                MemoryUsage=self.memory_usage,\n                CNNNetwork=self.neu_cnn.value,\n                RecurrentNetwork=self.val_filter(self.neu_recurrent_param),\n                UnitsNum=self.units_num,\n                Optimizer=self.neu_optimizer.value,\n                LossFunction=self.loss_func.value,\n                Decoder=self.decoder,\n                ModelName=model_name if model_name else self.model_name,\n                ModelField=self.model_field.value,\n                ModelScene=self.model_scene.value,\n                Category=self.category_param,\n                Resize=json.dumps(self.resize),\n                ImageChannel=self.image_channel,\n                ImageWidth=self.image_width,\n                ImageHeight=self.image_height,\n                MaxLabelNum=self.max_label_num,\n                AutoPadding=self.auto_padding,\n                OutputSplit=self.val_filter(self.output_split),\n                LabelFrom=self.label_from.value,\n                ExtractRegex=self.val_filter(self.extract_regex),\n                LabelSplit=self.val_filter(self.label_split),\n                DatasetTrainsPath=self.list_param(self.trains_path[DatasetType.TFRecords], intent=6),\n                DatasetValidationPath=self.list_param(self.validation_path[DatasetType.TFRecords], intent=6),\n                SourceTrainPath=self.list_param(self.trains_path[DatasetType.Directory], intent=6),\n                SourceValidationPath=self.list_param(self.validation_path[DatasetType.Directory], intent=6),\n                ValidationSetNum=self.validation_set_num,\n                SavedSteps=self.trains_save_steps,\n                ValidationSteps=self.trains_validation_steps,\n                EndAcc=self.trains_end_acc,\n                EndCost=self.trains_end_cost,\n                EndEpochs=self.trains_end_epochs,\n                BatchSize=self.batch_size,\n                ValidationBatchSize=self.validation_batch_size,\n                LearningRate=self.trains_learning_rate,\n                DA_Binaryzation=self.da_binaryzation,\n                DA_MedianBlur=self.da_median_blur,\n                DA_GaussianBlur=self.da_gaussian_blur,\n                DA_EqualizeHist=self.da_equalize_hist,\n                DA_Laplace=self.da_laplace,\n                DA_WarpPerspective=self.da_warp_perspective,\n                DA_Rotate=self.da_rotate,\n                DA_PepperNoise=self.da_sp_noise,\n                DA_Brightness=self.da_brightness,\n                DA_Saturation=self.da_saturation,\n                DA_Hue=self.da_hue,\n                DA_Gamma=self.da_gamma,\n                DA_ChannelSwap=self.da_channel_swap,\n                DA_RandomBlank=self.da_random_blank,\n                DA_RandomTransition=self.da_random_transition,\n                DA_RandomCaptcha=self.dict_param(self.da_random_captcha, intent=4),\n                Pre_Binaryzation=self.pre_binaryzation,\n                Pre_ReplaceTransparent=self.pre_replace_transparent,\n                Pre_HorizontalStitching=self.pre_horizontal_stitching,\n                Pre_ConcatFrames=self.pre_concat_frames,\n                Pre_BlendFrames=self.pre_blend_frames,\n            )\n        with open(model_conf_path if model_conf_path else self.model_conf_path, ""w"", encoding=""utf8"") as f:\n            f.write(model)\n\n    def output_config(self, target_model_name=None):\n        compiled_config_dir_path = os.path.join(self.output_path, ""model"")\n        if not os.path.exists(compiled_config_dir_path):\n            os.makedirs(compiled_config_dir_path)\n        compiled_config_path = os.path.join(compiled_config_dir_path, ""{}_model.yaml"".format(self.model_name))\n        self.update(model_conf_path=compiled_config_path, model_name=target_model_name)\n\n    def dataset_increasing_name(self, mode: RunMode):\n        dataset_group = os.listdir(self.dataset_root_path)\n        if len(dataset_group) < 1:\n            return ""Trains.0.tfrecords"" if mode == RunMode.Trains else ""Validation.0.tfrecords""\n        name_split = [i.split(""."") for i in dataset_group if mode.value in i]\n        last_index = max([int(i[1]) for i in name_split])\n        current_index = last_index + 1\n        name_prefix = name_split[0][0]\n        name_suffix = name_split[0][2]\n        return ""{}.{}.{}"".format(name_prefix, current_index, name_suffix)\n\n    def new(self, **argv):\n        self.memory_usage = argv.get(\'MemoryUsage\')\n        self.neu_cnn_param = argv.get(\'CNNNetwork\')\n        self.neu_recurrent_param = argv.get(\'RecurrentNetwork\')\n        self.units_num = argv.get(\'UnitsNum\')\n        self.neu_optimizer_param = argv.get(\'Optimizer\')\n        self.loss_func_param = argv.get(\'LossFunction\')\n        self.decoder = argv.get(\'Decoder\')\n        self.model_name = argv.get(\'ModelName\')\n        self.model_field_param = argv.get(\'ModelField\')\n        self.model_scene_param = argv.get(\'ModelScene\')\n\n        if isinstance(argv.get(\'Category\'), list):\n            self.category_param = json.dumps(argv.get(\'Category\'), ensure_ascii=False)\n        else:\n            self.category_param = argv.get(\'Category\')\n\n        self.resize = argv.get(\'Resize\')\n        self.image_channel = argv.get(\'ImageChannel\')\n        self.image_width = argv.get(\'ImageWidth\')\n        self.image_height = argv.get(\'ImageHeight\')\n        self.max_label_num = argv.get(\'MaxLabelNum\')\n        self.auto_padding = argv.get(\'AutoPadding\')\n        self.output_split = argv.get(\'OutputSplit\')\n        self.label_from_param = argv.get(\'LabelFrom\')\n        self.extract_regex = argv.get(\'ExtractRegex\')\n        self.label_split = argv.get(\'LabelSplit\')\n        self.trains_path[DatasetType.TFRecords] = argv.get(\'DatasetTrainsPath\')\n        self.validation_path[DatasetType.TFRecords] = argv.get(\'DatasetValidationPath\')\n        self.trains_path[DatasetType.Directory] = argv.get(\'SourceTrainPath\')\n        self.validation_path[DatasetType.Directory] = argv.get(\'SourceValidationPath\')\n        self.validation_set_num = argv.get(\'ValidationSetNum\')\n        self.trains_save_steps = argv.get(\'SavedSteps\')\n        self.trains_validation_steps = argv.get(\'ValidationSteps\')\n        self.trains_end_acc = argv.get(\'EndAcc\')\n        self.trains_end_cost = argv.get(\'EndCost\')\n        self.trains_end_epochs = argv.get(\'EndEpochs\')\n        self.batch_size = argv.get(\'BatchSize\')\n        self.validation_batch_size = argv.get(\'ValidationBatchSize\')\n        self.trains_learning_rate = argv.get(\'LearningRate\')\n        self.da_binaryzation = argv.get(\'DA_Binaryzation\')\n        self.da_median_blur = argv.get(\'DA_MedianBlur\')\n        self.da_gaussian_blur = argv.get(\'DA_GaussianBlur\')\n        self.da_equalize_hist = argv.get(\'DA_EqualizeHist\')\n        self.da_laplace = argv.get(\'DA_Laplace\')\n        self.da_warp_perspective = argv.get(\'DA_WarpPerspective\')\n        self.da_rotate = argv.get(\'DA_Rotate\')\n        self.da_sp_noise = argv.get(\'DA_PepperNoise\')\n        self.da_brightness = argv.get(\'DA_Brightness\')\n        self.da_saturation = argv.get(\'DA_Saturation\')\n        self.da_hue = argv.get(\'DA_Hue\')\n        self.da_gamma = argv.get(\'DA_Gamma\')\n        self.da_channel_swap = argv.get(\'DA_ChannelSwap\')\n        self.da_random_blank = argv.get(\'DA_RandomBlank\')\n        self.da_random_transition = argv.get(\'DA_RandomTransition\')\n        self.da_random_captcha = argv.get(\'DA_RandomCaptcha\')\n        self.pre_binaryzation = argv.get(\'Pre_Binaryzation\')\n        self.pre_replace_transparent = argv.get(\'Pre_ReplaceTransparent\')\n        self.pre_horizontal_stitching = argv.get(\'Pre_HorizontalStitching\')\n        self.pre_concat_frames = argv.get(\'Pre_ConcatFrames\')\n        self.pre_blend_frames = argv.get(\'Pre_BlendFrames\')\n\n    def println(self):\n        print(\'Loading Configuration...\')\n        print(\'---------------------------------------------------------------------------------\')\n        print(""PROJECT_PATH"", self.project_path)\n        print(\'MODEL_PATH:\', self.save_model)\n        print(\'COMPILE_MODEL_PATH:\', self.compile_model_path)\n        print(\'CATEGORY_NUM:\', self.category_num)\n        print(\'IMAGE_WIDTH: {}, IMAGE_HEIGHT: {}\'.format(\n            self.image_width, self.image_height)\n        )\n        print(\'NEURAL NETWORK: {}\'.format(self.neu_network_root))\n\n        print(\'---------------------------------------------------------------------------------\')\n\n\nif __name__ == \'__main__\':\n    name = ""demo""\n    c = ModelConfig(project_name=name)\n    c.println()\n    c.update()\n'"
constants.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nfrom enum import Enum, unique\n\n\n@unique\nclass ModelType(Enum):\n    """"""\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xb1\xbb\xe5\x88\xab\xe6\x9e\x9a\xe4\xb8\xbe""""""\n    PB = \'PB\'\n    ONNX = \'ONNX\'\n    TFLITE = \'TFLITE\'\n\n\n@unique\nclass DatasetType(Enum):\n    """"""\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\xb1\xbb\xe5\x88\xab\xe6\x9e\x9a\xe4\xb8\xbe""""""\n    Directory = \'Directory\'\n    TFRecords = \'TFRecords\'\n\n\n@unique\nclass LabelFrom(Enum):\n    """"""\xe6\xa0\x87\xe7\xad\xbe\xe6\x9d\xa5\xe6\xba\x90\xe6\x9e\x9a\xe4\xb8\xbe""""""\n    XML = \'XML\'\n    LMDB = \'LMDB\'\n    FileName = \'FileName\'\n    TXT = \'TXT\'\n\n\n@unique\nclass LossFunction(Enum):\n    """"""\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe6\x9e\x9a\xe4\xb8\xbe""""""\n    CrossEntropy = \'CrossEntropy\'\n    CTC = \'CTC\'\n\n\n@unique\nclass ModelScene(Enum):\n    """"""\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x9c\xba\xe6\x99\xaf\xe6\x9e\x9a\xe4\xb8\xbe""""""\n    Classification = \'Classification\'\n\n\n@unique\nclass ModelField(Enum):\n    """"""\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xb1\xbb\xe5\x88\xab\xe6\x9e\x9a\xe4\xb8\xbe""""""\n    Image = \'Image\'\n    Text = \'Text\'\n\n\n@unique\nclass RunMode(Enum):\n    """"""\xe8\xbf\x90\xe8\xa1\x8c\xe6\xa8\xa1\xe5\xbc\x8f\xe6\x9e\x9a\xe4\xb8\xbe""""""\n    Validation = \'Validation\'\n    Trains = \'Trains\'\n    Predict = \'Predict\'\n\n\n@unique\nclass CNNNetwork(Enum):\n    """"""\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe6\x9e\x9a\xe4\xb8\xbe""""""\n    CNNX = \'CNNX\'\n    CNN5 = \'CNN5\'\n    ResNetTiny = \'ResNetTiny\'\n    ResNet50 = \'ResNet50\'\n    DenseNet = \'DenseNet\'\n    MobileNetV2 = \'MobileNetV2\'\n\n\n@unique\nclass RecurrentNetwork(Enum):\n    """"""\xe5\xbe\xaa\xe7\x8e\xaf\xe5\xb1\x82\xe6\x9e\x9a\xe4\xb8\xbe""""""\n    NoRecurrent = \'NoRecurrent\'\n    GRU = \'GRU\'\n    BiGRU = \'BiGRU\'\n    GRUcuDNN = \'GRUcuDNN\'\n    LSTM = \'LSTM\'\n    BiLSTM = \'BiLSTM\'\n    LSTMcuDNN = \'LSTMcuDNN\'\n    BiLSTMcuDNN = \'BiLSTMcuDNN\'\n\n\n@unique\nclass Optimizer(Enum):\n    """"""\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\x9e\x9a\xe4\xb8\xbe""""""\n    RAdam = \'RAdam\'\n    Adam = \'Adam\'\n    Momentum = \'Momentum\'\n    AdaBound = \'AdaBound\'\n    SGD = \'SGD\'\n    AdaGrad = \'AdaGrad\'\n    RMSProp = \'RMSProp\'\n\n\n@unique\nclass SimpleCharset(Enum):\n    """"""\xe7\xae\x80\xe5\x8d\x95\xe5\xad\x97\xe7\xac\xa6\xe5\x88\x86\xe7\xb1\xbb\xe6\x9e\x9a\xe4\xb8\xbe""""""\n    NUMERIC = \'NUMERIC\'\n    ALPHANUMERIC = \'ALPHANUMERIC\'\n    ALPHANUMERIC_LOWER = \'ALPHANUMERIC_LOWER\'\n    ALPHANUMERIC_UPPER = \'ALPHANUMERIC_UPPER\'\n    ALPHABET_LOWER = \'ALPHABET_LOWER\'\n    ALPHABET_UPPER = \'ALPHABET_UPPER\'\n    ALPHABET = \'ALPHABET\'\n    ARITHMETIC = \'ARITHMETIC\'\n    FLOAT = \'FLOAT\'\n    CHS_3500 = \'CHS_3500\'\n    ALPHANUMERIC_CHS_3500_LOWER = \'ALPHANUMERIC_CHS_3500_LOWER\'\n\n'"
core.py,18,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport sys\nfrom config import RecurrentNetwork, RESIZE_MAP, CNNNetwork, Optimizer\nfrom network.CNN import *\nfrom network.MobileNet import MobileNetV2\nfrom network.DenseNet import DenseNet\nfrom network.GRU import GRU, BiGRU, GRUcuDNN\nfrom network.LSTM import LSTM, BiLSTM, BiLSTMcuDNN, LSTMcuDNN\nfrom network.ResNet import ResNet50, ResNetTiny\nfrom network.utils import NetworkUtils\nfrom optimizer.AdaBound import AdaBoundOptimizer\nfrom optimizer.RAdam import RAdamOptimizer\nfrom loss import *\nfrom encoder import *\nfrom decoder import *\nfrom fc import *\n\nimport tensorflow as tf\n\n\nclass NeuralNetwork(object):\n\n    """"""\n    \xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9e\x84\xe5\xbb\xba\xe7\xb1\xbb\n    """"""\n    def __init__(self, model_conf: ModelConfig, mode: RunMode, backbone: CNNNetwork, recurrent: RecurrentNetwork):\n        """"""\n\n        :param model_conf: \xe6\xa8\xa1\xe5\x9e\x8b\xe9\x85\x8d\xe7\xbd\xae\n        :param mode: \xe8\xbf\x90\xe8\xa1\x8c\xe6\xa8\xa1\xe5\xbc\x8f (Trains/Validation/Predict)\n        :param backbone:\n        :param recurrent:\n        """"""\n        self.model_conf = model_conf\n        self.decoder = Decoder(self.model_conf)\n        self.mode = mode\n        self.network = backbone\n        self.recurrent = recurrent\n        self.inputs = tf.keras.Input(dtype=tf.float32, shape=self.input_shape, name=\'input\')\n        self.labels = tf.keras.Input(dtype=tf.int32, shape=[None], sparse=True, name=\'labels\')\n        self.utils = NetworkUtils(mode)\n        self.merged_summary = None\n        self.optimizer = None\n        self.dataset_size = None\n\n    @property\n    def input_shape(self):\n        """"""\n        :return: tuple/list \xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84 Shape\n        """"""\n        return RESIZE_MAP[self.model_conf.loss_func](*self.model_conf.resize) + [self.model_conf.image_channel]\n\n    def build_graph(self):\n        """"""\n        \xe5\x9c\xa8\xe5\xbd\x93\xe5\x89\x8dSession\xe4\xb8\xad\xe6\x9e\x84\xe5\xbb\xba\xe7\xbd\x91\xe7\xbb\x9c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\n        """"""\n        self._build_model()\n\n    def build_train_op(self, dataset_size=None):\n        self.dataset_size = dataset_size\n        self._build_train_op()\n        self.merged_summary = tf.compat.v1.summary.merge_all()\n\n    def _build_model(self):\n\n        """"""\xe9\x80\x89\xe6\x8b\xa9\xe9\x87\x87\xe7\x94\xa8\xe5\x93\xaa\xe7\xa7\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c""""""\n        if self.network == CNNNetwork.CNN5:\n            x = CNN5(model_conf=self.model_conf, inputs=self.inputs, utils=self.utils).build()\n\n        elif self.network == CNNNetwork.CNNX:\n            x = CNNX(model_conf=self.model_conf, inputs=self.inputs, utils=self.utils).build()\n\n        elif self.network == CNNNetwork.ResNetTiny:\n            x = ResNetTiny(model_conf=self.model_conf, inputs=self.inputs, utils=self.utils).build()\n\n        elif self.network == CNNNetwork.ResNet50:\n            x = ResNet50(model_conf=self.model_conf, inputs=self.inputs, utils=self.utils).build()\n\n        elif self.network == CNNNetwork.DenseNet:\n            x = DenseNet(model_conf=self.model_conf, inputs=self.inputs, utils=self.utils).build()\n\n        elif self.network == CNNNetwork.MobileNetV2:\n            x = MobileNetV2(model_conf=self.model_conf, inputs=self.inputs, utils=self.utils).build()\n\n        else:\n            raise ValueError(\'This cnn neural network is not supported at this time.\')\n\n        """"""\xe9\x80\x89\xe6\x8b\xa9\xe9\x87\x87\xe7\x94\xa8\xe5\x93\xaa\xe7\xa7\x8d\xe5\xbe\xaa\xe7\x8e\xaf\xe7\xbd\x91\xe7\xbb\x9c""""""\n\n        # time_major = True: [max_time_step, batch_size, num_classes]\n        tf.compat.v1.logging.info(""CNN Output: {}"".format(x.get_shape()))\n\n        self.seq_len = tf.fill([tf.shape(x)[0]], tf.shape(x)[1], name=""seq_len"")\n\n        if self.recurrent == RecurrentNetwork.NoRecurrent:\n            self.recurrent_network_builder = None\n        elif self.recurrent == RecurrentNetwork.LSTM:\n            self.recurrent_network_builder = LSTM(model_conf=self.model_conf, inputs=x, utils=self.utils)\n        elif self.recurrent == RecurrentNetwork.BiLSTM:\n            self.recurrent_network_builder = BiLSTM(model_conf=self.model_conf, inputs=x, utils=self.utils)\n        elif self.recurrent == RecurrentNetwork.GRU:\n            self.recurrent_network_builder = GRU(model_conf=self.model_conf, inputs=x, utils=self.utils)\n        elif self.recurrent == RecurrentNetwork.BiGRU:\n            self.recurrent_network_builder = BiGRU(model_conf=self.model_conf, inputs=x, utils=self.utils)\n        elif self.recurrent == RecurrentNetwork.LSTMcuDNN:\n            self.recurrent_network_builder = LSTMcuDNN(model_conf=self.model_conf, inputs=x, utils=self.utils)\n        elif self.recurrent == RecurrentNetwork.BiLSTMcuDNN:\n            self.recurrent_network_builder = BiLSTMcuDNN(model_conf=self.model_conf, inputs=x, utils=self.utils)\n        elif self.recurrent == RecurrentNetwork.GRUcuDNN:\n            self.recurrent_network_builder = GRUcuDNN(model_conf=self.model_conf, inputs=x, utils=self.utils)\n        else:\n            raise ValueError(\'This recurrent neural network is not supported at this time.\')\n\n        logits = self.recurrent_network_builder.build() if self.recurrent_network_builder else x\n        if self.recurrent_network_builder and self.model_conf.loss_func != LossFunction.CTC:\n            raise ValueError(\'CTC loss must use recurrent neural network.\')\n\n        """"""\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xaeLoss\xe5\x87\xbd\xe6\x95\xb0\xe5\x8c\xba\xe5\x88\x86""""""\n        with tf.keras.backend.name_scope(\'output\'):\n            if self.model_conf.loss_func == LossFunction.CTC:\n                self.outputs = FullConnectedRNN(model_conf=self.model_conf, outputs=logits).build()\n            elif self.model_conf.loss_func == LossFunction.CrossEntropy:\n                self.outputs = FullConnectedCNN(model_conf=self.model_conf, outputs=logits).build()\n            return self.outputs\n\n    @property\n    def decay_steps(self):\n        if not self.dataset_size:\n            return 10000\n        epoch_step = int(self.dataset_size / self.model_conf.batch_size)\n        return int(epoch_step / 4)\n\n    def _build_train_op(self):\n        """"""\xe6\x9e\x84\xe5\xbb\xba\xe8\xae\xad\xe7\xbb\x83\xe6\x93\x8d\xe4\xbd\x9c\xe7\xac\xa6""""""\n\n        # \xe6\xad\xa5\xe6\x95\xb0\n        self.global_step = tf.train.get_or_create_global_step()\n\n        # Loss\xe5\x87\xbd\xe6\x95\xb0\n        if self.model_conf.loss_func == LossFunction.CTC:\n            self.loss = Loss.ctc(\n                labels=self.labels,\n                logits=self.outputs,\n                sequence_length=self.seq_len\n            )\n        elif self.model_conf.loss_func == LossFunction.CrossEntropy:\n            self.loss = Loss.cross_entropy(\n                labels=self.labels,\n                logits=self.outputs\n            )\n\n        self.cost = tf.reduce_mean(self.loss)\n\n        tf.compat.v1.summary.scalar(\'cost\', self.cost)\n\n        # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87 \xe6\x8c\x87\xe6\x95\xb0\xe8\xa1\xb0\xe5\x87\x8f\xe6\xb3\x95\n        self.lrn_rate = tf.compat.v1.train.exponential_decay(\n            self.model_conf.trains_learning_rate,\n            self.global_step,\n            staircase=True,\n            decay_steps=self.decay_steps,\n            decay_rate=0.98,\n        )\n        tf.compat.v1.summary.scalar(\'learning_rate\', self.lrn_rate)\n\n        if self.model_conf.neu_optimizer == Optimizer.AdaBound:\n            self.optimizer = AdaBoundOptimizer(\n                learning_rate=self.lrn_rate,\n                final_lr=0.001,\n                beta1=0.9,\n                beta2=0.999,\n                amsbound=True\n            )\n        elif self.model_conf.neu_optimizer == Optimizer.Adam:\n            self.optimizer = tf.train.AdamOptimizer(\n                learning_rate=self.lrn_rate\n            )\n        elif self.model_conf.neu_optimizer == Optimizer.RAdam:\n            self.optimizer = RAdamOptimizer(\n                learning_rate=self.lrn_rate,\n                warmup_proportion=0.1,\n                min_lr=1e-6\n            )\n        elif self.model_conf.neu_optimizer == Optimizer.Momentum:\n            self.optimizer = tf.train.MomentumOptimizer(\n                learning_rate=self.lrn_rate,\n                use_nesterov=True,\n                momentum=0.9,\n            )\n        elif self.model_conf.neu_optimizer == Optimizer.SGD:\n            self.optimizer = tf.train.GradientDescentOptimizer(\n                learning_rate=self.lrn_rate,\n            )\n        elif self.model_conf.neu_optimizer == Optimizer.AdaGrad:\n            self.optimizer = tf.train.AdagradOptimizer(\n                learning_rate=self.lrn_rate,\n            )\n        elif self.model_conf.neu_optimizer == Optimizer.RMSProp:\n            self.optimizer = tf.train.RMSPropOptimizer(\n                learning_rate=self.lrn_rate,\n            )\n\n        # BN \xe6\x93\x8d\xe4\xbd\x9c\xe7\xac\xa6\xe6\x9b\xb4\xe6\x96\xb0(moving_mean, moving_variance)\n        update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n        # \xe5\xb0\x86 train_op \xe5\x92\x8c update_ops \xe8\x9e\x8d\xe5\x90\x88\n        with tf.control_dependencies(update_ops):\n            self.train_op = self.optimizer.minimize(\n                    loss=self.cost,\n                    global_step=self.global_step,\n            )\n\n        # \xe8\xbd\xac\xe5\xbd\x95\xe5\xb1\x82-Loss\xe5\x87\xbd\xe6\x95\xb0\n        if self.model_conf.loss_func == LossFunction.CTC:\n            self.dense_decoded = self.decoder.ctc(\n                inputs=self.outputs,\n                sequence_length=self.seq_len\n            )\n        elif self.model_conf.loss_func == LossFunction.CrossEntropy:\n            self.dense_decoded = self.decoder.cross_entropy(\n                inputs=self.outputs\n            )\n\n\nif __name__ == \'__main__\':\n    pass\n'"
decoder.py,3,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport tensorflow as tf\nfrom config import ModelConfig\n\n\nclass Decoder:\n    """"""\n    \xe8\xbd\xac\xe5\xbd\x95\xe5\xb1\x82\xef\xbc\x9a\xe7\x94\xa8\xe4\xba\x8e\xe8\xa7\xa3\xe7\xa0\x81\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n    """"""\n    def __init__(self, model_conf: ModelConfig):\n        self.model_conf = model_conf\n        self.category_num = self.model_conf.category_num\n\n    def ctc(self, inputs, sequence_length):\n        """"""\xe9\x92\x88\xe5\xaf\xb9CTC Loss\xe7\x9a\x84\xe8\xa7\xa3\xe7\xa0\x81""""""\n        ctc_decode, _ = tf.nn.ctc_greedy_decoder(inputs, sequence_length)\n        decoded_sequences = tf.sparse.to_dense(ctc_decode[0], default_value=self.category_num, name=\'dense_decoded\')\n        return decoded_sequences\n\n    @staticmethod\n    def cross_entropy(inputs):\n        """"""\xe9\x92\x88\xe5\xaf\xb9CrossEntropy Loss\xe7\x9a\x84\xe8\xa7\xa3\xe7\xa0\x81""""""\n        return tf.argmax(inputs, 2, name=\'dense_decoded\')\n\n'"
encoder.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport io\nimport re\nimport cv2\nimport random\nimport PIL.Image\nimport numpy as np\nimport tensorflow as tf\nfrom exception import *\nfrom constants import RunMode\nfrom config import ModelConfig, LabelFrom, LossFunction\nfrom category import encode_maps, FULL_ANGLE_MAP\nfrom pretreatment import preprocessing\nfrom tools.gif_frames import concat_frames, blend_frame\n\n\nclass Encoder(object):\n    """"""\n    \xe7\xbc\x96\xe7\xa0\x81\xe5\xb1\x82\xef\xbc\x9a\xe7\x94\xa8\xe4\xba\x8e\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe8\xbe\x93\xe5\x85\xa5\xe7\xbc\x96\xe7\xa0\x81\xe4\xb8\xba\xe5\x8f\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n    """"""\n    def __init__(self, model_conf: ModelConfig, mode: RunMode):\n        self.model_conf = model_conf\n        self.mode = mode\n        self.category_param = self.model_conf.category_param\n\n    def image(self, path_or_bytes):\n        """"""\xe9\x92\x88\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe7\xb1\xbb\xe5\x9e\x8b\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe7\xbc\x96\xe7\xa0\x81""""""\n        # im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n        # The OpenCV cannot handle gif format images, it will return None.\n        # if im is None:\n\n        path_or_stream = io.BytesIO(path_or_bytes) if isinstance(path_or_bytes, bytes) else path_or_bytes\n        if not path_or_stream:\n            return ""Picture is corrupted: {}"".format(path_or_bytes)\n        try:\n            pil_image = PIL.Image.open(path_or_stream)\n        except OSError as e:\n            return ""{} - {}"".format(e, path_or_bytes)\n\n        if pil_image.mode == \'P\':\n            pil_image = pil_image.convert(\'RGB\')\n\n        rgb = pil_image.split()\n        if len(rgb) == 1 and self.model_conf.image_channel == 3:\n            return ""The number of image channels {} is inconsistent with the number of configured channels {}."".format(\n                len(rgb), self.model_conf.image_channel\n            )\n\n        size = pil_image.size\n\n        gif_handle = self.model_conf.pre_concat_frames != -1 or self.model_conf.pre_blend_frames != -1\n\n        if len(rgb) > 3 and self.model_conf.pre_replace_transparent and not gif_handle:\n            background = PIL.Image.new(\'RGBA\', pil_image.size, (255, 255, 255))\n            background.paste(pil_image, (0, 0, size[0], size[1]), pil_image)\n            background.convert(\'RGB\')\n            pil_image = background\n\n        if self.model_conf.pre_concat_frames != -1:\n            im = concat_frames(pil_image, need_frame=self.model_conf.pre_concat_frames)\n        elif self.model_conf.pre_blend_frames != -1:\n            im = blend_frame(pil_image, need_frame=self.model_conf.pre_blend_frames)\n        else:\n            im = np.array(pil_image)\n\n        if isinstance(im, list):\n            return None\n\n        if self.model_conf.image_channel == 1 and len(im.shape) == 3:\n            if self.mode == RunMode.Trains:\n                im = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY if bool(random.getrandbits(1)) else cv2.COLOR_BGR2GRAY)\n            else:\n                im = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n\n        im = preprocessing(\n            image=im,\n            binaryzation=self.model_conf.pre_binaryzation,\n        )\n\n        if self.mode == RunMode.Trains and bool(random.getrandbits(1)):\n            im = preprocessing(\n                image=im,\n                binaryzation=self.model_conf.da_binaryzation,\n                median_blur=self.model_conf.da_median_blur,\n                gaussian_blur=self.model_conf.da_gaussian_blur,\n                equalize_hist=self.model_conf.da_equalize_hist,\n                laplacian=self.model_conf.da_laplace,\n                rotate=self.model_conf.da_rotate,\n                warp_perspective=self.model_conf.da_warp_perspective,\n                sp_noise=self.model_conf.da_sp_noise,\n                random_brightness=self.model_conf.da_brightness,\n                random_saturation=self.model_conf.da_saturation,\n                random_hue=self.model_conf.da_hue,\n                random_gamma=self.model_conf.da_gamma,\n                random_channel_swap=self.model_conf.da_channel_swap,\n                random_blank=self.model_conf.da_random_blank,\n                random_transition=self.model_conf.da_random_transition,\n            ).astype(np.float32)\n\n        else:\n            im = im.astype(np.float32)\n        if self.model_conf.resize[0] == -1:\n            # random_ratio = random.choice([2.5, 3, 3.5, 3.2, 2.7, 2.75])\n            ratio = self.model_conf.resize[1] / size[1]\n            # random_width = int(random_ratio * RESIZE[1])\n            resize_width = int(ratio * size[0])\n            # resize_width = random_width if is_random else resize_width\n            im = cv2.resize(im, (resize_width, self.model_conf.resize[1]))\n        else:\n            im = cv2.resize(im, (self.model_conf.resize[0], self.model_conf.resize[1]))\n        im = im.swapaxes(0, 1)\n\n        if self.model_conf.image_channel == 1:\n            return np.array((im[:, :, np.newaxis]) / 255.)\n        else:\n            return np.array(im[:, :]) / 255.\n\n    def text(self, content):\n        """"""\xe9\x92\x88\xe5\xaf\xb9\xe6\x96\x87\xe6\x9c\xac\xe7\xb1\xbb\xe5\x9e\x8b\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe7\xbc\x96\xe7\xa0\x81""""""\n        if isinstance(content, bytes):\n            content = content.decode(""utf8"")\n\n        found = content\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe5\x8c\xb9\xe9\x85\x8d\xe5\x86\x85\xe7\xbd\xae\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x86\x99\xe8\xa7\x84\xe8\x8c\x83\xef\xbc\x8c\xe8\xa7\xa6\xe5\x8f\x91\xe8\x87\xaa\xe5\x8a\xa8\xe8\xbd\xac\xe6\x8d\xa2\n        if isinstance(self.category_param, str) and \'_LOWER\' in self.category_param:\n            found = found.lower()\n        if isinstance(self.category_param, str) and \'_UPPER\' in self.category_param:\n            found = found.upper()\n\n        # \xe6\xa0\x87\xe7\xad\xbe\xe6\x98\xaf\xe5\x90\xa6\xe5\x8c\x85\xe5\x90\xab\xe5\x88\x86\xe9\x9a\x94\xe7\xac\xa6\n        if self.model_conf.label_split:\n            labels = found.split(self.model_conf.label_split)\n        elif self.model_conf.max_label_num == 1:\n            labels = [found]\n        else:\n            labels = [_ for _ in found]\n        labels = self.filter_full_angle(labels)\n        try:\n            if not labels:\n                return [0]\n            # \xe6\xa0\xb9\xe6\x8d\xae\xe7\xb1\xbb\xe5\x88\xab\xe9\x9b\x86\xe5\x90\x88\xe6\x89\xbe\xe5\x88\xb0\xe5\xaf\xb9\xe5\xba\x94\xe6\x98\xa0\xe5\xb0\x84\xe7\xbc\x96\xe7\xa0\x81\xe4\xb8\xbadense\xe6\x95\xb0\xe7\xbb\x84\n            if self.model_conf.loss_func == LossFunction.CTC:\n                label = self.split_continuous_char(\n                    [encode_maps(self.model_conf.category)[i] for i in labels]\n                )\n            else:\n                label = self.auto_padding_char(\n                    [encode_maps(self.model_conf.category)[i] for i in labels]\n                )\n            return label\n\n        except KeyError as e:\n            return dict(e=e, label=content, char=e.args[0])\n            # exception(\n            #     \'The sample label {} contains invalid charset: {}.\'.format(\n            #         content, e.args[0]\n            #     ), ConfigException.SAMPLE_LABEL_ERROR\n            # )\n\n    def split_continuous_char(self, content):\n        # \xe4\xb8\xba\xe8\xbf\x9e\xe7\xbb\xad\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe6\x8f\x92\xe5\x85\xa5\xe7\xa9\xba\xe7\x99\xbd\xe7\xac\xa6\n        store_list = []\n        # blank_char = [self.model_conf.category_num] if bool(random.getrandbits(1)) else [0]\n        blank_char = [self.model_conf.category_num]\n        for i in range(len(content) - 1):\n            store_list.append(content[i])\n            if content[i] == content[i + 1]:\n                store_list += blank_char\n        store_list.append(content[-1])\n        return store_list\n\n    def auto_padding_char(self, content):\n        if len(content) < self.model_conf.max_label_num and self.model_conf.auto_padding:\n            remain_label_num = self.model_conf.max_label_num - len(content)\n            return [0] * remain_label_num + content\n            # return content + [0] * remain_label_num\n        return content\n\n    @staticmethod\n    def filter_full_angle(content):\n        return [FULL_ANGLE_MAP.get(i) if i in FULL_ANGLE_MAP.keys() else i for i in content if i != \' \']\n\n\nif __name__ == \'__main__\':\n    pass\n\n\n\n'"
exception.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport sys\nimport time\n\n""""""\n\xe6\xad\xa4\xe7\xb1\xbb\xe5\x8c\x85\xe5\x90\xab\xe5\x90\x84\xe7\xa7\x8d\xe5\xbc\x82\xe5\xb8\xb8\xe7\xb1\xbb\xe5\x88\xab\xef\xbc\x8c\xe5\xb8\x8c\xe6\x9c\x9b\xe5\xaf\xb9\xe5\xb7\xb2\xe7\x9f\xa5\xe5\x8f\xaf\xe8\x83\xbd\xe7\x9a\x84\xe5\xbc\x82\xe5\xb8\xb8\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbe\xbf\xe5\x87\xba\xe7\x8e\xb0\xe9\x97\xae\xe9\xa2\x98\xe6\x98\xaf\xe6\x96\xb9\xe4\xbe\xbf\xe5\xae\x9a\xe4\xbd\x8d\n""""""\n\n\nclass SystemException(RuntimeError):\n    def __init__(self, message, code=-1):\n        self.message = message\n        self.code = code\n\n\nclass Error(object):\n    def __init__(self, message, code=-1):\n        self.message = message\n        self.code = code\n        print(self.message)\n        time.sleep(5)\n        sys.exit(self.code)\n\n\ndef exception(text, code=-1):\n    raise SystemException(text, code)\n    # Error(text, code)\n\n\nclass ConfigException:\n    OPTIMIZER_NOT_SUPPORTED = -4072\n    NETWORK_NOT_SUPPORTED = -4071\n    LOSS_FUNC_NOT_SUPPORTED = -4061\n    MODEL_FIELD_NOT_SUPPORTED = -4052\n    MODEL_SCENE_NOT_SUPPORTED = -4051\n    SYS_CONFIG_PATH_NOT_EXIST = -4041\n    MODEL_CONFIG_PATH_NOT_EXIST = -4042\n    CATEGORY_NOT_EXIST = -4043\n    CATEGORY_INCORRECT = -4043\n    SAMPLE_LABEL_ERROR = -4044\n    GET_LABEL_REGEX_ERROR = -4045\n    ERROR_LABEL_FROM = -4046\n    INSUFFICIENT_SAMPLE = -5\n    VALIDATION_SET_SIZE_ERROR = -6\n\n\n'"
fuse_model.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\n\nimport os\nimport re\nimport base64\nimport pickle\nfrom config import ModelConfig\nfrom constants import ModelType\nfrom config import COMPILE_MODEL_MAP\n\n\ndef parse_model(source_bytes: bytes, key=None):\n    split_tag = b\'-#||#-\'\n\n    if not key:\n        key = [b""_____"" + i.encode(""utf8"") + b""_____"" for i in ""&coriander""]\n    if isinstance(key, str):\n        key = [b""_____"" + i.encode(""utf8"") + b""_____"" for i in key]\n    key_len_int = len(key)\n    model_bytes_list = []\n    graph_bytes_list = []\n    slice_index = source_bytes.index(key[0])\n    split_tag_len = len(split_tag)\n    slice_0 = source_bytes[0: slice_index].split(split_tag)\n    model_slice_len = len(slice_0[1])\n    graph_slice_len = len(slice_0[0])\n    slice_len = split_tag_len + model_slice_len + graph_slice_len\n\n    for i in range(key_len_int-1):\n        slice_index = source_bytes.index(key[i])\n        print(slice_index, slice_index - slice_len)\n        slices = source_bytes[slice_index - slice_len: slice_index].split(split_tag)\n        model_bytes_list.append(slices[1])\n        graph_bytes_list.append(slices[0])\n    slices = source_bytes.split(key[-2])[1][:-len(key[-1])].split(split_tag)\n\n    model_bytes_list.append(slices[1])\n    graph_bytes_list.append(slices[0])\n    model_bytes = b"""".join(model_bytes_list)\n    model_conf: ModelConfig = pickle.loads(model_bytes)\n    graph_bytes: bytes = b"""".join(graph_bytes_list)\n    return model_conf, graph_bytes\n\n\ndef concat_model(output_path, model_bytes, graph_bytes, key=None):\n    if not key:\n        key = [b""_____"" + i.encode(""utf8"") + b""_____"" for i in ""&coriander""]\n    if isinstance(key, str):\n        key = [b""_____"" + i.encode(""utf8"") + b""_____"" for i in key]\n    key_len_int = len(key)\n    model_slice_len = int(len(model_bytes) / key_len_int) + 1\n    graph_slice_len = int(len(graph_bytes) / key_len_int) + 1\n    model_slice = [model_bytes[i:i + model_slice_len] for i in range(0, len(model_bytes), model_slice_len)]\n\n    graph_slice = [graph_bytes[i:i + graph_slice_len] for i in range(0, len(graph_bytes), graph_slice_len)]\n\n    new_model = []\n    for i in range(key_len_int):\n        new_model.append(graph_slice[i] + b\'-#||#-\')\n        new_model.append(model_slice[i])\n        new_model.append(key[i])\n    new_model = b"""".join(new_model)\n    with open(output_path, ""wb"") as f:\n        f.write(new_model)\n    print(""Successfully write to model {}"".format(output_path))\n\n\ndef output_model(project_name: str, model_type: ModelType, key=None):\n    model_conf = ModelConfig(project_name, is_dev=False)\n\n    graph_parent_path = model_conf.compile_model_path\n    model_suffix = COMPILE_MODEL_MAP[model_type]\n    model_bytes = pickle.dumps(model_conf.conf)\n    graph_path = os.path.join(graph_parent_path, ""{}{}"".format(model_conf.model_name, model_suffix))\n\n    with open(graph_path, ""rb"") as f:\n        graph_bytes = f.read()\n\n    output_path = graph_path.replace("".pb"", "".pl"").replace("".onnx"", "".pl"").replace("".tflite"", "".pl"")\n    concat_model(output_path, model_bytes, graph_bytes, key)\n\n\nif __name__ == \'__main__\':\n    output_model("""", ModelType.PB)\n\n'"
loss.py,8,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport tensorflow as tf\nfrom config import ModelConfig\n\n\nclass Loss(object):\n\n    """"""\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8""""""\n    @staticmethod\n    def cross_entropy(labels, logits):\n        """"""\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0""""""\n\n        # return tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n        # return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n        # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n        # return tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)\n        target = tf.sparse.to_dense(labels)\n        # target = labels\n        print(\'logits\', logits.shape)\n        print(\'target\', target.shape)\n\n        # logits = tf.reshape(tensor=logits, shape=[tf.shape(labels)[0], None])\n        return tf.keras.backend.sparse_categorical_crossentropy(\n            target=target,\n            output=logits,\n            from_logits=True,\n        )\n\n    @staticmethod\n    def ctc(labels, logits, sequence_length):\n        """"""CTC \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0""""""\n\n        return tf.nn.ctc_loss_v2(\n            labels=labels,\n            logits=logits,\n            logit_length=sequence_length,\n            label_length=sequence_length,\n            blank_index=-1,\n            logits_time_major=True\n        )\n'"
make_dataset.py,4,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport sys\nimport random\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom config import *\nfrom constants import RunMode\n\n_RANDOM_SEED = 0\n\n\nclass DataSets:\n\n    """"""\xe6\xad\xa4\xe7\xb1\xbb\xe7\x94\xa8\xe4\xba\x8e\xe6\x89\x93\xe5\x8c\x85\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xbaTFRecords\xe6\xa0\xbc\xe5\xbc\x8f""""""\n    def __init__(self, model: ModelConfig):\n        self.ignore_list = [""Thumbs.db"", "".DS_Store""]\n        self.model: ModelConfig = model\n        if not os.path.exists(self.model.dataset_root_path):\n            os.makedirs(self.model.dataset_root_path)\n\n    @staticmethod\n    def read_image(path):\n        """"""\n        \xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\n        :param path: \xe5\x9b\xbe\xe7\x89\x87\xe8\xb7\xaf\xe5\xbe\x84\n        :return:\n        """"""\n        with open(path, ""rb"") as f:\n            return f.read()\n\n    def dataset_exists(self):\n        """"""\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\x98\xaf\xe5\x90\xa6\xe5\xad\x98\xe5\x9c\xa8\xe5\x88\xa4\xe6\x96\xad\xe5\x87\xbd\xe6\x95\xb0""""""\n        for file in (self.model.trains_path[DatasetType.TFRecords] + self.model.validation_path[DatasetType.TFRecords]):\n            if not os.path.exists(file):\n                return False\n        return True\n\n    @staticmethod\n    def bytes_feature(values):\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n\n    def input_to_tfrecords(self, input_data, label):\n        return tf.train.Example(features=tf.train.Features(feature={\n            \'input\': self.bytes_feature(input_data),\n            \'label\': self.bytes_feature(label),\n        }))\n\n    def convert_dataset_from_filename(self, output_filename, file_list, mode: RunMode, is_add=False):\n        if is_add:\n            output_filename = self.model.dataset_increasing_name(mode)\n            if not output_filename:\n                raise FileNotFoundError(\'Basic data set missing, please check.\')\n            output_filename = os.path.join(self.model.dataset_root_path, output_filename)\n        with tf.io.TFRecordWriter(output_filename) as writer:\n            pbar = tqdm(file_list)\n            for i, file_name in enumerate(pbar):\n                try:\n                    if file_name.split(""/"")[-1] in self.ignore_list:\n                        continue\n                    image_data = self.read_image(file_name)\n                    try:\n                        labels = re.search(self.model.extract_regex, file_name.split(PATH_SPLIT)[-1])\n                    except re.error as e:\n                        print(\'error:\', e)\n                        return\n                    if labels:\n                        labels = labels.group()\n                    else:\n                        raise NameError(\'invalid filename {}\'.format(file_name))\n                    labels = labels.encode(\'utf-8\')\n\n                    example = self.input_to_tfrecords(image_data, labels)\n                    writer.write(example.SerializeToString())\n                    pbar.set_description(\'[Processing dataset %s] [filename: %s]\' % (mode, file_name))\n\n                except IOError as e:\n                    print(\'could not read:\', file_list[1])\n                    print(\'error:\', e)\n                    print(\'skip it \\n\')\n\n    def convert_dataset_from_txt(self, output_filename, file_path, label_lines, mode: RunMode, is_add=False):\n        if is_add:\n            output_filename = self.model.dataset_increasing_name(mode)\n            if not output_filename:\n                raise FileNotFoundError(\'Basic data set missing, please check.\')\n            output_filename = os.path.join(self.model.dataset_root_path, output_filename)\n        file_list, label_list = [], []\n        for line in label_lines:\n            filename, label = line.split("" "", 1)\n            label = label.replace(""\\n"", """")\n            label_list.append(label.encode(\'utf-8\'))\n            path = os.path.join(file_path, filename)\n            file_list.append(path)\n\n        if os.path.exists(output_filename):\n            print(\'\xe5\xb7\xb2\xe5\xad\x98\xe5\x9c\xa8, \xe8\xb7\xb3\xe8\xbf\x87\')\n            return\n\n        with tf.io.TFRecordWriter(output_filename) as writer:\n            pbar = tqdm(file_list)\n            for i, file_name in enumerate(pbar):\n                try:\n                    image_data = self.read_image(file_name)\n                    labels = label_list[i]\n                    example = self.input_to_tfrecords(image_data, labels)\n                    writer.write(example.SerializeToString())\n                    pbar.set_description(\'[Processing dataset %s] [filename: %s]\' % (mode, file_name))\n                except IOError as e:\n                    print(\'could not read:\', file_list[1])\n                    print(\'error:\', e)\n                    print(\'skip it \\n\')\n\n    @staticmethod\n    def merge_source(source):\n        if isinstance(source, list):\n            origin_dataset = []\n            for trains_path in source:\n                origin_dataset += [\n                    os.path.join(trains_path, trains).replace(""\\\\"", ""/"") for trains in os.listdir(trains_path)\n                ]\n        elif isinstance(source, str):\n            origin_dataset = [os.path.join(source, trains) for trains in os.listdir(source)]\n        else:\n            return\n        random.seed(0)\n        random.shuffle(origin_dataset)\n        return origin_dataset\n\n    def make_dataset(self, trains_path=None, validation_path=None, is_add=False, callback=None, msg=None):\n        if self.dataset_exists() and not is_add:\n            state = ""EXISTS""\n            if callback:\n                callback()\n            if msg:\n                msg(state)\n            return\n\n        if not self.model.dataset_path_root:\n            state = ""CONF_ERROR""\n            if callback:\n                callback()\n            if msg:\n                msg(state)\n            return\n\n        trains_path = trains_path if is_add else self.model.trains_path[DatasetType.Directory]\n        validation_path = validation_path if is_add else self.model.validation_path[DatasetType.Directory]\n\n        trains_path = [trains_path] if isinstance(trains_path, str) else trains_path\n        validation_path = [validation_path] if isinstance(validation_path, str) else validation_path\n\n        if validation_path and not is_add:\n            if self.model.label_from == LabelFrom.FileName:\n                trains_dataset = self.merge_source(trains_path)\n                validation_dataset = self.merge_source(validation_path)\n                self.convert_dataset_from_filename(\n                    self.model.validation_path[DatasetType.TFRecords][-1 if is_add else 0],\n                    validation_dataset,\n                    mode=RunMode.Validation,\n                    is_add=is_add,\n                )\n                self.convert_dataset_from_filename(\n                    self.model.trains_path[DatasetType.TFRecords][-1 if is_add else 0],\n                    trains_dataset,\n                    mode=RunMode.Trains,\n                    is_add=is_add,\n                )\n            elif self.model.label_from == LabelFrom.TXT:\n\n                train_label_file = os.path.join(os.path.dirname(trains_path[0]), ""train.txt"")\n                val_label_file = os.path.join(os.path.dirname(validation_path[0]), ""val.txt"")\n\n                with open(train_label_file, ""r"", encoding=""utf8"") as f_train:\n                    train_label_line = f_train.readlines()\n\n                with open(val_label_file, ""r"", encoding=""utf8"") as f_val:\n                    val_label_line = f_val.readlines()\n\n                self.convert_dataset_from_txt(\n                    self.model.validation_path[DatasetType.TFRecords][-1 if is_add else 0],\n                    label_lines=val_label_line,\n                    file_path=validation_path[0],\n                    mode=RunMode.Validation,\n                    is_add=is_add,\n                )\n                self.convert_dataset_from_txt(\n                    self.model.trains_path[DatasetType.TFRecords][-1 if is_add else 0],\n                    label_lines=train_label_line,\n                    file_path=trains_path[0],\n                    mode=RunMode.Trains,\n                    is_add=is_add,\n                )\n\n        else:\n            if self.model.label_from == LabelFrom.FileName:\n                origin_dataset = self.merge_source(trains_path)\n                trains_dataset = origin_dataset[self.model.validation_set_num:]\n                if self.model.validation_set_num > 0:\n                    validation_dataset = origin_dataset[:self.model.validation_set_num]\n                    self.convert_dataset_from_filename(\n                        self.model.validation_path[DatasetType.TFRecords][-1 if is_add else 0],\n                        validation_dataset,\n                        mode=RunMode.Validation,\n                        is_add=is_add\n                    )\n                elif self.model.validation_set_num < 0:\n                    self.convert_dataset_from_filename(\n                        self.model.validation_path[DatasetType.TFRecords][-1 if is_add else 0],\n                        trains_dataset,\n                        mode=RunMode.Validation,\n                        is_add=is_add\n                    )\n                self.convert_dataset_from_filename(\n                    self.model.trains_path[DatasetType.TFRecords][-1 if is_add else 0],\n                    trains_dataset,\n                    mode=RunMode.Trains,\n                    is_add=is_add\n                )\n            elif self.model.label_from == LabelFrom.TXT:\n\n                train_label_file = os.path.join(os.path.dirname(trains_path[0]), ""train.txt"")\n\n                if not os.path.exists(train_label_file):\n                    msg(""Train label file not found!"")\n                    if callback:\n                        callback()\n                    return\n\n                with open(train_label_file, ""r"", encoding=""utf8"") as f:\n                    sample_label_line = f.readlines()\n\n                random.shuffle(sample_label_line)\n\n                train_label_line = sample_label_line[self.model.validation_set_num:]\n                val_label_line = sample_label_line[:self.model.validation_set_num]\n\n                self.convert_dataset_from_txt(\n                    self.model.validation_path[DatasetType.TFRecords][-1 if is_add else 0],\n                    label_lines=val_label_line,\n                    file_path=trains_path[0],\n                    mode=RunMode.Validation,\n                    is_add=is_add,\n                )\n                self.convert_dataset_from_txt(\n                    self.model.trains_path[DatasetType.TFRecords][-1 if is_add else 0],\n                    label_lines=train_label_line,\n                    file_path=trains_path[0],\n                    mode=RunMode.Trains,\n                    is_add=is_add,\n                )\n\n        state = ""DONE""\n        if callback:\n            callback()\n        if msg:\n            msg(state)\n        return\n\n\nif __name__ == \'__main__\':\n    model_conf = ModelConfig(sys.argv[-1])\n    _dataset = DataSets(model_conf)\n    _dataset.make_dataset()\n'"
predict_testing.py,9,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\n""""""\xe6\xad\xa4\xe8\x84\x9a\xe6\x9c\xac\xe7\x94\xa8\xe4\xba\x8e\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe6\xa3\x80\xe9\xaa\x8c\xe8\xae\xad\xe7\xbb\x83\xe6\x95\x88\xe6\x9e\x9c\xe7\x9a\x84\xe8\x84\x9a\xe6\x9c\xac\xef\xbc\x8c\xe5\x8a\x9f\xe8\x83\xbd\xe4\xb8\xba\xef\xbc\x9a\xe9\x80\x9a\xe8\xbf\x87\xe5\x90\xaf\xe5\x8a\xa8\xe5\x8f\x82\xe6\x95\xb0\xe5\x8a\xa0\xe8\xbd\xbd\xe3\x80\x90\xe5\xb7\xa5\xe7\xa8\x8b\xe5\x90\x8d\xe3\x80\x91\xe4\xb8\xad\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b""""""\nimport random\nimport numpy as np\nimport tensorflow as tf\nfrom config import *\nfrom constants import RunMode\nfrom encoder import Encoder\nfrom core import NeuralNetwork\n\n# argv = sys.argv[1]\n\n\nclass Predict:\n    def __init__(self, project_name):\n        self.model_conf = ModelConfig(project_name=project_name)\n        self.encoder = Encoder(model_conf=self.model_conf, mode=RunMode.Predict)\n\n    def get_image_batch(self, img_bytes):\n        if not img_bytes:\n            return []\n        return [self.encoder.image(index) for index in [img_bytes]]\n\n    @staticmethod\n    def decode_maps(categories):\n        """"""\xe8\xa7\xa3\xe7\xa0\x81\xe5\x99\xa8""""""\n        return {index: category for index, category in enumerate(categories, 0)}\n\n    def predict_func(self, image_batch, _sess, dense_decoded, op_input):\n        """"""\xe9\xa2\x84\xe6\xb5\x8b\xe5\x87\xbd\xe6\x95\xb0""""""\n        dense_decoded_code = _sess.run(dense_decoded, feed_dict={\n            op_input: image_batch,\n        })\n        # print(dense_decoded_code)\n        decoded_expression = []\n        for item in dense_decoded_code:\n            expression = \'\'\n            # print(item)\n            if isinstance(item, int) or isinstance(item, np.int64):\n                item = [item]\n            for class_index in item:\n                if class_index == -1 or class_index == self.model_conf.category_num:\n                    expression += \'\'\n                else:\n                    expression += self.decode_maps(self.model_conf.category)[class_index]\n            decoded_expression.append(expression)\n        return \'\'.join(decoded_expression) if len(decoded_expression) > 1 else decoded_expression[0]\n\n    def testing(self, image_dir, limit=None):\n\n        graph = tf.Graph()\n        sess = tf.Session(\n            graph=graph,\n            config=tf.ConfigProto(\n                # allow_soft_placement=True,\n                # log_device_placement=True,\n                gpu_options=tf.GPUOptions(\n                    allocator_type=\'BFC\',\n                    # allow_growth=True,  # it will cause fragmentation.\n                    per_process_gpu_memory_fraction=0.1\n                ))\n        )\n\n        with sess.graph.as_default():\n\n            sess.run(tf.global_variables_initializer())\n            # tf.keras.backend.set_session(session=sess)\n\n            model = NeuralNetwork(\n                self.model_conf,\n                RunMode.Predict,\n                self.model_conf.neu_cnn,\n                self.model_conf.neu_recurrent\n            )\n            model.build_graph()\n            model.build_train_op()\n\n            saver = tf.train.Saver(var_list=tf.global_variables())\n\n            """"""\xe4\xbb\x8e\xe9\xa1\xb9\xe7\x9b\xae\xe4\xb8\xad\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe6\xac\xa1\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\x82\xe6\x95\xb0""""""\n            saver.restore(sess, tf.train.latest_checkpoint(self.model_conf.model_root_path))\n            # model.build_graph()\n            # _ = tf.import_graph_def(graph_def, name="""")\n\n        """"""\xe5\xae\x9a\xe4\xb9\x89\xe6\x93\x8d\xe4\xbd\x9c\xe7\xac\xa6""""""\n        dense_decoded_op = sess.graph.get_tensor_by_name(""dense_decoded:0"")\n        x_op = sess.graph.get_tensor_by_name(\'input:0\')\n        """"""\xe5\x9b\xba\xe5\xae\x9a\xe7\xbd\x91\xe7\xbb\x9c""""""\n        sess.graph.finalize()\n\n        true_count = 0\n        false_count = 0\n        """"""\n        \xe4\xbb\xa5\xe4\xb8\x8b\xe4\xb8\xba\xe6\xa0\xb9\xe6\x8d\xae\xe8\xb7\xaf\xe5\xbe\x84\xe8\xb0\x83\xe7\x94\xa8\xe9\xa2\x84\xe6\xb5\x8b\xe5\x87\xbd\xe6\x95\xb0\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84demo\n        """"""\n        # Fill in your own sample path\n        dir_list = os.listdir(image_dir)\n        random.shuffle(dir_list)\n        lines = []\n        for i, p in enumerate(dir_list):\n            n = os.path.join(image_dir, p)\n            if limit and i > limit:\n                break\n            with open(n, ""rb"") as f:\n                b = f.read()\n\n            batch = self.get_image_batch(b)\n            if not batch:\n                continue\n            st = time.time()\n            predict_text = self.predict_func(\n                batch,\n                sess,\n                dense_decoded_op,\n                x_op,\n            )\n            et = time.time()\n            # t = p.split(""."")[0].lower() == predict_text.lower()\n            # csv_output = ""{},{}"".format(p.split(""."")[0], predict_text)\n            # lines.append(csv_output)\n            # print(csv_output)\n            # is_mark = \'_\' in p\n            # p = p.replace(""\\\\"", ""/"")\n            label = re.search(self.model_conf.extract_regex, p.split(PATH_SPLIT)[-1])\n            label = label.group() if label else p.split(""."")[0]\n            # if is_mark:\n            if \'LOWER\' in self.model_conf.category_param:\n                label = label.lower()\n                t = label == predict_text.lower()\n            elif \'UPPER\' in self.model_conf.category_param:\n                label = label.upper()\n                t = label == predict_text.upper()\n            else:\n                t = label == predict_text\n            # Used to verify test sets\n            if t:\n                true_count += 1\n            else:\n                false_count += 1\n            print(i, p, label, predict_text, t, true_count / (true_count + false_count), (et-st) * 1000)\n            # else:\n            #     print(i, p, predict_text, true_count / (true_count + false_count), (et - st) * 1000)\n            # with open(""competition_format.csv"", ""w"", encoding=""utf8"") as f:\n            #     f.write(""\\n"".join(lines))\n        sess.close()\n\n\nif __name__ == \'__main__\':\n\n    predict = Predict(project_name=sys.argv[1])\n    predict.testing(image_dir=r""H:\\TrainSet\\*"", limit=None)\n\n\n'"
pretreatment.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport cv2\nimport random\nimport numpy as np\n\n\nclass Pretreatment(object):\n    """"""\n    \xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe5\x8a\x9f\xe8\x83\xbd\xe5\x87\xbd\xe6\x95\xb0\xe9\x9b\x86\xe5\x90\x88\xef\xbc\x88\xe7\x9b\xae\xe5\x89\x8d\xe4\xbb\x85\xe7\x94\xa8\xe4\xba\x8e\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe5\x90\xaf\xe5\x8a\xa8\xef\xbc\x89\n    """"""\n\n    def __init__(self, origin):\n        self.origin = origin\n\n    def get(self):\n        return self.origin\n\n    def binarization(self, value: object, modify=False) -> np.ndarray:\n        if isinstance(value, list) and len(value) == 2:\n            value = random.randint(value[0], value[1])\n        elif isinstance(value, int):\n            value = value if (0 < value < 255) else -1\n        if value == -1:\n            return self.origin\n        ret, _binarization = cv2.threshold(self.origin, value, 255, cv2.THRESH_BINARY)\n        if modify:\n            self.origin = _binarization\n        return _binarization\n\n    def median_blur(self, value, modify=False) -> np.ndarray:\n        if not value:\n            return self.origin\n        value = random.randint(0, value)\n        value = value + 1 if value % 2 == 0 else value\n        _smooth = cv2.medianBlur(self.origin, value)\n        if modify:\n            self.origin = _smooth\n        return _smooth\n\n    def gaussian_blur(self, value, modify=False) -> np.ndarray:\n        if not value:\n            return self.origin\n        value = random.randint(0, value)\n        value = value + 1 if value % 2 == 0 else value\n        _blur = cv2.GaussianBlur(self.origin, (value, value), 0)\n        if modify:\n            self.origin = _blur\n        return _blur\n\n    def equalize_hist(self, value, modify=False) -> np.ndarray:\n        if not value:\n            return self.origin\n        _equalize_hist = cv2.equalizeHist(self.origin)\n        if modify:\n            self.origin = _equalize_hist\n        return _equalize_hist\n\n    def laplacian(self, value, modify=False) -> np.ndarray:\n        if not value:\n            return self.origin\n        _laplacian = cv2.convertScaleAbs(cv2.Laplacian(self.origin, cv2.CV_16S, ksize=3))\n        if modify:\n            self.origin = _laplacian\n        return _laplacian\n\n    def rotate(self, value, modify=False) -> np.ndarray:\n        if not value:\n            return self.origin\n        size = self.origin.shape\n        scale = 1.0\n        height, width = size[0], size[1]\n        center = (width // 2, height // 2)\n\n        if bool(random.getrandbits(1)):\n            angle = random.choice([\n                    -10, -20, -30, -45, -50, -60, -75, -90, -95, -100,\n                    10, 20, 30, 45, 50, 60, 75, 90, 95, 100\n                ])\n        else:\n            angle = -random.randint(-value, value)\n\n        m = cv2.getRotationMatrix2D(center, angle, scale)\n        _rotate = cv2.warpAffine(self.origin, m, (width, height))\n        # angle = -random.randint(-value, value)\n        # if abs(angle) > 15:\n        #     _img = cv2.resize(self.origin, (width, int(height / 2)))\n        #     center = (width / 4, height / 4)\n        # else:\n        #     _img = cv2.resize(self.origin, (width, height))\n        #     center = (width / 2, height / 2)\n        # _img = cv2.resize(self.origin, (width, height))\n        # m = cv2.getRotationMatrix2D(center, angle, 1.0)\n        # _rotate = cv2.warpAffine(_img, m, (width, height), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n        if modify:\n            self.origin = _rotate\n        return _rotate\n\n    def warp_perspective(self, modify=False) -> np.ndarray:\n        size = self.origin.shape\n        height, width = size[0], size[1]\n        size0 = random.randint(3, 9)\n        size1 = random.randint(25, 30)\n        size2 = random.randint(23, 27)\n        size3 = random.randint(33, 37)\n        pts1 = np.float32([[0, 0], [0, size1], [size1, size1], [size1, 0]])\n        pts2 = np.float32([[size0, 0], [-size0, size1], [size2, size1], [size3, 0]])\n        is_random = bool(random.getrandbits(1))\n        param = (pts2, pts1) if is_random else (pts1, pts2)\n        warp_mat = cv2.getPerspectiveTransform(*param)\n        dst = cv2.warpPerspective(self.origin, warp_mat, (width, height))\n        if modify:\n            self.origin = dst\n        return dst\n\n    def sp_noise(self, prob, modify=False):\n        size = self.origin.shape\n        output = np.zeros(self.origin.shape, np.uint8)\n        thres = 1 - prob\n        for i in range(size[0]):\n            for j in range(size[1]):\n                rdn = random.random()\n                if rdn < prob:\n                    output[i][j] = 0\n                elif rdn > thres:\n                    output[i][j] = 255\n                else:\n                    output[i][j] = self.origin[i][j]\n        if modify:\n            self.origin = output\n        return output\n\n    def random_brightness(self, modify=False):\n        beta = np.random.uniform(-84, 84)\n        output = np.uint8(np.clip((self.origin + beta), 0, 255))\n        if modify:\n            self.origin = output\n        return output\n\n    def random_saturation(self, modify=False):\n        if len(self.origin.shape) < 3:\n            return self.origin\n        factor = np.random.uniform(0.3, 2.0)\n        output = self.origin\n        output[:, :, 1] = np.clip(output[:, :, 1] * factor, 0, 255)\n        if modify:\n            self.origin = output\n        return output\n\n    def random_hue(self, max_delta=18, modify=False):\n        if len(self.origin.shape) < 3:\n            return self.origin\n        delta = np.random.uniform(-max_delta, max_delta)\n        output = self.origin\n        output[:, :, 0] = (output[:, :, 0] + delta) % 180.0\n        if modify:\n            self.origin = output\n        return output\n\n    def random_gamma(self, modify=False):\n        if len(self.origin.shape) < 3:\n            return self.origin\n        gamma = np.random.uniform(0.25, 2.0)\n        gamma_inv = 1.0 / gamma\n        table = np.array([((i / 255.0) ** gamma_inv) * 255 for i in np.arange(0, 256)]).astype(""uint8"")\n        output = cv2.LUT(self.origin, table)\n        if modify:\n            self.origin = output\n        return output\n\n    def random_channel_swap(self, modify=False):\n        if len(self.origin.shape) < 3:\n            return self.origin\n        permutations = ((0, 2, 1),\n                        (1, 0, 2), (1, 2, 0),\n                        (2, 0, 1), (2, 1, 0))\n        i = np.random.randint(5)\n        order = permutations[i]\n        output = self.origin[:, :, order]\n        if modify:\n            self.origin = output\n        return output\n\n    def random_blank(self, max_int, modify=False):\n        if len(self.origin.shape) < 2:\n            return self.origin\n        corp_range_w = random.randint(0, max_int)\n        corp_range_h = random.randint(0, max_int)\n        output = self.origin\n        random_p_h = -corp_range_h if bool(random.getrandbits(1)) else corp_range_h\n        random_v_h = 255 if bool(random.getrandbits(1)) else 0\n        random_p_w = -corp_range_w if bool(random.getrandbits(1)) else corp_range_w\n        random_v_w = 255 if bool(random.getrandbits(1)) else 0\n        if len(self.origin.shape) < 3:\n            output[random_p_h, :] = 255 if bool(random.getrandbits(1)) else random_v_h\n            output[:, random_p_w] = 255 if bool(random.getrandbits(1)) else random_v_w\n        else:\n            output[random_p_h, :, :] = 255 if bool(random.getrandbits(1)) else random_v_h\n            output[:, random_p_w, :] = 255 if bool(random.getrandbits(1)) else random_v_w\n        if modify:\n            self.origin = output\n        return output\n\n    def random_transition(self, max_int, modify=False):\n        size = self.origin.shape\n        height, width = size[0], size[1]\n        corp_range_w = random.randint(0, max_int)\n        corp_range_h = random.randint(0, max_int)\n        m = np.float32([[1, 0, corp_range_w], [0, 1, corp_range_h]])\n        random_color = random.randint(240, 255)\n        random_color = (random_color, random_color, random_color) if bool(random.getrandbits(1)) else (0, 0, 0)\n        output = cv2.warpAffine(self.origin, m, (width, height), borderValue=random_color)\n        if modify:\n            self.origin = output\n        return output\n\n\ndef preprocessing(\n        image,\n        binaryzation=-1,\n        median_blur=-1,\n        gaussian_blur=-1,\n        equalize_hist=False,\n        laplacian=False,\n        warp_perspective=False,\n        sp_noise=-1.0,\n        rotate=-1,\n        random_blank=-1,\n        random_transition=-1,\n        random_brightness=False,\n        random_gamma=False,\n        random_channel_swap=False,\n        random_saturation=False,\n        random_hue=False,\n):\n    """"""\n    \xe5\x90\x84\xe7\xa7\x8d\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe5\x90\xa6\xe5\x90\xaf\xe7\x94\xa8\xe5\x8f\x8a\xe5\x8f\x82\xe6\x95\xb0\xe9\x85\x8d\xe7\xbd\xae\n    :param random_transition: bool, \xe9\x9a\x8f\xe6\x9c\xba\xe4\xbd\x8d\xe7\xa7\xbb\n    :param random_blank: bool, \xe9\x9a\x8f\xe6\x9c\xba\xe5\xa1\xab\xe5\x85\x85\xe7\xa9\xba\xe7\x99\xbd\n    :param random_brightness: bool, \xe9\x9a\x8f\xe6\x9c\xba\xe4\xba\xae\xe5\xba\xa6\n    :param image: numpy\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe7\xbb\x84,\n    :param binaryzation: list-int\xe6\x95\xb0\xe5\xad\x97\xe8\x8c\x83\xe5\x9b\xb4, \xe4\xba\x8c\xe5\x80\xbc\xe5\x8c\x96\n    :param median_blur: int\xe6\x95\xb0\xe5\xad\x97,\n    :param gaussian_blur: int\xe6\x95\xb0\xe5\xad\x97,\n    :param equalize_hist: bool,\n    :param laplacian: bool, \xe6\x8b\x89\xe6\x99\xae\xe6\x8b\x89\xe6\x96\xaf\n    :param warp_perspective: bool, \xe9\x80\x8f\xe8\xa7\x86\xe5\x8f\x98\xe5\xbd\xa2\n    :param sp_noise: \xe6\xb5\xae\xe7\x82\xb9, \xe6\xa4\x92\xe7\x9b\x90\xe5\x99\xaa\xe5\xa3\xb0\n    :param rotate: \xe6\x95\xb0\xe5\xad\x97, \xe6\x97\x8b\xe8\xbd\xac\n    :return:\n    """"""\n    pretreatment = Pretreatment(image)\n    if rotate > 0 and bool(random.getrandbits(1)):\n        pretreatment.rotate(rotate, True)\n    if random_transition != -1 and bool(random.getrandbits(1)):\n        pretreatment.random_transition(5, True)\n    if 0 < sp_noise < 1 and bool(random.getrandbits(1)):\n        pretreatment.sp_noise(sp_noise, True)\n    if binaryzation != -1 and bool(random.getrandbits(1)):\n        pretreatment.binarization(binaryzation, True)\n    if median_blur != -1 and bool(random.getrandbits(1)):\n        pretreatment.median_blur(median_blur, True)\n    if gaussian_blur != -1 and bool(random.getrandbits(1)):\n        pretreatment.gaussian_blur(gaussian_blur, True)\n    if equalize_hist and bool(random.getrandbits(1)):\n        pretreatment.equalize_hist(True, True)\n    if laplacian and bool(random.getrandbits(1)):\n        pretreatment.laplacian(True, True)\n    if warp_perspective and bool(random.getrandbits(1)):\n        pretreatment.warp_perspective(True)\n    if random_brightness and bool(random.getrandbits(1)):\n        pretreatment.random_brightness(True)\n    if random_blank != -1 and bool(random.getrandbits(1)):\n        pretreatment.random_blank(2, True)\n    if random_gamma and bool(random.getrandbits(1)):\n        pretreatment.random_gamma(True)\n    if random_channel_swap and bool(random.getrandbits(1)):\n        pretreatment.random_channel_swap(True)\n    if random_saturation and bool(random.getrandbits(1)):\n        pretreatment.random_saturation(True)\n    if random_hue and bool(random.getrandbits(1)):\n        pretreatment.random_hue(18, True)\n    return pretreatment.get()\n\n\nif __name__ == \'__main__\':\n    import io\n    import os\n    import PIL.Image\n    import random\n    root_dir = r""H:\\img""\n    name = random.choice(os.listdir(root_dir))\n    # name = ""3956_b8cee4da-3530-11ea-9778-c2f9192435fa.png""\n    path = os.path.join(root_dir, name)\n    with open(path, ""rb"") as f:\n        path_or_bytes = f.read()\n    path_or_stream = io.BytesIO(path_or_bytes)\n    pil_image = PIL.Image.open(path_or_stream).convert(""L"")\n    im = np.array(pil_image)\n    im = preprocessing(\n        image=im,\n        binaryzation=150,\n        sp_noise=0.05,\n    ).astype(np.float32)\n    # im = im.swapaxes(0, 1)\n    # im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    cv_img = cv2.imencode(\'.png\', im)[1]\n    img_bytes = bytes(bytearray(cv_img))\n    with open(r""1.jpg"", ""wb"") as f:\n        f.write(img_bytes)\n'"
tf_graph_util.py,5,"b'# This file is adapted from https://github.com/tensorflow/tensorflow/blob/master\n# /tensorflow/python/framework/graph_util_impl.py\n#\n# Copyright 2015 The TensorFlow Authors, 2019 Analytics Zoo Authors.\n# All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Helpers to manipulate a tensor graph in python.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport copy\nimport re\nimport six\n\nfrom tensorflow.core.framework import attr_value_pb2\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.framework import node_def_pb2\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import deprecation\nfrom tensorflow.python.util.tf_export import tf_export\n\n_VARIABLE_OPS = {\n    ""Assign"",\n    ""AssignAdd"",\n    ""AssignSub"",\n    ""Queue"",\n    ""ScatterAdd"",\n    ""ScatterSub"",\n    ""ScatterUpdate"",\n    ""TruncatedNormal"",\n    ""Variable"",\n    ""VariableV2"",\n}\n\n\ndef _is_variable_op(op):\n    """"""Returns true if \'op\' refers to a Variable node.""""""\n    return op in _VARIABLE_OPS\n\n\n@deprecation.deprecated(\n    date=None,\n    instructions=""Use `tf.compat.v1.graph_util.must_run_on_cpu`"")\n@tf_export(v1=[""graph_util.must_run_on_cpu""])\ndef must_run_on_cpu(node, pin_variables_on_cpu=False):\n    """"""Returns True if the given node_def must run on CPU, otherwise False.\n    Args:\n      node: The node to be assigned to a device. Could be either an ops.Operation\n        or NodeDef.\n      pin_variables_on_cpu: If True, this function will return False if node_def\n        represents a variable-related op.\n    Returns:\n      True if the given node must run on CPU, otherwise False.\n    """"""\n\n    if isinstance(node, ops.Operation):\n        node_def = node.node_def\n    else:\n        assert isinstance(node, node_def_pb2.NodeDef)\n        node_def = node\n\n    # If the op is a variable-related op, should we pin it on CPU?\n    if pin_variables_on_cpu and _is_variable_op(node_def.op):\n        return True\n\n    # Constant operations producing a string or int32 must run on CPU.\n    if node_def.op == ""Const"":\n        # Get the value of the \'dtype\' attr\n        dtype = node_def.attr[""dtype""].type\n        if dtype == dtypes.string or dtype == dtypes.int32:\n            return True\n\n    if node_def.op in [""DynamicStitch"", ""ParallelDynamicStitch""]:\n        dtype = node_def.attr[""T""].type\n        if dtype == dtypes.int32:\n            # DynamicStitch on GPU only works for int32 values.\n            return True\n\n    if node_def.op in [""Cast""]:\n        dtype = node_def.attr[""SrcT""].type\n        if dtype == dtypes.int32:\n            # Cast on GPU does not works for int32 values.\n            return True\n    return False\n\n\n################################################################################\n#\n# device functions for use in with g.device(...)\n#\n################################################################################\n\n\ndef _node_name(n):\n    if n.startswith(""^""):\n        return n[1:]\n    else:\n        return n.split("":"")[0]\n\n\ndef _extract_graph_summary(graph_def):\n    """"""Extracts useful information from the graph and returns them.""""""\n    name_to_input_name = {}  # Keyed by the dest node name.\n    name_to_node = {}  # Keyed by node name.\n\n    # Keeps track of node sequences. It is important to still output the\n    # operations in the original order.\n    name_to_seq_num = {}  # Keyed by node name.\n    seq = 0\n    for node in graph_def.node:\n        n = _node_name(node.name)\n        name_to_node[n] = node\n        name_to_input_name[n] = [_node_name(x) for x in node.input]\n        if ""_class"" in node.attr:\n            # Prevent colocated nodes being lost\n            for v in node.attr[""_class""].list.s:\n                v_str = v.decode(""utf-8"")\n                if v_str.startswith(""loc:@""):\n                    colocated_node = v_str[5:]\n                    name_to_input_name[n].append(colocated_node)\n        name_to_seq_num[n] = seq\n        seq += 1\n    return name_to_input_name, name_to_node, name_to_seq_num\n\n\ndef _assert_nodes_are_present(name_to_node, nodes):\n    """"""Assert that nodes are present in the graph.""""""\n    for d in nodes:\n        assert d in name_to_node, ""%s is not in graph"" % d\n\n\ndef _bfs_for_reachable_nodes(target_nodes, name_to_input_name):\n    """"""Breadth first search for reachable nodes from target nodes.""""""\n    nodes_to_keep = set()\n    # Breadth first search to find all the nodes that we should keep.\n    next_to_visit = target_nodes[:]\n    while next_to_visit:\n        node = next_to_visit[0]\n        del next_to_visit[0]\n        if node in nodes_to_keep:\n            # Already visited this node.\n            continue\n        nodes_to_keep.add(node)\n        if node in name_to_input_name:\n            next_to_visit += name_to_input_name[node]\n    return nodes_to_keep\n\n\n@deprecation.deprecated(\n    date=None,\n    instructions=""Use `tf.compat.v1.graph_util.extract_sub_graph`"")\n@tf_export(v1=[""graph_util.extract_sub_graph""])\ndef extract_sub_graph(graph_def, dest_nodes):\n    """"""Extract the subgraph that can reach any of the nodes in \'dest_nodes\'.\n    Args:\n      graph_def: A graph_pb2.GraphDef proto.\n      dest_nodes: A list of strings specifying the destination node names.\n    Returns:\n      The GraphDef of the sub-graph.\n    Raises:\n      TypeError: If \'graph_def\' is not a graph_pb2.GraphDef proto.\n    """"""\n\n    if not isinstance(graph_def, graph_pb2.GraphDef):\n        raise TypeError(""graph_def must be a graph_pb2.GraphDef proto."")\n\n    if isinstance(dest_nodes, six.string_types):\n        raise TypeError(""dest_nodes must be a list."")\n\n    name_to_input_name, name_to_node, name_to_seq_num = _extract_graph_summary(\n        graph_def)\n    _assert_nodes_are_present(name_to_node, dest_nodes)\n\n    nodes_to_keep = _bfs_for_reachable_nodes(dest_nodes, name_to_input_name)\n\n    nodes_to_keep_list = sorted(\n        list(nodes_to_keep), key=lambda n: name_to_seq_num[n])\n    # Now construct the output GraphDef\n    out = graph_pb2.GraphDef()\n    for n in nodes_to_keep_list:\n        out.node.extend([copy.deepcopy(name_to_node[n])])\n    out.library.CopyFrom(graph_def.library)\n    out.versions.CopyFrom(graph_def.versions)\n\n    return out\n\n\n@deprecation.deprecated(\n    date=None,\n    instructions=""Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`""\n)\n@tf_export(v1=[""graph_util.tensor_shape_from_node_def_name""])\ndef tensor_shape_from_node_def_name(graph, input_name):\n    """"""Convenience function to get a shape from a NodeDef\'s input string.""""""\n    # To get a tensor, the name must be in the form <input>:<port>, for example\n    # \'Mul:0\'. The GraphDef input strings don\'t always have the port specified\n    # though, so if there isn\'t a colon we need to add a default \':0\' to the end.\n    if "":"" not in input_name:\n        canonical_name = input_name + "":0""\n    else:\n        canonical_name = input_name\n    tensor = graph.get_tensor_by_name(canonical_name)\n    shape = tensor.get_shape()\n    return shape\n\n\n@deprecation.deprecated(\n    date=None,\n    instructions=""Use `tf.compat.v1.graph_util.convert_variables_to_constants`"")\n@tf_export(v1=[""graph_util.convert_variables_to_constants""])\ndef convert_variables_to_constants(sess,\n                                   input_graph_def,\n                                   output_node_names,\n                                   variable_names_whitelist=None,\n                                   variable_names_blacklist=None):\n    """"""Replaces all the variables in a graph with constants of the same values.\n    If you have a trained graph containing Variable ops, it can be convenient to\n    convert them all to Const ops holding the same values. This makes it possible\n    to describe the network fully with a single GraphDef file, and allows the\n    removal of a lot of ops related to loading and saving the variables.\n    Args:\n      sess: Active TensorFlow session containing the variables.\n      input_graph_def: GraphDef object holding the network.\n      output_node_names: List of name strings for the result nodes of the graph.\n      variable_names_whitelist: The set of variable names to convert (by default,\n                                all variables are converted).\n      variable_names_blacklist: The set of variable names to omit converting\n                                to constants.\n    Returns:\n      GraphDef containing a simplified version of the original.\n    """"""\n\n    def trace_back_find_variable(origin_name, name_to_nodes):\n\n        nodes_in_path = set()\n        control_ops = [""Enter"", ""Exit"", ""NextIteration"", ""Switch""]\n\n        current_name = origin_name\n        while name_to_nodes[current_name].op != ""VarHandleOp"":\n            nodes_in_path.add(current_name)\n            current_node = name_to_nodes[current_name]\n            op_name = current_node.op\n            if op_name in control_ops or op_name == ""Identity"":\n                curr_input_name = _node_name(current_node.input[0])\n            else:\n                raise ValueError(""Op type %s should not be in the path "" +\n                                 ""between ReadVariableOp and VarHandleOp"" % current_node.op)\n            current_name = curr_input_name\n\n        return current_name, nodes_in_path\n\n    def create_const_op(node_name, dtype, data, data_shape=None):\n        """"""Creates a Const op.""""""\n        output_node = node_def_pb2.NodeDef()\n        output_node.op = ""Const""\n        output_node.name = node_name\n        output_node.attr[""dtype""].CopyFrom(dtype)\n        output_node.attr[""value""].CopyFrom(\n            attr_value_pb2.AttrValue(\n                tensor=tensor_util.make_tensor_proto(\n                    data, dtype=dtype.type, shape=data_shape)))\n        return output_node\n\n    # This graph only includes the nodes needed to evaluate the output nodes, and\n    # removes unneeded nodes like those involved in saving and assignment.\n    inference_graph = extract_sub_graph(input_graph_def, output_node_names)\n\n    # Identify the ops in the graph.\n    map_name_to_node = {\n        node.name: node for node in inference_graph.node\n    }\n\n    # Get list of variables.\n    variable_names = []\n    variable_dict_names = []\n    resource_identity_types = {}\n    read_variable_op_types = {}\n    for node in inference_graph.node:\n        if node.op in [""Variable"", ""VariableV2"", ""VarHandleOp""]:\n            variable_name = node.name\n            if ((variable_names_whitelist is not None\n                 and variable_name not in variable_names_whitelist)\n                or (variable_names_blacklist is not None\n                    and variable_name in variable_names_blacklist)):\n                continue\n            variable_dict_names.append(variable_name)\n            if node.op == ""VarHandleOp"":\n                variable_names.append(variable_name + ""/Read/ReadVariableOp:0"")\n            else:\n                variable_names.append(variable_name + "":0"")\n        elif node.op in [""ReadVariableOp"", ""ResourceGather"", ""VariableShape""]:\n            # There can be one or more Identity or control flow ops in between the ReadVariableOp\n            # and VarHandleOp.  Store them with the associated dtypes.\n            source_op_name, nodes_in_path = trace_back_find_variable(_node_name(node.input[0]),\n                                                                     map_name_to_node)\n            dtype = map_name_to_node[source_op_name].attr[""dtype""]\n            for node_name in nodes_in_path:\n                resource_identity_types[node_name] = dtype\n            read_variable_op_types[node.name] = dtype\n\n    # Gets map of variables and the associated data.\n    if variable_names:\n        returned_variables = sess.run(variable_names)\n    else:\n        returned_variables = []\n    variables_data_map = dict(zip(variable_dict_names, returned_variables))\n    logging.info(""Froze %d variables."", len(returned_variables))\n\n    # Reconstruct the graph with constants in place of variables.\n    output_graph_def = graph_pb2.GraphDef()\n    how_many_converted = 0\n    for input_node in inference_graph.node:\n        output_node = node_def_pb2.NodeDef()\n        if input_node.name in variables_data_map:\n            data = variables_data_map[input_node.name]\n            output_node = create_const_op(input_node.name, input_node.attr[""dtype""],\n                                          data, data.shape)\n            how_many_converted += 1\n        elif input_node.name in resource_identity_types:\n            # Converts the Identities of type RESOURCE_DT to the appropriate type\n            # based on the input they are referencing.\n            output_node.CopyFrom(input_node)\n            output_node.attr[""T""].CopyFrom(resource_identity_types[input_node.name])\n        elif input_node.op == ""ReadVariableOp"":\n            # The first branch converts all VarHandleOps of ResourceVariables to\n            # constants, so we need to convert the associated ReadVariableOps to\n            # Identity ops.\n            output_node.op = ""Identity""\n            output_node.name = input_node.name\n            output_node.input.extend([input_node.input[0]])\n            output_node.attr[""T""].CopyFrom(input_node.attr[""dtype""])\n            if ""_class"" in input_node.attr:\n                output_node.attr[""_class""].CopyFrom(input_node.attr[""_class""])\n        elif input_node.op == ""ResourceGather"":\n            # The first branch converts all VarHandleOps of ResourceGather to\n            # constants, so we need to convert the associated ResourceGather to Gather\n            # ops with a Const axis feeding into it.\n            if input_node.attr[""batch_dims""].i != 0:\n                raise ValueError(""batch_dims != 0 is not supported by freeze_graph."")\n            axis_data = input_node.attr[""batch_dims""].i\n            axis_node_name = input_node.name + ""/axis""\n            axis_dtype = input_node.attr[""Tindices""]\n            output_axis_node = create_const_op(axis_node_name, axis_dtype, axis_data)\n            output_graph_def.node.extend([output_axis_node])\n\n            output_node.op = ""GatherV2""\n            output_node.name = input_node.name\n            output_node.input.extend(\n                [input_node.input[0], input_node.input[1], axis_node_name])\n            output_node.attr[""Tparams""].CopyFrom(input_node.attr[""dtype""])\n            output_node.attr[""Tindices""].CopyFrom(input_node.attr[""Tindices""])\n            output_node.attr[""Taxis""].CopyFrom(axis_dtype)\n            if ""_class"" in input_node.attr:\n                output_node.attr[""_class""].CopyFrom(input_node.attr[""_class""])\n        elif input_node.op == ""VariableShape"":\n            output_node.op = ""Shape""\n            output_node.name = input_node.name\n            output_node.input.extend([input_node.input[0]])\n            output_node.attr[""T""].CopyFrom(read_variable_op_types[input_node.name])\n            output_node.attr[""out_type""].CopyFrom(input_node.attr[""out_type""])\n        else:\n            output_node.CopyFrom(input_node)\n        output_graph_def.node.extend([output_node])\n\n    output_graph_def.library.CopyFrom(inference_graph.library)\n    logging.info(""Converted %d variables to const ops."", how_many_converted)\n    return output_graph_def\n\n\n@deprecation.deprecated(\n    date=None,\n    instructions=""Use `tf.compat.v1.graph_util.remove_training_nodes`"")\n@tf_export(v1=[""graph_util.remove_training_nodes""])\ndef remove_training_nodes(input_graph, protected_nodes=None):\n    """"""Prunes out nodes that aren\'t needed for inference.\n    There are nodes like Identity and CheckNumerics that are only useful\n    during training, and can be removed in graphs that will be used for\n    nothing but inference. Here we identify and remove them, returning an\n    equivalent graph. To be specific, CheckNumerics nodes are always removed, and\n    Identity nodes that aren\'t involved in control edges are spliced out so that\n    their input and outputs are directly connected.\n    Args:\n      input_graph: Model to analyze and prune.\n      protected_nodes: An optional list of names of nodes to be kept\n        unconditionally. This is for example useful to preserve Identity output\n        nodes.\n    Returns:\n      A list of nodes with the unnecessary ones removed.\n    """"""\n    if not protected_nodes:\n        protected_nodes = []\n\n    types_to_remove = {""CheckNumerics"": True}\n\n    input_nodes = input_graph.node\n    names_to_remove = {}\n    for node in input_nodes:\n        if node.op in types_to_remove and node.name not in protected_nodes:\n            names_to_remove[node.name] = True\n\n    nodes_after_removal = []\n    for node in input_nodes:\n        if node.name in names_to_remove:\n            continue\n        new_node = node_def_pb2.NodeDef()\n        new_node.CopyFrom(node)\n        input_before_removal = node.input\n        del new_node.input[:]\n        for full_input_name in input_before_removal:\n            input_name = re.sub(r""^\\^"", """", full_input_name)\n            if input_name in names_to_remove:\n                continue\n            new_node.input.append(full_input_name)\n        nodes_after_removal.append(new_node)\n\n    types_to_splice = {""Identity"": True}\n    control_input_names = set()\n    node_names_with_control_input = set()\n    for node in nodes_after_removal:\n        for node_input in node.input:\n            if ""^"" in node_input:\n                control_input_names.add(node_input.replace(""^"", """"))\n                node_names_with_control_input.add(node.name)\n\n    names_to_splice = {}\n    for node in nodes_after_removal:\n        if node.op in types_to_splice and node.name not in protected_nodes:\n            # We don\'t want to remove nodes that have control edge inputs, because\n            # they might be involved in subtle dependency issues that removing them\n            # will jeopardize.\n            if node.name not in node_names_with_control_input:\n                names_to_splice[node.name] = node.input[0]\n\n    # We also don\'t want to remove nodes which are used as control edge inputs.\n    names_to_splice = {name: value for name, value in names_to_splice.items()\n                       if name not in control_input_names}\n\n    nodes_after_splicing = []\n    for node in nodes_after_removal:\n        if node.name in names_to_splice:\n            continue\n        new_node = node_def_pb2.NodeDef()\n        new_node.CopyFrom(node)\n        input_before_removal = node.input\n        del new_node.input[:]\n        for full_input_name in input_before_removal:\n            input_name = re.sub(r""^\\^"", """", full_input_name)\n            while input_name in names_to_splice:\n                full_input_name = names_to_splice[input_name]\n                input_name = re.sub(r""^\\^"", """", full_input_name)\n            new_node.input.append(full_input_name)\n        nodes_after_splicing.append(new_node)\n\n    output_graph = graph_pb2.GraphDef()\n    output_graph.node.extend(nodes_after_splicing)\n    return output_graph\n'"
tf_onnx_util.py,7,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n""""""\npython -m tf2onnx.convert : tool to convert a frozen tensorflow graph to onnx\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport sys\n\nimport tensorflow as tf\n\nfrom tf2onnx.tfonnx import process_tf_graph, tf_optimize\nfrom tf2onnx import constants, logging, utils, optimizer\nfrom tf_graph_util import convert_variables_to_constants\n# from tensorflow.python.framework.graph_util import convert_variables_to_constants\n\n# pylint: disable=unused-argument\n\n_HELP_TEXT = """"""\nUsage Examples:\n\npython -m tf2onnx.convert --saved-model saved_model_dir --output model.onnx\npython -m tf2onnx.convert --input frozen_graph.pb  --inputs X:0 --outputs output:0 --output model.onnx\npython -m tf2onnx.convert --checkpoint checkpoint.meta  --inputs X:0 --outputs output:0 --output model.onnx\n\nFor help and additional information see:\n    https://github.com/onnx/tensorflow-onnx\n\nIf you run into issues, open an issue here:\n    https://github.com/onnx/tensorflow-onnx/issues\n""""""\n\nlogger = logging.getLogger(constants.TF2ONNX_PACKAGE_NAME)\n\n\ndef freeze_session(sess, keep_var_names=None, output_names=None, clear_devices=True):\n    """"""Freezes the state of a session into a pruned computation graph.""""""\n    output_names = [i.split(\':\')[:-1][0] for i in output_names]\n    graph = sess.graph\n    with graph.as_default():\n        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n        output_names = output_names or []\n        output_names += [v.op.name for v in tf.global_variables()]\n        input_graph_def = graph.as_graph_def(add_shapes=True)\n        if clear_devices:\n            for node in input_graph_def.node:\n                node.device = """"\n        frozen_graph = convert_variables_to_constants(sess, input_graph_def, output_names, freeze_var_names)\n        return frozen_graph\n\n\ndef remove_redundant_inputs(frozen_graph, input_names):\n    """"""Remove redundant inputs not in frozen graph.""""""\n    frozen_inputs = []\n    # get inputs in frozen graph\n    for n in frozen_graph.node:\n        for inp in input_names:\n            if utils.node_name(inp) == n.name:\n                frozen_inputs.append(inp)\n    deleted_inputs = list(set(input_names) - set(frozen_inputs))\n    if deleted_inputs:\n        logger.warning(""inputs [%s] is not in frozen graph, delete them"", "","".join(deleted_inputs))\n    return frozen_inputs\n\n\ndef from_graphdef(sess, graph_def, model_path, input_names, output_names):\n    """"""Load tensorflow graph from graphdef.""""""\n    # make sure we start with clean default graph\n    with tf.io.gfile.GFile(model_path, \'rb\') as f:\n        graph_def.ParseFromString(f.read())\n        tf.import_graph_def(graph_def, name=\'\')\n        frozen_graph = freeze_session(sess, output_names=output_names)\n    input_names = remove_redundant_inputs(frozen_graph, input_names)\n    # clean up\n    return frozen_graph, input_names, output_names\n\n\ndef convert_onnx(sess, graph_def, input_path, inputs_op, outputs_op):\n\n    graphdef = input_path\n\n    if inputs_op:\n        inputs_op, shape_override = utils.split_nodename_and_shape(inputs_op)\n    if outputs_op:\n        outputs_op = outputs_op.split("","")\n\n    logging.basicConfig(level=logging.get_verbosity_level(True))\n\n    utils.set_debug_mode(True)\n\n    graph_def, inputs_op, outputs_op = from_graphdef(sess, graph_def, graphdef, inputs_op, outputs_op)\n    model_path = graphdef\n\n    graph_def = tf_optimize(inputs_op, outputs_op, graph_def, True)\n\n    with tf.Graph().as_default() as tf_graph:\n        tf.import_graph_def(graph_def, name=\'\')\n    with tf.Session(graph=tf_graph):\n        g = process_tf_graph(tf_graph,\n                             continue_on_error=False,\n                             target="","".join(constants.DEFAULT_TARGET),\n                             opset=10,\n                             custom_op_handlers=None,\n                             extra_opset=None,\n                             shape_override=None,\n                             input_names=inputs_op,\n                             output_names=outputs_op,\n                             inputs_as_nchw=None)\n\n    onnx_graph = optimizer.optimize_graph(g)\n    model_proto = onnx_graph.make_model(""converted from {}"".format(model_path))\n\n    # write onnx graph\n    logger.info("""")\n    logger.info(""Successfully converted TensorFlow model %s to ONNX"", model_path)\n    # if args.output:\n    output_path = input_path.replace("".pb"", "".onnx"")\n    utils.save_protobuf(output_path, model_proto)\n    logger.info(""ONNX model is saved at %s"", output_path)\n\n\nif __name__ == ""__main__"":\n    pass\n'"
trains.py,27,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\n\nimport tensorflow as tf\nimport core\nimport utils\nimport utils.data\nimport validation\nfrom config import *\nfrom tf_graph_util import convert_variables_to_constants\nfrom PIL import ImageFile\nfrom tf_onnx_util import convert_onnx\nfrom middleware.random_captcha import RandomCaptcha\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n\n\nclass Trains:\n\n    stop_flag: bool = False\n    """"""\xe8\xae\xad\xe7\xbb\x83\xe4\xbb\xbb\xe5\x8a\xa1\xe7\x9a\x84\xe7\xb1\xbb""""""\n\n    def __init__(self, model_conf: ModelConfig):\n        """"""\n        :param model_conf: \xe8\xaf\xbb\xe5\x8f\x96\xe5\xb7\xa5\xe7\xa8\x8b\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\n        """"""\n        self.model_conf = model_conf\n        self.validation = validation.Validation(self.model_conf)\n\n    @staticmethod\n    def compile_onnx(predict_sess, output_graph_def, input_path):\n        convert_onnx(\n            sess=predict_sess,\n            graph_def=output_graph_def,\n            input_path=input_path,\n            inputs_op=""input:0"",\n            outputs_op=""dense_decoded:0""\n        )\n\n    @staticmethod\n    def compile_tflite(input_path):\n        input_tensor_name = [""input""]\n        classes_tensor_name = [""dense_decoded""]\n\n        converter = tf.lite.TFLiteConverter.from_frozen_graph(\n            input_path,\n            input_tensor_name,\n            classes_tensor_name,\n        )\n        # converter.post_training_quantize = True\n        tflite_model = converter.convert()\n        output_path = input_path.replace("".pb"", "".tflite"")\n        with open(output_path, ""wb"") as f:\n            f.write(tflite_model)\n\n    def compile_graph(self, acc):\n        """"""\n        \xe7\xbc\x96\xe8\xaf\x91\xe5\xbd\x93\xe5\x89\x8d\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\x8b\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe4\xb8\xbapb\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe4\xbb\x85\xe4\xbd\x9c\xe4\xb8\xba\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x91\xbd\xe5\x90\x8d\xe7\x9a\x84\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\n        :param acc: \xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n        :return:\n        """"""\n        input_graph = tf.Graph()\n        predict_sess = tf.Session(graph=input_graph)\n\n        with predict_sess.graph.as_default():\n            model = core.NeuralNetwork(\n                model_conf=self.model_conf,\n                mode=RunMode.Predict,\n                backbone=self.model_conf.neu_cnn,\n                recurrent=self.model_conf.neu_recurrent\n            )\n            model.build_graph()\n            model.build_train_op()\n            input_graph_def = predict_sess.graph.as_graph_def()\n            saver = tf.train.Saver(var_list=tf.global_variables())\n            tf.logging.info(tf.train.latest_checkpoint(self.model_conf.model_root_path))\n            saver.restore(predict_sess, tf.train.latest_checkpoint(self.model_conf.model_root_path))\n\n            output_graph_def = convert_variables_to_constants(\n                predict_sess,\n                input_graph_def,\n                output_node_names=[\'dense_decoded\']\n            )\n\n        if not os.path.exists(self.model_conf.compile_model_path):\n            os.makedirs(self.model_conf.compile_model_path)\n\n        last_compile_model_path = (\n            os.path.join(self.model_conf.compile_model_path, ""{}.pb"".format(self.model_conf.model_name))\n        ).replace(\'.pb\', \'_{}.pb\'.format(int(acc * 10000)))\n\n        self.model_conf.output_config(target_model_name=""{}_{}"".format(self.model_conf.model_name, int(acc * 10000)))\n        with tf.io.gfile.GFile(last_compile_model_path, mode=\'wb\') as gf:\n            gf.write(output_graph_def.SerializeToString())\n\n        if self.model_conf.loss_func == LossFunction.CrossEntropy:\n            self.compile_onnx(predict_sess, output_graph_def, last_compile_model_path)\n        # self.compile_tflite(last_compile_model_path)\n\n    def achieve_cond(self, acc, cost, epoch):\n        achieve_accuracy = acc >= self.model_conf.trains_end_acc\n        achieve_cost = cost <= self.model_conf.trains_end_cost\n        achieve_epochs = epoch >= self.model_conf.trains_end_epochs\n        over_epochs = epoch > 10000\n        if (achieve_accuracy and achieve_epochs and achieve_cost) or over_epochs:\n            return True\n        return False\n\n    def init_captcha_gennerator(self, ran_captcha):\n\n        path = self.model_conf.da_random_captcha[\'FontPath\']\n        if not os.path.exists(path):\n            exception(""Font path does not exist."", code=-6754)\n        items = os.listdir(path)\n        fonts = [os.path.join(path, item) for item in items]\n        ran_captcha.sample = NUMBER + ALPHA_UPPER + ALPHA_LOWER\n        ran_captcha.fonts_list = fonts\n        ran_captcha.check_font()\n        ran_captcha.rgb_r = [0, 255]\n        ran_captcha.rgb_g = [0, 255]\n        ran_captcha.rgb_b = [0, 255]\n        ran_captcha.fonts_num = [4, 8]\n\n    def train_process(self):\n        """"""\n        \xe8\xae\xad\xe7\xbb\x83\xe4\xbb\xbb\xe5\x8a\xa1\n        :return:\n        """"""\n        # \xe8\xbe\x93\xe5\x87\xba\xe9\x87\x8d\xe8\xa6\x81\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae\xe5\x8f\x82\xe6\x95\xb0\n        self.model_conf.println()\n        # \xe5\xae\x9a\xe4\xb9\x89\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\n        model = core.NeuralNetwork(\n            mode=RunMode.Trains,\n            model_conf=self.model_conf,\n            backbone=self.model_conf.neu_cnn,\n            recurrent=self.model_conf.neu_recurrent\n        )\n        model.build_graph()\n\n        ran_captcha = RandomCaptcha()\n\n        if self.model_conf.da_random_captcha[\'Enable\']:\n            self.init_captcha_gennerator(ran_captcha=ran_captcha)\n\n        tf.compat.v1.logging.info(\'Loading Trains DataSet...\')\n        train_feeder = utils.data.DataIterator(\n            model_conf=self.model_conf, mode=RunMode.Trains, ran_captcha=ran_captcha\n        )\n        train_feeder.read_sample_from_tfrecords(self.model_conf.trains_path[DatasetType.TFRecords])\n\n        tf.compat.v1.logging.info(\'Loading Validation DataSet...\')\n        validation_feeder = utils.data.DataIterator(\n            model_conf=self.model_conf, mode=RunMode.Validation, ran_captcha=ran_captcha\n        )\n        validation_feeder.read_sample_from_tfrecords(self.model_conf.validation_path[DatasetType.TFRecords])\n\n        tf.logging.info(\'Total {} Trains DataSets\'.format(train_feeder.size))\n        tf.logging.info(\'Total {} Validation DataSets\'.format(validation_feeder.size))\n        if validation_feeder.size >= train_feeder.size:\n            exception(""The number of training sets cannot be less than the validation set."", )\n        if validation_feeder.size < self.model_conf.validation_batch_size:\n            exception(""The number of validation sets cannot be less than the validation batch size."", )\n\n        num_train_samples = train_feeder.size\n        num_validation_samples = validation_feeder.size\n\n        if num_validation_samples < self.model_conf.validation_batch_size:\n            self.model_conf.validation_batch_size = num_validation_samples\n            tf.logging.warn(\n                \'The number of validation sets is less than the validation batch size, \'\n                \'will use validation set size as validation batch size.\'.format(validation_feeder.size))\n\n        num_batches_per_epoch = int(num_train_samples / self.model_conf.batch_size)\n\n        model.build_train_op(num_train_samples)\n\n        # \xe4\xbc\x9a\xe8\xaf\x9d\xe9\x85\x8d\xe7\xbd\xae\n        sess_config = tf.compat.v1.ConfigProto(\n            # allow_soft_placement=True,\n            log_device_placement=False,\n            gpu_options=tf.compat.v1.GPUOptions(\n                allocator_type=\'BFC\',\n                allow_growth=True,  # it will cause fragmentation.\n                per_process_gpu_memory_fraction=self.model_conf.memory_usage)\n        )\n        accuracy = 0\n        epoch_count = 1\n\n        if num_train_samples < 500:\n            save_step = 10\n            trains_validation_steps = 50\n\n        else:\n            save_step = 100\n            trains_validation_steps = self.model_conf.trains_validation_steps\n\n        with tf.compat.v1.Session(config=sess_config) as sess:\n            init_op = tf.global_variables_initializer()\n            sess.run(init_op)\n            saver = tf.train.Saver(var_list=tf.global_variables(), max_to_keep=2)\n            train_writer = tf.compat.v1.summary.FileWriter(\'logs\', sess.graph)\n            # try:\n            checkpoint_state = tf.train.get_checkpoint_state(self.model_conf.model_root_path)\n            if checkpoint_state and checkpoint_state.model_checkpoint_path:\n                # \xe5\x8a\xa0\xe8\xbd\xbd\xe8\xa2\xab\xe4\xb8\xad\xe6\x96\xad\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe4\xbb\xbb\xe5\x8a\xa1\n                saver.restore(sess, checkpoint_state.model_checkpoint_path)\n\n            tf.logging.info(\'Start training...\')\n\n            # \xe8\xbf\x9b\xe5\x85\xa5\xe8\xae\xad\xe7\xbb\x83\xe4\xbb\xbb\xe5\x8a\xa1\xe5\xbe\xaa\xe7\x8e\xaf\n            while 1:\n\n                start_time = time.time()\n                batch_cost = 65535\n                # \xe6\x89\xb9\xe6\xac\xa1\xe5\xbe\xaa\xe7\x8e\xaf\n                for cur_batch in range(num_batches_per_epoch):\n\n                    if self.stop_flag:\n                        break\n\n                    batch_time = time.time()\n\n                    trains_batch = train_feeder.generate_batch_by_tfrecords(sess)\n\n                    batch_inputs, batch_labels = trains_batch\n\n                    feed = {\n                        model.inputs: batch_inputs,\n                        model.labels: batch_labels,\n                        model.utils.is_training: True\n                    }\n\n                    summary_str, batch_cost, step, _, seq_len = sess.run(\n                        [model.merged_summary, model.cost, model.global_step, model.train_op, model.seq_len],\n                        feed_dict=feed\n                    )\n                    train_writer.add_summary(summary_str, step)\n\n                    if step % save_step == 0 and step != 0:\n                        tf.logging.info(\n                            \'Step: {} Time: {:.3f} sec/batch, Cost = {:.8f}, BatchSize: {}, Shape[1]: {}\'.format(\n                                step,\n                                time.time() - batch_time,\n                                batch_cost,\n                                len(batch_inputs),\n                                seq_len[0]\n                            )\n                        )\n\n                    # \xe8\xbe\xbe\xe5\x88\xb0\xe4\xbf\x9d\xe5\xad\x98\xe6\xad\xa5\xe6\x95\xb0\xe5\xaf\xb9\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x87\xe7\xa8\x8b\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xad\x98\xe5\x82\xa8\n                    if step % save_step == 0 and step != 0:\n                        saver.save(sess, self.model_conf.save_model, global_step=step)\n\n                    # \xe8\xbf\x9b\xe5\x85\xa5\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe9\xaa\x8c\xe8\xaf\x81\xe7\x8e\xaf\xe8\x8a\x82\n                    if step % trains_validation_steps == 0 and step != 0:\n\n                        batch_time = time.time()\n                        validation_batch = validation_feeder.generate_batch_by_tfrecords(sess)\n\n                        test_inputs, test_labels = validation_batch\n                        val_feed = {\n                            model.inputs: test_inputs,\n                            model.labels: test_labels,\n                            model.utils.is_training: False\n                        }\n                        dense_decoded, lr = sess.run(\n                            [model.dense_decoded, model.lrn_rate],\n                            feed_dict=val_feed\n                        )\n                        # \xe8\xae\xa1\xe7\xae\x97\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n                        accuracy = self.validation.accuracy_calculation(\n                            validation_feeder.labels,\n                            dense_decoded,\n                        )\n                        log = ""Epoch: {}, Step: {}, Accuracy = {:.4f}, Cost = {:.5f}, "" \\\n                              ""Time = {:.3f} sec/batch, LearningRate: {}""\n                        tf.logging.info(log.format(\n                            epoch_count,\n                            step,\n                            accuracy,\n                            batch_cost,\n                            time.time() - batch_time,\n                            lr / len(validation_batch),\n                        ))\n\n                        # \xe6\xbb\xa1\xe8\xb6\xb3\xe7\xbb\x88\xe6\xad\xa2\xe6\x9d\xa1\xe4\xbb\xb6\xe4\xbd\x86\xe5\xb0\x9a\xe6\x9c\xaa\xe5\xae\x8c\xe6\x88\x90\xe5\xbd\x93\xe5\x89\x8depoch\xe6\x97\xb6\xe8\xb7\xb3\xe5\x87\xbaepoch\xe5\xbe\xaa\xe7\x8e\xaf\n                        if self.achieve_cond(acc=accuracy, cost=batch_cost, epoch=epoch_count):\n                            break\n\n                # \xe6\xbb\xa1\xe8\xb6\xb3\xe7\xbb\x88\xe6\xad\xa2\xe6\x9d\xa1\xe4\xbb\xb6\xe6\x97\xb6\xef\xbc\x8c\xe8\xb7\xb3\xe5\x87\xba\xe4\xbb\xbb\xe5\x8a\xa1\xe5\xbe\xaa\xe7\x8e\xaf\n                if self.stop_flag:\n                    break\n                if self.achieve_cond(acc=accuracy, cost=batch_cost, epoch=epoch_count):\n                    self.compile_graph(accuracy)\n                    tf.logging.info(\'Total Time: {} sec.\'.format(time.time() - start_time))\n                    break\n                epoch_count += 1\n\n\ndef main(argv):\n    project_name = argv[-1]\n    model_conf = ModelConfig(project_name=project_name)\n    Trains(model_conf).train_process()\n    tf.logging.info(\'Training completed.\')\n    pass\n\n\nif __name__ == \'__main__\':\n    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n    tf.compat.v1.app.run()\n'"
validation.py,5,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom config import ModelConfig\n\n\nclass Validation(object):\n    """"""\xe9\xaa\x8c\xe8\xaf\x81\xe7\xb1\xbb\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe8\xae\xa1\xe7\xae\x97""""""\n    def __init__(self, model: ModelConfig):\n        """"""\n        :param model: \xe8\xaf\xbb\xe5\x8f\x96\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\xe8\x8e\xb7\xe5\x8f\x96\xe5\xbd\x93\xe5\x89\x8d\xe5\xb7\xa5\xe7\xa8\x8b\xe7\x9a\x84\xe9\x87\x8d\xe8\xa6\x81\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x9acategory_num, category\n        """"""\n        self.model = model\n        self.category_num = self.model.category_num\n        self.category = self.model.category\n\n    def accuracy_calculation(self, original_seq, decoded_seq):\n        """"""\n        \xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe8\xae\xa1\xe7\xae\x97\xe5\x87\xbd\xe6\x95\xb0\n        :param original_seq: \xe5\xaf\x86\xe9\x9b\x86\xe6\x95\xb0\xe7\xbb\x84-Y\xe6\xa0\x87\xe7\xad\xbe\n        :param decoded_seq: \xe5\xaf\x86\xe9\x9b\x86\xe6\x95\xb0\xe7\xbb\x84-\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa0\x87\xe7\xad\xbe\n        :return:\n        """"""\n        if isinstance(decoded_seq, np.ndarray):\n            decoded_seq = decoded_seq.tolist()\n\n        ignore_value = [-1, self.category_num, 0]\n        original_seq_len = len(original_seq)\n        decoded_seq_len = len(decoded_seq)\n\n        if original_seq_len != decoded_seq_len:\n            tf.logging.error(original_seq)\n            tf.logging.error(decoded_seq)\n            tf.logging.error(\'original lengths {} is different from the decoded_seq {}, please check again\'.format(\n                original_seq_len,\n                decoded_seq_len\n            ))\n            return 0\n        count = 0\n\n        # Here is for debugging, positioning error source use\n        error_sample = []\n        for i, origin_label in enumerate(original_seq):\n\n            decoded_label = decoded_seq[i]\n            if isinstance(decoded_label, int):\n                decoded_label = [decoded_label]\n            processed_decoded_label = [j for j in decoded_label if j not in ignore_value]\n            processed_origin_label = [j for j in origin_label if j not in ignore_value]\n\n            if i < 5:\n                tf.logging.info(\n                    ""{} {} {} {} {} --> {} {}"".format(\n                        i,\n                        len(processed_origin_label),\n                        len(processed_decoded_label),\n                        origin_label,\n                        decoded_label,\n                        [self.category[_] if _ != self.category_num else \'-\' for _ in origin_label if _ != -1],\n                        [self.category[_] if _ != self.category_num else \'-\' for _ in decoded_label if _ != -1]\n                    )\n                )\n            if processed_origin_label == processed_decoded_label:\n                count += 1\n            # Training is not useful for decoding\n            # Here is for debugging, positioning error source use\n            if processed_origin_label != processed_decoded_label and len(error_sample) < 5:\n                error_sample.append({\n                    ""origin"": """".join([self.category[_] if _ != self.category_num else \'-\' for _ in origin_label if _ != -1]),\n                    ""decode"": """".join([self.category[_] if _ != self.category_num else \'-\' for _ in decoded_label if _ != -1])\n                })\n        tf.compat.v1.logging.error(json.dumps(error_sample, ensure_ascii=False))\n        return count * 1.0 / len(original_seq)'"
compat/__init__.py,0,b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>'
compat/upgrade.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport yaml\nimport json\n\n\nclass ModelConfig:\n\n    def __init__(self, model_conf: str):\n        self.model_conf = model_conf\n        self.system = None\n        self.device = None\n        self.device_usage = None\n        self.charset = None\n        self.split_char = None\n        self.gen_charset = None\n        self.char_exclude = None\n        self.model_name = None\n        self.model_type = None\n        self.image_height = None\n        self.image_width = None\n        self.image_channel = None\n        self.padding = None\n        self.lower_padding = None\n        self.resize = None\n        self.binaryzation = None\n        self.smooth = None\n        self.blur = None\n        self.replace_transparent = None\n        self.model_site = None\n        self.version = None\n        self.color_engine = None\n        self.cf_model = self.read_conf\n        self.model_exists = False\n        self.assignment()\n\n    def assignment(self):\n\n        system = self.cf_model.get(\'System\')\n        self.device = system.get(\'Device\') if system else None\n        self.device = self.device if self.device else ""cpu:0""\n        self.device_usage = system.get(\'DeviceUsage\') if system else None\n        self.device_usage = self.device_usage if self.device_usage else 0.01\n        self.charset = self.cf_model[\'Model\'].get(\'CharSet\')\n        self.char_exclude = self.cf_model[\'Model\'].get(\'CharExclude\')\n        self.model_name = self.cf_model[\'Model\'].get(\'ModelName\')\n        self.model_type = self.cf_model[\'Model\'].get(\'ModelType\')\n        self.model_site = self.cf_model[\'Model\'].get(\'Sites\')\n        self.model_site = self.model_site if self.model_site else []\n        self.version = self.cf_model[\'Model\'].get(\'Version\')\n        self.version = self.version if self.version else 1.0\n        self.split_char = self.cf_model[\'Model\'].get(\'SplitChar\')\n        self.split_char = \'\' if not self.split_char else self.split_char\n\n        self.image_height = self.cf_model[\'Model\'].get(\'ImageHeight\')\n        self.image_width = self.cf_model[\'Model\'].get(\'ImageWidth\')\n        self.image_channel = self.cf_model[\'Model\'].get(\'ImageChannel\')\n        self.image_channel = self.image_channel if self.image_channel else 1\n        self.binaryzation = self.cf_model[\'Pretreatment\'].get(\'Binaryzation\')\n        self.resize = self.cf_model[\'Pretreatment\'].get(\'Resize\')\n        self.resize = self.resize if self.resize else [self.image_width, self.image_height]\n        self.replace_transparent = self.cf_model[\'Pretreatment\'].get(\'ReplaceTransparent\')\n\n    @property\n    def read_conf(self):\n        with open(self.model_conf, \'r\', encoding=""utf-8"") as sys_fp:\n            sys_stream = sys_fp.read()\n            return yaml.load(sys_stream, Loader=yaml.SafeLoader)\n\n    def convert(self):\n        with open(""../model.template"", encoding=""utf8"") as f:\n            lines = f.readlines()\n            bc = """".join(lines)\n            model = bc.format(\n                MemoryUsage=0.7,\n                CNNNetwork=\'CNNX\',\n                RecurrentNetwork=\'GRU\',\n                UnitsNum=64,\n                Optimizer=\'Adam\',\n                LossFunction=\'CTC\',\n                Decoder=\'CTC\',\n                ModelName=self.model_name,\n                ModelField=\'Image\',\n                ModelScene=\'Classification\',\n                Category=self.charset,\n                Resize=json.dumps(self.resize),\n                ImageChannel=self.image_channel,\n                ImageWidth=self.image_width,\n                ImageHeight=self.image_height,\n                MaxLabelNum=4,\n                AutoPadding=False,\n                OutputSplit="""",\n                LabelFrom=""FileName"",\n                ExtractRegex="".*?(?=_)"",\n                LabelSplit=\'null\',\n                DatasetTrainsPath="""",\n                DatasetValidationPath="""",\n                SourceTrainPath="""",\n                SourceValidationPath="""",\n                ValidationSetNum=""300"",\n                SavedSteps=""500"",\n                ValidationSteps=""500"",\n                EndAcc=""0.98"",\n                EndCost=""0.05"",\n                EndEpochs=""2"",\n                BatchSize=""64"",\n                ValidationBatchSize=""300"",\n                LearningRate=""0.001"",\n                DA_Binaryzation=""-1"",\n                DA_MedianBlur=""-1"",\n                DA_GaussianBlur=""-1"",\n                DA_EqualizeHist=""False"",\n                DA_Laplace=""False"",\n                DA_WarpPerspective=""False"",\n                DA_Rotate=""-1"",\n                DA_PepperNoise=""-1"",\n                DA_Brightness=""False"",\n                DA_Saturation=""False"",\n                DA_Hue=""False"",\n                DA_Gamma=""False"",\n                DA_ChannelSwap=""False"",\n                DA_RandomBlank=""-1"",\n                DA_RandomTransition=""-1"",\n                Pre_Binaryzation=""-1"",\n                Pre_ReplaceTransparent=""False"",\n                Pre_HorizontalStitching=""False"",\n                Pre_ConcatFrames=""-1"",\n                Pre_BlendFrames=""-1"",\n            )\n        with open(self.model_conf.replace("".yaml"", ""_2.0.yaml""), ""w"", encoding=""utf8"") as f:\n            f.write(model)\n\n\nif __name__ == \'__main__\':\n    ModelConfig(model_conf=""model.yaml"").convert()\n'"
fc/__init__.py,0,b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\n\nfrom .cnn import FullConnectedCNN\nfrom .rnn import FullConnectedRNN'
fc/cnn.py,4,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport tensorflow as tf\nfrom tensorflow.python.keras.regularizers import l1_l2\nfrom config import ModelConfig, RunMode\nfrom exception import exception\n\nfrom network.utils import NetworkUtils\n\n\nclass FullConnectedCNN(object):\n    """"""\n    CNN\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\n    """"""\n    def __init__(self, model_conf: ModelConfig, outputs):\n        self.model_conf = model_conf\n\n        self.max_label_num = self.model_conf.max_label_num\n        if self.max_label_num == -1:\n            exception(text=""The scene must set the maximum number of label (MaxLabelNum)"", code=-998)\n        self.category_num = self.model_conf.category_num\n\n        flatten = tf.keras.layers.Flatten()(outputs)\n        shape_list = flatten.get_shape().as_list()\n\n        # print(shape_list[1], self.max_label_num)\n        outputs = tf.keras.layers.Reshape([self.max_label_num, int(shape_list[1] / self.max_label_num)])(flatten)\n        self.outputs = tf.keras.layers.Dense(\n            input_shape=outputs.shape,\n            units=self.category_num,\n        )(inputs=outputs)\n\n        print(""output to reshape ----------- "", self.outputs.shape)\n        self.outputs = tf.keras.layers.Reshape([self.max_label_num, self.category_num])(self.outputs)\n\n    def build(self):\n        return self.outputs\n'"
fc/rnn.py,3,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport tensorflow as tf\nfrom tensorflow.python.keras.regularizers import l1_l2\nfrom config import RunMode, ModelConfig\nfrom network.utils import NetworkUtils\n\n\nclass FullConnectedRNN(object):\n    """"""\n    RNN\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\n    """"""\n\n    def __init__(self, model_conf: ModelConfig, outputs):\n        self.model_conf = model_conf\n\n        self.dense = tf.keras.layers.Dense(\n            units=self.model_conf.category_num + 2,\n            kernel_initializer=tf.keras.initializers.he_normal(seed=None),\n            bias_initializer=\'zeros\',\n        )\n\n        self.outputs = self.dense(outputs)\n        self.predict = tf.keras.backend.permute_dimensions(self.outputs, pattern=(1, 0, 2))\n\n    def build(self):\n        return self.predict\n'"
gui/__init__.py,0,b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>'
gui/data_augmentation.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport json\nimport tkinter as tk\nimport tkinter.ttk as ttk\nfrom gui.utils import LayoutGUI\n\n\nclass DataAugmentationDialog(tk.Toplevel):\n\n    def __init__(self):\n        tk.Toplevel.__init__(self)\n        self.title(\'Data Augmentation\')\n        self.layout = {\n            \'global\': {\n                \'start\': {\'x\': 15, \'y\': 20},\n                \'space\': {\'x\': 15, \'y\': 25},\n                \'tiny_space\': {\'x\': 5, \'y\': 10}\n            }\n        }\n        self.data_augmentation_entity = None\n        self.da_random_captcha = {""Enable"": False, ""FontPath"": """"}\n        self.window_width = 750\n        self.window_height = 220\n\n        self.layout_utils = LayoutGUI(self.layout, self.window_width)\n        screenwidth = self.winfo_screenwidth()\n        screenheight = self.winfo_screenheight()\n        size = \'%dx%d+%d+%d\' % (\n            self.window_width,\n            self.window_height,\n            (screenwidth - self.window_width) / 2,\n            (screenheight - self.window_height) / 2\n        )\n        self.geometry(size)\n        # ============================= Group 4 =====================================\n        self.label_frame_augmentation = ttk.Labelframe(self, text=\'Data Augmentation\')\n        self.label_frame_augmentation.place(\n            x=self.layout[\'global\'][\'start\'][\'x\'],\n            y=self.layout[\'global\'][\'start\'][\'y\'],\n            width=725,\n            height=150\n        )\n\n        # \xe4\xba\x8c\xe5\x80\xbc\xe5\x8c\x96 - \xe6\xa0\x87\xe7\xad\xbe\n        self.binaryzation_text = ttk.Label(self, text=\'Binaryzation\', anchor=tk.W)\n        self.layout_utils.inside_widget(\n            src=self.binaryzation_text,\n            target=self.label_frame_augmentation,\n            width=72,\n            height=20,\n        )\n\n        # \xe4\xba\x8c\xe5\x80\xbc\xe5\x8c\x96 - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.binaryzation_val = tk.StringVar()\n        self.binaryzation_val.set(-1)\n        self.binaryzation_entry = ttk.Entry(self, textvariable=self.binaryzation_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.binaryzation_entry,\n            target=self.binaryzation_text,\n            width=55,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe6\xbb\xa4\xe6\xb3\xa2 - \xe6\xa0\x87\xe7\xad\xbe\n        self.median_blur_text = ttk.Label(self, text=\'Median Blur\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.median_blur_text,\n            target=self.binaryzation_entry,\n            width=80,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe6\xbb\xa4\xe6\xb3\xa2 - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.median_blur_val = tk.IntVar()\n        self.median_blur_val.set(-1)\n        self.median_blur_entry = ttk.Entry(self, textvariable=self.median_blur_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.median_blur_entry,\n            target=self.median_blur_text,\n            width=52,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe9\xab\x98\xe6\x96\xaf\xe6\xa8\xa1\xe7\xb3\x8a - \xe6\xa0\x87\xe7\xad\xbe\n        self.gaussian_blur_text = ttk.Label(self, text=\'Gaussian Blur\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.gaussian_blur_text,\n            target=self.median_blur_entry,\n            width=85,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe9\xab\x98\xe6\x96\xaf\xe6\xa8\xa1\xe7\xb3\x8a - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.gaussian_blur_val = tk.IntVar()\n        self.gaussian_blur_val.set(-1)\n        self.gaussian_blur_entry = ttk.Entry(self, textvariable=self.gaussian_blur_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.gaussian_blur_entry,\n            target=self.gaussian_blur_text,\n            width=62,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe6\xa4\x92\xe7\x9b\x90\xe5\x99\xaa\xe5\xa3\xb0 - \xe6\xa0\x87\xe7\xad\xbe\n        self.sp_noise_text = ttk.Label(self, text=\'Pepper Noise (0-1)\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.sp_noise_text,\n            target=self.gaussian_blur_entry,\n            width=110,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe6\xa4\x92\xe7\x9b\x90\xe5\x99\xaa\xe5\xa3\xb0 - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.sp_noise_val = tk.DoubleVar()\n        self.sp_noise_val.set(-1)\n        self.sp_noise_entry = ttk.Entry(self, textvariable=self.sp_noise_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.sp_noise_entry,\n            target=self.sp_noise_text,\n            width=71,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe6\x97\x8b\xe8\xbd\xac - \xe6\xa0\x87\xe7\xad\xbe\n        self.rotate_text = ttk.Label(self, text=\'Rotate (0-90)\', anchor=tk.W)\n        self.layout_utils.below_widget(\n            src=self.rotate_text,\n            target=self.binaryzation_text,\n            width=72,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe6\x97\x8b\xe8\xbd\xac - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.rotate_val = tk.IntVar()\n        self.rotate_val.set(-1)\n        self.rotate_entry = ttk.Entry(self, textvariable=self.rotate_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.rotate_entry,\n            target=self.rotate_text,\n            width=55,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe7\xa9\xba\xe7\x99\xbd\xe8\xbe\xb9\xe7\xbc\x98 - \xe6\xa0\x87\xe7\xad\xbe\n        self.random_blank_text = ttk.Label(self, text=\'Blank Border\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.random_blank_text,\n            target=self.rotate_entry,\n            width=72,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe7\xa9\xba\xe7\x99\xbd\xe8\xbe\xb9\xe7\xbc\x98 - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.random_blank_val = tk.IntVar()\n        self.random_blank_val.set(-1)\n        self.random_blank_entry = ttk.Entry(self, textvariable=self.random_blank_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.random_blank_entry,\n            target=self.random_blank_text,\n            width=55,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe8\xbe\xb9\xe7\xbc\x98\xe4\xbd\x8d\xe7\xa7\xbb - \xe6\xa0\x87\xe7\xad\xbe\n        self.random_transition_text = ttk.Label(self, text=\'Transition\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.random_transition_text,\n            target=self.random_blank_entry,\n            width=60,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe8\xbe\xb9\xe7\xbc\x98\xe4\xbd\x8d\xe7\xa7\xbb - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.random_transition_val = tk.IntVar()\n        self.random_transition_val.set(-1)\n        self.random_transition_entry = ttk.Entry(self, textvariable=self.random_transition_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.random_transition_entry,\n            target=self.random_transition_text,\n            width=55,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe5\xad\x97\xe4\xbd\x93 - \xe6\xa0\x87\xe7\xad\xbe\n        self.random_captcha_font_text = ttk.Label(self, text=\'RandomCaptcha - Font\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.random_captcha_font_text,\n            target=self.random_transition_entry,\n            width=130,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe5\xad\x97\xe4\xbd\x93\n        self.random_captcha_font_val = tk.StringVar()\n        self.random_captcha_font_val.set("""")\n        self.random_captcha_font_entry = ttk.Entry(self, textvariable=self.random_captcha_font_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.random_captcha_font_entry,\n            target=self.random_captcha_font_text,\n            width=75,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe9\x80\x8f\xe8\xa7\x86\xe5\x8f\x98\xe6\x8d\xa2 - \xe5\xa4\x9a\xe9\x80\x89\xe6\xa1\x86\n        self.warp_perspective_val = tk.IntVar()\n        self.warp_perspective_val.set(0)\n        self.warp_perspective = ttk.Checkbutton(\n            self, text=\'Distortion\', variable=self.warp_perspective_val, onvalue=1, offvalue=0\n        )\n        self.layout_utils.below_widget(\n            src=self.warp_perspective,\n            target=self.rotate_text,\n            width=80,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\xe5\x9d\x87\xe8\xa1\xa1\xe5\x8c\x96 - \xe5\xa4\x9a\xe9\x80\x89\xe6\xa1\x86\n        self.equalize_hist_val = tk.IntVar()\n        self.equalize_hist_val.set(0)\n        self.equalize_hist = ttk.Checkbutton(\n            self, text=\'EqualizeHist\', variable=self.equalize_hist_val, offvalue=0\n        )\n        self.layout_utils.next_to_widget(\n            src=self.equalize_hist,\n            target=self.warp_perspective,\n            width=100,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe6\x8b\x89\xe6\x99\xae\xe6\x8b\x89\xe6\x96\xaf - \xe5\xa4\x9a\xe9\x80\x89\xe6\xa1\x86\n        self.laplace_val = tk.IntVar()\n        self.laplace_val.set(0)\n        self.laplace = ttk.Checkbutton(\n            self, text=\'Laplace\', variable=self.laplace_val, onvalue=1, offvalue=0\n        )\n        self.layout_utils.next_to_widget(\n            src=self.laplace,\n            target=self.equalize_hist,\n            width=64,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe4\xba\xae\xe5\xba\xa6 - \xe5\xa4\x9a\xe9\x80\x89\xe6\xa1\x86\n        self.brightness_val = tk.IntVar()\n        self.brightness_val.set(0)\n        self.brightness = ttk.Checkbutton(\n            self, text=\'Brightness\', variable=self.brightness_val, offvalue=0\n        )\n        self.layout_utils.next_to_widget(\n            src=self.brightness,\n            target=self.laplace,\n            width=80,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe9\xa5\xb1\xe5\x92\x8c\xe5\xba\xa6 - \xe5\xa4\x9a\xe9\x80\x89\xe6\xa1\x86\n        self.saturation_val = tk.IntVar()\n        self.saturation_val.set(0)\n        self.saturation = ttk.Checkbutton(\n            self, text=\'Saturation\', variable=self.saturation_val, offvalue=0\n        )\n        self.layout_utils.next_to_widget(\n            src=self.saturation,\n            target=self.brightness,\n            width=80,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe8\x89\xb2\xe7\x9b\xb8 - \xe5\xa4\x9a\xe9\x80\x89\xe6\xa1\x86\n        self.hue_val = tk.IntVar()\n        self.hue_val.set(0)\n        self.hue = ttk.Checkbutton(\n            self, text=\'Hue\', variable=self.hue_val, offvalue=0\n        )\n        self.layout_utils.next_to_widget(\n            src=self.hue,\n            target=self.saturation,\n            width=50,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe9\x9a\x8f\xe6\x9c\xbaGamma - \xe5\xa4\x9a\xe9\x80\x89\xe6\xa1\x86\n        self.gamma_val = tk.IntVar()\n        self.gamma_val.set(0)\n        self.gamma = ttk.Checkbutton(\n            self, text=\'Gamma\', variable=self.gamma_val, offvalue=0\n        )\n        self.layout_utils.next_to_widget(\n            src=self.gamma,\n            target=self.hue,\n            width=80,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x9a\xe9\x81\x93 - \xe5\xa4\x9a\xe9\x80\x89\xe6\xa1\x86\n        self.channel_swap_val = tk.IntVar()\n        self.channel_swap_val.set(0)\n        self.channel_swap = ttk.Checkbutton(\n            self, text=\'Channel Swap\', variable=self.channel_swap_val, offvalue=0\n        )\n        self.layout_utils.next_to_widget(\n            src=self.channel_swap,\n            target=self.gamma,\n            width=100,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe4\xbf\x9d\xe5\xad\x98 - \xe6\x8c\x89\xe9\x92\xae\n        self.btn_save = ttk.Button(self, text=\'Save Configuration\', command=lambda: self.save_conf())\n        self.layout_utils.widget_from_right(\n            src=self.btn_save,\n            target=self.label_frame_augmentation,\n            width=120,\n            height=24,\n            tiny_space=True\n        )\n\n    def read_conf(self, entity):\n        self.data_augmentation_entity = entity\n        self.binaryzation_val.set(json.dumps(entity.binaryzation))\n        self.median_blur_val.set(entity.median_blur)\n        self.gaussian_blur_val.set(entity.gaussian_blur)\n        self.equalize_hist_val.set(entity.equalize_hist)\n        self.laplace_val.set(entity.laplace)\n        self.warp_perspective_val.set(entity.warp_perspective)\n        self.rotate_val.set(entity.rotate)\n        self.sp_noise_val.set(entity.sp_noise)\n        self.brightness_val.set(entity.brightness)\n        self.saturation_val.set(entity.saturation)\n        self.hue_val.set(entity.hue)\n        self.gamma_val.set(entity.gamma)\n        self.channel_swap_val.set(entity.channel_swap)\n        self.random_blank_val.set(entity.random_blank)\n        self.random_transition_val.set(entity.random_transition)\n        self.da_random_captcha = entity.random_captcha\n        if self.da_random_captcha[\'Enable\']:\n            self.random_captcha_font_val.set(self.da_random_captcha[\'FontPath\'])\n\n    def save_conf(self):\n        self.data_augmentation_entity.binaryzation = json.loads(self.binaryzation_val.get()) if self.binaryzation_val else []\n        self.data_augmentation_entity.median_blur = self.median_blur_val.get()\n        self.data_augmentation_entity.gaussian_blur = self.gaussian_blur_val.get()\n        self.data_augmentation_entity.rotate = self.rotate_val.get()\n        self.data_augmentation_entity.sp_noise = self.sp_noise_val.get()\n        self.data_augmentation_entity.random_blank = self.random_blank_val.get()\n        self.data_augmentation_entity.random_transition = self.random_transition_val.get()\n\n        if self.random_captcha_font_val.get():\n            self.data_augmentation_entity.random_captcha[\'Enable\'] = True\n            self.data_augmentation_entity.random_captcha[\'FontPath\'] = self.random_captcha_font_val.get()\n        else:\n            self.data_augmentation_entity.random_captcha[\'Enable\'] = False\n            self.data_augmentation_entity.random_captcha[\'FontPath\'] = """"\n\n        self.data_augmentation_entity.equalize_hist = True if self.equalize_hist_val.get() == 1 else False\n        self.data_augmentation_entity.laplace = True if self.laplace_val.get() == 1 else False\n        self.data_augmentation_entity.warp_perspective = True if self.warp_perspective_val.get() == 1 else False\n\n        self.data_augmentation_entity.brightness = True if self.brightness_val.get() == 1 else False\n        self.data_augmentation_entity.saturation = True if self.saturation_val.get() == 1 else False\n        self.data_augmentation_entity.hue = True if self.hue_val.get() == 1 else False\n        self.data_augmentation_entity.gamma = True if self.gamma_val.get() == 1 else False\n        self.data_augmentation_entity.channel_swap = True if self.channel_swap_val.get() == 1 else False\n\n        self.destroy()'"
gui/pretreatment.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport json\nimport tkinter as tk\nimport tkinter.ttk as ttk\nfrom tkinter import messagebox\nfrom gui.utils import LayoutGUI\n\n\nclass PretreatmentDialog(tk.Toplevel):\n\n    def __init__(self):\n        tk.Toplevel.__init__(self)\n        self.title(\'Data Pretreatment\')\n        self.layout = {\n            \'global\': {\n                \'start\': {\'x\': 15, \'y\': 20},\n                \'space\': {\'x\': 15, \'y\': 25},\n                \'tiny_space\': {\'x\': 5, \'y\': 10}\n            }\n        }\n        self.pretreatment_entity = None\n        self.window_width = 600\n        self.window_height = 220\n\n        self.layout_utils = LayoutGUI(self.layout, self.window_width)\n        screenwidth = self.winfo_screenwidth()\n        screenheight = self.winfo_screenheight()\n        size = \'%dx%d+%d+%d\' % (\n            self.window_width,\n            self.window_height,\n            (screenwidth - self.window_width) / 2,\n            (screenheight - self.window_height) / 2\n        )\n        self.geometry(size)\n        # ============================= Group 4 =====================================\n        self.label_frame_pretreatment = ttk.Labelframe(self, text=\'Data Pretreatment\')\n        self.label_frame_pretreatment.place(\n            x=self.layout[\'global\'][\'start\'][\'x\'],\n            y=self.layout[\'global\'][\'start\'][\'y\'],\n            width=575,\n            height=150\n        )\n\n        # \xe5\xb8\xa7\xe6\x8b\xbc\xe6\x8e\xa5 - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.concat_frames_val = tk.StringVar()\n        self.concat_frames_val.set("""")\n        self.concat_frames_entry = ttk.Entry(self, textvariable=self.concat_frames_val, justify=tk.LEFT)\n        self.concat_frames_entry[\'state\'] = tk.DISABLED\n\n        # \xe5\xb8\xa7\xe6\x8b\xbc\xe6\x8e\xa5 - \xe5\xa4\x8d\xe9\x80\x89\xe6\xa1\x86\n        self.concat_frames_check_val = tk.IntVar()\n        self.concat_frames_check = ttk.Checkbutton(\n            self,\n            text=\'GIF Frame Stitching\',\n            variable=self.concat_frames_check_val,\n            onvalue=1,\n            offvalue=0,\n            command=lambda: self.check_btn_event(src=self.concat_frames_check_val, entry=self.concat_frames_entry)\n        )\n        self.layout_utils.inside_widget(\n            src=self.concat_frames_check,\n            target=self.label_frame_pretreatment,\n            width=140,\n            height=20,\n        )\n\n        # \xe5\xb8\xa7\xe6\x8b\xbc\xe6\x8e\xa5 - \xe5\xb8\x83\xe5\xb1\x80\n        self.layout_utils.next_to_widget(\n            src=self.concat_frames_entry,\n            target=self.concat_frames_check,\n            width=100,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe5\xb8\xa7\xe8\x9e\x8d\xe5\x90\x88 - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.blend_frames_val = tk.StringVar()\n        self.blend_frames_val.set("""")\n        self.blend_frames_entry = ttk.Entry(self, textvariable=self.blend_frames_val, justify=tk.LEFT)\n        self.blend_frames_entry[\'state\'] = tk.DISABLED\n\n        # \xe5\xb8\xa7\xe8\x9e\x8d\xe5\x90\x88 - \xe5\xa4\x8d\xe9\x80\x89\xe6\xa1\x86\n        self.blend_frames_check_val = tk.IntVar()\n        self.blend_frames_check_val.set(0)\n        self.blend_frames_check = ttk.Checkbutton(\n            self, text=\'GIF Blend Frame\',\n            variable=self.blend_frames_check_val,\n            onvalue=1,\n            offvalue=0,\n            command=lambda: self.check_btn_event(src=self.blend_frames_check_val, entry=self.blend_frames_entry)\n        )\n\n        # \xe5\xb8\xa7\xe8\x9e\x8d\xe5\x90\x88 - \xe5\xb8\x83\xe5\xb1\x80\n        self.layout_utils.next_to_widget(\n            src=self.blend_frames_check,\n            target=self.concat_frames_entry,\n            width=120,\n            height=20,\n            tiny_space=False\n        )\n        self.layout_utils.next_to_widget(\n            src=self.blend_frames_entry,\n            target=self.blend_frames_check,\n            width=110,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe6\x9b\xbf\xe6\x8d\xa2\xe9\x80\x8f\xe6\x98\x8e - \xe5\xa4\x8d\xe9\x80\x89\xe6\xa1\x86\n        self.replace_transparent_check_val = tk.IntVar()\n        self.replace_transparent_check = ttk.Checkbutton(\n            self, text=\'Replace Transparent\',\n            variable=self.replace_transparent_check_val,\n            onvalue=1,\n            offvalue=0\n        )\n        self.layout_utils.below_widget(\n            src=self.replace_transparent_check,\n            target=self.concat_frames_check,\n            width=140,\n            height=20,\n        )\n\n        # \xe6\xb0\xb4\xe5\xb9\xb3\xe6\x8b\xbc\xe6\x8e\xa5 - \xe5\xa4\x8d\xe9\x80\x89\xe6\xa1\x86\n        self.horizontal_stitching_check_val = tk.IntVar()\n        self.horizontal_stitching_check_val.set(0)\n        self.horizontal_stitching_check = ttk.Checkbutton(\n            self, text=\'Horizontal Stitching\',\n            variable=self.horizontal_stitching_check_val,\n            onvalue=1,\n            offvalue=0\n        )\n        self.layout_utils.next_to_widget(\n            src=self.horizontal_stitching_check,\n            target=self.replace_transparent_check,\n            width=130,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe4\xba\x8c\xe5\x80\xbc\xe5\x8c\x96 - \xe6\xa0\x87\xe7\xad\xbe\n        self.binaryzation_text = ttk.Label(self, text=\'Binaryzation\', anchor=tk.W)\n        self.layout_utils.next_to_widget(\n            src=self.binaryzation_text,\n            target=self.horizontal_stitching_check,\n            width=75,\n            height=20,\n            tiny_space=False\n        )\n\n        # \xe4\xba\x8c\xe5\x80\xbc\xe5\x8c\x96 - \xe8\xbe\x93\xe5\x85\xa5\xe6\xa1\x86\n        self.binaryzation_val = tk.IntVar()\n        self.binaryzation_val.set(-1)\n        self.binaryzation_entry = ttk.Entry(self, textvariable=self.binaryzation_val, justify=tk.LEFT)\n        self.layout_utils.next_to_widget(\n            src=self.binaryzation_entry,\n            target=self.binaryzation_text,\n            width=55,\n            height=20,\n            tiny_space=True\n        )\n\n        # \xe4\xbf\x9d\xe5\xad\x98 - \xe6\x8c\x89\xe9\x92\xae\n        self.btn_save = ttk.Button(self, text=\'Save Configuration\', command=lambda: self.save_conf())\n        self.layout_utils.widget_from_right(\n            src=self.btn_save,\n            target=self.label_frame_pretreatment,\n            width=120,\n            height=24,\n            tiny_space=True\n        )\n\n    @staticmethod\n    def check_btn_event(src: tk.IntVar, entry: tk.Entry):\n        if src.get() == 1:\n            entry[\'state\'] = tk.NORMAL\n        else:\n            entry[\'state\'] = tk.DISABLED\n        return None\n\n    def read_conf(self, entity):\n        self.pretreatment_entity = entity\n\n        try:\n\n            if entity.blend_frames == -1:\n                self.blend_frames_check_val.set(0)\n                self.blend_frames_val.set(json.dumps([-1]))\n            else:\n                self.blend_frames_check_val.set(1)\n                self.blend_frames_entry[\'state\'] = tk.NORMAL\n                self.blend_frames_val.set(json.dumps(entity.blend_frames))\n\n            if entity.concat_frames == -1:\n                self.concat_frames_check_val.set(0)\n                self.concat_frames_val.set(json.dumps([0, -1]))\n            else:\n                self.concat_frames_check_val.set(1)\n                self.concat_frames_entry[\'state\'] = tk.NORMAL\n                self.concat_frames_val.set(json.dumps(entity.concat_frames))\n\n            self.horizontal_stitching_check_val.set(1 if entity.horizontal_stitching else 0)\n            self.replace_transparent_check_val.set(1 if entity.replace_transparent else 0)\n\n            self.binaryzation_val.set(entity.binaryzation)\n\n        except Exception as e:\n            messagebox.showerror(\n                e.__class__.__name__, json.dumps(e.args)\n            )\n            return\n\n    def save_conf(self):\n        try:\n\n            if self.concat_frames_check_val.get() == 1:\n                self.pretreatment_entity.concat_frames = json.loads(self.concat_frames_val.get())\n            if self.blend_frames_check_val.get() == 1:\n                self.pretreatment_entity.blend_frames = json.loads(self.blend_frames_val.get())\n            self.pretreatment_entity.horizontal_stitching = True if self.horizontal_stitching_check_val.get() == 1 else False\n            self.pretreatment_entity.replace_transparent = True if self.replace_transparent_check_val.get() == 1 else False\n            self.pretreatment_entity.binaryzation = self.binaryzation_val.get()\n        except Exception as e:\n            messagebox.showerror(\n                e.__class__.__name__, json.dumps(e.args)\n            )\n            return\n\n        self.destroy()'"
gui/utils.py,0,"b""#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\n\n\nclass LayoutGUI(object):\n\n    def __init__(self, layout, window_width):\n        self.layout = layout\n        self.window_width = window_width\n\n    def widget_from_right(self, src, target, width, height, tiny_space=False):\n        target_edge = self.object_edge_info(target)\n        src.place(\n            x=self.window_width - width - self.layout['global']['space']['x'],\n            y=target_edge['edge_y'] + self.layout['global']['tiny_space' if tiny_space else 'space']['y'],\n            width=width,\n            height=height\n        )\n\n    def before_widget(self, src, target, width, height, tiny_space=False):\n        target_edge = self.object_edge_info(target)\n        src.place(\n            x=target_edge['x'] - width - self.layout['global']['tiny_space' if tiny_space else 'space']['x'],\n            y=target_edge['y'],\n            width=width,\n            height=height\n        )\n\n    @staticmethod\n    def object_edge_info(obj):\n        info = obj.place_info()\n        x = int(info['x'])\n        y = int(info['y'])\n        edge_x = int(info['x']) + int(info['width'])\n        edge_y = int(info['y']) + int(info['height'])\n        return {'x': x, 'y': y, 'edge_x': edge_x, 'edge_y': edge_y}\n\n    def inside_widget(self, src, target, width, height):\n        target_edge = self.object_edge_info(target)\n        src.place(\n            x=target_edge['x'] + self.layout['global']['space']['x'],\n            y=target_edge['y'] + self.layout['global']['space']['y'],\n            width=width,\n            height=height\n        )\n\n    def below_widget(self, src, target, width, height, tiny_space=False):\n        target_edge = self.object_edge_info(target)\n        src.place(\n            x=target_edge['x'],\n            y=target_edge['edge_y'] + self.layout['global']['tiny_space' if tiny_space else 'space']['y'],\n            width=width,\n            height=height\n        )\n\n    def next_to_widget(self, src, target, width, height, tiny_space=False, offset_y=0):\n        target_edge = self.object_edge_info(target)\n        src.place(\n            x=target_edge['edge_x'] + self.layout['global']['tiny_space' if tiny_space else 'space']['x'],\n            y=target_edge['y'] + offset_y,\n            width=width,\n            height=height\n        )\n\n"""
middleware/__init__.py,0,b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>'
middleware/random_captcha.py,0,"b'from PIL import Image, ImageDraw, ImageFont\nfrom enum import Enum, unique\nfrom fontTools.ttLib import TTFont\nimport numpy as np\nimport io\nimport os\nimport base64\nimport hashlib\nimport time\nimport random\nimport logging\n\n\nclass BackgroundType(Enum):\n    RANDOM = \'random\'\n    IMAGE = \'image\'\n    RGB = \'rgb\'\n\n\nclass RandomCaptcha(object):\n    def __init__(self):\n        self.__width = [130, 160]\n        self.__height = [50, 60]\n        self.__background_mode = BackgroundType.RGB\n        self.__background_img_assests_path = None\n        self.__rgb = {\n            \'r\': [0, 255],\n            \'g\': [0, 255],\n            \'b\': [0, 255]\n        }\n        self.__fonts_list = []\n        self.__samples = []\n        self.__fonts_num = [4, 4]\n        self.__font_size = [26, 36]\n        self.__font_mode = 0\n        self.__max_line_count = 2\n        self.__max_point_count = 20\n\n    @property\n    def max_point_count(self):\n        return self.__max_point_count\n\n    @max_point_count.setter\n    def max_point_count(self, value: int):\n        self.__max_point_count = value\n\n    @property\n    def max_line_count(self):\n        return self.__max_line_count\n\n    @max_line_count.setter\n    def max_line_count(self, value: int):\n        self.__max_line_count = value\n\n    @property\n    def font_mode(self):\n        return self.__font_mode\n\n    @font_mode.setter\n    def font_mode(self, value: int):\n        self.__font_mode = value\n\n    @property\n    def font_size(self) -> list:\n        return self.__font_size\n\n    @font_size.setter\n    def font_size(self, value: list):\n        if type(value) == list and type(value[0]) == int and type(value[1]) == int and value[0] >= 0 and value[1] > 0 and value[0] < value[1]:\n            self.__font_size = value\n        else:\n            raise ValueError(""input value should be like [0, 255]"")\n\n    @property\n    def fonts_num(self) -> list:\n        return self.__fonts_num\n\n    @fonts_num.setter\n    def fonts_num(self, value: list):\n        self.__fonts_num = value\n\n    @property\n    def sample(self) -> list:\n        return self.__samples\n\n    @sample.setter\n    def sample(self, value: list):\n        self.__samples = value\n\n    @property\n    def fonts_list(self) -> list:\n        return self.__fonts_list\n\n    @fonts_list.setter\n    def fonts_list(self, value: list):\n        self.__fonts_list = value\n\n    @property\n    def rgb(self) -> dict:\n        return self.__rgb\n\n    @property\n    def rgb_r(self) -> list:\n        return self.__rgb[\'r\']\n\n    @rgb_r.setter\n    def rgb_r(self, value: list):\n        if type(value) == list and type(value[0]) == int and type(value[1]) == int and value[0] >= 0 and value[1] > 0 and value[0] < value[1] and value[0] <= 255:\n            self.__rgb[\'r\'] = value\n        else:\n            raise ValueError(""input value should be like [0, 255]"")\n\n    @property\n    def rgb_g(self) -> list:\n        return self.__rgb[\'g\']\n\n    @rgb_g.setter\n    def rgb_g(self, value: list):\n        if type(value) == list and type(value[0]) == int and type(value[1]) == int and value[0] >= 0 and value[1] > 0 and value[0] < value[1] and value[0] <= 255:\n            self.__rgb[\'g\'] = value\n        else:\n            raise ValueError(""input value should be like [0, 255]"")\n\n    @property\n    def rgb_b(self) -> list:\n        return self.__rgb[\'b\']\n\n    @rgb_b.setter\n    def rgb_b(self, value: list):\n        if type(value) == list and type(value[0]) == int and type(value[1]) == int and value[0] >= 0 and value[1] > 0 and value[0] < value[1]:\n            self.__rgb[\'b\'] = value\n        else:\n            raise ValueError(""input value should be like [0, 255]"")\n\n    @property\n    def background_mode(self) -> BackgroundType:\n        return self.__background_mode\n\n    @background_mode.setter\n    def background_mode(self, value: BackgroundType):\n        self.__background_mode = value\n\n    @property\n    def background_img_path(self) -> str:\n        return self.__background_img_assests_path\n\n    @background_img_path.setter\n    def background_img_path(self, value: str):\n        self.__background_img_assests_path = value\n\n    @property\n    def height(self):\n        return self.__height\n\n    @height.setter\n    def height(self, value):\n        self.__height = value\n\n    @property\n    def width(self):\n        return self.__width\n\n    @width.setter\n    def width(self, value):\n        self.__width = value\n\n    def check_font(self):\n        for font_type in self.fonts_list:\n            try:\n                font = TTFont(font_type)\n                uni_map = font[\'cmap\'].tables[0].ttFont.getBestCmap()\n                for item in self.sample:\n                    codepoint = ord(str(item))\n                    if codepoint in uni_map.keys():\n                        continue\n                    else:\n                        font.close()\n                        raise Exception(""{} not found!"".format(item))\n            except Exception as e:\n                try:\n                    os.remove(font_type)\n                except:\n                    pass\n                del self.fonts_list[self.fonts_list.index(font_type)]\n\n        pass\n\n    def set_text(self, __image: ImageDraw, img_width, img_height):\n\n        if img_width >= 150:\n            font_size = random.choice(range(self.font_size[0], self.font_size[1]))\n        else:\n            font_size = random.choice(range(self.font_size[0], int((self.font_size[0] + self.font_size[1])/2)))\n\n        font_num = random.choice(range(self.fonts_num[0], self.fonts_num[1]))\n        max_width = int(img_width / font_num)\n        max_height = int(img_height)\n        font_type = random.choice(self.fonts_list)\n        try:\n            font = ImageFont.truetype(font_type, font_size)\n        except OSError:\n            del self.fonts_list[self.fonts_list.index(font_type)]\n            raise Exception(""{} opened fail"")\n        labels = []\n        for idx in range(font_num):\n            fw = range(int(max_width - font_size))\n            if len(fw) > 0:\n                x = max_width * idx + random.choice(fw)\n            else:\n                x = max_width * idx\n            y = random.choice(range(int(max_height - font_size)))\n            f = random.choice(self.sample)\n            labels.append(f)\n            __image.text((x, y), f, font=font,\n                         fill=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n        return labels, font_type\n\n    def set_noise(self, __image: ImageDraw, img_width, img_height):\n        for i in range(self.max_line_count):\n            # \xe5\x99\xaa\xe7\xba\xbf\xe7\x9a\x84\xe8\xb5\xb7\xe7\x82\xb9\xe6\xa8\xaa\xe5\x9d\x90\xe6\xa0\x87\xe5\x92\x8c\xe7\xba\xb5\xe5\x9d\x90\xe6\xa0\x87\n            x1 = random.randint(0, img_width)\n            y1 = random.randint(0, img_height)\n            # \xe5\x99\xaa\xe7\xba\xbf\xe7\x9a\x84\xe7\xbb\x88\xe7\x82\xb9\xe6\xa8\xaa\xe5\x9d\x90\xe6\xa0\x87\xe5\x92\x8c\xe7\xba\xb5\xe5\x9d\x90\xe6\xa0\x87\n            x2 = random.randint(0, img_width)\n            y2 = random.randint(0, img_height)\n            # \xe9\x80\x9a\xe8\xbf\x87\xe7\x94\xbb\xe7\xac\x94\xe5\xaf\xb9\xe8\xb1\xa1draw.line((\xe8\xb5\xb7\xe7\x82\xb9\xe7\x9a\x84xy, \xe7\xbb\x88\xe7\x82\xb9\xe7\x9a\x84xy), fill=\'\xe9\xa2\x9c\xe8\x89\xb2\')\xe6\x9d\xa5\xe5\x88\x92\xe7\xba\xbf\n            __image.line((x1, y1, x2, y2),\n                         fill=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n        for i in range(self.max_point_count):\n            __image.point([random.randint(0, img_width), random.randint(0, img_height)], fill=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n            x = random.randint(0, img_width)\n            y = random.randint(0, img_height)\n            __image.arc((x, y, x + 4, y + 4), 0, 40, fill=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n\n    def set_content(self, __image: ImageDraw, img_width, img_height):\n        labels, font_type = self.set_text(__image, img_width, img_height)\n        self.set_noise(__image, img_width, img_height)\n        return labels, font_type\n\n    def create(self, mode: str = ""bytes"", img_format: str = ""png""):\n        if type(self.width) == list:\n            img_width = random.choice(range(self.width[0], self.width[1]))\n        else:\n            img_width = self.width\n        if type(self.height) == list:\n            img_height = random.choice(range(self.height[0], self.height[1]))\n        else:\n            img_height = self.height\n\n        background_mode = self.background_mode\n        if type(background_mode) is BackgroundType:\n            if background_mode.value == BackgroundType.RGB.value:\n                rgb_range = self.rgb\n                r_range = rgb_range[\'r\']\n                g_range = rgb_range[\'g\']\n                b_range = rgb_range[\'b\']\n                rgb = (random.randint(r_range[0], r_range[1]), random.randint(g_range[0], g_range[1]),\n                       random.randint(b_range[0], b_range[1]))\n                __image = Image.new(\'RGB\', (img_width, img_height), rgb)\n                img = ImageDraw.Draw(__image)\n                labels, font_type = self.set_content(img, img_width, img_height)\n                if mode == ""bytes"":\n                    img_byte_arr = io.BytesIO()\n                    __image.save(img_byte_arr, format=img_format)\n                    return img_byte_arr.getvalue(), labels, font_type\n                elif mode == ""numpy"":\n                    return np.array(__image), labels, font_type\n                elif mode == ""base64"":\n                    img_byte_arr = io.BytesIO()\n                    __image.save(img_byte_arr, format=img_format)\n                    _bytes = img_byte_arr.getvalue()\n                    return base64.b64encode(_bytes).decode(), labels, font_type\n                else:\n                    raise FutureWarning(""\xe6\x9a\x82\xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xb1\xbb\xe5\x9e\x8b"")\n            else:\n                raise FutureWarning(""\xe6\x9a\x82\xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81\xe7\x9a\x84\xe8\x83\x8c\xe6\x99\xaf\xe7\xb1\xbb\xe5\x9e\x8b"")\n        else:\n            raise TypeError(""background mode must be BGMODEL."")\n'"
network/CNN.py,14,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport tensorflow as tf\nfrom network.utils import NetworkUtils\nfrom config import ModelConfig\nfrom tensorflow.python.keras.regularizers import l1\n\n\nclass CNN5(object):\n\n    """"""\n    CNN5\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\n    """"""\n    def __init__(self, model_conf: ModelConfig, inputs: tf.Tensor, utils: NetworkUtils):\n        """"""\n        :param model_conf: \xe4\xbb\x8e\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\n        :param inputs: \xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe8\xbe\x93\xe5\x85\xa5 tf.keras.layers.Input / tf.Tensor \xe7\xb1\xbb\xe5\x9e\x8b\n        :param utils: \xe7\xbd\x91\xe7\xbb\x9c\xe5\xb7\xa5\xe5\x85\xb7\xe7\xb1\xbb\n        """"""\n        self.model_conf = model_conf\n        self.inputs = inputs\n        self.utils = utils\n        self.loss_func = self.model_conf.loss_func\n\n    def build(self):\n        with tf.keras.backend.name_scope(""CNN5""):\n            x = self.utils.cnn_layer(0, inputs=self.inputs, kernel_size=7, filters=32, strides=(1, 1))\n            x = self.utils.cnn_layer(1, inputs=x, kernel_size=5, filters=64, strides=(1, 2))\n            x = self.utils.cnn_layer(2, inputs=x, kernel_size=3, filters=128, strides=(1, 2))\n            x = self.utils.cnn_layer(3, inputs=x, kernel_size=3, filters=128, strides=(1, 2))\n            x = self.utils.cnn_layer(4, inputs=x, kernel_size=3, filters=64, strides=(1, 2))\n            shape_list = x.get_shape().as_list()\n            print(""x.get_shape()"", shape_list)\n            return self.utils.reshape_layer(x, self.loss_func, shape_list)\n\n\nclass CNNX(object):\n\n    """""" \xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84 """"""\n    def __init__(self, model_conf: ModelConfig, inputs: tf.Tensor, utils: NetworkUtils):\n        self.model_conf = model_conf\n        self.inputs = inputs\n        self.utils = utils\n        self.loss_func = self.model_conf.loss_func\n\n    def block(self, inputs, filters, kernel_size, strides, dilation_rate=(1, 1)):\n        inputs = tf.keras.layers.Conv2D(\n            filters=filters,\n            dilation_rate=dilation_rate,\n            kernel_size=kernel_size,\n            strides=strides,\n            kernel_regularizer=l1(0.1),\n            kernel_initializer=self.utils.msra_initializer(kernel_size, filters),\n            padding=\'SAME\',\n        )(inputs)\n        inputs = tf.layers.batch_normalization(\n            inputs,\n            reuse=False,\n            momentum=0.9,\n            training=self.utils.is_training\n        )\n        inputs = self.utils.hard_swish(inputs)\n        return inputs\n\n    def build(self):\n        with tf.keras.backend.name_scope(\'CNNX\'):\n            x = self.inputs\n\n            x = self.block(x, filters=16, kernel_size=7, strides=1)\n\n            max_pool0 = tf.keras.layers.MaxPooling2D(\n                pool_size=(1, 2),\n                strides=2,\n                padding=\'same\')(x)\n            max_pool1 = tf.keras.layers.MaxPooling2D(\n                pool_size=(1, 3),\n                strides=2,\n                padding=\'same\')(x)\n            max_pool2 = tf.keras.layers.MaxPooling2D(\n                pool_size=(1, 5),\n                strides=2,\n                padding=\'same\')(x)\n            max_pool3 = tf.keras.layers.MaxPooling2D(\n                pool_size=(1, 7),\n                strides=2,\n                padding=\'same\')(x)\n\n            multi_scale_pool = tf.keras.layers.Add()([max_pool0, max_pool1, max_pool2, max_pool3])\n\n            x = self.block(multi_scale_pool, filters=32, kernel_size=5, strides=1)\n\n            x1 = self.utils.inverted_res_block(x, filters=16, stride=2, expansion=6, block_id=1)\n            x1 = self.utils.inverted_res_block(x1, filters=16, stride=1, expansion=6, block_id=2)\n\n            x2 = tf.keras.layers.MaxPooling2D(\n                pool_size=(2, 2),\n                strides=2,\n                padding=\'same\')(x)\n            x = tf.keras.layers.Concatenate()([x2, x1])\n\n            x = self.utils.inverted_res_block(x, filters=32, stride=2, expansion=6, block_id=3)\n            x = self.utils.inverted_res_block(x, filters=32, stride=1, expansion=6, block_id=4)\n\n            x = self.utils.dense_block(x, 2, name=\'dense_block\')\n\n            x = self.utils.inverted_res_block(x, filters=64, stride=1, expansion=6, block_id=5)\n\n            shape_list = x.get_shape().as_list()\n            print(""x.get_shape()"", shape_list)\n\n            return self.utils.reshape_layer(x, self.loss_func, shape_list)\n'"
network/DenseNet.py,8,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\n# This network was temporarily suspended\nimport tensorflow as tf\nfrom network.utils import NetworkUtils\nfrom config import ModelConfig\n\n\nclass DenseNet(object):\n\n    def __init__(self, model_conf: ModelConfig, inputs: tf.Tensor, utils: NetworkUtils):\n        self.model_conf = model_conf\n        self.inputs = inputs\n        self.utils = utils\n        self.loss_func = self.model_conf.loss_func\n        self.type = {\n            \'121\': [6, 12, 24, 16],\n            \'169\': [6, 12, 32, 32],\n            \'201\': [6, 12, 48, 32]\n        }\n        self.blocks = self.type[\'121\']\n        self.padding = ""SAME""\n\n    def build(self):\n\n        with tf.keras.backend.name_scope(\'DenseNet\'):\n\n            x = tf.keras.layers.Conv2D(64, 3, strides=2, use_bias=False, name=\'conv1/conv\', padding=\'same\')(self.inputs)\n            x = tf.layers.batch_normalization(\n                x,\n                epsilon=1.001e-5,\n                axis=3,\n                reuse=False,\n                momentum=0.9,\n                name=\'conv1/bn\',\n                training=self.utils.is_training,\n            )\n\n            x = tf.keras.layers.LeakyReLU(0.01, name=\'conv1/relu\')(x)\n            x = tf.keras.layers.MaxPooling2D(3, strides=2, name=\'pool1\', padding=\'same\')(x)\n            x = self.utils.dense_block(x, self.blocks[0], name=\'conv2\')\n            x = self.utils.transition_block(x, 0.5, name=\'pool2\')\n            x = self.utils.dense_block(x, self.blocks[1], name=\'conv3\')\n            x = self.utils.transition_block(x, 0.5, name=\'pool3\')\n            x = self.utils.dense_block(x, self.blocks[2], name=\'conv4\')\n            x = self.utils.transition_block(x, 0.5, name=\'pool4\')\n            x = self.utils.dense_block(x, self.blocks[3], name=\'conv5\')\n            x = tf.layers.batch_normalization(\n                x,\n                epsilon=1.001e-5,\n                axis=3,\n                reuse=False,\n                momentum=0.9,\n                name=\'bn\',\n                training=self.utils.is_training,\n            )\n\n            x = tf.keras.layers.LeakyReLU(0.01, name=\'conv6/relu\')(x)\n\n            shape_list = x.get_shape().as_list()\n            print(""x.get_shape()"", shape_list)\n\n            return self.utils.reshape_layer(x, self.loss_func, shape_list)\n'"
network/GRU.py,14,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\n\nimport tensorflow as tf\nfrom config import RunMode, ModelConfig\nfrom network.utils import NetworkUtils\n\n\nclass GRU(object):\n\n    def __init__(self, model_conf: ModelConfig, inputs: tf.Tensor, utils: NetworkUtils):\n        """"""\n        :param model_conf: \xe9\x85\x8d\xe7\xbd\xae\n        :param inputs: \xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe8\xbe\x93\xe5\x85\xa5tf.keras.layers.Input/tf.Tensor\xe7\xb1\xbb\xe5\x9e\x8b\n        :param utils: \xe7\xbd\x91\xe7\xbb\x9c\xe5\xb7\xa5\xe5\x85\xb7\xe7\xb1\xbb\n        """"""\n        self.model_conf = model_conf\n        self.inputs = inputs\n        self.utils = utils\n        self.layer = None\n\n    def build(self):\n        """"""\n        \xe5\xbe\xaa\xe7\x8e\xaf\xe5\xb1\x82\xe6\x9e\x84\xe5\xbb\xba\xe5\x8f\x82\xe6\x95\xb0\n        :return: \xe8\xbf\x94\xe5\x9b\x9e\xe5\xbe\xaa\xe7\x8e\xaf\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\n        """"""\n        with tf.keras.backend.name_scope(\'GRU\'):\n            mask = tf.keras.layers.Masking()(self.inputs)\n            self.layer = tf.keras.layers.GRU(\n                units=self.model_conf.units_num * 2,\n                return_sequences=True,\n                input_shape=mask.shape,\n                # reset_after=True,\n            )\n            outputs = self.layer(mask, training=self.utils.is_training)\n        return outputs\n\n\nclass BiGRU(object):\n\n    def __init__(self, model_conf: ModelConfig, inputs: tf.Tensor, utils: NetworkUtils):\n        self.model_conf = model_conf\n        self.inputs = inputs\n        self.utils = utils\n        self.layer = None\n\n    def build(self):\n        with tf.keras.backend.name_scope(\'BiGRU\'):\n            mask = tf.keras.layers.Masking()(self.inputs)\n            self.layer = tf.keras.layers.Bidirectional(\n                layer=tf.keras.layers.GRU(\n                    units=self.model_conf.units_num,\n                    return_sequences=True,\n                ),\n                input_shape=mask.shape,\n            )\n            outputs = self.layer(mask, training=self.utils.is_training)\n        return outputs\n\n\nclass GRUcuDNN(object):\n\n    def __init__(self, model_conf: ModelConfig, inputs: tf.Tensor, utils: NetworkUtils):\n        self.model_conf = model_conf\n        self.inputs = inputs\n        self.utils = utils\n        self.layer = None\n\n    def build(self):\n        with tf.keras.backend.name_scope(\'GRU\'):\n            mask = tf.keras.layers.Masking()(self.inputs)\n            self.layer = tf.keras.layers.GRU(\n                units=self.model_conf.units_num * 2,\n                return_sequences=True,\n                input_shape=mask.shape,\n                reset_after=True\n            )\n            outputs = self.layer(mask, training=self.utils.is_training)\n        return outputs\n'"
network/LSTM.py,17,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport tensorflow as tf\nfrom config import RunMode, ModelConfig\nfrom network.utils import NetworkUtils\n\n\nclass LSTM(object):\n    """"""\n    LSTM \xe7\xbd\x91\xe7\xbb\x9c\xe5\xae\x9e\xe7\x8e\xb0\n    """"""\n    def __init__(self, model_conf: ModelConfig, inputs: tf.Tensor, utils: NetworkUtils):\n        """"""\n        :param model_conf: \xe9\x85\x8d\xe7\xbd\xae\n        :param inputs: \xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe8\xbe\x93\xe5\x85\xa5 tf.keras.layers.Input / tf.Tensor \xe7\xb1\xbb\xe5\x9e\x8b\n        :param utils: \xe7\xbd\x91\xe7\xbb\x9c\xe5\xb7\xa5\xe5\x85\xb7\xe7\xb1\xbb\n        """"""\n        self.model_conf = model_conf\n        self.inputs = inputs\n        self.utils = utils\n        self.layer = None\n\n    def build(self):\n        """"""\n        \xe5\xbe\xaa\xe7\x8e\xaf\xe5\xb1\x82\xe6\x9e\x84\xe5\xbb\xba\xe5\x8f\x82\xe6\x95\xb0\n        :return: \xe8\xbf\x94\xe5\x9b\x9e\xe5\xbe\xaa\xe7\x8e\xaf\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\n        """"""\n        with tf.keras.backend.name_scope(\'LSTM\'):\n            mask = tf.keras.layers.Masking()(self.inputs)\n            self.layer = tf.keras.layers.LSTM(\n                units=self.model_conf.units_num * 2,\n                return_sequences=True,\n                input_shape=mask.shape,\n                dropout=0.2,\n                recurrent_dropout=0.1\n            )\n            outputs = self.layer(mask, training=self.utils.is_training)\n        return outputs\n\n\nclass BiLSTM(object):\n\n    def __init__(self, model_conf: ModelConfig, inputs: tf.Tensor, utils: NetworkUtils):\n        """"""\xe5\x90\x8c\xe4\xb8\x8a""""""\n        self.model_conf = model_conf\n        self.inputs = inputs\n        self.utils = utils\n        self.layer = None\n\n    def build(self):\n        """"""\xe5\x90\x8c\xe4\xb8\x8a""""""\n        with tf.keras.backend.name_scope(\'BiLSTM\'):\n            mask = tf.keras.layers.Masking()(self.inputs)\n            self.layer = tf.keras.layers.Bidirectional(\n                layer=tf.keras.layers.LSTM(\n                    units=self.model_conf.units_num,\n                    return_sequences=True,\n                ),\n                input_shape=mask.shape,\n            )\n            outputs = self.layer(mask, training=self.utils.is_training)\n        return outputs\n\n\nclass LSTMcuDNN(object):\n\n    def __init__(self, model_conf: ModelConfig, inputs: tf.Tensor, utils: NetworkUtils):\n        """"""\xe5\x90\x8c\xe4\xb8\x8a""""""\n        self.model_conf = model_conf\n        self.inputs = inputs\n        self.utils = utils\n        self.layer = None\n\n    def build(self):\n        """"""\xe5\x90\x8c\xe4\xb8\x8a""""""\n        with tf.keras.backend.name_scope(\'LSTM\'):\n            self.layer = tf.keras.layers.CuDNNLSTM(\n                units=self.model_conf.units_num * 2,\n                return_sequences=True,\n            )\n            outputs = self.layer(self.inputs, training=self.utils.is_training)\n        return outputs\n\n\nclass BiLSTMcuDNN(object):\n\n    def __init__(self, model_conf: ModelConfig, inputs: tf.Tensor, utils: NetworkUtils):\n        """"""\xe5\x90\x8c\xe4\xb8\x8a""""""\n        self.model_conf = model_conf\n        self.inputs = inputs\n        self.utils = utils\n        self.layer = None\n\n    def build(self):\n        """"""\xe5\x90\x8c\xe4\xb8\x8a""""""\n        with tf.keras.backend.name_scope(\'BiLSTM\'):\n            self.layer = tf.keras.layers.Bidirectional(\n                layer=tf.keras.layers.CuDNNLSTM(\n                    units=self.model_conf.units_num,\n                    return_sequences=True\n                )\n            )\n            outputs = self.layer(self.inputs, training=self.utils.is_training)\n        return outputs\n'"
network/MobileNet.py,8,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\n# This network was temporarily suspended\nimport tensorflow as tf\nfrom network.utils import NetworkUtils\nfrom config import ModelConfig\n\n\nclass MobileNetV2(object):\n\n    def __init__(self, model_conf: ModelConfig, inputs: tf.Tensor, utils: NetworkUtils):\n        self.model_conf = model_conf\n        self.inputs = inputs\n        self.utils = utils\n        self.loss_func = self.model_conf.loss_func\n        self.last_block_filters = 1280\n        self.padding = ""SAME""\n\n    def first_layer(self, inputs):\n        x = tf.keras.layers.Conv2D(\n            filters=32,\n            kernel_size=(3, 3),\n            strides=(2, 2),\n            padding=\'same\',\n            kernel_initializer=\'he_normal\',\n            name=\'conv1\')(inputs)\n        x = tf.layers.batch_normalization(\n            x,\n            reuse=False,\n            momentum=0.9,\n            training=self.utils.is_training\n        )\n        # x = self.utils.BatchNormalization(name=\'bn_conv1\', momentum=0.999)(x, training=self.utils.is_training)\n        x = tf.keras.layers.LeakyReLU(0.01)(x)\n\n        return x\n\n    def pwise_block(self, inputs):\n        x = tf.keras.layers.Conv2D(\n            self.last_block_filters,\n            kernel_size=1,\n            use_bias=False,\n            name=\'Conv_1\')(inputs)\n        x = tf.layers.batch_normalization(\n            x,\n            reuse=False,\n            momentum=0.9,\n            training=self.utils.is_training\n        )\n\n        x = tf.keras.layers.ReLU(6., name=\'out_relu\')(x)\n        return x\n\n    def build(self):\n\n        with tf.keras.backend.name_scope(\'MobileNetV2\'):\n\n            x = self.first_layer(self.inputs)\n\n            x = self.utils.inverted_res_block(x, filters=16, stride=1, expansion=1, block_id=0)\n\n            x = self.utils.inverted_res_block(x, filters=24, stride=2, expansion=6, block_id=1)\n            x = self.utils.inverted_res_block(x, filters=24, stride=1, expansion=6, block_id=2)\n\n            x = self.utils.inverted_res_block(x, filters=32, stride=2, expansion=6, block_id=3)\n            x = self.utils.inverted_res_block(x, filters=32, stride=1, expansion=6, block_id=4)\n            x = self.utils.inverted_res_block(x, filters=32, stride=1, expansion=6, block_id=5)\n\n            x = self.utils.inverted_res_block(x, filters=64, stride=2, expansion=6, block_id=6)\n            x = self.utils.inverted_res_block(x, filters=64, stride=1, expansion=6, block_id=7)\n            x = self.utils.inverted_res_block(x, filters=64, stride=1, expansion=6, block_id=8)\n            x = self.utils.inverted_res_block(x, filters=64, stride=1, expansion=6, block_id=9)\n\n            x = self.utils.inverted_res_block(x, filters=96, stride=1, expansion=6, block_id=10)\n            x = self.utils.inverted_res_block(x, filters=96, stride=1, expansion=6, block_id=11)\n            x = self.utils.inverted_res_block(x, filters=96, stride=1, expansion=6, block_id=12)\n\n            x = self.utils.inverted_res_block(x, filters=160, stride=2, expansion=6, block_id=13)\n            x = self.utils.inverted_res_block(x, filters=160, stride=1, expansion=6, block_id=14)\n            x = self.utils.inverted_res_block(x, filters=160, stride=1, expansion=6, block_id=15)\n\n            x = self.utils.inverted_res_block(x, filters=320, stride=1, expansion=6, block_id=16)\n\n            x = self.pwise_block(x)\n\n            shape_list = x.get_shape().as_list()\n            print(""x.get_shape()"", shape_list)\n\n            return self.utils.reshape_layer(x, self.loss_func, shape_list)\n'"
network/ResNet.py,8,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\n\nimport tensorflow as tf\nfrom network.utils import NetworkUtils\nfrom config import ModelConfig\n\n\nclass ResNetUtils(object):\n\n    def __init__(self, utils: NetworkUtils):\n        self.utils = utils\n\n    def first_layer(self, inputs):\n        x = tf.keras.layers.Conv2D(\n            filters=64,\n            kernel_size=(7, 7),\n            strides=(2, 2),\n            padding=\'same\',\n            kernel_initializer=\'he_normal\',\n            name=\'conv1\')(inputs)\n        x = tf.layers.batch_normalization(\n            x,\n            reuse=False,\n            momentum=0.9,\n            training=self.utils.is_training,\n            name=\'bn_conv1\',\n        )\n        x = tf.keras.layers.LeakyReLU(0.01)(x)\n        x = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\',)(x)\n        return x\n\n\nclass ResNet50(object):\n\n    def __init__(self, model_conf: ModelConfig, inputs: tf.Tensor, utils: NetworkUtils):\n        self.model_conf = model_conf\n        self.inputs = inputs\n        self.utils = utils\n        self.loss_func = self.model_conf.loss_func\n\n    def build(self):\n\n        with tf.keras.backend.name_scope(\'ResNet50\'):\n            x = ResNetUtils(self.utils).first_layer(self.inputs)\n            x = self.utils.residual_building_block(x, 3, [64, 64, 256], stage=2, block=\'a\', strides=(1, 1))\n            x = self.utils.identity_block(x, 3, [64, 64, 256], stage=2, block=\'b\')\n            x = self.utils.identity_block(x, 3, [64, 64, 256], stage=2, block=\'c\')\n\n            x = self.utils.residual_building_block(x, 3, [128, 128, 512], stage=3, block=\'a\')\n            x = self.utils.identity_block(x, 3, [128, 128, 512], stage=3, block=\'b\')\n            x = self.utils.identity_block(x, 3, [128, 128, 512], stage=3, block=\'c\')\n            x = self.utils.identity_block(x, 3, [128, 128, 512], stage=3, block=\'d\')\n\n            x = self.utils.residual_building_block(x, 3, [256, 256, 1024], stage=4, block=\'a\')\n            x = self.utils.identity_block(x, 3, [256, 256, 1024], stage=4, block=\'b\')\n            x = self.utils.identity_block(x, 3, [256, 256, 1024], stage=4, block=\'c\')\n            x = self.utils.identity_block(x, 3, [256, 256, 1024], stage=4, block=\'d\')\n            x = self.utils.identity_block(x, 3, [256, 256, 1024], stage=4, block=\'e\')\n            x = self.utils.identity_block(x, 3, [256, 256, 1024], stage=4, block=\'f\')\n\n            x = self.utils.residual_building_block(x, 3, [512, 512, 2048], stage=5, block=\'a\', strides=(1, 1))\n            x = self.utils.identity_block(x, 3, [512, 512, 2048], stage=5, block=\'b\')\n            x = self.utils.identity_block(x, 3, [512, 512, 2048], stage=5, block=\'c\')\n\n            print(""x.get_shape()"", x.get_shape())\n            shape_list = x.get_shape().as_list()\n            return self.utils.reshape_layer(x, self.loss_func, shape_list)\n\n\nclass ResNetTiny(object):\n\n    def __init__(self, model_conf: ModelConfig, inputs: tf.Tensor, utils: NetworkUtils):\n        self.model_conf = model_conf\n        self.inputs = inputs\n        self.utils = utils\n        self.loss_func = self.model_conf.loss_func\n\n    def build(self):\n\n        with tf.keras.backend.name_scope(\'ResNetTiny\'):\n            x = ResNetUtils(self.utils).first_layer(self.inputs)\n            x = self.utils.residual_building_block(x, 3, [64, 64, 128], stage=2, block=\'a\', strides=(1, 1), s2=False)\n            x = self.utils.identity_block(x, 3, [64, 64, 128], stage=2, block=\'b\')\n\n            x = self.utils.residual_building_block(x, 3, [128, 128, 256], stage=3, block=\'a\', s1=False, s2=False)\n            x = self.utils.identity_block(x, 3, [128, 128, 256], stage=3, block=\'b\')\n\n            x = self.utils.residual_building_block(x, 3, [256, 256, 512], stage=4, block=\'a\', s1=False, s2=False)\n            x = self.utils.identity_block(x, 3, [256, 256, 512], stage=4, block=\'b\')\n\n            x = self.utils.residual_building_block(x, 3, [512, 512, 1024], stage=5, block=\'a\', strides=(1, 1), s1=False)\n            x = self.utils.identity_block(x, 3, [512, 512, 1024], stage=5, block=\'b\')\n\n            shape_list = x.get_shape().as_list()\n            print(""x.get_shape()"", shape_list)\n\n            return self.utils.reshape_layer(x, self.loss_func, shape_list)\n'"
network/utils.py,64,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport math\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.keras.regularizers import l2, l1_l2, l1\nfrom config import RunMode, LossFunction, exception, ConfigException\n\n\nclass NetworkUtils(object):\n    """"""\n    \xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x84\xe5\x90\x88\xe5\x9d\x97 - \xe7\xbb\x86\xe8\x8a\x82\xe5\xae\x9e\xe7\x8e\xb0\n    \xe8\xaf\xb4\xe6\x98\x8e: \xe6\x9c\xac\xe7\xb1\xbb\xe4\xb8\xad\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84BN\xe5\xae\x9e\xe7\x8e\xb0\xe9\x83\xbd\xe9\x87\x87\xe7\x94\xa8: tf.layers.batch_normalization\n    \xe4\xb8\xba\xe4\xbb\x80\xe4\xb9\x88\xe4\xb8\x8d\xe7\x94\xa8 \xe3\x80\x90tf.keras.layers.BatchNormalization/tf.layers.BatchNormalization]\n    \xe5\x89\x8d\xe8\x80\x85: `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)`\n    should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n    \xe5\xb0\x9d\xe8\xaf\x95\xe8\xbf\x87\xe4\xbb\xa5\xe4\xb8\x8b\xe6\x94\xb9\xe8\xbf\x9b\xe6\x97\xa0\xe6\x9e\x9c:\n    --------------------------------------------------------------------------------------\n        class BatchNormalization(tf.keras.layers.BatchNormalization):\n\n            def call(self, *args, **kwargs):\n                outputs = super(BatchNormalization, self).call(*args, **kwargs)\n                for u in self.updates:\n                    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, u)\n                return outputs\n    --------------------------------------------------------------------------------------\n    \xe5\x90\x8e\xe8\x80\x85: \xe8\x99\xbd\xe7\x84\xb6 BN \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 tf.Operation \xe5\x9c\xa8 tf.GraphKeys.UPDATE_OPS \xe4\xb8\xad, \xe4\xbd\x86\xe6\x98\xaf[Predict]\xe6\xa8\xa1\xe5\xbc\x8f\xe4\xb8\x8b\xe4\xbe\x9d\xe6\x97\xa7\xe7\xbb\x93\xe6\x9e\x9c\xe6\xac\xa0\xe4\xbd\xb3\n    """"""\n\n    def __init__(self, mode: RunMode):\n        """"""\n        :param mode: RunMode \xe7\xb1\xbb, \xe4\xb8\xbb\xe8\xa6\x81\xe7\x94\xa8\xe4\xba\x8e\xe6\x8e\xa7\xe5\x88\xb6 is_training \xe6\xa0\x87\xe5\xbf\x97\n        """"""\n        self.mode = mode\n        self.is_training = self._is_training()\n\n    def _is_training(self):\n        """""" \xe5\x8f\x96\xe6\xb6\x88 is_training \xe5\x8d\xa0\xe4\xbd\x8d\xe7\xac\xa6\xe4\xbd\x9c\xe4\xb8\xba[Predict]\xe6\xa8\xa1\xe5\xbc\x8f\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe4\xbe\x9d\xe8\xb5\x96 """"""\n        return False if self.mode == RunMode.Predict else tf.keras.backend.placeholder(dtype=tf.bool)\n\n    @staticmethod\n    def hard_swish(x, name=\'hard_swish\'):\n        with tf.name_scope(name):\n            h_swish = x * tf.nn.relu6(x + 3) / 6\n            return h_swish\n\n    @staticmethod\n    def msra_initializer(kl, dl):\n        """""" MSRA weight initializer\n        (https://arxiv.org/pdf/1502.01852.pdf)\n        Keyword arguments:\n        kl -- kernel size\n        dl -- filter numbers\n        """"""\n\n        stddev = math.sqrt(2. / (kl ** 2 * dl))\n        return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n\n    def reshape_layer(self, input_tensor, loss_func, shape_list):\n        if loss_func == LossFunction.CTC:\n            output_tensor = tf.keras.layers.TimeDistributed(\n                layer=tf.keras.layers.Flatten(),\n            )(inputs=input_tensor, training=self.is_training)\n        elif loss_func == LossFunction.CrossEntropy:\n            output_tensor = tf.keras.layers.Reshape([shape_list[1], shape_list[2] * shape_list[3]])(input_tensor)\n        else:\n            raise exception(""The current loss function is not supported."", ConfigException.LOSS_FUNC_NOT_SUPPORTED)\n        return output_tensor\n\n    def cnn_layer(self, index, inputs, filters, kernel_size, strides):\n        """"""\xe5\x8d\xb7\xe7\xa7\xaf-BN-\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0-\xe6\xb1\xa0\xe5\x8c\x96\xe7\xbb\x93\xe6\x9e\x84\xe5\x9d\x97""""""\n\n        with tf.keras.backend.name_scope(\'unit-{}\'.format(index + 1)):\n            x = tf.keras.layers.Conv2D(\n                filters=filters,\n                kernel_size=kernel_size,\n                strides=strides[0],\n                kernel_regularizer=l1(0.01),\n                kernel_initializer=self.msra_initializer(kernel_size, filters),\n                padding=\'same\',\n                name=\'cnn-{}\'.format(index + 1),\n            )(inputs)\n            x = tf.layers.batch_normalization(\n                x,\n                fused=True,\n                renorm_clipping={\n                    \'rmax\': 3,\n                    \'rmin\': 0.3333,\n                    \'dmax\': 5\n                } if index == 0 else None,\n                reuse=False,\n                momentum=0.9,\n                name=\'bn{}\'.format(index + 1),\n                training=self.is_training\n            )\n            x = tf.keras.layers.LeakyReLU(0.01)(x)\n            x = tf.keras.layers.MaxPooling2D(\n                pool_size=(2, 2),\n                strides=strides[1],\n                padding=\'same\',\n            )(x)\n        return x\n\n    def dense_building_block(self, input_tensor, growth_rate, name, dropout_rate=None):\n        """"""A building block for a dense block.\n\n        # Arguments\n            input_tensor: input tensor.\n            growth_rate: float, growth rate at dense layers.\n            name: string, block label.\n\n        # Returns\n            Output tensor for the block.\n        """"""\n        # 1x1 Convolution (Bottleneck layer)\n        x = tf.layers.batch_normalization(\n            input_tensor,\n            reuse=False,\n            momentum=0.9,\n            training=self.is_training,\n            name=name + \'_0_bn\',\n        )\n        x = tf.keras.layers.LeakyReLU(0.01, name=name + \'_0_relu\')(x)\n        x = tf.keras.layers.Conv2D(\n            filters=4 * growth_rate,\n            kernel_size=1,\n            use_bias=False,\n            padding=\'same\',\n            name=name + \'_1_conv\')(x)\n\n        if dropout_rate:\n            x = tf.keras.layers.Dropout(dropout_rate)(x)\n\n        # 3x3 Convolution\n        x = tf.layers.batch_normalization(\n            x,\n            reuse=False,\n            momentum=0.9,\n            training=self.is_training,\n            name=name + \'_1_bn\',\n        )\n        x = tf.keras.layers.LeakyReLU(0.01, name=name + \'_1_relu\')(x)\n        x = tf.keras.layers.Conv2D(\n            filters=growth_rate,\n            kernel_size=3,\n            padding=\'same\',\n            use_bias=False,\n            name=name + \'_2_conv\')(x)\n\n        if dropout_rate:\n            x = tf.keras.layers.Dropout(dropout_rate)(x)\n\n        x = tf.keras.layers.Concatenate(name=name + \'_concat\')([input_tensor, x])\n        return x\n\n    def dense_block(self, input_tensor, blocks, name):\n        """"""A dense block.\n\n        # Arguments\n            input_tensor: input tensor.\n            blocks: integer, the number of building blocks.\n            name: string, block label.\n\n        # Returns conv_block\n            output tensor for the block.\n        """"""\n        for i in range(blocks):\n            input_tensor = self.dense_building_block(input_tensor, 32, name=name + \'_block\' + str(i + 1))\n        return input_tensor\n\n    def transition_block(self, input_tensor, reduction, name):\n        """"""A transition block.\n\n        # Arguments\n            input_tensor: input tensor.\n            reduction: float, compression rate at transition layers.\n            name: string, block label.\n\n        # Returns\n            output tensor for the block.\n        """"""\n        x = tf.layers.batch_normalization(\n            input_tensor,\n            reuse=False,\n            momentum=0.9,\n            training=self.is_training,\n            name=name + \'_bn\'\n        )\n        x = tf.keras.layers.LeakyReLU(0.01)(x)\n        x = tf.keras.layers.Conv2D(\n            filters=int(tf.keras.backend.int_shape(x)[3] * reduction),\n            kernel_size=1,\n            use_bias=False,\n            padding=\'same\',\n            name=name + \'_conv\')(x)\n        x = tf.keras.layers.AveragePooling2D(2, strides=2, name=name + \'_pool\', padding=\'same\')(x)\n        return x\n\n    def residual_building_block(self, input_tensor, kernel_size, filters, stage, block, strides=(2, 2), s1=True,\n                                s2=True):\n        """"""A block that has a conv layer at shortcut.\n\n        # Arguments\n            input_tensor: input tensor\n            kernel_size: default 3, the kernel size of\n                middle conv layer at main path\n            filters: list of integers, the filters of 3 conv layer at main path\n            stage: integer, current stage label, used for generating layer names\n            block: \'a\',\'b\'..., current block label, used for generating layer names\n            strides: Strides for the first conv layer in the block.\n\n        # Returns\n            Output tensor for the block.\n\n        Note that from stage 3,\n        the first conv layer at main path is with strides=(2, 2)\n        And the shortcut should have strides=(2, 2) as well\n        """"""\n        filters1, filters2, filters3 = filters\n        conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n        bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n        x = tf.keras.layers.Conv2D(\n            filters=filters1,\n            kernel_size=(1, 1),\n            strides=strides,\n            kernel_initializer=\'he_normal\',\n            padding=\'same\',\n            name=conv_name_base + \'2a\')(input_tensor)\n        x = tf.layers.batch_normalization(\n            x,\n            reuse=False,\n            momentum=0.9,\n            training=self.is_training,\n            name=bn_name_base + \'2a\'\n        )\n        x = tf.keras.layers.LeakyReLU(0.01)(x)\n\n        x = tf.keras.layers.Conv2D(\n            filters=filters2,\n            kernel_size=kernel_size,\n            padding=\'same\',\n            kernel_initializer=\'he_normal\',\n            name=conv_name_base + \'2b\')(x)\n        x = tf.layers.batch_normalization(\n            x,\n            reuse=False,\n            momentum=0.9,\n            training=self.is_training,\n            name=bn_name_base + \'2b\'\n        )\n        x = tf.keras.layers.LeakyReLU(0.01)(x)\n\n        x = tf.keras.layers.Conv2D(\n            filters=filters3,\n            kernel_size=(1, 1),\n            kernel_initializer=\'he_normal\',\n            padding=\'same\',\n            name=conv_name_base + \'2c\')(x)\n        x = tf.layers.batch_normalization(\n            x,\n            reuse=False,\n            momentum=0.9,\n            training=self.is_training,\n            name=bn_name_base + \'2c\'\n        )\n\n        shortcut = tf.keras.layers.Conv2D(\n            filters=filters3,\n            kernel_size=(1, 1),\n            strides=strides,\n            kernel_initializer=\'he_normal\',\n            padding=\'same\',\n            name=conv_name_base + \'1\')(input_tensor)\n        shortcut = tf.layers.batch_normalization(\n            shortcut,\n            reuse=False,\n            momentum=0.9,\n            training=self.is_training,\n            name=bn_name_base + \'1\'\n        )\n\n        x = tf.keras.layers.add([x, shortcut])\n        x = tf.keras.layers.LeakyReLU(0.01)(x)\n        return x\n\n    def identity_block(self, input_tensor, kernel_size, filters, stage, block):\n        """"""The identity block is the block that has no conv layer at shortcut.\n\n        # Arguments\n            input_tensor: input tensor\n            kernel_size: default 3, the kernel size of\n                middle conv layer at main path\n            filters: list of integers, the filters of 3 conv layer at main path\n            stage: integer, current stage label, used for generating layer names\n            block: \'a\',\'b\'..., current block label, used for generating layer names\n\n        # Returns\n            Output tensor for the block.\n        """"""\n        filters1, filters2, filters3 = filters\n        bn_axis = 3\n        conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n        bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n        x = tf.keras.layers.Conv2D(\n            filters=filters1,\n            kernel_size=(1, 1),\n            kernel_initializer=\'he_normal\',\n            padding=\'same\',\n            name=conv_name_base + \'2a\'\n        )(input_tensor)\n        x = tf.layers.batch_normalization(\n            x,\n            axis=bn_axis,\n            reuse=False,\n            momentum=0.9,\n            training=self.is_training,\n            name=bn_name_base + \'2a\',\n        )\n        x = tf.keras.layers.LeakyReLU(0.01)(x)\n\n        x = tf.keras.layers.Conv2D(\n            filters=filters2,\n            kernel_size=kernel_size,\n            padding=\'same\',\n            kernel_initializer=\'he_normal\',\n            name=conv_name_base + \'2b\'\n        )(x)\n        x = tf.layers.batch_normalization(\n            x,\n            axis=bn_axis,\n            reuse=False,\n            momentum=0.9,\n            training=self.is_training,\n            name=bn_name_base + \'2b\',\n        )\n        x = tf.keras.layers.LeakyReLU(0.01)(x)\n\n        x = tf.keras.layers.Conv2D(\n            filters=filters3,\n            kernel_size=(1, 1),\n            padding=\'same\',\n            kernel_initializer=\'he_normal\',\n            name=conv_name_base + \'2c\')(x)\n        x = tf.layers.batch_normalization(\n            x,\n            axis=bn_axis,\n            reuse=False,\n            momentum=0.9,\n            training=self.is_training,\n            name=bn_name_base + \'2c\',\n        )\n        x = tf.keras.layers.add([x, input_tensor])\n        x = tf.keras.layers.LeakyReLU(0.01)(x)\n        return x\n\n    @staticmethod\n    def _make_divisible(v, divisor, min_value=None):\n        if min_value is None:\n            min_value = divisor\n        new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n        # Make sure that round down does not go down by more than 10%.\n        if new_v < 0.9 * v:\n            new_v += divisor\n        return new_v\n\n    def inverted_res_block(self, input_tensor, expansion, stride, filters, block_id):\n        channel_axis = 1 if tf.keras.backend.image_data_format() == \'channels_first\' else -1\n\n        in_channels = tf.keras.backend.int_shape(input_tensor)[channel_axis]\n        pointwise_filters = int(filters)\n        x = input_tensor\n        prefix = \'block_{}_\'.format(block_id)\n\n        if block_id:\n            # Expand\n            x = tf.keras.layers.Conv2D(\n                expansion * in_channels,\n                kernel_size=1,\n                padding=\'same\',\n                use_bias=False,\n                activation=None,\n                name=prefix + \'expand\'\n            )(x)\n            x = tf.layers.batch_normalization(\n                x,\n                reuse=False,\n                momentum=0.9,\n                training=self.is_training\n            )\n            x = self.hard_swish(x)\n\n        else:\n            prefix = \'expanded_conv_\'\n\n        # Depthwise\n        x = tf.keras.layers.DepthwiseConv2D(\n            kernel_size=3,\n            strides=stride,\n            activation=None,\n            use_bias=False,\n            padding=\'same\',\n            name=prefix + \'depthwise\'\n        )(x)\n        x = tf.layers.batch_normalization(\n            x,\n            reuse=False,\n            momentum=0.9,\n            training=self.is_training\n        )\n\n        x = self.hard_swish(x)\n\n        # Project\n        x = tf.keras.layers.Conv2D(\n            pointwise_filters,\n            kernel_size=1,\n            padding=\'same\',\n            use_bias=False,\n            activation=None,\n            name=prefix + \'project\'\n        )(x)\n        x = tf.layers.batch_normalization(\n            x,\n            reuse=False,\n            momentum=0.9,\n            training=self.is_training\n        )\n\n        if in_channels == pointwise_filters and stride == 1:\n            return tf.keras.layers.Add(name=prefix + \'add\')([input_tensor, x])\n        return x\n'"
optimizer/AdaBound.py,5,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport tensorflow as tf\nfrom distutils.version import StrictVersion\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.ops.clip_ops import clip_by_value\n\n""""""Implements AdaBound algorithm.\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): Adam learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\n    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n        https://openreview.net/forum?id=Bkg3g2R9FX\n    """"""\n\n\nclass AdaBoundOptimizer(optimizer.Optimizer):\n    def __init__(self, learning_rate=0.001, final_lr=0.1, beta1=0.9, beta2=0.999,\n                 gamma=1e-3, epsilon=1e-8, amsbound=False,\n                 use_locking=False, name=""AdaBound""):\n        super(AdaBoundOptimizer, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._final_lr = final_lr\n        self._beta1 = beta1\n        self._beta2 = beta2\n        self._epsilon = epsilon\n\n        self._gamma = gamma\n        self._amsbound = amsbound\n\n        self._lr_t = None\n        self._beta1_t = None\n        self._beta2_t = None\n        self._epsilon_t = None\n\n    def _create_slots(self, var_list):\n        first_var = min(var_list, key=lambda x: x.name)\n        if StrictVersion(tf.__version__) >= StrictVersion(\'1.10.0\'):\n            graph = None if context.executing_eagerly() else ops.get_default_graph()\n        else:\n            graph = ops.get_default_graph()\n        create_new = self._get_non_slot_variable(""beta1_power"", graph) is None\n        if not create_new and context.in_graph_mode():\n            create_new = (self._get_non_slot_variable(""beta1_power"", graph).graph is not first_var.graph)\n\n        if create_new:\n            self._create_non_slot_variable(initial_value=self._beta1,\n                                           name=""beta1_power"",\n                                           colocate_with=first_var)\n            self._create_non_slot_variable(initial_value=self._beta2,\n                                           name=""beta2_power"",\n                                           colocate_with=first_var)\n            self._create_non_slot_variable(initial_value=self._gamma,\n                                           name=""gamma_multi"",\n                                           colocate_with=first_var)\n        # Create slots for the first and second moments.\n        for v in var_list :\n            self._zeros_slot(v, ""m"", self._name)\n            self._zeros_slot(v, ""v"", self._name)\n            self._zeros_slot(v, ""vhat"", self._name)\n\n    def _prepare(self):\n        self._lr_t = ops.convert_to_tensor(self._lr)\n        self._base_lr_t = ops.convert_to_tensor(self._lr)\n        self._beta1_t = ops.convert_to_tensor(self._beta1)\n        self._beta2_t = ops.convert_to_tensor(self._beta2)\n        self._epsilon_t = ops.convert_to_tensor(self._epsilon)\n        self._gamma_t = ops.convert_to_tensor(self._gamma)\n\n    def _apply_dense(self, grad, var):\n        if StrictVersion(tf.__version__) >= StrictVersion(\'1.10.0\'):\n            graph = None if context.executing_eagerly() else ops.get_default_graph()\n        else:\n            graph = ops.get_default_graph()\n        beta1_power = math_ops.cast(self._get_non_slot_variable(""beta1_power"", graph=graph), var.dtype.base_dtype)\n        beta2_power = math_ops.cast(self._get_non_slot_variable(""beta2_power"", graph=graph), var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n        gamma_multi = math_ops.cast(self._get_non_slot_variable(""gamma_multi"", graph=graph), var.dtype.base_dtype)\n\n        step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n        final_lr = self._final_lr * lr_t / base_lr_t\n        lower_bound = final_lr * (1. - 1. / (gamma_multi + 1.))\n        upper_bound = final_lr * (1. + 1. / (gamma_multi))\n\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_scaled_g_values = grad * (1 - beta1_t)\n        m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n        v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values, use_locking=self._use_locking)\n\n        # amsgrad\n        vhat = self.get_slot(var, ""vhat"")\n        if self._amsbound :\n            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n            v_sqrt = math_ops.sqrt(vhat_t)\n        else:\n            vhat_t = state_ops.assign(vhat, vhat)\n            v_sqrt = math_ops.sqrt(v_t)\n\n        # Compute the bounds\n        step_size_bound = step_size / (v_sqrt + epsilon_t)\n        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n\n        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n\n    def _resource_apply_dense(self, grad, var):\n        if StrictVersion(tf.__version__) >= StrictVersion(\'1.10.0\'):\n            graph = None if context.executing_eagerly() else ops.get_default_graph()\n        else:\n            graph = ops.get_default_graph()\n        beta1_power = math_ops.cast(self._get_non_slot_variable(""beta1_power"", graph=graph), grad.dtype.base_dtype)\n        beta2_power = math_ops.cast(self._get_non_slot_variable(""beta2_power"", graph=graph), grad.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, grad.dtype.base_dtype)\n        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, grad.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, grad.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, grad.dtype.base_dtype)\n        gamma_multi = math_ops.cast(self._get_non_slot_variable(""gamma_multi"", graph=graph), var.dtype.base_dtype)\n\n        step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n        final_lr = self._final_lr * lr_t / base_lr_t\n        lower_bound = final_lr * (1. - 1. / (gamma_multi + 1.))\n        upper_bound = final_lr * (1. + 1. / (gamma_multi))\n\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_scaled_g_values = grad * (1 - beta1_t)\n        m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n        v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values, use_locking=self._use_locking)\n\n        # amsgrad\n        vhat = self.get_slot(var, ""vhat"")\n        if self._amsbound:\n            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n            v_sqrt = math_ops.sqrt(vhat_t)\n        else:\n            vhat_t = state_ops.assign(vhat, vhat)\n            v_sqrt = math_ops.sqrt(v_t)\n\n        # Compute the bounds\n        step_size_bound = step_size / (v_sqrt + epsilon_t)\n        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n\n        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n\n        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n\n    def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n        if StrictVersion(tf.__version__) >= StrictVersion(\'1.10.0\'):\n            graph = None if context.executing_eagerly() else ops.get_default_graph()\n        else:\n            graph = ops.get_default_graph()\n        beta1_power = math_ops.cast(self._get_non_slot_variable(""beta1_power"", graph=graph), var.dtype.base_dtype)\n        beta2_power = math_ops.cast(self._get_non_slot_variable(""beta2_power"", graph=graph), var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n        gamma_t = math_ops.cast(self._gamma_t, var.dtype.base_dtype)\n\n        step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n        final_lr = self._final_lr * lr_t / base_lr_t\n        lower_bound = final_lr * (1. - 1. / (gamma_t + 1.))\n        upper_bound = final_lr * (1. + 1. / (gamma_t))\n\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_scaled_g_values = grad * (1 - beta1_t)\n        m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\n        with ops.control_dependencies([m_t]):\n            m_t = scatter_add(m, indices, m_scaled_g_values)\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n        v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking)\n        with ops.control_dependencies([v_t]):\n            v_t = scatter_add(v, indices, v_scaled_g_values)\n\n        # amsgrad\n        vhat = self.get_slot(var, ""vhat"")\n        if self._amsbound:\n            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n            v_sqrt = math_ops.sqrt(vhat_t)\n        else:\n            vhat_t = state_ops.assign(vhat, vhat)\n            v_sqrt = math_ops.sqrt(v_t)\n\n        # Compute the bounds\n        step_size_bound = step_size / (v_sqrt + epsilon_t)\n        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n\n        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n\n        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n\n    def _apply_sparse(self, grad, var):\n        return self._apply_sparse_shared(\n            grad.values, var, grad.indices,\n            lambda x, i, v: state_ops.scatter_add(  # pylint: disable=g-long-lambda\n                x, i, v, use_locking=self._use_locking))\n\n    def _resource_scatter_add(self, x, i, v):\n        with ops.control_dependencies(\n                [resource_variable_ops.resource_scatter_add(x, i, v)]):\n            return x.value()\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        return self._apply_sparse_shared(\n            grad, var, indices, self._resource_scatter_add)\n\n    def _finish(self, update_ops, name_scope):\n        # Update the power accumulators.\n        with ops.control_dependencies(update_ops):\n            if StrictVersion(tf.__version__) >= StrictVersion(\'1.10.0\'):\n                graph = None if context.executing_eagerly() else ops.get_default_graph()\n            else:\n                graph = ops.get_default_graph()\n            beta1_power = self._get_non_slot_variable(""beta1_power"", graph=graph)\n            beta2_power = self._get_non_slot_variable(""beta2_power"", graph=graph)\n            gamma_multi = self._get_non_slot_variable(""gamma_multi"", graph=graph)\n            with ops.colocate_with(beta1_power):\n                update_beta1 = beta1_power.assign(\n                    beta1_power * self._beta1_t,\n                    use_locking=self._use_locking)\n                update_beta2 = beta2_power.assign(\n                    beta2_power * self._beta2_t,\n                    use_locking=self._use_locking)\n                update_gamma = gamma_multi.assign(\n                    gamma_multi + self._gamma_t,\n                    use_locking=self._use_locking)\n        return control_flow_ops.group(*update_ops + [update_beta1, update_beta2, update_gamma], name=name_scope)\n'"
optimizer/RAdam.py,4,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport tensorflow as tf\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops, state_ops, array_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.training import optimizer\n\n\n__all__ = [\'RAdamOptimizer\']\n\n\nclass RAdamOptimizer(optimizer.Optimizer):\n    """"""RAdam optimizer.\n    According to the paper\n    [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf).\n    """"""\n\n    def __init__(self,\n                 learning_rate=0.001,\n                 beta1=0.9,\n                 beta2=0.999,\n                 epsilon=1e-7,\n                 weight_decay=0.,\n                 amsgrad=False,\n                 total_steps=0,\n                 warmup_proportion=0.1,\n                 min_lr=0.,\n                 use_locking=False,\n                 name=""RAdam""):\n        r""""""Construct a new Adam optimizer.\n        Args:\n            learning_rate: A Tensor or a floating point value.    The learning rate.\n            beta1: A float value or a constant float tensor. The exponential decay\n                rate for the 1st moment estimates.\n            beta2: A float value or a constant float tensor. The exponential decay\n                rate for the 2nd moment estimates.\n            epsilon: A small constant for numerical stability. This epsilon is\n                ""epsilon hat"" in the Kingma and Ba paper (in the formula just before\n                Section 2.1), not the epsilon in Algorithm 1 of the paper.\n            weight_decay: A floating point value. Weight decay for each param.\n            amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n                the paper ""On the Convergence of Adam and beyond"".\n            total_steps: An integer. Total number of training steps.\n                Enable warmup by setting a positive value.\n            warmup_proportion: A floating point value. The proportion of increasing steps.\n            min_lr: A floating point value. Minimum learning rate after warmup.\n            name: Optional name for the operations created when applying gradients.\n                Defaults to ""Adam"".    @compatibility(eager) When eager execution is\n                enabled, `learning_rate`, `beta_1`, `beta_2`, and `epsilon` can each be\n                a callable that takes no arguments and returns the actual value to use.\n                This can be useful for changing these values across different\n                invocations of optimizer functions. @end_compatibility\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n                `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n                gradients by value, `decay` is included for backward compatibility to\n                allow time inverse decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        """"""\n        super(RAdamOptimizer, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._beta1 = beta1\n        self._beta2 = beta2\n        self._epsilon = epsilon\n        self._weight_decay = weight_decay\n        self._amsgrad = amsgrad\n        self._total_steps = float(total_steps)\n        self._warmup_proportion = warmup_proportion\n        self._min_lr = min_lr\n        self._initial_weight_decay = weight_decay\n        self._initial_total_steps = total_steps\n\n        self._lr_t = None\n        self._step_t = None\n        self._beta1_t = None\n        self._beta2_t = None\n        self._epsilon_t = None\n        self._weight_decay_t = None\n        self._total_steps_t = None\n        self._warmup_proportion_t = None\n        self._min_lr_t = None\n\n    def _get_beta_accumulators(self):\n        with ops.init_scope():\n            if context.executing_eagerly():\n                graph = None\n            else:\n                graph = ops.get_default_graph()\n            return (self._get_non_slot_variable(""step"", graph=graph),\n                    self._get_non_slot_variable(""beta1_power"", graph=graph),\n                    self._get_non_slot_variable(""beta2_power"", graph=graph))\n\n    def _create_slots(self, var_list):\n        first_var = min(var_list, key=lambda x: x.name)\n        self._create_non_slot_variable(initial_value=1.0, name=""step"", colocate_with=first_var)\n        self._create_non_slot_variable(initial_value=self._beta1, name=""beta1_power"", colocate_with=first_var)\n        self._create_non_slot_variable(initial_value=self._beta2, name=""beta2_power"", colocate_with=first_var)\n        for v in var_list:\n            self._zeros_slot(v, ""m"", self._name)\n            self._zeros_slot(v, ""v"", self._name)\n            if self._amsgrad:\n                self._zeros_slot(v, ""vhat"", self._name)\n\n    def _prepare(self):\n        lr = self._call_if_callable(self._lr)\n        beta1 = self._call_if_callable(self._beta1)\n        beta2 = self._call_if_callable(self._beta2)\n        epsilon = self._call_if_callable(self._epsilon)\n        weight_decay = self._call_if_callable(self._weight_decay)\n        total_steps = self._call_if_callable(self._total_steps)\n        warmup_proportion = self._call_if_callable(self._warmup_proportion)\n        min_lr = self._call_if_callable(self._min_lr)\n\n        self._lr_t = ops.convert_to_tensor(lr, name=""learning_rate"")\n        self._beta1_t = ops.convert_to_tensor(beta1, name=""beta1"")\n        self._beta2_t = ops.convert_to_tensor(beta2, name=""beta2"")\n        self._epsilon_t = ops.convert_to_tensor(epsilon, name=""epsilon"")\n        self._weight_decay_t = ops.convert_to_tensor(weight_decay, name=""weight_decay"")\n        self._total_steps_t = ops.convert_to_tensor(total_steps, name=""total_steps"")\n        self._warmup_proportion_t = ops.convert_to_tensor(warmup_proportion, name=""warmup_proportion"")\n        self._min_lr_t = ops.convert_to_tensor(min_lr, name=""min_lr"")\n\n    def _apply_dense(self, grad, var):\n        return self._resource_apply_dense(grad, var)\n\n    def _resource_apply_dense(self, grad, var):\n        step, beta1_power, beta2_power = self._get_beta_accumulators()\n        beta1_power = math_ops.cast(beta1_power, var.dtype.base_dtype)\n        beta2_power = math_ops.cast(beta2_power, var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n\n        if self._initial_total_steps > 0:\n            total_steps = math_ops.cast(self._total_steps_t, var.dtype.base_dtype)\n            warmup_proportion = math_ops.cast(self._warmup_proportion_t, var.dtype.base_dtype)\n            min_lr = math_ops.cast(self._min_lr_t, var.dtype.base_dtype)\n            warmup_steps = total_steps * warmup_proportion\n            decay_steps = math_ops.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(\n                step <= warmup_steps,\n                lr_t * (step / warmup_steps),\n                lr_t + decay_rate * math_ops.minimum(step - warmup_steps, decay_steps),\n            )\n\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n\n        sma_inf = 2.0 / (1.0 - beta2_t) - 1.0\n        sma_t = sma_inf - 2.0 * step * beta2_power / (1.0 - beta2_power)\n\n        m = self.get_slot(var, ""m"")\n        m_t = state_ops.assign(m, beta1_t * m + (1.0 - beta1_t) * grad, use_locking=self._use_locking)\n        m_corr_t = m_t / (1.0 - beta1_power)\n\n        v = self.get_slot(var, ""v"")\n        v_t = state_ops.assign(v, beta2_t * v + (1.0 - beta2_t) * math_ops.square(grad), use_locking=self._use_locking)\n        if self._amsgrad:\n            vhat = self.get_slot(var, \'vhat\')\n            vhat_t = state_ops.assign(vhat, math_ops.maximum(vhat, v_t), use_locking=self._use_locking)\n            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta2_power))\n        else:\n            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta2_power))\n\n        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                            (sma_t - 2.0) / (sma_inf - 2.0) *\n                            sma_inf / sma_t)\n\n        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n\n        if self._initial_weight_decay > 0.0:\n            var_t += math_ops.cast(self._weight_decay_t, var.dtype.base_dtype) * var\n\n        var_update = state_ops.assign_sub(var, lr_t * var_t, use_locking=self._use_locking)\n\n        updates = [var_update, m_t, v_t]\n        if self._amsgrad:\n            updates.append(vhat_t)\n        return control_flow_ops.group(*updates)\n\n    def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n        step, beta1_power, beta2_power = self._get_beta_accumulators()\n        beta1_power = math_ops.cast(beta1_power, var.dtype.base_dtype)\n        beta2_power = math_ops.cast(beta2_power, var.dtype.base_dtype)\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n\n        if self._initial_total_steps > 0:\n            total_steps = math_ops.cast(self._total_steps_t, var.dtype.base_dtype)\n            warmup_proportion = math_ops.cast(self._warmup_proportion_t, var.dtype.base_dtype)\n            min_lr = math_ops.cast(self._min_lr_t, var.dtype.base_dtype)\n            warmup_steps = total_steps * warmup_proportion\n            decay_steps = math_ops.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(\n                step <= warmup_steps,\n                lr_t * (step / warmup_steps),\n                lr_t + decay_rate * math_ops.minimum(step - warmup_steps, decay_steps),\n            )\n\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n\n        sma_inf = 2.0 / (1.0 - beta2_t) - 1.0\n        sma_t = sma_inf - 2.0 * step * beta2_power / (1.0 - beta2_power)\n\n        m = self.get_slot(var, ""m"")\n        m_scaled_g_values = grad * (1 - beta1_t)\n        m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\n        with ops.control_dependencies([m_t]):\n            m_t = scatter_add(m, indices, m_scaled_g_values)\n        m_corr_t = m_t / (1.0 - beta1_power)\n\n        v = self.get_slot(var, ""v"")\n        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n        v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking)\n        with ops.control_dependencies([v_t]):\n            v_t = scatter_add(v, indices, v_scaled_g_values)\n        if self._amsgrad:\n            vhat = self.get_slot(var, \'vhat\')\n            vhat_t = state_ops.assign(vhat, math_ops.maximum(vhat, v_t), use_locking=self._use_locking)\n            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta2_power))\n        else:\n            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta2_power))\n\n        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                            (sma_t - 2.0) / (sma_inf - 2.0) *\n                            sma_inf / sma_t)\n\n        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n\n        if self._initial_weight_decay > 0.0:\n            var_t += math_ops.cast(self._weight_decay_t, var.dtype.base_dtype) * var\n\n        var_t = lr_t * var_t\n        var_update = state_ops.scatter_sub(\n                    var,\n                    indices,\n                    array_ops.gather(var_t, indices),\n                    use_locking=self._use_locking)\n\n        updates = [var_update, m_t, v_t]\n        if self._amsgrad:\n            updates.append(vhat_t)\n        return control_flow_ops.group(*updates)\n\n    def _apply_sparse(self, grad, var):\n        return self._apply_sparse_shared(\n            grad.values,\n            var,\n            grad.indices,\n            lambda x, i, v: state_ops.scatter_add(x, i, v, use_locking=self._use_locking))\n\n    def _resource_scatter_add(self, x, i, v):\n        with ops.control_dependencies([resource_variable_ops.resource_scatter_add(x.handle, i, v)]):\n            return x.value()\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        return self._apply_sparse_shared(grad, var, indices, self._resource_scatter_add)\n\n    def _finish(self, update_ops, name_scope):\n        with ops.control_dependencies(update_ops):\n            step, beta1_power, beta2_power = self._get_beta_accumulators()\n            with ops.colocate_with(beta1_power):\n                update_step = step.assign(step + 1.0, use_locking=self._use_locking)\n                update_beta1 = beta1_power.assign(beta1_power * self._beta1_t, use_locking=self._use_locking)\n                update_beta2 = beta2_power.assign(beta2_power * self._beta2_t, use_locking=self._use_locking)\n        return control_flow_ops.group(*update_ops + [update_step, update_beta1, update_beta2], name=name_scope)'"
optimizer/__init__.py,0,b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>'
tools/gif_frames.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\n\nimport cv2\nimport numpy as np\nfrom PIL import ImageSequence\n\n\ndef split_frames(image_obj, need_frame=None):\n    if not need_frame:\n        need_frame = [0]\n    image_seq = ImageSequence.all_frames(image_obj)\n    image_arr_last = [np.asarray(image_seq[-1])] if -1 in need_frame and len(need_frame) > 1 else []\n    image_arr = [np.asarray(item) for i, item in enumerate(image_seq) if (i in need_frame or need_frame == [-1])]\n    image_arr += image_arr_last\n    return image_arr\n\n\ndef concat_arr(img_arr):\n    if len(img_arr) < 1:\n        return img_arr\n    all_slice = img_arr[0]\n    for im_slice in img_arr[1:]:\n        all_slice = np.concatenate((all_slice, im_slice), axis=1)\n    return all_slice\n\n\ndef numpy_to_bytes(numpy_arr):\n    cv_img = cv2.imencode(\'.png\', numpy_arr)[1]\n    img_bytes = bytes(bytearray(cv_img))\n    return img_bytes\n\n\ndef concat_frames(image_obj, need_frame=None):\n    if not need_frame:\n        need_frame = [0]\n    img_arr = split_frames(image_obj, need_frame)\n    img_arr = concat_arr(img_arr)\n    return img_arr\n\n\ndef blend_arr(img_arr):\n    if len(img_arr) < 1:\n        return img_arr\n    all_slice = img_arr[0]\n    for im_slice in img_arr[1:]:\n        all_slice = cv2.addWeighted(all_slice, 0.5, im_slice, 0.5, 0)\n    all_slice = cv2.equalizeHist(all_slice)\n    return all_slice\n\n\ndef blend_frame(image_obj, need_frame=None):\n    if not need_frame:\n        need_frame = [-1]\n    img_arr = split_frames(image_obj, need_frame)\n    img_arr = blend_arr(img_arr)\n    return img_arr\n\n\nif __name__ == ""__main__"":\n    pass'"
tools/package.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nfrom PyInstaller.__main__ import run\n\n\ndef package(prefix):\n    """"""\xe5\x9f\xba\xe4\xba\x8ePyInstaller\xe6\x89\x93\xe5\x8c\x85\xe7\xbc\x96\xe8\xaf\x91\xe4\xb8\xba\xe5\x8d\x95\xe5\x8f\xaf\xe6\x89\xa7\xe8\xa1\x8c\xe6\x96\x87\xe4\xbb\xb6""""""\n    opts = [\'{}app.spec\'.format(prefix), \'--distpath={}dist\'.format(prefix), \'--workpath={}build\'.format(prefix)]\n    run(opts)\n\n\nif __name__ == \'__main__\':\n    try:\n        package(""../"")\n    except FileNotFoundError:\n        package(""/"")\n'"
utils/__init__.py,0,b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\n\n# from . import sparse\n# from . import data'
utils/data.py,15,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport os\nimport hashlib\nimport utils\nimport random\nimport utils.sparse\nimport tensorflow as tf\nimport numpy as np\nfrom constants import RunMode, ModelField, DatasetType, LossFunction\nfrom config import ModelConfig, EXCEPT_FORMAT_MAP\nfrom encoder import Encoder\nfrom exception import exception\n\n\nclass DataIterator:\n    """"""\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xbf\xad\xe4\xbb\xa3\xe7\xb1\xbb""""""\n\n    def __init__(self, model_conf: ModelConfig, mode: RunMode, ran_captcha=None):\n        """"""\n        :param model_conf: \xe5\xb7\xa5\xe7\xa8\x8b\xe9\x85\x8d\xe7\xbd\xae\n        :param mode: \xe8\xbf\x90\xe8\xa1\x8c\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x88\xe5\x8c\xba\xe5\x88\x86\xef\xbc\x9a\xe8\xae\xad\xe7\xbb\x83/\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x89\n        """"""\n        self.model_conf = model_conf\n        self.mode = mode\n        self.path_map = {\n            RunMode.Trains: self.model_conf.trains_path[DatasetType.TFRecords],\n            RunMode.Validation: self.model_conf.validation_path[DatasetType.TFRecords]\n        }\n        self.batch_map = {\n            RunMode.Trains: self.model_conf.batch_size,\n            RunMode.Validation: self.model_conf.validation_batch_size\n        }\n        self.data_dir = self.path_map[mode]\n        self.next_element = None\n        self.image_path = []\n        self.label_list = []\n        self._label_list = []\n        self._size = 0\n        self.encoder = Encoder(self.model_conf, self.mode)\n        self.ran_captcha = ran_captcha\n\n    @staticmethod\n    def parse_example(serial_example):\n\n        features = tf.io.parse_single_example(\n            serial_example,\n            features={\n                \'label\': tf.io.FixedLenFeature([], tf.string),\n                \'input\': tf.io.FixedLenFeature([], tf.string),\n            }\n        )\n        _input = tf.cast(features[\'input\'], tf.string)\n        _label = tf.cast(features[\'label\'], tf.string)\n\n        return _input, _label\n\n    @staticmethod\n    def total_sample(file_name):\n        sample_nums = 0\n        for _ in tf.python_io.tf_record_iterator(file_name):\n            sample_nums += 1\n        return sample_nums\n\n    def read_sample_from_tfrecords(self, path):\n        """"""\n        \xe4\xbb\x8eTFRecords\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96\xe6\xa0\xb7\xe6\x9c\xac\n        :param path: TFRecords\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n        :return:\n        """"""\n        if isinstance(path, list):\n            for p in path:\n                self._size += self.total_sample(p)\n        else:\n            self._size = self.total_sample(path)\n\n        min_after_dequeue = 1000\n        batch = self.batch_map[self.mode]\n        if self.model_conf.da_random_captcha[\'Enable\']:\n            batch = random.randint(int(batch / 3 * 2), batch)\n\n        dataset_train = tf.data.TFRecordDataset(\n            filenames=path,\n            num_parallel_reads=20\n        ).map(self.parse_example)\n        dataset_train = dataset_train.shuffle(\n            min_after_dequeue,\n            reshuffle_each_iteration=True\n        ).batch(batch, drop_remainder=True).repeat()\n        iterator = tf.compat.v1.data.make_one_shot_iterator(dataset_train)\n        self.next_element = iterator.get_next()\n\n    @property\n    def size(self):\n        """"""\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0""""""\n        return self._size\n\n    @property\n    def labels(self):\n        """"""\xe6\xa0\x87\xe7\xad\xbe""""""\n        return self.label_list\n\n    @staticmethod\n    def to_sparse(input_batch, label_batch):\n        """"""\xe5\xaf\x86\xe9\x9b\x86\xe8\xbe\x93\xe5\x85\xa5\xe8\xbd\xac\xe7\xa8\x80\xe7\x96\x8f""""""\n        batch_inputs = input_batch\n        batch_labels = utils.sparse.sparse_tuple_from_sequences(label_batch)\n        return batch_inputs, batch_labels\n\n    def generate_captcha(self, num) -> (list, list):\n        _images = []\n        _labels = []\n        for i in range(num):\n            try:\n                image, labels, font_type = self.ran_captcha.create()\n                _images.append(image)\n                _labels.append(\'\'.join(labels).encode())\n            except Exception as e:\n                print(e)\n                pass\n        return _images, _labels\n\n    def generate_batch_by_tfrecords(self, session):\n        """"""\xe6\xa0\xb9\xe6\x8d\xaeTFRecords\xe7\x94\x9f\xe6\x88\x90\xe5\xbd\x93\xe5\x89\x8d\xe6\x89\xb9\xe6\xac\xa1\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\xba\xe5\xbd\x93\xe5\x89\x8dTensorFlow\xe4\xbc\x9a\xe8\xaf\x9d\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\xba\xe7\xa8\x80\xe7\x96\x8f\xe5\x9e\x8bX\xe5\x92\x8cY""""""\n        # print(session.graph)\n        batch = self.batch_map[self.mode]\n\n        _input, _label = session.run(self.next_element)\n        if self.model_conf.da_random_captcha[\'Enable\']:\n            remain_batch = batch - len(_label)\n            extra_input, extra_label = self.generate_captcha(remain_batch)\n            _input = np.concatenate((_input, extra_input), axis=0)\n            _label = np.concatenate((_label, extra_label), axis=0)\n\n        input_batch = []\n        label_batch = []\n        for index, (i1, i2) in enumerate(zip(_input, _label)):\n            try:\n                label_array = self.encoder.text(i2)\n                if self.model_conf.model_field == ModelField.Image:\n                    input_array = self.encoder.image(i1)\n                else:\n                    input_array = self.encoder.text(i1)\n\n                if input_array is None:\n                    tf.logging.warn(\n                        ""{}, \\nCannot identify image file labeled: {}, ignored."".format(input_array, label_array))\n                    continue\n\n                if isinstance(input_array, str):\n                    tf.logging.warn(""{}, \\nInput errors labeled: {}, ignored."".format(input_array, label_array))\n                    continue\n                if isinstance(label_array, dict):\n                    tf.logging.warn(""The sample label {} contains invalid charset: {}."".format(\n                        label_array[\'label\'], label_array[\'char\']\n                    ))\n                    continue\n\n                if input_array.shape[-1] != self.model_conf.image_channel:\n                    pass\n                    # tf.logging.warn(""{}, \\nInput shape: {}, ignored."".format(\n                    #     self.model_conf.image_channel, input_array.shape[-1])\n                    # )\n                    continue\n\n                label_len_correct = len(label_array) != self.model_conf.max_label_num\n                using_cross_entropy = self.model_conf.loss_func == LossFunction.CrossEntropy\n                if label_len_correct and using_cross_entropy and not self.model_conf.auto_padding:\n                    tf.logging.warn(""The number of labels must be fixed when using cross entropy, label: {}, ""\n                                    ""the number of tags is incorrect, ignored."".format(i2))\n                    continue\n\n                if len(label_array) > self.model_conf.max_label_num and using_cross_entropy:\n                    tf.logging.warn(\n                        ""The number of label[{}] exceeds the maximum number of labels, ignored.{}"".format(i2,\n                                                                                                          label_array))\n                    continue\n\n                input_batch.append(input_array)\n                label_batch.append(label_array)\n            except OSError:\n                random_suffix = hashlib.md5(i1).hexdigest()\n                file_format = EXCEPT_FORMAT_MAP[self.model_conf.model_field]\n                with open(file=""oserror_{}.{}"".format(random_suffix, file_format), mode=""wb"") as f:\n                    f.write(i1)\n                continue\n\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe5\x9b\xbe\xe7\x89\x87\xe5\xb0\xba\xe5\xaf\xb8\xe4\xb8\x8d\xe5\x9b\xba\xe5\xae\x9a\xe5\x88\x99padding\xe5\xbd\x93\xe5\x89\x8d\xe6\x89\xb9\xe6\xac\xa1\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\xae\xbd\xe5\xba\xa6\xe4\xbd\x9c\xe4\xb8\xba\xe5\xba\x8f\xe5\x88\x97\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n        if self.model_conf.model_field == ModelField.Image and self.model_conf.resize[0] == -1:\n            input_batch = tf.keras.preprocessing.sequence.pad_sequences(\n                sequences=input_batch,\n                maxlen=None,\n                dtype=\'float32\',\n                padding=\'post\',\n                truncating=\'post\',\n                value=0\n            )\n\n        self.label_list = label_batch\n        return self.to_sparse(input_batch, self.label_list)\n'"
utils/sparse.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n# Author: kerlomz <kerlomz@gmail.com>\nimport numpy as np\n\n\ndef sparse_tuple_from_sequences(sequences, dtype=np.int32):\n    """"""\xe5\xaf\x86\xe9\x9b\x86\xe5\xba\x8f\xe5\x88\x97\xe8\xbd\xac\xe7\xa8\x80\xe7\x96\x8f\xe5\xba\x8f\xe5\x88\x97""""""\n    indices = []\n    values = []\n    for n, seq in enumerate(sequences):\n        indices.extend(zip([n] * len(seq), range(0, len(seq), 1)))\n        values.extend(seq)\n\n    indices = np.asarray(indices, dtype=np.int64)\n    try:\n        values = np.asarray(values, dtype=dtype)\n    except Exception as e:\n        print(e, values)\n    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\n    return indices, values, shape\n'"
