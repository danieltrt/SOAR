file_path,api_count,code
config.py,0,"b'from models import *\n\n\nmodel_zoo = [\'DCGAN\', \'LSGAN\', \'WGAN\', \'WGAN-GP\', \'EBGAN\', \'BEGAN\', \'DRAGAN\', \'CoulombGAN\']\n\ndef get_model(mtype, name, training):\n    model = None\n    if mtype == \'DCGAN\':\n        model = dcgan.DCGAN\n    elif mtype == \'LSGAN\':\n        model = lsgan.LSGAN\n    elif mtype == \'WGAN\':\n        model = wgan.WGAN\n    elif mtype == \'WGAN-GP\':\n        model = wgan_gp.WGAN_GP\n    elif mtype == \'EBGAN\':\n        model = ebgan.EBGAN\n    elif mtype == \'BEGAN\':\n        model = began.BEGAN\n    elif mtype == \'DRAGAN\':\n        model = dragan.DRAGAN\n    elif mtype == \'COULOMBGAN\':\n        model = coulombgan.CoulombGAN\n    else:\n        assert False, mtype + \' is not in the model zoo\'\n\n    assert model, mtype + \' is work in progress\'\n\n    return model(name=name, training=training)\n\n\ndef get_dataset(dataset_name):\n    celebA_64 = \'./data/celebA_tfrecords/*.tfrecord\'\n    celebA_128 = \'./data/celebA_128_tfrecords/*.tfrecord\'\n    lsun_bedroom_128 = \'./data/lsun/bedroom_128_tfrecords/*.tfrecord\'\n\n    if dataset_name == \'celeba\':\n        path = celebA_128\n        n_examples = 202599\n    elif dataset_name == \'lsun\':\n        path = lsun_bedroom_128\n        n_examples = 3033042\n    else:\n        raise ValueError(\'{} is does not supported. dataset must be celeba or lsun.\'.format(dataset_name))\n\n    return path, n_examples\n\n\ndef pprint_args(FLAGS):\n    print(""\\nParameters:"")\n    for attr, value in sorted(vars(FLAGS).items()):\n        print(""{}={}"".format(attr.upper(), value))\n    print("""")\n\n'"
convert.py,9,"b'# coding: utf-8\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport scipy.misc\r\nimport os\r\nimport glob\r\n\r\n\r\ndef _bytes_features(value):\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\r\n\r\n\r\ndef _int64_features(value):\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\r\n\r\n\r\n# preproc for celebA\r\ndef center_crop(im, output_size):\r\n    output_height, output_width = output_size\r\n    h, w = im.shape[:2]\r\n    if h < output_height and w < output_width:\r\n        raise ValueError(""image is small"")\r\n\r\n    offset_h = int((h - output_height) / 2)\r\n    offset_w = int((w - output_width) / 2)\r\n    return im[offset_h:offset_h+output_height, offset_w:offset_w+output_width, :]\r\n\r\n\r\ndef convert(source_dir, target_dir, crop_size, out_size, exts=[\'\'], num_shards=128, tfrecords_prefix=\'\'):\r\n    if not tf.gfile.Exists(source_dir):\r\n        print(\'source_dir does not exists\')\r\n        return\r\n    \r\n    if tfrecords_prefix and not tfrecords_prefix.endswith(\'-\'):\r\n        tfrecords_prefix += \'-\'\r\n\r\n    if tf.gfile.Exists(target_dir):\r\n        print(""{} is Already exists"".format(target_dir))\r\n        return\r\n    else:\r\n        tf.gfile.MakeDirs(target_dir)\r\n\r\n    # get meta-data\r\n    path_list = []\r\n    for ext in exts:\r\n        pattern = \'*.\' + ext if ext != \'\' else \'*\'\r\n        path = os.path.join(source_dir, pattern)\r\n        path_list.extend(glob.glob(path))\r\n\r\n    # shuffle path_list\r\n    np.random.shuffle(path_list)\r\n    num_files = len(path_list)\r\n    num_per_shard = num_files // num_shards # Last shard will have more files\r\n\r\n    print(\'# of files: {}\'.format(num_files))\r\n    print(\'# of shards: {}\'.format(num_shards))\r\n    print(\'# files per shards: {}\'.format(num_per_shard))\r\n\r\n    # convert to tfrecords\r\n    shard_idx = 0\r\n    writer = None\r\n    for i, path in enumerate(path_list):\r\n        if i % num_per_shard == 0 and shard_idx < num_shards:\r\n            shard_idx += 1\r\n            tfrecord_fn = \'{}{:0>4d}-of-{:0>4d}.tfrecord\'.format(tfrecords_prefix, shard_idx, num_shards)\r\n            tfrecord_path = os.path.join(target_dir, tfrecord_fn)\r\n            print(""Writing {} ..."".format(tfrecord_path))\r\n            if shard_idx > 1:\r\n                writer.close()\r\n            writer = tf.python_io.TFRecordWriter(tfrecord_path)\r\n\r\n        # mode=\'RGB\' read even grayscale image as RGB shape\r\n        im = scipy.misc.imread(path, mode=\'RGB\')\r\n        # preproc\r\n        try:\r\n            im = center_crop(im, crop_size)\r\n        except Exception as e:\r\n            # print(""im_path: {}"".format(path))\r\n            # print(""im_shape: {}"".format(im.shape))\r\n            print(""[Exception] {}"".format(e))\r\n            continue\r\n\r\n        im = scipy.misc.imresize(im, out_size)\r\n        example = tf.train.Example(features=tf.train.Features(feature={\r\n            # ""shape"": _int64_features(im.shape),\r\n            ""image"": _bytes_features([im.tostring()])\r\n        }))\r\n        writer.write(example.SerializeToString())\r\n\r\n    writer.close()\r\n\r\n\r\n\'\'\' Below function burrowed from https://github.com/fyu/lsun.\r\nProcess: LMDB => images => tfrecords\r\nIt is more efficient method to skip intermediate images, but that is a little messy job.\r\nThe method through images is inefficient but convenient.\r\n\'\'\'\r\ndef export_images(db_path, out_dir, flat=False, limit=-1):\r\n    print(\'Exporting {} to {}\'.format(db_path, out_dir))\r\n    env = lmdb.open(db_path, map_size=1099511627776, max_readers=100, readonly=True)\r\n    num_images = env.stat()[\'entries\']\r\n    count = 0\r\n    with env.begin(write=False) as txn:\r\n        cursor = txn.cursor()\r\n        for key, val in cursor:\r\n            if not flat:\r\n                image_out_dir = join(out_dir, \'/\'.join(key[:6]))\r\n            else:\r\n                image_out_dir = out_dir\r\n            if not exists(image_out_dir):\r\n                os.makedirs(image_out_dir)\r\n            image_out_path = join(image_out_dir, key + \'.webp\')\r\n            with open(image_out_path, \'w\') as fp:\r\n                fp.write(val)\r\n            count += 1\r\n            if count == limit:\r\n                break\r\n            if count % 10000 == 0:\r\n                print(\'{}/{} ...\'.format(count, num_images))\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    # CelebA\r\n    convert(\'./data/celebA\', \'./data/celebA_128_tfrecords\', crop_size=[128, 128], out_size=[128, 128], \r\n        exts=[\'jpg\'], num_shards=128, tfrecords_prefix=\'celebA\')\r\n\r\n    # LSUN\r\n    # export_images(\'./tf.gans-comparison/data/lsun/bedroom_val_lmdb/\', \r\n    #     \'./tf.gans-comparison/data/lsun/bedroom_val_images/\', flat=True)\r\n    # convert(\'./data/lsun/bedroom_train_images\', \'./data/lsun/bedroom_128_tfrecords\', crop_size=[128, 128], \r\n    #     out_size=[128, 128], exts=[\'webp\'], num_shards=128, tfrecords_prefix=\'lsun_bedroom\')\r\n'"
download.py,0,"b'from __future__ import print_function\r\nimport os\r\nimport sys\r\nimport gzip\r\nimport json\r\nimport shutil\r\nimport zipfile\r\nimport argparse\r\nimport requests\r\nimport subprocess\r\nfrom tqdm import tqdm\r\nfrom six.moves import urllib\r\n\r\n""""""\r\nBurrowed from https://github.com/carpedm20/DCGAN-tensorflow/blob/master/download.py\r\n""""""\r\n\r\n""""""\r\nModification of https://github.com/stanfordnlp/treelstm/blob/master/scripts/download.py\r\nDownloads the following:\r\n- Celeb-A dataset\r\n- LSUN dataset\r\n- MNIST dataset\r\n""""""\r\n\r\n\r\nparser = argparse.ArgumentParser(description=\'Download dataset for DCGAN.\')\r\nparser.add_argument(\'datasets\', metavar=\'N\', type=str, nargs=\'+\', choices=[\'celebA\', \'lsun\', \'mnist\'],\r\n           help=\'name of dataset to download [celebA, lsun, mnist]\')\r\n\r\ndef download(url, dirpath):\r\n  filename = url.split(\'/\')[-1]\r\n  filepath = os.path.join(dirpath, filename)\r\n  u = urllib.request.urlopen(url)\r\n  f = open(filepath, \'wb\')\r\n  filesize = int(u.headers[""Content-Length""])\r\n  print(""Downloading: %s Bytes: %s"" % (filename, filesize))\r\n\r\n  downloaded = 0\r\n  block_sz = 8192\r\n  status_width = 70\r\n  while True:\r\n    buf = u.read(block_sz)\r\n    if not buf:\r\n      print(\'\')\r\n      break\r\n    else:\r\n      print(\'\', end=\'\\r\')\r\n    downloaded += len(buf)\r\n    f.write(buf)\r\n    status = ((""[%-"" + str(status_width + 1) + ""s] %3.2f%%"") %\r\n      (\'=\' * int(float(downloaded) / filesize * status_width) + \'>\', downloaded * 100. / filesize))\r\n    print(status, end=\'\')\r\n    sys.stdout.flush()\r\n  f.close()\r\n  return filepath\r\n\r\ndef download_file_from_google_drive(id, destination):\r\n  URL = ""https://docs.google.com/uc?export=download""\r\n  session = requests.Session()\r\n\r\n  response = session.get(URL, params={ \'id\': id }, stream=True)\r\n  token = get_confirm_token(response)\r\n\r\n  if token:\r\n    params = { \'id\' : id, \'confirm\' : token }\r\n    response = session.get(URL, params=params, stream=True)\r\n\r\n  save_response_content(response, destination)\r\n\r\ndef get_confirm_token(response):\r\n  for key, value in response.cookies.items():\r\n    if key.startswith(\'download_warning\'):\r\n      return value\r\n  return None\r\n\r\ndef save_response_content(response, destination, chunk_size=32*1024):\r\n  total_size = int(response.headers.get(\'content-length\', 0))\r\n  with open(destination, ""wb"") as f:\r\n    for chunk in tqdm(response.iter_content(chunk_size), total=total_size,\r\n              unit=\'B\', unit_scale=True, desc=destination):\r\n      if chunk: # filter out keep-alive new chunks\r\n        f.write(chunk)\r\n\r\ndef unzip(filepath):\r\n  print(""Extracting: "" + filepath)\r\n  dirpath = os.path.dirname(filepath)\r\n  with zipfile.ZipFile(filepath) as zf:\r\n    zf.extractall(dirpath)\r\n  os.remove(filepath)\r\n\r\ndef download_celeb_a(dirpath):\r\n  data_dir = \'celebA\'\r\n  if os.path.exists(os.path.join(dirpath, data_dir)):\r\n    print(\'Found Celeb-A - skip\')\r\n    return\r\n\r\n  filename, drive_id  = ""img_align_celeba.zip"", ""0B7EVK8r0v71pZjFTYXZWM3FlRnM""\r\n  save_path = os.path.join(dirpath, filename)\r\n\r\n  if os.path.exists(save_path):\r\n    print(\'[*] {} already exists\'.format(save_path))\r\n  else:\r\n    download_file_from_google_drive(drive_id, save_path)\r\n\r\n  zip_dir = \'\'\r\n  with zipfile.ZipFile(save_path) as zf:\r\n    zip_dir = zf.namelist()[0]\r\n    zf.extractall(dirpath)\r\n  os.remove(save_path)\r\n  os.rename(os.path.join(dirpath, zip_dir), os.path.join(dirpath, data_dir))\r\n\r\ndef _list_categories(tag):\r\n  url = \'http://lsun.cs.princeton.edu/htbin/list.cgi?tag=\' + tag\r\n  f = urllib.request.urlopen(url)\r\n  return json.loads(f.read())\r\n\r\ndef _download_lsun(out_dir, category, set_name, tag):\r\n  url = \'http://lsun.cs.princeton.edu/htbin/download.cgi?tag={tag}\' \\\r\n      \'&category={category}&set={set_name}\'.format(**locals())\r\n  print(url)\r\n  if set_name == \'test\':\r\n    out_name = \'test_lmdb.zip\'\r\n  else:\r\n    out_name = \'{category}_{set_name}_lmdb.zip\'.format(**locals())\r\n  out_path = os.path.join(out_dir, out_name)\r\n  cmd = [\'curl\', url, \'-o\', out_path]\r\n  print(\'Downloading\', category, set_name, \'set\')\r\n  subprocess.call(cmd)\r\n\r\ndef download_lsun(dirpath):\r\n  data_dir = os.path.join(dirpath, \'lsun\')\r\n  if os.path.exists(data_dir):\r\n    print(\'Found LSUN - skip\')\r\n    return\r\n  else:\r\n    os.mkdir(data_dir)\r\n\r\n  tag = \'latest\'\r\n  #categories = _list_categories(tag)\r\n  categories = [\'bedroom\']\r\n\r\n  for category in categories:\r\n    _download_lsun(data_dir, category, \'train\', tag)\r\n    _download_lsun(data_dir, category, \'val\', tag)\r\n  _download_lsun(data_dir, \'\', \'test\', tag)\r\n\r\ndef download_mnist(dirpath):\r\n  data_dir = os.path.join(dirpath, \'mnist\')\r\n  if os.path.exists(data_dir):\r\n    print(\'Found MNIST - skip\')\r\n    return\r\n  else:\r\n    os.mkdir(data_dir)\r\n  url_base = \'http://yann.lecun.com/exdb/mnist/\'\r\n  file_names = [\'train-images-idx3-ubyte.gz\',\r\n                \'train-labels-idx1-ubyte.gz\',\r\n                \'t10k-images-idx3-ubyte.gz\',\r\n                \'t10k-labels-idx1-ubyte.gz\']\r\n  for file_name in file_names:\r\n    url = (url_base+file_name).format(**locals())\r\n    print(url)\r\n    out_path = os.path.join(data_dir,file_name)\r\n    cmd = [\'curl\', url, \'-o\', out_path]\r\n    print(\'Downloading \', file_name)\r\n    subprocess.call(cmd)\r\n    cmd = [\'gzip\', \'-d\', out_path]\r\n    print(\'Decompressing \', file_name)\r\n    subprocess.call(cmd)\r\n\r\ndef prepare_data_dir(path = \'./data\'):\r\n  if not os.path.exists(path):\r\n    os.mkdir(path)\r\n\r\nif __name__ == \'__main__\':\r\n  args = parser.parse_args()\r\n  prepare_data_dir()\r\n\r\n  if any(name in args.datasets for name in [\'CelebA\', \'celebA\', \'celeba\']):\r\n    download_celeb_a(\'./data\')\r\n  if \'lsun\' in args.datasets:\r\n    download_lsun(\'./data\')\r\n  if \'mnist\' in args.datasets:\r\n    download_mnist(\'./data\')\r\n\r\n'"
eval.py,8,"b'#coding: utf-8\nimport tensorflow as tf\nimport numpy as np\nimport utils\nimport config\nimport os, glob\nimport scipy.misc\nfrom argparse import ArgumentParser\nslim = tf.contrib.slim\n\n\ndef build_parser():\n    parser = ArgumentParser()\n    models_str = \' / \'.join(config.model_zoo)\n    parser.add_argument(\'--model\', help=models_str, required=True) \n    parser.add_argument(\'--name\', help=\'default: name=model\')\n    parser.add_argument(\'--dataset\', \'-D\', help=\'CelebA / LSUN\', required=True)\n    parser.add_argument(\'--sample_size\', \'-N\', help=\'# of samples. It should be a square number. (default: 16)\',\n        default=16, type=int)\n\n    return parser\n\n\ndef sample_z(shape):\n    return np.random.normal(size=shape)\n\n\ndef get_all_checkpoints(ckpt_dir, force=False):\n    \'\'\'\n    When the learning is interrupted and resumed, all checkpoints can not be fetched with get_checkpoint_state \n    (The checkpoint state is rewritten from the point of resume). \n    This function fetch all checkpoints forcely when arguments force=True.\n    \'\'\'\n\n    if force:\n        ckpts = os.listdir(ckpt_dir) # get all fns\n        ckpts = map(lambda p: os.path.splitext(p)[0], ckpts) # del ext\n        ckpts = set(ckpts) # unique\n        ckpts = filter(lambda x: x.split(\'-\')[-1].isdigit(), ckpts) # filter non-ckpt\n        ckpts = sorted(ckpts, key=lambda x: int(x.split(\'-\')[-1])) # sort\n        ckpts = map(lambda x: os.path.join(ckpt_dir, x), ckpts) # fn => path\n    else:\n        ckpts = tf.train.get_checkpoint_state(ckpt_dir).all_model_checkpoint_paths\n    \n    return ckpts\n\n\ndef eval(model, name, dataset, sample_shape=[4,4], load_all_ckpt=True):\n    if name == None:\n        name = model.name\n    dir_name = os.path.join(\'eval\', dataset, name)\n    if tf.gfile.Exists(dir_name):\n        tf.gfile.DeleteRecursively(dir_name)\n    tf.gfile.MakeDirs(dir_name)\n\n    restorer = tf.train.Saver(slim.get_model_variables())\n\n    config = tf.ConfigProto()\n    best_gpu = utils.get_best_gpu()\n    config.gpu_options.visible_device_list = str(best_gpu)\n    with tf.Session(config=config) as sess:\n        ckpt_path = os.path.join(\'checkpoints\', dataset, name)\n        ckpts = get_all_checkpoints(ckpt_path, force=load_all_ckpt)\n        size = sample_shape[0] * sample_shape[1]\n\n        z_ = sample_z([size, model.z_dim])\n\n        for v in ckpts:\n            print(""Evaluating {} ..."".format(v))\n            restorer.restore(sess, v)\n            global_step = int(v.split(\'/\')[-1].split(\'-\')[-1])\n            \n            fake_samples = sess.run(model.fake_sample, {model.z: z_})\n\n            # inverse transform: [-1, 1] => [0, 1]\n            fake_samples = (fake_samples + 1.) / 2.\n            merged_samples = utils.merge(fake_samples, size=sample_shape)\n            fn = ""{:0>6d}.png"".format(global_step)\n            scipy.misc.imsave(os.path.join(dir_name, fn), merged_samples)\n\n\n\'\'\'\nYou can create a gif movie through imagemagick on the commandline:\n$ convert -delay 20 eval/* movie.gif\n\'\'\'\n# def to_gif(dir_name=\'eval\'):\n#     images = []\n#     for path in glob.glob(os.path.join(dir_name, \'*.png\')):\n#         im = scipy.misc.imread(path)\n#         images.append(im)\n\n#     # make_gif(images, dir_name + \'/movie.gif\', duration=10, true_image=True)\n#     imageio.mimsave(\'movie.gif\', images, duration=0.2)\n\n\nif __name__ == ""__main__"":\n    parser = build_parser()\n    FLAGS = parser.parse_args()\n    FLAGS.model = FLAGS.model.upper()\n    FLAGS.dataset = FLAGS.dataset.lower()\n    if FLAGS.name is None:\n        FLAGS.name = FLAGS.model.lower()\n    config.pprint_args(FLAGS)\n\n    N = FLAGS.sample_size**0.5\n    assert N == int(N), \'sample size should be a square number\'\n\n    # training=False => build generator only\n    model = config.get_model(FLAGS.model, FLAGS.name, training=False)\n    eval(model, dataset=FLAGS.dataset, name=FLAGS.name, sample_shape=[int(N),int(N)], load_all_ckpt=True)\n'"
inputpipe.py,16,"b'# coding: utf-8\nimport tensorflow as tf\n\n\ndef read_parse_preproc(filename_queue):\n    \'\'\' read, parse, and preproc single example. \'\'\'\n    with tf.variable_scope(\'read_parse_preproc\'):\n        reader = tf.TFRecordReader()\n        key, records = reader.read(filename_queue)\n        \n        # parse records\n        features = tf.parse_single_example(\n            records,\n            features={\n                ""image"": tf.FixedLenFeature([], tf.string)\n            }\n        )\n\n        image = tf.decode_raw(features[""image""], tf.uint8)\n        image = tf.reshape(image, [128, 128, 3]) # The image_shape must be explicitly specified\n        image = tf.image.resize_images(image, [64, 64])\n        image = tf.cast(image, tf.float32)\n        image = image / 127.5 - 1.0 # preproc - normalize\n        \n        return [image]\n\n\n# https://www.tensorflow.org/programmers_guide/reading_data\ndef get_batch(tfrecords_list, batch_size, shuffle=False, num_threads=1, min_after_dequeue=None, num_epochs=None):\n    name = ""batch"" if not shuffle else ""shuffle_batch""\n    with tf.variable_scope(name):\n        filename_queue = tf.train.string_input_producer(tfrecords_list, shuffle=shuffle, num_epochs=num_epochs)\n        data_point = read_parse_preproc(filename_queue)\n        \n        if min_after_dequeue is None:\n            min_after_dequeue = batch_size * 10\n        capacity = min_after_dequeue + 3*batch_size\n        if shuffle:\n            batch = tf.train.shuffle_batch(data_point, batch_size=batch_size, capacity=capacity, \n                min_after_dequeue=min_after_dequeue, num_threads=num_threads, allow_smaller_final_batch=True)\n        else:\n            batch = tf.train.batch(data_point, batch_size, capacity=capacity, num_threads=num_threads, \n                allow_smaller_final_batch=True)\n        \n        return batch\n\n\ndef get_batch_join(tfrecords_list, batch_size, shuffle=False, num_threads=1, min_after_dequeue=None, num_epochs=None):\n    name = ""batch_join"" if not shuffle else ""shuffle_batch_join""\n    with tf.variable_scope(name):\n        filename_queue = tf.train.string_input_producer(tfrecords_list, shuffle=shuffle, num_epochs=num_epochs)\n        example_list = [read_parse_preproc(filename_queue) for _ in range(num_threads)]\n        \n        if min_after_dequeue is None:\n            min_after_dequeue = batch_size * 10\n        capacity = min_after_dequeue + 3*batch_size\n        if shuffle:\n            batch = tf.train.shuffle_batch_join(tensors_list=example_list, batch_size=batch_size, capacity=capacity, \n                                                min_after_dequeue=min_after_dequeue, allow_smaller_final_batch=True)\n        else:\n            batch = tf.train.batch_join(example_list, batch_size, capacity=capacity, allow_smaller_final_batch=True)\n            \n        return batch\n\n\n# interfaces\ndef shuffle_batch_join(tfrecords_list, batch_size, num_threads, num_epochs, min_after_dequeue=None):\n    return get_batch_join(tfrecords_list, batch_size, shuffle=True, num_threads=num_threads, \n        num_epochs=num_epochs, min_after_dequeue=min_after_dequeue)\n\ndef batch_join(tfrecords_list, batch_size, num_threads, num_epochs, min_after_dequeue=None):\n    return get_batch_join(tfrecords_list, batch_size, shuffle=False, num_threads=num_threads, \n        num_epochs=num_epochs, min_after_dequeue=min_after_dequeue)\n\ndef shuffle_batch(tfrecords_list, batch_size, num_threads, num_epochs, min_after_dequeue=None):\n    return get_batch(tfrecords_list, batch_size, shuffle=True, num_threads=num_threads, \n        num_epochs=num_epochs, min_after_dequeue=min_after_dequeue)\n\ndef batch(tfrecords_list, batch_size, num_threads, num_epochs, min_after_dequeue=None):\n    return get_batch(tfrecords_list, batch_size, shuffle=False, num_threads=num_threads, \n        num_epochs=num_epochs, min_after_dequeue=min_after_dequeue)\n'"
ops.py,2,"b'# coding: utf-8\n\nimport tensorflow as tf\nslim = tf.contrib.slim\n\n\ndef lrelu(inputs, leak=0.2, scope=""lrelu""):\n    """"""\n    https://github.com/tensorflow/tensorflow/issues/4079\n    """"""\n    with tf.variable_scope(scope):\n        f1 = 0.5 * (1 + leak)\n        f2 = 0.5 * (1 - leak)\n        return f1 * inputs + f2 * abs(inputs)\n'"
train.py,14,"b'# coding: utf-8\n\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport numpy as np\nimport inputpipe as ip\nimport glob, os, sys\nfrom argparse import ArgumentParser\nimport utils, config\n\n\ndef build_parser():\n    parser = ArgumentParser()\n    parser.add_argument(\'--num_epochs\', default=20, help=\'default: 20\', type=int)\n    parser.add_argument(\'--batch_size\', default=128, help=\'default: 128\', type=int)\n    parser.add_argument(\'--num_threads\', default=4, help=\'# of data read threads (default: 4)\', type=int)\n    models_str = \' / \'.join(config.model_zoo)\n    parser.add_argument(\'--model\', help=models_str, required=True) # DRAGAN, CramerGAN\n    parser.add_argument(\'--name\', help=\'default: name=model\')\n    parser.add_argument(\'--dataset\', \'-D\', help=\'CelebA / LSUN\', required=True)\n    parser.add_argument(\'--ckpt_step\', default=5000, help=\'# of steps for saving checkpoint (default: 5000)\', type=int)\n    parser.add_argument(\'--renew\', action=\'store_true\', help=\'train model from scratch - \\\n        clean saved checkpoints and summaries\', default=False)\n\n    return parser\n\n\ndef input_pipeline(glob_pattern, batch_size, num_threads, num_epochs):\n    tfrecords_list = glob.glob(glob_pattern)\n    # num_examples = utils.num_examples_from_tfrecords(tfrecords_list) # takes too long time for lsun\n    X = ip.shuffle_batch_join(tfrecords_list, batch_size=batch_size, num_threads=num_threads, num_epochs=num_epochs)\n    return X\n\n\ndef sample_z(shape):\n    return np.random.normal(size=shape)\n\n\ndef train(model, dataset, input_op, num_epochs, batch_size, n_examples, ckpt_step, renew=False):\n    # n_examples = 202599 # same as util.num_examples_from_tfrecords(glob.glob(\'./data/celebA_tfrecords/*.tfrecord\'))\n    # 1 epoch = 1583 steps\n    print(""\\n# of examples: {}"".format(n_examples))\n    print(""steps per epoch: {}\\n"".format(n_examples//batch_size))\n\n    summary_path = os.path.join(\'./summary/\', dataset, model.name)\n    ckpt_path = os.path.join(\'./checkpoints\', dataset, model.name)\n    if renew:\n        if os.path.exists(summary_path):\n            tf.gfile.DeleteRecursively(summary_path)\n        if os.path.exists(ckpt_path):\n            tf.gfile.DeleteRecursively(ckpt_path)\n    if not os.path.exists(ckpt_path):\n        tf.gfile.MakeDirs(ckpt_path)\n\n    config = tf.ConfigProto()\n    best_gpu = utils.get_best_gpu()\n    config.gpu_options.visible_device_list = str(best_gpu) # Works same as CUDA_VISIBLE_DEVICES!\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer()) # for epochs \n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord)\n\n        # https://github.com/tensorflow/tensorflow/issues/10972        \n        # TensorFlow 1.2 has much bugs for text summary\n        # make config_summary before define of summary_writer - bypass bug of tensorboard\n        \n        # It seems that batch_size should have been contained in the model config ... \n        total_steps = int(np.ceil(n_examples * num_epochs / float(batch_size))) # total global step\n        config_list = [\n            (\'num_epochs\', num_epochs),\n            (\'total_iteration\', total_steps),\n            (\'batch_size\', batch_size), \n            (\'dataset\', dataset)\n        ]\n        model_config_list = [[k, str(w)] for k, w in sorted(model.args.items()) + config_list]\n        model_config_summary_op = tf.summary.text(model.name + \'/config\', tf.convert_to_tensor(model_config_list), \n            collections=[])\n        model_config_summary = sess.run(model_config_summary_op)\n\n        # print to console\n        print(""\\n====== Process info ======="")\n        print(""argv: {}"".format(\' \'.join(sys.argv)))\n        print(""PID: {}"".format(os.getpid()))\n        print(""====== Model configs ======"")\n        for k, v in model_config_list:\n            print(""{}: {}"".format(k, v))\n        print(""===========================\\n"")\n\n        summary_writer = tf.summary.FileWriter(summary_path, flush_secs=30, graph=sess.graph)\n        summary_writer.add_summary(model_config_summary)\n        pbar = tqdm(total=total_steps, desc=\'global_step\')\n        saver = tf.train.Saver(max_to_keep=9999) # save all checkpoints\n        global_step = 0\n\n        ckpt = tf.train.get_checkpoint_state(ckpt_path)\n        if ckpt:\n            saver.restore(sess, ckpt.model_checkpoint_path)\n            global_step = sess.run(model.global_step)\n            print(\'\\n[!] Restore from {} ... starting global step is {}\\n\'.format(ckpt.model_checkpoint_path, global_step))\n            pbar.update(global_step)\n\n        try:\n            # If training process was resumed from checkpoints, input pipeline cannot detect\n            # when training should stop. So we need `global_step < total_step` condition.\n            while not coord.should_stop() and global_step < total_steps:\n                # model.all_summary_op contains histogram summary and image summary which are heavy op\n                summary_op = model.summary_op if global_step % 100 == 0 else model.all_summary_op\n\n                batch_X = sess.run(input_op)\n                batch_z = sample_z([batch_size, model.z_dim])\n\n                _, summary = sess.run([model.D_train_op, summary_op], {model.X: batch_X, model.z: batch_z})\n                _, global_step = sess.run([model.G_train_op, model.global_step], {model.z: batch_z})\n\n                summary_writer.add_summary(summary, global_step=global_step)\n\n                if global_step % 10 == 0:\n                    pbar.update(10)\n\n                    if global_step % ckpt_step == 0:\n                        saver.save(sess, ckpt_path+\'/\'+model.name, global_step=global_step)\n\n        except tf.errors.OutOfRangeError:\n            print(\'\\nDone -- epoch limit reached\\n\')\n        finally:\n            coord.request_stop()\n\n        coord.join(threads)\n        summary_writer.close()\n        pbar.close()\n\n\nif __name__ == ""__main__"":\n    parser = build_parser()\n    FLAGS = parser.parse_args()\n    FLAGS.model = FLAGS.model.upper()\n    FLAGS.dataset = FLAGS.dataset.lower()\n    if FLAGS.name is None:\n        FLAGS.name = FLAGS.model.lower()\n    config.pprint_args(FLAGS)\n\n    # get information for dataset\n    dataset_pattern, n_examples = config.get_dataset(FLAGS.dataset)\n    # input pipeline\n    X = input_pipeline(dataset_pattern, batch_size=FLAGS.batch_size, \n        num_threads=FLAGS.num_threads, num_epochs=FLAGS.num_epochs)\n\n    model = config.get_model(FLAGS.model, FLAGS.name, training=True)\n    train(model=model, dataset=FLAGS.dataset, input_op=X, num_epochs=FLAGS.num_epochs, batch_size=FLAGS.batch_size, \n        n_examples=n_examples, ckpt_step=FLAGS.ckpt_step, renew=FLAGS.renew)\n'"
utils.py,13,"b'# coding: utf-8\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\'\'\'https://stackoverflow.com/questions/37604289/tkinter-tclerror-no-display-name-and-no-display-environment-variable\nMatplotlib chooses Xwindows backend by default. You need to set matplotlib do not use Xwindows backend.\n- `matplotlib.use(\'Agg\')`\n- Or add to .config/matplotlib/matplotlibrc line backend : Agg.\n- Or when connect to server use ssh -X ... command to use Xwindows.\n\'\'\'\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport scipy.misc\nimport numpy as np\n\n\ndef get_best_gpu():\n    \'\'\'Dependency: pynvml (for gpu memory informations)\n    return type is integer (gpu_id)\n    \'\'\'\n    try:\n        from pynvml import nvmlInit, nvmlDeviceGetCount, nvmlDeviceGetHandleByIndex, nvmlDeviceGetName, nvmlDeviceGetMemoryInfo\n    except Exception as e:\n        print(\'[!] {} => Use default GPU settings ...\\n\'.format(e))\n        return \'\'\n\n    print(\'\\n===== Check GPU memory =====\')\n\n    # byte to megabyte\n    def to_mb(x):\n        return int(x/1024./1024.) \n\n    best_idx = -1\n    best_free = 0.\n    nvmlInit()\n    n_gpu = nvmlDeviceGetCount()\n    for i in range(n_gpu):\n        handle = nvmlDeviceGetHandleByIndex(i)\n        name = nvmlDeviceGetName(handle)\n        mem = nvmlDeviceGetMemoryInfo(handle)\n\n        total = to_mb(mem.total)\n        free = to_mb(mem.free)\n        used = to_mb(mem.used)\n        free_ratio = mem.free / float(mem.total)\n\n        print(""{} - {}/{} MB (free: {} MB - {:.2%})"".format(name, used, total, free, free_ratio))\n\n        if free > best_free:\n            best_free = free\n            best_idx = i\n\n    print(\'\\nSelected GPU is gpu:{}\'.format(best_idx))\n    print(\'============================\\n\')\n\n    return best_idx\n\n\n# Iterate the whole dataset and count the numbers\n# CelebA contains about 200k examples with 128 tfrecord files and it takes about 1.5s to iterate\ndef num_examples_from_tfrecords(tfrecords_list):\n    num_examples = 0 \n    for path in tfrecords_list:\n        num_examples += sum(1 for _ in tf.python_io.tf_record_iterator(path))\n    return num_examples\n\n\ndef expected_shape(tensor, expected):\n    """"""batch size N shouldn\'t be set. \n    you can use shape of tensor instead of tensor itself.\n    \n    Usage:\n    # batch size N is skipped.\n    expected_shape(tensor, [28, 28, 1])\n    expected_shape(tensor.shape, [28, 28, 1])\n    """"""\n    if isinstance(tensor, tf.Tensor):\n        shape = tensor.shape[1:]\n    else:\n        shape = tensor[1:]\n    shape = map(lambda x: x.value, shape)\n    err_msg = \'wrong shape {} (expected shape is {})\'.format(shape, expected)\n    assert shape == expected, err_msg\n    # if not shape == expected:\n    #     warnings.warn(\'wrong shape {} (expected shape is {})\'.format(shape, expected))\n\n\ndef plot(samples, shape=(4,4), figratio=0.75):\n    """"""only for square-size samples\n    wh = sqrt(samples.size)\n    figratio: small-size = 0.75 (default) / big-size = 1.0\n    """"""\n    if len(samples) != shape[0]*shape[1]:\n        print(""Error: # of samples = {} but shape is {}"".format(len(samples), shape))\n        return\n    \n    h_figsize = shape[0] * figratio\n    w_figsize = shape[1] * figratio\n    fig = plt.figure(figsize=(w_figsize, h_figsize))\n    gs = gridspec.GridSpec(shape[0], shape[1])\n    gs.update(wspace=0.05, hspace=0.05)\n\n    for i, sample in enumerate(samples):\n        ax = plt.subplot(gs[i])\n        plt.axis(\'off\')\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        ax.set_aspect(\'equal\')\n        plt.imshow(sample) # checks cmap ...\n\n    return fig\n\n\ndef show_all_variables():\n    model_vars = tf.trainable_variables()\n    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n\n\ndef merge(images, size):\n    """"""merge images - burrowed from @carpedm20.\n\n    checklist before/after imsave:\n    * are images post-processed? for example - denormalization\n    * is np.squeeze required? maybe for grayscale...\n    """"""\n    h, w = images.shape[1], images.shape[2]\n    if (images.shape[3] in (3,4)):\n        c = images.shape[3]\n        img = np.zeros((h * size[0], w * size[1], c))\n        for idx, image in enumerate(images):\n            i = idx % size[1]\n            j = idx // size[1]\n            img[j * h:j * h + h, i * w:i * w + w, :] = image\n        return img\n    elif images.shape[3]==1:\n        img = np.zeros((h * size[0], w * size[1]))\n        for idx, image in enumerate(images):\n            i = idx % size[1]\n            j = idx // size[1]\n            img[j * h:j * h + h, i * w:i * w + w] = image[:,:,0]\n        return img\n    else:\n        raise ValueError(\'in merge(images,size) images parameter must have dimensions: HxW or HxWx3 or HxWx4\')\n\n\n\'\'\'Sugar for gradients histograms\n# D_train_op = tf.train.AdamOptimizer(learning_rate=self.D_lr, beta1=self.beta1, beta2=self.beta2).\\\n#     minimize(D_loss, var_list=D_vars)\nD_opt = tf.train.AdamOptimizer(learning_rate=self.D_lr, beta1=self.beta1, beta2=self.beta2)\nD_grads = tf.gradients(D_loss, D_vars)\nD_grads_and_vars = list(zip(D_grads, D_vars))\nD_train_op = D_opt.apply_gradients(grads_and_vars=D_grads_and_vars)\n\n# G_train_op = tf.train.AdamOptimizer(learning_rate=self.G_lr, beta1=self.beta1, beta2=self.beta2).\\\n#     minimize(G_loss, var_list=G_vars, global_step=global_step)\nG_opt = tf.train.AdamOptimizer(learning_rate=self.G_lr, beta1=self.beta1, beta2=self.beta2)\nG_grads = tf.gradients(G_loss, G_vars)\nG_grads_and_vars = list(zip(G_grads, G_vars))\nG_train_op = G_opt.apply_gradients(grads_and_vars=G_grads_and_vars, global_step=global_step)\n\n\nfor var in tf.trainable_variables():\n    tf.summary.histogram(var.op.name, var)\n\nfor grad, var in D_grads_and_vars:\n    tf.summary.histogram(\'D/\' + var.name + \'/gradient\', grad)\nfor grad, var in G_grads_and_vars:\n    tf.summary.histogram(\'G/\' + var.name + \'/gradient\', grad)\n\'\'\'\n'"
models/__init__.py,0,"b'from os.path import dirname, basename, isfile\nimport glob\n\n\ndef get_all_modules_cwd():\n    modules = glob.glob(dirname(__file__)+""/*.py"")\n    return [basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(\'__init__.py\')]\n\n\n__all__ = get_all_modules_cwd()'"
models/basemodel.py,4,"b'# coding: utf-8\n\n\'\'\'BaseModel for Generative Adversarial Netowrks.\n\'\'\'\n\nimport tensorflow as tf\nslim = tf.contrib.slim\n\n\nclass BaseModel(object):\n    FAKE_MAX_OUTPUT = 6\n\n    def __init__(self, name, training, D_lr, G_lr, image_shape=[64, 64, 3], z_dim=100):\n        self.name = name\n        self.shape = image_shape\n        self.bn_params = {\n            ""decay"": 0.99,\n            ""epsilon"": 1e-5,\n            ""scale"": True,\n            ""is_training"": training\n        }\n        self.z_dim = z_dim\n        self.D_lr = D_lr\n        self.G_lr = G_lr\n        self.args = vars(self).copy() # dict\n\n        if training == True:\n            self._build_train_graph()\n        else:\n            self._build_gen_graph()\n\n\n    def _build_gen_graph(self):\n        \'\'\'build computational graph for generation (evaluation)\'\'\'\n        with tf.variable_scope(self.name):\n            self.z = tf.placeholder(tf.float32, [None, self.z_dim])\n            self.fake_sample = tf.clip_by_value(self._generator(self.z), -1., 1.)\n\n\n    def _build_train_graph(self, X):\n        \'\'\'build computational graph for training\'\'\'\n        pass'"
models/began.py,56,"b""# coding: utf-8\nimport tensorflow as tf\nslim = tf.contrib.slim\nfrom utils import expected_shape\nimport ops\nfrom basemodel import BaseModel\n\n\nclass BEGAN(BaseModel):\n    def __init__(self, name, training, D_lr=1e-4, G_lr=1e-4, image_shape=[64, 64, 3], z_dim=64, gamma=0.5):\n        self.gamma = gamma\n        self.decay_step = 3000\n        self.decay_rate = 0.95\n        self.beta1 = 0.5\n        self.lambd_k = 0.001\n        self.nf = 128\n        self.lr_lower_bound = 2e-5\n        super(BEGAN, self).__init__(name=name, training=training, D_lr=D_lr, G_lr=G_lr, \n            image_shape=image_shape, z_dim=z_dim)\n\n    def _build_train_graph(self):\n        with tf.variable_scope(self.name):\n            X = tf.placeholder(tf.float32, [None] + self.shape)\n            z = tf.placeholder(tf.float32, [None, self.z_dim])\n            global_step = tf.Variable(0, name='global_step', trainable=False)\n\n            G = self._generator(z)\n            # Discriminator is not called an energy function in BEGAN. The naming is from EBGAN.\n            D_real_energy = self._discriminator(X)\n            D_fake_energy = self._discriminator(G, reuse=True)\n\n            k = tf.Variable(0., name='k', trainable=False)\n            with tf.variable_scope('D_loss'):\n                D_loss = D_real_energy - k * D_fake_energy\n            with tf.variable_scope('G_loss'):\n                G_loss = D_fake_energy\n            with tf.variable_scope('balance'):\n                balance = self.gamma*D_real_energy - D_fake_energy\n            with tf.variable_scope('M'):\n                M = D_real_energy + tf.abs(balance)\n\n            D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/D/')\n            G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/G/')\n\n            D_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/D/')\n            G_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/G/')\n\n            # The authors suggest decaying learning rate by 0.5 when the convergence mesure stall\n            # carpedm20 decays by 0.5 per 100000 steps\n            # Heumi decays by 0.95 per 2000 steps (https://github.com/Heumi/BEGAN-tensorflow/)\n            D_lr = tf.train.exponential_decay(self.D_lr, global_step, self.decay_step, self.decay_rate, staircase=True)\n            D_lr = tf.maximum(D_lr, self.lr_lower_bound)\n            G_lr = tf.train.exponential_decay(self.G_lr, global_step, self.decay_step, self.decay_rate, staircase=True)\n            G_lr = tf.maximum(G_lr, self.lr_lower_bound)\n\n            with tf.variable_scope('D_train_op'):\n                with tf.control_dependencies(D_update_ops):\n                    D_train_op = tf.train.AdamOptimizer(learning_rate=D_lr, beta1=self.beta1).\\\n                        minimize(D_loss, var_list=D_vars)\n            with tf.variable_scope('G_train_op'):\n                with tf.control_dependencies(G_update_ops):\n                    G_train_op = tf.train.AdamOptimizer(learning_rate=G_lr, beta1=self.beta1).\\\n                        minimize(G_loss, var_list=G_vars, global_step=global_step)\n\n            # It should be ops `define` under control_dependencies\n            with tf.control_dependencies([D_train_op]): # should be iterable\n                with tf.variable_scope('update_k'):\n                    update_k = tf.assign(k, tf.clip_by_value(k + self.lambd_k * balance, 0., 1.)) # define\n            D_train_op = update_k # run op\n\n            # summaries\n            # per-step summary\n            self.summary_op = tf.summary.merge([\n                tf.summary.scalar('G_loss', G_loss),\n                tf.summary.scalar('D_loss', D_loss),\n                tf.summary.scalar('D_energy/real', D_real_energy),\n                tf.summary.scalar('D_energy/fake', D_fake_energy),\n                tf.summary.scalar('convergence_measure', M),\n                tf.summary.scalar('balance', balance),\n                tf.summary.scalar('k', k),\n                tf.summary.scalar('D_lr', D_lr),\n                tf.summary.scalar('G_lr', G_lr)\n            ])\n\n            # sparse-step summary\n            # Generator of BEGAN does not use tanh activation func.\n            # So the generated sample (fake sample) can exceed the image bound [-1, 1].\n            fake_sample = tf.clip_by_value(G, -1., 1.)\n            tf.summary.image('fake_sample', fake_sample, max_outputs=self.FAKE_MAX_OUTPUT)\n            tf.summary.histogram('G_hist', G) # for checking out of bound\n            # histogram all varibles\n            # for var in tf.trainable_variables():\n            #     tf.summary.histogram(var.op.name, var)\n\n            self.all_summary_op = tf.summary.merge_all()\n\n            # accesible points\n            self.X = X\n            self.z = z\n            self.D_train_op = D_train_op\n            self.G_train_op = G_train_op\n            self.fake_sample = fake_sample\n            self.global_step = global_step\n\n    def _encoder(self, X, reuse=False):\n        with tf.variable_scope('encoder', reuse=reuse):\n            nf = self.nf\n            nh = self.z_dim\n\n            with slim.arg_scope([slim.conv2d], kernel_size=[3,3], padding='SAME', activation_fn=tf.nn.elu):\n                net = slim.conv2d(X, nf)\n\n                net = slim.conv2d(net, nf)\n                net = slim.conv2d(net, nf)\n                net = slim.conv2d(net, nf*2, stride=2) # 32x32\n\n                net = slim.conv2d(net, nf*2)\n                net = slim.conv2d(net, nf*2)\n                net = slim.conv2d(net, nf*3, stride=2) # 16x16\n\n                net = slim.conv2d(net, nf*3)\n                net = slim.conv2d(net, nf*3)\n                net = slim.conv2d(net, nf*4, stride=2) # 8x8\n\n                net = slim.conv2d(net, nf*4)\n                net = slim.conv2d(net, nf*4)\n                net = slim.conv2d(net, nf*4)\n\n            net = slim.flatten(net)\n            h = slim.fully_connected(net, nh, activation_fn=None)\n\n            return h\n\n    def _decoder(self, h, reuse=False):\n        with tf.variable_scope('decoder', reuse=reuse):\n            nf = self.nf\n            nh = self.z_dim\n\n            h0 = slim.fully_connected(h, 8*8*nf, activation_fn=None) # h0\n            net = tf.reshape(h0, [-1, 8, 8, nf])\n\n            with slim.arg_scope([slim.conv2d], kernel_size=[3,3], padding='SAME', activation_fn=tf.nn.elu):\n                net = slim.conv2d(net, nf)\n                net = slim.conv2d(net, nf)\n                net = tf.image.resize_nearest_neighbor(net, [16, 16]) # upsampling\n\n                net = slim.conv2d(net, nf)\n                net = slim.conv2d(net, nf)\n                net = tf.image.resize_nearest_neighbor(net, [32, 32])\n\n                net = slim.conv2d(net, nf)\n                net = slim.conv2d(net, nf)\n                net = tf.image.resize_nearest_neighbor(net, [64, 64])\n\n                net = slim.conv2d(net, nf)\n                net = slim.conv2d(net, nf)\n\n                net = slim.conv2d(net, 3, activation_fn=None)\n\n            return net\n\n    def _discriminator(self, X, reuse=False):\n        with tf.variable_scope('D', reuse=reuse):\n            h = self._encoder(X, reuse=reuse)\n            x_recon = self._decoder(h, reuse=reuse)\n\n            energy = tf.abs(X-x_recon) # L1 loss\n            energy = tf.reduce_mean(energy)\n\n            return energy\n\n    def _generator(self, z, reuse=False):\n        with tf.variable_scope('G', reuse=reuse):\n            x_fake = self._decoder(z, reuse=reuse)\n\n            return x_fake\n"""
models/coulombgan.py,58,"b""# coding: utf-8\r\n# Reference code: https://github.com/bioinf-jku/coulomb_gan\r\nimport tensorflow as tf\r\nimport numpy as np\r\nslim = tf.contrib.slim\r\nfrom utils import expected_shape\r\nimport ops\r\nfrom basemodel import BaseModel\r\n\r\n\r\ndef sd_matrix(a, b, name='square_distance_matrix'):\r\n    with tf.variable_scope(name):\r\n        '''Square distance matrix\r\n        a, b: [N, tensor] (N = batch size)\r\n        return: [N, N] (square distance matrix for every tensor pairs)\r\n        '''\r\n        batch_size = tf.shape(a)[0]\r\n        a = tf.reshape(a, [batch_size, 1, -1])\r\n        b = tf.reshape(b, [1, batch_size, -1])\r\n        return tf.reduce_sum((b-a)**2, axis=2)\r\n\r\n\r\ndef plummer_kernel(a, b, kernel_dim, kernel_eps, name='plummer_kernel'):\r\n    # plummer kernel represents `influence`. \r\n    with tf.variable_scope(name):\r\n        r = sd_matrix(a, b) + kernel_eps**2\r\n        d = kernel_dim-2\r\n        return r**(-d/2.)\r\n\r\n\r\n# Burrowed from ref code and modified to paper-style.\r\ndef get_potentials(x, y, kernel_dim, kernel_eps):\r\n    '''\r\n    This is alsmost the same `calculate_potential`, but\r\n        px, py = get_potentials(x, y)\r\n    is faster than:\r\n        px = calculate_potential(x, y, x)\r\n        py = calculate_potential(x, y, y)\r\n    because we calculate the cross terms only once.\r\n    '''\r\n    x_fixed = tf.stop_gradient(x)\r\n    y_fixed = tf.stop_gradient(y)\r\n    pk_xx = plummer_kernel(x_fixed, x, kernel_dim, kernel_eps)\r\n    pk_yx = plummer_kernel(y, x, kernel_dim, kernel_eps)\r\n    pk_yy = plummer_kernel(y_fixed, y, kernel_dim, kernel_eps)\r\n    batch_size = tf.shape(x)[0]\r\n    pk_xx = tf.matrix_set_diag(pk_xx, tf.ones(shape=[batch_size], dtype=pk_xx.dtype))\r\n    pk_yy = tf.matrix_set_diag(pk_yy, tf.ones(shape=[batch_size], dtype=pk_yy.dtype))\r\n    kxx = tf.reduce_mean(pk_xx, axis=0)\r\n    kyx = tf.reduce_mean(pk_yx, axis=0)\r\n    kxy = tf.reduce_mean(pk_yx, axis=1)\r\n    kyy = tf.reduce_mean(pk_yy, axis=0)\r\n    pot_x = kyx - kxx\r\n    pot_y = kyy - kyx\r\n    pot_x = tf.reshape(pot_x, [batch_size, -1])\r\n    pot_y = tf.reshape(pot_y, [batch_size, -1])\r\n    return pot_x, pot_y\r\n\r\n\r\ndef calc_potential(x, y, a, kernel_dim, kernel_eps, name='potential'):\r\n    '''Paper notations are used in this function\r\n    x: fake\r\n    y: real\r\n    \r\n    return: potential of a\r\n    '''\r\n\r\n    with tf.variable_scope(name):\r\n        # Why does stop_gradient not apply to a?\r\n        x = tf.stop_gradient(x)\r\n        y = tf.stop_gradient(y)\r\n        kxa = tf.reduce_mean(plummer_kernel(x, a, kernel_dim, kernel_eps), axis=0)\r\n        kya = tf.reduce_mean(plummer_kernel(y, a, kernel_dim, kernel_eps), axis=0)\r\n        # kxa: influence of fake on a\r\n        # kya: influence of real on a\r\n        p = kya - kxa\r\n        p = tf.reshape(p, [-1, 1])\r\n        return p\r\n\r\n\r\n'''\r\nOriginally, D_lr=5e-5 and G_lr=1e-4 in the paper.\r\nIt takes too long to train, so I used higher learning rates (5 times each).\r\n'''\r\nclass CoulombGAN(BaseModel):\r\n    def __init__(self, name, training, D_lr=25e-5, G_lr=5e-4, image_shape=[64, 64, 3], z_dim=32):\r\n        self.beta1 = 0.5\r\n        self.kernel_dim = 3\r\n        self.kernel_eps = 1.\r\n        super(CoulombGAN, self).__init__(name=name, training=training, D_lr=D_lr, G_lr=G_lr, \r\n            image_shape=image_shape, z_dim=z_dim)\r\n\r\n    def _build_train_graph(self):\r\n        with tf.variable_scope(self.name):\r\n            X = tf.placeholder(tf.float32, [None] + self.shape)\r\n            z = tf.placeholder(tf.float32, [None, self.z_dim])\r\n            global_step = tf.Variable(0, name='global_step', trainable=False)\r\n\r\n            G = self._generator(z)\r\n            D_real = self._discriminator(X)\r\n            D_fake = self._discriminator(G, reuse=True)\r\n\r\n            '''\r\n            D estimates potential and G minimize D_fake (estimated potential of fake). \r\n            It means that minimize distance the between real and fake \r\n            while maximizing the distance between fake and fake.\r\n\r\n            P(a) = k(a, real) - k(a, fake).\r\n            So, \r\n            P(real) = k(real, real) - k(real, fake),\r\n            P(fake) = k(fake, real) - k(fake, fake).\r\n            '''\r\n\r\n            # get_potentials function is more efficient but it is more readable and intuitive\r\n            # to calculate potential for each real and fake samples separately.\r\n            # Further, there was no significant difference in efficiency as a result of the experiment.\r\n            P_real = calc_potential(G, X, X, kernel_dim=self.kernel_dim, kernel_eps=self.kernel_eps, name='P_real')\r\n            P_fake = calc_potential(G, X, G, kernel_dim=self.kernel_dim, kernel_eps=self.kernel_eps, name='P_fake')\r\n            D_loss_real = tf.losses.mean_squared_error(D_real, P_real)\r\n            D_loss_fake = tf.losses.mean_squared_error(D_fake, P_fake)\r\n            D_loss = D_loss_real + D_loss_fake\r\n            G_loss = -tf.reduce_mean(D_fake)\r\n\r\n            D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/D/')\r\n            G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/G/')\r\n\r\n            D_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/D/')\r\n            G_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/G/')\r\n\r\n            with tf.control_dependencies(D_update_ops):\r\n                D_train_op = tf.train.AdamOptimizer(learning_rate=self.D_lr, beta1=self.beta1).\\\r\n                    minimize(D_loss, var_list=D_vars)\r\n            with tf.control_dependencies(G_update_ops):\r\n                G_train_op = tf.train.AdamOptimizer(learning_rate=self.G_lr, beta1=self.beta1).\\\r\n                    minimize(G_loss, var_list=G_vars, global_step=global_step)\r\n\r\n            # summaries\r\n            # per-step summary\r\n            self.summary_op = tf.summary.merge([\r\n                tf.summary.scalar('G_loss', G_loss),\r\n                tf.summary.scalar('D_loss', D_loss),\r\n                tf.summary.scalar('potential/real_mean', tf.reduce_mean(P_real)),\r\n                tf.summary.scalar('potential/fake_mean', tf.reduce_mean(P_fake))\r\n                # tf.summary.scalar('potential/real', P_real),\r\n                # tf.summary.scalar('potential/fake', P_fake),\r\n                # tf.summary.scalar('disc/real', D_real),\r\n                # tf.summary.scalar('disc/fake', D_fake)\r\n            ])\r\n\r\n            # sparse-step summary\r\n            tf.summary.image('fake_sample', G, max_outputs=self.FAKE_MAX_OUTPUT)\r\n            tf.summary.histogram('potential/real', P_real)\r\n            tf.summary.histogram('potential/fake', P_fake)\r\n            self.all_summary_op = tf.summary.merge_all()\r\n\r\n            # accesible points\r\n            self.X = X\r\n            self.z = z\r\n            self.D_train_op = D_train_op\r\n            self.G_train_op = G_train_op\r\n            self.fake_sample = G\r\n            self.global_step = global_step\r\n\r\n    # Discriminator of CoulombGAN uses double channels of DCGAN\r\n    def _discriminator(self, X, reuse=False):\r\n        with tf.variable_scope('D', reuse=reuse):\r\n            net = X\r\n            \r\n            with slim.arg_scope([slim.conv2d], kernel_size=[5,5], stride=2, padding='SAME', activation_fn=ops.lrelu, \r\n                normalizer_fn=slim.batch_norm, normalizer_params=self.bn_params):\r\n                net = slim.conv2d(net, 128, normalizer_fn=None)\r\n                net = slim.conv2d(net, 256)\r\n                net = slim.conv2d(net, 512)\r\n                net = slim.conv2d(net, 1024)\r\n                expected_shape(net, [4, 4, 1024])\r\n\r\n            net = slim.flatten(net)\r\n            logits = slim.fully_connected(net, 1, activation_fn=None)\r\n\r\n            return logits # potential\r\n\r\n    def _generator(self, z, reuse=False):\r\n        with tf.variable_scope('G', reuse=reuse):\r\n            net = z\r\n            net = slim.fully_connected(net, 4*4*1024, activation_fn=tf.nn.relu)\r\n            net = tf.reshape(net, [-1, 4, 4, 1024])\r\n\r\n            with slim.arg_scope([slim.conv2d_transpose], kernel_size=[5,5], stride=2, padding='SAME', \r\n                activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm, normalizer_params=self.bn_params):\r\n                net = slim.conv2d_transpose(net, 512)\r\n                expected_shape(net, [8, 8, 512])\r\n                net = slim.conv2d_transpose(net, 256)\r\n                expected_shape(net, [16, 16, 256])\r\n                net = slim.conv2d_transpose(net, 128)\r\n                expected_shape(net, [32, 32, 128])\r\n                net = slim.conv2d_transpose(net, 3, activation_fn=tf.nn.tanh, normalizer_fn=None)\r\n                expected_shape(net, [64, 64, 3])\r\n\r\n                return net\r\n"""
models/dcgan.py,32,"b""# coding: utf-8\r\nimport tensorflow as tf\r\nslim = tf.contrib.slim\r\nfrom utils import expected_shape\r\nimport ops\r\nfrom basemodel import BaseModel\r\n\r\n'''Original hyperparams:\r\noptimizer - SGD\r\ninit - stddev 0.02\r\n'''\r\n\r\nclass DCGAN(BaseModel):\r\n    def __init__(self, name, training, D_lr=2e-4, G_lr=2e-4, image_shape=[64, 64, 3], z_dim=100):\r\n        self.beta1 = 0.5\r\n        super(DCGAN, self).__init__(name=name, training=training, D_lr=D_lr, G_lr=G_lr, \r\n            image_shape=image_shape, z_dim=z_dim)\r\n\r\n    def _build_train_graph(self):\r\n        with tf.variable_scope(self.name):\r\n            X = tf.placeholder(tf.float32, [None] + self.shape)\r\n            z = tf.placeholder(tf.float32, [None, self.z_dim])\r\n            global_step = tf.Variable(0, name='global_step', trainable=False)\r\n\r\n            G = self._generator(z)\r\n            D_real_prob, D_real_logits = self._discriminator(X)\r\n            D_fake_prob, D_fake_logits = self._discriminator(G, reuse=True)\r\n\r\n            G_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(D_fake_logits), logits=D_fake_logits)\r\n            D_loss_real = tf.losses.sigmoid_cross_entropy(tf.ones_like(D_real_logits), logits=D_real_logits)\r\n            D_loss_fake = tf.losses.sigmoid_cross_entropy(tf.zeros_like(D_fake_logits), logits=D_fake_logits)\r\n            D_loss = D_loss_real + D_loss_fake\r\n\r\n            D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/D/')\r\n            G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/G/')\r\n\r\n            D_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/D/')\r\n            G_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/G/')\r\n\r\n            with tf.control_dependencies(D_update_ops):\r\n                D_train_op = tf.train.AdamOptimizer(learning_rate=self.D_lr, beta1=self.beta1).\\\r\n                    minimize(D_loss, var_list=D_vars)\r\n            with tf.control_dependencies(G_update_ops):\r\n                # learning rate 2e-4/1e-3\r\n                G_train_op = tf.train.AdamOptimizer(learning_rate=self.G_lr, beta1=self.beta1).\\\r\n                    minimize(G_loss, var_list=G_vars, global_step=global_step)\r\n\r\n            # summaries\r\n            # per-step summary\r\n            self.summary_op = tf.summary.merge([\r\n                tf.summary.scalar('G_loss', G_loss),\r\n                tf.summary.scalar('D_loss', D_loss),\r\n                tf.summary.scalar('D_loss/real', D_loss_real),\r\n                tf.summary.scalar('D_loss/fake', D_loss_fake)\r\n            ])\r\n\r\n            # sparse-step summary\r\n            tf.summary.image('fake_sample', G, max_outputs=self.FAKE_MAX_OUTPUT)\r\n            tf.summary.histogram('real_probs', D_real_prob)\r\n            tf.summary.histogram('fake_probs', D_fake_prob)\r\n            self.all_summary_op = tf.summary.merge_all()\r\n\r\n            # accesible points\r\n            self.X = X\r\n            self.z = z\r\n            self.D_train_op = D_train_op\r\n            self.G_train_op = G_train_op\r\n            self.fake_sample = G\r\n            self.global_step = global_step\r\n\r\n    def _discriminator(self, X, reuse=False):\r\n        with tf.variable_scope('D', reuse=reuse):\r\n            net = X\r\n            \r\n            with slim.arg_scope([slim.conv2d], kernel_size=[5,5], stride=2, padding='SAME', activation_fn=ops.lrelu, \r\n                normalizer_fn=slim.batch_norm, normalizer_params=self.bn_params):\r\n                net = slim.conv2d(net, 64, normalizer_fn=None)\r\n                expected_shape(net, [32, 32, 64])\r\n                net = slim.conv2d(net, 128)\r\n                expected_shape(net, [16, 16, 128])\r\n                net = slim.conv2d(net, 256)\r\n                expected_shape(net, [8, 8, 256])\r\n                net = slim.conv2d(net, 512)\r\n                expected_shape(net, [4, 4, 512])\r\n\r\n            net = slim.flatten(net)\r\n            logits = slim.fully_connected(net, 1, activation_fn=None)\r\n            prob = tf.sigmoid(logits)\r\n\r\n            return prob, logits\r\n\r\n    def _generator(self, z, reuse=False):\r\n        with tf.variable_scope('G', reuse=reuse):\r\n            net = z\r\n            net = slim.fully_connected(net, 4*4*1024, activation_fn=tf.nn.relu)\r\n            net = tf.reshape(net, [-1, 4, 4, 1024])\r\n\r\n            with slim.arg_scope([slim.conv2d_transpose], kernel_size=[5,5], stride=2, padding='SAME', \r\n                activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm, normalizer_params=self.bn_params):\r\n                net = slim.conv2d_transpose(net, 512)\r\n                expected_shape(net, [8, 8, 512])\r\n                net = slim.conv2d_transpose(net, 256)\r\n                expected_shape(net, [16, 16, 256])\r\n                net = slim.conv2d_transpose(net, 128)\r\n                expected_shape(net, [32, 32, 128])\r\n                net = slim.conv2d_transpose(net, 3, activation_fn=tf.nn.tanh, normalizer_fn=None)\r\n                expected_shape(net, [64, 64, 3])\r\n\r\n                return net\r\n"""
models/dragan.py,37,"b""# coding: utf-8\r\nimport tensorflow as tf\r\nslim = tf.contrib.slim\r\nfrom utils import expected_shape\r\nimport ops\r\nfrom basemodel import BaseModel\r\n\r\n'''\r\nDRAGAN has similar gradient penalty to WGAN-GP, although different motivation.\r\nIt is also similar to DCGAN except for gradient penalty.\r\n'''\r\n\r\nclass DRAGAN(BaseModel):\r\n    def __init__(self, name, training, D_lr=1e-4, G_lr=1e-4, image_shape=[64, 64, 3], z_dim=100):\r\n        self.beta1 = 0.5\r\n        self.beta2 = 0.9\r\n        self.ld = 10. # lambda\r\n        self.C = 0.5\r\n        super(DRAGAN, self).__init__(name=name, training=training, D_lr=D_lr, G_lr=G_lr, \r\n            image_shape=image_shape, z_dim=z_dim)\r\n\r\n    def _build_train_graph(self):\r\n        with tf.variable_scope(self.name):\r\n            X = tf.placeholder(tf.float32, [None] + self.shape)\r\n            z = tf.placeholder(tf.float32, [None, self.z_dim])\r\n            global_step = tf.Variable(0, name='global_step', trainable=False)\r\n\r\n            G = self._generator(z)\r\n            D_real_prob, D_real_logits = self._discriminator(X)\r\n            D_fake_prob, D_fake_logits = self._discriminator(G, reuse=True)\r\n\r\n            G_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(D_fake_logits), logits=D_fake_logits)\r\n            D_loss_real = tf.losses.sigmoid_cross_entropy(tf.ones_like(D_real_logits), logits=D_real_logits)\r\n            D_loss_fake = tf.losses.sigmoid_cross_entropy(tf.zeros_like(D_fake_logits), logits=D_fake_logits)\r\n            D_loss = D_loss_real + D_loss_fake\r\n\r\n            # Gradient Penalty (GP)\r\n            # perturbed minibatch: x_noise = x_i + noise_i\r\n            # x_hat = alpha*x + (1-alpha)*x_noise = x_i + (1-alpha)*noise_i\r\n\r\n            shape = tf.shape(X)\r\n            eps = tf.random_uniform(shape=shape, minval=0., maxval=1.)\r\n            x_mean, x_var = tf.nn.moments(X, axes=[0,1,2,3])\r\n            x_std = tf.sqrt(x_var) # magnitude of noise decides the size of local region\r\n            noise = self.C*x_std*eps # delta in paper\r\n            # Author suggested U[0,1] in original paper, but he admitted it is bug in github\r\n            # (https://github.com/kodalinaveen3/DRAGAN). It should be two-sided.\r\n            alpha = tf.random_uniform(shape=[shape[0], 1, 1, 1], minval=-1., maxval=1.) \r\n            xhat = tf.clip_by_value(X + alpha*noise, -1., 1.) # x_hat should be in the space of X\r\n\r\n            D_xhat_prob, D_xhat_logits = self._discriminator(xhat, reuse=True)\r\n            # Originally, the paper suggested D_xhat_prob instead of D_xhat_logits.\r\n            # But D_xhat_prob (D with sigmoid) causes numerical problem (NaN in gradient).\r\n            D_xhat_grad = tf.gradients(D_xhat_logits, xhat)[0] # gradient of D(x_hat)\r\n            D_xhat_grad_norm = tf.norm(D_xhat_grad, axis=1)  # l2 norm\r\n            # D_xhat_grad_norm = tf.sqrt(tf.reduce_sum(tf.square(D_xhat_grad), axis=[1]))\r\n            GP = self.ld * tf.reduce_mean(tf.square(D_xhat_grad_norm - 1.))\r\n            D_loss += GP\r\n\r\n            D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/discriminator/')\r\n            G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/generator/')\r\n\r\n            # DRAGAN does not use BN, so you don't need to set control dependencies for update ops.\r\n            D_train_op = tf.train.AdamOptimizer(learning_rate=self.D_lr, beta1=self.beta1, beta2=self.beta2).\\\r\n                minimize(D_loss, var_list=D_vars)\r\n            G_train_op = tf.train.AdamOptimizer(learning_rate=self.G_lr, beta1=self.beta1, beta2=self.beta2).\\\r\n                minimize(G_loss, var_list=G_vars, global_step=global_step)\r\n\r\n            # summaries\r\n            # per-step summary\r\n            self.summary_op = tf.summary.merge([\r\n                tf.summary.scalar('G_loss', G_loss),\r\n                tf.summary.scalar('D_loss', D_loss),\r\n                tf.summary.scalar('GP', GP)\r\n            ])\r\n\r\n            # sparse-step summary\r\n            tf.summary.image('fake_sample', G, max_outputs=self.FAKE_MAX_OUTPUT)\r\n            tf.summary.histogram('real_probs', D_real_prob)\r\n            tf.summary.histogram('fake_probs', D_fake_prob)\r\n            self.all_summary_op = tf.summary.merge_all()\r\n\r\n            # accesible points\r\n            self.X = X\r\n            self.z = z\r\n            self.D_train_op = D_train_op \r\n            self.G_train_op = G_train_op\r\n            self.fake_sample = G\r\n            self.global_step = global_step\r\n\r\n    # DRAGAN does not use BN\r\n    # DCGAN architecture\r\n    def _discriminator(self, X, reuse=False):\r\n        with tf.variable_scope('discriminator', reuse=reuse):\r\n            net = X\r\n            \r\n            with slim.arg_scope([slim.conv2d], kernel_size=[5,5], stride=2, activation_fn=ops.lrelu):\r\n                net = slim.conv2d(net, 64)\r\n                expected_shape(net, [32, 32, 64])\r\n                net = slim.conv2d(net, 128)\r\n                expected_shape(net, [16, 16, 128])\r\n                net = slim.conv2d(net, 256)\r\n                expected_shape(net, [8, 8, 256])\r\n                net = slim.conv2d(net, 512)\r\n                expected_shape(net, [4, 4, 512])\r\n\r\n            net = slim.flatten(net)\r\n            logits = slim.fully_connected(net, 1, activation_fn=None)\r\n            prob = tf.nn.sigmoid(logits)\r\n\r\n            return prob, logits\r\n\r\n    def _generator(self, z, reuse=False):\r\n        with tf.variable_scope('generator', reuse=reuse):\r\n            net = z\r\n            net = slim.fully_connected(net, 4*4*1024, activation_fn=tf.nn.relu)\r\n            net = tf.reshape(net, [-1, 4, 4, 1024])\r\n\r\n            with slim.arg_scope([slim.conv2d_transpose], kernel_size=[5,5], stride=2, activation_fn=tf.nn.relu):\r\n                net = slim.conv2d_transpose(net, 512)\r\n                expected_shape(net, [8, 8, 512])\r\n                net = slim.conv2d_transpose(net, 256)\r\n                expected_shape(net, [16, 16, 256])\r\n                net = slim.conv2d_transpose(net, 128)\r\n                expected_shape(net, [32, 32, 128])\r\n                net = slim.conv2d_transpose(net, 3, activation_fn=tf.nn.tanh, normalizer_fn=None)\r\n                expected_shape(net, [64, 64, 3])\r\n\r\n                return net\r\n"""
models/ebgan.py,37,"b""# coding: utf-8\nimport tensorflow as tf\nslim = tf.contrib.slim\nfrom utils import expected_shape\nimport ops\nfrom basemodel import BaseModel\n\n\nclass EBGAN(BaseModel):\n    def __init__(self, name, training, D_lr=1e-3, G_lr=1e-3, image_shape=[64, 64, 3], z_dim=100, \n        pt_weight=0.1, margin=20.):\n        ''' The default value of pt_weight and margin is taken from the paper for celebA. '''\n        self.pt_weight = pt_weight\n        self.m = margin\n        self.beta1 = 0.5\n        super(EBGAN, self).__init__(name=name, training=training, D_lr=D_lr, G_lr=G_lr, \n            image_shape=image_shape, z_dim=z_dim)\n\n    def _build_train_graph(self):\n        with tf.variable_scope(self.name):\n            X = tf.placeholder(tf.float32, [None] + self.shape)\n            z = tf.placeholder(tf.float32, [None, self.z_dim])\n            global_step = tf.Variable(0, name='global_step', trainable=False)\n\n            G = self._generator(z)\n            D_real_latent, D_real_energy = self._discriminator(X)\n            D_fake_latent, D_fake_energy = self._discriminator(G, reuse=True)\n\n            D_fake_hinge = tf.maximum(0., self.m - D_fake_energy) # hinge_loss\n            D_loss = D_real_energy + D_fake_hinge\n            G_loss = D_fake_energy\n            PT = self.pt_regularizer(D_fake_latent)\n            pt_loss = self.pt_weight * PT\n            G_loss += pt_loss\n\n            D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/D/')\n            G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/G/')\n\n            D_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/D/')\n            G_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/G/')\n\n            with tf.control_dependencies(D_update_ops):\n                D_train_op = tf.train.AdamOptimizer(learning_rate=self.D_lr, beta1=self.beta1).\\\n                    minimize(D_loss, var_list=D_vars)\n            with tf.control_dependencies(G_update_ops):\n                G_train_op = tf.train.AdamOptimizer(learning_rate=self.G_lr, beta1=self.beta1).\\\n                    minimize(G_loss, var_list=G_vars, global_step=global_step)\n\n            # summaries\n            # per-step summary\n            self.summary_op = tf.summary.merge([\n                tf.summary.scalar('G_loss', G_loss),\n                tf.summary.scalar('D_loss', D_loss),\n                tf.summary.scalar('PT', PT),\n                tf.summary.scalar('pt_loss', pt_loss),\n                tf.summary.scalar('D_energy/real', D_real_energy),\n                tf.summary.scalar('D_energy/fake', D_fake_energy),\n                tf.summary.scalar('D_fake_hinge', D_fake_hinge)\n            ])\n\n            # sparse-step summary\n            tf.summary.image('fake_sample', G, max_outputs=self.FAKE_MAX_OUTPUT)\n            self.all_summary_op = tf.summary.merge_all()\n\n            # accesible points\n            self.X = X\n            self.z = z\n            self.D_train_op = D_train_op\n            self.G_train_op = G_train_op\n            self.fake_sample = G\n            self.global_step = global_step\n\n    def _discriminator(self, X, reuse=False):\n        with tf.variable_scope('D', reuse=reuse):\n            net = X\n            \n            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], kernel_size=[4,4], stride=2, padding='SAME', \n                activation_fn=ops.lrelu, normalizer_fn=slim.batch_norm, normalizer_params=self.bn_params):\n                # encoder\n                net = slim.conv2d(net, 64, normalizer_fn=None) # 32x32\n                net = slim.conv2d(net, 128) # 16x16\n                net = slim.conv2d(net, 256) # 8x8\n                latent = net\n                expected_shape(latent, [8, 8, 256])\n                # decoder\n                net = slim.conv2d_transpose(net, 128) # 16x16\n                net = slim.conv2d_transpose(net, 64) # 32x32\n                x_recon = slim.conv2d_transpose(net, 3, activation_fn=None, normalizer_fn=None)\n                expected_shape(x_recon, [64, 64, 3])\n            \n            energy = tf.sqrt(tf.reduce_sum(tf.square(X-x_recon), axis=[1,2,3])) # l2-norm error\n            energy = tf.reduce_mean(energy)\n\n            return latent, energy\n\n    def _generator(self, z, reuse=False):\n        with tf.variable_scope('G', reuse=reuse):\n            net = z\n            net = slim.fully_connected(net, 4*4*1024, activation_fn=tf.nn.relu)\n            net = tf.reshape(net, [-1, 4, 4, 1024])\n\n            with slim.arg_scope([slim.conv2d_transpose], kernel_size=[4,4], stride=2, padding='SAME', \n                activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm, normalizer_params=self.bn_params):\n                net = slim.conv2d_transpose(net, 512)\n                expected_shape(net, [8, 8, 512])\n                net = slim.conv2d_transpose(net, 256)\n                expected_shape(net, [16, 16, 256])\n                net = slim.conv2d_transpose(net, 128)\n                expected_shape(net, [32, 32, 128])\n                net = slim.conv2d_transpose(net, 3, activation_fn=tf.nn.tanh, normalizer_fn=None)\n                expected_shape(net, [64, 64, 3])\n\n                return net\n\n    # lf: latent features\n    def pt_regularizer(self, lf):\n        eps = 1e-8 # epsilon for numerical stability\n        lf = slim.flatten(lf)\n        # l2_norm = tf.sqrt(tf.reduce_sum(tf.square(lf), axis=1, keep_dims=True))\n        l2_norm = tf.norm(lf, axis=1, keep_dims=True)\n        expected_shape(l2_norm, [1])\n        unit_lf = lf / (l2_norm + eps) \n        cos_sim = tf.square(tf.matmul(unit_lf, unit_lf, transpose_b=True)) # [N, h_dim] x [h_dim, N] = [N, N]\n        N = tf.cast(tf.shape(lf)[0], tf.float32) # batch_size\n        pt_loss = (tf.reduce_sum(cos_sim)-N) / (N*(N-1))\n        return pt_loss\n        \n"""
models/lsgan.py,31,"b""# coding: utf-8\nimport tensorflow as tf\nslim = tf.contrib.slim\nfrom utils import expected_shape\nimport ops\nfrom basemodel import BaseModel\n\n\nclass LSGAN(BaseModel):\n    def __init__(self, name, training, D_lr=1e-3, G_lr=1e-3, image_shape=[64, 64, 3], z_dim=1024, a=0., b=1., c=1.):\n        '''\n        a: fake label\n        b: real label\n        c: real label for G (The value that G wants to deceive D - intuitively same as real label b) \n\n        Pearson chi-square divergence: a=-1, b=1, c=0.\n        Intuitive (real label 1, fake label 0): a=0, b=c=1.\n        '''\n        self.a = a\n        self.b = b\n        self.c = c\n        self.beta1 = 0.5\n        super(LSGAN, self).__init__(name=name, training=training, D_lr=D_lr, G_lr=G_lr, \n            image_shape=image_shape, z_dim=z_dim)\n\n    def _build_train_graph(self):\n        with tf.variable_scope(self.name):\n            X = tf.placeholder(tf.float32, [None] + self.shape)\n            z = tf.placeholder(tf.float32, [None, self.z_dim])\n            global_step = tf.Variable(0, name='global_step', trainable=False)\n\n            G = self._generator(z)\n            D_real = self._discriminator(X)\n            D_fake = self._discriminator(G, reuse=True)\n\n            D_loss_real = 0.5 * tf.reduce_mean(tf.square(D_real - self.b)) # self.b\n            D_loss_fake = 0.5 * tf.reduce_mean(tf.square(D_fake - self.a)) # self.a\n            D_loss = D_loss_real + D_loss_fake\n            G_loss = 0.5 * tf.reduce_mean(tf.square(D_fake - self.c)) # self.c\n\n            D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/D/')\n            G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/G/')\n\n            D_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/D/')\n            G_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/G/')\n            \n            with tf.control_dependencies(D_update_ops):\n                D_train_op = tf.train.AdamOptimizer(learning_rate=self.D_lr, beta1=self.beta1).\\\n                    minimize(D_loss, var_list=D_vars)\n\n            with tf.control_dependencies(G_update_ops):\n                G_train_op = tf.train.AdamOptimizer(learning_rate=self.G_lr, beta1=self.beta1).\\\n                    minimize(G_loss, var_list=G_vars, global_step=global_step)\n\n            # summaries\n            # per-step summary\n            self.summary_op = tf.summary.merge([\n                tf.summary.scalar('G/loss', G_loss),\n                tf.summary.scalar('D/loss', D_loss),\n                tf.summary.scalar('D/loss/real', D_loss_real),\n                tf.summary.scalar('D/loss/fake', D_loss_fake)\n            ])\n\n            # sparse-step summary\n            tf.summary.image('G/fake_sample', G, max_outputs=self.FAKE_MAX_OUTPUT)\n            tf.summary.histogram('D/real_value', D_real)\n            tf.summary.histogram('D/fake_value', D_fake)\n\n            self.all_summary_op = tf.summary.merge_all()\n\n            # accesible points\n            self.X = X\n            self.z = z\n            self.D_train_op = D_train_op\n            self.G_train_op = G_train_op\n            self.fake_sample = G\n            self.global_step = global_step\n\n    def _discriminator(self, X, reuse=False):\n        with tf.variable_scope('D', reuse=reuse):\n            net = X\n            \n            with slim.arg_scope([slim.conv2d], kernel_size=[5,5], stride=2, padding='SAME', activation_fn=ops.lrelu, \n                normalizer_fn=slim.batch_norm, normalizer_params=self.bn_params):\n            \n                net = slim.conv2d(net, 64, normalizer_fn=None)\n                expected_shape(net, [32, 32, 64])\n                net = slim.conv2d(net, 128)\n                expected_shape(net, [16, 16, 128])\n                net = slim.conv2d(net, 256)\n                expected_shape(net, [8, 8, 256])\n                net = slim.conv2d(net, 512)\n                expected_shape(net, [4, 4, 512])\n\n            net = slim.flatten(net)\n            d_value = slim.fully_connected(net, 1, activation_fn=None)\n\n            return d_value\n\n    # Originally, LSGAN used 112x112 LSUN images\n    # We used 64x64 CelebA images\n    def _generator(self, z, reuse=False):\n        with tf.variable_scope('G', reuse=reuse):\n            net = z\n            net = slim.fully_connected(net, 4*4*256, activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm, \n                normalizer_params=self.bn_params)\n            net = tf.reshape(net, [-1, 4, 4, 256])\n\n            with slim.arg_scope([slim.conv2d_transpose], kernel_size=[3,3], padding='SAME', activation_fn=tf.nn.relu, \n                normalizer_fn=slim.batch_norm, normalizer_params=self.bn_params):\n\n                net = slim.conv2d_transpose(net, 256, stride=2)\n                net = slim.conv2d_transpose(net, 256, stride=1)\n                expected_shape(net, [8, 8, 256])\n                net = slim.conv2d_transpose(net, 256, stride=2)\n                net = slim.conv2d_transpose(net, 256, stride=1)\n                expected_shape(net, [16, 16, 256])\n                net = slim.conv2d_transpose(net, 128, stride=2)\n                expected_shape(net, [32, 32, 128])\n                net = slim.conv2d_transpose(net, 64, stride=2)\n                expected_shape(net, [64, 64, 64])\n                net = slim.conv2d_transpose(net, 3, stride=1, activation_fn=tf.nn.tanh, normalizer_fn=None)\n                expected_shape(net, [64, 64, 3])\n\n                return net\n"""
models/wgan.py,35,"b""# coding: utf-8\r\nimport tensorflow as tf\r\nslim = tf.contrib.slim\r\nfrom utils import expected_shape\r\nimport ops\r\nfrom basemodel import BaseModel\r\n\r\n'''\r\nbased on DCGAN.\r\n\r\nWGAN:\r\nWD = max_f [ Ex[f(x)] - Ez[f(g(z))] ] where f has K-Lipschitz constraint\r\nJ = min WD (G_loss)\r\n'''\r\n\r\nclass WGAN(BaseModel):\r\n    def __init__(self, name, training, D_lr=5e-5, G_lr=5e-5, image_shape=[64, 64, 3], z_dim=100):\r\n        self.ld = 10. # lambda\r\n        self.n_critic = 5\r\n        super(WGAN, self).__init__(name=name, training=training, D_lr=D_lr, G_lr=G_lr, \r\n            image_shape=image_shape, z_dim=z_dim)\r\n\r\n    def _build_train_graph(self):\r\n        with tf.variable_scope(self.name):\r\n            X = tf.placeholder(tf.float32, [None] + self.shape)\r\n            z = tf.placeholder(tf.float32, [None, self.z_dim])\r\n            global_step = tf.Variable(0, name='global_step', trainable=False)\r\n\r\n            G = self._generator(z)\r\n            C_real = self._critic(X)\r\n            C_fake = self._critic(G, reuse=True)\r\n\r\n            W_dist = tf.reduce_mean(C_real - C_fake)\r\n            C_loss = -W_dist\r\n            G_loss = tf.reduce_mean(-C_fake)\r\n\r\n            C_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/critic/')\r\n            G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/generator/')\r\n\r\n            C_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/critic/')\r\n            G_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/generator/')\r\n\r\n            # In the paper, critic networks has been trained n_critic times for each training step.\r\n            # Here I adjust learning rate instead.\r\n            with tf.control_dependencies(C_update_ops):\r\n                C_train_op = tf.train.RMSPropOptimizer(learning_rate=self.D_lr*self.n_critic).\\\r\n                    minimize(C_loss, var_list=C_vars)\r\n            with tf.control_dependencies(G_update_ops):\r\n                G_train_op = tf.train.RMSPropOptimizer(learning_rate=self.G_lr).\\\r\n                    minimize(G_loss, var_list=G_vars, global_step=global_step)\r\n\r\n            # weight clipping\r\n            ''' It is right that clips gamma of the batch_norm? '''\r\n            \r\n            # ver 1. clips all variables in critic\r\n            C_clips = [tf.assign(var, tf.clip_by_value(var, -0.01, 0.01)) for var in C_vars] # with gamma\r\n\r\n            # ver 2. does not work\r\n            # C_clips = [tf.assign(var, tf.clip_by_value(var, -0.01, 0.01)) for var in C_vars if 'gamma' not in var.op.name] # without gamma\r\n\r\n            # ver 3. works but strange\r\n            # C_clips = []\r\n            # for var in C_vars:\r\n            #     if 'gamma' not in var.op.name:\r\n            #         C_clips.append(tf.assign(var, tf.clip_by_value(var, -0.01, 0.01)))\r\n            #     else:\r\n            #         C_clips.append(tf.assign(var, tf.clip_by_value(var, -1.00, 1.00)))\r\n\r\n            with tf.control_dependencies([C_train_op]): # should be iterable\r\n                C_train_op = tf.tuple(C_clips) # tf.group ?\r\n\r\n            # summaries\r\n            # per-step summary\r\n            self.summary_op = tf.summary.merge([\r\n                tf.summary.scalar('G_loss', G_loss),\r\n                tf.summary.scalar('C_loss', C_loss),\r\n                tf.summary.scalar('W_dist', W_dist)\r\n            ])\r\n\r\n            # sparse-step summary\r\n            tf.summary.image('fake_sample', G, max_outputs=self.FAKE_MAX_OUTPUT)\r\n            # tf.summary.histogram('real_probs', D_real_prob)\r\n            # tf.summary.histogram('fake_probs', D_fake_prob)\r\n            self.all_summary_op = tf.summary.merge_all()\r\n\r\n            # accesible points\r\n            self.X = X\r\n            self.z = z\r\n            self.D_train_op = C_train_op # compatibility for train.py\r\n            self.G_train_op = G_train_op\r\n            self.fake_sample = G\r\n            self.global_step = global_step\r\n\r\n    def _critic(self, X, reuse=False):\r\n        ''' K-Lipschitz function '''\r\n        with tf.variable_scope('critic', reuse=reuse):\r\n            net = X\r\n            \r\n            with slim.arg_scope([slim.conv2d], kernel_size=[5,5], stride=2, activation_fn=ops.lrelu, \r\n                normalizer_fn=slim.batch_norm, normalizer_params=self.bn_params):\r\n                net = slim.conv2d(net, 64, normalizer_fn=None)\r\n                expected_shape(net, [32, 32, 64])\r\n                net = slim.conv2d(net, 128)\r\n                expected_shape(net, [16, 16, 128])\r\n                net = slim.conv2d(net, 256)\r\n                expected_shape(net, [8, 8, 256])\r\n                net = slim.conv2d(net, 512)\r\n                expected_shape(net, [4, 4, 512])\r\n\r\n            net = slim.flatten(net)\r\n            net = slim.fully_connected(net, 1, activation_fn=None)\r\n\r\n            return net\r\n\r\n    def _generator(self, z, reuse=False):\r\n        with tf.variable_scope('generator', reuse=reuse):\r\n            net = z\r\n            net = slim.fully_connected(net, 4*4*1024, activation_fn=tf.nn.relu)\r\n            net = tf.reshape(net, [-1, 4, 4, 1024])\r\n\r\n            with slim.arg_scope([slim.conv2d_transpose], kernel_size=[5,5], stride=2, activation_fn=tf.nn.relu, \r\n                normalizer_fn=slim.batch_norm, normalizer_params=self.bn_params):\r\n                net = slim.conv2d_transpose(net, 512)\r\n                expected_shape(net, [8, 8, 512])\r\n                net = slim.conv2d_transpose(net, 256)\r\n                expected_shape(net, [16, 16, 256])\r\n                net = slim.conv2d_transpose(net, 128)\r\n                expected_shape(net, [32, 32, 128])\r\n                net = slim.conv2d_transpose(net, 3, activation_fn=tf.nn.tanh, normalizer_fn=None)\r\n                expected_shape(net, [64, 64, 3])\r\n\r\n                return net\r\n"""
models/wgan_gp.py,46,"b""# coding: utf-8\r\nimport tensorflow as tf\r\nslim = tf.contrib.slim\r\nfrom utils import expected_shape\r\nimport ops\r\nfrom basemodel import BaseModel\r\n\r\n'''\r\nWGAN:\r\nWD = max_f [ Ex[f(x)] - Ez[f(g(z))] ] where f has K-Lipschitz constraint\r\nJ = min WD (G_loss)\r\n\r\n+ GP:\r\nInstead of weight clipping, WGAN-GP proposed gradient penalty.\r\n'''\r\n\r\nclass WGAN_GP(BaseModel):\r\n    def __init__(self, name, training, D_lr=1e-4, G_lr=1e-4, image_shape=[64, 64, 3], z_dim=100):\r\n        self.beta1 = 0.0\r\n        self.beta2 = 0.9\r\n        self.ld = 10. # lambda\r\n        self.n_critic = 5\r\n        super(WGAN_GP, self).__init__(name=name, training=training, D_lr=D_lr, G_lr=G_lr, \r\n            image_shape=image_shape, z_dim=z_dim)\r\n\r\n    def _build_train_graph(self):\r\n        with tf.variable_scope(self.name):\r\n            X = tf.placeholder(tf.float32, [None] + self.shape)\r\n            z = tf.placeholder(tf.float32, [None, self.z_dim])\r\n            global_step = tf.Variable(0, name='global_step', trainable=False)\r\n\r\n            # `critic` named from wgan (wgan-gp use the term `discriminator` rather than `critic`)\r\n            G = self._generator(z)\r\n            C_real = self._critic(X)\r\n            C_fake = self._critic(G, reuse=True)\r\n\r\n            W_dist = tf.reduce_mean(C_real - C_fake)\r\n            C_loss = -W_dist\r\n            G_loss = tf.reduce_mean(-C_fake)\r\n\r\n            # Gradient Penalty (GP)\r\n            eps = tf.random_uniform(shape=[tf.shape(X)[0], 1, 1, 1], minval=0., maxval=1.)\r\n            x_hat = eps*X + (1.-eps)*G \r\n            C_xhat = self._critic(x_hat, reuse=True)\r\n            C_xhat_grad = tf.gradients(C_xhat, x_hat)[0] # gradient of D(x_hat)\r\n            C_xhat_grad_norm = tf.norm(slim.flatten(C_xhat_grad), axis=1)  # l2 norm\r\n            GP = self.ld * tf.reduce_mean(tf.square(C_xhat_grad_norm - 1.))\r\n            C_loss += GP\r\n\r\n            C_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/critic/')\r\n            G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name+'/generator/')\r\n\r\n            C_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/critic/')\r\n            G_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name+'/generator/')\r\n\r\n            n_critic = 5\r\n            lr = 1e-4\r\n            with tf.control_dependencies(C_update_ops):\r\n                C_train_op = tf.train.AdamOptimizer(learning_rate=self.D_lr*n_critic, beta1=self.beta1, beta2=self.beta2).\\\r\n                    minimize(C_loss, var_list=C_vars)\r\n            with tf.control_dependencies(G_update_ops):\r\n                G_train_op = tf.train.AdamOptimizer(learning_rate=self.G_lr, beta1=self.beta1, beta2=self.beta2).\\\r\n                    minimize(G_loss, var_list=G_vars, global_step=global_step)\r\n\r\n            # summaries\r\n            # per-step summary\r\n            self.summary_op = tf.summary.merge([\r\n                tf.summary.scalar('G_loss', G_loss),\r\n                tf.summary.scalar('C_loss', C_loss),\r\n                tf.summary.scalar('W_dist', W_dist),\r\n                tf.summary.scalar('GP', GP)\r\n            ])\r\n\r\n            # sparse-step summary\r\n            tf.summary.image('fake_sample', G, max_outputs=self.FAKE_MAX_OUTPUT)\r\n            # tf.summary.histogram('real_probs', D_real_prob)\r\n            # tf.summary.histogram('fake_probs', D_fake_prob)\r\n            self.all_summary_op = tf.summary.merge_all()\r\n\r\n            # accesible points\r\n            self.X = X\r\n            self.z = z\r\n            self.D_train_op = C_train_op # train.py \xec\x99\x80\xec\x9d\x98 accesibility \xeb\xa5\xbc \xec\x9c\x84\xed\x95\xb4... \xed\x9d\xa0... \xea\xb5\xac\xeb\xa6\xb0\xeb\x8d\xb0...\r\n            self.G_train_op = G_train_op\r\n            self.fake_sample = G\r\n            self.global_step = global_step\r\n\r\n    def _critic(self, X, reuse=False):\r\n        return self._good_critic(X, reuse)\r\n\r\n    def _generator(self, z, reuse=False):\r\n        return self._good_generator(z, reuse)\r\n\r\n    def _dcgan_critic(self, X, reuse=False):\r\n        '''\r\n        K-Lipschitz function.\r\n        WGAN-GP does not use critic in batch norm.\r\n        '''\r\n        with tf.variable_scope('critic', reuse=reuse):\r\n            net = X\r\n            \r\n            with slim.arg_scope([slim.conv2d], kernel_size=[5,5], stride=2, padding='SAME', activation_fn=ops.lrelu):\r\n                net = slim.conv2d(net, 64)\r\n                expected_shape(net, [32, 32, 64])\r\n                net = slim.conv2d(net, 128)\r\n                expected_shape(net, [16, 16, 128])\r\n                net = slim.conv2d(net, 256)\r\n                expected_shape(net, [8, 8, 256])\r\n                net = slim.conv2d(net, 512)\r\n                expected_shape(net, [4, 4, 512])\r\n\r\n            net = slim.flatten(net)\r\n            net = slim.fully_connected(net, 1, activation_fn=None)\r\n\r\n            return net\r\n\r\n    def _dcgan_generator(self, z, reuse=False):\r\n        with tf.variable_scope('generator', reuse=reuse):\r\n            net = z\r\n            net = slim.fully_connected(net, 4*4*1024, activation_fn=tf.nn.relu)\r\n            net = tf.reshape(net, [-1, 4, 4, 1024])\r\n\r\n            with slim.arg_scope([slim.conv2d_transpose], kernel_size=[5,5], stride=2, activation_fn=tf.nn.relu, \r\n                normalizer_fn=slim.batch_norm, normalizer_params=self.bn_params):\r\n                net = slim.conv2d_transpose(net, 512)\r\n                expected_shape(net, [8, 8, 512])\r\n                net = slim.conv2d_transpose(net, 256)\r\n                expected_shape(net, [16, 16, 256])\r\n                net = slim.conv2d_transpose(net, 128)\r\n                expected_shape(net, [32, 32, 128])\r\n                net = slim.conv2d_transpose(net, 3, activation_fn=tf.nn.tanh, normalizer_fn=None)\r\n                expected_shape(net, [64, 64, 3])\r\n\r\n                return net\r\n\r\n    '''\r\n    ResNet architecture from appendix C in the paper.\r\n    https://github.com/igul222/improved_wgan_training/blob/master/gan_64x64.py - GoodGenerator / GoodDiscriminator\r\n    layer norm in D, batch norm in G.\r\n    some details are ignored in this implemenation.\r\n    '''\r\n    def _residual_block(self, X, nf_output, resample, kernel_size=[3,3], name='res_block'):\r\n        with tf.variable_scope(name):\r\n            input_shape = X.shape\r\n            nf_input = input_shape[-1]\r\n            if resample == 'down': # Downsample\r\n                shortcut = slim.avg_pool2d(X, [2,2])\r\n                shortcut = slim.conv2d(shortcut, nf_output, kernel_size=[1,1], activation_fn=None) # init xavier\r\n\r\n                net = slim.layer_norm(X, activation_fn=tf.nn.relu)\r\n                net = slim.conv2d(net, nf_input, kernel_size=kernel_size, biases_initializer=None) # skip bias\r\n                net = slim.layer_norm(net, activation_fn=tf.nn.relu)\r\n                net = slim.conv2d(net, nf_output, kernel_size=kernel_size)\r\n                net = slim.avg_pool2d(net, [2,2])\r\n\r\n                return net + shortcut\r\n            elif resample == 'up': # Upsample\r\n                upsample_shape = map(lambda x: int(x)*2, input_shape[1:3])\r\n                shortcut = tf.image.resize_nearest_neighbor(X, upsample_shape) \r\n                shortcut = slim.conv2d(shortcut, nf_output, kernel_size=[1,1], activation_fn=None)\r\n\r\n                net = slim.batch_norm(X, activation_fn=tf.nn.relu, **self.bn_params)\r\n                net = tf.image.resize_nearest_neighbor(net, upsample_shape) \r\n                net = slim.conv2d(net, nf_output, kernel_size=kernel_size, biases_initializer=None) # skip bias\r\n                net = slim.batch_norm(net, activation_fn=tf.nn.relu, **self.bn_params)\r\n                net = slim.conv2d(net, nf_output, kernel_size=kernel_size)\r\n\r\n                return net + shortcut\r\n            else:\r\n                raise Exception('invalid resample value')\r\n\r\n    def _good_generator(self, z, reuse=False):\r\n        with tf.variable_scope('generator', reuse=reuse):\r\n            nf = 64\r\n            net = slim.fully_connected(z, 4*4*8*nf, activation_fn=None) # 4x4x512\r\n            net = tf.reshape(net, [-1, 4, 4, 8*nf])\r\n            net = self._residual_block(net, 8*nf, resample='up', name='res_block1') # 8x8x512\r\n            net = self._residual_block(net, 4*nf, resample='up', name='res_block2') # 16x16x256\r\n            net = self._residual_block(net, 2*nf, resample='up', name='res_block3') # 32x32x128\r\n            net = self._residual_block(net, 1*nf, resample='up', name='res_block4') # 64x64x64\r\n            expected_shape(net, [64, 64, 64])\r\n            net = slim.batch_norm(net, activation_fn=tf.nn.relu, **self.bn_params)\r\n            net = slim.conv2d(net, 3, kernel_size=[3,3], activation_fn=tf.nn.tanh)\r\n            expected_shape(net, [64, 64, 3])\r\n\r\n            return net\r\n\r\n    def _good_critic(self, X, reuse=False):\r\n        with tf.variable_scope('critic', reuse=reuse):\r\n            nf = 64\r\n            net = slim.conv2d(X, nf, [3,3], activation_fn=None) # 64x64x64\r\n            net = self._residual_block(net, 2*nf, resample='down', name='res_block1') # 32x32x128\r\n            net = self._residual_block(net, 4*nf, resample='down', name='res_block2') # 16x16x256\r\n            net = self._residual_block(net, 8*nf, resample='down', name='res_block3') # 8x8x512\r\n            net = self._residual_block(net, 8*nf, resample='down', name='res_block4') # 4x4x512\r\n            expected_shape(net, [4, 4, 512])\r\n            net = slim.flatten(net)\r\n            net = slim.fully_connected(net, 1, activation_fn=None)\r\n\r\n            return net\r\n"""
