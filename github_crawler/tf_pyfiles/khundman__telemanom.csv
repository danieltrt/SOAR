file_path,api_count,code
example.py,0,"b""from telemanom.detector import Detector\nimport argparse\n\nparser = argparse.ArgumentParser(description='Parse path to anomaly labels if provided.')\nparser.add_argument('-l', '--labels_path', default=None, required=False)\nargs = parser.parse_args()\n\nif __name__ == '__main__':\n    detector = Detector(labels_path=args.labels_path)\n    detector.run()"""
telemanom/__init__.py,0,b''
telemanom/channel.py,0,"b'import numpy as np\nimport os\nimport logging\n\nlogger = logging.getLogger(\'telemanom\')\n\n\nclass Channel:\n    def __init__(self, config, chan_id):\n        """"""\n        Load and reshape channel values (predicted and actual).\n\n        Args:\n            config (obj): Config object containing parameters for processing\n            chan_id (str): channel id\n\n        Attributes:\n            id (str): channel id\n            config (obj): see Args\n            X_train (arr): training inputs with dimensions\n                [timesteps, l_s, input dimensions)\n            X_test (arr): test inputs with dimensions\n                [timesteps, l_s, input dimensions)\n            y_train (arr): actual channel training values with dimensions\n                [timesteps, n_predictions, 1)\n            y_test (arr): actual channel test values with dimensions\n                [timesteps, n_predictions, 1)\n            train (arr): train data loaded from .npy file\n            test(arr): test data loaded from .npy file\n        """"""\n\n        self.id = chan_id\n        self.config = config\n        self.X_train = None\n        self.y_train = None\n        self.X_test = None\n        self.y_test = None\n        self.y_hat = None\n        self.train = None\n        self.test = None\n\n    def shape_data(self, arr, train=True):\n        """"""Shape raw input streams for ingestion into LSTM. config.l_s specifies\n        the sequence length of prior timesteps fed into the model at\n        each timestep t.\n\n        Args:\n            arr (np array): array of input streams with\n                dimensions [timesteps, 1, input dimensions]\n            train (bool): If shaping training data, this indicates\n                data can be shuffled\n        """"""\n\n        data = []\n        for i in range(len(arr) - self.config.l_s - self.config.n_predictions):\n            data.append(arr[i:i + self.config.l_s + self.config.n_predictions])\n        data = np.array(data)\n\n        assert len(data.shape) == 3\n\n        if train:\n            np.random.shuffle(data)\n            self.X_train = data[:, :-self.config.n_predictions, :]\n            self.y_train = data[:, -self.config.n_predictions:, 0]  # telemetry value is at position 0\n        else:\n            self.X_test = data[:, :-self.config.n_predictions, :]\n            self.y_test = data[:, -self.config.n_predictions:, 0]  # telemetry value is at position 0\n\n    def load_data(self):\n        """"""\n        Load train and test data from local.\n        """"""\n        try:\n            self.train = np.load(os.path.join(""data"", ""train"", ""{}.npy"".format(self.id)))\n            self.test = np.load(os.path.join(""data"", ""test"", ""{}.npy"".format(self.id)))\n\n        except FileNotFoundError as e:\n            logger.critical(e)\n            logger.critical(""Source data not found, may need to add data to repo: <link>"")\n\n        self.shape_data(self.train)\n        self.shape_data(self.test, train=False)'"
telemanom/detector.py,0,"b'import os\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime as dt\nimport logging\n\nfrom telemanom.helpers import Config\nfrom telemanom.errors import Errors\nimport telemanom.helpers as helpers\nfrom telemanom.channel import Channel\nfrom telemanom.modeling import Model\n\nlogger = helpers.setup_logging()\n\nclass Detector:\n    def __init__(self, labels_path=None, result_path=\'results/\',\n                 config_path=\'config.yaml\'):\n        """"""\n        Top-level class for running anomaly detection over a group of channels\n        with values stored in .npy files. Also evaluates performance against a\n        set of labels if provided.\n\n        Args:\n            labels_path (str): path to .csv containing labeled anomaly ranges\n                for group of channels to be processed\n            result_path (str): directory indicating where to stick result .csv\n            config_path (str): path to config.yaml\n\n        Attributes:\n            labels_path (str): see Args\n            results (list of dicts): holds dicts of results for each channel\n            result_df (dataframe): results converted to pandas dataframe\n            chan_df (dataframe): holds all channel information from labels .csv\n            result_tracker (dict): if labels provided, holds results throughout\n                processing for logging\n            config (obj):  Channel class object containing train/test data\n                for X,y for a single channel\n            y_hat (arr): predicted channel values\n            id (str): datetime id for tracking different runs\n            result_path (str): see Args\n        """"""\n\n        self.labels_path = labels_path\n        self.results = []\n        self.result_df = None\n        self.chan_df = None\n\n        self.result_tracker = {\n            \'true_positives\': 0,\n            \'false_positives\': 0,\n            \'false_negatives\': 0\n        }\n\n        self.config = Config(config_path)\n        self.y_hat = None\n\n        if not self.config.predict and self.config.use_id:\n            self.id = self.config.use_id\n        else:\n            self.id = dt.now().strftime(\'%Y-%m-%d_%H.%M.%S\')\n\n        helpers.make_dirs(self.id)\n\n        # add logging FileHandler based on ID\n        hdlr = logging.FileHandler(\'data/logs/%s.log\' % self.id)\n        formatter = logging.Formatter(\'%(asctime)s %(levelname)s %(message)s\')\n        hdlr.setFormatter(formatter)\n        logger.addHandler(hdlr)\n\n        self.result_path = result_path\n\n        if self.labels_path:\n            self.chan_df = pd.read_csv(labels_path)\n        else:\n            chan_ids = [x.split(\'.\')[0] for x in os.listdir(\'data/test/\')]\n            self.chan_df = pd.DataFrame({""chan_id"": chan_ids})\n\n        logger.info(""{} channels found for processing.""\n                    .format(len(self.chan_df)))\n\n    def evaluate_sequences(self, errors, label_row):\n        """"""\n        Compare identified anomalous sequences with labeled anomalous sequences.\n\n        Args:\n            errors (obj): Errors class object containing detected anomaly\n                sequences for a channel\n            label_row (pandas Series): Contains labels and true anomaly details\n                for a channel\n\n        Returns:\n            result_row (dict): anomaly detection accuracy and results\n        """"""\n\n        result_row = {\n            \'false_positives\': 0,\n            \'false_negatives\': 0,\n            \'true_positives\': 0,\n            \'fp_sequences\': [],\n            \'tp_sequences\': [],\n            \'num_true_anoms\': 0\n        }\n\n        matched_true_seqs = []\n\n        label_row[\'anomaly_sequences\'] = eval(label_row[\'anomaly_sequences\'])\n        result_row[\'num_true_anoms\'] += len(label_row[\'anomaly_sequences\'])\n        result_row[\'scores\'] = errors.anom_scores\n\n        if len(errors.E_seq) == 0:\n            result_row[\'false_negatives\'] = result_row[\'num_true_anoms\']\n\n        else:\n            true_indices_grouped = [list(range(e[0], e[1]+1)) for e in label_row[\'anomaly_sequences\']]\n            true_indices_flat = set([i for group in true_indices_grouped for i in group])\n\n            for e_seq in errors.E_seq:\n                i_anom_predicted = set(range(e_seq[0], e_seq[1]+1))\n\n                matched_indices = list(i_anom_predicted & true_indices_flat)\n                valid = True if len(matched_indices) > 0 else False\n\n                if valid:\n\n                    result_row[\'tp_sequences\'].append(e_seq)\n\n                    true_seq_index = [i for i in range(len(true_indices_grouped)) if\n                                      len(np.intersect1d(list(i_anom_predicted), true_indices_grouped[i])) > 0]\n\n                    if not true_seq_index[0] in matched_true_seqs:\n                        matched_true_seqs.append(true_seq_index[0])\n                        result_row[\'true_positives\'] += 1\n\n                else:\n                    result_row[\'fp_sequences\'].append([e_seq[0], e_seq[1]])\n                    result_row[\'false_positives\'] += 1\n\n            result_row[""false_negatives""] = len(np.delete(label_row[\'anomaly_sequences\'],\n                                                          matched_true_seqs, axis=0))\n\n        logger.info(\'Channel Stats: TP: {}  FP: {}  FN: {}\'.format(result_row[\'true_positives\'],\n                                                                   result_row[\'false_positives\'],\n                                                                   result_row[\'false_negatives\']))\n\n        for key, value in result_row.items():\n            if key in self.result_tracker:\n                self.result_tracker[key] += result_row[key]\n\n        return result_row\n\n    def log_final_stats(self):\n        """"""\n        Log final stats at end of experiment.\n        """"""\n\n        if self.labels_path:\n\n            logger.info(\'Final Totals:\')\n            logger.info(\'-----------------\')\n            logger.info(\'True Positives: {}\'\n                        .format(self.result_tracker[\'true_positives\']))\n            logger.info(\'False Positives: {}\'\n                        .format(self.result_tracker[\'false_positives\']))\n            logger.info(\'False Negatives: {}\\n\'\n                        .format(self.result_tracker[\'false_negatives\']))\n            try:\n                logger.info(\'Precision: {0:.2f}\'\n                            .format(float(self.result_tracker[\'true_positives\'])\n                                    / float(self.result_tracker[\'true_positives\']\n                                            + self.result_tracker[\'false_positives\'])))\n                logger.info(\'Recall: {0:.2f}\'\n                            .format(float(self.result_tracker[\'true_positives\'])\n                                    / float(self.result_tracker[\'true_positives\']\n                                            + self.result_tracker[\'false_negatives\'])))\n            except ZeroDivisionError:\n                logger.info(\'Precision: NaN\')\n                logger.info(\'Recall: NaN\')\n\n        else:\n            logger.info(\'Final Totals:\')\n            logger.info(\'-----------------\')\n            logger.info(\'Total channel sets evaluated: {}\'\n                        .format(len(self.result_df)))\n            logger.info(\'Total anomalies found: {}\'\n                        .format(self.result_df[\'n_predicted_anoms\'].sum()))\n            logger.info(\'Avg normalized prediction error: {}\'\n                        .format(self.result_df[\'normalized_pred_error\'].mean()))\n            logger.info(\'Total number of values evaluated: {}\'\n                        .format(self.result_df[\'num_test_values\'].sum()))\n\n\n    def run(self):\n        """"""\n        Initiate processing for all channels.\n        """"""\n        for i, row in self.chan_df.iterrows():\n            logger.info(\'Stream # {}: {}\'.format(i+1, row.chan_id))\n            channel = Channel(self.config, row.chan_id)\n            channel.load_data()\n\n            if self.config.predict:\n                model = Model(self.config, self.id, channel)\n                channel = model.batch_predict(channel)\n            else:\n                channel.y_hat = np.load(os.path.join(\'data\', self.id, \'y_hat\',\n                                                     \'{}.npy\'\n                                                     .format(channel.id)))\n\n            errors = Errors(channel, self.config, self.id)\n            errors.process_batches(channel)\n\n            result_row = {\n                \'run_id\': self.id,\n                \'chan_id\': row.chan_id,\n                \'num_train_values\': len(channel.X_train),\n                \'num_test_values\': len(channel.X_test),\n                \'n_predicted_anoms\': len(errors.E_seq),\n                \'normalized_pred_error\': errors.normalized,\n                \'anom_scores\': errors.anom_scores\n            }\n\n            if self.labels_path:\n                result_row = {**result_row,\n                              **self.evaluate_sequences(errors, row)}\n                result_row[\'spacecraft\'] = row[\'spacecraft\']\n                result_row[\'anomaly_sequences\'] = row[\'anomaly_sequences\']\n                result_row[\'class\'] = row[\'class\']\n                self.results.append(result_row)\n\n                logger.info(\'Total true positives: {}\'\n                            .format(self.result_tracker[\'true_positives\']))\n                logger.info(\'Total false positives: {}\'\n                            .format(self.result_tracker[\'false_positives\']))\n                logger.info(\'Total false negatives: {}\\n\'\n                            .format(self.result_tracker[\'false_negatives\']))\n\n            else:\n                result_row[\'anomaly_sequences\'] = errors.E_seq\n                self.results.append(result_row)\n\n                logger.info(\'{} anomalies found\'\n                            .format(result_row[\'n_predicted_anoms\']))\n                logger.info(\'anomaly sequences start/end indices: {}\'\n                            .format(result_row[\'anomaly_sequences\']))\n                logger.info(\'number of test values: {}\'\n                            .format(result_row[\'num_test_values\']))\n                logger.info(\'anomaly scores: {}\\n\'\n                            .format(result_row[\'anom_scores\']))\n\n            self.result_df = pd.DataFrame(self.results)\n            self.result_df.to_csv(\n                os.path.join(self.result_path, \'{}.csv\'.format(self.id)),\n                index=False)\n\n        self.log_final_stats()'"
telemanom/errors.py,0,"b'import numpy as np\nimport pandas as pd\nimport more_itertools as mit\nimport os\nimport logging\n\nlogger = logging.getLogger(\'telemanom\')\n\n\nclass Errors:\n    def __init__(self, channel, config, run_id):\n        """"""\n        Batch processing of errors between actual and predicted values\n        for a channel.\n\n        Args:\n            channel (obj): Channel class object containing train/test data\n                for X,y for a single channel\n            config (obj): Config object containing parameters for processing\n            run_id (str): Datetime referencing set of predictions in use\n\n        Attributes:\n            config (obj): see Args\n            window_size (int): number of trailing batches to use in error\n                calculation\n            n_windows (int): number of windows in test values for channel\n            i_anom (arr): indices of anomalies in channel test values\n            E_seq (arr of tuples): array of (start, end) indices for each\n                continuous anomaly sequence in test values\n            anom_scores (arr): score indicating relative severity of each\n                anomaly sequence in E_seq\n            e (arr): errors in prediction (predicted - actual)\n            e_s (arr): exponentially-smoothed errors in prediction\n            normalized (arr): prediction errors as a percentage of the range\n                of the channel values\n        """"""\n\n        self.config = config\n        self.window_size = self.config.window_size\n        self.n_windows = int((channel.y_test.shape[0] -\n                              (self.config.batch_size * self.window_size))\n                             / self.config.batch_size)\n        self.i_anom = np.array([])\n        self.E_seq = []\n        self.anom_scores = []\n\n        # raw prediction error\n        self.e = [abs(y_h-y_t[0]) for y_h, y_t in\n                  zip(channel.y_hat, channel.y_test)]\n\n        smoothing_window = int(self.config.batch_size * self.config.window_size\n                               * self.config.smoothing_perc)\n        if not len(channel.y_hat) == len(channel.y_test):\n            raise ValueError(\'len(y_hat) != len(y_test): {}, {}\'\n                             .format(len(channel.y_hat), len(channel.y_test)))\n\n        # smoothed prediction error\n        self.e_s = pd.DataFrame(self.e).ewm(span=smoothing_window)\\\n            .mean().values.flatten()\n\n        # for values at beginning < sequence length, just use avg\n        if not channel.id == \'C-2\':  # anomaly occurs early in window\n            self.e_s[:self.config.l_s] = \\\n                [np.mean(self.e_s[:self.config.l_s * 2])] * self.config.l_s\n\n        np.save(os.path.join(\'data\', run_id, \'smoothed_errors\', \'{}.npy\'\n                             .format(channel.id)),\n                np.array(self.e_s))\n\n        self.normalized = np.mean(self.e / np.ptp(channel.y_test))\n        logger.info(""normalized prediction error: {0:.2f}""\n                    .format(self.normalized))\n\n    def adjust_window_size(self, channel):\n        """"""\n        Decrease the historical error window size (h) if number of test\n        values is limited.\n\n        Args:\n            channel (obj): Channel class object containing train/test data\n                for X,y for a single channel\n        """"""\n\n        while self.n_windows < 0:\n            self.window_size -= 1\n            self.n_windows = int((channel.y_test.shape[0]\n                                 - (self.config.batch_size * self.window_size))\n                                 / self.config.batch_size)\n            if self.window_size == 1 and self.n_windows < 0:\n                raise ValueError(\'Batch_size ({}) larger than y_test (len={}). \'\n                                 \'Adjust in config.yaml.\'\n                                 .format(self.config.batch_size,\n                                         channel.y_test.shape[0]))\n\n    def merge_scores(self):\n        """"""\n        If anomalous sequences from subsequent batches are adjacent they\n        will automatically be combined. This combines the scores for these\n        initial adjacent sequences (scores are calculated as each batch is\n        processed) where applicable.\n        """"""\n\n        merged_scores = []\n        score_end_indices = []\n\n        for i, score in enumerate(self.anom_scores):\n            if not score[\'start_idx\']-1 in score_end_indices:\n                merged_scores.append(score[\'score\'])\n                score_end_indices.append(score[\'end_idx\'])\n\n    def process_batches(self, channel):\n        """"""\n        Top-level function for the Error class that loops through batches\n        of values for a channel.\n\n        Args:\n            channel (obj): Channel class object containing train/test data\n                for X,y for a single channel\n        """"""\n\n        self.adjust_window_size(channel)\n\n        for i in range(0, self.n_windows+1):\n            prior_idx = i * self.config.batch_size\n            idx = (self.config.window_size * self.config.batch_size) \\\n                  + (i * self.config.batch_size)\n            if i == self.n_windows:\n                idx = channel.y_test.shape[0]\n\n            window = ErrorWindow(channel, self.config, prior_idx, idx, self, i)\n\n            window.find_epsilon()\n            window.find_epsilon(inverse=True)\n\n            window.compare_to_epsilon(self)\n            window.compare_to_epsilon(self, inverse=True)\n\n            if len(window.i_anom) == 0 and len(window.i_anom_inv) == 0:\n                continue\n\n            window.prune_anoms()\n            window.prune_anoms(inverse=True)\n\n            if len(window.i_anom) == 0 and len(window.i_anom_inv) == 0:\n                continue\n\n            window.i_anom = np.sort(np.unique(\n                np.append(window.i_anom, window.i_anom_inv))).astype(\'int\')\n            window.score_anomalies(prior_idx)\n\n            # update indices to reflect true indices in full set of values\n            self.i_anom = np.append(self.i_anom, window.i_anom + prior_idx)\n            self.anom_scores = self.anom_scores + window.anom_scores\n\n        if len(self.i_anom) > 0:\n            # group anomalous indices into continuous sequences\n            groups = [list(group) for group in\n                      mit.consecutive_groups(self.i_anom)]\n            self.E_seq = [(int(g[0]), int(g[-1])) for g in groups\n                          if not g[0] == g[-1]]\n\n            # additional shift is applied to indices so that they represent the\n            # position in the original data array, obtained from the .npy files,\n            # and not the position on y_test (See PR #27).\n            self.E_seq = [(e_seq[0] + self.config.l_s,\n                           e_seq[1] + self.config.l_s) for e_seq in self.E_seq]\n\n            self.merge_scores()\n\n\nclass ErrorWindow:\n    def __init__(self, channel, config, start_idx, end_idx, errors, window_num):\n        """"""\n        Data and calculations for a specific window of prediction errors.\n        Includes finding thresholds, pruning, and scoring anomalous sequences\n        for errors and inverted errors (flipped around mean) - significant drops\n        in values can also be anomalous.\n\n        Args:\n            channel (obj): Channel class object containing train/test data\n                for X,y for a single channel\n            config (obj): Config object containing parameters for processing\n            start_idx (int): Starting index for window within full set of\n                channel test values\n            end_idx (int): Ending index for window within full set of channel\n                test values\n            errors (arr): Errors class object\n            window_num (int): Current window number within channel test values\n\n        Attributes:\n            i_anom (arr): indices of anomalies in window\n            i_anom_inv (arr): indices of anomalies in window of inverted\n                telemetry values\n            E_seq (arr of tuples): array of (start, end) indices for each\n                continuous anomaly sequence in window\n            E_seq_inv (arr of tuples): array of (start, end) indices for each\n                continuous anomaly sequence in window of inverted telemetry\n                values\n            non_anom_max (float): highest smoothed error value below epsilon\n            non_anom_max_inv (float): highest smoothed error value below\n                epsilon_inv\n            config (obj): see Args\n            anom_scores (arr): score indicating relative severity of each\n                anomaly sequence in E_seq within a window\n            window_num (int): see Args\n            sd_lim (int): default number of standard deviations to use for\n                threshold if no winner or too many anomalous ranges when scoring\n                candidate thresholds\n            sd_threshold (float): number of standard deviations for calculation\n                of best anomaly threshold\n            sd_threshold_inv (float): same as above for inverted channel values\n            e_s (arr): exponentially-smoothed prediction errors in window\n            e_s_inv (arr): inverted e_s\n            sd_e_s (float): standard deviation of e_s\n            mean_e_s (float): mean of e_s\n            epsilon (float): threshold for e_s above which an error is\n                considered anomalous\n            epsilon_inv (float): threshold for inverted e_s above which an error\n                is considered anomalous\n            y_test (arr): Actual telemetry values for window\n            sd_values (float): st dev of y_test\n            perc_high (float): the 95th percentile of y_test values\n            perc_low (float): the 5th percentile of y_test values\n            inter_range (float): the range between perc_high - perc_low\n            num_to_ignore (int): number of values to ignore initially when\n                looking for anomalies\n        """"""\n\n        self.i_anom = np.array([])\n        self.E_seq = np.array([])\n        self.non_anom_max = -1000000\n        self.i_anom_inv = np.array([])\n        self.E_seq_inv = np.array([])\n        self.non_anom_max_inv = -1000000\n\n        self.config = config\n        self.anom_scores = []\n\n        self.window_num = window_num\n\n        self.sd_lim = 12.0\n        self.sd_threshold = self.sd_lim\n        self.sd_threshold_inv = self.sd_lim\n\n        self.e_s = errors.e_s[start_idx:end_idx]\n\n        self.mean_e_s = np.mean(self.e_s)\n        self.sd_e_s = np.std(self.e_s)\n        self.e_s_inv = np.array([self.mean_e_s + (self.mean_e_s - e)\n                                 for e in self.e_s])\n\n        self.epsilon = self.mean_e_s + self.sd_lim * self.sd_e_s\n        self.epsilon_inv = self.mean_e_s + self.sd_lim * self.sd_e_s\n\n        self.y_test = channel.y_test[start_idx:end_idx]\n        self.sd_values = np.std(self.y_test)\n\n        self.perc_high, self.perc_low = np.percentile(self.y_test, [95, 5])\n        self.inter_range = self.perc_high - self.perc_low\n\n        # ignore initial error values until enough history for processing\n        self.num_to_ignore = self.config.l_s * 2\n        # if y_test is small, ignore fewer\n        if len(channel.y_test) < 2500:\n            self.num_to_ignore = self.config.l_s\n        if len(channel.y_test) < 1800:\n            self.num_to_ignore = 0\n\n    def find_epsilon(self, inverse=False):\n        """"""\n        Find the anomaly threshold that maximizes function representing\n        tradeoff between:\n            a) number of anomalies and anomalous ranges\n            b) the reduction in mean and st dev if anomalous points are removed\n            from errors\n        (see https://arxiv.org/pdf/1802.04431.pdf)\n\n        Args:\n            inverse (bool): If true, epsilon is calculated for inverted errors\n        """"""\n        e_s = self.e_s if not inverse else self.e_s_inv\n\n        max_score = -10000000\n\n        for z in np.arange(2.5, self.sd_lim, 0.5):\n            epsilon = self.mean_e_s + (self.sd_e_s * z)\n\n            pruned_e_s = e_s[e_s < epsilon]\n\n            i_anom = np.argwhere(e_s >= epsilon).reshape(-1,)\n            buffer = np.arange(1, self.config.error_buffer)\n            i_anom = np.sort(np.concatenate((i_anom,\n                                            np.array([i+buffer for i in i_anom])\n                                             .flatten(),\n                                            np.array([i-buffer for i in i_anom])\n                                             .flatten())))\n            i_anom = i_anom[(i_anom < len(e_s)) & (i_anom >= 0)]\n            i_anom = np.sort(np.unique(i_anom))\n\n            if len(i_anom) > 0:\n                # group anomalous indices into continuous sequences\n                groups = [list(group) for group\n                          in mit.consecutive_groups(i_anom)]\n                E_seq = [(g[0], g[-1]) for g in groups if not g[0] == g[-1]]\n\n                mean_perc_decrease = (self.mean_e_s - np.mean(pruned_e_s)) \\\n                                     / self.mean_e_s\n                sd_perc_decrease = (self.sd_e_s - np.std(pruned_e_s)) \\\n                                   / self.sd_e_s\n                score = (mean_perc_decrease + sd_perc_decrease) \\\n                        / (len(E_seq) ** 2 + len(i_anom))\n\n                # sanity checks / guardrails\n                if score >= max_score and len(E_seq) <= 5 and \\\n                        len(i_anom) < (len(e_s) * 0.5):\n                    max_score = score\n                    if not inverse:\n                        self.sd_threshold = z\n                        self.epsilon = self.mean_e_s + z * self.sd_e_s\n                    else:\n                        self.sd_threshold_inv = z\n                        self.epsilon_inv = self.mean_e_s + z * self.sd_e_s\n\n    def compare_to_epsilon(self, errors_all, inverse=False):\n        """"""\n        Compare smoothed error values to epsilon (error threshold) and group\n        consecutive errors together into sequences.\n\n        Args:\n            errors_all (obj): Errors class object containing list of all\n            previously identified anomalies in test set\n        """"""\n\n        e_s = self.e_s if not inverse else self.e_s_inv\n        epsilon = self.epsilon if not inverse else self.epsilon_inv\n\n        # Check: scale of errors compared to values too small?\n        if not (self.sd_e_s > (.05 * self.sd_values) or max(self.e_s)\n                > (.05 * self.inter_range)) or not max(self.e_s) > 0.05:\n            return\n\n        i_anom = np.argwhere((e_s >= epsilon) &\n                             (e_s > 0.05 * self.inter_range)).reshape(-1,)\n\n        if len(i_anom) == 0:\n            return\n        buffer = np.arange(1, self.config.error_buffer+1)\n        i_anom = np.sort(np.concatenate((i_anom,\n                                         np.array([i + buffer for i in i_anom])\n                                         .flatten(),\n                                         np.array([i - buffer for i in i_anom])\n                                         .flatten())))\n        i_anom = i_anom[(i_anom < len(e_s)) & (i_anom >= 0)]\n\n        # if it is first window, ignore initial errors (need some history)\n        if self.window_num == 0:\n            i_anom = i_anom[i_anom >= self.num_to_ignore]\n        else:\n            i_anom = i_anom[i_anom >= len(e_s) - self.config.batch_size]\n\n        i_anom = np.sort(np.unique(i_anom))\n\n        # capture max of non-anomalous values below the threshold\n        # (used in filtering process)\n        batch_position = self.window_num * self.config.batch_size\n        window_indices = np.arange(0, len(e_s)) + batch_position\n        adj_i_anom = i_anom + batch_position\n        window_indices = np.setdiff1d(window_indices,\n                                      np.append(errors_all.i_anom, adj_i_anom))\n        candidate_indices = np.unique(window_indices - batch_position)\n        non_anom_max = np.max(np.take(e_s, candidate_indices))\n\n        # group anomalous indices into continuous sequences\n        groups = [list(group) for group in mit.consecutive_groups(i_anom)]\n        E_seq = [(g[0], g[-1]) for g in groups if not g[0] == g[-1]]\n\n        if inverse:\n            self.i_anom_inv = i_anom\n            self.E_seq_inv = E_seq\n            self.non_anom_max_inv = non_anom_max\n        else:\n            self.i_anom = i_anom\n            self.E_seq = E_seq\n            self.non_anom_max = non_anom_max\n\n    def prune_anoms(self, inverse=False):\n        """"""\n        Remove anomalies that don\'t meet minimum separation from the next\n        closest anomaly or error value\n\n        Args:\n            inverse (bool): If true, epsilon is calculated for inverted errors\n        """"""\n\n        E_seq = self.E_seq if not inverse else self.E_seq_inv\n        e_s = self.e_s if not inverse else self.e_s_inv\n        non_anom_max = self.non_anom_max if not inverse \\\n            else self.non_anom_max_inv\n\n        if len(E_seq) == 0:\n            return\n\n        E_seq_max = np.array([max(e_s[e[0]:e[1]+1]) for e in E_seq])\n        E_seq_max_sorted = np.sort(E_seq_max)[::-1]\n        E_seq_max_sorted = np.append(E_seq_max_sorted, [non_anom_max])\n\n        i_to_remove = np.array([])\n        for i in range(0, len(E_seq_max_sorted)-1):\n            if (E_seq_max_sorted[i] - E_seq_max_sorted[i+1]) \\\n                    / E_seq_max_sorted[i] < self.config.p:\n                i_to_remove = np.append(i_to_remove, np.argwhere(\n                    E_seq_max == E_seq_max_sorted[i]))\n            else:\n                i_to_remove = np.array([])\n        i_to_remove[::-1].sort()\n\n        if len(i_to_remove) > 0:\n            E_seq = np.delete(E_seq, i_to_remove, axis=0)\n\n        if len(E_seq) == 0 and inverse:\n            self.i_anom_inv = np.array([])\n            return\n        elif len(E_seq) == 0 and not inverse:\n            self.i_anom = np.array([])\n            return\n\n        indices_to_keep = np.concatenate([range(e_seq[0], e_seq[-1]+1)\n                                          for e_seq in E_seq])\n\n        if not inverse:\n            mask = np.isin(self.i_anom, indices_to_keep)\n            self.i_anom = self.i_anom[mask]\n        else:\n            mask_inv = np.isin(self.i_anom_inv, indices_to_keep)\n            self.i_anom_inv = self.i_anom_inv[mask_inv]\n\n    def score_anomalies(self, prior_idx):\n        """"""\n        Calculate anomaly scores based on max distance from epsilon\n        for each anomalous sequence.\n\n        Args:\n            prior_idx (int): starting index of window within full set of test\n                values for channel\n        """"""\n\n        groups = [list(group) for group in mit.consecutive_groups(self.i_anom)]\n\n        for e_seq in groups:\n\n            score_dict = {\n                ""start_idx"": e_seq[0] + prior_idx,\n                ""end_idx"": e_seq[-1] + prior_idx,\n                ""score"": 0\n            }\n\n            score = max([abs(self.e_s[i] - self.epsilon)\n                         / (self.mean_e_s + self.sd_e_s) for i in\n                         range(e_seq[0], e_seq[-1] + 1)])\n            inv_score = max([abs(self.e_s_inv[i] - self.epsilon_inv)\n                             / (self.mean_e_s + self.sd_e_s) for i in\n                             range(e_seq[0], e_seq[-1] + 1)])\n\n            # the max score indicates whether anomaly was from regular\n            # or inverted errors\n            score_dict[\'score\'] = max([score, inv_score])\n            self.anom_scores.append(score_dict)'"
telemanom/helpers.py,0,"b'import logging\nimport yaml\nimport json\nimport sys\nimport os\n\nlogger = logging.getLogger(\'telemanom\')\nsys.path.append(\'../telemanom\')\n\n\nclass Config:\n    """"""Loads parameters from config.yaml into global object\n\n    """"""\n\n    def __init__(self, path_to_config):\n\n        self.path_to_config = path_to_config\n\n        if os.path.isfile(path_to_config):\n            pass\n        else:\n            self.path_to_config = \'../{}\'.format(self.path_to_config)\n\n        with open(self.path_to_config, ""r"") as f:\n            self.dictionary = yaml.load(f.read(), Loader=yaml.FullLoader)\n\n        for k, v in self.dictionary.items():\n            setattr(self, k, v)\n\n    def build_group_lookup(self, path_to_groupings):\n\n        channel_group_lookup = {}\n\n        with open(path_to_groupings, ""r"") as f:\n            groupings = json.loads(f.read())\n\n            for subsystem in groupings.keys():\n                for subgroup in groupings[subsystem].keys():\n                    for chan in groupings[subsystem][subgroup]:\n                        channel_group_lookup[chan[""key""]] = {}\n                        channel_group_lookup[chan[""key""]][""subsystem""] = subsystem\n                        channel_group_lookup[chan[""key""]][""subgroup""] = subgroup\n\n        return channel_group_lookup\n\n\ndef make_dirs(_id):\n    \'\'\'Create directories for storing data in repo (using datetime ID) if they don\'t already exist\'\'\'\n\n    config = Config(""config.yaml"")\n\n    if not config.train or not config.predict:\n        if not os.path.isdir(\'data/%s\' %config.use_id):\n            raise ValueError(""Run ID {} is not valid. If loading prior models or predictions, must provide valid ID."".format(_id))\n\n    paths = [\'data\', \'data/%s\' %_id, \'data/logs\', \'data/%s/models\' %_id, \'data/%s/smoothed_errors\' %_id, \'data/%s/y_hat\' %_id]\n\n    for p in paths:\n        if not os.path.isdir(p):\n            os.mkdir(p)\n\ndef setup_logging():\n    \'\'\'Configure logging object to track parameter settings, training, and evaluation.\n    \n    Args:\n        config(obj): Global object specifying system runtime params.\n\n    Returns:\n        logger (obj): Logging object\n        _id (str): Unique identifier generated from datetime for storing data/models/results\n    \'\'\'\n\n    logger = logging.getLogger(\'telemanom\')\n    logger.setLevel(logging.INFO)\n\n    stdout = logging.StreamHandler(sys.stdout)\n    stdout.setLevel(logging.INFO)\n    logger.addHandler(stdout)\n\n    return logger'"
telemanom/modeling.py,0,"b'from keras.models import Sequential, load_model\nfrom keras.callbacks import History, EarlyStopping, Callback\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.core import Dense, Activation, Dropout\nimport numpy as np\nimport os\nimport logging\n\n# suppress tensorflow CPU speedup warnings\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nlogger = logging.getLogger(\'telemanom\')\n\n\nclass Model:\n    def __init__(self, config, run_id, channel):\n        """"""\n        Loads/trains RNN and predicts future telemetry values for a channel.\n\n        Args:\n            config (obj): Config object containing parameters for processing\n                and model training\n            run_id (str): Datetime referencing set of predictions in use\n            channel (obj): Channel class object containing train/test data\n                for X,y for a single channel\n\n        Attributes:\n            config (obj): see Args\n            chan_id (str): channel id\n            run_id (str): see Args\n            y_hat (arr): predicted channel values\n            model (obj): trained RNN model for predicting channel values\n        """"""\n\n        self.config = config\n        self.chan_id = channel.id\n        self.run_id = run_id\n        self.y_hat = np.array([])\n        self.model = None\n\n        if not self.config.train:\n            try:\n                self.load()\n            except FileNotFoundError:\n                path = os.path.join(\'data\', self.config.use_id, \'models\',\n                                    self.chan_id + \'.h5\')\n                logger.warning(\'Training new model, couldn\\\'t find existing \'\n                               \'model at {}\'.format(path))\n                self.train_new(channel)\n                self.save()\n        else:\n            self.train_new(channel)\n            self.save()\n\n    def load(self):\n        """"""\n        Load model for channel.\n        """"""\n\n        logger.info(\'Loading pre-trained model\')\n        self.model = load_model(os.path.join(\'data\', self.config.use_id,\n                                             \'models\', self.chan_id + \'.h5\'))\n\n    def train_new(self, channel):\n        """"""\n        Train LSTM model according to specifications in config.yaml.\n\n        Args:\n            channel (obj): Channel class object containing train/test data\n                for X,y for a single channel\n        """"""\n\n        cbs = [History(), EarlyStopping(monitor=\'val_loss\',\n                                        patience=self.config.patience,\n                                        min_delta=self.config.min_delta,\n                                        verbose=0)]\n\n        self.model = Sequential()\n\n        self.model.add(LSTM(\n            self.config.layers[0],\n            input_shape=(None, channel.X_train.shape[2]),\n            return_sequences=True))\n        self.model.add(Dropout(self.config.dropout))\n\n        self.model.add(LSTM(\n            self.config.layers[1],\n            return_sequences=False))\n        self.model.add(Dropout(self.config.dropout))\n\n        self.model.add(Dense(\n            self.config.n_predictions))\n        self.model.add(Activation(\'linear\'))\n\n        self.model.compile(loss=self.config.loss_metric,\n                           optimizer=self.config.optimizer)\n\n        self.model.fit(channel.X_train,\n                       channel.y_train,\n                       batch_size=self.config.lstm_batch_size,\n                       epochs=self.config.epochs,\n                       validation_split=self.config.validation_split,\n                       callbacks=cbs,\n                       verbose=True)\n\n    def save(self):\n        """"""\n        Save trained model.\n        """"""\n\n        self.model.save(os.path.join(\'data\', self.run_id, \'models\',\n                                     \'{}.h5\'.format(self.chan_id)))\n\n    def aggregate_predictions(self, y_hat_batch, method=\'first\'):\n        """"""\n        Aggregates predictions for each timestep. When predicting n steps\n        ahead where n > 1, will end up with multiple predictions for a\n        timestep.\n\n        Args:\n            y_hat_batch (arr): predictions shape (<batch length>, <n_preds)\n            method (string): indicates how to aggregate for a timestep - ""first""\n                or ""mean""\n        """"""\n\n        agg_y_hat_batch = np.array([])\n\n        for t in range(len(y_hat_batch)):\n\n            start_idx = t - self.config.n_predictions\n            start_idx = start_idx if start_idx >= 0 else 0\n\n            # predictions pertaining to a specific timestep lie along diagonal\n            y_hat_t = np.flipud(y_hat_batch[start_idx:t+1]).diagonal()\n\n            if method == \'first\':\n                agg_y_hat_batch = np.append(agg_y_hat_batch, [y_hat_t[0]])\n            elif method == \'mean\':\n                agg_y_hat_batch = np.append(agg_y_hat_batch, np.mean(y_hat_t))\n\n        agg_y_hat_batch = agg_y_hat_batch.reshape(len(agg_y_hat_batch), 1)\n        self.y_hat = np.append(self.y_hat, agg_y_hat_batch)\n\n    def batch_predict(self, channel):\n        """"""\n        Used trained LSTM model to predict test data arriving in batches.\n\n        Args:\n            channel (obj): Channel class object containing train/test data\n                for X,y for a single channel\n\n        Returns:\n            channel (obj): Channel class object with y_hat values as attribute\n        """"""\n\n        num_batches = int((channel.y_test.shape[0] - self.config.l_s)\n                          / self.config.batch_size)\n        if num_batches < 0:\n            raise ValueError(\'l_s ({}) too large for stream length {}.\'\n                             .format(self.config.l_s, channel.y_test.shape[0]))\n\n        # simulate data arriving in batches, predict each batch\n        for i in range(0, num_batches + 1):\n            prior_idx = i * self.config.batch_size\n            idx = (i + 1) * self.config.batch_size\n\n            if i + 1 == num_batches + 1:\n                # remaining values won\'t necessarily equal batch size\n                idx = channel.y_test.shape[0]\n\n            X_test_batch = channel.X_test[prior_idx:idx]\n            y_hat_batch = self.model.predict(X_test_batch)\n            self.aggregate_predictions(y_hat_batch)\n\n        self.y_hat = np.reshape(self.y_hat, (self.y_hat.size,))\n\n        channel.y_hat = self.y_hat\n\n        np.save(os.path.join(\'data\', self.run_id, \'y_hat\', \'{}.npy\'\n                             .format(self.chan_id)), self.y_hat)\n\n        return channel\n'"
telemanom/plotting.py,0,"b'import numpy as np\nimport os\nimport pandas as pd\nimport sys\nfrom telemanom.helpers import Config\nsys.path.append(\'..\')\n\nclass Plotter:\n    def __init__(self, run_id, config_path=\'config.yaml\'):\n        """"""\n        For plotting results in Jupyter notebook.\n\n        Args:\n            run_id (str): Datetime referencing set of predictions in use\n            config_path (str): path to config.yaml file\n\n        Attributes:\n            config (obj): Config object containing parameters for processing\n            run_id (str): see Args\n            result_df (dataframe): holds anomaly detection results for each\n                channel\n            labels_available (bool): True if labeled anomalous ranges provided\n                else False\n        """"""\n\n        self.config = Config(config_path)\n        self.run_id = run_id\n        self.result_df = pd.read_csv(os.path.join(\'..\', \'results\', \'{}.csv\'\n                                                  .format(self.run_id)))\n        self.labels_available = True if \'true_positives\' \\\n                                        in self.result_df.columns else False\n\n        self.plot_values = {}\n\n    def create_shapes(self, ranges, sequence_type, _min, _max, plot_values):\n        """"""\n        Create shapes for regions to highlight in plotly vizzes (true and\n        predicted anomaly sequences). Will plot labeled anomalous ranges if\n        available.\n\n        Args:\n            ranges (list of tuples): tuple of start and end indices for anomaly\n                sequences for a channel\n            sequence_type (str): ""predict"" if predicted values else\n                ""true"" if actual values. Determines colors.\n            _min (float): min y value of series\n            _max (float): max y value of series\n            plot_values (dict): dictionary of different series to be plotted\n                (predicted, actual, errors, training data)\n\n        Returns:\n            (dict) shape specifications for plotly\n        """"""\n\n        if not _max:\n            _max = max(plot_values[\'smoothed_errors\'])\n\n        color = \'red\' if sequence_type == \'true\' else \'blue\'\n        shapes = []\n\n        for r in ranges:\n            shape = {\n                \'type\': \'rect\',\n                \'x0\': r[0] - self.config.l_s,\n                \'y0\': _min,\n                \'x1\': r[1] - self.config.l_s,\n                \'y1\': _max,\n                \'fillcolor\': color,\n                \'opacity\': 0.2,\n                \'line\': {\n                    \'width\': 0,\n                }\n            }\n            shapes.append(shape)\n\n        return shapes\n\n    def all_result_summary(self):\n        """"""\n        Print aggregated results.\n        """"""\n\n        if self.labels_available:\n            print(\'True Positives: {}\'\n                  .format(self.result_df[\'true_positives\'].sum()))\n            print(\'False Positives: {}\'\n                  .format(self.result_df[\'false_positives\'].sum()))\n            print(\'False Negatives: {}\\n\'\n                  .format(self.result_df[\'false_negatives\'].sum()))\n            try:\n                print(\'Precision: {0:.2f}\'.format(\n                    float(self.result_df[\'true_positives\'].sum()) /\n                    float(self.result_df[\'true_positives\'].sum()\n                          + self.result_df[\'false_positives\'].sum())))\n                print(\'Recall: {0:.2f}\'.format(\n                    float(self.result_df[\'true_positives\'].sum()) /\n                    float(self.result_df[\'true_positives\'].sum()\n                        + self.result_df[\'false_negatives\'].sum())))\n            except ZeroDivisionError:\n                print(\'Precision: NaN\')\n                print(\'Recall: NaN\')\n        else:\n            print(\'Total channel sets evaluated: {}\'\n                  .format(len(self.result_df)))\n            print(\'Total anomalies found: {}\'\n                  .format(self.result_df[\'n_predicted_anoms\'].sum()))\n            print(\'Avg normalized prediction error: {}\'\n                  .format(self.result_df[\'normalized_pred_error\'].mean()))\n            print(\'Total number of values evaluated: {}\'\n                  .format(self.result_df[\'num_test_values\'].sum()))\n\n    def channel_result_summary(self, channel, plot_values):\n        """"""\n        Print results for a channel.\n\n        Args:\n            channel (obj): Channel class object containing train/test data\n                for X,y for a single channel\n            plot_values (dict): dictionary of different series to be plotted\n                (predicted, actual, errors, training data)\n        """"""\n\n        if \'spacecraft\' in channel:\n            print(\'Spacecraft: {}\'.format(channel[\'spacecraft\'].values[0]))\n\n        print(\'Channel: {}\'.format(channel[\'chan_id\'].values[0]))\n        print(\'Normalized prediction error: {0:.2f}\'\n              .format(float(channel[\'normalized_pred_error\'].values[0])))\n\n        if self.labels_available:\n            print(\'Anomaly class(es): {}\'.format(channel[\'class\'].values[0]))\n            print(""------------------"")\n            print(\'True Positives: {}\'\n                  .format(channel[\'true_positives\'].values[0]))\n            print(\'False Positives: {}\'\n                  .format(channel[\'false_positives\'].values[0]))\n            print(\'False Negatives: {}\'\n                  .format(channel[\'false_negatives\'].values[0]))\n            print(\'------------------\')\n        else:\n            print(\'Number of anomalies found: {}\'\n                  .format(channel[\'n_predicted_anoms\'].values[0]))\n            print(\'Anomaly sequences start/end indices: {}\'\n                  .format(channel[\'anomaly_sequences\'].values[0]))\n\n        print(\'Predicted anomaly scores: {}\'.format(channel[\'anom_scores\']\n                                                    .values[0]))\n        print(\'Number of values: {}\'.format(len(plot_values[\'test\'])))\n\n    def plot_channel(self, channel_id, plot_train=False, plot_errors=True):\n        """"""\n        Generate interactive plots for a channel. By default it prints actual\n        and predicted telemetry values.\n\n        Args:\n            channel_id (str): channel id\n            plot_train (bool): If true, plot training data in separate plot\n            plot_errors (bool): If true, plot prediction errors in separate plot\n        """"""\n        channel = self.result_df[self.result_df[\'chan_id\'] == channel_id]\n\n        plot_values = {\n            \'y_hat\': np.load(os.path.join(\'..\', \'data\', self.run_id, \'y_hat\',\n                                          \'{}.npy\'.format(channel_id))),\n            \'smoothed_errors\': np.load(os.path.join(\'..\', \'data\', self.run_id,\n                                                    \'smoothed_errors\',\n                                                    \'{}.npy\'.format(channel_id))),\n            \'test\': np.load(os.path.join(\'..\', \'data\', \'test\', \'{}.npy\'\n                                         .format(channel_id))),\n            \'train\': np.load(os.path.join(\'..\', \'data\', \'train\', \'{}.npy\'\n                                          .format(channel_id)))\n        }\n\n        self.channel_result_summary(channel, plot_values)\n\n        sequence_type = \'true\' if self.labels_available else \'predicted\'\n        y_shapes = self.create_shapes(eval(channel[\'anomaly_sequences\'].values[0]),\n                                      sequence_type, -1, 1, plot_values)\n        e_shapes = self.create_shapes(eval(channel[\'anomaly_sequences\'].values[0]),\n                                      sequence_type, 0, None, plot_values)\n\n        if self.labels_available:\n            y_shapes += self.create_shapes(eval(channel[\'tp_sequences\'].values[0])\n                                           + eval(channel[\'fp_sequences\'].values[0]),\n                                           \'predicted\', -1, 1, plot_values)\n            e_shapes += self.create_shapes(eval(channel[\'tp_sequences\'].values[0])\n                                           + eval(channel[\'fp_sequences\'].values[0]),\n                                           \'predicted\', 0, None, plot_values)\n\n        train_df = pd.DataFrame({\n            \'train\': plot_values[\'train\'][:,0]\n        })\n\n        y_df = pd.DataFrame({\n            \'y_hat\': plot_values[\'y_hat\'].reshape(-1,)\n        })\n\n        y = plot_values[\'test\'][self.config.l_s:-self.config.n_predictions][:,0]\n        y_df[\'y\'] = y\n        if not len(y) == len(plot_values[\'y_hat\']):\n            modified_l_s = len(plot_values[\'y_test\']) \\\n                           - len(plot_values[\'y_hat\']) - 1\n            y_df[\'y\'] = y[modified_l_s:-1]\n\n        e_df = pd.DataFrame({\n            \'e_s\': plot_values[\'smoothed_errors\'].reshape(-1,)\n        })\n\n        y_layout = {\n            \'title\': \'y / y_hat comparison\',\n            \'shapes\': y_shapes,\n        }\n\n        e_layout = {\n            \'title\': ""Smoothed Errors (e_s)"",\n            \'shapes\': e_shapes,\n        }\n\n        if plot_train:\n            train_df.iplot(kind=\'scatter\', color=\'green\',\n                           layout={\'title\': ""Training Data""})\n\n        y_df.iplot(kind=\'scatter\', layout=y_layout)\n\n        if plot_errors:\n            e_df.iplot(kind=\'scatter\', layout=e_layout, color=\'red\')\n\n    def plot_all(self, plot_train=False, plot_errors=True):\n        """"""\n        Loop through all channels and plot.\n\n        Args:\n            plot_train (bool): If true, plot training data in separate plot\n            plot_errors (bool): If true, plot prediction errors in separate plot\n        """"""\n\n        for idx, channel in self.result_df.iterrows():\n\n            self.plot_channel(channel[\'chan_id\'], plot_train=plot_train,\n                              plot_errors=plot_errors)\n\n\n\n\n\n\n'"
