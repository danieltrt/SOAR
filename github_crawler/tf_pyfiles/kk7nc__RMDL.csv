file_path,api_count,code
setup.py,0,"b""import io\nimport os\n\n\nimport numpy\nfrom setuptools import Extension\nfrom setuptools import setup, find_packages\nfrom os import path\n\n__author__ = 'Kamran Kowsari <kowsari.net>'\n\n\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\ndef readfile(file):\n    with open(path.join(here, file)) as f:\n        long_description = f.read()\n    return long_description\n\n\nsetup(\n    name='RMDL',\n    version='1.0.5',\n    description='RMDL: Random Multimodel Deep Learning for Classification',\n    long_description=readfile('README.rst'),\n    classifiers=[\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.4',\n        'Environment :: Console',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)',\n        'Operating System :: OS Independent',\n        'Topic :: Software Development :: Libraries',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'Topic :: Scientific/Engineering :: Information Analysis',\n        'Topic :: Scientific/Engineering :: Image Recognition',\n        'Topic :: Text Editors :: Text Processing',\n    ],\n    url='https://github.com/kk7nc/RMDL',\n    author='Kamran Kowsari',\n    author_email='kk7nc@virginia.edu',\n    install_requires=[\n        'matplotlib>=2.1.2',\n        'numpy>=1.12.1',\n        'pandas>=0.22.0',\n        'scipy',\n        'tensorflow',\n        'keras>=2.0.9',\n        'scikit-learn>=0.19.0',\n        'nltk>=3.2.4'\n    ],\n    packages=find_packages()\n)\n\n"""
Examples/CIFAR.py,0,"b'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\nRMDL: Random Multimodel Deep Learning for Classification\n\n * Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n * Last Update: May 3rd, 2018\n * This file is part of  RMDL project, University of Virginia.\n * Free to use, change, share and distribute source code of RMDL\n * Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n * Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n * Comments and Error: email: kk7nc@virginia.edu\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\n\n\nfrom keras.datasets import cifar10\nfrom RMDL import RMDL_Image as RMDL\n\nif __name__ == ""__main__"":\n    number_of_classes = 40\n    shape = (64, 64, 1)\n\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    x_train = x_train.astype(\'float32\')\n    x_test = x_test.astype(\'float32\')\n    x_train /= 255\n    x_test /= 255\n\n    batch_size = 100\n    sparse_categorical = 0\n    n_epochs = [500, 500, 500]  ## DNN--RNN-CNN\n    Random_Deep = [3, 0, 3]  ## DNN--RNN-CNN\n\n    RMDL.Image_Classification(x_train, y_train, x_test, y_test,(32,32,3),\n                              batch_size=batch_size,random_deep=Random_Deep,\n                              epochs=n_epochs)\n'"
Examples/IMDB.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\n\nfrom RMDL import text_feature_extraction as txt\nfrom keras.datasets import imdb\nimport numpy as np\nfrom RMDL import RMDL_Text as RMDL\n\nif __name__ == ""__main__"":\n    print(""Load IMDB dataset...."")\n    MAX_NB_WORDS = 75000\n    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=MAX_NB_WORDS)\n    print(len(X_train))\n    print(y_test)\n    word_index = imdb.get_word_index()\n    index_word = {v: k for k, v in word_index.items()}\n    X_train = [txt.text_cleaner(\' \'.join(index_word.get(w) for w in x)) for x in X_train]\n    X_test = [txt.text_cleaner(\' \'.join(index_word.get(w) for w in x)) for x in X_test]\n    X_train = np.array(X_train)\n    X_train = np.array(X_train).ravel()\n    print(X_train.shape)\n    X_test = np.array(X_test)\n    X_test = np.array(X_test).ravel()\n\n    batch_size = 100\n    sparse_categorical = 0\n    n_epochs = [500, 500, 500]  ## DNN--RNN-CNN\n    Random_Deep = [3, 3,3]  ## DNN--RNN-CNN\n\n    RMDL.Text_Classification(X_train, y_train, X_test, y_test,\n                             batch_size=batch_size,\n                             sparse_categorical=sparse_categorical,\n                             random_deep=Random_Deep,\n                             epochs=n_epochs)\n'"
Examples/LFW.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom RMDL import RMDL_Image as RMDL\n\nif __name__ == ""__main__"":\n\n    shape = (125, 94, 1)\n\n    lfw_people = fetch_lfw_people(min_faces_per_person=70,resize=1.0)\n\n    # introspect the images arrays to find the shapes (for plotting)\n    print(lfw_people.images.shape)\n\n    # for machine learning we use the 2 data directly (as relative pixel\n    # positions info is ignored by this model)\n    X = lfw_people.data\n    n_features = X.shape[1]\n    # the label to predict is the id of the person\n    y = lfw_people.target\n    target_names = lfw_people.target_names\n    n_classes = target_names.shape[0]\n    # #############################################################################\n    # Split into a training set and a test set using a stratified k fold\n\n    # split into a training and testing set\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.25, random_state=42)\n\n\n    X_train = X_train.reshape(X_train.shape[0], 125, 94, 1).astype(\'float32\')\n    X_test = X_test.reshape(X_test.shape[0], 125, 94, 1).astype(\'float32\')\n    number_of_classes = np.max(y_train)+1\n    X_train /= 255\n    X_test /= 255\n    batch_size = 100\n    sparse_categorical = 0\n    n_epochs = [5000, 500, 500]  ## DNN--RNN-CNN\n    Random_Deep = [3, 3, 3]  ## DNN--RNN-CNN\n    RMDL.Image_Classification(X_train, y_train, X_test, y_test,\n                              shape,\n                              batch_size=batch_size,\n                              random_deep=Random_Deep,\n                              epochs=n_epochs)'"
Examples/MNIST.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\nfrom keras.datasets import mnist\nimport numpy as np\n\n\nfrom RMDL import RMDL_Image as RMDL\n\nif __name__ == ""__main__"":\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n    X_train_D = X_train.reshape(X_train.shape[0], 28, 28, 1).astype(\'float32\')\n    X_test_D = X_test.reshape(X_test.shape[0], 28, 28, 1).astype(\'float32\')\n    X_train = X_train_D / 255.0\n    X_test = X_test_D / 255.0\n    number_of_classes = np.unique(y_train).shape[0]\n    shape = (28, 28, 1)\n    batch_size = 128\n    sparse_categorical = 0\n\n    n_epochs = [100, 100, 100]  ## DNN--RNN-CNN\n    Random_Deep = [0, 0, 3]  ## DNN--RNN-CNN\n    RMDL.Image_Classification(X_train, y_train, X_test, y_test,shape,\n                             batch_size=batch_size,\n                             sparse_categorical=True,\n                             random_deep=Random_Deep,\n                             epochs=n_epochs)\n'"
Examples/ORL.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.model_selection import train_test_split\nfrom RMDL import RMDL_Image as RMDL\n\nif __name__ == ""__main__"":\n    number_of_classes = 40\n    shape = (64, 64, 1)\n    data = fetch_olivetti_faces()\n    X_train, X_test, y_train, y_test = train_test_split(data.data,\n                                                    data.target, stratify=data.target, test_size=40)\n    X_train = X_train.reshape(X_train.shape[0], 64, 64, 1).astype(\'float32\')\n    X_test = X_test.reshape(X_test.shape[0], 64, 64, 1).astype(\'float32\')\n\n    batch_size = 100\n    sparse_categorical = 0\n    n_epochs = [150, 150, 150]  ## DNN--RNN-CNN\n    Random_Deep = [0, 0, 3]  ## DNN--RNN-CNN\n    RMDL.Image_Classification(X_train, y_train, X_test, y_test,\n                              shape,\n                              random_optimizor=False,\n                              batch_size=batch_size,\n                              random_deep=Random_Deep,\n                              epochs=n_epochs)'"
Examples/Reuters21578.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nimport sys\nimport os\nimport nltk\nnltk.download(""reuters"")\nfrom nltk.corpus import reuters\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport numpy as np\nfrom RMDL import RMDL_Text as RMDL\n\nif __name__ == ""__main__"":\n    documents = reuters.fileids()\n    train_docs_id = list(filter(lambda doc: doc.startswith(""train""),\n                                documents))\n    test_docs_id = list(filter(lambda doc: doc.startswith(""test""),\n                               documents))\n    X_train = [(reuters.raw(doc_id)) for doc_id in train_docs_id]\n    X_test = [(reuters.raw(doc_id)) for doc_id in test_docs_id]\n    mlb = MultiLabelBinarizer()\n    y_train = mlb.fit_transform([reuters.categories(doc_id)\n                                 for doc_id in train_docs_id])\n    y_test = mlb.transform([reuters.categories(doc_id)\n                            for doc_id in test_docs_id])\n    y_train = np.argmax(y_train, axis=1)\n    y_test = np.argmax(y_test, axis=1)\n    batch_size = 100\n    sparse_categorical = 0\n    n_epochs = [120, 120, 120]  ## DNN--RNN-CNN\n    Random_Deep = [3, 3, 3]  ## DNN--RNN-CNN\n    RMDL.Text_Classification(X_train, y_train, X_test, y_test,\n                             batch_size=batch_size,\n                             sparse_categorical=True,\n                             random_deep=Random_Deep,\n                             epochs=n_epochs)\n'"
Examples/Text_classification_demo.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\n#A very simple and minimal demo to show the use of Classification model ,\n#It will clear the doubt how to format the data for model\n\n#importing libraries\nimport sys\nimport os\nfrom RMDL import text_feature_extraction as txt\nfrom keras.datasets import imdb\nimport numpy as np\nfrom RMDL import RMDL_Text as RMDL\n\n\n\n#Data description\n\n#sentences should be in this format:\nsentences=[\'everyone please come check our newest song in memories of Martin Luther  King Jr\', \'Came here to check the views, goodbye.\', \'sub my channel for no reason.\', \'Check out my dubstep song ""Fireball"", made with Fruity Loops. I really took  time in it.\', \'2 billion Coming soon\', \'Why dafuq is a Korean song so big in the USA. Does that mean we support  Koreans? Last time I checked they wanted to bomb us.\', \'Check my channel please! And listen to the best music ever \', \'SUB 4 SUB PLEASE LIKE THIS COMMENT I WANT A SUCCESFULL YOUTUBE SO PPLEASE LIKE THIS  COMMENT AND SUBSCRIBE IT ONLY TAKES 10 SECONDS PLEASE IF YOU SUBSCRIBE ILL  SUBSCRIBE BACK THANKS\', \' Hey everyone!! I have just started my first YT channel i would be grateful  if some of you peoples could check out my first clip in BF4! and give me  some advice on how my video was and how i could improve it. ALSO be sure to  go check out the about to see what Im all about. Thanks for your time :) .  and to haters. You Hate, I WIN\', \'The projects After Effects, Music, Foto, Web sites and another you can find  and buy here\']\n\n#labels can be one hot encoded or like this:\nlabels = [1, 0, 1, 1, 0, 0, 1, 1, 1, 1]\n\n\n#is your labels are like this then you can directly feed to network\n\n#After formatting your data like above data Let\'s use the network and feed the data\n\n\n#split the data into train and test dataset\n\nprint(len(sentences))\nsplit_data = int(len(sentences) * 0.85)\n\ntrain_sentences = sentences[:split_data]\ntrain_labels = labels[:split_data]\n\ntest_sentences = sentences[split_data:]\ntest_labels = labels[split_data:]\n\n\n#batch_size should not be very small neither too big\nbatch_size = 2\n\n\nsparse_categorical = 0\n\n#epoch for DNN , RNN and CNN\nn_epochs = [5, 5, 5]  ## DNN--RNN-CNN\nRandom_Deep = [3, 3, 3]  ## DNN--RNN-CNN\nno_of_classes = 2\nRMDL.Text_Classification(np.array(train_sentences), np.array(train_labels), np.array(test_sentences),\n                         np.array(test_labels),\n                         batch_size=batch_size,\n                         sparse_categorical=sparse_categorical,\n                         random_deep=Random_Deep,\n                         epochs=n_epochs, no_of_classes=2)\n\n\n#output\n#\n# Found 129 unique tokens.\n# (10, 500)\n# Total 400000 word vectors.\n# 2\n# DNN 0\n# <keras.optimizers.Adagrad object at 0x7f00801bbb70>\n# Train on 8 samples, validate on 2 samples\n# Epoch 1/5\n#  - 0s - loss: 0.8781 - acc: 0.5000 - val_loss: 0.1762 - val_acc: 1.0000\n#\n# Epoch 00001: val_acc improved from -inf to 1.00000, saving model to weights\\weights_DNN_0.hdf5\n# Epoch 2/5\n#  - 0s - loss: 0.9983 - acc: 0.7500 - val_loss: 0.0240 - val_acc: 1.0000\n'"
Examples/Text_pandas.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\n\nfrom RMDL import RMDL_Text as RMDL\nimport sys\nsys.path.append(\'../Download_datasets\')\nfrom sklearn.cross_validation import train_test_split\nfrom RMDL import text_feature_extraction as txt\nimport numpy as np\nimport pandas as pd\n\n\nif __name__ == ""__main__"":\n    file_x = ""X.csv""\n    file_y = ""Y_.csv""\n    content = pd.read_csv(file_x, encoding=""utf-8"")\n    Label = pd.read_csv(file_y, encoding=""utf-8"")\n    # content = content.as_matrix()\n    content = content.ix[:, 1]\n    content = np.array(content).ravel()\n    print(np.array(content).transpose().shape)\n    Label = Label.as_matrix()\n    Label = np.matrix(Label)\n    np.random.seed(7)\n    # print(Label)\n    content = [txt.text_cleaner(x,deep_clean=True) for x in content]\n    X_train, X_test, y_train, y_test = train_test_split(content, Label, test_size=0.1, random_state=42)\n    batch_size = 256\n    sparse_categorical = 0\n    n_epochs = [100, 100, 100]  ## DNN--RNN-CNN\n    Random_Deep = [2, 2, 2]  ## DNN--RNN-CNN\n\n    RMDL.Text_Classification(X_train, y_train, X_test, y_test,\n                             batch_size=batch_size,\n                             sparse_categorical=True,\n                             random_deep=Random_Deep,\n                             epochs=n_epochs)'"
Examples/WOS11967.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\nimport os\nfrom RMDL import text_feature_extraction as txt\nfrom sklearn.model_selection import train_test_split\nfrom RMDL.Download import Download_WOS as WOS\nimport numpy as np\nfrom RMDL import RMDL_Text as RMDL\n\nif __name__ == ""__main__"":\n    path_WOS = WOS.download_and_extract()\n    fname = os.path.join(path_WOS,""WebOfScience/WOS11967/X.txt"")\n    fnamek = os.path.join(path_WOS,""WebOfScience/WOS11967/Y.txt"")\n    with open(fname, encoding=""utf-8"") as f:\n        content = f.readlines()\n        content = [txt.text_cleaner(x) for x in content]\n    with open(fnamek) as fk:\n        contentk = fk.readlines()\n    contentk = [x.strip() for x in contentk]\n    Label = np.matrix(contentk, dtype=int)\n    Label = np.transpose(Label)\n    np.random.seed(7)\n    print(Label.shape)\n    X_train, X_test, y_train, y_test = train_test_split(content, Label, test_size=0.2, random_state=4)\n\n    batch_size = 100\n    sparse_categorical = 0\n    n_epochs = [5000, 500, 500]  ## DNN--RNN-CNN\n    Random_Deep = [3, 3, 3]  ## DNN--RNN-CNN\n\n    RMDL.Text_Classification(X_train, y_train, X_test, y_test,\n                             batch_size=batch_size,\n                             sparse_categorical=True,\n                             random_deep=Random_Deep,\n                             epochs=n_epochs)'"
Examples/WOS46985.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\nimport os\nfrom RMDL import text_feature_extraction as txt\nfrom sklearn.model_selection import train_test_split\nfrom RMDL.Download import Download_WOS as WOS\nimport numpy as np\nfrom RMDL import RMDL_Text as RMDL\n\nif __name__ == ""__main__"":\n    path_WOS = WOS.download_and_extract()\n    fname = os.path.join(path_WOS,""WebOfScience/WOS46985/X.txt"")\n    fnamek = os.path.join(path_WOS,""WebOfScience/WOS46985/Y.txt"")\n    with open(fname, encoding=""utf-8"") as f:\n        content = f.readlines()\n        content = [txt.text_cleaner(x) for x in content]\n    with open(fnamek) as fk:\n        contentk = fk.readlines()\n    contentk = [x.strip() for x in contentk]\n    Label = np.matrix(contentk, dtype=int)\n    Label = np.transpose(Label)\n    np.random.seed(7)\n    print(Label.shape)\n    X_train, X_test, y_train, y_test = train_test_split(content, Label, test_size=0.2, random_state=4)\n\n    batch_size = 100\n    sparse_categorical = 0\n    n_epochs = [5000, 500, 500]  ## DNN--RNN-CNN\n    Random_Deep = [3, 3, 3]  ## DNN--RNN-CNN\n\n    RMDL.Text_Classification(X_train, y_train, X_test, y_test,\n                             batch_size=batch_size,\n                             sparse_categorical=True,\n                             random_deep=Random_Deep,\n                             epochs=n_epochs)'"
Examples/WOS5736.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\n\nimport os\nimport numpy as np\nfrom RMDL import text_feature_extraction as txt\nfrom RMDL.Download import Download_WOS as WOS\nfrom RMDL import RMDL_Text as RMDL\nfrom sklearn.cross_validation import train_test_split\n\n\n\nif __name__ == ""__main__"":\n    path_WOS = WOS.download_and_extract()\n    fname = os.path.join(path_WOS,""WebOfScience/WOS5736/X.txt"")\n    fnamek = os.path.join(path_WOS,""WebOfScience/WOS5736/Y.txt"")\n    with open(fname, encoding=""utf-8"") as f:\n        content = f.readlines()\n        content = [txt.text_cleaner(x) for x in content]\n    with open(fnamek) as fk:\n        contentk = fk.readlines()\n    contentk = [x.strip() for x in contentk]\n    Label = np.matrix(contentk, dtype=int)\n    Label = np.transpose(Label)\n    np.random.seed(7)\n    print(Label.shape)\n    X_train, X_test, y_train, y_test = train_test_split(content, Label, test_size=0.2, random_state=42)\n\n    batch_size = 100\n    sparse_categorical = 0\n    n_epochs = [100, 100, 100]  ## DNN--RNN-CNN\n    Random_Deep = [3, 3, 3]  ## DNN--RNN-CNN\n\n    RMDL.Text_Classification(X_train, y_train, X_test, y_test,\n                             batch_size=batch_size,\n                             sparse_categorical=True,\n                             random_deep=Random_Deep,\n                             epochs=n_epochs,no_of_classes=12)\n'"
Examples/_20NewsGroup.py,0,"b'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\nRMDL: Random Multimodel Deep Learning for Classification\n\n * Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n * Last Update: May 3rd, 2018\n * This file is part of  RMDL project, University of Virginia.\n * Free to use, change, share and distribute source code of RMDL\n * Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n * Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n * Comments and Error: email: kk7nc@virginia.edu\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom RMDL import RMDL_Text as RMDL\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nif __name__ == ""__main__"":\n    newsgroups_train = fetch_20newsgroups(subset=\'train\')\n    newsgroups_test = fetch_20newsgroups(subset=\'test\')\n    X_train = newsgroups_train.data\n    X_test = newsgroups_test.data\n    y_train = newsgroups_train.target\n    y_test = newsgroups_test.target\n    batch_size = 100\n    sparse_categorical = 0\n    print(len(X_train))\n    n_epochs = [500, 500, 500]  ## DNN--RNN-CNN\n    Random_Deep = [3,3, 3]  ## DNN--RNN-CNN\n\n    RMDL.Text_Classification(X_train, y_train, X_test, y_test,\n                             batch_size=batch_size,\n                             sparse_categorical=True,\n                             random_deep=Random_Deep,\n                             epochs=n_epochs)\n'"
RMDL/BuildModel.py,3,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\nfrom keras.models import Sequential\nimport numpy as np\nfrom keras.constraints import maxnorm\nfrom keras.layers import Dense, Flatten\nfrom keras.layers import Conv1D,MaxPooling2D, \\\n    MaxPooling1D, Embedding, Dropout,\\\n    GRU,TimeDistributed,Conv2D,\\\n    Activation,LSTM,Input\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.layers.core import Lambda\nfrom keras.layers.merge import Concatenate\nimport tensorflow as tf\nfrom keras import optimizers\nimport random\n\ndef optimizors(random_optimizor):\n    if random_optimizor:\n        i = random.randint(1,3)\n        if i==0:\n            opt = optimizers.SGD()\n        elif i==1:\n            opt= optimizers.RMSprop()\n        elif i==2:\n            opt= optimizers.Adagrad()\n        elif i==3:\n            opt = optimizers.Adam()\n        elif i==4:\n            opt =optimizers.Nadam()\n        print(opt)\n    else:\n        opt= optimizers.Adam()\n    return opt\n\n\n\ndef slice_batch(x, n_gpus, part):\n    """"""\n    Divide the input batch into [n_gpus] slices, and obtain slice number [part].\n    i.e. if len(x)=10, then slice_batch(x, 2, 1) will return x[5:].\n    """"""\n\n    sh = K.shape(x)\n    L = sh[0] // n_gpus\n    if part == n_gpus - 1:\n        return x[part*L:]\n    return x[part*L:(part+1)*L]\n\ndef to_multi_gpu(model, n_gpus=2):\n    """"""\n    Given a keras [model], return an equivalent model which parallelizes\n    the computation over [n_gpus] GPUs.\n\n    Each GPU gets a slice of the input batch, applies the model on that slice\n    and later the outputs of the models are concatenated to a single tensor,\n    hence the user sees a model that behaves the same as the original.\n    """"""\n\n    with tf.device(\'/cpu:0\'):\n        x = Input(model.input_shape[1:], name=""input1"")\n\n    towers = []\n    for g in range(n_gpus):\n        with tf.device(\'/gpu:\' + str(g)):\n            slice_g = Lambda(slice_batch,\n                             lambda shape: shape,\n                             arguments={\'n_gpus\':n_gpus, \'part\':g})(x)\n            towers.append(model(slice_g))\n\n    with tf.device(\'/cpu:0\'):\n        merged = Concatenate(axis=0)(towers)\n\n    return Model(inputs=[x], outputs=[merged])\n\n\ndef Build_Model_DNN_Image(shape, number_of_classes, sparse_categorical, min_hidden_layer_dnn,max_hidden_layer_dnn,\n                          min_nodes_dnn, max_nodes_dnn, random_optimizor, dropout):\n    \'\'\'\n    buildModel_DNN_image(shape, nClasses,sparse_categorical)\n    Build Deep neural networks Model for text classification\n    Shape is input feature space\n    nClasses is number of classes\n    \'\'\'\n\n    model = Sequential()\n    values = list(range(min_nodes_dnn,max_nodes_dnn))\n    Numberof_NOde = random.choice(values)\n    Lvalues = list(range(min_hidden_layer_dnn,max_hidden_layer_dnn))\n    nLayers =random.choice(Lvalues)\n    print(shape)\n    model.add(Flatten(input_shape=shape))\n    model.add(Dense(Numberof_NOde,activation=\'relu\'))\n    model.add(Dropout(dropout))\n    for i in range(0,nLayers-1):\n        Numberof_NOde = random.choice(values)\n        model.add(Dense(Numberof_NOde,activation=\'relu\'))\n        model.add(Dropout(dropout))\n    model.add(Dense(number_of_classes, activation=\'softmax\'))\n    model_tmp = model\n    if sparse_categorical:\n        model.compile(loss=\'sparse_categorical_crossentropy\',\n                      optimizer=optimizors(random_optimizor),\n                      metrics=[\'accuracy\'])\n    else:\n        model.compile(loss=\'categorical_crossentropy\',\n                      optimizer=optimizors(random_optimizor),\n                      metrics=[\'accuracy\'])\n    return model,model_tmp\n\n\ndef Build_Model_DNN_Text(shape, nClasses, sparse_categorical,\n                         min_hidden_layer_dnn, max_hidden_layer_dnn, min_nodes_dnn,\n                         max_nodes_dnn, random_optimizor, dropout):\n    """"""\n    buildModel_DNN_Tex(shape, nClasses,sparse_categorical)\n    Build Deep neural networks Model for text classification\n    Shape is input feature space\n    nClasses is number of classes\n    """"""\n    model = Sequential()\n    layer = list(range(min_hidden_layer_dnn,max_hidden_layer_dnn))\n    node = list(range(min_nodes_dnn, max_nodes_dnn))\n\n\n    Numberof_NOde =  random.choice(node)\n    nLayers = random.choice(layer)\n\n    Numberof_NOde_old = Numberof_NOde\n    model.add(Dense(Numberof_NOde,input_dim=shape,activation=\'relu\'))\n    model.add(Dropout(dropout))\n    for i in range(0,nLayers):\n        Numberof_NOde = random.choice(node)\n        model.add(Dense(Numberof_NOde,input_dim=Numberof_NOde_old,activation=\'relu\'))\n        model.add(Dropout(dropout))\n        Numberof_NOde_old = Numberof_NOde\n    model.add(Dense(nClasses, activation=\'softmax\'))\n    model_tem = model\n    if sparse_categorical:\n        model.compile(loss=\'sparse_categorical_crossentropy\',\n                      optimizer=optimizors(random_optimizor),\n                      metrics=[\'accuracy\'])\n    else:\n        model.compile(loss=\'categorical_crossentropy\',\n                      optimizer=optimizors(random_optimizor),\n                      metrics=[\'accuracy\'])\n    return model,model_tem\n\n\ndef Build_Model_CNN_Image(shape, nclasses, sparse_categorical,\n                          min_hidden_layer_cnn, max_hidden_layer_cnn, min_nodes_cnn,\n                          max_nodes_cnn, random_optimizor, dropout):\n    """"""""""\n    def Image_model_CNN(num_classes,shape):\n    num_classes is number of classes,\n    shape is (w,h,p)\n    """"""""""\n\n    model = Sequential()\n    values = list(range(min_nodes_cnn,max_nodes_cnn))\n    Layers = list(range(min_hidden_layer_cnn, max_hidden_layer_cnn))\n    Layer = random.choice(Layers)\n    Filter = random.choice(values)\n    model.add(Conv2D(Filter, (3, 3), padding=\'same\', input_shape=shape))\n    model.add(Activation(\'relu\'))\n    model.add(Conv2D(Filter, (3, 3)))\n    model.add(Activation(\'relu\'))\n\n    for i in range(0,Layer):\n        Filter = random.choice(values)\n        model.add(Conv2D(Filter, (3, 3),padding=\'same\'))\n        model.add(Activation(\'relu\'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(dropout))\n\n    model.add(Flatten())\n    model.add(Dense(256, activation=\'relu\'))\n    model.add(Dropout(dropout))\n    model.add(Dense(nclasses,activation=\'softmax\',kernel_constraint=maxnorm(3)))\n    model_tmp = model\n    if sparse_categorical:\n        model.compile(loss=\'sparse_categorical_crossentropy\',\n                      optimizer=optimizors(random_optimizor),\n                      metrics=[\'accuracy\'])\n    else:\n        model.compile(loss=\'categorical_crossentropy\',\n                      optimizer=optimizors(random_optimizor),\n                      metrics=[\'accuracy\'])\n\n    return model,model_tmp\n\n\n\n\ndef Build_Model_RNN_Image(shape,\n                          nclasses,\n                          sparse_categorical,\n                          min_nodes_rnn,\n                          max_nodes_rnn,\n                          random_optimizor,\n                          dropout):\n    """"""\n        def Image_model_RNN(num_classes,shape):\n        num_classes is number of classes,\n        shape is (w,h,p)\n    """"""\n    values = list(range(min_nodes_rnn,max_nodes_rnn))\n    node =  random.choice(values)\n\n    x = Input(shape=shape)\n\n    # Encodes a row of pixels using TimeDistributed Wrapper.\n    encoded_rows = TimeDistributed(LSTM(node,recurrent_dropout=dropout))(x)\n    node = random.choice(values)\n    # Encodes columns of encoded rows.\n    encoded_columns = LSTM(node,recurrent_dropout=dropout)(encoded_rows)\n\n    # Final predictions and model.\n    #prediction = Dense(256, activation=\'relu\')(encoded_columns)\n    prediction = Dense(nclasses, activation=\'softmax\')(encoded_columns)\n    model = Model(x, prediction)\n    model_tmp = model\n    if sparse_categorical:\n        model.compile(loss=\'sparse_categorical_crossentropy\',\n                  optimizer=optimizors(random_optimizor),\n                  metrics=[\'accuracy\'])\n    else:\n        model.compile(loss=\'categorical_crossentropy\',\n                  optimizer=optimizors(random_optimizor),\n                  metrics=[\'accuracy\'])\n    return model,model_tmp\n\n\ndef Build_Model_RNN_Text(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, sparse_categorical,\n                         min_hidden_layer_rnn, max_hidden_layer_rnn, min_nodes_rnn, max_nodes_rnn, random_optimizor, dropout):\n    """"""\n    def buildModel_RNN(word_index, embeddings_index, nClasses, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, sparse_categorical):\n    word_index in word index ,\n    embeddings_index is embeddings index, look at data_helper.py\n    nClasses is number of classes,\n    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences\n    """"""\n\n    model = Sequential()\n    values = list(range(min_nodes_rnn,max_nodes_rnn))\n    values_layer = list(range(min_hidden_layer_rnn,max_hidden_layer_rnn))\n\n    layer = random.choice(values_layer)\n    print(layer)\n    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            if len(embedding_matrix[i]) != len(embedding_vector):\n                print(""could not broadcast input array from shape"", str(len(embedding_matrix[i])),\n                      ""into shape"", str(len(embedding_vector)), "" Please make sure your""\n                                                                "" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,"")\n                exit(1)\n            embedding_matrix[i] = embedding_vector\n    model.add(Embedding(len(word_index) + 1,\n                                EMBEDDING_DIM,\n                                weights=[embedding_matrix],\n                                input_length=MAX_SEQUENCE_LENGTH,\n                                trainable=True))\n\n    gru_node = random.choice(values)\n    print(gru_node)\n    for i in range(0,layer):\n        model.add(GRU(gru_node,return_sequences=True, recurrent_dropout=0.2))\n        model.add(Dropout(dropout))\n    model.add(GRU(gru_node, recurrent_dropout=0.2))\n    model.add(Dropout(dropout))\n    model.add(Dense(256, activation=\'relu\'))\n    model.add(Dense(nclasses, activation=\'softmax\'))\n\n    model_tmp = model\n    #model = to_multi_gpu(model, 3)\n\n\n    if sparse_categorical:\n        model.compile(loss=\'sparse_categorical_crossentropy\',\n                      optimizer=optimizors(random_optimizor),\n                      metrics=[\'accuracy\'])\n    else:\n        model.compile(loss=\'categorical_crossentropy\',\n                      optimizer=optimizors(random_optimizor),\n                      metrics=[\'accuracy\'])\n    return model,model_tmp\n\n\ndef Build_Model_CNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, sparse_categorical,\n                       min_hidden_layer_cnn, max_hidden_layer_cnn, min_nodes_cnn, max_nodes_cnn, random_optimizor,\n                       dropout, simple_model=False):\n\n    """"""\n        def buildModel_CNN(word_index,embeddings_index,nClasses,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,Complexity=0):\n        word_index in word index ,\n        embeddings_index is embeddings index, look at data_helper.py\n        nClasses is number of classes,\n        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n        Complexity we have two different CNN model as follows\n        F=0 is simple CNN with [1 5] hidden layer\n        Complexity=2 is more complex model of CNN with filter_length of range [1 10]\n    """"""\n\n    model = Sequential()\n    if simple_model:\n        embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n        for word, i in word_index.items():\n            embedding_vector = embeddings_index.get(word)\n            if embedding_vector is not None:\n                if len(embedding_matrix[i]) !=len(embedding_vector):\n                    print(""could not broadcast input array from shape"",str(len(embedding_matrix[i])),\n                                     ""into shape"",str(len(embedding_vector)),"" Please make sure your""\n                                     "" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,"")\n                    exit(1)\n                # words not found in embedding index will be all-zeros.\n                embedding_matrix[i] = embedding_vector\n        model.add(Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=True))\n        values = list(range(min_nodes_cnn,max_nodes_cnn))\n        Layer = list(range(min_hidden_layer_cnn,max_hidden_layer_cnn))\n        Layer = random.choice(Layer)\n        for i in range(0,Layer):\n            Filter = random.choice(values)\n            model.add(Conv1D(Filter, 5, activation=\'relu\'))\n            model.add(Dropout(dropout))\n            model.add(MaxPooling1D(5))\n\n        model.add(Flatten())\n        Filter = random.choice(values)\n        model.add(Dense(Filter, activation=\'relu\'))\n        model.add(Dropout(dropout))\n        Filter = random.choice(values)\n        model.add(Dense(Filter, activation=\'relu\'))\n        model.add(Dropout(dropout))\n\n        model.add(Dense(nclasses, activation=\'softmax\'))\n        model_tmp = model\n        #model = Model(sequence_input, preds)\n        if sparse_categorical:\n            model.compile(loss=\'sparse_categorical_crossentropy\',\n                          optimizer=optimizors(random_optimizor),\n                          metrics=[\'accuracy\'])\n        else:\n            model.compile(loss=\'categorical_crossentropy\',\n                          optimizer=optimizors(random_optimizor),\n                          metrics=[\'accuracy\'])\n    else:\n        embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n        for word, i in word_index.items():\n            embedding_vector = embeddings_index.get(word)\n            if embedding_vector is not None:\n                # words not found in embedding index will be all-zeros.\n                if len(embedding_matrix[i]) !=len(embedding_vector):\n                    print(""could not broadcast input array from shape"",str(len(embedding_matrix[i])),\n                                     ""into shape"",str(len(embedding_vector)),"" Please make sure your""\n                                     "" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,"")\n                    exit(1)\n\n                embedding_matrix[i] = embedding_vector\n\n        embedding_layer = Embedding(len(word_index) + 1,\n                                    EMBEDDING_DIM,\n                                    weights=[embedding_matrix],\n                                    input_length=MAX_SEQUENCE_LENGTH,\n                                    trainable=True)\n\n        # applying a more complex convolutional approach\n        convs = []\n        values_layer = list(range(min_hidden_layer_cnn,max_hidden_layer_cnn))\n        filter_sizes = []\n        layer = random.choice(values_layer)\n        print(""Filter  "",layer)\n        for fl in range(0,layer):\n            filter_sizes.append((fl+2))\n\n        values_node = list(range(min_nodes_cnn,max_nodes_cnn))\n        node = random.choice(values_node)\n        print(""Node  "", node)\n        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\'int32\')\n        embedded_sequences = embedding_layer(sequence_input)\n\n        for fsz in filter_sizes:\n            l_conv = Conv1D(node, kernel_size=fsz, activation=\'relu\')(embedded_sequences)\n            l_pool = MaxPooling1D(5)(l_conv)\n            #l_pool = Dropout(0.25)(l_pool)\n            convs.append(l_pool)\n\n        l_merge = Concatenate(axis=1)(convs)\n        l_cov1 = Conv1D(node, 5, activation=\'relu\')(l_merge)\n        l_cov1 = Dropout(dropout)(l_cov1)\n        l_pool1 = MaxPooling1D(5)(l_cov1)\n        l_cov2 = Conv1D(node, 5, activation=\'relu\')(l_pool1)\n        l_cov2 = Dropout(dropout)(l_cov2)\n        l_pool2 = MaxPooling1D(30)(l_cov2)\n        l_flat = Flatten()(l_pool2)\n        l_dense = Dense(1024, activation=\'relu\')(l_flat)\n        l_dense = Dropout(dropout)(l_dense)\n        l_dense = Dense(512, activation=\'relu\')(l_dense)\n        l_dense = Dropout(dropout)(l_dense)\n        preds = Dense(nclasses, activation=\'softmax\')(l_dense)\n        model = Model(sequence_input, preds)\n        model_tmp = model\n        if sparse_categorical:\n            model.compile(loss=\'sparse_categorical_crossentropy\',\n                          optimizer=optimizors(random_optimizor),\n                          metrics=[\'accuracy\'])\n        else:\n            model.compile(loss=\'categorical_crossentropy\',\n                          optimizer=optimizors(random_optimizor),\n                          metrics=[\'accuracy\'])\n\n\n    return model,model_tmp\n'"
RMDL/Global.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\n\nimport numpy as np\nimport os\n\ndef setup():\n    np.set_printoptions(threshold=np.inf)\n    np.random.seed(7)\n    if not os.path.exists("".\\weights""):\n        os.makedirs("".\\weights"")\n\n\n'"
RMDL/Plot.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom pylab import *\nimport itertools\n\ndef RMDL_epoch(history_):\n    Number_of_models = len(history_)\n    caption=[]\n    for i in range(0,len(history_)):\n        caption.append(\'RDL \'+str(i+1))\n    plt.legend(caption, loc=\'upper right\')\n    for i in range(0, Number_of_models):\n        plt.plot(history_[i].history[\'accuracy\'])\n        plt.title(\'model train accuracy\')\n        plt.ylabel(\'accuracy\')\n        plt.xlabel(\'epoch\')\n    plt.legend(caption, loc=\'upper right\')\n    plt.show()\n\n\n\n    for i in range(0, Number_of_models):\n        plt.plot(history_[i].history[\'val_accuracy\'])\n        plt.title(\'model test accuracy\')\n        plt.ylabel(\'accuracy\')\n        plt.xlabel(\'epoch\')\n\n    plt.show()\n    plt.legend(caption, loc=\'upper right\')\n    for i in range(0, Number_of_models):\n        # summarize history for loss\n        plt.plot(history_[i].history[\'loss\'])\n\n        plt.title(\'model train loss \')\n        plt.ylabel(\'loss\')\n        plt.xlabel(\'epoch\')\n\n    plt.legend(caption, loc=\'upper right\')\n    plt.show()\n    plt.legend(\n        caption, loc=\'upper right\')\n    for i in range(0, Number_of_models):\n        # summarize history for loss\n        plt.plot(history_[i].history[\'val_loss\'])\n\n        plt.title(\'model loss test\')\n        plt.ylabel(\'loss\')\n        plt.xlabel(\'epoch\')\n    plt.legend(caption, loc=\'upper right\')\n    plt.show()\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title=\'Confusion matrix\',\n                          cmap=plt.cm.Blues):\n    """"""\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    """"""\n    if normalize:\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n        #print(""Normalized confusion matrix"")\n    #else:\n       # print(\'Confusion matrix, without normalization\')\n\n    #print(cm)\n\n    plt.imshow(cm, interpolation=\'nearest\', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = \'.2f\' if normalize else \'d\'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=""center"",\n                 color=""white"" if cm[i, j] > thresh else ""black"")\n\n    plt.tight_layout()\n    plt.ylabel(\'True label\')\n    plt.xlabel(\'Predicted label\')\n    plt.show()\n\n\ndef accuracy(y_test,final_y):\n    np.set_printoptions(precision=2)\n    y_test_temp = np.argmax(y_test, axis=1)\n    F_score = accuracy_score(y_test_temp, final_y)\n    F1 = precision_recall_fscore_support(y_test_temp, final_y, average=\'micro\')\n    F2 = precision_recall_fscore_support(y_test_temp, final_y, average=\'macro\')\n    F3 = precision_recall_fscore_support(y_test_temp, final_y, average=\'weighted\')\n    print(F_score)\n    print(F1)\n    print(F2)\n    print(F3)\n'"
RMDL/RMDL_Image.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom RMDL import Plot as Plot\nimport gc\nfrom sklearn.metrics import confusion_matrix\nimport collections\nfrom sklearn.metrics import f1_score\nfrom RMDL import BuildModel as BuildModel\nfrom RMDL import Global as G\nfrom keras.callbacks import ModelCheckpoint\nnp.random.seed(7)\n\n\ndef Image_Classification(x_train, y_train, x_test, y_test, shape, batch_size=128,\n                         sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500], plot=False,\n                         min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n                         max_hidden_layer_rnn=5, min_nodes_rnn=32, max_nodes_rnn=128,\n                         min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n                         random_state=42, random_optimizor=True, dropout=0.05):\n    """"""\n    def Image_Classification(x_train, y_train, x_test, y_test, shape, batch_size=128,\n                             sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500], plot=False,\n                             min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n                             min_hidden_layer_rnn=1, max_hidden_layer_rnn=5, min_nodes_rnn=32, max_nodes_rnn=128,\n                             min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n                             random_state=42, random_optimizor=True, dropout=0.05):\n\n            Parameters\n            ----------\n                x_train : string\n                    input X for training\n                y_train : int\n                    input Y for training\n                x_test : string\n                    input X for testing\n                x_test : int\n                    input Y for testing\n                shape : np.shape\n                    shape of image. The most common situation would be a 2D input with shape (batch_size, input_dim).\n                batch_size : Integer, , optional\n                    Number of samples per gradient update. If unspecified, it will default to 128\n                MAX_NB_WORDS: int, optional\n                    Maximum number of unique words in datasets, it will default to 75000.\n                GloVe_dir: String, optional\n                    Address of GloVe or any pre-trained directory, it will default to null which glove.6B.zip will be download.\n                GloVe_dir: String, optional\n                    Which version of GloVe or pre-trained word emending will be used, it will default to glove.6B.50d.txt.\n                    NOTE: if you use other version of GloVe EMBEDDING_DIM must be same dimensions.\n                sparse_categorical: bool.\n                    When target\'s dataset is (n,1) should be True, it will default to True.\n                random_deep: array of int [3], optional\n                    Number of ensembled model used in RMDL random_deep[0] is number of DNN, random_deep[1] is number of RNN, random_deep[0] is number of CNN, it will default to [3, 3, 3].\n                epochs: array of int [3], optional\n                    Number of epochs in each ensembled model used in RMDL epochs[0] is number of epochs used in DNN, epochs[1] is number of epochs used in RNN, epochs[0] is number of epochs used in CNN, it will default to [500, 500, 500].\n                plot: bool, optional\n                    True: shows confusion matrix and accuracy and loss\n                min_hidden_layer_dnn: Integer, optional\n                    Lower Bounds of hidden layers of DNN used in RMDL, it will default to 1.\n                max_hidden_layer_dnn: Integer, optional\n                    Upper bounds of hidden layers of DNN used in RMDL, it will default to 8.\n                min_nodes_dnn: Integer, optional\n                    Lower bounds of nodes in each layer of DNN used in RMDL, it will default to 128.\n                max_nodes_dnn: Integer, optional\n                    Upper bounds of nodes in each layer of DNN used in RMDL, it will default to 1024.\n                min_hidden_layer_rnn: Integer, optional\n                    Lower Bounds of hidden layers of RNN used in RMDL, it will default to 1.\n                min_hidden_layer_rnn: Integer, optional\n                    Upper Bounds of hidden layers of RNN used in RMDL, it will default to 5.\n                min_nodes_rnn: Integer, optional\n                    Lower bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 32.\n                max_nodes_rnn: Integer, optional\n                    Upper bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 128.\n                min_hidden_layer_cnn: Integer, optional\n                    Lower Bounds of hidden layers of CNN used in RMDL, it will default to 3.\n                max_hidden_layer_cnn: Integer, optional\n                    Upper Bounds of hidden layers of CNN used in RMDL, it will default to 10.\n                min_nodes_cnn: Integer, optional\n                    Lower bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 128.\n                min_nodes_cnn: Integer, optional\n                    Upper bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 512.\n                random_state : Integer, optional\n                    RandomState instance or None, optional (default=None)\n                    If Integer, random_state is the seed used by the random number generator;\n                random_optimizor : bool, optional\n                    If False, all models use adam optimizer. If True, all models use random optimizers. it will default to True\n                dropout: Float, optional\n                    between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.\n\n        """"""\n\n    if len(x_train) != len(y_train):\n        raise ValueError(\'shape of x_train and y_train must be equal\'\n                         \'The x_train has \' + str(len(x_train)) +\n                         \'The x_train has\' +\n                         str(len(y_train)))\n\n    if len(x_test) != len(y_test):\n        raise ValueError(\'shape of x_test and y_test must be equal \'\n                         \'The x_train has \' + str(len(x_test)) +\n                         \'The y_test has \' +\n                         str(len(y_test)))\n\n    np.random.seed(random_state)\n    G.setup()\n    y_proba = []\n\n    score = []\n    history_ = []\n    if sparse_categorical:\n        number_of_classes = np.max(y_train)+1\n    else:\n        number_of_classes = np.shape(y_train)[0]\n\n    i =0\n    while i < random_deep[0]:\n        try:\n            print(""DNN "", i, ""\\n"")\n            model_DNN, model_tmp = BuildModel.Build_Model_DNN_Image(shape,\n                                                                    number_of_classes,\n                                                                    sparse_categorical,\n                                                                    min_hidden_layer_dnn,\n                                                                    max_hidden_layer_dnn,\n                                                                    min_nodes_dnn,\n                                                                    max_nodes_dnn,\n                                                                    random_optimizor,\n                                                                    dropout)\n\n\n            filepath = ""weights\\weights_DNN_"" + str(i) + "".hdf5""\n            checkpoint = ModelCheckpoint(filepath,\n                                         monitor=\'val_acc\',\n                                         verbose=1,\n                                         save_best_only=True,\n                                         mode=\'max\')\n            callbacks_list = [checkpoint]\n\n            history = model_DNN.fit(x_train, y_train,\n                                    validation_data=(x_test, y_test),\n                                    epochs=epochs[0],\n                                    batch_size=batch_size,\n                                    callbacks=callbacks_list,\n                                    verbose=2)\n            history_.append(history)\n            model_tmp.load_weights(filepath)\n\n            if sparse_categorical == 0:\n                model_tmp.compile(loss=\'sparse_categorical_crossentropy\',\n                                  optimizer=\'adam\',\n                                  metrics=[\'accuracy\'])\n            else:\n                model_tmp.compile(loss=\'categorical_crossentropy\',\n                                  optimizer=\'adam\',\n                                  metrics=[\'accuracy\'])\n\n            y_pr = model_tmp.predict_classes(x_test, batch_size=batch_size)\n            y_proba.append(np.array(y_pr))\n            score.append(accuracy_score(y_test, y_pr))\n            i = i + 1\n            del model_tmp\n            del model_DNN\n            gc.collect()\n        except:\n            print(""Error in model"", i, ""try to re-generate an other model"")\n            if max_hidden_layer_dnn > 3:\n                max_hidden_layer_dnn -= 1\n            if max_nodes_dnn > 256:\n                max_nodes_dnn -= 8\n\n\n    i =0\n    while i < random_deep[1]:\n        try:\n            print(""RNN "", i, ""\\n"")\n            model_RNN, model_tmp = BuildModel.Build_Model_RNN_Image(shape,\n                                                                    number_of_classes,\n                                                                    sparse_categorical,\n                                                                    min_nodes_rnn,\n                                                                    max_nodes_rnn,\n                                                                    random_optimizor,\n                                                                    dropout)\n\n            filepath = ""weights\\weights_RNN_"" + str(i) + "".hdf5""\n            checkpoint = ModelCheckpoint(filepath,\n                                         monitor=\'val_acc\',\n                                         verbose=1,\n                                         save_best_only=True,\n                                         mode=\'max\')\n            callbacks_list = [checkpoint]\n\n            history = model_RNN.fit(x_train, y_train,\n                                    validation_data=(x_test, y_test),\n                                    epochs=epochs[1],\n                                    batch_size=batch_size,\n                                    verbose=2,\n                                    callbacks=callbacks_list)\n\n            model_tmp.load_weights(filepath)\n            model_tmp.compile(loss=\'sparse_categorical_crossentropy\',\n                              optimizer=\'rmsprop\',\n                              metrics=[\'accuracy\'])\n            history_.append(history)\n\n            y_pr = model_tmp.predict(x_test, batch_size=batch_size)\n            y_pr = np.argmax(y_pr, axis=1)\n            y_proba.append(np.array(y_pr))\n            score.append(accuracy_score(y_test, y_pr))\n            i = i+1\n            del model_tmp\n            del model_RNN\n            gc.collect()\n        except:\n            print(""Error in model"", i, "" try to re-generate another model"")\n            if max_hidden_layer_rnn > 3:\n                max_hidden_layer_rnn -= 1\n            if max_nodes_rnn > 64:\n                max_nodes_rnn -= 2\n\n    # reshape to be [samples][pixels][width][height]\n    i=0\n    while i < random_deep[2]:\n        try:\n            print(""CNN "", i, ""\\n"")\n            model_CNN, model_tmp = BuildModel.Build_Model_CNN_Image(shape,\n                                                                    number_of_classes,\n                                                                    sparse_categorical,\n                                                                    min_hidden_layer_cnn,\n                                                                    max_hidden_layer_cnn,\n                                                                    min_nodes_cnn,\n                                                                    max_nodes_cnn,\n                                                                    random_optimizor,\n                                                                    dropout)\n\n            filepath = ""weights\\weights_CNN_"" + str(i) + "".hdf5""\n            checkpoint = ModelCheckpoint(filepath, monitor=\'val_acc\', verbose=1, save_best_only=True,\n                                         mode=\'max\')\n            callbacks_list = [checkpoint]\n\n            history = model_CNN.fit(x_train, y_train,\n                                    validation_data=(x_test, y_test),\n                                    epochs=epochs[2],\n                                    batch_size=batch_size,\n                                    callbacks=callbacks_list,\n                                    verbose=2)\n            history_.append(history)\n            model_tmp.load_weights(filepath)\n            model_tmp.compile(loss=\'sparse_categorical_crossentropy\',\n                              optimizer=\'adam\',\n                              metrics=[\'accuracy\'])\n\n            y_pr = model_tmp.predict_classes(x_test, batch_size=batch_size)\n            y_proba.append(np.array(y_pr))\n            score.append(accuracy_score(y_test, y_pr))\n            i = i+1\n            del model_tmp\n            del model_CNN\n            gc.collect()\n        except:\n            print(""Error in model"", i, "" try to re-generate another model"")\n            if max_hidden_layer_cnn > 5:\n                max_hidden_layer_cnn -= 1\n            if max_nodes_cnn > 128:\n                max_nodes_cnn -= 2\n                min_nodes_cnn -= 1\n\n\n\n    y_proba = np.array(y_proba).transpose()\n    print(y_proba.shape)\n    final_y = []\n    for i in range(0, y_proba.shape[0]):\n        a = np.array(y_proba[i, :])\n        a = collections.Counter(a).most_common()[0][0]\n        final_y.append(a)\n    F_score = accuracy_score(y_test, final_y)\n    F1 = f1_score(y_test, final_y, average=\'micro\')\n    F2 = f1_score(y_test, final_y, average=\'macro\')\n    F3 = f1_score(y_test, final_y, average=\'weighted\')\n    cnf_matrix = confusion_matrix(y_test, final_y)\n    # Compute confusion matrix\n    np.set_printoptions(precision=2)\n    if plot:\n        # Plot non-normalized confusion matrix\n        classes = list(range(0,np.max(y_test)+1))\n        Plot.plot_confusion_matrix(cnf_matrix, classes=classes,\n                         title=\'Confusion matrix, without normalization\')\n        Plot.plot_confusion_matrix(cnf_matrix, classes=classes,normalize=True,\n                              title=\'Confusion matrix, without normalization\')\n\n    if plot:\n        Plot.RMDL_epoch(history_)\n\n    print(y_proba.shape)\n    print(""Accuracy of"",len(score),""models:"",score)\n    print(""Accuracy:"",F_score)\n    print(""F1_Micro:"",F1)\n    print(""F1_Macro:"",F2)\n    print(""F1_weighted:"",F3)\n'"
RMDL/RMDL_Text.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\nimport gc\nimport os\nimport numpy as np\nimport collections\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom keras.callbacks import ModelCheckpoint\nfrom RMDL import BuildModel as BuildModel\nfrom RMDL.Download import Download_Glove as GloVe\nfrom RMDL import text_feature_extraction as txt\nfrom RMDL import Global as G\nfrom RMDL import Plot as Plot\n\n\ndef Text_Classification(x_train, y_train, x_test,  y_test, batch_size=128,\n                        EMBEDDING_DIM=50,MAX_SEQUENCE_LENGTH = 500, MAX_NB_WORDS = 75000,\n                        GloVe_dir="""", GloVe_file = ""glove.6B.50d.txt"",\n                        sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500],  plot=False,\n                        min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n                        min_hidden_layer_rnn=1, max_hidden_layer_rnn=5, min_nodes_rnn=32,  max_nodes_rnn=128,\n                        min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n                        random_state=42, random_optimizor=True, dropout=0.5,no_of_classes=0):\n\n\n    """"""\n    Text_Classification(x_train, y_train, x_test,  y_test, batch_size=128,\n                        EMBEDDING_DIM=50,MAX_SEQUENCE_LENGTH = 500, MAX_NB_WORDS = 75000,\n                        GloVe_dir="""", GloVe_file = ""glove.6B.50d.txt"",\n                        sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500],  plot=False,\n                        min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n                        min_hidden_layer_rnn=1, max_hidden_layer_rnn=5, min_nodes_rnn=32,  max_nodes_rnn=128,\n                        min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n                        random_state=42, random_optimizor=True, dropout=0.5):\n\n        Parameters\n        ----------\n            batch_size : Integer, , optional\n                Number of samples per gradient update. If unspecified, it will default to 128\n            MAX_NB_WORDS: int, optional\n                Maximum number of unique words in datasets, it will default to 75000.\n            GloVe_dir: String, optional\n                Address of GloVe or any pre-trained directory, it will default to null which glove.6B.zip will be download.\n            GloVe_dir: String, optional\n                Which version of GloVe or pre-trained word emending will be used, it will default to glove.6B.50d.txt.\n                NOTE: if you use other version of GloVe EMBEDDING_DIM must be same dimensions.\n            sparse_categorical: bool.\n                When target\'s dataset is (n,1) should be True, it will default to True.\n            random_deep: array of int [3], optional\n                Number of ensembled model used in RMDL random_deep[0] is number of DNN, random_deep[1] is number of RNN, random_deep[0] is number of CNN, it will default to [3, 3, 3].\n            epochs: array of int [3], optional\n                Number of epochs in each ensembled model used in RMDL epochs[0] is number of epochs used in DNN, epochs[1] is number of epochs used in RNN, epochs[0] is number of epochs used in CNN, it will default to [500, 500, 500].\n            plot: bool, optional\n                True: shows confusion matrix and accuracy and loss\n            min_hidden_layer_dnn: Integer, optional\n                Lower Bounds of hidden layers of DNN used in RMDL, it will default to 1.\n            max_hidden_layer_dnn: Integer, optional\n                Upper bounds of hidden layers of DNN used in RMDL, it will default to 8.\n            min_nodes_dnn: Integer, optional\n                Lower bounds of nodes in each layer of DNN used in RMDL, it will default to 128.\n            max_nodes_dnn: Integer, optional\n                Upper bounds of nodes in each layer of DNN used in RMDL, it will default to 1024.\n            min_hidden_layer_rnn: Integer, optional\n                Lower Bounds of hidden layers of RNN used in RMDL, it will default to 1.\n            min_hidden_layer_rnn: Integer, optional\n                Upper Bounds of hidden layers of RNN used in RMDL, it will default to 5.\n            min_nodes_rnn: Integer, optional\n                Lower bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 32.\n            max_nodes_rnn: Integer, optional\n                Upper bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 128.\n            min_hidden_layer_cnn: Integer, optional\n                Lower Bounds of hidden layers of CNN used in RMDL, it will default to 3.\n            max_hidden_layer_cnn: Integer, optional\n                Upper Bounds of hidden layers of CNN used in RMDL, it will default to 10.\n            min_nodes_cnn: Integer, optional\n                Lower bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 128.\n            min_nodes_cnn: Integer, optional\n                Upper bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 512.\n            random_state : Integer, optional\n                RandomState instance or None, optional (default=None)\n                If Integer, random_state is the seed used by the random number generator;\n            random_optimizor : bool, optional\n                If False, all models use adam optimizer. If True, all models use random optimizers. it will default to True\n            dropout: Float, optional\n                between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.\n\n    """"""\n    np.random.seed(random_state)\n\n\n    glove_directory = GloVe_dir\n    GloVe_file = GloVe_file\n\n    print(""Done1"")\n\n    GloVe_needed = random_deep[1] != 0 or random_deep[2] != 0\n    \n    # example_input  = [0,1,3]\n    # example_output :\n    # \n    # [[1 0 0 0]\n    #  [0 1 0 0]\n    #  [0 0 0 1]]\n    \n    def one_hot_encoder(value, label_data_):\n\n        label_data_[value] = 1\n\n        return label_data_\n\n    def _one_hot_values(labels_data):\n        encoded = [0] * len(labels_data)\n\n        for index_no, value in enumerate(labels_data):\n            max_value = [0] * (np.max(labels_data) + 1)\n\n            encoded[index_no] = one_hot_encoder(value, max_value)\n\n        return np.array(encoded)\n\n    if not isinstance(y_train[0], list) and not isinstance(y_train[0], np.ndarray) and not sparse_categorical:\n        #checking if labels are one hot or not otherwise dense_layer will give shape error \n        \n        print(""converted_into_one_hot"")\n        y_train = _one_hot_values(y_train)\n        y_test = _one_hot_values(y_test)\n            \n\n\n\n\n    if GloVe_needed:\n        if glove_directory == """":\n            GloVe_directory = GloVe.download_and_extract()\n            GloVe_DIR = os.path.join(GloVe_directory, GloVe_file)\n        else:\n            GloVe_DIR = os.path.join(glove_directory, GloVe_file)\n\n        if not os.path.isfile(GloVe_DIR):\n            print(""Could not find %s Set GloVe Directory in Global.py "", GloVe)\n            exit()\n\n    G.setup()\n    if random_deep[0] != 0:\n        x_train_tfidf, x_test_tfidf = txt.loadData(x_train, x_test,MAX_NB_WORDS=MAX_NB_WORDS)\n    if random_deep[1] != 0 or random_deep[2] != 0 :\n        print(GloVe_DIR)\n        x_train_embedded, x_test_embedded, word_index, embeddings_index = txt.loadData_Tokenizer(x_train, x_test,GloVe_DIR,MAX_NB_WORDS,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM)\n\n    del x_train\n    del x_test\n    gc.collect()\n\n    y_pr = []\n    History = []\n    score = []\n\n    if no_of_classes==0:\n        #checking no_of_classes\n        #np.max(data)+1 will not work for one_hot encoding labels\n        if sparse_categorical:\n            number_of_classes = np.max(y_train) + 1\n        else:\n            number_of_classes = len(y_train[0])\n    else:\n        number_of_classes = no_of_classes\n    print(number_of_classes)\n\n\n    i = 0\n    while i < random_deep[0]:\n        # model_DNN.append(Sequential())\n        try:\n            print(""DNN "" + str(i))\n            filepath = ""weights\\weights_DNN_"" + str(i) + "".hdf5""\n            checkpoint = ModelCheckpoint(filepath,\n                                         monitor=\'val_accuracy\',\n                                         verbose=1,\n                                         save_best_only=True,\n                                         mode=\'max\')\n            callbacks_list = [checkpoint]\n\n            model_DNN, model_tmp = BuildModel.Build_Model_DNN_Text(x_train_tfidf.shape[1],\n                                                                   number_of_classes,\n                                                                   sparse_categorical,\n                                                                   min_hidden_layer_dnn,\n                                                                   max_hidden_layer_dnn,\n                                                                   min_nodes_dnn,\n                                                                   max_nodes_dnn,\n                                                                   random_optimizor,\n                                                                   dropout)\n            model_history = model_DNN.fit(x_train_tfidf, y_train,\n                              validation_data=(x_test_tfidf, y_test),\n                              epochs=epochs[0],\n                              batch_size=batch_size,\n                              callbacks=callbacks_list,\n                              verbose=2)\n            History.append(model_history)\n\n            model_tmp.load_weights(filepath)\n            if sparse_categorical:\n                model_tmp.compile(loss=\'sparse_categorical_crossentropy\',\n                                  optimizer=\'adam\',\n                                  metrics=[\'accuracy\'])\n\n                y_pr_ = model_tmp.predict_classes(x_test_tfidf,\n                                                  batch_size=batch_size)\n                y_pr.append(np.array(y_pr_))\n                score.append(accuracy_score(y_test, y_pr_))\n            else:\n                model_tmp.compile(loss=\'categorical_crossentropy\',\n                                  optimizer=\'adam\',\n                                  metrics=[\'accuracy\'])\n\n                y_pr_ = model_tmp.predict(x_test_tfidf,\n                                          batch_size=batch_size)\n\n                y_pr_ = np.argmax(y_pr_, axis=1)\n                y_pr.append(np.array(y_pr_))\n                y_test_temp = np.argmax(y_test, axis=1)\n                score.append(accuracy_score(y_test_temp, y_pr_))\n            # print(y_proba)\n            i += 1\n            del model_tmp\n            del model_DNN\n\n        except Exception as e:\n\n            print(""Check the Error \\n {} "".format(e))\n\n            print(""Error in model"", i, ""try to re-generate another model"")\n            if max_hidden_layer_dnn > 3:\n                max_hidden_layer_dnn -= 1\n            if max_nodes_dnn > 256:\n                max_nodes_dnn -= 8\n\n    try:\n        del x_train_tfidf\n        del x_test_tfidf\n        gc.collect()\n    except:\n        pass\n\n    i=0\n    while i < random_deep[1]:\n        try:\n            print(""RNN "" + str(i))\n            filepath = ""weights\\weights_RNN_"" + str(i) + "".hdf5""\n            checkpoint = ModelCheckpoint(filepath,\n                                         monitor=\'val_accuracy\',\n                                         verbose=1,\n                                         save_best_only=True,\n                                         mode=\'max\')\n            callbacks_list = [checkpoint]\n\n            model_RNN, model_tmp = BuildModel.Build_Model_RNN_Text(word_index,\n                                                                   embeddings_index,\n                                                                   number_of_classes,\n                                                                   MAX_SEQUENCE_LENGTH,\n                                                                   EMBEDDING_DIM,\n                                                                   sparse_categorical,\n                                                                   min_hidden_layer_rnn,\n                                                                   max_hidden_layer_rnn,\n                                                                   min_nodes_rnn,\n                                                                   max_nodes_rnn,\n                                                                   random_optimizor,\n                                                                   dropout)\n\n            model_history = model_RNN.fit(x_train_embedded, y_train,\n                              validation_data=(x_test_embedded, y_test),\n                              epochs=epochs[1],\n                              batch_size=batch_size,\n                              callbacks=callbacks_list,\n                              verbose=2)\n            History.append(model_history)\n\n            if sparse_categorical:\n                model_tmp.load_weights(filepath)\n                model_tmp.compile(loss=\'sparse_categorical_crossentropy\',\n                                  optimizer=\'rmsprop\',\n                                  metrics=[\'accuracy\'])\n\n                y_pr_ = model_tmp.predict_classes(x_test_embedded, batch_size=batch_size)\n                y_pr.append(np.array(y_pr_))\n                score.append(accuracy_score(y_test, y_pr_))\n            else:\n                model_tmp.load_weights(filepath)\n                model_tmp.compile(loss=\'categorical_crossentropy\',\n                                  optimizer=\'rmsprop\',\n                                  metrics=[\'accuracy\'])\n                y_pr_ = model_tmp.predict(x_test_embedded, batch_size=batch_size)\n                y_pr_ = np.argmax(y_pr_, axis=1)\n                y_pr.append(np.array(y_pr_))\n                y_test_temp = np.argmax(y_test, axis=1)\n                score.append(accuracy_score(y_test_temp, y_pr_))\n            i += 1\n            del model_tmp\n            del model_RNN\n            gc.collect()\n        except:\n            print(""Error in model"", i, ""try to re-generate another model"")\n            if max_hidden_layer_rnn > 3:\n                max_hidden_layer_rnn -= 1\n            if max_nodes_rnn > 64:\n                max_nodes_rnn -= 2\n\n    gc.collect()\n\n    i = 0\n    while i < random_deep[2]:\n        try:\n            print(""CNN "" + str(i))\n\n            model_CNN, model_tmp = BuildModel.Build_Model_CNN_Text(word_index,\n                                                                   embeddings_index,\n                                                                   number_of_classes,\n                                                                   MAX_SEQUENCE_LENGTH,\n                                                                   EMBEDDING_DIM,\n                                                                   sparse_categorical,\n                                                                   min_hidden_layer_cnn,\n                                                                   max_hidden_layer_cnn,\n                                                                   min_nodes_cnn,\n                                                                   max_nodes_cnn,\n                                                                   random_optimizor,\n                                                                   dropout)\n\n\n\n            filepath = ""weights\\weights_CNN_"" + str(i) + "".hdf5""\n            checkpoint = ModelCheckpoint(filepath, monitor=\'val_accuracy\', verbose=1, save_best_only=True,\n                                         mode=\'max\')\n            callbacks_list = [checkpoint]\n\n            model_history = model_CNN.fit(x_train_embedded, y_train,\n                                          validation_data=(x_test_embedded, y_test),\n                                          epochs=epochs[2],\n                                          batch_size=batch_size,\n                                          callbacks=callbacks_list,\n                                          verbose=2)\n            History.append(model_history)\n\n            model_tmp.load_weights(filepath)\n            if sparse_categorical:\n                model_tmp.compile(loss=\'sparse_categorical_crossentropy\',\n                                  optimizer=\'rmsprop\',\n                                  metrics=[\'accuracy\'])\n            else:\n                model_tmp.compile(loss=\'categorical_crossentropy\',\n                                  optimizer=\'rmsprop\',\n                                  metrics=[\'accuracy\'])\n\n            y_pr_ = model_tmp.predict(x_test_embedded, batch_size=batch_size)\n            y_pr_ = np.argmax(y_pr_, axis=1)\n            y_pr.append(np.array(y_pr_))\n\n            if sparse_categorical:\n                score.append(accuracy_score(y_test, y_pr_))\n            else:\n                y_test_temp = np.argmax(y_test, axis=1)\n                score.append(accuracy_score(y_test_temp, y_pr_))\n            i += 1\n\n            del model_tmp\n            del model_CNN\n            gc.collect()\n        except:\n            print(""Error in model"", i, ""try to re-generate an other model"")\n            if max_hidden_layer_cnn > 5:\n                max_hidden_layer_cnn -= 1\n            if max_nodes_cnn > 128:\n                max_nodes_cnn -= 2\n                min_nodes_cnn -= 1\n\n    gc.collect()\n\n\n    y_proba = np.array(y_pr).transpose()\n\n    final_y = []\n\n    for i in range(0, y_proba.shape[0]):\n        a = np.array(y_proba[i, :])\n        a = collections.Counter(a).most_common()[0][0]\n        final_y.append(a)\n    if sparse_categorical:\n        F_score = accuracy_score(y_test, final_y)\n        F1 = precision_recall_fscore_support(y_test, final_y, average=\'micro\')\n        F2 = precision_recall_fscore_support(y_test, final_y, average=\'macro\')\n        F3 = precision_recall_fscore_support(y_test, final_y, average=\'weighted\')\n        cnf_matrix = confusion_matrix(y_test, final_y)\n        # Compute confusion matrix\n        # Plot non-normalized confusion matrix\n\n        if plot:\n            classes = list(range(0, np.max(y_test)+1))\n            Plot.plot_confusion_matrix(cnf_matrix, classes=classes,\n                                       title=\'Confusion matrix, without normalization\')\n\n            # Plot normalized confusion matrix\n\n            Plot.plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True,\n                                       title=\'Normalized confusion matrix\')\n    else:\n        y_test_temp = np.argmax(y_test, axis=1)\n        F_score = accuracy_score(y_test_temp, final_y)\n        F1 = precision_recall_fscore_support(y_test_temp, final_y, average=\'micro\')\n        F2 = precision_recall_fscore_support(y_test_temp, final_y, average=\'macro\')\n        F3 = precision_recall_fscore_support(y_test_temp, final_y, average=\'weighted\')\n    if plot:\n        Plot.RMDL_epoch(History)\n    print(y_proba.shape)\n    print(""Accuracy of"",len(score),""models:"",score)\n    print(""Accuracy:"",F_score)\n    print(""F1_Micro:"",F1)\n    print(""F1_Macro:"",F2)\n    print(""F1_weighted:"",F3)\n'"
RMDL/__init__.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nfrom __future__ import absolute_import\n\nfrom . import BuildModel\nfrom . import Global\nfrom . import RMDL_Text\nfrom . import RMDL_Image\nfrom . import text_feature_extraction\nfrom . import Plot\n\n__version__ = \'1.0.3\''"
RMDL/text_feature_extraction.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nnltk.download(""stopwords"")\ncachedStopWords = stopwords.words(""english"")\n\ndef transliterate(line):\n    cedilla2latin = [[u\'\xc3\x81\', u\'A\'], [u\'\xc3\xa1\', u\'a\'], [u\'\xc4\x8c\', u\'C\'], [u\'\xc4\x8d\', u\'c\'], [u\'\xc5\xa0\', u\'S\'], [u\'\xc5\xa1\', u\'s\']]\n    tr = dict([(a[0], a[1]) for (a) in cedilla2latin])\n    new_line = """"\n    for letter in line:\n        if letter in tr:\n            new_line += tr[letter]\n        else:\n            new_line += letter\n    return new_line\n\n\n\n\n\n\n\n\ndef text_cleaner(text,\n                 deep_clean=False,\n                 stem= True,\n                 stop_words=True,\n                 translite_rate=True):\n    rules = [\n        {r\'>\\s+\': u\'>\'},  # remove spaces after a tag opens or closes\n        {r\'\\s+\': u\' \'},  # replace consecutive spaces\n        {r\'\\s*<br\\s*/?>\\s*\': u\'\\n\'},  # newline after a <br>\n        {r\'</(div)\\s*>\\s*\': u\'\\n\'},  # newline after </p> and </div> and <h1/>...\n        {r\'</(p|h\\d)\\s*>\\s*\': u\'\\n\\n\'},  # newline after </p> and </div> and <h1/>...\n        {r\'<head>.*<\\s*(/head|body)[^>]*>\': u\'\'},  # remove <head> to </head>\n        {r\'<a\\s+href=""([^""]+)""[^>]*>.*</a>\': r\'\\1\'},  # show links instead of texts\n        {r\'[ \\t]*<[^<]*?/?>\': u\'\'},  # remove remaining tags\n        {r\'^\\s+\': u\'\'}  # remove spaces at the beginning\n\n    ]\n\n    if deep_clean:\n        text = text.replace(""."", """")\n        text = text.replace(""["", "" "")\n        text = text.replace("","", "" "")\n        text = text.replace(""]"", "" "")\n        text = text.replace(""("", "" "")\n        text = text.replace("")"", "" "")\n        text = text.replace(""\\"""", """")\n        text = text.replace(""-"", "" "")\n        text = text.replace(""="", "" "")\n        text = text.replace(""?"", "" "")\n        text = text.replace(""!"", "" "")\n\n        for rule in rules:\n            for (k, v) in rule.items():\n                regex = re.compile(k)\n                text = regex.sub(v, text)\n            text = text.rstrip()\n            text = text.strip()\n        text = text.replace(\'+\', \' \').replace(\'.\', \' \').replace(\',\', \' \').replace(\':\', \' \')\n        text = re.sub(""(^|\\W)\\d+($|\\W)"", "" "", text)\n        if translite_rate:\n            text = transliterate(text)\n        if stem:\n            text = PorterStemmer().stem(text)\n        text = WordNetLemmatizer().lemmatize(text)\n        if stop_words:\n            stop_words = set(stopwords.words(\'english\'))\n            word_tokens = word_tokenize(text)\n            text = [w for w in word_tokens if not w in stop_words]\n            text = \' \'.join(str(e) for e in text)\n    else:\n        for rule in rules:\n            for (k, v) in rule.items():\n                regex = re.compile(k)\n                text = regex.sub(v, text)\n            text = text.rstrip()\n            text = text.strip()\n    return text.lower()\n\ndef loadData_Tokenizer(X_train, X_test,GloVe_DIR,MAX_NB_WORDS,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM):\n    np.random.seed(7)\n    text = np.concatenate((X_train, X_test), axis=0)\n    text = np.array(text)\n    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n    tokenizer.fit_on_texts(text)\n    sequences = tokenizer.texts_to_sequences(text)\n    word_index = tokenizer.word_index\n    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    print(\'Found %s unique tokens.\' % len(word_index))\n    indices = np.arange(text.shape[0])\n    # np.random.shuffle(indices)\n    text = text[indices]\n    print(text.shape)\n    X_train = text[0:len(X_train), ]\n    X_test = text[len(X_train):, ]\n    embeddings_index = {}\n    f = open(GloVe_DIR, encoding=""utf8"")\n    for line in f:\n\n        values = line.split()\n        word = values[0]\n        try:\n            coefs = np.asarray(values[1:], dtype=\'float32\')\n        except:\n            pass\n        embeddings_index[word] = coefs\n    f.close()\n    print(\'Total %s word vectors.\' % len(embeddings_index))\n    return (X_train, X_test, word_index,embeddings_index)\n\ndef loadData(X_train, X_test,MAX_NB_WORDS=75000):\n    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n    X_train = vectorizer_x.fit_transform(X_train).toarray()\n    X_test = vectorizer_x.transform(X_test).toarray()\n    print(""tf-idf with"",str(np.array(X_train).shape[1]),""features"")\n    return (X_train,X_test)'"
RMDL/Download/Download_Glove.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\n\nfrom __future__ import print_function\n\nimport os, sys, tarfile\nimport numpy as np\nimport zipfile\n\nif sys.version_info >= (3, 0, 0):\n    import urllib.request as urllib  # ugly but works\nelse:\n    import urllib\n\nprint(sys.version_info)\n\n# image shape\n\n\n# path to the directory with the data\nDATA_DIR = \'.\\Glove\'\n\n# url of the binary data\n\n\n\n# path to the binary train file with image data\n\n\ndef download_and_extract(data=\'Wikipedia\'):\n    """"""\n    Download and extract the GloVe\n    :return: None\n    """"""\n\n    if data==\'Wikipedia\':\n        DATA_URL = \'http://nlp.stanford.edu/data/glove.6B.zip\'\n    elif data==\'Common_Crawl_840B\':\n        DATA_URL = \'http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip\'\n    elif data==\'Common_Crawl_42B\':\n        DATA_URL = \'http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\'\n    elif data==\'Twitter\':\n        DATA_URL = \'http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip\'\n    else:\n        print(""prameter should be Twitter, Common_Crawl_42B, Common_Crawl_840B, or Wikipedia"")\n        exit(0)\n\n\n    dest_directory = DATA_DIR\n    if not os.path.exists(dest_directory):\n        os.makedirs(dest_directory)\n    filename = DATA_URL.split(\'/\')[-1]\n    filepath = os.path.join(dest_directory, filename)\n    print(filepath)\n\n    path = os.path.abspath(dest_directory)\n    if not os.path.exists(filepath):\n        def _progress(count, block_size, total_size):\n            sys.stdout.write(\'\\rDownloading %s %.2f%%\' % (filename,\n                                                          float(count * block_size) / float(total_size) * 100.0))\n            sys.stdout.flush()\n\n        filepath, _ = urllib.urlretrieve(DATA_URL, filepath)#, reporthook=_progress)\n\n\n        zip_ref = zipfile.ZipFile(filepath, \'r\')\n        zip_ref.extractall(DATA_DIR)\n        zip_ref.close()\n    return path\n'"
RMDL/Download/Download_WOS.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\n\nfrom __future__ import print_function\n\nimport os, sys, tarfile\nimport numpy as np\n\nif sys.version_info >= (3, 0, 0):\n    import urllib.request as urllib  # ugly but works\nelse:\n    import urllib\n\nprint(sys.version_info)\n\n# image shape\n\n\n# path to the directory with the data\nDATA_DIR = \'.\\data_WOS\'\n\n# url of the binary data\nDATA_URL = \'http://kowsari.net/WebOfScience.tar.gz\'\n\n\n# path to the binary train file with image data\n\n\ndef download_and_extract():\n    """"""\n    Download and extract the WOS datasets\n    :return: None\n    """"""\n    dest_directory = DATA_DIR\n    if not os.path.exists(dest_directory):\n        os.makedirs(dest_directory)\n    filename = DATA_URL.split(\'/\')[-1]\n    filepath = os.path.join(dest_directory, filename)\n\n\n    path = os.path.abspath(dest_directory)\n    if not os.path.exists(filepath):\n        def _progress(count, block_size, total_size):\n            sys.stdout.write(\'\\rDownloading %s %.2f%%\' % (filename,\n                                                          float(count * block_size) / float(total_size) * 100.0))\n            sys.stdout.flush()\n\n        filepath, _ = urllib.urlretrieve(DATA_URL, filepath, reporthook=_progress)\n\n        print(\'Downloaded\', filename)\n\n        tarfile.open(filepath, \'r\').extractall(dest_directory)\n    return path\n'"
RMDL/Download/__init__.py,0,"b'""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\nRMDL: Random Multimodel Deep Learning for Classification\n\n* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n* Last Update: Oct 26, 2018\n* This file is part of  RMDL project, University of Virginia.\n* Free to use, change, share and distribute source code of RMDL\n* Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n* Link: https://dl.acm.org/citation.cfm?id=3206111\n* Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n* Link :  http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=79&id=823\n* Comments and Error: email: kk7nc@virginia.edu\n\n""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""\n\nfrom __future__ import absolute_import\n\nfrom . import Download_WOS\nfrom . import Download_Glove\n\n\n__version__ = \'1.0.3\''"
