file_path,api_count,code
tools/freeze_graph.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts checkpoint variables into Const ops in a standalone GraphDef file.\n\nThis script is designed to take a GraphDef proto, a SaverDef proto, and a set of\nvariable values stored in a checkpoint file, and output a GraphDef with all of\nthe variable ops converted into const ops containing the values of the\nvariables.\n\nIt\'s useful to do this when we need to load a single file in C++, especially in\nenvironments like mobile or embedded where we may not have access to the\nRestoreTensor ops and file loading calls that they rely on.\n\nAn example of command-line usage is:\nbazel build tensorflow/python/tools:freeze_graph && \\\nbazel-bin/tensorflow/python/tools/freeze_graph \\\n--input_graph=some_graph_def.pb \\\n--input_checkpoint=model.ckpt-8361242 \\\n--output_graph=/tmp/frozen_graph.pb --output_node_names=softmax\n\nYou can also look at freeze_graph_test.py for an example of how to use it.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nfrom google.protobuf import text_format\n\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.protobuf import saver_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.training import saver as saver_lib\n\nFLAGS = None\n\n\ndef freeze_graph(input_graph,\n                 input_saver,\n                 input_binary,\n                 input_checkpoint,\n                 output_node_names,\n                 restore_op_name,\n                 filename_tensor_name,\n                 output_graph,\n                 clear_devices,\n                 initializer_nodes,\n                 variable_names_blacklist=""""):\n  """"""Converts all variables in a graph and checkpoint into constants.""""""\n\n  if not gfile.Exists(input_graph):\n    print(""Input graph file \'"" + input_graph + ""\' does not exist!"")\n    return -1\n\n  if input_saver and not gfile.Exists(input_saver):\n    print(""Input saver file \'"" + input_saver + ""\' does not exist!"")\n    return -1\n\n  # \'input_checkpoint\' may be a prefix if we\'re using Saver V2 format\n  if not saver_lib.checkpoint_exists(input_checkpoint):\n    print(""Input checkpoint \'"" + input_checkpoint + ""\' doesn\'t exist!"")\n    return -1\n\n  if not output_node_names:\n    print(""You need to supply the name of a node to --output_node_names."")\n    return -1\n\n  input_graph_def = graph_pb2.GraphDef()\n  mode = ""rb"" if input_binary else ""r""\n  with gfile.FastGFile(input_graph, mode) as f:\n    if input_binary:\n      input_graph_def.ParseFromString(f.read())\n    else:\n      text_format.Merge(f.read().decode(""utf-8""), input_graph_def)\n  # Remove all the explicit device specifications for this node. This helps to\n  # make the graph more portable.\n  if clear_devices:\n    for node in input_graph_def.node:\n      node.device = """"\n  _ = importer.import_graph_def(input_graph_def, name="""")\n\n  with session.Session() as sess:\n    if input_saver:\n      with gfile.FastGFile(input_saver, mode) as f:\n        saver_def = saver_pb2.SaverDef()\n        if input_binary:\n          saver_def.ParseFromString(f.read())\n        else:\n          text_format.Merge(f.read(), saver_def)\n        saver = saver_lib.Saver(saver_def=saver_def)\n        saver.restore(sess, input_checkpoint)\n    else:\n      sess.run([restore_op_name], {filename_tensor_name: input_checkpoint})\n      if initializer_nodes:\n        sess.run(initializer_nodes)\n\n    variable_names_blacklist = (variable_names_blacklist.split("","") if\n                                variable_names_blacklist else None)\n    output_graph_def = graph_util.convert_variables_to_constants(\n        sess,\n        input_graph_def,\n        output_node_names.split("",""),\n        variable_names_blacklist=variable_names_blacklist)\n\n  with gfile.GFile(output_graph, ""wb"") as f:\n    f.write(output_graph_def.SerializeToString())\n  print(""%d ops in the final graph."" % len(output_graph_def.node))\n\n\ndef main(unused_args):\n  freeze_graph(FLAGS.input_graph, FLAGS.input_saver, FLAGS.input_binary,\n               FLAGS.input_checkpoint, FLAGS.output_node_names,\n               FLAGS.restore_op_name, FLAGS.filename_tensor_name,\n               FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes,\n               FLAGS.variable_names_blacklist)\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n  parser.add_argument(\n      ""--input_graph"",\n      type=str,\n      default="""",\n      help=""TensorFlow \\\'GraphDef\\\' file to load."")\n  parser.add_argument(\n      ""--input_saver"",\n      type=str,\n      default="""",\n      help=""TensorFlow saver file to load."")\n  parser.add_argument(\n      ""--input_checkpoint"",\n      type=str,\n      default="""",\n      help=""TensorFlow variables file to load."")\n  parser.add_argument(\n      ""--output_graph"",\n      type=str,\n      default="""",\n      help=""Output \\\'GraphDef\\\' file name."")\n  parser.add_argument(\n      ""--input_binary"",\n      nargs=""?"",\n      const=True,\n      type=""bool"",\n      default=False,\n      help=""Whether the input files are in binary format."")\n  parser.add_argument(\n      ""--output_node_names"",\n      type=str,\n      default="""",\n      help=""The name of the output nodes, comma separated."")\n  parser.add_argument(\n      ""--restore_op_name"",\n      type=str,\n      default=""save/restore_all"",\n      help=""The name of the master restore operator."")\n  parser.add_argument(\n      ""--filename_tensor_name"",\n      type=str,\n      default=""save/Const:0"",\n      help=""The name of the tensor holding the save path."")\n  parser.add_argument(\n      ""--clear_devices"",\n      nargs=""?"",\n      const=True,\n      type=""bool"",\n      default=True,\n      help=""Whether to remove device specifications."")\n  parser.add_argument(\n      ""--initializer_nodes"",\n      type=str,\n      default="""",\n      help=""comma separated list of initializer nodes to run before freezing."")\n  parser.add_argument(\n      ""--variable_names_blacklist"",\n      type=str,\n      default="""",\n      help=""""""\\\n      comma separated list of variables to skip converting to constants\\\n      """""")\n  FLAGS, unparsed = parser.parse_known_args()\n  app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
kcws/cc/dump_vocab.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2016-11-20 15:04:18\n# @Last Modified by:   Koth\n# @Last Modified time: 2016-11-20 15:07:51\nimport sys\nimport os\nimport w2v\n\n\ndef main(argc, argv):\n  if argc < 3:\n    print(""Usage:%s <word2vec_vocab_path> <output_path>"" % (argv[0]))\n    sys.exit(1)\n  vob = w2v.Word2vecVocab()\n  vob.Load(argv[1])\n  vob.DumpBasicVocab(argv[2])\n\n\nif __name__ == \'__main__\':\n  main(len(sys.argv), sys.argv)'"
kcws/cc/prepare_test_file.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2016-11-22 21:20:59\n# @Last Modified by:   Koth\n# @Last Modified time: 2016-11-22 21:39:22\n\nimport sys\nimport os\n\n\ndef main(argc, argv):\n    if argc < 3:\n        print(""Usage:%s <input> <output>"" % (argv[0]))\n        sys.exit(1)\n    inp = open(argv[1], ""r"")\n    oup = open(argv[2], ""w"")\n    totalLine = 0\n    while True:\n        line = inp.readline()\n        if not line:\n            break\n        line = line.strip()\n        if not line or len(line) == 0:\n            continue\n        ustr = unicode(line.decode(""utf8""))\n        if len(ustr) >= 80 or len(ustr) < 10:\n            continue\n        oup.write(""%s\\n"" % (line))\n        totalLine += 1\n    print(""totalLine:%d"" % (totalLine))\n\n\nif __name__ == \'__main__\':\n    main(len(sys.argv), sys.argv)\n'"
kcws/train/bilstm.py,19,"b'#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\n# File: bilstm.py\n# Project: /e/code/kcws\n# Created: Thu Aug 03 2017\n# Author: Koth Chen\n# Copyright (c) 2017 Koth\n#\n# <<licensetext>>\n\nimport tensorflow as tf\n\n\nclass Model:\n    def __init__(self,\n                 numHidden,\n                 maxSeqLen,\n                 numTags):\n        self.num_hidden = numHidden\n        self.num_tags = numTags\n        self.max_seq_len = maxSeqLen\n        self.W = tf.get_variable(\n            shape=[numHidden * 2, numTags],\n            initializer=tf.contrib.layers.xavier_initializer(),\n            name=""weights"",\n            regularizer=tf.contrib.layers.l2_regularizer(0.001))\n        self.b = tf.Variable(tf.zeros([numTags], name=""bias""))\n\n    def inference(self, X, length, reuse=False):\n        length_64 = tf.cast(length, tf.int64)\n        with tf.variable_scope(""bilstm"", reuse=reuse):\n            forward_output, _ = tf.nn.dynamic_rnn(\n                tf.contrib.rnn.LSTMCell(self.num_hidden,\n                                        reuse=reuse),\n                X,\n                dtype=tf.float32,\n                sequence_length=length,\n                scope=""RNN_forward"")\n            backward_output_, _ = tf.nn.dynamic_rnn(\n                tf.contrib.rnn.LSTMCell(self.num_hidden,\n                                        reuse=reuse),\n                inputs=tf.reverse_sequence(X,\n                                           length_64,\n                                           seq_dim=1),\n                dtype=tf.float32,\n                sequence_length=length,\n                scope=""RNN_backword"")\n\n        backward_output = tf.reverse_sequence(backward_output_,\n                                              length_64,\n                                              seq_dim=1)\n\n        output = tf.concat([forward_output, backward_output], 2)\n        output = tf.reshape(output, [-1, self.num_hidden * 2])\n        if reuse is None or not reuse:\n            output = tf.nn.dropout(output, 0.5)\n\n        matricized_unary_scores = tf.matmul(output, self.W) + self.b\n        unary_scores = tf.reshape(\n            matricized_unary_scores,\n            [-1, self.max_seq_len, self.num_tags],\n            name=""Reshape_7"" if reuse else None)\n        return unary_scores\n'"
kcws/train/filter_sentence.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2016-11-16 22:46:50\n# @Last Modified by:   Koth\n# @Last Modified time: 2016-11-21 22:40:47\nimport sys\nimport random\n\n\ndef main(argc, argv):\n    if argc < 2:\n        print(""Usage:%s <input>"" % (argv[0]))\n        sys.exit(1)\n    SENTENCE_LEN = 80\n    fp = open(argv[1], ""r"")\n    nl = 0\n    bad = 0\n    test = 0\n    tr_p = open(""train.txt"", ""w"")\n    te_p = open(""test.txt"", ""w"")\n    while True:\n        line = fp.readline()\n        if not line:\n            break\n        line = line.strip()\n        if not line:\n            continue\n        ss = line.split(\' \')\n\n        if len(ss) != (2 * SENTENCE_LEN):\n            print(""len is:%d"" % (len(ss)))\n            continue\n        numV = 0\n        for i in range(SENTENCE_LEN):\n            if int(ss[i]) != 0:\n                numV += 1\n                if numV > 2:\n                    break\n        if numV <= 2:\n            bad += 1\n        else:\n            r = random.random()\n            if r <= 0.02 and test < 8000:\n                te_p.write(""%s\\n"" % (line))\n                test += 1\n            else:\n                tr_p.write(""%s\\n"" % (line))\n        nl += 1\n    fp.close()\n    print(""got bad:%d"" % (bad))\n\n\nif __name__ == \'__main__\':\n    main(len(sys.argv), sys.argv)\n'"
kcws/train/generate_char_embedding.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2016-11-30 19:59:15\n# @Last Modified by:   Koth\n# @Last Modified time: 2016-11-30 20:58:29\nimport sys\nimport w2v\n\nSEQ_LEN = 5\n\n\ndef processFile(inp, oup, vob):\n  global SEQ_LEN\n  while True:\n    line = inp.readline()\n    if not line:\n      break\n    line = line.strip()\n    if not line:\n      continue\n    ss = line.split(""  "")\n    x = []\n    y = []\n    for s in ss:\n      ustr = unicode(s.decode(""utf-8""))\n      if len(ustr) < 1:\n        continue\n      nn = len(ustr)\n      for i in range(nn):\n        theStr = str(ustr[i].encode(""utf8""))\n        x.append(str(vob.GetWordIndex(theStr)))\n        if i == (nn - 1):\n          y.append(1)\n        else:\n          y.append(0)\n    nn = len(x)\n    for i in range(nn):\n      seqLen = SEQ_LEN\n      if y[i] == 1:\n        seqLen = 2\n      hasStop = (y[i] == 1)\n      for j in range(1, seqLen):\n        if (i + j + 1) > nn:\n          continue\n        newX = x[i:i + j + 1]\n        for k in range(j + 1, SEQ_LEN):\n          newX.append(""0"")\n        newY = 0\n        if y[i + j] == 1:\n          if not hasStop:\n            newY = 1\n          hasStop = True\n        line = "" "".join(newX)\n        line += "" "" + str(newY)\n        oup.write(""%s\\n"" % (line))\n\n\ndef main(argc, argv):\n  if argc < 4:\n    print(""Usage: %s <input>  <output> <vec>"" % (argv[0]))\n    sys.exit(1)\n  vob = w2v.Word2vecVocab()\n  vob.Load(argv[3])\n  inp = open(argv[1], ""r"")\n  oup = open(argv[2], ""w"")\n  processFile(inp, oup, vob)\n\n\nif __name__ == \'__main__\':\n  main(len(sys.argv), sys.argv)'"
kcws/train/generate_pos_train.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2017-01-25 16:30:27\n# @Last Modified by:   Koth\n# @Last Modified time: 2017-01-31 11:52:33\n\nimport sys\nimport os\nimport w2v\n\ntotalLine = 0\nlongLine = 0\n\nMAX_LEN = 50\ntotalChars = 0\n\n\nclass Sentence:\n  def __init__(self):\n    self.tokens = []\n    self.markWrong = False\n\n  def addToken(self, token):\n    self.tokens.append(token)\n\n  def generate_train_line(self, out, word_vob, char_vob):\n    nl = len(self.tokens)\n    if nl < 3:\n      return\n    wordi = []\n    chari = []\n    labeli = []\n    if nl > MAX_LEN:\n      nl = MAX_LEN\n    for ti in range(nl):\n      t = self.tokens[ti]\n      idx = word_vob.GetWordIndex(t.token)\n      wordi.append(str(idx))\n      labeli.append(str(t.posTag))\n      nc = len(t.chars)\n      if nc > 5:\n        lc = t.chars[nc - 1]\n        t.chars[4] = lc\n        nc = 5\n      for i in range(nc):\n        idx = char_vob.GetWordIndex(str(t.chars[i].encode(""utf8"")))\n        chari.append(str(idx))\n      for i in range(nc, 5):\n        chari.append(""0"")\n    for i in range(nl, MAX_LEN):\n      wordi.append(""0"")\n      labeli.append(""0"")\n      for ii in range(5):\n        chari.append(str(ii))\n    line = "" "".join(wordi)\n    line += "" ""\n    line += "" "".join(chari)\n    line += "" ""\n    line += "" "".join(labeli)\n    out.write(""%s\\n"" % (line))\n\n\nclass Token:\n  def __init__(self, token, posTag):\n    self.token = token\n    ustr = unicode(token.decode(\'utf8\'))\n    self.chars = []\n    for u in ustr:\n      self.chars.append(u)\n    self.posTag = posTag\n\n\ndef processToken(token, sentence, out, end, word_vob, char_vob, pos_vob):\n  global totalLine\n  global longLine\n  global totalChars\n  global MAX_LEN\n  nn = len(token)\n  while nn > 0 and token[nn - 1] != \'/\':\n    nn = nn - 1\n  pos = token[nn:]\n  if (not pos[0:1].isalpha()) or pos[0:1].isupper():\n    sentence.markWrong = True\n    return False\n  if len(pos) > 2:\n    pos = pos[:2]\n  if pos not in pos_vob:\n    print(""mark wrong for:[%s]"" % (pos))\n    sentence.markWrong = True\n    return False\n  token = token[:nn - 1].strip()\n  sentence.addToken(Token(token, pos_vob[pos]))\n  return True\n\n\ndef processLine(line, out, word_vob, char_vob, pos_vob):\n  line = line.strip()\n  nn = len(line)\n  seeLeftB = False\n  start = 0\n  sentence = Sentence()\n  try:\n    for i in range(nn):\n      if line[i] == \' \':\n        if not seeLeftB:\n          token = line[start:i]\n          if token.startswith(\'[\'):\n            tokenLen = len(token)\n            while tokenLen > 0 and token[tokenLen - 1] != \']\':\n              tokenLen = tokenLen - 1\n            token = token[1:tokenLen - 1]\n            ss = token.split(\' \')\n            for s in ss:\n              processToken(s, sentence, out, False, word_vob, char_vob,\n                           pos_vob)\n          else:\n            processToken(token, sentence, out, False, word_vob, char_vob,\n                         pos_vob)\n          start = i + 1\n      elif line[i] == \'[\':\n        seeLeftB = True\n      elif line[i] == \']\':\n        seeLeftB = False\n    if start < nn:\n      token = line[start:]\n      if token.startswith(\'[\'):\n        tokenLen = len(token)\n        while tokenLen > 0 and token[tokenLen - 1] != \']\':\n          tokenLen = tokenLen - 1\n        token = token[1:tokenLen - 1]\n        ss = token.split(\' \')\n        ns = len(ss)\n        for i in range(ns - 1):\n          processToken(ss[i], sentence, out, False, word_vob, char_vob,\n                       pos_vob)\n        processToken(ss[-1], sentence, out, True, word_vob, char_vob, pos_vob)\n      else:\n        processToken(token, sentence, out, True, word_vob, char_vob, pos_vob)\n    if (not sentence.markWrong) and len(sentence.tokens) > 0:\n      # OKay,generating training line\n      sentence.generate_train_line(out, word_vob, char_vob)\n  except Exception as e:\n    raise (e)\n    pass\n\n\ndef loadPosVob(path, vob):\n  fp = open(path, ""r"")\n  for line in fp.readlines():\n    line = line.strip()\n    if not line:\n      continue\n    ss = line.split(""\\t"")\n    vob[ss[0]] = int(ss[1])\n  pass\n\n\ndef main(argc, argv):\n  global totalLine\n  global longLine\n  global totalChars\n  if argc < 6:\n    print(""Usage:%s <word_vob> <char_vob> <pos_vob>  <dir> <output>"" %\n          (argv[0]))\n    sys.exit(1)\n  wvobPath = argv[1]\n  cvobpath = argv[2]\n  pvobPath = argv[3]\n  rootDir = argv[4]\n  word_vob = w2v.Word2vecVocab()\n  word_vob.Load(wvobPath)\n  char_vob = w2v.Word2vecVocab()\n  char_vob.Load(cvobpath)\n  posVob = {}\n  loadPosVob(pvobPath, posVob)\n  out = open(argv[5], ""w"")\n  for dirName, subdirList, fileList in os.walk(rootDir):\n    curDir = os.path.join(rootDir, dirName)\n    for file in fileList:\n      if file.endswith("".txt""):\n        curFile = os.path.join(curDir, file)\n        #print(""processing:%s"" % (curFile))\n        fp = open(curFile, ""r"")\n        for line in fp.readlines():\n          line = line.strip()\n          processLine(line, out, word_vob, char_vob, posVob)\n        fp.close()\n  out.close()\n  print(""total:%d, long lines:%d, chars:%d"" %\n        (totalLine, longLine, totalChars))\n\n\nif __name__ == \'__main__\':\n  main(len(sys.argv), sys.argv)'"
kcws/train/generate_train_free.py,0,"b'#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\n# File: generate_train_free.py\n# Project: /e/code/kcws\n# Created: Thu Jul 27 2017\n# Author: Koth Chen\n# Copyright (c) 2017 Koth\n#\n# <<licensetext>>\n\n\nimport sys\nimport os\nimport w2v\nimport fire\nfrom sentence import Sentence\n\ntotalLine = 0\nlongLine = 0\n\nMAX_LEN = 80\ntotalChars = 0\n\n\ndef processLine(line, vob, out):\n    global totalLine\n    global longLine\n    global totalChars\n    ss = line.split(""\\t"")\n\n    sentence = Sentence()\n    nn = len(ss)\n    for i in range(nn):\n        ts = ss[i].split("" "")\n        ustr = unicode(ts[0].decode(\'utf8\'))\n        sentence.addToken(ustr)\n    if sentence.chars > MAX_LEN:\n        longLine += 1\n    else:\n        x = []\n        y = []\n        totalChars += sentence.chars\n        sentence.generate_tr_line(x, y, vob)\n        nn = len(x)\n        assert (nn == len(y))\n        for j in range(nn, MAX_LEN):\n            x.append(0)\n            y.append(0)\n            line = \'\'\n        for i in range(MAX_LEN):\n            if i > 0:\n                line += "" ""\n            line += str(x[i])\n        for j in range(MAX_LEN):\n            line += "" "" + str(y[j])\n        out.write(""%s\\n"" % (line))\n    totalLine += 1\n\n\ndef doGen(inputPath, outputPath, vocabPath):\n    global totalLine\n    global longLine\n    global totalChars\n    vob = w2v.Word2vecVocab()\n    vob.Load(vocabPath)\n    with open(inputPath, ""r"") as inp:\n        with open(outputPath, ""w"") as out:\n            for line in inp.readlines():\n                line = line.strip()\n                if not line:\n                    continue\n                processLine(line, vob, out)\n    print(""total:%d, long lines:%d, chars:%d"" %\n          (totalLine, longLine, totalChars))\n\n\ndef main():\n    fire.Fire()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
kcws/train/generate_training.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth Chen\n# @Date:   2016-10-21 16:17:53\n# @Last Modified by:   Koth\n# @Last Modified time: 2017-01-25 16:54:11\n\nimport sys\nimport os\nimport w2v\nfrom sentence import Sentence\n\ntotalLine = 0\nlongLine = 0\n\nMAX_LEN = 80\ntotalChars = 0\n\n\ndef processToken(token, sentence, out, end, vob):\n    global totalLine\n    global longLine\n    global totalChars\n    global MAX_LEN\n    nn = len(token)\n    while nn > 0 and token[nn - 1] != \'/\':\n        nn = nn - 1\n\n    token = token[:nn - 1].strip()\n    if token != \'\xe3\x80\x82\':\n        ustr = unicode(token.decode(\'utf8\'))\n        sentence.addToken(ustr)\n    uline = u\'\'\n    if token == \'\xe3\x80\x82\' or end:\n        if sentence.chars > MAX_LEN:\n            longLine += 1\n        else:\n            x = []\n            y = []\n            totalChars += sentence.chars\n            sentence.generate_tr_line(x, y, vob)\n            nn = len(x)\n            assert (nn == len(y))\n            for j in range(nn, MAX_LEN):\n                x.append(0)\n                y.append(0)\n            line = \'\'\n            for i in range(MAX_LEN):\n                if i > 0:\n                    line += "" ""\n                line += str(x[i])\n            for j in range(MAX_LEN):\n                line += "" "" + str(y[j])\n            out.write(""%s\\n"" % (line))\n        totalLine += 1\n        sentence.clear()\n\n\ndef processLine(line, out, vob):\n    line = line.strip()\n    nn = len(line)\n    seeLeftB = False\n    start = 0\n    sentence = Sentence()\n    try:\n        for i in range(nn):\n            if line[i] == \' \':\n                if not seeLeftB:\n                    token = line[start:i]\n                    if token.startswith(\'[\'):\n                        tokenLen = len(token)\n                        while tokenLen > 0 and token[tokenLen - 1] != \']\':\n                            tokenLen = tokenLen - 1\n                        token = token[1:tokenLen - 1]\n                        ss = token.split(\' \')\n                        for s in ss:\n                            processToken(s, sentence, out, False, vob)\n                    else:\n                        processToken(token, sentence, out, False, vob)\n                    start = i + 1\n            elif line[i] == \'[\':\n                seeLeftB = True\n            elif line[i] == \']\':\n                seeLeftB = False\n        if start < nn:\n            token = line[start:]\n            if token.startswith(\'[\'):\n                tokenLen = len(token)\n                while tokenLen > 0 and token[tokenLen - 1] != \']\':\n                    tokenLen = tokenLen - 1\n                token = token[1:tokenLen - 1]\n                ss = token.split(\' \')\n                ns = len(ss)\n                for i in range(ns - 1):\n                    processToken(ss[i], sentence, out, False, vob)\n                processToken(ss[-1], sentence, out, True, vob)\n            else:\n                processToken(token, sentence, out, True, vob)\n    except Exception as e:\n        pass\n\n\ndef main(argc, argv):\n    global totalLine\n    global longLine\n    global totalChars\n    if argc < 4:\n        print(""Usage:%s <vob> <dir> <output>"" % (argv[0]))\n        sys.exit(1)\n    vobPath = argv[1]\n    rootDir = argv[2]\n    vob = w2v.Word2vecVocab()\n    vob.Load(vobPath)\n    out = open(argv[3], ""w"")\n    for dirName, subdirList, fileList in os.walk(rootDir):\n        curDir = os.path.join(rootDir, dirName)\n        for file in fileList:\n            if file.endswith("".txt""):\n                curFile = os.path.join(curDir, file)\n                #print(""processing:%s"" % (curFile))\n                fp = open(curFile, ""r"")\n                for line in fp.readlines():\n                    line = line.strip()\n                    processLine(line, out, vob)\n                fp.close()\n    out.close()\n    print(""total:%d, long lines:%d, chars:%d"" %\n          (totalLine, longLine, totalChars))\n\n\nif __name__ == \'__main__\':\n    main(len(sys.argv), sys.argv)\n'"
kcws/train/idcnn.py,22,"b'#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\n# File: idcnn.py\n# Project: /Users/tech/code/kcws\n# Created: Mon Jul 31 2017\n# Author: Koth Chen\n# Copyright (c) 2017 Koth\n#\n# <<licensetext>>\n\nimport tensorflow as tf\n\n\nclass Model:\n    def __init__(self,\n                 layers,\n                 filterWidth,\n                 numFilter,\n                 embeddingDim,\n                 maxSeqLen,\n                 numTags,\n                 repeatTimes=4):\n        self.layers = layers\n        self.filter_width = filterWidth\n        self.num_filter = numFilter\n        self.embedding_dim = embeddingDim\n        self.repeat_times = repeatTimes\n        self.num_tags = numTags\n        self.max_seq_len = maxSeqLen\n\n    def inference(self, X, reuse=False):\n        with tf.variable_scope(""idcnn"", reuse=reuse):\n            filter_weights = tf.get_variable(\n                ""idcnn_filter"",\n                shape=[1, self.filter_width, self.embedding_dim,\n                       self.num_filter],\n                initializer=tf.contrib.layers.xavier_initializer())\n            layerInput = tf.nn.conv2d(X,\n                                      filter_weights,\n                                      strides=[1, 1, 1, 1],\n                                      padding=""SAME"",\n                                      name=""init_layer"")\n            finalOutFromLayers = []\n            totalWidthForLastDim = 0\n            for j in range(self.repeat_times):\n                for i in range(len(self.layers)):\n                    dilation = self.layers[i][\'dilation\']\n                    isLast = True if i == (len(self.layers) - 1) else False\n                    with tf.variable_scope(""atrous-conv-layer-%d"" % i,\n                                           reuse=True\n                                           if (reuse or j > 0) else False):\n                        w = tf.get_variable(\n                            ""filterW"",\n                            shape=[1, self.filter_width, self.num_filter,\n                                   self.num_filter],\n                            initializer=tf.contrib.layers.xavier_initializer())\n                        b = tf.get_variable(""filterB"", shape=[self.num_filter])\n                        conv = tf.nn.atrous_conv2d(layerInput,\n                                                   w,\n                                                   rate=dilation,\n                                                   padding=""SAME"")\n                        conv = tf.nn.bias_add(conv, b)\n                        conv = tf.nn.relu(conv)\n                        if isLast:\n                            finalOutFromLayers.append(conv)\n                            totalWidthForLastDim += self.num_filter\n                        layerInput = conv\n            finalOut = tf.concat(axis=3, values=finalOutFromLayers)\n            keepProb = 1.0 if reuse else 0.5\n            finalOut = tf.nn.dropout(finalOut, keepProb)\n\n            finalOut = tf.squeeze(finalOut, [1])\n            finalOut = tf.reshape(finalOut, [-1, totalWidthForLastDim])\n\n            finalW = tf.get_variable(\n                ""finalW"",\n                shape=[totalWidthForLastDim, self.num_tags],\n                initializer=tf.contrib.layers.xavier_initializer())\n\n            finalB = tf.get_variable(""finalB"",\n                                     initializer=tf.constant(\n                                         0.001, shape=[self.num_tags]))\n\n            scores = tf.nn.xw_plus_b(finalOut, finalW, finalB, name=""scores"")\n        if reuse:\n            scores = tf.reshape(scores, [-1, self.max_seq_len, self.num_tags],\n                                name=""Reshape_7"")\n        else:\n            scores = tf.reshape(scores, [-1, self.max_seq_len, self.num_tags],\n                                name=None)\n        return scores\n'"
kcws/train/merge_vec.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2016-12-02 13:02:30\n# @Last Modified by:   Koth\n# @Last Modified time: 2016-12-02 13:35:42\nimport sys\n\n\ndef main(argc, argv):\n  if argc < 3:\n    print(""Usage:%s <w2v> <glove>"" % (argv[0]))\n    sys.exit(1)\n  inwp = open(argv[1], ""r"")\n  ingp = open(argv[2], ""r"")\n  oup = open(""merged_vec.txt"", ""w"")\n  inwp.readline()\n  fmap = {}\n  n1 = 0\n  n2 = 0\n  k1 = -1\n  k2 = -1\n  while True:\n    line = inwp.readline()\n    if not line:\n      break\n    n1 += 1\n    line = line.strip()\n    ss = line.split(\' \')\n    nn = len(ss)\n    if k1 == -1:\n      k1 = nn - 1\n    else:\n      assert (k1 == (nn - 1))\n    if ss[0] == \'</s>\':\n      ss[0] = \'<unk>\'\n    fv = "" "".join(ss[1:])\n    fmap[ss[0]] = fv\n  while True:\n    line = ingp.readline()\n    if not line:\n      break\n    n2 += 1\n    line = line.strip()\n    ss = line.split(\' \')\n    nn = len(ss)\n    if k2 == -1:\n      k2 = nn - 1\n    else:\n      assert (k2 == (nn - 1))\n    assert (ss[0] in fmap)\n    fv = "" "".join(ss[1:])\n    fmap[ss[0]] += "" "" + fv\n  assert (n1 == n2)\n  oup.write(""%d %d\\n"" % (n1, k1 + k2))\n  fv = fmap[""<unk>""]\n  oup.write(""<unk> %s\\n"" % (fv))\n  for k, v in fmap.iteritems():\n    if k == \'<unk>\':\n      continue\n    oup.write(""%s %s\\n"" % (k, v))\n  oup.close()\n\n\nif __name__ == \'__main__\':\n  main(len(sys.argv), sys.argv)\n'"
kcws/train/prepare_pos.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2017-01-25 11:46:37\n# @Last Modified by:   Koth\n# @Last Modified time: 2017-01-25 12:05:16\n\nimport sys\nimport os\n\ntotalLine = 0\nlongLine = 0\nmaxLen = 80\n\n\ndef processToken(token, collect, out, end):\n  global totalLine\n  global longLine\n  global maxLen\n  nn = len(token)\n  #print token\n  while nn > 0 and token[nn - 1] != \'/\':\n    nn = nn - 1\n\n  token = token[:nn - 1].strip()\n  if not token:\n    return\n  out.write(""%s "" % (token))\n  if end:\n    out.write(""\\n"")\n\n\ndef processLine(line, out):\n  line = line.strip()\n  nn = len(line)\n  seeLeftB = False\n  start = 0\n  collect = []\n  try:\n    for i in range(nn):\n      if line[i] == \' \':\n        if not seeLeftB:\n          token = line[start:i]\n          if token.startswith(\'[\'):\n            tokenLen = len(token)\n            while tokenLen > 0 and token[tokenLen - 1] != \']\':\n              tokenLen = tokenLen - 1\n            token = token[1:tokenLen - 1]\n            ss = token.split(\' \')\n            for s in ss:\n              processToken(s, collect, out, False)\n          else:\n            processToken(token, collect, out, False)\n          start = i + 1\n      elif line[i] == \'[\':\n        seeLeftB = True\n      elif line[i] == \']\':\n        seeLeftB = False\n    if start < nn:\n      token = line[start:]\n      if token.startswith(\'[\'):\n        tokenLen = len(token)\n        while tokenLen > 0 and token[tokenLen - 1] != \']\':\n          tokenLen = tokenLen - 1\n        token = token[1:tokenLen - 1]\n        ss = token.split(\' \')\n        ns = len(ss)\n        for i in range(ns - 1):\n          processToken(ss[i], collect, out, False)\n        processToken(ss[-1], collect, out, True)\n      else:\n        processToken(token, collect, out, True)\n  except Exception as e:\n    pass\n\n\ndef main(argc, argv):\n  global totalLine\n  global longLine\n  if argc < 3:\n    print(""Usage:%s <dir> <output>"" % (argv[0]))\n    sys.exit(1)\n  rootDir = argv[1]\n  out = open(argv[2], ""w"")\n  for dirName, subdirList, fileList in os.walk(rootDir):\n    curDir = os.path.join(rootDir, dirName)\n    for file in fileList:\n      if file.endswith("".txt""):\n        curFile = os.path.join(curDir, file)\n        # print(""processing:%s"" % (curFile))\n        fp = open(curFile, ""r"")\n        for line in fp.readlines():\n          line = line.strip()\n          processLine(line, out)\n        fp.close()\n  out.close()\n  print(""total:%d, long lines:%d"" % (totalLine, longLine))\n\n\nif __name__ == \'__main__\':\n  main(len(sys.argv), sys.argv)\n'"
kcws/train/process_anno_file.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth Chen\n# @Date:   2016-10-15 14:49:40\n# @Last Modified by:   Koth\n# @Last Modified time: 2016-12-09 20:33:30\nimport sys\nimport os\n\ntotalLine = 0\nlongLine = 0\nmaxLen = 80\n\n\ndef processToken(token, collect, out, end):\n  global totalLine\n  global longLine\n  global maxLen\n  nn = len(token)\n  #print token\n  while nn > 0 and token[nn - 1] != \'/\':\n    nn = nn - 1\n\n  token = token[:nn - 1].strip()\n  ustr = unicode(token.decode(\'utf8\'))\n  for u in ustr:\n    collect.append(u)\n  uline = u\'\'\n  if token == \'\xe3\x80\x82\' or end:\n    if len(collect) > maxLen:\n      longLine += 1\n    totalLine += 1\n    for s in collect:\n      if uline:\n        uline = uline + u"" "" + s\n      else:\n        uline = s\n    out.write(""%s\\n"" % (str(uline.encode(\'utf8\'))))\n    del collect[:]\n\n\ndef processLine(line, out):\n  line = line.strip()\n  nn = len(line)\n  seeLeftB = False\n  start = 0\n  collect = []\n  try:\n    for i in range(nn):\n      if line[i] == \' \':\n        if not seeLeftB:\n          token = line[start:i]\n          if token.startswith(\'[\'):\n            tokenLen = len(token)\n            while tokenLen > 0 and token[tokenLen - 1] != \']\':\n              tokenLen = tokenLen - 1\n            token = token[1:tokenLen - 1]\n            ss = token.split(\' \')\n            for s in ss:\n              processToken(s, collect, out, False)\n          else:\n            processToken(token, collect, out, False)\n          start = i + 1\n      elif line[i] == \'[\':\n        seeLeftB = True\n      elif line[i] == \']\':\n        seeLeftB = False\n    if start < nn:\n      token = line[start:]\n      if token.startswith(\'[\'):\n        tokenLen = len(token)\n        while tokenLen > 0 and token[tokenLen - 1] != \']\':\n          tokenLen = tokenLen - 1\n        token = token[1:tokenLen - 1]\n        ss = token.split(\' \')\n        ns = len(ss)\n        for i in range(ns - 1):\n          processToken(ss[i], collect, out, False)\n        processToken(ss[-1], collect, out, True)\n      else:\n        processToken(token, collect, out, True)\n  except Exception as e:\n    pass\n\n\ndef main(argc, argv):\n  global totalLine\n  global longLine\n  if argc < 3:\n    print(""Usage:%s <dir> <output>"" % (argv[0]))\n    sys.exit(1)\n  rootDir = argv[1]\n  out = open(argv[2], ""w"")\n  for dirName, subdirList, fileList in os.walk(rootDir):\n    curDir = os.path.join(rootDir, dirName)\n    for file in fileList:\n      if file.endswith("".txt""):\n        curFile = os.path.join(curDir, file)\n        # print(""processing:%s"" % (curFile))\n        fp = open(curFile, ""r"")\n        for line in fp.readlines():\n          line = line.strip()\n          processLine(line, out)\n        fp.close()\n  out.close()\n  print(""total:%d, long lines:%d"" % (totalLine, longLine))\n\n\nif __name__ == \'__main__\':\n  main(len(sys.argv), sys.argv)'"
kcws/train/process_icwb.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2016-11-27 12:01:18\n# @Last Modified by:   Koth\n# @Last Modified time: 2016-11-27 20:26:31\nimport sys\nimport w2v\n\nSEQ_LEN = 80\n\n\ndef processToken(x, y, tok, vob):\n  if len(tok) == 1:\n    x.append(vob.GetWordIndex(str(tok[0].encode(""utf8""))))\n    y.append(0)\n  else:\n    nn = len(tok)\n    for i in range(nn):\n      x.append(vob.GetWordIndex(str(tok[i].encode(""utf8""))))\n      if i == 0:\n        y.append(1)\n      elif i == (nn - 1):\n        y.append(3)\n      else:\n        y.append(2)\n\n\ndef processFile(inp, oup, mode, vob):\n  global SEQ_LEN\n  while True:\n    line = inp.readline()\n    if not line:\n      break\n    line = line.strip()\n    if not line:\n      continue\n    ss = line.split(""  "")\n    oline = """"\n    x = []\n    y = []\n    for s in ss:\n      ustr = unicode(s.decode(""utf-8""))\n      if len(ustr) < 1:\n        continue\n      if mode == 0:\n        for i in range(len(ustr)):\n          oline += str(ustr[i].encode(""utf8""))\n          oline += "" ""\n      else:\n        processToken(x, y, ustr, vob)\n    if mode != 0:\n      nn = len(x)\n      for i in range(nn, SEQ_LEN):\n        x.append(0)\n        y.append(0)\n      for i in range(SEQ_LEN):\n        oline += str(x[i]) + "" ""\n      for i in range(SEQ_LEN):\n        oline += str(y[i]) + "" ""\n    olen = len(oline)\n    oline = oline[:olen - 1]\n    oup.write(""%s\\n"" % (oline))\n\n\ndef main(argc, argv):\n  if argc < 3:\n    print(\n        ""Usage: %s <input>  <output> [model | 0 for w2v , 1 for training]  [vec_path | if mode if not 0]""\n        % (argv[0]))\n    sys.exit(1)\n  mode = 0\n  vob = None\n  if argc > 4:\n    mode = int(argv[3])\n    vob = w2v.Word2vecVocab()\n    vob.Load(argv[4])\n  inp = open(argv[1], ""r"")\n  oup = open(argv[2], ""w"")\n  processFile(inp, oup, mode, vob)\n\n\nif __name__ == \'__main__\':\n  main(len(sys.argv), sys.argv)'"
kcws/train/process_people.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2016-11-29 09:20:36\n# @Last Modified by:   Koth\n# @Last Modified time: 2016-11-29 15:58:30\n\nimport sys\nimport w2v\n\nSEQ_LEN = 80\n\n\ndef processToken(x, y, tok, vob):\n  if len(tok) == 1:\n    x.append(vob.GetWordIndex(str(tok[0].encode(""utf8""))))\n    y.append(0)\n  else:\n    nn = len(tok)\n    for i in range(nn):\n      x.append(vob.GetWordIndex(str(tok[i].encode(""utf8""))))\n      if i == 0:\n        y.append(1)\n      elif i == (nn - 1):\n        y.append(3)\n      else:\n        y.append(2)\n\n\ndef processFile(inp, oup, mode, vob):\n  global SEQ_LEN\n  while True:\n    line = inp.readline()\n    if not line:\n      break\n    line = line.strip()\n    if not line:\n      continue\n    ss = line.split(""  "")\n    oline = """"\n    x = []\n    y = []\n    for s in ss:\n      pos = s.find(""/"")\n      if not pos:\n        print(""fatal error \'/\' not found"")\n        sys.exit(0)\n      s = s[:pos]\n      ustr = unicode(s.decode(""utf-8""))\n      if len(ustr) < 1:\n        continue\n      if mode == 0:\n        for i in range(len(ustr)):\n          oline += str(ustr[i].encode(""utf8""))\n          oline += "" ""\n      else:\n        processToken(x, y, ustr, vob)\n    if mode != 0:\n      nn = len(x)\n      for i in range(nn, SEQ_LEN):\n        x.append(0)\n        y.append(0)\n      for i in range(SEQ_LEN):\n        oline += str(x[i]) + "" ""\n      for i in range(SEQ_LEN):\n        oline += str(y[i]) + "" ""\n    olen = len(oline)\n    oline = oline[:olen - 1]\n    oup.write(""%s\\n"" % (oline))\n\n\ndef main(argc, argv):\n  if argc < 3:\n    print(\n        ""Usage: %s <input>  <output> [model | 0 for w2v , 1 for training]  [vec_path | if mode if not 0]""\n        % (argv[0]))\n    sys.exit(1)\n  mode = 0\n  vob = None\n  if argc > 4:\n    mode = int(argv[3])\n    vob = w2v.Word2vecVocab()\n    vob.Load(argv[4])\n  inp = open(argv[1], ""r"")\n  oup = open(argv[2], ""w"")\n  processFile(inp, oup, mode, vob)\n\n\nif __name__ == \'__main__\':\n  main(len(sys.argv), sys.argv)'"
kcws/train/replace_unk.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2016-12-09 19:37:43\n# @Last Modified by:   Koth\n# @Last Modified time: 2016-12-09 19:49:37\nimport sys\n\n\ndef main(argc, argv):\n  if argc < 4:\n    print(""Usage:%s <vob> <input> <output>"" % (argv[0]))\n    sys.exit(1)\n  vp = open(argv[1], ""r"")\n  inp = open(argv[2], ""r"")\n  oup = open(argv[3], ""w"")\n  vobsMap = {}\n  for line in vp:\n    line = line.strip()\n    ss = line.split("" "")\n    vobsMap[ss[0]] = 1\n  while True:\n    line = inp.readline()\n    if not line:\n      break\n    line = line.strip()\n    if not line:\n      continue\n    ss = line.split("" "")\n    tokens = []\n    for s in ss:\n      if s in vobsMap:\n        tokens.append(s)\n      else:\n        tokens.append(""<UNK>"")\n    oup.write(""%s\\n"" % ("" "".join(tokens)))\n  oup.close()\n  inp.close()\n  vp.close()\n\n\nif __name__ == \'__main__\':\n  main(len(sys.argv), sys.argv)'"
kcws/train/sampling_for_train.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2016-12-01 09:30:11\n# @Last Modified by:   Koth\n# @Last Modified time: 2016-12-01 10:19:15\nimport sys\nimport random\n\n\ndef main(argc, argv):\n  if argc < 2:\n    print(""Usage: %s <input>"" % (argv[0]))\n    sys.exit(1)\n  inp = open(argv[1], ""r"")\n  trp = open(""train.txt"", ""w"")\n  tep = open(""test.txt"", ""w"")\n  sampleNum = 5000\n  if argc > 2:\n    sampleNum = int(argv[2])\n  allf = []\n  allp = []\n  nf = 0\n  np = 0\n  while True:\n    line = inp.readline()\n    if not line:\n      break\n    line = line.strip()\n    if not line:\n      continue\n    ss = line.split("" "")\n    assert (len(ss) == 6)\n    if int(ss[5]) == 0:\n      nf += 1\n      if len(allf) < sampleNum:\n        allf.append(line)\n      else:\n        k = random.randint(0, nf - 1)\n        if k < sampleNum:\n          trp.write(""%s\\n"" % (allf[k]))\n          allf[k] = line\n        else:\n          trp.write(""%s\\n"" % (line))\n    else:\n      np += 1\n      if len(allp) < sampleNum:\n        allp.append(line)\n      else:\n        k = random.randint(0, np - 1)\n        if k < sampleNum:\n          trp.write(""%s\\n"" % (allp[k]))\n          allp[k] = line\n        else:\n          trp.write(""%s\\n"" % (line))\n  for s in allp:\n    tep.write(""%s\\n"" % (s))\n  for s in allf:\n    tep.write(""%s\\n"" % (s))\n\n\nif __name__ == \'__main__\':\n  main(len(sys.argv), sys.argv)'"
kcws/train/sentence.py,0,"b'#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\n# File: sentence.py\n# Project: /e/code/kcws\n# Created: Thu Jul 27 2017\n# Author: Koth Chen\n# Copyright (c) 2017 Koth\n#\n# <<licensetext>>\n\n\nclass Sentence:\n    def __init__(self):\n        self.tokens = []\n        self.chars = 0\n\n    def addToken(self, t):\n        self.chars += len(t)\n        self.tokens.append(t)\n\n    def clear(self):\n        self.tokens = []\n        self.chars = 0\n\n    # label -1, unknown\n    # 0-> \'S\'\n    # 1-> \'B\'\n    # 2-> \'M\'\n    # 3-> \'E\'\n    def generate_tr_line(self, x, y, vob):\n        for t in self.tokens:\n            if len(t) == 1:\n                x.append(vob.GetWordIndex(str(t[0].encode(""utf8""))))\n                y.append(0)\n            else:\n                nn = len(t)\n                for i in range(nn):\n                    x.append(vob.GetWordIndex(str(t[i].encode(""utf8""))))\n                    if i == 0:\n                        y.append(1)\n                    elif i == (nn - 1):\n                        y.append(3)\n                    else:\n                        y.append(2)\n'"
kcws/train/stats_pos.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2017-01-25 14:55:00\n# @Last Modified by:   Koth\n# @Last Modified time: 2017-04-07 22:12:33\n\nimport sys\nimport os\n\ntotalLine = 0\nlongLine = 0\nmaxLen = 80\nposMap = {}\n\n\ndef processToken(token, collect, out, end):\n    global totalLine\n    global longLine\n    global maxLen\n    global posMap\n    nn = len(token)\n    oline = token\n    while nn > 0 and token[nn - 1] != \'/\':\n        nn = nn - 1\n    pos = token[nn:]\n    token = token[:nn - 1].strip()\n    if not token:\n        return\n    if (not pos[0:1].isalpha()) or pos[0:1].isupper():\n        return\n    if len(pos) > 2:\n        pos = pos[:2]\n    posMap.setdefault(pos, 0)\n    posMap[pos] += 1\n    out.write(""%s %s\\t"" % (token, pos))\n    if end:\n        out.write(""\\n"")\n\n\ndef processLine(line, out):\n    line = line.strip()\n    nn = len(line)\n    seeLeftB = False\n    start = 0\n    collect = []\n    try:\n        for i in range(nn):\n            if line[i] == \' \':\n                if not seeLeftB:\n                    token = line[start:i]\n                    if token.startswith(\'[\'):\n                        tokenLen = len(token)\n                        while tokenLen > 0 and token[tokenLen - 1] != \']\':\n                            tokenLen = tokenLen - 1\n                        token = token[1:tokenLen - 1]\n                        ss = token.split(\' \')\n                        for s in ss:\n                            processToken(s, collect, out, False)\n                    else:\n                        processToken(token, collect, out, False)\n                    start = i + 1\n            elif line[i] == \'[\':\n                seeLeftB = True\n            elif line[i] == \']\':\n                seeLeftB = False\n        if start < nn:\n            token = line[start:]\n            if token.startswith(\'[\'):\n                tokenLen = len(token)\n                while tokenLen > 0 and token[tokenLen - 1] != \']\':\n                    tokenLen = tokenLen - 1\n                token = token[1:tokenLen - 1]\n                ss = token.split(\' \')\n                ns = len(ss)\n                for i in range(ns - 1):\n                    processToken(ss[i], collect, out, False)\n                processToken(ss[-1], collect, out, True)\n            else:\n                processToken(token, collect, out, True)\n    except Exception as e:\n        pass\n\n\ndef main(argc, argv):\n    global totalLine\n    global longLine\n    global posMap\n    if argc < 4:\n        print(""Usage:%s <dir> <pos_vob_out> <for_train_out>"" % (argv[0]))\n        sys.exit(1)\n    rootDir = argv[1]\n    out = open(argv[3], ""w"")\n    tagvobFp = open(argv[2], ""w"")\n    for dirName, subdirList, fileList in os.walk(rootDir):\n        curDir = os.path.join(rootDir, dirName)\n        for file in fileList:\n            if file.endswith("".txt""):\n                curFile = os.path.join(curDir, file)\n                fp = open(curFile, ""r"")\n                for line in fp.readlines():\n                    line = line.strip()\n                    processLine(line, out)\n                fp.close()\n    out.close()\n    print(""total:%d, long lines:%d"" % (totalLine, longLine))\n    print(""total pos tags:%d"" % (len(posMap)))\n    idx = 0\n    for k, v in posMap.iteritems():\n        tagvobFp.write(""%s\\t%d\\n"" % (k, idx + 1))\n        idx += 1\n\n\nif __name__ == \'__main__\':\n    main(len(sys.argv), sys.argv)\n'"
kcws/train/train_cws.py,37,"b'# -*- coding: utf-8 -*-\n# @Author: Koth Chen\n# @Date:   2016-07-26 13:48:32\n# @Last Modified by:   Koth\n# @Last Modified time: 2017-04-07 23:04:45\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport tensorflow as tf\nimport os\nfrom idcnn import Model as IdCNN\nfrom bilstm import Model as BiLSTM\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'train_data_path\', ""newcorpus/2014_train.txt"",\n                           \'Training data dir\')\ntf.app.flags.DEFINE_string(\'test_data_path\', ""newcorpus/2014_test.txt"",\n                           \'Test data dir\')\ntf.app.flags.DEFINE_string(\'log_dir\', ""logs"", \'The log  dir\')\ntf.app.flags.DEFINE_string(""word2vec_path"", ""newcorpus/vec.txt"",\n                           ""the word2vec data path"")\n\ntf.app.flags.DEFINE_integer(""max_sentence_len"", 80,\n                            ""max num of tokens per query"")\ntf.app.flags.DEFINE_integer(""embedding_size"", 50, ""embedding size"")\ntf.app.flags.DEFINE_integer(""num_tags"", 4, ""BMES"")\ntf.app.flags.DEFINE_integer(""num_hidden"", 100, ""hidden unit number"")\ntf.app.flags.DEFINE_integer(""batch_size"", 100, ""num example per mini batch"")\ntf.app.flags.DEFINE_integer(""train_steps"", 150000, ""trainning steps"")\ntf.app.flags.DEFINE_float(""learning_rate"", 0.001, ""learning rate"")\ntf.app.flags.DEFINE_bool(""use_idcnn"", True, ""whether use the idcnn"")\ntf.app.flags.DEFINE_integer(""track_history"", 6, ""track max history accuracy"")\n\n\ndef do_load_data(path):\n    x = []\n    y = []\n    fp = open(path, ""r"")\n    for line in fp.readlines():\n        line = line.rstrip()\n        if not line:\n            continue\n        ss = line.split("" "")\n        assert (len(ss) == (FLAGS.max_sentence_len * 2))\n        lx = []\n        ly = []\n        for i in range(FLAGS.max_sentence_len):\n            lx.append(int(ss[i]))\n            ly.append(int(ss[i + FLAGS.max_sentence_len]))\n        x.append(lx)\n        y.append(ly)\n    fp.close()\n    return np.array(x), np.array(y)\n\n\nclass Model:\n    def __init__(self, embeddingSize, distinctTagNum, c2vPath, numHidden):\n        self.embeddingSize = embeddingSize\n        self.distinctTagNum = distinctTagNum\n        self.numHidden = numHidden\n        self.c2v = self.load_w2v(c2vPath, FLAGS.embedding_size)\n        self.words = tf.Variable(self.c2v, name=""words"")\n        layers = [\n            {\n                \'dilation\': 1\n            },\n            {\n                \'dilation\': 1\n            },\n            {\n                \'dilation\': 2\n            },\n        ]\n        if FLAGS.use_idcnn:\n            self.model = IdCNN(layers, 3, FLAGS.num_hidden, FLAGS.embedding_size,\n                               FLAGS.max_sentence_len, FLAGS.num_tags)\n        else:\n            self.model = BiLSTM(\n                FLAGS.num_hidden, FLAGS.max_sentence_len, FLAGS.num_tags)\n        self.trains_params = None\n        self.inp = tf.placeholder(tf.int32,\n                                  shape=[None, FLAGS.max_sentence_len],\n                                  name=""input_placeholder"")\n        pass\n\n    def length(self, data):\n        used = tf.sign(tf.abs(data))\n        length = tf.reduce_sum(used, reduction_indices=1)\n        length = tf.cast(length, tf.int32)\n        return length\n\n    def inference(self, X, reuse=None, trainMode=True):\n        word_vectors = tf.nn.embedding_lookup(self.words, X)\n        length = self.length(X)\n        reuse = False if trainMode else True\n        if FLAGS.use_idcnn:\n            word_vectors = tf.expand_dims(word_vectors, 1)\n            unary_scores = self.model.inference(word_vectors, reuse=reuse)\n        else:\n            unary_scores = self.model.inference(\n                word_vectors, length, reuse=reuse)\n        return unary_scores, length\n\n    def loss(self, X, Y):\n        P, sequence_length = self.inference(X)\n        log_likelihood, self.transition_params = tf.contrib.crf.crf_log_likelihood(\n            P, Y, sequence_length)\n        loss = tf.reduce_mean(-log_likelihood)\n        return loss\n\n    def load_w2v(self, path, expectDim):\n        fp = open(path, ""r"")\n        print(""load data from:"", path)\n        line = fp.readline().strip()\n        ss = line.split("" "")\n        total = int(ss[0])\n        dim = int(ss[1])\n        assert (dim == expectDim)\n        ws = []\n        mv = [0 for i in range(dim)]\n        second = -1\n        for t in range(total):\n            if ss[0] == \'<UNK>\':\n                second = t\n            line = fp.readline().strip()\n            ss = line.split("" "")\n            assert (len(ss) == (dim + 1))\n            vals = []\n            for i in range(1, dim + 1):\n                fv = float(ss[i])\n                mv[i - 1] += fv\n                vals.append(fv)\n            ws.append(vals)\n        for i in range(dim):\n            mv[i] = mv[i] / total\n        assert (second != -1)\n        # append one more token , maybe useless\n        ws.append(mv)\n        if second != 1:\n            t = ws[1]\n            ws[1] = ws[second]\n            ws[second] = t\n        fp.close()\n        return np.asarray(ws, dtype=np.float32)\n\n    def test_unary_score(self):\n        P, sequence_length = self.inference(self.inp,\n                                            reuse=True,\n                                            trainMode=False)\n        return P, sequence_length\n\n\ndef read_csv(batch_size, file_name):\n    filename_queue = tf.train.string_input_producer([file_name])\n    reader = tf.TextLineReader(skip_header_lines=0)\n    key, value = reader.read(filename_queue)\n    # decode_csv will convert a Tensor from type string (the text line) in\n    # a tuple of tensor columns with the specified defaults, which also\n    # sets the data type for each column\n    decoded = tf.decode_csv(\n        value,\n        field_delim=\' \',\n        record_defaults=[[0] for i in range(FLAGS.max_sentence_len * 2)])\n\n    # batch actually reads the file and loads ""batch_size"" rows in a single\n    # tensor\n    return tf.train.shuffle_batch(decoded,\n                                  batch_size=batch_size,\n                                  capacity=batch_size * 50,\n                                  min_after_dequeue=batch_size)\n\n\ndef test_evaluate(sess, unary_score, test_sequence_length, transMatrix, inp,\n                  tX, tY):\n    totalEqual = 0\n    batchSize = FLAGS.batch_size\n    totalLen = tX.shape[0]\n    numBatch = int((tX.shape[0] - 1) / batchSize) + 1\n    correct_labels = 0\n    total_labels = 0\n    for i in range(numBatch):\n        endOff = (i + 1) * batchSize\n        if endOff > totalLen:\n            endOff = totalLen\n        y = tY[i * batchSize:endOff]\n        feed_dict = {inp: tX[i * batchSize:endOff]}\n        unary_score_val, test_sequence_length_val = sess.run(\n            [unary_score, test_sequence_length], feed_dict)\n        for tf_unary_scores_, y_, sequence_length_ in zip(\n                unary_score_val, y, test_sequence_length_val):\n            # print(""seg len:%d"" % (sequence_length_))\n            tf_unary_scores_ = tf_unary_scores_[:sequence_length_]\n            y_ = y_[:sequence_length_]\n            viterbi_sequence, _ = tf.contrib.crf.viterbi_decode(\n                tf_unary_scores_, transMatrix)\n            # Evaluate word-level accuracy.\n            correct_labels += np.sum(np.equal(viterbi_sequence, y_))\n            total_labels += sequence_length_\n    accuracy = 100.0 * correct_labels / float(total_labels)\n    print(""Accuracy: %.3f%%"" % accuracy)\n    return accuracy\n\n\ndef inputs(path):\n    whole = read_csv(FLAGS.batch_size, path)\n    features = tf.transpose(tf.stack(whole[0:FLAGS.max_sentence_len]))\n    label = tf.transpose(tf.stack(whole[FLAGS.max_sentence_len:]))\n    return features, label\n\n\ndef train(total_loss):\n    return tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(total_loss)\n\n\ndef main(unused_argv):\n    curdir = os.path.dirname(os.path.realpath(__file__))\n    trainDataPath = tf.app.flags.FLAGS.train_data_path\n    if not trainDataPath.startswith(""/""):\n        trainDataPath = curdir + ""/../../"" + trainDataPath\n    graph = tf.Graph()\n    with graph.as_default():\n        model = Model(FLAGS.embedding_size, FLAGS.num_tags,\n                      FLAGS.word2vec_path, FLAGS.num_hidden)\n        print(""train data path:"", trainDataPath)\n        X, Y = inputs(trainDataPath)\n        tX, tY = do_load_data(tf.app.flags.FLAGS.test_data_path)\n        total_loss = model.loss(X, Y)\n        train_op = train(total_loss)\n        test_unary_score, test_sequence_length = model.test_unary_score()\n        sv = tf.train.Supervisor(graph=graph, logdir=FLAGS.log_dir)\n        with sv.managed_session(master=\'\') as sess:\n            # actual training loop\n            training_steps = FLAGS.train_steps\n            trackHist = 0\n            bestAcc = 0\n            tf.train.write_graph(sess.graph.as_graph_def(),\n                                 FLAGS.log_dir, ""graph.pb"", as_text=False)\n            for step in range(training_steps):\n                if sv.should_stop():\n                    break\n                try:\n                    _, trainsMatrix = sess.run(\n                        [train_op, model.transition_params])\n                    # for debugging and learning purposes, see how the loss\n                    # gets decremented thru training steps\n                    if (step + 1) % 100 == 0:\n                        print(""[%d] loss: [%r]"" %\n                              (step + 1, sess.run(total_loss)))\n                    if (step + 1) % 1000 == 0 or step == 0:\n                        acc = test_evaluate(sess, test_unary_score,\n                                            test_sequence_length, trainsMatrix,\n                                            model.inp, tX, tY)\n                        if acc > bestAcc:\n                            if step:\n                                sv.saver.save(\n                                    sess, FLAGS.log_dir + \'/best_model\')\n                            bestAcc = acc\n                            trackHist = 0\n                        elif trackHist > FLAGS.track_history:\n                            print(\n                                ""always not good enough in last %d histories, best accuracy:%.3f""\n                                % (trackHist, bestAcc))\n                            break\n                        else:\n                            trackHist += 1\n                except KeyboardInterrupt, e:\n                    sv.saver.save(sess,\n                                  FLAGS.log_dir + \'/model\',\n                                  global_step=(step + 1))\n                    raise e\n            sv.saver.save(sess, FLAGS.log_dir + \'/finnal-model\')\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
kcws/train/train_embedding.py,54,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2016-11-30 21:07:24\n# @Last Modified by:   Koth\n# @Last Modified time: 2016-12-01 13:04:36\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import learn\nimport os\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'train_data_path\', ""/Users/tech/code/kcws/train.txt"", \'Training data dir\')\ntf.app.flags.DEFINE_string(\'test_data_path\', ""./test.txt"", \'Test data dir\')\ntf.app.flags.DEFINE_string(\'log_dir\', ""logs"", \'The log  dir\')\ntf.app.flags.DEFINE_string(\'embedding_result\', ""embedding.txt"", \'The log  dir\')\ntf.app.flags.DEFINE_integer(""max_sentence_len"", 5,\n                            ""max num of tokens per query"")\ntf.app.flags.DEFINE_integer(""embedding_size"", 25, ""embedding size"")\ntf.app.flags.DEFINE_integer(""num_hidden"", 20, ""hidden unit number"")\ntf.app.flags.DEFINE_integer(""batch_size"", 100, ""num example per mini batch"")\ntf.app.flags.DEFINE_integer(""train_steps"", 50000, ""trainning steps"")\ntf.app.flags.DEFINE_float(""learning_rate"", 0.001, ""learning rate"")\ntf.app.flags.DEFINE_integer(""num_words"", 5902, ""embedding size"")\n\n\ndef do_load_data(path):\n  x = []\n  y = []\n  fp = open(path, ""r"")\n  for line in fp.readlines():\n    line = line.rstrip()\n    if not line:\n      continue\n    ss = line.split("" "")\n    assert (len(ss) == (FLAGS.max_sentence_len + 1))\n    lx = []\n    for i in range(FLAGS.max_sentence_len):\n      lx.append(int(ss[i]))\n    x.append(lx)\n    y.append(int(ss[FLAGS.max_sentence_len]))\n  fp.close()\n  return np.array(x), np.array(y)\n\n\nclass Model:\n  def __init__(self, embeddingSize, numHidden):\n    self.embeddingSize = embeddingSize\n    self.numHidden = numHidden\n    self.words = tf.Variable(\n        tf.truncated_normal([FLAGS.num_words, embeddingSize]),\n        name=""words"")\n    with tf.variable_scope(\'Softmax\') as scope:\n      self.W = tf.get_variable(\n          shape=[numHidden * 2, 2],\n          initializer=tf.truncated_normal_initializer(stddev=0.01),\n          name=""weights"",\n          regularizer=tf.contrib.layers.l2_regularizer(0.001))\n      self.b = tf.Variable(tf.zeros([2], name=""bias""))\n    self.inp = tf.placeholder(tf.int32,\n                              shape=[None, FLAGS.max_sentence_len],\n                              name=""input_placeholder"")\n    self.tp = tf.placeholder(tf.int64, shape=[None], name=""target_placeholder"")\n    pass\n\n  def length(self, data):\n    used = tf.sign(tf.abs(data))\n    length = tf.reduce_sum(used, reduction_indices=1)\n    length = tf.cast(length, tf.int32)\n    return length\n\n  def inference(self, X, reuse=None, trainMode=True):\n    length = self.length(X)\n    length_64 = tf.cast(length, tf.int64)\n    word_vectors = tf.nn.embedding_lookup(self.words, X)\n    if trainMode:\n      word_vectors = tf.nn.dropout(word_vectors, 0.5)\n    with tf.variable_scope(""rnn_fwbw"", reuse=reuse) as scope:\n      _, forward_output = tf.nn.dynamic_rnn(\n          tf.nn.rnn_cell.LSTMCell(self.numHidden),\n          word_vectors,\n          dtype=tf.float32,\n          sequence_length=length,\n          scope=""RNN_forward"")\n      _, backward_output = tf.nn.dynamic_rnn(\n          tf.nn.rnn_cell.LSTMCell(self.numHidden),\n          inputs=tf.reverse_sequence(word_vectors,\n                                     length_64,\n                                     seq_dim=1),\n          dtype=tf.float32,\n          sequence_length=length,\n          scope=""RNN_backword"")\n    output = tf.concat(1, [forward_output[1], backward_output[1]])\n    logit = tf.batch_matmul(output, self.W)\n    return logit\n\n  def loss(self, X, Y):\n    P = self.inference(X)\n    entroyp = tf.nn.sigmoid_cross_entropy_with_logits(P, Y)\n    loss = tf.reduce_mean(entroyp)\n    return loss\n\n  def test_correct_num(self):\n    logits = self.inference(self.inp, reuse=True, trainMode=False)\n    targets = tf.argmax(logits, axis=1)\n    return tf.reduce_sum(tf.cast(tf.equal(targets, self.tp), tf.int64))\n\n\ndef read_csv(batch_size, file_name):\n  filename_queue = tf.train.string_input_producer([file_name])\n  reader = tf.TextLineReader(skip_header_lines=0)\n  key, value = reader.read(filename_queue)\n  # decode_csv will convert a Tensor from type string (the text line) in\n  # a tuple of tensor columns with the specified defaults, which also\n  # sets the data type for each column\n  decoded = tf.decode_csv(\n      value,\n      field_delim=\' \',\n      record_defaults=[[0] for i in range(FLAGS.max_sentence_len + 1)])\n\n  # batch actually reads the file and loads ""batch_size"" rows in a single tensor\n  return tf.train.shuffle_batch(decoded,\n                                batch_size=batch_size,\n                                capacity=batch_size * 50,\n                                min_after_dequeue=batch_size)\n\n\ndef test_evaluate(sess, calcCorrectOp, inp, tp, tX, tY):\n  totalEqual = 0\n  batchSize = FLAGS.batch_size\n  totalLen = tX.shape[0]\n  numBatch = int((tX.shape[0] - 1) / batchSize) + 1\n  correct_labels = 0\n  total_labels = 0\n  for i in range(numBatch):\n    endOff = (i + 1) * batchSize\n    if endOff > totalLen:\n      endOff = totalLen\n    y = tY[i * batchSize:endOff]\n    feed_dict = {inp: tX[i * batchSize:endOff], tp: tY[i * batchSize:endOff]}\n    count = sess.run([calcCorrectOp], feed_dict)\n    correct_labels += count[0]\n  accuracy = 100.0 * correct_labels / float(totalLen)\n  print(""Accuracy: %.2f%%"" % accuracy)\n\n\ndef inputs(path):\n  whole = read_csv(FLAGS.batch_size, path)\n  features = tf.transpose(tf.pack(whole[0:FLAGS.max_sentence_len]))\n  label = tf.one_hot(\n      tf.transpose(tf.pack(whole[FLAGS.max_sentence_len])),\n      depth=2)\n  return features, label\n\n\ndef train(total_loss):\n  return tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(total_loss)\n\n\ndef write_embedding_result(sess, word_op, fp):\n  all_words = sess.run(word_op)\n  nn = len(all_words)\n  nc = len(all_words[0])\n  assert (nc == FLAGS.embedding_size)\n  fp.write(""%d %d\\n"" % (nn, FLAGS.embedding_size))\n  for i in range(nn):\n    line = str(i)\n    for j in range(FLAGS.embedding_size):\n      line += "" "" + str(all_words[i][j])\n    fp.write(""%s\\n"" % (line))\n\n\ndef main(unused_argv):\n  curdir = os.path.dirname(os.path.realpath(__file__))\n  trainDataPath = tf.app.flags.FLAGS.train_data_path\n  if not trainDataPath.startswith(""/""):\n    trainDataPath = curdir + ""/"" + trainDataPath\n  embedding_out = open(FLAGS.embedding_result, ""w"")\n  graph = tf.Graph()\n  with graph.as_default():\n    model = Model(FLAGS.embedding_size, FLAGS.num_hidden)\n    print(""train data path:"", trainDataPath)\n    X, Y = inputs(trainDataPath)\n    tX, tY = do_load_data(tf.app.flags.FLAGS.test_data_path)\n    total_loss = model.loss(X, Y)\n    train_op = train(total_loss)\n    calc_correct_op = model.test_correct_num()\n    sv = tf.train.Supervisor(graph=graph, logdir=FLAGS.log_dir)\n    with sv.managed_session(master=\'\') as sess:\n      # actual training loop\n      training_steps = FLAGS.train_steps\n      for step in range(training_steps):\n        if sv.should_stop():\n          break\n        try:\n          _ = sess.run([train_op])\n          # for debugging and learning purposes, see how the loss gets decremented thru training steps\n          if step % 100 == 0:\n            print(""[%d] loss: [%r]"" % (step, sess.run(total_loss)))\n          if step % 1000 == 0:\n            test_evaluate(sess, calc_correct_op, model.inp, model.tp, tX, tY)\n        except KeyboardInterrupt, e:\n          write_embedding_result(sess, model.words, embedding_out)\n          sv.saver.save(sess, FLAGS.log_dir + \'/model\', global_step=step + 1)\n          raise e\n      write_embedding_result(sess, model.words, embedding_out)\n      sv.saver.save(sess, FLAGS.log_dir + \'/finnal-model\')\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
kcws/train/train_pos.py,80,"b'# -*- coding: utf-8 -*-\n# @Author: Koth\n# @Date:   2017-01-24 16:13:14\n# @Last Modified by:   Koth\n# @Last Modified time: 2017-04-07 23:02:50\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport tensorflow as tf\nimport os\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'train_data_path\', ""pos/train.txt"",\n                           \'Training data dir\')\ntf.app.flags.DEFINE_string(\'test_data_path\', ""pos/test.txt"", \'Test data dir\')\ntf.app.flags.DEFINE_string(\'log_dir\', ""pos_logs"", \'The log  dir\')\ntf.app.flags.DEFINE_string(""word_word2vec_path"", ""pos/word_vec.txt"",\n                           ""the word2vec data path"")\ntf.app.flags.DEFINE_string(""char_word2vec_path"", ""pos/char_vec.txt"",\n                           ""the charater word2vec data path"")\ntf.app.flags.DEFINE_integer(""max_sentence_len"", 50,\n                            ""max num of tokens per query"")\ntf.app.flags.DEFINE_integer(""embedding_word_size"", 150, ""embedding size"")\ntf.app.flags.DEFINE_integer(""embedding_char_size"", 50, ""second embedding size"")\ntf.app.flags.DEFINE_integer(""num_tags"", 74, ""num pos tags"")\ntf.app.flags.DEFINE_integer(""char_window_size"", 2,\n                            ""the window size of char convolution"")\ntf.app.flags.DEFINE_integer(""max_chars_per_word"", 5,\n                            ""max number of characters per word "")\ntf.app.flags.DEFINE_integer(""num_hidden"", 100, ""hidden unit number"")\ntf.app.flags.DEFINE_integer(""batch_size"", 64, ""num example per mini batch"")\ntf.app.flags.DEFINE_integer(""train_steps"", 50000, ""trainning steps"")\ntf.app.flags.DEFINE_float(""learning_rate"", 0.001, ""learning rate"")\n\n\ndef do_load_data(path):\n    wx = []\n    cx = []\n    y = []\n    fp = open(path, ""r"")\n    ln = 0\n    for line in fp:\n        line = line.rstrip()\n        ln += 1\n        if not line:\n            continue\n        ss = line.split("" "")\n        if len(ss) != (FLAGS.max_sentence_len *\n                       (2 + FLAGS.max_chars_per_word)):\n            print(""[line:%d]len ss:%d,origin len:%d\\n%s"" %\n                  (ln, len(ss), len(line), line))\n        assert (len(ss) == (FLAGS.max_sentence_len *\n                            (2 + FLAGS.max_chars_per_word)))\n        lwx = []\n        lcx = []\n        ly = []\n        for i in range(FLAGS.max_sentence_len):\n            lwx.append(int(ss[i]))\n            for k in range(FLAGS.max_chars_per_word):\n                lcx.append(int(ss[FLAGS.max_sentence_len + i *\n                                  FLAGS.max_chars_per_word + k]))\n            ly.append(int(ss[i + FLAGS.max_sentence_len * (\n                FLAGS.max_chars_per_word + 1)]))\n        wx.append(lwx)\n        cx.append(lcx)\n        y.append(ly)\n    fp.close()\n    return np.array(wx), np.array(cx), np.array(y)\n\n\nclass Model:\n    def __init__(self, distinctTagNum, w2vPath, c2vPath, numHidden):\n        self.distinctTagNum = distinctTagNum\n        self.numHidden = numHidden\n        self.w2v = self.load_w2v(w2vPath, FLAGS.embedding_word_size)\n        self.c2v = self.load_w2v(c2vPath, FLAGS.embedding_char_size)\n        self.words = tf.Variable(self.w2v, name=""words"")\n        self.chars = tf.Variable(self.c2v, name=""chars"")\n        with tf.variable_scope(\'Softmax\') as scope:\n            self.W = tf.get_variable(\n                shape=[numHidden * 2, distinctTagNum],\n                initializer=tf.truncated_normal_initializer(stddev=0.01),\n                name=""weights"",\n                regularizer=tf.contrib.layers.l2_regularizer(0.001))\n            self.b = tf.Variable(tf.zeros([distinctTagNum], name=""bias""))\n        with tf.variable_scope(\'CNN_Layer\') as scope:\n            self.filter = tf.get_variable(\n                ""filters_1"",\n                shape=[2, FLAGS.embedding_char_size, 1,\n                       FLAGS.embedding_char_size],\n                regularizer=tf.contrib.layers.l2_regularizer(0.0001),\n                initializer=tf.truncated_normal_initializer(stddev=0.01),\n                dtype=tf.float32)\n        self.trains_params = None\n        self.inp_w = tf.placeholder(tf.int32,\n                                    shape=[None, FLAGS.max_sentence_len],\n                                    name=""input_words"")\n        self.inp_c = tf.placeholder(\n            tf.int32,\n            shape=[None, FLAGS.max_sentence_len * FLAGS.max_chars_per_word],\n            name=""input_chars"")\n        pass\n\n    def length(self, data):\n        used = tf.sign(tf.abs(data))\n        length = tf.reduce_sum(used, reduction_indices=1)\n        length = tf.cast(length, tf.int32)\n        return length\n\n    def char_convolution(self, vecs):\n        conv1 = tf.nn.conv2d(vecs,\n                             self.filter, [1, 1, FLAGS.embedding_char_size, 1],\n                             padding=\'VALID\')\n        conv1 = tf.nn.relu(conv1)\n        pool1 = tf.nn.max_pool(\n            conv1,\n            ksize=[1, FLAGS.max_chars_per_word - FLAGS.char_window_size + 1, 1,\n                   1],\n            strides=[1, FLAGS.max_chars_per_word - FLAGS.char_window_size + 1,\n                     1, 1],\n            padding=\'SAME\')\n        pool1 = tf.squeeze(pool1, [1, 2])\n        return pool1\n\n    def inference(self, wX, cX, reuse=None, trainMode=True):\n        word_vectors = tf.nn.embedding_lookup(self.words, wX)\n        char_vectors = tf.nn.embedding_lookup(self.chars, cX)\n        char_vectors = tf.reshape(char_vectors, [-1, FLAGS.max_sentence_len,\n                                                 FLAGS.max_chars_per_word,\n                                                 FLAGS.embedding_char_size])\n        char_vectors = tf.transpose(char_vectors, perm=[1, 0, 2, 3])\n        char_vectors = tf.expand_dims(char_vectors, -1)\n        length = self.length(wX)\n        length_64 = tf.cast(length, tf.int64)\n\n        # do conv\n        def do_char_conv(x): return self.char_convolution(x)\n        char_vectors_x = tf.map_fn(do_char_conv, char_vectors)\n        char_vectors_x = tf.transpose(char_vectors_x, perm=[1, 0, 2])\n        word_vectors = tf.concat([word_vectors, char_vectors_x], axis=2)\n        # if trainMode:\n        #  word_vectors = tf.nn.dropout(word_vectors, 0.5)\n        reuse = None if trainMode else True\n        with tf.variable_scope(""rnn_fwbw"", reuse=reuse) as scope:\n            forward_output, _ = tf.nn.dynamic_rnn(\n                tf.contrib.rnn.LSTMCell(self.numHidden,\n                                        reuse=reuse),\n                word_vectors,\n                dtype=tf.float32,\n                sequence_length=length,\n                scope=""RNN_forward"")\n            backward_output_, _ = tf.nn.dynamic_rnn(\n                tf.contrib.rnn.LSTMCell(self.numHidden,\n                                        reuse=reuse),\n                inputs=tf.reverse_sequence(word_vectors,\n                                           length_64,\n                                           seq_dim=1),\n                dtype=tf.float32,\n                sequence_length=length,\n                scope=""RNN_backword"")\n\n        backward_output = tf.reverse_sequence(backward_output_,\n                                              length_64,\n                                              seq_dim=1)\n\n        output = tf.concat([forward_output, backward_output], 2)\n        output = tf.reshape(output, [-1, self.numHidden * 2])\n        if trainMode:\n            output = tf.nn.dropout(output, 0.5)\n\n        matricized_unary_scores = tf.matmul(output, self.W) + self.b\n        # matricized_unary_scores = tf.nn.log_softmax(matricized_unary_scores)\n        unary_scores = tf.reshape(\n            matricized_unary_scores,\n            [-1, FLAGS.max_sentence_len, self.distinctTagNum])\n\n        return unary_scores, length\n\n    def loss(self, wX, cX, Y):\n        P, sequence_length = self.inference(wX, cX)\n        log_likelihood, self.transition_params = tf.contrib.crf.crf_log_likelihood(\n            P, Y, sequence_length)\n        loss = tf.reduce_mean(-log_likelihood)\n        return loss\n\n    def load_w2v(self, path, expectDim):\n        fp = open(path, ""r"")\n        print(""load data from:"", path)\n        line = fp.readline().strip()\n        ss = line.split("" "")\n        total = int(ss[0])\n        dim = int(ss[1])\n        assert (dim == expectDim)\n        ws = []\n        mv = [0 for i in range(dim)]\n        second = -1\n        for t in range(total):\n            if ss[0] == \'<UNK>\':\n                second = t\n            line = fp.readline().strip()\n            ss = line.split("" "")\n            assert (len(ss) == (dim + 1))\n            vals = []\n            for i in range(1, dim + 1):\n                fv = float(ss[i])\n                mv[i - 1] += fv\n                vals.append(fv)\n            ws.append(vals)\n        for i in range(dim):\n            mv[i] = mv[i] / total\n        assert (second != -1)\n        # append one more token , maybe useless\n        ws.append(mv)\n        if second != 1:\n            t = ws[1]\n            ws[1] = ws[second]\n            ws[second] = t\n        fp.close()\n        return np.asarray(ws, dtype=np.float32)\n\n    def test_unary_score(self):\n        P, sequence_length = self.inference(self.inp_w,\n                                            self.inp_c,\n                                            reuse=True,\n                                            trainMode=False)\n        return P, sequence_length\n\n\ndef read_csv(batch_size, file_name):\n    filename_queue = tf.train.string_input_producer([file_name])\n    reader = tf.TextLineReader(skip_header_lines=0)\n    key, value = reader.read(filename_queue)\n    # decode_csv will convert a Tensor from type string (the text line) in\n    # a tuple of tensor columns with the specified defaults, which also\n    # sets the data type for each column\n    decoded = tf.decode_csv(value,\n                            field_delim=\' \',\n                            record_defaults=[\n                                [0]\n                                for i in range(FLAGS.max_sentence_len * (\n                                    FLAGS.max_chars_per_word + 2))\n                            ])\n\n    # batch actually reads the file and loads ""batch_size"" rows in a single\n    # tensor\n    return tf.train.shuffle_batch(decoded,\n                                  batch_size=batch_size,\n                                  capacity=batch_size * 40,\n                                  min_after_dequeue=batch_size)\n\n\ndef test_evaluate(sess, unary_score, test_sequence_length, transMatrix, inp_w,\n                  inp_c, twX, tcX, tY):\n    totalEqual = 0\n    batchSize = FLAGS.batch_size\n    totalLen = twX.shape[0]\n    numBatch = int((twX.shape[0] - 1) / batchSize) + 1\n    correct_labels = 0\n    total_labels = 0\n    for i in range(numBatch):\n        endOff = (i + 1) * batchSize\n        if endOff > totalLen:\n            endOff = totalLen\n        y = tY[i * batchSize:endOff]\n        feed_dict = {inp_w: twX[i * batchSize:endOff],\n                     inp_c: tcX[i * batchSize:endOff]}\n        unary_score_val, test_sequence_length_val = sess.run(\n            [unary_score, test_sequence_length], feed_dict)\n        for tf_unary_scores_, y_, sequence_length_ in zip(\n                unary_score_val, y, test_sequence_length_val):\n            # print(""seg len:%d"" % (sequence_length_))\n            tf_unary_scores_ = tf_unary_scores_[:sequence_length_]\n            y_ = y_[:sequence_length_]\n            viterbi_sequence, _ = tf.contrib.crf.viterbi_decode(\n                tf_unary_scores_, transMatrix)\n            # Evaluate word-level accuracy.\n            correct_labels += np.sum(np.equal(viterbi_sequence, y_))\n            total_labels += sequence_length_\n    accuracy = 100.0 * correct_labels / float(total_labels)\n    print(""Accuracy: %.3f%%"" % accuracy)\n\n\ndef inputs(path):\n    whole = read_csv(FLAGS.batch_size, path)\n    features = tf.transpose(tf.stack(whole[0:FLAGS.max_sentence_len]))\n    char_features = tf.transpose(tf.stack(whole[FLAGS.max_sentence_len:(\n        FLAGS.max_chars_per_word + 1) * FLAGS.max_sentence_len]))\n    label = tf.transpose(tf.stack(whole[(FLAGS.max_chars_per_word + 1) *\n                                        FLAGS.max_sentence_len:]))\n    return features, char_features, label\n\n\ndef train(total_loss):\n    return tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(total_loss)\n\n\ndef main(unused_argv):\n    curdir = os.path.dirname(os.path.realpath(__file__))\n    trainDataPath = tf.app.flags.FLAGS.train_data_path\n    if not trainDataPath.startswith(""/""):\n        trainDataPath = curdir + ""/../../"" + trainDataPath\n    graph = tf.Graph()\n    with graph.as_default():\n        model = Model(FLAGS.num_tags, FLAGS.word_word2vec_path,\n                      FLAGS.char_word2vec_path, FLAGS.num_hidden)\n        print(""train data path:"", trainDataPath)\n        wX, cX, Y = inputs(trainDataPath)\n        twX, tcX, tY = do_load_data(tf.app.flags.FLAGS.test_data_path)\n        total_loss = model.loss(wX, cX, Y)\n        train_op = train(total_loss)\n        test_unary_score, test_sequence_length = model.test_unary_score()\n        sv = tf.train.Supervisor(graph=graph, logdir=FLAGS.log_dir)\n        with sv.managed_session(master=\'\') as sess:\n            # actual training loop\n            training_steps = FLAGS.train_steps\n            tf.train.write_graph(sess.graph.as_graph_def(),\n                                 FLAGS.log_dir, ""graph.pb"", as_text=False)\n            for step in range(training_steps):\n                if sv.should_stop():\n                    break\n                try:\n                    _, trainsMatrix = sess.run(\n                        [train_op, model.transition_params])\n                    # for debugging and learning purposes, see how the loss\n                    # gets decremented thru training steps\n                    if (step + 1) % 100 == 0:\n                        print(""[%d] loss: [%r]"" %\n                              (step + 1, sess.run(total_loss)))\n                    if (step + 1) % 1000 == 0:\n                        test_evaluate(sess, test_unary_score,\n                                      test_sequence_length, trainsMatrix,\n                                      model.inp_w, model.inp_c, twX, tcX, tY)\n                except KeyboardInterrupt, e:\n                    sv.saver.save(sess,\n                                  FLAGS.log_dir + \'/model\',\n                                  global_step=(step + 1))\n                    raise e\n            sv.saver.save(sess, FLAGS.log_dir + \'/finnal-model\')\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
third_party/setuptools/pkg_resources.py,0,"b'""""""\nPackage resource API\n--------------------\n\nA resource is a logical file contained within a package, or a logical\nsubdirectory thereof.  The package resource API expects resource names\nto have their path parts separated with ``/``, *not* whatever the local\npath separator is.  Do not use os.path operations to manipulate resource\nnames being passed into the API.\n\nThe package resource API is designed to work with normal filesystem packages,\n.egg files, and unpacked .egg files.  It can also work in a limited way with\n.zip files and with custom PEP 302 loaders that support the ``get_data()``\nmethod.\n""""""\n\nimport sys\nimport os\nimport time\nimport re\nimport imp\nimport zipfile\nimport zipimport\nimport warnings\nimport stat\nimport functools\nimport pkgutil\nimport token\nimport symbol\nimport operator\nimport platform\nfrom pkgutil import get_importer\n\ntry:\n    from urlparse import urlparse, urlunparse\nexcept ImportError:\n    from urllib.parse import urlparse, urlunparse\n\ntry:\n    frozenset\nexcept NameError:\n    from sets import ImmutableSet as frozenset\ntry:\n    basestring\n    next = lambda o: o.next()\n    from cStringIO import StringIO as BytesIO\nexcept NameError:\n    basestring = str\n    from io import BytesIO\n    def execfile(fn, globs=None, locs=None):\n        if globs is None:\n            globs = globals()\n        if locs is None:\n            locs = globs\n        exec(compile(open(fn).read(), fn, \'exec\'), globs, locs)\n\n# capture these to bypass sandboxing\nfrom os import utime\ntry:\n    from os import mkdir, rename, unlink\n    WRITE_SUPPORT = True\nexcept ImportError:\n    # no write support, probably under GAE\n    WRITE_SUPPORT = False\n\nfrom os import open as os_open\nfrom os.path import isdir, split\n\n# Avoid try/except due to potential problems with delayed import mechanisms.\nif sys.version_info >= (3, 3) and sys.implementation.name == ""cpython"":\n    import importlib._bootstrap as importlib_bootstrap\nelse:\n    importlib_bootstrap = None\n\ntry:\n    import parser\nexcept ImportError:\n    pass\n\ndef _bypass_ensure_directory(name, mode=0o777):\n    # Sandbox-bypassing version of ensure_directory()\n    if not WRITE_SUPPORT:\n        raise IOError(\'""os.mkdir"" not supported on this platform.\')\n    dirname, filename = split(name)\n    if dirname and filename and not isdir(dirname):\n        _bypass_ensure_directory(dirname)\n        mkdir(dirname, mode)\n\n\n_state_vars = {}\n\ndef _declare_state(vartype, **kw):\n    globals().update(kw)\n    _state_vars.update(dict.fromkeys(kw, vartype))\n\ndef __getstate__():\n    state = {}\n    g = globals()\n    for k, v in _state_vars.items():\n        state[k] = g[\'_sget_\'+v](g[k])\n    return state\n\ndef __setstate__(state):\n    g = globals()\n    for k, v in state.items():\n        g[\'_sset_\'+_state_vars[k]](k, g[k], v)\n    return state\n\ndef _sget_dict(val):\n    return val.copy()\n\ndef _sset_dict(key, ob, state):\n    ob.clear()\n    ob.update(state)\n\ndef _sget_object(val):\n    return val.__getstate__()\n\ndef _sset_object(key, ob, state):\n    ob.__setstate__(state)\n\n_sget_none = _sset_none = lambda *args: None\n\n\ndef get_supported_platform():\n    """"""Return this platform\'s maximum compatible version.\n\n    distutils.util.get_platform() normally reports the minimum version\n    of Mac OS X that would be required to *use* extensions produced by\n    distutils.  But what we want when checking compatibility is to know the\n    version of Mac OS X that we are *running*.  To allow usage of packages that\n    explicitly require a newer version of Mac OS X, we must also know the\n    current version of the OS.\n\n    If this condition occurs for any other platform with a version in its\n    platform strings, this function should be extended accordingly.\n    """"""\n    plat = get_build_platform()\n    m = macosVersionString.match(plat)\n    if m is not None and sys.platform == ""darwin"":\n        try:\n            plat = \'macosx-%s-%s\' % (\'.\'.join(_macosx_vers()[:2]), m.group(3))\n        except ValueError:\n            # not Mac OS X\n            pass\n    return plat\n\n__all__ = [\n    # Basic resource access and distribution/entry point discovery\n    \'require\', \'run_script\', \'get_provider\',  \'get_distribution\',\n    \'load_entry_point\', \'get_entry_map\', \'get_entry_info\',\n    \'iter_entry_points\',\n    \'resource_string\', \'resource_stream\', \'resource_filename\',\n    \'resource_listdir\', \'resource_exists\', \'resource_isdir\',\n\n    # Environmental control\n    \'declare_namespace\', \'working_set\', \'add_activation_listener\',\n    \'find_distributions\', \'set_extraction_path\', \'cleanup_resources\',\n    \'get_default_cache\',\n\n    # Primary implementation classes\n    \'Environment\', \'WorkingSet\', \'ResourceManager\',\n    \'Distribution\', \'Requirement\', \'EntryPoint\',\n\n    # Exceptions\n    \'ResolutionError\', \'VersionConflict\', \'DistributionNotFound\',\n    \'UnknownExtra\', \'ExtractionError\',\n\n    # Parsing functions and string utilities\n    \'parse_requirements\', \'parse_version\', \'safe_name\', \'safe_version\',\n    \'get_platform\', \'compatible_platforms\', \'yield_lines\', \'split_sections\',\n    \'safe_extra\', \'to_filename\', \'invalid_marker\', \'evaluate_marker\',\n\n    # filesystem utilities\n    \'ensure_directory\', \'normalize_path\',\n\n    # Distribution ""precedence"" constants\n    \'EGG_DIST\', \'BINARY_DIST\', \'SOURCE_DIST\', \'CHECKOUT_DIST\', \'DEVELOP_DIST\',\n\n    # ""Provider"" interfaces, implementations, and registration/lookup APIs\n    \'IMetadataProvider\', \'IResourceProvider\', \'FileMetadata\',\n    \'PathMetadata\', \'EggMetadata\', \'EmptyProvider\', \'empty_provider\',\n    \'NullProvider\', \'EggProvider\', \'DefaultProvider\', \'ZipProvider\',\n    \'register_finder\', \'register_namespace_handler\', \'register_loader_type\',\n    \'fixup_namespace_packages\', \'get_importer\',\n\n    # Deprecated/backward compatibility only\n    \'run_main\', \'AvailableDistributions\',\n]\n\nclass ResolutionError(Exception):\n    """"""Abstract base for dependency resolution errors""""""\n    def __repr__(self):\n        return self.__class__.__name__+repr(self.args)\n\nclass VersionConflict(ResolutionError):\n    """"""An already-installed version conflicts with the requested version""""""\n\nclass DistributionNotFound(ResolutionError):\n    """"""A requested distribution was not found""""""\n\nclass UnknownExtra(ResolutionError):\n    """"""Distribution doesn\'t have an ""extra feature"" of the given name""""""\n_provider_factories = {}\n\nPY_MAJOR = sys.version[:3]\nEGG_DIST = 3\nBINARY_DIST = 2\nSOURCE_DIST = 1\nCHECKOUT_DIST = 0\nDEVELOP_DIST = -1\n\ndef register_loader_type(loader_type, provider_factory):\n    """"""Register `provider_factory` to make providers for `loader_type`\n\n    `loader_type` is the type or class of a PEP 302 ``module.__loader__``,\n    and `provider_factory` is a function that, passed a *module* object,\n    returns an ``IResourceProvider`` for that module.\n    """"""\n    _provider_factories[loader_type] = provider_factory\n\ndef get_provider(moduleOrReq):\n    """"""Return an IResourceProvider for the named module or requirement""""""\n    if isinstance(moduleOrReq, Requirement):\n        return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]\n    try:\n        module = sys.modules[moduleOrReq]\n    except KeyError:\n        __import__(moduleOrReq)\n        module = sys.modules[moduleOrReq]\n    loader = getattr(module, \'__loader__\', None)\n    return _find_adapter(_provider_factories, loader)(module)\n\ndef _macosx_vers(_cache=[]):\n    if not _cache:\n        import platform\n        version = platform.mac_ver()[0]\n        # fallback for MacPorts\n        if version == \'\':\n            import plistlib\n            plist = \'/System/Library/CoreServices/SystemVersion.plist\'\n            if os.path.exists(plist):\n                if hasattr(plistlib, \'readPlist\'):\n                    plist_content = plistlib.readPlist(plist)\n                    if \'ProductVersion\' in plist_content:\n                        version = plist_content[\'ProductVersion\']\n\n        _cache.append(version.split(\'.\'))\n    return _cache[0]\n\ndef _macosx_arch(machine):\n    return {\'PowerPC\': \'ppc\', \'Power_Macintosh\': \'ppc\'}.get(machine, machine)\n\ndef get_build_platform():\n    """"""Return this platform\'s string for platform-specific distributions\n\n    XXX Currently this is the same as ``distutils.util.get_platform()``, but it\n    needs some hacks for Linux and Mac OS X.\n    """"""\n    try:\n        # Python 2.7 or >=3.2\n        from sysconfig import get_platform\n    except ImportError:\n        from distutils.util import get_platform\n\n    plat = get_platform()\n    if sys.platform == ""darwin"" and not plat.startswith(\'macosx-\'):\n        try:\n            version = _macosx_vers()\n            machine = os.uname()[4].replace("" "", ""_"")\n            return ""macosx-%d.%d-%s"" % (int(version[0]), int(version[1]),\n                _macosx_arch(machine))\n        except ValueError:\n            # if someone is running a non-Mac darwin system, this will fall\n            # through to the default implementation\n            pass\n    return plat\n\nmacosVersionString = re.compile(r""macosx-(\\d+)\\.(\\d+)-(.*)"")\ndarwinVersionString = re.compile(r""darwin-(\\d+)\\.(\\d+)\\.(\\d+)-(.*)"")\n# XXX backward compat\nget_platform = get_build_platform\n\n\ndef compatible_platforms(provided, required):\n    """"""Can code for the `provided` platform run on the `required` platform?\n\n    Returns true if either platform is ``None``, or the platforms are equal.\n\n    XXX Needs compatibility checks for Linux and other unixy OSes.\n    """"""\n    if provided is None or required is None or provided==required:\n        # easy case\n        return True\n\n    # Mac OS X special cases\n    reqMac = macosVersionString.match(required)\n    if reqMac:\n        provMac = macosVersionString.match(provided)\n\n        # is this a Mac package?\n        if not provMac:\n            # this is backwards compatibility for packages built before\n            # setuptools 0.6. All packages built after this point will\n            # use the new macosx designation.\n            provDarwin = darwinVersionString.match(provided)\n            if provDarwin:\n                dversion = int(provDarwin.group(1))\n                macosversion = ""%s.%s"" % (reqMac.group(1), reqMac.group(2))\n                if dversion == 7 and macosversion >= ""10.3"" or \\\n                        dversion == 8 and macosversion >= ""10.4"":\n\n                    #import warnings\n                    #warnings.warn(""Mac eggs should be rebuilt to ""\n                    #    ""use the macosx designation instead of darwin."",\n                    #    category=DeprecationWarning)\n                    return True\n            return False    # egg isn\'t macosx or legacy darwin\n\n        # are they the same major version and machine type?\n        if provMac.group(1) != reqMac.group(1) or \\\n                provMac.group(3) != reqMac.group(3):\n            return False\n\n        # is the required OS major update >= the provided one?\n        if int(provMac.group(2)) > int(reqMac.group(2)):\n            return False\n\n        return True\n\n    # XXX Linux and other platforms\' special cases should go here\n    return False\n\n\ndef run_script(dist_spec, script_name):\n    """"""Locate distribution `dist_spec` and run its `script_name` script""""""\n    ns = sys._getframe(1).f_globals\n    name = ns[\'__name__\']\n    ns.clear()\n    ns[\'__name__\'] = name\n    require(dist_spec)[0].run_script(script_name, ns)\n\n# backward compatibility\nrun_main = run_script\n\ndef get_distribution(dist):\n    """"""Return a current distribution object for a Requirement or string""""""\n    if isinstance(dist, basestring): dist = Requirement.parse(dist)\n    if isinstance(dist, Requirement): dist = get_provider(dist)\n    if not isinstance(dist, Distribution):\n        raise TypeError(""Expected string, Requirement, or Distribution"", dist)\n    return dist\n\ndef load_entry_point(dist, group, name):\n    """"""Return `name` entry point of `group` for `dist` or raise ImportError""""""\n    return get_distribution(dist).load_entry_point(group, name)\n\ndef get_entry_map(dist, group=None):\n    """"""Return the entry point map for `group`, or the full entry map""""""\n    return get_distribution(dist).get_entry_map(group)\n\ndef get_entry_info(dist, group, name):\n    """"""Return the EntryPoint object for `group`+`name`, or ``None``""""""\n    return get_distribution(dist).get_entry_info(group, name)\n\n\nclass IMetadataProvider:\n\n    def has_metadata(name):\n        """"""Does the package\'s distribution contain the named metadata?""""""\n\n    def get_metadata(name):\n        """"""The named metadata resource as a string""""""\n\n    def get_metadata_lines(name):\n        """"""Yield named metadata resource as list of non-blank non-comment lines\n\n       Leading and trailing whitespace is stripped from each line, and lines\n       with ``#`` as the first non-blank character are omitted.""""""\n\n    def metadata_isdir(name):\n        """"""Is the named metadata a directory?  (like ``os.path.isdir()``)""""""\n\n    def metadata_listdir(name):\n        """"""List of metadata names in the directory (like ``os.listdir()``)""""""\n\n    def run_script(script_name, namespace):\n        """"""Execute the named script in the supplied namespace dictionary""""""\n\n\nclass IResourceProvider(IMetadataProvider):\n    """"""An object that provides access to package resources""""""\n\n    def get_resource_filename(manager, resource_name):\n        """"""Return a true filesystem path for `resource_name`\n\n        `manager` must be an ``IResourceManager``""""""\n\n    def get_resource_stream(manager, resource_name):\n        """"""Return a readable file-like object for `resource_name`\n\n        `manager` must be an ``IResourceManager``""""""\n\n    def get_resource_string(manager, resource_name):\n        """"""Return a string containing the contents of `resource_name`\n\n        `manager` must be an ``IResourceManager``""""""\n\n    def has_resource(resource_name):\n        """"""Does the package contain the named resource?""""""\n\n    def resource_isdir(resource_name):\n        """"""Is the named resource a directory?  (like ``os.path.isdir()``)""""""\n\n    def resource_listdir(resource_name):\n        """"""List of resource names in the directory (like ``os.listdir()``)""""""\n\n\nclass WorkingSet(object):\n    """"""A collection of active distributions on sys.path (or a similar list)""""""\n\n    def __init__(self, entries=None):\n        """"""Create working set from list of path entries (default=sys.path)""""""\n        self.entries = []\n        self.entry_keys = {}\n        self.by_key = {}\n        self.callbacks = []\n\n        if entries is None:\n            entries = sys.path\n\n        for entry in entries:\n            self.add_entry(entry)\n\n    @classmethod\n    def _build_master(cls):\n        """"""\n        Prepare the master working set.\n        """"""\n        ws = cls()\n        try:\n            from __main__ import __requires__\n        except ImportError:\n            # The main program does not list any requirements\n            return ws\n\n        # ensure the requirements are met\n        try:\n            ws.require(__requires__)\n        except VersionConflict:\n            return cls._build_from_requirements(__requires__)\n\n        return ws\n\n    @classmethod\n    def _build_from_requirements(cls, req_spec):\n        """"""\n        Build a working set from a requirement spec. Rewrites sys.path.\n        """"""\n        # try it without defaults already on sys.path\n        # by starting with an empty path\n        ws = cls([])\n        reqs = parse_requirements(req_spec)\n        dists = ws.resolve(reqs, Environment())\n        for dist in dists:\n            ws.add(dist)\n\n        # add any missing entries from sys.path\n        for entry in sys.path:\n            if entry not in ws.entries:\n                ws.add_entry(entry)\n\n        # then copy back to sys.path\n        sys.path[:] = ws.entries\n        return ws\n\n    def add_entry(self, entry):\n        """"""Add a path item to ``.entries``, finding any distributions on it\n\n        ``find_distributions(entry, True)`` is used to find distributions\n        corresponding to the path entry, and they are added.  `entry` is\n        always appended to ``.entries``, even if it is already present.\n        (This is because ``sys.path`` can contain the same value more than\n        once, and the ``.entries`` of the ``sys.path`` WorkingSet should always\n        equal ``sys.path``.)\n        """"""\n        self.entry_keys.setdefault(entry, [])\n        self.entries.append(entry)\n        for dist in find_distributions(entry, True):\n            self.add(dist, entry, False)\n\n    def __contains__(self, dist):\n        """"""True if `dist` is the active distribution for its project""""""\n        return self.by_key.get(dist.key) == dist\n\n    def find(self, req):\n        """"""Find a distribution matching requirement `req`\n\n        If there is an active distribution for the requested project, this\n        returns it as long as it meets the version requirement specified by\n        `req`.  But, if there is an active distribution for the project and it\n        does *not* meet the `req` requirement, ``VersionConflict`` is raised.\n        If there is no active distribution for the requested project, ``None``\n        is returned.\n        """"""\n        dist = self.by_key.get(req.key)\n        if dist is not None and dist not in req:\n            # XXX add more info\n            raise VersionConflict(dist, req)\n        else:\n            return dist\n\n    def iter_entry_points(self, group, name=None):\n        """"""Yield entry point objects from `group` matching `name`\n\n        If `name` is None, yields all entry points in `group` from all\n        distributions in the working set, otherwise only ones matching\n        both `group` and `name` are yielded (in distribution order).\n        """"""\n        for dist in self:\n            entries = dist.get_entry_map(group)\n            if name is None:\n                for ep in entries.values():\n                    yield ep\n            elif name in entries:\n                yield entries[name]\n\n    def run_script(self, requires, script_name):\n        """"""Locate distribution for `requires` and run `script_name` script""""""\n        ns = sys._getframe(1).f_globals\n        name = ns[\'__name__\']\n        ns.clear()\n        ns[\'__name__\'] = name\n        self.require(requires)[0].run_script(script_name, ns)\n\n    def __iter__(self):\n        """"""Yield distributions for non-duplicate projects in the working set\n\n        The yield order is the order in which the items\' path entries were\n        added to the working set.\n        """"""\n        seen = {}\n        for item in self.entries:\n            if item not in self.entry_keys:\n                # workaround a cache issue\n                continue\n\n            for key in self.entry_keys[item]:\n                if key not in seen:\n                    seen[key]=1\n                    yield self.by_key[key]\n\n    def add(self, dist, entry=None, insert=True, replace=False):\n        """"""Add `dist` to working set, associated with `entry`\n\n        If `entry` is unspecified, it defaults to the ``.location`` of `dist`.\n        On exit from this routine, `entry` is added to the end of the working\n        set\'s ``.entries`` (if it wasn\'t already present).\n\n        `dist` is only added to the working set if it\'s for a project that\n        doesn\'t already have a distribution in the set, unless `replace=True`.\n        If it\'s added, any callbacks registered with the ``subscribe()`` method\n        will be called.\n        """"""\n        if insert:\n            dist.insert_on(self.entries, entry)\n\n        if entry is None:\n            entry = dist.location\n        keys = self.entry_keys.setdefault(entry,[])\n        keys2 = self.entry_keys.setdefault(dist.location,[])\n        if not replace and dist.key in self.by_key:\n            # ignore hidden distros\n            return\n\n        self.by_key[dist.key] = dist\n        if dist.key not in keys:\n            keys.append(dist.key)\n        if dist.key not in keys2:\n            keys2.append(dist.key)\n        self._added_new(dist)\n\n    def resolve(self, requirements, env=None, installer=None,\n            replace_conflicting=False):\n        """"""List all distributions needed to (recursively) meet `requirements`\n\n        `requirements` must be a sequence of ``Requirement`` objects.  `env`,\n        if supplied, should be an ``Environment`` instance.  If\n        not supplied, it defaults to all distributions available within any\n        entry or distribution in the working set.  `installer`, if supplied,\n        will be invoked with each requirement that cannot be met by an\n        already-installed distribution; it should return a ``Distribution`` or\n        ``None``.\n\n        Unless `replace_conflicting=True`, raises a VersionConflict exception if\n        any requirements are found on the path that have the correct name but\n        the wrong version.  Otherwise, if an `installer` is supplied it will be\n        invoked to obtain the correct version of the requirement and activate\n        it.\n        """"""\n\n        # set up the stack\n        requirements = list(requirements)[::-1]\n        # set of processed requirements\n        processed = {}\n        # key -> dist\n        best = {}\n        to_activate = []\n\n        while requirements:\n            # process dependencies breadth-first\n            req = requirements.pop(0)\n            if req in processed:\n                # Ignore cyclic or redundant dependencies\n                continue\n            dist = best.get(req.key)\n            if dist is None:\n                # Find the best distribution and add it to the map\n                dist = self.by_key.get(req.key)\n                if dist is None or (dist not in req and replace_conflicting):\n                    ws = self\n                    if env is None:\n                        if dist is None:\n                            env = Environment(self.entries)\n                        else:\n                            # Use an empty environment and workingset to avoid\n                            # any further conflicts with the conflicting\n                            # distribution\n                            env = Environment([])\n                            ws = WorkingSet([])\n                    dist = best[req.key] = env.best_match(req, ws, installer)\n                    if dist is None:\n                        #msg = (""The \'%s\' distribution was not found on this ""\n                        #       ""system, and is required by this application."")\n                        #raise DistributionNotFound(msg % req)\n\n                        # unfortunately, zc.buildout uses a str(err)\n                        # to get the name of the distribution here..\n                        raise DistributionNotFound(req)\n                to_activate.append(dist)\n            if dist not in req:\n                # Oops, the ""best"" so far conflicts with a dependency\n                raise VersionConflict(dist, req) # XXX put more info here\n            requirements.extend(dist.requires(req.extras)[::-1])\n            processed[req] = True\n\n        # return list of distros to activate\n        return to_activate\n\n    def find_plugins(self, plugin_env, full_env=None, installer=None,\n            fallback=True):\n        """"""Find all activatable distributions in `plugin_env`\n\n        Example usage::\n\n            distributions, errors = working_set.find_plugins(\n                Environment(plugin_dirlist)\n            )\n            map(working_set.add, distributions)  # add plugins+libs to sys.path\n            print \'Could not load\', errors        # display errors\n\n        The `plugin_env` should be an ``Environment`` instance that contains\n        only distributions that are in the project\'s ""plugin directory"" or\n        directories. The `full_env`, if supplied, should be an ``Environment``\n        contains all currently-available distributions.  If `full_env` is not\n        supplied, one is created automatically from the ``WorkingSet`` this\n        method is called on, which will typically mean that every directory on\n        ``sys.path`` will be scanned for distributions.\n\n        `installer` is a standard installer callback as used by the\n        ``resolve()`` method. The `fallback` flag indicates whether we should\n        attempt to resolve older versions of a plugin if the newest version\n        cannot be resolved.\n\n        This method returns a 2-tuple: (`distributions`, `error_info`), where\n        `distributions` is a list of the distributions found in `plugin_env`\n        that were loadable, along with any other distributions that are needed\n        to resolve their dependencies.  `error_info` is a dictionary mapping\n        unloadable plugin distributions to an exception instance describing the\n        error that occurred. Usually this will be a ``DistributionNotFound`` or\n        ``VersionConflict`` instance.\n        """"""\n\n        plugin_projects = list(plugin_env)\n        # scan project names in alphabetic order\n        plugin_projects.sort()\n\n        error_info = {}\n        distributions = {}\n\n        if full_env is None:\n            env = Environment(self.entries)\n            env += plugin_env\n        else:\n            env = full_env + plugin_env\n\n        shadow_set = self.__class__([])\n        # put all our entries in shadow_set\n        list(map(shadow_set.add, self))\n\n        for project_name in plugin_projects:\n\n            for dist in plugin_env[project_name]:\n\n                req = [dist.as_requirement()]\n\n                try:\n                    resolvees = shadow_set.resolve(req, env, installer)\n\n                except ResolutionError:\n                    v = sys.exc_info()[1]\n                    # save error info\n                    error_info[dist] = v\n                    if fallback:\n                        # try the next older version of project\n                        continue\n                    else:\n                        # give up on this project, keep going\n                        break\n\n                else:\n                    list(map(shadow_set.add, resolvees))\n                    distributions.update(dict.fromkeys(resolvees))\n\n                    # success, no need to try any more versions of this project\n                    break\n\n        distributions = list(distributions)\n        distributions.sort()\n\n        return distributions, error_info\n\n    def require(self, *requirements):\n        """"""Ensure that distributions matching `requirements` are activated\n\n        `requirements` must be a string or a (possibly-nested) sequence\n        thereof, specifying the distributions and versions required.  The\n        return value is a sequence of the distributions that needed to be\n        activated to fulfill the requirements; all relevant distributions are\n        included, even if they were already activated in this working set.\n        """"""\n        needed = self.resolve(parse_requirements(requirements))\n\n        for dist in needed:\n            self.add(dist)\n\n        return needed\n\n    def subscribe(self, callback):\n        """"""Invoke `callback` for all distributions (including existing ones)""""""\n        if callback in self.callbacks:\n            return\n        self.callbacks.append(callback)\n        for dist in self:\n            callback(dist)\n\n    def _added_new(self, dist):\n        for callback in self.callbacks:\n            callback(dist)\n\n    def __getstate__(self):\n        return (\n            self.entries[:], self.entry_keys.copy(), self.by_key.copy(),\n            self.callbacks[:]\n        )\n\n    def __setstate__(self, e_k_b_c):\n        entries, keys, by_key, callbacks = e_k_b_c\n        self.entries = entries[:]\n        self.entry_keys = keys.copy()\n        self.by_key = by_key.copy()\n        self.callbacks = callbacks[:]\n\n\nclass Environment(object):\n    """"""Searchable snapshot of distributions on a search path""""""\n\n    def __init__(self, search_path=None, platform=get_supported_platform(),\n            python=PY_MAJOR):\n        """"""Snapshot distributions available on a search path\n\n        Any distributions found on `search_path` are added to the environment.\n        `search_path` should be a sequence of ``sys.path`` items.  If not\n        supplied, ``sys.path`` is used.\n\n        `platform` is an optional string specifying the name of the platform\n        that platform-specific distributions must be compatible with.  If\n        unspecified, it defaults to the current platform.  `python` is an\n        optional string naming the desired version of Python (e.g. ``\'3.3\'``);\n        it defaults to the current version.\n\n        You may explicitly set `platform` (and/or `python`) to ``None`` if you\n        wish to map *all* distributions, not just those compatible with the\n        running platform or Python version.\n        """"""\n        self._distmap = {}\n        self.platform = platform\n        self.python = python\n        self.scan(search_path)\n\n    def can_add(self, dist):\n        """"""Is distribution `dist` acceptable for this environment?\n\n        The distribution must match the platform and python version\n        requirements specified when this environment was created, or False\n        is returned.\n        """"""\n        return (self.python is None or dist.py_version is None\n            or dist.py_version==self.python) \\\n            and compatible_platforms(dist.platform, self.platform)\n\n    def remove(self, dist):\n        """"""Remove `dist` from the environment""""""\n        self._distmap[dist.key].remove(dist)\n\n    def scan(self, search_path=None):\n        """"""Scan `search_path` for distributions usable in this environment\n\n        Any distributions found are added to the environment.\n        `search_path` should be a sequence of ``sys.path`` items.  If not\n        supplied, ``sys.path`` is used.  Only distributions conforming to\n        the platform/python version defined at initialization are added.\n        """"""\n        if search_path is None:\n            search_path = sys.path\n\n        for item in search_path:\n            for dist in find_distributions(item):\n                self.add(dist)\n\n    def __getitem__(self, project_name):\n        """"""Return a newest-to-oldest list of distributions for `project_name`\n\n        Uses case-insensitive `project_name` comparison, assuming all the\n        project\'s distributions use their project\'s name converted to all\n        lowercase as their key.\n\n        """"""\n        distribution_key = project_name.lower()\n        return self._distmap.get(distribution_key, [])\n\n    def add(self, dist):\n        """"""Add `dist` if we ``can_add()`` it and it has not already been added\n        """"""\n        if self.can_add(dist) and dist.has_version():\n            dists = self._distmap.setdefault(dist.key, [])\n            if dist not in dists:\n                dists.append(dist)\n                dists.sort(key=operator.attrgetter(\'hashcmp\'), reverse=True)\n\n    def best_match(self, req, working_set, installer=None):\n        """"""Find distribution best matching `req` and usable on `working_set`\n\n        This calls the ``find(req)`` method of the `working_set` to see if a\n        suitable distribution is already active.  (This may raise\n        ``VersionConflict`` if an unsuitable version of the project is already\n        active in the specified `working_set`.)  If a suitable distribution\n        isn\'t active, this method returns the newest distribution in the\n        environment that meets the ``Requirement`` in `req`.  If no suitable\n        distribution is found, and `installer` is supplied, then the result of\n        calling the environment\'s ``obtain(req, installer)`` method will be\n        returned.\n        """"""\n        dist = working_set.find(req)\n        if dist is not None:\n            return dist\n        for dist in self[req.key]:\n            if dist in req:\n                return dist\n        # try to download/install\n        return self.obtain(req, installer)\n\n    def obtain(self, requirement, installer=None):\n        """"""Obtain a distribution matching `requirement` (e.g. via download)\n\n        Obtain a distro that matches requirement (e.g. via download).  In the\n        base ``Environment`` class, this routine just returns\n        ``installer(requirement)``, unless `installer` is None, in which case\n        None is returned instead.  This method is a hook that allows subclasses\n        to attempt other ways of obtaining a distribution before falling back\n        to the `installer` argument.""""""\n        if installer is not None:\n            return installer(requirement)\n\n    def __iter__(self):\n        """"""Yield the unique project names of the available distributions""""""\n        for key in self._distmap.keys():\n            if self[key]: yield key\n\n    def __iadd__(self, other):\n        """"""In-place addition of a distribution or environment""""""\n        if isinstance(other, Distribution):\n            self.add(other)\n        elif isinstance(other, Environment):\n            for project in other:\n                for dist in other[project]:\n                    self.add(dist)\n        else:\n            raise TypeError(""Can\'t add %r to environment"" % (other,))\n        return self\n\n    def __add__(self, other):\n        """"""Add an environment or distribution to an environment""""""\n        new = self.__class__([], platform=None, python=None)\n        for env in self, other:\n            new += env\n        return new\n\n\n# XXX backward compatibility\nAvailableDistributions = Environment\n\n\nclass ExtractionError(RuntimeError):\n    """"""An error occurred extracting a resource\n\n    The following attributes are available from instances of this exception:\n\n    manager\n        The resource manager that raised this exception\n\n    cache_path\n        The base directory for resource extraction\n\n    original_error\n        The exception instance that caused extraction to fail\n    """"""\n\n\nclass ResourceManager:\n    """"""Manage resource extraction and packages""""""\n    extraction_path = None\n\n    def __init__(self):\n        self.cached_files = {}\n\n    def resource_exists(self, package_or_requirement, resource_name):\n        """"""Does the named resource exist?""""""\n        return get_provider(package_or_requirement).has_resource(resource_name)\n\n    def resource_isdir(self, package_or_requirement, resource_name):\n        """"""Is the named resource an existing directory?""""""\n        return get_provider(package_or_requirement).resource_isdir(\n            resource_name\n        )\n\n    def resource_filename(self, package_or_requirement, resource_name):\n        """"""Return a true filesystem path for specified resource""""""\n        return get_provider(package_or_requirement).get_resource_filename(\n            self, resource_name\n        )\n\n    def resource_stream(self, package_or_requirement, resource_name):\n        """"""Return a readable file-like object for specified resource""""""\n        return get_provider(package_or_requirement).get_resource_stream(\n            self, resource_name\n        )\n\n    def resource_string(self, package_or_requirement, resource_name):\n        """"""Return specified resource as a string""""""\n        return get_provider(package_or_requirement).get_resource_string(\n            self, resource_name\n        )\n\n    def resource_listdir(self, package_or_requirement, resource_name):\n        """"""List the contents of the named resource directory""""""\n        return get_provider(package_or_requirement).resource_listdir(\n            resource_name\n        )\n\n    def extraction_error(self):\n        """"""Give an error message for problems extracting file(s)""""""\n\n        old_exc = sys.exc_info()[1]\n        cache_path = self.extraction_path or get_default_cache()\n\n        err = ExtractionError(""""""Can\'t extract file(s) to egg cache\n\nThe following error occurred while trying to extract file(s) to the Python egg\ncache:\n\n  %s\n\nThe Python egg cache directory is currently set to:\n\n  %s\n\nPerhaps your account does not have write access to this directory?  You can\nchange the cache directory by setting the PYTHON_EGG_CACHE environment\nvariable to point to an accessible directory.\n"""""" % (old_exc, cache_path)\n        )\n        err.manager = self\n        err.cache_path = cache_path\n        err.original_error = old_exc\n        raise err\n\n    def get_cache_path(self, archive_name, names=()):\n        """"""Return absolute location in cache for `archive_name` and `names`\n\n        The parent directory of the resulting path will be created if it does\n        not already exist.  `archive_name` should be the base filename of the\n        enclosing egg (which may not be the name of the enclosing zipfile!),\n        including its "".egg"" extension.  `names`, if provided, should be a\n        sequence of path name parts ""under"" the egg\'s extraction location.\n\n        This method should only be called by resource providers that need to\n        obtain an extraction location, and only for names they intend to\n        extract, as it tracks the generated names for possible cleanup later.\n        """"""\n        extract_path = self.extraction_path or get_default_cache()\n        target_path = os.path.join(extract_path, archive_name+\'-tmp\', *names)\n        try:\n            _bypass_ensure_directory(target_path)\n        except:\n            self.extraction_error()\n\n        self._warn_unsafe_extraction_path(extract_path)\n\n        self.cached_files[target_path] = 1\n        return target_path\n\n    @staticmethod\n    def _warn_unsafe_extraction_path(path):\n        """"""\n        If the default extraction path is overridden and set to an insecure\n        location, such as /tmp, it opens up an opportunity for an attacker to\n        replace an extracted file with an unauthorized payload. Warn the user\n        if a known insecure location is used.\n\n        See Distribute #375 for more details.\n        """"""\n        if os.name == \'nt\' and not path.startswith(os.environ[\'windir\']):\n            # On Windows, permissions are generally restrictive by default\n            #  and temp directories are not writable by other users, so\n            #  bypass the warning.\n            return\n        mode = os.stat(path).st_mode\n        if mode & stat.S_IWOTH or mode & stat.S_IWGRP:\n            msg = (""%s is writable by group/others and vulnerable to attack ""\n                ""when ""\n                ""used with get_resource_filename. Consider a more secure ""\n                ""location (set with .set_extraction_path or the ""\n                ""PYTHON_EGG_CACHE environment variable)."" % path)\n            warnings.warn(msg, UserWarning)\n\n    def postprocess(self, tempname, filename):\n        """"""Perform any platform-specific postprocessing of `tempname`\n\n        This is where Mac header rewrites should be done; other platforms don\'t\n        have anything special they should do.\n\n        Resource providers should call this method ONLY after successfully\n        extracting a compressed resource.  They must NOT call it on resources\n        that are already in the filesystem.\n\n        `tempname` is the current (temporary) name of the file, and `filename`\n        is the name it will be renamed to by the caller after this routine\n        returns.\n        """"""\n\n        if os.name == \'posix\':\n            # Make the resource executable\n            mode = ((os.stat(tempname).st_mode) | 0o555) & 0o7777\n            os.chmod(tempname, mode)\n\n    def set_extraction_path(self, path):\n        """"""Set the base path where resources will be extracted to, if needed.\n\n        If you do not call this routine before any extractions take place, the\n        path defaults to the return value of ``get_default_cache()``.  (Which\n        is based on the ``PYTHON_EGG_CACHE`` environment variable, with various\n        platform-specific fallbacks.  See that routine\'s documentation for more\n        details.)\n\n        Resources are extracted to subdirectories of this path based upon\n        information given by the ``IResourceProvider``.  You may set this to a\n        temporary directory, but then you must call ``cleanup_resources()`` to\n        delete the extracted files when done.  There is no guarantee that\n        ``cleanup_resources()`` will be able to remove all extracted files.\n\n        (Note: you may not change the extraction path for a given resource\n        manager once resources have been extracted, unless you first call\n        ``cleanup_resources()``.)\n        """"""\n        if self.cached_files:\n            raise ValueError(\n                ""Can\'t change extraction path, files already extracted""\n            )\n\n        self.extraction_path = path\n\n    def cleanup_resources(self, force=False):\n        """"""\n        Delete all extracted resource files and directories, returning a list\n        of the file and directory names that could not be successfully removed.\n        This function does not have any concurrency protection, so it should\n        generally only be called when the extraction path is a temporary\n        directory exclusive to a single process.  This method is not\n        automatically called; you must call it explicitly or register it as an\n        ``atexit`` function if you wish to ensure cleanup of a temporary\n        directory used for extractions.\n        """"""\n        # XXX\n\ndef get_default_cache():\n    """"""Determine the default cache location\n\n    This returns the ``PYTHON_EGG_CACHE`` environment variable, if set.\n    Otherwise, on Windows, it returns a ""Python-Eggs"" subdirectory of the\n    ""Application Data"" directory.  On all other systems, it\'s ""~/.python-eggs"".\n    """"""\n    try:\n        return os.environ[\'PYTHON_EGG_CACHE\']\n    except KeyError:\n        pass\n\n    if os.name!=\'nt\':\n        return os.path.expanduser(\'~/.python-eggs\')\n\n    # XXX this may be locale-specific!\n    app_data = \'Application Data\'\n    app_homes = [\n        # best option, should be locale-safe\n        ((\'APPDATA\',), None),\n        ((\'USERPROFILE\',), app_data),\n        ((\'HOMEDRIVE\',\'HOMEPATH\'), app_data),\n        ((\'HOMEPATH\',), app_data),\n        ((\'HOME\',), None),\n        # 95/98/ME\n        ((\'WINDIR\',), app_data),\n    ]\n\n    for keys, subdir in app_homes:\n        dirname = \'\'\n        for key in keys:\n            if key in os.environ:\n                dirname = os.path.join(dirname, os.environ[key])\n            else:\n                break\n        else:\n            if subdir:\n                dirname = os.path.join(dirname, subdir)\n            return os.path.join(dirname, \'Python-Eggs\')\n    else:\n        raise RuntimeError(\n            ""Please set the PYTHON_EGG_CACHE enviroment variable""\n        )\n\ndef safe_name(name):\n    """"""Convert an arbitrary string to a standard distribution name\n\n    Any runs of non-alphanumeric/. characters are replaced with a single \'-\'.\n    """"""\n    return re.sub(\'[^A-Za-z0-9.]+\', \'-\', name)\n\n\ndef safe_version(version):\n    """"""Convert an arbitrary string to a standard version string\n\n    Spaces become dots, and all other non-alphanumeric characters become\n    dashes, with runs of multiple dashes condensed to a single dash.\n    """"""\n    version = version.replace(\' \',\'.\')\n    return re.sub(\'[^A-Za-z0-9.]+\', \'-\', version)\n\n\ndef safe_extra(extra):\n    """"""Convert an arbitrary string to a standard \'extra\' name\n\n    Any runs of non-alphanumeric characters are replaced with a single \'_\',\n    and the result is always lowercased.\n    """"""\n    return re.sub(\'[^A-Za-z0-9.]+\', \'_\', extra).lower()\n\n\ndef to_filename(name):\n    """"""Convert a project or version name to its filename-escaped form\n\n    Any \'-\' characters are currently replaced with \'_\'.\n    """"""\n    return name.replace(\'-\',\'_\')\n\n\nclass MarkerEvaluation(object):\n    values = {\n        \'os_name\': lambda: os.name,\n        \'sys_platform\': lambda: sys.platform,\n        \'python_full_version\': platform.python_version,\n        \'python_version\': lambda: platform.python_version()[:3],\n        \'platform_version\': platform.version,\n        \'platform_machine\': platform.machine,\n        \'python_implementation\': platform.python_implementation,\n    }\n\n    @classmethod\n    def is_invalid_marker(cls, text):\n        """"""\n        Validate text as a PEP 426 environment marker; return an exception\n        if invalid or False otherwise.\n        """"""\n        try:\n            cls.evaluate_marker(text)\n        except SyntaxError:\n            return cls.normalize_exception(sys.exc_info()[1])\n        return False\n\n    @staticmethod\n    def normalize_exception(exc):\n        """"""\n        Given a SyntaxError from a marker evaluation, normalize the error\n        message:\n         - Remove indications of filename and line number.\n         - Replace platform-specific error messages with standard error\n           messages.\n        """"""\n        subs = {\n            \'unexpected EOF while parsing\': \'invalid syntax\',\n            \'parenthesis is never closed\': \'invalid syntax\',\n        }\n        exc.filename = None\n        exc.lineno = None\n        exc.msg = subs.get(exc.msg, exc.msg)\n        return exc\n\n    @classmethod\n    def and_test(cls, nodelist):\n        # MUST NOT short-circuit evaluation, or invalid syntax can be skipped!\n        items = [\n            cls.interpret(nodelist[i])\n            for i in range(1, len(nodelist), 2)\n        ]\n        return functools.reduce(operator.and_, items)\n\n    @classmethod\n    def test(cls, nodelist):\n        # MUST NOT short-circuit evaluation, or invalid syntax can be skipped!\n        items = [\n            cls.interpret(nodelist[i])\n            for i in range(1, len(nodelist), 2)\n        ]\n        return functools.reduce(operator.or_, items)\n\n    @classmethod\n    def atom(cls, nodelist):\n        t = nodelist[1][0]\n        if t == token.LPAR:\n            if nodelist[2][0] == token.RPAR:\n                raise SyntaxError(""Empty parentheses"")\n            return cls.interpret(nodelist[2])\n        msg = ""Language feature not supported in environment markers""\n        raise SyntaxError(msg)\n\n    @classmethod\n    def comparison(cls, nodelist):\n        if len(nodelist) > 4:\n            msg = ""Chained comparison not allowed in environment markers""\n            raise SyntaxError(msg)\n        comp = nodelist[2][1]\n        cop = comp[1]\n        if comp[0] == token.NAME:\n            if len(nodelist[2]) == 3:\n                if cop == \'not\':\n                    cop = \'not in\'\n                else:\n                    cop = \'is not\'\n        try:\n            cop = cls.get_op(cop)\n        except KeyError:\n            msg = repr(cop) + "" operator not allowed in environment markers""\n            raise SyntaxError(msg)\n        return cop(cls.evaluate(nodelist[1]), cls.evaluate(nodelist[3]))\n\n    @classmethod\n    def get_op(cls, op):\n        ops = {\n            symbol.test: cls.test,\n            symbol.and_test: cls.and_test,\n            symbol.atom: cls.atom,\n            symbol.comparison: cls.comparison,\n            \'not in\': lambda x, y: x not in y,\n            \'in\': lambda x, y: x in y,\n            \'==\': operator.eq,\n            \'!=\': operator.ne,\n        }\n        if hasattr(symbol, \'or_test\'):\n            ops[symbol.or_test] = cls.test\n        return ops[op]\n\n    @classmethod\n    def evaluate_marker(cls, text, extra=None):\n        """"""\n        Evaluate a PEP 426 environment marker on CPython 2.4+.\n        Return a boolean indicating the marker result in this environment.\n        Raise SyntaxError if marker is invalid.\n\n        This implementation uses the \'parser\' module, which is not implemented\n        on\n        Jython and has been superseded by the \'ast\' module in Python 2.6 and\n        later.\n        """"""\n        return cls.interpret(parser.expr(text).totuple(1)[1])\n\n    @classmethod\n    def _markerlib_evaluate(cls, text):\n        """"""\n        Evaluate a PEP 426 environment marker using markerlib.\n        Return a boolean indicating the marker result in this environment.\n        Raise SyntaxError if marker is invalid.\n        """"""\n        import _markerlib\n        # markerlib implements Metadata 1.2 (PEP 345) environment markers.\n        # Translate the variables to Metadata 2.0 (PEP 426).\n        env = _markerlib.default_environment()\n        for key in env.keys():\n            new_key = key.replace(\'.\', \'_\')\n            env[new_key] = env.pop(key)\n        try:\n            result = _markerlib.interpret(text, env)\n        except NameError:\n            e = sys.exc_info()[1]\n            raise SyntaxError(e.args[0])\n        return result\n\n    if \'parser\' not in globals():\n        # Fall back to less-complete _markerlib implementation if \'parser\' module\n        # is not available.\n        evaluate_marker = _markerlib_evaluate\n\n    @classmethod\n    def interpret(cls, nodelist):\n        while len(nodelist)==2: nodelist = nodelist[1]\n        try:\n            op = cls.get_op(nodelist[0])\n        except KeyError:\n            raise SyntaxError(""Comparison or logical expression expected"")\n        return op(nodelist)\n\n    @classmethod\n    def evaluate(cls, nodelist):\n        while len(nodelist)==2: nodelist = nodelist[1]\n        kind = nodelist[0]\n        name = nodelist[1]\n        if kind==token.NAME:\n            try:\n                op = cls.values[name]\n            except KeyError:\n                raise SyntaxError(""Unknown name %r"" % name)\n            return op()\n        if kind==token.STRING:\n            s = nodelist[1]\n            if not cls._safe_string(s):\n                raise SyntaxError(\n                    ""Only plain strings allowed in environment markers"")\n            return s[1:-1]\n        msg = ""Language feature not supported in environment markers""\n        raise SyntaxError(msg)\n\n    @staticmethod\n    def _safe_string(cand):\n        return (\n            cand[:1] in ""\'\\"""" and\n            not cand.startswith(\'""""""\') and\n            not cand.startswith(""\'\'\'"") and\n            \'\\\\\' not in cand\n        )\n\ninvalid_marker = MarkerEvaluation.is_invalid_marker\nevaluate_marker = MarkerEvaluation.evaluate_marker\n\nclass NullProvider:\n    """"""Try to implement resources and metadata for arbitrary PEP 302 loaders""""""\n\n    egg_name = None\n    egg_info = None\n    loader = None\n\n    def __init__(self, module):\n        self.loader = getattr(module, \'__loader__\', None)\n        self.module_path = os.path.dirname(getattr(module, \'__file__\', \'\'))\n\n    def get_resource_filename(self, manager, resource_name):\n        return self._fn(self.module_path, resource_name)\n\n    def get_resource_stream(self, manager, resource_name):\n        return BytesIO(self.get_resource_string(manager, resource_name))\n\n    def get_resource_string(self, manager, resource_name):\n        return self._get(self._fn(self.module_path, resource_name))\n\n    def has_resource(self, resource_name):\n        return self._has(self._fn(self.module_path, resource_name))\n\n    def has_metadata(self, name):\n        return self.egg_info and self._has(self._fn(self.egg_info, name))\n\n    if sys.version_info <= (3,):\n        def get_metadata(self, name):\n            if not self.egg_info:\n                return """"\n            return self._get(self._fn(self.egg_info, name))\n    else:\n        def get_metadata(self, name):\n            if not self.egg_info:\n                return """"\n            return self._get(self._fn(self.egg_info, name)).decode(""utf-8"")\n\n    def get_metadata_lines(self, name):\n        return yield_lines(self.get_metadata(name))\n\n    def resource_isdir(self, resource_name):\n        return self._isdir(self._fn(self.module_path, resource_name))\n\n    def metadata_isdir(self, name):\n        return self.egg_info and self._isdir(self._fn(self.egg_info, name))\n\n    def resource_listdir(self, resource_name):\n        return self._listdir(self._fn(self.module_path, resource_name))\n\n    def metadata_listdir(self, name):\n        if self.egg_info:\n            return self._listdir(self._fn(self.egg_info, name))\n        return []\n\n    def run_script(self, script_name, namespace):\n        script = \'scripts/\'+script_name\n        if not self.has_metadata(script):\n            raise ResolutionError(""No script named %r"" % script_name)\n        script_text = self.get_metadata(script).replace(\'\\r\\n\', \'\\n\')\n        script_text = script_text.replace(\'\\r\', \'\\n\')\n        script_filename = self._fn(self.egg_info, script)\n        namespace[\'__file__\'] = script_filename\n        if os.path.exists(script_filename):\n            execfile(script_filename, namespace, namespace)\n        else:\n            from linecache import cache\n            cache[script_filename] = (\n                len(script_text), 0, script_text.split(\'\\n\'), script_filename\n            )\n            script_code = compile(script_text, script_filename,\'exec\')\n            exec(script_code, namespace, namespace)\n\n    def _has(self, path):\n        raise NotImplementedError(\n            ""Can\'t perform this operation for unregistered loader type""\n        )\n\n    def _isdir(self, path):\n        raise NotImplementedError(\n            ""Can\'t perform this operation for unregistered loader type""\n        )\n\n    def _listdir(self, path):\n        raise NotImplementedError(\n            ""Can\'t perform this operation for unregistered loader type""\n        )\n\n    def _fn(self, base, resource_name):\n        if resource_name:\n            return os.path.join(base, *resource_name.split(\'/\'))\n        return base\n\n    def _get(self, path):\n        if hasattr(self.loader, \'get_data\'):\n            return self.loader.get_data(path)\n        raise NotImplementedError(\n            ""Can\'t perform this operation for loaders without \'get_data()\'""\n        )\n\nregister_loader_type(object, NullProvider)\n\n\nclass EggProvider(NullProvider):\n    """"""Provider based on a virtual filesystem""""""\n\n    def __init__(self, module):\n        NullProvider.__init__(self, module)\n        self._setup_prefix()\n\n    def _setup_prefix(self):\n        # we assume here that our metadata may be nested inside a ""basket""\n        # of multiple eggs; that\'s why we use module_path instead of .archive\n        path = self.module_path\n        old = None\n        while path!=old:\n            if path.lower().endswith(\'.egg\'):\n                self.egg_name = os.path.basename(path)\n                self.egg_info = os.path.join(path, \'EGG-INFO\')\n                self.egg_root = path\n                break\n            old = path\n            path, base = os.path.split(path)\n\nclass DefaultProvider(EggProvider):\n    """"""Provides access to package resources in the filesystem""""""\n\n    def _has(self, path):\n        return os.path.exists(path)\n\n    def _isdir(self, path):\n        return os.path.isdir(path)\n\n    def _listdir(self, path):\n        return os.listdir(path)\n\n    def get_resource_stream(self, manager, resource_name):\n        return open(self._fn(self.module_path, resource_name), \'rb\')\n\n    def _get(self, path):\n        with open(path, \'rb\') as stream:\n            return stream.read()\n\nregister_loader_type(type(None), DefaultProvider)\n\nif importlib_bootstrap is not None:\n    register_loader_type(importlib_bootstrap.SourceFileLoader, DefaultProvider)\n\n\nclass EmptyProvider(NullProvider):\n    """"""Provider that returns nothing for all requests""""""\n\n    _isdir = _has = lambda self, path: False\n    _get = lambda self, path: \'\'\n    _listdir = lambda self, path: []\n    module_path = None\n\n    def __init__(self):\n        pass\n\nempty_provider = EmptyProvider()\n\n\ndef build_zipmanifest(path):\n    """"""\n    This builds a similar dictionary to the zipimport directory\n    caches.  However instead of tuples, ZipInfo objects are stored.\n\n    The translation of the tuple is as follows:\n      * [0] - zipinfo.filename on stock pythons this needs ""/"" --> os.sep\n              on pypy it is the same (one reason why distribute did work\n              in some cases on pypy and win32).\n      * [1] - zipinfo.compress_type\n      * [2] - zipinfo.compress_size\n      * [3] - zipinfo.file_size\n      * [4] - len(utf-8 encoding of filename) if zipinfo & 0x800\n              len(ascii encoding of filename) otherwise\n      * [5] - (zipinfo.date_time[0] - 1980) << 9 |\n               zipinfo.date_time[1] << 5 | zipinfo.date_time[2]\n      * [6] - (zipinfo.date_time[3] - 1980) << 11 |\n               zipinfo.date_time[4] << 5 | (zipinfo.date_time[5] // 2)\n      * [7] - zipinfo.CRC\n    """"""\n    zipinfo = dict()\n    with ContextualZipFile(path) as zfile:\n        for zitem in zfile.namelist():\n            zpath = zitem.replace(\'/\', os.sep)\n            zipinfo[zpath] = zfile.getinfo(zitem)\n            assert zipinfo[zpath] is not None\n    return zipinfo\n\n\nclass ContextualZipFile(zipfile.ZipFile):\n    """"""\n    Supplement ZipFile class to support context manager for Python 2.6\n    """"""\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.close()\n\n    def __new__(cls, *args, **kwargs):\n        """"""\n        Construct a ZipFile or ContextualZipFile as appropriate\n        """"""\n        if hasattr(zipfile.ZipFile, \'__exit__\'):\n            return zipfile.ZipFile(*args, **kwargs)\n        return super(ContextualZipFile, cls).__new__(cls)\n\n\nclass ZipProvider(EggProvider):\n    """"""Resource support for zips and eggs""""""\n\n    eagers = None\n\n    def __init__(self, module):\n        EggProvider.__init__(self, module)\n        self.zipinfo = build_zipmanifest(self.loader.archive)\n        self.zip_pre = self.loader.archive+os.sep\n\n    def _zipinfo_name(self, fspath):\n        # Convert a virtual filename (full path to file) into a zipfile subpath\n        # usable with the zipimport directory cache for our target archive\n        if fspath.startswith(self.zip_pre):\n            return fspath[len(self.zip_pre):]\n        raise AssertionError(\n            ""%s is not a subpath of %s"" % (fspath, self.zip_pre)\n        )\n\n    def _parts(self, zip_path):\n        # Convert a zipfile subpath into an egg-relative path part list\n        fspath = self.zip_pre+zip_path  # pseudo-fs path\n        if fspath.startswith(self.egg_root+os.sep):\n            return fspath[len(self.egg_root)+1:].split(os.sep)\n        raise AssertionError(\n            ""%s is not a subpath of %s"" % (fspath, self.egg_root)\n        )\n\n    def get_resource_filename(self, manager, resource_name):\n        if not self.egg_name:\n            raise NotImplementedError(\n                ""resource_filename() only supported for .egg, not .zip""\n            )\n        # no need to lock for extraction, since we use temp names\n        zip_path = self._resource_to_zip(resource_name)\n        eagers = self._get_eager_resources()\n        if \'/\'.join(self._parts(zip_path)) in eagers:\n            for name in eagers:\n                self._extract_resource(manager, self._eager_to_zip(name))\n        return self._extract_resource(manager, zip_path)\n\n    @staticmethod\n    def _get_date_and_size(zip_stat):\n        size = zip_stat.file_size\n        # ymdhms+wday, yday, dst\n        date_time = zip_stat.date_time + (0, 0, -1)\n        # 1980 offset already done\n        timestamp = time.mktime(date_time)\n        return timestamp, size\n\n    def _extract_resource(self, manager, zip_path):\n\n        if zip_path in self._index():\n            for name in self._index()[zip_path]:\n                last = self._extract_resource(\n                    manager, os.path.join(zip_path, name)\n                )\n            # return the extracted directory name\n            return os.path.dirname(last)\n\n        timestamp, size = self._get_date_and_size(self.zipinfo[zip_path])\n\n        if not WRITE_SUPPORT:\n            raise IOError(\'""os.rename"" and ""os.unlink"" are not supported \'\n                          \'on this platform\')\n        try:\n\n            real_path = manager.get_cache_path(\n                self.egg_name, self._parts(zip_path)\n            )\n\n            if self._is_current(real_path, zip_path):\n                return real_path\n\n            outf, tmpnam = _mkstemp("".$extract"", dir=os.path.dirname(real_path))\n            os.write(outf, self.loader.get_data(zip_path))\n            os.close(outf)\n            utime(tmpnam, (timestamp, timestamp))\n            manager.postprocess(tmpnam, real_path)\n\n            try:\n                rename(tmpnam, real_path)\n\n            except os.error:\n                if os.path.isfile(real_path):\n                    if self._is_current(real_path, zip_path):\n                        # the file became current since it was checked above,\n                        #  so proceed.\n                        return real_path\n                    # Windows, del old file and retry\n                    elif os.name==\'nt\':\n                        unlink(real_path)\n                        rename(tmpnam, real_path)\n                        return real_path\n                raise\n\n        except os.error:\n            # report a user-friendly error\n            manager.extraction_error()\n\n        return real_path\n\n    def _is_current(self, file_path, zip_path):\n        """"""\n        Return True if the file_path is current for this zip_path\n        """"""\n        timestamp, size = self._get_date_and_size(self.zipinfo[zip_path])\n        if not os.path.isfile(file_path):\n            return False\n        stat = os.stat(file_path)\n        if stat.st_size!=size or stat.st_mtime!=timestamp:\n            return False\n        # check that the contents match\n        zip_contents = self.loader.get_data(zip_path)\n        with open(file_path, \'rb\') as f:\n            file_contents = f.read()\n        return zip_contents == file_contents\n\n    def _get_eager_resources(self):\n        if self.eagers is None:\n            eagers = []\n            for name in (\'native_libs.txt\', \'eager_resources.txt\'):\n                if self.has_metadata(name):\n                    eagers.extend(self.get_metadata_lines(name))\n            self.eagers = eagers\n        return self.eagers\n\n    def _index(self):\n        try:\n            return self._dirindex\n        except AttributeError:\n            ind = {}\n            for path in self.zipinfo:\n                parts = path.split(os.sep)\n                while parts:\n                    parent = os.sep.join(parts[:-1])\n                    if parent in ind:\n                        ind[parent].append(parts[-1])\n                        break\n                    else:\n                        ind[parent] = [parts.pop()]\n            self._dirindex = ind\n            return ind\n\n    def _has(self, fspath):\n        zip_path = self._zipinfo_name(fspath)\n        return zip_path in self.zipinfo or zip_path in self._index()\n\n    def _isdir(self, fspath):\n        return self._zipinfo_name(fspath) in self._index()\n\n    def _listdir(self, fspath):\n        return list(self._index().get(self._zipinfo_name(fspath), ()))\n\n    def _eager_to_zip(self, resource_name):\n        return self._zipinfo_name(self._fn(self.egg_root, resource_name))\n\n    def _resource_to_zip(self, resource_name):\n        return self._zipinfo_name(self._fn(self.module_path, resource_name))\n\nregister_loader_type(zipimport.zipimporter, ZipProvider)\n\n\nclass FileMetadata(EmptyProvider):\n    """"""Metadata handler for standalone PKG-INFO files\n\n    Usage::\n\n        metadata = FileMetadata(""/path/to/PKG-INFO"")\n\n    This provider rejects all data and metadata requests except for PKG-INFO,\n    which is treated as existing, and will be the contents of the file at\n    the provided location.\n    """"""\n\n    def __init__(self, path):\n        self.path = path\n\n    def has_metadata(self, name):\n        return name==\'PKG-INFO\'\n\n    def get_metadata(self, name):\n        if name==\'PKG-INFO\':\n            with open(self.path,\'rU\') as f:\n                metadata = f.read()\n            return metadata\n        raise KeyError(""No metadata except PKG-INFO is available"")\n\n    def get_metadata_lines(self, name):\n        return yield_lines(self.get_metadata(name))\n\n\nclass PathMetadata(DefaultProvider):\n    """"""Metadata provider for egg directories\n\n    Usage::\n\n        # Development eggs:\n\n        egg_info = ""/path/to/PackageName.egg-info""\n        base_dir = os.path.dirname(egg_info)\n        metadata = PathMetadata(base_dir, egg_info)\n        dist_name = os.path.splitext(os.path.basename(egg_info))[0]\n        dist = Distribution(basedir, project_name=dist_name, metadata=metadata)\n\n        # Unpacked egg directories:\n\n        egg_path = ""/path/to/PackageName-ver-pyver-etc.egg""\n        metadata = PathMetadata(egg_path, os.path.join(egg_path,\'EGG-INFO\'))\n        dist = Distribution.from_filename(egg_path, metadata=metadata)\n    """"""\n\n    def __init__(self, path, egg_info):\n        self.module_path = path\n        self.egg_info = egg_info\n\n\nclass EggMetadata(ZipProvider):\n    """"""Metadata provider for .egg files""""""\n\n    def __init__(self, importer):\n        """"""Create a metadata provider from a zipimporter""""""\n\n        self.zipinfo = build_zipmanifest(importer.archive)\n        self.zip_pre = importer.archive+os.sep\n        self.loader = importer\n        if importer.prefix:\n            self.module_path = os.path.join(importer.archive, importer.prefix)\n        else:\n            self.module_path = importer.archive\n        self._setup_prefix()\n\n_declare_state(\'dict\', _distribution_finders = {})\n\ndef register_finder(importer_type, distribution_finder):\n    """"""Register `distribution_finder` to find distributions in sys.path items\n\n    `importer_type` is the type or class of a PEP 302 ""Importer"" (sys.path item\n    handler), and `distribution_finder` is a callable that, passed a path\n    item and the importer instance, yields ``Distribution`` instances found on\n    that path item.  See ``pkg_resources.find_on_path`` for an example.""""""\n    _distribution_finders[importer_type] = distribution_finder\n\ndef find_distributions(path_item, only=False):\n    """"""Yield distributions accessible via `path_item`""""""\n    importer = get_importer(path_item)\n    finder = _find_adapter(_distribution_finders, importer)\n    return finder(importer, path_item, only)\n\ndef find_eggs_in_zip(importer, path_item, only=False):\n    """"""\n    Find eggs in zip files; possibly multiple nested eggs.\n    """"""\n    if importer.archive.endswith(\'.whl\'):\n        # wheels are not supported with this finder\n        # they don\'t have PKG-INFO metadata, and won\'t ever contain eggs\n        return\n    metadata = EggMetadata(importer)\n    if metadata.has_metadata(\'PKG-INFO\'):\n        yield Distribution.from_filename(path_item, metadata=metadata)\n    if only:\n        # don\'t yield nested distros\n        return\n    for subitem in metadata.resource_listdir(\'/\'):\n        if subitem.endswith(\'.egg\'):\n            subpath = os.path.join(path_item, subitem)\n            for dist in find_eggs_in_zip(zipimport.zipimporter(subpath), subpath):\n                yield dist\n\nregister_finder(zipimport.zipimporter, find_eggs_in_zip)\n\ndef find_nothing(importer, path_item, only=False):\n    return ()\nregister_finder(object, find_nothing)\n\ndef find_on_path(importer, path_item, only=False):\n    """"""Yield distributions accessible on a sys.path directory""""""\n    path_item = _normalize_cached(path_item)\n\n    if os.path.isdir(path_item) and os.access(path_item, os.R_OK):\n        if path_item.lower().endswith(\'.egg\'):\n            # unpacked egg\n            yield Distribution.from_filename(\n                path_item, metadata=PathMetadata(\n                    path_item, os.path.join(path_item,\'EGG-INFO\')\n                )\n            )\n        else:\n            # scan for .egg and .egg-info in directory\n            for entry in os.listdir(path_item):\n                lower = entry.lower()\n                if lower.endswith(\'.egg-info\') or lower.endswith(\'.dist-info\'):\n                    fullpath = os.path.join(path_item, entry)\n                    if os.path.isdir(fullpath):\n                        # egg-info directory, allow getting metadata\n                        metadata = PathMetadata(path_item, fullpath)\n                    else:\n                        metadata = FileMetadata(fullpath)\n                    yield Distribution.from_location(\n                        path_item, entry, metadata, precedence=DEVELOP_DIST\n                    )\n                elif not only and lower.endswith(\'.egg\'):\n                    dists = find_distributions(os.path.join(path_item, entry))\n                    for dist in dists:\n                        yield dist\n                elif not only and lower.endswith(\'.egg-link\'):\n                    with open(os.path.join(path_item, entry)) as entry_file:\n                        entry_lines = entry_file.readlines()\n                    for line in entry_lines:\n                        if not line.strip():\n                            continue\n                        path = os.path.join(path_item, line.rstrip())\n                        dists = find_distributions(path)\n                        for item in dists:\n                            yield item\n                        break\nregister_finder(pkgutil.ImpImporter, find_on_path)\n\nif importlib_bootstrap is not None:\n    register_finder(importlib_bootstrap.FileFinder, find_on_path)\n\n_declare_state(\'dict\', _namespace_handlers={})\n_declare_state(\'dict\', _namespace_packages={})\n\n\ndef register_namespace_handler(importer_type, namespace_handler):\n    """"""Register `namespace_handler` to declare namespace packages\n\n    `importer_type` is the type or class of a PEP 302 ""Importer"" (sys.path item\n    handler), and `namespace_handler` is a callable like this::\n\n        def namespace_handler(importer, path_entry, moduleName, module):\n            # return a path_entry to use for child packages\n\n    Namespace handlers are only called if the importer object has already\n    agreed that it can handle the relevant path item, and they should only\n    return a subpath if the module __path__ does not already contain an\n    equivalent subpath.  For an example namespace handler, see\n    ``pkg_resources.file_ns_handler``.\n    """"""\n    _namespace_handlers[importer_type] = namespace_handler\n\ndef _handle_ns(packageName, path_item):\n    """"""Ensure that named package includes a subpath of path_item (if needed)""""""\n\n    importer = get_importer(path_item)\n    if importer is None:\n        return None\n    loader = importer.find_module(packageName)\n    if loader is None:\n        return None\n    module = sys.modules.get(packageName)\n    if module is None:\n        module = sys.modules[packageName] = imp.new_module(packageName)\n        module.__path__ = []\n        _set_parent_ns(packageName)\n    elif not hasattr(module,\'__path__\'):\n        raise TypeError(""Not a package:"", packageName)\n    handler = _find_adapter(_namespace_handlers, importer)\n    subpath = handler(importer, path_item, packageName, module)\n    if subpath is not None:\n        path = module.__path__\n        path.append(subpath)\n        loader.load_module(packageName)\n        for path_item in path:\n            if path_item not in module.__path__:\n                module.__path__.append(path_item)\n    return subpath\n\ndef declare_namespace(packageName):\n    """"""Declare that package \'packageName\' is a namespace package""""""\n\n    imp.acquire_lock()\n    try:\n        if packageName in _namespace_packages:\n            return\n\n        path, parent = sys.path, None\n        if \'.\' in packageName:\n            parent = \'.\'.join(packageName.split(\'.\')[:-1])\n            declare_namespace(parent)\n            if parent not in _namespace_packages:\n                __import__(parent)\n            try:\n                path = sys.modules[parent].__path__\n            except AttributeError:\n                raise TypeError(""Not a package:"", parent)\n\n        # Track what packages are namespaces, so when new path items are added,\n        # they can be updated\n        _namespace_packages.setdefault(parent,[]).append(packageName)\n        _namespace_packages.setdefault(packageName,[])\n\n        for path_item in path:\n            # Ensure all the parent\'s path items are reflected in the child,\n            # if they apply\n            _handle_ns(packageName, path_item)\n\n    finally:\n        imp.release_lock()\n\ndef fixup_namespace_packages(path_item, parent=None):\n    """"""Ensure that previously-declared namespace packages include path_item""""""\n    imp.acquire_lock()\n    try:\n        for package in _namespace_packages.get(parent,()):\n            subpath = _handle_ns(package, path_item)\n            if subpath: fixup_namespace_packages(subpath, package)\n    finally:\n        imp.release_lock()\n\ndef file_ns_handler(importer, path_item, packageName, module):\n    """"""Compute an ns-package subpath for a filesystem or zipfile importer""""""\n\n    subpath = os.path.join(path_item, packageName.split(\'.\')[-1])\n    normalized = _normalize_cached(subpath)\n    for item in module.__path__:\n        if _normalize_cached(item)==normalized:\n            break\n    else:\n        # Only return the path if it\'s not already there\n        return subpath\n\nregister_namespace_handler(pkgutil.ImpImporter, file_ns_handler)\nregister_namespace_handler(zipimport.zipimporter, file_ns_handler)\n\nif importlib_bootstrap is not None:\n    register_namespace_handler(importlib_bootstrap.FileFinder, file_ns_handler)\n\n\ndef null_ns_handler(importer, path_item, packageName, module):\n    return None\n\nregister_namespace_handler(object, null_ns_handler)\n\n\ndef normalize_path(filename):\n    """"""Normalize a file/dir name for comparison purposes""""""\n    return os.path.normcase(os.path.realpath(filename))\n\ndef _normalize_cached(filename, _cache={}):\n    try:\n        return _cache[filename]\n    except KeyError:\n        _cache[filename] = result = normalize_path(filename)\n        return result\n\ndef _set_parent_ns(packageName):\n    parts = packageName.split(\'.\')\n    name = parts.pop()\n    if parts:\n        parent = \'.\'.join(parts)\n        setattr(sys.modules[parent], name, sys.modules[packageName])\n\n\ndef yield_lines(strs):\n    """"""Yield non-empty/non-comment lines of a ``basestring`` or sequence""""""\n    if isinstance(strs, basestring):\n        for s in strs.splitlines():\n            s = s.strip()\n            # skip blank lines/comments\n            if s and not s.startswith(\'#\'):\n                yield s\n    else:\n        for ss in strs:\n            for s in yield_lines(ss):\n                yield s\n\n# whitespace and comment\nLINE_END = re.compile(r""\\s*(#.*)?$"").match\n# line continuation\nCONTINUE = re.compile(r""\\s*\\\\\\s*(#.*)?$"").match\n# Distribution or extra\nDISTRO = re.compile(r""\\s*((\\w|[-.])+)"").match\n# ver. info\nVERSION = re.compile(r""\\s*(<=?|>=?|==|!=)\\s*((\\w|[-.])+)"").match\n# comma between items\nCOMMA = re.compile(r""\\s*,"").match\nOBRACKET = re.compile(r""\\s*\\["").match\nCBRACKET = re.compile(r""\\s*\\]"").match\nMODULE = re.compile(r""\\w+(\\.\\w+)*$"").match\nEGG_NAME = re.compile(\n    r""(?P<name>[^-]+)""\n    r""( -(?P<ver>[^-]+) (-py(?P<pyver>[^-]+) (-(?P<plat>.+))? )? )?"",\n    re.VERBOSE | re.IGNORECASE\n).match\n\ncomponent_re = re.compile(r\'(\\d+ | [a-z]+ | \\.| -)\', re.VERBOSE)\nreplace = {\'pre\':\'c\', \'preview\':\'c\',\'-\':\'final-\',\'rc\':\'c\',\'dev\':\'@\'}.get\n\ndef _parse_version_parts(s):\n    for part in component_re.split(s):\n        part = replace(part, part)\n        if not part or part==\'.\':\n            continue\n        if part[:1] in \'0123456789\':\n            # pad for numeric comparison\n            yield part.zfill(8)\n        else:\n            yield \'*\'+part\n\n    # ensure that alpha/beta/candidate are before final\n    yield \'*final\'\n\ndef parse_version(s):\n    """"""Convert a version string to a chronologically-sortable key\n\n    This is a rough cross between distutils\' StrictVersion and LooseVersion;\n    if you give it versions that would work with StrictVersion, then it behaves\n    the same; otherwise it acts like a slightly-smarter LooseVersion. It is\n    *possible* to create pathological version coding schemes that will fool\n    this parser, but they should be very rare in practice.\n\n    The returned value will be a tuple of strings.  Numeric portions of the\n    version are padded to 8 digits so they will compare numerically, but\n    without relying on how numbers compare relative to strings.  Dots are\n    dropped, but dashes are retained.  Trailing zeros between alpha segments\n    or dashes are suppressed, so that e.g. ""2.4.0"" is considered the same as\n    ""2.4"". Alphanumeric parts are lower-cased.\n\n    The algorithm assumes that strings like ""-"" and any alpha string that\n    alphabetically follows ""final""  represents a ""patch level"".  So, ""2.4-1""\n    is assumed to be a branch or patch of ""2.4"", and therefore ""2.4.1"" is\n    considered newer than ""2.4-1"", which in turn is newer than ""2.4"".\n\n    Strings like ""a"", ""b"", ""c"", ""alpha"", ""beta"", ""candidate"" and so on (that\n    come before ""final"" alphabetically) are assumed to be pre-release versions,\n    so that the version ""2.4"" is considered newer than ""2.4a1"".\n\n    Finally, to handle miscellaneous cases, the strings ""pre"", ""preview"", and\n    ""rc"" are treated as if they were ""c"", i.e. as though they were release\n    candidates, and therefore are not as new as a version string that does not\n    contain them, and ""dev"" is replaced with an \'@\' so that it sorts lower than\n    than any other pre-release tag.\n    """"""\n    parts = []\n    for part in _parse_version_parts(s.lower()):\n        if part.startswith(\'*\'):\n            # remove \'-\' before a prerelease tag\n            if part<\'*final\':\n                while parts and parts[-1]==\'*final-\': parts.pop()\n            # remove trailing zeros from each series of numeric parts\n            while parts and parts[-1]==\'00000000\':\n                parts.pop()\n        parts.append(part)\n    return tuple(parts)\nclass EntryPoint(object):\n    """"""Object representing an advertised importable object""""""\n\n    def __init__(self, name, module_name, attrs=(), extras=(), dist=None):\n        if not MODULE(module_name):\n            raise ValueError(""Invalid module name"", module_name)\n        self.name = name\n        self.module_name = module_name\n        self.attrs = tuple(attrs)\n        self.extras = Requirement.parse((""x[%s]"" % \',\'.join(extras))).extras\n        self.dist = dist\n\n    def __str__(self):\n        s = ""%s = %s"" % (self.name, self.module_name)\n        if self.attrs:\n            s += \':\' + \'.\'.join(self.attrs)\n        if self.extras:\n            s += \' [%s]\' % \',\'.join(self.extras)\n        return s\n\n    def __repr__(self):\n        return ""EntryPoint.parse(%r)"" % str(self)\n\n    def load(self, require=True, env=None, installer=None):\n        if require: self.require(env, installer)\n        entry = __import__(self.module_name, globals(), globals(),\n            [\'__name__\'])\n        for attr in self.attrs:\n            try:\n                entry = getattr(entry, attr)\n            except AttributeError:\n                raise ImportError(""%r has no %r attribute"" % (entry, attr))\n        return entry\n\n    def require(self, env=None, installer=None):\n        if self.extras and not self.dist:\n            raise UnknownExtra(""Can\'t require() without a distribution"", self)\n        reqs = self.dist.requires(self.extras)\n        items = working_set.resolve(reqs, env, installer)\n        list(map(working_set.add, items))\n\n    @classmethod\n    def parse(cls, src, dist=None):\n        """"""Parse a single entry point from string `src`\n\n        Entry point syntax follows the form::\n\n            name = some.module:some.attr [extra1, extra2]\n\n        The entry name and module name are required, but the ``:attrs`` and\n        ``[extras]`` parts are optional\n        """"""\n        try:\n            attrs = extras = ()\n            name, value = src.split(\'=\',1)\n            if \'[\' in value:\n                value, extras = value.split(\'[\',1)\n                req = Requirement.parse(""x[""+extras)\n                if req.specs: raise ValueError\n                extras = req.extras\n            if \':\' in value:\n                value, attrs = value.split(\':\',1)\n                if not MODULE(attrs.rstrip()):\n                    raise ValueError\n                attrs = attrs.rstrip().split(\'.\')\n        except ValueError:\n            raise ValueError(\n                ""EntryPoint must be in \'name=module:attrs [extras]\' format"",\n                src\n            )\n        else:\n            return cls(name.strip(), value.strip(), attrs, extras, dist)\n\n    @classmethod\n    def parse_group(cls, group, lines, dist=None):\n        """"""Parse an entry point group""""""\n        if not MODULE(group):\n            raise ValueError(""Invalid group name"", group)\n        this = {}\n        for line in yield_lines(lines):\n            ep = cls.parse(line, dist)\n            if ep.name in this:\n                raise ValueError(""Duplicate entry point"", group, ep.name)\n            this[ep.name]=ep\n        return this\n\n    @classmethod\n    def parse_map(cls, data, dist=None):\n        """"""Parse a map of entry point groups""""""\n        if isinstance(data, dict):\n            data = data.items()\n        else:\n            data = split_sections(data)\n        maps = {}\n        for group, lines in data:\n            if group is None:\n                if not lines:\n                    continue\n                raise ValueError(""Entry points must be listed in groups"")\n            group = group.strip()\n            if group in maps:\n                raise ValueError(""Duplicate group name"", group)\n            maps[group] = cls.parse_group(group, lines, dist)\n        return maps\n\n\ndef _remove_md5_fragment(location):\n    if not location:\n        return \'\'\n    parsed = urlparse(location)\n    if parsed[-1].startswith(\'md5=\'):\n        return urlunparse(parsed[:-1] + (\'\',))\n    return location\n\n\nclass Distribution(object):\n    """"""Wrap an actual or potential sys.path entry w/metadata""""""\n    PKG_INFO = \'PKG-INFO\'\n\n    def __init__(self, location=None, metadata=None, project_name=None,\n            version=None, py_version=PY_MAJOR, platform=None,\n            precedence=EGG_DIST):\n        self.project_name = safe_name(project_name or \'Unknown\')\n        if version is not None:\n            self._version = safe_version(version)\n        self.py_version = py_version\n        self.platform = platform\n        self.location = location\n        self.precedence = precedence\n        self._provider = metadata or empty_provider\n\n    @classmethod\n    def from_location(cls, location, basename, metadata=None,**kw):\n        project_name, version, py_version, platform = [None]*4\n        basename, ext = os.path.splitext(basename)\n        if ext.lower() in _distributionImpl:\n            # .dist-info gets much metadata differently\n            match = EGG_NAME(basename)\n            if match:\n                project_name, version, py_version, platform = match.group(\n                    \'name\',\'ver\',\'pyver\',\'plat\'\n                )\n            cls = _distributionImpl[ext.lower()]\n        return cls(\n            location, metadata, project_name=project_name, version=version,\n            py_version=py_version, platform=platform, **kw\n        )\n\n    @property\n    def hashcmp(self):\n        return (\n            getattr(self, \'parsed_version\', ()),\n            self.precedence,\n            self.key,\n            _remove_md5_fragment(self.location),\n            self.py_version,\n            self.platform,\n        )\n\n    def __hash__(self):\n        return hash(self.hashcmp)\n\n    def __lt__(self, other):\n        return self.hashcmp < other.hashcmp\n\n    def __le__(self, other):\n        return self.hashcmp <= other.hashcmp\n\n    def __gt__(self, other):\n        return self.hashcmp > other.hashcmp\n\n    def __ge__(self, other):\n        return self.hashcmp >= other.hashcmp\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            # It\'s not a Distribution, so they are not equal\n            return False\n        return self.hashcmp == other.hashcmp\n\n    def __ne__(self, other):\n        return not self == other\n\n    # These properties have to be lazy so that we don\'t have to load any\n    # metadata until/unless it\'s actually needed.  (i.e., some distributions\n    # may not know their name or version without loading PKG-INFO)\n\n    @property\n    def key(self):\n        try:\n            return self._key\n        except AttributeError:\n            self._key = key = self.project_name.lower()\n            return key\n\n    @property\n    def parsed_version(self):\n        try:\n            return self._parsed_version\n        except AttributeError:\n            self._parsed_version = pv = parse_version(self.version)\n            return pv\n\n    @property\n    def version(self):\n        try:\n            return self._version\n        except AttributeError:\n            for line in self._get_metadata(self.PKG_INFO):\n                if line.lower().startswith(\'version:\'):\n                    self._version = safe_version(line.split(\':\',1)[1].strip())\n                    return self._version\n            else:\n                tmpl = ""Missing \'Version:\' header and/or %s file""\n                raise ValueError(tmpl % self.PKG_INFO, self)\n\n    @property\n    def _dep_map(self):\n        try:\n            return self.__dep_map\n        except AttributeError:\n            dm = self.__dep_map = {None: []}\n            for name in \'requires.txt\', \'depends.txt\':\n                for extra, reqs in split_sections(self._get_metadata(name)):\n                    if extra:\n                        if \':\' in extra:\n                            extra, marker = extra.split(\':\',1)\n                            if invalid_marker(marker):\n                                # XXX warn\n                                reqs=[]\n                            elif not evaluate_marker(marker):\n                                reqs=[]\n                        extra = safe_extra(extra) or None\n                    dm.setdefault(extra,[]).extend(parse_requirements(reqs))\n            return dm\n\n    def requires(self, extras=()):\n        """"""List of Requirements needed for this distro if `extras` are used""""""\n        dm = self._dep_map\n        deps = []\n        deps.extend(dm.get(None,()))\n        for ext in extras:\n            try:\n                deps.extend(dm[safe_extra(ext)])\n            except KeyError:\n                raise UnknownExtra(\n                    ""%s has no such extra feature %r"" % (self, ext)\n                )\n        return deps\n\n    def _get_metadata(self, name):\n        if self.has_metadata(name):\n            for line in self.get_metadata_lines(name):\n                yield line\n\n    def activate(self, path=None):\n        """"""Ensure distribution is importable on `path` (default=sys.path)""""""\n        if path is None: path = sys.path\n        self.insert_on(path)\n        if path is sys.path:\n            fixup_namespace_packages(self.location)\n            for pkg in self._get_metadata(\'namespace_packages.txt\'):\n                if pkg in sys.modules:\n                    declare_namespace(pkg)\n\n    def egg_name(self):\n        """"""Return what this distribution\'s standard .egg filename should be""""""\n        filename = ""%s-%s-py%s"" % (\n            to_filename(self.project_name), to_filename(self.version),\n            self.py_version or PY_MAJOR\n        )\n\n        if self.platform:\n            filename += \'-\'+self.platform\n        return filename\n\n    def __repr__(self):\n        if self.location:\n            return ""%s (%s)"" % (self, self.location)\n        else:\n            return str(self)\n\n    def __str__(self):\n        try: version = getattr(self,\'version\',None)\n        except ValueError: version = None\n        version = version or ""[unknown version]""\n        return ""%s %s"" % (self.project_name, version)\n\n    def __getattr__(self, attr):\n        """"""Delegate all unrecognized public attributes to .metadata provider""""""\n        if attr.startswith(\'_\'):\n            raise AttributeError(attr)\n        return getattr(self._provider, attr)\n\n    @classmethod\n    def from_filename(cls, filename, metadata=None, **kw):\n        return cls.from_location(\n            _normalize_cached(filename), os.path.basename(filename), metadata,\n            **kw\n        )\n\n    def as_requirement(self):\n        """"""Return a ``Requirement`` that matches this distribution exactly""""""\n        return Requirement.parse(\'%s==%s\' % (self.project_name, self.version))\n\n    def load_entry_point(self, group, name):\n        """"""Return the `name` entry point of `group` or raise ImportError""""""\n        ep = self.get_entry_info(group, name)\n        if ep is None:\n            raise ImportError(""Entry point %r not found"" % ((group, name),))\n        return ep.load()\n\n    def get_entry_map(self, group=None):\n        """"""Return the entry point map for `group`, or the full entry map""""""\n        try:\n            ep_map = self._ep_map\n        except AttributeError:\n            ep_map = self._ep_map = EntryPoint.parse_map(\n                self._get_metadata(\'entry_points.txt\'), self\n            )\n        if group is not None:\n            return ep_map.get(group,{})\n        return ep_map\n\n    def get_entry_info(self, group, name):\n        """"""Return the EntryPoint object for `group`+`name`, or ``None``""""""\n        return self.get_entry_map(group).get(name)\n\n    def insert_on(self, path, loc = None):\n        """"""Insert self.location in path before its nearest parent directory""""""\n\n        loc = loc or self.location\n        if not loc:\n            return\n\n        nloc = _normalize_cached(loc)\n        bdir = os.path.dirname(nloc)\n        npath= [(p and _normalize_cached(p) or p) for p in path]\n\n        for p, item in enumerate(npath):\n            if item==nloc:\n                break\n            elif item==bdir and self.precedence==EGG_DIST:\n                # if it\'s an .egg, give it precedence over its directory\n                if path is sys.path:\n                    self.check_version_conflict()\n                path.insert(p, loc)\n                npath.insert(p, nloc)\n                break\n        else:\n            if path is sys.path:\n                self.check_version_conflict()\n            path.append(loc)\n            return\n\n        # p is the spot where we found or inserted loc; now remove duplicates\n        while 1:\n            try:\n                np = npath.index(nloc, p+1)\n            except ValueError:\n                break\n            else:\n                del npath[np], path[np]\n                # ha!\n                p = np\n\n        return\n\n    def check_version_conflict(self):\n        if self.key==\'setuptools\':\n            # ignore the inevitable setuptools self-conflicts  :(\n            return\n\n        nsp = dict.fromkeys(self._get_metadata(\'namespace_packages.txt\'))\n        loc = normalize_path(self.location)\n        for modname in self._get_metadata(\'top_level.txt\'):\n            if (modname not in sys.modules or modname in nsp\n                    or modname in _namespace_packages):\n                continue\n            if modname in (\'pkg_resources\', \'setuptools\', \'site\'):\n                continue\n            fn = getattr(sys.modules[modname], \'__file__\', None)\n            if fn and (normalize_path(fn).startswith(loc) or\n                       fn.startswith(self.location)):\n                continue\n            issue_warning(\n                ""Module %s was already imported from %s, but %s is being added""\n                "" to sys.path"" % (modname, fn, self.location),\n            )\n\n    def has_version(self):\n        try:\n            self.version\n        except ValueError:\n            issue_warning(""Unbuilt egg for ""+repr(self))\n            return False\n        return True\n\n    def clone(self,**kw):\n        """"""Copy this distribution, substituting in any changed keyword args""""""\n        for attr in (\n            \'project_name\', \'version\', \'py_version\', \'platform\', \'location\',\n            \'precedence\'\n        ):\n            kw.setdefault(attr, getattr(self, attr, None))\n        kw.setdefault(\'metadata\', self._provider)\n        return self.__class__(**kw)\n\n    @property\n    def extras(self):\n        return [dep for dep in self._dep_map if dep]\n\n\nclass DistInfoDistribution(Distribution):\n    """"""Wrap an actual or potential sys.path entry w/metadata, .dist-info style""""""\n    PKG_INFO = \'METADATA\'\n    EQEQ = re.compile(r""([\\(,])\\s*(\\d.*?)\\s*([,\\)])"")\n\n    @property\n    def _parsed_pkg_info(self):\n        """"""Parse and cache metadata""""""\n        try:\n            return self._pkg_info\n        except AttributeError:\n            from email.parser import Parser\n            metadata = self.get_metadata(self.PKG_INFO)\n            self._pkg_info = Parser().parsestr(metadata)\n            return self._pkg_info\n\n    @property\n    def _dep_map(self):\n        try:\n            return self.__dep_map\n        except AttributeError:\n            self.__dep_map = self._compute_dependencies()\n            return self.__dep_map\n\n    def _preparse_requirement(self, requires_dist):\n        """"""Convert \'Foobar (1); baz\' to (\'Foobar ==1\', \'baz\')\n        Split environment marker, add == prefix to version specifiers as\n        necessary, and remove parenthesis.\n        """"""\n        parts = requires_dist.split(\';\', 1) + [\'\']\n        distvers = parts[0].strip()\n        mark = parts[1].strip()\n        distvers = re.sub(self.EQEQ, r""\\1==\\2\\3"", distvers)\n        distvers = distvers.replace(\'(\', \'\').replace(\')\', \'\')\n        return (distvers, mark)\n\n    def _compute_dependencies(self):\n        """"""Recompute this distribution\'s dependencies.""""""\n        from _markerlib import compile as compile_marker\n        dm = self.__dep_map = {None: []}\n\n        reqs = []\n        # Including any condition expressions\n        for req in self._parsed_pkg_info.get_all(\'Requires-Dist\') or []:\n            distvers, mark = self._preparse_requirement(req)\n            parsed = next(parse_requirements(distvers))\n            parsed.marker_fn = compile_marker(mark)\n            reqs.append(parsed)\n\n        def reqs_for_extra(extra):\n            for req in reqs:\n                if req.marker_fn(override={\'extra\':extra}):\n                    yield req\n\n        common = frozenset(reqs_for_extra(None))\n        dm[None].extend(common)\n\n        for extra in self._parsed_pkg_info.get_all(\'Provides-Extra\') or []:\n            extra = safe_extra(extra.strip())\n            dm[extra] = list(frozenset(reqs_for_extra(extra)) - common)\n\n        return dm\n\n\n_distributionImpl = {\n    \'.egg\': Distribution,\n    \'.egg-info\': Distribution,\n    \'.dist-info\': DistInfoDistribution,\n    }\n\n\ndef issue_warning(*args,**kw):\n    level = 1\n    g = globals()\n    try:\n        # find the first stack frame that is *not* code in\n        # the pkg_resources module, to use for the warning\n        while sys._getframe(level).f_globals is g:\n            level += 1\n    except ValueError:\n        pass\n    from warnings import warn\n    warn(stacklevel = level+1, *args, **kw)\n\n\ndef parse_requirements(strs):\n    """"""Yield ``Requirement`` objects for each specification in `strs`\n\n    `strs` must be an instance of ``basestring``, or a (possibly-nested)\n    iterable thereof.\n    """"""\n    # create a steppable iterator, so we can handle \\-continuations\n    lines = iter(yield_lines(strs))\n\n    def scan_list(ITEM, TERMINATOR, line, p, groups, item_name):\n\n        items = []\n\n        while not TERMINATOR(line, p):\n            if CONTINUE(line, p):\n                try:\n                    line = next(lines)\n                    p = 0\n                except StopIteration:\n                    raise ValueError(\n                        ""\\\\ must not appear on the last nonblank line""\n                    )\n\n            match = ITEM(line, p)\n            if not match:\n                msg = ""Expected "" + item_name + "" in""\n                raise ValueError(msg, line, ""at"", line[p:])\n\n            items.append(match.group(*groups))\n            p = match.end()\n\n            match = COMMA(line, p)\n            if match:\n                # skip the comma\n                p = match.end()\n            elif not TERMINATOR(line, p):\n                msg = ""Expected \',\' or end-of-list in""\n                raise ValueError(msg, line, ""at"", line[p:])\n\n        match = TERMINATOR(line, p)\n        if match: p = match.end()   # skip the terminator, if any\n        return line, p, items\n\n    for line in lines:\n        match = DISTRO(line)\n        if not match:\n            raise ValueError(""Missing distribution spec"", line)\n        project_name = match.group(1)\n        p = match.end()\n        extras = []\n\n        match = OBRACKET(line, p)\n        if match:\n            p = match.end()\n            line, p, extras = scan_list(\n                DISTRO, CBRACKET, line, p, (1,), ""\'extra\' name""\n            )\n\n        line, p, specs = scan_list(VERSION, LINE_END, line, p, (1, 2),\n            ""version spec"")\n        specs = [(op, safe_version(val)) for op, val in specs]\n        yield Requirement(project_name, specs, extras)\n\n\nclass Requirement:\n    def __init__(self, project_name, specs, extras):\n        """"""DO NOT CALL THIS UNDOCUMENTED METHOD; use Requirement.parse()!""""""\n        self.unsafe_name, project_name = project_name, safe_name(project_name)\n        self.project_name, self.key = project_name, project_name.lower()\n        index = [\n            (parse_version(v), state_machine[op], op, v)\n            for op, v in specs\n        ]\n        index.sort()\n        self.specs = [(op, ver) for parsed, trans, op, ver in index]\n        self.index, self.extras = index, tuple(map(safe_extra, extras))\n        self.hashCmp = (\n            self.key,\n            tuple((op, parsed) for parsed, trans, op, ver in index),\n            frozenset(self.extras),\n        )\n        self.__hash = hash(self.hashCmp)\n\n    def __str__(self):\n        specs = \',\'.join([\'\'.join(s) for s in self.specs])\n        extras = \',\'.join(self.extras)\n        if extras: extras = \'[%s]\' % extras\n        return \'%s%s%s\' % (self.project_name, extras, specs)\n\n    def __eq__(self, other):\n        return isinstance(other, Requirement) and self.hashCmp==other.hashCmp\n\n    def __contains__(self, item):\n        if isinstance(item, Distribution):\n            if item.key != self.key:\n                return False\n            # only get if we need it\n            if self.index:\n                item = item.parsed_version\n        elif isinstance(item, basestring):\n            item = parse_version(item)\n        last = None\n        # -1, 0, 1\n        compare = lambda a, b: (a > b) - (a < b)\n        for parsed, trans, op, ver in self.index:\n            # Indexing: 0, 1, -1\n            action = trans[compare(item, parsed)]\n            if action==\'F\':\n                return False\n            elif action==\'T\':\n                return True\n            elif action==\'+\':\n                last = True\n            elif action==\'-\' or last is None:\n                last = False\n        # no rules encountered\n        if last is None: last = True\n        return last\n\n    def __hash__(self):\n        return self.__hash\n\n    def __repr__(self): return ""Requirement.parse(%r)"" % str(self)\n\n    @staticmethod\n    def parse(s):\n        reqs = list(parse_requirements(s))\n        if reqs:\n            if len(reqs)==1:\n                return reqs[0]\n            raise ValueError(""Expected only one requirement"", s)\n        raise ValueError(""No requirements found"", s)\n\nstate_machine = {\n    #       =><\n    \'<\': \'--T\',\n    \'<=\': \'T-T\',\n    \'>\': \'F+F\',\n    \'>=\': \'T+F\',\n    \'==\': \'T..\',\n    \'!=\': \'F++\',\n}\n\n\ndef _get_mro(cls):\n    """"""Get an mro for a type or classic class""""""\n    if not isinstance(cls, type):\n        class cls(cls, object): pass\n        return cls.__mro__[1:]\n    return cls.__mro__\n\ndef _find_adapter(registry, ob):\n    """"""Return an adapter factory for `ob` from `registry`""""""\n    for t in _get_mro(getattr(ob, \'__class__\', type(ob))):\n        if t in registry:\n            return registry[t]\n\n\ndef ensure_directory(path):\n    """"""Ensure that the parent directory of `path` exists""""""\n    dirname = os.path.dirname(path)\n    if not os.path.isdir(dirname):\n        os.makedirs(dirname)\n\ndef split_sections(s):\n    """"""Split a string or iterable thereof into (section, content) pairs\n\n    Each ``section`` is a stripped version of the section header (""[section]"")\n    and each ``content`` is a list of stripped lines excluding blank lines and\n    comment-only lines.  If there are any such lines before the first section\n    header, they\'re returned in a first ``section`` of ``None``.\n    """"""\n    section = None\n    content = []\n    for line in yield_lines(s):\n        if line.startswith(""[""):\n            if line.endswith(""]""):\n                if section or content:\n                    yield section, content\n                section = line[1:-1].strip()\n                content = []\n            else:\n                raise ValueError(""Invalid section heading"", line)\n        else:\n            content.append(line)\n\n    # wrap up last segment\n    yield section, content\n\ndef _mkstemp(*args,**kw):\n    from tempfile import mkstemp\n    old_open = os.open\n    try:\n        # temporarily bypass sandboxing\n        os.open = os_open\n        return mkstemp(*args,**kw)\n    finally:\n        # and then put it back\n        os.open = old_open\n\n\n# Set up global resource manager (deliberately not state-saved)\n_manager = ResourceManager()\ndef _initialize(g):\n    for name in dir(_manager):\n        if not name.startswith(\'_\'):\n            g[name] = getattr(_manager, name)\n_initialize(globals())\n\n# Prepare the master working set and make the ``require()`` API available\nworking_set = WorkingSet._build_master()\n_declare_state(\'object\', working_set=working_set)\n\nrequire = working_set.require\niter_entry_points = working_set.iter_entry_points\nadd_activation_listener = working_set.subscribe\nrun_script = working_set.run_script\n# backward compatibility\nrun_main = run_script\n# Activate all distributions already on sys.path, and ensure that\n# all distributions added to the working set in the future (e.g. by\n# calling ``require()``) will get activated as well.\nadd_activation_listener(lambda dist: dist.activate())\nworking_set.entries=[]\nlist(map(working_set.add_entry, sys.path)) # match order\n'"
third_party/crow/amalgamate/merge_all.py,0,"b'""""""Merges all the header files.""""""\nfrom glob import glob\nfrom os import path as pt\nimport re\nfrom collections import defaultdict\nimport sys\n\nheader_path = ""../include""\nif len(sys.argv) > 1:\n    header_path = sys.argv[1]\n\nOUTPUT = \'crow_all.h\'\nre_depends = re.compile(\'^#include ""(.*)""\', re.MULTILINE)\nheaders = [x.rsplit(\'/\', 1)[-1] for x in glob(pt.join(header_path, \'*.h*\'))]\nheaders += [\'crow/\' + x.rsplit(\'/\', 1)[-1] for x in glob(pt.join(header_path, \'crow/*.h*\'))]\nprint(headers)\nedges = defaultdict(list)\nfor header in headers:\n    d = open(pt.join(header_path, header)).read()\n    match = re_depends.findall(d)\n    for m in match:\n        # m should included before header\n        edges[m].append(header)\n\nvisited = defaultdict(bool)\norder = []\n\n\ndef dfs(x):\n    """"""Ensure all header files are visited.""""""\n    visited[x] = True\n    for y in edges[x]:\n        if not visited[y]:\n            dfs(y)\n    order.append(x)\n\nfor header in headers:\n    if not visited[header]:\n        dfs(header)\n\norder = order[::-1]\nfor x in edges:\n    print(x, edges[x])\nfor x in edges:\n    for y in edges[x]:\n        assert order.index(x) < order.index(y), \'cyclic include detected\'\n\nprint(order)\nbuild = []\nfor header in order:\n    d = open(pt.join(header_path, header)).read()\n    build.append(re_depends.sub(lambda x: \'\\n\', d))\n    build.append(\'\\n\')\n\nopen(OUTPUT, \'w\').write(\'\\n\'.join(build))\n'"
third_party/crow/examples/example.py,0,"b'from flask import Flask\napp = Flask(__name__)\n\n@app.route(""/"")\ndef hello():\n    return ""Hello World!""\n\n@app.route(""/about/<path:path>/hello"")\ndef hello1(path):\n    return ""about1""\n\n@app.route(""/about"")\ndef hello2():\n    return ""about2""\n\nprint app.url_map\n\nif __name__ == ""__main__"":\n    app.run(host=""0.0.0.0"", port=8888)\n'"
third_party/crow/examples/example_test.py,0,"b'import urllib\nassert ""Hello World!"" ==  urllib.urlopen(\'http://localhost:18080\').read()\nassert ""About Crow example."" ==  urllib.urlopen(\'http://localhost:18080/about\').read()\nassert 404 == urllib.urlopen(\'http://localhost:18080/list\').getcode()\nassert ""3 bottles of beer!"" == urllib.urlopen(\'http://localhost:18080/hello/3\').read()\nassert ""100 bottles of beer!"" == urllib.urlopen(\'http://localhost:18080/hello/100\').read()\nassert 400 == urllib.urlopen(\'http://localhost:18080/hello/500\').getcode()\nassert ""3"" == urllib.urlopen(\'http://localhost:18080/add_json\', data=\'{""a"":1,""b"":2}\').read()\nassert ""3"" == urllib.urlopen(\'http://localhost:18080/add/1/2\').read()\n\n# test persistent connection\nimport socket\nimport time\ns = socket.socket()\ns.connect((\'localhost\', 18080))\nfor i in xrange(10):\n    s.send(\'\'\'GET / HTTP/1.1\nHost: localhost\\r\\n\\r\\n\'\'\');\n    assert \'Hello World!\' in s.recv(1024)\n\n# test large\ns = socket.socket()\ns.connect((\'localhost\', 18080))\ns.send(\'\'\'GET /large HTTP/1.1\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n\'\'\')\nr = \'\'\nwhile True:\n     d = s.recv(1024*1024)\n     if not d:\n         break;\n     r += d\n     print len(r), len(d)\nprint len(r), r[:100]\nassert len(r) > 512*1024\n\n# test timeout\ns = socket.socket()\ns.connect((\'localhost\', 18080))\n# invalid request, connection will be closed after timeout\ns.send(\'\'\'GET / HTTP/1.1\nhHhHHefhwjkefhklwejfklwejf\n\'\'\')\nprint s.recv(1024)\n\n'"
third_party/python/cpplint/cpplint.py,2,"b'#!/usr/bin/env python2.7\n#\n# Copyright (c) 2009 Google Inc. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#    * Redistributions of source code must retain the above copyright\n# notice, this list of conditions and the following disclaimer.\n#    * Redistributions in binary form must reproduce the above\n# copyright notice, this list of conditions and the following disclaimer\n# in the documentation and/or other materials provided with the\n# distribution.\n#    * Neither the name of Google Inc. nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n""""""Does google-lint on c++ files.\n\nThe goal of this script is to identify places in the code that *may*\nbe in non-compliance with google style.  It does not attempt to fix\nup these problems -- the point is to educate.  It does also not\nattempt to find all problems, or to ensure that everything it does\nfind is legitimately a problem.\n\nIn particular, we can get very confused by /* and // inside strings!\nWe do a small hack, which is to ignore //\'s with ""\'s after them on the\nsame line, but it is far from perfect (in either direction).\n""""""\n\nimport codecs\nimport copy\nimport getopt\nimport math  # for log\nimport os\nimport re\nimport sre_compile\nimport string\nimport sys\nimport unicodedata\n\n\n_USAGE = """"""\nSyntax: cpplint.py [--verbose=#] [--output=vs7] [--filter=-x,+y,...]\n                   [--counting=total|toplevel|detailed] [--root=subdir]\n                   [--linelength=digits]\n        <file> [file] ...\n\n  The style guidelines this tries to follow are those in\n    http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml\n\n  Every problem is given a confidence score from 1-5, with 5 meaning we are\n  certain of the problem, and 1 meaning it could be a legitimate construct.\n  This will miss some errors, and is not a substitute for a code review.\n\n  To suppress false-positive errors of a certain category, add a\n  \'NOLINT(category)\' comment to the line.  NOLINT or NOLINT(*)\n  suppresses errors of all categories on that line.\n\n  The files passed in will be linted; at least one file must be provided.\n  Default linted extensions are .cc, .cpp, .cu, .cuh and .h.  Change the\n  extensions with the --extensions flag.\n\n  Flags:\n\n    output=vs7\n      By default, the output is formatted to ease emacs parsing.  Visual Studio\n      compatible output (vs7) may also be used.  Other formats are unsupported.\n\n    verbose=#\n      Specify a number 0-5 to restrict errors to certain verbosity levels.\n\n    filter=-x,+y,...\n      Specify a comma-separated list of category-filters to apply: only\n      error messages whose category names pass the filters will be printed.\n      (Category names are printed with the message and look like\n      ""[whitespace/indent]"".)  Filters are evaluated left to right.\n      ""-FOO"" and ""FOO"" means ""do not print categories that start with FOO"".\n      ""+FOO"" means ""do print categories that start with FOO"".\n\n      Examples: --filter=-whitespace,+whitespace/braces\n                --filter=whitespace,runtime/printf,+runtime/printf_format\n                --filter=-,+build/include_what_you_use\n\n      To see a list of all the categories used in cpplint, pass no arg:\n         --filter=\n\n    counting=total|toplevel|detailed\n      The total number of errors found is always printed. If\n      \'toplevel\' is provided, then the count of errors in each of\n      the top-level categories like \'build\' and \'whitespace\' will\n      also be printed. If \'detailed\' is provided, then a count\n      is provided for each category like \'build/class\'.\n\n    root=subdir\n      The root directory used for deriving header guard CPP variable.\n      By default, the header guard CPP variable is calculated as the relative\n      path to the directory that contains .git, .hg, or .svn.  When this flag\n      is specified, the relative path is calculated from the specified\n      directory. If the specified directory does not exist, this flag is\n      ignored.\n\n      Examples:\n        Assuming that src/.git exists, the header guard CPP variables for\n        src/chrome/browser/ui/browser.h are:\n\n        No flag => CHROME_BROWSER_UI_BROWSER_H_\n        --root=chrome => BROWSER_UI_BROWSER_H_\n        --root=chrome/browser => UI_BROWSER_H_\n\n    linelength=digits\n      This is the allowed line length for the project. The default value is\n      80 characters.\n\n      Examples:\n        --linelength=120\n\n    extensions=extension,extension,...\n      The allowed file extensions that cpplint will check\n\n      Examples:\n        --extensions=hpp,cpp\n\n    cpplint.py supports per-directory configurations specified in CPPLINT.cfg\n    files. CPPLINT.cfg file can contain a number of key=value pairs.\n    Currently the following options are supported:\n\n      set noparent\n      filter=+filter1,-filter2,...\n      exclude_files=regex\n      linelength=80\n\n    ""set noparent"" option prevents cpplint from traversing directory tree\n    upwards looking for more .cfg files in parent directories. This option\n    is usually placed in the top-level project directory.\n\n    The ""filter"" option is similar in function to --filter flag. It specifies\n    message filters in addition to the |_DEFAULT_FILTERS| and those specified\n    through --filter command-line flag.\n\n    ""exclude_files"" allows to specify a regular expression to be matched against\n    a file name. If the expression matches, the file is skipped and not run\n    through liner.\n\n    ""linelength"" allows to specify the allowed line length for the project.\n\n    CPPLINT.cfg has an effect on files in the same directory and all\n    sub-directories, unless overridden by a nested configuration file.\n\n      Example file:\n        filter=-build/include_order,+build/include_alpha\n        exclude_files=.*\\.cc\n\n    The above example disables build/include_order warning and enables\n    build/include_alpha as well as excludes all .cc from being\n    processed by linter, in the current directory (where the .cfg\n    file is located) and all sub-directories.\n""""""\n\n# We categorize each error message we print.  Here are the categories.\n# We want an explicit list so we can list them all in cpplint --filter=.\n# If you add a new error message with a new category, add it to the list\n# here!  cpplint_unittest.py should tell you if you forget to do this.\n_ERROR_CATEGORIES = [\n    \'build/class\',\n    \'build/c++11\',\n    \'build/deprecated\',\n    \'build/endif_comment\',\n    \'build/explicit_make_pair\',\n    \'build/forward_decl\',\n    \'build/header_guard\',\n    \'build/include\',\n    \'build/include_alpha\',\n    \'build/include_order\',\n    \'build/include_what_you_use\',\n    \'build/namespaces\',\n    \'build/printf_format\',\n    \'build/storage_class\',\n    \'legal/copyright\',\n    \'readability/alt_tokens\',\n    \'readability/braces\',\n    \'readability/casting\',\n    \'readability/check\',\n    \'readability/constructors\',\n    \'readability/fn_size\',\n    \'readability/function\',\n    \'readability/inheritance\',\n    \'readability/multiline_comment\',\n    \'readability/multiline_string\',\n    \'readability/namespace\',\n    \'readability/nolint\',\n    \'readability/nul\',\n    \'readability/strings\',\n    \'readability/todo\',\n    \'readability/utf8\',\n    \'runtime/arrays\',\n    \'runtime/casting\',\n    \'runtime/explicit\',\n    \'runtime/int\',\n    \'runtime/init\',\n    \'runtime/invalid_increment\',\n    \'runtime/member_string_references\',\n    \'runtime/memset\',\n    \'runtime/indentation_namespace\',\n    \'runtime/operator\',\n    \'runtime/printf\',\n    \'runtime/printf_format\',\n    \'runtime/references\',\n    \'runtime/string\',\n    \'runtime/threadsafe_fn\',\n    \'runtime/vlog\',\n    \'whitespace/blank_line\',\n    \'whitespace/braces\',\n    \'whitespace/comma\',\n    \'whitespace/comments\',\n    \'whitespace/empty_conditional_body\',\n    \'whitespace/empty_loop_body\',\n    \'whitespace/end_of_line\',\n    \'whitespace/ending_newline\',\n    \'whitespace/forcolon\',\n    \'whitespace/indent\',\n    \'whitespace/line_length\',\n    \'whitespace/newline\',\n    \'whitespace/operators\',\n    \'whitespace/parens\',\n    \'whitespace/semicolon\',\n    \'whitespace/tab\',\n    \'whitespace/todo\',\n    ]\n\n# These error categories are no longer enforced by cpplint, but for backwards-\n# compatibility they may still appear in NOLINT comments.\n_LEGACY_ERROR_CATEGORIES = [\n    \'readability/streams\',\n    ]\n\n# The default state of the category filter. This is overridden by the --filter=\n# flag. By default all errors are on, so only add here categories that should be\n# off by default (i.e., categories that must be enabled by the --filter= flags).\n# All entries here should start with a \'-\' or \'+\', as in the --filter= flag.\n_DEFAULT_FILTERS = [\'-build/include_alpha\']\n\n# We used to check for high-bit characters, but after much discussion we\n# decided those were OK, as long as they were in UTF-8 and didn\'t represent\n# hard-coded international strings, which belong in a separate i18n file.\n\n# C++ headers\n_CPP_HEADERS = frozenset([\n    # Legacy\n    \'algobase.h\',\n    \'algo.h\',\n    \'alloc.h\',\n    \'builtinbuf.h\',\n    \'bvector.h\',\n    \'complex.h\',\n    \'defalloc.h\',\n    \'deque.h\',\n    \'editbuf.h\',\n    \'fstream.h\',\n    \'function.h\',\n    \'hash_map\',\n    \'hash_map.h\',\n    \'hash_set\',\n    \'hash_set.h\',\n    \'hashtable.h\',\n    \'heap.h\',\n    \'indstream.h\',\n    \'iomanip.h\',\n    \'iostream.h\',\n    \'istream.h\',\n    \'iterator.h\',\n    \'list.h\',\n    \'map.h\',\n    \'multimap.h\',\n    \'multiset.h\',\n    \'ostream.h\',\n    \'pair.h\',\n    \'parsestream.h\',\n    \'pfstream.h\',\n    \'procbuf.h\',\n    \'pthread_alloc\',\n    \'pthread_alloc.h\',\n    \'rope\',\n    \'rope.h\',\n    \'ropeimpl.h\',\n    \'set.h\',\n    \'slist\',\n    \'slist.h\',\n    \'stack.h\',\n    \'stdiostream.h\',\n    \'stl_alloc.h\',\n    \'stl_relops.h\',\n    \'streambuf.h\',\n    \'stream.h\',\n    \'strfile.h\',\n    \'strstream.h\',\n    \'tempbuf.h\',\n    \'tree.h\',\n    \'type_traits.h\',\n    \'vector.h\',\n    # 17.6.1.2 C++ library headers\n    \'algorithm\',\n    \'array\',\n    \'atomic\',\n    \'bitset\',\n    \'chrono\',\n    \'codecvt\',\n    \'complex\',\n    \'condition_variable\',\n    \'deque\',\n    \'exception\',\n    \'forward_list\',\n    \'fstream\',\n    \'functional\',\n    \'future\',\n    \'initializer_list\',\n    \'iomanip\',\n    \'ios\',\n    \'iosfwd\',\n    \'iostream\',\n    \'istream\',\n    \'iterator\',\n    \'limits\',\n    \'list\',\n    \'locale\',\n    \'map\',\n    \'memory\',\n    \'mutex\',\n    \'new\',\n    \'numeric\',\n    \'ostream\',\n    \'queue\',\n    \'random\',\n    \'ratio\',\n    \'regex\',\n    \'set\',\n    \'sstream\',\n    \'stack\',\n    \'stdexcept\',\n    \'streambuf\',\n    \'string\',\n    \'strstream\',\n    \'system_error\',\n    \'thread\',\n    \'tuple\',\n    \'typeindex\',\n    \'typeinfo\',\n    \'type_traits\',\n    \'unordered_map\',\n    \'unordered_set\',\n    \'utility\',\n    \'valarray\',\n    \'vector\',\n    # 17.6.1.2 C++ headers for C library facilities\n    \'cassert\',\n    \'ccomplex\',\n    \'cctype\',\n    \'cerrno\',\n    \'cfenv\',\n    \'cfloat\',\n    \'cinttypes\',\n    \'ciso646\',\n    \'climits\',\n    \'clocale\',\n    \'cmath\',\n    \'csetjmp\',\n    \'csignal\',\n    \'cstdalign\',\n    \'cstdarg\',\n    \'cstdbool\',\n    \'cstddef\',\n    \'cstdint\',\n    \'cstdio\',\n    \'cstdlib\',\n    \'cstring\',\n    \'ctgmath\',\n    \'ctime\',\n    \'cuchar\',\n    \'cwchar\',\n    \'cwctype\',\n    ])\n\n\n# These headers are excluded from [build/include] and [build/include_order]\n# checks:\n# - Anything not following google file name conventions (containing an\n#   uppercase character, such as Python.h or nsStringAPI.h, for example).\n# - Lua headers.\n_THIRD_PARTY_HEADERS_PATTERN = re.compile(\n    r\'^(?:[^/]*[A-Z][^/]*\\.h|lua\\.h|lauxlib\\.h|lualib\\.h)$\')\n\n\n# Assertion macros.  These are defined in base/logging.h and\n# testing/base/gunit.h.  Note that the _M versions need to come first\n# for substring matching to work.\n_CHECK_MACROS = [\n    \'DCHECK\', \'CHECK\',\n    \'EXPECT_TRUE_M\', \'EXPECT_TRUE\',\n    \'ASSERT_TRUE_M\', \'ASSERT_TRUE\',\n    \'EXPECT_FALSE_M\', \'EXPECT_FALSE\',\n    \'ASSERT_FALSE_M\', \'ASSERT_FALSE\',\n    ]\n\n# Replacement macros for CHECK/DCHECK/EXPECT_TRUE/EXPECT_FALSE\n_CHECK_REPLACEMENT = dict([(m, {}) for m in _CHECK_MACROS])\n\nfor op, replacement in [(\'==\', \'EQ\'), (\'!=\', \'NE\'),\n                        (\'>=\', \'GE\'), (\'>\', \'GT\'),\n                        (\'<=\', \'LE\'), (\'<\', \'LT\')]:\n  _CHECK_REPLACEMENT[\'DCHECK\'][op] = \'DCHECK_%s\' % replacement\n  _CHECK_REPLACEMENT[\'CHECK\'][op] = \'CHECK_%s\' % replacement\n  _CHECK_REPLACEMENT[\'EXPECT_TRUE\'][op] = \'EXPECT_%s\' % replacement\n  _CHECK_REPLACEMENT[\'ASSERT_TRUE\'][op] = \'ASSERT_%s\' % replacement\n  _CHECK_REPLACEMENT[\'EXPECT_TRUE_M\'][op] = \'EXPECT_%s_M\' % replacement\n  _CHECK_REPLACEMENT[\'ASSERT_TRUE_M\'][op] = \'ASSERT_%s_M\' % replacement\n\nfor op, inv_replacement in [(\'==\', \'NE\'), (\'!=\', \'EQ\'),\n                            (\'>=\', \'LT\'), (\'>\', \'LE\'),\n                            (\'<=\', \'GT\'), (\'<\', \'GE\')]:\n  _CHECK_REPLACEMENT[\'EXPECT_FALSE\'][op] = \'EXPECT_%s\' % inv_replacement\n  _CHECK_REPLACEMENT[\'ASSERT_FALSE\'][op] = \'ASSERT_%s\' % inv_replacement\n  _CHECK_REPLACEMENT[\'EXPECT_FALSE_M\'][op] = \'EXPECT_%s_M\' % inv_replacement\n  _CHECK_REPLACEMENT[\'ASSERT_FALSE_M\'][op] = \'ASSERT_%s_M\' % inv_replacement\n\n# Alternative tokens and their replacements.  For full list, see section 2.5\n# Alternative tokens [lex.digraph] in the C++ standard.\n#\n# Digraphs (such as \'%:\') are not included here since it\'s a mess to\n# match those on a word boundary.\n_ALT_TOKEN_REPLACEMENT = {\n    \'and\': \'&&\',\n    \'bitor\': \'|\',\n    \'or\': \'||\',\n    \'xor\': \'^\',\n    \'compl\': \'~\',\n    \'bitand\': \'&\',\n    \'and_eq\': \'&=\',\n    \'or_eq\': \'|=\',\n    \'xor_eq\': \'^=\',\n    \'not\': \'!\',\n    \'not_eq\': \'!=\'\n    }\n\n# Compile regular expression that matches all the above keywords.  The ""[ =()]""\n# bit is meant to avoid matching these keywords outside of boolean expressions.\n#\n# False positives include C-style multi-line comments and multi-line strings\n# but those have always been troublesome for cpplint.\n_ALT_TOKEN_REPLACEMENT_PATTERN = re.compile(\n    r\'[ =()](\' + (\'|\'.join(_ALT_TOKEN_REPLACEMENT.keys())) + r\')(?=[ (]|$)\')\n\n\n# These constants define types of headers for use with\n# _IncludeState.CheckNextIncludeOrder().\n_C_SYS_HEADER = 1\n_CPP_SYS_HEADER = 2\n_LIKELY_MY_HEADER = 3\n_POSSIBLE_MY_HEADER = 4\n_OTHER_HEADER = 5\n\n# These constants define the current inline assembly state\n_NO_ASM = 0       # Outside of inline assembly block\n_INSIDE_ASM = 1   # Inside inline assembly block\n_END_ASM = 2      # Last line of inline assembly block\n_BLOCK_ASM = 3    # The whole block is an inline assembly block\n\n# Match start of assembly blocks\n_MATCH_ASM = re.compile(r\'^\\s*(?:asm|_asm|__asm|__asm__)\'\n                        r\'(?:\\s+(volatile|__volatile__))?\'\n                        r\'\\s*[{(]\')\n\n\n_regexp_compile_cache = {}\n\n# {str, set(int)}: a map from error categories to sets of linenumbers\n# on which those errors are expected and should be suppressed.\n_error_suppressions = {}\n\n# The root directory used for deriving header guard CPP variable.\n# This is set by --root flag.\n_root = None\n\n# The allowed line length of files.\n# This is set by --linelength flag.\n_line_length = 80\n\n# The allowed extensions for file names\n# This is set by --extensions flag.\n_valid_extensions = set([\'cc\', \'h\', \'cpp\', \'cu\', \'cuh\'])\n\ndef ParseNolintSuppressions(filename, raw_line, linenum, error):\n  """"""Updates the global list of error-suppressions.\n\n  Parses any NOLINT comments on the current line, updating the global\n  error_suppressions store.  Reports an error if the NOLINT comment\n  was malformed.\n\n  Args:\n    filename: str, the name of the input file.\n    raw_line: str, the line of input text, with comments.\n    linenum: int, the number of the current line.\n    error: function, an error handler.\n  """"""\n  matched = Search(r\'\\bNOLINT(NEXTLINE)?\\b(\\([^)]+\\))?\', raw_line)\n  if matched:\n    if matched.group(1):\n      suppressed_line = linenum + 1\n    else:\n      suppressed_line = linenum\n    category = matched.group(2)\n    if category in (None, \'(*)\'):  # => ""suppress all""\n      _error_suppressions.setdefault(None, set()).add(suppressed_line)\n    else:\n      if category.startswith(\'(\') and category.endswith(\')\'):\n        category = category[1:-1]\n        if category in _ERROR_CATEGORIES:\n          _error_suppressions.setdefault(category, set()).add(suppressed_line)\n        elif category not in _LEGACY_ERROR_CATEGORIES:\n          error(filename, linenum, \'readability/nolint\', 5,\n                \'Unknown NOLINT error category: %s\' % category)\n\n\ndef ResetNolintSuppressions():\n  """"""Resets the set of NOLINT suppressions to empty.""""""\n  _error_suppressions.clear()\n\n\ndef IsErrorSuppressedByNolint(category, linenum):\n  """"""Returns true if the specified error category is suppressed on this line.\n\n  Consults the global error_suppressions map populated by\n  ParseNolintSuppressions/ResetNolintSuppressions.\n\n  Args:\n    category: str, the category of the error.\n    linenum: int, the current line number.\n  Returns:\n    bool, True iff the error should be suppressed due to a NOLINT comment.\n  """"""\n  return (linenum in _error_suppressions.get(category, set()) or\n          linenum in _error_suppressions.get(None, set()))\n\n\ndef Match(pattern, s):\n  """"""Matches the string with the pattern, caching the compiled regexp.""""""\n  # The regexp compilation caching is inlined in both Match and Search for\n  # performance reasons; factoring it out into a separate function turns out\n  # to be noticeably expensive.\n  if pattern not in _regexp_compile_cache:\n    _regexp_compile_cache[pattern] = sre_compile.compile(pattern)\n  return _regexp_compile_cache[pattern].match(s)\n\n\ndef ReplaceAll(pattern, rep, s):\n  """"""Replaces instances of pattern in a string with a replacement.\n\n  The compiled regex is kept in a cache shared by Match and Search.\n\n  Args:\n    pattern: regex pattern\n    rep: replacement text\n    s: search string\n\n  Returns:\n    string with replacements made (or original string if no replacements)\n  """"""\n  if pattern not in _regexp_compile_cache:\n    _regexp_compile_cache[pattern] = sre_compile.compile(pattern)\n  return _regexp_compile_cache[pattern].sub(rep, s)\n\n\ndef Search(pattern, s):\n  """"""Searches the string for the pattern, caching the compiled regexp.""""""\n  if pattern not in _regexp_compile_cache:\n    _regexp_compile_cache[pattern] = sre_compile.compile(pattern)\n  return _regexp_compile_cache[pattern].search(s)\n\n\nclass _IncludeState(object):\n  """"""Tracks line numbers for includes, and the order in which includes appear.\n\n  include_list contains list of lists of (header, line number) pairs.\n  It\'s a lists of lists rather than just one flat list to make it\n  easier to update across preprocessor boundaries.\n\n  Call CheckNextIncludeOrder() once for each header in the file, passing\n  in the type constants defined above. Calls in an illegal order will\n  raise an _IncludeError with an appropriate error message.\n\n  """"""\n  # self._section will move monotonically through this set. If it ever\n  # needs to move backwards, CheckNextIncludeOrder will raise an error.\n  _INITIAL_SECTION = 0\n  _MY_H_SECTION = 1\n  _C_SECTION = 2\n  _CPP_SECTION = 3\n  _OTHER_H_SECTION = 4\n\n  _TYPE_NAMES = {\n      _C_SYS_HEADER: \'C system header\',\n      _CPP_SYS_HEADER: \'C++ system header\',\n      _LIKELY_MY_HEADER: \'header this file implements\',\n      _POSSIBLE_MY_HEADER: \'header this file may implement\',\n      _OTHER_HEADER: \'other header\',\n      }\n  _SECTION_NAMES = {\n      _INITIAL_SECTION: ""... nothing. (This can\'t be an error.)"",\n      _MY_H_SECTION: \'a header this file implements\',\n      _C_SECTION: \'C system header\',\n      _CPP_SECTION: \'C++ system header\',\n      _OTHER_H_SECTION: \'other header\',\n      }\n\n  def __init__(self):\n    self.include_list = [[]]\n    self.ResetSection(\'\')\n\n  def FindHeader(self, header):\n    """"""Check if a header has already been included.\n\n    Args:\n      header: header to check.\n    Returns:\n      Line number of previous occurrence, or -1 if the header has not\n      been seen before.\n    """"""\n    for section_list in self.include_list:\n      for f in section_list:\n        if f[0] == header:\n          return f[1]\n    return -1\n\n  def ResetSection(self, directive):\n    """"""Reset section checking for preprocessor directive.\n\n    Args:\n      directive: preprocessor directive (e.g. ""if"", ""else"").\n    """"""\n    # The name of the current section.\n    self._section = self._INITIAL_SECTION\n    # The path of last found header.\n    self._last_header = \'\'\n\n    # Update list of includes.  Note that we never pop from the\n    # include list.\n    if directive in (\'if\', \'ifdef\', \'ifndef\'):\n      self.include_list.append([])\n    elif directive in (\'else\', \'elif\'):\n      self.include_list[-1] = []\n\n  def SetLastHeader(self, header_path):\n    self._last_header = header_path\n\n  def CanonicalizeAlphabeticalOrder(self, header_path):\n    """"""Returns a path canonicalized for alphabetical comparison.\n\n    - replaces ""-"" with ""_"" so they both cmp the same.\n    - removes \'-inl\' since we don\'t require them to be after the main header.\n    - lowercase everything, just in case.\n\n    Args:\n      header_path: Path to be canonicalized.\n\n    Returns:\n      Canonicalized path.\n    """"""\n    return header_path.replace(\'-inl.h\', \'.h\').replace(\'-\', \'_\').lower()\n\n  def IsInAlphabeticalOrder(self, clean_lines, linenum, header_path):\n    """"""Check if a header is in alphabetical order with the previous header.\n\n    Args:\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      header_path: Canonicalized header to be checked.\n\n    Returns:\n      Returns true if the header is in alphabetical order.\n    """"""\n    # If previous section is different from current section, _last_header will\n    # be reset to empty string, so it\'s always less than current header.\n    #\n    # If previous line was a blank line, assume that the headers are\n    # intentionally sorted the way they are.\n    if (self._last_header > header_path and\n        Match(r\'^\\s*#\\s*include\\b\', clean_lines.elided[linenum - 1])):\n      return False\n    return True\n\n  def CheckNextIncludeOrder(self, header_type):\n    """"""Returns a non-empty error message if the next header is out of order.\n\n    This function also updates the internal state to be ready to check\n    the next include.\n\n    Args:\n      header_type: One of the _XXX_HEADER constants defined above.\n\n    Returns:\n      The empty string if the header is in the right order, or an\n      error message describing what\'s wrong.\n\n    """"""\n    error_message = (\'Found %s after %s\' %\n                     (self._TYPE_NAMES[header_type],\n                      self._SECTION_NAMES[self._section]))\n\n    last_section = self._section\n\n    if header_type == _C_SYS_HEADER:\n      if self._section <= self._C_SECTION:\n        self._section = self._C_SECTION\n      else:\n        self._last_header = \'\'\n        return error_message\n    elif header_type == _CPP_SYS_HEADER:\n      if self._section <= self._CPP_SECTION:\n        self._section = self._CPP_SECTION\n      else:\n        self._last_header = \'\'\n        return error_message\n    elif header_type == _LIKELY_MY_HEADER:\n      if self._section <= self._MY_H_SECTION:\n        self._section = self._MY_H_SECTION\n      else:\n        self._section = self._OTHER_H_SECTION\n    elif header_type == _POSSIBLE_MY_HEADER:\n      if self._section <= self._MY_H_SECTION:\n        self._section = self._MY_H_SECTION\n      else:\n        # This will always be the fallback because we\'re not sure\n        # enough that the header is associated with this file.\n        self._section = self._OTHER_H_SECTION\n    else:\n      assert header_type == _OTHER_HEADER\n      self._section = self._OTHER_H_SECTION\n\n    if last_section != self._section:\n      self._last_header = \'\'\n\n    return \'\'\n\n\nclass _CppLintState(object):\n  """"""Maintains module-wide state..""""""\n\n  def __init__(self):\n    self.verbose_level = 1  # global setting.\n    self.error_count = 0    # global count of reported errors\n    # filters to apply when emitting error messages\n    self.filters = _DEFAULT_FILTERS[:]\n    # backup of filter list. Used to restore the state after each file.\n    self._filters_backup = self.filters[:]\n    self.counting = \'total\'  # In what way are we counting errors?\n    self.errors_by_category = {}  # string to int dict storing error counts\n\n    # output format:\n    # ""emacs"" - format that emacs can parse (default)\n    # ""vs7"" - format that Microsoft Visual Studio 7 can parse\n    self.output_format = \'emacs\'\n\n  def SetOutputFormat(self, output_format):\n    """"""Sets the output format for errors.""""""\n    self.output_format = output_format\n\n  def SetVerboseLevel(self, level):\n    """"""Sets the module\'s verbosity, and returns the previous setting.""""""\n    last_verbose_level = self.verbose_level\n    self.verbose_level = level\n    return last_verbose_level\n\n  def SetCountingStyle(self, counting_style):\n    """"""Sets the module\'s counting options.""""""\n    self.counting = counting_style\n\n  def SetFilters(self, filters):\n    """"""Sets the error-message filters.\n\n    These filters are applied when deciding whether to emit a given\n    error message.\n\n    Args:\n      filters: A string of comma-separated filters (eg ""+whitespace/indent"").\n               Each filter should start with + or -; else we die.\n\n    Raises:\n      ValueError: The comma-separated filters did not all start with \'+\' or \'-\'.\n                  E.g. ""-,+whitespace,-whitespace/indent,whitespace/badfilter""\n    """"""\n    # Default filters always have less priority than the flag ones.\n    self.filters = _DEFAULT_FILTERS[:]\n    self.AddFilters(filters)\n\n  def AddFilters(self, filters):\n    """""" Adds more filters to the existing list of error-message filters. """"""\n    for filt in filters.split(\',\'):\n      clean_filt = filt.strip()\n      if clean_filt:\n        self.filters.append(clean_filt)\n    for filt in self.filters:\n      if not (filt.startswith(\'+\') or filt.startswith(\'-\')):\n        raise ValueError(\'Every filter in --filters must start with + or -\'\n                         \' (%s does not)\' % filt)\n\n  def BackupFilters(self):\n    """""" Saves the current filter list to backup storage.""""""\n    self._filters_backup = self.filters[:]\n\n  def RestoreFilters(self):\n    """""" Restores filters previously backed up.""""""\n    self.filters = self._filters_backup[:]\n\n  def ResetErrorCounts(self):\n    """"""Sets the module\'s error statistic back to zero.""""""\n    self.error_count = 0\n    self.errors_by_category = {}\n\n  def IncrementErrorCount(self, category):\n    """"""Bumps the module\'s error statistic.""""""\n    self.error_count += 1\n    if self.counting in (\'toplevel\', \'detailed\'):\n      if self.counting != \'detailed\':\n        category = category.split(\'/\')[0]\n      if category not in self.errors_by_category:\n        self.errors_by_category[category] = 0\n      self.errors_by_category[category] += 1\n\n  def PrintErrorCounts(self):\n    """"""Print a summary of errors by category, and the total.""""""\n    for category, count in self.errors_by_category.iteritems():\n      sys.stderr.write(\'Category \\\'%s\\\' errors found: %d\\n\' %\n                       (category, count))\n    if self.error_count > 0:\n      sys.stderr.write(\'Total errors found: %d\\n\' % self.error_count)\n\n_cpplint_state = _CppLintState()\n\n\ndef _OutputFormat():\n  """"""Gets the module\'s output format.""""""\n  return _cpplint_state.output_format\n\n\ndef _SetOutputFormat(output_format):\n  """"""Sets the module\'s output format.""""""\n  _cpplint_state.SetOutputFormat(output_format)\n\n\ndef _VerboseLevel():\n  """"""Returns the module\'s verbosity setting.""""""\n  return _cpplint_state.verbose_level\n\n\ndef _SetVerboseLevel(level):\n  """"""Sets the module\'s verbosity, and returns the previous setting.""""""\n  return _cpplint_state.SetVerboseLevel(level)\n\n\ndef _SetCountingStyle(level):\n  """"""Sets the module\'s counting options.""""""\n  _cpplint_state.SetCountingStyle(level)\n\n\ndef _Filters():\n  """"""Returns the module\'s list of output filters, as a list.""""""\n  return _cpplint_state.filters\n\n\ndef _SetFilters(filters):\n  """"""Sets the module\'s error-message filters.\n\n  These filters are applied when deciding whether to emit a given\n  error message.\n\n  Args:\n    filters: A string of comma-separated filters (eg ""whitespace/indent"").\n             Each filter should start with + or -; else we die.\n  """"""\n  _cpplint_state.SetFilters(filters)\n\ndef _AddFilters(filters):\n  """"""Adds more filter overrides.\n\n  Unlike _SetFilters, this function does not reset the current list of filters\n  available.\n\n  Args:\n    filters: A string of comma-separated filters (eg ""whitespace/indent"").\n             Each filter should start with + or -; else we die.\n  """"""\n  _cpplint_state.AddFilters(filters)\n\ndef _BackupFilters():\n  """""" Saves the current filter list to backup storage.""""""\n  _cpplint_state.BackupFilters()\n\ndef _RestoreFilters():\n  """""" Restores filters previously backed up.""""""\n  _cpplint_state.RestoreFilters()\n\nclass _FunctionState(object):\n  """"""Tracks current function name and the number of lines in its body.""""""\n\n  _NORMAL_TRIGGER = 250  # for --v=0, 500 for --v=1, etc.\n  _TEST_TRIGGER = 400    # about 50% more than _NORMAL_TRIGGER.\n\n  def __init__(self):\n    self.in_a_function = False\n    self.lines_in_function = 0\n    self.current_function = \'\'\n\n  def Begin(self, function_name):\n    """"""Start analyzing function body.\n\n    Args:\n      function_name: The name of the function being tracked.\n    """"""\n    self.in_a_function = True\n    self.lines_in_function = 0\n    self.current_function = function_name\n\n  def Count(self):\n    """"""Count line in current function body.""""""\n    if self.in_a_function:\n      self.lines_in_function += 1\n\n  def Check(self, error, filename, linenum):\n    """"""Report if too many lines in function body.\n\n    Args:\n      error: The function to call with any errors found.\n      filename: The name of the current file.\n      linenum: The number of the line to check.\n    """"""\n    if Match(r\'T(EST|est)\', self.current_function):\n      base_trigger = self._TEST_TRIGGER\n    else:\n      base_trigger = self._NORMAL_TRIGGER\n    trigger = base_trigger * 2**_VerboseLevel()\n\n    if self.lines_in_function > trigger:\n      error_level = int(math.log(self.lines_in_function / base_trigger, 2))\n      # 50 => 0, 100 => 1, 200 => 2, 400 => 3, 800 => 4, 1600 => 5, ...\n      if error_level > 5:\n        error_level = 5\n      error(filename, linenum, \'readability/fn_size\', error_level,\n            \'Small and focused functions are preferred:\'\n            \' %s has %d non-comment lines\'\n            \' (error triggered by exceeding %d lines).\'  % (\n                self.current_function, self.lines_in_function, trigger))\n\n  def End(self):\n    """"""Stop analyzing function body.""""""\n    self.in_a_function = False\n\n\nclass _IncludeError(Exception):\n  """"""Indicates a problem with the include order in a file.""""""\n  pass\n\n\nclass FileInfo(object):\n  """"""Provides utility functions for filenames.\n\n  FileInfo provides easy access to the components of a file\'s path\n  relative to the project root.\n  """"""\n\n  def __init__(self, filename):\n    self._filename = filename\n\n  def FullName(self):\n    """"""Make Windows paths like Unix.""""""\n    return os.path.abspath(self._filename).replace(\'\\\\\', \'/\')\n\n  def RepositoryName(self):\n    """"""FullName after removing the local path to the repository.\n\n    If we have a real absolute path name here we can try to do something smart:\n    detecting the root of the checkout and truncating /path/to/checkout from\n    the name so that we get header guards that don\'t include things like\n    ""C:\\Documents and Settings\\..."" or ""/home/username/..."" in them and thus\n    people on different computers who have checked the source out to different\n    locations won\'t see bogus errors.\n    """"""\n    fullname = self.FullName()\n\n    if os.path.exists(fullname):\n      project_dir = os.path.dirname(fullname)\n\n      if os.path.exists(os.path.join(project_dir, "".svn"")):\n        # If there\'s a .svn file in the current directory, we recursively look\n        # up the directory tree for the top of the SVN checkout\n        root_dir = project_dir\n        one_up_dir = os.path.dirname(root_dir)\n        while os.path.exists(os.path.join(one_up_dir, "".svn"")):\n          root_dir = os.path.dirname(root_dir)\n          one_up_dir = os.path.dirname(one_up_dir)\n\n        prefix = os.path.commonprefix([root_dir, project_dir])\n        return fullname[len(prefix) + 1:]\n\n      # Not SVN <= 1.6? Try to find a git, hg, or svn top level directory by\n      # searching up from the current path.\n      root_dir = os.path.dirname(fullname)\n      while (root_dir != os.path.dirname(root_dir) and\n             not os.path.exists(os.path.join(root_dir, "".git"")) and\n             not os.path.exists(os.path.join(root_dir, "".hg"")) and\n             not os.path.exists(os.path.join(root_dir, "".svn""))):\n        root_dir = os.path.dirname(root_dir)\n\n      if (os.path.exists(os.path.join(root_dir, "".git"")) or\n          os.path.exists(os.path.join(root_dir, "".hg"")) or\n          os.path.exists(os.path.join(root_dir, "".svn""))):\n        prefix = os.path.commonprefix([root_dir, project_dir])\n        return fullname[len(prefix) + 1:]\n\n    # Don\'t know what to do; header guard warnings may be wrong...\n    return fullname\n\n  def Split(self):\n    """"""Splits the file into the directory, basename, and extension.\n\n    For \'chrome/browser/browser.cc\', Split() would\n    return (\'chrome/browser\', \'browser\', \'.cc\')\n\n    Returns:\n      A tuple of (directory, basename, extension).\n    """"""\n\n    googlename = self.RepositoryName()\n    project, rest = os.path.split(googlename)\n    return (project,) + os.path.splitext(rest)\n\n  def BaseName(self):\n    """"""File base name - text after the final slash, before the final period.""""""\n    return self.Split()[1]\n\n  def Extension(self):\n    """"""File extension - text following the final period.""""""\n    return self.Split()[2]\n\n  def NoExtension(self):\n    """"""File has no source file extension.""""""\n    return \'/\'.join(self.Split()[0:2])\n\n  def IsSource(self):\n    """"""File has a source file extension.""""""\n    return self.Extension()[1:] in (\'c\', \'cc\', \'cpp\', \'cxx\')\n\n\ndef _ShouldPrintError(category, confidence, linenum):\n  """"""If confidence >= verbose, category passes filter and is not suppressed.""""""\n\n  # There are three ways we might decide not to print an error message:\n  # a ""NOLINT(category)"" comment appears in the source,\n  # the verbosity level isn\'t high enough, or the filters filter it out.\n  if IsErrorSuppressedByNolint(category, linenum):\n    return False\n\n  if confidence < _cpplint_state.verbose_level:\n    return False\n\n  is_filtered = False\n  for one_filter in _Filters():\n    if one_filter.startswith(\'-\'):\n      if category.startswith(one_filter[1:]):\n        is_filtered = True\n    elif one_filter.startswith(\'+\'):\n      if category.startswith(one_filter[1:]):\n        is_filtered = False\n    else:\n      assert False  # should have been checked for in SetFilter.\n  if is_filtered:\n    return False\n\n  return True\n\n\ndef Error(filename, linenum, category, confidence, message):\n  """"""Logs the fact we\'ve found a lint error.\n\n  We log where the error was found, and also our confidence in the error,\n  that is, how certain we are this is a legitimate style regression, and\n  not a misidentification or a use that\'s sometimes justified.\n\n  False positives can be suppressed by the use of\n  ""cpplint(category)""  comments on the offending line.  These are\n  parsed into _error_suppressions.\n\n  Args:\n    filename: The name of the file containing the error.\n    linenum: The number of the line containing the error.\n    category: A string used to describe the ""category"" this bug\n      falls under: ""whitespace"", say, or ""runtime"".  Categories\n      may have a hierarchy separated by slashes: ""whitespace/indent"".\n    confidence: A number from 1-5 representing a confidence score for\n      the error, with 5 meaning that we are certain of the problem,\n      and 1 meaning that it could be a legitimate construct.\n    message: The error message.\n  """"""\n  if _ShouldPrintError(category, confidence, linenum):\n    _cpplint_state.IncrementErrorCount(category)\n    if _cpplint_state.output_format == \'vs7\':\n      sys.stderr.write(\'%s(%s):  %s  [%s] [%d]\\n\' % (\n          filename, linenum, message, category, confidence))\n    elif _cpplint_state.output_format == \'eclipse\':\n      sys.stderr.write(\'%s:%s: warning: %s  [%s] [%d]\\n\' % (\n          filename, linenum, message, category, confidence))\n    else:\n      sys.stderr.write(\'%s:%s:  %s  [%s] [%d]\\n\' % (\n          filename, linenum, message, category, confidence))\n\n\n# Matches standard C++ escape sequences per 2.13.2.3 of the C++ standard.\n_RE_PATTERN_CLEANSE_LINE_ESCAPES = re.compile(\n    r\'\\\\([abfnrtv?""\\\\\\\']|\\d+|x[0-9a-fA-F]+)\')\n# Match a single C style comment on the same line.\n_RE_PATTERN_C_COMMENTS = r\'/\\*(?:[^*]|\\*(?!/))*\\*/\'\n# Matches multi-line C style comments.\n# This RE is a little bit more complicated than one might expect, because we\n# have to take care of space removals tools so we can handle comments inside\n# statements better.\n# The current rule is: We only clear spaces from both sides when we\'re at the\n# end of the line. Otherwise, we try to remove spaces from the right side,\n# if this doesn\'t work we try on left side but only if there\'s a non-character\n# on the right.\n_RE_PATTERN_CLEANSE_LINE_C_COMMENTS = re.compile(\n    r\'(\\s*\' + _RE_PATTERN_C_COMMENTS + r\'\\s*$|\' +\n    _RE_PATTERN_C_COMMENTS + r\'\\s+|\' +\n    r\'\\s+\' + _RE_PATTERN_C_COMMENTS + r\'(?=\\W)|\' +\n    _RE_PATTERN_C_COMMENTS + r\')\')\n\n\ndef IsCppString(line):\n  """"""Does line terminate so, that the next symbol is in string constant.\n\n  This function does not consider single-line nor multi-line comments.\n\n  Args:\n    line: is a partial line of code starting from the 0..n.\n\n  Returns:\n    True, if next character appended to \'line\' is inside a\n    string constant.\n  """"""\n\n  line = line.replace(r\'\\\\\', \'XX\')  # after this, \\\\"" does not match to \\""\n  return ((line.count(\'""\') - line.count(r\'\\""\') - line.count(""\'\\""\'"")) & 1) == 1\n\n\ndef CleanseRawStrings(raw_lines):\n  """"""Removes C++11 raw strings from lines.\n\n    Before:\n      static const char kData[] = R""(\n          multi-line string\n          )"";\n\n    After:\n      static const char kData[] = """"\n          (replaced by blank line)\n          """";\n\n  Args:\n    raw_lines: list of raw lines.\n\n  Returns:\n    list of lines with C++11 raw strings replaced by empty strings.\n  """"""\n\n  delimiter = None\n  lines_without_raw_strings = []\n  for line in raw_lines:\n    if delimiter:\n      # Inside a raw string, look for the end\n      end = line.find(delimiter)\n      if end >= 0:\n        # Found the end of the string, match leading space for this\n        # line and resume copying the original lines, and also insert\n        # a """" on the last line.\n        leading_space = Match(r\'^(\\s*)\\S\', line)\n        line = leading_space.group(1) + \'""""\' + line[end + len(delimiter):]\n        delimiter = None\n      else:\n        # Haven\'t found the end yet, append a blank line.\n        line = \'""""\'\n\n    # Look for beginning of a raw string, and replace them with\n    # empty strings.  This is done in a loop to handle multiple raw\n    # strings on the same line.\n    while delimiter is None:\n      # Look for beginning of a raw string.\n      # See 2.14.15 [lex.string] for syntax.\n      matched = Match(r\'^(.*)\\b(?:R|u8R|uR|UR|LR)""([^\\s\\\\()]*)\\((.*)$\', line)\n      if matched:\n        delimiter = \')\' + matched.group(2) + \'""\'\n\n        end = matched.group(3).find(delimiter)\n        if end >= 0:\n          # Raw string ended on same line\n          line = (matched.group(1) + \'""""\' +\n                  matched.group(3)[end + len(delimiter):])\n          delimiter = None\n        else:\n          # Start of a multi-line raw string\n          line = matched.group(1) + \'""""\'\n      else:\n        break\n\n    lines_without_raw_strings.append(line)\n\n  # TODO(unknown): if delimiter is not None here, we might want to\n  # emit a warning for unterminated string.\n  return lines_without_raw_strings\n\n\ndef FindNextMultiLineCommentStart(lines, lineix):\n  """"""Find the beginning marker for a multiline comment.""""""\n  while lineix < len(lines):\n    if lines[lineix].strip().startswith(\'/*\'):\n      # Only return this marker if the comment goes beyond this line\n      if lines[lineix].strip().find(\'*/\', 2) < 0:\n        return lineix\n    lineix += 1\n  return len(lines)\n\n\ndef FindNextMultiLineCommentEnd(lines, lineix):\n  """"""We are inside a comment, find the end marker.""""""\n  while lineix < len(lines):\n    if lines[lineix].strip().endswith(\'*/\'):\n      return lineix\n    lineix += 1\n  return len(lines)\n\n\ndef RemoveMultiLineCommentsFromRange(lines, begin, end):\n  """"""Clears a range of lines for multi-line comments.""""""\n  # Having // dummy comments makes the lines non-empty, so we will not get\n  # unnecessary blank line warnings later in the code.\n  for i in range(begin, end):\n    lines[i] = \'/**/\'\n\n\ndef RemoveMultiLineComments(filename, lines, error):\n  """"""Removes multiline (c-style) comments from lines.""""""\n  lineix = 0\n  while lineix < len(lines):\n    lineix_begin = FindNextMultiLineCommentStart(lines, lineix)\n    if lineix_begin >= len(lines):\n      return\n    lineix_end = FindNextMultiLineCommentEnd(lines, lineix_begin)\n    if lineix_end >= len(lines):\n      error(filename, lineix_begin + 1, \'readability/multiline_comment\', 5,\n            \'Could not find end of multi-line comment\')\n      return\n    RemoveMultiLineCommentsFromRange(lines, lineix_begin, lineix_end + 1)\n    lineix = lineix_end + 1\n\n\ndef CleanseComments(line):\n  """"""Removes //-comments and single-line C-style /* */ comments.\n\n  Args:\n    line: A line of C++ source.\n\n  Returns:\n    The line with single-line comments removed.\n  """"""\n  commentpos = line.find(\'//\')\n  if commentpos != -1 and not IsCppString(line[:commentpos]):\n    line = line[:commentpos].rstrip()\n  # get rid of /* ... */\n  return _RE_PATTERN_CLEANSE_LINE_C_COMMENTS.sub(\'\', line)\n\n\nclass CleansedLines(object):\n  """"""Holds 4 copies of all lines with different preprocessing applied to them.\n\n  1) elided member contains lines without strings and comments.\n  2) lines member contains lines without comments.\n  3) raw_lines member contains all the lines without processing.\n  4) lines_without_raw_strings member is same as raw_lines, but with C++11 raw\n     strings removed.\n  All these members are of <type \'list\'>, and of the same length.\n  """"""\n\n  def __init__(self, lines):\n    self.elided = []\n    self.lines = []\n    self.raw_lines = lines\n    self.num_lines = len(lines)\n    self.lines_without_raw_strings = CleanseRawStrings(lines)\n    for linenum in range(len(self.lines_without_raw_strings)):\n      self.lines.append(CleanseComments(\n          self.lines_without_raw_strings[linenum]))\n      elided = self._CollapseStrings(self.lines_without_raw_strings[linenum])\n      self.elided.append(CleanseComments(elided))\n\n  def NumLines(self):\n    """"""Returns the number of lines represented.""""""\n    return self.num_lines\n\n  @staticmethod\n  def _CollapseStrings(elided):\n    """"""Collapses strings and chars on a line to simple """" or \'\' blocks.\n\n    We nix strings first so we\'re not fooled by text like \'""http://""\'\n\n    Args:\n      elided: The line being processed.\n\n    Returns:\n      The line with collapsed strings.\n    """"""\n    if _RE_PATTERN_INCLUDE.match(elided):\n      return elided\n\n    # Remove escaped characters first to make quote/single quote collapsing\n    # basic.  Things that look like escaped characters shouldn\'t occur\n    # outside of strings and chars.\n    elided = _RE_PATTERN_CLEANSE_LINE_ESCAPES.sub(\'\', elided)\n\n    # Replace quoted strings and digit separators.  Both single quotes\n    # and double quotes are processed in the same loop, otherwise\n    # nested quotes wouldn\'t work.\n    collapsed = \'\'\n    while True:\n      # Find the first quote character\n      match = Match(r\'^([^\\\'""]*)([\\\'""])(.*)$\', elided)\n      if not match:\n        collapsed += elided\n        break\n      head, quote, tail = match.groups()\n\n      if quote == \'""\':\n        # Collapse double quoted strings\n        second_quote = tail.find(\'""\')\n        if second_quote >= 0:\n          collapsed += head + \'""""\'\n          elided = tail[second_quote + 1:]\n        else:\n          # Unmatched double quote, don\'t bother processing the rest\n          # of the line since this is probably a multiline string.\n          collapsed += elided\n          break\n      else:\n        # Found single quote, check nearby text to eliminate digit separators.\n        #\n        # There is no special handling for floating point here, because\n        # the integer/fractional/exponent parts would all be parsed\n        # correctly as long as there are digits on both sides of the\n        # separator.  So we are fine as long as we don\'t see something\n        # like ""0.\'3"" (gcc 4.9.0 will not allow this literal).\n        if Search(r\'\\b(?:0[bBxX]?|[1-9])[0-9a-fA-F]*$\', head):\n          match_literal = Match(r\'^((?:\\\'?[0-9a-zA-Z_])*)(.*)$\', ""\'"" + tail)\n          collapsed += head + match_literal.group(1).replace(""\'"", \'\')\n          elided = match_literal.group(2)\n        else:\n          second_quote = tail.find(\'\\\'\')\n          if second_quote >= 0:\n            collapsed += head + ""\'\'""\n            elided = tail[second_quote + 1:]\n          else:\n            # Unmatched single quote\n            collapsed += elided\n            break\n\n    return collapsed\n\n\ndef FindEndOfExpressionInLine(line, startpos, stack):\n  """"""Find the position just after the end of current parenthesized expression.\n\n  Args:\n    line: a CleansedLines line.\n    startpos: start searching at this position.\n    stack: nesting stack at startpos.\n\n  Returns:\n    On finding matching end: (index just after matching end, None)\n    On finding an unclosed expression: (-1, None)\n    Otherwise: (-1, new stack at end of this line)\n  """"""\n  for i in xrange(startpos, len(line)):\n    char = line[i]\n    if char in \'([{\':\n      # Found start of parenthesized expression, push to expression stack\n      stack.append(char)\n    elif char == \'<\':\n      # Found potential start of template argument list\n      if i > 0 and line[i - 1] == \'<\':\n        # Left shift operator\n        if stack and stack[-1] == \'<\':\n          stack.pop()\n          if not stack:\n            return (-1, None)\n      elif i > 0 and Search(r\'\\boperator\\s*$\', line[0:i]):\n        # operator<, don\'t add to stack\n        continue\n      else:\n        # Tentative start of template argument list\n        stack.append(\'<\')\n    elif char in \')]}\':\n      # Found end of parenthesized expression.\n      #\n      # If we are currently expecting a matching \'>\', the pending \'<\'\n      # must have been an operator.  Remove them from expression stack.\n      while stack and stack[-1] == \'<\':\n        stack.pop()\n      if not stack:\n        return (-1, None)\n      if ((stack[-1] == \'(\' and char == \')\') or\n          (stack[-1] == \'[\' and char == \']\') or\n          (stack[-1] == \'{\' and char == \'}\')):\n        stack.pop()\n        if not stack:\n          return (i + 1, None)\n      else:\n        # Mismatched parentheses\n        return (-1, None)\n    elif char == \'>\':\n      # Found potential end of template argument list.\n\n      # Ignore ""->"" and operator functions\n      if (i > 0 and\n          (line[i - 1] == \'-\' or Search(r\'\\boperator\\s*$\', line[0:i - 1]))):\n        continue\n\n      # Pop the stack if there is a matching \'<\'.  Otherwise, ignore\n      # this \'>\' since it must be an operator.\n      if stack:\n        if stack[-1] == \'<\':\n          stack.pop()\n          if not stack:\n            return (i + 1, None)\n    elif char == \';\':\n      # Found something that look like end of statements.  If we are currently\n      # expecting a \'>\', the matching \'<\' must have been an operator, since\n      # template argument list should not contain statements.\n      while stack and stack[-1] == \'<\':\n        stack.pop()\n      if not stack:\n        return (-1, None)\n\n  # Did not find end of expression or unbalanced parentheses on this line\n  return (-1, stack)\n\n\ndef CloseExpression(clean_lines, linenum, pos):\n  """"""If input points to ( or { or [ or <, finds the position that closes it.\n\n  If lines[linenum][pos] points to a \'(\' or \'{\' or \'[\' or \'<\', finds the\n  linenum/pos that correspond to the closing of the expression.\n\n  TODO(unknown): cpplint spends a fair bit of time matching parentheses.\n  Ideally we would want to index all opening and closing parentheses once\n  and have CloseExpression be just a simple lookup, but due to preprocessor\n  tricks, this is not so easy.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    pos: A position on the line.\n\n  Returns:\n    A tuple (line, linenum, pos) pointer *past* the closing brace, or\n    (line, len(lines), -1) if we never find a close.  Note we ignore\n    strings and comments when matching; and the line we return is the\n    \'cleansed\' line at linenum.\n  """"""\n\n  line = clean_lines.elided[linenum]\n  if (line[pos] not in \'({[<\') or Match(r\'<[<=]\', line[pos:]):\n    return (line, clean_lines.NumLines(), -1)\n\n  # Check first line\n  (end_pos, stack) = FindEndOfExpressionInLine(line, pos, [])\n  if end_pos > -1:\n    return (line, linenum, end_pos)\n\n  # Continue scanning forward\n  while stack and linenum < clean_lines.NumLines() - 1:\n    linenum += 1\n    line = clean_lines.elided[linenum]\n    (end_pos, stack) = FindEndOfExpressionInLine(line, 0, stack)\n    if end_pos > -1:\n      return (line, linenum, end_pos)\n\n  # Did not find end of expression before end of file, give up\n  return (line, clean_lines.NumLines(), -1)\n\n\ndef FindStartOfExpressionInLine(line, endpos, stack):\n  """"""Find position at the matching start of current expression.\n\n  This is almost the reverse of FindEndOfExpressionInLine, but note\n  that the input position and returned position differs by 1.\n\n  Args:\n    line: a CleansedLines line.\n    endpos: start searching at this position.\n    stack: nesting stack at endpos.\n\n  Returns:\n    On finding matching start: (index at matching start, None)\n    On finding an unclosed expression: (-1, None)\n    Otherwise: (-1, new stack at beginning of this line)\n  """"""\n  i = endpos\n  while i >= 0:\n    char = line[i]\n    if char in \')]}\':\n      # Found end of expression, push to expression stack\n      stack.append(char)\n    elif char == \'>\':\n      # Found potential end of template argument list.\n      #\n      # Ignore it if it\'s a ""->"" or "">="" or ""operator>""\n      if (i > 0 and\n          (line[i - 1] == \'-\' or\n           Match(r\'\\s>=\\s\', line[i - 1:]) or\n           Search(r\'\\boperator\\s*$\', line[0:i]))):\n        i -= 1\n      else:\n        stack.append(\'>\')\n    elif char == \'<\':\n      # Found potential start of template argument list\n      if i > 0 and line[i - 1] == \'<\':\n        # Left shift operator\n        i -= 1\n      else:\n        # If there is a matching \'>\', we can pop the expression stack.\n        # Otherwise, ignore this \'<\' since it must be an operator.\n        if stack and stack[-1] == \'>\':\n          stack.pop()\n          if not stack:\n            return (i, None)\n    elif char in \'([{\':\n      # Found start of expression.\n      #\n      # If there are any unmatched \'>\' on the stack, they must be\n      # operators.  Remove those.\n      while stack and stack[-1] == \'>\':\n        stack.pop()\n      if not stack:\n        return (-1, None)\n      if ((char == \'(\' and stack[-1] == \')\') or\n          (char == \'[\' and stack[-1] == \']\') or\n          (char == \'{\' and stack[-1] == \'}\')):\n        stack.pop()\n        if not stack:\n          return (i, None)\n      else:\n        # Mismatched parentheses\n        return (-1, None)\n    elif char == \';\':\n      # Found something that look like end of statements.  If we are currently\n      # expecting a \'<\', the matching \'>\' must have been an operator, since\n      # template argument list should not contain statements.\n      while stack and stack[-1] == \'>\':\n        stack.pop()\n      if not stack:\n        return (-1, None)\n\n    i -= 1\n\n  return (-1, stack)\n\n\ndef ReverseCloseExpression(clean_lines, linenum, pos):\n  """"""If input points to ) or } or ] or >, finds the position that opens it.\n\n  If lines[linenum][pos] points to a \')\' or \'}\' or \']\' or \'>\', finds the\n  linenum/pos that correspond to the opening of the expression.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    pos: A position on the line.\n\n  Returns:\n    A tuple (line, linenum, pos) pointer *at* the opening brace, or\n    (line, 0, -1) if we never find the matching opening brace.  Note\n    we ignore strings and comments when matching; and the line we\n    return is the \'cleansed\' line at linenum.\n  """"""\n  line = clean_lines.elided[linenum]\n  if line[pos] not in \')}]>\':\n    return (line, 0, -1)\n\n  # Check last line\n  (start_pos, stack) = FindStartOfExpressionInLine(line, pos, [])\n  if start_pos > -1:\n    return (line, linenum, start_pos)\n\n  # Continue scanning backward\n  while stack and linenum > 0:\n    linenum -= 1\n    line = clean_lines.elided[linenum]\n    (start_pos, stack) = FindStartOfExpressionInLine(line, len(line) - 1, stack)\n    if start_pos > -1:\n      return (line, linenum, start_pos)\n\n  # Did not find start of expression before beginning of file, give up\n  return (line, 0, -1)\n\n\ndef CheckForCopyright(filename, lines, error):\n  """"""Logs an error if no Copyright message appears at the top of the file.""""""\n\n  # We\'ll say it should occur by line 10. Don\'t forget there\'s a\n  # dummy line at the front.\n  for line in xrange(1, min(len(lines), 11)):\n    if re.search(r\'Copyright\', lines[line], re.I): break\n  else:                       # means no copyright line was found\n    error(filename, 0, \'legal/copyright\', 5,\n          \'No copyright message found.  \'\n          \'You should have a line: ""Copyright [year] <Copyright Owner>""\')\n\n\ndef GetIndentLevel(line):\n  """"""Return the number of leading spaces in line.\n\n  Args:\n    line: A string to check.\n\n  Returns:\n    An integer count of leading spaces, possibly zero.\n  """"""\n  indent = Match(r\'^( *)\\S\', line)\n  if indent:\n    return len(indent.group(1))\n  else:\n    return 0\n\n\ndef GetHeaderGuardCPPVariable(filename):\n  """"""Returns the CPP variable that should be used as a header guard.\n\n  Args:\n    filename: The name of a C++ header file.\n\n  Returns:\n    The CPP variable that should be used as a header guard in the\n    named file.\n\n  """"""\n\n  # Restores original filename in case that cpplint is invoked from Emacs\'s\n  # flymake.\n  filename = re.sub(r\'_flymake\\.h$\', \'.h\', filename)\n  filename = re.sub(r\'/\\.flymake/([^/]*)$\', r\'/\\1\', filename)\n  # Replace \'c++\' with \'cpp\'.\n  filename = filename.replace(\'C++\', \'cpp\').replace(\'c++\', \'cpp\')\n  \n  fileinfo = FileInfo(filename)\n  file_path_from_root = fileinfo.RepositoryName()\n  if _root:\n    file_path_from_root = re.sub(\'^\' + _root + os.sep, \'\', file_path_from_root)\n  return re.sub(r\'[^a-zA-Z0-9]\', \'_\', file_path_from_root).upper() + \'_\'\n\n\ndef CheckForHeaderGuard(filename, clean_lines, error):\n  """"""Checks that the file contains a header guard.\n\n  Logs an error if no #ifndef header guard is present.  For other\n  headers, checks that the full pathname is used.\n\n  Args:\n    filename: The name of the C++ header file.\n    clean_lines: A CleansedLines instance containing the file.\n    error: The function to call with any errors found.\n  """"""\n\n  # Don\'t check for header guards if there are error suppression\n  # comments somewhere in this file.\n  #\n  # Because this is silencing a warning for a nonexistent line, we\n  # only support the very specific NOLINT(build/header_guard) syntax,\n  # and not the general NOLINT or NOLINT(*) syntax.\n  raw_lines = clean_lines.lines_without_raw_strings\n  for i in raw_lines:\n    if Search(r\'//\\s*NOLINT\\(build/header_guard\\)\', i):\n      return\n\n  cppvar = GetHeaderGuardCPPVariable(filename)\n\n  ifndef = \'\'\n  ifndef_linenum = 0\n  define = \'\'\n  endif = \'\'\n  endif_linenum = 0\n  for linenum, line in enumerate(raw_lines):\n    linesplit = line.split()\n    if len(linesplit) >= 2:\n      # find the first occurrence of #ifndef and #define, save arg\n      if not ifndef and linesplit[0] == \'#ifndef\':\n        # set ifndef to the header guard presented on the #ifndef line.\n        ifndef = linesplit[1]\n        ifndef_linenum = linenum\n      if not define and linesplit[0] == \'#define\':\n        define = linesplit[1]\n    # find the last occurrence of #endif, save entire line\n    if line.startswith(\'#endif\'):\n      endif = line\n      endif_linenum = linenum\n\n  if not ifndef or not define or ifndef != define:\n    error(filename, 0, \'build/header_guard\', 5,\n          \'No #ifndef header guard found, suggested CPP variable is: %s\' %\n          cppvar)\n    return\n\n  # The guard should be PATH_FILE_H_, but we also allow PATH_FILE_H__\n  # for backward compatibility.\n  if ifndef != cppvar:\n    error_level = 0\n    if ifndef != cppvar + \'_\':\n      error_level = 5\n\n    ParseNolintSuppressions(filename, raw_lines[ifndef_linenum], ifndef_linenum,\n                            error)\n    error(filename, ifndef_linenum, \'build/header_guard\', error_level,\n          \'#ifndef header guard has wrong style, please use: %s\' % cppvar)\n\n  # Check for ""//"" comments on endif line.\n  ParseNolintSuppressions(filename, raw_lines[endif_linenum], endif_linenum,\n                          error)\n  match = Match(r\'#endif\\s*//\\s*\' + cppvar + r\'(_)?\\b\', endif)\n  if match:\n    if match.group(1) == \'_\':\n      # Issue low severity warning for deprecated double trailing underscore\n      error(filename, endif_linenum, \'build/header_guard\', 0,\n            \'#endif line should be ""#endif  // %s""\' % cppvar)\n    return\n\n  # Didn\'t find the corresponding ""//"" comment.  If this file does not\n  # contain any ""//"" comments at all, it could be that the compiler\n  # only wants ""/**/"" comments, look for those instead.\n  no_single_line_comments = True\n  for i in xrange(1, len(raw_lines) - 1):\n    line = raw_lines[i]\n    if Match(r\'^(?:(?:\\\'(?:\\.|[^\\\'])*\\\')|(?:""(?:\\.|[^""])*"")|[^\\\'""])*//\', line):\n      no_single_line_comments = False\n      break\n\n  if no_single_line_comments:\n    match = Match(r\'#endif\\s*/\\*\\s*\' + cppvar + r\'(_)?\\s*\\*/\', endif)\n    if match:\n      if match.group(1) == \'_\':\n        # Low severity warning for double trailing underscore\n        error(filename, endif_linenum, \'build/header_guard\', 0,\n              \'#endif line should be ""#endif  /* %s */""\' % cppvar)\n      return\n\n  # Didn\'t find anything\n  error(filename, endif_linenum, \'build/header_guard\', 5,\n        \'#endif line should be ""#endif  // %s""\' % cppvar)\n\n\ndef CheckHeaderFileIncluded(filename, include_state, error):\n  """"""Logs an error if a .cc file does not include its header.""""""\n\n  # Do not check test files\n  if filename.endswith(\'_test.cc\') or filename.endswith(\'_unittest.cc\'):\n    return\n\n  fileinfo = FileInfo(filename)\n  headerfile = filename[0:len(filename) - 2] + \'h\'\n  if not os.path.exists(headerfile):\n    return\n  headername = FileInfo(headerfile).RepositoryName()\n  first_include = 0\n  for section_list in include_state.include_list:\n    for f in section_list:\n      if headername in f[0] or f[0] in headername:\n        return\n      if not first_include:\n        first_include = f[1]\n\n  error(filename, first_include, \'build/include\', 5,\n        \'%s should include its header file %s\' % (fileinfo.RepositoryName(),\n                                                  headername))\n\n\ndef CheckForBadCharacters(filename, lines, error):\n  """"""Logs an error for each line containing bad characters.\n\n  Two kinds of bad characters:\n\n  1. Unicode replacement characters: These indicate that either the file\n  contained invalid UTF-8 (likely) or Unicode replacement characters (which\n  it shouldn\'t).  Note that it\'s possible for this to throw off line\n  numbering if the invalid UTF-8 occurred adjacent to a newline.\n\n  2. NUL bytes.  These are problematic for some tools.\n\n  Args:\n    filename: The name of the current file.\n    lines: An array of strings, each representing a line of the file.\n    error: The function to call with any errors found.\n  """"""\n  for linenum, line in enumerate(lines):\n    if u\'\\ufffd\' in line:\n      error(filename, linenum, \'readability/utf8\', 5,\n            \'Line contains invalid UTF-8 (or Unicode replacement character).\')\n    if \'\\0\' in line:\n      error(filename, linenum, \'readability/nul\', 5, \'Line contains NUL byte.\')\n\n\ndef CheckForNewlineAtEOF(filename, lines, error):\n  """"""Logs an error if there is no newline char at the end of the file.\n\n  Args:\n    filename: The name of the current file.\n    lines: An array of strings, each representing a line of the file.\n    error: The function to call with any errors found.\n  """"""\n\n  # The array lines() was created by adding two newlines to the\n  # original file (go figure), then splitting on \\n.\n  # To verify that the file ends in \\n, we just have to make sure the\n  # last-but-two element of lines() exists and is empty.\n  if len(lines) < 3 or lines[-2]:\n    error(filename, len(lines) - 2, \'whitespace/ending_newline\', 5,\n          \'Could not find a newline character at the end of the file.\')\n\n\ndef CheckForMultilineCommentsAndStrings(filename, clean_lines, linenum, error):\n  """"""Logs an error if we see /* ... */ or ""..."" that extend past one line.\n\n  /* ... */ comments are legit inside macros, for one line.\n  Otherwise, we prefer // comments, so it\'s ok to warn about the\n  other.  Likewise, it\'s ok for strings to extend across multiple\n  lines, as long as a line continuation character (backslash)\n  terminates each line. Although not currently prohibited by the C++\n  style guide, it\'s ugly and unnecessary. We don\'t do well with either\n  in this lint program, so we warn about both.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n\n  # Remove all \\\\ (escaped backslashes) from the line. They are OK, and the\n  # second (escaped) slash may trigger later \\"" detection erroneously.\n  line = line.replace(\'\\\\\\\\\', \'\')\n\n  if line.count(\'/*\') > line.count(\'*/\'):\n    error(filename, linenum, \'readability/multiline_comment\', 5,\n          \'Complex multi-line /*...*/-style comment found. \'\n          \'Lint may give bogus warnings.  \'\n          \'Consider replacing these with //-style comments, \'\n          \'with #if 0...#endif, \'\n          \'or with more clearly structured multi-line comments.\')\n\n  if (line.count(\'""\') - line.count(\'\\\\""\')) % 2:\n    error(filename, linenum, \'readability/multiline_string\', 5,\n          \'Multi-line string (""..."") found.  This lint script doesn\\\'t \'\n          \'do well with such strings, and may give bogus warnings.  \'\n          \'Use C++11 raw strings or concatenation instead.\')\n\n\n# (non-threadsafe name, thread-safe alternative, validation pattern)\n#\n# The validation pattern is used to eliminate false positives such as:\n#  _rand();               // false positive due to substring match.\n#  ->rand();              // some member function rand().\n#  ACMRandom rand(seed);  // some variable named rand.\n#  ISAACRandom rand();    // another variable named rand.\n#\n# Basically we require the return value of these functions to be used\n# in some expression context on the same line by matching on some\n# operator before the function name.  This eliminates constructors and\n# member function calls.\n_UNSAFE_FUNC_PREFIX = r\'(?:[-+*/=%^&|(<]\\s*|>\\s+)\'\n_THREADING_LIST = (\n    (\'asctime(\', \'asctime_r(\', _UNSAFE_FUNC_PREFIX + r\'asctime\\([^)]+\\)\'),\n    (\'ctime(\', \'ctime_r(\', _UNSAFE_FUNC_PREFIX + r\'ctime\\([^)]+\\)\'),\n    (\'getgrgid(\', \'getgrgid_r(\', _UNSAFE_FUNC_PREFIX + r\'getgrgid\\([^)]+\\)\'),\n    (\'getgrnam(\', \'getgrnam_r(\', _UNSAFE_FUNC_PREFIX + r\'getgrnam\\([^)]+\\)\'),\n    (\'getlogin(\', \'getlogin_r(\', _UNSAFE_FUNC_PREFIX + r\'getlogin\\(\\)\'),\n    (\'getpwnam(\', \'getpwnam_r(\', _UNSAFE_FUNC_PREFIX + r\'getpwnam\\([^)]+\\)\'),\n    (\'getpwuid(\', \'getpwuid_r(\', _UNSAFE_FUNC_PREFIX + r\'getpwuid\\([^)]+\\)\'),\n    (\'gmtime(\', \'gmtime_r(\', _UNSAFE_FUNC_PREFIX + r\'gmtime\\([^)]+\\)\'),\n    (\'localtime(\', \'localtime_r(\', _UNSAFE_FUNC_PREFIX + r\'localtime\\([^)]+\\)\'),\n    (\'rand(\', \'rand_r(\', _UNSAFE_FUNC_PREFIX + r\'rand\\(\\)\'),\n    (\'strtok(\', \'strtok_r(\',\n     _UNSAFE_FUNC_PREFIX + r\'strtok\\([^)]+\\)\'),\n    (\'ttyname(\', \'ttyname_r(\', _UNSAFE_FUNC_PREFIX + r\'ttyname\\([^)]+\\)\'),\n    )\n\n\ndef CheckPosixThreading(filename, clean_lines, linenum, error):\n  """"""Checks for calls to thread-unsafe functions.\n\n  Much code has been originally written without consideration of\n  multi-threading. Also, engineers are relying on their old experience;\n  they have learned posix before threading extensions were added. These\n  tests guide the engineers to use thread-safe functions (when using\n  posix directly).\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n  for single_thread_func, multithread_safe_func, pattern in _THREADING_LIST:\n    # Additional pattern matching check to confirm that this is the\n    # function we are looking for\n    if Search(pattern, line):\n      error(filename, linenum, \'runtime/threadsafe_fn\', 2,\n            \'Consider using \' + multithread_safe_func +\n            \'...) instead of \' + single_thread_func +\n            \'...) for improved thread safety.\')\n\n\ndef CheckVlogArguments(filename, clean_lines, linenum, error):\n  """"""Checks that VLOG() is only used for defining a logging level.\n\n  For example, VLOG(2) is correct. VLOG(INFO), VLOG(WARNING), VLOG(ERROR), and\n  VLOG(FATAL) are not.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n  if Search(r\'\\bVLOG\\((INFO|ERROR|WARNING|DFATAL|FATAL)\\)\', line):\n    error(filename, linenum, \'runtime/vlog\', 5,\n          \'VLOG() should be used with numeric verbosity level.  \'\n          \'Use LOG() if you want symbolic severity levels.\')\n\n# Matches invalid increment: *count++, which moves pointer instead of\n# incrementing a value.\n_RE_PATTERN_INVALID_INCREMENT = re.compile(\n    r\'^\\s*\\*\\w+(\\+\\+|--);\')\n\n\ndef CheckInvalidIncrement(filename, clean_lines, linenum, error):\n  """"""Checks for invalid increment *count++.\n\n  For example following function:\n  void increment_counter(int* count) {\n    *count++;\n  }\n  is invalid, because it effectively does count++, moving pointer, and should\n  be replaced with ++*count, (*count)++ or *count += 1.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n  if _RE_PATTERN_INVALID_INCREMENT.match(line):\n    error(filename, linenum, \'runtime/invalid_increment\', 5,\n          \'Changing pointer instead of value (or unused value of operator*).\')\n\n\ndef IsMacroDefinition(clean_lines, linenum):\n  if Search(r\'^#define\', clean_lines[linenum]):\n    return True\n\n  if linenum > 0 and Search(r\'\\\\$\', clean_lines[linenum - 1]):\n    return True\n\n  return False\n\n\ndef IsForwardClassDeclaration(clean_lines, linenum):\n  return Match(r\'^\\s*(\\btemplate\\b)*.*class\\s+\\w+;\\s*$\', clean_lines[linenum])\n\n\nclass _BlockInfo(object):\n  """"""Stores information about a generic block of code.""""""\n\n  def __init__(self, seen_open_brace):\n    self.seen_open_brace = seen_open_brace\n    self.open_parentheses = 0\n    self.inline_asm = _NO_ASM\n    self.check_namespace_indentation = False\n\n  def CheckBegin(self, filename, clean_lines, linenum, error):\n    """"""Run checks that applies to text up to the opening brace.\n\n    This is mostly for checking the text after the class identifier\n    and the ""{"", usually where the base class is specified.  For other\n    blocks, there isn\'t much to check, so we always pass.\n\n    Args:\n      filename: The name of the current file.\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      error: The function to call with any errors found.\n    """"""\n    pass\n\n  def CheckEnd(self, filename, clean_lines, linenum, error):\n    """"""Run checks that applies to text after the closing brace.\n\n    This is mostly used for checking end of namespace comments.\n\n    Args:\n      filename: The name of the current file.\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      error: The function to call with any errors found.\n    """"""\n    pass\n\n  def IsBlockInfo(self):\n    """"""Returns true if this block is a _BlockInfo.\n\n    This is convenient for verifying that an object is an instance of\n    a _BlockInfo, but not an instance of any of the derived classes.\n\n    Returns:\n      True for this class, False for derived classes.\n    """"""\n    return self.__class__ == _BlockInfo\n\n\nclass _ExternCInfo(_BlockInfo):\n  """"""Stores information about an \'extern ""C""\' block.""""""\n\n  def __init__(self):\n    _BlockInfo.__init__(self, True)\n\n\nclass _ClassInfo(_BlockInfo):\n  """"""Stores information about a class.""""""\n\n  def __init__(self, name, class_or_struct, clean_lines, linenum):\n    _BlockInfo.__init__(self, False)\n    self.name = name\n    self.starting_linenum = linenum\n    self.is_derived = False\n    self.check_namespace_indentation = True\n    if class_or_struct == \'struct\':\n      self.access = \'public\'\n      self.is_struct = True\n    else:\n      self.access = \'private\'\n      self.is_struct = False\n\n    # Remember initial indentation level for this class.  Using raw_lines here\n    # instead of elided to account for leading comments.\n    self.class_indent = GetIndentLevel(clean_lines.raw_lines[linenum])\n\n    # Try to find the end of the class.  This will be confused by things like:\n    #   class A {\n    #   } *x = { ...\n    #\n    # But it\'s still good enough for CheckSectionSpacing.\n    self.last_line = 0\n    depth = 0\n    for i in range(linenum, clean_lines.NumLines()):\n      line = clean_lines.elided[i]\n      depth += line.count(\'{\') - line.count(\'}\')\n      if not depth:\n        self.last_line = i\n        break\n\n  def CheckBegin(self, filename, clean_lines, linenum, error):\n    # Look for a bare \':\'\n    if Search(\'(^|[^:]):($|[^:])\', clean_lines.elided[linenum]):\n      self.is_derived = True\n\n  def CheckEnd(self, filename, clean_lines, linenum, error):\n    # If there is a DISALLOW macro, it should appear near the end of\n    # the class.\n    seen_last_thing_in_class = False\n    for i in xrange(linenum - 1, self.starting_linenum, -1):\n      match = Search(\n          r\'\\b(DISALLOW_COPY_AND_ASSIGN|DISALLOW_IMPLICIT_CONSTRUCTORS)\\(\' +\n          self.name + r\'\\)\',\n          clean_lines.elided[i])\n      if match:\n        if seen_last_thing_in_class:\n          error(filename, i, \'readability/constructors\', 3,\n                match.group(1) + \' should be the last thing in the class\')\n        break\n\n      if not Match(r\'^\\s*$\', clean_lines.elided[i]):\n        seen_last_thing_in_class = True\n\n    # Check that closing brace is aligned with beginning of the class.\n    # Only do this if the closing brace is indented by only whitespaces.\n    # This means we will not check single-line class definitions.\n    indent = Match(r\'^( *)\\}\', clean_lines.elided[linenum])\n    if indent and len(indent.group(1)) != self.class_indent:\n      if self.is_struct:\n        parent = \'struct \' + self.name\n      else:\n        parent = \'class \' + self.name\n      error(filename, linenum, \'whitespace/indent\', 3,\n            \'Closing brace should be aligned with beginning of %s\' % parent)\n\n\nclass _NamespaceInfo(_BlockInfo):\n  """"""Stores information about a namespace.""""""\n\n  def __init__(self, name, linenum):\n    _BlockInfo.__init__(self, False)\n    self.name = name or \'\'\n    self.starting_linenum = linenum\n    self.check_namespace_indentation = True\n\n  def CheckEnd(self, filename, clean_lines, linenum, error):\n    """"""Check end of namespace comments.""""""\n    line = clean_lines.raw_lines[linenum]\n\n    # Check how many lines is enclosed in this namespace.  Don\'t issue\n    # warning for missing namespace comments if there aren\'t enough\n    # lines.  However, do apply checks if there is already an end of\n    # namespace comment and it\'s incorrect.\n    #\n    # TODO(unknown): We always want to check end of namespace comments\n    # if a namespace is large, but sometimes we also want to apply the\n    # check if a short namespace contained nontrivial things (something\n    # other than forward declarations).  There is currently no logic on\n    # deciding what these nontrivial things are, so this check is\n    # triggered by namespace size only, which works most of the time.\n    if (linenum - self.starting_linenum < 10\n        and not Match(r\'};*\\s*(//|/\\*).*\\bnamespace\\b\', line)):\n      return\n\n    # Look for matching comment at end of namespace.\n    #\n    # Note that we accept C style ""/* */"" comments for terminating\n    # namespaces, so that code that terminate namespaces inside\n    # preprocessor macros can be cpplint clean.\n    #\n    # We also accept stuff like ""// end of namespace <name>."" with the\n    # period at the end.\n    #\n    # Besides these, we don\'t accept anything else, otherwise we might\n    # get false negatives when existing comment is a substring of the\n    # expected namespace.\n    if self.name:\n      # Named namespace\n      if not Match((r\'};*\\s*(//|/\\*).*\\bnamespace\\s+\' + re.escape(self.name) +\n                    r\'[\\*/\\.\\\\\\s]*$\'),\n                   line):\n        error(filename, linenum, \'readability/namespace\', 5,\n              \'Namespace should be terminated with ""// namespace %s""\' %\n              self.name)\n    else:\n      # Anonymous namespace\n      if not Match(r\'};*\\s*(//|/\\*).*\\bnamespace[\\*/\\.\\\\\\s]*$\', line):\n        # If ""// namespace anonymous"" or ""// anonymous namespace (more text)"",\n        # mention ""// anonymous namespace"" as an acceptable form\n        if Match(r\'}.*\\b(namespace anonymous|anonymous namespace)\\b\', line):\n          error(filename, linenum, \'readability/namespace\', 5,\n                \'Anonymous namespace should be terminated with ""// namespace""\'\n                \' or ""// anonymous namespace""\')\n        else:\n          error(filename, linenum, \'readability/namespace\', 5,\n                \'Anonymous namespace should be terminated with ""// namespace""\')\n\n\nclass _PreprocessorInfo(object):\n  """"""Stores checkpoints of nesting stacks when #if/#else is seen.""""""\n\n  def __init__(self, stack_before_if):\n    # The entire nesting stack before #if\n    self.stack_before_if = stack_before_if\n\n    # The entire nesting stack up to #else\n    self.stack_before_else = []\n\n    # Whether we have already seen #else or #elif\n    self.seen_else = False\n\n\nclass NestingState(object):\n  """"""Holds states related to parsing braces.""""""\n\n  def __init__(self):\n    # Stack for tracking all braces.  An object is pushed whenever we\n    # see a ""{"", and popped when we see a ""}"".  Only 3 types of\n    # objects are possible:\n    # - _ClassInfo: a class or struct.\n    # - _NamespaceInfo: a namespace.\n    # - _BlockInfo: some other type of block.\n    self.stack = []\n\n    # Top of the previous stack before each Update().\n    #\n    # Because the nesting_stack is updated at the end of each line, we\n    # had to do some convoluted checks to find out what is the current\n    # scope at the beginning of the line.  This check is simplified by\n    # saving the previous top of nesting stack.\n    #\n    # We could save the full stack, but we only need the top.  Copying\n    # the full nesting stack would slow down cpplint by ~10%.\n    self.previous_stack_top = []\n\n    # Stack of _PreprocessorInfo objects.\n    self.pp_stack = []\n\n  def SeenOpenBrace(self):\n    """"""Check if we have seen the opening brace for the innermost block.\n\n    Returns:\n      True if we have seen the opening brace, False if the innermost\n      block is still expecting an opening brace.\n    """"""\n    return (not self.stack) or self.stack[-1].seen_open_brace\n\n  def InNamespaceBody(self):\n    """"""Check if we are currently one level inside a namespace body.\n\n    Returns:\n      True if top of the stack is a namespace block, False otherwise.\n    """"""\n    return self.stack and isinstance(self.stack[-1], _NamespaceInfo)\n\n  def InExternC(self):\n    """"""Check if we are currently one level inside an \'extern ""C""\' block.\n\n    Returns:\n      True if top of the stack is an extern block, False otherwise.\n    """"""\n    return self.stack and isinstance(self.stack[-1], _ExternCInfo)\n\n  def InClassDeclaration(self):\n    """"""Check if we are currently one level inside a class or struct declaration.\n\n    Returns:\n      True if top of the stack is a class/struct, False otherwise.\n    """"""\n    return self.stack and isinstance(self.stack[-1], _ClassInfo)\n\n  def InAsmBlock(self):\n    """"""Check if we are currently one level inside an inline ASM block.\n\n    Returns:\n      True if the top of the stack is a block containing inline ASM.\n    """"""\n    return self.stack and self.stack[-1].inline_asm != _NO_ASM\n\n  def InTemplateArgumentList(self, clean_lines, linenum, pos):\n    """"""Check if current position is inside template argument list.\n\n    Args:\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      pos: position just after the suspected template argument.\n    Returns:\n      True if (linenum, pos) is inside template arguments.\n    """"""\n    while linenum < clean_lines.NumLines():\n      # Find the earliest character that might indicate a template argument\n      line = clean_lines.elided[linenum]\n      match = Match(r\'^[^{};=\\[\\]\\.<>]*(.)\', line[pos:])\n      if not match:\n        linenum += 1\n        pos = 0\n        continue\n      token = match.group(1)\n      pos += len(match.group(0))\n\n      # These things do not look like template argument list:\n      #   class Suspect {\n      #   class Suspect x; }\n      if token in (\'{\', \'}\', \';\'): return False\n\n      # These things look like template argument list:\n      #   template <class Suspect>\n      #   template <class Suspect = default_value>\n      #   template <class Suspect[]>\n      #   template <class Suspect...>\n      if token in (\'>\', \'=\', \'[\', \']\', \'.\'): return True\n\n      # Check if token is an unmatched \'<\'.\n      # If not, move on to the next character.\n      if token != \'<\':\n        pos += 1\n        if pos >= len(line):\n          linenum += 1\n          pos = 0\n        continue\n\n      # We can\'t be sure if we just find a single \'<\', and need to\n      # find the matching \'>\'.\n      (_, end_line, end_pos) = CloseExpression(clean_lines, linenum, pos - 1)\n      if end_pos < 0:\n        # Not sure if template argument list or syntax error in file\n        return False\n      linenum = end_line\n      pos = end_pos\n    return False\n\n  def UpdatePreprocessor(self, line):\n    """"""Update preprocessor stack.\n\n    We need to handle preprocessors due to classes like this:\n      #ifdef SWIG\n      struct ResultDetailsPageElementExtensionPoint {\n      #else\n      struct ResultDetailsPageElementExtensionPoint : public Extension {\n      #endif\n\n    We make the following assumptions (good enough for most files):\n    - Preprocessor condition evaluates to true from #if up to first\n      #else/#elif/#endif.\n\n    - Preprocessor condition evaluates to false from #else/#elif up\n      to #endif.  We still perform lint checks on these lines, but\n      these do not affect nesting stack.\n\n    Args:\n      line: current line to check.\n    """"""\n    if Match(r\'^\\s*#\\s*(if|ifdef|ifndef)\\b\', line):\n      # Beginning of #if block, save the nesting stack here.  The saved\n      # stack will allow us to restore the parsing state in the #else case.\n      self.pp_stack.append(_PreprocessorInfo(copy.deepcopy(self.stack)))\n    elif Match(r\'^\\s*#\\s*(else|elif)\\b\', line):\n      # Beginning of #else block\n      if self.pp_stack:\n        if not self.pp_stack[-1].seen_else:\n          # This is the first #else or #elif block.  Remember the\n          # whole nesting stack up to this point.  This is what we\n          # keep after the #endif.\n          self.pp_stack[-1].seen_else = True\n          self.pp_stack[-1].stack_before_else = copy.deepcopy(self.stack)\n\n        # Restore the stack to how it was before the #if\n        self.stack = copy.deepcopy(self.pp_stack[-1].stack_before_if)\n      else:\n        # TODO(unknown): unexpected #else, issue warning?\n        pass\n    elif Match(r\'^\\s*#\\s*endif\\b\', line):\n      # End of #if or #else blocks.\n      if self.pp_stack:\n        # If we saw an #else, we will need to restore the nesting\n        # stack to its former state before the #else, otherwise we\n        # will just continue from where we left off.\n        if self.pp_stack[-1].seen_else:\n          # Here we can just use a shallow copy since we are the last\n          # reference to it.\n          self.stack = self.pp_stack[-1].stack_before_else\n        # Drop the corresponding #if\n        self.pp_stack.pop()\n      else:\n        # TODO(unknown): unexpected #endif, issue warning?\n        pass\n\n  # TODO(unknown): Update() is too long, but we will refactor later.\n  def Update(self, filename, clean_lines, linenum, error):\n    """"""Update nesting state with current line.\n\n    Args:\n      filename: The name of the current file.\n      clean_lines: A CleansedLines instance containing the file.\n      linenum: The number of the line to check.\n      error: The function to call with any errors found.\n    """"""\n    line = clean_lines.elided[linenum]\n\n    # Remember top of the previous nesting stack.\n    #\n    # The stack is always pushed/popped and not modified in place, so\n    # we can just do a shallow copy instead of copy.deepcopy.  Using\n    # deepcopy would slow down cpplint by ~28%.\n    if self.stack:\n      self.previous_stack_top = self.stack[-1]\n    else:\n      self.previous_stack_top = None\n\n    # Update pp_stack\n    self.UpdatePreprocessor(line)\n\n    # Count parentheses.  This is to avoid adding struct arguments to\n    # the nesting stack.\n    if self.stack:\n      inner_block = self.stack[-1]\n      depth_change = line.count(\'(\') - line.count(\')\')\n      inner_block.open_parentheses += depth_change\n\n      # Also check if we are starting or ending an inline assembly block.\n      if inner_block.inline_asm in (_NO_ASM, _END_ASM):\n        if (depth_change != 0 and\n            inner_block.open_parentheses == 1 and\n            _MATCH_ASM.match(line)):\n          # Enter assembly block\n          inner_block.inline_asm = _INSIDE_ASM\n        else:\n          # Not entering assembly block.  If previous line was _END_ASM,\n          # we will now shift to _NO_ASM state.\n          inner_block.inline_asm = _NO_ASM\n      elif (inner_block.inline_asm == _INSIDE_ASM and\n            inner_block.open_parentheses == 0):\n        # Exit assembly block\n        inner_block.inline_asm = _END_ASM\n\n    # Consume namespace declaration at the beginning of the line.  Do\n    # this in a loop so that we catch same line declarations like this:\n    #   namespace proto2 { namespace bridge { class MessageSet; } }\n    while True:\n      # Match start of namespace.  The ""\\b\\s*"" below catches namespace\n      # declarations even if it weren\'t followed by a whitespace, this\n      # is so that we don\'t confuse our namespace checker.  The\n      # missing spaces will be flagged by CheckSpacing.\n      namespace_decl_match = Match(r\'^\\s*namespace\\b\\s*([:\\w]+)?(.*)$\', line)\n      if not namespace_decl_match:\n        break\n\n      new_namespace = _NamespaceInfo(namespace_decl_match.group(1), linenum)\n      self.stack.append(new_namespace)\n\n      line = namespace_decl_match.group(2)\n      if line.find(\'{\') != -1:\n        new_namespace.seen_open_brace = True\n        line = line[line.find(\'{\') + 1:]\n\n    # Look for a class declaration in whatever is left of the line\n    # after parsing namespaces.  The regexp accounts for decorated classes\n    # such as in:\n    #   class LOCKABLE API Object {\n    #   };\n    class_decl_match = Match(\n        r\'^(\\s*(?:template\\s*<[\\w\\s<>,:]*>\\s*)?\'\n        r\'(class|struct)\\s+(?:[A-Z_]+\\s+)*(\\w+(?:::\\w+)*))\'\n        r\'(.*)$\', line)\n    if (class_decl_match and\n        (not self.stack or self.stack[-1].open_parentheses == 0)):\n      # We do not want to accept classes that are actually template arguments:\n      #   template <class Ignore1,\n      #             class Ignore2 = Default<Args>,\n      #             template <Args> class Ignore3>\n      #   void Function() {};\n      #\n      # To avoid template argument cases, we scan forward and look for\n      # an unmatched \'>\'.  If we see one, assume we are inside a\n      # template argument list.\n      end_declaration = len(class_decl_match.group(1))\n      if not self.InTemplateArgumentList(clean_lines, linenum, end_declaration):\n        self.stack.append(_ClassInfo(\n            class_decl_match.group(3), class_decl_match.group(2),\n            clean_lines, linenum))\n        line = class_decl_match.group(4)\n\n    # If we have not yet seen the opening brace for the innermost block,\n    # run checks here.\n    if not self.SeenOpenBrace():\n      self.stack[-1].CheckBegin(filename, clean_lines, linenum, error)\n\n    # Update access control if we are inside a class/struct\n    if self.stack and isinstance(self.stack[-1], _ClassInfo):\n      classinfo = self.stack[-1]\n      access_match = Match(\n          r\'^(.*)\\b(public|private|protected|signals)(\\s+(?:slots\\s*)?)?\'\n          r\':(?:[^:]|$)\',\n          line)\n      if access_match:\n        classinfo.access = access_match.group(2)\n\n        # Check that access keywords are indented +1 space.  Skip this\n        # check if the keywords are not preceded by whitespaces.\n        indent = access_match.group(1)\n        if (len(indent) != classinfo.class_indent + 1 and\n            Match(r\'^\\s*$\', indent)):\n          if classinfo.is_struct:\n            parent = \'struct \' + classinfo.name\n          else:\n            parent = \'class \' + classinfo.name\n          slots = \'\'\n          if access_match.group(3):\n            slots = access_match.group(3)\n          error(filename, linenum, \'whitespace/indent\', 3,\n                \'%s%s: should be indented +1 space inside %s\' % (\n                    access_match.group(2), slots, parent))\n\n    # Consume braces or semicolons from what\'s left of the line\n    while True:\n      # Match first brace, semicolon, or closed parenthesis.\n      matched = Match(r\'^[^{;)}]*([{;)}])(.*)$\', line)\n      if not matched:\n        break\n\n      token = matched.group(1)\n      if token == \'{\':\n        # If namespace or class hasn\'t seen a opening brace yet, mark\n        # namespace/class head as complete.  Push a new block onto the\n        # stack otherwise.\n        if not self.SeenOpenBrace():\n          self.stack[-1].seen_open_brace = True\n        elif Match(r\'^extern\\s*""[^""]*""\\s*\\{\', line):\n          self.stack.append(_ExternCInfo())\n        else:\n          self.stack.append(_BlockInfo(True))\n          if _MATCH_ASM.match(line):\n            self.stack[-1].inline_asm = _BLOCK_ASM\n\n      elif token == \';\' or token == \')\':\n        # If we haven\'t seen an opening brace yet, but we already saw\n        # a semicolon, this is probably a forward declaration.  Pop\n        # the stack for these.\n        #\n        # Similarly, if we haven\'t seen an opening brace yet, but we\n        # already saw a closing parenthesis, then these are probably\n        # function arguments with extra ""class"" or ""struct"" keywords.\n        # Also pop these stack for these.\n        if not self.SeenOpenBrace():\n          self.stack.pop()\n      else:  # token == \'}\'\n        # Perform end of block checks and pop the stack.\n        if self.stack:\n          self.stack[-1].CheckEnd(filename, clean_lines, linenum, error)\n          self.stack.pop()\n      line = matched.group(2)\n\n  def InnermostClass(self):\n    """"""Get class info on the top of the stack.\n\n    Returns:\n      A _ClassInfo object if we are inside a class, or None otherwise.\n    """"""\n    for i in range(len(self.stack), 0, -1):\n      classinfo = self.stack[i - 1]\n      if isinstance(classinfo, _ClassInfo):\n        return classinfo\n    return None\n\n  def CheckCompletedBlocks(self, filename, error):\n    """"""Checks that all classes and namespaces have been completely parsed.\n\n    Call this when all lines in a file have been processed.\n    Args:\n      filename: The name of the current file.\n      error: The function to call with any errors found.\n    """"""\n    # Note: This test can result in false positives if #ifdef constructs\n    # get in the way of brace matching. See the testBuildClass test in\n    # cpplint_unittest.py for an example of this.\n    for obj in self.stack:\n      if isinstance(obj, _ClassInfo):\n        error(filename, obj.starting_linenum, \'build/class\', 5,\n              \'Failed to find complete declaration of class %s\' %\n              obj.name)\n      elif isinstance(obj, _NamespaceInfo):\n        error(filename, obj.starting_linenum, \'build/namespaces\', 5,\n              \'Failed to find complete declaration of namespace %s\' %\n              obj.name)\n\n\ndef CheckForNonStandardConstructs(filename, clean_lines, linenum,\n                                  nesting_state, error):\n  r""""""Logs an error if we see certain non-ANSI constructs ignored by gcc-2.\n\n  Complain about several constructs which gcc-2 accepts, but which are\n  not standard C++.  Warning about these in lint is one way to ease the\n  transition to new compilers.\n  - put storage class first (e.g. ""static const"" instead of ""const static"").\n  - ""%lld"" instead of %qd"" in printf-type functions.\n  - ""%1$d"" is non-standard in printf-type functions.\n  - ""\\%"" is an undefined character escape sequence.\n  - text after #endif is not allowed.\n  - invalid inner-style forward declaration.\n  - >? and <? operators, and their >?= and <?= cousins.\n\n  Additionally, check for constructor/destructor style violations and reference\n  members, as it is very convenient to do so while checking for\n  gcc-2 compliance.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: A callable to which errors are reported, which takes 4 arguments:\n           filename, line number, error level, and message\n  """"""\n\n  # Remove comments from the line, but leave in strings for now.\n  line = clean_lines.lines[linenum]\n\n  if Search(r\'printf\\s*\\(.*"".*%[-+ ]?\\d*q\', line):\n    error(filename, linenum, \'runtime/printf_format\', 3,\n          \'%q in format strings is deprecated.  Use %ll instead.\')\n\n  if Search(r\'printf\\s*\\(.*"".*%\\d+\\$\', line):\n    error(filename, linenum, \'runtime/printf_format\', 2,\n          \'%N$ formats are unconventional.  Try rewriting to avoid them.\')\n\n  # Remove escaped backslashes before looking for undefined escapes.\n  line = line.replace(\'\\\\\\\\\', \'\')\n\n  if Search(r\'(""|\\\').*\\\\(%|\\[|\\(|{)\', line):\n    error(filename, linenum, \'build/printf_format\', 3,\n          \'%, [, (, and { are undefined character escapes.  Unescape them.\')\n\n  # For the rest, work with both comments and strings removed.\n  line = clean_lines.elided[linenum]\n\n  if Search(r\'\\b(const|volatile|void|char|short|int|long\'\n            r\'|float|double|signed|unsigned\'\n            r\'|schar|u?int8|u?int16|u?int32|u?int64)\'\n            r\'\\s+(register|static|extern|typedef)\\b\',\n            line):\n    error(filename, linenum, \'build/storage_class\', 5,\n          \'Storage class (static, extern, typedef, etc) should be first.\')\n\n  if Match(r\'\\s*#\\s*endif\\s*[^/\\s]+\', line):\n    error(filename, linenum, \'build/endif_comment\', 5,\n          \'Uncommented text after #endif is non-standard.  Use a comment.\')\n\n  if Match(r\'\\s*class\\s+(\\w+\\s*::\\s*)+\\w+\\s*;\', line):\n    error(filename, linenum, \'build/forward_decl\', 5,\n          \'Inner-style forward declarations are invalid.  Remove this line.\')\n\n  if Search(r\'(\\w+|[+-]?\\d+(\\.\\d*)?)\\s*(<|>)\\?=?\\s*(\\w+|[+-]?\\d+)(\\.\\d*)?\',\n            line):\n    error(filename, linenum, \'build/deprecated\', 3,\n          \'>? and <? (max and min) operators are non-standard and deprecated.\')\n\n  if Search(r\'^\\s*const\\s*string\\s*&\\s*\\w+\\s*;\', line):\n    # TODO(unknown): Could it be expanded safely to arbitrary references,\n    # without triggering too many false positives? The first\n    # attempt triggered 5 warnings for mostly benign code in the regtest, hence\n    # the restriction.\n    # Here\'s the original regexp, for the reference:\n    # type_name = r\'\\w+((\\s*::\\s*\\w+)|(\\s*<\\s*\\w+?\\s*>))?\'\n    # r\'\\s*const\\s*\' + type_name + \'\\s*&\\s*\\w+\\s*;\'\n    error(filename, linenum, \'runtime/member_string_references\', 2,\n          \'const string& members are dangerous. It is much better to use \'\n          \'alternatives, such as pointers or simple constants.\')\n\n  # Everything else in this function operates on class declarations.\n  # Return early if the top of the nesting stack is not a class, or if\n  # the class head is not completed yet.\n  classinfo = nesting_state.InnermostClass()\n  if not classinfo or not classinfo.seen_open_brace:\n    return\n\n  # The class may have been declared with namespace or classname qualifiers.\n  # The constructor and destructor will not have those qualifiers.\n  base_classname = classinfo.name.split(\'::\')[-1]\n\n  # Look for single-argument constructors that aren\'t marked explicit.\n  # Technically a valid construct, but against style. Also look for\n  # non-single-argument constructors which are also technically valid, but\n  # strongly suggest something is wrong.\n  explicit_constructor_match = Match(\n      r\'\\s+(?:inline\\s+)?(explicit\\s+)?(?:inline\\s+)?%s\\s*\'\n      r\'\\(((?:[^()]|\\([^()]*\\))*)\\)\'\n      % re.escape(base_classname),\n      line)\n\n  if explicit_constructor_match:\n    is_marked_explicit = explicit_constructor_match.group(1)\n\n    if not explicit_constructor_match.group(2):\n      constructor_args = []\n    else:\n      constructor_args = explicit_constructor_match.group(2).split(\',\')\n\n    # collapse arguments so that commas in template parameter lists and function\n    # argument parameter lists don\'t split arguments in two\n    i = 0\n    while i < len(constructor_args):\n      constructor_arg = constructor_args[i]\n      while (constructor_arg.count(\'<\') > constructor_arg.count(\'>\') or\n             constructor_arg.count(\'(\') > constructor_arg.count(\')\')):\n        constructor_arg += \',\' + constructor_args[i + 1]\n        del constructor_args[i + 1]\n      constructor_args[i] = constructor_arg\n      i += 1\n\n    defaulted_args = [arg for arg in constructor_args if \'=\' in arg]\n    noarg_constructor = (not constructor_args or  # empty arg list\n                         # \'void\' arg specifier\n                         (len(constructor_args) == 1 and\n                          constructor_args[0].strip() == \'void\'))\n    onearg_constructor = ((len(constructor_args) == 1 and  # exactly one arg\n                           not noarg_constructor) or\n                          # all but at most one arg defaulted\n                          (len(constructor_args) >= 1 and\n                           not noarg_constructor and\n                           len(defaulted_args) >= len(constructor_args) - 1))\n    initializer_list_constructor = bool(\n        onearg_constructor and\n        Search(r\'\\bstd\\s*::\\s*initializer_list\\b\', constructor_args[0]))\n    copy_constructor = bool(\n        onearg_constructor and\n        Match(r\'(const\\s+)?%s(\\s*<[^>]*>)?(\\s+const)?\\s*(?:<\\w+>\\s*)?&\'\n              % re.escape(base_classname), constructor_args[0].strip()))\n\n    if (not is_marked_explicit and\n        onearg_constructor and\n        not initializer_list_constructor and\n        not copy_constructor):\n      if defaulted_args:\n        error(filename, linenum, \'runtime/explicit\', 5,\n              \'Constructors callable with one argument \'\n              \'should be marked explicit.\')\n      else:\n        error(filename, linenum, \'runtime/explicit\', 5,\n              \'Single-parameter constructors should be marked explicit.\')\n    elif is_marked_explicit and not onearg_constructor:\n      if noarg_constructor:\n        error(filename, linenum, \'runtime/explicit\', 5,\n              \'Zero-parameter constructors should not be marked explicit.\')\n      else:\n        error(filename, linenum, \'runtime/explicit\', 0,\n              \'Constructors that require multiple arguments \'\n              \'should not be marked explicit.\')\n\n\ndef CheckSpacingForFunctionCall(filename, clean_lines, linenum, error):\n  """"""Checks for the correctness of various spacing around function calls.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n\n  # Since function calls often occur inside if/for/while/switch\n  # expressions - which have their own, more liberal conventions - we\n  # first see if we should be looking inside such an expression for a\n  # function call, to which we can apply more strict standards.\n  fncall = line    # if there\'s no control flow construct, look at whole line\n  for pattern in (r\'\\bif\\s*\\((.*)\\)\\s*{\',\n                  r\'\\bfor\\s*\\((.*)\\)\\s*{\',\n                  r\'\\bwhile\\s*\\((.*)\\)\\s*[{;]\',\n                  r\'\\bswitch\\s*\\((.*)\\)\\s*{\'):\n    match = Search(pattern, line)\n    if match:\n      fncall = match.group(1)    # look inside the parens for function calls\n      break\n\n  # Except in if/for/while/switch, there should never be space\n  # immediately inside parens (eg ""f( 3, 4 )"").  We make an exception\n  # for nested parens ( (a+b) + c ).  Likewise, there should never be\n  # a space before a ( when it\'s a function argument.  I assume it\'s a\n  # function argument when the char before the whitespace is legal in\n  # a function name (alnum + _) and we\'re not starting a macro. Also ignore\n  # pointers and references to arrays and functions coz they\'re too tricky:\n  # we use a very simple way to recognize these:\n  # "" (something)(maybe-something)"" or\n  # "" (something)(maybe-something,"" or\n  # "" (something)[something]""\n  # Note that we assume the contents of [] to be short enough that\n  # they\'ll never need to wrap.\n  if (  # Ignore control structures.\n      not Search(r\'\\b(if|for|while|switch|return|new|delete|catch|sizeof)\\b\',\n                 fncall) and\n      # Ignore pointers/references to functions.\n      not Search(r\' \\([^)]+\\)\\([^)]*(\\)|,$)\', fncall) and\n      # Ignore pointers/references to arrays.\n      not Search(r\' \\([^)]+\\)\\[[^\\]]+\\]\', fncall)):\n    if Search(r\'\\w\\s*\\(\\s(?!\\s*\\\\$)\', fncall):      # a ( used for a fn call\n      error(filename, linenum, \'whitespace/parens\', 4,\n            \'Extra space after ( in function call\')\n    elif Search(r\'\\(\\s+(?!(\\s*\\\\)|\\()\', fncall):\n      error(filename, linenum, \'whitespace/parens\', 2,\n            \'Extra space after (\')\n    if (Search(r\'\\w\\s+\\(\', fncall) and\n        not Search(r\'#\\s*define|typedef|using\\s+\\w+\\s*=\', fncall) and\n        not Search(r\'\\w\\s+\\((\\w+::)*\\*\\w+\\)\\(\', fncall) and\n        not Search(r\'\\bcase\\s+\\(\', fncall)):\n      # TODO(unknown): Space after an operator function seem to be a common\n      # error, silence those for now by restricting them to highest verbosity.\n      if Search(r\'\\boperator_*\\b\', line):\n        error(filename, linenum, \'whitespace/parens\', 0,\n              \'Extra space before ( in function call\')\n      else:\n        error(filename, linenum, \'whitespace/parens\', 4,\n              \'Extra space before ( in function call\')\n    # If the ) is followed only by a newline or a { + newline, assume it\'s\n    # part of a control statement (if/while/etc), and don\'t complain\n    if Search(r\'[^)]\\s+\\)\\s*[^{\\s]\', fncall):\n      # If the closing parenthesis is preceded by only whitespaces,\n      # try to give a more descriptive error message.\n      if Search(r\'^\\s+\\)\', fncall):\n        error(filename, linenum, \'whitespace/parens\', 2,\n              \'Closing ) should be moved to the previous line\')\n      else:\n        error(filename, linenum, \'whitespace/parens\', 2,\n              \'Extra space before )\')\n\n\ndef IsBlankLine(line):\n  """"""Returns true if the given line is blank.\n\n  We consider a line to be blank if the line is empty or consists of\n  only white spaces.\n\n  Args:\n    line: A line of a string.\n\n  Returns:\n    True, if the given line is blank.\n  """"""\n  return not line or line.isspace()\n\n\ndef CheckForNamespaceIndentation(filename, nesting_state, clean_lines, line,\n                                 error):\n  is_namespace_indent_item = (\n      len(nesting_state.stack) > 1 and\n      nesting_state.stack[-1].check_namespace_indentation and\n      isinstance(nesting_state.previous_stack_top, _NamespaceInfo) and\n      nesting_state.previous_stack_top == nesting_state.stack[-2])\n\n  if ShouldCheckNamespaceIndentation(nesting_state, is_namespace_indent_item,\n                                     clean_lines.elided, line):\n    CheckItemIndentationInNamespace(filename, clean_lines.elided,\n                                    line, error)\n\n\ndef CheckForFunctionLengths(filename, clean_lines, linenum,\n                            function_state, error):\n  """"""Reports for long function bodies.\n\n  For an overview why this is done, see:\n  http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml#Write_Short_Functions\n\n  Uses a simplistic algorithm assuming other style guidelines\n  (especially spacing) are followed.\n  Only checks unindented functions, so class members are unchecked.\n  Trivial bodies are unchecked, so constructors with huge initializer lists\n  may be missed.\n  Blank/comment lines are not counted so as to avoid encouraging the removal\n  of vertical space and comments just to get through a lint check.\n  NOLINT *on the last line of a function* disables this check.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    function_state: Current function name and lines in body so far.\n    error: The function to call with any errors found.\n  """"""\n  lines = clean_lines.lines\n  line = lines[linenum]\n  joined_line = \'\'\n\n  starting_func = False\n  regexp = r\'(\\w(\\w|::|\\*|\\&|\\s)*)\\(\'  # decls * & space::name( ...\n  match_result = Match(regexp, line)\n  if match_result:\n    # If the name is all caps and underscores, figure it\'s a macro and\n    # ignore it, unless it\'s TEST or TEST_F.\n    function_name = match_result.group(1).split()[-1]\n    if function_name == \'TEST\' or function_name == \'TEST_F\' or (\n        not Match(r\'[A-Z_]+$\', function_name)):\n      starting_func = True\n\n  if starting_func:\n    body_found = False\n    for start_linenum in xrange(linenum, clean_lines.NumLines()):\n      start_line = lines[start_linenum]\n      joined_line += \' \' + start_line.lstrip()\n      if Search(r\'(;|})\', start_line):  # Declarations and trivial functions\n        body_found = True\n        break                              # ... ignore\n      elif Search(r\'{\', start_line):\n        body_found = True\n        function = Search(r\'((\\w|:)*)\\(\', line).group(1)\n        if Match(r\'TEST\', function):    # Handle TEST... macros\n          parameter_regexp = Search(r\'(\\(.*\\))\', joined_line)\n          if parameter_regexp:             # Ignore bad syntax\n            function += parameter_regexp.group(1)\n        else:\n          function += \'()\'\n        function_state.Begin(function)\n        break\n    if not body_found:\n      # No body for the function (or evidence of a non-function) was found.\n      error(filename, linenum, \'readability/fn_size\', 5,\n            \'Lint failed to find start of function body.\')\n  elif Match(r\'^\\}\\s*$\', line):  # function end\n    function_state.Check(error, filename, linenum)\n    function_state.End()\n  elif not Match(r\'^\\s*$\', line):\n    function_state.Count()  # Count non-blank/non-comment lines.\n\n\n_RE_PATTERN_TODO = re.compile(r\'^//(\\s*)TODO(\\(.+?\\))?:?(\\s|$)?\')\n\n\ndef CheckComment(line, filename, linenum, next_line_start, error):\n  """"""Checks for common mistakes in comments.\n\n  Args:\n    line: The line in question.\n    filename: The name of the current file.\n    linenum: The number of the line to check.\n    next_line_start: The first non-whitespace column of the next line.\n    error: The function to call with any errors found.\n  """"""\n  commentpos = line.find(\'//\')\n  if commentpos != -1:\n    # Check if the // may be in quotes.  If so, ignore it\n    # Comparisons made explicit for clarity -- pylint: disable=g-explicit-bool-comparison\n    if (line.count(\'""\', 0, commentpos) -\n        line.count(\'\\\\""\', 0, commentpos)) % 2 == 0:   # not in quotes\n      # Allow one space for new scopes, two spaces otherwise:\n      if (not (Match(r\'^.*{ *//\', line) and next_line_start == commentpos) and\n          ((commentpos >= 1 and\n            line[commentpos-1] not in string.whitespace) or\n           (commentpos >= 2 and\n            line[commentpos-2] not in string.whitespace))):\n        error(filename, linenum, \'whitespace/comments\', 2,\n              \'At least two spaces is best between code and comments\')\n\n      # Checks for common mistakes in TODO comments.\n      comment = line[commentpos:]\n      match = _RE_PATTERN_TODO.match(comment)\n      if match:\n        # One whitespace is correct; zero whitespace is handled elsewhere.\n        leading_whitespace = match.group(1)\n        if len(leading_whitespace) > 1:\n          error(filename, linenum, \'whitespace/todo\', 2,\n                \'Too many spaces before TODO\')\n\n        username = match.group(2)\n        if not username:\n          error(filename, linenum, \'readability/todo\', 2,\n                \'Missing username in TODO; it should look like \'\n                \'""// TODO(my_username): Stuff.""\')\n\n        middle_whitespace = match.group(3)\n        # Comparisons made explicit for correctness -- pylint: disable=g-explicit-bool-comparison\n        if middle_whitespace != \' \' and middle_whitespace != \'\':\n          error(filename, linenum, \'whitespace/todo\', 2,\n                \'TODO(my_username) should be followed by a space\')\n\n      # If the comment contains an alphanumeric character, there\n      # should be a space somewhere between it and the // unless\n      # it\'s a /// or //! Doxygen comment.\n      if (Match(r\'//[^ ]*\\w\', comment) and\n          not Match(r\'(///|//\\!)(\\s+|$)\', comment)):\n        error(filename, linenum, \'whitespace/comments\', 4,\n              \'Should have a space between // and comment\')\n\n\ndef CheckAccess(filename, clean_lines, linenum, nesting_state, error):\n  """"""Checks for improper use of DISALLOW* macros.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]  # get rid of comments and strings\n\n  matched = Match((r\'\\s*(DISALLOW_COPY_AND_ASSIGN|\'\n                   r\'DISALLOW_IMPLICIT_CONSTRUCTORS)\'), line)\n  if not matched:\n    return\n  if nesting_state.stack and isinstance(nesting_state.stack[-1], _ClassInfo):\n    if nesting_state.stack[-1].access != \'private\':\n      error(filename, linenum, \'readability/constructors\', 3,\n            \'%s must be in the private: section\' % matched.group(1))\n\n  else:\n    # Found DISALLOW* macro outside a class declaration, or perhaps it\n    # was used inside a function when it should have been part of the\n    # class declaration.  We could issue a warning here, but it\n    # probably resulted in a compiler error already.\n    pass\n\n\ndef CheckSpacing(filename, clean_lines, linenum, nesting_state, error):\n  """"""Checks for the correctness of various spacing issues in the code.\n\n  Things we check for: spaces around operators, spaces after\n  if/for/while/switch, no spaces around parens in function calls, two\n  spaces between code and comment, don\'t start a block with a blank\n  line, don\'t end a function with a blank line, don\'t add a blank line\n  after public/protected/private, don\'t have too many blank lines in a row.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  """"""\n\n  # Don\'t use ""elided"" lines here, otherwise we can\'t check commented lines.\n  # Don\'t want to use ""raw"" either, because we don\'t want to check inside C++11\n  # raw strings,\n  raw = clean_lines.lines_without_raw_strings\n  line = raw[linenum]\n\n  # Before nixing comments, check if the line is blank for no good\n  # reason.  This includes the first line after a block is opened, and\n  # blank lines at the end of a function (ie, right before a line like \'}\'\n  #\n  # Skip all the blank line checks if we are immediately inside a\n  # namespace body.  In other words, don\'t issue blank line warnings\n  # for this block:\n  #   namespace {\n  #\n  #   }\n  #\n  # A warning about missing end of namespace comments will be issued instead.\n  #\n  # Also skip blank line checks for \'extern ""C""\' blocks, which are formatted\n  # like namespaces.\n  if (IsBlankLine(line) and\n      not nesting_state.InNamespaceBody() and\n      not nesting_state.InExternC()):\n    elided = clean_lines.elided\n    prev_line = elided[linenum - 1]\n    prevbrace = prev_line.rfind(\'{\')\n    # TODO(unknown): Don\'t complain if line before blank line, and line after,\n    #                both start with alnums and are indented the same amount.\n    #                This ignores whitespace at the start of a namespace block\n    #                because those are not usually indented.\n    if prevbrace != -1 and prev_line[prevbrace:].find(\'}\') == -1:\n      # OK, we have a blank line at the start of a code block.  Before we\n      # complain, we check if it is an exception to the rule: The previous\n      # non-empty line has the parameters of a function header that are indented\n      # 4 spaces (because they did not fit in a 80 column line when placed on\n      # the same line as the function name).  We also check for the case where\n      # the previous line is indented 6 spaces, which may happen when the\n      # initializers of a constructor do not fit into a 80 column line.\n      exception = False\n      if Match(r\' {6}\\w\', prev_line):  # Initializer list?\n        # We are looking for the opening column of initializer list, which\n        # should be indented 4 spaces to cause 6 space indentation afterwards.\n        search_position = linenum-2\n        while (search_position >= 0\n               and Match(r\' {6}\\w\', elided[search_position])):\n          search_position -= 1\n        exception = (search_position >= 0\n                     and elided[search_position][:5] == \'    :\')\n      else:\n        # Search for the function arguments or an initializer list.  We use a\n        # simple heuristic here: If the line is indented 4 spaces; and we have a\n        # closing paren, without the opening paren, followed by an opening brace\n        # or colon (for initializer lists) we assume that it is the last line of\n        # a function header.  If we have a colon indented 4 spaces, it is an\n        # initializer list.\n        exception = (Match(r\' {4}\\w[^\\(]*\\)\\s*(const\\s*)?(\\{\\s*$|:)\',\n                           prev_line)\n                     or Match(r\' {4}:\', prev_line))\n\n      if not exception:\n        error(filename, linenum, \'whitespace/blank_line\', 2,\n              \'Redundant blank line at the start of a code block \'\n              \'should be deleted.\')\n    # Ignore blank lines at the end of a block in a long if-else\n    # chain, like this:\n    #   if (condition1) {\n    #     // Something followed by a blank line\n    #\n    #   } else if (condition2) {\n    #     // Something else\n    #   }\n    if linenum + 1 < clean_lines.NumLines():\n      next_line = raw[linenum + 1]\n      if (next_line\n          and Match(r\'\\s*}\', next_line)\n          and next_line.find(\'} else \') == -1):\n        error(filename, linenum, \'whitespace/blank_line\', 3,\n              \'Redundant blank line at the end of a code block \'\n              \'should be deleted.\')\n\n    matched = Match(r\'\\s*(public|protected|private):\', prev_line)\n    if matched:\n      error(filename, linenum, \'whitespace/blank_line\', 3,\n            \'Do not leave a blank line after ""%s:""\' % matched.group(1))\n\n  # Next, check comments\n  next_line_start = 0\n  if linenum + 1 < clean_lines.NumLines():\n    next_line = raw[linenum + 1]\n    next_line_start = len(next_line) - len(next_line.lstrip())\n  CheckComment(line, filename, linenum, next_line_start, error)\n\n  # get rid of comments and strings\n  line = clean_lines.elided[linenum]\n\n  # You shouldn\'t have spaces before your brackets, except maybe after\n  # \'delete []\' or \'return []() {};\'\n  if Search(r\'\\w\\s+\\[\', line) and not Search(r\'(?:delete|return)\\s+\\[\', line):\n    error(filename, linenum, \'whitespace/braces\', 5,\n          \'Extra space before [\')\n\n  # In range-based for, we wanted spaces before and after the colon, but\n  # not around ""::"" tokens that might appear.\n  if (Search(r\'for *\\(.*[^:]:[^: ]\', line) or\n      Search(r\'for *\\(.*[^: ]:[^:]\', line)):\n    error(filename, linenum, \'whitespace/forcolon\', 2,\n          \'Missing space around colon in range-based for loop\')\n\n\ndef CheckOperatorSpacing(filename, clean_lines, linenum, error):\n  """"""Checks for horizontal spacing around operators.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n\n  # Don\'t try to do spacing checks for operator methods.  Do this by\n  # replacing the troublesome characters with something else,\n  # preserving column position for all other characters.\n  #\n  # The replacement is done repeatedly to avoid false positives from\n  # operators that call operators.\n  while True:\n    match = Match(r\'^(.*\\boperator\\b)(\\S+)(\\s*\\(.*)$\', line)\n    if match:\n      line = match.group(1) + (\'_\' * len(match.group(2))) + match.group(3)\n    else:\n      break\n\n  # We allow no-spaces around = within an if: ""if ( (a=Foo()) == 0 )"".\n  # Otherwise not.  Note we only check for non-spaces on *both* sides;\n  # sometimes people put non-spaces on one side when aligning =\'s among\n  # many lines (not that this is behavior that I approve of...)\n  if ((Search(r\'[\\w.]=\', line) or\n       Search(r\'=[\\w.]\', line))\n      and not Search(r\'\\b(if|while|for) \', line)\n      # Operators taken from [lex.operators] in C++11 standard.\n      and not Search(r\'(>=|<=|==|!=|&=|\\^=|\\|=|\\+=|\\*=|\\/=|\\%=)\', line)\n      and not Search(r\'operator=\', line)):\n    error(filename, linenum, \'whitespace/operators\', 4,\n          \'Missing spaces around =\')\n\n  # It\'s ok not to have spaces around binary operators like + - * /, but if\n  # there\'s too little whitespace, we get concerned.  It\'s hard to tell,\n  # though, so we punt on this one for now.  TODO.\n\n  # You should always have whitespace around binary operators.\n  #\n  # Check <= and >= first to avoid false positives with < and >, then\n  # check non-include lines for spacing around < and >.\n  #\n  # If the operator is followed by a comma, assume it\'s be used in a\n  # macro context and don\'t do any checks.  This avoids false\n  # positives.\n  #\n  # Note that && is not included here.  Those are checked separately\n  # in CheckRValueReference\n  match = Search(r\'[^<>=!\\s](==|!=|<=|>=|\\|\\|)[^<>=!\\s,;\\)]\', line)\n  if match:\n    error(filename, linenum, \'whitespace/operators\', 3,\n          \'Missing spaces around %s\' % match.group(1))\n  elif not Match(r\'#.*include\', line):\n    # Look for < that is not surrounded by spaces.  This is only\n    # triggered if both sides are missing spaces, even though\n    # technically should should flag if at least one side is missing a\n    # space.  This is done to avoid some false positives with shifts.\n    match = Match(r\'^(.*[^\\s<])<[^\\s=<,]\', line)\n    if match:\n      (_, _, end_pos) = CloseExpression(\n          clean_lines, linenum, len(match.group(1)))\n      if end_pos <= -1:\n        error(filename, linenum, \'whitespace/operators\', 3,\n              \'Missing spaces around <\')\n\n    # Look for > that is not surrounded by spaces.  Similar to the\n    # above, we only trigger if both sides are missing spaces to avoid\n    # false positives with shifts.\n    match = Match(r\'^(.*[^-\\s>])>[^\\s=>,]\', line)\n    if match:\n      (_, _, start_pos) = ReverseCloseExpression(\n          clean_lines, linenum, len(match.group(1)))\n      if start_pos <= -1:\n        error(filename, linenum, \'whitespace/operators\', 3,\n              \'Missing spaces around >\')\n\n  # We allow no-spaces around << when used like this: 10<<20, but\n  # not otherwise (particularly, not when used as streams)\n  #\n  # We also allow operators following an opening parenthesis, since\n  # those tend to be macros that deal with operators.\n  match = Search(r\'(operator|[^\\s(<])(?:L|UL|ULL|l|ul|ull)?<<([^\\s,=<])\', line)\n  if (match and not (match.group(1).isdigit() and match.group(2).isdigit()) and\n      not (match.group(1) == \'operator\' and match.group(2) == \';\')):\n    error(filename, linenum, \'whitespace/operators\', 3,\n          \'Missing spaces around <<\')\n\n  # We allow no-spaces around >> for almost anything.  This is because\n  # C++11 allows "">>"" to close nested templates, which accounts for\n  # most cases when "">>"" is not followed by a space.\n  #\n  # We still warn on "">>"" followed by alpha character, because that is\n  # likely due to "">>"" being used for right shifts, e.g.:\n  #   value >> alpha\n  #\n  # When "">>"" is used to close templates, the alphanumeric letter that\n  # follows would be part of an identifier, and there should still be\n  # a space separating the template type and the identifier.\n  #   type<type<type>> alpha\n  match = Search(r\'>>[a-zA-Z_]\', line)\n  if match:\n    error(filename, linenum, \'whitespace/operators\', 3,\n          \'Missing spaces around >>\')\n\n  # There shouldn\'t be space around unary operators\n  match = Search(r\'(!\\s|~\\s|[\\s]--[\\s;]|[\\s]\\+\\+[\\s;])\', line)\n  if match:\n    error(filename, linenum, \'whitespace/operators\', 4,\n          \'Extra space for operator %s\' % match.group(1))\n\n\ndef CheckParenthesisSpacing(filename, clean_lines, linenum, error):\n  """"""Checks for horizontal spacing around parentheses.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n\n  # No spaces after an if, while, switch, or for\n  match = Search(r\' (if\\(|for\\(|while\\(|switch\\()\', line)\n  if match:\n    error(filename, linenum, \'whitespace/parens\', 5,\n          \'Missing space before ( in %s\' % match.group(1))\n\n  # For if/for/while/switch, the left and right parens should be\n  # consistent about how many spaces are inside the parens, and\n  # there should either be zero or one spaces inside the parens.\n  # We don\'t want: ""if ( foo)"" or ""if ( foo   )"".\n  # Exception: ""for ( ; foo; bar)"" and ""for (foo; bar; )"" are allowed.\n  match = Search(r\'\\b(if|for|while|switch)\\s*\'\n                 r\'\\(([ ]*)(.).*[^ ]+([ ]*)\\)\\s*{\\s*$\',\n                 line)\n  if match:\n    if len(match.group(2)) != len(match.group(4)):\n      if not (match.group(3) == \';\' and\n              len(match.group(2)) == 1 + len(match.group(4)) or\n              not match.group(2) and Search(r\'\\bfor\\s*\\(.*; \\)\', line)):\n        error(filename, linenum, \'whitespace/parens\', 5,\n              \'Mismatching spaces inside () in %s\' % match.group(1))\n    if len(match.group(2)) not in [0, 1]:\n      error(filename, linenum, \'whitespace/parens\', 5,\n            \'Should have zero or one spaces inside ( and ) in %s\' %\n            match.group(1))\n\n\ndef CheckCommaSpacing(filename, clean_lines, linenum, error):\n  """"""Checks for horizontal spacing near commas and semicolons.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  raw = clean_lines.lines_without_raw_strings\n  line = clean_lines.elided[linenum]\n\n  # You should always have a space after a comma (either as fn arg or operator)\n  #\n  # This does not apply when the non-space character following the\n  # comma is another comma, since the only time when that happens is\n  # for empty macro arguments.\n  #\n  # We run this check in two passes: first pass on elided lines to\n  # verify that lines contain missing whitespaces, second pass on raw\n  # lines to confirm that those missing whitespaces are not due to\n  # elided comments.\n  if (Search(r\',[^,\\s]\', ReplaceAll(r\'\\boperator\\s*,\\s*\\(\', \'F(\', line)) and\n      Search(r\',[^,\\s]\', raw[linenum])):\n    error(filename, linenum, \'whitespace/comma\', 3,\n          \'Missing space after ,\')\n\n  # You should always have a space after a semicolon\n  # except for few corner cases\n  # TODO(unknown): clarify if \'if (1) { return 1;}\' is requires one more\n  # space after ;\n  if Search(r\';[^\\s};\\\\)/]\', line):\n    error(filename, linenum, \'whitespace/semicolon\', 3,\n          \'Missing space after ;\')\n\n\ndef CheckBracesSpacing(filename, clean_lines, linenum, error):\n  """"""Checks for horizontal spacing near commas.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n\n  # Except after an opening paren, or after another opening brace (in case of\n  # an initializer list, for instance), you should have spaces before your\n  # braces. And since you should never have braces at the beginning of a line,\n  # this is an easy test.\n  match = Match(r\'^(.*[^ ({>]){\', line)\n  if match:\n    # Try a bit harder to check for brace initialization.  This\n    # happens in one of the following forms:\n    #   Constructor() : initializer_list_{} { ... }\n    #   Constructor{}.MemberFunction()\n    #   Type variable{};\n    #   FunctionCall(type{}, ...);\n    #   LastArgument(..., type{});\n    #   LOG(INFO) << type{} << "" ..."";\n    #   map_of_type[{...}] = ...;\n    #   ternary = expr ? new type{} : nullptr;\n    #   OuterTemplate<InnerTemplateConstructor<Type>{}>\n    #\n    # We check for the character following the closing brace, and\n    # silence the warning if it\'s one of those listed above, i.e.\n    # ""{.;,)<>]:"".\n    #\n    # To account for nested initializer list, we allow any number of\n    # closing braces up to ""{;,)<"".  We can\'t simply silence the\n    # warning on first sight of closing brace, because that would\n    # cause false negatives for things that are not initializer lists.\n    #   Silence this:         But not this:\n    #     Outer{                if (...) {\n    #       Inner{...}            if (...){  // Missing space before {\n    #     };                    }\n    #\n    # There is a false negative with this approach if people inserted\n    # spurious semicolons, e.g. ""if (cond){};"", but we will catch the\n    # spurious semicolon with a separate check.\n    (endline, endlinenum, endpos) = CloseExpression(\n        clean_lines, linenum, len(match.group(1)))\n    trailing_text = \'\'\n    if endpos > -1:\n      trailing_text = endline[endpos:]\n    for offset in xrange(endlinenum + 1,\n                         min(endlinenum + 3, clean_lines.NumLines() - 1)):\n      trailing_text += clean_lines.elided[offset]\n    if not Match(r\'^[\\s}]*[{.;,)<>\\]:]\', trailing_text):\n      error(filename, linenum, \'whitespace/braces\', 5,\n            \'Missing space before {\')\n\n  # Make sure \'} else {\' has spaces.\n  if Search(r\'}else\', line):\n    error(filename, linenum, \'whitespace/braces\', 5,\n          \'Missing space before else\')\n\n  # You shouldn\'t have a space before a semicolon at the end of the line.\n  # There\'s a special case for ""for"" since the style guide allows space before\n  # the semicolon there.\n  if Search(r\':\\s*;\\s*$\', line):\n    error(filename, linenum, \'whitespace/semicolon\', 5,\n          \'Semicolon defining empty statement. Use {} instead.\')\n  elif Search(r\'^\\s*;\\s*$\', line):\n    error(filename, linenum, \'whitespace/semicolon\', 5,\n          \'Line contains only semicolon. If this should be an empty statement, \'\n          \'use {} instead.\')\n  elif (Search(r\'\\s+;\\s*$\', line) and\n        not Search(r\'\\bfor\\b\', line)):\n    error(filename, linenum, \'whitespace/semicolon\', 5,\n          \'Extra space before last semicolon. If this should be an empty \'\n          \'statement, use {} instead.\')\n\n\ndef IsDecltype(clean_lines, linenum, column):\n  """"""Check if the token ending on (linenum, column) is decltype().\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: the number of the line to check.\n    column: end column of the token to check.\n  Returns:\n    True if this token is decltype() expression, False otherwise.\n  """"""\n  (text, _, start_col) = ReverseCloseExpression(clean_lines, linenum, column)\n  if start_col < 0:\n    return False\n  if Search(r\'\\bdecltype\\s*$\', text[0:start_col]):\n    return True\n  return False\n\n\ndef IsTemplateParameterList(clean_lines, linenum, column):\n  """"""Check if the token ending on (linenum, column) is the end of template<>.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: the number of the line to check.\n    column: end column of the token to check.\n  Returns:\n    True if this token is end of a template parameter list, False otherwise.\n  """"""\n  (_, startline, startpos) = ReverseCloseExpression(\n      clean_lines, linenum, column)\n  if (startpos > -1 and\n      Search(r\'\\btemplate\\s*$\', clean_lines.elided[startline][0:startpos])):\n    return True\n  return False\n\n\ndef IsRValueType(typenames, clean_lines, nesting_state, linenum, column):\n  """"""Check if the token ending on (linenum, column) is a type.\n\n  Assumes that text to the right of the column is ""&&"" or a function\n  name.\n\n  Args:\n    typenames: set of type names from template-argument-list.\n    clean_lines: A CleansedLines instance containing the file.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    linenum: the number of the line to check.\n    column: end column of the token to check.\n  Returns:\n    True if this token is a type, False if we are not sure.\n  """"""\n  prefix = clean_lines.elided[linenum][0:column]\n\n  # Get one word to the left.  If we failed to do so, this is most\n  # likely not a type, since it\'s unlikely that the type name and ""&&""\n  # would be split across multiple lines.\n  match = Match(r\'^(.*)(\\b\\w+|[>*)&])\\s*$\', prefix)\n  if not match:\n    return False\n\n  # Check text following the token.  If it\'s ""&&>"" or ""&&,"" or ""&&..."", it\'s\n  # most likely a rvalue reference used inside a template.\n  suffix = clean_lines.elided[linenum][column:]\n  if Match(r\'&&\\s*(?:[>,]|\\.\\.\\.)\', suffix):\n    return True\n\n  # Check for known types and end of templates:\n  #   int&& variable\n  #   vector<int>&& variable\n  #\n  # Because this function is called recursively, we also need to\n  # recognize pointer and reference types:\n  #   int* Function()\n  #   int& Function()\n  if (match.group(2) in typenames or\n      match.group(2) in [\'char\', \'char16_t\', \'char32_t\', \'wchar_t\', \'bool\',\n                         \'short\', \'int\', \'long\', \'signed\', \'unsigned\',\n                         \'float\', \'double\', \'void\', \'auto\', \'>\', \'*\', \'&\']):\n    return True\n\n  # If we see a close parenthesis, look for decltype on the other side.\n  # decltype would unambiguously identify a type, anything else is\n  # probably a parenthesized expression and not a type.\n  if match.group(2) == \')\':\n    return IsDecltype(\n        clean_lines, linenum, len(match.group(1)) + len(match.group(2)) - 1)\n\n  # Check for casts and cv-qualifiers.\n  #   match.group(1)  remainder\n  #   --------------  ---------\n  #   const_cast<     type&&\n  #   const           type&&\n  #   type            const&&\n  if Search(r\'\\b(?:const_cast\\s*<|static_cast\\s*<|dynamic_cast\\s*<|\'\n            r\'reinterpret_cast\\s*<|\\w+\\s)\\s*$\',\n            match.group(1)):\n    return True\n\n  # Look for a preceding symbol that might help differentiate the context.\n  # These are the cases that would be ambiguous:\n  #   match.group(1)  remainder\n  #   --------------  ---------\n  #   Call         (   expression &&\n  #   Declaration  (   type&&\n  #   sizeof       (   type&&\n  #   if           (   expression &&\n  #   while        (   expression &&\n  #   for          (   type&&\n  #   for(         ;   expression &&\n  #   statement    ;   type&&\n  #   block        {   type&&\n  #   constructor  {   expression &&\n  start = linenum\n  line = match.group(1)\n  match_symbol = None\n  while start >= 0:\n    # We want to skip over identifiers and commas to get to a symbol.\n    # Commas are skipped so that we can find the opening parenthesis\n    # for function parameter lists.\n    match_symbol = Match(r\'^(.*)([^\\w\\s,])[\\w\\s,]*$\', line)\n    if match_symbol:\n      break\n    start -= 1\n    line = clean_lines.elided[start]\n\n  if not match_symbol:\n    # Probably the first statement in the file is an rvalue reference\n    return True\n\n  if match_symbol.group(2) == \'}\':\n    # Found closing brace, probably an indicate of this:\n    #   block{} type&&\n    return True\n\n  if match_symbol.group(2) == \';\':\n    # Found semicolon, probably one of these:\n    #   for(; expression &&\n    #   statement; type&&\n\n    # Look for the previous \'for(\' in the previous lines.\n    before_text = match_symbol.group(1)\n    for i in xrange(start - 1, max(start - 6, 0), -1):\n      before_text = clean_lines.elided[i] + before_text\n    if Search(r\'for\\s*\\([^{};]*$\', before_text):\n      # This is the condition inside a for-loop\n      return False\n\n    # Did not find a for-init-statement before this semicolon, so this\n    # is probably a new statement and not a condition.\n    return True\n\n  if match_symbol.group(2) == \'{\':\n    # Found opening brace, probably one of these:\n    #   block{ type&& = ... ; }\n    #   constructor{ expression && expression }\n\n    # Look for a closing brace or a semicolon.  If we see a semicolon\n    # first, this is probably a rvalue reference.\n    line = clean_lines.elided[start][0:len(match_symbol.group(1)) + 1]\n    end = start\n    depth = 1\n    while True:\n      for ch in line:\n        if ch == \';\':\n          return True\n        elif ch == \'{\':\n          depth += 1\n        elif ch == \'}\':\n          depth -= 1\n          if depth == 0:\n            return False\n      end += 1\n      if end >= clean_lines.NumLines():\n        break\n      line = clean_lines.elided[end]\n    # Incomplete program?\n    return False\n\n  if match_symbol.group(2) == \'(\':\n    # Opening parenthesis.  Need to check what\'s to the left of the\n    # parenthesis.  Look back one extra line for additional context.\n    before_text = match_symbol.group(1)\n    if linenum > 1:\n      before_text = clean_lines.elided[linenum - 1] + before_text\n    before_text = match_symbol.group(1)\n\n    # Patterns that are likely to be types:\n    #   [](type&&\n    #   for (type&&\n    #   sizeof(type&&\n    #   operator=(type&&\n    #\n    if Search(r\'(?:\\]|\\bfor|\\bsizeof|\\boperator\\s*\\S+\\s*)\\s*$\', before_text):\n      return True\n\n    # Patterns that are likely to be expressions:\n    #   if (expression &&\n    #   while (expression &&\n    #   : initializer(expression &&\n    #   , initializer(expression &&\n    #   ( FunctionCall(expression &&\n    #   + FunctionCall(expression &&\n    #   + (expression &&\n    #\n    # The last \'+\' represents operators such as \'+\' and \'-\'.\n    if Search(r\'(?:\\bif|\\bwhile|[-+=%^(<!?:,&*]\\s*)$\', before_text):\n      return False\n\n    # Something else.  Check that tokens to the left look like\n    #   return_type function_name\n    match_func = Match(r\'^(.*\\S.*)\\s+\\w(?:\\w|::)*(?:<[^<>]*>)?\\s*$\',\n                       match_symbol.group(1))\n    if match_func:\n      # Check for constructors, which don\'t have return types.\n      if Search(r\'\\b(?:explicit|inline)$\', match_func.group(1)):\n        return True\n      implicit_constructor = Match(r\'\\s*(\\w+)\\((?:const\\s+)?(\\w+)\', prefix)\n      if (implicit_constructor and\n          implicit_constructor.group(1) == implicit_constructor.group(2)):\n        return True\n      return IsRValueType(typenames, clean_lines, nesting_state, linenum,\n                          len(match_func.group(1)))\n\n    # Nothing before the function name.  If this is inside a block scope,\n    # this is probably a function call.\n    return not (nesting_state.previous_stack_top and\n                nesting_state.previous_stack_top.IsBlockInfo())\n\n  if match_symbol.group(2) == \'>\':\n    # Possibly a closing bracket, check that what\'s on the other side\n    # looks like the start of a template.\n    return IsTemplateParameterList(\n        clean_lines, start, len(match_symbol.group(1)))\n\n  # Some other symbol, usually something like ""a=b&&c"".  This is most\n  # likely not a type.\n  return False\n\n\ndef IsDeletedOrDefault(clean_lines, linenum):\n  """"""Check if current constructor or operator is deleted or default.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n  Returns:\n    True if this is a deleted or default constructor.\n  """"""\n  open_paren = clean_lines.elided[linenum].find(\'(\')\n  if open_paren < 0:\n    return False\n  (close_line, _, close_paren) = CloseExpression(\n      clean_lines, linenum, open_paren)\n  if close_paren < 0:\n    return False\n  return Match(r\'\\s*=\\s*(?:delete|default)\\b\', close_line[close_paren:])\n\n\ndef IsRValueAllowed(clean_lines, linenum, typenames):\n  """"""Check if RValue reference is allowed on a particular line.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    typenames: set of type names from template-argument-list.\n  Returns:\n    True if line is within the region where RValue references are allowed.\n  """"""\n  # Allow region marked by PUSH/POP macros\n  for i in xrange(linenum, 0, -1):\n    line = clean_lines.elided[i]\n    if Match(r\'GOOGLE_ALLOW_RVALUE_REFERENCES_(?:PUSH|POP)\', line):\n      if not line.endswith(\'PUSH\'):\n        return False\n      for j in xrange(linenum, clean_lines.NumLines(), 1):\n        line = clean_lines.elided[j]\n        if Match(r\'GOOGLE_ALLOW_RVALUE_REFERENCES_(?:PUSH|POP)\', line):\n          return line.endswith(\'POP\')\n\n  # Allow operator=\n  line = clean_lines.elided[linenum]\n  if Search(r\'\\boperator\\s*=\\s*\\(\', line):\n    return IsDeletedOrDefault(clean_lines, linenum)\n\n  # Allow constructors\n  match = Match(r\'\\s*(?:[\\w<>]+::)*([\\w<>]+)\\s*::\\s*([\\w<>]+)\\s*\\(\', line)\n  if match and match.group(1) == match.group(2):\n    return IsDeletedOrDefault(clean_lines, linenum)\n  if Search(r\'\\b(?:explicit|inline)\\s+[\\w<>]+\\s*\\(\', line):\n    return IsDeletedOrDefault(clean_lines, linenum)\n\n  if Match(r\'\\s*[\\w<>]+\\s*\\(\', line):\n    previous_line = \'ReturnType\'\n    if linenum > 0:\n      previous_line = clean_lines.elided[linenum - 1]\n    if Match(r\'^\\s*$\', previous_line) or Search(r\'[{}:;]\\s*$\', previous_line):\n      return IsDeletedOrDefault(clean_lines, linenum)\n\n  # Reject types not mentioned in template-argument-list\n  while line:\n    match = Match(r\'^.*?(\\w+)\\s*&&(.*)$\', line)\n    if not match:\n      break\n    if match.group(1) not in typenames:\n      return False\n    line = match.group(2)\n\n  # All RValue types that were in template-argument-list should have\n  # been removed by now.  Those were allowed, assuming that they will\n  # be forwarded.\n  #\n  # If there are no remaining RValue types left (i.e. types that were\n  # not found in template-argument-list), flag those as not allowed.\n  return line.find(\'&&\') < 0\n\n\ndef GetTemplateArgs(clean_lines, linenum):\n  """"""Find list of template arguments associated with this function declaration.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: Line number containing the start of the function declaration,\n             usually one line after the end of the template-argument-list.\n  Returns:\n    Set of type names, or empty set if this does not appear to have\n    any template parameters.\n  """"""\n  # Find start of function\n  func_line = linenum\n  while func_line > 0:\n    line = clean_lines.elided[func_line]\n    if Match(r\'^\\s*$\', line):\n      return set()\n    if line.find(\'(\') >= 0:\n      break\n    func_line -= 1\n  if func_line == 0:\n    return set()\n\n  # Collapse template-argument-list into a single string\n  argument_list = \'\'\n  match = Match(r\'^(\\s*template\\s*)<\', clean_lines.elided[func_line])\n  if match:\n    # template-argument-list on the same line as function name\n    start_col = len(match.group(1))\n    _, end_line, end_col = CloseExpression(clean_lines, func_line, start_col)\n    if end_col > -1 and end_line == func_line:\n      start_col += 1  # Skip the opening bracket\n      argument_list = clean_lines.elided[func_line][start_col:end_col]\n\n  elif func_line > 1:\n    # template-argument-list one line before function name\n    match = Match(r\'^(.*)>\\s*$\', clean_lines.elided[func_line - 1])\n    if match:\n      end_col = len(match.group(1))\n      _, start_line, start_col = ReverseCloseExpression(\n          clean_lines, func_line - 1, end_col)\n      if start_col > -1:\n        start_col += 1  # Skip the opening bracket\n        while start_line < func_line - 1:\n          argument_list += clean_lines.elided[start_line][start_col:]\n          start_col = 0\n          start_line += 1\n        argument_list += clean_lines.elided[func_line - 1][start_col:end_col]\n\n  if not argument_list:\n    return set()\n\n  # Extract type names\n  typenames = set()\n  while True:\n    match = Match(r\'^[,\\s]*(?:typename|class)(?:\\.\\.\\.)?\\s+(\\w+)(.*)$\',\n                  argument_list)\n    if not match:\n      break\n    typenames.add(match.group(1))\n    argument_list = match.group(2)\n  return typenames\n\n\ndef CheckRValueReference(filename, clean_lines, linenum, nesting_state, error):\n  """"""Check for rvalue references.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  """"""\n  # Find lines missing spaces around &&.\n  # TODO(unknown): currently we don\'t check for rvalue references\n  # with spaces surrounding the && to avoid false positives with\n  # boolean expressions.\n  line = clean_lines.elided[linenum]\n  match = Match(r\'^(.*\\S)&&\', line)\n  if not match:\n    match = Match(r\'(.*)&&\\S\', line)\n  if (not match) or \'(&&)\' in line or Search(r\'\\boperator\\s*$\', match.group(1)):\n    return\n\n  # Either poorly formed && or an rvalue reference, check the context\n  # to get a more accurate error message.  Mostly we want to determine\n  # if what\'s to the left of ""&&"" is a type or not.\n  typenames = GetTemplateArgs(clean_lines, linenum)\n  and_pos = len(match.group(1))\n  if IsRValueType(typenames, clean_lines, nesting_state, linenum, and_pos):\n    if not IsRValueAllowed(clean_lines, linenum, typenames):\n      error(filename, linenum, \'build/c++11\', 3,\n            \'RValue references are an unapproved C++ feature.\')\n  else:\n    error(filename, linenum, \'whitespace/operators\', 3,\n          \'Missing spaces around &&\')\n\n\ndef CheckSectionSpacing(filename, clean_lines, class_info, linenum, error):\n  """"""Checks for additional blank line issues related to sections.\n\n  Currently the only thing checked here is blank line before protected/private.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    class_info: A _ClassInfo objects.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  # Skip checks if the class is small, where small means 25 lines or less.\n  # 25 lines seems like a good cutoff since that\'s the usual height of\n  # terminals, and any class that can\'t fit in one screen can\'t really\n  # be considered ""small"".\n  #\n  # Also skip checks if we are on the first line.  This accounts for\n  # classes that look like\n  #   class Foo { public: ... };\n  #\n  # If we didn\'t find the end of the class, last_line would be zero,\n  # and the check will be skipped by the first condition.\n  if (class_info.last_line - class_info.starting_linenum <= 24 or\n      linenum <= class_info.starting_linenum):\n    return\n\n  matched = Match(r\'\\s*(public|protected|private):\', clean_lines.lines[linenum])\n  if matched:\n    # Issue warning if the line before public/protected/private was\n    # not a blank line, but don\'t do this if the previous line contains\n    # ""class"" or ""struct"".  This can happen two ways:\n    #  - We are at the beginning of the class.\n    #  - We are forward-declaring an inner class that is semantically\n    #    private, but needed to be public for implementation reasons.\n    # Also ignores cases where the previous line ends with a backslash as can be\n    # common when defining classes in C macros.\n    prev_line = clean_lines.lines[linenum - 1]\n    if (not IsBlankLine(prev_line) and\n        not Search(r\'\\b(class|struct)\\b\', prev_line) and\n        not Search(r\'\\\\$\', prev_line)):\n      # Try a bit harder to find the beginning of the class.  This is to\n      # account for multi-line base-specifier lists, e.g.:\n      #   class Derived\n      #       : public Base {\n      end_class_head = class_info.starting_linenum\n      for i in range(class_info.starting_linenum, linenum):\n        if Search(r\'\\{\\s*$\', clean_lines.lines[i]):\n          end_class_head = i\n          break\n      if end_class_head < linenum - 1:\n        error(filename, linenum, \'whitespace/blank_line\', 3,\n              \'""%s:"" should be preceded by a blank line\' % matched.group(1))\n\n\ndef GetPreviousNonBlankLine(clean_lines, linenum):\n  """"""Return the most recent non-blank line and its line number.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file contents.\n    linenum: The number of the line to check.\n\n  Returns:\n    A tuple with two elements.  The first element is the contents of the last\n    non-blank line before the current line, or the empty string if this is the\n    first non-blank line.  The second is the line number of that line, or -1\n    if this is the first non-blank line.\n  """"""\n\n  prevlinenum = linenum - 1\n  while prevlinenum >= 0:\n    prevline = clean_lines.elided[prevlinenum]\n    if not IsBlankLine(prevline):     # if not a blank line...\n      return (prevline, prevlinenum)\n    prevlinenum -= 1\n  return (\'\', -1)\n\n\ndef CheckBraces(filename, clean_lines, linenum, error):\n  """"""Looks for misplaced braces (e.g. at the end of line).\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n\n  line = clean_lines.elided[linenum]        # get rid of comments and strings\n\n  if Match(r\'\\s*{\\s*$\', line):\n    # We allow an open brace to start a line in the case where someone is using\n    # braces in a block to explicitly create a new scope, which is commonly used\n    # to control the lifetime of stack-allocated variables.  Braces are also\n    # used for brace initializers inside function calls.  We don\'t detect this\n    # perfectly: we just don\'t complain if the last non-whitespace character on\n    # the previous non-blank line is \',\', \';\', \':\', \'(\', \'{\', or \'}\', or if the\n    # previous line starts a preprocessor block.\n    prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]\n    if (not Search(r\'[,;:}{(]\\s*$\', prevline) and\n        not Match(r\'\\s*#\', prevline)):\n      error(filename, linenum, \'whitespace/braces\', 4,\n            \'{ should almost always be at the end of the previous line\')\n\n  # An else clause should be on the same line as the preceding closing brace.\n  if Match(r\'\\s*else\\b\\s*(?:if\\b|\\{|$)\', line):\n    prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]\n    if Match(r\'\\s*}\\s*$\', prevline):\n      error(filename, linenum, \'whitespace/newline\', 4,\n            \'An else should appear on the same line as the preceding }\')\n\n  # If braces come on one side of an else, they should be on both.\n  # However, we have to worry about ""else if"" that spans multiple lines!\n  if Search(r\'else if\\s*\\(\', line):       # could be multi-line if\n    brace_on_left = bool(Search(r\'}\\s*else if\\s*\\(\', line))\n    # find the ( after the if\n    pos = line.find(\'else if\')\n    pos = line.find(\'(\', pos)\n    if pos > 0:\n      (endline, _, endpos) = CloseExpression(clean_lines, linenum, pos)\n      brace_on_right = endline[endpos:].find(\'{\') != -1\n      if brace_on_left != brace_on_right:    # must be brace after if\n        error(filename, linenum, \'readability/braces\', 5,\n              \'If an else has a brace on one side, it should have it on both\')\n  elif Search(r\'}\\s*else[^{]*$\', line) or Match(r\'[^}]*else\\s*{\', line):\n    error(filename, linenum, \'readability/braces\', 5,\n          \'If an else has a brace on one side, it should have it on both\')\n\n  # Likewise, an else should never have the else clause on the same line\n  if Search(r\'\\belse [^\\s{]\', line) and not Search(r\'\\belse if\\b\', line):\n    error(filename, linenum, \'whitespace/newline\', 4,\n          \'Else clause should never be on same line as else (use 2 lines)\')\n\n  # In the same way, a do/while should never be on one line\n  if Match(r\'\\s*do [^\\s{]\', line):\n    error(filename, linenum, \'whitespace/newline\', 4,\n          \'do/while clauses should not be on a single line\')\n\n  # Check single-line if/else bodies. The style guide says \'curly braces are not\n  # required for single-line statements\'. We additionally allow multi-line,\n  # single statements, but we reject anything with more than one semicolon in\n  # it. This means that the first semicolon after the if should be at the end of\n  # its line, and the line after that should have an indent level equal to or\n  # lower than the if. We also check for ambiguous if/else nesting without\n  # braces.\n  if_else_match = Search(r\'\\b(if\\s*\\(|else\\b)\', line)\n  if if_else_match and not Match(r\'\\s*#\', line):\n    if_indent = GetIndentLevel(line)\n    endline, endlinenum, endpos = line, linenum, if_else_match.end()\n    if_match = Search(r\'\\bif\\s*\\(\', line)\n    if if_match:\n      # This could be a multiline if condition, so find the end first.\n      pos = if_match.end() - 1\n      (endline, endlinenum, endpos) = CloseExpression(clean_lines, linenum, pos)\n    # Check for an opening brace, either directly after the if or on the next\n    # line. If found, this isn\'t a single-statement conditional.\n    if (not Match(r\'\\s*{\', endline[endpos:])\n        and not (Match(r\'\\s*$\', endline[endpos:])\n                 and endlinenum < (len(clean_lines.elided) - 1)\n                 and Match(r\'\\s*{\', clean_lines.elided[endlinenum + 1]))):\n      while (endlinenum < len(clean_lines.elided)\n             and \';\' not in clean_lines.elided[endlinenum][endpos:]):\n        endlinenum += 1\n        endpos = 0\n      if endlinenum < len(clean_lines.elided):\n        endline = clean_lines.elided[endlinenum]\n        # We allow a mix of whitespace and closing braces (e.g. for one-liner\n        # methods) and a single \\ after the semicolon (for macros)\n        endpos = endline.find(\';\')\n        if not Match(r\';[\\s}]*(\\\\?)$\', endline[endpos:]):\n          # Semicolon isn\'t the last character, there\'s something trailing.\n          # Output a warning if the semicolon is not contained inside\n          # a lambda expression.\n          if not Match(r\'^[^{};]*\\[[^\\[\\]]*\\][^{}]*\\{[^{}]*\\}\\s*\\)*[;,]\\s*$\',\n                       endline):\n            error(filename, linenum, \'readability/braces\', 4,\n                  \'If/else bodies with multiple statements require braces\')\n        elif endlinenum < len(clean_lines.elided) - 1:\n          # Make sure the next line is dedented\n          next_line = clean_lines.elided[endlinenum + 1]\n          next_indent = GetIndentLevel(next_line)\n          # With ambiguous nested if statements, this will error out on the\n          # if that *doesn\'t* match the else, regardless of whether it\'s the\n          # inner one or outer one.\n          if (if_match and Match(r\'\\s*else\\b\', next_line)\n              and next_indent != if_indent):\n            error(filename, linenum, \'readability/braces\', 4,\n                  \'Else clause should be indented at the same level as if. \'\n                  \'Ambiguous nested if/else chains require braces.\')\n          elif next_indent > if_indent:\n            error(filename, linenum, \'readability/braces\', 4,\n                  \'If/else bodies with multiple statements require braces\')\n\n\ndef CheckTrailingSemicolon(filename, clean_lines, linenum, error):\n  """"""Looks for redundant trailing semicolon.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n\n  line = clean_lines.elided[linenum]\n\n  # Block bodies should not be followed by a semicolon.  Due to C++11\n  # brace initialization, there are more places where semicolons are\n  # required than not, so we use a whitelist approach to check these\n  # rather than a blacklist.  These are the places where ""};"" should\n  # be replaced by just ""}"":\n  # 1. Some flavor of block following closing parenthesis:\n  #    for (;;) {};\n  #    while (...) {};\n  #    switch (...) {};\n  #    Function(...) {};\n  #    if (...) {};\n  #    if (...) else if (...) {};\n  #\n  # 2. else block:\n  #    if (...) else {};\n  #\n  # 3. const member function:\n  #    Function(...) const {};\n  #\n  # 4. Block following some statement:\n  #    x = 42;\n  #    {};\n  #\n  # 5. Block at the beginning of a function:\n  #    Function(...) {\n  #      {};\n  #    }\n  #\n  #    Note that naively checking for the preceding ""{"" will also match\n  #    braces inside multi-dimensional arrays, but this is fine since\n  #    that expression will not contain semicolons.\n  #\n  # 6. Block following another block:\n  #    while (true) {}\n  #    {};\n  #\n  # 7. End of namespaces:\n  #    namespace {};\n  #\n  #    These semicolons seems far more common than other kinds of\n  #    redundant semicolons, possibly due to people converting classes\n  #    to namespaces.  For now we do not warn for this case.\n  #\n  # Try matching case 1 first.\n  match = Match(r\'^(.*\\)\\s*)\\{\', line)\n  if match:\n    # Matched closing parenthesis (case 1).  Check the token before the\n    # matching opening parenthesis, and don\'t warn if it looks like a\n    # macro.  This avoids these false positives:\n    #  - macro that defines a base class\n    #  - multi-line macro that defines a base class\n    #  - macro that defines the whole class-head\n    #\n    # But we still issue warnings for macros that we know are safe to\n    # warn, specifically:\n    #  - TEST, TEST_F, TEST_P, MATCHER, MATCHER_P\n    #  - TYPED_TEST\n    #  - INTERFACE_DEF\n    #  - EXCLUSIVE_LOCKS_REQUIRED, SHARED_LOCKS_REQUIRED, LOCKS_EXCLUDED:\n    #\n    # We implement a whitelist of safe macros instead of a blacklist of\n    # unsafe macros, even though the latter appears less frequently in\n    # google code and would have been easier to implement.  This is because\n    # the downside for getting the whitelist wrong means some extra\n    # semicolons, while the downside for getting the blacklist wrong\n    # would result in compile errors.\n    #\n    # In addition to macros, we also don\'t want to warn on\n    #  - Compound literals\n    #  - Lambdas\n    #  - alignas specifier with anonymous structs:\n    closing_brace_pos = match.group(1).rfind(\')\')\n    opening_parenthesis = ReverseCloseExpression(\n        clean_lines, linenum, closing_brace_pos)\n    if opening_parenthesis[2] > -1:\n      line_prefix = opening_parenthesis[0][0:opening_parenthesis[2]]\n      macro = Search(r\'\\b([A-Z_]+)\\s*$\', line_prefix)\n      func = Match(r\'^(.*\\])\\s*$\', line_prefix)\n      if ((macro and\n           macro.group(1) not in (\n               \'TEST\', \'TEST_F\', \'MATCHER\', \'MATCHER_P\', \'TYPED_TEST\',\n               \'EXCLUSIVE_LOCKS_REQUIRED\', \'SHARED_LOCKS_REQUIRED\',\n               \'LOCKS_EXCLUDED\', \'INTERFACE_DEF\')) or\n          (func and not Search(r\'\\boperator\\s*\\[\\s*\\]\', func.group(1))) or\n          Search(r\'\\b(?:struct|union)\\s+alignas\\s*$\', line_prefix) or\n          Search(r\'\\s+=\\s*$\', line_prefix)):\n        match = None\n    if (match and\n        opening_parenthesis[1] > 1 and\n        Search(r\'\\]\\s*$\', clean_lines.elided[opening_parenthesis[1] - 1])):\n      # Multi-line lambda-expression\n      match = None\n\n  else:\n    # Try matching cases 2-3.\n    match = Match(r\'^(.*(?:else|\\)\\s*const)\\s*)\\{\', line)\n    if not match:\n      # Try matching cases 4-6.  These are always matched on separate lines.\n      #\n      # Note that we can\'t simply concatenate the previous line to the\n      # current line and do a single match, otherwise we may output\n      # duplicate warnings for the blank line case:\n      #   if (cond) {\n      #     // blank line\n      #   }\n      prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]\n      if prevline and Search(r\'[;{}]\\s*$\', prevline):\n        match = Match(r\'^(\\s*)\\{\', line)\n\n  # Check matching closing brace\n  if match:\n    (endline, endlinenum, endpos) = CloseExpression(\n        clean_lines, linenum, len(match.group(1)))\n    if endpos > -1 and Match(r\'^\\s*;\', endline[endpos:]):\n      # Current {} pair is eligible for semicolon check, and we have found\n      # the redundant semicolon, output warning here.\n      #\n      # Note: because we are scanning forward for opening braces, and\n      # outputting warnings for the matching closing brace, if there are\n      # nested blocks with trailing semicolons, we will get the error\n      # messages in reversed order.\n      error(filename, endlinenum, \'readability/braces\', 4,\n            ""You don\'t need a ; after a }"")\n\n\ndef CheckEmptyBlockBody(filename, clean_lines, linenum, error):\n  """"""Look for empty loop/conditional body with only a single semicolon.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n\n  # Search for loop keywords at the beginning of the line.  Because only\n  # whitespaces are allowed before the keywords, this will also ignore most\n  # do-while-loops, since those lines should start with closing brace.\n  #\n  # We also check ""if"" blocks here, since an empty conditional block\n  # is likely an error.\n  line = clean_lines.elided[linenum]\n  matched = Match(r\'\\s*(for|while|if)\\s*\\(\', line)\n  if matched:\n    # Find the end of the conditional expression\n    (end_line, end_linenum, end_pos) = CloseExpression(\n        clean_lines, linenum, line.find(\'(\'))\n\n    # Output warning if what follows the condition expression is a semicolon.\n    # No warning for all other cases, including whitespace or newline, since we\n    # have a separate check for semicolons preceded by whitespace.\n    if end_pos >= 0 and Match(r\';\', end_line[end_pos:]):\n      if matched.group(1) == \'if\':\n        error(filename, end_linenum, \'whitespace/empty_conditional_body\', 5,\n              \'Empty conditional bodies should use {}\')\n      else:\n        error(filename, end_linenum, \'whitespace/empty_loop_body\', 5,\n              \'Empty loop bodies should use {} or continue\')\n\n\ndef FindCheckMacro(line):\n  """"""Find a replaceable CHECK-like macro.\n\n  Args:\n    line: line to search on.\n  Returns:\n    (macro name, start position), or (None, -1) if no replaceable\n    macro is found.\n  """"""\n  for macro in _CHECK_MACROS:\n    i = line.find(macro)\n    if i >= 0:\n      # Find opening parenthesis.  Do a regular expression match here\n      # to make sure that we are matching the expected CHECK macro, as\n      # opposed to some other macro that happens to contain the CHECK\n      # substring.\n      matched = Match(r\'^(.*\\b\' + macro + r\'\\s*)\\(\', line)\n      if not matched:\n        continue\n      return (macro, len(matched.group(1)))\n  return (None, -1)\n\n\ndef CheckCheck(filename, clean_lines, linenum, error):\n  """"""Checks the use of CHECK and EXPECT macros.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n\n  # Decide the set of replacement macros that should be suggested\n  lines = clean_lines.elided\n  (check_macro, start_pos) = FindCheckMacro(lines[linenum])\n  if not check_macro:\n    return\n\n  # Find end of the boolean expression by matching parentheses\n  (last_line, end_line, end_pos) = CloseExpression(\n      clean_lines, linenum, start_pos)\n  if end_pos < 0:\n    return\n\n  # If the check macro is followed by something other than a\n  # semicolon, assume users will log their own custom error messages\n  # and don\'t suggest any replacements.\n  if not Match(r\'\\s*;\', last_line[end_pos:]):\n    return\n\n  if linenum == end_line:\n    expression = lines[linenum][start_pos + 1:end_pos - 1]\n  else:\n    expression = lines[linenum][start_pos + 1:]\n    for i in xrange(linenum + 1, end_line):\n      expression += lines[i]\n    expression += last_line[0:end_pos - 1]\n\n  # Parse expression so that we can take parentheses into account.\n  # This avoids false positives for inputs like ""CHECK((a < 4) == b)"",\n  # which is not replaceable by CHECK_LE.\n  lhs = \'\'\n  rhs = \'\'\n  operator = None\n  while expression:\n    matched = Match(r\'^\\s*(<<|<<=|>>|>>=|->\\*|->|&&|\\|\\||\'\n                    r\'==|!=|>=|>|<=|<|\\()(.*)$\', expression)\n    if matched:\n      token = matched.group(1)\n      if token == \'(\':\n        # Parenthesized operand\n        expression = matched.group(2)\n        (end, _) = FindEndOfExpressionInLine(expression, 0, [\'(\'])\n        if end < 0:\n          return  # Unmatched parenthesis\n        lhs += \'(\' + expression[0:end]\n        expression = expression[end:]\n      elif token in (\'&&\', \'||\'):\n        # Logical and/or operators.  This means the expression\n        # contains more than one term, for example:\n        #   CHECK(42 < a && a < b);\n        #\n        # These are not replaceable with CHECK_LE, so bail out early.\n        return\n      elif token in (\'<<\', \'<<=\', \'>>\', \'>>=\', \'->*\', \'->\'):\n        # Non-relational operator\n        lhs += token\n        expression = matched.group(2)\n      else:\n        # Relational operator\n        operator = token\n        rhs = matched.group(2)\n        break\n    else:\n      # Unparenthesized operand.  Instead of appending to lhs one character\n      # at a time, we do another regular expression match to consume several\n      # characters at once if possible.  Trivial benchmark shows that this\n      # is more efficient when the operands are longer than a single\n      # character, which is generally the case.\n      matched = Match(r\'^([^-=!<>()&|]+)(.*)$\', expression)\n      if not matched:\n        matched = Match(r\'^(\\s*\\S)(.*)$\', expression)\n        if not matched:\n          break\n      lhs += matched.group(1)\n      expression = matched.group(2)\n\n  # Only apply checks if we got all parts of the boolean expression\n  if not (lhs and operator and rhs):\n    return\n\n  # Check that rhs do not contain logical operators.  We already know\n  # that lhs is fine since the loop above parses out && and ||.\n  if rhs.find(\'&&\') > -1 or rhs.find(\'||\') > -1:\n    return\n\n  # At least one of the operands must be a constant literal.  This is\n  # to avoid suggesting replacements for unprintable things like\n  # CHECK(variable != iterator)\n  #\n  # The following pattern matches decimal, hex integers, strings, and\n  # characters (in that order).\n  lhs = lhs.strip()\n  rhs = rhs.strip()\n  match_constant = r\'^([-+]?(\\d+|0[xX][0-9a-fA-F]+)[lLuU]{0,3}|"".*""|\\\'.*\\\')$\'\n  if Match(match_constant, lhs) or Match(match_constant, rhs):\n    # Note: since we know both lhs and rhs, we can provide a more\n    # descriptive error message like:\n    #   Consider using CHECK_EQ(x, 42) instead of CHECK(x == 42)\n    # Instead of:\n    #   Consider using CHECK_EQ instead of CHECK(a == b)\n    #\n    # We are still keeping the less descriptive message because if lhs\n    # or rhs gets long, the error message might become unreadable.\n    error(filename, linenum, \'readability/check\', 2,\n          \'Consider using %s instead of %s(a %s b)\' % (\n              _CHECK_REPLACEMENT[check_macro][operator],\n              check_macro, operator))\n\n\ndef CheckAltTokens(filename, clean_lines, linenum, error):\n  """"""Check alternative keywords being used in boolean expressions.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n\n  # Avoid preprocessor lines\n  if Match(r\'^\\s*#\', line):\n    return\n\n  # Last ditch effort to avoid multi-line comments.  This will not help\n  # if the comment started before the current line or ended after the\n  # current line, but it catches most of the false positives.  At least,\n  # it provides a way to workaround this warning for people who use\n  # multi-line comments in preprocessor macros.\n  #\n  # TODO(unknown): remove this once cpplint has better support for\n  # multi-line comments.\n  if line.find(\'/*\') >= 0 or line.find(\'*/\') >= 0:\n    return\n\n  for match in _ALT_TOKEN_REPLACEMENT_PATTERN.finditer(line):\n    error(filename, linenum, \'readability/alt_tokens\', 2,\n          \'Use operator %s instead of %s\' % (\n              _ALT_TOKEN_REPLACEMENT[match.group(1)], match.group(1)))\n\n\ndef GetLineWidth(line):\n  """"""Determines the width of the line in column positions.\n\n  Args:\n    line: A string, which may be a Unicode string.\n\n  Returns:\n    The width of the line in column positions, accounting for Unicode\n    combining characters and wide characters.\n  """"""\n  if isinstance(line, unicode):\n    width = 0\n    for uc in unicodedata.normalize(\'NFC\', line):\n      if unicodedata.east_asian_width(uc) in (\'W\', \'F\'):\n        width += 2\n      elif not unicodedata.combining(uc):\n        width += 1\n    return width\n  else:\n    return len(line)\n\n\ndef CheckStyle(filename, clean_lines, linenum, file_extension, nesting_state,\n               error):\n  """"""Checks rules from the \'C++ style rules\' section of cppguide.html.\n\n  Most of these rules are hard to test (naming, comment style), but we\n  do what we can.  In particular we check for 2-space indents, line lengths,\n  tab usage, spaces inside code, etc.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    file_extension: The extension (without the dot) of the filename.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  """"""\n\n  # Don\'t use ""elided"" lines here, otherwise we can\'t check commented lines.\n  # Don\'t want to use ""raw"" either, because we don\'t want to check inside C++11\n  # raw strings,\n  raw_lines = clean_lines.lines_without_raw_strings\n  line = raw_lines[linenum]\n\n  if line.find(\'\\t\') != -1:\n    error(filename, linenum, \'whitespace/tab\', 1,\n          \'Tab found; better to use spaces\')\n\n  # One or three blank spaces at the beginning of the line is weird; it\'s\n  # hard to reconcile that with 2-space indents.\n  # NOTE: here are the conditions rob pike used for his tests.  Mine aren\'t\n  # as sophisticated, but it may be worth becoming so:  RLENGTH==initial_spaces\n  # if(RLENGTH > 20) complain = 0;\n  # if(match($0, "" +(error|private|public|protected):"")) complain = 0;\n  # if(match(prev, ""&& *$"")) complain = 0;\n  # if(match(prev, ""\\\\|\\\\| *$"")) complain = 0;\n  # if(match(prev, ""[\\"",=><] *$"")) complain = 0;\n  # if(match($0, "" <<"")) complain = 0;\n  # if(match(prev, "" +for \\\\("")) complain = 0;\n  # if(prevodd && match(prevprev, "" +for \\\\("")) complain = 0;\n  scope_or_label_pattern = r\'\\s*\\w+\\s*:\\s*\\\\?$\'\n  classinfo = nesting_state.InnermostClass()\n  initial_spaces = 0\n  cleansed_line = clean_lines.elided[linenum]\n  while initial_spaces < len(line) and line[initial_spaces] == \' \':\n    initial_spaces += 1\n  if line and line[-1].isspace():\n    error(filename, linenum, \'whitespace/end_of_line\', 4,\n          \'Line ends in whitespace.  Consider deleting these extra spaces.\')\n  # There are certain situations we allow one space, notably for\n  # section labels, and also lines containing multi-line raw strings.\n  elif ((initial_spaces == 1 or initial_spaces == 3) and\n        not Match(scope_or_label_pattern, cleansed_line) and\n        not (clean_lines.raw_lines[linenum] != line and\n             Match(r\'^\\s*""""\', line))):\n    error(filename, linenum, \'whitespace/indent\', 3,\n          \'Weird number of spaces at line-start.  \'\n          \'Are you using a 2-space indent?\')\n\n  # Check if the line is a header guard.\n  is_header_guard = False\n  if file_extension == \'h\':\n    cppvar = GetHeaderGuardCPPVariable(filename)\n    if (line.startswith(\'#ifndef %s\' % cppvar) or\n        line.startswith(\'#define %s\' % cppvar) or\n        line.startswith(\'#endif  // %s\' % cppvar)):\n      is_header_guard = True\n  # #include lines and header guards can be long, since there\'s no clean way to\n  # split them.\n  #\n  # URLs can be long too.  It\'s possible to split these, but it makes them\n  # harder to cut&paste.\n  #\n  # The ""$Id:...$"" comment may also get very long without it being the\n  # developers fault.\n  if (not line.startswith(\'#include\') and not is_header_guard and\n      not Match(r\'^\\s*//.*http(s?)://\\S*$\', line) and\n      not Match(r\'^// \\$Id:.*#[0-9]+ \\$$\', line)):\n    line_width = GetLineWidth(line)\n    extended_length = int((_line_length * 1.25))\n    if line_width > extended_length:\n      error(filename, linenum, \'whitespace/line_length\', 4,\n            \'Lines should very rarely be longer than %i characters\' %\n            extended_length)\n    elif line_width > _line_length:\n      error(filename, linenum, \'whitespace/line_length\', 2,\n            \'Lines should be <= %i characters long\' % _line_length)\n\n  if (cleansed_line.count(\';\') > 1 and\n      # for loops are allowed two ;\'s (and may run over two lines).\n      cleansed_line.find(\'for\') == -1 and\n      (GetPreviousNonBlankLine(clean_lines, linenum)[0].find(\'for\') == -1 or\n       GetPreviousNonBlankLine(clean_lines, linenum)[0].find(\';\') != -1) and\n      # It\'s ok to have many commands in a switch case that fits in 1 line\n      not ((cleansed_line.find(\'case \') != -1 or\n            cleansed_line.find(\'default:\') != -1) and\n           cleansed_line.find(\'break;\') != -1)):\n    error(filename, linenum, \'whitespace/newline\', 0,\n          \'More than one command on the same line\')\n\n  # Some more style checks\n  CheckBraces(filename, clean_lines, linenum, error)\n  CheckTrailingSemicolon(filename, clean_lines, linenum, error)\n  CheckEmptyBlockBody(filename, clean_lines, linenum, error)\n  CheckAccess(filename, clean_lines, linenum, nesting_state, error)\n  CheckSpacing(filename, clean_lines, linenum, nesting_state, error)\n  CheckOperatorSpacing(filename, clean_lines, linenum, error)\n  CheckParenthesisSpacing(filename, clean_lines, linenum, error)\n  CheckCommaSpacing(filename, clean_lines, linenum, error)\n  CheckBracesSpacing(filename, clean_lines, linenum, error)\n  CheckSpacingForFunctionCall(filename, clean_lines, linenum, error)\n  CheckRValueReference(filename, clean_lines, linenum, nesting_state, error)\n  CheckCheck(filename, clean_lines, linenum, error)\n  CheckAltTokens(filename, clean_lines, linenum, error)\n  classinfo = nesting_state.InnermostClass()\n  if classinfo:\n    CheckSectionSpacing(filename, clean_lines, classinfo, linenum, error)\n\n\n_RE_PATTERN_INCLUDE = re.compile(r\'^\\s*#\\s*include\\s*([<""])([^>""]*)[>""].*$\')\n# Matches the first component of a filename delimited by -s and _s. That is:\n#  _RE_FIRST_COMPONENT.match(\'foo\').group(0) == \'foo\'\n#  _RE_FIRST_COMPONENT.match(\'foo.cc\').group(0) == \'foo\'\n#  _RE_FIRST_COMPONENT.match(\'foo-bar_baz.cc\').group(0) == \'foo\'\n#  _RE_FIRST_COMPONENT.match(\'foo_bar-baz.cc\').group(0) == \'foo\'\n_RE_FIRST_COMPONENT = re.compile(r\'^[^-_.]+\')\n\n\ndef _DropCommonSuffixes(filename):\n  """"""Drops common suffixes like _test.cc or -inl.h from filename.\n\n  For example:\n    >>> _DropCommonSuffixes(\'foo/foo-inl.h\')\n    \'foo/foo\'\n    >>> _DropCommonSuffixes(\'foo/bar/foo.cc\')\n    \'foo/bar/foo\'\n    >>> _DropCommonSuffixes(\'foo/foo_internal.h\')\n    \'foo/foo\'\n    >>> _DropCommonSuffixes(\'foo/foo_unusualinternal.h\')\n    \'foo/foo_unusualinternal\'\n\n  Args:\n    filename: The input filename.\n\n  Returns:\n    The filename with the common suffix removed.\n  """"""\n  for suffix in (\'test.cc\', \'regtest.cc\', \'unittest.cc\',\n                 \'inl.h\', \'impl.h\', \'internal.h\'):\n    if (filename.endswith(suffix) and len(filename) > len(suffix) and\n        filename[-len(suffix) - 1] in (\'-\', \'_\')):\n      return filename[:-len(suffix) - 1]\n  return os.path.splitext(filename)[0]\n\n\ndef _IsTestFilename(filename):\n  """"""Determines if the given filename has a suffix that identifies it as a test.\n\n  Args:\n    filename: The input filename.\n\n  Returns:\n    True if \'filename\' looks like a test, False otherwise.\n  """"""\n  if (filename.endswith(\'_test.cc\') or\n      filename.endswith(\'_unittest.cc\') or\n      filename.endswith(\'_regtest.cc\')):\n    return True\n  else:\n    return False\n\n\ndef _ClassifyInclude(fileinfo, include, is_system):\n  """"""Figures out what kind of header \'include\' is.\n\n  Args:\n    fileinfo: The current file cpplint is running over. A FileInfo instance.\n    include: The path to a #included file.\n    is_system: True if the #include used <> rather than """".\n\n  Returns:\n    One of the _XXX_HEADER constants.\n\n  For example:\n    >>> _ClassifyInclude(FileInfo(\'foo/foo.cc\'), \'stdio.h\', True)\n    _C_SYS_HEADER\n    >>> _ClassifyInclude(FileInfo(\'foo/foo.cc\'), \'string\', True)\n    _CPP_SYS_HEADER\n    >>> _ClassifyInclude(FileInfo(\'foo/foo.cc\'), \'foo/foo.h\', False)\n    _LIKELY_MY_HEADER\n    >>> _ClassifyInclude(FileInfo(\'foo/foo_unknown_extension.cc\'),\n    ...                  \'bar/foo_other_ext.h\', False)\n    _POSSIBLE_MY_HEADER\n    >>> _ClassifyInclude(FileInfo(\'foo/foo.cc\'), \'foo/bar.h\', False)\n    _OTHER_HEADER\n  """"""\n  # This is a list of all standard c++ header files, except\n  # those already checked for above.\n  is_cpp_h = include in _CPP_HEADERS\n\n  if is_system:\n    if is_cpp_h:\n      return _CPP_SYS_HEADER\n    else:\n      return _C_SYS_HEADER\n\n  # If the target file and the include we\'re checking share a\n  # basename when we drop common extensions, and the include\n  # lives in . , then it\'s likely to be owned by the target file.\n  target_dir, target_base = (\n      os.path.split(_DropCommonSuffixes(fileinfo.RepositoryName())))\n  include_dir, include_base = os.path.split(_DropCommonSuffixes(include))\n  if target_base == include_base and (\n      include_dir == target_dir or\n      include_dir == os.path.normpath(target_dir + \'/../public\')):\n    return _LIKELY_MY_HEADER\n\n  # If the target and include share some initial basename\n  # component, it\'s possible the target is implementing the\n  # include, so it\'s allowed to be first, but we\'ll never\n  # complain if it\'s not there.\n  target_first_component = _RE_FIRST_COMPONENT.match(target_base)\n  include_first_component = _RE_FIRST_COMPONENT.match(include_base)\n  if (target_first_component and include_first_component and\n      target_first_component.group(0) ==\n      include_first_component.group(0)):\n    return _POSSIBLE_MY_HEADER\n\n  return _OTHER_HEADER\n\n\n\ndef CheckIncludeLine(filename, clean_lines, linenum, include_state, error):\n  """"""Check rules that are applicable to #include lines.\n\n  Strings on #include lines are NOT removed from elided line, to make\n  certain tasks easier. However, to prevent false positives, checks\n  applicable to #include lines in CheckLanguage must be put here.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    include_state: An _IncludeState instance in which the headers are inserted.\n    error: The function to call with any errors found.\n  """"""\n  fileinfo = FileInfo(filename)\n  line = clean_lines.lines[linenum]\n\n  # ""include"" should use the new style ""foo/bar.h"" instead of just ""bar.h""\n  # Only do this check if the included header follows google naming\n  # conventions.  If not, assume that it\'s a 3rd party API that\n  # requires special include conventions.\n  #\n  # We also make an exception for Lua headers, which follow google\n  # naming convention but not the include convention.\n  match = Match(r\'#include\\s*""([^/]+\\.h)""\', line)\n  if match and not _THIRD_PARTY_HEADERS_PATTERN.match(match.group(1)):\n    error(filename, linenum, \'build/include\', 4,\n          \'Include the directory when naming .h files\')\n\n  # we shouldn\'t include a file more than once. actually, there are a\n  # handful of instances where doing so is okay, but in general it\'s\n  # not.\n  match = _RE_PATTERN_INCLUDE.search(line)\n  if match:\n    include = match.group(2)\n    is_system = (match.group(1) == \'<\')\n    duplicate_line = include_state.FindHeader(include)\n    if duplicate_line >= 0:\n      error(filename, linenum, \'build/include\', 4,\n            \'""%s"" already included at %s:%s\' %\n            (include, filename, duplicate_line))\n    elif (include.endswith(\'.cc\') and\n          os.path.dirname(fileinfo.RepositoryName()) != os.path.dirname(include)):\n      error(filename, linenum, \'build/include\', 4,\n            \'Do not include .cc files from other packages\')\n    elif not _THIRD_PARTY_HEADERS_PATTERN.match(include):\n      include_state.include_list[-1].append((include, linenum))\n\n      # We want to ensure that headers appear in the right order:\n      # 1) for foo.cc, foo.h  (preferred location)\n      # 2) c system files\n      # 3) cpp system files\n      # 4) for foo.cc, foo.h  (deprecated location)\n      # 5) other google headers\n      #\n      # We classify each include statement as one of those 5 types\n      # using a number of techniques. The include_state object keeps\n      # track of the highest type seen, and complains if we see a\n      # lower type after that.\n      error_message = include_state.CheckNextIncludeOrder(\n          _ClassifyInclude(fileinfo, include, is_system))\n      if error_message:\n        error(filename, linenum, \'build/include_order\', 4,\n              \'%s. Should be: %s.h, c system, c++ system, other.\' %\n              (error_message, fileinfo.BaseName()))\n      canonical_include = include_state.CanonicalizeAlphabeticalOrder(include)\n      if not include_state.IsInAlphabeticalOrder(\n          clean_lines, linenum, canonical_include):\n        error(filename, linenum, \'build/include_alpha\', 4,\n              \'Include ""%s"" not in alphabetical order\' % include)\n      include_state.SetLastHeader(canonical_include)\n\n\n\ndef _GetTextInside(text, start_pattern):\n  r""""""Retrieves all the text between matching open and close parentheses.\n\n  Given a string of lines and a regular expression string, retrieve all the text\n  following the expression and between opening punctuation symbols like\n  (, [, or {, and the matching close-punctuation symbol. This properly nested\n  occurrences of the punctuations, so for the text like\n    printf(a(), b(c()));\n  a call to _GetTextInside(text, r\'printf\\(\') will return \'a(), b(c())\'.\n  start_pattern must match string having an open punctuation symbol at the end.\n\n  Args:\n    text: The lines to extract text. Its comments and strings must be elided.\n           It can be single line and can span multiple lines.\n    start_pattern: The regexp string indicating where to start extracting\n                   the text.\n  Returns:\n    The extracted text.\n    None if either the opening string or ending punctuation could not be found.\n  """"""\n  # TODO(unknown): Audit cpplint.py to see what places could be profitably\n  # rewritten to use _GetTextInside (and use inferior regexp matching today).\n\n  # Give opening punctuations to get the matching close-punctuations.\n  matching_punctuation = {\'(\': \')\', \'{\': \'}\', \'[\': \']\'}\n  closing_punctuation = set(matching_punctuation.itervalues())\n\n  # Find the position to start extracting text.\n  match = re.search(start_pattern, text, re.M)\n  if not match:  # start_pattern not found in text.\n    return None\n  start_position = match.end(0)\n\n  assert start_position > 0, (\n      \'start_pattern must ends with an opening punctuation.\')\n  assert text[start_position - 1] in matching_punctuation, (\n      \'start_pattern must ends with an opening punctuation.\')\n  # Stack of closing punctuations we expect to have in text after position.\n  punctuation_stack = [matching_punctuation[text[start_position - 1]]]\n  position = start_position\n  while punctuation_stack and position < len(text):\n    if text[position] == punctuation_stack[-1]:\n      punctuation_stack.pop()\n    elif text[position] in closing_punctuation:\n      # A closing punctuation without matching opening punctuations.\n      return None\n    elif text[position] in matching_punctuation:\n      punctuation_stack.append(matching_punctuation[text[position]])\n    position += 1\n  if punctuation_stack:\n    # Opening punctuations left without matching close-punctuations.\n    return None\n  # punctuations match.\n  return text[start_position:position - 1]\n\n\n# Patterns for matching call-by-reference parameters.\n#\n# Supports nested templates up to 2 levels deep using this messy pattern:\n#   < (?: < (?: < [^<>]*\n#               >\n#           |   [^<>] )*\n#         >\n#     |   [^<>] )*\n#   >\n_RE_PATTERN_IDENT = r\'[_a-zA-Z]\\w*\'  # =~ [[:alpha:]][[:alnum:]]*\n_RE_PATTERN_TYPE = (\n    r\'(?:const\\s+)?(?:typename\\s+|class\\s+|struct\\s+|union\\s+|enum\\s+)?\'\n    r\'(?:\\w|\'\n    r\'\\s*<(?:<(?:<[^<>]*>|[^<>])*>|[^<>])*>|\'\n    r\'::)+\')\n# A call-by-reference parameter ends with \'& identifier\'.\n_RE_PATTERN_REF_PARAM = re.compile(\n    r\'(\' + _RE_PATTERN_TYPE + r\'(?:\\s*(?:\\bconst\\b|[*]))*\\s*\'\n    r\'&\\s*\' + _RE_PATTERN_IDENT + r\')\\s*(?:=[^,()]+)?[,)]\')\n# A call-by-const-reference parameter either ends with \'const& identifier\'\n# or looks like \'const type& identifier\' when \'type\' is atomic.\n_RE_PATTERN_CONST_REF_PARAM = (\n    r\'(?:.*\\s*\\bconst\\s*&\\s*\' + _RE_PATTERN_IDENT +\n    r\'|const\\s+\' + _RE_PATTERN_TYPE + r\'\\s*&\\s*\' + _RE_PATTERN_IDENT + r\')\')\n\n\ndef CheckLanguage(filename, clean_lines, linenum, file_extension,\n                  include_state, nesting_state, error):\n  """"""Checks rules from the \'C++ language rules\' section of cppguide.html.\n\n  Some of these rules are hard to test (function overloading, using\n  uint32 inappropriately), but we do the best we can.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    file_extension: The extension (without the dot) of the filename.\n    include_state: An _IncludeState instance in which the headers are inserted.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  """"""\n  # If the line is empty or consists of entirely a comment, no need to\n  # check it.\n  line = clean_lines.elided[linenum]\n  if not line:\n    return\n\n  match = _RE_PATTERN_INCLUDE.search(line)\n  if match:\n    CheckIncludeLine(filename, clean_lines, linenum, include_state, error)\n    return\n\n  # Reset include state across preprocessor directives.  This is meant\n  # to silence warnings for conditional includes.\n  match = Match(r\'^\\s*#\\s*(if|ifdef|ifndef|elif|else|endif)\\b\', line)\n  if match:\n    include_state.ResetSection(match.group(1))\n\n  # Make Windows paths like Unix.\n  fullname = os.path.abspath(filename).replace(\'\\\\\', \'/\')\n  \n  # Perform other checks now that we are sure that this is not an include line\n  CheckCasts(filename, clean_lines, linenum, error)\n  CheckGlobalStatic(filename, clean_lines, linenum, error)\n  CheckPrintf(filename, clean_lines, linenum, error)\n\n  if file_extension == \'h\':\n    # TODO(unknown): check that 1-arg constructors are explicit.\n    #                How to tell it\'s a constructor?\n    #                (handled in CheckForNonStandardConstructs for now)\n    # TODO(unknown): check that classes declare or disable copy/assign\n    #                (level 1 error)\n    pass\n\n  # Check if people are using the verboten C basic types.  The only exception\n  # we regularly allow is ""unsigned short port"" for port.\n  if Search(r\'\\bshort port\\b\', line):\n    if not Search(r\'\\bunsigned short port\\b\', line):\n      error(filename, linenum, \'runtime/int\', 4,\n            \'Use ""unsigned short"" for ports, not ""short""\')\n  else:\n    match = Search(r\'\\b(short|long(?! +double)|long long)\\b\', line)\n    if match:\n      error(filename, linenum, \'runtime/int\', 4,\n            \'Use int16/int64/etc, rather than the C type %s\' % match.group(1))\n\n  # Check if some verboten operator overloading is going on\n  # TODO(unknown): catch out-of-line unary operator&:\n  #   class X {};\n  #   int operator&(const X& x) { return 42; }  // unary operator&\n  # The trick is it\'s hard to tell apart from binary operator&:\n  #   class Y { int operator&(const Y& x) { return 23; } }; // binary operator&\n  if Search(r\'\\boperator\\s*&\\s*\\(\\s*\\)\', line):\n    error(filename, linenum, \'runtime/operator\', 4,\n          \'Unary operator& is dangerous.  Do not use it.\')\n\n  # Check for suspicious usage of ""if"" like\n  # } if (a == b) {\n  if Search(r\'\\}\\s*if\\s*\\(\', line):\n    error(filename, linenum, \'readability/braces\', 4,\n          \'Did you mean ""else if""? If not, start a new line for ""if"".\')\n\n  # Check for potential format string bugs like printf(foo).\n  # We constrain the pattern not to pick things like DocidForPrintf(foo).\n  # Not perfect but it can catch printf(foo.c_str()) and printf(foo->c_str())\n  # TODO(unknown): Catch the following case. Need to change the calling\n  # convention of the whole function to process multiple line to handle it.\n  #   printf(\n  #       boy_this_is_a_really_long_variable_that_cannot_fit_on_the_prev_line);\n  printf_args = _GetTextInside(line, r\'(?i)\\b(string)?printf\\s*\\(\')\n  if printf_args:\n    match = Match(r\'([\\w.\\->()]+)$\', printf_args)\n    if match and match.group(1) != \'__VA_ARGS__\':\n      function_name = re.search(r\'\\b((?:string)?printf)\\s*\\(\',\n                                line, re.I).group(1)\n      error(filename, linenum, \'runtime/printf\', 4,\n            \'Potential format string bug. Do %s(""%%s"", %s) instead.\'\n            % (function_name, match.group(1)))\n\n  # Check for potential memset bugs like memset(buf, sizeof(buf), 0).\n  match = Search(r\'memset\\s*\\(([^,]*),\\s*([^,]*),\\s*0\\s*\\)\', line)\n  if match and not Match(r""^\'\'|-?[0-9]+|0x[0-9A-Fa-f]$"", match.group(2)):\n    error(filename, linenum, \'runtime/memset\', 4,\n          \'Did you mean ""memset(%s, 0, %s)""?\'\n          % (match.group(1), match.group(2)))\n\n  if Search(r\'\\busing namespace\\b\', line):\n    error(filename, linenum, \'build/namespaces\', 5,\n          \'Do not use namespace using-directives.  \'\n          \'Use using-declarations instead.\')\n\n  # Detect variable-length arrays.\n  match = Match(r\'\\s*(.+::)?(\\w+) [a-z]\\w*\\[(.+)];\', line)\n  if (match and match.group(2) != \'return\' and match.group(2) != \'delete\' and\n      match.group(3).find(\']\') == -1):\n    # Split the size using space and arithmetic operators as delimiters.\n    # If any of the resulting tokens are not compile time constants then\n    # report the error.\n    tokens = re.split(r\'\\s|\\+|\\-|\\*|\\/|<<|>>]\', match.group(3))\n    is_const = True\n    skip_next = False\n    for tok in tokens:\n      if skip_next:\n        skip_next = False\n        continue\n\n      if Search(r\'sizeof\\(.+\\)\', tok): continue\n      if Search(r\'arraysize\\(\\w+\\)\', tok): continue\n\n      tok = tok.lstrip(\'(\')\n      tok = tok.rstrip(\')\')\n      if not tok: continue\n      if Match(r\'\\d+\', tok): continue\n      if Match(r\'0[xX][0-9a-fA-F]+\', tok): continue\n      if Match(r\'k[A-Z0-9]\\w*\', tok): continue\n      if Match(r\'(.+::)?k[A-Z0-9]\\w*\', tok): continue\n      if Match(r\'(.+::)?[A-Z][A-Z0-9_]*\', tok): continue\n      # A catch all for tricky sizeof cases, including \'sizeof expression\',\n      # \'sizeof(*type)\', \'sizeof(const type)\', \'sizeof(struct StructName)\'\n      # requires skipping the next token because we split on \' \' and \'*\'.\n      if tok.startswith(\'sizeof\'):\n        skip_next = True\n        continue\n      is_const = False\n      break\n    if not is_const:\n      error(filename, linenum, \'runtime/arrays\', 1,\n            \'Do not use variable-length arrays.  Use an appropriately named \'\n            ""(\'k\' followed by CamelCase) compile-time constant for the size."")\n\n  # Check for use of unnamed namespaces in header files.  Registration\n  # macros are typically OK, so we allow use of ""namespace {"" on lines\n  # that end with backslashes.\n  if (file_extension == \'h\'\n      and Search(r\'\\bnamespace\\s*{\', line)\n      and line[-1] != \'\\\\\'):\n    error(filename, linenum, \'build/namespaces\', 4,\n          \'Do not use unnamed namespaces in header files.  See \'\n          \'http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml#Namespaces\'\n          \' for more information.\')\n\n\ndef CheckGlobalStatic(filename, clean_lines, linenum, error):\n  """"""Check for unsafe global or static objects.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n\n  # Match two lines at a time to support multiline declarations\n  if linenum + 1 < clean_lines.NumLines() and not Search(r\'[;({]\', line):\n    line += clean_lines.elided[linenum + 1].strip()\n\n  # Check for people declaring static/global STL strings at the top level.\n  # This is dangerous because the C++ language does not guarantee that\n  # globals with constructors are initialized before the first access.\n  match = Match(\n      r\'((?:|static +)(?:|const +))string +([a-zA-Z0-9_:]+)\\b(.*)\',\n      line)\n\n  # Remove false positives:\n  # - String pointers (as opposed to values).\n  #    string *pointer\n  #    const string *pointer\n  #    string const *pointer\n  #    string *const pointer\n  #\n  # - Functions and template specializations.\n  #    string Function<Type>(...\n  #    string Class<Type>::Method(...\n  #\n  # - Operators.  These are matched separately because operator names\n  #   cross non-word boundaries, and trying to match both operators\n  #   and functions at the same time would decrease accuracy of\n  #   matching identifiers.\n  #    string Class::operator*()\n  if (match and\n      not Search(r\'\\bstring\\b(\\s+const)?\\s*\\*\\s*(const\\s+)?\\w\', line) and\n      not Search(r\'\\boperator\\W\', line) and\n      not Match(r\'\\s*(<.*>)?(::[a-zA-Z0-9_]+)*\\s*\\(([^""]|$)\', match.group(3))):\n    error(filename, linenum, \'runtime/string\', 4,\n          \'For a static/global string constant, use a C style string instead: \'\n          \'""%schar %s[]"".\' %\n          (match.group(1), match.group(2)))\n\n  if Search(r\'\\b([A-Za-z0-9_]*_)\\(\\1\\)\', line):\n    error(filename, linenum, \'runtime/init\', 4,\n          \'You seem to be initializing a member variable with itself.\')\n\n\ndef CheckPrintf(filename, clean_lines, linenum, error):\n  """"""Check for printf related issues.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n\n  # When snprintf is used, the second argument shouldn\'t be a literal.\n  match = Search(r\'snprintf\\s*\\(([^,]*),\\s*([0-9]*)\\s*,\', line)\n  if match and match.group(2) != \'0\':\n    # If 2nd arg is zero, snprintf is used to calculate size.\n    error(filename, linenum, \'runtime/printf\', 3,\n          \'If you can, use sizeof(%s) instead of %s as the 2nd arg \'\n          \'to snprintf.\' % (match.group(1), match.group(2)))\n\n  # Check if some verboten C functions are being used.\n  if Search(r\'\\bsprintf\\s*\\(\', line):\n    error(filename, linenum, \'runtime/printf\', 5,\n          \'Never use sprintf. Use snprintf instead.\')\n  match = Search(r\'\\b(strcpy|strcat)\\s*\\(\', line)\n  if match:\n    error(filename, linenum, \'runtime/printf\', 4,\n          \'Almost always, snprintf is better than %s\' % match.group(1))\n\n\ndef IsDerivedFunction(clean_lines, linenum):\n  """"""Check if current line contains an inherited function.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n  Returns:\n    True if current line contains a function with ""override""\n    virt-specifier.\n  """"""\n  # Scan back a few lines for start of current function\n  for i in xrange(linenum, max(-1, linenum - 10), -1):\n    match = Match(r\'^([^()]*\\w+)\\(\', clean_lines.elided[i])\n    if match:\n      # Look for ""override"" after the matching closing parenthesis\n      line, _, closing_paren = CloseExpression(\n          clean_lines, i, len(match.group(1)))\n      return (closing_paren >= 0 and\n              Search(r\'\\boverride\\b\', line[closing_paren:]))\n  return False\n\n\ndef IsOutOfLineMethodDefinition(clean_lines, linenum):\n  """"""Check if current line contains an out-of-line method definition.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n  Returns:\n    True if current line contains an out-of-line method definition.\n  """"""\n  # Scan back a few lines for start of current function\n  for i in xrange(linenum, max(-1, linenum - 10), -1):\n    if Match(r\'^([^()]*\\w+)\\(\', clean_lines.elided[i]):\n      return Match(r\'^[^()]*\\w+::\\w+\\(\', clean_lines.elided[i]) is not None\n  return False\n\n\ndef IsInitializerList(clean_lines, linenum):\n  """"""Check if current line is inside constructor initializer list.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n  Returns:\n    True if current line appears to be inside constructor initializer\n    list, False otherwise.\n  """"""\n  for i in xrange(linenum, 1, -1):\n    line = clean_lines.elided[i]\n    if i == linenum:\n      remove_function_body = Match(r\'^(.*)\\{\\s*$\', line)\n      if remove_function_body:\n        line = remove_function_body.group(1)\n\n    if Search(r\'\\s:\\s*\\w+[({]\', line):\n      # A lone colon tend to indicate the start of a constructor\n      # initializer list.  It could also be a ternary operator, which\n      # also tend to appear in constructor initializer lists as\n      # opposed to parameter lists.\n      return True\n    if Search(r\'\\}\\s*,\\s*$\', line):\n      # A closing brace followed by a comma is probably the end of a\n      # brace-initialized member in constructor initializer list.\n      return True\n    if Search(r\'[{};]\\s*$\', line):\n      # Found one of the following:\n      # - A closing brace or semicolon, probably the end of the previous\n      #   function.\n      # - An opening brace, probably the start of current class or namespace.\n      #\n      # Current line is probably not inside an initializer list since\n      # we saw one of those things without seeing the starting colon.\n      return False\n\n  # Got to the beginning of the file without seeing the start of\n  # constructor initializer list.\n  return False\n\n\ndef CheckForNonConstReference(filename, clean_lines, linenum,\n                              nesting_state, error):\n  """"""Check for non-const references.\n\n  Separate from CheckLanguage since it scans backwards from current\n  line, instead of scanning forward.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  """"""\n  # Do nothing if there is no \'&\' on current line.\n  line = clean_lines.elided[linenum]\n  if \'&\' not in line:\n    return\n\n  # If a function is inherited, current function doesn\'t have much of\n  # a choice, so any non-const references should not be blamed on\n  # derived function.\n  if IsDerivedFunction(clean_lines, linenum):\n    return\n\n  # Don\'t warn on out-of-line method definitions, as we would warn on the\n  # in-line declaration, if it isn\'t marked with \'override\'.\n  if IsOutOfLineMethodDefinition(clean_lines, linenum):\n    return\n\n  # Long type names may be broken across multiple lines, usually in one\n  # of these forms:\n  #   LongType\n  #       ::LongTypeContinued &identifier\n  #   LongType::\n  #       LongTypeContinued &identifier\n  #   LongType<\n  #       ...>::LongTypeContinued &identifier\n  #\n  # If we detected a type split across two lines, join the previous\n  # line to current line so that we can match const references\n  # accordingly.\n  #\n  # Note that this only scans back one line, since scanning back\n  # arbitrary number of lines would be expensive.  If you have a type\n  # that spans more than 2 lines, please use a typedef.\n  if linenum > 1:\n    previous = None\n    if Match(r\'\\s*::(?:[\\w<>]|::)+\\s*&\\s*\\S\', line):\n      # previous_line\\n + ::current_line\n      previous = Search(r\'\\b((?:const\\s*)?(?:[\\w<>]|::)+[\\w<>])\\s*$\',\n                        clean_lines.elided[linenum - 1])\n    elif Match(r\'\\s*[a-zA-Z_]([\\w<>]|::)+\\s*&\\s*\\S\', line):\n      # previous_line::\\n + current_line\n      previous = Search(r\'\\b((?:const\\s*)?(?:[\\w<>]|::)+::)\\s*$\',\n                        clean_lines.elided[linenum - 1])\n    if previous:\n      line = previous.group(1) + line.lstrip()\n    else:\n      # Check for templated parameter that is split across multiple lines\n      endpos = line.rfind(\'>\')\n      if endpos > -1:\n        (_, startline, startpos) = ReverseCloseExpression(\n            clean_lines, linenum, endpos)\n        if startpos > -1 and startline < linenum:\n          # Found the matching < on an earlier line, collect all\n          # pieces up to current line.\n          line = \'\'\n          for i in xrange(startline, linenum + 1):\n            line += clean_lines.elided[i].strip()\n\n  # Check for non-const references in function parameters.  A single \'&\' may\n  # found in the following places:\n  #   inside expression: binary & for bitwise AND\n  #   inside expression: unary & for taking the address of something\n  #   inside declarators: reference parameter\n  # We will exclude the first two cases by checking that we are not inside a\n  # function body, including one that was just introduced by a trailing \'{\'.\n  # TODO(unknown): Doesn\'t account for \'catch(Exception& e)\' [rare].\n  if (nesting_state.previous_stack_top and\n      not (isinstance(nesting_state.previous_stack_top, _ClassInfo) or\n           isinstance(nesting_state.previous_stack_top, _NamespaceInfo))):\n    # Not at toplevel, not within a class, and not within a namespace\n    return\n\n  # Avoid initializer lists.  We only need to scan back from the\n  # current line for something that starts with \':\'.\n  #\n  # We don\'t need to check the current line, since the \'&\' would\n  # appear inside the second set of parentheses on the current line as\n  # opposed to the first set.\n  if linenum > 0:\n    for i in xrange(linenum - 1, max(0, linenum - 10), -1):\n      previous_line = clean_lines.elided[i]\n      if not Search(r\'[),]\\s*$\', previous_line):\n        break\n      if Match(r\'^\\s*:\\s+\\S\', previous_line):\n        return\n\n  # Avoid preprocessors\n  if Search(r\'\\\\\\s*$\', line):\n    return\n\n  # Avoid constructor initializer lists\n  if IsInitializerList(clean_lines, linenum):\n    return\n\n  # We allow non-const references in a few standard places, like functions\n  # called ""swap()"" or iostream operators like ""<<"" or "">>"".  Do not check\n  # those function parameters.\n  #\n  # We also accept & in static_assert, which looks like a function but\n  # it\'s actually a declaration expression.\n  whitelisted_functions = (r\'(?:[sS]wap(?:<\\w:+>)?|\'\n                           r\'operator\\s*[<>][<>]|\'\n                           r\'static_assert|COMPILE_ASSERT\'\n                           r\')\\s*\\(\')\n  if Search(whitelisted_functions, line):\n    return\n  elif not Search(r\'\\S+\\([^)]*$\', line):\n    # Don\'t see a whitelisted function on this line.  Actually we\n    # didn\'t see any function name on this line, so this is likely a\n    # multi-line parameter list.  Try a bit harder to catch this case.\n    for i in xrange(2):\n      if (linenum > i and\n          Search(whitelisted_functions, clean_lines.elided[linenum - i - 1])):\n        return\n\n  decls = ReplaceAll(r\'{[^}]*}\', \' \', line)  # exclude function body\n  for parameter in re.findall(_RE_PATTERN_REF_PARAM, decls):\n    if not Match(_RE_PATTERN_CONST_REF_PARAM, parameter):\n      error(filename, linenum, \'runtime/references\', 2,\n            \'Is this a non-const reference? \'\n            \'If so, make const or use a pointer: \' +\n            ReplaceAll(\' *<\', \'<\', parameter))\n\n\ndef CheckCasts(filename, clean_lines, linenum, error):\n  """"""Various cast related checks.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n\n  # Check to see if they\'re using an conversion function cast.\n  # I just try to capture the most common basic types, though there are more.\n  # Parameterless conversion functions, such as bool(), are allowed as they are\n  # probably a member operator declaration or default constructor.\n  match = Search(\n      r\'(\\bnew\\s+|\\S<\\s*(?:const\\s+)?)?\\b\'\n      r\'(int|float|double|bool|char|int32|uint32|int64|uint64)\'\n      r\'(\\([^)].*)\', line)\n  expecting_function = ExpectingFunctionArgs(clean_lines, linenum)\n  if match and not expecting_function:\n    matched_type = match.group(2)\n\n    # matched_new_or_template is used to silence two false positives:\n    # - New operators\n    # - Template arguments with function types\n    #\n    # For template arguments, we match on types immediately following\n    # an opening bracket without any spaces.  This is a fast way to\n    # silence the common case where the function type is the first\n    # template argument.  False negative with less-than comparison is\n    # avoided because those operators are usually followed by a space.\n    #\n    #   function<double(double)>   // bracket + no space = false positive\n    #   value < double(42)         // bracket + space = true positive\n    matched_new_or_template = match.group(1)\n\n    # Avoid arrays by looking for brackets that come after the closing\n    # parenthesis.\n    if Match(r\'\\([^()]+\\)\\s*\\[\', match.group(3)):\n      return\n\n    # Other things to ignore:\n    # - Function pointers\n    # - Casts to pointer types\n    # - Placement new\n    # - Alias declarations\n    matched_funcptr = match.group(3)\n    if (matched_new_or_template is None and\n        not (matched_funcptr and\n             (Match(r\'\\((?:[^() ]+::\\s*\\*\\s*)?[^() ]+\\)\\s*\\(\',\n                    matched_funcptr) or\n              matched_funcptr.startswith(\'(*)\'))) and\n        not Match(r\'\\s*using\\s+\\S+\\s*=\\s*\' + matched_type, line) and\n        not Search(r\'new\\(\\S+\\)\\s*\' + matched_type, line)):\n      error(filename, linenum, \'readability/casting\', 4,\n            \'Using deprecated casting style.  \'\n            \'Use static_cast<%s>(...) instead\' %\n            matched_type)\n\n  if not expecting_function:\n    CheckCStyleCast(filename, clean_lines, linenum, \'static_cast\',\n                    r\'\\((int|float|double|bool|char|u?int(16|32|64))\\)\', error)\n\n  # This doesn\'t catch all cases. Consider (const char * const)""hello"".\n  #\n  # (char *) ""foo"" should always be a const_cast (reinterpret_cast won\'t\n  # compile).\n  if CheckCStyleCast(filename, clean_lines, linenum, \'const_cast\',\n                     r\'\\((char\\s?\\*+\\s?)\\)\\s*""\', error):\n    pass\n  else:\n    # Check pointer casts for other than string constants\n    CheckCStyleCast(filename, clean_lines, linenum, \'reinterpret_cast\',\n                    r\'\\((\\w+\\s?\\*+\\s?)\\)\', error)\n\n  # In addition, we look for people taking the address of a cast.  This\n  # is dangerous -- casts can assign to temporaries, so the pointer doesn\'t\n  # point where you think.\n  #\n  # Some non-identifier character is required before the \'&\' for the\n  # expression to be recognized as a cast.  These are casts:\n  #   expression = &static_cast<int*>(temporary());\n  #   function(&(int*)(temporary()));\n  #\n  # This is not a cast:\n  #   reference_type&(int* function_param);\n  match = Search(\n      r\'(?:[^\\w]&\\(([^)*][^)]*)\\)[\\w(])|\'\n      r\'(?:[^\\w]&(static|dynamic|down|reinterpret)_cast\\b)\', line)\n  if match:\n    # Try a better error message when the & is bound to something\n    # dereferenced by the casted pointer, as opposed to the casted\n    # pointer itself.\n    parenthesis_error = False\n    match = Match(r\'^(.*&(?:static|dynamic|down|reinterpret)_cast\\b)<\', line)\n    if match:\n      _, y1, x1 = CloseExpression(clean_lines, linenum, len(match.group(1)))\n      if x1 >= 0 and clean_lines.elided[y1][x1] == \'(\':\n        _, y2, x2 = CloseExpression(clean_lines, y1, x1)\n        if x2 >= 0:\n          extended_line = clean_lines.elided[y2][x2:]\n          if y2 < clean_lines.NumLines() - 1:\n            extended_line += clean_lines.elided[y2 + 1]\n          if Match(r\'\\s*(?:->|\\[)\', extended_line):\n            parenthesis_error = True\n\n    if parenthesis_error:\n      error(filename, linenum, \'readability/casting\', 4,\n            (\'Are you taking an address of something dereferenced \'\n             \'from a cast?  Wrapping the dereferenced expression in \'\n             \'parentheses will make the binding more obvious\'))\n    else:\n      error(filename, linenum, \'runtime/casting\', 4,\n            (\'Are you taking an address of a cast?  \'\n             \'This is dangerous: could be a temp var.  \'\n             \'Take the address before doing the cast, rather than after\'))\n\n\ndef CheckCStyleCast(filename, clean_lines, linenum, cast_type, pattern, error):\n  """"""Checks for a C-style cast by looking for the pattern.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    cast_type: The string for the C++ cast to recommend.  This is either\n      reinterpret_cast, static_cast, or const_cast, depending.\n    pattern: The regular expression used to find C-style casts.\n    error: The function to call with any errors found.\n\n  Returns:\n    True if an error was emitted.\n    False otherwise.\n  """"""\n  line = clean_lines.elided[linenum]\n  match = Search(pattern, line)\n  if not match:\n    return False\n\n  # Exclude lines with keywords that tend to look like casts\n  context = line[0:match.start(1) - 1]\n  if Match(r\'.*\\b(?:sizeof|alignof|alignas|[_A-Z][_A-Z0-9]*)\\s*$\', context):\n    return False\n\n  # Try expanding current context to see if we one level of\n  # parentheses inside a macro.\n  if linenum > 0:\n    for i in xrange(linenum - 1, max(0, linenum - 5), -1):\n      context = clean_lines.elided[i] + context\n  if Match(r\'.*\\b[_A-Z][_A-Z0-9]*\\s*\\((?:\\([^()]*\\)|[^()])*$\', context):\n    return False\n\n  # operator++(int) and operator--(int)\n  if context.endswith(\' operator++\') or context.endswith(\' operator--\'):\n    return False\n\n  # A single unnamed argument for a function tends to look like old\n  # style cast.  If we see those, don\'t issue warnings for deprecated\n  # casts, instead issue warnings for unnamed arguments where\n  # appropriate.\n  #\n  # These are things that we want warnings for, since the style guide\n  # explicitly require all parameters to be named:\n  #   Function(int);\n  #   Function(int) {\n  #   ConstMember(int) const;\n  #   ConstMember(int) const {\n  #   ExceptionMember(int) throw (...);\n  #   ExceptionMember(int) throw (...) {\n  #   PureVirtual(int) = 0;\n  #   [](int) -> bool {\n  #\n  # These are functions of some sort, where the compiler would be fine\n  # if they had named parameters, but people often omit those\n  # identifiers to reduce clutter:\n  #   (FunctionPointer)(int);\n  #   (FunctionPointer)(int) = value;\n  #   Function((function_pointer_arg)(int))\n  #   Function((function_pointer_arg)(int), int param)\n  #   <TemplateArgument(int)>;\n  #   <(FunctionPointerTemplateArgument)(int)>;\n  remainder = line[match.end(0):]\n  if Match(r\'^\\s*(?:;|const\\b|throw\\b|final\\b|override\\b|[=>{),]|->)\',\n           remainder):\n    # Looks like an unnamed parameter.\n\n    # Don\'t warn on any kind of template arguments.\n    if Match(r\'^\\s*>\', remainder):\n      return False\n\n    # Don\'t warn on assignments to function pointers, but keep warnings for\n    # unnamed parameters to pure virtual functions.  Note that this pattern\n    # will also pass on assignments of ""0"" to function pointers, but the\n    # preferred values for those would be ""nullptr"" or ""NULL"".\n    matched_zero = Match(r\'^\\s=\\s*(\\S+)\\s*;\', remainder)\n    if matched_zero and matched_zero.group(1) != \'0\':\n      return False\n\n    # Don\'t warn on function pointer declarations.  For this we need\n    # to check what came before the ""(type)"" string.\n    if Match(r\'.*\\)\\s*$\', line[0:match.start(0)]):\n      return False\n\n    # Don\'t warn if the parameter is named with block comments, e.g.:\n    #  Function(int /*unused_param*/);\n    raw_line = clean_lines.raw_lines[linenum]\n    if \'/*\' in raw_line:\n      return False\n\n    # Passed all filters, issue warning here.\n    error(filename, linenum, \'readability/function\', 3,\n          \'All parameters should be named in a function\')\n    return True\n\n  # At this point, all that should be left is actual casts.\n  error(filename, linenum, \'readability/casting\', 4,\n        \'Using C-style cast.  Use %s<%s>(...) instead\' %\n        (cast_type, match.group(1)))\n\n  return True\n\n\ndef ExpectingFunctionArgs(clean_lines, linenum):\n  """"""Checks whether where function type arguments are expected.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n\n  Returns:\n    True if the line at \'linenum\' is inside something that expects arguments\n    of function types.\n  """"""\n  line = clean_lines.elided[linenum]\n  return (Match(r\'^\\s*MOCK_(CONST_)?METHOD\\d+(_T)?\\(\', line) or\n          (linenum >= 2 and\n           (Match(r\'^\\s*MOCK_(?:CONST_)?METHOD\\d+(?:_T)?\\((?:\\S+,)?\\s*$\',\n                  clean_lines.elided[linenum - 1]) or\n            Match(r\'^\\s*MOCK_(?:CONST_)?METHOD\\d+(?:_T)?\\(\\s*$\',\n                  clean_lines.elided[linenum - 2]) or\n            Search(r\'\\bstd::m?function\\s*\\<\\s*$\',\n                   clean_lines.elided[linenum - 1]))))\n\n\n_HEADERS_CONTAINING_TEMPLATES = (\n    (\'<deque>\', (\'deque\',)),\n    (\'<functional>\', (\'unary_function\', \'binary_function\',\n                      \'plus\', \'minus\', \'multiplies\', \'divides\', \'modulus\',\n                      \'negate\',\n                      \'equal_to\', \'not_equal_to\', \'greater\', \'less\',\n                      \'greater_equal\', \'less_equal\',\n                      \'logical_and\', \'logical_or\', \'logical_not\',\n                      \'unary_negate\', \'not1\', \'binary_negate\', \'not2\',\n                      \'bind1st\', \'bind2nd\',\n                      \'pointer_to_unary_function\',\n                      \'pointer_to_binary_function\',\n                      \'ptr_fun\',\n                      \'mem_fun_t\', \'mem_fun\', \'mem_fun1_t\', \'mem_fun1_ref_t\',\n                      \'mem_fun_ref_t\',\n                      \'const_mem_fun_t\', \'const_mem_fun1_t\',\n                      \'const_mem_fun_ref_t\', \'const_mem_fun1_ref_t\',\n                      \'mem_fun_ref\',\n                     )),\n    (\'<limits>\', (\'numeric_limits\',)),\n    (\'<list>\', (\'list\',)),\n    (\'<map>\', (\'map\', \'multimap\',)),\n    (\'<memory>\', (\'allocator\',)),\n    (\'<queue>\', (\'queue\', \'priority_queue\',)),\n    (\'<set>\', (\'set\', \'multiset\',)),\n    (\'<stack>\', (\'stack\',)),\n    (\'<string>\', (\'char_traits\', \'basic_string\',)),\n    (\'<tuple>\', (\'tuple\',)),\n    (\'<utility>\', (\'pair\',)),\n    (\'<vector>\', (\'vector\',)),\n\n    # gcc extensions.\n    # Note: std::hash is their hash, ::hash is our hash\n    (\'<hash_map>\', (\'hash_map\', \'hash_multimap\',)),\n    (\'<hash_set>\', (\'hash_set\', \'hash_multiset\',)),\n    (\'<slist>\', (\'slist\',)),\n    )\n\n_RE_PATTERN_STRING = re.compile(r\'\\bstring\\b\')\n\n_re_pattern_algorithm_header = []\nfor _template in (\'copy\', \'max\', \'min\', \'min_element\', \'sort\', \'swap\',\n                  \'transform\'):\n  # Match max<type>(..., ...), max(..., ...), but not foo->max, foo.max or\n  # type::max().\n  _re_pattern_algorithm_header.append(\n      (re.compile(r\'[^>.]\\b\' + _template + r\'(<.*?>)?\\([^\\)]\'),\n       _template,\n       \'<algorithm>\'))\n\n_re_pattern_templates = []\nfor _header, _templates in _HEADERS_CONTAINING_TEMPLATES:\n  for _template in _templates:\n    _re_pattern_templates.append(\n        (re.compile(r\'(\\<|\\b)\' + _template + r\'\\s*\\<\'),\n         _template + \'<>\',\n         _header))\n\n\ndef FilesBelongToSameModule(filename_cc, filename_h):\n  """"""Check if these two filenames belong to the same module.\n\n  The concept of a \'module\' here is a as follows:\n  foo.h, foo-inl.h, foo.cc, foo_test.cc and foo_unittest.cc belong to the\n  same \'module\' if they are in the same directory.\n  some/path/public/xyzzy and some/path/internal/xyzzy are also considered\n  to belong to the same module here.\n\n  If the filename_cc contains a longer path than the filename_h, for example,\n  \'/absolute/path/to/base/sysinfo.cc\', and this file would include\n  \'base/sysinfo.h\', this function also produces the prefix needed to open the\n  header. This is used by the caller of this function to more robustly open the\n  header file. We don\'t have access to the real include paths in this context,\n  so we need this guesswork here.\n\n  Known bugs: tools/base/bar.cc and base/bar.h belong to the same module\n  according to this implementation. Because of this, this function gives\n  some false positives. This should be sufficiently rare in practice.\n\n  Args:\n    filename_cc: is the path for the .cc file\n    filename_h: is the path for the header path\n\n  Returns:\n    Tuple with a bool and a string:\n    bool: True if filename_cc and filename_h belong to the same module.\n    string: the additional prefix needed to open the header file.\n  """"""\n\n  if not filename_cc.endswith(\'.cc\'):\n    return (False, \'\')\n  filename_cc = filename_cc[:-len(\'.cc\')]\n  if filename_cc.endswith(\'_unittest\'):\n    filename_cc = filename_cc[:-len(\'_unittest\')]\n  elif filename_cc.endswith(\'_test\'):\n    filename_cc = filename_cc[:-len(\'_test\')]\n  filename_cc = filename_cc.replace(\'/public/\', \'/\')\n  filename_cc = filename_cc.replace(\'/internal/\', \'/\')\n\n  if not filename_h.endswith(\'.h\'):\n    return (False, \'\')\n  filename_h = filename_h[:-len(\'.h\')]\n  if filename_h.endswith(\'-inl\'):\n    filename_h = filename_h[:-len(\'-inl\')]\n  filename_h = filename_h.replace(\'/public/\', \'/\')\n  filename_h = filename_h.replace(\'/internal/\', \'/\')\n\n  files_belong_to_same_module = filename_cc.endswith(filename_h)\n  common_path = \'\'\n  if files_belong_to_same_module:\n    common_path = filename_cc[:-len(filename_h)]\n  return files_belong_to_same_module, common_path\n\n\ndef UpdateIncludeState(filename, include_dict, io=codecs):\n  """"""Fill up the include_dict with new includes found from the file.\n\n  Args:\n    filename: the name of the header to read.\n    include_dict: a dictionary in which the headers are inserted.\n    io: The io factory to use to read the file. Provided for testability.\n\n  Returns:\n    True if a header was successfully added. False otherwise.\n  """"""\n  headerfile = None\n  try:\n    headerfile = io.open(filename, \'r\', \'utf8\', \'replace\')\n  except IOError:\n    return False\n  linenum = 0\n  for line in headerfile:\n    linenum += 1\n    clean_line = CleanseComments(line)\n    match = _RE_PATTERN_INCLUDE.search(clean_line)\n    if match:\n      include = match.group(2)\n      include_dict.setdefault(include, linenum)\n  return True\n\n\ndef CheckForIncludeWhatYouUse(filename, clean_lines, include_state, error,\n                              io=codecs):\n  """"""Reports for missing stl includes.\n\n  This function will output warnings to make sure you are including the headers\n  necessary for the stl containers and functions that you use. We only give one\n  reason to include a header. For example, if you use both equal_to<> and\n  less<> in a .h file, only one (the latter in the file) of these will be\n  reported as a reason to include the <functional>.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    include_state: An _IncludeState instance.\n    error: The function to call with any errors found.\n    io: The IO factory to use to read the header file. Provided for unittest\n        injection.\n  """"""\n  required = {}  # A map of header name to linenumber and the template entity.\n                 # Example of required: { \'<functional>\': (1219, \'less<>\') }\n\n  for linenum in xrange(clean_lines.NumLines()):\n    line = clean_lines.elided[linenum]\n    if not line or line[0] == \'#\':\n      continue\n\n    # String is special -- it is a non-templatized type in STL.\n    matched = _RE_PATTERN_STRING.search(line)\n    if matched:\n      # Don\'t warn about strings in non-STL namespaces:\n      # (We check only the first match per line; good enough.)\n      prefix = line[:matched.start()]\n      if prefix.endswith(\'std::\') or not prefix.endswith(\'::\'):\n        required[\'<string>\'] = (linenum, \'string\')\n\n    for pattern, template, header in _re_pattern_algorithm_header:\n      if pattern.search(line):\n        required[header] = (linenum, template)\n\n    # The following function is just a speed up, no semantics are changed.\n    if not \'<\' in line:  # Reduces the cpu time usage by skipping lines.\n      continue\n\n    for pattern, template, header in _re_pattern_templates:\n      if pattern.search(line):\n        required[header] = (linenum, template)\n\n  # The policy is that if you #include something in foo.h you don\'t need to\n  # include it again in foo.cc. Here, we will look at possible includes.\n  # Let\'s flatten the include_state include_list and copy it into a dictionary.\n  include_dict = dict([item for sublist in include_state.include_list\n                       for item in sublist])\n\n  # Did we find the header for this file (if any) and successfully load it?\n  header_found = False\n\n  # Use the absolute path so that matching works properly.\n  abs_filename = FileInfo(filename).FullName()\n\n  # For Emacs\'s flymake.\n  # If cpplint is invoked from Emacs\'s flymake, a temporary file is generated\n  # by flymake and that file name might end with \'_flymake.cc\'. In that case,\n  # restore original file name here so that the corresponding header file can be\n  # found.\n  # e.g. If the file name is \'foo_flymake.cc\', we should search for \'foo.h\'\n  # instead of \'foo_flymake.h\'\n  abs_filename = re.sub(r\'_flymake\\.cc$\', \'.cc\', abs_filename)\n\n  # include_dict is modified during iteration, so we iterate over a copy of\n  # the keys.\n  header_keys = include_dict.keys()\n  for header in header_keys:\n    (same_module, common_path) = FilesBelongToSameModule(abs_filename, header)\n    fullpath = common_path + header\n    if same_module and UpdateIncludeState(fullpath, include_dict, io):\n      header_found = True\n\n  # If we can\'t find the header file for a .cc, assume it\'s because we don\'t\n  # know where to look. In that case we\'ll give up as we\'re not sure they\n  # didn\'t include it in the .h file.\n  # TODO(unknown): Do a better job of finding .h files so we are confident that\n  # not having the .h file means there isn\'t one.\n  if filename.endswith(\'.cc\') and not header_found:\n    return\n\n  # All the lines have been processed, report the errors found.\n  for required_header_unstripped in required:\n    template = required[required_header_unstripped][1]\n    if required_header_unstripped.strip(\'<>""\') not in include_dict:\n      error(filename, required[required_header_unstripped][0],\n            \'build/include_what_you_use\', 4,\n            \'Add #include \' + required_header_unstripped + \' for \' + template)\n\n\n_RE_PATTERN_EXPLICIT_MAKEPAIR = re.compile(r\'\\bmake_pair\\s*<\')\n\n\ndef CheckMakePairUsesDeduction(filename, clean_lines, linenum, error):\n  """"""Check that make_pair\'s template arguments are deduced.\n\n  G++ 4.6 in C++11 mode fails badly if make_pair\'s template arguments are\n  specified explicitly, and such use isn\'t intended in any case.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n  match = _RE_PATTERN_EXPLICIT_MAKEPAIR.search(line)\n  if match:\n    error(filename, linenum, \'build/explicit_make_pair\',\n          4,  # 4 = high confidence\n          \'For C++11-compatibility, omit template arguments from make_pair\'\n          \' OR use pair directly OR if appropriate, construct a pair directly\')\n\n\ndef CheckDefaultLambdaCaptures(filename, clean_lines, linenum, error):\n  """"""Check that default lambda captures are not used.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n\n  # A lambda introducer specifies a default capture if it starts with ""[=""\n  # or if it starts with ""[&"" _not_ followed by an identifier.\n  match = Match(r\'^(.*)\\[\\s*(?:=|&[^\\w])\', line)\n  if match:\n    # Found a potential error, check what comes after the lambda-introducer.\n    # If it\'s not open parenthesis (for lambda-declarator) or open brace\n    # (for compound-statement), it\'s not a lambda.\n    line, _, pos = CloseExpression(clean_lines, linenum, len(match.group(1)))\n    if pos >= 0 and Match(r\'^\\s*[{(]\', line[pos:]):\n      error(filename, linenum, \'build/c++11\',\n            4,  # 4 = high confidence\n            \'Default lambda captures are an unapproved C++ feature.\')\n\n\ndef CheckRedundantVirtual(filename, clean_lines, linenum, error):\n  """"""Check if line contains a redundant ""virtual"" function-specifier.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  # Look for ""virtual"" on current line.\n  line = clean_lines.elided[linenum]\n  virtual = Match(r\'^(.*)(\\bvirtual\\b)(.*)$\', line)\n  if not virtual: return\n\n  # Ignore ""virtual"" keywords that are near access-specifiers.  These\n  # are only used in class base-specifier and do not apply to member\n  # functions.\n  if (Search(r\'\\b(public|protected|private)\\s+$\', virtual.group(1)) or\n      Match(r\'^\\s+(public|protected|private)\\b\', virtual.group(3))):\n    return\n\n  # Ignore the ""virtual"" keyword from virtual base classes.  Usually\n  # there is a column on the same line in these cases (virtual base\n  # classes are rare in google3 because multiple inheritance is rare).\n  if Match(r\'^.*[^:]:[^:].*$\', line): return\n\n  # Look for the next opening parenthesis.  This is the start of the\n  # parameter list (possibly on the next line shortly after virtual).\n  # TODO(unknown): doesn\'t work if there are virtual functions with\n  # decltype() or other things that use parentheses, but csearch suggests\n  # that this is rare.\n  end_col = -1\n  end_line = -1\n  start_col = len(virtual.group(2))\n  for start_line in xrange(linenum, min(linenum + 3, clean_lines.NumLines())):\n    line = clean_lines.elided[start_line][start_col:]\n    parameter_list = Match(r\'^([^(]*)\\(\', line)\n    if parameter_list:\n      # Match parentheses to find the end of the parameter list\n      (_, end_line, end_col) = CloseExpression(\n          clean_lines, start_line, start_col + len(parameter_list.group(1)))\n      break\n    start_col = 0\n\n  if end_col < 0:\n    return  # Couldn\'t find end of parameter list, give up\n\n  # Look for ""override"" or ""final"" after the parameter list\n  # (possibly on the next few lines).\n  for i in xrange(end_line, min(end_line + 3, clean_lines.NumLines())):\n    line = clean_lines.elided[i][end_col:]\n    match = Search(r\'\\b(override|final)\\b\', line)\n    if match:\n      error(filename, linenum, \'readability/inheritance\', 4,\n            (\'""virtual"" is redundant since function is \'\n             \'already declared as ""%s""\' % match.group(1)))\n\n    # Set end_col to check whole lines after we are done with the\n    # first line.\n    end_col = 0\n    if Search(r\'[^\\w]\\s*$\', line):\n      break\n\n\ndef CheckRedundantOverrideOrFinal(filename, clean_lines, linenum, error):\n  """"""Check if line contains a redundant ""override"" or ""final"" virt-specifier.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  # Look for closing parenthesis nearby.  We need one to confirm where\n  # the declarator ends and where the virt-specifier starts to avoid\n  # false positives.\n  line = clean_lines.elided[linenum]\n  declarator_end = line.rfind(\')\')\n  if declarator_end >= 0:\n    fragment = line[declarator_end:]\n  else:\n    if linenum > 1 and clean_lines.elided[linenum - 1].rfind(\')\') >= 0:\n      fragment = line\n    else:\n      return\n\n  # Check that at most one of ""override"" or ""final"" is present, not both\n  if Search(r\'\\boverride\\b\', fragment) and Search(r\'\\bfinal\\b\', fragment):\n    error(filename, linenum, \'readability/inheritance\', 4,\n          (\'""override"" is redundant since function is \'\n           \'already declared as ""final""\'))\n\n\n\n\n# Returns true if we are at a new block, and it is directly\n# inside of a namespace.\ndef IsBlockInNameSpace(nesting_state, is_forward_declaration):\n  """"""Checks that the new block is directly in a namespace.\n\n  Args:\n    nesting_state: The _NestingState object that contains info about our state.\n    is_forward_declaration: If the class is a forward declared class.\n  Returns:\n    Whether or not the new block is directly in a namespace.\n  """"""\n  if is_forward_declaration:\n    if len(nesting_state.stack) >= 1 and (\n        isinstance(nesting_state.stack[-1], _NamespaceInfo)):\n      return True\n    else:\n      return False\n\n  return (len(nesting_state.stack) > 1 and\n          nesting_state.stack[-1].check_namespace_indentation and\n          isinstance(nesting_state.stack[-2], _NamespaceInfo))\n\n\ndef ShouldCheckNamespaceIndentation(nesting_state, is_namespace_indent_item,\n                                    raw_lines_no_comments, linenum):\n  """"""This method determines if we should apply our namespace indentation check.\n\n  Args:\n    nesting_state: The current nesting state.\n    is_namespace_indent_item: If we just put a new class on the stack, True.\n      If the top of the stack is not a class, or we did not recently\n      add the class, False.\n    raw_lines_no_comments: The lines without the comments.\n    linenum: The current line number we are processing.\n\n  Returns:\n    True if we should apply our namespace indentation check. Currently, it\n    only works for classes and namespaces inside of a namespace.\n  """"""\n\n  is_forward_declaration = IsForwardClassDeclaration(raw_lines_no_comments,\n                                                     linenum)\n\n  if not (is_namespace_indent_item or is_forward_declaration):\n    return False\n\n  # If we are in a macro, we do not want to check the namespace indentation.\n  if IsMacroDefinition(raw_lines_no_comments, linenum):\n    return False\n\n  return IsBlockInNameSpace(nesting_state, is_forward_declaration)\n\n\n# Call this method if the line is directly inside of a namespace.\n# If the line above is blank (excluding comments) or the start of\n# an inner namespace, it cannot be indented.\ndef CheckItemIndentationInNamespace(filename, raw_lines_no_comments, linenum,\n                                    error):\n  line = raw_lines_no_comments[linenum]\n  if Match(r\'^\\s+\', line):\n    error(filename, linenum, \'runtime/indentation_namespace\', 4,\n          \'Do not indent within a namespace\')\n\n\ndef ProcessLine(filename, file_extension, clean_lines, line,\n                include_state, function_state, nesting_state, error,\n                extra_check_functions=[]):\n  """"""Processes a single line in the file.\n\n  Args:\n    filename: Filename of the file that is being processed.\n    file_extension: The extension (dot not included) of the file.\n    clean_lines: An array of strings, each representing a line of the file,\n                 with comments stripped.\n    line: Number of line being processed.\n    include_state: An _IncludeState instance in which the headers are inserted.\n    function_state: A _FunctionState instance which counts function lines, etc.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: A callable to which errors are reported, which takes 4 arguments:\n           filename, line number, error level, and message\n    extra_check_functions: An array of additional check functions that will be\n                           run on each source line. Each function takes 4\n                           arguments: filename, clean_lines, line, error\n  """"""\n  raw_lines = clean_lines.raw_lines\n  ParseNolintSuppressions(filename, raw_lines[line], line, error)\n  nesting_state.Update(filename, clean_lines, line, error)\n  CheckForNamespaceIndentation(filename, nesting_state, clean_lines, line,\n                               error)\n  if nesting_state.InAsmBlock(): return\n  CheckForFunctionLengths(filename, clean_lines, line, function_state, error)\n  CheckForMultilineCommentsAndStrings(filename, clean_lines, line, error)\n  CheckStyle(filename, clean_lines, line, file_extension, nesting_state, error)\n  CheckLanguage(filename, clean_lines, line, file_extension, include_state,\n                nesting_state, error)\n  CheckForNonConstReference(filename, clean_lines, line, nesting_state, error)\n  CheckForNonStandardConstructs(filename, clean_lines, line,\n                                nesting_state, error)\n  CheckVlogArguments(filename, clean_lines, line, error)\n  CheckPosixThreading(filename, clean_lines, line, error)\n  CheckInvalidIncrement(filename, clean_lines, line, error)\n  CheckMakePairUsesDeduction(filename, clean_lines, line, error)\n  CheckDefaultLambdaCaptures(filename, clean_lines, line, error)\n  CheckRedundantVirtual(filename, clean_lines, line, error)\n  CheckRedundantOverrideOrFinal(filename, clean_lines, line, error)\n  for check_fn in extra_check_functions:\n    check_fn(filename, clean_lines, line, error)\n\ndef FlagCxx11Features(filename, clean_lines, linenum, error):\n  """"""Flag those c++11 features that we only allow in certain places.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  """"""\n  line = clean_lines.elided[linenum]\n\n  # Note:In Heron, we don\'t have restrictions for using these libs below, hence commenting out this section\n\n  # Flag unapproved C++11 headers.\n  # include = Match(r\'\\s*#\\s*include\\s+[<""]([^<""]+)["">]\', line)\n  # if include and include.group(1) in (\'cfenv\',\n  #                                   \'condition_variable\',\n  #                                   \'fenv.h\',\n  #                                   \'future\',\n  #                                   \'mutex\',\n  #                                   \'thread\',\n  #                                   \'chrono\',\n  #                                   \'ratio\',\n  #                                   \'regex\',\n  #                                   \'system_error\',\n  #                                  ):\n  #  error(filename, linenum, \'build/c++11\', 5,\n  #        (\'<%s> is an unapproved C++11 header.\') % include.group(1))\n\n  # The only place where we need to worry about C++11 keywords and library\n  # features in preprocessor directives is in macro definitions.\n  if Match(r\'\\s*#\', line) and not Match(r\'\\s*#\\s*define\\b\', line): return\n\n  # These are classes and free functions.  The classes are always\n  # mentioned as std::*, but we only catch the free functions if\n  # they\'re not found by ADL.  They\'re alphabetical by header.\n  for top_name in (\n      # type_traits\n      \'alignment_of\',\n      \'aligned_union\',\n      ):\n    if Search(r\'\\bstd::%s\\b\' % top_name, line):\n      error(filename, linenum, \'build/c++11\', 5,\n            (\'std::%s is an unapproved C++11 class or function.  Send c-style \'\n             \'an example of where it would make your code more readable, and \'\n             \'they may let you use it.\') % top_name)\n\n\ndef ProcessFileData(filename, file_extension, lines, error,\n                    extra_check_functions=[]):\n  """"""Performs lint checks and reports any errors to the given error function.\n\n  Args:\n    filename: Filename of the file that is being processed.\n    file_extension: The extension (dot not included) of the file.\n    lines: An array of strings, each representing a line of the file, with the\n           last element being empty if the file is terminated with a newline.\n    error: A callable to which errors are reported, which takes 4 arguments:\n           filename, line number, error level, and message\n    extra_check_functions: An array of additional check functions that will be\n                           run on each source line. Each function takes 4\n                           arguments: filename, clean_lines, line, error\n  """"""\n  lines = ([\'// marker so line numbers and indices both start at 1\'] + lines +\n           [\'// marker so line numbers end in a known way\'])\n\n  include_state = _IncludeState()\n  function_state = _FunctionState()\n  nesting_state = NestingState()\n\n  ResetNolintSuppressions()\n\n  CheckForCopyright(filename, lines, error)\n\n  RemoveMultiLineComments(filename, lines, error)\n  clean_lines = CleansedLines(lines)\n\n  if file_extension == \'h\':\n    CheckForHeaderGuard(filename, clean_lines, error)\n\n  for line in xrange(clean_lines.NumLines()):\n    ProcessLine(filename, file_extension, clean_lines, line,\n                include_state, function_state, nesting_state, error,\n                extra_check_functions)\n    FlagCxx11Features(filename, clean_lines, line, error)\n  nesting_state.CheckCompletedBlocks(filename, error)\n\n  CheckForIncludeWhatYouUse(filename, clean_lines, include_state, error)\n  \n  # Check that the .cc file has included its header if it exists.\n  if file_extension == \'cc\':\n    CheckHeaderFileIncluded(filename, include_state, error)\n\n  # We check here rather than inside ProcessLine so that we see raw\n  # lines rather than ""cleaned"" lines.\n  CheckForBadCharacters(filename, lines, error)\n\n  CheckForNewlineAtEOF(filename, lines, error)\n\ndef ProcessConfigOverrides(filename):\n  """""" Loads the configuration files and processes the config overrides.\n\n  Args:\n    filename: The name of the file being processed by the linter.\n\n  Returns:\n    False if the current |filename| should not be processed further.\n  """"""\n\n  abs_filename = os.path.abspath(filename)\n  cfg_filters = []\n  keep_looking = True\n  while keep_looking:\n    abs_path, base_name = os.path.split(abs_filename)\n    if not base_name:\n      break  # Reached the root directory.\n\n    cfg_file = os.path.join(abs_path, ""CPPLINT.cfg"")\n    abs_filename = abs_path\n    if not os.path.isfile(cfg_file):\n      continue\n    \n    file_handle = open(cfg_file)\n    try:\n      try:\n        for line in file_handle:\n          line, _, _ = line.partition(\'#\')  # Remove comments.\n          if not line.strip():\n            continue\n    \n          name, _, val = line.partition(\'=\')\n          name = name.strip()\n          val = val.strip()\n          if name == \'set noparent\':\n            keep_looking = False\n          elif name == \'filter\':\n            cfg_filters.append(val)\n          elif name == \'exclude_files\':\n            # When matching exclude_files pattern, use the base_name of\n            # the current file name or the directory name we are processing.\n            # For example, if we are checking for lint errors in /foo/bar/baz.cc\n            # and we found the .cfg file at /foo/CPPLINT.cfg, then the config\n            # file\'s ""exclude_files"" filter is meant to be checked against ""bar""\n            # and not ""baz"" nor ""bar/baz.cc"".\n            if base_name:\n              pattern = re.compile(val)\n              if pattern.match(base_name):\n                sys.stderr.write(\'Ignoring ""%s"": file excluded by ""%s"". \'\n                                 \'File path component ""%s"" matches \'\n                                 \'pattern ""%s""\\n\' %\n                                 (filename, cfg_file, base_name, val))\n                return False\n          elif name == \'linelength\':\n            global _line_length\n            try:\n                _line_length = int(val)\n            except ValueError:\n                sys.stderr.write(\'Line length must be numeric.\')\n          else:\n            sys.stderr.write(\n                \'Invalid configuration option (%s) in file %s\\n\' %\n                (name, cfg_file))\n    \n      except IOError:\n        sys.stderr.write(\n          ""Skipping config file \'%s\': Can\'t open for reading\\n"" % cfg_file)\n        keep_looking = False\n    finally:\n      file_handle.close()\n\n  # Apply all the accumulated filters in reverse order (top-level directory\n  # config options having the least priority).\n  for filter in reversed(cfg_filters):\n     _AddFilters(filter)\n\n  return True\n\n\ndef ProcessFile(filename, vlevel, extra_check_functions=[]):\n  """"""Does google-lint on a single file.\n\n  Args:\n    filename: The name of the file to parse.\n\n    vlevel: The level of errors to report.  Every error of confidence\n    >= verbose_level will be reported.  0 is a good default.\n\n    extra_check_functions: An array of additional check functions that will be\n                           run on each source line. Each function takes 4\n                           arguments: filename, clean_lines, line, error\n  """"""\n\n  _SetVerboseLevel(vlevel)\n  _BackupFilters()\n\n  if not ProcessConfigOverrides(filename):\n    _RestoreFilters()\n    return\n\n  lf_lines = []\n  crlf_lines = []\n  try:\n    # Support the UNIX convention of using ""-"" for stdin.  Note that\n    # we are not opening the file with universal newline support\n    # (which codecs doesn\'t support anyway), so the resulting lines do\n    # contain trailing \'\\r\' characters if we are reading a file that\n    # has CRLF endings.\n    # If after the split a trailing \'\\r\' is present, it is removed\n    # below.\n    if filename == \'-\':\n      lines = codecs.StreamReaderWriter(sys.stdin,\n                                        codecs.getreader(\'utf8\'),\n                                        codecs.getwriter(\'utf8\'),\n                                        \'replace\').read().split(\'\\n\')\n    else:\n      lines = codecs.open(filename, \'r\', \'utf8\', \'replace\').read().split(\'\\n\')\n\n    # Remove trailing \'\\r\'.\n    # The -1 accounts for the extra trailing blank line we get from split()\n    for linenum in range(len(lines) - 1):\n      if lines[linenum].endswith(\'\\r\'):\n        lines[linenum] = lines[linenum].rstrip(\'\\r\')\n        crlf_lines.append(linenum + 1)\n      else:\n        lf_lines.append(linenum + 1)\n\n  except IOError:\n    sys.stderr.write(\n        ""Skipping input \'%s\': Can\'t open for reading\\n"" % filename)\n    _RestoreFilters()\n    return\n\n  # Note, if no dot is found, this will give the entire filename as the ext.\n  file_extension = filename[filename.rfind(\'.\') + 1:]\n\n  # When reading from stdin, the extension is unknown, so no cpplint tests\n  # should rely on the extension.\n  if filename != \'-\' and file_extension not in _valid_extensions:\n    sys.stderr.write(\'Ignoring %s; not a valid file name \'\n                     \'(%s)\\n\' % (filename, \', \'.join(_valid_extensions)))\n  else:\n    ProcessFileData(filename, file_extension, lines, Error,\n                    extra_check_functions)\n\n    # If end-of-line sequences are a mix of LF and CR-LF, issue\n    # warnings on the lines with CR.\n    #\n    # Don\'t issue any warnings if all lines are uniformly LF or CR-LF,\n    # since critique can handle these just fine, and the style guide\n    # doesn\'t dictate a particular end of line sequence.\n    #\n    # We can\'t depend on os.linesep to determine what the desired\n    # end-of-line sequence should be, since that will return the\n    # server-side end-of-line sequence.\n    if lf_lines and crlf_lines:\n      # Warn on every line with CR.  An alternative approach might be to\n      # check whether the file is mostly CRLF or just LF, and warn on the\n      # minority, we bias toward LF here since most tools prefer LF.\n      for linenum in crlf_lines:\n        Error(filename, linenum, \'whitespace/newline\', 1,\n              \'Unexpected \\\\r (^M) found; better to use only \\\\n\')\n\n  # The following print out is commented out to reduce the amount of log\n  # sys.stderr.write(\'Done processing %s\\n\' % filename)\n  _RestoreFilters()\n\n\ndef PrintUsage(message):\n  """"""Prints a brief usage string and exits, optionally with an error message.\n\n  Args:\n    message: The optional error message.\n  """"""\n  sys.stderr.write(_USAGE)\n  if message:\n    sys.exit(\'\\nFATAL ERROR: \' + message)\n  else:\n    sys.exit(1)\n\n\ndef PrintCategories():\n  """"""Prints a list of all the error-categories used by error messages.\n\n  These are the categories used to filter messages via --filter.\n  """"""\n  sys.stderr.write(\'\'.join(\'  %s\\n\' % cat for cat in _ERROR_CATEGORIES))\n  sys.exit(0)\n\n\ndef ParseArguments(args):\n  """"""Parses the command line arguments.\n\n  This may set the output format and verbosity level as side-effects.\n\n  Args:\n    args: The command line arguments:\n\n  Returns:\n    The list of filenames to lint.\n  """"""\n  try:\n    (opts, filenames) = getopt.getopt(args, \'\', [\'help\', \'output=\', \'verbose=\',\n                                                 \'counting=\',\n                                                 \'filter=\',\n                                                 \'root=\',\n                                                 \'linelength=\',\n                                                 \'extensions=\'])\n  except getopt.GetoptError:\n    PrintUsage(\'Invalid arguments.\')\n\n  verbosity = _VerboseLevel()\n  output_format = _OutputFormat()\n  filters = \'\'\n  counting_style = \'\'\n\n  for (opt, val) in opts:\n    if opt == \'--help\':\n      PrintUsage(None)\n    elif opt == \'--output\':\n      if val not in (\'emacs\', \'vs7\', \'eclipse\'):\n        PrintUsage(\'The only allowed output formats are emacs, vs7 and eclipse.\')\n      output_format = val\n    elif opt == \'--verbose\':\n      verbosity = int(val)\n    elif opt == \'--filter\':\n      filters = val\n      if not filters:\n        PrintCategories()\n    elif opt == \'--counting\':\n      if val not in (\'total\', \'toplevel\', \'detailed\'):\n        PrintUsage(\'Valid counting options are total, toplevel, and detailed\')\n      counting_style = val\n    elif opt == \'--root\':\n      global _root\n      _root = val\n    elif opt == \'--linelength\':\n      global _line_length\n      try:\n          _line_length = int(val)\n      except ValueError:\n          PrintUsage(\'Line length must be digits.\')\n    elif opt == \'--extensions\':\n      global _valid_extensions\n      try:\n          _valid_extensions = set(val.split(\',\'))\n      except ValueError:\n          PrintUsage(\'Extensions must be comma seperated list.\')\n\n  if not filenames:\n    PrintUsage(\'No files were specified.\')\n\n  _SetOutputFormat(output_format)\n  _SetVerboseLevel(verbosity)\n  _SetFilters(filters)\n  _SetCountingStyle(counting_style)\n\n  return filenames\n\n\ndef main():\n  filenames = ParseArguments(sys.argv[1:])\n\n  # Change stderr to write with replacement characters so we don\'t die\n  # if we try to print something containing non-ASCII characters.\n  sys.stderr = codecs.StreamReaderWriter(sys.stderr,\n                                         codecs.getreader(\'utf8\'),\n                                         codecs.getwriter(\'utf8\'),\n                                         \'replace\')\n\n  _cpplint_state.ResetErrorCounts()\n  for filename in filenames:\n    ProcessFile(filename, _cpplint_state.verbose_level)\n  _cpplint_state.PrintErrorCounts()\n\n  sys.exit(_cpplint_state.error_count > 0)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
third_party/python/pylint/main.py,0,"b""import pylint\n\nif __name__ == '__main__':\n    pylint.run_pylint()\n"""
third_party/python/semver/semver.py,0,"b'# -*- coding: utf-8 -*-\n\nimport re\n\n_REGEX = re.compile(\'^(?P<major>(?:0|[1-9][0-9]*))\'\n                    \'\\.(?P<minor>(?:0|[1-9][0-9]*))\'\n                    \'\\.(?P<patch>(?:0|[1-9][0-9]*))\'\n                    \'(\\-(?P<prerelease>[0-9A-Za-z-]+(\\.[0-9A-Za-z-]+)*))?\'\n                    \'(\\+(?P<build>[0-9A-Za-z-]+(\\.[0-9A-Za-z-]+)*))?$\')\n\n_LAST_NUMBER = re.compile(r\'(?:[^\\d]*(\\d+)[^\\d]*)+\')\n\nif not hasattr(__builtins__, \'cmp\'):\n    cmp = lambda a, b: (a > b) - (a < b)\n\n\ndef parse(version):\n    """"""\n    Parse version to major, minor, patch, pre-release, build parts.\n    """"""\n    match = _REGEX.match(version)\n    if match is None:\n        raise ValueError(\'%s is not valid SemVer string\' % version)\n\n    verinfo = match.groupdict()\n\n    verinfo[\'major\'] = int(verinfo[\'major\'])\n    verinfo[\'minor\'] = int(verinfo[\'minor\'])\n    verinfo[\'patch\'] = int(verinfo[\'patch\'])\n\n    return verinfo\n\n\ndef compare(ver1, ver2):\n    def nat_cmp(a, b):\n        a, b = a or \'\', b or \'\'\n        convert = lambda text: (2, int(text)) if re.match(\'[0-9]+\', text) else (1, text)\n        split_key = lambda key: [convert(c) for c in key.split(\'.\')]\n        return cmp(split_key(a), split_key(b))\n\n    def compare_by_keys(d1, d2):\n        for key in [\'major\', \'minor\', \'patch\']:\n            v = cmp(d1.get(key), d2.get(key))\n            if v:\n                return v\n\n        rc1, rc2 = d1.get(\'prerelease\'), d2.get(\'prerelease\')\n        rccmp = nat_cmp(rc1, rc2)\n\n        build_1, build_2 = d1.get(\'build\'), d2.get(\'build\')\n        build_cmp = nat_cmp(build_1, build_2)\n\n        if not rccmp and not build_cmp:\n            return 0\n        if not rc1 and not build_1:\n            return 1\n        elif not rc2 and not build_2:\n            return -1\n\n        return rccmp or build_cmp\n\n    v1, v2 = parse(ver1), parse(ver2)\n\n    return compare_by_keys(v1, v2)\n\n\ndef match(version, match_expr):\n    prefix = match_expr[:2]\n    if prefix in (\'>=\', \'<=\', \'==\'):\n        match_version = match_expr[2:]\n    elif prefix and prefix[0] in (\'>\', \'<\', \'=\'):\n        prefix = prefix[0]\n        match_version = match_expr[1:]\n    else:\n        raise ValueError(""match_expr parameter should be in format <op><ver>, ""\n                         ""where <op> is one of [\'<\', \'>\', \'==\', \'<=\', \'>=\']. ""\n                         ""You provided: %r"" % match_expr)\n\n    possibilities_dict = {\n        \'>\': (1,),\n        \'<\': (-1,),\n        \'==\': (0,),\n        \'>=\': (0, 1),\n        \'<=\': (-1, 0)\n    }\n\n    possibilities = possibilities_dict[prefix]\n    cmp_res = compare(version, match_version)\n\n    return cmp_res in possibilities\n\n\ndef max_ver(ver1, ver2):\n    cmp_res = compare(ver1, ver2)\n    if cmp_res == 0 or cmp_res == 1:\n        return ver1\n    else:\n        return ver2\n\n\ndef min_ver(ver1, ver2):\n    cmp_res = compare(ver1, ver2)\n    if cmp_res == 0 or cmp_res == -1:\n        return ver1\n    else:\n        return ver2\n\n\ndef format_version(major, minor, patch, prerelease=None, build=None):\n    version = ""%d.%d.%d"" % (major, minor, patch)\n    if prerelease is not None:\n        version = version + ""-%s"" % prerelease\n\n    if build is not None:\n        version = version + ""+%s"" % build\n\n    return version\n\n\ndef _increment_string(string):\n    # look for the last sequence of number(s) in a string and increment, from:\n    # http://code.activestate.com/recipes/442460-increment-numbers-in-a-string/#c1\n    match = _LAST_NUMBER.search(string)\n    if match:\n        next_ = str(int(match.group(1))+1)\n        start, end = match.span(1)\n        string = string[:max(end - len(next_), start)] + next_ + string[end:]\n    return string\n\n\ndef bump_major(version):\n    verinfo = parse(version)\n    return format_version(verinfo[\'major\'] + 1, 0, 0)\n\ndef bump_minor(version):\n    verinfo = parse(version)\n    return format_version(verinfo[\'major\'], verinfo[\'minor\'] + 1, 0)\n\ndef bump_patch(version):\n    verinfo = parse(version)\n    return format_version(verinfo[\'major\'], verinfo[\'minor\'], verinfo[\'patch\'] + 1)\n\ndef bump_prerelease(version):\n    verinfo = parse(version)\n    verinfo[\'prerelease\'] = _increment_string(verinfo[\'prerelease\'] or \'rc.0\')\n    return format_version(verinfo[\'major\'], verinfo[\'minor\'], verinfo[\'patch\'],\n                          verinfo[\'prerelease\'])\n\ndef bump_build(version):\n    verinfo = parse(version)\n    verinfo[\'build\'] = _increment_string(verinfo[\'build\'] or \'build.0\')\n    return format_version(verinfo[\'major\'], verinfo[\'minor\'], verinfo[\'patch\'],\n                          verinfo[\'prerelease\'], verinfo[\'build\'])\n'"
third_party/python/semver/setup.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom distutils.core import setup\n\nwith open('README.md') as f:\n    LONG_DESCRIPTION = f.read()\n\nsetup(\n    name='semver',\n    version='2.4.1',\n    description='Python package to work with Semantic Versioning (http://semver.org/)',\n    long_description=LONG_DESCRIPTION,\n    author='Konstantine Rybnikov',\n    author_email='k-bx@k-bx.com',\n    url='https://github.com/k-bx/python-semver',\n    download_url='https://github.com/k-bx/python-semver/downloads',\n    py_modules=['semver'],\n    include_package_data=True,\n    license='BSD',\n    classifiers=[\n        'Environment :: Web Environment',\n        'Framework :: Django',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: BSD License',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.2',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n    ],\n)\n"""
third_party/crow/tests/template/test.py,0,"b'#!/usr/bin/env python\nfrom __future__ import print_function\nimport glob\nimport json\nimport os\nimport subprocess\nfor testfile in glob.glob(""*.json""):\n    testdoc = json.load(open(testfile))\n    for test in testdoc[""tests""]:\n        if ""lambda"" in test[""data""]:\n            continue\n        open(\'data\', \'w\').write(json.dumps(test[""data""]))\n        open(\'template\', \'w\').write(test[""template""])\n        if ""partials"" in test:\n            open(\'partials\', \'w\').write(json.dumps(test[""partials""]))\n        else:\n            open(\'partials\', \'w\').write(""{}"")\n        ret = subprocess.check_output(""./mustachetest"").decode(\'utf8\')\n        print(testfile, test[""name""])\n        if ret != test[""expected""]:\n            if \'partials\' in test:\n                print(\'partials:\', json.dumps(test[""partials""]))\n            print(json.dumps(test[""data""]))\n            print(test[""template""])\n            print(\'Expected:\',repr(test[""expected""]))\n            print(\'Actual:\',repr(ret))\n        assert ret == test[""expected""]\n        os.unlink(\'data\')\n        os.unlink(\'template\')\n        os.unlink(\'partials\')\n'"
