file_path,api_count,code
config.py,0,"b'# global configuration for GAN training\n\nimport argparse\n\n\nargs_list = []\nparser = argparse.ArgumentParser()\n\n\ndef add_arg_group(name):\n    """"""\n    :param name: A str. Argument group.\n    :return: An list. Arguments.\n    """"""\n    arg = parser.add_argument_group(name)\n    args_list.append(arg)\n    return arg\n\n\ndef get_config():\n    cfg, un_parsed = parser.parse_known_args()\n    return cfg, un_parsed\n\n\n# Model\nmodel_arg = add_arg_group(\'Model\')\nmodel_arg.add_argument(\'--model_path\', type=str, default=""./model/"")\nmodel_arg.add_argument(\'--output\', type=str, default=""./gen_img/"")\n\n# DataSet\ndata_arg = add_arg_group(\'DataSet\')\ndata_arg.add_argument(\'--mnist\', type=str, default=""./mnist/"")\ndata_arg.add_argument(\'--fashion_mnist\', type=str, default=""./fashion-mnist/"")\ndata_arg.add_argument(\'--cifar10\', type=str, default=""./cifar10/"")\ndata_arg.add_argument(\'--cifar100\', type=str, default=""./cifar100/"")\ndata_arg.add_argument(\'--celeba\', type=str, default=""/media/zero/data/CelebA/"")\ndata_arg.add_argument(\'--celeba-hq\', type=str, default=""./CelebA-HQ/"")\ndata_arg.add_argument(\'--div2k\', type=str, default=""./DIV2K/"")\ndata_arg.add_argument(\'--pix2pix\', type=str, default=""./pix2pix/"")\n\n# Misc\nmisc_arg = add_arg_group(\'Misc\')\nmisc_arg.add_argument(\'--device\', type=str, default=\'gpu\')\nmisc_arg.add_argument(\'--n_threads\', type=int, default=8,\n                      help=\'the number of workers for speeding up\')\nmisc_arg.add_argument(\'--seed\', type=int, default=1337)\nmisc_arg.add_argument(\'--verbose\', type=bool, default=True)\n'"
datasets.py,12,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport sys\nimport cv2\nimport h5py\nimport numpy as np\nimport tensorflow as tf\n\nfrom glob import glob\nfrom tqdm import tqdm\nfrom multiprocessing import Pool\nfrom sklearn.model_selection import train_test_split\n\n\nseed = 1337\n\n\ndef one_hot(labels_dense, num_classes=10):\n    num_labels = labels_dense.shape[0]\n    index_offset = np.arange(num_labels) * num_classes\n    labels_one_hot = np.zeros((num_labels, num_classes))\n    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n    return labels_one_hot\n\n\nclass DataSetLoader:\n\n    @staticmethod\n    def get_extension(ext):\n        if ext in [\'jpg\', \'png\']:\n            return \'img\'\n        elif ext == \'tfr\':\n            return \'tfr\'\n        elif ext == \'h5\':\n            return \'h5\'\n        elif ext == \'npy\':\n            return \'npy\'\n        else:\n            raise ValueError(""[-] There\'is no supporting file... [%s] :("" % ext)\n\n    @staticmethod\n    def get_img(path, size=(64, 64), interp=cv2.INTER_CUBIC):\n        img = cv2.imread(path, cv2.IMREAD_COLOR)[..., ::-1]  # BGR to RGB\n        if img.shape[0] == size[0]:\n            return img\n        else:\n            return cv2.resize(img, size, interp)\n\n    @staticmethod\n    def parse_tfr_tf(record):\n        features = tf.parse_single_example(record, features={\n            \'shape\': tf.FixedLenFeature([3], tf.int64),\n            \'data\': tf.FixedLenFeature([], tf.string)})\n        data = tf.decode_raw(features[\'data\'], tf.uint8)\n        return tf.reshape(data, features[\'shape\'])\n\n    @staticmethod\n    def parse_tfr_np(record):\n        ex = tf.train.Example()\n        ex.ParseFromString(record)\n        shape = ex.features.feature[\'shape\'].int64_list.value\n        data = ex.features.feature[\'data\'].bytes_list.value[0]\n        return np.fromstring(data, np.uint8).reshape(shape)\n\n    @staticmethod\n    def img_scaling(img, scale=\'0,1\'):\n        if scale == \'0,1\':\n            try:\n                img /= 255.\n            except TypeError:  # ufunc \'true divide\' output ~\n                img = np.true_divide(img, 255.0, casting=\'unsafe\')\n        elif scale == \'-1,1\':\n            try:\n                img = (img / 127.5) - 1.\n            except TypeError:\n                img = np.true_divide(img, 127.5, casting=\'unsafe\') - 1.\n        else:\n            raise ValueError(""[-] Only \'0,1\' or \'-1,1\' please - (%s)"" % scale)\n\n        return img\n\n    def __init__(self, path, size=None, name=\'to_tfr\', use_save=False, save_file_name=\'\',\n                 buffer_size=4096, n_threads=8,\n                 use_image_scaling=True, image_scale=\'0,1\', img_save_method=cv2.INTER_LINEAR, debug=True):\n\n        self.op = name.split(\'_\')\n        self.debug = debug\n\n        try:\n            assert len(self.op) == 2\n        except AssertionError:\n            raise AssertionError(""[-] Invalid Target Types :("")\n\n        self.size = size\n\n        try:\n            assert self.size\n        except AssertionError:\n            raise AssertionError(""[-] Invalid Target Sizes :("")\n\n        # To-DO\n        # Supporting 4D Image\n        self.height = size[0]\n        self.width = size[1]\n        self.channel = size[2]\n\n        self.path = path\n\n        try:\n            assert os.path.exists(self.path)\n        except AssertionError:\n            raise AssertionError(""[-] Path(%s) does not exist :("" % self.path)\n\n        self.buffer_size = buffer_size\n        self.n_threads = n_threads\n\n        if os.path.isfile(self.path):\n            self.file_list = [self.path]\n            self.file_ext = self.path.split(\'.\')[-1]\n            self.file_names = [self.path]\n        else:\n            self.file_list = sorted(os.listdir(self.path))\n            self.file_ext = self.file_list[0].split(\'.\')[-1]\n            self.file_names = glob(self.path + \'/*\')\n        self.raw_data = np.ndarray([], dtype=np.uint8)  # (N, H * W * C)\n\n        if self.debug:\n            print(""[*] Detected Path            is [%s]"" % self.path)\n            print(""[*] Detected File Extension  is [%s]"" % self.file_ext)\n            print(""[*] Detected First File Name is [%s] (%d File(s))"" % (self.file_names[0], len(self.file_names)))\n\n        self.types = (\'img\', \'tfr\', \'h5\', \'npy\')  # Supporting Data Types\n        self.op_src = self.get_extension(self.file_ext)\n        self.op_dst = self.op[1]\n\n        try:\n            chk_src, chk_dst = False, False\n            for t in self.types:\n                if self.op_src == t:\n                    chk_src = True\n                if self.op_dst == t:\n                    chk_dst = True\n            assert chk_src and chk_dst\n        except AssertionError:\n            raise AssertionError(""[-] Invalid Operation Types (%s, %s) :("" % (self.op_src, self.op_dst))\n\n        self.img_save_method = img_save_method\n\n        if self.op_src == self.types[0]:\n            self.load_img()\n        elif self.op_src == self.types[1]:\n            self.load_tfr()\n        elif self.op_src == self.types[2]:\n            self.load_h5()\n        elif self.op_src == self.types[3]:\n            self.load_npy()\n        else:\n            raise NotImplementedError(""[-] Not Supported Type :("")\n\n        # Random Shuffle\n        order = np.arange(self.raw_data.shape[0])\n        np.random.RandomState(seed).shuffle(order)\n        self.raw_data = self.raw_data[order]\n\n        # Clip [0, 255]\n        try:\n            self.raw_data = np.rint(self.raw_data).clip(0, 255).astype(np.uint8)\n        except MemoryError:\n            pass\n\n        self.use_save = use_save\n        self.save_file_name = save_file_name\n\n        if self.use_save:\n            try:\n                assert self.save_file_name\n            except AssertionError:\n                raise AssertionError(""[-] Empty save-file name :("")\n\n            if self.op_dst == self.types[0]:\n                self.convert_to_img()\n            elif self.op_dst == self.types[1]:\n                self.tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n                self.tfr_writer = tf.python_io.TFRecordWriter(self.save_file_name + "".tfrecords"", self.tfr_opt)\n                self.convert_to_tfr()\n            elif self.op_dst == self.types[2]:\n                self.convert_to_h5()\n            elif self.op_dst == self.types[3]:\n                self.convert_to_npy()\n            else:\n                raise NotImplementedError(""[-] Not Supported Type :("")\n\n        self.use_image_scaling = use_image_scaling\n        self.img_scale = image_scale\n\n        if self.use_image_scaling:\n            self.raw_data = self.img_scaling(self.raw_data, self.img_scale)\n\n    def load_img(self):\n        self.raw_data = np.zeros((len(self.file_list), self.height * self.width * self.channel),\n                                 dtype=np.uint8)\n\n        for i, fn in tqdm(enumerate(self.file_names)):\n            self.raw_data[i] = self.get_img(fn, (self.height, self.width), self.img_save_method).flatten()\n            if self.debug:  # just once\n                print(""[*] Image Shape   : "", self.raw_data[i].shape)\n                print(""[*] Image Size    : "", self.raw_data[i].size)\n                print(""[*] Image MIN/MAX :  (%d, %d)"" % (np.min(self.raw_data[i]), np.max(self.raw_data[i])))\n                self.debug = False\n\n    def load_tfr(self):\n        self.raw_data = tf.data.TFRecordDataset(self.file_names, compression_type=\'\', buffer_size=self.buffer_size)\n        self.raw_data = self.raw_data.map(self.parse_tfr_tf, num_parallel_calls=self.n_threads)\n\n    def load_h5(self, size=0, offset=0):\n        init = True\n\n        for fl in self.file_list:  # For multiple .h5 files\n            with h5py.File(fl, \'r\') as hf:\n                data = hf[\'images\']\n                full_size = len(data)\n\n                if size == 0:\n                    size = full_size\n\n                n_chunks = int(np.ceil(full_size / size))\n                if offset >= n_chunks:\n                    print(""[*] Looping from back to start."")\n                    offset %= n_chunks\n                if offset == n_chunks - 1:\n                    print(""[-] Not enough data available, clipping to end."")\n                    data = data[offset * size:]\n                else:\n                    data = data[offset * size:(offset + 1) * size]\n\n                data = np.array(data, dtype=np.uint8)\n                print(""[+] "", fl, "" => Image size : "", data.shape)\n\n                if init:\n                    self.raw_data = data\n                    init = False\n\n                    if self.debug:  # just once\n                        print(""[*] Image Shape   : "", self.raw_data[0].shape)\n                        print(""[*] Image Size    : "", self.raw_data[0].size)\n                        print(""[*] Image MIN/MAX :  (%d, %d)"" % (np.min(self.raw_data[0]), np.max(self.raw_data[0])))\n                        self.debug = False\n\n                    continue\n                else:\n                    self.raw_data = np.concatenate((self.raw_data, data))\n\n    def load_npy(self):\n        self.raw_data = np.rollaxis(np.squeeze(np.load(self.file_names), axis=0), 0, 3)\n\n        if self.debug:  # just once\n            print(""[*] Image Shape   : "", self.raw_data[0].shape)\n            print(""[*] Image Size    : "", self.raw_data[0].size)\n            print(""[*] Image MIN/MAX :  (%d, %d)"" % (np.min(self.raw_data[0]), np.max(self.raw_data[0])))\n            self.debug = False\n\n    def convert_to_img(self):\n        def to_img(i):\n            cv2.imwrite(\'imgHQ%05d.png\' % i, cv2.COLOR_BGR2RGB)\n            return True\n\n        raw_data_shape = self.raw_data.shape  # (N, H * W * C)\n\n        try:\n            assert os.path.exists(self.save_file_name)\n        except AssertionError:\n            print(""[-] There\'s no %s :("" % self.save_file_name)\n            print(""[*] Make directory at %s... "" % self.save_file_name)\n            os.mkdir(self.save_file_name)\n\n        ii = [i for i in range(raw_data_shape[0])]\n\n        pool = Pool(self.n_threads)\n        print(pool.map(to_img, ii))\n\n    def convert_to_tfr(self):\n        for data in self.raw_data:\n            ex = tf.train.Example(features=tf.train.Features(feature={\n                \'shape\': tf.train.Feature(int64_list=tf.train.Int64List(value=data.shape)),\n                \'data\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[data.tostring()]))\n            }))\n            self.tfr_writer.write(ex.SerializeToString())\n\n    def convert_to_h5(self):\n        with h5py.File(self.save_file_name, \'w\') as f:\n            f.create_dataset(""images"", data=self.raw_data)\n\n    def convert_to_npy(self):\n        np.save(self.save_file_name, self.raw_data)\n\n\nclass MNISTDataSet:\n\n    def __init__(self, use_split=False, split_rate=0.15, random_state=42, ds_path=None):\n        self.use_split = use_split\n        self.split_rate = split_rate\n        self.random_state = random_state\n\n        self.ds_path = ds_path\n\n        try:\n            assert self.ds_path\n        except AssertionError:\n            raise AssertionError(""[-] MNIST DataSet Path is required!"")\n\n        from tensorflow.examples.tutorials.mnist import input_data\n        self.data = input_data.read_data_sets(self.ds_path, one_hot=True)  # download MNIST\n\n        # training data\n        self.train_data = self.data.train\n\n        self.train_images = self.train_data.images\n        self.train_labels = self.train_data.labels\n        self.valid_images = None\n        self.valid_labels = None\n\n        # test data\n        self.test_data = self.data.test\n\n        self.test_images = self.test_data.images\n        self.test_labels = self.test_data.labels\n\n        # split training data set into train, valid\n        if self.use_split:\n            self.train_images, self.valid_images, self.train_labels, self.valid_labels = \\\n                train_test_split(self.train_images, self.train_labels,\n                                 test_size=self.split_rate,\n                                 random_state=self.random_state)\n\n\nclass CiFarDataSet:\n\n    @staticmethod\n    def unpickle(file):\n        import pickle\n\n        # WARN: Only for python3, NOT FOR python2\n        assert sys.version_info >= (3, 0)\n\n        with open(file, \'rb\') as f:\n            return pickle.load(f, encoding=\'bytes\')\n\n    def __init__(self, height=32, width=32, channel=3,\n                 use_split=False, split_rate=0.2, random_state=42, ds_name=""cifar-10"", ds_path=None):\n\n        """"""\n        # General Settings\n        :param height: input image height, default 32\n        :param width: input image width, default 32\n        :param channel: input image channel, default 3 (RGB)\n        - in case of CIFAR, image size is 32 x 32 x 3 (HWC).\n\n        # Pre-Processing Option\n        :param use_split: training DataSet splitting, default True\n        :param split_rate: image split rate (into train & test), default 0.2\n        :param random_state: random seed for shuffling, default 42\n\n        # DataSet Option\n        :param ds_name: DataSet\'s name, default cifar-10\n        :param ds_path: DataSet\'s path, default None\n        """"""\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n\n        self.use_split = use_split\n        self.split_rate = split_rate\n        self.random_state = random_state\n\n        self.ds_name = ds_name\n        self.ds_path = ds_path  # DataSet path\n        self.n_classes = 10     # DataSet the number of classes, default 10\n\n        self.train_images = None\n        self.valid_images = None\n        self.test_images = None\n\n        self.train_labels = None\n        self.valid_labels = None\n        self.test_labels = None\n\n        try:\n            assert self.ds_path\n        except AssertionError:\n            raise AssertionError(""[-] CIFAR10/100 DataSets\' Path is required!"")\n\n        if self.ds_name == ""cifar-10"":\n            self.cifar_10()   # loading Cifar-10\n        elif self.ds_name == ""cifar-100"":\n            self.cifar_100()  # loading Cifar-100\n        else:\n            raise NotImplementedError(""[-] Only \'cifar-10\' or \'cifar-100\'"")\n\n    def cifar_10(self):\n        self.n_classes = 10  # labels\n\n        train_batch_1 = self.unpickle(""{0}/data_batch_1"".format(self.ds_path))\n        train_batch_2 = self.unpickle(""{0}/data_batch_2"".format(self.ds_path))\n        train_batch_3 = self.unpickle(""{0}/data_batch_3"".format(self.ds_path))\n        train_batch_4 = self.unpickle(""{0}/data_batch_4"".format(self.ds_path))\n        train_batch_5 = self.unpickle(""{0}/data_batch_5"".format(self.ds_path))\n\n        # training data & label\n        train_data = np.concatenate([\n            train_batch_1[b\'data\'],\n            train_batch_2[b\'data\'],\n            train_batch_3[b\'data\'],\n            train_batch_4[b\'data\'],\n            train_batch_5[b\'data\'],\n        ], axis=0)\n\n        train_labels = np.concatenate([\n            train_batch_1[b\'labels\'],\n            train_batch_2[b\'labels\'],\n            train_batch_3[b\'labels\'],\n            train_batch_4[b\'labels\'],\n            train_batch_5[b\'labels\'],\n        ], axis=0)\n\n        # Image size : 32x32x3\n        train_images = np.swapaxes(train_data.reshape([-1,\n                                                       self.height,\n                                                       self.width,\n                                                       self.channel], order=\'F\'), 1, 2)\n\n        # test data & label\n        test_batch = self.unpickle(""{0}/test_batch"".format(self.ds_path))\n\n        test_data = test_batch[b\'data\']\n        test_labels = np.array(test_batch[b\'labels\'])\n\n        # image size : 32x32x3\n        test_images = np.swapaxes(test_data.reshape([-1,\n                                                     self.height,\n                                                     self.width,\n                                                     self.channel], order=\'F\'), 1, 2)\n\n        # split training data set into train / val\n        if self.use_split:\n            train_images, valid_images, train_labels, valid_labels = \\\n                train_test_split(train_images, train_labels,\n                                 test_size=self.split_rate,\n                                 random_state=self.random_state)\n\n            self.valid_images = valid_images\n            self.valid_labels = one_hot(valid_labels, self.n_classes)\n\n        self.train_images = train_images\n        self.test_images = test_images\n\n        self.train_labels = one_hot(train_labels, self.n_classes)\n        self.test_labels = one_hot(test_labels, self.n_classes)\n\n    def cifar_100(self):\n        self.n_classes = 100  # labels\n\n        # training data & label\n        train_batch = self.unpickle(""{0}/train"".format(self.ds_path))\n\n        train_data = np.concatenate([train_batch[b\'data\']], axis=0)\n        train_labels = np.concatenate([train_batch[b\'fine_labels\']], axis=0)\n        train_images = np.swapaxes(train_data.reshape([-1,\n                                                       self.height,\n                                                       self.width,\n                                                       self.channel], order=\'F\'), 1, 2)\n\n        # test data & label\n        test_batch = self.unpickle(""{0}/test"".format(self.ds_path))\n\n        test_data = np.concatenate([test_batch[b\'data\']], axis=0)\n        test_labels = np.concatenate([test_batch[b\'fine_labels\']], axis=0)\n        test_images = np.swapaxes(test_data.reshape([-1,\n                                                     self.height,\n                                                     self.width,\n                                                     self.channel], order=\'F\'), 1, 2)\n\n        # split training data set into train / val\n        if self.use_split:\n            train_images, valid_images, train_labels, valid_labels = \\\n                train_test_split(train_images, train_labels,\n                                 test_size=self.split_rate,\n                                 random_state=self.random_state)\n\n            self.valid_images = valid_images\n            self.valid_labels = one_hot(valid_labels, self.n_classes)\n\n        self.train_images = train_images\n        self.test_images = test_images\n\n        self.train_labels = one_hot(train_labels, self.n_classes)\n        self.test_labels = one_hot(test_labels, self.n_classes)\n\n\nclass CelebADataSet:\n\n    """"""\n    This Class for CelebA & CelebA-HQ DataSets.\n        - saving images as .h5 file for more faster loading.\n        - Actually, CelebA-HQ DataSet is kinda encrypted. So if u wanna use it, decrypt first!\n            There\'re a few codes that download & decrypt CelebA-HQ DataSet.\n    """"""\n\n    def __init__(self,\n                 height=64, width=64, channel=3, attr_labels=(),\n                 n_threads=30, use_split=False, split_rate=0.2, random_state=42,\n                 ds_image_path=None, ds_label_path=None, ds_type=""CelebA"", use_img_scale=True, img_scale=""-1,1"",\n                 use_save=False, save_type=\'to_h5\', save_file_name=None,\n                 use_concat_data=False):\n\n        """"""\n        # General Settings\n        :param height: image height\n        :param width: image width\n        :param channel: image channel\n        - in case of CelebA,    image size is  64  x  64  x 3 (HWC)\n        - in case of CelebA-HQ, image size is 1024 x 1024 x 3 (HWC)\n        :param attr_labels: attributes of CelebA DataSet\n        - in case of CelebA,    the number of attributes is 40\n\n        # Pre-Processing Option\n        :param n_threads: the number of threads\n        :param use_split: splitting train DataSet into train/val\n        :param split_rate: image split rate (into train & val)\n        :param random_state: random seed for shuffling, default 42\n\n        # DataSet Settings\n        :param ds_image_path: DataSet\'s Image Path\n        :param ds_label_path: DataSet\'s Label Path\n        :param ds_type: which DataSet is\n        :param use_img_scale: using img scaling?\n        :param img_scale: img normalize\n        :param use_save: saving into another file format\n        :param save_type: file format to save\n        :param save_file_name: file name to save\n        :param use_concat_data: concatenate images & labels\n        """"""\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        \'\'\'\n        # Available attributes\n        [\n         5_o_Clock_Shadow, Arched_Eyebrows, Attractive, Bags_Under_Eyes, Bald, Bangs, Big_Lips, Big_Nose, Black_Hair,\n         Blond_Hair, Blurry, Brown_Hair, Bushy_Eyebrows, Chubby, Double_Chin, Eyeglasses, Goatee, Gray_Hair,\n         Heavy_Makeup, High_Cheekbones, Male, Mouth_Slightly_Open, Mustache, Narrow_Eyes, No_Beard, Oval_Face,\n         Pale_Skin, Pointy_Nose, Receding_Hairline, Rosy_Cheeks, Sideburns, Smiling, Straight_Hair, Wavy_Hair,\n         Wearing_Earrings, Wearing_Hat, Wearing_Lipstick, Wearing_Necklace, Wearing_Necktie, Young\n        ]\n        \'\'\'\n        self.attr_labels = attr_labels\n        self.image_shape = (self.height, self.width, self.channel)  # (H, W, C)\n\n        self.n_threads = n_threads\n        self.use_split = use_split\n        self.split_rate = split_rate\n        self.random_state = random_state\n\n        self.attr = []      # loaded labels\n        self.images = []\n        self.labels = {}\n\n        """"""\n        Expected DataSet\'s Path Example\n        CelebA    : CelebA/ (sub-folder : Anno/..., Img/... )\n        CelebA-HQ : CelebA-HQ/ (sub-folder : ...npy, ...png )\n        Labels    : CelebA/Anno/...txt\n        \n        Expected DatSet\'s Type\n        \'CelebA\' or \'CelebA-HQ\'\n        """"""\n        self.ds_image_path = ds_image_path\n        self.ds_label_path = ds_label_path\n        self.ds_type = ds_type\n\n        self.use_img_scale = use_img_scale\n        self.img_scale = img_scale\n\n        try:\n            assert self.ds_image_path and self.ds_label_path\n        except AssertionError:\n            raise AssertionError(""[-] CelebA/CelebA-HQ DataSets\' Path is required! (%s)"")\n\n        if self.ds_type == ""CelebA"":\n            self.num_images = 202599  # the number of CelebA    images\n        elif self.ds_type == ""CelebA-HQ"":\n            self.num_images = 30000   # the number of CelebA-HQ images\n\n            tmp_path = self.ds_image_path + ""/imgHQ00000.""\n            if os.path.exists(tmp_path + ""dat""):\n                raise FileNotFoundError(""[-] You need to decrypt .dat file first!\\n"" +\n                                        ""[-] plz, use original PGGAN repo or""\n                                        "" this repo https://github.com/nperraud/download-celebA-HQ"")\n        else:\n            raise NotImplementedError(""[-] \'ds_type\' muse be \'CelebA\' or \'CelebA-HQ\'"")\n\n        self.use_save = use_save\n        self.save_type = save_type\n        self.save_file_name = save_file_name\n\n        self.use_concat_data = use_concat_data\n\n        try:\n            if self.use_save:\n                assert self.save_file_name\n        except AssertionError:\n            raise AssertionError(""[-] save-file/folder-name is required!"")\n\n        self.images = DataSetLoader(path=self.ds_image_path,\n                                    size=self.image_shape,\n                                    use_save=self.use_save,\n                                    name=self.save_type,\n                                    save_file_name=self.save_file_name,\n                                    use_image_scaling=use_img_scale,\n                                    image_scale=self.img_scale).raw_data  # numpy arrays\n        self.labels = self.load_attr(path=self.ds_label_path)\n\n        if self.use_concat_data:\n            self.images = self.concat_data(self.images, self.labels)\n\n        # split training data set into train / val\n        if self.use_split:\n            self.train_images, self.valid_images, self.train_labels, self.valid_labels = \\\n                train_test_split(self.images, self.labels,\n                                 test_size=self.split_rate,\n                                 random_state=self.random_state)\n\n    def load_attr(self, path):\n        with open(path, \'r\') as f:\n            img_attr = []\n\n            self.num_images = int(f.readline().strip())\n            self.attr = (f.readline().strip()).split(\' \')\n\n            print(""[*] the number of images     : %d"" % self.num_images)\n            print(""[*] the number of attributes : %d/%d"" % (len(self.attr_labels), len(self.attr)))\n\n            for fn in f.readlines():\n                row = fn.strip().split()\n                # img_name = row[0]\n                attr = [int(x) for x in row[1:]]\n\n                tmp = [attr[self.attr.index(x)] for x in self.attr_labels]\n                tmp = [1. if x == 1 else 0. for x in tmp]  # one-hot labeling\n\n                img_attr.append(tmp)\n\n            return np.asarray(img_attr)\n\n    def concat_data(self, img, label):\n        label = np.tile(np.reshape(label, [-1, 1, 1, len(self.attr_labels)]), [1, self.height, self.width, 1])\n        return np.concatenate([img, label], axis=3)\n\n\nclass Pix2PixDataSet:\n\n    def __init__(self, height=64, width=64, channel=3,\n                 use_split=False, split_rate=0.15, random_state=42, n_threads=8,\n                 ds_path=None, ds_name=None, use_save=False, save_type=\'to_h5\', save_file_name=None):\n\n        """"""\n        # General Settings\n        :param height: image height, default 64\n        :param width: image width, default 64\n        :param channel: image channel, default 3 (RGB)\n\n        # Pre-Processing Option\n        :param use_split: using DataSet split, default False\n        :param split_rate: image split rate (into train & test), default 0.2\n        :param random_state: random seed for shuffling, default 42\n        :param n_threads: the number of threads for multi-threading, default 8\n\n        # DataSet Option\n        :param ds_path: DataSet\'s Path, default None\n        :param ds_name: DataSet\'s Name, default None\n        :param use_save: saving into another file format\n        :param save_type: file format to save\n        :param save_file_name: file name to save\n        """"""\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = (self.height, self.width, self.channel)\n\n        self.use_split = use_split\n        self.split_rate = split_rate\n        self.random_state = random_state\n        self.n_threads = n_threads  # change this value to the fitted value for ur system\n\n        """"""\n        Expected ds_path : pix2pix/...\n        Expected ds_name : apple2orange\n        """"""\n        self.ds_path = ds_path\n        self.ds_name = ds_name\n        # single grid : testA, testB, (trainA, trainB)\n        # double grid : train, val, (test, sample)\n        self.ds_single_grid = [\'apple2orange\', \'horse2zebra\', \'monet2photo\', \'summer2winter_yosemite\', \'vangogh2photo\',\n                               \'ae_photos\', \'cezanne2photo\', \'ukivoe2photo\', \'iphone2dslr_flower\']\n        self.ds_double_grid = [\'cityscapes\', \'edges2handbags\', \'edges2shoes\', \'facades\', \'maps\']\n\n        # Single Grid DatSet - the number of images\n        self.n_sg_images_a = 400\n        self.n_sg_images_b = 6287\n        # Double Grid DatSet - the number of images\n        self.n_dg_images_a = 0\n        self.n_dg_images_b = 0\n\n        self.use_save = use_save\n        self.save_type = save_type\n        self.save_file_name = save_file_name\n\n        try:\n            if self.use_save:\n                assert self.save_file_name\n        except AssertionError:\n            raise AssertionError(""[-] save-file/folder-name is required!"")\n\n        if self.ds_name in self.ds_single_grid:\n            self.images_a = DataSetLoader(path=self.ds_path + ""/"" + self.ds_name + ""/trainA/"",\n                                          size=self.image_shape,\n                                          use_save=self.use_save,\n                                          name=self.save_type,\n                                          save_file_name=self.save_file_name,\n                                          use_image_scaling=True,\n                                          image_scale=\'0,1\').raw_data  # numpy arrays\n\n            self.images_b = DataSetLoader(path=self.ds_path + ""/"" + self.ds_name + ""/trainB/"",\n                                          size=self.image_shape,\n                                          use_save=self.use_save,\n                                          name=self.save_type,\n                                          save_file_name=self.save_file_name,\n                                          use_image_scaling=True,\n                                          image_scale=\'0,1\').raw_data  # numpy arrays\n            self.n_images_a = self.n_sg_images_a\n            self.n_images_b = self.n_sg_images_b\n        elif self.ds_name in self.ds_double_grid:\n            # To-Do\n            # Implement this!\n            self.n_images_a = self.n_dg_images_a\n            self.n_images_b = self.n_dg_images_b\n        else:\n            raise NotImplementedError(""[-] Not Implemented yet"")\n\n\nclass ImageNetDataSet:\n\n    def __init__(self):\n        pass\n\n\nclass Div2KDataSet:\n\n    def __init__(self, hr_height=384, hr_width=384, lr_height=96, lr_width=96, channel=3,\n                 use_split=False, split_rate=0.1, random_state=42, n_threads=8,\n                 ds_path=None, ds_name=None, use_img_scale=True,\n                 ds_hr_path=None, ds_lr_path=None,\n                 use_save=False, save_type=\'to_h5\', save_file_name=None):\n\n        """"""\n        # General Settings\n        :param hr_height: input HR image height, default 384\n        :param hr_width: input HR image width, default 384\n        :param lr_height: input LR image height, default 96\n        :param lr_width: input LR image width, default 96\n        :param channel: input image channel, default 3 (RGB)\n        - in case of Div2K - ds x4, image size is 384 x 384 x 3 (HWC).\n\n        # Pre-Processing Option\n        :param split_rate: image split rate (into train & test), default 0.1\n        :param random_state: random seed for shuffling, default 42\n        :param n_threads: the number of threads for multi-threading, default 8\n\n        # DataSet Option\n        :param ds_path: DataSet\'s Path, default None\n        :param ds_name: DataSet\'s Name, default None\n        :param use_img_scale: using img scaling?\n        :param ds_hr_path: DataSet High Resolution path\n        :param ds_lr_path: DataSet Low Resolution path\n        :param use_save: saving into another file format\n        :param save_type: file format to save\n        :param save_file_name: file name to save\n        """"""\n\n        self.hr_height = hr_height\n        self.hr_width = hr_width\n        self.lr_height = lr_height\n        self.lr_width = lr_width\n        self.channel = channel\n        self.hr_shape = (self.hr_height, self.hr_width, self.channel)\n        self.lr_shape = (self.lr_height, self.lr_width, self.channel)\n\n        self.use_split = use_split\n        self.split_rate = split_rate\n        self.random_state = random_state\n        self.num_threads = n_threads  # change this value to the fitted value for ur system\n\n        """"""\n        Expected ds_path : div2k/...\n        Expected ds_name : X4\n        """"""\n        self.ds_path = ds_path\n        self.ds_name = ds_name\n        self.ds_hr_path = ds_hr_path\n        self.ds_lr_path = ds_lr_path\n\n        try:\n            assert self.ds_path\n        except AssertionError:\n            try:\n                assert self.ds_hr_path and self.ds_lr_path\n            except AssertionError:\n                raise AssertionError(""[-] DataSet\'s path is required!"")\n\n        self.use_save = use_save\n        self.save_type = save_type\n        self.save_file_name = save_file_name\n\n        try:\n            if self.use_save:\n                assert self.save_file_name\n            else:\n                self.save_file_name = """"\n        except AssertionError:\n            raise AssertionError(""[-] save-file/folder-name is required!"")\n\n        self.n_images = 800\n        self.n_images_val = 100\n\n        self.use_img_scaling = use_img_scale\n\n        if self.ds_path:  # like .h5 or .tfr\n            self.ds_hr_path = self.ds_path + ""/DIV2K_train_HR/""\n            self.ds_lr_path = self.ds_hr_path  # self.ds_path + ""/DIV2K_train_LR_bicubic/"" + self.ds_name + ""/""\n\n        self.hr_images = DataSetLoader(path=self.ds_hr_path,\n                                       size=self.hr_shape,\n                                       use_save=self.use_save,\n                                       name=self.save_type,\n                                       save_file_name=self.save_file_name + ""-hr.h5"",\n                                       use_image_scaling=self.use_img_scaling,\n                                       image_scale=\'-1,1\',\n                                       img_save_method=cv2.INTER_LINEAR).raw_data  # numpy arrays\n\n        self.lr_images = DataSetLoader(path=self.ds_lr_path,\n                                       size=self.lr_shape,\n                                       use_save=self.use_save,\n                                       name=self.save_type,\n                                       save_file_name=self.save_file_name + ""-lr.h5"",\n                                       use_image_scaling=self.use_img_scaling,\n                                       image_scale=\'-1,1\',\n                                       img_save_method=cv2.INTER_CUBIC).raw_data  # numpy arrays\n\n\nclass UrbanSoundDataSet:\n\n    def __init__(self):\n        pass\n\n\nclass DataIterator:\n\n    def __init__(self, x, y, batch_size, label_off=False):\n        self.x = x\n        self.label_off = label_off\n        if not self.label_off:\n            self.y = y\n        self.batch_size = batch_size\n        self.num_examples = num_examples = x.shape[0]\n        self.num_batches = num_examples // batch_size\n        self.pointer = 0\n\n        assert (self.batch_size <= self.num_examples)\n\n    def next_batch(self):\n        start = self.pointer\n        self.pointer += self.batch_size\n\n        if self.pointer > self.num_examples:\n            perm = np.arange(self.num_examples)\n            np.random.shuffle(perm)\n\n            self.x = self.x[perm]\n            if not self.label_off:\n                self.y = self.y[perm]\n\n            start = 0\n            self.pointer = self.batch_size\n\n        end = self.pointer\n\n        if not self.label_off:\n            return self.x[start:end], self.y[start:end]\n        else:\n            return self.x[start:end]\n\n    def iterate(self):\n        for step in range(self.num_batches):\n            yield self.next_batch()\n'"
image_utils.py,0,"b'import numpy as np\nimport scipy.misc\n\n\ndef transform(images, inv_type=\'255\'):\n    if inv_type == \'255\':\n        images /= 255.\n    elif inv_type == \'127\':\n        images = (images / 127.5) - 1.\n    else:\n        raise NotImplementedError(""[-] Only 255 and 127"")\n\n    return images.astype(np.float32)\n\n\ndef inverse_transform(images, inv_type=\'255\'):\n    if inv_type == \'255\':    # [ 0  1]\n        images *= 255\n    elif inv_type == \'127\':  # [-1, 1]\n        images = (images + 1) * (255 / 2.)\n    else:\n        raise NotImplementedError(""[-] Only 255 and 127"")\n\n    # clipped by [0, 255]\n    images[images > 255] = 255\n    images[images < 0] = 0\n\n    return images.astype(np.uint8)\n\n\ndef merge(images, size):\n    h, w = images.shape[1], images.shape[2]\n\n    img = np.zeros((h * size[0], w * size[1], 3))\n    for idx, image in enumerate(images):\n        i = idx % size[1]\n        j = idx // size[1]\n        img[j * h:j * h + h, i * w:i * w + w, :] = image\n\n    return img\n\n\ndef save_image(images, size, path):\n    return scipy.misc.imsave(path, merge(images, size))\n\n\ndef save_images(images, size, image_path, inv_type=\'255\'):\n    return save_image(inverse_transform(images, inv_type), size, image_path)\n\n\ndef img_save(img, path, inv_type=\'255\'):\n    return scipy.misc.imsave(path, inverse_transform(img, inv_type))\n    # return cv2.imwrite(path, inverse_transform(img, inv_type))\n'"
tfutil.py,92,"b'""""""\nInspired by https://github.com/tkarras/progressive_growing_of_gans/blob/master/tfutil.py\n""""""\n\nimport functools\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import functional_ops\n\nseed = 1337\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\nbatch_size = 64\n\n\n# ---------------------------------------------------------------------------------------------\n# For convenience :)\n\n\ndef run(*args, **kwargs):\n    return tf.get_default_session().run(*args, **kwargs)\n\n\ndef is_tf_expression(x):\n    return isinstance(x, tf.Tensor) or isinstance(x, tf.Variable) or isinstance(x, tf.Operation)\n\n\ndef safe_log(x, eps=1e-12):\n    with tf.name_scope(""safe_log""):\n        return tf.log(x + eps)\n\n\ndef safe_log2(x, eps=1e-12):\n    with tf.name_scope(""safe_log2""):\n        return tf.log(x + eps) * np.float32(1. / np.log(2.))\n\n\ndef lerp(a, b, t):\n    with tf.name_scope(""lerp""):\n        return a + (b - a) * t\n\n\ndef lerp_clip(a, b, t):\n    with tf.name_scope(""lerp_clip""):\n        return a + (b - a) * tf.clip_by_value(t, 0., 1.)\n\n\ndef gaussian_noise(x, std=5e-2):\n    noise = tf.random_normal(x.get_shape(), mean=0., stddev=std, dtype=tf.float32)\n    return x + noise\n\n\n# ---------------------------------------------------------------------------------------------\n# Image Sampling with TF\n\n\ndef down_sampling(img, interp=tf.image.ResizeMethod.BILINEAR):\n    shape = img.get_shape()  # [batch, height, width, channels]\n\n    h2 = int(shape[1] // 2)\n    w2 = int(shape[2] // 2)\n\n    return tf.image.resize_images(img, [h2, w2], interp)\n\n\ndef up_sampling(img, interp=tf.image.ResizeMethod.BILINEAR):\n    shape = img.get_shape()  # [batch, height, width, channels]\n\n    h2 = int(shape[1] * 2)\n    w2 = int(shape[2] * 2)\n\n    return tf.image.resize_images(img, [h2, w2], interp)\n\n\n# ---------------------------------------------------------------------------------------------\n# Optimizer\n\n\nclass Optimizer(object):\n\n    def __init__(self,\n                 name=\'train\',\n                 optimizer=\'tf.train.AdamOptimizer\',\n                 learning_rate=1e-3,\n                 use_loss_scaling=False,\n                 loss_scaling_init=64.,\n                 loss_scaling_inc=5e-4,\n                 loss_scaling_dec=1.,\n                 use_grad_scaling=False,\n                 grad_scaling=7.,\n                 **kwargs):\n        self.name = name\n        self.optimizer = optimizer\n        self.learning_rate = learning_rate\n\n        self.use_loss_scaling = use_loss_scaling\n        self.loss_scaling_init = loss_scaling_init\n        self.loss_scaling_inc = loss_scaling_inc\n        self.loss_scaling_dec = loss_scaling_dec\n\n        self.use_grad_scaling = use_grad_scaling\n        self.grad_scaling = grad_scaling\n\n\n# ---------------------------------------------------------------------------------------------\n# Network\n\n\nclass Network:\n\n    def __init__(self):\n        pass\n\n\n# ---------------------------------------------------------------------------------------------\n# Functions\n\n\nw_init = tf.contrib.layers.variance_scaling_initializer(factor=1., mode=\'FAN_AVG\', uniform=True)\nb_init = tf.zeros_initializer()\n\nreg = 5e-4\nw_reg = tf.contrib.layers.l2_regularizer(reg)\n\neps = 1e-5\n\n\n# Layers\n\n\ndef conv2d_alt(x, f=64, k=3, s=1, pad=0, pad_type=\'zero\', use_bias=True, sn=False, name=\'conv2d\'):\n    with tf.variable_scope(name):\n        if pad_type == \'zero\':\n            x = tf.pad(x, [[0, 0], [pad, pad], [pad, pad], [0, 0]])\n        elif pad_type == \'reflect\':\n            x = tf.pad(x, [[0, 0], [pad, pad], [pad, pad], [0, 0]], mode=\'REFLECT\')\n        else:\n            raise NotImplementedError(""[-] Only \'zero\' & \'reflect\' are supported :("")\n\n        if sn:\n            w = tf.get_variable(\'kernel\', shape=[k, k, x.get_shape()[-1], f],\n                                initializer=w_init, regularizer=w_reg)\n            x = tf.nn.conv2d(x, filter=spectral_norm(w), strides=[1, s, s, 1], padding=\'VALID\')\n\n            if use_bias:\n                b = tf.get_variable(\'bias\', shape=[f], initializer=b_init)\n                x = tf.nn.bias_add(x, b)\n        else:\n            x = conv2d(x, f, k, s, name=name)\n\n        return x\n\n\ndef conv2d(x, f=64, k=3, s=1, pad=\'SAME\', reuse=None, is_train=True, name=\'conv2d\'):\n    """"""\n    :param x: input\n    :param f: filters\n    :param k: kernel size\n    :param s: strides\n    :param pad: padding\n    :param reuse: reusable\n    :param is_train: trainable\n    :param name: scope name\n    :return: net\n    """"""\n    return tf.layers.conv2d(inputs=x,\n                            filters=f, kernel_size=k, strides=s,\n                            kernel_initializer=w_init,\n                            kernel_regularizer=w_reg,\n                            bias_initializer=b_init,\n                            padding=pad,\n                            reuse=reuse,\n                            name=name)\n\n\ndef conv1d(x, f=64, k=3, s=1, pad=\'SAME\', reuse=None, is_train=True, name=\'conv1d\'):\n    """"""\n    :param x: input\n    :param f: filters\n    :param k: kernel size\n    :param s: strides\n    :param pad: padding\n    :param reuse: reusable\n    :param is_train: trainable\n    :param name: scope name\n    :return: net\n    """"""\n    return tf.layers.conv1d(inputs=x,\n                            filters=f, kernel_size=k, strides=s,\n                            kernel_initializer=w_init,\n                            kernel_regularizer=w_reg,\n                            bias_initializer=b_init,\n                            padding=pad,\n                            reuse=reuse,\n                            name=name)\n\n\ndef sub_pixel_conv2d(x, f, s=2):\n    """"""reference : https://github.com/tensorlayer/SRGAN/blob/master/tensorlayer/layers.py""""""\n\n    if f is None:\n        f = int(int(x.get_shape()[-1]) / (s ** 2))\n\n    bsize, a, b, c = x.get_shape().as_list()\n    bsize = tf.shape(x)[0]\n\n    x_s = tf.split(x, s, 3)\n    x_r = tf.concat(x_s, 2)\n\n    return tf.reshape(x_r, (bsize, s * a, s * b, f))\n\n\ndef deconv2d_alt(x, f=64, k=3, s=1, use_bias=True, sn=False, name=\'deconv2d\'):\n    with tf.variable_scope(name):\n        if sn:\n            w = tf.get_variable(\'kernel\', shape=[k, k, x.get_shape()[-1], f],\n                                initializer=w_init, regularizer=w_reg)\n            x = tf.nn.conv2d_transpose(x, filter=spectral_norm(w), strides=[1, s, s, 1], padding=\'SAME\',\n                                       output_shape=[x.get_shape()[0], x.get_shape()[1] * s, x.get_shape()[2] * s, f])\n\n            if use_bias:\n                b = tf.get_variable(\'bias\', shape=[f], initializer=b_init)\n                x = tf.nn.bias_add(x, b)\n        else:\n            x = deconv2d(x, f, k, s, name=name)\n\n        return x\n\n\ndef deconv2d(x, f=64, k=3, s=1, pad=\'SAME\', reuse=None, name=\'deconv2d\'):\n    """"""\n    :param x: input\n    :param f: filters\n    :param k: kernel size\n    :param s: strides\n    :param pad: padding\n    :param reuse: reusable\n    :param is_train: trainable\n    :param name: scope name\n    :return: net\n    """"""\n    return tf.layers.conv2d_transpose(inputs=x,\n                                      filters=f, kernel_size=k, strides=s,\n                                      kernel_initializer=w_init,\n                                      kernel_regularizer=w_reg,\n                                      bias_initializer=b_init,\n                                      padding=pad,\n                                      reuse=reuse,\n                                      name=name)\n\n\ndef dense_alt(x, f=1024, sn=False, use_bias=True, name=\'fc\'):\n    with tf.variable_scope(name):\n        x = flatten(x)\n\n        if sn:\n            w = tf.get_variable(\'kernel\', shape=[x.get_shape()[-1], f],\n                                initializer=w_init, regularizer=w_reg, dtype=tf.float32)\n            x = tf.matmul(x, spectral_norm(w))\n\n            if use_bias:\n                b = tf.get_variable(\'bias\', shape=[f], initializer=b_init)\n                x = tf.nn.bias_add(x, b)\n        else:\n            x = dense(x, f, name=name)\n\n        return x\n\n\ndef dense(x, f=1024, reuse=None, name=\'fc\'):\n    """"""\n    :param x: input\n    :param f: fully connected units\n    :param reuse: reusable\n    :param name: scope name\n    :param is_train: trainable\n    :return: net\n    """"""\n    return tf.layers.dense(inputs=x,\n                           units=f,\n                           kernel_initializer=w_init,\n                           kernel_regularizer=w_reg,\n                           bias_initializer=b_init,\n                           reuse=reuse,\n                           name=name)\n\n\ndef flatten(x):\n    return tf.layers.flatten(x)\n\n\ndef hw_flatten(x):\n    if is_tf_expression(x):\n        return tf.reshape(x, shape=[x.get_shape()[0], -1, x.get_shape()[-1]])\n    else:\n        return np.reshape(x, [x.shape[0], -1, x.shape[-1]])\n\n\n# Normalize\n\n\ndef l2_norm(x, eps=1e-12):\n    return x / (tf.sqrt(tf.reduce_sum(tf.square(x))) + eps)\n\n\ndef batch_norm(x, momentum=0.9, center=True, scaling=True, is_train=True, reuse=None, name=""bn""):\n    return tf.layers.batch_normalization(inputs=x,\n                                         momentum=momentum,\n                                         epsilon=eps,\n                                         center=center,\n                                         scale=scaling,\n                                         training=is_train,\n                                         reuse=reuse,\n                                         name=name)\n\n\ndef instance_norm(x, std=2e-2, affine=True, reuse=None, name=""""):\n    with tf.variable_scope(\'instance_normalize-%s\' % name, reuse=reuse):\n        mean, variance = tf.nn.moments(x, [1, 2], keepdims=True)\n\n        normalized = tf.div(x - mean, tf.sqrt(variance + eps))\n\n        if not affine:\n            return normalized\n        else:\n            depth = x.get_shape()[3]  # input channel\n\n            scale = tf.get_variable(\'scale\', [depth],\n                                    initializer=tf.random_normal_initializer(mean=1., stddev=std, dtype=tf.float32))\n            offset = tf.get_variable(\'offset\', [depth],\n                                     initializer=tf.zeros_initializer())\n\n        return scale * normalized + offset\n\n\ndef pixel_norm(x):\n    return x / tf.sqrt(tf.reduce_mean(tf.square(x), axis=[1, 2, 3]) + eps)\n\n\ndef spectral_norm(x, gain=1., n_iter=1):\n    x_shape = x.get_shape()\n\n    x = tf.reshape(x, (-1, x_shape[-1]))  # (n * h * w, c)\n\n    u = tf.get_variable(\'u\',\n                        shape=(1, x_shape[-1]),\n                        initializer=tf.truncated_normal_initializer(stddev=gain),\n                        trainable=False)\n\n    u_hat = u\n    v_hat = None\n    for _ in range(n_iter):\n        v_ = tf.matmul(u_hat, tf.transpose(x))\n        v_hat = l2_norm(v_)\n\n        u_ = tf.matmul(v_hat, x)\n        u_hat = l2_norm(u_)\n\n    sigma = tf.matmul(tf.matmul(v_hat, x), tf.transpose(u_hat))\n    x_norm = x / sigma\n\n    with tf.control_dependencies([u.assign(u_hat)]):\n        x_norm = tf.reshape(x_norm, x_shape)\n\n    return x_norm\n\n\n# Activations\n\n\ndef prelu(x, stddev=1e-2, reuse=False, name=\'prelu\'):\n    with tf.variable_scope(name):\n        if reuse:\n            tf.get_variable_scope().reuse_variables()\n\n        _alpha = tf.get_variable(\'_alpha\',\n                                 shape=[1],\n                                 initializer=tf.constant_initializer(stddev),\n                                 # initializer=tf.random_normal_initializer(stddev)\n                                 dtype=x.dtype)\n\n        return tf.maximum(_alpha * x, x)\n\n\n# Pooling\n\n\ndef global_avg_pooling(x):\n    return tf.reduce_mean(x, axis=[1, 2])\n\n\n# Losses\n\n\ndef l1_loss(x, y):\n    return tf.reduce_mean(tf.abs(x - y))\n\n\ndef l2_loss(x, y):\n    return tf.nn.l2_loss(y - x)\n\n\ndef mse_loss(x, y, n, is_mean=False):  # ~ l2_loss\n    if is_mean:\n        return tf.reduce_mean(tf.reduce_mean(tf.squared_difference(x, y)))\n    else:\n        return tf.reduce_mean(tf.reduce_sum(tf.squared_difference(x, y)))\n\n\ndef rmse_loss(x, y, n):\n    return tf.sqrt(mse_loss(x, y, n))\n\n\ndef psnr_loss(x, y, n):\n    return 20. * tf.log(tf.reduce_max(x) / mse_loss(x, y, n))\n\n\ndef sce_loss(data, label):\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=data, labels=label))\n\n\ndef softce_loss(data, label):\n    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=data, labels=label))\n\n\ndef ssoftce_loss(data, label):\n    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=data, labels=label))\n\n\n# metrics\n\ndef inception_score(images, img_size=(299, 299), n_splits=10):\n    """""" referenced from https://github.com/tsc2017/Inception-Score/blob/master/inception_score.py """"""\n    assert type(images) == np.ndarray\n    assert len(images.shape) == 4\n    assert images.shape[-1] == 3\n\n    images = np.clip(images, 0., 255.)  # clipped into [0, 255]\n\n    def inception_feat(img, n_splits=1):\n        # img = tf.transpose(img, [0, 2, 3, 1])\n        img = tf.image.resize_bilinear(img, img_size)\n\n        generated_images_list = array_ops.split(img, num_or_size_splits=n_splits)\n\n        logits = functional_ops.map_fn(\n            fn=functools.partial(tf.contrib.gan.eval.run_inception, output_tensor=""logits:0""),\n            elems=array_ops.stack(generated_images_list),\n            parallel_iterations=1,\n            back_prop=False,\n            swap_memory=True,\n            name=""RunClassifier""\n        )\n        logits = array_ops.concat(array_ops.unstack(logits), axis=0)\n        return logits\n\n    inception_images = tf.placeholder(tf.float32, [None, None, None, 3], name=""inception-images"")\n    logits = inception_feat(inception_images)\n\n    def get_inception_probs(x, n_classes=1000):\n        n_batches = len(x) // batch_size\n\n        preds = np.zeros([len(x), n_classes], dtype=np.float32)\n        for i in range(n_batches):\n            inp = x[i * batch_size:(i + 1) * batch_size] / 255. * 2 - 1.  # scaled into [-1, 1]\n            preds[i * batch_size:(i + 1) * batch_size] = logits.eval({inception_images: inp})[:, :n_classes]\n        preds = np.exp(preds) / np.sum(np.exp(preds), 1, keepdims=True)\n        return preds\n\n    def preds2score(preds, splits=10):\n        scores = []\n        for i in range(splits):\n            part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n            kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, axis=0), axis=0)))\n            kl = np.mean(np.sum(kl, axis=1))\n            scores.append(np.exp(kl))\n        return np.mean(scores), np.std(scores)\n\n    preds = get_inception_probs(images)\n    mean, std = preds2score(preds, splits=n_splits)\n    return mean, std\n\n\ndef fid_score(real_img, fake_img, img_size=(299, 299), n_splits=10):\n    assert type(real_img) == np.ndarray and type(fake_img) == np.ndarray\n    assert len(real_img.shape) == 4 and len(fake_img.shape) == 4\n    assert real_img.shape[-1] == 3 and fake_img.shape[-1] == 3\n    assert real_img.shape == fake_img.shape\n\n    real_img = np.clip(real_img, 0., 255.)  # clipped into [0, 255]\n    fake_img = np.clip(fake_img, 0., 255.)  # clipped into [0, 255]\n\n    inception_images = tf.placeholder(tf.float32, [None, None, None, 3], name=""inception-images"")\n    real_acts = tf.placeholder(tf.float32, [None, None], name=""real_activations"")\n    fake_acts = tf.placeholder(tf.float32, [None, None], name=""fake_activations"")\n\n    def inception_activation(images, n_splits=1):\n        # images = tf.transpose(images, [0, 2, 3, 1])\n        images = tf.image.resize_bilinear(images, img_size)\n\n        generated_images_list = array_ops.split(images, num_or_size_splits=n_splits)\n\n        acts = functional_ops.map_fn(\n            fn=functools.partial(tf.contrib.gan.eval.run_inception, output_tensor=""pool_3:0""),\n            elems=array_ops.stack(generated_images_list),\n            parallel_iterations=1,\n            back_prop=False,\n            swap_memory=True,\n            name=""RunClassifier""\n        )\n        acts = array_ops.concat(array_ops.unstack(acts), axis=0)\n        return acts\n\n    activations = inception_activation(inception_images)\n\n    def get_inception_activations(x, feats=2048):\n        n_batches = len(x) // batch_size\n\n        acts = np.zeros([len(x), feats], dtype=np.float32)\n        for i in range(n_batches):\n            inp = x[i * batch_size:(i + 1) * batch_size] / 255. * 2 - 1.  # scaled into [-1, 1]\n            acts[i * batch_size:(i + 1) * batch_size] = activations.eval({inception_images: inp})\n        acts = np.exp(acts) / np.sum(np.exp(acts), 1, keepdims=True)\n        return acts\n\n    def get_fid(real, fake):\n        return tf.contrib.gan.eval.frechet_classifier_distance_from_activations(real_acts, fake_acts).eval(\n            feed_dict={\n                real_acts: real,\n                fake_acts: fake,\n            }\n        )\n\n    real_img_acts = get_inception_activations(real_img)\n    fake_img_acts = get_inception_activations(fake_img)\n\n    fid = get_fid(real_img_acts, fake_img_acts)\n    return fid\n'"
3DGAN/3dgan_model.py,0,b'# initial file\n'
3DGAN/3dgan_train.py,0,b'# initial file\n'
ACGAN/acgan_model.py,34,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass ACGAN:\n\n    def __init__(self, s, batch_size=100, height=32, width=32, channel=3, n_classes=10,\n                 sample_num=10 * 10, sample_size=10,\n                 df_dim=16, gf_dim=384, z_dim=100, lr=2e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 100\n        :param height: image height, default 32\n        :param width: image width, default 32\n        :param channel: image channel, default 3\n        :param n_classes: DataSet\'s classes, default 10\n\n        # Output Settings\n        :param sample_num: the number of output images, default 100\n        :param sample_size: sample image size, default 10\n\n        # For CNN model\n        :param df_dim: discriminator filter, default 16\n        :param gf_dim: generator filter, default 384\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 100\n        :param lr: learning rate, default 2e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = [self.batch_size, self.height, self.width, self.channel]\n        self.n_classes = n_classes\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n\n        self.z_dim = z_dim\n        self.beta1 = 0.5\n        self.beta2 = 0.999\n        self.lr = lr\n\n        # pre-defined\n        self.g_loss = 0.\n        self.d_loss = 0.\n        self.c_loss = 0.\n        \n        self.g = None\n        self.g_test = None\n        \n        self.d_op = None\n        self.g_op = None\n        self.c_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32,\n                                shape=[None, self.height, self.width, self.channel],\n                                name=""x-image"")                                                    # (-1, 32, 32, 3)\n        self.y = tf.placeholder(tf.float32, shape=[None, self.n_classes], name=""y-label"")          # (-1, 10)\n        self.y_rnd = tf.placeholder(tf.float32, shape=[None, self.n_classes], name=""y-rnd-label"")  # (-1, 10)\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=""z-noise"")              # (-1, 100)\n\n        self.build_acgan()  # build ACGAN model\n\n    def discriminator(self, x, reuse=None):\n        """"""\n        # Following a D Network, CiFar-like-hood, referred in the paper\n        :param x: images\n        :param y: labels\n        :param reuse: re-usable\n        :return: classification, probability (fake or real), network\n        """"""\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            x = t.conv2d(x, self.df_dim, 3, 2, name=\'disc-conv2d-1\')\n            x = tf.nn.leaky_relu(x, alpha=0.2)\n            x = tf.layers.dropout(x, 0.5, name=\'disc-dropout2d-1\')\n\n            for i in range(5):\n                x = t.conv2d(x, self.df_dim * (2 ** (i + 1)), k=3, s=(i % 2 + 1), name=\'disc-conv2d-%d\' % (i + 2))\n                x = t.batch_norm(x, reuse=reuse, name=""disc-bn-%d"" % (i + 1))\n                x = tf.nn.leaky_relu(x, alpha=0.2)\n                x = tf.layers.dropout(x, 0.5, name=\'disc-dropout2d-%d\' % (i + 1))\n\n            net = tf.layers.flatten(x)\n\n            cat = t.dense(net, self.n_classes, name=\'disc-fc-cat\')\n            disc = t.dense(net, 1, name=\'disc-fc-disc\')\n\n            return cat, disc, net\n\n    def generator(self, z, y, reuse=None, is_train=True):\n        """"""\n        # Following a G Network, CiFar-like-hood, referred in the paper\n        :param z: noise\n        :param y: image label\n        :param reuse: re-usable\n        :param is_train: trainable\n        :return: prob\n        """"""\n        with tf.variable_scope(""generator"", reuse=reuse):\n            x = tf.concat([z, y], axis=1)  # (-1, 110)\n\n            x = t.dense(x, self.gf_dim, name=\'gen-fc-1\')\n            x = tf.nn.relu(x)\n\n            x = tf.reshape(x, (-1, 4, 4, 24))\n\n            for i in range(1, 3):\n                x = t.deconv2d(x, self.gf_dim // (2 ** i), 5, 2, name=\'gen-deconv2d-%d\' % (i + 1))\n                x = t.batch_norm(x, is_train=is_train, reuse=reuse, name=""gen-bn-%d"" % i)\n                x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.channel, 5, 2, name=\'gen-deconv2d-4\')\n            x = tf.nn.tanh(x)  # scaling to [-1, 1]\n\n            return x\n\n    def build_acgan(self):\n        # Generator\n        self.g = self.generator(self.z, self.y)\n\n        # Discriminator\n        c_real, d_real, _ = self.discriminator(self.x)\n        c_fake, d_fake, _ = self.discriminator(self.g, reuse=True)\n\n        # sigmoid ce loss\n        d_real_loss = t.sce_loss(d_real, tf.ones_like(d_real))\n        d_fake_loss = t.sce_loss(d_fake, tf.zeros_like(d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n        self.g_loss = t.sce_loss(d_fake, tf.ones_like(d_fake))\n\n        # softmax ce loss\n        c_real_loss = t.softce_loss(c_real, self.y)\n        c_fake_loss = t.softce_loss(c_fake, self.y)\n        self.c_loss = c_real_loss + c_fake_loss\n\n        # self.d_loss = self.d_loss + self.c_loss\n        # self.g_loss = self.g_loss - self.c_loss\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/c_real_loss"", c_real_loss)\n        tf.summary.scalar(""loss/c_fake_loss"", c_fake_loss)\n        tf.summary.scalar(""loss/c_loss"", self.c_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n        c_params = [v for v in t_vars if v.name.startswith(\'d\') or v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(self.lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(self.lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.g_loss, var_list=g_params)\n        self.c_op = tf.train.AdamOptimizer(self.lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.c_loss, var_list=c_params)\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
ACGAN/acgan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport acgan_model as acgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CiFarDataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/ACGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epochs\': 101,\n    \'batch_size\': 100,\n    \'global_step\': 50001,\n    \'logging_interval\': 500,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # Loading Cifar-10 DataSet\n    ds = DataSet(height=32,\n                 width=32,\n                 channel=3,\n                 ds_path=""D:/DataSet/cifar/cifar-10-batches-py/"",\n                 ds_name=\'cifar-10\')\n\n    ds_iter = DataIterator(x=iu.transform(ds.train_images, \'127\'),\n                           y=ds.train_labels,\n                           batch_size=train_step[\'batch_size\'],\n                           label_off=False)  # using label # maybe someday, i\'ll change this param\'s name\n\n    # Generated image save\n    test_images = iu.transform(ds.test_images[:100], inv_type=\'127\')\n    iu.save_images(test_images,\n                   size=[10, 10],\n                   image_path=results[\'output\'] + \'sample.png\',\n                   inv_type=\'127\')\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # ACGAN Model\n        model = acgan.ACGAN(s,\n                            batch_size=train_step[\'batch_size\'],\n                            n_classes=ds.n_classes)\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        sample_y = np.zeros(shape=[model.sample_num, model.n_classes])\n        for i in range(10):\n            sample_y[10 * i:10 * (i + 1), i] = 1\n\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        global_step = saved_global_step\n        start_epoch = global_step // (len(ds.train_images) // model.batch_size)           # recover n_epoch\n        ds_iter.pointer = saved_global_step % (len(ds.train_images) // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epochs\']):\n            for batch_x, batch_y in ds_iter.iterate():\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update D network\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.y: batch_y,\n                                      model.z: batch_z,\n                                  })\n\n                # Update G/C networks\n                _, g_loss, _, c_loss = s.run([model.g_op, model.g_loss, model.c_op, model.c_loss],\n                                             feed_dict={\n                                                 model.x: batch_x,\n                                                 model.y: batch_y,\n                                                 model.z: batch_z,\n                                             })\n\n                if global_step % train_step[\'logging_interval\'] == 0:\n                    batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                    d_loss, g_loss, c_loss, summary = s.run([model.d_loss, model.g_loss, model.c_loss, model.merged],\n                                                            feed_dict={\n                                                                model.x: batch_x,\n                                                                model.y: batch_y,\n                                                                model.z: batch_z,\n                                                            })\n\n                    # Print loss\n                    print(""[+] Epoch %04d Step %08d => "" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss),\n                          "" C loss : {:.8f}"".format(c_loss))\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.y: sample_y,\n                                        model.z: sample_z,\n                                    })\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{:08d}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step)\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
AdaGAN/adagan_model.py,27,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass AdaGAN:\n\n    def __init__(self, s, batch_size=64, height=28, width=28, channel=1, n_classes=10,\n                 sample_num=64, sample_size=8,\n                 n_input=784, df_dim=16, gf_dim=16, fc_unit=256, z_dim=100, d_lr=1e-3, g_lr=5e-3, c_lr=1e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 32\n        :param height: input image height, default 28\n        :param width: input image width, default 28\n        :param channel: input image channel, default 1 (gray-scale)\n        :param n_classes: input DataSet\'s classes\n\n        # Output Settings\n        :param sample_num: the number of output images, default 64\n        :param sample_size: sample image size, default 8\n\n        # Hyper Parameters\n        :param n_input: input image size, default 784(28x28)\n        :param df_dim: D net filter, default 16\n        :param gf_dim: G net filter, default 16\n        :param fc_unit: fully connected units, default 256\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 100\n        :param d_lr: discriminator learning rate, default 1e-3\n        :param g_lr: generator learning rate, default 5e-3\n        :param c_lr: classifier learning rate, default 1e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = [self.batch_size, self.height, self.width, self.channel]\n        self.n_classes = n_classes\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.n_input = n_input\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n        self.fc_unit = fc_unit\n\n        self.z_dim = z_dim\n        self.beta1 = 0.5\n        self.d_lr = d_lr\n        self.g_lr = g_lr\n        self.c_lr = c_lr\n\n        self.d_loss = 0.\n        self.g_loss = 0.\n        self.c_loss = 0.\n\n        self.g = None\n\n        self.d_op = None\n        self.g_op = None\n        self.c_op = None\n\n        self.merged = None\n        self.saver = None\n        self.writer = None\n\n        # Placeholder\n        self.x = tf.placeholder(tf.float32, shape=[None, self.n_input], name=""x-image"")  # (-1, 784)\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')    # (-1, 100)\n\n        self.build_adagan()  # build AdaGAN model\n\n    def classifier(self, x, reuse=None):\n        with tf.variable_scope(""classifier"", reuse=reuse):\n            pass\n\n    def discriminator(self, x, reuse=None):\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            for i in range(1, 3):\n                x = t.conv2d(x, self.df_dim * i, 5, 2, name=\'disc-conv2d-%d\' % i)\n                x = t.batch_norm(x, name=\'disc-bn-%d\' % i)\n                x = tf.nn.leaky_relu(x, alpha=0.3)\n\n            x = t.flatten(x)\n\n            logits = t.dense(x, 1, name=\'disc-fc-1\')\n            prob = tf.nn.sigmoid(logits)\n            return prob, logits\n\n    def generator(self, z, reuse=None, is_train=True):\n        with tf.variable_scope(""generator"", reuse=reuse):\n            x = t.dense(z, self.gf_dim * 7 * 7, name=\'gen-fc-1\')\n            x = t.batch_norm(x, name=\'gen-bn-1\')\n            x = tf.nn.leaky_relu(x, alpha=0.3)\n\n            x = tf.reshape(x, [-1, 7, 7, self.gf_dim])\n\n            for i in range(1, 3):\n                x = t.deconv2d(x, self.gf_dim, 5, 2, name=\'gen-deconv2d-%d\' % (i + 1))\n                x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-%d\' % (i + 1))\n                x = tf.nn.leaky_relu(x, alpha=0.3)\n\n            x = t.deconv2d(x, 1, 5, 1, name=\'gen-deconv2d-3\')\n            x = tf.nn.sigmoid(x)\n            return x\n\n    def build_adagan(self):\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        d_real, _ = self.discriminator(self.x)\n        d_fake, _ = self.discriminator(self.g, reuse=True)\n\n        # Losses\n        d_real_loss = -tf.reduce_mean(t.safe_log(d_real))\n        d_fake_loss = -tf.reduce_mean(t.safe_log(1. - d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n        self.g_loss = tf.reduce_mean(t.safe_log(d_fake))\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n        tf.summary.scalar(""loss/c_loss"", self.c_loss)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n        c_params = [v for v in t_vars if v.name.startswith(\'c\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.d_lr,\n                                           beta1=self.beta1).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.g_lr,\n                                           beta1=self.beta1).minimize(self.g_loss, var_list=g_params)\n        self.c_op = tf.train.AdamOptimizer(learning_rate=self.c_lr,\n                                           beta1=self.beta1).minimize(self.c_loss, var_list=c_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
AdaGAN/adagan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport adagan_model as adagan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import MNISTDataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/AdaGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'batch_size\': 128,\n    \'global_step\': 250001,\n    \'logging_interval\': 1000,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # MNIST Dataset load\n    mnist = DataSet(ds_path=""D:\\\\DataSet/mnist/"").data\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # AdaGAN Model\n        model = adagan.AdaGAN(s, batch_size=train_step[\'batch_size\'])\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        for global_step in range(saved_global_step, train_step[\'global_step\']):\n            batch_x, _ = mnist.train.next_batch(model.batch_size)\n            batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n            # Update D network\n            _, d_loss = s.run([model.d_op, model.d_loss],\n                              feed_dict={\n                                  model.x: batch_x,\n                                  model.z: batch_z,\n                              })\n\n            # Update G network\n            for _ in range(2):\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n            if global_step % train_step[\'logging_interval\'] == 0:\n                summary = s.run(model.merged,\n                                feed_dict={\n                                    model.x: batch_x,\n                                    model.z: batch_z,\n                                })\n\n                # Print loss\n                print(""[+] Step %08d => "" % global_step,\n                      "" D loss : {:.8f}"".format(d_loss),\n                      "" G loss : {:.8f}"".format(g_loss))\n\n                # Training G model with sample image and noise\n                sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                samples = s.run(model.g,\n                                feed_dict={\n                                    model.z: sample_z,\n                                })\n\n                samples = np.reshape(samples, model.image_shape)\n\n                # Summary saver\n                model.writer.add_summary(summary, global_step)\n\n                # Export image generated by model G\n                sample_image_height = model.sample_size\n                sample_image_width = model.sample_size\n                sample_dir = results[\'output\'] + \'train_{:08d}.png\'.format(global_step)\n\n                # Generated image save\n                iu.save_images(samples,\n                               size=[sample_image_height, sample_image_width],\n                               image_path=sample_dir)\n\n                # Model save\n                model.saver.save(s, results[\'model\'], global_step)\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
AnoGAN/anogan_model.py,30,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass AnoGAN:\n\n    def __init__(self, s, batch_size=16, height=64, width=64, channel=3, n_classes=41,\n                 sample_num=1, sample_size=1,\n                 df_dim=64, gf_dim=64, fc_unit=1024, lambda_=1e-1, z_dim=128, g_lr=2e-4, d_lr=2e-4,\n                 detect=False, use_label=False):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 16\n        :param height: image height, default 64\n        :param width: image width, default 64\n        :param channel: image channel, default 3 (RGB)\n        - in case of Celeb-A, image size is 108x108x3(HWC).\n        :param n_classes: the number of classes, default 41\n\n        # Output Settings\n        :param sample_num: the number of output images, default 1\n        :param sample_size: sample image size, default 1\n\n        # For CNN model\n        :param df_dim: discriminator filter, default 64\n        :param gf_dim: generator filter, default 64\n        :param fc_unit: fully connected units, default 1024\n\n        # Training Option\n        :param lambda_: anomaly lambda value, default 1e-1\n        :param z_dim: z dimension (kinda noise), default 128\n        :param g_lr: generator learning rate, default 1e-4\n        :param d_lr: discriminator learning rate, default 1e-4\n        :param detect: anomalies detection if True, just training a model, default False\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n        self.test_batch_size = 1\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.n_classes = n_classes\n        self.image_shape = [self.batch_size, self.height, self.width, self.channel]\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n        self.fc_unit = fc_unit\n\n        self.lambda_ = lambda_\n        self.z_dim = z_dim\n        self.beta1 = .5\n        self.beta2 = .9\n        self.d_lr = d_lr\n        self.g_lr = g_lr\n\n        self.detect = detect\n        self.use_label = use_label\n\n        # pre-defined\n        self.d_fake_loss = 0.\n        self.d_real_loss = 0.\n        self.d_loss = 0.\n        self.g_loss = 0.\n        self.r_loss = 0.\n        self.ano_loss = 0.\n\n        self.g = None\n        self.g_test = None\n\n        self.d_op = None\n        self.g_op = None\n        self.ano_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32,\n                                shape=[None, self.height, self.width, self.channel],\n                                name=""x-image"")                                                # (-1, 64, 64, 3)\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')          # (-1, 128)\n        if self.use_label:\n            self.y = tf.placeholder(tf.float32, shape=[None, self.n_classes], name=\'y-label\')  # (-1, 41)\n        else:\n            self.y = None\n\n        self.build_anogan()  # build AnoGAN model\n\n    def discriminator(self, x, y=None, reuse=None, is_train=True):\n        """"""\n        :param x: images\n        :param y: labels\n        :param reuse: re-usable\n        :param is_train: en/disable batch_norm, default True\n        :return: fm, logits\n        """"""\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            if y:\n                raise NotImplementedError(""[-] Not Implemented Yet..."")\n\n            x = t.conv2d(x, f=self.gf_dim * 1, name=""disc-conv2d-1"")\n            x = tf.nn.leaky_relu(x)\n\n            for i in range(1, 4):\n                x = t.conv2d(x, f=self.gf_dim * (2 ** i), name=""disc-conv2d-%d"" % (i + 1))\n                x = t.batch_norm(x, is_train=is_train, name=\'disc-bn-%d\' % (i + 1))\n                x = tf.nn.leaky_relu(x)\n\n            feature_match = x   # (-1, 8, 8, 512)\n\n            x = t.flatten(x)\n\n            x = t.dense(x, 1, name=\'disc-fc-1\')\n\n            return feature_match, x\n\n    def generator(self, z, y=None, reuse=None, is_train=True):\n        """"""\n        :param z: embeddings\n        :param y: labels\n        :param reuse: re-usable\n        :param is_train: en/disable batch_norm, default True\n        :return: prob\n        """"""\n        with tf.variable_scope(""generator"", reuse=reuse):\n            if y:\n                raise NotImplementedError(""[-] Not Implemented Yet..."")\n\n            x = t.dense(z, f=self.fc_unit, name=\'gen-fc-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = tf.reshape(x, [-1, 8, 8, self.fc_unit // (8 * 8)])\n\n            for i in range(1, 4):\n                x = t.deconv2d(x, f=self.gf_dim * (2 ** i), name=""gen-conv2d-%d"" % i)\n                x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-%d\' % i)\n                x = tf.nn.leaky_relu(x)\n\n            x = t.deconv2d(x, f=3, s=1, name=""gen-conv2d-4"")  # (-1, 64, 64, 3)\n            x = tf.sigmoid(x)  # [0, 1]\n            return x\n\n    def build_anogan(self):\n        # Generator\n        self.g = self.generator(self.z, self.y)\n        self.g_test = self.generator(self.z, self.y, reuse=True, is_train=False)\n\n        # Discriminator\n        d_real_fm, d_real = self.discriminator(self.x)\n        d_fake_fm, d_fake = self.discriminator(self.g_test, reuse=True)\n\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'disc\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'gen\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.d_lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(loss=self.d_loss,\n                                                                                        var_list=d_params)\n\n        if self.detect:\n            self.d_loss = t.l1_loss(d_fake_fm, d_real_fm)  # disc     loss\n            self.r_loss = t.l1_loss(self.x, self.g)        # residual loss\n            self.ano_loss = (1. - self.lambda_) * self.r_loss + self.lambda_ * self.d_loss\n\n            tf.summary.scalar(""loss/d_loss"", self.d_loss)\n            tf.summary.scalar(""loss/r_loss"", self.r_loss)\n            tf.summary.scalar(""loss/ano_loss"", self.ano_loss)\n\n            self.ano_op = tf.train.AdamOptimizer(learning_rate=self.g_lr,\n                                                 beta1=self.beta1, beta2=self.beta2).minimize(loss=self.ano_loss,\n                                                                                              var_list=g_params)\n        else:\n            self.d_real_loss = t.sce_loss(d_real, tf.ones_like(d_real))\n            self.d_fake_loss = t.sce_loss(d_fake, tf.zeros_like(d_fake))\n            self.d_loss = self.d_real_loss + self.d_fake_loss\n            self.g_loss = t.sce_loss(d_fake, tf.ones_like(d_fake))\n\n            # Summary\n            tf.summary.scalar(""loss/d_fake_loss"", self.d_fake_loss)\n            tf.summary.scalar(""loss/d_real_loss"", self.d_real_loss)\n            tf.summary.scalar(""loss/d_loss"", self.d_loss)\n            tf.summary.scalar(""loss/g_loss"", self.r_loss)\n\n            self.g_op = tf.train.AdamOptimizer(learning_rate=self.d_lr,\n                                               beta1=self.beta1, beta2=self.beta2).minimize(loss=self.d_loss,\n                                                                                            var_list=d_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        if not self.detect:\n            self.writer = tf.summary.FileWriter(\'./orig-model/\', self.s.graph)\n        else:\n            self.writer = tf.summary.FileWriter(\'./ano-model/\', self.s.graph)\n'"
AnoGAN/anogan_train.py,6,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport os\nimport sys\nimport time\n\nimport anogan_model as anogan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CelebADataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'orig-model\': \'./orig-model/AnoGAN-model.ckpt\',\n    \'ano-model\': \'./ano-model/AnoGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epoch\': 100,\n    \'batch_size\': 64,\n    \'logging_step\': 2000,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        if os.path.exists(""./orig-model/""):\n            detect = True  # There has to be pre-trained file\n        else:\n            detect = False\n\n        # AnoGAN Model\n        model = anogan.AnoGAN(detect=detect,\n                              use_label=False)  # AnoGAN\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        # loading CelebA DataSet\n        ds = DataSet(height=64,\n                     width=64,\n                     channel=3,\n                     ds_image_path=""D:\\\\DataSet/CelebA/CelebA-64.h5"",\n                     ds_label_path=""D:\\\\DataSet/CelebA/Anno/list_attr_celeba.txt"",\n                     # ds_image_path=""D:\\\\DataSet/CelebA/Img/img_align_celeba/"",\n                     ds_type=""CelebA"",\n                     use_save=False,\n                     save_file_name=""D:\\\\DataSet/CelebA/CelebA-128.h5"",\n                     save_type=""to_h5"",\n                     use_img_scale=False,\n                     # img_scale=""-1,1""\n                     )\n\n        # saving sample images\n        test_images = np.reshape(iu.transform(ds.images[:16], inv_type=\'127\'), (16, 64, 64, 3))\n        iu.save_images(test_images,\n                       size=[4, 4],\n                       image_path=results[\'output\'] + \'sample.png\',\n                       inv_type=\'127\')\n\n        ds_iter = DataIterator(x=ds.images,\n                               y=None,\n                               batch_size=train_step[\'batch_size\'],\n                               label_off=True)\n\n        # To-Do\n        # Getting anomaly data\n\n        # Load model & Graph & Weights\n        if not detect or not os.path.exists(""./ano-model/""):\n            ckpt = tf.train.get_checkpoint_state(\'./orig-model/\')\n        else:\n            ckpt = tf.train.get_checkpoint_state(\'./ano-model/\')\n\n        saved_global_step = 0\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        global_step = saved_global_step\n        start_epoch = global_step // (ds.num_images // model.batch_size)           # recover n_epoch\n        ds_iter.pointer = saved_global_step % (ds.num_images // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epoch\']):\n            for batch_images in ds_iter.iterate():\n                batch_x = np.reshape(batch_images, [-1] + model.image_shape[1:])\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update D network\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.z: batch_z,\n                                  })\n\n                if global_step % train_step[\'logging_step\'] == 0:\n                    batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                    # Summary\n                    d_loss, g_loss, summary = s.run([model.d_loss, model.g_loss, model.merged],\n                                                    feed_dict={\n                                                        model.x: batch_x,\n                                                        model.z: batch_z,\n                                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %04d Step %07d =>"" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss))\n\n                    # Summary saver\n                    model.writer.add_summary(summary, epoch)\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                    samples = s.run(model.g_test,\n                                    feed_dict={\n                                        model.z: sample_z,\n                                    })\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{0}_{1}.png\'.format(epoch, global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir)\n\n                    # Model save\n                    if not detect:\n                        model.saver.save(s, results[\'orig-model\'], global_step=global_step)\n                    else:\n                        model.saver.save(s, results[\'ano-model\'], global_step=global_step)\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
BEGAN/began_model.py,48,"b'import tensorflow as tf\nimport numpy as np\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass BEGAN:\n\n    def __init__(self, s, batch_size=16, height=64, width=64, channel=3,\n                 sample_num=10 * 10, sample_size=10,\n                 df_dim=64, gf_dim=64, gamma=0.5, lambda_k=1e-3, z_dim=128, g_lr=2e-4, d_lr=2e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 16\n        :param height: image height, default 64\n        :param width: image width, default 64\n        :param channel  image channel, default 3 (RGB)\n        - in case of Celeb-A, image size is 32x32x3/64x64x3(HWC).\n\n        # Output Settings\n        :param sample_num: the number of output images, default 100\n        :param sample_size: sample image size, default 10\n\n        # For CNN model\n        :param df_dim: discriminator filter, default 64\n        :param gf_dim: generator filter, default 64\n\n        # Training Option\n        :param gamma: gamma value, default 0.5\n        :param lambda_k: lr adjustment value lambda k, default 1e-3\n        :param z_dim: z dimension (kinda noise), default 128\n        :param g_lr: generator learning rate, default 1e-4\n        :param d_lr: discriminator learning rate, default 1e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = [self.batch_size, self.height, self.width, self.channel]\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n\n        self.gamma = gamma  # 0.3 ~ 0.7\n        self.lambda_k = lambda_k\n        self.z_dim = z_dim\n        self.beta1 = .5\n        self.beta2 = .9\n        self.d_lr = tf.Variable(d_lr, name=\'d_lr\')\n        self.g_lr = tf.Variable(g_lr, name=\'g_lr\')\n        self.lr_decay_rate = .5\n        self.lr_low_boundary = 1e-5\n\n        # pre-defined\n        self.d_real = 0.\n        self.d_fake = 0.\n        self.g_loss = 0.\n        self.d_loss = 0.\n        self.m_global = 0.\n        self.balance = 0.\n\n        self.g = None\n\n        self.k_update = None\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # LR/k update\n        self.k = tf.Variable(0., trainable=False, name=\'k_t\')  # 0 < k_t < 1, k_0 = 0\n\n        self.lr_update_step = 1e5\n        self.d_lr_update = tf.assign(self.d_lr, tf.maximum(self.d_lr * self.lr_decay_rate, self.lr_low_boundary))\n        self.g_lr_update = tf.assign(self.g_lr, tf.maximum(self.g_lr * self.lr_decay_rate, self.lr_low_boundary))\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel],\n                                name=""x-image"")  # (-1, 64, 64, 3)\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim],\n                                name=\'z-noise\')  # (-1, 128)\n\n        self.build_began()  # build BEGAN model\n\n    def encoder(self, x, reuse=None):\n        """"""\n        :param x: Input images (32x32x3 or 64x64x3)\n        :param reuse: re-usable\n        :return: embeddings\n        """"""\n        with tf.variable_scope(\'encoder\', reuse=reuse):\n            repeat = int(np.log2(self.height)) - 2\n\n            x = t.conv2d(x, f=self.df_dim, name=""enc-conv2d-1"")\n            x = tf.nn.elu(x)\n\n            for i in range(1, repeat + 1):\n                f = self.df_dim * i\n\n                x = t.conv2d(x, f, 3, 1, name=""enc-conv2d-%d"" % (i * 2))\n                x = tf.nn.elu(x)\n                x = t.conv2d(x, f, 3, 1, name=""enc-conv2d-%d"" % (i * 2 + 1))\n                x = tf.nn.elu(x)\n\n                if i < repeat:\n                    """"""\n                        You can choose one of them. max-pool or avg-pool or conv-pool.\n                        Speed Order : conv-pool > avg-pool > max-pool. i guess :)\n                    """"""\n                    x = t.conv2d(x, f, 3, 2, name=\'enc-conv2d-pool-%d\' % i)  # conv pooling\n                    x = tf.nn.elu(x)\n\n            x = t.flatten(x)\n\n            x = t.dense(x, self.z_dim, name=\'enc-fc-1\')  # normally, (-1, 128)\n            return x\n\n    def decoder(self, z, reuse=None):\n        """"""\n        :param z: embeddings\n        :param reuse: re-usable\n        :return: logits\n        """"""\n        with tf.variable_scope(\'decoder\', reuse=reuse):\n            repeat = int(np.log2(self.height)) - 2\n\n            x = t.dense(z, self.z_dim * 8 * 8, name=\'dec-fc-1\')\n            x = tf.reshape(x, [-1, 8, 8, self.z_dim])\n\n            # shortcut = tf.identity(x, name=\'shortcut\')\n\n            for i in range(1, repeat + 1):\n                x = t.conv2d(x, self.gf_dim, 3, 1, name=""dec-conv2d-%d"" % (i * 2 - 1))\n                x = tf.nn.elu(x)\n                x = t.conv2d(x, self.gf_dim, 3, 1, name=""dec-conv2d-%d"" % (i * 2))\n                x = tf.nn.elu(x)\n\n                # if i < 3:\n                #     x = tf.add(x, shortcut)\n\n                if i < repeat:\n                    x = t.up_sampling(x, tf.image.ResizeMethod.NEAREST_NEIGHBOR)  # NN up-sampling\n\n            x = t.conv2d(x, f=self.channel)\n            x = tf.nn.tanh(x)\n            return x\n\n    def discriminator(self, x, reuse=None):\n        """"""\n        :param x: images\n        :param reuse: re-usable\n        :return: logits\n        """"""\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            z = self.encoder(x, reuse=reuse)\n            x = self.decoder(z, reuse=reuse)\n            return x\n\n    def generator(self, z, reuse=None):\n        """"""\n        :param z: embeddings\n        :param reuse: re-usable\n        :return: logits\n        """"""\n        with tf.variable_scope(""generator"", reuse=reuse):\n            repeat = int(np.log2(self.height)) - 2\n\n            x = t.dense(z, self.z_dim * 8 * 8, name=\'gen-fc-1\')\n            x = tf.nn.elu(x)\n\n            x = tf.reshape(x, [-1, 8, 8, self.z_dim])\n\n            # shortcut = tf.identity(x, name=\'shortcut\')\n\n            for i in range(1, repeat + 1):\n                x = t.conv2d(x, f=self.gf_dim, name=""gen-conv2d-%d"" % (i * 2 - 1))\n                x = tf.nn.elu(x)\n                x = t.conv2d(x, f=self.gf_dim, name=""gen-conv2d-%d"" % (i * 2))\n                x = tf.nn.elu(x)\n\n                # if i < 3:\n                #     x = tf.add(x, shortcut)\n\n                if i < repeat:\n                    x = t.up_sampling(x, tf.image.ResizeMethod.NEAREST_NEIGHBOR)  # NN up-sampling\n\n            x = t.conv2d(x, f=self.channel, name=\'gen-conv2d-%d\' % (2 * repeat + 1))\n            x = tf.nn.tanh(x)\n            return x\n\n    def build_began(self):\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        d_real = self.discriminator(self.x)\n        d_fake = self.discriminator(self.g, reuse=True)\n\n        # Loss\n        d_real_loss = t.l1_loss(self.x, d_real)\n        d_fake_loss = t.l1_loss(self.g, d_fake)\n        self.d_loss = d_real_loss - self.k * d_fake_loss\n        self.g_loss = d_fake_loss\n\n        # Convergence Metric\n        self.balance = self.gamma * d_real_loss - self.g_loss  # (=d_fake_loss)\n        self.m_global = d_real_loss + tf.abs(self.balance)\n\n        # k_t update\n        self.k_update = tf.assign(self.k,  tf.clip_by_value(self.k + self.lambda_k * self.balance, 0, 1))\n\n        # Summary\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n        tf.summary.scalar(""misc/balance"", self.balance)\n        tf.summary.scalar(""misc/m_global"", self.m_global)\n        tf.summary.scalar(""misc/k_t"", self.k)\n        tf.summary.scalar(""misc/d_lr"", self.d_lr)\n        tf.summary.scalar(""misc/g_lr"", self.g_lr)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.d_lr_update,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.g_lr_update,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
BEGAN/began_train.py,6,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport began_model as began\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CelebADataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/BEGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epoch\': 25,\n    \'batch_size\': 16,\n    \'logging_step\': 1000,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # loading CelebA DataSet\n    ds = DataSet(height=64,\n                 width=64,\n                 channel=3,\n                 ds_image_path=""/home/zero/hdd/DataSet/CelebA/CelebA-64.h5"",\n                 ds_label_path=""/home/zero/hdd/DataSet/CelebA/Anno/list_attr_celeba.txt"",\n                 # ds_image_path=""/home/zero/hdd/DataSet/CelebA/Img/img_align_celeba/"",\n                 ds_type=""CelebA"",\n                 use_save=False,\n                 save_file_name=""/home/zero/hdd/DataSet/CelebA/CelebA-64.h5"",\n                 save_type=""to_h5"",\n                 use_img_scale=False,\n                 # img_scale=""-1,1""\n                 )\n\n    # saving sample images\n    test_images = np.reshape(iu.transform(ds.images[:100], inv_type=\'127\'), (100, 64, 64, 3))\n    iu.save_images(test_images,\n                   size=[10, 10],\n                   image_path=results[\'output\'] + \'sample.png\',\n                   inv_type=\'127\')\n\n    ds_iter = DataIterator(x=ds.images,\n                           y=None,\n                           batch_size=train_step[\'batch_size\'],\n                           label_off=True)\n\n    # GPU configure\n    gpu_config = tf.GPUOptions(allow_growth=True)\n    config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_config)\n\n    with tf.Session(config=config) as s:\n        # BEGAN Model\n        model = began.BEGAN(s, batch_size=train_step[\'batch_size\'], gamma=0.5)  # BEGAN\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        print(""[*] Reading checkpoints..."")\n\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        global_step = saved_global_step\n        start_epoch = global_step // (ds.num_images // model.batch_size)           # recover n_epoch\n        ds_iter.pointer = saved_global_step % (ds.num_images // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epoch\']):\n            for batch_x in ds_iter.iterate():\n                batch_x = iu.transform(batch_x, inv_type=\'127\')\n                batch_x = np.reshape(batch_x, (model.batch_size, model.height, model.width, model.channel))\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update D network\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.z: batch_z,\n                                  })\n\n                # Update k_t\n                _, k, m_global = s.run([model.k_update, model.k, model.m_global],\n                                       feed_dict={\n                                            model.x: batch_x,\n                                            model.z: batch_z,\n                                       })\n\n                if global_step % train_step[\'logging_step\'] == 0:\n                    summary = s.run(model.merged,\n                                    feed_dict={\n                                        model.x: batch_x,\n                                        model.z: batch_z,\n                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %03d Step %07d =>"" % (epoch, global_step),\n                          "" D loss : {:.6f}"".format(d_loss),\n                          "" G loss : {:.6f}"".format(g_loss),\n                          "" k : {:.6f}"".format(k),\n                          "" M : {:.6f}"".format(m_global))\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.z: sample_z,\n                                    })\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{0}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step=global_step)\n\n                # Learning Rate update\n                if global_step and global_step % model.lr_update_step == 0:\n                    s.run([model.g_lr_update, model.d_lr_update])\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
BGAN/bgan_model.py,22,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass BGAN:\n\n    def __init__(self, s, batch_size=64, height=28, width=28, channel=1, n_classes=10,\n                 sample_num=10 * 10, sample_size=10,\n                 n_input=784, fc_unit=256, z_dim=128, g_lr=1e-4, d_lr=1e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: input image height, default 28\n        :param width: input image width, default 28\n        :param channel: input image channel, default 1 (gray-scale)\n        - in case of MNIST, image size is 28x28x1(HWC).\n        :param n_classes: input dataset\'s classes\n        - in case of MNIST, 10 (0 ~ 9)\n\n        # Output Settings\n        :param sample_num: the number of output images, default 100\n        :param sample_size: sample image size, default 10\n\n        # For DNN model\n        :param n_input: input image size, default 784(28x28)\n        :param fc_unit: fully connected units, default 256\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 128\n        :param g_lr: generator learning rate, default 1e-4\n        :param d_lr: discriminator learning rate, default 1e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = [self.batch_size, self.height, self.width, self.channel]\n        self.n_classes = n_classes\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.n_input = n_input\n        self.fc_unit = fc_unit\n\n        self.z_dim = z_dim\n        self.beta1 = .5\n        self.beta2 = .9\n        self.d_lr, self.g_lr = d_lr, g_lr\n\n        # pre-defined\n        self.d_loss = 0.\n        self.g_loss = 0.\n\n        self.g = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholder\n        self.x = tf.placeholder(tf.float32, shape=[None, self.n_input], name=""x-image"")  # (-1, 784)\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')    # (-1, 100)\n\n        self.build_bgan()  # build BGAN model\n\n    def discriminator(self, x, reuse=None):\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            for i in range(2):\n                x = t.dense(x, self.fc_unit, name=\'disc-fc-%d\' % (i + 1))\n                x = tf.nn.leaky_relu(x)\n\n            logits = t.dense(x, 1, name=\'disc-fc-3\')\n            prob = tf.nn.sigmoid(logits)\n\n        return prob, logits\n\n    def generator(self, z, reuse=None, is_train=True):\n        with tf.variable_scope(""generator"", reuse=reuse):\n            x = z\n            for i in range(2):\n                x = t.dense(x, self.fc_unit, name=\'gen-fc-%d\' % (i + 1))\n                x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-%d\' % (i + 1))\n                x = tf.nn.leaky_relu(x)\n\n            logits = t.dense(x, self.n_input, name=\'gen-fc-3\')\n            prob = tf.nn.sigmoid(logits)\n\n        return prob\n\n    def build_bgan(self):\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        d_real, _ = self.discriminator(self.x)\n        d_fake, _ = self.discriminator(self.g, reuse=True)\n\n        # Losses\n        d_real_loss = -tf.reduce_mean(t.safe_log(d_real))\n        d_fake_loss = -tf.reduce_mean(t.safe_log(1. - d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n        self.g_loss = tf.reduce_mean(tf.square(t.safe_log(d_fake) + t.safe_log(1. - d_fake))) / 2.\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.d_lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.g_lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
BGAN/bgan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport bgan_model as bgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import MNISTDataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/BGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'global_step\': 200001,\n    \'logging_interval\': 1000,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # MNIST Dataset load\n    mnist = DataSet(ds_path=""D:/DataSet/mnist/"").data\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # BGAN Model\n        model = bgan.BGAN(s)\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        d_loss = 0.\n        d_overpowered = False\n        for global_step in range(saved_global_step, train_step[\'global_step\']):\n            batch_x, _ = mnist.train.next_batch(model.batch_size)\n            batch_x = batch_x.reshape(-1, model.n_input)\n            batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n            # Update D network\n            if not d_overpowered:\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n            # Update G network\n            _, g_loss = s.run([model.g_op, model.g_loss],\n                              feed_dict={\n                                  model.x: batch_x,\n                                  model.z: batch_z,\n                              })\n\n            d_overpowered = d_loss < g_loss / 2.\n\n            if global_step % train_step[\'logging_interval\'] == 0:\n                batch_x, _ = mnist.train.next_batch(model.batch_size)\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                d_loss, g_loss, summary = s.run([model.d_loss, model.g_loss, model.merged],\n                                                feed_dict={\n                                                    model.x: batch_x,\n                                                    model.z: batch_z,\n                                                })\n\n                # Print loss\n                print(""[+] Step %08d => "" % global_step,\n                      "" D loss : {:.8f}"".format(d_loss),\n                      "" G loss : {:.8f}"".format(g_loss))\n\n                # Training G model with sample image and noise\n                sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                samples = s.run(model.g,\n                                feed_dict={\n                                    model.z: sample_z,\n                                })\n\n                samples = np.reshape(samples, [-1] + model.image_shape[1:])\n\n                # Summary saver\n                model.writer.add_summary(summary, global_step)\n\n                # Export image generated by model G\n                sample_image_height = model.sample_size\n                sample_image_width = model.sample_size\n                sample_dir = results[\'output\'] + \'train_{:08d}.png\'.format(global_step)\n\n                # Generated image save\n                iu.save_images(samples,\n                               size=[sample_image_height, sample_image_width],\n                               image_path=sample_dir)\n\n                # Model save\n                model.saver.save(s, results[\'model\'], global_step=global_step)\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
BigGAN/biggan_model.py,53,"b'import tensorflow as tf\nimport numpy as np\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\nnp.random.seed(777)\ntf.set_random_seed(777)  # reproducibility\n\n\nclass BigGAN:\n\n    def __init__(self, s, batch_size=64, height=128, width=128, channel=3, n_classes=10,\n                 sample_num=10 * 10, sample_size=10,\n                 df_dim=64, gf_dim=64, fc_unit=512, z_dim=128, lr=1e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: image height, default 128\n        :param width: image width, default 128\n        :param channel: image channel, default 3\n        :param n_classes: number of classes, default 10\n\n        # Output Settings\n        :param sample_num: the number of output images, default 100\n        :param sample_size: sample image size, default 10\n\n        # For Model\n        :param df_dim: discriminator conv filter, default 64\n        :param gf_dim: generator conv filter, default 64\n        :param fc_unit: the number of fully connected layer units, default 512\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 128\n        :param lr: learning rate, default 1e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = [self.batch_size, self.height, self.width, self.channel]\n        self.n_classes = n_classes\n\n        assert self.height == self.width\n        assert self.height in [128, 256, 512]\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n        self.fc_unit = fc_unit\n\n        self.up_sampling = True\n\n        self.gain = 2 ** 0.5\n\n        self.z_dim = z_dim\n        self.beta1 = 0.\n        self.beta2 = .9\n        self.lr = lr\n\n        self.res_block_disc = None\n        self.res_block_gen = None\n        if self.height == 128:\n            self.res_block_disc = ([16, 8, 4, 2], [1])\n            self.res_block_gen = ([1], [2, 4, 8, 16, 16])\n        elif self.height == 256:\n            self.res_block_disc = ([16, 8, 8, 4, 2], [1])\n            self.res_block_gen = ([1, 2], [4, 8, 8, 16, 16])\n        elif self.height == 512:\n            self.res_block_disc = ([16, 8, 8, 4], [2, 1, 1])\n            self.res_block_gen = ([1, 1, 2], [4, 8, 8, 16, 16])\n        else:\n            raise NotImplementedError\n\n        # pre-defined\n        self.g_loss = 0.\n        self.d_loss = 0.\n        self.c_loss = 0.\n\n        self.inception_score = None\n        self.fid_score = None\n\n        self.g = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32,\n                                shape=[None, self.height, self.width, self.channel],\n                                name=""x-image"")  # (bs, 128, 128, 3)\n        self.y = tf.placeholder(tf.float32,\n                                shape=[None, self.n_classes],\n                                name=""y-label"")  # (bs, n_classes)\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=""z-noise"")  # (-1, 128)\n\n        self.build_sagan()  # build BigGAN model\n\n    @staticmethod\n    def res_block(x, f, scale_type, use_bn=True, name=""""):\n        with tf.variable_scope(""res_block-%s"" % name):\n            assert scale_type in [""up"", ""down""]\n            scale_up = False if scale_type == ""down"" else True\n\n            ssc = x\n\n            x = t.batch_norm(x, name=""bn-1"") if use_bn else x\n            x = tf.nn.relu(x)\n            x = t.conv2d_alt(x, f, sn=True, name=""conv2d-1"")\n\n            x = t.batch_norm(x, name=""bn-2"") if use_bn else x\n            x = tf.nn.relu(x)\n\n            if not scale_up:\n                x = t.conv2d_alt(x, f, sn=True, name=""conv2d-2"")\n                x = tf.layers.average_pooling2d(x, pool_size=(2, 2))\n            else:\n                x = t.deconv2d_alt(x, f, sn=True, name=""up-sampling"")\n\n            return x + ssc\n\n    @staticmethod\n    def self_attention(x, f_, reuse=None):\n        with tf.variable_scope(""attention"", reuse=reuse):\n            f = t.conv2d_alt(x, f_ // 8, k=1, s=1, sn=True, name=\'attention-conv2d-f\')\n            g = t.conv2d_alt(x, f_ // 8, k=1, s=1, sn=True, name=\'attention-conv2d-g\')\n            h = t.conv2d_alt(x, f_, k=1, s=1, sn=True, name=\'attention-conv2d-h\')\n\n            f, g, h = t.hw_flatten(f), t.hw_flatten(g), t.hw_flatten(h)\n\n            s = tf.matmul(g, f, transpose_b=True)\n            attention_map = tf.nn.softmax(s, axis=-1, name=\'attention_map\')\n\n            o = tf.reshape(tf.matmul(attention_map, h), shape=x.get_shape())\n            gamma = tf.get_variable(\'gamma\', shape=[1], initializer=tf.zeros_initializer())\n\n            x = gamma * o + x\n            return x\n\n    @staticmethod\n    def non_local_block(x, f, sub_sampling=False, name=""nonlocal""):\n        """""" non-local block, https://arxiv.org/pdf/1711.07971.pdf """"""\n        with tf.variable_scope(""non_local_block-%s"" % name):\n            with tf.name_scope(""theta""):\n                theta = t.conv2d(x, f=f, k=1, s=1, name=""theta"")\n                if sub_sampling:\n                    theta = tf.layers.max_pooling2d(theta, pool_size=(2, 2), name=""max_pool-theta"")\n                theta = tf.reshape(theta, (-1, theta.get_shape().as_list()[-1]))\n\n            with tf.name_scope(""phi""):\n                phi = t.conv2d(x, f=f, k=1, s=1, name=""phi"")\n                if sub_sampling:\n                    phi = tf.layers.max_pooling2d(theta, pool_size=(2, 2), name=""max_pool-phi"")\n                phi = tf.reshape(phi, (-1, phi.get_shape().as_list()[-1]))\n                phi = tf.transpose(phi, [1, 0])\n\n            with tf.name_scope(""g""):\n                g = t.conv2d(x, f=f, k=1, s=1, name=""g"")\n                if sub_sampling:\n                    g = tf.layers.max_pooling2d(theta, pool_size=(2, 2), name=""max_pool-g"")\n                g = tf.reshape(g, (-1, g.get_shape().as_list()[-1]))\n\n            with tf.name_scope(""self-attention""):\n                theta_phi = tf.tensordot(theta, phi, axis=-1)\n                theta_phi = tf.nn.softmax(theta_phi)\n\n                theta_phi_g = tf.tensordot(theta_phi, g, axis=-1)\n\n            theta_phi_g = t.conv2d(theta_phi_g, f=f, k=1, s=1, name=""theta_phi_g"")\n            return x + theta_phi_g\n\n    def discriminator(self, x, reuse=None):\n        """"""\n        :param x: images\n        :param reuse: re-usable\n        :return: classification, probability (fake or real), network\n        """"""\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            f = 1 * self.channel\n\n            x = self.res_block(x, f=f, scale_type=""down"", name=""disc-res1"")\n\n            x = self.self_attention(x, f_=f)\n\n            for i in range(4):\n                f *= 2\n                x = self.res_block(x, f=f, scale_type=""down"", name=""disc-res%d"" % (i + 1))\n\n            x = self.res_block(x, f=f, scale_type=""down"", use_bn=False, name=""disc-res5"")\n            x = tf.nn.relu(x)\n\n            with tf.name_scope(""global_sum_pooling""):\n                x_shape = x.get_shape().as_list()\n                x = tf.reduce_mean(x, axis=-1) * (x_shape[1] * x_shape[2])\n\n            x = t.dense_alt(x, 1, sn=True, name=\'disc-dense-last\')\n            return x\n\n    def generator(self, z, reuse=None):\n        """"""\n        :param z: noise\n        :param reuse: re-usable\n        :return: prob\n        """"""\n        with tf.variable_scope(""generator"", reuse=reuse):\n            # split\n            z = tf.split(z, num_or_size_splits=4, axis=-1)  # expected [None, 32] * 4\n\n            # linear projection\n            x = t.dense_alt(z, f=4 * 4 * 16 * self.channel, sn=True, use_bias=False, name=""gen-dense-1"")\n            x = tf.nn.relu(x)\n\n            x = tf.reshape(x, (-1, 4, 4, 16 * self.channel))\n\n            res = x\n\n            f = 16 * self.channel\n            for i in range(4):\n                res = self.res_block(res,\n                                     f=f,\n                                     scale_type=""up"",\n                                     name=""gen-res%d"" % (i + 1))\n                f //= 2\n\n            x = self.self_attention(res, f_=f * 2)\n\n            x = self.res_block(x, f=1 * self.channel, scale_type=""up"", name=""gen-res4"")\n\n            x = t.batch_norm(x, name=""gen-bn-last"")  # <- noise\n            x = tf.nn.relu(x)\n            x = t.conv2d_alt(x, f=self.channel, k=3, sn=True, name=""gen-conv2d-last"")\n\n            x = tf.nn.tanh(x)\n            return x\n\n    def build_sagan(self):\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        d_real = self.discriminator(self.x)\n        d_fake = self.discriminator(self.g, reuse=True)\n\n        # Losses\n        d_real_loss = t.sce_loss(d_real, tf.ones_like(d_real))\n        d_fake_loss = t.sce_loss(d_fake, tf.zeros_like(d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n        self.g_loss = t.sce_loss(d_fake, tf.ones_like(d_fake))\n\n        self.inception_score = t.inception_score(self.g)\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n        tf.summary.scalar(""metric/inception_score"", self.inception_score)\n        tf.summary.scalar(""metric/fid_score"", self.fid_score)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(self.lr * 4,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(self.lr * 1,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
BigGAN/biggan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport os\nimport sys\nimport time\n\nimport biggan_model as biggan\n\nsys.path.append(\'../\')\nimport image_utils as iu\n\nfrom config import get_config\nfrom datasets import DataIterator\nfrom datasets import CelebADataSet as DataSet\n\n\ncfg, _ = get_config()\n\n\ntrain_step = {\n    \'batch_size\': 64,\n    \'global_step\': 1000001,\n    \'logging_interval\': 500,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    height, width, channel = 128, 128, 3\n\n    # loading CelebA DataSet\n    ds = DataSet(height=height,\n                 width=height,\n                 channel=channel,\n                 # ds_image_path=""D:\\\\DataSet/CelebA/CelebA-%d.h5"" % height,\n                 ds_label_path=os.path.join(cfg.celeba, ""Anno/list_attr_celeba.txt""),\n                 ds_image_path=os.path.join(cfg.celeba, ""Img/img_align_celeba/""),\n                 ds_type=""CelebA"",\n                 use_save=True,\n                 save_file_name=os.path.join(cfg.celeba, ""CelebA-%d.h5"" % height),\n                 save_type=""to_h5"",\n                 use_img_scale=False,\n                 )\n\n    # saving sample images\n    test_images = np.reshape(iu.transform(ds.images[:16], inv_type=\'127\'), (16, height, width, channel))\n    iu.save_images(test_images,\n                   size=[4, 4],\n                   image_path=os.path.join(cfg.output, ""sample.png""),\n                   inv_type=\'127\')\n\n    ds_iter = DataIterator(x=ds.images,\n                           y=None,\n                           batch_size=train_step[\'batch_size\'],\n                           label_off=True)\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # BigGAN Model\n        model = biggan.BigGAN(s,\n                              height=height, width=width, channel=channel,\n                              batch_size=train_step[\'batch_size\'])\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        print(""[*] Reading checkpoints..."")\n\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(cfg.model_path)\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        global_step = saved_global_step\n        start_epoch = global_step // (ds.num_images // model.batch_size)  # recover n_epoch\n        ds_iter.pointer = saved_global_step % (ds.num_images // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epochs\']):\n            for batch_x in ds_iter.iterate():\n                batch_x = iu.transform(batch_x, inv_type=\'127\')\n                batch_x = np.reshape(batch_x, (model.batch_size, model.height, model.width, model.channel))\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update D network\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                if global_step % train_step[\'logging_interval\'] == 0:\n                    summary = s.run(model.merged,\n                                    feed_dict={\n                                        model.x: batch_x,\n                                        model.z: batch_z,\n                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %04d Step %08d => "" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss))\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                    samples = s.run(model.g_test,\n                                    feed_dict={\n                                        model.z: sample_z,\n                                    })\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = os.path.join(cfg.output, \'train_{:08d}.png\'.format(global_step))\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, os.path.join(cfg.model_path, ""BigGAN.ckpt""), global_step)\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
CGAN/cgan_model.py,47,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)\n\n\nclass CGAN:\n\n    def __init__(self, s, batch_size=32, height=28, width=28, channel=1, n_classes=10,\n                 sample_num=10 * 10, sample_size=10,\n                 n_input=784, fc_unit=256, z_dim=100, g_lr=8e-4, d_lr=8e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 32\n        :param height: input image height, default 28\n        :param width: input image width, default 28\n        :param channel: input image channel, default 1 (gray-scale)\n        - in case of MNIST, image size is 28x28x1(HWC).\n        :param n_classes: input dataset\'s classes\n        - in case of MNIST, 10 (0 ~ 9)\n\n        # Output Settings\n        :param sample_num: the number of output images, default 64\n        :param sample_size: sample image size, default 8\n\n        # For DNN model\n        :param n_input: input image size, default 784 (28x28)\n        :param fc_unit: fully connected units, default 256\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 100\n        :param g_lr: generator learning rate, default 8e-4\n        :param d_lr: discriminator learning rate, default 8e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.n_classes = n_classes\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.n_input = n_input\n        self.fc_unit = fc_unit\n\n        self.z_dim = z_dim\n        self.g_lr = g_lr\n        self.d_lr = d_lr\n        self.beta1 = 0.5\n\n        # pre-defined\n        self.d_loss = 0.\n        self.g_loss = 0.\n\n        self.g = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32, shape=[None, self.n_input], name=""x-image"")        # (-1, 784)\n        self.c = tf.placeholder(tf.float32, shape=[None, self.n_classes], name=\'c-condition\')  # (-1, 10)\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')          # (-1, 100)\n        self.do_rate = tf.placeholder(tf.float32, shape=[], name=\'do_rate\')\n\n        self.build_cgan()  # build CGAN model\n\n    def discriminator(self, x, y, do_rate=0.5, reuse=None):\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            """"""\n            x = t.dense(x, self.fc_unit * 5, name=\'disc-fc-x\')\n            x = tf.reshape(x, (-1, self.fc_unit, 5))\n            x = tf.reduce_max(x, axis=-1, keepdims=False, name=\'disc-maxout-x\')\n            x = tf.nn.relu(x)\n            x = tf.layers.dropout(x, do_rate, name=\'disc-do-x\')\n\n            y = t.dense(y, (self.fc_unit // 4) * 5, name=\'disc-fc-y\')\n            y = tf.reshape(y, (-1, (self.fc_unit // 4), 5))\n            y = tf.reduce_max(y, axis=-1, keepdims=False, name=\'disc-maxout-y\')\n            y = tf.nn.relu(y)\n            y = tf.layers.dropout(y, do_rate, name=\'disc-do-y\')\n            """"""\n\n            x = tf.concat([x, y], axis=1)\n\n            x = t.dense(x, self.fc_unit * 5, name=\'disc-fc-1\')\n            x = tf.reshape(x, (-1, self.fc_unit, 5))\n            x = tf.reduce_max(x, axis=-1, keepdims=False, name=\'disc-maxout-1\')\n            x = tf.nn.relu(x)\n            x = tf.layers.dropout(x, do_rate, name=\'disc-do-1\')\n\n            x = t.dense(x, self.fc_unit * 4, name=\'disc-fc-2\')\n            x = tf.reshape(x, (-1, self.fc_unit, 4))\n            x = tf.reduce_max(x, axis=-1, keepdims=False, name=\'disc-maxout-2\')\n            x = tf.nn.relu(x)\n            x = tf.layers.dropout(x, do_rate, name=\'disc-do-2\')\n\n            x = t.dense(x, 1, name=\'disc-fc-3\')\n            x = tf.sigmoid(x)\n            return x\n\n    def generator(self, z, y, do_rate=0.5, reuse=None):\n        with tf.variable_scope(""generator"", reuse=reuse):\n            x = tf.concat([z, y], axis=1)\n\n            x = t.dense(x, self.fc_unit * 1, name=\'gen-fc-1\')\n            x = tf.nn.relu(x)\n            x = tf.layers.dropout(x, do_rate, name=\'gen-do-1\')\n\n            x = t.dense(x, self.fc_unit * 4, name=\'gen-fc-2\')\n            x = tf.nn.relu(x)\n            x = tf.layers.dropout(x, do_rate, name=\'gen-do-2\')\n\n            x = t.dense(x, self.n_input, name=\'gen-fc-3\')\n            x = tf.sigmoid(x)\n            return x\n\n    def build_cgan(self):\n        # Generator\n        self.g = self.generator(self.z, self.c, self.do_rate)\n\n        # Discriminator\n        d_real = self.discriminator(self.x, self.c, self.do_rate)\n        d_fake = self.discriminator(self.g, self.c, self.do_rate, reuse=True)\n\n        # Losses\n        d_real_loss = -tf.reduce_mean(t.safe_log(d_real))\n        d_fake_loss = -tf.reduce_mean(t.safe_log(1. - d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n        self.g_loss = -tf.reduce_mean(t.safe_log(d_fake))\n        # d_real_loss = t.sce_loss(d_real, tf.ones_like(d_real))\n        # d_fake_loss = t.sce_loss(d_fake, tf.zeros_like(d_fake))\n        # self.d_loss = d_real_loss + d_fake_loss\n        # self.g_loss = t.sce_loss(d_fake, tf.ones_like(d_fake))\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Collect trainer values\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        # Optimizer\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.d_lr,\n                                           beta1=self.beta1).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.g_lr,\n                                           beta1=self.beta1).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
CGAN/cgan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport cgan_model as cgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import MNISTDataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/CGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'batch_size\': 100,\n    \'global_step\': 200001,\n    \'logging_interval\': 1000,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # MNIST Dataset Load\n    mnist = DataSet(ds_path=""D:\\\\DataSet/mnist/"").data\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # CGAN Model\n        model = cgan.CGAN(s, batch_size=train_step[\'batch_size\'])\n\n        # initializing\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        sample_y = np.zeros(shape=[model.sample_num, model.n_classes])\n        for i in range(10):\n            sample_y[10 * i:10 * (i + 1), i] = 1\n\n        for global_step in range(saved_global_step, train_step[\'global_step\']):\n            batch_x, batch_y = mnist.train.next_batch(model.batch_size)\n            batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n            # Update D network\n            _, d_loss = s.run([model.d_op, model.d_loss],\n                              feed_dict={\n                                  model.x: batch_x,\n                                  model.c: batch_y,\n                                  model.z: batch_z,\n                                  model.do_rate: 0.5,\n                              })\n\n            # Update G network\n            _, g_loss = s.run([model.g_op, model.g_loss],\n                              feed_dict={\n                                  model.c: batch_y,\n                                  model.z: batch_z,\n                                  model.do_rate: 0.5,\n                              })\n\n            # Logging\n            if global_step % train_step[\'logging_interval\'] == 0:\n                batch_x, batch_y = mnist.test.next_batch(model.batch_size)\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                d_loss, g_loss, summary = s.run([model.d_loss, model.g_loss, model.merged],\n                                                feed_dict={\n                                                    model.x: batch_x,\n                                                    model.c: batch_y,\n                                                    model.z: batch_z,\n                                                    model.do_rate: 0.5,\n                                                })\n\n                # Print Loss\n                print(""[+] Step %08d => "" % global_step,\n                      "" D loss : {:.8f}"".format(d_loss),\n                      "" G loss : {:.8f}"".format(g_loss))\n\n                # Training G model with sample image and noise\n                sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                samples = s.run(model.g,\n                                feed_dict={\n                                    model.c: sample_y,\n                                    model.z: sample_z,\n                                    model.do_rate: 0.0,\n                                })\n\n                samples = np.reshape(samples, [-1, 28, 28, 1])\n\n                # Summary saver\n                model.writer.add_summary(summary, global_step)\n\n                # Export image generated by model G\n                sample_image_height = model.sample_size\n                sample_image_width = model.sample_size\n                sample_dir = results[\'output\'] + \'train_{:08d}.png\'.format(global_step)\n\n                # Generated image save\n                iu.save_images(samples,\n                               size=[sample_image_height, sample_image_width],\n                               image_path=sample_dir)\n\n                # Model save\n                model.saver.save(s, results[\'model\'], global_step)\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
CoGAN/cogan_model.py,35,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass CoGAN:\n\n    def __init__(self, s, batch_size=64, height=28, width=28, channel=1, n_classes=10,\n                 sample_num=8 * 8, sample_size=8,\n                 n_input=784, fc_d_unit=512, fc_g_unit=1024, df_dim=32, gf_dim=64, z_dim=128, lr=2e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: input image height, default 28\n        :param width: input image width, default 28\n        :param channel: input image channel, default 1 (gray-scale)\n        - in case of MNIST, image size is 28x28x1(HWC).\n        :param n_classes: input DataSet\'s classes\n        - in case of MNIST, 10 (0 ~ 9)\n\n        # Output Settings\n        :param sample_num: the number of output images, default 64\n        :param sample_size: sample image size, default 8\n\n        # For DNN model\n        :param n_input: input image size, default 784(28x28)\n        :param fc_d_unit: discriminator fully connected units, default 512\n        :param fc_g_unit: generator fully connected units, default 1024\n        :param df_dim: the number of disc filters, default 32\n        :param gf_dim: the number of gen filters, default 64\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 128\n        :param lr: learning rate, default 2e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = [self.batch_size, self.height, self.width, self.channel]\n        self.n_classes = n_classes\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.n_input = n_input\n        self.fc_d_unit = fc_d_unit\n        self.fc_g_unit = fc_g_unit\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n\n        self.z_dim = z_dim\n        self.beta1 = .5\n        self.beta2 = .999\n        self.lr = lr\n\n        # pre-defined\n        self.d_loss = 0.\n        self.d_1_loss = 0.\n        self.d_2_loss = 0.\n        self.g_loss = 0.\n        self.g_1_loss = 0.\n        self.g_2_loss = 0.\n\n        self.g_1 = None\n        self.g_2 = None\n        self.g_sample_1 = None\n        self.g_sample_2 = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholder\n        self.x_1 = tf.placeholder(tf.float32, shape=[None, self.n_input], name=""x-image1"")  # (-1, 784)\n        self.x_2 = tf.placeholder(tf.float32, shape=[None, self.n_input], name=""x-image2"")  # (-1, 784)\n        self.y = tf.placeholder(tf.float32, shape=[None, self.n_classes], name=""y-label"")   # (-1, 10)\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')       # (-1, 128)\n\n        self.build_cogan()  # build CoGAN model\n\n    def discriminator(self, x, y=None, share_params=False, reuse=False, name=""""):\n        with tf.variable_scope(""discriminator-%s"" % name, reuse=reuse):\n            x = tf.reshape(x, (-1, self.height, self.width, self.channel))\n\n            x = t.conv2d(x, f=self.df_dim * 1, k=5, s=2, reuse=False, name=\'disc-\' + name + \'-conv2d-1\')\n            x = t.prelu(x, reuse=False, name=\'disc-\' + name + \'-prelu-1\')\n            # x = tf.layers.max_pooling2d(x, pool_size=2, strides=2, padding=\'SAME\',\n            #                             name=\'disc-\' + name + \'-max_pool2d-1\')\n\n            x = t.conv2d(x, f=self.df_dim * 2, k=5, s=2, reuse=False, name=\'disc-\' + name + \'-conv2d-2\')\n            x = t.batch_norm(x, reuse=False, name=\'disc-\' + name + \'-bn-1\')\n            x = t.prelu(x, reuse=False, name=\'disc-\' + name + \'-prelu-2\')\n            # x = tf.layers.max_pooling2d(x, pool_size=2, strides=2, padding=\'SAME\',\n            #                             name=\'disc-\' + name + \'-max_pool2d-2\')\n\n            x = tf.layers.flatten(x)\n\n        x = t.dense(x, self.fc_d_unit, reuse=share_params, name=\'disc-fc-1\')\n        x = t.batch_norm(x, reuse=share_params, name=\'disc-bn-2\')\n        x = t.prelu(x, reuse=share_params, name=\'disc-prelu-3\')\n\n        x = t.dense(x, 1, reuse=share_params, name=\'disc-fc-2\')\n        return x\n\n    def generator(self, z, y=None, share_params=False, reuse=False, name=""""):\n        x = t.dense(z, self.fc_g_unit, reuse=share_params, name=\'gen-fc-1\')\n        x = t.batch_norm(x, reuse=share_params, name=\'gen-bn-1\')\n        x = t.prelu(x, reuse=share_params, name=\'gen-prelu-1\')\n\n        x = t.dense(x, self.gf_dim * 8 * 7 * 7, reuse=share_params, name=\'gen-fc-2\')\n        x = t.batch_norm(x, reuse=share_params, name=\'gen-bn-2\')\n        x = t.prelu(x, reuse=share_params, name=\'gen-prelu-2\')\n\n        x = tf.reshape(x, (-1, 7, 7, self.gf_dim * 8))\n\n        for i in range(1, 3):\n            x = t.deconv2d(x, f=self.gf_dim * 4 // i, k=3, s=2, reuse=share_params, name=\'gen-deconv2d-%d\' % i)\n            x = t.batch_norm(x, reuse=share_params, name=""gen-bn-%d"" % (i + 2))\n            x = t.prelu(x, reuse=share_params, name=\'gen-prelu-%d\' % (i + 2))\n\n        """"""\n        x = z  # tf.concat([z, y], axis=1)\n\n        loop = 5\n        for i in range(1, loop):\n            x = t.dense(x, self.fc_g_unit, reuse=share_params, name=\'gen-fc-%d\' % i)\n            x = t.batch_norm(x, reuse=share_params, name=\'gen-bn-%d\' % i)\n            x = t.prelu(x, reuse=share_params, name=\'gen-prelu-%d\' % i)\n        """"""\n\n        with tf.variable_scope(""generator-%s"" % name, reuse=reuse):\n            x = t.deconv2d(x, f=self.channel, k=6, s=1, reuse=False, name=\'gen-\' + name + \'-deconv2d-3\')\n            x = tf.nn.sigmoid(x, name=\'gen\' + name + \'-sigmoid-1\')\n            """"""\n            x = t.dense(x, self.n_input, reuse=False, name=\'gen-\' + name + \'-fc-%d\' % loop)\n            x = tf.nn.sigmoid(x)\n            """"""\n\n        return x\n\n    def build_cogan(self):\n        # Generator\n        self.g_1 = self.generator(self.z, self.y, share_params=False, reuse=False, name=\'g1\')\n        self.g_2 = self.generator(self.z, self.y, share_params=True, reuse=False, name=\'g2\')\n\n        self.g_sample_1 = self.generator(self.z, self.y, share_params=True, reuse=True, name=\'g1\')\n        self.g_sample_2 = self.generator(self.z, self.y, share_params=True, reuse=True, name=\'g2\')\n\n        # Discriminator\n        d_1_real = self.discriminator(self.x_1, self.y, share_params=False, reuse=False, name=\'d1\')\n        d_2_real = self.discriminator(self.x_2, self.y, share_params=True, reuse=False, name=\'d2\')\n        d_1_fake = self.discriminator(self.g_1, self.y, share_params=True, reuse=True, name=\'d1\')\n        d_2_fake = self.discriminator(self.g_2, self.y, share_params=True, reuse=True, name=\'d2\')\n\n        # Losses\n        d_1_real_loss = t.sce_loss(d_1_real, tf.ones_like(d_1_real))\n        d_1_fake_loss = t.sce_loss(d_1_fake, tf.zeros_like(d_1_fake))\n        d_2_real_loss = t.sce_loss(d_2_real, tf.ones_like(d_2_real))\n        d_2_fake_loss = t.sce_loss(d_2_fake, tf.zeros_like(d_2_fake))\n        self.d_1_loss = d_1_real_loss + d_1_fake_loss\n        self.d_2_loss = d_2_real_loss + d_2_fake_loss\n        self.d_loss = self.d_1_loss + self.d_2_loss\n\n        g_1_loss = t.sce_loss(d_1_fake, tf.ones_like(d_1_fake))\n        g_2_loss = t.sce_loss(d_2_fake, tf.ones_like(d_2_fake))\n        self.g_loss = g_1_loss + g_2_loss\n\n        # Summary\n        tf.summary.scalar(""loss/d_1_real_loss"", d_1_real_loss)\n        tf.summary.scalar(""loss/d_1_fake_loss"", d_1_fake_loss)\n        tf.summary.scalar(""loss/d_2_real_loss"", d_2_real_loss)\n        tf.summary.scalar(""loss/d_2_fake_loss"", d_2_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_1_loss"", g_1_loss)\n        tf.summary.scalar(""loss/g_2_loss"", g_2_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
CoGAN/cogan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport cogan_model as cogan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import MNISTDataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/CoGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'batch_size\': 128,\n    \'global_step\': 12501,\n    \'logging_interval\': 250,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # MNIST Dataset load\n    mnist = DataSet(ds_path=""D:\\\\DataSet/mnist/"").data\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # CoGAN Model\n        model = cogan.CoGAN(s, batch_size=train_step[\'batch_size\'])\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        sample_x, _ = mnist.test.next_batch(model.sample_num)\n        sample_x = np.reshape(sample_x, newshape=[model.sample_num] + model.image_shape[1:])\n\n        sample_rot_x = np.rot90(sample_x[:], 1, axes=(-3, -2))\n\n        # Generated image save\n        iu.save_images(sample_x,\n                       size=[model.sample_size, model.sample_size],\n                       image_path=\'./gen_img/sample_1.png\')\n        iu.save_images(sample_rot_x,\n                       size=[model.sample_size, model.sample_size],\n                       image_path=\'./gen_img/sample_2.png\')\n\n        sample_y = np.zeros(shape=[model.sample_num, model.n_classes])\n        for i in range(10):\n            sample_y[10 * i:10 * (i + 1), i] = 1\n\n        for global_step in range(saved_global_step, train_step[\'global_step\']):\n            batch_x, batch_y = mnist.train.next_batch(model.batch_size)\n            batch_rot_x = np.reshape(np.rot90(np.reshape(batch_x, model.image_shape), 1, axes=(-3, -2)),\n                                     (-1, model.n_input))\n            batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n            # Update D network\n            _, d_loss = s.run([model.d_op, model.d_loss],\n                              feed_dict={\n                                  model.x_1: batch_x,\n                                  model.x_2: batch_rot_x,  # batch_x\n                                  # model.y: batch_y,\n                                  model.z: batch_z,\n                              })\n\n            # Update G network\n            _, g_loss = s.run([model.g_op, model.g_loss],\n                              feed_dict={\n                                  model.x_1: batch_x,\n                                  model.x_2: batch_rot_x,\n                                  # model.y: batch_y,\n                                  model.z: batch_z,\n                              })\n\n            if global_step % train_step[\'logging_interval\'] == 0:\n                summary = s.run(model.merged,\n                                feed_dict={\n                                    model.x_1: batch_x,\n                                    model.x_2: batch_rot_x,\n                                    # model.y: batch_y,\n                                    model.z: batch_z,\n                                })\n\n                # Print loss\n                print(""[+] Step %08d => "" % global_step,\n                      "" D loss : {:.8f}"".format(d_loss),\n                      "" G loss : {:.8f}"".format(g_loss))\n\n                # Training G model with sample image and noise\n                sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                samples_1 = s.run(model.g_sample_1,\n                                  feed_dict={\n                                      # model.y: sample_y,\n                                      model.z: sample_z,\n                                  })\n                samples_2 = s.run(model.g_sample_2,\n                                  feed_dict={\n                                      # model.y: sample_y,\n                                      model.z: sample_z,\n                                  })\n\n                # Summary saver\n                model.writer.add_summary(summary, global_step)\n\n                # Export image generated by model G\n                sample_image_height = model.sample_size\n                sample_image_width = model.sample_size\n\n                sample_dir_1 = results[\'output\'] + \'train_1_{:08d}.png\'.format(global_step)\n                sample_dir_2 = results[\'output\'] + \'train_2_{:08d}.png\'.format(global_step)\n\n                # Generated image save\n                iu.save_images(samples_1,\n                               size=[sample_image_height, sample_image_width],\n                               image_path=sample_dir_1)\n                iu.save_images(samples_2,\n                               size=[sample_image_height, sample_image_width],\n                               image_path=sample_dir_2)\n\n                # Model save\n                model.saver.save(s, results[\'model\'], global_step)\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()'"
CycleGAN/cyclegan_model.py,47,"b'import tensorflow as tf\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass CycleGAN:\n\n    def __init__(self, s, batch_size=8, height=128, width=128, channel=3,\n                 sample_num=1 * 1, sample_size=1,\n                 df_dim=64, gf_dim=32, fd_unit=512, g_lr=2e-4, d_lr=2e-4, epsilon=1e-9):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 8\n        :param height: input image height, default 128\n        :param width: input image width, default 128\n        :param channel: input image channel, default 3 (RGB)\n        - in case of Celeb-A, image size is 128x128x3(HWC).\n\n        # Output Settings\n        :param sample_num: the number of output images, default 4\n        :param sample_size: sample image size, default 2\n\n        # For CNN model\n        :param df_dim: discriminator filter, default 64\n        :param gf_dim: generator filter, default 32\n        :param fd_unit: fully connected units, default 512\n\n        # Training Option\n        :param g_lr: generator learning rate, default 2e-4\n        :param d_lr: discriminator learning rate, default 2e-4\n        :param epsilon: epsilon, default 1e-9\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = [self.batch_size, self.height, self.width, self.channel]\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n        self.fd_unit = fd_unit\n\n        self.beta1 = .5\n        self.beta2 = .999\n        self.d_lr = d_lr\n        self.g_lr = g_lr\n        self.lambda_ = 10.\n        self.lambda_cycle = 10.\n        self.n_train_critic = 10\n        self.eps = epsilon\n\n        # pre-defined\n        self.g_a2b = None\n        self.g_b2a = None\n        self.g_a2b2a = None\n        self.g_b2a2b = None\n\n        self.w_a = None\n        self.w_b = None\n        self.w = None\n\n        self.gp_a = None\n        self.gp_b = None\n        self.gp = None\n\n        self.g_loss = 0.\n        self.g_a_loss = 0.\n        self.g_b_loss = 0.\n        self.d_loss = 0.\n        self.cycle_loss = 0.\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # placeholders\n        self.a = tf.placeholder(tf.float32,\n                                [None, self.height, self.width, self.channel], name=\'image-a\')\n        self.b = tf.placeholder(tf.float32,\n                                [None, self.height, self.width, self.channel], name=\'image-b\')\n        self.lr_decay = tf.placeholder(tf.float32, None, name=\'learning_rate-decay\')\n\n        self.build_cyclegan()  # build CycleGAN\n\n    def discriminator(self, x, reuse=None, name=""""):\n        """"""\n        :param x: 128x128x3 images\n        :param reuse: re-usability\n        :param name: name\n\n        :return: logits, prob\n        """"""\n        with tf.variable_scope(\'discriminator-%s\' % name, reuse=reuse):\n            def residual_block(x, f, name=\'\'):\n                x = t.conv2d(x, f=f, k=4, s=2, name=\'disc-conv2d-%s\' % name)\n                x = t.instance_norm(x, name=\'disc-ins_norm-%s\' % name)\n                x = tf.nn.leaky_relu(x, alpha=0.2)\n                return x\n\n            x = t.conv2d(x, f=self.df_dim, name=\'disc-conv2d-0\')\n            x = tf.nn.leaky_relu(x, alpha=0.2)\n\n            x = residual_block(x, f=self.df_dim * 2, name=\'1\')\n            x = residual_block(x, f=self.df_dim * 4, name=\'2\')\n            x = residual_block(x, f=self.df_dim * 8, name=\'3\')\n            # for 256x256x3 images\n            # x = residual_block(x, f=self.df_dim * 8, name=\'4\')\n            # x = residual_block(x, f=self.df_dim * 8, name=\'5\')\n\n            logits = t.conv2d(x, f=1, name=\'disc-con2d-last\')\n            # prob = tf.nn.sigmoid(logits)\n\n            return logits\n\n    def generator(self, x, reuse=None, name=""""):\n        """""" The form of Auto-Encoder\n        :param x: 128x128x3 images\n        :param reuse: re-usability\n        :param name: name\n\n        :return: logits, prob\n        """"""\n        with tf.variable_scope(\'generator-%s\' % name, reuse=reuse):\n            def d(x, f, name=\'\'):\n                x = t.conv2d(x, f=f, k=3, s=2, name=\'gen-d-conv2d-%s\' % name)\n                x = t.instance_norm(x, name=\'gen-d-ins_norm-%s\' % name)\n                x = tf.nn.relu(x)\n                return x\n\n            def R(x, f, name=\'\'):\n                x = t.conv2d(x, f=f, k=3, s=1, name=\'gen-R-conv2d-%s-0\' % name)\n                x = t.conv2d(x, f=f, k=3, s=1, name=\'gen-R-conv2d-%s-1\' % name)\n                x = t.instance_norm(x, name=\'gen-R-ins_norm-%s\' % name)\n                x = tf.nn.relu(x)\n                return x\n\n            def u(x, f, name=\'\'):\n                x = t.deconv2d(x, f=f, k=3, s=2, name=\'gen-u-deconv2d-%s\' % name)\n                x = t.instance_norm(x, name=\'gen-u-ins_norm-%s\' % name)\n                x = tf.nn.relu(x)\n                return x\n\n            x = t.conv2d(x, f=self.gf_dim, k=7, s=1, name=\'gen-conv2d-0\')\n\n            x = d(x, self.gf_dim * 2, name=\'1\')\n            x = d(x, self.gf_dim * 4, name=\'2\')\n\n            for i in range(1, 7):\n                x = R(x, self.gf_dim * 4, name=str(i))\n\n            x = u(x, self.gf_dim * 4, name=\'1\')\n            x = u(x, self.gf_dim * 2, name=\'2\')\n\n            logits = t.conv2d(x, f=3, k=7, s=1, name=\'gen-conv2d-1\')\n            prob = tf.nn.tanh(logits)\n\n            return prob\n\n    def build_cyclegan(self):\n        # Generator\n        with tf.variable_scope(""generator-a2b""):\n            self.g_a2b = self.generator(self.a, name=""a2b"")  # a to b\n        with tf.variable_scope(""generator-b2a""):\n            self.g_b2a = self.generator(self.b, name=""b2a"")  # b to a\n\n        with tf.variable_scope(""generator-b2a"", reuse=True):\n            self.g_a2b2a = self.generator(self.g_a2b, reuse=True, name=""b2a"")  # a to b to a\n        with tf.variable_scope(""generator-a2b"", reuse=True):\n            self.g_b2a2b = self.generator(self.g_b2a, reuse=True, name=""a2b"")  # b to a to b\n\n        # Classifier\n        with tf.variable_scope(""discriminator-a""):\n            alpha = tf.random_uniform(shape=[self.batch_size, 1, 1, 1], minval=0., maxval=1.)\n            a_hat = alpha * self.a + (1. - alpha) * self.g_b2a\n\n            d_a = self.discriminator(self.a)\n            d_b2a = self.discriminator(self.g_b2a, reuse=True)\n            d_a_hat = self.discriminator(a_hat, reuse=True)\n        with tf.variable_scope(""discriminator-b""):\n            alpha = tf.random_uniform(shape=[self.batch_size, 1, 1, 1], minval=0., maxval=1.)\n            b_hat = alpha * self.b + (1. - alpha) * self.g_a2b\n\n            d_b = self.discriminator(self.b)\n            d_a2b = self.discriminator(self.g_a2b, reuse=True)\n            d_b_hat = self.discriminator(b_hat, reuse=True)\n\n        # Training Ops\n        self.w_a = tf.reduce_mean(d_a) - tf.reduce_mean(d_b2a)\n        self.w_b = tf.reduce_mean(d_b) - tf.reduce_mean(d_a2b)\n        self.w = self.w_a + self.w_b\n\n        self.gp_a = tf.reduce_mean(\n            (tf.sqrt(tf.reduce_sum(tf.gradients(d_a_hat, a_hat)[0] ** 2, reduction_indices=[1, 2, 3])) - 1.) ** 2\n        )\n        self.gp_b = tf.reduce_mean(\n            (tf.sqrt(tf.reduce_sum(tf.gradients(d_b_hat, b_hat)[0] ** 2, reduction_indices=[1, 2, 3])) - 1.) ** 2\n        )\n        self.gp = self.gp_a + self.gp_b\n\n        self.d_loss = self.lambda_ * self.gp - self.w\n\n        cycle_a_loss = tf.reduce_mean(tf.reduce_mean(tf.abs(self.a - self.g_a2b2a), reduction_indices=[1, 2, 3]))\n        cycle_b_loss = tf.reduce_mean(tf.reduce_mean(tf.abs(self.b - self.g_b2a2b), reduction_indices=[1, 2, 3]))\n        self.cycle_loss = cycle_a_loss + cycle_b_loss\n\n        # using adv loss\n        self.g_a_loss = -1. * tf.reduce_mean(d_b2a)\n        self.g_b_loss = -1. * tf.reduce_mean(d_a2b)\n        self.g_loss = self.g_a_loss + self.g_b_loss + self.lambda_cycle * self.cycle_loss\n\n        # Summary\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/cycle_loss"", self.cycle_loss)\n        tf.summary.scalar(""loss/cycle_a_loss"", cycle_a_loss)\n        tf.summary.scalar(""loss/cycle_b_loss"", cycle_b_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n        tf.summary.scalar(""loss/g_a_loss"", self.g_a_loss)\n        tf.summary.scalar(""loss/g_b_loss"", self.g_b_loss)\n        tf.summary.scalar(""misc/gradient_penalty"", self.gp)\n        tf.summary.scalar(""misc/g_lr"", self.g_lr)\n        tf.summary.scalar(""misc/d_lr"", self.d_lr)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.d_lr * self.lr_decay,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.g_lr * self.lr_decay,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
CycleGAN/cyclegan_train.py,4,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport cyclegan_model as cyclegan\n\nsys.path.append(\'../\')\n\nimport image_utils as iu\nfrom datasets import Pix2PixDataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/CycleGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epochs\': 201,\n    \'batch_size\': 8,\n    \'logging_step\': 100,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # CycleGAN Model\n        model = cyclegan.CycleGAN(s,\n                                  height=128,\n                                  width=128,\n                                  channel=3,\n                                  batch_size=train_step[\'batch_size\'])\n\n        # Celeb-A DataSet images\n        ds = DataSet(height=128,\n                     width=128,\n                     channel=3,\n                     ds_path=""D:/DataSet/pix2pix/"",\n                     ds_name=\'vangogh2photo\')\n\n        img_a = ds.images_a\n        img_b = ds.images_b\n\n        print(""[*] image A shape : "", img_a.shape)\n        print(""[*] image B shape : "", img_b.shape)\n\n        n_sample = model.sample_num\n\n        sample_image_height = model.sample_size\n        sample_image_width = model.sample_size\n        sample_dir_a = results[\'output\'] + \'valid_a.png\'\n        sample_dir_b = results[\'output\'] + \'valid_b.png\'\n\n        sample_a, sample_b = img_a[:n_sample], img_b[:n_sample]\n        sample_a = np.reshape(sample_a, [-1] + model.image_shape[1:])\n        sample_b = np.reshape(sample_b, [-1] + model.image_shape[1:])\n\n        # Generated image save\n        iu.save_images(sample_a, [sample_image_height, sample_image_width], sample_dir_a)\n        iu.save_images(sample_b, [sample_image_height, sample_image_width], sample_dir_b)\n\n        print(""[+] pre-processing elapsed time : {:.8f}s"".format(time.time() - start_time))\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        global_step = 0\n        for epoch in range(train_step[\'epochs\']):\n            # learning rate decay\n            lr_decay = 1.\n            if epoch >= 100 and epoch % 10 == 0:\n                lr_decay = (train_step[\'epochs\'] - epoch) / (train_step[\'epochs\'] / 2.)\n\n            # re-implement DataIterator for multi-input\n            pointer = 0\n            num_images = min(ds.n_images_a, ds.n_images_b)\n            for i in range(num_images // train_step[\'batch_size\']):\n                start = pointer\n                pointer += train_step[\'batch_size\']\n\n                if pointer > num_images:  # if ended 1 epoch\n                    # Shuffle training DataSet\n                    perm_a, perm_b = np.arange(ds.n_images_a), np.arange(ds.n_images_b)\n\n                    np.random.shuffle(perm_a)\n                    np.random.shuffle(perm_b)\n\n                    img_a, img_b = img_a[perm_a], img_a[perm_b]\n\n                    start = 0\n                    pointer = train_step[\'batch_size\']\n\n                end = pointer\n\n                batch_a = np.reshape(img_a[start:end], model.image_shape)\n                batch_b = np.reshape(img_a[start:end], model.image_shape)\n\n                for _ in range(model.n_train_critic):\n                    s.run(model.d_op,\n                          feed_dict={\n                              model.a: batch_a,\n                              model.b: batch_b,\n                              model.lr_decay: lr_decay,\n                          })\n\n                w, gp, g_loss, cycle_loss, _ = s.run([model.w, model.gp, model.g_loss, model.cycle_loss, model.g_op],\n                                                     feed_dict={\n                                                         model.a: batch_a,\n                                                         model.b: batch_b,\n                                                         model.lr_decay: lr_decay,\n                                                     })\n\n                if global_step % train_step[\'logging_step\'] == 0:\n                    # Summary\n                    summary = s.run(model.merged,\n                                    feed_dict={\n                                        model.a: batch_a,\n                                        model.b: batch_b,\n                                        model.lr_decay: lr_decay,\n                                    })\n\n                    # Print loss\n                    print(""[+] Global Step %08d =>"" % global_step,\n                          "" G loss     : {:.8f}"".format(g_loss),\n                          "" Cycle loss : {:.8f}"".format(cycle_loss),\n                          "" w          : {:.8f}"".format(w),\n                          "" gp         : {:.8f}"".format(gp))\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step=global_step)\n\n                    # Training G model with sample image and noise\n                    samples_a2b = s.run(model.g_a2b,\n                                        feed_dict={\n                                            model.a: sample_a,\n                                            model.b: sample_b,\n                                            model.lr_decay: lr_decay,\n                                        })\n                    samples_b2a = s.run(model.g_b2a,\n                                        feed_dict={\n                                            model.a: sample_a,\n                                            model.b: sample_b,\n                                            model.lr_decay: lr_decay,\n                                        })\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir_a2b = results[\'output\'] + \'train_a2b_{0}.png\'.format(global_step)\n                    sample_dir_b2a = results[\'output\'] + \'train_b2a_{0}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples_a2b, [sample_image_height, sample_image_width], sample_dir_a2b)\n                    iu.save_images(samples_b2a, [sample_image_height, sample_image_width], sample_dir_b2a)\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step=global_step)\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
DCGAN/dcgan_model.py,33,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)\n\n\nclass DCGAN:\n\n    def __init__(self, s, batch_size=64, height=64, width=64, channel=3,\n                 sample_num=8 * 8, sample_size=8,\n                 z_dim=128, gf_dim=64, df_dim=64, lr=2e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: input image height, default 64\n        :param width: input image width, default 64\n        :param channel: input image channel, default 3 (RGB)\n        - in case of CelebA, image size is 64x64x3(HWC).\n\n        # Output Settings\n        :param sample_num: the number of sample images, default 64\n        :param sample_size: sample image size, default 8\n\n        # Model Settings\n        :param z_dim: z noise dimension, default 128\n        :param gf_dim: the number of generator filters, default 64\n        :param df_dim: the number of discriminator filters, default 64\n\n        # Training Settings\n        :param lr: learning rate, default 2e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n        self.channel = channel\n\n        self.sample_size = sample_size\n        self.sample_num = sample_num\n\n        self.image_shape = [self.height, self.width, self.channel]\n\n        self.z_dim = z_dim\n\n        self.gf_dim = gf_dim\n        self.df_dim = df_dim\n\n        # pre-defined\n        self.d_loss = 0.\n        self.g_loss = 0.\n\n        self.g = None\n        self.g_test = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel], name=\'x-images\')\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')\n\n        # Training Options\n        self.beta1 = 0.5  # 0.9 is not good at oscillation & instability\n        self.lr = lr      # 1e-3 is too high...\n\n        self.bulid_dcgan()  # build DCGAN model\n\n    def discriminator(self, x, reuse=None):\n        with tf.variable_scope(\'discriminator\', reuse=reuse):\n            x = t.conv2d(x, self.df_dim * 1, 5, 2, name=\'disc-conv2d-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 2, 5, 2, name=\'disc-conv2d-2\')\n            x = t.batch_norm(x, name=\'disc-bn-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 4, 5, 2, name=\'disc-conv2d-3\')\n            x = t.batch_norm(x, name=\'disc-bn-2\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 8, 5, 2, name=\'disc-conv2d-4\')\n            x = t.batch_norm(x, name=\'disc-bn-3\')\n            x = tf.nn.leaky_relu(x)\n\n            x = tf.layers.flatten(x)\n\n            logits = t.dense(x, 1, name=\'disc-fc-1\')\n            prob = tf.nn.sigmoid(logits)\n\n            return prob, logits\n\n    def generator(self, z, reuse=None, is_train=True):\n        with tf.variable_scope(\'generator\', reuse=reuse):\n            x = t.dense(z, self.gf_dim * 8 * 4 * 4, name=\'gen-fc-1\')\n\n            x = tf.reshape(x, [-1, 4, 4, self.gf_dim * 8])\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-1\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.gf_dim * 4, 5, 2, name=\'gen-deconv2d-1\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-2\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x,  self.gf_dim * 2, 5, 2, name=\'gen-deconv2d-2\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-3\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x,  self.gf_dim * 1, 5, 2, name=\'gen-deconv2d-3\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-4\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.channel, 5, 2, name=\'gen-deconv2d-4\')\n            x = tf.nn.tanh(x)\n\n            return x\n\n    def bulid_dcgan(self):\n        # Generator\n        self.g = self.generator(self.z)\n        self.g_test = self.generator(self.z, reuse=True, is_train=False)\n\n        # Discriminator\n        _, d_real = self.discriminator(self.x)\n        _, d_fake = self.discriminator(self.g, reuse=True)\n\n        # Losses\n        """"""\n        d_real_loss = -tf.reduce_mean(log(d_real))\n        d_fake_loss = -tf.reduce_mean(log(1. - d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n        self.g_loss = -tf.reduce_mean(log(d_fake))\n        """"""\n        d_real_loss = t.sce_loss(d_real, tf.ones_like(d_real))\n        d_fake_loss = t.sce_loss(d_fake, tf.zeros_like(d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n        self.g_loss = t.sce_loss(d_fake, tf.ones_like(d_fake))\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Collect trainer values\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        # Optimizer\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model Saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
DCGAN/dcgan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport dcgan_model as dcgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CelebADataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/DCGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epoch\': 25,\n    \'batch_size\': 128,\n    \'logging_interval\': 400,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # DCGAN model\n        model = dcgan.DCGAN(s, batch_size=train_step[\'batch_size\'])\n\n        # Initializing variables\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        # Training, Test data set\n        # loading CelebA DataSet\n        ds = DataSet(height=64,\n                     width=64,\n                     channel=3,\n                     ds_image_path=""D:\\\\DataSet/CelebA/CelebA-64.h5"",\n                     ds_label_path=""D:\\\\DataSet/CelebA/Anno/list_attr_celeba.txt"",\n                     # ds_image_path=""D:\\\\DataSet/CelebA/Img/img_align_celeba/"",\n                     ds_type=""CelebA"",\n                     use_save=False,\n                     save_file_name=""D:\\\\DataSet/CelebA/CelebA-64.h5"",\n                     save_type=""to_h5"",\n                     use_img_scale=False,\n                     # img_scale=""-1,1""\n                     )\n\n        # saving sample images\n        test_images = np.reshape(iu.transform(ds.images[:16], inv_type=\'127\'), (16, 64, 64, 3))\n        iu.save_images(test_images,\n                       size=[4, 4],\n                       image_path=results[\'output\'] + \'sample.png\',\n                       inv_type=\'127\')\n\n        ds_iter = DataIterator(x=ds.images,\n                               y=None,\n                               batch_size=train_step[\'batch_size\'],\n                               label_off=True)\n\n        global_step = saved_global_step\n        start_epoch = global_step // (len(ds.train_images) // model.batch_size)           # recover n_epoch\n        ds_iter.pointer = saved_global_step % (len(ds.train_images) // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epoch\']):\n            for batch_x in ds_iter.iterate():\n                batch_x = np.reshape(iu.transform(batch_x, inv_type=\'127\'),\n                                     (model.batch_size, model.height, model.width, model.channel))\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update D network\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z\n                                  })\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                if global_step % train_step[\'logging_interval\'] == 0:\n                    d_loss, g_loss, summary = s.run([model.d_loss, model.g_loss, model.merged],\n                                                    feed_dict={\n                                                        model.x: batch_x,\n                                                        model.z: batch_z,\n                                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %03d Step %05d => "" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss))\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim])\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.z: sample_z,\n                                    })\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{0}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step)\n\n                global_step += 1\n\n        end_time = time.time() - start_time  # Clocking end\n\n        # Elapsed time\n        print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n        # Close tf.Session\n        s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
DRAGAN/dragan_model.py,28,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)\n\n\nclass DRAGAN:\n\n    def __init__(self, s, batch_size=16, height=28, width=28, channel=1, n_classes=10,\n                 sample_num=10 * 10, sample_size=10,\n                 z_dim=128, fc_unit=512):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: input image height, default 28\n        :param width: input image width, default 28\n        :param channel: input image channel, default 1 (Gray-Scale)\n        - in case of MNIST, image size is 28x28x1(HWC).\n        :param n_classes: the number of classes, default 10\n\n        # Output Settings\n        :param sample_num: the number of sample images, default 100\n        :param sample_size: sample image size, default 10\n\n        # Model Settings\n        :param z_dim: z noise dimension, default 128\n        :param fc_unit: the number of fc units, default 512\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.n_classes = n_classes\n\n        self.sample_size = sample_size\n        self.sample_num = sample_num\n\n        self.image_shape = [self.height, self.width, self.channel]\n        self.n_input = self.height * self.width * self.channel\n\n        self.z_dim = z_dim\n\n        self.fc_unit = fc_unit\n\n        # pre-defined\n        self.d_loss = 0.\n        self.g_loss = 0.\n        self.gp = 0.\n\n        self.g = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel],\n                                name=\'x-images\')\n        self.x_p = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel],\n                                  name=\'x-perturbed-images\')\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')\n\n        # Training Options\n        self.lambda_ = 10.  # Higher lambda value, More stable. But slower...\n        self.beta1 = .5\n        self.beta2 = .9\n        self.lr = 2e-4\n\n        self.bulid_dragan()  # build DRAGAN model\n\n    def discriminator(self, x, reuse=None):\n        with tf.variable_scope(\'discriminator\', reuse=reuse):\n            x = tf.layers.flatten(x)\n\n            for i in range(1, 5):\n                x = t.dense(x, self.fc_unit, name=\'disc-fc-%d\' % i)\n                x = tf.nn.leaky_relu(x)\n\n            x = t.dense(x, 1, name=\'disc-fc-5\')\n            return x\n\n    def generator(self, z, reuse=None):\n        with tf.variable_scope(\'generator\', reuse=reuse):\n            x = z\n            for i in range(1, 5):\n                x = t.dense(x, self.fc_unit, name=\'gen-fc-%d\' % i)\n                x = tf.nn.relu(x)\n\n            x = t.dense(x, self.n_input, name=\'gen-fc-5\')\n            x = tf.nn.sigmoid(x)\n            return x\n\n    def bulid_dragan(self):\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        d_real = self.discriminator(self.x)\n        d_fake = self.discriminator(self.g, reuse=True)\n\n        # sce losses\n        d_real_loss = t.sce_loss(d_real, tf.ones_like(d_real))\n        d_fake_loss = t.sce_loss(d_fake, tf.zeros_like(d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n        self.g_loss = t.sce_loss(d_fake, tf.ones_like(d_fake))\n\n        # DRAGAN loss with GP (gradient penalty)\n        alpha = tf.random_uniform(shape=[self.batch_size, 1, 1, 1], minval=0., maxval=1., name=\'alpha\')\n        diff = self.x_p - self.x\n        interpolates = self.x + alpha * diff\n        d_inter = self.discriminator(interpolates, reuse=True)\n        grads = tf.gradients(d_inter, [interpolates])[0]\n        slopes = tf.sqrt(tf.reduce_sum(tf.square(grads), reduction_indices=[1]))\n        self.gp = tf.reduce_mean(tf.square(slopes - 1.))\n\n        # update d_loss with gp\n        self.d_loss += self.lambda_ * self.gp\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n        tf.summary.scalar(""misc/gp"", self.gp)\n\n        # Collect trainer values\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        # Optimizer\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model Saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
DRAGAN/dragan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport dragan_model as dragan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import MNISTDataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/DRAGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'batch_size\': 64,\n    \'global_step\': 200001,\n    \'logging_interval\': 2000,\n}\n\n\nnp.random.seed(777)\n\n\ndef get_perturbed_images(images):\n    return images + .5 * images.std() * np.random.random(images.shape)\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # DRAGAN model\n        model = dragan.DRAGAN(s, batch_size=train_step[\'batch_size\'])\n\n        # Initializing variables\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %s"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        # MNIST DataSet images\n        mnist = DataSet(ds_path=""D:\\\\DataSet/mnist/"").data\n\n        for global_step in range(saved_global_step, train_step[\'global_step\']):\n            batch_x, _ = mnist.train.next_batch(model.batch_size)\n            batch_x_p = get_perturbed_images(batch_x)\n            batch_x = np.reshape(batch_x, [-1] + model.image_shape)\n            batch_x_p = np.reshape(batch_x_p, [-1] + model.image_shape)\n\n            batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n            # Update D network\n            _, d_loss = s.run([model.d_op, model.d_loss],\n                              feed_dict={\n                                  model.x: batch_x,\n                                  model.x_p: batch_x_p,\n                                  model.z: batch_z,\n                              })\n\n            # Update G network\n            _, g_loss = s.run([model.g_op, model.g_loss],\n                              feed_dict={\n                                  model.z: batch_z,\n                              })\n\n            if global_step % train_step[\'logging_interval\'] == 0:\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                d_loss, g_loss, summary = s.run([model.d_loss, model.g_loss, model.merged],\n                                                feed_dict={\n                                                    model.x: batch_x,\n                                                    model.x_p: batch_x_p,\n                                                    model.z: batch_z,\n                                                })\n\n                # Print loss\n                print(""[+] Global Step %05d => "" % global_step,\n                      "" D loss : {:.8f}"".format(d_loss),\n                      "" G loss : {:.8f}"".format(g_loss))\n\n                # Training G model with sample image and noise\n                sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                samples = s.run(model.g,\n                                feed_dict={\n                                    model.z: sample_z,\n                                })\n\n                samples = np.reshape(samples, [-1] + model.image_shape)\n\n                # Summary saver\n                model.writer.add_summary(summary, global_step)\n\n                # Export image generated by model G\n                sample_image_height = model.sample_size\n                sample_image_width = model.sample_size\n                sample_dir = results[\'output\'] + \'train_{0}.png\'.format(global_step)\n\n                # Generated image save\n                iu.save_images(samples,\n                               size=[sample_image_height, sample_image_width],\n                               image_path=sample_dir)\n\n                # Model save\n                model.saver.save(s, results[\'model\'], global_step)\n\n            global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
DeblurGAN/deblurgan_model.py,38,"b'import tensorflow as tf\n\nimport sys\n\nimport vgg19\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)\n\n\nclass DeblurGAN:\n\n    def __init__(self, s, batch_size=64, height=64, width=64, channel=3,\n                 sample_num=8 * 8, sample_size=8,\n                 z_dim=128, gf_dim=64, df_dim=64, lr=2e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: input image height, default 64\n        :param width: input image width, default 64\n        :param channel: input image channel, default 3 (RGB)\n        - in case of CelebA, image size is 64x64x3(HWC).\n\n        # Output Settings\n        :param sample_num: the number of sample images, default 64\n        :param sample_size: sample image size, default 8\n\n        # Model Settings\n        :param z_dim: z noise dimension, default 128\n        :param gf_dim: the number of generator filters, default 64\n        :param df_dim: the number of discriminator filters, default 64\n\n        # Training Settings\n        :param lr: learning rate, default 2e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n        self.channel = channel\n\n        self.sample_size = sample_size\n        self.sample_num = sample_num\n\n        self.image_shape = [self.height, self.width, self.channel]\n\n        self.z_dim = z_dim\n\n        self.gf_dim = gf_dim\n        self.df_dim = df_dim\n\n        self.vgg19 = None\n        self.vgg_image_shape = [224, 224, 3]\n        self.vgg_mean = [103.939, 116.779, 123.68]\n\n        # pre-defined\n        self.d_loss = 0.\n        self.g_loss = 0.\n\n        self.g = None\n        self.g_test = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel], name=\'x-images\')\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')\n\n        # Training Options\n        self.beta1 = 0.5  # 0.9 is not good at oscillation & instability\n        self.lr = lr      # 1e-3 is too high...\n\n        self.bulid_deblurgan()  # build DeblurGAN model\n\n    def discriminator(self, x, reuse=None):\n        with tf.variable_scope(\'discriminator\', reuse=reuse):\n            x = t.conv2d(x, self.df_dim * 1, 4, 2, name=\'disc-conv2d-1\')\n            x = tf.nn.leaky_relu(x)\n\n            for i in range(1, 3):\n                x = t.conv2d(x, self.df_dim * (2 ** i), 4, 2, name=\'disc-conv2d-%d\' % (i + 1))\n                x = t.instance_norm(x, reuse=reuse, name=\'disc-inst_norm-%d\' % i)\n                x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 8, 4, 1, name=\'disc-conv2d-4\')\n            x = t.instance_norm(x, reuse=reuse, name=\'disc-inst_norm-3\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, 1, 4, 1, name=\'disc-conv2d-5\')\n\n            return x\n\n    def generator(self, x, reuse=None):\n        with tf.variable_scope(\'generator\', reuse=reuse):\n            def residual_block(x, f, name=""""):\n                with tf.variable_scope(name, reuse=reuse):\n                    skip_connection = tf.identity(x, name=\'gen-skip_connection-1\')\n\n                    x = t.conv2d(x, f, 3, 1, name=\'gen-conv2d-1\')\n                    x = t.instance_norm(x, reuse=reuse, name=\'gen-inst_norm-1\')\n                    x = tf.nn.relu(x)\n                    x = t.conv2d(x, f, 3, 1, name=\'gen-conv2d-2\')\n                    x = tf.nn.relu(x)\n\n                    return skip_connection + x\n\n            shortcut = tf.identity(x, name=\'shortcut-init\')\n\n            x = t.conv2d(x, self.gf_dim * 1, 7, 1, name=\'gen-conv2d-1\')\n            x = t.instance_norm(x, affine=False, reuse=reuse, name=\'gen-inst_norm-1\')\n            x = tf.nn.relu(x)\n\n            for i in range(1, 3):\n                x = t.conv2d(x, self.gf_dim * (2 ** i), 3, 2, name=\'gen-conv2d-%d\' % (i + 1))\n                x = t.instance_norm(x, affine=False, reuse=reuse, name=\'gen-inst_norm-%d\' % (i + 1))\n                x = tf.nn.relu(x)\n\n            # 9 Residual Blocks\n            for i in range(9):\n                x = residual_block(x, self.gf_dim * 4, name=\'gen-residual_block-%d\' % (i + 1))\n\n            for i in range(1, 3):\n                x = t.deconv2d(x, self.gf_dim * (2 ** i), 3, 2, name=\'gen-deconv2d-%d\' % i)\n                x = t.instance_norm(x, affine=False, reuse=reuse, name=\'gen-inst_norm-%d\' % (i + 3))\n                x = tf.nn.relu(x)\n\n            x = t.conv2d(x, self.gf_dim * 1, 7, 1, name=\'gen-conv2d-4\')\n            x = tf.nn.tanh(x)\n            return shortcut + x\n\n    def build_vgg19(self, x, reuse=None):\n        with tf.variable_scope(""vgg19"", reuse=reuse):\n            # image re-scaling\n            x = tf.cast((x + 1) / 2, dtype=tf.float32)  # [-1, 1] to [0, 1]\n            x = tf.cast(x * 255., dtype=tf.float32)     # [0, 1]  to [0, 255]\n\n            r, g, b = tf.split(x, 3, 3)\n            bgr = tf.concat([b - self.vgg_mean[0],\n                             g - self.vgg_mean[1],\n                             r - self.vgg_mean[2]], axis=3)\n\n            self.vgg19 = vgg19.VGG19(bgr)\n\n            net = self.vgg19.vgg19_net[\'conv3_3\']\n            return net\n\n    def bulid_deblurgan(self):\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        _, d_real = self.discriminator(self.x)\n        _, d_fake = self.discriminator(self.g, reuse=True)\n\n        # Losses\n        """"""\n        d_real_loss = -tf.reduce_mean(log(d_real))\n        d_fake_loss = -tf.reduce_mean(log(1. - d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n        self.g_loss = -tf.reduce_mean(log(d_fake))\n        """"""\n        d_real_loss = t.sce_loss(d_real, tf.ones_like(d_real))\n        d_fake_loss = t.sce_loss(d_fake, tf.zeros_like(d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n        self.g_loss = t.sce_loss(d_fake, tf.ones_like(d_fake))\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Collect trainer values\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        # Optimizer\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model Saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
DeblurGAN/deblurgan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport deblurgan_model as deblurgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CelebADataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/DCGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epoch\': 25,\n    \'batch_size\': 128,\n    \'logging_interval\': 500,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # DeblurGAN model\n        model = deblurgan.DeblurGAN(s, batch_size=train_step[\'batch_size\'])\n\n        # Initializing variables\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        # Training, Test data set\n        # loading CelebA DataSet\n        ds = DataSet(height=64,\n                     width=64,\n                     channel=3,\n                     ds_image_path=""D:\\\\DataSet/CelebA/CelebA-64.h5"",\n                     ds_label_path=""D:\\\\DataSet/CelebA/Anno/list_attr_celeba.txt"",\n                     # ds_image_path=""D:\\\\DataSet/CelebA/Img/img_align_celeba/"",\n                     ds_type=""CelebA"",\n                     use_save=False,\n                     save_file_name=""D:\\\\DataSet/CelebA/CelebA-64.h5"",\n                     save_type=""to_h5"",\n                     use_img_scale=False,\n                     # img_scale=""-1,1""\n                     )\n\n        # saving sample images\n        test_images = np.reshape(iu.transform(ds.images[:16], inv_type=\'127\'), (16, 64, 64, 3))\n        iu.save_images(test_images,\n                       size=[4, 4],\n                       image_path=results[\'output\'] + \'sample.png\',\n                       inv_type=\'127\')\n\n        ds_iter = DataIterator(x=ds.images,\n                               y=None,\n                               batch_size=train_step[\'batch_size\'],\n                               label_off=True)\n\n        global_step = saved_global_step\n        start_epoch = global_step // (len(ds.train_images) // model.batch_size)           # recover n_epoch\n        ds_iter.pointer = saved_global_step % (len(ds.train_images) // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epoch\']):\n            for batch_x in ds_iter.iterate():\n                batch_x = np.reshape(iu.transform(batch_x, inv_type=\'127\'),\n                                     (model.batch_size, model.height, model.width, model.channel))\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update D network\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z\n                                  })\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                if global_step % train_step[\'logging_interval\'] == 0:\n                    d_loss, g_loss, summary = s.run([model.d_loss, model.g_loss, model.merged],\n                                                    feed_dict={\n                                                        model.x: batch_x,\n                                                        model.z: batch_z,\n                                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %03d Step %05d => "" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss))\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim])\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.z: sample_z,\n                                    })\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{0}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step)\n\n                global_step += 1\n\n        end_time = time.time() - start_time  # Clocking end\n\n        # Elapsed time\n        print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n        # Close tf.Session\n        s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
DeblurGAN/vgg19.py,8,"b'from urllib.request import urlretrieve\n\nimport tensorflow as tf\nimport numpy as np\nimport scipy.io\nimport os\n\n\nvgg19_download_link = \'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat\'\nvgg19_file_name = \'imagenet-vgg-verydeep-19.mat\'\n\n\ndef vgg19_download(file_name, expected_bytes=534904783):\n    """""" Download the pre-trained VGG-19 model if it\'s not already downloaded """"""\n\n    if os.path.exists(file_name):\n        print(""[*] VGG-19 pre-trained model already exists"")\n        return\n\n    print(""[*] Downloading the VGG-19 pre-trained model..."")\n\n    file_name, _ = urlretrieve(vgg19_download_link, vgg19_file_name)\n    file_stat = os.stat(file_name)\n\n    if file_stat.st_size == expected_bytes:\n        print(\'[+] Successfully downloaded VGG-19 pre-trained model\', file_name)\n    else:\n        raise Exception(\'[-] File \' + file_name + \' might be corrupted :(\')\n\n\ndef conv2d_layer(input_, weights, bias):\n    """""" convolution 2d layer with bias """"""\n    x = tf.nn.conv2d(input_, filter=weights, strides=(1, 1, 1, 1), padding=\'SAME\')\n    x = tf.nn.bias_add(x, bias)\n    return x\n\n\ndef pool2d_layer(input_, pool=\'avg\'):\n    """""" pooling 2c layer with max or avg """"""\n    if pool == \'avg\':\n        x = tf.nn.avg_pool(input_, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding=\'SAME\')\n    else:\n        x = tf.nn.max_pool(input_, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding=\'SAME\')\n    return x\n\n\nclass VGG19(object):\n\n    def __init__(self, input_image):\n        vgg19_download(vgg19_file_name)  # download vgg19 pre-trained model\n\n        self.vgg19_layers = (\n            \'conv1_1\', \'relu1_1\', \'conv1_2\', \'relu1_2\', \'pool1\',\n            \'conv2_1\', \'relu2_1\', \'conv2_2\', \'relu2_2\', \'pool2\',\n            \'conv3_1\', \'relu3_1\', \'conv3_2\', \'relu3_2\', \'conv3_3\',\n            \'relu3_3\', \'conv3_4\', \'relu3_4\', \'pool3\',\n            \'conv4_1\', \'relu4_1\', \'conv4_2\', \'relu4_2\', \'conv4_3\',\n            \'relu4_3\', \'conv4_4\', \'relu4_4\', \'pool4\',\n            \'conv5_1\', \'relu5_1\', \'conv5_2\', \'relu5_2\', \'conv5_3\',\n            \'relu5_3\', \'conv5_4\', \'relu5_4\'\n        )\n\n        self.mean_pixels = np.array([123.68, 116.779, 103.939]).reshape((1, 1, 1, 3))\n\n        self.weights = scipy.io.loadmat(vgg19_file_name)[\'layers\'][0]\n\n        self.input_img = input_image\n        self.vgg19_net = self.build(self.input_img)\n\n    def _get_weight(self, idx, layer_name):\n        weight = self.weights[idx][0][0][2][0][0]\n        bias = self.weights[idx][0][0][2][0][1].reshape(-1)\n\n        assert layer_name == self.weights[idx][0][0][0][0]\n\n        with tf.variable_scope(layer_name):\n            weight = tf.constant(weight, name=\'weights\')\n            bias = tf.constant(bias, name=\'bias\')\n        return weight, bias\n\n    def build(self, img):\n        x = {}  # network\n        net = img\n\n        for idx, name in enumerate(self.vgg19_layers):\n            layer_name = name[:4]\n\n            if layer_name == \'conv\':\n                weight, bias = self._get_weight(idx, name)\n                net = conv2d_layer(net, weight, bias)\n            elif layer_name == \'relu\':\n                net = tf.nn.relu(net)\n            elif layer_name == \'pool\':\n                net = pool2d_layer(net)\n\n            x[name] = net\n\n        return x\n'"
DiscoGAN/discogan_model.py,50,"b'import tensorflow as tf\nimport numpy as np\nimport tfutil as t\n\n\ntf.set_random_seed(777)\nnp.random.seed(777)\n\n\nclass DiscoGAN:\n\n    def __init__(self, s, batch_size=64, height=64, width=64, channel=3,\n                 sample_size=32, sample_num=64, z_dim=128, gf_dim=32, df_dim=32,\n                 learning_rate=2e-4, beta1=0.5, beta2=0.999, eps=1e-9):\n\n        self.s = s\n        self.batch_size = batch_size\n        self.sample_size = sample_size\n        self.sample_num = sample_num\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = (self.height, self.width, self.channel)\n\n        self.z_dim = z_dim\n\n        self.eps = eps\n        self.mm1 = beta1\n        self.mm2 = beta2\n\n        self.gf_dim = gf_dim\n        self.df_dim = df_dim\n\n        self.lr = learning_rate\n\n        self.build_discogan()\n\n    def discriminator(self, x, scope_name, reuse=None):\n        with tf.variable_scope(""%s"" % scope_name, reuse=reuse):\n            x = t.conv2d(x, f=self.df_dim, k=4, s=1)  # 64 x 64 x 3\n            x = tf.nn.leaky_relu(x)\n\n            for i in range(np.log2(x.get_shape()[1]) - 2):  # 0 ~ 3\n                x = t.conv2d(x, self.df_dim * (2 ** (i + 1)), k=4, s=2)\n                x = t.batch_norm(x)\n                x = tf.nn.leaky_relu(x)\n\n            #  (-1, 4, 4, 512)\n            x = tf.layers.flatten(x)\n\n            x = t.dense(x, 512)\n            x = tf.nn.leaky_relu(x)\n\n            x = t.dense(x, 1)\n            x = tf.sigmoid(x)\n\n            return x\n\n    def generator(self, z, scope_name, reuse=None, is_train=True):\n        with tf.variable_scope(""%s"" % scope_name, reuse=reuse):\n            x = t.dense(z, 4 * 4 * 8 * self.gf_dim)\n            x = tf.nn.leaky_relu(x)\n\n            x = tf.layers.flatten(x)\n            x = tf.reshape(x, (-1, 4, 4, 8))\n\n            for i in range(np.log2(self.height) - 2):  # 0 ~ 3\n                x = t.deconv2d(x, self.gf_dim * (2 ** (i + 1)), k=4, s=2)\n                x = t.batch_norm(x, is_train=is_train)\n                x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, 3)\n\n            return x\n\n    def build_discogan(self):\n        self.A = tf.placeholder(tf.float32, shape=[None,\n                                                   self.height,\n                                                   self.width,\n                                                   self.channel], name=\'trainA\')\n        self.B = tf.placeholder(tf.float32, shape=[None,\n                                                   self.height,\n                                                   self.width,\n                                                   self.channel], name=\'trainA\')\n        # generator\n        # s : shoes, b : bags, 2 : to\n        self.G_AB = self.generator(self.A, ""generator_AB"")\n        self.G_BA = self.generator(self.B, ""generator_BA"")\n\n        self.G_ABA = self.generator(self.G_AB, ""generator_AB"", reuse=True)\n        self.G_BAB = self.generator(self.G_BA, ""generator_BA"", reuse=True)\n\n        # discriminator\n        self.D_s_real = self.discriminator(self.A, ""discriminator_real"")\n        self.D_b_real = self.discriminator(self.B, ""discriminator_fake"")\n\n        self.D_s_fake = self.discriminator(self.G_ABA, ""discriminator_real"", reuse=True)\n        self.D_b_fake = self.discriminator(self.G_BAB, ""discriminator_fake"", reuse=True)\n\n        # loss\n        self.s_loss = tf.reduce_sum(tf.losses.mean_squared_error(self.A, self.G_ABA))\n        self.b_loss = tf.reduce_sum(tf.losses.mean_squared_error(self.B, self.G_BAB))\n\n        # self.g_shoes_loss = tf.reduce_sum(tf.square(self.D_s_fake - 1)) / 2\n        # self.g_bags_loss = tf.reduce_sum(tf.square(self.D_b_fake - 1)) / 2\n\n        # self.d_shoes_real_loss = tf.reduce_sum(tf.square(self.D_s_real - 1)) / 2\n        # self.d_shoes_fake_loss = tf.reduce_sum(tf.square(self.D_s_fake)) / 2\n        # self.d_bags_real_loss = tf.reduce_sum(tf.square(self.D_b_real - 1)) / 2\n        # self.d_bags_fake_loss = tf.reduce_sum(tf.square(self.D_b_fake)) / 2\n\n        # self.d_shoes_loss = self.d_shoes_real_loss + self.d_shoes_fake_loss\n        # self.d_bags_loss = self.d_bags_real_loss + self.d_bags_fake_loss\n\n        # self.g_loss = 10 * (self.s_loss + self.b_loss) + self.g_shoes_loss + self.g_bags_loss\n        # self.d_loss = self.d_shoes_loss + self.d_bags_loss\n        # sigmoid cross entropy loss\n        self.g_s_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_s_fake,\n                                                                               labels=tf.ones_like(self.D_s_fake)))\n        self.g_b_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_b_fake,\n                                                                               labels=tf.ones_like(self.D_b_fake)))\n\n        self.d_s_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_s_real,\n                                                                                    labels=tf.ones_like(self.D_s_real)))\n        self.d_b_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_b_real,\n                                                                                    labels=tf.ones_like(self.D_b_real)))\n        self.d_s_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_s_fake,\n                                                                                    labels=tf.zeros_like(self.D_s_fake)))\n        self.d_b_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_b_fake,\n                                                                                    labels=tf.zeros_like(self.D_b_fake)))\n\n        self.g_loss = (self.s_loss + self.g_s_loss) + (self.b_loss + self.g_b_loss)\n\n        self.d_s_loss = self.d_s_real_loss + self.d_s_fake_loss\n        self.d_b_loss = self.d_b_real_loss + self.d_b_fake_loss\n        self.d_loss = self.d_s_loss + self.d_b_loss\n\n        # collect trainer values\n        # vars = tf.trainable_variables()\n        self.d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, ""discriminator_*"")\n        self.g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, ""generator_*"")\n\n        # summary\n        self.g_shoes_sum = tf.summary.histogram(""G_A"", self.g_s_loss)\n        self.g_bags_sum = tf.summary.histogram(""G_B"", self.g_b_loss)\n\n        self.d_shoes_sum = tf.summary.histogram(""D_A"", self.d_s_loss)\n        self.d_bags_sum = tf.summary.histogram(""D_B"", self.d_b_loss)\n\n        self.g_loss_sum = tf.summary.scalar(""g_loss"", self.g_loss)\n        self.d_loss_sum = tf.summary.scalar(""d_loss"", self.d_loss)\n\n        self.saver = tf.train.Saver()\n\n        # train op\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=self.mm1, beta2=self.mm2).\\\n            minimize(self.g_loss, var_list=self.g_vars)\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=self.mm1, beta2=self.mm2).\\\n            minimize(self.d_loss, var_list=self.d_vars)\n\n        # merge summary\n        self.g_sum = tf.summary.merge([self.g_loss_sum, self.g_shoes_sum, self.g_bags_sum])\n        self.d_sum = tf.summary.merge([self.d_loss_sum, self.d_shoes_sum, self.d_bags_sum])\n        self.merged = tf.summary.merge_all()\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
DiscoGAN/discogan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\n# import numpy as np\n\nimport time\nimport discogan\n\nimport sys\nsys.path.insert(0, \'../\')\n\nimport image_utils as iu\nfrom datasets import Pix2PixDataSet as DataSets\n\n\nresults = {\n    \'sample_output\': \'./gen_img/\',\n    \'model\': \'./model/DiscoGAN-model.ckpt\'\n}\nparas = {\n    \'epoch\': 200,\n    \'batch_size\': 64,\n    \'logging_interval\': 5\n}\n\n\ndef main():\n    start_time = time.time()  # clocking start\n\n    # Dataset\n    dataset = DataSets(height=64,\n                       width=64,\n                       channel=3,\n                       ds_path=\'D:/DataSets/pix2pix/\',\n                       ds_name=""vangogh2photo"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    with tf.Session(config=config) as s:\n        # DiscoGAN model\n        model = discogan.DiscoGAN(s)\n\n        # load model & graph & weight\n        global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            global_step = ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1]\n            print(""[+] global step : %s"" % global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        # initializing variables\n        tf.global_variables_initializer().run()\n\n        d_overpowered = False  # G loss > D loss * 2\n        for epoch in range(paras[\'epoch\']):\n            for step in range(1000):\n                offset_a = (step * paras[\'batch_size\']) % (dataset.images_a.shape[0] - paras[\'batch_size\'])\n                offset_b = (step * paras[\'batch_size\']) % (dataset.images_b.shape[0] - paras[\'batch_size\'])\n\n                # batch data set\n                batch_a = dataset.images_a[offset_a:(offset_a + paras[\'batch_size\']), :]\n                batch_b = dataset.images_b[offset_b:(offset_b + paras[\'batch_size\']), :]\n\n                # update D network\n                if not d_overpowered:\n                    s.run(model.d_op, feed_dict={model.A: batch_a})\n\n                # update G network\n                s.run(model.g_op, feed_dict={model.B: batch_b})\n\n                if epoch % paras[\'logging_interval\'] == 0:\n                    d_loss, g_loss, summary = s.run([\n                        model.d_loss,\n                        model.g_loss,\n                        model.merged\n                    ], feed_dict={\n                        model.A: batch_a,\n                        model.B: batch_b\n                    })\n\n                    # print loss\n                    print(""[+] Epoch %03d Step %04d => "" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss))\n\n                    # update overpowered\n                    d_overpowered = d_loss < g_loss / 2.\n\n                    # training G model with sample image and noise\n                    ab_samples = s.run(model.G_s2b, feed_dict={model.A: batch_a})\n                    ba_samples = s.run(model.G_b2s, feed_dict={model.B: batch_b})\n\n                    # summary saver\n                    model.writer.add_summary(summary, global_step=global_step)\n\n                    # export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_ab_dir = results[\'sample_output\'] + \'train_A_{0}_{1}.png\'.format(epoch, global_step)\n                    sample_ba_dir = results[\'sample_output\'] + \'train_B_{0}_{1}.png\'.format(epoch, global_step)\n\n                    # Generated image save\n                    iu.save_images(ab_samples, size=[sample_image_height, sample_image_width],\n                                   image_path=sample_ab_dir)\n                    iu.save_images(ba_samples, size=[sample_image_height, sample_image_width],\n                                   image_path=sample_ba_dir)\n\n                    # model save\n                    model.saver.save(s, results[\'model\'], global_step=global_step)\n\n        end_time = time.time() - start_time\n\n        # elapsed time\n        print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n        # close tf.Session\n        s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
DualGAN/dualgan_model.py,18,"b'import tensorflow as tf\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass DualGAN:\n\n    def __init__(self, s, batch_size=64, height=64, width=64, channel=3,\n                 sample_num=16 * 16, sample_size=16,\n                 df_dim=64, gf_dim=64,\n                 lambda_a=20., lambda_b=20., z_dim=128, g_lr=1e-4, d_lr=1e-4, epsilon=1e-9):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: input image height, default 64\n        :param width: input image width, default 64\n        :param channel: input image channel, default 3 (RGB)\n        - in case of Celeb-A, image size is 32x32x3(HWC).\n\n        # Output Settings\n        :param sample_num: the number of output images, default 256\n        :param sample_size: sample image size, default 16\n\n        # For CNN model\n        :param df_dim: discriminator filter, default 64\n        :param gf_dim: generator filter, default 64\n\n        # Training Option\n        :param lambda_a: weight of A recovery loss, default 20\n        :param lambda_a: weight of B recovery loss, default 20\n        :param z_dim: z dimension (kinda noise), default 128\n        :param g_lr: generator learning rate, default 1e-4\n        :param d_lr: discriminator learning rate, default 1e-4\n        :param epsilon: epsilon, default 1e-9\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = [self.batch_size, self.height, self.width, self.channel]\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n\n        self.lambda_a = lambda_a\n        self.lambda_b = lambda_b\n        self.z_dim = z_dim\n        self.decay = .9\n        self.d_lr = d_lr\n        self.g_lr = g_lr\n        self.eps = epsilon\n\n        # pre-defined\n        self.d_real = 0.\n        self.d_fake = 0.\n        self.g_loss = 0.\n        self.d_loss = 0.\n\n        self.g = None\n        self.g_test = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x_A = tf.placeholder(tf.float32,\n                                  shape=[None, self.height, self.width, self.channel],\n                                  name=""x-image-A"")\n        self.x_B = tf.placeholder(tf.float32,\n                                  shape=[None, self.height, self.width, self.channel],\n                                  name=""x-image-B"")\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')  # (-1, 128)\n\n        self.build_dualgan()  # build DualGAN model\n\n    def discriminator(self, x, reuse=None):\n        """"""\n        :param x: images\n        :param reuse: re-usable\n        :return: logits\n        """"""\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            x = t.conv2d(x, self.df_dim * 1, name=\'d-conv2d-1\')\n            x = tf.nn.leaky_relu(x)\n\n            for i in range(1, 3):\n                x = t.conv2d(x, self.df_dim * (2 ** i), name=\'d-conv2d-%d\' % (i + 1))\n                x = t.batch_norm(x)\n                x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, 1, s=1, name=\'d-conv2d-4\')\n\n            return x\n\n    def generator(self, z, reuse=None):\n        """"""\n        :param z: embeddings\n        :param reuse: re-usable\n        :return: logits\n        """"""\n        with tf.variable_scope(""generator"", reuse=reuse):\n\n            return z\n\n    def build_dualgan(self):\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        d_real = self.discriminator(self.x)\n        d_fake = self.discriminator(self.g, reuse=True)\n\n        # Loss\n        d_real_loss = t.l1_loss(self.x, d_real)\n        d_fake_loss = t.l1_loss(self.g, d_fake)\n        self.d_loss = d_real_loss + d_fake_loss\n        self.g_loss = d_fake_loss\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.RMSPropOptimizer(learning_rate=self.d_lr,\n                                              decay=self.decay).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.RMSPropOptimizer(learning_rate=self.g_lr,\n                                              decay=self.decay).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
DualGAN/dualgan_train.py,4,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport dualgan_model as dualgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CelebADataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/DualGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epoch\': 64,\n    \'batch_size\': 64,\n    \'logging_step\': 2500,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # DualGAN Model\n        model = dualgan.DualGAN(s)  # DualGAN\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        # Celeb-A DataSet images\n        ds = DataSet(height=32,\n                     width=32,\n                     channel=3,\n                     ds_path=""D:/DataSets/CelebA/"",\n                     ds_type=""CelebA"").images\n        ds_iter = DataIterator(ds, None, train_step[\'batch_size\'],\n                               label_off=True)\n\n        global_step = 0\n        for epoch in range(train_step[\'epoch\']):\n            for batch_images in ds_iter.iterate():\n                batch_x = np.reshape(batch_images, [-1] + model.image_shape[1:])\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update D network\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.z: batch_z,\n                                  })\n\n                # Update k_t\n                _, k, m_global = s.run([model.k_update, model.k, model.m_global],\n                                       feed_dict={\n                                            model.x: batch_x,\n                                            model.z: batch_z,\n                                       })\n\n                if global_step % train_step[\'logging_step\'] == 0:\n                    batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                    # Summary\n                    _, k, m_global, d_loss, g_loss, summary = s.run([model.k_update, model.k, model.m_global,\n                                                                     model.d_loss, model.g_loss, model.merged],\n                                                                    feed_dict={\n                                                                        model.x: batch_x,\n                                                                        model.z: batch_z,\n                                                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %04d Step %07d =>"" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss),\n                          "" k : {:.8f}"".format(k),\n                          "" M : {:.8f}"".format(m_global))\n\n                    # Summary saver\n                    model.writer.add_summary(summary, epoch)\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.z: sample_z,\n                                    })\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{0}_{1}.png\'.format(epoch, global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir)\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step=global_step)\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
EBGAN/ebgan_model.py,34,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass EBGAN:\n\n    @staticmethod\n    def pullaway_loss(x, n):\n        # PullAway Loss # 2.4 Repelling Regularizer in 1609.03126.pdf\n        normalized = x / tf.sqrt(tf.reduce_sum(tf.square(x), 1, keepdims=True))\n        similarity = tf.matmul(normalized, normalized, transpose_b=True)\n        return (tf.reduce_sum(similarity) - n) / (n * (n - 1))\n\n    def __init__(self, s, batch_size=64, height=64, width=64, channel=3, n_classes=41,\n                 sample_num=4 * 4, sample_size=4,\n                 df_dim=64, gf_dim=64, z_dim=128, g_lr=2e-4, d_lr=2e-4,\n                 enable_pull_away=True):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: input image height, default 64\n        :param width: input image width, default 64\n        :param channel: input image channel, default 3\n        :param n_classes: input DataSet\'s classes\n\n        # Output Settings\n        :param sample_num: the number of output images, default 100\n        :param sample_size: sample image size, default 10\n\n        # For CNN model\n        :param df_dim: discriminator filter, default 64\n        :param gf_dim: generator filter, default 64\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 128\n        :param g_lr: generator learning rate, default 2e-4\n        :param d_lr: discriminator learning rate, default 2e-4\n        :param enable_pull_away: enabling PULL-AWAY loss, default True\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = [None, self.height, self.width, self.channel]\n        self.n_classes = n_classes\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n\n        self.z_dim = z_dim\n        self.beta1 = 0.5\n        self.beta2 = 0.9\n        self.d_lr, self.g_lr = d_lr, g_lr\n        self.EnablePullAway = enable_pull_away\n        self.pt_lambda = 0.1\n\n        # 1 is enough. But in case of the large batch, it needs value more than 1.\n        # 20 for CelebA, 80 for LSUN\n        self.margin = 20.  # max(1., self.batch_size / 64.)\n\n        self.g_loss = 0.\n        self.d_loss = 0.\n        self.pt_loss = 0.\n\n        self.g = None\n        self.g_test = None\n\n        self.d_op = None\n        self.g_op = None\n\n        # pre-defined\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32, shape=self.image_shape, name=""x-image"")    # (-1, 64, 64, 3)\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')  # (-1, 128)\n\n        self.build_ebgan()  # build EBGAN model\n\n    def encoder(self, x, reuse=None):\n        """"""\n        (64)4c2s - (128)4c2s - (256)4c2s\n        :param x: images\n        :param reuse: re-usable\n        :return: logits\n        """"""\n        with tf.variable_scope(\'encoder\', reuse=reuse):\n            x = t.conv2d(x, self.df_dim * 1, 4, 2, name=\'enc-conv2d-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 2, 4, 2, name=\'enc-conv2d-2\')\n            x = t.batch_norm(x, name=\'enc-bn-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 4, 4, 2, name=\'enc-conv2d-3\')\n            x = t.batch_norm(x, name=\'enc-bn-2\')\n            x = tf.nn.leaky_relu(x)\n            return x\n\n    def decoder(self, x, reuse=None):\n        """"""\n        (128)4c2s - (64)4c2s - (3)4c2s\n        :param x: embeddings\n        :param reuse: re-usable\n        :return: prob\n        """"""\n        with tf.variable_scope(\'decoder\', reuse=reuse):\n            x = t.deconv2d(x, self.df_dim * 2, 4, 2, name=\'dec-deconv2d-1\')\n            x = t.batch_norm(x, name=\'dec-bn-1\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.df_dim * 1, 4, 2, name=\'dec-deconv2d-2\')\n            x = t.batch_norm(x, name=\'dec-bn-2\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.channel, 4, 2, name=\'dec-deconv2d-3\')\n            x = tf.nn.tanh(x)\n            return x\n\n    def discriminator(self, x, reuse=None):\n        """"""\n        :param x: images\n        :param reuse: re-usable\n        :return: prob, embeddings, gen-ed_image\n        """"""\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            embeddings = self.encoder(x, reuse=reuse)\n            decoded = self.decoder(embeddings, reuse=reuse)\n\n            return embeddings, decoded\n\n    def generator(self, z, reuse=None, is_train=True):\n        """"""\n        # referred architecture in the paper\n        : (1024)4c - (512)4c2s - (256)4c2s - (128)4c2s - (3)4c2s\n        :param z: embeddings\n        :param reuse: re-usable\n        :param is_train: trainable\n        :return: prob\n        """"""\n        with tf.variable_scope(""generator"", reuse=reuse):\n            x = t.dense(z, self.gf_dim * 8 * 4 * 4, name=\'gen-fc-1\')\n\n            x = tf.reshape(x, (-1, 4, 4, self.gf_dim * 8))\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-1\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.gf_dim * 4, 4, 2, name=\'gen-deconv2d-1\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-2\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.gf_dim * 2, 4, 2, name=\'gen-deconv2d-2\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-3\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.gf_dim * 1, 4, 2, name=\'gen-deconv2d-3\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-4\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.channel, 4, 2, name=\'gen-deconv2d-4\')\n            x = tf.nn.tanh(x)\n\n            return x\n\n    def build_ebgan(self):\n        # Generator\n        self.g = self.generator(self.z)\n        self.g_test = self.generator(self.z, reuse=True, is_train=False)\n\n        # Discriminator\n        d_embed_real, d_decode_real = self.discriminator(self.x)\n        d_embed_fake, d_decode_fake = self.discriminator(self.g, reuse=True)\n\n        d_real_loss = t.mse_loss(d_decode_real, self.x, self.batch_size)\n        d_fake_loss = t.mse_loss(d_decode_fake, self.g, self.batch_size)\n        self.d_loss = d_real_loss + tf.maximum(0., self.margin - d_fake_loss)\n\n        if self.EnablePullAway:\n            self.pt_loss = self.pullaway_loss(d_embed_fake, self.batch_size)\n\n        self.g_loss = d_fake_loss + self.pt_lambda * self.pt_loss\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n        tf.summary.scalar(""loss/pt_loss"", self.pt_loss)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.d_lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.g_lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
EBGAN/ebgan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport ebgan_model as ebgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CelebADataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/EBGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epochs\': 25,\n    \'batch_size\': 64,\n    \'global_step\': 300001,\n    \'logging_interval\': 1000,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # loading CelebA DataSet\n    ds = DataSet(height=64,\n                 width=64,\n                 channel=3,\n                 ds_image_path=""/home/zero/hdd/DataSet/CelebA/CelebA-64.h5"",\n                 ds_label_path=""/home/zero/hdd/DataSet/CelebA/Anno/list_attr_celeba.txt"",\n                 # ds_image_path=""/home/zero/hdd/DataSet/CelebA/Img/img_align_celeba/"",\n                 ds_type=""CelebA"",\n                 use_save=False,\n                 save_file_name=""/home/zero/hdd/DataSet/CelebA/CelebA-64.h5"",\n                 save_type=""to_h5"",\n                 use_img_scale=False,\n                 # img_scale=""-1,1""\n                 )\n\n    # saving sample images\n    test_images = np.reshape(iu.transform(ds.images[:16], inv_type=\'127\'), (16, 64, 64, 3))\n    iu.save_images(test_images,\n                   size=[4, 4],\n                   image_path=results[\'output\'] + \'sample.png\',\n                   inv_type=\'127\')\n\n    ds_iter = DataIterator(x=ds.images,\n                           y=None,\n                           batch_size=train_step[\'batch_size\'],\n                           label_off=True)\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # EBGAN Model\n        model = ebgan.EBGAN(s, enable_pull_away=True)  # using pull away loss # EBGAN-PT\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %s"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        global_step = saved_global_step\n        start_epoch = global_step // (ds.num_images // model.batch_size)  # recover n_epoch\n        ds_iter.pointer = saved_global_step % (ds.num_images // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epochs\']):\n            for batch_x in ds_iter.iterate():\n                batch_x = iu.transform(batch_x, inv_type=\'127\')\n                batch_x = np.reshape(batch_x, (model.batch_size, model.height, model.width, model.channel))\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.z: batch_z,\n                                  })\n\n                # Logging\n                if global_step % train_step[\'logging_interval\'] == 0:\n                    summary = s.run(model.merged,\n                                    feed_dict={\n                                        model.x: batch_x,\n                                        model.z: batch_z,\n                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %02d Step %08d => "" % (epoch, global_step),\n                          "" D  loss : {:.8f}"".format(d_loss),\n                          "" G  loss : {:.8f}"".format(g_loss))\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.z: sample_z,\n                                    })\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{:08d}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step)\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
FGAN/fgan_model.py,44,"b'import tensorflow as tf\nimport numpy as np\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)\n\n\nclass FGAN:\n\n    def __init__(self, s, batch_size=64, height=28, width=28, channel=1,\n                 sample_num=8 * 8, sample_size=8,\n                 z_dim=128, dfc_unit=256, gfc_unit=1024, lr=2e-4,\n                 divergence_method=\'KL\', use_tricky_g_loss=False):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: input image height, default 28\n        :param width: input image width, default 28\n        :param channel: input image channel, default 1\n\n        # Output Settings\n        :param sample_num: the number of sample images, default 64\n        :param sample_size: sample image size, default 8\n\n        # Model Settings\n        :param z_dim: z noise dimension, default 128\n        :param dfc_unit: the number of fully connected units used at disc, default 256\n        :param gfc_unit: the number of fully connected units used at gen, default 1024\n\n        # Training Settings\n        :param lr: learning rate, default 2e-4\n        :param divergence_method: the method of f-divergences, default \'KL\'\n        :param use_tricky_g_loss: use g_loss referred in f-GAN Section 3.2, default False\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n        self.channel = channel\n\n        self.sample_size = sample_size\n        self.sample_num = sample_num\n\n        self.image_shape = [self.height, self.width, self.channel]\n        self.n_input = self.height * self.width * self.channel\n\n        self.z_dim = z_dim\n\n        self.dfc_unit = dfc_unit\n        self.gfc_unit = gfc_unit\n\n        # pre-defined\n        self.d_loss = 0.\n        self.g_loss = 0.\n\n        self.g = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32, shape=[None, self.n_input], name=\'x-images\')\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')\n\n        # Training Options\n        self.beta1 = 0.5\n        self.lr = lr\n\n        self.divergence = divergence_method\n        self.use_tricky_g_loss = use_tricky_g_loss\n\n        self.bulid_fgan()  # build f-GAN model\n\n    def discriminator(self, x, reuse=None):\n        with tf.variable_scope(\'discriminator\', reuse=reuse):\n            x = t.dense(x, self.dfc_unit, name=\'disc-fc-1\')\n            x = tf.nn.elu(x)\n\n            x = t.dense(x, self.dfc_unit, name=\'disc-fc-2\')\n            x = tf.nn.elu(x)\n\n            x = tf.layers.flatten(x)\n\n            x = t.dense(x, 1, name=\'disc-fc-3\')\n            return x\n\n    def generator(self, z, reuse=None, is_train=True):\n        with tf.variable_scope(\'generator\', reuse=reuse):\n            x = t.dense(z, self.gfc_unit, name=\'gen-fc-1\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-1\')\n            x = tf.nn.relu(x)\n\n            x = t.dense(x, self.gfc_unit, name=\'gen-fc-2\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-2\')\n            x = tf.nn.relu(x)\n\n            x = t.dense(x, self.n_input, name=\'gen-fc-3\')\n            x = tf.nn.sigmoid(x)\n            return x\n\n    def bulid_fgan(self):\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        d_real = self.discriminator(self.x)\n        d_fake = self.discriminator(self.g, reuse=True)\n\n        # Losses\n        if self.divergence == \'GAN\':\n            def activation(x): return -tf.reduce_mean(-t.safe_log(1. + tf.exp(-x)))\n\n            def conjugate(x): return -tf.reduce_mean(-t.safe_log(1. - tf.exp(x)))\n        elif self.divergence == \'KL\':  # tf.distribution.kl_divergence\n            def activation(x): return -tf.reduce_mean(x)\n\n            def conjugate(x): return -tf.reduce_mean(tf.exp(x - 1.))\n        elif self.divergence == \'Reverse-KL\':\n            def activation(x): return -tf.reduce_mean(-tf.exp(x))\n\n            def conjugate(x): return -tf.reduce_mean(-1. - x)  # remove log\n        elif self.divergence == \'JS\':\n            def activation(x): return -tf.reduce_mean(tf.log(2.) - t.safe_log(1. + tf.exp(-x)))\n\n            def conjugate(x): return -tf.reduce_mean(-t.safe_log(2. - tf.exp(x)))\n        elif self.divergence == \'JS-Weighted\':\n            def activation(x): return -tf.reduce_mean(-np.pi * np.log(np.pi) - t.safe_log(1. + tf.exp(-x)))\n\n            def conjugate(x): return -tf.reduce_mean((1. - np.pi) *\n                                                     t.safe_log((1. - np.pi) / (1. - np.pi * tf.exp(x / np.pi))))\n        elif self.divergence == \'Squared-Hellinger\':\n            def activation(x): return -tf.reduce_mean(1. - tf.exp(x))\n\n            def conjugate(x): return -tf.reduce_mean(x / (1. - x))\n        elif self.divergence == \'Pearson\':\n            def activation(x): return -tf.reduce_mean(x)\n\n            def conjugate(x): return -tf.reduce_mean(tf.square(x) / 4. + x)\n        elif self.divergence == \'Neyman\':\n            def activation(x): return -tf.reduce_mean(1. - tf.exp(x))\n\n            def conjugate(x): return -tf.reduce_mean(2. - 2. * tf.sqrt(1. - x))\n        elif self.divergence == \'Jeffrey\':\n            from scipy.special import lambertw\n\n            def activation(x): return -tf.reduce_mean(x)\n\n            def conjugate(x):\n                lambert_w = lambertw(self.s.run(tf.exp(1. - x)))  # need to be replaced with another tensor func\n                return -tf.reduce_mean(lambert_w + 1. / lambert_w + x - 2.)\n        elif self.divergence == \'Total-Variation\':\n            def activation(x): return -tf.reduce_mean(tf.nn.tanh(x) / 2.)\n\n            def conjugate(x): return -tf.reduce_mean(x)\n        else:\n            raise NotImplementedError(""[-] Not Implemented f-divergence %s"" % self.divergence)\n\n        d_real_loss = activation(d_real)\n        d_fake_loss = conjugate(d_fake)\n        self.d_loss = d_real_loss - d_fake_loss\n        if self.use_tricky_g_loss:\n            self.g_loss = activation(d_fake)\n        else:\n            self.g_loss = d_fake_loss\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Collect trainer values\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        # Optimizer\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model Saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/%s/\' % self.divergence, self.s.graph)\n'"
FGAN/fgan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport fgan_model as fgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import MNISTDataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/\',\n}\n\ntrain_step = {\n    \'batch_size\': 4096,\n    \'global_steps\': 20001,\n    \'logging_interval\': 1000,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # Loading MNIST DataSet\n    mnist = DataSet(ds_path=""D:\\\\DataSet/mnist/"").data\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    idx = 1\n    divergences = [\'GAN\', \'KL\', \'Reverse-KL\', \'JS\',\n                   \'JS-Weighted\', \'Squared-Hellinger\', \'Pearson\', \'Neyman\',\n                   \'Jeffrey\', \'Total-Variation\']\n    assert (0 <= idx < len(divergences))\n\n    results[\'output\'] += \'%s/\' % divergences[idx]\n    results[\'model\'] += \'%s/fGAN-model.ckpt\' % divergences[idx]\n\n    with tf.Session(config=config) as s:\n        # f-GAN model\n        model = fgan.FGAN(s, batch_size=train_step[\'batch_size\'],\n                          divergence_method=divergences[idx],\n                          use_tricky_g_loss=True)\n\n        # Initializing variables\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n\n        ckpt = tf.train.get_checkpoint_state(\'./model/%s/\' % divergences[idx])\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        for global_step in range(saved_global_step, train_step[\'global_steps\']):\n            batch_x, _ = mnist.train.next_batch(model.batch_size)\n            batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n            # Update D network\n            _, d_loss = s.run([model.d_op, model.d_loss],\n                              feed_dict={\n                                  model.x: batch_x,\n                                  model.z: batch_z,\n                              })\n\n            # Update G network\n            _, g_loss = s.run([model.g_op, model.g_loss],\n                              feed_dict={\n                                  model.x: batch_x,\n                                  model.z: batch_z,\n                              })\n\n            if global_step % train_step[\'logging_interval\'] == 0:\n                summary = s.run(model.merged,\n                                feed_dict={\n                                    model.x: batch_x,\n                                    model.z: batch_z,\n                                })\n\n                # Print loss\n                print(""[+] Global step %06d => "" % global_step,\n                      "" D loss : {:.8f}"".format(d_loss),\n                      "" G loss : {:.8f}"".format(g_loss))\n\n                # Training G model with sample image and noise\n                sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim])\n                samples = s.run(model.g,\n                                feed_dict={\n                                    model.z: sample_z,\n                                })\n                samples = np.reshape(samples, (-1, 28, 28, 1))\n\n                # Summary saver\n                model.writer.add_summary(summary, global_step)\n\n                # Export image generated by model G\n                sample_image_height = model.sample_size\n                sample_image_width = model.sample_size\n                sample_dir = results[\'output\'] + \'train_{0}.png\'.format(global_step)\n\n                # Generated image save\n                iu.save_images(samples,\n                               size=[sample_image_height, sample_image_width],\n                               image_path=sample_dir,\n                               inv_type=\'255\')\n\n                # Model save\n                model.saver.save(s, results[\'model\'], global_step)\n\n        end_time = time.time() - start_time  # Clocking end\n\n        # Elapsed time\n        print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n        # Close tf.Session\n        s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
GAN/gan_model.py,21,"b'import tensorflow as tf\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass GAN:\n\n    def __init__(self, s, batch_size=32, height=28, width=28, channel=1, n_classes=10,\n                 sample_num=10 * 10, sample_size=10,\n                 n_input=784, fc_unit=128, z_dim=128, g_lr=8e-4, d_lr=8e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 32\n        :param height: input image height, default 28\n        :param width: input image width, default 28\n        :param channel: input image channel, default 1 (gray-scale)\n        - in case of MNIST, image size is 28x28x1(HWC).\n        :param n_classes: input dataset\'s classes\n        - in case of MNIST, 10 (0 ~ 9)\n\n        # Output Settings\n        :param sample_num: the number of output images, default 64\n        :param sample_size: sample image size, default 8\n\n        # For DNN model\n        :param n_input: input image size, default 784(28x28)\n        :param fc_unit: fully connected units, default 128\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 128\n        :param g_lr: generator learning rate, default 8e-4\n        :param d_lr: discriminator learning rate, default 8e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.n_classes = n_classes\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.n_input = n_input\n        self.fc_unit = fc_unit\n\n        self.z_dim = z_dim\n        self.beta1 = 0.5\n        self.d_lr, self.g_lr = d_lr, g_lr\n\n        # pre-defined\n        self.d_loss = 0.\n        self.g_loss = 0.\n\n        self.g = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholder\n        self.x = tf.placeholder(tf.float32, shape=[None, self.n_input], name=""x-image"")  # (-1, 784)\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')    # (-1, 100)\n\n        self.build_gan()  # build GAN model\n\n    def discriminator(self, x, reuse=None):\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            x = t.dense(x, self.fc_unit, name=\'disc-fc-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.dense(x, 1, name=\'discd-fc-2\')\n\n            return x\n\n    def generator(self, z, reuse=None):\n        with tf.variable_scope(""generator"", reuse=reuse):\n            x = t.dense(z, self.fc_unit, name=\'gen-fc-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.dense(x, self.n_input, name=\'gen-fc-2\')\n            x = tf.nn.sigmoid(x)\n\n            return x\n\n    def build_gan(self):\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        d_real = self.discriminator(self.x)\n        d_fake = self.discriminator(self.g, reuse=True)\n\n        # General GAN loss function referred in the paper\n        """"""\n        self.g_loss = -tf.reduce_mean(t.safe_log(d_fake))\n        self.d_loss = -tf.reduce_mean(t.safe_log(d_real) + t.safe_log(1. - d_fake))\n        """"""\n\n        # Softmax Loss\n        # Z_B = sigma x\xe2\x88\x88B exp(\xe2\x88\x92\xce\xbc(x)), \xe2\x88\x92\xce\xbc(x) is discriminator\n        z_b = tf.reduce_sum(tf.exp(-d_real)) + tf.reduce_sum(tf.exp(-d_fake)) + t.eps\n\n        b_plus = self.batch_size\n        b_minus = self.batch_size * 2\n\n        # L_G = sigma x\xe2\x88\x88B+ \xce\xbc(x)/abs(B) + sigma x\xe2\x88\x88B- \xce\xbc(x)/abs(B) + ln(Z_B), B+ : batch _size\n        self.g_loss = tf.reduce_sum(d_real / b_plus) + tf.reduce_sum(d_fake / b_minus) + t.safe_log(z_b)\n\n        # L_D = sigma x\xe2\x88\x88B+ \xce\xbc(x)/abs(B) + ln(Z_B), B+ : batch _size\n        self.d_loss = tf.reduce_sum(d_real / b_plus) + t.safe_log(z_b)\n\n        # Summary\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.d_lr,\n                                           beta1=self.beta1).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.g_lr,\n                                           beta1=self.beta1).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
GAN/gan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport gan_model as gan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import MNISTDataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/GAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'global_step\': 200001,\n    \'logging_interval\': 1000,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # MNIST Dataset load\n    mnist = DataSet(ds_path=""D:/DataSet/mnist/"").data\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # GAN Model\n        model = gan.GAN(s)\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %s"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        d_loss = 0.\n        d_overpowered = False\n        for global_step in range(saved_global_step, train_step[\'global_step\']):\n            batch_x, _ = mnist.train.next_batch(model.batch_size)\n            batch_x = batch_x.reshape(-1, model.n_input)\n            batch_z = np.random.uniform(-1., 1., size=[model.batch_size, model.z_dim]).astype(np.float32)\n\n            # Update D network\n            if not d_overpowered:\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n            # Update G network\n            _, g_loss = s.run([model.g_op, model.g_loss],\n                              feed_dict={\n                                  model.x: batch_x,\n                                  model.z: batch_z,\n                              })\n\n            d_overpowered = d_loss < (g_loss / 2.)\n\n            if global_step % train_step[\'logging_interval\'] == 0:\n                batch_x, _ = mnist.test.next_batch(model.batch_size)\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                d_loss, g_loss, summary = s.run([model.d_loss, model.g_loss, model.merged],\n                                                feed_dict={\n                                                    model.x: batch_x,\n                                                    model.z: batch_z,\n                                                })\n\n                d_overpowered = d_loss < (g_loss / 2.)\n\n                # Print loss\n                print(""[+] Step %08d => "" % global_step,\n                      "" D loss : {:.8f}"".format(d_loss),\n                      "" G loss : {:.8f}"".format(g_loss))\n\n                # Training G model with sample image and noise\n                sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                samples = s.run(model.g,\n                                feed_dict={\n                                    model.z: sample_z,\n                                })\n\n                samples = np.reshape(samples, [-1, model.height, model.width, model.channel])\n\n                # Summary saver\n                model.writer.add_summary(summary, global_step)\n\n                # Export image generated by model G\n                sample_image_height = model.sample_size\n                sample_image_width = model.sample_size\n                sample_dir = results[\'output\'] + \'train_{:08d}.png\'.format(global_step)\n\n                # Generated image save\n                iu.save_images(samples,\n                               size=[sample_image_height, sample_image_width],\n                               image_path=sample_dir)\n\n                # Model save\n                model.saver.save(s, results[\'model\'], global_step)\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))  # took about 370s on my machine\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
InfoGAN/infogan_model.py,37,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass InfoGAN:\n\n    def __init__(self, s, batch_size=16, height=32, width=32, channel=3,\n                 sample_num=10 * 10, sample_size=10,\n                 df_dim=64, gf_dim=64, fc_unit=128, n_categories=10, n_continous_factor=1,\n                 z_dim=128, g_lr=1e-3, d_lr=2e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 16\n        :param height: input image height, default 32\n        :param width: input image width, default 32\n        :param channel: input image channel, default 3 (RGB)\n\n        # Output Settings\n        :param sample_num: the number of output images, default 100\n        :param sample_size: sample image size, default 10\n\n        # Hyper-parameters\n        :param df_dim: discriminator filter, default 64\n        :param gf_dim: generator filter, default 64\n        :param fc_unit: fully connected unit, default 128\n\n        # Training Option\n        :param n_categories: the number of categories, default 10\n        :param n_continous_factor: the number of cont factors, default 1\n        :param z_dim: z dimension (kinda noise), default 128\n        :param g_lr: generator learning rate, default 1e-3\n        :param d_lr: discriminator learning rate, default 2e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = [self.batch_size, self.height, self.width, self.channel]\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n        self.fc_unit = fc_unit\n\n        """"""\n        - MNIST\n        n_cat : 10, n_cont : 2, z : 62 => embeddings : 10 + 2 + 62 = 74\n        - SVHN\n        n_cat : 10, n_cont : 4, z : 124 => embeddings : 40 + 124 = 168\n        - Celeb-A\n        n_cat : 10, n_cont : 10, z : 128 => embeddings : 100 + 128 = 228\n        """"""\n        self.n_cat = n_categories         # category dist, label\n        self.n_cont = n_continous_factor  # gaussian dist, rotate, etc\n        self.z_dim = z_dim\n        self.lambda_ = 1.  # sufficient for discrete latent codes # less than 1\n\n        self.beta1 = 0.5\n        self.beta2 = 0.999\n        self.d_lr = d_lr\n        self.g_lr = g_lr\n\n        # pre-defined\n        self.d_real = 0.\n        self.d_fake = 0.\n\n        self.g_loss = 0.\n        self.d_adv_loss = 0.\n        self.d_loss = 0.\n\n        self.g = None\n        self.g_test = None\n\n        self.d_op = None\n        self.g_op = None\n        self.q_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32,\n                                shape=[None, self.height, self.width, self.channel],\n                                name=""x-image"")                                                     # (-1, 32, 32, 3)\n        self.c = tf.placeholder(tf.float32, shape=[None, self.n_cont + self.n_cat], name=\'c-cond\')  # (-1, 11)\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')               # (-1, 128)\n\n        self.build_infogan()  # build InfoGAN model\n\n    def discriminator(self, x, reuse=None):\n        """"""\n        :param x: images\n        :param reuse: re-usable\n        :return: logits\n        """"""\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            x = t.conv2d(x, self.df_dim * 1, 4, 2, name=\'disc-conv2d-1\')\n            x = tf.nn.leaky_relu(x, alpha=0.1)\n\n            x = t.conv2d(x, self.df_dim * 2, 4, 2, name=\'disc-conv2d-2\')\n            x = t.batch_norm(x, name=\'disc-bn-1\')\n            x = tf.nn.leaky_relu(x, alpha=0.1)\n\n            x = t.conv2d(x, self.df_dim * 4, 4, 2, name=\'disc-conv2d-3\')\n            x = t.batch_norm(x, name=\'disc-bn-2\')\n            x = tf.nn.leaky_relu(x, alpha=0.1)\n\n            x = t.conv2d(x, self.df_dim * 8, 4, 2, name=\'disc-conv2d-4\')\n            x = t.batch_norm(x, name=\'disc-bn-3\')\n            x = tf.nn.leaky_relu(x, alpha=0.1)\n\n            x = tf.layers.flatten(x)\n\n            x = t.dense(x, self.fc_unit, name=\'disc-fc-1\')\n            x = t.batch_norm(x, name=\'disc-bn-4\')\n            x = tf.nn.leaky_relu(x, alpha=0.1)\n\n            x = t.dense(x, 1 + self.n_cont + self.n_cat, name=\'disc-fc-2\')\n            prob, cont, cat = x[:, 0], x[:, 1:1 + self.n_cont], x[:, 1 + self.n_cont:]  # logits\n\n            prob = tf.nn.sigmoid(prob)  # probability\n            cat = tf.nn.softmax(cat)    # categories\n\n            return prob, cont, cat\n\n    def generator(self, z, c, reuse=None, is_train=True):\n        """"""\n        :param z: 139 z-noise\n        :param c: 10 categories * 10 dimensions\n        :param reuse: re-usable\n        :param is_train: trainable\n        :return: prob\n        """"""\n        with tf.variable_scope(""generator"", reuse=reuse):\n            x = tf.concat([z, c], axis=1)  # (-1, 128 + 1 + 10)\n\n            x = t.dense(x, 2 * 2 * 512, name=\'gen-fc-1\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-1\')\n            x = tf.nn.relu(x)\n\n            x = tf.reshape(x, (-1, 2, 2, 512))\n\n            x = t.deconv2d(x, self.gf_dim * 8, 4, 2, name=\'gen-deconv2d-1\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-2\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.gf_dim * 4, 4, 2, name=\'gen-deconv2d-2\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.gf_dim * 2, 4, 2, name=\'gen-deconv2d-3\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.gf_dim * 1, 4, 2, name=\'gen-deconv2d-4\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, 3, 4, 2, name=\'gen-deconv2d-5\')\n            x = tf.nn.tanh(x)\n            return x\n\n    def build_infogan(self):\n        # Generator\n        self.g = self.generator(self.z, self.c)\n        # self.g_test = self.generator(self.z, self.c, reuse=True, is_train=False)\n\n        # Discriminator\n        d_real, d_real_cont, d_real_cat = self.discriminator(self.x)\n        d_fake, d_fake_cont, d_fake_cat = self.discriminator(self.g, reuse=True)\n\n        # Losses\n        self.d_adv_loss = -tf.reduce_mean(t.safe_log(d_real) + t.safe_log(1. - d_fake))\n\n        d_cont_loss = tf.reduce_mean(tf.square(d_fake_cont / .5))\n        cat = self.c[:, self.n_cont:]\n        d_cat_loss = -(tf.reduce_mean(tf.reduce_sum(cat * d_fake_cont)) + tf.reduce_mean(cat * cat))\n\n        d_info_loss = self.lambda_ * (d_cont_loss + d_cat_loss)\n\n        self.d_loss = self.d_adv_loss + d_info_loss\n        self.g_loss = -tf.reduce_mean(t.safe_log(d_fake)) + d_info_loss\n\n        # Summary\n        tf.summary.scalar(""loss/d_adv_loss"", self.d_adv_loss)\n        tf.summary.scalar(""loss/d_cont_loss"", d_cont_loss)\n        tf.summary.scalar(""loss/d_cat_loss"", d_cat_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.d_lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.g_lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
InfoGAN/infogan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport infogan_model as infogan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CelebADataSet as DataSet\n\n\nnp.random.seed(1337)\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/InfoGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epochs\': 5,\n    \'batch_size\': 16,\n    \'global_step\': 200001,\n    \'logging_interval\': 1000,\n}\n\n\ndef gen_category(n_size, n_dim):\n    return np.random.randn(n_size, n_dim) * .5  # gaussian\n\n\ndef gen_continuous(n_size, n_dim):\n    code = np.zeros((n_size, n_dim))\n    code[range(n_size), np.random.randint(0, n_dim, n_size)] = 1\n    return code\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # loading CelebA DataSet\n    labels = [\'Black_Hair\', \'Blond_Hair\', \'Blurry\', \'Eyeglasses\', \'Gray_Hair\', \'Male\', \'Smiling\', \'Wavy_Hair\',\n              \'Wearing_Hat\', \'Young\']\n\n    ds = DataSet(height=64,\n                 width=64,\n                 channel=3,\n                 ds_image_path=""/home/zero/hdd/DataSet/CelebA/CelebA-64.h5"",\n                 ds_label_path=""/home/zero/hdd/DataSet/CelebA/Anno/list_attr_celeba.txt"",\n                 attr_labels=labels,\n                 # ds_image_path=""D:\\\\DataSet/CelebA/Img/img_align_celeba/"",\n                 ds_type=""CelebA"",\n                 use_save=False,\n                 save_file_name=""D:\\\\DataSet/CelebA/CelebA-64.h5"",\n                 save_type=""to_h5"",\n                 use_img_scale=False,\n                 # img_scale=""-1,1""\n                 )\n\n    # saving sample images\n    test_images = np.reshape(iu.transform(ds.images[:16], inv_type=\'127\'), (16, 64, 64, 3))\n    iu.save_images(test_images,\n                   size=[4, 4],\n                   image_path=results[\'output\'] + \'sample.png\',\n                   inv_type=\'127\')\n\n    ds_iter = DataIterator(x=ds.images,\n                           y=ds.labels,\n                           batch_size=train_step[\'batch_size\'],\n                           label_off=False)\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # InfoGAN Model\n        model = infogan.InfoGAN(s,\n                                height=64,\n                                width=64,\n                                channel=3,\n                                batch_size=train_step[\'batch_size\'],\n                                n_categories=len(ds.labels))\n        # fixed z-noise\n        sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %s"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        global_step = saved_global_step\n        start_epoch = global_step // (ds.num_images // model.batch_size)  # recover n_epoch\n        ds_iter.pointer = saved_global_step % (ds.num_images // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epochs\']):\n            for batch_x, batch_y in ds_iter.iterate():\n                batch_x = iu.transform(batch_x, inv_type=\'127\')\n                batch_x = np.reshape(batch_x, (model.batch_size, model.height, model.width, model.channel))\n\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                batch_z_con = gen_continuous(model.batch_size, model.n_continous_factor)\n                batch_z_cat = gen_category(model.batch_size, model.n_categories)\n                batch_c = np.concatenate((batch_z_con, batch_z_cat), axis=1)\n\n                # Update D network\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.c: batch_c,\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                })\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.c: batch_c,\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                # Logging\n                if global_step % train_step[\'logging_interval\'] == 0:\n                    summary = s.run(model.merged,\n                                    feed_dict={\n                                        model.c: batch_c,\n                                        model.x: batch_x,\n                                        model.z: batch_z,\n                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %02d Step %08d => "" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss))\n\n                    # Training G model with sample image and noise\n                    sample_z_con = np.zeros((model.sample_num, model.n_continous_factor))\n                    for i in range(10):\n                        sample_z_con[10 * i: 10 * (i + 1), 0] = np.linspace(-2, 2, 10)\n\n                    sample_z_cat = np.zeros((model.sample_num, model.n_categories))\n                    for i in range(10):\n                        sample_z_cat[10 * i: 10 * (i + 1), i] = 1\n\n                    sample_c = np.concatenate((sample_z_con, sample_z_cat), axis=1)\n\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.c: sample_c,\n                                        model.z: sample_z,\n                                    })\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{:08d}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step)\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
LAPGAN/lapgan_model.py,46,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)\n\n\n# In image_utils, up/down_sampling\ndef image_sampling(img, sampling_type=\'down\'):\n    shape = img.get_shape()  # [batch, height, width, channels]\n\n    if sampling_type == \'down\':\n        h = int(shape[1] // 2)\n        w = int(shape[2] // 2)\n    else:  # \'up\'\n        h = int(shape[1] * 2)\n        w = int(shape[2] * 2)\n\n    return tf.image.resize_images(img, [h, w], tf.image.ResizeMethod.BILINEAR)\n\n\nclass LAPGAN:\n\n    def __init__(self, s, batch_size=128, height=32, width=32, channel=3, n_classes=10,\n                 sample_num=10 * 10, sample_size=10,\n                 z_dim=128, gf_dim=64, df_dim=64, d_fc_unit=512, g_fc_unit=1024):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 128\n        :param height: input image height, default 32\n        :param width: input image width, default 32\n        :param channel: input image channel, default 3 (RGB)\n        :param n_classes: the number of classes, default 10\n        - in case of CIFAR, image size is 32x32x3(HWC), classes are 10.\n\n        # Output Settings\n        :param sample_size: sample image size, default 8\n        :param sample_num: the number of sample images, default 64\n\n        # Model Settings\n        :param z_dim: z noise dimension, default 128\n        :param gf_dim: the number of generator filters, default 64\n        :param df_dim: the number of discriminator filters, default 64\n        :param d_fc_unit: the number of fully connected filters used at Disc, default 512\n        :param g_fc_unit: the number of fully connected filters used at Gen, default 1024\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.n_classes = n_classes\n\n        self.sample_size = sample_size\n        self.sample_num = sample_num\n\n        self.image_shape = [self.height, self.width, self.channel]\n\n        self.z_dim = z_dim\n\n        self.gf_dim = gf_dim\n        self.df_dim = df_dim\n        self.d_fc_unit = d_fc_unit\n        self.g_fc_unit = g_fc_unit\n\n        # Placeholders\n        self.y = tf.placeholder(tf.float32, shape=[None, self.n_classes],\n                                name=\'y-classes\')  # one_hot\n        self.x1_fine = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel],\n                                      name=\'x-images\')\n\n        self.x1_scaled = image_sampling(self.x1_fine, \'down\')\n        self.x1_coarse = image_sampling(self.x1_scaled, \'up\')\n        self.x1_diff = self.x1_fine - self.x1_coarse\n\n        self.x2_fine = self.x1_scaled  # [16, 16]\n        self.x2_scaled = image_sampling(self.x2_fine, \'down\')\n        self.x2_coarse = image_sampling(self.x2_scaled, \'up\')\n        self.x2_diff = self.x2_fine - self.x2_coarse\n\n        self.x3_fine = self.x2_scaled  # [8, 8]\n\n        self.z = []\n        self.z_noises = [32 * 32, 16 * 16, self.z_dim]\n        for i in range(3):\n            self.z.append(tf.placeholder(tf.float32,\n                                         shape=[None, self.z_noises[i]],\n                                         name=\'z-noise_{0}\'.format(i)))\n\n        self.do_rate = tf.placeholder(tf.float32, None, name=\'do-rate\')\n\n        self.g = []       # generators\n        self.g_loss = []  # generator losses\n\n        self.d_reals = []       # discriminator_real logits\n        self.d_fakes = []       # discriminator_fake logits\n        self.d_loss = []        # discriminator_real losses\n\n        # Training Options\n        self.d_op = []\n        self.g_op = []\n\n        self.beta1 = 0.5\n        self.beta2 = 0.9\n        self.lr = 8e-4\n\n        self.saver = None\n        self.merged = None\n        self.writer = None\n\n        self.bulid_lapgan()  # build LAPGAN model\n\n    def discriminator(self, x1, x2, y, scale=32, reuse=None):\n        """"""\n        :param x1: image to discriminate\n        :param x2: down-up sampling-ed images\n        :param y: classes\n        :param scale: image size\n        :param reuse: variable re-use\n        :return: logits\n        """"""\n\n        assert (scale % 8 == 0)  # 32, 16, 8\n\n        with tf.variable_scope(\'discriminator_{0}\'.format(scale), reuse=reuse):\n            if scale == 8:\n                x1 = tf.reshape(x1, [-1, scale * scale * 3])  # tf.layers.flatten(x1)\n\n                h = tf.concat([x1, y], axis=1)\n\n                h = t.dense(h, self.d_fc_unit, name=\'disc-fc-1\')\n                h = tf.nn.relu(h)\n                h = tf.layers.dropout(h, 0.5, name=\'disc-dropout-1\')\n\n                h = t.dense(h, self.d_fc_unit // 2, name=\'d-fc-2\')\n                h = tf.nn.relu(h)\n                h = tf.layers.dropout(h, 0.5, name=\'disc-dropout-2\')\n\n                h = t.dense(h, 1, name=\'disc-fc-3\')\n            else:\n                x = x1 + x2\n\n                y = t.dense(y, scale * scale, name=\'disc-fc-y\')\n                y = tf.nn.relu(y)\n\n                y = tf.reshape(y, [-1, scale, scale, 1])\n\n                h = tf.concat([x, y], axis=3)  # (-1, scale, scale, channel + 1)\n\n                h = t.conv2d(h, self.df_dim * 1, 5, 1, pad=\'SAME\', name=\'disc-conv2d-1\')\n                h = tf.nn.relu(h)\n                h = tf.layers.dropout(h, 0.5, name=\'disc-dropout-1\')\n\n                h = t.conv2d(h, self.df_dim * 1, 5, 1, pad=\'SAME\', name=\'disc-conv2d-2\')\n                h = tf.nn.relu(h)\n                h = tf.layers.dropout(h, 0.5, name=\'disc-dropout-2\')\n\n                h = tf.layers.flatten(h)\n\n                h = t.dense(h, 1, name=\'disc-fc-2\')\n\n            return h\n\n    def generator(self, x, y, z, scale=32, reuse=None, do_rate=0.5):\n        """"""\n        :param x: images to fake\n        :param y: classes\n        :param z: noise\n        :param scale: image size\n        :param reuse: variable re-use\n        :param do_rate: dropout rate\n        :return: logits\n        """"""\n\n        assert (scale % 8 == 0)  # 32, 16, 8\n\n        with tf.variable_scope(\'generator_{0}\'.format(scale), reuse=reuse):\n            if scale == 8:\n                h = tf.concat([z, y], axis=1)\n\n                h = t.dense(h, self.g_fc_unit, name=\'gen-fc-1\')\n                h = tf.nn.relu(h)\n                h = tf.layers.dropout(h, do_rate, name=\'gen-dropout-1\')\n\n                h = t.dense(h, self.g_fc_unit, name=\'gen-fc-2\')\n                h = tf.nn.relu(h)\n                h = tf.layers.dropout(h, do_rate, name=\'gen-dropout-2\')\n\n                h = t.dense(h, self.channel * 8 * 8, name=\'gen-fc-3\')\n\n                h = tf.reshape(h, [-1, 8, 8, self.channel])\n            else:\n                y = t.dense(y, scale * scale, name=\'gen-fc-y\')\n\n                y = tf.reshape(y, [-1, scale, scale, 1])\n                z = tf.reshape(z, [-1, scale, scale, 1])\n\n                h = tf.concat([z, y, x], axis=3)  # concat into 5 dims\n\n                h = t.conv2d(h, self.gf_dim * 1, 5, 1, name=\'gen-deconv2d-1\')\n                h = tf.nn.relu(h)\n\n                h = t.conv2d(h, self.gf_dim * 1, 5, 1, name=\'gen-deconv2d-2\')\n                h = tf.nn.relu(h)\n\n                h = t.conv2d(h, self.channel, 5, 1, name=\'gen-conv2d-3\')\n\n            h = tf.nn.tanh(h)\n\n            return h\n\n    def bulid_lapgan(self):\n        # Generator & Discriminator\n        g1 = self.generator(x=self.x1_coarse, y=self.y, z=self.z[0], scale=32, do_rate=self.do_rate)\n        d1_fake = self.discriminator(x1=g1, x2=self.x1_coarse, y=self.y, scale=32)\n        d1_real = self.discriminator(x1=self.x1_diff, x2=self.x1_coarse, y=self.y, scale=32, reuse=True)\n\n        g2 = self.generator(x=self.x2_coarse, y=self.y, z=self.z[1], scale=16, do_rate=self.do_rate)\n        d2_fake = self.discriminator(x1=g2, x2=self.x2_coarse, y=self.y, scale=16)\n        d2_real = self.discriminator(x1=self.x2_diff, x2=self.x2_coarse, y=self.y, scale=16, reuse=True)\n\n        g3 = self.generator(x=None, y=self.y, z=self.z[2], scale=8, do_rate=self.do_rate)\n        d3_fake = self.discriminator(x1=g3, x2=None, y=self.y, scale=8)\n        d3_real = self.discriminator(x1=self.x3_fine, x2=None, y=self.y, scale=8, reuse=True)\n\n        self.g = [g1, g2, g3]\n        self.d_reals = [d1_real, d2_real, d3_real]\n        self.d_fakes = [d1_fake, d2_fake, d3_fake]\n\n        # Losses\n        with tf.variable_scope(\'loss\'):\n            for i in range(len(self.g)):\n                self.d_loss.append(t.sce_loss(self.d_reals[i], tf.ones_like(self.d_reals[i])) +\n                                   t.sce_loss(self.d_fakes[i], tf.zeros_like(self.d_fakes[i])))\n                self.g_loss.append(t.sce_loss(self.d_fakes[i], tf.ones_like(self.d_fakes[i])))\n\n        # Summary\n        for i in range(len(self.g)):\n            tf.summary.scalar(\'loss/d_loss_{0}\'.format(i), self.d_loss[i])\n            tf.summary.scalar(\'loss/g_loss_{0}\'.format(i), self.g_loss[i])\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        for idx, i in enumerate([32, 16, 8]):\n            self.d_op.append(tf.train.AdamOptimizer(learning_rate=self.lr,\n                                                    beta1=self.beta1, beta2=self.beta2).\n                             minimize(loss=self.d_loss[idx],\n                                      var_list=[v for v in t_vars if v.name.startswith(\'discriminator_{0}\'.format(i))]))\n            self.g_op.append(tf.train.AdamOptimizer(learning_rate=self.lr,\n                                                    beta1=self.beta1, beta2=self.beta2).\n                             minimize(loss=self.g_loss[idx],\n                                      var_list=[v for v in t_vars if v.name.startswith(\'generator_{0}\'.format(i))]))\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model Saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
LAPGAN/lapgan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport lapgan_model as lapgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CiFarDataSet as DataSet\n\n\nnp.random.seed(1337)\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/LAPGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epoch\': 200,\n    \'batch_size\': 64,\n    \'logging_interval\': 1000,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # Training, test data set\n    ds = DataSet(height=32,\n                 width=32,\n                 channel=3,\n                 ds_path=\'D:\\\\DataSet/cifar/cifar-10-batches-py/\',\n                 ds_name=\'cifar-10\')\n\n    ds_iter = DataIterator(ds.train_images, ds.train_labels,\n                           train_step[\'batch_size\'])\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # LAPGAN model\n        model = lapgan.LAPGAN(s, batch_size=train_step[\'batch_size\'])\n\n        # Initializing variables\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %s"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        sample_y = np.zeros(shape=[model.sample_num, model.n_classes])\n        for i in range(10):\n            sample_y[10 * i:10 * (i + 1), i] = 1\n\n        global_step = saved_global_step\n        start_epoch = global_step // (len(ds.train_images) // model.batch_size)  # recover n_epoch\n        ds_iter.pointer = saved_global_step % (len(ds.train_images) // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epoch\']):\n            for batch_images, batch_labels in ds_iter.iterate():\n                batch_x = iu.transform(batch_images, inv_type=\'127\')\n\n                z = []\n                for i in range(3):\n                    z.append(np.random.uniform(-1., 1., [train_step[\'batch_size\'], model.z_noises[i]]))\n\n                # Update D/G networks\n                img_fake, img_coarse, d_loss_1, g_loss_1, \\\n                _, _, _, d_loss_2, g_loss_2, \\\n                _, _, d_loss_3, g_loss_3, \\\n                _, _, _, _, _, _ = s.run([\n                    model.g[0], model.x1_coarse, model.d_loss[0], model.g_loss[0],\n\n                    model.x2_fine, model.g[1], model.x2_coarse, model.d_loss[1], model.g_loss[1],\n\n                    model.x3_fine, model.g[2], model.d_loss[2], model.g_loss[2],\n\n                    model.d_op[0], model.g_op[0], model.d_op[1], model.g_op[1], model.d_op[2], model.g_op[2],\n                ],\n                    feed_dict={\n                        model.x1_fine: batch_x,  # images\n                        model.y: batch_labels,   # classes\n                        model.z[0]: z[0], model.z[1]: z[1], model.z[2]: z[2],  # z-noises\n                        model.do_rate: 0.5,\n                    })\n\n                # Logging\n                if global_step % train_step[\'logging_interval\'] == 0:\n                    batch_x = ds.test_images[np.random.randint(0, len(ds.test_images), model.sample_num)]\n                    batch_x = iu.transform(batch_x, inv_type=\'127\')\n\n                    z = []\n                    for i in range(3):\n                        z.append(np.random.uniform(-1., 1., [model.sample_num, model.z_noises[i]]))\n\n                    # Update D/G networks\n                    img_fake, img_coarse, d_loss_1, g_loss_1, \\\n                    _, _, _, d_loss_2, g_loss_2, \\\n                    _, _, d_loss_3, g_loss_3, \\\n                    _, _, _, _, _, _, summary = s.run([\n                        model.g[0], model.x1_coarse, model.d_loss[0], model.g_loss[0],\n\n                        model.x2_fine, model.g[1], model.x2_coarse, model.d_loss[1], model.g_loss[1],\n\n                        model.x3_fine, model.g[2], model.d_loss[2], model.g_loss[2],\n\n                        model.d_op[0], model.g_op[0], model.d_op[1], model.g_op[1], model.d_op[2], model.g_op[2],\n\n                        model.merged,\n                    ],\n                        feed_dict={\n                            model.x1_fine: batch_x,  # images\n                            model.y: sample_y,       # classes\n                            model.z[0]: z[0], model.z[1]: z[1], model.z[2]: z[2],  # z-noises\n                            model.do_rate: 0.,\n                        })\n\n                    # Print loss\n                    d_loss = (d_loss_1 + d_loss_2 + d_loss_3) / 3.\n                    g_loss = (g_loss_1 + g_loss_2 + g_loss_3) / 3.\n                    print(""[+] Epoch %03d Step %05d => "" % (epoch, global_step),\n                          "" Avg D loss : {:.8f}"".format(d_loss),\n                          "" Avg G loss : {:.8f}"".format(g_loss))\n\n                    # Training G model with sample image and noise\n                    samples = img_fake + img_coarse\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)  # time saving\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{0}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples, size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step)\n\n                global_step += 1\n\n        end_time = time.time() - start_time  # Clocking end\n\n        # Elapsed time\n        print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n        # Close tf.Session\n        s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
LSGAN/lsgan_model.py,28,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass LSGAN:\n\n    def __init__(self, s, batch_size=64, height=32, width=32, channel=3, n_classes=10,\n                 sample_num=10 * 10, sample_size=10,\n                 df_dim=64, gf_dim=64, fc_unit=1024, z_dim=128, lr=2e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: input image height, default 32\n        :param width: input image width, default 32\n        :param channel: input image channel, default 3 (gray-scale)\n        :param n_classes: input DataSet\'s classes\n\n        # Output Settings\n        :param sample_num: the number of output images, default 64\n        :param sample_size: sample image size, default 8\n\n        # For CNN model\n        :param df_dim: discriminator filter, default 64\n        :param gf_dim: generator filter, default 64\n        :param fc_unit: the number of fully connected filters, default 1024\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 128\n        :param lr: learning rate, default 2e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = [self.batch_size, self.height, self.width, self.channel]\n        self.n_classes = n_classes\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n        self.fc_unit = fc_unit\n\n        self.z_dim = z_dim\n        self.beta1 = 0.5\n        self.lr = lr\n\n        # pre-defined\n        self.g_loss = 0.\n        self.d_loss = 0.\n\n        self.g = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholder\n        self.x = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel],\n                                name=""x-image"")  # (-1, 64, 64, 3)\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim],\n                                name=\'z-noise\')  # (-1, 128)\n\n        self.build_lsgan()  # build LSGAN model\n\n    def discriminator(self, x, reuse=None):\n        """""" Same as DCGAN Disc Net """"""\n        with tf.variable_scope(\'discriminator\', reuse=reuse):\n            x = t.conv2d(x, self.df_dim * 1, 5, 2, name=\'disc-conv2d-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 2, 5, 2, name=\'disc-conv2d-2\')\n            x = t.batch_norm(x, name=\'disc-bn-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 4, 5, 2, name=\'disc-conv2d-3\')\n            x = t.batch_norm(x, name=\'disc-bn-2\')\n            x = tf.nn.leaky_relu(x)\n\n            x = tf.layers.flatten(x)\n\n            logits = t.dense(x, 1, name=\'disc-fc-1\')\n            prob = tf.nn.sigmoid(logits)\n\n            return prob, logits\n\n    def generator(self, z, reuse=None, is_train=True):\n        """""" Same as DCGAN Gen Net """"""\n        with tf.variable_scope(\'generator\', reuse=reuse):\n            x = t.dense(z, self.gf_dim * 4 * 4 * 4, name=\'gen-fc-1\')\n\n            x = tf.reshape(x, [-1, 4, 4, self.gf_dim * 4])\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-1\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x,  self.gf_dim * 2, 5, 2, name=\'gen-deconv2d-1\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-2\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x,  self.gf_dim * 1, 5, 2, name=\'gen-deconv2d-2\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-3\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.channel, 5, 2, name=\'gen-deconv2d-3\')\n            x = tf.nn.tanh(x)\n\n            return x\n\n    def build_lsgan(self):\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        d_real = self.discriminator(self.x)\n        d_fake = self.discriminator(self.g, reuse=True)\n\n        # LSGAN Loss\n        d_real_loss = t.mse_loss(d_real, tf.ones_like(d_real), self.batch_size)\n        d_fake_loss = t.mse_loss(d_fake, tf.zeros_like(d_fake), self.batch_size)\n        self.d_loss = (d_real_loss + d_fake_loss) / 2.\n        self.g_loss = t.mse_loss(d_fake, tf.ones_like(d_fake), self.batch_size)\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
LSGAN/lsgan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport lsgan_model as lsgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CiFarDataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/LSGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epoch\': 201,\n    \'batch_size\': 64,\n    \'global_step\': 200001,\n    \'logging_interval\': 1000,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # Training, Test data set\n    # loading Cifar DataSet\n    ds = DataSet(height=32,\n                 width=32,\n                 channel=3,\n                 ds_path=\'D:\\\\DataSet/cifar/cifar-10-batches-py/\',\n                 ds_name=\'cifar-10\')\n\n    # saving sample images\n    test_images = np.reshape(iu.transform(ds.test_images[:16], inv_type=\'127\'), (16, 32, 32, 3))\n    iu.save_images(test_images,\n                   size=[4, 4],\n                   image_path=results[\'output\'] + \'sample.png\',\n                   inv_type=\'127\')\n\n    ds_iter = DataIterator(x=ds.train_images,\n                           y=None,\n                           batch_size=train_step[\'batch_size\'],\n                           label_off=True)\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # GAN Model\n        model = lsgan.LSGAN(s, train_step[\'batch_size\'])\n\n        # Initializing variables\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        global_step = saved_global_step\n        start_epoch = global_step // (len(ds.train_images) // model.batch_size)\n        ds_iter.pointer = saved_global_step % (len(ds.train_images) // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epoch\']):\n            for batch_x in ds_iter.iterate():\n                batch_x = iu.transform(batch_x, inv_type=\'127\')\n                batch_x = np.reshape(batch_x, [-1] + model.image_shape[1:])\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update D network\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z\n                                  })\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z\n                                  })\n\n                # Logging\n                if global_step % train_step[\'logging_interval\'] == 0:\n                    d_loss, g_loss, summary = s.run([model.d_loss, model.g_loss, model.merged],\n                                                    feed_dict={\n                                                        model.x: batch_x,\n                                                        model.z: batch_z\n                                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %02d Step %08d => "" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss))\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.z: sample_z,\n                                    })\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{:08d}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step)\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
MAGAN/adamax.py,2,"b'# taken from https://github.com/openai/iaf/blob/master/tf_utils/adamax.py\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.training import optimizer\nimport tensorflow as tf\n\n\nclass AdamaxOptimizer(optimizer.Optimizer):\n    """"""Optimizer that implements the Adamax algorithm.\n    See [Kingma et. al., 2014](http://arxiv.org/abs/1412.6980)\n    ([pdf](http://arxiv.org/pdf/1412.6980.pdf)).\n    @@__init__\n    """"""\n\n    def __init__(self, learning_rate=1e-3, beta1=0.9, beta2=0.999, use_locking=False, name=""AdaMaxOptimizer""):\n        super(AdamaxOptimizer, self).__init__(use_locking, name)\n        self._lr = learning_rate\n        self._beta1 = beta1\n        self._beta2 = beta2\n\n        # Tensor versions of the constructor arguments, created in _prepare().\n        self._lr_t = None\n        self._beta1_t = None\n        self._beta2_t = None\n\n    def _prepare(self):\n        self._lr_t = ops.convert_to_tensor(self._lr, name=""learning_rate"")\n        self._beta1_t = ops.convert_to_tensor(self._beta1, name=""beta1"")\n        self._beta2_t = ops.convert_to_tensor(self._beta2, name=""beta2"")\n\n    def _create_slots(self, var_list):\n        # Create slots for the first and second moments.\n        for v in var_list:\n            self._zeros_slot(v, ""m"", self._name)\n            self._zeros_slot(v, ""v"", self._name)\n\n    def _apply_dense(self, grad, var):\n        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n        if var.dtype.base_dtype == tf.float16:\n            eps = 1e-7  # Can\'t use 1e-8 due to underflow -- not sure if it makes a big difference.\n        else:\n            eps = 1e-8\n\n        v = self.get_slot(var, ""v"")\n        v_t = v.assign(beta1_t * v + (1. - beta1_t) * grad)\n        m = self.get_slot(var, ""m"")\n        m_t = m.assign(tf.maximum(beta2_t * m + eps, tf.abs(grad)))\n        g_t = v_t / m_t\n\n        var_update = state_ops.assign_sub(var, lr_t * g_t)\n        return control_flow_ops.group(*[var_update, m_t, v_t])\n\n    def _apply_sparse(self, grad, var):\n        raise NotImplementedError(""Sparse gradient updates are not supported."")\n'"
MAGAN/magan_model.py,24,"b'import tensorflow as tf\n\nimport sys\n\nfrom adamax import AdamaxOptimizer\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass MAGAN:\n\n    def __init__(self, s, batch_size=64, height=64, width=64, channel=3, n_classes=41,\n                 sample_num=10 * 10, sample_size=10,\n                 df_dim=64, gf_dim=64, fc_unit=512, z_dim=350, lr=5e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: input image height, default 64\n        :param width: input image width, default 64\n        :param channel: input image channel, default 3\n        :param n_classes: input DataSet\'s classes, default 41\n\n        # Output Settings\n        :param sample_num: the number of output images, default 100\n        :param sample_size: sample image size, default 10\n\n        # For CNN model\n        :param df_dim: discriminator conv2d filter, default 64\n        :param gf_dim: generator conv2d filter, default 64\n        :param fc_unit: the number of fully connected filters, default 512\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 350\n        :param lr: generator learning rate, default 5e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = [self.batch_size, self.height, self.width, self.channel]\n        self.n_classes = n_classes\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n        self.fc_unit = fc_unit\n\n        self.z_dim = z_dim\n        self.beta1 = 0.5\n        self.lr = lr\n        self.pt_lambda = 0.1\n\n        # pre-defined\n        self.g_loss = 0.\n        self.d_loss = 0.\n        self.d_real_loss = 0.\n        self.d_fake_loss = 0.\n\n        self.g = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel], name=""x-image"")\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')\n        self.m = tf.placeholder(tf.float32, name=\'margin\')\n\n        self.build_magan()  # build MAGAN model\n\n    def encoder(self, x, reuse=None):\n        """"""\n        :param x: images\n        :param reuse: re-usable\n        :return: logits\n        """"""\n        with tf.variable_scope(\'encoder\', reuse=reuse):\n            for i in range(1, 5):\n                x = t.conv2d(x, self.df_dim * (2 ** (i - 1)), 4, 2, name=\'enc-conv2d-%d\' % i)\n                if i > 1:\n                    x = t.batch_norm(x, name=\'enc-bn-%d\' % (i - 1))\n                x = tf.nn.leaky_relu(x)\n\n            return x\n\n    def decoder(self, z, reuse=None):\n        """"""\n        :param z: embeddings\n        :param reuse: re-usable\n        :return: prob\n        """"""\n        with tf.variable_scope(\'decoder\', reuse=reuse):\n            x = z\n            for i in range(1, 4):\n                x = t.deconv2d(x, self.df_dim * 8 // (2 ** i), 4, 2, name=\'dec-deconv2d-%d\' % i)\n                x = t.batch_norm(x, name=\'dec-bn-%d\' % i)\n                x = tf.nn.leaky_relu(x)\n\n            x = t.deconv2d(x, self.channel, 4, 2, name=\'enc-deconv2d-4\')\n            x = tf.nn.tanh(x)\n            return x\n\n    def discriminator(self, x, reuse=None):\n        """"""\n        :param x: images\n        :param reuse: re-usable\n        :return: prob, embeddings, gen-ed_image\n        """"""\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            embeddings = self.encoder(x, reuse=reuse)\n            decoded = self.decoder(embeddings, reuse=reuse)\n\n            return embeddings, decoded\n\n    def generator(self, z, reuse=None, is_train=True):\n        """"""\n        :param z: embeddings\n        :param reuse: re-usable\n        :param is_train: trainable\n        :return: prob\n        """"""\n        with tf.variable_scope(""generator"", reuse=reuse):\n            x = tf.reshape(z, (-1, 1, 1, self.z_dim))\n\n            x = t.deconv2d(x, self.df_dim * 8, 4, 1, pad=\'VALID\', name=\'gen-deconv2d-1\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-1\')\n            x = tf.nn.relu(x)\n\n            for i in range(1, 4):\n                x = t.deconv2d(x, self.df_dim * 8 // (2 ** i), 4, 2, name=\'gen-deconv2d-%d\' % (i + 1))\n                x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-%d\' % (i + 1))\n                x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.channel, 4, 2, name=\'gen-deconv2d-5\')\n            x = tf.nn.tanh(x)\n            return x\n\n    def build_magan(self):\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        _, d_real = self.discriminator(self.x)\n        _, d_fake = self.discriminator(self.g, reuse=True)\n\n        self.d_real_loss = t.mse_loss(self.x, d_real, self.batch_size)\n        self.d_fake_loss = t.mse_loss(self.g, d_fake, self.batch_size)\n        self.d_loss = self.d_real_loss + tf.maximum(0., self.m - self.d_fake_loss)\n        self.g_loss = self.d_fake_loss\n\n        # Summary\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/d_real_loss"", self.d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", self.d_fake_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = AdamaxOptimizer(learning_rate=self.lr, beta1=self.beta1).minimize(self.d_loss,\n                                                                                      var_list=d_params)\n        self.g_op = AdamaxOptimizer(learning_rate=self.lr, beta1=self.beta1).minimize(self.g_loss,\n                                                                                      var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
MAGAN/magan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport magan_model as magan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CelebADataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/MAGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epochs\': 50,\n    \'batch_size\': 64,\n    \'global_step\': 200001,\n    \'logging_interval\': 1000,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # loading CelebA DataSet\n    ds = DataSet(height=64,\n                 width=64,\n                 channel=3,\n                 ds_image_path=""D:/DataSet/CelebA/CelebA-64.h5"",\n                 ds_label_path=""D:/DataSet/CelebA/Anno/list_attr_celeba.txt"",\n                 # ds_image_path=""D:/DataSet/CelebA/Img/img_align_celeba/"",\n                 ds_type=""CelebA"",\n                 use_save=False,\n                 save_file_name=""D:/DataSet/CelebA/CelebA-64.h5"",\n                 save_type=""to_h5"",\n                 use_img_scale=False,\n                 img_scale=""-1,1"")\n\n    # saving sample images\n    test_images = np.reshape(iu.transform(ds.images[:100], inv_type=\'127\'), (100, 64, 64, 3))\n    iu.save_images(test_images,\n                   size=[10, 10],\n                   image_path=results[\'output\'] + \'sample.png\',\n                   inv_type=\'127\')\n\n    ds_iter = DataIterator(x=ds.images,\n                           y=None,\n                           batch_size=train_step[\'batch_size\'],\n                           label_off=True)\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # MAGAN Model\n        model = magan.MAGAN(s)\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %s"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        n_steps = ds.num_images // model.batch_size  # training set size\n\n        # Pre-Train\n        print(""[*] pre-training - getting proper Margin"")\n\n        margin = 0  # 3.0585415484215974\n        if margin == 0:\n            sum_d_loss = 0.\n            for i in range(2):\n                for batch_x in ds_iter.iterate():\n                    batch_x = np.reshape(iu.transform(batch_x, inv_type=\'127\'),\n                                         (model.batch_size, model.height, model.width, model.channel))\n                    batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                    _, d_real_loss = s.run([model.d_op, model.d_real_loss],\n                                           feed_dict={\n                                               model.x: batch_x,\n                                               model.z: batch_z,\n                                               model.m: 0.,\n                                           })\n                    sum_d_loss += d_real_loss\n\n                print(""[*] Epoch {:1d} Sum of d_real_loss : {:.8f}"".format(i + 1, sum_d_loss))\n\n            # Initial margin value\n            margin = (sum_d_loss / n_steps)\n\n        print(""[+] Margin : {0}"".format(margin))\n\n        old_margin = margin\n        s_g_0 = np.inf  # Sg_0 = infinite\n\n        global_step = saved_global_step\n        start_epoch = global_step // (ds.num_images // model.batch_size)           # recover n_epoch\n        ds_iter.pointer = saved_global_step % (ds.num_images // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epochs\']):\n            s_d, s_g = 0., 0.\n            for batch_x in ds_iter.iterate():\n                batch_x = iu.transform(batch_x, inv_type=\'127\')\n                batch_x = np.reshape(batch_x, (model.batch_size, model.height, model.width, model.channel))\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update D network\n                _, d_loss, d_real_loss = s.run([model.d_op, model.d_loss, model.d_real_loss],\n                                               feed_dict={\n                                                   model.x: batch_x,\n                                                   model.z: batch_z,\n                                                   model.m: margin,\n                                               })\n\n                # Update D real sample\n                s_d += np.sum(d_real_loss)\n\n                # Update G network\n                _, g_loss, d_fake_loss = s.run([model.g_op, model.g_loss, model.d_fake_loss],\n                                               feed_dict={\n                                                   model.x: batch_x,\n                                                   model.z: batch_z,\n                                                   model.m: margin,\n                                               })\n\n                # Update G fake sample\n                s_g += np.sum(d_fake_loss)\n\n                # Logging\n                if global_step % train_step[\'logging_interval\'] == 0:\n                    summary = s.run(model.merged,\n                                    feed_dict={\n                                        model.x: batch_x,\n                                        model.z: batch_z,\n                                        model.m: margin,\n                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %03d Global Step %05d => "" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss))\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.z: sample_z,\n                                        model.m: margin,\n                                    })\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{:08d}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step)\n\n                global_step += 1\n\n            # Update margin\n            if s_d / n_steps < margin and s_d < s_g and s_g_0 <= s_g:\n                margin = s_d / n_steps\n                print(""[*] Margin updated from {:8f} to {:8f}"".format(old_margin, margin))\n                old_margin = margin\n\n            s_g_0 = s_g\n\n            # Convergence Measure\n            e_d = s_d / n_steps\n            e_g = s_g / n_steps\n            l_ = e_d + np.abs(e_d - e_g)\n\n            print(""[+] Epoch %03d "" % epoch, "" L : {:.8f}"".format(l_))\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
MRGAN/mrgan_model.py,45,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)\n\n\nclass MRGAN:\n\n    def __init__(self, s, batch_size=64, height=64, width=64, channel=3,\n                 sample_num=8 * 8, sample_size=8,\n                 z_dim=128, gf_dim=64, df_dim=64, lr=1e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: input image height, default 64\n        :param width: input image width, default 64\n        :param channel: input image channel, default 3 (RGB)\n        - in case of CelebA, image size is 64x64x3(HWC).\n\n        # Output Settings\n        :param sample_num: the number of sample images, default 64\n        :param sample_size: sample image size, default 8\n\n        # Model Settings\n        :param z_dim: z noise dimension, default 128\n        :param gf_dim: the number of generator filters, default 64\n        :param df_dim: the number of discriminator filters, default 64\n\n        # Training Settings\n        :param lr: learning rate, default 1e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n        self.channel = channel\n\n        self.sample_size = sample_size\n        self.sample_num = sample_num\n\n        self.image_shape = [self.height, self.width, self.channel]\n\n        self.z_dim = z_dim\n\n        self.gf_dim = gf_dim\n        self.df_dim = df_dim\n\n        self.lambda_1 = 0.2\n        self.lambda_2 = 0.4\n\n        # pre-defined\n        self.d_loss = 0.\n        self.e_loss = 0.\n        self.g_loss = 0.\n\n        self.g = None\n        self.g_reg = None\n\n        self.d_op = None\n        self.g_op = None\n        self.e_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel], name=\'x-images\')\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')\n\n        # Training Options\n        self.beta1 = 0.5\n        self.lr = lr\n\n        self.bulid_mrgan()  # build MRGAN model\n\n    def encoder(self, x, reuse=None):\n        with tf.variable_scope(\'encoder\', reuse=reuse):\n            x = t.conv2d(x, self.df_dim * 1, 5, 2, name=\'enc-conv2d-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 2, 5, 2, name=\'enc-conv2d-2\')\n            x = t.batch_norm(x, name=\'enc-bn-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 4, 5, 2, name=\'enc-conv2d-3\')\n            x = t.batch_norm(x, name=\'enc-bn-2\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 8, 5, 2, name=\'enc-conv2d-4\')\n            x = t.batch_norm(x, name=\'enc-bn-3\')\n            x = tf.nn.leaky_relu(x)\n\n            x = tf.layers.flatten(x)\n\n            x = t.dense(x, self.z_dim, name=\'enc-fc-1\')\n            return x\n\n    def discriminator(self, x, reuse=None):\n        with tf.variable_scope(\'discriminator\', reuse=reuse):\n            x = t.conv2d(x, self.df_dim * 1, 5, 2, name=\'disc-conv2d-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 2, 5, 2, name=\'disc-conv2d-2\')\n            x = t.batch_norm(x, name=\'disc-bn-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 4, 5, 2, name=\'disc-conv2d-3\')\n            x = t.batch_norm(x, name=\'disc-bn-2\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, self.df_dim * 8, 5, 2, name=\'disc-conv2d-4\')\n            x = t.batch_norm(x, name=\'disc-bn-3\')\n            x = tf.nn.leaky_relu(x)\n\n            x = tf.layers.flatten(x)\n\n            x = t.dense(x, 1, name=\'disc-fc-1\')\n            x = tf.nn.sigmoid(x)\n            return x\n\n    def generator(self, z, reuse=None, is_train=True):\n        with tf.variable_scope(\'generator\', reuse=reuse):\n            x = t.dense(z, self.gf_dim * 8 * 4 * 4, name=\'gen-fc-1\')\n\n            x = tf.reshape(x, [-1, 4, 4, self.gf_dim * 8])\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-1\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.gf_dim * 4, 5, 2, name=\'gen-deconv2d-1\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-2\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x,  self.gf_dim * 2, 5, 2, name=\'gen-deconv2d-2\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-3\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x,  self.gf_dim * 1, 5, 2, name=\'gen-deconv2d-3\')\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-4\')\n            x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.channel, 5, 2, name=\'gen-deconv2d-4\')\n            x = tf.nn.tanh(x)\n\n            return x\n\n    def bulid_mrgan(self):\n        # Generator\n        self.g = self.generator(self.z)\n        self.g_reg = self.generator(self.encoder(self.x), reuse=True)\n\n        # Discriminator\n        d_real = self.discriminator(self.x)\n        d_real_reg = self.discriminator(self.g_reg, reuse=True)\n        d_fake = self.discriminator(self.g, reuse=True)\n\n        # Losses\n        # Manifold Step\n        # d_loss_1 = tf.reduce_mean(t.safe_log(d_real) + t.safe_log(1. - d_real_reg))\n        # g_loss_1 = tf.reduce_mean(self.lambda_1 * t.safe_log(d_real_reg)) - \\\n        #            t.mse_loss(self.x, self.g_reg, self.batch_size)\n        # Diffusion Step\n        # d_loss_2 = tf.reduce_mean(t.safe_log(d_real_reg) + t.safe_log(1. - d_fake))\n        # g_loss_2 = tf.reduce_mean(t.safe_log(d_fake))\n\n        d_real_loss = -tf.reduce_mean(t.safe_log(d_real))\n        d_fake_loss = -tf.reduce_mean(t.safe_log(1. - d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n        e_mse_loss = self.lambda_1 * t.mse_loss(self.x, self.g_reg, self.batch_size, is_mean=True)\n        e_adv_loss = self.lambda_2 * tf.reduce_mean(t.safe_log(d_real_reg))\n        self.e_loss = e_adv_loss + e_mse_loss\n        self.g_loss = -tf.reduce_mean(t.safe_log(d_fake)) + self.e_loss\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/e_adv_loss"", e_adv_loss)\n        tf.summary.scalar(""loss/e_mse_loss"", e_mse_loss)\n        tf.summary.scalar(""loss/e_loss"", self.e_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Collect trainer values\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n        e_params = [v for v in t_vars if v.name.startswith(\'e\')]\n\n        # Optimizer\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1).minimize(self.g_loss, var_list=g_params)\n        self.e_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1).minimize(self.e_loss, var_list=e_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model Saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)'"
MRGAN/mrgan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport mrgan_model as mrgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CelebADataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/MRGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epoch\': 25,\n    \'batch_size\': 128,\n    \'logging_interval\': 400,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    height, width, channel = 64, 64, 3\n\n    # loading CelebA DataSet\n    ds = DataSet(height=height,\n                 width=width,\n                 channel=channel,\n                 ds_image_path=""D:\\\\DataSet/CelebA/CelebA-64.h5"",\n                 ds_label_path=""D:\\\\DataSet/CelebA/Anno/list_attr_celeba.txt"",\n                 # ds_image_path=""D:\\\\DataSet/CelebA/Img/img_align_celeba/"",\n                 ds_type=""CelebA"",\n                 use_save=False,\n                 save_file_name=""D:\\\\DataSet/CelebA/CelebA-64.h5"",\n                 save_type=""to_h5"",\n                 use_img_scale=False,\n                 # img_scale=""-1,1""\n                 )\n\n    # saving sample images\n    test_images = np.reshape(iu.transform(ds.images[:16], inv_type=\'127\'), (16, height, width, channel))\n    iu.save_images(test_images,\n                   size=[4, 4],\n                   image_path=results[\'output\'] + \'sample.png\',\n                   inv_type=\'127\')\n\n    ds_iter = DataIterator(x=ds.images,\n                           y=None,\n                           batch_size=train_step[\'batch_size\'],\n                           label_off=True)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # MRGAN model\n        model = mrgan.MRGAN(s, height=height, width=width, channel=channel,\n                            batch_size=train_step[\'batch_size\'])\n\n        # Initializing variables\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        global_step = saved_global_step\n        start_epoch = global_step // (ds.num_images // model.batch_size)           # recover n_epoch\n        ds_iter.pointer = saved_global_step % (ds.num_images // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epoch\']):\n            for batch_x in ds_iter.iterate():\n                batch_x = np.reshape(iu.transform(batch_x, inv_type=\'127\'),\n                                     (model.batch_size, model.height, model.width, model.channel))\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update D network\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z\n                                  })\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n                # Update E network\n                _, e_loss = s.run([model.e_op, model.e_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                if global_step % train_step[\'logging_interval\'] == 0:\n                    summary = s.run(model.merged,\n                                    feed_dict={\n                                        model.x: batch_x,\n                                        model.z: batch_z,\n                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %03d Step %05d => "" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss),\n                          "" E loss : {:.8f}"".format(e_loss))\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim])\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.z: sample_z,\n                                    })\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{0}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step)\n\n                global_step += 1\n\n        end_time = time.time() - start_time  # Clocking end\n\n        # Elapsed time\n        print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n        # Close tf.Session\n        s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
PGGAN/pggan_model.py,50,"b'import tensorflow as tf\nimport numpy as np\n\nimport tfutil as t\n\ntf.set_random_seed(777)  # reproducibility\nnp.random.seed(777)      # reproducibility\n\n\nhe_normal = tf.contrib.layers.variance_scaling_initializer(factor=1., mode=\'FAN_AVG\', uniform=True)\nl2_reg = tf.contrib.layers.l2_regularizer\n\n\ndef pixel_norm(x, eps=1e-8):\n    return x * tf.rsqrt(tf.reduce_mean(tf.square(x), axis=1, keepdims=True) + eps)\n\n\ndef resize_nn(x, size):\n    return tf.image.resize_nearest_neighbor(x, size=(int(size), int(size)))\n\n\ndef bacth_concat(x, eps=1e-8, averaging=\'all\'):\n    """"""\n    ref : https://github.com/zhangqianhui/progressive_growing_of_gans_tensorflow/blob/master/ops.py#L145\n    """"""\n    adj_std = lambda x_, **kwargs: tf.sqrt(tf.reduce_mean((x_ - tf.reduce_mean(x_, **kwargs)) ** 2, **kwargs) + eps)\n\n    val_ = adj_std(x, axis=0, keepdims=True)\n    if averaging == \'all\':\n        val_ = tf.reduce_mean(val_, keepdims=True)\n    val_ = tf.tile(val_, multiples=[tf.shape(x)[0], 4, 4, 1])\n    return tf.concat([x, val_], axis=3)\n\n\nclass PGGAN:\n\n    def __init__(self, s, batch_size=16, input_height=128, input_width=128, input_channel=3,\n                 pg=1, pg_t=False, sample_num=1 * 1, sample_size=1, output_height=128, output_width=128,\n                 df_dim=64, gf_dim=64, z_dim=512, lr=1e-4, epsilon=1e-9):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 16\n        :param input_height: input image height, default 128\n        :param input_width: input image width, default 128\n        :param input_channel: input image channel, default 3 (RGB)\n        - in case of Celeb-A, image size is 128x128x3(HWC).\n\n        # Output Settings\n        :param pg: size of the image for model?, default 1\n        :param pg_t: pg status, default False\n        :param sample_num: the number of output images, default 1\n        :param sample_size: sample image size, default 1\n        :param output_height: output images height, default 128\n        :param output_width: output images width, default 128\n\n        # For CNN model\n        :param df_dim: discriminator filter, default 64\n        :param gf_dim: generator filter, default 64\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 512\n        :param lr: learning rate, default 1e-4\n        :param epsilon: epsilon, default 1e-9\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.input_height = input_height\n        self.input_width = input_width\n        self.input_channel = input_channel\n        self.image_shape = [self.batch_size, self.input_height, self.input_width, self.input_channel]\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n        self.output_height = output_height\n        self.output_width = output_width\n\n        self.pg = pg\n        self.pg_t = pg_t\n        self.output_size = 4 * pow(2, self.pg - 1)\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n\n        self.z_dim = z_dim\n        self.beta1 = 0.\n        self.beta2 = .99\n        self.lr = lr\n        self.eps = epsilon\n\n        # pre-defined\n        self.d_real = 0.\n        self.d_fake = 0.\n        self.g_loss = 0.\n        self.d_loss = 0.\n        self.gp = 0.\n        self.gp_target = 1.\n        self.gp_lambda = 10.  # slower convergence but good\n        self.gp_w = 1e-3\n\n        self.g = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n        self.r_saver = None\n        self.out_saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32,\n                                shape=[None, self.output_size, self.output_size, self.input_channel],\n                                name=""x-image"")\n        self.z = tf.placeholder(tf.float32,\n                                shape=[None, self.z_dim],\n                                name=\'z-noise\')\n        self.step_pl = tf.placeholder(tf.float32, shape=None)\n        self.alpha_trans = tf.Variable(initial_value=0., trainable=False, name=\'alpha_trans\')\n        self.alpha_trans_update = None\n\n        self.build_pggan()  # build PGGAN model\n\n    def discriminator(self, x, pg=1, pg_t=False, reuse=None):\n        def nf(n):\n            return min(1024 // (2 ** n), self.z_dim)\n\n        with tf.variable_scope(""disc"", reuse=reuse):\n            if pg_t:\n                x_out = tf.layers.average_pooling2d(x, pool_size=2, strides=2)\n                x_out = t.conv2d(x_out, nf(pg - 2), k=1, s=1, name=\'disc_out_conv2d-%d\' % x_out.get_shape()[1])\n                x_out = tf.nn.leaky_relu(x_out)\n\n            x = t.conv2d(x, nf(pg - 1), k=1, s=1, name=\'disc_out_conv2d-%d\' % x.get_shape()[1])\n            x = tf.nn.leaky_relu(x)\n\n            for i in range(pg - 1):\n                x = t.conv2d(x, nf(pg - 1 - i), k=1, s=1, name=\'disc_n_1_conv2d-%d\' % x.get_shape()[1])\n                x = tf.nn.leaky_relu(x)\n\n                x = t.conv2d(x, nf(pg - 2 - i), k=1, s=1, name=\'disc_n_2_conv2d-%d\' % x.get_shape()[1])\n                x = tf.nn.leaky_relu(x)\n\n                x = tf.layers.average_pooling2d(x, pool_size=2, strides=2)\n\n                if i == 0 and pg_t:\n                    x = (1. - self.alpha_trans) * x_out + self.alpha_trans * x\n\n            x = bacth_concat(x)\n\n            x = t.conv2d(x, nf(1), k=3, s=1, name=\'disc_n_1_conv2d-%d\' % x.get_shape()[1])\n            x = tf.nn.leaky_relu(x)\n\n            x = t.conv2d(x, nf(1), k=4, s=1, pad=\'VALID\', name=\'disc_n_2_conv2d-%d\' % x.get_shape()[1])\n            x = tf.nn.leaky_relu(x)\n\n            x = tf.layers.flatten(x)\n\n            x = tf.layers.dense(x, 1, name=\'disc_n_fc\')\n\n            return x\n\n    def generator(self, z, pg=1, pg_t=False, reuse=None):\n        def nf(n):\n            return min(1024 // (2 ** n), self.z_dim)\n\n        def block(x, fs, name=""0""):\n            x = resize_nn(x, x.get_shape()[1] * 2)\n            x = t.conv2d(x, fs, k=3, s=1, name=\'gen_n_%s_conv2d-%d\' % (name, x.get_shape()[1]))\n            x = tf.nn.leaky_relu(x)\n            x = pixel_norm(x)\n            return x\n\n        with tf.variable_scope(""gen"", reuse=reuse):\n            x = tf.reshape(z, [-1, 1, 1, nf(1)])\n            x = t.conv2d(x, nf(1), k=4, s=1, name=\'gen_n_1_conv2d\')\n            x = tf.nn.leaky_relu(x)\n            x = pixel_norm(x)\n\n            x = tf.reshape(x, [-1, 4, 4, nf(1)])\n            x = t.conv2d(x, nf(1), k=3, s=1, name=\'gen_n_2_conv2d\')\n            x = tf.nn.leaky_relu(x)\n            x = pixel_norm(x)\n\n            x_out = None\n            for i in range(pg - 1):\n                if i == pg - 2 and pg_t:\n                    x_out = t.conv2d(x, 3, k=1, s=1, name=\'gen_out_conv2d-%d\' % x.get_shape()[1])  # to RGB images\n                    x_out = resize_nn(x_out, x_out.get_shape()[1] * 2)                           # up-sampling\n\n                x = block(x, nf(i + 1), name=""1"")\n                x = block(x, nf(i + 1), name=""2"")\n\n            x = t.conv2d(x, 3, k=1, s=1, name=\'gen_out_conv2d-%d\' % x.get_shape()[1])  # to RGB images\n\n            if pg == 1:\n                return x\n\n            if pg_t:\n                x = (1. - self.alpha_trans) * x_out + self.alpha_trans * x\n\n            return x\n\n    def build_pggan(self):\n        self.alpha_trans_update = self.alpha_trans.assign(self.step_pl / 32000)\n\n        # Generator\n        self.g = self.generator(self.z, self.pg, self.pg_t)\n\n        # Discriminator\n        d_real = self.discriminator(self.x, self.pg, self.pg_t)\n        d_fake = self.discriminator(self.g, self.pg, self.pg_t, reuse=True)\n\n        # Loss ()\n        d_real_loss = tf.reduce_mean(d_real)\n        d_fake_loss = tf.reduce_mean(d_fake)\n        self.d_loss = d_real_loss - d_fake_loss\n        self.g_loss = d_fake_loss\n\n        # Gradient Penalty\n        diff = self.g - self.x\n        alpha = tf.random_uniform(shape=[self.batch_size, 1, 1, 1], minval=0., maxval=1.)\n        interp = self.x + (alpha * diff)\n        d_interp = self.discriminator(interp, self.pg, self.pg_t, reuse=True)\n        grads = tf.gradients(d_interp, [interp])[0]\n        slopes = tf.sqrt(tf.reduce_sum(tf.square(grads), reduction_indices=[1, 2, 3]))\n        self.gp = tf.reduce_mean(tf.square(slopes - self.gp_target))\n\n        self.d_loss += (self.gp_lambda / (self.gp_target ** 2)) * self.gp + \\\n            self.gp_w * tf.reduce_mean(tf.square(d_real - 0.))\n\n        # Summary\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n        tf.summary.scalar(""misc/gp"", self.gp)\n\n        # Training Parameters\n        t_vars = tf.trainable_variables()\n\n        d_params = [v for v in t_vars if v.name.startswith(\'disc\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'gen\')]\n\n        d_n_params = [v for v in d_params if \'disc_n\' in v.name]\n        g_n_params = [v for v in g_params if \'gen_n\' in v.name]\n\n        d_n_out_params = [v for v in d_params if \'disc_out\' in v.name]\n        g_n_out_params = [v for v in g_params if \'gen_out\' in v.name]\n\n        d_n_nwm_params = [v for v in d_n_params if \'%d\' % self.output_size not in v.name]  # nwm : not new model\n        g_n_nwm_params = [v for v in g_n_params if \'%d\' % self.output_size not in v.name]  # nwm : not new model\n\n        d_n_out_nwm_params = [v for v in d_n_out_params if \'%d\' % self.output_size not in v.name]\n        g_n_out_nwm_params = [v for v in g_n_out_params if \'%d\' % self.output_size not in v.name]\n\n        # Optimizer\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        mtk = 2\n        self.saver = tf.train.Saver(var_list=d_params + g_params,\n                                    max_to_keep=mtk, name=\'saver\')\n        self.r_saver = tf.train.Saver(var_list=d_n_nwm_params + g_n_nwm_params,\n                                      max_to_keep=mtk, name=\'r_saver\')\n        if len(d_n_out_nwm_params + g_n_out_nwm_params):\n            self.out_saver = tf.train.Saver(var_list=d_n_out_nwm_params + g_n_out_nwm_params,\n                                            max_to_keep=mtk, name=\'out_saver\')\n\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
PGGAN/pggan_train.py,4,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\nfrom scipy.ndimage import zoom\nfrom skimage.transform import resize\n\nimport sys\nimport time\nimport random\n\nimport pggan_model as pggan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CelebADataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'checkpoint\': \'./model/checkpoint-\',\n    \'model\': \'./model/PGGAN-model-\'\n}\n\ntrain_step = {\n    \'epoch\': 10000,\n    \'batch_size\': 16,\n    \'logging_step\': 1000,\n}\n\npg = [1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6]\nassert len(pg) == 11\n\nr_pg = [1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6]\nassert len(r_pg) == 11\n\n\ndef image_resize(x, s=128):\n    imgs = []\n    for i in range(x.shape[0]):\n        imgs.append(resize(x[i, :, :, :], output_shape=(s, s), preserve_range=True))\n    return np.asarray(imgs)\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # Celeb-A DataSet images\n    ds = DataSet(input_height=1024,\n                 input_width=1024,\n                 input_channel=3,\n                 ds_type=""CelebA-HQ"",\n                 ds_path=""/home/zero/hdd/DataSet/CelebA-HQ"").images\n    n_ds = 30000\n    dataset_iter = DataIterator(ds, None, train_step[\'batch_size\'],\n                                label_off=True)\n\n    rnd = random.randint(0, n_ds)\n    sample_x = ds[rnd]\n    sample_x = np.reshape(sample_x, [-1, 1024, 1024, 3])\n\n    # Export real image\n    valid_image_height = 1\n    valid_image_width = 1\n    sample_dir = results[\'output\'] + \'valid.png\'\n\n    # Generated image save\n    iu.save_images(sample_x, size=[valid_image_height, valid_image_width], image_path=sample_dir,\n                   inv_type=\'127\')\n    print(""[+] sample image saved!"")\n\n    print(""[+] pre-processing took {:.8f}s"".format(time.time() - start_time))\n\n    # GPU configure\n    gpu_config = tf.GPUOptions(allow_growth=True)\n    config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_config)\n\n    for idx, n_pg in enumerate(pg):\n\n        with tf.Session(config=config) as s:\n            pg_t = False if idx % 2 == 0 else True\n\n            # PGGAN Model\n            model = pggan.PGGAN(s, pg=n_pg, pg_t=pg_t)  # PGGAN\n\n            # Initializing\n            s.run(tf.global_variables_initializer())\n\n            if not n_pg == 1 and not n_pg == 7:\n                if pg_t:\n                    model.r_saver.restore(s, results[\'model\'] + \'%d-%d.ckpt\' % (idx, r_pg[idx]))\n                    model.out_saver.restore(s, results[\'model\'] + \'%d-%d.ckpt\' % (idx, r_pg[idx]))\n                else:\n                    model.saver.restore(s, results[\'model\'] + \'%d-%d.ckpt\' % (idx, r_pg[idx]))\n\n            global_step = 0\n            for epoch in range(train_step[\'epoch\']):\n                # Later, adding n_critic for optimizing D net\n                for batch_images in dataset_iter.iterate():\n                    batch_x = np.reshape(batch_images, (-1, 128, 128, 3))\n                    batch_x = (batch_x + 1.) * 127.5  # re-scaling to (0, 255)\n                    batch_x = image_resize(batch_x, s=model.output_size)\n                    batch_x = (batch_x / 127.5) - 1.  # re-scaling to (-1, 1)\n                    batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                    if pg_t and not pg == 0:\n                        alpha = global_step / 32000.\n                        low_batch_x = zoom(batch_x, zoom=[1., .5, .5, 1.])\n                        low_batch_x = zoom(low_batch_x, zoom=[1., 2., 2., 1.])\n                        batch_x = alpha * batch_x + (1. - alpha) * low_batch_x\n\n                    # Update D network\n                    _, d_loss = s.run([model.d_op, model.d_loss],\n                                      feed_dict={\n                                          model.x: batch_x,\n                                          model.z: batch_z,\n                                      })\n\n                    # Update G network\n                    _, g_loss = s.run([model.g_op, model.g_loss],\n                                      feed_dict={\n                                          model.z: batch_z,\n                                      })\n\n                    # Update alpha_trans\n                    s.run(model.alpha_trans_update,\n                          feed_dict={\n                              model.step_pl: global_step\n                          })\n\n                    if global_step % train_step[\'logging_step\'] == 0:\n                        gp, d_loss, g_loss, summary = s.run([model.gp,\n                                                             model.d_loss, model.g_loss, model.merged],\n                                                            feed_dict={\n                                                                model.x: batch_x,\n                                                                model.z: batch_z,\n                                                            })\n\n                        # Print loss\n                        print(""[+] PG %d Epoch %03d Step %07d =>"" % (n_pg, epoch, global_step),\n                              "" D loss : {:.6f}"".format(d_loss),\n                              "" G loss : {:.6f}"".format(g_loss),\n                              "" GP     : {:.6f}"".format(gp),\n                              )\n\n                        # Summary saver\n                        model.writer.add_summary(summary, global_step)\n\n                        # Training G model with sample image and noise\n                        sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n\n                        samples = s.run(model.g,\n                                        feed_dict={\n                                            model.z: sample_z,\n                                        })\n                        samples = np.clip(samples, -1, 1)\n\n                        # Export image generated by model G\n                        sample_image_height = 1\n                        sample_image_width = 1\n                        sample_dir = results[\'output\'] + \'train_{0}.png\'.format(global_step)\n\n                        # Generated image save\n                        iu.save_images(samples,\n                                       size=[sample_image_height, sample_image_width],\n                                       image_path=sample_dir,\n                                       inv_type=\'127\')\n\n                        # Model save\n                        model.saver.save(s, results[\'model\'] + \'%d-%d.ckpt\' % (idx, n_pg), global_step=global_step)\n\n                    global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
SAGAN/sagan_model.py,41,"b'import tensorflow as tf\nimport numpy as np\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\nfrom config import get_config\n\ncfg, _ = get_config()\n\nnp.random.seed(cfg.seed)\ntf.set_random_seed(cfg.seed)  # reproducibility\n\n\nclass SAGAN:\n\n    def __init__(self, s, batch_size=64, height=64, width=64, channel=3, n_classes=10,\n                 sample_num=10 * 10, sample_size=10,\n                 df_dim=64, gf_dim=64, z_dim=128, lr=1e-4,\n                 use_gp=False, use_hinge_loss=True,\n                 graph_path=""./model""):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: image height, default 64\n        :param width: image width, default 64\n        :param channel: image channel, default 3\n        :param n_classes: DataSet\'s classes, default 10\n\n        # Output Settings\n        :param sample_num: the number of output images, default 100\n        :param sample_size: sample image size, default 10\n\n        # For Model\n        :param df_dim: discriminator conv filter, default 64\n        :param gf_dim: generator conv filter, default 64\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 128\n        :param lr: learning rate, default 1e-4\n        :param use_gp: using gradient penalty, default False\n        :param use_hinge_loss: using hinge loss, default True\n\n        # Etc\n        :param graph_path: path to save graph file, default ""./model""\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.image_shape = [self.batch_size, self.height, self.width, self.channel]\n        self.n_classes = n_classes\n\n        self.n_layer = int(np.log2(self.height)) - 2  # 5\n        assert self.height == self.width\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n\n        self.up_sampling = True\n\n        self.z_dim = z_dim\n        self.beta1 = 0.\n        self.beta2 = .9\n        self.lr = lr\n\n        self.gp = 0.\n        self.lambda_ = 10.  # for gradient penalty\n\n        self.graph_path = graph_path\n\n        # pre-defined\n        self.g_loss = 0.\n        self.d_loss = 0.\n        self.c_loss = 0.\n\n        self.g = None\n        self.g_test = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        self.use_gp = use_gp\n        self.use_hinge_loss = use_hinge_loss\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32,\n                                shape=[self.batch_size, self.height, self.width, self.channel],\n                                name=""x-image"")  # (64, 64, 64, 3)\n        self.z = tf.placeholder(tf.float32, shape=[self.batch_size, self.z_dim], name=""z-noise"")  # (-1, 128)\n        self.z_test = tf.placeholder(tf.float32, shape=[self.sample_num, self.z_dim], name=""z-test-noise"")  # (-1, 128)\n\n        self.build_sagan()  # build SAGAN model\n\n    @staticmethod\n    def attention(x, f_, reuse=None):\n        with tf.variable_scope(""attention"", reuse=reuse):\n            f = t.conv2d_alt(x, f_ // 8, 1, 1, sn=True, name=\'attention-conv2d-f\')\n            g = t.conv2d_alt(x, f_ // 8, 1, 1, sn=True, name=\'attention-conv2d-g\')\n            h = t.conv2d_alt(x, f_, 1, 1, sn=True, name=\'attention-conv2d-h\')\n\n            f, g, h = t.hw_flatten(f), t.hw_flatten(g), t.hw_flatten(h)\n\n            s = tf.matmul(g, f, transpose_b=True)\n            attention_map = tf.nn.softmax(s, axis=-1, name=\'attention_map\')\n\n            o = tf.reshape(tf.matmul(attention_map, h), shape=x.get_shape())\n            gamma = tf.get_variable(\'gamma\', shape=[1], initializer=tf.zeros_initializer())\n\n            x = gamma * o + x\n            return x\n\n    def discriminator(self, x, reuse=None):\n        """"""\n        :param x: images\n        :param y: labels\n        :param reuse: re-usable\n        :return: classification, probability (fake or real), network\n        """"""\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            f = self.gf_dim\n\n            x = t.conv2d_alt(x, f, 4, 2, pad=1, sn=True, name=\'disc-conv2d-1\')\n            x = tf.nn.leaky_relu(x, alpha=0.1)\n\n            for i in range(self.n_layer // 2):\n                x = t.conv2d_alt(x, f * 2, 4, 2, pad=1, sn=True, name=\'disc-conv2d-%d\' % (i + 2))\n                x = tf.nn.leaky_relu(x, alpha=0.1)\n\n                f *= 2\n\n            # Self-Attention Layer\n            x = self.attention(x, f, reuse=reuse)\n\n            for i in range(self.n_layer // 2, self.n_layer):\n                x = t.conv2d_alt(x, f * 2, 4, 2, pad=1, sn=True, name=\'disc-conv2d-%d\' % (i + 2))\n                x = tf.nn.leaky_relu(x, alpha=0.1)\n\n                f *= 2\n\n            x = t.flatten(x)\n\n            x = t.dense_alt(x, 1, sn=True, name=\'disc-fc-1\')\n            return x\n\n    def generator(self, z, reuse=None, is_train=True):\n        """"""\n        :param z: noise\n        :param y: image label\n        :param reuse: re-usable\n        :param is_train: trainable\n        :return: prob\n        """"""\n        with tf.variable_scope(""generator"", reuse=reuse):\n            f = self.gf_dim * 8\n\n            x = t.dense_alt(z, 4 * 4 * f, sn=True, name=\'gen-fc-1\')\n\n            x = tf.reshape(x, (-1, 4, 4, f))\n\n            for i in range(self.n_layer // 2):\n                if self.up_sampling:\n                    x = t.up_sampling(x, interp=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                    x = t.conv2d_alt(x, f // 2, 5, 1, pad=2, sn=True, use_bias=False, name=\'gen-conv2d-%d\' % (i + 1))\n                else:\n                    x = t.deconv2d_alt(x, f // 2, 4, 2, sn=True, use_bias=False, name=\'gen-deconv2d-%d\' % (i + 1))\n\n                x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-%d\' % i)\n                x = tf.nn.relu(x)\n\n                f //= 2\n\n            # Self-Attention Layer\n            x = self.attention(x, f, reuse=reuse)\n\n            for i in range(self.n_layer // 2, self.n_layer):\n                if self.up_sampling:\n                    x = t.up_sampling(x, interp=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                    x = t.conv2d_alt(x, f // 2, 5, 1, pad=2, sn=True, use_bias=False, name=\'gen-conv2d-%d\' % (i + 1))\n                else:\n                    x = t.deconv2d_alt(x, f // 2, 4, 2, sn=True, use_bias=False, name=\'gen-deconv2d-%d\' % (i + 1))\n\n                x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-%d\' % i)\n                x = tf.nn.relu(x)\n\n                f //= 2\n\n            x = t.conv2d_alt(x, self.channel, 5, 1, pad=2, sn=True, name=\'gen-conv2d-%d\' % (self.n_layer + 1))\n            x = tf.nn.tanh(x)\n            return x\n\n    def build_sagan(self):\n        # Generator\n        self.g = self.generator(self.z)\n        self.g_test = self.generator(self.z_test, reuse=True)\n\n        # Discriminator\n        d_real = self.discriminator(self.x)\n        d_fake = self.discriminator(self.g, reuse=True)\n\n        # Losses\n        if self.use_hinge_loss:\n            d_real_loss = tf.reduce_mean(tf.nn.relu(1. - d_real))\n            d_fake_loss = tf.reduce_mean(tf.nn.relu(1. + d_fake))\n            self.d_loss = d_real_loss + d_fake_loss\n            self.g_loss = -tf.reduce_mean(d_fake)\n        else:\n            d_real_loss = t.sce_loss(d_real, tf.ones_like(d_real))\n            d_fake_loss = t.sce_loss(d_fake, tf.zeros_like(d_fake))\n            self.d_loss = d_real_loss + d_fake_loss\n            self.g_loss = t.sce_loss(d_fake, tf.ones_like(d_fake))\n\n        # gradient-penalty\n        if self.use_gp:\n            alpha = tf.random_uniform(shape=[self.batch_size, 1, 1, 1], minval=0., maxval=1., name=\'alpha\')\n            interp = alpha * self.x + (1. - alpha) * self.g\n            d_interp = self.discriminator(interp, reuse=True)\n            gradients = tf.gradients(d_interp, interp)[0]\n            slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=1))\n            self.gp = tf.reduce_mean(tf.square(slopes - 1.))\n\n            # Update D loss\n            self.d_loss += self.lambda_ * self.gp\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n        if self.use_gp:\n            tf.summary.scalar(""misc/gp"", self.gp)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(self.lr * 4,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(self.lr * 1,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(self.graph_path, self.s.graph)\n'"
SAGAN/sagan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport os\nimport sys\nimport time\n\nimport sagan_model as sagan\n\nsys.path.append(\'../\')\nimport tfutil as t\nimport image_utils as iu\n\nfrom config import get_config\nfrom datasets import DataIterator\nfrom datasets import CelebADataSet as DataSet\n\n\ncfg, _ = get_config()\n\n\ntrain_step = {\n    \'epochs\': 11,\n    \'batch_size\': 64,\n    \'global_step\': 10001,\n    \'logging_interval\': 500,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    height, width, channel = 128, 128, 3\n\n    # loading CelebA DataSet # from \'raw images\' or \'h5\'\n    use_h5 = True\n    if not use_h5:\n        ds = DataSet(height=height,\n                     width=height,\n                     channel=channel,\n                     # ds_image_path=""D:\\\\DataSet/CelebA/CelebA-%d.h5"" % height,\n                     ds_label_path=os.path.join(cfg.celeba, ""Anno/list_attr_celeba.txt""),\n                     ds_image_path=os.path.join(cfg.celeba, ""Img/img_align_celeba/""),\n                     ds_type=""CelebA"",\n                     use_save=True,\n                     save_file_name=os.path.join(cfg.celeba, ""CelebA-%d.h5"" % height),\n                     save_type=""to_h5"",\n                     use_img_scale=False,\n                     )\n    else:\n        ds = DataSet(height=height,\n                     width=height,\n                     channel=channel,\n                     ds_image_path=os.path.join(cfg.celeba, ""CelebA-%d.h5"" % height),\n                     ds_label_path=os.path.join(cfg.celeba, ""Anno/list_attr_celeba.txt""),\n                     # ds_image_path=os.path.join(cfg.celeba, ""Img/img_align_celeba/""),\n                     ds_type=""CelebA"",\n                     use_save=False,\n                     # save_file_name=os.path.join(cfg.celeba, ""CelebA-%d.h5"" % height),\n                     # save_type=""to_h5"",\n                     use_img_scale=False,\n                     )\n\n    num_images = ds.num_images\n\n    # saving sample images\n    test_images = np.reshape(iu.transform(ds.images[:16], inv_type=\'127\'), (16, height, width, channel))\n    iu.save_images(test_images,\n                   size=[4, 4],\n                   image_path=os.path.join(cfg.output, ""sample.png""),\n                   inv_type=\'127\')\n\n    ds_iter = DataIterator(x=ds.images,\n                           y=None,\n                           batch_size=train_step[\'batch_size\'],\n                           label_off=True)\n\n    del ds\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # SAGAN Model\n        model = sagan.SAGAN(s,\n                            height=height, width=width, channel=channel,\n                            batch_size=train_step[\'batch_size\'],\n                            use_gp=False,\n                            use_hinge_loss=True\n                            )\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        print(""[*] Reading checkpoints..."")\n\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(cfg.model_path)\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        global_step = saved_global_step\n        start_epoch = global_step // (num_images // model.batch_size)           # recover n_epoch\n        ds_iter.pointer = saved_global_step % (num_images // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epochs\']):\n            for batch_x in ds_iter.iterate():\n                batch_x = iu.transform(batch_x, inv_type=\'127\')\n                batch_x = np.reshape(batch_x, (model.batch_size, model.height, model.width, model.channel))\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update D network\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                if global_step % train_step[\'logging_interval\'] == 0:\n                    summary = s.run(model.merged,\n                                    feed_dict={\n                                        model.x: batch_x,\n                                        model.z: batch_z,\n                                    })\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                    samples = s.run(model.g_test,\n                                    feed_dict={\n                                        model.z_test: sample_z,\n                                    })\n\n                    # is_mean, is_std = t.inception_score(iu.inverse_transform(samples, inv_type=\'127\'))\n                    # fid_score = t.fid_score(real_img=batch_x, fake_img=samples[:model.batch_size])\n\n                    # Print loss\n                    print(""[+] Epoch %04d Step %08d => "" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss),\n                          # "" Inception Score : {:.2f} (\xc2\xb1{:.2f})"".format(is_mean, is_std),\n                          # "" FID Score : {:.2f}"".format(fid_score)\n                          )\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = os.path.join(cfg.output, \'train_{:08d}.png\'.format(global_step))\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, os.path.join(cfg.model_path, ""SAGAN.ckpt""), global_step)\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
SEGAN/ops.py,24,"b'import tensorflow as tf\n\n\nclass VBN(object):\n    """"""\n    Virtual Batch Normalization\n    (modified from https://github.com/openai/improved-gan/ definition)\n    """"""\n\n    def __init__(self, x, name, epsilon=1e-5):\n        """"""\n        x is the reference batch\n        """"""\n        assert isinstance(epsilon, float)\n\n        shape = x.get_shape().as_list()\n        assert len(shape) == 3, shape\n        with tf.variable_scope(name):\n            assert name.startswith(""d"") or name.startswith(""g"")\n            self.epsilon = epsilon\n            self.name = name\n            self.mean = tf.reduce_mean(x, [0, 1], keep_dims=True)\n            self.mean_sq = tf.reduce_mean(tf.square(x), [0, 1], keep_dims=True)\n            self.batch_size = int(x.get_shape()[0])\n\n            assert x is not None\n            assert self.mean is not None\n            assert self.mean_sq is not None\n\n            out = self._normalize(x, self.mean, self.mean_sq, ""reference"")\n            self.reference_output = out\n\n    def __call__(self, x):\n        with tf.variable_scope(self.name):\n            new_coeff = 1. / (self.batch_size + 1.)\n            old_coeff = 1. - new_coeff\n            new_mean = tf.reduce_mean(x, [0, 1], keep_dims=True)\n            new_mean_sq = tf.reduce_mean(tf.square(x), [0, 1], keep_dims=True)\n            mean = new_coeff * new_mean + old_coeff * self.mean\n            mean_sq = new_coeff * new_mean_sq + old_coeff * self.mean_sq\n            out = self._normalize(x, mean, mean_sq, ""live"")\n\n            return out\n\n    def _normalize(self, x, mean, mean_sq, message):\n        # make sure this is called with a variable scope\n        shape = x.get_shape().as_list()\n        assert len(shape) == 3\n        self.gamma = tf.get_variable(""gamma"", [shape[-1]],\n                                     initializer=tf.random_normal_initializer(1., 0.02))\n        gamma = tf.reshape(self.gamma, [1, 1, -1])\n        self.beta = tf.get_variable(""beta"", [shape[-1]],\n                                    initializer=tf.constant_initializer(0.))\n        beta = tf.reshape(self.beta, [1, 1, -1])\n\n        assert self.epsilon is not None\n        assert mean_sq is not None\n        assert mean is not None\n\n        std = tf.sqrt(self.epsilon + mean_sq - tf.square(mean))\n\n        out = x - mean\n        out = out / std\n        out = out * gamma\n        out = out + beta\n\n        return out\n\n\ndef gaussian_noise_layer(input_layer, std=.5):\n    noise = tf.random_normal(shape=input_layer.get_shape().as_list(),\n                             mean=.0, stddev=std,\n                             dtype=tf.float32)\n    return input_layer + noise\n\n\ndef conv1d(x, f=64, k=1, s=1, reuse=False, bias=False, pad=\'SAME\', name=\'conv1d\'):\n    """"""\n    :param x: input\n    :param f: filters, default 64\n    :param k: kernel size, default 1\n    :param s: strides, default 1\n    :param reuse: param re-usability, default False\n    :param bias: using bias, default False\n    :param pad: padding (valid or same), default same\n    :param name: scope name, default conv2d\n    :return: covn2d net\n    """"""\n    return tf.layers.conv1d(x,\n                            filters=f, kernel_size=k, strides=s,\n                            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n                            kernel_regularizer=tf.contrib.layers.l2_regularizer(5e-4),\n                            use_bias=bias,\n                            padding=pad,\n                            reuse=reuse,\n                            name=name)\n\n\ndef conv2d(x, f=64, k=5, s=2, reuse=False, bias=False, pad=\'SAME\', name=\'conv2d\'):\n    """"""\n    :param x: input\n    :param f: filters, default 64\n    :param k: kernel size, default 3\n    :param s: strides, default 2\n    :param reuse: param re-usability, default False\n    :param bias: using bias, default False\n    :param pad: padding (valid or same), default same\n    :param name: scope name, default conv2d\n    :return: covn2d net\n    """"""\n    return tf.layers.conv2d(x,\n                            filters=f, kernel_size=k, strides=s,\n                            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n                            kernel_regularizer=tf.contrib.layers.l2_regularizer(5e-4),\n                            use_bias=bias,\n                            padding=pad,\n                            reuse=reuse,\n                            name=name)\n\n\ndef deconv2d(x, f=64, k=5, s=2, reuse=False, bias=False, pad=\'SAME\', name=\'deconv2d\'):\n    """"""\n    :param x: input\n    :param f: filters, default 64\n    :param k: kernel size, default 3\n    :param s: strides, default 2\n    :param reuse: param re-usability, default False\n    :param bias: using bias, default False\n    :param pad: padding (valid or same), default same\n    :param name: scope name, default deconv2d\n    :return: decovn2d net\n    """"""\n    return tf.layers.conv2d_transpose(x,\n                                      filters=f, kernel_size=k, strides=s,\n                                      kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(5e-4),\n                                      use_bias=bias,\n                                      padding=pad,\n                                      reuse=reuse,\n                                      name=name)\n'"
SEGAN/segan_model.py,21,"b'import tensorflow as tf\nimport ops\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass SEGAN:\n\n    def __init__(self, s, batch_size=64, input_height=28, input_width=28, input_channel=1, n_classes=10,\n                 sample_num=8 * 8, sample_size=8, output_height=28, output_width=28,\n                 n_input=784, fc_unit=1024, df_dim=64, gf_dim=64,\n                 z_dim=128, g_lr=2e-4, d_lr=2e-4, epsilon=1e-12):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param input_height: input image height, default 28\n        :param input_width: input image width, default 28\n        :param input_channel: input image channel, default 1 (gray-scale)\n\n        # Output Settings\n        :param sample_num: the number of output images, default 64\n        :param sample_size: sample image size, default 8\n        :param output_height: output images height, default 28\n        :param output_width: output images width, default 28\n\n        # For DNN model\n        :param n_input: input image size, default 784(28x28)\n        :param fc_unit: fully connected units, default 1024\n        :param df_dim: the number of disc filters, default 64\n        :param gf_dim: the number of gen filters, default 64\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 128\n        :param g_lr: generator learning rate, default 2e-4\n        :param d_lr: discriminator learning rate, default 2e-4\n        :param epsilon: epsilon, default 1e-12\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.input_height = input_height\n        self.input_width = input_width\n        self.input_channel = input_channel\n        self.image_shape = [self.batch_size, self.input_height, self.input_width, self.input_channel]\n        self.n_classes = n_classes\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n        self.output_height = output_height\n        self.output_width = output_width\n\n        self.n_input = n_input\n        self.fc_unit = fc_unit\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n\n        self.z_dim = z_dim\n        self.beta1 = .5\n        self.beta2 = .999\n        self.d_lr, self.g_lr = d_lr, g_lr\n        self.eps = epsilon\n\n        # pre-defined\n        self.d_loss = 0.\n        self.d_1_loss = 0.\n        self.d_2_loss = 0.\n        self.g_loss = 0.\n        self.g_1_loss = 0.\n        self.g_2_loss = 0.\n\n        self.g = None\n        self.g_sample = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholder\n        self.x = tf.placeholder(tf.float32, shape=[self.batch_size,\n                                                   self.input_height, self.input_width, self.input_channel],\n                                name=""x-sound"")\n        self.z = tf.placeholder(tf.float32, shape=[self.batch_size, self.z_dim],\n                                name=\'z-noise\')\n\n        # ops\n        self.ops = ops.VBN()\n\n        self.num_blocks = [16, 32, 32, 64, 64, 128, 128, 256, 256, 512, 1024]\n\n        self.build_segan()  # build SEGAN model\n\n    def discriminator(self, x, reuse=False):\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            def residual_block(x, name=\'residual_block\'):\n                x = ops.conv2d(x)\n                x = self.ops(x)\n                x = tf.nn.leaky_relu(x)\n                return x\n\n            if len(x) == 2:\n                x = tf.expand_dims(x, axis=-1)\n            else:\n                raise ValueError(""[-] disc: waveform must be 2, 3-D"")\n\n            for idx, f in enumerate(self.num_blocks):\n                x = residual_block(x)\n\n\n            return x\n\n    def generator(self, z, reuse=False, training=True):\n        with tf.variable_scope(""generator"", reuse=reuse):\n\n            return z\n\n    def build_segan(self):\n        def sce_loss(x, y):\n            return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y))\n\n        # Generator\n        self.g = self.generator(self.z)\n        self.g_sample = self.generator(self.z, reuse=True, training=False)\n\n        # Discriminator\n        d_real = self.discriminator(self.x, reuse=False)\n        d_fake = self.discriminator(self.g, reuse=True)\n\n        # Losses\n        d_real_loss = sce_loss(d_real, tf.ones_like(d_real))\n        d_fake_loss = sce_loss(d_fake, tf.zeros_like(d_fake))\n        self.d_1_loss = d_real_loss + d_fake_loss\n        self.d_2_loss = d_real_loss + d_fake_loss\n        self.d_loss = self.d_1_loss + self.d_2_loss\n\n        self.g_loss = sce_loss(d_fake, tf.ones_like(d_fake))\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.d_lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.g_lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
SEGAN/segan_train.py,4,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport segan_model as segan\n\nsys.path.append(\'../\')\nimport image_utils as iu\n# from datasets import UrbanSoundDataSet as DataSet\nfrom datasets import MNISTDataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'checkpoint\': \'./model/checkpoint\',\n    \'model\': \'./model/SEGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'global_step\': 150001,\n    \'logging_interval\': 1500,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # UrbanSound8K Dataset load\n    mnist = MNISTDataSet().data\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # CoGAN Model\n        model = segan.SEGAN(s)\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        sample_x, _ = mnist.test.next_batch(model.sample_num)\n        sample_y = np.zeros(shape=[model.sample_num, model.n_classes])\n        for i in range(10):\n            sample_y[10 * i:10 * (i + 1), i] = 1\n\n        for step in range(train_step[\'global_step\']):\n            batch_x, batch_y = mnist.train.next_batch(model.batch_size)\n            batch_x = np.reshape(batch_x, model.image_shape)\n            batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n            # Update D network\n            _, d_loss = s.run([model.d_op, model.d_loss],\n                              feed_dict={\n                                  model.x_1: batch_x,\n                                  model.x_2: batch_x,\n                                  # model.y: batch_y,\n                                  model.z: batch_z,\n                              })\n\n            # Update G network\n            _, g_loss = s.run([model.g_op, model.g_loss],\n                              feed_dict={\n                                  model.x_1: batch_x,\n                                  model.x_2: batch_x,\n                                  # model.y: batch_y,\n                                  model.z: batch_z,\n                              })\n\n            if step % train_step[\'logging_interval\'] == 0:\n                batch_x, batch_y = mnist.train.next_batch(model.batch_size)\n                batch_x = np.reshape(batch_x, model.image_shape)\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                d_loss, g_loss, summary = s.run([model.d_loss, model.g_loss, model.merged],\n                                                feed_dict={\n                                                    model.x_1: batch_x,\n                                                    model.x_2: batch_x,\n                                                    # model.y: batch_y,\n                                                    model.z: batch_z,\n                                                })\n\n                # Print loss\n                print(""[+] Step %08d => "" % step,\n                      "" D loss : {:.8f}"".format(d_loss),\n                      "" G loss : {:.8f}"".format(g_loss))\n\n                sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n\n                # Training G model with sample image and noise\n                samples_1 = s.run(model.g_sample_1,\n                                  feed_dict={\n                                      # model.y: sample_y,\n                                      model.z: sample_z,\n                                  })\n\n                samples_2 = s.run(model.g_sample_2,\n                                  feed_dict={\n                                      # model.y: sample_y,\n                                      model.z: sample_z,\n                                  })\n\n                samples_1 = np.reshape(samples_1, [-1] + model.image_shape[1:])\n                samples_2 = np.reshape(samples_2, [-1] + model.image_shape[1:])\n\n                # Summary saver\n                model.writer.add_summary(summary, global_step=step)\n\n                # Export image generated by model G\n                sample_image_height = model.sample_size\n                sample_image_width = model.sample_size\n\n                sample_dir_1 = results[\'output\'] + \'train_1_{:08d}.png\'.format(step)\n                sample_dir_2 = results[\'output\'] + \'train_2_{:08d}.png\'.format(step)\n\n                # Generated image save\n                iu.save_images(samples_1,\n                               size=[sample_image_height, sample_image_width],\n                               image_path=sample_dir_1)\n                iu.save_images(samples_2,\n                               size=[sample_image_height, sample_image_width],\n                               image_path=sample_dir_2)\n\n                # Model save\n                model.saver.save(s, results[\'model\'], global_step=step)\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()'"
SGAN/sgan_model.py,82,"b'import tensorflow as tf\n\n\ntf.set_random_seed(777)\n\n\ndef conv2d(x, f=64, k=5, d=2, pad=\'SAME\', name=\'conv2d\'):\n    """"""\n    :param x: input\n    :param f: filters, default 64\n    :param k: kernel size, default 3\n    :param d: strides, default 2\n    :param pad: padding (valid or same), default same\n    :param name: scope name, default conv2d\n    :return: covn2d net\n    """"""\n    return tf.layers.conv2d(x,\n                            filters=f, kernel_size=k, strides=d,\n                            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n                            kernel_regularizer=tf.contrib.layers.l2_regularizer(5e-4),\n                            bias_initializer=tf.zeros_initializer(),\n                            padding=pad, name=name)\n\n\ndef deconv2d(x, f=64, k=5, d=2, pad=\'SAME\', name=\'deconv2d\'):\n    """"""\n    :param x: input\n    :param f: filters, default 64\n    :param k: kernel size, default 3\n    :param d: strides, default 2\n    :param pad: padding (valid or same), default same\n    :param name: scope name, default deconv2d\n    :return: decovn2d net\n    """"""\n    return tf.layers.conv2d_transpose(x,\n                                      filters=f, kernel_size=k, strides=d,\n                                      kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(5e-4),\n                                      bias_initializer=tf.zeros_initializer(),\n                                      padding=pad, name=name)\n\n\ndef batch_norm(x, momentum=0.9, eps=1e-5):\n    return tf.layers.batch_normalization(inputs=x,\n                                         momentum=momentum,\n                                         epsilon=eps,\n                                         scale=True,\n                                         training=True)\n\n\ndef gaussian_noise(x, std=5e-2):\n    noise = tf.random_normal(x.get_shape(), mean=0., stddev=std, dtype=tf.float32)\n    return x + noise\n\n\nclass SGAN:\n\n    def __init__(self, s, batch_size=64, input_height=28, input_width=28, input_channel=1,\n                 n_classes=10, n_input=784, sample_size=8, sample_num=64,\n                 z_dim=128, gf_dim=128, df_dim=96, fc_unit=256):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param input_height: input image height, default 28\n        :param input_width: input image width, default 28\n        :param input_channel: input image channel, default 1 (gray-scale)\n        - in case of MNIST, image size is 28x28x1(HWC).\n        :param n_classes: the classes, default 10\n        - in case of MNIST, there\'re 10 classes\n        :param n_input: flatten size of image, default 784\n\n        # Output Settings\n        :param sample_size: sample image size, default 8\n        :param sample_num: the number of sample images, default 64\n\n        # Model Settings\n        :param z_dim: z noise dimension, default 128\n        :param gf_dim: the number of generator filters, default 128\n        :param df_dim: the number of discriminator filters, default 32\n        :param fc_unit: fully connected units, default 256\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n        self.input_height = input_height\n        self.input_width = input_width\n        self.input_channel = input_channel\n        self.n_classes = n_classes\n        self.n_input = n_input\n\n        self.sample_size = sample_size\n        self.sample_num = sample_num\n\n        self.image_shape = [self.input_height, self.input_width, self.input_channel]\n\n        self.z_dim = z_dim\n\n        self.gf_dim = gf_dim\n        self.df_dim = df_dim\n        self.fc_unit = fc_unit\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32, shape=[None, self.n_input], name=\'x-images\')\n        self.y = tf.placeholder(tf.float32, shape=[None, self.n_classes], name=\'y-classes\')\n        self.z_1 = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise-1\')\n        self.z_2 = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise-2\')\n\n        # Training Options\n        self.beta1 = 0.5\n        self.beta2 = 0.9\n        self.lr = 2e-4\n\n        self.d_1_loss = 0.\n        self.d_0_loss = 0.\n        self.g_1_loss = 0.\n        self.g_0_loss = 0.\n\n        self.d_w_loss = 1.   # weight for adversarial loss\n        self.c_w_loss = 1.   # weight for conditional loss\n        self.e_w_loss = 10.  # weight for entropy loss\n\n        # pre-defined\n        self.g_0 = None\n        self.g_1 = None\n\n        self.d_1_op = None\n        self.d_0_op = None\n        self.g_1_op = None\n        self.g_0_op = None\n\n        self.merged = None\n        self.saver = None\n        self.writer = None\n\n        self.bulid_sgan()  # build SGAN model\n\n    def encoder(self, x, reuse=None):\n        """"""\n        :param x: images, (-1, 28, 28, 1)\n        :param reuse: re-usability\n        :return: embeddings(256), y-labels\' prob\n        """"""\n        with tf.variable_scope(\'encoder\', reuse=reuse):\n            x = tf.reshape(x, [-1] + self.image_shape)  # (-1, 28, 28, 1)\n\n            for i in range(1, 3):\n                x = conv2d(x, self.df_dim, name=\'enc-conv2d-%d\' % i)\n                x = tf.nn.leaky_relu(x)  # tf.nn.relu(x)\n                # x = tf.layers.max_pooling2d(x, pool_size=2, strides=2, name=\'enc-max_pool2d-%d\' % i)\n\n            x = tf.layers.flatten(x)\n\n            x = tf.layers.dense(x, self.fc_unit, name=\'enc-fc-1\')\n            x = tf.nn.leaky_relu(x)\n\n            logits = tf.layers.dense(x, self.n_classes, name=\'enc-fc-2\')\n            prob = tf.nn.softmax(logits)\n\n            return logits, prob\n\n    def discriminator_1(self, x, reuse=None):\n        """"""\n        :param x: features, (-1, 256)\n        :param reuse: re-usability\n        :return: z prob, disc prob\n        """"""\n        with tf.variable_scope(\'discriminator_1\', reuse=reuse):\n            for i in range(1, 3):\n                x = tf.layers.dense(x, self.fc_unit, name=\'d_1-fc-%d\' % i)\n\n            z = tf.layers.dense(x, self.z_dim, activation=tf.nn.sigmoid, name=\'d_1-fc-3\')\n            logits = tf.layers.dense(x, self.input_channel, name=\'d_1-fc-4\')\n\n            return z, logits\n\n    def discriminator_0(self, x, reuse=None):\n        """"""\n        :param x: MNIST image, (-1, 784)\n        :param reuse: re-usability\n        :return: z prob, disc prob\n        """"""\n        with tf.variable_scope(\'discriminator_0\', reuse=reuse):\n            x = tf.reshape(x, [-1] + self.image_shape)  # (-1, 28, 28, 1)\n            x = gaussian_noise(x)\n\n            x = conv2d(x, self.df_dim * 1, name=\'d_0-conv2d-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = conv2d(x, self.df_dim * 2, name=\'d_0-conv2d-2\')\n            x = batch_norm(x)\n            x = tf.nn.leaky_relu(x)\n\n            x = conv2d(x, self.df_dim * 4, name=\'d_0-conv2d-3\')\n            x = batch_norm(x)\n            x = tf.nn.leaky_relu(x)\n\n            x = tf.layers.flatten(x)\n\n            x = tf.layers.dense(x, self.fc_unit, name=\'d_0-fc-1\')\n\n            z = tf.layers.dense(x, self.z_dim, activation=tf.nn.sigmoid, name=\'d_0-fc-2\')\n            logits = tf.layers.dense(x, self.input_channel, name=\'d_0-fc-3\')\n\n            return z, logits\n\n    def generator_1(self, z, y, reuse=None):\n        with tf.variable_scope(\'generator_1\', reuse=reuse):\n            # z1 : (batch_size, 64)\n            # y  : (batch_size, 10)\n            # x  : (batch_size, 74)\n            x = tf.concat([z, y], axis=1)\n\n            x = tf.layers.dense(x, self.fc_unit * 2, name=\'g_1-fc-1\')\n            x = batch_norm(x)\n            x = tf.nn.leaky_relu(x)\n\n            x = tf.layers.dense(x, self.fc_unit * 2, name=\'g_1-fc-2\')\n            x = batch_norm(x)\n            x = tf.nn.leaky_relu(x)\n\n            x = tf.layers.dense(x, self.fc_unit * 1, name=\'g_1-fc-3\')\n            x = tf.nn.leaky_relu(x)\n\n            return x\n\n    def generator_0(self, z, h, reuse=None):\n        with tf.variable_scope(\'generator_0\', reuse=reuse):\n            z = tf.layers.dense(z, self.gf_dim, name=\'g_0-fc-1\')\n            z = tf.nn.leaky_relu(z)\n\n            # z : (batch_size, 128)\n            # h : (batch_size, 256)\n            # x : (batch_size, 374)\n            x = tf.concat([h, z], axis=1)\n\n            x = tf.layers.dense(x, self.gf_dim * 7 * 7, name=\'g_0-fc-2\')\n            x = batch_norm(x)\n            x = tf.nn.leaky_relu(x)\n\n            x = tf.reshape(x, [-1, 7, 7, self.gf_dim])\n\n            for i in range(1, 3):\n                f = self.gf_dim // i\n                x = deconv2d(x, f, name=\'g_0-deconv2d-%d\' % i)\n                x = batch_norm(x)\n                x = tf.nn.leaky_relu(x)\n\n            logits = deconv2d(x, self.input_channel, d=1, name=\'g_0-deconv2d-3\')  # 28x28x1\n            # prob = tf.nn.sigmoid(logits)\n\n            return logits\n\n    def bulid_sgan(self):\n        # Generator\n        self.g_1 = self.generator_1(self.z_2, self.y)    # embeddings\n        self.g_0 = self.generator_0(self.z_1, self.g_1)  # generated image\n\n        # Encoder\n        enc_real_c, enc_real_f_prob = self.encoder(self.x)\n        enc_fake_c, enc_fake_f_prob = self.encoder(self.g_0, reuse=True)\n\n        # Discriminator\n        d_1_real_z_prob, d_1_real = self.discriminator_1(enc_real_f_prob)\n        d_1_fake_z_prob, d_1_fake = self.discriminator_1(self.g_1, reuse=True)\n        d_0_real_z_prob, d_0_real = self.discriminator_0(self.x)\n        d_0_fake_z_prob, d_0_fake = self.discriminator_0(self.g_0, reuse=True)\n\n        # Losses\n        # Discriminator 1\n        d_1_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_1_real,\n                                                                               labels=tf.ones_like(d_1_real)))\n        d_1_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_1_fake,\n                                                                               labels=tf.zeros_like(d_1_fake)))\n        g_1_ent = tf.reduce_mean(tf.square(d_1_fake_z_prob - self.z_2))\n        self.d_1_loss = 0.5 * self.d_w_loss * (d_1_real_loss + d_1_fake_loss) + self.e_w_loss * g_1_ent\n\n        # Discriminator 0\n        d_0_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_0_real,\n                                                                               labels=tf.ones_like(d_0_real)))\n        d_0_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_0_fake,\n                                                                               labels=tf.zeros_like(d_0_fake)))\n        g_0_ent = tf.reduce_mean(tf.square(d_0_fake_z_prob - self.z_1))\n        self.d_0_loss = 0.5 * self.d_w_loss * (d_0_real_loss + d_0_fake_loss) + self.e_w_loss * g_0_ent\n\n        # Generator 1\n        g_1_adv_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_1_fake,\n                                                                              labels=tf.ones_like(d_1_fake)))\n        g_1_cond_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=enc_real_c,\n                                                                               labels=self.y))\n        self.g_1_loss = self.d_w_loss * g_1_adv_loss + self.c_w_loss * g_1_cond_loss + self.e_w_loss * g_1_ent\n\n        # Generator 0\n        g_0_adv_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_0_fake,\n                                                                              labels=tf.ones_like(d_0_fake)))\n        g_0_cond_loss = tf.reduce_mean(tf.square(enc_fake_f_prob - enc_real_f_prob))\n        self.g_0_loss = self.d_w_loss * g_0_adv_loss + self.c_w_loss * g_0_cond_loss + self.e_w_loss * g_0_ent\n\n        # Summary\n        tf.summary.scalar(""loss/d_1_loss"", self.d_1_loss)\n        tf.summary.scalar(""loss/d_0_loss"", self.d_0_loss)\n        tf.summary.scalar(""loss/g_1_loss"", self.g_1_loss)\n        tf.summary.scalar(""loss/g_0_loss"", self.g_0_loss)\n\n        # Collect trainer values\n        t_vars = tf.trainable_variables()\n        d_1_params = [v for v in t_vars if v.name.startswith(\'discriminator_1\')]\n        d_0_params = [v for v in t_vars if v.name.startswith(\'discriminator_0\')]\n        g_1_params = [v for v in t_vars if v.name.startswith(\'generator_1\')]\n        g_0_params = [v for v in t_vars if v.name.startswith(\'generator_0\')]\n\n        # Optimizer\n        self.d_1_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                             beta1=self.beta1).minimize(self.d_1_loss, var_list=d_1_params)\n        self.d_0_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                             beta1=self.beta1).minimize(self.d_0_loss, var_list=d_0_params)\n        self.g_1_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                             beta1=self.beta1).minimize(self.g_1_loss, var_list=g_1_params)\n        self.g_0_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                             beta1=self.beta1).minimize(self.g_0_loss, var_list=g_0_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model Saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
SGAN/sgan_train.py,4,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport sgan_model as sgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import MNISTDataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'checkpoint\': \'./model/checkpoint\',\n    \'model\': \'./model/SGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'global_step\': 250001,\n    \'logging_interval\': 2500,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # MNIST Dataset load\n    mnist = DataSet(ds_path=""./"").data\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # SGAN Model\n        model = sgan.SGAN(s)\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        sample_x, sample_y = mnist.test.next_batch(model.sample_num)\n        # sample_x = np.reshape(sample_x, [model.sample_num, model.n_input])\n\n        d_overpowered = False\n        for step in range(train_step[\'global_step\']):\n            batch_x, batch_y = mnist.train.next_batch(model.batch_size)\n            # batch_x = np.reshape(batch_x, [model.batch_size, model.n_input])\n            batch_z_0 = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n            batch_z_1 = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n            # Update D network\n            if not d_overpowered:\n                _, d_0_loss, _, _ = s.run([model.d_0_op, model.d_0_loss, model.d_1_op, model.d_1_loss],\n                                          feed_dict={\n                                              model.x: batch_x,\n                                              model.y: batch_y,\n                                              model.z_1: batch_z_1,\n                                              model.z_0: batch_z_0,\n                                          })\n\n            # Update G network\n            _, g_0_loss, _, _ = s.run([model.g_0_op, model.g_0_loss, model.g_1_op, model.g_1_loss],\n                                      feed_dict={\n                                          model.x: batch_x,\n                                          model.y: batch_y,\n                                          model.z_1: batch_z_1,\n                                          model.z_0: batch_z_0,\n                                      })\n\n            d_overpowered = d_0_loss < g_0_loss / 2\n\n            if step % train_step[\'logging_interval\'] == 0:\n                batch_x, batch_y = mnist.train.next_batch(model.batch_size)\n                # batch_x = np.reshape(batch_x, [model.batch_size, model.n_input])\n                batch_z_0 = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n                batch_z_1 = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                d_0_loss, _, g_0_loss, _, summary = s.run([model.d_0_loss, model.d_1_loss,\n                                                           model.g_0_loss, model.g_1_loss,\n                                                           model.merged],\n                                                          feed_dict={\n                                                              model.x: batch_x,\n                                                              model.y: batch_y,\n                                                              model.z_1: batch_z_1,\n                                                              model.z_0: batch_z_0,\n                                                          })\n\n                d_overpowered = d_0_loss < g_0_loss / 2\n\n                # Print loss\n                print(""[+] Step %08d => "" % step,\n                      "" D loss : {:.8f}"".format(d_0_loss),\n                      "" G loss : {:.8f}"".format(g_0_loss))\n\n                # Training G model with sample image and noise\n                sample_z_0 = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                sample_z_1 = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                _, samples = s.run([model.g_1, model.g_0],\n                                   feed_dict={\n                                       model.y: sample_y,\n                                       model.z_1: sample_z_1,\n                                       model.z_0: sample_z_0,\n                                   })\n\n                samples = np.reshape(samples, [model.batch_size] + model.image_shape)\n\n                # Summary saver\n                model.writer.add_summary(summary, step)\n\n                # Export image generated by model G\n                sample_image_height = model.sample_size\n                sample_image_width = model.sample_size\n                sample_dir = results[\'output\'] + \'train_{:08d}.png\'.format(step)\n\n                # Generated image save\n                iu.save_images(samples,\n                               size=[sample_image_height, sample_image_width],\n                               image_path=sample_dir)\n\n                # Model save\n                model.saver.save(s, results[\'model\'], global_step=step)\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
SRGAN/srgan_model.py,45,"b'import tensorflow as tf\n\nimport vgg19\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\nclass SRGAN:\n\n    def __init__(self, s, batch_size=16, height=384, width=384, channel=3,\n                 sample_num=1 * 1, sample_size=1,\n                 df_dim=64, gf_dim=64, lr=1e-4, use_vgg19=True):\n\n        """""" Super-Resolution GAN Class\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 16\n        :param height: input image height, default 384\n        :param width: input image width, default 384\n        :param channel: input image channel, default 3 (RGB)\n        - in case of DIV2K-HR, image size is 384x384x3(HWC).\n\n        # Output Settings\n        :param sample_num: the number of output images, default 1\n        :param sample_size: sample image size, default 1\n\n        # For CNN model\n        :param df_dim: discriminator filter, default 64\n        :param gf_dim: generator filter, default 64\n\n        # Training Option\n        :param lr: learning rate, default 1e-4\n        :param use_vgg19: using pre-trained vgg19 bottle-neck features, default False\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n\n        self.lr_image_shape = [None, self.height // 4, self.width // 4, self.channel]\n        self.hr_image_shape = [None, self.height, self.width, self.channel]\n\n        self.vgg_image_shape = [224, 224, 3]\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n\n        self.beta1 = 0.9\n        self.beta2 = 0.999\n\n        self.lr_decay_rate = 1e-1\n        self.lr_low_boundary = 1e-5\n        self.lr_update_step = 1e5\n        self.lr_update_epoch = 1000\n\n        self.vgg_mean = [103.939, 116.779, 123.68]\n\n        # pre-defined\n        self.d_real = 0.\n        self.d_fake = 0.\n        self.d_loss = 0.\n        self.g_adv_loss = 0.\n        self.g_cnt_loss = 0.\n        self.g_loss = 0.\n        self.psnr = 0.\n\n        self.use_vgg19 = use_vgg19\n        self.vgg19 = None\n\n        self.g = None\n\n        self.adv_scaling = 1e-3\n        self.cnt_scaling = 1. / 12.75  # 6e-3\n\n        self.d_op = None\n        self.g_op = None\n        self.g_init_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x_hr = tf.placeholder(tf.float32, shape=self.hr_image_shape, name=""x-image-hr"")  # (-1, 384, 384, 3)\n        self.x_lr = tf.placeholder(tf.float32, shape=self.lr_image_shape, name=""x-image-lr"")  # (-1, 96, 96, 3)\n\n        self.lr = tf.placeholder(tf.float32, name=\'lr\')\n\n        self.build_srgan()  # build SRGAN model\n\n    def discriminator(self, x, reuse=None):\n        """"""\n        # Following a network architecture referred in the paper\n        :param x: Input images (-1, 384, 384, 3)\n        :param reuse: re-usability\n        :return: HR (High Resolution) or SR (Super Resolution) images\n        """"""\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            x = t.conv2d(x, self.df_dim, 3, 1, name=\'n64s1-1\')\n            x = tf.nn.leaky_relu(x)\n\n            strides = [2, 1]\n            filters = [1, 2, 2, 4, 4, 8, 8]\n\n            for i, f in enumerate(filters):\n                x = t.conv2d(x, f=f, k=3, s=strides[i % 2], name=\'n%ds%d-%d\' % (f, strides[i % 2], i + 1))\n                x = t.batch_norm(x, name=\'n%d-bn-%d\' % (f, i + 1))\n                x = tf.nn.leaky_relu(x)\n\n            x = tf.layers.flatten(x)  # (-1, 96 * 96 * 64)\n\n            x = t.dense(x, 1024, name=\'disc-fc-1\')\n            x = tf.nn.leaky_relu(x)\n\n            x = t.dense(x, 1, name=\'disc-fc-2\')\n            # x = tf.nn.sigmoid(x)\n            return x\n\n    def generator(self, x, reuse=None, is_train=True):\n        """"""\n        :param x: LR (Low Resolution) images, (-1, 96, 96, 3)\n        :param reuse: scope re-usability\n        :param is_train: is trainable, default True\n        :return: SR (Super Resolution) images, (-1, 384, 384, 3)\n        """"""\n\n        with tf.variable_scope(""generator"", reuse=reuse):\n            def residual_block(x, f, name="""", _is_train=True):\n                with tf.variable_scope(name):\n                    shortcut = tf.identity(x, name=\'n64s1-shortcut\')\n\n                    x = t.conv2d(x, f, 3, 1, name=""n64s1-1"")\n                    x = t.batch_norm(x, is_train=_is_train, name=""n64s1-bn-1"")\n                    x = t.prelu(x, reuse=reuse, name=\'n64s1-prelu-1\')\n                    x = t.conv2d(x, f, 3, 1, name=""n64s1-2"")\n                    x = t.batch_norm(x, is_train=_is_train, name=""n64s1-bn-2"")\n                    x = tf.add(x, shortcut)\n\n                    return x\n\n            x = t.conv2d(x, self.gf_dim, 9, 1, name=\'n64s1-1\')\n            x = t.prelu(x, name=\'n64s1-prelu-1\')\n\n            skip_conn = tf.identity(x, name=\'skip_connection\')\n\n            # B residual blocks\n            for i in range(1, 17):  # (1, 9)\n                x = residual_block(x, self.gf_dim, name=\'b-residual_block_%d\' % i, _is_train=is_train)\n\n            x = t.conv2d(x, self.gf_dim, 3, 1, name=\'n64s1-3\')\n            x = t.batch_norm(x, is_train=is_train, name=\'n64s1-bn-3\')\n\n            x = tf.add(x, skip_conn)\n\n            # sub-pixel conv2d blocks\n            for i in range(1, 3):\n                x = t.conv2d(x, self.gf_dim * 4, 3, 1, name=\'n256s1-%d\' % (i + 2))\n                x = t.sub_pixel_conv2d(x, f=None, s=2)\n                x = t.prelu(x, name=\'n256s1-prelu-%d\' % i)\n\n            x = t.conv2d(x, self.channel, 9, 1, name=\'n3s1\')  # (-1, 384, 384, 3)\n            x = tf.nn.tanh(x)\n            return x\n\n    def build_vgg19(self, x, reuse=None):\n        with tf.variable_scope(""vgg19"", reuse=reuse):\n            # image re-scaling\n            x = tf.cast((x + 1) / 2, dtype=tf.float32)  # [-1, 1] to [0, 1]\n            x = tf.cast(x * 255., dtype=tf.float32)     # [0, 1]  to [0, 255]\n\n            r, g, b = tf.split(x, 3, 3)\n            bgr = tf.concat([b - self.vgg_mean[0],\n                             g - self.vgg_mean[1],\n                             r - self.vgg_mean[2]], axis=3)\n\n            self.vgg19 = vgg19.VGG19(bgr)\n\n            net = self.vgg19.vgg19_net[\'conv5_4\']\n\n            return net  # last layer\n\n    def build_srgan(self):\n        # Generator\n        self.g = self.generator(self.x_lr)\n\n        # Discriminator\n        d_real = self.discriminator(self.x_hr)\n        d_fake = self.discriminator(self.g, reuse=True)\n\n        # Losses\n        # d_real_loss = -tf.reduce_mean(t.safe_log(d_real))\n        # d_fake_loss = -tf.reduce_mean(t.safe_log(1. - d_fake))\n        d_real_loss = t.sce_loss(d_real, tf.ones_like(d_real))\n        d_fake_loss = t.sce_loss(d_fake, tf.zeros_like(d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n\n        if self.use_vgg19:\n            x_vgg_real = tf.image.resize_images(self.x_hr, size=self.vgg_image_shape[:2], align_corners=False)\n            x_vgg_fake = tf.image.resize_images(self.g, size=self.vgg_image_shape[:2], align_corners=False)\n\n            vgg_bottle_real = self.build_vgg19(x_vgg_real)\n            vgg_bottle_fake = self.build_vgg19(x_vgg_fake, reuse=True)\n\n            self.g_cnt_loss = self.cnt_scaling * t.mse_loss(vgg_bottle_fake, vgg_bottle_real, self.batch_size,\n                                                            is_mean=True)\n        else:\n            self.g_cnt_loss = t.mse_loss(self.g, self.x_hr, self.batch_size, is_mean=True)\n\n        # self.g_adv_loss = self.adv_scaling * tf.reduce_mean(-1. * t.safe_log(d_fake))\n        self.g_adv_loss = self.adv_scaling * t.sce_loss(d_fake, tf.ones_like(d_fake))\n        self.g_loss = self.g_adv_loss + self.g_cnt_loss\n\n        def inverse_transform(img):\n            return (img + 1.) * 127.5\n\n        # calculate PSNR\n        g, x_hr = inverse_transform(self.g), inverse_transform(self.x_hr)\n        self.psnr = t.psnr_loss(g, x_hr, self.batch_size)\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_cnt_loss"", self.g_cnt_loss)\n        tf.summary.scalar(""loss/g_adv_loss"", self.g_adv_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n        tf.summary.scalar(""misc/psnr"", self.psnr)\n        tf.summary.scalar(""misc/lr"", self.lr)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(loss=self.d_loss,\n                                                                                        var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(loss=self.g_loss,\n                                                                                        var_list=g_params)\n\n        # pre-train\n        self.g_init_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n                                                beta1=self.beta1, beta2=self.beta2).minimize(loss=self.g_cnt_loss,\n                                                                                             var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=2)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
SRGAN/srgan_train.py,7,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport srgan_model as srgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import Div2KDataSet as DataSet\n\n\nnp.random.seed(1337)\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/SRGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'batch_size\': 16,\n    \'init_epochs\': 100,\n    \'train_epochs\': 1501,\n    \'global_step\': 200001,\n    \'logging_interval\': 100,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # Div2K - Track 1: Bicubic downscaling - x4 DataSet load\n    """"""\n    ds = DataSet(ds_path=""/home/zero/hdd/DataSet/DIV2K/"",\n                 ds_name=""X4"",\n                 use_save=True,\n                 save_type=""to_h5"",\n                 save_file_name=""/home/zero/hdd/DataSet/DIV2K/DIV2K"",\n                 use_img_scale=True)\n    """"""\n    ds = DataSet(ds_hr_path=""/home/zero/hdd/DataSet/DIV2K/DIV2K-hr.h5"",\n                 ds_lr_path=""/home/zero/hdd/DataSet/DIV2K/DIV2K-lr.h5"",\n                 use_img_scale=True)\n\n    hr, lr = ds.hr_images, ds.lr_images\n\n    print(""[+] Loaded HR image "", hr.shape)\n    print(""[+] Loaded LR image "", lr.shape)\n\n    # GPU configure\n    gpu_config = tf.GPUOptions(allow_growth=True)\n    config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False, gpu_options=gpu_config)\n\n    with tf.Session(config=config) as s:\n        with tf.device(""/gpu:1""):  # Change\n            # SRGAN Model\n            model = srgan.SRGAN(s, batch_size=train_step[\'batch_size\'],\n                                use_vgg19=False)\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % global_step, "" successfully loaded"")\n        else:\n            global_step = 0\n            print(\'[-] No checkpoint file found\')\n\n        start_epoch = global_step // (ds.n_images // train_step[\'batch_size\'])\n\n        rnd = np.random.randint(0, ds.n_images)\n        sample_x_hr, sample_x_lr = hr[rnd], lr[rnd]\n\n        sample_x_hr, sample_x_lr = \\\n            np.reshape(sample_x_hr, [1] + model.hr_image_shape[1:]), \\\n            np.reshape(sample_x_lr, [1] + model.lr_image_shape[1:])\n\n        # Export real image\n        # valid_image_height = model.sample_size\n        # valid_image_width = model.sample_size\n        sample_hr_dir, sample_lr_dir = results[\'output\'] + \'valid_hr.png\', results[\'output\'] + \'valid_lr.png\'\n\n        # Generated image save\n        iu.save_images(sample_x_hr,\n                       size=[1, 1],\n                       image_path=sample_hr_dir,\n                       inv_type=\'127\')\n\n        iu.save_images(sample_x_lr,\n                       size=[1, 1],\n                       image_path=sample_lr_dir,\n                       inv_type=\'127\')\n\n        learning_rate = 1e-4\n        for epoch in range(start_epoch, train_step[\'train_epochs\']):\n            pointer = 0\n            for i in range(ds.n_images // train_step[\'batch_size\']):\n                start = pointer\n                pointer += train_step[\'batch_size\']\n\n                if pointer > ds.n_images:  # if 1 epoch is ended\n                    # Shuffle training DataSet\n                    perm = np.arange(ds.n_images)\n                    np.random.shuffle(perm)\n\n                    hr, lr = hr[perm], lr[perm]\n\n                    start = 0\n                    pointer = train_step[\'batch_size\']\n\n                end = pointer\n\n                batch_x_hr, batch_x_lr = hr[start:end], lr[start:end]\n\n                # reshape\n                batch_x_hr = np.reshape(batch_x_hr, [train_step[\'batch_size\']] + model.hr_image_shape[1:])\n                batch_x_lr = np.reshape(batch_x_lr, [train_step[\'batch_size\']] + model.lr_image_shape[1:])\n\n                # Update Only G network\n                d_loss, g_loss, g_init_loss = 0., 0., 0.\n                if epoch <= train_step[\'init_epochs\']:\n                    _, g_init_loss = s.run([model.g_init_op, model.g_cnt_loss],\n                                           feed_dict={\n                                               model.x_hr: batch_x_hr,\n                                               model.x_lr: batch_x_lr,\n                                               model.lr: learning_rate,\n                                           })\n                # Update G/D network\n                else:\n                    _, d_loss = s.run([model.d_op, model.d_loss],\n                                      feed_dict={\n                                          model.x_hr: batch_x_hr,\n                                          model.x_lr: batch_x_lr,\n                                          model.lr: learning_rate,\n                                      })\n\n                    _, g_loss = s.run([model.g_op, model.g_loss],\n                                      feed_dict={\n                                          model.x_hr: batch_x_hr,\n                                          model.x_lr: batch_x_lr,\n                                          model.lr: learning_rate,\n                                      })\n\n                if i % train_step[\'logging_interval\'] == 0:\n                    # Print loss\n                    if epoch <= train_step[\'init_epochs\']:\n                        print(""[+] Epoch %04d Step %08d => "" % (epoch, global_step),\n                              "" MSE loss : {:.8f}"".format(g_init_loss))\n                    else:\n                        print(""[+] Epoch %04d Step %08d => "" % (epoch, global_step),\n                              "" D loss : {:.8f}"".format(d_loss),\n                              "" G loss : {:.8f}"".format(g_loss))\n\n                        summary = s.run(model.merged,\n                                        feed_dict={\n                                            model.x_hr: batch_x_hr,\n                                            model.x_lr: batch_x_lr,\n                                            model.lr: learning_rate,\n                                        })\n\n                        # Summary saver\n                        model.writer.add_summary(summary, global_step)\n\n                    # Training G model with sample image and noise\n                    sample_x_lr = np.reshape(sample_x_lr, [model.sample_num] + model.lr_image_shape[1:])\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.x_lr: sample_x_lr,\n                                        model.lr: learning_rate,\n                                    })\n\n                    # Export image generated by model G\n                    # sample_image_height = model.output_height\n                    # sample_image_width = model.output_width\n                    sample_dir = results[\'output\'] + \'train_{:08d}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[1, 1],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step)\n\n                # Learning Rate update\n                if epoch and epoch % model.lr_update_epoch == 0:\n                    learning_rate *= model.lr_decay_rate\n                    learning_rate = max(learning_rate, model.lr_low_boundary)\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
SRGAN/vgg19.py,8,"b'from urllib.request import urlretrieve\n\nimport tensorflow as tf\nimport numpy as np\nimport scipy.io\nimport os\n\n\nvgg19_download_link = \'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat\'\nvgg19_file_name = \'imagenet-vgg-verydeep-19.mat\'\n\n\ndef vgg19_download(file_name, expected_bytes=534904783):\n    """""" Download the pre-trained VGG-19 model if it\'s not already downloaded """"""\n\n    if os.path.exists(file_name):\n        print(""[*] VGG-19 pre-trained model already exists"")\n        return\n\n    print(""[*] Downloading the VGG-19 pre-trained model..."")\n\n    file_name, _ = urlretrieve(vgg19_download_link, vgg19_file_name)\n    file_stat = os.stat(file_name)\n\n    if file_stat.st_size == expected_bytes:\n        print(\'[+] Successfully downloaded VGG-19 pre-trained model\', file_name)\n    else:\n        raise Exception(\'[-] File \' + file_name + \' might be corrupted :(\')\n\n\ndef conv2d_layer(input_, weights, bias):\n    """""" convolution 2d layer with bias """"""\n    x = tf.nn.conv2d(input_, filter=weights, strides=(1, 1, 1, 1), padding=\'SAME\')\n    x = tf.nn.bias_add(x, bias)\n    return x\n\n\ndef pool2d_layer(input_, pool=\'avg\'):\n    """""" pooling 2c layer with max or avg """"""\n    if pool == \'avg\':\n        x = tf.nn.avg_pool(input_, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding=\'SAME\')\n    else:\n        x = tf.nn.max_pool(input_, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding=\'SAME\')\n    return x\n\n\nclass VGG19(object):\n\n    def __init__(self, input_image):\n        vgg19_download(vgg19_file_name)  # download vgg19 pre-trained model\n\n        self.vgg19_layers = (\n            \'conv1_1\', \'relu1_1\', \'conv1_2\', \'relu1_2\', \'pool1\',\n            \'conv2_1\', \'relu2_1\', \'conv2_2\', \'relu2_2\', \'pool2\',\n            \'conv3_1\', \'relu3_1\', \'conv3_2\', \'relu3_2\', \'conv3_3\',\n            \'relu3_3\', \'conv3_4\', \'relu3_4\', \'pool3\',\n            \'conv4_1\', \'relu4_1\', \'conv4_2\', \'relu4_2\', \'conv4_3\',\n            \'relu4_3\', \'conv4_4\', \'relu4_4\', \'pool4\',\n            \'conv5_1\', \'relu5_1\', \'conv5_2\', \'relu5_2\', \'conv5_3\',\n            \'relu5_3\', \'conv5_4\', \'relu5_4\'\n        )\n\n        self.mean_pixels = np.array([123.68, 116.779, 103.939]).reshape((1, 1, 1, 3))\n\n        self.weights = scipy.io.loadmat(vgg19_file_name)[\'layers\'][0]\n\n        self.input_img = input_image\n        self.vgg19_net = self.build(self.input_img)\n\n    def _get_weight(self, idx, layer_name):\n        weight = self.weights[idx][0][0][2][0][0]\n        bias = self.weights[idx][0][0][2][0][1].reshape(-1)\n\n        assert layer_name == self.weights[idx][0][0][0][0]\n\n        with tf.variable_scope(layer_name):\n            weight = tf.constant(weight, name=\'weights\')\n            bias = tf.constant(bias, name=\'bias\')\n        return weight, bias\n\n    def build(self, img):\n        x = {}  # network\n        net = img\n\n        for idx, name in enumerate(self.vgg19_layers):\n            layer_name = name[:4]\n\n            if layer_name == \'conv\':\n                weight, bias = self._get_weight(idx, name)\n                net = conv2d_layer(net, weight, bias)\n            elif layer_name == \'relu\':\n                net = tf.nn.relu(net)\n            elif layer_name == \'pool\':\n                net = pool2d_layer(net)\n\n            x[name] = net\n\n        return x\n'"
SalGAN/salgan_model.py,0,b'# initial python file\n'
SalGAN/salgan_train.py,0,b'# initial python file\n'
SeqGAN/seqgan_model.py,0,b'# initial python file\n'
SeqGAN/seqgan_train.py,0,b'# initial python file\n'
StarGAN/dataset.py,0,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport h5py\nimport numpy as np\nfrom glob import glob\nfrom tqdm import tqdm\nfrom scipy.misc import imread, imresize\n\n\'\'\'\nThis dataset is for Celeb-A\n\n- Celeb-A\n    Celeb-A DataSets can be downloaded at http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n\n    Celeb-A link : https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg\n\n    OR you can download following python code (but it doesn\'t work as well when i\'m trying)\n    code link : https://github.com/carpedm20/DCGAN-tensorflow/blob/master/download.py\n\'\'\'\n\nDataSets = {\n    # Linux\n    # \'celeb-a\': \'/home/zero/hdd/DataSet/Celeb-A/img_align_celeba/\',\n    # \'celeb-a-attr\': \'/home/zero/hdd/DataSet/Celeb-A/list_attr_celeba.txt\',\n    # \'celeb-a-32x32-h5\': \'/home/zero/hdd/DataSet/Celeb-A/celeb-a-32x32.h5\',\n    # \'celeb-a-64x64-h5\': \'/home/zero/hdd/DataSet/Celeb-A/celeb-a-64x64.h5\',\n    # Windows\n    \'celeb-a\': \'D:\\\\DataSet\\\\Celeb-A\\\\img_align_celeba\\\\\',\n    \'celeb-a-attr\': \'D:\\\\DataSet\\\\Celeb-A\\\\list_attr_celeba.txt\',\n    \'celeb-a-32x32-h5\': \'D:\\\\DataSet\\\\Celeb-A\\\\celeb-a-32x32.h5\',\n    \'celeb-a-64x64-h5\': \'D:\\\\DataSet\\\\Celeb-A\\\\celeb-a-64x64.h5\',\n}\n\n\nclass CelebADataSet:\n\n    def __init__(self, batch_size=128, input_height=64, input_width=64, input_channel=3, attr_labels=(),\n                 output_height=64, output_width=64, output_channel=3,\n                 split_rate=0.2, random_state=42, num_threads=8, mode=\'w\'):\n\n        """"""\n        # General Settings\n        :param batch_size: training batch size, default 128\n        :param input_height: input image height, default 64\n        :param input_width: input image width, default 64\n        :param input_channel: input image channel, default 3 (RGB)\n        - in case of Celeb-A, image size is 64x64x3(HWC).\n        :param attr_labels: attributes of Celeb-A image, default empty tuple\n        - in case of Celeb-A, the number of attributes is 40\n\n        # Output Settings\n        :param output_height: output images height, default 64\n        :param output_width: output images width, default 64\n        :param output_channel: output images channel, default 3\n\n        # Pre-Processing Option\n        :param split_rate: image split rate (into train & test), default 0.2\n        :param random_state: random seed for shuffling, default 42\n        :param num_threads: the number of threads for multi-threading, default 8\n\n        # DataSet Option\n        :param mode: file mode(RW), default w\n        """"""\n\n        self.batch_size = batch_size\n        self.input_height = input_height\n        self.input_width = input_width\n        self.input_channel = input_channel\n        \'\'\'\n        # Available attributes\n        [\n         5_o_Clock_Shadow, Arched_Eyebrows, Attractive, Bags_Under_Eyes, Bald, Bangs, Big_Lips, Big_Nose, Black_Hair,\n         Blond_Hair, Blurry, Brown_Hair, Bushy_Eyebrows, Chubby, Double_Chin, Eyeglasses, Goatee, Gray_Hair,\n         Heavy_Makeup, High_Cheekbones, Male, Mouth_Slightly_Open, Mustache, Narrow_Eyes, No_Beard, Oval_Face,\n         Pale_Skin, Pointy_Nose, Receding_Hairline, Rosy_Cheeks, Sideburns, Smiling, Straight_Hair, Wavy_Hair,\n         Wearing_Earrings, Wearing_Hat, Wearing_Lipstick, Wearing_Necklace, Wearing_Necktie, Young\n        ]\n        \'\'\'\n        self.attr_labels = attr_labels\n        self.image_shape = [self.batch_size, self.input_height, self.input_width, self.input_channel]\n\n        self.output_height = output_height\n        self.output_width = output_width\n        self.output_channel = output_channel\n\n        self.split_rate = split_rate\n        self.random_state = random_state\n        self.num_threads = num_threads  # change this value to the fitted value for ur system\n        self.mode = mode\n\n        self.path = """"  # DataSet path\n        self.files = """"  # files\' name\n        self.n_classes = 0  # DataSet the number of classes, default 10\n\n        self.data = []  # loaded images\n        self.attr = []\n        self.num_images = 202599\n        self.images = []\n        self.labels = {}\n        self.ds_name = """"  # DataSet Name (by image size)\n\n        self.celeb_a(mode=self.mode)  # load Celeb-A\n\n    def celeb_a(self, mode):\n        def get_image(path, w, h):\n            img = imread(path).astype(np.float)\n\n            orig_h, orig_w = img.shape[:2]\n            new_h = int(orig_h * w / orig_w)\n\n            img = imresize(img, (new_h, w))\n            margin = int(round((new_h - h) / 2))\n\n            return img[margin:margin + h]\n\n        if self.input_height == 32:\n            self.ds_name = \'celeb-a-32x32-h5\'\n        elif self.input_height == 64:\n            self.ds_name = \'celeb-a-64x64-h5\'\n\n        self.labels = self.load_attr()    # selected attributes info (list)\n\n        if mode == \'w\':\n            self.files = glob(os.path.join(DataSets[\'celeb-a\'], ""*.jpg""))\n            self.files = np.sort(self.files)\n\n            self.data = np.zeros((len(self.files), self.input_height * self.input_width * self.input_channel),\n                                 dtype=np.uint8)\n\n            print(""[*] Image size : "", self.data.shape)\n\n            assert (len(self.files) == self.num_images)\n\n            for n, f_name in tqdm(enumerate(self.files)):\n                image = get_image(f_name, self.input_width, self.input_height)\n                self.data[n] = image.flatten()\n\n            # write .h5 file for reusing later...\n            with h5py.File(\'\'.join([DataSets[self.ds_name]]), \'w\') as f:\n                f.create_dataset(""images"", data=self.data)\n\n        self.images = self.load_data(size=self.num_images)\n\n    def load_data(self, size, offset=0):\n        """"""\n            From great jupyter notebook by Tim Sainburg:\n            http://github.com/timsainb/Tensorflow-MultiGPU-VAE-GAN\n        """"""\n        with h5py.File(DataSets[self.ds_name], \'r\') as hf:\n            faces = hf[\'images\']\n\n            full_size = len(faces)\n            if size is None:\n                size = full_size\n\n            n_chunks = int(np.ceil(full_size / size))\n            if offset >= n_chunks:\n                print(""[*] Looping from back to start."")\n                offset = offset % n_chunks\n\n            if offset == n_chunks - 1:\n                print(""[-] Not enough data available, clipping to end."")\n                faces = faces[offset * size:]\n            else:\n                faces = faces[offset * size:(offset + 1) * size]\n\n            faces = np.array(faces, dtype=np.float16)\n\n        print(""[+] Image size : "", faces.shape)\n\n        return faces / 255.\n\n    def load_attr(self):\n        with open(DataSets[\'celeb-a-attr\'], \'r\') as f:\n            img_attr = []\n\n            self.num_images = int(f.readline().strip())\n            self.attr = (f.readline().strip()).split(\' \')\n\n            print(""[*] the number of images     : %d"" % self.num_images)\n            print(""[*] the number of attributes : %d/%d"" % (len(self.attr_labels), len(self.attr)))\n\n            for fn in f.readlines():\n                row = fn.strip().split()\n                # img_name = row[0]\n                attr = [int(x) for x in row[1:]]\n\n                tmp = [attr[self.attr.index(x)] for x in self.attr_labels]\n                tmp = [1. if x == 1 else 0. for x in tmp]  # one-hot labeling\n\n                img_attr.append(tmp)\n\n            return np.asarray(img_attr)\n\n    def concat_data(self, img, label):\n        label = np.tile(np.reshape(label, [-1, 1, 1, len(self.attr_labels)]),\n                        [1, self.input_height, self.input_width, 1])\n\n        return np.concatenate([img, label], axis=3)\n\n\nclass DataIterator:\n\n    def __init__(self, x, y, batch_size, label_off=False):\n        self.x = x\n        self.label_off = label_off\n        if not self.label_off:\n            self.y = y\n        self.batch_size = batch_size\n        self.num_examples = num_examples = x.shape[0]\n        self.num_batches = num_examples // batch_size\n        self.pointer = 0\n\n        assert (self.batch_size <= self.num_examples)\n\n    def next_batch(self):\n        start = self.pointer\n        self.pointer += self.batch_size\n\n        if self.pointer > self.num_examples:\n            perm = np.arange(self.num_examples)\n            np.random.shuffle(perm)\n\n            self.x = self.x[perm]\n            if not self.label_off:\n                self.y = self.y[perm]\n\n            start = 0\n            self.pointer = self.batch_size\n\n        end = self.pointer\n\n        if not self.label_off:\n            return self.x[start:end], self.y[start:end]\n        else:\n            return self.x[start:end]\n\n    def iterate(self):\n        for step in range(self.num_batches):\n            yield self.next_batch()\n'"
StarGAN/stargan_model.py,35,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\ndef residual_block(x, f, name=""0""):\n    with tf.variable_scope(""residual_block-"" + name):\n        scope_name = ""residual_block-"" + name\n\n        x = t.conv2d(x, f=f, k=3, s=1)\n        x = t.instance_norm(x, affine=True, name=scope_name + \'_0\')\n        x = tf.nn.relu(x)\n\n        x = t.conv2d(x, f=f, k=3, s=1)\n        x = t.instance_norm(x, affine=True, name=scope_name + \'_1\')\n\n        return x\n\n\nclass StarGAN:\n\n    def __init__(self, s, batch_size=32, height=64, width=64, channel=3, attr_labels=(),\n                 sample_num=1 * 1, sample_size=1,\n                 df_dim=64, gf_dim=64, g_lr=1e-4, d_lr=1e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 32\n        :param height: input image height, default 4\n        :param width: input image width, default 64\n        :param channel: input image channel, default 3 (RGB)\n        - in case of Celeb-A, image size is 64x64x3(HWC).\n        :param attr_labels: attributes of Celeb-A image, default empty list\n        - in case of Celeb-A, the number of attributes is 40\n\n        # Output Settings\n        :param sample_num: the number of output images, default 1\n        :param sample_size: sample image size, default 64\n\n        # Hyper Parameters\n        :param df_dim: discriminator filter, default 64\n        :param gf_dim: generator filter, default 64\n\n        # Training Option\n        :param g_lr: generator learning rate, default 1e-4\n        :param d_lr: discriminator learning rate, default 1e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.height = height\n        self.width = width\n        self.channel = channel\n\n        self.attr_labels = attr_labels\n        self.n_classes = len(self.attr_labels)  # Select 10 of them\n        self.image_shape = [None, self.height, self.width, self.channel]\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n\n        # Model Hyper-Parameters\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n\n        self.d_lr = d_lr\n        self.g_lr = g_lr\n\n        self.lambda_cls = 1.   #\n        self.lambda_rec = 10.  #\n        self.lambda_gp = .25   # gradient penalty\n\n        # Training Setting\n        self.beta1 = 0.5\n        self.beta2 = 0.999\n\n        self.d_real = 0\n        self.d_fake = 0\n        self.g_loss = 0.\n        self.d_loss = 0.\n\n        # Placeholders\n        self.x_A = tf.placeholder(tf.float32,\n                                  shape=[None,\n                                         self.height, self.width, self.channel + self.n_classes],\n                                  name=\'x-image-A\')  # input image\n        self.x_B = tf.placeholder(tf.float32,\n                                  shape=[None,\n                                         self.height, self.width, self.channel + self.n_classes],\n                                  name=\'x-image-B\')  # target image\n        self.fake_x_B = tf.placeholder(tf.float32, shape=self.image_shape, name=\'x-image-fake-B\')\n        self.y_B = tf.placeholder(tf.float32, shape=[None, self.n_classes], name=\'y-label-B\')\n\n        self.lr_decay = tf.placeholder(tf.float32, shape=None, name=\'lr-decay\')\n        self.epsilon = tf.placeholder(tf.float32, shape=[None, 1, 1, 1], name=\'epsilon\')\n\n        # pre-defined\n        self.fake_A = None\n        self.fake_B = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        self.build_stargan()  # build StarGAN model\n\n    def discriminator(self, x, reuse=None):\n        """"""\n        :param x: images\n        :param reuse: re-usable\n        :return: logits\n        """"""\n        with tf.variable_scope(""discriminator"", reuse=reuse):\n            def conv_lrelu(x, f, k, s):\n                x = t.conv2d(x, f=f, k=k, s=s)\n                x = tf.nn.leaky_relu(x)\n                return x\n\n            for i in range(6):\n                x = conv_lrelu(x, f=self.df_dim * (2 ** (i + 1)), k=4, s=2)\n\n            x = t.conv2d(x, f=1 + self.n_classes, k=1, s=1)\n\n            x = tf.layers.flatten(x)  # (-1, 1 + n_classes_1)\n\n            out_real = x[:, 0]  # (-1, 1)\n            out_aux = x[:, 1:]  # (-1, n_classes_1)\n\n            return out_real, out_aux\n\n    def generator(self, x, reuse=None):\n        """"""\n        :param x: images\n        :param reuse: re-usable\n        :return: logits\n        """"""\n        with tf.variable_scope(""generator"", reuse=reuse):\n            def conv_in_relu(x, f, k, s, de=False, name=""""):\n                if not de:\n                    x = t.conv2d(x, f=f, k=k, s=s)\n                else:\n                    x = t.deconv2d(x, f=f, k=k, s=s)\n\n                x = t.instance_norm(x, name=name)\n                x = tf.nn.relu(x)\n                return x\n\n            x = conv_in_relu(x, f=self.gf_dim * 1, k=7, s=1, name=""1"")\n\n            # down-sampling\n            x = conv_in_relu(x, f=self.gf_dim * 2, k=4, s=2, name=""2"")\n            x = conv_in_relu(x, f=self.gf_dim * 4, k=4, s=2, name=""3"")\n\n            # bottleneck\n            for i in range(6):\n                x = residual_block(x, f=self.gf_dim * 4, name=str(i))\n\n            # up-sampling\n            x = conv_in_relu(x, self.gf_dim * 2, k=4, s=2, de=True, name=""4"")\n            x = conv_in_relu(x, self.gf_dim * 1, k=4, s=2, de=True, name=""5"")\n\n            x = t.deconv2d(x, f=3, k=7, s=1)\n            x = tf.nn.tanh(x)\n\n            return x\n\n    def build_stargan(self):\n        def gp_loss(real, fake, eps=self.epsilon):\n            # alpha = tf.random_uniform(shape=real.get_shape(), minval=0., maxval=1., name=\'alpha\')\n            # diff = fake - real  # fake data - real data\n            # interpolates = real + alpha * diff\n            interpolates = eps * real + (1. - eps) * fake\n            d_interp = self.discriminator(interpolates, reuse=True)\n            gradients = tf.gradients(d_interp, [interpolates])[0]\n            slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n            gradient_penalty = tf.reduce_mean(tf.square(slopes - 1.))\n            return gradient_penalty\n\n        x_img_a = self.x_A[:, :, :, :self.channel]\n        x_attr_a = self.x_A[:, :, :, self.channel:]\n        x_img_b = self.x_B[:, :, :, :self.channel]\n        # x_attr_b = self.x_B[:, :, :, self.channel:]\n\n        # Generator\n        self.fake_B = self.generator(self.x_A)\n        gen_in = tf.concat([self.fake_B, x_attr_a], axis=3)\n        self.fake_A = self.generator(gen_in, reuse=True)\n\n        # Discriminator\n        d_src_real_b, d_aux_real_b = self.discriminator(x_img_b)\n        g_src_fake_b, g_aux_fake_b = self.discriminator(self.fake_B, reuse=True)    # used at updating G net\n        d_src_fake_b, d_aux_fake_b = self.discriminator(self.fake_x_B, reuse=True)  # used at updating D net\n\n        # using WGAN-GP losses\n        gp = gp_loss(x_img_b, self.fake_x_B)\n        d_src_loss = tf.reduce_mean(d_src_fake_b) - tf.reduce_mean(d_src_real_b) + gp\n        d_aux_loss = t.sce_loss(d_aux_real_b, self.y_B)\n\n        self.d_loss = d_src_loss + self.lambda_cls * d_aux_loss\n        g_src_loss = -tf.reduce_mean(g_src_fake_b)\n        g_aux_fake_loss = t.sce_loss(g_aux_fake_b, self.y_B)\n        g_rec_loss = t.l1_loss(x_img_a, self.fake_A)\n        self.g_loss = g_src_loss + self.lambda_cls * g_aux_fake_loss + self.lambda_rec * g_rec_loss\n\n        # Summary\n        tf.summary.scalar(""loss/d_src_loss"", d_src_loss)\n        tf.summary.scalar(""loss/d_aux_loss"", d_aux_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_src_loss"", g_src_loss)\n        tf.summary.scalar(""loss/g_aux_fake_loss"", g_aux_fake_loss)\n        tf.summary.scalar(""loss/g_rec_loss"", g_rec_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.d_lr * self.lr_decay,\n                                           beta1=self.beta1).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.g_lr * self.lr_decay,\n                                           beta1=self.beta1).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
StarGAN/stargan_train.py,4,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport stargan_model as stargan\nfrom dataset import CelebADataSet as DataSet\n\nsys.path.append(\'../\')\nimport image_utils as iu\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/StarGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epoch\': 100,\n    \'batch_size\': 32,\n    \'logging_step\': 500,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # pre-chosen\n        attr_labels = [\n            \'Big_Nose\', \'Black_Hair\', \'Blond_Hair\', \'Blurry\', \'Brown_Hair\',\n            \'Bushy_Eyebrows\', \'Chubby\', \'Double_Chin\', \'Eyeglasses\', \'Gray_Hair\'\n        ]\n\n        # StarGAN Model\n        model = stargan.StarGAN(s, attr_labels=attr_labels)  # StarGAN\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        # loading CelebA DataSet\n        ds = DataSet(height=64,\n                     width=64,\n                     channel=3,\n                     ds_image_path=""D:/DataSet/CelebA/CelebA-64.h5"",\n                     ds_label_path=""D:/DataSet/CelebA/Anno/list_attr_celeba.txt"",\n                     # ds_image_path=""D:/DataSet/CelebA/Img/img_align_celeba/"",\n                     ds_type=""CelebA"",\n                     use_save=False,\n                     save_file_name=""D:/DataSet/CelebA/CelebA-64.h5"",\n                     save_type=""to_h5"",\n                     use_img_scale=False,\n                     img_scale=""-1,1"")\n\n        # x_A # Celeb-A\n        img_a = np.reshape(ds.images, [-1, 64, 64, 3])\n        attr_a = ds.labels\n\n        # x_B # Celeb-A # copied from x_A\n        # later it\'ll be replaced to another DataSet like RaFD, used in the paper\n        # but i can\'t find proper(good) DataSets, so i just do with single-domain (Celeb-A)\n        # img_b = img_a[:]\n        # attr_b = attr_a[:]\n\n        # ds_a_iter = DataIterator(img_a, attr_a, train_step[\'batch_size\'])\n        # ds_b_iter = DataIterator(img_b, attr_b, train_step[\'batch_size\'])\n\n        print(""[+] pre-processing elapsed time : {:.8f}s"".format(time.time() - start_time))\n        print(""[*] image_A     :"", img_a.shape, "" attribute A :"", attr_a.shape)\n\n        global_step = 0\n        for epoch in range(train_step[\'epoch\']):\n            # learning rate decay\n            lr_decay = 1.\n            if epoch >= train_step[\'epoch\']:\n                lr_decay = (train_step[\'epoch\'] - epoch) / (train_step[\'epoch\'] / 2.)\n\n            # re-implement DataIterator for multi-input\n            pointer = 0\n            for i in range(ds.num_images // train_step[\'batch_size\']):\n                start = pointer\n                pointer += train_step[\'batch_size\']\n\n                if pointer > ds.num_images:  # if ended 1 epoch\n                    # Shuffle training DataSet\n                    perm = np.arange(ds.num_images)\n                    np.random.shuffle(perm)\n\n                    # To-Do\n                    # Getting Proper DataSet\n                    img_a, img_b = img_a[perm], img_a[perm]\n                    attr_a, attr_b = attr_a[perm], attr_a[perm]\n\n                    start = 0\n                    pointer = train_step[\'batch_size\']\n\n                end = pointer\n\n                x_a, y_a = img_a[start:end], attr_a[start:end][:]\n                x_b, y_b = img_a[start:end], attr_a[start:end][:]\n\n                x_a = iu.transform(x_a, inv_type=\'127\')\n                x_b = iu.transform(x_b, inv_type=\'127\')\n\n                batch_a = ds.concat_data(x_a, y_a)\n                batch_b = ds.concat_data(x_b, y_b)\n                eps = np.random.rand(train_step[\'batch_size\'], 1, 1, 1)\n\n                # Generate fake_B\n                fake_b = s.run(model.fake_B, feed_dict={model.x_A: batch_a})\n\n                # Update D network - 5 times\n                for _ in range(5):\n                    _, d_loss = s.run([model.d_op, model.d_loss],\n                                      feed_dict={\n                                          model.x_B: batch_b,\n                                          model.y_B: y_b,\n                                          model.fake_x_B: fake_b,\n                                          model.lr_decay: lr_decay,\n                                          model.epsilon: eps,\n                                    })\n\n                # Update G network - 1 time\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.x_A: batch_a,\n                                      model.x_B: batch_b,\n                                      model.y_B: y_b,\n                                      model.lr_decay: lr_decay,\n                                      model.epsilon: eps,\n                                  })\n\n                if global_step % train_step[\'logging_step\'] == 0:\n                    eps = np.random.rand(train_step[\'batch_size\'], 1, 1, 1)\n\n                    # Summary\n                    samples, d_loss, g_loss, summary = s.run([model.fake_A, model.d_loss, model.g_loss, model.merged],\n                                                             feed_dict={\n                                                                 model.x_A: batch_a,\n                                                                 model.x_B: batch_b,\n                                                                 model.y_B: y_b,\n                                                                 model.fake_x_B: fake_b,\n                                                                 model.lr_decay: lr_decay,\n                                                                 model.epsilon: eps,\n                                                             })\n\n                    # Print loss\n                    print(""[+] Epoch %04d Step %07d =>"" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss))\n\n                    # Summary saver\n                    model.writer.add_summary(summary, epoch)\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{0}_{1}.png\'.format(epoch, global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step)\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
TempoGAN/tempogan_model.py,30,"b'import tensorflow as tf\n\n\ntf.set_random_seed(777)  # reproducibility\n\n\ndef conv2d(x, f=64, k=3, d=1, reg=5e-4, act=None, pad=\'SAME\', name=\'conv2d\'):\n    """"""\n    :param x: input\n    :param f: filters, default 64\n    :param k: kernel size, default 3\n    :param d: strides, default 2\n    :param reg: weight regularizer, default 5e-4\n    :param act: activation function, default elu\n    :param pad: padding (valid or same), default same\n    :param name: scope name, default conv2d\n    :return: conv2d net\n    """"""\n    return tf.layers.conv2d(x,\n                            filters=f, kernel_size=k, strides=d,\n                            # kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n                            # kernel_regularizer=tf.contrib.layers.l2_regularizer(reg),\n                            kernel_initializer=tf.truncated_normal_initializer(0., 0.02),\n                            bias_initializer=tf.zeros_initializer(),\n                            activation=act,\n                            padding=pad, name=name)\n\n\ndef resize_nn(x, size):\n    return tf.image.resize_nearest_neighbor(x, size=(int(size), int(size)))\n\n\nclass TempoGAN:\n\n    def __init__(self, s, batch_size=16, input_height=64, input_width=64, input_channel=3,\n                 sample_num=8 * 8, sample_size=8, output_height=64, output_width=64,\n                 df_dim=64, gf_dim=64,\n                 z_dim=256, g_lr=2e-4, d_lr=2e-4, epsilon=1e-9):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 16\n        :param input_height: input image height, default 64\n        :param input_width: input image width, default 64\n        :param input_channel: input image channel, default 3 (RGB)\n        - in case of Celeb-A, image size is 32x32x3/64x64x3(HWC).\n\n        # Output Settings\n        :param sample_num: the number of output images, default 64\n        :param sample_size: sample image size, default 64\n        :param output_height: output images height, default 64\n        :param output_width: output images width, default 64\n\n        # For CNN model\n        :param df_dim: discriminator filter, default 64\n        :param gf_dim: generator filter, default 64\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 256\n        :param g_lr: generator learning rate, default 2e-4\n        :param d_lr: discriminator learning rate, default 2e-4\n        :param epsilon: epsilon, default 1e-9\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n\n        self.input_height = input_height\n        self.input_width = input_width\n        self.input_channel = input_channel\n        self.image_shape = [self.batch_size, self.input_height, self.input_width, self.input_channel]\n\n        self.sample_num = sample_num\n        self.sample_size = sample_size\n        self.output_height = output_height\n        self.output_width = output_width\n\n        self.df_dim = df_dim\n        self.gf_dim = gf_dim\n\n        self.z_dim = z_dim\n        self.beta1 = .5\n        self.beta2 = .9\n        self.d_lr = tf.Variable(d_lr, name=\'d_lr\')\n        self.g_lr = tf.Variable(g_lr, name=\'g_lr\')\n        self.lr_decay_rate = .05\n        self.lr_low_boundary = 1e-5\n        self.eps = epsilon\n\n        # pre-defined\n        self.d_real = 0.\n        self.d_fake = 0.\n        self.g_loss = 0.\n        self.d_loss = 0.\n\n        self.g = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # LR/k update\n        self.k = tf.Variable(0., trainable=False, name=\'k_t\')  # 0 < k_t < 1, k_0 = 0\n\n        self.lr_update_step = 100000\n        self.d_lr_update = tf.assign(self.d_lr, tf.maximum(self.d_lr * self.lr_decay_rate, self.lr_low_boundary))\n        self.g_lr_update = tf.assign(self.g_lr, tf.maximum(self.g_lr * self.lr_decay_rate, self.lr_low_boundary))\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32,\n                                shape=[None, self.input_height, self.input_width, self.input_channel],\n                                name=""x-image"")  # (-1, 32 or 64, 32 or 64, 3)\n        self.z = tf.placeholder(tf.float32,\n                                shape=[None, self.z_dim],\n                                name=\'z-noise\')  # (-1, 128)\n\n        self.build_tempogan()  # build TempoGAN model\n\n    def discriminator_s(self, x, y=None, reuse=None):\n        """"""\n        :param x: images\n        :param reuse: re-usable\n        :return: logits\n        """"""\n        with tf.variable_scope(""discriminator_s"", reuse=reuse):\n\n            return x\n\n    def discriminator_t(self, x, y=None, reuse=None):\n        """"""\n        :param x: images\n        :param reuse: re-usable\n        :return: logits\n        """"""\n        with tf.variable_scope(""discriminator_t"", reuse=reuse):\n\n            return x\n\n    def generator(self, z, reuse=None):\n        """"""\n        :param z: embeddings\n        :param reuse: re-usable\n        :return: logits\n        """"""\n        with tf.variable_scope(""generator"", reuse=reuse):\n\n            return z\n\n    def build_tempogan(self):\n        def l1_loss(x, y):\n            return tf.reduce_mean(tf.abs(x - y))\n\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        d_real = self.discriminator_s(self.x)\n        d_fake = self.discriminator_s(self.g, reuse=True)\n\n        # Loss\n        d_real_loss = l1_loss(self.x, d_real)\n        d_fake_loss = l1_loss(self.g, d_fake)\n        self.d_loss = d_real_loss - self.k * d_fake_loss\n        self.g_loss = d_fake_loss\n\n        # Summary\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n        tf.summary.scalar(""misc/d_lr"", self.d_lr)\n        tf.summary.scalar(""misc/g_lr"", self.g_lr)\n\n        # Optimizer\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.d_lr_update,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.g_lr_update,\n                                           beta1=self.beta1, beta2=self.beta2).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
TempoGAN/tempogan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport tempogan_model as tempogan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CelebADataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'checkpoint\': \'./model/checkpoint\',\n    \'model\': \'./model/TempoGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epoch\': 24,\n    \'batch_size\': 16,\n    \'logging_step\': 2000,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # GPU configure\n    gpu_config = tf.GPUOptions(allow_growth=True)\n    config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_config)\n\n    with tf.Session(config=config) as s:\n        # TempoGAN Model\n        model = tempogan.TempoGAN(s)  # TempoGAN\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        # Celeb-A DataSet images\n        ds = DataSet(input_height=64,\n                     input_width=64,\n                     input_channel=3,\n                     ds_path=""/home/zero/hdd/DataSet/CelebA"").images\n        dataset_iter = DataIterator(ds, None, train_step[\'batch_size\'],\n                                    label_off=True)\n\n        sample_x = ds[:model.sample_num]\n        sample_x = np.reshape(sample_x, [-1] + model.image_shape[1:])\n        sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n\n        # Export real image\n        valid_image_height = model.sample_size\n        valid_image_width = model.sample_size\n        sample_dir = results[\'output\'] + \'valid.png\'\n\n        # Generated image save\n        iu.save_images(sample_x, size=[valid_image_height, valid_image_width], image_path=sample_dir,\n                       inv_type=\'127\')\n\n        global_step = 0\n        for epoch in range(train_step[\'epoch\']):\n            for batch_images in dataset_iter.iterate():\n                batch_x = np.reshape(batch_images, [-1] + model.image_shape[1:])\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update D network\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.z: batch_z,\n                                  })\n\n                # Update k_t\n                _, k, m_global = s.run([model.k_update, model.k, model.m_global],\n                                       feed_dict={\n                                            model.x: batch_x,\n                                            model.z: batch_z,\n                                       })\n\n                if global_step % train_step[\'logging_step\'] == 0:\n                    _, k, m_global, d_loss, g_loss, summary = s.run([model.k_update, model.k, model.m_global,\n                                                                     model.d_loss, model.g_loss, model.merged],\n                                                                    feed_dict={\n                                                                        model.x: batch_x,\n                                                                        model.z: batch_z,\n                                                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %03d Step %07d =>"" % (epoch, global_step),\n                          "" D loss : {:.6f}"".format(d_loss),\n                          "" G loss : {:.6f}"".format(g_loss),\n                          "" k : {:.6f}"".format(k),\n                          "" M : {:.6f}"".format(m_global))\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Training G model with sample image and noise\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.x: sample_x,\n                                        model.z: sample_z,\n                                    })\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{0}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step=global_step)\n\n                # Learning Rate update\n                if global_step and global_step % model.lr_update_step == 0:\n                    s.run([model.g_lr_update, model.d_lr_update])\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
UGAN/ugan_model.py,24,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)\n\n\nclass UGAN:\n\n    def __init__(self, s, batch_size=64, height=32, width=32, channel=3,\n                 sample_num=8 * 8, sample_size=8,\n                 z_dim=256, gf_dim=64, df_dim=64, d_lr=2e-4, g_lr=1e-4):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 64\n        :param height: input image height, default 32\n        :param width: input image width, default 32\n        :param channel: input image channel, default 3 (RGB)\n\n        # Output Settings\n        :param sample_num: the number of sample images, default 64\n        :param sample_size: sample image size, default 8\n\n        # Model Settings\n        :param z_dim: z noise dimension, default 256\n        :param gf_dim: the number of generator filters, default 64\n        :param df_dim: the number of discriminator filters, default 64\n\n        # Training Settings\n        :param d_lr: learning rate, default 2e-4\n        :param g_lr: learning rate, default 1e-4\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n        self.channel = channel\n\n        self.sample_size = sample_size\n        self.sample_num = sample_num\n\n        self.image_shape = [self.height, self.width, self.channel]\n\n        self.z_dim = z_dim\n\n        self.gf_dim = gf_dim\n        self.df_dim = df_dim\n\n        # pre-defined\n        self.d_loss = 0.\n        self.g_loss = 0.\n\n        self.g = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel], name=\'x-images\')\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name=\'z-noise\')\n\n        # Training Options\n        self.beta1 = 0.5\n        self.d_lr = d_lr\n        self.g_lr = g_lr\n\n        self.bulid_ugan()  # build UGAN model\n\n    def discriminator(self, x, reuse=None):\n        with tf.variable_scope(\'discriminator\', reuse=reuse):\n            for i in range(1, 4):\n                x = t.conv2d(x, self.gf_dim * (2 ** (i - 1)), 3, 2, name=\'disc-conv2d-%d\' % i)\n                x = t.batch_norm(x, name=\'disc-bn-%d\' % i)\n                x = tf.nn.leaky_relu(x, alpha=0.3)\n\n            x = tf.layers.flatten(x)\n\n            x = t.dense(x, 1, name=\'disc-fc-1\')\n            return x\n\n    def generator(self, z, reuse=None, is_train=True):\n        with tf.variable_scope(\'generator\', reuse=reuse):\n            x = t.dense(z, self.gf_dim * 8 * 4 * 4, name=\'gen-fc-1\')\n\n            x = tf.reshape(x, [-1, 4, 4, self.gf_dim * 8])\n            x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-1\')\n            x = tf.nn.relu(x)\n\n            for i in range(1, 4):\n                x = t.deconv2d(x, self.gf_dim * 4, 3, 2, name=\'gen-deconv2d-%d\' % i)\n                x = t.batch_norm(x, is_train=is_train, name=\'gen-bn-%d\' % (i + 1))\n                x = tf.nn.relu(x)\n\n            x = t.conv2d(x, self.channel, 3, name=\'gen-conv2d-1\')\n            x = tf.nn.sigmoid(x)\n            return x\n\n    def bulid_ugan(self):\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        d_real = self.discriminator(self.x)\n        d_fake = self.discriminator(self.g, reuse=True)\n\n        # Losses\n        d_real_loss = t.sce_loss(d_real, tf.ones_like(d_real))\n        d_fake_loss = t.sce_loss(d_fake, tf.zeros_like(d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n        self.g_loss = t.sce_loss(d_fake, tf.ones_like(d_fake))\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n\n        # Collect trainer values\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'d\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'g\')]\n\n        # Optimizer\n        self.d_op = tf.train.AdamOptimizer(learning_rate=self.d_lr,\n                                           beta1=self.beta1).minimize(self.d_loss, var_list=d_params)\n        self.g_op = tf.train.AdamOptimizer(learning_rate=self.g_lr,\n                                           beta1=self.beta1).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model Saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
UGAN/ugan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport ugan_model as ugan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CiFarDataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/UGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epoch\': 25,\n    \'batch_size\': 64,\n    \'global_steps\': 100001,\n    \'logging_interval\': 500,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # Loading Cifar-10 DataSet\n    ds = DataSet(height=32,\n                 width=32,\n                 channel=3,\n                 ds_path=""D:/DataSet/cifar/cifar-10-batches-py/"",\n                 ds_name=\'cifar-10\')\n\n    ds_iter = DataIterator(x=iu.transform(ds.train_images, \'255\'),\n                           y=ds.train_labels,\n                           batch_size=train_step[\'batch_size\'],\n                           label_off=True)  # using label # maybe someday, i\'ll change this param\'s name\n\n    # Generated image save\n    test_images = iu.transform(ds.test_images[:100], inv_type=\'255\')\n    iu.save_images(test_images,\n                   size=[10, 10],\n                   image_path=results[\'output\'] + \'sample.png\',\n                   inv_type=\'255\')\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # UGAN model\n        model = ugan.UGAN(s, batch_size=train_step[\'batch_size\'])\n\n        # Initializing variables\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %d"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        global_step = saved_global_step\n        start_epoch = global_step // (len(ds.train_images) // model.batch_size)  # recover n_epoch\n        ds_iter.pointer = saved_global_step % (len(ds.train_images) // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epoch\']):\n            for batch_x in ds_iter.iterate():\n                batch_x = np.reshape(batch_x, (model.batch_size, model.height, model.width, model.channel))\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update D network\n                _, d_loss = s.run([model.d_op, model.d_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z\n                                  })\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                if global_step % train_step[\'logging_interval\'] == 0:\n                    summary = s.run(model.merged,\n                                    feed_dict={\n                                        model.x: batch_x,\n                                        model.z: batch_z,\n                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %03d Step %05d => "" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss))\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim])\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.z: sample_z,\n                                    })\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{0}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'255\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step)\n\n                global_step += 1\n\n        end_time = time.time() - start_time  # Clocking end\n\n        # Elapsed time\n        print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n        # Close tf.Session\n        s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
WGAN/wgan_model.py,53,"b'import tensorflow as tf\n\nimport sys\n\nsys.path.append(\'../\')\nimport tfutil as t\n\n\ntf.set_random_seed(777)\n\n\nclass WGAN:\n\n    def __init__(self, s, batch_size=32, height=32, width=32, channel=3, n_classes=10,\n                 sample_num=8 * 8, sample_size=8,\n                 z_dim=128, gf_dim=64, df_dim=64, fc_unit=512,\n                 enable_gp=True):\n\n        """"""\n        # General Settings\n        :param s: TF Session\n        :param batch_size: training batch size, default 32\n        :param height: input image height, default 32\n        :param width: input image width, default 32\n        :param channel: input image channel, default 3\n        :param n_classes: input DataSet\'s classes\n\n        # Output Settings\n        :param sample_num: the number of output images, default 64\n        :param sample_size: sample image size, default 8\n\n        # Training Option\n        :param z_dim: z dimension (kinda noise), default 128\n        :param gf_dim: the number of generator filters, default 64\n        :param df_dim: the number of discriminator filters, default 64\n        :param fc_unit: the number of fc units, default 512\n        :param enable_gp: enabling gradient penalty, default True\n        """"""\n\n        self.s = s\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n        self.channel = channel\n        self.n_classes = n_classes\n\n        self.sample_size = sample_size\n        self.sample_num = sample_num\n\n        self.image_shape = [self.height, self.width, self.channel]\n\n        self.z_dim = z_dim\n        self.gf_dim = gf_dim\n        self.df_dim = df_dim\n        self.fc_unit = fc_unit\n\n        # Training Options - based on the WGAN paper\n        self.beta1 = 0.  # 0.5\n        self.beta2 = .9  # 0.999\n        self.lr = 1e-4\n        self.critic = 5\n        self.clip = .01\n        self.d_clip = []  # (-0.01 ~ 0.01)\n        self.d_lambda = 10.\n        self.decay = .9\n\n        self.EnableGP = enable_gp\n\n        # pre-defined\n        self.d_loss = 0.\n        self.g_loss = 0.\n        self.gradient_penalty = 0.\n\n        self.g = None\n\n        self.d_op = None\n        self.g_op = None\n\n        self.merged = None\n        self.writer = None\n        self.saver = None\n\n        # Placeholders\n        self.x = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel],\n                                name=\'x-images\')\n        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim],\n                                name=\'z-noise\')\n\n        self.build_wgan()  # build WGAN model\n\n    """"""ResNet like model for Cifar-10 DataSet\n    def mean_pool_conv(self, x, f, reuse=None, name=""""):\n        with tf.variable_scope(name, reuse=reuse):\n            x = tf.add_n([x[:, ::2, ::2, :], x[:, 1::2, ::2, :], x[:, ::2, 1::2, :], x[:, 1::2, 1::2, :]]) / 4.\n            x = t.conv2d(x, f, 3, 1)\n            return x\n\n    def conv_mean_pool(self, x, f, reuse=None, name=""""):\n        with tf.variable_scope(name, reuse=reuse):\n            x = t.conv2d(x, f, 3, 1)\n            x = tf.add_n([x[:, ::2, ::2, :], x[:, 1::2, ::2, :], x[:, ::2, 1::2, :], x[:, 1::2, 1::2, :]]) / 4.\n            return x\n\n    def upsample_conv(self, x, f, reuse=None, name=""""):\n        with tf.variable_scope(name, reuse=reuse):\n            x = tf.concat([x, x, x, x], axis=-1)\n            x = tf.depth_to_space(x, 2)\n            x = t.conv2d(x, f, 3, 1)\n            return x\n\n    def residual_block(self, x, f, sampling=None, reuse=None, name=""""):\n        with tf.variable_scope(name, reuse=reuse):\n            shortcut = tf.identity(x, name=(name + ""-"" + sampling + ""-identity""))\n\n            is_gen = name.startswith(\'gen\')\n\n            if is_gen:\n                x = t.batch_norm(x, name=\'%s-bn-1\' % (name + ""-"" + sampling))\n            x = tf.nn.relu(x)\n\n            if sampling == \'up\':\n                x = self.upsample_conv(x, f, reuse, name + ""-"" + sampling + ""-upsample_conv-1"")\n            elif sampling == \'down\' or sampling == \'none\':\n                x = t.conv2d(x, f, name=\'%s-conv2d-1\' % (name + ""-"" + sampling))\n\n            if is_gen:\n                x = t.batch_norm(x, name=\'%s-bn-2\' % (name + ""-"" + sampling))\n            x = tf.nn.relu(x)\n\n            if sampling == \'up\' or sampling == \'none\':\n                x = t.conv2d(x, f, name=\'%s-conv2d-2\' % (name + ""-"" + sampling))\n            elif sampling == \'down\':\n                x = self.conv_mean_pool(x, f, reuse, name + ""-"" + sampling + ""-conv_mean_pool-1"")\n\n            if sampling == \'up\':\n                shortcut = self.upsample_conv(shortcut, f, reuse, name + ""-"" + sampling + ""-upsample_conv-2"")\n            elif sampling == \'down\':\n                shortcut = self.conv_mean_pool(shortcut, f, reuse, name + ""-"" + sampling + ""-conv_mean_pool-2"")\n\n            return shortcut + x\n\n    def residual_block_init(self, x, f, reuse=None, name=""""):\n        with tf.variable_scope(name, reuse=reuse):\n            shortcut = tf.identity(x)\n\n            x = t.conv2d(x, f, 1, name=\'rb_init-conv2d-1\')\n            x = tf.nn.relu(x)\n\n            x = self.conv_mean_pool(x, f, reuse=reuse, name=\'rb_init-conv_mean_pool-1\')\n            shortcut = self.mean_pool_conv(shortcut, 1, reuse=reuse, name=\'rb_init-mean_pool_conv-1\')\n\n            return shortcut + x\n\n    def discriminator(self, x, reuse=None):\n        with tf.variable_scope(\'discriminator\', reuse=reuse):\n            x = self.residual_block_init(x, self.z_dim, reuse=reuse, name=\'disc-res_block_init\')\n\n            x = self.residual_block(x, self.z_dim, reuse=reuse, sampling=\'down\', name=\'disc-res_block-1\')\n            x = self.residual_block(x, self.z_dim, reuse=reuse, sampling=\'none\', name=\'disc-res_block-2\')\n            x = self.residual_block(x, self.z_dim, reuse=reuse, sampling=\'none\', name=\'disc-res_block-3\')\n\n            x = tf.nn.relu(x)\n\n            x = t.global_avg_pooling(x)\n\n            x = t.dense(x, 1, name=\'disc-fc-1\')\n            return x\n\n    def generator(self, z, reuse=None):\n        with tf.variable_scope(\'generator\', reuse=reuse):\n            x = t.dense(z, self.z_dim * 4 * 4, name=""gen-fc-1"")\n            x = tf.reshape(x, [-1, 4, 4, self.z_dim])\n\n            for i in range(1, 4):\n                x = self.residual_block(x, self.z_dim, sampling=\'up\', name=\'gen-res_block-%d\' % i)\n\n            x = t.batch_norm(x, name=\'gen-bn-1\')\n            x = tf.nn.relu(x)\n\n            x = t.conv2d(x, 3, 3, 1, name=""gen-conv2d-1"")\n            x = tf.nn.tanh(x)\n            return x\n    """"""\n\n    def generator(self, z, reuse=None):\n        with tf.variable_scope(\'generator\', reuse=reuse):\n            x = t.dense(z, self.gf_dim * 4 * 4 * 4, name=\'gen-fc-1\')\n            x = t.batch_norm(x, reuse=reuse, name=\'gen-bn-1\')\n            x = tf.nn.relu(x)\n\n            x = tf.reshape(x, (-1, 4, 4, self.gf_dim * 4))\n\n            for i in range(1, 4):\n                x = t.deconv2d(x, self.gf_dim * 4 // (2 ** (i - 1)), 5, 2, name=\'gen-deconv2d-%d\' % i)\n                x = t.batch_norm(x, reuse=reuse, name=\'gen-bn-%d\' % (i + 1))\n                x = tf.nn.relu(x)\n\n            x = t.deconv2d(x, self.channel, 5, 1, name=\'gen-deconv2d-5\')\n            x = tf.nn.tanh(x)\n            return x\n\n    def discriminator(self, x, reuse=None):\n        with tf.variable_scope(\'discriminator\', reuse=reuse):\n            x = tf.reshape(x, (-1, self.height, self.width, self.channel))\n\n            x = t.conv2d(x, self.df_dim, 5, 2, name=\'disc-conv2d-1\')\n            x = tf.nn.leaky_relu(x)\n\n            for i in range(1, 3):\n                x = t.conv2d(x, self.df_dim, 5, 2, name=\'disc-conv2d-%d\' % (i + 1))\n                x = t.batch_norm(x, reuse=reuse, name=\'disc-bn-%d\' % i)\n                x = tf.nn.leaky_relu(x)\n\n            x = t.flatten(x)\n\n            x = t.dense(x, 1, name=\'disc-fc-1\')\n            return x\n\n    def build_wgan(self):\n        # Generator\n        self.g = self.generator(self.z)\n\n        # Discriminator\n        d_real = self.discriminator(self.x)\n        d_fake = self.discriminator(self.g, reuse=True)\n\n        # Losses\n        d_real_loss = t.sce_loss(d_real, tf.ones_like(d_real))\n        d_fake_loss = t.sce_loss(d_fake, tf.zeros_like(d_fake))\n        self.d_loss = d_real_loss + d_fake_loss\n        self.g_loss = t.sce_loss(d_fake, tf.ones_like(d_fake))\n\n        # The gradient penalty loss\n        if self.EnableGP:\n            alpha = tf.random_uniform(shape=[self.batch_size, 1, 1, 1], minval=0., maxval=1., name=\'alpha\')\n            diff = self.g - self.x  # fake data - real data\n            interpolates = self.x + alpha * diff\n            d_interp = self.discriminator(interpolates, reuse=True)\n            gradients = tf.gradients(d_interp, [interpolates])[0]\n            slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n            self.gradient_penalty = tf.reduce_mean(tf.square(slopes - 1.))\n\n            # Update D loss\n            self.d_loss += self.d_lambda * self.gradient_penalty\n\n        # Summary\n        tf.summary.scalar(""loss/d_real_loss"", d_real_loss)\n        tf.summary.scalar(""loss/d_fake_loss"", d_fake_loss)\n        tf.summary.scalar(""loss/d_loss"", self.d_loss)\n        tf.summary.scalar(""loss/g_loss"", self.g_loss)\n        if self.EnableGP:\n            tf.summary.scalar(""misc/gp"", self.gradient_penalty)\n\n        # Collect trainer values\n        t_vars = tf.trainable_variables()\n        d_params = [v for v in t_vars if v.name.startswith(\'discriminator\')]\n        g_params = [v for v in t_vars if v.name.startswith(\'generator\')]\n\n        if not self.EnableGP:\n            self.d_clip = [v.assign(tf.clip_by_value(v, -self.clip, self.clip)) for v in d_params]\n\n        # Optimizer\n        if self.EnableGP:\n            self.d_op = tf.train.AdamOptimizer(learning_rate=self.lr * 2,\n                                               beta1=self.beta1, beta2=self.beta2).minimize(loss=self.d_loss,\n                                                                                            var_list=d_params)\n            self.g_op = tf.train.AdamOptimizer(learning_rate=self.lr * 2,\n                                               beta1=self.beta1, beta2=self.beta2).minimize(loss=self.g_loss,\n                                                                                            var_list=g_params)\n        else:\n            self.d_op = tf.train.RMSPropOptimizer(learning_rate=self.lr,\n                                                  decay=self.decay).minimize(self.d_loss, var_list=d_params)\n            self.g_op = tf.train.RMSPropOptimizer(learning_rate=self.lr,\n                                                  decay=self.decay).minimize(self.g_loss, var_list=g_params)\n\n        # Merge summary\n        self.merged = tf.summary.merge_all()\n\n        # Model Saver\n        self.saver = tf.train.Saver(max_to_keep=1)\n        self.writer = tf.summary.FileWriter(\'./model/\', self.s.graph)\n'"
WGAN/wgan_train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nimport sys\nimport time\n\nimport wgan_model as wgan\n\nsys.path.append(\'../\')\nimport image_utils as iu\nfrom datasets import DataIterator\nfrom datasets import CiFarDataSet as DataSet\n\n\nresults = {\n    \'output\': \'./gen_img/\',\n    \'model\': \'./model/WGAN-model.ckpt\'\n}\n\ntrain_step = {\n    \'epochs\': 100,\n    \'batch_size\': 64,\n    \'global_step\': 50001,\n    \'logging_interval\': 500,\n}\n\n\ndef main():\n    start_time = time.time()  # Clocking start\n\n    # Loading Cifar-10 DataSet\n    ds = DataSet(height=32,\n                 width=32,\n                 channel=3,\n                 ds_path=""D:/DataSet/cifar/cifar-10-batches-py/"",\n                 ds_name=\'cifar-10\')\n\n    ds_iter = DataIterator(x=iu.transform(ds.train_images, \'127\'),\n                           y=ds.train_labels,\n                           batch_size=train_step[\'batch_size\'],\n                           label_off=True)  # using label # maybe someday, i\'ll change this param\'s name\n\n    # Generated image save\n    test_images = iu.transform(ds.test_images[:100], inv_type=\'127\')\n    iu.save_images(test_images,\n                   size=[10, 10],\n                   image_path=results[\'output\'] + \'sample.png\',\n                   inv_type=\'127\')\n\n    # GPU configure\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as s:\n        # WGAN Model\n        model = wgan.WGAN(s,\n                          batch_size=train_step[\'batch_size\'],\n                          height=32,\n                          width=32,\n                          channel=3,\n                          enable_gp=True)  # WGAN-GP\n\n        # Initializing\n        s.run(tf.global_variables_initializer())\n\n        # Load model & Graph & Weights\n        saved_global_step = 0\n        ckpt = tf.train.get_checkpoint_state(\'./model/\')\n        if ckpt and ckpt.model_checkpoint_path:\n            model.saver.restore(s, ckpt.model_checkpoint_path)\n\n            saved_global_step = int(ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1])\n            print(""[+] global step : %s"" % saved_global_step, "" successfully loaded"")\n        else:\n            print(\'[-] No checkpoint file found\')\n\n        global_step = saved_global_step\n        start_epoch = global_step // (len(ds.train_images) // model.batch_size)           # recover n_epoch\n        ds_iter.pointer = saved_global_step % (len(ds.train_images) // model.batch_size)  # recover n_iter\n        for epoch in range(start_epoch, train_step[\'epochs\']):\n            for _ in range(ds_iter.num_batches):\n                # Update critic\n                model.critic = 5\n                if global_step % 500 == 0 or global_step < 25:\n                    model.critic = 100\n                if model.EnableGP:\n                    model.critic = 1\n\n                for _ in range(model.critic):\n                    batch_x = ds_iter.next_batch()\n                    batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                    # Update d_clip\n                    if not model.EnableGP:\n                        s.run(model.d_clip)\n\n                    # Update D network\n                    _, d_loss = s.run([model.d_op, model.d_loss],\n                                      feed_dict={\n                                          model.x: batch_x,\n                                          model.z: batch_z,\n                                      })\n\n                batch_x = ds_iter.next_batch()\n                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)\n\n                # Update G network\n                _, g_loss = s.run([model.g_op, model.g_loss],\n                                  feed_dict={\n                                      model.x: batch_x,\n                                      model.z: batch_z,\n                                  })\n\n                # Logging\n                if global_step % train_step[\'logging_interval\'] == 0:\n                    summary = s.run(model.merged,\n                                    feed_dict={\n                                        model.x: batch_x,\n                                        model.z: batch_z,\n                                    })\n\n                    # Print loss\n                    print(""[+] Epoch %04d Step %08d => "" % (epoch, global_step),\n                          "" D loss : {:.8f}"".format(d_loss),\n                          "" G loss : {:.8f}"".format(g_loss))\n\n                    # Training G model with sample image and noise\n                    sample_z = np.random.uniform(-1., 1., [model.sample_num, model.z_dim]).astype(np.float32)\n                    samples = s.run(model.g,\n                                    feed_dict={\n                                        model.z: sample_z,\n                                    })\n\n                    # Summary saver\n                    model.writer.add_summary(summary, global_step)\n\n                    # Export image generated by model G\n                    sample_image_height = model.sample_size\n                    sample_image_width = model.sample_size\n                    sample_dir = results[\'output\'] + \'train_{:08d}.png\'.format(global_step)\n\n                    # Generated image save\n                    iu.save_images(samples,\n                                   size=[sample_image_height, sample_image_width],\n                                   image_path=sample_dir,\n                                   inv_type=\'127\')\n\n                    # Model save\n                    model.saver.save(s, results[\'model\'], global_step)\n\n                global_step += 1\n\n    end_time = time.time() - start_time  # Clocking end\n\n    # Elapsed time\n    print(""[+] Elapsed time {:.8f}s"".format(end_time))\n\n    # Close tf.Session\n    s.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
