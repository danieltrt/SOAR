file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n\n#\n# created by kpe on 22.10.2018 at 11:46\n#\n\nfrom setuptools import setup, find_packages, convert_path\n\n\ndef _version():\n    ns = {}\n    with open(convert_path(""bert/version.py""), ""r"") as fh:\n        exec(fh.read(), ns)\n    return ns[\'__version__\']\n\n\n__version__ = _version()\n\n\nwith open(""README.rst"", ""r"", encoding=""utf-8"") as fh:\n    long_description = fh.read()\n\nwith open(""requirements.txt"", ""r"") as reader:\n    install_requires = list(map(lambda x: x.strip(), reader.readlines()))\n\nsetup(name=""bert-for-tf2"",\n      version=__version__,\n      url=""https://github.com/kpe/bert-for-tf2/"",\n      description=""A TensorFlow 2.0 Keras implementation of BERT."",\n      long_description=long_description,\n      long_description_content_type=""text/x-rst"",\n      keywords=""tensorflow keras bert"",\n      license=""MIT"",\n\n      author=""kpe"",\n      author_email=""kpe.git@gmailbox.org"",\n      packages=find_packages(exclude=[""tests""]),\n      package_data={"""": [""*.txt"", ""*.rst""]},\n\n      zip_safe=True,\n      install_requires=install_requires,\n      python_requires="">=3.5"",\n      classifiers=[\n          ""Development Status :: 5 - Production/Stable"",\n          ""License :: OSI Approved :: MIT License"",\n          ""Programming Language :: Python"",\n          ""Programming Language :: Python :: 3"",\n          ""Programming Language :: Python :: 3.5"",\n          ""Programming Language :: Python :: 3.6"",\n          ""Programming Language :: Python :: 3.7"",\n          ""Programming Language :: Python :: Implementation :: CPython"",\n          ""Programming Language :: Python :: Implementation :: PyPy""])\n'"
bert/__init__.py,0,"b'# coding=utf-8\n#\n# created by kpe on 15.Mar.2019 at 15:28\n#\nfrom __future__ import division, absolute_import, print_function\n\nfrom .version import __version__\n\nfrom .attention import AttentionLayer\nfrom .layer import Layer\nfrom .model import BertModelLayer\n\nfrom .tokenization import bert_tokenization\nfrom .tokenization import albert_tokenization\n\nfrom .loader import StockBertConfig, load_stock_weights, params_from_pretrained_ckpt\nfrom .loader import load_stock_weights as load_bert_weights\nfrom .loader import bert_models_google, fetch_google_bert_model\nfrom .loader_albert import load_albert_weights, albert_params\nfrom .loader_albert import albert_models_tfhub, albert_models_brightmart\nfrom .loader_albert import fetch_tfhub_albert_model, fetch_brightmart_albert_model, fetch_google_albert_model\n'"
bert/attention.py,17,"b'# coding=utf-8\n#\n# created by kpe on 15.Mar.2019 at 12:52\n#\n\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras import backend as K\n\nfrom bert.layer import Layer\n\n\nclass AttentionLayer(Layer):\n    class Params(Layer.Params):\n        num_heads         = None\n        size_per_head     = None\n        initializer_range = 0.02\n        query_activation  = None\n        key_activation    = None\n        value_activation  = None\n        attention_dropout = 0.1\n        negative_infinity = -10000.0  # used for attention scores before softmax\n\n    @staticmethod\n    def create_attention_mask(from_shape, input_mask):\n        """"""\n        Creates 3D attention.\n        :param from_shape:  [batch_size, from_seq_len, ...]\n        :param input_mask:  [batch_size, seq_len]\n        :return: [batch_size, from_seq_len, seq_len]\n        """"""\n\n        mask = tf.cast(tf.expand_dims(input_mask, axis=1), tf.float32)                   # [B, 1, T]\n        ones = tf.expand_dims(tf.ones(shape=from_shape[:2], dtype=tf.float32), axis=-1)  # [B, F, 1]\n        mask = ones * mask  # broadcast along two dimensions\n\n        return mask  # [B, F, T]\n\n    def _construct(self, **kwargs):\n        super()._construct(**kwargs)\n        self.query_activation = self.params.query_activation\n        self.key_activation   = self.params.key_activation\n        self.value_activation = self.params.value_activation\n\n        self.query_layer = None\n        self.key_layer   = None\n        self.value_layer = None\n\n        self.supports_masking = True\n\n    # noinspection PyAttributeOutsideInit\n    def build(self, input_shape):\n        self.input_spec = keras.layers.InputSpec(shape=input_shape)\n\n        dense_units = self.params.num_heads * self.params.size_per_head  # N*H\n        #\n        # B, F, T, N, H - batch, from_seq_len, to_seq_len, num_heads, size_per_head\n        #\n        self.query_layer = keras.layers.Dense(units=dense_units, activation=self.query_activation,\n                                              kernel_initializer=self.create_initializer(),\n                                              name=""query"")\n        self.key_layer   = keras.layers.Dense(units=dense_units, activation=self.key_activation,\n                                              kernel_initializer=self.create_initializer(),\n                                              name=""key"")\n        self.value_layer = keras.layers.Dense(units=dense_units, activation=self.value_activation,\n                                              kernel_initializer=self.create_initializer(),\n                                              name=""value"")\n        self.dropout_layer = keras.layers.Dropout(self.params.attention_dropout)\n\n        super(AttentionLayer, self).build(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        from_shape = input_shape\n\n        # from_shape         # [B, F, W]   [batch_size, from_seq_length, from_width]\n        # input_mask_shape   # [B, F]\n\n        output_shape = [from_shape[0], from_shape[1], self.params.num_heads * self.params.size_per_head]\n\n        return output_shape  # [B, F, N*H]\n\n    # noinspection PyUnusedLocal\n    def call(self, inputs, mask=None, training=None, **kwargs):\n        from_tensor = inputs\n        to_tensor   = inputs\n        if mask is None:\n            sh = self.get_shape_list(from_tensor)\n            mask = tf.ones(sh[:2], dtype=tf.int32)\n        attention_mask = AttentionLayer.create_attention_mask(tf.shape(input=from_tensor), mask)\n\n        #  from_tensor shape - [batch_size, from_seq_length, from_width]\n        input_shape  = tf.shape(input=from_tensor)\n        batch_size, from_seq_len, from_width = input_shape[0], input_shape[1], input_shape[2]\n        to_seq_len = from_seq_len\n\n        # [B, F, N*H] -> [B, N, F, H]\n        def transpose_for_scores(input_tensor, seq_len):\n            output_shape = [batch_size, seq_len,\n                            self.params.num_heads, self.params.size_per_head]\n            output_tensor = K.reshape(input_tensor, output_shape)\n            return tf.transpose(a=output_tensor, perm=[0, 2, 1, 3])  # [B,N,F,H]\n\n        query = self.query_layer(from_tensor)  # [B,F, N*H] [batch_size, from_seq_len, N*H]\n        key   = self.key_layer(to_tensor)      # [B,T, N*H]\n        value = self.value_layer(to_tensor)    # [B,T, N*H]\n\n        query = transpose_for_scores(query, from_seq_len)           # [B, N, F, H]\n        key   = transpose_for_scores(key,   to_seq_len)             # [B, N, T, H]\n\n        attention_scores = tf.matmul(query, key, transpose_b=True)  # [B, N, F, T]\n        attention_scores = attention_scores / tf.sqrt(float(self.params.size_per_head))\n\n        if attention_mask is not None:\n            attention_mask = tf.expand_dims(attention_mask, axis=1)  # [B, 1, F, T]\n            # {1, 0} -> {0.0, -inf}\n            adder = (1.0 - tf.cast(attention_mask, tf.float32)) * self.params.negative_infinity\n            attention_scores = tf.add(attention_scores, adder)  # adding to softmax -> its like removing them entirely\n\n        # scores to probabilities\n        attention_probs = tf.nn.softmax(attention_scores)           # [B, N, F, T]\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout_layer(attention_probs,\n                                             training=training)    # [B, N, F, T]\n\n        # [B,T,N,H]\n        value = tf.reshape(value, [batch_size, to_seq_len,\n                                   self.params.num_heads, self.params.size_per_head])\n        value = tf.transpose(a=value, perm=[0, 2, 1, 3])                                # [B, N, T, H]\n\n        context_layer = tf.matmul(attention_probs, value)                               # [B, N, F, H]\n        context_layer = tf.transpose(a=context_layer, perm=[0, 2, 1, 3])                # [B, F, N, H]\n\n        output_shape = [batch_size, from_seq_len,\n                        self.params.num_heads * self.params.size_per_head]\n        context_layer = tf.reshape(context_layer, output_shape)\n        return context_layer                                                            # [B, F, N*H]\n\n    # noinspection PyUnusedLocal\n    def compute_mask(self, inputs, mask=None):\n        return mask   # [B, F]\n\n'"
bert/embeddings.py,13,"b'# coding=utf-8\n#\n# created by kpe on 28.Mar.2019 at 12:33\n#\n\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\nimport params_flow as pf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\n\nimport bert\n\n\nclass PositionEmbeddingLayer(bert.Layer):\n    class Params(bert.Layer.Params):\n        max_position_embeddings  = 512\n        hidden_size              = 128\n\n    # noinspection PyUnusedLocal\n    def _construct(self, **kwargs):\n        super()._construct(**kwargs)\n        self.embedding_table = None\n\n    # noinspection PyAttributeOutsideInit\n    def build(self, input_shape):\n        # input_shape: () of seq_len\n        if input_shape is not None:\n            assert input_shape.ndims == 0\n            self.input_spec = keras.layers.InputSpec(shape=input_shape, dtype=\'int32\')\n        else:\n            self.input_spec = keras.layers.InputSpec(shape=(), dtype=\'int32\')\n\n        self.embedding_table = self.add_weight(name=""embeddings"",\n                                               dtype=K.floatx(),\n                                               shape=[self.params.max_position_embeddings, self.params.hidden_size],\n                                               initializer=self.create_initializer())\n        super(PositionEmbeddingLayer, self).build(input_shape)\n\n    # noinspection PyUnusedLocal\n    def call(self, inputs, **kwargs):\n        # just return the embedding after verifying\n        # that seq_len is less than max_position_embeddings\n        seq_len = inputs\n\n        assert_op = tf.compat.v2.debugging.assert_less_equal(seq_len, self.params.max_position_embeddings)\n\n        with tf.control_dependencies([assert_op]):\n            # slice to seq_len\n            full_position_embeddings = tf.slice(self.embedding_table,\n                                                [0, 0],\n                                                [seq_len, -1])\n        output = full_position_embeddings\n        return output\n\n\nclass EmbeddingsProjector(bert.Layer):\n    class Params(bert.Layer.Params):\n        hidden_size                  = 768\n        embedding_size               = None   # None for BERT, not None for ALBERT\n        project_embeddings_with_bias = True   # in ALBERT - True for Google, False for brightmart/albert_zh\n\n    # noinspection PyUnusedLocal\n    def _construct(self, **kwargs):\n        super()._construct(**kwargs)\n        self.projector_layer      = None   # for ALBERT\n        self.projector_bias_layer = None   # for ALBERT\n\n    def build(self, input_shape):\n        emb_shape = input_shape\n        self.input_spec = keras.layers.InputSpec(shape=emb_shape)\n        assert emb_shape[-1] == self.params.embedding_size\n\n        # ALBERT word embeddings projection\n        self.projector_layer = self.add_weight(name=""projector"",\n                                               shape=[self.params.embedding_size,\n                                                      self.params.hidden_size],\n                                               dtype=K.floatx())\n        if self.params.project_embeddings_with_bias:\n            self.projector_bias_layer = self.add_weight(name=""bias"",\n                                                        shape=[self.params.hidden_size],\n                                                        dtype=K.floatx())\n        super(EmbeddingsProjector, self).build(input_shape)\n\n    def call(self, inputs, **kwargs):\n        input_embedding = inputs\n        assert input_embedding.shape[-1] == self.params.embedding_size\n\n        # ALBERT: project embedding to hidden_size\n        output = tf.matmul(input_embedding, self.projector_layer)\n        if self.projector_bias_layer is not None:\n            output = tf.add(output, self.projector_bias_layer)\n\n        return output\n\n\nclass BertEmbeddingsLayer(bert.Layer):\n    class Params(PositionEmbeddingLayer.Params,\n                 EmbeddingsProjector.Params):\n        vocab_size               = None\n        use_token_type           = True\n        use_position_embeddings  = True\n        token_type_vocab_size    = 2\n        hidden_size              = 768\n        hidden_dropout           = 0.1\n\n        extra_tokens_vocab_size  = None  # size of the extra (task specific) token vocabulary (using negative token ids)\n\n        #\n        # ALBERT support - set embedding_size (or None for BERT)\n        #\n        embedding_size               = None   # None for BERT, not None for ALBERT\n        project_embeddings_with_bias = True   # in ALBERT - True for Google, False for brightmart/albert_zh\n        project_position_embeddings  = True   # in ALEBRT - True for Google, False for brightmart/albert_zh\n\n        mask_zero                    = False\n\n    # noinspection PyUnusedLocal\n    def _construct(self, **kwargs):\n        super()._construct(**kwargs)\n        self.word_embeddings_layer       = None\n        self.extra_word_embeddings_layer = None   # for task specific tokens (negative token ids)\n        self.token_type_embeddings_layer = None\n        self.position_embeddings_layer   = None\n        self.word_embeddings_projector_layer = None   # for ALBERT\n        self.layer_norm_layer = None\n        self.dropout_layer    = None\n\n        self.support_masking = self.params.mask_zero\n\n    # noinspection PyAttributeOutsideInit\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            assert len(input_shape) == 2\n            input_ids_shape, token_type_ids_shape = input_shape\n            self.input_spec = [keras.layers.InputSpec(shape=input_ids_shape),\n                               keras.layers.InputSpec(shape=token_type_ids_shape)]\n        else:\n            input_ids_shape = input_shape\n            self.input_spec = keras.layers.InputSpec(shape=input_ids_shape)\n\n        # use either hidden_size for BERT or embedding_size for ALBERT\n        embedding_size = self.params.hidden_size if self.params.embedding_size is None else self.params.embedding_size\n\n        self.word_embeddings_layer = keras.layers.Embedding(\n            input_dim=self.params.vocab_size,\n            output_dim=embedding_size,\n            mask_zero=self.params.mask_zero,\n            name=""word_embeddings""\n        )\n        if self.params.extra_tokens_vocab_size is not None:\n            self.extra_word_embeddings_layer = keras.layers.Embedding(\n                input_dim=self.params.extra_tokens_vocab_size + 1,  # +1 is for a <pad>/0 vector\n                output_dim=embedding_size,\n                mask_zero=self.params.mask_zero,\n                embeddings_initializer=self.create_initializer(),\n                name=""extra_word_embeddings""\n            )\n\n        # ALBERT word embeddings projection\n        if self.params.embedding_size is not None:\n            self.word_embeddings_projector_layer = EmbeddingsProjector.from_params(\n                self.params, name=""word_embeddings_projector"")\n\n        position_embedding_size = embedding_size if self.params.project_position_embeddings else self.params.hidden_size\n\n        if self.params.use_token_type:\n            self.token_type_embeddings_layer = keras.layers.Embedding(\n                input_dim=self.params.token_type_vocab_size,\n                output_dim=position_embedding_size,\n                mask_zero=False,\n                name=""token_type_embeddings""\n            )\n        if self.params.use_position_embeddings:\n            self.position_embeddings_layer = PositionEmbeddingLayer.from_params(\n                self.params,\n                name=""position_embeddings"",\n                hidden_size=position_embedding_size\n            )\n\n        self.layer_norm_layer = pf.LayerNormalization(name=""LayerNorm"")\n        self.dropout_layer    = keras.layers.Dropout(rate=self.params.hidden_dropout)\n\n        super(BertEmbeddingsLayer, self).build(input_shape)\n\n    def call(self, inputs, mask=None, training=None):\n        if isinstance(inputs, list):\n            assert 2 == len(inputs), ""Expecting inputs to be a [input_ids, token_type_ids] list""\n            input_ids, token_type_ids = inputs\n        else:\n            input_ids      = inputs\n            token_type_ids = None\n\n        input_ids = tf.cast(input_ids, dtype=tf.int32)\n\n        if self.extra_word_embeddings_layer is not None:\n            token_mask   = tf.cast(tf.greater_equal(input_ids, 0), tf.int32)\n            extra_mask   = tf.cast(tf.less(input_ids, 0), tf.int32)\n            token_ids    = token_mask * input_ids\n            extra_tokens = extra_mask * (-input_ids)\n            token_output = self.word_embeddings_layer(token_ids)\n            extra_output = self.extra_word_embeddings_layer(extra_tokens)\n            embedding_output = tf.add(token_output,\n                                      extra_output * tf.expand_dims(tf.cast(extra_mask, K.floatx()), axis=-1))\n        else:\n            embedding_output = self.word_embeddings_layer(input_ids)\n\n        # ALBERT: for brightmart/albert_zh weights - project only token embeddings\n        if not self.params.project_position_embeddings:\n            if self.word_embeddings_projector_layer:\n                embedding_output = self.word_embeddings_projector_layer(embedding_output)\n\n        if token_type_ids is not None:\n            token_type_ids    = tf.cast(token_type_ids, dtype=tf.int32)\n            embedding_output += self.token_type_embeddings_layer(token_type_ids)\n\n        if self.position_embeddings_layer is not None:\n            seq_len  = input_ids.shape.as_list()[1]\n            emb_size = embedding_output.shape[-1]\n\n            pos_embeddings = self.position_embeddings_layer(seq_len)\n            # broadcast over all dimension except the last two [..., seq_len, width]\n            broadcast_shape = [1] * (embedding_output.shape.ndims - 2) + [seq_len, emb_size]\n            embedding_output += tf.reshape(pos_embeddings, broadcast_shape)\n\n        embedding_output = self.layer_norm_layer(embedding_output)\n        embedding_output = self.dropout_layer(embedding_output, training=training)\n\n        # ALBERT: for google-research/albert weights - project all embeddings\n        if self.params.project_position_embeddings:\n            if self.word_embeddings_projector_layer:\n                embedding_output = self.word_embeddings_projector_layer(embedding_output)\n\n        return embedding_output   # [B, seq_len, hidden_size]\n\n    def compute_mask(self, inputs, mask=None):\n        if isinstance(inputs, list):\n            assert 2 == len(inputs), ""Expecting inputs to be a [input_ids, token_type_ids] list""\n            input_ids, token_type_ids = inputs\n        else:\n            input_ids      = inputs\n            token_type_ids = None\n\n        if not self.support_masking:\n            return None\n\n        return tf.not_equal(input_ids, 0)\n'"
bert/layer.py,3,"b'# coding=utf-8\n#\n# created by kpe on 28.Mar.2019 at 12:46\n#\n\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\n\nimport params_flow as pf\nfrom params_flow.activations import gelu\n\n\nclass Layer(pf.Layer):\n    """""" Common abstract base layer for all BERT layers. """"""\n    class Params(pf.Layer.Params):\n        initializer_range = 0.02\n\n    def create_initializer(self):\n        return tf.keras.initializers.TruncatedNormal(stddev=self.params.initializer_range)\n\n    @staticmethod\n    def get_activation(activation_string):\n        if not isinstance(activation_string, str):\n            return activation_string\n        if not activation_string:\n            return None\n\n        act = activation_string.lower()\n        if act == ""linear"":\n            return None\n        elif act == ""relu"":\n            return tf.nn.relu\n        elif act == ""gelu"":\n            return gelu\n        elif act == ""tanh"":\n            return tf.tanh\n        else:\n            raise ValueError(""Unsupported activation: %s"" % act)\n'"
bert/loader.py,4,"b'# coding=utf-8\n#\n# created by kpe on 28.Mar.2019 at 14:01\n#\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport re\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport params_flow as pf\nimport params\n\nfrom bert.model import BertModelLayer\n\nbert_models_google = {\n    ""uncased_L-12_H-768_A-12"":      ""https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"",\n    ""uncased_L-24_H-1024_A-16"":     ""https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip"",\n    ""cased_L-12_H-768_A-12"":        ""https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip"",\n    ""cased_L-24_H-1024_A-16"":       ""https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip"",\n    ""multi_cased_L-12_H-768_A-12"":  ""https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip"",\n    ""multilingual_L-12_H-768_A-12"": ""https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip"",\n    ""chinese_L-12_H-768_A-12"":  ""https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip"",\n    ""wwm_uncased_L-24_H-1024_A-16"": ""https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip"",\n    ""wwm_cased_L-24_H-1024_A-16"":   ""https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip"",\n}\n\n\ndef fetch_google_bert_model(model_name: str, fetch_dir: str):\n    if model_name not in bert_models_google:\n        raise ValueError(""BERT model with name:[{}] not found, try one of:{}"".format(\n            model_name, bert_models_google))\n    else:\n        fetch_url = bert_models_google[model_name]\n\n    fetched_file = pf.utils.fetch_url(fetch_url, fetch_dir=fetch_dir)\n    fetched_dir = pf.utils.unpack_archive(fetched_file)\n    fetched_dir = os.path.join(fetched_dir, model_name)\n    return fetched_dir\n\n\ndef map_from_stock_variale_name(name, prefix=""bert""):\n    name = name.split("":"")[0]\n    ns   = name.split(""/"")\n    pns  = prefix.split(""/"")\n\n    # assert ns[0] == ""bert""\n\n    name = ""/"".join(pns + ns[1:])\n    ns = name.split(""/"")\n\n    if ns[1] not in [""encoder"", ""embeddings""]:\n        return None\n    if ns[1] == ""embeddings"":\n        if ns[2] == ""LayerNorm"":\n            return name\n        else:\n            return name + ""/embeddings""\n    if ns[1] == ""encoder"":\n        if ns[3] == ""intermediate"":\n            return ""/"".join(ns[:4] + ns[5:])\n        else:\n            return name\n    return None\n\n\ndef map_to_stock_variable_name(name, prefix=""bert""):\n    name = name.split("":"")[0]\n    ns   = name.split(""/"")\n    pns  = prefix.split(""/"")\n\n    if ns[:len(pns)] != pns:\n        return None\n\n    name = ""/"".join([""bert""] + ns[len(pns):])\n    ns   = name.split(""/"")\n\n    if ns[1] not in [""encoder"", ""embeddings""]:\n        return None\n    if ns[1] == ""embeddings"":\n        if ns[2] == ""LayerNorm"":\n            return name\n        elif ns[2] == ""word_embeddings_projector"":\n            ns[2] = ""word_embeddings_2""\n            if ns[3] == ""projector"":\n                ns[3] = ""embeddings""\n                return ""/"".join(ns[:-1])\n            return ""/"".join(ns)\n        else:\n            return ""/"".join(ns[:-1])\n    if ns[1] == ""encoder"":\n        if ns[3] == ""intermediate"":\n            return ""/"".join(ns[:4] + [""dense""] + ns[4:])\n        else:\n            return name\n    return None\n\n\nclass StockBertConfig(params.Params):\n    attention_probs_dropout_prob = None  # 0.1\n    hidden_act                   = None  # ""gelu""\n    hidden_dropout_prob          = None  # 0.1,\n    hidden_size                  = None  # 768,\n    initializer_range            = None  # 0.02,\n    intermediate_size            = None  # 3072,\n    max_position_embeddings      = None  # 512,\n    num_attention_heads          = None  # 12,\n    num_hidden_layers            = None  # 12,\n    type_vocab_size              = None  # 2,\n    vocab_size                   = None  # 30522\n\n    # ALBERT params\n    # directionality             = None  # ""bidi""\n    # pooler_fc_size             = None  # 768,\n    # pooler_num_attention_heads = None  # 12,\n    # pooler_num_fc_layers       = None  # 3,\n    # pooler_size_per_head       = None  # 128,\n    # pooler_type                = None  # ""first_token_transform"",\n    ln_type                      = None  # ""postln""   # used for detecting brightmarts weights\n    embedding_size               = None  # 128\n\n    def to_bert_model_layer_params(self):\n        return map_stock_config_to_params(self)\n\n\ndef map_stock_config_to_params(bc):\n    """"""\n    Converts the original BERT or ALBERT config dictionary\n    to a `BertModelLayer.Params` instance.\n    :return: a `BertModelLayer.Params` instance.\n    """"""\n    bert_params = BertModelLayer.Params(\n        num_layers=bc.num_hidden_layers,\n        num_heads=bc.num_attention_heads,\n        hidden_size=bc.hidden_size,\n        hidden_dropout=bc.hidden_dropout_prob,\n        attention_dropout=bc.attention_probs_dropout_prob,\n\n        intermediate_size=bc.intermediate_size,\n        intermediate_activation=bc.hidden_act,\n\n        vocab_size=bc.vocab_size,\n        use_token_type=True,\n        use_position_embeddings=True,\n        token_type_vocab_size=bc.type_vocab_size,\n        max_position_embeddings=bc.max_position_embeddings,\n\n        embedding_size=bc.embedding_size,\n        shared_layer=bc.embedding_size is not None,\n    )\n    return bert_params\n\n\ndef params_from_pretrained_ckpt(bert_ckpt_dir):\n    json_config_files = tf.io.gfile.glob(os.path.join(bert_ckpt_dir, ""*_config*.json""))\n    if len(json_config_files) != 1:\n        raise ValueError(""Can\'t glob for BERT config json at: {}/*_config*.json"".format(bert_ckpt_dir))\n\n    config_file_name = os.path.basename(json_config_files[0])\n    bert_config_file = os.path.join(bert_ckpt_dir, config_file_name)\n\n    with tf.io.gfile.GFile(bert_config_file, ""r"") as reader:\n        bc = StockBertConfig.from_json_string(reader.read())\n        bert_params = map_stock_config_to_params(bc)\n        is_brightmart_weights = bc[""ln_type""] is not None\n        bert_params.project_position_embeddings  = not is_brightmart_weights  # ALBERT: False for brightmart/weights\n        bert_params.project_embeddings_with_bias = not is_brightmart_weights  # ALBERT: False for brightmart/weights\n\n    return bert_params\n\n\ndef _checkpoint_exists(ckpt_path):\n    cktp_files = tf.io.gfile.glob(ckpt_path + ""*"")\n    return len(cktp_files) > 0\n\n\ndef bert_prefix(bert: BertModelLayer):\n    re_bert = re.compile(r\'(.*)/(embeddings|encoder)/(.+):0\')\n    match = re_bert.match(bert.weights[0].name)\n    assert match, ""Unexpected bert layer: {} weight:{}"".format(bert, bert.weights[0].name)\n    prefix = match.group(1)\n    return prefix\n\n\ndef load_stock_weights(bert: BertModelLayer, ckpt_path, map_to_stock_fn=map_to_stock_variable_name):\n    """"""\n    Use this method to load the weights from a pre-trained BERT checkpoint into a bert layer.\n\n    :param bert: a BertModelLayer instance within a built keras model.\n    :param ckpt_path: checkpoint path, i.e. `uncased_L-12_H-768_A-12/bert_model.ckpt` or `albert_base_zh/albert_model.ckpt`\n    :return: list of weights with mismatched shapes. This can be used to extend\n    the segment/token_type embeddings.\n    """"""\n    assert isinstance(bert, BertModelLayer), ""Expecting a BertModelLayer instance as first argument""\n    assert _checkpoint_exists(ckpt_path), ""Checkpoint does not exist: {}"".format(ckpt_path)\n    ckpt_reader = tf.train.load_checkpoint(ckpt_path)\n\n    stock_weights = set(ckpt_reader.get_variable_to_dtype_map().keys())\n\n    prefix = bert_prefix(bert)\n\n    loaded_weights = set()\n    skip_count = 0\n    weight_value_tuples = []\n    skipped_weight_value_tuples = []\n\n    bert_params = bert.weights\n    param_values = keras.backend.batch_get_value(bert.weights)\n    for ndx, (param_value, param) in enumerate(zip(param_values, bert_params)):\n        stock_name = map_to_stock_fn(param.name, prefix)\n\n        if ckpt_reader.has_tensor(stock_name):\n            ckpt_value = ckpt_reader.get_tensor(stock_name)\n\n            if param_value.shape != ckpt_value.shape:\n                print(""loader: Skipping weight:[{}] as the weight shape:[{}] is not compatible ""\n                      ""with the checkpoint:[{}] shape:{}"".format(param.name, param.shape,\n                                                                 stock_name, ckpt_value.shape))\n                skipped_weight_value_tuples.append((param, ckpt_value))\n                continue\n\n            weight_value_tuples.append((param, ckpt_value))\n            loaded_weights.add(stock_name)\n        else:\n            print(""loader: No value for:[{}], i.e.:[{}] in:[{}]"".format(param.name, stock_name, ckpt_path))\n            skip_count += 1\n    keras.backend.batch_set_value(weight_value_tuples)\n\n    print(""Done loading {} BERT weights from: {} into {} (prefix:{}). ""\n          ""Count of weights not found in the checkpoint was: [{}]. ""\n          ""Count of weights with mismatched shape: [{}]"".format(\n              len(weight_value_tuples), ckpt_path, bert, prefix, skip_count, len(skipped_weight_value_tuples)))\n\n    print(""Unused weights from checkpoint:"",\n          ""\\n\\t"" + ""\\n\\t"".join(sorted(stock_weights.difference(loaded_weights))))\n\n    return skipped_weight_value_tuples  # (bert_weight, value_from_ckpt)\n'"
bert/loader_albert.py,14,"b'# coding=utf-8\n#\n# created by kpe on 28.10.2019 at 2:02 PM\n#\n\nfrom __future__ import division, absolute_import, print_function\n\nimport os\nimport re\nimport urllib\nimport params_flow as pf\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom bert import BertModelLayer, loader\n\nalbert_models_tfhub = {\n    ""albert_base"":    ""https://tfhub.dev/google/albert_base/{version}?tf-hub-format=compressed"",\n    ""albert_large"":   ""https://tfhub.dev/google/albert_large/{version}?tf-hub-format=compressed"",\n    ""albert_xlarge"":  ""https://tfhub.dev/google/albert_xlarge/{version}?tf-hub-format=compressed"",\n    ""albert_xxlarge"": ""https://tfhub.dev/google/albert_xxlarge/{version}?tf-hub-format=compressed"",\n}\n\nalbert_models_brightmart = {\n    ""albert_tiny"":        ""https://storage.googleapis.com/albert_zh/albert_tiny.zip"",\n    ""albert_tiny_489k"":   ""https://storage.googleapis.com/albert_zh/albert_tiny_489k.zip"",\n    ""albert_base"":        ""https://storage.googleapis.com/albert_zh/albert_base_zh.zip"",\n    ""albert_base_36k"":    ""https://storage.googleapis.com/albert_zh/albert_base_zh_additional_36k_steps.zip"",\n    ""albert_large"":       ""https://storage.googleapis.com/albert_zh/albert_large_zh.zip"",\n    ""albert_xlarge"":      ""https://storage.googleapis.com/albert_zh/albert_xlarge_zh_177k.zip"",\n    ""albert_xlarge_183k"": ""https://storage.googleapis.com/albert_zh/albert_xlarge_zh_183k.zip"",\n}\n\nalbert_models_google = {\n    ""albert_base_zh"": ""https://storage.googleapis.com/albert_models/albert_base_zh.tar.gz"",\n    ""albert_large_zh"": ""https://storage.googleapis.com/albert_models/albert_large_zh.tar.gz"",\n    ""albert_xlarge_zh"": ""https://storage.googleapis.com/albert_models/albert_xlarge_zh.tar.gz"",\n    ""albert_xxlarge_zh"": ""https://storage.googleapis.com/albert_models/albert_xxlarge_zh.tar.gz"",\n\n    ""albert_base_v2"": ""https://storage.googleapis.com/albert_models/albert_base_v2.tar.gz"",\n    ""albert_large_v2"": ""https://storage.googleapis.com/albert_models/albert_large_v2.tar.gz"",\n    ""albert_xlarge_v2"": ""https://storage.googleapis.com/albert_models/albert_xlarge_v2.tar.gz"",\n    ""albert_xxlarge_v2"": ""https://storage.googleapis.com/albert_models/albert_xxlarge_v2.tar.gz""\n}\n\nconfig_albert_base = {\n    ""attention_probs_dropout_prob"": 0.1,\n    ""hidden_act"": ""gelu"",\n    ""hidden_dropout_prob"": 0.1,\n    ""embedding_size"": 128,\n    ""hidden_size"": 768,\n    ""initializer_range"": 0.02,\n    ""intermediate_size"": 3072,\n    ""max_position_embeddings"": 512,\n    ""num_attention_heads"": 12,\n    ""num_hidden_layers"": 12,\n    ""num_hidden_groups"": 1,\n    ""net_structure_type"": 0,\n    ""gap_size"": 0,\n    ""num_memory_blocks"": 0,\n    ""inner_group_num"": 1,\n    ""down_scale_factor"": 1,\n    ""type_vocab_size"": 2,\n    ""vocab_size"": 30000\n}\n\nconfig_albert_large = {\n    ""attention_probs_dropout_prob"": 0.1,\n    ""hidden_act"": ""gelu"",\n    ""hidden_dropout_prob"": 0.1,\n    ""embedding_size"": 128,\n    ""hidden_size"": 1024,\n    ""initializer_range"": 0.02,\n    ""intermediate_size"": 4096,\n    ""max_position_embeddings"": 512,\n    ""num_attention_heads"": 16,\n    ""num_hidden_layers"": 24,\n    ""num_hidden_groups"": 1,\n    ""net_structure_type"": 0,\n    ""gap_size"": 0,\n    ""num_memory_blocks"": 0,\n    ""inner_group_num"": 1,\n    ""down_scale_factor"": 1,\n    ""type_vocab_size"": 2,\n    ""vocab_size"": 30000\n}\nconfig_albert_xlarge = {\n    ""attention_probs_dropout_prob"": 0.1,\n    ""hidden_act"": ""gelu"",\n    ""hidden_dropout_prob"": 0.1,\n    ""embedding_size"": 128,\n    ""hidden_size"": 2048,\n    ""initializer_range"": 0.02,\n    ""intermediate_size"": 8192,\n    ""max_position_embeddings"": 512,\n    ""num_attention_heads"": 16,\n    ""num_hidden_layers"": 24,\n    ""num_hidden_groups"": 1,\n    ""net_structure_type"": 0,\n    ""gap_size"": 0,\n    ""num_memory_blocks"": 0,\n    ""inner_group_num"": 1,\n    ""down_scale_factor"": 1,\n    ""type_vocab_size"": 2,\n    ""vocab_size"": 30000\n}\n\nconfig_albert_xxlarge = {\n    ""attention_probs_dropout_prob"": 0,\n    ""hidden_act"": ""gelu"",\n    ""hidden_dropout_prob"": 0,\n    ""embedding_size"": 128,\n    ""hidden_size"": 4096,\n    ""initializer_range"": 0.02,\n    ""intermediate_size"": 16384,\n    ""max_position_embeddings"": 512,\n    ""num_attention_heads"": 64,\n    ""num_hidden_layers"": 12,\n    ""num_hidden_groups"": 1,\n    ""net_structure_type"": 0,\n    ""layers_to_keep"": [],\n    ""gap_size"": 0,\n    ""num_memory_blocks"": 0,\n    ""inner_group_num"": 1,\n    ""down_scale_factor"": 1,\n    ""type_vocab_size"": 2,\n    ""vocab_size"": 30000\n}\n\nalbert_models_config = {\n    ""albert_base"":    config_albert_base,\n    ""albert_large"":   config_albert_large,\n    ""albert_xlarge"":  config_albert_xlarge,\n    ""albert_xxlarge"": config_albert_xxlarge,\n}\n\n\ndef albert_params(albert_model: str):\n    """"""Returns the ALBERT params for the specified TFHub model.\n\n    :param albert_model: either a model name or a checkpoint directory\n                         containing an assets/albert_config.json\n    """"""\n    if tf.io.gfile.isdir(albert_model):\n        config_file = os.path.join(albert_model, ""assets"", ""albert_config.json"")      # google tfhub v2 weights\n        if not tf.io.gfile.exists(config_file):\n            config_file = os.path.join(albert_model, ""albert_config.json"")  # google non-tfhub v2 weights\n        if tf.io.gfile.exists(config_file):\n            stock_config = loader.StockBertConfig.from_json_file(config_file)\n        else:\n            raise ValueError(""No google-research ALBERT model found under:[{}] expecting albert_config.json or assets/albert_config.json"".format(albert_model))\n    else:\n        if albert_model in albert_models_config:                                    # google tfhub v1 weights\n            albert_config = albert_models_config[albert_model]\n            stock_config = loader.StockBertConfig.from_dict(albert_config, return_instance=True, return_unused=False)\n        else:\n            raise ValueError(""ALBERT model with name:[{}] not one of tfhub/google-research albert models, try one of:{}"".format(\n                albert_model, albert_models_tfhub))\n\n    params = loader.map_stock_config_to_params(stock_config)\n    return params\n\n\ndef fetch_brightmart_albert_model(model_name: str, fetch_dir: str):\n    if model_name not in albert_models_brightmart:\n        raise ValueError(""ALBERT model with name:[{}] not found at brightmart/albert_zh, try one of:{}"".format(\n            model_name, albert_models_brightmart))\n    else:\n        fetch_url = albert_models_brightmart[model_name]\n\n    fetched_file = pf.utils.fetch_url(fetch_url, fetch_dir=fetch_dir)\n    fetched_dir = pf.utils.unpack_archive(fetched_file)\n    return fetched_dir\n\n\ndef fetch_google_albert_model(model_name: str, fetch_dir: str):\n    if model_name not in albert_models_google:\n        raise ValueError(""ALBERT model with name:[{}] not found at google-research/ALBERT, try one of:{}"".format(\n            model_name, albert_models_google))\n    else:\n        fetch_url = albert_models_google[model_name]\n\n    fetched_file = pf.utils.fetch_url(fetch_url, fetch_dir=fetch_dir)\n    fetched_dir = pf.utils.unpack_archive(fetched_file)\n    fetched_dir = tf.io.gfile.glob(os.path.join(fetched_dir, ""*"", ""model.ckpt-best.meta""))[0]\n    fetched_dir = os.path.dirname(fetched_dir)\n    return fetched_dir\n\n\ndef fetch_tfhub_albert_model(albert_model: str, fetch_dir: str, version=""2""):\n    """"""\n    Fetches a pre-trained ALBERT model from TFHub.\n    :param albert_model: TFHub model URL or a model name like albert_base, albert_large, etc.\n    :param fetch_dir:\n    :return:\n    """"""\n    if albert_model.startswith(""http""):\n        fetch_url = albert_model\n    elif albert_model not in albert_models_tfhub:\n        raise ValueError(""ALBERT model with name:[{}] not found in tfhub/google, try one of:{}"".format(\n            albert_model, albert_models_tfhub))\n    else:\n        fetch_url = albert_models_tfhub[albert_model].format(version=version)\n\n    name, version = urllib.parse.urlparse(fetch_url).path.split(""/"")[-2:]\n    local_file_name = ""{}.tar.gz"".format(name)\n\n    print(""Fetching ALBERT model: {} version: {}"".format(name, version))\n\n    fetched_file = pf.utils.fetch_url(fetch_url, fetch_dir=fetch_dir, local_file_name=local_file_name)\n    fetched_dir = pf.utils.unpack_archive(fetched_file)\n\n    return fetched_dir\n\n\ndef map_to_tfhub_albert_variable_name(name, prefix=""bert""):\n\n    name = re.compile(""encoder/layer_shared/intermediate/(?=kernel|bias)"").sub(\n        ""encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/"", name)\n    name = re.compile(""encoder/layer_shared/output/dense/(?=kernel|bias)"").sub(\n        ""encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/"", name)\n\n    name = name.replace(""encoder/layer_shared/output/dense"",               ""encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense"")\n    name = name.replace(""encoder/layer_shared/attention/output/LayerNorm"", ""encoder/transformer/group_0/inner_group_0/LayerNorm"")\n    name = name.replace(""encoder/layer_shared/output/LayerNorm"", ""encoder/transformer/group_0/inner_group_0/LayerNorm_1"")\n    name = name.replace(""encoder/layer_shared/attention"",        ""encoder/transformer/group_0/inner_group_0/attention_1"")\n\n    name = name.replace(""embeddings/word_embeddings_projector/projector"",\n                        ""encoder/embedding_hidden_mapping_in/kernel"")\n    name = name.replace(""embeddings/word_embeddings_projector/bias"",\n                        ""encoder/embedding_hidden_mapping_in/bias"")\n\n    name = name.split("":"")[0]\n    ns   = name.split(""/"")\n    pns  = prefix.split(""/"")\n\n    if ns[:len(pns)] != pns:\n        return None\n\n    name = ""/"".join([""bert""] + ns[len(pns):])\n    ns   = name.split(""/"")\n\n    if ns[1] not in [""encoder"", ""embeddings""]:\n        return None\n    if ns[1] == ""embeddings"":\n        if ns[2] == ""LayerNorm"":\n            return name\n        else:\n            return ""/"".join(ns[:-1])\n    if ns[1] == ""encoder"":\n        if ns[3] == ""intermediate"":\n            return ""/"".join(ns[:4] + [""dense""] + ns[4:])\n        else:\n            return name\n    return None\n\n\ndef _is_tfhub_model(tfhub_model_path):\n    try:\n        assets_files     = tf.io.gfile.glob(os.path.join(tfhub_model_path, ""assets/*""))\n        variables_files  = tf.io.gfile.glob(os.path.join(tfhub_model_path, ""variables/variables.*""))\n        pb_files = tf.io.gfile.glob(os.path.join(tfhub_model_path, ""*.pb""))\n    except tf.errors.NotFoundError:\n        assets_files, variables_files, pb_files = [], [], []\n\n    return len(pb_files) >= 2 and len(assets_files) >= 1 and len(variables_files) >= 2\n\n\ndef _is_google_model(ckpt_path):\n    ckpt_index = os.path.isfile(ckpt_path + "".index"")\n    if ckpt_index:\n        config_path = tf.io.gfile.glob(os.path.join(os.path.dirname(ckpt_path), ""albert_config.json""))\n        ckpt_meta   = tf.io.gfile.glob(os.path.join(os.path.dirname(ckpt_path), ""model.ckpt-best.meta""))\n        return config_path and ckpt_meta\n    return False\n\n\ndef load_albert_weights(bert: BertModelLayer, tfhub_model_path, tags=[]):\n    """"""\n    Use this method to load the weights from a pre-trained BERT checkpoint into a bert layer.\n\n    :param bert: a BertModelLayer instance within a built keras model.\n    :param ckpt_path: checkpoint path, i.e. `uncased_L-12_H-768_A-12/bert_model.ckpt` or `albert_base_zh/albert_model.ckpt`\n    :return: list of weights with mismatched shapes. This can be used to extend\n    the segment/token_type embeddings.\n    """"""\n\n    if not _is_tfhub_model(tfhub_model_path):\n        if _is_google_model(tfhub_model_path):\n            print(""Loading google-research/ALBERT weights..."")\n            map_to_stock_fn = map_to_tfhub_albert_variable_name\n        else:\n            print(""Loading brightmart/albert_zh weights..."")\n            map_to_stock_fn = loader.map_to_stock_variable_name\n        return loader.load_stock_weights(bert, tfhub_model_path, map_to_stock_fn=map_to_stock_fn)\n\n    assert isinstance(bert, BertModelLayer), ""Expecting a BertModelLayer instance as first argument""\n    prefix = loader.bert_prefix(bert)\n\n    with tf.Graph().as_default():\n        sm = tf.compat.v2.saved_model.load(tfhub_model_path, tags=tags)\n        with tf.compat.v1.Session() as sess:\n            sess.run(tf.compat.v1.global_variables_initializer())\n            stock_values = {v.name.split("":"")[0]: v.read_value() for v in sm.variables}\n            stock_values = sess.run(stock_values)\n\n    # print(""\\n"".join([str((n, v.shape)) for n,v in stock_values.items()]))\n\n    loaded_weights = set()\n    skip_count = 0\n    weight_value_tuples = []\n    skipped_weight_value_tuples = []\n\n    bert_params = bert.weights\n    param_values = keras.backend.batch_get_value(bert.weights)\n    for ndx, (param_value, param) in enumerate(zip(param_values, bert_params)):\n        stock_name = map_to_tfhub_albert_variable_name(param.name, prefix)\n\n        if stock_name in stock_values:\n            ckpt_value = stock_values[stock_name]\n\n            if param_value.shape != ckpt_value.shape:\n                print(""loader: Skipping weight:[{}] as the weight shape:[{}] is not compatible ""\n                      ""with the checkpoint:[{}] shape:{}"".format(param.name, param.shape,\n                                                                 stock_name, ckpt_value.shape))\n                skipped_weight_value_tuples.append((param, ckpt_value))\n                continue\n\n            weight_value_tuples.append((param, ckpt_value))\n            loaded_weights.add(stock_name)\n        else:\n            print(""loader: No value for:[{}], i.e.:[{}] in:[{}]"".format(param.name, stock_name, tfhub_model_path))\n            skip_count += 1\n    keras.backend.batch_set_value(weight_value_tuples)\n\n    print(""Done loading {} BERT weights from: {} into {} (prefix:{}). ""\n          ""Count of weights not found in the checkpoint was: [{}]. ""\n          ""Count of weights with mismatched shape: [{}]"".format(\n              len(weight_value_tuples), tfhub_model_path, bert, prefix, skip_count, len(skipped_weight_value_tuples)))\n    print(""Unused weights from saved model:"",\n          ""\\n\\t"" + ""\\n\\t"".join(sorted(set(stock_values.keys()).difference(loaded_weights))))\n\n    return skipped_weight_value_tuples  # (bert_weight, value_from_ckpt)\n\n'"
bert/model.py,0,"b'# coding=utf-8\n#\n# created by kpe on 28.Mar.2019 at 12:33\n#\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom tensorflow import keras\nimport params_flow as pf\n\nfrom bert.layer import Layer\nfrom bert.embeddings import BertEmbeddingsLayer\nfrom bert.transformer import TransformerEncoderLayer\n\n\nclass BertModelLayer(Layer):\n    """"""\n    Implementation of BERT (arXiv:1810.04805), adapter-BERT (arXiv:1902.00751) and ALBERT (arXiv:1909.11942).\n\n    See: https://arxiv.org/pdf/1810.04805.pdf - BERT\n         https://arxiv.org/pdf/1902.00751.pdf - adapter-BERT\n         https://arxiv.org/pdf/1909.11942.pdf - ALBERT\n\n    """"""\n    class Params(BertEmbeddingsLayer.Params,\n                 TransformerEncoderLayer.Params):\n        pass\n\n    # noinspection PyUnusedLocal\n    def _construct(self, **kwargs):\n        super()._construct(**kwargs)\n        self.embeddings_layer = BertEmbeddingsLayer.from_params(\n            self.params,\n            name=""embeddings""\n        )\n        # create all transformer encoder sub-layers\n        self.encoders_layer = TransformerEncoderLayer.from_params(\n            self.params,\n            name=""encoder""\n        )\n\n        self.support_masking  = True\n\n    # noinspection PyAttributeOutsideInit\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            assert len(input_shape) == 2\n            input_ids_shape, token_type_ids_shape = input_shape\n            self.input_spec = [keras.layers.InputSpec(shape=input_ids_shape),\n                               keras.layers.InputSpec(shape=token_type_ids_shape)]\n        else:\n            input_ids_shape = input_shape\n            self.input_spec = keras.layers.InputSpec(shape=input_ids_shape)\n        super(BertModelLayer, self).build(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            assert len(input_shape) == 2\n            input_ids_shape, _ = input_shape\n        else:\n            input_ids_shape = input_shape\n\n        output_shape = list(input_ids_shape) + [self.params.hidden_size]\n        return output_shape\n\n    def apply_adapter_freeze(self):\n        """""" Should be called once the model has been built to freeze\n        all bet the adapter and layer normalization layers in BERT.\n        """"""\n        if self.params.adapter_size is not None:\n            def freeze_selector(layer):\n                return layer.name not in [""adapter-up"", ""adapter-down"", ""LayerNorm"", ""extra_word_embeddings""]\n            pf.utils.freeze_leaf_layers(self, freeze_selector)\n\n    def call(self, inputs, mask=None, training=None):\n        if mask is None:\n            mask = self.embeddings_layer.compute_mask(inputs)\n\n        embedding_output = self.embeddings_layer(inputs, mask=mask, training=training)\n        output           = self.encoders_layer(embedding_output, mask=mask, training=training)\n        return output   # [B, seq_len, hidden_size]\n\n'"
bert/transformer.py,4,"b'# coding=utf-8\n#\n# created by kpe on 20.Mar.2019 at 16:30\n#\n\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\nfrom tensorflow.python import keras\n\nfrom params_flow import LayerNormalization\n\nfrom bert.attention import AttentionLayer\nfrom bert.layer import Layer\n\n\nclass ProjectionLayer(Layer):\n    class Params(Layer.Params):\n        hidden_size        = None\n        hidden_dropout     = 0.1\n        initializer_range  = 0.02\n        adapter_size       = None       # bottleneck size of the adapter - arXiv:1902.00751\n        adapter_activation = ""gelu""\n        adapter_init_scale = 1e-3\n\n    def _construct(self, **kwargs):\n        super()._construct(**kwargs)\n        self.dense      = None\n        self.dropout    = None\n        self.layer_norm = None\n\n        self.adapter_down = None\n        self.adapter_up   = None\n\n        self.supports_masking = True\n\n    # noinspection PyAttributeOutsideInit\n    def build(self, input_shape):\n        assert isinstance(input_shape, list) and 2 == len(input_shape)\n        out_shape, residual_shape = input_shape\n        self.input_spec = [keras.layers.InputSpec(shape=out_shape),\n                           keras.layers.InputSpec(shape=residual_shape)]\n\n        self.dense = keras.layers.Dense(units=self.params.hidden_size,\n                                        kernel_initializer=self.create_initializer(),\n                                        name=""dense"")\n        self.dropout    = keras.layers.Dropout(rate=self.params.hidden_dropout)\n        self.layer_norm = LayerNormalization(name=""LayerNorm"")\n\n        if self.params.adapter_size is not None:\n            self.adapter_down = keras.layers.Dense(units=self.params.adapter_size,\n                                                   kernel_initializer=tf.keras.initializers.TruncatedNormal(\n                                                       stddev=self.params.adapter_init_scale),\n                                                   activation=self.get_activation(self.params.adapter_activation),\n                                                   name=""adapter-down"")\n            self.adapter_up   = keras.layers.Dense(units=self.params.hidden_size,\n                                                   kernel_initializer=tf.keras.initializers.TruncatedNormal(\n                                                       stddev=self.params.adapter_init_scale),\n                                                   name=""adapter-up"")\n\n        super(ProjectionLayer, self).build(input_shape)\n\n    def call(self, inputs, mask=None, training=None, **kwargs):\n        output, residual = inputs\n        output = self.dense(output)\n        output = self.dropout(output, training=training)\n\n        if self.adapter_down is not None:\n            adapted = self.adapter_down(output)\n            adapted = self.adapter_up(adapted)\n            output = tf.add(output, adapted)\n\n        output = self.layer_norm(tf.add(output, residual))\n        return output\n\n\nclass TransformerSelfAttentionLayer(Layer):\n    class Params(ProjectionLayer.Params,\n                 AttentionLayer.Params):\n        hidden_size         = None\n        num_heads           = None\n        hidden_dropout      = None\n        attention_dropout   = 0.1\n        initializer_range   = 0.02\n\n    def _construct(self, **kwargs):\n        super()._construct(**kwargs)\n        params = self.params\n        if params.hidden_size % params.num_heads != 0:\n            raise ValueError(""The hidden_size:[{}] is not a multiple of num_heads:[{}]"".format(params.hidden_size,\n                                                                                               params.num_heads))\n        self.size_per_head = params.hidden_size // params.num_heads\n        assert params.size_per_head is None or self.size_per_head == params.size_per_head\n\n        self.attention_layer     = None\n        self.attention_projector = None\n\n        self.supports_masking = True\n\n    def build(self, input_shape):\n        self.input_spec = keras.layers.InputSpec(shape=input_shape)\n\n        self.attention_layer = AttentionLayer.from_params(\n            self.params,\n            size_per_head=self.size_per_head,\n            name=""self"",\n        )\n        self.attention_projector = ProjectionLayer.from_params(\n            self.params,\n            name=""output"",\n        )\n\n        super(TransformerSelfAttentionLayer, self).build(input_shape)\n\n    def call(self, inputs, mask=None, training=None):\n        layer_input = inputs\n\n        #\n        # TODO: is it OK to recompute the 3D attention mask in each attention layer\n        #\n        attention_head   = self.attention_layer(layer_input, mask=mask, training=training)\n        attention_output = self.attention_projector([attention_head, layer_input], mask=mask, training=training)\n\n        return attention_output\n\n\nclass SingleTransformerEncoderLayer(Layer):\n    """"""\n    Multi-headed, single layer for the Transformer from \'Attention is All You Need\' (arXiv: 1706.03762).\n\n    See also: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n    """"""\n\n    class Params(TransformerSelfAttentionLayer.Params,\n                 ProjectionLayer.Params):\n        intermediate_size       = None\n        intermediate_activation = ""gelu""\n\n    def _construct(self, **kwargs):\n        super()._construct(**kwargs)\n        params = self.params\n        if params.hidden_size % params.num_heads != 0:\n            raise ValueError(""The hidden_size:[{}] is not a multiple of num_heads:[{}]"".format(params.hidden_size,\n                                                                                               params.num_heads))\n        self.size_per_head = params.hidden_size // params.num_heads\n\n        self.self_attention_layer = None\n        self.intermediate_layer   = None\n        self.output_projector     = None\n\n        self.supports_masking = True\n\n    def build(self, input_shape):\n        self.input_spec = keras.layers.InputSpec(shape=input_shape)  # [B, seq_len, hidden_size]\n\n        self.self_attention_layer = TransformerSelfAttentionLayer.from_params(\n            self.params,\n            name=""attention""\n        )\n        self.intermediate_layer = keras.layers.Dense(\n            name=""intermediate"",\n            units=self.params.intermediate_size,\n            activation=self.get_activation(self.params.intermediate_activation),\n            kernel_initializer=self.create_initializer()\n        )\n        self.output_projector = ProjectionLayer.from_params(\n            self.params,\n            name=""output"",\n        )\n\n        super(SingleTransformerEncoderLayer, self).build(input_shape)\n\n    def call(self, inputs, mask=None, training=None):\n        layer_input = inputs\n\n        attention_output    = self.self_attention_layer(layer_input, mask=mask, training=training)\n\n        # intermediate\n        intermediate_output = self.intermediate_layer(attention_output)\n\n        # output\n        layer_output = self.output_projector([intermediate_output, attention_output], mask=mask)\n\n        return layer_output\n\n\nclass TransformerEncoderLayer(Layer):\n    """"""\n    Multi-headed, multi-layer Transformer from \'Attention is All You Need\' (arXiv: 1706.03762).\n\n    Implemented for BERT, with support for ALBERT (sharing encoder layer params).\n\n    See also: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n    """"""\n\n    class Params(SingleTransformerEncoderLayer.Params):\n        num_layers     = None\n        out_layer_ndxs = None   # [-1]\n\n        shared_layer   = False  # False for BERT, True for ALBERT\n\n    def _construct(self, **kwargs):\n        super()._construct(**kwargs)\n        self.encoder_layers   = []\n        self.shared_layer     = None  # for ALBERT\n        self.supports_masking = True\n\n    def build(self, input_shape):\n        self.input_spec = keras.layers.InputSpec(shape=input_shape)\n\n        # create all transformer encoder sub-layers\n        if self.params.shared_layer:\n            # ALBERT: share params\n            self.shared_layer = SingleTransformerEncoderLayer.from_params(self.params, name=""layer_shared"")\n        else:\n            # BERT\n            for layer_ndx in range(self.params.num_layers):\n                encoder_layer = SingleTransformerEncoderLayer.from_params(\n                    self.params,\n                    name=""layer_{}"".format(layer_ndx),\n                )\n                self.encoder_layers.append(encoder_layer)\n\n        super(TransformerEncoderLayer, self).build(input_shape)\n\n    def call(self, inputs, mask=None, training=None):\n        layer_output = inputs\n\n        layer_outputs = []\n        for layer_ndx in range(self.params.num_layers):\n            encoder_layer = self.encoder_layers[layer_ndx] if self.encoder_layers else self.shared_layer\n            layer_input = layer_output\n\n            layer_output = encoder_layer(layer_input, mask=mask, training=training)\n            layer_outputs.append(layer_output)\n\n        if self.params.out_layer_ndxs is None:\n            # return the final layer only\n            final_output = layer_output\n        else:\n            final_output = []\n            for ndx in self.params.out_layer_ndxs:\n                final_output.append(layer_outputs[ndx])\n            final_output = tuple(final_output)\n\n        return final_output\n\n\n'"
bert/version.py,0,"b'__version__ = ""0.14.5""\n'"
tests/__init__.py,0,"b'# coding=utf-8\n#\n# created by kpe on 15.Mar.2019 at 12:57\n#\nfrom __future__ import division, absolute_import, print_function\n'"
tests/test_adapter_finetune.py,8,"b'# coding=utf-8\n#\n# created by kpe on 02.Sep.2019 at 23:57\n#\n\nfrom __future__ import absolute_import, division, print_function\n\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport params_flow as pf\n\nimport bert\n\nfrom .test_common import MiniBertFactory, AbstractBertTest\n\n\nclass TestAdapterFineTuning(AbstractBertTest):\n    """"""\n    Demonstrates a fine tuning workflow using adapte-BERT with\n    storing the fine tuned and frozen pre-trained weights in\n    separate checkpoint files.\n    """"""\n\n    def setUp(self) -> None:\n        tf.compat.v1.reset_default_graph()\n        tf.compat.v1.enable_eager_execution()\n        print(""Eager Execution:"", tf.executing_eagerly())\n\n        # build a dummy bert\n        self.ckpt_path = MiniBertFactory.create_mini_bert_weights()\n        self.ckpt_dir = os.path.dirname(self.ckpt_path)\n        self.tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file=os.path.join(self.ckpt_dir, ""vocab.txt""), do_lower_case=True)\n\n    def test_coverage_improve(self):\n        bert_params = bert.params_from_pretrained_ckpt(self.ckpt_dir)\n        model, l_bert = self.build_model(bert_params, 1)\n        for weight in model.weights:\n            l_bert_prefix = bert.loader.bert_prefix(l_bert)\n\n            stock_name = bert.loader.map_to_stock_variable_name(weight.name, l_bert_prefix)\n\n            if stock_name is None:\n                print(""No BERT stock weight for"", weight.name)\n                continue\n\n            keras_name = bert.loader.map_from_stock_variale_name(stock_name, l_bert_prefix)\n            self.assertEqual(weight.name.split("":"")[0], keras_name)\n\n    @staticmethod\n    def build_model(bert_params, max_seq_len):\n        # enable adapter-BERT\n        bert_params.adapter_size = 2\n        l_bert = bert.BertModelLayer.from_params(bert_params)\n        model = keras.models.Sequential([\n            l_bert,\n            keras.layers.Lambda(lambda seq: seq[:, 0, :]),\n            keras.layers.Dense(3, name=""test_cls"")\n        ])\n        model.compile(optimizer=keras.optimizers.Adam(),\n                      loss=keras.losses.SparseCategoricalCrossentropy(),\n                      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n        # build for a given max_seq_len\n        model.build(input_shape=(None, max_seq_len))\n        return model, l_bert\n\n    def test_regularization(self):\n        # create a BERT layer with config from the checkpoint\n        bert_params = bert.params_from_pretrained_ckpt(self.ckpt_dir)\n\n        max_seq_len = 12\n\n        model, l_bert = self.build_model(bert_params, max_seq_len=max_seq_len)\n        l_bert.apply_adapter_freeze()\n        model.summary()\n\n        kernel_regularizer = keras.regularizers.l2(0.01)\n        bias_regularizer   = keras.regularizers.l2(0.01)\n\n        pf.utils.add_dense_layer_loss(model,\n                                      kernel_regularizer=kernel_regularizer,\n                                      bias_regularizer=bias_regularizer)\n        # prepare the data\n        inputs, targets = [""hello world"", ""goodbye""], [1, 2]\n        tokens = [self.tokenizer.tokenize(toks) for toks in inputs]\n        tokens = [self.tokenizer.convert_tokens_to_ids(toks) for toks in tokens]\n        tokens = [toks + [0]*(max_seq_len - len(toks)) for toks in tokens]\n        x = np.array(tokens)\n        y = np.array(targets)\n        # fine tune\n        model.fit(x, y, epochs=3)\n\n    def test_finetuning_workflow(self):\n        # create a BERT layer with config from the checkpoint\n        bert_params = bert.params_from_pretrained_ckpt(self.ckpt_dir)\n\n        max_seq_len = 12\n\n        model, l_bert = self.build_model(bert_params, max_seq_len=max_seq_len)\n        model.summary()\n\n        # freeze non-adapter weights\n        l_bert.apply_adapter_freeze()\n        model.summary()\n\n        # load the BERT weights from the pre-trained model\n        bert.load_stock_weights(l_bert, self.ckpt_path)\n\n        # prepare the data\n        inputs, targets = [""hello world"", ""goodbye""], [1, 2]\n        tokens = [self.tokenizer.tokenize(toks) for toks in inputs]\n        tokens = [self.tokenizer.convert_tokens_to_ids(toks) for toks in tokens]\n        tokens = [toks + [0]*(max_seq_len - len(toks)) for toks in tokens]\n        x = np.array(tokens)\n        y = np.array(targets)\n\n        # fine tune\n        model.fit(x, y, epochs=3)\n\n        # preserve the logits for comparison before and after restoring the fine-tuned model\n        logits = model.predict(x)\n\n        # now store the adapter weights only\n\n        # old fashion - using saver\n        #  finetuned_weights = {w.name: w.value() for w in model.trainable_weights}\n        #  saver = tf.compat.v1.train.Saver(finetuned_weights)\n        #  fine_path = saver.save(tf.compat.v1.keras.backend.get_session(), fine_ckpt)\n\n        fine_ckpt = os.path.join(self.ckpt_dir, ""fine-tuned.ckpt"")\n        finetuned_weights = {w.name: w for w in model.trainable_weights}\n        checkpoint = tf.train.Checkpoint(**finetuned_weights)\n        fine_path = checkpoint.save(file_prefix=fine_ckpt)\n        print(""fine tuned ckpt:"", fine_path)\n\n        # build new model\n        tf.compat.v1.keras.backend.clear_session()\n        model, l_bert = self.build_model(bert_params, max_seq_len=max_seq_len)\n        l_bert.apply_adapter_freeze()\n\n        # load the BERT weights from the pre-trained checkpoint\n        bert.load_stock_weights(l_bert, self.ckpt_path)\n\n        # load the fine tuned classifier model weights\n        finetuned_weights = {w.name: w for w in model.trainable_weights}\n        checkpoint = tf.train.Checkpoint(**finetuned_weights)\n        load_status = checkpoint.restore(fine_path)\n        load_status.assert_consumed().run_restore_ops()\n\n        logits_restored = model.predict(x)\n\n        # check the predictions of the restored model\n        self.assertTrue(np.allclose(logits_restored, logits, 1e-6))\n'"
tests/test_adapter_freeze.py,3,"b'# coding=utf-8\n#\n# created by kpe on 09.08.2019 at 10:38 PM\n#\n\nfrom __future__ import division, absolute_import, print_function\n\n\nimport unittest\n\nimport os\nimport tempfile\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport bert\n\nfrom .test_common import AbstractBertTest, MiniBertFactory\n\n\nclass AdapterFreezeTest(AbstractBertTest):\n\n    def setUp(self) -> None:\n        tf.compat.v1.reset_default_graph()\n        tf.compat.v1.enable_eager_execution()\n        print(""Eager Execution:"", tf.executing_eagerly())\n\n    def test_adapter_freezing(self):\n        bert_params = bert.BertModelLayer.Params(hidden_size=32,\n                                                 vocab_size=67,\n                                                 max_position_embeddings=64,\n                                                 num_layers=1,\n                                                 num_heads=1,\n                                                 intermediate_size=4,\n                                                 use_token_type=False)\n\n        def to_model(bert_params):\n            l_bert = bert.BertModelLayer.from_params(bert_params)\n\n            token_ids = keras.layers.Input(shape=(21,))\n            seq_out = l_bert(token_ids)\n            model = keras.Model(inputs=[token_ids], outputs=seq_out)\n\n            model.build(input_shape=(None, 21))\n            l_bert.apply_adapter_freeze()\n\n            return model\n\n        model = to_model(bert_params)\n        model.summary()\n        print(""trainable wegihts:"", len(model.trainable_weights))\n        self.assertEqual(20, len(model.trainable_weights))\n        for weight in model.trainable_weights:\n            print(weight.name, weight.shape)\n\n        bert_params.adapter_size = 16\n\n        model = to_model(bert_params)\n        model.summary()\n        print(""trainable weights:"", len(model.trainable_weights))\n        self.assertEqual(14, len(model.trainable_weights))\n        for weight in model.trainable_weights:\n            print(weight.name, weight.shape)\n\n    def test_bert_freeze(self):\n        model_dir = tempfile.TemporaryDirectory().name\n        os.makedirs(model_dir)\n        save_path = MiniBertFactory.create_mini_bert_weights(model_dir)\n        tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file=os.path.join(model_dir, ""vocab.txt""), do_lower_case=True)\n\n        # prepare input\n        max_seq_len  = 24\n        input_str_batch    = [""hello, bert!"", ""how are you doing!""]\n\n        input_ids, token_type_ids = self.prepare_input_batch(input_str_batch, tokenizer, max_seq_len)\n\n        bert_ckpt_file   = os.path.join(model_dir, ""bert_model.ckpt"")\n\n        bert_params = bert.params_from_pretrained_ckpt(model_dir)\n        bert_params.adapter_size = 4\n        l_bert = bert.BertModelLayer.from_params(bert_params)\n\n        model = keras.models.Sequential([\n            l_bert,\n        ])\n\n        model.build(input_shape=(None, max_seq_len))\n\n        model.summary()\n        l_bert.apply_adapter_freeze()\n        model.summary()\n\n        bert.load_stock_weights(l_bert, bert_ckpt_file)\n        #l_bert.embeddings_layer.trainable = False\n\n        model.summary()\n\n        orig_weight_values = []\n        for weight in l_bert.weights:\n            orig_weight_values.append(weight.numpy())\n\n        model.compile(optimizer=keras.optimizers.Adam(),\n                      loss=keras.losses.mean_squared_error,\n                      run_eagerly=True)\n\n        trainable_count = len(l_bert.trainable_weights)\n\n        orig_pred = model.predict(input_ids)\n        model.fit(x=input_ids, y=np.zeros_like(orig_pred),\n          batch_size=2,\n          epochs=4)\n\n        trained_count = 0\n        for ndx, weight in enumerate(l_bert.weights):\n            weight_equal = np.array_equal(weight.numpy(), orig_weight_values[ndx])\n            print(""{}: {}"".format(weight_equal, weight.name))\n            if not weight_equal:\n                trained_count += 1\n\n        print(""  trained weights:"", trained_count)\n        print(""trainable weights:"", trainable_count)\n        self.assertEqual(trained_count, trainable_count)\n\n        model.summary()\n\n    def test_adapter_albert_freeze(self):\n        model_dir = tempfile.TemporaryDirectory().name\n        os.makedirs(model_dir)\n        # for tokenizer only\n        save_path = MiniBertFactory.create_mini_bert_weights(model_dir)\n        tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file=os.path.join(model_dir, ""vocab.txt""), do_lower_case=True)\n\n        # prepare input\n        max_seq_len  = 28\n        input_str_batch    = [""hello, albert!"", ""how are you doing!""]\n        input_ids, token_type_ids = self.prepare_input_batch(input_str_batch, tokenizer, max_seq_len,\n                                                             extra_token_count=3)\n\n        bert_params = bert.BertModelLayer.Params(\n            attention_dropout=0.1,\n            intermediate_activation=""gelu"",\n            hidden_dropout=0.1,\n            hidden_size=8,\n            initializer_range=0.02,\n            intermediate_size=32,\n            max_position_embeddings=32,\n            num_heads=2,\n            num_layers=2,\n            token_type_vocab_size=2,\n            vocab_size=len(tokenizer.vocab),\n\n            adapter_size=2,\n\n            embedding_size=4,\n            extra_tokens_vocab_size=3,\n            shared_layer=True,\n        )\n        l_bert = bert.BertModelLayer.from_params(bert_params)\n\n        model = keras.models.Sequential([\n            l_bert,\n        ])\n\n        model.build(input_shape=(None, max_seq_len))\n\n        model.summary()\n        l_bert.apply_adapter_freeze()\n        model.summary()\n\n        orig_weight_values = []\n        for weight in l_bert.weights:\n            orig_weight_values.append(weight.numpy())\n\n        model.compile(optimizer=keras.optimizers.Adam(),\n                      loss=keras.losses.mean_squared_error,\n                      run_eagerly=True)\n\n        trainable_count = len(l_bert.trainable_weights)\n\n        orig_pred = model.predict(input_ids)\n        model.fit(x=input_ids, y=np.zeros_like(orig_pred),\n          batch_size=2,\n          epochs=4)\n\n        trained_count = 0\n        for ndx, weight in enumerate(l_bert.weights):\n            weight_equal = np.array_equal(weight.numpy(), orig_weight_values[ndx])\n            print(""trained:[{}]: {}"".format(not weight_equal, weight.name))\n            if not weight_equal:\n                trained_count += 1\n\n        print(""  trained weights:"", trained_count)\n        print(""trainable weights:"", trainable_count)\n        self.assertEqual(trained_count, trainable_count)\n\n        model.summary()\n\n\n\n\n\n\n'"
tests/test_albert_create.py,6,"b'# coding=utf-8\n#\n# created by kpe on 10.Oct.2019 at 15:41\n#\n\nfrom __future__ import absolute_import, division, print_function\nimport unittest\n\nimport os\nimport tempfile\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport bert\n\nfrom .test_common import AbstractBertTest, MiniBertFactory\n\n\nclass AlbertTest(AbstractBertTest):\n\n    def setUp(self) -> None:\n        tf.compat.v1.reset_default_graph()\n        tf.compat.v1.enable_eager_execution()\n        print(""Eager Execution:"", tf.executing_eagerly())\n\n    def test_albert(self):\n        bert_params = bert.BertModelLayer.Params(hidden_size=32,\n                                                 vocab_size=67,\n                                                 max_position_embeddings=64,\n                                                 num_layers=1,\n                                                 num_heads=1,\n                                                 intermediate_size=4,\n                                                 use_token_type=False,\n\n                                                 embedding_size=16,  # using ALBERT instead of BERT\n                                                 project_embeddings_with_bias=True,\n                                                 shared_layer=True,\n                                                 extra_tokens_vocab_size=3,\n                                                 )\n\n\n        def to_model(bert_params):\n            l_bert = bert.BertModelLayer.from_params(bert_params)\n\n            token_ids = keras.layers.Input(shape=(21,))\n            seq_out = l_bert(token_ids)\n            model = keras.Model(inputs=[token_ids], outputs=seq_out)\n\n            model.build(input_shape=(None, 21))\n            l_bert.apply_adapter_freeze()\n\n            return model\n\n        model = to_model(bert_params)\n        model.summary()\n\n        print(""trainable_weights:"", len(model.trainable_weights))\n        for weight in model.trainable_weights:\n            print(weight.name, weight.shape)\n        self.assertEqual(23, len(model.trainable_weights))\n\n        # adapter-ALBERT  :-)\n\n        bert_params.adapter_size = 16\n\n        model = to_model(bert_params)\n        model.summary()\n\n        print(""trainable_weights:"", len(model.trainable_weights))\n        for weight in model.trainable_weights:\n            print(weight.name, weight.shape)\n        self.assertEqual(15, len(model.trainable_weights))\n\n        print(""non_trainable_weights:"", len(model.non_trainable_weights))\n        for weight in model.non_trainable_weights:\n            print(weight.name, weight.shape)\n        self.assertEqual(16, len(model.non_trainable_weights))\n\n    def test_albert_load_base_google_weights(self):  # for coverage mainly\n        albert_model_name = ""albert_base""\n        albert_dir = bert.fetch_tfhub_albert_model(albert_model_name, "".models"")\n        model_params = bert.albert_params(albert_model_name)\n\n        l_bert = bert.BertModelLayer.from_params(model_params, name=""albert"")\n\n        model = keras.models.Sequential([\n            keras.layers.InputLayer(input_shape=(8,), dtype=tf.int32, name=""input_ids""),\n            l_bert,\n            keras.layers.Lambda(lambda x: x[:, 0, :]),\n            keras.layers.Dense(2),\n        ])\n        model.build(input_shape=(None, 8))\n        model.compile(optimizer=keras.optimizers.Adam(),\n            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n            metrics=[keras.metrics.SparseCategoricalAccuracy(name=""acc"")])\n\n        bert.load_albert_weights(l_bert, albert_dir)\n\n        model.summary()\n\n    def test_albert_params(self):\n        albert_model_name = ""albert_base""\n        albert_dir = bert.fetch_tfhub_albert_model(albert_model_name, "".models"")\n        dir_params = bert.albert_params(albert_dir)\n        dir_params.attention_dropout = 0.1  # diff between README and assets/albert_config.json\n        dir_params.hidden_dropout = 0.1\n        name_params = bert.albert_params(albert_model_name)\n        self.assertEqual(name_params, dir_params)\n\n        # coverage\n        model_params = dir_params\n        model_params.vocab_size = model_params.vocab_size\n        model_params.adapter_size = 1\n        l_bert = bert.BertModelLayer.from_params(model_params, name=""albert"")\n        l_bert(tf.zeros((1, 128)))\n        bert.load_albert_weights(l_bert, albert_dir)\n\n    def test_albert_zh_fetch_and_load(self):\n        albert_model_name = ""albert_tiny""\n        albert_dir = bert.fetch_brightmart_albert_model(albert_model_name, "".models"")\n\n        model_params = bert.params_from_pretrained_ckpt(albert_dir)\n        model_params.vocab_size = model_params.vocab_size + 2\n        model_params.adapter_size = 1\n        l_bert = bert.BertModelLayer.from_params(model_params, name=""albert"")\n        l_bert(tf.zeros((1, 128)))\n        res = bert.load_albert_weights(l_bert, albert_dir)\n        self.assertTrue(len(res) > 0)\n\n    def test_coverage(self):\n        try:\n            bert.fetch_google_bert_model(""not-existent_bert_model"", "".models"")\n        except:\n            pass'"
tests/test_attention.py,0,"b'# coding=utf-8\n#\n# created by kpe on 30.Jul.2019 at 16:41\n#\n\nfrom __future__ import absolute_import, division, print_function\n\n\nimport unittest\n\nimport random\n\nimport bert\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\nclass TestAttention(unittest.TestCase):\n\n    def test_attention(self):\n        am = bert.AttentionLayer.create_attention_mask(from_shape=[2, 3, 5],   # B,S,..\n                                                       input_mask=[[2], [1]]   # B,seq_len\n                                                       )\n        print(am)  # [batch_size, from_seq_len, seq_len]\n\n    def test_compute_shape(self):\n        l_att = bert.AttentionLayer(num_heads=2, size_per_head=2)\n        l_att.compute_output_shape(input_shape=(16, 8, 2))'"
tests/test_common.py,8,"b'# coding=utf-8\n#\n# created by kpe on 25.Jul.2019 at 13:30\n#\n\nfrom __future__ import absolute_import, division, print_function\n\n\nimport os\nimport string\nimport unittest\nimport tempfile\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.python import keras\n\nimport bert\nfrom bert import bert_tokenization\n\n\nclass MiniBertFactory:\n\n    @staticmethod\n    def create_mini_bert_weights(model_dir=None):\n        model_dir = model_dir if model_dir is not None else tempfile.TemporaryDirectory().name\n        os.makedirs(model_dir, exist_ok=True)\n\n        from bert.loader import StockBertConfig\n\n        bert_tokens = [""[PAD]"", ""[UNK]"", ""[CLS]"", ""[SEP]"", ""[MASK]""]\n        bert_config = StockBertConfig(\n            attention_probs_dropout_prob = 0.1,\n            hidden_act                   = ""gelu"",\n            hidden_dropout_prob          = 0.1,\n            hidden_size                  = 8,\n            initializer_range            = 0.02,\n            intermediate_size            = 32,\n            max_position_embeddings      = 32,\n            num_attention_heads          = 2,\n            num_hidden_layers            = 2,\n            type_vocab_size              = 2,\n            vocab_size                   = len(string.ascii_lowercase)*2 + len(bert_tokens),\n        )\n\n        print(""creating mini BERT at:"", model_dir)\n\n        bert_config_file = os.path.join(model_dir, ""bert_config.json"")\n        bert_vocab_file  = os.path.join(model_dir, ""vocab.txt"")\n\n        with open(bert_config_file, ""w"") as f:\n            f.write(bert_config.to_json_string())\n        with open(bert_vocab_file, ""w"") as f:\n            f.write(""\\n"".join(list(string.ascii_lowercase) + bert_tokens))\n            f.write(""\\n"".join([""##""+tok for tok in list(string.ascii_lowercase)]))\n\n        with tf.Graph().as_default():\n            _ = MiniBertFactory.create_stock_bert_graph(bert_config_file, 16)\n            saver = tf.compat.v1.train.Saver(max_to_keep=1, save_relative_paths=True)\n\n            with tf.compat.v1.Session() as sess:\n                sess.run(tf.compat.v1.global_variables_initializer())\n                ckpt_path = os.path.join(model_dir, ""bert_model.ckpt"")\n                save_path = saver.save(sess, ckpt_path, write_meta_graph=True)\n                print(""saving to:"", save_path)\n\n        bert_tokenization.validate_case_matches_checkpoint(True, save_path)\n\n        return save_path\n\n    @staticmethod\n    def create_stock_bert_graph(bert_config_file, max_seq_len):\n        from tests.ext.modeling import BertModel, BertConfig\n\n        tf_placeholder = tf.compat.v1.placeholder\n\n        pl_input_ids      = tf_placeholder(tf.int32, shape=(1, max_seq_len))\n        pl_mask           = tf_placeholder(tf.int32, shape=(1, max_seq_len))\n        pl_token_type_ids = tf_placeholder(tf.int32, shape=(1, max_seq_len))\n\n        bert_config = BertConfig.from_json_file(bert_config_file)\n        s_model = BertModel(config=bert_config,\n                            is_training=False,\n                            input_ids=pl_input_ids,\n                            input_mask=pl_mask,\n                            token_type_ids=pl_token_type_ids,\n                            use_one_hot_embeddings=False)\n\n        return s_model, pl_input_ids, pl_mask, pl_token_type_ids\n\n\nclass AbstractBertTest(unittest.TestCase):\n\n    @staticmethod\n    def create_mini_bert_weights():\n        model_dir = tempfile.TemporaryDirectory().name\n        # model_dir = ""/tmp/mini_bert/"";\n        os.makedirs(model_dir, exist_ok=True)\n        save_path = MiniBertFactory.create_mini_bert_weights(model_dir)\n        print(""mini_bert save_path"", save_path)\n        print(""\\n\\t"".join([""""] + os.listdir(model_dir)))\n        return model_dir\n\n    def prepare_input_batch(self, input_str_batch, tokenizer, max_seq_len, extra_token_count=0):\n        input_ids_batch    = []\n        token_type_ids_batch = []\n\n        def extra_token_gen():\n            token = 0\n            while True:\n                yield - ((token % extra_token_count) + 1)\n                token += 1\n\n        extra_token = extra_token_gen()\n\n        for input_str in input_str_batch:\n            input_tokens = tokenizer.tokenize(input_str)\n            input_tokens = [""[CLS]""] + input_tokens + [""[SEP]""]\n\n            print(""input_tokens len:"", len(input_tokens))\n\n            input_ids      = tokenizer.convert_tokens_to_ids(input_tokens)\n            if extra_token_count > 0:\n                input_ids = [next(extra_token)] + input_ids + [next(extra_token)]\n            input_ids      = input_ids             + [0]*(max_seq_len - len(input_ids))\n            token_type_ids = [0]*len(input_ids)\n\n            input_ids_batch.append(input_ids)\n            token_type_ids_batch.append(token_type_ids)\n\n        input_ids      = np.array(input_ids_batch, dtype=np.int32)\n        token_type_ids = np.array(token_type_ids_batch, dtype=np.int32)\n\n        return input_ids, token_type_ids'"
tests/test_compare_activations.py,11,"b'# coding=utf-8\n#\n# created by kpe on 23.May.2019 at 17:10\n#\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport string\nimport unittest\nimport tempfile\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.python import keras\n\nfrom bert import bert_tokenization\n\nfrom .test_common import AbstractBertTest, MiniBertFactory\n\n\nclass CompareBertActivationsTest(AbstractBertTest):\n\n    def setUp(self):\n        tf.compat.v1.reset_default_graph()\n        keras.backend.clear_session()\n        tf.compat.v1.disable_eager_execution()\n        print(""Eager Execution:"", tf.executing_eagerly())\n\n    @staticmethod\n    def load_stock_model(model_dir, max_seq_len):\n        from tests.ext.modeling import BertModel, BertConfig, get_assignment_map_from_checkpoint\n\n        tf.compat.v1.reset_default_graph()  # to scope naming for checkpoint loading (if executed more than once)\n\n        bert_config_file = os.path.join(model_dir, ""bert_config.json"")\n        bert_ckpt_file   = os.path.join(model_dir, ""bert_model.ckpt"")\n\n        pl_input_ids      = tf.compat.v1.placeholder(tf.int32, shape=(1, max_seq_len))\n        pl_mask           = tf.compat.v1.placeholder(tf.int32, shape=(1, max_seq_len))\n        pl_token_type_ids = tf.compat.v1.placeholder(tf.int32, shape=(1, max_seq_len))\n\n        bert_config = BertConfig.from_json_file(bert_config_file)\n\n        s_model = BertModel(config=bert_config,\n                            is_training=False,\n                            input_ids=pl_input_ids,\n                            input_mask=pl_mask,\n                            token_type_ids=pl_token_type_ids,\n                            use_one_hot_embeddings=False)\n\n        tvars = tf.compat.v1.trainable_variables()\n        (assignment_map, initialized_var_names) = get_assignment_map_from_checkpoint(tvars, bert_ckpt_file)\n        tf.compat.v1.train.init_from_checkpoint(bert_ckpt_file, assignment_map)\n\n        return s_model, pl_input_ids, pl_token_type_ids, pl_mask\n\n    @staticmethod\n    def predict_on_stock_model(model_dir, input_ids, input_mask, token_type_ids):\n        max_seq_len = input_ids.shape[-1]\n        (s_model,\n         pl_input_ids, pl_token_type_ids, pl_mask) = CompareBertActivationsTest.load_stock_model(model_dir, max_seq_len)\n\n        with tf.compat.v1.Session() as sess:\n            sess.run(tf.compat.v1.global_variables_initializer())\n\n            s_res = sess.run(\n                s_model.get_sequence_output(),\n                feed_dict={pl_input_ids:      input_ids,\n                           pl_token_type_ids: token_type_ids,\n                           pl_mask:           input_mask,\n                           })\n        return s_res\n\n    @staticmethod\n    def load_keras_model(model_dir, max_seq_len):\n        from tensorflow.python import keras\n        from bert import BertModelLayer\n        from bert.loader import StockBertConfig, load_stock_weights, params_from_pretrained_ckpt\n\n        bert_config_file = os.path.join(model_dir, ""bert_config.json"")\n        bert_ckpt_file   = os.path.join(model_dir, ""bert_model.ckpt"")\n\n        l_bert = BertModelLayer.from_params(params_from_pretrained_ckpt(model_dir))\n\n        l_input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype=\'int32\', name=""input_ids"")\n        l_token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype=\'int32\', name=""token_type_ids"")\n\n        output = l_bert([l_input_ids, l_token_type_ids])\n\n        model = keras.Model(inputs=[l_input_ids, l_token_type_ids], outputs=output)\n        model.build(input_shape=[(None, max_seq_len),\n                                 (None, max_seq_len)])\n\n        load_stock_weights(l_bert, bert_ckpt_file)\n        return model\n\n    @staticmethod\n    def predict_on_keras_model(model_dir, input_ids, input_mask, token_type_ids):\n        max_seq_len = input_ids.shape[-1]\n\n        model = CompareBertActivationsTest.load_keras_model(model_dir, max_seq_len)\n\n        k_res = model.predict([input_ids, token_type_ids])\n        return k_res\n\n    def test_compare(self):\n\n        model_dir = tempfile.TemporaryDirectory().name\n        os.makedirs(model_dir)\n        save_path = MiniBertFactory.create_mini_bert_weights(model_dir)\n        tokenizer = bert_tokenization.FullTokenizer(vocab_file=os.path.join(model_dir, ""vocab.txt""), do_lower_case=True)\n\n        # prepare input\n        max_seq_len  = 16\n        input_str    = ""hello, bert!""\n        input_tokens = tokenizer.tokenize(input_str)\n        input_tokens = [""[CLS]""] + input_tokens + [""[SEP]""]\n        input_ids    = tokenizer.convert_tokens_to_ids(input_tokens)\n        input_ids      = input_ids             + [0]*(max_seq_len - len(input_tokens))\n        input_mask     = [0]*len(input_tokens) + [0]*(max_seq_len - len(input_tokens)) # FIXME: input_mask broken - chane to [1]*\n        token_type_ids = [0]*len(input_tokens) + [0]*(max_seq_len - len(input_tokens))\n\n        input_ids      = np.array([input_ids], dtype=np.int32)\n        input_mask     = np.array([input_mask], dtype=np.int32)\n        token_type_ids = np.array([token_type_ids], dtype=np.int32)\n\n        print(""   tokens:"", input_tokens)\n        print(""input_ids:{}/{}:{}"".format(len(input_tokens), max_seq_len, input_ids), input_ids.shape, token_type_ids)\n\n        bert_1_seq_out = CompareBertActivationsTest.predict_on_stock_model(model_dir, input_ids, input_mask, token_type_ids)\n        bert_2_seq_out = CompareBertActivationsTest.predict_on_keras_model(model_dir, input_ids, input_mask, token_type_ids)\n\n        np.set_printoptions(precision=9, threshold=20, linewidth=200, sign=""+"", floatmode=""fixed"")\n\n        print(""stock bert res"", bert_1_seq_out.shape)\n        print(""keras bert res"", bert_2_seq_out.shape)\n\n        print(""stock bert res:\\n {}"".format(bert_1_seq_out[0, :2, :10]), bert_1_seq_out.dtype)\n        print(""keras bert_res:\\n {}"".format(bert_2_seq_out[0, :2, :10]), bert_2_seq_out.dtype)\n\n        abs_diff = np.abs(bert_1_seq_out - bert_2_seq_out).flatten()\n        print(""abs diff:"", np.max(abs_diff), np.argmax(abs_diff))\n        self.assertTrue(np.allclose(bert_1_seq_out, bert_2_seq_out, atol=1e-6))\n\n    def test_finetune(self):\n\n\n        model_dir = tempfile.TemporaryDirectory().name\n        os.makedirs(model_dir)\n        save_path = MiniBertFactory.create_mini_bert_weights(model_dir)\n        tokenizer = bert_tokenization.FullTokenizer(vocab_file=os.path.join(model_dir, ""vocab.txt""), do_lower_case=True)\n\n        # prepare input\n        max_seq_len  = 24\n        input_str_batch    = [""hello, bert!"", ""how are you doing!""]\n\n        input_ids_batch    = []\n        token_type_ids_batch = []\n        for input_str in input_str_batch:\n            input_tokens = tokenizer.tokenize(input_str)\n            input_tokens = [""[CLS]""] + input_tokens + [""[SEP]""]\n\n            print(""input_tokens len:"", len(input_tokens))\n\n            input_ids      = tokenizer.convert_tokens_to_ids(input_tokens)\n            input_ids      = input_ids             + [0]*(max_seq_len - len(input_tokens))\n            token_type_ids = [0]*len(input_tokens) + [0]*(max_seq_len - len(input_tokens))\n\n            input_ids_batch.append(input_ids)\n            token_type_ids_batch.append(token_type_ids)\n\n        input_ids      = np.array(input_ids_batch, dtype=np.int32)\n        token_type_ids = np.array(token_type_ids_batch, dtype=np.int32)\n\n        print(""   tokens:"", input_tokens)\n        print(""input_ids:{}/{}:{}"".format(len(input_tokens), max_seq_len, input_ids), input_ids.shape, token_type_ids)\n\n        model = CompareBertActivationsTest.load_keras_model(model_dir, max_seq_len)\n        model.compile(optimizer=keras.optimizers.Adam(),\n                      loss=keras.losses.mean_squared_error)\n\n        pres = model.predict([input_ids, token_type_ids])  # just for fetching the shape of the output\n        print(""pres:"", pres.shape)\n\n        model.fit(x=(input_ids, token_type_ids),\n                  y=np.zeros_like(pres),\n                  batch_size=2,\n                  epochs=2)\n'"
tests/test_eager.py,6,"b'# coding=utf-8\n#\n# created by kpe on 25.Jul.2019 at 13:24\n#\n\nfrom __future__ import absolute_import, division, print_function\n\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport params_flow as pf\n\nfrom bert import loader, BertModelLayer\n\nfrom .test_common import AbstractBertTest\n\n\nclass LoaderTest(AbstractBertTest):\n\n    def setUp(self) -> None:\n        tf.compat.v1.reset_default_graph()\n        tf.compat.v1.enable_eager_execution()\n        print(""Eager Execution:"", tf.executing_eagerly())\n\n    def test_coverage_improve(self):\n        for act in [""relu"", ""gelu"", ""linear"", None]:\n            BertModelLayer.get_activation(act)\n        try:\n            BertModelLayer.get_activation(""None"")\n        except ValueError:\n            pass\n\n    def test_eager_loading(self):\n        print(""Eager Execution:"", tf.executing_eagerly())\n\n        # a temporal mini bert model_dir\n        model_dir = self.create_mini_bert_weights()\n\n        bert_params = loader.params_from_pretrained_ckpt(model_dir)\n        bert_params.adapter_size = 32\n        bert = BertModelLayer.from_params(bert_params, name=""bert"")\n\n        model = keras.models.Sequential([\n            keras.layers.InputLayer(input_shape=(32,)),\n            bert,\n            keras.layers.Lambda(lambda x: x[:, 0, :]),\n            keras.layers.Dense(2)\n        ])\n\n        model.build(input_shape=(None, 128))\n        model.compile(optimizer=keras.optimizers.Adam(),\n                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                      metrics=[keras.metrics.SparseCategoricalAccuracy(name=""acc"")],\n                      run_eagerly=True)\n\n        loader.load_stock_weights(bert, model_dir)\n\n        model.summary()\n\n    def test_concat(self):\n        model_dir = self.create_mini_bert_weights()\n\n        bert_params = loader.params_from_pretrained_ckpt(model_dir)\n        bert_params.adapter_size = 32\n        bert = BertModelLayer.from_params(bert_params, name=""bert"")\n\n        max_seq_len = 4\n\n        model = keras.models.Sequential([\n            keras.layers.InputLayer(input_shape=(max_seq_len,)),\n            bert,\n            keras.layers.TimeDistributed(keras.layers.Dense(bert_params.hidden_size)),\n            keras.layers.TimeDistributed(keras.layers.LayerNormalization()),\n            keras.layers.TimeDistributed(keras.layers.Activation(""tanh"")),\n\n            pf.Concat([\n                keras.layers.Lambda(lambda x: tf.math.reduce_max(x, axis=1)),  # GlobalMaxPooling1D\n                keras.layers.Lambda(lambda x: tf.math.reduce_mean(x, axis=1)),  # GlobalAvgPooling1\n            ]),\n\n            keras.layers.Dense(units=bert_params.hidden_size),\n            keras.layers.Activation(""tanh""),\n\n            keras.layers.Dense(units=2)\n        ])\n\n        model.build(input_shape=(None, max_seq_len))\n        model.summary()\n\n        model.compile(optimizer=keras.optimizers.Adam(),\n                      loss=[keras.losses.SparseCategoricalCrossentropy(from_logits=True)],\n                      metrics=[keras.metrics.SparseCategoricalAccuracy()],\n                      run_eagerly = True)\n\n        loader.load_stock_weights(bert, model_dir)\n\n        model.summary()\n'"
tests/test_extend_segments.py,7,"b'# coding=utf-8\n#\n# created by kpe on 02.Sep.2019 at 11:57\n#\n\nfrom __future__ import absolute_import, division, print_function\n\nimport unittest\n\nimport os\nimport re\nimport tempfile\n\nimport bert\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom .test_common import AbstractBertTest, MiniBertFactory\n\n#tf.enable_eager_execution()\n#tf.disable_eager_execution()\n\n\nclass TestExtendSegmentVocab(AbstractBertTest):\n\n    def setUp(self) -> None:\n        tf.compat.v1.reset_default_graph()\n        tf.compat.v1.enable_eager_execution()\n        print(""Eager Execution:"", tf.executing_eagerly())\n\n    def test_extend_pretrained_segments(self):\n\n        model_dir = tempfile.TemporaryDirectory().name\n        os.makedirs(model_dir)\n        save_path = MiniBertFactory.create_mini_bert_weights(model_dir)\n        tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file=os.path.join(model_dir, ""vocab.txt""), do_lower_case=True)\n\n        ckpt_dir = os.path.dirname(save_path)\n        bert_params = bert.params_from_pretrained_ckpt(ckpt_dir)\n\n        self.assertEqual(bert_params.token_type_vocab_size, 2)\n        bert_params.token_type_vocab_size = 4\n\n        l_bert = bert.BertModelLayer.from_params(bert_params)\n\n        # we dummy call the layer once in order to instantiate the weights\n        l_bert([np.array([[1, 1, 0]]),\n                np.array([[1, 0, 0]])])#, mask=[[True, True, False]])\n\n        #\n        # - load the weights from a pre-trained model,\n        # - expect a mismatch for the token_type embeddings\n        # - use the segment/token type id=0 embedding for the missing token types\n        #\n        mismatched = bert.load_stock_weights(l_bert, save_path)\n\n        self.assertEqual(1, len(mismatched), ""token_type embeddings should have mismatched shape"")\n\n        for weight, value in mismatched:\n            if re.match(""(.*)embeddings/token_type_embeddings/embeddings:0"", weight.name):\n                seg0_emb = value[:1, :]\n                new_segment_embeddings = np.repeat(seg0_emb, (weight.shape[0]-value.shape[0]), axis=0)\n                new_value = np.concatenate([value, new_segment_embeddings], axis=0)\n                keras.backend.batch_set_value([(weight, new_value)])\n\n        tte = l_bert.embeddings_layer.token_type_embeddings_layer.weights[0]\n\n        if not tf.executing_eagerly():\n            with tf.keras.backend.get_session() as sess:\n                tte, = sess.run((tte, ))\n\n        self.assertTrue(np.allclose(seg0_emb, tte[0], 1e-6))\n        self.assertFalse(np.allclose(seg0_emb, tte[1], 1e-6))\n        self.assertTrue(np.allclose(seg0_emb, tte[2], 1e-6))\n        self.assertTrue(np.allclose(seg0_emb, tte[3], 1e-6))\n\n        bert_params.token_type_vocab_size = 4\n        print(""token_type_vocab_size"", bert_params.token_type_vocab_size)\n        print(l_bert.embeddings_layer.trainable_weights[1])\n\n\n'"
tests/test_extend_tokens.py,3,"b'# coding=utf-8\n#\n# created by kpe on 04.11.2019 at 2:07 PM\n#\n\nfrom __future__ import division, absolute_import, print_function\n\nimport unittest\n\nimport os\nimport tempfile\n\nimport numpy as np\nimport tensorflow as tf\n\nimport bert\n\nfrom .test_common import AbstractBertTest, MiniBertFactory\n\n\nclass TestExtendSegmentVocab(AbstractBertTest):\n\n    def setUp(self) -> None:\n        tf.compat.v1.reset_default_graph()\n        tf.compat.v1.enable_eager_execution()\n        print(""Eager Execution:"", tf.executing_eagerly())\n\n    def test_extend_pretrained_tokens(self):\n        model_dir = tempfile.TemporaryDirectory().name\n        os.makedirs(model_dir)\n        save_path = MiniBertFactory.create_mini_bert_weights(model_dir)\n        tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file=os.path.join(model_dir, ""vocab.txt""), do_lower_case=True)\n\n        ckpt_dir = os.path.dirname(save_path)\n        bert_params = bert.params_from_pretrained_ckpt(ckpt_dir)\n\n        self.assertEqual(bert_params.token_type_vocab_size, 2)\n        bert_params.extra_tokens_vocab_size = 3\n\n        l_bert = bert.BertModelLayer.from_params(bert_params)\n\n        # we dummy call the layer once in order to instantiate the weights\n        l_bert([np.array([[1, 1, 0]]), np.array([[1, 0, 0]])], mask=[[True, True, False]])\n\n        mismatched = bert.load_stock_weights(l_bert, save_path)\n        self.assertEqual(0, len(mismatched), ""token_type embeddings should have mismatched shape"")\n\n        l_bert([np.array([[1, -3, 0]]), np.array([[1, 0, 0]])], mask=[[True, True, False]])\n'"
bert/tokenization/__init__.py,0,"b'# coding=utf-8\n#\n# created by kpe on 18.Nov.2019 at 07:18\n#\nfrom __future__ import division, absolute_import, print_function\n\n'"
bert/tokenization/albert_tokenization.py,7,"b'# coding=utf-8\n# Copyright 2018 The Google AI Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Lint as: python2, python3\n# coding=utf-8\n""""""Tokenization classes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport unicodedata\nimport six\nfrom six.moves import range\nimport tensorflow.compat.v1 as tf\n\nSPIECE_UNDERLINE = u""\xe2\x96\x81"".encode(""utf-8"")\n\n\ndef validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n    """"""Checks whether the casing config is consistent with the checkpoint name.""""""\n\n    # The casing has to be passed in by the user and there is no explicit check\n    # as to whether it matches the checkpoint. The casing information probably\n    # should have been stored in the bert_config.json file, but it\'s not, so\n    # we have to heuristically detect it to validate.\n\n    if not init_checkpoint:\n        return\n\n    m = re.match(""^.*?([A-Za-z0-9_-]+)/bert_model.ckpt"",\n                 six.ensure_str(init_checkpoint))\n    if m is None:\n        return\n\n    model_name = m.group(1)\n\n    lower_models = [\n        ""uncased_L-24_H-1024_A-16"", ""uncased_L-12_H-768_A-12"",\n        ""multilingual_L-12_H-768_A-12"", ""chinese_L-12_H-768_A-12""\n    ]\n\n    cased_models = [\n        ""cased_L-12_H-768_A-12"", ""cased_L-24_H-1024_A-16"",\n        ""multi_cased_L-12_H-768_A-12""\n    ]\n\n    is_bad_config = False\n    if model_name in lower_models and not do_lower_case:\n        is_bad_config = True\n        actual_flag = ""False""\n        case_name = ""lowercased""\n        opposite_flag = ""True""\n\n    if model_name in cased_models and do_lower_case:\n        is_bad_config = True\n        actual_flag = ""True""\n        case_name = ""cased""\n        opposite_flag = ""False""\n\n    if is_bad_config:\n        raise ValueError(\n            ""You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. ""\n            ""However, `%s` seems to be a %s model, so you ""\n            ""should pass in `--do_lower_case=%s` so that the fine-tuning matches ""\n            ""how the model was pre-training. If this error is wrong, please ""\n            ""just comment out this check."" % (actual_flag, init_checkpoint,\n                                              model_name, case_name, opposite_flag))\n\n\ndef preprocess_text(inputs, remove_space=True, lower=False):\n    """"""preprocess data by removing extra space and normalize data.""""""\n    outputs = inputs\n    if remove_space:\n        outputs = "" "".join(inputs.strip().split())\n\n    if six.PY2 and isinstance(outputs, str):\n        try:\n            outputs = six.ensure_text(outputs, ""utf-8"")\n        except UnicodeDecodeError:\n            outputs = six.ensure_text(outputs, ""latin-1"")\n\n    outputs = unicodedata.normalize(""NFKD"", outputs)\n    outputs = """".join([c for c in outputs if not unicodedata.combining(c)])\n    if lower:\n        outputs = outputs.lower()\n\n    return outputs\n\n\ndef encode_pieces(sp_model, text, return_unicode=True, sample=False):\n    """"""turn sentences into word pieces.""""""\n\n    if six.PY2 and isinstance(text, six.text_type):\n        text = six.ensure_binary(text, ""utf-8"")\n\n    if not sample:\n        pieces = sp_model.EncodeAsPieces(text)\n    else:\n        pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n    new_pieces = []\n    for piece in pieces:\n        piece = printable_text(piece)\n        if len(piece) > 1 and piece[-1] == "","" and piece[-2].isdigit():\n            cur_pieces = sp_model.EncodeAsPieces(\n                six.ensure_binary(piece[:-1]).replace(SPIECE_UNDERLINE, b""""))\n            if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n                if len(cur_pieces[0]) == 1:\n                    cur_pieces = cur_pieces[1:]\n                else:\n                    cur_pieces[0] = cur_pieces[0][1:]\n            cur_pieces.append(piece[-1])\n            new_pieces.extend(cur_pieces)\n        else:\n            new_pieces.append(piece)\n\n    # note(zhiliny): convert back to unicode for py2\n    if six.PY2 and return_unicode:\n        ret_pieces = []\n        for piece in new_pieces:\n            if isinstance(piece, str):\n                piece = six.ensure_text(piece, ""utf-8"")\n            ret_pieces.append(piece)\n        new_pieces = ret_pieces\n\n    return new_pieces\n\n\ndef encode_ids(sp_model, text, sample=False):\n    pieces = encode_pieces(sp_model, text, return_unicode=False, sample=sample)\n    ids = [sp_model.PieceToId(piece) for piece in pieces]\n    return ids\n\n\ndef convert_to_unicode(text):\n    """"""Converts `text` to Unicode (if it\'s not already), assuming utf-8 input.""""""\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return six.ensure_text(text, ""utf-8"", ""ignore"")\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return six.ensure_text(text, ""utf-8"", ""ignore"")\n        elif isinstance(text, six.text_type):\n            return text\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    else:\n        raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef printable_text(text):\n    """"""Returns text encoded in a way suitable for print or `tf.logging`.""""""\n\n    # These functions want `str` for both Python2 and Python3, but in one case\n    # it\'s a Unicode string and in the other it\'s a byte string.\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return six.ensure_text(text, ""utf-8"", ""ignore"")\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, six.text_type):\n            return six.ensure_binary(text, ""utf-8"")\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    else:\n        raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef load_vocab(vocab_file):\n    """"""Loads a vocabulary file into a dictionary.""""""\n    vocab = collections.OrderedDict()\n    with tf.io.gfile.GFile(vocab_file, ""r"") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            if token:\n                token = token.split()[0]\n            if token not in vocab:\n                vocab[token] = len(vocab)\n    return vocab\n\n\ndef convert_by_vocab(vocab, items):\n    """"""Converts a sequence of [tokens|ids] using the vocab.""""""\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n    return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n    return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n    """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass FullTokenizer(object):\n    """"""Runs end-to-end tokenziation.""""""\n\n    def __init__(self, vocab_file, do_lower_case=True, spm_model_file=None):\n        self.vocab = None\n        self.sp_model = None\n        if spm_model_file:\n            import sentencepiece as spm\n\n            self.sp_model = spm.SentencePieceProcessor()\n            tf.compat.v1.logging.info(""loading sentence piece model"")\n            self.sp_model.Load(spm_model_file)\n            # Note(mingdachen): For the purpose of consisent API, we are\n            # generating a vocabulary for the sentence piece tokenizer.\n            self.vocab = {self.sp_model.IdToPiece(i): i for i\n                          in range(self.sp_model.GetPieceSize())}\n        else:\n            self.vocab = load_vocab(vocab_file)\n            self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n            self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n\n    @classmethod\n    def from_scratch(cls, vocab_file, do_lower_case, spm_model_file):\n        return FullTokenizer(vocab_file, do_lower_case, spm_model_file)\n\n    @classmethod\n    def from_hub_module(cls, hub_module, spm_model_file):\n        """"""Get the vocab file and casing info from the Hub module.""""""\n        import tensorflow_hub as hub\n        with tf.Graph().as_default():\n            albert_module = hub.Module(hub_module)\n            tokenization_info = albert_module(signature=""tokenization_info"",\n                                              as_dict=True)\n            with tf.Session() as sess:\n                vocab_file, do_lower_case = sess.run(\n                    [tokenization_info[""vocab_file""],\n                     tokenization_info[""do_lower_case""]])\n        return FullTokenizer(\n            vocab_file=vocab_file, do_lower_case=do_lower_case,\n            spm_model_file=spm_model_file)\n\n    def tokenize(self, text):\n        if self.sp_model:\n            split_tokens = encode_pieces(self.sp_model, text, return_unicode=False)\n        else:\n            split_tokens = []\n            for token in self.basic_tokenizer.tokenize(text):\n                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                    split_tokens.append(sub_token)\n\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        if self.sp_model:\n            tf.compat.v1.logging.info(""using sentence piece tokenzier."")\n            return [self.sp_model.PieceToId(\n                printable_text(token)) for token in tokens]\n        else:\n            return convert_by_vocab(self.vocab, tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        if self.sp_model:\n            tf.compat.v1.logging.info(""using sentence piece tokenzier."")\n            return [self.sp_model.IdToPiece(id_) for id_ in ids]\n        else:\n            return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n    """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n    def __init__(self, do_lower_case=True):\n        """"""Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        """"""\n        self.do_lower_case = do_lower_case\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text.""""""\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn\'t\n        # matter since the English models were not trained on any Chinese data\n        # and generally don\'t have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        """"""Strips accents from a piece of text.""""""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    def _run_split_on_punc(self, text):\n        """"""Splits punctuation on a piece of text.""""""\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        """"""Adds whitespace around any CJK character.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    def _is_chinese_char(self, cp):\n        """"""Checks whether CP is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        """"""Performs invalid character removal and whitespace cleanup on text.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenziation.""""""\n\n    def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=200):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = ""unaffable""\n          output = [""un"", ""##aff"", ""##able""]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n\n        Returns:\n          A list of wordpiece tokens.\n        """"""\n\n        text = convert_to_unicode(text)\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + six.ensure_str(substr)\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    """"""Checks whether `chars` is a whitespace character.""""""\n    # \\t, \\n, and \\r are technically control characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char):\n    """"""Checks whether `chars` is a control character.""""""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return False\n    cat = unicodedata.category(char)\n    if cat in (""Cc"", ""Cf""):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n'"
bert/tokenization/bert_tokenization.py,2,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport unicodedata\nimport six\nimport tensorflow as tf\n\n\ndef validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n    """"""Checks whether the casing config is consistent with the checkpoint name.""""""\n\n    # The casing has to be passed in by the user and there is no explicit check\n    # as to whether it matches the checkpoint. The casing information probably\n    # should have been stored in the bert_config.json file, but it\'s not, so\n    # we have to heuristically detect it to validate.\n\n    if not init_checkpoint:\n        return\n\n    m = re.match(""^.*?([A-Za-z0-9_-]+)/bert_model.ckpt"", init_checkpoint)\n    if m is None:\n        return\n\n    model_name = m.group(1)\n\n    lower_models = [\n        ""uncased_L-24_H-1024_A-16"", ""uncased_L-12_H-768_A-12"",\n        ""multilingual_L-12_H-768_A-12"", ""chinese_L-12_H-768_A-12""\n    ]\n\n    cased_models = [\n        ""cased_L-12_H-768_A-12"", ""cased_L-24_H-1024_A-16"",\n        ""multi_cased_L-12_H-768_A-12""\n    ]\n\n    is_bad_config = False\n    if model_name in lower_models and not do_lower_case:\n        is_bad_config = True\n        actual_flag = ""False""\n        case_name = ""lowercased""\n        opposite_flag = ""True""\n\n    if model_name in cased_models and do_lower_case:\n        is_bad_config = True\n        actual_flag = ""True""\n        case_name = ""cased""\n        opposite_flag = ""False""\n\n    if is_bad_config:\n        raise ValueError(\n            ""You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. ""\n            ""However, `%s` seems to be a %s model, so you ""\n            ""should pass in `--do_lower_case=%s` so that the fine-tuning matches ""\n            ""how the model was pre-training. If this error is wrong, please ""\n            ""just comment out this check."" % (actual_flag, init_checkpoint,\n                                              model_name, case_name, opposite_flag))\n\n\ndef convert_to_unicode(text):\n    """"""Converts `text` to Unicode (if it\'s not already), assuming utf-8 input.""""""\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(""utf-8"", ""ignore"")\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text.decode(""utf-8"", ""ignore"")\n        elif isinstance(text, unicode):\n            return text\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    else:\n        raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef printable_text(text):\n    """"""Returns text encoded in a way suitable for print or `tf.logging`.""""""\n\n    # These functions want `str` for both Python2 and Python3, but in one case\n    # it\'s a Unicode string and in the other it\'s a byte string.\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(""utf-8"", ""ignore"")\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, unicode):\n            return text.encode(""utf-8"")\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    else:\n        raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef load_vocab(vocab_file):\n    """"""Loads a vocabulary file into a dictionary.""""""\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.io.gfile.GFile(vocab_file, ""r"") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef convert_by_vocab(vocab, items):\n    """"""Converts a sequence of [tokens|ids] using the vocab.""""""\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n    return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n    return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n    """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass FullTokenizer(object):\n    """"""Runs end-to-end tokenziation.""""""\n\n    def __init__(self, vocab_file, do_lower_case=True):\n        self.vocab = load_vocab(vocab_file)\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n    def tokenize(self, text):\n        split_tokens = []\n        for token in self.basic_tokenizer.tokenize(text):\n            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                split_tokens.append(sub_token)\n\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        return convert_by_vocab(self.vocab, tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n    """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n    def __init__(self, do_lower_case=True):\n        """"""Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        """"""\n        self.do_lower_case = do_lower_case\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text.""""""\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn\'t\n        # matter since the English models were not trained on any Chinese data\n        # and generally don\'t have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        """"""Strips accents from a piece of text.""""""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    def _run_split_on_punc(self, text):\n        """"""Splits punctuation on a piece of text.""""""\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        """"""Adds whitespace around any CJK character.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    def _is_chinese_char(self, cp):\n        """"""Checks whether CP is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        """"""Performs invalid character removal and whitespace cleanup on text.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenziation.""""""\n\n    def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=200):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = ""unaffable""\n          output = [""un"", ""##aff"", ""##able""]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n\n        Returns:\n          A list of wordpiece tokens.\n        """"""\n\n        text = convert_to_unicode(text)\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    """"""Checks whether `chars` is a whitespace character.""""""\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char):\n    """"""Checks whether `chars` is a control character.""""""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return False\n    cat = unicodedata.category(char)\n    if cat in (""Cc"", ""Cf""):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n'"
tests/ext/__init__.py,0,"b'# coding=utf-8\n#\n# created by kpe on 28.Mar.2019 at 15:56\n#\n\nfrom __future__ import absolute_import, division, print_function\n'"
tests/ext/modeling.py,89,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""The main BERT model and related functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nimport math\nimport re\nimport numpy as np\nimport six\nimport tensorflow as tf\n\n\n\nclass BertConfig(object):\n  """"""Configuration for `BertModel`.""""""\n\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=""gelu"",\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=16,\n               initializer_range=0.02):\n    """"""Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    """"""\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    """"""Constructs a `BertConfig` from a json file of parameters.""""""\n    with tf.io.gfile.GFile(json_file, ""r"") as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    """"""Serializes this instance to a Python dictionary.""""""\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    """"""Serializes this instance to a JSON string.""""""\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\nclass BertModel(object):\n  """"""BERT model (""Bidirectional Encoder Representations from Transformers"").\n\n  Example usage:\n\n  ```python\n  # Already been converted into WordPiece token ids\n  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n\n  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n\n  model = modeling.BertModel(config=config, is_training=True,\n    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n\n  label_embeddings = tf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  """"""\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    """"""Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to ""bert"".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    """"""\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.compat.v1.variable_scope(scope, default_name=""bert""):\n      with tf.compat.v1.variable_scope(""embeddings""):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=""word_embeddings"",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=""token_type_embeddings"",\n            use_position_embeddings=True,\n            position_embedding_name=""position_embeddings"",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.compat.v1.variable_scope(""encoder""):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The ""pooler"" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.compat.v1.variable_scope(""pooler""):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.compat.v1.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    """"""Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    """"""\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    """"""Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    """"""\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n\ndef gelu(x):\n  """"""Gaussian Error Linear Unit.\n\n  This is a smoother version of the RELU.\n  Original paper: https://arxiv.org/abs/1606.08415\n  Args:\n    x: float Tensor to perform activation.\n\n  Returns:\n    `x` with the GELU activation applied.\n  """"""\n  cdf = 0.5 * (1.0 + tf.tanh(\n      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n  return x * cdf\n\n\ndef get_activation(activation_string):\n  """"""Maps a string to a Python function, e.g., ""relu"" => `tf.nn.relu`.\n\n  Args:\n    activation_string: String name of the activation function.\n\n  Returns:\n    A Python function corresponding to the activation function. If\n    `activation_string` is None, empty, or ""linear"", this will return None.\n    If `activation_string` is not a string, it will return `activation_string`.\n\n  Raises:\n    ValueError: The `activation_string` does not correspond to a known\n      activation.\n  """"""\n\n  # We assume that anything that""s not a string is already an activation\n  # function, so we just return it.\n  if not isinstance(activation_string, six.string_types):\n    return activation_string\n\n  if not activation_string:\n    return None\n\n  act = activation_string.lower()\n  if act == ""linear"":\n    return None\n  elif act == ""relu"":\n    return tf.nn.relu\n  elif act == ""gelu"":\n    return gelu\n  elif act == ""tanh"":\n    return tf.tanh\n  else:\n    raise ValueError(""Unsupported activation: %s"" % act)\n\n\ndef get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n  """"""Compute the union of the current variables and checkpoint variables.""""""\n  assignment_map = {}\n  initialized_variable_names = {}\n\n  name_to_variable = collections.OrderedDict()\n  for var in tvars:\n    name = var.name\n    m = re.match(""^(.*):\\\\d+$"", name)\n    if m is not None:\n      name = m.group(1)\n    name_to_variable[name] = var\n\n  init_vars = tf.train.list_variables(init_checkpoint)\n\n  assignment_map = collections.OrderedDict()\n  for x in init_vars:\n    (name, var) = (x[0], x[1])\n    if name not in name_to_variable:\n      continue\n    assignment_map[name] = name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + "":0""] = 1\n\n  return (assignment_map, initialized_variable_names)\n\n\ndef dropout(input_tensor, dropout_prob):\n  """"""Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probability of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  """"""\n  if dropout_prob is None or dropout_prob == 0.0:\n    return input_tensor\n\n  output = tf.nn.dropout(input_tensor, rate=dropout_prob)\n  return output\n\n\ndef layer_norm(input_tensor, name=None):\n  """"""Run layer normalization on the last dimension of the tensor.""""""\n  # return tf.contrib.layers.layer_norm(\n  #     inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n  \n  epsilon         = 1e-12\n\n  input_shape = input_tensor.shape\n  with tf.compat.v1.variable_scope(""LayerNorm""):\n      gamma = tf.compat.v1.get_variable(name=""gamma"", shape=input_shape[-1:], initializer=tf.compat.v1.initializers.ones(), trainable=True)\n      beta = tf.compat.v1.get_variable(name=""beta"", shape=input_shape[-1:], initializer=tf.compat.v1.initializers.zeros(), trainable=True)\n\n  x = input_tensor\n  if tf.__version__.startswith(""2.""):\n    mean, var = tf.nn.moments(x=x, axes=-1, keepdims=True)\n  else:\n    mean, var = tf.nn.moments(x, axes=-1, keep_dims=True)\n\n  inv = gamma * tf.math.rsqrt(var + epsilon)\n  res = x * tf.cast(inv, x.dtype) + tf.cast(beta - mean * inv, x.dtype)\n\n  return res\n\n\n\ndef layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n  """"""Runs layer normalization followed by dropout.""""""\n  output_tensor = layer_norm(input_tensor, name)\n  output_tensor = dropout(output_tensor, dropout_prob)\n  return output_tensor\n\n\ndef create_initializer(initializer_range=0.02):\n  """"""Creates a `truncated_normal_initializer` with the given range.""""""\n  return tf.compat.v1.initializers.truncated_normal(stddev=initializer_range)\n\n\ndef embedding_lookup(input_ids,\n                     vocab_size,\n                     embedding_size=128,\n                     initializer_range=0.02,\n                     word_embedding_name=""word_embeddings"",\n                     use_one_hot_embeddings=False):\n  """"""Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  """"""\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.compat.v1.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  flat_input_ids = tf.reshape(input_ids, [-1])\n  if use_one_hot_embeddings:\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.gather(embedding_table, flat_input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n\ndef embedding_postprocessor(input_tensor,\n                            use_token_type=False,\n                            token_type_ids=None,\n                            token_type_vocab_size=16,\n                            token_type_embedding_name=""token_type_embeddings"",\n                            use_position_embeddings=True,\n                            position_embedding_name=""position_embeddings"",\n                            initializer_range=0.02,\n                            max_position_embeddings=512,\n                            dropout_prob=0.1):\n  """"""Performs various post-processing on a word embedding tensor.\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length,\n      embedding_size].\n    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      Must be specified if `use_token_type` is True.\n    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n    token_type_embedding_name: string. The name of the embedding table variable\n      for token type ids.\n    use_position_embeddings: bool. Whether to add position embeddings for the\n      position of each token in the sequence.\n    position_embedding_name: string. The name of the embedding table variable\n      for positional embeddings.\n    initializer_range: float. Range of the weight initialization.\n    max_position_embeddings: int. Maximum sequence length that might ever be\n      used with this model. This can be longer than the sequence length of\n      input_tensor, but cannot be shorter.\n    dropout_prob: float. Dropout probability applied to the final output tensor.\n\n  Returns:\n    float tensor with same shape as `input_tensor`.\n\n  Raises:\n    ValueError: One of the tensor shapes or input values is invalid.\n  """"""\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  width = input_shape[2]\n\n  output = input_tensor\n\n  if use_token_type:\n    if token_type_ids is None:\n      raise ValueError(""`token_type_ids` must be specified if""\n                       ""`use_token_type` is True."")\n    token_type_table = tf.compat.v1.get_variable(\n        name=token_type_embedding_name,\n        shape=[token_type_vocab_size, width],\n        initializer=create_initializer(initializer_range), use_resource=False)\n    # This vocab will be small so we always do one-hot here, since it is always\n    # faster for a small vocabulary.\n    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n    token_type_embeddings = tf.reshape(token_type_embeddings,\n                                       [batch_size, seq_length, width])\n    output += token_type_embeddings\n\n  if use_position_embeddings:\n    assert_op = tf.compat.v1.assert_less_equal(seq_length, max_position_embeddings)\n    with tf.control_dependencies([assert_op]):\n      full_position_embeddings = tf.compat.v1.get_variable(\n          name=position_embedding_name,\n          shape=[max_position_embeddings, width],\n          initializer=create_initializer(initializer_range), use_resource=False)\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n\ndef create_attention_mask_from_input_mask(from_tensor, to_mask):\n  """"""Create 3D attention mask from a 2D tensor mask.\n\n  Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  """"""\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  batch_size = from_shape[0]\n  from_seq_length = from_shape[1]\n\n  to_shape = get_shape_list(to_mask, expected_rank=2)\n  to_seq_length = to_shape[1]\n\n  to_mask = tf.cast(\n      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n  # We don\'t assume that `from_tensor` is a mask (although it could be). We\n  # don\'t actually care if we attend *from* padding tokens (only *to* padding)\n  # tokens so we create a tensor of all ones.\n  #\n  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n  broadcast_ones = tf.ones(\n      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n  # Here we broadcast along two dimensions to create the mask.\n  mask = broadcast_ones * to_mask\n\n  return mask\n\n\ndef attention_layer(from_tensor,\n                    to_tensor,\n                    attention_mask=None,\n                    num_attention_heads=1,\n                    size_per_head=512,\n                    query_act=None,\n                    key_act=None,\n                    value_act=None,\n                    attention_probs_dropout_prob=0.0,\n                    initializer_range=0.02,\n                    do_return_2d_tensor=False,\n                    batch_size=None,\n                    from_seq_length=None,\n                    to_seq_length=None):\n  """"""Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on ""Attention\n  is all you Need"". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a ""query"" tensor and\n  `to_tensor` into ""key"" and ""value"" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchanged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n      attention probabilities.\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  """"""\n\n  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width):\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(a=output_tensor, perm=[0, 2, 1, 3])\n    return output_tensor\n\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n\n  if len(from_shape) != len(to_shape):\n    raise ValueError(\n        ""The rank of `from_tensor` must match the rank of `to_tensor`."")\n\n  if len(from_shape) == 3:\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n  elif len(from_shape) == 2:\n    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n      raise ValueError(\n          ""When passing in rank 2 tensors to attention_layer, the values ""\n          ""for `batch_size`, `from_seq_length`, and `to_seq_length` ""\n          ""must all be specified."")\n\n  # Scalar dimensions referenced here:\n  #   B = batch size (number of sequences)\n  #   F = `from_tensor` sequence length\n  #   T = `to_tensor` sequence length\n  #   N = `num_attention_heads`\n  #   H = `size_per_head`\n\n  from_tensor_2d = reshape_to_matrix(from_tensor)\n  to_tensor_2d = reshape_to_matrix(to_tensor)\n\n  # `query_layer` = [B*F, N*H]\n  query_layer = tf.compat.v1.layers.dense(\n      from_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=query_act,\n      name=""query"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `key_layer` = [B*T, N*H]\n  key_layer = tf.compat.v1.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=key_act,\n      name=""key"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `value_layer` = [B*T, N*H]\n  value_layer = tf.compat.v1.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=value_act,\n      name=""value"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `query_layer` = [B, N, F, H]\n  query_layer = transpose_for_scores(query_layer, batch_size,\n                                     num_attention_heads, from_seq_length,\n                                     size_per_head)\n\n  # `key_layer` = [B, N, T, H]\n  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n                                   to_seq_length, size_per_head)\n\n  # Take the dot product between ""query"" and ""key"" to get the raw\n  # attention scores.\n  # `attention_scores` = [B, N, F, T]\n  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n  attention_scores = tf.multiply(attention_scores,\n                                 1.0 / math.sqrt(float(size_per_head)))\n\n  if attention_mask is not None:\n    # `attention_mask` = [B, 1, F, T]\n    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n\n    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n    # masked positions, this operation will create a tensor which is 0.0 for\n    # positions we want to attend and -10000.0 for masked positions.\n    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n\n    # Since we are adding it to the raw scores before the softmax, this is\n    # effectively the same as removing these entirely.\n    attention_scores += adder\n\n  # Normalize the attention scores to probabilities.\n  # `attention_probs` = [B, N, F, T]\n  attention_probs = tf.nn.softmax(attention_scores)\n\n  # This is actually dropping out entire tokens to attend to, which might\n  # seem a bit unusual, but is taken from the original Transformer paper.\n  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n\n  # `value_layer` = [B, T, N, H]\n  value_layer = tf.reshape(\n      value_layer,\n      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n\n  # `value_layer` = [B, N, T, H]\n  value_layer = tf.transpose(a=value_layer, perm=[0, 2, 1, 3])\n\n  # `context_layer` = [B, N, F, H]\n  context_layer = tf.matmul(attention_probs, value_layer)\n\n  # `context_layer` = [B, F, N, H]\n  context_layer = tf.transpose(a=context_layer, perm=[0, 2, 1, 3])\n\n  if do_return_2d_tensor:\n    # `context_layer` = [B*F, N*H]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n  else:\n    # `context_layer` = [B, F, N*H]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n\n  return context_layer\n\n\ndef transformer_model(input_tensor,\n                      attention_mask=None,\n                      hidden_size=768,\n                      num_hidden_layers=12,\n                      num_attention_heads=12,\n                      intermediate_size=3072,\n                      intermediate_act_fn=gelu,\n                      hidden_dropout_prob=0.1,\n                      attention_probs_dropout_prob=0.1,\n                      initializer_range=0.02,\n                      do_return_all_layers=False):\n  """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".\n\n  This is almost an exact implementation of the original Transformer encoder.\n\n  See the original paper:\n  https://arxiv.org/abs/1706.03762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  """"""\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        ""The hidden size (%d) is not a multiple of the number of attention ""\n        ""heads (%d)"" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(""The width of the input tensor (%d) != hidden size (%d)"" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.compat.v1.variable_scope(""layer_%d"" % layer_idx):\n      layer_input = prev_output\n\n      with tf.compat.v1.variable_scope(""attention""):\n        attention_heads = []\n        with tf.compat.v1.variable_scope(""self""):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.compat.v1.variable_scope(""output""):\n          attention_output = tf.compat.v1.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the ""intermediate"" hidden layer.\n      with tf.compat.v1.variable_scope(""intermediate""):\n        intermediate_output = tf.compat.v1.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.compat.v1.variable_scope(""output""):\n        layer_output = tf.compat.v1.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n\ndef get_shape_list(tensor, expected_rank=None, name=None):\n  """"""Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  if expected_rank is not None:\n    assert_rank(tensor, expected_rank, name)\n\n  shape = tensor.shape.as_list()\n\n  non_static_indexes = []\n  for (index, dim) in enumerate(shape):\n    if dim is None:\n      non_static_indexes.append(index)\n\n  if not non_static_indexes:\n    return shape\n\n  dyn_shape = tf.shape(input=tensor)\n  for index in non_static_indexes:\n    shape[index] = dyn_shape[index]\n  return shape\n\n\ndef reshape_to_matrix(input_tensor):\n  """"""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""\n  ndims = input_tensor.shape.ndims\n  if ndims < 2:\n    raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %\n                     (input_tensor.shape))\n  if ndims == 2:\n    return input_tensor\n\n  width = input_tensor.shape[-1]\n  output_tensor = tf.reshape(input_tensor, [-1, width])\n  return output_tensor\n\n\ndef reshape_from_matrix(output_tensor, orig_shape_list):\n  """"""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""\n  if len(orig_shape_list) == 2:\n    return output_tensor\n\n  output_shape = get_shape_list(output_tensor)\n\n  orig_dims = orig_shape_list[0:-1]\n  width = output_shape[-1]\n\n  return tf.reshape(output_tensor, orig_dims + [width])\n\n\ndef assert_rank(tensor, expected_rank, name=None):\n  """"""Raises an exception if the tensor rank is not of the expected rank.\n\n  Args:\n    tensor: A tf.Tensor to check the rank of.\n    expected_rank: Python integer or list of integers, expected rank.\n    name: Optional name of the tensor for the error message.\n\n  Raises:\n    ValueError: If the expected shape doesn\'t match the actual shape.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  expected_rank_dict = {}\n  if isinstance(expected_rank, six.integer_types):\n    expected_rank_dict[expected_rank] = True\n  else:\n    for x in expected_rank:\n      expected_rank_dict[x] = True\n\n  actual_rank = tensor.shape.ndims\n  if actual_rank not in expected_rank_dict:\n    scope_name = tf.compat.v1.get_variable_scope().name\n    raise ValueError(\n        ""For the tensor `%s` in scope `%s`, the actual rank ""\n        ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %\n        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n'"
tests/nonci/__init__.py,0,"b'# coding=utf-8\n#\n# created by kpe on 23.May.2019 at 16:05\n#\n\nfrom __future__ import absolute_import, division, print_function\n'"
tests/nonci/test_attention.py,16,"b'# coding=utf-8\n#\n# created by kpe on 15.Mar.2019 at 15:30\n#\n\nfrom __future__ import absolute_import, division, print_function\n\n\nimport unittest\n\nimport random\nimport numpy as np\n\n\nimport tensorflow as tf\n\nfrom bert.attention import AttentionLayer\n\n\n# tf.enable_v2_behavior()\n# tf.enable_eager_execution()\n\n\nclass MaskFlatten(tf.keras.layers.Flatten):\n\n    def __init__(self, **kwargs):\n        self.supports_masking = True\n        super(MaskFlatten, self).__init__(**kwargs)\n\n    def compute_mask(self, _, mask=None):\n        return mask\n\n\nclass BertAttentionTest(unittest.TestCase):\n\n    @staticmethod\n    def data_generator(batch_size=32, max_len=10):             # ([batch_size, 10], [10])\n        while True:\n            data = np.zeros((batch_size, max_len))\n            tag = np.zeros(batch_size, dtype=\'int32\')\n            for i in range(batch_size):\n                datum_len = random.randint(1, max_len - 1)\n                total = 0\n                for j in range(datum_len):\n                    data[i, j] = random.randint(1, 4)\n                    total += data[i, j]\n                tag[i] = total % 2\n            yield data, tag\n\n    def test_attention(self):\n        max_seq_len = random.randint(5, 10)\n        count = 0\n        for data, tag in self.data_generator(4, max_seq_len):\n            count += 1\n            print(data, tag)\n            if count > 2:\n                break\n\n        class AModel(tf.keras.models.Model):\n            def __init__(self, **kwargs):\n                super(AModel, self).__init__(**kwargs)\n                self.embedding = tf.keras.layers.Embedding(input_dim=5, output_dim=3, mask_zero=True)\n                self.attention = AttentionLayer(num_heads=5, size_per_head=3)\n                self.timedist  = tf.keras.layers.TimeDistributed(MaskFlatten())\n                self.bigru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8))\n                self.softmax = tf.keras.layers.Dense(units=2, activation=""softmax"")\n\n            #def build(self, input_shape):\n            #    super(AModel,self).build(input_shape)\n\n            def call(self, inputs, training=None, mask=None):\n                out = inputs\n                out = self.embedding(out)\n                out = self.attention(out)\n                out = self.timedist(out)\n                out = self.bigru(out)\n                out = self.softmax(out)\n                return out\n\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Embedding(input_dim=5, output_dim=3, mask_zero=True),\n            AttentionLayer(num_heads=5, size_per_head=3),\n            tf.keras.layers.TimeDistributed(MaskFlatten()),\n            tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8)),\n            tf.keras.layers.Dense(units=2, activation=""softmax"")\n        ])\n\n        #model = AModel()\n        model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.003),\n                      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n        # model.build(input_shape=(None, max_seq_len))\n\n        model.build()\n        model.summary()\n\n        model.fit_generator(\n            generator=self.data_generator(64, max_seq_len),\n            steps_per_epoch=100,\n            epochs=10,\n            validation_data=self.data_generator(8, max_seq_len),\n            validation_steps=10,\n            #callbacks=[\n            #    keras.callbacks.EarlyStopping(monitor=\'val_sparse_categorical_accuracy\', patience=5),\n            #],\n        )\n'"
tests/nonci/test_bert.py,1,"b'# coding=utf-8\n#\n# created by kpe on 26.Mar.2019 at 14:11\n#\n\nfrom __future__ import absolute_import, division, print_function\n\nimport random\n\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow import keras\n\nfrom bert import BertModelLayer\n\n\ntf.compat.v1.enable_eager_execution()\n\n\nclass MaskFlatten(keras.layers.Flatten):\n\n    def __init__(self, **kwargs):\n        self.supports_masking = True\n        super(MaskFlatten, self).__init__(**kwargs)\n\n    def compute_mask(self, _, mask=None):\n        return mask\n\n\ndef parity_ds_generator(batch_size=32, max_len=10, max_int=4, modulus=2):\n    """"""\n    Generates a parity calculation dataset (seq -> sum(seq) mod 2),\n    where seq is a sequence of length less than max_len\n    of integers in [1..max_int).\n    """"""\n    while True:\n        data = np.zeros((batch_size, max_len))\n        tag = np.zeros(batch_size, dtype=\'int32\')\n        for i in range(batch_size):\n            datum_len = random.randint(1, max_len - 1)\n            total = 0\n            for j in range(datum_len):\n                data[i, j] = random.randint(1, max_int)\n                total += data[i, j]\n            tag[i] = total % modulus\n        yield data, tag                  # ([batch_size, max_len], [max_len])\n\n\nclass RawBertTest(unittest.TestCase):\n\n    def test_simple(self):\n        max_seq_len = 10\n        bert = BertModelLayer(\n            vocab_size=5,\n            max_position_embeddings=10,\n            hidden_size=15,\n            num_layers=2,\n            num_heads=5,\n            intermediate_size=4,\n            use_token_type=False\n        )\n        model = keras.Sequential([\n            bert,\n            keras.layers.Lambda(lambda x: x[:, -0, ...]),        # [B, 2]\n            keras.layers.Dense(units=2, activation=""softmax""),   # [B, 10, 2]\n        ])\n\n        model.build(input_shape=(None, max_seq_len))\n\n        model.compile(optimizer=keras.optimizers.Adam(lr=0.002),\n                      loss=keras.losses.sparse_categorical_crossentropy,\n                      metrics=[keras.metrics.sparse_categorical_accuracy]\n                      )\n\n        model.summary(line_length=120)\n\n        for ndx, var in enumerate(model.trainable_variables):\n            print(""{:5d}"".format(ndx), var.name, var.shape, var.dtype)\n\n        model.fit_generator(generator=parity_ds_generator(64, max_seq_len),\n                            steps_per_epoch=100,\n                            epochs=10,\n                            validation_data=parity_ds_generator(32, max_seq_len),  # TODO: can\'t change max_seq_len (but transformer alone can)\n                            validation_steps=10,\n                            callbacks=[\n                                keras.callbacks.EarlyStopping(monitor=\'val_sparse_categorical_accuracy\', patience=5),\n                            ],\n                            )\n\n\n\n'"
tests/nonci/test_compare_pretrained.py,13,"b'# coding=utf-8\n#\n# created by kpe on 27.Mar.2019 at 15:37\n#\n\nfrom __future__ import absolute_import, division, print_function\n\nimport unittest\nimport os\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python import keras\n\n\nimport bert\nfrom bert.tokenization.bert_tokenization import FullTokenizer\n\ntf.compat.v1.disable_eager_execution()\n\n\nclass TestCompareBertsOnPretrainedWeight(unittest.TestCase):\n    def setUp(self) -> None:\n        self.bert_name = ""uncased_L-12_H-768_A-12""\n        self.bert_ckpt_dir = bert.fetch_google_bert_model(self.bert_name, fetch_dir="".models"")\n        self.bert_ckpt_file = os.path.join(self.bert_ckpt_dir, ""bert_model.ckpt"")\n        self.bert_config_file = os.path.join(self.bert_ckpt_dir, ""bert_config.json"")\n\n    def test_bert_original_weights(self):\n        print(""bert checkpoint: "", self.bert_ckpt_file)\n        bert_vars = tf.train.list_variables(self.bert_ckpt_file)\n        for ndx, var in enumerate(bert_vars):\n            print(""{:3d}"".format(ndx), var)\n\n    def create_bert_model(self, max_seq_len=18):\n        bert_params = bert.loader.params_from_pretrained_ckpt(self.bert_ckpt_dir)\n        l_bert = bert.BertModelLayer.from_params(bert_params, name=""bert"")\n\n        input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype=\'int32\', name=""input_ids"")\n        token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype=\'int32\', name=""token_type_ids"")\n        output = l_bert([input_ids, token_type_ids])\n\n        model = keras.Model(inputs=[input_ids, token_type_ids], outputs=output)\n\n        return model, l_bert, (input_ids, token_type_ids)\n\n    def test_keras_weights(self):\n        max_seq_len = 18\n        model, l_bert, inputs = self.create_bert_model(18)\n\n        model.build(input_shape=[(None, max_seq_len),\n                                 (None, max_seq_len)])\n\n        model.summary()\n\n        for ndx, var in enumerate(l_bert.trainable_variables):\n            print(""{:3d}"".format(ndx), var.name, var.shape)\n\n        #for ndx, var in enumerate(model.trainable_variables):\n        #    print(""{:3d}"".format(ndx), var.name, var.shape)\n\n    def test___compare_weights(self):\n\n        tf.compat.v1.reset_default_graph()\n\n        max_seq_len = 18\n        model, l_bert, inputs = self.create_bert_model(18)\n        model.build(input_shape=[(None, max_seq_len),\n                                 (None, max_seq_len)])\n\n        stock_vars = tf.train.list_variables(self.bert_ckpt_file)\n        stock_vars = {name: list(shape) for name, shape in stock_vars}\n\n        keras_vars = model.trainable_variables\n        keras_vars = {var.name.split("":"")[0]: var.shape.as_list() for var in keras_vars}\n\n        matched_vars   = set()\n        unmatched_vars = set()\n        shape_errors   = set()\n\n        for name in stock_vars:\n            bert_name  = name\n            keras_name = bert.loader.map_from_stock_variale_name(bert_name)\n            if keras_name in keras_vars:\n                if keras_vars[keras_name] == stock_vars[bert_name]:\n                    matched_vars.add(bert_name)\n                else:\n                    shape_errors.add(bert_name)\n            else:\n                unmatched_vars.add(bert_name)\n\n        print(""bert -> keras:"")\n        print(""     matched count:"", len(matched_vars))\n        print(""   unmatched count:"", len(unmatched_vars))\n        print("" shape error count:"", len(shape_errors))\n\n        print(""unmatched:\\n"", ""\\n "".join(unmatched_vars))\n\n        self.assertEqual(197, len(matched_vars))\n        self.assertEqual(9, len(unmatched_vars))\n        self.assertEqual(0, len(shape_errors))\n\n        matched_vars   = set()\n        unmatched_vars = set()\n        shape_errors   = set()\n\n        for name in keras_vars:\n            keras_name = name\n            bert_name  = bert.loader.map_to_stock_variable_name(keras_name)\n            if bert_name in stock_vars:\n                if stock_vars[bert_name] == keras_vars[keras_name]:\n                    matched_vars.add(keras_name)\n                else:\n                    shape_errors.add(keras_name)\n            else:\n                unmatched_vars.add(keras_name)\n\n        print(""keras -> bert:"")\n        print(""     matched count:"", len(matched_vars))\n        print(""   unmatched count:"", len(unmatched_vars))\n        print("" shape error count:"", len(shape_errors))\n\n        print(""unmatched:\\n"", ""\\n "".join(unmatched_vars))\n        self.assertEqual(197, len(matched_vars))\n        self.assertEqual(0, len(unmatched_vars))\n        self.assertEqual(0, len(shape_errors))\n\n\n\n    def predict_on_keras_model(self, input_ids, input_mask, token_type_ids):\n        max_seq_len = input_ids.shape[-1]\n        model, l_bert, k_inputs = self.create_bert_model(max_seq_len)\n        model.build(input_shape=[(None, max_seq_len),\n                                 (None, max_seq_len)])\n        bert.load_stock_weights(l_bert, self.bert_ckpt_file)\n        k_res = model.predict([input_ids, token_type_ids])\n        return k_res\n\n    def predict_on_stock_model(self, input_ids, input_mask, token_type_ids):\n        from tests.ext.modeling import BertModel, BertConfig, get_assignment_map_from_checkpoint\n\n        tf.compat.v1.reset_default_graph()\n\n        tf_placeholder = tf.compat.v1.placeholder\n\n        max_seq_len       = input_ids.shape[-1]\n        pl_input_ids      = tf.compat.v1.placeholder(tf.int32, shape=(1, max_seq_len))\n        pl_mask           = tf.compat.v1.placeholder(tf.int32, shape=(1, max_seq_len))\n        pl_token_type_ids = tf.compat.v1.placeholder(tf.int32, shape=(1, max_seq_len))\n\n        bert_config = BertConfig.from_json_file(self.bert_config_file)\n        tokenizer = FullTokenizer(vocab_file=os.path.join(self.bert_ckpt_dir, ""vocab.txt""))\n\n        s_model = BertModel(config=bert_config,\n                               is_training=False,\n                               input_ids=pl_input_ids,\n                               input_mask=pl_mask,\n                               token_type_ids=pl_token_type_ids,\n                               use_one_hot_embeddings=False)\n\n        tvars = tf.compat.v1.trainable_variables()\n        (assignment_map, initialized_var_names) = get_assignment_map_from_checkpoint(tvars, self.bert_ckpt_file)\n        tf.compat.v1.train.init_from_checkpoint(self.bert_ckpt_file, assignment_map)\n\n        with tf.compat.v1.Session() as sess:\n            sess.run(tf.compat.v1.global_variables_initializer())\n\n            s_res = sess.run(\n                s_model.get_sequence_output(),\n                feed_dict={pl_input_ids:      input_ids,\n                           pl_token_type_ids: token_type_ids,\n                           pl_mask:           input_mask,\n                           })\n        return s_res\n\n    def test_direct_keras_to_stock_compare(self):\n        from tests.ext.modeling import BertModel, BertConfig, get_assignment_map_from_checkpoint\n\n        bert_config = BertConfig.from_json_file(self.bert_config_file)\n        tokenizer = FullTokenizer(vocab_file=os.path.join(self.bert_ckpt_dir, ""vocab.txt""))\n\n        # prepare input\n        max_seq_len  = 6\n        input_str    = ""Hello, Bert!""\n        input_tokens = tokenizer.tokenize(input_str)\n        input_tokens = [""[CLS]""] + input_tokens + [""[SEP]""]\n        input_ids    = tokenizer.convert_tokens_to_ids(input_tokens)\n        input_ids      = input_ids             + [0]*(max_seq_len - len(input_tokens))\n        input_mask     = [1]*len(input_tokens) + [0]*(max_seq_len - len(input_tokens))\n        token_type_ids = [0]*len(input_tokens) + [0]*(max_seq_len - len(input_tokens))\n\n        input_ids      = np.array([input_ids], dtype=np.int32)\n        input_mask     = np.array([input_mask], dtype=np.int32)\n        token_type_ids = np.array([token_type_ids], dtype=np.int32)\n\n        print(""   tokens:"", input_tokens)\n        print(""input_ids:{}/{}:{}"".format(len(input_tokens), max_seq_len, input_ids), input_ids.shape, token_type_ids)\n\n        s_res = self.predict_on_stock_model(input_ids, input_mask, token_type_ids)\n        k_res = self.predict_on_keras_model(input_ids, input_mask, token_type_ids)\n\n        np.set_printoptions(precision=9, threshold=20, linewidth=200, sign=""+"", floatmode=""fixed"")\n        print(""s_res"", s_res.shape)\n        print(""k_res"", k_res.shape)\n\n        print(""s_res:\\n {}"".format(s_res[0, :2, :10]), s_res.dtype)\n        print(""k_res:\\n {}"".format(k_res[0, :2, :10]), k_res.dtype)\n\n        adiff = np.abs(s_res-k_res).flatten()\n        print(""diff:"", np.max(adiff), np.argmax(adiff))\n        self.assertTrue(np.allclose(s_res, k_res, atol=1e-6))\n\n\n'"
tests/nonci/test_load_pretrained_weights.py,0,"b'# coding=utf-8\n#\n# created by kpe on 10.Oct.2019 at 16:26\n#\n\nfrom __future__ import absolute_import, division, print_function\n\nimport unittest\n\nimport os\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport bert\n\n\nclass TestLoadPreTrainedWeights(unittest.TestCase):\n\n    def build_model(self, bert_params):\n        l_bert = bert.BertModelLayer.from_params(bert_params, name=""bert"")\n\n        l_input_ids = keras.layers.Input(shape=(128,), dtype=\'int32\', name=""input_ids"")\n        l_token_type_ids = keras.layers.Input(shape=(128,), dtype=\'int32\', name=""token_type_ids"")\n        output = l_bert([l_input_ids, l_token_type_ids])\n        output = keras.layers.Lambda(lambda x: x[:, 0, :])(output)\n        output = keras.layers.Dense(2)(output)\n        model = keras.Model(inputs=[l_input_ids, l_token_type_ids], outputs=output)\n\n        model.build(input_shape=(None, 128))\n        model.compile(optimizer=keras.optimizers.Adam(),\n                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                      metrics=[keras.metrics.SparseCategoricalAccuracy(name=""acc"")])\n\n        for weight in l_bert.weights:\n            print(weight.name)\n\n        return model, l_bert\n\n    def test_bert_google_weights(self):\n        bert_model_name = ""uncased_L-12_H-768_A-12""\n        bert_dir = bert.fetch_google_bert_model(bert_model_name, "".models"")\n        bert_ckpt = os.path.join(bert_dir, ""bert_model.ckpt"")\n\n        bert_params = bert.params_from_pretrained_ckpt(bert_dir)\n        model, l_bert = self.build_model(bert_params)\n\n        skipped_weight_value_tuples = bert.load_bert_weights(l_bert, bert_ckpt)\n        self.assertEqual(0, len(skipped_weight_value_tuples))\n        model.summary()\n\n    def test_albert_chinese_weights(self):\n        albert_model_name = ""albert_base""\n        albert_dir = bert.fetch_brightmart_albert_model(albert_model_name, "".models"")\n        albert_ckpt = os.path.join(albert_dir, ""albert_model.ckpt"")\n\n        albert_params = bert.params_from_pretrained_ckpt(albert_dir)\n        model, l_bert = self.build_model(albert_params)\n\n        skipped_weight_value_tuples = bert.load_albert_weights(l_bert, albert_ckpt)\n        self.assertEqual(0, len(skipped_weight_value_tuples))\n        model.summary()\n\n    def test_albert_google_weights(self):\n        albert_model_name = ""albert_base""\n        albert_dir = bert.fetch_tfhub_albert_model(albert_model_name, "".models"")\n\n        albert_params = bert.albert_params(albert_model_name)\n        model, l_bert = self.build_model(albert_params)\n\n        skipped_weight_value_tuples = bert.load_albert_weights(l_bert, albert_dir)\n        self.assertEqual(0, len(skipped_weight_value_tuples))\n        model.summary()\n\n    def test_albert_google_weights_non_tfhub(self):\n        albert_model_name = ""albert_base_v2""\n        albert_dir = bert.fetch_google_albert_model(albert_model_name, "".models"")\n        model_ckpt = os.path.join(albert_dir, ""model.ckpt-best"")\n\n        albert_params = bert.albert_params(albert_dir)\n        model, l_bert = self.build_model(albert_params)\n\n        skipped_weight_value_tuples = bert.load_albert_weights(l_bert, model_ckpt)\n        self.assertEqual(0, len(skipped_weight_value_tuples))\n        model.summary()\n'"
tests/nonci/test_multi_lang.py,3,"b'# coding=utf-8\n#\n# created by kpe on 05.06.2019 at 9:01 PM\n#\n\nfrom __future__ import division, absolute_import, print_function\n\nimport os\n\nimport unittest\n\nimport tensorflow as tf\nimport bert\n\n\nclass TestMultiLang(unittest.TestCase):\n    def setUp(self) -> None:\n        self.bert_name = ""multilingual_L-12_H-768_A-12""\n        self.bert_ckpt_dir = bert.fetch_google_bert_model(self.bert_name, fetch_dir="".models"")\n        self.bert_ckpt_file = os.path.join(self.bert_ckpt_dir, ""bert_model.ckpt"")\n        self.bert_config_file = os.path.join(self.bert_ckpt_dir, ""bert_config.json"")\n\n    def test_multi(self):\n        print(self.bert_ckpt_dir)\n        bert_params = bert.loader.params_from_pretrained_ckpt(self.bert_ckpt_dir)\n        bert_params.adapter_size = 32\n        l_bert = bert.BertModelLayer.from_params(bert_params, name=""bert"")\n\n        max_seq_len=128\n        l_input_ids      = tf.keras.layers.Input(shape=(max_seq_len,), dtype=\'int32\', name=""input_ids"")\n        l_token_type_ids = tf.keras.layers.Input(shape=(max_seq_len,), dtype=\'int32\', name=""token_type_ids"")\n        output = l_bert([l_input_ids, l_token_type_ids])\n\n        model = tf.keras.Model(inputs=[l_input_ids, l_token_type_ids], outputs=output)\n        model.build(input_shape=[(None, max_seq_len),\n                                 (None, max_seq_len)])\n\n        bert.load_stock_weights(l_bert, self.bert_ckpt_file)\n\n        model.summary()\n\n'"
tests/nonci/test_stock_weights.py,4,"b'# coding=utf-8\n#\n# created by kpe on 25.Jul.2019 at 12:23\n#\n\nfrom __future__ import absolute_import, division, print_function\n\n\nimport unittest\nimport math\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.python import keras\n\nimport bert\n\n#tf.enable_eager_execution()\n#tf.disable_eager_execution()\n\n\ndef flatten_layers(root_layer):\n    if isinstance(root_layer, keras.layers.Layer):\n        yield root_layer\n    for layer in root_layer._layers:\n        for sub_layer in flatten_layers(layer):\n            yield sub_layer\n\n\ndef freeze_bert_layers(l_bert):\n    """"""\n    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n    """"""\n    for layer in flatten_layers(l_bert):\n        if layer.name in [""LayerNorm"", ""adapter-down"", ""adapter-up""]:\n            layer.trainable = True\n        elif len(layer._layers) == 0:\n            layer.trainable = False\n        l_bert.embeddings_layer.trainable = False\n\n\ndef create_learning_rate_scheduler(max_learn_rate=5e-5,\n                                   end_learn_rate=1e-7,\n                                   warmup_epoch_count=10,\n                                   total_epoch_count=90):\n\n    def lr_scheduler(epoch):\n        if epoch < warmup_epoch_count:\n            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n        else:\n            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n        return float(res)\n    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n\n    return learning_rate_scheduler\n\n\nclass TestWeightsLoading(unittest.TestCase):\n    #bert_ckpt_dir = "".models/uncased_L-12_H-768_A-12/""\n    #bert_ckpt_file = bert_ckpt_dir + ""bert_model.ckpt""\n    #bert_config_file = bert_ckpt_dir + ""bert_config.json""\n\n    def setUp(self) -> None:\n        self.bert_name = ""uncased_L-12_H-768_A-12""\n        self.bert_ckpt_dir = bert.fetch_google_bert_model(self.bert_name, fetch_dir="".models"")\n        self.bert_ckpt_file = os.path.join(self.bert_ckpt_dir, ""bert_model.ckpt"")\n        self.bert_config_file = os.path.join(self.bert_ckpt_dir, ""bert_config.json"")\n\n    def test_load_pretrained(self):\n        print(""Eager Execution:"", tf.executing_eagerly())\n\n        bert_params = bert.loader.params_from_pretrained_ckpt(self.bert_ckpt_dir)\n        bert_params.adapter_size = 32\n        l_bert = bert.BertModelLayer.from_params(bert_params, name=""bert"")\n\n        model = keras.models.Sequential([\n            keras.layers.InputLayer(input_shape=(128,)),\n            l_bert,\n            keras.layers.Lambda(lambda x: x[:, 0, :]),\n            keras.layers.Dense(2)\n        ])\n\n        # we need to freeze before build/compile - otherwise keras counts the params twice\n        if bert_params.adapter_size is not None:\n            freeze_bert_layers(l_bert)\n\n        model.build(input_shape=(None, 128))\n        model.compile(optimizer=keras.optimizers.Adam(),\n            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n            metrics=[keras.metrics.SparseCategoricalAccuracy(name=""acc"")])\n\n        bert.load_stock_weights(l_bert, self.bert_ckpt_file)\n\n        model.summary()\n\n\n'"
tests/nonci/test_transformer.py,0,"b'# coding=utf-8\n#\n# created by kpe on 21.Mar.2019 at 13:30\n#\n\nfrom __future__ import absolute_import, division, print_function\n\nimport random\n\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python import keras\n\n\nfrom bert.transformer import TransformerEncoderLayer\n\n\nclass MaskFlatten(keras.layers.Flatten):\n\n    def __init__(self, **kwargs):\n        self.supports_masking = True\n        super(MaskFlatten, self).__init__(**kwargs)\n\n    def compute_mask(self, _, mask=None):\n        return mask\n\n\ndef parity_ds_generator(batch_size=32, max_len=10, max_int=4, modulus=2):\n    """"""\n    Generates a parity calculation dataset (seq -> sum(seq) mod 2),\n    where seq is a sequence of length less than max_len\n    of integers in [1..max_int).\n    """"""\n    while True:\n        data = np.zeros((batch_size, max_len))\n        tag = np.zeros(batch_size, dtype=\'int32\')\n        for i in range(batch_size):\n            datum_len = random.randint(1, max_len - 1)\n            total = 0\n            for j in range(datum_len):\n                data[i, j] = random.randint(1, max_int)\n                total += data[i, j]\n            tag[i] = total % modulus\n        yield data, tag                  # ([batch_size, max_len], [max_len])\n\n\nclass TransformerTest(unittest.TestCase):\n\n    def test_simple(self):\n        max_seq_len = 10\n        model = keras.Sequential([\n            keras.layers.Embedding(input_dim=5, output_dim=15, mask_zero=True),      # [B, 10, 12]\n            TransformerEncoderLayer(\n                hidden_size=15,\n                num_heads=5,\n                num_layers=2,\n                intermediate_size=8,\n                hidden_dropout=0.1),                                        # [B, 10, 6]\n            keras.layers.TimeDistributed(\n                keras.layers.Dense(units=2, activation=""softmax"")),         # [B, 10, 2]\n            keras.layers.Lambda(lambda x: x[:, -0, ...])                    # [B, 2]\n            ])\n\n        model.build(input_shape=(None, max_seq_len))\n\n        model.compile(optimizer=keras.optimizers.Adam(lr=0.003),\n                      loss=keras.losses.sparse_categorical_crossentropy,\n                      metrics=[keras.metrics.sparse_categorical_accuracy]\n                      )\n        model.summary(line_length=120)\n\n        model.fit_generator(generator=parity_ds_generator(64, max_seq_len),\n                            steps_per_epoch=100,\n                            epochs=20,\n                            validation_data=parity_ds_generator(12, -4+max_seq_len),\n                            validation_steps=10,\n                            callbacks=[\n                                keras.callbacks.EarlyStopping(monitor=\'val_sparse_categorical_accuracy\', patience=5),\n                            ],\n                            )\n\n\n\n\n'"
