file_path,api_count,code
components/build_image.py,0,"b'# -*- coding: utf-8 -*-\n""""""Script to build images.\n\nFor example,\npython build_image.py --tf_version=1.6 --platform=gpu tf_serving\n""""""\nimport argparse\nimport datetime\nfrom itertools import chain\nimport logging\nimport os\nimport subprocess\nimport sys\nimport time\nimport yaml\n\n\ndef run(command,\n        cwd=None,\n        env=None,\n        polling_interval=datetime.timedelta(seconds=1)):\n  """"""Run a subprocess.\n  Copied from kubeflow/test so it\'s easier to run locally.\n  TODO(lunkai): refactor to dedup.\n  Any subprocess output is emitted through the logging modules.\n  Returns:\n    output: A string containing the output.\n  """"""\n  logging.info(""Running: %s \\ncwd=%s"", "" "".join(command), cwd)\n\n  if not env:\n    env = os.environ\n  else:\n    keys = sorted(env.keys())\n\n    lines = []\n    for k in keys:\n      lines.append(""{0}={1}"".format(k, env[k]))\n    logging.info(""Running: Environment:\\n%s"", ""\\n"".join(lines))\n\n  process = subprocess.Popen(\n      command,\n      cwd=cwd,\n      env=env,\n      stdout=subprocess.PIPE,\n      stderr=subprocess.STDOUT)\n\n  logging.info(""Subprocess output:\\n"")\n  output = []\n  while process.poll() is None:\n    process.stdout.flush()\n    for line in iter(process.stdout.readline, b\'\'):\n      output.append(line.strip())\n      logging.info(line.strip())\n\n    time.sleep(polling_interval.total_seconds())\n\n  process.stdout.flush()\n  for line in iter(process.stdout.readline, b\'\'):\n    output.append(line.strip())\n    logging.info(line.strip())\n\n  if process.returncode != 0:\n    raise subprocess.CalledProcessError(\n        process.returncode, ""cmd: {0} exited with code {1}"".format(\n            "" "".join(command), process.returncode), ""\\n"".join(output))\n\n  return ""\\n"".join(output)\n\n\ndef wait_for_docker_daemon(timeout=60):\n  """"""Waiting for docker daemon to be ready. This is needed in DinD scenario.""""""\n  start_time = time.time()\n  while time.time() - start_time < timeout:\n    try:\n      subprocess.check_call([""docker"", ""ps""])\n    except subprocess.CalledProcessError:\n      time.sleep(5)\n    # Daemon ready.\n    logging.info(""docker daemon ready.\\n"")\n    return\n  # Timeout.\n  logging.error(""Timeout waiting for docker daemon\\n"")\n  # TODO(lunkai): use TimeoutError when we use py3.\n  raise RuntimeError\n\n\ndef get_build_args(config):\n  """"""\n  Make the list of params for docker build from config.\n\n  For example, if the config is {""a"": 1, ""b"": 2}\n  This should return\n  [""--build-arg"", ""a=1"", ""--build-arg"", ""b=2""]\n  """"""\n  config_list = [key + ""="" + val for key, val in config.items()]\n  return list(chain.from_iterable([[""--build-arg"", x] for x in config_list]))\n\n\ndef get_config(context_dir, version):\n  """"""Returns a dict of configuration from the version-config file.""""""\n  config_file = os.path.join(context_dir, ""versions"", version,\n                             ""version-config.json"")\n  with open(config_file) as f:\n    config = yaml.load(f)\n  return config\n\n\ndef build_tf_serving(args):\n  wait_for_docker_daemon()\n  dir_path = os.path.dirname(os.path.realpath(__file__))\n  context_dir = os.path.join(dir_path, ""k8s-model-server/images"")\n  version = args.tf_version if args.platform == ""cpu"" else args.tf_version + ""gpu""\n\n  config = get_config(context_dir, version)\n  build_args = get_build_args(config)\n  image_name = ""{}/tensorflow-serving-{}:{}"".format(args.registry, version,\n                                                    args.tag)\n\n  command = list(\n      chain(\n          [""docker"", ""build"", ""--pull""], build_args,\n          [""-t"", image_name, ""-f"", ""Dockerfile.{}"".format(args.platform), "".""]))\n  run(command, cwd=context_dir)\n\n  if args.push_gcr:\n    run([\n        ""gcloud"", ""auth"", ""activate-service-account"", ""--key-file"",\n        os.environ[\'GOOGLE_APPLICATION_CREDENTIALS\']\n    ])\n    run([""gcloud"", ""docker"", ""--"", ""push"", image_name])\n\n\ndef build_tf_notebook(args):\n  wait_for_docker_daemon()\n  dir_path = os.path.dirname(os.path.realpath(__file__))\n  context_dir = os.path.join(dir_path, ""tensorflow-notebook-image"")\n  version = args.tf_version if args.platform == ""cpu"" else args.tf_version + ""gpu""\n\n  config = get_config(context_dir, version)\n  build_args = get_build_args(config)\n  image_name = ""{}/tensorflow-{}-notebook-{}:{}"".format(\n      args.registry, args.tf_version, args.platform, args.tag)\n\n  command = list(\n      chain([""docker"", ""build"", ""--pull""], build_args,\n            [""-t"", image_name, ""-f"", ""Dockerfile"", "".""]))\n  run(command, cwd=context_dir)\n\n  if args.push_gcr:\n    run([\n        ""gcloud"", ""auth"", ""activate-service-account"", ""--key-file"",\n        os.environ[\'GOOGLE_APPLICATION_CREDENTIALS\']\n    ])\n    run([""gcloud"", ""docker"", ""--"", ""push"", image_name])\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  subparsers = parser.add_subparsers()\n\n  parser.add_argument(\n      ""--registry"",\n      default=""gcr.io/kubeflow-images-public"",\n      help=""The registry of the image"")\n  parser.add_argument(""--tag"", default=""latest"", help=""The image tag"")\n  parser.add_argument(""--tf_version"", default=""1.6"", help=""Tensorflow version"")\n  parser.add_argument(""--platform"", default=""cpu"", help=""cpu or gpu"")\n  parser.add_argument(\n      ""--push_gcr"",\n      action=\'store_true\',\n      default=False,\n      help=""Whether to push the image after building."")\n\n  parser_tf_serving = subparsers.add_parser(""tf_serving"")\n  parser_tf_serving.set_defaults(func=build_tf_serving)\n\n  parser_tf_notebook = subparsers.add_parser(""tf_notebook"")\n  parser_tf_notebook.set_defaults(func=build_tf_notebook)\n\n  args = parser.parse_args()\n  args.func(args)\n\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
hack/convert_manifest_to_jsonnet.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A simple utility to convert a manifest to corresponding jsonnet.""""""\nimport argparse\nimport json\nimport yaml\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser(description=""Convert manifest."")\n  parser.add_argument(\n      ""--manifest"",\n      type=str,\n      required=True,\n  )\n\n  args = parser.parse_args()\n\n  with open(args.manifest) as hf:\n    manifest = hf.read()\n\n  components = manifest.split(""---"")\n\n  index = 0\n  for c in components:\n    t = c.strip()\n    if not t:\n      continue\n\n    d = yaml.load(t)\n    j = json.dumps(d, indent=2, sort_keys=True)\n    print(""component_{0}:: {1}, \\n"".format(index, j))\n    index += 1\n'"
releasing/sync_images.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""This script synchronizes Docker image in image_tags.yaml to your own registry\n\nExample:\npython sync_images.py --registry registry.aliyuncs.com\n\nThis would sync up the docker images for kubeflow releasing to your own docker\nregistry.\n""""""\n\nimport sys\nimport os\nimport getopt\nimport time\nimport dateutil.parser\nimport re\nimport yaml\nimport logging\nimport argparse\nimport subprocess\n\n\ndef normalize_repo(repo):\n  repo_names = repo.split(\'/\', 3)\n  if len(repo_names) == 1:\n    repo_names = [\'docker.io\', \'library\', repo_names[0]]\n  if len(repo_names) == 2:\n    repo_names = [\'docker.io\', repo_names[0], repo_names[1]]\n  if len(repo_names) == 4:\n    # gcr.io/kubeflow-images-public/katib/tfevent-metrics-collector:v0.4.0\n    # -> gcr.io/katib/tfevent-metrics-collector:v0.4.0\n    repo_names = [repo_names[0], repo_names[2], repo_names[3]]\n  return repo_names\n\n\ndef main(unparsed_args=None):  # pylint: disable=too-many-locals\n  logging.getLogger().setLevel(logging.INFO)  # pylint: disable=too-many-locals\n  # create the top-level parser\n  parser = argparse.ArgumentParser(\n      description=""sync up the kubeflow docker images to your own registry"")\n\n  parser.add_argument(\n      ""--images_file"",\n      default=""image_tags.yaml"",\n      type=str,\n      help=""Yaml file containing the tags to sync up."")\n\n  parser.add_argument(\n      ""--registry"",\n      default=""registry.aliyuncs.com"",\n      type=str,\n      help=(""docker registry e.g. registry.aliyuncs.com""))\n\n  args = parser.parse_args()\n\n  with open(args.images_file) as hf:\n    config = yaml.load(hf)\n\n  if not config:\n    raise ValueError(""No images could be load from %s"" % args.images_file)\n\n  # Loop over all the images and sync to your registry\n\n  for image in config[""images""]:\n    name = image[""name""]\n    for v in image[""versions""]:\n      for tag in v[""tags""]:\n        repo_names = normalize_repo(name)\n        source = name + "":"" + tag\n        registry = args.registry\n        namespace = repo_names[1]\n        newName = repo_names[2]\n        new_repo_name = registry + \'/\' + namespace + \'/\' + newName\n        dest = new_repo_name + "":"" + tag\n        logging.info(""Sync up the image %s to %s"", source, dest)\n        logging.info(""Pulling %s"", source)\n        rc = subprocess.call([""docker"", ""pull"", source])\n        if rc != 0:\n          logging.info(""Failed to Pull %s"", source)\n          continue\n        logging.info(""Tagging the image %s to %s"", source, dest)\n        rc = subprocess.call([""docker"", ""tag"", source, dest])\n        if rc != 0:\n          logging.info(""Failed to tag the image %s to %s"", source, dest)\n          continue\n        logging.info(""Push %s"", dest)\n        rc = subprocess.call([""docker"", ""push"", dest])\n        if rc != 0:\n          logging.info(""Failed to push the image %s"", dest)\n          continue\n  logging.info(""Done."")\n\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
scripts/update_prototype.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis script updates parameters values in jsonnet configs.\nExample\n  python update_prototype --file=all.jsonnet \\\n    --values=tfJobImage=gcr.io/kubeflow/tf-operator:v20180301\nwill change lines with a format like\n// @optionalParam tfJobImage string gcr.io/kubeflow-images-public/tf_operator:v20180226-403 The image for the TfJob controller.  # noqa: E501\nand\ntfJobImage:: ""gcr.io/kubeflow-images-public/tf_operator:v20180226-403"",\nto\n// @optionalParam tfJobImage string gcr.io/kubeflow/tf-operator:v20180301\nand\ntfJobImage:: ""gcr.io/kubeflow/tf-operator:v20180301"",\nrespectively.\n""""""\nimport argparse\nimport os\nimport re\n\n\n# TODO(jlewi): This code is now in\n# py/kubeflow/kubeflow/ci/update_jupdate_jupyter_web_app.py\ndef main():\n  parser = argparse.ArgumentParser(\n      description=""Update ksonnet prototypes parameters\' values"")\n  parser.add_argument(\n      ""--file"", action=""store"", dest=""file"", help=""Prototype file name"")\n  parser.add_argument(\n      ""--values"",\n      action=""store"",\n      dest=""values"",\n      help=""Comma separated param=value pairs. Ex.: a=b,c=1"")\n  args = parser.parse_args()\n\n  if not os.path.exists(args.file):\n    raise IOError(""File "" + args.file + "" not found!"")\n\n  regexps = {}\n  for pair in args.values.split("",""):\n    if ""="" not in pair:\n      raise Exception((""Malformed --values. Values pairs must contain =, e.g. ""\n                       ""param=value""))\n    param, value = pair.split(""="")\n    r = re.compile(r""([ \\t]*"" + param + "":+ ?\\""?)[^\\"",]+(\\""?,?)"")\n    v = r""\\g<1>"" + value + r""\\2""\n    regexps[param] = (r, v, value)\n\n  with open(args.file) as f:\n    prototype = f.read().split(""\\n"")\n  replacements = 0\n  for i, line in enumerate(prototype):\n    for param in regexps.keys():\n      if param not in line:\n        continue\n      if line.startswith(""//""):\n        prototype[i] = re.sub(\n            r""(// @\\w+ )"" + param + r""( \\w+ )[^ ]+(.*)"",  # noqa: W605\n            r""\\g<1>"" + param + r""\\2"" + regexps[param][2] + r""\\3"",\n            line)\n        replacements += 1\n        continue\n      prototype[i] = re.sub(regexps[param][0], regexps[param][1], line)\n      if line != prototype[i]:\n        replacements += 1\n  if replacements == 0:\n    raise Exception(\n        ""No replacements made, are you sure you specified correct param?"")\n  if replacements < len(regexps):\n    raise Warning(""Made less replacements then number of params. Typo?"")\n  temp_file = args.file + "".tmp""\n  with open(temp_file, ""w"") as w:\n    w.write(""\\n"".join(prototype))\n  os.rename(temp_file, args.file)\n  print(""Successfully made %d replacements"" % replacements)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
scripts/upgrade_ks_app.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport argparse\nimport logging\nimport os.path\nimport shutil\nimport subprocess\nimport yaml\n\n# The path used by the bootstrapper\nBOOTSTRAPPER_REGISTRY = ""/opt/registries/kubeflow/kubeflow""\n\n# The current release of Kubeflow. This should be upgraded on every release.\nCURRENT_RELEASE = ""github.com/kubeflow/kubeflow/tree/v0.2.0-rc.1/kubeflow""\n\n# The default name for the registry.\nDEFAULT_REGISTRY_NAME = ""kubeflow""\n\n\ndef main():\n  logging.basicConfig(\n      level=logging.INFO,\n      format=""%(levelname)s|%(asctime)s %(message)s"",\n      datefmt=""%Y-%m-%dT%H:%M:%S"",\n  )\n  logging.getLogger().setLevel(logging.INFO)\n\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--app_dir"",\n      default=os.getcwd(),\n      type=str,\n      help=""The directory of the ksonnet app."")\n  parser.add_argument(\n      ""--registry"",\n      default=CURRENT_RELEASE,\n      type=str,\n      help=(\n          ""The Kubeflow registry to use. This can be a GitHub link like ""\n          ""{0} that points at a specific version of the registry. ""\n          ""To specify the name of the registry in your ksonnet app ""\n          ""you can use the from <name>=<registry URL>"").format(CURRENT_RELEASE))\n\n  args = parser.parse_args()\n\n  if ""="" in args.registry:\n    registry_name, registry_url = args.registry.split(""="", 1)\n  else:\n    registry_name = DEFAULT_REGISTRY_NAME\n    registry_url = args.registry\n\n  app_dir = args.app_dir\n  logging.info(""Processing app: %s"", app_dir)\n  with open(os.path.join(app_dir, ""app.yaml""), ""r"") as f:\n    app = yaml.load(f)\n\n  registries = app[\'registries\']\n  libraries = app[\'libraries\']\n\n  for name in registries.iterkeys():\n    if name != registry_name:\n      logging.info(""Skipping registry %s"", name)\n      continue\n\n    if registries[name][""uri""].startswith(""file""):\n      # File registries are not stored in .ksonnet\n      # TODO(jlewi): This messes with bootstrapper because we might want  to\n      # switch from using the file URI to using the git location.\n      logging.info(""Skipping registry %s because it is a file URI"" % name)\n      continue\n    target = os.path.join(app_dir, "".ksonnet/registries"", name)\n    if not os.path.exists(target):\n      logging.info(""Warning directory %s doesn\'t exist; skipping"" % target)\n      continue\n    shutil.rmtree(target)\n\n  libs_to_remove = []\n  for name in libraries.iterkeys():\n    lib_registry = libraries[name][""registry""]\n    if lib_registry != registry_name:\n      continue\n    libs_to_remove.append(name)\n    target = os.path.join(app_dir, ""vendor"", lib_registry, name)\n    if not os.path.exists(target):\n      logging.info(""Directory does not exist: %s"", target)\n      continue\n    shutil.rmtree(target)\n\n  # Remove the registry from app.yaml\n  if registry_name in app[""registries""]:\n    del app[""registries""][registry_name]\n\n  # Remove libraries from this registry\n  for lib in libs_to_remove:\n    del app[""libraries""][lib]\n\n  with open(\'app.yaml\', \'w\') as f:\n    yaml.dump(app, f, default_flow_style=False)\n\n  logging.info(""Adding registry %s point at %s"", registry_name, registry_url)\n  subprocess.call([\'ks\', \'registry\', \'add\', registry_name, registry_url])\n\n  for name in libs_to_remove:\n    package = ""{0}/{1}"".format(registry_name, name)\n    logging.info(""Installing package %s"", package)\n    subprocess.call([\'ks\', \'pkg\', \'install\', package])\n\n\nif __name__ == ""__main__"":\n  main()\n'"
testing/__init__.py,0,"b'# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
testing/auth.py,0,"b'import logging\n\nfrom . import gcp_util as gcp\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=(""%(levelname)s | %(lineno)d | AUTH | %(message)s""),\n)\n\n\ndef login_to_kubeflow_iap(driver, kubeflow_url):\n    """"""\n    This function logs in to the kubeflow cluster via IAP\n    """"""\n    service_account_credentials = gcp.get_service_account_credentials(\n        ""CLIENT_ID""\n    )\n    google_open_id_connect_token = gcp.get_google_open_id_connect_token(\n        service_account_credentials\n    )\n\n    driver.header_overrides = {\n        ""Authorization"": ""Bearer {}"".format(google_open_id_connect_token)\n    }\n\n    driver.get(kubeflow_url)\n\n\ndef login_to_kubeflow_dex(driver, kubeflow_url, username, password):\n    """"""\n    This function logs in to the kubeflow cluster via DEX\n    """"""\n    driver.get(kubeflow_url)\n    username_input = driver.find_element_by_id(""login"")\n    password_input = driver.find_element_by_id(""password"")\n    login_button = driver.find_element_by_id(""submit-login"")\n\n    username_input.send_keys(username)\n    password_input.send_keys(password)\n    login_button.click()\n'"
testing/deploy_utils.py,0,"b'# -*- coding: utf-8 -*-\nimport argparse\nimport datetime\nimport json\nimport logging\nimport os\nimport shutil\nimport ssl\nimport tempfile\nimport time\nimport uuid\n\nimport requests\nimport yaml\nfrom googleapiclient import discovery, errors\nfrom kubernetes import client as k8s_client\nfrom kubernetes.client import rest\nfrom kubernetes.config import kube_config\nfrom oauth2client.client import GoogleCredentials\n\nfrom kubeflow.testing import test_util, util  # pylint: disable=no-name-in-module  # noqa: E501\nfrom testing import vm_util\n\n\ndef get_gcp_identity():\n  identity = util.run([""gcloud"", ""config"", ""get-value"", ""account""])\n  logging.info(""Current GCP account: %s"", identity)\n  return identity\n\n\ndef create_k8s_client():\n  # We need to load the kube config so that we can have credentials to\n  # talk to the APIServer.\n  util.load_kube_config(persist_config=False)\n\n  # Create an API client object to talk to the K8s master.\n  api_client = k8s_client.ApiClient()\n\n  return api_client\n\n\ndef _setup_test(api_client, run_label):\n  """"""Create the namespace for the test.\n\n  Returns:\n    test_dir: The local test directory.\n  """"""\n\n  api = k8s_client.CoreV1Api(api_client)\n  namespace = k8s_client.V1Namespace()\n  namespace.api_version = ""v1""\n  namespace.kind = ""Namespace""\n  namespace.metadata = k8s_client.V1ObjectMeta(\n      name=run_label, labels={\n          ""app"": ""kubeflow-e2e-test"",\n      })\n\n  try:\n    logging.info(""Creating namespace %s"", namespace.metadata.name)\n    namespace = api.create_namespace(namespace)\n    logging.info(""Namespace %s created."", namespace.metadata.name)\n  except rest.ApiException as e:\n    if e.status == 409:\n      logging.info(""Namespace %s already exists."", namespace.metadata.name)\n    else:\n      raise\n\n  return namespace\n\n\ndef setup_kubeflow_ks_app(dir, namespace, github_token, api_client):\n  """"""Create a ksonnet app for Kubeflow""""""\n  util.makedirs(dir)\n\n  logging.info(""Using test directory: %s"", dir)\n\n  namespace_name = namespace\n\n  namespace = _setup_test(api_client, namespace_name)\n  logging.info(""Using namespace: %s"", namespace)\n  if github_token:\n    logging.info(""Setting GITHUB_TOKEN to %s."", github_token)\n    # Set a GITHUB_TOKEN so that we don\'t rate limited by GitHub;\n    # see: https://github.com/ksonnet/ksonnet/issues/233\n    os.environ[""GITHUB_TOKEN""] = github_token\n\n  if not os.getenv(""GITHUB_TOKEN""):\n    logging.warning(""GITHUB_TOKEN not set; you will probably hit Github API ""\n                    ""limits."")\n  # Initialize a ksonnet app.\n  app_name = ""kubeflow-test-"" + uuid.uuid4().hex[0:4]\n  util.run([\n      ""ks"",\n      ""init"",\n      app_name,\n  ], cwd=dir)\n\n  app_dir = os.path.join(dir, app_name)\n\n  # Set the default namespace.\n  util.run([""ks"", ""env"", ""set"", ""default"", ""--namespace="" + namespace_name],\n           cwd=app_dir)\n\n  kubeflow_registry = ""github.com/kubeflow/kubeflow/tree/master/kubeflow""\n  util.run([""ks"", ""registry"", ""add"", ""kubeflow"", kubeflow_registry],\n           cwd=app_dir)\n\n  # Install required packages\n  packages = [\n      ""kubeflow/common"", ""kubeflow/gcp"", ""kubeflow/jupyter"",\n      ""kubeflow/tf-serving"", ""kubeflow/tf-job"", ""kubeflow/tf-training"",\n      ""kubeflow/pytorch-job"", ""kubeflow/argo"", ""kubeflow/katib""\n  ]\n\n  # Instead of installing packages we edit the app.yaml file directly for p in\n  # packages:\n  # util.run([""ks"", ""pkg"", ""install"", p], cwd=app_dir)\n  app_file = os.path.join(app_dir, ""app.yaml"")\n  with open(app_file) as f:\n    app_yaml = yaml.load(f)\n\n  libraries = {}\n  for pkg in packages:\n    pkg = pkg.split(""/"")[1]\n    libraries[pkg] = {\n        \'gitVersion\': {\n            \'commitSha\': \'fake\',\n            \'refSpec\': \'fake\'\n        },\n        \'name\': pkg,\n        \'registry\': ""kubeflow""\n    }\n  app_yaml[\'libraries\'] = libraries\n\n  with open(app_file, ""w"") as f:\n    yaml.dump(app_yaml, f)\n\n  # Create vendor directory with a symlink to the src so that we use the code\n  # at the desired commit.\n  target_dir = os.path.join(app_dir, ""vendor"", ""kubeflow"")\n\n  REPO_ORG = ""kubeflow""\n  REPO_NAME = ""kubeflow""\n  REGISTRY_PATH = ""kubeflow""\n  source = os.path.join(dir, ""src"", REPO_ORG, REPO_NAME, REGISTRY_PATH)\n  logging.info(""Creating link %s -> %s"", target_dir, source)\n  os.symlink(source, target_dir)\n\n  return app_dir\n\n\ndef log_operation_status(operation):\n  """"""A callback to use with wait_for_operation.""""""\n  name = operation.get(""name"", """")\n  status = operation.get(""status"", """")\n  logging.info(""Operation %s status %s"", name, status)\n\n\ndef wait_for_operation(client,\n                       project,\n                       op_id,\n                       timeout=datetime.timedelta(hours=1),\n                       polling_interval=datetime.timedelta(seconds=5),\n                       status_callback=log_operation_status):\n  """"""Wait for the specified operation to complete.\n\n  Args:\n    client: Client for the API that owns the operation.\n    project: project\n    op_id: Operation id.\n    timeout: A datetime.timedelta expressing the amount of time to wait before\n      giving up.\n    polling_interval: A datetime.timedelta to represent the amount of time to\n      wait between requests polling for the operation status.\n\n  Returns:\n    op: The final operation.\n\n  Raises:\n    TimeoutError: if we timeout waiting for the operation to complete.\n  """"""\n  endtime = datetime.datetime.now() + timeout\n  while True:\n    try:\n      op = client.operations().get(project=project, operation=op_id).execute()\n\n      if status_callback:\n        status_callback(op)\n\n      status = op.get(""status"", """")\n      # Need to handle other status\'s\n      if status == ""DONE"":\n        return op\n    except ssl.SSLError as e:\n      logging.error(""Ignoring error %s"", e)\n    if datetime.datetime.now() > endtime:\n      raise TimeoutError(\n          ""Timed out waiting for op: {0} to complete."".format(op_id))\n    time.sleep(polling_interval.total_seconds())\n\n  # Linter complains if we don\'t have a return here even though its unreachable\n  return None\n'"
testing/gcp_util.py,0,"b'import argparse\nimport base64\nimport datetime\nimport logging\nimport os\nimport errno\nimport shutil\nimport subprocess\nimport tempfile\nimport threading\nfrom functools import partial\nfrom multiprocessing import Process\nfrom time import sleep\nfrom google.auth.transport.requests import Request\nfrom googleapiclient import discovery\nfrom oauth2client.client import GoogleCredentials\n\nimport requests\nimport yaml\nimport google.auth\nimport google.auth.compute_engine.credentials\nimport google.auth.iam\nimport google.oauth2.credentials\nimport google.oauth2.service_account\nfrom retrying import retry\nfrom requests.exceptions import SSLError\nfrom requests.exceptions import ConnectionError as ReqConnectionError\n\nIAM_SCOPE = ""https://www.googleapis.com/auth/iam""\nOAUTH_TOKEN_URI = ""https://www.googleapis.com/oauth2/v4/token""\nCOOKIE_NAME = ""KUBEFLOW-AUTH-KEY""\n\ndef get_service_account_credentials(client_id_key):\n  # Figure out what environment we\'re running in and get some preliminary\n  # information about the service account.\n  credentials, _ = google.auth.default(scopes=[IAM_SCOPE])\n  if isinstance(credentials, google.oauth2.credentials.Credentials):\n    raise Exception(""make_iap_request is only supported for service ""\n                    ""accounts."")\n\n  # For service account\'s using the Compute Engine metadata service,\n  # service_account_email isn\'t available until refresh is called.\n  credentials.refresh(Request())\n\n  signer_email = credentials.service_account_email\n  if isinstance(credentials,\n                google.auth.compute_engine.credentials.Credentials):\n    signer = google.auth.iam.Signer(Request(), credentials, signer_email)\n  else:\n    # A Signer object can sign a JWT using the service account\'s key.\n    signer = credentials.signer\n\n  # Construct OAuth 2.0 service account credentials using the signer\n  # and email acquired from the bootstrap credentials.\n  return google.oauth2.service_account.Credentials(\n      signer,\n      signer_email,\n      token_uri=OAUTH_TOKEN_URI,\n      additional_claims={""target_audience"": may_get_env_var(client_id_key)})\n\ndef get_google_open_id_connect_token(service_account_credentials):\n  service_account_jwt = (\n      service_account_credentials._make_authorization_grant_assertion())\n  request = google.auth.transport.requests.Request()\n  body = {\n      ""assertion"": service_account_jwt,\n      ""grant_type"": google.oauth2._client._JWT_GRANT_TYPE,\n  }\n  token_response = google.oauth2._client._token_endpoint_request(\n      request, OAUTH_TOKEN_URI, body)\n  return token_response[""id_token""]\n\ndef may_get_env_var(name):\n  env_val = os.getenv(name)\n  if env_val:\n    logging.info(""%s is set"" % name)\n    return env_val\n  else:\n    raise Exception(""%s not set"" % name)\n\ndef iap_is_ready(url, wait_min=15):\n  """"""\n  Checks if the kubeflow endpoint is ready.\n\n  Args:\n    url: The url endpoint\n  Returns:\n    True if the url is ready\n  """"""\n  google_open_id_connect_token = None\n\n  service_account_credentials = get_service_account_credentials(""CLIENT_ID"")\n  google_open_id_connect_token = get_google_open_id_connect_token(\n      service_account_credentials)\n  # Wait up to 30 minutes for IAP access test.\n  num_req = 0\n  end_time = datetime.datetime.now() + datetime.timedelta(\n      minutes=wait_min)\n  while datetime.datetime.now() < end_time:\n    num_req += 1\n    logging.info(""Trying url: %s"", url)\n    try:\n      resp = None\n      resp = requests.request(\n          ""GET"",\n          url,\n          headers={\n              ""Authorization"":\n              ""Bearer {}"".format(google_open_id_connect_token)\n          },\n          verify=False)\n      logging.info(resp.text)\n      if resp.status_code == 200:\n        logging.info(""Endpoint is ready for %s!"", url)\n        return True\n      else:\n        logging.info(\n            ""%s: Endpoint not ready, request number: %s"" % (url, num_req))\n    except Exception as e:\n      logging.info(""%s: Endpoint not ready, exception caught %s, request number: %s"" %\n                   (url, str(e), num_req))\n    sleep(10)\n  return False\n\ndef basic_auth_is_ready(url, username, password, wait_min=15):\n  get_url = url + ""/kflogin""\n  post_url = url + ""/apikflogin""\n\n  req_num = 0\n  end_time = datetime.datetime.now() + datetime.timedelta(\n      minutes=wait_min)\n  while datetime.datetime.now() < end_time:\n    req_num += 1\n    logging.info(""Trying url: %s request number %s"" % (get_url, req_num))\n    resp = None\n    try:\n      resp = requests.request(\n          ""GET"",\n          get_url,\n          verify=False)\n    except SSLError as e:\n      logging.warning(""%s: Endpoint SSL handshake error: %s; request number: %s"" % (url, e, req_num))\n    except ReqConnectionError:\n      logging.info(\n          ""%s: Endpoint not ready, request number: %s"" % (url, req_num))\n    if not resp or resp.status_code != 200:\n      logging.info(""Basic auth login is not ready, request number %s: %s"" % (req_num, get_url))\n    else:\n      break\n    sleep(10)\n\n  logging.info(""%s: endpoint is ready, testing login API; request number %s"" % (get_url, req_num))\n  resp = requests.post(\n      post_url,\n      auth=(username, password),\n      headers={\n          ""x-from-login"": ""true"",\n      },\n      verify=False)\n  logging.info(""%s: %s"" % (post_url, resp.text))\n  if resp.status_code != 205:\n    logging.error(""%s: login is failed"", post_url)\n    return False\n\n  cookie = None\n  for c in resp.cookies:\n    if c.name == COOKIE_NAME:\n      cookie = c\n      break\n  if cookie is None:\n    logging.error(""%s: auth cookie cannot be found; name: %s"" % (post_url, COOKIE_NAME))\n    return False\n\n  resp = requests.get(\n      url,\n      cookies={\n          cookie.name: cookie.value,\n      },\n      verify=False)\n  logging.info(""%s: %s"" % (url, resp.status_code))\n  logging.info(resp.content)\n  return resp.status_code == 200\n'"
testing/get_gke_credentials.py,0,"b'# -*- coding: utf-8 -*-\nimport argparse\nimport logging\nimport os\nimport yaml\nfrom kubeflow.testing import test_helper, util\nfrom kubernetes.config import kube_config\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--cluster"",\n      default=None,\n      type=str,\n      help=(\n          ""The name of the cluster. If not set assumes the script is running in""\n          "" a cluster and uses that cluster.""))\n  parser.add_argument(\n      ""--zone"",\n      default=""us-east1-d"",\n      type=str,\n      help=""The zone for the cluster."")\n  parser.add_argument(\n      ""--project"", default=None, type=str, help=""The project to use."")\n  args, _ = parser.parse_known_args()\n  return args\n\n\ndef get_gke_credentials(test_case):\n  """"""Configure kubeconfig to talk to the supplied GKE cluster.""""""\n  args = parse_args()\n  util.maybe_activate_service_account()\n  config_file = os.path.expanduser(kube_config.KUBE_CONFIG_DEFAULT_LOCATION)\n  logging.info(""Using Kubernetes config file: %s"", config_file)\n  project = args.project\n  cluster_name = args.cluster\n  zone = args.zone\n  logging.info(""Using cluster: %s in project: %s in zone: %s"", cluster_name,\n               project, zone)\n  # Print out config to help debug issues with accounts and\n  # credentials.\n  util.run([""gcloud"", ""config"", ""list""])\n  util.configure_kubectl(project, zone, cluster_name)\n\n  # We want to modify the KUBECONFIG file to remove the gcloud commands\n  # for any users that are authenticating using service accounts.\n  # This will allow the script to be truly headless and not require gcloud.\n  # More importantly, kubectl will properly attach auth.info scope so that\n  # RBAC rules can be applied to the email and not the id.\n  # See https://github.com/kubernetes/kubernetes/pull/58141\n  #\n  # TODO(jlewi): We might want to check GOOGLE_APPLICATION_CREDENTIALS\n  # to see whether we are actually using a service account. If we aren\'t\n  # using a service account then we might not want to delete the gcloud\n  # commands.\n  logging.info(""Modifying kubeconfig %s"", config_file)\n  with open(config_file, ""r"") as hf:\n    config = yaml.load(hf)\n\n  for user in config[""users""]:\n    auth_provider = user.get(""user"", {}).get(""auth-provider"", {})\n    if auth_provider.get(""name"") != ""gcp"":\n      continue\n    logging.info(""Modifying user %s which has gcp auth provider"", user[""name""])\n    if ""config"" in auth_provider:\n      logging.info(""Deleting config from user %s"", user[""name""])\n      del auth_provider[""config""]\n\n      # This is a hack because the python client library will complain\n      # about an invalid config if there is no config field.\n      #\n      # It looks like the code checks here but that doesn\'t seem to work\n      # https://github.com/kubernetes-client/python-base/blob/master/config/kube_config.py#L209\n      auth_provider[""config""] = {\n          ""dummy"": ""dummy"",\n      }\n  logging.info(""Writing update kubeconfig:\\n %s"", yaml.dump(config))\n  with open(config_file, ""w"") as hf:\n    yaml.dump(config, hf)\n\n\ndef main():\n  test_case = test_helper.TestCase(\n      name=\'get_gke_credentials\', test_func=get_gke_credentials)\n  test_suite = test_helper.init(\n      name=\'get_gke_credentials\', test_cases=[test_case])\n  test_suite.run()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
testing/katib_studyjob_test.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nLaunch a simple katib studyjob and verify that it runs.\n\nTODO(ricliu): This code shares a lot in common with the e2etest for tf-operator.\nConsider merging the common code into a CRD library. There are only some minor\ndifferences - for example TFJob Status has a list of job conditions, whereas\nKatib Studyjob status only shows the most recent condition.\n""""""\n\nimport argparse\nimport datetime\nimport logging\nimport multiprocessing\nimport os\nimport re\nimport subprocess\nimport time\n\nfrom kubernetes import client as k8s_client\nfrom kubeflow.testing import test_helper, util\nfrom retrying import retry\n\nNAMESPACE = ""default""\nSTUDY_JOB_GROUP = ""kubeflow.org""\nSTUDY_JOB_PLURAL = ""studyjobs""\nSTUDY_JOB_KIND = ""StudyJob""\nTIMEOUT = 120\n\n\n# TODO: TimeoutError is a built in exception in python3 so we can\n# delete this when we go to Python3.\nclass TimeoutError(Exception):  # pylint: disable=redefined-builtin\n  """"""An error indicating an operation timed out.""""""\n\n\nclass JobTimeoutError(TimeoutError):\n  """"""An error indicating the job timed out.\n    The job spec/status can be found in .job.\n  """"""\n\n  def __init__(self, message, job):\n    super(JobTimeoutError, self).__init__(message)\n    self.job = job\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--src_dir"", default="""", type=str, help=""The kubeflow src directory"")\n  parser.add_argument(\n      ""--studyjob_version"",\n      default=""v1alpha1"",\n      type=str,\n      help=""Which katib study job version to use"")\n  args, _ = parser.parse_known_args()\n  return args\n\n\n@retry(stop_max_attempt_number=3)\ndef create_app_and_job(args, namespace, name):\n  try:\n    util.run([\n        ""ks"", ""init"", ""katib-app"", ""--skip-default-registries"",\n        ""--namespace="" + namespace\n    ])\n  except subprocess.CalledProcessError as e:\n    # Keep going if the app already exists. This is a sign the a previous\n    # attempt failed and we are retrying.\n    if not re.search("".*already exists.*"", e.output):\n      raise\n\n  os.chdir(""katib-app"")\n  try:\n    util.run([""ks"", ""registry"", ""add"", ""kubeflow"", args.src_dir + ""/kubeflow""])\n  except subprocess.CalledProcessError as e:\n    # Keep going if the registry has already been added.\n    # This is a sign the a previous attempt failed and we are retrying.\n    if not re.search("".*already exists.*"", e.output):\n      raise\n\n  try:\n    util.run([""ks"", ""pkg"", ""install"", ""kubeflow/examples""])\n  except subprocess.CalledProcessError as e:\n    # Keep going if the package has already been added.\n    # This is a sign the a previous attempt failed and we are retrying.\n    if not re.search("".*already exists.*"", e.output):\n      raise\n\n  if args.studyjob_version == ""v1alpha1"":\n    prototype_name = ""katib-studyjob-test-v1alpha1""\n  else:\n    raise ValueError(\n        ""Unrecognized value for studyjob_version: %s"" % args.studyjob_version)\n\n  util.run([""ks"", ""generate"", prototype_name, name])\n  util.run([""ks"", ""apply"", ""default"", ""-c"", ""katib-studyjob-test""])\n\n\n@retry(wait_fixed=10000, stop_max_attempt_number=20)\ndef log_status(study_job):\n  """"""A callback to use with wait_for_job.""""""\n  condition = study_job.get(""status"", {}).get(""condition"")\n  logging.info(""Job %s in namespace %s; uid=%s; condition=%s"",\n               study_job.get(""metadata"", {}).get(""name""),\n               study_job.get(""metadata"", {}).get(""namespace""),\n               study_job.get(""metadata"", {}).get(""uid""), condition)\n\n\n# This is a modification of\n# https://github.com/kubeflow/tf-operator/blob/master/py/tf_job_client.py#L119.\n# pylint: disable=too-many-arguments\ndef wait_for_condition(client,\n                       namespace,\n                       name,\n                       expected_condition,\n                       version=""v1alpha1"",\n                       timeout=datetime.timedelta(minutes=10),\n                       polling_interval=datetime.timedelta(seconds=30),\n                       status_callback=None):\n  """"""Waits until any of the specified conditions occur.\n  Args:\n    client: K8s api client.\n    namespace: namespace for the job.\n    name: Name of the job.\n    expected_condition: A list of conditions. Function waits until any of the\n      supplied conditions is reached.\n    timeout: How long to wait for the job.\n    polling_interval: How often to poll for the status of the job.\n    status_callback: (Optional): Callable. If supplied this callable is\n      invoked after we poll the job. Callable takes a single argument which is\n      the job.\n  """"""\n  crd_api = k8s_client.CustomObjectsApi(client)\n  end_time = datetime.datetime.now() + timeout\n  while True:\n    # By setting async_req=True ApiClient returns multiprocessing.pool.AsyncResult\n    # If we don\'t set async_req=True then it could potentially block\n    # forever.\n    thread = crd_api.get_namespaced_custom_object(\n        STUDY_JOB_GROUP,\n        version,\n        namespace,\n        STUDY_JOB_PLURAL,\n        name,\n        async_req=True)\n\n    # Try to get the result but timeout.\n    results = None\n    try:\n      results = thread.get(TIMEOUT)\n    except multiprocessing.TimeoutError:\n      logging.error(""Timeout trying to get StudyJob."")\n    except Exception as e:\n      logging.error(\n          ""There was a problem waiting for StudyJob %s.%s; Exception; %s"", name,\n          name, e)\n      raise\n\n    if results:\n      if status_callback:\n        status_callback(results)\n\n      condition = results.get(""status"", {}).get(""condition"")\n      if condition in expected_condition:\n        return results\n\n    if datetime.datetime.now() + polling_interval > end_time:\n      raise JobTimeoutError(\n          ""Timeout waiting for job {0} in namespace {1} to enter one of the ""\n          ""conditions {2}."".format(name, namespace, expected_condition),\n          results)\n\n    time.sleep(polling_interval.seconds)\n\n  # Linter complains if we don\'t have a return statement even though\n  # this code is unreachable.\n  return None\n\n\ndef test_katib(test_case):  # pylint: disable=redefined-outer-name\n  args = parse_args()\n  namespace = NAMESPACE\n  name = ""katib-studyjob-test""\n\n  util.load_kube_config()\n  api_client = k8s_client.ApiClient()\n  create_app_and_job(args, namespace, name)\n  try:\n    wait_for_condition(\n        api_client, namespace, name, [""Running""], status_callback=log_status)\n    logging.info(""StudyJob launched successfully"")\n  except Exception as e:\n    logging.error(""Test failed waiting for job; %s"", e)\n    test_case.add_failure_info(e.message)\n\n\nif __name__ == ""__main__"":\n  test_case = test_helper.TestCase(name=""test_katib"", test_func=test_katib)\n  test_suite = test_helper.init(name=""test_katib"", test_cases=[test_case])\n  test_suite.run()\n'"
testing/run_with_retry.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nrun_with_retry runs the given binary with the given number of retries\n\nThis is intended primary for retrying bash scripts. Ideally, we sould use\nargo\'s retryStrategy, but there is a bug in it\'s implementation:\nhttps://github.com/argoproj/argo/issues/885\n\nExample:\n  python run_with_retry --retries=5 -- bash my_flaky_script.sh\n\nThis runs bash my_flaky_script.sh upto 5 times till it succeeds\n""""""\nimport argparse\nfrom kubeflow.testing import test_helper, util\nfrom retrying import retry\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--retries"", required=True, type=int, help=""The number of retries."")\n\n  parser.add_argument(\'remaining_args\', nargs=argparse.REMAINDER)\n  args, _ = parser.parse_known_args()\n  return args\n\n\ndef run_with_retry(_):\n  """"""Deploy Kubeflow.""""""\n  args = parse_args()\n\n  @retry(stop_max_attempt_number=args.retries)\n  def run():\n    util.run(args.remaining_args[1:])\n\n  run()\n\n\ndef main():\n  test_case = test_helper.TestCase(\n      name=\'run_with_retry\', test_func=run_with_retry)\n  test_suite = test_helper.init(name=\'run_with_retry\', test_cases=[test_case])\n  test_suite.run()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
testing/test_deploy.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Test deploying Kubeflow.\n\nRequirements:\n  This project assumes the py directory in github.com/kubeflow/tf-operator corresponds\n  to a top level Python package on the Python path.\n\n  TODO(jlewi): Come up with a better story for how we reuse the py package\n  in kubeflow/tf-operator. We should probably turn that into a legit Python pip\n  package that is built and released as part of the kubeflow/tf-operator project.\n""""""\n\nimport argparse\nimport datetime\nimport json\nimport logging\nimport os\nimport errno\nimport re\nimport shutil\nimport subprocess\nimport tempfile\nimport time\nimport uuid\n\nimport requests\nimport yaml\nfrom googleapiclient import discovery, errors\nfrom kubernetes import client as k8s_client\nfrom kubernetes.client import rest\nfrom kubernetes.config import kube_config\nfrom oauth2client.client import GoogleCredentials\n\nfrom kubeflow.testing import test_util, util  # pylint: disable=no-name-in-module\nfrom testing import vm_util\n\n# The ksonnet binary\nks = ""ks-13""\n\ndef _setup_test(api_client, run_label):\n  """"""Create the namespace for the test.\n\n  Returns:\n    test_dir: The local test directory.\n  """"""\n\n  api = k8s_client.CoreV1Api(api_client)\n  namespace = k8s_client.V1Namespace()\n  namespace.api_version = ""v1""\n  namespace.kind = ""Namespace""\n  namespace.metadata = k8s_client.V1ObjectMeta(\n      name=run_label, labels={\n          ""app"": ""kubeflow-e2e-test"",\n      })\n\n  try:\n    logging.info(""Creating namespace %s"", namespace.metadata.name)\n    namespace = api.create_namespace(namespace)\n    logging.info(""Namespace %s created."", namespace.metadata.name)\n  except rest.ApiException as e:\n    if e.status == 409:\n      logging.info(""Namespace %s already exists."", namespace.metadata.name)\n    else:\n      raise\n\n  return namespace\n\n\ndef create_k8s_client(_):\n  # We need to load the kube config so that we can have credentials to\n  # talk to the APIServer.\n  util.load_kube_config(persist_config=False)\n\n  # Create an API client object to talk to the K8s master.\n  api_client = k8s_client.ApiClient()\n\n  return api_client\n\n\n# TODO(jlewi): We should make this a reusable function in kubeflow/testing\n# because we will probably want to use it in other places as well.\ndef setup_kubeflow_ks_app(args, api_client):\n  """"""Create a ksonnet app for Kubeflow""""""\n  try:\n    os.makedirs(args.test_dir)\n  except OSError as exc:  # Python >2.5\n    if exc.errno == errno.EEXIST and os.path.isdir(args.test_dir):\n      pass\n    else:\n      raise\n\n  logging.info(""Using test directory: %s"", args.test_dir)\n\n  namespace_name = args.namespace\n\n  namespace = _setup_test(api_client, namespace_name)\n  logging.info(""Using namespace: %s"", namespace)\n  if args.github_token:\n    logging.info(""Setting GITHUB_TOKEN to %s."", args.github_token)\n    # Set a GITHUB_TOKEN so that we don\'t rate limited by GitHub;\n    # see: https://github.com/ksonnet/ksonnet/issues/233\n    os.environ[""GITHUB_TOKEN""] = args.github_token\n\n  if not os.getenv(""GITHUB_TOKEN""):\n    logging.warning(""GITHUB_TOKEN not set; you will probably hit Github API ""\n                    ""limits."")\n  # Initialize a ksonnet app.\n  app_name = ""kubeflow-test-"" + uuid.uuid4().hex[0:4]\n  util.run([\n      ks,\n      ""init"",\n      app_name,\n  ], cwd=args.test_dir)\n\n  app_dir = os.path.join(args.test_dir, app_name)\n\n  kubeflow_registry = ""github.com/kubeflow/kubeflow/tree/master/kubeflow""\n  util.run([ks, ""registry"", ""add"", ""kubeflow"", kubeflow_registry],\n           cwd=app_dir)\n\n  # Install required packages\n  packages = [\n      ""kubeflow/common"", ""kubeflow/tf-training"", ""kubeflow/pytorch-job"",\n      ""kubeflow/argo""\n  ]\n\n  # Instead of installing packages we edit the app.yaml file directly\n  #for p in packages:\n  # util.run([ks, ""pkg"", ""install"", p], cwd=app_dir)\n  app_file = os.path.join(app_dir, ""app.yaml"")\n  with open(app_file) as f:\n    app_yaml = yaml.load(f)\n\n  libraries = {}\n  for pkg in packages:\n    pkg = pkg.split(""/"")[1]\n    libraries[pkg] = {\n        \'gitVersion\': {\n            \'commitSha\': \'fake\',\n            \'refSpec\': \'fake\'\n        },\n        \'name\': pkg,\n        \'registry\': ""kubeflow""\n    }\n  app_yaml[\'libraries\'] = libraries\n\n  with open(app_file, ""w"") as f:\n    yaml.dump(app_yaml, f)\n\n  # Create vendor directory with a symlink to the src\n  # so that we use the code at the desired commit.\n  target_dir = os.path.join(app_dir, ""vendor"", ""kubeflow"")\n\n  REPO_ORG = ""kubeflow""\n  REPO_NAME = ""kubeflow""\n  REGISTRY_PATH = ""kubeflow""\n  source = os.path.join(args.test_dir, ""src"", REPO_ORG, REPO_NAME,\n                        REGISTRY_PATH)\n  logging.info(""Creating link %s -> %s"", target_dir, source)\n  os.symlink(source, target_dir)\n\n  return app_dir\n\n\ndef deploy_model(args):\n  """"""Deploy a TF model using the TF serving component.""""""\n  api_client = create_k8s_client(args)\n  app_dir = setup_kubeflow_ks_app(args, api_client)\n\n  logging.info(""Deploying tf-serving."")\n  params = {}\n  for pair in args.params.split("",""):\n    k, v = pair.split(""="", 1)\n    if k != ""namespace"":\n      params[k] = v\n    else:\n      namespace = v\n\n  if namespace == None:\n    raise ValueError(""namespace must be supplied in args."")\n\n  # deployment component\n  deployComponent = ""modelServer""\n  generate_command = [\n      ks, ""generate"", ""tf-serving-deployment-gcp"", deployComponent\n  ]\n  util.run(generate_command, cwd=app_dir)\n  ks_deploy(\n      app_dir,\n      deployComponent,\n      params,\n      env=None,\n      account=None,\n      namespace=namespace)\n\n  # service component\n  serviceComponent = ""modelServer-service""\n  generate_command = [ks, ""generate"", ""tf-serving-service"", serviceComponent]\n  util.run(generate_command, cwd=app_dir)\n  ks_deploy(\n      app_dir,\n      serviceComponent,\n      params,\n      env=None,\n      account=None,\n      namespace=namespace)\n\n  core_api = k8s_client.CoreV1Api(api_client)\n  deploy = core_api.read_namespaced_service(args.deploy_name, args.namespace)\n  cluster_ip = deploy.spec.cluster_ip\n\n  if not cluster_ip:\n    raise ValueError(""inception service wasn\'t assigned a cluster ip."")\n  util.wait_for_deployment(\n      api_client, namespace, args.deploy_name, timeout_minutes=10)\n  logging.info(""Verified TF serving started."")\n\n\ndef test_successful_deployment(deployment_name):\n  """""" Tests if deployment_name is successfully running using kubectl """"""\n  # TODO use the python kubernetes library to get deployment status\n  # This is using kubectl right now\n  retries = 20\n  i = 0\n  while True:\n    if i == retries:\n      raise Exception(\'Deployment failed: \' + deployment_name)\n    try:\n      output = util.run([""kubectl"", ""get"", ""deployment"", deployment_name, ""-n"", ""kubeflow""])\n      logging.info(""output = \\n"" + output)\n      if output.count(\'\\n\') == 1:\n        output = output.split(\'\\n\')[1]\n        output = re.split(\' +\', output)\n        desired_pods = output[1]\n        current_pods = output[2]\n        uptodate_pods = output[3]\n        available_pods = output[4]\n        logging.info(""desired_pods "" + desired_pods)\n        logging.info(""current_pods "" + current_pods)\n        logging.info(""uptodate_pods "" + uptodate_pods)\n        logging.info(""available_pods "" + available_pods)\n        if desired_pods == current_pods and \\\n           desired_pods == uptodate_pods and \\\n           desired_pods == available_pods:\n          return True\n    except subprocess.CalledProcessError as e:\n      logging.error(e)\n    logging.info(""Sleeping 5 seconds and retrying.."")\n    time.sleep(5)\n    i += 1\n\n\ndef test_katib(args):\n  test_successful_deployment(\'katib-db\')\n  test_successful_deployment(\'katib-manager\')\n  test_successful_deployment(\'katib-ui\')\n  test_successful_deployment(\'katib-controller\')\n\n\ndef deploy_argo(args):\n  api_client = create_k8s_client(args)\n  app_dir = setup_kubeflow_ks_app(args, api_client)\n\n  component = ""argo""\n  logging.info(""Deploying argo"")\n  generate_command = [ks, ""generate"", ""argo"", component, ""--name"", ""argo""]\n  util.run(generate_command, cwd=app_dir)\n\n  ks_deploy(app_dir, component, {}, env=None, account=None, namespace=None)\n\n  # Create a hello world workflow\n  util.run([\n      ""kubectl"", ""create"", ""-n"", ""default"", ""-f"",\n      ""https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-world.yaml""\n  ],\n           cwd=app_dir)\n\n  # Wait for 200 seconds to check if the hello-world pod was created\n  retries = 20\n  i = 0\n  while True:\n    if i == retries:\n      raise Exception(\'Failed to run argo workflow\')\n    output = util.run([\n        ""kubectl"", ""get"", ""pods"", ""-n"", ""default"",\n        ""-lworkflows.argoproj.io/workflow""\n    ])\n    if ""hello-world-"" in output:\n      return True\n    time.sleep(10)\n    i += 1\n\n\ndef deploy_pytorchjob(args):\n  """"""Deploy Pytorchjob using the pytorch-job component""""""\n  api_client = create_k8s_client(args)\n  app_dir = setup_kubeflow_ks_app(args, api_client)\n\n  component = ""example-job""\n  logging.info(""Deploying pytorch."")\n  generate_command = [ks, ""generate"", ""pytorch-job"", component]\n\n  util.run(generate_command, cwd=app_dir)\n\n  params = {}\n  for pair in args.params.split("",""):\n    k, v = pair.split(""="", 1)\n    params[k] = v\n\n  ks_deploy(app_dir, component, params, env=None, account=None, namespace=None)\n\n\ndef teardown(args):\n  # Delete the namespace\n  logging.info(""Deleting namespace %s"", args.namespace)\n  api_client = create_k8s_client(args)\n  core_api = k8s_client.CoreV1Api(api_client)\n  core_api.delete_namespace(args.namespace, {})\n\n\ndef determine_test_name(args):\n  if args.deploy_name:\n    return args.func.__name__ + ""-"" + args.deploy_name\n  return args.func.__name__\n\n\n# TODO(jlewi): We should probably make this a generic function in\n# kubeflow.testing.`\ndef wrap_test(args):\n  """"""Run the tests given by args.func and output artifacts as necessary.\n  """"""\n  test_name = determine_test_name(args)\n  test_case = test_util.TestCase()\n  test_case.class_name = ""KubeFlow""\n  test_case.name = args.workflow_name + ""-"" + test_name\n  try:\n\n    def run():\n      args.func(args)\n\n    test_util.wrap_test(run, test_case)\n  finally:\n    # Test grid has problems with underscores in the name.\n    # https://github.com/kubeflow/kubeflow/issues/631\n    # TestGrid currently uses the regex junit_(^_)*.xml so we only\n    # want one underscore after junit.\n    junit_name = test_case.name.replace(""_"", ""-"")\n    junit_path = os.path.join(args.artifacts_dir,\n                              ""junit_{0}.xml"".format(junit_name))\n    logging.info(""Writing test results to %s"", junit_path)\n    test_util.create_junit_xml_file([test_case], junit_path)\n\n\n# TODO(jlewi): We should probably make this a reusable function since a\n# lot of test code code use it.\ndef ks_deploy(app_dir,\n              component,\n              params,\n              env=None,\n              account=None,\n              namespace=None):\n  """"""Deploy the specified ksonnet component.\n  Args:\n    app_dir: The ksonnet directory\n    component: Name of the component to deployed\n    params: A dictionary of parameters to set; can be empty but should not be\n      None.\n    env: (Optional) The environment to use, if none is specified a new one\n      is created.\n    account: (Optional) The account to use.\n    namespace: (Optional) The namespace to use when adding the environment\n  Raises:\n    ValueError: If input arguments aren\'t valid.\n  """"""\n  if not component:\n    raise ValueError(""component can\'t be None."")\n\n  # TODO(jlewi): It might be better if the test creates the app and uses\n  # the latest stable release of the ksonnet configs. That however will cause\n  # problems when we make changes to the TFJob operator that require changes\n  # to the ksonnet configs. One advantage of checking in the app is that\n  # we can modify the files in vendor if needed so that changes to the code\n  # and config can be submitted in the same pr.\n  now = datetime.datetime.now()\n  if not env:\n    env = ""e2e-"" + now.strftime(""%m%d-%H%M-"") + uuid.uuid4().hex[0:4]\n\n  logging.info(""Using app directory: %s"", app_dir)\n\n  if not namespace:\n    util.run([ks, ""env"", ""add"", env], cwd=app_dir)\n  else:\n    util.run([ks, ""env"", ""add"", env, ""--namespace="" + namespace], cwd=app_dir)\n\n  for k, v in params.iteritems():\n    util.run([ks, ""param"", ""set"", ""--env="" + env, component, k, v],\n             cwd=app_dir)\n\n  apply_command = [ks, ""apply"", env, ""-c"", component]\n  if account:\n    apply_command.append(""--as="" + account)\n  util.run(apply_command, cwd=app_dir)\n\n\ndef modify_minikube_config(config_path, certs_dir):\n  """"""Modify the kube config file used with minikube.\n\n  This function changes the location of the certificates to certs_dir.\n\n  Args:\n    config_path: The path of the Kubernetes config file.\n    certs_dir: The directory where the certs to use with minikube are stored.\n  """"""\n  with open(config_path, ""r"") as hf:\n    config = yaml.load(hf)\n\n  for cluster in config[""clusters""]:\n    authority = cluster[""cluster""][""certificate-authority""]\n    authority = os.path.join(certs_dir, os.path.basename(authority))\n    cluster[""cluster""][""certificate-authority""] = authority\n\n    for user in config[""users""]:\n      for k in [""client-certificate"", ""client-key""]:\n        user[""user""][k] = os.path.join(certs_dir,\n                                       os.path.basename(user[""user""][k]))\n\n  logging.info(""Updating path of certificates in %s"", config_path)\n  with open(config_path, ""w"") as hf:\n    yaml.dump(config, hf)\n\n\ndef deploy_minikube(args):\n  """"""Create a VM and setup minikube.""""""\n\n  credentials = GoogleCredentials.get_application_default()\n  gce = discovery.build(\n      ""compute"", ""v1"", credentials=credentials, cache_discovery=False)\n  instances = gce.instances()\n  body = {\n      ""name"":\n      args.vm_name,\n      ""machineType"":\n      ""zones/{0}/machineTypes/n1-standard-16"".format(args.zone),\n      ""disks"": [\n          {\n              ""boot"": True,\n              ""initializeParams"": {\n                  ""sourceImage"":\n                  ""projects/ubuntu-os-cloud/global/images/family/ubuntu-1604-lts"",\n                  ""diskSizeGb"": 100,\n                  ""autoDelete"": True,\n              },\n          },\n      ],\n      ""networkInterfaces"": [\n          {\n              ""accessConfigs"": [\n                  {\n                      ""name"": ""external-nat"",\n                      ""type"": ""ONE_TO_ONE_NAT"",\n                  },\n              ],\n              ""network"":\n              ""global/networks/default"",\n          },\n      ],\n  }\n  request = instances.insert(project=args.project, zone=args.zone, body=body)\n  response = None\n  try:\n    response = request.execute()\n    print(""done"")\n  except errors.HttpError as e:\n    if not e.content:\n      raise\n    content = json.loads(e.content)\n    if content.get(""error"", {}).get(""code"") == requests.codes.CONFLICT:\n      # We don\'t want to keep going so we reraise the error after logging\n      # a helpful error message.\n      logging.error(\n          ""Either the VM or the disk %s already exists in zone ""\n          ""%s in project %s "", args.vm_name, args.zone, args.project)\n      raise\n    else:\n      raise\n\n  op_id = response.get(""name"")\n  final_op = vm_util.wait_for_operation(gce, args.project, args.zone, op_id)\n\n  logging.info(""Final result for insert operation: %s"", final_op)\n  if final_op.get(""status"") != ""DONE"":\n    raise ValueError(""Insert operation has status %s"", final_op.get(""status""))\n\n  if final_op.get(""error""):\n    message = ""Insert operation resulted in error %s"".format(\n        final_op.get(""error""))\n    logging.error(message)\n    raise ValueError(message)\n\n  # Locate the install minikube script.\n  install_script = os.path.join(\n      os.path.dirname(__file__), ""install_minikube.sh"")\n\n  if not os.path.exists(install_script):\n    logging.error(""Could not find minikube install script: %s"", install_script)\n\n  vm_util.wait_for_vm(args.project, args.zone, args.vm_name)\n  vm_util.execute_script(args.project, args.zone, args.vm_name, install_script)\n\n  # Copy the .kube and .minikube files to test_dir\n  target = ""~/.kube""\n  full_target = ""{0}:{1}"".format(args.vm_name, target)\n  logging.info(""Copying %s to %s"", target, args.test_dir)\n  util.run([\n      ""gcloud"", ""compute"", ""--project="" + args.project, ""scp"", ""--recurse"",\n      full_target, args.test_dir, ""--zone="" + args.zone\n  ])\n\n  # The .minikube directory contains some really large ISO and other files that we don\'t need; so we\n  # only copy the files we need.\n  minikube_dir = os.path.join(args.test_dir, "".minikube"")\n  try:\n    os.makedirs(minikube_dir)\n  except OSError as exc:  # Python >2.5\n    if exc.errno == errno.EEXIST and os.path.isdir(minikube_dir):\n      pass\n    else:\n      raise\n\n  for target in [""~/.minikube/*.crt"", ""~/.minikube/client.key""]:\n    full_target = ""{0}:{1}"".format(args.vm_name, target)\n    logging.info(""Copying %s to %s"", target, minikube_dir)\n    util.run([\n        ""gcloud"", ""compute"", ""--project="" + args.project, ""scp"", ""--recurse"",\n        full_target, minikube_dir, ""--zone="" + args.zone\n    ])\n\n  config_path = os.path.join(args.test_dir, "".kube"", ""config"")\n  modify_minikube_config(config_path, minikube_dir)\n\n\ndef teardown_minikube(args):\n  """"""Delete the VM used for minikube.""""""\n\n  credentials = GoogleCredentials.get_application_default()\n  gce = discovery.build(""compute"", ""v1"", credentials=credentials)\n  instances = gce.instances()\n\n  request = instances.delete(\n      project=args.project, zone=args.zone, instance=args.vm_name)\n\n  response = request.execute()\n\n  op_id = response.get(""name"")\n  final_op = vm_util.wait_for_operation(gce, args.project, args.zone, op_id)\n\n  logging.info(""Final result for delete operation: %s"", final_op)\n  if final_op.get(""status"") != ""DONE"":\n    raise ValueError(""Delete operation has status %s"", final_op.get(""status""))\n\n  if final_op.get(""error""):\n    message = ""Delete operation resulted in error %s"".format(\n        final_op.get(""error""))\n    logging.error(message)\n    raise ValueError(message)\n\n  # Ensure the disk is deleted. The disk should be auto-deleted with\n  # the VM but just in case we issue a delete request anyway.\n  disks = gce.disks()\n  request = disks.delete(\n      project=args.project, zone=args.zone, disk=args.vm_name)\n\n  response = None\n  try:\n    response = request.execute()\n  except errors.HttpError as e:\n    if not e.content:\n      raise\n    content = json.loads(e.content)\n    if content.get(""error"", {}).get(""code"") == requests.codes.NOT_FOUND:\n      logging.info(""Disk %s in zone %s in project %s already deleted."",\n                   args.vm_name, args.zone, args.project)\n    else:\n      raise\n\n  if response:\n    logging.info(""Waiting for disk to be deleted."")\n    op_id = response.get(""name"")\n    final_op = vm_util.wait_for_operation(gce, args.project, args.zone, op_id)\n\n    logging.info(""Final result for disk delete operation: %s"", final_op)\n    if final_op.get(""status"") != ""DONE"":\n      raise ValueError(""Disk delete operation has status %s"",\n                       final_op.get(""status""))\n\n    if final_op.get(""error""):\n      message = ""Delete disk operation resulted in error %s"".format(\n          final_op.get(""error""))\n      logging.error(message)\n      raise ValueError(message)\n\n\ndef get_gcp_identity():\n  identity = util.run_and_output([""gcloud"", ""config"", ""get-value"", ""account""])\n  logging.info(""Current GCP account: %s"", identity)\n  return identity\n\n\ndef main():  # pylint: disable=too-many-locals,too-many-statements\n  logging.getLogger().setLevel(logging.INFO)  # pylint: disable=too-many-locals\n  # create the top-level parser\n  parser = argparse.ArgumentParser(description=""Test Kubeflow E2E."")\n\n  parser.add_argument(\n      ""--test_dir"",\n      default="""",\n      type=str,\n      help=""Directory to use for all the test files. If not set a temporary ""\n      ""directory is created."")\n\n  parser.add_argument(\n      ""--artifacts_dir"",\n      default="""",\n      type=str,\n      help=""Directory to use for artifacts that should be preserved after ""\n      ""the test runs. Defaults to test_dir if not set."")\n\n  parser.add_argument(\n      ""--as_gcloud_user"",\n      dest=""as_gcloud_user"",\n      action=""store_true"",\n      help=(""Impersonate the user corresponding to the gcloud ""\n            ""command with kubectl and ks.""))\n  parser.add_argument(\n      ""--no-as_gcloud_user"", dest=""as_gcloud_user"", action=""store_false"")\n  parser.set_defaults(as_gcloud_user=False)\n\n  # TODO(jlewi): This should not be a global flag.\n  parser.add_argument(\n      ""--project"", default=None, type=str, help=""The project to use."")\n\n  # TODO(jlewi): This should not be a global flag.\n  parser.add_argument(\n      ""--namespace"", default=None, type=str, help=(""The namespace to use.""))\n\n  parser.add_argument(\n      ""--github_token"",\n      default=None,\n      type=str,\n      help=(\n          ""The GitHub API token to use. This is needed since ksonnet uses the ""\n          ""GitHub API and without it we get rate limited. For more info see: ""\n          ""https://github.com/ksonnet/ksonnet/blob/master/docs""\n          ""/troubleshooting.md. Can also be set using environment variable ""\n          ""GITHUB_TOKEN.""))\n\n  parser.add_argument(\n      ""--deploy_name"", default="""", type=str, help=""The name of the deployment."")\n\n  parser.add_argument(\n      ""--workflow_name"", default="""", type=str, help=""The name of the workflow."")\n\n  subparsers = parser.add_subparsers()\n\n  parser_teardown = subparsers.add_parser(\n      ""teardown"", help=""teardown the test infrastructure."")\n\n  parser_teardown.set_defaults(func=teardown)\n\n  parser_tf_serving = subparsers.add_parser(\n      ""deploy_model"", help=""Deploy a TF serving model."")\n\n  parser_tf_serving.set_defaults(func=deploy_model)\n\n  parser_tf_serving.add_argument(\n      ""--params"",\n      default="""",\n      type=str,\n      help=(""Comma separated list of parameters to set on the model.""))\n\n  parser_pytorch_job = subparsers.add_parser(\n      ""deploy_pytorchjob"", help=""Deploy a pytorch-job"")\n\n  parser_pytorch_job.set_defaults(func=deploy_pytorchjob)\n\n  parser_pytorch_job.add_argument(\n      ""--params"",\n      default="""",\n      type=str,\n      help=(""Comma separated list of parameters to set on the model.""))\n\n  parser_argo_job = subparsers.add_parser(""deploy_argo"", help=""Deploy argo"")\n\n  parser_argo_job.set_defaults(func=deploy_argo)\n\n  parser_katib_test = subparsers.add_parser(""test_katib"", help=""Test Katib"")\n\n  parser_katib_test.set_defaults(func=test_katib)\n\n  parser_minikube = subparsers.add_parser(\n      ""deploy_minikube"", help=""Setup a K8s cluster on minikube."")\n\n  parser_minikube.set_defaults(func=deploy_minikube)\n\n  parser_minikube.add_argument(\n      ""--vm_name"", required=True, type=str, help=""The name of the VM to use."")\n\n  parser_minikube.add_argument(\n      ""--zone"",\n      default=""us-east1-d"",\n      type=str,\n      help=""The zone for the cluster."")\n\n  parser_teardown_minikube = subparsers.add_parser(\n      ""teardown_minikube"", help=""Delete the VM running minikube."")\n\n  parser_teardown_minikube.set_defaults(func=teardown_minikube)\n\n  parser_teardown_minikube.add_argument(\n      ""--zone"",\n      default=""us-east1-d"",\n      type=str,\n      help=""The zone for the cluster."")\n\n  parser_teardown_minikube.add_argument(\n      ""--vm_name"", required=True, type=str, help=""The name of the VM to use."")\n\n  args = parser.parse_args()\n\n  if not args.test_dir:\n    logging.info(""--test_dir not set; using a temporary directory."")\n\n    now = datetime.datetime.now()\n    label = ""test_deploy-"" + now.strftime(""%m%d-%H%M-"") + uuid.uuid4().hex[0:4]\n\n    # Create a temporary directory for this test run\n    args.test_dir = os.path.join(tempfile.gettempdir(), label)\n\n  if not args.artifacts_dir:\n    args.artifacts_dir = args.test_dir\n\n  test_log = os.path.join(\n      args.artifacts_dir, ""logs"",\n      ""test_deploy."" + args.func.__name__ + args.deploy_name + "".log.txt"")\n\n  try:\n    os.makedirs(os.path.dirname(test_log))\n  except OSError as exc:  # Python >2.5\n    if exc.errno == errno.EEXIST and os.path.isdir(os.path.dirname(test_log)):\n      pass\n    else:\n      raise\n\n  # TODO(jlewi): We should make this a util routine in kubeflow.testing.util\n  # Setup a logging file handler. This way we can upload the log outputs\n  # to gubernator.\n  root_logger = logging.getLogger()\n\n  file_handler = logging.FileHandler(test_log)\n  root_logger.addHandler(file_handler)\n  # We need to explicitly set the formatter because it will not pick up\n  # the BasicConfig.\n  formatter = logging.Formatter(\n      fmt=(""%(levelname)s|%(asctime)s""\n           ""|%(pathname)s|%(lineno)d| %(message)s""),\n      datefmt=""%Y-%m-%dT%H:%M:%S"")\n  file_handler.setFormatter(formatter)\n  logging.info(""Logging to %s"", test_log)\n  util.run([ks, ""version""])\n\n  util.maybe_activate_service_account()\n  config_file = os.path.expanduser(kube_config.KUBE_CONFIG_DEFAULT_LOCATION)\n\n  # Print out the config to help debugging.\n  output = util.run_and_output([""gcloud"", ""config"", ""config-helper""])\n  logging.info(""gcloud config: \\n%s"", output)\n  wrap_test(args)\n\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
testing/test_deploy_app.py,0,"b'# -*- coding: utf-8 -*-\n# Script to start deployment api and make request to it.\nimport argparse\nimport base64\nimport datetime\nimport logging\nimport os\nimport errno\nimport shutil\nimport subprocess\nimport tempfile\nimport threading\nfrom functools import partial\nfrom multiprocessing import Process\nfrom time import sleep\nfrom google.auth.transport.requests import Request\nfrom googleapiclient import discovery\nfrom oauth2client.client import GoogleCredentials\nfrom prometheus_client import start_http_server, Gauge, Counter\n\nimport requests\nimport yaml\nimport google.auth\nimport google.auth.compute_engine.credentials\nimport google.auth.iam\nimport google.oauth2.credentials\nimport google.oauth2.service_account\nfrom retrying import retry\n\nfrom kubeflow.testing import test_util\n\nFILE_PATH = os.path.dirname(os.path.abspath(__file__))\nSSL_DIR = os.path.join(FILE_PATH, ""sslcert"")\nSSL_BUCKET = \'kubeflow-ci-deploy-cert\'\nIAM_SCOPE = \'https://www.googleapis.com/auth/iam\'\nOAUTH_TOKEN_URI = \'https://www.googleapis.com/oauth2/v4/token\'\nMETHOD = \'GET\'\nSERVICE_HEALTH = Gauge(\n    \'deployment_service_status\',\n    \'0: normal; 1: deployment not successful; 2: service down\')\nPROBER_HEALTH = Gauge(\'prober_health\', \'0: normal; 1: not working\')\nLOADTEST_HEALTH = Gauge(\'loadtest_health\', \'0: normal; 1: not working\')\nLOADTEST_SUCCESS = Gauge(\'loadtest_success\',\n                         \'number of successful requests in current load test\')\nSUCCESS_COUNT = Counter(\'deployment_success_count\',\n                        \'accumulative count of successful deployment\')\nFAILURE_COUNT = Counter(\'deployment_failure_count\',\n                        \'accumulative count of failed deployment\')\nLOADTEST_ZONE = [\n    \'us-central1-a\', \'us-central1-c\', \'us-east1-c\', \'us-east1-d\', \'us-west1-b\'\n]\n\n\nclass requestThread(threading.Thread):\n\n  def __init__(self, target_url, req_data, google_open_id_connect_token):\n    threading.Thread.__init__(self)\n    self.target_url = target_url\n    self.req_data = req_data\n    self.google_open_id_connect_token = google_open_id_connect_token\n\n  def run(self):\n    try:\n      resp = requests.post(\n          ""https://%s/kfctl/e2eDeploy"" % self.target_url,\n          json=self.req_data,\n          headers={\n              \'Authorization\':\n              \'Bearer {}\'.format(self.google_open_id_connect_token)\n          })\n      if resp.status_code != 200:\n        logging.error(""request failed:%s\\n request data:%s""\n                      % (resp, self.req_data))\n        # Mark service down if return code abnormal\n        SERVICE_HEALTH.set(2)\n    except Exception as e:\n      logging.error(e)\n      SERVICE_HEALTH.set(2)\n\n\ndef may_get_env_var(name):\n  env_val = os.getenv(name)\n  if env_val:\n    logging.info(""%s is set"" % name)\n    return env_val\n  else:\n    raise Exception(""%s not set"" % name)\n\n\ndef getZone(args, deployment):\n  if args.mode == ""loadtest"":\n    return LOADTEST_ZONE[int(deployment[-1]) % len(LOADTEST_ZONE)]\n  return args.zone\n\n\ndef get_target_url(args):\n  if args.mode == ""loadtest"":\n    return ""deploy-staging.kubeflow.cloud""\n  if args.mode == ""prober"":\n    return ""deploy.kubeflow.cloud""\n  raise RuntimeError(""No default target url for test mode %s !"" % args.mode)\n\n\ndef prepare_request_data(args, deployment):\n  logging.info(""prepare deploy call data"")\n  with open(\n      os.path.join(FILE_PATH, ""../bootstrap/config/gcp_prototype.yaml""),\n      \'r\') as conf_input:\n    defaultApp = yaml.load(conf_input)[""defaultApp""]\n\n  for param in defaultApp[""parameters""]:\n    if param[""name""] == ""acmeEmail"":\n      param[""value""] = args.email\n    if param[""name""] == ""ipName"":\n      param[""value""] = deployment + ""-ip""\n    if param[""name""] == ""hostname"":\n      param[""value""] = ""%s.endpoints.%s.cloud.goog"" % (deployment, args.project)\n  defaultApp[\'registries\'][0][\'version\'] = args.kfversion\n\n  access_token = util_run(\n      \'gcloud auth application-default print-access-token\'.split(\' \'),\n      cwd=FILE_PATH)\n\n  client_id = may_get_env_var(""CLIENT_ID"")\n  client_secret = may_get_env_var(""CLIENT_SECRET"")\n  credentials = GoogleCredentials.get_application_default()\n  crm = discovery.build(\'cloudresourcemanager\', \'v1\', credentials=credentials)\n  project = crm.projects().get(projectId=args.project).execute()\n  logging.info(""project info: %s"", project)\n  request_data = {\n      ""AppConfig"": defaultApp,\n      ""Apply"": True,\n      ""AutoConfigure"": True,\n      ""ClientId"": base64.b64encode(client_id.encode()).decode(""utf-8""),\n      ""ClientSecret"": base64.b64encode(client_secret.encode()).decode(""utf-8""),\n      ""Cluster"": deployment,\n      ""Email"": args.email,\n      ""IpName"": deployment + \'-ip\',\n      ""Name"": deployment,\n      ""Namespace"": \'kubeflow\',\n      ""Project"": args.project,\n      ""ProjectNumber"": project[""projectNumber""],\n      # service account client id of account: kubeflow-testing@kubeflow-ci.iam.gserviceaccount.com\n      ""SAClientId"": args.sa_client_id,\n      ""Token"": access_token,\n      ""Zone"": getZone(args, deployment)\n  }\n  return request_data\n\n\ndef make_e2e_call(args):\n  if not clean_up_resource(args, set([args.deployment])):\n    raise RuntimeError(""Failed to cleanup resource"")\n  req_data = prepare_request_data(args, args.deployment)\n  resp = requests.post(\n      ""http://kubeflow-controller.%s.svc.cluster.local:8080/kfctl/e2eDeploy"" %\n      args.namespace,\n      json=req_data)\n  if resp.status_code != 200:\n    raise RuntimeError(""deploy request received status code: %s, message: %s"" %\n                       (resp.status_code, resp.text))\n  logging.info(""deploy call done"")\n\n\n# Make 1 deployment request to service url, return if request call successful.\ndef make_prober_call(args, service_account_credentials):\n  logging.info(""start new prober call"")\n  req_data = prepare_request_data(args, args.deployment)\n  google_open_id_connect_token = get_google_open_id_connect_token(\n      service_account_credentials)\n  try:\n    resp = requests.post(\n        ""https://%s/kfctl/e2eDeploy"" % get_target_url(args),\n        json=req_data,\n        headers={\n            \'Authorization\': \'Bearer {}\'.format(google_open_id_connect_token)\n        })\n    if resp.status_code != 200:\n      # Mark service down if return code abnormal\n      SERVICE_HEALTH.set(2)\n      return False\n  except Exception as e:\n    logging.error(e)\n    SERVICE_HEALTH.set(2)\n    return False\n  logging.info(""prober call done"")\n  return True\n\n\n# For each deployment, make a request to service url, return if all requests call successful.\ndef make_loadtest_call(args, service_account_credentials, projects, deployments):\n  logging.info(""start new load test call"")\n  google_open_id_connect_token = get_google_open_id_connect_token(\n      service_account_credentials)\n  threads = []\n  for project in projects:\n    args.project = project\n    for deployment in deployments:\n      req_data = prepare_request_data(args, deployment)\n      threads.append(\n          requestThread(\n              get_target_url(args), req_data, google_open_id_connect_token))\n  for t in threads:\n    t.start()\n  for t in threads:\n    t.join()\n  if SERVICE_HEALTH._value.get() == 2:\n    return False\n  logging.info(""load test call done"")\n  return True\n\n\ndef get_gcs_path(mode, project, deployment):\n  return os.path.join(SSL_BUCKET, mode, project, deployment)\n\n\n# Insert ssl cert into GKE cluster\ndef insert_ssl_cert(args, deployment):\n  logging.info(""Wait till deployment is done and GKE cluster is up"")\n  credentials = GoogleCredentials.get_application_default()\n\n  service = discovery.build(\'deploymentmanager\', \'v2\', credentials=credentials)\n  # Wait up to 10 minutes till GKE cluster up and available.\n  end_time = datetime.datetime.now() + datetime.timedelta(minutes=10)\n  while datetime.datetime.now() < end_time:\n    sleep(5)\n    try:\n      request = service.deployments().get(\n          project=args.project, deployment=deployment)\n      response = request.execute()\n      if response[\'operation\'][\'status\'] != \'DONE\':\n        logging.info(""Deployment running"")\n        continue\n    except Exception as e:\n      logging.info(""Deployment hasn\'t started"")\n      continue\n    break\n\n  ssl_local_dir = os.path.join(SSL_DIR, args.project, deployment)\n  if os.path.exists(ssl_local_dir):\n    shutil.rmtree(ssl_local_dir)\n  os.makedirs(ssl_local_dir)\n  logging.info(""donwload ssl cert and insert to GKE cluster"")\n  try:\n    # TODO: switch to client lib\n    gcs_path = get_gcs_path(args.mode, args.project, deployment)\n    util_run((""gsutil cp gs://%s/* %s"" % (gcs_path, ssl_local_dir)).split(\' \'))\n  except Exception:\n    logging.warning(""ssl cert for %s doesn\'t exist in gcs"" % args.mode)\n    # clean up local dir\n    shutil.rmtree(ssl_local_dir)\n    return True\n  try:\n    create_secret(args, deployment, ssl_local_dir)\n  except Exception as e:\n    logging.error(e)\n    return False\n  return True\n\n\n@retry(wait_fixed=2000, stop_max_delay=15000)\ndef create_secret(args, deployment, ssl_local_dir):\n  util_run(\n      (""gcloud container clusters get-credentials %s --zone %s --project %s"" %\n       (deployment, getZone(args, deployment), args.project)).split(\' \'))\n  util_run((""kubectl create -f %s"" % ssl_local_dir).split(\' \'))\n\n\n# deployments: set(string) which contains all deployment names in current test round.\ndef check_deploy_status(args, deployments):\n  num_deployments = len(deployments)\n  logging.info(""check deployment status"")\n  service_account_credentials = get_service_account_credentials(""CLIENT_ID"")\n\n  google_open_id_connect_token = get_google_open_id_connect_token(\n      service_account_credentials)\n  # Wait up to 30 minutes for IAP access test.\n  num_req = 0\n  end_time = datetime.datetime.now() + datetime.timedelta(\n      minutes=args.iap_wait_min)\n  success_deploy = set()\n  while datetime.datetime.now() < end_time and len(deployments) > 0:\n    sleep(10)\n    num_req += 1\n\n    for deployment in deployments:\n      url = ""https://%s.endpoints.%s.cloud.goog"" % (deployment, args.project)\n      logging.info(""Trying url: %s"", url)\n      try:\n        resp = requests.request(\n            METHOD,\n            url,\n            headers={\n                \'Authorization\':\n                \'Bearer {}\'.format(google_open_id_connect_token)\n            },\n            verify=False)\n        if resp.status_code == 200:\n          success_deploy.add(deployment)\n          logging.info(""IAP is ready for %s!"", url)\n        else:\n          logging.info(\n              ""%s: IAP not ready, request number: %s"" % (deployment, num_req))\n      except Exception:\n        logging.info(""%s: IAP not ready, exception caught, request number: %s"" %\n                     (deployment, num_req))\n    deployments = deployments.difference(success_deploy)\n\n  for deployment in success_deploy:\n    try:\n      ssl_local_dir = os.path.join(SSL_DIR, args.project, deployment)\n      try:\n        os.makedirs(ssl_local_dir)\n      except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(ssl_local_dir):\n          pass\n        else:\n          raise\n      util_run((\n          ""gcloud container clusters get-credentials %s --zone %s --project %s""\n          % (deployment, getZone(args, deployment), args.project)).split(\' \'))\n      for sec in [""envoy-ingress-tls"", ""letsencrypt-prod-secret""]:\n        sec_data = util_run(\n            (""kubectl get secret %s -n kubeflow -o yaml"" % sec).split(\' \'))\n        with open(os.path.join(ssl_local_dir, sec + "".yaml""),\n                  \'w+\') as sec_file:\n          sec_file.write(sec_data)\n          sec_file.close()\n      # TODO: switch to client lib\n      gcs_path = get_gcs_path(args.mode, args.project, deployment)\n      util_run(\n          (""gsutil cp %s/* gs://%s/"" % (ssl_local_dir, gcs_path)).split(\' \'))\n    except Exception:\n      logging.error(""%s: failed uploading ssl cert"" % deployment)\n\n  # return number of successful deployments\n  return num_deployments - len(deployments)\n\n\ndef get_service_account_credentials(client_id_key):\n  # Figure out what environment we\'re running in and get some preliminary\n  # information about the service account.\n  credentials, _ = google.auth.default(scopes=[IAM_SCOPE])\n  if isinstance(credentials, google.oauth2.credentials.Credentials):\n    raise Exception(\'make_iap_request is only supported for service \'\n                    \'accounts.\')\n\n  # For service account\'s using the Compute Engine metadata service,\n  # service_account_email isn\'t available until refresh is called.\n  credentials.refresh(Request())\n\n  signer_email = credentials.service_account_email\n  if isinstance(credentials,\n                google.auth.compute_engine.credentials.Credentials):\n    signer = google.auth.iam.Signer(Request(), credentials, signer_email)\n  else:\n    # A Signer object can sign a JWT using the service account\'s key.\n    signer = credentials.signer\n\n  # Construct OAuth 2.0 service account credentials using the signer\n  # and email acquired from the bootstrap credentials.\n  return google.oauth2.service_account.Credentials(\n      signer,\n      signer_email,\n      token_uri=OAUTH_TOKEN_URI,\n      additional_claims={\'target_audience\': may_get_env_var(client_id_key)})\n\n\ndef get_google_open_id_connect_token(service_account_credentials):\n  service_account_jwt = (\n      service_account_credentials._make_authorization_grant_assertion())\n  request = google.auth.transport.requests.Request()\n  body = {\n      \'assertion\': service_account_jwt,\n      \'grant_type\': google.oauth2._client._JWT_GRANT_TYPE,\n  }\n  token_response = google.oauth2._client._token_endpoint_request(\n      request, OAUTH_TOKEN_URI, body)\n  return token_response[\'id_token\']\n\n\ndef delete_gcloud_resource(args, keyword, filter=\'\', dlt_params=[]):\n  # TODO: switch to client lib\n  get_cmd = \'gcloud compute %s list --project=%s --format=""value(name)""\' % (\n      keyword, args.project)\n  elements = util_run(get_cmd + filter, shell=True)\n  for element in elements.split(\'\\n\'):\n    dlt_cmd = \'gcloud compute %s delete -q --project=%s %s\' % (\n        keyword, args.project, element)\n    try:\n      util_run(dlt_cmd.split(\' \') + dlt_params)\n    except Exception as e:\n      logging.warning(\'Cannot remove %s %s\' % (keyword, element))\n      logging.warning(e)\n\n\ndef clean_up_resource(args, deployments):\n  """"""Clean up deployment / app config from previous test\n\n  Args:\n    args: The args from ArgParse.\n    deployments set(string): which contains all deployment names in current test round.\n  Returns:\n    bool: True if cleanup is done\n  """"""\n  logging.info(\n      ""Clean up project resource (backend service and deployment)"")\n\n  # Will reuse source repo for continuous tests\n  # Within 7 days after repo deleted, source repo won\'t allow recreation with same name\n\n  # Delete deployment\n  credentials = GoogleCredentials.get_application_default()\n  service = discovery.build(\'deploymentmanager\', \'v2\', credentials=credentials)\n  delete_done = False\n  for deployment in deployments:\n    try:\n      request = service.deployments().delete(\n          project=args.project, deployment=deployment)\n      request.execute()\n    except Exception as e:\n      logging.info(""Deployment doesn\'t exist, continue"")\n  # wait up to 10 minutes till delete finish.\n  end_time = datetime.datetime.now() + datetime.timedelta(minutes=10)\n  while datetime.datetime.now() < end_time:\n    sleep(10)\n    try:\n      request = service.deployments().list(project=args.project)\n      response = request.execute()\n      if (\'deployments\' not in response) or (len(deployments & set(\n          d[\'name\'] for d in response[\'deployments\'])) == 0):\n        delete_done = True\n        break\n    except Exception:\n      logging.info(""Failed listing current deployments, retry in 10 seconds"")\n\n  # Delete forwarding-rules\n  delete_gcloud_resource(args, \'forwarding-rules\', dlt_params=[\'--global\'])\n  # Delete target-http-proxies\n  delete_gcloud_resource(args, \'target-http-proxies\')\n  # Delete target-http-proxies\n  delete_gcloud_resource(args, \'target-https-proxies\')\n  # Delete url-maps\n  delete_gcloud_resource(args, \'url-maps\')\n  # Delete backend-services\n  delete_gcloud_resource(args, \'backend-services\', dlt_params=[\'--global\'])\n  # Delete instance-groups\n  for zone in LOADTEST_ZONE:\n    delete_gcloud_resource(\n        args,\n        \'instance-groups unmanaged\',\n        filter=\' --filter=INSTANCES:0\',\n        dlt_params=[\'--zone=\' + zone])\n  # Delete ssl-certificates\n  delete_gcloud_resource(args, \'ssl-certificates\')\n  # Delete health-checks\n  delete_gcloud_resource(args, \'health-checks\')\n\n  if not delete_done:\n    logging.error(""failed to clean up resources for project %s deployments %s"",\n                  args.project, deployments)\n  return delete_done\n\n\ndef util_run(command,\n             cwd=None,\n             env=None,\n             shell=False,\n             polling_interval=datetime.timedelta(seconds=1)):\n  """"""Run a subprocess.\n\n  Any subprocess output is emitted through the logging modules.\n\n  Returns:\n    output: A string containing the output.\n  """"""\n  logging.info(""Running: %s \\ncwd=%s"", "" "".join(command), cwd)\n\n  if not env:\n    env = os.environ\n  else:\n    keys = sorted(env.keys())\n\n    lines = []\n    for k in keys:\n      lines.append(""{0}={1}"".format(k, env[k]))\n    logging.info(""Running: Environment:\\n%s"", ""\\n"".join(lines))\n\n  process = subprocess.Popen(\n      command,\n      cwd=cwd,\n      env=env,\n      shell=shell,\n      stdout=subprocess.PIPE,\n      stderr=subprocess.STDOUT)\n\n  # logging.info(""Subprocess output:\\n"")\n  output = []\n  while process.poll() is None:\n    process.stdout.flush()\n    for line in iter(process.stdout.readline, \'\'):\n      output.append(line.strip(\'\\n\'))\n      # logging.info(line.strip())\n\n    sleep(polling_interval.total_seconds())\n\n  process.stdout.flush()\n  for line in iter(process.stdout.readline, b\'\'):\n    output.append(line.strip(\'\\n\'))\n    # logging.info(line.strip())\n\n  if process.returncode != 0:\n    raise subprocess.CalledProcessError(\n        process.returncode, ""cmd: {0} exited with code {1}"".format(\n            "" "".join(command), process.returncode), ""\\n"".join(output))\n\n  return ""\\n"".join(output)\n\ndef clean_up_project_resource(args, projects, deployments):\n  proc = []\n  for project in projects:\n    args.project = project\n    p = Process(target = partial(clean_up_resource, args, deployments))\n    p.start()\n    proc.append(p)\n\n  for p in proc:\n    p.join()\n\ndef upload_load_test_ssl_cert(args, projects, deployments):\n  for project in projects:\n    args.project = project\n    for deployment in deployments:\n      insert_ssl_cert(args, deployment)\n\ndef check_load_test_results(args, projects, deployments):\n  num_deployments = len(deployments)\n  total_success = 0\n  # deadline for checking all the results.\n  end_time = datetime.datetime.now() + datetime.timedelta(\n      minutes=args.iap_wait_min)\n  for project in projects:\n    args.project = project\n    # set the deadline for each check.\n    now = datetime.datetime.now()\n    if end_time < now:\n      args.iap_wait_min = 1\n    else:\n      delta = end_time - now\n      args.iap_wait_min = delta.seconds / 60 + 1\n    num_success = check_deploy_status(args, deployments)\n    total_success += num_success\n    logging.info(""%s out of %s deployments succeed for project %s"",\n                 num_success, num_deployments, project)\n    # We only wait 1 minute for subsequent checks because we already waited forIAP since we already\n    args.iap_wait_min = 1\n    LOADTEST_SUCCESS.set(num_success)\n    if num_success == num_deployments:\n      SUCCESS_COUNT.inc()\n    else:\n      FAILURE_COUNT.inc()\n  logging.info(""%s out of %s deployments succeed in total"",\n                total_success, num_deployments * len(projects))\n\n\ndef run_load_test(args):\n  num_deployments = args.number_deployments_per_project\n  num_projects = args.number_projects\n  start_http_server(8000)\n  LOADTEST_SUCCESS.set(num_deployments)\n  LOADTEST_HEALTH.set(0)\n  service_account_credentials = get_service_account_credentials(\n      ""SERVICE_CLIENT_ID"")\n  deployments = set(\n      [\'kubeflow\' + str(i) for i in range(1, num_deployments + 1)])\n  projects = [args.project_prefix + str(i)\n             for i in range(1, num_projects + 1)]\n  logging.info(""deployments: %s"" % deployments)\n  logging.info(""projects: %s"" % projects)\n\n  clean_up_project_resource(args, projects, deployments)\n\n  if not make_loadtest_call(\n    args, service_account_credentials, projects, deployments):\n    LOADTEST_SUCCESS.set(0)\n    FAILURE_COUNT.inc()\n    logging.error(""load test request failed"")\n    return\n\n  upload_load_test_ssl_cert(args, projects, deployments)\n\n  check_load_test_results(args, projects, deployments)\n\n  clean_up_project_resource(args, projects, deployments)\n\n\n\n\ndef run_e2e_test(args):\n  sleep(args.wait_sec)\n  make_e2e_call(args)\n  insert_ssl_cert(args, args.deployment)\n  if not check_deploy_status(args, set([args.deployment])):\n    raise RuntimeError(""IAP endpoint not ready after 30 minutes, time out..."")\n  logging.info(""Test finished."")\n\n\ndef wrap_test(args):\n  """"""Run the tests given by args.func and output artifacts as necessary.\n  """"""\n  test_name = ""bootstrapper""\n  test_case = test_util.TestCase()\n  test_case.class_name = ""KubeFlow""\n  test_case.name = args.workflow_name + ""-"" + test_name\n  try:\n\n    def run():\n      args.func(args)\n\n    test_util.wrap_test(run, test_case)\n  finally:\n    # Test grid has problems with underscores in the name.\n    # https://github.com/kubeflow/kubeflow/issues/631\n    # TestGrid currently uses the regex junit_(^_)*.xml so we only\n    # want one underscore after junit.\n    junit_name = test_case.name.replace(""_"", ""-"")\n    junit_path = os.path.join(args.artifacts_dir,\n                              ""junit_{0}.xml"".format(junit_name))\n    logging.info(""Writing test results to %s"", junit_path)\n    test_util.create_junit_xml_file([test_case], junit_path)\n\n\n# Clone repos to tmp folder and build docker images\ndef main(unparsed_args=None):\n  parser = argparse.ArgumentParser(\n      description=""Start deployment api and make request to it."")\n\n  parser.add_argument(\n      ""--deployment"",\n      default=""periodic-test"",\n      type=str,\n      help=""Deployment name."")\n  parser.add_argument(\n      ""--email"",\n      default=""google-kubeflow-support@google.com"",\n      type=str,\n      help=""Email used during e2e test"")\n  parser.add_argument(\n      ""--project"",\n      default=""kubeflow-ci-deployment"",\n      type=str,\n      help=""e2e test project id"")\n  parser.add_argument(\n      ""--project_prefix"",\n      default=""kf-gcp-deploy-test"",\n      type=str,\n      help=""project prefix for load test"")\n  parser.add_argument(\n      ""--number_projects"",\n      default=""2"",\n      type=int,\n      help=""number of projects used in load test"")\n  parser.add_argument(\n      ""--number_deployments_per_project"",\n      default=""5"",\n      type=int,\n      help=""number of deployments per project used in load test"")\n  parser.add_argument(\n      ""--namespace"",\n      default="""",\n      type=str,\n      help=""namespace where deployment service is running"")\n  parser.add_argument(\n      ""--wait_sec"", default=120, type=int, help=""oauth client secret"")\n  parser.add_argument(\n      ""--iap_wait_min"", default=30, type=int, help=""minutes to wait for IAP"")\n  parser.add_argument(\n      ""--zone"", default=""us-east1-d"", type=str, help=""GKE cluster zone"")\n  parser.add_argument(\n      ""--sa_client_id"",\n      default=""111670663612681935351"",\n      type=str,\n      help=""Service account client id"")\n  parser.add_argument(\n      ""--kfversion"",\n      default=""v0.4.1"",\n      type=str,\n      help=""Service account client id"")\n  parser.add_argument(\n      ""--mode"",\n      default=""e2e"",\n      type=str,\n      help=""offer three test mode: e2e, prober, and loadtest"")\n  # args for e2e test\n  parser.set_defaults(func=run_e2e_test)\n  parser.add_argument(\n      ""--artifacts_dir"",\n      default="""",\n      type=str,\n      help=""Directory to use for artifacts that should be preserved after ""\n      ""the test runs. Defaults to test_dir if not set."")\n  parser.add_argument(\n      ""--workflow_name"",\n      default=""deployapp"",\n      type=str,\n      help=""The name of the workflow."")\n\n  args = parser.parse_args(args=unparsed_args)\n\n  if not args.artifacts_dir:\n    args.artifacts_dir = tempfile.gettempdir()\n\n  util_run(\n      (\'gcloud auth activate-service-account --key-file=\' +\n       may_get_env_var(""GOOGLE_APPLICATION_CREDENTIALS"")).split(\' \'),\n      cwd=FILE_PATH)\n  if args.mode == ""e2e"":\n    wrap_test(args)\n\n  if args.mode == ""prober"":\n    start_http_server(8000)\n    SERVICE_HEALTH.set(0)\n    PROBER_HEALTH.set(0)\n    service_account_credentials = get_service_account_credentials(\n        ""SERVICE_CLIENT_ID"")\n    while True:\n      sleep(args.wait_sec)\n      if not clean_up_resource(args, set([args.deployment])):\n        PROBER_HEALTH.set(1)\n        FAILURE_COUNT.inc()\n        logging.error(\n            ""request cleanup failed, retry in %s seconds"" % args.wait_sec)\n        continue\n      PROBER_HEALTH.set(0)\n      if make_prober_call(args, service_account_credentials):\n        if insert_ssl_cert(args, args.deployment):\n          PROBER_HEALTH.set(0)\n        else:\n          PROBER_HEALTH.set(1)\n          FAILURE_COUNT.inc()\n          logging.error(""request insert_ssl_cert failed, retry in %s seconds"" %\n                        args.wait_sec)\n          continue\n        if check_deploy_status(args, set([args.deployment])):\n          SERVICE_HEALTH.set(0)\n          SUCCESS_COUNT.inc()\n        else:\n          SERVICE_HEALTH.set(1)\n          FAILURE_COUNT.inc()\n      else:\n        SERVICE_HEALTH.set(2)\n        FAILURE_COUNT.inc()\n        logging.error(\n            ""prober request failed, retry in %s seconds"" % args.wait_sec)\n\n  if args.mode == ""loadtest"":\n    run_load_test(args)\n\n\nif __name__ == \'__main__\':\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger(\'googleapiclient.discovery_cache\').setLevel(logging.ERROR)\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
testing/test_deploy_test.py,0,"b'# -*- coding: utf-8 -*-\nimport tempfile\nimport unittest\nimport yaml\n\nfrom testing import test_deploy\n\n\nclass TestDeploy(unittest.TestCase):\n\n  def testModifyMinikubeConfig(self):\n    """"""Test modeify_minikube_config""""""\n\n    config_path = None\n    with tempfile.NamedTemporaryFile(delete=False) as hf:\n      config_path = hf.name\n      hf.write(""""""apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority: /home/jlewi/.minikube/ca.crt\n    server: https://10.240.0.18:8443\n  name: minikube\ncontexts:\n- context:\n    cluster: minikube\n    user: minikube\n  name: minikube\ncurrent-context: minikube\nkind: Config\npreferences: {}\nusers:\n- name: minikube\n  user:\n    as-user-extra: {}\n    client-certificate: /home/jlewi/.minikube/client.crt\n    client-key: /home/jlewi/.minikube/client.key\n"""""")\n\n    test_deploy.modify_minikube_config(config_path, ""/test/.minikube"")\n\n    # Load the output.\n    with open(config_path) as hf:\n      config = yaml.load(hf)\n\n    expected = {\n        ""apiVersion"":\n        ""v1"",\n        ""clusters"": [{\n            ""cluster"": {\n                ""certificate-authority"": ""/test/.minikube/ca.crt"",\n                ""server"": ""https://10.240.0.18:8443""\n            },\n            ""name"": ""minikube""\n        }],\n        ""contexts"": [{\n            ""context"": {\n                ""cluster"": ""minikube"",\n                ""user"": ""minikube""\n            },\n            ""name"": ""minikube""\n        }],\n        ""current-context"":\n        ""minikube"",\n        ""kind"":\n        ""Config"",\n        ""preferences"": {},\n        ""users"": [{\n            ""name"": ""minikube"",\n            ""user"": {\n                ""as-user-extra"": {},\n                ""client-certificate"": ""/test/.minikube/client.crt"",\n                ""client-key"": ""/test/.minikube/client.key""\n            }\n        }]\n    }\n\n    self.assertDictEqual(expected, config)\n\n\nif __name__ == ""__main__"":\n  unittest.main()\n'"
testing/test_flake8.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Run flake8 tests\n\nThis test goes through all Python files in the specified test_files_dirs\ndirectories and runs flake8 <filename> and reports the results\n\nExample invocation\n\npython -m testing.test_flake8 --test_files_dirs=/kubeflow/application/tests,/kubeflow/common/tests,/kubeflow/jupyter/tests,/kubeflow/iap/tests,/kubeflow/gcp/tests,/kubeflow/tensorboard/tests,/kubeflow/examples/tests,/kubeflow/metacontroller/tests,/kubeflow/profiles/tests,/kubeflow/tf-training/tests  # noqa: E501\n\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport logging\nimport os\n\nfrom kubeflow.testing import test_helper, util\n\nFLAKE8_OPTS = """"""--count --select=E901,E999,F821,F822,F823 --show-source\n                 --statistics"""""".split()\n\n# Test only files which end in \'.py\' or have no suffix\n\n\ndef should_test(file_path):\n  _, ext = os.path.splitext(file_path.lower())\n  return ext in (\'.py\', \'\')\n\n\ndef run(test_files_dirs, test_case):\n  # Go through each Python file in test_files_dirs and run flake8\n  for test_files_dir in test_files_dirs:\n    for root, _, files in os.walk(test_files_dir):\n      for test_file in files:\n        full_path = os.path.join(root, test_file)\n        assert root == os.path.dirname(full_path)\n        if should_test(full_path):\n          logging.info(""Testing: %s"", test_file)\n          try:\n            output = util.run([\'flake8\', full_path] + FLAKE8_OPTS, cwd=root)\n            try:\n              parsed = json.loads(output)\n            except AttributeError:\n              logging.error(\n                  ""Output of flake8 could not be parsed as json; ""\n                  ""output: %s"", output)\n              parsed = {}\n\n            if not hasattr(parsed, ""get""):\n              # Legacy style tests emit true rather than a json object.\n              # Parsing the string as json converts it to a bool so we\n              # just use parsed as test_passed\n              # Old style tests actually use std.assert so flake8 will\n              # actually return an error in the case the test did\n              # not pass.\n              logging.warn(\n                  ""flake8 is using old style and not emitting an object. ""\n                  ""Result was: %s. Output will be treated as a boolean"", output)\n              test_passed = parsed\n            else:\n              test_passed = parsed.get(""pass"", False)\n\n            if not test_passed:\n              msg = \'{} test failed\'.format(test_file)\n              test_case.add_failure_info(msg)\n              logging.error(\n                  \'{}. See Subprocess output for details.\'.format(msg))\n          except Exception as e:\n            msg = \'{} test failed\'.format(test_file)\n            test_case.add_failure_info(msg)\n            logging.error(\'{} with exception {}. See Subprocess output for \'\n                          \'details.\'.format(msg, e))\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--test_files_dirs"",\n      default=""."",\n      type=str,\n      help=""Comma separated directories containing Python files"")\n  args, _ = parser.parse_known_args()\n  return args\n\n\ndef test_flake8(test_case):  # pylint: disable=redefined-outer-name\n  args = parse_args()\n  if not args.test_files_dirs:\n    raise ValueError(\'--test_files_dirs needs to be set\')\n  run(args.test_files_dirs.split(\',\'), test_case)\n\n\nif __name__ == ""__main__"":\n  test_case = test_helper.TestCase(name=\'test_flake8\', test_func=test_flake8)\n  test_suite = test_helper.init(\n      name=\'flake8_test_suite\', test_cases=[test_case])\n  test_suite.run()\n'"
testing/test_jsonnet.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Run jsonnet tests\n\nThis test goes through all jsonnet files specified by the test_files_dirs\ndirectory and runs jsonnet eval <filename> and reports the results\n\nExample invocation\n\npython -m testing.test_jsonnet --test_files_dirs=/kubeflow/application/tests,/kubeflow/common/tests,/kubeflow/jupyter/tests,/kubeflow/iap/tests,/kubeflow/gcp/tests,/kubeflow/tensorboard/tests,/kubeflow/examples/tests,/kubeflow/metacontroller/tests,/kubeflow/profiles/tests,/kubeflow/tf-training/tests,/kubeflow/kubebench/tests --artifacts_dir=/tmp/artifacts  # noqa: E501\n\nTODO(jlewi): Should we use pytest to create a parameterized test with respect\nto directory? See https://docs.pytest.org/en/latest/example/parametrize.html\n""""""\n\nfrom __future__ import print_function\n\nimport logging\nimport json\nimport os\n\nimport argparse\n\nfrom kubeflow.testing import test_helper, util\n\n\n# We should test all files which end in .jsonnet or .libsonnet\n# except ksonnet prototype definitions - they require additional\n# dependencies\ndef should_test(file_path):\n  _, ext = os.path.splitext(file_path)\n  if ext not in (\'.jsonnet\', \'.libsonnet\'):\n    return False\n  parts = file_path.split(\'/\')\n  if len(parts) < 2:\n    raise ValueError(\'Invalid file : {}\'.format(file_path))\n  return parts[-2] != \'prototypes\'\n\n\ndef is_excluded(file_name, exclude_dirs):\n  for exclude_dir in exclude_dirs:\n    if file_name.startswith(exclude_dir):\n      return True\n  return False\n\n\ndef run(test_files_dirs, jsonnet_path_args, exclude_dirs, test_case):\n  # Go through each jsonnet file in test_files_dirs and run jsonnet eval\n  for test_files_dir in test_files_dirs:\n    for root, _, files in os.walk(test_files_dir):\n      if is_excluded(root, exclude_dirs):\n        logging.info(""Skipping %s"", root)\n        continue\n\n      for test_file in files:\n        full_path = os.path.join(root, test_file)\n        if should_test(full_path):\n          logging.info(""Testing: %s"", test_file)\n          try:\n            output = util.run(\n                [\'jsonnet\', \'eval\', full_path] + jsonnet_path_args,\n                cwd=os.path.dirname(full_path))\n            try:\n              parsed = json.loads(output)\n            except AttributeError:\n              logging.error(\n                  ""Output of jsonnet eval could not be parsed as json; ""\n                  ""output: %s"", output)\n              parsed = {}\n\n            if not hasattr(parsed, ""get""):\n              # Legacy style tests emit true rather than a json object.\n              # Parsing the string as json converts it to a bool so we\n              # just use parsed as test_passed\n              # Old style tests actually use std.assert so jsonnet eval\n              # will actually return an error in the case the test didn\'t\n              # pass.\n              logging.warn(\n                  ""jsonnet test is using old style and not emitting an object. ""\n                  ""Result was: %s. Output will be treated as a boolean"", output)\n              test_passed = parsed\n            else:\n              test_passed = parsed.get(""pass"", False)\n\n            if not test_passed:\n              test_case.add_failure_info(\'{} test failed\'.format(test_file))\n              logging.error(\n                  \'%s test failed. See Subprocess output for details.\',\n                  test_file)\n          except Exception as e:\n            test_case.add_failure_info(\'{} test failed\'.format(test_file))\n            logging.error(\n                \'%s test failed with exception %s. \'\n                \'See Subprocess output for details.\', e, test_file)\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--test_files_dirs"",\n      default="""",\n      type=str,\n      help=""Comma separated directories where test jsonnet test files are ""\n      ""stored"")\n  parser.add_argument(\n      ""--jsonnet_path_dirs"",\n      default="""",\n      type=str,\n      help=""Comma separated directories used by jsonnet to find additional ""\n      ""libraries"")\n\n  parser.add_argument(\n      ""--exclude_dirs"",\n      default="""",\n      type=str,\n      help=""Comma separated directories which should be excluded from the test"")\n\n  args, _ = parser.parse_known_args()\n  return args\n\n\ndef test_jsonnet(test_case):  # pylint: disable=redefined-outer-name\n  args = parse_args()\n\n  if not args.test_files_dirs:\n    raise ValueError(\'--test_files_dirs needs to be set\')\n\n  test_files_dirs = args.test_files_dirs.split(\',\')\n\n  jsonnet_path_args = []\n  if len(args.jsonnet_path_dirs) > 0:\n    for jsonnet_path_dir in args.jsonnet_path_dirs.split(\',\'):\n      jsonnet_path_args.append(\'--jpath\')\n      jsonnet_path_args.append(jsonnet_path_dir)\n\n  exclude_dirs = []\n  if args.exclude_dirs:\n    exclude_dirs = args.exclude_dirs.split(\',\')\n\n  run(test_files_dirs, jsonnet_path_args, exclude_dirs, test_case)\n\n\nif __name__ == ""__main__"":\n  # TODO(https://github.com/kubeflow/kubeflow/issues/4159):\n  # quick hack to disable the failing test.\n  print(""Error: skipping test_jsonnet because it is failing ""\n        ""https://github.com/kubeflow/kubeflow/issues/4159"")\n  # test_case = test_helper.TestCase(name=\'test_jsonnet\', test_func=test_jsonnet)\n  # test_suite = test_helper.init(\n  #    name=\'jsonnet_test_suite\', test_cases=[test_case])\n  # test_suite.run()\n'"
testing/test_jwa.py,0,"b'import datetime\nimport logging\nimport os\nfrom time import sleep\nfrom urllib import parse\n\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\n\nimport pytest\nfrom seleniumwire import webdriver\n\nfrom . import auth\n\nKUBEFLOW_URL = os.environ.get(""KUBEFLOW_URL"", ""http://localhost:8081"")\nKF_NAMESPACE = os.environ.get(""KF_NAMESPACE"", ""kimwnasptd"")\nAUTH_METHOD = os.environ.get(""AUTH_METHOD"", ""dex"")  # dex, iap\n\nSTATE_READY = ""READY""\nSTATE_WAITING = ""WAITING""\nSTATE_WARNING = ""WARNING""\nSTATE_ERROR = ""ERROR""\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=(""%(levelname)s | %(lineno)d | E2E TEST | %(message)s""),\n)\n\n\ndef login_to_kubeflow(driver):\n    if AUTH_METHOD == ""dex"":\n        username = os.environ.get(""DEX_USERNAME"", ""kimwnasptd@kubeflow.org"")\n        password = os.environ.get(""DEX_PASSWORD"", ""asdf"")\n        auth.login_to_kubeflow_dex(driver, KUBEFLOW_URL, username, password)\n    elif AUTH_METHOD == ""iap"":\n        auth.login_to_kubeflow_iap(driver, KUBEFLOW_URL)\n    else:\n        logging.warning(""No authentication method for: \'{AUTH_METHOD}\'"")\n\n\n@pytest.fixture(scope=""class"")\ndef tests_setup(request):\n    """"""\n    Create a driver to use for all the test cases. Also handle to login to\n    kubeflow\n    """"""\n    logging.info(""Initializing the Selenium Driver"")\n    driver = webdriver.Firefox()\n    driver.maximize_window()\n\n    # use the same driver for all the test cases\n    request.cls.driver = driver\n\n    login_to_kubeflow(driver)\n\n    # Run the test cases\n    yield driver\n    # After all the test cases where run\n\n    logging.info(""Closing the Selenium Driver"")\n    driver.close()\n\n\ndef create_selector_query(selectors_list):\n    """"""\n    Get a list of selectors and append them to a whole one liner that will get\n    the element by combining the querySelector function with the provided\n    selectors\n\n    If the value `shadowRoot` is given, then the shadow-root element will be\n    used in the query\n    """"""\n    query = ""return document""\n    for selector in selectors_list:\n        if selector == ""shadowRoot"":\n            query += "".shadowRoot""\n            continue\n\n        query += f"".querySelector(\'{selector}\')""\n\n    return query\n\n\n# Return the Status depending on the material icon\ndef icon_to_status(icon_text):\n    if icon_text == ""check_circle"":\n        return STATE_READY\n\n    if icon_text == ""clear"":\n        return STATE_ERROR\n\n    if icon_text == ""warning"":\n        return STATE_WARNING\n\n    raise ValueError(f""Got unexpected state icon \'{icon_text}\'"")\n\n\n# --- Page Classes\nclass CentralDashboard:\n    driver = None\n    jwa = None\n\n    iframe_selector = [\n        ""main-page"",\n        ""shadowRoot"",\n        ""app-drawer-layout"",\n        ""app-header-layout"",\n        ""main"",\n        ""neon-animated-pages"",\n        \'neon-animatable[page=""iframe""]\',\n        ""iframe-container"",\n        ""shadowRoot"",\n        ""#iframe"",\n    ]\n\n    namespace_button_selector = [\n        ""main-page"",\n        ""shadowRoot"",\n        ""app-drawer-layout"",\n        ""app-header-layout"",\n        ""app-header"",\n        ""app-toolbar"",\n        ""namespace-selector"",\n        ""shadowRoot"",\n        ""paper-menu-button"",\n        \'paper-button[id=""dropdown-trigger""]\',\n    ]\n\n    namespaces_list_selector = [\n        ""main-page"",\n        ""shadowRoot"",\n        ""app-drawer-layout"",\n        ""app-header-layout"",\n        ""app-header"",\n        ""app-toolbar"",\n        ""namespace-selector"",\n        ""shadowRoot"",\n        ""paper-menu-button"",\n        ""paper-listbox"",\n    ]\n\n    def __init__(self, driver):\n        self.driver = driver\n        self.jwa = JWA(self)\n\n    def switch_selenium_context(self):\n        self.driver.switch_to.default_content()\n\n    def navigate_to_home(self):\n        logging.info(""Navigating to CentralDashboard"")\n        self.driver.get(parse.urljoin(KUBEFLOW_URL, ""/""))\n\n    def get_iframe(self):\n        self.switch_selenium_context()\n        iframe_script = create_selector_query(self.iframe_selector)\n        return self.driver.execute_script(iframe_script)\n\n    def select_namespace(self, namespace):\n        logging.info(f""Switching to namespace \'{namespace}\'"")\n\n        # Open the namespace select\n        ns_btn = self.driver.execute_script(\n            create_selector_query(self.namespace_button_selector)\n        )\n        ns_btn.click()\n\n        # Choose the namespace\n        ns_list = self.driver.execute_script(\n            create_selector_query(self.namespaces_list_selector)\n        )\n\n        namespaces = ns_list.find_elements_by_tag_name(""paper-item"")\n        for ns in namespaces:\n            if ns.text == namespace:\n                self.driver.execute_script(""arguments[0].click()"", ns)\n                logging.info(f""Switched to {namespace}"")\n                return\n\n        logging.error(f""Couldn\'t locate namespace \'{namespace}\'"")\n        assert False\n\n\nclass NotebookRow:\n    STATUS_COL = 0\n    NAME_COL = 1\n    IMAGE_COL = 2\n\n    def __init__(self, row):\n        self.row = row\n        self.tds = row.find_elements_by_tag_name(""td"")\n\n    def get_status(self):\n        """"""\n        Return a STATE_ value depending on the html element for the status\n        """"""\n        status_elem = self.tds[self.STATUS_COL]\n        status_icon = status_elem.find_element_by_tag(""mat-icon"")\n        if status_icon is None:\n            # Check for spinner\n            if status_elem.find_element_by_tag(""mat-spinner"") is None:\n                raise ValueError(""Status should be a spinner"")\n\n            return STATE_WAITING\n\n        return icon_to_status(status_icon)\n\n    def get_name(self):\n        """"""\n        Return the name of the Notebook from the corresponding td\n        """"""\n        return self.tds[self.NAME_COL].text\n\n    def get_image(self):\n        """"""\n        Return the image of the Notebook from the corresponding td\n        """"""\n        return self.tds[self.IMAGE_COL].text\n\n\nclass JWAIndexPage:\n    nb_table_selector = [\n        ""app-root"",\n        ""app-main-table-router"",\n        ""app-main-table"",\n        ""div"",\n        ""app-resource-table"",\n    ]\n\n    def __init__(self, driver):\n        self.driver = driver\n\n    def navigate(self):\n        logging.info(""Navigating to JWA\'s index page"")\n        self.driver.get(parse.urljoin(KUBEFLOW_URL, ""/_/jupyter/""))\n\n    def appeared(self):\n        try:\n            WebDriverWait(self.driver, 3).until(\n                EC.presence_of_element_located(\n                    (By.TAG_NAME, ""app-resource-table"")\n                )\n            )\n\n            return True\n        except TimeoutException:\n            logging.warning(""Couldn\'t locate the Notebooks Table"")\n            return False\n\n        return False\n\n    def get_notebook_rows(self):\n        table = self.driver.execute_script(\n            create_selector_query(\n                self.nb_table_selector + [""div"", ""table"", ""tbody""]\n            )\n        )\n\n        trs = table.find_elements_by_tag_name(""tr"")\n        return [NotebookRow(tr) for tr in trs]\n\n    def wait_for_notebook_state(self, notebook, state, max_waiting_seconds=3):\n        """"""\n        Wait until the requested notebook becomes the specified state in the\n        index page. If the Notebook doesn\'t yet appear, the function will retry\n        to find it.\n        """"""\n        end_time = datetime.datetime.now() + datetime.timedelta(\n            seconds=max_waiting_seconds\n        )\n        while datetime.datetime.now() < end_time:\n            notebook_rows = self.get_notebook_rows()\n            for nb in notebook_rows:\n                name = nb.get_name()\n                if name != notebook:\n                    continue\n\n                logging.info(f""Located Notebook {name} in the table"")\n                if nb.get_status() != state:\n                    continue\n\n                logging.info(f""Notebook \'{name}\' is in state {state}"")\n                return True\n\n            sleep(1)\n\n        logging.warning(f""Notebook \'{notebook}\' isn\'t in state {state}"")\n        return False\n\n\nclass JWAFormPage:\n    def __init__(self, driver):\n        self.driver = driver\n\n    def navigate(self):\n        logging.info(""Navigating to JWA\'s form page"")\n        self.driver.get(parse.urljoin(KUBEFLOW_URL, ""/_/jupyter/new""))\n\n    def appeared(self):\n        try:\n            WebDriverWait(self.driver, 3).until(\n                EC.presence_of_element_located((By.TAG_NAME, ""form""))\n            )\n\n            return True\n        except TimeoutException:\n            logging.warning(""Couldn\'t locate the form"")\n            return False\n\n        return False\n\n\nclass JWA:\n    driver = None\n    dashboard = None\n    iframe = None\n\n    index_page = None\n    form_page = None\n\n    snack_bar_selector = [\n        "".cdk-overlay-container"",\n        ""div"",\n        ""div"",\n        ""snack-bar-container"",\n        ""app-snack-bar"",\n        ""div"",\n    ]\n\n    def __init__(self, dashboard):\n        self.driver = dashboard.driver\n        self.iframe = dashboard.get_iframe()\n        self.dashboard = dashboard\n        self.driver.switch_to.frame(self.iframe)\n\n        self.index_page = JWAIndexPage(self.driver)\n        self.form_page = JWAFormPage(self.driver)\n\n    def switch_selenium_context(self):\n        self.iframe = self.dashboard.get_iframe()\n        self.driver.switch_to.frame(self.iframe)\n\n    def wait_for_snack_bar(self, max_wait_seconds=3):\n        """"""\n        Check if the snack bar shows up with the specific status.\n\n        Returns: snack_status: string, snack_log: string\n        """"""\n        try:\n            WebDriverWait(self.driver, max_wait_seconds).until(\n                EC.presence_of_element_located((By.TAG_NAME, ""app-snack-bar""))\n            )\n\n            popup_icon = self.driver.execute_script(\n                create_selector_query(self.snack_bar_selector + [""mat-icon""])\n            ).text\n\n            popup_text = self.driver.execute_script(\n                create_selector_query(self.snack_bar_selector + [""span""])\n            ).text\n\n            status = icon_to_status(popup_icon)\n            logging.info(f""Located snackbar with status \'{status}\'"")\n            return status, popup_text\n        except TimeoutException:\n            logging.warning(""Timeout reached waiting for snackbar to appear"")\n            return """", """"\n\n        logging.warning(""Couldn\'t locate the snackbar"")\n        return """", """"\n\n\n# --- Test Classes ---\nclass TestJWA:\n    def test_jwa_index_loaded_without_errors(self, tests_setup):\n        """"""\n        Ensure that the UI is loaded AND no pop-up error has appeared\n        """"""\n        dashboard = CentralDashboard(self.driver)\n        jwa = dashboard.jwa\n        jwa.index_page.navigate()\n\n        dashboard.switch_selenium_context()\n        dashboard.select_namespace(KF_NAMESPACE)\n\n        # Test if the index page has loaded\n        jwa.switch_selenium_context()\n        assert jwa.index_page.appeared()\n        logging.info(""JWA\'s index page successfully rendered"")\n\n        # Test if an error appeared as a snackbar\n        snack_type, snack_log = jwa.wait_for_snack_bar()\n        if snack_type == STATE_ERROR or snack_type == STATE_WARNING:\n            logging.error(f""An error occured on index page: \'{snack_log}\'"")\n            assert False\n\n        logging.info(""JWA\'s index page loaded without errors"")\n\n    def test_jwa_form_loaded_without_errors(self, tests_setup):\n        """"""\n        Ensure that the New Notebook Form is loaded without error popups\n        """"""\n        dashboard = CentralDashboard(self.driver)\n        jwa = dashboard.jwa\n        jwa.form_page.navigate()\n\n        dashboard.switch_selenium_context()\n        dashboard.select_namespace(KF_NAMESPACE)\n\n        # Test if the index page has loaded\n        jwa.form_page.navigate()\n        jwa.switch_selenium_context()\n        assert jwa.form_page.appeared()\n        logging.info(""JWA\'s form page successfully rendered"")\n\n        # Test if an error appeared as a snackbar\n        snack_type, snack_log = jwa.wait_for_snack_bar(5)\n        if snack_type == STATE_ERROR or snack_type == STATE_WARNING:\n            logging.error(f""An error occured on form page: \'{snack_log}\'"")\n            assert False\n\n        logging.info(""JWA\'s form page loaded without errors"")\n'"
testing/test_tf_serving.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport logging\nimport numbers\nimport os\nimport time\n\nfrom six.moves import xrange\n\nfrom grpc.beta import implementations\nfrom kubernetes import client as k8s_client\nimport requests\nimport tensorflow as tf\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\n\nfrom kubeflow.testing import test_util\nfrom kubeflow.testing import util\n\n\ndef almost_equal(a, b, tol=0.001):\n  """"""\n  Compares two json objects (assuming same structure) with tolerance on numbers\n  """"""\n  if isinstance(a, dict):\n    for key in a.keys():\n      if not almost_equal(a[key], b[key]):\n        return False\n    return True\n  elif isinstance(a, list):\n    for i in xrange(len(a)):\n      if not almost_equal(a[i], b[i]):\n        return False\n    return True\n  elif isinstance(a, numbers.Number):\n    return abs(a - b) < tol\n  else:\n    return a == b\n\n\ndef main():\n  parser = argparse.ArgumentParser(\'Label an image using Inception\')\n  parser.add_argument(\n      \'-p\',\n      \'--port\',\n      type=int,\n      default=9000,\n      help=\'Port at which Inception model is being served\')\n  parser.add_argument(\n      ""--namespace"", required=True, type=str, help=(""The namespace to use.""))\n  parser.add_argument(\n      ""--service_name"",\n      required=True,\n      type=str,\n      help=(""The TF serving service to use.""))\n  parser.add_argument(\n      ""--artifacts_dir"",\n      default="""",\n      type=str,\n      help=""Directory to use for artifacts that should be preserved after ""\n      ""the test runs. Defaults to test_dir if not set."")\n  parser.add_argument(\n      ""--input_path"", required=True, type=str, help=(""The input file to use.""))\n  parser.add_argument(""--result_path"", type=str, help=(""The expected result.""))\n  parser.add_argument(\n      ""--workflow_name"",\n      default=""tfserving"",\n      type=str,\n      help=""The name of the workflow."")\n\n  args = parser.parse_args()\n\n  t = test_util.TestCase()\n  t.class_name = ""Kubeflow""\n  t.name = args.workflow_name + ""-"" + args.service_name\n\n  start = time.time()\n\n  util.load_kube_config(persist_config=False)\n  api_client = k8s_client.ApiClient()\n  core_api = k8s_client.CoreV1Api(api_client)\n  try:\n    with open(args.input_path) as f:\n      instances = json.loads(f.read())\n\n    service = core_api.read_namespaced_service(args.service_name,\n                                               args.namespace)\n    service_ip = service.spec.cluster_ip\n    model_urls = [\n        ""http://"" + service_ip +\n        "":8500/v1/models/mnist:predict"",  # tf serving\'s http server\n    ]\n    for model_url in model_urls:\n      logging.info(""Try predicting with endpoint {}"".format(model_url))\n      num_try = 1\n      result = None\n      while True:\n        try:\n          result = requests.post(model_url, json=instances)\n          assert (result.status_code == 200)\n        except Exception as e:\n          num_try += 1\n          if num_try > 10:\n            raise\n          logging.info(\'prediction failed: {}. Retrying...\'.format(e))\n          time.sleep(5)\n        else:\n          break\n      logging.info(\'Got result: {}\'.format(result.text))\n      if args.result_path:\n        with open(args.result_path) as f:\n          expected_result = json.loads(f.read())\n          logging.info(\'Expected result: {}\'.format(expected_result))\n          assert (almost_equal(expected_result, json.loads(result.text)))\n  except Exception as e:\n    t.failure = ""Test failed; "" + e.message\n    raise\n  finally:\n    t.time = time.time() - start\n    junit_path = os.path.join(\n        args.artifacts_dir,\n        ""junit_kubeflow-tf-serving-image-{}.xml"".format(args.service_name))\n    logging.info(""Writing test results to %s"", junit_path)\n    test_util.create_junit_xml_file([t], junit_path)\n    # Pause to collect Stackdriver logs.\n    time.sleep(60)\n\n\nif __name__ == \'__main__\':\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
testing/vm_util.py,0,"b'# -*- coding: utf-8 -*-\n""""""Utilities for working with VMs as part of our tests.""""""\n\nimport datetime\nimport logging\nimport os\nimport socket\nimport ssl\nimport subprocess\nimport time\nimport uuid\n\nfrom kubeflow.testing import util\n\n# TODO(jlewi): Should we move this to kubeflow/testing\n\n\ndef wait_for_operation(client,\n                       project,\n                       zone,\n                       op_id,\n                       timeout=datetime.timedelta(hours=1),\n                       polling_interval=datetime.timedelta(seconds=5)):\n  """"""\n  Wait for the specified operation to complete.\n\n  Args:\n    client: Client for the API that owns the operation.\n    project: project\n    zone: Zone. Set to none if its a global operation\n    op_id: Operation id/name.\n    timeout: A datetime.timedelta expressing the amount of time to wait before\n        giving up.\n    polling_interval: A datetime.timedelta to represent the amount of time to\n        wait between requests polling for the operation status.\n\n  Returns:\n    op: The final operation.\n\n  Raises:\n    TimeoutError: if we timeout waiting for the operation to complete.\n  """"""\n  endtime = datetime.datetime.now() + timeout\n  while True:\n    try:\n      if zone:\n        op = client.zoneOperations().get(\n            project=project, zone=zone, operation=op_id).execute()\n      else:\n        op = client.globalOperations().get(\n            project=project, operation=op_id).execute()\n    except socket.error as e:\n      logging.error(""Ignoring error %s"", e)\n      continue\n    except ssl.SSLError as e:\n      logging.error(""Ignoring error %s"", e)\n      continue\n    status = op.get(""status"", """")\n    # Need to handle other status\'s\n    if status == ""DONE"":\n      return op\n    if datetime.datetime.now() > endtime:\n      raise TimeoutError(\n          ""Timed out waiting for op: {0} to complete."".format(op_id))\n    time.sleep(polling_interval.total_seconds())\n\n\ndef wait_for_vm(project,\n                zone,\n                vm,\n                timeout=datetime.timedelta(minutes=5),\n                polling_interval=datetime.timedelta(seconds=10)):\n  """"""\n  Wait for the VM to be ready. This is measured by trying to ssh into the VM\n\n  timeout: A datetime.timedelta expressing the amount of time to wait\n      before giving up.\n  polling_interval: A datetime.timedelta to represent the amount of time\n        to wait between requests polling for the operation status.\n  Raises:\n    TimeoutError: if we timeout waiting for the operation to complete.\n  """"""\n  endtime = datetime.datetime.now() + timeout\n  while True:\n    try:\n      util.run([\n          ""gcloud"", ""compute"", ""--project="" + project, ""ssh"", ""--zone="" + zone,\n          vm, ""--"", ""echo hello world""\n      ])\n      logging.info(""VM is ready"")\n      return\n    except subprocess.CalledProcessError:\n      pass\n\n    if datetime.datetime.now() > endtime:\n      raise util.TimeoutError(\n          (""Timed out waiting for VM to {0} be sshable. Check firewall""\n           ""rules aren\'t blocking ssh."").format(vm))\n\n    time.sleep(polling_interval.total_seconds())\n\n\ndef execute(project, zone, vm, commands):\n  """"""Execute the supplied commands on the VM.""""""\n  util.run([\n      ""gcloud"", ""compute"", ""--project="" + project, ""ssh"", ""--zone="" + zone, vm,\n      ""--"", "" && "".join(commands)\n  ])\n\n\ndef execute_script(project, zone, vm, script):\n  """"""Execute the specified script on the VM.""""""\n\n  target_path = os.path.join(\n      ""/tmp"",\n      os.path.basename(script) + ""."" + uuid.uuid4().hex[0:4])\n\n  target = ""{0}:{1}"".format(vm, target_path)\n  logging.info(""Copying %s to %s"", script, target)\n  util.run([\n      ""gcloud"", ""compute"", ""--project="" + project, ""scp"", script, target,\n      ""--zone="" + zone\n  ])\n\n  util.run([\n      ""gcloud"", ""compute"", ""--project="" + project, ""ssh"", ""--zone="" + zone, vm,\n      ""--"", ""chmod a+rx "" + target_path + "" && "" + target_path\n  ])\n'"
testing/wait_for_deployment.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2018 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Wait for kubeflow deployment.\n\nRight now, it only checks for the presence of tfjobs and pytorchjobs crd. More\nthings can be added incrementally.\npython -m testing.wait_for_deployment --cluster=kubeflow-testing \\\n    --project=kubeflow-ci --zone=us-east1-d --timeout=3\n\nTODO(jlewi): Waiting for the CRD\'s to be created probably isn\'t that useful.\nI think that will be nearly instantaneous. If we\'re going to wait for something\nit should probably be waiting for the controllers to actually be deployed.\nWe can probably get rid of this and just use wait_for_kubeflow.py.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nimport datetime\nimport logging\nimport subprocess\nimport time\n\nfrom kubeflow.testing import test_helper, util\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--timeout"", default=5, type=int, help=""Timeout in minutes"")\n  args, _ = parser.parse_known_args()\n  return args\n\n\ndef wait_for_resource(resource, end_time):\n  while True:\n    if datetime.datetime.now() > end_time:\n      raise RuntimeError(""Timed out waiting for "" + resource)\n    try:\n      if \'error\' not in util.run([""kubectl"", ""get"", resource]).lower():\n        logging.info(""Found "" + resource)\n        break\n    except subprocess.CalledProcessError as e:\n      logging.info(\n          ""Could not find {}. Sleeping for 10 seconds.."".format(resource))\n      time.sleep(10)\n\n\ndef test_wait_for_deployment(test_case):  # pylint: disable=redefined-outer-name\n  args = parse_args()\n  util.maybe_activate_service_account()\n  util.load_kube_config()\n  end_time = datetime.datetime.now() + datetime.timedelta(0, args.timeout * 60)\n  wait_for_resource(""crd/tfjobs.kubeflow.org"", end_time)\n  wait_for_resource(""crd/pytorchjobs.kubeflow.org"", end_time)\n  wait_for_resource(""crd/studyjobs.kubeflow.org"", end_time)\n  logging.info(""Found all resources successfully"")\n\n\nif __name__ == ""__main__"":\n  test_case = test_helper.TestCase(\n      name=""test_wait_for_deployment"", test_func=test_wait_for_deployment)\n  test_suite = test_helper.init(name="""", test_cases=[test_case])\n  test_suite.run()\n'"
testing/wait_for_kubeflow.py,0,"b'""""""Wait for Kubeflow to be deployed.\n\n\nTODO(jlewi): With 0.5 and kfctl go binary this test is replaced by\nkf_is_ready_test.py.\n""""""\nimport argparse\nimport logging\n\nfrom testing import deploy_utils\nfrom kubeflow.testing import test_helper\nfrom kubeflow.testing import util  # pylint: disable=no-name-in-module\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--namespace"", default=None, type=str, help=(""The namespace to use.""))\n\n  args, _ = parser.parse_known_args()\n  return args\n\n\ndef deploy_kubeflow(_):\n  """"""Deploy Kubeflow.""""""\n  args = parse_args()\n  namespace = args.namespace\n  api_client = deploy_utils.create_k8s_client()\n\n  util.load_kube_config()\n\n  # Verify that Jupyter is actually deployed.\n  jupyter_name = ""jupyter""\n  logging.info(""Verifying TfHub started."")\n  util.wait_for_statefulset(api_client, namespace, jupyter_name)\n\n  # Verify that core components are actually deployed.\n  deployment_names = [\n      ""tf-job-operator"", ""pytorch-operator"", ""studyjob-controller""\n  ]\n  for deployment_name in deployment_names:\n    logging.info(""Verifying that %s started..."", deployment_name)\n    util.wait_for_deployment(api_client, namespace, deployment_name)\n\n\ndef main():\n  test_case = test_helper.TestCase(\n      name=\'deploy_kubeflow\', test_func=deploy_kubeflow)\n  test_suite = test_helper.init(name=\'deploy_kubeflow\', test_cases=[test_case])\n  test_suite.run()\n\n\nif __name__ == ""__main__"":\n  main()\n'"
build/boilerplate/boilerplate.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2016 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nimport argparse\nimport copy\nimport os\nimport re\nimport sys\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    ""filenames"",\n    help=""list of files to check, all files if unspecified"",\n    nargs=\'*\')\nargs = parser.parse_args()\n\nrootdir = os.path.dirname(__file__) + ""/../../""\nrootdir = os.path.abspath(rootdir)\n\n\ndef get_refs():\n  ref_file = open(os.path.join(rootdir, ""build/boilerplate/boilerplate.txt""))\n  ref = ref_file.read().splitlines()\n  ref_file.close()\n  refs = {}\n  for extension in [""sh"", ""go"", ""py""]:\n    refs[extension] = copy.copy(ref)\n    prefix = """"\n    if extension == ""go"":\n      prefix = ""//""\n    else:\n      prefix = ""#""\n    for i in range(len(refs[extension])):\n      if len(refs[extension][i]) != 0:\n        p = prefix + "" ""\n      else:\n        p = prefix\n      refs[extension][i] = p + refs[extension][i]\n  return refs\n\n\ndef file_passes(filename, refs, regexs):\n  try:\n    f = open(filename, \'r\')\n  except:  # noqa: E722\n    return False\n\n  data = f.read()\n  f.close()\n\n  extension = file_extension(filename)\n  ref = refs[extension]\n\n  # remove build tags from the top of Go files\n  if extension == ""go"":\n    p = regexs[""go_build_constraints""]\n    (data, found) = p.subn("""", data, 1)\n\n  # remove shebang from the top of shell files\n  if extension == ""sh"" or extension == ""py"":\n    p = regexs[""shebang""]\n    (data, found) = p.subn("""", data, 1)\n\n  data = data.splitlines()\n\n  # if our test file is smaller than the reference it surely fails!\n  if len(ref) > len(data):\n    return False\n\n  # trim our file to the same number of lines as the reference file\n  data = data[:len(ref)]\n\n  p = regexs[""year""]\n  for d in data:\n    if p.search(d):\n      return False\n\n  # Replace all occurrences of the regex ""2016|2015|2014"" with ""YEAR""\n  p = regexs[""date""]\n  for i, d in enumerate(data):\n    (data[i], found) = p.subn(\'YEAR\', d)\n    if found != 0:\n      break\n\n  # if we don\'t match the reference at this point, fail\n  if ref != data:\n    return False\n\n  return True\n\n\ndef file_extension(filename):\n  return os.path.splitext(filename)[1].split(""."")[-1].lower()\n\n\nskipped_dirs = [\'Godeps\', \'vendor\', \'third_party\', \'_gopath\', \'_output\', \'.git\']\n\n\ndef normalize_files(files):\n  newfiles = []\n  for pathname in files:\n    if any(x in pathname for x in skipped_dirs):\n      continue\n    newfiles.append(pathname)\n  for i, pathname in enumerate(newfiles):\n    if not os.path.isabs(pathname):\n      newfiles[i] = os.path.join(rootdir, pathname)\n  return newfiles\n\n\ndef get_files(extensions):\n  files = []\n  if len(args.filenames) > 0:\n    files = args.filenames\n  else:\n    for root, dirs, walkfiles in os.walk(rootdir):\n      # don\'t visit certain dirs. This is just a performance improvement as we\n      # would prune these later in normalize_files(). But doing it cuts down the\n      # amount of filesystem walking we do and cuts down the size of the file\n      # list\n      for d in skipped_dirs:\n        if d in dirs:\n          dirs.remove(d)\n\n      for name in walkfiles:\n        pathname = os.path.join(root, name)\n        files.append(pathname)\n\n  files = normalize_files(files)\n  outfiles = []\n  for pathname in files:\n    extension = file_extension(pathname)\n    if extension in extensions:\n      outfiles.append(pathname)\n  return outfiles\n\n\ndef get_regexs():\n  regexs = {}\n  # Search for ""YEAR"" which exists in the boilerplate, but shouldn\'t in the\n  # real thing\n  regexs[""year""] = re.compile(\'YEAR\')\n  # dates can be 2014, 2015 or 2016, company holder names can be anything\n  regexs[""date""] = re.compile(\'(2014|2015|2016|2017|2018|2019|2020)\')\n  # strip // +build \\n\\n build constraints\n  regexs[""go_build_constraints""] = re.compile(r""^(// \\+build.*\\n)+\\n"",\n                                              re.MULTILINE)\n  # strip #!.* from shell scripts\n  regexs[""shebang""] = re.compile(r""^(#!.*\\n)\\n*"", re.MULTILINE)\n  return regexs\n\n\ndef main():\n  regexs = get_regexs()\n  refs = get_refs()\n  filenames = get_files(refs.keys())\n\n  for filename in filenames:\n    if not file_passes(filename, refs, regexs):\n      print(filename, file=sys.stdout)\n\n\nif __name__ == ""__main__"":\n  sys.exit(main())\n'"
components/echo-server/main.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nGoogle Cloud Endpoints sample application.\n\nDemonstrates how to create a simple echo API as well as how to deal with\nvarious authentication methods.\n""""""\n\nimport base64\nimport json\nimport logging\n\nfrom flask import Flask, jsonify, request\nfrom flask_cors import cross_origin\nfrom six.moves import http_client\n\napp = Flask(__name__)\n\n\ndef _base64_decode(encoded_str):\n  # Add paddings manually if necessary.\n  num_missed_paddings = 4 - len(encoded_str) % 4\n  if num_missed_paddings != 4:\n    encoded_str += b\'=\' * num_missed_paddings\n  return base64.b64decode(encoded_str).decode(\'utf-8\')\n\n\n@app.route(\'/echo\', methods=[\'POST\'])\ndef echo():\n  """"""Simple echo service.""""""\n  message = request.get_json().get(\'message\', \'\')\n  return jsonify({\'message\': message})\n\n\n@app.route(\'/\')\n@app.route(\'/headers\')\ndef headers():\n  return jsonify({\'headers\': request.headers.to_list()})\n\n\ndef auth_info():\n  """"""Retrieves the authenication information from Google Cloud Endpoints.""""""\n  encoded_info = request.headers.get(\'X-Endpoint-API-UserInfo\', None)\n\n  if encoded_info:\n    info_json = _base64_decode(encoded_info)\n    user_info = json.loads(info_json)\n  else:\n    user_info = {\'id\': \'anonymous\'}\n\n  return jsonify(user_info)\n\n\n@app.route(\'/auth/info/googlejwt\', methods=[\'GET\'])\ndef auth_info_google_jwt():\n  """"""Auth info with Google signed JWT.""""""\n  return auth_info()\n\n\n@app.route(\'/auth/info/googleidtoken\', methods=[\'GET\'])\ndef auth_info_google_id_token():\n  """"""Auth info with Google ID token.""""""\n  return auth_info()\n\n\n@app.route(\'/auth/info/firebase\', methods=[\'GET\'])\n@cross_origin(send_wildcard=True)\ndef auth_info_firebase():\n  """"""Auth info with Firebase auth.""""""\n  return auth_info()\n\n\n@app.errorhandler(http_client.INTERNAL_SERVER_ERROR)\ndef unexpected_error(e):\n  """"""Handle exceptions by returning swagger-compliant json.""""""\n  logging.exception(\'An error occurred while processing the request.\')\n  response = jsonify({\n      \'code\': http_client.INTERNAL_SERVER_ERROR,\n      \'message\': \'Exception: {}\'.format(e)\n  })\n  response.status_code = http_client.INTERNAL_SERVER_ERROR\n  return response\n\n\nif __name__ == \'__main__\':\n  # This is used when running locally. Gunicorn is used to run the application\n  # on Google App Engine. See entrypoint in app.yaml.\n  app.run(host=\'127.0.0.1\', port=8080, debug=True)\n'"
components/echo-server/main_test.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nimport json\nimport os\nimport pytest\n\nimport main\n\n\n@pytest.fixture\ndef client(monkeypatch):\n  monkeypatch.chdir(os.path.dirname(main.__file__))\n  main.app.testing = True\n  client = main.app.test_client()\n  return client\n\n\ndef test_echo(client):\n  r = client.post(\n      \'/echo\',\n      data=\'{""message"": ""Hello""}\',\n      headers={\'Content-Type\': \'application/json\'})\n\n  assert r.status_code == 200\n  data = json.loads(r.data.decode(\'utf-8\'))\n  assert data[\'message\'] == \'Hello\'\n\n\ndef test_auth_info(client):\n  endpoints = [\n      \'/auth/info/googlejwt\', \'/auth/info/googleidtoken\', \'/auth/info/firebase\'\n  ]\n\n  encoded_info = base64.b64encode(json.dumps({\'id\': \'123\'}).encode(\'utf-8\'))\n\n  for endpoint in endpoints:\n    r = client.get(endpoint, headers={\'Content-Type\': \'application/json\'})\n\n    assert r.status_code == 200\n    data = json.loads(r.data.decode(\'utf-8\'))\n    assert data[\'id\'] == \'anonymous\'\n\n    r = client.get(\n        endpoint,\n        headers={\n            \'Content-Type\': \'application/json\',\n            \'X-Endpoint-API-UserInfo\': encoded_info\n        })\n\n    assert r.status_code == 200\n    data = json.loads(r.data.decode(\'utf-8\'))\n    assert data[\'id\'] == \'123\'\n\n\ndef test_cors(client):\n  r = client.options(\'/auth/info/firebase\', headers={\'Origin\': \'example.com\'})\n  assert r.status_code == 200\n  assert r.headers[\'Access-Control-Allow-Origin\'] == \'*\'\n'"
components/https-redirect/main.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A simple flask app to redirect all requests to https.""""""\n\nimport logging\n\nfrom flask import Flask, jsonify, redirect, request\n\napp = Flask(__name__)\n\n\n@app.route(\'/healthz\')\ndef health_check():\n  return jsonify({\'isHealthy\': True})\n\n\n@app.route(\'/\')\n@app.route(\'/<path:path>\')\ndef all_handler(path=None):\n  new_url = request.url\n  if request.scheme == ""http"":\n    prefix = ""http""\n    new_url = ""https"" + new_url[len(prefix):]\n  logging.info(""Redirecting to: %s"", new_url)\n\n  response = redirect(new_url)\n\n  # For ""/"" we return a 200 (ok) and not a 302 (redirect) because on GKE we want\n  # to be able to use this to redirect http://mydomain.com/ to\n  # https://mydomain.com/. However, the Ingress sets up the GCP loadbalancer\n  # health check requires that a 200 be served on ""/"". So if we return a 302\n  # the backend will be considered unhealthy.\n  if not path:\n    response.status_code = 200\n  return response\n\n\nif __name__ == \'__main__\':\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  app.run(host=\'127.0.0.1\', port=8080, debug=False)\n'"
components/https-redirect/main_test.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport main\n\n\nclass TestRedirect(unittest.TestCase):\n\n  def test_non_empty_path(self):\n    main.app.testing = True\n    client = main.app.test_client()\n\n    endpoint = \'/hello/world\'\n    r = client.get(endpoint, headers={\'Content-Type\': \'application/json\'})\n\n    self.assertEqual(302, r.status_code)\n    self.assertEqual(\n        r.data,\n        \'<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">\\n<title>Redirecting...</title>\\n<h1>Redirecting...</h1>\\n<p>You should be redirected automatically to target URL: <a href=""https://localhost/hello/world"">https://localhost/hello/world</a>.  If not click the link.\'  # noqa: E501\n    )\n\n  def test_empty_path(self):\n    main.app.testing = True\n    client = main.app.test_client()\n\n    endpoint = \'/\'\n    r = client.get(endpoint, headers={\'Content-Type\': \'application/json\'})\n\n    self.assertEqual(200, r.status_code)\n    self.assertEqual(\n        r.data,\n        \'<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">\\n<title>Redirecting...</title>\\n<h1>Redirecting...</h1>\\n<p>You should be redirected automatically to target URL: <a href=""https://localhost/"">https://localhost/</a>.  If not click the link.\'  # noqa: E501\n    )\n\n  def test_health_check(self):\n    main.app.testing = True\n    client = main.app.test_client()\n\n    endpoint = \'/healthz\'\n    r = client.get(endpoint, headers={\'Content-Type\': \'application/json\'})\n\n    self.assertEqual(200, r.status_code)\n\n\nif __name__ == ""__main__"":\n  unittest.main()\n'"
docs/gke/iap_request.py,0,"b'# -*- coding: utf-8 -*-\n# Adapted from\n# https://cloud.google.com/iap/docs/authentication-howto#iap-make-request-python\nimport argparse\nimport google.auth\nimport google.auth.app_engine\nimport google.auth.compute_engine.credentials\nimport google.auth.iam\nfrom google.auth.transport.requests import Request\nimport google.oauth2.credentials\nimport google.oauth2.service_account\nimport requests\nimport requests_toolbelt.adapters.appengine\n\nIAM_SCOPE = \'https://www.googleapis.com/auth/iam\'\nOAUTH_TOKEN_URI = \'https://www.googleapis.com/oauth2/v4/token\'\n\n\ndef get_service_account_token(client_id):\n  """"""\n  Get open id connect token for default service account.\n\n  Returns:\n    The open id connect token for default service account.\n  """"""\n  # Figure out what environment we\'re running in and get some preliminary\n  # information about the service account.\n  bootstrap_credentials, _ = google.auth.default(scopes=[IAM_SCOPE])\n  if isinstance(bootstrap_credentials, google.oauth2.credentials.Credentials):\n    raise Exception(\'make_iap_request is only supported for service \'\n                    \'accounts.\')\n  elif isinstance(bootstrap_credentials, google.auth.app_engine.Credentials):\n    requests_toolbelt.adapters.appengine.monkeypatch()\n\n  # For service account\'s using the Compute Engine metadata service,\n  # service_account_email isn\'t available until refresh is called.\n  bootstrap_credentials.refresh(Request())\n\n  signer_email = bootstrap_credentials.service_account_email\n  if isinstance(bootstrap_credentials,\n                google.auth.compute_engine.credentials.Credentials):\n    # Since the Compute Engine metadata service doesn\'t expose the service\n    # account key, we use the IAM signBlob API to sign instead.\n    # In order for this to work:\n    #\n    # 1. Your VM needs the https://www.googleapis.com/auth/iam scope.\n    #    You can specify this specific scope when creating a VM  through the API\n    #    or gcloud. When using Cloud Console, you\'ll need to specify the\n    #    ""full access to all Cloud APIs"" scope. A VM\'s scopes can only be\n    #    specified at creation time.\n    #\n    # 2. The VM\'s default service account needs the ""Service Account Actor""\n    #    role. This can be found under the ""Project"" category in Cloud Console,\n    #    or roles/iam.serviceAccountActor in gcloud.\n    signer = google.auth.iam.Signer(Request(), bootstrap_credentials,\n                                    signer_email)\n  else:\n    # A Signer object can sign a JWT using the service account\'s key.\n    signer = bootstrap_credentials.signer\n\n  # Construct OAuth 2.0 service account credentials using the signer and email\n  # acquired from the bootstrap credentials.\n  service_account_credentials = google.oauth2.service_account.Credentials(\n      signer,\n      signer_email,\n      token_uri=OAUTH_TOKEN_URI,\n      additional_claims={\'target_audience\': client_id})\n\n  # service_account_credentials gives us a JWT signed by the service account.\n  # Next, we use that to obtain an OpenID Connect token, which is a JWT signed\n  # by Google.\n  return get_google_open_id_connect_token(\n      service_account_credentials), signer_email\n\n\ndef get_google_open_id_connect_token(service_account_credentials):\n  """"""\n  Get an OpenID Connect token issued by Google for the service account.\n\n  This function:\n    1. Generates a JWT signed with the service account\'s private key containing\n       a special ""target_audience"" claim.\n\n    2. Sends it to the OAUTH_TOKEN_URI endpoint. Because the JWT in #1 has a\n       target_audience claim, that endpoint will respond with an OpenID Connect\n       token for the service account -- in other words, a JWT signed by\n       *Google*. The aud claim in this JWT will be set to the value from the\n       target_audience claim in #1.\n\n  For more information, see\n  https://developers.google.com/identity/protocols/OAuth2ServiceAccount .\n  The HTTP/REST example on that page describes the JWT structure and\n  demonstrates how to call the token endpoint. (The example on that page shows\n  how to get an OAuth2 access token; this code is using a modified version of it\n  to get an OpenID Connect token.)\n  """"""\n\n  service_account_jwt = (\n      service_account_credentials._make_authorization_grant_assertion())\n  request = google.auth.transport.requests.Request()\n  body = {\n      \'assertion\': service_account_jwt,\n      \'grant_type\': google.oauth2._client._JWT_GRANT_TYPE,\n  }\n  token_response = google.oauth2._client._token_endpoint_request(\n      request, OAUTH_TOKEN_URI, body)\n  return token_response[\'id_token\']\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'url\', help=\'URL of the host model\')\n  parser.add_argument(\'client_id\', help=\'The client id used to setup IAP\')\n  parser.add_argument(\'--input\', help=\'The input file.\')\n  args = parser.parse_args()\n\n  token, signer_email = get_service_account_token(args.client_id)\n  if args.input:\n    with open(args.input) as f:\n      data = f.read()\n    resp = requests.post(\n        args.url,\n        verify=False,\n        data=data,\n        headers={\'Authorization\': \'Bearer {}\'.format(token)})\n  else:\n    resp = requests.get(\n        args.url,\n        verify=False,\n        headers={\'Authorization\': \'Bearer {}\'.format(token)})\n  if resp.status_code == 403:\n    raise Exception(\n        \'Service account {} does not have permission to \'\n        \'access the IAP-protected application.\'.format(signer_email))\n  elif resp.status_code != 200:\n    raise Exception(\'Bad response from application: {!r} / {!r} / {!r}\'.format(\n        resp.status_code, resp.headers, resp.text))\n  else:\n    print(resp.text)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
metric-collector/service-readiness/kubeflow-readiness.py,0,"b'# -*- coding: utf-8 -*-\nimport argparse\nimport google.auth\nimport google.auth.app_engine\nimport google.auth.compute_engine.credentials\nimport google.auth.iam\nimport google.oauth2.credentials\nimport google.oauth2.service_account\nimport logging\nimport requests\nfrom google.auth.transport.requests import Request\nfrom kubernetes import client, config\nfrom kubernetes.client import V1Event, V1ObjectMeta\nfrom prometheus_client import start_http_server, Gauge\nfrom time import sleep, time\n\nIAM_SCOPE = \'https://www.googleapis.com/auth/iam\'\nOAUTH_TOKEN_URI = \'https://www.googleapis.com/oauth2/v4/token\'\nMETHOD = \'GET\'\nKUBEFLOW_AVAILABILITY = \\\n    Gauge(\'kubeflow_availability\',\n          \'Signal of whether IAP protected kubeflow is available\')\n\n\ndef metric_update(args, google_open_id_connect_token):\n  resp = requests.request(\n      METHOD,\n      args.url,\n      headers={\n          \'Authorization\': \'Bearer {}\'.format(google_open_id_connect_token)\n      })\n  if resp.status_code == 200:\n    KUBEFLOW_AVAILABILITY.set(1)\n    return 1\n  else:\n    KUBEFLOW_AVAILABILITY.set(0)\n    return 0\n\n\ndef main(unparsed_args=None):\n  parser = argparse.ArgumentParser(\n      description=""Output signal of kubeflow service readiness."")\n\n  parser.add_argument(\n      ""--url"", default="""", type=str, help=""kubeflow IAP-protected url"")\n  parser.add_argument(\n      ""--client_id"",\n      default="""",\n      type=str,\n      help=""Service account json credential file"")\n\n  args = parser.parse_args(args=unparsed_args)\n\n  if args.url == """" or args.client_id == """":\n    logging.info(""Url or client_id is empty, exit"")\n    return\n\n  # Figure out what environment we\'re running in and get some preliminary\n  # information about the service account.\n  credentials, _ = google.auth.default(scopes=[IAM_SCOPE])\n  if isinstance(credentials, google.oauth2.credentials.Credentials):\n    raise Exception(\'make_iap_request is only supported for service \'\n                    \'accounts.\')\n\n  # For service account\'s using the Compute Engine metadata service,\n  # service_account_email isn\'t available until refresh is called.\n  credentials.refresh(Request())\n\n  signer_email = credentials.service_account_email\n  if isinstance(credentials,\n                google.auth.compute_engine.credentials.Credentials):\n    # Since the Compute Engine metadata service doesn\'t expose the service\n    # account key, we use the IAM signBlob API to sign instead.\n    # In order for this to work:\n    #\n    # 1. Your VM needs the https://www.googleapis.com/auth/iam scope. You\n    #    can specify this specific scope when creating a VM through the API\n    #    or gcloud. When using Cloud Console, you\'ll need to specify the\n    #    full access to all Cloud APIs"" scope. A VM\'s scopes can only be\n    #    specified at creation time.\n    #\n    # 2. The VM\'s default service account needs the ""Service Account Actor""\n    #    role. This can be found under the ""Project"" category in Cloud\n    #    Console, or roles/iam.serviceAccountActor in gcloud.\n    signer = google.auth.iam.Signer(Request(), credentials, signer_email)\n  else:\n    # A Signer object can sign a JWT using the service account\'s key.\n    signer = credentials.signer\n\n  # Construct OAuth 2.0 service account credentials using the signer and\n  # email acquired from the bootstrap credentials.\n  service_account_credentials = google.oauth2.service_account.Credentials(\n      signer,\n      signer_email,\n      token_uri=OAUTH_TOKEN_URI,\n      additional_claims={\'target_audience\': args.client_id})\n\n  token_refresh_time = 0\n  last_status = -1\n  config.load_incluster_config()\n  coreApi = client.CoreV1Api()\n  while True:\n    if time() > token_refresh_time:\n      # service_account_credentials gives us a JWT signed by the service\n      # account. Next, we use that to obtain an OpenID Connect token,\n      # which is a JWT signed by Google.\n      google_open_id_connect_token = get_google_open_id_connect_token(\n          service_account_credentials)\n      token_refresh_time = time() + 1800\n    url_status = metric_update(args, google_open_id_connect_token)\n    if url_status != last_status:\n      last_status = url_status\n      # get service centraldashboard, attach event to it.\n      svcs = coreApi.list_namespaced_service(\n          \'kubeflow\', label_selector=""app=centraldashboard"")\n      while len(svcs.to_dict()[\'items\']) == 0:\n        logging.info(""Service centraldashboard not ready..."")\n        sleep(10)\n        svcs = coreApi.list_namespaced_service(\n            \'kubeflow\', label_selector=""app=centraldashboard"")\n      uid = svcs.to_dict()[\'items\'][0][\'metadata\'][\'uid\']\n      kf_status = ""up"" if url_status == 1 else ""down""\n      new_event = V1Event(\n          action=""Kubeflow service status update: "" + kf_status,\n          api_version=""v1"",\n          kind=""Event"",\n          message=""Service "" + kf_status + ""; service url: "" + args.url,\n          reason=""Kubeflow Service is "" + kf_status,\n          involved_object=client.V1ObjectReference(\n              api_version=""v1"",\n              kind=""Service"",\n              name=""centraldashboard"",\n              namespace=""kubeflow"",\n              uid=uid),\n          metadata=V1ObjectMeta(generate_name=\'kubeflow-service.\',),\n          type=""Normal"")\n      event = coreApi.create_namespaced_event(""kubeflow"", new_event)\n      print(""New status event created. action=\'%s\'"" % str(event.action))\n\n    # Update status every 10 sec\n    sleep(10)\n\n\ndef get_google_open_id_connect_token(service_account_credentials):\n  """"""\n  Get an OpenID Connect token issued by Google for the service account.\n\n  This function:\n\n    1. Generates a JWT signed with the service account\'s private key containing\n       a special ""target_audience"" claim.\n\n    2. Sends it to the OAUTH_TOKEN_URI endpoint. Because the JWT in #1 has a\n       target_audience claim, that endpoint will respond with an OpenID Connect\n       token for the service account -- in other words, a JWT signed by\n       *Google*. The aud claim in this JWT will be set to the value from the\n       target_audience claim in #1.\n\n  For more information, see\n  https://developers.google.com/identity/protocols/OAuth2ServiceAccount\n  The HTTP/REST example on that page describes the JWT structure and\n  demonstrates how to call the token endpoint. (The example on that paga shows\n  how to get an OAuth2 access token; this code is using a modified version of it\n  to get an OpenID Connect token.)\n  """"""\n\n  service_account_jwt = (\n      service_account_credentials._make_authorization_grant_assertion())\n  request = google.auth.transport.requests.Request()\n  body = {\n      \'assertion\': service_account_jwt,\n      \'grant_type\': google.oauth2._client._JWT_GRANT_TYPE,\n  }\n  token_response = google.oauth2._client._token_endpoint_request(\n      request, OAUTH_TOKEN_URI, body)\n  return token_response[\'id_token\']\n\n\nif __name__ == \'__main__\':\n  start_http_server(8000)\n  main()\n'"
py/kubeflow/__init__.py,0,"b""__path__ = __import__('pkgutil').extend_path(__path__, __name__)\n"""
releasing/hubsync/hubsync.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# [START app]\n\n# import modules\n\nimport json\nimport subprocess\nimport yaml\n\n# declaring some variables\nimages = []\ntimeout = \'7200s\'\nfilename = \'cloudbuild.yaml\'\nbuilder = \'gcr.io/cloud-builders/docker\'\nkfRepo = \'gcr.io/kubeflow-images-public/\'\nmyRepo = \'gcr.io/<my_repo>\'\n\n# Get Auth\nwith open(\'keys.yaml\', \'r\') as keyfile:\n  kcfg = yaml.load(keyfile)\n\nlogin = kcfg[\'username\']\npswd = kcfg[\'password\']\n\n# build a json file with all files.\nrepos = subprocess.run([\n    ""gcloud"", ""--project=kubeflow-images-public"", ""container"", ""images"", ""list"",\n    ""--format=json""\n],\n                       stdout=subprocess.PIPE,\n                       stderr=subprocess.STDOUT)\nmy_json = json.loads(repos.stdout.decode(\'utf8\').strip().replace(""\'"", \'""\'))\nfor data in my_json:\n  for name, image in data.items():\n    # get Tags and Repos\n    raw_images = subprocess.run([\n        ""gcloud"", ""container"", ""images"", ""list"", ""--repository="" + image + """",\n        ""--format=json""\n    ],\n                                stdout=subprocess.PIPE,\n                                stderr=subprocess.STDOUT)\n    imgData = raw_images.stdout.decode(""utf-8"")\n    if ""[]"" not in imgData:\n      imgJson = json.loads(\n          raw_images.stdout.decode(""utf-8"").strip().replace(""\'"", \'""\'))\n      for stuff in imgJson:\n        for a, b in stuff.items():\n          images.append(b)\n      images.append(image)\n\nfor item in images:\n  getTags = subprocess.run([\n      ""gcloud"", ""--project=kubeflow-images-public"", ""container"", ""images"",\n      ""list-tags"", item, ""--format=json"", ""--limit=1""\n  ],\n                           stdout=subprocess.PIPE,\n                           stderr=subprocess.STDOUT)\n  preTags = json.loads(getTags.stdout.decode(\'utf8\').replace(""\'"", \'""\'))\n  for datum in preTags:\n    t = datum[\'digest\']\n    s = item[30:]\n    myTag = item + ""@"" + t\n    theyaml = {\n        \'timeout\':\n        timeout,\n        \'steps\': [\n            {\n                \'name\': builder,\n                \'args\': [\'login\', \'-u\', login, \'-p\', pswd],\n                \'timeout\': timeout,\n            },\n            {\n                \'name\': builder,\n                \'args\': [\'pull\', myTag],\n                \'timeout\': timeout,\n            },\n            {\n                \'name\': builder,\n                \'args\': [\'tag\', myTag, myRepo + s],\n                \'timeout\': timeout,\n            },\n            {\n                \'name\': builder,\n                \'args\': [\'push\', myRepo + s],\n                \'timeout\': timeout,\n            },\n        ]\n    }\n    with open(filename, \'a\') as outfile:\n      yaml.dump(theyaml, outfile, default_flow_style=False)\n\n    subprocess.run([""gcloud"", ""builds"", ""submit"", ""--config"", filename],\n                   stdout=subprocess.PIPE,\n                   stderr=subprocess.STDOUT)\n'"
scripts/gke/delete_role_bindings.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# A simple script to delete all role bindings for the service accounts created\n# as part of a Kubeflow deployment. This is an effort to deal with:\n# https://github.com/kubeflow/kubeflow/issues/953\nimport argparse\nimport logging\nimport json\nimport subprocess\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--project"", default=None, type=str, help=(""The project.""))\n  parser.add_argument(\n      ""--service_account"", type=str, help=(""The service account.""))\n\n  args = parser.parse_args()\n  output = subprocess.check_output([\n      ""gcloud"",\n      ""projects"",\n      ""get-iam-policy"",\n      ""--format=json"",\n      args.project,\n  ])\n\n  bindings = json.loads(output)\n  roles = []\n  entry = ""serviceAccount:"" + args.service_account\n  for b in bindings[""bindings""]:\n    if entry in b[""members""]:\n      roles.append(b[""role""])\n  # TODO(jlewi): Can we issue a single gcloud command.\n  for r in roles:\n    command = [\n        ""gcloud"",\n        ""projects"",\n        ""remove-iam-policy-binding"",\n        args.project,\n        ""--member"",\n        entry,\n        ""--role"",\n        r,\n    ]\n    print("" "".join(command))\n    subprocess.call(command)\n'"
scripts/gke/iam_patch.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# A python script which manages IAM binding patches declaratively IAM policy\n# patch can be defined in a separate file declaratively and it can either be\n# added or removed from a projects iam policy\n#\n# Usage\n#   python iam_patch.py --action=add --project=agwliamtest \\\n#     --iam_bindings_file=iam_bindings.yaml\n#   python iam_patch.py --action=remove --project=agwliamtest \\\n#     --iam_bindings_file=iam_bindings.yaml\nimport argparse\nimport logging\nimport subprocess\nimport sys\nimport tempfile\nimport time\nimport yaml\n\n\ndef parse_args():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--action"",\n      default=""add"",\n      type=str,\n      help=(""The action to take. Valid values: add, remove""))\n  parser.add_argument(\n      ""--project"", default=None, type=str, help=(""The project.""))\n  parser.add_argument(\n      ""--iam_bindings_file"",\n      default=None,\n      type=str,\n      help=(""The IAM bindings file.""))\n  parser.set_defaults(dry_run=False)\n  parser.add_argument(\n      \'--dry_run\',\n      dest=\'dry_run\',\n      action=\'store_true\',\n      help=(""Don\'t patch the final IAM policy, only print it""))  # noqa: E501\n  return parser.parse_args()\n\n\ndef get_current_iam_policy(project):\n  """"""Fetches and returns the current iam policy as a yaml object""""""\n  return yaml.load(\n      subprocess.check_output([""gcloud"", ""projects"", ""get-iam-policy"",\n                               project]))\n\n\ndef iam_policy_to_dict(bindings):\n  """"""\n  iam_policy_to_dict takes an iam policy binding in the GCP API format and\n  converts it into a python dict so that it can be easily updated\n  """"""\n  bindings_dict = dict()\n  for binding in bindings:\n    role = binding[\'role\']\n    bindings_dict[role] = set(binding[\'members\'])\n  return bindings_dict\n\n\ndef iam_dict_to_policy(bindings_dict):\n  """"""\n  iam_dict_to_policy takes an iam policy binding in the dict format and\n  converts it into GCP API format so that it can be sent to GCP IAM API for\n  an update\n  """"""\n  bindings = []\n  for k, v in bindings_dict.items():\n    bindings.append({""role"": k, ""members"": list(v)})\n  return bindings\n\n\ndef apply_iam_bindings_patch(current_policy, bindings_patch, action):\n  """"""\n  Patches the current policy with the supplied patch.\n  action can be add or remove.\n  """"""\n  for item in bindings_patch[\'bindings\']:\n    members = item[\'members\']\n    roles = item[\'roles\']\n    for role in roles:\n      if role not in current_policy.keys():\n        current_policy[role] = set()\n      if action == ""add"":\n        current_policy[role].update(members)\n      else:\n        current_policy[role].difference_update(members)\n  return current_policy\n\n\ndef patch_iam_policy(args):\n  """"""\n  Fetches the current IAM policy, patches it with the bindings supplied in\n  --iam_bindings_file and updates the new iam policy\n  """"""\n  current_policy = get_current_iam_policy(args.project)\n  logging.info(""Current IAM Policy"")\n  logging.info(\n      yaml.dump(current_policy, default_flow_style=False, default_style=\'\'))\n  current_policy_bindings_dict = iam_policy_to_dict(current_policy[\'bindings\'])\n  with open(args.iam_bindings_file) as iam_bindings_file:\n    bindings_patch = yaml.load(iam_bindings_file.read())\n  current_policy_bindings_dict = apply_iam_bindings_patch(\n      current_policy_bindings_dict, bindings_patch, args.action)\n  current_policy[\'bindings\'] = iam_dict_to_policy(current_policy_bindings_dict)\n  logging.info(""Updated Policy"")\n  logging.info(""\\n"" + yaml.dump(\n      current_policy, default_flow_style=False, default_style=\'\'))\n  updated_policy_file = tempfile.NamedTemporaryFile(delete=False)\n  with open(updated_policy_file.name, \'w\') as f:\n    yaml.dump(current_policy, f, default_flow_style=False)\n  logging.debug(""Temp file %s"", updated_policy_file.name)\n  if not args.dry_run:\n    subprocess.check_call([\n        ""gcloud"", ""projects"", ""set-iam-policy"", args.project,\n        updated_policy_file.name\n    ])\n  else:\n    logging.info(""Skipping patching the IAM policy because --dry_run was set"")\n\n\nif __name__ == ""__main__"":\n  logging.getLogger().setLevel(logging.INFO)\n  args = parse_args()\n\n  if args.action not in [""add"", ""remove""]:\n    raise ValueError(""invalid --action. Valid values are add, remove"")\n\n  for i in range(5):\n    try:\n      patch_iam_policy(args)\n      logging.info(""Successfully patched IAM policy"")\n      break\n    except Exception as e:\n      logging.error(e)\n      if i < 4:\n        logging.info(""Retrying in 15 seconds.."")\n        time.sleep(15)\n      else:\n        logging.error(""Patching IAM policy failed"")\n        sys.exit(1)\n'"
scripts/gke/iam_patch_test.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Unit tests for iam_patch.py\nimport unittest\n\nimport iam_patch\n\n\nclass Test(unittest.TestCase):\n\n  def test_apply_iam_bindings_patch_add(self):\n    current_policy = {}\n    bindings_patch = {\n        ""bindings"": [{\n            ""members"": [\'admin-sa\'],\n            ""roles"": [""roles/source.admin""]\n        }]\n    }\n    expected = {\'roles/source.admin\': set([\'admin-sa\'])}\n    result = iam_patch.apply_iam_bindings_patch(current_policy, bindings_patch,\n                                                ""add"")\n    self.assertEqual(result, expected)\n\n  def test_apply_iam_bindings_patch_remove(self):\n    current_policy = {\'roles/source.admin\': set([\'admin-sa\'])}\n    bindings_patch = {\n        ""bindings"": [{\n            ""members"": [\'admin-sa\'],\n            ""roles"": [""roles/source.admin""]\n        }]\n    }\n    expected = {\'roles/source.admin\': set()}\n    result = iam_patch.apply_iam_bindings_patch(current_policy, bindings_patch,\n                                                ""remove"")\n    self.assertEqual(result, expected)\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
testing/kfctl/conftest.py,0,"b'import pytest\n\ndef pytest_addoption(parser):\n  parser.addoption(\n      ""--app_path"", action=""store"", default="""",\n      help=""Path where the KF application should be stored"")\n  \n  parser.addoption(\n      ""--app_name"", action=""store"", default="""",\n      help=""Name of the KF application"")\n\n  parser.addoption(\n      ""--kfctl_path"", action=""store"", default="""",\n      help=""Path to kfctl."")\n\n  parser.addoption(\n      ""--namespace"", action=""store"", default=""kubeflow"",\n      help=""Namespace to use."")\n\n  parser.addoption(\n      ""--project"", action=""store"", default=""kubeflow-ci-deployment"",\n      help=""GCP project to deploy Kubeflow to"")\n  \n  parser.addoption(\n      ""--config_path"", action=""store"", default="""",\n      help=""The config to use for kfctl init"")\n  parser.addoption(\n      ""--build_and_apply"", action=""store"", default=""False"",\n      help=""Whether to build and apply or apply in kfctl""\n  )\n  # TODO(jlewi): This flag is deprecated this should be determined now from the KFDef spec.\n  parser.addoption(\n      ""--use_basic_auth"", action=""store"", default=""False"",\n      help=""Use basic auth."")\n\n  # TODO(jlewi): This flag is deprecated this should be determined now from the KFDef spec\n  parser.addoption(\n      ""--use_istio"", action=""store"", default=""True"",\n      help=""Use istio."")\n\n  parser.addoption(\n      ""--cluster_creation_script"", action=""store"", default="""",\n      help=""The script to use to create a K8s cluster before running kfctl."")\n\n  parser.addoption(\n      ""--cluster_deletion_script"", action=""store"", default="""",\n      help=""The script to use to delete a K8s cluster before running kfctl."")\n\n@pytest.fixture\ndef app_path(request):\n  return request.config.getoption(""--app_path"")\n\n@pytest.fixture\ndef app_name(request):\n  return request.config.getoption(""--app_name"")\n\n@pytest.fixture\ndef kfctl_path(request):\n  return request.config.getoption(""--kfctl_path"")\n\n@pytest.fixture\ndef namespace(request):\n  return request.config.getoption(""--namespace"")\n\n@pytest.fixture\ndef project(request):\n  return request.config.getoption(""--project"")\n\n@pytest.fixture\ndef config_path(request):\n  return request.config.getoption(""--config_path"")\n\n@pytest.fixture\ndef cluster_creation_script(request):\n  return request.config.getoption(""--cluster_creation_script"")\n\n@pytest.fixture\ndef cluster_deletion_script(request):\n  return request.config.getoption(""--cluster_deletion_script"")\n\n@pytest.fixture\ndef build_and_apply(request):\n  value = request.config.getoption(""--build_and_apply"").lower()\n\n  if value in [""t"", ""true""]:\n    return True\n  else:\n    return False\n\n\n@pytest.fixture\ndef use_basic_auth(request):\n  value = request.config.getoption(""--use_basic_auth"").lower()\n\n  if value in [""t"", ""true""]:\n    return True\n  else:\n    return False\n\n@pytest.fixture\ndef use_istio(request):\n  value = request.config.getoption(""--use_istio"").lower()\n\n  if value in [""t"", ""true""]:\n    return True\n  else:\n    return False\n'"
testing/kfctl/endpoint_ready_test.py,0,"b'import datetime\nimport json\nimport logging\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom retrying import retry\n\nimport pytest\n\nfrom kubeflow.testing import util\nfrom testing import deploy_utils\nfrom testing import gcp_util\n\n# There\'s really no good reason to run test_endpoint during presubmits.\n# We shouldn\'t need it to feel confident that kfctl is working.\n@pytest.mark.skipif(os.getenv(""JOB_TYPE"") == ""presubmit"",\n                    reason=""test endpoint doesn\'t run in presubmits"")\ndef test_endpoint_is_ready(record_xml_attribute, project, app_path, app_name, use_basic_auth):\n  """"""Test that Kubeflow was successfully deployed.\n\n  Args:\n    project: The gcp project that we deployed kubeflow\n    app_name: The name of the kubeflow deployment\n  """"""\n  util.set_pytest_junit(record_xml_attribute, ""test_endpoint_is_ready"")\n\n  url = ""https://{}.endpoints.{}.cloud.goog"".format(app_name, project)\n  if use_basic_auth:\n    with open(os.path.join(app_path, ""login.json""), ""r"") as f:\n      login = json.load(f)\n      # Let it fail if login info cannot be found.\n      username = login[""username""]\n      password = login[""password""]\n    if not gcp_util.basic_auth_is_ready(url, username, password):\n      raise Exception(""Basic auth endpoint is not ready"")\n  else:\n    # Owned by project kubeflow-ci-deployment.\n    os.environ[""CLIENT_ID""] = ""29647740582-7meo6c7a9a76jvg54j0g2lv8lrsb4l8g.apps.googleusercontent.com""\n    if not gcp_util.iap_is_ready(url):\n      raise Exception(""IAP endpoint is not ready"")\n\nif __name__ == ""__main__"":\n  logging.basicConfig(level=logging.INFO,\n                      format=(\'%(levelname)s|%(asctime)s\'\n                              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n                      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                      )\n  logging.getLogger().setLevel(logging.INFO)\n  pytest.main()\n'"
testing/kfctl/kf_is_ready_test.py,0,"b'import datetime\nimport logging\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nimport yaml\nfrom retrying import retry\n\nimport googleapiclient.discovery\nfrom oauth2client.client import GoogleCredentials\n\nimport pytest\n\nfrom kubeflow.testing import util\nfrom testing import deploy_utils\n\ndef set_logging():\n  logging.basicConfig(level=logging.INFO,\n                      format=(\'%(levelname)s|%(asctime)s\'\n                              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n                      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                      )\n  logging.getLogger().setLevel(logging.INFO)\n\ndef get_platform_app_name(app_path):\n  with open(os.path.join(app_path, ""tmp.yaml"")) as f:\n    kfdef = yaml.safe_load(f)\n  app_name = kfdef[""metadata""][""name""]\n  platform = """"\n  apiVersion = kfdef[""apiVersion""].strip().split(""/"")\n  if len(apiVersion) != 2:\n    raise RuntimeError(""Invalid apiVersion: "" + kfdef[""apiVersion""].strip())\n  if apiVersion[-1] == ""v1alpha1"":\n    platform = kfdef[""spec""][""platform""]\n  elif apiVersion[-1] == ""v1beta1"":\n    for plugin in kfdef[""spec""].get(""plugins"", []):\n      if plugin.get(""kind"", """") == ""KfGcpPlugin"":\n        platform = ""gcp""\n      elif plugin.get(""kind"", """") == ""KfExistingArriktoPlugin"":\n        platform = ""existing_arrikto""\n  else:\n    raise RuntimeError(""Unknown version: "" + apiVersion[-1])\n  return platform, app_name\n\n\ndef test_katib_is_ready(record_xml_attribute, namespace):\n  """"""Test that Kubeflow was successfully deployed.\n\n  Args:\n    namespace: The namespace Kubeflow is deployed to.\n  """"""\n  set_logging()\n  util.set_pytest_junit(record_xml_attribute, ""test_katib_is_ready"")\n\n  # Need to activate account for scopes.\n  if os.getenv(""GOOGLE_APPLICATION_CREDENTIALS""):\n    util.run([""gcloud"", ""auth"", ""activate-service-account"",\n              ""--key-file="" + os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]])\n\n  api_client = deploy_utils.create_k8s_client()\n\n  util.load_kube_config()\n\n  deployment_names = [\n    ""katib-controller"",\n    ""katib-db"",\n    ""katib-manager"",\n    ""katib-ui"",\n  ]\n  for deployment_name in deployment_names:\n    logging.info(""Verifying that deployment %s started..."", deployment_name)\n    util.wait_for_deployment(api_client, namespace, deployment_name, 10)\n\n\ndef test_kf_is_ready(record_xml_attribute, namespace, use_basic_auth, use_istio,\n                     app_path):\n  """"""Test that Kubeflow was successfully deployed.\n\n  Args:\n    namespace: The namespace Kubeflow is deployed to.\n  """"""\n  set_logging()\n  util.set_pytest_junit(record_xml_attribute, ""test_kf_is_ready"")\n\n  # Need to activate account for scopes.\n  if os.getenv(""GOOGLE_APPLICATION_CREDENTIALS""):\n    util.run([""gcloud"", ""auth"", ""activate-service-account"",\n              ""--key-file="" + os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]])\n\n  api_client = deploy_utils.create_k8s_client()\n\n  util.load_kube_config()\n\n  # Verify that components are actually deployed.\n  # TODO(jlewi): We need to parameterize this list based on whether\n  # we are using IAP or basic auth.\n  # TODO(yanniszark): This list is incomplete and missing a lot of components.\n  deployment_names = [\n      ""argo-ui"",\n      ""centraldashboard"",\n      ""jupyter-web-app-deployment"",\n      ""minio"",\n      ""ml-pipeline"",\n      ""ml-pipeline-persistenceagent"",\n      ""ml-pipeline-scheduledworkflow"",\n      ""ml-pipeline-ui"",\n      ""ml-pipeline-viewer-controller-deployment"",\n      ""mysql"",\n      ""notebook-controller-deployment"",\n      ""profiles-deployment"",\n      ""pytorch-operator"",\n      ""tf-job-operator"",\n      ""workflow-controller"",\n  ]\n\n  stateful_set_names = []\n\n  platform, _ = get_platform_app_name(app_path)\n\n  ingress_related_deployments = [\n    ""istio-egressgateway"",\n    ""istio-ingressgateway"",\n    ""istio-pilot"",\n    ""istio-policy"",\n    ""istio-sidecar-injector"",\n    ""istio-telemetry"",\n    ""istio-tracing"",\n    ""prometheus"",\n  ]\n  ingress_related_stateful_sets = []\n\n  knative_namespace = ""knative-serving""\n  knative_related_deployments = [\n          ""activator"",\n          ""autoscaler"",\n          ""controller"",\n  ]\n\n  if platform == ""gcp"":\n    deployment_names.extend([""cloud-endpoints-controller""])\n    stateful_set_names.extend([""kfserving-controller-manager""])\n    if use_basic_auth:\n      deployment_names.extend([""basic-auth-login""])\n      ingress_related_stateful_sets.extend([""backend-updater""])\n    else:\n      ingress_related_deployments.extend([""iap-enabler""])\n      ingress_related_stateful_sets.extend([""backend-updater""])\n  elif platform == ""existing_arrikto"":\n    deployment_names.extend([""dex""])\n    ingress_related_deployments.extend([""authservice""])\n    knative_related_deployments = []\n\n\n  # TODO(jlewi): Might want to parallelize this.\n  for deployment_name in deployment_names:\n    logging.info(""Verifying that deployment %s started..."", deployment_name)\n    util.wait_for_deployment(api_client, namespace, deployment_name, 10)\n\n  ingress_namespace = ""istio-system"" if use_istio else namespace\n  for deployment_name in ingress_related_deployments:\n    logging.info(""Verifying that deployment %s started..."", deployment_name)\n    util.wait_for_deployment(api_client, ingress_namespace, deployment_name, 10)\n\n\n  all_stateful_sets = [(namespace, name) for name in stateful_set_names]\n  all_stateful_sets.extend([(ingress_namespace, name) for name in ingress_related_stateful_sets])\n\n  for ss_namespace, name in all_stateful_sets:\n    logging.info(""Verifying that stateful set %s.%s started..."", ss_namespace, name)\n    try:\n      util.wait_for_statefulset(api_client, ss_namespace, name)\n    except:\n      # Collect debug information by running describe\n      util.run([""kubectl"", ""-n"", ss_namespace, ""describe"", ""statefulsets"", name])\n      raise\n\n  # TODO(jlewi): We should verify that the ingress is created and healthy.\n\n  for deployment_name in knative_related_deployments:\n    logging.info(""Verifying that deployment %s started..."", deployment_name)\n    util.wait_for_deployment(api_client, knative_namespace, deployment_name, 10)\n\n\ndef test_gcp_access(record_xml_attribute, namespace, app_path, project):\n  """"""Test that Kubeflow gcp was configured with workload_identity and GCP service account credentails.\n\n  Args:\n    namespace: The namespace Kubeflow is deployed to.\n  """"""\n  set_logging()\n  util.set_pytest_junit(record_xml_attribute, ""test_gcp_access"")\n\n  # Need to activate account for scopes.\n  if os.getenv(""GOOGLE_APPLICATION_CREDENTIALS""):\n    util.run([""gcloud"", ""auth"", ""activate-service-account"",\n              ""--key-file="" + os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]])\n\n  api_client = deploy_utils.create_k8s_client()\n\n  platform, app_name = get_platform_app_name(app_path)\n  if platform == ""gcp"":\n    # check secret\n    util.check_secret(api_client, namespace, ""user-gcp-sa"")\n\n    cred = GoogleCredentials.get_application_default()\n    # Create the Cloud IAM service object\n    service = googleapiclient.discovery.build(\'iam\', \'v1\', credentials=cred)\n\n    userSa = \'projects/%s/serviceAccounts/%s-user@%s.iam.gserviceaccount.com\' % (project, app_name, project)\n    adminSa = \'serviceAccount:%s-admin@%s.iam.gserviceaccount.com\' % (app_name, project)\n\n    request = service.projects().serviceAccounts().getIamPolicy(resource=userSa)\n    response = request.execute()\n    roleToMembers = {}\n    for binding in response[\'bindings\']:\n      roleToMembers[binding[\'role\']] = set(binding[\'members\'])\n\n    if \'roles/owner\' not in roleToMembers:\n      raise Exception(""roles/owner missing in iam-policy of %s"" % userSa)\n\n    if adminSa not in roleToMembers[\'roles/owner\']:\n      raise Exception(""Admin %v should be owner of user %s"" % (adminSa, userSa))\n\n    workloadIdentityRole = \'roles/iam.workloadIdentityUser\'\n    if workloadIdentityRole not in roleToMembers:\n      raise Exception(""roles/iam.workloadIdentityUser missing in iam-policy of %s"" % userSa)\n\n\nif __name__ == ""__main__"":\n  logging.basicConfig(level=logging.INFO,\n                      format=(\'%(levelname)s|%(asctime)s\'\n                              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n                      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                      )\n  logging.getLogger().setLevel(logging.INFO)\n  pytest.main()\n'"
testing/kfctl/kfctl_delete_test.py,0,"b'""""""Run kfctl delete as a pytest.\n\nWe use this in order to generate a junit_xml file.\n""""""\nimport datetime\nimport logging\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom retrying import retry\n\nimport pytest\n\nfrom kubeflow.testing import util\nfrom googleapiclient import discovery\nfrom oauth2client.client import GoogleCredentials\n\n# TODO(gabrielwen): Move this to a separate test ""kfctl_go_check_post_delete""\ndef get_endpoints_list(project):\n  cred = GoogleCredentials.get_application_default()\n  services_mgt = discovery.build(\'servicemanagement\', \'v1\', credentials=cred, cache_discovery=False)\n  services = services_mgt.services()\n  next_page_token = None\n  endpoints = []\n\n  while True:\n    results = services.list(producerProjectId=project,\n                            pageToken=next_page_token).execute()\n\n    for s in results.get(""services"", {}):\n      name = s.get(""serviceName"", """")\n      endpoints.append(name)\n    if not ""nextPageToken"" in results:\n      break\n    next_page_token = results[""nextPageToken""]\n\n  return endpoints\n\n# TODO(https://github.com/kubeflow/kfctl/issues/56): test_kfctl_delete is flaky\n# and more importantly failures block upload of GCS artifacts so for now we mark\n# it as expected to fail.\n@pytest.mark.xfail\ndef test_kfctl_delete(record_xml_attribute, kfctl_path, app_path, project,\n                      cluster_deletion_script):\n  util.set_pytest_junit(record_xml_attribute, ""test_kfctl_delete"")\n\n  # TODO(yanniszark): split this into a separate workflow step\n  if cluster_deletion_script:\n    logging.info(""cluster_deletion_script specified: %s"", cluster_deletion_script)\n    util.run([""/bin/bash"", ""-c"", cluster_deletion_script])\n    return\n\n  if not kfctl_path:\n    raise ValueError(""kfctl_path is required"")\n\n  if not app_path:\n    raise ValueError(""app_path is required"")\n\n  logging.info(""Using kfctl path %s"", kfctl_path)\n  logging.info(""Using app path %s"", app_path)\n\n  # We see failures because delete will try to update the IAM policy which only allows\n  # 1 update at a time. To deal with this we do retries.\n  # This has a potential downside of hiding errors that are fixed by retrying.\n  @retry(stop_max_delay=60*3*1000)\n  def run_delete():\n    util.run([kfctl_path, ""delete"", ""--delete_storage"", ""-V"", ""-f"", os.path.join(app_path, ""tmp.yaml"")],\n             cwd=app_path)\n\n  run_delete()\n\n  # Use services.list instead of services.get because error returned is not\n  # 404, it\'s 403 which is confusing.\n  name = os.path.basename(app_path)\n  endpoint_name = ""{deployment}.endpoints.{project}.cloud.goog"".format(\n      deployment=name,\n      project=project)\n  logging.info(""Verify endpoint service is deleted: "" + endpoint_name)\n  if endpoint_name in get_endpoints_list(project):\n    msg = ""Endpoint is not deleted: "" + endpoint_name\n    logging.error(msg)\n    raise AssertionError(msg)\n  else:\n    logging.info(""Verified endpoint service is deleted."")\n\nif __name__ == ""__main__"":\n  logging.basicConfig(level=logging.INFO,\n                      format=(\'%(levelname)s|%(asctime)s\'\n                              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n                      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                      )\n  logging.getLogger().setLevel(logging.INFO)\n  pytest.main()\n'"
testing/kfctl/kfctl_go_test.py,0,"b'import logging\nimport os\n\nimport pytest\n\nfrom kubeflow.kubeflow.ci import kfctl_go_test_utils as kfctl_util\nfrom kubeflow.testing import util\n\ndef test_build_kfctl_go(record_xml_attribute, app_path, project, use_basic_auth,\n                        use_istio, config_path, build_and_apply,\n                        cluster_creation_script):\n  """"""Test building and deploying Kubeflow.\n\n  Args:\n    app_path: The path to the Kubeflow app.\n    project: The GCP project to use.\n    use_basic_auth: Whether to use basic_auth.\n    use_istio: Whether to use Istio or not\n    config_path: Path to the KFDef spec file.\n    cluster_creation_script: script invoked to create a new cluster\n    build_and_apply: whether to build and apply or apply\n  """"""\n  util.set_pytest_junit(record_xml_attribute, ""test_build_kfctl_go"")\n\n  # Need to activate account for scopes.\n  if os.getenv(""GOOGLE_APPLICATION_CREDENTIALS""):\n    util.run([\n        ""gcloud"", ""auth"", ""activate-service-account"",\n        ""--key-file="" + os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]\n    ])\n\n  # TODO(yanniszark): split this into a separate workflow step\n  if cluster_creation_script:\n      logging.info(""Cluster creation script specified: %s"", cluster_creation_script)\n      util.run([""/bin/bash"", ""-c"", cluster_creation_script])\n\n\n  kfctl_path = kfctl_util.build_kfctl_go()\n  app_path = kfctl_util.kfctl_deploy_kubeflow(\n                  app_path, project, use_basic_auth,\n                  use_istio, config_path, kfctl_path, build_and_apply)\n  if not cluster_creation_script:\n      kfctl_util.verify_kubeconfig(app_path)\n\nif __name__ == ""__main__"":\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.getLogger().setLevel(logging.INFO)\n  pytest.main()\n'"
testing/kfctl/kfctl_second_apply.py,0,"b'import logging\nimport os\n\nimport pytest\n\nfrom kubeflow.kubeflow.ci import kfctl_go_test_utils as kfctl_util\nfrom kubeflow.testing import util\n\n\n@pytest.mark.skipif(os.getenv(""JOB_TYPE"") == ""presubmit"",\n                    reason=""test second apply doesn\'t run in presubmits"")\ndef test_second_apply(record_xml_attribute, app_path):\n  """"""Test that we can run kfctl apply again with error.\n\n  Args:\n    kfctl_path: The path to kfctl binary.\n    app_path: The app dir of kubeflow deployment.\n  """"""\n  _, kfctl_path = kfctl_util.get_kfctl_go_build_dir_binary_path()\n  if not os.path.exists(kfctl_path):\n    msg = ""kfctl Go binary not found: {path}"".format(path=kfctl_path)\n    logging.error(msg)\n    raise RuntimeError(msg)\n  util.run([kfctl_path, ""apply"", ""-V"", ""-f="" + os.path.join(app_path, ""tmp.yaml"")], cwd=app_path)\n'"
tf-controller-examples/tf-cnn/create_job_specs.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2017 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A simple script to generate TfJob templates based on various parameters.""""""\n\nimport argparse\nimport datetime\nimport logging\nimport yaml\n\nTF_JOB_GROUP = ""tensorflow.org""\nTF_JOB_VERSION = ""v1alpha1""\nTF_JOB_PLURAL = ""tfjobs""\nTF_JOB_KIND = ""TfJob""\n\n\n# See\n# https://stackoverflow.com/questions/21016220/is-it-possible-to-emit-valid-yaml-with-anchors-references-disabled-using-ruby  # noqa: E501\nclass ExplicitDumper(yaml.SafeDumper):\n  """"""A dumper that will never emit aliases.""""""\n\n  def ignore_aliases(self, data):\n    return True\n\n\nif __name__ == ""__main__"":\n  logging.getLogger().setLevel(logging.INFO)  # pylint: disable=too-many-locals\n  parser = argparse.ArgumentParser(description=""Create TfJob specs."")\n\n  parser.add_argument(\n      ""--cpu_image"",\n      type=str,\n      required=True,\n      help=""The docker image for CPU jobs."")\n\n  parser.add_argument(\n      ""--gpu_image"",\n      type=str,\n      required=True,\n      help=""The docker image for GPU jobs."")\n\n  parser.add_argument(\n      ""--num_workers"",\n      type=int,\n      default=1,\n      help=""The number of workers to use."")\n\n  parser.add_argument(\n      ""--output"",\n      type=str,\n      help=""(Optional) the file to write the template to."")\n\n  parser.add_argument(\n      ""--gpu"", dest=""use_gpu"", action=""store_true"", help=""Use gpus."")\n  parser.add_argument(\n      ""--no-gpu"", dest=""use_gpu"", action=""store_false"", help=""Do not use gpus."")\n\n  parser.set_defaults(use_gpu=True)\n\n  args = parser.parse_args()\n\n  namespace = ""default""\n  job_name = ""inception-"" + datetime.datetime.now().strftime(""%y%m%d-%H%M%S"")\n  if args.use_gpu:\n    job_name += ""-gpu""\n  else:\n    job_name += ""-cpu""\n\n  job_name += ""-{0}"".format(args.num_workers)\n\n  body = {}\n  body[\'apiVersion\'] = TF_JOB_GROUP + ""/"" + TF_JOB_VERSION\n  body[\'kind\'] = TF_JOB_KIND\n  body[\'metadata\'] = {}\n  body[\'metadata\'][\'name\'] = job_name\n  body[\'metadata\'][\'namespace\'] = namespace\n\n  clone_on_cpu = not args.use_gpu\n\n  body[""spec""] = {}\n  body[""spec""][""replicaSpecs""] = []\n\n  working_dir = ""/opt/tf-benchmarks/scripts/tf_cnn_benchmarks""\n\n  num_workers = args.num_workers\n  num_ps = 1\n\n  command = [\n      ""python"",\n      ""tf_cnn_benchmarks.py"",\n      ""--batch_size=32"",\n      ""--model=resnet50"",\n      ""--variable_update=parameter_server"",\n      # tf_cnn_benchmarks uses print for logging and if we don\'t set\n      # flush_stdout the buffer isn\'t outputted until the program ends..\n      ""--flush_stdout=true"",\n  ]\n\n  if args.use_gpu:\n    command.append(""--num_gpus=1"")\n  else:\n    # We need to set num_gpus=1 even if not using GPUs because otherwise\n    # the devie list is empty because of this code\n    # https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/benchmark_cnn.py#L775  # noqa: E501\n    command.append(""--num_gpus=1"")\n    command.append(""--local_parameter_device=cpu"")\n    command.append(""--device=cpu"")\n    command.append(""--data_format=NHWC"")\n\n  # Add the master spec. The master only acts as the chief and doesn\'t do\n  # any training so it can always use the CPU image.\n  master_spec = {\n      ""replicas"": 1,\n      ""tfReplicaType"": ""MASTER"",\n      ""template"": {\n          ""spec"": {\n              ""containers"": [{\n                  ""image"": args.cpu_image,\n                  ""name"": ""tensorflow"",\n                  ""workingDir"": working_dir,\n                  ""args"": command,\n              }],\n              ""restartPolicy"":\n              ""OnFailure"",\n          }\n      }\n  }\n\n  body[""spec""][""replicaSpecs""].append(master_spec)\n\n  worker_image = args.cpu_image\n  if args.use_gpu:\n    worker_image = args.gpu_image\n\n  worker_spec = {\n      ""replicas"": num_workers,\n      ""tfReplicaType"": ""WORKER"",\n      ""template"": {\n          ""spec"": {\n              ""containers"": [{\n                  ""image"": worker_image,\n                  ""name"": ""tensorflow"",\n                  ""workingDir"": working_dir,\n                  ""args"": command,\n              }],\n              ""restartPolicy"":\n              ""OnFailure"",\n          }\n      }\n  }\n\n  if args.use_gpu:\n    worker_spec[""template""][""spec""][""containers""][0][""resources""] = {\n        ""limits"": {\n            ""nvidia.com/gpu"": 1,\n        }\n    }\n\n  body[""spec""][""replicaSpecs""].append(worker_spec)\n\n  ps_spec = {\n      ""replicas"": num_ps,\n      ""tfReplicaType"": ""PS"",\n      ""template"": {\n          ""spec"": {\n              ""containers"": [{\n                  ""image"": args.cpu_image,\n                  ""name"": ""tensorflow"",\n                  ""workingDir"": working_dir,\n                  ""args"": command,\n              }],\n              ""restartPolicy"":\n              ""OnFailure"",\n          }\n      }\n  }\n\n  body[""spec""][""replicaSpecs""].append(ps_spec)\n\n  body[""spec""][""tfImage""] = args.cpu_image\n\n  # Tensorboard is crashing with TF 1.5\n  # body[""spec""][""tensorBoard""] = {\n  #   ""logDir"": job_dir\n  # }\n\n  spec = yaml.dump(body, Dumper=ExplicitDumper, default_flow_style=False)\n\n  if args.output:\n    logging.info(""Writing to %s"", args.output)\n    with open(args.output, ""w"") as hf:\n      hf.write(spec)\n  else:\n    print(spec)\n'"
tf-controller-examples/tf-cnn/launcher.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2017 The Kubeflow Authors All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA launcher suitable for invoking tf_cnn_benchmarks using TfJob.\n\nAll the launcher does is turn TF_CONFIG environment variable into extra\narguments to append to the command line.\n""""""\nimport json\nimport logging\nimport os\nimport subprocess\nimport sys\nimport time\n\n\ndef run_and_stream(cmd):\n  logging.info(""Running %s"", "" "".join(cmd))\n  process = subprocess.Popen(\n      cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\n  while process.poll() is None:\n    process.stdout.flush()\n    if process.stderr:\n      process.stderr.flush()\n    sys.stderr.flush()\n    sys.stdout.flush()\n    for line in iter(process.stdout.readline, b\'\'):\n      process.stdout.flush()\n      logging.info(line.strip())\n\n  sys.stderr.flush()\n  sys.stdout.flush()\n  process.stdout.flush()\n  if process.stderr:\n    process.stderr.flush()\n  for line in iter(process.stdout.readline, b\'\'):\n    logging.info(line.strip())\n\n  if process.returncode != 0:\n    raise ValueError(""cmd: {0} exited with code {1}"".format(\n        "" "".join(cmd), process.returncode))\n\n\nif __name__ == ""__main__"":\n  logging.getLogger().setLevel(logging.INFO)\n  logging.basicConfig(\n      level=logging.INFO,\n      format=(\'%(levelname)s|%(asctime)s\'\n              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n  )\n  logging.info(""Launcher started."")\n  tf_config = os.environ.get(\'TF_CONFIG\', \'{}\')\n  tf_config_json = json.loads(tf_config)\n  cluster = tf_config_json.get(\'cluster\', {})\n  job_name = tf_config_json.get(\'task\', {}).get(\'type\', """")\n  task_index = tf_config_json.get(\'task\', {}).get(\'index\', """")\n\n  command = sys.argv[1:]\n  ps_hosts = "","".join(cluster.get(""ps"", []))\n  worker_hosts = "","".join(cluster.get(""worker"", []))\n  command.append(""--job_name="" + job_name)\n  command.append(""--ps_hosts="" + ps_hosts)\n  command.append(""--worker_hosts="" + worker_hosts)\n  command.append(""--task_index={0}"".format(task_index))\n\n  logging.info(""Command to run: %s"", "" "".join(command))\n  with open(""/opt/run_benchmarks.sh"", ""w"") as hf:\n    hf.write(""#!/bin/bash\\n"")\n    hf.write("" "".join(command))\n    hf.write(""\\n"")\n\n  run_and_stream(command)\n  logging.info(""Finished: %s"", "" "".join(command))\n  # We don\'t want to terminate because TfJob will just restart the job.\n  while True:\n    logging.info(""Command ran successfully sleep for ever."")\n    time.sleep(600)\n'"
components/jupyter-web-app/backend/main.py,0,"b'import os\nimport sys\nimport logging\nfrom flask_cors import CORS\nfrom kubeflow_jupyter.common import settings\nfrom kubeflow_jupyter.default.app import app as default\nfrom kubeflow_jupyter.rok.app import app as rok\n\nlogger = logging.getLogger(""entrypoint"")\n\n# Get the UIs\nui = os.environ.get(""UI"", ""default"")\napps = {\n    ""default"": default,\n    ""rok"": rok\n}\n\ntry:\n    app = apps[ui]\n\n    if ""--dev"" in sys.argv:\n        settings.DEV_MODE = True\n\n        logger.warning(""Enabling CORS"")\n        CORS(app)\n\n    app.run(host=""0.0.0.0"")\nexcept KeyError:\n    logger.warning(""There is no "" + ui + "" UI to load."")\n    exit(1)\n'"
components/notebook-controller/loadtest/start_notebooks.py,0,"b""# This script aims to load test Kubeflow Notebook controller by starting\n# certain nubmer of Kubeflow Notebook custom resources.\n#\n# Before the test, make sure you have connected to your desired Kubeflow cluster\n# and have enough Kubernetes resources (or have autoscaling turned on).\n#\n# To start the load test, you can run\n#   python3.8 start_notebooks.py -l <#notebooks> -n <namespace>\n#\n# After the test, you can delete the resources via the following command\n#   python3.8 start_notebooks.py -l <#notebooks> -n <namespace> -p delete\n\nimport argparse\nimport subprocess\nimport yaml\n\nparser = argparse.ArgumentParser(\n    description='Validate all URLs in the kubeflow.org website'\n)\n\nparser.add_argument(\n    '-l',\n    '--load',\n    dest='num_notebooks',\n    nargs='?',\n    default=3,\n    type=int,\n    help='Number of notebooks to start the load test. (Default: %(default)s)',\n)\n\nparser.add_argument(\n    '-n',\n    '--namespace',\n    dest='namespace',\n    nargs='?',\n    default='kubeflow',\n    help='Namespace to start the workload. (Default: %(default)s)',\n)\n\nparser.add_argument(\n    '-p',\n    '--operation',\n    dest='operation',\n    nargs='?',\n    default='apply',\n    help='\\'apply\\' or \\'delete\\'. (Default: %(default)s)',\n)\n\n\ndef write_notebook_config(config, name, num):\n  config['metadata']['name'] = 'jupyter-test-' + str(num)\n  config['spec']['template']['spec']['containers'][0]['name'\n                                                     ] = 'notebook-' + str(num)\n  config['spec']['template']['spec']['volumes'][0]['persistentVolumeClaim'][\n      'claimName'] = 'test-vol-' + str(num)\n  with open(name, 'w') as f:\n    print(yaml.dump(config), file=f)\n\n\ndef write_pvc_config(config, name, num):\n  config['metadata']['name'] = 'test-vol-' + str(num)\n  with open(name, 'w') as f:\n    print(yaml.dump(config), file=f)\n\n\ndef main():\n  args = parser.parse_args()\n  assert args.operation == 'apply' or args.operation == 'delete'\n  notebook_config = None\n  pvc_config = None\n  with open('jupyter_test.yaml', 'r') as f:\n    notebook_config = yaml.safe_load(f.read())\n  with open('jupyter_pvc.yaml', 'r') as f:\n    pvc_config = yaml.safe_load(f.read())\n  for i in range(args.num_notebooks):\n    notebook_name = f'jupyter_test_{i}.yaml'\n    pvc_name = f'jupyter_pvc_{i}.yaml'\n    write_notebook_config(notebook_config, notebook_name, i)\n    write_pvc_config(pvc_config, pvc_name, i)\n    print(f'kubectl {args.operation} -f {notebook_name} ...')\n    subprocess.run([\n        'kubectl', args.operation, '-f', notebook_name, '-n', args.namespace\n    ],\n                   capture_output=True,\n                   check=True)\n    print(f'kubectl {args.operation} -f {pvc_name} ...')\n    subprocess.run([\n        'kubectl', args.operation, '-f', pvc_name, '-n', args.namespace\n    ],\n                   capture_output=True,\n                   check=True)\n\n\nif __name__ == '__main__':\n  main()\n"""
components/notebook-controller/third_party/concatenate_license.py,0,"b'# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport requests\nimport sys\nimport traceback\n\nparser = argparse.ArgumentParser(\n  description=\'Generate dependencies json from license.csv file.\')\nparser.add_argument(\n  \'license_info_file\',\n  nargs=\'?\',\n  default=\'license_info.csv\',\n  help=\'CSV file with license info fetched from github using get-github-license-info CLI tool.\'\n    +\'(default: %(default)s)\',\n)\nparser.add_argument(\n  \'-o\',\n  \'--output\',\n  dest=\'output_file\',\n  nargs=\'?\',\n  default=\'license.txt\',\n  help=\n  \'Concatenated license file path this command generates. (default: %(default)s)\'\n)\nargs = parser.parse_args()\n\n\ndef fetch_license_text(download_link):\n  response = requests.get(download_link)\n  assert response.ok, \'Fetching {} failed with {} {}\'.format(\n    download_link, response.status_code, response.reason)\n  return response.text\n\n\ndef main():\n  with open(args.license_info_file,\n            \'r\') as license_info_file, open(args.output_file,\n                                            \'w\') as output_file:\n    repo_failed = []\n    for line in license_info_file:\n      line = line.strip()\n      [repo, license_link, license_name,\n        license_download_link] = line.split(\',\')\n      try:\n        print(\'Repo {} has license download link {}\'.format(\n            repo, license_download_link),\n              file=sys.stderr)\n        license_text = fetch_license_text(license_download_link)\n        print(\n            \'--------------------------------------------------------------------------------\',\n            file=output_file,\n        )\n        print(\'{}  {}  {}\'.format(repo, license_name, license_link),\n              file=output_file)\n        print(\n            \'--------------------------------------------------------------------------------\',\n            file=output_file,\n        )\n        print(license_text, file=output_file)\n      except Exception as e: # pylint: disable=broad-except\n        print(\'[failed]\', e, file=sys.stderr)\n        traceback.print_exc(file=sys.stderr)\n        repo_failed.append(repo)\n    print(\'Failed to download license file for {} repos.\'.format(len(repo_failed)), file=sys.stderr)\n    for repo in repo_failed:\n      print(repo, file=sys.stderr)\n\n\nmain()\n'"
components/openmpi-controller/controller/__init__.py,0,b''
components/openmpi-controller/controller/controller.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nfrom kubernetes import client, config\nfrom kubernetes.config.config_exception import ConfigException\nfrom pathlib import Path\n\nfrom util import log, api_retry, long_poll, s3_copy\n\nSIG_DIR = \'.openmpi-controller\'\nSIGCONT = f\'{SIG_DIR}/SIGCONT\'\nSIGTERM = f\'{SIG_DIR}/SIGTERM\'\nPHASE_SUCCEEDED = \'Succeeded\'\nPHASE_FAILED = \'Failed\'\nNVIDIA_VERSION_PATH = \'/proc/driver/nvidia/version\'\n\n\nclass Controller:\n  """"""\n  Controller is a sidecar container that extends the ""main"" container\n  (openmpi-job). It communicates with the main container using a shared volume\n  mounted at the working directory. It communicates with the master pod using\n  kubernetes API.\n  """"""\n\n  def __init__(self, namespace, master, num_gpus, timeout_secs,\n               download_data_from, download_data_to, upload_data_from,\n               upload_data_to):\n    self.namespace = namespace\n    self.master = master\n    self.num_gpus = num_gpus\n    self.timeout_secs = timeout_secs\n    self.download_data_from = download_data_from\n    self.download_data_to = download_data_to\n    self.upload_data_from = upload_data_from\n    self.upload_data_to = upload_data_to\n    self._validate_args()\n    Path(SIG_DIR).mkdir()\n\n  def __enter__(self):\n    log(\'controller entered\')\n    try:\n      config.load_incluster_config()\n    except ConfigException:\n      config.load_kube_config()\n\n    self.api = client.CoreV1Api()\n    return self\n\n  def __exit__(self, exc_type, exc_val, exc_tb):\n    log(\'controller exited\')\n    Path(SIGTERM).touch()\n\n  def wait_ready(self):\n    if self.num_gpus > 0:\n      self._wait_nvidia_driver_present()\n    self._download_data()\n    Path(SIGCONT).touch()\n\n  def wait_done(self):\n    self._wait_master_terminated()\n    self._upload_data()\n\n  def _validate_args(self):\n    if (self.download_data_from and\n        self.download_data_to) or (self.upload_data_from and\n                                   self.upload_data_to):\n      if not os.environ.get(\'AWS_ACCESS_KEY_ID\'):\n        raise ValueError(\'AWS_ACCESS_KEY_ID not set\')\n\n      if not os.environ.get(\'AWS_SECRET_ACCESS_KEY\'):\n        raise ValueError(\'AWS_SECRET_ACCESS_KEY not set\')\n\n  def _wait_nvidia_driver_present(self):\n    log(\'waiting for nvidia driver to be installed\')\n    long_poll(self._poll_nvidia_driver_version, timeout_secs=self.timeout_secs)\n\n  def _wait_master_terminated(self):\n    log(\'waiting for master to terminate\')\n    long_poll(self._poll_master_phase)\n\n  def _poll_nvidia_driver_version(self):\n    # Driver installer is expected to be installed externally.\n    # See https://cloud.google.com/kubernetes-engine/docs/concepts/gpus#installing_drivers for GCP instructions.  # noqa: E501\n    version_path = Path(NVIDIA_VERSION_PATH)\n    if not version_path.exists():\n      log(f\'nvidia driver not found\')\n      return None\n    version = version_path.read_text()\n    log(f\'nvidia version: {version}\')\n    return version\n\n  def _poll_master_phase(self):\n    phase = self._query_master_phase()\n    log(f\'{self.master} is in ""{phase}"" phase\')\n    if phase not in (PHASE_SUCCEEDED, PHASE_FAILED):\n      return None\n    return phase\n\n  @api_retry\n  def _query_master_phase(self):\n    pod = self.api.read_namespaced_pod(self.master, self.namespace)\n    return pod.status.phase\n\n  def _download_data(self):\n    if self.download_data_from and self.download_data_to:\n      Path(self.download_data_to).mkdir(exist_ok=True)\n      log(f\'downloading data from {self.download_data_from} to \'\n          \'{self.download_data_to}\')\n      s3_copy(self.download_data_from, self.download_data_to)\n\n  def _upload_data(self):\n    if self.upload_data_from and self.upload_data_to:\n      if Path(self.upload_data_from).exists():\n        log(f\'uploading data from {self.upload_data_from} to \'\n            \'{self.upload_data_to}\')\n        s3_copy(self.upload_data_from, self.upload_data_to)\n'"
components/openmpi-controller/controller/main.py,0,"b""# -*- coding: utf-8 -*-\nfrom argparse import ArgumentParser\n\nfrom controller import Controller\n\n\ndef main():\n  parser = ArgumentParser()\n  parser.add_argument('--namespace', type=str, required=True)\n  parser.add_argument('--master', type=str, required=True)\n  parser.add_argument('--num-gpus', type=int, default=0)\n  parser.add_argument('--timeout-secs', type=int, default=300)\n  parser.add_argument('--download-data-from', type=str)\n  parser.add_argument('--download-data-to', type=str)\n  parser.add_argument('--upload-data-from', type=str)\n  parser.add_argument('--upload-data-to', type=str)\n  args = parser.parse_args()\n\n  with Controller(\n      namespace=args.namespace,\n      master=args.master,\n      num_gpus=args.num_gpus,\n      timeout_secs=args.timeout_secs,\n      download_data_from=args.download_data_from,\n      download_data_to=args.download_data_to,\n      upload_data_from=args.upload_data_from,\n      upload_data_to=args.upload_data_to) as ctl:\n    ctl.wait_ready()\n    ctl.wait_done()\n\n\nif __name__ == '__main__':\n  main()\n"""
components/openmpi-controller/controller/util.py,0,"b'# -*- coding: utf-8 -*-\nfrom kubernetes.client.rest import ApiException\nfrom retrying import retry\nfrom subprocess import Popen, PIPE\n\nRETRY_MAX_ATTEMPTS = 5\nRETRY_BACKOFF_MS = 1000\nPOLL_BACKOFF_MS = 10000\n\napi_retry = retry(\n    stop_max_attempt_number=RETRY_MAX_ATTEMPTS,\n    wait_exponential_multiplier=RETRY_BACKOFF_MS,\n    retry_on_exception=lambda e: isinstance(e, ApiException))\n\n\nclass S3Exception(Exception):\n  pass\n\n\ndef log(msg):\n  print(msg, flush=True)\n\n\ndef long_poll(poll_fn, timeout_secs=None):\n\n  @retry(\n      stop_max_delay=timeout_secs * 1000 if timeout_secs else None,\n      wait_fixed=POLL_BACKOFF_MS,\n      retry_on_exception=lambda _: False,\n      retry_on_result=lambda result: not result)\n  def poll_wrapper():\n    return poll_fn()\n\n  return poll_wrapper()\n\n\ndef exec_command(command):\n  process = Popen(\n      command, stdin=None, stdout=PIPE, stderr=PIPE, shell=True, close_fds=True)\n  stdout, stderr = process.communicate()\n  return process.returncode, stdout, stderr\n\n\n@retry(\n    stop_max_attempt_number=RETRY_MAX_ATTEMPTS,\n    wait_exponential_multiplier=RETRY_BACKOFF_MS,\n    retry_on_exception=lambda e: isinstance(e, S3Exception))\ndef s3_copy(copy_from, copy_to):\n  exit_code, stdout, stderr = exec_command(\n      f\'aws s3 cp --recursive ""{copy_from}"" ""{copy_to}""\')\n  if exit_code != 0:\n    raise S3Exception(f\'s3 copy failed with exit code \'\n                      \'{exit_code}:\\nstdout:{stdout}\\nstderr:{stderr}\')\n'"
py/kubeflow/kubeflow/__init__.py,0,b''
testing/kfctl/scripts/delete_existing_cluster.py,0,"b'#!/usr/bin/env python3\n\nimport os\nimport logging\nfrom googleapiclient import discovery\nfrom oauth2client.client import GoogleCredentials\nfrom kubeflow.testing import util\n\n\ndef must_getenv(name):\n    value = os.getenv(name)\n    if not name:\n        logging.fatal(""Environment variable %s is not set"", name)\n        raise ValueError()\n    return value\n\n\nif __name__ == ""__main__"":\n\n    util.run([\n        ""gcloud"", ""auth"", ""activate-service-account"", ""--key-file"",\n        must_getenv(""GOOGLE_APPLICATION_CREDENTIALS"")\n    ])\n\n    cluster_name = ""kfctl-arr-"" + must_getenv(""REPO_NAME"") + ""-"" + must_getenv(""BUILD_ID"")\n    credentials = GoogleCredentials.get_application_default()\n    service = discovery.build(\'container\', \'v1\', credentials=credentials, cache_discovery=False)\n    util.delete_cluster(service, cluster_name, ""kubeflow-ci"", ""us-central1-a"")'"
components/jupyter-web-app/backend/kubeflow_jupyter/__init__.py,0,b''
py/kubeflow/kubeflow/ci/__init__.py,0,b''
py/kubeflow/kubeflow/ci/application_util.py,0,"b'""""""Utilities for updating various Kubeflow applications.""""""\n\nimport logging\nimport os\nimport pathlib\nimport tempfile\n\nfrom kubeflow.testing import util # pylint: disable=no-name-in-module\n\nimport yaml\n\ndef set_kustomize_image(kustomize_file, image_name, image):\n  """"""Set the image using kustomize.\n\n  Args:\n    kustomize_file: Path to the kustomize file\n    image_name: The name for the image as defined in the images section\n      of the kustomization file\n    image: New image to set\n\n  Returns:\n    True if the image was updated and false other wise\n  """"""\n  kustomize_dir = os.path.dirname(kustomize_file)\n\n  with open(kustomize_file) as hf:\n    config = yaml.load(hf)\n\n  old_image = """"\n  for i in config.get(""images""):\n    if i[""name""] == image_name:\n      old_image = i.get(""newName"", image_name) + "":"" + i.get(""newTag"", """")\n      break\n\n  if old_image == image:\n    logging.info(""Not updating %s; image is already %s"", kustomize_file,\n                     image)\n\n    return False\n\n  util.run([""kustomize"", ""edit"", ""set"", ""image"",\n            ""{0}={1}"".format(image_name, image)],\n           cwd=kustomize_dir)\n\n  return True\n\ndef regenerate_manifest_tests(manifests_dir):\n  """"""Regenerate manifest tests\n\n  Args:\n    manifests_dir: Directory where kubeflow/manifests is\n      checked out\n  """"""\n  # See https://github.com/kubeflow/manifests/issues/317\n  # We can only run make generate under our GOPATH\n  # So first we have to ensure the source code is linked\n  # from our gopath.\n  go_path = os.getenv(""GOPATH"")\n\n  if not go_path:\n    raise ValueError(""GOPATH not set"")\n\n  parent_dir = os.path.join(go_path, ""src"",\n                            ""github.com"", ""kubeflow"")\n\n  if not os.path.exists(parent_dir):\n    logging.info(""Creating directory %s"", parent_dir)\n    os.makedirs(parent_dir)\n  else:\n    logging.info(""Directory %s already exists"", parent_dir)\n\n  target = os.path.join(parent_dir, ""manifests"")\n\n  if os.path.exists(target):\n    logging.info(""%s already exists"", target)\n    p = pathlib.Path(target)\n    if p.resolve() != pathlib.Path(manifests_dir):\n      raise ValueError(""%s exists but doesn\'t point to %s"",\n                       target, manifests_dir)\n  else:\n    logging.info(""Creating symlink %s -> %s"", target, manifests_dir)\n    os.symlink(manifests_dir, target)\n\n  test_dir = os.path.join(target, ""tests"")\n  with tempfile.NamedTemporaryFile(delete=False, mode=""w"") as hf:\n    hf.write(""#!/bin/bash\\n"")\n    hf.write(""set -ex\\n"")\n    hf.write(""cd {0}\\n"".format(test_dir))\n    hf.write(""make generate \\n"")\n    script = hf.name\n\n  # TODO(jlewi): This is a weird hack to run make generate for the tests.\n  # make generate needs to be run from ${GOPATH}/src/kubeflow/manifests.\n  # Simply setting cwd doesn\'t appear to impact the script; probably something\n  # to do with symlinks? So we write a simply script that executes a CD\n  # and then runs make generate.\n  util.run([""bash"", script], cwd=os.path.join(target, ""tests""))\n'"
py/kubeflow/kubeflow/ci/application_util_test.py,0,"b'import logging\nimport os\nimport shutil\nimport tempfile\nimport unittest\n\nfrom kubeflow.kubeflow.ci import application_util\nfrom kubeflow.kubeflow.ci import update_jupyter_web_app\n\nimport yaml\n\n\nclass ApplicationUttilTest(unittest.TestCase):\n  def test_set_image(self):\n    """"""Verify that we can set the image""""""\n    temp_dir = tempfile.mkdtemp()\n    this_dir = os.path.dirname(__file__)\n    test_app = os.path.join(this_dir, ""test_data"", ""test_app"")\n    logging.info(""Copying %s to %s"", test_app, temp_dir)\n    app_dir = os.path.join(temp_dir, ""test_app"")\n    shutil.copytree(test_app, app_dir)\n\n    kustomize_file = os.path.join(app_dir, ""kustomization.yaml"")\n    image_name = update_jupyter_web_app.JUPYTER_WEB_APP_IMAGE_NAME\n    new_image = ""gcr.io/newrepo/newwebapp:1.0""\n    application_util.set_kustomize_image(kustomize_file, image_name, new_image)\n\n    with open(os.path.join(app_dir, ""kustomization.yaml"")) as hf:\n      new_config = yaml.load(hf)\n\n    self.assertEqual(new_config[""images""][0][""newName""],\n                     ""gcr.io/newrepo/newwebapp"")\n    self.assertEqual(new_config[""images""][0][""newTag""], ""1.0"")\n\nif __name__ == ""__main__"":\n  logging.getLogger().setLevel(logging.INFO)\n  unittest.main()'"
py/kubeflow/kubeflow/ci/profiles_test.py,0,"b'""""""Test profiles custom resource.\n\nThis file tests Profile custom resource creation and deletion.\nCreation:\n  Reads a profile.yaml file and creates profile using create_cluster_custom_object\n  verifies Profile, namespace, serviceAccounts, rolebindings are available\n  Profile, namespace are with same names\n  ServiceAccounts: ""default-editor"" and ""default-viewer"" are created\n  Rolebindings: \'kubeflow-admin\', \'kubeflow-edit\', \'kubeflow-view\'\n                are present in the namespace\n\nDeletion:\n  Using delete_cluster_custom_object, delete profile\n  verifies Profile and namespace no longer exist.\n  For this ApiException is expected.\n\nIt is an integration test as it depends on having access to\na Kubeflow cluster with the custom resource test installed.\n\nWe use the pytest framework because\n  1. It can output results in junit format for prow/gubernator\n  2. It has good support for configuring tests using command line arguments\n    (https://docs.pytest.org/en/latest/example/simple.html)\nPython Path Requirements:\n  kubeflow/testing/py - https://github.com/kubeflow/testing/tree/master/py\n    * Provides utilities for testing\n\nManually running the test\n  1. Configure your KUBECONFIG file to point to the desired cluster\n""""""\n\nimport logging\nimport os\nimport time\n\nimport pytest\nimport yaml\n\nfrom kubernetes import client as k8s_client\nfrom kubernetes.client.rest import ApiException\nfrom kubernetes.config import kube_config\n\nfrom kubeflow.testing import util\n\nfrom retrying import retry\n\nGROUP = ""kubeflow.org""\nPLURAL = ""profiles""\nKIND = ""Profile""\nVERSION = ""v1beta1""\n\nlogging.basicConfig(level=logging.INFO,\n                    format=(\'%(levelname)s|%(asctime)s\'\n                            \'|%(pathname)s|%(lineno)d| %(message)s\'),\n                    datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                    )\nlogging.getLogger().setLevel(logging.INFO)\n\ndef deleteProfile(api_client, group, version, name):\n  k8s_co = k8s_client.CustomObjectsApi(api_client)\n  resp = k8s_co.delete_cluster_custom_object(\n      group=group,\n      version=version,\n      plural=PLURAL,\n      name=name,\n      body=k8s_client.V1DeleteOptions(),\n      grace_period_seconds=0) #zero means delete immediately\n  logging.info(""Profile deleted:\\n%s"", yaml.safe_dump(resp))\n  time.sleep(20)\n\ndef verifyProfileDeletion(api_client, group, version, name):\n  k8s_co = k8s_client.CustomObjectsApi(api_client)\n  status = \'\\""status\\"":\\""Failure\\"",\\""message\\"":\'\n  with pytest.raises(ApiException) as e:\n    resp = k8s_co.get_cluster_custom_object(\n        group=group,\n        version=version,\n        plural=PLURAL,\n        name=name)\n    logging.info(resp)\n  logging.info(""Expected exception %s\\n"", str(e.value))\n  excMsg = \'{0}\\""{1}.{2} \\\\\\""{3}\\\\\\"" not found\\""\'.format(\n             status, PLURAL, GROUP, name)\n  assert excMsg in str(e.value)\n\n  coreV1 = k8s_client.CoreV1Api(api_client)\n  with pytest.raises(ApiException) as e:\n    resp = coreV1.read_namespace(name)\n    logging.info(resp)\n  logging.info(""Expected exception %s\\n"", str(e.value))\n  excMsg = \'{0}\\""{1} \\\\\\""{2}\\\\\\"" not found\\""\'.format(\n             status, \'namespaces\', name)\n  time.sleep(30)\n  assert excMsg in str(e.value)\n\ndef verifyRolebindings(api_client, name):\n  rbacV1 = k8s_client.RbacAuthorizationV1Api(api_client)\n  rolebindingsList = rbacV1.list_namespaced_role_binding(namespace=name, watch=False)\n  rb_dict = {}\n  for i in rolebindingsList.items:\n    rb_dict[i.role_ref.name] = i.metadata.name\n\n  if {\'kubeflow-admin\', \'kubeflow-edit\', \'kubeflow-view\'} <= rb_dict.keys():\n    logging.info(""all default rolebindings are present\\n"")\n  else:\n    msg = ""Default rolebindings {0}, {1}, {2} not found\\n {3}"".format(\n            \'kubeflow-admin\', \'kubeflow-edit\', \'kubeflow-view\', rolebindingsList)\n    logging.error(msg)\n    raise RuntimeError(msg)\n\ndef verifyServiceAccounts(api_client, name):\n  #Verify if serviceAccount\'s ""default-editor"" and ""default-viewer"" are created\n  # in the \'name\' namespace\n  DEFAULT_EDITOR = ""default-editor""\n  DEFAULT_VIEWER = ""default-viewer""\n  foundDefEditor = False\n  foundDefViewer = False\n\n  coreV1 = k8s_client.CoreV1Api(api_client)\n  saList = coreV1.list_namespaced_service_account(namespace=name, watch=False)\n  for i in saList.items:\n    saName = i.metadata.name\n    if saName == DEFAULT_EDITOR:\n      foundDefEditor = True\n    elif saName == DEFAULT_VIEWER:\n      foundDefViewer = True\n    else:\n      logging.info(""found additional service account: %s\\n"", saName)\n  if (not foundDefEditor) or (not foundDefViewer):\n    msg = ""Missing default service accounts {0}, {1}\\n {2}"".format(\n            DEFAULT_EDITOR, DEFAULT_VIEWER, saList)\n    logging.error(msg)\n    raise RuntimeError(msg)\n\ndef verifyNamespaceCreation(api_client, name):\n  # Verifies the namespace is created with profile \'name\' specified.\n  coreV1 = k8s_client.CoreV1Api(api_client)\n  retry_read_namespace = retry(\n    wait_exponential_multiplier=1000,  # wait 2^i * 1000 ms, on the i-th retry\n    wait_exponential_max=60000,  # 60 sec max\n  )(coreV1.read_namespace)\n  resp = retry_read_namespace(name)\n  logging.info(""found namespace: %s"", resp)\n\ndef verifyProfileCreation(api_client, group, version, name):\n  k8s_co = k8s_client.CustomObjectsApi(api_client)\n  retry_read_profile = retry(\n    wait_exponential_multiplier=1000,  # wait 2^i * 1000 ms, on the i-th retry\n    wait_exponential_max=60000,  # 60 sec max\n  )(k8s_co.get_cluster_custom_object)\n  resp = retry_read_profile(\n      group=group,\n      version=version,\n      plural=PLURAL,\n      name=name)\n  logging.info(resp)\n\ndef createProfile(api_client, profileTestYamlFile):\n  name = \'kubeflow-user1\' # The name of the profile, also the namespace\'s name.\n\n  with open(profileTestYamlFile) as params:\n    wf_result = yaml.load(params)\n    group, version = wf_result[\'apiVersion\'].split(\'/\')\n    name = wf_result[\'metadata\'][\'name\']\n    k8s_co = k8s_client.CustomObjectsApi(api_client)\n    resp = k8s_co.create_cluster_custom_object(\n      group=group,\n      version=version,\n      plural=PLURAL,\n      body=wf_result)\n    logging.info(""Profile created:\\n%s"", yaml.safe_dump(resp))\n  # Profiles status can be one of Succeeded, Failed, Unknown\n  # TODO: check if status comes by using callbacks.\n  time.sleep(20)\n  return group, version, name\n\ndef test_profiles(record_xml_attribute, profileFile= ""test_data/profile_v1beta1_profile.yaml""):\n  util.set_pytest_junit(record_xml_attribute, ""test_profile_e2e"")\n  app_credentials = os.getenv(""GOOGLE_APPLICATION_CREDENTIALS"")\n  util.maybe_activate_service_account()\n  # util.load_kube_config appears to hang on python3\n  kube_config.load_kube_config()\n  api_client = k8s_client.ApiClient()\n  profileYamlFile = profileFile\n\n  #Profile Creation\n  group, version, name = createProfile(api_client, profileYamlFile)\n  verifyProfileCreation(api_client, group, version, name)\n  verifyNamespaceCreation(api_client, name)\n  verifyServiceAccounts(api_client, name)\n  verifyRolebindings(api_client, name)\n\n  #Profile deletion\n  deleteProfile(api_client, group, version, name)\n  verifyProfileDeletion(api_client, group, version, name)\n\nif __name__ == ""__main__"":\n  logging.basicConfig(level=logging.INFO,\n                      format=(\'%(levelname)s|%(asctime)s\'\n                              \'|%(pathname)s|%(lineno)d| %(message)s\'),\n                      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                      )\n  logging.getLogger().setLevel(logging.INFO)\n  pytest.main()\n'"
components/jupyter-web-app/backend/kubeflow_jupyter/common/__init__.py,0,b''
components/jupyter-web-app/backend/kubeflow_jupyter/common/api.py,0,"b'import json\nfrom kubernetes import client, config\nfrom kubernetes.config import ConfigException\nfrom kubernetes.client.rest import ApiException\nfrom . import auth\nfrom . import utils\n\nlogger = utils.create_logger(__name__)\n\ntry:\n    # Load configuration inside the Pod\n    config.load_incluster_config()\nexcept ConfigException:\n    # Load configuration for testing\n    config.load_kube_config()\n\n# Create the Apis\nv1_core = client.CoreV1Api()\ncustom_api = client.CustomObjectsApi()\nstorage_api = client.StorageV1Api()\n\n\ndef parse_error(e):\n    try:\n        err = json.loads(e.body)[""message""]\n    except (json.JSONDecodeError, KeyError, AttributeError):\n        err = str(e)\n\n    return err\n\n\n# Wrapper Functions for error handling\ndef wrap_resp(rsrc, fn, *args, **kwargs):\n    \'\'\'\n    rsrc: Name of the resource, used as the dict key\n    fn: function to get the resource\n    \'\'\'\n    data = {\n        ""success"": True,\n        ""log"": """"\n    }\n\n    try:\n        data[rsrc] = fn(*args, **kwargs)\n    except ApiException as e:\n        data[rsrc] = {}\n        data[""success""] = False\n        data[""log""] = parse_error(e)\n    except Exception as e:\n        data[rsrc] = {}\n        data[""success""] = False\n        data[""log""] = parse_error(e)\n\n    return data\n\n\ndef wrap(fn, *args, **kwargs):\n    \'\'\'\n    fn: function to get the resource\n    \'\'\'\n    data = {\n        ""success"": True,\n        ""log"": """"\n    }\n\n    try:\n        fn(*args, **kwargs)\n    except ApiException as e:\n        data[""success""] = False\n        data[""log""] = parse_error(e)\n    except Exception as e:\n        data[""success""] = False\n        data[""log""] = parse_error(e)\n\n    return data\n\n\n# API Functions\n# GETers\n@auth.needs_authorization(""list"", """", ""v1"", ""persistentvolumeclaims"")\ndef list_pvcs(namespace):\n    return wrap_resp(\n        ""pvcs"",\n        v1_core.list_namespaced_persistent_volume_claim,\n        namespace=namespace\n    )\n\n\n@auth.needs_authorization(""list"", ""kubeflow.org"", ""v1beta1"", ""notebooks"")\ndef list_notebooks(namespace):\n    return wrap_resp(\n        ""notebooks"",\n        custom_api.list_namespaced_custom_object,\n        ""kubeflow.org"",\n        ""v1beta1"",\n        namespace,\n        ""notebooks""\n    )\n\n\n# We don\'t do a subject access review on notebook events because\n# notebook events are cluster scoped resources. Users however are only\n# granted access to particular namespacs. We rely on the notebook webserver\n# to filter out information a user shouldn\'t see.\ndef list_notebook_events(namespace, nb_name):\n    \'\'\'\n    V1EventList with events whose source the Notebook with \'nb_name\' from namespace \'namespace\'\n    \'\'\'\n    return wrap_resp(\n        ""notebook-events"",\n        v1_core.list_namespaced_event,\n        namespace=namespace,\n        field_selector=""involvedObject.kind=Notebook,involvedObject.name="" + nb_name\n    )\n\n\n@auth.needs_authorization(""list"", ""kubeflow.org"", ""v1alpha1"", ""poddefaults"")\ndef list_poddefaults(namespace):\n    return wrap_resp(\n        ""poddefaults"",\n        custom_api.list_namespaced_custom_object,\n        ""kubeflow.org"",\n        ""v1alpha1"",\n        namespace,\n        ""poddefaults""\n    )\n\n\n@auth.needs_authorization(""get"", """", ""v1"", ""secrets"")\ndef get_secret(name, namespace):\n    return wrap_resp(\n        ""secret"",\n        v1_core.read_namespaced_secret,\n        name,\n        namespace\n    )\n\n\n@auth.needs_authorization(""list"", """", ""v1"", ""namespaces"")\ndef list_namespaces():\n    return wrap_resp(\n        ""namespaces"",\n        v1_core.list_namespace\n    )\n\n\n# @auth.needs_authorization(""list"", ""storage.k8s.io"", ""v1"", ""storageclasses"")\n# NOTE: This function is only used from the backend in order to determine if a\n# default StorageClass is set. Currently, the role aggregation does not use a\n# ClusterRoleBinding, thus we can\'t currently give this permission to a user.\n# The backend does not expose any endpoint that would allow an unauthorized\n# user to list the storage classes using this function.\ndef list_storageclasses():\n    return wrap_resp(\n        ""storageclasses"",\n        storage_api.list_storage_class\n    )\n\n\n# POSTers\n@auth.needs_authorization(""create"", ""kubeflow.org"", ""v1beta1"", ""notebooks"")\ndef create_notebook(notebook, namespace):\n    return wrap(\n        custom_api.create_namespaced_custom_object,\n        ""kubeflow.org"",\n        ""v1beta1"",\n        namespace,\n        ""notebooks"",\n        notebook\n    )\n\n\n@auth.needs_authorization(""create"", """", ""v1"", ""persistentvolumeclaims"")\ndef create_pvc(pvc, namespace):\n    return wrap_resp(\n        ""pvc"",\n        v1_core.create_namespaced_persistent_volume_claim,\n        namespace,\n        pvc\n    )\n\n# DELETErs\n@auth.needs_authorization(""delete"", ""kubeflow.org"", ""v1beta1"", ""notebooks"")\ndef delete_notebook(notebook_name, namespace):\n    return wrap(\n        custom_api.delete_namespaced_custom_object,\n        ""kubeflow.org"",\n        ""v1beta1"",\n        namespace,\n        ""notebooks"",\n        notebook_name,\n        client.V1DeleteOptions(propagation_policy=""Foreground"")\n    )\n\n\n# Readiness Probe helper\ndef can_connect_to_k8s():\n    try:\n        custom_api.list_namespaced_custom_object(\n            ""kubeflow.org"",\n            ""v1beta1"",\n            ""default"",\n            ""notebooks"",\n        )\n        return True\n\n    except ApiException:\n        return False\n\n    return True\n'"
components/jupyter-web-app/backend/kubeflow_jupyter/common/auth.py,0,"b'import functools\nfrom kubernetes import client, config\nfrom kubernetes.config import ConfigException\nfrom kubernetes.client.rest import ApiException\nfrom . import utils\nfrom . import settings\n\nlogger = utils.create_logger(__name__)\n\ntry:\n    # Load configuration inside the Pod\n    config.load_incluster_config()\nexcept ConfigException:\n    # Load configuration for testing\n    config.load_kube_config()\n\n# The API object for submitting SubjecAccessReviews\napi = client.AuthorizationV1Api()\n\n\ndef create_subject_access_review(user, verb, namespace, group, version,\n                                 resource):\n    \'\'\'\n    Create the SubjecAccessReview object which we will use to determine if the\n    user is authorized.\n    \'\'\'\n    return client.V1SubjectAccessReview(\n        spec=client.V1SubjectAccessReviewSpec(\n            user=user,\n            resource_attributes=client.V1ResourceAttributes(\n                group=group,\n                namespace=namespace,\n                verb=verb,\n                resource=resource,\n                version=version\n            )\n        )\n    )\n\n\ndef is_authorized(user, verb, namespace, group, version, resource):\n    \'\'\'\n    Create a SubjectAccessReview to the K8s API to determine if the user is\n    authorized to perform a specific verb on a resource.\n    \'\'\'\n    if settings.DEV_MODE:\n        logger.warning(\n            (""Running in developement mode. No authorization checks will be""\n             "" issued"")\n        )\n        return True\n\n    if user is None:\n        logger.warning(\n            (""No user credentials were found! Make sure you""\n             "" have correctly set the USERID_HEADER in the""\n             "" Jupyter Web App\'s deployment."")\n        )\n        return False\n\n    sar = create_subject_access_review(user, verb, namespace, group, version,\n                                       resource)\n    try:\n        obj = api.create_subject_access_review(sar)\n    except ApiException as e:\n        logger.error(\n            ""Error submitting SubjecAccessReview: {}, {}"".format(\n                sar, utils.parse_error(e))\n        )\n        return False\n\n    if obj.status is not None:\n        return obj.status.allowed\n    else:\n        logger.error(""SubjectAccessReview doesn\'t have status."")\n        return False\n\n\ndef needs_authorization(verb, group, version, resource):\n    \'\'\'\n    This function will serve as a decorator. It will be used to make sure that\n    the decorated function is authorized to perform the corresponding k8s api\n    verb on a specific resource.\n    \'\'\'\n    def wrapper(func):\n        @functools.wraps(func)\n        def runner(*args, **kwargs):\n            user = utils.get_username_from_request()\n            namespace = kwargs.get(""namespace"", None)\n\n            if is_authorized(user, verb, namespace, group, version, resource):\n                return func(*args, **kwargs)\n            else:\n                msg = (""User {} is not authorized to {} {} for namespace: ""\n                       ""{}"").format(user,\n                                    verb,\n                                    f""{group}.{version}.{resource}"",\n                                    namespace)\n                return {\n                    ""success"": False,\n                    ""log"": msg,\n                }\n\n        return runner\n\n    return wrapper\n'"
components/jupyter-web-app/backend/kubeflow_jupyter/common/base_app.py,0,"b'import datetime as dt\n\nfrom flask import jsonify, request, Blueprint\nfrom kubernetes import client\nfrom . import api\nfrom . import utils\n\n# The BaseApp is a Blueprint that other UIs will use\napp = Blueprint(""base_app"", __name__)\nlogger = utils.create_logger(__name__)\n\n\n# Helper function for getting the prefix of the webapp\ndef prefix():\n    if request.headers.get(""x-forwarded-prefix""):\n        return request.headers.get(""x-forwarded-prefix"")\n    else:\n        return """"\n\n\n# REST Routes\n@app.route(""/api/namespaces/<namespace>/notebooks"")\ndef get_notebooks(namespace):\n    data = api.list_notebooks(namespace=namespace)\n\n    if not data[""success""]:\n        return jsonify(data)\n\n    items = []\n    for nb in data[""notebooks""][""items""]:\n        nb_name = nb[""metadata""][""name""]\n        nb_creation_time = dt.datetime.strptime(\n            nb[""metadata""][""creationTimestamp""], ""%Y-%m-%dT%H:%M:%SZ"")\n        nb_events = api.list_notebook_events(namespace, nb_name)\n        if not nb_events[""success""]:\n            return jsonify(nb_events)\n        # User can delete and then create a nb server with the same name\n        # Make sure previous events are not taken into account\n        nb_events = filter(lambda e: utils.event_timestamp(e) >= nb_creation_time,\n                           nb_events[""notebook-events""].items)\n        items.append(utils.process_resource(nb, nb_events))\n\n    data[""notebooks""] = items\n    return jsonify(data)\n\n\n@app.route(""/api/namespaces/<namespace>/poddefaults"")\ndef get_poddefaults(namespace):\n    data = api.list_poddefaults(namespace=namespace)\n\n    if not data[""success""]:\n        return jsonify(data)\n\n    # Return a list of (label, desc) with the pod defaults\n    pdefaults = []\n    for pd in data[""poddefaults""][""items""]:\n        label = list(pd[""spec""][""selector""][""matchLabels""].keys())[0]\n        if ""desc"" in pd[""spec""]:\n            desc = pd[""spec""][""desc""]\n        else:\n            desc = pd[""metadata""][""name""]\n\n        pdefaults.append({""label"": label, ""desc"": desc})\n\n    logger.info(""Found poddefaults: {}"".format(pdefaults))\n    data[""poddefaults""] = pdefaults\n    return jsonify(data)\n\n\n@app.route(""/api/namespaces/<namespace>/pvcs"")\ndef get_pvcs(namespace):\n    data = api.list_pvcs(namespace=namespace)\n    if not data[""success""]:\n        return jsonify(data)\n\n    data[""pvcs""] = [utils.process_pvc(pvc) for pvc in data[""pvcs""].items]\n\n    return jsonify(data)\n\n\n@app.route(""/api/namespaces"")\ndef get_namespaces():\n    data = api.list_namespaces()\n\n    # Result must be jsonify-able\n    if data[""success""]:\n        nmsps = data[""namespaces""]\n        data[""namespaces""] = [ns.metadata.name for ns in nmsps.items]\n\n    return jsonify(data)\n\n\n@app.route(""/api/storageclasses/default"")\ndef get_default_storageclass():\n    data = api.list_storageclasses()\n    if not data[""success""]:\n        return jsonify({\n            ""success"": False,\n            ""log"": data[""log""]\n        })\n\n    strg_classes = data[""storageclasses""].items\n    for strgclss in strg_classes:\n        annotations = strgclss.metadata.annotations\n        if annotations is None:\n            continue\n\n        # List of possible annotations\n        keys = [\n            ""storageclass.kubernetes.io/is-default-class"",\n            ""storageclass.beta.kubernetes.io/is-default-class""  # GKE\n        ]\n\n        for key in keys:\n            is_default = annotations.get(key, ""false"")\n\n            if is_default == ""true"":\n                return jsonify({\n                    ""success"": True,\n                    ""defaultStorageClass"": strgclss.metadata.name\n                })\n\n    # No StorageClass is default\n    return jsonify({\n        ""success"": True,\n        ""defaultStorageClass"": """"\n    })\n\n\n@app.route(""/api/config"")\ndef get_config():\n    data = {""success"": True}\n\n    data[""config""] = utils.spawner_ui_config()\n    return jsonify(data)\n\n\n# POSTers\n@app.route(""/api/namespaces/<namespace>/pvcs"", methods=[""POST""])\ndef post_pvc(namespace):\n    body = request.get_json()\n\n    pvc = client.V1PersistentVolumeClaim(\n        metadata=client.V1ObjectMeta(\n            name=body[""name""],\n            namespace=namespace\n        ),\n        spec=client.V1PersistentVolumeClaimSpec(\n            access_modes=[body[""mode""]],\n            resources=client.V1ResourceRequirements(\n                requests={\n                    ""storage"": body[""size""] + ""Gi""\n                }\n            )\n        )\n    )\n\n    return jsonify(api.create_pvc(pvc, namespace=namespace))\n\n\n# DELETErs\n@app.route(""/api/namespaces/<namespace>/notebooks/<notebook>"",\n           methods=[""DELETE""])\ndef delete_notebook(namespace, notebook):\n    return jsonify(api.delete_notebook(notebook, namespace=namespace))\n\n\n# Liveness/Readiness Probes\n@app.route(""/healthz/liveness"", methods=[""GET""])\ndef liveness_probe():\n    return jsonify(""alive""), 200\n\n\n@app.route(""/healthz/readiness"", methods=[""GET""])\ndef readiness_probe():\n    # Check if the backend can communicate with the k8s API Server\n    if not api.can_connect_to_k8s():\n        return jsonify(""not ready""), 503\n\n    return jsonify(""ready""), 200\n\n\nif __name__ == ""__main__"":\n    app.run(host=""0.0.0.0"")\n'"
components/jupyter-web-app/backend/kubeflow_jupyter/common/settings.py,0,"b""# Variables for configuring the Backend's behavior\nDEV_MODE = False\n"""
components/jupyter-web-app/backend/kubeflow_jupyter/common/utils.py,0,"b'import datetime as dt\nimport json\nimport logging\nimport os\nimport sys\n\nimport yaml\nfrom collections import defaultdict\nfrom flask import request\nfrom kubernetes import client\n\nfrom . import api\n\n# The backend will send the first config it will successfully load\nCONFIGS = [\n    ""/etc/config/spawner_ui_config.yaml"",\n    ""./kubeflow_jupyter/common/yaml/spawner_ui_config.yaml"",\n]\n\n# The values of the headers to look for the User info\nUSER_HEADER = os.getenv(""USERID_HEADER"", ""X-Goog-Authenticated-User-Email"")\nUSER_PREFIX = os.getenv(""USERID_PREFIX"", ""accounts.google.com:"")\n\nEVENT_TYPE_NORMAL = ""Normal""\nEVENT_TYPE_WARNING = ""Warning""\n\nSTATUS_ERROR = ""error""\nSTATUS_WARNING = ""warning""\nSTATUS_RUNNING = ""running""\nSTATUS_WAITING = ""waiting""\n\n\n# Logging\ndef create_logger(name):\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setFormatter(\n        logging.Formatter(\n            ""%(asctime)s | %(name)s | %(levelname)s | %(message)s""\n        )\n    )\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n    logger.addHandler(handler)\n    return logger\n\n\nlogger = create_logger(__name__)\n\n\n# Utils\ndef get_username_from_request():\n    if USER_HEADER not in request.headers:\n        logger.debug(""User header not present!"")\n        username = None\n    else:\n        user = request.headers[USER_HEADER]\n        username = user.replace(USER_PREFIX, """")\n        logger.debug(\n            ""User: \'{}\' | Headers: \'{}\' \'{}\'"".format(\n                username, USER_HEADER, USER_PREFIX\n            )\n        )\n\n    return username\n\n\ndef load_param_yaml(f, **kwargs):\n    c = None\n    try:\n        with open(f, ""r"") as f:\n            c = f.read().format(**kwargs)\n    except IOError:\n        logger.info(""Error opening: {}"".format(f))\n        return None\n\n    try:\n        if yaml.safe_load(c) is None:\n            # YAML exists but is empty\n            return {}\n        else:\n            # YAML exists and is not empty\n            return yaml.safe_load(c)\n    except yaml.YAMLError as e:\n        logger.warning(""Couldn\'t load yaml: {}"".format(e))\n        return None\n\n\ndef spawner_ui_config():\n    for config in CONFIGS:\n        c = None\n        try:\n            with open(config, ""r"") as f:\n                c = f.read()\n        except IOError:\n            logger.warning(""Config file \'{}\' is not found"".format(config))\n            continue\n\n        try:\n            if yaml.safe_load(c) is None:\n                # YAML exists but is empty\n                return {}\n            else:\n                # YAML exists and is not empty\n                logger.info(""Sending config file \'{}\'"".format(config))\n                return yaml.safe_load(c)[""spawnerFormDefaults""]\n        except yaml.YAMLError:\n            logger.error(""Notebook config is not a valid yaml"")\n            return {}\n        except AttributeError as e:\n            logger.error(\n                ""Can\'t load the config at {}: {}"".format(config, str(e))\n            )\n\n    logger.warning(""Couldn\'t load any config"")\n    return {}\n\n\ndef get_uptime(then):\n    now = dt.datetime.now()\n    then = dt.datetime.strptime(then, ""%Y-%m-%dT%H:%M:%SZ"")\n    diff = now - then.replace(tzinfo=None)\n\n    days = diff.days\n    hours = int(diff.seconds / 3600)\n    mins = int((diff.seconds % 3600) / 60)\n\n    age = """"\n    if days > 0:\n        if days == 1:\n            age = str(days) + "" day""\n        else:\n            age = str(days) + "" days""\n    else:\n        if hours > 0:\n            if hours == 1:\n                age = str(hours) + "" hour""\n            else:\n                age = str(hours) + "" hours""\n        else:\n            if mins == 0:\n                return ""just now""\n            if mins == 1:\n                age = str(mins) + "" min""\n            else:\n                age = str(mins) + "" mins""\n\n    return age + "" ago""\n\n\ndef handle_storage_class(vol):\n    # handle the StorageClass\n    if ""class"" not in vol:\n        return None\n    if vol[""class""] == ""{empty}"":\n        return """"\n    if vol[""class""] == ""{none}"":\n        return None\n    else:\n        return vol[""class""]\n\n\n# Volume handling functions\ndef volume_from_config(config_vol, notebook):\n    """"""\n    Create a Volume Dict from the config.yaml. This dict has the same fields as\n    a Volume returned from the frontend\n    """"""\n    vol_name = config_vol[""name""][""value""].replace(\n        ""{notebook-name}"", notebook[""name""]\n    )\n    vol_class = handle_storage_class(config_vol[""class""][""value""])\n\n    return {\n        ""name"": vol_name,\n        ""type"": config_vol[""type""][""value""],\n        ""size"": config_vol[""size""][""value""],\n        ""mode"": config_vol[""accessModes""][""value""],\n        ""path"": config_vol[""mountPath""][""value""],\n        ""class"": vol_class,\n        ""extraFields"": config_vol.get(""extra"", {}),\n    }\n\n\ndef pvc_from_dict(vol, namespace):\n    if vol is None:\n        return None\n\n    return client.V1PersistentVolumeClaim(\n        metadata=client.V1ObjectMeta(name=vol[""name""], namespace=namespace,),\n        spec=client.V1PersistentVolumeClaimSpec(\n            access_modes=[vol[""mode""]],\n            storage_class_name=handle_storage_class(vol),\n            resources=client.V1ResourceRequirements(\n                requests={""storage"": vol[""size""]}\n            ),\n        ),\n    )\n\n\ndef get_workspace_vol(body, defaults):\n    """"""\n    Checks the config and the form values and returns a Volume Dict for the\n    workspace. If the workspace is readOnly, then the value from the config\n    will be used instead. The Volume Dict has the same format as the Volume\n    interface of the frontend.\n    """"""\n    default_ws = volume_from_config(defaults[""workspaceVolume""][""value""], body)\n    form_ws = body.get(""workspace"", None)\n\n    if defaults[""workspaceVolume""].get(""readOnly"", False):\n        ws = default_ws\n        logger.info(""Using the default Workspace Volume: {}"".format(ws))\n    elif form_ws is not None:\n        ws = form_ws\n        logger.info(""Using form\'s Workspace Volume: {}"".format(ws))\n    else:\n        ws = default_ws\n        logger.info(""Using the default Workspace Volume: {}"".format(ws))\n\n    return ws\n\n\ndef get_data_vols(body, defaults):\n    """"""\n    Checks the config and the form values and returns a list of Volume\n    Dictionaries for the Notebook\'s Data Volumes. If the Data Volumes are\n    readOnly, then the value from the config will be used instead. The Volume\n    Dict has the same format as the Volume interface of the frontend.\n    """"""\n    default_vols = [\n        volume_from_config(vol[""value""], body)\n        for vol in defaults[""dataVolumes""][""value""]\n    ]\n    form_vols = body.get(""datavols"", [])\n\n    if defaults[""dataVolumes""].get(""readOnly"", False):\n        vols = default_vols\n        logger.info(""Using the default Data Volumes: {}"".format(vols))\n    elif ""datavols"" in body:\n        vols = form_vols\n        logger.info(""Using the form\'s Data Volumes: {}"".format(vols))\n    else:\n        vols = default_vols\n        logger.info(""Using the default Data Volumes: {}"".format(vols))\n\n    return vols\n\n\n# Functions for transforming the data from k8s api\ndef process_pvc(rsrc):\n    # VAR: change this function according to the main resource\n    res = {\n        ""name"": rsrc.metadata.name,\n        ""namespace"": rsrc.metadata.namespace,\n        ""size"": rsrc.spec.resources.requests[""storage""],\n        ""mode"": rsrc.spec.access_modes[0],\n        ""class"": rsrc.spec.storage_class_name,\n    }\n    return res\n\n\ndef process_resource(rsrc, rsrc_events):\n    # VAR: change this function according to the main resource\n    cntr = rsrc[""spec""][""template""][""spec""][""containers""][0]\n    status, reason = process_status(rsrc, rsrc_events)\n\n    res = {\n        ""name"": rsrc[""metadata""][""name""],\n        ""namespace"": rsrc[""metadata""][""namespace""],\n        ""age"": get_uptime(rsrc[""metadata""][""creationTimestamp""]),\n        ""image"": cntr[""image""],\n        ""shortImage"": cntr[""image""].split(""/"")[-1],\n        ""cpu"": cntr[""resources""][""requests""][""cpu""],\n        ""memory"": cntr[""resources""][""requests""][""memory""],\n        ""volumes"": [v[""name""] for v in cntr[""volumeMounts""]],\n        ""status"": status,\n        ""reason"": reason,\n    }\n    return res\n\n\ndef process_status(rsrc, rsrc_events):\n    """"""\n    Return status and reason. Status may be [running|waiting|warning|error]\n    """"""\n    # If the Notebook is being deleted, the status will be waiting\n    if ""deletionTimestamp"" in rsrc[""metadata""]:\n        return STATUS_WAITING, ""Deleting Notebook Server""\n\n    # Check the status\n    try:\n        s = rsrc[""status""][""containerState""]\n    except KeyError:\n        s = """"\n\n    # Use conditions on the Jupyter notebook (i.e., s) to determine overall status\n    # If no container state is available, we try to extract information about why\n    # the notebook is not starting from the notebook\'s events (see find_error_event)\n    if ""running"" in s:\n        status, reason = STATUS_RUNNING, ""Running""\n    elif ""terminated"" in s:\n        status, reason = STATUS_ERROR, ""The Pod has Terminated""\n    else:\n        if ""waiting"" in s:\n            reason = s[""waiting""][""reason""]\n            status = STATUS_ERROR if reason == ""ImagePullBackOff"" else STATUS_WAITING\n        else:\n            status, reason = STATUS_WAITING, ""Scheduling the Pod""\n        # Provide the user with detailed information (if any) about why the notebook is not starting\n        status_event, reason_event = find_error_event(rsrc_events)\n        if status_event:\n            status, reason = status_event, reason_event\n\n    return status, reason\n\n\ndef find_error_event(rsrc_events):\n    \'\'\'\n    Returns status and reason from the latest event that surfaces the cause\n    of why the resource could not be created. For a Notebook, it can be due to:\n\n          EVENT_TYPE      EVENT_REASON      DESCRIPTION   \n          Warning         FailedCreate      pods ""x"" is forbidden: error looking up service account ... (originated in statefulset)\n          Warning         FailedScheduling  0/1 nodes are available: 1 Insufficient cpu (originated in pod)\n\n    \'\'\'\n    for e in sorted(rsrc_events, key=event_timestamp, reverse=True):\n        if e.type == EVENT_TYPE_WARNING:\n            return STATUS_WAITING, e.message\n    return None, None\n\n\ndef event_timestamp(event):\n    return event.metadata.creation_timestamp.replace(tzinfo=None)\n\n\n# Notebook YAML processing\ndef set_notebook_image(notebook, body, defaults):\n    """"""\n    If the image is set to readOnly, use only the value from the config\n    """"""\n    if defaults[""image""].get(""readOnly"", False):\n        image = defaults[""image""][""value""]\n        logger.info(""Using default Image: "" + image)\n    elif body.get(""customImageCheck"", False):\n        image = body[""customImage""]\n        logger.info(""Using form\'s custom Image: "" + image)\n    elif ""image"" in body:\n        image = body[""image""]\n        logger.info(""Using form\'s Image: "" + image)\n    else:\n        image = defaults[""image""][""value""]\n        logger.info(""Using default Image: "" + image)\n\n    notebook[""spec""][""template""][""spec""][""containers""][0][""image""] = image\n\n\ndef set_notebook_cpu(notebook, body, defaults):\n    container = notebook[""spec""][""template""][""spec""][""containers""][0]\n\n    if defaults[""cpu""].get(""readOnly"", False):\n        cpu = defaults[""cpu""][""value""]\n        logger.info(""Using default CPU: "" + cpu)\n    elif body.get(""cpu"", """"):\n        cpu = body[""cpu""]\n        logger.info(""Using form\'s CPU: "" + cpu)\n    else:\n        cpu = defaults[""cpu""][""value""]\n        logger.info(""Using default CPU: "" + cpu)\n\n    container[""resources""][""requests""][""cpu""] = cpu\n\n\ndef set_notebook_memory(notebook, body, defaults):\n    container = notebook[""spec""][""template""][""spec""][""containers""][0]\n\n    if defaults[""memory""].get(""readOnly"", False):\n        memory = defaults[""memory""][""value""]\n        logger.info(""Using default Memory: "" + memory)\n    elif body.get(""memory"", """"):\n        memory = body[""memory""]\n        logger.info(""Using form\'s Memory: "" + memory)\n    else:\n        memory = defaults[""memory""][""value""]\n        logger.info(""Using default Memory: "" + memory)\n\n    container[""resources""][""requests""][""memory""] = memory\n\n\ndef set_notebook_gpus(notebook, body, defaults):\n    gpus = None\n    gpuDefaults = defaults.get(""gpus"", {})\n    if gpuDefaults.get(""readOnly"", False):\n        # The server should not allow the user to set the GPUs\n        # if the config\'s value is readOnly. Use the config\'s value\n        gpus = gpuDefaults[""value""]\n        logger.info(f""Using default GPU config: {gpus}"")\n\n    elif ""gpus"" not in body:\n        # Try to load the default values. If they don\'t exist, don\'t use GPUs\n        if ""gpus"" not in defaults:\n            logger.info(\n                ""No \'gpus\' value in either the form\'s body or in""\n                "" the default config\'s values. Will not use any GPUs""\n            )\n            return\n        else:\n            gpus = gpuDefaults[""value""]\n            logger.info(f""Using default GPU config: {gpus}"")\n\n    else:\n        # Make sure the GPUs value in the request is properly formatted\n        gpus = body[""gpus""]\n        logger.info(f""Using form\'s GPUs: {gpus}"")\n\n        if ""num"" not in gpus:\n            logger.error(""\'gpus\' must have a \'num\' field"")\n            return\n\n        if gpus[""num""] != ""none"" and ""vendor"" not in gpus:\n            logger.error(""\'gpus\' must have a \'vendor\' field"")\n            return\n\n        if gpus[""num""] != ""none"":\n            try:\n                int(gpus[""num""])\n            except ValueError:\n                logger.error(f""gpus.num is not a valid number: {gpus[\'num\']}"")\n                return\n\n    # Add the GPU annotation\n    if gpus[""num""] == ""none"":\n        return\n\n    container = notebook[""spec""][""template""][""spec""][""containers""][0]\n    vendor = gpus[""vendor""]\n    num = int(gpus[""num""])\n\n    limits = container[""resources""].get(""limits"", {})\n    limits[vendor] = num\n\n    container[""resources""][""limits""] = limits\n\n\ndef set_notebook_configurations(notebook, body, defaults):\n    notebook_labels = notebook[""metadata""][""labels""]\n\n    if defaults[""configurations""].get(""readOnly"", False):\n        labels = defaults[""configurations""][""value""]\n        logger.info(""Using default Configurations: {}"".format(labels))\n    elif body.get(""configurations"", None) is not None:\n        labels = body[""configurations""]\n        logger.info(""Using form\'s Configurations: {}"".format(labels))\n    else:\n        labels = defaults[""configurations""][""value""]\n        logger.info(""Using default Configurations: {}"".format(labels))\n\n    if not isinstance(labels, list):\n        logger.warning(\n            ""Labels for PodDefaults are not list: {}"".format(labels)\n        )\n        return\n\n    for l in labels:\n        notebook_labels[l] = ""true""\n\n\ndef set_notebook_extra_resources(notebook, body, defaults):\n    r = {""success"": True, ""log"": """"}\n    container = notebook[""spec""][""template""][""spec""][""containers""][0]\n\n    if defaults[""extraResources""].get(""readOnly"", False):\n        resources_str = defaults[""extraResources""][""value""]\n        logger.info(""Using the default Extra Resources: "" + resources_str)\n    elif body.get(""extra"", """"):\n        resources_str = body[""extra""]\n        logger.info(""Using the form\'s Extra Resources: "" + resources_str)\n    else:\n        resources_str = defaults[""extraResources""][""value""]\n        logger.info(""Using the default Extra Resources: "" + resources_str)\n\n    try:\n        extra = json.loads(resources_str)\n    except Exception as e:\n        r[""success""] = False\n        r[""log""] = api.parse_error(e)\n        return r\n\n    container[""resources""][""limits""] = extra\n    return r\n\n\ndef set_notebook_shm(notebook, body, defaults):\n    if defaults[""shm""].get(""readOnly"", False):\n        if not defaults[""shm""][""value""]:\n            return\n    elif ""shm"" in body:\n        if not body[""shm""]:\n            return\n    else:\n        if not defaults[""shm""][""value""]:\n            return\n\n    notebook_spec = notebook[""spec""][""template""][""spec""]\n    notebook_cont = notebook[""spec""][""template""][""spec""][""containers""][0]\n\n    shm_volume = {""name"": ""dshm"", ""emptyDir"": {""medium"": ""Memory""}}\n    notebook_spec[""volumes""].append(shm_volume)\n    shm_mnt = {""mountPath"": ""/dev/shm"", ""name"": ""dshm""}\n    notebook_cont[""volumeMounts""].append(shm_mnt)\n\n\ndef add_notebook_volume(notebook, vol_name, claim, mnt_path):\n    spec = notebook[""spec""][""template""][""spec""]\n    container = notebook[""spec""][""template""][""spec""][""containers""][0]\n\n    volume = {""name"": vol_name, ""persistentVolumeClaim"": {""claimName"": claim}}\n    spec[""volumes""].append(volume)\n\n    # Container Mounts\n    mnt = {""mountPath"": mnt_path, ""name"": vol_name}\n    container[""volumeMounts""].append(mnt)\n\n\ndef add_notebook_volume_secret(nb, secret, secret_name, mnt_path, mode):\n    # Create the volume in the Pod\n    spec = nb[""spec""][""template""][""spec""]\n    container = nb[""spec""][""template""][""spec""][""containers""][0]\n\n    volume = {\n        ""name"": secret,\n        ""secret"": {""defaultMode"": mode, ""secretName"": secret_name, },\n    }\n    spec[""volumes""].append(volume)\n\n    # Container volumeMounts\n    mnt = {\n        ""mountPath"": mnt_path,\n        ""name"": secret,\n    }\n    container[""volumeMounts""].append(mnt)\n'"
components/jupyter-web-app/backend/kubeflow_jupyter/default/app.py,0,"b'from flask import Flask, request, jsonify, send_from_directory\nfrom ..common.base_app import app as base\nfrom ..common import utils, api\n\napp = Flask(__name__)\napp.register_blueprint(base)\nlogger = utils.create_logger(__name__)\n\nNOTEBOOK = ""./kubeflow_jupyter/common/yaml/notebook.yaml""\n\n\n# POSTers\n@app.route(""/api/namespaces/<namespace>/notebooks"", methods=[""POST""])\ndef post_notebook(namespace):\n    body = request.get_json()\n    defaults = utils.spawner_ui_config()\n    logger.info(""Got Notebook: {}"".format(body))\n\n    notebook = utils.load_param_yaml(NOTEBOOK,\n                                     name=body[""name""],\n                                     namespace=namespace,\n                                     serviceAccount=""default-editor"")\n\n    utils.set_notebook_image(notebook, body, defaults)\n    utils.set_notebook_cpu(notebook, body, defaults)\n    utils.set_notebook_memory(notebook, body, defaults)\n    utils.set_notebook_gpus(notebook, body, defaults)\n    utils.set_notebook_configurations(notebook, body, defaults)\n\n    # Workspace Volume\n    workspace_vol = utils.get_workspace_vol(body, defaults)\n    if not body.get(""noWorkspace"", False) and workspace_vol[""type""] == ""New"":\n        # Create the PVC\n        ws_pvc = utils.pvc_from_dict(workspace_vol, namespace)\n\n        logger.info(""Creating Workspace Volume: {}"".format(ws_pvc.to_dict()))\n        r = api.create_pvc(ws_pvc, namespace=namespace)\n        if not r[""success""]:\n            return jsonify(r)\n\n    if not body.get(""noWorkspace"", False) and workspace_vol[""type""] != ""None"":\n        utils.add_notebook_volume(\n            notebook,\n            workspace_vol[""name""],\n            workspace_vol[""name""],\n            ""/home/jovyan"",\n        )\n\n    # Add the Data Volumes\n    for vol in utils.get_data_vols(body, defaults):\n        if vol[""type""] == ""New"":\n            # Create the PVC\n            dtvol_pvc = utils.pvc_from_dict(vol, namespace)\n\n            logger.info(""Creating Data Volume {}:"".format(dtvol_pvc))\n            r = api.create_pvc(dtvol_pvc, namespace=namespace)\n            if not r[""success""]:\n                return jsonify(r)\n\n        utils.add_notebook_volume(\n            notebook,\n            vol[""name""],\n            vol[""name""],\n            vol[""path""]\n        )\n\n    # shm\n    utils.set_notebook_shm(notebook, body, defaults)\n\n    logger.info(""Creating Notebook: {}"".format(notebook))\n    return jsonify(api.create_notebook(notebook, namespace=namespace))\n\n\n# Since Angular is a SPA, we serve index.html every time\n@app.route(""/"")\ndef serve_root():\n    return send_from_directory(""./static/"", ""index.html"")\n\n\n@app.route(""/<path:path>"", methods=[""GET""])\ndef static_proxy(path):\n    logger.info(""Sending file \'/static/{}\' for path: {}"".format(path, path))\n    return send_from_directory(""./static/"", path)\n\n\n@app.errorhandler(404)\ndef page_not_found(e):\n    logger.info(""Sending file \'index.html\'"")\n    return send_from_directory(""./static/"", ""index.html"")\n'"
components/jupyter-web-app/backend/kubeflow_jupyter/rok/app.py,0,"b'import base64\nfrom flask import Flask, request, jsonify, send_from_directory\nfrom ..common.base_app import app as base\nfrom ..common import utils, api\nfrom . import rok\n\n# Use the BaseApp, override the POST Notebook Endpoint\napp = Flask(__name__)\napp.register_blueprint(base)\nlogger = utils.create_logger(__name__)\n\nNOTEBOOK = ""./kubeflow_jupyter/common/yaml/notebook.yaml""\n\n\n# GETers\n@app.route(""/api/rok/namespaces/<namespace>/token"")\ndef get_token(namespace):\n    \'\'\'Retrieve the token to authenticate with Rok.\'\'\'\n    secret = None\n    name = rok.rok_secret_name()\n    token = {\n        ""name"": name,\n        ""value"": """",\n    }\n\n    data = api.get_secret(name, namespace=namespace)\n    if not data[""success""]:\n        logger.warning(""Couldn\'t load ROK token in namespace \'{}\': {}"".format(\n            namespace, data[""log""]\n        ))\n        data[""token""] = token\n        return jsonify(data)\n\n    secret = data[""secret""]\n    if secret.data is None:\n        logger.warning(\n            ""ROK Secret doesn\'t exist in namespace \'%s\'"" % namespace\n        )\n        return jsonify({\n            ""success"": False,\n            ""log"": ""ROK Secret doesn\'t exist in namespace \'%s\'"" % namespace,\n            ""token"": token\n        })\n\n    token = secret.data.get(""token"", """")\n    data[""token""] = {\n        ""value"": base64.b64decode(token).decode(""utf-8""),\n        ""name"": name\n    }\n    del data[""secret""]\n\n    return jsonify(data)\n\n\n# POSTers\n@app.route(""/api/namespaces/<namespace>/notebooks"", methods=[""POST""])\ndef post_notebook(namespace):\n    body = request.get_json()\n    defaults = utils.spawner_ui_config()\n    logger.info(""Got Notebook: {}"".format(body))\n\n    notebook = utils.load_param_yaml(NOTEBOOK,\n                                     name=body[""name""],\n                                     namespace=namespace,\n                                     serviceAccount=""default-editor"")\n\n    rok.attach_rok_token_secret(notebook)\n    utils.set_notebook_image(notebook, body, defaults)\n    utils.set_notebook_cpu(notebook, body, defaults)\n    utils.set_notebook_memory(notebook, body, defaults)\n    utils.set_notebook_gpus(notebook, body, defaults)\n    utils.set_notebook_configurations(notebook, body, defaults)\n\n    # Workspace Volume\n    workspace_vol = utils.get_workspace_vol(body, defaults)\n    if not body.get(""noWorkspace"", False) and workspace_vol[""type""] != ""None"":\n        # Create the PVC\n        ws_pvc = rok.rok_pvc_from_dict(workspace_vol, namespace)\n\n        if workspace_vol[""type""] == ""Existing"":\n            rok.add_workspace_volume_annotations(ws_pvc, workspace_vol)\n\n        logger.info(""Creating Workspace Volume: {}"".format(ws_pvc.to_dict()))\n        r = api.create_pvc(ws_pvc, namespace=namespace)\n        if not r[""success""]:\n            return jsonify(r)\n\n        utils.add_notebook_volume(\n            notebook,\n            r[""pvc""].metadata.name,\n            r[""pvc""].metadata.name,\n            ""/home/jovyan"",\n        )\n\n    # Add the Data Volumes\n    for vol in utils.get_data_vols(body, defaults):\n        # Create the PVC\n        dtvol_pvc = rok.rok_pvc_from_dict(vol, namespace)\n\n        if vol[""type""] == ""Existing"":\n            rok.add_data_volume_annotations(dtvol_pvc, vol)\n\n        logger.info(""Creating Data Volume {}:"".format(dtvol_pvc))\n        r = api.create_pvc(dtvol_pvc, namespace=namespace)\n        if not r[""success""]:\n            return jsonify(r)\n\n        utils.add_notebook_volume(\n            notebook,\n            r[""pvc""].metadata.name,\n            r[""pvc""].metadata.name,\n            vol[""path""]\n        )\n\n    # shm\n    utils.set_notebook_shm(notebook, body, defaults)\n\n    logger.info(""Creating Notebook: {}"".format(notebook))\n    return jsonify(api.create_notebook(notebook, namespace=namespace))\n\n\n# Since Angular is a SPA, we serve index.html every time\n@app.route(""/"")\ndef serve_root():\n    return send_from_directory(""./static/"", ""index.html"")\n\n\n@app.route(""/<path:path>"", methods=[""GET""])\ndef static_proxy(path):\n    return send_from_directory(""./static/"", path)\n\n\n@app.errorhandler(404)\ndef page_not_found(e):\n    logger.info(""Sending file \'index.html\'"")\n    return send_from_directory(""./static/"", ""index.html"")\n'"
components/jupyter-web-app/backend/kubeflow_jupyter/rok/rok.py,0,"b'import os\nfrom ..common import utils\n\nROK_SECRET_MOUNT = ""/var/run/secrets/rok""\nlogger = utils.create_logger(__name__)\n\n\ndef parse_user_template(string):\n    return string.format(username=""user"")\n\n\ndef rok_secret_name():\n    secret = os.environ.get(""ROK_SECRET_NAME"", ""secret-rok-user"")\n    secret = parse_user_template(secret)\n    return secret\n\n\ndef attach_rok_token_secret(notebook):\n    # Mount the Rok token as a Volume\n    secret_name = rok_secret_name()\n    secret_volume_name = ""volume-%s"" % secret_name\n    utils.add_notebook_volume_secret(\n        notebook,\n        secret_volume_name,\n        secret_name,\n        ROK_SECRET_MOUNT,\n        420\n    )\n\n    # Set ENV variables needed for rok cli\n    notebook[""spec""][""template""][""spec""][""containers""][0][""env""] += [\n        {\n            ""name"": ""ROK_GW_TOKEN"",\n            ""value"": ""file:%s/token"" % ROK_SECRET_MOUNT\n        },\n        {\n            ""name"": ""ROK_GW_URL"",\n            ""value"": ""file:%s/url"" % ROK_SECRET_MOUNT\n        },\n        {\n            ""name"": ""ROK_GW_PARAM_REGISTER_JUPYTER_LAB"",\n            ""value"": notebook[""metadata""][""name""] + ""-0""\n        },\n    ]\n\n\ndef get_pvc_name(pvc):\n    name = """"\n    try:\n        name = pvc.metadata.generate_name\n    except AttributeError:\n        name = pvc.metadata.name\n\n    return name\n\n\ndef add_workspace_volume_annotations(pvc, vol):\n    \'\'\'\n    Attach the needed annotation to the PVC k8s object\n    \'\'\'\n    name = get_pvc_name(pvc)\n    \n    annotations = {\n        ""rok/creds-secret-name"": rok_secret_name(),\n        ""jupyter-workspace"": name,\n    }\n\n    if vol[""type""] == ""Existing"":\n        annotations[""rok/origin""] = vol[""extraFields""].get(""rokUrl"", """")\n\n    labels = {""component"": ""singleuser-storage""}\n\n    pvc.metadata.annotations = annotations\n    pvc.metadata.labels = labels\n\n\ndef add_data_volume_annotations(pvc, vol):\n    name = get_pvc_name(pvc)\n\n    annotations = {\n        ""rok/creds-secret-name"": rok_secret_name(),\n        ""jupyter-dataset"": name,\n    }\n\n    if vol[""type""] == ""Existing"":\n        annotations[""rok/origin""] = vol[""extraFields""].get(""rokUrl"", """")\n\n    labels = {""component"": ""singleuser-storage""}\n\n    pvc.metadata.annotations = annotations\n    pvc.metadata.labels = labels\n\n\ndef rok_pvc_from_dict(vol, namespace):\n    pvc = utils.pvc_from_dict(vol, namespace)\n    pvc.metadata.name = None\n    pvc.metadata.generate_name = vol[""name""] + ""-""\n    return pvc\n'"
