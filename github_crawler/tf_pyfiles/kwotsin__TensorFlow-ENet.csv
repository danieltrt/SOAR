file_path,api_count,code
enet.py,39,"b'import tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import initializers\nslim = tf.contrib.slim\n\n\'\'\'\n============================================================================\nENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation\n============================================================================\nBased on the paper: https://arxiv.org/pdf/1606.02147.pdf\n\'\'\'\n@slim.add_arg_scope\ndef prelu(x, scope, decoder=False):\n    \'\'\'\n    Performs the parametric relu operation. This implementation is based on:\n    https://stackoverflow.com/questions/39975676/how-to-implement-prelu-activation-in-tensorflow\n\n    For the decoder portion, prelu becomes just a normal prelu\n\n    INPUTS:\n    - x(Tensor): a 4D Tensor that undergoes prelu\n    - scope(str): the string to name your prelu operation\'s alpha variable.\n    - decoder(bool): if True, prelu becomes a normal relu.\n\n    OUTPUTS:\n    - pos + neg / x (Tensor): gives prelu output only during training; otherwise, just return x.\n\n    \'\'\'\n    #If decoder, then perform relu and just return the output\n    if decoder:\n        return tf.nn.relu(x, name=scope)\n\n    alpha= tf.get_variable(scope + \'alpha\', x.get_shape()[-1],\n                       initializer=tf.constant_initializer(0.0),\n                        dtype=tf.float32)\n    pos = tf.nn.relu(x)\n    neg = alpha * (x - abs(x)) * 0.5\n    return pos + neg\n\ndef spatial_dropout(x, p, seed, scope, is_training=True):\n    \'\'\'\n    Performs a 2D spatial dropout that drops layers instead of individual elements in an input feature map.\n    Note that p stands for the probability of dropping, but tf.nn.relu uses probability of keeping.\n\n    ------------------\n    Technical Details\n    ------------------\n    The noise shape must be of shape [batch_size, 1, 1, num_channels], with the height and width set to 1, because\n    it will represent either a 1 or 0 for each layer, and these 1 or 0 integers will be broadcasted to the entire\n    dimensions of each layer they interact with such that they can decide whether each layer should be entirely\n    \'dropped\'/set to zero or have its activations entirely kept.\n    --------------------------\n\n    INPUTS:\n    - x(Tensor): a 4D Tensor of the input feature map.\n    - p(float): a float representing the probability of dropping a layer\n    - seed(int): an integer for random seeding the random_uniform distribution that runs under tf.nn.relu\n    - scope(str): the string name for naming the spatial_dropout\n    - is_training(bool): to turn on dropout only when training. Optional.\n\n    OUTPUTS:\n    - output(Tensor): a 4D Tensor that is in exactly the same size as the input x,\n                      with certain layers having their elements all set to 0 (i.e. dropped).\n    \'\'\'\n    if is_training:\n        keep_prob = 1.0 - p\n        input_shape = x.get_shape().as_list()\n        noise_shape = tf.constant(value=[input_shape[0], 1, 1, input_shape[3]])\n        output = tf.nn.dropout(x, keep_prob, noise_shape, seed=seed, name=scope)\n\n        return output\n\n    return x\n\ndef unpool(updates, mask, k_size=[1, 2, 2, 1], output_shape=None, scope=\'\'):\n    \'\'\'\n    Unpooling function based on the implementation by Panaetius at https://github.com/tensorflow/tensorflow/issues/2169\n\n    INPUTS:\n    - inputs(Tensor): a 4D tensor of shape [batch_size, height, width, num_channels] that represents the input block to be upsampled\n    - mask(Tensor): a 4D tensor that represents the argmax values/pooling indices of the previously max-pooled layer\n    - k_size(list): a list of values representing the dimensions of the unpooling filter.\n    - output_shape(list): a list of values to indicate what the final output shape should be after unpooling\n    - scope(str): the string name to name your scope\n\n    OUTPUTS:\n    - ret(Tensor): the returned 4D tensor that has the shape of output_shape.\n\n    \'\'\'\n    with tf.variable_scope(scope):\n        mask = tf.cast(mask, tf.int32)\n        input_shape = tf.shape(updates, out_type=tf.int32)\n        #  calculation new shape\n        if output_shape is None:\n            output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n\n        # calculation indices for batch, height, width and feature maps\n        one_like_mask = tf.ones_like(mask, dtype=tf.int32)\n        batch_shape = tf.concat([[input_shape[0]], [1], [1], [1]], 0)\n        batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int32), shape=batch_shape)\n        b = one_like_mask * batch_range\n        y = mask // (output_shape[2] * output_shape[3])\n        x = (mask // output_shape[3]) % output_shape[2] #mask % (output_shape[2] * output_shape[3]) // output_shape[3]\n        feature_range = tf.range(output_shape[3], dtype=tf.int32)\n        f = one_like_mask * feature_range\n\n        # transpose indices & reshape update values to one dimension\n        updates_size = tf.size(updates)\n        indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\n        values = tf.reshape(updates, [updates_size])\n        ret = tf.scatter_nd(indices, values, output_shape)\n        return ret\n\n@slim.add_arg_scope\ndef initial_block(inputs, is_training=True, scope=\'initial_block\'):\n    \'\'\'\n    The initial block for Enet has 2 branches: The convolution branch and Maxpool branch.\n\n    The conv branch has 13 layers, while the maxpool branch gives 3 layers corresponding to the RGB channels.\n    Both output layers are then concatenated to give an output of 16 layers.\n\n    NOTE: Does not need to store pooling indices since it won\'t be used later for the final upsampling.\n\n    INPUTS:\n    - inputs(Tensor): A 4D tensor of shape [batch_size, height, width, channels]\n\n    OUTPUTS:\n    - net_concatenated(Tensor): a 4D Tensor that contains the \n    \'\'\'\n    #Convolutional branch\n    net_conv = slim.conv2d(inputs, 13, [3,3], stride=2, activation_fn=None, scope=scope+\'_conv\')\n    net_conv = slim.batch_norm(net_conv, is_training=is_training, fused=True, scope=scope+\'_batchnorm\')\n    net_conv = prelu(net_conv, scope=scope+\'_prelu\')\n\n    #Max pool branch\n    net_pool = slim.max_pool2d(inputs, [2,2], stride=2, scope=scope+\'_max_pool\')\n\n    #Concatenated output - does it matter max pool comes first or conv comes first? probably not.\n    net_concatenated = tf.concat([net_conv, net_pool], axis=3, name=scope+\'_concat\')\n    return net_concatenated\n\n@slim.add_arg_scope\ndef bottleneck(inputs,\n               output_depth,\n               filter_size,\n               regularizer_prob,\n               projection_ratio=4,\n               seed=0,\n               is_training=True,\n               downsampling=False,\n               upsampling=False,\n               pooling_indices=None,\n               output_shape=None,\n               dilated=False,\n               dilation_rate=None,\n               asymmetric=False,\n               decoder=False,\n               scope=\'bottleneck\'):\n    \'\'\'\n    The bottleneck module has three different kinds of variants:\n\n    1. A regular convolution which you can decide whether or not to downsample.\n    2. A dilated convolution, which requires you to have a dilation factor.\n    3. An asymmetric convolution that has a decomposed filter size of 5x1 and 1x5 separately.\n\n    INPUTS:\n    - inputs(Tensor): a 4D Tensor of the previous convolutional block of shape [batch_size, height, width, num_channels].\n    - output_depth(int): an integer indicating the output depth of the output convolutional block.\n    - filter_size(int): an integer that gives the height and width of the filter size to use for a regular/dilated convolution.\n    - regularizer_prob(float): the float p that represents the prob of dropping a layer for spatial dropout regularization.\n    - projection_ratio(int): the amount of depth to reduce for initial 1x1 projection. Depth is divided by projection ratio. Default is 4.\n    - seed(int): an integer for the random seed used in the random normal distribution within dropout.\n    - is_training(bool): a boolean value to indicate whether or not is training. Decides batch_norm and prelu activity.\n\n    - downsampling(bool): if True, a max-pool2D layer is added to downsample the spatial sizes.\n    - upsampling(bool): if True, the upsampling bottleneck is activated but requires pooling indices to upsample.\n    - pooling_indices(Tensor): the argmax values that are obtained after performing tf.nn.max_pool_with_argmax.\n    - output_shape(list): A list of integers indicating the output shape of the unpooling layer.\n    - dilated(bool): if True, then dilated convolution is done, but requires a dilation rate to be given.\n    - dilation_rate(int): the dilation factor for performing atrous convolution/dilated convolution.\n    - asymmetric(bool): if True, then asymmetric convolution is done, and the only filter size used here is 5.\n    - decoder(bool): if True, then all the prelus become relus according to ENet author.\n    - scope(str): a string name that names your bottleneck.\n\n    OUTPUTS:\n    - net(Tensor): The convolution block output after a bottleneck\n    - pooling_indices(Tensor): If downsample, then this tensor is produced for use in upooling later.\n    - inputs_shape(list): The shape of the input to the downsampling conv block. For use in unpooling later.\n\n    \'\'\'\n    #Calculate the depth reduction based on the projection ratio used in 1x1 convolution.\n    reduced_depth = int(inputs.get_shape().as_list()[3] / projection_ratio)\n\n    with slim.arg_scope([prelu], decoder=decoder):\n\n        #=============DOWNSAMPLING BOTTLENECK====================\n        if downsampling:\n            #=============MAIN BRANCH=============\n            #Just perform a max pooling\n            net_main, pooling_indices = tf.nn.max_pool_with_argmax(inputs,\n                                                                   ksize=[1,2,2,1],\n                                                                   strides=[1,2,2,1],\n                                                                   padding=\'SAME\',\n                                                                   name=scope+\'_main_max_pool\')\n\n            #First get the difference in depth to pad, then pad with zeros only on the last dimension.\n            inputs_shape = inputs.get_shape().as_list()\n            depth_to_pad = abs(inputs_shape[3] - output_depth)\n            paddings = tf.convert_to_tensor([[0,0], [0,0], [0,0], [0, depth_to_pad]])\n            net_main = tf.pad(net_main, paddings=paddings, name=scope+\'_main_padding\')\n\n            #=============SUB BRANCH==============\n            #First projection that has a 2x2 kernel and stride 2\n            net = slim.conv2d(inputs, reduced_depth, [2,2], stride=2, scope=scope+\'_conv1\')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm1\')\n            net = prelu(net, scope=scope+\'_prelu1\')\n\n            #Second conv block\n            net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], scope=scope+\'_conv2\')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm2\')\n            net = prelu(net, scope=scope+\'_prelu2\')\n\n            #Final projection with 1x1 kernel\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+\'_conv3\')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm3\')\n            net = prelu(net, scope=scope+\'_prelu3\')\n\n            #Regularizer\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+\'_spatial_dropout\')\n\n            #Finally, combine the two branches together via an element-wise addition\n            net = tf.add(net, net_main, name=scope+\'_add\')\n            net = prelu(net, scope=scope+\'_last_prelu\')\n\n            #also return inputs shape for convenience later\n            return net, pooling_indices, inputs_shape\n\n        #============DILATION CONVOLUTION BOTTLENECK====================\n        #Everything is the same as a regular bottleneck except for the dilation rate argument\n        elif dilated:\n            #Check if dilation rate is given\n            if not dilation_rate:\n                raise ValueError(\'Dilation rate is not given.\')\n\n            #Save the main branch for addition later\n            net_main = inputs\n\n            #First projection with 1x1 kernel (dimensionality reduction)\n            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+\'_conv1\')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm1\')\n            net = prelu(net, scope=scope+\'_prelu1\')\n\n            #Second conv block --- apply dilated convolution here\n            net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], rate=dilation_rate, scope=scope+\'_dilated_conv2\')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm2\')\n            net = prelu(net, scope=scope+\'_prelu2\')\n\n            #Final projection with 1x1 kernel (Expansion)\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+\'_conv3\')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm3\')\n            net = prelu(net, scope=scope+\'_prelu3\')\n\n            #Regularizer\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+\'_spatial_dropout\')\n            net = prelu(net, scope=scope+\'_prelu4\')\n\n            #Add the main branch\n            net = tf.add(net_main, net, name=scope+\'_add_dilated\')\n            net = prelu(net, scope=scope+\'_last_prelu\')\n\n            return net\n\n        #===========ASYMMETRIC CONVOLUTION BOTTLENECK==============\n        #Everything is the same as a regular bottleneck except for a [5,5] kernel decomposed into two [5,1] then [1,5]\n        elif asymmetric:\n            #Save the main branch for addition later\n            net_main = inputs\n\n            #First projection with 1x1 kernel (dimensionality reduction)\n            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+\'_conv1\')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm1\')\n            net = prelu(net, scope=scope+\'_prelu1\')\n\n            #Second conv block --- apply asymmetric conv here\n            net = slim.conv2d(net, reduced_depth, [filter_size, 1], scope=scope+\'_asymmetric_conv2a\')\n            net = slim.conv2d(net, reduced_depth, [1, filter_size], scope=scope+\'_asymmetric_conv2b\')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm2\')\n            net = prelu(net, scope=scope+\'_prelu2\')\n\n            #Final projection with 1x1 kernel\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+\'_conv3\')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm3\')\n            net = prelu(net, scope=scope+\'_prelu3\')\n\n            #Regularizer\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+\'_spatial_dropout\')\n            net = prelu(net, scope=scope+\'_prelu4\')\n\n            #Add the main branch\n            net = tf.add(net_main, net, name=scope+\'_add_asymmetric\')\n            net = prelu(net, scope=scope+\'_last_prelu\')\n\n            return net\n\n        #============UPSAMPLING BOTTLENECK================\n        #Everything is the same as a regular one, except convolution becomes transposed.\n        elif upsampling:\n            #Check if pooling indices is given\n            if pooling_indices == None:\n                raise ValueError(\'Pooling indices are not given.\')\n\n            #Check output_shape given or not\n            if output_shape == None:\n                raise ValueError(\'Output depth is not given\')\n\n            #=======MAIN BRANCH=======\n            #Main branch to upsample. output shape must match with the shape of the layer that was pooled initially, in order\n            #for the pooling indices to work correctly. However, the initial pooled layer was padded, so need to reduce dimension\n            #before unpooling. In the paper, padding is replaced with convolution for this purpose of reducing the depth!\n            net_unpool = slim.conv2d(inputs, output_depth, [1,1], scope=scope+\'_main_conv1\')\n            net_unpool = slim.batch_norm(net_unpool, is_training=is_training, scope=scope+\'batch_norm1\')\n            net_unpool = unpool(net_unpool, pooling_indices, output_shape=output_shape, scope=\'unpool\')\n\n            #======SUB BRANCH=======\n            #First 1x1 projection to reduce depth\n            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+\'_conv1\')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm2\')\n            net = prelu(net, scope=scope+\'_prelu1\')\n\n            #Second conv block -----------------------------> NOTE: using tf.nn.conv2d_transpose for variable input shape.\n            net_unpool_shape = net_unpool.get_shape().as_list()\n            output_shape = [net_unpool_shape[0], net_unpool_shape[1], net_unpool_shape[2], reduced_depth]\n            output_shape = tf.convert_to_tensor(output_shape)\n            filter_size = [filter_size, filter_size, reduced_depth, reduced_depth]\n            filters = tf.get_variable(shape=filter_size, initializer=initializers.xavier_initializer(), dtype=tf.float32, name=scope+\'_transposed_conv2_filters\')\n\n            # net = slim.conv2d_transpose(net, reduced_depth, [filter_size, filter_size], stride=2, scope=scope+\'_transposed_conv2\')\n            net = tf.nn.conv2d_transpose(net, filter=filters, strides=[1,2,2,1], output_shape=output_shape, name=scope+\'_transposed_conv2\')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm3\')\n            net = prelu(net, scope=scope+\'_prelu2\')\n\n            #Final projection with 1x1 kernel\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+\'_conv3\')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm4\')\n            net = prelu(net, scope=scope+\'_prelu3\')\n\n            #Regularizer\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+\'_spatial_dropout\')\n            net = prelu(net, scope=scope+\'_prelu4\')\n\n            #Finally, add the unpooling layer and the sub branch together\n            net = tf.add(net, net_unpool, name=scope+\'_add_upsample\')\n            net = prelu(net, scope=scope+\'_last_prelu\')\n\n            return net\n\n        #OTHERWISE, just perform a regular bottleneck!\n        #==============REGULAR BOTTLENECK==================\n        #Save the main branch for addition later\n        net_main = inputs\n\n        #First projection with 1x1 kernel\n        net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+\'_conv1\')\n        net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm1\')\n        net = prelu(net, scope=scope+\'_prelu1\')\n\n        #Second conv block\n        net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], scope=scope+\'_conv2\')\n        net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm2\')\n        net = prelu(net, scope=scope+\'_prelu2\')\n\n        #Final projection with 1x1 kernel\n        net = slim.conv2d(net, output_depth, [1,1], scope=scope+\'_conv3\')\n        net = slim.batch_norm(net, is_training=is_training, scope=scope+\'_batch_norm3\')\n        net = prelu(net, scope=scope+\'_prelu3\')\n\n        #Regularizer\n        net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+\'_spatial_dropout\')\n        net = prelu(net, scope=scope+\'_prelu4\')\n\n        #Add the main branch\n        net = tf.add(net_main, net, name=scope+\'_add_regular\')\n        net = prelu(net, scope=scope+\'_last_prelu\')\n\n        return net\n\n#Now actually start building the network\ndef ENet(inputs,\n         num_classes,\n         batch_size,\n         num_initial_blocks=1,\n         stage_two_repeat=2,\n         skip_connections=True,\n         reuse=None,\n         is_training=True,\n         scope=\'ENet\'):\n    \'\'\'\n    The ENet model for real-time semantic segmentation!\n\n    INPUTS:\n    - inputs(Tensor): a 4D Tensor of shape [batch_size, image_height, image_width, num_channels] that represents one batch of preprocessed images.\n    - num_classes(int): an integer for the number of classes to predict. This will determine the final output channels as the answer.\n    - batch_size(int): the batch size to explictly set the shape of the inputs in order for operations to work properly.\n    - num_initial_blocks(int): the number of times to repeat the initial block.\n    - stage_two_repeat(int): the number of times to repeat stage two in order to make the network deeper.\n    - skip_connections(bool): if True, add the corresponding encoder feature maps to the decoder. They are of exact same shapes.\n    - reuse(bool): Whether or not to reuse the variables for evaluation.\n    - is_training(bool): if True, switch on batch_norm and prelu only during training, otherwise they are turned off.\n    - scope(str): a string that represents the scope name for the variables.\n\n    OUTPUTS:\n    - net(Tensor): a 4D Tensor output of shape [batch_size, image_height, image_width, num_classes], where each pixel has a one-hot encoded vector\n                      determining the label of the pixel.\n    \'\'\'\n    #Set the shape of the inputs first to get the batch_size information\n    inputs_shape = inputs.get_shape().as_list()\n    inputs.set_shape(shape=(batch_size, inputs_shape[1], inputs_shape[2], inputs_shape[3]))\n\n    with tf.variable_scope(scope, reuse=reuse):\n        #Set the primary arg scopes. Fused batch_norm is faster than normal batch norm.\n        with slim.arg_scope([initial_block, bottleneck], is_training=is_training),\\\n             slim.arg_scope([slim.batch_norm], fused=True), \\\n             slim.arg_scope([slim.conv2d, slim.conv2d_transpose], activation_fn=None): \n            #=================INITIAL BLOCK=================\n            net = initial_block(inputs, scope=\'initial_block_1\')\n            for i in xrange(2, max(num_initial_blocks, 1) + 1):\n                net = initial_block(net, scope=\'initial_block_\' + str(i))\n\n            #Save for skip connection later\n            if skip_connections:\n                net_one = net\n\n            #===================STAGE ONE=======================\n            net, pooling_indices_1, inputs_shape_1 = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, downsampling=True, scope=\'bottleneck1_0\')\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope=\'bottleneck1_1\')\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope=\'bottleneck1_2\')\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope=\'bottleneck1_3\')\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope=\'bottleneck1_4\')\n\n            #Save for skip connection later\n            if skip_connections:\n                net_two = net\n\n            #regularization prob is 0.1 from bottleneck 2.0 onwards\n            with slim.arg_scope([bottleneck], regularizer_prob=0.1):\n                net, pooling_indices_2, inputs_shape_2 = bottleneck(net, output_depth=128, filter_size=3, downsampling=True, scope=\'bottleneck2_0\')\n                \n                #Repeat the stage two at least twice to get stage 2 and 3:\n                for i in xrange(2, max(stage_two_repeat, 2) + 2):\n                    net = bottleneck(net, output_depth=128, filter_size=3, scope=\'bottleneck\'+str(i)+\'_1\')\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=2, scope=\'bottleneck\'+str(i)+\'_2\')\n                    net = bottleneck(net, output_depth=128, filter_size=5, asymmetric=True, scope=\'bottleneck\'+str(i)+\'_3\')\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=4, scope=\'bottleneck\'+str(i)+\'_4\')\n                    net = bottleneck(net, output_depth=128, filter_size=3, scope=\'bottleneck\'+str(i)+\'_5\')\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=8, scope=\'bottleneck\'+str(i)+\'_6\')\n                    net = bottleneck(net, output_depth=128, filter_size=5, asymmetric=True, scope=\'bottleneck\'+str(i)+\'_7\')\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=16, scope=\'bottleneck\'+str(i)+\'_8\')\n\n            with slim.arg_scope([bottleneck], regularizer_prob=0.1, decoder=True):\n                #===================STAGE FOUR========================\n                bottleneck_scope_name = ""bottleneck"" + str(i + 1)\n\n                #The decoder section, so start to upsample.\n                net = bottleneck(net, output_depth=64, filter_size=3, upsampling=True,\n                                 pooling_indices=pooling_indices_2, output_shape=inputs_shape_2, scope=bottleneck_scope_name+\'_0\')\n\n                #Perform skip connections here\n                if skip_connections:\n                    net = tf.add(net, net_two, name=bottleneck_scope_name+\'_skip_connection\')\n\n                net = bottleneck(net, output_depth=64, filter_size=3, scope=bottleneck_scope_name+\'_1\')\n                net = bottleneck(net, output_depth=64, filter_size=3, scope=bottleneck_scope_name+\'_2\')\n\n                #===================STAGE FIVE========================\n                bottleneck_scope_name = ""bottleneck"" + str(i + 2)\n\n                net = bottleneck(net, output_depth=16, filter_size=3, upsampling=True,\n                                 pooling_indices=pooling_indices_1, output_shape=inputs_shape_1, scope=bottleneck_scope_name+\'_0\')\n\n                #perform skip connections here\n                if skip_connections:\n                    net = tf.add(net, net_one, name=bottleneck_scope_name+\'_skip_connection\')\n\n                net = bottleneck(net, output_depth=16, filter_size=3, scope=bottleneck_scope_name+\'_1\')\n\n            #=============FINAL CONVOLUTION=============\n            logits = slim.conv2d_transpose(net, num_classes, [2,2], stride=2, scope=\'fullconv\')\n            probabilities = tf.nn.softmax(logits, name=\'logits_to_softmax\')\n\n        return logits, probabilities\n\n\ndef ENet_arg_scope(weight_decay=2e-4,\n                   batch_norm_decay=0.1,\n                   batch_norm_epsilon=0.001):\n  \'\'\'\n  The arg scope for enet model. The weight decay is 2e-4 as seen in the paper.\n  Batch_norm decay is 0.1 (momentum 0.1) according to official implementation.\n\n  INPUTS:\n  - weight_decay(float): the weight decay for weights variables in conv2d and separable conv2d\n  - batch_norm_decay(float): decay for the moving average of batch_norm momentums.\n  - batch_norm_epsilon(float): small float added to variance to avoid dividing by zero.\n\n  OUTPUTS:\n  - scope(arg_scope): a tf-slim arg_scope with the parameters needed for xception.\n  \'\'\'\n  # Set weight_decay for weights in conv2d and separable_conv2d layers.\n  with slim.arg_scope([slim.conv2d],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    # Set parameters for batch_norm.\n    with slim.arg_scope([slim.batch_norm],\n                        decay=batch_norm_decay,\n                        epsilon=batch_norm_epsilon) as scope:\n      return scope\n'"
get_class_weights.py,0,"b'import numpy as np\nimport os\nfrom scipy.misc import imread\nimport ast\n\nimage_dir = ""./dataset/trainannot""\nimage_files = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith(\'.png\')]\n\ndef ENet_weighing(image_files=image_files, num_classes=12):\n    \'\'\'\n    The custom class weighing function as seen in the ENet paper.\n\n    INPUTS:\n    - image_files(list): a list of image_filenames which element can be read immediately\n\n    OUTPUTS:\n    - class_weights(list): a list of class weights where each index represents each class label and the element is the class weight for that label.\n\n    \'\'\'\n    #initialize dictionary with all 0\n    label_to_frequency = {}\n    for i in xrange(num_classes):\n        label_to_frequency[i] = 0\n\n    for n in xrange(len(image_files)):\n        image = imread(image_files[n])\n\n        #For each label in each image, sum up the frequency of the label and add it to label_to_frequency dict\n        for i in xrange(num_classes):\n            class_mask = np.equal(image, i)\n            class_mask = class_mask.astype(np.float32)\n            class_frequency = np.sum(class_mask)\n\n            label_to_frequency[i] += class_frequency\n\n    #perform the weighing function label-wise and append the label\'s class weights to class_weights\n    class_weights = []\n    total_frequency = sum(label_to_frequency.values())\n    for label, frequency in label_to_frequency.items():\n        class_weight = 1 / np.log(1.02 + (frequency / total_frequency))\n        class_weights.append(class_weight)\n\n    #Set the last class_weight to 0.0\n    class_weights[-1] = 0.0\n\n    return class_weights\n\ndef median_frequency_balancing(image_files=image_files, num_classes=12):\n    \'\'\'\n    Perform median frequency balancing on the image files, given by the formula:\n    f = Median_freq_c / total_freq_c\n\n    where median_freq_c is the median frequency of the class for all pixels of C that appeared in images\n    and total_freq_c is the total number of pixels of c in the total pixels of the images where c appeared.\n\n    INPUTS:\n    - image_files(list): a list of image_filenames which element can be read immediately\n    - num_classes(int): the number of classes of pixels in all images\n\n    OUTPUTS:\n    - class_weights(list): a list of class weights where each index represents each class label and the element is the class weight for that label.\n\n    \'\'\'\n    #Initialize all the labels key with a list value\n    label_to_frequency_dict = {}\n    for i in xrange(num_classes):\n        label_to_frequency_dict[i] = []\n\n    for n in xrange(len(image_files)):\n        image = imread(image_files[n])\n\n        #For each image sum up the frequency of each label in that image and append to the dictionary if frequency is positive.\n        for i in xrange(num_classes):\n            class_mask = np.equal(image, i)\n            class_mask = class_mask.astype(np.float32)\n            class_frequency = np.sum(class_mask)\n\n            if class_frequency != 0.0:\n                label_to_frequency_dict[i].append(class_frequency)\n\n    class_weights = []\n\n    #Get the total pixels to calculate total_frequency later\n    total_pixels = 0\n    for frequencies in label_to_frequency_dict.values():\n        total_pixels += sum(frequencies)\n\n    for i, j in label_to_frequency_dict.items():\n        j = sorted(j) #To obtain the median, we got to sort the frequencies\n\n        median_frequency = np.median(j) / sum(j)\n        total_frequency = sum(j) / total_pixels\n        median_frequency_balanced = median_frequency / total_frequency\n        class_weights.append(median_frequency_balanced)\n\n    #Set the last class_weight to 0.0 as it\'s the background class\n    class_weights[-1] = 0.0\n\n    return class_weights\n\nif __name__ == ""__main__"":\n    median_frequency_balancing(image_files, num_classes=12)\n    ENet_weighing(image_files, num_classes=12)'"
predict_segmentation.py,13,"b'import tensorflow as tf\nimport os\nimport matplotlib.pyplot as plt\nfrom enet import ENet, ENet_arg_scope\nfrom preprocessing import preprocess\nfrom scipy.misc import imsave\nimport numpy as np\nslim = tf.contrib.slim\n\nimage_dir = \'./dataset/test/\'\nimages_list = sorted([os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith(\'.png\')])\n\ncheckpoint_dir = ""./checkpoint_mfb""\ncheckpoint = tf.train.latest_checkpoint(checkpoint_dir)\n\nnum_initial_blocks = 1\nskip_connections = False\nstage_two_repeat = 2\n\'\'\'\n#Labels to colours are obtained from here:\nhttps://github.com/alexgkendall/SegNet-Tutorial/blob/c922cc4a4fcc7ce279dd998fb2d4a8703f34ebd7/Scripts/test_segmentation_camvid.py\n\nHowever, the road_marking class is collapsed into the road class in the dataset provided.\n\nClasses:\n------------\nSky = [128,128,128]\nBuilding = [128,0,0]\nPole = [192,192,128]\nRoad_marking = [255,69,0]\nRoad = [128,64,128]\nPavement = [60,40,222]\nTree = [128,128,0]\nSignSymbol = [192,128,128]\nFence = [64,64,128]\nCar = [64,0,128]\nPedestrian = [64,64,0]\nBicyclist = [0,128,192]\nUnlabelled = [0,0,0]\n\'\'\'\nlabel_to_colours =    {0: [128,128,128],\n                     1: [128,0,0],\n                     2: [192,192,128],\n                     3: [128,64,128],\n                     4: [60,40,222],\n                     5: [128,128,0],\n                     6: [192,128,128],\n                     7: [64,64,128],\n                     8: [64,0,128],\n                     9: [64,64,0],\n                     10: [0,128,192],\n                     11: [0,0,0]}\n\n#Create the photo directory\nphoto_dir = checkpoint_dir + ""/test_images""\nif not os.path.exists(photo_dir):\n    os.mkdir(photo_dir)\n\n#Create a function to convert each pixel label to colour.\ndef grayscale_to_colour(image):\n    print \'Converting image...\'\n    image = image.reshape((360, 480, 1))\n    image = np.repeat(image, 3, axis=-1)\n    for i in xrange(image.shape[0]):\n        for j in xrange(image.shape[1]):\n            label = int(image[i][j][0])\n            image[i][j] = np.array(label_to_colours[label])\n\n    return image\n\n\nwith tf.Graph().as_default() as graph:\n    images_tensor = tf.train.string_input_producer(images_list, shuffle=False)\n    reader = tf.WholeFileReader()\n    key, image_tensor = reader.read(images_tensor)\n    image = tf.image.decode_png(image_tensor, channels=3)\n    # image = tf.image.resize_image_with_crop_or_pad(image, 360, 480)\n    # image = tf.cast(image, tf.float32)\n    image = preprocess(image)\n    images = tf.train.batch([image], batch_size = 10, allow_smaller_final_batch=True)\n\n    #Create the model inference\n    with slim.arg_scope(ENet_arg_scope()):\n        logits, probabilities = ENet(images,\n                                     num_classes=12,\n                                     batch_size=10,\n                                     is_training=True,\n                                     reuse=None,\n                                     num_initial_blocks=num_initial_blocks,\n                                     stage_two_repeat=stage_two_repeat,\n                                     skip_connections=skip_connections)\n\n    variables_to_restore = slim.get_variables_to_restore()\n    saver = tf.train.Saver(variables_to_restore)\n    def restore_fn(sess):\n        return saver.restore(sess, checkpoint)\n\n    predictions = tf.argmax(probabilities, -1)\n    predictions = tf.cast(predictions, tf.float32)\n    print \'HERE\', predictions.get_shape()\n\n    sv = tf.train.Supervisor(logdir=None, init_fn=restore_fn)\n    \n    with sv.managed_session() as sess:\n\n        for i in xrange(len(images_list) / 10 + 1):\n            segmentations = sess.run(predictions)\n            # print segmentations.shape\n\n            for j in xrange(segmentations.shape[0]):\n                #Stop at the 233rd image as it\'s repeated\n                if i*10 + j == 223:\n                    break\n\n                converted_image = grayscale_to_colour(segmentations[j])\n                print \'Saving image %s/%s\' %(i*10 + j, len(images_list))\n                plt.axis(\'off\')\n                plt.imshow(converted_image)\n                imsave(photo_dir + ""/image_%s.png"" %(i*10 + j), converted_image)\n                # plt.show()'"
preprocessing.py,6,"b""import tensorflow as tf\n\ndef preprocess(image, annotation=None, height=360, width=480):\n    '''\n    Performs preprocessing for one set of image and annotation for feeding into network.\n    NO scaling of any sort will be done as per original paper.\n\n    INPUTS:\n    - image (Tensor): the image input 3D Tensor of shape [height, width, 3]\n    - annotation (Tensor): the annotation input 3D Tensor of shape [height, width, 1]\n    - height (int): the output height to reshape the image and annotation into\n    - width (int): the output width to reshape the image and annotation into\n\n    OUTPUTS:\n    - preprocessed_image(Tensor): the reshaped image tensor\n    - preprocessed_annotation(Tensor): the reshaped annotation tensor\n    '''\n\n    #Convert the image and annotation dtypes to tf.float32 if needed\n    if image.dtype != tf.float32:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # image = tf.cast(image, tf.float32)\n\n    image = tf.image.resize_image_with_crop_or_pad(image, height, width)\n    image.set_shape(shape=(height, width, 3))\n\n    if not annotation == None:\n        annotation = tf.image.resize_image_with_crop_or_pad(annotation, height, width)\n        annotation.set_shape(shape=(height, width, 1))\n\n        return image, annotation\n\n    return image"""
test_enet.py,28,"b'import tensorflow as tf\nfrom tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\nfrom tensorflow.python.platform import tf_logging as logging\nfrom enet import ENet, ENet_arg_scope\nfrom preprocessing import preprocess\nimport os\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nslim = tf.contrib.slim\n\n#============INPUT ARGUMENTS================\nflags = tf.app.flags\n\n#Directories\nflags.DEFINE_string(\'dataset_dir\', \'./dataset\', \'The dataset directory to find the train, validation and test images.\')\nflags.DEFINE_string(\'checkpoint_dir\', \'./log/original\', \'The checkpoint directory to restore your mode.l\')\nflags.DEFINE_string(\'logdir\', \'./log/original_test\', \'The log directory for event files created during test evaluation.\')\nflags.DEFINE_boolean(\'save_images\', True, \'If True, saves 10 images to your logdir for visualization.\')\n\n#Evaluation information\nflags.DEFINE_integer(\'num_classes\', 12, \'The number of classes to predict.\')\nflags.DEFINE_integer(\'batch_size\', 10, \'The batch_size for evaluation.\')\nflags.DEFINE_integer(\'image_height\', 360, ""The input height of the images."")\nflags.DEFINE_integer(\'image_width\', 480, ""The input width of the images."")\nflags.DEFINE_integer(\'num_epochs\', 10, ""The number of epochs to evaluate your model."")\n\n#Architectural changes\nflags.DEFINE_integer(\'num_initial_blocks\', 1, \'The number of initial blocks to use in ENet.\')\nflags.DEFINE_integer(\'stage_two_repeat\', 2, \'The number of times to repeat stage two.\')\nflags.DEFINE_boolean(\'skip_connections\', False, \'If True, perform skip connections from encoder to decoder.\')\n\nFLAGS = flags.FLAGS\n\n#==========NAME HANDLING FOR CONVENIENCE==============\nnum_classes = FLAGS.num_classes\nbatch_size = FLAGS.batch_size\nimage_height = FLAGS.image_height\nimage_width = FLAGS.image_width\nnum_epochs = FLAGS.num_epochs\n\nsave_images = FLAGS.save_images\n\n#Architectural changes\nnum_initial_blocks = FLAGS.num_initial_blocks\nstage_two_repeat = FLAGS.stage_two_repeat\nskip_connections = FLAGS.skip_connections\n\ndataset_dir = FLAGS.dataset_dir\ncheckpoint_dir = FLAGS.checkpoint_dir\nphoto_dir = os.path.join(FLAGS.logdir, ""images"")\nlogdir = FLAGS.logdir\n\n#===============PREPARATION FOR TRAINING==================\n#Checkpoint directories\ncheckpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n\n#Dataset directories\nimage_files = sorted([os.path.join(dataset_dir, \'test\', file) for file in os.listdir(dataset_dir + ""/test"") if file.endswith(\'.png\')])\nannotation_files = sorted([os.path.join(dataset_dir, ""testannot"", file) for file in os.listdir(dataset_dir + ""/testannot"") if file.endswith(\'.png\')])\n\nnum_batches_per_epoch = len(image_files) / batch_size\nnum_steps_per_epoch = num_batches_per_epoch\n\n#=============EVALUATION=================\ndef run():\n    with tf.Graph().as_default() as graph:\n        tf.logging.set_verbosity(tf.logging.INFO)\n\n        #===================TEST BRANCH=======================\n        #Load the files into one input queue\n        images = tf.convert_to_tensor(image_files)\n        annotations = tf.convert_to_tensor(annotation_files)\n        input_queue = tf.train.slice_input_producer([images, annotations])\n\n        #Decode the image and annotation raw content\n        image = tf.read_file(input_queue[0])\n        image = tf.image.decode_image(image, channels=3)\n        annotation = tf.read_file(input_queue[1])\n        annotation = tf.image.decode_image(annotation)\n\n        #preprocess and batch up the image and annotation\n        preprocessed_image, preprocessed_annotation = preprocess(image, annotation, image_height, image_width)\n        images, annotations = tf.train.batch([preprocessed_image, preprocessed_annotation], batch_size=batch_size, allow_smaller_final_batch=True)\n\n        #Create the model inference\n        with slim.arg_scope(ENet_arg_scope()):\n            logits, probabilities = ENet(images,\n                                         num_classes,\n                                         batch_size=batch_size,\n                                         is_training=True,\n                                         reuse=None,\n                                         num_initial_blocks=num_initial_blocks,\n                                         stage_two_repeat=stage_two_repeat,\n                                         skip_connections=skip_connections)\n\n        # Set up the variables to restore and restoring function from a saver.\n        exclude = []\n        variables_to_restore = slim.get_variables_to_restore(exclude=exclude)\n\n        saver = tf.train.Saver(variables_to_restore)\n        def restore_fn(sess):\n            return saver.restore(sess, checkpoint_file)\n\n        #perform one-hot-encoding on the ground truth annotation to get same shape as the logits\n        annotations = tf.reshape(annotations, shape=[batch_size, image_height, image_width])\n        annotations_ohe = tf.one_hot(annotations, num_classes, axis=-1)\n        annotations = tf.cast(annotations, tf.int64)\n\n        #State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n        predictions = tf.argmax(probabilities, -1)\n        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, annotations)\n        mean_IOU, mean_IOU_update = tf.contrib.metrics.streaming_mean_iou(predictions=predictions, labels=annotations, num_classes=num_classes)\n        per_class_accuracy, per_class_accuracy_update = tf.metrics.mean_per_class_accuracy(labels=annotations, predictions=predictions, num_classes=num_classes)\n        metrics_op = tf.group(accuracy_update, mean_IOU_update, per_class_accuracy_update)\n\n        #Create the global step and an increment op for monitoring\n        global_step = get_or_create_global_step()\n        global_step_op = tf.assign(global_step, global_step + 1) #no apply_gradient method so manually increasing the global_step\n\n        #Create a evaluation step function\n        def eval_step(sess, metrics_op, global_step):\n            \'\'\'\n            Simply takes in a session, runs the metrics op and some logging information.\n            \'\'\'\n            start_time = time.time()\n            _, global_step_count, accuracy_value, mean_IOU_value, per_class_accuracy_value = sess.run([metrics_op, global_step_op, accuracy, mean_IOU, per_class_accuracy])\n            time_elapsed = time.time() - start_time\n\n            #Log some information\n            logging.info(\'Global Step %s: Streaming Accuracy: %.4f     Streaming Mean IOU: %.4f     Per-class Accuracy: %.4f (%.2f sec/step)\',\n                         global_step_count, accuracy_value, mean_IOU_value, per_class_accuracy_value, time_elapsed)\n\n            return accuracy_value, mean_IOU_value, per_class_accuracy_value\n\n        #Create your summaries\n        tf.summary.scalar(\'Monitor/test_accuracy\', accuracy)\n        tf.summary.scalar(\'Monitor/test_mean_per_class_accuracy\', per_class_accuracy)\n        tf.summary.scalar(\'Monitor/test_mean_IOU\', mean_IOU)\n        my_summary_op = tf.summary.merge_all()\n\n        #Define your supervisor for running a managed session. Do not run the summary_op automatically or else it will consume too much memory\n        sv = tf.train.Supervisor(logdir = logdir, summary_op = None, init_fn=restore_fn)\n\n        #Run the managed session\n        with sv.managed_session() as sess:\n            for step in range(int(num_steps_per_epoch * num_epochs)):\n                #print vital information every start of the epoch as always\n                if step % num_batches_per_epoch == 0:\n                    accuracy_value, mean_IOU_value = sess.run([accuracy, mean_IOU])\n                    logging.info(\'Epoch: %s/%s\', step / num_batches_per_epoch + 1, num_epochs)\n                    logging.info(\'Current Streaming Accuracy: %.4f\', accuracy_value)\n                    logging.info(\'Current Streaming Mean IOU: %.4f\', mean_IOU_value)\n                    \n                #Compute summaries every 10 steps and continue evaluating\n                if step % 10 == 0:\n                    test_accuracy, test_mean_IOU, test_per_class_accuracy = eval_step(sess, metrics_op = metrics_op, global_step = sv.global_step)\n                    summaries = sess.run(my_summary_op)\n                    sv.summary_computed(sess, summaries)\n                    \n                #Otherwise just run as per normal\n                else:\n                    test_accuracy, test_mean_IOU, test_per_class_accuracy = eval_step(sess, metrics_op = metrics_op, global_step = sv.global_step)\n\n            #At the end of all the evaluation, show the final accuracy\n            logging.info(\'Final Streaming Accuracy: %.4f\', test_accuracy)\n            logging.info(\'Final Mean IOU: %.4f\', test_mean_IOU)\n            logging.info(\'Final Per Class Accuracy %.4f\', test_per_class_accuracy)\n\n            #Show end of evaluation\n            logging.info(\'Finished evaluating!\')\n\n            #Save the images\n            if save_images:\n                if not os.path.exists(photo_dir):\n                    os.mkdir(photo_dir)\n\n                #Save the image visualizations for the first 10 images.\n                logging.info(\'Saving the images now...\')\n                predictions_val, annotations_val = sess.run([predictions, annotations])\n\n                for i in xrange(10):\n                    predicted_annotation = predictions_val[i]\n                    annotation = annotations_val[i]\n\n                    plt.subplot(1,2,1)\n                    plt.imshow(predicted_annotation)\n                    plt.subplot(1,2,2)\n                    plt.imshow(annotation)\n                    plt.savefig(photo_dir+""/image_"" + str(i))\n\nif __name__ == \'__main__\':\n    run()'"
train_enet.py,52,"b'import tensorflow as tf\nfrom tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\nfrom tensorflow.python.platform import tf_logging as logging\nfrom enet import ENet, ENet_arg_scope\nfrom preprocessing import preprocess\nfrom get_class_weights import ENet_weighing, median_frequency_balancing\nimport os\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nslim = tf.contrib.slim\n\n#==============INPUT ARGUMENTS==================\nflags = tf.app.flags\n\n#Directory arguments\nflags.DEFINE_string(\'dataset_dir\', \'./dataset\', \'The dataset directory to find the train, validation and test images.\')\nflags.DEFINE_string(\'logdir\', \'./log/original\', \'The log directory to save your checkpoint and event files.\')\nflags.DEFINE_boolean(\'save_images\', True, \'Whether or not to save your images.\')\nflags.DEFINE_boolean(\'combine_dataset\', False, \'If True, combines the validation with the train dataset.\')\n\n#Training arguments\nflags.DEFINE_integer(\'num_classes\', 12, \'The number of classes to predict.\')\nflags.DEFINE_integer(\'batch_size\', 10, \'The batch_size for training.\')\nflags.DEFINE_integer(\'eval_batch_size\', 25, \'The batch size used for validation.\')\nflags.DEFINE_integer(\'image_height\', 360, ""The input height of the images."")\nflags.DEFINE_integer(\'image_width\', 480, ""The input width of the images."")\nflags.DEFINE_integer(\'num_epochs\', 300, ""The number of epochs to train your model."")\nflags.DEFINE_integer(\'num_epochs_before_decay\', 100, \'The number of epochs before decaying your learning rate.\')\nflags.DEFINE_float(\'weight_decay\', 2e-4, ""The weight decay for ENet convolution layers."")\nflags.DEFINE_float(\'learning_rate_decay_factor\', 1e-1, \'The learning rate decay factor.\')\nflags.DEFINE_float(\'initial_learning_rate\', 5e-4, \'The initial learning rate for your training.\')\nflags.DEFINE_string(\'weighting\', ""MFB"", \'Choice of Median Frequency Balancing or the custom ENet class weights.\')\n\n#Architectural changes\nflags.DEFINE_integer(\'num_initial_blocks\', 1, \'The number of initial blocks to use in ENet.\')\nflags.DEFINE_integer(\'stage_two_repeat\', 2, \'The number of times to repeat stage two.\')\nflags.DEFINE_boolean(\'skip_connections\', False, \'If True, perform skip connections from encoder to decoder.\')\n\nFLAGS = flags.FLAGS\n\n#==========NAME HANDLING FOR CONVENIENCE==============\nnum_classes = FLAGS.num_classes\nbatch_size = FLAGS.batch_size\nimage_height = FLAGS.image_height\nimage_width = FLAGS.image_width\neval_batch_size = FLAGS.eval_batch_size #Can be larger than train_batch as no need to backpropagate gradients.\ncombine_dataset = FLAGS.combine_dataset\n\n#Training parameters\ninitial_learning_rate = FLAGS.initial_learning_rate\nnum_epochs_before_decay = FLAGS.num_epochs_before_decay\nnum_epochs =FLAGS.num_epochs\nlearning_rate_decay_factor = FLAGS.learning_rate_decay_factor\nweight_decay = FLAGS.weight_decay\nepsilon = 1e-8\n\n#Architectural changes\nnum_initial_blocks = FLAGS.num_initial_blocks\nstage_two_repeat = FLAGS.stage_two_repeat\nskip_connections = FLAGS.skip_connections\n\n#Use median frequency balancing or not\nweighting = FLAGS.weighting\n\n#Visualization and where to save images\nsave_images = FLAGS.save_images\nphoto_dir = os.path.join(FLAGS.logdir, ""images"")\n\n#Directories\ndataset_dir = FLAGS.dataset_dir\nlogdir = FLAGS.logdir\n\n#===============PREPARATION FOR TRAINING==================\n#Get the images into a list\nimage_files = sorted([os.path.join(dataset_dir, \'train\', file) for file in os.listdir(dataset_dir + ""/train"") if file.endswith(\'.png\')])\nannotation_files = sorted([os.path.join(dataset_dir, ""trainannot"", file) for file in os.listdir(dataset_dir + ""/trainannot"") if file.endswith(\'.png\')])\n\nimage_val_files = sorted([os.path.join(dataset_dir, \'val\', file) for file in os.listdir(dataset_dir + ""/val"") if file.endswith(\'.png\')])\nannotation_val_files = sorted([os.path.join(dataset_dir, ""valannot"", file) for file in os.listdir(dataset_dir + ""/valannot"") if file.endswith(\'.png\')])\n\nif combine_dataset:\n    image_files += image_val_files\n    annotation_files += annotation_val_files\n\n#Know the number steps to take before decaying the learning rate and batches per epoch\nnum_batches_per_epoch = len(image_files) / batch_size\nnum_steps_per_epoch = num_batches_per_epoch\ndecay_steps = int(num_epochs_before_decay * num_steps_per_epoch)\n\n#=================CLASS WEIGHTS===============================\n#Median frequency balancing class_weights\nif weighting == ""MFB"":\n    class_weights = median_frequency_balancing()\n    print ""========= Median Frequency Balancing Class Weights =========\\n"", class_weights\n\n#Inverse weighing probability class weights\nelif weighting == ""ENET"":\n    class_weights = ENet_weighing()\n    print ""========= ENet Class Weights =========\\n"", class_weights\n\n#============= TRAINING =================\ndef weighted_cross_entropy(onehot_labels, logits, class_weights):\n    \'\'\'\n    A quick wrapper to compute weighted cross entropy. \n\n    ------------------\n    Technical Details\n    ------------------\n    The class_weights list can be multiplied by onehot_labels directly because the last dimension\n    of onehot_labels is 12 and class_weights (length 12) can broadcast across that dimension, which is what we want. \n    Then we collapse the last dimension for the class_weights to get a shape of (batch_size, height, width, 1)\n    to get a mask with each pixel\'s value representing the class_weight.\n\n    This mask can then be that can be broadcasted to the intermediate output of logits\n    and onehot_labels when calculating the cross entropy loss.\n    ------------------\n\n    INPUTS:\n    - onehot_labels(Tensor): the one-hot encoded labels of shape (batch_size, height, width, num_classes)\n    - logits(Tensor): the logits output from the model that is of shape (batch_size, height, width, num_classes)\n    - class_weights(list): A list where each index is the class label and the value of the index is the class weight.\n\n    OUTPUTS:\n    - loss(Tensor): a scalar Tensor that is the weighted cross entropy loss output.\n    \'\'\'\n    weights = onehot_labels * class_weights\n    weights = tf.reduce_sum(weights, 3)\n    loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits, weights=weights)\n\n    return loss\n\ndef run():\n    with tf.Graph().as_default() as graph:\n        tf.logging.set_verbosity(tf.logging.INFO)\n\n        #===================TRAINING BRANCH=======================\n        #Load the files into one input queue\n        images = tf.convert_to_tensor(image_files)\n        annotations = tf.convert_to_tensor(annotation_files)\n        input_queue = tf.train.slice_input_producer([images, annotations]) #Slice_input producer shuffles the data by default.\n\n        #Decode the image and annotation raw content\n        image = tf.read_file(input_queue[0])\n        image = tf.image.decode_image(image, channels=3)\n        annotation = tf.read_file(input_queue[1])\n        annotation = tf.image.decode_image(annotation)\n\n        #preprocess and batch up the image and annotation\n        preprocessed_image, preprocessed_annotation = preprocess(image, annotation, image_height, image_width)\n        images, annotations = tf.train.batch([preprocessed_image, preprocessed_annotation], batch_size=batch_size, allow_smaller_final_batch=True)\n\n        #Create the model inference\n        with slim.arg_scope(ENet_arg_scope(weight_decay=weight_decay)):\n            logits, probabilities = ENet(images,\n                                         num_classes,\n                                         batch_size=batch_size,\n                                         is_training=True,\n                                         reuse=None,\n                                         num_initial_blocks=num_initial_blocks,\n                                         stage_two_repeat=stage_two_repeat,\n                                         skip_connections=skip_connections)\n\n        #perform one-hot-encoding on the ground truth annotation to get same shape as the logits\n        annotations = tf.reshape(annotations, shape=[batch_size, image_height, image_width])\n        annotations_ohe = tf.one_hot(annotations, num_classes, axis=-1)\n\n        #Actually compute the loss\n        loss = weighted_cross_entropy(logits=logits, onehot_labels=annotations_ohe, class_weights=class_weights)\n        total_loss = tf.losses.get_total_loss()\n\n        #Create the global step for monitoring the learning_rate and training.\n        global_step = get_or_create_global_step()\n\n        #Define your exponentially decaying learning rate\n        lr = tf.train.exponential_decay(\n            learning_rate = initial_learning_rate,\n            global_step = global_step,\n            decay_steps = decay_steps,\n            decay_rate = learning_rate_decay_factor,\n            staircase = True)\n\n        #Now we can define the optimizer that takes on the learning rate\n        optimizer = tf.train.AdamOptimizer(learning_rate=lr, epsilon=epsilon)\n\n        #Create the train_op.\n        train_op = slim.learning.create_train_op(total_loss, optimizer)\n\n        #State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n        predictions = tf.argmax(probabilities, -1)\n        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, annotations)\n        mean_IOU, mean_IOU_update = tf.contrib.metrics.streaming_mean_iou(predictions=predictions, labels=annotations, num_classes=num_classes)\n        metrics_op = tf.group(accuracy_update, mean_IOU_update)\n\n        #Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.\n        def train_step(sess, train_op, global_step, metrics_op):\n            \'\'\'\n            Simply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step\n            \'\'\'\n            #Check the time for each sess run\n            start_time = time.time()\n            total_loss, global_step_count, accuracy_val, mean_IOU_val, _ = sess.run([train_op, global_step, accuracy, mean_IOU, metrics_op])\n            time_elapsed = time.time() - start_time\n\n            #Run the logging to show some results\n            logging.info(\'global step %s: loss: %.4f (%.2f sec/step)    Current Streaming Accuracy: %.4f    Current Mean IOU: %.4f\', global_step_count, total_loss, time_elapsed, accuracy_val, mean_IOU_val)\n\n            return total_loss, accuracy_val, mean_IOU_val\n\n        #================VALIDATION BRANCH========================\n        #Load the files into one input queue\n        images_val = tf.convert_to_tensor(image_val_files)\n        annotations_val = tf.convert_to_tensor(annotation_val_files)\n        input_queue_val = tf.train.slice_input_producer([images_val, annotations_val])\n\n        #Decode the image and annotation raw content\n        image_val = tf.read_file(input_queue_val[0])\n        image_val = tf.image.decode_jpeg(image_val, channels=3)\n        annotation_val = tf.read_file(input_queue_val[1])\n        annotation_val = tf.image.decode_png(annotation_val)\n\n        #preprocess and batch up the image and annotation\n        preprocessed_image_val, preprocessed_annotation_val = preprocess(image_val, annotation_val, image_height, image_width)\n        images_val, annotations_val = tf.train.batch([preprocessed_image_val, preprocessed_annotation_val], batch_size=eval_batch_size, allow_smaller_final_batch=True)\n\n        with slim.arg_scope(ENet_arg_scope(weight_decay=weight_decay)):\n            logits_val, probabilities_val = ENet(images_val,\n                                                 num_classes,\n                                                 batch_size=eval_batch_size,\n                                                 is_training=True,\n                                                 reuse=True,\n                                                 num_initial_blocks=num_initial_blocks,\n                                                 stage_two_repeat=stage_two_repeat,\n                                                 skip_connections=skip_connections)\n\n        #perform one-hot-encoding on the ground truth annotation to get same shape as the logits\n        annotations_val = tf.reshape(annotations_val, shape=[eval_batch_size, image_height, image_width])\n        annotations_ohe_val = tf.one_hot(annotations_val, num_classes, axis=-1)\n\n        #State the metrics that you want to predict. We get a predictions that is not one_hot_encoded. ----> Should we use OHE instead?\n        predictions_val = tf.argmax(probabilities_val, -1)\n        accuracy_val, accuracy_val_update = tf.contrib.metrics.streaming_accuracy(predictions_val, annotations_val)\n        mean_IOU_val, mean_IOU_val_update = tf.contrib.metrics.streaming_mean_iou(predictions=predictions_val, labels=annotations_val, num_classes=num_classes)\n        metrics_op_val = tf.group(accuracy_val_update, mean_IOU_val_update)\n\n        #Create an output for showing the segmentation output of validation images\n        segmentation_output_val = tf.cast(predictions_val, dtype=tf.float32)\n        segmentation_output_val = tf.reshape(segmentation_output_val, shape=[-1, image_height, image_width, 1])\n        segmentation_ground_truth_val = tf.cast(annotations_val, dtype=tf.float32)\n        segmentation_ground_truth_val = tf.reshape(segmentation_ground_truth_val, shape=[-1, image_height, image_width, 1])\n\n        def eval_step(sess, metrics_op):\n            \'\'\'\n            Simply takes in a session, runs the metrics op and some logging information.\n            \'\'\'\n            start_time = time.time()\n            _, accuracy_value, mean_IOU_value = sess.run([metrics_op, accuracy_val, mean_IOU_val])\n            time_elapsed = time.time() - start_time\n\n            #Log some information\n            logging.info(\'---VALIDATION--- Validation Accuracy: %.4f    Validation Mean IOU: %.4f    (%.2f sec/step)\', accuracy_value, mean_IOU_value, time_elapsed)\n\n            return accuracy_value, mean_IOU_value\n\n        #=====================================================\n\n        #Now finally create all the summaries you need to monitor and group them into one summary op.\n        tf.summary.scalar(\'Monitor/Total_Loss\', total_loss)\n        tf.summary.scalar(\'Monitor/validation_accuracy\', accuracy_val)\n        tf.summary.scalar(\'Monitor/training_accuracy\', accuracy)\n        tf.summary.scalar(\'Monitor/validation_mean_IOU\', mean_IOU_val)\n        tf.summary.scalar(\'Monitor/training_mean_IOU\', mean_IOU)\n        tf.summary.scalar(\'Monitor/learning_rate\', lr)\n        tf.summary.image(\'Images/Validation_original_image\', images_val, max_outputs=1)\n        tf.summary.image(\'Images/Validation_segmentation_output\', segmentation_output_val, max_outputs=1)\n        tf.summary.image(\'Images/Validation_segmentation_ground_truth\', segmentation_ground_truth_val, max_outputs=1)\n        my_summary_op = tf.summary.merge_all()\n\n        #Define your supervisor for running a managed session. Do not run the summary_op automatically or else it will consume too much memory\n        sv = tf.train.Supervisor(logdir=logdir, summary_op=None, init_fn=None)\n\n        # Run the managed session\n        with sv.managed_session() as sess:\n            for step in xrange(int(num_steps_per_epoch * num_epochs)):\n                #At the start of every epoch, show the vital information:\n                if step % num_batches_per_epoch == 0:\n                    logging.info(\'Epoch %s/%s\', step/num_batches_per_epoch + 1, num_epochs)\n                    learning_rate_value = sess.run([lr])\n                    logging.info(\'Current Learning Rate: %s\', learning_rate_value)\n\n                #Log the summaries every 10 steps or every end of epoch, which ever lower.\n                if step % min(num_steps_per_epoch, 10) == 0:\n                    loss, training_accuracy, training_mean_IOU = train_step(sess, train_op, sv.global_step, metrics_op=metrics_op)\n\n                    #Check the validation data only at every third of an epoch\n                    if step % (num_steps_per_epoch / 3) == 0:\n                        for i in xrange(len(image_val_files) / eval_batch_size):\n                            validation_accuracy, validation_mean_IOU = eval_step(sess, metrics_op_val)\n\n                    summaries = sess.run(my_summary_op)\n                    sv.summary_computed(sess, summaries)\n                    \n                #If not, simply run the training step\n                else:\n                    loss, training_accuracy,training_mean_IOU = train_step(sess, train_op, sv.global_step, metrics_op=metrics_op)\n\n            #We log the final training loss\n            logging.info(\'Final Loss: %s\', loss)\n            logging.info(\'Final Training Accuracy: %s\', training_accuracy)\n            logging.info(\'Final Training Mean IOU: %s\', training_mean_IOU)\n            logging.info(\'Final Validation Accuracy: %s\', validation_accuracy)\n            logging.info(\'Final Validation Mean IOU: %s\', validation_mean_IOU)\n\n            #Once all the training has been done, save the log files and checkpoint model\n            logging.info(\'Finished training! Saving model to disk now.\')\n            sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\n\n            if save_images:\n                if not os.path.exists(photo_dir):\n                    os.mkdir(photo_dir)\n\n                #Plot the predictions - check validation images only\n                logging.info(\'Saving the images now...\')\n                predictions_value, annotations_value = sess.run([predictions_val, annotations_val])\n\n                for i in xrange(eval_batch_size):\n                    predicted_annotation = predictions_value[i]\n                    annotation = annotations_value[i]\n\n                    plt.subplot(1,2,1)\n                    plt.imshow(predicted_annotation)\n                    plt.subplot(1,2,2)\n                    plt.imshow(annotation)\n                    plt.savefig(photo_dir+""/image_"" + str(i))\n\nif __name__ == \'__main__\':\n    run()'"
dataset/train/read_img.py,0,"b""from scipy.misc import imread\nimport os\n\nfiles = [file for file in os.listdir('.') if file.endswith('.png')]\n\nfor file in files:\n\timage = imread(file)\n\tif image.shape[2] != 3:\n\t\tprint image.shape[2]"""
