file_path,api_count,code
eval_flowers.py,12,"b""import tensorflow as tf\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\nimport inception_preprocessing\nfrom inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\nimport time\nimport os\nfrom train_flowers import get_split, load_batch\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nslim = tf.contrib.slim\n\n#State your log directory where you can retrieve your model\nlog_dir = './log'\n\n#Create a new evaluation log directory to visualize the validation process\nlog_eval = './log_eval_test'\n\n#State the dataset directory where the validation set is found\ndataset_dir = '.'\n\n#State the batch_size to evaluate each time, which can be a lot more than the training batch\nbatch_size = 36\n\n#State the number of epochs to evaluate\nnum_epochs = 1\n\n#Get the latest checkpoint file\ncheckpoint_file = tf.train.latest_checkpoint(log_dir)\n\ndef run():\n    #Create log_dir for evaluation information\n    if not os.path.exists(log_eval):\n        os.mkdir(log_eval)\n\n    #Just construct the graph from scratch again\n    with tf.Graph().as_default() as graph:\n        tf.logging.set_verbosity(tf.logging.INFO)\n        #Get the dataset first and load one batch of validation images and labels tensors. Set is_training as False so as to use the evaluation preprocessing\n        dataset = get_split('validation', dataset_dir)\n        images, raw_images, labels = load_batch(dataset, batch_size = batch_size, is_training = False)\n\n        #Create some information about the training steps\n        num_batches_per_epoch = dataset.num_samples / batch_size\n        num_steps_per_epoch = num_batches_per_epoch\n\n        #Now create the inference model but set is_training=False\n        with slim.arg_scope(inception_resnet_v2_arg_scope()):\n            logits, end_points = inception_resnet_v2(images, num_classes = dataset.num_classes, is_training = False)\n\n        # #get all the variables to restore from the checkpoint file and create the saver function to restore\n        variables_to_restore = slim.get_variables_to_restore()\n        saver = tf.train.Saver(variables_to_restore)\n        def restore_fn(sess):\n            return saver.restore(sess, checkpoint_file)\n\n        #Just define the metrics to track without the loss or whatsoever\n        predictions = tf.argmax(end_points['Predictions'], 1)\n        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n        metrics_op = tf.group(accuracy_update)\n\n        #Create the global step and an increment op for monitoring\n        global_step = get_or_create_global_step()\n        global_step_op = tf.assign(global_step, global_step + 1) #no apply_gradient method so manually increasing the global_step\n        \n\n        #Create a evaluation step function\n        def eval_step(sess, metrics_op, global_step):\n            '''\n            Simply takes in a session, runs the metrics op and some logging information.\n            '''\n            start_time = time.time()\n            _, global_step_count, accuracy_value = sess.run([metrics_op, global_step_op, accuracy])\n            time_elapsed = time.time() - start_time\n\n            #Log some information\n            logging.info('Global Step %s: Streaming Accuracy: %.4f (%.2f sec/step)', global_step_count, accuracy_value, time_elapsed)\n\n            return accuracy_value\n\n\n        #Define some scalar quantities to monitor\n        tf.summary.scalar('Validation_Accuracy', accuracy)\n        my_summary_op = tf.summary.merge_all()\n\n        #Get your supervisor\n        sv = tf.train.Supervisor(logdir = log_eval, summary_op = None, saver = None, init_fn = restore_fn)\n\n        #Now we are ready to run in one session\n        with sv.managed_session() as sess:\n            for step in xrange(num_steps_per_epoch * num_epochs):\n                sess.run(sv.global_step)\n                #print vital information every start of the epoch as always\n                if step % num_batches_per_epoch == 0:\n                    logging.info('Epoch: %s/%s', step / num_batches_per_epoch + 1, num_epochs)\n                    logging.info('Current Streaming Accuracy: %.4f', sess.run(accuracy))\n                    \n                #Compute summaries every 10 steps and continue evaluating\n                if step % 10 == 0:\n                    eval_step(sess, metrics_op = metrics_op, global_step = sv.global_step)\n                    summaries = sess.run(my_summary_op)\n                    sv.summary_computed(sess, summaries)\n                    \n\n                #Otherwise just run as per normal\n                else:\n                    eval_step(sess, metrics_op = metrics_op, global_step = sv.global_step)\n\n            #At the end of all the evaluation, show the final accuracy\n            logging.info('Final Streaming Accuracy: %.4f', sess.run(accuracy))\n\n            #Now we want to visualize the last batch's images just to see what our model has predicted\n            raw_images, labels, predictions = sess.run([raw_images, labels, predictions])\n            for i in range(10):\n                image, label, prediction = raw_images[i], labels[i], predictions[i]\n                prediction_name, label_name = dataset.labels_to_name[prediction], dataset.labels_to_name[label]\n                text = 'Prediction: %s \\n Ground Truth: %s' %(prediction_name, label_name)\n                img_plot = plt.imshow(image)\n\n                #Set up the plot and hide axes\n                plt.title(text)\n                img_plot.axes.get_yaxis().set_ticks([])\n                img_plot.axes.get_xaxis().set_ticks([])\n                plt.show()\n\n            logging.info('Model evaluation has completed! Visit TensorBoard for more information regarding your evaluation.')\n\nif __name__ == '__main__':\n    run()\n"""
inception_preprocessing.py,62,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n      image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # A large fraction of image datasets contain a human-annotated bounding\n    # box delineating the region of the image containing the object of interest.\n    # We choose to create a new bounding box for the object which is a randomly\n    # distorted version of the human-annotated bounding box that obeys an\n    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n    # bounding box. If no box is supplied, then we assume the bounding box is\n    # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n    return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True,\n                         scope=None):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n  with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                         dtype=tf.float32,\n                         shape=[1, 1, 4])\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                  bbox)\n    tf.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    image_with_distorted_box = tf.image.draw_bounding_boxes(\n        tf.expand_dims(image, 0), distorted_bbox)\n    tf.summary.image(\'images_with_distorted_bounding_box\',\n                     image_with_distorted_box)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [height, width], method=method),\n        num_cases=num_resize_cases)\n\n    tf.summary.image(\'cropped_resized_image\',\n                     tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors. There are 4 ways to do it.\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, ordering: distort_color(x, ordering, fast_mode),\n        num_cases=4)\n\n    tf.summary.image(\'final_distorted_image\',\n                     tf.expand_dims(distorted_image, 0))\n    distorted_image = tf.subtract(distorted_image, 0.5)\n    distorted_image = tf.multiply(distorted_image, 2.0)\n    return distorted_image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would cropt the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    return image\n\n\ndef preprocess_image(image, height, width,\n                     is_training=False,\n                     bbox=None,\n                     fast_mode=True):\n  """"""Pre-process one image for training or evaluation.\n\n  Args:\n    image: 3-D Tensor [height, width, channels] with the image.\n    height: integer, image expected height.\n    width: integer, image expected width.\n    is_training: Boolean. If true it would transform an image for train,\n      otherwise it would transform it for evaluation.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations.\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  """"""\n  if is_training:\n    return preprocess_for_train(image, height, width, bbox, fast_mode)\n  else:\n    return preprocess_for_eval(image, height, width)\n'"
inception_resnet_v2.py,39,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 35x35 resnet block.""""""\n  with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n      tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 17x17 resnet block.""""""\n  with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                  scope=\'Conv2d_0b_1x7\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                  scope=\'Conv2d_0c_7x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 8x8 resnet block.""""""\n  with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                  scope=\'Conv2d_0b_1x3\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                  scope=\'Conv2d_0c_3x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n                        dropout_keep_prob=0.8,\n                        reuse=None,\n                        scope=\'InceptionResnetV2\'):\n  """"""Creates the Inception Resnet V2 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs], reuse=reuse):\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n\n        # 149 x 149 x 32\n        net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n                          scope=\'Conv2d_1a_3x3\')\n        end_points[\'Conv2d_1a_3x3\'] = net\n        # 147 x 147 x 32\n        net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n                          scope=\'Conv2d_2a_3x3\')\n        end_points[\'Conv2d_2a_3x3\'] = net\n        # 147 x 147 x 64\n        net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n        end_points[\'Conv2d_2b_3x3\'] = net\n        # 73 x 73 x 64\n        net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                              scope=\'MaxPool_3a_3x3\')\n        end_points[\'MaxPool_3a_3x3\'] = net\n        # 73 x 73 x 80\n        net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n                          scope=\'Conv2d_3b_1x1\')\n        end_points[\'Conv2d_3b_1x1\'] = net\n        # 71 x 71 x 192\n        net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n                          scope=\'Conv2d_4a_3x3\')\n        end_points[\'Conv2d_4a_3x3\'] = net\n        # 35 x 35 x 192\n        net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                              scope=\'MaxPool_5a_3x3\')\n        end_points[\'MaxPool_5a_3x3\'] = net\n\n        # 35 x 35 x 320\n        with tf.variable_scope(\'Mixed_5b\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                        scope=\'Conv2d_0b_5x5\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                        scope=\'Conv2d_0c_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n                                         scope=\'AvgPool_0a_3x3\')\n            tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                       scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_1,\n                              tower_conv2_2, tower_pool_1])\n\n        end_points[\'Mixed_5b\'] = net\n        net = slim.repeat(net, 10, block35, scale=0.17)\n\n        # 17 x 17 x 1024\n        with tf.variable_scope(\'Mixed_6a\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\'VALID\',\n                                     scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                        stride=2, padding=\'VALID\',\n                                        scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                         scope=\'MaxPool_1a_3x3\')\n          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_2, tower_pool])\n\n        end_points[\'Mixed_6a\'] = net\n        net = slim.repeat(net, 20, block17, scale=0.10)\n\n        # Auxillary tower\n        with tf.variable_scope(\'AuxLogits\'):\n          aux = slim.avg_pool2d(net, 5, stride=3, padding=\'VALID\',\n                                scope=\'Conv2d_1a_3x3\')\n          aux = slim.conv2d(aux, 128, 1, scope=\'Conv2d_1b_1x1\')\n          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n                            padding=\'VALID\', scope=\'Conv2d_2a_5x5\')\n          aux = slim.flatten(aux)\n          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n                                     scope=\'Logits\')\n          end_points[\'AuxLogits\'] = aux\n\n        with tf.variable_scope(\'Mixed_7a\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                       padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                         scope=\'MaxPool_1a_3x3\')\n          net = tf.concat(axis=3, values=[tower_conv_1, tower_conv1_1,\n                              tower_conv2_2, tower_pool])\n\n        end_points[\'Mixed_7a\'] = net\n\n        net = slim.repeat(net, 9, block8, scale=0.20)\n        net = block8(net, activation_fn=None)\n\n        net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n        end_points[\'Conv2d_7b_1x1\'] = net\n\n        with tf.variable_scope(\'Logits\'):\n          end_points[\'PrePool\'] = net\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a_8x8\')\n          net = slim.flatten(net)\n\n          net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                             scope=\'Dropout\')\n\n          end_points[\'PreLogitsFlatten\'] = net\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n\n    return logits, end_points\ninception_resnet_v2.default_image_size = 299\n\n\ndef inception_resnet_v2_arg_scope(weight_decay=0.00004,\n                                  batch_norm_decay=0.9997,\n                                  batch_norm_epsilon=0.001):\n  """"""Yields the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n  """"""\n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon,\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n'"
train_flowers.py,28,"b'import tensorflow as tf\nfrom tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\nfrom tensorflow.python.platform import tf_logging as logging\nimport inception_preprocessing\nfrom inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\nimport os\nimport time\nslim = tf.contrib.slim\n\n#================ DATASET INFORMATION ======================\n#State dataset directory where the tfrecord files are located\ndataset_dir = \'.\'\n\n#State where your log file is at. If it doesn\'t exist, create it.\nlog_dir = \'./log\'\n\n#State where your checkpoint file is\ncheckpoint_file = \'./inception_resnet_v2_2016_08_30.ckpt\'\n\n#State the image size you\'re resizing your images to. We will use the default inception size of 299.\nimage_size = 299\n\n#State the number of classes to predict:\nnum_classes = 5\n\n#State the labels file and read it\nlabels_file = \'./labels.txt\'\nlabels = open(labels_file, \'r\')\n\n#Create a dictionary to refer each label to their string name\nlabels_to_name = {}\nfor line in labels:\n    label, string_name = line.split(\':\')\n    string_name = string_name[:-1] #Remove newline\n    labels_to_name[int(label)] = string_name\n\n#Create the file pattern of your TFRecord files so that it could be recognized later on\nfile_pattern = \'flowers_%s_*.tfrecord\'\n\n#Create a dictionary that will help people understand your dataset better. This is required by the Dataset class later.\nitems_to_descriptions = {\n    \'image\': \'A 3-channel RGB coloured flower image that is either tulips, sunflowers, roses, dandelion, or daisy.\',\n    \'label\': \'A label that is as such -- 0:daisy, 1:dandelion, 2:roses, 3:sunflowers, 4:tulips\'\n}\n\n\n#================= TRAINING INFORMATION ==================\n#State the number of epochs to train\nnum_epochs = 1\n\n#State your batch size\nbatch_size = 8\n\n#Learning rate information and configuration (Up to you to experiment)\ninitial_learning_rate = 0.0002\nlearning_rate_decay_factor = 0.7\nnum_epochs_before_decay = 2\n\n#============== DATASET LOADING ======================\n#We now create a function that creates a Dataset class which will give us many TFRecord files to feed in the examples into a queue in parallel.\ndef get_split(split_name, dataset_dir, file_pattern=file_pattern, file_pattern_for_counting=\'flowers\'):\n    \'\'\'\n    Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\n    set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\n    Your file_pattern is very important in locating the files later. \n\n    INPUTS:\n    - split_name(str): \'train\' or \'validation\'. Used to get the correct data split of tfrecord files\n    - dataset_dir(str): the dataset directory where the tfrecord files are located\n    - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n    - file_pattern_for_counting(str): the string name to identify your tfrecord files for counting\n\n    OUTPUTS:\n    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n    \'\'\'\n\n    #First check whether the split_name is train or validation\n    if split_name not in [\'train\', \'validation\']:\n        raise ValueError(\'The split_name %s is not recognized. Please input either train or validation as the split_name\' % (split_name))\n\n    #Create the full path for a general file_pattern to locate the tfrecord_files\n    file_pattern_path = os.path.join(dataset_dir, file_pattern % (split_name))\n\n    #Count the total number of examples in all of these shard\n    num_samples = 0\n    file_pattern_for_counting = file_pattern_for_counting + \'_\' + split_name\n    tfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if file.startswith(file_pattern_for_counting)]\n    for tfrecord_file in tfrecords_to_count:\n        for record in tf.python_io.tf_record_iterator(tfrecord_file):\n            num_samples += 1\n\n    #Create a reader, which must be a TFRecord reader in this case\n    reader = tf.TFRecordReader\n\n    #Create the keys_to_features dictionary for the decoder\n    keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'jpg\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n    }\n\n    #Create the items_to_handlers dictionary for the decoder.\n    items_to_handlers = {\n    \'image\': slim.tfexample_decoder.Image(),\n    \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n    }\n\n    #Start to create the decoder\n    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n\n    #Create the labels_to_name file\n    labels_to_name_dict = labels_to_name\n\n    #Actually create the dataset\n    dataset = slim.dataset.Dataset(\n        data_sources = file_pattern_path,\n        decoder = decoder,\n        reader = reader,\n        num_readers = 4,\n        num_samples = num_samples,\n        num_classes = num_classes,\n        labels_to_name = labels_to_name_dict,\n        items_to_descriptions = items_to_descriptions)\n\n    return dataset\n\n\ndef load_batch(dataset, batch_size, height=image_size, width=image_size, is_training=True):\n    \'\'\'\n    Loads a batch for training.\n\n    INPUTS:\n    - dataset(Dataset): a Dataset class object that is created from the get_split function\n    - batch_size(int): determines how big of a batch to train\n    - height(int): the height of the image to resize to during preprocessing\n    - width(int): the width of the image to resize to during preprocessing\n    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\n\n    OUTPUTS:\n    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n    - labels(Tensor): the batch\'s labels with the shape (batch_size,) (requires one_hot_encoding).\n\n    \'\'\'\n    #First create the data_provider object\n    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n        dataset,\n        common_queue_capacity = 24 + 3 * batch_size,\n        common_queue_min = 24)\n\n    #Obtain the raw image using the get method\n    raw_image, label = data_provider.get([\'image\', \'label\'])\n\n    #Perform the correct preprocessing for this image depending if it is training or evaluating\n    image = inception_preprocessing.preprocess_image(raw_image, height, width, is_training)\n\n    #As for the raw images, we just do a simple reshape to batch it up\n    raw_image = tf.expand_dims(raw_image, 0)\n    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n    raw_image = tf.squeeze(raw_image)\n\n    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\n    images, raw_images, labels = tf.train.batch(\n        [image, raw_image, label],\n        batch_size = batch_size,\n        num_threads = 4,\n        capacity = 4 * batch_size,\n        allow_smaller_final_batch = True)\n\n    return images, raw_images, labels\n\ndef run():\n    #Create the log directory here. Must be done here otherwise import will activate this unneededly.\n    if not os.path.exists(log_dir):\n        os.mkdir(log_dir)\n\n    #======================= TRAINING PROCESS =========================\n    #Now we start to construct the graph and build our model\n    with tf.Graph().as_default() as graph:\n        tf.logging.set_verbosity(tf.logging.INFO) #Set the verbosity to INFO level\n\n        #First create the dataset and load one batch\n        dataset = get_split(\'train\', dataset_dir, file_pattern=file_pattern)\n        images, _, labels = load_batch(dataset, batch_size=batch_size)\n\n        #Know the number steps to take before decaying the learning rate and batches per epoch\n        num_batches_per_epoch = int(dataset.num_samples / batch_size)\n        num_steps_per_epoch = num_batches_per_epoch #Because one step is one batch processed\n        decay_steps = int(num_epochs_before_decay * num_steps_per_epoch)\n\n        #Create the model inference\n        with slim.arg_scope(inception_resnet_v2_arg_scope()):\n            logits, end_points = inception_resnet_v2(images, num_classes = dataset.num_classes, is_training = True)\n\n        #Define the scopes that you want to exclude for restoration\n        exclude = [\'InceptionResnetV2/Logits\', \'InceptionResnetV2/AuxLogits\']\n        variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n\n        #Perform one-hot-encoding of the labels (Try one-hot-encoding within the load_batch function!)\n        one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n\n        #Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced with checks\n        loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n        total_loss = tf.losses.get_total_loss()    #obtain the regularization losses as well\n\n        #Create the global step for monitoring the learning_rate and training.\n        global_step = get_or_create_global_step()\n\n        #Define your exponentially decaying learning rate\n        lr = tf.train.exponential_decay(\n            learning_rate = initial_learning_rate,\n            global_step = global_step,\n            decay_steps = decay_steps,\n            decay_rate = learning_rate_decay_factor,\n            staircase = True)\n\n        #Now we can define the optimizer that takes on the learning rate\n        optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n\n        #Create the train_op.\n        train_op = slim.learning.create_train_op(total_loss, optimizer)\n\n        #State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n        predictions = tf.argmax(end_points[\'Predictions\'], 1)\n        probabilities = end_points[\'Predictions\']\n        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n        metrics_op = tf.group(accuracy_update, probabilities)\n\n\n        #Now finally create all the summaries you need to monitor and group them into one summary op.\n        tf.summary.scalar(\'losses/Total_Loss\', total_loss)\n        tf.summary.scalar(\'accuracy\', accuracy)\n        tf.summary.scalar(\'learning_rate\', lr)\n        my_summary_op = tf.summary.merge_all()\n\n        #Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.\n        def train_step(sess, train_op, global_step):\n            \'\'\'\n            Simply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step\n            \'\'\'\n            #Check the time for each sess run\n            start_time = time.time()\n            total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n            time_elapsed = time.time() - start_time\n\n            #Run the logging to print some results\n            logging.info(\'global step %s: loss: %.4f (%.2f sec/step)\', global_step_count, total_loss, time_elapsed)\n\n            return total_loss, global_step_count\n\n        #Now we create a saver function that actually restores the variables from a checkpoint file in a sess\n        saver = tf.train.Saver(variables_to_restore)\n        def restore_fn(sess):\n            return saver.restore(sess, checkpoint_file)\n\n        #Define your supervisor for running a managed session. Do not run the summary_op automatically or else it will consume too much memory\n        sv = tf.train.Supervisor(logdir = log_dir, summary_op = None, init_fn = restore_fn)\n\n\n        #Run the managed session\n        with sv.managed_session() as sess:\n            for step in xrange(num_steps_per_epoch * num_epochs):\n                #At the start of every epoch, show the vital information:\n                if step % num_batches_per_epoch == 0:\n                    logging.info(\'Epoch %s/%s\', step/num_batches_per_epoch + 1, num_epochs)\n                    learning_rate_value, accuracy_value = sess.run([lr, accuracy])\n                    logging.info(\'Current Learning Rate: %s\', learning_rate_value)\n                    logging.info(\'Current Streaming Accuracy: %s\', accuracy_value)\n\n                    # optionally, print your logits and predictions for a sanity check that things are going fine.\n                    logits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\n                    print \'logits: \\n\', logits_value\n                    print \'Probabilities: \\n\', probabilities_value\n                    print \'predictions: \\n\', predictions_value\n                    print \'Labels:\\n:\', labels_value\n\n                #Log the summaries every 10 step.\n                if step % 10 == 0:\n                    loss, _ = train_step(sess, train_op, sv.global_step)\n                    summaries = sess.run(my_summary_op)\n                    sv.summary_computed(sess, summaries)\n                    \n                #If not, simply run the training step\n                else:\n                    loss, _ = train_step(sess, train_op, sv.global_step)\n\n            #We log the final training loss and accuracy\n            logging.info(\'Final Loss: %s\', loss)\n            logging.info(\'Final Accuracy: %s\', sess.run(accuracy))\n\n            #Once all the training has been done, save the log files and checkpoint model\n            logging.info(\'Finished training! Saving model to disk now.\')\n            # saver.save(sess, ""./flowers_model.ckpt"")\n            sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\n\n\nif __name__ == \'__main__\':\n    run()\n\n                \n\n'"
