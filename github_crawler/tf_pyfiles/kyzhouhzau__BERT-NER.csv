file_path,api_count,code
BERT_NER.py,50,"b'#! usr/bin/env python3\r\n# -*- coding:utf-8 -*-\r\n""""""\r\n# Copyright 2018 The Google AI Language Team Authors.\r\n# Copyright 2019 The BioNLP-HZAU Kaiyin Zhou\r\n# Time:2019/04/08\r\n""""""\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport collections\r\nimport os\r\nimport pickle\r\nfrom absl import flags,logging\r\nfrom bert import modeling\r\nfrom bert import optimization\r\nfrom bert import tokenization\r\nimport tensorflow as tf\r\nimport metrics\r\nimport numpy as np\r\nFLAGS = flags.FLAGS\r\n\r\n## Required parameters\r\nflags.DEFINE_string(\r\n    ""data_dir"", None,\r\n    ""The input data dir. Should contain the .tsv files (or other data files) ""\r\n    ""for the task."")\r\n\r\nflags.DEFINE_string(\r\n    ""bert_config_file"", None,\r\n    ""The config json file corresponding to the pre-trained BERT model. ""\r\n    ""This specifies the model architecture."")\r\n\r\nflags.DEFINE_string(""task_name"", None, ""The name of the task to train."")\r\n\r\nflags.DEFINE_string(""vocab_file"", None,\r\n                    ""The vocabulary file that the BERT model was trained on."")\r\n\r\nflags.DEFINE_string(\r\n    ""output_dir"", None,\r\n    ""The output directory where the model checkpoints will be written."")\r\n\r\n## Other parameters\r\n\r\nflags.DEFINE_string(\r\n    ""init_checkpoint"", None,\r\n    ""Initial checkpoint (usually from a pre-trained BERT model)."")\r\n\r\n# if you download cased checkpoint you should use ""False"",if uncased you should use\r\n# ""True""\r\n# if we used in bio-medical field\xef\xbc\x8cdon\'t do lower case would be better!\r\n\r\nflags.DEFINE_bool(\r\n    ""do_lower_case"", True,\r\n    ""Whether to lower case the input text. Should be True for uncased ""\r\n    ""models and False for cased models."")\r\n\r\nflags.DEFINE_integer(\r\n    ""max_seq_length"", 128,\r\n    ""The maximum total input sequence length after WordPiece tokenization. ""\r\n    ""Sequences longer than this will be truncated, and sequences shorter ""\r\n    ""than this will be padded."")\r\n\r\nflags.DEFINE_bool(""do_train"", False, ""Whether to run training."")\r\n\r\nflags.DEFINE_bool(""do_eval"", False, ""Whether to run eval on the dev set."")\r\n\r\nflags.DEFINE_bool(\r\n    ""do_predict"", False,\r\n    ""Whether to run the model in inference mode on the test set."")\r\n\r\nflags.DEFINE_integer(""train_batch_size"", 32, ""Total batch size for training."")\r\n\r\nflags.DEFINE_integer(""eval_batch_size"", 8, ""Total batch size for eval."")\r\n\r\nflags.DEFINE_integer(""predict_batch_size"", 8, ""Total batch size for predict."")\r\n\r\nflags.DEFINE_float(""learning_rate"", 5e-5, ""The initial learning rate for Adam."")\r\n\r\nflags.DEFINE_float(""num_train_epochs"", 3.0,\r\n                   ""Total number of training epochs to perform."")\r\n\r\nflags.DEFINE_float(\r\n    ""warmup_proportion"", 0.1,\r\n    ""Proportion of training to perform linear learning rate warmup for. ""\r\n    ""E.g., 0.1 = 10% of training."")\r\n\r\nflags.DEFINE_integer(""save_checkpoints_steps"", 1000,\r\n                     ""How often to save the model checkpoint."")\r\n\r\nflags.DEFINE_integer(""iterations_per_loop"", 1000,\r\n                     ""How many steps to make in each estimator call."")\r\n\r\nflags.DEFINE_bool(""use_tpu"", False, ""Whether to use TPU or GPU/CPU."")\r\n\r\nflags.DEFINE_string(\r\n    ""tpu_name"", None,\r\n    ""The Cloud TPU to use for training. This should be either the name ""\r\n    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 ""\r\n    ""url."")\r\n\r\nflags.DEFINE_string(\r\n    ""tpu_zone"", None,\r\n    ""[Optional] GCE zone where the Cloud TPU is located in. If not ""\r\n    ""specified, we will attempt to automatically detect the GCE project from ""\r\n    ""metadata."")\r\n\r\nflags.DEFINE_string(\r\n    ""gcp_project"", None,\r\n    ""[Optional] Project name for the Cloud TPU-enabled project. If not ""\r\n    ""specified, we will attempt to automatically detect the GCE project from ""\r\n    ""metadata."")\r\n\r\nflags.DEFINE_string(""master"", None, ""[Optional] TensorFlow master URL."")\r\n\r\nflags.DEFINE_integer(\r\n    ""num_tpu_cores"", 8,\r\n    ""Only used if `use_tpu` is True. Total number of TPU cores to use."")\r\n\r\nflags.DEFINE_string(""middle_output"", ""middle_data"", ""Dir was used to store middle data!"")\r\nflags.DEFINE_bool(""crf"", True, ""use crf!"")\r\n\r\nclass InputExample(object):\r\n  """"""A single training/test example for simple sequence classification.""""""\r\n\r\n  def __init__(self, guid, text, label=None):\r\n    """"""Constructs a InputExample.\r\n\r\n    Args:\r\n      guid: Unique id for the example.\r\n      text_a: string. The untokenized text of the first sequence. For single\r\n        sequence tasks, only this sequence must be specified.\r\n      label: (Optional) string. The label of the example. This should be\r\n        specified for train and dev examples, but not for test examples.\r\n    """"""\r\n    self.guid = guid\r\n    self.text = text\r\n    self.label = label\r\n\r\nclass PaddingInputExample(object):\r\n  """"""Fake example so the num input examples is a multiple of the batch size.\r\n\r\n  When running eval/predict on the TPU, we need to pad the number of examples\r\n  to be a multiple of the batch size, because the TPU requires a fixed batch\r\n  size. The alternative is to drop the last batch, which is bad because it means\r\n  the entire output data won\'t be generated.\r\n\r\n  We use this class instead of `None` because treating `None` as padding\r\n  battches could cause silent errors.\r\n  """"""\r\n\r\nclass InputFeatures(object):\r\n  """"""A single set of features of data.""""""\r\n\r\n  def __init__(self,\r\n               input_ids,\r\n               mask,\r\n               segment_ids,\r\n               label_ids,\r\n               is_real_example=True):\r\n    self.input_ids = input_ids\r\n    self.mask = mask\r\n    self.segment_ids = segment_ids\r\n    self.label_ids = label_ids\r\n    self.is_real_example = is_real_example\r\n\r\nclass DataProcessor(object):\r\n    """"""Base class for data converters for sequence classification data sets.""""""\r\n\r\n    def get_train_examples(self, data_dir):\r\n        """"""Gets a collection of `InputExample`s for the train set.""""""\r\n        raise NotImplementedError()\r\n\r\n    def get_dev_examples(self, data_dir):\r\n        """"""Gets a collection of `InputExample`s for the dev set.""""""\r\n        raise NotImplementedError()\r\n\r\n    def get_labels(self):\r\n        """"""Gets the list of labels for this data set.""""""\r\n        raise NotImplementedError()\r\n\r\n    @classmethod\r\n    def _read_data(cls,input_file):\r\n        """"""Read a BIO data!""""""\r\n        rf = open(input_file,\'r\')\r\n        lines = [];words = [];labels = []\r\n        for line in rf:\r\n            word = line.strip().split(\' \')[0]\r\n            label = line.strip().split(\' \')[-1]\r\n            # here we dont do ""DOCSTART"" check\r\n            if len(line.strip())==0 and words[-1] == \'.\':\r\n                l = \' \'.join([label for label in labels if len(label) > 0])\r\n                w = \' \'.join([word for word in words if len(word) > 0])\r\n                lines.append((l,w))\r\n                words=[]\r\n                labels = []\r\n            words.append(word)\r\n            labels.append(label)\r\n        rf.close()\r\n        return lines\r\n\r\nclass NerProcessor(DataProcessor):\r\n    def get_train_examples(self, data_dir):\r\n        return self._create_example(\r\n            self._read_data(os.path.join(data_dir, ""train.txt"")), ""train""\r\n        )\r\n\r\n    def get_dev_examples(self, data_dir):\r\n        return self._create_example(\r\n            self._read_data(os.path.join(data_dir, ""dev.txt"")), ""dev""\r\n        )\r\n\r\n    def get_test_examples(self,data_dir):\r\n        return self._create_example(\r\n            self._read_data(os.path.join(data_dir, ""test.txt"")), ""test""\r\n        )\r\n\r\n\r\n    def get_labels(self):\r\n        """"""\r\n        here ""X"" used to represent ""##eer"",""##soo"" and so on!\r\n        ""[PAD]"" for padding\r\n        :return:\r\n        """"""\r\n        return [""[PAD]"",""B-MISC"", ""I-MISC"", ""O"", ""B-PER"", ""I-PER"", ""B-ORG"", ""I-ORG"", ""B-LOC"", ""I-LOC"", ""X"",""[CLS]"",""[SEP]""]\r\n\r\n    def _create_example(self, lines, set_type):\r\n        examples = []\r\n        for (i, line) in enumerate(lines):\r\n            guid = ""%s-%s"" % (set_type, i)\r\n            texts = tokenization.convert_to_unicode(line[1])\r\n            labels = tokenization.convert_to_unicode(line[0])\r\n            examples.append(InputExample(guid=guid, text=texts, label=labels))\r\n        return examples\r\n\r\n\r\ndef convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, mode):\r\n    """"""\r\n    :param ex_index: example num\r\n    :param example:\r\n    :param label_list: all labels\r\n    :param max_seq_length:\r\n    :param tokenizer: WordPiece tokenization\r\n    :param mode:\r\n    :return: feature\r\n\r\n    IN this part we should rebuild input sentences to the following format.\r\n    example:[Jim,Hen,##son,was,a,puppet,##eer]\r\n    labels: [I-PER,I-PER,X,O,O,O,X]\r\n\r\n    """"""\r\n    label_map = {}\r\n    #here start with zero this means that ""[PAD]"" is zero\r\n    for (i,label) in enumerate(label_list):\r\n        label_map[label] = i\r\n    with open(FLAGS.middle_output+""/label2id.pkl"",\'wb\') as w:\r\n        pickle.dump(label_map,w)\r\n    textlist = example.text.split(\' \')\r\n    labellist = example.label.split(\' \')\r\n    tokens = []\r\n    labels = []\r\n    for i,(word,label) in enumerate(zip(textlist,labellist)):\r\n        token = tokenizer.tokenize(word)\r\n        tokens.extend(token)\r\n        for i,_ in enumerate(token):\r\n            if i==0:\r\n                labels.append(label)\r\n            else:\r\n                labels.append(""X"")\r\n    # only Account for [CLS] with ""- 1"".\r\n    if len(tokens) >= max_seq_length - 1:\r\n        tokens = tokens[0:(max_seq_length - 1)]\r\n        labels = labels[0:(max_seq_length - 1)]\r\n    ntokens = []\r\n    segment_ids = []\r\n    label_ids = []\r\n    ntokens.append(""[CLS]"")\r\n    segment_ids.append(0)\r\n    label_ids.append(label_map[""[CLS]""])\r\n    for i, token in enumerate(tokens):\r\n        ntokens.append(token)\r\n        segment_ids.append(0)\r\n        label_ids.append(label_map[labels[i]])\r\n    # after that we don\'t add ""[SEP]"" because we want a sentence don\'t have\r\n    # stop tag, because i think its not very necessary.\r\n    # or if add ""[SEP]"" the model even will cause problem, special the crf layer was used.\r\n    input_ids = tokenizer.convert_tokens_to_ids(ntokens)\r\n    mask = [1]*len(input_ids)\r\n    #use zero to padding and you should\r\n    while len(input_ids) < max_seq_length:\r\n        input_ids.append(0)\r\n        mask.append(0)\r\n        segment_ids.append(0)\r\n        label_ids.append(0)\r\n        ntokens.append(""[PAD]"")\r\n    assert len(input_ids) == max_seq_length\r\n    assert len(mask) == max_seq_length\r\n    assert len(segment_ids) == max_seq_length\r\n    assert len(label_ids) == max_seq_length\r\n    assert len(ntokens) == max_seq_length\r\n    if ex_index < 3:\r\n        logging.info(""*** Example ***"")\r\n        logging.info(""guid: %s"" % (example.guid))\r\n        logging.info(""tokens: %s"" % "" "".join(\r\n            [tokenization.printable_text(x) for x in tokens]))\r\n        logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\r\n        logging.info(""input_mask: %s"" % "" "".join([str(x) for x in mask]))\r\n        logging.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\r\n        logging.info(""label_ids: %s"" % "" "".join([str(x) for x in label_ids]))\r\n    feature = InputFeatures(\r\n        input_ids=input_ids,\r\n        mask=mask,\r\n        segment_ids=segment_ids,\r\n        label_ids=label_ids,\r\n    )\r\n    # we need ntokens because if we do predict it can help us return to original token.\r\n    return feature,ntokens,label_ids\r\n\r\ndef filed_based_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_file,mode=None):\r\n    writer = tf.python_io.TFRecordWriter(output_file)\r\n    batch_tokens = []\r\n    batch_labels = []\r\n    for (ex_index, example) in enumerate(examples):\r\n        if ex_index % 5000 == 0:\r\n            logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))\r\n        feature,ntokens,label_ids = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, mode)\r\n        batch_tokens.extend(ntokens)\r\n        batch_labels.extend(label_ids)\r\n        def create_int_feature(values):\r\n            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\r\n            return f\r\n\r\n        features = collections.OrderedDict()\r\n        features[""input_ids""] = create_int_feature(feature.input_ids)\r\n        features[""mask""] = create_int_feature(feature.mask)\r\n        features[""segment_ids""] = create_int_feature(feature.segment_ids)\r\n        features[""label_ids""] = create_int_feature(feature.label_ids)\r\n        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\r\n        writer.write(tf_example.SerializeToString())\r\n    # sentence token in each batch\r\n    writer.close()\r\n    return batch_tokens,batch_labels\r\n\r\ndef file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder):\r\n    name_to_features = {\r\n        ""input_ids"": tf.FixedLenFeature([seq_length], tf.int64),\r\n        ""mask"": tf.FixedLenFeature([seq_length], tf.int64),\r\n        ""segment_ids"": tf.FixedLenFeature([seq_length], tf.int64),\r\n        ""label_ids"": tf.FixedLenFeature([seq_length], tf.int64),\r\n\r\n    }\r\n    def _decode_record(record, name_to_features):\r\n        example = tf.parse_single_example(record, name_to_features)\r\n        for name in list(example.keys()):\r\n            t = example[name]\r\n            if t.dtype == tf.int64:\r\n                t = tf.to_int32(t)\r\n            example[name] = t\r\n        return example\r\n\r\n    def input_fn(params):\r\n        batch_size = params[""batch_size""]\r\n        d = tf.data.TFRecordDataset(input_file)\r\n        if is_training:\r\n            d = d.repeat()\r\n            d = d.shuffle(buffer_size=100)\r\n        d = d.apply(tf.data.experimental.map_and_batch(\r\n            lambda record: _decode_record(record, name_to_features),\r\n            batch_size=batch_size,\r\n            drop_remainder=drop_remainder\r\n        ))\r\n        return d\r\n    return input_fn\r\n\r\n# all above are related to data preprocess\r\n# Following i about the model\r\n\r\ndef hidden2tag(hiddenlayer,numclass):\r\n    linear = tf.keras.layers.Dense(numclass,activation=None)\r\n    return linear(hiddenlayer)\r\n\r\ndef crf_loss(logits,labels,mask,num_labels,mask2len):\r\n    """"""\r\n    :param logits:\r\n    :param labels:\r\n    :param mask2len:each sample\'s length\r\n    :return:\r\n    """"""\r\n    #TODO\r\n    with tf.variable_scope(""crf_loss""):\r\n        trans = tf.get_variable(\r\n                ""transition"",\r\n                shape=[num_labels,num_labels],\r\n                initializer=tf.contrib.layers.xavier_initializer()\r\n        )\r\n    \r\n    log_likelihood,transition = tf.contrib.crf.crf_log_likelihood(logits,labels,transition_params =trans ,sequence_lengths=mask2len)\r\n    loss = tf.math.reduce_mean(-log_likelihood)\r\n   \r\n    return loss,transition\r\n\r\ndef softmax_layer(logits,labels,num_labels,mask):\r\n    logits = tf.reshape(logits, [-1, num_labels])\r\n    labels = tf.reshape(labels, [-1])\r\n    mask = tf.cast(mask,dtype=tf.float32)\r\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\r\n    loss = tf.losses.softmax_cross_entropy(logits=logits,onehot_labels=one_hot_labels)\r\n    loss *= tf.reshape(mask, [-1])\r\n    loss = tf.reduce_sum(loss)\r\n    total_size = tf.reduce_sum(mask)\r\n    total_size += 1e-12 # to avoid division by 0 for all-0 weights\r\n    loss /= total_size\r\n    # predict not mask we could filtered it in the prediction part.\r\n    probabilities = tf.math.softmax(logits, axis=-1)\r\n    predict = tf.math.argmax(probabilities, axis=-1)\r\n    return loss, predict\r\n\r\n\r\ndef create_model(bert_config, is_training, input_ids, mask,\r\n                 segment_ids, labels, num_labels, use_one_hot_embeddings):\r\n    model = modeling.BertModel(\r\n        config = bert_config,\r\n        is_training=is_training,\r\n        input_ids=input_ids,\r\n        input_mask=mask,\r\n        token_type_ids=segment_ids,\r\n        use_one_hot_embeddings=use_one_hot_embeddings\r\n        )\r\n\r\n    output_layer = model.get_sequence_output()\r\n    #output_layer shape is\r\n    if is_training:\r\n        output_layer = tf.keras.layers.Dropout(rate=0.1)(output_layer)\r\n    logits = hidden2tag(output_layer,num_labels)\r\n    # TODO test shape\r\n    logits = tf.reshape(logits,[-1,FLAGS.max_seq_length,num_labels])\r\n    if FLAGS.crf:\r\n        mask2len = tf.reduce_sum(mask,axis=1)\r\n        loss, trans = crf_loss(logits,labels,mask,num_labels,mask2len)\r\n        predict,viterbi_score = tf.contrib.crf.crf_decode(logits, trans, mask2len)\r\n        return (loss, logits,predict)\r\n\r\n    else:\r\n        loss,predict  = softmax_layer(logits, labels, num_labels, mask)\r\n\r\n        return (loss, logits, predict)\r\n\r\ndef model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\r\n                     num_train_steps, num_warmup_steps, use_tpu,\r\n                     use_one_hot_embeddings):\r\n    def model_fn(features, labels, mode, params):\r\n        logging.info(""*** Features ***"")\r\n        for name in sorted(features.keys()):\r\n            logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))\r\n        input_ids = features[""input_ids""]\r\n        mask = features[""mask""]\r\n        segment_ids = features[""segment_ids""]\r\n        label_ids = features[""label_ids""]\r\n        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n        if FLAGS.crf:\r\n            (total_loss, logits,predicts) = create_model(bert_config, is_training, input_ids,\r\n                                                            mask, segment_ids, label_ids,num_labels, \r\n                                                            use_one_hot_embeddings)\r\n\r\n        else:\r\n            (total_loss, logits, predicts) = create_model(bert_config, is_training, input_ids,\r\n                                                            mask, segment_ids, label_ids,num_labels, \r\n                                                            use_one_hot_embeddings)\r\n        tvars = tf.trainable_variables()\r\n        scaffold_fn = None\r\n        initialized_variable_names=None\r\n        if init_checkpoint:\r\n            (assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars,init_checkpoint)\r\n            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n            if use_tpu:\r\n                def tpu_scaffold():\r\n                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n                    return tf.train.Scaffold()\r\n                scaffold_fn = tpu_scaffold\r\n            else:\r\n\r\n                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n        logging.info(""**** Trainable Variables ****"")\r\n        for var in tvars:\r\n            init_string = """"\r\n            if var.name in initialized_variable_names:\r\n                init_string = "", *INIT_FROM_CKPT*""\r\n            logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,\r\n                            init_string)\r\n\r\n        \r\n\r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n            train_op = optimization.create_optimizer(total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\r\n            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n                mode=mode,\r\n                loss=total_loss,\r\n                train_op=train_op,\r\n                scaffold_fn=scaffold_fn)\r\n\r\n        elif mode == tf.estimator.ModeKeys.EVAL:\r\n            def metric_fn(label_ids, logits,num_labels,mask):\r\n                predictions = tf.math.argmax(logits, axis=-1, output_type=tf.int32)\r\n                cm = metrics.streaming_confusion_matrix(label_ids, predictions, num_labels-1, weights=mask)\r\n                return {\r\n                    ""confusion_matrix"":cm\r\n                }\r\n                #\r\n            eval_metrics = (metric_fn, [label_ids, logits, num_labels, mask])\r\n            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n                mode=mode,\r\n                loss=total_loss,\r\n                eval_metrics=eval_metrics,\r\n                scaffold_fn=scaffold_fn)\r\n        else:\r\n            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n                mode=mode, predictions=predicts, scaffold_fn=scaffold_fn\r\n            )\r\n        return output_spec\r\n\r\n    return model_fn\r\n\r\n\r\ndef _write_base(batch_tokens,id2label,prediction,batch_labels,wf,i):\r\n    token = batch_tokens[i]\r\n    predict = id2label[prediction]\r\n    true_l = id2label[batch_labels[i]]\r\n    if token!=""[PAD]"" and token!=""[CLS]"" and true_l!=""X"":\r\n        #\r\n        if predict==""X"" and not predict.startswith(""##""):\r\n            predict=""O""\r\n        line = ""{}\\t{}\\t{}\\n"".format(token,true_l,predict)\r\n        wf.write(line)\r\n\r\ndef Writer(output_predict_file,result,batch_tokens,batch_labels,id2label):\r\n    with open(output_predict_file,\'w\') as wf:\r\n\r\n        if  FLAGS.crf:\r\n            predictions  = []\r\n            for m,pred in enumerate(result):\r\n                predictions.extend(pred)\r\n            for i,prediction in enumerate(predictions):\r\n                _write_base(batch_tokens,id2label,prediction,batch_labels,wf,i)\r\n                \r\n        else:\r\n            for i,prediction in enumerate(result):\r\n                _write_base(batch_tokens,id2label,prediction,batch_labels,wf,i)\r\n            \r\n\r\n\r\ndef main(_):\r\n    logging.set_verbosity(logging.INFO)\r\n    processors = {""ner"": NerProcessor}\r\n    if not FLAGS.do_train and not FLAGS.do_eval:\r\n        raise ValueError(""At least one of `do_train` or `do_eval` must be True."")\r\n    bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\r\n    if FLAGS.max_seq_length > bert_config.max_position_embeddings:\r\n        raise ValueError(\r\n            ""Cannot use sequence length %d because the BERT model ""\r\n            ""was only trained up to sequence length %d"" %\r\n            (FLAGS.max_seq_length, bert_config.max_position_embeddings))\r\n    task_name = FLAGS.task_name.lower()\r\n    if task_name not in processors:\r\n        raise ValueError(""Task not found: %s"" % (task_name))\r\n    processor = processors[task_name]()\r\n\r\n    label_list = processor.get_labels()\r\n\r\n    tokenizer = tokenization.FullTokenizer(\r\n        vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\r\n    tpu_cluster_resolver = None\r\n    if FLAGS.use_tpu and FLAGS.tpu_name:\r\n        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\r\n            FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\r\n    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\r\n    run_config = tf.contrib.tpu.RunConfig(\r\n        cluster=tpu_cluster_resolver,\r\n        master=FLAGS.master,\r\n        model_dir=FLAGS.output_dir,\r\n        save_checkpoints_steps=FLAGS.save_checkpoints_steps,\r\n        tpu_config=tf.contrib.tpu.TPUConfig(\r\n            iterations_per_loop=FLAGS.iterations_per_loop,\r\n            num_shards=FLAGS.num_tpu_cores,\r\n            per_host_input_for_training=is_per_host))\r\n    train_examples = None\r\n    num_train_steps = None\r\n    num_warmup_steps = None\r\n\r\n    if FLAGS.do_train:\r\n        train_examples = processor.get_train_examples(FLAGS.data_dir)\r\n\r\n        num_train_steps = int(\r\n            len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\r\n        num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\r\n    model_fn = model_fn_builder(\r\n        bert_config=bert_config,\r\n        num_labels=len(label_list),\r\n        init_checkpoint=FLAGS.init_checkpoint,\r\n        learning_rate=FLAGS.learning_rate,\r\n        num_train_steps=num_train_steps,\r\n        num_warmup_steps=num_warmup_steps,\r\n        use_tpu=FLAGS.use_tpu,\r\n        use_one_hot_embeddings=FLAGS.use_tpu)\r\n    estimator = tf.contrib.tpu.TPUEstimator(\r\n        use_tpu=FLAGS.use_tpu,\r\n        model_fn=model_fn,\r\n        config=run_config,\r\n        train_batch_size=FLAGS.train_batch_size,\r\n        eval_batch_size=FLAGS.eval_batch_size,\r\n        predict_batch_size=FLAGS.predict_batch_size)\r\n\r\n\r\n    if FLAGS.do_train:\r\n        train_file = os.path.join(FLAGS.output_dir, ""train.tf_record"")\r\n        _,_ = filed_based_convert_examples_to_features(\r\n            train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\r\n        logging.info(""***** Running training *****"")\r\n        logging.info(""  Num examples = %d"", len(train_examples))\r\n        logging.info(""  Batch size = %d"", FLAGS.train_batch_size)\r\n        logging.info(""  Num steps = %d"", num_train_steps)\r\n        train_input_fn = file_based_input_fn_builder(\r\n            input_file=train_file,\r\n            seq_length=FLAGS.max_seq_length,\r\n            is_training=True,\r\n            drop_remainder=True)\r\n        estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n    if FLAGS.do_eval:\r\n        eval_examples = processor.get_dev_examples(FLAGS.data_dir)\r\n        eval_file = os.path.join(FLAGS.output_dir, ""eval.tf_record"")\r\n        batch_tokens,batch_labels = filed_based_convert_examples_to_features(\r\n            eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\r\n\r\n        logging.info(""***** Running evaluation *****"")\r\n        logging.info(""  Num examples = %d"", len(eval_examples))\r\n        logging.info(""  Batch size = %d"", FLAGS.eval_batch_size)\r\n        # if FLAGS.use_tpu:\r\n        #     eval_steps = int(len(eval_examples) / FLAGS.eval_batch_size)\r\n        # eval_drop_remainder = True if FLAGS.use_tpu else False\r\n        eval_input_fn = file_based_input_fn_builder(\r\n            input_file=eval_file,\r\n            seq_length=FLAGS.max_seq_length,\r\n            is_training=False,\r\n            drop_remainder=False)\r\n        result = estimator.evaluate(input_fn=eval_input_fn)\r\n        output_eval_file = os.path.join(FLAGS.output_dir, ""eval_results.txt"")\r\n        with open(output_eval_file,""w"") as wf:\r\n            logging.info(""***** Eval results *****"")\r\n            confusion_matrix = result[""confusion_matrix""]\r\n            p,r,f = metrics.calculate(confusion_matrix,len(label_list)-1)\r\n            logging.info(""***********************************************"")\r\n            logging.info(""********************P = %s*********************"",  str(p))\r\n            logging.info(""********************R = %s*********************"",  str(r))\r\n            logging.info(""********************F = %s*********************"",  str(f))\r\n            logging.info(""***********************************************"")\r\n\r\n\r\n    if FLAGS.do_predict:\r\n        with open(FLAGS.middle_output+\'/label2id.pkl\', \'rb\') as rf:\r\n            label2id = pickle.load(rf)\r\n            id2label = {value: key for key, value in label2id.items()}\r\n   \r\n        predict_examples = processor.get_test_examples(FLAGS.data_dir)\r\n\r\n        predict_file = os.path.join(FLAGS.output_dir, ""predict.tf_record"")\r\n        batch_tokens,batch_labels = filed_based_convert_examples_to_features(predict_examples, label_list,\r\n                                                 FLAGS.max_seq_length, tokenizer,\r\n                                                 predict_file)\r\n\r\n        logging.info(""***** Running prediction*****"")\r\n        logging.info(""  Num examples = %d"", len(predict_examples))\r\n        logging.info(""  Batch size = %d"", FLAGS.predict_batch_size)\r\n\r\n        predict_input_fn = file_based_input_fn_builder(\r\n            input_file=predict_file,\r\n            seq_length=FLAGS.max_seq_length,\r\n            is_training=False,\r\n            drop_remainder=False)\r\n\r\n        result = estimator.predict(input_fn=predict_input_fn)\r\n        output_predict_file = os.path.join(FLAGS.output_dir, ""label_test.txt"")\r\n        #here if the tag is ""X"" means it belong to its before token, here for convenient evaluate use\r\n        # conlleval.pl we  discarding it directly\r\n        Writer(output_predict_file,result,batch_tokens,batch_labels,id2label)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    flags.mark_flag_as_required(""data_dir"")\r\n    flags.mark_flag_as_required(""task_name"")\r\n    flags.mark_flag_as_required(""vocab_file"")\r\n    flags.mark_flag_as_required(""bert_config_file"")\r\n    flags.mark_flag_as_required(""output_dir"")\r\n    tf.app.run()\r\n'"
function_test.py,0,"b'#! usr/bin/env python3\n# -*- coding:utf-8 -*-\n""""""\n@Author:zhoukaiyin\n""""""\n\n\ndef _read_data(input_file):\n    """"""Read a BIO data!""""""\n    rf = open(input_file, \'r\')\n    lines = []; words = []; labels = []\n    for line in rf:\n        word = line.strip().split(\' \')[0]\n        label = line.strip().split(\' \')[-1]\n        # here we dont do ""DOCSTART"" check\n        if len(line.strip()) == 0 and words[-1] == \'.\':\n            l = \' \'.join([label for label in labels if len(label) > 0])\n            w = \' \'.join([word for word in words if len(word) > 0])\n            lines.append((l, w))\n            words = []\n            labels = []\n        words.append(word)\n        labels.append(label)\n    return lines\n\ndef main():\n   lines =  _read_data(""./data/train.txt"")\n   print(lines)\nmain()\n'"
metrics.py,1,"b'#! usr/bin/env python3\n# -*- coding:utf-8 -*-\n""""""\n# Copyright 2016 Google\n# Copyright 2019 The BioNLP-HZAU Kaiyin Zhou\n# Time:2019/04/08\n""""""\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import confusion_matrix\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nimport numpy as np\n\ndef metric_variable(shape, dtype, validate_shape=True, name=None):\n    """"""Create variable in `GraphKeys.(LOCAL|METRIC_VARIABLES)` collections.\n    If running in a `DistributionStrategy` context, the variable will be\n    ""tower local"". This means:\n    *   The returned object will be a container with separate variables\n        per replica/tower of the model.\n    *   When writing to the variable, e.g. using `assign_add` in a metric\n        update, the update will be applied to the variable local to the\n        replica/tower.\n    *   To get a metric\'s result value, we need to sum the variable values\n        across the replicas/towers before computing the final answer.\n        Furthermore, the final answer should be computed once instead of\n        in every replica/tower. Both of these are accomplished by\n        running the computation of the final result value inside\n        `tf.contrib.distribution_strategy_context.get_tower_context(\n        ).merge_call(fn)`.\n        Inside the `merge_call()`, ops are only added to the graph once\n        and access to a tower-local variable in a computation returns\n        the sum across all replicas/towers.\n    Args:\n        shape: Shape of the created variable.\n        dtype: Type of the created variable.\n        validate_shape: (Optional) Whether shape validation is enabled for\n        the created variable.\n        name: (Optional) String name of the created variable.\n    Returns:\n        A (non-trainable) variable initialized to zero, or if inside a\n        `DistributionStrategy` scope a tower-local variable container.\n    """"""\n    # Note that synchronization ""ON_READ"" implies trainable=False.\n    return variable_scope.variable(\n        lambda: array_ops.zeros(shape, dtype),\n        collections=[\n            ops.GraphKeys.LOCAL_VARIABLES, ops.GraphKeys.METRIC_VARIABLES\n        ],\n        validate_shape=validate_shape,\n        synchronization=variable_scope.VariableSynchronization.ON_READ,\n        aggregation=variable_scope.VariableAggregation.SUM,\n        name=name)\n\ndef streaming_confusion_matrix(labels, predictions, num_classes, weights=None):\n    """"""Calculate a streaming confusion matrix.\n    Calculates a confusion matrix. For estimation over a stream of data,\n    the function creates an  `update_op` operation.\n    Args:\n        labels: A `Tensor` of ground truth labels with shape [batch size] and of\n        type `int32` or `int64`. The tensor will be flattened if its rank > 1.\n        predictions: A `Tensor` of prediction results for semantic labels, whose\n        shape is [batch size] and type `int32` or `int64`. The tensor will be\n        flattened if its rank > 1.\n        num_classes: The possible number of labels the prediction task can\n        have. This value must be provided, since a confusion matrix of\n        dimension = [num_classes, num_classes] will be allocated.\n        weights: Optional `Tensor` whose rank is either 0, or the same rank as\n        `labels`, and must be broadcastable to `labels` (i.e., all dimensions must\n        be either `1`, or the same as the corresponding `labels` dimension).\n    Returns:\n        total_cm: A `Tensor` representing the confusion matrix.\n        update_op: An operation that increments the confusion matrix.\n    """"""\n    # Local variable to accumulate the predictions in the confusion matrix.\n    total_cm = metric_variable(\n        [num_classes, num_classes], dtypes.float64, name=\'total_confusion_matrix\')\n\n    # Cast the type to int64 required by confusion_matrix_ops.\n    predictions = math_ops.to_int64(predictions)\n    labels = math_ops.to_int64(labels)\n    num_classes = math_ops.to_int64(num_classes)\n\n    # Flatten the input if its rank > 1.\n    if predictions.get_shape().ndims > 1:\n        predictions = array_ops.reshape(predictions, [-1])\n\n    if labels.get_shape().ndims > 1:\n        labels = array_ops.reshape(labels, [-1])\n\n    if (weights is not None) and (weights.get_shape().ndims > 1):\n        weights = array_ops.reshape(weights, [-1])\n\n    # Accumulate the prediction to current confusion matrix.\n    current_cm = confusion_matrix.confusion_matrix(\n        labels, predictions, num_classes, weights=weights, dtype=dtypes.float64)\n    update_op = state_ops.assign_add(total_cm, current_cm)\n    return (total_cm, update_op)\n\n\ndef calculate(total_cm, num_class):\n    precisions = []\n    recalls = []\n    fs = []\n    for i in range(num_class):\n        rowsum, colsum = np.sum(total_cm[i]), np.sum(total_cm[r][i] for r in range(num_class))\n        precision = total_cm[i][i] / float(colsum+1e-12)\n        recall = total_cm[i][i] / float(rowsum+1e-12)\n        f = 2 * precision * recall / (precision + recall+1e-12)\n        precisions.append(precision)\n        recalls.append(recall)\n        fs.append(f)\n    return np.mean(precisions), np.mean(recalls), np.mean(fs)\n'"
old_version/BERT_NER.py,78,"b'#! usr/bin/env python3\n# -*- coding:utf-8 -*-\n""""""\nCopyright 2018 The Google AI Language Team Authors.\nBASED ON Google_BERT.\n@Author:zhoukaiyin\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\nfrom bert import modeling\nfrom bert import optimization\nfrom bert import tokenization\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score,precision_score,recall_score\nfrom tensorflow.python.ops import math_ops\nimport tf_metrics\nimport pickle\nflags = tf.flags\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    ""data_dir"", None,\n    ""The input datadir."",\n)\n\nflags.DEFINE_string(\n    ""bert_config_file"", None,\n    ""The config json file corresponding to the pre-trained BERT model.""\n)\n\nflags.DEFINE_string(\n    ""task_name"", None, ""The name of the task to train.""\n)\n\nflags.DEFINE_string(\n    ""output_dir"", None,\n    ""The output directory where the model checkpoints will be written.""\n)\n\n## Other parameters\nflags.DEFINE_string(\n    ""init_checkpoint"", None,\n    ""Initial checkpoint (usually from a pre-trained BERT model).""\n)\n\nflags.DEFINE_bool(\n    ""do_lower_case"", True,\n    ""Whether to lower case the input text.""\n)\n\nflags.DEFINE_integer(\n    ""max_seq_length"", 128,\n    ""The maximum total input sequence length after WordPiece tokenization.""\n)\n\nflags.DEFINE_bool(\n    ""do_train"", False,\n    ""Whether to run training.""\n)\nflags.DEFINE_bool(""use_tpu"", False, ""Whether to use TPU or GPU/CPU."")\n\nflags.DEFINE_bool(""do_eval"", False, ""Whether to run eval on the dev set."")\n\nflags.DEFINE_bool(""do_predict"", False,""Whether to run the model in inference mode on the test set."")\n\nflags.DEFINE_integer(""train_batch_size"", 32, ""Total batch size for training."")\n\nflags.DEFINE_integer(""eval_batch_size"", 8, ""Total batch size for eval."")\n\nflags.DEFINE_integer(""predict_batch_size"", 8, ""Total batch size for predict."")\n\nflags.DEFINE_float(""learning_rate"", 5e-5, ""The initial learning rate for Adam."")\n\nflags.DEFINE_float(""num_train_epochs"", 3.0, ""Total number of training epochs to perform."")\n\n\n\nflags.DEFINE_float(\n    ""warmup_proportion"", 0.1,\n    ""Proportion of training to perform linear learning rate warmup for. ""\n    ""E.g., 0.1 = 10% of training."")\n\nflags.DEFINE_integer(""save_checkpoints_steps"", 1000,\n                     ""How often to save the model checkpoint."")\n\nflags.DEFINE_integer(""iterations_per_loop"", 1000,\n                     ""How many steps to make in each estimator call."")\n\nflags.DEFINE_string(""vocab_file"", None,\n                    ""The vocabulary file that the BERT model was trained on."")\ntf.flags.DEFINE_string(""master"", None, ""[Optional] TensorFlow master URL."")\nflags.DEFINE_integer(\n    ""num_tpu_cores"", 8,\n    ""Only used if `use_tpu` is True. Total number of TPU cores to use."")\n\nclass InputExample(object):\n    """"""A single training/test example for simple sequence classification.""""""\n\n    def __init__(self, guid, text, label=None):\n        """"""Constructs a InputExample.\n\n        Args:\n          guid: Unique id for the example.\n          text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n          label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        """"""\n        self.guid = guid\n        self.text = text\n        self.label = label\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_ids,):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_ids = label_ids\n        #self.label_mask = label_mask\n\n\nclass DataProcessor(object):\n    """"""Base class for data converters for sequence classification data sets.""""""\n\n    def get_train_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the train set.""""""\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the dev set.""""""\n        raise NotImplementedError()\n\n    def get_labels(self):\n        """"""Gets the list of labels for this data set.""""""\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_data(cls, input_file):\n        """"""Reads a BIO data.""""""\n        with open(input_file) as f:\n            lines = []\n            words = []\n            labels = []\n            for line in f:\n                contends = line.strip()\n                word = line.strip().split(\' \')[0]\n                label = line.strip().split(\' \')[-1]\n                if contends.startswith(""-DOCSTART-""):\n                    words.append(\'\')\n                    continue\n                if len(contends) == 0 and words[-1] == \'.\':\n                    l = \' \'.join([label for label in labels if len(label) > 0])\n                    w = \' \'.join([word for word in words if len(word) > 0])\n                    lines.append([l, w])\n                    words = []\n                    labels = []\n                    continue\n                words.append(word)\n                labels.append(label)\n            return lines\n\n\nclass NerProcessor(DataProcessor):\n    def get_train_examples(self, data_dir):\n        return self._create_example(\n            self._read_data(os.path.join(data_dir, ""train.txt"")), ""train""\n        )\n\n    def get_dev_examples(self, data_dir):\n        return self._create_example(\n            self._read_data(os.path.join(data_dir, ""dev.txt"")), ""dev""\n        )\n\n    def get_test_examples(self,data_dir):\n        return self._create_example(\n            self._read_data(os.path.join(data_dir, ""test.txt"")), ""test"")\n\n\n    def get_labels(self):\n        return [""B-MISC"", ""I-MISC"", ""O"", ""B-PER"", ""I-PER"", ""B-ORG"", ""I-ORG"", ""B-LOC"", ""I-LOC"", ""X"",""[CLS]"",""[SEP]""]\n\n    def _create_example(self, lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = ""%s-%s"" % (set_type, i)\n            text = tokenization.convert_to_unicode(line[1])\n            label = tokenization.convert_to_unicode(line[0])\n            examples.append(InputExample(guid=guid, text=text, label=label))\n        return examples\n\n\ndef write_tokens(tokens,mode):\n    if mode==""test"":\n        path = os.path.join(FLAGS.output_dir, ""token_""+mode+"".txt"")\n        wf = open(path,\'a\')\n        for token in tokens:\n            if token!=""**NULL**"":\n                wf.write(token+\'\\n\')\n        wf.close()\n\ndef convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer,mode):\n    label_map = {}\n    for (i, label) in enumerate(label_list,1):\n        label_map[label] = i\n    with open(\'./output/label2id.pkl\',\'wb\') as w:\n        pickle.dump(label_map,w)\n    textlist = example.text.split(\' \')\n    labellist = example.label.split(\' \')\n    tokens = []\n    labels = []\n    for i, word in enumerate(textlist):\n        token = tokenizer.tokenize(word)\n        tokens.extend(token)\n        label_1 = labellist[i]\n        for m in range(len(token)):\n            if m == 0:\n                labels.append(label_1)\n            else:\n                labels.append(""X"")\n    # tokens = tokenizer.tokenize(example.text)\n    if len(tokens) >= max_seq_length - 1:\n        tokens = tokens[0:(max_seq_length - 2)]\n        labels = labels[0:(max_seq_length - 2)]\n    ntokens = []\n    segment_ids = []\n    label_ids = []\n    ntokens.append(""[CLS]"")\n    segment_ids.append(0)\n    # append(""O"") or append(""[CLS]"") not sure!\n    label_ids.append(label_map[""[CLS]""])\n    for i, token in enumerate(tokens):\n        ntokens.append(token)\n        segment_ids.append(0)\n        label_ids.append(label_map[labels[i]])\n    ntokens.append(""[SEP]"")\n    segment_ids.append(0)\n    # append(""O"") or append(""[SEP]"") not sure!\n    label_ids.append(label_map[""[SEP]""])\n    input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n    input_mask = [1] * len(input_ids)\n    #label_mask = [1] * len(input_ids)\n    while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n        # we don\'t concerned about it!\n        label_ids.append(0)\n        ntokens.append(""**NULL**"")\n        #label_mask.append(0)\n    # print(len(input_ids))\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n    assert len(label_ids) == max_seq_length\n    #assert len(label_mask) == max_seq_length\n\n    if ex_index < 5:\n        tf.logging.info(""*** Example ***"")\n        tf.logging.info(""guid: %s"" % (example.guid))\n        tf.logging.info(""tokens: %s"" % "" "".join(\n            [tokenization.printable_text(x) for x in tokens]))\n        tf.logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n        tf.logging.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n        tf.logging.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n        tf.logging.info(""label_ids: %s"" % "" "".join([str(x) for x in label_ids]))\n        #tf.logging.info(""label_mask: %s"" % "" "".join([str(x) for x in label_mask]))\n\n    feature = InputFeatures(\n        input_ids=input_ids,\n        input_mask=input_mask,\n        segment_ids=segment_ids,\n        label_ids=label_ids,\n        #label_mask = label_mask\n    )\n    write_tokens(ntokens,mode)\n    return feature\n\n\ndef filed_based_convert_examples_to_features(\n        examples, label_list, max_seq_length, tokenizer, output_file,mode=None\n):\n    writer = tf.python_io.TFRecordWriter(output_file)\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 5000 == 0:\n            tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer,mode)\n\n        def create_int_feature(values):\n            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n            return f\n\n        features = collections.OrderedDict()\n        features[""input_ids""] = create_int_feature(feature.input_ids)\n        features[""input_mask""] = create_int_feature(feature.input_mask)\n        features[""segment_ids""] = create_int_feature(feature.segment_ids)\n        features[""label_ids""] = create_int_feature(feature.label_ids)\n        #features[""label_mask""] = create_int_feature(feature.label_mask)\n        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n        writer.write(tf_example.SerializeToString())\n\n\ndef file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n    name_to_features = {\n        ""input_ids"": tf.FixedLenFeature([seq_length], tf.int64),\n        ""input_mask"": tf.FixedLenFeature([seq_length], tf.int64),\n        ""segment_ids"": tf.FixedLenFeature([seq_length], tf.int64),\n        ""label_ids"": tf.FixedLenFeature([seq_length], tf.int64),\n        # ""label_ids"":tf.VarLenFeature(tf.int64),\n        #""label_mask"": tf.FixedLenFeature([seq_length], tf.int64),\n    }\n\n    def _decode_record(record, name_to_features):\n        example = tf.parse_single_example(record, name_to_features)\n        for name in list(example.keys()):\n            t = example[name]\n            if t.dtype == tf.int64:\n                t = tf.to_int32(t)\n            example[name] = t\n        return example\n\n    def input_fn(params):\n        batch_size = params[""batch_size""]\n        d = tf.data.TFRecordDataset(input_file)\n        if is_training:\n            d = d.repeat()\n            d = d.shuffle(buffer_size=100)\n        d = d.apply(tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder\n        ))\n        return d\n    return input_fn\n\n\ndef create_model(bert_config, is_training, input_ids, input_mask,\n                 segment_ids, labels, num_labels, use_one_hot_embeddings):\n    model = modeling.BertModel(\n        config=bert_config,\n        is_training=is_training,\n        input_ids=input_ids,\n        input_mask=input_mask,\n        token_type_ids=segment_ids,\n        use_one_hot_embeddings=use_one_hot_embeddings\n    )\n\n    output_layer = model.get_sequence_output()\n\n    hidden_size = output_layer.shape[-1].value\n\n    output_weight = tf.get_variable(\n        ""output_weights"", [num_labels, hidden_size],\n        initializer=tf.truncated_normal_initializer(stddev=0.02)\n    )\n    output_bias = tf.get_variable(\n        ""output_bias"", [num_labels], initializer=tf.zeros_initializer()\n    )\n    with tf.variable_scope(""loss""):\n        if is_training:\n            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n        output_layer = tf.reshape(output_layer, [-1, hidden_size])\n        logits = tf.matmul(output_layer, output_weight, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        logits = tf.reshape(logits, [-1, FLAGS.max_seq_length, 13])\n        # mask = tf.cast(input_mask,tf.float32)\n        # loss = tf.contrib.seq2seq.sequence_loss(logits,labels,mask)\n        # return (loss, logits, predict)\n        ##########################################################################\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_sum(per_example_loss)\n        probabilities = tf.nn.softmax(logits, axis=-1)\n        predict = tf.argmax(probabilities,axis=-1)\n        return (loss, per_example_loss, logits,predict)\n        ##########################################################################\n        \ndef model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n                     num_train_steps, num_warmup_steps, use_tpu,\n                     use_one_hot_embeddings):\n    def model_fn(features, labels, mode, params):\n        tf.logging.info(""*** Features ***"")\n        for name in sorted(features.keys()):\n            tf.logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))\n        input_ids = features[""input_ids""]\n        input_mask = features[""input_mask""]\n        segment_ids = features[""segment_ids""]\n        label_ids = features[""label_ids""]\n        #label_mask = features[""label_mask""]\n        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n\n        (total_loss,  per_example_loss,logits,predicts) = create_model(\n            bert_config, is_training, input_ids, input_mask,segment_ids, label_ids,\n            num_labels, use_one_hot_embeddings)\n        tvars = tf.trainable_variables()\n        scaffold_fn = None\n        if init_checkpoint:\n            (assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars,init_checkpoint)\n            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n            if use_tpu:\n                def tpu_scaffold():\n                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n                    return tf.train.Scaffold()\n                scaffold_fn = tpu_scaffold\n            else:\n                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n        tf.logging.info(""**** Trainable Variables ****"")\n\n        for var in tvars:\n            init_string = """"\n            if var.name in initialized_variable_names:\n                init_string = "", *INIT_FROM_CKPT*""\n            tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,\n                            init_string)\n        output_spec = None\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            train_op = optimization.create_optimizer(\n                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n                mode=mode,\n                loss=total_loss,\n                train_op=train_op,\n                scaffold_fn=scaffold_fn)\n        elif mode == tf.estimator.ModeKeys.EVAL:\n            \n            def metric_fn(per_example_loss, label_ids, logits):\n            # def metric_fn(label_ids, logits):\n                predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n                precision = tf_metrics.precision(label_ids,predictions,13,[1,2,4,5,6,7,8,9],average=""macro"")\n                recall = tf_metrics.recall(label_ids,predictions,13,[1,2,4,5,6,7,8,9],average=""macro"")\n                f = tf_metrics.f1(label_ids,predictions,13,[1,2,4,5,6,7,8,9],average=""macro"")\n                #\n                return {\n                    ""eval_precision"":precision,\n                    ""eval_recall"":recall,\n                    ""eval_f"": f,\n                    #""eval_loss"": loss,\n                }\n            eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n            # eval_metrics = (metric_fn, [label_ids, logits])\n            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n                mode=mode,\n                loss=total_loss,\n                eval_metrics=eval_metrics,\n                scaffold_fn=scaffold_fn)\n        else:\n            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n                mode = mode,predictions= predicts,scaffold_fn=scaffold_fn\n            )\n        return output_spec\n    return model_fn\n\n\ndef main(_):\n    tf.logging.set_verbosity(tf.logging.INFO)\n    processors = {\n        ""ner"": NerProcessor\n    }\n    if not FLAGS.do_train and not FLAGS.do_eval:\n        raise ValueError(""At least one of `do_train` or `do_eval` must be True."")\n\n    bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n\n    if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n        raise ValueError(\n            ""Cannot use sequence length %d because the BERT model ""\n            ""was only trained up to sequence length %d"" %\n            (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n\n    task_name = FLAGS.task_name.lower()\n    if task_name not in processors:\n        raise ValueError(""Task not found: %s"" % (task_name))\n    processor = processors[task_name]()\n\n    label_list = processor.get_labels()\n\n    tokenizer = tokenization.FullTokenizer(\n        vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n    tpu_cluster_resolver = None\n    if FLAGS.use_tpu and FLAGS.tpu_name:\n        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n            FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n\n    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n\n    run_config = tf.contrib.tpu.RunConfig(\n        cluster=tpu_cluster_resolver,\n        master=FLAGS.master,\n        model_dir=FLAGS.output_dir,\n        save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n        tpu_config=tf.contrib.tpu.TPUConfig(\n            iterations_per_loop=FLAGS.iterations_per_loop,\n            num_shards=FLAGS.num_tpu_cores,\n            per_host_input_for_training=is_per_host))\n\n    train_examples = None\n    num_train_steps = None\n    num_warmup_steps = None\n\n    if FLAGS.do_train:\n        train_examples = processor.get_train_examples(FLAGS.data_dir)\n        num_train_steps = int(\n            len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n        num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n\n    model_fn = model_fn_builder(\n        bert_config=bert_config,\n        num_labels=len(label_list)+1,\n        init_checkpoint=FLAGS.init_checkpoint,\n        learning_rate=FLAGS.learning_rate,\n        num_train_steps=num_train_steps,\n        num_warmup_steps=num_warmup_steps,\n        use_tpu=FLAGS.use_tpu,\n        use_one_hot_embeddings=FLAGS.use_tpu)\n\n    estimator = tf.contrib.tpu.TPUEstimator(\n        use_tpu=FLAGS.use_tpu,\n        model_fn=model_fn,\n        config=run_config,\n        train_batch_size=FLAGS.train_batch_size,\n        eval_batch_size=FLAGS.eval_batch_size,\n        predict_batch_size=FLAGS.predict_batch_size)\n\n    if FLAGS.do_train:\n        train_file = os.path.join(FLAGS.output_dir, ""train.tf_record"")\n        filed_based_convert_examples_to_features(\n            train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\n        tf.logging.info(""***** Running training *****"")\n        tf.logging.info(""  Num examples = %d"", len(train_examples))\n        tf.logging.info(""  Batch size = %d"", FLAGS.train_batch_size)\n        tf.logging.info(""  Num steps = %d"", num_train_steps)\n        train_input_fn = file_based_input_fn_builder(\n            input_file=train_file,\n            seq_length=FLAGS.max_seq_length,\n            is_training=True,\n            drop_remainder=True)\n        estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n    if FLAGS.do_eval:\n        eval_examples = processor.get_dev_examples(FLAGS.data_dir)\n        eval_file = os.path.join(FLAGS.output_dir, ""eval.tf_record"")\n        filed_based_convert_examples_to_features(\n            eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\n\n        tf.logging.info(""***** Running evaluation *****"")\n        tf.logging.info(""  Num examples = %d"", len(eval_examples))\n        tf.logging.info(""  Batch size = %d"", FLAGS.eval_batch_size)\n        eval_steps = None\n        if FLAGS.use_tpu:\n            eval_steps = int(len(eval_examples) / FLAGS.eval_batch_size)\n        eval_drop_remainder = True if FLAGS.use_tpu else False\n        eval_input_fn = file_based_input_fn_builder(\n            input_file=eval_file,\n            seq_length=FLAGS.max_seq_length,\n            is_training=False,\n            drop_remainder=eval_drop_remainder)\n        result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n        output_eval_file = os.path.join(FLAGS.output_dir, ""eval_results.txt"")\n        with open(output_eval_file, ""w"") as writer:\n            tf.logging.info(""***** Eval results *****"")\n            for key in sorted(result.keys()):\n                tf.logging.info(""  %s = %s"", key, str(result[key]))\n                writer.write(""%s = %s\\n"" % (key, str(result[key])))\n    if FLAGS.do_predict:\n        token_path = os.path.join(FLAGS.output_dir, ""token_test.txt"")\n        with open(\'./output/label2id.pkl\',\'rb\') as rf:\n            label2id = pickle.load(rf)\n            id2label = {value:key for key,value in label2id.items()}\n        if os.path.exists(token_path):\n            os.remove(token_path)\n        predict_examples = processor.get_test_examples(FLAGS.data_dir)\n\n        predict_file = os.path.join(FLAGS.output_dir, ""predict.tf_record"")\n        filed_based_convert_examples_to_features(predict_examples, label_list,\n                                                FLAGS.max_seq_length, tokenizer,\n                                                predict_file,mode=""test"")\n                            \n        tf.logging.info(""***** Running prediction*****"")\n        tf.logging.info(""  Num examples = %d"", len(predict_examples))\n        tf.logging.info(""  Batch size = %d"", FLAGS.predict_batch_size)\n        if FLAGS.use_tpu:\n            # Warning: According to tpu_estimator.py Prediction on TPU is an\n            # experimental feature and hence not supported here\n            raise ValueError(""Prediction in TPU not supported"")\n        predict_drop_remainder = True if FLAGS.use_tpu else False\n        predict_input_fn = file_based_input_fn_builder(\n            input_file=predict_file,\n            seq_length=FLAGS.max_seq_length,\n            is_training=False,\n            drop_remainder=predict_drop_remainder)\n\n        result = estimator.predict(input_fn=predict_input_fn)\n        output_predict_file = os.path.join(FLAGS.output_dir, ""label_test.txt"")\n        with open(output_predict_file,\'w\') as writer:\n            for prediction in result:\n                output_line = ""\\n"".join(id2label[id] for id in prediction if id!=0) + ""\\n""\n                writer.write(output_line)\n\nif __name__ == ""__main__"":\n    flags.mark_flag_as_required(""data_dir"")\n    flags.mark_flag_as_required(""task_name"")\n    flags.mark_flag_as_required(""vocab_file"")\n    flags.mark_flag_as_required(""bert_config_file"")\n    flags.mark_flag_as_required(""output_dir"")\n    tf.app.run()\n\n\n'"
old_version/tf_metrics.py,22,"b'""""""\nMulticlass\nfrom: \nhttps://github.com/guillaumegenthial/tf_metrics/blob/master/tf_metrics/__init__.py\n\n""""""\n\n__author__ = ""Guillaume Genthial""\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops.metrics_impl import _streaming_confusion_matrix\n\n\ndef precision(labels, predictions, num_classes, pos_indices=None,\n              weights=None, average=\'micro\'):\n    """"""Multi-class precision metric for Tensorflow\n    Parameters\n    ----------\n    labels : Tensor of tf.int32 or tf.int64\n        The true labels\n    predictions : Tensor of tf.int32 or tf.int64\n        The predictions, same shape as labels\n    num_classes : int\n        The number of classes\n    pos_indices : list of int, optional\n        The indices of the positive classes, default is all\n    weights : Tensor of tf.int32, optional\n        Mask, must be of compatible shape with labels\n    average : str, optional\n        \'micro\': counts the total number of true positives, false\n            positives, and false negatives for the classes in\n            `pos_indices` and infer the metric from it.\n        \'macro\': will compute the metric separately for each class in\n            `pos_indices` and average. Will not account for class\n            imbalance.\n        \'weighted\': will compute the metric separately for each class in\n            `pos_indices` and perform a weighted average by the total\n            number of true labels for each class.\n    Returns\n    -------\n    tuple of (scalar float Tensor, update_op)\n    """"""\n    cm, op = _streaming_confusion_matrix(\n        labels, predictions, num_classes, weights)\n    pr, _, _ = metrics_from_confusion_matrix(\n        cm, pos_indices, average=average)\n    op, _, _ = metrics_from_confusion_matrix(\n        op, pos_indices, average=average)\n    return (pr, op)\n\n\ndef recall(labels, predictions, num_classes, pos_indices=None, weights=None,\n           average=\'micro\'):\n    """"""Multi-class recall metric for Tensorflow\n    Parameters\n    ----------\n    labels : Tensor of tf.int32 or tf.int64\n        The true labels\n    predictions : Tensor of tf.int32 or tf.int64\n        The predictions, same shape as labels\n    num_classes : int\n        The number of classes\n    pos_indices : list of int, optional\n        The indices of the positive classes, default is all\n    weights : Tensor of tf.int32, optional\n        Mask, must be of compatible shape with labels\n    average : str, optional\n        \'micro\': counts the total number of true positives, false\n            positives, and false negatives for the classes in\n            `pos_indices` and infer the metric from it.\n        \'macro\': will compute the metric separately for each class in\n            `pos_indices` and average. Will not account for class\n            imbalance.\n        \'weighted\': will compute the metric separately for each class in\n            `pos_indices` and perform a weighted average by the total\n            number of true labels for each class.\n    Returns\n    -------\n    tuple of (scalar float Tensor, update_op)\n    """"""\n    cm, op = _streaming_confusion_matrix(\n        labels, predictions, num_classes, weights)\n    _, re, _ = metrics_from_confusion_matrix(\n        cm, pos_indices, average=average)\n    _, op, _ = metrics_from_confusion_matrix(\n        op, pos_indices, average=average)\n    return (re, op)\n\n\ndef f1(labels, predictions, num_classes, pos_indices=None, weights=None,\n       average=\'micro\'):\n    return fbeta(labels, predictions, num_classes, pos_indices, weights,\n                 average)\n\n\ndef fbeta(labels, predictions, num_classes, pos_indices=None, weights=None,\n          average=\'micro\', beta=1):\n    """"""Multi-class fbeta metric for Tensorflow\n    Parameters\n    ----------\n    labels : Tensor of tf.int32 or tf.int64\n        The true labels\n    predictions : Tensor of tf.int32 or tf.int64\n        The predictions, same shape as labels\n    num_classes : int\n        The number of classes\n    pos_indices : list of int, optional\n        The indices of the positive classes, default is all\n    weights : Tensor of tf.int32, optional\n        Mask, must be of compatible shape with labels\n    average : str, optional\n        \'micro\': counts the total number of true positives, false\n            positives, and false negatives for the classes in\n            `pos_indices` and infer the metric from it.\n        \'macro\': will compute the metric separately for each class in\n            `pos_indices` and average. Will not account for class\n            imbalance.\n        \'weighted\': will compute the metric separately for each class in\n            `pos_indices` and perform a weighted average by the total\n            number of true labels for each class.\n    beta : int, optional\n        Weight of precision in harmonic mean\n    Returns\n    -------\n    tuple of (scalar float Tensor, update_op)\n    """"""\n    cm, op = _streaming_confusion_matrix(\n        labels, predictions, num_classes, weights)\n    _, _, fbeta = metrics_from_confusion_matrix(\n        cm, pos_indices, average=average, beta=beta)\n    _, _, op = metrics_from_confusion_matrix(\n        op, pos_indices, average=average, beta=beta)\n    return (fbeta, op)\n\n\ndef safe_div(numerator, denominator):\n    """"""Safe division, return 0 if denominator is 0""""""\n    numerator, denominator = tf.to_float(numerator), tf.to_float(denominator)\n    zeros = tf.zeros_like(numerator, dtype=numerator.dtype)\n    denominator_is_zero = tf.equal(denominator, zeros)\n    return tf.where(denominator_is_zero, zeros, numerator / denominator)\n\n\ndef pr_re_fbeta(cm, pos_indices, beta=1):\n    """"""Uses a confusion matrix to compute precision, recall and fbeta""""""\n    num_classes = cm.shape[0]\n    neg_indices = [i for i in range(num_classes) if i not in pos_indices]\n    cm_mask = np.ones([num_classes, num_classes])\n    cm_mask[neg_indices, neg_indices] = 0\n    diag_sum = tf.reduce_sum(tf.diag_part(cm * cm_mask))\n\n    cm_mask = np.ones([num_classes, num_classes])\n    cm_mask[:, neg_indices] = 0\n    tot_pred = tf.reduce_sum(cm * cm_mask)\n\n    cm_mask = np.ones([num_classes, num_classes])\n    cm_mask[neg_indices, :] = 0\n    tot_gold = tf.reduce_sum(cm * cm_mask)\n\n    pr = safe_div(diag_sum, tot_pred)\n    re = safe_div(diag_sum, tot_gold)\n    fbeta = safe_div((1. + beta**2) * pr * re, beta**2 * pr + re)\n\n    return pr, re, fbeta\n\n\ndef metrics_from_confusion_matrix(cm, pos_indices=None, average=\'micro\',\n                                  beta=1):\n    """"""Precision, Recall and F1 from the confusion matrix\n    Parameters\n    ----------\n    cm : tf.Tensor of type tf.int32, of shape (num_classes, num_classes)\n        The streaming confusion matrix.\n    pos_indices : list of int, optional\n        The indices of the positive classes\n    beta : int, optional\n        Weight of precision in harmonic mean\n    average : str, optional\n        \'micro\', \'macro\' or \'weighted\'\n    """"""\n    num_classes = cm.shape[0]\n    if pos_indices is None:\n        pos_indices = [i for i in range(num_classes)]\n\n    if average == \'micro\':\n        return pr_re_fbeta(cm, pos_indices, beta)\n    elif average in {\'macro\', \'weighted\'}:\n        precisions, recalls, fbetas, n_golds = [], [], [], []\n        for idx in pos_indices:\n            pr, re, fbeta = pr_re_fbeta(cm, [idx], beta)\n            precisions.append(pr)\n            recalls.append(re)\n            fbetas.append(fbeta)\n            cm_mask = np.zeros([num_classes, num_classes])\n            cm_mask[idx, :] = 1\n            n_golds.append(tf.to_float(tf.reduce_sum(cm * cm_mask)))\n\n        if average == \'macro\':\n            pr = tf.reduce_mean(precisions)\n            re = tf.reduce_mean(recalls)\n            fbeta = tf.reduce_mean(fbetas)\n            return pr, re, fbeta\n        if average == \'weighted\':\n            n_gold = tf.reduce_sum(n_golds)\n            pr_sum = sum(p * n for p, n in zip(precisions, n_golds))\n            pr = safe_div(pr_sum, n_gold)\n            re_sum = sum(r * n for r, n in zip(recalls, n_golds))\n            re = safe_div(re_sum, n_gold)\n            fbeta_sum = sum(f * n for f, n in zip(fbetas, n_golds))\n            fbeta = safe_div(fbeta_sum, n_gold)\n            return pr, re, fbeta\n\n    else:\n        raise NotImplementedError()'"
