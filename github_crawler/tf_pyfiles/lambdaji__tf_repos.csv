file_path,api_count,code
DeepMTL/Feature_pipeline/get_ai_fmap.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n""""""\nget feature map\n""""""\nimport sys\nreload(sys)\nsys.setdefaultencoding(""utf-8"")\nfrom multiprocessing import Pool as ThreadPool\nimport argparse\nimport glob\nfrom collections import defaultdict\n\ndef get_feat_stat(if_str):\n    output = open(if_str +\'.stat\',\'w\')\n    feat_dict = defaultdict(lambda: 0)\n    with open(if_str,""rt"") as f:\n        for line in f:\n            try:\n                splits = line.strip().split(\'\\t\')\n                filed, feat = ff.split(\':\')\n                field_dict[filed] += int(cnts)\n                feat_dict[feat] += int(cnts)\n                if int(feat) > max_feat:\n                    max_feat = int(feat)\n            except:\n                #output.write(line)\n                continue\n\n    output.write(""lines\\t{0}\\n"".format(num))\n\n    output.write(""--------------\\n"")\n    output.write(""max_feat\\t{0}\\t{1}\\n"".format(max_feat,len(feat_dict)))\n\n    output.write(""--------------\\n"")\n    for key,val in field_dict.iteritems():\n        output.write(""{0}\\t{1}\\n"".format(key,val))\n\n    output.write(""--------------\\n"")\n    feat_cnts = defaultdict(lambda: 0)\n    for key,val in feat_dict.iteritems():\n        feat_cnts[val] += 1\n        output.write(""{0}\\t{1}\\n"".format(key,val))\n\n    out = open(if_str +\'.cnts\',\'w\')\n    for key,val in feat_cnts.iteritems():\n        out.write(""{0}\\t{1}\\n"".format(key,val))\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--threads"",\n        type=int,\n        default=10,\n        help=""threads num""\n        )\n    parser.add_argument(\n        ""--input_dir"",\n        type=str,\n        default=""./"",\n        help=""input data dir""\n        )\n    parser.add_argument(\n        ""--output_dir"",\n        type=str,\n        default=""./"",\n        help=""feature map output dir""\n        )\n\n\n    FLAGS, unparsed = parser.parse_known_args()\n    print(\'threads \', FLAGS.threads)\n    print(\'input_dir \', FLAGS.input_dir)\n    print(\'output_dir \', FLAGS.output_dir)\n\n    file_list = glob.glob(FLAGS.input_dir+\'/feat_map*\')\n    print(\'file_list size \', len(file_list))\n    pool = ThreadPool(FLAGS.threads) # Sets the pool size\n    pool.map(get_feat_stat, file_list)\n    pool.close()\n    pool.join()\n'"
DeepMTL/Feature_pipeline/get_ai_tfrecord.py,16,"b'#!/usr/bin/env python\n#coding=utf-8\n""""""\ntfrecord for <<Deep Interest Network for Click-Through Rate Prediction>> and <<Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate>>\n\nby lambdaji\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport glob\n\nimport tensorflow as tf\nimport numpy as np\nimport re\nfrom multiprocessing import Pool as ThreadPool\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nLOG = tf.logging\n\ntf.app.flags.DEFINE_string(""input_dir"", ""./"", ""input dir"")\ntf.app.flags.DEFINE_string(""output_dir"", ""./"", ""output dir"")\ntf.app.flags.DEFINE_integer(""threads"", 16, ""threads num"")\n\n#\xe4\xbf\x9d\xe8\xaf\x81\xe9\xa1\xba\xe5\xba\x8f\xe4\xbb\xa5\xe5\x8f\x8a\xe5\xad\x97\xe6\xae\xb5\xe6\x95\xb0\xe9\x87\x8f\n#User_Fileds = set([\'101\',\'109_14\',\'110_14\',\'127_14\',\'150_14\',\'121\',\'122\',\'124\',\'125\',\'126\',\'127\',\'128\',\'129\'])\n#Ad_Fileds = set([\'205\',\'206\',\'207\',\'210\',\'216\'])\n#Context_Fileds = set([\'508\',\'509\',\'702\',\'853\',\'301\'])\nCommon_Fileds   = {\'101\':1,\'121\':2,\'122\':3,\'124\':4,\'125\':5,\'126\':6,\'127\':7,\'128\':8,\'129\':9,\'205\':10,\'301\':11}\nUMH_Fileds      = {\'109_14\':(\'u_cat\',12),\'110_14\':(\'u_shop\',13),\'127_14\':(\'u_brand\',14),\'150_14\':(\'u_int\',15)}      #user multi-hot feature\nAd_Fileds       = {\'206\':(\'a_cat\',16),\'207\':(\'a_shop\',17),\'210\':(\'a_int\',18),\'216\':(\'a_brand\',19)}                  #ad feature for DIN\n\n#40362692,0,0,216:9342395:1.0 301:9351665:1.0 205:7702673:1.0 206:8317829:1.0 207:8967741:1.0 508:9356012:2.30259 210:9059239:1.0 210:9042796:1.0 210:9076972:1.0 210:9103884:1.0 210:9063064:1.0 127_14:3529789:2.3979 127_14:3806412:2.70805\ndef gen_tfrecords(in_file):\n    basename = os.path.basename(in_file) + "".tfrecord""\n    out_file = os.path.join(FLAGS.output_dir, basename)\n    tfrecord_out = tf.python_io.TFRecordWriter(out_file)\n    with open(in_file) as fi:\n        for line in fi:\n            fields = line.strip().split(\',\')\n            if len(fields) != 4:\n                continue\n            #1 label\n            y = [float(fields[1])]\n            z = [float(fields[2])]\n            feature = {\n                ""y"": tf.train.Feature(float_list = tf.train.FloatList(value=y)),\n                ""z"": tf.train.Feature(float_list = tf.train.FloatList(value=z))\n             }\n\n            splits = re.split(\'[ :]\', fields[3])\n            ffv = np.reshape(splits,(-1,3))\n            #common_mask = np.array([v in Common_Fileds for v in ffv[:,0]])\n            #af_mask = np.array([v in Ad_Fileds for v in ffv[:,0]])\n            #cf_mask = np.array([v in Context_Fileds for v in ffv[:,0]])\n\n            #2 \xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe7\x89\xb9\xe6\xae\x8a\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\n            feat_ids = np.array([])\n            #feat_vals = np.array([])\n            for f, def_id in Common_Fileds.iteritems():\n                if f in ffv[:,0]:\n                    mask = np.array(f == ffv[:,0])\n                    feat_ids = np.append(feat_ids, ffv[mask,1])\n                    #np.append(feat_vals,ffv[mask,2].astype(np.float))\n                else:\n                    feat_ids = np.append(feat_ids, def_id)\n                    #np.append(feat_vals,1.0)\n            feature.update({""feat_ids"": tf.train.Feature(int64_list=tf.train.Int64List(value=feat_ids.astype(np.int)))})\n                            #""feat_vals"": tf.train.Feature(float_list=tf.train.FloatList(value=feat_vals))})\n\n            #3 \xe7\x89\xb9\xe6\xae\x8a\xe5\xad\x97\xe6\xae\xb5\xe5\x8d\x95\xe7\x8b\xac\xe5\xa4\x84\xe7\x90\x86\n            for f, (fname, def_id) in UMH_Fileds.iteritems():\n                if f in ffv[:,0]:\n                    mask = np.array(f == ffv[:,0])\n                    feat_ids = ffv[mask,1]\n                    feat_vals= ffv[mask,2]\n                else:\n                    feat_ids = np.array([def_id])\n                    feat_vals = np.array([1.0])\n                feature.update({fname+""ids"": tf.train.Feature(int64_list=tf.train.Int64List(value=feat_ids.astype(np.int))),\n                                fname+""vals"": tf.train.Feature(float_list=tf.train.FloatList(value=feat_vals.astype(np.float)))})\n\n            for f, (fname, def_id) in Ad_Fileds.iteritems():\n                if f in ffv[:,0]:\n                    mask = np.array(f == ffv[:,0])\n                    feat_ids = ffv[mask,1]\n                else:\n                    feat_ids = np.array([def_id])\n                feature.update({fname+""ids"": tf.train.Feature(int64_list=tf.train.Int64List(value=feat_ids.astype(np.int)))})\n\n            # serialized to Example\n            example = tf.train.Example(features = tf.train.Features(feature = feature))\n            serialized = example.SerializeToString()\n            tfrecord_out.write(serialized)\n            #num_lines += 1\n            #if num_lines % 10000 == 0:\n            #    print(""Process %d"" % num_lines)\n    tfrecord_out.close()\n\ndef main(_):\n    if not os.path.exists(FLAGS.output_dir):\n        os.mkdir(FLAGS.output_dir)\n    file_list = glob.glob(os.path.join(FLAGS.input_dir, ""*-*""))\n    print(""total files: %d"" % len(file_list))\n\n    pool = ThreadPool(FLAGS.threads) # Sets the pool size\n    pool.map(gen_tfrecords, file_list)\n    pool.close()\n    pool.join()\n\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
DeepMTL/Feature_pipeline/get_feat_cnts.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n"""""" stat fields/feature cnts """"""\nimport sys\nreload(sys)\nsys.setdefaultencoding(""utf-8"")\nfrom multiprocessing import Pool as ThreadPool\nimport argparse\nimport glob\nfrom collections import defaultdict\n\ndef get_feat_stat(if_str):\n    output = open(if_str +\'.stat\',\'w\')\n    field_dict = defaultdict(lambda: 0)\n    feat_dict = defaultdict(lambda: 0)\n    num = 0\n    max_feat = 0\n    with open(if_str,""rt"") as f:\n        for line in f:\n            try:\n                num += 1\n                ff, cnts = line.strip().split(\'\\t\')\n                filed, feat = ff.split(\':\')\n                field_dict[filed] += int(cnts)\n                feat_dict[feat] += int(cnts)\n                if int(feat) > max_feat:\n                    max_feat = int(feat)\n            except:\n                #output.write(line)\n                continue\n\n    output.write(""lines\\t{0}\\n"".format(num))\n\n    output.write(""--------------\\n"")\n    output.write(""max_feat\\t{0}\\t{1}\\n"".format(max_feat,len(feat_dict)))\n\n    output.write(""--------------\\n"")\n    for key,val in field_dict.iteritems():\n        output.write(""{0}\\t{1}\\n"".format(key,val))\n\n    output.write(""--------------\\n"")\n    feat_cnts = defaultdict(lambda: 0)\n    for key,val in feat_dict.iteritems():\n        feat_cnts[val] += 1\n        output.write(""{0}\\t{1}\\n"".format(key,val))\n\n    out = open(if_str +\'.cnts\',\'w\')\n    for key,val in feat_cnts.iteritems():\n        out.write(""{0}\\t{1}\\n"".format(key,val))\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--threads"",\n        type=int,\n        default=10,\n        help=""threads num""\n        )\n    parser.add_argument(\n        ""--input_dir"",\n        type=str,\n        default=""./"",\n        help=""input data dir""\n        )\n    parser.add_argument(\n        ""--output_dir"",\n        type=str,\n        default=""./"",\n        help=""feature map output dir""\n        )\n\n\n    FLAGS, unparsed = parser.parse_known_args()\n    print(\'threads \', FLAGS.threads)\n    print(\'input_dir \', FLAGS.input_dir)\n    print(\'output_dir \', FLAGS.output_dir)\n\n    file_list = glob.glob(FLAGS.input_dir+\'/feat_map*\')\n    print(\'file_list size \', len(file_list))\n    pool = ThreadPool(FLAGS.threads) # Sets the pool size\n    pool.map(get_feat_stat, file_list)\n    pool.close()\n    pool.join()\n'"
DeepMTL/Feature_pipeline/get_join_mapper.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n"""""" make train dateset\n""""""\nimport sys\nreload(sys)\nsys.setdefaultencoding(""utf-8"")\n#from collections import defaultdict\n\ndef doParseLog():\n    """"""parse log on hdfs""""""\n    for line in sys.stdin:\n        #recv = recv + 1\n        try:\n            splits = line.strip().split(\',\')\n            split_len = len(splits)\n            feat_lists = []\n            #common_feature_index|feat_num|feat_list\n            if( split_len == 3):\n                feat_strs = splits[2]\n                for fstr in feat_strs.split(\'\\x01\'):\n                    filed, feat_val = fstr.split(\'\\x02\')\n                    feat, val = feat_val.split(\'\\x03\')\n                    feat_lists.append(\'%s:%s:%s\' % (filed,feat,val))\n\n                # mapper\xe6\x8a\x8acommon_feature_index\xe4\xbd\x9c\xe4\xb8\xbakey\xef\xbc\x8c\xe4\xbf\x9d\xe8\xaf\x81Skeleton \xe5\x92\x8c Common Features\xe4\xb8\xa4\xe4\xbb\xbd\xe6\x95\xb0\xe6\x8d\xae\xe8\x90\xbd\xe5\x88\xb0\xe5\x90\x8c\xe4\xb8\x80\xe4\xb8\xaareduce\xe4\xb8\x8a\n                print ""{0}\\t{1}\\t{2}"".format(splits[0], \'common\', \' \'.join(feat_lists))\n            #sample_id|y|z|common_feature_index|feat_num|feat_list\n            elif(split_len == 6):\n                # y=0 & z=1\xe8\xbf\x87\xe6\xbb\xa4\n                if(splits[1] == \'0\' and splits[2] == \'1\'):\n                    continue\n                feat_strs = splits[5]\n                for fstr in feat_strs.split(\'\\x01\'):\n                    filed, feat_val = fstr.split(\'\\x02\')\n                    feat, val = feat_val.split(\'\\x03\')\n                    feat_lists.append(\'%s:%s:%s\' % (filed,feat,val))\n\n                # mapper\xe6\x8a\x8acommon_feature_index\xe4\xbd\x9c\xe4\xb8\xbakey\xef\xbc\x8c\xe4\xbf\x9d\xe8\xaf\x81Skeleton \xe5\x92\x8c Common Features\xe4\xb8\xa4\xe4\xbb\xbd\xe6\x95\xb0\xe6\x8d\xae\xe8\x90\xbd\xe5\x88\xb0\xe5\x90\x8c\xe4\xb8\x80\xe4\xb8\xaareduce\xe4\xb8\x8a\n                print ""{0}\\t{1}\\t{2},{3},{4},{5}"".format(splits[3], \'sample\', splits[0], splits[1], splits[2], \' \'.join(feat_lists))\n        except:\n            continue\n        #print recv\n\nif __name__ == \'__main__\':\n    doParseLog()\n'"
DeepMTL/Feature_pipeline/get_join_reducer.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n"""""" make train date set\n""""""\nimport sys\nreload(sys)\nsys.setdefaultencoding(""utf-8"")\nfrom collections import defaultdict\n\ndef doParseLog():\n    """"""parse log on hdfs""""""\n    #recv = 0\n    common_logs = defaultdict(lambda: \'\')\n    sample_list = []\n    for line in sys.stdin:\n        #recv = recv + 1\n        try:\n            common_feature_index, log_type, fstrs = line.strip().split(\'\\t\')\n            if log_type == \'sample\':\n                sample_list.append(line)\n            elif log_type == \'common\':\n                common_logs[common_feature_index] = fstrs\n        except:\n            continue\n\n    for sample in sample_list:\n        try:\n            common_feature_index, _, sample_str = sample.strip().split(\'\\t\')\n            common_str = common_logs.get(common_feature_index)\n            if common_str:\n                print ""{0} {1}"".format(sample_str, common_str)\n            else:\n                print ""{0}"".format(sample_str)\n        except:\n            continue\n    #print recv\n\nif __name__ == \'__main__\':\n    doParseLog()\n'"
DeepMTL/Feature_pipeline/get_remap_mapper.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n"""""" re map feat_id """"""\nimport sys\nreload(sys)\nsys.setdefaultencoding(""utf-8"")\nfrom collections import defaultdict\nimport random\n\ndef load_fcnts(if_str):\n    feat_cnts_dict = defaultdict(lambda: 0)\n    new_id = 20\n    with open(if_str) as f:\n        for line in f:\n            fid, cnts = line.strip().split(\'\\t\')\n            if feat_cnts_dict.get(fid):\n                continue\n            if int(cnts) >= 20:             #cutoff=20\n                feat_cnts_dict[fid] = new_id\n                new_id = new_id + 1\n    return feat_cnts_dict\n\ndef doParseLog(feat_cnts_dict):\n    """"""parse log on hdfs""""""\n    for line in sys.stdin:\n        #recv = recv + 1\n        try:\n            splits = line.strip().split(\',\')\n            # y=0 & z=1\xe8\xbf\x87\xe6\xbb\xa4\n            if(splits[1] == \'0\' and splits[2] == \'1\'):\n                continue\n            # remap feat_id\n            feat_lists = []\n            for fstr in splits[3].split(\' \'):\n                f,fid,val = fstr.split(\':\')\n                new_id = feat_cnts_dict.get(fid)\n                if new_id:\n                    feat_lists.append(\'%s:%d:%s\' % (f,new_id,val))\n            ri = random.randint(0, 2147483647)      #shuffle\n            print ""{0}\\t{1},{2},{3},{4}"".format(ri, splits[0], splits[1], splits[2], \' \'.join(feat_lists))\n        except:\n            continue\n\nif __name__ == \'__main__\':\n    feat_cnts_file, = sys.argv[1:2]\n    feat_cnts_dict = load_fcnts(feat_cnts_file)\n    doParseLog(feat_cnts_dict)\n'"
DeepMTL/Feature_pipeline/get_stat_mapper.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n"""""" make train dateset\n""""""\nimport sys\nreload(sys)\nsys.setdefaultencoding(""utf-8"")\nfrom collections import defaultdict\n\ndef doParseLog():\n    """"""parse log on hdfs""""""\n    cnts_dict = defaultdict(lambda: 0)\n    for line in sys.stdin:\n        #recv = recv + 1\n        try:\n            splits = line.strip().split(\',\')\n            for fstr in splits[3].split(\' \'):\n                feat,_ = fstr.rsplit(\':\',1)\n                cnts_dict[feat] += 1\n        except:\n            continue\n    for key,val in cnts_dict.iteritems():\n        print ""{0}\\t{1}"".format(key,val)\n\nif __name__ == \'__main__\':\n    doParseLog()\n'"
DeepMTL/Feature_pipeline/get_stat_reducer.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n"""""" make train date set\n""""""\nimport sys\nreload(sys)\nsys.setdefaultencoding(""utf-8"")\nfrom collections import defaultdict\n\ndef doParseLog():\n    """"""parse log on hdfs""""""\n    cnts_dict = defaultdict(lambda: 0)\n    for line in sys.stdin:\n        #recv = recv + 1\n        try:\n            key, val = line.strip().split(\'\\t\')\n            cnts_dict[key] += int(val)\n        except:\n            continue\n    for key,val in cnts_dict.iteritems():\n        print ""{0}\\t{1}"".format(key,val)\n\nif __name__ == \'__main__\':\n    doParseLog()\n'"
DeepMTL/Feature_pipeline/get_tfrecord.py,16,"b'#!/usr/bin/env python\n#coding=utf-8\n""""""\ntfrecord for <<Deep Interest Network for Click-Through Rate Prediction>> and <<Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate>>\n\nby lambdaji\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport glob\n\nimport tensorflow as tf\nimport numpy as np\nimport re\nfrom multiprocessing import Pool as ThreadPool\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nLOG = tf.logging\n\ntf.app.flags.DEFINE_string(""input_dir"", ""./"", ""input dir"")\ntf.app.flags.DEFINE_string(""output_dir"", ""./"", ""output dir"")\ntf.app.flags.DEFINE_integer(""threads"", 16, ""threads num"")\n\n#\xe4\xbf\x9d\xe8\xaf\x81\xe9\xa1\xba\xe5\xba\x8f\xe4\xbb\xa5\xe5\x8f\x8a\xe5\xad\x97\xe6\xae\xb5\xe6\x95\xb0\xe9\x87\x8f\n#User_Fileds = set([\'101\',\'109_14\',\'110_14\',\'127_14\',\'150_14\',\'121\',\'122\',\'124\',\'125\',\'126\',\'127\',\'128\',\'129\'])\n#Ad_Fileds = set([\'205\',\'206\',\'207\',\'210\',\'216\'])\n#Context_Fileds = set([\'508\',\'509\',\'702\',\'853\',\'301\'])\nCommon_Fileds   = {\'101\':\'1\',\'121\':\'2\',\'122\':\'3\',\'124\':\'4\',\'125\':\'5\',\'126\':\'6\',\'127\':\'7\',\'128\':\'8\',\'129\':\'9\',\'205\':\'10\',\'301\':\'11\'}\nUMH_Fileds      = {\'109_14\':(\'u_cat\',\'12\'),\'110_14\':(\'u_shop\',\'13\'),\'127_14\':(\'u_brand\',\'14\'),\'150_14\':(\'u_int\',\'15\')}      #user multi-hot feature\nAd_Fileds       = {\'206\':(\'a_cat\',\'16\'),\'207\':(\'a_shop\',\'17\'),\'210\':(\'a_int\',\'18\'),\'216\':(\'a_brand\',\'19\')}                  #ad feature for DIN\n\n#40362692,0,0,216:9342395:1.0 301:9351665:1.0 205:7702673:1.0 206:8317829:1.0 207:8967741:1.0 508:9356012:2.30259 210:9059239:1.0 210:9042796:1.0 210:9076972:1.0 210:9103884:1.0 210:9063064:1.0 127_14:3529789:2.3979 127_14:3806412:2.70805\ndef gen_tfrecords(in_file):\n    basename = os.path.basename(in_file) + "".tfrecord""\n    out_file = os.path.join(FLAGS.output_dir, basename)\n    tfrecord_out = tf.python_io.TFRecordWriter(out_file)\n    with open(in_file) as fi:\n        for line in fi:\n            fields = line.strip().split(\',\')\n            if len(fields) != 4:\n                continue\n            #1 label\n            y = [float(fields[1])]\n            z = [float(fields[2])]\n            feature = {\n                ""y"": tf.train.Feature(float_list = tf.train.FloatList(value=y)),\n                ""z"": tf.train.Feature(float_list = tf.train.FloatList(value=z))\n             }\n\n            splits = re.split(\'[ :]\', fields[3])\n            ffv = np.reshape(splits,(-1,3))\n            #common_mask = np.array([v in Common_Fileds for v in ffv[:,0]])\n            #af_mask = np.array([v in Ad_Fileds for v in ffv[:,0]])\n            #cf_mask = np.array([v in Context_Fileds for v in ffv[:,0]])\n\n            #2 \xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe7\x89\xb9\xe6\xae\x8a\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\n            feat_ids = np.array([])\n            #feat_vals = np.array([])\n            for f, def_id in Common_Fileds.iteritems():\n                if f in ffv[:,0]:\n                    mask = np.array(f == ffv[:,0])\n                    feat_ids = np.append(feat_ids, ffv[mask,1])\n                    #np.append(feat_vals,ffv[mask,2].astype(np.float))\n                else:\n                    feat_ids = np.append(feat_ids, def_id)\n                    #np.append(feat_vals,1.0)\n            feature.update({""feat_ids"": tf.train.Feature(int64_list=tf.train.Int64List(value=feat_ids.astype(np.int)))})\n                            #""feat_vals"": tf.train.Feature(float_list=tf.train.FloatList(value=feat_vals))})\n\n            #3 \xe7\x89\xb9\xe6\xae\x8a\xe5\xad\x97\xe6\xae\xb5\xe5\x8d\x95\xe7\x8b\xac\xe5\xa4\x84\xe7\x90\x86\n            for f, (fname, def_id) in UMH_Fileds.iteritems():\n                if f in ffv[:,0]:\n                    mask = np.array(f == ffv[:,0])\n                    feat_ids = ffv[mask,1]\n                    feat_vals= ffv[mask,2]\n                else:\n                    feat_ids = np.array([def_id])\n                    feat_vals = np.array([1.0])\n                feature.update({fname+""ids"": tf.train.Feature(int64_list=tf.train.Int64List(value=feat_ids.astype(np.int))),\n                                fname+""vals"": tf.train.Feature(float_list=tf.train.FloatList(value=feat_vals.astype(np.float)))})\n\n            for f, (fname, def_id) in Ad_Fileds.iteritems():\n                if f in ffv[:,0]:\n                    mask = np.array(f == ffv[:,0])\n                    feat_ids = ffv[mask,1]\n                else:\n                    feat_ids = np.array([def_id])\n                feature.update({fname+""ids"": tf.train.Feature(int64_list=tf.train.Int64List(value=feat_ids.astype(np.int)))})\n\n            # serialized to Example\n            example = tf.train.Example(features = tf.train.Features(feature = feature))\n            serialized = example.SerializeToString()\n            tfrecord_out.write(serialized)\n            #num_lines += 1\n            #if num_lines % 10000 == 0:\n            #    print(""Process %d"" % num_lines)\n    tfrecord_out.close()\n\ndef main(_):\n    if not os.path.exists(FLAGS.output_dir):\n        os.mkdir(FLAGS.output_dir)\n    file_list = glob.glob(os.path.join(FLAGS.input_dir, ""*-*""))\n    print(""total files: %d"" % len(file_list))\n\n    pool = ThreadPool(FLAGS.threads) # Sets the pool size\n    pool.map(gen_tfrecords, file_list)\n    pool.close()\n    pool.join()\n\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
DeepMTL/Model_pipeline/DeepCvrMTL.py,121,"b'#!/usr/bin/env python\n#coding=utf-8\n""""""\nTensorFlow Implementation of <<Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate>>\nand <<Product-based Neural Networks for User Response Prediction>> with the fellowing features\xef\xbc\x9a\n#1 Input pipline using Dataset high level API, Support parallel and prefetch reading\n#2 Train pipline using Coustom Estimator by rewriting model_fn\n#3 Support distincted training by TF_CONFIG\n#4 Support export servable model for TensorFlow Serving\n\nhttps://zhuanlan.zhihu.com/p/37562283\nhttps://tianchi.aliyun.com/datalab/dataSet.html?dataId=408\n\nby lambdaji\n""""""\n#from __future__ import absolute_import\n#from __future__ import division\n#from __future__ import print_function\n\n#import argparse\nimport shutil\n#import sys\nimport os\nimport json\nimport glob\nfrom datetime import date, timedelta\nfrom time import time\n\nimport random\nimport tensorflow as tf\n\n#################### CMD Arguments ####################\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""dist_mode"", 0, ""distribuion mode {0-loacal, 1-single_dist, 2-multi_dist}"")\ntf.app.flags.DEFINE_string(""ps_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""worker_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""job_name"", \'\', ""One of \'ps\', \'worker\'"")\ntf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")\ntf.app.flags.DEFINE_integer(""num_threads"", 16, ""Number of threads"")\ntf.app.flags.DEFINE_integer(""feature_size"", 0, ""Number of features"")\ntf.app.flags.DEFINE_integer(""field_size"", 0, ""Number of common fields"")\ntf.app.flags.DEFINE_integer(""embedding_size"", 32, ""Embedding size"")\ntf.app.flags.DEFINE_integer(""num_epochs"", 10, ""Number of epochs"")\ntf.app.flags.DEFINE_integer(""batch_size"", 64, ""Number of batch size"")\ntf.app.flags.DEFINE_integer(""log_steps"", 1000, ""save summary every steps"")\ntf.app.flags.DEFINE_float(""learning_rate"", 0.0005, ""learning rate"")\ntf.app.flags.DEFINE_float(""l2_reg"", 0.0001, ""L2 regularization"")\ntf.app.flags.DEFINE_string(""loss_type"", \'log_loss\', ""loss type {square_loss, log_loss}"")\ntf.app.flags.DEFINE_float(""ctr_task_wgt"", 0.5, ""loss weight of ctr task"")\ntf.app.flags.DEFINE_string(""optimizer"", \'Adam\', ""optimizer type {Adam, Adagrad, GD, Momentum}"")\ntf.app.flags.DEFINE_string(""deep_layers"", \'256,128,64\', ""deep layers"")\ntf.app.flags.DEFINE_string(""dropout"", \'0.5,0.5,0.5\', ""dropout rate"")\ntf.app.flags.DEFINE_boolean(""batch_norm"", False, ""perform batch normaization (True or False)"")\ntf.app.flags.DEFINE_float(""batch_norm_decay"", 0.9, ""decay for the moving average(recommend trying decay=0.9)"")\ntf.app.flags.DEFINE_string(""data_dir"", \'\', ""data dir"")\ntf.app.flags.DEFINE_string(""dt_dir"", \'\', ""data dt partition"")\ntf.app.flags.DEFINE_string(""model_dir"", \'\', ""model check point dir"")\ntf.app.flags.DEFINE_string(""servable_model_dir"", \'\', ""export servable model for TensorFlow Serving"")\ntf.app.flags.DEFINE_string(""task_type"", \'train\', ""task type {train, infer, eval, export}"")\ntf.app.flags.DEFINE_boolean(""clear_existing_model"", False, ""clear existing model or not"")\n\n#40362692,0,0,216:9342395:1.0 301:9351665:1.0 205:7702673:1.0 206:8317829:1.0 207:8967741:1.0 508:9356012:2.30259 210:9059239:1.0 210:9042796:1.0 210:9076972:1.0 210:9103884:1.0 210:9063064:1.0 127_14:3529789:2.3979 127_14:3806412:2.70805\ndef input_fn(filenames, batch_size=32, num_epochs=1, perform_shuffle=False):\n    print(\'Parsing\', filenames)\n    def _parse_fn(record):\n        features = {\n            ""y"": tf.FixedLenFeature([], tf.float32),\n            ""z"": tf.FixedLenFeature([], tf.float32),\n            ""feat_ids"": tf.FixedLenFeature([FLAGS.field_size], tf.int64),\n            #""feat_vals"": tf.FixedLenFeature([None], tf.float32),\n            ""u_catids"": tf.VarLenFeature(tf.int64),\n            ""u_catvals"": tf.VarLenFeature(tf.float32),\n            ""u_shopids"": tf.VarLenFeature(tf.int64),\n            ""u_shopvals"": tf.VarLenFeature(tf.float32),\n            ""u_intids"": tf.VarLenFeature(tf.int64),\n            ""u_intvals"": tf.VarLenFeature(tf.float32),\n            ""u_brandids"": tf.VarLenFeature(tf.int64),\n            ""u_brandvals"": tf.VarLenFeature(tf.float32),\n            ""a_catids"": tf.FixedLenFeature([], tf.int64),\n            ""a_shopids"": tf.FixedLenFeature([], tf.int64),\n            ""a_brandids"": tf.FixedLenFeature([], tf.int64),\n            ""a_intids"": tf.VarLenFeature(tf.int64)\n        }\n        parsed = tf.parse_single_example(record, features)\n        y = parsed.pop(\'y\')\n        z = parsed.pop(\'z\')\n        return parsed, {""y"": y, ""z"": z}\n\n    # Extract lines from input files using the Dataset API, can pass one filename or filename list\n    dataset = tf.data.TFRecordDataset(filenames).map(_parse_fn, num_parallel_calls=10).prefetch(500000)    # multi-thread pre-process then prefetch\n\n    # Randomizes input using a window of 256 elements (read into memory)\n    if perform_shuffle:\n        dataset = dataset.shuffle(buffer_size=256)\n\n    # epochs from blending together.\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.batch(batch_size) # Batch size to use\n    #dataset = dataset.padded_batch(batch_size, padded_shapes=({""feeds_ids"": [None], ""feeds_vals"": [None], ""title_ids"": [None]}, [None]))   #\xe4\xb8\x8d\xe5\xae\x9a\xe9\x95\xbf\xe8\xa1\xa5\xe9\xbd\x90\n\n    #return dataset.make_one_shot_iterator()\n    iterator = dataset.make_one_shot_iterator()\n    batch_features, batch_labels = iterator.get_next()\n    #return tf.reshape(batch_ids,shape=[-1,field_size]), tf.reshape(batch_vals,shape=[-1,field_size]), batch_labels\n    return batch_features, batch_labels\n\ndef model_fn(features, labels, mode, params):\n    """"""Bulid Model function f(x) for Estimator.""""""\n    #------hyperparameters----\n    field_size = params[""field_size""]\n    feature_size = params[""feature_size""]\n    embedding_size = params[""embedding_size""]\n    l2_reg = params[""l2_reg""]\n    learning_rate = params[""learning_rate""]\n    #optimizer = params[""optimizer""]\n    layers = map(int, params[""deep_layers""].split(\',\'))\n    dropout = map(float, params[""dropout""].split(\',\'))\n    ctr_task_wgt = params[""ctr_task_wgt""]\n    common_dims = field_size*embedding_size\n\n    #------bulid weights------\n    Feat_Emb = tf.get_variable(name=\'embeddings\', shape=[feature_size, embedding_size], initializer=tf.glorot_normal_initializer())\n\n    #------build feaure-------\n    #{U-A-X-C\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe7\x89\xb9\xe6\xae\x8a\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81}\n    feat_ids    = features[\'feat_ids\']\n    #feat_vals   = features[\'feat_vals\']\n    #{User multi-hot}\n    u_catids    = features[\'u_catids\']\n    u_catvals   = features[\'u_catvals\']\n    u_shopids   = features[\'u_shopids\']\n    u_shopvals  = features[\'u_shopvals\']\n    u_intids    = features[\'u_intids\']\n    u_intvals   = features[\'u_intvals\']\n    u_brandids  = features[\'u_brandids\']\n    u_brandvals = features[\'u_brandvals\']\n    #{Ad}\n    a_catids    = features[\'a_catids\']\n    a_shopids   = features[\'a_shopids\']\n    a_brandids  = features[\'a_brandids\']\n    a_intids    = features[\'a_intids\']      #multi-hot\n    #{X multi-hot}\n    #x_intids    = features[\'x_intids\']\n    #x_intvals   = features[\'x_intvals\']\n\n    y = labels[\'y\']\n    z = labels[\'z\']\n    #y = tf.Print(y, [y], message=""This is y: "")\n    #z = tf.Print(z, [z], message=""This is z: "")\n\n    #------build f(x)------\n    with tf.variable_scope(""Shared-Embedding-layer""):\n        common_embs = tf.nn.embedding_lookup(Feat_Emb, feat_ids)     # None * F\' * K\n        #common_embs = tf.multiply(common_embs, feat_vals)\n        u_cat_emb   = tf.nn.embedding_lookup_sparse(Feat_Emb, sp_ids=u_catids,  sp_weights=u_catvals,   combiner=""sum"")               # None * K\n        u_shop_emb  = tf.nn.embedding_lookup_sparse(Feat_Emb, sp_ids=u_shopids, sp_weights=u_shopvals,  combiner=""sum"")\n        u_brand_emb = tf.nn.embedding_lookup_sparse(Feat_Emb, sp_ids=u_brandids,sp_weights=u_brandvals, combiner=""sum"")\n        u_int_emb   = tf.nn.embedding_lookup_sparse(Feat_Emb, sp_ids=u_intids,  sp_weights=u_intvals,   combiner=""sum"")\n        a_int_emb   = tf.nn.embedding_lookup_sparse(Feat_Emb, sp_ids=a_intids,  sp_weights=None,        combiner=""sum"")\n        a_cat_emb   = tf.nn.embedding_lookup(Feat_Emb, a_catids)\n        a_shop_emb  = tf.nn.embedding_lookup(Feat_Emb, a_shopids)\n        a_brand_emb = tf.nn.embedding_lookup(Feat_Emb, a_brandids)\n\n        x_concat = tf.concat([tf.reshape(common_embs,shape=[-1, common_dims]),u_cat_emb,u_shop_emb,u_brand_emb,u_int_emb,a_cat_emb,a_shop_emb,a_brand_emb,a_int_emb],axis=1)    # None * (F * K)\n\n    with tf.name_scope(""CVR_Task""):\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            train_phase = True\n        else:\n            train_phase = False\n        x_cvr = x_concat\n        for i in range(len(layers)):\n            x_cvr = tf.contrib.layers.fully_connected(inputs=x_cvr, num_outputs=layers[i], \\\n            \tweights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'cvr_mlp%d\' % i)\n\n            if FLAGS.batch_norm:\n\t\t\t\tx_cvr = batch_norm_layer(x_cvr, train_phase=train_phase, scope_bn=\'cvr_bn_%d\' %i)   \t#\xe6\x94\xbe\xe5\x9c\xa8RELU\xe4\xb9\x8b\xe5\x90\x8e https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu\n            if mode == tf.estimator.ModeKeys.TRAIN:\n\t\t\t\tx_cvr = tf.nn.dropout(x_cvr, keep_prob=dropout[i])                              \t#Apply Dropout after all BN layers and set dropout=0.8(drop_ratio=0.2)\n\n        y_cvr = tf.contrib.layers.fully_connected(inputs=x_cvr, num_outputs=1, activation_fn=tf.identity, \\\n            weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'cvr_out\')\n        y_cvr = tf.reshape(y_cvr,shape=[-1])\n\n    with tf.name_scope(""CTR_Task""):\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            train_phase = True\n        else:\n            train_phase = False\n\n        x_ctr = x_concat\n        for i in range(len(layers)):\n            x_ctr = tf.contrib.layers.fully_connected(inputs=x_ctr, num_outputs=layers[i], \\\n            \tweights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'ctr_mlp%d\' % i)\n\n            if FLAGS.batch_norm:\n\t\t\t\tx_ctr = batch_norm_layer(x_ctr, train_phase=train_phase, scope_bn=\'ctr_bn_%d\' %i)   \t#\xe6\x94\xbe\xe5\x9c\xa8RELU\xe4\xb9\x8b\xe5\x90\x8e https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu\n            if mode == tf.estimator.ModeKeys.TRAIN:\n\t\t\t\tx_ctr = tf.nn.dropout(x_ctr, keep_prob=dropout[i])                              \t#Apply Dropout after all BN layers and set dropout=0.8(drop_ratio=0.2)\n\n        y_ctr = tf.contrib.layers.fully_connected(inputs=x_ctr, num_outputs=1, activation_fn=tf.identity, \\\n            weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'ctr_out\')\n        y_ctr = tf.reshape(y_ctr,shape=[-1])\n\n    with tf.variable_scope(""MTL-Layer""):\n        pctr = tf.sigmoid(y_ctr)\n        pcvr = tf.sigmoid(y_cvr)\n        pctcvr = pctr*pcvr\n\n    predictions={""pcvr"": pcvr, ""pctr"": pctr, ""pctcvr"": pctcvr}\n    export_outputs = {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: tf.estimator.export.PredictOutput(predictions)}\n    # Provide an estimator spec for `ModeKeys.PREDICT`\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                export_outputs=export_outputs)\n\n    #------bulid loss------\n    ctr_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_ctr, labels=y))\n    #cvr_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_ctcvr, labels=z))\n    cvr_loss = tf.reduce_mean(tf.losses.log_loss(predictions=pctcvr, labels=z))\n    loss = ctr_task_wgt * ctr_loss + (1  -ctr_task_wgt) * cvr_loss + l2_reg * tf.nn.l2_loss(Feat_Emb)\n\n    tf.summary.scalar(\'ctr_loss\', ctr_loss)\n    tf.summary.scalar(\'cvr_loss\', cvr_loss)\n\n    # Provide an estimator spec for `ModeKeys.EVAL`\n    eval_metric_ops = {\n        ""CTR_AUC"": tf.metrics.auc(y, pctr),\n        ""CVR_AUC"": tf.metrics.auc(z, pcvr),\n        ""CTCVR_AUC"": tf.metrics.auc(z, pctcvr)\n    }\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                eval_metric_ops=eval_metric_ops)\n\n    #------bulid optimizer------\n    if FLAGS.optimizer == \'Adam\':\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8)\n    elif FLAGS.optimizer == \'Adagrad\':\n        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate, initial_accumulator_value=1e-8)\n    elif FLAGS.optimizer == \'Momentum\':\n        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95)\n    elif FLAGS.optimizer == \'ftrl\':\n        optimizer = tf.train.FtrlOptimizer(learning_rate)\n\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n\n    # Provide an estimator spec for `ModeKeys.TRAIN` modes\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                train_op=train_op)\n\ndef batch_norm_layer(x, train_phase, scope_bn):\n    bn_train = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True, updates_collections=None, is_training=True,  reuse=None, scope=scope_bn)\n    bn_infer = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True, updates_collections=None, is_training=False, reuse=True, scope=scope_bn)\n    z = tf.cond(tf.cast(train_phase, tf.bool), lambda: bn_train, lambda: bn_infer)\n    return z\n\ndef set_dist_env():\n    if FLAGS.dist_mode == 1:        # \xe6\x9c\xac\xe5\x9c\xb0\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xb5\x8b\xe8\xaf\x95\xe6\xa8\xa1\xe5\xbc\x8f1 chief, 1 ps, 1 evaluator\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        chief_hosts = FLAGS.chief_hosts.split(\',\')\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # \xe6\x97\xa0worker\xe5\x8f\x82\xe6\x95\xb0\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n    elif FLAGS.dist_mode == 2:      # \xe9\x9b\x86\xe7\xbe\xa4\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xa8\xa1\xe5\xbc\x8f\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        worker_hosts = FLAGS.worker_hosts.split(\',\')\n        chief_hosts = worker_hosts[0:1] # get first worker as chief\n        worker_hosts = worker_hosts[2:] # the rest as worker\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'worker_host\', worker_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # use #worker=0 as chief\n        if job_name == ""worker"" and task_index == 0:\n            job_name = ""chief""\n        # use #worker=1 as evaluator\n        if job_name == ""worker"" and task_index == 1:\n            job_name = \'evaluator\'\n            task_index = 0\n        # the others as worker\n        if job_name == ""worker"" and task_index > 1:\n            task_index -= 2\n\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'worker\': worker_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n\ndef main(_):\n    #------check Arguments------\n    if FLAGS.dt_dir == """":\n        FLAGS.dt_dir = (date.today() + timedelta(-1)).strftime(\'%Y%m%d\')\n    FLAGS.model_dir = FLAGS.model_dir + FLAGS.dt_dir\n    #FLAGS.data_dir  = FLAGS.data_dir + FLAGS.dt_dir\n\n    print(\'task_type \', FLAGS.task_type)\n    print(\'model_dir \', FLAGS.model_dir)\n    print(\'data_dir \', FLAGS.data_dir)\n    print(\'dt_dir \', FLAGS.dt_dir)\n    print(\'num_epochs \', FLAGS.num_epochs)\n    print(\'feature_size \', FLAGS.feature_size)\n    print(\'field_size \', FLAGS.field_size)\n    print(\'embedding_size \', FLAGS.embedding_size)\n    print(\'batch_size \', FLAGS.batch_size)\n    print(\'deep_layers \', FLAGS.deep_layers)\n    print(\'dropout \', FLAGS.dropout)\n    print(\'loss_type \', FLAGS.loss_type)\n    print(\'optimizer \', FLAGS.optimizer)\n    print(\'learning_rate \', FLAGS.learning_rate)\n    print(\'l2_reg \', FLAGS.l2_reg)\n    print(\'ctr_task_wgt \', FLAGS.ctr_task_wgt)\n\n    #------init Envs------\n    tr_files = glob.glob(""%s/tr/*tfrecord"" % FLAGS.data_dir)\n    random.shuffle(tr_files)\n    print(""tr_files:"", tr_files)\n    va_files = glob.glob(""%s/te/*tfrecord"" % FLAGS.data_dir)\n    print(""va_files:"", va_files)\n    te_files = glob.glob(""%s/te/*tfrecord"" % FLAGS.data_dir)\n    print(""te_files:"", te_files)\n\n    if FLAGS.clear_existing_model:\n        try:\n            shutil.rmtree(FLAGS.model_dir)\n        except Exception as e:\n            print(e, ""at clear_existing_model"")\n        else:\n            print(""existing model cleaned at %s"" % FLAGS.model_dir)\n\n    set_dist_env()\n\n    #------bulid Tasks------\n    model_params = {\n        ""field_size"": FLAGS.field_size,\n        ""feature_size"": FLAGS.feature_size,\n        ""embedding_size"": FLAGS.embedding_size,\n        ""learning_rate"": FLAGS.learning_rate,\n        ""l2_reg"": FLAGS.l2_reg,\n        ""deep_layers"": FLAGS.deep_layers,\n\t\t""dropout"": FLAGS.dropout,\n        ""ctr_task_wgt"":FLAGS.ctr_task_wgt\n    }\n    config = tf.estimator.RunConfig().replace(session_config = tf.ConfigProto(device_count={\'GPU\':0, \'CPU\':FLAGS.num_threads}),\n            log_step_count_steps=FLAGS.log_steps, save_summary_steps=FLAGS.log_steps)\n    Estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=FLAGS.model_dir, params=model_params, config=config)\n\n    if FLAGS.task_type == \'train\':\n        train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(tr_files, num_epochs=FLAGS.num_epochs, batch_size=FLAGS.batch_size))\n        eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size), steps=None, start_delay_secs=1000, throttle_secs=1200)\n        tf.estimator.train_and_evaluate(Estimator, train_spec, eval_spec)\n    elif FLAGS.task_type == \'eval\':\n        Estimator.evaluate(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size))\n    elif FLAGS.task_type == \'infer\':\n        preds = Estimator.predict(input_fn=lambda: input_fn(te_files, num_epochs=1, batch_size=FLAGS.batch_size), predict_keys=""prob"")\n        with open(FLAGS.data_dir+""/pred.txt"", ""w"") as fo:\n            for prob in preds:\n                fo.write(""%f\\t%f\\n"" % (prob[\'pctr\'], prob[\'pcvr\']))\n    elif FLAGS.task_type == \'export\':\n        print(""Not Implemented, Do It Yourself!"")\n        #feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n\n        #feature_spec = {\n        #    \'feat_ids\': tf.FixedLenFeature(dtype=tf.int64, shape=[None, FLAGS.field_size]),\n        #    \'feat_vals\': tf.FixedLenFeature(dtype=tf.float32, shape=[None, FLAGS.field_size])\n        #}\n        #serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n\n        #feature_spec = {\n        #    \'feat_ids\': tf.placeholder(dtype=tf.int64, shape=[None, FLAGS.field_size], name=\'feat_ids\'),\n        #    \'feat_vals\': tf.placeholder(dtype=tf.float32, shape=[None, FLAGS.field_size], name=\'feat_vals\')\n        #}\n        #serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_spec)\n        #Estimator.export_savedmodel(FLAGS.servable_model_dir, serving_input_receiver_fn)\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
deep_ctr/Feature_pipeline/get_aliccp_tfrecord.py,16,"b'#!/usr/bin/env python\n#coding=utf-8\n""""""\ntfrecord for <<Deep Interest Network for Click-Through Rate Prediction>> and <<Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate>>\n\nby lambdaji\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport glob\n\nimport tensorflow as tf\nimport numpy as np\nimport re\nfrom multiprocessing import Pool as ThreadPool\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nLOG = tf.logging\n\ntf.app.flags.DEFINE_string(""input_dir"", ""./"", ""input dir"")\ntf.app.flags.DEFINE_string(""output_dir"", ""./"", ""output dir"")\ntf.app.flags.DEFINE_integer(""threads"", 16, ""threads num"")\n\n#\xe4\xbf\x9d\xe8\xaf\x81\xe9\xa1\xba\xe5\xba\x8f\xe4\xbb\xa5\xe5\x8f\x8a\xe5\xad\x97\xe6\xae\xb5\xe6\x95\xb0\xe9\x87\x8f\n#User_Fileds = set([\'101\',\'109_14\',\'110_14\',\'127_14\',\'150_14\',\'121\',\'122\',\'124\',\'125\',\'126\',\'127\',\'128\',\'129\'])\n#Ad_Fileds = set([\'205\',\'206\',\'207\',\'210\',\'216\'])\n#Context_Fileds = set([\'508\',\'509\',\'702\',\'853\',\'301\'])\nCommon_Fileds   = {\'101\':\'1\',\'121\':\'2\',\'122\':\'3\',\'124\':\'4\',\'125\':\'5\',\'126\':\'6\',\'127\':\'7\',\'128\':\'8\',\'129\':\'9\',\'205\':\'10\',\'301\':\'11\'}\nUMH_Fileds      = {\'109_14\':(\'u_cat\',\'12\'),\'110_14\':(\'u_shop\',\'13\'),\'127_14\':(\'u_brand\',\'14\'),\'150_14\':(\'u_int\',\'15\')}      #user multi-hot feature\nAd_Fileds       = {\'206\':(\'a_cat\',\'16\'),\'207\':(\'a_shop\',\'17\'),\'210\':(\'a_int\',\'18\'),\'216\':(\'a_brand\',\'19\')}                  #ad feature for DIN\n\n#40362692,0,0,216:9342395:1.0 301:9351665:1.0 205:7702673:1.0 206:8317829:1.0 207:8967741:1.0 508:9356012:2.30259 210:9059239:1.0 210:9042796:1.0 210:9076972:1.0 210:9103884:1.0 210:9063064:1.0 127_14:3529789:2.3979 127_14:3806412:2.70805\ndef gen_tfrecords(in_file):\n    basename = os.path.basename(in_file) + "".tfrecord""\n    out_file = os.path.join(FLAGS.output_dir, basename)\n    tfrecord_out = tf.python_io.TFRecordWriter(out_file)\n    with open(in_file) as fi:\n        for line in fi:\n            fields = line.strip().split(\',\')\n            if len(fields) != 4:\n                continue\n            #1 label\n            y = [float(fields[1])]\n            z = [float(fields[2])]\n            feature = {\n                ""y"": tf.train.Feature(float_list = tf.train.FloatList(value=y)),\n                ""z"": tf.train.Feature(float_list = tf.train.FloatList(value=z))\n             }\n\n            splits = re.split(\'[ :]\', fields[3])\n            ffv = np.reshape(splits,(-1,3))\n            #common_mask = np.array([v in Common_Fileds for v in ffv[:,0]])\n            #af_mask = np.array([v in Ad_Fileds for v in ffv[:,0]])\n            #cf_mask = np.array([v in Context_Fileds for v in ffv[:,0]])\n\n            #2 \xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe7\x89\xb9\xe6\xae\x8a\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\n            feat_ids = np.array([])\n            #feat_vals = np.array([])\n            for f, def_id in Common_Fileds.iteritems():\n                if f in ffv[:,0]:\n                    mask = np.array(f == ffv[:,0])\n                    feat_ids = np.append(feat_ids, ffv[mask,1])\n                    #np.append(feat_vals,ffv[mask,2].astype(np.float))\n                else:\n                    feat_ids = np.append(feat_ids, def_id)\n                    #np.append(feat_vals,1.0)\n            feature.update({""feat_ids"": tf.train.Feature(int64_list=tf.train.Int64List(value=feat_ids.astype(np.int)))})\n                            #""feat_vals"": tf.train.Feature(float_list=tf.train.FloatList(value=feat_vals))})\n\n            #3 \xe7\x89\xb9\xe6\xae\x8a\xe5\xad\x97\xe6\xae\xb5\xe5\x8d\x95\xe7\x8b\xac\xe5\xa4\x84\xe7\x90\x86\n            for f, (fname, def_id) in UMH_Fileds.iteritems():\n                if f in ffv[:,0]:\n                    mask = np.array(f == ffv[:,0])\n                    feat_ids = ffv[mask,1]\n                    feat_vals= ffv[mask,2]\n                else:\n                    feat_ids = np.array([def_id])\n                    feat_vals = np.array([1.0])\n                feature.update({fname+""ids"": tf.train.Feature(int64_list=tf.train.Int64List(value=feat_ids.astype(np.int))),\n                                fname+""vals"": tf.train.Feature(float_list=tf.train.FloatList(value=feat_vals.astype(np.float)))})\n\n            for f, (fname, def_id) in Ad_Fileds.iteritems():\n                if f in ffv[:,0]:\n                    mask = np.array(f == ffv[:,0])\n                    feat_ids = ffv[mask,1]\n                else:\n                    feat_ids = np.array([def_id])\n                feature.update({fname+""ids"": tf.train.Feature(int64_list=tf.train.Int64List(value=feat_ids.astype(np.int)))})\n\n            # serialized to Example\n            example = tf.train.Example(features = tf.train.Features(feature = feature))\n            serialized = example.SerializeToString()\n            tfrecord_out.write(serialized)\n            #num_lines += 1\n            #if num_lines % 10000 == 0:\n            #    print(""Process %d"" % num_lines)\n    tfrecord_out.close()\n\ndef main(_):\n    if not os.path.exists(FLAGS.output_dir):\n        os.mkdir(FLAGS.output_dir)\n    file_list = glob.glob(os.path.join(FLAGS.input_dir, ""*-*""))\n    print(""total files: %d"" % len(file_list))\n\n    pool = ThreadPool(FLAGS.threads) # Sets the pool size\n    pool.map(gen_tfrecords, file_list)\n    pool.close()\n    pool.join()\n\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
deep_ctr/Feature_pipeline/get_criteo_feature.py,0,"b'# coding=utf8\n""""""\nThis code referenced from [here](https://github.com/PaddlePaddle/models/blob/develop/deep_fm/preprocess.py)\n-For numerical features,normalzied to continous values.\n-For categorical features, removed long-tailed data appearing less than 200 times.\n\nTODO\xef\xbc\x9a\n#1 \xe8\xbf\x9e\xe7\xbb\xad\xe7\x89\xb9\xe5\xbe\x81 \xe7\xa6\xbb\xe6\x95\xa3\xe5\x8c\x96\n#2 Nagetive down sampling\n""""""\nimport os\nimport sys\n# import click\nimport random\nimport collections\nimport argparse\nfrom multiprocessing import Pool as ThreadPool\n\n# There are 13 integer features and 26 categorical features\ncontinous_features = range(1, 14)\ncategorial_features = range(14, 40)\n\n# Clip integer features. The clip point for each integer feature\n# is derived from the 95% quantile of the total values in each feature\ncontinous_clip = [20, 600, 100, 50, 64000, 500, 100, 50, 500, 10, 10, 10, 50]\n\n\nclass CategoryDictGenerator:\n    """"""\n    Generate dictionary for each of the categorical features\n    """"""\n\n    def __init__(self, num_feature):\n        self.dicts = []\n        self.num_feature = num_feature\n        for i in range(0, num_feature):\n            self.dicts.append(collections.defaultdict(int))\n\n    def build(self, datafile, categorial_features, cutoff=0):\n        with open(datafile, \'r\') as f:\n            for line in f:\n                features = line.rstrip(\'\\n\').split(\'\\t\')\n                for i in range(0, self.num_feature):\n                    if features[categorial_features[i]] != \'\':\n                        self.dicts[i][features[categorial_features[i]]] += 1\n        for i in range(0, self.num_feature):\n            self.dicts[i] = filter(lambda x: x[1] >= cutoff, self.dicts[i].items())\n            self.dicts[i] = sorted(self.dicts[i], key=lambda x: (-x[1], x[0]))\n            vocabs, _ = list(zip(*self.dicts[i]))\n            self.dicts[i] = dict(zip(vocabs, range(1, len(vocabs) + 1)))\n            self.dicts[i][\'<unk>\'] = 0\n\n    def gen(self, idx, key):\n        if key not in self.dicts[idx]:\n            res = self.dicts[idx][\'<unk>\']\n        else:\n            res = self.dicts[idx][key]\n        return res\n\n    def dicts_sizes(self):\n        return list(map(len, self.dicts))\n\n\nclass ContinuousFeatureGenerator:\n    """"""\n    Normalize the integer features to [0, 1] by min-max normalization\n    """"""\n\n    def __init__(self, num_feature):\n        self.num_feature = num_feature\n        self.min = [sys.maxsize] * num_feature\n        self.max = [-sys.maxsize] * num_feature\n\n    def build(self, datafile, continous_features):\n        with open(datafile, \'r\') as f:\n            for line in f:\n                features = line.rstrip(\'\\n\').split(\'\\t\')\n                for i in range(0, self.num_feature):\n                    val = features[continous_features[i]]\n                    if val != \'\':\n                        val = int(val)\n                        if val > continous_clip[i]:\n                            val = continous_clip[i]\n                        self.min[i] = min(self.min[i], val)\n                        self.max[i] = max(self.max[i], val)\n\n    def gen(self, idx, val):\n        if val == \'\':\n            return 0.0\n        val = float(val)\n        return (val - self.min[idx]) / (self.max[idx] - self.min[idx])\n\n\n# @click.command(""preprocess"")\n# @click.option(""--datadir"", type=str, help=""Path to raw criteo dataset"")\n# @click.option(""--outdir"", type=str, help=""Path to save the processed data"")\ndef preprocess(datadir, outdir):\n    """"""\n    All the 13 integer features are normalzied to continous values and these\n    continous features are combined into one vecotr with dimension 13.\n    Each of the 26 categorical features are one-hot encoded and all the one-hot\n    vectors are combined into one sparse binary vector.\n    """"""\n    # pool = ThreadPool(FLAGS.threads) # Sets the pool size\n    dists = ContinuousFeatureGenerator(len(continous_features))\n    dists.build(FLAGS.input_dir + \'train.txt\', continous_features)\n    # pool.apply(dists.build, args=(FLAGS.input_dir + \'train.txt\', continous_features,))\n\n    dicts = CategoryDictGenerator(len(categorial_features))\n    dicts.build(FLAGS.input_dir + \'train.txt\', categorial_features, cutoff=FLAGS.cutoff)\n    # pool.apply(dicts.build, args=(FLAGS.input_dir + \'train.txt\', categorial_features,))\n\n    # pool.close()\n    # pool.join()\n\n    output = open(FLAGS.output_dir + \'feature_map\', \'w\')\n    for i in continous_features:\n        output.write(""{0} {1}\\n"".format(\'I\' + str(i), i))\n    dict_sizes = dicts.dicts_sizes()\n    categorial_feature_offset = [dists.num_feature]\n    for i in range(1, len(categorial_features) + 1):\n        offset = categorial_feature_offset[i - 1] + dict_sizes[i - 1]\n        categorial_feature_offset.append(offset)\n        for key, val in dicts.dicts[i - 1].items():\n            output.write(""{0} {1}\\n"".format(\'C\' + str(i) + \'|\' + key, categorial_feature_offset[i - 1] + val + 1))\n\n    random.seed(0)\n\n    # 90% of the data are used for training, and 10% of the data are used\n    # for validation.\n    with open(FLAGS.output_dir + \'tr.libsvm\', \'w\') as out_train:\n        with open(FLAGS.output_dir + \'va.libsvm\', \'w\') as out_valid:\n            with open(FLAGS.input_dir + \'train.txt\', \'r\') as f:\n                for line in f:\n                    features = line.rstrip(\'\\n\').split(\'\\t\')\n\n                    feat_vals = []\n                    for i in range(0, len(continous_features)):\n                        val = dists.gen(i, features[continous_features[i]])\n                        feat_vals.append(\n                            str(continous_features[i]) + \':\' + ""{0:.6f}"".format(val).rstrip(\'0\').rstrip(\'.\'))\n\n                    for i in range(0, len(categorial_features)):\n                        val = dicts.gen(i, features[categorial_features[i]]) + categorial_feature_offset[i]\n                        feat_vals.append(str(val) + \':1\')\n\n                    label = features[0]\n                    if random.randint(0, 9999) % 10 != 0:\n                        out_train.write(""{0} {1}\\n"".format(label, \' \'.join(feat_vals)))\n                    else:\n                        out_valid.write(""{0} {1}\\n"".format(label, \' \'.join(feat_vals)))\n\n    with open(FLAGS.output_dir + \'te.libsvm\', \'w\') as out:\n        with open(FLAGS.input_dir + \'test.txt\', \'r\') as f:\n            for line in f:\n                features = line.rstrip(\'\\n\').split(\'\\t\')\n\n                feat_vals = []\n                for i in range(0, len(continous_features)):\n                    val = dists.gen(i, features[continous_features[i] - 1])\n                    feat_vals.append(str(continous_features[i]) + \':\' + ""{0:.6f}"".format(val).rstrip(\'0\').rstrip(\'.\'))\n\n                for i in range(0, len(categorial_features)):\n                    val = dicts.gen(i, features[categorial_features[i] - 1]) + categorial_feature_offset[i]\n                    feat_vals.append(str(val) + \':1\')\n\n                out.write(""{0} {1}\\n"".format(label, \' \'.join(feat_vals)))\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--threads"",\n        type=int,\n        default=2,\n        help=""threads num""\n    )\n    parser.add_argument(\n        ""--input_dir"",\n        type=str,\n        default="""",\n        help=""input data dir""\n    )\n    parser.add_argument(\n        ""--output_dir"",\n        type=str,\n        default="""",\n        help=""feature map output dir""\n    )\n    parser.add_argument(\n        ""--cutoff"",\n        type=int,\n        default=200,\n        help=""cutoff long-tailed categorical values""\n    )\n\n    FLAGS, unparsed = parser.parse_known_args()\n    print(\'threads \', FLAGS.threads)\n    print(\'input_dir \', FLAGS.input_dir)\n    print(\'output_dir \', FLAGS.output_dir)\n    print(\'cutoff \', FLAGS.cutoff)\n\n    preprocess(FLAGS.input_dir, FLAGS.output_dir)\n'"
deep_ctr/Feature_pipeline/get_frape_feature.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n""""""\nlabel map to {0, 1}\n""""""\nimport sys\nreload(sys)\nsys.setdefaultencoding(""utf-8"")\nfrom multiprocessing import Pool as ThreadPool\nimport argparse\nimport glob\nfrom collections import defaultdict\n\n#-1 451:1 4149:1 5041:1 5046:1 5053:1 5055:1 5058:1 5060:1 5069:1 5149:1\n\ndef get_frape_feature(if_str):\n    output = open(if_str.split(\'.\')[0] +\'_.libsvm\',\'w\')\n    num = 0\n    with open(if_str,""rt"") as f:\n        for line in f:\n            try:\n                label, feats = line.strip().split(\' \', 1)\n                if label == \'-1\':\n                    label = \'0\'\n\n                output.write(""{0} {1}\\n"".format(label, feats))\n            except:\n                #output.write(line)\n                continue\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--threads"",\n        type=int,\n        default=10,\n        help=""threads num""\n        )\n    parser.add_argument(\n        ""--input_dir"",\n        type=str,\n        default="""",\n        help=""input data dir""\n        )\n    parser.add_argument(\n        ""--output_dir"",\n        type=str,\n        default="""",\n        help=""feature map output dir""\n        )\n\n\n    FLAGS, unparsed = parser.parse_known_args()\n    print(\'threads \', FLAGS.threads)\n    print(\'input_dir \', FLAGS.input_dir)\n    print(\'output_dir \', FLAGS.output_dir)\n\n    file_list = glob.glob(FLAGS.input_dir+\'/*libsvm\')\n    print(\'file_list size \', len(file_list))\n    pool = ThreadPool(FLAGS.threads) # Sets the pool size\n    pool.map(get_frape_feature, file_list)\n    pool.close()\n    pool.join()\n'"
deep_ctr/Feature_pipeline/get_smart_feature.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n""""""\nbml to csv\n""""""\nimport sys\nreload(sys)\nsys.setdefaultencoding(""utf-8"")\nfrom multiprocessing import Pool as ThreadPool\nimport argparse\nimport glob\nfrom collections import defaultdict\n\n#0,ADR,7.0,PHONE,OOV,0,\xe7\xbb\x8f\xe9\xaa\x8c,\xe6\x9f\xa5\xe7\x9c\x8b,22,Mon,0,0.06808,0.01559,0.01714,0.00524,0.01983,0.01053,0.01708,0.01273,0.01161,0.00794,0.00938,0.00867,0.0086,0.0059,0.07692,0.00577,0.00562,\n#1207,1235,1354,1379,1363,1382,1304,1313,1202,1251,1101,1136,970,693,1184,843,980,710,1173,700,766,570,430,732,340,475,684,566,632,460,480,453,390,440,582,334,533,521,346,421,372,770,707,575,470,674,394,530,332,225,583,532,270,510,513,387,701,264,251,496,230,689,678,515,299,569,786,671,321,599,498,324,532,404,259,582,342,232,700,688,468,623,722,627,280,546,327,522,356,188,297,523,378,398,125,202,330,737,603,376\n\n#\xe4\xbf\x9d\xe8\xaf\x81\xe9\xa1\xba\xe5\xba\x8f\xe4\xbb\xa5\xe5\x8f\x8a\xe6\x95\xb0\xe9\x87\x8f\n#Constants\nxgb_trees = 100\n# Column Title\nCSV_COLUMNS = [ ""is_click"",""u_pl"",""u_ppvn"",""u_de"",""u_os"",""u_t"",""a_m_w"",""a_b_w"",""c_h"",""c_w"",""c_al"",\n                ""u_ctr"",""a_a_ctr"",""a_t_ctr"",""c_q_ctr"",""c_al_ctr"",""c_n_ctr"",""c_t_ctr"",""c_t_n_ctr"",\n                ""u_a_city_ctr"",""u_a_age_ctr"",""u_a_x_ctr"",""u_a_g_ctr"",""u_a_c_ctr"",""c_q_a_ctr"",""c_q_t_sim"",""c_q_adtype_ctr"",""c_mw_a_ctr"" ]\nXGB_COLUMNS = [ \'xgbf_%d\' % i for i in range(xgb_trees) ]\nCSV_COLUMNS = CSV_COLUMNS + XGB_COLUMNS\n\ndef get_feature_map(file_list):\n    output = open(FLAGS.output_dir + \'feature_map\',\'w\')\n    feature_map = defaultdict(lambda: 0)\n    fid = 1\n    for fname in CSV_COLUMNS:\n        feature_map[CSV_COLUMNS[i] + \'|UNK\'] = fid\n        fid += 1\n\n    for if_str in file_list:\n        with open(if_str,""rt"") as f:\n            for line in f:\n                try:\n                    splits = line.strip().split("","")\n                    for i in range(1,len(splits[1:])):\n                        if(i >= 11 and i <= 27):\n                            key = CSV_COLUMNS[i]\n                        else:\n                            key = CSV_COLUMNS[i] + \'|\' + splits[i]\n                        if feature_map.get(key) == None:\n                            feature_map[key] = fid\n                            fid += 1\n                except:\n                    #output.write(line)\n                    continue\n\n    for key,val in feature_map.iteritems():\n        output.write(""{0} {1}\\n"".format(key,val))\n\ndef get_smart_feature(if_str):\n    feature_map = defaultdict(lambda: 0)\n    with open(FLAGS.output_dir + \'feature_map\',\'rt\') as f:\n        for line in f:\n            try:\n                splits = line.strip().split("" "")\n                feature_map[splits[0]] = splits[1]\n            except:\n                continue\n    prefix = \'\'\n    if FLAGS.task_type == \'tr\':\n        prefix = prefix + \'_\' + if_str.rsplit(\'_\')[3]\n    output = open(FLAGS.output_dir + FLAGS.task_type + prefix +\'.libsvm\',\'w\')\n    with open(if_str,""rt"") as f:\n        for line in f:\n            try:\n                splits = line.strip().split("","")\n                is_click = splits[0]\n                feat = []\n                for i in range(1,len(splits[1:])):\n                    if(i >= 11 and i <= 27):\n                        key = CSV_COLUMNS[i]\n                        fid = feature_map.get(key)\n                        feat.append(str(fid) + \':\' + splits[i])\n                    else:\n                        key = CSV_COLUMNS[i] + \'|\' + splits[i]\n                        fid = feature_map.get(key)\n                        if fid == None:\n                            fid = feature_map.get(CSV_COLUMNS[i] + \'|UNK\')\n                        feat.append(str(fid) + \':1\')\n\n                output.write(""{0} {1}\\n"".format(is_click,\' \'.join(feat)))\n            except:\n                #output.write(line)\n                continue\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--threads"",\n        type=int,\n        default=10,\n        help=""threads num""\n        )\n    parser.add_argument(\n        ""--input_dir"",\n        type=str,\n        default="""",\n        help=""input data dir""\n        )\n    parser.add_argument(\n        ""--output_dir"",\n        type=str,\n        default="""",\n        help=""feature map output dir""\n        )\n    parser.add_argument(\n        ""--task_type"",\n        type=str,\n        default=""tr"",\n        help=""{tr,va,te}""\n        )\n\n    FLAGS, unparsed = parser.parse_known_args()\n    print(\'threads \', FLAGS.threads)\n    print(\'input_dir \', FLAGS.input_dir)\n    print(\'output_dir \', FLAGS.output_dir)\n    print(\'task_type \', FLAGS.task_type)\n\n    if FLAGS.task_type == \'tr\':\n        file_list = glob.glob(FLAGS.input_dir+\'/*part*\')\n        #FeatureMapGenerator\n        #get_feature_map(file_list)\n    elif FLAGS.task_type == \'va\':\n        file_list = glob.glob(FLAGS.input_dir+\'/*verify\')\n    elif FLAGS.task_type == \'te\':\n        file_list = glob.glob(FLAGS.input_dir+\'/*test\')\n\n    print(\'file_list size \', len(file_list))\n    pool = ThreadPool(FLAGS.threads) # Sets the pool size\n    pool.map(get_smart_feature, file_list)\n    pool.close()\n    pool.join()\n'"
deep_ctr/Model_pipeline/AFM.py,106,"b'#!/usr/bin/env python\n#coding=utf-8\n""""""\nTensorFlow Implementation of <<Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks>>\nwith the fellowing features\xef\xbc\x9a\n#1 Input pipline using Dataset high level API, Support parallel and prefetch reading\n#2 Train pipline using Coustom Estimator by rewriting model_fn\n#3 Support distincted training by TF_CONFIG\n#4 Support export model for online predicting service using TensorFlow Serving\n\nby lambdaji\n""""""\n#from __future__ import absolute_import\n#from __future__ import division\n#from __future__ import print_function\n\n#import argparse\nimport shutil\n#import sys\nimport os\nimport json\nimport glob\nfrom datetime import date, timedelta\nfrom time import time\n#import gc\n#from multiprocessing import Process\n\n#import math\nimport random\n#import pandas as pd\n#import numpy as np\nimport tensorflow as tf\n\n#################### CMD Arguments ####################\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""dist_mode"", 0, ""distribuion mode {0-loacal, 1-single_dist, 2-multi_dist}"")\ntf.app.flags.DEFINE_string(""ps_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""worker_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""job_name"", \'\', ""One of \'ps\', \'worker\'"")\ntf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")\ntf.app.flags.DEFINE_integer(""num_threads"", 10, ""Number of threads"")\ntf.app.flags.DEFINE_integer(""feature_size"", 0, ""Number of features"")\ntf.app.flags.DEFINE_integer(""field_size"", 0, ""Number of fields"")\ntf.app.flags.DEFINE_integer(""embedding_size"", 256, ""Embedding size"")\ntf.app.flags.DEFINE_integer(""num_epochs"", 10, ""Number of epochs"")\ntf.app.flags.DEFINE_integer(""batch_size"", 128, ""Number of batch size"")\ntf.app.flags.DEFINE_integer(""log_steps"", 1000, ""save summary every steps"")\ntf.app.flags.DEFINE_float(""learning_rate"", 0.1, ""learning rate"")\ntf.app.flags.DEFINE_float(""l2_reg"", 1.0, ""L2 regularization"")\ntf.app.flags.DEFINE_string(""loss_type"", \'log_loss\', ""loss type {square_loss, log_loss}"")\ntf.app.flags.DEFINE_string(""optimizer"", \'Adam\', ""optimizer type {Adam, Adagrad, GD, Momentum}"")\ntf.app.flags.DEFINE_string(""attention_layers"", \'256\', ""Attention Net mlp layers"")\ntf.app.flags.DEFINE_string(""dropout"", \'1.0,0.5\', ""dropout rate"")\ntf.app.flags.DEFINE_string(""data_dir"", \'\', ""data dir"")\ntf.app.flags.DEFINE_string(""dt_dir"", \'\', ""data dt partition"")\ntf.app.flags.DEFINE_string(""model_dir"", \'\', ""model check point dir"")\ntf.app.flags.DEFINE_string(""servable_model_dir"", \'\', ""export servable model for TensorFlow Serving"")\ntf.app.flags.DEFINE_string(""task_type"", \'train\', ""task type {train, infer, eval, export}"")\ntf.app.flags.DEFINE_boolean(""clear_existing_model"", False, ""clear existing model or not"")\n\n#1 1:0.5 2:0.03519 3:1 4:0.02567 7:0.03708 8:0.01705 9:0.06296 10:0.18185 11:0.02497 12:1 14:0.02565 15:0.03267 17:0.0247 18:0.03158 20:1 22:1 23:0.13169 24:0.02933 27:0.18159 31:0.0177 34:0.02888 38:1 51:1 63:1 132:1 164:1 236:1\ndef input_fn(filenames, batch_size=32, num_epochs=1, perform_shuffle=False):\n    print(\'Parsing\', filenames)\n    def decode_libsvm(line):\n        #columns = tf.decode_csv(value, record_defaults=CSV_COLUMN_DEFAULTS)\n        #features = dict(zip(CSV_COLUMNS, columns))\n        #labels = features.pop(LABEL_COLUMN)\n        columns = tf.string_split([line], \' \')\n        labels = tf.string_to_number(columns.values[0], out_type=tf.float32)\n        splits = tf.string_split(columns.values[1:], \':\')\n        id_vals = tf.reshape(splits.values,splits.dense_shape)\n        feat_ids, feat_vals = tf.split(id_vals,num_or_size_splits=2,axis=1)\n        feat_ids = tf.string_to_number(feat_ids, out_type=tf.int32)\n        feat_vals = tf.string_to_number(feat_vals, out_type=tf.float32)\n        #feat_ids = tf.reshape(feat_ids,shape=[-1,FLAGS.field_size])\n        #for i in range(splits.dense_shape.eval()[0]):\n        #    feat_ids.append(tf.string_to_number(splits.values[2*i], out_type=tf.int32))\n        #    feat_vals.append(tf.string_to_number(splits.values[2*i+1]))\n        #return tf.reshape(feat_ids,shape=[-1,field_size]), tf.reshape(feat_vals,shape=[-1,field_size]), labels\n        return {""feat_ids"": feat_ids, ""feat_vals"": feat_vals}, labels\n\n    # Extract lines from input files using the Dataset API, can pass one filename or filename list\n    dataset = tf.data.TextLineDataset(filenames).map(decode_libsvm, num_parallel_calls=10).prefetch(500000)    # multi-thread pre-process then prefetch\n\n    # Randomizes input using a window of 256 elements (read into memory)\n    if perform_shuffle:\n        dataset = dataset.shuffle(buffer_size=256)\n\n    # epochs from blending together.\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.batch(batch_size) # Batch size to use\n\n    #return dataset.make_one_shot_iterator()\n    iterator = dataset.make_one_shot_iterator()\n    batch_features, batch_labels = iterator.get_next()\n    #return tf.reshape(batch_ids,shape=[-1,field_size]), tf.reshape(batch_vals,shape=[-1,field_size]), batch_labels\n    return batch_features, batch_labels\n\ndef model_fn(features, labels, mode, params):\n    """"""Bulid Model function f(x) for Estimator.""""""\n    #------hyperparameters----\n    field_size = params[""field_size""]\n    feature_size = params[""feature_size""]\n    embedding_size = params[""embedding_size""]\n    l2_reg = params[""l2_reg""]\n    learning_rate = params[""learning_rate""]\n    #optimizer = params[""optimizer""]\n    layers = map(int, params[""attention_layers""].split(\',\'))\n    dropout = map(float, params[""dropout""].split(\',\'))\n\n    #------bulid weights------\n    Global_Bias = tf.get_variable(name=\'bias\', shape=[1], initializer=tf.constant_initializer(0.0))\n    Feat_Bias = tf.get_variable(name=\'linear\', shape=[feature_size], initializer=tf.glorot_normal_initializer())\n    Feat_Emb = tf.get_variable(name=\'emb\', shape=[feature_size,embedding_size], initializer=tf.glorot_normal_initializer())\n\n    #------build feaure-------\n    feat_ids  = features[\'feat_ids\']\n    feat_ids = tf.reshape(feat_ids,shape=[-1,field_size])\n    feat_vals = features[\'feat_vals\']\n    feat_vals = tf.reshape(feat_vals,shape=[-1,field_size])\n\n    #------build f(x)------\n    with tf.variable_scope(""Linear-part""):\n        feat_wgts = tf.nn.embedding_lookup(Feat_Bias, feat_ids) # None * F * 1\n        y_linear = tf.reduce_sum(tf.multiply(feat_wgts, feat_vals),1)\n\n    with tf.variable_scope(""Pairwise-Interaction-Layer""):\n        embeddings = tf.nn.embedding_lookup(Feat_Emb, feat_ids) # None * F * K\n        feat_vals = tf.reshape(feat_vals, shape=[-1, field_size, 1])\n        embeddings = tf.multiply(embeddings, feat_vals) #vij*xi\n\n        num_interactions = field_size*(field_size-1)/2\n        element_wise_product_list = []\n        for i in range(0, field_size):\n            for j in range(i+1, field_size):\n                element_wise_product_list.append(tf.multiply(embeddings[:,i,:], embeddings[:,j,:]))\n        element_wise_product = tf.stack(element_wise_product_list) \t\t\t\t\t\t\t\t# (F*(F-1)) * None * K\n        element_wise_product = tf.transpose(element_wise_product, perm=[1,0,2]) \t\t\t\t# None * (F*(F-1)) * K\n        #interactions = tf.reduce_sum(element_wise_product, 2, name=""interactions"")\n\n    with tf.variable_scope(""Attention-part""):\n        deep_inputs = tf.reshape(element_wise_product, shape=[-1, embedding_size]) \t\t\t\t# (None * (F*(F-1))) * K\n        for i in range(len(layers)):\n            deep_inputs = tf.contrib.layers.fully_connected(inputs=deep_inputs, num_outputs=layers[i], \\\n                weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'mlp%d\' % i)\n\n        aij = tf.contrib.layers.fully_connected(inputs=deep_inputs, num_outputs=1, activation_fn=tf.identity, \\\n            weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'attention_out\')# (None * (F*(F-1))) * 1\n\n        #aij_reshape = tf.reshape(aij, shape=[-1, num_interactions, 1])\t\t\t\t\t\t\t# None * (F*(F-1)) * 1\n        aij_softmax = tf.nn.softmax(tf.reshape(aij, shape=[-1, num_interactions, 1]), dim=1, name=\'attention_soft\')\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            aij_softmax = tf.nn.dropout(aij_softmax, keep_prob=dropout[0])\n\n    with tf.variable_scope(""Attention-based-Pooling""):\n        y_emb = tf.reduce_sum(tf.multiply(aij_softmax, element_wise_product), 1) \t\t\t\t# None * K\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            y_emb = tf.nn.dropout(y_emb, keep_prob=dropout[1])\n\n        y_d = tf.contrib.layers.fully_connected(inputs=y_emb, num_outputs=1, activation_fn=tf.identity, \\\n            weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'deep_out\')\t\t# None * 1\n        y_deep = tf.reshape(y_d,shape=[-1])\n\n    with tf.variable_scope(""AFM-out""):\n        #y_bias = Global_Bias * tf.ones_like(labels, dtype=tf.float32)  # None * 1  warning;\xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\x8d\xe8\x83\xbd\xe7\x94\xa8label\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe8\xb0\x83\xe7\x94\xa8predict/export\xe5\x87\xbd\xe6\x95\xb0\xe4\xbc\x9a\xe5\x87\xba\xe9\x94\x99\xef\xbc\x8ctrain/evaluate\xe6\xad\xa3\xe5\xb8\xb8\xef\xbc\x9b\xe5\x88\x9d\xe6\xad\xa5\xe5\x88\xa4\xe6\x96\xadestimator\xe5\x81\x9a\xe4\xba\x86\xe4\xbc\x98\xe5\x8c\x96\xef\xbc\x8c\xe7\x94\xa8\xe4\xb8\x8d\xe5\x88\xb0label\xe6\x97\xb6\xe4\xb8\x8d\xe4\xbc\xa0\n        y_bias = Global_Bias * tf.ones_like(y_deep, dtype=tf.float32)   # None * 1\n        y = y_bias + y_linear + y_deep\n        pred = tf.sigmoid(y)\n\n    predictions={""prob"": pred}\n    export_outputs = {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: tf.estimator.export.PredictOutput(predictions)}\n    # Provide an estimator spec for `ModeKeys.PREDICT`\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                export_outputs=export_outputs)\n\n    #------bulid loss------\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=labels)) + \\\n        l2_reg * tf.nn.l2_loss(Feat_Bias) + l2_reg * tf.nn.l2_loss(Feat_Emb)\n\n    # Provide an estimator spec for `ModeKeys.EVAL`\n    eval_metric_ops = {\n        ""auc"": tf.metrics.auc(labels, pred)\n    }\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                eval_metric_ops=eval_metric_ops)\n\n    #------bulid optimizer------\n    if FLAGS.optimizer == \'Adam\':\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8)\n    elif FLAGS.optimizer == \'Adagrad\':\n        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate, initial_accumulator_value=1e-8)\n    elif FLAGS.optimizer == \'Momentum\':\n        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95)\n    elif FLAGS.optimizer == \'ftrl\':\n        optimizer = tf.train.FtrlOptimizer(learning_rate)\n\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n\n    # Provide an estimator spec for `ModeKeys.TRAIN` modes\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                train_op=train_op)\n\n    # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\n    #return tf.estimator.EstimatorSpec(\n    #        mode=mode,\n    #        loss=loss,\n    #        train_op=train_op,\n    #        predictions={""prob"": pred},\n    #        eval_metric_ops=eval_metric_ops)\n\ndef set_dist_env():\n    if FLAGS.dist_mode == 1:        # \xe6\x9c\xac\xe5\x9c\xb0\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xb5\x8b\xe8\xaf\x95\xe6\xa8\xa1\xe5\xbc\x8f1 chief, 1 ps, 1 evaluator\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        chief_hosts = FLAGS.chief_hosts.split(\',\')\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # \xe6\x97\xa0worker\xe5\x8f\x82\xe6\x95\xb0\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n    elif FLAGS.dist_mode == 2:      # \xe9\x9b\x86\xe7\xbe\xa4\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xa8\xa1\xe5\xbc\x8f\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        worker_hosts = FLAGS.worker_hosts.split(\',\')\n        chief_hosts = worker_hosts[0:1] # get first worker as chief\n        worker_hosts = worker_hosts[2:] # the rest as worker\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'worker_host\', worker_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # use #worker=0 as chief\n        if job_name == ""worker"" and task_index == 0:\n            job_name = ""chief""\n        # use #worker=1 as evaluator\n        if job_name == ""worker"" and task_index == 1:\n            job_name = \'evaluator\'\n            task_index = 0\n        # the others as worker\n        if job_name == ""worker"" and task_index > 1:\n            task_index -= 2\n\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'worker\': worker_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n\ndef main(_):\n    #------check Arguments------\n    if FLAGS.dt_dir == """":\n        FLAGS.dt_dir = (date.today() + timedelta(-1)).strftime(\'%Y%m%d\')\n    FLAGS.model_dir = FLAGS.model_dir + FLAGS.dt_dir\n    #FLAGS.data_dir  = FLAGS.data_dir + FLAGS.dt_dir\n\n    print(\'task_type \', FLAGS.task_type)\n    print(\'model_dir \', FLAGS.model_dir)\n    print(\'data_dir \', FLAGS.data_dir)\n    print(\'dt_dir \', FLAGS.dt_dir)\n    print(\'num_epochs \', FLAGS.num_epochs)\n    print(\'feature_size \', FLAGS.feature_size)\n    print(\'field_size \', FLAGS.field_size)\n    print(\'embedding_size \', FLAGS.embedding_size)\n    print(\'batch_size \', FLAGS.batch_size)\n    print(\'attention_layers \', FLAGS.attention_layers)\n    print(\'dropout \', FLAGS.dropout)\n    print(\'loss_type \', FLAGS.loss_type)\n    print(\'optimizer \', FLAGS.optimizer)\n    print(\'learning_rate \', FLAGS.learning_rate)\n    print(\'l2_reg \', FLAGS.l2_reg)\n\n    #------init Envs------\n    tr_files = glob.glob(""%s/tr*libsvm"" % FLAGS.data_dir)\n    random.shuffle(tr_files)\n    print(""tr_files:"", tr_files)\n    va_files = glob.glob(""%s/va*libsvm"" % FLAGS.data_dir)\n    print(""va_files:"", va_files)\n    te_files = glob.glob(""%s/te*libsvm"" % FLAGS.data_dir)\n    print(""te_files:"", te_files)\n\n    if FLAGS.clear_existing_model:\n        try:\n            shutil.rmtree(FLAGS.model_dir)\n        except Exception as e:\n            print(e, ""at clear_existing_model"")\n        else:\n            print(""existing model cleaned at %s"" % FLAGS.model_dir)\n\n    set_dist_env()\n\n    #------bulid Tasks------\n    model_params = {\n        ""field_size"": FLAGS.field_size,\n        ""feature_size"": FLAGS.feature_size,\n        ""embedding_size"": FLAGS.embedding_size,\n        ""learning_rate"": FLAGS.learning_rate,\n        ""l2_reg"": FLAGS.l2_reg,\n        ""attention_layers"": FLAGS.attention_layers,\n        ""dropout"": FLAGS.dropout\n    }\n    config = tf.estimator.RunConfig().replace(session_config = tf.ConfigProto(device_count={\'GPU\':0, \'CPU\':FLAGS.num_threads}),\n            log_step_count_steps=FLAGS.log_steps, save_summary_steps=FLAGS.log_steps)\n    Estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=FLAGS.model_dir, params=model_params, config=config)\n\n    if FLAGS.task_type == \'train\':\n        train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(tr_files, num_epochs=FLAGS.num_epochs, batch_size=FLAGS.batch_size))\n        eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size), steps=None, start_delay_secs=1000, throttle_secs=1200)\n        tf.estimator.train_and_evaluate(Estimator, train_spec, eval_spec)\n    elif FLAGS.task_type == \'eval\':\n        Estimator.evaluate(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size))\n    elif FLAGS.task_type == \'infer\':\n        preds = Estimator.predict(input_fn=lambda: input_fn(te_files, num_epochs=1, batch_size=FLAGS.batch_size), predict_keys=""prob"")\n        with open(FLAGS.data_dir+""/pred.txt"", ""w"") as fo:\n            for prob in preds:\n                fo.write(""%f\\n"" % (prob[\'prob\']))\n    elif FLAGS.task_type == \'export\':\n        #feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n        #feature_spec = {\n        #    \'feat_ids\': tf.FixedLenFeature(dtype=tf.int64, shape=[None, FLAGS.field_size]),\n        #    \'feat_vals\': tf.FixedLenFeature(dtype=tf.float32, shape=[None, FLAGS.field_size])\n        #}\n        #serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n        feature_spec = {\n            \'feat_ids\': tf.placeholder(dtype=tf.int64, shape=[None, FLAGS.field_size], name=\'feat_ids\'),\n            \'feat_vals\': tf.placeholder(dtype=tf.float32, shape=[None, FLAGS.field_size], name=\'feat_vals\')\n        }\n        serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_spec)\n        Estimator.export_savedmodel(FLAGS.servable_model_dir, serving_input_receiver_fn)\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
deep_ctr/Model_pipeline/DCN.py,107,"b'#!/usr/bin/env python\n# coding=utf-8\n""""""\nTensorFlow Implementation of <<Deep & Cross Network for Ad Click Predictions>> with the fellowing features\xef\xbc\x9a\n#1 Input pipline using Dataset high level API, Support parallel and prefetch reading\n#2 Train pipline using Coustom Estimator by rewriting model_fn\n#3 Support distincted training using TF_CONFIG\n#4 Support export_model for TensorFlow Serving\n\nby lambdaji\n""""""\n# from __future__ import absolute_import\n# from __future__ import division\n# from __future__ import print_function\n\n# import argparse\nimport shutil\n# import sys\nimport os\nimport json\nimport glob\nfrom datetime import date, timedelta\nfrom time import time\n# import gc\n# from multiprocessing import Process\n\n# import math\nimport random\n# import pandas as pd\n# import numpy as np\nimport tensorflow as tf\n\n#################### CMD Arguments ####################\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""dist_mode"", 0, ""distribuion mode {0-loacal, 1-single_dist, 2-multi_dist}"")\ntf.app.flags.DEFINE_string(""ps_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""worker_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""job_name"", \'\', ""One of \'ps\', \'worker\'"")\ntf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")\ntf.app.flags.DEFINE_integer(""num_threads"", 16, ""Number of threads"")\ntf.app.flags.DEFINE_integer(""feature_size"", 0, ""Number of features"")\ntf.app.flags.DEFINE_integer(""field_size"", 0, ""Number of fields"")\ntf.app.flags.DEFINE_integer(""embedding_size"", 32, ""Embedding size"")\ntf.app.flags.DEFINE_integer(""num_epochs"", 10, ""Number of epochs"")\ntf.app.flags.DEFINE_integer(""batch_size"", 64, ""Number of batch size"")\ntf.app.flags.DEFINE_integer(""log_steps"", 1000, ""save summary every steps"")\ntf.app.flags.DEFINE_float(""learning_rate"", 0.0005, ""learning rate"")\ntf.app.flags.DEFINE_float(""l2_reg"", 0.0001, ""L2 regularization"")\n# tf.app.flags.DEFINE_string(""loss_type"", \'log_loss\', ""loss type {square_loss, log_loss}"")\ntf.app.flags.DEFINE_string(""optimizer"", \'Adam\', ""optimizer type {Adam, Adagrad, GD, Momentum}"")\ntf.app.flags.DEFINE_string(""deep_layers"", \'256,128,64\', ""deep layers"")\ntf.app.flags.DEFINE_integer(""cross_layers"", 3, ""cross layers, polynomial degree"")\ntf.app.flags.DEFINE_string(""dropout"", \'0.5,0.5,0.5\', ""dropout rate"")\ntf.app.flags.DEFINE_boolean(""batch_norm"", False, ""perform batch normaization (True or False)"")\ntf.app.flags.DEFINE_float(""batch_norm_decay"", 0.9, ""decay for the moving average(recommend trying decay=0.9)"")\ntf.app.flags.DEFINE_string(""data_dir"", \'\', ""data dir"")\ntf.app.flags.DEFINE_string(""dt_dir"", \'\', ""data dt partition"")\ntf.app.flags.DEFINE_string(""model_dir"", \'\', ""model check point dir"")\ntf.app.flags.DEFINE_string(""servable_model_dir"", \'\', ""export servable model for TensorFlow Serving"")\ntf.app.flags.DEFINE_string(""task_type"", \'train\', ""task type {train, infer, eval, export}"")\ntf.app.flags.DEFINE_boolean(""clear_existing_model"", False, ""clear existing model or not"")\n\n\n# 1 1:0.5 2:0.03519 3:1 4:0.02567 7:0.03708 8:0.01705 9:0.06296 10:0.18185 11:0.02497 12:1 14:0.02565 15:0.03267 17:0.0247 18:0.03158 20:1 22:1 23:0.13169 24:0.02933 27:0.18159 31:0.0177 34:0.02888 38:1 51:1 63:1 132:1 164:1 236:1\ndef input_fn(filenames, batch_size=32, num_epochs=1, perform_shuffle=False):\n    print(\'Parsing\', filenames)\n\n    def decode_libsvm(line):\n        # columns = tf.decode_csv(value, record_defaults=CSV_COLUMN_DEFAULTS)\n        # features = dict(zip(CSV_COLUMNS, columns))\n        # labels = features.pop(LABEL_COLUMN)\n        columns = tf.string_split([line], \' \')\n        labels = tf.string_to_number(columns.values[0], out_type=tf.float32)\n        splits = tf.string_split(columns.values[1:], \':\')\n        id_vals = tf.reshape(splits.values, splits.dense_shape)\n        feat_ids, feat_vals = tf.split(id_vals, num_or_size_splits=2, axis=1)\n        feat_ids = tf.string_to_number(feat_ids, out_type=tf.int32)\n        feat_vals = tf.string_to_number(feat_vals, out_type=tf.float32)\n        # feat_ids = tf.reshape(feat_ids,shape=[-1,FLAGS.field_size])\n        # for i in range(splits.dense_shape.eval()[0]):\n        #    feat_ids.append(tf.string_to_number(splits.values[2*i], out_type=tf.int32))\n        #    feat_vals.append(tf.string_to_number(splits.values[2*i+1]))\n        # return tf.reshape(feat_ids,shape=[-1,field_size]), tf.reshape(feat_vals,shape=[-1,field_size]), labels\n        return {""feat_ids"": feat_ids, ""feat_vals"": feat_vals}, labels\n\n    # Extract lines from input files using the Dataset API, can pass one filename or filename list\n    dataset = tf.data.TextLineDataset(filenames).map(decode_libsvm, num_parallel_calls=10).prefetch(\n        500000)  # multi-thread pre-process then prefetch\n\n    # Randomizes input using a window of 256 elements (read into memory)\n    if perform_shuffle:\n        dataset = dataset.shuffle(buffer_size=256)\n\n    # epochs from blending together.\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.batch(batch_size)  # Batch size to use\n\n    # return dataset.make_one_shot_iterator()\n    iterator = dataset.make_one_shot_iterator()\n    batch_features, batch_labels = iterator.get_next()\n    # return tf.reshape(batch_ids,shape=[-1,field_size]), tf.reshape(batch_vals,shape=[-1,field_size]), batch_labels\n    return batch_features, batch_labels\n\n\ndef model_fn(features, labels, mode, params):\n    """"""Bulid Model function f(x) for Estimator.""""""\n    # ------hyperparameters----\n    field_size = params[""field_size""]\n    feature_size = params[""feature_size""]\n    embedding_size = params[""embedding_size""]\n    l2_reg = params[""l2_reg""]\n    learning_rate = params[""learning_rate""]\n    # batch_norm_decay = params[""batch_norm_decay""]\n    # optimizer = params[""optimizer""]\n    deep_layers = list(map(int, params[""deep_layers""].split(\',\')))\n    cross_layers = params[""cross_layers""]\n    dropout = list(map(float, params[""dropout""].split(\',\')))\n\n    # ------bulid weights------\n    Cross_B = tf.get_variable(name=\'cross_b\', shape=[cross_layers, field_size * embedding_size],\n                              initializer=tf.glorot_normal_initializer())\n    Cross_W = tf.get_variable(name=\'cross_w\', shape=[cross_layers, field_size * embedding_size],\n                              initializer=tf.glorot_normal_initializer())\n    Feat_Emb = tf.get_variable(name=\'emb\', shape=[feature_size, embedding_size],\n                               initializer=tf.glorot_normal_initializer())\n\n    # ------build feaure-------\n    feat_ids = features[\'feat_ids\']\n    feat_ids = tf.reshape(feat_ids, shape=[-1, field_size])\n    feat_vals = features[\'feat_vals\']\n    feat_vals = tf.reshape(feat_vals, shape=[-1, field_size])\n\n    # ------build f(x)------\n    with tf.variable_scope(""Embedding-layer""):\n        embeddings = tf.nn.embedding_lookup(Feat_Emb, feat_ids)  # None * F * K\n        feat_vals = tf.reshape(feat_vals, shape=[-1, field_size, 1])\n        embeddings = tf.multiply(embeddings, feat_vals)  # None * F * K\n        x0 = tf.reshape(embeddings, shape=[-1, field_size * embedding_size])  # None * (F*K)\n\n    with tf.variable_scope(""Cross-Network""):\n        xl = x0\n        for l in range(cross_layers):\n            wl = tf.reshape(Cross_W[l], shape=[-1, 1])  # (F*K) * 1\n            xlw = tf.matmul(xl, wl)  # None * 1\n            xl = x0 * xlw + xl + Cross_B[l]  # None * (F*K) broadcast\n\n    with tf.variable_scope(""Deep-Network""):\n        if FLAGS.batch_norm:\n            # normalizer_fn = tf.contrib.layers.batch_norm\n            # normalizer_fn = tf.layers.batch_normalization\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                train_phase = True\n                # normalizer_params = {\'decay\': batch_norm_decay, \'center\': True, \'scale\': True, \'updates_collections\': None, \'is_training\': True, \'reuse\': None}\n            else:\n                train_phase = False\n                # normalizer_params = {\'decay\': batch_norm_decay, \'center\': True, \'scale\': True, \'updates_collections\': None, \'is_training\': False, \'reuse\': True}\n        else:\n            normalizer_fn = None\n            normalizer_params = None\n\n        x_deep = x0\n        for i in range(len(deep_layers)):\n            # if FLAGS.batch_norm:\n            #    deep_inputs = batch_norm_layer(deep_inputs, train_phase=train_phase, scope_bn=\'bn_%d\' %i)\n            # normalizer_params.update({\'scope\': \'bn_%d\' %i})\n            x_deep = tf.contrib.layers.fully_connected(inputs=x_deep, num_outputs=deep_layers[i],\n                                                       # normalizer_fn=normalizer_fn, normalizer_params=normalizer_params, \\\n                                                       weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg),\n                                                       scope=\'mlp%d\' % i)\n            if FLAGS.batch_norm:\n                x_deep = batch_norm_layer(x_deep, train_phase=train_phase,\n                                          scope_bn=\'bn_%d\' % i)  # \xe6\x94\xbe\xe5\x9c\xa8RELU\xe4\xb9\x8b\xe5\x90\x8e https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                x_deep = tf.nn.dropout(x_deep, keep_prob=dropout[\n                    i])  # Apply Dropout after all BN layers and set dropout=0.8(drop_ratio=0.2)\n                # x_deep = tf.layers.dropout(inputs=x_deep, rate=dropout[i], training=mode == tf.estimator.ModeKeys.TRAIN)\n\n    with tf.variable_scope(""DCN-out""):\n        x_stack = tf.concat([xl, x_deep], 1)  # None * ( F*K+ deep_layers[i])\n        y = tf.contrib.layers.fully_connected(inputs=x_stack, num_outputs=1, activation_fn=tf.identity,\n                                              weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg),\n                                              scope=\'out_layer\')\n        y = tf.reshape(y, shape=[-1])\n        pred = tf.sigmoid(y)\n\n    predictions = {""prob"": pred}\n    export_outputs = {\n        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: tf.estimator.export.PredictOutput(\n            predictions)}\n    # Provide an estimator spec for `ModeKeys.PREDICT`\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions=predictions,\n            export_outputs=export_outputs)\n\n    # ------bulid loss------\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=labels)) + \\\n           l2_reg * tf.nn.l2_loss(Cross_B) + l2_reg * tf.nn.l2_loss(Cross_W) + l2_reg * tf.nn.l2_loss(Feat_Emb)\n\n    # Provide an estimator spec for `ModeKeys.EVAL`\n    eval_metric_ops = {\n        ""auc"": tf.metrics.auc(labels, pred)\n    }\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions=predictions,\n            loss=loss,\n            eval_metric_ops=eval_metric_ops)\n\n    # ------bulid optimizer------\n    if FLAGS.optimizer == \'Adam\':\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8)\n    elif FLAGS.optimizer == \'Adagrad\':\n        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate, initial_accumulator_value=1e-8)\n    elif FLAGS.optimizer == \'Momentum\':\n        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95)\n    elif FLAGS.optimizer == \'ftrl\':\n        optimizer = tf.train.FtrlOptimizer(learning_rate)\n\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n\n    # Provide an estimator spec for `ModeKeys.TRAIN` modes\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions=predictions,\n            loss=loss,\n            train_op=train_op)\n\n    # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\n    # return tf.estimator.EstimatorSpec(\n    #        mode=mode,\n    #        loss=loss,\n    #        train_op=train_op,\n    #        predictions={""prob"": pred},\n    #        eval_metric_ops=eval_metric_ops)\n\n\ndef batch_norm_layer(x, train_phase, scope_bn):\n    bn_train = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True,\n                                            updates_collections=None, is_training=True, reuse=None, scope=scope_bn)\n    bn_infer = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True,\n                                            updates_collections=None, is_training=False, reuse=True, scope=scope_bn)\n    z = tf.cond(tf.cast(train_phase, tf.bool), lambda: bn_train, lambda: bn_infer)\n    return z\n\n\ndef set_dist_env():\n    if FLAGS.dist_mode == 1:  # \xe6\x9c\xac\xe5\x9c\xb0\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xb5\x8b\xe8\xaf\x95\xe6\xa8\xa1\xe5\xbc\x8f1 chief, 1 ps, 1 evaluator\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        chief_hosts = FLAGS.chief_hosts.split(\',\')\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # \xe6\x97\xa0worker\xe5\x8f\x82\xe6\x95\xb0\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index}\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n    elif FLAGS.dist_mode == 2:  # \xe9\x9b\x86\xe7\xbe\xa4\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xa8\xa1\xe5\xbc\x8f\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        worker_hosts = FLAGS.worker_hosts.split(\',\')\n        chief_hosts = worker_hosts[0:1]  # get first worker as chief\n        worker_hosts = worker_hosts[2:]  # the rest as worker\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'worker_host\', worker_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # use #worker=0 as chief\n        if job_name == ""worker"" and task_index == 0:\n            job_name = ""chief""\n        # use #worker=1 as evaluator\n        if job_name == ""worker"" and task_index == 1:\n            job_name = \'evaluator\'\n            task_index = 0\n        # the others as worker\n        if job_name == ""worker"" and task_index > 1:\n            task_index -= 2\n\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'worker\': worker_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index}\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n\n\ndef main(_):\n    # ------check Arguments------\n    if FLAGS.dt_dir == """":\n        FLAGS.dt_dir = (date.today() + timedelta(-1)).strftime(\'%Y%m%d\')\n    FLAGS.model_dir = FLAGS.model_dir + FLAGS.dt_dir\n    # FLAGS.data_dir  = FLAGS.data_dir + FLAGS.dt_dir\n\n    print(\'task_type \', FLAGS.task_type)\n    print(\'model_dir \', FLAGS.model_dir)\n    print(\'data_dir \', FLAGS.data_dir)\n    print(\'dt_dir \', FLAGS.dt_dir)\n    print(\'num_epochs \', FLAGS.num_epochs)\n    print(\'feature_size \', FLAGS.feature_size)\n    print(\'field_size \', FLAGS.field_size)\n    print(\'embedding_size \', FLAGS.embedding_size)\n    print(\'batch_size \', FLAGS.batch_size)\n    print(\'deep_layers \', FLAGS.deep_layers)\n    print(\'cross_layers \', FLAGS.cross_layers)\n    print(\'dropout \', FLAGS.dropout)\n    # print(\'loss_type \', FLAGS.loss_type)\n    print(\'optimizer \', FLAGS.optimizer)\n    print(\'learning_rate \', FLAGS.learning_rate)\n    print(\'batch_norm_decay \', FLAGS.batch_norm_decay)\n    print(\'batch_norm \', FLAGS.batch_norm)\n    print(\'l2_reg \', FLAGS.l2_reg)\n\n    # ------init Envs------\n    tr_files = glob.glob(""%s/tr*libsvm"" % FLAGS.data_dir)\n    random.shuffle(tr_files)\n    print(""tr_files:"", tr_files)\n    va_files = glob.glob(""%s/va*libsvm"" % FLAGS.data_dir)\n    print(""va_files:"", va_files)\n    te_files = glob.glob(""%s/te*libsvm"" % FLAGS.data_dir)\n    print(""te_files:"", te_files)\n\n    if FLAGS.clear_existing_model:\n        try:\n            shutil.rmtree(FLAGS.model_dir)\n        except Exception as e:\n            print(e, ""at clear_existing_model"")\n        else:\n            print(""existing model cleaned at %s"" % FLAGS.model_dir)\n\n    set_dist_env()\n\n    # ------bulid Tasks------\n    model_params = {\n        ""field_size"": FLAGS.field_size,\n        ""feature_size"": FLAGS.feature_size,\n        ""embedding_size"": FLAGS.embedding_size,\n        ""learning_rate"": FLAGS.learning_rate,\n        ""batch_norm_decay"": FLAGS.batch_norm_decay,\n        ""l2_reg"": FLAGS.l2_reg,\n        ""deep_layers"": FLAGS.deep_layers,\n        ""cross_layers"": FLAGS.cross_layers,\n        ""dropout"": FLAGS.dropout\n    }\n    config = tf.estimator.RunConfig().replace(\n        session_config=tf.ConfigProto(device_count={\'GPU\': 0, \'CPU\': FLAGS.num_threads}),\n        log_step_count_steps=FLAGS.log_steps, save_summary_steps=FLAGS.log_steps)\n    Estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=FLAGS.model_dir, params=model_params, config=config)\n\n    if FLAGS.task_type == \'train\':\n        train_spec = tf.estimator.TrainSpec(\n            input_fn=lambda: input_fn(tr_files, num_epochs=FLAGS.num_epochs, batch_size=FLAGS.batch_size))\n        eval_spec = tf.estimator.EvalSpec(\n            input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size), steps=None,\n            start_delay_secs=1000, throttle_secs=1200)\n        tf.estimator.train_and_evaluate(Estimator, train_spec, eval_spec)\n    elif FLAGS.task_type == \'eval\':\n        Estimator.evaluate(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size))\n    elif FLAGS.task_type == \'infer\':\n        preds = Estimator.predict(input_fn=lambda: input_fn(te_files, num_epochs=1, batch_size=FLAGS.batch_size),\n                                  predict_keys=""prob"")\n        with open(FLAGS.data_dir + ""/pred.txt"", ""w"") as fo:\n            for prob in preds:\n                fo.write(""%f\\n"" % (prob[\'prob\']))\n    elif FLAGS.task_type == \'export\':\n        # feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n        # feature_spec = {\n        #    \'feat_ids\': tf.FixedLenFeature(dtype=tf.int64, shape=[None, FLAGS.field_size]),\n        #    \'feat_vals\': tf.FixedLenFeature(dtype=tf.float32, shape=[None, FLAGS.field_size])\n        # }\n        # serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n        feature_spec = {\n            \'feat_ids\': tf.placeholder(dtype=tf.int64, shape=[None, FLAGS.field_size], name=\'feat_ids\'),\n            \'feat_vals\': tf.placeholder(dtype=tf.float32, shape=[None, FLAGS.field_size], name=\'feat_vals\')\n        }\n        serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_spec)\n        Estimator.export_savedmodel(FLAGS.servable_model_dir, serving_input_receiver_fn)\n\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
deep_ctr/Model_pipeline/DIN.py,122,"b'#!/usr/bin/env python\n#coding=utf-8\n""""""\nTensorFlow Implementation of <<Deep Interest Network for Click-Through Rate Prediction>>\nDataset desc: https://tianchi.aliyun.com/datalab/dataSet.html?dataId=408\n\nby lambdaji\n""""""\n#from __future__ import absolute_import\n#from __future__ import division\n#from __future__ import print_function\n\n#import argparse\nimport shutil\n#import sys\nimport os\nimport json\nimport glob\nfrom datetime import date, timedelta\nfrom time import time\n\nimport random\nimport tensorflow as tf\n\n#################### CMD Arguments ####################\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""dist_mode"", 0, ""distribuion mode {0-loacal, 1-single_dist, 2-multi_dist}"")\ntf.app.flags.DEFINE_string(""ps_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""worker_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""job_name"", \'\', ""One of \'ps\', \'worker\'"")\ntf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")\ntf.app.flags.DEFINE_integer(""num_threads"", 16, ""Number of threads"")\ntf.app.flags.DEFINE_integer(""feature_size"", 0, ""Number of features"")\ntf.app.flags.DEFINE_integer(""field_size"", 0, ""Number of fields"")\ntf.app.flags.DEFINE_integer(""embedding_size"", 32, ""Embedding size"")\ntf.app.flags.DEFINE_integer(""num_epochs"", 10, ""Number of epochs"")\ntf.app.flags.DEFINE_integer(""batch_size"", 64, ""Number of batch size"")\ntf.app.flags.DEFINE_integer(""log_steps"", 1000, ""save summary every steps"")\ntf.app.flags.DEFINE_float(""learning_rate"", 0.0005, ""learning rate"")\ntf.app.flags.DEFINE_float(""l2_reg"", 0.0001, ""L2 regularization"")\ntf.app.flags.DEFINE_string(""loss_type"", \'log_loss\', ""loss type {square_loss, log_loss}"")\ntf.app.flags.DEFINE_string(""optimizer"", \'Adam\', ""optimizer type {Adam, Adagrad, GD, Momentum}"")\ntf.app.flags.DEFINE_string(""deep_layers"", \'256,128,64\', ""deep layers"")\ntf.app.flags.DEFINE_string(""dropout"", \'0.5,0.5,0.5\', ""dropout rate"")\ntf.app.flags.DEFINE_boolean(""attention_pooling"", True, ""attention pooling"")\ntf.app.flags.DEFINE_string(""attention_layers"", \'256\', ""Attention Net mlp layers"")\ntf.app.flags.DEFINE_boolean(""batch_norm"", False, ""perform batch normaization (True or False)"")\ntf.app.flags.DEFINE_float(""batch_norm_decay"", 0.9, ""decay for the moving average(recommend trying decay=0.9)"")\ntf.app.flags.DEFINE_string(""data_dir"", \'\', ""data dir"")\ntf.app.flags.DEFINE_string(""dt_dir"", \'\', ""data dt partition"")\ntf.app.flags.DEFINE_string(""model_dir"", \'\', ""model check point dir"")\ntf.app.flags.DEFINE_string(""servable_model_dir"", \'\', ""export servable model for TensorFlow Serving"")\ntf.app.flags.DEFINE_string(""task_type"", \'train\', ""task type {train, infer, eval, export}"")\ntf.app.flags.DEFINE_boolean(""clear_existing_model"", False, ""clear existing model or not"")\n\n#1 1:0.5 2:0.03519 3:1 4:0.02567 7:0.03708 8:0.01705 9:0.06296 10:0.18185 11:0.02497 12:1 14:0.02565 15:0.03267 17:0.0247 18:0.03158 20:1 22:1 23:0.13169 24:0.02933 27:0.18159 31:0.0177 34:0.02888 38:1 51:1 63:1 132:1 164:1 236:1\ndef input_fn(filenames, batch_size=32, num_epochs=1, perform_shuffle=False):\n    print(\'Parsing\', filenames)\n    def _parse_fn(record):\n        features = {\n            ""y"": tf.FixedLenFeature([], tf.float32),\n            ""z"": tf.FixedLenFeature([], tf.float32),\n            ""feat_ids"": tf.FixedLenFeature([FLAGS.field_size], tf.int64),\n            #""feat_vals"": tf.FixedLenFeature([None], tf.float32),\n            ""u_catids"": tf.VarLenFeature(tf.int64),\n            ""u_catvals"": tf.VarLenFeature(tf.float32),\n            ""u_shopids"": tf.VarLenFeature(tf.int64),\n            ""u_shopvals"": tf.VarLenFeature(tf.float32),\n            ""u_intids"": tf.VarLenFeature(tf.int64),\n            ""u_intvals"": tf.VarLenFeature(tf.float32),\n            ""u_brandids"": tf.VarLenFeature(tf.int64),\n            ""u_brandvals"": tf.VarLenFeature(tf.float32),\n            ""a_catids"": tf.FixedLenFeature([], tf.int64),\n            ""a_shopids"": tf.FixedLenFeature([], tf.int64),\n            ""a_brandids"": tf.FixedLenFeature([], tf.int64),\n            ""a_intids"": tf.VarLenFeature(tf.int64)\n        }\n        parsed = tf.parse_single_example(record, features)\n        y = parsed.pop(\'y\')\n        #z = parsed[""z""]\n        return parsed, y\n\n    # Extract lines from input files using the Dataset API, can pass one filename or filename list\n    dataset = tf.data.TFRecordDataset(filenames).map(_parse_fn, num_parallel_calls=10).prefetch(500000)    # multi-thread pre-process then prefetch\n\n    # Randomizes input using a window of 256 elements (read into memory)\n    if perform_shuffle:\n        dataset = dataset.shuffle(buffer_size=256)\n\n    # epochs from blending together.\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.batch(batch_size) # Batch size to use\n    #dataset = dataset.padded_batch(batch_size, padded_shapes=?)   #\xe4\xb8\x8d\xe5\xae\x9a\xe9\x95\xbf\xe8\xa1\xa5\xe9\xbd\x90 \xe6\x88\xaa\xe8\x87\xb3TF1.8 Batching of padded sparse tensors is not currently supported\n\n    #return dataset.make_one_shot_iterator()\n    iterator = dataset.make_one_shot_iterator()\n    batch_features, batch_labels = iterator.get_next()\n    #return tf.reshape(batch_ids,shape=[-1,field_size]), tf.reshape(batch_vals,shape=[-1,field_size]), batch_labels\n    return batch_features, batch_labels\n\ndef model_fn(features, labels, mode, params):\n    """"""Bulid Model function f(x) for Estimator.""""""\n    #------hyperparameters----\n    field_size = params[""field_size""]\n    feature_size = params[""feature_size""]\n    embedding_size = params[""embedding_size""]\n    l2_reg = params[""l2_reg""]\n    learning_rate = params[""learning_rate""]\n    #batch_norm_decay = params[""batch_norm_decay""]\n    #optimizer = params[""optimizer""]\n    layers = map(int, params[""deep_layers""].split(\',\'))\n    dropout = map(float, params[""dropout""].split(\',\'))\n    attention_layers = map(int, params[""attention_layers""].split(\',\'))\n    common_dims = field_size*embedding_size\n\n    #------bulid weights------\n    Feat_Emb = tf.get_variable(name=\'embeddings\', shape=[feature_size, embedding_size], initializer=tf.glorot_normal_initializer())\n\n    #------build feaure-------\n    #{U-A-X-C\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe7\x89\xb9\xe6\xae\x8a\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81}\n    feat_ids    = features[\'feat_ids\']\n    #feat_vals   = features[\'feat_vals\']\n    #{User multi-hot}\n    u_catids    = features[\'u_catids\']\n    u_catvals   = features[\'u_catvals\']\n    u_shopids   = features[\'u_shopids\']\n    u_shopvals  = features[\'u_shopvals\']\n    u_intids    = features[\'u_intids\']\n    u_intvals   = features[\'u_intvals\']\n    u_brandids  = features[\'u_brandids\']\n    u_brandvals = features[\'u_brandvals\']\n    #{Ad}\n    a_catids    = features[\'a_catids\']\n    a_shopids   = features[\'a_shopids\']\n    a_brandids  = features[\'a_brandids\']\n    a_intids    = features[\'a_intids\']                                          # multi-hot\n    #{X multi-hot}\n    #x_intids    = features[\'x_intids\']\n    #x_intvals   = features[\'x_intvals\']\n\n    #------build f(x)------\n    with tf.variable_scope(""Embedding-layer""):\n        common_embs = tf.nn.embedding_lookup(Feat_Emb, feat_ids)                # None * F\' * K\n        #uac_emb     = tf.multiply(common_embs, feat_vals)\n        a_cat_emb   = tf.nn.embedding_lookup(Feat_Emb, a_catids)                # None * K\n        a_shop_emb  = tf.nn.embedding_lookup(Feat_Emb, a_shopids)\n        a_brand_emb = tf.nn.embedding_lookup(Feat_Emb, a_brandids)\n        a_int_emb   = tf.nn.embedding_lookup_sparse(Feat_Emb, sp_ids=a_intids, sp_weights=None, combiner=""sum"")\n\n    with tf.variable_scope(""Field-wise-Pooling-layer"", reuse=tf.AUTO_REUSE):\n        if FLAGS.attention_pooling:\n            def attention_unit(Feat_Emb, sp_ids, sp_weights, a_xx_emb):\n                dense_ids = tf.sparse_tensor_to_dense(sp_ids)\n                dense_wgt = tf.expand_dims(tf.sparse_tensor_to_dense(sp_weights), axis=-1)                          # None * P * 1\n                dense_emb = tf.nn.embedding_lookup(Feat_Emb, dense_ids)                                             # None * P * K\n                dense_emb = tf.multiply(dense_emb, dense_wgt)\n                dense_mask = tf.expand_dims(tf.cast(dense_ids > 0, tf.float32), axis=-1)                            # None * P * 1     0=padding >0\xe5\x8f\x96\xe9\x9d\x9epadding\n                #dense_mask = tf.sequence_mask(dense_ids, ?)                                                        # None * P\n                padded_dim = tf.shape(dense_ids)[1]\n                ub_ebm    = tf.reshape(dense_emb, shape=[-1, embedding_size])\n                ax_emb    = tf.reshape(tf.tile(a_xx_emb,[1, padded_dim]), shape=[-1, embedding_size])               # None * K --> (None * P) * K     \xe6\xb3\xa8\xe6\x84\x8f\xe8\xb7\x9fdense_emb reshape\xe9\xa1\xba\xe5\xba\x8f\xe4\xbf\x9d\xe6\x8c\x81\xe4\xb8\x80\xe8\x87\xb4\n                x_inputs  = tf.concat([ub_ebm, ub_ebm - ax_emb, ax_emb], axis=1) \t\t                            # (None * P) * 3K\n                for i in range(len(attention_layers)):\n                    x_inputs = tf.contrib.layers.fully_connected(inputs=x_inputs, num_outputs=layers[i], scope=\'att_fc%d\' % i)\n                    if FLAGS.batch_norm:\n                        x_inputs = batch_norm_layer(x_inputs, train_phase=train_phase, scope_bn=\'att_bn_%d\' %i)\n                    if mode == tf.estimator.ModeKeys.TRAIN:\n                        x_inputs = tf.nn.dropout(x_inputs, keep_prob=dropout[i])\n                att_wgt = tf.contrib.layers.fully_connected(inputs=x_inputs, num_outputs=1, activation_fn=tf.sigmoid, scope=\'att_out\')    #(None * P) * 1\n                att_wgt = tf.reshape(att_wgt, shape=[-1, padded_dim, 1])                                            # None * P * 1\n                wgt_emb = tf.multiply(dense_emb, att_wgt)                                                           # None * P * K\n                wgt_emb = tf.reduce_sum(tf.multiply(wgt_emb, dense_mask), 1) \t\t\t\t                        # None * K\n                return wgt_emb\n\n            u_cat_emb   = attention_unit(Feat_Emb, u_catids,   u_catvals,   a_cat_emb)\n            u_shop_emb  = attention_unit(Feat_Emb, u_shopids,  u_shopvals,  a_shop_emb)\n            u_brand_emb = attention_unit(Feat_Emb, u_brandids, u_brandvals, a_brand_emb)\n            u_int_emb   = attention_unit(Feat_Emb, u_intids,   u_intvals,   a_int_emb)\n        else:\n            u_cat_emb   = tf.nn.embedding_lookup_sparse(Feat_Emb, sp_ids=u_catids,  sp_weights=u_catvals,   combiner=""sum"")               # None * K\n            u_shop_emb  = tf.nn.embedding_lookup_sparse(Feat_Emb, sp_ids=u_shopids, sp_weights=u_shopvals,  combiner=""sum"")\n            u_brand_emb = tf.nn.embedding_lookup_sparse(Feat_Emb, sp_ids=u_brandids,sp_weights=u_brandvals, combiner=""sum"")\n            u_int_emb   = tf.nn.embedding_lookup_sparse(Feat_Emb, sp_ids=u_intids,  sp_weights=u_intvals,   combiner=""sum"")\n\n    with tf.variable_scope(""MLP-layer""):\n        if FLAGS.batch_norm:\n            #normalizer_fn = tf.contrib.layers.batch_norm\n            #normalizer_fn = tf.layers.batch_normalization\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                train_phase = True\n                #normalizer_params = {\'decay\': batch_norm_decay, \'center\': True, \'scale\': True, \'updates_collections\': None, \'is_training\': True, \'reuse\': None}\n            else:\n                train_phase = False\n                #normalizer_params = {\'decay\': batch_norm_decay, \'center\': True, \'scale\': True, \'updates_collections\': None, \'is_training\': False, \'reuse\': True}\n        else:\n            normalizer_fn = None\n            normalizer_params = None\n\n        x_deep = tf.concat([tf.reshape(common_embs,shape=[-1, common_dims]),u_cat_emb,u_shop_emb,u_brand_emb,u_int_emb,a_cat_emb,a_shop_emb,a_brand_emb,a_int_emb],axis=1) # None * (F*K)\n        for i in range(len(layers)):\n            #if FLAGS.batch_norm:\n            #    deep_inputs = batch_norm_layer(deep_inputs, train_phase=train_phase, scope_bn=\'bn_%d\' %i)\n                #normalizer_params.update({\'scope\': \'bn_%d\' %i})\n            x_deep = tf.contrib.layers.fully_connected(inputs=x_deep, num_outputs=layers[i], weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'mlp%d\' % i)\n            if FLAGS.batch_norm:\n                x_deep = batch_norm_layer(x_deep, train_phase=train_phase, scope_bn=\'bn_%d\' %i)   #\xe6\x94\xbe\xe5\x9c\xa8RELU\xe4\xb9\x8b\xe5\x90\x8e https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                x_deep = tf.nn.dropout(x_deep, keep_prob=dropout[i])                              #Apply Dropout after all BN layers and set dropout=0.8(drop_ratio=0.2)\n\n    with tf.variable_scope(""DIN-out""):\n        y_deep = tf.contrib.layers.fully_connected(inputs=x_deep, num_outputs=1, activation_fn=tf.identity, \\\n                weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'din_out\')\n        y = tf.reshape(y_deep,shape=[-1])\n        pred = tf.sigmoid(y)\n\n    predictions={""prob"": pred}\n    export_outputs = {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: tf.estimator.export.PredictOutput(predictions)}\n    # Provide an estimator spec for `ModeKeys.PREDICT`\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                export_outputs=export_outputs)\n\n    #------bulid loss------\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=labels)) + l2_reg * tf.nn.l2_loss(Feat_Emb)\n\n    # Provide an estimator spec for `ModeKeys.EVAL`\n    eval_metric_ops = {\n        ""auc"": tf.metrics.auc(labels, pred)\n    }\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                eval_metric_ops=eval_metric_ops)\n\n    #------bulid optimizer------\n    if FLAGS.optimizer == \'Adam\':\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8)\n    elif FLAGS.optimizer == \'Adagrad\':\n        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate, initial_accumulator_value=1e-8)\n    elif FLAGS.optimizer == \'Momentum\':\n        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95)\n    elif FLAGS.optimizer == \'ftrl\':\n        optimizer = tf.train.FtrlOptimizer(learning_rate)\n\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n\n    # Provide an estimator spec for `ModeKeys.TRAIN` modes\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                train_op=train_op)\n\ndef batch_norm_layer(x, train_phase, scope_bn):\n    bn_train = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True, updates_collections=None, is_training=True,  reuse=None, scope=scope_bn)\n    bn_infer = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True, updates_collections=None, is_training=False, reuse=True, scope=scope_bn)\n    z = tf.cond(tf.cast(train_phase, tf.bool), lambda: bn_train, lambda: bn_infer)\n    return z\n\ndef set_dist_env():\n    if FLAGS.dist_mode == 1:        # \xe6\x9c\xac\xe5\x9c\xb0\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xb5\x8b\xe8\xaf\x95\xe6\xa8\xa1\xe5\xbc\x8f1 chief, 1 ps, 1 evaluator\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        chief_hosts = FLAGS.chief_hosts.split(\',\')\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # \xe6\x97\xa0worker\xe5\x8f\x82\xe6\x95\xb0\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n    elif FLAGS.dist_mode == 2:      # \xe9\x9b\x86\xe7\xbe\xa4\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xa8\xa1\xe5\xbc\x8f\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        worker_hosts = FLAGS.worker_hosts.split(\',\')\n        chief_hosts = worker_hosts[0:1] # get first worker as chief\n        worker_hosts = worker_hosts[2:] # the rest as worker\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'worker_host\', worker_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # use #worker=0 as chief\n        if job_name == ""worker"" and task_index == 0:\n            job_name = ""chief""\n        # use #worker=1 as evaluator\n        if job_name == ""worker"" and task_index == 1:\n            job_name = \'evaluator\'\n            task_index = 0\n        # the others as worker\n        if job_name == ""worker"" and task_index > 1:\n            task_index -= 2\n\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'worker\': worker_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n\ndef main(_):\n    #------check Arguments------\n    if FLAGS.dt_dir == """":\n        FLAGS.dt_dir = (date.today() + timedelta(-1)).strftime(\'%Y%m%d\')\n    FLAGS.model_dir = FLAGS.model_dir + FLAGS.dt_dir\n    #FLAGS.data_dir  = FLAGS.data_dir + FLAGS.dt_dir\n\n    print(\'task_type \', FLAGS.task_type)\n    print(\'model_dir \', FLAGS.model_dir)\n    print(\'data_dir \', FLAGS.data_dir)\n    print(\'dt_dir \', FLAGS.dt_dir)\n    print(\'num_epochs \', FLAGS.num_epochs)\n    print(\'feature_size \', FLAGS.feature_size)\n    print(\'field_size \', FLAGS.field_size)\n    print(\'embedding_size \', FLAGS.embedding_size)\n    print(\'batch_size \', FLAGS.batch_size)\n    print(\'deep_layers \', FLAGS.deep_layers)\n    print(\'dropout \', FLAGS.dropout)\n    print(\'attention_pooling\', FLAGS.attention_pooling)\n    print(\'attention_layers\', FLAGS.attention_layers)\n    print(\'loss_type \', FLAGS.loss_type)\n    print(\'optimizer \', FLAGS.optimizer)\n    print(\'learning_rate \', FLAGS.learning_rate)\n    print(\'batch_norm_decay \', FLAGS.batch_norm_decay)\n    print(\'batch_norm \', FLAGS.batch_norm)\n    print(\'l2_reg \', FLAGS.l2_reg)\n\n    #------init Envs------\n    tr_files = glob.glob(""%s/tr/*tfrecord"" % FLAGS.data_dir)\n    random.shuffle(tr_files)\n    print(""tr_files:"", tr_files)\n    va_files = glob.glob(""%s/te/*tfrecord"" % FLAGS.data_dir)\n    print(""va_files:"", va_files)\n    te_files = glob.glob(""%s/te/*tfrecord"" % FLAGS.data_dir)\n    print(""te_files:"", te_files)\n\n    if FLAGS.clear_existing_model:\n        try:\n            shutil.rmtree(FLAGS.model_dir)\n        except Exception as e:\n            print(e, ""at clear_existing_model"")\n        else:\n            print(""existing model cleaned at %s"" % FLAGS.model_dir)\n\n    set_dist_env()\n\n    #------bulid Tasks------\n    model_params = {\n        ""field_size"": FLAGS.field_size,\n        ""feature_size"": FLAGS.feature_size,\n        ""embedding_size"": FLAGS.embedding_size,\n        ""learning_rate"": FLAGS.learning_rate,\n        ""batch_norm_decay"": FLAGS.batch_norm_decay,\n        ""l2_reg"": FLAGS.l2_reg,\n        ""deep_layers"": FLAGS.deep_layers,\n        ""dropout"": FLAGS.dropout,\n        ""attention_layers"": FLAGS.attention_layers\n    }\n    config = tf.estimator.RunConfig().replace(session_config = tf.ConfigProto(device_count={\'GPU\':0, \'CPU\':FLAGS.num_threads}),\n            log_step_count_steps=FLAGS.log_steps, save_summary_steps=FLAGS.log_steps)\n    Estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=FLAGS.model_dir, params=model_params, config=config)\n\n    if FLAGS.task_type == \'train\':\n        train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(tr_files, num_epochs=FLAGS.num_epochs, batch_size=FLAGS.batch_size))\n        eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size), steps=None, start_delay_secs=1000, throttle_secs=1200)\n        tf.estimator.train_and_evaluate(Estimator, train_spec, eval_spec)\n    elif FLAGS.task_type == \'eval\':\n        Estimator.evaluate(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size))\n    elif FLAGS.task_type == \'infer\':\n        preds = Estimator.predict(input_fn=lambda: input_fn(te_files, num_epochs=1, batch_size=FLAGS.batch_size), predict_keys=""prob"")\n        with open(FLAGS.data_dir+""/pred.txt"", ""w"") as fo:\n            for prob in preds:\n                fo.write(""%f\\n"" % (prob[\'prob\']))\n    elif FLAGS.task_type == \'export\':\n        #feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n        feature_spec = {\n            \'feat_ids\': tf.FixedLenFeature(dtype=tf.int64, shape=[None, FLAGS.field_size]),\n            \'feat_vals\': tf.FixedLenFeature(dtype=tf.float32, shape=[None, FLAGS.field_size])\n        }\n        serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n        #feature_spec = {\n        #    \'feat_ids\': tf.placeholder(dtype=tf.int64, shape=[None, FLAGS.field_size], name=\'feat_ids\'),\n        #    \'feat_vals\': tf.placeholder(dtype=tf.float32, shape=[None, FLAGS.field_size], name=\'feat_vals\')\n        #}\n        #serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_spec)\n        Estimator.export_savedmodel(FLAGS.servable_model_dir, serving_input_receiver_fn)\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
deep_ctr/Model_pipeline/DeepFM.py,110,"b'#!/usr/bin/env python\n#coding=utf-8\n""""""\nTensorFlow Implementation of <<DeepFM: A Factorization-Machine based Neural Network for CTR Prediction>> with the fellowing features\xef\xbc\x9a\n#1 Input pipline using Dataset high level API, Support parallel and prefetch reading\n#2 Train pipline using Coustom Estimator by rewriting model_fn\n#3 Support distincted training using TF_CONFIG\n#4 Support export_model for TensorFlow Serving\n\nby lambdaji\n""""""\n#from __future__ import absolute_import\n#from __future__ import division\n#from __future__ import print_function\n\n#import argparse\nimport shutil\n#import sys\nimport os\nimport json\nimport glob\nfrom datetime import date, timedelta\nfrom time import time\n#import gc\n#from multiprocessing import Process\n\n#import math\nimport random\n#import pandas as pd\n#import numpy as np\nimport tensorflow as tf\n\n#################### CMD Arguments ####################\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""dist_mode"", 0, ""distribuion mode {0-loacal, 1-single_dist, 2-multi_dist}"")\ntf.app.flags.DEFINE_string(""ps_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""worker_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""job_name"", \'\', ""One of \'ps\', \'worker\'"")\ntf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")\ntf.app.flags.DEFINE_integer(""num_threads"", 16, ""Number of threads"")\ntf.app.flags.DEFINE_integer(""feature_size"", 0, ""Number of features"")\ntf.app.flags.DEFINE_integer(""field_size"", 0, ""Number of fields"")\ntf.app.flags.DEFINE_integer(""embedding_size"", 32, ""Embedding size"")\ntf.app.flags.DEFINE_integer(""num_epochs"", 10, ""Number of epochs"")\ntf.app.flags.DEFINE_integer(""batch_size"", 64, ""Number of batch size"")\ntf.app.flags.DEFINE_integer(""log_steps"", 1000, ""save summary every steps"")\ntf.app.flags.DEFINE_float(""learning_rate"", 0.0005, ""learning rate"")\ntf.app.flags.DEFINE_float(""l2_reg"", 0.0001, ""L2 regularization"")\ntf.app.flags.DEFINE_string(""loss_type"", \'log_loss\', ""loss type {square_loss, log_loss}"")\ntf.app.flags.DEFINE_string(""optimizer"", \'Adam\', ""optimizer type {Adam, Adagrad, GD, Momentum}"")\ntf.app.flags.DEFINE_string(""deep_layers"", \'256,128,64\', ""deep layers"")\ntf.app.flags.DEFINE_string(""dropout"", \'0.5,0.5,0.5\', ""dropout rate"")\ntf.app.flags.DEFINE_boolean(""batch_norm"", False, ""perform batch normaization (True or False)"")\ntf.app.flags.DEFINE_float(""batch_norm_decay"", 0.9, ""decay for the moving average(recommend trying decay=0.9)"")\ntf.app.flags.DEFINE_string(""data_dir"", \'\', ""data dir"")\ntf.app.flags.DEFINE_string(""dt_dir"", \'\', ""data dt partition"")\ntf.app.flags.DEFINE_string(""model_dir"", \'\', ""model check point dir"")\ntf.app.flags.DEFINE_string(""servable_model_dir"", \'\', ""export servable model for TensorFlow Serving"")\ntf.app.flags.DEFINE_string(""task_type"", \'train\', ""task type {train, infer, eval, export}"")\ntf.app.flags.DEFINE_boolean(""clear_existing_model"", False, ""clear existing model or not"")\n\n#1 1:0.5 2:0.03519 3:1 4:0.02567 7:0.03708 8:0.01705 9:0.06296 10:0.18185 11:0.02497 12:1 14:0.02565 15:0.03267 17:0.0247 18:0.03158 20:1 22:1 23:0.13169 24:0.02933 27:0.18159 31:0.0177 34:0.02888 38:1 51:1 63:1 132:1 164:1 236:1\ndef input_fn(filenames, batch_size=32, num_epochs=1, perform_shuffle=False):\n    print(\'Parsing\', filenames)\n    def decode_libsvm(line):\n        #columns = tf.decode_csv(value, record_defaults=CSV_COLUMN_DEFAULTS)\n        #features = dict(zip(CSV_COLUMNS, columns))\n        #labels = features.pop(LABEL_COLUMN)\n        columns = tf.string_split([line], \' \')\n        labels = tf.string_to_number(columns.values[0], out_type=tf.float32)\n        splits = tf.string_split(columns.values[1:], \':\')\n        id_vals = tf.reshape(splits.values,splits.dense_shape)\n        feat_ids, feat_vals = tf.split(id_vals,num_or_size_splits=2,axis=1)\n        feat_ids = tf.string_to_number(feat_ids, out_type=tf.int32)\n        feat_vals = tf.string_to_number(feat_vals, out_type=tf.float32)\n        #feat_ids = tf.reshape(feat_ids,shape=[-1,FLAGS.field_size])\n        #for i in range(splits.dense_shape.eval()[0]):\n        #    feat_ids.append(tf.string_to_number(splits.values[2*i], out_type=tf.int32))\n        #    feat_vals.append(tf.string_to_number(splits.values[2*i+1]))\n        #return tf.reshape(feat_ids,shape=[-1,field_size]), tf.reshape(feat_vals,shape=[-1,field_size]), labels\n        return {""feat_ids"": feat_ids, ""feat_vals"": feat_vals}, labels\n\n    # Extract lines from input files using the Dataset API, can pass one filename or filename list\n    dataset = tf.data.TextLineDataset(filenames).map(decode_libsvm, num_parallel_calls=10).prefetch(500000)    # multi-thread pre-process then prefetch\n\n    # Randomizes input using a window of 256 elements (read into memory)\n    if perform_shuffle:\n        dataset = dataset.shuffle(buffer_size=256)\n\n    # epochs from blending together.\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.batch(batch_size) # Batch size to use\n\n    #return dataset.make_one_shot_iterator()\n    iterator = dataset.make_one_shot_iterator()\n    batch_features, batch_labels = iterator.get_next()\n    #return tf.reshape(batch_ids,shape=[-1,field_size]), tf.reshape(batch_vals,shape=[-1,field_size]), batch_labels\n    return batch_features, batch_labels\n\ndef model_fn(features, labels, mode, params):\n    """"""Bulid Model function f(x) for Estimator.""""""\n    #------hyperparameters----\n    field_size = params[""field_size""]\n    feature_size = params[""feature_size""]\n    embedding_size = params[""embedding_size""]\n    l2_reg = params[""l2_reg""]\n    learning_rate = params[""learning_rate""]\n    #batch_norm_decay = params[""batch_norm_decay""]\n    #optimizer = params[""optimizer""]\n    layers  = map(int, params[""deep_layers""].split(\',\'))\n    dropout = map(float, params[""dropout""].split(\',\'))\n\n    #------bulid weights------\n    FM_B = tf.get_variable(name=\'fm_bias\', shape=[1], initializer=tf.constant_initializer(0.0))\n    FM_W = tf.get_variable(name=\'fm_w\', shape=[feature_size], initializer=tf.glorot_normal_initializer())\n    FM_V = tf.get_variable(name=\'fm_v\', shape=[feature_size, embedding_size], initializer=tf.glorot_normal_initializer())\n\n    #------build feaure-------\n    feat_ids  = features[\'feat_ids\']\n    feat_ids = tf.reshape(feat_ids,shape=[-1,field_size])\n    feat_vals = features[\'feat_vals\']\n    feat_vals = tf.reshape(feat_vals,shape=[-1,field_size])\n\n    #------build f(x)------\n    with tf.variable_scope(""First-order""):\n        feat_wgts = tf.nn.embedding_lookup(FM_W, feat_ids)              # None * F * 1\n        y_w = tf.reduce_sum(tf.multiply(feat_wgts, feat_vals),1)\n\n    with tf.variable_scope(""Second-order""):\n        embeddings = tf.nn.embedding_lookup(FM_V, feat_ids)             # None * F * K\n        feat_vals = tf.reshape(feat_vals, shape=[-1, field_size, 1])\n        embeddings = tf.multiply(embeddings, feat_vals)                 #vij*xi\n        sum_square = tf.square(tf.reduce_sum(embeddings,1))\n        square_sum = tf.reduce_sum(tf.square(embeddings),1)\n        y_v = 0.5*tf.reduce_sum(tf.subtract(sum_square, square_sum),1)\t# None * 1\n\n    with tf.variable_scope(""Deep-part""):\n        if FLAGS.batch_norm:\n            #normalizer_fn = tf.contrib.layers.batch_norm\n            #normalizer_fn = tf.layers.batch_normalization\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                train_phase = True\n                #normalizer_params = {\'decay\': batch_norm_decay, \'center\': True, \'scale\': True, \'updates_collections\': None, \'is_training\': True, \'reuse\': None}\n            else:\n                train_phase = False\n                #normalizer_params = {\'decay\': batch_norm_decay, \'center\': True, \'scale\': True, \'updates_collections\': None, \'is_training\': False, \'reuse\': True}\n        else:\n            normalizer_fn = None\n            normalizer_params = None\n\n        deep_inputs = tf.reshape(embeddings,shape=[-1,field_size*embedding_size]) # None * (F*K)\n        for i in range(len(layers)):\n            #if FLAGS.batch_norm:\n            #    deep_inputs = batch_norm_layer(deep_inputs, train_phase=train_phase, scope_bn=\'bn_%d\' %i)\n                #normalizer_params.update({\'scope\': \'bn_%d\' %i})\n            deep_inputs = tf.contrib.layers.fully_connected(inputs=deep_inputs, num_outputs=layers[i], \\\n                #normalizer_fn=normalizer_fn, normalizer_params=normalizer_params, \\\n                weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'mlp%d\' % i)\n            if FLAGS.batch_norm:\n                deep_inputs = batch_norm_layer(deep_inputs, train_phase=train_phase, scope_bn=\'bn_%d\' %i)   #\xe6\x94\xbe\xe5\x9c\xa8RELU\xe4\xb9\x8b\xe5\x90\x8e https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                deep_inputs = tf.nn.dropout(deep_inputs, keep_prob=dropout[i])                              #Apply Dropout after all BN layers and set dropout=0.8(drop_ratio=0.2)\n                #deep_inputs = tf.layers.dropout(inputs=deep_inputs, rate=dropout[i], training=mode == tf.estimator.ModeKeys.TRAIN)\n\n        y_deep = tf.contrib.layers.fully_connected(inputs=deep_inputs, num_outputs=1, activation_fn=tf.identity, \\\n                weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'deep_out\')\n        y_d = tf.reshape(y_deep,shape=[-1])\n        #sig_wgts = tf.get_variable(name=\'sigmoid_weights\', shape=[layers[-1]], initializer=tf.glorot_normal_initializer())\n        #sig_bias = tf.get_variable(name=\'sigmoid_bias\', shape=[1], initializer=tf.constant_initializer(0.0))\n        #deep_out = tf.nn.xw_plus_b(deep_inputs,sig_wgts,sig_bias,name=\'deep_out\')\n\n    with tf.variable_scope(""DeepFM-out""):\n        #y_bias = FM_B * tf.ones_like(labels, dtype=tf.float32)  # None * 1  warning;\xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\x8d\xe8\x83\xbd\xe7\x94\xa8label\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe8\xb0\x83\xe7\x94\xa8predict/export\xe5\x87\xbd\xe6\x95\xb0\xe4\xbc\x9a\xe5\x87\xba\xe9\x94\x99\xef\xbc\x8ctrain/evaluate\xe6\xad\xa3\xe5\xb8\xb8\xef\xbc\x9b\xe5\x88\x9d\xe6\xad\xa5\xe5\x88\xa4\xe6\x96\xadestimator\xe5\x81\x9a\xe4\xba\x86\xe4\xbc\x98\xe5\x8c\x96\xef\xbc\x8c\xe7\x94\xa8\xe4\xb8\x8d\xe5\x88\xb0label\xe6\x97\xb6\xe4\xb8\x8d\xe4\xbc\xa0\n        y_bias = FM_B * tf.ones_like(y_d, dtype=tf.float32)      # None * 1\n        y = y_bias + y_w + y_v + y_d\n        pred = tf.sigmoid(y)\n\n    predictions={""prob"": pred}\n    export_outputs = {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: tf.estimator.export.PredictOutput(predictions)}\n    # Provide an estimator spec for `ModeKeys.PREDICT`\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                export_outputs=export_outputs)\n\n    #------bulid loss------\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=labels)) + \\\n        l2_reg * tf.nn.l2_loss(FM_W) + \\\n        l2_reg * tf.nn.l2_loss(FM_V)\n\n    # Provide an estimator spec for `ModeKeys.EVAL`\n    eval_metric_ops = {\n        ""auc"": tf.metrics.auc(labels, pred)\n    }\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                eval_metric_ops=eval_metric_ops)\n\n    #------bulid optimizer------\n    if FLAGS.optimizer == \'Adam\':\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8)\n    elif FLAGS.optimizer == \'Adagrad\':\n        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate, initial_accumulator_value=1e-8)\n    elif FLAGS.optimizer == \'Momentum\':\n        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95)\n    elif FLAGS.optimizer == \'ftrl\':\n        optimizer = tf.train.FtrlOptimizer(learning_rate)\n\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n\n    # Provide an estimator spec for `ModeKeys.TRAIN` modes\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                train_op=train_op)\n\n    # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\n    #return tf.estimator.EstimatorSpec(\n    #        mode=mode,\n    #        loss=loss,\n    #        train_op=train_op,\n    #        predictions={""prob"": pred},\n    #        eval_metric_ops=eval_metric_ops)\n\ndef batch_norm_layer(x, train_phase, scope_bn):\n    bn_train = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True, updates_collections=None, is_training=True,  reuse=None, scope=scope_bn)\n    bn_infer = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True, updates_collections=None, is_training=False, reuse=True, scope=scope_bn)\n    z = tf.cond(tf.cast(train_phase, tf.bool), lambda: bn_train, lambda: bn_infer)\n    return z\n\ndef set_dist_env():\n    if FLAGS.dist_mode == 1:        # \xe6\x9c\xac\xe5\x9c\xb0\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xb5\x8b\xe8\xaf\x95\xe6\xa8\xa1\xe5\xbc\x8f1 chief, 1 ps, 1 evaluator\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        chief_hosts = FLAGS.chief_hosts.split(\',\')\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # \xe6\x97\xa0worker\xe5\x8f\x82\xe6\x95\xb0\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n    elif FLAGS.dist_mode == 2:      # \xe9\x9b\x86\xe7\xbe\xa4\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xa8\xa1\xe5\xbc\x8f\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        worker_hosts = FLAGS.worker_hosts.split(\',\')\n        chief_hosts = worker_hosts[0:1] # get first worker as chief\n        worker_hosts = worker_hosts[2:] # the rest as worker\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'worker_host\', worker_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # use #worker=0 as chief\n        if job_name == ""worker"" and task_index == 0:\n            job_name = ""chief""\n        # use #worker=1 as evaluator\n        if job_name == ""worker"" and task_index == 1:\n            job_name = \'evaluator\'\n            task_index = 0\n        # the others as worker\n        if job_name == ""worker"" and task_index > 1:\n            task_index -= 2\n\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'worker\': worker_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n\ndef main(_):\n    #------check Arguments------\n    if FLAGS.dt_dir == """":\n        FLAGS.dt_dir = (date.today() + timedelta(-1)).strftime(\'%Y%m%d\')\n    FLAGS.model_dir = FLAGS.model_dir + FLAGS.dt_dir\n    #FLAGS.data_dir  = FLAGS.data_dir + FLAGS.dt_dir\n\n    print(\'task_type \', FLAGS.task_type)\n    print(\'model_dir \', FLAGS.model_dir)\n    print(\'data_dir \', FLAGS.data_dir)\n    print(\'dt_dir \', FLAGS.dt_dir)\n    print(\'num_epochs \', FLAGS.num_epochs)\n    print(\'feature_size \', FLAGS.feature_size)\n    print(\'field_size \', FLAGS.field_size)\n    print(\'embedding_size \', FLAGS.embedding_size)\n    print(\'batch_size \', FLAGS.batch_size)\n    print(\'deep_layers \', FLAGS.deep_layers)\n    print(\'dropout \', FLAGS.dropout)\n    print(\'loss_type \', FLAGS.loss_type)\n    print(\'optimizer \', FLAGS.optimizer)\n    print(\'learning_rate \', FLAGS.learning_rate)\n    print(\'batch_norm_decay \', FLAGS.batch_norm_decay)\n    print(\'batch_norm \', FLAGS.batch_norm)\n    print(\'l2_reg \', FLAGS.l2_reg)\n\n    #------init Envs------\n    tr_files = glob.glob(""%s/tr*libsvm"" % FLAGS.data_dir)\n    random.shuffle(tr_files)\n    print(""tr_files:"", tr_files)\n    va_files = glob.glob(""%s/va*libsvm"" % FLAGS.data_dir)\n    print(""va_files:"", va_files)\n    te_files = glob.glob(""%s/te*libsvm"" % FLAGS.data_dir)\n    print(""te_files:"", te_files)\n\n    if FLAGS.clear_existing_model:\n        try:\n            shutil.rmtree(FLAGS.model_dir)\n        except Exception as e:\n            print(e, ""at clear_existing_model"")\n        else:\n            print(""existing model cleaned at %s"" % FLAGS.model_dir)\n\n    set_dist_env()\n\n    #------bulid Tasks------\n    model_params = {\n        ""field_size"": FLAGS.field_size,\n        ""feature_size"": FLAGS.feature_size,\n        ""embedding_size"": FLAGS.embedding_size,\n        ""learning_rate"": FLAGS.learning_rate,\n        ""batch_norm_decay"": FLAGS.batch_norm_decay,\n        ""l2_reg"": FLAGS.l2_reg,\n        ""deep_layers"": FLAGS.deep_layers,\n        ""dropout"": FLAGS.dropout\n    }\n    config = tf.estimator.RunConfig().replace(session_config = tf.ConfigProto(device_count={\'GPU\':0, \'CPU\':FLAGS.num_threads}),\n            log_step_count_steps=FLAGS.log_steps, save_summary_steps=FLAGS.log_steps)\n    DeepFM = tf.estimator.Estimator(model_fn=model_fn, model_dir=FLAGS.model_dir, params=model_params, config=config)\n\n    if FLAGS.task_type == \'train\':\n        train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(tr_files, num_epochs=FLAGS.num_epochs, batch_size=FLAGS.batch_size))\n        eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size), steps=None, start_delay_secs=1000, throttle_secs=1200)\n        tf.estimator.train_and_evaluate(DeepFM, train_spec, eval_spec)\n    elif FLAGS.task_type == \'eval\':\n        DeepFM.evaluate(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size))\n    elif FLAGS.task_type == \'infer\':\n        preds = DeepFM.predict(input_fn=lambda: input_fn(te_files, num_epochs=1, batch_size=FLAGS.batch_size), predict_keys=""prob"")\n        with open(FLAGS.data_dir+""/pred.txt"", ""w"") as fo:\n            for prob in preds:\n                fo.write(""%f\\n"" % (prob[\'prob\']))\n    elif FLAGS.task_type == \'export\':\n        #feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n        #feature_spec = {\n        #    \'feat_ids\': tf.FixedLenFeature(dtype=tf.int64, shape=[None, FLAGS.field_size]),\n        #    \'feat_vals\': tf.FixedLenFeature(dtype=tf.float32, shape=[None, FLAGS.field_size])\n        #}\n        #serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n        feature_spec = {\n            \'feat_ids\': tf.placeholder(dtype=tf.int64, shape=[None, FLAGS.field_size], name=\'feat_ids\'),\n            \'feat_vals\': tf.placeholder(dtype=tf.float32, shape=[None, FLAGS.field_size], name=\'feat_vals\')\n        }\n        serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_spec)\n        DeepFM.export_savedmodel(FLAGS.servable_model_dir, serving_input_receiver_fn)\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
deep_ctr/Model_pipeline/DeepMVM.py,116,"b'#!/usr/bin/env python\n#coding=utf-8\n""""""\nTensorFlow Implementation of Deep & Multi-view Machines with the fellowing features\xef\xbc\x9a\n#1 Input pipline using Dataset high level API, Support parallel and prefetch reading\n#2 Train pipline using Coustom Estimator by rewriting model_fn\n#3 Support distincted training using TF_CONFIG\n#4 Support export_model for TensorFlow Serving\n\nby lambdaji\n""""""\n#from __future__ import absolute_import\n#from __future__ import division\n#from __future__ import print_function\n\n#import argparse\nimport shutil\n#import sys\nimport os\nimport json\nimport glob\nfrom datetime import date, timedelta\nfrom time import time\n#import gc\n#from multiprocessing import Process\n\n#import math\nimport random\n#import pandas as pd\n#import numpy as np\nimport tensorflow as tf\n\n#################### CMD Arguments ####################\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""dist_mode"", 0, ""distribuion mode {0-loacal, 1-single_dist, 2-multi_dist}"")\ntf.app.flags.DEFINE_string(""ps_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""worker_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""job_name"", \'\', ""One of \'ps\', \'worker\'"")\ntf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")\ntf.app.flags.DEFINE_integer(""num_threads"", 16, ""Number of threads"")\ntf.app.flags.DEFINE_integer(""feature_size"", 0, ""Number of features"")\ntf.app.flags.DEFINE_integer(""field_size"", 0, ""Number of fields"")\ntf.app.flags.DEFINE_integer(""embedding_size"", 32, ""Embedding size"")\ntf.app.flags.DEFINE_integer(""num_epochs"", 10, ""Number of epochs"")\ntf.app.flags.DEFINE_integer(""batch_size"", 64, ""Number of batch size"")\ntf.app.flags.DEFINE_integer(""log_steps"", 1000, ""save summary every steps"")\ntf.app.flags.DEFINE_float(""learning_rate"", 0.0005, ""learning rate"")\ntf.app.flags.DEFINE_float(""l2_reg"", 0.0001, ""L2 regularization"")\ntf.app.flags.DEFINE_string(""loss_type"", \'log_loss\', ""loss type {square_loss, log_loss}"")\ntf.app.flags.DEFINE_string(""optimizer"", \'Adam\', ""optimizer type {Adam, Adagrad, GD, Momentum}"")\ntf.app.flags.DEFINE_string(""deep_layers"", \'256,128,64\', ""deep layers"")\ntf.app.flags.DEFINE_string(""dropout"", \'0.5,0.5,0.5\', ""dropout rate"")\ntf.app.flags.DEFINE_boolean(""batch_norm"", False, ""perform batch normaization (True or False)"")\ntf.app.flags.DEFINE_float(""batch_norm_decay"", 0.9, ""decay for the moving average(recommend trying decay=0.9)"")\ntf.app.flags.DEFINE_string(""data_dir"", \'\', ""data dir"")\ntf.app.flags.DEFINE_string(""dt_dir"", \'\', ""data dt partition"")\ntf.app.flags.DEFINE_string(""model_dir"", \'\', ""model check point dir"")\ntf.app.flags.DEFINE_string(""servable_model_dir"", \'\', ""export servable model for TensorFlow Serving"")\ntf.app.flags.DEFINE_string(""task_type"", \'train\', ""task type {train, infer, eval, export}"")\ntf.app.flags.DEFINE_boolean(""clear_existing_model"", False, ""clear existing model or not"")\n\n#1 1:0.5 2:0.03519 3:1 4:0.02567 7:0.03708 8:0.01705 9:0.06296 10:0.18185 11:0.02497 12:1 14:0.02565 15:0.03267 17:0.0247 18:0.03158 20:1 22:1 23:0.13169 24:0.02933 27:0.18159 31:0.0177 34:0.02888 38:1 51:1 63:1 132:1 164:1 236:1\ndef input_fn(filenames, batch_size=32, num_epochs=1, perform_shuffle=False):\n    print(\'Parsing\', filenames)\n    def decode_libsvm(line):\n        #columns = tf.decode_csv(value, record_defaults=CSV_COLUMN_DEFAULTS)\n        #features = dict(zip(CSV_COLUMNS, columns))\n        #labels = features.pop(LABEL_COLUMN)\n        columns = tf.string_split([line], \' \')\n        labels = tf.string_to_number(columns.values[0], out_type=tf.float32)\n        splits = tf.string_split(columns.values[1:], \':\')\n        id_vals = tf.reshape(splits.values,splits.dense_shape)\n        feat_ids, feat_vals = tf.split(id_vals,num_or_size_splits=2,axis=1)\n        feat_ids = tf.string_to_number(feat_ids, out_type=tf.int32)\n        feat_vals = tf.string_to_number(feat_vals, out_type=tf.float32)\n        #feat_ids = tf.reshape(feat_ids,shape=[-1,FLAGS.field_size])\n        #for i in range(splits.dense_shape.eval()[0]):\n        #    feat_ids.append(tf.string_to_number(splits.values[2*i], out_type=tf.int32))\n        #    feat_vals.append(tf.string_to_number(splits.values[2*i+1]))\n        #return tf.reshape(feat_ids,shape=[-1,field_size]), tf.reshape(feat_vals,shape=[-1,field_size]), labels\n        return {""feat_ids"": feat_ids, ""feat_vals"": feat_vals}, labels\n\n    # Extract lines from input files using the Dataset API, can pass one filename or filename list\n    dataset = tf.data.TextLineDataset(filenames).map(decode_libsvm, num_parallel_calls=10).prefetch(500000)    # multi-thread pre-process then prefetch\n\n    # Randomizes input using a window of 256 elements (read into memory)\n    if perform_shuffle:\n        dataset = dataset.shuffle(buffer_size=256)\n\n    # epochs from blending together.\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.batch(batch_size) # Batch size to use\n\n    #return dataset.make_one_shot_iterator()\n    iterator = dataset.make_one_shot_iterator()\n    batch_features, batch_labels = iterator.get_next()\n    #return tf.reshape(batch_ids,shape=[-1,field_size]), tf.reshape(batch_vals,shape=[-1,field_size]), batch_labels\n    return batch_features, batch_labels\n\ndef model_fn(features, labels, mode, params):\n    """"""Bulid Model function f(x) for Estimator.""""""\n    #------hyperparameters----\n    field_size = params[""field_size""]\n    feature_size = params[""feature_size""]\n    embedding_size = params[""embedding_size""]\n    l2_reg = params[""l2_reg""]\n    learning_rate = params[""learning_rate""]\n    #batch_norm_decay = params[""batch_norm_decay""]\n    #optimizer = params[""optimizer""]\n    layers  = map(int, params[""deep_layers""].split(\',\'))\n    dropout = map(float, params[""dropout""].split(\',\'))\n\n    #------bulid weights------\n    #FM_B = tf.get_variable(name=\'fm_bias\', shape=[1], initializer=tf.constant_initializer(0.0))\n    #FM_W = tf.get_variable(name=\'fm_w\', shape=[feature_size], initializer=tf.glorot_normal_initializer())\n    #FM_V = tf.get_variable(name=\'fm_v\', shape=[feature_size, embedding_size], initializer=tf.glorot_normal_initializer())\n    MVM_W = tf.get_variable(name=\'mvm_w\', shape=[feature_size, embedding_size], initializer=tf.glorot_normal_initializer())\n    MVM_B = tf.get_variable(name=\'mvm_b\', shape=[field_size, embedding_size], initializer=tf.glorot_normal_initializer())\n\n    #------build feaure-------\n    feat_ids  = features[\'feat_ids\']\n    feat_ids = tf.reshape(feat_ids,shape=[-1,field_size])\n    feat_vals = features[\'feat_vals\']\n    feat_vals = tf.reshape(feat_vals,shape=[-1,field_size])\n\n    #------build f(x)------\n    #with tf.variable_scope(""First-order""):\n    #    feat_wgts = tf.nn.embedding_lookup(FM_W, feat_ids) # None * F * 1\n    #    y_w = tf.reduce_sum(tf.multiply(feat_wgts, feat_vals),1)\n\n    #with tf.variable_scope(""Second-order""):\n    #    embeddings = tf.nn.embedding_lookup(FM_V, feat_ids) # None * F * K\n    #    feat_vals = tf.reshape(feat_vals, shape=[-1, field_size, 1])\n    #    embeddings = tf.multiply(embeddings, feat_vals) #vij*xi\n    #    sum_square = tf.square(tf.reduce_sum(embeddings,1))\n    #    square_sum = tf.reduce_sum(tf.square(embeddings),1)\n    #    y_v = 0.5*tf.reduce_sum(tf.subtract(sum_square, square_sum),1)\t# None * 1\n\n    with tf.variable_scope(""Embedding-layer""):\n        embeddings = tf.nn.embedding_lookup(MVM_W, feat_ids) \t\t    # None * F * K\n        feat_vals = tf.reshape(feat_vals, shape=[-1, field_size, 1])\n        embeddings = tf.multiply(embeddings, feat_vals) \t\t\t\t# None * F * K\n\n    with tf.variable_scope(""MVM-part""):\n        all_order  = tf.add(embeddings, MVM_B)\n        x_mvm   = all_order[:,0,:]                                      # None * 1 * K\n        for i in range(1, field_size):\n            x_mvm = tf.multiply(x_mvm, all_order[:,i,:])\n\n        x_mvm   =  tf.reshape(x_mvm, shape=[-1, embedding_size])        # None * K\n\n    with tf.variable_scope(""Deep-part""):\n        if FLAGS.batch_norm:\n            #normalizer_fn = tf.contrib.layers.batch_norm\n            #normalizer_fn = tf.layers.batch_normalization\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                train_phase = True\n                #normalizer_params = {\'decay\': batch_norm_decay, \'center\': True, \'scale\': True, \'updates_collections\': None, \'is_training\': True, \'reuse\': None}\n            else:\n                train_phase = False\n                #normalizer_params = {\'decay\': batch_norm_decay, \'center\': True, \'scale\': True, \'updates_collections\': None, \'is_training\': False, \'reuse\': True}\n        else:\n            normalizer_fn = None\n            normalizer_params = None\n\n        x_deep = tf.reshape(embeddings,shape=[-1,field_size*embedding_size]) # None * (F*K)\n        for i in range(len(layers)):\n            #if FLAGS.batch_norm:\n            #    deep_inputs = batch_norm_layer(deep_inputs, train_phase=train_phase, scope_bn=\'bn_%d\' %i)\n                #normalizer_params.update({\'scope\': \'bn_%d\' %i})\n            x_deep = tf.contrib.layers.fully_connected(inputs=x_deep, num_outputs=layers[i], \\\n                #normalizer_fn=normalizer_fn, normalizer_params=normalizer_params, \\\n                weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'mlp%d\' % i)\n            if FLAGS.batch_norm:\n                x_deep = batch_norm_layer(x_deep, train_phase=train_phase, scope_bn=\'bn_%d\' %i)   #\xe6\x94\xbe\xe5\x9c\xa8RELU\xe4\xb9\x8b\xe5\x90\x8e https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                x_deep = tf.nn.dropout(x_deep, keep_prob=dropout[i])                              #Apply Dropout after all BN layers and set dropout=0.8(drop_ratio=0.2)\n                #x_deep = tf.layers.dropout(inputs=x_deep, rate=dropout[i], training=mode == tf.estimator.ModeKeys.TRAIN)\n\n    with tf.variable_scope(""DeepMVM-out""):\n        x_stack = tf.concat([x_mvm, x_deep], axis=1)\t# None * ( F*K+ deep_layers[i])\n        y_deep = tf.contrib.layers.fully_connected(inputs=x_stack, num_outputs=1, activation_fn=tf.identity, \\\n                weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'deep_out\')\n        y = tf.reshape(y_deep,shape=[-1])\n        pred = tf.sigmoid(y)\n\n    predictions={""prob"": pred}\n    export_outputs = {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: tf.estimator.export.PredictOutput(predictions)}\n    # Provide an estimator spec for `ModeKeys.PREDICT`\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                export_outputs=export_outputs)\n\n    #------bulid loss------\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=labels)) + \\\n        l2_reg * tf.nn.l2_loss(MVM_W) + \\\n        l2_reg * tf.nn.l2_loss(MVM_B)\n\n    # Provide an estimator spec for `ModeKeys.EVAL`\n    eval_metric_ops = {\n        ""auc"": tf.metrics.auc(labels, pred)\n    }\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                eval_metric_ops=eval_metric_ops)\n\n    #------bulid optimizer------\n    if FLAGS.optimizer == \'Adam\':\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8)\n    elif FLAGS.optimizer == \'Adagrad\':\n        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate, initial_accumulator_value=1e-8)\n    elif FLAGS.optimizer == \'Momentum\':\n        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95)\n    elif FLAGS.optimizer == \'ftrl\':\n        optimizer = tf.train.FtrlOptimizer(learning_rate)\n\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n\n    # Provide an estimator spec for `ModeKeys.TRAIN` modes\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                train_op=train_op)\n\n    # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\n    #return tf.estimator.EstimatorSpec(\n    #        mode=mode,\n    #        loss=loss,\n    #        train_op=train_op,\n    #        predictions={""prob"": pred},\n    #        eval_metric_ops=eval_metric_ops)\n\ndef batch_norm_layer(x, train_phase, scope_bn):\n    bn_train = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True, updates_collections=None, is_training=True,  reuse=None, scope=scope_bn)\n    bn_infer = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True, updates_collections=None, is_training=False, reuse=True, scope=scope_bn)\n    z = tf.cond(tf.cast(train_phase, tf.bool), lambda: bn_train, lambda: bn_infer)\n    return z\n\ndef set_dist_env():\n    if FLAGS.dist_mode == 1:        # \xe6\x9c\xac\xe5\x9c\xb0\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xb5\x8b\xe8\xaf\x95\xe6\xa8\xa1\xe5\xbc\x8f1 chief, 1 ps, 1 evaluator\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        chief_hosts = FLAGS.chief_hosts.split(\',\')\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # \xe6\x97\xa0worker\xe5\x8f\x82\xe6\x95\xb0\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n    elif FLAGS.dist_mode == 2:      # \xe9\x9b\x86\xe7\xbe\xa4\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xa8\xa1\xe5\xbc\x8f\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        worker_hosts = FLAGS.worker_hosts.split(\',\')\n        chief_hosts = worker_hosts[0:1] # get first worker as chief\n        worker_hosts = worker_hosts[2:] # the rest as worker\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'worker_host\', worker_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # use #worker=0 as chief\n        if job_name == ""worker"" and task_index == 0:\n            job_name = ""chief""\n        # use #worker=1 as evaluator\n        if job_name == ""worker"" and task_index == 1:\n            job_name = \'evaluator\'\n            task_index = 0\n        # the others as worker\n        if job_name == ""worker"" and task_index > 1:\n            task_index -= 2\n\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'worker\': worker_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n\ndef main(_):\n    #------check Arguments------\n    if FLAGS.dt_dir == """":\n        FLAGS.dt_dir = (date.today() + timedelta(-1)).strftime(\'%Y%m%d\')\n    FLAGS.model_dir = FLAGS.model_dir + FLAGS.dt_dir\n    #FLAGS.data_dir  = FLAGS.data_dir + FLAGS.dt_dir\n\n    print(\'task_type \', FLAGS.task_type)\n    print(\'model_dir \', FLAGS.model_dir)\n    print(\'data_dir \', FLAGS.data_dir)\n    print(\'dt_dir \', FLAGS.dt_dir)\n    print(\'num_epochs \', FLAGS.num_epochs)\n    print(\'feature_size \', FLAGS.feature_size)\n    print(\'field_size \', FLAGS.field_size)\n    print(\'embedding_size \', FLAGS.embedding_size)\n    print(\'batch_size \', FLAGS.batch_size)\n    print(\'deep_layers \', FLAGS.deep_layers)\n    print(\'dropout \', FLAGS.dropout)\n    print(\'loss_type \', FLAGS.loss_type)\n    print(\'optimizer \', FLAGS.optimizer)\n    print(\'learning_rate \', FLAGS.learning_rate)\n    print(\'batch_norm_decay \', FLAGS.batch_norm_decay)\n    print(\'batch_norm \', FLAGS.batch_norm)\n    print(\'l2_reg \', FLAGS.l2_reg)\n\n    #------init Envs------\n    tr_files = glob.glob(""%s/tr*libsvm"" % FLAGS.data_dir)\n    random.shuffle(tr_files)\n    print(""tr_files:"", tr_files)\n    va_files = glob.glob(""%s/va*libsvm"" % FLAGS.data_dir)\n    print(""va_files:"", va_files)\n    te_files = glob.glob(""%s/te*libsvm"" % FLAGS.data_dir)\n    print(""te_files:"", te_files)\n\n    if FLAGS.clear_existing_model:\n        try:\n            shutil.rmtree(FLAGS.model_dir)\n        except Exception as e:\n            print(e, ""at clear_existing_model"")\n        else:\n            print(""existing model cleaned at %s"" % FLAGS.model_dir)\n\n    set_dist_env()\n\n    model_params = {\n        ""field_size"": FLAGS.field_size,\n        ""feature_size"": FLAGS.feature_size,\n        ""embedding_size"": FLAGS.embedding_size,\n        ""learning_rate"": FLAGS.learning_rate,\n        ""batch_norm_decay"": FLAGS.batch_norm_decay,\n        ""l2_reg"": FLAGS.l2_reg,\n        ""deep_layers"": FLAGS.deep_layers,\n        ""dropout"": FLAGS.dropout\n    }\n    config = tf.estimator.RunConfig().replace(session_config = tf.ConfigProto(device_count={\'GPU\':0, \'CPU\':FLAGS.num_threads}),\n            log_step_count_steps=FLAGS.log_steps, save_summary_steps=FLAGS.log_steps)\n    Estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=FLAGS.model_dir, params=model_params, config=config)\n\n    if FLAGS.task_type == \'train\':\n        train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(tr_files, num_epochs=FLAGS.num_epochs, batch_size=FLAGS.batch_size))\n        eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size), steps=None, start_delay_secs=1000, throttle_secs=1200)\n        tf.estimator.train_and_evaluate(Estimator, train_spec, eval_spec)\n    elif FLAGS.task_type == \'eval\':\n        Estimator.evaluate(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size))\n    elif FLAGS.task_type == \'infer\':\n        preds = Estimator.predict(input_fn=lambda: input_fn(te_files, num_epochs=1, batch_size=FLAGS.batch_size), predict_keys=""prob"")\n        with open(FLAGS.data_dir+""/pred.txt"", ""w"") as fo:\n            for prob in preds:\n                fo.write(""%f\\n"" % (prob[\'prob\']))\n    elif FLAGS.task_type == \'export\':\n        #feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n        #feature_spec = {\n        #    \'feat_ids\': tf.FixedLenFeature(dtype=tf.int64, shape=[None, FLAGS.field_size]),\n        #    \'feat_vals\': tf.FixedLenFeature(dtype=tf.float32, shape=[None, FLAGS.field_size])\n        #}\n        #serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n        feature_spec = {\n            \'feat_ids\': tf.placeholder(dtype=tf.int64, shape=[None, FLAGS.field_size], name=\'feat_ids\'),\n            \'feat_vals\': tf.placeholder(dtype=tf.float32, shape=[None, FLAGS.field_size], name=\'feat_vals\')\n        }\n        serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_spec)\n        Estimator.export_savedmodel(FLAGS.servable_model_dir, serving_input_receiver_fn)\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
deep_ctr/Model_pipeline/NFM.py,101,"b'#!/usr/bin/env python\n#coding=utf-8\n""""""\nTensorFlow Implementation of <<Neural Factorization Machines for Sparse Predictive Analytics>> with the fellowing features\xef\xbc\x9a\n#1 Input pipline using Dataset high level API, Support parallel and prefetch reading\n#2 Train pipline using Coustom Estimator by rewriting model_fn\n#3 Support distincted training by TF_CONFIG\n#4 Support export model for TensorFlow Serving\n\nby lambdaji\n""""""\n#from __future__ import absolute_import\n#from __future__ import division\n#from __future__ import print_function\n\n#import argparse\nimport shutil\n#import sys\nimport os\nimport json\nimport glob\nfrom datetime import date, timedelta\nfrom time import time\n#import gc\n#from multiprocessing import Process\n\n#import math\nimport random\n#import pandas as pd\n#import numpy as np\nimport tensorflow as tf\n\n#################### CMD Arguments ####################\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""dist_mode"", 0, ""distribuion mode {0-loacal, 1-single_dist, 2-multi_dist}"")\ntf.app.flags.DEFINE_string(""ps_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""worker_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""job_name"", \'\', ""One of \'ps\', \'worker\'"")\ntf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")\ntf.app.flags.DEFINE_integer(""num_threads"", 16, ""Number of threads"")\ntf.app.flags.DEFINE_integer(""feature_size"", 0, ""Number of features"")\ntf.app.flags.DEFINE_integer(""field_size"", 0, ""Number of fields"")\ntf.app.flags.DEFINE_integer(""embedding_size"", 64, ""Embedding size"")\ntf.app.flags.DEFINE_integer(""num_epochs"", 10, ""Number of epochs"")\ntf.app.flags.DEFINE_integer(""batch_size"", 128, ""Number of batch size"")\ntf.app.flags.DEFINE_integer(""log_steps"", 1000, ""save summary every steps"")\ntf.app.flags.DEFINE_float(""learning_rate"", 0.05, ""learning rate"")\ntf.app.flags.DEFINE_float(""l2_reg"", 0.001, ""L2 regularization"")\ntf.app.flags.DEFINE_string(""loss_type"", \'log_loss\', ""loss type {square_loss, log_loss}"")\ntf.app.flags.DEFINE_string(""optimizer"", \'Adam\', ""optimizer type {Adam, Adagrad, GD, Momentum}"")\ntf.app.flags.DEFINE_string(""deep_layers"", \'128,64\', ""deep layers"")\ntf.app.flags.DEFINE_string(""dropout"", \'0.5,0.8,0.8\', ""dropout rate"")\ntf.app.flags.DEFINE_boolean(""batch_norm"", False, ""perform batch normaization (True or False)"")\ntf.app.flags.DEFINE_float(""batch_norm_decay"", 0.9, ""decay for the moving average(recommend trying decay=0.9)"")\ntf.app.flags.DEFINE_string(""data_dir"", \'\', ""data dir"")\ntf.app.flags.DEFINE_string(""dt_dir"", \'\', ""data dt partition"")\ntf.app.flags.DEFINE_string(""model_dir"", \'\', ""model check point dir"")\ntf.app.flags.DEFINE_string(""servable_model_dir"", \'\', ""export servable model for TensorFlow Serving"")\ntf.app.flags.DEFINE_string(""task_type"", \'train\', ""task type {train, infer, eval, export}"")\ntf.app.flags.DEFINE_boolean(""clear_existing_model"", False, ""clear existing model or not"")\n\n#1 1:0.5 2:0.03519 3:1 4:0.02567 7:0.03708 8:0.01705 9:0.06296 10:0.18185 11:0.02497 12:1 14:0.02565 15:0.03267 17:0.0247 18:0.03158 20:1 22:1 23:0.13169 24:0.02933 27:0.18159 31:0.0177 34:0.02888 38:1 51:1 63:1 132:1 164:1 236:1\ndef input_fn(filenames, batch_size=32, num_epochs=1, perform_shuffle=False):\n    print(\'Parsing\', filenames)\n    def decode_libsvm(line):\n        #columns = tf.decode_csv(value, record_defaults=CSV_COLUMN_DEFAULTS)\n        #features = dict(zip(CSV_COLUMNS, columns))\n        #labels = features.pop(LABEL_COLUMN)\n        columns = tf.string_split([line], \' \')\n        labels = tf.string_to_number(columns.values[0], out_type=tf.float32)\n        splits = tf.string_split(columns.values[1:], \':\')\n        id_vals = tf.reshape(splits.values,splits.dense_shape)\n        feat_ids, feat_vals = tf.split(id_vals,num_or_size_splits=2,axis=1)\n        feat_ids = tf.string_to_number(feat_ids, out_type=tf.int32)\n        feat_vals = tf.string_to_number(feat_vals, out_type=tf.float32)\n        return {""feat_ids"": feat_ids, ""feat_vals"": feat_vals}, labels\n\n    # Extract lines from input files using the Dataset API, can pass one filename or filename list\n    dataset = tf.data.TextLineDataset(filenames).map(decode_libsvm, num_parallel_calls=10).prefetch(500000)    # multi-thread pre-process then prefetch\n\n    # Randomizes input using a window of 256 elements (read into memory)\n    if perform_shuffle:\n        dataset = dataset.shuffle(buffer_size=256)\n\n    # epochs from blending together.\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.batch(batch_size) # Batch size to use\n\n    iterator = dataset.make_one_shot_iterator()\n    batch_features, batch_labels = iterator.get_next()\n    #return tf.reshape(batch_ids,shape=[-1,field_size]), tf.reshape(batch_vals,shape=[-1,field_size]), batch_labels\n    return batch_features, batch_labels\n\ndef model_fn(features, labels, mode, params):\n    """"""Bulid Model function f(x) for Estimator.""""""\n    #------hyperparameters----\n    field_size = params[""field_size""]\n    feature_size = params[""feature_size""]\n    embedding_size = params[""embedding_size""]\n    l2_reg = params[""l2_reg""]\n    learning_rate = params[""learning_rate""]\n    #optimizer = params[""optimizer""]\n    layers = map(int, params[""deep_layers""].split(\',\'))\n    dropout = map(float, params[""dropout""].split(\',\'))\n\n    #------bulid weights------\n    Global_Bias = tf.get_variable(name=\'bias\', shape=[1], initializer=tf.constant_initializer(0.0))\n    Feat_Bias = tf.get_variable(name=\'linear\', shape=[feature_size], initializer=tf.glorot_normal_initializer())\n    Feat_Emb = tf.get_variable(name=\'emb\', shape=[feature_size,embedding_size], initializer=tf.glorot_normal_initializer())\n\n    #------build feaure-------\n    feat_ids  = features[\'feat_ids\']\n    feat_ids = tf.reshape(feat_ids,shape=[-1,field_size])\n    feat_vals = features[\'feat_vals\']\n    feat_vals = tf.reshape(feat_vals,shape=[-1,field_size])\n\n    #------build f(x)------\n    with tf.variable_scope(""Linear-part""):\n        feat_wgts = tf.nn.embedding_lookup(Feat_Bias, feat_ids) \t\t# None * F * 1\n        y_linear = tf.reduce_sum(tf.multiply(feat_wgts, feat_vals),1)\n\n    with tf.variable_scope(""BiInter-part""):\n        embeddings = tf.nn.embedding_lookup(Feat_Emb, feat_ids) \t\t# None * F * K\n        feat_vals = tf.reshape(feat_vals, shape=[-1, field_size, 1])\n        embeddings = tf.multiply(embeddings, feat_vals) \t\t\t\t# vij * xi\n        sum_square_emb = tf.square(tf.reduce_sum(embeddings,1))\n        square_sum_emb = tf.reduce_sum(tf.square(embeddings),1)\n        deep_inputs = 0.5*tf.subtract(sum_square_emb, square_sum_emb)\t# None * K\n\n    with tf.variable_scope(""Deep-part""):\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            train_phase = True\n        else:\n            train_phase = False\n\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            deep_inputs = tf.nn.dropout(deep_inputs, keep_prob=dropout[0]) \t\t\t\t\t\t# None * K\n        for i in range(len(layers)):\n            deep_inputs = tf.contrib.layers.fully_connected(inputs=deep_inputs, num_outputs=layers[i], \\\n                weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'mlp%d\' % i)\n\n            if FLAGS.batch_norm:\n                deep_inputs = batch_norm_layer(deep_inputs, train_phase=train_phase, scope_bn=\'bn_%d\' %i)   #\xe6\x94\xbe\xe5\x9c\xa8RELU\xe4\xb9\x8b\xe5\x90\x8e https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                deep_inputs = tf.nn.dropout(deep_inputs, keep_prob=dropout[i])                              #Apply Dropout after all BN layers and set dropout=0.8(drop_ratio=0.2)\n                #deep_inputs = tf.layers.dropout(inputs=deep_inputs, rate=dropout[i], training=mode == tf.estimator.ModeKeys.TRAIN)\n\n        y_deep = tf.contrib.layers.fully_connected(inputs=deep_inputs, num_outputs=1, activation_fn=tf.identity, \\\n            weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'deep_out\')\n        y_d = tf.reshape(y_deep,shape=[-1])\n\n    with tf.variable_scope(""NFM-out""):\n        #y_bias = Global_Bias * tf.ones_like(labels, dtype=tf.float32)  # None * 1  warning;\xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\x8d\xe8\x83\xbd\xe7\x94\xa8label\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe8\xb0\x83\xe7\x94\xa8predict/export\xe5\x87\xbd\xe6\x95\xb0\xe4\xbc\x9a\xe5\x87\xba\xe9\x94\x99\xef\xbc\x8ctrain/evaluate\xe6\xad\xa3\xe5\xb8\xb8\xef\xbc\x9b\xe5\x88\x9d\xe6\xad\xa5\xe5\x88\xa4\xe6\x96\xadestimator\xe5\x81\x9a\xe4\xba\x86\xe4\xbc\x98\xe5\x8c\x96\xef\xbc\x8c\xe7\x94\xa8\xe4\xb8\x8d\xe5\x88\xb0label\xe6\x97\xb6\xe4\xb8\x8d\xe4\xbc\xa0\n        y_bias = Global_Bias * tf.ones_like(y_d, dtype=tf.float32)     \t# None * 1\n        y = y_bias + y_linear + y_d\n        pred = tf.sigmoid(y)\n\n    predictions={""prob"": pred}\n    export_outputs = {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: tf.estimator.export.PredictOutput(predictions)}\n    # Provide an estimator spec for `ModeKeys.PREDICT`\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                export_outputs=export_outputs)\n\n    #------bulid loss------\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=labels)) + \\\n        l2_reg * tf.nn.l2_loss(Feat_Bias) + l2_reg * tf.nn.l2_loss(Feat_Emb)\n\n    # Provide an estimator spec for `ModeKeys.EVAL`\n    eval_metric_ops = {\n        ""auc"": tf.metrics.auc(labels, pred)\n    }\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                eval_metric_ops=eval_metric_ops)\n\n    #------bulid optimizer------\n    if FLAGS.optimizer == \'Adam\':\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8)\n    elif FLAGS.optimizer == \'Adagrad\':\n        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate, initial_accumulator_value=1e-8)\n    elif FLAGS.optimizer == \'Momentum\':\n        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95)\n    elif FLAGS.optimizer == \'ftrl\':\n        optimizer = tf.train.FtrlOptimizer(learning_rate)\n\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n\n    # Provide an estimator spec for `ModeKeys.TRAIN` modes\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                train_op=train_op)\n\n    # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\n    #return tf.estimator.EstimatorSpec(\n    #        mode=mode,\n    #        loss=loss,\n    #        train_op=train_op,\n    #        predictions={""prob"": pred},\n    #        eval_metric_ops=eval_metric_ops)\n\ndef batch_norm_layer(x, train_phase, scope_bn):\n    bn_train = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True, updates_collections=None, is_training=True,  reuse=None, scope=scope_bn)\n    bn_infer = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True, updates_collections=None, is_training=False, reuse=True, scope=scope_bn)\n    z = tf.cond(tf.cast(train_phase, tf.bool), lambda: bn_train, lambda: bn_infer)\n    return z\n\ndef set_dist_env():\n    if FLAGS.dist_mode == 1:        # \xe6\x9c\xac\xe5\x9c\xb0\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xb5\x8b\xe8\xaf\x95\xe6\xa8\xa1\xe5\xbc\x8f1 chief, 1 ps, 1 evaluator\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        chief_hosts = FLAGS.chief_hosts.split(\',\')\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # \xe6\x97\xa0worker\xe5\x8f\x82\xe6\x95\xb0\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n    elif FLAGS.dist_mode == 2:      # \xe9\x9b\x86\xe7\xbe\xa4\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xa8\xa1\xe5\xbc\x8f\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        worker_hosts = FLAGS.worker_hosts.split(\',\')\n        chief_hosts = worker_hosts[0:1] # get first worker as chief\n        worker_hosts = worker_hosts[2:] # the rest as worker\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'worker_host\', worker_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # use #worker=0 as chief\n        if job_name == ""worker"" and task_index == 0:\n            job_name = ""chief""\n        # use #worker=1 as evaluator\n        if job_name == ""worker"" and task_index == 1:\n            job_name = \'evaluator\'\n            task_index = 0\n        # the others as worker\n        if job_name == ""worker"" and task_index > 1:\n            task_index -= 2\n\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'worker\': worker_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n\ndef main(_):\n    #------check Arguments------\n    if FLAGS.dt_dir == """":\n        FLAGS.dt_dir = (date.today() + timedelta(-1)).strftime(\'%Y%m%d\')\n    FLAGS.model_dir = FLAGS.model_dir + FLAGS.dt_dir\n    #FLAGS.data_dir  = FLAGS.data_dir + FLAGS.dt_dir\n\n    print(\'task_type \', FLAGS.task_type)\n    print(\'model_dir \', FLAGS.model_dir)\n    print(\'data_dir \', FLAGS.data_dir)\n    print(\'dt_dir \', FLAGS.dt_dir)\n    print(\'num_epochs \', FLAGS.num_epochs)\n    print(\'feature_size \', FLAGS.feature_size)\n    print(\'field_size \', FLAGS.field_size)\n    print(\'embedding_size \', FLAGS.embedding_size)\n    print(\'batch_size \', FLAGS.batch_size)\n    print(\'deep_layers \', FLAGS.deep_layers)\n    print(\'dropout \', FLAGS.dropout)\n    print(\'loss_type \', FLAGS.loss_type)\n    print(\'optimizer \', FLAGS.optimizer)\n    print(\'learning_rate \', FLAGS.learning_rate)\n    print(\'l2_reg \', FLAGS.l2_reg)\n\n    #------init Envs------\n    tr_files = glob.glob(""%s/tr*libsvm"" % FLAGS.data_dir)\n    random.shuffle(tr_files)\n    print(""tr_files:"", tr_files)\n    va_files = glob.glob(""%s/va*libsvm"" % FLAGS.data_dir)\n    print(""va_files:"", va_files)\n    te_files = glob.glob(""%s/te*libsvm"" % FLAGS.data_dir)\n    print(""te_files:"", te_files)\n\n    if FLAGS.clear_existing_model:\n        try:\n            shutil.rmtree(FLAGS.model_dir)\n        except Exception as e:\n            print(e, ""at clear_existing_model"")\n        else:\n            print(""existing model cleaned at %s"" % FLAGS.model_dir)\n\n    set_dist_env()\n\n    #------bulid Tasks------\n    model_params = {\n        ""field_size"": FLAGS.field_size,\n        ""feature_size"": FLAGS.feature_size,\n        ""embedding_size"": FLAGS.embedding_size,\n        ""learning_rate"": FLAGS.learning_rate,\n        ""l2_reg"": FLAGS.l2_reg,\n        ""deep_layers"": FLAGS.deep_layers,\n        ""dropout"": FLAGS.dropout\n    }\n    config = tf.estimator.RunConfig().replace(session_config = tf.ConfigProto(device_count={\'GPU\':0, \'CPU\':FLAGS.num_threads}),\n            log_step_count_steps=FLAGS.log_steps, save_summary_steps=FLAGS.log_steps)\n    Estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=FLAGS.model_dir, params=model_params, config=config)\n\n    if FLAGS.task_type == \'train\':\n        train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(tr_files, num_epochs=FLAGS.num_epochs, batch_size=FLAGS.batch_size))\n        eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size), steps=None, start_delay_secs=1000, throttle_secs=1200)\n        tf.estimator.train_and_evaluate(Estimator, train_spec, eval_spec)\n    elif FLAGS.task_type == \'eval\':\n        Estimator.evaluate(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size))\n    elif FLAGS.task_type == \'infer\':\n        preds = Estimator.predict(input_fn=lambda: input_fn(te_files, num_epochs=1, batch_size=FLAGS.batch_size), predict_keys=""prob"")\n        with open(FLAGS.data_dir+""/pred.txt"", ""w"") as fo:\n            for prob in preds:\n                fo.write(""%f\\n"" % (prob[\'prob\']))\n    elif FLAGS.task_type == \'export\':\n        #feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n        #feature_spec = {\n        #    \'feat_ids\': tf.FixedLenFeature(dtype=tf.int64, shape=[None, FLAGS.field_size]),\n        #    \'feat_vals\': tf.FixedLenFeature(dtype=tf.float32, shape=[None, FLAGS.field_size])\n        #}\n        #serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n        feature_spec = {\n            \'feat_ids\': tf.placeholder(dtype=tf.int64, shape=[None, FLAGS.field_size], name=\'feat_ids\'),\n            \'feat_vals\': tf.placeholder(dtype=tf.float32, shape=[None, FLAGS.field_size], name=\'feat_vals\')\n        }\n        serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_spec)\n        Estimator.export_savedmodel(FLAGS.servable_model_dir, serving_input_receiver_fn)\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
deep_ctr/Model_pipeline/PNN.py,116,"b'#!/usr/bin/env python\n#coding=utf-8\n""""""\nTensorFlow Implementation of <<Deep Learning over Multi-Field Categorical Data: A Case Study on User Response Prediction>>\nand <<Product-based Neural Networks for User Response Prediction>> with the fellowing features\xef\xbc\x9a\n#1 Input pipline using Dataset high level API, Support parallel and prefetch reading\n#2 Train pipline using Coustom Estimator by rewriting model_fn\n#3 Support distincted training by TF_CONFIG\n#4 Support export servable model for TensorFlow Serving\n\nby lambdaji\n""""""\n#from __future__ import absolute_import\n#from __future__ import division\n#from __future__ import print_function\n\n#import argparse\nimport shutil\n#import sys\nimport os\nimport json\nimport glob\nfrom datetime import date, timedelta\nfrom time import time\n#import gc\n#from multiprocessing import Process\n\n#import math\nimport random\n#import pandas as pd\n#import numpy as np\nimport tensorflow as tf\n\n#################### CMD Arguments ####################\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""dist_mode"", 0, ""distribuion mode {0-loacal, 1-single_dist, 2-multi_dist}"")\ntf.app.flags.DEFINE_string(""ps_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""worker_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""job_name"", \'\', ""One of \'ps\', \'worker\'"")\ntf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")\ntf.app.flags.DEFINE_integer(""num_threads"", 16, ""Number of threads"")\ntf.app.flags.DEFINE_integer(""feature_size"", 0, ""Number of features"")\ntf.app.flags.DEFINE_integer(""field_size"", 0, ""Number of fields"")\ntf.app.flags.DEFINE_integer(""embedding_size"", 32, ""Embedding size"")\ntf.app.flags.DEFINE_integer(""num_epochs"", 10, ""Number of epochs"")\ntf.app.flags.DEFINE_integer(""batch_size"", 64, ""Number of batch size"")\ntf.app.flags.DEFINE_integer(""log_steps"", 1000, ""save summary every steps"")\ntf.app.flags.DEFINE_float(""learning_rate"", 0.0005, ""learning rate"")\ntf.app.flags.DEFINE_float(""l2_reg"", 0.0001, ""L2 regularization"")\ntf.app.flags.DEFINE_string(""loss_type"", \'log_loss\', ""loss type {square_loss, log_loss}"")\ntf.app.flags.DEFINE_string(""optimizer"", \'Adam\', ""optimizer type {Adam, Adagrad, GD, Momentum}"")\ntf.app.flags.DEFINE_string(""deep_layers"", \'256,128,64\', ""deep layers"")\ntf.app.flags.DEFINE_string(""dropout"", \'0.5,0.5,0.5\', ""dropout rate"")\ntf.app.flags.DEFINE_boolean(""batch_norm"", False, ""perform batch normaization (True or False)"")\ntf.app.flags.DEFINE_float(""batch_norm_decay"", 0.9, ""decay for the moving average(recommend trying decay=0.9)"")\ntf.app.flags.DEFINE_string(""data_dir"", \'\', ""data dir"")\ntf.app.flags.DEFINE_string(""dt_dir"", \'\', ""data dt partition"")\ntf.app.flags.DEFINE_string(""model_dir"", \'\', ""model check point dir"")\ntf.app.flags.DEFINE_string(""servable_model_dir"", \'\', ""export servable model for TensorFlow Serving"")\ntf.app.flags.DEFINE_string(""task_type"", \'train\', ""task type {train, infer, eval, export}"")\ntf.app.flags.DEFINE_string(""model_type"", \'Inner\', ""model type {FNN, Inner, Outer}"")\ntf.app.flags.DEFINE_boolean(""clear_existing_model"", False, ""clear existing model or not"")\n\n#1 1:0.5 2:0.03519 3:1 4:0.02567 7:0.03708 8:0.01705 9:0.06296 10:0.18185 11:0.02497 12:1 14:0.02565 15:0.03267 17:0.0247 18:0.03158 20:1 22:1 23:0.13169 24:0.02933 27:0.18159 31:0.0177 34:0.02888 38:1 51:1 63:1 132:1 164:1 236:1\ndef input_fn(filenames, batch_size=32, num_epochs=1, perform_shuffle=False):\n    print(\'Parsing\', filenames)\n    def decode_libsvm(line):\n        #columns = tf.decode_csv(value, record_defaults=CSV_COLUMN_DEFAULTS)\n        #features = dict(zip(CSV_COLUMNS, columns))\n        #labels = features.pop(LABEL_COLUMN)\n        columns = tf.string_split([line], \' \')\n        labels = tf.string_to_number(columns.values[0], out_type=tf.float32)\n        splits = tf.string_split(columns.values[1:], \':\')\n        id_vals = tf.reshape(splits.values,splits.dense_shape)\n        feat_ids, feat_vals = tf.split(id_vals,num_or_size_splits=2,axis=1)\n        feat_ids = tf.string_to_number(feat_ids, out_type=tf.int32)\n        feat_vals = tf.string_to_number(feat_vals, out_type=tf.float32)\n        #feat_ids = tf.reshape(feat_ids,shape=[-1,FLAGS.field_size])\n        #for i in range(splits.dense_shape.eval()[0]):\n        #    feat_ids.append(tf.string_to_number(splits.values[2*i], out_type=tf.int32))\n        #    feat_vals.append(tf.string_to_number(splits.values[2*i+1]))\n        #return tf.reshape(feat_ids,shape=[-1,field_size]), tf.reshape(feat_vals,shape=[-1,field_size]), labels\n        return {""feat_ids"": feat_ids, ""feat_vals"": feat_vals}, labels\n\n    # Extract lines from input files using the Dataset API, can pass one filename or filename list\n    dataset = tf.data.TextLineDataset(filenames).map(decode_libsvm, num_parallel_calls=10).prefetch(500000)    # multi-thread pre-process then prefetch\n\n    # Randomizes input using a window of 256 elements (read into memory)\n    if perform_shuffle:\n        dataset = dataset.shuffle(buffer_size=256)\n\n    # epochs from blending together.\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.batch(batch_size) # Batch size to use\n\n    #return dataset.make_one_shot_iterator()\n    iterator = dataset.make_one_shot_iterator()\n    batch_features, batch_labels = iterator.get_next()\n    #return tf.reshape(batch_ids,shape=[-1,field_size]), tf.reshape(batch_vals,shape=[-1,field_size]), batch_labels\n    return batch_features, batch_labels\n\ndef model_fn(features, labels, mode, params):\n    """"""Bulid Model function f(x) for Estimator.""""""\n    #------hyperparameters----\n    field_size = params[""field_size""]\n    feature_size = params[""feature_size""]\n    embedding_size = params[""embedding_size""]\n    l2_reg = params[""l2_reg""]\n    learning_rate = params[""learning_rate""]\n    #optimizer = params[""optimizer""]\n    layers = map(int, params[""deep_layers""].split(\',\'))\n    dropout = map(float, params[""dropout""].split(\',\'))\n    num_pairs = field_size * (field_size - 1) / 2\n\n    #------bulid weights------\n    Global_Bias = tf.get_variable(name=\'bias\', shape=[1], initializer=tf.constant_initializer(0.0))\n    Feat_Bias = tf.get_variable(name=\'linear\', shape=[feature_size], initializer=tf.glorot_normal_initializer())\n    Feat_Emb = tf.get_variable(name=\'emb\', shape=[feature_size, embedding_size], initializer=tf.glorot_normal_initializer())\n    #Prod_Kernel = tf.get_variable(name=\'kernel\', shape=[embedding_size, num_pairs, embedding_size], initializer=tf.glorot_normal_initializer())\n\n\n    #------build feaure-------\n    feat_ids  = features[\'feat_ids\']\t\t\t\t\t\t\t\t\t# None * F * 1\n    feat_ids = tf.reshape(feat_ids,shape=[-1,field_size])\n    feat_vals = features[\'feat_vals\']\t\t\t\t\t\t\t\t\t# None * F * 1\n    feat_vals = tf.reshape(feat_vals,shape=[-1,field_size])\n\n    #------build f(x)------\n    with tf.variable_scope(""Linear-part""):\n        feat_wgts = tf.nn.embedding_lookup(Feat_Bias, feat_ids) \t\t# None * F * 1\n        y_linear = tf.reduce_sum(tf.multiply(feat_wgts, feat_vals),1)\n\n    with tf.variable_scope(""Embedding-layer""):\n        embeddings = tf.nn.embedding_lookup(Feat_Emb, feat_ids) \t\t# None * F * K\n        feat_vals = tf.reshape(feat_vals, shape=[-1, field_size, 1])\n        embeddings = tf.multiply(embeddings, feat_vals) \t\t\t\t# None * F * K\n\n    with tf.variable_scope(""Product-layer""):\n\t\tif FLAGS.model_type == \'FNN\':\n\t\t\tdeep_inputs = tf.reshape(embeddings,shape=[-1,field_size*embedding_size])\n\t\telif FLAGS.model_type == \'Inner\':\n\t\t\trow = []\n\t\t\tcol = []\n\t\t\tfor i in range(field_size-1):\n\t\t\t\tfor j in range(i+1, field_size):\n\t\t\t\t\trow.append(i)\n\t\t\t\t\tcol.append(j)\n\t\t\tp = tf.gather(embeddings, row, axis=1)\n\t\t\tq = tf.gather(embeddings, col, axis=1)\n\t        #p = tf.reshape(p, [-1, num_pairs, embedding_size])\n            #q = tf.reshape(q, [-1, num_pairs, embedding_size])\n\t\t\tinner = tf.reshape(tf.reduce_sum(p * q, [-1]), [-1, num_pairs])\t\t\t\t\t\t\t\t\t\t# None * (F*(F-1)/2)\n\t\t\tdeep_inputs = tf.concat([tf.reshape(embeddings,shape=[-1,field_size*embedding_size]), inner], 1)\t# None * ( F*K+F*(F-1)/2 )\n\t\telif FLAGS.model_type == \'Outer\':             #ERROR: NOT ready yet\n\t\t\trow = []\n\t\t\tcol = []\n\t\t\tfor i in range(field_size-1):\n\t\t\t\tfor j in range(i+1, field_size):\n\t\t\t\t\trow.append(i)\n\t\t\t\t\tcol.append(j)\n\t\t\tp = tf.gather(embeddings, row, axis=1)\n\t\t\tq = tf.gather(embeddings, col, axis=1)\n\t        #p = tf.reshape(p, [-1, num_pairs, embedding_size])\n            #q = tf.reshape(q, [-1, num_pairs, embedding_size])\n\t\t\t#einsum(\'i,j->ij\', p, q)  # output[i,j] = p[i]*q[j]\t\t\t\t# Outer product\n\t\t\touter = tf.reshape(tf.einsum(\'api,apj->apij\', p, q), [-1, num_pairs*embedding_size*embedding_size])\t# None * (F*(F-1)/2*K*K)\n\t\t\tdeep_inputs = tf.concat([tf.reshape(embeddings,shape=[-1,field_size*embedding_size]), outer], 1)\t# None * ( F*K+F*(F-1)/2*K*K )\n\n\n    with tf.variable_scope(""Deep-part""):\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            train_phase = True\n        else:\n            train_phase = False\n\n        for i in range(len(layers)):\n            deep_inputs = tf.contrib.layers.fully_connected(inputs=deep_inputs, num_outputs=layers[i], \\\n            \tweights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'mlp%d\' % i)\n\n            if FLAGS.batch_norm:\n\t\t\t\tdeep_inputs = batch_norm_layer(deep_inputs, train_phase=train_phase, scope_bn=\'bn_%d\' %i)   \t#\xe6\x94\xbe\xe5\x9c\xa8RELU\xe4\xb9\x8b\xe5\x90\x8e https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu\n            if mode == tf.estimator.ModeKeys.TRAIN:\n\t\t\t\tdeep_inputs = tf.nn.dropout(deep_inputs, keep_prob=dropout[i])                              \t#Apply Dropout after all BN layers and set dropout=0.8(drop_ratio=0.2)\n            \t#deep_inputs = tf.layers.dropout(inputs=deep_inputs, rate=dropout[i], training=mode == tf.estimator.ModeKeys.TRAIN)\n\n        y_deep = tf.contrib.layers.fully_connected(inputs=deep_inputs, num_outputs=1, activation_fn=tf.identity, \\\n            weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), scope=\'deep_out\')\n        y_d = tf.reshape(y_deep,shape=[-1])\n\n    with tf.variable_scope(""PNN-out""):\n        #y_bias = Global_Bias * tf.ones_like(labels, dtype=tf.float32)  # None * 1  warning;\xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\x8d\xe8\x83\xbd\xe7\x94\xa8label\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe8\xb0\x83\xe7\x94\xa8predict/export\xe5\x87\xbd\xe6\x95\xb0\xe4\xbc\x9a\xe5\x87\xba\xe9\x94\x99\xef\xbc\x8ctrain/evaluate\xe6\xad\xa3\xe5\xb8\xb8\xef\xbc\x9b\xe5\x88\x9d\xe6\xad\xa5\xe5\x88\xa4\xe6\x96\xadestimator\xe5\x81\x9a\xe4\xba\x86\xe4\xbc\x98\xe5\x8c\x96\xef\xbc\x8c\xe7\x94\xa8\xe4\xb8\x8d\xe5\x88\xb0label\xe6\x98\xaf\xe4\xb8\x8d\xe4\xbc\xa0\n        y_bias = Global_Bias * tf.ones_like(y_d, dtype=tf.float32)      # None * 1\n        y = y_bias + y_linear + y_d\n        pred = tf.sigmoid(y)\n\n    predictions={""prob"": pred}\n    export_outputs = {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: tf.estimator.export.PredictOutput(predictions)}\n    # Provide an estimator spec for `ModeKeys.PREDICT`\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                export_outputs=export_outputs)\n\n    #------bulid loss------\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=labels)) + \\\n        l2_reg * tf.nn.l2_loss(Feat_Bias) + l2_reg * tf.nn.l2_loss(Feat_Emb)\n\n    # Provide an estimator spec for `ModeKeys.EVAL`\n    eval_metric_ops = {\n        ""auc"": tf.metrics.auc(labels, pred)\n    }\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                eval_metric_ops=eval_metric_ops)\n\n    #------bulid optimizer------\n    if FLAGS.optimizer == \'Adam\':\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8)\n    elif FLAGS.optimizer == \'Adagrad\':\n        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate, initial_accumulator_value=1e-8)\n    elif FLAGS.optimizer == \'Momentum\':\n        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95)\n    elif FLAGS.optimizer == \'ftrl\':\n        optimizer = tf.train.FtrlOptimizer(learning_rate)\n\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n\n    # Provide an estimator spec for `ModeKeys.TRAIN` modes\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        return tf.estimator.EstimatorSpec(\n                mode=mode,\n                predictions=predictions,\n                loss=loss,\n                train_op=train_op)\n\n    # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\n    #return tf.estimator.EstimatorSpec(\n    #        mode=mode,\n    #        loss=loss,\n    #        train_op=train_op,\n    #        predictions={""prob"": pred},\n    #        eval_metric_ops=eval_metric_ops)\n\ndef batch_norm_layer(x, train_phase, scope_bn):\n    bn_train = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True, updates_collections=None, is_training=True,  reuse=None, scope=scope_bn)\n    bn_infer = tf.contrib.layers.batch_norm(x, decay=FLAGS.batch_norm_decay, center=True, scale=True, updates_collections=None, is_training=False, reuse=True, scope=scope_bn)\n    z = tf.cond(tf.cast(train_phase, tf.bool), lambda: bn_train, lambda: bn_infer)\n    return z\n\ndef set_dist_env():\n    if FLAGS.dist_mode == 1:        # \xe6\x9c\xac\xe5\x9c\xb0\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xb5\x8b\xe8\xaf\x95\xe6\xa8\xa1\xe5\xbc\x8f1 chief, 1 ps, 1 evaluator\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        chief_hosts = FLAGS.chief_hosts.split(\',\')\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # \xe6\x97\xa0worker\xe5\x8f\x82\xe6\x95\xb0\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n    elif FLAGS.dist_mode == 2:      # \xe9\x9b\x86\xe7\xbe\xa4\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\xa8\xa1\xe5\xbc\x8f\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        worker_hosts = FLAGS.worker_hosts.split(\',\')\n        chief_hosts = worker_hosts[0:1] # get first worker as chief\n        worker_hosts = worker_hosts[2:] # the rest as worker\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        print(\'ps_host\', ps_hosts)\n        print(\'worker_host\', worker_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        print(\'job_name\', job_name)\n        print(\'task_index\', str(task_index))\n        # use #worker=0 as chief\n        if job_name == ""worker"" and task_index == 0:\n            job_name = ""chief""\n        # use #worker=1 as evaluator\n        if job_name == ""worker"" and task_index == 1:\n            job_name = \'evaluator\'\n            task_index = 0\n        # the others as worker\n        if job_name == ""worker"" and task_index > 1:\n            task_index -= 2\n\n        tf_config = {\n            \'cluster\': {\'chief\': chief_hosts, \'worker\': worker_hosts, \'ps\': ps_hosts},\n            \'task\': {\'type\': job_name, \'index\': task_index }\n        }\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n\ndef main(_):\n    #------check Arguments------\n    if FLAGS.dt_dir == """":\n        FLAGS.dt_dir = (date.today() + timedelta(-1)).strftime(\'%Y%m%d\')\n    FLAGS.model_dir = FLAGS.model_dir + FLAGS.dt_dir\n    #FLAGS.data_dir  = FLAGS.data_dir + FLAGS.dt_dir\n\n    print(\'task_type \', FLAGS.task_type)\n    print(\'model_type \', FLAGS.model_type)\n    print(\'model_dir \', FLAGS.model_dir)\n    print(\'data_dir \', FLAGS.data_dir)\n    print(\'dt_dir \', FLAGS.dt_dir)\n    print(\'num_epochs \', FLAGS.num_epochs)\n    print(\'feature_size \', FLAGS.feature_size)\n    print(\'field_size \', FLAGS.field_size)\n    print(\'embedding_size \', FLAGS.embedding_size)\n    print(\'batch_size \', FLAGS.batch_size)\n    print(\'deep_layers \', FLAGS.deep_layers)\n    print(\'dropout \', FLAGS.dropout)\n    print(\'loss_type \', FLAGS.loss_type)\n    print(\'optimizer \', FLAGS.optimizer)\n    print(\'learning_rate \', FLAGS.learning_rate)\n    print(\'l2_reg \', FLAGS.l2_reg)\n\n    #------init Envs------\n    tr_files = glob.glob(""%s/tr*libsvm"" % FLAGS.data_dir)\n    random.shuffle(tr_files)\n    print(""tr_files:"", tr_files)\n    va_files = glob.glob(""%s/va*libsvm"" % FLAGS.data_dir)\n    print(""va_files:"", va_files)\n    te_files = glob.glob(""%s/te*libsvm"" % FLAGS.data_dir)\n    print(""te_files:"", te_files)\n\n    if FLAGS.clear_existing_model:\n        try:\n            shutil.rmtree(FLAGS.model_dir)\n        except Exception as e:\n            print(e, ""at clear_existing_model"")\n        else:\n            print(""existing model cleaned at %s"" % FLAGS.model_dir)\n\n    set_dist_env()\n\n    #------bulid Tasks------\n    model_params = {\n        ""field_size"": FLAGS.field_size,\n        ""feature_size"": FLAGS.feature_size,\n        ""embedding_size"": FLAGS.embedding_size,\n        ""learning_rate"": FLAGS.learning_rate,\n        ""l2_reg"": FLAGS.l2_reg,\n        ""deep_layers"": FLAGS.deep_layers,\n\t\t""model_type"": FLAGS.model_type,\n\t\t""dropout"": FLAGS.dropout\n    }\n    config = tf.estimator.RunConfig().replace(session_config = tf.ConfigProto(device_count={\'GPU\':0, \'CPU\':FLAGS.num_threads}),\n            log_step_count_steps=FLAGS.log_steps, save_summary_steps=FLAGS.log_steps)\n    Estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=FLAGS.model_dir, params=model_params, config=config)\n\n    if FLAGS.task_type == \'train\':\n        train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(tr_files, num_epochs=FLAGS.num_epochs, batch_size=FLAGS.batch_size))\n        eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size), steps=None, start_delay_secs=1000, throttle_secs=1200)\n        tf.estimator.train_and_evaluate(Estimator, train_spec, eval_spec)\n    elif FLAGS.task_type == \'eval\':\n        Estimator.evaluate(input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size))\n    elif FLAGS.task_type == \'infer\':\n        preds = Estimator.predict(input_fn=lambda: input_fn(te_files, num_epochs=1, batch_size=FLAGS.batch_size), predict_keys=""prob"")\n        with open(FLAGS.data_dir+""/pred.txt"", ""w"") as fo:\n            for prob in preds:\n                fo.write(""%f\\n"" % (prob[\'prob\']))\n    elif FLAGS.task_type == \'export\':\n        #feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n        #feature_spec = {\n        #    \'feat_ids\': tf.FixedLenFeature(dtype=tf.int64, shape=[None, FLAGS.field_size]),\n        #    \'feat_vals\': tf.FixedLenFeature(dtype=tf.float32, shape=[None, FLAGS.field_size])\n        #}\n        #serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n        feature_spec = {\n            \'feat_ids\': tf.placeholder(dtype=tf.int64, shape=[None, FLAGS.field_size], name=\'feat_ids\'),\n            \'feat_vals\': tf.placeholder(dtype=tf.float32, shape=[None, FLAGS.field_size], name=\'feat_vals\')\n        }\n        serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_spec)\n        Estimator.export_savedmodel(FLAGS.servable_model_dir, serving_input_receiver_fn)\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
deep_ctr/Model_pipeline/wide_n_deep.py,43,"b'#!/usr/bin/env python\n# coding=utf-8\n\n# from __future__ import absolute_import\n# from __future__ import division\n# from __future__ import print_function\n\nimport argparse\nimport shutil\nimport sys\nimport os\nimport glob\nimport json\n# import threading\nimport random\nfrom datetime import date, timedelta\n\n# import numpy as np\nimport tensorflow as tf\n\n#################### CMD Arguments ####################\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_boolean(""dist_mode"", False, ""run use distribuion mode or not"")\ntf.app.flags.DEFINE_string(""ps_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""worker_hosts"", \'\', ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""job_name"", \'\', ""One of \'ps\', \'worker\'"")\ntf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")\ntf.app.flags.DEFINE_integer(""num_threads"", 10, ""Number of threads"")\n# tf.app.flags.DEFINE_integer(""feature_size"", 0, ""Number of features"")\n# tf.app.flags.DEFINE_integer(""field_size"", 0, ""Number of fields"")\ntf.app.flags.DEFINE_integer(""embedding_size"", 32, ""Embedding size"")\ntf.app.flags.DEFINE_integer(""num_epochs"", 10, ""Number of epochs"")\ntf.app.flags.DEFINE_integer(""batch_size"", 128, ""batch size"")\ntf.app.flags.DEFINE_string(""deep_layers"", \'256,128,64\', ""deep layers"")\ntf.app.flags.DEFINE_integer(""log_steps"", 1000, ""save summary every steps"")\ntf.app.flags.DEFINE_integer(""throttle_secs"", 600, ""evaluate every 10mins"")\n# tf.app.flags.DEFINE_float(""learning_rate"", 0.0005, ""learning rate"")\n# tf.app.flags.DEFINE_float(""l2_reg"", 0.0001, ""L2 regularization"")\n# tf.app.flags.DEFINE_string(""loss_type"", \'log_loss\', ""loss type {square_loss, log_loss}"")\n# tf.app.flags.DEFINE_string(""optimizer"", \'Adam\', ""optimizer type {Adam, Adagrad, GD, Momentum}"")\ntf.app.flags.DEFINE_string(""data_dir"", \'\', ""data dir"")\ntf.app.flags.DEFINE_string(""dt_dir"", \'\', ""data dt partition"")\ntf.app.flags.DEFINE_string(""model_dir"", \'\', ""model check point dir"")\ntf.app.flags.DEFINE_string(""servable_model_dir"", \'\', ""export servable model for TensorFlow Serving"")\ntf.app.flags.DEFINE_string(""task_type"", \'train\', ""task type {train, predict, export}"")\ntf.app.flags.DEFINE_string(""model_type"", \'wide_n_deep\', ""model type {\'wide\', \'deep\', \'wide_n_deep\'}"")\ntf.app.flags.DEFINE_boolean(""clear_existing_model"", False, ""clear existing model or not"")\n\n###############################################################################\n#\n#       { < u, a, c, xgb >, y }\n#\n################################################################################\n# There are 13 integer features and 26 categorical features\nC_COLUMNS = [\'I\' + str(i) for i in range(1, 14)]\nD_COLUMNS = [\'C\' + str(i) for i in range(14, 40)]\nLABEL_COLUMN = ""is_click""\nCSV_COLUMNS = [LABEL_COLUMN] + C_COLUMNS + D_COLUMNS\n# Columns Defaults\nCSV_COLUMN_DEFAULTS = [[0.0]]\nC_COLUMN_DEFAULTS = [[0.0] for i in range(13)]\nD_COLUMN_DEFAULTS = [[0] for i in range(26)]\nCSV_COLUMN_DEFAULTS = CSV_COLUMN_DEFAULTS + C_COLUMN_DEFAULTS + D_COLUMN_DEFAULTS\nprint(CSV_COLUMN_DEFAULTS)\n\n\ndef input_fn(filenames, num_epochs, batch_size=1):\n    def parse_csv(line):\n        print(\'Parsing\', filenames)\n        columns = tf.decode_csv(line, record_defaults=CSV_COLUMN_DEFAULTS)\n        features = dict(zip(CSV_COLUMNS, columns))\n        labels = features.pop(LABEL_COLUMN)\n        return features, labels\n\n    # Extract lines from input files using the Dataset API.\n    dataset = tf.data.TextLineDataset(filenames)  # can pass one filename or filename list\n\n    # multi-thread pre-process then prefetch\n    dataset = dataset.map(parse_csv, num_parallel_calls=10).prefetch(500000)\n\n    # We call repeat after shuffling, rather than before, to prevent separate\n    # epochs from blending together.\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.batch(batch_size)\n\n    iterator = dataset.make_one_shot_iterator()\n    features, labels = iterator.get_next()\n\n    return features, labels\n\n\ndef build_feature():\n    # 1 { continuous base columns }\n    deep_cbc = [tf.feature_column.numeric_column(colname) for colname in C_COLUMNS]\n\n    # 2 { categorical base columns }\n    deep_dbc = [tf.feature_column.categorical_column_with_identity(key=colname, num_buckets=10000, default_value=0) for\n                colname in D_COLUMNS]\n\n    # 3 { embedding columns }\n    deep_emb = [tf.feature_column.embedding_column(c, dimension=FLAGS.embedding_size) for c in deep_dbc]\n\n    # 3 { wide columns and deep columns }\n    wide_columns = deep_cbc + deep_dbc\n    deep_columns = deep_cbc + deep_emb\n\n    return wide_columns, deep_columns\n\n\n################################################################################\n#\n#       f(x) / loss / Optimizer\n#\n################################################################################\ndef build_estimator(model_dir, model_type, wide_columns, deep_columns):\n    """"""Build an estimator.""""""\n\n    if FLAGS.clear_existing_model:\n        try:\n            shutil.rmtree(model_dir)\n        except Exception as e:\n            print(e, ""at clear_existing_model"")\n        else:\n            print(""existing model cleaned at %s"" % model_dir)\n\n    hidden_units = map(int, FLAGS.deep_layers.split("",""))\n    config = tf.estimator.RunConfig().replace(\n        session_config=tf.ConfigProto(device_count={\'GPU\': 0, \'CPU\': FLAGS.num_threads}),\n        save_checkpoints_steps=FLAGS.log_steps, log_step_count_steps=FLAGS.log_steps,\n        save_summary_steps=FLAGS.log_steps)\n    # bulid model\n    if model_type == ""wide"":\n        estimator = tf.estimator.LinearClassifier(\n            model_dir=model_dir,\n            feature_columns=wide_columns,\n            config=config)\n    elif model_type == ""deep"":\n        estimator = tf.estimator.DNNClassifier(\n            model_dir=model_dir,\n            feature_columns=deep_columns,\n            hidden_units=hidden_units,\n            config=config)\n    else:\n        estimator = tf.estimator.DNNLinearCombinedClassifier(\n            model_dir=model_dir,\n            linear_feature_columns=wide_columns,\n            dnn_feature_columns=deep_columns,\n            dnn_hidden_units=hidden_units,\n            config=config)\n\n    return estimator\n\n\ndef set_dist_env():\n    if FLAGS.dist_mode:\n        ps_hosts = FLAGS.ps_hosts.split(\',\')\n        worker_hosts = FLAGS.worker_hosts.split(\',\')\n        chief_hosts = worker_hosts[0:1]  # get first worker as chief\n        worker_hosts = worker_hosts[2:]  # the rest as worker\n        print(\'ps_host\', ps_hosts)\n        print(\'worker_host\', worker_hosts)\n        print(\'chief_hosts\', chief_hosts)\n        task_index = FLAGS.task_index\n        job_name = FLAGS.job_name\n        # use #worker=0 as chief\n        if job_name == ""worker"" and task_index == 0:\n            job_name = ""chief""\n        # use #worker=1 as evaluator\n        if job_name == ""worker"" and task_index == 1:\n            job_name = \'evaluator\'\n            task_index = 0\n        # the others as worker\n        if job_name == ""worker"" and task_index > 1:\n            task_index -= 2\n\n        tf_config = {\'cluster\': {\'chief\': chief_hosts, \'worker\': worker_hosts, \'ps\': ps_hosts},\n                     \'task\': {\'type\': job_name, \'index\': task_index}}\n        print(json.dumps(tf_config))\n        os.environ[\'TF_CONFIG\'] = json.dumps(tf_config)\n\n\ndef main(_):\n    # ------check Arguments------\n    if FLAGS.dt_dir == """":\n        FLAGS.dt_dir = (date.today() + timedelta(-1)).strftime(\'%Y%m%d\')\n    FLAGS.model_dir = FLAGS.model_dir + FLAGS.dt_dir\n    # FLAGS.data_dir  = FLAGS.data_dir + FLAGS.dt_dir\n\n    print(\'task_type \', FLAGS.task_type)\n    print(\'model_type \', FLAGS.model_type)\n    print(\'model_dir \', FLAGS.model_dir)\n    print(\'servable_model_dir \', FLAGS.servable_model_dir)\n    print(\'dt_dir \', FLAGS.dt_dir)\n    print(\'data_dir \', FLAGS.data_dir)\n    print(\'num_epochs \', FLAGS.num_epochs)\n    print(\'embedding_size \', FLAGS.embedding_size)\n    print(\'deep_layers \', FLAGS.deep_layers)\n    print(\'batch_size \', FLAGS.batch_size)\n\n    # ------init Envs------\n    tr_files = glob.glob(""%s/tr*csv"" % FLAGS.data_dir)\n    random.shuffle(tr_files)\n    print(""tr_files:"", tr_files)\n    va_files = glob.glob(""%s/va*csv"" % FLAGS.data_dir)\n    print(""va_files:"", va_files)\n    te_files = glob.glob(""%s/te*csv"" % FLAGS.data_dir)\n    print(""te_files:"", te_files)\n\n    # ------build Tasks------\n    # build wide_columns, deep_columns\n    wide_columns, deep_columns = build_feature()\n\n    # build model\n    wide_n_deep = build_estimator(FLAGS.model_dir, FLAGS.model_type, wide_columns, deep_columns)\n\n    if FLAGS.task_type == ""train"":\n        set_dist_env()\n        train_spec = tf.estimator.TrainSpec(\n            input_fn=lambda: input_fn(tr_files, num_epochs=FLAGS.num_epochs, batch_size=FLAGS.batch_size))\n        eval_spec = tf.estimator.EvalSpec(\n            input_fn=lambda: input_fn(va_files, num_epochs=1, batch_size=FLAGS.batch_size),\n            # exporters = lastest_exporter,\n            steps=None,  # evaluate the whole eval file\n            start_delay_secs=1800,\n            throttle_secs=FLAGS.throttle_secs)  # evaluate every 10min for wide\n        tf.estimator.train_and_evaluate(wide_n_deep, train_spec, eval_spec)\n    elif FLAGS.task_type == ""predict"":\n        pred = wide_n_deep.predict(input_fn=lambda: input_fn(te_files, num_epochs=1, batch_size=FLAGS.batch_size),\n                                   predict_keys=""probabilities"")\n        with open(FLAGS.data_dir + ""/pred.txt"", ""w"") as fo:\n            for prob in pred:\n                fo.write(""%f\\n"" % (prob[\'probabilities\'][1]))\n    elif FLAGS.task_type == ""export_model"":\n        if FLAGS.model_type == ""wide"":\n            feature_columns = wide_columns\n        elif FLAGS.model_type == ""deep"":\n            feature_columns = deep_columns\n        elif FLAGS.model_type == ""wide_n_deep"":\n            feature_columns = wide_columns + deep_columns\n        feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n        serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n        wide_n_deep.export_savedmodel(FLAGS.servable_model_dir, serving_input_receiver_fn)\n\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
