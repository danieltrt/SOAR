file_path,api_count,code
setup.py,0,"b'from setuptools import find_packages, setup\n\n\ndef readme():\n    with open(""README.md"", ""r"") as f:\n        return f.read()\n\n\nsetup(\n    name=""larq"",\n    version=""0.9.6"",\n    python_requires="">=3.6"",\n    author=""Plumerai"",\n    author_email=""lukas@plumerai.co.uk"",\n    description=""An Open Source Machine Learning Library for Training Binarized Neural Networks"",\n    long_description=readme(),\n    long_description_content_type=""text/markdown"",\n    url=""https://larq.dev/"",\n    packages=find_packages(exclude=[""larq.snapshots""]),\n    license=""Apache 2.0"",\n    install_requires=[\n        ""numpy >= 1.15.4, < 2.0"",\n        ""terminaltables>=3.1.0"",\n        ""dataclasses ; python_version<\'3.7\'"",\n    ],\n    extras_require={\n        ""tensorflow"": [""tensorflow>=1.14.0""],\n        ""tensorflow_gpu"": [""tensorflow-gpu>=1.14.0""],\n        ""test"": [\n            ""black==19.10b0"",\n            ""flake8>=3.7.9,<3.9.0"",\n            ""isort~=4.3.21"",\n            ""packaging>=19.2,<21.0"",\n            ""pytest>=5.2.4,<5.5.0"",\n            ""pytest-cov>=2.8.1,<2.10.0"",\n            ""pytest-xdist>=1.30,<1.33"",\n            ""pytest-mock>=2.0,<3.2"",\n            ""pytype>=2020.01.07,<2020.7.0"",\n            ""snapshottest~=0.5.1"",\n        ],\n    },\n    classifiers=[\n        ""Development Status :: 4 - Beta"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Education"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3 :: Only"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Programming Language :: Python :: 3.8"",\n        ""Topic :: Scientific/Engineering"",\n        ""Topic :: Scientific/Engineering :: Mathematics"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n        ""Topic :: Software Development"",\n        ""Topic :: Software Development :: Libraries"",\n        ""Topic :: Software Development :: Libraries :: Python Modules"",\n    ],\n)\n'"
larq/__init__.py,0,"b'import larq.activations as activations\nimport larq.callbacks as callbacks\nimport larq.constraints as constraints\nimport larq.context as context\nimport larq.layers as layers\nimport larq.math as math\nimport larq.metrics as metrics\nimport larq.models as models\nimport larq.optimizers as optimizers\nimport larq.quantizers as quantizers\nimport larq.utils as utils\n\n__all__ = [\n    ""layers"",\n    ""activations"",\n    ""callbacks"",\n    ""constraints"",\n    ""context"",\n    ""math"",\n    ""metrics"",\n    ""models"",\n    ""quantizers"",\n    ""optimizers"",\n    ""utils"",\n]\n'"
larq/activations.py,7,"b'""""""Activations can either be used through an `Activation` layer, or through the\n`activation` argument supported by all forward layers:\n\n```python\nimport tensorflow as tf\nimport larq as lq\n\nmodel.add(lq.layers.QuantDense(64))\nmodel.add(tf.keras.layers.Activation(\'hard_tanh\'))\n```\n\nThis is equivalent to:\n\n```python\nmodel.add(lq.layers.QuantDense(64, activation=\'hard_tanh\'))\n```\n\nYou can also pass an element-wise TensorFlow function as an activation:\n\n```python\nmodel.add(lq.layers.QuantDense(64, activation=lq.activations.hard_tanh))\n```\n""""""\n\nimport tensorflow as tf\n\nfrom larq import utils\n\n\n@utils.register_keras_custom_object\ndef hard_tanh(x: tf.Tensor) -> tf.Tensor:\n    """"""Hard tanh activation function.\n    ```plot-activation\n    activations.hard_tanh\n    ```\n\n    # Arguments\n        x: Input tensor.\n\n    # Returns\n        Hard tanh activation.\n    """"""\n    return tf.clip_by_value(x, -1, 1)\n\n\n@utils.register_keras_custom_object\ndef leaky_tanh(x: tf.Tensor, alpha: float = 0.2) -> tf.Tensor:\n    r""""""Leaky tanh activation function.\n    Similar to hard tanh, but with non-zero slopes as in leaky ReLU.\n\n    ```plot-activation\n    activations.leaky_tanh\n    ```\n\n    # Arguments\n        x: Input tensor.\n        alpha: Slope of the activation function outside of [-1, 1].\n\n    # Returns\n        Leaky tanh activation.\n    """"""\n    return (\n        tf.clip_by_value(x, -1, 1)\n        + (tf.math.maximum(x, 1) - 1) * alpha\n        + (tf.math.minimum(x, -1) + 1) * alpha\n    )\n'"
larq/activations_test.py,7,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport larq as lq\nfrom larq.testing_utils import generate_real_values_with_zeros\n\n\n@pytest.mark.parametrize(""name"", [""hard_tanh"", ""leaky_tanh""])\ndef test_serialization(name):\n    fn = tf.keras.activations.get(name)\n    ref_fn = getattr(lq.activations, name)\n    assert fn == ref_fn\n    config = tf.keras.activations.serialize(fn)\n    fn = tf.keras.activations.deserialize(config)\n    assert fn == ref_fn\n\n\ndef test_hard_tanh():\n    real_values = generate_real_values_with_zeros()\n    x = tf.keras.backend.placeholder(ndim=2)\n    f = tf.keras.backend.function([x], [lq.activations.hard_tanh(x)])\n    result = f([real_values])[0]\n    np.testing.assert_allclose(result, np.clip(real_values, -1, 1))\n\n\ndef test_leaky_tanh():\n    @np.vectorize\n    def leaky_tanh(x, alpha):\n        if x <= -1:\n            return -1 + alpha * (x + 1)\n        elif x <= 1:\n            return x\n        else:\n            return 1 + alpha * (x - 1)\n\n    real_values = generate_real_values_with_zeros()\n    x = tf.keras.backend.placeholder(ndim=2)\n    f = tf.keras.backend.function([x], [lq.activations.leaky_tanh(x)])\n    result = f([real_values])[0]\n    np.testing.assert_allclose(result, leaky_tanh(real_values, alpha=0.2))\n'"
larq/callbacks.py,1,"b'from typing import Any, Callable, MutableMapping, Optional\n\nfrom tensorflow import keras\n\n\nclass HyperparameterScheduler(keras.callbacks.Callback):\n    """"""Generic hyperparameter scheduler.\n\n    !!! example\n        ```python\n        bop = lq.optimizers.Bop(threshold=1e-6, gamma=1e-3)\n        adam = tf.keras.optimizers.Adam(0.01)\n        optimizer = lq.optimizers.CaseOptimizer(\n            (lq.optimizers.Bop.is_binary_variable, bop), default_optimizer=adam,\n        )\n        callbacks = [\n            HyperparameterScheduler(lambda x: 0.001 * (0.1 ** (x // 30)), ""gamma"", bop)\n        ]\n        ```\n\n    # Arguments\n        schedule: a function that takes an epoch index as input\n            (integer, indexed from 0) and returns a new hyperparameter as output.\n        hyperparameter: str. the name of the hyperparameter to be scheduled.\n        optimizer: the optimizer that contains the hyperparameter that will be scheduled.\n            Defaults to `self.model.optimizer` if `optimizer == None`.\n        update_freq: str (optional), denotes on what update_freq to change the\n            hyperparameter. Can be either ""epoch"" (default) or ""step"".\n        verbose: int. 0: quiet, 1: update messages.\n        log_name: str (optional), under which name to log this hyperparameter to\n            Tensorboard. If `None`, defaults to `hyperparameter`. Use this if you have\n            several schedules for the same hyperparameter on different optimizers.\n    """"""\n\n    def __init__(\n        self,\n        schedule: Callable,\n        hyperparameter: str,\n        optimizer: Optional[keras.optimizers.Optimizer] = None,\n        update_freq: str = ""epoch"",\n        verbose: int = 0,\n        log_name: Optional[str] = None,\n    ):\n        super(HyperparameterScheduler, self).__init__()\n        self.optimizer = optimizer\n        self.schedule = schedule\n        self.hyperparameter = hyperparameter\n        self.log_name = log_name or hyperparameter\n        self.verbose = verbose\n\n        if update_freq not in [""epoch"", ""step""]:\n            raise ValueError(\n                ""HyperparameterScheduler.update_freq can only be \'step\' or \'epoch\'.""\n                f"" Received value \'{update_freq}\'""\n            )\n\n        self.update_freq = update_freq\n\n    def set_model(self, model: keras.models.Model) -> None:\n        super().set_model(model)\n        if self.optimizer is None:\n            # It is not possible for a model to reach this state and not have\n            # an optimizer, so we can safely access it here.\n            self.optimizer = model.optimizer\n\n        if not hasattr(self.optimizer, self.hyperparameter):\n            raise ValueError(\n                f\'Optimizer must have a ""{self.hyperparameter}"" attribute.\'\n            )\n\n    def set_hyperparameter(self, t: int) -> Any:\n        hp = getattr(self.optimizer, self.hyperparameter)\n        try:  # new API\n            hyperparameter_val = keras.backend.get_value(hp)\n            hyperparameter_val = self.schedule(t, hyperparameter_val)\n        except TypeError:  # Support for old API for backward compatibility\n            hyperparameter_val = self.schedule(t)\n        keras.backend.set_value(hp, hyperparameter_val)\n        return hp\n\n    def on_batch_begin(\n        self, batch: int, logs: Optional[MutableMapping[str, Any]] = None\n    ) -> None:\n        if not self.update_freq == ""step"":\n            return\n\n        # We use optimizer.iterations (i.e. global step), since batch only\n        # reflects the batch index in the current epoch.\n        batch = keras.backend.get_value(self.optimizer.iterations)\n        hp = self.set_hyperparameter(batch)\n\n        if self.verbose > 0:\n            print(\n                f""Batch {batch}: {self.log_name} is now {keras.backend.get_value(hp)}.""\n            )\n\n    def on_epoch_begin(\n        self, epoch: int, logs: Optional[MutableMapping[str, Any]] = None\n    ) -> None:\n        if not self.update_freq == ""epoch"":\n            return\n\n        hp = self.set_hyperparameter(epoch)\n\n        if self.verbose > 0:\n            print(\n                f""Epoch {epoch}: {self.log_name} is now {keras.backend.get_value(hp)}.""\n            )\n\n    def on_epoch_end(\n        self, epoch: int, logs: Optional[MutableMapping[str, Any]] = None\n    ) -> None:\n        logs = logs or {}\n        hp = getattr(self.optimizer, self.hyperparameter)\n        logs[self.log_name] = keras.backend.get_value(hp)\n'"
larq/callbacks_test.py,10,"b'import math\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom tensorflow.python.keras import testing_utils\n\nimport larq as lq\nfrom larq import testing_utils as lq_testing_utils\nfrom larq.callbacks import HyperparameterScheduler\n\n\nclass TestHyperparameterScheduler:\n    def _create_data_and_model(self, train_samples=1000):\n        np.random.seed(1337)\n        (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n            train_samples=train_samples,\n            test_samples=0,\n            input_shape=(10,),\n            num_classes=2,\n        )\n        y_train = tf.keras.utils.to_categorical(y_train)\n\n        model = lq_testing_utils.get_small_bnn_model(\n            x_train.shape[1], 20, y_train.shape[1]\n        )\n\n        return x_train, y_train, model\n\n    def test_normal_optimizer(self):\n        x_train, y_train, model = self._create_data_and_model()\n\n        model.compile(\n            loss=""categorical_crossentropy"",\n            optimizer=tf.keras.optimizers.Adam(0.01),\n            metrics=[""accuracy""],\n        )\n\n        def scheduler(x):\n            return 1.0 / (1.0 + x)\n\n        # We shouldn\' t need to specify the optimizer\n        test_scheduler = HyperparameterScheduler(\n            schedule=scheduler, hyperparameter=""lr"", verbose=1,\n        )\n\n        num_epochs = 2\n        model.fit(\n            x_train,\n            y_train,\n            epochs=num_epochs,\n            batch_size=16,\n            callbacks=[test_scheduler],\n            verbose=0,\n        )\n\n        np.testing.assert_almost_equal(\n            tf.keras.backend.get_value(model.optimizer.lr),\n            scheduler(num_epochs - 1),\n            decimal=8,\n        )\n\n    def test_per_step(self):\n        train_samples = 20\n        x_train, y_train, model = self._create_data_and_model(train_samples)\n\n        model.compile(\n            loss=""categorical_crossentropy"",\n            optimizer=tf.keras.optimizers.Adam(0.01),\n            metrics=[""accuracy""],\n        )\n\n        def scheduler(x):\n            return 1.0 / (1.0 + x)\n\n        # Test that we don\'t accept incorrect `update_freq`\n        with pytest.raises(ValueError):\n            HyperparameterScheduler(\n                schedule=scheduler, hyperparameter=""lr"", update_freq=""wrong"",\n            )\n\n        # The actual scheduler we\'ll use\n        test_scheduler = HyperparameterScheduler(\n            schedule=scheduler, hyperparameter=""lr"", update_freq=""step"", verbose=1,\n        )\n\n        num_epochs = 1\n        batch_size = 10\n        model.fit(\n            x_train,\n            y_train,\n            epochs=num_epochs,\n            batch_size=16,\n            callbacks=[test_scheduler],\n            verbose=0,\n        )\n\n        np.testing.assert_almost_equal(\n            tf.keras.backend.get_value(model.optimizer.lr),\n            scheduler(math.ceil(train_samples / batch_size) - 1),\n            decimal=8,\n        )\n\n    def test_case_optimizer(self):\n        x_train, y_train, model = self._create_data_and_model()\n\n        bop = lq.optimizers.Bop(threshold=1e-6, gamma=1e-3)\n        adam = tf.keras.optimizers.Adam(0.01)\n        case_optimizer = lq.optimizers.CaseOptimizer(\n            (lq.optimizers.Bop.is_binary_variable, bop), default_optimizer=adam,\n        )\n\n        model.compile(\n            loss=""categorical_crossentropy"",\n            optimizer=case_optimizer,\n            metrics=[""accuracy""],\n        )\n\n        def scheduler(x):\n            return 1.0 / (1.0 + x)\n\n        cbk_gamma_scheduler = HyperparameterScheduler(\n            schedule=scheduler,\n            optimizer=model.optimizer.optimizers[0],\n            hyperparameter=""gamma"",\n            verbose=1,\n        )\n        cbk_threshold_scheduler = HyperparameterScheduler(\n            schedule=scheduler,\n            optimizer=model.optimizer.optimizers[0],\n            hyperparameter=""threshold"",\n            verbose=1,\n        )\n        cbk_lr_scheduler = HyperparameterScheduler(\n            schedule=scheduler,\n            optimizer=model.optimizer.optimizers[1],\n            hyperparameter=""lr"",\n            verbose=1,\n        )\n\n        num_epochs = 3\n        model.fit(\n            x_train,\n            y_train,\n            epochs=num_epochs,\n            batch_size=16,\n            callbacks=[cbk_gamma_scheduler, cbk_lr_scheduler, cbk_threshold_scheduler],\n            verbose=0,\n        )\n\n        np.testing.assert_almost_equal(\n            tf.keras.backend.get_value(model.optimizer.optimizers[0].gamma),\n            scheduler(num_epochs - 1),\n            decimal=8,\n        )\n\n        np.testing.assert_almost_equal(\n            tf.keras.backend.get_value(model.optimizer.optimizers[0].threshold),\n            scheduler(num_epochs - 1),\n            decimal=8,\n        )\n\n        np.testing.assert_almost_equal(\n            tf.keras.backend.get_value(model.optimizer.optimizers[1].lr),\n            scheduler(num_epochs - 1),\n            decimal=8,\n        )\n\n    def test_wrong_param(self):\n        x_train, y_train, model = self._create_data_and_model()\n\n        model.compile(\n            loss=""categorical_crossentropy"",\n            optimizer=tf.keras.optimizers.Adam(0.01),\n            metrics=[""accuracy""],\n        )\n\n        def scheduler(x):\n            return 1.0 / (1.0 + x)\n\n        wrong_scheduler = HyperparameterScheduler(\n            schedule=scheduler, hyperparameter=""invalid_param"", verbose=1,\n        )\n\n        with pytest.raises(ValueError):\n            model.fit(\n                x_train,\n                y_train,\n                epochs=1,\n                batch_size=16,\n                callbacks=[wrong_scheduler],\n                verbose=0,\n            )\n'"
larq/conftest.py,7,"b'import pytest\nimport tensorflow as tf\nfrom packaging import version\nfrom tensorflow.python.eager import context\n\nfrom larq import context as lq_context\n\n\n@pytest.fixture\ndef eager_mode():\n    """"""pytest fixture for running test in eager mode""""""\n    with context.eager_mode():\n        yield\n\n\n@pytest.fixture\ndef graph_mode():\n    """"""pytest fixture for running test in graph mode""""""\n    with context.graph_mode():\n        with tf.compat.v1.Session().as_default():\n            yield\n            tf.keras.backend.clear_session()\n\n\n@pytest.fixture(params=[""eager"", ""graph""])\ndef eager_and_graph_mode(request):\n    """"""pytest fixture for running test in eager and graph mode""""""\n    if request.param == ""graph"":\n        with context.graph_mode():\n            with tf.compat.v1.Session().as_default():\n                yield request.param\n                tf.keras.backend.clear_session()\n    else:\n        with context.eager_mode():\n            yield request.param\n\n\n@pytest.fixture(params=[""graph"", ""tf_eager"", ""tf_keras_eager""])\ndef keras_should_run_eagerly(request):\n    """"""Fixture to run in graph and two eager modes.\n\n    The modes are:\n    - Graph mode\n    - TensorFlow eager and Keras eager\n    - TensorFlow eager and Keras not eager\n\n    The `tf.context` sets graph/eager mode for TensorFlow. The yield is True if Keras\n    should run eagerly.\n    """"""\n\n    if request.param == ""graph"":\n        if version.parse(tf.__version__) >= version.parse(""2""):\n            pytest.skip(""Skipping graph mode for TensorFlow 2+."")\n\n        with context.graph_mode():\n            yield\n    else:\n        with context.eager_mode():\n            yield request.param == ""tf_keras_eager""\n\n\n@pytest.fixture(params=[False, True])\ndef distribute_scope(request):\n    if request.param is True:\n        with tf.distribute.MirroredStrategy([""cpu:0""]).scope():\n            yield request.param\n    else:\n        yield request.param\n\n\n@pytest.fixture(params=[True, False])\ndef quantized(request):\n    """"""pytest fixture for running test quantized and non-quantized""""""\n    with lq_context.quantized_scope(request.param):\n        yield request.param\n'"
larq/conftest_test.py,7,"b'import tensorflow as tf\n\nfrom larq import context\n\n\ndef test_eager_and_graph_mode_fixture(eager_and_graph_mode):\n    if eager_and_graph_mode == ""eager"":\n        assert tf.executing_eagerly()\n    else:\n        assert not tf.executing_eagerly()\n        assert tf.compat.v1.get_default_session() is not None\n\n\ndef test_eager_mode_fixture(eager_mode):\n    assert tf.executing_eagerly()\n\n\ndef test_graph_mode_fixture(graph_mode):\n    assert not tf.executing_eagerly()\n    assert tf.compat.v1.get_default_session() is not None\n\n\ndef test_distribute_scope(distribute_scope):\n    assert tf.distribute.has_strategy() is distribute_scope\n\n\ndef test_quantize_scope(quantized):\n    assert context.should_quantize() == quantized\n'"
larq/constraints.py,3,"b'""""""Functions from the `constraints` module allow setting constraints\n(eg. weight clipping) on network parameters during optimization.\n\nThe penalties are applied on a per-layer basis. The exact API will depend on the layer,\nbut the layers `QuantDense`, `QuantConv1D`, `QuantConv2D` and `QuantConv3D` have a\nunified API.\n\nThese layers expose 2 keyword arguments:\n\n- `kernel_constraint` for the main weights matrix\n- `bias_constraint` for the bias.\n\n```python\nimport larq as lq\n\nlq.layers.QuantDense(64, kernel_constraint=""weight_clip"")\nlq.layers.QuantDense(64, kernel_constraint=lq.constraints.WeightClip(2.))\n```\n""""""\n\nfrom typing import Any, Mapping\n\nimport tensorflow as tf\n\nfrom larq import utils\n\n\n@utils.register_keras_custom_object\nclass WeightClip(tf.keras.constraints.Constraint):\n    """"""Weight Clip constraint\n\n    Constrains the weights incident to each hidden unit\n    to be between `[-clip_value, clip_value]`.\n\n    # Arguments\n        clip_value: The value to clip incoming weights.\n    """"""\n\n    def __init__(self, clip_value: float = 1):\n        self.clip_value = clip_value\n\n    def __call__(self, x: tf.Tensor) -> tf.Tensor:\n        return tf.clip_by_value(x, -self.clip_value, self.clip_value)\n\n    def get_config(self) -> Mapping[str, Any]:\n        return {""clip_value"": self.clip_value}\n\n\n# Aliases\n@utils.register_keras_custom_object\nclass weight_clip(WeightClip):\n    pass\n'"
larq/constraints_test.py,5,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport larq as lq\nfrom larq.testing_utils import generate_real_values_with_zeros\n\n\n@pytest.mark.parametrize(""name"", [""weight_clip""])\ndef test_serialization(name):\n    fn = tf.keras.constraints.get(name)\n    ref_fn = getattr(lq.constraints, name)()\n    assert fn.__class__ == ref_fn.__class__\n    config = tf.keras.constraints.serialize(fn)\n    fn = tf.keras.constraints.deserialize(config)\n    assert fn.__class__ == ref_fn.__class__\n\n\ndef test_clip():\n    real_values = generate_real_values_with_zeros()\n    clip_instance = lq.constraints.weight_clip(clip_value=0.75)\n    result = clip_instance(tf.keras.backend.variable(real_values))\n    result = tf.keras.backend.eval(result)\n    np.testing.assert_allclose(result, np.clip(real_values, -0.75, 0.75))\n'"
larq/context.py,1,"b'""""""Context managers that configure global behaviour of Larq.""""""\n\nimport contextlib\nimport threading\n\n__all__ = [\n    ""metrics_scope"",\n    ""quantized_scope"",\n    ""get_training_metrics"",\n    ""should_quantize"",\n]\n\n\n_quantized_scope = threading.local()\n_quantized_scope.should_quantize = False\n\n\n@contextlib.contextmanager\ndef quantized_scope(quantize):\n    """"""A context manager to define the behaviour of `QuantizedVariable`.\n\n    !!! example\n        ```python\n        model.save(""full_precision_model.h5"")  # save full precision latent weights\n        fp_weights = model.get_weights()  # get latent weights\n\n        with larq.context.quantized_scope(True):\n            model.save(""binary_model.h5"")  # save binarized weights\n            weights = model.get_weights()  # get binarized weights\n        ```\n\n    # Arguments\n        quantize: If `should_quantize` is `True`, `QuantizedVariable` will return their\n            quantized value in the forward pass. If `False`, `QuantizedVariable` will\n            act as a latent variable.\n    """"""\n    backup = should_quantize()\n    _quantized_scope.should_quantize = quantize\n    yield quantize\n    _quantized_scope.should_quantize = backup\n\n\ndef should_quantize():\n    """"""Returns the current quantized scope.""""""\n    return getattr(_quantized_scope, ""should_quantize"", False)\n\n\n_global_training_metrics = set()\n_available_metrics = {""flip_ratio""}\n\n\n@contextlib.contextmanager\ndef metrics_scope(metrics=[]):\n    """"""A context manager to set the training metrics to be used in quantizers.\n\n    !!! example\n        ```python\n        with larq.context.metrics_scope([""flip_ratio""]):\n            model = tf.keras.models.Sequential(\n                [larq.layers.QuantDense(3, kernel_quantizer=""ste_sign"", input_shape=(32,))]\n            )\n        model.compile(loss=""mse"", optimizer=""sgd"")\n        ```\n\n    # Arguments\n        metrics: Iterable of metrics to add to quantizers defined inside this context.\n            Currently only the `flip_ratio` metric is available.\n    """"""\n    for metric in metrics:\n        if metric not in _available_metrics:\n            raise ValueError(\n                f""Unknown training metric \'{metric}\'. Available metrics: {_available_metrics}.""\n            )\n    backup = _global_training_metrics.copy()\n    _global_training_metrics.update(metrics)\n    yield _global_training_metrics\n    _global_training_metrics.clear()\n    _global_training_metrics.update(backup)\n\n\ndef get_training_metrics():\n    """"""Retrieves a live reference to the training metrics in the current scope.\n\n    Updating and clearing training metrics using `larq.context.metrics_scope` is\n    preferred, but `get_training_metrics` can be used to directly access them.\n\n    !!! example\n        ```python\n        get_training_metrics().clear()\n        get_training_metrics().add(""flip_ratio"")\n        ```\n\n    # Returns\n        A set of training metrics in the current scope.\n    """"""\n    return _global_training_metrics\n'"
larq/context_test.py,0,"b'import pytest\n\nfrom larq import context\n\n\ndef test_scope():\n    assert context.get_training_metrics() == set()\n    with context.metrics_scope([""flip_ratio""]):\n        assert context.get_training_metrics() == {""flip_ratio""}\n    assert context.get_training_metrics() == set()\n    with pytest.raises(ValueError, match=r"".*unknown_metric.*""):\n        with context.metrics_scope([""flip_ratio"", ""unknown_metric""]):\n            pass\n'"
larq/layers.py,13,"b'""""""Each Quantized Layer requires a `input_quantizer` and `kernel_quantizer` that\ndescribes the way of quantizing the activation of the previous layer and the weights\nrespectively.\n\nIf both `input_quantizer` and `kernel_quantizer` are `None` the layer\nis equivalent to a full precision layer.\n""""""\n\nimport tensorflow as tf\n\nfrom larq import utils\nfrom larq.layers_base import (\n    QuantizerBase,\n    QuantizerBaseConv,\n    QuantizerDepthwiseBase,\n    QuantizerSeparableBase,\n)\n\n\n@utils.register_keras_custom_object\nclass QuantDense(QuantizerBase, tf.keras.layers.Dense):\n    """"""Just your regular densely-connected quantized NN layer.\n\n    `QuantDense` implements the operation:\n    `output = activation(dot(input_quantizer(input), kernel_quantizer(kernel)) + bias)`,\n    where `activation` is the element-wise activation function passed as the\n    `activation` argument, `kernel` is a weights matrix created by the layer, and `bias`\n    is a bias vector created by the layer (only applicable if `use_bias` is `True`).\n    `input_quantizer` and `kernel_quantizer` are the element-wise quantization\n    functions to use. If both quantization functions are `None` this layer is\n    equivalent to `Dense`.\n\n    !!! note """"\n        If the input to the layer has a rank greater than 2, then it is flattened\n        prior to the initial dot product with `kernel`.\n\n    !!! example\n        ```python\n        # as first layer in a sequential model:\n        model = Sequential()\n        model.add(\n            QuantDense(\n                32,\n                input_quantizer=""ste_sign"",\n                kernel_quantizer=""ste_sign"",\n                kernel_constraint=""weight_clip"",\n                input_shape=(16,),\n            )\n        )\n        # now the model will take as input arrays of shape (*, 16)\n        # and output arrays of shape (*, 32)\n\n        # after the first layer, you don\'t need to specify\n        # the size of the input anymore:\n        model.add(\n            QuantDense(\n                32,\n                input_quantizer=""ste_sign"",\n                kernel_quantizer=""ste_sign"",\n                kernel_constraint=""weight_clip"",\n            )\n        )\n        ```\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use. If you don\'t specify anything,\n            no activation is applied (`a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        input_quantizer: Quantization function applied to the input of the layer.\n        kernel_quantizer: Quantization function applied to the `kernel` weights matrix.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer function applied to the `kernel` weights matrix.\n        bias_regularizer: Regularizer function applied to the bias vector.\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its ""activation"").\n        kernel_constraint: Constraint function applied to the `kernel` weights matrix.\n        bias_constraint: Constraint function applied to the bias vector.\n\n    # Input shape\n        N-D tensor with shape: `(batch_size, ..., input_dim)`. The most common situation\n        would be a 2D input with shape `(batch_size, input_dim)`.\n\n    # Output shape\n        N-D tensor with shape: `(batch_size, ..., units)`. For instance, for a 2D input\n        with shape `(batch_size, input_dim)`, the output would have shape\n        `(batch_size, units)`.\n    """"""\n\n    def __init__(\n        self,\n        units,\n        activation=None,\n        use_bias=True,\n        input_quantizer=None,\n        kernel_quantizer=None,\n        kernel_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n        super().__init__(\n            units,\n            activation=activation,\n            use_bias=use_bias,\n            input_quantizer=input_quantizer,\n            kernel_quantizer=kernel_quantizer,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n\n\n@utils.register_keras_custom_object\nclass QuantConv1D(QuantizerBase, QuantizerBaseConv, tf.keras.layers.Conv1D):\n    """"""1D quantized convolution layer (e.g. temporal convolution).\n\n    This layer creates a convolution kernel that is convolved with the layer input\n    over a single spatial (or temporal) dimension to produce a tensor of outputs.\n    `input_quantizer` and `kernel_quantizer` are the element-wise quantization\n    functions to use. If both quantization functions are `None` this layer is\n    equivalent to `Conv1D`.\n    If `use_bias` is True, a bias vector is created and added to the outputs.\n    Finally, if `activation` is not `None`, it is applied to the outputs as well.\n\n    When using this layer as the first layer in a model, provide an `input_shape`\n    argument (tuple of integers or `None`, e.g. `(10, 128)` for sequences of\n    10 vectors of 128-dimensional vectors, or `(None, 128)` for variable-length\n    sequences of 128-dimensional vectors.\n\n    # Arguments\n        filters: Integer, the dimensionality of the output space\n            (i.e. the number of output filters in the convolution).\n        kernel_size: An integer or tuple/list of a single integer,\n            specifying the length of the 1D convolution window.\n        strides: An integer or tuple/list of a single integer, specifying the stride\n            length of the convolution. Specifying any stride value != 1 is incompatible\n            with specifying any `dilation_rate` value != 1.\n        padding: One of `""valid""`, `""causal""` or `""same""` (case-insensitive). `""causal""`\n            results in causal (dilated) convolutions, e.g. output[t] does not depend on\n            input[t+1:]. Useful when modeling temporal data where the model should not\n            violate the temporal order. See [WaveNet: A Generative Model for Raw Audio,\n                section 2.1](https://arxiv.org/abs/1609.03499).\n        pad_values: The pad value to use when `padding=""same""`.\n        data_format: A string, one of `channels_last` (default) or `channels_first`.\n        dilation_rate: an integer or tuple/list of a single integer, specifying the\n            dilation rate to use for dilated convolution. Currently, specifying any\n            `dilation_rate` value != 1 is incompatible with specifying any `strides`\n            value != 1.\n        activation: Activation function to use. If you don\'t specify anything, no\n            activation is applied (`a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        input_quantizer: Quantization function applied to the input of the layer.\n        kernel_quantizer: Quantization function applied to the `kernel` weights matrix.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer function applied to the `kernel` weights matrix.\n        bias_regularizer: Regularizer function applied to the bias vector.\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its ""activation"").\n        kernel_constraint: Constraint function applied to the kernel matrix.\n        bias_constraint: Constraint function applied to the bias vector.\n\n    # Input shape\n        3D tensor with shape: `(batch_size, steps, input_dim)`\n\n    # Output shape\n        3D tensor with shape: `(batch_size, new_steps, filters)`.\n        `steps` value might have changed due to padding or strides.\n    """"""\n\n    def __init__(\n        self,\n        filters,\n        kernel_size,\n        strides=1,\n        padding=""valid"",\n        pad_values=0.0,\n        data_format=""channels_last"",\n        dilation_rate=1,\n        activation=None,\n        use_bias=True,\n        input_quantizer=None,\n        kernel_quantizer=None,\n        kernel_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n        super().__init__(\n            filters,\n            kernel_size,\n            strides=strides,\n            padding=padding,\n            pad_values=pad_values,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            activation=activation,\n            use_bias=use_bias,\n            input_quantizer=input_quantizer,\n            kernel_quantizer=kernel_quantizer,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n\n\n@utils.register_keras_custom_object\nclass QuantConv2D(QuantizerBase, QuantizerBaseConv, tf.keras.layers.Conv2D):\n    """"""2D quantized convolution layer (e.g. spatial convolution over images).\n\n    This layer creates a convolution kernel that is convolved\n    with the layer input to produce a tensor of outputs.\n    `input_quantizer` and `kernel_quantizer` are the element-wise quantization\n    functions to use. If both quantization functions are `None` this layer is\n    equivalent to `Conv2D`. If `use_bias` is True, a bias vector is created\n    and added to the outputs. Finally, if `activation` is not `None`,\n    it is applied to the outputs as well.\n\n    When using this layer as the first layer in a model, provide the keyword argument\n    `input_shape` (tuple of integers, does not include the sample axis),\n    e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures in\n    `data_format=""channels_last""`.\n\n    # Arguments\n        filters: Integer, the dimensionality of the output space\n            (i.e. the number of output filters in the convolution).\n        kernel_size: An integer or tuple/list of 2 integers, specifying the\n            height and width of the 2D convolution window. Can be a single integer\n            to specify the same value for all spatial dimensions.\n        strides: An integer or tuple/list of 2 integers, specifying the strides of\n            the convolution along the height and width. Can be a single integer to\n            specify the same value for all spatial dimensions. Specifying any stride\n            value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n        padding: one of `""valid""` or `""same""` (case-insensitive).\n        pad_values: The pad value to use when `padding=""same""`.\n        data_format: A string, one of `channels_last` (default) or `channels_first`.\n            The ordering of the dimensions in the inputs. `channels_last` corresponds to\n            inputs with shape `(batch, height, width, channels)` while `channels_first`\n            corresponds to inputs with shape `(batch, channels, height, width)`. It\n            defaults to the `image_data_format` value found in your Keras config file at\n            `~/.keras/keras.json`. If you never set it, then it will be ""channels_last"".\n        dilation_rate: an integer or tuple/list of 2 integers, specifying the dilation\n            rate to use for dilated convolution. Can be a single integer to specify the\n            same value for all spatial dimensions. Currently, specifying any\n            `dilation_rate` value != 1 is incompatible with specifying any stride value\n            != 1.\n        activation: Activation function to use. If you don\'t specify anything,\n            no activation is applied (`a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        input_quantizer: Quantization function applied to the input of the layer.\n        kernel_quantizer: Quantization function applied to the `kernel` weights matrix.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer function applied to the `kernel` weights matrix.\n        bias_regularizer: Regularizer function applied to the bias vector.\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its ""activation"").\n        kernel_constraint: Constraint function applied to the kernel matrix.\n        bias_constraint: Constraint function applied to the bias vector.\n\n    # Input shape\n        4D tensor with shape:\n        `(samples, channels, rows, cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, rows, cols, channels)` if data_format=\'channels_last\'.\n\n    # Output shape\n        4D tensor with shape:\n        `(samples, filters, new_rows, new_cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, new_rows, new_cols, filters)` if data_format=\'channels_last\'.\n        `rows` and `cols` values might have changed due to padding.\n    """"""\n\n    def __init__(\n        self,\n        filters,\n        kernel_size,\n        strides=(1, 1),\n        padding=""valid"",\n        pad_values=0.0,\n        data_format=None,\n        dilation_rate=(1, 1),\n        activation=None,\n        use_bias=True,\n        input_quantizer=None,\n        kernel_quantizer=None,\n        kernel_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n        super().__init__(\n            filters,\n            kernel_size,\n            strides=strides,\n            padding=padding,\n            pad_values=pad_values,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            activation=activation,\n            use_bias=use_bias,\n            input_quantizer=input_quantizer,\n            kernel_quantizer=kernel_quantizer,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n\n\n@utils.register_keras_custom_object\nclass QuantConv3D(QuantizerBase, QuantizerBaseConv, tf.keras.layers.Conv3D):\n    """"""3D convolution layer (e.g. spatial convolution over volumes).\n\n    This layer creates a convolution kernel that is convolved\n    with the layer input to produce a tensor of\n    outputs. `input_quantizer` and `kernel_quantizer` are the element-wise quantization\n    functions to use. If both quantization functions are `None` this layer is\n    equivalent to `Conv3D`. If `use_bias` is True, a bias vector is created and\n    added to the outputs. Finally, if `activation` is not `None`,\n    it is applied to the outputs as well.\n\n    When using this layer as the first layer in a model, provide the keyword argument\n    `input_shape` (tuple of integers, does not include the sample axis),\n    e.g. `input_shape=(128, 128, 128, 1)` for 128x128x128 volumes\n    with a single channel, in `data_format=""channels_last""`.\n\n    # Arguments\n        filters: Integer, the dimensionality of the output space\n            (i.e. the number of output filters in the convolution).\n        kernel_size: An integer or tuple/list of 3 integers, specifying the\n            depth, height and width of the 3D convolution window. Can be a single\n            integer to specify the same value for all spatial dimensions.\n        strides: An integer or tuple/list of 3 integers, specifying the strides of the\n            convolution along each spatial dimension. Can be a single integer to specify\n            the same value for all spatial dimensions. Specifying any stride value != 1\n            is incompatible with specifying any `dilation_rate` value != 1.\n        padding: one of `""valid""` or `""same""` (case-insensitive).\n        pad_values: The pad value to use when `padding=""same""`.\n        data_format: A string, one of `channels_last` (default) or `channels_first`.\n            The ordering of the dimensions in the inputs. `channels_last` corresponds to\n            inputs with shape\n            `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)` while\n            `channels_first` corresponds to inputs with shape\n            `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`. It defaults\n            to the `image_data_format` value found in your Keras config file at\n            `~/.keras/keras.json`. If you never set it, then it will be ""channels_last"".\n        dilation_rate: an integer or tuple/list of 2 integers, specifying the dilation\n            rate to use for dilated convolution. Can be a single integer to specify the\n            same value for all spatial dimensions. Currently, specifying any\n            `dilation_rate` value != 1 is incompatible with specifying any stride value\n            != 1.\n        activation: Activation function to use. If you don\'t specify anything,\n            no activation is applied (`a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        input_quantizer: Quantization function applied to the input of the layer.\n        kernel_quantizer: Quantization function applied to the `kernel` weights matrix.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer function applied to the `kernel` weights matrix.\n        bias_regularizer: Regularizer function applied to the bias vector.\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its ""activation"").\n        kernel_constraint: Constraint function applied to the kernel matrix.\n        bias_constraint: Constraint function applied to the bias vector.\n\n    # Input shape\n        5D tensor with shape:\n        `(samples, channels, conv_dim1, conv_dim2, conv_dim3)` if\n            data_format=\'channels_first\'\n        or 5D tensor with shape:\n        `(samples, conv_dim1, conv_dim2, conv_dim3, channels)` if\n            data_format=\'channels_last\'.\n\n    # Output shape\n        5D tensor with shape:\n        `(samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3)` if\n            data_format=\'channels_first\'\n        or 5D tensor with shape:\n        `(samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters)` if\n            data_format=\'channels_last\'.\n        `new_conv_dim1`, `new_conv_dim2` and `new_conv_dim3` values might have\n            changed due to padding.\n    """"""\n\n    def __init__(\n        self,\n        filters,\n        kernel_size,\n        strides=(1, 1, 1),\n        padding=""valid"",\n        pad_values=0.0,\n        data_format=None,\n        dilation_rate=(1, 1, 1),\n        activation=None,\n        use_bias=True,\n        input_quantizer=None,\n        kernel_quantizer=None,\n        kernel_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n        super().__init__(\n            filters,\n            kernel_size,\n            strides=strides,\n            padding=padding,\n            pad_values=pad_values,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            activation=activation,\n            use_bias=use_bias,\n            input_quantizer=input_quantizer,\n            kernel_quantizer=kernel_quantizer,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n\n\n@utils.register_keras_custom_object\nclass QuantDepthwiseConv2D(\n    QuantizerDepthwiseBase, QuantizerBaseConv, tf.keras.layers.DepthwiseConv2D\n):\n    """"""""Quantized depthwise separable 2D convolution.\n    Depthwise Separable convolutions consists in performing just the first step in a\n    depthwise spatial convolution (which acts on each input channel separately).\n    The `depth_multiplier` argument controls how many output channels are generated per\n    input channel in the depthwise step.\n\n    # Arguments\n        kernel_size: An integer or tuple/list of 2 integers, specifying the height and\n            width of the 2D convolution window. Can be a single integer to specify the\n            same value for all spatial dimensions.\n        strides: An integer or tuple/list of 2 integers, specifying the strides of the\n            convolution along the height and width. Can be a single integer to specify\n            the same value for all spatial dimensions. Specifying any stride value != 1\n            is incompatible with specifying any `dilation_rate` value != 1.\n        padding: one of `\'valid\'` or `\'same\'` (case-insensitive).\n        pad_values: The pad value to use when `padding=""same""`.\n        depth_multiplier: The number of depthwise convolution output channels for each\n            input channel. The total number of depthwise convolution output channels\n            will be equal to `filters_in * depth_multiplier`.\n        data_format: A string, one of `channels_last` (default) or `channels_first`.\n            The ordering of the dimensions in the inputs. `channels_last` corresponds to\n            inputs with shape `(batch, height, width, channels)` while `channels_first`\n            corresponds to inputs with shape `(batch, channels, height, width)`.\n            It defaults to the `image_data_format` value found in your\n            Keras config file at `~/.keras/keras.json`.\n            If you never set it, then it will be \'channels_last\'.\n        activation: Activation function to use.\n            If you don\'t specify anything, no activation is applied (ie. `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        input_quantizer: Quantization function applied to the input of the layer.\n        depthwise_quantizer: Quantization function applied to the `depthwise_kernel`\n            weights matrix.\n        depthwise_initializer: Initializer for the depthwise kernel matrix.\n        bias_initializer: Initializer for the bias vector.\n        depthwise_regularizer: Regularizer function applied to the depthwise kernel\n            matrix.\n        bias_regularizer: Regularizer function applied to the bias vector.\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \'activation\').\n        depthwise_constraint: Constraint function applied to the depthwise kernel\n            matrix.\n        bias_constraint: Constraint function applied to the bias vector.\n\n    # Input shape\n        4D tensor with shape:\n        `[batch, channels, rows, cols]` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `[batch, rows, cols, channels]` if data_format=\'channels_last\'.\n\n    # Output shape\n        4D tensor with shape:\n        `[batch, filters, new_rows, new_cols]` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `[batch, new_rows, new_cols, filters]` if data_format=\'channels_last\'.\n        `rows` and `cols` values might have changed due to padding.\n    """"""\n\n    def __init__(\n        self,\n        kernel_size,\n        strides=(1, 1),\n        padding=""valid"",\n        pad_values=0.0,\n        depth_multiplier=1,\n        data_format=None,\n        activation=None,\n        use_bias=True,\n        input_quantizer=None,\n        depthwise_quantizer=None,\n        depthwise_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        depthwise_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        depthwise_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n        super().__init__(\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            pad_values=pad_values,\n            depth_multiplier=depth_multiplier,\n            data_format=data_format,\n            activation=activation,\n            use_bias=use_bias,\n            input_quantizer=input_quantizer,\n            depthwise_quantizer=depthwise_quantizer,\n            depthwise_initializer=depthwise_initializer,\n            bias_initializer=bias_initializer,\n            depthwise_regularizer=depthwise_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            depthwise_constraint=depthwise_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n\n\n@utils.register_keras_custom_object\nclass QuantSeparableConv1D(\n    QuantizerSeparableBase, QuantizerBaseConv, tf.keras.layers.SeparableConv1D\n):\n    """"""Depthwise separable 1D quantized convolution.\n\n    This layer performs a depthwise convolution that acts separately on channels,\n    followed by a pointwise convolution that mixes channels.\n    `input_quantizer`, `depthwise_quantizer` and `pointwise_quantizer` are the\n    element-wise quantization functions to use. If all quantization functions are `None`\n    this layer is equivalent to `SeparableConv1D`. If `use_bias` is True and\n    a bias initializer is provided, it adds a bias vector to the output.\n    It then optionally applies an activation function to produce the final output.\n\n    # Arguments\n        filters: Integer, the dimensionality of the output space (i.e. the number\n            of filters in the convolution).\n        kernel_size: A single integer specifying the spatial dimensions of the filters.\n        strides: A single integer specifying the strides of the convolution.\n            Specifying any `stride` value != 1 is incompatible with specifying\n            any `dilation_rate` value != 1.\n        padding: One of `""valid""`, `""same""`, or `""causal""` (case-insensitive).\n        pad_values: The pad value to use when `padding=""same""`.\n        data_format: A string, one of `channels_last` (default) or `channels_first`.\n            The ordering of the dimensions in the inputs. `channels_last` corresponds\n            to inputs with shape `(batch, length, channels)` while `channels_first`\n            corresponds to inputs with shape `(batch, channels, length)`.\n        dilation_rate: A single integer, specifying the dilation rate to use for dilated\n            convolution. Currently, specifying any `dilation_rate` value != 1 is\n            incompatible with specifying any stride value != 1.\n        depth_multiplier: The number of depthwise convolution output channels for\n            each input channel. The total number of depthwise convolution output\n            channels will be equal to `num_filters_in * depth_multiplier`.\n        activation: Activation function. Set it to None to maintain a linear activation.\n        use_bias: Boolean, whether the layer uses a bias.\n        input_quantizer: Quantization function applied to the input of the layer.\n        depthwise_quantizer: Quantization function applied to the depthwise kernel.\n        pointwise_quantizer: Quantization function applied to the pointwise kernel.\n        depthwise_initializer: An initializer for the depthwise convolution kernel.\n        pointwise_initializer: An initializer for the pointwise convolution kernel.\n        bias_initializer: An initializer for the bias vector. If None, the default\n            initializer will be used.\n        depthwise_regularizer: Optional regularizer for the depthwise convolution\n            kernel.\n        pointwise_regularizer: Optional regularizer for the pointwise convolution\n            kernel.\n        bias_regularizer: Optional regularizer for the bias vector.\n        activity_regularizer: Optional regularizer function for the output.\n        depthwise_constraint: Optional projection function to be applied to the\n            depthwise kernel after being updated by an `Optimizer`\n            (e.g. used for norm constraints or value constraints for layer weights).\n            The function must take as input the unprojected variable and must return\n            the projected variable (which must have the same shape). Constraints are\n            not safe to use when doing asynchronous distributed training.\n        pointwise_constraint: Optional projection function to be applied to the\n            pointwise kernel after being updated by an `Optimizer`.\n        bias_constraint: Optional projection function to be applied to the\n            bias after being updated by an `Optimizer`.\n        trainable: Boolean, if `True` the weights of this layer will be marked as\n            trainable (and listed in `layer.trainable_weights`).\n        name: A string, the name of the layer.\n    """"""\n\n    def __init__(\n        self,\n        filters,\n        kernel_size,\n        strides=1,\n        padding=""valid"",\n        pad_values=0.0,\n        data_format=None,\n        dilation_rate=1,\n        depth_multiplier=1,\n        activation=None,\n        use_bias=True,\n        input_quantizer=None,\n        depthwise_quantizer=None,\n        pointwise_quantizer=None,\n        depthwise_initializer=""glorot_uniform"",\n        pointwise_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        depthwise_regularizer=None,\n        pointwise_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        depthwise_constraint=None,\n        pointwise_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n        super().__init__(\n            filters,\n            kernel_size,\n            strides=strides,\n            padding=padding,\n            pad_values=pad_values,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            depth_multiplier=depth_multiplier,\n            activation=activation,\n            use_bias=use_bias,\n            input_quantizer=input_quantizer,\n            depthwise_quantizer=depthwise_quantizer,\n            pointwise_quantizer=pointwise_quantizer,\n            depthwise_initializer=depthwise_initializer,\n            pointwise_initializer=pointwise_initializer,\n            bias_initializer=bias_initializer,\n            depthwise_regularizer=depthwise_regularizer,\n            pointwise_regularizer=pointwise_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            depthwise_constraint=depthwise_constraint,\n            pointwise_constraint=pointwise_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n\n\n@utils.register_keras_custom_object\nclass QuantSeparableConv2D(\n    QuantizerSeparableBase, QuantizerBaseConv, tf.keras.layers.SeparableConv2D\n):\n    """"""Depthwise separable 2D convolution.\n\n    Separable convolutions consist in first performing a depthwise spatial convolution\n    (which acts on each input channel separately) followed by a pointwise convolution\n    which mixes together the resulting output channels. The `depth_multiplier` argument\n    controls how many output channels are generated per input channel\n    in the depthwise step.\n    `input_quantizer`, `depthwise_quantizer` and `pointwise_quantizer` are the\n    element-wise quantization functions to use. If all quantization functions are `None`\n    this layer is equivalent to `SeparableConv1D`. If `use_bias` is True and\n    a bias initializer is provided, it adds a bias vector to the output.\n    It then optionally applies an activation function to produce the final output.\n\n    Intuitively, separable convolutions can be understood as a way to factorize a\n    convolution kernel into two smaller kernels,\n    or as an extreme version of an Inception block.\n\n    # Arguments\n        filters: Integer, the dimensionality of the output space\n            (i.e. the number of output filters in the convolution).\n        kernel_size: An integer or tuple/list of 2 integers, specifying the height and\n            width of the 2D convolution window. Can be a single integer to specify the\n            same value for all spatial dimensions.\n        strides: An integer or tuple/list of 2 integers, specifying the strides of the\n            convolution along the height and width. Can be a single integer to specify\n            the same value for all spatial dimensions. Specifying any stride value != 1\n            is incompatible with specifying any `dilation_rate` value != 1.\n        padding: one of `""valid""` or `""same""` (case-insensitive).\n        pad_values: The pad value to use when `padding=""same""`.\n        data_format: A string, one of `channels_last` (default) or `channels_first`.\n            The ordering of the dimensions in the inputs. `channels_last` corresponds to\n            inputs with shape `(batch, height, width, channels)` while `channels_first`\n            corresponds to inputs with shape `(batch, channels, height, width)`. It\n            defaults to the `image_data_format` value found in your Keras config file at\n            `~/.keras/keras.json`. If you never set it, then it will be ""channels_last"".\n        dilation_rate: an integer or tuple/list of 2 integers, specifying the dilation\n            rate to use for dilated convolution. Can be a single integer to specify the\n            same value for all spatial dimensions. Currently, specifying any\n            `dilation_rate` value != 1 is incompatible with specifying any stride value\n            != 1.\n        depth_multiplier: The number of depthwise convolution output channels for each\n            input channel. The total number of depthwise convolution output channels\n            will be equal to `filters_in * depth_multiplier`.\n        activation: Activation function to use. If you don\'t specify anything,\n            no activation is applied (`a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        input_quantizer: Quantization function applied to the input of the layer.\n        depthwise_quantizer: Quantization function applied to the depthwise kernel\n            matrix.\n        pointwise_quantizer: Quantization function applied to the pointwise kernel\n            matrix.\n        depthwise_initializer: Initializer for the depthwise kernel matrix.\n        pointwise_initializer: Initializer for the pointwise kernel matrix.\n        bias_initializer: Initializer for the bias vector.\n        depthwise_regularizer: Regularizer function applied to the depthwise kernel\n            matrix.\n        pointwise_regularizer: Regularizer function applied to the pointwise kernel\n            matrix.\n        bias_regularizer: Regularizer function applied to the bias vector.\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its ""activation"").\n        depthwise_constraint: Constraint function applied to the depthwise kernel\n            matrix.\n        pointwise_constraint: Constraint function applied to the pointwise kernel\n            matrix.\n        bias_constraint: Constraint function applied to the bias vector.`\n\n    # Input shape\n        4D tensor with shape:\n        `(batch, channels, rows, cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(batch, rows, cols, channels)` if data_format=\'channels_last\'.\n\n    # Output shape\n        4D tensor with shape:\n        `(batch, filters, new_rows, new_cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(batch, new_rows, new_cols, filters)` if data_format=\'channels_last\'.\n        `rows` and `cols` values might have changed due to padding.\n    """"""\n\n    def __init__(\n        self,\n        filters,\n        kernel_size,\n        strides=(1, 1),\n        padding=""valid"",\n        pad_values=0.0,\n        data_format=None,\n        dilation_rate=(1, 1),\n        depth_multiplier=1,\n        activation=None,\n        use_bias=True,\n        input_quantizer=None,\n        depthwise_quantizer=None,\n        pointwise_quantizer=None,\n        depthwise_initializer=""glorot_uniform"",\n        pointwise_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        depthwise_regularizer=None,\n        pointwise_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        depthwise_constraint=None,\n        pointwise_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n        super().__init__(\n            filters,\n            kernel_size,\n            strides=strides,\n            padding=padding,\n            pad_values=pad_values,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            depth_multiplier=depth_multiplier,\n            activation=activation,\n            use_bias=use_bias,\n            input_quantizer=input_quantizer,\n            depthwise_quantizer=depthwise_quantizer,\n            pointwise_quantizer=pointwise_quantizer,\n            depthwise_initializer=depthwise_initializer,\n            pointwise_initializer=pointwise_initializer,\n            bias_initializer=bias_initializer,\n            depthwise_regularizer=depthwise_regularizer,\n            pointwise_regularizer=pointwise_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            depthwise_constraint=depthwise_constraint,\n            pointwise_constraint=pointwise_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n\n\n@utils.register_keras_custom_object\nclass QuantConv2DTranspose(QuantizerBase, tf.keras.layers.Conv2DTranspose):\n    """"""Transposed quantized convolution layer (sometimes called Deconvolution).\n\n    The need for transposed convolutions generally arises from the desire to use a\n    transformation going in the opposite direction of a normal convolution, i.e.,\n    from something that has the shape of the output of some convolution to something\n    that has the shape of its input while maintaining a connectivity pattern\n    that is compatible with said convolution. `input_quantizer` and `kernel_quantizer`\n    are the element-wise quantization functions to use. If both quantization functions\n    are `None` this layer is equivalent to `Conv2DTranspose`.\n\n    When using this layer as the first layer in a model, provide the keyword argument\n    `input_shape` (tuple of integers, does not include the sample axis), e.g.\n    `input_shape=(128, 128, 3)` for 128x128 RGB pictures in\n    `data_format=""channels_last""`.\n\n    # Arguments\n        filters: Integer, the dimensionality of the output space\n            (i.e. the number of output filters in the convolution).\n        kernel_size: An integer or tuple/list of 2 integers, specifying the\n            height and width of the 2D convolution window. Can be a single integer\n            to specify the same value for all spatial dimensions.\n        strides: An integer or tuple/list of 2 integers, specifying the strides of\n            the convolution along the height and width. Can be a single integer to\n            specify the same value for all spatial dimensions. Specifying any stride\n            value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n        padding: one of `""valid""` or `""same""` (case-insensitive).\n        output_padding: An integer or tuple/list of 2 integers, specifying the amount\n            of padding along the height and width of the output tensor. Can be a single\n            integer to specify the same value for all spatial dimensions. The amount of\n            output padding along a given dimension must be lower than the stride along\n            that same dimension.\n            If set to `None` (default), the output shape is inferred.\n        data_format: A string, one of `channels_last` (default) or `channels_first`. The\n            ordering of the dimensions in the inputs. `channels_last` corresponds to\n            inputs with shape `(batch, height, width, channels)` while `channels_first`\n            corresponds to inputs with shape `(batch, channels, height, width)`. It\n            defaults to the `image_data_format` value found in your Keras config file at\n            `~/.keras/keras.json`. If you never set it, then it will be ""channels_last"".\n        dilation_rate: an integer or tuple/list of 2 integers, specifying the dilation\n            rate to use for dilated convolution. Can be a single integer to specify the\n            same value for all spatial dimensions. Currently, specifying any\n            `dilation_rate` value != 1 is incompatible with specifying any stride value\n            != 1.\n        activation: Activation function to use. If you don\'t specify anything,\n            no activation is applied (`a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        input_quantizer: Quantization function applied to the input of the layer.\n        kernel_quantizer: Quantization function applied to the `kernel` weights matrix.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer function applied to the `kernel` weights matrix.\n        bias_regularizer: Regularizer function applied to the bias vector.\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its ""activation"").\n        kernel_constraint: Constraint function applied to the kernel matrix.\n        bias_constraint: Constraint function applied to the bias vector.\n\n    # Input shape\n        4D tensor with shape:\n        `(batch, channels, rows, cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(batch, rows, cols, channels)` if data_format=\'channels_last\'.\n\n    # Output shape\n        4D tensor with shape:\n        `(batch, filters, new_rows, new_cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(batch, new_rows, new_cols, filters)` if data_format=\'channels_last\'.\n        `rows` and `cols` values might have changed due to padding.\n\n    # References\n        - [A guide to convolution arithmetic for deep\n            learning](https://arxiv.org/abs/1603.07285v1)\n        - [Deconvolutional\n            Networks](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)\n    """"""\n\n    def __init__(\n        self,\n        filters,\n        kernel_size,\n        strides=(1, 1),\n        padding=""valid"",\n        output_padding=None,\n        data_format=None,\n        dilation_rate=(1, 1),\n        activation=None,\n        use_bias=True,\n        input_quantizer=None,\n        kernel_quantizer=None,\n        kernel_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n        super().__init__(\n            filters,\n            kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            activation=activation,\n            use_bias=use_bias,\n            input_quantizer=input_quantizer,\n            kernel_quantizer=kernel_quantizer,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n\n\n@utils.register_keras_custom_object\nclass QuantConv3DTranspose(QuantizerBase, tf.keras.layers.Conv3DTranspose):\n    """"""Transposed quantized convolution layer (sometimes called Deconvolution).\n\n    The need for transposed convolutions generally arises\n    from the desire to use a transformation going in the opposite direction\n    of a normal convolution, i.e., from something that has the shape of the\n    output of some convolution to something that has the shape of its input\n    while maintaining a connectivity pattern that is compatible with\n    said convolution. `input_quantizer` and `kernel_quantizer`\n    are the element-wise quantization functions to use. If both quantization functions\n    are `None` this layer is equivalent to `Conv3DTranspose`.\n\n    When using this layer as the first layer in a model, provide the keyword argument\n    `input_shape` (tuple of integers, does not include the sample axis),\n    e.g. `input_shape=(128, 128, 128, 3)` for a 128x128x128 volume with 3 channels\n    if `data_format=""channels_last""`.\n\n    # Arguments\n        filters: Integer, the dimensionality of the output space\n            (i.e. the number of output filters in the convolution).\n        kernel_size: An integer or tuple/list of 3 integers, specifying the depth, height\n            and width of the 3D convolution window. Can be a single integer to specify the\n            same value for all spatial dimensions.\n        strides: An integer or tuple/list of 3 integers, specifying the strides of the\n            convolution along the depth, height and width. Can be a single integer to\n            specify the same value for all spatial dimensions. Specifying any stride\n            value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n        padding: one of `""valid""` or `""same""` (case-insensitive).\n        output_padding: An integer or tuple/list of 3 integers, specifying the amount\n            of padding along the depth, height, and width. Can be a single integer to\n            specify the same value for all spatial dimensions. The amount of output\n            padding along a given dimension must be lower than the stride along that\n            same dimension. If set to `None` (default), the output shape is inferred.\n        data_format: A string, one of `channels_last` (default) or `channels_first`. The\n            ordering of the dimensions in the inputs. `channels_last` corresponds to\n            inputs with shape `(batch, depth, height, width, channels)` while\n            `channels_first` corresponds to inputs with shape\n            `(batch, channels, depth, height, width)`. It defaults to the\n            `image_data_format` value found in your Keras config file at\n            `~/.keras/keras.json`. If you never set it, then it will be ""channels_last"".\n        dilation_rate: an integer or tuple/list of 2 integers, specifying the dilation\n            rate to use for dilated convolution. Can be a single integer to specify the\n            same value for all spatial dimensions. Currently, specifying any\n            `dilation_rate` value != 1 is incompatible with specifying any stride value\n            != 1.\n        activation: Activation function to use. If you don\'t specify anything,\n            no activation is applied (`a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        input_quantizer: Quantization function applied to the input of the layer.\n        kernel_quantizer: Quantization function applied to the `kernel` weights matrix.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer function applied to the `kernel` weights matrix.\n        bias_regularizer: Regularizer function applied to the bias vector.\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its ""activation"").\n        kernel_constraint: Constraint function applied to the kernel matrix.\n        bias_constraint: Constraint function applied to the bias vector.\n\n    # Input shape\n        5D tensor with shape:\n        `(batch, channels, depth, rows, cols)` if data_format=\'channels_first\'\n        or 5D tensor with shape:\n        `(batch, depth, rows, cols, channels)` if data_format=\'channels_last\'.\n\n    # Output shape\n        5D tensor with shape:\n        `(batch, filters, new_depth, new_rows, new_cols)` if data_format=\'channels_first\'\n        or 5D tensor with shape:\n        `(batch, new_depth, new_rows, new_cols, filters)` if data_format=\'channels_last\'.\n        `depth` and `rows` and `cols` values might have changed due to padding.\n\n    # References\n        - [A guide to convolution arithmetic for deep\n            learning](https://arxiv.org/abs/1603.07285v1)\n        - [Deconvolutional\n            Networks](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)\n    """"""\n\n    def __init__(\n        self,\n        filters,\n        kernel_size,\n        strides=(1, 1, 1),\n        padding=""valid"",\n        output_padding=None,\n        data_format=None,\n        activation=None,\n        use_bias=True,\n        input_quantizer=None,\n        kernel_quantizer=None,\n        kernel_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n        super().__init__(\n            filters,\n            kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            activation=activation,\n            use_bias=use_bias,\n            input_quantizer=input_quantizer,\n            kernel_quantizer=kernel_quantizer,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n\n\n@utils.register_keras_custom_object\nclass QuantLocallyConnected1D(QuantizerBase, tf.keras.layers.LocallyConnected1D):\n    """"""Locally-connected quantized layer for 1D inputs.\n\n    The `QuantLocallyConnected1D` layer works similarly to the `QuantConv1D` layer,\n    except that weights are unshared, that is, a different set of filters is applied\n    at each different patch of the input. `input_quantizer` and `kernel_quantizer`\n    are the element-wise quantization functions to use. If both quantization functions\n    are `None` this layer is equivalent to `LocallyConnected1D`.\n\n    !!! example\n        ```python\n        # apply a unshared weight convolution 1d of length 3 to a sequence with\n        # 10 timesteps, with 64 output filters\n        model = Sequential()\n        model.add(QuantLocallyConnected1D(64, 3, input_shape=(10, 32)))\n        # now model.output_shape == (None, 8, 64)\n        # add a new conv1d on top\n        model.add(QuantLocallyConnected1D(32, 3))\n        # now model.output_shape == (None, 6, 32)\n        ```\n\n    # Arguments\n        filters: Integer, the dimensionality of the output space\n            (i.e. the number of output filters in the convolution).\n        kernel_size: An integer or tuple/list of a single integer,\n            specifying the length of the 1D convolution window.\n        strides: An integer or tuple/list of a single integer, specifying the stride\n            length of the convolution. Specifying any stride value != 1 is incompatible\n            with specifying any `dilation_rate` value != 1.\n        padding: Currently only supports `""valid""` (case-insensitive).\n            `""same""` may be supported in the future.\n        data_format: A string, one of `channels_last` (default) or `channels_first`.\n            The ordering of the dimensions in the inputs. `channels_last` corresponds\n            to inputs with shape `(batch, length, channels)` while `channels_first`\n            corresponds to inputs with shape `(batch, channels, length)`. It defaults\n            to the `image_data_format` value found in your Keras config file at\n            `~/.keras/keras.json`. If you never set it, then it will be ""channels_last"".\n        activation: Activation function to use. If you don\'t specify anything,\n            no activation is applied (`a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        input_quantizer: Quantization function applied to the input of the layer.\n        kernel_quantizer: Quantization function applied to the `kernel` weights matrix.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer function applied to the `kernel` weights matrix.\n        bias_regularizer: Regularizer function applied to the bias vector.\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its ""activation"").\n        kernel_constraint: Constraint function applied to the kernel matrix.\n        bias_constraint: Constraint function applied to the bias vector.\n        implementation: implementation mode, either `1` or `2`.\n            `1` loops over input spatial locations to perform the forward pass.\n            It is memory-efficient but performs a lot of (small) ops.\n\n            `2` stores layer weights in a dense but sparsely-populated 2D matrix\n            and implements the forward pass as a single matrix-multiply. It uses\n            a lot of RAM but performs few (large) ops.\n\n            Depending on the inputs, layer parameters, hardware, and\n            `tf.executing_eagerly()` one implementation can be dramatically faster\n            (e.g. 50X) than another.\n\n            It is recommended to benchmark both in the setting of interest to pick\n            the most efficient one (in terms of speed and memory usage).\n\n            Following scenarios could benefit from setting `implementation=2`:\n\n            - eager execution;\n            - inference;\n            - running on CPU;\n            - large amount of RAM available;\n            - small models (few filters, small kernel);\n            - using `padding=same` (only possible with `implementation=2`).\n\n    # Input shape\n        3D tensor with shape: `(batch_size, steps, input_dim)`\n\n    # Output shape\n        3D tensor with shape: `(batch_size, new_steps, filters)`\n        `steps` value might have changed due to padding or strides.\n    """"""\n\n    def __init__(\n        self,\n        filters,\n        kernel_size,\n        strides=1,\n        padding=""valid"",\n        data_format=None,\n        activation=None,\n        use_bias=True,\n        input_quantizer=None,\n        kernel_quantizer=None,\n        kernel_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        implementation=1,\n        **kwargs,\n    ):\n        super().__init__(\n            filters,\n            kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            activation=activation,\n            use_bias=use_bias,\n            input_quantizer=input_quantizer,\n            kernel_quantizer=kernel_quantizer,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            implementation=implementation,\n            **kwargs,\n        )\n\n\n@utils.register_keras_custom_object\nclass QuantLocallyConnected2D(QuantizerBase, tf.keras.layers.LocallyConnected2D):\n    """"""Locally-connected quantized layer for 2D inputs.\n\n    The `QuantLocallyConnected2D` layer works similarly to the `QuantConv2D` layer,\n    except that weights are unshared, that is, a different set of filters is applied\n    at each different patch of the input. `input_quantizer` and `kernel_quantizer`\n    are the element-wise quantization functions to use. If both quantization functions\n    are `None` this layer is equivalent to `LocallyConnected2D`.\n\n    !!! example\n        ```python\n        # apply a 3x3 unshared weights convolution with 64 output filters on a\n        32x32 image\n        # with `data_format=""channels_last""`:\n        model = Sequential()\n        model.add(QuantLocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))\n        # now model.output_shape == (None, 30, 30, 64)\n        # notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64\n        parameters\n\n        # add a 3x3 unshared weights convolution on top, with 32 output filters:\n        model.add(QuantLocallyConnected2D(32, (3, 3)))\n        # now model.output_shape == (None, 28, 28, 32)\n        ```\n\n    # Arguments\n        filters: Integer, the dimensionality of the output space\n            (i.e. the number of output filters in the convolution).\n        kernel_size: An integer or tuple/list of 2 integers, specifying the\n            width and height of the 2D convolution window. Can be a single integer to\n            specify the same value for all spatial dimensions.\n        strides: An integer or tuple/list of 2 integers, specifying the strides of the\n            convolution along the width and height. Can be a single integer to specify\n            the same value for all spatial dimensions.\n        padding: Currently only support `""valid""` (case-insensitive).\n            `""same""` will be supported in future.\n        data_format: A string, one of `channels_last` (default) or `channels_first`.\n            The ordering of the dimensions in the inputs. `channels_last` corresponds to\n            inputs with shape `(batch, height, width, channels)` while `channels_first`\n            corresponds to inputs with shape `(batch, channels, height, width)`. It\n            defaults to the `image_data_format` value found in your Keras config file at\n            `~/.keras/keras.json`. If you never set it, then it will be ""channels_last"".\n        activation: Activation function to use. If you don\'t specify anything,\n            no activation is applied (`a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        input_quantizer: Quantization function applied to the input of the layer.\n        kernel_quantizer: Quantization function applied to the `kernel` weights matrix.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer function applied to the `kernel` weights matrix.\n        bias_regularizer: Regularizer function applied to the bias vector.\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its ""activation"").\n        kernel_constraint: Constraint function applied to the kernel matrix.\n        bias_constraint: Constraint function applied to the bias vector.\n        implementation: implementation mode, either `1` or `2`.\n            `1` loops over input spatial locations to perform the forward pass.\n            It is memory-efficient but performs a lot of (small) ops.\n\n            `2` stores layer weights in a dense but sparsely-populated 2D matrix\n            and implements the forward pass as a single matrix-multiply. It uses\n            a lot of RAM but performs few (large) ops.\n\n            Depending on the inputs, layer parameters, hardware, and\n            `tf.executing_eagerly()` one implementation can be dramatically faster\n            (e.g. 50X) than another.\n\n            It is recommended to benchmark both in the setting of interest to pick\n            the most efficient one (in terms of speed and memory usage).\n\n            Following scenarios could benefit from setting `implementation=2`:\n\n            - eager execution;\n            - inference;\n            - running on CPU;\n            - large amount of RAM available;\n            - small models (few filters, small kernel);\n            - using `padding=same` (only possible with `implementation=2`).\n\n    # Input shape\n        4D tensor with shape:\n        `(samples, channels, rows, cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, rows, cols, channels)` if data_format=\'channels_last\'.\n\n    # Output shape\n        4D tensor with shape:\n        `(samples, filters, new_rows, new_cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, new_rows, new_cols, filters)` if data_format=\'channels_last\'.\n        `rows` and `cols` values might have changed due to padding.\n    """"""\n\n    def __init__(\n        self,\n        filters,\n        kernel_size,\n        strides=(1, 1),\n        padding=""valid"",\n        data_format=None,\n        activation=None,\n        use_bias=True,\n        input_quantizer=None,\n        kernel_quantizer=None,\n        kernel_initializer=""glorot_uniform"",\n        bias_initializer=""zeros"",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        implementation=1,\n        **kwargs,\n    ):\n        super().__init__(\n            filters,\n            kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            activation=activation,\n            use_bias=use_bias,\n            input_quantizer=input_quantizer,\n            kernel_quantizer=kernel_quantizer,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            implementation=implementation,\n            **kwargs,\n        )\n'"
larq/layers_base.py,11,"b'import logging\nfrom typing import Optional\n\nimport tensorflow as tf\n\nfrom larq import context, quantizers, utils\nfrom larq.quantized_variable import QuantizedVariable\nfrom larq.quantizers import Quantizer\n\nlog = logging.getLogger(__name__)\n\n\ndef _compute_padded_size(stride, dilation_rate, input_size, filter_size):\n    if input_size is None:\n        return None\n    effective_filter_size = (filter_size - 1) * dilation_rate + 1\n    output_size = (input_size + stride - 1) // stride\n    padded_size = (output_size - 1) * stride + effective_filter_size\n    if tf.is_tensor(input_size):\n        return tf.math.maximum(padded_size, input_size)\n    return max(padded_size, input_size)\n\n\ndef _compute_padding(stride, dilation_rate, input_size, filter_size):\n    padded_size = _compute_padded_size(stride, dilation_rate, input_size, filter_size)\n    total_padding = padded_size - input_size\n    padding = total_padding // 2\n    return padding, padding + (total_padding % 2)\n\n\nclass BaseLayer(tf.keras.layers.Layer):\n    """"""Base class for defining quantized layers.\n\n    `input_quantizer` is the element-wise quantization functions to use.\n    If `input_quantizer=None` this layer is equivalent to `tf.keras.layers.Layer`.\n    """"""\n\n    def __init__(self, *args, input_quantizer=None, **kwargs):\n        self.input_quantizer = quantizers.get(input_quantizer)\n        super().__init__(*args, **kwargs)\n\n    def call(self, inputs):\n        if self.input_quantizer:\n            inputs = self.input_quantizer(inputs)\n        with context.quantized_scope(True):\n            return super().call(inputs)\n\n    def get_config(self):\n        return {\n            **super().get_config(),\n            ""input_quantizer"": quantizers.serialize(self.input_quantizer),\n        }\n\n    def _get_quantizer(self, name) -> Optional[Quantizer]:\n        """"""Get quantizer for given kernel name""""""\n        return None\n\n    def _add_variable_with_custom_getter(self, name: str, **kwargs):\n        quantizer = self._get_quantizer(name)\n        if quantizer is None:\n            return super()._add_variable_with_custom_getter(name, **kwargs)\n\n        old_getter = kwargs.pop(""getter"")\n\n        # Wrap `getter` with a version that returns a `QuantizedVariable`.\n        def getter(*args, **kwargs):\n            variable = old_getter(*args, **kwargs)\n            return QuantizedVariable.from_variable(variable, quantizer)\n\n        return super()._add_variable_with_custom_getter(name, getter=getter, **kwargs)\n\n\nclass QuantizerBase(BaseLayer):\n    """"""Base class for defining quantized layers with a single kernel.\n\n    `kernel_quantizer` is the element-wise quantization functions to use.\n    If `kernel_quantizer=None` this layer is equivalent to `BaseLayer`.\n    """"""\n\n    def __init__(self, *args, kernel_quantizer=None, **kwargs):\n        self.kernel_quantizer = quantizers.get_kernel_quantizer(kernel_quantizer)\n\n        super().__init__(*args, **kwargs)\n        if kernel_quantizer and not self.kernel_constraint:\n            log.warning(\n                ""Using a weight quantizer without setting `kernel_constraint` ""\n                ""may result in starved weights (where the gradient is always zero).""\n            )\n\n    def _get_quantizer(self, name: str) -> Optional[Quantizer]:\n        return self.kernel_quantizer if name == ""kernel"" else None\n\n    def get_config(self):\n        return {\n            **super().get_config(),\n            ""kernel_quantizer"": quantizers.serialize(self.kernel_quantizer),\n        }\n\n\nclass QuantizerBaseConv(tf.keras.layers.Layer):\n    """"""Base class for defining quantized conv layers""""""\n\n    def __init__(self, *args, pad_values=0.0, **kwargs):\n        self.pad_values = pad_values\n        super().__init__(*args, **kwargs)\n        self._is_native_padding = self.padding != ""same"" or (\n            not tf.is_tensor(self.pad_values) and self.pad_values == 0.0\n        )\n\n    def _get_spatial_padding_same(self, shape):\n        return [\n            _compute_padding(stride, dilation_rate, shape[i], filter_size)\n            for i, (stride, dilation_rate, filter_size) in enumerate(\n                zip(self.strides, self.dilation_rate, self.kernel_size)\n            )\n        ]\n\n    def _get_spatial_shape(self, input_shape):\n        return (\n            input_shape[1:-1]\n            if self.data_format == ""channels_last""\n            else input_shape[2:]\n        )\n\n    def _get_padding_same(self, inputs):\n        input_shape = inputs.shape\n        if not input_shape[1:].is_fully_defined():\n            input_shape = tf.shape(inputs)\n        padding = self._get_spatial_padding_same(self._get_spatial_shape(input_shape))\n        return (\n            [[0, 0], *padding, [0, 0]]\n            if self.data_format == ""channels_last""\n            else [[0, 0], [0, 0], *padding]\n        )\n\n    def _get_padding_same_shape(self, input_shape):\n        spatial_input_shape = self._get_spatial_shape(input_shape)\n        spatial_shape = [\n            _compute_padded_size(stride, dilation, size, filter_size)\n            for size, stride, dilation, filter_size in zip(\n                spatial_input_shape, self.strides, self.dilation_rate, self.kernel_size,\n            )\n        ]\n        if self.data_format == ""channels_last"":\n            return tf.TensorShape([input_shape[0], *spatial_shape, input_shape[-1]])\n        return tf.TensorShape([*input_shape[:2], *spatial_shape])\n\n    def build(self, input_shape):\n        if self._is_native_padding:\n            super().build(input_shape)\n        else:\n            with utils.patch_object(self, ""padding"", ""valid""):\n                super().build(self._get_padding_same_shape(input_shape))\n\n    def call(self, inputs):\n        if self._is_native_padding:\n            return super().call(inputs)\n\n        inputs = tf.pad(\n            inputs, self._get_padding_same(inputs), constant_values=self.pad_values\n        )\n        with utils.patch_object(self, ""padding"", ""valid""):\n            return super().call(inputs)\n\n    def get_config(self):\n        return {\n            **super().get_config(),\n            ""pad_values"": tf.keras.backend.get_value(self.pad_values),\n        }\n\n\nclass QuantizerDepthwiseBase(BaseLayer):\n    """"""Base class for defining depthwise quantized layers\n\n    `depthwise_quantizer` is the element-wise quantization functions to use.\n    If `depthwise_quantizer=None` this layer is equivalent to `BaseLayer`.\n    """"""\n\n    def __init__(\n        self, *args, depthwise_quantizer: Optional[Quantizer] = None, **kwargs,\n    ):\n        self.depthwise_quantizer = quantizers.get_kernel_quantizer(depthwise_quantizer)\n\n        super().__init__(*args, **kwargs)\n        if depthwise_quantizer and not self.depthwise_constraint:\n            log.warning(\n                ""Using a weight quantizer without setting `depthwise_constraint` ""\n                ""may result in starved weights (where the gradient is always zero).""\n            )\n\n    def _get_quantizer(self, name: str) -> Optional[Quantizer]:\n        return self.depthwise_quantizer if name == ""depthwise_kernel"" else None\n\n    def get_config(self):\n        return {\n            **super().get_config(),\n            ""depthwise_quantizer"": quantizers.serialize(self.depthwise_quantizer),\n        }\n\n\nclass QuantizerSeparableBase(BaseLayer):\n    """"""Base class for defining separable quantized layers.\n\n    `depthwise_quantizer` and `pointwise_quantizer` are the element-wise quantization\n    functions to use. If all quantization functions are `None` this layer is equivalent\n    to `BaseLayer`.\n    """"""\n\n    def __init__(\n        self,\n        *args,\n        depthwise_quantizer: Optional[Quantizer] = None,\n        pointwise_quantizer: Optional[Quantizer] = None,\n        **kwargs,\n    ):\n        self.depthwise_quantizer = quantizers.get_kernel_quantizer(depthwise_quantizer)\n        self.pointwise_quantizer = quantizers.get_kernel_quantizer(pointwise_quantizer)\n\n        super().__init__(*args, **kwargs)\n        if depthwise_quantizer and not self.depthwise_constraint:\n            log.warning(\n                ""Using `depthwise_quantizer` without setting `depthwise_constraint` ""\n                ""may result in starved weights (where the gradient is always zero).""\n            )\n        if pointwise_quantizer and not self.pointwise_constraint:\n            log.warning(\n                ""Using `pointwise_quantizer` without setting `pointwise_constraint` ""\n                ""may result in starved weights (where the gradient is always zero).""\n            )\n\n    def _get_quantizer(self, name: str) -> Optional[Quantizer]:\n        if name == ""depthwise_kernel"":\n            return self.depthwise_quantizer\n        if name == ""pointwise_kernel"":\n            return self.pointwise_quantizer\n        return None\n\n    def get_config(self):\n        return {\n            **super().get_config(),\n            ""depthwise_quantizer"": quantizers.serialize(self.depthwise_quantizer),\n            ""pointwise_quantizer"": quantizers.serialize(self.pointwise_quantizer),\n        }\n'"
larq/layers_test.py,32,"b'import inspect\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport larq as lq\nfrom larq import testing_utils\n\nPARAMS_ALL_LAYERS = [\n    (lq.layers.QuantDense, tf.keras.layers.Dense, (3, 2), dict(units=3)),\n    (\n        lq.layers.QuantConv1D,\n        tf.keras.layers.Conv1D,\n        (2, 3, 7),\n        dict(filters=2, kernel_size=3),\n    ),\n    (\n        lq.layers.QuantConv2D,\n        tf.keras.layers.Conv2D,\n        (2, 3, 7, 6),\n        dict(filters=2, kernel_size=3),\n    ),\n    (\n        lq.layers.QuantConv3D,\n        tf.keras.layers.Conv3D,\n        (2, 3, 7, 6, 5),\n        dict(filters=2, kernel_size=3),\n    ),\n    (\n        lq.layers.QuantConv2DTranspose,\n        tf.keras.layers.Conv2DTranspose,\n        (2, 3, 7, 6),\n        dict(filters=2, kernel_size=3),\n    ),\n    (\n        lq.layers.QuantConv3DTranspose,\n        tf.keras.layers.Conv3DTranspose,\n        (2, 3, 7, 6, 5),\n        dict(filters=2, kernel_size=3),\n    ),\n    (\n        lq.layers.QuantLocallyConnected1D,\n        tf.keras.layers.LocallyConnected1D,\n        (2, 8, 5),\n        dict(filters=4, kernel_size=3),\n    ),\n    (\n        lq.layers.QuantLocallyConnected2D,\n        tf.keras.layers.LocallyConnected2D,\n        (8, 6, 10, 4),\n        dict(filters=3, kernel_size=3),\n    ),\n]\n\nPARAMS_SEP_LAYERS = [\n    (lq.layers.QuantSeparableConv1D, tf.keras.layers.SeparableConv1D, (2, 3, 7)),\n    (lq.layers.QuantSeparableConv2D, tf.keras.layers.SeparableConv2D, (2, 3, 7, 6)),\n]\n\n\nclass TestLayers:\n    @pytest.mark.parametrize(\n        ""quantized_layer, layer, input_shape, kwargs"", PARAMS_ALL_LAYERS\n    )\n    def test_binarization(\n        self, quantized_layer, layer, input_shape, kwargs, keras_should_run_eagerly\n    ):\n        input_data = testing_utils.random_input(input_shape)\n        random_weight = np.random.random() - 0.5\n\n        with lq.context.metrics_scope([""flip_ratio""]):\n            quant_output = testing_utils.layer_test(\n                quantized_layer,\n                kwargs=dict(\n                    **kwargs,\n                    kernel_quantizer=""ste_sign"",\n                    input_quantizer=""ste_sign"",\n                    kernel_initializer=tf.keras.initializers.constant(random_weight),\n                ),\n                input_data=input_data,\n                should_run_eagerly=keras_should_run_eagerly,\n            )\n\n        fp_model = tf.keras.models.Sequential(\n            [\n                layer(\n                    **kwargs,\n                    kernel_initializer=tf.keras.initializers.constant(\n                        np.sign(random_weight)\n                    ),\n                    input_shape=input_shape[1:],\n                )\n            ]\n        )\n\n        np.testing.assert_allclose(quant_output, fp_model.predict(np.sign(input_data)))\n\n    @pytest.mark.parametrize(""quantized_layer, layer, input_shape"", PARAMS_SEP_LAYERS)\n    def test_separable_layers(\n        self, quantized_layer, layer, input_shape, keras_should_run_eagerly\n    ):\n        input_data = testing_utils.random_input(input_shape)\n        random_d_kernel = np.random.random() - 0.5\n        random_p_kernel = np.random.random() - 0.5\n\n        with lq.context.metrics_scope([""flip_ratio""]):\n            quant_output = testing_utils.layer_test(\n                quantized_layer,\n                kwargs=dict(\n                    filters=3,\n                    kernel_size=3,\n                    depthwise_quantizer=""ste_sign"",\n                    pointwise_quantizer=""ste_sign"",\n                    input_quantizer=""ste_sign"",\n                    depthwise_initializer=tf.keras.initializers.constant(\n                        random_d_kernel\n                    ),\n                    pointwise_initializer=tf.keras.initializers.constant(\n                        random_p_kernel\n                    ),\n                ),\n                input_data=input_data,\n                should_run_eagerly=keras_should_run_eagerly,\n            )\n\n        fp_model = tf.keras.models.Sequential(\n            [\n                layer(\n                    filters=3,\n                    kernel_size=3,\n                    depthwise_initializer=tf.keras.initializers.constant(\n                        np.sign(random_d_kernel)\n                    ),\n                    pointwise_initializer=tf.keras.initializers.constant(\n                        np.sign(random_p_kernel)\n                    ),\n                    input_shape=input_shape[1:],\n                )\n            ]\n        )\n\n        np.testing.assert_allclose(quant_output, fp_model.predict(np.sign(input_data)))\n\n    def test_depthwise_layers(self, keras_should_run_eagerly):\n        input_data = testing_utils.random_input((2, 3, 7, 6))\n        random_weight = np.random.random() - 0.5\n\n        with lq.context.metrics_scope([""flip_ratio""]):\n            quant_output = testing_utils.layer_test(\n                lq.layers.QuantDepthwiseConv2D,\n                kwargs=dict(\n                    kernel_size=3,\n                    depthwise_quantizer=""ste_sign"",\n                    input_quantizer=""ste_sign"",\n                    depthwise_initializer=tf.keras.initializers.constant(random_weight),\n                ),\n                input_data=input_data,\n                should_run_eagerly=keras_should_run_eagerly,\n            )\n\n        fp_model = tf.keras.models.Sequential(\n            [\n                tf.keras.layers.DepthwiseConv2D(\n                    kernel_size=3,\n                    depthwise_initializer=tf.keras.initializers.constant(\n                        np.sign(random_weight)\n                    ),\n                    input_shape=input_data.shape[1:],\n                )\n            ]\n        )\n\n        np.testing.assert_allclose(quant_output, fp_model.predict(np.sign(input_data)))\n\n    @pytest.mark.parametrize(\n        ""layer_cls, input_dim"",\n        [\n            (lq.layers.QuantConv1D, 3),\n            (lq.layers.QuantConv2D, 4),\n            (lq.layers.QuantConv3D, 5),\n            (lq.layers.QuantSeparableConv1D, 3),\n            (lq.layers.QuantSeparableConv2D, 4),\n            (lq.layers.QuantDepthwiseConv2D, 4),\n        ],\n    )\n    @pytest.mark.parametrize(""data_format"", [""channels_last"", ""channels_first""])\n    @pytest.mark.parametrize(""dilation"", [True, False])\n    def test_non_zero_padding_layers(\n        self, mocker, layer_cls, input_dim, data_format, dilation\n    ):\n        inputs = np.zeros(np.random.randint(5, 20, size=input_dim), np.float32)\n        kernel = tuple(np.random.randint(3, 7, size=input_dim - 2))\n        rand_tuple = tuple(np.random.randint(1, 4, size=input_dim - 2))\n        if not dilation and layer_cls in (\n            lq.layers.QuantSeparableConv2D,\n            lq.layers.QuantDepthwiseConv2D,\n        ):\n            rand_tuple = int(rand_tuple[0])\n        kwargs = {""dilation_rate"": rand_tuple} if dilation else {""strides"": rand_tuple}\n\n        args = (kernel,) if layer_cls == lq.layers.QuantDepthwiseConv2D else (2, kernel)\n        ref_layer = layer_cls(*args, padding=""same"", **kwargs)\n        spy = mocker.spy(tf, ""pad"")\n        layer = layer_cls(*args, padding=""same"", pad_values=1.0, **kwargs)\n        layer.build(inputs.shape)\n        conv_op = getattr(layer, ""_convolution_op"", None)\n        assert layer(inputs).shape == ref_layer(inputs).shape\n        spy.assert_called_once_with(mocker.ANY, mocker.ANY, constant_values=1.0)\n        assert conv_op == getattr(layer, ""_convolution_op"", None)\n\n    @pytest.mark.parametrize(\n        ""layer_cls"",\n        [\n            lq.layers.QuantConv1D,\n            lq.layers.QuantConv2D,\n            lq.layers.QuantConv3D,\n            lq.layers.QuantSeparableConv1D,\n            lq.layers.QuantSeparableConv2D,\n            lq.layers.QuantDepthwiseConv2D,\n        ],\n    )\n    @pytest.mark.parametrize(""data_format"", [""channels_last"", ""channels_first""])\n    @pytest.mark.parametrize(""static"", [True, False])\n    def test_non_zero_padding_shapes(self, layer_cls, data_format, static):\n        layer = layer_cls(\n            16, 3, padding=""same"", pad_values=1.0, data_format=data_format\n        )\n        input_shape = [32 if static else None] * layer.rank + [3]\n        if data_format == ""channels_first"":\n            input_shape = reversed(input_shape)\n        input = tf.keras.layers.Input(shape=input_shape)\n\n        layer(input)\n        if static:\n            for dim in layer.output_shape[1:]:\n                assert dim is not None\n\n\nclass TestLayerWarns:\n    def test_layer_warns(self, caplog):\n        lq.layers.QuantDense(5, kernel_quantizer=""ste_sign"")\n        assert len(caplog.records) >= 1\n        assert ""kernel_constraint"" in caplog.text\n\n    def test_layer_does_not_warn(self, caplog):\n        lq.layers.QuantDense(\n            5, kernel_quantizer=""ste_sign"", kernel_constraint=""weight_clip""\n        )\n        assert ""kernel_constraint"" not in caplog.text\n\n    def test_depthwise_layer_warns(self, caplog):\n        lq.layers.QuantDepthwiseConv2D(5, depthwise_quantizer=""ste_sign"")\n        assert len(caplog.records) >= 1\n        assert ""depthwise_constraint"" in caplog.text\n\n    def test_depthwise_layer_does_not_warn(self, caplog):\n        lq.layers.QuantDepthwiseConv2D(\n            5, depthwise_quantizer=""ste_sign"", depthwise_constraint=""weight_clip""\n        )\n        assert ""depthwise_constraint"" not in caplog.text\n\n    def test_separable_layer_warns(self, caplog):\n        lq.layers.QuantSeparableConv2D(\n            3, 3, depthwise_quantizer=""ste_sign"", pointwise_quantizer=""ste_sign""\n        )\n        assert ""depthwise_constraint"" in caplog.text\n        assert ""pointwise_constraint"" in caplog.text\n\n    def test_separable_layer_does_not_warn(self, caplog):\n        lq.layers.QuantSeparableConv2D(\n            3,\n            3,\n            depthwise_quantizer=""ste_sign"",\n            pointwise_quantizer=""ste_sign"",\n            depthwise_constraint=""weight_clip"",\n            pointwise_constraint=""weight_clip"",\n        )\n        assert caplog.records == []\n\n\n@pytest.mark.parametrize(\n    ""quant_layer,layer"",\n    [\n        (lq.layers.QuantDense, tf.keras.layers.Dense),\n        (lq.layers.QuantConv1D, tf.keras.layers.Conv1D),\n        (lq.layers.QuantConv2D, tf.keras.layers.Conv2D),\n        (lq.layers.QuantConv3D, tf.keras.layers.Conv3D),\n        (lq.layers.QuantConv2DTranspose, tf.keras.layers.Conv2DTranspose),\n        (lq.layers.QuantConv3DTranspose, tf.keras.layers.Conv3DTranspose),\n        (lq.layers.QuantLocallyConnected1D, tf.keras.layers.LocallyConnected1D),\n        (lq.layers.QuantLocallyConnected2D, tf.keras.layers.LocallyConnected2D),\n        (lq.layers.QuantDepthwiseConv2D, tf.keras.layers.DepthwiseConv2D),\n    ],\n)\ndef test_layer_kwargs(quant_layer, layer):\n    quant_params = inspect.signature(quant_layer).parameters\n    params = inspect.signature(layer).parameters\n\n    quant_params_list = list(quant_params.keys())\n    params_list = list(params.keys())\n\n    for p in (\n        ""input_quantizer"",\n        ""kernel_quantizer"",\n        ""depthwise_quantizer"",\n        ""pointwise_quantizer"",\n        ""pad_values"",\n    ):\n        try:\n            quant_params_list.remove(p)\n        except ValueError:\n            pass\n    assert quant_params_list == params_list\n\n    for param in params_list:\n        assert quant_params.get(param).default == params.get(param).default  # type: ignore\n'"
larq/math.py,3,"b'""""""Math operations that are specific to extremely quantized networks.""""""\n\nimport tensorflow as tf\n\n\ndef sign(x):\n    r""""""A sign function that will never be zero\n    \\\\[\n    f(x) = \\begin{cases}\n      -1 & x < 0 \\\\\\\n      \\hphantom{-}1 & x \\geq 0\n    \\end{cases}\n    \\\\]\n\n    This function is similar to\n    [`tf.math.sign`](https://www.tensorflow.org/api_docs/python/tf/math/sign) but will\n    return a binary value and will never be zero.\n\n    # Arguments\n        `x`: Input Tensor\n\n    # Returns\n        A Tensor with same type as `x`.\n    """"""\n    return tf.sign(tf.sign(x) + 0.1)\n\n\ndef heaviside(x):\n    r""""""Heaviside step function with output values 0 and 1.\n\n    \\\\[\n    q(x) = \\begin{cases}\n    +1 & x > 0 \\\\\\\n    \\hphantom{+}0 & x \\leq 0\n    \\end{cases}\n    \\\\]\n\n    # Arguments\n        `x`: Input Tensor\n\n    # Returns\n        A Tensor with same type as `x`.\n    """"""\n    return tf.sign(tf.nn.relu(x))\n'"
larq/math_test.py,4,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport larq as lq\nfrom larq.testing_utils import generate_real_values_with_zeros\n\n\n@pytest.mark.parametrize(""fn"", [lq.math.sign])\ndef test_sign(fn):\n    x = tf.keras.backend.placeholder(ndim=2)\n    f = tf.keras.backend.function([x], [fn(x)])\n    binarized_values = np.random.choice([-1, 1], size=(2, 5)).astype(np.float32)\n    result = f(binarized_values)[0]\n    np.testing.assert_allclose(result, binarized_values)\n\n    real_values = generate_real_values_with_zeros()\n    result = f(real_values)[0]\n    assert not np.any(result == 0)\n    assert np.all(result[real_values < 0] == -1)\n    assert np.all(result[real_values >= 0] == 1)\n\n    zero_values = np.zeros((2, 5))\n    result = f(zero_values)[0]\n    assert np.all(result == 1)\n\n\n@pytest.mark.parametrize(""fn"", [lq.math.heaviside])\ndef test_heaviside(fn):\n    x = tf.keras.backend.placeholder(ndim=2)\n    f = tf.keras.backend.function([x], [fn(x)])\n    binarized_values = np.random.choice([0, 1], size=(2, 5))\n    result = f([binarized_values])[0]\n    np.testing.assert_allclose(result, binarized_values)\n\n    real_values = generate_real_values_with_zeros()\n    result = f([real_values])[0]\n    assert np.all(result[real_values <= 0] == 0)\n    assert np.all(result[real_values > 0] == 1)\n'"
larq/metrics.py,19,"b'""""""We add metrics specific to extremely quantized networks using a\n`larq.context.metrics_scope` rather than through the `metrics` parameter of\n`model.compile()`, where most common metrics reside. This is because, to calculate\nmetrics like the `flip_ratio`, we need a layer\'s kernel or activation and not just the\n`y_true` and `y_pred` that Keras passes to metrics defined in the usual way.\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom larq import utils\n\n\n@utils.register_alias(""flip_ratio"")\n@utils.register_keras_custom_object\nclass FlipRatio(tf.keras.metrics.Metric):\n    """"""Computes the mean ratio of changed values in a given tensor.\n\n    !!! example\n        ```python\n        m = metrics.FlipRatio()\n        m.update_state((1, 1))  # result: 0\n        m.update_state((2, 2))  # result: 1\n        m.update_state((1, 2))  # result: 0.75\n        print(\'Final result: \', m.result().numpy())  # Final result: 0.75\n        ```\n\n    # Arguments\n        name: Name of the metric.\n        values_dtype: Data type of the tensor for which to track changes.\n        dtype: Data type of the moving mean.\n    """"""\n\n    def __init__(self, values_dtype=""int8"", name=""flip_ratio"", dtype=None):\n        super().__init__(name=name, dtype=dtype)\n        self.built = False\n        self.values_dtype = tf.as_dtype(values_dtype)\n\n    def build(self, input_shape):\n        self._previous_values = self.add_weight(\n            ""previous_values"",\n            shape=input_shape,\n            dtype=self.values_dtype,\n            initializer=tf.keras.initializers.zeros,\n            aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,\n        )\n        self.total = self.add_weight(\n            ""total"",\n            initializer=tf.keras.initializers.zeros,\n            aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,\n        )\n        self.count = self.add_weight(\n            ""count"",\n            initializer=tf.keras.initializers.zeros,\n            aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,\n        )\n        self._size = tf.cast(np.prod(input_shape), self.dtype)\n        self.built = True\n\n    def update_state(self, values, sample_weight=None):\n        values = tf.cast(values, self.values_dtype)\n\n        if not self.built:\n            with tf.name_scope(self.name), tf.init_scope():\n                self.build(values.shape)\n\n        unchanged_values = tf.math.count_nonzero(\n            tf.equal(self._previous_values, values)\n        )\n        flip_ratio = 1 - (\n            tf.cast(unchanged_values, self.dtype) / tf.cast(self._size, self.dtype)\n        )\n\n        update_total_op = self.total.assign_add(flip_ratio * tf.sign(self.count))\n        with tf.control_dependencies([update_total_op]):\n            update_count_op = self.count.assign_add(1)\n            with tf.control_dependencies([update_count_op]):\n                return self._previous_values.assign(values)\n\n    def result(self):\n        return tf.compat.v1.div_no_nan(self.total, self.count - 1)\n\n    def reset_states(self):\n        tf.keras.backend.batch_set_value(\n            [(v, 0) for v in self.variables if v is not self._previous_values]\n        )\n\n    def get_config(self):\n        return {**super().get_config(), ""values_dtype"": self.values_dtype.name}\n'"
larq/metrics_test.py,9,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom larq import metrics\n\n\ndef test_config():\n    mcv = metrics.FlipRatio(values_dtype=""int16"", name=""mcv"", dtype=tf.float16)\n    assert mcv.name == ""mcv""\n    assert mcv.stateful\n    assert mcv.dtype == tf.float16\n    assert mcv.values_dtype == tf.int16\n\n    mcv2 = metrics.FlipRatio.from_config(mcv.get_config())\n    assert mcv2.name == ""mcv""\n    assert mcv2.stateful\n    assert mcv2.dtype == tf.float16\n    assert mcv2.values_dtype == tf.int16\n\n\ndef test_metric(eager_mode):\n    mcv = metrics.FlipRatio()\n    mcv.build((2,))\n    assert 0 == mcv.result().numpy()\n    assert 0 == mcv.total.numpy()\n    assert 0 == mcv.count.numpy()\n\n    mcv.update_state(np.array([1, 1]))\n    assert all([1, 1] == mcv._previous_values.numpy())\n    assert 0 == mcv.total.numpy()\n    assert 1 == mcv.count.numpy()\n    assert 0 == mcv.result().numpy()\n\n    mcv.update_state(np.array([2, 2]))\n    assert all([2, 2] == mcv._previous_values.numpy())\n    assert 1 == mcv.total.numpy()\n    assert 2 == mcv.count.numpy()\n    assert 1 == mcv.result().numpy()\n\n    mcv.update_state(np.array([1, 2]))\n    assert all([1, 2] == mcv._previous_values.numpy())\n    assert 1.5 == mcv.total.numpy()\n    assert 3 == mcv.count.numpy()\n    assert 1.5 / 2 == mcv.result().numpy()\n\n\ndef test_metric_implicit_build(eager_mode):\n    mcv = metrics.FlipRatio()\n\n    mcv.update_state(np.array([1, 1]))\n    assert all([1, 1] == mcv._previous_values.numpy())\n    assert 0 == mcv.total.numpy()\n    assert 1 == mcv.count.numpy()\n    assert 0 == mcv.result().numpy()\n\n    mcv.update_state(np.array([2, 2]))\n    assert all([2, 2] == mcv._previous_values.numpy())\n    assert 1 == mcv.total.numpy()\n    assert 2 == mcv.count.numpy()\n    assert 1 == mcv.result().numpy()\n\n    mcv.update_state(np.array([1, 2]))\n    assert all([1, 2] == mcv._previous_values.numpy())\n    assert 1.5 == mcv.total.numpy()\n    assert 3 == mcv.count.numpy()\n    assert 1.5 / 2 == mcv.result().numpy()\n\n\ndef test_metric_wrong_shape(eager_mode):\n    mcv = metrics.FlipRatio()\n    mcv.build((3,))\n    with pytest.raises((ValueError, tf.errors.InvalidArgumentError)):\n        mcv.update_state(np.array([1, 1]))\n\n\ndef test_metric_in_graph_mode(graph_mode):\n    mcv = metrics.FlipRatio()\n    mcv.build((2,))\n\n    new_state = tf.compat.v1.placeholder(dtype=tf.float32, shape=[2])\n    update_state_op = mcv.update_state(new_state)\n    metric_value = mcv.result()\n\n    with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.variables_initializer(mcv.variables))\n\n        sess.run(update_state_op, feed_dict={new_state: [1, 1]})\n        sess.run(update_state_op, feed_dict={new_state: [2, 2]})\n        sess.run(update_state_op, feed_dict={new_state: [1, 2]})\n\n        previous, total, count, result = sess.run(\n            [mcv._previous_values, mcv.total, mcv.count, metric_value]\n        )\n\n    assert all([1, 2] == previous)\n    assert 1.5 == total\n    assert 3 == count\n    assert 1.5 / 2 == result\n'"
larq/models.py,16,"b'import itertools\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Iterator, Mapping, Optional, Sequence, TypeVar, Union\n\nimport numpy as np\nimport tensorflow as tf\nfrom terminaltables import AsciiTable\n\nimport larq.layers as lq_layers\nfrom larq.utils import memory_as_readable_str\n\n__all__ = [""summary""]\n\nop_count_supported_layer_types = (\n    lq_layers.QuantConv2D,\n    lq_layers.QuantSeparableConv2D,\n    lq_layers.QuantDepthwiseConv2D,\n    lq_layers.QuantDense,\n    tf.keras.layers.Conv2D,\n    tf.keras.layers.SeparableConv2D,\n    tf.keras.layers.DepthwiseConv2D,\n    tf.keras.layers.Dense,\n    tf.keras.layers.Flatten,\n    tf.keras.layers.BatchNormalization,\n    tf.keras.layers.MaxPool2D,\n    tf.keras.layers.AveragePooling2D,\n)\n\nmac_containing_layers = (\n    lq_layers.QuantConv2D,\n    lq_layers.QuantSeparableConv2D,\n    lq_layers.QuantDepthwiseConv2D,\n    lq_layers.QuantDense,\n    tf.keras.layers.Conv2D,\n    tf.keras.layers.SeparableConv2D,\n    tf.keras.layers.DepthwiseConv2D,\n    tf.keras.layers.Dense,\n)\n\n\nT = TypeVar(""T"")\n\n\ndef _flatten(lst: Iterator[Iterator[T]]) -> Sequence[T]:\n    return list(itertools.chain.from_iterable(lst))\n\n\ndef _bitsize_as_str(bitsize: int) -> str:\n    bitsize_names = {8: ""byte"", 8 * 1024: ""kB""}\n\n    try:\n        return bitsize_names[bitsize]\n    except KeyError:\n        raise NotImplementedError()\n\n\ndef _number_as_readable_str(num: float) -> str:\n    # The initial rounding here is necessary so that e.g. `999000` gets\n    # formatted as `1.000 M` rather than `1000 k`\n    num = float(f""{num:.3g}"")\n\n    # For numbers less than 1000, output them directly, stripping any trailing\n    # zeros and decimal places.\n    if num < 1000:\n        return str(num).rstrip(""0"").rstrip(""."")\n\n    # For numbers that are at least 1000 trillion (1 quadrillion) format with\n    # scientific notation (3 s.f. = 2 d.p. in scientific notation).\n    if num >= 1e15:\n        return f""{num:.2E}""\n\n    # Count the magnitude.\n    magnitude = 0\n    while abs(num) >= 1000 and magnitude < 4:\n        magnitude += 1\n        num /= 1000.0\n\n    # \':.3g\' formats the number with 3 significant figures, without stripping trailing\n    # zeros.\n    num = f""{num:.3g}"".rstrip(""."")\n    unit = ["""", "" k"", "" M"", "" B"", "" T""][magnitude]\n    return num + unit\n\n\ndef _format_table_entry(x: float, units: int = 1) -> Union[float, str]:\n    try:\n        assert not np.isnan(x)\n        if type(x) == str or x == 0 or units == 1:\n            return x\n        return x / units\n    except Exception:\n        return ""?""\n\n\nclass WeightProfile:\n    def __init__(self, weight, trainable: bool = True):\n        self._weight = weight\n        self.bitwidth = getattr(weight, ""precision"", 32)\n        self.trainable = trainable\n\n    @property\n    def count(self) -> int:\n        return int(np.prod(self._weight.shape.as_list()))\n\n    @property\n    def memory(self) -> int:\n        return self.bitwidth * self.count\n\n    @property\n    def fp_equivalent_memory(self) -> int:\n        return 32 * self.count\n\n    @property\n    def int8_fp_weights_memory(self) -> int:\n        """"""Count any 32- or 16-bit weights as 8 bits instead.""""""\n\n        if self.bitwidth > 8:\n            return self.count * 8\n        return self.bitwidth * self.count\n\n    def is_bias(self) -> bool:\n        return ""bias"" in self._weight.name\n\n\n@dataclass\nclass OperationProfile:\n    n: int\n    precision: int\n    op_type: str\n\n\nclass LayerProfile:\n    def __init__(self, layer: tf.keras.layers.Layer):\n        self._layer = layer\n\n        weights = layer.weights\n        if isinstance(layer, tf.keras.layers.BatchNormalization):\n            fused_pairs = [(""beta"", ""moving_mean""), (""gamma"", ""moving_variance"")]\n            for pair in fused_pairs:\n                names = [w.name.split(""/"")[-1].replace("":0"", """") for w in weights]\n                if pair[0] in names and pair[1] in names:\n                    weights.pop(names.index(pair[0]))\n\n        self.weight_profiles = [\n            WeightProfile(\n                weight, trainable=any(weight is w for w in layer.trainable_weights),\n            )\n            for weight in weights\n        ]\n\n        self.op_profiles = []\n\n        if isinstance(layer, mac_containing_layers) and self.output_pixels:\n            for p in self.weight_profiles:\n                if not p.is_bias():\n                    self.op_profiles.append(\n                        OperationProfile(\n                            n=p.count * self.output_pixels,\n                            precision=max(self.input_precision or 32, p.bitwidth),\n                            op_type=""mac"",\n                        )\n                    )\n\n    @property\n    def memory(self) -> int:\n        return sum(p.memory for p in self.weight_profiles)\n\n    @property\n    def int8_fp_weights_memory(self) -> int:\n        return sum(p.int8_fp_weights_memory for p in self.weight_profiles)\n\n    @property\n    def fp_equivalent_memory(self) -> int:\n        return sum(p.fp_equivalent_memory for p in self.weight_profiles)\n\n    def weight_count(\n        self, bitwidth: Optional[int] = None, trainable: Optional[bool] = None\n    ) -> int:\n        count = 0\n        for p in self.weight_profiles:\n            if (bitwidth is None or p.bitwidth == bitwidth) and (\n                trainable is None or p.trainable == trainable\n            ):\n                count += p.count\n        return count\n\n    def op_count(\n        self, op_type: Optional[str] = None, precision: Optional[int] = None\n    ) -> Optional[int]:\n        if op_type != ""mac"":\n            raise ValueError(""Currently only counting of MAC-operations is supported."")\n\n        if (\n            isinstance(self._layer, op_count_supported_layer_types)\n            and self.output_pixels\n        ):\n            count = 0\n            for op in self.op_profiles:\n                if (precision is None or op.precision == precision) and (\n                    op_type is None or op.op_type == op_type\n                ):\n                    count += op.n\n            return count\n        return None\n\n    @property\n    def input_precision(self) -> Optional[int]:\n        try:\n            return self._layer.input_quantizer.precision\n        except AttributeError:\n            return None\n\n    @property\n    def output_shape(self) -> Optional[Sequence[int]]:\n        try:\n            return tuple(dim if dim else -1 for dim in self._layer.output_shape)\n        except AttributeError:\n            return None\n\n    @property\n    def output_shape_str(self) -> str:\n        try:\n            return str(self.output_shape or ""multiple"")\n        except RuntimeError:\n            return ""?""\n\n    @property\n    def output_pixels(self) -> Optional[int]:\n        """"""Number of pixels for a single feature map (1 for fully connected layers).""""""\n        if not self.output_shape:\n            return None\n        if len(self.output_shape) == 4:\n            return int(np.prod(self.output_shape[1:3]))\n        if len(self.output_shape) == 2:\n            return 1\n        raise NotImplementedError()\n\n    @property\n    def unique_param_bidtwidths(self) -> Sequence[int]:\n        return sorted(set([p.bitwidth for p in self.weight_profiles]))\n\n    @property\n    def unique_op_precisions(self) -> Sequence[int]:\n        return sorted(set([op.precision for op in self.op_profiles]))\n\n    def generate_table_row(\n        self, table_config: Mapping[str, Any]\n    ) -> Sequence[Union[str, float]]:\n        row = [self._layer.name, self.input_precision or ""-"", self.output_shape_str]\n        for i in table_config[""param_bidtwidths""]:\n            n = self.weight_count(i)\n            n = _format_table_entry(n, table_config[""param_units""])\n            row.append(n)\n        row.append(_format_table_entry(self.memory, table_config[""memory_units""]))\n        for i in table_config[""mac_precisions""]:\n            n = self.op_count(""mac"", i)\n            n = _format_table_entry(n, table_config[""mac_units""])\n            row.append(n)\n\n        return row\n\n\nclass ModelProfile(LayerProfile):\n    def __init__(self, model: tf.keras.models.Model):\n        self.layer_profiles = [LayerProfile(layer) for layer in model.layers]\n\n    @property\n    def memory(self) -> int:\n        return sum(lp.memory for lp in self.layer_profiles)\n\n    @property\n    def int8_fp_weights_memory(self) -> int:\n        return sum(lp.int8_fp_weights_memory for lp in self.layer_profiles)\n\n    @property\n    def fp_equivalent_memory(self) -> int:\n        return sum(lp.fp_equivalent_memory for lp in self.layer_profiles)\n\n    def weight_count(\n        self, bitwidth: Optional[int] = None, trainable: Optional[bool] = None\n    ) -> int:\n        return sum(lp.weight_count(bitwidth, trainable) for lp in self.layer_profiles)\n\n    def op_count(\n        self, op_type: Optional[str] = None, bitwidth: Optional[int] = None\n    ) -> int:\n        return sum(lp.op_count(op_type, bitwidth) or 0 for lp in self.layer_profiles)\n\n    @property\n    def unique_param_bidtwidths(self) -> Sequence[int]:\n        return sorted(\n            set(_flatten(lp.unique_param_bidtwidths for lp in self.layer_profiles))\n        )\n\n    @property\n    def unique_op_precisions(self) -> Sequence[int]:\n        return sorted(\n            set(_flatten(lp.unique_op_precisions for lp in self.layer_profiles))\n        )\n\n    def _generate_table_header(self, table_config: Mapping[str, Any]) -> Sequence[str]:\n        return [\n            ""Layer"",\n            ""Input prec.\\n(bit)"",\n            ""Outputs"",\n            *(\n                f""# {i}-bit\\nx {table_config[\'param_units\']}""\n                for i in table_config[""param_bidtwidths""]\n            ),\n            f""Memory\\n({_bitsize_as_str(table_config[\'memory_units\'])})"",\n            *(f""{i}-bit MACs"" for i in table_config[""mac_precisions""]),\n        ]\n\n    def _generate_table_total(\n        self, table_config: Mapping[str, Any]\n    ) -> Sequence[Union[float, str]]:\n        row = [""Total"", """", """"]\n        for i in table_config[""param_bidtwidths""]:\n            row.append(\n                _format_table_entry(self.weight_count(i), table_config[""param_units""])\n            )\n        row.append(_format_table_entry(self.memory, table_config[""memory_units""]))\n        for i in table_config[""mac_precisions""]:\n            row.append(\n                _format_table_entry(self.op_count(""mac"", i), table_config[""mac_units""])\n            )\n        return row\n\n    def generate_table(\n        self, include_macs: bool = True\n    ) -> Sequence[Sequence[Union[float, str]]]:\n        table_config = {\n            ""param_bidtwidths"": self.unique_param_bidtwidths,\n            ""mac_precisions"": self.unique_op_precisions if include_macs else [],\n            ""param_units"": 1,\n            ""memory_units"": 8 * 1024,\n            ""mac_units"": 1,\n        }\n\n        table = []\n\n        table.append(self._generate_table_header(table_config))\n\n        for lp in self.layer_profiles:\n            table.append(lp.generate_table_row(table_config))\n\n        table.append(self._generate_table_total(table_config))\n\n        return table\n\n    def generate_summary(\n        self, include_macs: bool = True\n    ) -> Sequence[Sequence[Union[str, float]]]:\n        summary = [\n            [""Total params"", _number_as_readable_str(self.weight_count())],\n            [\n                ""Trainable params"",\n                _number_as_readable_str(self.weight_count(trainable=True)),\n            ],\n            [\n                ""Non-trainable params"",\n                _number_as_readable_str(self.weight_count(trainable=False)),\n            ],\n            [""Model size"", memory_as_readable_str(self.memory)],\n            [\n                ""Model size (8-bit FP weights)"",\n                memory_as_readable_str(self.int8_fp_weights_memory),\n            ],\n            [""Float-32 Equivalent"", memory_as_readable_str(self.fp_equivalent_memory)],\n            [\n                ""Compression Ratio of Memory"",\n                self.memory / max(1e-8, self.fp_equivalent_memory),\n            ],\n        ]\n\n        if include_macs:\n            binarization_ratio = self.op_count(""mac"", 1) / max(\n                1, self.op_count(op_type=""mac"")\n            )\n            ternarization_ratio = self.op_count(""mac"", 2) / max(\n                1, self.op_count(op_type=""mac"")\n            )\n            summary.append(\n                [\n                    ""Number of MACs"",\n                    _number_as_readable_str(self.op_count(op_type=""mac"")),\n                ]\n            )\n            if binarization_ratio > 0:\n                summary.append(\n                    [""Ratio of MACs that are binarized"", f""{binarization_ratio:.4f}""]\n                )\n            if ternarization_ratio > 0:\n                summary.append(\n                    [""Ratio of MACs that are ternarized"", f""{ternarization_ratio:.4f}""]\n                )\n\n        return summary\n\n\ndef sanitize_table(table_data: Sequence[Sequence[Any]]) -> Sequence[Sequence[str]]:\n    return [\n        [f""{v:.2f}"" if type(v) == float else str(v) for v in row] for row in table_data\n    ]\n\n\nclass LayersTable(AsciiTable):\n    def __init__(self, table_data, title=None):\n        super().__init__(sanitize_table(table_data), title=title)\n        self.inner_column_border = False\n        self.justify_columns = {\n            i: ""left"" if i == 0 else ""right"" for i in range(len(table_data[0]))\n        }\n        self.inner_footing_row_border = True\n        self.inner_heading_row_border = True\n\n\nclass SummaryTable(AsciiTable):\n    def __init__(self, table_data, title=None):\n        super().__init__(sanitize_table(table_data), title=title)\n        self.inner_column_border = False\n        self.inner_heading_row_border = False\n\n\ndef summary(\n    model: tf.keras.models.Model,\n    print_fn: Callable[[str], Any] = None,\n    include_macs: bool = True,\n) -> None:\n    """"""Prints a string summary of the network.\n\n    The summary includes the following information per layer:\n\n    - input precision,\n    - output dimension,\n    - weight count (broken down by bidtwidth),\n    - memory footprint in kilobytes (`8*1024` 1-bit weights = 1 kB),\n    - number of multiply-accumulate (MAC) operations broken down by precision (*optional & expermental*).\n\n    A single MAC operation contains both a multiplication and an addition. The precision\n    of a MAC operation is defined as the maximum bitwidth of its inputs.\n\n    Additionally, the following overall statistics for the model are supplied:\n\n    - total number of weights,\n    - total number of trainable weights,\n    - total number of non-trainable weights,\n    - model size,\n    - model size (8-bit FP weights): memory footprint if FP weights were 8 bit,\n    - float-32 equivalent size: memory footprint if all weights were 32 bit,\n    - compression ratio achieved by quantizing weights,\n    - total number of MAC operations,\n    - ratio of MAC operations that is binarized and can be accelated with XNOR-gates.\n\n    # Arguments\n        model: model instance.\n        print_fn: Print function to use. Defaults to `print`. You can set it to a custom\n            function in order to capture the string summary.\n        include_macs: whether or not to include the number of MAC-operations in the\n            summary.\n\n    # Raises\n        ValueError: if called before the model is built.\n    """"""\n\n    if not model.built:\n        raise ValueError(\n            ""This model has not yet been built. Build the model first by calling ""\n            ""`model.build()` or calling `model.fit()` with some data, or specify an ""\n            ""`input_shape` argument in the first layer(s) for automatic build.""\n        )\n\n    if not print_fn:\n        print_fn = print\n\n    model_profile = ModelProfile(model)\n    print_fn(\n        LayersTable(model_profile.generate_table(), title=f""{model.name} stats"").table\n    )\n    print_fn(\n        SummaryTable(\n            model_profile.generate_summary(include_macs), title=f""{model.name} summary""\n        ).table\n    )\n'"
larq/models_test.py,12,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport larq as lq\nfrom larq.models import ModelProfile\n\n\nclass ToyModel(tf.keras.Model):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.conv = lq.layers.QuantConv2D(\n            filters=32,\n            kernel_size=(3, 3),\n            kernel_quantizer=""ste_sign"",\n            input_shape=(64, 64, 1),\n            padding=""same"",\n        )\n        self.pool = tf.keras.layers.GlobalAvgPool2D()\n        self.dense = tf.keras.layers.Dense(10, activation=""softmax"")\n\n    def call(self, inputs):\n        return self.dense(self.pool(self.conv(inputs)))\n\n\ndef get_profile_model():\n    return tf.keras.models.Sequential(\n        [\n            lq.layers.QuantConv2D(\n                filters=32,\n                kernel_size=(3, 3),\n                kernel_quantizer=""ste_sign"",\n                input_shape=(64, 64, 1),\n                padding=""same"",\n            ),\n            tf.keras.layers.MaxPooling2D((2, 2)),\n            lq.layers.QuantDepthwiseConv2D(\n                kernel_size=3,\n                strides=(3, 3),\n                input_quantizer=lq.quantizers.SteTern(),\n                depthwise_quantizer=lq.quantizers.SteTern(),\n                padding=""same"",\n                pad_values=1.0,\n                use_bias=False,\n            ),\n            tf.keras.layers.BatchNormalization(scale=False),\n            lq.layers.QuantSeparableConv2D(\n                32,\n                (3, 3),\n                input_quantizer=""ste_sign"",\n                depthwise_quantizer=""ste_sign"",\n                pointwise_quantizer=""ste_sign"",\n                padding=""same"",\n            ),\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(10, trainable=False),\n        ]\n    )\n\n\ndef test_model_profile():\n    profile = ModelProfile(get_profile_model())\n    assert len(profile.layer_profiles) == 7\n\n\ndef test_layer_profile():\n    profile = ModelProfile(get_profile_model())\n\n    kernel_count = [\n        32 * 3 * 3 * 1,\n        0,\n        32 * 3 * 3,\n        0,\n        32 * 3 * 3 * 1 + 32 * 1 * 1 * 32,\n        0,\n        32 * 11 * 11 * 10,\n    ]\n    bias_count = [32, 0, 0, 64, 32, 0, 10]\n    param_count = [k + b for k, b in zip(kernel_count, bias_count)]\n    memory = [  # bits * (c * w * h * b) + bits * bias\n        1 * (32 * 3 * 3 * 1) + 32 * 32,\n        0,\n        2 * (32 * 3 * 3),\n        32 * (2 * 32),\n        1 * (32 * 3 * 3 * 1 + 32 * 1 * 1 * 32) + 32 * 32,\n        0,\n        32 * (32 * 11 * 11 * 10 + 10),\n    ]\n    int8_fp_weights_mem = [\n        1 * (32 * 3 * 3 * 1) + 8 * 32,\n        0,\n        2 * (32 * 3 * 3),\n        8 * (32 * 2),\n        1 * (32 * 3 * 3 * 1 + 32 * 1 * 1 * 32) + 8 * 32,\n        0,\n        8 * (32 * 11 * 11 * 10 + 10),\n    ]\n    fp_equiv_mem = [32 * n for n in param_count]\n    input_precision = [None, None, 2, None, 1, None, None]\n    output_shape = [\n        (-1, 64, 64, 32),\n        (-1, 32, 32, 32),\n        (-1, 11, 11, 32),\n        (-1, 11, 11, 32),\n        (-1, 11, 11, 32),\n        (-1, 11 * 11 * 32),\n        (-1, 10),\n    ]\n    output_pixels = [int(np.prod(os[1:-1])) for os in output_shape]\n    unique_param_bidtwidths = [[1, 32], [], [2], [32], [1, 32], [], [32]]\n    unique_op_precisions = [[32], [], [2], [], [1], [], [32]]\n    mac_count = [params * pixels for params, pixels in zip(kernel_count, output_pixels)]\n    bin_mac_count = [\n        mc if (1 in pb and ip == 1) else 0\n        for mc, pb, ip in zip(mac_count, unique_param_bidtwidths, input_precision)\n    ]\n\n    profiles = profile.layer_profiles\n    for i in range(len(profiles)):\n        print(f""Testing layer {i}..."")\n        assert profiles[i].input_precision == input_precision[i]\n        assert profiles[i].output_shape == output_shape[i]\n        assert profiles[i].output_pixels == output_pixels[i]\n        assert profiles[i].weight_count() == param_count[i]\n        assert profiles[i].unique_param_bidtwidths == unique_param_bidtwidths[i]\n        assert profiles[i].unique_op_precisions == unique_op_precisions[i]\n        assert profiles[i].memory == memory[i]\n        assert profiles[i].fp_equivalent_memory == fp_equiv_mem[i]\n        assert profiles[i].int8_fp_weights_memory == int8_fp_weights_mem[i]\n        assert profiles[i].op_count(""mac"") == mac_count[i]\n        assert profiles[i].op_count(""mac"", 1) == bin_mac_count[i]\n\n\ndef test_summary(snapshot, capsys):\n    model = get_profile_model()\n    lq.models.summary(model)\n    captured = capsys.readouterr()\n    snapshot.assert_match(captured.out)\n\n    # A model with no weights\n    model = tf.keras.models.Sequential(\n        [tf.keras.layers.Lambda(lambda x: tf.zeros(2), input_shape=(32, 32))]\n    )\n    lq.models.summary(model)\n    captured = capsys.readouterr()\n    snapshot.assert_match(captured.out)\n\n\ndef test_subclass_model_summary(snapshot, capsys):\n    model = ToyModel()\n    model.build((None, 32, 32, 3))\n    lq.models.summary(model)\n    captured = capsys.readouterr()\n    snapshot.assert_match(captured.out)\n\n\ndef test_summary_invalid_model():\n    with pytest.raises(ValueError):\n        lq.models.summary(tf.keras.Model())\n\n\ndef test_bitsize_invalid_key():\n    with pytest.raises(NotImplementedError):\n        lq.models._bitsize_as_str(-1)\n\n\ndef test_number_as_readable_str_large():\n    assert lq.models._number_as_readable_str(1e16) == ""1.00E+16""\n\n\n@pytest.fixture(autouse=True)\ndef run_around_tests():\n    tf.keras.backend.clear_session()\n    yield\n'"
larq/optimizers.py,28,"b'""""""Neural networks with extremely low-precision weights and activations, such as\nBinarized Neural Networks (BNNs), usually contain a mix of low-precision weights (e.g.\n1-bit) and  higher-precision weights (e.g. 8-bit, 16-bit, or 32-bit). Examples of this\ninclude the first and last layers of image classificiation models, which have\nhigher-precision weights in most BNN architectures from the literature.\n\nTraining a BNN, then, consists of optimizing both low-precision and higher-precision\nweights. In `larq`, we provide a mechanism to target different bit-precision variables\nwith different optimizers using the `CaseOptimizer` class. Modeled after the\n[`tf.case`](https://www.tensorflow.org/api_docs/python/tf/case) signature,\n`CaseOptimizer` accepts pairs of predicates and optimizers. A predicate, given a\nvariable, decides whether its optimizer should train that variable.\n\nA `CaseOptimizer` behaves much like any other\n[Keras optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers), and\nonce you instantiate it you can pass it to your `model.compile()` as usual. To\ninstantiate a `CaseOptimzer`, pass one or a list of `(predicate, optimizer)` tuples,\nalong with a `default` optimizer which trains any variables not claimed by another\noptimizer. A variable may not be claimed by more than one optimizer\'s predicate.\n\n!!! example\n    ```python\n    no_op_quantizer = lq.quantizers.NoOp(precision=1)\n    layer = lq.layers.QuantDense(16, kernel_quantizer=no_op_quantizer)\n\n    case_optimizer = lq.optimizers.CaseOptimizer(\n        (\n            lq.optimizers.Bop.is_binary_variable,  # predicate\n            lq.optimizers.Bop(threshold=1e-6, gamma=1e-3),  # optimizer\n        ),\n        default_optimizer=tf.keras.optimizers.Adam(0.01),\n    )\n    ```\n""""""\n\n\nimport warnings\nfrom copy import deepcopy\nfrom typing import Callable, Optional, Tuple\n\nimport tensorflow as tf\n\nimport larq as lq\nfrom larq import utils\n\n__all__ = [""Bop"", ""CaseOptimizer""]\n\n\n@utils.register_keras_custom_object\nclass CaseOptimizer(tf.keras.optimizers.Optimizer):\n    """"""An optmizer wrapper that applies different optimizers to a subset of variables.\n\n    An optimizer is used to train a variable iff its accompanying predicate evaluates to\n    `True`.\n\n    For each variable, at most one optimizer\'s predicate may evaluate to `True`. If no\n    optimizer\'s predicate evaluates to `True` for a variable, it is trained with the\n    `default_optimizer`. If a variable is claimed by no optimizers and\n    `default_optimizer == None`, the variable is not trained.\n\n    # Arguments\n        predicate_optimizer_pairs: One or more `(pred, tf.keras.optimizers.Optimizer)`\n            pairs, where `pred`  takes one `tf.Variable` as argument and returns `True`\n            if the optimizer should be used for that variable, e.g. `pred(var) == True`.\n        default_optimizer: A `tf.keras.optimizers.Optimizer` to be applied to any\n            variable not claimed by any other optimizer. (Must be passed as keyword\n            argument.)\n    """"""\n\n    _HAS_AGGREGATE_GRAD = True\n\n    def __init__(\n        self,\n        *predicate_optimizer_pairs: Tuple[\n            Callable[[tf.Variable], bool], tf.keras.optimizers.Optimizer\n        ],\n        default_optimizer: Optional[tf.keras.optimizers.Optimizer] = None,\n        name: str = ""optimizer_case"",\n    ):\n        super().__init__(name=name)\n\n        # Type checks for (predicate, optimizer) pairs\n        for i, (predicate, optimizer) in enumerate(predicate_optimizer_pairs):\n            if not callable(predicate):\n                raise TypeError(\n                    f""Expected callable predicate at `predicate_optimizer_pairs[{i}][0]` but got `{type(predicate)}`.""\n                )\n            if not isinstance(optimizer, tf.keras.optimizers.Optimizer):\n                raise TypeError(\n                    f""Expected `tf.keras.optimizers.Optimizer` at `predicate_optimizer_pairs[{i}][1]` but got `{type(optimizer)}`.""\n                )\n\n        # Type check for default optimizers\n        if default_optimizer is not None and not isinstance(\n            default_optimizer, tf.keras.optimizers.Optimizer\n        ):\n            raise TypeError(\n                f""Expected `tf.keras.optimizers.Optimizer` for `default_optimizer` but got `{type(default_optimizer)}`.""\n            )\n\n        self.pred_opt_pairs = predicate_optimizer_pairs\n        self.default = default_optimizer\n\n        self.var_opt_mapping = None\n\n        # List of optimizers ending in `default_optimizer`, for easier internal access\n        self.optimizers = [opt for (_, opt) in self.pred_opt_pairs]\n\n        if self.default:\n            self.optimizers.append(self.default)\n            self.DEFAULT_OPT_INDEX = len(self.pred_opt_pairs)\n\n        # Track optimizers to support reloading via tf.train.Checkpoint\n        for i, optimizer in enumerate(self.optimizers):\n            self._track_trackable(optimizer, name=f""optimizer_{i}"")\n\n    @property\n    def weights(self):\n        weights = []\n        for optimizer in self.optimizers:\n            weights.extend(optimizer.weights)\n        return weights\n\n    def apply_gradients(self, grads_and_vars, name: Optional[str] = None, **kwargs):\n        """"""Apply gradients to variables for each optimizer.\n\n        On the first call to `apply_gradients()`, compute the mapping from variables to\n        optimizers and cache it in the `self.var_opt_mapping` dict for serialization and\n        faster access.\n        """"""\n\n        if self.var_opt_mapping is None:\n            # Convert `grads_and_vars` to list so we can iterate multiple times over it\n            grads_and_vars = list(grads_and_vars)\n            self._compute_var_opt_mapping(grads_and_vars)\n\n        # Split gradients and variables into a separate list for each optimizer\n        grad_var_lists = [[] for _ in range(len(self.pred_opt_pairs) + 1)]\n        for grad, var in grads_and_vars:\n            if var.name in self.var_opt_mapping:\n                grad_var_lists[self.var_opt_mapping[var.name]].append((grad, var))\n\n        with tf.init_scope():\n            for optimizer, opt_grads_and_vars in zip(self.optimizers, grad_var_lists):\n                optimizer._create_slots([v for (_, v) in grads_and_vars])\n\n        return tf.distribute.get_replica_context().merge_call(\n            self._apply_gradients, args=(grad_var_lists, name), kwargs=kwargs\n        )\n\n    def _apply_gradients(self, distribution, grad_var_lists, name, **kwargs):\n        # Apply gradients to each optimizer\n        with tf.name_scope(self._name):\n            train_ops = [\n                distribution.extended.call_for_each_replica(\n                    optimizer.apply_gradients, args=(opt_grads_and_vars,), kwargs=kwargs\n                )\n                for optimizer, opt_grads_and_vars in zip(\n                    self.optimizers, grad_var_lists\n                )\n            ]\n\n            return tf.group(*train_ops, name=name or ""train_with_group"")\n\n    def get_config(self):\n        optimizer_configs = [opt.get_config() for (_, opt) in self.pred_opt_pairs]\n        default_config = self.default.get_config()\n\n        config = {\n            ""optimizer_configs"": [\n                {""class_name"": optimizer_config[""name""], ""config"": optimizer_config}\n                for optimizer_config in optimizer_configs\n            ],\n            ""default_config"": {\n                ""class_name"": default_config[""name""],\n                ""config"": default_config,\n            },\n            ""var_opt_mapping"": self.var_opt_mapping,  # serialized instead of `pred`s\n        }\n        return {**super().get_config(), **config}\n\n    @classmethod\n    def from_config(cls, original_config, custom_objects=None):\n        config = deepcopy(original_config)\n\n        case_optimizer = cls(\n            *[  # `(pred, opt)` tuples\n                (\n                    lambda _: False,  # placeholder callable (`pred` is not serialized)\n                    tf.keras.optimizers.deserialize(  # optimizer `opt`\n                        opt_config, custom_objects=custom_objects\n                    ),\n                )\n                for opt_config in config[""optimizer_configs""]\n            ],\n            default_optimizer=tf.keras.optimizers.deserialize(\n                config[""default_config""], custom_objects=custom_objects\n            ),\n        )\n\n        # Since we no longer have the `pred`s, we set the mapping explicitly\n        case_optimizer.var_opt_mapping = config[""var_opt_mapping""]\n\n        return case_optimizer\n\n    def _compute_var_opt_mapping(self, grads_and_vars):\n        """"""Compute a unique mapping from variables to optimizer indices.""""""\n\n        self.var_opt_mapping = {}\n\n        for grad, var in grads_and_vars:\n            num_optimizers = 0\n\n            # Find the optimizer(s) that want to claim this variable\n            for optimizer_index, (predicate, _) in enumerate(self.pred_opt_pairs):\n                if predicate(var):\n                    self.var_opt_mapping[var.name] = optimizer_index\n                    num_optimizers += 1\n\n            if num_optimizers > 1:\n                raise ValueError(f""Variable `{var}` claimed by multiple optimizers."")\n            if num_optimizers == 0:\n                if self.default is not None:\n                    self.var_opt_mapping[var.name] = self.DEFAULT_OPT_INDEX\n                else:\n                    warnings.warn(\n                        f""No `default_optimizer` provided to train variable `{var}`.""\n                    )\n\n        # Make sure that each optimizer touches at least one variable\n        for optimizer_index, (_, optimizer) in enumerate(self.pred_opt_pairs):\n            if optimizer_index not in self.var_opt_mapping.values():\n                raise ValueError(\n                    f""Optimizer `{optimizer}` did not claim any variables.""\n                )\n\n\n@utils.register_keras_custom_object\nclass Bop(tf.keras.optimizers.Optimizer):\n    """"""Binary optimizer (Bop).\n\n    Bop is a latent-free optimizer for Binarized Neural Networks (BNNs) and\n    Binary Weight Networks (BWN).\n\n    Bop maintains an exponential moving average of the gradients controlled by\n    `gamma`. If this average exceeds the `threshold`, a weight is flipped.\n\n    The hyperparameter `gamma` is somewhat analogues to the learning rate in\n    SGD methods: a high `gamma` results in rapid convergence but also makes\n    training more noisy.\n\n    Note that the default `threshold` is not optimal for all situations.\n    Setting the threshold too high results in little learning, while setting it\n    too low results in overly noisy behaviour.\n\n    !!! warning\n        The `is_binary_variable` check of this optimizer will only target variables that\n        have been explicitly marked as being binary using `NoOp(precision=1)`.\n\n    !!! example\n        ```python\n        no_op_quantizer = lq.quantizers.NoOp(precision=1)\n        layer = lq.layers.QuantDense(16, kernel_quantizer=no_op_quantizer)\n\n        optimizer = lq.optimizers.CaseOptimizer(\n            (lq.optimizers.Bop.is_binary_variable, lq.optimizers.Bop()),\n            default_optimizer=tf.keras.optimizers.Adam(0.01),  # for FP weights\n        )\n        ```\n\n    # Arguments\n        threshold: magnitude of average gradient signal required to flip a weight.\n        gamma: the adaptivity rate.\n        name: name of the optimizer.\n\n    # References\n        - [Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization](https://papers.nips.cc/paper/8971-latent-weights-do-not-exist-rethinking-binarized-neural-network-optimization)\n    """"""\n\n    _HAS_AGGREGATE_GRAD = True\n\n    def __init__(\n        self, threshold: float = 1e-8, gamma: float = 1e-4, name: str = ""Bop"", **kwargs\n    ):\n        super().__init__(name=name, **kwargs)\n\n        self._set_hyper(""threshold"", threshold)\n        self._set_hyper(""gamma"", gamma)\n\n    def _create_slots(self, var_list):\n        for var in var_list:\n            self.add_slot(var, ""m"")\n\n    def _get_decayed_hyper(self, name: str, var_dtype):\n        hyper = self._get_hyper(name, var_dtype)\n        if isinstance(hyper, tf.keras.optimizers.schedules.LearningRateSchedule):\n            local_step = tf.cast(self.iterations, var_dtype)\n            hyper = tf.cast(hyper(local_step), var_dtype)\n        return hyper\n\n    def _resource_apply_dense(self, grad, var):\n        var_dtype = var.dtype.base_dtype\n        gamma = self._get_decayed_hyper(""gamma"", var_dtype)\n        threshold = self._get_decayed_hyper(""threshold"", var_dtype)\n        m = self.get_slot(var, ""m"")\n\n        m_t = m.assign_add(gamma * (grad - m))\n        var_t = lq.math.sign(-tf.sign(var * m_t - threshold) * var)\n        return var.assign(var_t).op\n\n    def get_config(self):\n        config = {\n            ""threshold"": self._serialize_hyperparameter(""threshold""),\n            ""gamma"": self._serialize_hyperparameter(""gamma""),\n        }\n        return {**super().get_config(), **config}\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        for hyper in (""gamma"", ""threshold""):\n            if hyper in config and isinstance(config[hyper], dict):\n                config[hyper] = tf.keras.optimizers.schedules.deserialize(\n                    config[hyper], custom_objects=custom_objects\n                )\n        return cls(**config)\n\n    @staticmethod\n    def is_binary_variable(var: tf.Variable) -> bool:\n        """"""Returns `True` for variables with `var.precision == 1`.\n\n        This is an example of a predictate that can be used by the `CaseOptimizer`.\n\n        # Arguments\n            var: a `tf.Variable`.\n        """"""\n        return getattr(var, ""precision"", 32) == 1\n'"
larq/optimizers_test.py,30,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.python.keras import testing_utils\n\nimport larq as lq\nfrom larq import testing_utils as lq_testing_utils\n\n\ndef _assert_weights(weights, expected):\n    for w, e in zip(weights, expected):\n        np.testing.assert_allclose(np.squeeze(w), e)\n\n\ndef _test_optimizer(\n    optimizer, target=0.75, test_kernels_are_binary=True, trainable_bn=True\n):\n    np.random.seed(1337)\n    (x_train, y_train), _ = testing_utils.get_test_data(\n        train_samples=1000, test_samples=0, input_shape=(10,), num_classes=2\n    )\n    y_train = keras.utils.to_categorical(y_train)\n\n    model = lq_testing_utils.get_small_bnn_model(\n        x_train.shape[1], 20, y_train.shape[1], trainable_bn=trainable_bn\n    )\n    model.compile(loss=""categorical_crossentropy"", optimizer=optimizer, metrics=[""acc""])\n\n    initial_vars = [tf.keras.backend.get_value(w) for w in model.trainable_weights]\n\n    history = model.fit(x_train, y_train, epochs=2, batch_size=16, verbose=0)\n\n    trained_vars = [tf.keras.backend.get_value(w) for w in model.trainable_weights]\n\n    # check all trainable variables have actually been updated\n    for v0, v1 in zip(initial_vars, trained_vars):\n        assert not np.all(v0 == v1)\n\n    # Note that when kernels are treated as latent weights they need not be\n    # binary (see https://arxiv.org/abs/1906.02107 for further discussion)\n    if test_kernels_are_binary:\n        for layer in model.layers:\n            if ""quant"" in layer.name:\n                for weight in layer.trainable_weights:\n                    assert np.all(np.isin(tf.keras.backend.get_value(weight), [-1, 1]))\n\n    assert history.history[""acc""][-1] >= target\n\n\ndef _test_serialization(optimizer):\n    config = keras.optimizers.serialize(optimizer)\n    optim = keras.optimizers.deserialize(config)\n    new_config = keras.optimizers.serialize(optim)\n    assert config == new_config\n\n\nclass TestCaseOptimizer:\n    def test_type_check_predicate(self):\n        with pytest.raises(TypeError):\n            lq.optimizers.CaseOptimizer((False, lq.optimizers.Bop()))\n\n    def test_type_check_optimizer(self):\n        with pytest.raises(TypeError):\n            lq.optimizers.CaseOptimizer((lq.optimizers.Bop.is_binary_variable, False))\n\n    def test_type_check_default(self):\n        with pytest.raises(TypeError):\n            lq.optimizers.CaseOptimizer(\n                (lq.optimizers.Bop.is_binary_variable, lq.optimizers.Bop()),\n                default_optimizer=False,\n            )\n\n    def test_overlapping_predicates(self):\n        with pytest.raises(ValueError):\n            naughty_case_opt = lq.optimizers.CaseOptimizer(\n                (lambda var: True, lq.optimizers.Bop()),\n                (lambda var: True, lq.optimizers.Bop()),\n            )\n            _test_optimizer(naughty_case_opt)\n\n    def test_missing_default(self):\n        with pytest.warns(Warning):\n            naughty_case_opt = lq.optimizers.CaseOptimizer(\n                (lq.optimizers.Bop.is_binary_variable, lq.optimizers.Bop()),\n            )\n\n            # Simple MNIST model\n            mnist = tf.keras.datasets.mnist\n            (train_images, train_labels), _ = mnist.load_data()\n            model = tf.keras.Sequential(\n                [\n                    tf.keras.layers.Flatten(input_shape=(28, 28)),\n                    lq.layers.QuantDense(\n                        64,\n                        input_quantizer=""ste_sign"",\n                        kernel_quantizer=lq.quantizers.NoOp(precision=1),\n                        activation=""relu"",\n                    ),\n                    tf.keras.layers.Dense(10, activation=""softmax""),\n                ]\n            )\n            model.compile(\n                loss=""sparse_categorical_crossentropy"",\n                optimizer=naughty_case_opt,\n                metrics=[""acc""],\n            )\n\n            # Should raise on first call to apply_gradients()\n            model.fit(train_images[:1], train_labels[:1], epochs=1)\n\n    def test_wrong_predicate(self):\n        """"""Make sure we throw when an optimizer does not claim variables.""""""\n\n        with pytest.raises(ValueError):\n            naughty_case_opt = lq.optimizers.CaseOptimizer(\n                (lambda var: False, lq.optimizers.Bop()),\n                default_optimizer=tf.keras.optimizers.Adam(0.01),\n            )\n\n            # Simple MNIST model\n            mnist = tf.keras.datasets.mnist\n            (train_images, train_labels), _ = mnist.load_data()\n            model = tf.keras.Sequential(\n                [\n                    tf.keras.layers.Flatten(input_shape=(28, 28)),\n                    tf.keras.layers.Dense(128, activation=""relu""),\n                    tf.keras.layers.Dense(10, activation=""softmax""),\n                ]\n            )\n            model.compile(\n                loss=""sparse_categorical_crossentropy"",\n                optimizer=naughty_case_opt,\n                metrics=[""acc""],\n            )\n\n            # Should raise on first call to apply_gradients()\n            model.fit(train_images[:1], train_labels[:1], epochs=1)\n\n    def test_weights(self):\n        (train_images, train_labels), _ = tf.keras.datasets.mnist.load_data()\n        model = tf.keras.Sequential(\n            [\n                tf.keras.layers.Flatten(input_shape=(28, 28)),\n                lq.layers.QuantDense(\n                    64,\n                    input_quantizer=""ste_sign"",\n                    kernel_quantizer=lq.quantizers.NoOp(precision=1),\n                    activation=""relu"",\n                ),\n                tf.keras.layers.Dense(10, activation=""softmax""),\n            ]\n        )\n        model.compile(\n            loss=""sparse_categorical_crossentropy"",\n            optimizer=lq.optimizers.CaseOptimizer(\n                (lq.optimizers.Bop.is_binary_variable, lq.optimizers.Bop()),\n                default_optimizer=tf.keras.optimizers.Adam(0.01),\n            ),\n        )\n        model.fit(train_images[:1], train_labels[:1], epochs=1)\n\n        opt_weights = model.optimizer.weights\n        assert len(opt_weights) != 0\n        checked_weights = 0\n        for opt in model.optimizer.optimizers:\n            for weight in opt.weights:\n                assert weight is opt_weights[checked_weights]\n                checked_weights += 1\n        assert checked_weights == len(opt_weights)\n\n    def test_checkpoint(self, eager_mode, tmp_path):\n        # Build and run a simple model.\n        var = tf.Variable([2.0])\n        opt = tf.keras.optimizers.SGD(1.0, momentum=1.0)\n        opt = lq.optimizers.CaseOptimizer((lambda var: True, opt))\n        opt.minimize(lambda: var + 1.0, var_list=[var])\n        slot_var = opt.optimizers[0].get_slot(var, ""momentum"")\n        slot_value = slot_var.numpy().item()\n\n        # Save a checkpoint.\n        checkpoint = tf.train.Checkpoint(optimizer=opt, var=var)\n        save_path = checkpoint.save(tmp_path / ""ckpt"")\n\n        # Run model again.\n        opt.minimize(lambda: var + 1.0, var_list=[var])\n        assert slot_var.numpy().item() != slot_value\n\n        # Load checkpoint and ensure loss scale is back to its original value.\n        status = checkpoint.restore(save_path)\n        status.assert_consumed()\n        status.run_restore_ops()\n        assert slot_var.numpy().item() == slot_value\n\n\nclass TestBopOptimizer:\n    def test_bop_accuracy(self):\n        _test_optimizer(\n            lq.optimizers.CaseOptimizer(\n                (lq.optimizers.Bop.is_binary_variable, lq.optimizers.Bop()),\n                default_optimizer=tf.keras.optimizers.Adam(0.01),\n            ),\n            test_kernels_are_binary=True,\n        )\n        # test optimizer on model with only binary trainable vars (low accuracy)\n        _test_optimizer(\n            lq.optimizers.CaseOptimizer(\n                (lq.optimizers.Bop.is_binary_variable, lq.optimizers.Bop()),\n                default_optimizer=tf.keras.optimizers.Adam(0.01),\n            ),\n            test_kernels_are_binary=True,\n            trainable_bn=False,\n            target=0,\n        )\n\n    def test_mixed_precision(self):\n        opt = lq.optimizers.CaseOptimizer(\n            (lq.optimizers.Bop.is_binary_variable, lq.optimizers.Bop()),\n            default_optimizer=tf.keras.optimizers.Adam(0.01),\n        )\n        opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, ""dynamic"")\n        _test_optimizer(opt, test_kernels_are_binary=True)\n\n    def test_bop_tf_1_14_schedules(self):\n        _test_optimizer(\n            lq.optimizers.CaseOptimizer(\n                (\n                    lq.optimizers.Bop.is_binary_variable,\n                    lq.optimizers.Bop(\n                        threshold=tf.keras.optimizers.schedules.InverseTimeDecay(\n                            3.0, decay_steps=1.0, decay_rate=0.5\n                        ),\n                        gamma=tf.keras.optimizers.schedules.InverseTimeDecay(\n                            3.0, decay_steps=1.0, decay_rate=0.5\n                        ),\n                    ),\n                ),\n                default_optimizer=tf.keras.optimizers.Adam(0.01),\n            ),\n            test_kernels_are_binary=True,\n        )\n\n    def test_bop_serialization(self):\n        _test_serialization(\n            lq.optimizers.CaseOptimizer(\n                (lq.optimizers.Bop.is_binary_variable, lq.optimizers.Bop()),\n                default_optimizer=tf.keras.optimizers.Adam(0.01),\n            ),\n        )\n\n    @pytest.mark.parametrize(\n        ""hyper"", [5e-4, tf.keras.optimizers.schedules.PolynomialDecay(5e-4, 100)],\n    )\n    def test_bop_serialization_schedule(self, hyper):\n        bop = lq.optimizers.Bop(gamma=hyper, threshold=hyper,)\n        new_bop = lq.optimizers.Bop.from_config(bop.get_config())\n        assert isinstance(new_bop._get_hyper(""gamma""), type(bop._get_hyper(""gamma"")))\n        assert isinstance(\n            new_bop._get_hyper(""threshold""), type(bop._get_hyper(""threshold""))\n        )\n'"
larq/quantized_variable.py,9,"b'""""""Contains QuantizedVariable, a variable that can be quantized in the forward pass.""""""\n\nfrom typing import Optional\n\nimport tensorflow as tf\nfrom tensorflow.python.distribute.values import (  # type: ignore\n    AggregatingVariable,\n    DistributedVariable,\n)\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import resource_variable_ops\n\nfrom larq import context\nfrom larq.quantizers import Quantizer\n\n\nclass QuantizedVariable(tf.Variable):\n    """"""A Variable that can be quantized in the forward pass in applicable contexts.""""""\n\n    def __init__(\n        self,\n        variable: tf.Variable,\n        quantizer: Optional[Quantizer] = None,\n        precision: Optional[int] = None,\n    ):\n        """"""Creates an QuantizedVariable instance.\n\n        # Arguments\n            variable: A floating-point resource variable to wrap.\n            quantizer: An optional quantizer to transform the floating-point\n                variable to a fake quantized variable.\n            precision: An optional integer defining the precision of the quantized\n                variable. If `None`, `quantizer.precision` is used.\n        """"""\n        if not resource_variable_ops.is_resource_variable(variable):\n            raise ValueError(\n                ""`variable` must be of type `tf.ResourceVariable`, ""\n                f""but got `{type(variable)}`.""\n            )\n        if not (quantizer is None or callable(quantizer)):\n            raise ValueError(\n                ""`quantizer` must be `callable` or `None`, ""\n                f""but got `{type(quantizer)}`.""\n            )\n        if not (precision is None or type(precision) == int):\n            raise ValueError(\n                ""`precision` must be of type `int` or `None`, ""\n                f""but got `{type(precision)}`.""\n            )\n        self.latent_variable = variable\n        self.quantizer = quantizer\n        self.precision = precision or getattr(quantizer, ""precision"", None)\n\n    @classmethod\n    def from_variable(\n        cls,\n        variable: tf.Variable,\n        quantizer: Optional[Quantizer] = None,\n        precision: Optional[int] = None,\n    ):\n        """"""Creates a QuantizedVariable that wraps another variable.\n\n        This typically just returns `QuantizedVariable(variable)`. But, if the variable\n        is a DistributedVariable or one of its subclasses, we instead dynamically\n        create a class that subclasses from both QuantizedVariable and\n        variable.__class__. This is so the returned variable will still pass\n        `isinstance(variable, variable.__class__)`, which is required for\n        DistributedVariables and its subclasses to work properly.\n\n        # Arguments\n            variable: A floating-point resource variable to wrap.\n            quantizer: An optional quantizer to transform the floating-point variable to\n                a fake quantized variable.\n            precision: An optional integer defining the precision of the quantized\n                variable. If `None`, `quantizer.precision` is used.\n\n        # Returns\n            A QuantizedVariable that wraps the variable.\n        """"""\n        if not isinstance(variable, (DistributedVariable, AggregatingVariable)):  # type: ignore\n            return cls(variable, quantizer, precision)\n\n        class QuantizedDistributedVariable(cls, variable.__class__):\n            """"""A QuantizedVariable that also subclasses from `variable.__class__`.\n\n            `variable.__class__` is either a `DistributedVariable` or an\n            `AggregatingVariable`.\n            """"""\n\n            def get(self, *args, **kwargs):\n                # For some reason this is needed to make unit `x + x` pass on TF 1.14\n                return self._quantize(self.latent_variable.get(*args, **kwargs))\n\n        return QuantizedDistributedVariable(variable, quantizer, precision)\n\n    @staticmethod\n    def _maybe_wrap(\n        variable: tf.Variable,\n        quantizer: Optional[Quantizer],\n        precision: Optional[int],\n        wrap: bool = True,\n    ) -> tf.Variable:\n        """"""Creates an QuantizedVariable that wraps another variable if applicable.\n\n        This function is used to wrap the return value of QuantizedVariable.assign.\n        Unfortunately MirroredVariable.assign will (incorrectly) return a Mirrored\n        value instead of a MirroredVariable. So we cannot properly wrap it in an\n        AutoCastVariable. We return the original variable in that case.\n\n        # Arguments\n            variable: A tf.Variable or op.\n            quantizer: An optional quantizer to transform the floating-point variable to\n                a fake quantized variable.\n            precision: An optional integer defining the precision of the quantized\n                variable. If `None`, `quantizer.precision` is used.\n            wrap: A boolean to define whether to wrap the variable in a\n                `QuantizedVariable`.\n\n        # Returns\n            A QuantizedVariable if wrap is True and variable is a resource variable.\n        """"""\n        if wrap and resource_variable_ops.is_resource_variable(variable):\n            return QuantizedVariable.from_variable(variable, quantizer, precision)\n        return variable\n\n    def _quantize(self, value):\n        if self.quantizer and context.should_quantize():\n            return self.quantizer(value)\n        return value\n\n    def value(self):\n        return self._quantize(self.latent_variable.value())\n\n    def read_value(self):\n        return self._quantize(self.latent_variable.read_value())\n\n    def numpy(self):\n        return self._quantize(self.latent_variable).numpy()\n\n    def sparse_read(self, *args, **kwargs):\n        return self._quantize(self.latent_variable.sparse_read(*args, **kwargs))\n\n    def gather_nd(self, *args, **kwargs):\n        return self._quantize(self.latent_variable.gather_nd(*args, **kwargs))\n\n    def __getattr__(self, name):\n        return getattr(self.latent_variable, name)\n\n    def _dense_var_to_tensor(self, *args, **kwargs):\n        return self._quantize(\n            self.latent_variable._dense_var_to_tensor(*args, **kwargs)\n        )\n\n    def eval(self, session=None):\n        return self._quantize(self.latent_variable).eval(session=session)\n\n    def initialized_value(self):\n        return self._quantize(self.latent_variable.initialized_value())\n\n    @property\n    def initial_value(self):\n        return self._quantize(self.latent_variable.initial_value)\n\n    def _should_act_as_resource_variable(self):\n        """"""Pass resource_variable_ops.is_resource_variable check.""""""\n        pass\n\n    @staticmethod\n    def _get_name(obj) -> str:\n        try:\n            return obj.__name__\n        except AttributeError:\n            return obj.__class__.__name__\n\n    def __repr__(self) -> str:\n        repr_ = (\n            f""<{self.__class__.__name__} \'{self.name}\' ""\n            f""shape={self.shape} dtype={self.dtype.name}""\n        )\n        if self.quantizer is not None:\n            repr_ += f"" quantizer={self._get_name(self.quantizer)}""\n        if self.precision is not None:\n            repr_ += f"" precision={self.precision}""\n        if tf.executing_eagerly() and not self._in_graph_mode:\n            return f""{repr_} numpy={ops.numpy_text(self.read_value(), is_repr=True)}>""\n        return f""{repr_}>""\n\n    # Method delegations: We delegate the following methods to self.latent_variable.\n    # Each of these methods simply calls the same method on self.latent_variable. The\n    # base Variable raises NotImplementedError for most of these, so we must\n    # override them.\n    #\n    # We do not define the following methods from Variable for the following\n    # reasons:\n    #   * \'ref\': Instead we inherit the definition from Variable.\n    #     If we defined and delegated to Variable, the ref of an QuantizedVariable\n    #     would be the same as the ref of the underlying variable, which would be\n    #     strange as they are different Python objects.\n\n    def set_shape(self, *args, **kwargs):\n        return self.latent_variable.set_shape(self, *args, **kwargs)\n\n    @property\n    def trainable(self):\n        return self.latent_variable.trainable\n\n    @property\n    def synchronization(self):\n        return self.latent_variable.synchronization\n\n    @property\n    def aggregation(self):\n        return self.latent_variable.aggregation\n\n    @property\n    def constraint(self):\n        return self.latent_variable.constraint\n\n    def assign(self, value, use_locking=None, name=None, read_value=True):\n        op = self.latent_variable.assign(value, use_locking, name, read_value)\n        return self._maybe_wrap(op, self.quantizer, self.precision, wrap=read_value)\n\n    def assign_add(self, delta, use_locking=None, name=None, read_value=True):\n        op = self.latent_variable.assign_add(delta, use_locking, name, read_value)\n        return self._maybe_wrap(op, self.quantizer, self.precision, wrap=read_value)\n\n    def assign_sub(self, delta, use_locking=None, name=None, read_value=True):\n        op = self.latent_variable.assign_sub(delta, use_locking, name, read_value)\n        return self._maybe_wrap(op, self.quantizer, self.precision, wrap=read_value)\n\n    def scatter_sub(self, *args, **kwargs):\n        var = self.latent_variable.scatter_sub(*args, **kwargs)\n        return self._maybe_wrap(var, self.quantizer, self.precision)\n\n    def scatter_add(self, *args, **kwargs):\n        var = self.latent_variable.scatter_add(*args, **kwargs)\n        return self._maybe_wrap(var, self.quantizer, self.precision)\n\n    def scatter_max(self, *args, **kwargs):\n        var = self.latent_variable.scatter_max(*args, **kwargs)\n        return self._maybe_wrap(var, self.quantizer, self.precision)\n\n    def scatter_min(self, *args, **kwargs):\n        var = self.latent_variable.scatter_min(*args, **kwargs)\n        return self._maybe_wrap(var, self.quantizer, self.precision)\n\n    def scatter_mul(self, *args, **kwargs):\n        var = self.latent_variable.scatter_mul(*args, **kwargs)\n        return self._maybe_wrap(var, self.quantizer, self.precision)\n\n    def scatter_div(self, *args, **kwargs):\n        var = self.latent_variable.scatter_div(*args, **kwargs)\n        return self._maybe_wrap(var, self.quantizer, self.precision)\n\n    def scatter_update(self, *args, **kwargs):\n        var = self.latent_variable.scatter_update(*args, **kwargs)\n        return self._maybe_wrap(var, self.quantizer, self.precision)\n\n    def batch_scatter_update(self, *args, **kwargs):\n        var = self.latent_variable.batch_scatter_update(*args, **kwargs)\n        return self._maybe_wrap(var, self.quantizer, self.precision)\n\n    def scatter_nd_sub(self, *args, **kwargs):\n        var = self.latent_variable.scatter_nd_sub(*args, **kwargs)\n        return self._maybe_wrap(var, self.quantizer, self.precision)\n\n    def scatter_nd_add(self, *args, **kwargs):\n        var = self.latent_variable.scatter_nd_add(*args, **kwargs)\n        return self._maybe_wrap(var, self.quantizer, self.precision)\n\n    def scatter_nd_update(self, *args, **kwargs):\n        var = self.latent_variable.scatter_nd_update(*args, **kwargs)\n        return self._maybe_wrap(var, self.quantizer, self.precision)\n\n    def count_up_to(self, *args, **kwargs):\n        return self.latent_variable.count_up_to(*args, **kwargs)\n\n    def load(self, *args, **kwargs):\n        return self.latent_variable.load(*args, **kwargs)\n\n    @property\n    def dtype(self):\n        return self.latent_variable.dtype\n\n    @property\n    def name(self):\n        return self.latent_variable.name\n\n    @property\n    def _shared_name(self):\n        return self.latent_variable._shared_name\n\n    @property\n    def initializer(self):\n        return self.latent_variable.initializer\n\n    @property\n    def device(self):\n        return self.latent_variable.device\n\n    @property\n    def op(self):\n        return self.latent_variable.op\n\n    @property\n    def graph(self):\n        return self.latent_variable.graph\n\n    @property\n    def shape(self):\n        return self.latent_variable.shape\n\n    def get_shape(self):\n        return self.latent_variable.get_shape()\n\n    def _gather_saveables_for_checkpoint(self):\n        # By delegating this method to the wrapped variable, checkpoints with\n        # QuantizedVariables are identical to checkpoints with normal variables.\n        # Therefore models checkpointed with QuantizedVariables can be restored on\n        # models with normal variables, and vice versa.\n        return self.latent_variable._gather_saveables_for_checkpoint()\n\n    # TODO: Maybe encode the fact the variable is an QuantizedVariable in to_proto().\n    def to_proto(self, *args, **kwargs):\n        return self.latent_variable.to_proto(*args, **kwargs)\n\n    def from_proto(self, *args, **kwargs):\n        return self.latent_variable.from_proto(*args, **kwargs)\n\n    # Delegate the private attributes _handle_name and _initializer_op to\n    # self._variable. SavedModel sets these attributes when loading a model. For\n    # example, it sets _handle_name here:\n    # https://github.com/tensorflow/tensorflow/blob/db26bd574fa95b5bdd53c08463dd19407cc0297e/tensorflow/python/keras/saving/saved_model/load.py#L211\n    # We need to expose these attributes on AutoCastVariable as well for\n    # SavedModel to work properly.\n    # TODO: Find a better way to support SavedModel. Exposing private attributes is\n    # hacky and difficult to maintain.\n    # For more info see https://github.com/tensorflow/tensorflow/commit/1fcda57f37c2ac854cabf1c3462eb14e39d36c60\n    @property\n    def _handle_name(self):\n        return self._variable._handle_name\n\n    @_handle_name.setter\n    def _handle_name(self, handle_name):\n        self._variable._handle_name = handle_name\n\n    @property\n    def _initializer_op(self):\n        return self._variable._initializer_op\n\n    @_initializer_op.setter\n    def _initializer_op(self, initializer_op):\n        self._variable._initializer_op = initializer_op\n\n    def _as_graph_element(self):\n        return self._quantize(self.latent_variable._as_graph_element())\n\n\nQuantizedVariable._OverloadAllOperators()\ntf.register_tensor_conversion_function(\n    QuantizedVariable, QuantizedVariable._dense_var_to_tensor\n)\nops.register_dense_tensor_like_type(QuantizedVariable)\n'"
larq/quantized_variable_test.py,24,"b'import pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_almost_equal, assert_array_equal\nfrom packaging import version\nfrom tensorflow.python.distribute.values import DistributedVariable\n\nfrom larq import context\nfrom larq.quantized_variable import QuantizedVariable\nfrom larq.testing_utils import evaluate\n\n\ndef get_var(val, dtype=None, name=None):\n    return tf.compat.v1.Variable(val, use_resource=True, dtype=dtype, name=name)\n\n\ndef test_inheritance(distribute_scope):\n    variable = get_var(3.0)\n    quantized_variable = QuantizedVariable.from_variable(variable)\n    assert isinstance(quantized_variable, QuantizedVariable)\n    assert isinstance(quantized_variable, tf.Variable)\n    assert isinstance(quantized_variable, DistributedVariable) is distribute_scope  # type: ignore\n\n\ndef test_read(eager_and_graph_mode, distribute_scope):\n    x = QuantizedVariable.from_variable(get_var(3.5), quantizer=lambda x: 2 * x)\n    evaluate(x.initializer)\n\n    assert evaluate(x) == 3.5\n    assert evaluate(x.value()) == 3.5\n    assert evaluate(x.read_value()) == 3.5\n    assert evaluate(tf.identity(x)) == 3.5\n\n    with context.quantized_scope(True):\n        assert evaluate(x) == 7\n        assert evaluate(x.value()) == 7\n        assert evaluate(x.read_value()) == 7\n        assert evaluate(tf.identity(x)) == 7\n\n\ndef test_sparse_reads(eager_and_graph_mode):\n    x = QuantizedVariable.from_variable(get_var([1.0, 2.0]), quantizer=lambda x: 2 * x)\n    evaluate(x.initializer)\n\n    assert evaluate(x.sparse_read([0])) == 1\n    assert evaluate(x.gather_nd([0])) == 1\n    with context.quantized_scope(True):\n        assert evaluate(x.sparse_read([0])) == 2\n        assert evaluate(x.gather_nd([0])) == 2\n\n\ndef test_read_nested_scopes(eager_and_graph_mode, distribute_scope):\n    x = QuantizedVariable.from_variable(get_var(3.5), quantizer=lambda x: 2 * x)\n    evaluate(x.initializer)\n    with context.quantized_scope(True):\n        assert evaluate(x.read_value()) == 7\n        with context.quantized_scope(False):\n            assert evaluate(x.read_value()) == 3.5\n        assert evaluate(x.read_value()) == 7\n\n\ndef test_method_delegations(eager_and_graph_mode, distribute_scope):\n    x = QuantizedVariable.from_variable(get_var(3.5), quantizer=lambda x: 2 * x)\n    with context.quantized_scope(True):\n        evaluate(x.initializer)\n        assert evaluate(x.value()) == 7\n        assert evaluate(x.read_value()) == 7\n        assert x.trainable\n        if version.parse(tf.__version__) > version.parse(""1.14""):\n            assert x.synchronization == x.latent_variable.synchronization\n        assert x.aggregation == x.latent_variable.aggregation\n        assert evaluate(x.initialized_value()) == 7\n        if not tf.executing_eagerly():\n            if not distribute_scope:\n                # These functions are not supported for DistributedVariables\n                x.load(4.5)\n                assert x.eval() == 9\n            assert evaluate(x.initial_value) == 7\n            assert x.op == x.latent_variable.op\n            assert x.graph == x.latent_variable.graph\n        if not distribute_scope:\n            # These attributes are not supported for DistributedVariables\n            assert x.constraint is None\n            assert x.initializer == x.latent_variable.initializer\n        assert evaluate(x.assign(4)) == 8\n        assert evaluate(x.assign_add(1)) == 10\n        assert evaluate(x.assign_sub(1.5)) == 7\n        assert x.name == x.latent_variable.name\n        assert x.device == x.latent_variable.device\n        assert x.shape == ()\n        assert x.get_shape() == ()\n\n\ndef test_scatter_method_delegations(eager_and_graph_mode):\n    x = QuantizedVariable.from_variable(get_var([3.5, 4]), quantizer=lambda x: 2 * x)\n    evaluate(x.initializer)\n    with context.quantized_scope(True):\n        assert_array_equal(evaluate(x.value()), [7, 8])\n\n        def slices(val, index):\n            return tf.IndexedSlices(\n                values=tf.constant(val, dtype=tf.float32),\n                indices=tf.constant(index, dtype=tf.int32),\n                dense_shape=tf.constant([2], dtype=tf.int32),\n            )\n\n        assert_array_equal(evaluate(x.scatter_sub(slices(0.5, 0))), [6, 8])\n        assert_array_equal(evaluate(x.scatter_add(slices(0.5, 0))), [7, 8])\n        if version.parse(tf.__version__) > version.parse(""1.14""):\n            assert_array_equal(evaluate(x.scatter_max(slices(4.5, 1))), [7, 9])\n            assert_array_equal(evaluate(x.scatter_min(slices(4.0, 1))), [7, 8])\n            assert_array_equal(evaluate(x.scatter_mul(slices(2.0, 1))), [7, 16])\n            assert_array_equal(evaluate(x.scatter_div(slices(2.0, 1))), [7, 8])\n        assert_array_equal(evaluate(x.scatter_update(slices(2, 1))), [7, 4])\n        assert_array_equal(evaluate(x.scatter_nd_sub([[0], [1]], [0.5, 1.0])), [6, 2])\n        assert_array_equal(evaluate(x.scatter_nd_add([[0], [1]], [0.5, 1.0])), [7, 4])\n        assert_array_equal(\n            evaluate(x.scatter_nd_update([[0], [1]], [0.5, 1.0])), [1, 2]\n        )\n\n\ndef test_overloads(eager_and_graph_mode, quantized, distribute_scope):\n    if quantized:\n        x = QuantizedVariable.from_variable(get_var(3.5), quantizer=lambda x: 2 * x)\n    else:\n        x = QuantizedVariable.from_variable(get_var(7.0))\n    evaluate(x.initializer)\n    assert_almost_equal(8, evaluate(x + 1))\n    assert_almost_equal(10, evaluate(3 + x))\n    assert_almost_equal(14, evaluate(x + x))\n    assert_almost_equal(5, evaluate(x - 2))\n    assert_almost_equal(6, evaluate(13 - x))\n    assert_almost_equal(0, evaluate(x - x))\n    assert_almost_equal(14, evaluate(x * 2))\n    assert_almost_equal(21, evaluate(3 * x))\n    assert_almost_equal(49, evaluate(x * x))\n    assert_almost_equal(3.5, evaluate(x / 2))\n    assert_almost_equal(1.5, evaluate(10.5 / x))\n    assert_almost_equal(3, evaluate(x // 2))\n    assert_almost_equal(2, evaluate(15 // x))\n    assert_almost_equal(1, evaluate(x % 2))\n    assert_almost_equal(2, evaluate(16 % x))\n    assert evaluate(x < 12)\n    assert evaluate(x <= 12)\n    assert not evaluate(x > 12)\n    assert not evaluate(x >= 12)\n    assert not evaluate(12 < x)\n    assert not evaluate(12 <= x)\n    assert evaluate(12 > x)\n    assert evaluate(12 >= x)\n    assert_almost_equal(343, evaluate(pow(x, 3)))\n    assert_almost_equal(128, evaluate(pow(2, x)))\n    assert_almost_equal(-7, evaluate(-x))\n    assert_almost_equal(7, evaluate(abs(x)))\n\n\ndef test_tensor_equality(quantized, eager_mode):\n    if quantized:\n        x = QuantizedVariable.from_variable(\n            get_var([3.5, 4.0, 4.5]), quantizer=lambda x: 2 * x\n        )\n    else:\n        x = QuantizedVariable.from_variable(get_var([7.0, 8.0, 9.0]))\n    evaluate(x.initializer)\n    assert_array_equal(evaluate(x), [7.0, 8.0, 9.0])\n    if version.parse(tf.__version__) >= version.parse(""2""):\n        assert_array_equal(x == [7.0, 8.0, 10.0], [True, True, False])\n        assert_array_equal(x != [7.0, 8.0, 10.0], [False, False, True])\n\n\ndef test_assign(eager_and_graph_mode, quantized, distribute_scope):\n    x = QuantizedVariable.from_variable(\n        get_var(0.0, tf.float64), quantizer=lambda x: 2 * x\n    )\n    evaluate(x.initializer)\n\n    latent_value = 3.14\n    value = latent_value * 2 if quantized else latent_value\n\n    # Assign float32 values\n    lv = tf.constant(latent_value, dtype=tf.float64)\n    assert_almost_equal(evaluate(x.assign(lv)), value)\n    assert_almost_equal(evaluate(x.assign_add(lv)), value * 2)\n    assert_almost_equal(evaluate(x.assign_sub(lv)), value)\n\n    # Assign Python floats\n    assert_almost_equal(evaluate(x.assign(0.0)), 0.0)\n    assert_almost_equal(evaluate(x.assign(latent_value)), value)\n    assert_almost_equal(evaluate(x.assign_add(latent_value)), value * 2)\n    assert_almost_equal(evaluate(x.assign_sub(latent_value)), value)\n\n    # Assign multiple times\n    assign = x.assign(0.0)\n    assert_almost_equal(evaluate(assign), 0.0)\n    assert_almost_equal(evaluate(assign.assign(latent_value)), value)\n    if version.parse(tf.__version__) >= version.parse(""2.2""):\n        assert_almost_equal(\n            evaluate(x.assign_add(latent_value).assign_add(latent_value)), value * 3\n        )\n        assert_almost_equal(evaluate(x), value * 3)\n        assert_almost_equal(\n            evaluate(x.assign_sub(latent_value).assign_sub(latent_value)), value\n        )\n        assert_almost_equal(evaluate(x), value)\n\n    # Assign with read_value=False\n    assert_almost_equal(evaluate(x.assign(0.0)), 0.0)\n    assert evaluate(x.assign(latent_value, read_value=False)) is None\n    assert_almost_equal(evaluate(x), value)\n    assert evaluate(x.assign_add(latent_value, read_value=False)) is None\n    assert_almost_equal(evaluate(x), 2 * value)\n    assert evaluate(x.assign_sub(latent_value, read_value=False)) is None\n    assert_almost_equal(evaluate(x), value)\n\n    # Use the tf.assign functions instead of the var.assign methods.\n    assert_almost_equal(evaluate(tf.compat.v1.assign(x, 0.0)), 0.0)\n    assert_almost_equal(evaluate(tf.compat.v1.assign(x, latent_value)), value)\n    assert_almost_equal(evaluate(tf.compat.v1.assign_add(x, latent_value)), value * 2)\n    assert_almost_equal(evaluate(tf.compat.v1.assign_sub(x, latent_value)), value)\n\n\ndef test_checkpoint(tmp_path, eager_and_graph_mode):\n    x = QuantizedVariable.from_variable(get_var(0.0), quantizer=lambda x: 2 * x)\n    evaluate(x.initializer)\n    evaluate(x.assign(123.0))\n\n    checkpoint = tf.train.Checkpoint(x=x)\n    save_path = checkpoint.save(tmp_path)\n    evaluate(x.assign(234.0))\n    checkpoint.restore(save_path).assert_consumed().run_restore_ops()\n    assert isinstance(x, QuantizedVariable)\n    assert evaluate(x) == 123.0\n    with context.quantized_scope(True):\n        assert evaluate(x) == 123.0 * 2\n\n\ndef test_invalid_wrapped_usage(distribute_scope):\n    with pytest.raises(ValueError, match=""`variable` must be of type""):\n        QuantizedVariable.from_variable(tf.constant([1.0]))\n    with pytest.raises(ValueError, match=""`quantizer` must be `callable` or `None`""):\n        QuantizedVariable.from_variable(get_var([1.0]), 1)  # type: ignore\n    with pytest.raises(ValueError, match=""`precision` must be of type `int` or `None`""):\n        QuantizedVariable.from_variable(get_var([1.0]), precision=1.0)  # type: ignore\n\n\ndef test_repr(snapshot, eager_and_graph_mode):\n    x = get_var(0.0, name=""x"")\n\n    class Quantizer:\n        def __call__(self, x):\n            return x\n\n    snapshot.assert_match(\n        repr(QuantizedVariable.from_variable(x, quantizer=lambda x: 2 * x))\n    )\n    snapshot.assert_match(\n        repr(QuantizedVariable.from_variable(x, quantizer=Quantizer()))\n    )\n    snapshot.assert_match(repr(QuantizedVariable.from_variable(x, precision=1)))\n\n\n@pytest.mark.parametrize(""should_quantize"", [True, False])\ndef test_optimizer(eager_mode, should_quantize):\n    x = QuantizedVariable.from_variable(get_var(1.0), quantizer=lambda x: -x)\n    opt = tf.keras.optimizers.SGD(1.0)\n\n    def loss():\n        with context.quantized_scope(should_quantize):\n            return x + 1.0\n\n    @tf.function\n    def f():\n        opt.minimize(loss, var_list=[x])\n\n    f()\n    if should_quantize:\n        assert evaluate(x) == 2.0\n        with context.quantized_scope(should_quantize):\n            assert evaluate(x) == -2.0\n    else:\n        assert evaluate(x) == 0.0\n'"
larq/quantizers.py,32,"b'""""""A Quantizer defines the way of transforming a full precision input to a\nquantized output and the pseudo-gradient method used for the backwards pass.\n\nQuantizers can either be used through quantizer arguments that are supported\nfor Larq layers, such as `input_quantizer` and `kernel_quantizer`; or they\ncan be used similar to activations, i.e. either through an `Activation` layer,\nor through the `activation` argument supported by all forward layers:\n\n```python\nimport tensorflow as tf\nimport larq as lq\n...\nx = lq.layers.QuantDense(64, activation=None)(x)\nx = lq.layers.QuantDense(64, input_quantizer=""ste_sign"")(x)\n```\n\nis equivalent to:\n\n```python\nx = lq.layers.QuantDense(64)(x)\nx = tf.keras.layers.Activation(""ste_sign"")(x)\nx = lq.layers.QuantDense(64)(x)\n```\n\nas well as:\n\n```python\nx = lq.layers.QuantDense(64, activation=""ste_sign"")(x)\nx = lq.layers.QuantDense(64)(x)\n```\n\nWe highly recommend using the first of these formulations: for the\nother two formulations, intermediate layers - like batch normalization or\naverage pooling - and shortcut connections may result in non-binary input\nto the convolutions.\n\nQuantizers can either be referenced by string or called directly.\nThe following usages are equivalent:\n\n```python\nlq.layers.QuantDense(64, kernel_quantizer=""ste_sign"")\n```\n```python\nlq.layers.QuantDense(64, kernel_quantizer=lq.quantizers.SteSign(clip_value=1.0))\n```\n""""""\n\nfrom typing import Callable, Union\n\nimport tensorflow as tf\n\nfrom larq import context, math, metrics as lq_metrics, utils\n\n__all__ = [\n    ""ApproxSign"",\n    ""DoReFa"",\n    ""DoReFaQuantizer"",\n    ""MagnitudeAwareSign"",\n    ""NoOp"",\n    ""NoOpQuantizer"",\n    ""SteHeaviside"",\n    ""SteSign"",\n    ""SteTern"",\n    ""SwishSign"",\n]\n\n\ndef _clipped_gradient(x, dy, clip_value):\n    """"""Calculate `clipped_gradent * dy`.""""""\n\n    if clip_value is None:\n        return dy\n\n    zeros = tf.zeros_like(dy)\n    mask = tf.math.less_equal(tf.math.abs(x), clip_value)\n    return tf.where(mask, dy, zeros)\n\n\ndef ste_sign(x: tf.Tensor, clip_value: float = 1.0) -> tf.Tensor:\n    @tf.custom_gradient\n    def _call(x):\n        def grad(dy):\n            return _clipped_gradient(x, dy, clip_value)\n\n        return math.sign(x), grad\n\n    return _call(x)\n\n\ndef _scaled_sign(x):  # pragma: no cover\n    return 1.3 * ste_sign(x)\n\n\n@tf.custom_gradient\ndef approx_sign(x: tf.Tensor) -> tf.Tensor:\n    def grad(dy):\n        abs_x = tf.math.abs(x)\n        zeros = tf.zeros_like(dy)\n        mask = tf.math.less_equal(abs_x, 1.0)\n        return tf.where(mask, (1 - abs_x) * 2 * dy, zeros)\n\n    return math.sign(x), grad\n\n\ndef swish_sign(x: tf.Tensor, beta: float = 5.0) -> tf.Tensor:\n    @tf.custom_gradient\n    def _call(x):\n        def grad(dy):\n            b_x = beta * x\n            return dy * beta * (2 - b_x * tf.tanh(b_x * 0.5)) / (1 + tf.cosh(b_x))\n\n        return math.sign(x), grad\n\n    return _call(x)\n\n\ndef ste_tern(\n    x: tf.Tensor,\n    threshold_value: float = 0.05,\n    ternary_weight_networks: bool = False,\n    clip_value: float = 1.0,\n) -> tf.Tensor:\n    @tf.custom_gradient\n    def _call(x):\n        if ternary_weight_networks:\n            threshold = 0.7 * tf.reduce_sum(tf.abs(x)) / tf.cast(tf.size(x), x.dtype)\n        else:\n            threshold = threshold_value\n\n        def grad(dy):\n            return _clipped_gradient(x, dy, clip_value)\n\n        return tf.sign(tf.sign(x + threshold) + tf.sign(x - threshold)), grad\n\n    return _call(x)\n\n\ndef ste_heaviside(x: tf.Tensor, clip_value: float = 1.0) -> tf.Tensor:\n    @tf.custom_gradient\n    def _call(x):\n        def grad(dy):\n            return _clipped_gradient(x, dy, clip_value)\n\n        return math.heaviside(x), grad\n\n    return _call(x)\n\n\nclass BaseQuantizer(tf.keras.layers.Layer):\n    """"""Base class for defining quantizers with Larq metrics.""""""\n\n    def __init__(self, *args, metrics=None, **kwargs):\n        self._custom_metrics = metrics\n        super().__init__(*args, **kwargs)\n\n    def build(self, input_shape):\n        if self._custom_metrics and ""flip_ratio"" in self._custom_metrics:\n            self.flip_ratio = lq_metrics.FlipRatio(name=f""flip_ratio/{self.name}"")\n\n    def call(self, inputs):\n        if hasattr(self, ""flip_ratio""):\n            self.add_metric(self.flip_ratio(inputs))\n        return inputs\n\n    @property\n    def non_trainable_weights(self):\n        return []\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n\n@utils.register_keras_custom_object\nclass NoOp(BaseQuantizer):\n    r""""""Instantiates a serializable no-op quantizer.\n\n    \\\\[\n    q(x) = x\n    \\\\]\n\n    !!! warning\n        This quantizer will not change the input variable. It is only intended to mark\n        variables with a desired precision that will be recognized by optimizers like\n        `Bop` and add training metrics to track variable changes.\n\n    !!! example\n        ```python\n        layer = lq.layers.QuantDense(\n            16, kernel_quantizer=lq.quantizers.NoOpQuantizer(precision=1),\n        )\n        layer.build((32,))\n        assert layer.kernel.precision == 1\n        ```\n\n    # Arguments\n        precision: Set the desired precision of the variable. This can be used to tag\n        metrics: An array of metrics to add to the layer. If `None` the metrics set in\n            `larq.context.metrics_scope` are used. Currently only the `flip_ratio`\n            metric is available.\n    """"""\n    precision = None\n\n    def __init__(self, precision: int, **kwargs):\n        self.precision = precision\n        super().__init__(**kwargs)\n\n    def get_config(self):\n        return {**super().get_config(), ""precision"": self.precision}\n\n\n# `NoOp` used to be called `NoOpQuantizer`; this alias is for\n# backwards-compatibility.\nNoOpQuantizer = NoOp\n\n\n@utils.register_alias(""ste_sign"")\n@utils.register_keras_custom_object\nclass SteSign(BaseQuantizer):\n    r""""""Instantiates a serializable binary quantizer.\n\n    \\\\[\n    q(x) = \\begin{cases}\n      -1 & x < 0 \\\\\\\n      1 & x \\geq 0\n    \\end{cases}\n    \\\\]\n\n    The gradient is estimated using the Straight-Through Estimator\n    (essentially the binarization is replaced by a clipped identity on the\n    backward pass).\n    \\\\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases}\n      1 & \\left|x\\right| \\leq \\texttt{clip_value} \\\\\\\n      0 & \\left|x\\right| > \\texttt{clip_value}\n    \\end{cases}\\\\]\n\n    ```plot-activation\n    quantizers.SteSign\n    ```\n\n    # Arguments\n        clip_value: Threshold for clipping gradients. If `None` gradients are not\n            clipped.\n        metrics: An array of metrics to add to the layer. If `None` the metrics set in\n            `larq.context.metrics_scope` are used. Currently only the `flip_ratio`\n            metric is available.\n\n    # References\n        - [Binarized Neural Networks: Training Deep Neural Networks with Weights and\n            Activations Constrained to +1 or -1](https://arxiv.org/abs/1602.02830)\n    """"""\n    precision = 1\n\n    def __init__(self, clip_value: float = 1.0, **kwargs):\n        self.clip_value = clip_value\n        super().__init__(**kwargs)\n\n    def call(self, inputs):\n        outputs = ste_sign(inputs, clip_value=self.clip_value)\n        return super().call(outputs)\n\n    def get_config(self):\n        return {**super().get_config(), ""clip_value"": self.clip_value}\n\n\n@utils.register_alias(""approx_sign"")\n@utils.register_keras_custom_object\nclass ApproxSign(BaseQuantizer):\n    r""""""Instantiates a serializable binary quantizer.\n    \\\\[\n    q(x) = \\begin{cases}\n      -1 & x < 0 \\\\\\\n      1 & x \\geq 0\n    \\end{cases}\n    \\\\]\n\n    The gradient is estimated using the ApproxSign method.\n    \\\\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases}\n      (2 - 2 \\left|x\\right|) & \\left|x\\right| \\leq 1 \\\\\\\n      0 & \\left|x\\right| > 1\n    \\end{cases}\n    \\\\]\n\n    ```plot-activation\n    quantizers.ApproxSign\n    ```\n\n    # Arguments\n        metrics: An array of metrics to add to the layer. If `None` the metrics set in\n            `larq.context.metrics_scope` are used. Currently only the `flip_ratio`\n            metric is available.\n\n    # References\n        - [Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved\n            Representational Capability and Advanced Training\n            Algorithm](https://arxiv.org/abs/1808.00278)\n    """"""\n    precision = 1\n\n    def call(self, inputs):\n        outputs = approx_sign(inputs)\n        return super().call(outputs)\n\n\n@utils.register_alias(""ste_heaviside"")\n@utils.register_keras_custom_object\nclass SteHeaviside(BaseQuantizer):\n    r""""""\n    Instantiates a binarization quantizer with output values 0 and 1.\n    \\\\[\n    q(x) = \\begin{cases}\n    +1 & x > 0 \\\\\\\n    0 & x \\leq 0\n    \\end{cases}\n    \\\\]\n\n    The gradient is estimated using the Straight-Through Estimator\n    (essentially the binarization is replaced by a clipped identity on the\n    backward pass).\n\n    \\\\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases}\n    1 & \\left|x\\right| \\leq 1 \\\\\\\n    0 & \\left|x\\right| > 1\n    \\end{cases}\\\\]\n\n    ```plot-activation\n    quantizers.SteHeaviside\n    ```\n\n    # Arguments\n        clip_value: Threshold for clipping gradients. If `None` gradients are not\n            clipped.\n        metrics: An array of metrics to add to the layer. If `None` the metrics set in\n            `larq.context.metrics_scope` are used. Currently only the `flip_ratio`\n            metric is available.\n\n    # Returns\n        AND Binarization function\n    """"""\n    precision = 1\n\n    def __init__(self, clip_value: float = 1.0, **kwargs):\n        self.clip_value = clip_value\n        super().__init__(**kwargs)\n\n    def call(self, inputs):\n        outputs = ste_heaviside(inputs, clip_value=self.clip_value)\n        return super().call(outputs)\n\n    def get_config(self):\n        return {**super().get_config(), ""clip_value"": self.clip_value}\n\n\n@utils.register_alias(""swish_sign"")\n@utils.register_keras_custom_object\nclass SwishSign(BaseQuantizer):\n    r""""""Sign binarization function.\n\n    \\\\[\n    q(x) = \\begin{cases}\n      -1 & x < 0 \\\\\\\n      1 & x \\geq 0\n    \\end{cases}\n    \\\\]\n\n    The gradient is estimated using the SignSwish method.\n\n    \\\\[\n    \\frac{\\partial q_{\\beta}(x)}{\\partial x} = \\frac{\\beta\\left\\\\{2-\\beta x \\tanh \\left(\\frac{\\beta x}{2}\\right)\\right\\\\}}{1+\\cosh (\\beta x)}\n    \\\\]\n\n    ```plot-activation\n    quantizers.SwishSign\n    ```\n    # Arguments\n        beta: Larger values result in a closer approximation to the derivative of the\n            sign.\n        metrics: An array of metrics to add to the layer. If `None` the metrics set in\n            `larq.context.metrics_scope` are used. Currently only the `flip_ratio`\n            metric is available.\n\n    # Returns\n        SwishSign quantization function\n\n    # References\n        - [BNN+: Improved Binary Network Training](https://arxiv.org/abs/1812.11800)\n    """"""\n    precision = 1\n\n    def __init__(self, beta: float = 5.0, **kwargs):\n        self.beta = beta\n        super().__init__(**kwargs)\n\n    def call(self, inputs):\n        outputs = swish_sign(inputs, beta=self.beta)\n        return super().call(outputs)\n\n    def get_config(self):\n        return {**super().get_config(), ""beta"": self.beta}\n\n\n@utils.register_alias(""magnitude_aware_sign"")\n@utils.register_keras_custom_object\nclass MagnitudeAwareSign(BaseQuantizer):\n    r""""""Instantiates a serializable magnitude-aware sign quantizer for Bi-Real Net.\n\n    A scaled sign function computed according to Section 3.3 in\n    [Zechun Liu et al](https://arxiv.org/abs/1808.00278).\n\n    ```plot-activation\n    quantizers._scaled_sign\n    ```\n\n    # Arguments\n        clip_value: Threshold for clipping gradients. If `None` gradients are not\n            clipped.\n        metrics: An array of metrics to add to the layer. If `None` the metrics set in\n            `larq.context.metrics_scope` are used. Currently only the `flip_ratio`\n            metric is available.\n\n    # References\n        - [Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved\n        Representational Capability and Advanced Training\n        Algorithm](https://arxiv.org/abs/1808.00278)\n\n    """"""\n    precision = 1\n\n    def __init__(self, clip_value: float = 1.0, **kwargs):\n        self.clip_value = clip_value\n        super().__init__(**kwargs)\n\n    def call(self, inputs):\n        scale_factor = tf.stop_gradient(\n            tf.reduce_mean(tf.abs(inputs), axis=list(range(len(inputs.shape) - 1)))\n        )\n\n        outputs = scale_factor * ste_sign(inputs, clip_value=self.clip_value)\n        return super().call(outputs)\n\n    def get_config(self):\n        return {**super().get_config(), ""clip_value"": self.clip_value}\n\n\n@utils.register_alias(""ste_tern"")\n@utils.register_keras_custom_object\nclass SteTern(BaseQuantizer):\n    r""""""Instantiates a serializable ternarization quantizer.\n\n    \\\\[\n    q(x) = \\begin{cases}\n    +1 & x > \\Delta \\\\\\\n    0 & |x| < \\Delta \\\\\\\n     -1 & x < - \\Delta\n    \\end{cases}\n    \\\\]\n\n    where \\\\(\\Delta\\\\) is defined as the threshold and can be passed as an argument,\n    or can be calculated as per the Ternary Weight Networks original paper, such that\n\n    \\\\[\n    \\Delta = \\frac{0.7}{n} \\sum_{i=1}^{n} |W_i|\n    \\\\]\n    where we assume that \\\\(W_i\\\\) is generated from a normal distribution.\n\n    The gradient is estimated using the Straight-Through Estimator\n    (essentially the Ternarization is replaced by a clipped identity on the\n    backward pass).\n    \\\\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases}\n    1 & \\left|x\\right| \\leq \\texttt{clip_value} \\\\\\\n    0 & \\left|x\\right| > \\texttt{clip_value}\n    \\end{cases}\\\\]\n\n    ```plot-activation\n    quantizers.SteTern\n    ```\n\n    # Arguments\n        threshold_value: The value for the threshold, \\\\(\\Delta\\\\).\n        ternary_weight_networks: Boolean of whether to use the\n            Ternary Weight Networks threshold calculation.\n        clip_value: Threshold for clipping gradients. If `None` gradients are not\n            clipped.\n        metrics: An array of metrics to add to the layer. If `None` the metrics set in\n            `larq.context.metrics_scope` are used. Currently only the `flip_ratio`\n            metric is available.\n\n    # References\n        - [Ternary Weight Networks](https://arxiv.org/abs/1605.04711)\n    """"""\n\n    precision = 2\n\n    def __init__(\n        self,\n        threshold_value: float = 0.05,\n        ternary_weight_networks: bool = False,\n        clip_value: float = 1.0,\n        **kwargs,\n    ):\n        self.threshold_value = threshold_value\n        self.ternary_weight_networks = ternary_weight_networks\n        self.clip_value = clip_value\n        super().__init__(**kwargs)\n\n    def call(self, inputs):\n        outputs = ste_tern(\n            inputs,\n            threshold_value=self.threshold_value,\n            ternary_weight_networks=self.ternary_weight_networks,\n            clip_value=self.clip_value,\n        )\n        return super().call(outputs)\n\n    def get_config(self):\n        return {\n            **super().get_config(),\n            ""threshold_value"": self.threshold_value,\n            ""ternary_weight_networks"": self.ternary_weight_networks,\n            ""clip_value"": self.clip_value,\n        }\n\n\n@utils.register_alias(""dorefa_quantizer"")\n@utils.register_keras_custom_object\nclass DoReFa(BaseQuantizer):\n    r""""""Instantiates a serializable k_bit quantizer as in the DoReFa paper.\n\n    \\\\[\n    q(x) = \\begin{cases}\n    0 & x < \\frac{1}{2n} \\\\\\\n    \\frac{i}{n} & \\frac{2i-1}{2n} < x < \\frac{2i+1}{2n} \\text{ for } i \\in \\\\{1,n-1\\\\}\\\\\\\n     1 & \\frac{2n-1}{2n} < x\n    \\end{cases}\n    \\\\]\n\n    where \\\\(n = 2^{\\text{k_bit}} - 1\\\\). The number of bits, k_bit, needs to be passed\n    as an argument.\n    The gradient is estimated using the Straight-Through Estimator\n    (essentially the binarization is replaced by a clipped identity on the\n    backward pass).\n    \\\\[\\frac{\\partial q(x)}{\\partial x} = \\begin{cases}\n    1 &  0 \\leq x \\leq 1 \\\\\\\n    0 & \\text{else}\n    \\end{cases}\\\\]\n\n    !!! warning\n       While the DoReFa paper describes how to do quantization for both weights and\n       activations, this implementation is only valid for activations, and this\n       quantizer should therefore not be used as a kernel quantizer.\n\n    ```plot-activation\n    quantizers.DoReFa\n    ```\n\n    # Arguments\n        k_bit: number of bits for the quantization.\n        metrics: An array of metrics to add to the layer. If `None` the metrics set in\n            `larq.context.metrics_scope` are used. Currently only the `flip_ratio`\n            metric is available.\n\n    # Returns\n        Quantization function\n\n    # References\n        - [DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low\n            Bitwidth Gradients](https://arxiv.org/abs/1606.06160)\n    """"""\n    precision = None\n\n    def __init__(self, k_bit: int = 2, **kwargs):\n        self.precision = k_bit\n        super().__init__(**kwargs)\n\n    def call(self, inputs):\n        inputs = tf.clip_by_value(inputs, 0.0, 1.0)\n\n        @tf.custom_gradient\n        def _k_bit_with_identity_grad(x):\n            n = 2 ** self.precision - 1\n            return tf.round(x * n) / n, lambda dy: dy\n\n        outputs = _k_bit_with_identity_grad(inputs)\n        return super().call(outputs)\n\n    def get_config(self):\n        return {**super().get_config(), ""k_bit"": self.precision}\n\n\n# `DoReFa` used to be called `DoReFaQuantizer`; this alias is for\n# backwards-compatibility.\nDoReFaQuantizer = DoReFa\n\n\nQuantizer = Union[tf.keras.layers.Layer, Callable[[tf.Tensor], tf.Tensor]]\n\n\ndef serialize(quantizer: tf.keras.layers.Layer):\n    return tf.keras.utils.serialize_keras_object(quantizer)\n\n\ndef deserialize(name, custom_objects=None):\n    return tf.keras.utils.deserialize_keras_object(\n        name,\n        module_objects=globals(),\n        custom_objects=custom_objects,\n        printable_module_name=""quantization function"",\n    )\n\n\ndef get(identifier):\n    if identifier is None:\n        return None\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n    if isinstance(identifier, str):\n        return deserialize(str(identifier))\n    if callable(identifier):\n        return identifier\n    raise ValueError(\n        f""Could not interpret quantization function identifier: {identifier}""\n    )\n\n\ndef get_kernel_quantizer(identifier):\n    """"""Returns a quantizer from identifier and adds default kernel quantizer metrics.\n\n    # Arguments\n        identifier: Function or string\n\n    # Returns\n        `Quantizer` or `None`\n    """"""\n    quantizer = get(identifier)\n    if isinstance(quantizer, BaseQuantizer) and not quantizer._custom_metrics:\n        quantizer._custom_metrics = list(context.get_training_metrics())\n    return quantizer\n'"
larq/quantizers_test.py,40,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\nfrom packaging import version\n\nimport larq as lq\nfrom larq import testing_utils\n\n\nclass DummyTrainableQuantizer(tf.keras.layers.Layer):\n    """"""Used to test whether we can set layers as quantizers without any throws.""""""\n\n    _custom_metrics = None\n\n    def build(self, input_shape):\n        self.dummy_weight = self.add_weight(""dummy_weight"", trainable=True)\n        super().build(input_shape)\n\n    def call(self, inputs):\n        return self.dummy_weight * inputs\n\n\nclass TestCommonFunctionality:\n    """"""Test functionality common to all quantizers, like serialization and usage.""""""\n\n    @pytest.mark.parametrize(""module"", [lq.quantizers, tf.keras.activations])\n    @pytest.mark.parametrize(\n        ""name,ref_cls"",\n        [\n            (""ste_sign"", lq.quantizers.SteSign),\n            (""approx_sign"", lq.quantizers.ApproxSign),\n            (""ste_heaviside"", lq.quantizers.SteHeaviside),\n            (""magnitude_aware_sign"", lq.quantizers.MagnitudeAwareSign),\n            (""swish_sign"", lq.quantizers.SwishSign),\n            (""ste_tern"", lq.quantizers.SteTern),\n        ],\n    )\n    def test_serialization(self, module, name, ref_cls):\n        fn = module.get(name)\n        assert fn.__class__ == ref_cls\n        fn = module.get(ref_cls())\n        assert fn.__class__ == ref_cls\n        assert type(fn.precision) == int\n        if module == tf.keras.activations and version.parse(\n            tf.__version__\n        ) < version.parse(""1.15""):\n            pytest.skip(\n                ""TensorFlow < 1.15 does not support Quantizer classes as activations""\n            )\n        config = module.serialize(fn)\n        fn = module.deserialize(config)\n        assert fn.__class__ == ref_cls\n        assert type(fn.precision) == int\n\n    def test_noop_serialization(self):\n        fn = lq.quantizers.get(lq.quantizers.NoOp(precision=1))\n        assert fn.__class__ == lq.quantizers.NoOp\n        assert fn.precision == 1\n        config = lq.quantizers.serialize(fn)\n        fn = lq.quantizers.deserialize(config)\n        assert fn.__class__ == lq.quantizers.NoOp\n        assert fn.precision == 1\n\n    def test_invalid_usage(self):\n        with pytest.raises(ValueError):\n            lq.quantizers.get(42)\n        with pytest.raises(ValueError):\n            lq.quantizers.get(""unknown"")\n\n    @pytest.mark.parametrize(""quantizer"", [""input_quantizer"", ""kernel_quantizer""])\n    def test_layer_as_quantizer(self, quantizer, keras_should_run_eagerly):\n        """"""Test whether a keras.layers.Layer can be used as quantizer.""""""\n\n        input_data = testing_utils.random_input((1, 10))\n\n        model = tf.keras.Sequential(\n            [lq.layers.QuantDense(1, **{quantizer: DummyTrainableQuantizer()})]\n        )\n        model.compile(optimizer=""sgd"", loss=""mse"", run_eagerly=keras_should_run_eagerly)\n        model.fit(input_data, np.ones((1,)), epochs=1)\n\n        assert any([""dummy_weight"" in var.name for var in model.trainable_variables])\n\n\nclass TestQuantization:\n    """"""Test binarization and ternarization.""""""\n\n    @pytest.mark.parametrize(\n        ""fn"",\n        [\n            ""ste_sign"",\n            lq.quantizers.SteSign(),\n            ""approx_sign"",\n            lq.quantizers.ApproxSign(),\n            ""swish_sign"",\n            lq.quantizers.SwishSign(),\n        ],\n    )\n    def test_xnor_binarization(self, fn):\n        x = tf.keras.backend.placeholder(ndim=2)\n        f = tf.keras.backend.function([x], [lq.quantizers.get(fn)(x)])\n        binarized_values = np.random.choice([-1, 1], size=(2, 5))\n        result = f([binarized_values])[0]\n        np.testing.assert_allclose(result, binarized_values)\n\n        real_values = testing_utils.generate_real_values_with_zeros()\n        result = f([real_values])[0]\n        assert not np.any(result == 0)\n        assert np.all(result[real_values < 0] == -1)\n        assert np.all(result[real_values >= 0] == 1)\n\n        zero_values = np.zeros((2, 5))\n        result = f([zero_values])[0]\n        assert np.all(result == 1)\n\n    @pytest.mark.parametrize(""fn"", [""ste_heaviside"", lq.quantizers.SteHeaviside()])\n    def test_and_binarization(self, fn):\n        x = tf.keras.backend.placeholder(ndim=2)\n        f = tf.keras.backend.function([x], [lq.quantizers.get(fn)(x)])\n\n        binarized_values = np.random.choice([0, 1], size=(2, 5))\n        result = f([binarized_values])[0]\n        np.testing.assert_allclose(result, binarized_values)\n\n        real_values = testing_utils.generate_real_values_with_zeros()\n        result = f([real_values])[0]\n        assert np.all(result[real_values <= 0] == 0)\n        assert np.all(result[real_values > 0] == 1)\n\n    def test_magnitude_aware_sign_binarization(self, eager_mode):\n        a = np.random.uniform(-2, 2, (3, 2, 2, 3))\n        x = tf.Variable(a)\n        y = lq.quantizers.MagnitudeAwareSign()(x)\n\n        assert y.shape == x.shape\n\n        # check sign\n        np.testing.assert_allclose(tf.sign(y).numpy(), np.sign(a))\n\n        # check magnitude\n        np.testing.assert_allclose(\n            tf.reduce_mean(tf.abs(y), axis=[0, 1, 2]).numpy(),\n            [np.mean(np.reshape(np.abs(a[:, :, :, i]), [-1])) for i in range(3)],\n        )\n\n    @pytest.mark.parametrize(\n        ""fn"",\n        [\n            ""ste_tern"",\n            lq.quantizers.SteTern(),\n            lq.quantizers.SteTern(ternary_weight_networks=True),\n            lq.quantizers.SteTern(threshold_value=np.random.uniform(0.01, 0.8)),\n        ],\n    )\n    def test_ternarization_basic(self, fn):\n        x = tf.keras.backend.placeholder(ndim=2)\n        f = tf.keras.backend.function([x], [lq.quantizers.get(fn)(x)])\n\n        ternarized_values = np.random.choice([-1, 0, 1], size=(4, 10))\n        result = f([ternarized_values])[0]\n        np.testing.assert_allclose(result, ternarized_values)\n        assert not np.any(result > 1)\n        assert not np.any(result < -1)\n        assert np.any(result == -1)\n        assert np.any(result == 1)\n        assert np.any(result == 0)\n\n        real_values = testing_utils.generate_real_values_with_zeros()\n        result = f([real_values])[0]\n        assert not np.any(result > 1)\n        assert not np.any(result < -1)\n        assert np.any(result == -1)\n        assert np.any(result == 1)\n        assert np.any(result == 0)\n\n    @pytest.mark.parametrize(""fn"", [""ste_tern"", lq.quantizers.SteTern()])\n    def test_ternarization_with_default_threshold(self, fn):\n        x = tf.keras.backend.placeholder(ndim=2)\n        test_threshold = 0.05  # This is the default\n        f = tf.keras.backend.function([x], [lq.quantizers.get(fn)(x)])\n\n        real_values = testing_utils.generate_real_values_with_zeros()\n        result = f([real_values])[0]\n        assert np.all(result[real_values > test_threshold] == 1)\n        assert np.all(result[real_values < -test_threshold] == -1)\n        assert np.all(result[np.abs(real_values) < test_threshold] == 0)\n        assert not np.any(result > 1)\n        assert not np.any(result < -1)\n\n    def test_ternarization_with_custom_threshold(self):\n        x = tf.keras.backend.placeholder(ndim=2)\n        test_threshold = np.random.uniform(0.01, 0.8)\n        fn = lq.quantizers.SteTern(threshold_value=test_threshold)\n        f = tf.keras.backend.function([x], [fn(x)])\n\n        real_values = testing_utils.generate_real_values_with_zeros()\n        result = f([real_values])[0]\n        assert np.all(result[real_values > test_threshold] == 1)\n        assert np.all(result[real_values < -test_threshold] == -1)\n        assert np.all(result[np.abs(real_values) < test_threshold] == 0)\n        assert not np.any(result > 1)\n        assert not np.any(result < -1)\n\n    def test_ternarization_with_ternary_weight_networks(self):\n        x = tf.keras.backend.placeholder(ndim=2)\n        real_values = testing_utils.generate_real_values_with_zeros()\n        test_threshold = 0.7 * np.sum(np.abs(real_values)) / np.size(real_values)\n        fn = lq.quantizers.SteTern(ternary_weight_networks=True)\n        f = tf.keras.backend.function([x], [fn(x)])\n\n        result = f([real_values])[0]\n        assert np.all(result[real_values > test_threshold] == 1)\n        assert np.all(result[real_values < -test_threshold] == -1)\n        assert np.all(result[np.abs(real_values) < test_threshold] == 0)\n        assert not np.any(result > 1)\n        assert not np.any(result < -1)\n\n    def test_dorefa_quantize(self):\n        x = tf.keras.backend.placeholder(ndim=2)\n        f = tf.keras.backend.function([x], [lq.quantizers.DoReFa(2)(x)])\n        real_values = testing_utils.generate_real_values_with_zeros()\n        result = f([real_values])[0]\n        k_bit = 2\n        n = 2 ** k_bit - 1\n        assert not np.any(result > 1)\n        assert not np.any(result < 0)\n        for i in range(n + 1):\n            assert np.all(\n                result[\n                    (real_values > (2 * i - 1) / (2 * n))\n                    & (real_values < (2 * i + 1) / (2 * n))\n                ]\n                == i / n\n            )\n\n\nclass TestGradients:\n    """"""Test gradients for different quantizers.""""""\n\n    @pytest.mark.parametrize(\n        ""fn"",\n        [\n            lq.quantizers.SteSign(clip_value=None),\n            lq.quantizers.SteTern(clip_value=None),\n            lq.quantizers.SteHeaviside(clip_value=None),\n        ],\n    )\n    def test_identity_ste_grad(self, eager_mode, fn):\n        x = testing_utils.generate_real_values_with_zeros(shape=(8, 3, 3, 16))\n        tf_x = tf.Variable(x)\n        with tf.GradientTape() as tape:\n            activation = fn(tf_x)\n        grad = tape.gradient(activation, tf_x)\n        np.testing.assert_allclose(grad.numpy(), np.ones_like(x))\n\n    @pytest.mark.parametrize(\n        ""fn"",\n        [\n            lq.quantizers.SteSign(),\n            lq.quantizers.SteTern(),\n            lq.quantizers.SteHeaviside(),\n        ],\n    )\n    def test_ste_grad(self, eager_mode, fn):\n        @np.vectorize\n        def ste_grad(x):\n            if np.abs(x) <= 1:\n                return 1.0\n            return 0.0\n\n        x = testing_utils.generate_real_values_with_zeros(shape=(8, 3, 3, 16))\n        tf_x = tf.Variable(x)\n        with tf.GradientTape() as tape:\n            activation = fn(tf_x)\n        grad = tape.gradient(activation, tf_x)\n        np.testing.assert_allclose(grad.numpy(), ste_grad(x))\n\n    # Test with and without default threshold\n    def test_swish_grad(self, eager_mode):\n        def swish_grad(x, beta):\n            return (\n                beta * (2 - beta * x * np.tanh(beta * x / 2)) / (1 + np.cosh(beta * x))\n            )\n\n        x = testing_utils.generate_real_values_with_zeros(shape=(8, 3, 3, 16))\n        tf_x = tf.Variable(x)\n        with tf.GradientTape() as tape:\n            activation = lq.quantizers.SwishSign()(tf_x)\n        grad = tape.gradient(activation, tf_x)\n        np.testing.assert_allclose(grad.numpy(), swish_grad(x, beta=5.0))\n\n        with tf.GradientTape() as tape:\n            activation = lq.quantizers.SwishSign(beta=10.0)(tf_x)\n        grad = tape.gradient(activation, tf_x)\n        np.testing.assert_allclose(grad.numpy(), swish_grad(x, beta=10.0))\n\n    def test_approx_sign_grad(self, eager_mode):\n        @np.vectorize\n        def approx_sign_grad(x):\n            if np.abs(x) <= 1:\n                return 2 - 2 * np.abs(x)\n            return 0.0\n\n        x = testing_utils.generate_real_values_with_zeros(shape=(8, 3, 3, 16))\n        tf_x = tf.Variable(x)\n        with tf.GradientTape() as tape:\n            activation = lq.quantizers.ApproxSign()(tf_x)\n        grad = tape.gradient(activation, tf_x)\n        np.testing.assert_allclose(grad.numpy(), approx_sign_grad(x))\n\n    def test_magnitude_aware_sign_grad(self, eager_mode):\n        a = np.random.uniform(-2, 2, (3, 2, 2, 3))\n        x = tf.Variable(a)\n        with tf.GradientTape() as tape:\n            y = lq.quantizers.MagnitudeAwareSign()(x)\n        grad = tape.gradient(y, x)\n\n        scale_vector = [\n            np.mean(np.reshape(np.abs(a[:, :, :, i]), [-1])) for i in range(3)\n        ]\n\n        np.testing.assert_allclose(\n            grad.numpy(), np.where(abs(a) < 1, np.ones(a.shape) * scale_vector, 0)\n        )\n\n    def test_dorefa_ste_grad(self, eager_mode):\n        @np.vectorize\n        def ste_grad(x):\n            if x <= 1 and x >= 0:\n                return 1.0\n            return 0.0\n\n        x = testing_utils.generate_real_values_with_zeros(shape=(8, 3, 3, 16))\n        tf_x = tf.Variable(x)\n        with tf.GradientTape() as tape:\n            activation = lq.quantizers.DoReFa(2)(tf_x)\n        grad = tape.gradient(activation, tf_x)\n        np.testing.assert_allclose(grad.numpy(), ste_grad(x))\n\n\n@pytest.mark.parametrize(\n    ""quantizer"",\n    [\n        (""ste_sign"", lq.quantizers.SteSign),\n        (""approx_sign"", lq.quantizers.ApproxSign),\n        (""ste_heaviside"", lq.quantizers.SteHeaviside),\n        (""swish_sign"", lq.quantizers.SwishSign),\n        (""magnitude_aware_sign"", lq.quantizers.MagnitudeAwareSign),\n        (""ste_tern"", lq.quantizers.SteTern),\n        (""dorefa_quantizer"", lq.quantizers.DoReFa),\n    ],\n)\ndef test_metrics(quantizer):\n    quantizer_str, quantizer_cls = quantizer\n\n    # No metric\n    model = tf.keras.models.Sequential(\n        [lq.layers.QuantDense(3, kernel_quantizer=quantizer_str, input_shape=(32,))]\n    )\n    model.compile(loss=""mse"", optimizer=""sgd"")\n    assert len(model.layers[0]._metrics) == 0\n\n    # Metric added using scope\n    with lq.context.metrics_scope([""flip_ratio""]):\n        model = tf.keras.models.Sequential(\n            [lq.layers.QuantDense(3, kernel_quantizer=quantizer_str, input_shape=(32,))]\n        )\n    model.compile(loss=""mse"", optimizer=""sgd"")\n\n    if version.parse(tf.__version__) > version.parse(""1.14""):\n        assert len(model.layers[0].kernel_quantizer._metrics) == 1\n    else:\n        # In TF1.14, call() gets called twice, resulting in having an extra initial\n        # metrics copy.\n        assert len(model.layers[0].kernel_quantizer._metrics) == 2\n\n    # Metric added explicitly to quantizer\n    model = tf.keras.models.Sequential(\n        [\n            lq.layers.QuantDense(\n                3,\n                kernel_quantizer=quantizer_cls(metrics=[""flip_ratio""]),\n                input_shape=(32,),\n            )\n        ]\n    )\n    model.compile(loss=""mse"", optimizer=""sgd"")\n    if version.parse(tf.__version__) > version.parse(""1.14""):\n        assert len(model.layers[0].kernel_quantizer._metrics) == 1\n    else:\n        # In TF1.14, call() gets called twice, resulting in having an extra initial\n        # metrics copy.\n        assert len(model.layers[0].kernel_quantizer._metrics) == 2\n\n\ndef test_get_kernel_quantizer_assigns_metrics():\n    with lq.context.metrics_scope([""flip_ratio""]):\n        ste_sign = lq.quantizers.get_kernel_quantizer(""ste_sign"")\n        assert ""flip_ratio"" in lq.context.get_training_metrics()\n\n    assert isinstance(ste_sign, lq.quantizers.SteSign)\n    assert ""flip_ratio"" in ste_sign._custom_metrics\n\n\ndef test_get_kernel_quantizer_accepts_function():\n    custom_quantizer = lq.quantizers.get_kernel_quantizer(lambda x: x)\n    assert callable(custom_quantizer)\n    assert not hasattr(custom_quantizer, ""_custom_metrics"")\n\n\ndef test_backwards_compat_aliases():\n    assert lq.quantizers.DoReFaQuantizer == lq.quantizers.DoReFa\n    assert lq.quantizers.NoOpQuantizer == lq.quantizers.NoOp\n'"
larq/testing_utils.py,14,"b'import numpy as np\nimport tensorflow as tf\n\nimport larq as lq\n\n\ndef _eval_tensor(tensor):\n    if tensor is None:\n        return None\n    elif callable(tensor):\n        return _eval_helper(tensor())\n    else:\n        return tensor.numpy()\n\n\ndef _eval_helper(tensors):\n    if tensors is None:\n        return None\n    return tf.nest.map_structure(_eval_tensor, tensors)\n\n\ndef evaluate(tensors):\n    if tf.executing_eagerly():\n        return _eval_helper(tensors)\n    else:\n        sess = tf.compat.v1.get_default_session()\n        return sess.run(tensors)\n\n\ndef generate_real_values_with_zeros(low=-2, high=2, shape=(4, 10)):\n    real_values = np.random.uniform(low, high, shape)\n    real_values = np.insert(real_values, 1, 0, axis=1)\n    return real_values\n\n\ndef get_small_bnn_model(input_dim, num_hidden, output_dim, trainable_bn=True):\n    model = tf.keras.models.Sequential()\n    model.add(\n        lq.layers.QuantDense(\n            units=num_hidden,\n            kernel_quantizer=""ste_sign"",\n            kernel_constraint=""weight_clip"",\n            activation=""relu"",\n            input_shape=(input_dim,),\n            use_bias=False,\n        )\n    )\n    model.add(tf.keras.layers.BatchNormalization(trainable=trainable_bn))\n    model.add(\n        lq.layers.QuantDense(\n            units=output_dim,\n            kernel_quantizer=""ste_sign"",\n            kernel_constraint=""weight_clip"",\n            input_quantizer=""ste_sign"",\n            activation=""softmax"",\n            use_bias=False,\n        )\n    )\n    return model\n\n\ndef random_input(shape):\n    for i, dim in enumerate(shape):\n        if dim is None:\n            shape[i] = np.random.randint(1, 4)\n    data = 10 * np.random.random(shape) - 0.5\n    return data.astype(""float32"")\n\n\n# This is a fork of https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/testing_utils.py#L72\n# as recommended in https://github.com/tensorflow/tensorflow/issues/28601#issuecomment-492810252\ndef layer_test(\n    layer_cls,\n    kwargs=None,\n    input_shape=None,\n    input_dtype=None,\n    input_data=None,\n    expected_output=None,\n    expected_output_dtype=None,\n    should_run_eagerly=False,\n):\n    """"""Test routine for a layer with a single input and single output.\n    Arguments:\n      layer_cls: Layer class object.\n      kwargs: Optional dictionary of keyword arguments for instantiating the\n        layer.\n      input_shape: Input shape tuple.\n      input_dtype: Data type of the input data.\n      input_data: Numpy array of input data.\n      expected_output: Shape tuple for the expected shape of the output.\n      expected_output_dtype: Data type expected for the output.\n    Returns:\n      The output data (Numpy array) returned by the layer, for additional\n      checks to be done by the calling code.\n    Raises:\n      ValueError: if `input_shape is None`.\n    """"""\n    if input_data is None:\n        if input_shape is None:\n            raise ValueError(""input_shape is None"")\n        if not input_dtype:\n            input_dtype = ""float32""\n        input_data_shape = list(input_shape)\n        for i, e in enumerate(input_data_shape):\n            if e is None:\n                input_data_shape[i] = np.random.randint(1, 4)\n        input_data = 10 * np.random.random(input_data_shape)\n        if input_dtype[:5] == ""float"":\n            input_data -= 0.5\n        input_data = input_data.astype(input_dtype)\n    elif input_shape is None:\n        input_shape = input_data.shape\n    if input_dtype is None:\n        input_dtype = input_data.dtype\n    if expected_output_dtype is None:\n        expected_output_dtype = input_dtype\n\n    # instantiation\n    kwargs = kwargs or {}\n    layer = layer_cls(**kwargs)\n\n    # test get_weights , set_weights at layer level\n    weights = layer.get_weights()\n    layer.set_weights(weights)\n\n    # test in functional API\n    x = tf.keras.layers.Input(shape=input_shape[1:], dtype=input_dtype)\n    y = layer(x)\n    if tf.keras.backend.dtype(y) != expected_output_dtype:\n        raise AssertionError(\n            ""When testing layer %s, for input %s, found output ""\n            ""dtype=%s but expected to find %s.\\nFull kwargs: %s""\n            % (\n                layer_cls.__name__,\n                x,\n                tf.keras.backend.dtype(y),\n                expected_output_dtype,\n                kwargs,\n            )\n        )\n    # check shape inference\n    model = tf.keras.models.Model(x, y)\n    expected_output_shape = tuple(\n        layer.compute_output_shape(tf.TensorShape(input_shape)).as_list()\n    )\n    actual_output = model.predict(input_data)\n    actual_output_shape = actual_output.shape\n    for expected_dim, actual_dim in zip(expected_output_shape, actual_output_shape):\n        if expected_dim is not None:\n            if expected_dim != actual_dim:\n                raise AssertionError(\n                    ""When testing layer %s, for input %s, found output_shape=""\n                    ""%s but expected to find %s.\\nFull kwargs: %s""\n                    % (\n                        layer_cls.__name__,\n                        x,\n                        actual_output_shape,\n                        expected_output_shape,\n                        kwargs,\n                    )\n                )\n    if expected_output is not None:\n        np.testing.assert_allclose(actual_output, expected_output, rtol=1e-3)\n\n    # test serialization, weight setting at model level\n    model_config = model.get_config()\n    recovered_model = tf.keras.models.Model.from_config(model_config)\n    if model.weights:\n        weights = model.get_weights()\n        recovered_model.set_weights(weights)\n        output = recovered_model.predict(input_data)\n        np.testing.assert_allclose(output, actual_output, rtol=2e-3)\n\n    # Recreate layer to prevent layer metrics from being configured multiple times.\n    layer = layer_cls(**kwargs)\n    # test training mode (e.g. useful for dropout tests)\n    # Rebuild the model to avoid the graph being reused between predict() and\n    # train(). This was causing some error for layer with Defun as it body.\n    # See b/120160788 for more details. This should be mitigated after 2.0.\n    model = tf.keras.models.Model(x, layer(x))\n    model.compile(\n        ""rmsprop"", ""mse"", weighted_metrics=[""acc""], run_eagerly=should_run_eagerly,\n    )\n    model.train_on_batch(input_data, actual_output)\n\n    # test as first layer in Sequential API\n    layer_config = layer.get_config()\n    layer_config[""batch_input_shape""] = input_shape\n    layer = layer.__class__.from_config(layer_config)\n\n    model = tf.keras.models.Sequential()\n    model.add(layer)\n    actual_output = model.predict(input_data)\n    actual_output_shape = actual_output.shape\n    for expected_dim, actual_dim in zip(expected_output_shape, actual_output_shape):\n        if expected_dim is not None:\n            if expected_dim != actual_dim:\n                raise AssertionError(\n                    ""When testing layer %s **after deserialization**, ""\n                    ""for input %s, found output_shape=""\n                    ""%s but expected to find inferred shape %s.\\nFull kwargs: %s""\n                    % (\n                        layer_cls.__name__,\n                        x,\n                        actual_output_shape,\n                        expected_output_shape,\n                        kwargs,\n                    )\n                )\n    if expected_output is not None:\n        np.testing.assert_allclose(actual_output, expected_output, rtol=1e-3)\n\n    # test serialization, weight setting at model level\n    model_config = model.get_config()\n    recovered_model = tf.keras.models.Sequential.from_config(model_config)\n    if model.weights:\n        weights = model.get_weights()\n        recovered_model.set_weights(weights)\n        output = recovered_model.predict(input_data)\n        np.testing.assert_allclose(output, actual_output, rtol=2e-3)\n\n    # for further checks in the caller function\n    return actual_output\n'"
larq/utils.py,1,"b'from contextlib import contextmanager\n\nfrom tensorflow.keras.utils import get_custom_objects\n\n\ndef memory_as_readable_str(num_bits: int) -> str:\n    """"""Generate a human-readable string for the memory size.\n\n    1 KiB = 1024 B; we use the binary prefix (KiB) [1,2] instead of the decimal prefix\n    (KB) to avoid any confusion with multiplying by 1000 instead of 1024.\n\n    [1] https://en.wikipedia.org/wiki/Binary_prefix\n    [2] https://physics.nist.gov/cuu/Units/binary.html\n    """"""\n\n    suffixes = [""B"", ""KiB"", ""MiB"", ""GiB""]\n    num_bytes = num_bits / 8\n\n    for i, suffix in enumerate(suffixes):\n        rounded = num_bytes / (1024 ** i)\n        if rounded < 1024:\n            break\n\n    return f""{rounded:,.2f} {suffix}""\n\n\ndef register_keras_custom_object(cls):\n    """"""See https://github.com/tensorflow/addons/blob/master/tensorflow_addons/utils/keras_utils.py#L25""""""\n    get_custom_objects()[cls.__name__] = cls\n    return cls\n\n\ndef register_alias(name: str):\n    """"""A decorator to register a custom keras object under a given alias.\n    !!! example\n        ```python\n        @utils.register_alias(""degeneration"")\n        class Degeneration(tf.keras.metrics.Metric):\n            pass\n        ```\n    """"""\n\n    def register_func(cls):\n        get_custom_objects()[name] = cls\n        return cls\n\n    return register_func\n\n\ndef set_precision(precision: int = 32):\n    """"""A decorator to set the precision of a quantizer function\n\n    # Arguments\n        precision: An integer defining the precision of the output.\n    """"""\n\n    def decorator(function):\n        setattr(function, ""precision"", precision)\n        return function\n\n    return decorator\n\n\n@contextmanager\ndef patch_object(object, name, value):\n    """"""Temporarily overwrite attribute on object""""""\n    old_value = getattr(object, name)\n    setattr(object, name, value)\n    yield\n    setattr(object, name, old_value)\n'"
larq/utils_test.py,0,"b'import larq as lq\n\n\ndef test_memory_as_readable_str():\n    correct_strings = [  # 2^i bits, from i = 0 to 74\n        ""0.12 B"",\n        ""0.25 B"",\n        ""0.50 B"",\n        ""1.00 B"",\n        ""2.00 B"",\n        ""4.00 B"",\n        ""8.00 B"",\n        ""16.00 B"",\n        ""32.00 B"",\n        ""64.00 B"",\n        ""128.00 B"",\n        ""256.00 B"",\n        ""512.00 B"",\n        ""1.00 KiB"",\n        ""2.00 KiB"",\n        ""4.00 KiB"",\n        ""8.00 KiB"",\n        ""16.00 KiB"",\n        ""32.00 KiB"",\n        ""64.00 KiB"",\n        ""128.00 KiB"",\n        ""256.00 KiB"",\n        ""512.00 KiB"",\n        ""1.00 MiB"",\n        ""2.00 MiB"",\n        ""4.00 MiB"",\n        ""8.00 MiB"",\n        ""16.00 MiB"",\n        ""32.00 MiB"",\n        ""64.00 MiB"",\n        ""128.00 MiB"",\n        ""256.00 MiB"",\n        ""512.00 MiB"",\n        ""1.00 GiB"",\n        ""2.00 GiB"",\n        ""4.00 GiB"",\n        ""8.00 GiB"",\n        ""16.00 GiB"",\n        ""32.00 GiB"",\n        ""64.00 GiB"",\n        ""128.00 GiB"",\n        ""256.00 GiB"",\n        ""512.00 GiB"",\n        ""1,024.00 GiB"",\n    ]\n\n    for i, correct_string in enumerate(correct_strings):\n        assert lq.utils.memory_as_readable_str(2 ** i) == correct_string\n'"
larq/snapshots/__init__.py,0,b''
larq/snapshots/snap_models_test.py,0,"b""# -*- coding: utf-8 -*-\n# snapshottest: v1 - https://goo.gl/zC4yUc\nfrom __future__ import unicode_literals\n\nfrom snapshottest import Snapshot\n\n\nsnapshots = Snapshot()\n\nsnapshots['test_summary 1'] = '''+sequential stats----------------------------------------------------------------------------------------------------------------+\n| Layer                   Input prec.           Outputs  # 1-bit  # 2-bit  # 32-bit  Memory  1-bit MACs  2-bit MACs  32-bit MACs |\n|                               (bit)                        x 1      x 1       x 1    (kB)                                      |\n+--------------------------------------------------------------------------------------------------------------------------------+\n| quant_conv2d                      -  (-1, 64, 64, 32)      288        0        32    0.16           0           0      1179648 |\n| max_pooling2d                     -  (-1, 32, 32, 32)        0        0         0       0           0           0            0 |\n| quant_depthwise_conv2d            2  (-1, 11, 11, 32)        0      288         0    0.07           0       34848            0 |\n| batch_normalization               -  (-1, 11, 11, 32)        0        0        64    0.25           0           0            0 |\n| quant_separable_conv2d            1  (-1, 11, 11, 32)     1312        0        32    0.29      158752           0            0 |\n| flatten                           -        (-1, 3872)        0        0         0       0           0           0            0 |\n| dense                             -          (-1, 10)        0        0     38730  151.29           0           0        38720 |\n+--------------------------------------------------------------------------------------------------------------------------------+\n| Total                                                     1600      288     38858  152.05      158752       34848      1218368 |\n+--------------------------------------------------------------------------------------------------------------------------------+\n+sequential summary-----------------------------+\n| Total params                       40.7 k     |\n| Trainable params                   1.95 k     |\n| Non-trainable params               38.8 k     |\n| Model size                         152.05 KiB |\n| Model size (8-bit FP weights)      38.21 KiB  |\n| Float-32 Equivalent                159.16 KiB |\n| Compression Ratio of Memory        0.96       |\n| Number of MACs                     1.41 M     |\n| Ratio of MACs that are binarized   0.1124     |\n| Ratio of MACs that are ternarized  0.0247     |\n+-----------------------------------------------+\n'''\n\nsnapshots['test_summary 2'] = '''+sequential_1 stats--------------------+\n| Layer   Input prec.  Outputs  Memory |\n|               (bit)             (kB) |\n+--------------------------------------+\n| lambda            -     (2,)       0 |\n+--------------------------------------+\n| Total                              0 |\n+--------------------------------------+\n+sequential_1 summary-------------------+\n| Total params                   0      |\n| Trainable params               0      |\n| Non-trainable params           0      |\n| Model size                     0.00 B |\n| Model size (8-bit FP weights)  0.00 B |\n| Float-32 Equivalent            0.00 B |\n| Compression Ratio of Memory    0.00   |\n| Number of MACs                 0      |\n+---------------------------------------+\n'''\n\nsnapshots['test_subclass_model_summary 1'] = '''+toy_model stats-------------------------------------------------------------+\n| Layer                     Input prec.   Outputs  # 1-bit  # 32-bit  Memory |\n|                                 (bit)                x 1       x 1    (kB) |\n+----------------------------------------------------------------------------+\n| quant_conv2d                        -  multiple      864        32    0.23 |\n| global_average_pooling2d            -  multiple        0         0       0 |\n| dense                               -  multiple        0       330    1.29 |\n+----------------------------------------------------------------------------+\n| Total                                                864       362    1.52 |\n+----------------------------------------------------------------------------+\n+toy_model summary------------------------+\n| Total params                   1.23 k   |\n| Trainable params               1.23 k   |\n| Non-trainable params           0        |\n| Model size                     1.52 KiB |\n| Model size (8-bit FP weights)  470.00 B |\n| Float-32 Equivalent            4.79 KiB |\n| Compression Ratio of Memory    0.32     |\n| Number of MACs                 0        |\n+-----------------------------------------+\n'''\n"""
larq/snapshots/snap_quantized_variable_test.py,0,"b'# -*- coding: utf-8 -*-\n# snapshottest: v1 - https://goo.gl/zC4yUc\nfrom __future__ import unicode_literals\n\nfrom snapshottest import Snapshot\n\nsnapshots = Snapshot()\n\nsnapshots[\'test_repr[eager] 1\'] = ""<QuantizedVariable \'x:0\' shape=() dtype=float32 quantizer=<lambda> numpy=0.0>""\n\nsnapshots[\'test_repr[eager] 2\'] = ""<QuantizedVariable \'x:0\' shape=() dtype=float32 quantizer=Quantizer numpy=0.0>""\n\nsnapshots[\'test_repr[eager] 3\'] = ""<QuantizedVariable \'x:0\' shape=() dtype=float32 precision=1 numpy=0.0>""\n\nsnapshots[\'test_repr[graph] 1\'] = ""<QuantizedVariable \'x:0\' shape=() dtype=float32 quantizer=<lambda>>""\n\nsnapshots[\'test_repr[graph] 2\'] = ""<QuantizedVariable \'x:0\' shape=() dtype=float32 quantizer=Quantizer>""\n\nsnapshots[\'test_repr[graph] 3\'] = ""<QuantizedVariable \'x:0\' shape=() dtype=float32 precision=1>""\n'"
