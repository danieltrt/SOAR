file_path,api_count,code
my-object-detection.py,0,"b'from __future__ import print_function\nfrom function.realtime import *\nfrom function.video import *\nimport argparse\nimport os\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nif __name__ == \'__main__\':\n\n    # construct the argument parse and parse the arguments\n    ap = argparse.ArgumentParser()\n    ap.add_argument(""-n"", ""--num-frames"", type=int, default=0,\n\t            help=""# of frames to loop over for FPS test"")\n    ap.add_argument(""-d"", ""--display"", type=int, default=0,\n\t            help=""Whether or not frames should be displayed"")\n    ap.add_argument(""-o"", ""--output"", type=int, default=0,\n\t            help=""Whether or not modified videos shall be writen"")\n    ap.add_argument(""-on"", ""--output-name"", type=str, default=""output"",\n\t            help=""Name of the output video file"")\n    ap.add_argument(""-I"", ""--input-device"", type=int, default=0,\n\t            help=""Device number input"")\n    ap.add_argument(""-i"", ""--input-videos"", type=str, default="""",\n\t            help=""Path to videos input, overwrite device input if used"")\n    ap.add_argument(\'-w\', \'--num-workers\', dest=\'num_workers\', type=int,\n                        default=2, help=\'Number of workers.\')\n    ap.add_argument(\'-q-size\', \'--queue-size\', dest=\'queue_size\', type=int,\n                        default=5, help=\'Size of the queue.\')\n    ap.add_argument(\'-l\', \'--logger-debug\', dest=\'logger_debug\', type=int,\n                        default=0, help=\'Print logger debug\')\n    ap.add_argument(\'-f\', \'--fullscreen\', dest=\'full_screen\', type=int,\n                        default=0, help=\'enable full screen\')\n    args = vars(ap.parse_args())\n\n    # Use realtime function if no video has been provided\n    if args[\'input_videos\'] == """":\n        realtime(args)\n\n    # Use video function else\n    else:\n        video(args)\n'"
function/realtime.py,0,"b'from __future__ import print_function\nfrom utils.app_utils import *\nfrom utils.objDet_utils import *\nimport argparse\nimport multiprocessing\nfrom multiprocessing import Queue, Pool\nimport cv2\n\ndef realtime(args):\n    """"""\n    Read and apply object detection to input real time stream (webcam)\n    """"""\n\n    # If display is off while no number of frames limit has been define: set diplay to on\n    if((not args[""display""]) & (args[""num_frames""] < 0)):\n        print(""\\nSet display to on\\n"")\n        args[""display""] = 1\n\n    # Set the multiprocessing logger to debug if required\n    if args[""logger_debug""]:\n        logger = multiprocessing.log_to_stderr()\n        logger.setLevel(multiprocessing.SUBDEBUG)\n\n    # Multiprocessing: Init input and output Queue and pool of workers\n    input_q = Queue(maxsize=args[""queue_size""])\n    output_q = Queue(maxsize=args[""queue_size""])\n    pool = Pool(args[""num_workers""], worker, (input_q,output_q))\n\n    # created a threaded video stream and start the FPS counter\n    vs = WebcamVideoStream(src=args[""input_device""]).start()\n    fps = FPS().start()\n\n    # Define the output codec and create VideoWriter object\n    if args[""output""]:\n        fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n        out = cv2.VideoWriter(\'outputs/{}.avi\'.format(args[""output_name""]),\n                              fourcc, vs.getFPS()/args[""num_workers""], (vs.getWidth(), vs.getHeight()))\n\n\n    # Start reading and treating the video stream\n    if args[""display""] > 0:\n        print()\n        print(""====================================================================="")\n        print(""Starting video acquisition. Press \'q\' (on the video windows) to stop."")\n        print(""====================================================================="")\n        print()\n\n    countFrame = 0\n    while True:\n        # Capture frame-by-frame\n        ret, frame = vs.read()\n        countFrame = countFrame + 1\n        if ret:\n            input_q.put(frame)\n            output_rgb = cv2.cvtColor(output_q.get(), cv2.COLOR_RGB2BGR)\n\n            # write the frame\n            if args[""output""]:\n                out.write(output_rgb)\n\n            # Display the resulting frame\n            if args[""display""]:\n                ## full screen\n                if args[""full_screen""]:\n                    cv2.namedWindow(""frame"", cv2.WND_PROP_FULLSCREEN)\n                    cv2.setWindowProperty(""frame"",cv2.WND_PROP_FULLSCREEN,cv2.WINDOW_FULLSCREEN)\n                cv2.imshow(""frame"", output_rgb)\n\n                fps.update()\n            elif countFrame >= args[""num_frames""]:\n                break\n\n        else:\n            break\n\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n\n    # When everything done, release the capture\n    fps.stop()\n    pool.terminate()\n    vs.stop()\n    if args[""output""]:\n        out.release()\n    cv2.destroyAllWindows()\n\n'"
function/video.py,0,"b'from __future__ import print_function\nfrom utils.app_utils import *\nfrom utils.objDet_utils import *\nimport argparse\nimport multiprocessing\nfrom multiprocessing import Queue, Pool\nfrom queue import PriorityQueue\nimport cv2\n\n\ndef video(args):\n    """"""\n    Read and apply object detection to input video stream\n    """"""\n\n    # Set the multiprocessing logger to debug if required\n    if args[""logger_debug""]:\n        logger = multiprocessing.log_to_stderr()\n        logger.setLevel(multiprocessing.SUBDEBUG)\n\n    # Multiprocessing: Init input and output Queue, output Priority Queue and pool of workers\n    input_q = Queue(maxsize=args[""queue_size""])\n    output_q = Queue(maxsize=args[""queue_size""])\n    output_pq = PriorityQueue(maxsize=3*args[""queue_size""])\n    pool = Pool(args[""num_workers""], worker, (input_q,output_q))\n    \n    # created a threaded video stream and start the FPS counter\n    vs = cv2.VideoCapture(""inputs/{}"".format(args[""input_videos""]))\n    fps = FPS().start()\n\n    # Define the codec and create VideoWriter object\n    if args[""output""]:\n        fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n        out = cv2.VideoWriter(\'outputs/{}.avi\'.format(args[""output_name""]),\n                              fourcc, vs.get(cv2.CAP_PROP_FPS),\n                              (int(vs.get(cv2.CAP_PROP_FRAME_WIDTH)),\n                               int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n\n    # Start reading and treating the video stream\n    if args[""display""] > 0:\n        print()\n        print(""====================================================================="")\n        print(""Starting video acquisition. Press \'q\' (on the video windows) to stop."")\n        print(""====================================================================="")\n        print()\n\n    countReadFrame = 0\n    countWriteFrame = 1\n    nFrame = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n    firstReadFrame = True\n    firstTreatedFrame = True\n    firstUsedFrame = True\n    while True:\n        # Check input queue is not full\n        if not input_q.full():\n            # Read frame and store in input queue\n            ret, frame = vs.read()\n            if ret:            \n                input_q.put((int(vs.get(cv2.CAP_PROP_POS_FRAMES)),frame))\n                countReadFrame = countReadFrame + 1\n                if firstReadFrame:\n                    print("" --> Reading first frames from input file. Feeding input queue.\\n"")\n                    firstReadFrame = False\n\n        # Check output queue is not empty\n        if not output_q.empty():\n            # Recover treated frame in output queue and feed priority queue\n            output_pq.put(output_q.get())\n            if firstTreatedFrame:\n                print("" --> Recovering the first treated frame.\\n"")\n                firstTreatedFrame = False\n                \n        # Check output priority queue is not empty\n        if not output_pq.empty():\n            prior, output_frame = output_pq.get()\n            if prior > countWriteFrame:\n                output_pq.put((prior, output_frame))\n            else:\n                countWriteFrame = countWriteFrame + 1\n                output_rgb = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)\n\n                # Write the frame in file\n                if args[""output""]:\n                    out.write(output_rgb)\n\n                # Display the resulting frame\n                if args[""display""]:\n                    cv2.imshow(\'frame\', output_rgb)\n                    fps.update()\n\n                if firstUsedFrame:\n                    print("" --> Start using recovered frame (displaying and/or writing).\\n"")\n                    firstUsedFrame = False\n\n                \n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n\n        print(""Read frames: %-3i %% -- Write frame: %-3i %%"" % (int(countReadFrame/nFrame * 100), int(countWriteFrame/nFrame * 100)), end =\'\\r\')\n        if((not ret) & input_q.empty() & output_q.empty() & output_pq.empty()):\n            break\n\n\n    print(""\\nFile have been successfully read and treated:\\n  --> {}/{} read frames \\n  --> {}/{} write frames \\n"".format(countReadFrame,nFrame,countWriteFrame-1,nFrame))\n    \n    # When everything done, release the capture\n    fps.stop()\n    pool.terminate()\n    vs.release()\n    if args[""output""]:\n        out.release()\n    cv2.destroyAllWindows()\n    \n'"
utils/app_utils.py,0,"b'# import the necessary packages\nfrom threading import Thread\nimport datetime\nimport cv2\n\nclass FPS:\n    def __init__(self):\n        # store the start time, end time, and total number of frames\n        # that were examined between the start and end intervals\n        self._start = None\n        self._end = None\n        self._numFrames = 0\n\n    def start(self):\n        # start the timer\n        self._start = datetime.datetime.now()\n        return self\n    \n    def stop(self):\n        # stop the timer\n        self._end = datetime.datetime.now()\n\n    def update(self):\n        # increment the total number of frames examined during the\n        # start and end intervals\n        self._numFrames += 1\n\n    def elapsed(self):\n        # return the total number of seconds between the start and\n        # end interval\n        return (self._end - self._start).total_seconds()\n\n    def fps(self):\n        # compute the (approximate) frames per second\n        return self._numFrames / self.elapsed()\n\n    \nclass WebcamVideoStream:\n    def __init__(self, src=0):\n        # initialize the video camera stream and read the first frame\n        # from the stream\n        self.stream = cv2.VideoCapture(src)\n        (self.grabbed, self.frame) = self.stream.read()\n        \n        # initialize the variable used to indicate if the thread should\n        # be stopped\n        self.stopped = False\n\n    def start(self):\n        # start the thread to read frames from the video stream\n        Thread(target=self.update, args=()).start()\n        return self\n \n    def update(self):\n        # keep looping infinitely until the thread is stopped\n        while True:\n            # if the thread indicator variable is set, stop the thread\n            if self.stopped:\n                return\n                    \n            # otherwise, read the next frame from the stream\n            (self.grabbed, self.frame) = self.stream.read()\n \n    def read(self):\n        # return the frame most recently read\n        return self.grabbed, self.frame\n \n    def stop(self):\n        # indicate that the thread should be stopped\n        self.stopped = True\n\n    def getWidth(self):\n        # Get the width of the frames\n        return int(self.stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n\n    def getHeight(self):\n        # Get the height of the frames\n        return int(self.stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    def getFPS(self):\n        # Get the frame rate of the frames\n        return int(self.stream.get(cv2.CAP_PROP_FPS))\n\n    def isOpen(self):\n        # Get the frame rate of the frames\n        return self.stream.isOpened()\n\n    def setFramePosition(self, framePos):\n        self.stream.set(cv2.CAP_PROP_POS_FRAMES, framePos)\n\n    def getFramePosition(self):\n        return int(self.stream.get(cv2.CAP_PROP_POS_FRAMES))\n\n    def getFrameCount(self):\n        return int(self.stream.get(cv2.CAP_PROP_FRAME_COUNT))\n'"
utils/objDet_utils.py,5,"b""import os\nfrom utils.app_utils import *\nimport numpy as np\nimport tensorflow as tf\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as vis_util\n\n\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\nPATH_TO_CKPT = 'model/frozen_inference_graph.pb'\n\n# List of the strings that is used to add correct label for each box.\nPATH_TO_LABELS = 'model/mscoco_label_map.pbtxt'\n\nNUM_CLASSES = 90\n\n# Loading label map\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES,\n                                                            use_display_name=True)\ncategory_index = label_map_util.create_category_index(categories)\n\ndef detect_objects(image_np, sess, detection_graph):\n    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n    image_np_expanded = np.expand_dims(image_np, axis=0)\n    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n\n    # Each box represents a part of the image where a particular object was detected.\n    boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n\n    # Each score represent how level of confidence for each of the objects.\n    # Score is shown on the result image, together with the class label.\n    scores = detection_graph.get_tensor_by_name('detection_scores:0')\n    classes = detection_graph.get_tensor_by_name('detection_classes:0')\n    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n\n    # Actual detection.\n    (boxes, scores, classes, num_detections) = sess.run(\n        [boxes, scores, classes, num_detections],\n        feed_dict={image_tensor: image_np_expanded})\n\n    # Visualization of the results of a detection.\n    vis_util.visualize_boxes_and_labels_on_image_array(\n        image_np,\n        np.squeeze(boxes),\n        np.squeeze(classes).astype(np.int32),\n        np.squeeze(scores),\n        category_index,\n        use_normalized_coordinates=True,\n        line_thickness=4)\n\n    return image_np\n\n\n\ndef worker(input_q, output_q):\n    # Load a (frozen) Tensorflow model into memory.\n    detection_graph = tf.Graph()\n    with detection_graph.as_default():\n        od_graph_def = tf.compat.v1.GraphDef()\n        with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n            serialized_graph = fid.read()\n            od_graph_def.ParseFromString(serialized_graph)\n            tf.import_graph_def(od_graph_def, name='')\n        sess = tf.compat.v1.Session(graph=detection_graph)\n\n    fps = FPS().start()\n    while True:\n        fps.update()\n        frame = input_q.get()\n\n        # Check frame object is a 2-D array (video) or 1-D (webcam)\n        if len(frame) == 2:\n            frame_rgb = cv2.cvtColor(frame[1], cv2.COLOR_BGR2RGB)\n            output_q.put((frame[0], detect_objects(frame_rgb, sess, detection_graph)))\n        else:\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            output_q.put(detect_objects(frame_rgb, sess, detection_graph))\n    fps.stop()\n    sess.close()\n"""
