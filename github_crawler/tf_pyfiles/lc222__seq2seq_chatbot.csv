file_path,api_count,code
__init__.py,0,b''
chatbot.py,23,"b'""""""Most of the code comes from seq2seq tutorial. Binary for training conversation models and decoding from them.\n\nRunning this program without --decode will  tokenize it in a very basic way,\nand then start training a model saving checkpoints to --train_dir.\n\nRunning with --decode starts an interactive loop so you can see how\nthe current checkpoint performs\n\nSee the following papers for more information on neural translation models.\n * http://arxiv.org/abs/1409.3215\n * http://arxiv.org/abs/1409.0473\n * http://arxiv.org/abs/1412.2007\n""""""\n\nimport math\nimport sys\nimport time\nfrom data_utils import *\nfrom seq2seq_model import *\nfrom tqdm import tqdm\n\ntf.app.flags.DEFINE_float(""learning_rate"", 0.001, ""Learning rate."")\ntf.app.flags.DEFINE_integer(""batch_size"", 256, ""Batch size to use during training."")\ntf.app.flags.DEFINE_integer(""numEpochs"", 30, ""Batch size to use during training."")\ntf.app.flags.DEFINE_integer(""size"", 512, ""Size of each model layer."")\ntf.app.flags.DEFINE_integer(""num_layers"", 3, ""Number of layers in the model."")\ntf.app.flags.DEFINE_integer(""en_vocab_size"", 40000, ""English vocabulary size."")\ntf.app.flags.DEFINE_integer(""en_de_seq_len"", 20, ""English vocabulary size."")\ntf.app.flags.DEFINE_integer(""max_train_data_size"", 0, ""Limit on the size of training data (0: no limit)."")\ntf.app.flags.DEFINE_integer(""steps_per_checkpoint"", 100, ""How many training steps to do per checkpoint."")\ntf.app.flags.DEFINE_string(""train_dir"", \'./tmp\', ""How many training steps to do per checkpoint."")\ntf.app.flags.DEFINE_integer(""beam_size"", 5, ""How many training steps to do per checkpoint."")\ntf.app.flags.DEFINE_boolean(""beam_search"", True, ""Set to True for beam_search."")\ntf.app.flags.DEFINE_boolean(""decode"", True, ""Set to True for interactive decoding."")\nFLAGS = tf.app.flags.FLAGS\n\ndef create_model(session, forward_only, beam_search, beam_size = 5):\n    """"""Create translation model and initialize or load parameters in session.""""""\n    model = Seq2SeqModel(\n        FLAGS.en_vocab_size, FLAGS.en_vocab_size, [10, 10],\n        FLAGS.size, FLAGS.num_layers, FLAGS.batch_size,\n        FLAGS.learning_rate, forward_only=forward_only, beam_search=beam_search, beam_size=beam_size)\n    ckpt = tf.train.latest_checkpoint(FLAGS.train_dir)\n    model_path = \'E:\\PycharmProjects\\Seq-to-Seq\\seq2seq_chatbot\\\\tmp\\chat_bot.ckpt-0\'\n    if forward_only:\n        model.saver.restore(session, model_path)\n    elif ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n        print(""Reading model parameters from %s"" % ckpt.model_checkpoint_path)\n        model.saver.restore(session, ckpt.model_checkpoint_path)\n    else:\n        print(""Created model with fresh parameters."")\n        session.run(tf.initialize_all_variables())\n    return model\n\ndef train():\n    # prepare dataset\n    data_path = \'E:\\PycharmProjects\\Seq-to-Seq\\seq2seq_chatbot\\data\\dataset-cornell-length10-filter1-vocabSize40000.pkl\'\n    word2id, id2word, trainingSamples = loadDataset(data_path)\n    with tf.Session() as sess:\n        print(""Creating %d layers of %d units."" % (FLAGS.num_layers, FLAGS.size))\n        model = create_model(sess, False, beam_search=False, beam_size=5)\n        current_step = 0\n        for e in range(FLAGS.numEpochs):\n            print(""----- Epoch {}/{} -----"".format(e + 1, FLAGS.numEpochs))\n            batches = getBatches(trainingSamples, FLAGS.batch_size, model.en_de_seq_len)\n            for nextBatch in tqdm(batches, desc=""Training""):\n                _, step_loss = model.step(sess, nextBatch.encoderSeqs, nextBatch.decoderSeqs, nextBatch.targetSeqs,\n                                          nextBatch.weights, goToken)\n                current_step += 1\n                if current_step % FLAGS.steps_per_checkpoint == 0:\n                    perplexity = math.exp(float(step_loss)) if step_loss < 300 else float(\'inf\')\n                    tqdm.write(""----- Step %d -- Loss %.2f -- Perplexity %.2f"" % (current_step, step_loss, perplexity))\n                    checkpoint_path = os.path.join(FLAGS.train_dir, ""chat_bot.ckpt"")\n                    model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n\ndef decode():\n    with tf.Session() as sess:\n        beam_size = FLAGS.beam_size\n        beam_search = FLAGS.beam_search\n        model = create_model(sess, True, beam_search=beam_search, beam_size=beam_size)\n        model.batch_size = 1\n        data_path = \'E:\\PycharmProjects\\Seq-to-Seq\\seq2seq_chatbot\\data\\dataset-cornell-length10-filter1-vocabSize40000.pkl\'\n        word2id, id2word, trainingSamples = loadDataset(data_path)\n\n        if beam_search:\n            sys.stdout.write(""> "")\n            sys.stdout.flush()\n            sentence = sys.stdin.readline()\n            while sentence:\n                batch = sentence2enco(sentence, word2id, model.en_de_seq_len)\n                beam_path, beam_symbol = model.step(sess, batch.encoderSeqs, batch.decoderSeqs, batch.targetSeqs,\n                                                    batch.weights, goToken)\n                paths = [[] for _ in range(beam_size)]\n                curr = [i for i in range(beam_size)]\n                num_steps = len(beam_path)\n                for i in range(num_steps-1, -1, -1):\n                    for kk in range(beam_size):\n                        paths[kk].append(beam_symbol[i][curr[kk]])\n                        curr[kk] = beam_path[i][curr[kk]]\n                recos = set()\n                print(""Replies --------------------------------------->"")\n                for kk in range(beam_size):\n                    foutputs = [int(logit) for logit in paths[kk][::-1]]\n                    if eosToken in foutputs:\n                        foutputs = foutputs[:foutputs.index(eosToken)]\n                    rec = "" "".join([tf.compat.as_str(id2word[output]) for output in foutputs if output in id2word])\n                    if rec not in recos:\n                        recos.add(rec)\n                        print(rec)\n                print(""> "", """")\n                sys.stdout.flush()\n                sentence = sys.stdin.readline()\n        # else:\n        #     sys.stdout.write(""> "")\n        #     sys.stdout.flush()\n        #     sentence = sys.stdin.readline()\n        #\n        #     while sentence:\n        #           # Get token-ids for the input sentence.\n        #           token_ids = sentence_to_token_ids(tf.compat.as_bytes(sentence), vocab)\n        #           # Which bucket does it belong to?\n        #           bucket_id = min([b for b in xrange(len(_buckets))\n        #                            if _buckets[b][0] > len(token_ids)])\n        #           # for loc in locs:\n        #               # Get a 1-element batch to feed the sentence to the model.\n        #           encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n        #                   {bucket_id: [(token_ids, [],)]}, bucket_id)\n        #\n        #           _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n        #                                                target_weights, bucket_id, True,beam_search)\n        #           # This is a greedy decoder - outputs are just argmaxes of output_logits.\n        #\n        #           outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n        #           # If there is an EOS symbol in outputs, cut them at that point.\n        #           if EOS_ID in outputs:\n        #               # print outputs\n        #               outputs = outputs[:outputs.index(EOS_ID)]\n        #\n        #           print("" "".join([tf.compat.as_str(rev_vocab[output]) for output in outputs]))\n        #           print(""> "", """")\n        #           sys.stdout.flush()\n        #           sentence = sys.stdin.readline()\n\ndef main(_):\n  if FLAGS.decode:\n    decode()\n  else:\n    train()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
data_utils.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport nltk\n\nimport pickle\nimport random\n\npadToken, goToken, eosToken, unknownToken = 0, 1, 2, 3\n\nclass Batch:\n    #batch\xe7\xb1\xbb\xef\xbc\x8c\xe9\x87\x8c\xe9\x9d\xa2\xe5\x8c\x85\xe5\x90\xab\xe4\xba\x86encoder\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8cdecoder\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8cdecoder\xe6\xa0\x87\xe7\xad\xbe\xef\xbc\x8cdecoder\xe6\xa0\xb7\xe6\x9c\xac\xe9\x95\xbf\xe5\xba\xa6mask\n    def __init__(self):\n        self.encoderSeqs = []\n        self.decoderSeqs = []\n        self.targetSeqs = []\n        self.weights = []\n\ndef loadDataset(filename):\n    '''\n    \xe8\xaf\xbb\xe5\x8f\x96\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe6\x8d\xae\n    :param filename: \xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x97\xe5\x85\xb8\xef\xbc\x8c\xe5\x8c\x85\xe5\x90\xabword2id\xe3\x80\x81id2word\xe5\x88\x86\xe5\x88\xab\xe6\x98\xaf\xe5\x8d\x95\xe8\xaf\x8d\xe4\xb8\x8e\xe7\xb4\xa2\xe5\xbc\x95\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\xad\x97\xe5\x85\xb8\xe5\x92\x8c\xe5\x8f\x8d\xe5\xba\x8f\xe5\xad\x97\xe5\x85\xb8\xef\xbc\x8c\n                    trainingSamples\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\x80\xe6\x9d\xa1\xe9\x83\xbd\xe6\x98\xafQA\xe5\xaf\xb9\n    :return: word2id, id2word, trainingSamples\n    '''\n    dataset_path = os.path.join(filename)\n    print('Loading dataset from {}'.format(dataset_path))\n    with open(dataset_path, 'rb') as handle:\n        data = pickle.load(handle)  # Warning: If adding something here, also modifying saveDataset\n        word2id = data['word2id']\n        id2word = data['id2word']\n        trainingSamples = data['trainingSamples']\n    return word2id, id2word, trainingSamples\n\ndef createBatch(samples, en_de_seq_len):\n    '''\n    \xe6\xa0\xb9\xe6\x8d\xae\xe7\xbb\x99\xe5\x87\xba\xe7\x9a\x84samples\xef\xbc\x88\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x89\xef\xbc\x8c\xe8\xbf\x9b\xe8\xa1\x8cpadding\xe5\xb9\xb6\xe6\x9e\x84\xe9\x80\xa0\xe6\x88\x90placeholder\xe6\x89\x80\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\xbd\xa2\xe5\xbc\x8f\n    :param samples: \xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x88\x97\xe8\xa1\xa8\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe9\x83\xbd\xe6\x98\xaf[question\xef\xbc\x8c answer]\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8cid\n    :param en_de_seq_len: \xe5\x88\x97\xe8\xa1\xa8\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe8\xa1\xa8\xe7\xa4\xbasource\xe7\xab\xaf\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe8\xa1\xa8\xe7\xa4\xbatarget\xe7\xab\xaf\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n    :return: \xe5\xa4\x84\xe7\x90\x86\xe5\xae\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbc\xa0\xe5\x85\xa5feed_dict\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\xa0\xbc\xe5\xbc\x8f\n    '''\n    batch = Batch()\n    #\xe6\xa0\xb9\xe6\x8d\xae\xe6\xa0\xb7\xe6\x9c\xac\xe9\x95\xbf\xe5\xba\xa6\xe8\x8e\xb7\xe5\xbe\x97batch size\xe5\xa4\xa7\xe5\xb0\x8f\n    batchSize = len(samples)\n    #\xe5\xb0\x86\xe6\xaf\x8f\xe6\x9d\xa1\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe5\x92\x8c\xe7\xad\x94\xe6\xa1\x88\xe5\x88\x86\xe5\xbc\x80\xe4\xbc\xa0\xe5\x85\xa5\xe5\x88\xb0\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\xe4\xb8\xad\n    for i in range(batchSize):\n        sample = samples[i]\n        batch.encoderSeqs.append(list(reversed(sample[0])))  # \xe5\xb0\x86\xe8\xbe\x93\xe5\x85\xa5\xe5\x8f\x8d\xe5\xba\x8f\xef\xbc\x8c\xe5\x8f\xaf\xe6\x8f\x90\xe9\xab\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x95\x88\xe6\x9e\x9c\n        batch.decoderSeqs.append([goToken] + sample[1] + [eosToken])  # Add the <go> and <eos> tokens\n        batch.targetSeqs.append(batch.decoderSeqs[-1][1:])  # Same as decoder, but shifted to the left (ignore the <go>)\n        # \xe5\xb0\x86\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0PAD\xe5\x88\xb0\xe6\x8c\x87\xe5\xae\x9a\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c\xe5\xb9\xb6\xe6\x9e\x84\xe9\x80\xa0weights\xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6mask\xe6\xa0\x87\xe5\xbf\x97\n        batch.encoderSeqs[i] = [padToken] * (en_de_seq_len[0] - len(batch.encoderSeqs[i])) + batch.encoderSeqs[i]\n        batch.weights.append([1.0] * len(batch.targetSeqs[i]) + [0.0] * (en_de_seq_len[1] - len(batch.targetSeqs[i])))\n        batch.decoderSeqs[i] = batch.decoderSeqs[i] + [padToken] * (en_de_seq_len[1] - len(batch.decoderSeqs[i]))\n        batch.targetSeqs[i] = batch.targetSeqs[i] + [padToken] * (en_de_seq_len[1] - len(batch.targetSeqs[i]))\n\n    #--------------------\xe6\x8e\xa5\xe4\xb8\x8b\xe6\x9d\xa5\xe5\xb0\xb1\xe6\x98\xaf\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8creshape\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\x8f\x98\xe6\x88\x90\xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6*batch_size\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae------------------------\n    encoderSeqsT = []  # Corrected orientation\n    for i in range(en_de_seq_len[0]):\n        encoderSeqT = []\n        for j in range(batchSize):\n            encoderSeqT.append(batch.encoderSeqs[j][i])\n        encoderSeqsT.append(encoderSeqT)\n    batch.encoderSeqs = encoderSeqsT\n\n    decoderSeqsT = []\n    targetSeqsT = []\n    weightsT = []\n    for i in range(en_de_seq_len[1]):\n        decoderSeqT = []\n        targetSeqT = []\n        weightT = []\n        for j in range(batchSize):\n            decoderSeqT.append(batch.decoderSeqs[j][i])\n            targetSeqT.append(batch.targetSeqs[j][i])\n            weightT.append(batch.weights[j][i])\n        decoderSeqsT.append(decoderSeqT)\n        targetSeqsT.append(targetSeqT)\n        weightsT.append(weightT)\n    batch.decoderSeqs = decoderSeqsT\n    batch.targetSeqs = targetSeqsT\n    batch.weights = weightsT\n\n    return batch\n\ndef getBatches(data, batch_size, en_de_seq_len):\n    '''\n    \xe6\xa0\xb9\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\xe5\x87\xba\xe6\x9d\xa5\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8cbatch_size\xe5\xb0\x86\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x86\xe6\x88\x90\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\xb0\x8fbatch\xe3\x80\x82\xe5\xaf\xb9\xe6\xaf\x8f\xe4\xb8\xaabatch\xe7\xb4\xa2\xe5\xbc\x95\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe8\xb0\x83\xe7\x94\xa8createBatch\xe5\x87\xbd\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xa4\x84\xe7\x90\x86\n    :param data: loadDataset\xe5\x87\xbd\xe6\x95\xb0\xe8\xaf\xbb\xe5\x8f\x96\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84trainingSamples\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xafQA\xe5\xaf\xb9\xe7\x9a\x84\xe5\x88\x97\xe8\xa1\xa8\n    :param batch_size: batch\xe5\xa4\xa7\xe5\xb0\x8f\n    :param en_de_seq_len: \xe5\x88\x97\xe8\xa1\xa8\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe8\xa1\xa8\xe7\xa4\xbasource\xe7\xab\xaf\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe8\xa1\xa8\xe7\xa4\xbatarget\xe7\xab\xaf\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n    :return: \xe5\x88\x97\xe8\xa1\xa8\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe9\x83\xbd\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x8f\xaf\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbc\xa0\xe5\x85\xa5feed_dict\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\n    '''\n    #\xe6\xaf\x8f\xe4\xb8\xaaepoch\xe4\xb9\x8b\xe5\x89\x8d\xe9\x83\xbd\xe8\xa6\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84shuffle\n    random.shuffle(data)\n    batches = []\n    data_len = len(data)\n    def genNextSamples():\n        for i in range(0, data_len, batch_size):\n            yield data[i:min(i + batch_size, data_len)]\n\n    for samples in genNextSamples():\n        batch = createBatch(samples, en_de_seq_len)\n        batches.append(batch)\n    return batches\n\ndef sentence2enco(sentence, word2id, en_de_seq_len):\n    '''\n    \xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\xb0\x86\xe7\x94\xa8\xe6\x88\xb7\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5feed\xe8\xbf\x9b\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe7\x8e\xb0\xe5\xb0\x86\xe5\x8f\xa5\xe5\xad\x90\xe8\xbd\xac\xe5\x8c\x96\xe6\x88\x90id\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe8\xb0\x83\xe7\x94\xa8createBatch\xe5\xa4\x84\xe7\x90\x86\n    :param sentence: \xe7\x94\xa8\xe6\x88\xb7\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\n    :param word2id: \xe5\x8d\x95\xe8\xaf\x8d\xe4\xb8\x8eid\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe5\xaf\xb9\xe5\xba\x94\xe5\x85\xb3\xe7\xb3\xbb\xe5\xad\x97\xe5\x85\xb8\n    :param en_de_seq_len: \xe5\x88\x97\xe8\xa1\xa8\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe8\xa1\xa8\xe7\xa4\xbasource\xe7\xab\xaf\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe8\xa1\xa8\xe7\xa4\xbatarget\xe7\xab\xaf\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n    :return: \xe5\xa4\x84\xe7\x90\x86\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x8f\xaf\xe7\x9b\xb4\xe6\x8e\xa5feed\xe8\xbf\x9b\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\n    '''\n    if sentence == '':\n        return None\n    #\xe5\x88\x86\xe8\xaf\x8d\n    tokens = nltk.word_tokenize(sentence)\n    if len(tokens) > en_de_seq_len[0]:\n        return None\n    #\xe5\xb0\x86\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8d\x95\xe8\xaf\x8d\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbaid\n    wordIds = []\n    for token in tokens:\n        wordIds.append(word2id.get(token, unknownToken))\n    #\xe8\xb0\x83\xe7\x94\xa8createBatch\xe6\x9e\x84\xe9\x80\xa0batch\n    batch = createBatch([[wordIds, []]], en_de_seq_len)\n    return batch\n"""
seq2seq.py,13,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport copy\n\n# We disable pylint because we need python3 compatibility.\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nfrom six.moves import zip  # pylint: disable=redefined-builtin\n\nfrom tensorflow.contrib.rnn.python.ops import core_rnn_cell\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.util import nest\n\nLinear = rnn_cell_impl._Linear  # pylint: disable=protected-access,invalid-name\n\ndef _extract_beam_search(embedding, beam_size, num_symbols, embedding_size, output_projection=None):\n\n    def loop_function(prev, i, log_beam_probs, beam_path, beam_symbols):\n        if output_projection is not None:\n            prev = nn_ops.xw_plus_b(prev, output_projection[0], output_projection[1])\n        # \xe5\xaf\xb9\xe8\xbe\x93\xe5\x87\xba\xe6\xa6\x82\xe7\x8e\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x92\x8c\xe5\x8f\x96log\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe5\xba\x8f\xe5\x88\x97\xe6\xa6\x82\xe7\x8e\x87\xe7\x9b\xb8\xe4\xb9\x98\xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8f\x98\xe6\x88\x90\xe6\xa6\x82\xe7\x8e\x87\xe7\x9b\xb8\xe5\x8a\xa0\n        probs = tf.log(tf.nn.softmax(prev))\n        if i == 1:\n            probs = tf.reshape(probs[0, :], [-1, num_symbols])\n        if i > 1:\n            # \xe5\xb0\x86\xe5\xbd\x93\xe5\x89\x8d\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe4\xb8\x8e\xe4\xb9\x8b\xe5\x89\x8d\xe5\xba\x8f\xe5\x88\x97\xe6\xa6\x82\xe7\x8e\x87\xe7\x9b\xb8\xe5\x8a\xa0\xe5\xbe\x97\xe5\x88\xb0\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb9\x8b\xe5\x89\x8d\xe6\x9c\x89beam_szie\xe4\xb8\xaa\xe5\xba\x8f\xe5\x88\x97\xef\xbc\x8c\xe6\x9c\xac\xe6\xac\xa1\xe4\xba\xa7\xe7\x94\x9fnum_symbols\xe4\xb8\xaa\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\n            # \xe6\x89\x80\xe4\xbb\xa5reshape\xe6\x88\x90\xe8\xbf\x99\xe6\xa0\xb7\xe7\x9a\x84tensor\n            probs = tf.reshape(probs + log_beam_probs[-1], [-1, beam_size * num_symbols])\n        # \xe9\x80\x89\xe5\x87\xba\xe6\xa6\x82\xe7\x8e\x87\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x89\x8dbeam_size\xe4\xb8\xaa\xe5\xba\x8f\xe5\x88\x97,\xe4\xbb\x8ebeam_size * num_symbols\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe4\xb8\xad\xe9\x80\x89\xe5\x87\xbabeam_size\xe4\xb8\xaa\n        best_probs, indices = tf.nn.top_k(probs, beam_size)\n        indices = tf.stop_gradient(tf.squeeze(tf.reshape(indices, [-1, 1])))\n        best_probs = tf.stop_gradient(tf.reshape(best_probs, [-1, 1]))\n\n        # beam_size * num_symbols\xef\xbc\x8c\xe7\x9c\x8b\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\x98\xaf\xe5\x93\xaa\xe4\xb8\xaa\xe5\xba\x8f\xe5\x88\x97\xe5\x92\x8c\xe5\x8d\x95\xe8\xaf\x8d\n        symbols = indices % num_symbols  # Which word in vocabulary.\n        beam_parent = indices // num_symbols  # Which hypothesis it came from.\n        beam_symbols.append(symbols)\n        beam_path.append(beam_parent)\n        log_beam_probs.append(best_probs)\n\n        # \xe5\xaf\xb9beam-search\xe9\x80\x89\xe5\x87\xba\xe7\x9a\x84beam size\xe4\xb8\xaa\xe5\x8d\x95\xe8\xaf\x8d\xe8\xbf\x9b\xe8\xa1\x8cembedding\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\n        emb_prev = embedding_ops.embedding_lookup(embedding, symbols)\n        emb_prev = tf.reshape(emb_prev, [-1, embedding_size])\n        return emb_prev\n\n    return loop_function\n\ndef beam_attention_decoder(decoder_inputs,\n                          initial_state,\n                          attention_states,\n                          cell,\n                           embedding,\n                          output_size=None,\n                          num_heads=1,\n                          loop_function=None,\n                          dtype=None,\n                          scope=None,\n                          initial_state_attention=False, output_projection=None, beam_size=10):\n    if not decoder_inputs:\n        raise ValueError(""Must provide at least 1 input to attention decoder."")\n    if num_heads < 1:\n        raise ValueError(""With less than 1 heads, use a non-attention decoder."")\n    if not attention_states.get_shape()[1:2].is_fully_defined():\n        raise ValueError(""Shape[1] and [2] of attention_states must be known: %s""\n                         % attention_states.get_shape())\n    if output_size is None:\n        output_size = cell.output_size\n\n    with variable_scope.variable_scope(scope or ""attention_decoder"", dtype=dtype) as scope:\n        dtype = scope.dtype\n        # batch_size = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n        attn_length = attention_states.get_shape()[1].value\n        if attn_length is None:\n            attn_length = array_ops.shape(attention_states)[1]\n        attn_size = attention_states.get_shape()[2].value\n\n        # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n        hidden = array_ops.reshape(attention_states, [-1, attn_length, 1, attn_size])\n        hidden_features = []\n        v = []\n        attention_vec_size = attn_size  # Size of query vectors for attention.\n        for a in xrange(num_heads):\n            k = variable_scope.get_variable(""AttnW_%d"" % a, [1, 1, attn_size, attention_vec_size])\n            hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], ""SAME""))\n            v.append(variable_scope.get_variable(""AttnV_%d"" % a, [attention_vec_size]))\n\n        state = []\n        # \xe5\xb0\x86encoder\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe9\x9a\x90\xe5\xb1\x82\xe7\x8a\xb6\xe6\x80\x81\xe6\x89\xa9\xe5\xb1\x95\xe6\x88\x90beam_size\xe7\xbb\xb4\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xbadecoder\xe9\x98\xb6\xe6\xae\xb5\xe7\x9a\x84batch_size\xe6\x98\xafbeam_size\xe3\x80\x82\n        # initial_state\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x88\x97\xe8\xa1\xa8\xef\xbc\x8cRNN\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe5\xb1\x82\xe5\xb0\xb1\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe9\x83\xbd\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaaLSTMStateTuple\xef\xbc\x8c\xe5\x8c\x85\xe5\x90\xabh,c\xe4\xb8\xa4\xe4\xb8\xaa\xe9\x9a\x90\xe5\xb1\x82\xe7\x8a\xb6\xe6\x80\x81\n        # \xe6\x89\x80\xe4\xbb\xa5\xe8\xa6\x81\xe5\xb0\x86\xe5\x85\xb6\xe6\x89\xa9\xe5\xb1\x95\xe6\x88\x90beam_size\xe7\xbb\xb4\xef\xbc\x8c\xe5\x85\xb6\xe5\xae\x9e\xe6\x98\xaf\xe6\x8a\x8ac\xe5\x92\x8ch\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x89\xa9\xe5\xb1\x95\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe5\x86\x8d\xe5\x90\x88\xe6\x88\x90LSTMStateTuple\xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xba\x86\n        for layers in initial_state:\n            c = [layers.c] * beam_size\n            h = [layers.h] * beam_size\n            c = tf.concat(c, 0)\n            h = tf.concat(h, 0)\n            state.append(rnn_cell_impl.LSTMStateTuple(c, h))\n        state = tuple(state)\n        # state_size = int(initial_state.get_shape().with_rank(2)[1])\n        # states = []\n        # for kk in range(beam_size):\n        #     states.append(initial_state)\n        # state = tf.concat(states, 0)\n        # state = initial_state\n\n        def attention(query):\n            ds = []  # Results of attention reads will be stored here.\n            if nest.is_sequence(query):  # If the query is a tuple, flatten it.\n                query_list = nest.flatten(query)\n                for q in query_list:  # Check that ndims == 2 if specified.\n                    ndims = q.get_shape().ndims\n                    if ndims:\n                        assert ndims == 2\n                query = array_ops.concat(query_list, 1)\n            for a in xrange(num_heads):\n                with variable_scope.variable_scope(""Attention_%d"" % a):\n                    y = Linear(query, attention_vec_size, True)(query)\n                    y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n                    # Attention mask is a softmax of v^T * tanh(...).\n                    s = math_ops.reduce_sum(v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n                    a = nn_ops.softmax(s)\n                    # Now calculate the attention-weighted vector d.\n                    d = math_ops.reduce_sum(array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden, [1, 2])\n                    ds.append(array_ops.reshape(d, [-1, attn_size]))\n            return ds\n\n        outputs = []\n        prev = None\n        # attention\xe4\xb9\x9f\xe8\xa6\x81\xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x90beam_size\xe4\xb8\xba\xe7\x9a\x84tensor\n        batch_attn_size = array_ops.stack([beam_size, attn_size])\n        attns = [array_ops.zeros(batch_attn_size, dtype=dtype) for _ in xrange(num_heads)]\n        for a in attns:  # Ensure the second shape of attention vectors is set.\n            a.set_shape([None, attn_size])\n        if initial_state_attention:\n            attns = attention(initial_state)\n\n        log_beam_probs, beam_path, beam_symbols = [], [], []\n        for i, inp in enumerate(decoder_inputs):\n            if i > 0:\n                variable_scope.get_variable_scope().reuse_variables()\n            # If loop_function is set, we use it instead of decoder_inputs.\n            if i == 0:\n                #i=0\xe6\x97\xb6\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe6\x97\xb6\xe4\xb8\x80\xe4\xb8\xaabatch_szie=beam_size\xe7\x9a\x84tensor\xef\xbc\x8c\xe4\xb8\x94\xe9\x87\x8c\xe9\x9d\xa2\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84\xe5\x80\xbc\xe9\x83\xbd\xe6\x98\xaf\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xef\xbc\x8c\xe9\x83\xbd\xe6\x98\xaf<GO>\xe6\xa0\x87\xe5\xbf\x97\n                inp = tf.nn.embedding_lookup(embedding, tf.constant(1, dtype=tf.int32, shape=[beam_size]))\n\n            if loop_function is not None and prev is not None:\n                with variable_scope.variable_scope(""loop_function"", reuse=True):\n                    inp = loop_function(prev, i, log_beam_probs, beam_path, beam_symbols)\n            # Merge input and previous attentions into one vector of the right size.\n            input_size = inp.get_shape().with_rank(2)[1]\n            if input_size.value is None:\n                raise ValueError(""Could not infer input size from input: %s"" % inp.name)\n            inputs = [inp] + attns\n            x = Linear(inputs, input_size, True)(inputs)\n\n            # Run the RNN.\n            cell_output, state = cell(x, state)\n            # Run the attention mechanism.\n            if i == 0 and initial_state_attention:\n                with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True):\n                    attns = attention(state)\n            else:\n                attns = attention(state)\n\n            with variable_scope.variable_scope(""AttnOutputProjection""):\n                inputs = [cell_output] + attns\n                output = Linear(inputs, output_size, True)(inputs)\n            if loop_function is not None:\n                prev = output\n            outputs.append(tf.argmax(nn_ops.xw_plus_b(output, output_projection[0], output_projection[1]), axis=1))\n\n    return outputs, state, tf.reshape(tf.concat(beam_path, 0), [-1, beam_size]), tf.reshape(tf.concat(beam_symbols, 0),\n                                                                                            [-1, beam_size])\n\ndef embedding_attention_decoder(decoder_inputs,\n                                initial_state,\n                                attention_states,\n                                cell,\n                                num_symbols,\n                                embedding_size,\n                                num_heads=1,\n                                output_size=None,\n                                output_projection=None,\n                                feed_previous=False,\n                                update_embedding_for_previous=True,\n                                dtype=None,\n                                scope=None,\n                                initial_state_attention=False, beam_search=True, beam_size=10):\n    if output_size is None:\n        output_size = cell.output_size\n    if output_projection is not None:\n        proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n        proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n    with variable_scope.variable_scope(scope or ""embedding_attention_decoder"", dtype=dtype) as scope:\n        embedding = variable_scope.get_variable(""embedding"", [num_symbols, embedding_size])\n        emb_inp = [embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs]\n        loop_function = _extract_beam_search(embedding, beam_size, num_symbols, embedding_size, output_projection)\n        return beam_attention_decoder(\n            emb_inp, initial_state, attention_states, cell, embedding, output_size=output_size,\n            num_heads=num_heads, loop_function=loop_function,\n            initial_state_attention=initial_state_attention, output_projection=output_projection,\n            beam_size=beam_size)\n\n\ndef embedding_attention_seq2seq(encoder_inputs,\n                                decoder_inputs,\n                                cell,\n                                num_encoder_symbols,\n                                num_decoder_symbols,\n                                embedding_size,\n                                num_heads=1,\n                                output_projection=None,\n                                feed_previous=False,\n                                dtype=None,\n                                scope=None,\n                                initial_state_attention=False, beam_search=True, beam_size=10):\n    with variable_scope.variable_scope(scope or ""embedding_attention_seq2seq"", dtype=dtype) as scope:\n        dtype = scope.dtype\n        # Encoder.\n        encoder_cell = copy.deepcopy(cell)\n        encoder_cell = core_rnn_cell.EmbeddingWrapper(encoder_cell, embedding_classes=num_encoder_symbols, embedding_size=embedding_size)\n        encoder_outputs, encoder_state = rnn.static_rnn(encoder_cell, encoder_inputs, dtype=dtype)\n\n        # First calculate a concatenation of encoder outputs to put attention on.\n        top_states = [array_ops.reshape(e, [-1, 1, cell.output_size]) for e in encoder_outputs]\n        attention_states = array_ops.concat(top_states, 1)\n\n        # Decoder.\n        output_size = None\n        if output_projection is None:\n            cell = core_rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n            output_size = num_decoder_symbols\n\n        return embedding_attention_decoder(\n            decoder_inputs,\n            encoder_state,\n            attention_states,\n            cell,\n            num_decoder_symbols,\n            embedding_size,\n            num_heads=num_heads,\n            output_size=output_size,\n            output_projection=output_projection,\n            feed_previous=feed_previous,\n            initial_state_attention=initial_state_attention, beam_search=beam_search, beam_size=beam_size)\n\n'"
seq2seq_model.py,21,"b'import tensorflow as tf\nfrom seq2seq import embedding_attention_seq2seq\nclass Seq2SeqModel():\n\n    def __init__(self, source_vocab_size, target_vocab_size, en_de_seq_len, hidden_size, num_layers,\n                 batch_size, learning_rate, num_samples=1024,\n                 forward_only=False, beam_search=True, beam_size=10):\n        \'\'\'\n        \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\xb9\xb6\xe5\x88\x9b\xe5\xbb\xba\xe6\xa8\xa1\xe5\x9e\x8b\n        :param source_vocab_size:encoder\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84vocab size\n        :param target_vocab_size: decoder\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84vocab size\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe8\xb7\x9f\xe4\xb8\x8a\xe9\x9d\xa2\xe4\xb8\x80\xe6\xa0\xb7\n        :param en_de_seq_len: \xe6\xba\x90\xe5\x92\x8c\xe7\x9b\xae\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n        :param hidden_size: RNN\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe5\x8d\x95\xe5\x85\x83\xe4\xb8\xaa\xe6\x95\xb0\n        :param num_layers: RNN\xe5\xa0\x86\xe5\x8f\xa0\xe7\x9a\x84\xe5\xb1\x82\xe6\x95\xb0\n        :param batch_size: batch\xe5\xa4\xa7\xe5\xb0\x8f\n        :param learning_rate: \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n        :param num_samples: \xe8\xae\xa1\xe7\xae\x97loss\xe6\x97\xb6\xe5\x81\x9asampled softmax\xe6\x97\xb6\xe7\x9a\x84\xe9\x87\x87\xe6\xa0\xb7\xe6\x95\xb0\n        :param forward_only: \xe9\xa2\x84\xe6\xb5\x8b\xe6\x97\xb6\xe6\x8c\x87\xe5\xae\x9a\xe4\xb8\xba\xe7\x9c\x9f\n        :param beam_search: \xe9\xa2\x84\xe6\xb5\x8b\xe6\x97\xb6\xe6\x98\xaf\xe9\x87\x87\xe7\x94\xa8greedy search\xe8\xbf\x98\xe6\x98\xafbeam search\n        :param beam_size: beam search\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n        \'\'\'\n        self.source_vocab_size = source_vocab_size\n        self.target_vocab_size = target_vocab_size\n        self.en_de_seq_len = en_de_seq_len\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.batch_size = batch_size\n        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n        self.num_samples = num_samples\n        self.forward_only = forward_only\n        self.beam_search = beam_search\n        self.beam_size = beam_size\n        self.global_step = tf.Variable(0, trainable=False)\n\n        output_projection = None\n        softmax_loss_function = None\n        # \xe5\xae\x9a\xe4\xb9\x89\xe9\x87\x87\xe6\xa0\xb7loss\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xbc\xa0\xe5\x85\xa5\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84sequence_loss_by_example\xe5\x87\xbd\xe6\x95\xb0\n        if num_samples > 0 and num_samples < self.target_vocab_size:\n            w = tf.get_variable(\'proj_w\', [hidden_size, self.target_vocab_size])\n            w_t = tf.transpose(w)\n            b = tf.get_variable(\'proj_b\', [self.target_vocab_size])\n            output_projection = (w, b)\n            #\xe8\xb0\x83\xe7\x94\xa8sampled_softmax_loss\xe5\x87\xbd\xe6\x95\xb0\xe8\xae\xa1\xe7\xae\x97sample loss\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe5\x8f\xaf\xe4\xbb\xa5\xe8\x8a\x82\xe7\x9c\x81\xe8\xae\xa1\xe7\xae\x97\xe6\x97\xb6\xe9\x97\xb4\n            def sample_loss(logits, labels):\n                labels = tf.reshape(labels, [-1, 1])\n                return tf.nn.sampled_softmax_loss(w_t, b, labels=labels, inputs=logits, num_sampled=num_samples, num_classes=self.target_vocab_size)\n            softmax_loss_function = sample_loss\n\n        self.keep_drop = tf.placeholder(tf.float32)\n        # \xe5\xae\x9a\xe4\xb9\x89encoder\xe5\x92\x8cdecoder\xe9\x98\xb6\xe6\xae\xb5\xe7\x9a\x84\xe5\xa4\x9a\xe5\xb1\x82dropout RNNCell\n        def create_rnn_cell():\n            encoDecoCell = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n            encoDecoCell = tf.contrib.rnn.DropoutWrapper(encoDecoCell, input_keep_prob=1.0, output_keep_prob=self.keep_drop)\n            return encoDecoCell\n        encoCell = tf.contrib.rnn.MultiRNNCell([create_rnn_cell() for _ in range(num_layers)])\n\n        # \xe5\xae\x9a\xe4\xb9\x89\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84placeholder\xef\xbc\x8c\xe9\x87\x87\xe7\x94\xa8\xe4\xba\x86\xe5\x88\x97\xe8\xa1\xa8\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\n        self.encoder_inputs = []\n        self.decoder_inputs = []\n        self.decoder_targets = []\n        self.target_weights = []\n        for i in range(en_de_seq_len[0]):\n            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None, ], name=""encoder{0}"".format(i)))\n        for i in range(en_de_seq_len[1]):\n            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None, ], name=""decoder{0}"".format(i)))\n            self.decoder_targets.append(tf.placeholder(tf.int32, shape=[None, ], name=""target{0}"".format(i)))\n            self.target_weights.append(tf.placeholder(tf.float32, shape=[None, ], name=""weight{0}"".format(i)))\n\n        # test\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x8c\xe5\xb0\x86\xe4\xb8\x8a\xe4\xb8\x80\xe6\x97\xb6\xe5\x88\xbb\xe8\xbe\x93\xe5\x87\xba\xe5\xbd\x93\xe5\x81\x9a\xe4\xb8\x8b\xe4\xb8\x80\xe6\x97\xb6\xe5\x88\xbb\xe8\xbe\x93\xe5\x85\xa5\xe4\xbc\xa0\xe5\x85\xa5\n        if forward_only:\n            if beam_search:#\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xafbeam_search\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe5\x88\x99\xe8\xb0\x83\xe7\x94\xa8\xe8\x87\xaa\xe5\xb7\xb1\xe5\x86\x99\xe7\x9a\x84embedding_attention_seq2seq\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8d\xe6\x98\xaflegacy_seq2seq\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\n                self.beam_outputs, _, self.beam_path, self.beam_symbol = embedding_attention_seq2seq(\n                    self.encoder_inputs, self.decoder_inputs, encoCell, num_encoder_symbols=source_vocab_size,\n                    num_decoder_symbols=target_vocab_size, embedding_size=hidden_size,\n                    output_projection=output_projection, feed_previous=True)\n            else:\n                decoder_outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n                    self.encoder_inputs, self.decoder_inputs, encoCell, num_encoder_symbols=source_vocab_size,\n                    num_decoder_symbols=target_vocab_size, embedding_size=hidden_size,\n                    output_projection=output_projection, feed_previous=True)\n                # \xe5\x9b\xa0\xe4\xb8\xbaseq2seq\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe6\x9c\xaa\xe6\x8c\x87\xe5\xae\x9aoutput_projection\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe9\x9c\x80\xe8\xa6\x81\xe5\x9c\xa8\xe8\xbe\x93\xe5\x87\xba\xe4\xb9\x8b\xe5\x90\x8e\xe8\x87\xaa\xe5\xb7\xb1\xe8\xbf\x9b\xe8\xa1\x8coutput_projection\n                if output_projection is not None:\n                    self.outputs = tf.matmul(decoder_outputs, output_projection[0]) + output_projection[1]\n        else:\n            # \xe5\x9b\xa0\xe4\xb8\xba\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe5\xb0\x86output\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\x8b\xe4\xb8\x80\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xb8\x8d\xe7\x94\xa8output_projection\n            decoder_outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n                self.encoder_inputs, self.decoder_inputs, encoCell, num_encoder_symbols=source_vocab_size,\n                num_decoder_symbols=target_vocab_size, embedding_size=hidden_size, output_projection=output_projection,\n                feed_previous=False)\n            self.loss = tf.contrib.legacy_seq2seq.sequence_loss(\n                decoder_outputs, self.decoder_targets, self.target_weights, softmax_loss_function=softmax_loss_function)\n\n            # Initialize the optimizer\n            opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08)\n            self.optOp = opt.minimize(self.loss)\n\n        self.saver = tf.train.Saver(tf.all_variables())\n\n    def step(self, session, encoder_inputs, decoder_inputs, decoder_targets, target_weights, go_token_id):\n        #\xe4\xbc\xa0\xe5\x85\xa5\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\xb9\xb6\xe8\xae\xad\xe7\xbb\x83\xe6\x80\xa7\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        # \xe6\x9e\x84\xe5\xbb\xbasess.run\xe6\x97\xb6\xe7\x9a\x84feed_inpits\n        feed_dict = {}\n        if not self.forward_only:\n            feed_dict[self.keep_drop] = 0.5\n            for i in range(self.en_de_seq_len[0]):\n                feed_dict[self.encoder_inputs[i].name] = encoder_inputs[i]\n            for i in range(self.en_de_seq_len[1]):\n                feed_dict[self.decoder_inputs[i].name] = decoder_inputs[i]\n                feed_dict[self.decoder_targets[i].name] = decoder_targets[i]\n                feed_dict[self.target_weights[i].name] = target_weights[i]\n            run_ops = [self.optOp, self.loss]\n        else:\n            feed_dict[self.keep_drop] = 1.0\n            for i in range(self.en_de_seq_len[0]):\n                feed_dict[self.encoder_inputs[i].name] = encoder_inputs[i]\n            feed_dict[self.decoder_inputs[0].name] = [go_token_id]\n            if self.beam_search:\n                run_ops = [self.beam_path, self.beam_symbol]\n            else:\n                run_ops = [self.outputs]\n\n        outputs = session.run(run_ops, feed_dict)\n        if not self.forward_only:\n            return None, outputs[1]\n        else:\n            if self.beam_search:\n                return outputs[0], outputs[1]'"
