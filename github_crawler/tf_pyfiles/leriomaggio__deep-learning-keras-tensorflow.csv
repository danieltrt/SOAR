file_path,api_count,code
2. Deep Learning Frameworks/kaggle_data.py,0,"b'""""""\nUtility functions to load Kaggle Otto Group Challenge Data.\n\nSince these data/functions are used in many notebooks, it is better\nto centralise functions to load and manipulate data so\nto not replicate code.\n""""""\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\n\ndef load_data(path, train=True):\n    """"""Load data from a CSV File\n    \n    Parameters\n    ----------\n    path: str\n        The path to the CSV file\n        \n    train: bool (default True)\n        Decide whether or not data are *training data*.\n        If True, some random shuffling is applied.\n        \n    Return\n    ------\n    X: numpy.ndarray \n        The data as a multi dimensional array of floats\n    ids: numpy.ndarray\n        A vector of ids for each sample\n    """"""\n    df = pd.read_csv(path)\n    X = df.values.copy()\n    if train:\n        np.random.shuffle(X)  # https://youtu.be/uyUXoap67N8\n        X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n        return X, labels\n    else:\n        X, ids = X[:, 1:].astype(np.float32), X[:, 0].astype(str)\n        return X, ids\n        \n        \ndef preprocess_data(X, scaler=None):\n    """"""Preprocess input data by standardise features \n    by removing the mean and scaling to unit variance""""""\n    if not scaler:\n        scaler = StandardScaler()\n        scaler.fit(X)\n    X = scaler.transform(X)\n    return X, scaler\n\n\ndef preprocess_labels(labels, encoder=None, categorical=True):\n    """"""Encode labels with values among 0 and `n-classes-1`""""""\n    if not encoder:\n        encoder = LabelEncoder()\n        encoder.fit(labels)\n    y = encoder.transform(labels).astype(np.int32)\n    if categorical:\n        y = np_utils.to_categorical(y)\n    return y, encoder'"
2. Deep Learning Frameworks/mnist_data.py,11,"b'# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions for downloading and reading MNIST data.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\n\nimport numpy\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nSOURCE_URL = \'http://yann.lecun.com/exdb/mnist/\'\n\n\ndef maybe_download(filename, work_directory):\n  """"""Download the data from Yann\'s website, unless it\'s already here.""""""\n  if not tf.gfile.Exists(work_directory):\n    tf.gfile.MakeDirs(work_directory)\n  filepath = os.path.join(work_directory, filename)\n  if not tf.gfile.Exists(filepath):\n    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n    with tf.gfile.GFile(filepath) as f:\n      size = f.size()\n    print(\'Successfully downloaded\', filename, size, \'bytes.\')\n  return filepath\n\n\ndef _read32(bytestream):\n  dt = numpy.dtype(numpy.uint32).newbyteorder(\'>\')\n  return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n\n\ndef extract_images(filename):\n  """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""\n  print(\'Extracting\', filename)\n  with tf.gfile.Open(filename, \'rb\') as f, gzip.GzipFile(fileobj=f) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2051:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST image file: %s\' %\n          (magic, filename))\n    num_images = _read32(bytestream)\n    rows = _read32(bytestream)\n    cols = _read32(bytestream)\n    buf = bytestream.read(rows * cols * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n    data = data.reshape(num_images, rows, cols, 1)\n    return data\n\n\ndef dense_to_one_hot(labels_dense, num_classes=10):\n  """"""Convert class labels from scalars to one-hot vectors.""""""\n  num_labels = labels_dense.shape[0]\n  index_offset = numpy.arange(num_labels) * num_classes\n  labels_one_hot = numpy.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\n\n\ndef extract_labels(filename, one_hot=False):\n  """"""Extract the labels into a 1D uint8 numpy array [index].""""""\n  print(\'Extracting\', filename)\n  with tf.gfile.Open(filename, \'rb\') as f, gzip.GzipFile(fileobj=f) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2049:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST label file: %s\' %\n          (magic, filename))\n    num_items = _read32(bytestream)\n    buf = bytestream.read(num_items)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n    if one_hot:\n      return dense_to_one_hot(labels)\n    return labels\n\n\nclass DataSet(object):\n\n  def __init__(self, images, labels, fake_data=False, one_hot=False,\n               dtype=tf.float32):\n    """"""Construct a DataSet.\n\n    one_hot arg is used only if fake_data is true.  `dtype` can be either\n    `uint8` to leave the input as `[0, 255]`, or `float32` to rescale into\n    `[0, 1]`.\n    """"""\n    dtype = tf.as_dtype(dtype).base_dtype\n    if dtype not in (tf.uint8, tf.float32):\n      raise TypeError(\'Invalid image dtype %r, expected uint8 or float32\' %\n                      dtype)\n    if fake_data:\n      self._num_examples = 10000\n      self.one_hot = one_hot\n    else:\n      assert images.shape[0] == labels.shape[0], (\n          \'images.shape: %s labels.shape: %s\' % (images.shape,\n                                                 labels.shape))\n      self._num_examples = images.shape[0]\n\n      # Convert shape from [num examples, rows, columns, depth]\n      # to [num examples, rows*columns] (assuming depth == 1)\n      assert images.shape[3] == 1\n      images = images.reshape(images.shape[0],\n                              images.shape[1] * images.shape[2])\n      if dtype == tf.float32:\n        # Convert from [0, 255] -> [0.0, 1.0].\n        images = images.astype(numpy.float32)\n        images = numpy.multiply(images, 1.0 / 255.0)\n    self._images = images\n    self._labels = labels\n    self._epochs_completed = 0\n    self._index_in_epoch = 0\n\n  @property\n  def images(self):\n    return self._images\n\n  @property\n  def labels(self):\n    return self._labels\n\n  @property\n  def num_examples(self):\n    return self._num_examples\n\n  @property\n  def epochs_completed(self):\n    return self._epochs_completed\n\n  def next_batch(self, batch_size, fake_data=False):\n    """"""Return the next `batch_size` examples from this data set.""""""\n    if fake_data:\n      fake_image = [1] * 784\n      if self.one_hot:\n        fake_label = [1] + [0] * 9\n      else:\n        fake_label = 0\n      return [fake_image for _ in xrange(batch_size)], [\n          fake_label for _ in xrange(batch_size)]\n    start = self._index_in_epoch\n    self._index_in_epoch += batch_size\n    if self._index_in_epoch > self._num_examples:\n      # Finished epoch\n      self._epochs_completed += 1\n      # Shuffle the data\n      perm = numpy.arange(self._num_examples)\n      numpy.random.shuffle(perm)\n      self._images = self._images[perm]\n      self._labels = self._labels[perm]\n      # Start next epoch\n      start = 0\n      self._index_in_epoch = batch_size\n      assert batch_size <= self._num_examples\n    end = self._index_in_epoch\n    return self._images[start:end], self._labels[start:end]\n\n\ndef read_data_sets(train_dir, fake_data=False, one_hot=False, dtype=tf.float32):\n  class DataSets(object):\n    pass\n  data_sets = DataSets()\n\n  if fake_data:\n    def fake():\n      return DataSet([], [], fake_data=True, one_hot=one_hot, dtype=dtype)\n    data_sets.train = fake()\n    data_sets.validation = fake()\n    data_sets.test = fake()\n    return data_sets\n\n  TRAIN_IMAGES = \'train-images-idx3-ubyte.gz\'\n  TRAIN_LABELS = \'train-labels-idx1-ubyte.gz\'\n  TEST_IMAGES = \'t10k-images-idx3-ubyte.gz\'\n  TEST_LABELS = \'t10k-labels-idx1-ubyte.gz\'\n  VALIDATION_SIZE = 5000\n\n  local_file = maybe_download(TRAIN_IMAGES, train_dir)\n  train_images = extract_images(local_file)\n\n  local_file = maybe_download(TRAIN_LABELS, train_dir)\n  train_labels = extract_labels(local_file, one_hot=one_hot)\n\n  local_file = maybe_download(TEST_IMAGES, train_dir)\n  test_images = extract_images(local_file)\n\n  local_file = maybe_download(TEST_LABELS, train_dir)\n  test_labels = extract_labels(local_file, one_hot=one_hot)\n\n  validation_images = train_images[:VALIDATION_SIZE]\n  validation_labels = train_labels[:VALIDATION_SIZE]\n  train_images = train_images[VALIDATION_SIZE:]\n  train_labels = train_labels[VALIDATION_SIZE:]\n\n  data_sets.train = DataSet(train_images, train_labels, dtype=dtype)\n  data_sets.validation = DataSet(validation_images, validation_labels,\n                                 dtype=dtype)\n  data_sets.test = DataSet(test_images, test_labels, dtype=dtype)\n\n  return data_sets'"
6. AutoEncoders and Embeddings/word2vec.py,0,"b'from gensim.models import word2vec\nfrom os.path import join, exists, split\nimport os\nimport numpy as np\n\ndef train_word2vec(sentence_matrix, vocabulary_inv,\n                   num_features=300, min_word_count=1, context=10):\n    """"""\n    Trains, saves, loads Word2Vec model\n    Returns initial weights for embedding layer.\n   \n    inputs:\n    sentence_matrix # int matrix: num_sentences x max_sentence_len\n    vocabulary_inv  # dict {str:int}\n    num_features    # Word vector dimensionality                      \n    min_word_count  # Minimum word count                        \n    context         # Context window size \n    """"""\n    model_dir = \'word2vec_models\'\n    model_name = ""{:d}features_{:d}minwords_{:d}context"".format(num_features, min_word_count, context)\n    model_name = join(model_dir, model_name)\n    if exists(model_name):\n        embedding_model = word2vec.Word2Vec.load(model_name)\n        print(\'Loading existing Word2Vec model \\\'%s\\\'\' % split(model_name)[-1])\n    else:\n        # Set values for various parameters\n        num_workers = 2       # Number of threads to run in parallel\n        downsampling = 1e-3   # Downsample setting for frequent words\n        \n        # Initialize and train the model\n        print(""Training Word2Vec model..."")\n        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n                            size=num_features, min_count = min_word_count, \\\n                            window = context, sample = downsampling)\n        \n        # If we don\'t plan to train the model any further, calling \n        # init_sims will make the model much more memory-efficient.\n        embedding_model.init_sims(replace=True)\n        \n        # Saving the model for later use. You can load it later using Word2Vec.load()\n        if not exists(model_dir):\n            os.mkdir(model_dir)\n        print(\'Saving Word2Vec model \\\'%s\\\'\' % split(model_name)[-1])\n        embedding_model.save(model_name)\n    \n    #  add unknown words\n    embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n                                                        for w in vocabulary_inv])]\n    return embedding_weights\n\nif __name__==\'__main__\':\n    import data_helpers\n    print(""Loading data..."")\n    x, _, _, vocabulary_inv = data_helpers.load_data()\n    w = train_word2vec(x, vocabulary_inv)\n'"
6. AutoEncoders and Embeddings/word_embedding.py,0,"b'import numpy as np\nimport re\nimport itertools\nfrom collections import Counter\n""""""\nOriginal taken from https://github.com/dennybritz/cnn-text-classification-tf\n""""""\n\ndef clean_str(string):\n    """"""\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    """"""\n    string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)\n    string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n    string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n    string = re.sub(r""\\\'re"", "" \\\'re"", string)\n    string = re.sub(r""\\\'d"", "" \\\'d"", string)\n    string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n    string = re.sub(r"","", "" , "", string)\n    string = re.sub(r""!"", "" ! "", string)\n    string = re.sub(r""\\("", "" \\( "", string)\n    string = re.sub(r""\\)"", "" \\) "", string)\n    string = re.sub(r""\\?"", "" \\? "", string)\n    string = re.sub(r""\\s{2,}"", "" "", string)\n    return string.strip().lower()\n\n\ndef load_data_and_labels():\n    """"""\n    Loads MR polarity data from files, splits the data into words and generates labels.\n    Returns split sentences and labels.\n    """"""\n    # Load data from files\n    positive_examples = list(open(""../data/word_embeddings/rt-polarity.pos"", encoding=\'ISO-8859-1\').readlines())\n    positive_examples = [s.strip() for s in positive_examples]\n    negative_examples = list(open(""../data/word_embeddings/rt-polarity.neg"", encoding=\'ISO-8859-1\').readlines())\n    negative_examples = [s.strip() for s in negative_examples]\n    # Split by words\n    x_text = positive_examples + negative_examples\n    x_text = [clean_str(sent) for sent in x_text]\n    x_text = [s.split("" "") for s in x_text]\n    # Generate labels\n    positive_labels = [[0, 1] for _ in positive_examples]\n    negative_labels = [[1, 0] for _ in negative_examples]\n    y = np.concatenate([positive_labels, negative_labels], 0)\n    return [x_text, y]\n\n\ndef pad_sentences(sentences, padding_word=""<PAD/>""):\n    """"""\n    Pads all sentences to the same length. The length is defined by the longest sentence.\n    Returns padded sentences.\n    """"""\n    sequence_length = max(len(x) for x in sentences)\n    padded_sentences = []\n    for i in range(len(sentences)):\n        sentence = sentences[i]\n        num_padding = sequence_length - len(sentence)\n        new_sentence = sentence + [padding_word] * num_padding\n        padded_sentences.append(new_sentence)\n    return padded_sentences\n\n\ndef build_vocab(sentences):\n    """"""\n    Builds a vocabulary mapping from word to index based on the sentences.\n    Returns vocabulary mapping and inverse vocabulary mapping.\n    """"""\n    # Build vocabulary\n    word_counts = Counter(itertools.chain(*sentences))\n    # Mapping from index to word\n    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n    # Mapping from word to index\n    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n    return [vocabulary, vocabulary_inv]\n\n\ndef build_input_data(sentences, labels, vocabulary):\n    """"""\n    Maps sentencs and labels to vectors based on a vocabulary.\n    """"""\n    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n    y = np.array(labels)\n    return [x, y]\n\n\ndef load_data():\n    """"""\n    Loads and preprocessed data for the MR dataset.\n    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n    """"""\n    # Load and preprocess data\n    sentences, labels = load_data_and_labels()\n    sentences_padded = pad_sentences(sentences)\n    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n    x, y = build_input_data(sentences_padded, labels, vocabulary)\n    return [x, y, vocabulary, vocabulary_inv]\n\n\ndef batch_iter(data, batch_size, num_epochs):\n    """"""\n    Generates a batch iterator for a dataset.\n    """"""\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int(len(data)/batch_size) + 1\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        shuffle_indices = np.random.permutation(np.arange(data_size))\n        shuffled_data = data[shuffle_indices]\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]'"
solutions/sol_111.py,0,"b'ann = MLP(2, 10, 1)\n%timeit -n 1 -r 1 ann.train(zip(X,y), iterations=2)\nplot_decision_boundary(ann)\nplt.title(""Our next model with 10 hidden units"")\n'"
solutions/sol_112.py,0,"b'ann = MLP(2, 10, 1)\n%timeit -n 1 -r 1 ann.train(zip(X,y), iterations=100)\nplot_decision_boundary(ann)\nplt.title(""Our model with 10 hidden units and 100 iterations"")\n'"
solutions/sol_223.py,0,"b""from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Flatten, Activation\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.optimizers import Adadelta\n\ninput_shape = (3, 32, 32)\nnb_classes = 10\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=input_shape))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=Adadelta(),\n              metrics=['accuracy'])"""
solutions/sol_2311.py,0,b'y = w * x + b\nloss = K.mean(K.square(y-target))'
solutions/sol_2312.py,0,"b'grads = K.gradients(loss, [w,b])\nupdates = [(w, w-lr*grads[0]), (b, b-lr*grads[1])]'"
solutions/sol_2313.py,0,"b""plt.plot(range(len(loss_history)), loss_history, 'o', label='Linear Regression Training phase')\nplt.ylabel('cost')\nplt.xlabel('epoch')\nplt.legend()\nplt.show()"""
solutions/sol_312.py,0,"b""from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(784,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=SGD(), \n              metrics=['accuracy'])"""
solutions/sol_321.py,0,"b""from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(784,)))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(nb_classes, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=SGD(), \n              metrics=['accuracy'])"""
solutions/sol_821.py,0,"b""from keras.datasets import mnist\nfrom keras.utils import np_utils\n\nimg_rows, img_cols = 28, 28\nnb_classes = 10\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n    \nX_train = X_train.reshape(X_train.shape[0], img_rows*img_cols)\nX_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n\nX_train /= 255\nX_test /= 255\n\nprint('X_train shape:', X_train.shape)\nprint('y_train shape:', y_train.shape)\nprint('X_test shape:', X_test.shape)\nprint('y_test shape:', y_test.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n\ny_train = np_utils.to_categorical(y_train, nb_classes)\ny_test = np_utils.to_categorical(y_test, nb_classes)"""
