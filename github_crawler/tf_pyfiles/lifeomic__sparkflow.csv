file_path,api_count,code
setup.py,0,"b'from setuptools import setup\nfrom setuptools import find_packages\n\nsetup(name=\'sparkflow\',\n      version=\'0.7.0\',\n      description=\'Deep learning on Spark with Tensorflow\',\n      keywords = [\'tensorflow\', \'spark\', \'sparkflow\', \'machine learning\', \'lifeomic\', \'deep learning\'],\n      url=\'https://github.com/lifeomic/sparkflow\',\n      download_url=\'https://github.com/lifeomic/sparkflow/archive/0.7.0.tar.gz\',\n      author=\'Derek Miller\',\n      author_email=\'dmmiller612@gmail.com\',\n      long_description=open(""README.md"", ""r"", encoding=\'utf-8\').read(),\n      long_description_content_type=""text/markdown"",\n      install_requires=[\'tensorflow\', \'flask\', \'protobuf\', \'requests\', \'dill\'],\n      license=\'MIT\',\n      packages=find_packages(),\n      zip_safe=False)\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# SparkFlow documentation build configuration file, created by\n# sphinx-quickstart on Wed Sep  5 21:30:55 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = []\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = [\'.rst\', \'.md\']\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'SparkFlow\'\ncopyright = u\'2018, Derek Miller\'\nauthor = u\'Derek Miller\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = u\'0.3.1\'\n# The full version, including alpha/beta/rc tags.\nrelease = u\'0.3.1\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    \'**\': [\n        \'relations.html\',  # needs \'show_related\': True theme option to display\n        \'searchbox.html\',\n    ]\n}\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'SparkFlowdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'SparkFlow.tex\', u\'SparkFlow Documentation\',\n     u\'Derek Miller\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'sparkflow\', u\'SparkFlow Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'SparkFlow\', u\'SparkFlow Documentation\',\n     author, \'SparkFlow\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n'"
examples/__init__.py,0,b''
examples/autoencoder_example.py,6,"b'from pyspark.sql import SparkSession\nimport tensorflow as tf\nfrom pyspark.ml.feature import VectorAssembler, Normalizer\nfrom sparkflow.tensorflow_async import SparkAsyncDL, SparkAsyncDLModel\nfrom pyspark.sql.functions import rand\nfrom sparkflow.graph_utils import build_graph\n\n\ndef small_model():\n    x = tf.placeholder(""float"", shape=[None, 784], name=\'x\')\n    layer1 = tf.layers.dense(x, 256, activation=tf.nn.relu)\n    layer2 = tf.layers.dense(layer1, 128, activation=tf.nn.sigmoid, name=\'out\')\n    layer3 = tf.layers.dense(layer2, 256, activation=tf.nn.relu)\n    layer4 = tf.layers.dense(layer3, 784, activation=tf.nn.sigmoid)\n    loss = tf.losses.mean_squared_error(layer4, x)\n    return loss\n\nif __name__ == \'__main__\':\n    spark = SparkSession.builder \\\n        .appName(""examples"") \\\n        .master(\'local[4]\').config(\'spark.driver.memory\', \'2g\') \\\n        .getOrCreate()\n\n    df = spark.read.option(""inferSchema"", ""true"").csv(\'examples/mnist_train.csv\').orderBy(rand())\n    mg = build_graph(small_model)\n\n    va = VectorAssembler(inputCols=df.columns[1:785], outputCol=\'feats\').transform(df).select([\'feats\'])\n    na = Normalizer(inputCol=\'feats\', outputCol=\'features\', p=1.0).transform(va).select([\'features\'])\n\n    #demonstration of options. Not all are required\n    spark_model = SparkAsyncDL(\n        inputCol=\'features\',\n        tensorflowGraph=mg,\n        tfInput=\'x:0\',\n        tfLabel=None,\n        tfOutput=\'out/Sigmoid:0\',\n        tfOptimizer=\'adam\',\n        tfLearningRate=.001,\n        iters=10,\n        predictionCol=\'predicted\',\n        partitions=4,\n        miniBatchSize=256,\n        verbose=1\n    ).fit(na)\n\n    t = spark_model.transform(na).take(1)\n    print(t[0][\'predicted\'])\n'"
examples/cnn_example.py,11,"b'from pyspark.sql import SparkSession\nimport tensorflow as tf\nfrom pyspark.ml.feature import VectorAssembler, OneHotEncoder\nfrom sparkflow.tensorflow_async import SparkAsyncDL\nfrom pyspark.sql.functions import rand\nfrom sparkflow.graph_utils import build_graph\nfrom pyspark.ml.pipeline import Pipeline\n\n\ndef cnn_model():\n    x = tf.placeholder(tf.float32, shape=[None, 784], name=\'x\')\n    y = tf.placeholder(tf.float32, shape=[None, 10], name=\'y\')\n    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n    conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n    conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n    conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n    conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n    fc1 = tf.layers.flatten(conv2)\n    out = tf.layers.dense(fc1, 10)\n    z = tf.argmax(out, 1, name=\'out\')\n    loss = tf.losses.softmax_cross_entropy(y, out)\n    return loss\n\n\nif __name__ == \'__main__\':\n    spark = SparkSession.builder \\\n        .appName(""examples"") \\\n        .master(\'local[4]\').config(\'spark.driver.memory\', \'4g\') \\\n        .getOrCreate()\n\n    df = spark.read.option(""inferSchema"", ""true"").csv(\'examples/mnist_train.csv\').orderBy(rand())\n    mg = build_graph(cnn_model)\n    va = VectorAssembler(inputCols=df.columns[1:785], outputCol=\'features\')\n    encoded = OneHotEncoder(inputCol=\'_c0\', outputCol=\'labels\', dropLast=False)\n\n    spark_model = SparkAsyncDL(\n        inputCol=\'features\',\n        tensorflowGraph=mg,\n        tfInput=\'x:0\',\n        tfLabel=\'y:0\',\n        tfOptimizer=\'adam\',\n        miniBatchSize=300,\n        miniStochasticIters=-1,\n        shufflePerIter=True,\n        iters=50,\n        partitions=4,\n        tfLearningRate=.0001,\n        predictionCol=\'predicted\',\n        labelCol=\'labels\',\n        verbose=1\n    )\n\n    p = Pipeline(stages=[va, encoded, spark_model]).fit(df)\n    p.save(""cnn"")\n'"
examples/simple_dnn.py,7,"b'from pyspark.sql import SparkSession\nimport tensorflow as tf\nfrom pyspark.ml.feature import VectorAssembler, OneHotEncoder\nfrom sparkflow.tensorflow_async import SparkAsyncDL\nfrom pyspark.sql.functions import rand\nfrom sparkflow.graph_utils import build_graph\nfrom sparkflow.graph_utils import build_adam_config\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.pipeline import Pipeline, PipelineModel\nfrom sparkflow.pipeline_util import PysparkPipelineWrapper\n\n\ndef small_model():\n    x = tf.placeholder(tf.float32, shape=[None, 784], name=\'x\')\n    y = tf.placeholder(tf.float32, shape=[None, 10], name=\'y\')\n    layer1 = tf.layers.dense(x, 256, activation=tf.nn.relu, kernel_initializer=tf.glorot_uniform_initializer())\n    layer2 = tf.layers.dense(layer1, 256, activation=tf.nn.relu, kernel_initializer=tf.glorot_uniform_initializer())\n    out = tf.layers.dense(layer2, 10, kernel_initializer=tf.glorot_uniform_initializer())\n    z = tf.argmax(out, 1, name=\'out\')\n    loss = tf.losses.softmax_cross_entropy(y, out)\n    return loss\n\n\nif __name__ == \'__main__\':\n    spark = SparkSession.builder \\\n        .appName(""examples"") \\\n        .master(\'local[4]\').config(\'spark.driver.memory\', \'2g\') \\\n        .getOrCreate()\n\n    # Read in mnist_train.csv dataset\n    df = spark.read.option(""inferSchema"", ""true"").csv(\'examples/mnist_train.csv\').orderBy(rand())\n\n    # Build the tensorflow graph\n    mg = build_graph(small_model)\n\n    # Build the adam optimizer\n    adam_config = build_adam_config(learning_rate=0.001, beta1=0.9, beta2=0.999)\n\n    # Setup features\n    vector_assembler = VectorAssembler(inputCols=df.columns[1:785], outputCol=\'features\')\n    encoder = OneHotEncoder(inputCol=\'_c0\', outputCol=\'labels\', dropLast=False)\n\n    # Demonstration of options. Not all are required\n    spark_model = SparkAsyncDL(\n        inputCol=\'features\',\n        tensorflowGraph=mg,\n        tfInput=\'x:0\',\n        tfLabel=\'y:0\',\n        tfOutput=\'out:0\',\n        tfOptimizer=\'adam\',\n        miniBatchSize=300,\n        miniStochasticIters=1,\n        shufflePerIter=True,\n        iters=50,\n        predictionCol=\'predicted\',\n        labelCol=\'labels\',\n        partitions=4,\n        verbose=1,\n        optimizerOptions=adam_config\n    )\n\n    # Create and save the Pipeline\n    p = Pipeline(stages=[vector_assembler, encoder, spark_model]).fit(df)\n    p.save(\'simple_dnn\')\n\n    # Example of loading the pipeline\n    loaded_pipeline = PysparkPipelineWrapper.unwrap(PipelineModel.load(\'simple_dnn\'))\n\n    # Run predictions and evaluation\n    predictions = loaded_pipeline.transform(df)\n    evaluator = MulticlassClassificationEvaluator(\n        labelCol=""_c0"", predictionCol=""predicted"", metricName=""accuracy"")\n    accuracy = evaluator.evaluate(predictions)\n    print(""Test Error = %g"" % (1.0 - accuracy))\n'"
sparkflow/HogwildSparkModel.py,17,"b'from flask import Flask, request\nimport six.moves.cPickle as pickle\nfrom sparkflow.ml_util import tensorflow_get_weights, tensorflow_set_weights, handle_features, handle_feed_dict, handle_shuffle\n\nfrom google.protobuf import json_format\nimport socket\nimport time\nimport tensorflow as tf\nimport itertools\nfrom sparkflow.RWLock import RWLock\nfrom multiprocessing import Process\nimport multiprocessing\nimport uuid\nimport requests\n\n\nimport logging\nlog = logging.getLogger(\'werkzeug\')\nlog.setLevel(logging.ERROR)\n\n\ndef get_server_weights(master_url=\'localhost:5000\'):\n    """"""\n    This will get the raw weights, pickle load them, and return.\n    """"""\n    r = requests.get(\'http://{0}/parameters\'.format(master_url))\n    weights = pickle.loads(r.content)\n    return weights\n\n\ndef put_deltas_to_server(delta, master_url=\'localhost:5000\'):\n    """"""\n    This updates the master parameters. We just use simple pickle serialization here.\n    """"""\n    requests.post(\'http://{0}/update\'.format(master_url), data=pickle.dumps(delta, -1))\n\n\ndef handle_model(data, graph_json, tfInput, tfLabel=None,\n                 master_url=\'localhost:5000\', iters=1000,\n                 mini_batch_size=-1, shuffle=True,\n                 mini_stochastic_iters=-1, verbose=0, loss_callback=None):\n    is_supervised = tfLabel is not None\n    features, labels = handle_features(data, is_supervised)\n\n    gd = tf.MetaGraphDef()\n    gd = json_format.Parse(graph_json, gd)\n    new_graph = tf.Graph()\n    with tf.Session(graph=new_graph) as sess:\n        tf.train.import_meta_graph(gd)\n        loss_variable = tf.get_collection(tf.GraphKeys.LOSSES)[0]\n        sess.run(tf.global_variables_initializer())\n        trainable_variables = tf.trainable_variables()\n        grads = tf.gradients(loss_variable, trainable_variables)\n        grads = list(zip(grads, trainable_variables))\n        partition_id = uuid.uuid4().hex\n        for i in range(0, iters):\n            weights = get_server_weights(master_url)\n            tensorflow_set_weights(weights, vs=trainable_variables)\n            if shuffle:\n                features, labels = handle_shuffle(features, labels)\n\n            if mini_stochastic_iters >= 1:\n                for _ in range(0, mini_stochastic_iters):\n                    gradients = []\n                    feed_dict = handle_feed_dict(features, tfInput, tfLabel, labels, mini_batch_size)\n                    for x in range(len(grads)):\n                        gradients.append(grads[x][0].eval(feed_dict=feed_dict))\n                    try:\n                        put_deltas_to_server(gradients, master_url)\n                    except Exception:\n                        print(""Timeout error from partition %s"" % partition_id)\n            elif mini_batch_size >= 1:\n                for r in range(0, len(features), mini_batch_size):\n                    gradients = []\n                    weights = get_server_weights(master_url)\n                    tensorflow_set_weights(weights, vs=trainable_variables)\n                    feed_dict = handle_feed_dict(features, tfInput, tfLabel, labels, mini_batch_size, idx=r)\n                    for x in range(len(grads)):\n                        gradients.append(grads[x][0].eval(feed_dict=feed_dict))\n                    try:\n                        put_deltas_to_server(gradients, master_url)\n                    except Exception:\n                        print(""Timeout error from partition %s"" % partition_id)\n            else:\n                gradients = []\n                feed_dict = handle_feed_dict(features, tfInput, tfLabel, labels, mini_batch_size)\n                for x in range(len(grads)):\n                    gradients.append(grads[x][0].eval(feed_dict=feed_dict))\n                try:\n                    put_deltas_to_server(gradients, master_url)\n                except Exception:\n                    print(""Timeout error from partition %s"" % partition_id)\n\n            if verbose or loss_callback:\n                feed_dict = handle_feed_dict(features, tfInput, tfLabel, labels, -1)\n                loss = sess.run(loss_variable, feed_dict=feed_dict)\n                if verbose:\n                    print(""Partition Id: %s, Iteration: %i, Loss: %f"" % (partition_id, i, loss))\n                if loss_callback:\n                    loss_callback(loss, i, partition_id)\n\n\nclass HogwildSparkModel(object):\n    """"""\n    Hogwild implementation of spark and tensorflow. This sets up a service with Flask, and each of the nodes will\n    send their computed gradients to the driver, where they will be updated randomly. Without being thread safe,\n    this provides stochasticity, avoiding biases for large models\n    """"""\n\n    def __init__(self,\n                 tensorflowGraph=None,\n                 iters=1000,\n                 tfInput=None,\n                 tfLabel=None,\n                 optimizer=None,\n                 master_url=None,\n                 serverStartup=8,\n                 acquire_lock=False,\n                 mini_batch=-1,\n                 mini_stochastic_iters=-1,\n                 shuffle=True,\n                 verbose=0,\n                 partition_shuffles=1,\n                 loss_callback=None,\n                 port=5000):\n        self.tensorflowGraph = tensorflowGraph\n        self.iters = iters\n        self.tfInput = tfInput\n        self.tfLabel = tfLabel\n        self.acquire_lock = acquire_lock\n        graph = tf.MetaGraphDef()\n        metagraph = json_format.Parse(tensorflowGraph, graph)\n        self.start_server(metagraph, optimizer, port)\n        #allow server to start up on separate thread\n        time.sleep(serverStartup)\n        self.mini_batch = mini_batch\n        self.mini_stochastic_iters = mini_stochastic_iters\n        self.verbose = verbose\n        self.shuffle = shuffle\n        self.partition_shuffles= partition_shuffles\n        self.loss_callback = loss_callback\n        self.master_url = master_url if master_url is not None else HogwildSparkModel.determine_master(port)\n        self.port = port\n\n    @staticmethod\n    def determine_master(port):\n        """"""\n        Get the url of the driver node. This is kind of crap on mac.\n        """"""\n        try:\n            master_url = socket.gethostbyname(socket.gethostname()) + \':\' + str(port)\n            return master_url\n        except:\n            return \'localhost:\' + str(port)\n\n    def start_server(self, tg, optimizer, port):\n        """"""\n        Starts the server with a copy of the argument for weird tensorflow multiprocessing issues\n        """"""\n        try:\n            multiprocessing.set_start_method(\'spawn\')\n        except Exception as e:\n            pass\n        self.server = Process(target=self.start_service, args=(tg, optimizer, port))\n        self.server.daemon = True\n        self.server.start()\n\n    def stop_server(self):\n        """"""\n        Needs to get called when training is done\n        """"""\n        self.server.terminate()\n        self.server.join()\n\n    def start_service(self, metagraph, optimizer, port):\n        """"""\n        Asynchronous flask service. This may be a bit confusing why the server starts here and not init.\n        It is basically because this is ran in a separate process, and when python call fork, we want to fork from this\n        thread and not the master thread\n        """"""\n        app = Flask(__name__)\n        self.app = app\n        max_errors = self.iters\n        lock = RWLock()\n\n        server = tf.train.Server.create_local_server()\n        new_graph = tf.Graph()\n        with new_graph.as_default():\n            tf.train.import_meta_graph(metagraph)\n            loss_variable = tf.get_collection(tf.GraphKeys.LOSSES)[0]\n            trainable_variables = tf.trainable_variables()\n            grads = tf.gradients(loss_variable, trainable_variables)\n            grads = list(zip(grads, trainable_variables))\n            train_op = optimizer.apply_gradients(grads)\n            init = tf.global_variables_initializer()\n\n        glob_session = tf.Session(server.target, graph=new_graph)\n        with new_graph.as_default():\n            with glob_session.as_default():\n                glob_session.run(init)\n                self.weights = tensorflow_get_weights(trainable_variables)\n\n        cont = itertools.count()\n        lock_acquired = self.acquire_lock\n\n        @app.route(\'/\')\n        def home():\n            return \'Lifeomic\'\n\n        @app.route(\'/parameters\', methods=[\'GET\'])\n        def get_parameters():\n            if lock_acquired:\n                lock.acquire_read()\n            vs = pickle.dumps(self.weights)\n            if lock_acquired:\n                lock.release()\n            return vs\n\n        @app.route(\'/update\', methods=[\'POST\'])\n        def update_parameters():\n            with new_graph.as_default():\n                gradients = pickle.loads(request.data)\n                nu_feed = {}\n                for x, grad_var in enumerate(grads):\n                    nu_feed[grad_var[0]] = gradients[x]\n\n                if lock_acquired:\n                    lock.acquire_write()\n\n                with glob_session.as_default():\n                    try:\n                        glob_session.run(train_op, feed_dict=nu_feed)\n                        self.weights = tensorflow_get_weights(trainable_variables)\n                    except:\n                        error_cnt = cont.next()\n                        if error_cnt >= max_errors:\n                            raise Exception(""Too many failures during training"")\n                    finally:\n                        if lock_acquired:\n                            lock.release()\n\n            return \'completed\'\n\n        self.app.run(host=\'0.0.0.0\', use_reloader=False, threaded=True, port=port)\n\n    def train(self, rdd):\n        try:\n            tgraph = self.tensorflowGraph\n            tfInput = self.tfInput\n            tfLabel = self.tfLabel\n            master_url = self.master_url\n            iters = self.iters\n            mbs = self.mini_batch\n            msi = self.mini_stochastic_iters\n            cb = self.loss_callback\n            verbose = self.verbose\n            shuffle = self.shuffle\n            for i in range(self.partition_shuffles):\n                rdd.foreachPartition(lambda x: handle_model(x, tgraph, tfInput,\n                                                            tfLabel=tfLabel, master_url=master_url,\n                                                            iters=iters, mini_batch_size=mbs, shuffle=shuffle,\n                                                            mini_stochastic_iters=msi, verbose=verbose,\n                                                            loss_callback=cb))\n                if self.partition_shuffles - i > 1:\n                    num_partitions = rdd.getNumPartitions()\n                    rdd = rdd.repartition(num_partitions)\n            server_weights = get_server_weights(master_url)\n            self.stop_server()\n            return server_weights\n        except Exception as e:\n            self.stop_server()\n            raise e\n\n'"
sparkflow/RWLock.py,0,"b'""""""Simple reader-writer locks in Python\nMany readers can hold the lock XOR one and only one writer\nhttp://majid.info/blog/a-reader-writer-lock-for-python/\n""""""\nimport threading\n\nversion = """"""$Id: 04-1.html,v 1.3 2006/12/05 17:45:12 majid Exp $""""""\n\n\nclass RWLock:\n    """"""\n    A simple reader-writer lock Several readers can hold the lock\n    simultaneously, XOR one writer. Write locks have priority over reads to\n    prevent write starvation.\n    """"""\n    def __init__(self):\n        self.rwlock = 0\n        self.writers_waiting = 0\n        self.monitor = threading.Lock()\n        self.readers_ok = threading.Condition(self.monitor)\n        self.writers_ok = threading.Condition(self.monitor)\n\n    def acquire_read(self):\n        """"""\n        Acquire a read lock. Several threads can hold this typeof lock.\n        It is exclusive with write locks.\n        """"""\n        self.monitor.acquire()\n        while self.rwlock < 0 or self.writers_waiting:\n            self.readers_ok.wait()\n        self.rwlock += 1\n        self.monitor.release()\n\n    def acquire_write(self):\n        """"""\n        Acquire a write lock. Only one thread can hold this lock, and\n        only when no read locks are also held.\n        """"""\n        self.monitor.acquire()\n        while self.rwlock != 0:\n            self.writers_waiting += 1\n            self.writers_ok.wait()\n            self.writers_waiting -= 1\n        self.rwlock = -1\n        self.monitor.release()\n\n    def release(self):\n        """"""\n        Release a lock, whether read or write.\n        """"""\n        self.monitor.acquire()\n        if self.rwlock < 0:\n            self.rwlock = 0\n        else:\n            self.rwlock -= 1\n        wake_writers = self.writers_waiting and self.rwlock == 0\n        wake_readers = self.writers_waiting == 0\n        self.monitor.release()\n        if wake_writers:\n            self.writers_ok.acquire()\n            self.writers_ok.notify()\n            self.writers_ok.release()\n        elif wake_readers:\n            self.readers_ok.acquire()\n            self.readers_ok.notifyAll()\n            self.readers_ok.release()'"
sparkflow/__init__.py,0,b''
sparkflow/graph_utils.py,2,"b'import tensorflow as tf\nfrom google.protobuf import json_format\nimport json\n\n\ndef build_graph(func):\n    """"""\n    :param func: Function that includes tensorflow graph\n    :return json version of graph\n    """"""\n    first_graph = tf.Graph()\n    with first_graph.as_default() as g:\n        v = func()\n        mg = json_format.MessageToJson(tf.train.export_meta_graph())\n    return mg\n\n\ndef generate_config(**kwargs):\n    return json.dumps(kwargs)\n\n\ndef build_adam_config(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, use_locking=False):\n    return generate_config(learning_rate=learning_rate, beta1=beta1,\n                           beta2=beta2, epsilon=epsilon, use_locking=use_locking)\n\n\ndef build_rmsprop_config(learning_rate=0.001, decay=0.9,\n                         momentum=0.0, epsilon=1e-10, use_locking=False, centered=False):\n    return generate_config(learning_rate=learning_rate, decay=decay, momentum=momentum,\n                           epsilon=epsilon, use_locking=use_locking, centered=centered)\n\n\ndef build_momentum_config(learning_rate=0.001, momentum=0.9, use_locking=False, use_nesterov=False):\n    return generate_config(learning_rate=learning_rate, momentum=momentum,\n                           use_locking=use_locking, use_nesterov=use_nesterov)\n\n\ndef build_adadelta_config(learning_rate=0.001, rho=0.95, epsilon=1e-8, use_locking=False):\n    return generate_config(learning_rate=learning_rate, rho=rho, epsilon=epsilon, use_locking=use_locking)\n\n\ndef build_adagrad_config(learning_rate=0.001, initial_accumulator=0.1, use_locking=False):\n    return generate_config(learning_rate=learning_rate, initial_accumulator=initial_accumulator, use_locking=use_locking)\n\n\ndef build_gradient_descent(learning_rate=0.001, use_locking=False):\n    return generate_config(learning_rate=learning_rate, use_locking=use_locking)\n'"
sparkflow/ml_util.py,11,"b'import numpy as np\nimport tensorflow as tf\nimport json\nfrom google.protobuf import json_format\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql import Row\n\n\ndef tensorflow_get_weights(vs=None):\n    if not vs:\n        vs = tf.trainable_variables()\n    values = tf.get_default_session().run(vs)\n    return values\n\n\ndef tensorflow_set_weights(weights, vs=None):\n    assign_ops = []\n    feed_dict = {}\n    if not vs:\n        vs = tf.trainable_variables()\n    zipped_values = zip(vs, weights)\n    for var, value in zipped_values:\n        value = np.asarray(value)\n        assign_placeholder = tf.placeholder(var.dtype, shape=value.shape)\n        assign_op = var.assign(assign_placeholder)\n        assign_ops.append(assign_op)\n        feed_dict[assign_placeholder] = value\n    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)\n\n\ndef convert_weights_to_json(weights):\n    weights = [w.tolist() for w in weights]\n    weights_list = json.dumps(weights)\n    return weights_list\n\n\ndef convert_json_to_weights(json_weights):\n    loaded_weights = json.loads(json_weights)\n    loaded_weights = [np.asarray(x) for x in loaded_weights]\n    return loaded_weights\n\n\ndef calculate_weights(collected_weights):\n    size = len(collected_weights)\n    start = collected_weights[0]\n    if size > 1:\n        for i in range(1, size):\n            vs = collected_weights[i]\n            for x in range(0, len(vs)):\n                start[x] = start[x] + vs[x]\n    return [x / size for x in start]\n\n\ndef predict_func(rows, graph_json, prediction, graph_weights, inp, activation, tf_input, tf_dropout=None, to_keep_dropout=False):\n    rows = [r.asDict() for r in rows]\n    if len(rows) > 0:\n        graph = tf.MetaGraphDef()\n        graph = json_format.Parse(graph_json, graph)\n        loaded_weights = json.loads(graph_weights)\n        loaded_weights = [np.asarray(x) for x in loaded_weights]\n\n        A = [np.asarray(row[inp]) for row in rows]\n\n        new_graph = tf.Graph()\n        with tf.Session(graph=new_graph) as sess:\n            tf.train.import_meta_graph(graph)\n            sess.run(tf.global_variables_initializer())\n            tensorflow_set_weights(loaded_weights)\n            out_node = tf.get_default_graph().get_tensor_by_name(activation)\n            dropout_v = 1.0 if tf_dropout is not None and to_keep_dropout else 0.0\n            feed_dict = {tf_input: A} if tf_dropout is None else {tf_input: A, tf_dropout: dropout_v}\n\n            pred = sess.run(out_node, feed_dict=feed_dict)\n            for i in range(0, len(rows)):\n                row = rows[i]\n                try:\n                    # Vectors Dense are handled differently in python 3\n                    internal = float(pred[i])\n                    row[prediction] = internal\n                except:\n                    row[prediction] = Vectors.dense(pred[i])\n        return [Row(**a) for a in rows]\n    return []\n\n\ndef handle_features(data, is_supervised=False):\n    features = []\n    labels = []\n    for feature in data:\n        if is_supervised:\n            x,y = feature\n            if type(y) is int or type(y) is float:\n                labels.append([y])\n            else:\n                labels.append(y)\n        else:\n            x = feature\n        features.append(x)\n    features = np.asarray(features)\n    labels = np.asarray(labels) if is_supervised else None\n    return features, labels\n\n\ndef handle_feed_dict(train, tfInput, tfLabel=None, labels=None, mini_batch_size=-1, idx=None):\n    if mini_batch_size > train.shape[0]:\n        mini_batch_size = train.shape[0]-1\n\n    if mini_batch_size <= 0:\n        if tfLabel == None:\n            return {tfInput: train}\n        else:\n            return {tfInput: train, tfLabel: labels}\n\n    if idx is not None:\n        train_data = train[idx:idx+mini_batch_size]\n        if tfLabel == None:\n            return {tfInput: train_data}\n        else:\n            return {tfInput: train_data, tfLabel: labels[idx:idx+mini_batch_size]}\n\n    num_dims = np.random.choice(train.shape[0], mini_batch_size, replace=False)\n    features_mini = train[num_dims]\n\n    if tfLabel == None:\n        return {tfInput: features_mini}\n    else:\n        return {tfInput: features_mini, tfLabel: labels[num_dims]}\n\n\ndef handle_shuffle(features, labels):\n    shuff_idxs = np.random.choice(features.shape[0], features.shape[0], replace=False)\n    features = features[shuff_idxs]\n    labels = labels[shuff_idxs] if labels is not None else None\n    return features, labels\n\n'"
sparkflow/pipeline_util.py,0,"b'import dill\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.util import JavaMLReader, JavaMLWriter\nfrom pyspark.ml.feature import StopWordsRemover\nfrom pyspark.ml.wrapper import JavaParams\nfrom pyspark.context import SparkContext\nimport zlib\nimport sys\n\n""""""\nbased off below stackoverflow thread. Changes were made for performance.\ncredit: https://stackoverflow.com/questions/41399399/serialize-a-custom-transformer-using-python-to-be-used-within-a-pyspark-ml-pipel\n""""""\n\n\nclass PysparkObjId(object):\n    """"""\n    A class to specify constants used to idenify and setup python\n    Estimators, Transformers and Models so they can be serialized on there\n    own and from within a Pipline or PipelineModel.\n    """"""\n    def __init__(self):\n        super(PysparkObjId, self).__init__()\n\n    @staticmethod\n    def _getPyObjId():\n        return \'4c1740b00d3c4ff6806a1402321572cb\'\n\n    @staticmethod\n    def _getCarrierClass(javaName=False):\n        return \'org.apache.spark.ml.feature.StopWordsRemover\' if javaName else StopWordsRemover\n\n\ndef load_byte_array(stop_words):\n    swords = stop_words[0].split(\',\')[0:-1]\n    if sys.version_info[0] < 3:\n        lst = [chr(int(d)) for d in swords]\n        dmp = \'\'.join(lst)\n        dmp = zlib.decompress(dmp)\n        py_obj = dill.loads(dmp)\n        return py_obj\n    dmp = bytes([int(i) for i in swords])\n    dmp = zlib.decompress(dmp)\n    py_obj = dill.loads(dmp)\n    return py_obj\n\n\nclass PysparkPipelineWrapper(object):\n    """"""\n    A class to facilitate converting the stages of a Pipeline or PipelineModel\n    that were saved from PysparkReaderWriter.\n    """"""\n    def __init__(self):\n        super(PysparkPipelineWrapper, self).__init__()\n\n    @staticmethod\n    def unwrap(pipeline):\n        if not (isinstance(pipeline, Pipeline) or isinstance(pipeline, PipelineModel)):\n            raise TypeError(""Cannot recognize a pipeline of type %s."" % type(pipeline))\n\n        stages = pipeline.getStages() if isinstance(pipeline, Pipeline) else pipeline.stages\n        for i, stage in enumerate(stages):\n            if (isinstance(stage, Pipeline) or isinstance(stage, PipelineModel)):\n                stages[i] = PysparkPipelineWrapper.unwrap(stage)\n            if isinstance(stage, PysparkObjId._getCarrierClass()) and stage.getStopWords()[-1] == PysparkObjId._getPyObjId():\n                swords = stage.getStopWords()[:-1] # strip the id\n                py_obj = load_byte_array(swords)\n                stages[i] = py_obj\n\n        if isinstance(pipeline, Pipeline):\n            pipeline.setStages(stages)\n        else:\n            pipeline.stages = stages\n        return pipeline\n\n\nclass PysparkReaderWriter(object):\n    """"""\n    A mixin class so custom pyspark Estimators, Transformers and Models may\n    support saving and loading directly or be saved within a Pipline or PipelineModel.\n    """"""\n    def __init__(self):\n        super(PysparkReaderWriter, self).__init__()\n\n    def write(self):\n        """"""Returns an MLWriter instance for this ML instance.""""""\n        return JavaMLWriter(self)\n\n    @classmethod\n    def read(cls):\n        """"""Returns an MLReader instance for our clarrier class.""""""\n        return JavaMLReader(PysparkObjId._getCarrierClass())\n\n    @classmethod\n    def load(cls, path):\n        """"""Reads an ML instance from the input path, a shortcut of `read().load(path)`.""""""\n        swr_java_obj = cls.read().load(path)\n        return cls._from_java(swr_java_obj)\n\n    @classmethod\n    def _from_java(cls, java_obj):\n        """"""\n        Get the dumby the stopwords that are the characters of the dills dump plus our guid\n        and convert, via dill, back to our python instance.\n        """"""\n        swords = java_obj.getStopWords()[:-1] # strip the id\n        return load_byte_array(swords)\n\n    def _to_java(self):\n        """"""\n        Convert this instance to a dill dump, then to a list of strings with the unicode integer values of each character.\n        Use this list as a set of dumby stopwords and store in a StopWordsRemover instance\n        :return: Java object equivalent to this instance.\n        """"""\n        dmp = dill.dumps(self)\n        dmp = zlib.compress(dmp)\n        sc = SparkContext._active_spark_context\n        pylist = [str(i) + \',\' for i in bytearray(dmp)]\n        # convert bytes to string integer list\n        pylist = [\'\'.join(pylist)]\n        pylist.append(PysparkObjId._getPyObjId()) # add our id so PysparkPipelineWrapper can id us.\n        java_class = sc._gateway.jvm.java.lang.String\n        java_array = sc._gateway.new_array(java_class, len(pylist))\n        java_array[0:2] = pylist[0:2]\n        _java_obj = JavaParams._new_java_obj(PysparkObjId._getCarrierClass(javaName=True), self.uid)\n        _java_obj.setStopWords(java_array)\n        return _java_obj\n\n'"
sparkflow/tensorflow_async.py,10,"b'import tensorflow as tf\nfrom sparkflow.pipeline_util import PysparkReaderWriter\nimport numpy as np\n\nfrom pyspark.ml.param import Param, Params, TypeConverters\nfrom pyspark.ml.param.shared import HasInputCol, HasPredictionCol, HasLabelCol\nfrom pyspark.ml.base import Estimator\nfrom pyspark.ml import Model\nfrom pyspark.ml.util import Identifiable, MLReadable, MLWritable\nfrom pyspark import keyword_only\nfrom sparkflow.HogwildSparkModel import HogwildSparkModel\nfrom sparkflow.ml_util import convert_weights_to_json, predict_func\nfrom pyspark import SparkContext\nimport json\n\n\ndef build_optimizer(optimizer_name, learning_rate, optimizer_options):\n\n    available_optimizers = {\n        \'adam\': tf.train.AdamOptimizer,\n        \'rmsprop\': tf.train.RMSPropOptimizer,\n        \'momentum\': tf.train.MomentumOptimizer,\n        \'adadelta\': tf.train.AdadeltaOptimizer,\n        \'adagrad\': tf.train.AdagradOptimizer,\n        \'gradient_descent\': tf.train.GradientDescentOptimizer,\n        \'adagrad_da\': tf.train.AdagradDAOptimizer,\n        \'ftrl\': tf.train.FtrlOptimizer,\n        \'proximal_adagrad\': tf.train.ProximalAdagradOptimizer,\n        \'proximal_gradient_descent\': tf.train.ProximalGradientDescentOptimizer\n    }\n\n    if optimizer_options is None:\n        optimizer_options = {\n            ""learning_rate"": learning_rate,\n            ""use_locking"": False\n        }\n        if optimizer_name == \'momentum\':\n            optimizer_options[\'momentum\'] = 0.9\n\n    if optimizer_name in available_optimizers:\n        return available_optimizers[optimizer_name](**optimizer_options)\n    return available_optimizers[\'gradient_descent\'](**optimizer_options)\n\n\ndef handle_data(data, inp_col, label_col):\n    if label_col is None:\n        return np.asarray(data[inp_col])\n    return np.asarray(data[inp_col]), data[label_col]\n\n\nclass SparkAsyncDLModel(Model, HasInputCol, HasPredictionCol, PysparkReaderWriter, MLReadable, MLWritable, Identifiable):\n\n    modelJson = Param(Params._dummy(), ""modelJson"", """", typeConverter=TypeConverters.toString)\n    modelWeights = Param(Params._dummy(), ""modelWeights"", """", typeConverter=TypeConverters.toString)\n    tfOutput = Param(Params._dummy(), ""tfOutput"", """", typeConverter=TypeConverters.toString)\n    tfInput = Param(Params._dummy(), ""tfInput"", """", typeConverter=TypeConverters.toString)\n    tfDropout = Param(Params._dummy(), ""tfDropout"", """", typeConverter=TypeConverters.toString)\n    toKeepDropout = Param(Params._dummy(), ""toKeepDropout"", """", typeConverter=TypeConverters.toBoolean)\n\n    @keyword_only\n    def __init__(self,\n                 inputCol=None,\n                 modelJson=None,\n                 modelWeights=None,\n                 tfInput=None,\n                 tfOutput=None,\n                 tfDropout=None,\n                 toKeepDropout=None,\n                 predictionCol=None):\n        super(SparkAsyncDLModel, self).__init__()\n        self._setDefault(modelJson=None,  inputCol=\'encoded\',\n                         predictionCol=\'predicted\', tfOutput=None, tfInput=None,\n                         modelWeights=None, tfDropout=None, toKeepDropout=False)\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n\n    @keyword_only\n    def setParams(self,\n                  inputCol=None,\n                  modelJson=None,\n                  modelWeights=None,\n                  tfInput=None,\n                  tfOutput=None,\n                  tfDropout=None,\n                  toKeepDropout=None,\n                  predictionCol=None):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n    def _transform(self, dataset):\n        inp = self.getOrDefault(self.inputCol)\n        out = self.getOrDefault(self.predictionCol)\n        mod_json = self.getOrDefault(self.modelJson)\n        mod_weights = self.getOrDefault(self.modelWeights)\n        tf_input = self.getOrDefault(self.tfInput)\n        tf_output = self.getOrDefault(self.tfOutput)\n        tf_dropout = self.getOrDefault(self.tfDropout)\n        to_keep_dropout = self.getOrDefault(self.toKeepDropout)\n        return dataset.rdd.mapPartitions(lambda x: predict_func(x, mod_json, out, mod_weights, inp, tf_output, tf_input, tf_dropout, to_keep_dropout)).toDF()\n\n\nclass SparkAsyncDL(Estimator, HasInputCol, HasPredictionCol, HasLabelCol,PysparkReaderWriter, MLReadable, MLWritable, Identifiable):\n\n    tensorflowGraph = Param(Params._dummy(), ""tensorflowGraph"", """", typeConverter=TypeConverters.toString)\n    tfInput = Param(Params._dummy(), ""tfInput"", """", typeConverter=TypeConverters.toString)\n    tfOutput = Param(Params._dummy(), ""tfOutput"", """", typeConverter=TypeConverters.toString)\n    tfLabel = Param(Params._dummy(), ""tfLabel"", """", typeConverter=TypeConverters.toString)\n    tfOptimizer = Param(Params._dummy(), ""tfOptimizer"", """", typeConverter=TypeConverters.toString)\n    tfLearningRate = Param(Params._dummy(), ""tfLearningRate"", """", typeConverter=TypeConverters.toFloat)\n    iters = Param(Params._dummy(), ""iters"", """", typeConverter=TypeConverters.toInt)\n    partitions = Param(Params._dummy(), ""partitions"", """", typeConverter=TypeConverters.toInt)\n    miniBatchSize = Param(Params._dummy(), ""miniBatchSize"", """", typeConverter=TypeConverters.toInt)\n    miniStochasticIters = Param(Params._dummy(), ""miniStochasticIters"", """", typeConverter=TypeConverters.toInt)\n    verbose = Param(Params._dummy(), ""verbose"", """", typeConverter=TypeConverters.toInt)\n    acquireLock = Param(Params._dummy(), ""acquireLock"", """", typeConverter=TypeConverters.toBoolean)\n    shufflePerIter = Param(Params._dummy(), ""shufflePerIter"", """", typeConverter=TypeConverters.toBoolean)\n    tfDropout = Param(Params._dummy(), ""tfDropout"", """", typeConverter=TypeConverters.toString)\n    toKeepDropout = Param(Params._dummy(), ""toKeepDropout"", """", typeConverter=TypeConverters.toBoolean)\n    partitionShuffles = Param(Params._dummy(), ""partitionShuffles"", """", typeConverter=TypeConverters.toInt)\n    optimizerOptions = Param(Params._dummy(), ""optimizerOptions"", """", typeConverter=TypeConverters.toString)\n    port = Param(Params._dummy(), ""port"", """", typeConverter=TypeConverters.toInt)\n\n    @keyword_only\n    def __init__(self,\n                 inputCol=None,\n                 tensorflowGraph=None,\n                 tfInput=None,\n                 tfLabel=None,\n                 tfOutput=None,\n                 tfOptimizer=None,\n                 tfLearningRate=None,\n                 iters=None,\n                 predictionCol=None,\n                 partitions=None,\n                 miniBatchSize = None,\n                 miniStochasticIters=None,\n                 acquireLock=None,\n                 shufflePerIter=None,\n                 tfDropout=None,\n                 toKeepDropout=None,\n                 verbose=None,\n                 labelCol=None,\n                 partitionShuffles=None,\n                 optimizerOptions=None,\n                 port=None):\n        """"""\n        :param inputCol: Spark dataframe inputCol. Similar to other spark ml inputCols\n        :param tensorflowGraph: The protobuf tensorflow graph. You can use the utility function in graph_utils\n        to generate the graph for you\n        :param tfInput: The tensorflow input. This points us to the input variable name that you would like to use\n        for training\n        :param tfLabel: The tensorflow label. This is the variable name for the label.\n        :param tfOutput: The tensorflow raw output. This is for your loss function.\n        :param tfOptimizer: The optimization function you would like to use for training. Defaults to adam\n        :param tfLearningRate: Learning rate of the optimization function\n        :param iters: number of iterations of training\n        :param predictionCol: The prediction column name on the spark dataframe for transformations\n        :param partitions: Number of partitions to use for training (recommended on partition per instance)\n        :param miniBatchSize: size of the mini batch. A size of -1 means train on all rows\n        :param miniStochasticIters: If using a mini batch, you can choose number of mini iters you would like to do with the\n        batch size above per epoch. A value of -1 means that you would like to run mini-batches on all data in the partition\n        :param acquireLock: If you do not want to utilize hogwild training, this will set a lock\n        :param shufflePerIter: Specifies if you want to shuffle the features after each iteration\n        :param tfDropout: Specifies the dropout variable. This is important for predictions\n        :param toKeepDropout: Due to conflicting TF implementations, this specifies whether the dropout function means\n        to keep a percentage of values or to drop a percentage of values.\n        :param verbose: Specifies log level of training results\n        :param labelCol: Label column for training\n        :param partitionShuffles: This will shuffle your data after iterations are completed, then run again. For example,\n        if you have 2 partition shuffles and 100 iterations, it will run 100 iterations then reshuffle and run 100 iterations again.\n        The repartition hits performance and should be used with care.\n        :param optimizerOptions: Json options to apply to tensorflow optimizers.\n        :param port: Flask Port\n        """"""\n        super(SparkAsyncDL, self).__init__()\n        self._setDefault(inputCol=\'transformed\', tensorflowGraph=\'\',\n                         tfInput=\'x:0\', tfLabel=None, tfOutput=\'out/Sigmoid:0\',\n                         tfOptimizer=\'adam\', tfLearningRate=.01, partitions=5,\n                         miniBatchSize=128, miniStochasticIters=-1,\n                         shufflePerIter=True, tfDropout=None, acquireLock=False, verbose=0,\n                         iters=1000, toKeepDropout=False, predictionCol=\'predicted\', labelCol=None,\n                         partitionShuffles=1, optimizerOptions=None, port=5000)\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n\n    @keyword_only\n    def setParams(self,\n                  inputCol=None,\n                  tensorflowGraph=None,\n                  tfInput=None,\n                  tfLabel=None,\n                  tfOutput=None,\n                  tfOptimizer=None,\n                  tfLearningRate=None,\n                  iters=None,\n                  predictionCol=None,\n                  partitions=None,\n                  miniBatchSize = None,\n                  miniStochasticIters=None,\n                  acquireLock=None,\n                  shufflePerIter=None,\n                  tfDropout=None,\n                  toKeepDropout=None,\n                  verbose=None,\n                  labelCol=None,\n                  partitionShuffles=None,\n                  optimizerOptions=None,\n                  port=None):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n    def getTensorflowGraph(self):\n        return self.getOrDefault(self.tensorflowGraph)\n\n    def getIters(self):\n        return self.getOrDefault(self.iters)\n\n    def getTfInput(self):\n        return self.getOrDefault(self.tfInput)\n\n    def getTfLabel(self):\n        return self.getOrDefault(self.tfLabel)\n\n    def getTfOutput(self):\n        return self.getOrDefault(self.tfOutput)\n\n    def getTfOptimizer(self):\n        return self.getOrDefault(self.tfOptimizer)\n\n    def getTfLearningRate(self):\n        return self.getOrDefault(self.tfLearningRate)\n\n    def getPartitions(self):\n        return self.getOrDefault(self.partitions)\n\n    def getMiniBatchSize(self):\n        return self.getOrDefault(self.miniBatchSize)\n\n    def getMiniStochasticIters(self):\n        return self.getOrDefault(self.miniStochasticIters)\n\n    def getVerbose(self):\n        return self.getOrDefault(self.verbose)\n\n    def getAqcuireLock(self):\n        return self.getOrDefault(self.acquireLock)\n\n    def getShufflePerIter(self):\n        return self.getOrDefault(self.shufflePerIter)\n\n    def getTfDropout(self):\n        return self.getOrDefault(self.tfDropout)\n\n    def getToKeepDropout(self):\n        return self.getOrDefault(self.toKeepDropout)\n\n    def getPartitionShuffles(self):\n        return self.getOrDefault(self.partitionShuffles)\n\n    def getOptimizerOptions(self):\n        return self.getOrDefault(self.optimizerOptions)\n\n    def getPort(self):\n        return self.getOrDefault(self.port)\n\n    def _fit(self, dataset):\n        inp_col = self.getInputCol()\n        graph_json = self.getTensorflowGraph()\n        iters = self.getIters()\n        label = self.getLabelCol()\n        prediction = self.getPredictionCol()\n        tf_input = self.getTfInput()\n        tf_label = self.getTfLabel()\n        tf_output = self.getTfOutput()\n        optimizer_options = self.getOptimizerOptions()\n        if optimizer_options is not None:\n            optimizer_options = json.loads(optimizer_options)\n        tf_optimizer = build_optimizer(self.getTfOptimizer(), self.getTfLearningRate(), optimizer_options)\n        partitions = self.getPartitions()\n        acquire_lock = self.getAqcuireLock()\n        mbs = self.getMiniBatchSize()\n        msi = self.getMiniStochasticIters()\n        verbose = self.getVerbose()\n        spi = self.getShufflePerIter()\n        tf_dropout = self.getTfDropout()\n        to_keep_dropout = self.getToKeepDropout()\n        partition_shuffles = self.getPartitionShuffles()\n        port = self.getPort()\n\n        df = dataset.rdd.map(lambda x: handle_data(x, inp_col, label))\n        df = df.coalesce(partitions) if partitions < df.getNumPartitions() else df\n\n        spark_model = HogwildSparkModel(\n            tensorflowGraph=graph_json,\n            iters=iters,\n            tfInput=tf_input,\n            tfLabel=tf_label,\n            optimizer=tf_optimizer,\n            master_url=SparkContext._active_spark_context.getConf().get(""spark.driver.host"").__str__() + "":"" + str(port),\n            acquire_lock=acquire_lock,\n            mini_batch=mbs,\n            mini_stochastic_iters=msi,\n            shuffle=spi,\n            verbose=verbose,\n            partition_shuffles=partition_shuffles,\n            port=port\n        )\n\n        weights = spark_model.train(df)\n        json_weights = convert_weights_to_json(weights)\n\n        return SparkAsyncDLModel(\n            inputCol=inp_col,\n            modelJson=graph_json,\n            modelWeights=json_weights,\n            tfOutput=tf_output,\n            tfInput=tf_input,\n            tfDropout=tf_dropout,\n            toKeepDropout=to_keep_dropout,\n            predictionCol=prediction\n        )\n'"
sparkflow/tensorflow_model_loader.py,6,"b'import tensorflow as tf\nfrom sparkflow.tensorflow_async import SparkAsyncDLModel\nfrom google.protobuf import json_format\nfrom pyspark.ml.pipeline import PipelineModel\nimport json\n\n\ndef load_tensorflow_model(\n        path,\n        inputCol,\n        tfInput,\n        tfOutput,\n        predictionCol=\'predicted\',\n        tfDropout=None,\n        toKeepDropout=False):\n    with tf.Session(graph=tf.Graph()) as sess:\n        new_saver = tf.train.import_meta_graph(path + \'.meta\')\n        split = path.split(\'/\')\n        if len(split) > 1:\n            new_saver.restore(sess, tf.train.latest_checkpoint(""/"".join(split[:-1])))\n        else:\n            new_saver.restore(sess, tf.train.latest_checkpoint(split[0]))\n        vs = tf.trainable_variables()\n        weights = sess.run(vs)\n        json_graph = json_format.MessageToJson(tf.train.export_meta_graph())\n\n    weights = [w.tolist() for w in weights]\n    json_weights = json.dumps(weights)\n    return SparkAsyncDLModel(\n        inputCol=inputCol, modelJson=json_graph, modelWeights=json_weights,\n        tfInput=tfInput, tfOutput=tfOutput, predictionCol=predictionCol, tfDropout=tfDropout, toKeepDropout=toKeepDropout\n    )\n\n\ndef attach_tensorflow_model_to_pipeline(\n        path,\n        pipelineModel,\n        inputCol,\n        tfInput,\n        tfOutput,\n        predictionCol=\'predicted\',\n        tfDropout=None,\n        toKeepDropout=False ):\n    spark_model = load_tensorflow_model(path, inputCol, tfInput, tfOutput, predictionCol, tfDropout, toKeepDropout)\n    return PipelineModel(stages=[pipelineModel, spark_model])\n'"
tests/dl_runner.py,21,"b'from pyspark.sql import SparkSession\nimport tensorflow as tf\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.pipeline import Pipeline\nfrom sparkflow.pipeline_util import PysparkPipelineWrapper\nfrom pyspark.ml.pipeline import PipelineModel\nimport numpy as np\nfrom google.protobuf import json_format\nimport random\nfrom sparkflow.tensorflow_async import SparkAsyncDL, SparkAsyncDLModel\nfrom sparkflow.HogwildSparkModel import HogwildSparkModel\nfrom sparkflow.graph_utils import build_graph, build_adam_config, build_rmsprop_config\nimport unittest\nimport logging\n\nrandom.seed(12345)\n\n\nclass PysparkTest(unittest.TestCase):\n\n    @classmethod\n    def suppress_logging(cls):\n        logger = logging.getLogger(\'py4j\')\n        logger.setLevel(logging.WARN)\n\n    @classmethod\n    def create_testing_spark_session(cls):\n        return (SparkSession.builder\n                .master(\'local[2]\')\n                .appName(\'sparkflow\')\n                .getOrCreate())\n\n    @classmethod\n    def setUpClass(cls):\n        cls.suppress_logging()\n        cls.spark = cls.create_testing_spark_session()\n\n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n\n\nclass SparkFlowTests(PysparkTest):\n\n    @staticmethod\n    def create_model():\n        x = tf.placeholder(tf.float32, shape=[None, 2], name=\'x\')\n        layer1 = tf.layers.dense(x, 12, activation=tf.nn.relu)\n        layer2 = tf.layers.dense(layer1, 7, activation=tf.nn.relu)\n        out = tf.layers.dense(layer2, 1, name=\'outer\', activation=tf.nn.sigmoid)\n        y = tf.placeholder(tf.float32, shape=[None, 1], name=\'y\')\n        loss = tf.losses.mean_squared_error(y, out)\n        return loss\n\n    @staticmethod\n    def create_random_model():\n        x = tf.placeholder(tf.float32, shape=[None, 10], name=\'x\')\n        layer1 = tf.layers.dense(x, 12, activation=tf.nn.relu)\n        layer2 = tf.layers.dense(layer1, 7, activation=tf.nn.relu)\n        out = tf.layers.dense(layer2, 1, name=\'outer\', activation=tf.nn.sigmoid)\n        y = tf.placeholder(tf.float32, shape=[None, 1], name=\'y\')\n        loss = tf.losses.mean_squared_error(y, out)\n        return loss\n\n    @staticmethod\n    def create_autoencoder():\n        x = tf.placeholder(tf.float32, shape=[None, 10], name=\'x\')\n        encoder = tf.layers.dense(x, 5, activation=tf.nn.relu)\n        bottle_neck = tf.layers.dense(encoder, 2, activation=tf.nn.sigmoid, name=\'out\')\n        decoder = tf.layers.dense(bottle_neck, 5, activation=tf.nn.relu)\n        reconstructed = tf.layers.dense(decoder, 10)\n        loss = tf.losses.mean_squared_error(x, reconstructed)\n        return loss\n\n    @staticmethod\n    def calculate_errors(data):\n        nb_errors = 0\n        for d in data:\n            lab = d[\'label\']\n            predicted = 1 if d[\'predicted\'] >= 0.5 else 0\n            if predicted != lab:\n                nb_errors += 1\n        return nb_errors\n\n    def handle_assertions(self, spark_model, processed):\n        data = spark_model.fit(processed).transform(processed).take(10)\n        nb_errors = SparkFlowTests.calculate_errors(data)\n        self.assertTrue(nb_errors < len(data))\n\n    def generate_random_data(self):\n        dat = [(1.0, Vectors.dense(np.random.normal(0,1,10))) for _ in range(0, 200)]\n        dat2 = [(0.0, Vectors.dense(np.random.normal(2,1,10))) for _ in range(0, 200)]\n        dat.extend(dat2)\n        random.shuffle(dat)\n        return self.spark.createDataFrame(dat, [""label"", ""features""])\n\n    def test_save_model(self):\n        processed = self.generate_random_data()\n        mg = build_graph(SparkFlowTests.create_random_model)\n        spark_model = SparkAsyncDL(\n            inputCol=\'features\',\n            tensorflowGraph=mg,\n            tfInput=\'x:0\',\n            tfLabel=\'y:0\',\n            tfOutput=\'outer/Sigmoid:0\',\n            tfOptimizer=\'adam\',\n            tfLearningRate=.1,\n            iters=20,\n            partitions=2,\n            predictionCol=\'predicted\',\n            labelCol=\'label\'\n        )\n        fitted = spark_model.fit(processed)\n        fitted.save(\'saved_model\')\n        model = SparkAsyncDLModel.load(""saved_model"")\n        data = model.transform(processed).take(10)\n        nb_errors = SparkFlowTests.calculate_errors(data)\n        self.assertTrue(nb_errors < len(data))\n\n    def test_save_pipeline(self):\n        processed = self.generate_random_data()\n        mg = build_graph(SparkFlowTests.create_random_model)\n        spark_model = SparkAsyncDL(\n            inputCol=\'features\',\n            tensorflowGraph=mg,\n            tfInput=\'x:0\',\n            tfLabel=\'y:0\',\n            tfOutput=\'outer/Sigmoid:0\',\n            tfOptimizer=\'adam\',\n            tfLearningRate=.1,\n            iters=20,\n            partitions=2,\n            predictionCol=\'predicted\',\n            labelCol=\'label\'\n        )\n        p = Pipeline(stages=[spark_model]).fit(processed)\n        p.write().overwrite().save(\'example_pipeline\')\n        p = PysparkPipelineWrapper.unwrap(PipelineModel.load(\'example_pipeline\'))\n        data = p.transform(processed).take(10)\n        nb_errors = SparkFlowTests.calculate_errors(data)\n        self.assertTrue(nb_errors < len(data))\n\n    def test_adam_optimizer_options(self):\n        processed = self.generate_random_data()\n        mg = build_graph(SparkFlowTests.create_random_model)\n        options = build_adam_config(learning_rate=0.1, beta1=0.85, beta2=0.98, epsilon=1e-8)\n        spark_model = SparkAsyncDL(\n            inputCol=\'features\',\n            tensorflowGraph=mg,\n            tfInput=\'x:0\',\n            tfLabel=\'y:0\',\n            tfOutput=\'outer/Sigmoid:0\',\n            tfOptimizer=\'adam\',\n            tfLearningRate=.1,\n            iters=25,\n            partitions=2,\n            predictionCol=\'predicted\',\n            labelCol=\'label\',\n            verbose=1,\n            optimizerOptions=options\n        )\n        self.handle_assertions(spark_model, processed)\n\n    def test_small_sparse(self):\n        xor = [(0.0, Vectors.sparse(2,[0,1],[0.0,0.0])),\n               (0.0, Vectors.sparse(2,[0,1],[1.0,1.0])),\n               (1.0, Vectors.sparse(2,[0],[1.0])),\n               (1.0, Vectors.sparse(2,[1],[1.0]))]\n        processed = self.spark.createDataFrame(xor, [""label"", ""features""])\n\n        mg=build_graph(SparkFlowTests.create_model)\n        spark_model = SparkAsyncDL(\n            inputCol=\'features\',\n            tensorflowGraph=mg,\n            tfInput=\'x:0\',\n            tfLabel=\'y:0\',\n            tfOutput=\'outer/Sigmoid:0\',\n            tfOptimizer=\'adam\',\n            tfLearningRate=.1,\n            iters=35,\n            partitions=2,\n            predictionCol=\'predicted\',\n            labelCol=\'label\'\n        )\n        assert spark_model.fit(processed).transform(processed).collect() is not None\n\n    def test_spark_hogwild(self):\n        xor = [(0.0, Vectors.dense(np.array([0.0, 0.0]))),\n               (0.0, Vectors.dense(np.array([1.0, 1.0]))),\n               (1.0, Vectors.dense(np.array([1.0, 0.0]))),\n               (1.0, Vectors.dense(np.array([0.0, 1.0])))]\n        processed = self.spark.createDataFrame(xor, [""label"", ""features""]) \\\n            .coalesce(1).rdd.map(lambda x: (np.asarray(x[""features""]), x[""label""]))\n\n        first_graph = tf.Graph()\n        with first_graph.as_default() as g:\n            v = SparkFlowTests.create_model()\n            mg = json_format.MessageToJson(tf.train.export_meta_graph())\n\n        spark_model = HogwildSparkModel(\n            tensorflowGraph=mg,\n            iters=10,\n            tfInput=\'x:0\',\n            tfLabel=\'y:0\',\n            optimizer=tf.train.AdamOptimizer(learning_rate=.1),\n            master_url=\'localhost:5000\'\n        )\n\n        try:\n            weights = spark_model.train(processed)\n            self.assertTrue(len(weights) > 0)\n        except Exception as e:\n            spark_model.stop_server()\n            raise Exception(e.message)\n\n    def test_overlapping_guassians(self):\n        processed = self.generate_random_data()\n        mg = build_graph(SparkFlowTests.create_random_model)\n\n        spark_model = SparkAsyncDL(\n            inputCol=\'features\',\n            tensorflowGraph=mg,\n            tfInput=\'x:0\',\n            tfLabel=\'y:0\',\n            tfOutput=\'outer/Sigmoid:0\',\n            tfOptimizer=\'adam\',\n            tfLearningRate=.1,\n            iters=35,\n            partitions=2,\n            predictionCol=\'predicted\',\n            labelCol=\'label\'\n        )\n        self.handle_assertions(spark_model, processed)\n\n    def test_rmsprop(self):\n        processed = self.generate_random_data()\n        mg = build_graph(SparkFlowTests.create_random_model)\n        options = build_rmsprop_config(learning_rate=0.1, decay=0.95, momentum=0.1, centered=False)\n        spark_model = SparkAsyncDL(\n            inputCol=\'features\',\n            tensorflowGraph=mg,\n            tfInput=\'x:0\',\n            tfLabel=\'y:0\',\n            tfOutput=\'outer/Sigmoid:0\',\n            tfOptimizer=\'rmsprop\',\n            tfLearningRate=.1,\n            iters=25,\n            partitions=2,\n            predictionCol=\'predicted\',\n            labelCol=\'label\',\n            optimizerOptions=options\n        )\n        self.handle_assertions(spark_model, processed)\n\n    def test_multi_partition_shuffle(self):\n        processed = self.generate_random_data()\n        mg = build_graph(SparkFlowTests.create_random_model)\n        spark_model = SparkAsyncDL(\n            inputCol=\'features\',\n            tensorflowGraph=mg,\n            tfInput=\'x:0\',\n            tfLabel=\'y:0\',\n            tfOutput=\'outer/Sigmoid:0\',\n            tfOptimizer=\'adam\',\n            tfLearningRate=.1,\n            iters=20,\n            partitions=2,\n            predictionCol=\'predicted\',\n            labelCol=\'label\',\n            partitionShuffles=2\n        )\n        self.handle_assertions(spark_model, processed)\n\n    def test_auto_encoder(self):\n        processed = self.generate_random_data()\n        mg = build_graph(SparkFlowTests.create_autoencoder)\n        spark_model = SparkAsyncDL(\n            inputCol=\'features\',\n            tensorflowGraph=mg,\n            tfInput=\'x:0\',\n            tfLabel=None,\n            tfOutput=\'out/Sigmoid:0\',\n            tfOptimizer=\'adam\',\n            tfLearningRate=.001,\n            iters=10,\n            predictionCol=\'predicted\',\n            partitions=4,\n            miniBatchSize=10,\n            verbose=1\n        )\n        encoded = spark_model.fit(processed).transform(processed).take(10)\n        print(encoded[0][\'predicted\'])\n\n    def test_change_port(self):\n        processed = self.generate_random_data()\n        mg = build_graph(SparkFlowTests.create_random_model)\n\n        spark_model = SparkAsyncDL(\n            inputCol=\'features\',\n            tensorflowGraph=mg,\n            tfInput=\'x:0\',\n            tfLabel=\'y:0\',\n            tfOutput=\'outer/Sigmoid:0\',\n            tfOptimizer=\'adam\',\n            tfLearningRate=.1,\n            iters=35,\n            partitions=2,\n            predictionCol=\'predicted\',\n            labelCol=\'label\',\n            port=3000\n        )\n        self.handle_assertions(spark_model, processed)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
