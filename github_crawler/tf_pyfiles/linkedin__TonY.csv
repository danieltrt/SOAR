file_path,api_count,code
tony-examples/linearregression-mxnet/tony_mxnet_submit.py,0,"b'#!/bin/python2\n\nimport os\nimport datetime\nimport shutil\nimport tempfile\nimport re\nimport time\nimport logging\n\nlogging.basicConfig(format=\'%(process)d-%(levelname)s-%(message)s\', level=logging.INFO)\n\n\nfull_path=os.path.realpath(__file__)\nbase_dir = os.path.dirname( os.path.abspath(__file__) )\n\ncwd=os.getcwd()\nnow=datetime.datetime.now().strftime(""%Y%m%d_%H%M%S"")\n\ntask_params = [ ""--param1"", ""--param2"" ]\n\nlogging.info(""base_dir={}"".format(base_dir))\ncommand = \' \'.join( [\n\'java\', \'-cp\', \'`hadoop classpath`:{}/tony-cli/build/libs/tony-cli-0.3.21-all.jar\'.format(r\'/ebs/ds/hjeon/git/TonYMxNet\'), \'com.linkedin.tony.cli.ClusterSubmitter\',\n\'--python_venv=./MxNetPython3.5.zip\',\n\'--src_dir=""./src""\',\n\'--shell_env=DMLC_USE_KUBERNETES=1\',\n\'--shell_env=PS_VERBOSE=0\',\n\'--conf_file=./tony_mxnet_job.xml\',\n\'--executes=mxnet_dist_ex.py\',\n\'--task_params=""{}""\'.format( \' \'.join(task_params).replace(\'""\', \'\\""\') ),\n\'--python_binary_path=Python3.5/bin/python3\' ] )\n\n#\'--shell_env=PS_RESEND=10\',\nlogging.info(command) \nos.system(command)\n\n'"
tony-examples/mnist-pytorch/mnist_distributed.py,0,"b'\n""""""A deep MNIST classifier using convolutional layers.\n\nThis example was adapted from\nhttps://pytorch.org/docs/master/distributed.html\nhttps://pytorch.org/tutorials/intermediate/dist_tuto.html\nhttps://github.com/narumiruna/pytorch-distributed-example/blob/master/mnist/main.py\n\nEach worker reads the full MNIST dataset and asynchronously trains a CNN with dropout and using the Adam optimizer,\nupdating the model parameters on shared parameter servers.\n\nThe current training accuracy is printed out after every 100 steps.\n""""""\n\nfrom __future__ import division, print_function\n\nimport argparse\n\nimport os\nimport torch\nimport torch.nn.functional as F\nfrom torch import distributed, nn\nfrom torch.utils import data\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torchvision import datasets, transforms\n\n\nclass AverageMeter(object):\n\n    def __init__(self):\n        self.sum = 0\n        self.count = 0\n\n    def update(self, value, number):\n        self.sum += value * number\n        self.count += number\n\n    @property\n    def average(self):\n        return self.sum / self.count\n\n\nclass AccuracyMeter(object):\n\n    def __init__(self):\n        self.correct = 0\n        self.count = 0\n\n    def update(self, output, label):\n        predictions = output.data.argmax(dim=1)\n        correct = predictions.eq(label.data).sum().item()\n\n        self.correct += correct\n        self.count += output.size(0)\n\n    @property\n    def accuracy(self):\n        return self.correct / self.count\n\n\nclass Trainer(object):\n\n    def __init__(self, net, optimizer, train_loader, test_loader, device):\n        self.net = net\n        self.optimizer = optimizer\n        self.train_loader = train_loader\n        self.test_loader = test_loader\n        self.device = device\n\n    def train(self):\n        train_loss = AverageMeter()\n        train_acc = AccuracyMeter()\n\n        self.net.train()\n\n        for data, label in self.train_loader:\n            data = data.to(self.device)\n            label = label.to(self.device)\n\n            output = self.net(data)\n            loss = F.cross_entropy(output, label)\n\n            self.optimizer.zero_grad()\n            loss.backward()\n            # average the gradients\n            self.average_gradients()\n            self.optimizer.step()\n\n            train_loss.update(loss.item(), data.size(0))\n            train_acc.update(output, label)\n\n        return train_loss.average, train_acc.accuracy\n\n    def evaluate(self):\n        test_loss = AverageMeter()\n        test_acc = AccuracyMeter()\n\n        self.net.eval()\n\n        with torch.no_grad():\n            for data, label in self.test_loader:\n                data = data.to(self.device)\n                label = label.to(self.device)\n\n                output = self.net(data)\n                loss = F.cross_entropy(output, label)\n\n                test_loss.update(loss.item(), data.size(0))\n                test_acc.update(output, label)\n\n        return test_loss.average, test_acc.accuracy\n\n    def average_gradients(self):\n        world_size = distributed.get_world_size()\n\n        for p in self.net.parameters():\n            group = distributed.new_group(ranks=list(range(world_size)))\n\n            tensor = p.grad.data.cpu()\n\n            distributed.all_reduce(\n                tensor, op=distributed.reduce_op.SUM, group=group)\n\n            tensor /= float(world_size)\n\n            p.grad.data = tensor.to(self.device)\n\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc = nn.Linear(784, 10)\n\n    def forward(self, x):\n        return self.fc(x.view(x.size(0), -1))\n\n\ndef get_dataloader(root, batch_size):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.13066047740239478,), (0.3081078087569972,))\n    ])\n\n    train_set = datasets.MNIST(\n        root, train=True, transform=transform, download=True)\n    sampler = DistributedSampler(train_set)\n\n    train_loader = data.DataLoader(\n        train_set,\n        batch_size=batch_size,\n        shuffle=(sampler is None),\n        sampler=sampler)\n\n    test_loader = data.DataLoader(\n        datasets.MNIST(root, train=False, transform=transform, download=True),\n        batch_size=batch_size,\n        shuffle=False)\n\n    return train_loader, test_loader\n\n\ndef solve(args):\n    device = torch.device(\'cuda\' if args.cuda else \'cpu\')\n\n    net = Net().to(device)\n\n    optimizer = torch.optim.Adam(net.parameters(), lr=args.learning_rate)\n\n    train_loader, test_loader = get_dataloader(args.root, args.batch_size)\n\n    trainer = Trainer(net, optimizer, train_loader, test_loader, device)\n\n    for epoch in range(1, args.epochs + 1):\n        train_loss, train_acc = trainer.train()\n        test_loss, test_acc = trainer.evaluate()\n\n        print(\n            \'Epoch: {}/{},\'.format(epoch, args.epochs),\n            \'train loss: {:.6f}, train acc: {:.6f}, test loss: {:.6f}, test acc: {:.6f}.\'.\n                format(train_loss, train_acc, test_loss, test_acc))\n\n\ndef init_process(args):\n    distributed.init_process_group(\n        backend=args.backend,\n        init_method=args.init_method,\n        rank=args.rank,\n        world_size=args.world_size)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--backend\',\n        type=str,\n        default=\'tcp\',\n        help=\'Name of the backend to use.\')\n    parser.add_argument(\n        \'--init-method\',\n        \'-i\',\n        type=str,\n        default=os.environ.get(\'INIT_METHOD\', \'tcp://127.0.0.1:23456\'),\n        help=\'URL specifying how to initialize the package.\')\n    parser.add_argument(\n        \'--rank\', \'-r\',\n        type=int,\n        default=int(os.environ.get(\'RANK\')),\n        help=\'Rank of the current process.\')\n    parser.add_argument(\n        \'--world-size\',\n        \'-s\',\n        type=int,\n        default=int(os.environ.get(\'WORLD\')),\n        help=\'Number of processes participating in the job.\')\n    parser.add_argument(\'--epochs\', type=int, default=20)\n    parser.add_argument(\'--no-cuda\', action=\'store_true\')\n    parser.add_argument(\'--learning-rate\', \'-lr\', type=float, default=1e-3)\n    parser.add_argument(\'--root\', type=str, default=\'data\')\n    parser.add_argument(\'--batch-size\', type=int, default=128)\n    args = parser.parse_args()\n    args.cuda = torch.cuda.is_available() and not args.no_cuda\n    print(args)\n\n    init_process(args)\n    solve(args)\n\n\nif __name__ == \'__main__\':\n    main()'"
tony-examples/mnist-tensorflow/__init__.py,0,b''
tony-examples/mnist-tensorflow/mnist_distributed.py,50,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A deep MNIST classifier using convolutional layers.\n\nThis example was adapted from\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py.\n\nEach worker reads the full MNIST dataset and asynchronously trains a CNN with dropout and using the Adam optimizer,\nupdating the model parameters on shared parameter servers.\n\nThe current training accuracy is printed out after every 100 steps.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport json\nimport logging\nimport os\nimport sys\n\nimport tensorboard.program as tb_program\nimport tensorflow as tf\n\n# Environment variable containing port to launch TensorBoard on, set by TonY.\nTB_PORT_ENV_VAR = \'TB_PORT\'\n\n# Input/output directories\ntf.flags.DEFINE_string(\'data_dir\', \'/tmp/tensorflow/mnist/input_data\',\n                       \'Directory for storing input data\')\ntf.flags.DEFINE_string(\'working_dir\', \'/tmp/tensorflow/mnist/working_dir\',\n                       \'Directory under which events and output will be \'\n                       \'stored (in separate subdirectories).\')\n\n# Training parameters\ntf.flags.DEFINE_integer(""steps"", 1500,\n                        ""The number of training steps to execute."")\ntf.flags.DEFINE_integer(""batch_size"", 64, ""The batch size per step."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef deepnn(x):\n    """"""deepnn builds the graph for a deep net for classifying digits.\n\n    Args:\n      x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n      number of pixels in a standard MNIST image.\n\n    Returns:\n      A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n      equal to the logits of classifying the digit into one of 10 classes (the\n      digits 0-9). keep_prob is a scalar placeholder for the probability of\n      dropout.\n    """"""\n    # Reshape to use within a convolutional neural net.\n    # Last dimension is for ""features"" - there is only one here, since images are\n    # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n    with tf.name_scope(\'reshape\'):\n        x_image = tf.reshape(x, [-1, 28, 28, 1])\n\n    # First convolutional layer - maps one grayscale image to 32 feature maps.\n    with tf.name_scope(\'conv1\'):\n        W_conv1 = weight_variable([5, 5, 1, 32])\n        b_conv1 = bias_variable([32])\n        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n\n    # Pooling layer - downsamples by 2X.\n    with tf.name_scope(\'pool1\'):\n        h_pool1 = max_pool_2x2(h_conv1)\n\n    # Second convolutional layer -- maps 32 feature maps to 64.\n    with tf.name_scope(\'conv2\'):\n        W_conv2 = weight_variable([5, 5, 32, 64])\n        b_conv2 = bias_variable([64])\n        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n\n    # Second pooling layer.\n    with tf.name_scope(\'pool2\'):\n        h_pool2 = max_pool_2x2(h_conv2)\n\n    # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n    # is down to 7x7x64 feature maps -- maps this to 1024 features.\n    with tf.name_scope(\'fc1\'):\n        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n        b_fc1 = bias_variable([1024])\n\n        h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n    # Dropout - controls the complexity of the model, prevents co-adaptation of\n    # features.\n    with tf.name_scope(\'dropout\'):\n        keep_prob = tf.placeholder(tf.float32)\n        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n    # Map the 1024 features to 10 classes, one for each digit\n    with tf.name_scope(\'fc2\'):\n        W_fc2 = weight_variable([1024, 10])\n        b_fc2 = bias_variable([10])\n\n        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n    return y_conv, keep_prob\n\n\ndef conv2d(x, W):\n    """"""conv2d returns a 2d convolution layer with full stride.""""""\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n\ndef max_pool_2x2(x):\n    """"""max_pool_2x2 downsamples a feature map by 2X.""""""\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1], padding=\'SAME\')\n\n\ndef weight_variable(shape):\n    """"""weight_variable generates a weight variable of a given shape.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    """"""bias_variable generates a bias variable of a given shape.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n\ndef create_model():\n    """"""Creates our model and returns the target nodes to be run or populated""""""\n    # Create the model\n    x = tf.placeholder(tf.float32, [None, 784])\n\n    # Define loss and optimizer\n    y_ = tf.placeholder(tf.int64, [None])\n\n    # Build the graph for the deep net\n    y_conv, keep_prob = deepnn(x)\n\n    with tf.name_scope(\'loss\'):\n        cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_,\n                                                               logits=y_conv)\n        cross_entropy = tf.reduce_mean(cross_entropy)\n\n    global_step = tf.train.get_or_create_global_step()\n    with tf.name_scope(\'adam_optimizer\'):\n        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy,\n                                                           global_step=global_step)\n\n    with tf.name_scope(\'accuracy\'):\n        correct_prediction = tf.equal(tf.argmax(y_conv, 1), y_)\n        correct_prediction = tf.cast(correct_prediction, tf.float32)\n    accuracy = tf.reduce_mean(correct_prediction)\n\n    tf.summary.scalar(\'cross_entropy_loss\', cross_entropy)\n    tf.summary.scalar(\'accuracy\', accuracy)\n\n    merged = tf.summary.merge_all()\n\n    return x, y_, keep_prob, global_step, train_step, accuracy, merged\n\n\ndef start_tensorboard(logdir):\n    tb = tb_program.TensorBoard()\n    port = int(os.getenv(TB_PORT_ENV_VAR, 6006))\n    tb.configure(logdir=logdir, port=port)\n    tb.launch()\n    logging.info(""Starting TensorBoard with --logdir=%s"" % logdir)\n\n\ndef main(_):\n    logging.getLogger().setLevel(logging.INFO)\n\n    cluster_spec_str = os.environ[""CLUSTER_SPEC""]\n    cluster_spec = json.loads(cluster_spec_str)\n    ps_hosts = cluster_spec[\'ps\']\n    worker_hosts = cluster_spec[\'worker\']\n\n    # Create a cluster from the parameter server and worker hosts.\n    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})\n\n    # Create and start a server for the local task.\n    job_name = os.environ[""JOB_NAME""]\n    task_index = int(os.environ[""TASK_INDEX""])\n    server = tf.train.Server(cluster, job_name=job_name, task_index=task_index)\n\n    if job_name == ""ps"":\n        server.join()\n    elif job_name == ""worker"":\n        # Create our model graph. Assigns ops to the local worker by default.\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=""/job:worker/task:%d"" % task_index,\n                cluster=cluster)):\n            features, labels, keep_prob, global_step, train_step, accuracy, \\\n            merged = create_model()\n\n        if task_index is 0:  # chief worker\n            tf.gfile.MakeDirs(FLAGS.working_dir)\n            start_tensorboard(FLAGS.working_dir)\n\n        # The StopAtStepHook handles stopping after running given steps.\n        hooks = [tf.train.StopAtStepHook(num_steps=FLAGS.steps)]\n\n        # Filter all connections except that between ps and this worker to\n        # avoid hanging issues when one worker finishes. We are using\n        # asynchronous training so there is no need for the workers to\n        # communicate.\n        config_proto = tf.ConfigProto(\n            device_filters=[\'/job:ps\', \'/job:worker/task:%d\' % task_index])\n\n        with tf.train.MonitoredTrainingSession(master=server.target,\n                                               is_chief=(task_index == 0),\n                                               checkpoint_dir=FLAGS.working_dir,\n                                               hooks=hooks,\n                                               config=config_proto) as sess:\n            # Import data\n            logging.info(\'Extracting and loading input data...\')\n            mnist = input_data.read_data_sets(FLAGS.data_dir)\n\n            # Train\n            logging.info(\'Starting training\')\n            i = 0\n            while not sess.should_stop():\n                batch = mnist.train.next_batch(FLAGS.batch_size)\n                if i % 100 == 0:\n                    step, _, train_accuracy = sess.run(\n                        [global_step, train_step, accuracy],\n                        feed_dict={features: batch[0], labels: batch[1],\n                                   keep_prob: 1.0})\n                    logging.info(\'Step %d, training accuracy: %g\' % (\n                    step, train_accuracy))\n                else:\n                    sess.run([global_step, train_step],\n                             feed_dict={features: batch[0], labels: batch[1],\n                                        keep_prob: 0.5})\n                i += 1\n\n        logging.info(\'Done training!\')\n        sys.exit()\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
tony-examples/mnist-tensorflow/mnist_keras_distributed.py,32,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Runs MNIST using Distribution Strategies.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport logging\nimport numpy as np\nimport os\n\nfrom tensorboard.plugins.core import core_plugin\nimport tensorboard.program as tb_program\n\nimport tensorflow as tf\n\n\ndef get_args():\n    """"""Argument parser.\n\n      Returns:\n        Dictionary of arguments.\n      """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--working-dir\',\n        type=str,\n        required=True,\n        help=\'GCS location to write checkpoints and export models\')\n    parser.add_argument(\n        \'--num-epochs\',\n        type=float,\n        default=5,\n        help=\'number of times to go through the data, default=5\')\n    parser.add_argument(\n        \'--batch-size\',\n        default=128,\n        type=int,\n        help=\'number of records to read during each training step, default=128\')\n    parser.add_argument(\n        \'--learning-rate\',\n        default=.01,\n        type=float,\n        help=\'learning rate for gradient descent, default=.001\')\n    parser.add_argument(\n        \'--verbosity\',\n        choices=[\'DEBUG\', \'ERROR\', \'FATAL\', \'INFO\', \'WARN\'],\n        default=\'INFO\')\n    args, _ = parser.parse_known_args()\n    return args\n\n\ndef get_distribution_strategy():\n    cluster_spec = os.environ.get(""CLUSTER_SPEC"", None)\n    if cluster_spec:\n        cluster_spec = json.loads(cluster_spec)\n        job_index = int(os.environ[""TASK_INDEX""])\n        job_type = os.environ[""JOB_NAME""]\n        # Build cluster spec\n        os.environ[\'TF_CONFIG\'] = json.dumps(\n            {\'cluster\': cluster_spec,\n             \'task\': {\'type\': job_type, \'index\': job_index}})\n\n        print(\'Distribution enabled: \', os.environ[\'TF_CONFIG\'])\n    else:\n        print(\'Distribution is not enabled\')\n\n\ndef create_model(model_dir, config, learning_rate):\n    """"""Creates a Keras Sequential model with layers.\n\n        Args:\n          model_dir: (str) file path where training files will be written.\n          config: (tf.estimator.RunConfig) Configuration options to save model.\n          learning_rate: (int) Learning rate.\n\n        Returns:\n          A keras.Model\n        """"""\n    l = tf.keras.layers\n    model = tf.keras.Sequential(\n        [\n            l.Reshape(input_shape=(28 * 28,), target_shape=(28, 28, 1)),\n\n            l.Conv2D(filters=6, kernel_size=3, padding=\'same\',\n                     use_bias=False),\n            # no bias necessary before batch norm\n            l.BatchNormalization(scale=False, center=True),\n            # no batch norm scaling necessary before ""relu""\n            l.Activation(\'relu\'),  # activation after batch norm\n\n            l.Conv2D(filters=12, kernel_size=6, padding=\'same\',\n                     use_bias=False,\n                     strides=2),\n            l.BatchNormalization(scale=False, center=True),\n            l.Activation(\'relu\'),\n\n            l.Conv2D(filters=24, kernel_size=6, padding=\'same\',\n                     use_bias=False,\n                     strides=2),\n            l.BatchNormalization(scale=False, center=True),\n            l.Activation(\'relu\'),\n\n            l.Flatten(),\n            l.Dense(200, use_bias=False),\n            l.BatchNormalization(scale=False, center=True),\n            l.Activation(\'relu\'),\n            l.Dropout(0.5),  # Dropout on dense layer only\n\n            l.Dense(10, activation=\'softmax\')\n        ])\n    # Compile model with learning parameters.\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss=\'sparse_categorical_crossentropy\',\n        metrics=[\'accuracy\'])\n    tf.keras.backend.set_learning_phase(True)\n    model.summary()\n    estimator = tf.keras.estimator.model_to_estimator(\n        keras_model=model, model_dir=model_dir, config=config)\n    return estimator\n\n\ndef input_fn(features, labels, batch_size, mode):\n    """"""Input function.\n\n    Args:\n      features: (numpy.array) Training or eval data.\n      labels: (numpy.array) Labels for training or eval data.\n      batch_size: (int)\n      mode: tf.estimator.ModeKeys mode\n\n    Returns:\n      A tf.estimator.\n    """"""\n    # Default settings for training.\n    if labels is None:\n        inputs = features\n    else:\n        # Change numpy array shape.\n        inputs = (features, labels)\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n        dataset = dataset.prefetch(100)\n    if mode in (tf.estimator.ModeKeys.EVAL, tf.estimator.ModeKeys.PREDICT):\n        dataset = dataset.batch(batch_size)\n    return dataset\n\n\ndef serving_input_fn():\n    """"""Defines the features to be passed to the model during inference.\n\n    Expects already tokenized and padded representation of sentences\n\n    Returns:\n      A tf.estimator.export.ServingInputReceiver\n    """"""\n    feature_placeholder = tf.placeholder(tf.float32, [None, 28 * 28])\n    features = feature_placeholder\n    return tf.estimator.export.TensorServingInputReceiver(features,\n                                                          feature_placeholder)\n\n\ndef _get_session_config_from_env_var():\n    """"""Returns a tf.ConfigProto instance with appropriate device_filters set.""""""\n\n    tf_config = json.loads(os.environ.get(\'TF_CONFIG\', \'{}\'))\n\n    # GPU limit: TensorFlow by default allocates all GPU memory:\n    # If multiple workers run in same host you may see OOM errors:\n    # Use as workaround if not using Hadoop 3.1\n    # Change percentage accordingly:\n    # gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25)\n\n    if (tf_config and \'task\' in tf_config and \'type\' in tf_config[\'task\'] and\n        \'index\' in tf_config[\'task\']):\n        # Master should only communicate with itself and ps.\n        if tf_config[\'task\'][\'type\'] == \'master\':\n            return tf.ConfigProto(device_filters=[\'/job:ps\', \'/job:master\'])\n        # Worker should only communicate with itself and ps.\n        elif tf_config[\'task\'][\'type\'] == \'worker\':\n            return tf.ConfigProto(#gpu_options=gpu_options,\n                                  device_filters=[\n                                      \'/job:ps\',\n                                      \'/job:worker/task:%d\' % tf_config[\'task\'][\n                                          \'index\']\n                                  ])\n    return None\n\n\ndef start_tensorboard(logdir):\n    tb = tb_program.TensorBoard(plugins=[core_plugin.CorePluginLoader()])\n    port = int(os.getenv(\'TB_PORT\', 6006))\n    tb.configure(logdir=logdir, port=port)\n    tb.launch()\n    logging.info(""Starting TensorBoard with --logdir=%s"" % logdir)\n\n\ndef train_and_evaluate(args):\n    """"""Helper function: Trains and evaluates model.\n\n    Args:\n      args: (dict) Command line parameters passed from task.py\n    """"""\n    # Loads data.\n    (train_images, train_labels), (\n        test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n\n    # Scale values to a range of 0 to 1.\n    train_images = train_images / 255.0\n    test_images = test_images / 255.0\n\n    # Shape numpy array.\n    train_labels = np.asarray(train_labels).astype(\'int\').reshape((-1, 1))\n    test_labels = np.asarray(test_labels).astype(\'int\').reshape((-1, 1))\n\n    # Define training steps.\n    train_steps = len(train_images) / args.batch_size\n    get_distribution_strategy()\n\n    hook = tf.train.ProfilerHook(save_steps=100,\n                                 output_dir=args.working_dir,\n                                 show_memory=True)\n\n    # Define running config.\n    run_config = tf.estimator.RunConfig(\n        experimental_distribute=tf.contrib.distribute.DistributeConfig(\n            train_distribute=tf.contrib.distribute.ParameterServerStrategy(),\n            eval_distribute=tf.contrib.distribute.MirroredStrategy()),\n        session_config=_get_session_config_from_env_var(),\n        model_dir=args.working_dir,\n        save_summary_steps=100,\n        log_step_count_steps=100,\n        save_checkpoints_steps=500)\n    # Create estimator.\n    estimator = create_model(\n        model_dir=args.working_dir,\n        config=run_config,\n        learning_rate=args.learning_rate)\n    # Create TrainSpec.\n    train_spec = tf.estimator.TrainSpec(\n        input_fn=lambda: input_fn(\n            train_images,\n            train_labels,\n            args.batch_size,\n            mode=tf.estimator.ModeKeys.TRAIN),\n        # hooks=[hook], # Uncomment if needed to debug.\n        max_steps=train_steps)\n    # Create EvalSpec.\n    exporter = tf.estimator.FinalExporter(\'exporter\', serving_input_fn)\n    eval_spec = tf.estimator.EvalSpec(\n        input_fn=lambda: input_fn(\n            test_images,\n            test_labels,\n            args.batch_size,\n            mode=tf.estimator.ModeKeys.EVAL),\n        steps=None,\n        name=\'mnist-eval\',\n        exporters=[exporter],\n        start_delay_secs=10,\n        throttle_secs=10)\n\n    # Launch Tensorboard in a separate thread.\n    tf.gfile.MakeDirs(args.working_dir)\n    start_tensorboard(args.working_dir)\n\n    # Start training\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n\nif __name__ == \'__main__\':\n    args = get_args()\n    tf.logging.set_verbosity(args.verbosity)\n    train_and_evaluate(args)\n'"
tony-portal/bin/check_python_version.py,0,b'import sys\npy_version = sys.version_info\nmajor = py_version[0]\nminor = py_version[1]\nif not major > 2 and not minor > 6:\n    sys.exit(1)'
tony-portal/bin/xml_to_play_opts.py,0,"b'#!/export/apps/python/2.7/bin/python\n\n# This script parses tony-site.xml, then returns a string that\n# includes all the necessary flags for Play to consume.\n\nimport xml.etree.ElementTree as ET\nimport sys\n\n\nTONY_TO_PLAY_MAPPINGS = {\n    \'tony.https.port\': \'play.server.https.port\',\n    \'tony.https.keystore.path\': \'play.server.https.keyStore.path\',\n    \'tony.https.keystore.type\': \'play.server.https.keyStore.type\',\n    \'tony.https.keystore.password\': \'play.server.https.keyStore.password\',\n    \'tony.https.keystore.algorithm\': \'play.server.https.keyStore.algorithm\',\n    \'tony.http.port\': \'http.port\',\n    \'tony.secret.key\': \'play.http.secret.key\',\n}\n\nif len(sys.argv) < 2:\n    print(""Usage: python xml_to_play_opts.py <path/to/tony-site.xml>"")\n    sys.exit(1)\n\ntree = ET.parse(sys.argv[1])\nroot = tree.getroot()\n\n# merge XIncluded file properties\nxincludes = root.findall(\'{http://www.w3.org/2001/XInclude}include\')\nfor xinclude in xincludes:\n    included_file = xinclude.get(\'href\')\n    data = ET.parse(included_file).getroot()\n    root.extend(data)\n\n# construct Play options\nplay_opts = \'\'\nnodes = root.findall(\'property\')\nfor node in nodes:\n    name = node.find(\'name\').text\n    value = node.find(\'value\').text\n    if name in TONY_TO_PLAY_MAPPINGS:\n        name = TONY_TO_PLAY_MAPPINGS[name]\n    play_opts += \'-D\' + name + \'=\' + value + \' \'\n\nprint(play_opts)'"
tony-examples/linearregression-mxnet/src/mxnet_dist_ex.py,0,"b'print(""RUN PYTHON"")\n\nimport os\n\nrole = os.environ.get(""DMLC_ROLE"")\nprint(""start role: {}"".format(role))\nfor item, value in os.environ.items():\n    print(\'{}: [{}]\'.format(item, value))\n\nimport mxnet as mx\nif role != ""worker"":\n    print(""terminate job:{}"".format(role))\n    os.sys.exit(0)\n\nimport logging\nimport numpy as np\nimport time\nfrom sklearn.model_selection import train_test_split\n\nlogging.getLogger().setLevel(logging.DEBUG)\n\n\n# lineral equation\ndef f(x):\n  # a = 5\n  # b = 2\n  return 5 * x + 2\n\n# Data\nX = np.arange(100, step=0.001)\nY = f(X)\n\n# Split data for taining and evaluation\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y)\nkv_store = mx.kv.create(\'dist_async\')\n\nbatch_size = 1024\ntrain_iter = mx.io.NDArrayIter(X_train, \n                               Y_train, \n                               batch_size, \n                               shuffle=True,\n                               label_name=\'lin_reg_label\')\neval_iter = mx.io.NDArrayIter(X_test, \n                              Y_test, \n                              batch_size, \n                              shuffle=False)\n\nX = mx.sym.Variable(\'data\')\nY = mx.symbol.Variable(\'lin_reg_label\')\nfully_connected_layer  = mx.sym.FullyConnected(data=X, name=\'fc1\', num_hidden = 1)\nlro = mx.sym.LinearRegressionOutput(data=fully_connected_layer, label=Y, name=""lro"")\n\nmodel = mx.mod.Module(\n    symbol = lro ,\n    data_names=[\'data\'],\n    label_names = [\'lin_reg_label\']# network structure\n)\ntime1 = time.time()\n\nprint(""start training..."")\nmodel.fit(train_iter, eval_iter,\n            optimizer_params={\n                \'learning_rate\':0.000000002},\n            num_epoch=20,\n            eval_metric=\'mae\',\n            batch_end_callback\n                 = mx.callback.Speedometer(batch_size, 20),\n            kvstore=kv_store)\n\ntime2 = time.time()\nprint(\'training took %0.3f ms\' % ((time2 - time1) * 1000.0))\n'"
tony-core/src/test/resources/scripts/check_archive_file_localization.py,0,"b'#\n# Copyright 2018 LinkedIn Corporation. All rights reserved. Licensed under the BSD-2 Clause license.\n# See LICENSE in the project root for license information.\n#\nimport time\nimport logging\nimport os\nimport sys\n\ntime.sleep(1)\n\n# Set up logging.\nlog_root = logging.getLogger()\nlog_root.setLevel(logging.DEBUG)\nch = logging.StreamHandler(sys.stdout)\nch.setLevel(logging.DEBUG)\nlog_root.addHandler(ch)\n\nfiles = os.listdir(""./"")\nfor name in files:\n    logging.info(name)\nif not os.path.isfile(\'./common.zip\'):\n    logging.error(\'common.zip doesn\\\'t exist\')\n    exit(-1)\nif not os.path.isfile(\'./test20.zip\'):\n    logging.error(\'test20.zip doesn\\\'t exist\')\n    exit(-1)\nif not os.path.isfile(\'./test2.zip/123.xml\'):\n    logging.error(\'123.xml doesn\\\'t exist\')\n    exit(-1)\n'"
tony-core/src/test/resources/scripts/check_env_and_venv.py,0,"b""#\n# Copyright 2018 LinkedIn Corporation. All rights reserved. Licensed under the BSD-2 Clause license.\n# See LICENSE in the project root for license information.\n#\nimport time\nimport logging\nimport os\nimport sys\n\ntime.sleep(1)\n\n# Set up logging.\nlog_root = logging.getLogger()\nlog_root.setLevel(logging.DEBUG)\nch = logging.StreamHandler(sys.stdout)\nch.setLevel(logging.DEBUG)\nlog_root.addHandler(ch)\n\nif not os.path.isfile('venv/123.xml'):\n    logging.error('venv/123.xml doesn\\'t exist')\n    exit(-1)\n\nif os.environ['ENV_CHECK'] == 'ENV_CHECK':\n    logging.info('Found ENV_CHECK environment variable.')\n    exit(0)\nelse:\n    logging.error('Failed to find ENV_CHECK environment variable')\n    exit(1)\n"""
tony-core/src/test/resources/scripts/check_tb_port_set_in_chief_only.py,0,"b""#\n# Copyright 2019 LinkedIn Corporation. All rights reserved. Licensed under the BSD-2 Clause license.\n# See LICENSE in the project root for license information.\n#\nimport os\n\n\ntb_port = None\nif 'TB_PORT' in os.environ:\n    tb_port = os.environ['TB_PORT']\n\njob_name = os.environ['JOB_NAME']\n\nprint('TB_PORT is ' + str(tb_port))\nprint('JOB_NAME is ' + job_name)\n\nif tb_port and job_name != 'chief':\n    raise ValueError"""
tony-core/src/test/resources/scripts/exit_0.py,0,b'#\n# Copyright 2018 LinkedIn Corporation. All rights reserved. Licensed under the BSD-2 Clause license.\n# See LICENSE in the project root for license information.\n#\nimport time\n\ntime.sleep(1)\nexit(0)'
tony-core/src/test/resources/scripts/exit_0_check_env.py,0,"b'#\n# Copyright 2019 LinkedIn Corporation. All rights reserved. Licensed under the BSD-2 Clause license.\n# See LICENSE in the project root for license information.\n#\n\n""""""\nCopyright 2018 LinkedIn Corporation. All rights reserved. Licensed under the BSD-2 Clause license.\nSee LICENSE in the project root for license information.\n""""""\nimport time\nimport logging\nimport os\nimport sys\n\ntime.sleep(1)\n\n# Set up logging.\nlog_root = logging.getLogger()\nlog_root.setLevel(logging.DEBUG)\nch = logging.StreamHandler(sys.stdout)\nch.setLevel(logging.DEBUG)\nlog_root.addHandler(ch)\n\nif os.environ[\'ENV_CHECK\'] == \'ENV_CHECK\':\n    logging.info(\'Found ENV_CHECK environment variable.\')\n    exit(0)\nelse:\n    logging.error(\'Failed to find ENV_CHECK environment variable\')\n    exit(1)\n'"
tony-core/src/test/resources/scripts/exit_0_check_pytorchenv.py,0,"b""#\n# Copyright 2018 LinkedIn Corporation. All rights reserved. Licensed under the BSD-2 Clause license.\n# See LICENSE in the project root for license information.\n#\nimport time\nimport logging\nimport os\nimport sys\n\ntime.sleep(1)\n\n# Set up logging.\nlog_root = logging.getLogger()\nlog_root.setLevel(logging.DEBUG)\nch = logging.StreamHandler(sys.stdout)\nch.setLevel(logging.DEBUG)\nlog_root.addHandler(ch)\n\nif os.environ.get('RANK') is None:\n    logging.error('Failed to find RANK environment variable')\n    exit(1)\nif os.environ.get('WORLD') is None:\n    logging.error('Failed to find WORLD environment variable')\n    exit(1)\nif os.environ.get('INIT_METHOD') is None:\n    logging.error('Failed to find INIT_METHOD environment variable')\n    exit(1)\n\nexit(0)\n"""
tony-core/src/test/resources/scripts/exit_1.py,0,b'#\n# Copyright 2018 LinkedIn Corporation. All rights reserved. Licensed under the BSD-2 Clause license.\n# See LICENSE in the project root for license information.\n#\nimport time\n\n\ndef return_1():\n    time.sleep(1)\n    return 1\n\n\nexit(return_1())\n'
tony-core/src/test/resources/scripts/sleep_30.py,0,b'#\n# Copyright 2019 LinkedIn Corporation. All rights reserved. Licensed under the BSD-2 Clause license.\n# See LICENSE in the project root for license information.\n#\nimport time\n\ntime.sleep(30)'
