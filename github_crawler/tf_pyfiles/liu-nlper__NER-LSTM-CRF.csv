file_path,api_count,code
load_data.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n__author__ = \'jxliu.nlper@gmail.com\'\n""""""\n    \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\n""""""\nimport sys\nimport codecs\nimport pickle\nimport numpy as np\nfrom utils import map_item2id\n\n\ndef load_vocs(paths):\n    """"""\n    \xe5\x8a\xa0\xe8\xbd\xbdvocs\n    Args:\n        paths: list of str, voc\xe8\xb7\xaf\xe5\xbe\x84\n    Returns:\n        vocs: list of dict\n    """"""\n    vocs = []\n    for path in paths:\n        with open(path, \'rb\') as file_r:\n            vocs.append(pickle.load(file_r))\n    return vocs\n\n\ndef load_lookup_tables(paths):\n    """"""\n    \xe5\x8a\xa0\xe8\xbd\xbdlookup tables\n    Args:\n        paths: list of str, emb\xe8\xb7\xaf\xe5\xbe\x84\n    Returns:\n        lookup_tables: list of dict\n    """"""\n    lookup_tables = []\n    for path in paths:\n        with open(path, \'rb\', encoding=\'utf-8\') as file_r:\n            lookup_tables.append(pickle.load(file_r))\n    return lookup_tables\n\n\ndef init_data(path, feature_names, vocs, max_len, model=\'train\',\n              use_char_feature=False, word_len=None, sep=\'\\t\'):\n    """"""\n    \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae(\xe5\xbe\x85\xe4\xbc\x98\xe5\x8c\x96\xef\xbc\x8c\xe7\x9b\xae\xe5\x89\x8d\xe6\x98\xaf\xe4\xb8\x80\xe6\xac\xa1\xe6\x80\xa7\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb4\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86)\n    Args:\n        path: str, \xe6\x95\xb0\xe6\x8d\xae\xe8\xb7\xaf\xe5\xbe\x84\n        feature_names: list of str, \xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d\xe7\xa7\xb0\n        vocs: list of dict\n        max_len: int, \xe5\x8f\xa5\xe5\xad\x90\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n        model: str, in (\'train\', \'test\')\n        use_char_feature: bool\xef\xbc\x8c\xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8char\xe7\x89\xb9\xe5\xbe\x81\n        word_len: None or int\xef\xbc\x8c\xe5\x8d\x95\xe8\xaf\x8d\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n        sep: str, \xe7\x89\xb9\xe5\xbe\x81\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe5\x88\x86\xe5\x89\xb2\xe7\xac\xa6, default is \'\\t\'\n    Returns:\n        data_dict: dict\n    """"""\n    assert model in (\'train\', \'test\')\n    file_r = codecs.open(path, \'r\', encoding=\'utf-8\')\n    sentences = file_r.read().strip().split(\'\\n\\n\')\n    sentence_count = len(sentences)\n    feature_count = len(feature_names)\n    data_dict = dict()\n    for feature_name in feature_names:\n        data_dict[feature_name] = np.zeros((sentence_count, max_len), dtype=\'int32\')\n    # char feature\n    if use_char_feature:\n        data_dict[\'char\'] = np.zeros(\n            (sentence_count, max_len, word_len), dtype=\'int32\')\n        char_voc = vocs.pop(0)\n    if model == \'train\':\n        data_dict[\'label\'] = np.zeros((len(sentences), max_len), dtype=\'int32\')\n    for index, sentence in enumerate(sentences):\n        items = sentence.split(\'\\n\')\n        one_instance_items = []\n        [one_instance_items.append([]) for _ in range(len(feature_names)+1)]\n        for item in items:\n            feature_tokens = item.split(sep)\n            for j in range(feature_count):\n                one_instance_items[j].append(feature_tokens[j])\n            if model == \'train\':\n                one_instance_items[-1].append(feature_tokens[-1])\n        for i in range(len(feature_names)):\n            data_dict[feature_names[i]][index, :] = map_item2id(\n                one_instance_items[i], vocs[i], max_len)\n        if use_char_feature:\n            for i, word in enumerate(one_instance_items[0]):\n                if i >= max_len:\n                    break\n                data_dict[\'char\'][index][i, :] = map_item2id(\n                    word, char_voc, word_len)\n        if model == \'train\':\n            data_dict[\'label\'][index, :] = map_item2id(\n                one_instance_items[-1], vocs[-1], max_len)\n        sys.stdout.write(\'loading data: %d\\r\' % index)\n    file_r.close()\n    return data_dict\n'"
model.py,62,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n__author__ = \'jxliu.nlper@gmail.com\'\n""""""\n    \xe6\xa8\xa1\xe5\x9e\x8b: bi-lstm + crf\n""""""\nimport math\nimport numpy as np\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nfrom utils import uniform_tensor, get_sequence_actual_length, \\\n    zero_nil_slot, shuffle_matrix\n\n\ndef get_activation(activation=None):\n    """"""\n    Get activation function accord to the parameter \'activation\'\n    Args:\n        activation: str: \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\x90\x8d\xe7\xa7\xb0\n    Return:\n        \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n    """"""\n    if activation is None:\n        return None\n    elif activation == \'tanh\':\n        return tf.nn.tanh\n    elif activation == \'relu\':\n        return tf.nn.relu\n    elif activation == \'softmax\':\n        return tf.nn.softmax\n    elif activation == \'sigmoid\':\n        return tf.sigmoid\n    else:\n        raise Exception(\'Unknow activation function: %s\' % activation)\n\n\nclass MultiConvolutional3D(object):\n\n    def __init__(self, input_data, filter_length_list, nb_filter_list, padding=\'VALID\',\n                 activation=\'relu\', pooling=\'max\', name=\'Convolutional3D\'):\n        """"""3D\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n        Args:\n            input_data: 4D tensor of shape=[batch_size, sent_len, word_len, char_dim]\n                in_channels is set to 1 when use Convolutional3D.\n            filter_length_list: list of int, \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\x9e\x84\xe9\x80\xa0\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xef\xbc\x8c\xe5\x9c\xa8\n                Convolutional1D\xe4\xb8\xad\xef\xbc\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8shape=[filter_length, in_width, in_channels, nb_filters]\n            nb_filter_list: list of int, \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x95\xb0\xe9\x87\x8f\n            padding: \xe9\xbb\x98\xe8\xae\xa4\'VALID\'\xef\xbc\x8c\xe6\x9a\x82\xe6\x97\xb6\xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81\xe8\xae\xbe\xe6\x88\x90\'SAME\'\n        """"""\n        assert padding in (\'VALID\'), \'Unknow padding %s\' % padding\n        # assert padding in (\'VALID\', \'SAME\'), \'Unknow padding %s\' % padding\n\n        # expand dim\n        char_dim = int(input_data.get_shape()[-1])  # char\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\n        self._input_data = tf.expand_dims(input_data, -1)  # shape=[x, x, x, 1]\n        self._filter_length_list = filter_length_list\n        self._nb_filter_list = nb_filter_list\n        self._padding = padding\n        self._activation = get_activation(activation)\n        self._name = name\n\n        pooling_outpouts = []\n        for i in range(len(self._filter_length_list)):\n            filter_length = self._filter_length_list[i]\n            nb_filter = self._nb_filter_list[i]\n            with tf.variable_scope(\'%s_%d\' % (name, filter_length)) as scope:\n                # shape= [batch_size, sent_len-filter_length+1, word_len, 1, nb_filters]\n                conv_output = tf.contrib.layers.conv3d(\n                    inputs=self._input_data,\n                    num_outputs=nb_filter,\n                    kernel_size=[1, filter_length, char_dim],\n                    padding=self._padding)\n                # output\'s shape=[batch_size, new_height, 1, nb_filters]\n                act_output = (\n                    conv_output if activation is None\n                    else self._activation(conv_output))\n                # max pooling\xef\xbc\x8cshape = [batch_size, sent_len, nb_filters]\n                if pooling == \'max\':\n                    pooling_output = tf.reduce_max(tf.squeeze(act_output, [-2]), 2)\n                elif pooling == \'mean\':\n                    pooling_output = tf.reduce_mean(tf.squeeze(act_output, [-2]), 2)\n                else:\n                    raise Exception(\'pooling must in (max, mean)!\')\n                pooling_outpouts.append(pooling_output)\n\n                scope.reuse_variables()\n        # [batch_size, sent_len, sum(nb_filter_list]\n        self._output = tf.concat(pooling_outpouts, axis=-1)\n\n    @property\n    def output(self):\n        return self._output\n\n    @property\n    def output_dim(self):\n        return sum(self._nb_filter_list)\n\n\nclass SequenceLabelingModel(object):\n\n    def __init__(self, sequence_length, nb_classes, nb_hidden=512, num_layers=1,\n                 rnn_dropout=0., feature_names=None, feature_init_weight_dict=None,\n                 feature_weight_shape_dict=None, feature_weight_dropout_dict=None,\n                 dropout_rate=0., use_crf=True, path_model=None, nb_epoch=200,\n                 batch_size=128, train_max_patience=10, l2_rate=0.01, rnn_unit=\'lstm\',\n                 learning_rate=0.001, clip=None, use_char_feature=False, word_length=None,\n                 conv_filter_size_list=None, conv_filter_len_list=None, cnn_dropout_rate=0.):\n        """"""\n        Args:\n          sequence_length: int, \xe8\xbe\x93\xe5\x85\xa5\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84padding\xe5\x90\x8e\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n          nb_classes: int, \xe6\xa0\x87\xe7\xad\xbe\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\xe9\x87\x8f\n          nb_hidden: int, lstm/gru\xe5\xb1\x82\xe7\x9a\x84\xe7\xbb\x93\xe7\x82\xb9\xe6\x95\xb0\n          num_layers: int, lstm/gru\xe5\xb1\x82\xe6\x95\xb0\n          rnn_dropout: lstm\xe5\xb1\x82\xe7\x9a\x84dropout\xe5\x80\xbc\n\n          feature_names: list of str, \xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d\xe7\xa7\xb0\xe9\x9b\x86\xe5\x90\x88\n          feature_init_weight_dict: dict, \xe9\x94\xae:\xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d\xe7\xa7\xb0, \xe5\x80\xbc:np,array, \xe7\x89\xb9\xe5\xbe\x81\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x9d\x83\xe9\x87\x8d\xe5\xad\x97\xe5\x85\xb8\n          feature_weight_shape_dict: dict\xef\xbc\x8c\xe7\x89\xb9\xe5\xbe\x81embedding\xe6\x9d\x83\xe9\x87\x8d\xe7\x9a\x84shape\xef\xbc\x8c\xe9\x94\xae:\xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d\xe7\xa7\xb0, \xe5\x80\xbc: shape(tuple)\xe3\x80\x82\n          feature_weight_dropout_dict: feature name to float, feature weights dropout rate\n\n          dropout: float, dropout rate\n          use_crf: bool, \xe6\xa0\x87\xe7\xa4\xba\xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8crf\xe5\xb1\x82\n          path_model: str, \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\n          nb_epoch: int, \xe8\xae\xad\xe7\xbb\x83\xe6\x9c\x80\xe5\xa4\xa7\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\n          batch_size: int\n          train_max_patience: int, \xe5\x9c\xa8dev\xe4\xb8\x8a\xe7\x9a\x84loss\xe5\xaf\xb9\xe4\xba\x8etrain_max_patience\xe6\xac\xa1\xe6\xb2\xa1\xe6\x9c\x89\xe6\x8f\x90\xe5\x8d\x87\xef\xbc\x8c\xe5\x88\x99early stopping\n\n          l2_rate: float\n\n          rnn_unit: str, lstm or gru\n          learning_rate: float, default is 0.001\n          clip: None or float, gradients clip\n\n          use_char_feature: bool,\xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8\xe5\xad\x97\xe7\xac\xa6\xe7\x89\xb9\xe5\xbe\x81\n          word_length: int, \xe5\x8d\x95\xe8\xaf\x8d\xe9\x95\xbf\xe5\xba\xa6\n        """"""\n        self._sequence_length = sequence_length\n        self._nb_classes = nb_classes\n        self._nb_hidden = nb_hidden\n        self._num_layers = num_layers\n        self._rnn_dropout = rnn_dropout\n\n        self._feature_names = feature_names\n        self._feature_init_weight_dict = feature_init_weight_dict if \\\n            feature_init_weight_dict else dict()\n        self._feature_weight_shape_dict = feature_weight_shape_dict\n        self._feature_weight_dropout_dict = feature_weight_dropout_dict\n\n        self._dropout_rate = dropout_rate\n        self._use_crf = use_crf\n\n        self._path_model = path_model\n        self._nb_epoch = nb_epoch\n        self._batch_size = batch_size\n        self._train_max_patience = train_max_patience\n\n        self._l2_rate = l2_rate\n        self._rnn_unit = rnn_unit\n        self._learning_rate = learning_rate\n        self._clip = clip\n\n        self._use_char_feature = use_char_feature\n        self._word_length = word_length\n        self._conv_filter_len_list = conv_filter_len_list\n        self._conv_filter_size_list = conv_filter_size_list\n        self._cnn_dropout_rate = cnn_dropout_rate\n\n        assert len(feature_names) == len(list(set(feature_names))), \\\n            \'duplication of feature names!\'\n\n        # init ph, weights and dropout rate\n        self.input_feature_ph_dict = dict()\n        self.weight_dropout_ph_dict = dict()\n        self.feature_weight_dict = dict()\n        self.nil_vars = set()\n        self.dropout_rate_ph = tf.placeholder(tf.float32, name=\'dropout_rate_ph\')\n        self.rnn_dropout_rate_ph = tf.placeholder(tf.float32, name=\'rnn_dropout_rate_ph\')\n        # label ph\n        self.input_label_ph = tf.placeholder(\n            dtype=tf.int32, shape=[None, self._sequence_length], name=\'input_label_ph\')\n        if self._use_char_feature:\n            self.cnn_dropout_rate_ph = tf.placeholder(tf.float32, name=\'cnn_dropout_rate_ph\')\n\n        self.build_model()\n\n    def build_model(self):\n        for feature_name in self._feature_names:\n\n            # input ph\n            self.input_feature_ph_dict[feature_name] = tf.placeholder(\n                dtype=tf.int32, shape=[None, self._sequence_length],\n                name=\'input_feature_ph_%s\' % feature_name)\n\n            # dropout rate ph\n            self.weight_dropout_ph_dict[feature_name] = tf.placeholder(\n                tf.float32, name=\'dropout_ph_%s\' % feature_name)\n\n            # init feature weights, \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x9c\xaa\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\n            if feature_name not in self._feature_init_weight_dict:\n                feature_weight = uniform_tensor(\n                    shape=self._feature_weight_shape_dict[feature_name],\n                    name=\'f_w_%s\' % feature_name)\n                self.feature_weight_dict[feature_name] = tf.Variable(\n                    initial_value=feature_weight, name=\'feature_weigth_%s\' % feature_name)\n            else:\n                self.feature_weight_dict[feature_name] = tf.Variable(\n                    initial_value=self._feature_init_weight_dict[feature_name],\n                    name=\'feature_weight_%s\' % feature_name)\n                self.nil_vars.add(self.feature_weight_dict[feature_name].name)\n\n            # init dropout rate, \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x9c\xaa\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\n            if feature_name not in self._feature_weight_dropout_dict:\n                self._feature_weight_dropout_dict[feature_name] = 0.\n        # char feature\n        if self._use_char_feature:\n            # char feature weights\n            feature_weight = uniform_tensor(\n                shape=self._feature_weight_shape_dict[\'char\'], name=\'f_w_%s\' % \'char\')\n            self.feature_weight_dict[\'char\'] = tf.Variable(\n                initial_value=feature_weight, name=\'feature_weigth_%s\' % \'char\')\n            self.nil_vars.add(self.feature_weight_dict[\'char\'].name)\n            self.nil_vars.add(self.feature_weight_dict[\'char\'].name)\n            self.input_feature_ph_dict[\'char\'] = tf.placeholder(\n                dtype=tf.int32, shape=[None, self._sequence_length, self._word_length],\n                name=\'input_feature_ph_%s\' % \'char\')\n\n        # init embeddings\n        self.embedding_features = []\n        for feature_name in self._feature_names:\n            embedding_feature = tf.nn.dropout(tf.nn.embedding_lookup(\n                self.feature_weight_dict[feature_name],\n                ids=self.input_feature_ph_dict[feature_name],\n                name=\'embedding_feature_%s\' % feature_name),\n                keep_prob=1.-self.weight_dropout_ph_dict[feature_name],\n                name=\'embedding_feature_dropout_%s\' % feature_name)\n            self.embedding_features.append(embedding_feature)\n        # char embedding\n        if self._use_char_feature:\n            char_embedding_feature = tf.nn.embedding_lookup(\n                self.feature_weight_dict[\'char\'],\n                ids=self.input_feature_ph_dict[\'char\'],\n                name=\'embedding_feature_%s\' % \'char\')\n            # conv\n            couv_feature_char = MultiConvolutional3D(\n                char_embedding_feature, filter_length_list=self._conv_filter_len_list,\n                nb_filter_list=self._conv_filter_size_list).output\n            couv_feature_char = tf.nn.dropout(\n                couv_feature_char, keep_prob=1-self.cnn_dropout_rate_ph)\n\n        # concat all features\n        input_features = self.embedding_features[0] if len(self.embedding_features) == 1 \\\n            else tf.concat(values=self.embedding_features, axis=2, name=\'input_features\')\n        if self._use_char_feature:\n            input_features = tf.concat([input_features, couv_feature_char], axis=-1)\n\n        # multi bi-lstm layer\n        _fw_cells = []\n        _bw_cells = []\n        for _ in range(self._num_layers):\n            fw, bw = self._get_rnn_unit(self._rnn_unit)\n            _fw_cells.append(tf.nn.rnn_cell.DropoutWrapper(fw, output_keep_prob=1-self.rnn_dropout_rate_ph))\n            _bw_cells.append(tf.nn.rnn_cell.DropoutWrapper(bw, output_keep_prob=1-self.rnn_dropout_rate_ph))\n        fw_cell = tf.nn.rnn_cell.MultiRNNCell(_fw_cells)\n        bw_cell = tf.nn.rnn_cell.MultiRNNCell(_bw_cells)\n\n        # \xe8\xae\xa1\xe7\xae\x97self.input_features[feature_names[0]]\xe7\x9a\x84\xe5\xae\x9e\xe9\x99\x85\xe9\x95\xbf\xe5\xba\xa6(0\xe4\xb8\xbapadding\xe5\x80\xbc)\n        self.sequence_actual_length = get_sequence_actual_length(  # \xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe7\x9a\x84\xe5\xae\x9e\xe9\x99\x85\xe9\x95\xbf\xe5\xba\xa6\n            self.input_feature_ph_dict[self._feature_names[0]])\n        rnn_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n            fw_cell, bw_cell, input_features, scope=\'bi-lstm\',\n            dtype=tf.float32, sequence_length=self.sequence_actual_length)\n        # shape = [batch_size, max_len, nb_hidden*2]\n        lstm_output = tf.nn.dropout(\n            tf.concat(rnn_outputs, axis=2, name=\'lstm_output\'),\n            keep_prob=1.-self.dropout_rate_ph, name=\'lstm_output_dropout\')\n\n        # softmax\n        hidden_size = int(lstm_output.shape[-1])\n        self.outputs = tf.reshape(lstm_output, [-1, hidden_size], name=\'outputs\')\n        self.softmax_w = tf.get_variable(\'softmax_w\', [hidden_size, self._nb_classes])\n        self.softmax_b = tf.get_variable(\'softmax_b\', [self._nb_classes])\n        self.logits = tf.reshape(\n            tf.matmul(self.outputs, self.softmax_w) + self.softmax_b,\n            shape=[-1, self._sequence_length, self._nb_classes], name=\'logits\')\n\n        # \xe8\xae\xa1\xe7\xae\x97loss\n        self.loss = self.compute_loss()\n        self.l2_loss = self._l2_rate * (tf.nn.l2_loss(self.softmax_w) + tf.nn.l2_loss(self.softmax_b))\n\n        self.total_loss = self.loss + self.l2_loss\n\n        # train op\n        optimizer = tf.train.AdamOptimizer(learning_rate=self._learning_rate)\n        grads_and_vars = optimizer.compute_gradients(self.total_loss)\n        nil_grads_and_vars = []\n        for g, v in grads_and_vars:\n            if v.name in self.nil_vars:\n                nil_grads_and_vars.append((zero_nil_slot(g), v))\n            else:\n                nil_grads_and_vars.append((g, v))\n\n        global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n        if self._clip:\n            # clip by global norm\n            gradients, variables = zip(*nil_grads_and_vars)\n            gradients, _ = tf.clip_by_global_norm(gradients, self._clip)\n            self.train_op = optimizer.apply_gradients(\n                zip(gradients, variables), name=\'train_op\', global_step=global_step)\n        else:\n            self.train_op = optimizer.apply_gradients(\n                nil_grads_and_vars, name=\'train_op\', global_step=global_step)\n\n        # TODO sess, visible_device_list\xe5\xbe\x85\xe4\xbf\xae\xe6\x94\xb9\n        gpu_options = tf.GPUOptions(visible_device_list=\'0\', allow_growth=True)\n        self.sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n\n        # init all variable\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n    def _get_rnn_unit(self, rnn_unit):\n        if rnn_unit == \'lstm\':\n            fw_cell = rnn.BasicLSTMCell(self._nb_hidden, forget_bias=1., state_is_tuple=True)\n            bw_cell = rnn.BasicLSTMCell(self._nb_hidden, forget_bias=1., state_is_tuple=True)\n        elif rnn_unit == \'gru\':\n            fw_cell = rnn.GRUCell(self._nb_hidden)\n            bw_cell = rnn.GRUCell(self._nb_hidden)\n        else:\n            raise ValueError(\'rnn_unit must in (lstm, gru)!\')\n        return fw_cell, bw_cell\n\n    def fit(self, data_dict, dev_size=0.2, seed=1337):\n        """"""\n        \xe8\xae\xad\xe7\xbb\x83\n        Args:\n            data_dict: dict, \xe9\x94\xae: \xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d(or \'label\'), \xe5\x80\xbc: np.array\n            dev_size: float, \xe5\xbc\x80\xe5\x8f\x91\xe9\x9b\x86\xe6\x89\x80\xe5\x8d\xa0\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\xef\xbc\x8cdefault is 0.2\n\n            batch_size: int\n            seed: int, for shuffle data\n        """"""\n        data_train_dict, data_dev_dict = self.split_train_dev(data_dict, dev_size=dev_size)\n        self.saver = tf.train.Saver()  # save model\n        train_data_count = data_train_dict[\'label\'].shape[0]\n        nb_train = int(math.ceil(train_data_count / float(self._batch_size)))\n        min_dev_loss = 1000  # \xe5\x85\xa8\xe5\xb1\x80\xe6\x9c\x80\xe5\xb0\x8fdev loss, for early stopping)\n        current_patience = 0  # for early stopping\n        for step in range(self._nb_epoch):\n            print(\'Epoch %d / %d:\' % (step+1, self._nb_epoch))\n\n            # shuffle train data\n            data_list = [data_train_dict[\'label\']]\n            [data_list.append(data_train_dict[name]) for name in self._feature_names]\n            shuffle_matrix(*data_list, seed=seed)\n\n            # train\n            train_loss, l2_loss = 0., 0.\n            for i in tqdm(range(nb_train)):\n                feed_dict = dict()\n                batch_indices = np.arange(i * self._batch_size, (i + 1) * self._batch_size) \\\n                    if (i+1)*self._batch_size <= train_data_count else \\\n                    np.arange(i * self._batch_size, train_data_count)\n                # feature feed and dropout feed\n                for feature_name in self._feature_names:  # features\n                    # feature\n                    batch_data = data_train_dict[feature_name][batch_indices]\n                    item = {self.input_feature_ph_dict[feature_name]: batch_data}\n                    feed_dict.update(item)\n                    # dropout\n                    dropout_rate = self._feature_weight_dropout_dict[feature_name]\n                    item = {self.weight_dropout_ph_dict[feature_name]: dropout_rate}\n                    feed_dict.update(item)\n                if self._use_char_feature:\n                    batch_data = data_train_dict[\'char\'][batch_indices]\n                    item = {self.input_feature_ph_dict[\'char\']: batch_data}\n                    feed_dict.update(item)\n                    item = {self.cnn_dropout_rate_ph: self._cnn_dropout_rate}\n                    feed_dict.update(item)\n                feed_dict.update(\n                    {\n                        self.dropout_rate_ph: self._dropout_rate,\n                        self.rnn_dropout_rate_ph: self._rnn_dropout,\n                    })\n                # label feed\n                batch_label = data_train_dict[\'label\'][batch_indices]\n                feed_dict.update({self.input_label_ph: batch_label})\n\n                _, loss, l2_loss = self.sess.run([self.train_op, self.loss, self.l2_loss], feed_dict=feed_dict)\n                train_loss += loss\n            train_loss /= float(nb_train)\n\n            # \xe8\xae\xa1\xe7\xae\x97\xe5\x9c\xa8\xe5\xbc\x80\xe5\x8f\x91\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84loss\n            dev_loss = self.evaluate(data_dev_dict)\n\n            print(\'train loss: %f, dev loss: %f, l2 loss: %f\' % (train_loss, dev_loss, l2_loss))\n\n            # \xe6\xa0\xb9\xe6\x8d\xaedev\xe4\xb8\x8a\xe7\x9a\x84\xe8\xa1\xa8\xe7\x8e\xb0\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n            if not self._path_model:\n                continue\n            if dev_loss < min_dev_loss:\n                min_dev_loss = dev_loss\n                current_patience = 0\n                # save model\n                self.saver.save(self.sess, self._path_model)\n                print(\'model has saved to %s!\' % self._path_model)\n            else:\n                current_patience += 1\n                print(\'no improvement, current patience: %d / %d\' %\n                      (current_patience, self._train_max_patience))\n                if self._train_max_patience and current_patience >= self._train_max_patience:\n                    print(\'\\nfinished training! (early stopping, max patience: %d)\'\n                          % self._train_max_patience)\n                    return\n        print(\'\\nfinished training!\')\n        return\n\n    def split_train_dev(self, data_dict, dev_size=0.2):\n        """"""\n        \xe5\x88\x92\xe5\x88\x86\xe4\xb8\xba\xe5\xbc\x80\xe5\x8f\x91\xe9\x9b\x86\xe5\x92\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\n        Args:\n            data_dict: dict, \xe9\x94\xae: \xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d(or \'label\'), \xe5\x80\xbc: np.array\n            dev_size: float, \xe5\xbc\x80\xe5\x8f\x91\xe9\x9b\x86\xe6\x89\x80\xe5\x8d\xa0\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\xef\xbc\x8cdefault is 0.2\n        Returns:\n            data_train_dict, data_dev_dict: same type as data_dict\n        """"""\n        data_train_dict, data_dev_dict = dict(), dict()\n        for name in data_dict.keys():\n            boundary = int((1.-dev_size) * data_dict[name].shape[0])\n            data_train_dict[name] = data_dict[name][:boundary]\n            data_dev_dict[name] = data_dict[name][boundary:]\n        return data_train_dict, data_dev_dict\n\n    def evaluate(self, data_dict):\n        """"""\n        \xe8\xae\xa1\xe7\xae\x97loss\n        Args:\n            data_dict: dict\n        Return:\n            loss: float\n        """"""\n        data_count = data_dict[\'label\'].shape[0]\n        nb_eval = int(math.ceil(data_count / float(self._batch_size)))\n        eval_loss = 0.\n        for i in range(nb_eval):\n            feed_dict = dict()\n            batch_indices = np.arange(i * self._batch_size, (i + 1) * self._batch_size) \\\n                if (i+1)*self._batch_size <= data_count else \\\n                np.arange(i * self._batch_size, data_count)\n            for feature_name in self._feature_names:  # features and dropout\n                batch_data = data_dict[feature_name][batch_indices]\n                item = {self.input_feature_ph_dict[feature_name]: batch_data}\n                feed_dict.update(item)\n                # dropout\n                item = {self.weight_dropout_ph_dict[feature_name]: 0.}\n                feed_dict.update(item)\n            if self._use_char_feature:\n                batch_data = data_dict[\'char\'][batch_indices]\n                item = {self.input_feature_ph_dict[\'char\']: batch_data}\n                feed_dict.update(item)\n                item = {self.cnn_dropout_rate_ph: 0.}\n                feed_dict.update(item)\n            feed_dict.update({self.dropout_rate_ph: 0., self.rnn_dropout_rate_ph: 0.})\n            # label feed\n            batch_label = data_dict[\'label\'][batch_indices]\n            feed_dict.update({self.input_label_ph: batch_label})\n\n            loss = self.sess.run(self.loss, feed_dict=feed_dict)\n            eval_loss += loss\n        eval_loss /= float(nb_eval)\n        return eval_loss\n\n    def predict(self, data_test_dict):\n        """"""\n        \xe6\xa0\xb9\xe6\x8d\xae\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xa0\x87\xe8\xae\xb0\xe6\x95\xb0\xe6\x8d\xae\n        Args:\n            data_test_dict: dict\n        Return:\n            pass\n        """"""\n        print(\'predicting...\')\n        data_count = data_test_dict[self._feature_names[0]].shape[0]\n        nb_test = int(math.ceil(data_count / float(self._batch_size)))\n        result_sequences = []  # \xe6\xa0\x87\xe8\xae\xb0\xe7\xbb\x93\xe6\x9e\x9c\n        for i in tqdm(range(nb_test)):\n            feed_dict = dict()\n            batch_indices = np.arange(i * self._batch_size, (i + 1) * self._batch_size) \\\n                if (i+1)*self._batch_size <= data_count else \\\n                np.arange(i * self._batch_size, data_count)\n            for feature_name in self._feature_names:  # features and dropout\n                batch_data = data_test_dict[feature_name][batch_indices]\n                item = {self.input_feature_ph_dict[feature_name]: batch_data}\n                feed_dict.update(item)\n                # dropout\n                item = {self.weight_dropout_ph_dict[feature_name]: 0.}\n                feed_dict.update(item)\n            if self._use_char_feature:\n                batch_data = data_test_dict[\'char\'][batch_indices]\n                item = {self.input_feature_ph_dict[\'char\']: batch_data}\n                feed_dict.update(item)\n                item = {self.cnn_dropout_rate_ph: 0.}\n                feed_dict.update(item)\n            feed_dict.update({self.dropout_rate_ph: 0., self.rnn_dropout_rate_ph: 0.})\n\n            if self._use_crf:\n                logits, sequence_actual_length, transition_params = self.sess.run(\n                    [self.logits, self.sequence_actual_length, self.transition_params], feed_dict=feed_dict)\n                for logit, seq_len in zip(logits, sequence_actual_length):\n                    logit_actual = logit[:seq_len]\n                    viterbi_sequence, _ = tf.contrib.crf.viterbi_decode(\n                        logit_actual, transition_params)\n                    result_sequences.append(viterbi_sequence)\n            else:\n                logits, sequence_actual_length = self.sess.run(\n                    [self.logits, self.sequence_actual_length], feed_dict=feed_dict)\n                for logit, seq_len in zip(logits, sequence_actual_length):\n                    logit_actual = logit[:seq_len]\n                    sequence = np.argmax(logit_actual, axis=-1)\n                    result_sequences.append(sequence)\n        print(\'\xe5\x85\xb1\xe6\xa0\x87\xe8\xae\xb0\xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0: %d\' % data_count)\n        return result_sequences\n\n    def compute_loss(self):\n        """"""\n        \xe8\xae\xa1\xe7\xae\x97loss\n\n        Return:\n            loss: scalar\n        """"""\n        if not self._use_crf:\n            labels = tf.reshape(\n                tf.contrib.layers.one_hot_encoding(\n                    tf.reshape(self.input_label_ph, [-1]), num_classes=self._nb_classes),\n                shape=[-1, self._sequence_length, self._nb_classes])\n            logits = tf.nn.softmax(self.logits, dim=-1)\n            cross_entropy = -tf.reduce_sum(labels * tf.log(logits), axis=2)\n            mask = tf.sign(tf.reduce_max(tf.abs(labels), axis=2))\n            cross_entropy_masked = tf.reduce_sum(\n                cross_entropy*mask, axis=1) / tf.cast(self.sequence_actual_length, tf.float32)\n            return tf.reduce_mean(cross_entropy_masked)\n        else:\n            log_likelihood, self.transition_params = tf.contrib.crf.crf_log_likelihood(\n                self.logits, self.input_label_ph, self.sequence_actual_length)\n            return tf.reduce_mean(-log_likelihood)\n'"
preprocessing.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n__author__ = \'jxliu.nlper@gmail.com\'\n""""""\n    \xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n""""""\nimport sys\nimport yaml\nimport pickle\nimport codecs\nimport numpy as np\nfrom collections import defaultdict\nfrom utils import create_dictionary, load_embed_from_txt\n\n\ndef build_vocabulary(path_data, path_vocs_dict, min_counts_dict, columns,\n                     sequence_len_pt=98, use_char_featrue=False, word_len_pt=98):\n    """"""\n    \xe6\x9e\x84\xe5\xbb\xba\xe5\xad\x97\xe5\x85\xb8\n    Args:\n        path_data: str, \xe6\x95\xb0\xe6\x8d\xae\xe8\xb7\xaf\xe5\xbe\x84\n        path_vocs_dict: dict, \xe5\xad\x97\xe5\x85\xb8\xe5\xad\x98\xe6\x94\xbe\xe8\xb7\xaf\xe5\xbe\x84\n        min_counts_dict: dict, item\xe6\x9c\x80\xe5\xb0\x91\xe5\x87\xba\xe7\x8e\xb0\xe6\xac\xa1\xe6\x95\xb0\n        columns: list of str, \xe6\xaf\x8f\xe4\xb8\x80\xe5\x88\x97\xe7\x9a\x84\xe5\x90\x8d\xe7\xa7\xb0\n        sequence_len_pt: int\xef\xbc\x8c\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\xe7\x99\xbe\xe5\x88\x86\xe4\xbd\x8d\n        use_char_featrue: bool\xef\xbc\x8c\xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8\xe5\xad\x97\xe7\xac\xa6\xe7\x89\xb9\xe5\xbe\x81(\xe9\x92\x88\xe5\xaf\xb9\xe8\x8b\xb1\xe6\x96\x87)\n        word_len_pt: int\xef\xbc\x8c\xe5\x8d\x95\xe8\xaf\x8d\xe9\x95\xbf\xe5\xba\xa6\xe7\x99\xbe\xe5\x88\x86\xe4\xbd\x8d\n    Returns:\n        voc_size_1, voc_size_2, ...: int\n        sequence_length: \xe5\xba\x8f\xe5\x88\x97\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n    """"""\n    print(\'building vocs...\')\n    file_data = codecs.open(path_data, \'r\', encoding=\'utf-8\')\n    line = file_data.readline()\n\n    sequence_length_list = []  # \xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\n    # \xe8\xae\xa1\xe6\x95\xb0items\n    feature_item_dict_list = []\n    for i in range(len(columns)):\n        feature_item_dict_list.append(defaultdict(int))\n    # char feature\n    if use_char_featrue:\n        char_dict = defaultdict(int)\n        word_length_list = []  # \xe5\x8d\x95\xe8\xaf\x8d\xe9\x95\xbf\xe5\xba\xa6\n    sequence_length = 0\n    sentence_count = 0  # \xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0\n    while line:\n        line = line.rstrip()\n        if not line:\n            sentence_count += 1\n            sys.stdout.write(\'\xe5\xbd\x93\xe5\x89\x8d\xe5\xa4\x84\xe7\x90\x86\xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0: %d\\r\' % sentence_count)\n            sys.stdout.flush()\n            line = file_data.readline()\n            sequence_length_list.append(sequence_length)\n            sequence_length = 0\n            continue\n        items = line.split(\'\\t\')\n        sequence_length += 1\n        # print(items)\n        for i in range(len(columns)-1):\n            feature_item_dict_list[i][items[i]] += 1\n        # label\n        feature_item_dict_list[-1][items[-1]] += 1\n        # char feature\n        if use_char_featrue:\n            for c in items[0]:\n                char_dict[c] += 1\n            word_length_list.append(len(items[0]))\n        line = file_data.readline()\n    file_data.close()\n    # last instance\n    if sequence_length != 0:\n        sentence_count += 1\n        sys.stdout.write(\'\xe5\xbd\x93\xe5\x89\x8d\xe5\xa4\x84\xe7\x90\x86\xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0: %d\\r\' % sentence_count)\n        sequence_length_list.append(sequence_length)\n    print()\n\n    # \xe5\x86\x99\xe5\x85\xa5\xe6\x96\x87\xe4\xbb\xb6\n    voc_sizes = []\n    if use_char_featrue:  # char feature\n        size = create_dictionary(\n            char_dict, path_vocs_dict[\'char\'], start=2,\n            sort=True, min_count=min_counts_dict[\'char\'], overwrite=True)\n        voc_sizes.append(size)\n    for i, name in enumerate(columns):\n        start = 1 if i == len(columns) - 1 else 2\n        size = create_dictionary(\n            feature_item_dict_list[i], path_vocs_dict[name], start=start,\n            sort=True, min_count=min_counts_dict[name], overwrite=True)\n        print(\'voc: %s, size: %d\' % (path_vocs_dict[name], size))\n        voc_sizes.append(size)\n\n    print(\'\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\xe5\x88\x86\xe5\xb8\x83:\')\n    sentence_length = -1\n    option_len_pt = [90, 95, 98, 100]\n    if sequence_len_pt not in option_len_pt:\n        option_len_pt.append(sequence_len_pt)\n    for per in sorted(option_len_pt):\n        tmp = int(np.percentile(sequence_length_list, per))\n        if per == sequence_len_pt:\n            sentence_length = tmp\n            print(\'%3d percentile: %d (default)\' % (per, tmp))\n        else:\n            print(\'%3d percentile: %d\' % (per, tmp))\n    if use_char_featrue:\n        print(\'\xe5\x8d\x95\xe8\xaf\x8d\xe9\x95\xbf\xe5\xba\xa6\xe5\x88\x86\xe5\xb8\x83:\')\n        word_length = -1\n        option_len_pt = [90, 95, 98, 100]\n        if word_len_pt not in option_len_pt:\n            option_len_pt.append(word_len_pt)\n        for per in sorted(option_len_pt):\n            tmp = int(np.percentile(word_length_list, per))\n            if per == word_len_pt:\n                word_length = tmp\n                print(\'%3d percentile: %d (default)\' % (per, tmp))\n            else:\n                print(\'%3d percentile: %d\' % (per, tmp))\n\n    print(\'done!\')\n    lengths = [sentence_length]\n    if use_char_featrue:\n        lengths.append(word_length)\n    return voc_sizes, lengths\n\n\ndef main():\n    print(\'preprocessing...\')\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\n    with open(\'./config.yml\') as file_config:\n        config = yaml.load(file_config)\n\n    # \xe6\x9e\x84\xe5\xbb\xba\xe5\xad\x97\xe5\x85\xb8(\xe5\x90\x8c\xe6\x97\xb6\xe8\x8e\xb7\xe5\x8f\x96\xe8\xaf\x8d\xe8\xa1\xa8size\xef\xbc\x8c\xe5\xba\x8f\xe5\x88\x97\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6)\n    columns = config[\'model_params\'][\'feature_names\'] + [\'label\']\n    min_counts_dict, path_vocs_dict = defaultdict(int), dict()\n    feature_names = config[\'model_params\'][\'feature_names\']\n    for feature_name in feature_names:\n        min_counts_dict[feature_name] = \\\n            config[\'data_params\'][\'voc_params\'][feature_name][\'min_count\']\n        path_vocs_dict[feature_name] = \\\n            config[\'data_params\'][\'voc_params\'][feature_name][\'path\']\n    path_vocs_dict[\'label\'] = \\\n        config[\'data_params\'][\'voc_params\'][\'label\'][\'path\']\n\n    # char feature\n    min_counts_dict[\'char\'] = config[\'data_params\'][\'voc_params\'][\'char\'][\'min_count\']\n    path_vocs_dict[\'char\'] = config[\'data_params\'][\'voc_params\'][\'char\'][\'path\']\n\n    sequence_len_pt = config[\'model_params\'][\'sequence_len_pt\']\n    use_char_feature = config[\'model_params\'][\'use_char_feature\']\n    word_len_pt = config[\'model_params\'][\'word_len_pt\']\n    voc_sizes, lengths = build_vocabulary(\n        path_data=config[\'data_params\'][\'path_train\'], columns=columns,\n        min_counts_dict=min_counts_dict, path_vocs_dict=path_vocs_dict,\n        sequence_len_pt=sequence_len_pt, use_char_featrue=use_char_feature,\n        word_len_pt=word_len_pt)\n    if not use_char_feature:\n        sequence_length = lengths[0]\n    else:\n        sequence_length, word_length = lengths[:]\n\n    # \xe6\x9e\x84\xe5\xbb\xbaembedding\xe8\xa1\xa8\n    feature_dim_dict = dict()  # \xe5\xad\x98\xe5\x82\xa8\xe6\xaf\x8f\xe4\xb8\xaafeature\xe7\x9a\x84dim\n    for i, feature_name in enumerate(feature_names):\n        path_pre_train = config[\'model_params\'][\'embed_params\'][feature_name][\'path_pre_train\']\n        if not path_pre_train:\n            if i == 0:\n                feature_dim_dict[feature_name] = 64\n            else:\n                feature_dim_dict[feature_name] = 32\n            continue\n        path_pkl = config[\'model_params\'][\'embed_params\'][feature_name][\'path\']\n        path_voc = config[\'data_params\'][\'voc_params\'][feature_name][\'path\']\n        with open(path_voc, \'rb\') as file_r:\n            voc = pickle.load(file_r)\n        embedding_dict, vec_dim = load_embed_from_txt(path_pre_train)\n        feature_dim_dict[feature_name] = vec_dim\n        embedding_matrix = np.zeros((len(voc.keys())+2, vec_dim), dtype=\'float32\')\n        for item in voc:\n            if item in embedding_dict:\n                embedding_matrix[voc[item], :] = embedding_dict[item]\n            else:\n                embedding_matrix[voc[item], :] = np.random.uniform(-0.25, 0.25, size=(vec_dim))\n        with open(path_pkl, \'wb\') as file_w:\n            pickle.dump(embedding_matrix, file_w)\n\n    # \xe4\xbf\xae\xe6\x94\xb9config\xe4\xb8\xad\xe5\x90\x84\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81\xe7\x9a\x84shape\xef\xbc\x8cembedding\xe5\xa4\xa7\xe5\xb0\x8f\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba[64, 32, 32, ...]\n    if use_char_feature:\n        char_voc_size = voc_sizes.pop(0)\n    label_size = voc_sizes[-1]\n    voc_sizes = voc_sizes[:-1]\n    # \xe4\xbf\xae\xe6\x94\xb9nb_classes\n    config[\'model_params\'][\'nb_classes\'] = label_size\n    # \xe4\xbf\xae\xe6\x94\xb9embedding\xe8\xa1\xa8\xe7\x9a\x84shape\n    for i, feature_name in enumerate(feature_names):\n        if i == 0:\n            config[\'model_params\'][\'embed_params\'][feature_name][\'shape\'] = \\\n                [voc_sizes[i], feature_dim_dict[feature_name]]\n        else:\n            config[\'model_params\'][\'embed_params\'][feature_name][\'shape\'] = \\\n                [voc_sizes[i], feature_dim_dict[feature_name]]\n    # \xe4\xbf\xae\xe6\x94\xb9char\xe8\xa1\xa8\xe7\x9a\x84embedding\n    if use_char_feature:\n        # \xe9\xbb\x98\xe8\xae\xa416\xe7\xbb\xb4\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xae\xe4\xbb\xbb\xe5\x8a\xa1\xe8\xb0\x83\xe6\x95\xb4\n        config[\'model_params\'][\'embed_params\'][\'char\'][\'shape\'] = \\\n            [char_voc_size, 16]\n        config[\'model_params\'][\'word_length\'] = word_length\n    # \xe4\xbf\xae\xe6\x94\xb9\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\n    config[\'model_params\'][\'sequence_length\'] = sequence_length\n    # \xe5\x86\x99\xe5\x85\xa5\xe6\x96\x87\xe4\xbb\xb6\n    with codecs.open(\'./config.yml\', \'w\', encoding=\'utf-8\') as file_w:\n        yaml.dump(config, file_w)\n\n    print(\'all done!\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
test.py,1,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n__author__ = \'jxliu.nlper@gmail.com\'\n""""""\n    \xe6\xa0\x87\xe8\xae\xb0\xe6\x96\x87\xe4\xbb\xb6\n""""""\nimport codecs\nimport yaml\nimport pickle\nimport tensorflow as tf\nfrom load_data import load_vocs, init_data\nfrom model import SequenceLabelingModel\n\n\ndef main():\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\n    with open(\'./config.yml\') as file_config:\n        config = yaml.load(file_config)\n\n    feature_names = config[\'model_params\'][\'feature_names\']\n    use_char_feature = config[\'model_params\'][\'use_char_feature\']\n\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96embedding shape, dropouts, \xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84embedding\xe4\xb9\x9f\xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96)\n    feature_weight_shape_dict, feature_weight_dropout_dict, \\\n        feature_init_weight_dict = dict(), dict(), dict()\n    for feature_name in feature_names:\n        feature_weight_shape_dict[feature_name] = \\\n            config[\'model_params\'][\'embed_params\'][feature_name][\'shape\']\n        feature_weight_dropout_dict[feature_name] = \\\n            config[\'model_params\'][\'embed_params\'][feature_name][\'dropout_rate\']\n        path_pre_train = config[\'model_params\'][\'embed_params\'][feature_name][\'path\']\n        if path_pre_train:\n            with open(path_pre_train, \'rb\') as file_r:\n                feature_init_weight_dict[feature_name] = pickle.load(file_r)\n    # char embedding shape\n    if use_char_feature:\n        feature_weight_shape_dict[\'char\'] = \\\n            config[\'model_params\'][\'embed_params\'][\'char\'][\'shape\']\n        conv_filter_len_list = config[\'model_params\'][\'conv_filter_len_list\']\n        conv_filter_size_list = config[\'model_params\'][\'conv_filter_size_list\']\n    else:\n        conv_filter_len_list = None\n        conv_filter_size_list = None\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\n\n    # \xe5\x8a\xa0\xe8\xbd\xbdvocs\n    path_vocs = []\n    if use_char_feature:\n        path_vocs.append(config[\'data_params\'][\'voc_params\'][\'char\'][\'path\'])\n    for feature_name in feature_names:\n        path_vocs.append(config[\'data_params\'][\'voc_params\'][feature_name][\'path\'])\n    path_vocs.append(config[\'data_params\'][\'voc_params\'][\'label\'][\'path\'])\n    vocs = load_vocs(path_vocs)\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\n    sep_str = config[\'data_params\'][\'sep\']\n    assert sep_str in [\'table\', \'space\']\n    sep = \'\\t\' if sep_str == \'table\' else \' \'\n    max_len = config[\'model_params\'][\'sequence_length\']\n    word_len = config[\'model_params\'][\'word_length\']\n    data_dict = init_data(\n        path=config[\'data_params\'][\'path_test\'], feature_names=feature_names, sep=sep,\n        vocs=vocs, max_len=max_len, model=\'test\', use_char_feature=use_char_feature,\n        word_len=word_len)\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\n    model = SequenceLabelingModel(\n        sequence_length=config[\'model_params\'][\'sequence_length\'],\n        nb_classes=config[\'model_params\'][\'nb_classes\'],\n        nb_hidden=config[\'model_params\'][\'bilstm_params\'][\'num_units\'],\n        num_layers=config[\'model_params\'][\'bilstm_params\'][\'num_layers\'],\n        feature_weight_shape_dict=feature_weight_shape_dict,\n        feature_init_weight_dict=feature_init_weight_dict,\n        feature_weight_dropout_dict=feature_weight_dropout_dict,\n        dropout_rate=config[\'model_params\'][\'dropout_rate\'],\n        nb_epoch=config[\'model_params\'][\'nb_epoch\'], feature_names=feature_names,\n        batch_size=config[\'model_params\'][\'batch_size\'],\n        train_max_patience=config[\'model_params\'][\'max_patience\'],\n        use_crf=config[\'model_params\'][\'use_crf\'],\n        l2_rate=config[\'model_params\'][\'l2_rate\'],\n        rnn_unit=config[\'model_params\'][\'rnn_unit\'],\n        learning_rate=config[\'model_params\'][\'learning_rate\'],\n        use_char_feature=use_char_feature,\n        conv_filter_size_list=conv_filter_size_list,\n        conv_filter_len_list=conv_filter_len_list,\n        word_length=word_len,\n        path_model=config[\'model_params\'][\'path_model\'])\n    saver = tf.train.Saver()\n    saver.restore(model.sess, config[\'model_params\'][\'path_model\'])\n\n    # \xe6\xa0\x87\xe8\xae\xb0\n    result_sequences = model.predict(data_dict)\n\n    # \xe5\x86\x99\xe5\x85\xa5\xe6\x96\x87\xe4\xbb\xb6\n    label_voc = dict()\n    for key in vocs[-1]:\n        label_voc[vocs[-1][key]] = key\n    with codecs.open(config[\'data_params\'][\'path_test\'], \'r\', encoding=\'utf-8\') as file_r:\n        sentences = file_r.read().strip().split(\'\\n\\n\')\n    file_result = codecs.open(\n        config[\'data_params\'][\'path_result\'], \'w\', encoding=\'utf-8\')\n    for i, sentence in enumerate(sentences):\n        for j, item in enumerate(sentence.split(\'\\n\')):\n            if j < len(result_sequences[i]):\n                file_result.write(\'%s\\t%s\\n\' % (item, label_voc[result_sequences[i][j]]))\n            else:\n                file_result.write(\'%s\\tO\\n\' % item)\n        file_result.write(\'\\n\')\n\n    file_result.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n__author__ = \'jxliu.nlper@gmail.com\'\n""""""\n    \xe8\xae\xad\xe7\xbb\x83NER\xe6\xa8\xa1\xe5\x9e\x8b\n""""""\nimport yaml\nimport pickle\nfrom load_data import load_vocs, init_data\nfrom model import SequenceLabelingModel\n\n\ndef main():\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\n    with open(\'./config.yml\') as file_config:\n        config = yaml.load(file_config)\n\n    feature_names = config[\'model_params\'][\'feature_names\']\n    use_char_feature = config[\'model_params\'][\'use_char_feature\']\n\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96embedding shape, dropouts, \xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84embedding\xe4\xb9\x9f\xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96)\n    feature_weight_shape_dict, feature_weight_dropout_dict, \\\n        feature_init_weight_dict = dict(), dict(), dict()\n    for feature_name in feature_names:\n        feature_weight_shape_dict[feature_name] = \\\n            config[\'model_params\'][\'embed_params\'][feature_name][\'shape\']\n        feature_weight_dropout_dict[feature_name] = \\\n            config[\'model_params\'][\'embed_params\'][feature_name][\'dropout_rate\']\n        path_pre_train = config[\'model_params\'][\'embed_params\'][feature_name][\'path\']\n        if path_pre_train:\n            with open(path_pre_train, \'rb\') as file_r:\n                feature_init_weight_dict[feature_name] = pickle.load(file_r)\n    # char embedding shape\n    if use_char_feature:\n        feature_weight_shape_dict[\'char\'] = \\\n            config[\'model_params\'][\'embed_params\'][\'char\'][\'shape\']\n        conv_filter_len_list = config[\'model_params\'][\'conv_filter_len_list\']\n        conv_filter_size_list = config[\'model_params\'][\'conv_filter_size_list\']\n    else:\n        conv_filter_len_list = None\n        conv_filter_size_list = None\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\n\n    # \xe5\x8a\xa0\xe8\xbd\xbdvocs\n    path_vocs = []\n    if use_char_feature:\n        path_vocs.append(config[\'data_params\'][\'voc_params\'][\'char\'][\'path\'])\n    for feature_name in feature_names:\n        path_vocs.append(config[\'data_params\'][\'voc_params\'][feature_name][\'path\'])\n    path_vocs.append(config[\'data_params\'][\'voc_params\'][\'label\'][\'path\'])\n    vocs = load_vocs(path_vocs)\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\n    sep_str = config[\'data_params\'][\'sep\']\n    assert sep_str in [\'table\', \'space\']\n    sep = \'\\t\' if sep_str == \'table\' else \' \'\n    max_len = config[\'model_params\'][\'sequence_length\']\n    word_len = config[\'model_params\'][\'word_length\']\n    data_dict = init_data(\n        path=config[\'data_params\'][\'path_train\'], feature_names=feature_names, sep=sep,\n        vocs=vocs, max_len=max_len, model=\'train\', use_char_feature=use_char_feature,\n        word_len=word_len)\n\n    # \xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\n    model = SequenceLabelingModel(\n        sequence_length=config[\'model_params\'][\'sequence_length\'],\n        nb_classes=config[\'model_params\'][\'nb_classes\'],\n        nb_hidden=config[\'model_params\'][\'bilstm_params\'][\'num_units\'],\n        num_layers=config[\'model_params\'][\'bilstm_params\'][\'num_layers\'],\n        rnn_dropout=config[\'model_params\'][\'bilstm_params\'][\'rnn_dropout\'],\n        feature_weight_shape_dict=feature_weight_shape_dict,\n        feature_init_weight_dict=feature_init_weight_dict,\n        feature_weight_dropout_dict=feature_weight_dropout_dict,\n        dropout_rate=config[\'model_params\'][\'dropout_rate\'],\n        nb_epoch=config[\'model_params\'][\'nb_epoch\'], feature_names=feature_names,\n        batch_size=config[\'model_params\'][\'batch_size\'],\n        train_max_patience=config[\'model_params\'][\'max_patience\'],\n        use_crf=config[\'model_params\'][\'use_crf\'],\n        l2_rate=config[\'model_params\'][\'l2_rate\'],\n        rnn_unit=config[\'model_params\'][\'rnn_unit\'],\n        learning_rate=config[\'model_params\'][\'learning_rate\'],\n        clip=config[\'model_params\'][\'clip\'],\n        use_char_feature=use_char_feature,\n        conv_filter_size_list=conv_filter_size_list,\n        conv_filter_len_list=conv_filter_len_list,\n        cnn_dropout_rate=config[\'model_params\'][\'conv_dropout\'],\n        word_length=word_len,\n        path_model=config[\'model_params\'][\'path_model\'])\n\n    model.fit(\n        data_dict=data_dict, dev_size=config[\'model_params\'][\'dev_size\'])\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils.py,8,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n__author__ = \'jxliu.nlper@gmail.com\'\nimport os\nimport codecs\nimport pickle\nimport numpy as np\nimport tensorflow as tf\n\n\ndef uniform_tensor(shape, name, dtype=\'float32\'):\n    """"""\n    \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96tensor\n    Args:\n        shape: tuple\n        name: str\n    Return:\n        tensor\n    """"""\n    return tf.random_uniform(shape=shape, minval=-1.0, maxval=1.0, dtype=tf.float32, name=name)\n\n\ndef get_sequence_actual_length(tensor):\n    """"""\n    \xe8\x8e\xb7\xe5\x8f\x96tensor\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe9\x95\xbf\xe5\xba\xa6\n    Args:\n        tensor: a 2d tensor with shape (batch_size, max_len)\n    Return:\n        actual_len: a vector with length [batch_size]\n    """"""\n    actual_length = tf.reduce_sum(tf.sign(tensor), axis=1)\n    return tf.cast(actual_length, tf.int32)\n\n\ndef zero_nil_slot(t, name=None):\n    """"""\n    Overwrite the nil_slot (first 1 rows) of the input Tensor with zeros.\n    Args:\n        t: 2D tensor\n        name: str\n    Returns:\n        Same shape as t\n    """"""\n    with tf.name_scope(\'zero_nil_slot\'):\n        s = tf.shape(t)[1]\n        z = tf.zeros([1, s], dtype=tf.float32)\n        return tf.concat(\n            axis=0, name=name,\n            values=[z, tf.slice(t, [1, 0], [-1, -1])])\n\n\ndef shuffle_matrix(*args, **kw):\n    """"""\n    shuffle \xe5\x8f\xa5\xe7\x9f\xa9\xe9\x98\xb5\n    """"""\n    seed = kw[\'seed\'] if \'seed\' in kw else 1337\n    for arg in args:\n        np.random.seed(seed)\n        np.random.shuffle(arg)\n\n\ndef create_dictionary(token_dict, dic_path, start=0, sort=False,\n                      min_count=None, lower=False, overwrite=False):\n    """"""\n    \xe6\x9e\x84\xe5\xbb\xba\xe5\xad\x97\xe5\x85\xb8\xef\xbc\x8c\xe5\xb9\xb6\xe5\xb0\x86\xe6\x9e\x84\xe5\xbb\xba\xe7\x9a\x84\xe5\xad\x97\xe5\x85\xb8\xe5\x86\x99\xe5\x85\xa5pkl\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\n    Args:\n        token_dict: dict, [token_1:count_1, token_2:count_2, ...]\n        dic_path: \xe9\x9c\x80\xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84(\xe4\xbb\xa5pkl\xe7\xbb\x93\xe5\xb0\xbe)\n        start: int, voc\xe8\xb5\xb7\xe5\xa7\x8b\xe4\xb8\x8b\xe6\xa0\x87\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba0\n        sort: bool, \xe6\x98\xaf\xe5\x90\xa6\xe6\x8c\x89\xe9\xa2\x91\xe7\x8e\x87\xe6\x8e\x92\xe5\xba\x8f, \xe8\x8b\xa5\xe4\xb8\xbaFalse\xef\xbc\x8c\xe5\x88\x99\xe6\x8c\x89items\xe6\x8e\x92\xe5\xba\x8f\n        min_count: int, \xe8\xaf\x8d\xe6\x9c\x80\xe5\xb0\x91\xe5\x87\xba\xe7\x8e\xb0\xe6\xac\xa1\xe6\x95\xb0\xef\xbc\x8c\xe4\xbd\x8e\xe4\xba\x8e\xe6\xad\xa4\xe5\x80\xbc\xe7\x9a\x84\xe8\xaf\x8d\xe8\xa2\xab\xe8\xbf\x87\xe6\xbb\xa4\n        lower: bool, \xe6\x98\xaf\xe5\x90\xa6\xe8\xbd\xac\xe4\xb8\xba\xe5\xb0\x8f\xe5\x86\x99\n        overwrite: bool, \xe6\x98\xaf\xe5\x90\xa6\xe8\xa6\x86\xe7\x9b\x96\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\n    Returns:\n        voc size: int\n    """"""\n    if os.path.exists(dic_path) and not overwrite:\n        return 0\n    voc = dict()\n    if sort:\n        # sort\n        token_list = sorted(token_dict.items(), key=lambda d: d[1], reverse=True)\n        for i, item in enumerate(token_list):\n            if min_count and item[1] < min_count:\n                continue\n            index = i + start\n            key = item[0]\n            voc[key] = index\n    else:  # \xe6\x8c\x89items\xe6\x8e\x92\xe5\xba\x8f\n        if min_count:\n            items = sorted([item[0] for item in token_dict.items() if item[1] >= min_count])\n        else:\n            items = sorted([item[0] for item in token_dict.items()])\n        for i, item in enumerate(items):\n            item = item if not lower else item.lower()\n            index = i + start\n            voc[item] = index\n    # \xe5\x86\x99\xe5\x85\xa5\xe6\x96\x87\xe4\xbb\xb6\n    file = open(dic_path, \'wb\')\n    pickle.dump(voc, file)\n    file.close()\n    return len(voc.keys()) + start\n\n\ndef map_item2id(items, voc, max_len, none_word=1, lower=False, init_value=0, allow_error=True):\n    """"""\n    \xe5\xb0\x86word/pos\xe7\xad\x89\xe6\x98\xa0\xe5\xb0\x84\xe4\xb8\xbaid\n    Args:\n        items: list, \xe5\xbe\x85\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\x97\xe8\xa1\xa8\n        voc: \xe8\xaf\x8d\xe8\xa1\xa8\n        max_len: int, \xe5\xba\x8f\xe5\x88\x97\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n        none_word: \xe6\x9c\xaa\xe7\x99\xbb\xe5\xbd\x95\xe8\xaf\x8d\xe6\xa0\x87\xe5\x8f\xb7,\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba0\n        lower: bool, \xe6\x98\xaf\xe5\x90\xa6\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe5\xb0\x8f\xe5\x86\x99\n        init_value: default is 0, \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x9a\x84\xe5\x80\xbc\n    Returns:\n        arr: np.array, dtype=int32, shape=[max_len,]\n    """"""\n    assert type(none_word) == int\n    arr = np.zeros((max_len,), dtype=\'int32\') + init_value\n    min_range = min(max_len, len(items))\n    for i in range(min_range):  # \xe8\x8b\xa5items\xe9\x95\xbf\xe5\xba\xa6\xe5\xa4\xa7\xe4\xba\x8emax_len\xef\xbc\x8c\xe5\x88\x99\xe8\xa2\xab\xe6\x88\xaa\xe6\x96\xad\n        item = items[i] if not lower else items[i].lower()\n        if allow_error:\n            arr[i] = voc[item] if item in voc else none_word\n        else:\n            arr[i] = voc[item]\n    return arr\n\n\ndef build_lookup_table(vec_dim, token2id_dict, token2vec_dict=None, token_voc_start=1):\n    """"""\n    \xe6\x9e\x84\xe5\xbb\xbalook-up table\n    Args:\n        vec_dim: int, \xe5\x90\x91\xe9\x87\x8f\xe7\xbb\xb4\xe5\xba\xa6\n        token2id_dict: dict, \xe9\x94\xae: token\xef\xbc\x8c\xe5\x80\xbc: id\n        token2vec_dict: dict, \xe9\x94\xae: token\xef\xbc\x8c\xe5\x80\xbc: np.array(\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f)\n        token_voc_start: int, \xe8\xb5\xb7\xe5\xa7\x8b\xe4\xbd\x8d\xe7\xbd\xae\n    Return:\n        token_weight: np.array, shape=(table_size, dim)\n        unknow_token_count: int, \xe6\x9c\xaa\xe7\x99\xbb\xe5\xbd\x95\xe8\xaf\x8d\xe6\x95\xb0\xe9\x87\x8f\n    """"""\n    unknow_token_count = 0\n    token_voc_size = len(token2id_dict.keys()) + token_voc_start\n\n    if token2vec_dict is None:  # \xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        token_weight = np.random.normal(size=(token_voc_size, vec_dim)).astype(\'float32\')\n        for i in range(token_voc_start):\n            token_weight[i, :] = 0.\n        return token_weight, 0\n\n    token_weight = np.zeros((token_voc_size, vec_dim), dtype=\'float32\')\n    for token in token2id_dict:\n        index = token2id_dict[token]\n        if token in token2vec_dict:\n            token_weight[index, :] = token2vec_dict[token]\n        else:\n            unknow_token_count += 1\n            random_vec = np.random.uniform(-0.25, 0.25, size=(vec_dim,)).astype(\'float32\')\n            token_weight[index, :] = random_vec\n\n    return token_weight, unknow_token_count\n\n\ndef embedding_txt2pkl(path_txt, path_pkl):\n    """"""\n    \xe5\xb0\x86txt\xe6\x96\x87\xe4\xbb\xb6\xe8\xbd\xac\xe4\xb8\xbapkl\xe6\x96\x87\xe4\xbb\xb6\n    Args:\n        path_txt: str, txt\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84word embedding\xe8\xb7\xaf\xe5\xbe\x84\n        path_pkl: pkl\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n    """"""\n    print(\'convert txt to pkl...\')\n    from gensim.models.keyedvectors import KeyedVectors\n    assert path_txt.endswith(\'txt\')\n    word_vectors = KeyedVectors.load_word2vec_format(path_txt, binary=False)\n    word_dict = {}\n    for word in word_vectors.vocab:\n        word_dict[word] = word_vectors[word]\n    with open(path_pkl, \'wb\') as file_w:\n        pickle.dump(word_dict, file_w)\n    print(\'.txt file has wrote to: %s!\\n - embedding dim is %d.\' %\n          (path_pkl, word_vectors.vector_size))\n\n\ndef load_embed_from_txt(path):\n    """"""\n    \xe8\xaf\xbb\xe5\x8f\x96txt\xe6\x96\x87\xe4\xbb\xb6\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84embedding\n    Args:\n        path: str, \xe8\xb7\xaf\xe5\xbe\x84\n        start: int, \xe4\xbb\x8estart\xe5\xbc\x80\xe5\xa7\x8b\xe8\xaf\xbb\xe5\x8f\x96, default is 1\n    Returns:\n        embed_dict: dict\n    """"""\n    file_r = codecs.open(path, \'r\', encoding=\'utf-8\')\n    line = file_r.readline()\n    voc_size, vec_dim = map(int, line.split(\' \'))\n    embedding = dict()\n    line = file_r.readline()\n    while line:\n        items = line.split(\' \')\n        item = items[0]\n        vec = np.array(items[1:], dtype=\'float32\')\n        embedding[item] = vec\n        line = file_r.readline()\n    return embedding, vec_dim\n'"
Utils/train_word2vec_model.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n    \xe4\xbd\xbf\xe7\x94\xa8gensim\xe8\xae\xad\xe7\xbb\x83word vectors\n    Usage:\n        python3 train_word2vec_model.py path_of_corpus path_of_model\n""""""\nimport logging\nimport os.path\nimport sys\nfrom gensim.models.word2vec import LineSentence\nfrom time import time\nfrom gensim.models import Word2Vec\n\n\nif __name__ == \'__main__\':\n    t0 = time()\n\n    program = os.path.basename(sys.argv[0])\n    logger = logging.getLogger(program)\n\n    logging.basicConfig(format=\'%(asctime)s: %(levelname)s: %(message)s\')\n    logging.root.setLevel(level=logging.INFO)\n    logger.info(""running %s"" % \' \'.join(sys.argv))\n\n    # check and process input arguments\n    if len(sys.argv) < 3:\n        print(globals()[\'__doc__\'] % locals())\n        sys.exit(1)\n    inp, outp = sys.argv[1:3]  # corpus path and path to save model\n\n    model = Word2Vec(\n        sg=1, sentences=LineSentence(inp), size=50, window=5, min_count=3,\n        workers=16, iter=40)\n\n    # trim unneeded model memory = use(much) less RAM\n    # model.init_sims(replace=True)\n    model.wv.save_word2vec_format(outp, binary=False)\n\n    print(\'done in %ds!\' % (time()-t0))\n'"
Utils/trietree.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n""""""\n    Trie tree\n""""""\nfrom collections import defaultdict\nfrom time import time\n\n\nclass Node(object):\n\n    def __init__(self, value, children_value=None):\n        self._value = value\n        self._children = defaultdict()\n        self._end_flag = \'\'\n\n    @property\n    def children(self):\n        return self._children\n\n    @property\n    def value(self):\n        return self._value\n\n    @property\n    def end_flag(self):\n        return self._end_flag\n\n    def set_end_flag(self, value):\n        self._end_flag = value\n\t\t\n    def set_children(self, children):\n        self._children = children\n\n    def __str__(self):\n        return self._value\n\n\nclass TrieTree(object):\n\n    def __init__(self, entity_dict=None):\n        """"""\n        Args:\n            entity_dict: dict\n        """"""\n        self._root = Node(\'root\')\n        if entity_dict:\n            self.update_tree_batch(entity_dict)\n\n    def update_tree_batch(self, entity_dict):\n        """"""\n        Args:\n            entity_dict: dict\n        """"""\n        for entity in entity_dict:\n            entity_type = entity_dict[entity]\n            self.update_tree(entity, entity_type)\n\n    def update_tree(self, entity, entity_type):\n        """"""\n        Args:\n            entity: str\n            entity_type: str\n        """"""\n        name_len = len(entity)\n        node_pre = self.root\n        for i in range(name_len):\n            c = entity[i]\n            if c not in node_pre.children:\n                node = Node(c)\n                node_pre.children[c] = node\n                node_pre = node\n                if i == name_len-1:\n                    node.set_end_flag(entity_type)\n            else:\n                if i == name_len-1:\n                    node_pre.children[c].set_end_flag(entity_type)\n                node_pre = node_pre.children[c]\n\n    @property\n    def root(self):\n        return self._root\n\n    def show(self, node, level=0):\n        """"""\n        \xe8\xbe\x93\xe5\x87\xbatree\n        """"""\n        if not node:\n            return\n        if level == 0:\n            print(node)\n        else:\n            print(\'\xe2\x94\x94%s%s %s\' % (\'\xe2\x94\x80\' * level*2, node, node.end_flag))\n        for node_value in node.children:\n            end_flag = node.children[node_value].end_flag\n            print(\'\xe2\x94\x94%s%s %s\' % (\'\xe2\x94\x80\' * (level+1)*2, node_value, end_flag))\n            if not node.children[node_value]:\n                continue\n            for child in node.children[node_value].children:\n                self.show(node.children[node_value].children[child], level+2)\n\n\ndef match_sentence(sentence, root, start):\n    """"""\n    Args:\n        sentence: str\n        root: Node, root node\n        start: int, \xe5\x8f\xa5\xe5\xad\x90\xe7\x9a\x84\xe8\xb5\xb7\xe5\xa7\x8b\xe4\xbd\x8d\xe7\xbd\xae\n    Return:\n        end, entity_len, entity_type\n    """"""\n    current_node = root\n    index, entity_type = -1, \'\'\n    for i in range(start, len(sentence)):\n        if sentence[i] not in current_node.children:\n            if index == -1:\n                return start + 1, 0, \'\'\n            else:\n                return index+1, index+1-start, entity_type\n        current_node = current_node.children[sentence[i]]\n        if current_node.end_flag:\n            index = i\n            entity_type = current_node.end_flag\n    if index != -1:\n        return index+1, index+1-start, entity_type\n    return start+1, 0, \'\'\n\n\ndef demo():\n    # \xe6\x9e\x84\xe5\xbb\xba\xe6\xa0\x91\n    print(\'Building tree...\', end=\'\')\n    entity_dict = {\'\xe8\x8b\x8f\xe5\xb7\x9e\': \'GPE\', \'\xe8\x8b\x8f\xe5\xa4\xa7\': \'ORG\', \'\xe8\x8b\x8f\xe5\xb7\x9e\xe5\xa4\xa7\xe5\xad\xa6\': \'ORG\', \'\xe5\xb0\x8f\xe6\x98\x8e\': \'PER\', \'\xe6\xb1\x9f\xe8\x8b\x8f\': \'GPE\',\n                   \'\xe8\x8b\x8f\xe6\x9c\x89\xe6\x9c\x8b\': \'PER\', \'\xe6\xb1\x9f\xe8\x8b\x8f\xe5\xa4\xa7\xe5\xad\xa6\': \'ORG\', \'\xe4\xb8\xad\xe5\x8d\x8e\xe4\xba\xba\xe6\xb0\x91\xe5\x85\xb1\xe5\x92\x8c\xe5\x9b\xbd\': \'GPE\'}\n    tree = TrieTree(entity_dict)\n    print(\'done!\')\n    tree.show(tree.root)\n\n    # \xe6\xb5\x8b\xe8\xaf\x95\n    sentence = \'\xe6\x88\x91\xe4\xbd\x8f\xe5\x9c\xa8\xe4\xb8\xad\xe5\x8d\x8e\xe4\xba\xba\xe6\xb0\x91\xe5\x85\xb1\xe5\x92\x8c\xe5\x9b\xbd\xe6\xb1\x9f\xe8\x8b\x8f\xe7\x9c\x81\xe8\x8b\x8f\xe5\xb7\x9e\xe8\x8b\x8f\xe5\xb7\x9e\xe5\xa4\xa7\xe5\xad\xa6\xef\xbc\x8c\xe9\x82\xbb\xe5\xb1\x85\xe6\x98\xaf\xe8\x8b\x8f\xe5\xb7\x9e\xe5\xa4\xa7\xe7\x9a\x84\xe5\xb0\x8f\xe6\x98\x8e\xe3\x80\x82\xe8\x8b\x8f\xe6\x9c\x89\xe6\x9c\x8b\'\n    print(\'\\nsentence:\', sentence)\n    end, sent_len = 0, len(sentence)\n    print(\'\\nresult:\\nstart\\tend\\tentity\\tentity_type\')\n    while end < sent_len:\n        end, entity_len, entity_type = match_sentence(sentence, tree.root, end)\n        if entity_type:\n            print(\'%d\\t%d\\t%s\\t%s\' %\n                  (end-entity_len, end-1, sentence[end-entity_len:end], entity_type))\n    print(\'Done!\')\n\n\ndef demo_2():\n    # \xe6\x9e\x84\xe5\xbb\xba\xe6\xa0\x91\n    print(\'Building tree...\', end=\'\')\n    entity_dict = {\'\xe8\x8b\x8f\xe5\xb7\x9e\': \'GPE\', \'\xe8\x8b\x8f\xe5\xa4\xa7\': \'ORG\', \'\xe8\x8b\x8f\xe5\xb7\x9e\xe5\xa4\xa7\xe5\xad\xa6\': \'ORG\', \'\xe5\xb0\x8f\xe6\x98\x8e\': \'PER\', \'\xe6\xb1\x9f\xe8\x8b\x8f\': \'GPE\',\n                   \'\xe8\x8b\x8f\xe6\x9c\x89\xe6\x9c\x8b\': \'PER\', \'\xe6\xb1\x9f\xe8\x8b\x8f\xe5\xa4\xa7\xe5\xad\xa6\': \'ORG\', \'\xe4\xb8\xad\xe5\x8d\x8e\xe4\xba\xba\xe6\xb0\x91\xe5\x85\xb1\xe5\x92\x8c\xe5\x9b\xbd\': \'GPE\'}\n    tree = TrieTree(entity_dict)\n    print(\'done!\')\n    # tree.show(tree.root)\n\n    t0 = time()\n    # \xe6\xb5\x8b\xe8\xaf\x95\n    sentence_count = int(1e6)\n    sentences = [\'\xe6\x88\x91\xe4\xbd\x8f\xe5\x9c\xa8\xe4\xb8\xad\xe5\x8d\x8e\xe4\xba\xba\xe6\xb0\x91\xe5\x85\xb1\xe5\x92\x8c\xe5\x9b\xbd\xe6\xb1\x9f\xe8\x8b\x8f\xe7\x9c\x81\xe8\x8b\x8f\xe5\xb7\x9e\xe8\x8b\x8f\xe5\xb7\x9e\xe5\xa4\xa7\xe5\xad\xa6\xef\xbc\x8c\xe9\x82\xbb\xe5\xb1\x85\xe6\x98\xaf\xe8\x8b\x8f\xe5\xb7\x9e\xe5\xa4\xa7\xe7\x9a\x84\xe5\xb0\x8f\xe6\x98\x8e\xe3\x80\x82\xe8\x8b\x8f\xe6\x9c\x89\xe6\x9c\x8b\'] * sentence_count\n    for sentence in sentences:\n        end, sent_len = 0, len(sentence)\n        while end < sent_len:\n            end, entity_len, entity_type = match_sentence(sentence, tree.root, end)\n    duration = time() - t0\n    print(\'Done in %.1fs, %.1f sentences/s\' % (duration, sentence_count / duration))\n\n\nif __name__ == \'__main__\':\n    demo()\n'"
