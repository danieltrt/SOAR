file_path,api_count,code
eval.py,16,"b""# -*- coding:utf-8 -*-\nimport cv2\nimport time\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.client import timeline\nfrom utils.utils_tool import logger, cfg\nimport matplotlib.pyplot as plt\n\ntf.app.flags.DEFINE_string('test_data_path', None, '')\ntf.app.flags.DEFINE_string('gpu_list', '0', '')\ntf.app.flags.DEFINE_string('checkpoint_path', './', '')\ntf.app.flags.DEFINE_string('output_dir', './results/', '')\ntf.app.flags.DEFINE_bool('no_write_images', False, 'do not write images')\n\nfrom nets import model\nfrom pse import pse\n\nFLAGS = tf.app.flags.FLAGS\n\nlogger.setLevel(cfg.debug)\n\ndef get_images():\n    '''\n    find image files in test data path\n    :return: list of files found\n    '''\n    files = []\n    exts = ['jpg', 'png', 'jpeg', 'JPG']\n    for parent, dirnames, filenames in os.walk(FLAGS.test_data_path):\n        for filename in filenames:\n            for ext in exts:\n                if filename.endswith(ext):\n                    files.append(os.path.join(parent, filename))\n                    break\n    logger.info('Find {} images'.format(len(files)))\n    return files\n\n\ndef resize_image(im, max_side_len=1200):\n    '''\n    resize image to a size multiple of 32 which is required by the network\n    :param im: the resized image\n    :param max_side_len: limit of max image size to avoid out of memory in gpu\n    :return: the resized image and the resize ratio\n    '''\n    h, w, _ = im.shape\n\n    resize_w = w\n    resize_h = h\n\n    # limit the max side\n    if max(resize_h, resize_w) > max_side_len:\n        ratio = float(max_side_len) / resize_h if resize_h > resize_w else float(max_side_len) / resize_w\n    else:\n        ratio = 1.\n\n    #ratio = float(max_side_len) / resize_h if resize_h > resize_w else float(max_side_len) / resize_w\n\n\n    resize_h = int(resize_h * ratio)\n    resize_w = int(resize_w * ratio)\n\n    resize_h = resize_h if resize_h % 32 == 0 else (resize_h // 32 + 1) * 32\n    resize_w = resize_w if resize_w % 32 == 0 else (resize_w // 32 + 1) * 32\n    logger.info('resize_w:{}, resize_h:{}'.format(resize_w, resize_h))\n    im = cv2.resize(im, (int(resize_w), int(resize_h)))\n\n    ratio_h = resize_h / float(h)\n    ratio_w = resize_w / float(w)\n\n    return im, (ratio_h, ratio_w)\n\n\ndef detect(seg_maps, timer, image_w, image_h, min_area_thresh=10, seg_map_thresh=0.9, ratio = 1):\n    '''\n    restore text boxes from score map and geo map\n    :param seg_maps:\n    :param timer:\n    :param min_area_thresh:\n    :param seg_map_thresh: threshhold for seg map\n    :param ratio: compute each seg map thresh\n    :return:\n    '''\n    if len(seg_maps.shape) == 4:\n        seg_maps = seg_maps[0, :, :, ]\n    #get kernals, sequence: 0->n, max -> min\n    kernals = []\n    one = np.ones_like(seg_maps[..., 0], dtype=np.uint8)\n    zero = np.zeros_like(seg_maps[..., 0], dtype=np.uint8)\n    thresh = seg_map_thresh\n    for i in range(seg_maps.shape[-1]-1, -1, -1):\n        kernal = np.where(seg_maps[..., i]>thresh, one, zero)\n        kernals.append(kernal)\n        thresh = seg_map_thresh*ratio\n    start = time.time()\n    mask_res, label_values = pse(kernals, min_area_thresh)\n    timer['pse'] = time.time()-start\n    mask_res = np.array(mask_res)\n    mask_res_resized = cv2.resize(mask_res, (image_w, image_h), interpolation=cv2.INTER_NEAREST)\n    boxes = []\n    for label_value in label_values:\n        #(y,x)\n        points = np.argwhere(mask_res_resized==label_value)\n        points = points[:, (1,0)]\n        rect = cv2.minAreaRect(points)\n        box = cv2.boxPoints(rect)\n        boxes.append(box)\n\n    return np.array(boxes), kernals, timer\n\ndef show_score_geo(color_im, kernels, im_res):\n    fig = plt.figure()\n    cmap = plt.cm.hot\n    #\n    ax = fig.add_subplot(241)\n    im = kernels[0]*255\n    ax.imshow(im)\n\n    ax = fig.add_subplot(242)\n    im = kernels[1]*255\n    ax.imshow(im, cmap)\n\n    ax = fig.add_subplot(243)\n    im = kernels[2]*255\n    ax.imshow(im, cmap)\n\n    ax = fig.add_subplot(244)\n    im = kernels[3]*255\n    ax.imshow(im, cmap)\n\n    ax = fig.add_subplot(245)\n    im = kernels[4]*255\n    ax.imshow(im, cmap)\n\n    ax = fig.add_subplot(246)\n    im = kernels[5]*255\n    ax.imshow(im, cmap)\n\n    ax = fig.add_subplot(247)\n    im = color_im\n    ax.imshow(im)\n\n    ax = fig.add_subplot(248)\n    im = im_res\n    ax.imshow(im)\n\n    fig.show()\n\n\ndef main(argv=None):\n    import os\n    os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu_list\n\n    try:\n        os.makedirs(FLAGS.output_dir)\n    except OSError as e:\n        if e.errno != 17:\n            raise\n\n    with tf.get_default_graph().as_default():\n        input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='input_images')\n        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n        seg_maps_pred = model.model(input_images, is_training=False)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.997, global_step)\n        saver = tf.train.Saver(variable_averages.variables_to_restore())\n        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n            ckpt_state = tf.train.get_checkpoint_state(FLAGS.checkpoint_path)\n            model_path = os.path.join(FLAGS.checkpoint_path, os.path.basename(ckpt_state.model_checkpoint_path))\n            logger.info('Restore from {}'.format(model_path))\n            saver.restore(sess, model_path)\n\n            im_fn_list = get_images()\n            for im_fn in im_fn_list:\n                im = cv2.imread(im_fn)[:, :, ::-1]\n                logger.debug('image file:{}'.format(im_fn))\n\n                start_time = time.time()\n                im_resized, (ratio_h, ratio_w) = resize_image(im)\n                h, w, _ = im_resized.shape\n                # options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE)\n                # run_metadata = tf.RunMetadata()\n                timer = {'net': 0, 'pse': 0}\n                start = time.time()\n                seg_maps = sess.run(seg_maps_pred, feed_dict={input_images: [im_resized]})\n                timer['net'] = time.time() - start\n                # fetched_timeline = timeline.Timeline(run_metadata.step_stats)\n                # chrome_trace = fetched_timeline.generate_chrome_trace_format()\n                # with open(os.path.join(FLAGS.output_dir, os.path.basename(im_fn).split('.')[0]+'.json'), 'w') as f:\n                #     f.write(chrome_trace)\n\n                boxes, kernels, timer = detect(seg_maps=seg_maps, timer=timer, image_w=w, image_h=h)\n                logger.info('{} : net {:.0f}ms, pse {:.0f}ms'.format(\n                    im_fn, timer['net']*1000, timer['pse']*1000))\n\n                if boxes is not None:\n                    boxes = boxes.reshape((-1, 4, 2))\n                    boxes[:, :, 0] /= ratio_w\n                    boxes[:, :, 1] /= ratio_h\n                    h, w, _ = im.shape\n                    boxes[:, :, 0] = np.clip(boxes[:, :, 0], 0, w)\n                    boxes[:, :, 1] = np.clip(boxes[:, :, 1], 0, h)\n\n                duration = time.time() - start_time\n                logger.info('[timing] {}'.format(duration))\n\n                # save to file\n                if boxes is not None:\n                    res_file = os.path.join(\n                        FLAGS.output_dir,\n                        '{}.txt'.format(os.path.splitext(\n                            os.path.basename(im_fn))[0]))\n\n\n                    with open(res_file, 'w') as f:\n                        num =0\n                        for i in xrange(len(boxes)):\n                            # to avoid submitting errors\n                            box = boxes[i]\n                            if np.linalg.norm(box[0] - box[1]) < 5 or np.linalg.norm(box[3]-box[0]) < 5:\n                                continue\n\n                            num += 1\n\n                            f.write('{},{},{},{},{},{},{},{}\\r\\n'.format(\n                                box[0, 0], box[0, 1], box[1, 0], box[1, 1], box[2, 0], box[2, 1], box[3, 0], box[3, 1]))\n                            cv2.polylines(im[:, :, ::-1], [box.astype(np.int32).reshape((-1, 1, 2))], True, color=(255, 255, 0), thickness=2)\n                if not FLAGS.no_write_images:\n                    img_path = os.path.join(FLAGS.output_dir, os.path.basename(im_fn))\n                    cv2.imwrite(img_path, im[:, :, ::-1])\n                # show_score_geo(im_resized, kernels, im)\nif __name__ == '__main__':\n    tf.app.run()\n"""
train.py,56,"b""import time\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom utils.utils_tool import logger, cfg\n\ntf.app.flags.DEFINE_integer('input_size', 512, '')\ntf.app.flags.DEFINE_integer('batch_size_per_gpu', 8, '')\ntf.app.flags.DEFINE_integer('num_readers', 32, '')\ntf.app.flags.DEFINE_float('learning_rate', 0.00001, '')\ntf.app.flags.DEFINE_integer('max_steps', 100000, '')\ntf.app.flags.DEFINE_float('moving_average_decay', 0.997, '')\ntf.app.flags.DEFINE_string('gpu_list', '0', '')\ntf.app.flags.DEFINE_string('checkpoint_path', './resnet_train/', '')\ntf.app.flags.DEFINE_boolean('restore', False, 'whether to resotre from checkpoint')\ntf.app.flags.DEFINE_integer('save_checkpoint_steps', 1000, '')\ntf.app.flags.DEFINE_integer('save_summary_steps', 100, '')\ntf.app.flags.DEFINE_string('pretrained_model_path', None, '')\n\nfrom nets import model\nfrom utils.data_provider import data_provider\n\nFLAGS = tf.app.flags.FLAGS\n\ngpus = list(range(len(FLAGS.gpu_list.split(','))))\n\nlogger.setLevel(cfg.debug)\n\ndef tower_loss(images, seg_maps_gt, training_masks, reuse_variables=None):\n    # Build inference graph\n    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):\n        seg_maps_pred = model.model(images, is_training=True)\n\n    model_loss = model.loss(seg_maps_gt, seg_maps_pred, training_masks)\n    total_loss = tf.add_n([model_loss] + tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n\n    # add summary\n    if reuse_variables is None:\n        tf.summary.image('input', images)\n        tf.summary.image('seg_map_0_gt', seg_maps_gt[:, :, :, 0:1] * 255)\n        tf.summary.image('seg_map_0_pred', seg_maps_pred[:, :, :, 0:1] * 255)\n        tf.summary.image('training_masks', training_masks)\n        tf.summary.scalar('model_loss', model_loss)\n        tf.summary.scalar('total_loss', total_loss)\n\n    return total_loss, model_loss\n\n\ndef average_gradients(tower_grads):\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        grads = []\n        for g, _ in grad_and_vars:\n            expanded_g = tf.expand_dims(g, 0)\n            grads.append(expanded_g)\n\n        grad = tf.concat(grads, 0)\n        grad = tf.reduce_mean(grad, 0)\n\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n\n    return average_grads\n\n\ndef main(argv=None):\n    import os\n    os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu_list\n    if not tf.gfile.Exists(FLAGS.checkpoint_path):\n        tf.gfile.MkDir(FLAGS.checkpoint_path)\n    else:\n        if not FLAGS.restore:\n            tf.gfile.DeleteRecursively(FLAGS.checkpoint_path)\n            tf.gfile.MkDir(FLAGS.checkpoint_path)\n\n    input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='input_images')\n    input_seg_maps = tf.placeholder(tf.float32, shape=[None, None, None, 6], name='input_score_maps')\n    input_training_masks = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='input_training_masks')\n\n    global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n    learning_rate = tf.train.exponential_decay(FLAGS.learning_rate, global_step, decay_steps=10000, decay_rate=0.94, staircase=True)\n    # add summary\n    tf.summary.scalar('learning_rate', learning_rate)\n    # opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9)\n    opt = tf.train.AdamOptimizer(learning_rate)\n    # opt = tf.train.MomentumOptimizer(learning_rate, 0.9)\n\n\n    # split\n    input_images_split = tf.split(input_images, len(gpus))\n    input_seg_maps_split = tf.split(input_seg_maps, len(gpus))\n    input_training_masks_split = tf.split(input_training_masks, len(gpus))\n\n    tower_grads = []\n    reuse_variables = None\n    for i, gpu_id in enumerate(gpus):\n        with tf.device('/gpu:%d' % gpu_id):\n            with tf.name_scope('model_%d' % gpu_id) as scope:\n                iis = input_images_split[i]\n                isegs = input_seg_maps_split[i]\n                itms = input_training_masks_split[i]\n                total_loss, model_loss = tower_loss(iis, isegs, itms, reuse_variables)\n                batch_norm_updates_op = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope))\n                reuse_variables = True\n\n                grads = opt.compute_gradients(total_loss)\n                tower_grads.append(grads)\n\n    grads = average_gradients(tower_grads)\n    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n    summary_op = tf.summary.merge_all()\n    # save moving average\n    variable_averages = tf.train.ExponentialMovingAverage(\n        FLAGS.moving_average_decay, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n    # batch norm updates\n    with tf.control_dependencies([variables_averages_op, apply_gradient_op, batch_norm_updates_op]):\n        train_op = tf.no_op(name='train_op')\n\n    saver = tf.train.Saver(tf.global_variables())\n    summary_writer = tf.summary.FileWriter(FLAGS.checkpoint_path, tf.get_default_graph())\n\n    init = tf.global_variables_initializer()\n\n    if FLAGS.pretrained_model_path is not None:\n        variable_restore_op = slim.assign_from_checkpoint_fn(FLAGS.pretrained_model_path, slim.get_trainable_variables(),\n                                                             ignore_missing_vars=True)\n    gpu_options=tf.GPUOptions(allow_growth=True)\n    #gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.75)\n    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True)) as sess:\n        if FLAGS.restore:\n            logger.info('continue training from previous checkpoint')\n            ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n            logger.debug(ckpt)\n            saver.restore(sess, ckpt)\n        else:\n            sess.run(init)\n            if FLAGS.pretrained_model_path is not None:\n                variable_restore_op(sess)\n\n        data_generator = data_provider.get_batch(num_workers=FLAGS.num_readers,\n                                                 input_size=FLAGS.input_size,\n                                                 batch_size=FLAGS.batch_size_per_gpu * len(gpus))\n\n        start = time.time()\n        for step in range(FLAGS.max_steps):\n            data = next(data_generator)\n            ml, tl, _ = sess.run([model_loss, total_loss, train_op], feed_dict={input_images: data[0],\n                                                                                input_seg_maps: data[2],\n                                                                                input_training_masks: data[3]})\n            if np.isnan(tl):\n                logger.error('Loss diverged, stop training')\n                break\n\n            if step % 10 == 0:\n                avg_time_per_step = (time.time() - start)/10\n                avg_examples_per_second = (10 * FLAGS.batch_size_per_gpu * len(gpus))/(time.time() - start)\n                start = time.time()\n                logger.info('Step {:06d}, model loss {:.4f}, total loss {:.4f}, {:.2f} seconds/step, {:.2f} examples/second'.format(\n                    step, ml, tl, avg_time_per_step, avg_examples_per_second))\n\n            if step % FLAGS.save_checkpoint_steps == 0:\n                saver.save(sess, os.path.join(FLAGS.checkpoint_path, 'model.ckpt'), global_step=global_step)\n\n            if step % FLAGS.save_summary_steps == 0:\n                _, tl, summary_str = sess.run([train_op, total_loss, summary_op], feed_dict={input_images: data[0],\n                                                                                             input_seg_maps: data[2],\n                                                                                             input_training_masks: data[3]})\n                summary_writer.add_summary(summary_str, global_step=step)\n\nif __name__ == '__main__':\n    tf.app.run()\n"""
nets/__init__.py,0,b''
nets/model.py,22,"b""#-*- coding:utf-8 -*-\nimport tensorflow as tf\nfrom utils.utils_tool import logger\n\nfrom tensorflow.contrib import slim\n\ntf.app.flags.DEFINE_integer('text_scale', 512, '')\n\nfrom nets.resnet import resnet_v1\n\nFLAGS = tf.app.flags.FLAGS\n\n#TODO:bilinear or nearest_neighbor?\ndef unpool(inputs, rate):\n    return tf.image.resize_bilinear(inputs, size=[tf.shape(inputs)[1]*rate,  tf.shape(inputs)[2]*rate])\n\ndef mean_image_subtraction(images, means=[123.68, 116.78, 103.94]):\n    '''\n    image normalization\n    :param images:\n    :param means:\n    :return:\n    '''\n    num_channels = images.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n      raise ValueError('len(means) must match the number of channels')\n    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=images)\n    for i in range(num_channels):\n        channels[i] -= means[i]\n    return tf.concat(axis=3, values=channels)\n\ndef build_feature_pyramid(C, weight_decay):\n\n    '''\n    reference: https://github.com/CharlesShang/FastMaskRCNN\n    build P2, P3, P4, P5\n    :return: multi-scale feature map\n    '''\n\n    feature_pyramid = {}\n    with tf.variable_scope('build_feature_pyramid'):\n        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(weight_decay)):\n            feature_pyramid['P5'] = slim.conv2d(C['C5'],\n                                                num_outputs=256,\n                                                kernel_size=[1, 1],\n                                                stride=1,\n                                                scope='build_P5')\n\n            # feature_pyramid['P6'] = slim.max_pool2d(feature_pyramid['P5'],\n            #                                         kernel_size=[2, 2], stride=2, scope='build_P6')\n            # P6 is down sample of P5\n\n            for layer in range(4, 1, -1):\n                p, c = feature_pyramid['P' + str(layer + 1)], C['C' + str(layer)]\n                up_sample_shape = tf.shape(c)\n                up_sample = tf.image.resize_nearest_neighbor(p, [up_sample_shape[1], up_sample_shape[2]],\n                                                             name='build_P%d/up_sample_nearest_neighbor' % layer)\n\n                c = slim.conv2d(c, num_outputs=256, kernel_size=[1, 1], stride=1,\n                                scope='build_P%d/reduce_dimension' % layer)\n                p = up_sample + c\n                p = slim.conv2d(p, 256, kernel_size=[3, 3], stride=1,\n                                padding='SAME', scope='build_P%d/avoid_aliasing' % layer)\n                feature_pyramid['P' + str(layer)] = p\n    return feature_pyramid\n\ndef model(images, outputs = 6, weight_decay=1e-5, is_training=True):\n    '''\n    define the model, we use slim's implemention of resnet\n    '''\n    images = mean_image_subtraction(images)\n\n    with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=weight_decay)):\n        logits, end_points = resnet_v1.resnet_v1_50(images, is_training=is_training, scope='resnet_v1_50')\n\n    #no non-linearities in FPN article\n    feature_pyramid = build_feature_pyramid(end_points, weight_decay=weight_decay)\n    #unpool sample P\n    P_concat = []\n    for i in range(3, 0, -1):\n        P_concat.append(unpool(feature_pyramid['P'+str(i+2)], 2**i))\n    P_concat.append(feature_pyramid['P2'])\n    #F = C(P2,P3,P4,P5)\n    F = tf.concat(P_concat, axis=-1)\n\n    #reduce to 256 channels\n    with tf.variable_scope('feature_results'):\n        batch_norm_params = {\n            'decay': 0.997,\n            'epsilon': 1e-5,\n            'scale': True,\n            'is_training': is_training\n        }\n        with slim.arg_scope([slim.conv2d],\n                            activation_fn=tf.nn.relu,\n                            normalizer_fn=slim.batch_norm,\n                            normalizer_params=batch_norm_params,\n                            weights_regularizer=slim.l2_regularizer(weight_decay)):\n            F = slim.conv2d(F, 256, 3)\n        with slim.arg_scope([slim.conv2d],\n                            weights_regularizer=slim.l2_regularizer(weight_decay),\n                            activation_fn=None):\n            S = slim.conv2d(F, outputs, 1)\n\n    seg_S_pred = tf.nn.sigmoid(S)\n\n    return seg_S_pred\n\ndef dice_coefficient(y_true_cls, y_pred_cls,\n                     training_mask):\n    '''\n    dice loss\n    :param y_true_cls: ground truth\n    :param y_pred_cls: predict\n    :param training_mask:\n    :return:\n    '''\n    eps = 1e-5\n    intersection = tf.reduce_sum(y_true_cls * y_pred_cls * training_mask)\n    union = tf.reduce_sum(y_true_cls * training_mask) + tf.reduce_sum(y_pred_cls * training_mask) + eps\n    dice = 2 * intersection / union\n    loss = 1. - dice\n    # tf.summary.scalar('classification_dice_loss', loss)\n    return dice, loss\n\ndef loss(y_true_cls, y_pred_cls,\n         training_mask):\n    g1, g2, g3, g4, g5, g6 = tf.split(value=y_true_cls, num_or_size_splits=6, axis=3)\n    s1, s2, s3, s4, s5, s6 = tf.split(value=y_pred_cls, num_or_size_splits=6, axis=3)\n    Gn = [g1, g2, g3, g4, g5, g6]\n    Sn = [s1, s2, s3, s4, s5, s6]\n    _, Lc = dice_coefficient(Gn[5], Sn[5], training_mask=training_mask)\n    tf.summary.scalar('Lc_loss', Lc)\n\n    one = tf.ones_like(Sn[5])\n    zero = tf.zeros_like(Sn[5])\n    W = tf.where(Sn[5] >= 0.5, x=one, y=zero)\n    D = 0\n    for i in range(5):\n        di, _ = dice_coefficient(Gn[i]*W, Sn[i]*W, training_mask=training_mask)\n        D += di\n    Ls = 1-D/5.\n    tf.summary.scalar('Ls_loss', Ls)\n    lambda_ = 0.7\n    L = lambda_*Lc + (1-lambda_)*Ls\n    return L\n\n\n\n\n"""
pse/__init__.py,0,"b""import subprocess\nimport os\nimport numpy as np\nimport cv2\n\nBASE_DIR = os.path.dirname(os.path.realpath(__file__))\n\nif subprocess.call(['make', '-C', BASE_DIR]) != 0:  # return value\n    raise RuntimeError('Cannot compile pse: {}'.format(BASE_DIR))\n\ndef pse(kernals, min_area=5):\n    '''\n    :param kernals:\n    :param min_area:\n    :return:\n    '''\n    from .pse import pse_cpp\n    kernal_num = len(kernals)\n    if not kernal_num:\n        return np.array([]), []\n    kernals = np.array(kernals)\n    label_num, label = cv2.connectedComponents(kernals[kernal_num - 1].astype(np.uint8), connectivity=4)\n    label_values = []\n    for label_idx in range(1, label_num):\n        if np.sum(label == label_idx) < min_area:\n            label[label == label_idx] = 0\n            continue\n        label_values.append(label_idx)\n\n    pred = pse_cpp(label, kernals, c=6)\n\n    return pred, label_values\n\n\n"""
utils/__init__.py,0,b''
utils/utils_tool.py,0,"b""import logging\nfrom easydict import EasyDict as edict\nimport Queue\nimport numpy as np\nimport cv2\n\nlogging.basicConfig()\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\n\n__C = edict()\ncfg = __C\n\n#log level\n__C.error = logging.ERROR\n__C.warning = logging.WARNING\n__C.info = logging.INFO\n__C.debug = logging.DEBUG\n\n\ndef pse(kernals, min_area=5):\n    '''\n    reference https://github.com/whai362/PSENet/issues/15\n    :param kernals:\n    :param min_area:\n    :return:\n    '''\n    kernal_num = len(kernals)\n    if not kernal_num:\n        logger.error('not kernals!')\n        return np.array([]), []\n    pred = np.zeros(kernals[0].shape, dtype='int32')\n\n    label_num, label = cv2.connectedComponents(kernals[kernal_num - 1].astype(np.uint8), connectivity=4)\n    label_values = []\n    for label_idx in range(1, label_num):\n        if np.sum(label == label_idx) < min_area:\n            label[label == label_idx] = 0\n            continue\n        label_values.append(label_idx)\n\n    queue = Queue.Queue(maxsize=0)\n    next_queue = Queue.Queue(maxsize=0)\n    points = np.array(np.where(label > 0)).transpose((1, 0))\n\n\n    for point_idx in range(points.shape[0]):\n        x, y = points[point_idx, 0], points[point_idx, 1]\n        l = label[x, y]\n        queue.put((x, y, l))\n        pred[x, y] = l\n\n    dx = [-1, 1, 0, 0]\n    dy = [0, 0, -1, 1]\n    for kernal_idx in range(kernal_num - 2, -1, -1):\n        kernal = kernals[kernal_idx].copy()\n        while not queue.empty():\n            (x, y, l) = queue.get()\n\n            is_edge = True\n            for j in range(4):\n                tmpx = x + dx[j]\n                tmpy = y + dy[j]\n                if tmpx < 0 or tmpx >= kernal.shape[0] or tmpy < 0 or tmpy >= kernal.shape[1]:\n                    continue\n                if kernal[tmpx, tmpy] == 0 or pred[tmpx, tmpy] > 0:\n                    continue\n\n                queue.put((tmpx, tmpy, l))\n                pred[tmpx, tmpy] = l\n                is_edge = False\n            if is_edge:\n                next_queue.put((x, y, l))\n\n        # kernal[pred > 0] = 0\n        queue, next_queue = next_queue, queue\n\n        # points = np.array(np.where(pred > 0)).transpose((1, 0))\n        # for point_idx in range(points.shape[0]):\n        #     x, y = points[point_idx, 0], points[point_idx, 1]\n        #     l = pred[x, y]\n        #     queue.put((x, y, l))\n\n    return pred, label_values"""
nets/resnet/__init__.py,0,b''
nets/resnet/resnet_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\n\n\n\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n    """"""A named tuple describing a ResNet block.\n\n    Its parts are:\n      scope: The scope of the `Block`.\n      unit_fn: The ResNet unit function which takes as input a `Tensor` and\n        returns another `Tensor` with the output of the ResNet unit.\n      args: A list of length equal to the number of units in the `Block`. The list\n        contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n        block to serve as argument to unit_fn.\n    """"""\n\n\ndef subsample(inputs, factor, scope=None):\n    """"""Subsamples the input along the spatial dimensions.\n\n    Args:\n      inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n      factor: The subsampling factor.\n      scope: Optional variable_scope.\n\n    Returns:\n      output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n        input, either intact (if factor == 1) or subsampled (if factor > 1).\n    """"""\n    if factor == 1:\n        return inputs\n    else:\n        return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n    """"""Strided 2-D convolution with \'SAME\' padding.\n\n    When stride > 1, then we do explicit zero-padding, followed by conv2d with\n    \'VALID\' padding.\n\n    Note that\n\n       net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n    is equivalent to\n\n       net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n       net = subsample(net, factor=stride)\n\n    whereas\n\n       net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n    is different when the input\'s height or width is even, which is why we add the\n    current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n    Args:\n      inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n      num_outputs: An integer, the number of output filters.\n      kernel_size: An int with the kernel_size of the filters.\n      stride: An integer, the output stride.\n      rate: An integer, rate for atrous convolution.\n      scope: Scope.\n\n    Returns:\n      output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n        the convolution output.\n    """"""\n    if stride == 1:\n        return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                           padding=\'SAME\', scope=scope)\n    else:\n        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n        pad_total = kernel_size_effective - 1\n        pad_beg = pad_total // 2\n        pad_end = pad_total - pad_beg\n        inputs = tf.pad(inputs,\n                        [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n        return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                           rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       outputs_collections=None):\n    """"""Stacks ResNet `Blocks` and controls output feature density.\n\n    First, this function creates scopes for the ResNet in the form of\n    \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n    Second, this function allows the user to explicitly control the ResNet\n    output_stride, which is the ratio of the input to output spatial resolution.\n    This is useful for dense prediction tasks such as semantic segmentation or\n    object detection.\n\n    Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n    factor of 2 when transitioning between consecutive ResNet blocks. This results\n    to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n    half the nominal network stride (e.g., output_stride=4), then we compute\n    responses twice.\n\n    Control of the output feature density is implemented by atrous convolution.\n\n    Args:\n      net: A `Tensor` of size [batch, height, width, channels].\n      blocks: A list of length equal to the number of ResNet `Blocks`. Each\n        element is a ResNet `Block` object describing the units in the `Block`.\n      output_stride: If `None`, then the output will be computed at the nominal\n        network stride. If output_stride is not `None`, it specifies the requested\n        ratio of input to output spatial resolution, which needs to be equal to\n        the product of unit strides from the start up to some level of the ResNet.\n        For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n        then valid values for the output_stride are 1, 2, 6, 24 or None (which\n        is equivalent to output_stride=24).\n      outputs_collections: Collection to add the ResNet block outputs.\n\n    Returns:\n      net: Output tensor with stride equal to the specified output_stride.\n\n    Raises:\n      ValueError: If the target output_stride is not valid.\n    """"""\n    # The current_stride variable keeps track of the effective stride of the\n    # activations. This allows us to invoke atrous convolution whenever applying\n    # the next residual unit would result in the activations having stride larger\n    # than the target output_stride.\n    current_stride = 1\n\n    # The atrous convolution rate parameter.\n    rate = 1\n\n    for block in blocks:\n        with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n            for i, unit in enumerate(block.args):\n                if output_stride is not None and current_stride > output_stride:\n                    raise ValueError(\'The target output_stride cannot be reached.\')\n\n                with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n                    unit_depth, unit_depth_bottleneck, unit_stride = unit\n                    # If we have reached the target output_stride, then we need to employ\n                    # atrous convolution with stride=1 and multiply the atrous rate by the\n                    # current unit\'s stride for use in subsequent layers.\n                    if output_stride is not None and current_stride == output_stride:\n                        net = block.unit_fn(net,\n                                            depth=unit_depth,\n                                            depth_bottleneck=unit_depth_bottleneck,\n                                            stride=1,\n                                            rate=rate)\n                        rate *= unit_stride\n\n                    else:\n                        net = block.unit_fn(net,\n                                            depth=unit_depth,\n                                            depth_bottleneck=unit_depth_bottleneck,\n                                            stride=unit_stride,\n                                            rate=1)\n                        current_stride *= unit_stride\n            print(sc.name, net.shape)\n            net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n    if output_stride is not None and current_stride != output_stride:\n        raise ValueError(\'The target output_stride cannot be reached.\')\n\n    return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n    """"""Defines the default ResNet arg scope.\n\n    TODO(gpapan): The batch-normalization related default values above are\n      appropriate for use in conjunction with the reference ResNet models\n      released at https://github.com/KaimingHe/deep-residual-networks. When\n      training ResNets from scratch, they might need to be tuned.\n\n    Args:\n      weight_decay: The weight decay to use for regularizing the model.\n      batch_norm_decay: The moving average decay when estimating layer activation\n        statistics in batch normalization.\n      batch_norm_epsilon: Small constant to prevent division by zero when\n        normalizing activations by their variance in batch normalization.\n      batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n        activations in the batch normalization layer.\n\n    Returns:\n      An `arg_scope` to use for the resnet models.\n    """"""\n    batch_norm_params = {\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon,\n        \'scale\': batch_norm_scale,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n    }\n\n    with slim.arg_scope(\n            [slim.conv2d],\n            weights_regularizer=slim.l2_regularizer(weight_decay),\n            weights_initializer=slim.variance_scaling_initializer(),\n            activation_fn=tf.nn.relu,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n            # The following implies padding=\'SAME\' for pool1, which makes feature\n            # alignment easier for dense prediction tasks. This is also used in\n            # https://github.com/facebook/fb.resnet.torch. However the accompanying\n            # code of \'Deep Residual Learning for Image Recognition\' uses\n            # padding=\'VALID\' for pool1. You can switch to that choice by setting\n            # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n            with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n                return arg_sc\n'"
nets/resnet/resnet_v1.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\n# from __future__ import absolute_import\n# from __future__ import division\n# from __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nfrom . import resnet_utils\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n    """"""Bottleneck residual unit variant with BN after convolutions.\n\n    This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n    its definition. Note that we use here the bottleneck variant which has an\n    extra bottleneck layer.\n\n    When putting together two consecutive ResNet blocks that use this unit, one\n    should use stride = 2 in the last unit of the first block.\n\n    Args:\n      inputs: A tensor of size [batch, height, width, channels].\n      depth: The depth of the ResNet unit output.\n      depth_bottleneck: The depth of the bottleneck layers.\n      stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n        the units output compared to its input.\n      rate: An integer, rate for atrous convolution.\n      outputs_collections: Collection to add the ResNet unit output.\n      scope: Optional variable_scope.\n\n    Returns:\n      The ResNet unit\'s output.\n    """"""\n    with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n        depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                                   activation_fn=None, scope=\'shortcut\')\n\n        residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                               scope=\'conv1\')\n        residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                            rate=rate, scope=\'conv2\')\n        residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                               activation_fn=None, scope=\'conv3\')\n\n        output = tf.nn.relu(shortcut + residual)\n\n        return slim.utils.collect_named_outputs(outputs_collections,\n                                                sc.original_name_scope,\n                                                output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              reuse=None,\n              scope=None):\n    """"""Generator for v1 ResNet models.\n\n    This function generates a family of ResNet v1 models. See the resnet_v1_*()\n    methods for specific model instantiations, obtained by selecting different\n    block instantiations that produce ResNets of various depths.\n\n    Training for image classification on Imagenet is usually done with [224, 224]\n    inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n    block for the ResNets defined in [1] that have nominal stride equal to 32.\n    However, for dense prediction tasks we advise that one uses inputs with\n    spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n    this case the feature maps at the ResNet output will have spatial shape\n    [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n    and corners exactly aligned with the input image corners, which greatly\n    facilitates alignment of the features to the image. Using as input [225, 225]\n    images results in [8, 8] feature maps at the output of the last ResNet block.\n\n    For dense prediction tasks, the ResNet needs to run in fully-convolutional\n    (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n    have nominal stride equal to 32 and a good choice in FCN mode is to use\n    output_stride=16 in order to increase the density of the computed features at\n    small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n    Args:\n      inputs: A tensor of size [batch, height_in, width_in, channels].\n      blocks: A list of length equal to the number of ResNet blocks. Each element\n        is a resnet_utils.Block object describing the units in the block.\n      num_classes: Number of predicted classes for classification tasks. If None\n        we return the features before the logit layer.\n      is_training: whether is training or not.\n      global_pool: If True, we perform global average pooling before computing the\n        logits. Set to True for image classification, False for dense prediction.\n      output_stride: If None, then the output will be computed at the nominal\n        network stride. If output_stride is not None, it specifies the requested\n        ratio of input to output spatial resolution.\n      include_root_block: If True, include the initial convolution followed by\n        max-pooling, if False excludes it.\n      spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n          of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n      reuse: whether or not the network and its variables should be reused. To be\n        able to reuse \'scope\' must be given.\n      scope: Optional variable_scope.\n\n    Returns:\n      net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n        If global_pool is False, then height_out and width_out are reduced by a\n        factor of output_stride compared to the respective height_in and width_in,\n        else both height_out and width_out equal one. If num_classes is None, then\n        net is the output of the last ResNet block, potentially after global\n        average pooling. If num_classes is not None, net contains the pre-softmax\n        activations.\n      end_points: A dictionary from components of the network to the corresponding\n        activation.\n\n    Raises:\n      ValueError: If the target output_stride is not valid.\n    """"""\n    with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n        end_points_collection = sc.name + \'_end_points\'\n        with slim.arg_scope([slim.conv2d, bottleneck,\n                             resnet_utils.stack_blocks_dense],\n                            outputs_collections=end_points_collection):\n            with slim.arg_scope([slim.batch_norm], is_training=is_training):\n                net = inputs\n                if include_root_block:\n                    if output_stride is not None:\n                        if output_stride % 4 != 0:\n                            raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n                        output_stride /= 4\n                    net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n                    net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n\n                    net = slim.utils.collect_named_outputs(end_points_collection, \'C2\', net)\n\n                net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n\n                end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n\n                # end_points[\'pool2\'] = end_points[\'resnet_v1_50/pool1/MaxPool:0\']\n                try:\n                    end_points[\'C3\'] = end_points[\'resnet_v1_50/block1\']\n                    end_points[\'C4\'] = end_points[\'resnet_v1_50/block2\']\n                except:\n                    end_points[\'C3\'] = end_points[\'Detection/resnet_v1_50/block1\']\n                    end_points[\'C4\'] = end_points[\'Detection/resnet_v1_50/block2\']\n                end_points[\'C5\'] = net\n                # if global_pool:\n                #     # Global average pooling.\n                #     net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n                # if num_classes is not None:\n                #     net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                #                       normalizer_fn=None, scope=\'logits\')\n                # if spatial_squeeze:\n                #     logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n                # else:\n                #     logits = net\n                # # Convert end_points_collection into a dictionary of end_points.\n                # end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n                # if num_classes is not None:\n                #     end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n                return net, end_points\n\n\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n    """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n    ]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                     global_pool=global_pool, output_stride=output_stride,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n    """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n    ]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                     global_pool=global_pool, output_stride=output_stride,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n    """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                     global_pool=global_pool, output_stride=output_stride,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n    """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                     global_pool=global_pool, output_stride=output_stride,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n\n\nif __name__ == \'__main__\':\n    input = tf.placeholder(tf.float32, shape=(None, 224, 224, 3), name=\'input\')\n    with slim.arg_scope(resnet_arg_scope()) as sc:\n        logits = resnet_v1_50(input)'"
utils/data_provider/__init__.py,0,b''
utils/data_provider/data_provider.py,6,"b'# encoding:utf-8\nimport os\nimport glob\nimport time\nimport json\nimport csv\nimport traceback\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom utils.utils_tool import logger\nfrom utils.data_provider.data_util import GeneratorEnqueuer\nimport tensorflow as tf\nimport pyclipper\n\ntf.app.flags.DEFINE_string(\'training_data_path\', None,\n                           \'training dataset to use\')\ntf.app.flags.DEFINE_integer(\'max_image_large_side\', 1280,\n                            \'max image size of training\')\ntf.app.flags.DEFINE_integer(\'max_text_size\', 800,\n                            \'if the text in the input image is bigger than this, then we resize\'\n                            \'the image according to this\')\ntf.app.flags.DEFINE_integer(\'min_text_area_size\', 10,\n                            \'if the text area size is smaller than this, we ignore it during training\')\ntf.app.flags.DEFINE_float(\'min_crop_side_ratio\', 0.1,\n                          \'when doing random crop from input image, the\'\n                          \'min length of min(H, W\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef get_files(exts):\n    files = []\n    for ext in exts:\n        files.extend(glob.glob(\n            os.path.join(FLAGS.training_data_path, \'*.{}\'.format(ext))))\n    return files\n\ndef get_json_label():\n    label_file_list = get_files([\'json\'])\n    label = {}\n    for label_file in label_file_list:\n        with open(label_file, \'r\') as f:\n            json_label = json.load(f)\n\n            for k, v in json_label.items():\n                if not label.has_key(k):\n                    label[k] = v\n    return label\n\ndef load_annoataion(p):\n    \'\'\'\n    load annotation from the text file\n    :param p:\n    :return:\n    \'\'\'\n    text_polys = []\n    text_tags = []\n    if not os.path.exists(p):\n        return np.array(text_polys, dtype=np.float32)\n    with open(p, \'r\') as f:\n        reader = csv.reader(f)\n        for line in reader:\n            label = line[-1]\n            # strip BOM. \\ufeff for python3,  \\xef\\xbb\\bf for python2\n            line = [i.strip(\'\\ufeff\').strip(\'\\xef\\xbb\\xbf\') for i in line]\n\n            x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, line[:8]))\n            text_polys.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]])\n            #TODO:maybe add \'?\' for icpr2018 (michael)\n            if label == \'*\' or label == \'###\' or label == \'?\':\n                text_tags.append(True)\n            else:\n                text_tags.append(False)\n        return np.array(text_polys, dtype=np.float32), np.array(text_tags, dtype=np.bool)\n\ndef check_and_validate_polys(polys, tags, xxx_todo_changeme):\n    \'\'\'\n    check so that the text poly is in the same direction,\n    and also filter some invalid polygons\n    :param polys:\n    :param tags:\n    :return:\n    \'\'\'\n    (h, w) = xxx_todo_changeme\n    if polys.shape[0] == 0:\n        return [], []\n    polys[:, :, 0] = np.clip(polys[:, :, 0], 0, w-1)\n    polys[:, :, 1] = np.clip(polys[:, :, 1], 0, h-1)\n\n    validated_polys = []\n    validated_tags = []\n    for poly, tag in zip(polys, tags):\n        if abs(pyclipper.Area(poly))<1:\n            continue\n        #clockwise\n        if pyclipper.Orientation(poly):\n            poly = poly[::-1]\n\n        validated_polys.append(poly)\n        validated_tags.append(tag)\n    return np.array(validated_polys), np.array(validated_tags)\n\ndef crop_area(im, polys, tags, crop_background=False, max_tries=50):\n    \'\'\'\n    make random crop from the input image\n    :param im:\n    :param polys:\n    :param tags:\n    :param crop_background:\n    :param max_tries:\n    :return:\n    \'\'\'\n    h, w, _ = im.shape\n    pad_h = h//10\n    pad_w = w//10\n    h_array = np.zeros((h + pad_h*2), dtype=np.int32)\n    w_array = np.zeros((w + pad_w*2), dtype=np.int32)\n    for poly in polys:\n        poly = np.round(poly, decimals=0).astype(np.int32)\n        minx = np.min(poly[:, 0])\n        maxx = np.max(poly[:, 0])\n        w_array[minx+pad_w:maxx+pad_w] = 1\n        miny = np.min(poly[:, 1])\n        maxy = np.max(poly[:, 1])\n        h_array[miny+pad_h:maxy+pad_h] = 1\n    # ensure the cropped area not across a text\n    h_axis = np.where(h_array == 0)[0]\n    w_axis = np.where(w_array == 0)[0]\n    if len(h_axis) == 0 or len(w_axis) == 0:\n        return im, polys, tags\n    for i in range(max_tries):\n        xx = np.random.choice(w_axis, size=2)\n        xmin = np.min(xx) - pad_w\n        xmax = np.max(xx) - pad_w\n        xmin = np.clip(xmin, 0, w-1)\n        xmax = np.clip(xmax, 0, w-1)\n        yy = np.random.choice(h_axis, size=2)\n        ymin = np.min(yy) - pad_h\n        ymax = np.max(yy) - pad_h\n        ymin = np.clip(ymin, 0, h-1)\n        ymax = np.clip(ymax, 0, h-1)\n        if xmax - xmin < FLAGS.min_crop_side_ratio*w or ymax - ymin < FLAGS.min_crop_side_ratio*h:\n            # area too small\n            continue\n        if polys.shape[0] != 0:\n            poly_axis_in_area = (polys[:, :, 0] >= xmin) & (polys[:, :, 0] <= xmax) \\\n                                & (polys[:, :, 1] >= ymin) & (polys[:, :, 1] <= ymax)\n            selected_polys = np.where(np.sum(poly_axis_in_area, axis=1) == 4)[0]\n        else:\n            selected_polys = []\n        if len(selected_polys) == 0:\n            # no text in this area\n            if crop_background:\n                return im[ymin:ymax+1, xmin:xmax+1, :], polys[selected_polys], tags[selected_polys]\n            else:\n                continue\n        im = im[ymin:ymax+1, xmin:xmax+1, :]\n        polys = polys[selected_polys]\n        tags = tags[selected_polys]\n        polys[:, :, 0] -= xmin\n        polys[:, :, 1] -= ymin\n        return im, polys, tags\n\n    return im, polys, tags\n\ndef perimeter(poly):\n    try:\n        p=0\n        nums = poly.shape[0]\n        for i in range(nums):\n            p += abs(np.linalg.norm(poly[i%nums]-poly[(i+1)%nums]))\n        # logger.debug(\'perimeter:{}\'.format(p))\n        return p\n    except Exception as e:\n        traceback.print_exc()\n        raise e\n\ndef shrink_poly(poly, r):\n    try:\n        area_poly = abs(pyclipper.Area(poly))\n        perimeter_poly = perimeter(poly)\n        poly_s = []\n        pco = pyclipper.PyclipperOffset()\n        if perimeter_poly:\n            d=area_poly*(1-r*r)/perimeter_poly\n            pco.AddPath(poly, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n            poly_s = pco.Execute(-d)\n        return poly_s\n    except Exception as e:\n        traceback.print_exc()\n        raise e\n\n#TODO:filter small text(when shrincked region shape is 0 no matter what scale ratio is)\ndef generate_seg(im_size, polys, tags, image_name, scale_ratio):\n    \'\'\'\n    :param im_size: input image size\n    :param polys: input text regions\n    :param tags: ignore text regions tags\n    :param image_index: for log\n    :param scale_ratio:ground truth scale ratio, default[0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n    :return:\n    seg_maps: segmentation results with different scale ratio, save in different channel\n    training_mask: ignore text regions\n    \'\'\'\n    h, w = im_size\n    #mark different text poly\n    seg_maps = np.zeros((h,w,6), dtype=np.uint8)\n    # mask used during traning, to ignore some hard areas\n    training_mask = np.ones((h, w), dtype=np.uint8)\n    ignore_poly_mark = []\n    for i in range(len(scale_ratio)):\n        seg_map = np.zeros((h,w), dtype=np.uint8)\n        for poly_idx, poly_tag in enumerate(zip(polys, tags)):\n            poly = poly_tag[0]\n            tag = poly_tag[1]\n\n            # ignore ###\n            if i == 0 and tag:\n                cv2.fillPoly(training_mask, poly.astype(np.int32)[np.newaxis, :, :], 0)\n                ignore_poly_mark.append(poly_idx)\n\n            # seg map\n            shrinked_polys = []\n            if poly_idx not in ignore_poly_mark:\n                shrinked_polys = shrink_poly(poly.copy(), scale_ratio[i])\n\n            if not len(shrinked_polys) and poly_idx not in ignore_poly_mark:\n                logger.info(""before shrink poly area:{} len(shrinked_poly) is 0,image {}"".format(\n                    abs(pyclipper.Area(poly)),image_name))\n                # if the poly is too small, then ignore it during training\n                cv2.fillPoly(training_mask, poly.astype(np.int32)[np.newaxis, :, :], 0)\n                ignore_poly_mark.append(poly_idx)\n                continue\n            for shrinked_poly in shrinked_polys:\n                seg_map = cv2.fillPoly(seg_map, [np.array(shrinked_poly).astype(np.int32)], 1)\n\n        seg_maps[..., i] = seg_map\n    return seg_maps, training_mask\n\n\ndef generator(input_size=512, batch_size=32,\n              background_ratio=3./8,\n              random_scale=np.array([0.125, 0.25,0.5, 1, 2.0, 3.0]),\n              vis=False,\n              scale_ratio=np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1.0])):\n    \'\'\'\n    reference from https://github.com/argman/EAST\n    :param input_size:\n    :param batch_size:\n    :param background_ratio:\n    :param random_scale:\n    :param vis:\n    :param scale_ratio:ground truth scale ratio\n    :return:\n    \'\'\'\n    image_list = np.array(get_files([\'jpg\', \'png\', \'jpeg\', \'JPG\']))\n\n    logger.info(\'{} training images in {}\'.format(\n        image_list.shape[0], FLAGS.training_data_path))\n    index = np.arange(0, image_list.shape[0])\n\n    while True:\n        np.random.shuffle(index)\n        images = []\n        image_fns = []\n        seg_maps = []\n        training_masks = []\n        for i in index:\n            try:\n                im_fn = image_list[i]\n                im = cv2.imread(im_fn)\n                if im is None:\n                    logger.info(im_fn)\n                h, w, _ = im.shape\n                txt_fn = im_fn.replace(os.path.basename(im_fn).split(\'.\')[1], \'txt\')\n                if not os.path.exists(txt_fn):\n                    continue\n\n                text_polys, text_tags = load_annoataion(txt_fn)\n                if text_polys.shape[0] == 0:\n                    continue\n                text_polys, text_tags = check_and_validate_polys(text_polys, text_tags, (h, w))\n\n                # random scale this image\n                rd_scale = np.random.choice(random_scale)\n                im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale)\n                text_polys *= rd_scale\n                # random crop a area from image\n                if np.random.rand() < background_ratio:\n                    # crop background\n                    im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=True)\n                    if text_polys.shape[0] > 0:\n                        # cannot find background\n                        continue\n                    # pad and resize image\n                    new_h, new_w, _ = im.shape\n                    #max_h_w_i = np.max([new_h, new_w, input_size])\n                    im_padded = np.zeros((new_h, new_w, 3), dtype=np.uint8)\n                    im_padded[:new_h, :new_w, :] = im.copy()\n                    im = cv2.resize(im_padded, dsize=(input_size, input_size))\n                    seg_map_per_image = np.zeros((input_size, input_size, scale_ratio.shape[0]), dtype=np.uint8)\n                    training_mask = np.ones((input_size, input_size), dtype=np.uint8)\n                else:\n                    im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=False)\n                    if text_polys.shape[0] == 0:\n                        continue\n                    # h, w, _ = im.shape\n\n                    # pad the image to the training input size or the longer side of image\n                    new_h, new_w, _ = im.shape\n                    #max_h_w_i = np.max([new_h, new_w, input_size])\n                    im_padded = np.zeros((new_h, new_w, 3), dtype=np.uint8)\n                    im_padded[:new_h, :new_w, :] = im.copy()\n                    im = im_padded\n                    # resize the image to input size\n                    new_h, new_w, _ = im.shape\n                    resize_h = input_size\n                    resize_w = input_size\n                    im = cv2.resize(im, dsize=(resize_w, resize_h))\n                    resize_ratio_3_x = resize_w/float(new_w)\n                    resize_ratio_3_y = resize_h/float(new_h)\n                    text_polys[:, :, 0] *= resize_ratio_3_x\n                    text_polys[:, :, 1] *= resize_ratio_3_y\n                    new_h, new_w, _ = im.shape\n                    seg_map_per_image, training_mask = generate_seg((new_h, new_w), text_polys, text_tags,\n                                                                     image_list[i], scale_ratio)\n                    if not len(seg_map_per_image):\n                        logger.info(""len(seg_map)==0 image: %d "" % i)\n                        continue\n\n                if vis:\n                    fig, axs = plt.subplots(3, 3, figsize=(20, 30))\n                    axs[0, 0].imshow(im[..., ::-1])\n                    axs[0, 0].set_xticks([])\n                    axs[0, 0].set_yticks([])\n                    axs[0, 1].imshow(seg_map_per_image[..., 0])\n                    axs[0, 1].set_xticks([])\n                    axs[0, 1].set_yticks([])\n                    axs[0, 2].imshow(seg_map_per_image[..., 1])\n                    axs[0, 2].set_xticks([])\n                    axs[0, 2].set_yticks([])\n                    axs[1, 0].imshow(seg_map_per_image[..., 2])\n                    axs[1, 0].set_xticks([])\n                    axs[1, 0].set_yticks([])\n                    axs[1, 1].imshow(seg_map_per_image[..., 3])\n                    axs[1, 1].set_xticks([])\n                    axs[1, 1].set_yticks([])\n                    axs[1, 2].imshow(seg_map_per_image[..., 4])\n                    axs[1, 2].set_xticks([])\n                    axs[1, 2].set_yticks([])\n                    axs[2, 0].imshow(seg_map_per_image[..., 5])\n                    axs[2, 0].set_xticks([])\n                    axs[2, 0].set_yticks([])\n                    axs[2, 1].imshow(training_mask)\n                    axs[2, 1].set_xticks([])\n                    axs[2, 1].set_yticks([])\n                    plt.tight_layout()\n                    plt.show()\n                    plt.close()\n\n                images.append(im[..., ::-1].astype(np.float32))\n                image_fns.append(im_fn)\n                seg_maps.append(seg_map_per_image[::4, ::4, :].astype(np.float32))\n                training_masks.append(training_mask[::4, ::4, np.newaxis].astype(np.float32))\n\n                if len(images) == batch_size:\n                    yield images, image_fns, seg_maps,  training_masks\n                    images = []\n                    image_fns = []\n                    seg_maps = []\n                    training_masks = []\n            except Exception as e:\n                traceback.print_exc()\n                continue\n\n\ndef get_batch(num_workers, **kwargs):\n    try:\n        enqueuer = GeneratorEnqueuer(generator(**kwargs), use_multiprocessing=True)\n        enqueuer.start(max_queue_size=24, workers=num_workers)\n        generator_output = None\n        while True:\n            while enqueuer.is_running():\n                if not enqueuer.queue.empty():\n                    generator_output = enqueuer.queue.get()\n                    break\n                else:\n                    time.sleep(0.01)\n            yield generator_output\n            generator_output = None\n    finally:\n        if enqueuer is not None:\n            enqueuer.stop()\n\n\nif __name__ == \'__main__\':\n    gen = get_batch(num_workers=2, vis=True)\n    while True:\n        image, bbox, im_info = next(gen)\n        logger.debug(\'done\')\n'"
utils/data_provider/data_util.py,0,"b'import multiprocessing\nimport threading\nimport time\n\nimport numpy as np\n\ntry:\n    import queue\nexcept ImportError:\n    import Queue as queue\n\n\nclass GeneratorEnqueuer():\n    def __init__(self, generator,\n                 use_multiprocessing=False,\n                 wait_time=0.05,\n                 random_seed=None):\n        self.wait_time = wait_time\n        self._generator = generator\n        self._use_multiprocessing = use_multiprocessing\n        self._threads = []\n        self._stop_event = None\n        self.queue = None\n        self.random_seed = random_seed\n\n    def start(self, workers=1, max_queue_size=10):\n        def data_generator_task():\n            while not self._stop_event.is_set():\n                try:\n                    if self._use_multiprocessing or self.queue.qsize() < max_queue_size:\n                        generator_output = next(self._generator)\n                        self.queue.put(generator_output)\n                    else:\n                        time.sleep(self.wait_time)\n                except Exception:\n                    self._stop_event.set()\n                    raise\n\n        try:\n            if self._use_multiprocessing:\n                self.queue = multiprocessing.Queue(maxsize=max_queue_size)\n                self._stop_event = multiprocessing.Event()\n            else:\n                self.queue = queue.Queue()\n                self._stop_event = threading.Event()\n\n            for _ in range(workers):\n                if self._use_multiprocessing:\n                    # Reset random seed else all children processes\n                    # share the same seed\n                    np.random.seed(self.random_seed)\n                    thread = multiprocessing.Process(target=data_generator_task)\n                    thread.daemon = True\n                    if self.random_seed is not None:\n                        self.random_seed += 1\n                else:\n                    thread = threading.Thread(target=data_generator_task)\n                self._threads.append(thread)\n                thread.start()\n        except:\n            self.stop()\n            raise\n\n    def is_running(self):\n        return self._stop_event is not None and not self._stop_event.is_set()\n\n    def stop(self, timeout=None):\n        if self.is_running():\n            self._stop_event.set()\n\n        for thread in self._threads:\n            if thread.is_alive():\n                if self._use_multiprocessing:\n                    thread.terminate()\n                else:\n                    thread.join(timeout)\n\n        if self._use_multiprocessing:\n            if self.queue is not None:\n                self.queue.close()\n\n        self._threads = []\n        self._stop_event = None\n        self.queue = None\n\n    def get(self):\n        while self.is_running():\n            if not self.queue.empty():\n                inputs = self.queue.get()\n                if inputs is not None:\n                    yield inputs\n            else:\n                time.sleep(self.wait_time)\n'"
