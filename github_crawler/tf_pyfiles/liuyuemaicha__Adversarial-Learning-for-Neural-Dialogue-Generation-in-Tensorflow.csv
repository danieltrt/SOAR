file_path,api_count,code
al_neural_dialogue_train.py,8,"b'import os\n\nimport tensorflow as tf\nimport numpy as np\nimport sys\nimport time\nimport gen.generator as gens\nimport disc.hier_disc as h_disc\nimport random\nimport utils.conf as conf\nimport utils.data_utils as data_utils\n\ngen_config = conf.gen_config\ndisc_config = conf.disc_config\nevl_config = conf.disc_config\n\n\n# pre train discriminator\ndef disc_pre_train():\n    #discs.train_step(disc_config, evl_config)\n    h_disc.hier_train(disc_config, evl_config)\n\n\n# pre train generator\ndef gen_pre_train():\n    gens.train(gen_config)\n\n\n# gen data for disc training\ndef gen_disc():\n    gens.decoder(gen_config)\n\n\n# test gen model\ndef gen_test():\n    gens.test_decoder(gen_config)\n\n\n# prepare disc_data for discriminator and generator\ndef disc_train_data(sess, gen_model, vocab, source_inputs, source_outputs,\n                    encoder_inputs, decoder_inputs, target_weights, bucket_id, mc_search=False):\n    train_query, train_answer = [], []\n    query_len = gen_config.buckets[bucket_id][0]\n    answer_len = gen_config.buckets[bucket_id][1]\n\n    for query, answer in zip(source_inputs, source_outputs):\n        query = query[:query_len] + [int(data_utils.PAD_ID)] * (query_len - len(query) if query_len > len(query) else 0)\n        train_query.append(query)\n        answer = answer[:-1] # del tag EOS\n        answer = answer[:answer_len] + [int(data_utils.PAD_ID)] * (answer_len - len(answer) if answer_len > len(answer) else 0)\n        train_answer.append(answer)\n        train_labels = [1 for _ in source_inputs]\n\n\n    def decoder(num_roll):\n        for _ in xrange(num_roll):\n            _, _, output_logits = gen_model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id,\n                                                 forward_only=True, mc_search=mc_search)\n\n            seq_tokens = []\n            resps = []\n            for seq in output_logits:\n                row_token = []\n                for t in seq:\n                    row_token.append(int(np.argmax(t, axis=0)))\n                seq_tokens.append(row_token)\n\n            seq_tokens_t = []\n            for col in range(len(seq_tokens[0])):\n                seq_tokens_t.append([seq_tokens[row][col] for row in range(len(seq_tokens))])\n\n            for seq in seq_tokens_t:\n                if data_utils.EOS_ID in seq:\n                    resps.append(seq[:seq.index(data_utils.EOS_ID)][:gen_config.buckets[bucket_id][1]])\n                else:\n                    resps.append(seq[:gen_config.buckets[bucket_id][1]])\n\n            for i, output in enumerate(resps):\n                output = output[:answer_len] + [data_utils.PAD_ID] * (answer_len - len(output) if answer_len > len(output) else 0)\n                train_query.append(train_query[i])\n                train_answer.append(output)\n                train_labels.append(0)\n\n        return train_query, train_answer, train_labels\n\n    if mc_search:\n        train_query, train_answer, train_labels = decoder(gen_config.beam_size)\n    else:\n        train_query, train_answer, train_labels = decoder(1)\n\n    return train_query, train_answer, train_labels\n\n\ndef softmax(x):\n    prob = np.exp(x) / np.sum(np.exp(x), axis=0)\n    return prob\n\n\n# discriminator api\ndef disc_step(sess, bucket_id, disc_model, train_query, train_answer, train_labels, forward_only=False):\n    feed_dict={}\n\n    for i in xrange(len(train_query)):\n\n        feed_dict[disc_model.query[i].name] = train_query[i]\n\n    for i in xrange(len(train_answer)):\n        feed_dict[disc_model.answer[i].name] = train_answer[i]\n\n    feed_dict[disc_model.target.name]=train_labels\n\n    loss = 0.0\n    if forward_only:\n        fetches = [disc_model.b_logits[bucket_id]]\n        logits = sess.run(fetches, feed_dict)\n        logits = logits[0]\n    else:\n        fetches = [disc_model.b_train_op[bucket_id], disc_model.b_loss[bucket_id], disc_model.b_logits[bucket_id]]\n        train_op, loss, logits = sess.run(fetches,feed_dict)\n\n    # softmax operation\n    logits = np.transpose(softmax(np.transpose(logits)))\n\n    reward, gen_num = 0.0, 0\n    for logit, label in zip(logits, train_labels):\n        if int(label) == 0:\n            reward += logit[1]\n            gen_num += 1\n    reward = reward / gen_num\n\n    return reward, loss\n\n\n# Adversarial Learning for Neural Dialogue Generation\ndef al_train():\n    with tf.Session() as sess:\n\n        vocab, rev_vocab, dev_set, train_set = gens.prepare_data(gen_config)\n        for set in train_set:\n            print(""al train len: "", len(set))\n\n        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(gen_config.buckets))]\n        train_total_size = float(sum(train_bucket_sizes))\n        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n                               for i in xrange(len(train_bucket_sizes))]\n\n        disc_model = h_disc.create_model(sess, disc_config, disc_config.name_model)\n        gen_model = gens.create_model(sess, gen_config, forward_only=False, name_scope=gen_config.name_model)\n\n        current_step = 0\n        step_time, disc_loss, gen_loss, t_loss, batch_reward = 0.0, 0.0, 0.0, 0.0, 0.0\n        gen_loss_summary = tf.Summary()\n        disc_loss_summary = tf.Summary()\n\n        gen_writer = tf.summary.FileWriter(gen_config.tensorboard_dir, sess.graph)\n        disc_writer = tf.summary.FileWriter(disc_config.tensorboard_dir, sess.graph)\n\n        while True:\n            current_step += 1\n            start_time = time.time()\n            random_number_01 = np.random.random_sample()\n            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                         if train_buckets_scale[i] > random_number_01])\n            # disc_config.max_len = gen_config.buckets[bucket_id][0] + gen_config.buckets[bucket_id][1]\n\n            print(""==================Update Discriminator: %d====================="" % current_step)\n            # 1.Sample (X,Y) from real disc_data\n            # print(""bucket_id: %d"" %bucket_id)\n            encoder_inputs, decoder_inputs, target_weights, source_inputs, source_outputs = gen_model.get_batch(train_set, bucket_id, gen_config.batch_size)\n\n            # 2.Sample (X,Y) and (X, ^Y) through ^Y ~ G(*|X)\n            train_query, train_answer, train_labels = disc_train_data(sess, gen_model, vocab, source_inputs, source_outputs,\n                                                        encoder_inputs, decoder_inputs, target_weights, bucket_id, mc_search=False)\n            print(""==============================mc_search: False==================================="")\n            if current_step % 200 == 0:\n                print(""train_query: "", len(train_query))\n                print(""train_answer: "", len(train_answer))\n                print(""train_labels: "", len(train_labels))\n                for i in xrange(len(train_query)):\n                    print(""lable: "", train_labels[i])\n                    print(""train_answer_sentence: "", train_answer[i])\n                    print("" "".join([tf.compat.as_str(rev_vocab[output]) for output in train_answer[i]]))\n\n            train_query = np.transpose(train_query)\n            train_answer = np.transpose(train_answer)\n\n            # 3.Update D using (X, Y ) as positive examples and(X, ^Y) as negative examples\n            _, disc_step_loss = disc_step(sess, bucket_id, disc_model, train_query, train_answer, train_labels, forward_only=False)\n            disc_loss += disc_step_loss / disc_config.steps_per_checkpoint\n\n            print(""==================Update Generator: %d========================="" % current_step)\n            # 1.Sample (X,Y) from real disc_data\n            update_gen_data = gen_model.get_batch(train_set, bucket_id, gen_config.batch_size)\n            encoder, decoder, weights, source_inputs, source_outputs = update_gen_data\n\n            # 2.Sample (X,Y) and (X, ^Y) through ^Y ~ G(*|X) with Monte Carlo search\n            train_query, train_answer, train_labels = disc_train_data(sess, gen_model, vocab, source_inputs, source_outputs,\n                                                                encoder, decoder, weights, bucket_id, mc_search=True)\n\n            print(""=============================mc_search: True===================================="")\n            if current_step % 200 == 0:\n                for i in xrange(len(train_query)):\n                    print(""lable: "", train_labels[i])\n                    print("" "".join([tf.compat.as_str(rev_vocab[output]) for output in train_answer[i]]))\n\n            train_query = np.transpose(train_query)\n            train_answer = np.transpose(train_answer)\n\n            # 3.Compute Reward r for (X, ^Y ) using D.---based on Monte Carlo search\n            reward, _ = disc_step(sess, bucket_id, disc_model, train_query, train_answer, train_labels, forward_only=True)\n            batch_reward += reward / gen_config.steps_per_checkpoint\n            print(""step_reward: "", reward)\n\n            # 4.Update G on (X, ^Y ) using reward r\n            gan_adjusted_loss, gen_step_loss, _ =gen_model.step(sess, encoder, decoder, weights, bucket_id, forward_only=False,\n                                           reward=reward, up_reward=True, debug=True)\n            gen_loss += gen_step_loss / gen_config.steps_per_checkpoint\n\n            print(""gen_step_loss: "", gen_step_loss)\n            print(""gen_step_adjusted_loss: "", gan_adjusted_loss)\n\n            # 5.Teacher-Forcing: Update G on (X, Y )\n            t_adjusted_loss, t_step_loss, a = gen_model.step(sess, encoder, decoder, weights, bucket_id, forward_only=False)\n            t_loss += t_step_loss / gen_config.steps_per_checkpoint\n           \n            print(""t_step_loss: "", t_step_loss)\n            print(""t_adjusted_loss"", t_adjusted_loss)           # print(""normal: "", a)\n\n            if current_step % gen_config.steps_per_checkpoint == 0:\n\n                step_time += (time.time() - start_time) / gen_config.steps_per_checkpoint\n\n                print(""current_steps: %d, step time: %.4f, disc_loss: %.3f, gen_loss: %.3f, t_loss: %.3f, reward: %.3f""\n                      %(current_step, step_time, disc_loss, gen_loss, t_loss, batch_reward))\n\n                disc_loss_value = disc_loss_summary.value.add()\n                disc_loss_value.tag = disc_config.name_loss\n                disc_loss_value.simple_value = float(disc_loss)\n                disc_writer.add_summary(disc_loss_summary, int(sess.run(disc_model.global_step)))\n\n                gen_global_steps = sess.run(gen_model.global_step)\n                gen_loss_value = gen_loss_summary.value.add()\n                gen_loss_value.tag = gen_config.name_loss\n                gen_loss_value.simple_value = float(gen_loss)\n                t_loss_value = gen_loss_summary.value.add()\n                t_loss_value.tag = gen_config.teacher_loss\n                t_loss_value.simple_value = float(t_loss)\n                batch_reward_value = gen_loss_summary.value.add()\n                batch_reward_value.tag = gen_config.reward_name\n                batch_reward_value.simple_value = float(batch_reward)\n                gen_writer.add_summary(gen_loss_summary, int(gen_global_steps))\n\n                if current_step % (gen_config.steps_per_checkpoint * 2) == 0:\n                    print(""current_steps: %d, save disc model"" % current_step)\n                    disc_ckpt_dir = os.path.abspath(os.path.join(disc_config.train_dir, ""checkpoints""))\n                    if not os.path.exists(disc_ckpt_dir):\n                        os.makedirs(disc_ckpt_dir)\n                    disc_model_path = os.path.join(disc_ckpt_dir, ""disc.model"")\n                    disc_model.saver.save(sess, disc_model_path, global_step=disc_model.global_step)\n\n                    print(""current_steps: %d, save gen model"" % current_step)\n                    gen_ckpt_dir = os.path.abspath(os.path.join(gen_config.train_dir, ""checkpoints""))\n                    if not os.path.exists(gen_ckpt_dir):\n                        os.makedirs(gen_ckpt_dir)\n                    gen_model_path = os.path.join(gen_ckpt_dir, ""gen.model"")\n                    gen_model.saver.save(sess, gen_model_path, global_step=gen_model.global_step)\n\n                step_time, disc_loss, gen_loss, t_loss, batch_reward = 0.0, 0.0, 0.0, 0.0, 0.0\n                sys.stdout.flush()\n\n\ndef main(_):\n    # step_1 training gen model\n    gen_pre_train()\n\n    # model test\n    # gen_test()\n\n    # step_2 gen training data for disc\n    # gen_disc()\n\n    # step_3 training disc model\n    # disc_pre_train()\n\n    # step_4 training al model\n    # al_train()\n\n    # model test\n    # gen_test()\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
disc/__init__.py,0,b''
disc/hier_disc.py,8,"b'import tensorflow as tf\nimport numpy as np\nimport os\nimport time\nimport datetime\nimport random\nimport utils.data_utils as data_utils\nfrom hier_rnn_model import Hier_rnn_model\nfrom tensorflow.python.platform import gfile\nimport sys\n\nsys.path.append(""../utils"")\n\n\ndef evaluate(session, model, config, evl_inputs, evl_labels, evl_masks):\n    total_num = len(evl_inputs[0])\n\n    fetches = [model.correct_num, model.prediction, model.logits, model.target]\n    feed_dict = {}\n    for i in xrange(config.max_len):\n        feed_dict[model.input_data[i].name] = evl_inputs[i]\n    feed_dict[model.target.name] = evl_labels\n    feed_dict[model.mask_x.name] = evl_masks\n    correct_num, prediction, logits, target = session.run(fetches, feed_dict)\n\n    print(""total_num: "", total_num)\n    print(""correct_num: "", correct_num)\n    print(""prediction: "", prediction)\n    print(""target: "", target)\n\n    accuracy = float(correct_num) / total_num\n    return accuracy\n\n\ndef hier_read_data(config, query_path, answer_path, gen_path):\n    query_set = [[] for _ in config.buckets]\n    answer_set = [[] for _ in config.buckets]\n    gen_set = [[] for _ in config.buckets]\n    with gfile.GFile(query_path, mode=""r"") as query_file:\n        with gfile.GFile(answer_path, mode=""r"") as answer_file:\n            with gfile.GFile(gen_path, mode=""r"") as gen_file:\n                query, answer, gen = query_file.readline(), answer_file.readline(), gen_file.readline()\n                counter = 0\n                while query and answer and gen:\n                    counter += 1\n                    if counter % 100000 == 0:\n                        print(""  reading disc_data line %d"" % counter)\n                    query = [int(id) for id in query.strip().split()]\n                    answer = [int(id) for id in answer.strip().split()]\n                    gen = [int(id) for id in gen.strip().split()]\n                    for i, (query_size, answer_size) in enumerate(config.buckets):\n                        if len(query) <= query_size and len(answer) <= answer_size and len(gen) <= answer_size:\n                            query = query[:query_size] + [data_utils.PAD_ID] * (query_size - len(query) if query_size > len(query) else 0)\n                            query_set[i].append(query)\n                            answer = answer[:answer_size] + [data_utils.PAD_ID] * (answer_size - len(answer) if answer_size > len(answer) else 0)\n                            answer_set[i].append(answer)\n                            gen = gen[:answer_size] + [data_utils.PAD_ID] * (answer_size - len(gen) if answer_size > len(gen) else 0)\n                            gen_set[i].append(gen)\n                    query, answer, gen = query_file.readline(), answer_file.readline(), gen_file.readline()\n\n    return query_set, answer_set, gen_set\n\n\ndef hier_get_batch(config, max_set, query_set, answer_set, gen_set):\n    batch_size = config.batch_size\n    if batch_size % 2 == 1:\n        return IOError(""Error"")\n    train_query = []\n    train_answer = []\n    train_labels = []\n    half_size = batch_size / 2\n    for _ in range(half_size):\n        index = random.randint(0, max_set)\n        train_query.append(query_set[index])\n        train_answer.append(answer_set[index])\n        train_labels.append(1)\n        train_query.append(query_set[index])\n        train_answer.append(gen_set[index])\n        train_labels.append(0)\n    return train_query, train_answer, train_labels\n\n\ndef create_model(sess, config, name_scope, initializer=None):\n    with tf.variable_scope(name_or_scope=name_scope, initializer=initializer):\n        model = Hier_rnn_model(config=config, name_scope=name_scope)\n        disc_ckpt_dir = os.path.abspath(os.path.join(config.train_dir, ""checkpoints""))\n        ckpt = tf.train.get_checkpoint_state(disc_ckpt_dir)\n        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n            print(""Reading Hier Disc model parameters from %s"" % ckpt.model_checkpoint_path)\n            model.saver.restore(sess, ckpt.model_checkpoint_path)\n        else:\n            print(""Created Hier Disc model with fresh parameters."")\n            disc_global_variables = [gv for gv in tf.global_variables() if name_scope in gv.name]\n            sess.run(tf.variables_initializer(disc_global_variables))\n        return model\n\n\ndef prepare_data(config):\n    train_path = os.path.join(config.train_dir, ""train"")\n    voc_file_path = [train_path + "".query"", train_path + "".answer"", train_path + "".gen""]\n    vocab_path = os.path.join(config.train_dir, ""vocab%d.all"" % config.vocab_size)\n    data_utils.create_vocabulary(vocab_path, voc_file_path, config.vocab_size)\n    vocab, rev_vocab = data_utils.initialize_vocabulary(vocab_path)\n\n    print(""Preparing train disc_data in %s"" % config.train_dir)\n    train_query_path, train_answer_path, train_gen_path, dev_query_path, dev_answer_path, dev_gen_path = \\\n        data_utils.hier_prepare_disc_data(config.train_dir, vocab, config.vocab_size)\n    query_set, answer_set, gen_set = hier_read_data(config, train_query_path, train_answer_path, train_gen_path)\n    return query_set, answer_set, gen_set\n\n\ndef softmax(x):\n    prob = np.exp(x) / np.sum(np.exp(x), axis=0)\n    return prob\n\n\ndef hier_train(config_disc, config_evl):\n    config_evl.keep_prob = 1.0\n\n    print(""begin training"")\n\n    with tf.Session() as session:\n\n        print ""prepare_data""\n        query_set, answer_set, gen_set = prepare_data(config_disc)\n\n        train_bucket_sizes = [len(query_set[b]) for b in xrange(len(config_disc.buckets))]\n        train_total_size = float(sum(train_bucket_sizes))\n        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n                               for i in xrange(len(train_bucket_sizes))]\n        #dev_query_set, dev_answer_set, dev_gen_set = hier_read_data(dev_query_path, dev_answer_path, dev_gen_path)\n        for set in query_set:\n            print(""set length: "", len(set))\n\n        model = create_model(session, config_disc, name_scope=config_disc.name_model)\n\n        step_time, loss = 0.0, 0.0\n        current_step = 0\n        #previous_losses = []\n        step_loss_summary = tf.Summary()\n        disc_writer = tf.summary.FileWriter(config_disc.tensorboard_dir, session.graph)\n\n        while True:\n            random_number_01 = np.random.random_sample()\n            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                             if train_buckets_scale[i] > random_number_01])\n\n            start_time = time.time()\n\n            b_query, b_answer, b_gen = query_set[bucket_id], answer_set[bucket_id], gen_set[bucket_id]\n\n            train_query, train_answer, train_labels = hier_get_batch(config_disc, len(b_query)-1, b_query, b_answer, b_gen)\n\n            train_query = np.transpose(train_query)\n            train_answer = np.transpose(train_answer)\n\n            feed_dict = {}\n            for i in xrange(config_disc.buckets[bucket_id][0]):\n                feed_dict[model.query[i].name] = train_query[i]\n            for i in xrange(config_disc.buckets[bucket_id][1]):\n                feed_dict[model.answer[i].name] = train_answer[i]\n            feed_dict[model.target.name] = train_labels\n\n            fetches = [model.b_train_op[bucket_id], model.b_logits[bucket_id], model.b_loss[bucket_id], model.target]\n            train_op, logits, step_loss, target = session.run(fetches, feed_dict)\n\n            step_time += (time.time() - start_time) / config_disc.steps_per_checkpoint\n            loss += step_loss /config_disc.steps_per_checkpoint\n            current_step += 1\n\n            if current_step % config_disc.steps_per_checkpoint == 0:\n\n                disc_loss_value = step_loss_summary.value.add()\n                disc_loss_value.tag = config_disc.name_loss\n                disc_loss_value.simple_value = float(loss)\n\n                disc_writer.add_summary(step_loss_summary, int(session.run(model.global_step)))\n\n                print(""logits shape: "", np.shape(logits))\n\n                # softmax operation\n                logits = np.transpose(softmax(np.transpose(logits)))\n\n                reward = 0.0\n                for logit, label in zip(logits, train_labels):\n                    reward += logit[1]  # only for true probility\n                reward = reward / len(train_labels)\n                print(""reward: "", reward)\n\n\n                print(""current_step: %d, step_loss: %.4f"" %(current_step, step_loss))\n\n\n                if current_step % (config_disc.steps_per_checkpoint * 3) == 0:\n                    print(""current_step: %d, save_model"" % (current_step))\n                    disc_ckpt_dir = os.path.abspath(os.path.join(config_disc.train_dir, ""checkpoints""))\n                    if not os.path.exists(disc_ckpt_dir):\n                        os.makedirs(disc_ckpt_dir)\n                    disc_model_path = os.path.join(disc_ckpt_dir, ""disc.model"")\n                    model.saver.save(session, disc_model_path, global_step=model.global_step)\n\n\n                step_time, loss = 0.0, 0.0\n                sys.stdout.flush()\n\n'"
disc/hier_rnn_model.py,35,"b'import tensorflow as tf\nimport numpy as np\n\n\nclass Hier_rnn_model(object):\n    def __init__(self, config, name_scope, dtype=tf.float32):\n        #with tf.variable_scope(name_or_scope=scope_name):\n        emb_dim = config.embed_dim\n        num_layers = config.num_layers\n        vocab_size = config.vocab_size\n        #max_len = config.max_len\n        num_class = config.num_class\n        buckets = config.buckets\n        self.lr = config.lr\n        self.global_step = tf.Variable(initial_value=0, trainable=False)\n\n        self.query = []\n        self.answer = []\n        for i in range(buckets[-1][0]):\n            self.query.append(tf.placeholder(dtype=tf.int32, shape=[None], name=""query{0}"".format(i)))\n        for i in xrange(buckets[-1][1]):\n            self.answer.append(tf.placeholder(dtype=tf.int32, shape=[None], name=""answer{0}"".format(i)))\n\n        self.target = tf.placeholder(dtype=tf.int64, shape=[None], name=""target"")\n\n        encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(emb_dim)\n        encoder_mutil = tf.nn.rnn_cell.MultiRNNCell([encoder_cell] * num_layers)\n        encoder_emb = tf.nn.rnn_cell.EmbeddingWrapper(encoder_mutil, embedding_classes=vocab_size, embedding_size=emb_dim)\n\n        context_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=emb_dim)\n        context_multi = tf.nn.rnn_cell.MultiRNNCell([context_cell] * num_layers)\n\n        self.b_query_state = []\n        self.b_answer_state = []\n        self.b_state = []\n        self.b_logits = []\n        self.b_loss = []\n        #self.b_cost = []\n        self.b_train_op = []\n        for i, bucket in enumerate(buckets):\n            with tf.variable_scope(name_or_scope=""Hier_RNN_encoder"", reuse=True if i > 0 else None) as var_scope:\n                query_output, query_state = tf.nn.rnn(encoder_emb, inputs=self.query[:bucket[0]], dtype=tf.float32)\n                # output [max_len, batch_size, emb_dim]   state [num_layer, 2, batch_size, emb_dim]\n                var_scope.reuse_variables()\n                answer_output, answer_state = tf.nn.rnn(encoder_emb, inputs=self.answer[:bucket[1]], dtype=tf.float32)\n                self.b_query_state.append(query_state)\n                self.b_answer_state.append(answer_state)\n                context_input = [query_state[-1][1], answer_state[-1][1]]\n\n            with tf.variable_scope(name_or_scope=""Hier_RNN_context"", reuse=True if i > 0 else None):\n                output, state = tf.nn.rnn(context_multi, context_input, dtype=tf.float32)\n                self.b_state.append(state)\n                top_state = state[-1][1]  # [batch_size, emb_dim]\n\n            with tf.variable_scope(""Softmax_layer_and_output"", reuse=True if i > 0 else None):\n                softmax_w = tf.get_variable(""softmax_w"", [emb_dim, num_class], dtype=tf.float32)\n                softmax_b = tf.get_variable(""softmax_b"", [num_class], dtype=tf.float32)\n                logits = tf.matmul(top_state, softmax_w) + softmax_b\n                self.b_logits.append(logits)\n\n            with tf.name_scope(""loss""):\n                loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, self.target)\n                mean_loss = tf.reduce_mean(loss)\n                self.b_loss.append(mean_loss)\n\n            with tf.name_scope(""gradient_descent""):\n                disc_params = [var for var in tf.trainable_variables() if name_scope in var.name]\n                grads, norm = tf.clip_by_global_norm(tf.gradients(mean_loss, disc_params), config.max_grad_norm)\n                #optimizer = tf.train.GradientDescentOptimizer(self.lr)\n                optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n                train_op = optimizer.apply_gradients(zip(grads, disc_params), global_step=self.global_step)\n                self.b_train_op.append(train_op)\n\n        all_variables = [v for v in tf.global_variables() if name_scope in v.name]\n        self.saver = tf.train.Saver(all_variables)\n\n\nclass Config(object):\n    embed_dim = 12\n    lr = 0.1\n    num_class = 2\n    train_dir = \'./disc_data/\'\n    name_model = ""disc_model""\n    tensorboard_dir = ""./tensorboard/disc_log/""\n    name_loss = ""disc_loss""\n    num_layers = 3\n    vocab_size = 10\n    max_len = 50\n    batch_size = 1\n    init_scale = 0.1\n    buckets = [(5, 10), (10, 15), (20, 25), (40, 50), (50, 50)]\n    max_grad_norm = 5\n\n\ndef main(_):\n    with tf.Session() as sess:\n        query = [[1],[2],[3],[4],[5]]\n        answer = [[6],[7],[8],[9],[0],[0],[0],[0],[0],[0]]\n        target = [1]\n        config = Config\n        initializer = tf.random_uniform_initializer(-1 * config.init_scale, 1 * config.init_scale)\n        with tf.variable_scope(name_or_scope=""rnn_model"", initializer=initializer):\n            model = Hier_rnn_model(config, name_scope=config.name_model)\n            sess.run(tf.global_variables_initializer())\n        input_feed = {}\n        for i in range(config.buckets[0][0]):\n            input_feed[model.query[i].name] = query[i]\n        for i in range(config.buckets[0][1]):\n            input_feed[model.answer[i].name] = answer[i]\n        input_feed[model.target.name] = target\n\n        fetches = [model.b_train_op[0], model.b_query_state[0],  model.b_state[0], model.b_logits[0]]\n\n        train_op, query, state, logits = sess.run(fetches=fetches, feed_dict=input_feed)\n\n        print(""query: "", np.shape(query))\n\n    pass\n\nif __name__ == \'__main__\':\n    tf.app.run()\n\n\n\n'"
gen/__init__.py,0,b''
gen/gen_model.py,39,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\nimport sys\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\nimport utils.data_utils as data_utils\nimport gen.seq2seq as rl_seq2seq\nfrom tensorflow.python.ops import variable_scope\nsys.path.append(\'../utils\')\n\n\nclass Seq2SeqModel(object):\n\n    def __init__(self, config, name_scope, forward_only=False, num_samples=512, dtype=tf.float32):\n\n        # self.scope_name = scope_name\n        # with tf.variable_scope(self.scope_name):\n        source_vocab_size = config.vocab_size\n        target_vocab_size = config.vocab_size\n        emb_dim = config.emb_dim\n\n        self.buckets = config.buckets\n        self.learning_rate = tf.Variable(float(config.learning_rate), trainable=False, dtype=dtype)\n        self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * config.learning_rate_decay_factor)\n        self.global_step = tf.Variable(0, trainable=False)\n        self.batch_size = config.batch_size\n        self.num_layers = config.num_layers\n        self.max_gradient_norm = config.max_gradient_norm\n        self.mc_search = tf.placeholder(tf.bool, name=""mc_search"")\n        self.forward_only = tf.placeholder(tf.bool, name=""forward_only"")\n        self.up_reward = tf.placeholder(tf.bool, name=""up_reward"")\n        self.reward_bias = tf.get_variable(""reward_bias"", [1], dtype=tf.float32)\n        # If we use sampled softmax, we need an output projection.\n        output_projection = None\n        softmax_loss_function = None\n        # Sampled softmax only makes sense if we sample less than vocabulary size.\n        if num_samples > 0 and num_samples < target_vocab_size:\n            w_t = tf.get_variable(""proj_w"", [target_vocab_size, emb_dim], dtype=dtype)\n            w = tf.transpose(w_t)\n            b = tf.get_variable(""proj_b"", [target_vocab_size], dtype=dtype)\n            output_projection = (w, b)\n\n            def sampled_loss(inputs, labels):\n                labels = tf.reshape(labels, [-1, 1])\n                # We need to compute the sampled_softmax_loss using 32bit floats to\n                # avoid numerical instabilities.\n                local_w_t = tf.cast(w_t, tf.float32)\n                local_b = tf.cast(b, tf.float32)\n                local_inputs = tf.cast(inputs, tf.float32)\n                return tf.cast(\n                    tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\n                                               num_samples, target_vocab_size), dtype)\n\n            softmax_loss_function = sampled_loss\n\n        # Create the internal multi-layer cell for our RNN.\n        single_cell = tf.nn.rnn_cell.GRUCell(emb_dim)\n        cell = single_cell\n        if self.num_layers > 1:\n            cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * self.num_layers)\n\n        # The seq2seq function: we use embedding for the input and attention.\n        def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n            return rl_seq2seq.embedding_attention_seq2seq(\n                encoder_inputs,\n                decoder_inputs,\n                cell,\n                num_encoder_symbols= source_vocab_size,\n                num_decoder_symbols= target_vocab_size,\n                embedding_size= emb_dim,\n                output_projection=output_projection,\n                feed_previous=do_decode,\n                mc_search=self.mc_search,\n                dtype=dtype)\n\n        # Feeds for inputs.\n        self.encoder_inputs = []\n        self.decoder_inputs = []\n        self.target_weights = []\n        for i in xrange(self.buckets[-1][0]):  # Last bucket is the biggest one.\n            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=""encoder{0}"".format(i)))\n        for i in xrange(self.buckets[-1][1] + 1):\n            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=""decoder{0}"".format(i)))\n            self.target_weights.append(tf.placeholder(dtype, shape=[None], name=""weight{0}"".format(i)))\n        self.reward = [tf.placeholder(tf.float32, name=""reward_%i"" % i) for i in range(len(self.buckets))]\n\n        # Our targets are decoder inputs shifted by one.\n        targets = [self.decoder_inputs[i + 1] for i in xrange(len(self.decoder_inputs) - 1)]\n\n        self.outputs, self.losses, self.encoder_state = rl_seq2seq.model_with_buckets(\n            self.encoder_inputs, self.decoder_inputs, targets, self.target_weights,\n            self.buckets, source_vocab_size, self.batch_size,\n            lambda x, y: seq2seq_f(x, y, tf.select(self.forward_only, True, False)),\n            output_projection=output_projection, softmax_loss_function=softmax_loss_function)\n\n        for b in xrange(len(self.buckets)):\n            self.outputs[b] = [\n                tf.cond(\n                    self.forward_only,\n                    lambda: tf.matmul(output, output_projection[0]) + output_projection[1],\n                    lambda: output\n                )\n                for output in self.outputs[b]\n            ]\n        \n        if not forward_only:\n            with tf.name_scope(""gradient_descent""):\n                self.gradient_norms = []\n                self.updates = []\n                self.aj_losses = []\n                self.gen_params = [p for p in tf.trainable_variables() if name_scope in p.name]\n                #opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n                opt = tf.train.AdamOptimizer()\n                for b in xrange(len(self.buckets)):\n                    R =  tf.sub(self.reward[b], self.reward_bias)\n                    # self.reward[b] = self.reward[b] - reward_bias\n                    adjusted_loss = tf.cond(self.up_reward,\n                                              lambda:tf.mul(self.losses[b], self.reward[b]),\n                                              lambda: self.losses[b])\n\n                    # adjusted_loss =  tf.cond(self.up_reward,\n                    #                           lambda: tf.mul(self.losses[b], R),\n                    #                           lambda: self.losses[b])\n                    self.aj_losses.append(adjusted_loss)\n                    gradients = tf.gradients(adjusted_loss, self.gen_params)\n                    clipped_gradients, norm = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n                    self.gradient_norms.append(norm)\n                    self.updates.append(opt.apply_gradients(\n                        zip(clipped_gradients, self.gen_params), global_step=self.global_step))\n\n        self.gen_variables = [k for k in tf.global_variables() if name_scope in k.name]\n        self.saver = tf.train.Saver(self.gen_variables)\n\n    def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n           bucket_id, forward_only=True, reward=1, mc_search=False, up_reward=False, debug=True):\n        # Check if the sizes match.\n        encoder_size, decoder_size = self.buckets[bucket_id]\n        if len(encoder_inputs) != encoder_size:\n            raise ValueError(""Encoder length must be equal to the one in bucket,""\n                         "" %d != %d."" % (len(encoder_inputs), encoder_size))\n        if len(decoder_inputs) != decoder_size:\n            raise ValueError(""Decoder length must be equal to the one in bucket,""\n                         "" %d != %d."" % (len(decoder_inputs), decoder_size))\n        if len(target_weights) != decoder_size:\n            raise ValueError(""Weights length must be equal to the one in bucket,""\n                         "" %d != %d."" % (len(target_weights), decoder_size))\n\n        # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n\n        input_feed = {\n            self.forward_only.name: forward_only,\n            self.up_reward.name:  up_reward,\n            self.mc_search.name: mc_search\n        }\n        for l in xrange(len(self.buckets)):\n            input_feed[self.reward[l].name] = reward\n        for l in xrange(encoder_size):\n            input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n        for l in xrange(decoder_size):\n            input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n            input_feed[self.target_weights[l].name] = target_weights[l]\n\n        # Since our targets are decoder inputs shifted by one, we need one more.\n        last_target = self.decoder_inputs[decoder_size].name\n        input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n\n        # Output feed: depends on whether we do a backward step or not.\n        if not forward_only: # normal training\n            output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                       self.aj_losses[bucket_id],  # Gradient norm.\n                       self.losses[bucket_id]]  # Loss for this batch.\n        else: # testing or reinforcement learning\n            output_feed = [self.encoder_state[bucket_id], self.losses[bucket_id]]  # Loss for this batch.\n            for l in xrange(decoder_size):  # Output logits.\n                output_feed.append(self.outputs[bucket_id][l])\n\n        outputs = session.run(output_feed, input_feed)\n        if not forward_only:\n            return outputs[1], outputs[2], outputs[0]  # Gradient norm, loss, no outputs.\n        else:\n            return outputs[0], outputs[1], outputs[2:]  # encoder_state, loss, outputs.\n\n    def get_batch(self, train_data, bucket_id, batch_size, type=0):\n\n        encoder_size, decoder_size = self.buckets[bucket_id]\n        encoder_inputs, decoder_inputs = [], []\n\n        # pad them if needed, reverse encoder inputs and add GO to decoder.\n        batch_source_encoder, batch_source_decoder = [], []\n        # print(""bucket_id: %s"" %bucket_id)\n        if type == 1:\n            batch_size = 1\n        for batch_i in xrange(batch_size):\n            if type == 1:\n                encoder_input, decoder_input = train_data[bucket_id]\n            elif type == 2:\n                # print(""disc_data[bucket_id]: "", disc_data[bucket_id][0])\n                encoder_input_a, decoder_input = train_data[bucket_id][0]\n                encoder_input = encoder_input_a[batch_i]\n            elif type == 0:\n                encoder_input, decoder_input = random.choice(train_data[bucket_id])\n                # print(""train en: %s, de: %s"" %(encoder_input, decoder_input))\n\n            batch_source_encoder.append(encoder_input)\n            batch_source_decoder.append(decoder_input)\n            # Encoder inputs are padded and then reversed.\n            encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n            encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n\n            # Decoder inputs get an extra ""GO"" symbol, and are padded then.\n            decoder_pad_size = decoder_size - len(decoder_input) - 1\n            decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n                                [data_utils.PAD_ID] * decoder_pad_size)\n\n        # Now we create batch-major vectors from the disc_data selected above.\n        batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n\n        # Batch encoder inputs are just re-indexed encoder_inputs.\n        for length_idx in xrange(encoder_size):\n            batch_encoder_inputs.append(\n                np.array([encoder_inputs[batch_idx][length_idx]\n                          for batch_idx in xrange(batch_size)], dtype=np.int32))\n\n        # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n        for length_idx in xrange(decoder_size):\n            batch_decoder_inputs.append(\n                np.array([decoder_inputs[batch_idx][length_idx]\n                          for batch_idx in xrange(batch_size)], dtype=np.int32))\n\n            # Create target_weights to be 0 for targets that are padding.\n            batch_weight = np.ones(batch_size, dtype=np.float32)\n            for batch_idx in xrange(batch_size):\n                # We set weight to 0 if the corresponding target is a PAD symbol.\n                # The corresponding target is decoder_input shifted by 1 forward.\n                if length_idx < decoder_size - 1:\n                    target = decoder_inputs[batch_idx][length_idx + 1]\n                if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n                    batch_weight[batch_idx] = 0.0\n            batch_weights.append(batch_weight)\n\n        return (batch_encoder_inputs, batch_decoder_inputs, batch_weights, batch_source_encoder, batch_source_decoder)\n'"
gen/generator.py,15,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport random\nimport sys\nimport time\nimport pickle\nimport heapq\nimport tensorflow.python.platform\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nimport utils.data_utils as data_utils\nimport utils.conf as conf\nimport gen.gen_model as seq2seq_model\nfrom tensorflow.python.platform import gfile\nsys.path.append(\'../utils\')\n\n# We use a number of buckets and pad to the closest one for efficiency.\n# See seq2seq_model.Seq2SeqModel for details of how they work.\n\n\ndef read_data(config, source_path, target_path, max_size=None):\n    data_set = [[] for _ in config.buckets]\n    with gfile.GFile(source_path, mode=""r"") as source_file:\n        with gfile.GFile(target_path, mode=""r"") as target_file:\n            source, target = source_file.readline(), target_file.readline()\n            counter = 0\n            while source and target and (not max_size or counter < max_size):\n                counter += 1\n                if counter % 100000 == 0:\n                    print(""  reading disc_data line %d"" % counter)\n                    sys.stdout.flush()\n                source_ids = [int(x) for x in source.split()]\n                target_ids = [int(x) for x in target.split()]\n                target_ids.append(data_utils.EOS_ID)\n                for bucket_id, (source_size, target_size) in enumerate(config.buckets): #[bucket_id, (source_size, target_size)]\n                    if len(source_ids) < source_size and len(target_ids) < target_size:\n                        data_set[bucket_id].append([source_ids, target_ids])\n                        break\n                source, target = source_file.readline(), target_file.readline()\n    return data_set\n\n\ndef create_model(session, gen_config, forward_only, name_scope, initializer=None):\n    """"""Create translation model and initialize or load parameters in session.""""""\n    with tf.variable_scope(name_or_scope=name_scope, initializer=initializer):\n        model = seq2seq_model.Seq2SeqModel(gen_config,  name_scope=name_scope, forward_only=forward_only)\n        gen_ckpt_dir = os.path.abspath(os.path.join(gen_config.train_dir, ""checkpoints""))\n        ckpt = tf.train.get_checkpoint_state(gen_ckpt_dir)\n        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n            print(""Reading Gen model parameters from %s"" % ckpt.model_checkpoint_path)\n            model.saver.restore(session, ckpt.model_checkpoint_path)\n        else:\n            print(""Created Gen model with fresh parameters."")\n            gen_global_variables = [gv for gv in tf.global_variables() if name_scope in gv.name]\n            session.run(tf.variables_initializer(gen_global_variables))\n        return model\n\n\ndef prepare_data(gen_config):\n    train_path = os.path.join(gen_config.train_dir, ""chitchat.train"")\n    voc_file_path = [train_path+"".answer"", train_path+"".query""]\n    vocab_path = os.path.join(gen_config.train_dir, ""vocab%d.all"" % gen_config.vocab_size)\n    data_utils.create_vocabulary(vocab_path, voc_file_path, gen_config.vocab_size)\n    vocab, rev_vocab = data_utils.initialize_vocabulary(vocab_path)\n\n    print(""Preparing Chitchat gen_data in %s"" % gen_config.train_dir)\n    train_query, train_answer, dev_query, dev_answer = data_utils.prepare_chitchat_data(\n        gen_config.train_dir, vocab, gen_config.vocab_size)\n\n    # Read disc_data into buckets and compute their sizes.\n    print (""Reading development and training gen_data (limit: %d).""\n               % gen_config.max_train_data_size)\n    dev_set = read_data(gen_config, dev_query, dev_answer)\n    train_set = read_data(gen_config, train_query, train_answer, gen_config.max_train_data_size)\n\n    return vocab, rev_vocab, dev_set, train_set\n\n\ndef softmax(x):\n    prob = np.exp(x) / np.sum(np.exp(x), axis=0)\n    return prob\n\n\ndef train(gen_config):\n    vocab, rev_vocab, dev_set, train_set = prepare_data(gen_config)\n    for b_set in train_set:\n        print(""b_set: "", len(b_set))\n\n    with tf.Session() as sess:\n    #with tf.device(""/gpu:1""):\n        # Create model.\n        print(""Creating %d layers of %d units."" % (gen_config.num_layers, gen_config.emb_dim))\n        model = create_model(sess, gen_config, forward_only=False, name_scope=gen_config.name_model)\n\n        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(gen_config.buckets))]\n        train_total_size = float(sum(train_bucket_sizes))\n        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n                               for i in xrange(len(train_bucket_sizes))]\n\n        # This is the training loop.\n        step_time, loss = 0.0, 0.0\n        current_step = 0\n        #previous_losses = []\n\n        gen_loss_summary = tf.Summary()\n        gen_writer = tf.summary.FileWriter(gen_config.tensorboard_dir, sess.graph)\n\n        while True:\n            # Choose a bucket according to disc_data distribution. We pick a random number\n            # in [0, 1] and use the corresponding interval in train_buckets_scale.\n            random_number_01 = np.random.random_sample()\n            bucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\n\n            # Get a batch and make a step.\n            start_time = time.time()\n            encoder_inputs, decoder_inputs, target_weights, batch_source_encoder, batch_source_decoder = model.get_batch(\n                train_set, bucket_id, gen_config.batch_size)\n\n            _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only=False)\n\n            step_time += (time.time() - start_time) / gen_config.steps_per_checkpoint\n            loss += step_loss / gen_config.steps_per_checkpoint\n            current_step += 1\n\n            # Once in a while, we save checkpoint, print statistics, and run evals.\n            if current_step % gen_config.steps_per_checkpoint == 0:\n\n                bucket_value = gen_loss_summary.value.add()\n                bucket_value.tag = gen_config.name_loss\n                bucket_value.simple_value = float(loss)\n                gen_writer.add_summary(gen_loss_summary, int(model.global_step.eval()))\n\n                # Print statistics for the previous epoch.\n                perplexity = math.exp(loss) if loss < 300 else float(\'inf\')\n                print (""global step %d learning rate %.4f step-time %.2f perplexity ""\n                       ""%.2f"" % (model.global_step.eval(), model.learning_rate.eval(),\n                                 step_time, perplexity))\n                # Decrease learning rate if no improvement was seen over last 3 times.\n                # if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n                #     sess.run(model.learning_rate_decay_op)\n                # previous_losses.append(loss)\n                # Save checkpoint and zero timer and loss.\n\n                if current_step % (gen_config.steps_per_checkpoint * 3) == 0:\n                    print(""current_step: %d, save model"" %(current_step))\n                    gen_ckpt_dir = os.path.abspath(os.path.join(gen_config.train_dir, ""checkpoints""))\n                    if not os.path.exists(gen_ckpt_dir):\n                        os.makedirs(gen_ckpt_dir)\n                    checkpoint_path = os.path.join(gen_ckpt_dir, ""chitchat.model"")\n                    model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n\n\n                step_time, loss = 0.0, 0.0\n                # Run evals on development set and print their perplexity.\n                # for bucket_id in xrange(len(gen_config.buckets)):\n                #   encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n                #       dev_set, bucket_id)\n                #   _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n                #                                target_weights, bucket_id, True)\n                #   eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float(\'inf\')\n                #   print(""  eval: bucket %d perplexity %.2f"" % (bucket_id, eval_ppx))\n                sys.stdout.flush()\n\n\ndef test_decoder(gen_config):\n    with tf.Session() as sess:\n        model = create_model(sess, gen_config, forward_only=True, name_scope=gen_config.name_model)\n        model.batch_size = 1\n\n        train_path = os.path.join(gen_config.train_dir, ""chitchat.train"")\n        voc_file_path = [train_path + "".answer"", train_path + "".query""]\n        vocab_path = os.path.join(gen_config.train_dir, ""vocab%d.all"" % gen_config.vocab_size)\n        data_utils.create_vocabulary(vocab_path, voc_file_path, gen_config.vocab_size)\n        vocab, rev_vocab = data_utils.initialize_vocabulary(vocab_path)\n\n        sys.stdout.write(""> "")\n        sys.stdout.flush()\n        sentence = sys.stdin.readline()\n        while sentence:\n            token_ids = data_utils.sentence_to_token_ids(tf.compat.as_bytes(sentence), vocab)\n            print(""token_id: "", token_ids)\n            bucket_id = len(gen_config.buckets) - 1\n            for i, bucket in enumerate(gen_config.buckets):\n                if bucket[0] >= len(token_ids):\n                    bucket_id = i\n                    break\n            else:\n                print(""Sentence truncated: %s"", sentence)\n\n            encoder_inputs, decoder_inputs, target_weights, _, _ = model.get_batch({bucket_id: [(token_ids, [1])]},\n                                                         bucket_id, model.batch_size, type=0)\n\n            print(""bucket_id: "", bucket_id)\n            print(""encoder_inputs:"", encoder_inputs)\n            print(""decoder_inputs:"", decoder_inputs)\n            print(""target_weights:"", target_weights)\n\n            _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n\n            print(""output_logits"", np.shape(output_logits))\n\n            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n\n            if data_utils.EOS_ID in outputs:\n                outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n\n            print("" "".join([tf.compat.as_str(rev_vocab[output]) for output in outputs]))\n            print(""> "", end="""")\n            sys.stdout.flush()\n            sentence = sys.stdin.readline()\n\n\ndef decoder(gen_config):\n    vocab, rev_vocab, dev_set, train_set = prepare_data(gen_config)\n\n    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(gen_config.buckets))]\n    train_total_size = float(sum(train_bucket_sizes))\n    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n                           for i in xrange(len(train_bucket_sizes))]\n\n    with tf.Session() as sess:\n        model = create_model(sess, gen_config, forward_only=True, name_scope=gen_config.name_model)\n\n        disc_train_query = open(""train.query"", ""w"")\n        disc_train_answer = open(""train.answer"", ""w"")\n        disc_train_gen = open(""train.gen"", ""w"")\n\n        num_step = 0\n        while num_step < 10000:\n            print(""generating num_step: "", num_step)\n            random_number_01 = np.random.random_sample()\n            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                             if train_buckets_scale[i] > random_number_01])\n\n            encoder_inputs, decoder_inputs, target_weights, batch_source_encoder, batch_source_decoder = \\\n                model.get_batch(train_set, bucket_id, gen_config.batch_size)\n\n            _, _, out_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id,\n                                          forward_only=True)\n\n            tokens = []\n            resps = []\n            for seq in out_logits:\n                token = []\n                for t in seq:\n                    token.append(int(np.argmax(t, axis=0)))\n                tokens.append(token)\n            tokens_t = []\n            for col in range(len(tokens[0])):\n                tokens_t.append([tokens[row][col] for row in range(len(tokens))])\n\n            for seq in tokens_t:\n                if data_utils.EOS_ID in seq:\n                    resps.append(seq[:seq.index(data_utils.EOS_ID)][:gen_config.buckets[bucket_id][1]])\n                else:\n                    resps.append(seq[:gen_config.buckets[bucket_id][1]])\n\n            for query, answer, resp in zip(batch_source_encoder, batch_source_decoder, resps):\n\n                answer_str = "" "".join([str(rev_vocab[an]) for an in answer][:-1])\n                disc_train_answer.write(answer_str)\n                disc_train_answer.write(""\\n"")\n\n                query_str = "" "".join([str(rev_vocab[qu]) for qu in query])\n                disc_train_query.write(query_str)\n                disc_train_query.write(""\\n"")\n\n                resp_str = "" "".join([tf.compat.as_str(rev_vocab[output]) for output in resp])\n\n                disc_train_gen.write(resp_str)\n                disc_train_gen.write(""\\n"")\n            num_step += 1\n\n        disc_train_gen.close()\n        disc_train_query.close()\n        disc_train_answer.close()\n    pass\n\n\n\n\ndef decoder_bk(gen_config):\n    vocab, rev_vocab, dev_set, train_set = prepare_data(gen_config)\n\n    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(gen_config.buckets))]\n    train_total_size = float(sum(train_bucket_sizes))\n    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n                           for i in xrange(len(train_bucket_sizes))]\n\n    with tf.Session() as sess:\n        model = create_model(sess, gen_config, forward_only=True, name_scope=gen_config.name_model)\n\n        disc_train_query = open(""train.query"", ""w"")\n        disc_train_answer = open(""train.answer"", ""w"")\n        disc_train_gen = open(""train.gen"", ""w"")\n\n        num_step = 30000\n        while num_step > 0:\n            random_number_01 = np.random.random_sample()\n            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                             if train_buckets_scale[i] > random_number_01])\n\n            encoder_inputs, decoder_inputs, target_weights, batch_source_encoder, batch_source_decoder = model.get_batch(\n                train_set, bucket_id, gen_config.batch_size)\n\n            _, _, out_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only=True)\n\n            tokens = []\n            resps = []\n            for seq in out_logits:\n                # print(""seq: %s"" %seq)\n                token = []\n                for t in seq:\n                    token.append(int(np.argmax(t, axis=0)))\n                tokens.append(token)\n            tokens_t = []\n            for col in range(len(tokens[0])):\n                tokens_t.append([tokens[row][col] for row in range(len(tokens))])\n\n            for seq in tokens_t:\n                if data_utils.EOS_ID in seq:\n                    resps.append(seq[:seq.index(data_utils.EOS_ID)][:gen_config.buckets[bucket_id][1]])\n                else:\n                    resps.append(seq[:gen_config.buckets[bucket_id][1]])\n\n            for query, answer, resp in zip(batch_source_encoder, batch_source_decoder, resps):\n                disc_train_answer.write(answer)\n                disc_train_answer.write(""\\n"")\n\n                disc_train_query.write(query)\n                disc_train_query.write(""\\n"")\n\n                disc_train_gen.write(resp)\n                disc_train_gen.write(""\\n"")\n            num_step -= 1\n\n        disc_train_gen.close()\n        disc_train_query.close()\n        disc_train_answer.close()\n    pass\n\n\ndef get_predicted_sentence(sess, input_token_ids, vocab, model,\n                           beam_size, buckets, mc_search=True,debug=False):\n    def model_step(enc_inp, dec_inp, dptr, target_weights, bucket_id):\n        #model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n        _, _, logits = model.step(sess, enc_inp, dec_inp, target_weights, bucket_id, True)\n        prob = softmax(logits[dptr][0])\n        # print(""model_step @ %s"" % (datetime.now()))\n        return prob\n\n    def greedy_dec(output_logits):\n        selected_token_ids = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n        return selected_token_ids\n\n    #input_token_ids = data_utils.sentence_to_token_ids(input_sentence, vocab)\n    # Which bucket does it belong to?\n    bucket_id = min([b for b in range(len(buckets)) if buckets[b][0] > len(input_token_ids)])\n    outputs = []\n    feed_data = {bucket_id: [(input_token_ids, outputs)]}\n\n    # Get a 1-element batch to feed the sentence to the model.   None,bucket_id, True\n    encoder_inputs, decoder_inputs, target_weights, _, _ = model.get_batch(feed_data, bucket_id, 1)\n    if debug: print(""\\n[get_batch]\\n"", encoder_inputs, decoder_inputs, target_weights)\n\n    ### Original greedy decoding\n    if beam_size == 1 or (not mc_search):\n        _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n        return [{""dec_inp"": greedy_dec(output_logits), \'prob\': 1}]\n\n    # Get output logits for the sentence. # initialize beams as (log_prob, empty_string, eos)\n    beams, new_beams, results = [(1, {\'eos\': 0, \'dec_inp\': decoder_inputs, \'prob\': 1, \'prob_ts\': 1, \'prob_t\': 1})], [], []\n\n    for dptr in range(len(decoder_inputs)-1):\n      if dptr > 0:\n        target_weights[dptr] = [1.]\n        beams, new_beams = new_beams[:beam_size], []\n      if debug: print(""=====[beams]====="", beams)\n      heapq.heapify(beams)  # since we will srot and remove something to keep N elements\n      for prob, cand in beams:\n        if cand[\'eos\']:\n          results += [(prob, cand)]\n          continue\n\n        all_prob_ts = model_step(encoder_inputs, cand[\'dec_inp\'], dptr, target_weights, bucket_id)\n        all_prob_t  = [0]*len(all_prob_ts)\n        all_prob    = all_prob_ts\n\n        # suppress copy-cat (respond the same as input)\n        if dptr < len(input_token_ids):\n          all_prob[input_token_ids[dptr]] = all_prob[input_token_ids[dptr]] * 0.01\n\n        # beam search\n        for c in np.argsort(all_prob)[::-1][:beam_size]:\n          new_cand = {\n            \'eos\'     : (c == data_utils.EOS_ID),\n            \'dec_inp\' : [(np.array([c]) if i == (dptr+1) else k) for i, k in enumerate(cand[\'dec_inp\'])],\n            \'prob_ts\' : cand[\'prob_ts\'] * all_prob_ts[c],\n            \'prob_t\'  : cand[\'prob_t\'] * all_prob_t[c],\n            \'prob\'    : cand[\'prob\'] * all_prob[c],\n          }\n          new_cand = (new_cand[\'prob\'], new_cand) # for heapq can only sort according to list[0]\n\n          if (len(new_beams) < beam_size):\n            heapq.heappush(new_beams, new_cand)\n          elif (new_cand[0] > new_beams[0][0]):\n            heapq.heapreplace(new_beams, new_cand)\n\n    results += new_beams  # flush last cands\n\n    # post-process results\n    res_cands = []\n    for prob, cand in sorted(results, reverse=True):\n      res_cands.append(cand)\n    return res_cands\n\n\ndef gen_sample(sess ,gen_config, model, vocab, source_inputs, source_outputs, mc_search=True):\n    sample_inputs = []\n    sample_labels =[]\n    rep = []\n\n    for source_query, source_answer in zip(source_inputs, source_outputs):\n        sample_inputs.append(source_query+source_answer)\n        sample_labels.append(1)\n        responses = get_predicted_sentence(sess, source_query, vocab,\n                                           model, gen_config.beam_size, gen_config.buckets, mc_search)\n\n        for resp in responses:\n            if gen_config.beam_size == 1 or (not mc_search):\n                dec_inp = [dec for dec in resp[\'dec_inp\']]\n                rep.append(dec_inp)\n                dec_inp = dec_inp[:]\n            else:\n                dec_inp = [dec.tolist()[0] for dec in resp[\'dec_inp\'][:]]\n                rep.append(dec_inp)\n                dec_inp = dec_inp[1:]\n            print(""  (%s) -> %s"" % (resp[\'prob\'], dec_inp))\n            sample_neg = source_query + dec_inp\n            sample_inputs.append(sample_neg)\n            sample_labels.append(0)\n\n    return sample_inputs, sample_labels, rep\n    pass\n'"
gen/seq2seq.py,19,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# We disable pylint because we need python3 compatibility.\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nfrom six.moves import zip     # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom tensorflow.python import shape\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.util import nest\n\n# TODO(ebrevdo): Remove once _linear is fully deprecated.\nlinear = rnn_cell._linear  # pylint: disable=protected-access\n\n\ndef _argmax_or_mcsearch(embedding, output_projection=None, update_embedding=True, mc_search=False):\n    def loop_function(prev, _):\n        if output_projection is not None:\n            prev = nn_ops.xw_plus_b(prev, output_projection[0], output_projection[1])\n\n\n        if isinstance(mc_search, bool):\n            prev_symbol = tf.reshape(tf.multinomial(prev, 1), [-1]) if mc_search else math_ops.argmax(prev, 1)\n        else:\n            prev_symbol = tf.cond(mc_search, lambda: tf.reshape(tf.multinomial(prev, 1), [-1]), lambda: tf.argmax(prev, 1))\n\n\n        emb_prev = embedding_ops.embedding_lookup(embedding, prev_symbol)\n        if not update_embedding:\n            emb_prev = array_ops.stop_gradient(emb_prev)\n        return emb_prev\n    return loop_function\n\ndef _extract_argmax_and_embed(embedding, output_projection=None, update_embedding=True):\n  """"""Get a loop_function that extracts the previous symbol and embeds it.\n\n  Args:\n    embedding: embedding tensor for symbols.\n    output_projection: None or a pair (W, B). If provided, each fed previous\n      output will first be multiplied by W and added B.\n    update_embedding: Boolean; if False, the gradients will not propagate\n      through the embeddings.\n\n  Returns:\n    A loop function.\n  """"""\n  def loop_function(prev, _):\n    if output_projection is not None:\n      prev = nn_ops.xw_plus_b(\n          prev, output_projection[0], output_projection[1])\n    prev_symbol = math_ops.argmax(prev, 1)\n    # Note that gradients will not propagate through the second parameter of\n    # embedding_lookup.\n    emb_prev = embedding_ops.embedding_lookup(embedding, prev_symbol)\n    if not update_embedding:\n      emb_prev = array_ops.stop_gradient(emb_prev)\n    return emb_prev\n  return loop_function\n\n\ndef rnn_decoder(decoder_inputs, initial_state, cell, loop_function=None,\n                scope=None):\n  """"""RNN decoder for the sequence-to-sequence model.\n\n  Args:\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    initial_state: 2D Tensor with shape [batch_size x cell.state_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    loop_function: If not None, this function will be applied to the i-th output\n      in order to generate the i+1-st input, and decoder_inputs will be ignored,\n      except for the first element (""GO"" symbol). This can be used for decoding,\n      but also for training to emulate http://arxiv.org/abs/1506.03099.\n      Signature -- loop_function(prev, i) = next\n        * prev is a 2D Tensor of shape [batch_size x output_size],\n        * i is an integer, the step number (when advanced control is needed),\n        * next is a 2D Tensor of shape [batch_size x input_size].\n    scope: VariableScope for the created subgraph; defaults to ""rnn_decoder"".\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing generated outputs.\n      state: The state of each cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n        (Note that in some cases, like basic RNN cell or GRU cell, outputs and\n         states can be the same. They are different for LSTM cells though.)\n  """"""\n  with variable_scope.variable_scope(scope or ""rnn_decoder""):\n    state = initial_state\n    outputs = []\n    prev = None\n    for i, inp in enumerate(decoder_inputs):\n      if loop_function is not None and prev is not None:\n        with variable_scope.variable_scope(""loop_function"", reuse=True):\n          inp = loop_function(prev, i)\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n      output, state = cell(inp, state)\n      outputs.append(output)\n      if loop_function is not None:\n        prev = output\n  return outputs, state\n\n\ndef basic_rnn_seq2seq(\n    encoder_inputs, decoder_inputs, cell, dtype=dtypes.float32, scope=None):\n  """"""Basic RNN sequence-to-sequence model.\n\n  This model first runs an RNN to encode encoder_inputs into a state vector,\n  then runs decoder, initialized with the last encoder state, on decoder_inputs.\n  Encoder and decoder use the same RNN cell type, but don\'t share parameters.\n\n  Args:\n    encoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    dtype: The dtype of the initial state of the RNN cell (default: tf.float32).\n    scope: VariableScope for the created subgraph; default: ""basic_rnn_seq2seq"".\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing the generated outputs.\n      state: The state of each decoder cell in the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  """"""\n  with variable_scope.variable_scope(scope or ""basic_rnn_seq2seq""):\n    _, enc_state = rnn.rnn(cell, encoder_inputs, dtype=dtype)\n    return rnn_decoder(decoder_inputs, enc_state, cell)\n\n\ndef tied_rnn_seq2seq(encoder_inputs, decoder_inputs, cell,\n                     loop_function=None, dtype=dtypes.float32, scope=None):\n  """"""RNN sequence-to-sequence model with tied encoder and decoder parameters.\n\n  This model first runs an RNN to encode encoder_inputs into a state vector, and\n  then runs decoder, initialized with the last encoder state, on decoder_inputs.\n  Encoder and decoder use the same RNN cell and share parameters.\n\n  Args:\n    encoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    loop_function: If not None, this function will be applied to i-th output\n      in order to generate i+1-th input, and decoder_inputs will be ignored,\n      except for the first element (""GO"" symbol), see rnn_decoder for details.\n    dtype: The dtype of the initial state of the rnn cell (default: tf.float32).\n    scope: VariableScope for the created subgraph; default: ""tied_rnn_seq2seq"".\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing the generated outputs.\n      state: The state of each decoder cell in each time-step. This is a list\n        with length len(decoder_inputs) -- one item for each time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  """"""\n  with variable_scope.variable_scope(""combined_tied_rnn_seq2seq""):\n    scope = scope or ""tied_rnn_seq2seq""\n    _, enc_state = rnn.rnn(\n        cell, encoder_inputs, dtype=dtype, scope=scope)\n    variable_scope.get_variable_scope().reuse_variables()\n    return rnn_decoder(decoder_inputs, enc_state, cell,\n                       loop_function=loop_function, scope=scope)\n\n\ndef embedding_rnn_decoder(decoder_inputs,\n                          initial_state,\n                          cell,\n                          num_symbols,\n                          embedding_size,\n                          output_projection=None,\n                          feed_previous=False,\n                          update_embedding_for_previous=True,\n                          scope=None):\n\n  with variable_scope.variable_scope(scope or ""embedding_rnn_decoder"") as scope:\n    if output_projection is not None:\n      dtype = scope.dtype\n      proj_weights = ops.convert_to_tensor(output_projection[0], dtype=dtype)\n      proj_weights.get_shape().assert_is_compatible_with([None, num_symbols])\n      proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n      proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n    embedding = variable_scope.get_variable(""embedding"",\n                                            [num_symbols, embedding_size])\n    loop_function = _extract_argmax_and_embed(\n        embedding, output_projection,\n        update_embedding_for_previous) if feed_previous else None\n    emb_inp = (\n        embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs)\n    return rnn_decoder(emb_inp, initial_state, cell,\n                       loop_function=loop_function)\n\n\ndef embedding_rnn_seq2seq(encoder_inputs,\n                          decoder_inputs,\n                          cell,\n                          num_encoder_symbols,\n                          num_decoder_symbols,\n                          embedding_size,\n                          output_projection=None,\n                          feed_previous=False,\n                          dtype=None,\n                          scope=None):\n\n  with variable_scope.variable_scope(scope or ""embedding_rnn_seq2seq"") as scope:\n    if dtype is not None:\n      scope.set_dtype(dtype)\n    else:\n      dtype = scope.dtype\n\n    # Encoder.\n    encoder_cell = rnn_cell.EmbeddingWrapper(\n        cell, embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n\n    # Decoder.\n    if output_projection is None:\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n\n    if isinstance(feed_previous, bool):\n      return embedding_rnn_decoder(\n          decoder_inputs,\n          encoder_state,\n          cell,\n          num_decoder_symbols,\n          embedding_size,\n          output_projection=output_projection,\n          feed_previous=feed_previous,\n          scope=scope)\n\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n    def decoder(feed_previous_bool):\n      reuse = None if feed_previous_bool else True\n      with variable_scope.variable_scope(\n          variable_scope.get_variable_scope(), reuse=reuse) as scope:\n        outputs, state = embedding_rnn_decoder(\n            decoder_inputs, encoder_state, cell, num_decoder_symbols,\n            embedding_size, output_projection=output_projection,\n            feed_previous=feed_previous_bool,\n            update_embedding_for_previous=False)\n        state_list = [state]\n        if nest.is_sequence(state):\n          state_list = nest.flatten(state)\n        return outputs + state_list\n\n    outputs_and_state = control_flow_ops.cond(feed_previous,\n                                              lambda: decoder(True),\n                                              lambda: decoder(False))\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n    state_list = outputs_and_state[outputs_len:]\n    state = state_list[0]\n    if nest.is_sequence(encoder_state):\n      state = nest.pack_sequence_as(structure=encoder_state,\n                                    flat_sequence=state_list)\n    return outputs_and_state[:outputs_len], state\n\n\ndef embedding_tied_rnn_seq2seq(encoder_inputs,\n                               decoder_inputs,\n                               cell,\n                               num_symbols,\n                               embedding_size,\n                               num_decoder_symbols=None,\n                               output_projection=None,\n                               feed_previous=False,\n                               dtype=None,\n                               scope=None):\n  """"""Embedding RNN sequence-to-sequence model with tied (shared) parameters.\n\n  This model first embeds encoder_inputs by a newly created embedding (of shape\n  [num_symbols x input_size]). Then it runs an RNN to encode embedded\n  encoder_inputs into a state vector. Next, it embeds decoder_inputs using\n  the same embedding. Then it runs RNN decoder, initialized with the last\n  encoder state, on embedded decoder_inputs. The decoder output is over symbols\n  from 0 to num_decoder_symbols - 1 if num_decoder_symbols is none; otherwise it\n  is over 0 to num_symbols - 1.\n\n  Args:\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    decoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    num_symbols: Integer; number of symbols for both encoder and decoder.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    num_decoder_symbols: Integer; number of output symbols for decoder. If\n      provided, the decoder output is over symbols 0 to num_decoder_symbols - 1.\n      Otherwise, decoder output is over symbols 0 to num_symbols - 1. Note that\n      this assumes that the vocabulary is set up such that the first\n      num_decoder_symbols of num_symbols are part of decoding.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_symbols] and B has\n      shape [num_symbols]; if provided and feed_previous=True, each\n      fed previous output will first be multiplied by W and added B.\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first\n      of decoder_inputs will be used (the ""GO"" symbol), and all other decoder\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\n      If False, decoder_inputs are used as given (the standard decoder case).\n    dtype: The dtype to use for the initial RNN states (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""embedding_tied_rnn_seq2seq"".\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_symbols] containing the generated\n        outputs where output_symbols = num_decoder_symbols if\n        num_decoder_symbols is not None otherwise output_symbols = num_symbols.\n      state: The state of each decoder cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n\n  Raises:\n    ValueError: When output_projection has the wrong shape.\n  """"""\n  with variable_scope.variable_scope(\n      scope or ""embedding_tied_rnn_seq2seq"", dtype=dtype) as scope:\n    dtype = scope.dtype\n\n    if output_projection is not None:\n      proj_weights = ops.convert_to_tensor(output_projection[0], dtype=dtype)\n      proj_weights.get_shape().assert_is_compatible_with([None, num_symbols])\n      proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n      proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n    embedding = variable_scope.get_variable(\n        ""embedding"", [num_symbols, embedding_size], dtype=dtype)\n\n    emb_encoder_inputs = [embedding_ops.embedding_lookup(embedding, x)\n                          for x in encoder_inputs]\n    emb_decoder_inputs = [embedding_ops.embedding_lookup(embedding, x)\n                          for x in decoder_inputs]\n\n    output_symbols = num_symbols\n    if num_decoder_symbols is not None:\n      output_symbols = num_decoder_symbols\n    if output_projection is None:\n      cell = rnn_cell.OutputProjectionWrapper(cell, output_symbols)\n\n    if isinstance(feed_previous, bool):\n      loop_function = _extract_argmax_and_embed(\n          embedding, output_projection, True) if feed_previous else None\n      return tied_rnn_seq2seq(emb_encoder_inputs, emb_decoder_inputs, cell,\n                              loop_function=loop_function, dtype=dtype)\n\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n    def decoder(feed_previous_bool):\n      loop_function = _extract_argmax_and_embed(\n        embedding, output_projection, False) if feed_previous_bool else None\n      reuse = None if feed_previous_bool else True\n      with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                         reuse=reuse):\n        outputs, state = tied_rnn_seq2seq(\n            emb_encoder_inputs, emb_decoder_inputs, cell,\n            loop_function=loop_function, dtype=dtype)\n        state_list = [state]\n        if nest.is_sequence(state):\n          state_list = nest.flatten(state)\n        return outputs + state_list\n\n    outputs_and_state = control_flow_ops.cond(feed_previous,\n                                              lambda: decoder(True),\n                                              lambda: decoder(False))\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n    state_list = outputs_and_state[outputs_len:]\n    state = state_list[0]\n    # Calculate zero-state to know it\'s structure.\n    static_batch_size = encoder_inputs[0].get_shape()[0]\n    for inp in encoder_inputs[1:]:\n      static_batch_size.merge_with(inp.get_shape()[0])\n    batch_size = static_batch_size.value\n    if batch_size is None:\n      batch_size = array_ops.shape(encoder_inputs[0])[0]\n    zero_state = cell.zero_state(batch_size, dtype)\n    if nest.is_sequence(zero_state):\n      state = nest.pack_sequence_as(structure=zero_state,\n                                    flat_sequence=state_list)\n    return outputs_and_state[:outputs_len], state\n\n\ndef attention_decoder(decoder_inputs,\n                      initial_state,\n                      attention_states,\n                      cell,\n                      output_size=None,\n                      num_heads=1,\n                      loop_function=None,\n                      dtype=None,\n                      scope=None,\n                      initial_state_attention=False):\n  """"""RNN decoder with attention for the sequence-to-sequence model.\n\n  In this context ""attention"" means that, during decoding, the RNN can look up\n  information in the additional tensor attention_states, and it does this by\n  focusing on a few entries from the tensor. This model has proven to yield\n  especially good results in a number of sequence-to-sequence tasks. This\n  implementation is based on http://arxiv.org/abs/1412.7449 (see below for\n  details). It is recommended for complex sequence-to-sequence tasks.\n\n  Args:\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    attention_states: 3D Tensor [batch_size x attn_length x attn_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    output_size: Size of the output vectors; if None, we use cell.output_size.\n    num_heads: Number of attention heads that read from attention_states.\n    loop_function: If not None, this function will be applied to i-th output\n      in order to generate i+1-th input, and decoder_inputs will be ignored,\n      except for the first element (""GO"" symbol). This can be used for decoding,\n      but also for training to emulate http://arxiv.org/abs/1506.03099.\n      Signature -- loop_function(prev, i) = next\n        * prev is a 2D Tensor of shape [batch_size x output_size],\n        * i is an integer, the step number (when advanced control is needed),\n        * next is a 2D Tensor of shape [batch_size x input_size].\n    dtype: The dtype to use for the RNN initial state (default: tf.float32).\n    scope: VariableScope for the created subgraph; default: ""attention_decoder"".\n    initial_state_attention: If False (default), initial attentions are zero.\n      If True, initialize the attentions from the initial state and attention\n      states -- useful when we wish to resume decoding from a previously\n      stored decoder state and attention states.\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors of\n        shape [batch_size x output_size]. These represent the generated outputs.\n        Output i is computed from input i (which is either the i-th element\n        of decoder_inputs or loop_function(output {i-1}, i)) as follows.\n        First, we run the cell on a combination of the input and previous\n        attention masks:\n          cell_output, new_state = cell(linear(input, prev_attn), prev_state).\n        Then, we calculate new attention masks:\n          new_attn = softmax(V^T * tanh(W * attention_states + U * new_state))\n        and then we calculate the output:\n          output = linear(cell_output, new_attn).\n      state: The state of each decoder cell the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n\n  Raises:\n    ValueError: when num_heads is not positive, there are no inputs, shapes\n      of attention_states are not set, or input size cannot be inferred\n      from the input.\n  """"""\n  if not decoder_inputs:\n    raise ValueError(""Must provide at least 1 input to attention decoder."")\n  if num_heads < 1:\n    raise ValueError(""With less than 1 heads, use a non-attention decoder."")\n  if attention_states.get_shape()[2].value is None:\n    raise ValueError(""Shape[2] of attention_states must be known: %s""\n                     % attention_states.get_shape())\n  if output_size is None:\n    output_size = cell.output_size\n\n  with variable_scope.variable_scope(\n      scope or ""attention_decoder"", dtype=dtype) as scope:\n    dtype = scope.dtype\n\n    batch_size = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n    attn_length = attention_states.get_shape()[1].value\n    if attn_length is None:\n      attn_length = shape(attention_states)[1]\n    attn_size = attention_states.get_shape()[2].value\n\n    # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n    hidden = array_ops.reshape(\n        attention_states, [-1, attn_length, 1, attn_size])\n    hidden_features = []\n    v = []\n    attention_vec_size = attn_size  # Size of query vectors for attention.\n    for a in xrange(num_heads):\n      k = variable_scope.get_variable(""AttnW_%d"" % a,\n                                      [1, 1, attn_size, attention_vec_size])\n      hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], ""SAME""))\n      v.append(\n          variable_scope.get_variable(""AttnV_%d"" % a, [attention_vec_size]))\n\n    state = initial_state\n\n    def attention(query):\n      """"""Put attention masks on hidden using hidden_features and query.""""""\n      ds = []  # Results of attention reads will be stored here.\n      if nest.is_sequence(query):  # If the query is a tuple, flatten it.\n        query_list = nest.flatten(query)\n        for q in query_list:  # Check that ndims == 2 if specified.\n          ndims = q.get_shape().ndims\n          if ndims:\n            assert ndims == 2\n        query = array_ops.concat(1, query_list)\n      for a in xrange(num_heads):\n        with variable_scope.variable_scope(""Attention_%d"" % a):\n          y = linear(query, attention_vec_size, True)\n          y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n          # Attention mask is a softmax of v^T * tanh(...).\n          s = math_ops.reduce_sum(\n              v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n          a = nn_ops.softmax(s)\n          # Now calculate the attention-weighted vector d.\n          d = math_ops.reduce_sum(\n              array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n              [1, 2])\n          ds.append(array_ops.reshape(d, [-1, attn_size]))\n      return ds\n\n    outputs = []\n    prev = None\n    batch_attn_size = array_ops.pack([batch_size, attn_size])\n    attns = [array_ops.zeros(batch_attn_size, dtype=dtype)\n             for _ in xrange(num_heads)]\n    for a in attns:  # Ensure the second shape of attention vectors is set.\n      a.set_shape([None, attn_size])\n    if initial_state_attention:\n      attns = attention(initial_state)\n    for i, inp in enumerate(decoder_inputs):\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n      # If loop_function is set, we use it instead of decoder_inputs.\n      if loop_function is not None and prev is not None:\n        with variable_scope.variable_scope(""loop_function"", reuse=True):\n          inp = loop_function(prev, i)\n      # Merge input and previous attentions into one vector of the right size.\n      input_size = inp.get_shape().with_rank(2)[1]\n      if input_size.value is None:\n        raise ValueError(""Could not infer input size from input: %s"" % inp.name)\n      x = linear([inp] + attns, input_size, True)\n      # Run the RNN.\n      cell_output, state = cell(x, state)\n      # Run the attention mechanism.\n      if i == 0 and initial_state_attention:\n        with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                           reuse=True):\n          attns = attention(state)\n      else:\n        attns = attention(state)\n\n      with variable_scope.variable_scope(""AttnOutputProjection""):\n        output = linear([cell_output] + attns, output_size, True)\n      if loop_function is not None:\n        prev = output\n      outputs.append(output)\n\n  return outputs, state\n\n\ndef embedding_attention_decoder(decoder_inputs,\n                                initial_state,\n                                attention_states,\n                                cell,\n                                num_symbols,\n                                embedding_size,\n                                num_heads=1,\n                                output_size=None,\n                                output_projection=None,\n                                feed_previous=False,\n                                update_embedding_for_previous=True,\n                                dtype=None,\n                                scope=None,\n                                initial_state_attention=False,\n                                mc_search = False):\n  """"""RNN decoder with embedding and attention and a pure-decoding option.\n\n  Args:\n    decoder_inputs: A list of 1D batch-sized int32 Tensors (decoder inputs).\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    attention_states: 3D Tensor [batch_size x attn_length x attn_size].\n    cell: rnn_cell.RNNCell defining the cell function.\n    num_symbols: Integer, how many symbols come into the embedding.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    num_heads: Number of attention heads that read from attention_states.\n    output_size: Size of the output vectors; if None, use output_size.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_symbols] and B has shape\n      [num_symbols]; if provided and feed_previous=True, each fed previous\n      output will first be multiplied by W and added B.\n    feed_previous: Boolean; if True, only the first of decoder_inputs will be\n      used (the ""GO"" symbol), and all other decoder inputs will be generated by:\n        next = embedding_lookup(embedding, argmax(previous_output)),\n      In effect, this implements a greedy decoder. It can also be used\n      during training to emulate http://arxiv.org/abs/1506.03099.\n      If False, decoder_inputs are used as given (the standard decoder case).\n    update_embedding_for_previous: Boolean; if False and feed_previous=True,\n      only the embedding for the first symbol of decoder_inputs (the ""GO""\n      symbol) will be updated by back propagation. Embeddings for the symbols\n      generated from the decoder itself remain unchanged. This parameter has\n      no effect if feed_previous=False.\n    dtype: The dtype to use for the RNN initial states (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""embedding_attention_decoder"".\n    initial_state_attention: If False (default), initial attentions are zero.\n      If True, initialize the attentions from the initial state and attention\n      states -- useful when we wish to resume decoding from a previously\n      stored decoder state and attention states.\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing the generated outputs.\n      state: The state of each decoder cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n\n  Raises:\n    ValueError: When output_projection has the wrong shape.\n  """"""\n  if output_size is None:\n    output_size = cell.output_size\n  if output_projection is not None:\n    proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n    proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n  with variable_scope.variable_scope(\n      scope or ""embedding_attention_decoder"", dtype=dtype) as scope:\n\n    embedding = variable_scope.get_variable(""embedding"",\n                                            [num_symbols, embedding_size])\n\n    loop_function = None\n    if feed_previous == True:\n        loop_function = _argmax_or_mcsearch(embedding, output_projection, update_embedding_for_previous, mc_search)\n    # if isinstance(mc_search, bool):\n    #     if feed_previous == True and mc_search == True:\n    #         loop_function = _mc_argmax_and_embed(embedding, output_projection, update_embedding_for_previous)\n    #     elif feed_previous == True and mc_search == False:\n    #         loop_function = _extract_argmax_and_embed(embedding, output_projection, update_embedding_for_previous)\n    # elif (feed_previous == True):\n    #     loop_function = control_flow_ops.cond(mc_search,\n    #                           _mc_argmax_and_embed(embedding, output_projection, update_embedding_for_previous),\n    #                           _extract_argmax_and_embed(embedding, output_projection, update_embedding_for_previous))\n\n    emb_inp = [\n        embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs]\n    return attention_decoder(\n        emb_inp,\n        initial_state,\n        attention_states,\n        cell,\n        output_size=output_size,\n        num_heads=num_heads,\n        loop_function=loop_function,\n        initial_state_attention=initial_state_attention,\n        scope=scope)\n\n\ndef embedding_attention_seq2seq(encoder_inputs,\n                                decoder_inputs,\n                                cell,\n                                num_encoder_symbols,\n                                num_decoder_symbols,\n                                embedding_size,\n                                num_heads=1,\n                                output_projection=None,\n                                feed_previous=False,\n                                dtype=None,\n                                scope=None,\n                                initial_state_attention=False,\n                                mc_search=False):\n\n  with variable_scope.variable_scope(\n      scope or ""embedding_attention_seq2seq"", dtype=dtype) as scope:\n    dtype = scope.dtype\n    # Encoder.\n    encoder_cell = rnn_cell.EmbeddingWrapper(\n        cell, embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    encoder_outputs, encoder_state = rnn.rnn(\n        encoder_cell, encoder_inputs, dtype=dtype)\n\n    # First calculate a concatenation of encoder outputs to put attention on.\n    top_states = [array_ops.reshape(e, [-1, 1, cell.output_size])\n                  for e in encoder_outputs]\n    attention_states = array_ops.concat(1, top_states)\n\n    # Decoder.\n    output_size = None\n    if output_projection is None:\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n      output_size = num_decoder_symbols\n\n    if isinstance(feed_previous, bool):\n      outputs, state = embedding_attention_decoder(\n          decoder_inputs,\n          encoder_state,\n          attention_states,\n          cell,\n          num_decoder_symbols,\n          embedding_size,\n          num_heads=num_heads,\n          output_size=output_size,\n          output_projection=output_projection,\n          feed_previous=feed_previous,\n          initial_state_attention=initial_state_attention,\n          mc_search=mc_search,\n          scope=scope)\n      return outputs, state, encoder_state\n\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n    def decoder(feed_previous_bool):\n      reuse = None if feed_previous_bool else True\n      with variable_scope.variable_scope(\n          variable_scope.get_variable_scope(), reuse=reuse) as scope:\n        outputs, state = embedding_attention_decoder(\n            decoder_inputs,\n            encoder_state,\n            attention_states,\n            cell,\n            num_decoder_symbols,\n            embedding_size,\n            num_heads=num_heads,\n            output_size=output_size,\n            output_projection=output_projection,\n            feed_previous=feed_previous_bool,\n            update_embedding_for_previous=False,\n            initial_state_attention=initial_state_attention,\n            mc_search=mc_search,\n            scope=scope)\n        state_list = [state]\n        if nest.is_sequence(state):\n          state_list = nest.flatten(state)\n        return outputs + state_list\n\n    outputs_and_state = control_flow_ops.cond(feed_previous,\n                                              lambda: decoder(True),\n                                              lambda: decoder(False))\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n    state_list = outputs_and_state[outputs_len:]\n    state = state_list[0]\n    if nest.is_sequence(encoder_state):\n      state = nest.pack_sequence_as(structure=encoder_state,\n                                    flat_sequence=state_list)\n    return outputs_and_state[:outputs_len], state, encoder_state\n\n\ndef one2many_rnn_seq2seq(encoder_inputs,\n                         decoder_inputs_dict,\n                         cell,\n                         num_encoder_symbols,\n                         num_decoder_symbols_dict,\n                         embedding_size,\n                         feed_previous=False,\n                         dtype=None,\n                         scope=None):\n  """"""One-to-many RNN sequence-to-sequence model (multi-task).\n\n  This is a multi-task sequence-to-sequence model with one encoder and multiple\n  decoders. Reference to multi-task sequence-to-sequence learning can be found\n  here: http://arxiv.org/abs/1511.06114\n\n  Args:\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    decoder_inputs_dict: A dictionany mapping decoder name (string) to\n      the corresponding decoder_inputs; each decoder_inputs is a list of 1D\n      Tensors of shape [batch_size]; num_decoders is defined as\n      len(decoder_inputs_dict).\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    num_encoder_symbols: Integer; number of symbols on the encoder side.\n    num_decoder_symbols_dict: A dictionary mapping decoder name (string) to an\n      integer specifying number of symbols for the corresponding decoder;\n      len(num_decoder_symbols_dict) must be equal to num_decoders.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first of\n      decoder_inputs will be used (the ""GO"" symbol), and all other decoder\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\n      If False, decoder_inputs are used as given (the standard decoder case).\n    dtype: The dtype of the initial state for both the encoder and encoder\n      rnn cells (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""one2many_rnn_seq2seq""\n\n  Returns:\n    A tuple of the form (outputs_dict, state_dict), where:\n      outputs_dict: A mapping from decoder name (string) to a list of the same\n        length as decoder_inputs_dict[name]; each element in the list is a 2D\n        Tensors with shape [batch_size x num_decoder_symbol_list[name]]\n        containing the generated outputs.\n      state_dict: A mapping from decoder name (string) to the final state of the\n        corresponding decoder RNN; it is a 2D Tensor of shape\n        [batch_size x cell.state_size].\n  """"""\n  outputs_dict = {}\n  state_dict = {}\n\n  with variable_scope.variable_scope(\n      scope or ""one2many_rnn_seq2seq"", dtype=dtype) as scope:\n    dtype = scope.dtype\n\n    # Encoder.\n    encoder_cell = rnn_cell.EmbeddingWrapper(\n        cell, embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n\n    # Decoder.\n    for name, decoder_inputs in decoder_inputs_dict.items():\n      num_decoder_symbols = num_decoder_symbols_dict[name]\n\n      with variable_scope.variable_scope(""one2many_decoder_"" + str(\n          name)) as scope:\n        decoder_cell = rnn_cell.OutputProjectionWrapper(cell,\n                                                        num_decoder_symbols)\n        if isinstance(feed_previous, bool):\n          outputs, state = embedding_rnn_decoder(\n              decoder_inputs, encoder_state, decoder_cell, num_decoder_symbols,\n              embedding_size, feed_previous=feed_previous)\n        else:\n          # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n          def filled_embedding_rnn_decoder(feed_previous):\n            """"""The current decoder with a fixed feed_previous parameter.""""""\n            # pylint: disable=cell-var-from-loop\n            reuse = None if feed_previous else True\n            vs = variable_scope.get_variable_scope()\n            with variable_scope.variable_scope(vs, reuse=reuse):\n              outputs, state = embedding_rnn_decoder(\n                  decoder_inputs, encoder_state, decoder_cell,\n                  num_decoder_symbols, embedding_size,\n                  feed_previous=feed_previous)\n            # pylint: enable=cell-var-from-loop\n            state_list = [state]\n            if nest.is_sequence(state):\n              state_list = nest.flatten(state)\n            return outputs + state_list\n\n          outputs_and_state = control_flow_ops.cond(\n              feed_previous,\n              lambda: filled_embedding_rnn_decoder(True),\n              lambda: filled_embedding_rnn_decoder(False))\n          # Outputs length is the same as for decoder inputs.\n          outputs_len = len(decoder_inputs)\n          outputs = outputs_and_state[:outputs_len]\n          state_list = outputs_and_state[outputs_len:]\n          state = state_list[0]\n          if nest.is_sequence(encoder_state):\n            state = nest.pack_sequence_as(structure=encoder_state,\n                                          flat_sequence=state_list)\n      outputs_dict[name] = outputs\n      state_dict[name] = state\n\n  return outputs_dict, state_dict\n\n\n# def sequence_loss_by_mle(logits, targets, emb_dim, sequence_length, batch_size, name=None):\n#     pass\n#     if len(targets) != len(logits) or len(weights) != len(logits):\n#         raise ValueError(""Lengths of logits, weights, and targets must be the same ""\n#                          ""%d, %d, %d."" % (len(logits), len(weights), len(targets)))\n#     with ops.name_scope(name, ""sequence_loss_by_mle"",\n#                         logits + targets + weights):\n#\n#         pretrain_loss = -tf.reduce_sum(\n#             tf.one_hot(tf.to_int32(tf.reshape(targets, [-1])), emb_dim, 1.0, 0.0) * tf.log(\n#                 tf.clip_by_value(tf.reshape(logits, [-1, emb_dim]), 1e-20, 1.0)\n#             )\n#         ) / (sequence_length * batch_size)\n#\n#\n#\n#         log_perp_list = []\n#         for logit, target, weight in zip(logits, targets, weights):\n#             pass\n\n# def sequence_loss_by_example(logits, targets, weights,\n#                              average_across_timesteps=True,\n#                              softmax_loss_function=None,up_reward=None, policy_gradient=None, name=None):\n#   if len(targets) != len(logits) or len(weights) != len(logits):\n#     raise ValueError(""Lengths of logits, weights, and targets must be the same ""\n#                      ""%d, %d, %d."" % (len(logits), len(weights), len(targets)))\n#   with ops.name_scope(name, ""sequence_loss_by_example"",\n#                       logits + targets + weights):\n#     log_perp_list = []\n#     for logit, target, weight in zip(logits, targets, weights):\n#       if softmax_loss_function is None:\n#         # TODO(irving,ebrevdo): This reshape is needed because\n#         # sequence_loss_by_example is called with scalars sometimes, which\n#         # violates our general scalar strictness policy.\n#         target = array_ops.reshape(target, [-1])\n#         crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(\n#             logit, target)\n#       else:\n#         #crossent = softmax_loss_function(logit, target)\n#         crossent = tf.cond(up_reward,\n#                            lambda :policy_gradient(logit, target),\n#                            lambda :softmax_loss_function(logit,target))\n#       log_perp_list.append(crossent * weight)\n#     log_perps = math_ops.add_n(log_perp_list)\n#     if average_across_timesteps:\n#       total_size = math_ops.add_n(weights)\n#       total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n#       log_perps /= total_size\n#   return log_perps\n\ndef sequence_loss_by_example(logits, targets, weights,\n                             average_across_timesteps=True,\n                             softmax_loss_function=None, name=None):\n  if len(targets) != len(logits) or len(weights) != len(logits):\n    raise ValueError(""Lengths of logits, weights, and targets must be the same ""\n                     ""%d, %d, %d."" % (len(logits), len(weights), len(targets)))\n  with ops.name_scope(name, ""sequence_loss_by_example"",\n                      logits + targets + weights):\n    log_perp_list = []\n    for logit, target, weight in zip(logits, targets, weights):\n      if softmax_loss_function is None:\n        # TODO(irving,ebrevdo): This reshape is needed because\n        # sequence_loss_by_example is called with scalars sometimes, which\n        # violates our general scalar strictness policy.\n        target = array_ops.reshape(target, [-1])\n        crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(\n            logit, target)\n      else:\n        crossent = softmax_loss_function(logit, target)\n      log_perp_list.append(crossent * weight)\n    log_perps = math_ops.add_n(log_perp_list)\n    if average_across_timesteps:\n      total_size = math_ops.add_n(weights)\n      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n      log_perps /= total_size\n  return log_perps\n\n\ndef sequence_loss(logits, targets, weights,\n                  average_across_timesteps=True, average_across_batch=True,\n                  softmax_loss_function=None, name=None):\n  """"""Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\n\n  Args:\n    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    average_across_batch: If set, divide the returned cost by the batch size.\n    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n    name: Optional name for this operation, defaults to ""sequence_loss"".\n\n  Returns:\n    A scalar float Tensor: The average log-perplexity per symbol (weighted).\n\n  Raises:\n    ValueError: If len(logits) is different from len(targets) or len(weights).\n  """"""\n  with ops.name_scope(name, ""sequence_loss"", logits + targets + weights):\n    cost = math_ops.reduce_sum(sequence_loss_by_example(\n        logits, targets, weights,\n        average_across_timesteps=average_across_timesteps,\n        softmax_loss_function=softmax_loss_function))\n    if average_across_batch:\n      batch_size = array_ops.shape(targets[0])[0]\n      return cost / math_ops.cast(batch_size, cost.dtype)\n    else:\n      return cost\n\ndef sequence_loss_by_mle(logits, targets, vocab_size, sequence_length, batch_size, output_projection=None):\n    #print(""logits: "", np.shape(logits[0]))\n    #logits: [seq_len, batch_size, emb_dim]\n    #targets: [seq_len, batch_size]  =====transpose====> [batch_size, seq_len]\n    # labels = tf.to_int32(tf.transpose(targets))\n    #targets: [seq_len, batch_size] ====reshape[-1]====> [seq_len * batch_size]\n    labels = tf.to_int32(tf.reshape(targets, [-1]))\n\n    if output_projection is not None:\n      #logits = nn_ops.xw_plus_b(logits, output_projection[0], output_projection[1])\n      logits = [tf.matmul(logit, output_projection[0]) + output_projection[1] for logit in logits]\n\n    reshape_logits = tf.reshape(logits, [-1, vocab_size]) #[seq_len * batch_size, vocab_size]\n\n    prediction = tf.clip_by_value(reshape_logits, 1e-20, 1.0)\n\n    pretrain_loss = -tf.reduce_sum(\n        # [seq_len * batch_size , vocab_size]\n        tf.one_hot(labels, vocab_size, 1.0, 0.0) * tf.log(prediction)\n    ) / (sequence_length * batch_size)\n    return pretrain_loss\n\n\ndef model_with_buckets(encoder_inputs, decoder_inputs, targets, weights, buckets, vocab_size, batch_size, seq2seq,\n                       output_projection=None, softmax_loss_function=None, per_example_loss=False, name=None):\n  if len(encoder_inputs) < buckets[-1][0]:\n    raise ValueError(""Length of encoder_inputs (%d) must be at least that of la""\n                     ""st bucket (%d)."" % (len(encoder_inputs), buckets[-1][0]))\n  if len(targets) < buckets[-1][1]:\n    raise ValueError(""Length of targets (%d) must be at least that of last""\n                     ""bucket (%d)."" % (len(targets), buckets[-1][1]))\n  if len(weights) < buckets[-1][1]:\n    raise ValueError(""Length of weights (%d) must be at least that of last""\n                     ""bucket (%d)."" % (len(weights), buckets[-1][1]))\n\n  all_inputs = encoder_inputs + decoder_inputs + targets + weights\n  losses = []\n  outputs = []\n  encoder_states = []\n  with ops.name_scope(name, ""model_with_buckets"", all_inputs):\n    for j, bucket in enumerate(buckets):\n      with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                         reuse=True if j > 0 else None):\n        bucket_outputs, decoder_states, encoder_state = seq2seq(encoder_inputs[:bucket[0]],\n                                    decoder_inputs[:bucket[1]])\n        outputs.append(bucket_outputs)\n        #print(""bucket outputs: %s"" %bucket_outputs)\n        encoder_states.append(encoder_state)\n        if per_example_loss:\n          losses.append(sequence_loss_by_example(\n              outputs[-1], targets[:bucket[1]], weights[:bucket[1]],\n              softmax_loss_function=softmax_loss_function))\n        else:\n          # losses.append(sequence_loss_by_mle(outputs[-1], targets[:bucket[1]], vocab_size, bucket[1], batch_size, output_projection))\n          losses.append(sequence_loss(outputs[-1], targets[:bucket[1]], weights[:bucket[1]], softmax_loss_function=softmax_loss_function))\n\n  return outputs, losses, encoder_states'"
utils/__init__.py,0,b''
utils/conf.py,0,"b'__author__ = \'liuyuemaicha\'\nimport os\n\n\n# class disc_config(object):\n#     batch_size = 12\n#     lr = 0.2\n#     lr_decay = 0.9\n#     vocab_size = 250\n#     embed_dim = 100\n#     #hidden_neural_size = 128\n#     num_layers = 2\n#     train_dir = \'./disc_data/\'\n#     name_model = ""disc_model""\n#     tensorboard_dir = ""./tensorboard/disc_log/""\n#     name_loss = ""disc_loss""\n#     max_len = 50\n#     #query_len = 0\n#     valid_num = 100\n#     checkpoint_num = 10\n#     init_scale = 0.1\n#     num_class = 2\n#     keep_prob = 0.5\n#     #num_epoch = 60\n#     #max_decay_epoch = 30\n#     max_grad_norm = 5\n#     steps_per_checkpoint = 100\n#     buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n\n\nclass disc_config(object):\n    batch_size = 256\n    lr = 0.2\n    lr_decay = 0.9\n    vocab_size = 25000\n    embed_dim = 512\n    steps_per_checkpoint = 200\n    #hidden_neural_size = 128\n    num_layers = 2\n    train_dir = \'./disc_data/\'\n    name_model = ""disc_model""\n    tensorboard_dir = ""./tensorboard/disc_log/""\n    name_loss = ""disc_loss""\n    max_len = 50\n    piece_size = batch_size * steps_per_checkpoint\n    piece_dir = ""./disc_data/batch_piece/""\n    #query_len = 0\n    valid_num = 100\n    init_scale = 0.1\n    num_class = 2\n    keep_prob = 0.5\n    #num_epoch = 60\n    #max_decay_epoch = 30\n    max_grad_norm = 5\n    buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n\n\nclass gen_config(object):\n    beam_size = 7\n    learning_rate = 0.5\n    learning_rate_decay_factor = 0.99\n    max_gradient_norm = 5.0\n    batch_size = 128\n    emb_dim = 512\n    num_layers = 2\n    vocab_size = 25000\n    train_dir = ""./gen_data/""\n    name_model = ""st_model""\n    tensorboard_dir = ""./tensorboard/gen_log/""\n    name_loss = ""gen_loss""\n    teacher_loss = ""teacher_loss""\n    reward_name = ""reward""\n    max_train_data_size = 0\n    steps_per_checkpoint = 200\n    buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n    buckets_concat = [(5, 10), (10, 15), (20, 25), (40, 50), (100, 50)]\n\n\n# class gen_config(object):\n#     beam_size = 7\n#     learning_rate = 0.5\n#     learning_rate_decay_factor = 0.99\n#     max_gradient_norm = 5.0\n#     batch_size = 25\n#     emb_dim = 102\n#     num_layers = 2\n#     vocab_size = 250\n#     train_dir = ""./gen_data/""\n#     name_model = ""st_model""\n#     tensorboard_dir = ""./tensorboard/gen_log/""\n#     name_loss = ""gen_loss""\n#     teacher_loss = ""teacher_loss""\n#     max_train_data_size = 0\n#     steps_per_checkpoint = 100\n#     buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n#     buckets_concat = [(5, 10), (10, 15), (20, 25), (40, 50), (100, 50)]\n\n\nclass GSTConfig(object):\n    beam_size = 7\n    learning_rate = 0.5\n    learning_rate_decay_factor = 0.99\n    max_gradient_norm = 5.0\n    batch_size = 256\n    emb_dim = 1024\n    num_layers = 2\n    vocab_size = 25000\n    train_dir = ""./gst_data/""\n    name_model = ""st_model""\n    tensorboard_dir = ""./tensorboard/gst_log/""\n    name_loss = ""gst_loss""\n    max_train_data_size = 0\n    steps_per_checkpoint = 200\n    buckets =        [(5, 10), (10, 15), (20, 25), (40, 50)]\n    buckets_concat = [(5, 10), (10, 15), (20, 25), (40, 50), (100, 50)]\n\n\n'"
utils/data_helper.py,0,"b'""""""\ndescription: this file helps to load raw file and gennerate batch x,y\nauthor:luchi\ndate:22/11/2016\n""""""\nimport numpy as np\nimport cPickle as pkl\n\n#file path\ndataset_path=\'disc_data/subj0.pkl\'\n\ndef set_dataset_path(path):\n    dataset_path=path\n\n\n\n\ndef load_data(debug, max_len,batch_size,n_words=20000,valid_portion=0.1,sort_by_len=True):\n    f=open(dataset_path,\'rb\')\n    print (\'load disc_data from %s\',dataset_path)\n    train_set = np.array(pkl.load(f))\n    test_set = np.array(pkl.load(f))\n    print(""train_set: "", np.shape(train_set))\n    print(""test_set: "", np.shape(test_set))\n    f.close()\n\n    train_set_x,train_set_y = train_set\n\n    # w_f = open(""corpus"", ""w"")\n    # for x, y in zip(train_set_x, train_set_y):\n    #     w_f.write(str(x) + ""\\t"" + str(y) + ""\\n"")\n    # w_f.close()\n\n    #train_set length\n    n_samples= len(train_set_x)\n    #shuffle and generate train and valid dataset\n    sidx = np.random.permutation(n_samples)\n    if debug: print(""sidx: "", sidx)\n    n_train = int(np.round(n_samples * (1. - valid_portion)))\n    if debug: print(""n_train: "", n_train)\n\n    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n\n    if debug: print(""train_set_x[0]: "", train_set_x[0])\n    if debug: print(""train_set_y[0]: "", train_set_y[0])\n\n    if debug: print(""train_set_x[1]: "", train_set_x[1])\n    if debug: print(""train_set_y[1]: "", train_set_y[1])\n\n    train_set = (train_set_x, train_set_y)\n    valid_set = (valid_set_x, valid_set_y)\n    if debug: print(""train_set shape: "", np.shape(train_set))\n    if debug: print(""valid_set shape: "", np.shape(valid_set))\n\n    #remove unknow words\n    def remove_unk(x):\n        return [[1 if w >= n_words else w for w in sen] for sen in x]\n\n    test_set_x, test_set_y = test_set\n    valid_set_x, valid_set_y = valid_set\n    train_set_x, train_set_y = train_set\n\n    train_set_x = remove_unk(train_set_x)\n    valid_set_x = remove_unk(valid_set_x)\n    test_set_x = remove_unk(test_set_x)\n\n\n\n    def len_argsort(seq):\n        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n\n    if sort_by_len:\n        sorted_index = len_argsort(test_set_x)\n        test_set_x = [test_set_x[i] for i in sorted_index]\n        test_set_y = [test_set_y[i] for i in sorted_index]\n\n        sorted_index = len_argsort(valid_set_x)\n        valid_set_x = [valid_set_x[i] for i in sorted_index]\n        valid_set_y = [valid_set_y[i] for i in sorted_index]\n\n\n        sorted_index = len_argsort(train_set_x)\n        train_set_x = [train_set_x[i] for i in sorted_index]\n        train_set_y = [train_set_y[i] for i in sorted_index]\n\n    train_set=(train_set_x,train_set_y)\n    valid_set=(valid_set_x,valid_set_y)\n    test_set=(test_set_x,test_set_y)\n\n\n\n\n    new_train_set_x=np.zeros([len(train_set[0]),max_len])\n    if debug: print(""new_train_set: "", np.shape(new_train_set_x))\n    new_train_set_y=np.zeros(len(train_set[0]))\n    if debug: print(""new_train_set_y: "", np.shape(new_train_set_y))\n\n    new_valid_set_x=np.zeros([len(valid_set[0]),max_len])\n    new_valid_set_y=np.zeros(len(valid_set[0]))\n\n    new_test_set_x=np.zeros([len(test_set[0]),max_len])\n    new_test_set_y=np.zeros(len(test_set[0]))\n\n    mask_train_x=np.zeros([max_len,len(train_set[0])])\n    mask_test_x=np.zeros([max_len,len(test_set[0])])\n    mask_valid_x=np.zeros([max_len,len(valid_set[0])])\n\n\n    def padding_and_generate_mask(x,y,new_x,new_y,new_mask_x):\n\n        for i,(x,y) in enumerate(zip(x,y)):\n            #whether to remove sentences with length larger than maxlen\n            if len(x)<=max_len:\n                new_x[i,0:len(x)]=x\n                new_mask_x[0:len(x),i]=1\n                new_y[i]=y\n            else:\n                new_x[i]=(x[0:max_len])\n                new_mask_x[:,i]=1\n                new_y[i]=y\n        new_set =(new_x,new_y,new_mask_x)\n        del new_x,new_y\n        return new_set\n\n    train_set=padding_and_generate_mask(train_set[0],train_set[1],new_train_set_x,new_train_set_y,mask_train_x)\n    test_set=padding_and_generate_mask(test_set[0],test_set[1],new_test_set_x,new_test_set_y,mask_test_x)\n    valid_set=padding_and_generate_mask(valid_set[0],valid_set[1],new_valid_set_x,new_valid_set_y,mask_valid_x)\n\n    return train_set,valid_set,test_set\n\n\n#return batch dataset\ndef batch_iter(data,batch_size):\n\n    #get dataset and label\n    x,y,mask_x=data\n    x=np.array(x)\n    y=np.array(y)\n    data_size=len(x)\n    num_batches_per_epoch=int((data_size-1)/batch_size)\n    for batch_index in range(num_batches_per_epoch):\n        start_index=batch_index*batch_size\n        end_index=min((batch_index+1)*batch_size,data_size)\n        return_x = x[start_index:end_index]\n        return_y = y[start_index:end_index]\n        return_mask_x = mask_x[:,start_index:end_index]\n        # if(len(return_x)<batch_size):\n        #     print(len(return_x))\n        #     print return_x\n        #     print return_y\n        #     print return_mask_x\n        #     import sys\n        #     sys.exit(0)\n        yield (return_x,return_y,return_mask_x)\n\n\n'"
utils/data_utils.py,1,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for downloading disc_data from WMT, tokenizing, vocabularies.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport re\nimport tarfile\n\nfrom six.moves import urllib\n\nfrom tensorflow.python.platform import gfile\nimport tensorflow as tf\n\n# Special vocabulary symbols - we always put them at the start.\n_PAD = b""_PAD""\n_GO = b""_GO""\n_EOS = b""_EOS""\n_UNK = b""_UNK""\n_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n\nPAD_ID = 0\nGO_ID = 1\nEOS_ID = 2\nUNK_ID = 3\n\n# Regular expressions used to tokenize.\n_WORD_SPLIT = re.compile(b""([.,!?\\""\':;)(])"")\n_DIGIT_RE = re.compile(br""\\d"")\n\n# URLs for WMT disc_data.\n_WMT_ENFR_TRAIN_URL = ""http://www.statmt.org/wmt10/training-giga-fren.tar""\n_WMT_ENFR_DEV_URL = ""http://www.statmt.org/wmt15/dev-v2.tgz""\n\n\ndef maybe_download(directory, filename, url):\n  """"""Download filename from url unless it\'s already in directory.""""""\n  if not os.path.exists(directory):\n    print(""Creating directory %s"" % directory)\n    os.mkdir(directory)\n  filepath = os.path.join(directory, filename)\n  if not os.path.exists(filepath):\n    print(""Downloading %s to %s"" % (url, filepath))\n    filepath, _ = urllib.request.urlretrieve(url, filepath)\n    statinfo = os.stat(filepath)\n    print(""Succesfully downloaded"", filename, statinfo.st_size, ""bytes"")\n  return filepath\n\n\ndef gunzip_file(gz_path, new_path):\n  """"""Unzips from gz_path into new_path.""""""\n  print(""Unpacking %s to %s"" % (gz_path, new_path))\n  with gzip.open(gz_path, ""rb"") as gz_file:\n    with open(new_path, ""wb"") as new_file:\n      for line in gz_file:\n        new_file.write(line)\n\n\ndef get_wmt_enfr_train_set(directory):\n  """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n  train_path = os.path.join(directory, ""giga-fren.release2.fixed"")\n  if not (gfile.Exists(train_path +"".fr"") and gfile.Exists(train_path +"".en"")):\n    corpus_file = maybe_download(directory, ""training-giga-fren.tar"",\n                                 _WMT_ENFR_TRAIN_URL)\n    print(""Extracting tar file %s"" % corpus_file)\n    with tarfile.open(corpus_file, ""r"") as corpus_tar:\n      corpus_tar.extractall(directory)\n    gunzip_file(train_path + "".fr.gz"", train_path + "".fr"")\n    gunzip_file(train_path + "".en.gz"", train_path + "".en"")\n  return train_path\n\n\ndef get_wmt_enfr_dev_set(directory):\n  """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n  dev_name = ""newstest2013""\n  dev_path = os.path.join(directory, dev_name)\n  if not (gfile.Exists(dev_path + "".fr"") and gfile.Exists(dev_path + "".en"")):\n    dev_file = maybe_download(directory, ""dev-v2.tgz"", _WMT_ENFR_DEV_URL)\n    print(""Extracting tgz file %s"" % dev_file)\n    with tarfile.open(dev_file, ""r:gz"") as dev_tar:\n      fr_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".fr"")\n      en_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".en"")\n      fr_dev_file.name = dev_name + "".fr""  # Extract without ""dev/"" prefix.\n      en_dev_file.name = dev_name + "".en""\n      dev_tar.extract(fr_dev_file, directory)\n      dev_tar.extract(en_dev_file, directory)\n  return dev_path\n\n\ndef basic_tokenizer(sentence):\n  """"""Very basic tokenizer: split the sentence into a list of tokens.""""""\n  words = []\n  for space_separated_fragment in sentence.strip().split():\n    words.extend(_WORD_SPLIT.split(space_separated_fragment))\n  return [w for w in words if w]\n\n\ndef create_vocabulary(vocabulary_path, data_path_list, max_vocabulary_size,\n                      tokenizer=None, normalize_digits=True):\n  """"""Create vocabulary file (if it does not exist yet) from disc_data file.\n\n  Data file is assumed to contain one sentence per line. Each sentence is\n  tokenized and digits are normalized (if normalize_digits is set).\n  Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n  We write it to vocabulary_path in a one-token-per-line format, so that later\n  token in the first line gets id=0, second line gets id=1, and so on.\n\n  Args:\n    vocabulary_path: path where the vocabulary will be created.\n    data_path: disc_data file that will be used to create vocabulary.\n    max_vocabulary_size: limit on the size of the created vocabulary.\n    tokenizer: a function to use to tokenize each disc_data sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(vocabulary_path):\n    print(""Creating vocabulary %s from disc_data %s"" % (vocabulary_path, data_path_list))\n    vocab = {}\n    for data_path in data_path_list:\n        with gfile.GFile(data_path, mode=""rb"") as f:\n          counter = 0\n          for line in f:\n            counter += 1\n            if counter % 100000 == 0:\n              print(""  processing line %d"" % counter)\n            line = tf.compat.as_bytes(line)\n            tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n            for w in tokens:\n              word = _DIGIT_RE.sub(b""0"", w) if normalize_digits else w\n              if word in vocab:\n                vocab[word] += 1\n              else:\n                vocab[word] = 1\n\n    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n    if len(vocab_list) > max_vocabulary_size:\n      vocab_list = vocab_list[:max_vocabulary_size]\n    with gfile.GFile(vocabulary_path, mode=""wb"") as vocab_file:\n      for w in vocab_list:\n        vocab_file.write(w + b""\\n"")\n\n\ndef initialize_vocabulary(vocabulary_path):\n  """"""Initialize vocabulary from file.\n\n  We assume the vocabulary is stored one-item-per-line, so a file:\n    dog\n    cat\n  will result in a vocabulary {""dog"": 0, ""cat"": 1}, and this function will\n  also return the reversed-vocabulary [""dog"", ""cat""].\n\n  Args:\n    vocabulary_path: path to the file containing the vocabulary.\n\n  Returns:\n    a pair: the vocabulary (a dictionary mapping string to integers), and\n    the reversed vocabulary (a list, which reverses the vocabulary mapping).\n\n  Raises:\n    ValueError: if the provided vocabulary_path does not exist.\n  """"""\n  if gfile.Exists(vocabulary_path):\n    rev_vocab = []\n    with gfile.GFile(vocabulary_path, mode=""rb"") as f:\n      rev_vocab.extend(f.readlines())\n    rev_vocab = [line.strip() for line in rev_vocab]\n    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n    return vocab, rev_vocab\n  else:\n    raise ValueError(""Vocabulary file %s not found."", vocabulary_path)\n\n\ndef sentence_to_token_ids(sentence, vocabulary,\n                          tokenizer=None, normalize_digits=True):\n  """"""Convert a string to list of integers representing token-ids.\n\n  For example, a sentence ""I have a dog"" may become tokenized into\n  [""I"", ""have"", ""a"", ""dog""] and with vocabulary {""I"": 1, ""have"": 2,\n  ""a"": 4, ""dog"": 7""} this function will return [1, 2, 4, 7].\n\n  Args:\n    sentence: the sentence in bytes format to convert to token-ids.\n    vocabulary: a dictionary mapping tokens to integers.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n\n  Returns:\n    a list of integers, the token-ids for the sentence.\n  """"""\n\n  if tokenizer:\n    words = tokenizer(sentence)\n  else:\n    words = basic_tokenizer(sentence)\n  if not normalize_digits:\n    return [vocabulary.get(w, UNK_ID) for w in words]\n  # Normalize digits by 0 before looking words up in the vocabulary.\n  return [vocabulary.get(_DIGIT_RE.sub(b""0"", w), UNK_ID) for w in words]\n\n\ndef data_to_token_ids(data_path, target_path, vocabulary,\n                      tokenizer=None, normalize_digits=True):\n  """"""Tokenize disc_data file and turn into token-ids using given vocabulary file.\n\n  This function loads disc_data line-by-line from data_path, calls the above\n  sentence_to_token_ids, and saves the result to target_path. See comment\n  for sentence_to_token_ids on the details of token-ids format.\n\n  Args:\n    data_path: path to the disc_data file in one-sentence-per-line format.\n    target_path: path where the file with token-ids will be created.\n    vocabulary_path: path to the vocabulary file.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(target_path):\n    print(""Tokenizing disc_data in %s"" % data_path)\n    #print(""target path: "", target_path)\n    #vocab, _ = initialize_vocabulary(vocabulary_path)\n    with gfile.GFile(data_path, mode=""rb"") as data_file:\n      with gfile.GFile(target_path, mode=""w"") as tokens_file:\n        counter = 0\n        for line in data_file:\n          counter += 1\n          if counter % 100000 == 0:\n            print(""  tokenizing line %d"" % counter)\n          token_ids = sentence_to_token_ids(line, vocabulary, tokenizer,\n                                            normalize_digits)\n          tokens_file.write("" "".join([str(tok) for tok in token_ids]) + ""\\n"")\n\n\ndef prepare_chitchat_data(data_dir, vocabulary, vocabulary_size, tokenizer=None):\n  """"""Get WMT disc_data into data_dir, create vocabularies and tokenize disc_data.\n\n  Args:\n    data_dir: directory in which the disc_data sets will be stored.\n    en_vocabulary_size: size of the English vocabulary to create and use.\n    fr_vocabulary_size: size of the French vocabulary to create and use.\n    tokenizer: a function to use to tokenize each disc_data sentence;\n      if None, basic_tokenizer will be used.\n\n  Returns:\n    A tuple of 6 elements:\n      (1) path to the token-ids for English training disc_data-set,\n      (2) path to the token-ids for French training disc_data-set,\n      (3) path to the token-ids for English development disc_data-set,\n      (4) path to the token-ids for French development disc_data-set,\n      (5) path to the English vocabulary file,\n      (6) path to the French vocabulary file.\n  """"""\n  # Get wmt disc_data to the specified directory.\n  #train_path = get_wmt_enfr_train_set(data_dir)\n  train_path = os.path.join(data_dir, ""chitchat.train"")\n  #dev_path = get_wmt_enfr_dev_set(data_dir)\n  dev_path = os.path.join(data_dir, ""chitchat.dev"")\n  # fixed_path = os.path.join(data_dir, ""chitchat.fixed"")\n  # weibo_path = os.path.join(data_dir, ""chitchat.weibo"")\n  # qa_path = os.path.join(data_dir, ""chitchat.qa"")\n\n  # voc_file_path = [train_path+"".answer"", fixed_path+"".answer"", weibo_path+"".answer"", qa_path+"".answer"",\n  #                    train_path+"".query"", fixed_path+"".query"", weibo_path+"".query"", qa_path+"".query""]\n  #voc_query_path = [train_path+"".query"", fixed_path+"".query"", weibo_path+"".query"", qa_path+"".query""]\n  # Create vocabularies of the appropriate sizes.\n  #vocab_path = os.path.join(data_dir, ""vocab%d.all"" % vocabulary_size)\n  #query_vocab_path = os.path.join(data_dir, ""vocab%d.query"" % en_vocabulary_size)\n\n  #create_vocabulary(vocab_path, voc_file_path, vocabulary_size)\n\n\n  #create_vocabulary(query_vocab_path, voc_query_path, en_vocabulary_size)\n\n  # Create token ids for the training disc_data.\n  answer_train_ids_path = train_path + ("".ids%d.answer"" % vocabulary_size)\n  query_train_ids_path = train_path + ("".ids%d.query"" % vocabulary_size)\n  data_to_token_ids(train_path + "".answer"", answer_train_ids_path, vocabulary, tokenizer)\n  data_to_token_ids(train_path + "".query"", query_train_ids_path, vocabulary, tokenizer)\n\n  # Create token ids for the development disc_data.\n  answer_dev_ids_path = dev_path + ("".ids%d.answer"" % vocabulary_size)\n  query_dev_ids_path = dev_path + ("".ids%d.query"" % vocabulary_size)\n  data_to_token_ids(dev_path + "".answer"", answer_dev_ids_path, vocabulary, tokenizer)\n  data_to_token_ids(dev_path + "".query"", query_dev_ids_path, vocabulary, tokenizer)\n\n  return (query_train_ids_path, answer_train_ids_path,\n          query_dev_ids_path, answer_dev_ids_path)\n\ndef hier_prepare_disc_data(data_dir, vocabulary, vocabulary_size, tokenizer=None):\n  """"""Get WMT disc_data into data_dir, create vocabularies and tokenize disc_data.\n\n  Args:\n    data_dir: directory in which the disc_data sets will be stored.\n    en_vocabulary_size: size of the English vocabulary to create and use.\n    fr_vocabulary_size: size of the French vocabulary to create and use.\n    tokenizer: a function to use to tokenize each disc_data sentence;\n      if None, basic_tokenizer will be used.\n\n  Returns:\n    A tuple of 6 elements:\n      (1) path to the token-ids for English training disc_data-set,\n      (2) path to the token-ids for French training disc_data-set,\n      (3) path to the token-ids for English development disc_data-set,\n      (4) path to the token-ids for French development disc_data-set,\n      (5) path to the English vocabulary file,\n      (6) path to the French vocabulary file.\n  """"""\n  # Get wmt disc_data to the specified directory.\n  #train_path = get_wmt_enfr_train_set(data_dir)\n  train_path = os.path.join(data_dir, ""train"")\n  #dev_path = get_wmt_enfr_dev_set(data_dir)\n  dev_path = os.path.join(data_dir, ""dev"")\n\n  # Create token ids for the training disc_data.\n  query_train_ids_path = train_path + ("".ids%d.query"" % vocabulary_size)\n  answer_train_ids_path = train_path + ("".ids%d.answer"" % vocabulary_size)\n  gen_train_ids_path = train_path + ("".ids%d.gen"" % vocabulary_size)\n\n  data_to_token_ids(train_path + "".query"", query_train_ids_path, vocabulary, tokenizer)\n  data_to_token_ids(train_path + "".answer"", answer_train_ids_path, vocabulary, tokenizer)\n  data_to_token_ids(train_path + "".gen"", gen_train_ids_path, vocabulary, tokenizer)\n\n  # Create token ids for the development disc_data.\n  query_dev_ids_path = dev_path + ("".ids%d.query"" % vocabulary_size)\n  answer_dev_ids_path = dev_path + ("".ids%d.answer"" % vocabulary_size)\n  gen_dev_ids_path = dev_path + ("".ids%d.gen"" % vocabulary_size)\n\n  data_to_token_ids(dev_path + "".query"", query_dev_ids_path, vocabulary, tokenizer)\n  data_to_token_ids(dev_path + "".answer"", answer_dev_ids_path, vocabulary, tokenizer)\n  data_to_token_ids(dev_path + "".gen"", gen_dev_ids_path, vocabulary, tokenizer)\n\n  return (query_train_ids_path, answer_train_ids_path, gen_train_ids_path,\n          query_dev_ids_path, answer_dev_ids_path, gen_dev_ids_path)\n\ndef prepare_disc_data(data_dir, vocabulary, vocabulary_size, tokenizer=None):\n\n  train_path = os.path.join(data_dir, ""train"")\n  #dev_path = get_wmt_enfr_dev_set(data_dir)\n  dev_path = os.path.join(data_dir, ""dev"")\n\n  # Create token ids for the training data.\n  answer_train_ids_path = train_path + ("".ids%d.pos"" % vocabulary_size)\n  query_train_ids_path = train_path + ("".ids%d.neg"" % vocabulary_size)\n  data_to_token_ids(train_path + "".pos"", answer_train_ids_path, vocabulary, tokenizer)\n  data_to_token_ids(train_path + "".neg"", query_train_ids_path, vocabulary, tokenizer)\n\n  # Create token ids for the development data.\n  answer_dev_ids_path = dev_path + ("".ids%d.pos"" % vocabulary_size)\n  query_dev_ids_path = dev_path + ("".ids%d.neg"" % vocabulary_size)\n  data_to_token_ids(dev_path + "".pos"", answer_dev_ids_path, vocabulary, tokenizer)\n  data_to_token_ids(dev_path + "".neg"", query_dev_ids_path, vocabulary, tokenizer)\n\n  return (query_train_ids_path, answer_train_ids_path,\n          query_dev_ids_path, answer_dev_ids_path)\n\n\n\ndef prepare_defined_data(data_path, vocabulary, vocabulary_size, tokenizer=None):\n  #vocab_path = os.path.join(data_dir, ""vocab%d.all"" %vocabulary_size)\n  #query_vocab_path = os.path.join(data_dir, ""vocab%d.query"" %query_vocabulary_size)\n\n  answer_fixed_ids_path = data_path + ("".ids%d.answer"" % vocabulary_size)\n  query_fixed_ids_path = data_path + ("".ids%d.query"" % vocabulary_size)\n\n  data_to_token_ids(data_path + "".answer"", answer_fixed_ids_path, vocabulary, tokenizer)\n  data_to_token_ids(data_path + "".query"", query_fixed_ids_path, vocabulary, tokenizer)\n  return (query_fixed_ids_path, answer_fixed_ids_path)\n\ndef get_dummy_set(dummy_path, vocabulary, vocabulary_size, tokenizer=None):\n    dummy_ids_path = dummy_path + ("".ids%d"" % vocabulary_size)\n    data_to_token_ids(dummy_path, dummy_ids_path, vocabulary, tokenizer)\n    dummy_set = []\n    with gfile.GFile(dummy_ids_path, ""r"") as dummy_file:\n        line = dummy_file.readline()\n        counter = 0\n        while line:\n            counter += 1\n            dummy_set.append([int(x) for x in line.split()])\n            line = dummy_file.readline()\n    return dummy_set'"
