file_path,api_count,code
__init__.py,0,"b""__author__ = 'MORAN01'\n"""
conf.py,0,"b'__author__ = \'liuyuemaicha\'\n\n\nclass GSTConfig(object):\n    beam_size = 7\n    learning_rate = 0.5\n    learning_rate_decay_factor = 0.99\n    max_gradient_norm = 5.0\n    batch_size = 256\n    emb_dim = 1024\n    num_layers = 2\n    vocab_size = 25000\n    train_dir = ""./gst_data/""\n    name_model = ""st_model""\n    tensorboard_dir = ""./tensorboard/gst_log/""\n    name_loss = ""gst_loss""\n    max_train_data_size = 0\n    steps_per_checkpoint = 200\n    buckets =        [(5, 10), (10, 15), (20, 25), (40, 50)]\n    buckets_concat = [(5, 10), (10, 15), (20, 25), (40, 50), (100, 50)]\n\n\nclass GCCConfig(object):\n    beam_size = 7\n    learning_rate = 0.5\n    learning_rate_decay_factor = 0.99\n    max_gradient_norm = 5.0\n    batch_size = 128\n    emb_dim = 1024\n    num_layers = 2\n    vocab_size = 25000\n    train_dir = ""./gcc_data/""\n    name_model = ""cc_model""\n    tensorboard_dir = ""./tensorboard/gcc_log/""\n    name_loss = ""gcc_loss""\n    max_train_data_size = 0\n    steps_per_checkpoint = 200\n    buckets =        [(10, 10), (20, 15), (40, 25), (80, 50)]\n    buckets_concat = [(10, 10), (20, 15), (40, 25), (80, 50), (100,50)]\n\n\nclass GBKConfig(object):\n    beam_size = 7\n    learning_rate = 0.5\n    learning_rate_decay_factor = 0.99\n    max_gradient_norm = 5.0\n    batch_size = 256\n    emb_dim = 1024\n    num_layers = 2\n    vocab_size = 25000\n    train_dir = ""./gbk_data/""\n    name_model = ""bk_model""\n    tensorboard_dir = ""./tensorboard/gbk_log/""\n    name_loss = ""gbk_loss""\n    max_train_data_size = 0\n    steps_per_checkpoint = 200\n    buckets =        [(10, 5), (15, 10), (25, 20), (50, 40)]\n    buckets_concat = [(10, 5), (15, 10), (25, 20), (50, 40), (100, 50)]\n\n\nclass GRLConfig(object):\n    beam_size = 7\n    learning_rate = 0.5\n    learning_rate_decay_factor = 0.99\n    max_gradient_norm = 5.0\n    batch_size = 256\n    emb_dim = 1024\n    num_layers = 2\n    vocab_size = 25000\n    train_dir = ""./grl_data/""\n    name_model = ""rl_model""\n    tensorboard_dir = ""./tensorboard/grl_log/""\n    name_loss = ""grl_loss""\n    pre_name_loss = ""pre_rl_loss""\n    max_train_data_size = 0\n    steps_per_checkpoint = 200\n    buckets =        [(5, 10), (10, 15), (20, 25), (40, 50)]\n    buckets_concat = [(5, 10), (10, 15), (20, 25), (40, 50), (100, 50)]\n\nclass Pre_GRLConfig(object):\n    beam_size = 4\n    learning_rate = 0.5\n    learning_rate_decay_factor = 0.99\n    max_gradient_norm = 5.0\n    batch_size = 10\n    emb_dim = 512\n    num_layers = 2\n    vocab_size = 1000\n    train_dir = ""./pre_grl_data/""\n    name_model = ""rl_model""\n    tensorboard_dir = ""./tensorboard/grl_log/""\n    name_loss = ""grl_loss""\n    pre_name_loss = ""pre_rl_loss""\n    max_train_data_size = 0\n    steps_per_checkpoint = 200\n    buckets =        [(5, 10), (10, 15), (20, 25), (40, 50)]\n    buckets_concat = [(5, 10), (10, 15), (20, 25), (40, 50), (100, 50)]'"
data_utils.py,1,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for downloading data from WMT, tokenizing, vocabularies.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport re\nimport tarfile\n\nfrom six.moves import urllib\n\nfrom tensorflow.python.platform import gfile\nimport tensorflow as tf\n\n# Special vocabulary symbols - we always put them at the start.\n_PAD = b""_PAD""\n_GO = b""_GO""\n_EOS = b""_EOS""\n_UNK = b""_UNK""\n_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n\nPAD_ID = 0\nGO_ID = 1\nEOS_ID = 2\nUNK_ID = 3\n\n# Regular expressions used to tokenize.\n_WORD_SPLIT = re.compile(b""([.,!?\\""\':;)(])"")\n_DIGIT_RE = re.compile(br""\\d"")\n\n# URLs for WMT data.\n_WMT_ENFR_TRAIN_URL = ""http://www.statmt.org/wmt10/training-giga-fren.tar""\n_WMT_ENFR_DEV_URL = ""http://www.statmt.org/wmt15/dev-v2.tgz""\n\n\ndef maybe_download(directory, filename, url):\n  """"""Download filename from url unless it\'s already in directory.""""""\n  if not os.path.exists(directory):\n    print(""Creating directory %s"" % directory)\n    os.mkdir(directory)\n  filepath = os.path.join(directory, filename)\n  if not os.path.exists(filepath):\n    print(""Downloading %s to %s"" % (url, filepath))\n    filepath, _ = urllib.request.urlretrieve(url, filepath)\n    statinfo = os.stat(filepath)\n    print(""Succesfully downloaded"", filename, statinfo.st_size, ""bytes"")\n  return filepath\n\n\ndef gunzip_file(gz_path, new_path):\n  """"""Unzips from gz_path into new_path.""""""\n  print(""Unpacking %s to %s"" % (gz_path, new_path))\n  with gzip.open(gz_path, ""rb"") as gz_file:\n    with open(new_path, ""wb"") as new_file:\n      for line in gz_file:\n        new_file.write(line)\n\n\ndef get_wmt_enfr_train_set(directory):\n  """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n  train_path = os.path.join(directory, ""giga-fren.release2.fixed"")\n  if not (gfile.Exists(train_path +"".fr"") and gfile.Exists(train_path +"".en"")):\n    corpus_file = maybe_download(directory, ""training-giga-fren.tar"",\n                                 _WMT_ENFR_TRAIN_URL)\n    print(""Extracting tar file %s"" % corpus_file)\n    with tarfile.open(corpus_file, ""r"") as corpus_tar:\n      corpus_tar.extractall(directory)\n    gunzip_file(train_path + "".fr.gz"", train_path + "".fr"")\n    gunzip_file(train_path + "".en.gz"", train_path + "".en"")\n  return train_path\n\n\ndef get_wmt_enfr_dev_set(directory):\n  """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n  dev_name = ""newstest2013""\n  dev_path = os.path.join(directory, dev_name)\n  if not (gfile.Exists(dev_path + "".fr"") and gfile.Exists(dev_path + "".en"")):\n    dev_file = maybe_download(directory, ""dev-v2.tgz"", _WMT_ENFR_DEV_URL)\n    print(""Extracting tgz file %s"" % dev_file)\n    with tarfile.open(dev_file, ""r:gz"") as dev_tar:\n      fr_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".fr"")\n      en_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".en"")\n      fr_dev_file.name = dev_name + "".fr""  # Extract without ""dev/"" prefix.\n      en_dev_file.name = dev_name + "".en""\n      dev_tar.extract(fr_dev_file, directory)\n      dev_tar.extract(en_dev_file, directory)\n  return dev_path\n\n\ndef basic_tokenizer(sentence):\n  """"""Very basic tokenizer: split the sentence into a list of tokens.""""""\n  words = []\n  for space_separated_fragment in sentence.strip().split():\n    words.extend(_WORD_SPLIT.split(space_separated_fragment))\n  return [w for w in words if w]\n\n\ndef create_vocabulary(vocabulary_path, data_path_list, max_vocabulary_size,\n                      tokenizer=None, normalize_digits=True):\n  """"""Create vocabulary file (if it does not exist yet) from data file.\n\n  Data file is assumed to contain one sentence per line. Each sentence is\n  tokenized and digits are normalized (if normalize_digits is set).\n  Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n  We write it to vocabulary_path in a one-token-per-line format, so that later\n  token in the first line gets id=0, second line gets id=1, and so on.\n\n  Args:\n    vocabulary_path: path where the vocabulary will be created.\n    data_path: data file that will be used to create vocabulary.\n    max_vocabulary_size: limit on the size of the created vocabulary.\n    tokenizer: a function to use to tokenize each data sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(vocabulary_path):\n    print(""Creating vocabulary %s from data %s"" % (vocabulary_path, data_path_list))\n    vocab = {}\n    for data_path in data_path_list:\n        with gfile.GFile(data_path, mode=""rb"") as f:\n          counter = 0\n          for line in f:\n            counter += 1\n            if counter % 100000 == 0:\n              print(""  processing line %d"" % counter)\n            line = tf.compat.as_bytes(line)\n            tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n            for w in tokens:\n              word = _DIGIT_RE.sub(b""0"", w) if normalize_digits else w\n              if word in vocab:\n                vocab[word] += 1\n              else:\n                vocab[word] = 1\n\n    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n    if len(vocab_list) > max_vocabulary_size:\n      vocab_list = vocab_list[:max_vocabulary_size]\n    with gfile.GFile(vocabulary_path, mode=""wb"") as vocab_file:\n      for w in vocab_list:\n        vocab_file.write(w + b""\\n"")\n\n\ndef initialize_vocabulary(vocabulary_path):\n  """"""Initialize vocabulary from file.\n\n  We assume the vocabulary is stored one-item-per-line, so a file:\n    dog\n    cat\n  will result in a vocabulary {""dog"": 0, ""cat"": 1}, and this function will\n  also return the reversed-vocabulary [""dog"", ""cat""].\n\n  Args:\n    vocabulary_path: path to the file containing the vocabulary.\n\n  Returns:\n    a pair: the vocabulary (a dictionary mapping string to integers), and\n    the reversed vocabulary (a list, which reverses the vocabulary mapping).\n\n  Raises:\n    ValueError: if the provided vocabulary_path does not exist.\n  """"""\n  if gfile.Exists(vocabulary_path):\n    rev_vocab = []\n    with gfile.GFile(vocabulary_path, mode=""rb"") as f:\n      rev_vocab.extend(f.readlines())\n    rev_vocab = [line.strip() for line in rev_vocab]\n    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n    return vocab, rev_vocab\n  else:\n    raise ValueError(""Vocabulary file %s not found."", vocabulary_path)\n\n\ndef sentence_to_token_ids(sentence, vocabulary,\n                          tokenizer=None, normalize_digits=True):\n  """"""Convert a string to list of integers representing token-ids.\n\n  For example, a sentence ""I have a dog"" may become tokenized into\n  [""I"", ""have"", ""a"", ""dog""] and with vocabulary {""I"": 1, ""have"": 2,\n  ""a"": 4, ""dog"": 7""} this function will return [1, 2, 4, 7].\n\n  Args:\n    sentence: the sentence in bytes format to convert to token-ids.\n    vocabulary: a dictionary mapping tokens to integers.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n\n  Returns:\n    a list of integers, the token-ids for the sentence.\n  """"""\n\n  if tokenizer:\n    words = tokenizer(sentence)\n  else:\n    words = basic_tokenizer(sentence)\n  if not normalize_digits:\n    return [vocabulary.get(w, UNK_ID) for w in words]\n  # Normalize digits by 0 before looking words up in the vocabulary.\n  return [vocabulary.get(_DIGIT_RE.sub(b""0"", w), UNK_ID) for w in words]\n\n\ndef data_to_token_ids(data_path, target_path, vocabulary,\n                      tokenizer=None, normalize_digits=True):\n  """"""Tokenize data file and turn into token-ids using given vocabulary file.\n\n  This function loads data line-by-line from data_path, calls the above\n  sentence_to_token_ids, and saves the result to target_path. See comment\n  for sentence_to_token_ids on the details of token-ids format.\n\n  Args:\n    data_path: path to the data file in one-sentence-per-line format.\n    target_path: path where the file with token-ids will be created.\n    vocabulary_path: path to the vocabulary file.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(target_path):\n    print(""Tokenizing data in %s"" % data_path)\n    #vocab, _ = initialize_vocabulary(vocabulary_path)\n    with gfile.GFile(data_path, mode=""rb"") as data_file:\n      with gfile.GFile(target_path, mode=""w"") as tokens_file:\n        counter = 0\n        for line in data_file:\n          counter += 1\n          if counter % 100000 == 0:\n            print(""  tokenizing line %d"" % counter)\n          token_ids = sentence_to_token_ids(line, vocabulary, tokenizer,\n                                            normalize_digits)\n          tokens_file.write("" "".join([str(tok) for tok in token_ids]) + ""\\n"")\n\n\ndef prepare_chitchat_data(data_dir, vocabulary, vocabulary_size, tokenizer=None):\n  """"""Get WMT data into data_dir, create vocabularies and tokenize data.\n\n  Args:\n    data_dir: directory in which the data sets will be stored.\n    en_vocabulary_size: size of the English vocabulary to create and use.\n    fr_vocabulary_size: size of the French vocabulary to create and use.\n    tokenizer: a function to use to tokenize each data sentence;\n      if None, basic_tokenizer will be used.\n\n  Returns:\n    A tuple of 6 elements:\n      (1) path to the token-ids for English training data-set,\n      (2) path to the token-ids for French training data-set,\n      (3) path to the token-ids for English development data-set,\n      (4) path to the token-ids for French development data-set,\n      (5) path to the English vocabulary file,\n      (6) path to the French vocabulary file.\n  """"""\n  # Get wmt data to the specified directory.\n  #train_path = get_wmt_enfr_train_set(data_dir)\n  train_path = os.path.join(data_dir, ""chitchat.train"")\n  #dev_path = get_wmt_enfr_dev_set(data_dir)\n  dev_path = os.path.join(data_dir, ""chitchat.dev"")\n  # fixed_path = os.path.join(data_dir, ""chitchat.fixed"")\n  # weibo_path = os.path.join(data_dir, ""chitchat.weibo"")\n  # qa_path = os.path.join(data_dir, ""chitchat.qa"")\n\n  # voc_file_path = [train_path+"".answer"", fixed_path+"".answer"", weibo_path+"".answer"", qa_path+"".answer"",\n  #                    train_path+"".query"", fixed_path+"".query"", weibo_path+"".query"", qa_path+"".query""]\n  #voc_query_path = [train_path+"".query"", fixed_path+"".query"", weibo_path+"".query"", qa_path+"".query""]\n  # Create vocabularies of the appropriate sizes.\n  #vocab_path = os.path.join(data_dir, ""vocab%d.all"" % vocabulary_size)\n  #query_vocab_path = os.path.join(data_dir, ""vocab%d.query"" % en_vocabulary_size)\n\n  #create_vocabulary(vocab_path, voc_file_path, vocabulary_size)\n\n\n  #create_vocabulary(query_vocab_path, voc_query_path, en_vocabulary_size)\n\n  # Create token ids for the training data.\n  answer_train_ids_path = train_path + ("".ids%d.answer"" % vocabulary_size)\n  query_train_ids_path = train_path + ("".ids%d.query"" % vocabulary_size)\n  data_to_token_ids(train_path + "".answer"", answer_train_ids_path, vocabulary, tokenizer)\n  data_to_token_ids(train_path + "".query"", query_train_ids_path, vocabulary, tokenizer)\n\n  # Create token ids for the development data.\n  answer_dev_ids_path = dev_path + ("".ids%d.answer"" % vocabulary_size)\n  query_dev_ids_path = dev_path + ("".ids%d.query"" % vocabulary_size)\n  data_to_token_ids(dev_path + "".answer"", answer_dev_ids_path, vocabulary, tokenizer)\n  data_to_token_ids(dev_path + "".query"", query_dev_ids_path, vocabulary, tokenizer)\n\n  return (query_train_ids_path, answer_train_ids_path,\n          query_dev_ids_path, answer_dev_ids_path)\n\ndef prepare_defined_data(data_path, vocabulary, vocabulary_size, tokenizer=None):\n  #vocab_path = os.path.join(data_dir, ""vocab%d.all"" %vocabulary_size)\n  #query_vocab_path = os.path.join(data_dir, ""vocab%d.query"" %query_vocabulary_size)\n\n  answer_fixed_ids_path = data_path + ("".ids%d.answer"" % vocabulary_size)\n  query_fixed_ids_path = data_path + ("".ids%d.query"" % vocabulary_size)\n\n  data_to_token_ids(data_path + "".answer"", answer_fixed_ids_path, vocabulary, tokenizer)\n  data_to_token_ids(data_path + "".query"", query_fixed_ids_path, vocabulary, tokenizer)\n  return (query_fixed_ids_path, answer_fixed_ids_path)\n\ndef get_dummy_set(dummy_path, vocabulary, vocabulary_size, tokenizer=None):\n    dummy_ids_path = dummy_path + ("".ids%d"" % vocabulary_size)\n    data_to_token_ids(dummy_path, dummy_ids_path, vocabulary, tokenizer)\n    dummy_set = []\n    with gfile.GFile(dummy_ids_path, ""r"") as dummy_file:\n        line = dummy_file.readline()\n        counter = 0\n        while line:\n            counter += 1\n            dummy_set.append([int(x) for x in line.split()])\n            line = dummy_file.readline()\n    return dummy_set'"
grl_beam_decoder.py,96,"b'import tensorflow as tf\nfrom tensorflow.python.util import nest\n\n\ndef nest_map(func, nested):\n    if not nest.is_sequence(nested):\n        return func(nested)\n    flat = nest.flatten(nested)\n    return nest.pack_sequence_as(nested, list(map(func, flat)))\n\n\ndef sparse_boolean_mask(tensor, mask):\n    """"""\n    Creates a sparse tensor from masked elements of `tensor`\n\n    Inputs:\n      tensor: a 2-D tensor, [batch_size, T]\n      mask: a 2-D mask, [batch_size, T]\n\n    Output: a 2-D sparse tensor\n    """"""\n    mask_lens = tf.reduce_sum(tf.cast(mask, tf.int32), -1, keep_dims=True)\n    mask_shape = tf.shape(mask)\n    left_shifted_mask = tf.tile(\n        tf.expand_dims(tf.range(mask_shape[1]), 0),\n        [mask_shape[0], 1]\n    ) < mask_lens\n    return tf.SparseTensor(\n        indices=tf.where(left_shifted_mask),\n        values=tf.boolean_mask(tensor, mask),\n        shape=tf.cast(tf.pack([mask_shape[0], tf.reduce_max(mask_lens)]), tf.int64)  # For 2D only\n    )\n\n\ndef flat_batch_gather(flat_params, indices, validate_indices=None,\n                      batch_size=None,\n                      options_size=None):\n    """"""\n    Gather slices from `flat_params` according to `indices`, separately for each\n    example in a batch.\n\n    output[(b * indices_size + i), :, ..., :] = flat_params[(b * options_size + indices[b, i]), :, ..., :]\n\n    The arguments `batch_size` and `options_size`, if provided, are used instead\n    of looking up the shape from the inputs. This may help avoid redundant\n    computation (TODO: figure out if tensorflow\'s optimizer can do this automatically)\n\n    Args:\n      flat_params: A `Tensor`, [batch_size * options_size, ...]\n      indices: A `Tensor`, [batch_size, indices_size]\n      validate_indices: An optional `bool`. Defaults to `True`\n      batch_size: (optional) an integer or scalar tensor representing the batch size\n      options_size: (optional) an integer or scalar Tensor representing the number of options to choose from\n    """"""\n    if batch_size is None:\n        batch_size = indices.get_shape()[0].value\n        if batch_size is None:\n            batch_size = tf.shape(indices)[0]\n\n    if options_size is None:\n        options_size = flat_params.get_shape()[0].value\n        if options_size is None:\n            options_size = tf.shape(flat_params)[0] // batch_size\n        else:\n            options_size = options_size // batch_size\n\n    indices_offsets = tf.reshape(tf.range(batch_size) * options_size, [-1] + [1] * (len(indices.get_shape()) - 1))\n    indices_into_flat = indices + tf.cast(indices_offsets, indices.dtype)\n    flat_indices_into_flat = tf.reshape(indices_into_flat, [-1])\n\n    return tf.gather(flat_params, flat_indices_into_flat, validate_indices=validate_indices)\n\n\ndef batch_gather(params, indices, validate_indices=None,\n                 batch_size=None,\n                 options_size=None):\n    """"""\n    Gather slices from `params` according to `indices`, separately for each\n    example in a batch.\n\n    output[b, i, ..., j, :, ..., :] = params[b, indices[b, i, ..., j], :, ..., :]\n\n    The arguments `batch_size` and `options_size`, if provided, are used instead\n    of looking up the shape from the inputs. This may help avoid redundant\n    computation (TODO: figure out if tensorflow\'s optimizer can do this automatically)\n\n    Args:\n      params: A `Tensor`, [batch_size, options_size, ...]\n      indices: A `Tensor`, [batch_size, ...]\n      validate_indices: An optional `bool`. Defaults to `True`\n      batch_size: (optional) an integer or scalar tensor representing the batch size\n      options_size: (optional) an integer or scalar Tensor representing the number of options to choose from\n    """"""\n    if batch_size is None:\n        batch_size = params.get_shape()[0].merge_with(indices.get_shape()[0]).value\n        if batch_size is None:\n            batch_size = tf.shape(indices)[0]\n\n    if options_size is None:\n        options_size = params.get_shape()[1].value\n        if options_size is None:\n            options_size = tf.shape(params)[1]\n\n    batch_size_times_options_size = batch_size * options_size\n\n    # has no gradients implemented.\n    flat_params = tf.reshape(params, tf.concat(0, [[batch_size_times_options_size], tf.shape(params)[2:]]))\n\n    indices_offsets = tf.reshape(tf.range(batch_size) * options_size, [-1] + [1] * (len(indices.get_shape()) - 1))\n    indices_into_flat = indices + tf.cast(indices_offsets, indices.dtype)\n\n    return tf.gather(flat_params, indices_into_flat,validate_indices=validate_indices)\n\n\nclass BeamFlattenWrapper(tf.nn.rnn_cell.RNNCell):\n    def __init__(self, cell, beam_size):\n        self.cell = cell\n        self.beam_size = beam_size\n\n    @staticmethod\n    def merge_batch_beam(tensor):\n        remaining_shape = tf.shape(tensor)[2:]\n        res = tf.reshape(tensor, tf.concat(0, [[-1], remaining_shape]))\n        res.set_shape(tf.TensorShape((None,)).concatenate(tensor.get_shape()[2:]))\n        return res\n\n    def unmerge_batch_beam(self, tensor):\n        remaining_shape = tf.shape(tensor)[1:]\n        res = tf.reshape(tensor, tf.concat(0, [[-1, self.beam_size], remaining_shape]))\n        res.set_shape(tf.TensorShape((None, self.beam_size)).concatenate(tensor.get_shape()[1:]))\n        return res\n\n    def prepend_beam_size(self, element):\n        return tf.TensorShape(self.beam_size).concatenate(element)\n\n    def tile_along_beam(self, state):\n        if nest.is_sequence(state):\n            return nest_map(\n                lambda val: self.tile_along_beam(val),\n                state\n            )\n\n        if not isinstance(state, tf.Tensor):\n            raise ValueError(""State should be a sequence or tensor"")\n\n        tensor = state\n\n        tensor_shape = tensor.get_shape().with_rank_at_least(1)\n        new_tensor_shape = tensor_shape[:1].concatenate(self.beam_size).concatenate(tensor_shape[1:])\n\n        dynamic_tensor_shape = tf.unstack(tf.shape(tensor))\n        res = tf.expand_dims(tensor, 1)\n        res = tf.tile(res, [1, self.beam_size] + [1] * (tensor_shape.ndims - 1))\n        res = tf.reshape(res, [-1, self.beam_size] + list(dynamic_tensor_shape[1:]))\n        res.set_shape(new_tensor_shape)\n        return res\n\n    def __call__(self, inputs, state, scope=None):\n        flat_inputs = nest_map(self.merge_batch_beam, inputs)\n        flat_state = nest_map(self.merge_batch_beam, state)\n\n        flat_output, flat_next_state = self.cell(flat_inputs, flat_state, scope=scope)\n\n        output = nest_map(self.unmerge_batch_beam, flat_output)\n        next_state = nest_map(self.unmerge_batch_beam, flat_next_state)\n\n        return output, next_state\n\n    @property\n    def state_size(self):\n        return nest_map(self.prepend_beam_size, self.cell.state_size)\n\n    @property\n    def output_size(self):\n        return nest_map(self.prepend_beam_size, self.cell.output_size)\n\n\nclass BeamReplicateWrapper(tf.nn.rnn_cell.RNNCell):\n    def __init__(self, cell, beam_size):\n        self.cell = cell\n        self.beam_size = beam_size\n\n    def prepend_beam_size(self, element):\n        return tf.TensorShape(self.beam_size).concatenate(element)\n\n    def tile_along_beam(self, state):\n        if nest.is_sequence(state):\n            return nest_map(\n                lambda val: self.tile_along_beam(val),\n                state\n            )\n\n        if not isinstance(state, tf.Tensor):\n            raise ValueError(""State should be a sequence or tensor"")\n\n        tensor = state\n\n        tensor_shape = tensor.get_shape().with_rank_at_least(1)\n        new_tensor_shape = tensor_shape[:1].concatenate(self.beam_size).concatenate(tensor_shape[1:])\n\n        dynamic_tensor_shape = tf.unstack(tf.shape(tensor))\n        res = tf.expand_dims(tensor, 1)\n        res = tf.tile(res, [1, self.beam_size] + [1] * (tensor_shape.ndims - 1))\n        res = tf.reshape(res, [-1, self.beam_size] + list(dynamic_tensor_shape[1:]))\n        res.set_shape(new_tensor_shape)\n        return res\n\n    def __call__(self, inputs, state, scope=None):\n        varscope = scope or tf.get_variable_scope()\n\n        flat_inputs = nest.flatten(inputs)\n        flat_state = nest.flatten(state)\n\n        flat_inputs_unstacked = list(zip(*[tf.unstack(tensor, num=self.beam_size, axis=1) for tensor in flat_inputs]))\n        flat_state_unstacked = list(zip(*[tf.unstack(tensor, num=self.beam_size, axis=1) for tensor in flat_state]))\n\n        flat_output_unstacked = []\n        flat_next_state_unstacked = []\n        output_sample = None\n        next_state_sample = None\n\n        for i, (inputs_k, state_k) in enumerate(zip(flat_inputs_unstacked, flat_state_unstacked)):\n            inputs_k = nest.pack_sequence_as(inputs, inputs_k)\n            state_k = nest.pack_sequence_as(state, state_k)\n\n            if i == 0:\n                output_k, next_state_k = self.cell(inputs_k, state_k, scope=scope)\n            else:\n                with tf.variable_scope(varscope, reuse=True):\n                    output_k, next_state_k = self.cell(inputs_k, state_k, scope=varscope if scope is not None else None)\n\n            flat_output_unstacked.append(nest.flatten(output_k))\n            flat_next_state_unstacked.append(nest.flatten(next_state_k))\n\n            output_sample = output_k\n            next_state_sample = next_state_k\n\n        flat_output = [tf.stack(tensors, axis=1) for tensors in zip(*flat_output_unstacked)]\n        flat_next_state = [tf.stack(tensors, axis=1) for tensors in zip(*flat_next_state_unstacked)]\n\n        output = nest.pack_sequence_as(output_sample, flat_output)\n        next_state = nest.pack_sequence_as(next_state_sample, flat_next_state)\n\n        return output, next_state\n\n    @property\n    def state_size(self):\n        return nest_map(self.prepend_beam_size, self.cell.state_size)\n\n    @property\n    def output_size(self):\n        return nest_map(self.prepend_beam_size, self.cell.output_size)\n\n\nclass BeamSearchHelper(object):\n    INVALID_SCORE = -1e18  # top_k doesn\'t handle -inf well\n\n    def __init__(self, cell, beam_size, stop_token, initial_state, initial_input,\n                 max_len=200,\n                 output_projection=None,\n                 outputs_to_score_fn=None,\n                 tokens_to_inputs_fn=None,\n                 cell_transform=\'default\',\n                 scope=None):\n\n        self.beam_size = beam_size\n        self.stop_token = stop_token\n        self.max_len = max_len\n        self.scope = scope\n        self.output_projection = output_projection\n\n        if cell_transform == \'default\':\n            if type(cell) in [tf.nn.rnn_cell.LSTMCell,\n                              tf.nn.rnn_cell.GRUCell,\n                              tf.nn.rnn_cell.BasicLSTMCell,\n                              tf.nn.rnn_cell.BasicRNNCell]:\n                cell_transform = \'flatten\'\n            else:\n                cell_transform = \'replicate\'\n\n        if cell_transform == \'none\':\n            self.cell = cell\n            self.initial_state = initial_state\n            self.initial_input = initial_input\n        elif cell_transform == \'flatten\':\n            self.cell = BeamFlattenWrapper(cell, self.beam_size)\n            self.initial_state = self.cell.tile_along_beam(initial_state)\n            self.initial_input = self.cell.tile_along_beam(initial_input)\n        elif cell_transform == \'replicate\':\n            self.cell = BeamReplicateWrapper(cell, self.beam_size)\n            self.initial_state = self.cell.tile_along_beam(initial_state)\n            self.initial_input = self.cell.tile_along_beam(initial_input)\n        else:\n            raise ValueError(""cell_transform must be one of: \'default\', \'flatten\', \'replicate\', \'none\'"")\n\n        self._cell_transform_used = cell_transform\n\n        if outputs_to_score_fn is not None:\n            self.outputs_to_score_fn = outputs_to_score_fn\n        if tokens_to_inputs_fn is not None:\n            self.tokens_to_inputs_fn = tokens_to_inputs_fn\n\n        batch_size = tf.Dimension(None)\n        if not nest.is_sequence(self.initial_state):\n            batch_size = batch_size.merge_with(self.initial_state.get_shape()[0])\n        else:\n            for tensor in nest.flatten(self.initial_state):\n                batch_size = batch_size.merge_with(tensor.get_shape()[0])\n\n        if not nest.is_sequence(self.initial_input):\n            batch_size = batch_size.merge_with(self.initial_input.get_shape()[0])\n        else:\n            for tensor in nest.flatten(self.initial_input):\n                batch_size = batch_size.merge_with(tensor.get_shape()[0])\n\n        self.inferred_batch_size = batch_size.value\n        if self.inferred_batch_size is not None:\n            self.batch_size = self.inferred_batch_size\n        else:\n            if not nest.is_sequence(self.initial_state):\n                self.batch_size = tf.shape(self.initial_state)[0]\n            else:\n                self.batch_size = tf.shape(list(nest.flatten(self.initial_state))[0])[0]\n\n        self.inferred_batch_size_times_beam_size = None\n        if self.inferred_batch_size is not None:\n            self.inferred_batch_size_times_beam_size = self.inferred_batch_size * self.beam_size\n\n        self.batch_size_times_beam_size = self.batch_size * self.beam_size\n\n    @staticmethod\n    def outputs_to_score_fn(cell_output):\n        return tf.nn.log_softmax(cell_output)\n\n    @staticmethod\n    def tokens_to_inputs_fn(symbols):\n        return tf.expand_dims(symbols, -1)\n\n    def beam_setup(self, time):\n\n        emit_output = None\n        next_cell_state = self.initial_state\n        next_input = self.initial_input\n\n        # Set up the beam search tracking state\n        cand_symbols = tf.fill([self.batch_size_times_beam_size, 0], tf.constant(self.stop_token, dtype=tf.int32))\n        cand_logprobs = tf.ones((self.batch_size_times_beam_size,), dtype=tf.float32) * -float(\'inf\')\n\n        first_in_beam_mask = tf.equal(tf.range(self.batch_size_times_beam_size) % self.beam_size, 0)\n\n        beam_symbols = tf.fill([self.batch_size_times_beam_size, 0], tf.constant(self.stop_token, dtype=tf.int32))\n        beam_logprobs = tf.select(\n            first_in_beam_mask,\n            tf.fill([self.batch_size_times_beam_size], 0.0),\n            tf.fill([self.batch_size_times_beam_size], self.INVALID_SCORE)\n        )\n\n        # Set up correct dimensions for maintaining loop invariants.\n        # Note that the last dimension (initialized to zero) is not a loop invariant,\n        # so we need to clear it.\n        # inference so that _shape is not necessary?\n        cand_symbols._shape = tf.TensorShape((self.inferred_batch_size_times_beam_size, None))\n        cand_logprobs._shape = tf.TensorShape((self.inferred_batch_size_times_beam_size,))\n        beam_symbols._shape = tf.TensorShape((self.inferred_batch_size_times_beam_size, None))\n        beam_logprobs._shape = tf.TensorShape((self.inferred_batch_size_times_beam_size,))\n\n        next_loop_state = (\n            cand_symbols,\n            cand_logprobs,\n            beam_symbols,\n            beam_logprobs,\n        )\n\n        emit_output = tf.zeros(self.cell.output_size)\n        elements_finished = tf.zeros([self.batch_size], dtype=tf.bool)\n\n        return elements_finished, next_input, next_cell_state, emit_output, next_loop_state\n\n    def beam_loop(self, time, cell_output, cell_state, loop_state):\n        (\n            past_cand_symbols,  # [batch_size*beam_size, time-1]\n            past_cand_logprobs,  # [batch_size*beam_size]\n            past_beam_symbols,  # [batch_size*beam_size, time-1], right-aligned\n            past_beam_logprobs,  # [batch_size*beam_size]\n        ) = loop_state\n\n        # We don\'t actually use this, but emit_output is required to match the\n        # cell output size specfication. Otherwise we would leave this as None.\n        emit_output = cell_output\n\n        cell_output = tf.unstack(cell_output, axis=1)\n\n        if self.output_projection is not None:\n            tmp = [tf.nn.xw_plus_b(output_k,\n                                   self.output_projection[0],\n                                   self.output_projection[1])\n                   for output_k in cell_output]\n\n        cell_output = tf.stack(tmp, axis=1)\n\n        num_classes = int(cell_output.get_shape()[-1])\n\n        # 1. Get scores for all candidate sequences\n        logprobs = self.outputs_to_score_fn(cell_output)\n\n        logprobs_batched = tf.reshape(logprobs + tf.expand_dims(tf.reshape(past_beam_logprobs, [self.batch_size, self.beam_size]), 2),\n\t\t                              [self.batch_size, self.beam_size * num_classes])\n\n        # 2. Determine which states to pass to next iteration\n        nondone_mask = tf.reshape(\n            tf.cast(tf.equal(tf.range(num_classes), self.stop_token), tf.float32) * self.INVALID_SCORE,\n            [1, 1, num_classes])\n\n        nondone_mask = tf.reshape(tf.tile(nondone_mask, [1, self.beam_size, 1]),\n                                  [-1, self.beam_size * num_classes])\n\n        beam_logprobs, indices = tf.nn.top_k(logprobs_batched + nondone_mask, self.beam_size)\n        beam_logprobs = tf.reshape(beam_logprobs, [-1])\n\n        # For continuing to the next symbols\n        symbols = indices % num_classes  # [batch_size, self.beam_size]\n        parent_refs = indices // num_classes  # [batch_size, self.beam_size]\n\n        symbols_history = flat_batch_gather(past_beam_symbols, parent_refs, batch_size=self.batch_size, options_size=self.beam_size)\n\n        beam_symbols = tf.concat(1, [symbols_history, tf.reshape(symbols, [-1, 1])])\n\n        # Handle the output and the cell state shuffling\n        next_cell_state = nest_map(\n            lambda element: batch_gather(element, parent_refs, batch_size=self.batch_size, options_size=self.beam_size),\n            cell_state\n        )\n\n        next_input = self.tokens_to_inputs_fn(tf.reshape(symbols, [-1, self.beam_size]))\n\n        # 3. Update the candidate pool to include entries that just ended with a stop token\n        logprobs_done = tf.reshape(logprobs_batched, [-1, self.beam_size, num_classes])[:, :, self.stop_token]\n        done_parent_refs = tf.argmax(logprobs_done, 1)\n        done_symbols = flat_batch_gather(past_beam_symbols, done_parent_refs, batch_size=self.batch_size, options_size=self.beam_size)\n\n        done_symbols = tf.tile(done_symbols, [self.beam_size, 1])\n\n        logprobs_done_max = tf.reshape(logprobs_done, [-1])\n\n        cand_symbols_unpadded = tf.select(logprobs_done_max > past_cand_logprobs,\n                                          done_symbols,\n                                          past_cand_symbols)\n        cand_logprobs = tf.maximum(logprobs_done_max, past_cand_logprobs)\n\n        cand_symbols = tf.concat(1, [cand_symbols_unpadded, tf.fill([self.batch_size_times_beam_size, 1], self.stop_token)])\n\n        # 4. Check the stopping criteria\n        # elements_finished = tf.reduce_max(tf.reshape(beam_logprobs, [-1, self.beam_size]), 1) < cand_logprobs\n        finished = beam_logprobs < cand_logprobs\n        elements_finished = tf.reduce_all(finished)\n        elements_finished = tf.reshape(elements_finished, [self.batch_size])\n        if self.max_len is not None:\n            elements_finished_clip = (time >= self.max_len)\n            elements_finished |= elements_finished_clip\n\n        # 5. Prepare return values\n        for tensor in list(nest.flatten(next_input)) + list(nest.flatten(next_cell_state)):\n            tensor.set_shape(tf.TensorShape((self.inferred_batch_size, self.beam_size)).concatenate(tensor.get_shape()[2:]))\n\n        for tensor in [cand_symbols, cand_logprobs, elements_finished]:\n            tensor.set_shape(tf.TensorShape((self.inferred_batch_size,)).concatenate(tensor.get_shape()[1:]))\n\n        for tensor in [beam_symbols, beam_logprobs]:\n            tensor.set_shape(tf.TensorShape((self.inferred_batch_size_times_beam_size,)).concatenate(tensor.get_shape()[1:]))\n\n        next_loop_state = (cand_symbols,\n                           cand_logprobs,\n                           beam_symbols,\n                           beam_logprobs,)\n\n        return elements_finished, next_input, next_cell_state, emit_output, next_loop_state\n\n    def loop_fn(self, time, cell_output, cell_state, loop_state):\n        if cell_output is None:\n            return self.beam_setup(time)\n        else:\n            return self.beam_loop(time, cell_output, cell_state, loop_state)\n\n    def decode_dense(self):\n        emit_ta, final_state, final_loop_state = tf.nn.raw_rnn(self.cell, self.loop_fn, scope=self.scope)\n\n        cand_symbols, cand_logprobs, beam_symbols, beam_logprobs = final_loop_state\n        return cand_symbols, cand_logprobs\n\n    def decode_sparse(self, include_stop_tokens=True):\n        dense_symbols, logprobs = self.decode_dense()\n        mask = tf.not_equal(dense_symbols, self.stop_token)\n        if include_stop_tokens:\n            mask = tf.concat(1, [tf.ones_like(mask[:, :1]), mask[:, :-1]])\n        return sparse_boolean_mask(dense_symbols, mask), logprobs\n\n\ndef beam_decoder(cell, beam_size, stop_token, initial_state, initial_input, tokens_to_inputs_fn,\n                 output_projection=None,\n                 outputs_to_score_fn=None,\n                 max_len=None,\n                 cell_transform=\'default\',\n                 output_dense=False,\n                 scope=None):\n    with tf.variable_scope(scope or ""rnn_decoder"") as varscope:\n        helper = BeamSearchHelper(cell=cell,\n                                  beam_size=beam_size,\n                                  stop_token=stop_token,\n                                  initial_state=initial_state,\n                                  initial_input=initial_input,\n                                  output_projection=output_projection,\n                                  tokens_to_inputs_fn=tokens_to_inputs_fn,\n                                  outputs_to_score_fn=outputs_to_score_fn,\n                                  max_len=max_len,\n                                  cell_transform=cell_transform,\n                                  scope=varscope)\n\n        if output_dense:\n            return helper.decode_dense()\n        else:\n            return helper.decode_sparse()\n'"
grl_rnn_model.py,35,"b'import tensorflow as tf\r\nfrom tensorflow.python.ops import variable_scope\r\nimport numpy as np\r\nimport grl_seq2seq as rl_seq2seq\r\nimport data_utils\r\nimport random\r\nfrom math import log\r\n\r\n\r\nclass grl_model(object):\r\n    def __init__(self, grl_config, name_scope, num_samples=512, forward = False, beam_search=False, dtype=tf.float32):\r\n        self.buckets = grl_config.buckets_concat\r\n        self.beam_size = grl_config.beam_size\r\n        self.emb_dim = grl_config.emb_dim\r\n        self.batch_size = grl_config.batch_size\r\n        self.vocab_size = grl_config.vocab_size\r\n        #self.learning_rate = grl_config.learning_rate\r\n        self.learning_rate = tf.Variable(float(grl_config.learning_rate), trainable=False)\r\n        self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * grl_config.learning_rate_decay_factor)\r\n        self.dummy_dialogs = []\r\n        max_gradient_norm = grl_config.max_gradient_norm\r\n        num_layers = grl_config.num_layers\r\n\r\n        with tf.name_scope(""GRL_Cell""):\r\n            single_cell = tf.nn.rnn_cell.GRUCell(self.emb_dim)\r\n            cells = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\r\n\r\n        self.global_step = tf.Variable(0, trainable=False)\r\n        self.encoder_inputs = []\r\n        self.decoder_inputs = []\r\n        self.target_weights = []\r\n        for i in xrange(self.buckets[-1][0]):\r\n            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=""encoder{0}"".format(i)))\r\n        for i in xrange(self.buckets[-1][1] + 1):\r\n            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=""deocder{0}"".format(i)))\r\n            self.target_weights.append(tf.placeholder(dtype, shape=[None], name=""weight{0}"".format(i)))\r\n        self.forward_only = tf.placeholder(tf.bool, name=""forward_only"")\r\n        #self.beam_search = tf.placeholder(tf.bool, name=""beam_search"")\r\n        self.rewards = [tf.placeholder(tf.float32, name=""reward{0}"".format(i)) for i in xrange(len(self.buckets))]\r\n\r\n        # the top of decoder_inputs is mark\r\n        targets = [self.decoder_inputs[i+1] for i in xrange(len(self.decoder_inputs) - 1)]\r\n\r\n        output_projection = None\r\n        softmax_loss_function = None\r\n        if num_samples > 0 and num_samples < self.vocab_size:\r\n            w_t = tf.get_variable(""proj_w"", [self.vocab_size, self.emb_dim], dtype=dtype)\r\n            w = tf.transpose(w_t)\r\n            b = tf.get_variable(""proj_b"", [self.vocab_size], dtype=dtype)\r\n            output_projection = (w, b)\r\n\r\n            def sampled_loss(inputs, labels):\r\n                labels = tf.reshape(labels, [-1, 1])\r\n                # We need to compute the sampled_softmax_loss using 32bit floats to\r\n                # avoid numerical instabilities.\r\n                local_w_t = tf.cast(w_t, tf.float32)\r\n                local_b = tf.cast(b, tf.float32)\r\n                local_inputs = tf.cast(inputs, tf.float32)\r\n                return tf.cast( tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\r\n                                                           num_samples, self.vocab_size), dtype)\r\n                #return tf.nn.sampled_softmax_loss(w_t, b, inputs, labels, num_samples, self.vocab_size)\r\n\r\n            softmax_loss_function = sampled_loss\r\n\r\n        with tf.name_scope(""GRL_Seq2Seq""):\r\n            def seq2seq_f(encoder_inputs, decoder_inputs, forward_only):\r\n                # return rl_seq2seq.embedding_attention_seq2seq(encoder_inputs=encoder_inputs,\r\n                #                                               decoder_inputs=decoder_inputs,\r\n                #                                               cell= cells,\r\n                #                                               num_encoder_symbols=self.vocab_size,\r\n                #                                               num_decoder_symbols=self.vocab_size,\r\n                #                                               embedding_size=self.emb_dim,\r\n                #                                               output_projection=output_projection,\r\n                #                                               feed_previous=forward_only,\r\n                #                                               beam_size=self.beam_size,\r\n                #                                               dtype=dtype)\r\n                # return rl_seq2seq.embedding_rnn_seq2seq(encoder_inputs=encoder_inputs,\r\n                #                                         decoder_inputs=decoder_inputs,\r\n                #                                         cell=cells,\r\n                #                                         num_encoder_symbols=self.vocab_size,\r\n                #                                         num_decoder_symbols=self.vocab_size,\r\n                #                                         embedding_size=self.emb_dim,\r\n                #                                         output_projection=output_projection,\r\n                #                                         feed_previous=forward_only,\r\n                #                                         beam_search=forward_only,\r\n                #                                         beam_size=self.beam_size,\r\n                #                                         dtype=dtype)\r\n                return rl_seq2seq.embedding_attention_seq2seq(encoder_inputs=encoder_inputs,\r\n                                                              decoder_inputs=decoder_inputs,\r\n                                                              cell=cells,\r\n                                                              num_encoder_symbols=self.vocab_size,\r\n                                                              num_decoder_symbols=self.vocab_size,\r\n                                                              embedding_size=self.emb_dim,\r\n                                                              output_projection=output_projection,\r\n                                                              feed_previous=forward_only,\r\n                                                              # beam_search=forward_only,\r\n                                                              beam_size=self.beam_size,\r\n                                                              dtype=dtype\r\n                                                              )\r\n\r\n        #if beam_search:\r\n            # self.outputs, self.losses, self.encoder_states = rl_seq2seq.decode_model_with_buckets(\r\n            #                                             encoder_inputs=self.encoder_inputs,\r\n            #                                             decoder_inputs=self.decoder_inputs, targets=targets,\r\n            #                                             weights=self.target_weights, buckets=self.buckets,\r\n            #                                             seq2seq=lambda x,y:seq2seq_f(x,y,False),\r\n            #                                             softmax_loss_function=softmax_loss_function)\r\n        # else:\r\n        # self.outputs, self.losses, self.encoder_states = rl_seq2seq.model_with_buckets(\r\n        #                                     self.encoder_inputs, self.decoder_inputs, targets,\r\n        #                                     self.target_weights, self.buckets,\r\n        #                                     # lambda x, y: seq2seq_f(x, y,False),\r\n        #                                     lambda x, y: seq2seq_f(x,y, tf.select(self.forward_only, True, False)),\r\n        #                                     softmax_loss_function=softmax_loss_function)\r\n\r\n        self.outputs, self.losses, self.encoder_states= rl_seq2seq.model_with_buckets(encoder_inputs=self.encoder_inputs,\r\n                                                               decoder_inputs=self.decoder_inputs,\r\n                                                               targets=targets,\r\n                                                               weights=self.target_weights,\r\n                                                               buckets=self.buckets,\r\n                                                               seq2seq=lambda x, y: seq2seq_f(x, y, tf.select(self.forward_only, True, False)),\r\n                                                               softmax_loss_function=softmax_loss_function)\r\n        #\r\n        #     for b in xrange(len(self.buckets)):\r\n        #         self.outputs[b] = [\r\n        #             tf.cond(\r\n        #                 self.forward_only,\r\n        #                 lambda: tf.matmul(output, output_projection[0]) + output_projection[1],\r\n        #                 lambda: output\r\n        #             )\r\n        #             for output in self.outputs[b]\r\n        #         ]\r\n\r\n            # if not beam_search:\r\n            #     self.outputs, self.losses, self.encoder_states = rl_seq2seq.model_with_buckets(self.encoder_inputs,\r\n            #                                         self.decoder_inputs, targets, self.target_weights, self.buckets,\r\n            #                                     #lambda x, y: seq2seq_f(x, y, tf.select(self.forward_only, True, False)),\r\n            #                                         lambda x, y: seq2seq_f(x,y, False),\r\n            #                                          softmax_loss_function=softmax_loss_function)\r\n            # else:\r\n            #     self.outputs, self.probs, self.encoder_states = rl_seq2seq.decode_model_with_buckets(\r\n            #         encoder_inputs=self.encoder_inputs, decoder_inputs=self.decoder_inputs, targets=targets,\r\n            #         weights=self.target_weights, buckets=self.buckets, seq2seq=lambda x,y:seq2seq_f(x,y,True),\r\n            #         softmax_loss_function=softmax_loss_function\r\n            #     )\r\n\r\n        if not forward:\r\n            with tf.name_scope(""GRL_Gradient""):\r\n                self.t_vars = [v for v in tf.trainable_variables() if name_scope in v.name]\r\n                self.gradient_norms = []\r\n                self.updatas = []\r\n\r\n                opt = tf.train.GradientDescentOptimizer(self.learning_rate)\r\n                for b in xrange(len(self.buckets)):\r\n                    adjusted_losses = tf.mul(self.losses[b], self.rewards[b])\r\n                    gradients = tf.gradients(adjusted_losses, self.t_vars)\r\n                    clips_gradient, norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\r\n                    self.gradient_norms.append(norm)\r\n                    gradient_ops = opt.apply_gradients(zip(clips_gradient, self.t_vars), global_step=self.global_step)\r\n                    self.updatas.append(gradient_ops)\r\n\r\n        all_variables = [k for k in tf.global_variables() if name_scope in k.name]\r\n        self.saver = tf.train.Saver(all_variables)\r\n\r\n    def step(self, session, encoder_inputs, decoder_inputs, target_weights, reward, bucket_id, forward_only, beam_search):\r\n        encoder_size, decoder_size = self.buckets[bucket_id]\r\n\r\n        input_feed = {self.forward_only.name: forward_only}\r\n        #input_feed = [self.beam_search.name] = beam_search\r\n        for i in xrange(encoder_size):\r\n            input_feed[self.encoder_inputs[i].name] = encoder_inputs[i]\r\n        for i in xrange(decoder_size):\r\n            input_feed[self.decoder_inputs[i].name] = decoder_inputs[i]\r\n            input_feed[self.target_weights[i].name] = target_weights[i]\r\n        for i in xrange(len(self.buckets)):\r\n            input_feed[self.rewards[i].name] = reward\r\n\r\n        last_target = self.decoder_inputs[decoder_size].name\r\n        input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\r\n\r\n        if not forward_only:\r\n            output_feed = [self.updatas[bucket_id],\r\n                           self.gradient_norms[bucket_id],\r\n                           self.losses[bucket_id]]\r\n            updata, norm, loss = session.run(output_feed, input_feed)\r\n            return updata, norm, loss\r\n        else:\r\n            # output_feed = [self.probs[bucket_id], self.encoder_states[bucket_id]]\r\n            # for i in xrange(decoder_size):\r\n            #     output_feed.append(self.outputs[bucket_id][i])\r\n\r\n            output_feed = [self.probs[bucket_id], self.encoder_states[bucket_id], self.outputs[bucket_id]]\r\n\r\n            outputs = session.run(output_feed, input_feed)  # loss, states, logits\r\n            return outputs[0], outputs[1], outputs[2:]\r\n\r\n    def step_rl(self, session, st_model, cc_model, bk_model, encoder_inputs, decoder_inputs, target_weights,\r\n                batch_source_encoder, bucket_id):\r\n        init_inputs = [encoder_inputs, decoder_inputs, target_weights, bucket_id]\r\n\r\n        batch_mask = [1 for _ in xrange(self.batch_size)]\r\n\r\n        ep_rewards, ep_step_loss, enc_states = [], [], []\r\n        ep_encoder_inputs, ep_target_weights, ep_bucket_id = [], [], []\r\n        episode, dialog = 0, []\r\n\r\n        while True:\r\n            ep_encoder_inputs.append(batch_source_encoder)\r\n            step_loss, encoder_states, output_logits = self.step(session, encoder_inputs, decoder_inputs, target_weights,\r\n                                                                 reward=1, bucket_id=bucket_id, forward_only=True)\r\n            ep_target_weights.append(target_weights)\r\n            ep_bucket_id.append(bucket_id)\r\n            ep_step_loss.append(step_loss)\r\n\r\n            state_tran = np.transpose(encoder_states, axes=(1, 0, 2))\r\n            print(""state_tran: "", np.shape(state_tran))\r\n            state_vec = np.reshape(state_tran, (self.batch_size, -1))\r\n            print(""state_vec: "", np.shape(state_vec))\r\n            enc_states.append(state_vec)\r\n\r\n            resp_tokens = self.remove_type(output_logits, self.buckets[bucket_id], type=1)\r\n\r\n            try:\r\n                encoder_trans = np.transpose(ep_encoder_inputs, axes=(1, 0))\r\n            except ValueError:\r\n                encoder_trans = np.transpose(ep_encoder_inputs, axes=(1, 0, 2))\r\n            print (""[encoder_trans] shape: "", np.shape(encoder_trans))\r\n\r\n            for i, (resp, ep_encoder) in enumerate(zip(resp_tokens, encoder_trans)):\r\n                if (len(resp) <= 3) or (resp in self.dummy_dialogs) or (resp in ep_encoder.tolist()):\r\n                    batch_mask[i] = 0\r\n                    print(""make mask index: %d, batch_mask: %s"" % (i, batch_mask))\r\n            if sum(batch_mask) == 0 or episode > 9:\r\n                break\r\n\r\n            # ----[Reward]----------------------------------------\r\n            # r1: Ease of answering\r\n            r1 = [self.logProb(session, st_model, self.buckets, resp_tokens, [d for _ in resp_tokens],\r\n                               mask=batch_mask) for d in self.dummy_dialogs]\r\n            print(""r1: final value: "", r1)\r\n            r1 = -np.mean(r1) if r1 else 0\r\n\r\n            # r2: Information Flow\r\n            r2_list = []\r\n            if len(enc_states) < 4:\r\n                r2 = 0\r\n            else:\r\n                batch_vec_a, batch_vec_b = enc_states[-3], enc_states[-1]\r\n                for i, (vec_a, vec_b) in enumerate(zip(batch_vec_a, batch_vec_b)):\r\n                    if batch_mask[i] == 0: continue\r\n                    rr2 = sum(vec_a * vec_b) / sum(abs(vec_a) * abs(vec_b))\r\n                    # print(""vec_a*vec_b: %s"" %sum(vec_a*vec_b))\r\n                    # print(""r2: %s"" %r2)\r\n                    if (rr2 < 0):\r\n                        print(""rr2: "", rr2)\r\n                        print(""vec_a: "", vec_a)\r\n                        print(""vec_b: "", vec_b)\r\n                        rr2 = -rr2\r\n                    else:\r\n                        rr2 = -log(rr2)\r\n                    r2_list.append(rr2)\r\n                r2 = sum(r2_list) / len(r2_list)\r\n\r\n            # r3: Semantic Coherence\r\n            print(""r3: Semantic Coherence"")\r\n            if len(ep_encoder_inputs) < 4:\r\n                r3 = 0\r\n            else:\r\n                pi = ep_encoder_inputs[-3]\r\n                qi = ep_encoder_inputs[-2]\r\n                answer = ep_encoder_inputs[-1]\r\n                query = np.column_stack((pi, qi))\r\n                r3_1 = self.logProb(session, cc_model, self.buckets, query, answer, mask=batch_mask)\r\n                r3_2 = self.logProb(session, bk_model, self.buckets, answer, qi, mask=batch_mask)\r\n                print(""r3_1: "", r3_1)\r\n                print(""r3_2: "", r3_2)\r\n                r3 = r3_1 + r3_2\r\n\r\n            # Episode total reward\r\n            print(""r1: %s, r2: %s, r3: %s"" % (r1, r2, r3))\r\n            R = 0.25 * r1 + 0.25 * r2 + 0.5 * r3\r\n            ep_rewards.append(R)\r\n            # ----------------------------------------------------\r\n            episode += 1\r\n\r\n            # prepare for next dialogue\r\n            bk_id = []\r\n            for i in range(len(resp_tokens)):\r\n                bk_id.append(min([b for b in range(len(self.buckets)) if self.buckets[b][0] >= len(resp_tokens[i])]))\r\n            bucket_id = max(bk_id)\r\n            feed_data = {bucket_id: [(resp_tokens, [])]}\r\n            encoder_inputs, decoder_inputs, target_weights, batch_source_encoder, _ = self.get_batch(feed_data,\r\n                                                                                                     bucket_id, type=2)\r\n\r\n        if len(ep_rewards) == 0:\r\n            print(""ep_rewards is zero"")\r\n            ep_rewards.append(1)\r\n\r\n        print(""[Step] final:"", episode, ep_rewards)\r\n        # gradient decent according to batch rewards\r\n        # rto = 0.0\r\n        # if (len(ep_step_loss) <= 1) or (len(ep_rewards) <= 1) or (max(ep_rewards) - min(ep_rewards) == 0):\r\n        #     rto = 0.0\r\n        # else:\r\n        #     rto = (max(ep_step_loss) - min(ep_step_loss)) / (max(ep_rewards) - min(ep_rewards))\r\n        # advantage = [np.mean(ep_rewards) * rto] * len(self.buckets)\r\n\r\n        reward = [np.mean(ep_rewards)] * len(self.buckets)\r\n        print(""advantage: %s"" % reward)\r\n        updata, norm, loss = self.step(session, init_inputs[0], init_inputs[1], init_inputs[2], bucket_id=init_inputs[3],\r\n                                    reward=reward, forward_only=False)\r\n\r\n        return updata, norm, loss\r\n\r\n\r\n    # log(P(|a)b), the conditional likelyhood\r\n    def logProb(self, session, model, buckets, tokens_a, tokens_b, mask=None):\r\n        def softmax(x):\r\n            return np.exp(x) / np.sum(np.exp(x), axis=0)\r\n\r\n        # prepare for next dialogue\r\n        # bucket_id = min([b for b in range(len(buckets)) if buckets[b][0] > len(tokens_a) and buckets[b][1] > len(tokens_b)])\r\n        # print(""tokens_a: %s"" %tokens_a)\r\n        print(""tokens_b: %s"" % tokens_b)\r\n\r\n        bk_id = []\r\n        for i in xrange(len(tokens_a)):\r\n            bk_id.append(min([b for b in xrange(len(buckets))\r\n                              if buckets[b][0] >= len(tokens_a[i]) and buckets[b][1] >= len(tokens_b[i])]))\r\n        bucket_id = max(bk_id)\r\n\r\n        print(""bucket_id: %s"" % bucket_id)\r\n\r\n        feed_data = {bucket_id: zip(tokens_a, tokens_b)}\r\n\r\n        # print(""logProb feed_back: %s"" %feed_data[bucket_id])\r\n        encoder_inputs, decoder_inputs, target_weights, _, _ = self.get_batch(feed_data, bucket_id, type=1)\r\n        # print(""logProb: encoder: %s; decoder: %s"" %(encoder_inputs, decoder_inputs))\r\n        # step\r\n        _, _, output_logits = model.step(session, encoder_inputs, decoder_inputs, target_weights,\r\n                                         bucket_id, forward_only=True, force_dec_input=True)\r\n\r\n        logits_t = np.transpose(output_logits, (1, 0, 2))\r\n        print(""logits_t shape: "", np.shape(logits_t))\r\n\r\n        sum_p = []\r\n        for i, (tokens, logits) in enumerate(zip(tokens_b, logits_t)):\r\n            print(""tokens: %s, index: %d"" % (tokens, i))\r\n            # print(""logits: %s"" %logits)\r\n\r\n            # if np.sum(tokens) == 0: break\r\n            if mask[i] == 0: continue\r\n            p = 1\r\n            for t, logit in zip(tokens, logits):\r\n                # print(""logProb: logit: %s"" %logit)\r\n                norm = softmax(logit)[t]\r\n                # print (""t: %s, norm: %s"" %(t, norm))\r\n                p *= norm\r\n            if p < 1e-100:\r\n                # print (""p: "", p)\r\n                p = 1e-100\r\n            p = log(p) / len(tokens)\r\n            print (""logProb: p: %s"" % (p))\r\n            sum_p.append(p)\r\n        re = np.sum(sum_p) / len(sum_p)\r\n        # print(""logProb: P: %s"" %(re))\r\n        return re\r\n\r\n    def remove_type(self, sequence, bucket,type=0):\r\n        tokens = []\r\n        resps = []\r\n        if type == 0:\r\n            tokens = [i for i in [t for t in reversed(sequence)] if i.sum() != 0]\r\n        elif type == 1:\r\n        #print (""remove_type type=1 tokens: %s"" %sequence)\r\n\r\n            for seq in sequence:\r\n                 #print(""seq: %s"" %seq)\r\n                 token = []\r\n                 for t in seq:\r\n                     #print(""seq_t: %s"" %t)\r\n                     # t = list(t)\r\n                     # print(""list(t): %s"" %t)\r\n                     # t = np.array(t)\r\n                     # print(""array(t): %s"" %t)\r\n                     token.append(int(np.argmax(t, axis=0)))\r\n                 tokens.append(token)\r\n\r\n        #tokens = [i for i in [int(np.argmax(t, axis=1)) for t in [seq for seq in sequence]]]\r\n        #tokens = [i for i in [int(t.index(max(t))) for t in [seq for seq in sequence]]]\r\n        else:\r\n            print (""type only 0(encoder_inputs) or 1(decoder_outputs)"")\r\n        #print(""remove_type tokens: %s"" %tokens)\r\n        tokens_t = []\r\n        for col in range(len(tokens[0])):\r\n            tokens_t.append([tokens[row][col] for row in range(len(tokens))])\r\n\r\n        for seq in tokens_t:\r\n            if data_utils.EOS_ID in seq:\r\n                resps.append(seq[:seq.index(data_utils.EOS_ID)][:bucket[1]])\r\n            else:\r\n                resps.append(seq[:bucket[1]])\r\n        return resps\r\n\r\n    def get_batch(self, train_data, bucket_id, type=0):\r\n\r\n        encoder_size, decoder_size = self.buckets[bucket_id]\r\n        encoder_inputs, decoder_inputs = [], []\r\n\r\n        # print(""Batch_Size: %s"" %self.batch_size)\r\n        # Get a random batch of encoder and decoder inputs from data,\r\n        # pad them if needed, reverse encoder inputs and add GO to decoder.\r\n        batch_source_encoder, batch_source_decoder = [], []\r\n        # print(""bucket_id: %s"" %bucket_id)\r\n        for batch_i in xrange(self.batch_size):\r\n            if type == 1:\r\n                # feed_data = {bucket_id: zip(tokens_a, tokens_b)}\r\n                encoder_input, decoder_input = train_data[bucket_id][batch_i]\r\n            elif type == 2:\r\n                # feed_data = {bucket_id: [(resp_tokens, [])]}\r\n                encoder_input_a, decoder_input = train_data[bucket_id][0]\r\n                encoder_input = encoder_input_a[batch_i]\r\n            elif type == 0:\r\n                encoder_input, decoder_input = random.choice(train_data[bucket_id])\r\n                print(""train en: %s, de: %s"" % (encoder_input, decoder_input))\r\n\r\n            batch_source_encoder.append(encoder_input)\r\n            batch_source_decoder.append(decoder_input)\r\n            # Encoder inputs are padded and then reversed.\r\n            encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\r\n            encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\r\n\r\n            # Decoder inputs get an extra ""GO"" symbol, and are padded then.\r\n            decoder_pad_size = decoder_size - len(decoder_input) - 1\r\n            decoder_inputs.append([data_utils.GO_ID] + decoder_input +\r\n                                  [data_utils.PAD_ID] * decoder_pad_size)\r\n\r\n        # Now we create batch-major vectors from the data selected above.\r\n        batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\r\n\r\n        # Batch encoder inputs are just re-indexed encoder_inputs.\r\n        for length_idx in xrange(encoder_size):\r\n            batch_encoder_inputs.append(\r\n                np.array([encoder_inputs[batch_idx][length_idx]\r\n                          for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n        # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\r\n        for length_idx in xrange(decoder_size):\r\n            batch_decoder_inputs.append(\r\n                np.array([decoder_inputs[batch_idx][length_idx]\r\n                          for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n            # Create target_weights to be 0 for targets that are padding.\r\n            batch_weight = np.ones(self.batch_size, dtype=np.float32)\r\n            for batch_idx in xrange(self.batch_size):\r\n                # We set weight to 0 if the corresponding target is a PAD symbol.\r\n                # The corresponding target is decoder_input shifted by 1 forward.\r\n                if length_idx < decoder_size - 1:\r\n                    target = decoder_inputs[batch_idx][length_idx + 1]\r\n                if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\r\n                    batch_weight[batch_idx] = 0.0\r\n            batch_weights.append(batch_weight)\r\n\r\n        return batch_encoder_inputs, batch_decoder_inputs, batch_weights, batch_source_encoder, batch_source_decoder\r\n\r\n'"
grl_seq2seq.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# We disable pylint because we need python3 compatibility.\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nfrom six.moves import zip     # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom tensorflow.python import shape\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.util import nest\nfrom grl_beam_decoder import beam_decoder\n# TODO(ebrevdo): Remove once _linear is fully deprecated.\nlinear = rnn_cell._linear  # pylint: disable=protected-access\n\n\ndef _extract_argmax_and_embed(embedding, output_projection=None, update_embedding=True):\n  """"""Get a loop_function that extracts the previous symbol and embeds it.\n\n  Args:\n    embedding: embedding tensor for symbols.\n    output_projection: None or a pair (W, B). If provided, each fed previous\n      output will first be multiplied by W and added B.\n    update_embedding: Boolean; if False, the gradients will not propagate\n      through the embeddings.\n\n  Returns:\n    A loop function.\n  """"""\n  def loop_function(prev, _):\n    if output_projection is not None:\n      prev = nn_ops.xw_plus_b(\n          prev, output_projection[0], output_projection[1])\n    prev_symbol = math_ops.argmax(prev, 1)\n    # Note that gradients will not propagate through the second parameter of\n    # embedding_lookup.\n    emb_prev = embedding_ops.embedding_lookup(embedding, prev_symbol)\n    if not update_embedding:\n      emb_prev = array_ops.stop_gradient(emb_prev)\n    return emb_prev\n  return loop_function\n\n\ndef rnn_decoder(decoder_inputs, initial_state, cell, loop_function=None,\n                scope=None):\n  with variable_scope.variable_scope(scope or ""rnn_decoder""):\n    state = initial_state\n    outputs = []\n    prev = None\n    for i, inp in enumerate(decoder_inputs):\n      if loop_function is not None and prev is not None:\n        with variable_scope.variable_scope(""loop_function"", reuse=True):\n          inp = loop_function(prev, i)\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n      output, state = cell(inp, state)\n      outputs.append(output)\n      if loop_function is not None:\n        prev = output\n  return outputs, state\n\n\ndef beam_rnn_decoder(decoder_inputs, initial_state, cell, embedding,\n                     output_projection=None,\n                     beam_size=10,\n                     scope=None):\n    beams, probs = beam_decoder(cell, beam_size,\n                                stop_token=2,\n                                initial_state=initial_state,\n                                output_projection=output_projection,\n                                initial_input=decoder_inputs[0],\n                                tokens_to_inputs_fn=lambda tokens: embedding_ops.embedding_lookup(embedding, tokens),\n                                max_len=50,\n                                output_dense=True,\n                                scope=scope)\n    return beams, probs\n\n\ndef embedding_rnn_decoder(decoder_inputs, initial_state, cell, num_symbols,\n                          embedding_size,\n                          output_projection=None,\n                          feed_previous=False,\n                          update_embedding_for_previous=True,\n                          scope=None,\n                          beam_search=True,\n                          beam_size=10):\n\n  with variable_scope.variable_scope(scope or ""embedding_rnn_decoder"") as scope:\n    if output_projection is not None:\n      dtype = scope.dtype\n      proj_weights = ops.convert_to_tensor(output_projection[0], dtype=dtype)\n      proj_weights.get_shape().assert_is_compatible_with([None, num_symbols])\n      proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n      proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n    embedding = variable_scope.get_variable(""embedding"",\n                                            [num_symbols, embedding_size])\n\n    emb_inp = (embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs)\n    if beam_search:\n        return beam_rnn_decoder(emb_inp, initial_state, cell,\n                                    output_projection=output_projection,\n                                    embedding=embedding,\n                                    beam_size=beam_size)\n    else:\n        loop_function = _extract_argmax_and_embed(embedding, output_projection,\n                                                      update_embedding_for_previous) if feed_previous else None\n    \n    return rnn_decoder(emb_inp, initial_state, cell,\n                       loop_function=loop_function)\n\n\ndef embedding_rnn_seq2seq(encoder_inputs, decoder_inputs, cell, num_encoder_symbols,\n                          num_decoder_symbols, embedding_size,\n                          output_projection=None,\n                          feed_previous=False,\n                          dtype=None,\n                          scope=None,\n                          beam_search=True,\n                          beam_size=10):\n  with variable_scope.variable_scope(scope or ""embedding_rnn_seq2seq"") as scope:\n    if dtype is not None:\n      scope.set_dtype(dtype)\n    else:\n      dtype = scope.dtype\n\n    # Encoder.\n    encoder_cell = rnn_cell.EmbeddingWrapper(\n        cell, embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n\n    # Decoder.\n    if output_projection is None:\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n\n    if isinstance(feed_previous, bool):\n      return embedding_rnn_decoder(decoder_inputs, encoder_state, cell, num_decoder_symbols, embedding_size,\n \t\t\t\t\t\t\t       output_projection=output_projection,\n\t \t\t\t\t\t           feed_previous=feed_previous,\n        \t\t\t\t\t\t   scope=scope,\n\t\t\t\t\t\t\t\t   beam_search=beam_search,\n\t\t\t\t\t\t\t\t   beam_size=beam_size)\n\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n    def decoder(feed_previous_bool):\n      reuse = None if feed_previous_bool else True\n      with variable_scope.variable_scope(\n          variable_scope.get_variable_scope(), reuse=reuse) as scope:\n        outputs, state = embedding_rnn_decoder(\n            decoder_inputs, encoder_state, cell, num_decoder_symbols,\n            embedding_size, output_projection=output_projection,\n            feed_previous=feed_previous_bool,\n            update_embedding_for_previous=False,\n\t\t\tbeam_search=beam_search,\n\t\t\tbeam_size=beam_size)\n        state_list = [state]\n        if nest.is_sequence(state):\n          state_list = nest.flatten(state)\n        return outputs + state_list\n\n    outputs_and_state = control_flow_ops.cond(feed_previous,\n                                              lambda: decoder(True),\n                                              lambda: decoder(False))\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n    state_list = outputs_and_state[outputs_len:]\n    state = state_list[0]\n    if nest.is_sequence(encoder_state):\n      state = nest.pack_sequence_as(structure=encoder_state,\n                                    flat_sequence=state_list)\n    return outputs_and_state[:outputs_len], state\n\n\n\n\n\ndef attention_decoder(decoder_inputs, initial_state, attention_states, cell,\n                      output_size=None,\n                      num_heads=1,\n                      loop_function=None,\n                      dtype=None,\n                      scope=None,\n                      initial_state_attention=False):\n\n  if not decoder_inputs:\n    raise ValueError(""Must provide at least 1 input to attention decoder."")\n  if num_heads < 1:\n    raise ValueError(""With less than 1 heads, use a non-attention decoder."")\n  if attention_states.get_shape()[2].value is None:\n    raise ValueError(""Shape[2] of attention_states must be known: %s""\n                     % attention_states.get_shape())\n  if output_size is None:\n    output_size = cell.output_size\n\n  with variable_scope.variable_scope(scope or ""attention_decoder"", dtype=dtype) as scope:\n    dtype = scope.dtype\n\n    batch_size = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n    attn_length = attention_states.get_shape()[1].value\n    if attn_length is None:\n      attn_length = shape(attention_states)[1]\n    attn_size = attention_states.get_shape()[2].value\n\n    # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n    hidden = array_ops.reshape(\n        attention_states, [-1, attn_length, 1, attn_size])\n    hidden_features = []\n    v = []\n    attention_vec_size = attn_size  # Size of query vectors for attention.\n    for a in xrange(num_heads):\n      k = variable_scope.get_variable(""AttnW_%d"" % a,\n                                      [1, 1, attn_size, attention_vec_size])\n      hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], ""SAME""))\n      v.append(variable_scope.get_variable(""AttnV_%d"" % a,\n\t  \t\t\t\t\t\t\t\t\t   [attention_vec_size]))\n\n    state = initial_state\n\n    def attention(query):\n      """"""Put attention masks on hidden using hidden_features and query.""""""\n      ds = []  # Results of attention reads will be stored here.\n      if nest.is_sequence(query):  # If the query is a tuple, flatten it.\n        query_list = nest.flatten(query)\n        for q in query_list:  # Check that ndims == 2 if specified.\n          ndims = q.get_shape().ndims\n          if ndims:\n            assert ndims == 2\n        query = array_ops.concat(1, query_list)\n      for a in xrange(num_heads):\n        with variable_scope.variable_scope(""Attention_%d"" % a):\n          y = linear(query, attention_vec_size, True)\n          y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n          # Attention mask is a softmax of v^T * tanh(...).\n          s = math_ops.reduce_sum(\n              v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n          a = nn_ops.softmax(s)\n          # Now calculate the attention-weighted vector d.\n          d = math_ops.reduce_sum(\n              array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n              [1, 2])\n          ds.append(array_ops.reshape(d, [-1, attn_size]))\n      return ds\n\n    outputs = []\n    prev = None\n    batch_attn_size = array_ops.pack([batch_size, attn_size])\n    attns = [array_ops.zeros(batch_attn_size, dtype=dtype)\n             for _ in xrange(num_heads)]\n    for a in attns:  # Ensure the second shape of attention vectors is set.\n      a.set_shape([None, attn_size])\n    if initial_state_attention:\n      attns = attention(initial_state)\n    for i, inp in enumerate(decoder_inputs):\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n      # If loop_function is set, we use it instead of decoder_inputs.\n      if loop_function is not None and prev is not None:\n        with variable_scope.variable_scope(""loop_function"", reuse=True):\n          inp = loop_function(prev, i)\n      # Merge input and previous attentions into one vector of the right size.\n      input_size = inp.get_shape().with_rank(2)[1]\n      if input_size.value is None:\n        raise ValueError(""Could not infer input size from input: %s"" % inp.name)\n      x = linear([inp] + attns, input_size, True)\n      # Run the RNN.\n      cell_output, state = cell(x, state)\n      # Run the attention mechanism.\n      if i == 0 and initial_state_attention:\n        with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                           reuse=True):\n          attns = attention(state)\n      else:\n        attns = attention(state)\n\n      with variable_scope.variable_scope(""AttnOutputProjection""):\n        output = linear([cell_output] + attns, output_size, True)\n      if loop_function is not None:\n        prev = output\n      outputs.append(output)\n\n  return outputs, state\n\n\ndef embedding_attention_decoder(decoder_inputs, initial_state, attention_states,\n                                cell, num_symbols, embedding_size, num_heads=1,\n                                output_size=None, output_projection=None,\n                                feed_previous=False,\n                                update_embedding_for_previous=True,\n                                dtype=None, scope=None,\n                                initial_state_attention=False, beam_search=True, beam_size=10):\n  if output_size is None:\n    output_size = cell.output_size\n  if output_projection is not None:\n    proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n    proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n  with variable_scope.variable_scope(scope or ""embedding_attention_decoder"", dtype=dtype) as scope:\n\n    embedding = variable_scope.get_variable(""embedding"",\n\t\t\t\t\t\t\t\t\t\t\t[num_symbols, embedding_size])\n    emb_inp = [embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs]\n    if beam_search:\n\n        return beam_rnn_decoder(emb_inp, initial_state, cell,\n                                embedding=embedding,\n                                output_projection=output_projection,\n                                beam_size=beam_size,\n                                scope=scope)\n    else:\n        loop_function = _extract_argmax_and_embed(embedding, output_projection,\n                                                  update_embedding_for_previous) if feed_previous else None\n    \n    return attention_decoder(emb_inp, initial_state, attention_states, cell,\n        \t\t\t\t\toutput_size=output_size,\n      \t\t\t\t\t\tnum_heads=num_heads,\n        \t\t\t\t\tloop_function=loop_function,\n        \t\t\t\t\tinitial_state_attention=initial_state_attention,\n        \t\t\t\t\tscope=scope)\n\n\ndef embedding_attention_seq2seq(encoder_inputs, decoder_inputs, cell,\n                                num_encoder_symbols, num_decoder_symbols, embedding_size,\n                                num_heads=1,\n                                output_projection=None,\n                                feed_previous=False,\n                                dtype=None,\n                                scope=None,\n                                initial_state_attention=False,\n\t\t\t\t\t\t\t\tbeam_search=True,\n                                beam_size=10):\n\n  with variable_scope.variable_scope(scope or ""embedding_attention_seq2seq"", dtype=dtype) as scope:\n    dtype = scope.dtype\n    # Encoder.\n    encoder_cell = rnn_cell.EmbeddingWrapper(cell, \n\t\t\t\t\t\t\t\t\t\t\t embedding_classes=num_encoder_symbols,\n        \t\t\t\t\t\t\t\t\t embedding_size=embedding_size)\n    encoder_outputs, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n\n    # First calculate a concatenation of encoder outputs to put attention on.\n    top_states = [array_ops.reshape(e, [-1, 1, cell.output_size])\n                  for e in encoder_outputs]\n    attention_states = array_ops.concat(1, top_states)\n\n    # Decoder.\n    output_size = None\n    if output_projection is None:\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n      output_size = num_decoder_symbols\n\n    if isinstance(feed_previous, bool):\n      outputs, state = embedding_attention_decoder(decoder_inputs, encoder_state, attention_states,\n        \t\t\t\t\t\t\t\t\t\t   cell, num_decoder_symbols, embedding_size,\n   \t\t\t\t\t\t\t\t\t\t\t       num_heads=num_heads,\n      \t\t\t\t\t\t\t\t\t\t\t   output_size=output_size,\n      \t\t\t\t\t\t\t\t\t\t\t   output_projection=output_projection,\n      \t\t\t\t\t\t\t\t\t\t\t   feed_previous=feed_previous,\n       \t\t\t\t\t\t\t\t\t\t\t   initial_state_attention=initial_state_attention,\n       \t\t\t\t\t\t\t\t\t\t\t   scope=scope,\n\t\t\t\t\t\t\t\t\t\t\t\t   beam_search=beam_search,\n                                           \t\t   beam_size=beam_size)\n      return outputs, state, encoder_state\n\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n    def decoder(feed_previous_bool):\n      reuse = None if feed_previous_bool else True\n      with variable_scope.variable_scope(\n          variable_scope.get_variable_scope(), reuse=reuse) as scope:\n        outputs, state = embedding_attention_decoder(\n            decoder_inputs,\n            encoder_state,\n            attention_states,\n            cell,\n            num_decoder_symbols,\n            embedding_size,\n            num_heads=num_heads,\n            output_size=output_size,\n            output_projection=output_projection,\n            feed_previous=feed_previous_bool,\n            update_embedding_for_previous=False,\n            initial_state_attention=initial_state_attention,\n            scope=scope,\n\t\t\tbeam_search=beam_search,\n            beam_size=beam_size)\n        state_list = [state]\n        if nest.is_sequence(state):\n          state_list = nest.flatten(state)\n        return outputs + state_list\n\n    outputs_and_state = control_flow_ops.cond(feed_previous,\n                                              lambda: decoder(True),\n                                              lambda: decoder(False))\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n    state_list = outputs_and_state[outputs_len:]\n    state = state_list[0]\n    if nest.is_sequence(encoder_state):\n      state = nest.pack_sequence_as(structure=encoder_state,\n                                    flat_sequence=state_list)\n    return outputs_and_state[:outputs_len], state, encoder_state\n\n\ndef sequence_loss_by_example(logits, targets, weights,\n                             average_across_timesteps=True,\n                             softmax_loss_function=None,\n\t\t\t\t\t\t\t name=None):\n  if len(targets) != len(logits) or len(weights) != len(logits):\n    raise ValueError(""Lengths of logits, weights, and targets must be the same ""\n                     ""%d, %d, %d."" % (len(logits), len(weights), len(targets)))\n  with ops.name_scope(name, ""sequence_loss_by_example"", logits + targets + weights):\n    log_perp_list = []\n    for logit, target, weight in zip(logits, targets, weights):\n      if softmax_loss_function is None:\n        # TODO(irving,ebrevdo): This reshape is needed because\n        # sequence_loss_by_example is called with scalars sometimes, which\n        # violates our general scalar strictness policy.\n        target = array_ops.reshape(target, [-1])\n        crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(\n            logit, target)\n      else:\n        crossent = softmax_loss_function(logit, target)\n      log_perp_list.append(crossent * weight)\n    log_perps = math_ops.add_n(log_perp_list)\n    if average_across_timesteps:\n      total_size = math_ops.add_n(weights)\n      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n      log_perps /= total_size\n  return log_perps\n\n\ndef sequence_loss(logits, targets, weights,\n                  average_across_timesteps=True,\n\t\t\t\t  average_across_batch=True,\n                  softmax_loss_function=None,\n\t\t\t\t  name=None):\n  with ops.name_scope(name, ""sequence_loss"", logits + targets + weights):\n    cost = math_ops.reduce_sum(sequence_loss_by_example(logits, targets, weights,\n        average_across_timesteps=average_across_timesteps,\n        softmax_loss_function=softmax_loss_function))\n    if average_across_batch:\n      batch_size = array_ops.shape(targets[0])[0]\n      return cost / math_ops.cast(batch_size, cost.dtype)\n    else:\n      return cost\n\n\ndef model_with_buckets(encoder_inputs, decoder_inputs, targets, weights, buckets, seq2seq,\n\t\t\t\t\t\tsoftmax_loss_function=None,\n                       per_example_loss=False, name=None):\n  if len(encoder_inputs) < buckets[-1][0]:\n    raise ValueError(""Length of encoder_inputs (%d) must be at least that of la""\n                     ""st bucket (%d)."" % (len(encoder_inputs), buckets[-1][0]))\n  if len(targets) < buckets[-1][1]:\n    raise ValueError(""Length of targets (%d) must be at least that of last""\n                     ""bucket (%d)."" % (len(targets), buckets[-1][1]))\n  if len(weights) < buckets[-1][1]:\n    raise ValueError(""Length of weights (%d) must be at least that of last""\n                     ""bucket (%d)."" % (len(weights), buckets[-1][1]))\n\n  all_inputs = encoder_inputs + decoder_inputs + targets + weights\n  losses = []\n  outputs = []\n  encoder_states = []\n  with ops.name_scope(name, ""model_with_buckets"", all_inputs):\n    for j, bucket in enumerate(buckets):\n      with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                         reuse=True if j > 0 else None):\n        bucket_outputs, decoder_states, encoder_state = seq2seq(encoder_inputs[:bucket[0]],\n                                    decoder_inputs[:bucket[1]])\n        outputs.append(bucket_outputs)\n        #print(""bucket outputs: %s"" %bucket_outputs)\n        encoder_states.append(encoder_state)\n        if per_example_loss:\n          losses.append(sequence_loss_by_example(outputs[-1], targets[:bucket[1]], weights[:bucket[1]],\n              \t\t\t\t\t\t\t\t\tsoftmax_loss_function=softmax_loss_function))\n        else:\n          losses.append(sequence_loss(outputs[-1], targets[:bucket[1]], weights[:bucket[1]],\n          \t\t\t\t\t\t\t  softmax_loss_function=softmax_loss_function))\n\n  return outputs, losses, encoder_states\ndef decode_model_with_buckets(encoder_inputs, decoder_inputs, targets, weights, buckets, seq2seq,\n                              softmax_loss_function=None,\n                              per_example_loss=False,\n                              name=None):\n    """"""Create a sequence-to-sequence models with support for bucketing.\n\n    The seq2seq argument is a function that defines a sequence-to-sequence models,\n    e.g., seq2seq = lambda x, y: basic_rnn_seq2seq(x, y, rnn_cell.GRUCell(24))\n\n    Args:\n      encoder_inputs: A list of Tensors to feed the encoder; first seq2seq input.\n      decoder_inputs: A list of Tensors to feed the decoder; second seq2seq input.\n      targets: A list of 1D batch-sized int32 Tensors (desired output sequence).\n      weights: List of 1D batch-sized float-Tensors to weight the targets.\n      buckets: A list of pairs of (input size, output size) for each bucket.\n      seq2seq: A sequence-to-sequence models function; it takes 2 input that\n        agree with encoder_inputs and decoder_inputs, and returns a pair\n        consisting of outputs and states (as, e.g., basic_rnn_seq2seq).\n      softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n        to be used instead of the standard softmax (the default if this is None).\n      per_example_loss: Boolean. If set, the returned loss will be a batch-sized\n        tensor of losses for each sequence in the batch. If unset, it will be\n        a scalar with the averaged loss from all examples.\n      name: Optional name for this operation, defaults to ""model_with_buckets"".\n\n    Returns:\n      A tuple of the form (outputs, losses), where:\n        outputs: The outputs for each bucket. Its j\'th element consists of a list\n          of 2D Tensors of shape [batch_size x num_decoder_symbols] (jth outputs).\n        losses: List of scalar Tensors, representing losses for each bucket, or,\n          if per_example_loss is set, a list of 1D batch-sized float Tensors.\n\n    Raises:\n      ValueError: If length of encoder_inputsut, targets, or weights is smaller\n        than the largest (last) bucket.\n    """"""\n    if len(encoder_inputs) < buckets[-1][0]:\n        raise ValueError(""Length of encoder_inputs (%d) must be at least that of la""\n                         ""st bucket (%d)."" % (len(encoder_inputs), buckets[-1][0]))\n    if len(targets) < buckets[-1][1]:\n        raise ValueError(""Length of targets (%d) must be at least that of last""\n                         ""bucket (%d)."" % (len(targets), buckets[-1][1]))\n    if len(weights) < buckets[-1][1]:\n        raise ValueError(""Length of weights (%d) must be at least that of last""\n                         ""bucket (%d)."" % (len(weights), buckets[-1][1]))\n\n    all_inputs = encoder_inputs + decoder_inputs + targets + weights\n    states = []\n    outputs = []\n    with ops.name_scope(name, ""model_with_buckets"", all_inputs):\n        for j, bucket in enumerate(buckets):\n            with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True if j > 0 else None):\n                bucket_outputs, bucket_states = seq2seq(encoder_inputs[:bucket[0]],\n                                                        decoder_inputs[:bucket[1]])\n                states.append(bucket_states)\n                outputs.append(bucket_outputs)\n\n    return outputs, states\n'"
grl_train.py,26,"b'from __future__ import division\r\nfrom __future__ import print_function\r\n\r\n\r\nimport sys\r\nimport os\r\nimport time\r\nimport math\r\nimport tensorflow as tf\r\nfrom tensorflow.python.platform import gfile\r\nimport numpy as np\r\nimport gst_rnn_model\r\nimport grl_rnn_model\r\nimport data_utils\r\nimport conf\r\nimport pickle\r\nimport os.path\r\n\r\ngst_config = conf.GSTConfig\r\ngbk_config = conf.GBKConfig\r\ngrl_config = conf.GRLConfig\r\ngcc_config = conf.GCCConfig\r\npre_grl_config = conf.Pre_GRLConfig\r\n\r\ndef read_data(config, source_path, target_path, max_size=None):\r\n    data_set = [[] for _ in config.buckets]\r\n    with gfile.GFile(source_path, mode=""r"") as source_file:\r\n        with gfile.GFile(target_path, mode=""r"") as target_file:\r\n            source, target = source_file.readline(), target_file.readline()\r\n            counter = 0\r\n            while source and target and (not max_size or counter < max_size):\r\n                counter += 1\r\n                if counter % 100000 == 0:\r\n                    print(""reading data line %d"" % counter)\r\n                    sys.stdout.flush()\r\n                source_ids = [int(x) for x in source.strip().split()]\r\n                target_ids = [int(x) for x in target.strip().split()]\r\n                target_ids.append(data_utils.EOS_ID)\r\n                for bucket_id, (source_size, target_size) in enumerate(config.buckets):\r\n                    if len(source_ids) < source_size and len(target_ids) < target_size:\r\n                        data_set[bucket_id].append([source_ids, target_ids])\r\n                        break\r\n                source, target = source_file.readline(), target_file.readline()\r\n    return data_set\r\n\r\n\r\ndef prepare_data(config):\r\n    train_path = os.path.join(config.train_dir, ""chitchat.train"")\r\n    data_path_list = [train_path + "".answer"", train_path + "".query""]\r\n    vocab_path = os.path.join(config.train_dir, ""vocab%d.all"" % config.vocab_size)\r\n    data_utils.create_vocabulary(vocab_path, data_path_list, config.vocab_size)\r\n    vocab, rev_vocab = data_utils.initialize_vocabulary(vocab_path)\r\n    #\r\n    # if os.path.isfile(config.dev_set) and os.path.isfile(config.train_set):\r\n    #     dev_set_file = open(config.dev_set, ""rb"")\r\n    #     dev_set = pickle.load(dev_set_file)\r\n    #     dev_set_file.close()\r\n    #\r\n    #     train_set_file = open(config.train_set, ""rb"")\r\n    #     train_set = pickle.load(train_set_file)\r\n    #     train_set_file.close()\r\n    # else:\r\n    print(""Prepare Chitchat data in %s"" % config.train_dir)\r\n    train_query, train_answer, dev_query, dev_answer = data_utils.prepare_chitchat_data(\r\n        config.train_dir, vocab, config.vocab_size)\r\n\r\n    print(""Reading development and training data (limit: %d)."" % config.max_train_data_size)\r\n    dev_set = read_data(config, dev_query, dev_answer)\r\n    train_set = read_data(config, train_query, train_answer)\r\n\r\n        # dev_set_file = open(config.dev_set, ""wb"")\r\n        # pickle.dump(dev_set, dev_set_file)\r\n        # dev_set_file.close()\r\n        #\r\n        # train_set_file = open(config.train_set, ""wb"")\r\n        # pickle.dump(train_set, train_set_file)\r\n        # train_set_file.close()\r\n\r\n    return vocab, rev_vocab, dev_set, train_set\r\n\r\n\r\ndef create_st_model(session, st_config, forward_only, name_scope):\r\n    with tf.variable_scope(name_or_scope=name_scope):\r\n        st_model = gst_rnn_model.gst_model(gst_config=st_config, name_scope=name_scope, forward_only=forward_only)\r\n        ckpt = tf.train.get_checkpoint_state(os.path.join(st_config.train_dir, ""checkpoints""))\r\n        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\r\n            print(""Read %s model from %s"" % (name_scope, ckpt.model_checkpoint_path))\r\n            st_model.saver.restore(session, ckpt.model_checkpoint_path)\r\n        else:\r\n            print(""Creating %s model with fresh parameters"" % name_scope)\r\n            global_variables = [gv for gv in tf.global_variables() if name_scope in gv.name]\r\n            session.run(tf.variables_initializer(global_variables))\r\n            print(""Created %s model with fresh parameters"" % name_scope)\r\n        return st_model\r\n\r\n\r\ndef create_rl_model(session, rl_config, forward_only, name_scope):\r\n    with tf.variable_scope(name_or_scope=name_scope):\r\n        rl_model = grl_rnn_model.grl_model(grl_config=rl_config, name_scope=name_scope, forward=forward_only)\r\n        ckpt = tf.train.get_checkpoint_state(os.path.join(rl_config.train_dir, ""checkpoints""))\r\n        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\r\n            print(""Read %s model from %s"" % (name_scope, ckpt.model_checkpoint_path))\r\n            rl_model.saver.restore(session, ckpt.model_checkpoint_path)\r\n        else:\r\n            print(""Creating %s model with fresh parameters"" % name_scope)\r\n            global_variables = [gv for gv in tf.global_variables() if name_scope in gv.name]\r\n            session.run(tf.variables_initializer(global_variables))\r\n            print(""Created %s model with fresh parameters"" % name_scope)\r\n        return rl_model\r\n\r\n\r\ndef ce_standard_train(st_config):\r\n    vocab, rev_vocab, dev_set, train_set = prepare_data(st_config)\r\n    for b_set in train_set:\r\n        print(""b_set length: "", len(b_set))\r\n\r\n    with tf.Session() as sess:\r\n        print(""Creating %s %d layers of %d units"" %(st_config.name_model ,st_config.num_layers, st_config.emb_dim))\r\n        st_model = create_st_model(sess, st_config, False, st_config.name_model)\r\n\r\n        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(st_config.buckets))]\r\n        train_total_size = float(sum(train_bucket_sizes))\r\n        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\r\n                               for i in xrange(len(train_bucket_sizes))]\r\n\r\n        step_time, loss = 0.0, 0.0\r\n        current_step = 0\r\n        previous_losses = []\r\n        step_loss_summary = tf.Summary()\r\n        # merge = tf.merge_all_summaries()\r\n        st_writer = tf.summary.FileWriter(st_config.tensorboard_dir, sess.graph)\r\n\r\n        while True:\r\n            random_number_01 = np.random.random_sample()\r\n            bucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\r\n            print(""bucket_id: "", bucket_id)\r\n            # Get a batch and make a step.\r\n            start_time = time.time()\r\n            encoder_inputs, decoder_inputs, target_weights, batch_source_encoder, batch_source_decoder = \\\r\n                st_model.get_batch(train_set, bucket_id)\r\n            _, step_loss, _ = st_model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id,\r\n                                            forward_only=False)\r\n\r\n            step_time += (time.time() - start_time) / st_config.steps_per_checkpoint\r\n            loss += step_loss / st_config.steps_per_checkpoint\r\n            current_step += 1\r\n\r\n            # Once in a while, we save checkpoint, print statistics, and run evals.\r\n            if current_step % st_config.steps_per_checkpoint == 0:\r\n\r\n                bucket_value = step_loss_summary.value.add()\r\n                bucket_value.tag = st_config.name_loss\r\n                bucket_value.simple_value = float(loss)\r\n                st_writer.add_summary(step_loss_summary, int(sess.run(st_model.global_step)))\r\n\r\n                # Print statistics for the previous epoch.\r\n                perplexity = math.exp(loss) if loss < 300 else float(\'inf\')\r\n                print (""global step %d learning rate %.4f step-time %.2f perplexity ""\r\n                       ""%.2f"" % (st_model.global_step.eval(), st_model.learning_rate.eval(),\r\n                                 step_time, perplexity))\r\n                # Decrease learning rate if no improvement was seen over last 3 times.\r\n                if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\r\n                    sess.run(st_model.learning_rate_decay_op)\r\n                previous_losses.append(loss)\r\n                # Save checkpoint and zero timer and loss.\r\n                gen_ckpt_dir = os.path.abspath(os.path.join(st_config.train_dir, ""checkpoints""))\r\n                if not os.path.exists(gen_ckpt_dir):\r\n                    os.makedirs(gen_ckpt_dir)\r\n                checkpoint_path = os.path.join(gen_ckpt_dir, ""chitchat.model"")\r\n                st_model.saver.save(sess, checkpoint_path, global_step=st_model.global_step)\r\n                step_time, loss = 0.0, 0.0\r\n                # Run evals on development set and print their perplexity.\r\n                # for bucket_id in xrange(len(gen_config.buckets)):\r\n                #   encoder_inputs, decoder_inputs, target_weights = model.get_batch(\r\n                #       dev_set, bucket_id)\r\n                #   _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\r\n                #                                target_weights, bucket_id, True)\r\n                #   eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float(\'inf\')\r\n                #   print(""  eval: bucket %d perplexity %.2f"" % (bucket_id, eval_ppx))\r\n                sys.stdout.flush()\r\n\r\n\r\ndef pre_rl_train(rl_config):\r\n    vocab, rev_vocab, dev_set, train_set = prepare_data(rl_config)\r\n    for b_set in train_set:\r\n        print(""b_set length: "", len(b_set))\r\n\r\n    with tf.Session() as sess:\r\n        rl_model = create_rl_model(sess, rl_config=rl_config, forward_only=False, name_scope=rl_config.name_model)\r\n\r\n        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(rl_config.buckets))]\r\n        train_total_size = float(sum(train_bucket_sizes))\r\n        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\r\n                               for i in xrange(len(train_bucket_sizes))]\r\n        step_time, loss = 0.0, 0.0\r\n        current_step = 0\r\n        previous_losses = []\r\n        step_loss_summary = tf.Summary()\r\n        rl_writer = tf.summary.FileWriter(rl_config.tensorboard_dir, sess.graph)\r\n\r\n        while True:\r\n            random_number_01 = np.random.random_sample()\r\n            bucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\r\n\r\n            # Get a batch and make a step.\r\n            start_time = time.time()\r\n            encoder_inputs, decoder_inputs, target_weights, batch_source_encoder, _ = \\\r\n                rl_model.get_batch(train_set,bucket_id)\r\n\r\n            _, _, step_loss = rl_model.step(sess, encoder_inputs, decoder_inputs,target_weights,\r\n                                       reward=1, bucket_id=bucket_id, forward_only=False, beam_search=False)\r\n\r\n            step_time += (time.time() - start_time) / rl_config.steps_per_checkpoint\r\n            loss += step_loss / rl_config.steps_per_checkpoint\r\n            current_step += 1\r\n\r\n            if current_step % rl_config.steps_per_checkpoint == 0:\r\n\r\n                bucket_value = step_loss_summary.value.add()\r\n                bucket_value.tag = rl_config.pre_name_loss\r\n                bucket_value.simple_value = float(loss)\r\n                rl_writer.add_summary(step_loss_summary, int(sess.run(rl_model.global_step)))\r\n\r\n                # Print statistics for the previous epoch.\r\n                perplexity = math.exp(loss) if loss < 300 else float(\'inf\')\r\n                print (""global step %d learning rate %.4f step-time %.2f perplexity ""\r\n                       ""%.2f"" % (rl_model.global_step.eval(), rl_model.learning_rate.eval(),\r\n                                 step_time, perplexity))\r\n                # Decrease learning rate if no improvement was seen over last 3 times.\r\n                if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\r\n                    sess.run(rl_model.learning_rate_decay_op)\r\n                previous_losses.append(loss)\r\n                # Save checkpoint and zero timer and loss.\r\n                gen_ckpt_dir = os.path.abspath(os.path.join(rl_config.train_dir, ""checkpoints""))\r\n                if not os.path.exists(gen_ckpt_dir):\r\n                    os.makedirs(gen_ckpt_dir)\r\n                checkpoint_path = os.path.join(gen_ckpt_dir, ""chitchat.model"")\r\n                rl_model.saver.save(sess, checkpoint_path, global_step=rl_model.global_step)\r\n                step_time, loss = 0.0, 0.0\r\n                # Run evals on development set and print their perplexity.\r\n                # for bucket_id in xrange(len(gen_config.buckets)):\r\n                #   encoder_inputs, decoder_inputs, target_weights = model.get_batch(\r\n                #       dev_set, bucket_id)\r\n                #   _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\r\n                #                                target_weights, bucket_id, True)\r\n                #   eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float(\'inf\')\r\n                #   print(""  eval: bucket %d perplexity %.2f"" % (bucket_id, eval_ppx))\r\n                sys.stdout.flush()\r\n    pass\r\n\r\n\r\ndef train():\r\n    vocab, rev_vocab, dev_set, train_set = prepare_data(grl_config)\r\n    for b_set in train_set:\r\n        print(""b_set length: "", len(b_set))\r\n\r\n    with tf.Session() as sess:\r\n        st_model = create_st_model(sess, gst_config, True, gst_config.name_model)\r\n        bk_model = create_st_model(sess, gbk_config, True, gbk_config.name_model)\r\n        cc_model = create_st_model(sess, gcc_config, True, gcc_config.name_model)\r\n        rl_model = create_rl_model(sess, grl_config, False, grl_config.name_model)\r\n\r\n        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(grl_config.buckets))]\r\n        train_total_size = float(sum(train_bucket_sizes))\r\n        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\r\n                               for i in xrange(len(train_bucket_sizes))]\r\n\r\n        step_time, loss = 0.0, 0.0\r\n        current_step = 0\r\n        previous_losses = []\r\n        step_loss_summary = tf.Summary()\r\n        # merge = tf.merge_all_summaries()\r\n        rl_writer = tf.summary.FileWriter(grl_config.tensorboard_dir, sess.graph)\r\n        while True:\r\n            random_number_01 = np.random.random_sample()\r\n            bucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\r\n\r\n            # Get a batch and make a step.\r\n            start_time = time.time()\r\n            encoder_inputs, decoder_inputs, target_weights, batch_source_encoder, _ = \\\r\n                rl_model.get_batch(train_set,bucket_id)\r\n\r\n            updata, norm, step_loss = rl_model.step_rl(sess, st_model=st_model, bk_model=bk_model, encoder_inputs=encoder_inputs,\r\n                                               decoder_inputs=decoder_inputs, target_weights=target_weights,\r\n                                               batch_source_encoder=batch_source_encoder, bucket_id=bucket_id)\r\n\r\n            step_time += (time.time() - start_time) / grl_config.steps_per_checkpoint\r\n            loss += step_loss / grl_config.steps_per_checkpoint\r\n            current_step += 1\r\n\r\n            # Once in a while, we save checkpoint, print statistics, and run evals.\r\n            if current_step % grl_config.steps_per_checkpoint == 0:\r\n\r\n                bucket_value = step_loss_summary.value.add()\r\n                bucket_value.tag = grl_config.name_loss\r\n                bucket_value.simple_value = float(loss)\r\n                rl_writer.add_summary(step_loss_summary, int(sess.run(rl_model.global_step)))\r\n\r\n                # Print statistics for the previous epoch.\r\n                perplexity = math.exp(loss) if loss < 300 else float(\'inf\')\r\n                print (""global step %d learning rate %.4f step-time %.2f perplexity ""\r\n                       ""%.2f"" % (rl_model.global_step.eval(), rl_model.learning_rate.eval(),\r\n                                 step_time, perplexity))\r\n                # Decrease learning rate if no improvement was seen over last 3 times.\r\n                if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\r\n                    sess.run(rl_model.learning_rate_decay_op)\r\n                previous_losses.append(loss)\r\n                # Save checkpoint and zero timer and loss.\r\n                gen_ckpt_dir = os.path.abspath(os.path.join(grl_config.train_dir, ""checkpoints""))\r\n                if not os.path.exists(gen_ckpt_dir):\r\n                    os.makedirs(gen_ckpt_dir)\r\n                checkpoint_path = os.path.join(gen_ckpt_dir, ""chitchat.model"")\r\n                rl_model.saver.save(sess, checkpoint_path, global_step=rl_model.global_step)\r\n                step_time, loss = 0.0, 0.0\r\n                # Run evals on development set and print their perplexity.\r\n                # for bucket_id in xrange(len(gen_config.buckets)):\r\n                #   encoder_inputs, decoder_inputs, target_weights = model.get_batch(\r\n                #       dev_set, bucket_id)\r\n                #   _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\r\n                #                                target_weights, bucket_id, True)\r\n                #   eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float(\'inf\')\r\n                #   print(""  eval: bucket %d perplexity %.2f"" % (bucket_id, eval_ppx))\r\n                sys.stdout.flush()\r\n\r\n\r\ndef test_decoder(config):\r\n    train_path = os.path.join(config.train_dir, ""chitchat.train"")\r\n    data_path_list = [train_path + "".answer"", train_path + "".query""]\r\n    vocab_path = os.path.join(config.train_dir, ""vocab%d.all"" % config.vocab_size)\r\n    data_utils.create_vocabulary(vocab_path, data_path_list, config.vocab_size)\r\n    vocab, rev_vocab = data_utils.initialize_vocabulary(vocab_path)\r\n\r\n    with tf.Session() as sess:\r\n        if config.name_model in [gst_config.name_model, gcc_config.name_model, gbk_config.name_model]:\r\n            model = create_st_model(sess, config, forward_only=True, name_scope=config.name_model)\r\n\r\n        elif config.name_model in [grl_config.name_model, pre_grl_config.name_model]:\r\n            model = create_rl_model(sess, config, forward_only=True, name_scope=config.name_model)\r\n\r\n        model.batch_size = 1\r\n\r\n        sys.stdout.write(""> "")\r\n        sys.stdout.flush()\r\n        sentence = sys.stdin.readline()\r\n        while sentence:\r\n            token_ids = data_utils.sentence_to_token_ids(tf.compat.as_bytes(sentence), vocab)\r\n            print(""token_id: "", token_ids)\r\n            bucket_id = len(config.buckets) - 1\r\n            for i, bucket in enumerate(config.buckets):\r\n                if bucket[0] >= len(token_ids):\r\n                    bucket_id = i\r\n                    break\r\n            else:\r\n                print(""Sentence truncated: %s"", sentence)\r\n\r\n            encoder_inputs, decoder_inputs, target_weights, _, _ = model.get_batch({bucket_id: [(token_ids, [1])]},\r\n                                                                                   bucket_id)\r\n            # st_model step\r\n            if config.name_model in [gst_config.name_model, gcc_config.name_model, gbk_config.name_model]:\r\n                output_logits, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\r\n                outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\r\n                if data_utils.EOS_ID in outputs:\r\n                    outputs = outputs[:outputs.index(data_utils.EOS_ID)]\r\n                print("" "".join([str(rev_vocab[output]) for output in outputs]))\r\n\r\n            # beam_search step\r\n            elif config.name_model in [grl_config.name_model, pre_grl_config.name_model]:\r\n                _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, reward=1,\r\n                                                 bucket_id=bucket_id, forward_only=True)\r\n                for i, output in enumerate(output_logits):\r\n                    print(""index: %d, answer tokens: %s"" %(i, str(output)))\r\n                    if data_utils.EOS_ID in output:\r\n                        output = output[:output.index(data_utils.EOS_ID)]\r\n                    print("" "".join([str(rev_vocab[out]) for out in output]))\r\n\r\n            print(""> "", end="""")\r\n            sys.stdout.flush()\r\n            sentence = sys.stdin.readline()\r\n\r\n\r\ndef decoder(config):\r\n    vocab, rev_vocab, dev_set, train_set = prepare_data(config)\r\n\r\n    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(config.buckets))]\r\n    train_total_size = float(sum(train_bucket_sizes))\r\n    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\r\n                           for i in xrange(len(train_bucket_sizes))]\r\n\r\n    with tf.Session() as sess:\r\n        model = create_st_model(sess, config, forward_only=True, name_scope=config.name_model)\r\n\r\n        disc_train_query = open(""train.query"", ""w"")\r\n        disc_train_answer = open(""train.answer"", ""w"")\r\n        disc_train_gen = open(""train.gen"", ""w"")\r\n\r\n        num_step = 0\r\n        while num_step < 50000:\r\n            print(""generating num_step: "", num_step)\r\n            random_number_01 = np.random.random_sample()\r\n            bucket_id = min([i for i in xrange(len(train_buckets_scale))\r\n                             if train_buckets_scale[i] > random_number_01])\r\n\r\n            encoder_inputs, decoder_inputs, target_weights, batch_source_encoder, batch_source_decoder = \\\r\n                model.get_batch(train_set, bucket_id)\r\n\r\n            out_logits, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only=True)\r\n\r\n            tokens = []\r\n            resps = []\r\n            for seq in out_logits:\r\n                # print(""seq: %s"" %seq)\r\n                token = []\r\n                for t in seq:\r\n                    # print(""seq_t: %s"" %t)\r\n                    # t = list(t)\r\n                    # print(""list(t): %s"" %t)\r\n                    # t = np.array(t)\r\n                    # print(""array(t): %s"" %t)\r\n                    token.append(int(np.argmax(t, axis=0)))\r\n                tokens.append(token)\r\n            tokens_t = []\r\n            for col in range(len(tokens[0])):\r\n                tokens_t.append([tokens[row][col] for row in range(len(tokens))])\r\n\r\n            for seq in tokens_t:\r\n                if data_utils.EOS_ID in seq:\r\n                    resps.append(seq[:seq.index(data_utils.EOS_ID)][:config.buckets[bucket_id][1]])\r\n                else:\r\n                    resps.append(seq[:config.buckets[bucket_id][1]])\r\n\r\n            for query, answer, resp in zip(batch_source_encoder, batch_source_decoder, resps):\r\n\r\n                answer_str = "" "".join([str(rev_vocab[an]) for an in answer][:-1])\r\n                disc_train_answer.write(answer_str)\r\n                disc_train_answer.write(""\\n"")\r\n\r\n                query_str = "" "".join([str(rev_vocab[qu]) for qu in query])\r\n                disc_train_query.write(query_str)\r\n                disc_train_query.write(""\\n"")\r\n\r\n                resp_str = "" "".join([tf.compat.as_str(rev_vocab[output]) for output in resp])\r\n\r\n                disc_train_gen.write(resp_str)\r\n                disc_train_gen.write(""\\n"")\r\n            num_step += 1\r\n\r\n        disc_train_gen.close()\r\n        disc_train_query.close()\r\n        disc_train_answer.close()\r\n    pass\r\n\r\n\r\ndef main(_):\r\n    # model_1 P_backward(qi|a)\r\n    # ce_standard_train(gbk_config)\r\n\r\n    # model_2 P(a|pi,qi)\r\n    # ce_standard_train(gcc_config)\r\n\r\n    # model_3 P(s|a)\r\n    #ce_standard_train(gst_config)\r\n\r\n    # model_4.1 pre P_rl\r\n    #pre_rl_train(pre_grl_config)\r\n\r\n    # model_4.2 P_rl\r\n    train()\r\n\r\n    #test_decoder(gst_config)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    tf.app.run()\r\n'"
gst_rnn_model.py,31,"b'import tensorflow as tf\r\nimport gst_seq2seq as st_seq2seq\r\nimport numpy as np\r\nimport random\r\nimport data_utils\r\n\r\nclass gst_model(object):\r\n    def __init__(self, gst_config, name_scope, forward_only = False, num_samples = 512, dtype=tf.float32):\r\n        self.buckets = gst_config.buckets_concat\r\n        self.emb_dim = gst_config.emb_dim\r\n        self.batch_size = gst_config.batch_size\r\n        self.vocab_size = gst_config.vocab_size\r\n        #self.learning_rate = gst_config.learning_rate\r\n        self.learning_rate = tf.Variable(initial_value=float(gst_config.learning_rate), trainable=False, dtype=dtype)\r\n        self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * gst_config.learning_rate_decay_factor)\r\n\r\n        max_gradient_norm = gst_config.max_gradient_norm\r\n        num_layers = gst_config.num_layers\r\n\r\n        with tf.name_scope(""cell""):\r\n            single_cell = tf.nn.rnn_cell.GRUCell(self.emb_dim)\r\n            cells = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\r\n\r\n        self.global_step = tf.Variable(0, trainable=False)\r\n        self.encoder_inputs = []\r\n        self.decoder_inputs = []\r\n        self.target_weights = []\r\n        for i in xrange(self.buckets[-1][0]):\r\n            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=""encoder{0}"".format(i)))\r\n        for i in xrange(self.buckets[-1][1] + 1):\r\n            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=""deocder{0}"".format(i)))\r\n            self.target_weights.append(tf.placeholder(dtype, shape=[None], name=""weight{0}"".format(i)))\r\n        self.forward_only = tf.placeholder(tf.bool, name=""forward_only"")\r\n\r\n        # the top of decoder_inputs is mark\r\n        targets = [self.decoder_inputs[i + 1] for i in xrange(len(self.decoder_inputs) - 1)]\r\n\r\n        softmax_loss_function = None\r\n        output_projection = None\r\n        if num_samples < self.vocab_size:\r\n            w_t = tf.get_variable(""proj_w"", [self.vocab_size, self.emb_dim], dtype=dtype)\r\n            w = tf.transpose(w_t)\r\n            b = tf.get_variable(""proj_b"", [self.vocab_size], dtype=dtype)\r\n            output_projection = (w,b)\r\n\r\n            def sampled_loss(inputs, labels):\r\n                labels = tf.reshape(labels, [-1, 1])\r\n                # We need to compute the sampled_softmax_loss using 32bit floats to\r\n                # avoid numerical instabilities.\r\n                local_w_t = tf.cast(w_t, tf.float32)\r\n                local_b = tf.cast(b, tf.float32)\r\n                local_inputs = tf.cast(inputs, tf.float32)\r\n                return tf.cast(\r\n                    tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\r\n                                               num_samples, self.vocab_size),dtype)\r\n\r\n            softmax_loss_function = sampled_loss\r\n\r\n        with tf.name_scope(""st_seq2seq""):\r\n            def seq2seq_f(encoder_inputs, decoder_inputs, forward):\r\n                return st_seq2seq.embedding_attention_seq2seq(encoder_inputs=encoder_inputs,\r\n                                                              decoder_inputs=decoder_inputs,\r\n                                                              cell=cells,\r\n                                                              num_encoder_symbols=self.vocab_size,\r\n                                                              num_decoder_symbols=self.vocab_size,\r\n                                                              embedding_size=self.emb_dim,\r\n                                                              output_projection=output_projection,\r\n                                                              feed_previous=forward,\r\n                                                              dtype=dtype)\r\n            self.outputs, self.losses, _ = st_seq2seq.model_with_buckets(self.encoder_inputs, self.decoder_inputs,\r\n                                                                         targets, self.target_weights, self.buckets,\r\n                                                                         lambda x, y: seq2seq_f(x, y,\r\n                                                                             tf.select(self.forward_only,True, False)),\r\n                                                                         softmax_loss_function=softmax_loss_function)\r\n\r\n            for b in xrange(len(self.buckets)):\r\n                self.outputs[b] = [\r\n                            tf.cond(\r\n                                self.forward_only,\r\n                                lambda: tf.matmul(output, output_projection[0]) + output_projection[1],\r\n                                lambda: output\r\n                            )\r\n                            for output in self.outputs[b]\r\n                        ]\r\n\r\n        if not forward_only:\r\n            with tf.name_scope(""gst_radient""):\r\n                self.t_vars = [v for v in tf.trainable_variables() if name_scope in v.name]\r\n                self.gradient_norms = []\r\n                self.updatas = []\r\n\r\n                opt = tf.train.AdamOptimizer(learning_rate=0.001)\r\n                #opt = tf.train.GradientDescentOptimizer(self.learning_rate)\r\n                for b in xrange(len(self.buckets)):\r\n                    gradients = tf.gradients(self.losses[b], self.t_vars)\r\n                    clips_gradient, norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\r\n                    self.gradient_norms.append(norm)\r\n                    gradient_ops = opt.apply_gradients(zip(clips_gradient, self.t_vars), global_step=self.global_step)\r\n                    self.updatas.append(gradient_ops)\r\n\r\n        all_variables = [k for k in tf.global_variables() if name_scope in k.name]\r\n        self.saver = tf.train.Saver(all_variables)\r\n\r\n    def step(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only):\r\n        encoder_size, decoder_size = self.buckets[bucket_id]\r\n\r\n        input_feed = {self.forward_only.name: forward_only}\r\n        for i in xrange(encoder_size):\r\n            input_feed[self.encoder_inputs[i].name] = encoder_inputs[i]\r\n        for i in xrange(decoder_size):\r\n            input_feed[self.decoder_inputs[i].name] = decoder_inputs[i]\r\n            input_feed[self.target_weights[i].name] = target_weights[i]\r\n        last_target = self.decoder_inputs[decoder_size].name\r\n        input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\r\n\r\n        if not forward_only:\r\n            output_feed = [self.updatas[bucket_id],\r\n                           self.gradient_norms[bucket_id],\r\n                           self.losses[bucket_id]]\r\n            updata, norm, loss = session.run(output_feed, input_feed)\r\n            return updata, norm, loss\r\n        else:\r\n            output_feed = [self.outputs[bucket_id], self.losses[bucket_id]]\r\n            output, loss = session.run(output_feed, input_feed)\r\n            return output, loss\r\n\r\n    def get_batch(self, train_data, bucket_id):\r\n        encoder_size, decoder_size = self.buckets[bucket_id]\r\n        encoder_inputs, decoder_inputs = [], []\r\n        batch_source_encoder, batch_source_decoder = [], []\r\n\r\n        #print(""bucket_id: "", bucket_id)\r\n        for batch_i in xrange(self.batch_size):\r\n            encoder_input, decoder_input = random.choice(train_data[bucket_id])\r\n\r\n            batch_source_encoder.append(encoder_input)\r\n            batch_source_decoder.append(decoder_input)\r\n\r\n            #print(""encoder_input: "", encoder_input)\r\n            encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\r\n            encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\r\n            #print(""encoder_input pad: "", list(reversed(encoder_input + encoder_pad)))\r\n\r\n            #print(""decoder_input: "", decoder_input)\r\n            decoder_pad_size = decoder_size - len(decoder_input) - 1\r\n            decoder_inputs.append([data_utils.GO_ID] + decoder_input +\r\n                                  [data_utils.PAD_ID] * decoder_pad_size)\r\n            #print(""decoder_pad: "",[data_utils.GO_ID] + decoder_input + [data_utils.PAD_ID] * decoder_pad_size)\r\n\r\n        batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\r\n\r\n        for length_idx in xrange(encoder_size):\r\n            batch_encoder_inputs.append(\r\n                np.array([encoder_inputs[batch_idx][length_idx]\r\n                          for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n        for length_idx in xrange(decoder_size):\r\n            batch_decoder_inputs.append(\r\n                np.array([decoder_inputs[batch_idx][length_idx]\r\n                          for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n            batch_weight = np.ones(self.batch_size, dtype=np.float32)\r\n            for batch_idx in xrange(self.batch_size):\r\n                # We set weight to 0 if the corresponding target is a PAD symbol.\r\n                # The corresponding target is decoder_input shifted by 1 forward.\r\n                if length_idx < decoder_size - 1:\r\n                    target = decoder_inputs[batch_idx][length_idx + 1]\r\n                if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\r\n                    batch_weight[batch_idx] = 0.0\r\n            batch_weights.append(batch_weight)\r\n\r\n        return batch_encoder_inputs, batch_decoder_inputs, batch_weights, batch_source_encoder, batch_source_decoder'"
gst_seq2seq.py,6,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# We disable pylint because we need python3 compatibility.\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nfrom six.moves import zip     # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom tensorflow.python import shape\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.util import nest\n\n# TODO(ebrevdo): Remove once _linear is fully deprecated.\nlinear = rnn_cell._linear  # pylint: disable=protected-access\n\n\ndef _extract_argmax_and_embed(embedding, output_projection=None,\n                              update_embedding=True):\n  """"""Get a loop_function that extracts the previous symbol and embeds it.\n\n  Args:\n    embedding: embedding tensor for symbols.\n    output_projection: None or a pair (W, B). If provided, each fed previous\n      output will first be multiplied by W and added B.\n    update_embedding: Boolean; if False, the gradients will not propagate\n      through the embeddings.\n\n  Returns:\n    A loop function.\n  """"""\n  def loop_function(prev, _):\n    if output_projection is not None:\n      prev = nn_ops.xw_plus_b(\n          prev, output_projection[0], output_projection[1])\n    prev_symbol = math_ops.argmax(prev, 1)\n    # Note that gradients will not propagate through the second parameter of\n    # embedding_lookup.\n    emb_prev = embedding_ops.embedding_lookup(embedding, prev_symbol)\n    if not update_embedding:\n      emb_prev = array_ops.stop_gradient(emb_prev)\n    return emb_prev\n  return loop_function\n\n\ndef rnn_decoder(decoder_inputs, initial_state, cell, loop_function=None,\n                scope=None):\n  """"""RNN decoder for the sequence-to-sequence model.\n\n  Args:\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    initial_state: 2D Tensor with shape [batch_size x cell.state_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    loop_function: If not None, this function will be applied to the i-th output\n      in order to generate the i+1-st input, and decoder_inputs will be ignored,\n      except for the first element (""GO"" symbol). This can be used for decoding,\n      but also for training to emulate http://arxiv.org/abs/1506.03099.\n      Signature -- loop_function(prev, i) = next\n        * prev is a 2D Tensor of shape [batch_size x output_size],\n        * i is an integer, the step number (when advanced control is needed),\n        * next is a 2D Tensor of shape [batch_size x input_size].\n    scope: VariableScope for the created subgraph; defaults to ""rnn_decoder"".\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing generated outputs.\n      state: The state of each cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n        (Note that in some cases, like basic RNN cell or GRU cell, outputs and\n         states can be the same. They are different for LSTM cells though.)\n  """"""\n  with variable_scope.variable_scope(scope or ""rnn_decoder""):\n    state = initial_state\n    outputs = []\n    prev = None\n    for i, inp in enumerate(decoder_inputs):\n      if loop_function is not None and prev is not None:\n        with variable_scope.variable_scope(""loop_function"", reuse=True):\n          inp = loop_function(prev, i)\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n      output, state = cell(inp, state)\n      outputs.append(output)\n      if loop_function is not None:\n        prev = output\n  return outputs, state\n\n\ndef basic_rnn_seq2seq(\n    encoder_inputs, decoder_inputs, cell, dtype=dtypes.float32, scope=None):\n  """"""Basic RNN sequence-to-sequence model.\n\n  This model first runs an RNN to encode encoder_inputs into a state vector,\n  then runs decoder, initialized with the last encoder state, on decoder_inputs.\n  Encoder and decoder use the same RNN cell type, but don\'t share parameters.\n\n  Args:\n    encoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    dtype: The dtype of the initial state of the RNN cell (default: tf.float32).\n    scope: VariableScope for the created subgraph; default: ""basic_rnn_seq2seq"".\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing the generated outputs.\n      state: The state of each decoder cell in the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  """"""\n  with variable_scope.variable_scope(scope or ""basic_rnn_seq2seq""):\n    _, enc_state = rnn.rnn(cell, encoder_inputs, dtype=dtype)\n    return rnn_decoder(decoder_inputs, enc_state, cell)\n\n\ndef tied_rnn_seq2seq(encoder_inputs, decoder_inputs, cell,\n                     loop_function=None, dtype=dtypes.float32, scope=None):\n  """"""RNN sequence-to-sequence model with tied encoder and decoder parameters.\n\n  This model first runs an RNN to encode encoder_inputs into a state vector, and\n  then runs decoder, initialized with the last encoder state, on decoder_inputs.\n  Encoder and decoder use the same RNN cell and share parameters.\n\n  Args:\n    encoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    loop_function: If not None, this function will be applied to i-th output\n      in order to generate i+1-th input, and decoder_inputs will be ignored,\n      except for the first element (""GO"" symbol), see rnn_decoder for details.\n    dtype: The dtype of the initial state of the rnn cell (default: tf.float32).\n    scope: VariableScope for the created subgraph; default: ""tied_rnn_seq2seq"".\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing the generated outputs.\n      state: The state of each decoder cell in each time-step. This is a list\n        with length len(decoder_inputs) -- one item for each time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  """"""\n  with variable_scope.variable_scope(""combined_tied_rnn_seq2seq""):\n    scope = scope or ""tied_rnn_seq2seq""\n    _, enc_state = rnn.rnn(\n        cell, encoder_inputs, dtype=dtype, scope=scope)\n    variable_scope.get_variable_scope().reuse_variables()\n    return rnn_decoder(decoder_inputs, enc_state, cell,\n                       loop_function=loop_function, scope=scope)\n\n\ndef embedding_rnn_decoder(decoder_inputs,\n                          initial_state,\n                          cell,\n                          num_symbols,\n                          embedding_size,\n                          output_projection=None,\n                          feed_previous=False,\n                          update_embedding_for_previous=True,\n                          scope=None):\n\n  with variable_scope.variable_scope(scope or ""embedding_rnn_decoder"") as scope:\n    if output_projection is not None:\n      dtype = scope.dtype\n      proj_weights = ops.convert_to_tensor(output_projection[0], dtype=dtype)\n      proj_weights.get_shape().assert_is_compatible_with([None, num_symbols])\n      proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n      proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n    embedding = variable_scope.get_variable(""embedding"",\n                                            [num_symbols, embedding_size])\n    loop_function = _extract_argmax_and_embed(\n        embedding, output_projection,\n        update_embedding_for_previous) if feed_previous else None\n    emb_inp = (\n        embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs)\n    return rnn_decoder(emb_inp, initial_state, cell,\n                       loop_function=loop_function)\n\n\ndef embedding_rnn_seq2seq(encoder_inputs,\n                          decoder_inputs,\n                          cell,\n                          num_encoder_symbols,\n                          num_decoder_symbols,\n                          embedding_size,\n                          output_projection=None,\n                          feed_previous=False,\n                          dtype=None,\n                          scope=None):\n\n  with variable_scope.variable_scope(scope or ""embedding_rnn_seq2seq"") as scope:\n    if dtype is not None:\n      scope.set_dtype(dtype)\n    else:\n      dtype = scope.dtype\n\n    # Encoder.\n    encoder_cell = rnn_cell.EmbeddingWrapper(\n        cell, embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n\n    # Decoder.\n    if output_projection is None:\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n\n    if isinstance(feed_previous, bool):\n      return embedding_rnn_decoder(\n          decoder_inputs,\n          encoder_state,\n          cell,\n          num_decoder_symbols,\n          embedding_size,\n          output_projection=output_projection,\n          feed_previous=feed_previous,\n          scope=scope)\n\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n    def decoder(feed_previous_bool):\n      reuse = None if feed_previous_bool else True\n      with variable_scope.variable_scope(\n          variable_scope.get_variable_scope(), reuse=reuse) as scope:\n        outputs, state = embedding_rnn_decoder(\n            decoder_inputs, encoder_state, cell, num_decoder_symbols,\n            embedding_size, output_projection=output_projection,\n            feed_previous=feed_previous_bool,\n            update_embedding_for_previous=False)\n        state_list = [state]\n        if nest.is_sequence(state):\n          state_list = nest.flatten(state)\n        return outputs + state_list\n\n    outputs_and_state = control_flow_ops.cond(feed_previous,\n                                              lambda: decoder(True),\n                                              lambda: decoder(False))\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n    state_list = outputs_and_state[outputs_len:]\n    state = state_list[0]\n    if nest.is_sequence(encoder_state):\n      state = nest.pack_sequence_as(structure=encoder_state,\n                                    flat_sequence=state_list)\n    return outputs_and_state[:outputs_len], state\n\n\ndef embedding_tied_rnn_seq2seq(encoder_inputs,\n                               decoder_inputs,\n                               cell,\n                               num_symbols,\n                               embedding_size,\n                               num_decoder_symbols=None,\n                               output_projection=None,\n                               feed_previous=False,\n                               dtype=None,\n                               scope=None):\n  """"""Embedding RNN sequence-to-sequence model with tied (shared) parameters.\n\n  This model first embeds encoder_inputs by a newly created embedding (of shape\n  [num_symbols x input_size]). Then it runs an RNN to encode embedded\n  encoder_inputs into a state vector. Next, it embeds decoder_inputs using\n  the same embedding. Then it runs RNN decoder, initialized with the last\n  encoder state, on embedded decoder_inputs. The decoder output is over symbols\n  from 0 to num_decoder_symbols - 1 if num_decoder_symbols is none; otherwise it\n  is over 0 to num_symbols - 1.\n\n  Args:\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    decoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    num_symbols: Integer; number of symbols for both encoder and decoder.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    num_decoder_symbols: Integer; number of output symbols for decoder. If\n      provided, the decoder output is over symbols 0 to num_decoder_symbols - 1.\n      Otherwise, decoder output is over symbols 0 to num_symbols - 1. Note that\n      this assumes that the vocabulary is set up such that the first\n      num_decoder_symbols of num_symbols are part of decoding.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_symbols] and B has\n      shape [num_symbols]; if provided and feed_previous=True, each\n      fed previous output will first be multiplied by W and added B.\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first\n      of decoder_inputs will be used (the ""GO"" symbol), and all other decoder\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\n      If False, decoder_inputs are used as given (the standard decoder case).\n    dtype: The dtype to use for the initial RNN states (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""embedding_tied_rnn_seq2seq"".\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_symbols] containing the generated\n        outputs where output_symbols = num_decoder_symbols if\n        num_decoder_symbols is not None otherwise output_symbols = num_symbols.\n      state: The state of each decoder cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n\n  Raises:\n    ValueError: When output_projection has the wrong shape.\n  """"""\n  with variable_scope.variable_scope(\n      scope or ""embedding_tied_rnn_seq2seq"", dtype=dtype) as scope:\n    dtype = scope.dtype\n\n    if output_projection is not None:\n      proj_weights = ops.convert_to_tensor(output_projection[0], dtype=dtype)\n      proj_weights.get_shape().assert_is_compatible_with([None, num_symbols])\n      proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n      proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n    embedding = variable_scope.get_variable(\n        ""embedding"", [num_symbols, embedding_size], dtype=dtype)\n\n    emb_encoder_inputs = [embedding_ops.embedding_lookup(embedding, x)\n                          for x in encoder_inputs]\n    emb_decoder_inputs = [embedding_ops.embedding_lookup(embedding, x)\n                          for x in decoder_inputs]\n\n    output_symbols = num_symbols\n    if num_decoder_symbols is not None:\n      output_symbols = num_decoder_symbols\n    if output_projection is None:\n      cell = rnn_cell.OutputProjectionWrapper(cell, output_symbols)\n\n    if isinstance(feed_previous, bool):\n      loop_function = _extract_argmax_and_embed(\n          embedding, output_projection, True) if feed_previous else None\n      return tied_rnn_seq2seq(emb_encoder_inputs, emb_decoder_inputs, cell,\n                              loop_function=loop_function, dtype=dtype)\n\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n    def decoder(feed_previous_bool):\n      loop_function = _extract_argmax_and_embed(\n        embedding, output_projection, False) if feed_previous_bool else None\n      reuse = None if feed_previous_bool else True\n      with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                         reuse=reuse):\n        outputs, state = tied_rnn_seq2seq(\n            emb_encoder_inputs, emb_decoder_inputs, cell,\n            loop_function=loop_function, dtype=dtype)\n        state_list = [state]\n        if nest.is_sequence(state):\n          state_list = nest.flatten(state)\n        return outputs + state_list\n\n    outputs_and_state = control_flow_ops.cond(feed_previous,\n                                              lambda: decoder(True),\n                                              lambda: decoder(False))\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n    state_list = outputs_and_state[outputs_len:]\n    state = state_list[0]\n    # Calculate zero-state to know it\'s structure.\n    static_batch_size = encoder_inputs[0].get_shape()[0]\n    for inp in encoder_inputs[1:]:\n      static_batch_size.merge_with(inp.get_shape()[0])\n    batch_size = static_batch_size.value\n    if batch_size is None:\n      batch_size = array_ops.shape(encoder_inputs[0])[0]\n    zero_state = cell.zero_state(batch_size, dtype)\n    if nest.is_sequence(zero_state):\n      state = nest.pack_sequence_as(structure=zero_state,\n                                    flat_sequence=state_list)\n    return outputs_and_state[:outputs_len], state\n\n\ndef attention_decoder(decoder_inputs,\n                      initial_state,\n                      attention_states,\n                      cell,\n                      output_size=None,\n                      num_heads=1,\n                      loop_function=None,\n                      dtype=None,\n                      scope=None,\n                      initial_state_attention=False):\n  """"""RNN decoder with attention for the sequence-to-sequence model.\n\n  In this context ""attention"" means that, during decoding, the RNN can look up\n  information in the additional tensor attention_states, and it does this by\n  focusing on a few entries from the tensor. This model has proven to yield\n  especially good results in a number of sequence-to-sequence tasks. This\n  implementation is based on http://arxiv.org/abs/1412.7449 (see below for\n  details). It is recommended for complex sequence-to-sequence tasks.\n\n  Args:\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    attention_states: 3D Tensor [batch_size x attn_length x attn_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    output_size: Size of the output vectors; if None, we use cell.output_size.\n    num_heads: Number of attention heads that read from attention_states.\n    loop_function: If not None, this function will be applied to i-th output\n      in order to generate i+1-th input, and decoder_inputs will be ignored,\n      except for the first element (""GO"" symbol). This can be used for decoding,\n      but also for training to emulate http://arxiv.org/abs/1506.03099.\n      Signature -- loop_function(prev, i) = next\n        * prev is a 2D Tensor of shape [batch_size x output_size],\n        * i is an integer, the step number (when advanced control is needed),\n        * next is a 2D Tensor of shape [batch_size x input_size].\n    dtype: The dtype to use for the RNN initial state (default: tf.float32).\n    scope: VariableScope for the created subgraph; default: ""attention_decoder"".\n    initial_state_attention: If False (default), initial attentions are zero.\n      If True, initialize the attentions from the initial state and attention\n      states -- useful when we wish to resume decoding from a previously\n      stored decoder state and attention states.\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors of\n        shape [batch_size x output_size]. These represent the generated outputs.\n        Output i is computed from input i (which is either the i-th element\n        of decoder_inputs or loop_function(output {i-1}, i)) as follows.\n        First, we run the cell on a combination of the input and previous\n        attention masks:\n          cell_output, new_state = cell(linear(input, prev_attn), prev_state).\n        Then, we calculate new attention masks:\n          new_attn = softmax(V^T * tanh(W * attention_states + U * new_state))\n        and then we calculate the output:\n          output = linear(cell_output, new_attn).\n      state: The state of each decoder cell the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n\n  Raises:\n    ValueError: when num_heads is not positive, there are no inputs, shapes\n      of attention_states are not set, or input size cannot be inferred\n      from the input.\n  """"""\n  if not decoder_inputs:\n    raise ValueError(""Must provide at least 1 input to attention decoder."")\n  if num_heads < 1:\n    raise ValueError(""With less than 1 heads, use a non-attention decoder."")\n  if attention_states.get_shape()[2].value is None:\n    raise ValueError(""Shape[2] of attention_states must be known: %s""\n                     % attention_states.get_shape())\n  if output_size is None:\n    output_size = cell.output_size\n\n  with variable_scope.variable_scope(\n      scope or ""attention_decoder"", dtype=dtype) as scope:\n    dtype = scope.dtype\n\n    batch_size = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n    attn_length = attention_states.get_shape()[1].value\n    if attn_length is None:\n      attn_length = shape(attention_states)[1]\n    attn_size = attention_states.get_shape()[2].value\n\n    # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n    hidden = array_ops.reshape(\n        attention_states, [-1, attn_length, 1, attn_size])\n    hidden_features = []\n    v = []\n    attention_vec_size = attn_size  # Size of query vectors for attention.\n    for a in xrange(num_heads):\n      k = variable_scope.get_variable(""AttnW_%d"" % a,\n                                      [1, 1, attn_size, attention_vec_size])\n      hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], ""SAME""))\n      v.append(\n          variable_scope.get_variable(""AttnV_%d"" % a, [attention_vec_size]))\n\n    state = initial_state\n\n    def attention(query):\n      """"""Put attention masks on hidden using hidden_features and query.""""""\n      ds = []  # Results of attention reads will be stored here.\n      if nest.is_sequence(query):  # If the query is a tuple, flatten it.\n        query_list = nest.flatten(query)\n        for q in query_list:  # Check that ndims == 2 if specified.\n          ndims = q.get_shape().ndims\n          if ndims:\n            assert ndims == 2\n        query = array_ops.concat(1, query_list)\n      for a in xrange(num_heads):\n        with variable_scope.variable_scope(""Attention_%d"" % a):\n          y = linear(query, attention_vec_size, True)\n          y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n          # Attention mask is a softmax of v^T * tanh(...).\n          s = math_ops.reduce_sum(\n              v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n          a = nn_ops.softmax(s)\n          # Now calculate the attention-weighted vector d.\n          d = math_ops.reduce_sum(\n              array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n              [1, 2])\n          ds.append(array_ops.reshape(d, [-1, attn_size]))\n      return ds\n\n    outputs = []\n    prev = None\n    batch_attn_size = array_ops.pack([batch_size, attn_size])\n    attns = [array_ops.zeros(batch_attn_size, dtype=dtype)\n             for _ in xrange(num_heads)]\n    for a in attns:  # Ensure the second shape of attention vectors is set.\n      a.set_shape([None, attn_size])\n    if initial_state_attention:\n      attns = attention(initial_state)\n    for i, inp in enumerate(decoder_inputs):\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n      # If loop_function is set, we use it instead of decoder_inputs.\n      if loop_function is not None and prev is not None:\n        with variable_scope.variable_scope(""loop_function"", reuse=True):\n          inp = loop_function(prev, i)\n      # Merge input and previous attentions into one vector of the right size.\n      input_size = inp.get_shape().with_rank(2)[1]\n      if input_size.value is None:\n        raise ValueError(""Could not infer input size from input: %s"" % inp.name)\n      x = linear([inp] + attns, input_size, True)\n      # Run the RNN.\n      cell_output, state = cell(x, state)\n      # Run the attention mechanism.\n      if i == 0 and initial_state_attention:\n        with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                           reuse=True):\n          attns = attention(state)\n      else:\n        attns = attention(state)\n\n      with variable_scope.variable_scope(""AttnOutputProjection""):\n        output = linear([cell_output] + attns, output_size, True)\n      if loop_function is not None:\n        prev = output\n      outputs.append(output)\n\n  return outputs, state\n\n\ndef embedding_attention_decoder(decoder_inputs,\n                                initial_state,\n                                attention_states,\n                                cell,\n                                num_symbols,\n                                embedding_size,\n                                num_heads=1,\n                                output_size=None,\n                                output_projection=None,\n                                feed_previous=False,\n                                update_embedding_for_previous=True,\n                                dtype=None,\n                                scope=None,\n                                initial_state_attention=False):\n  """"""RNN decoder with embedding and attention and a pure-decoding option.\n\n  Args:\n    decoder_inputs: A list of 1D batch-sized int32 Tensors (decoder inputs).\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    attention_states: 3D Tensor [batch_size x attn_length x attn_size].\n    cell: rnn_cell.RNNCell defining the cell function.\n    num_symbols: Integer, how many symbols come into the embedding.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    num_heads: Number of attention heads that read from attention_states.\n    output_size: Size of the output vectors; if None, use output_size.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_symbols] and B has shape\n      [num_symbols]; if provided and feed_previous=True, each fed previous\n      output will first be multiplied by W and added B.\n    feed_previous: Boolean; if True, only the first of decoder_inputs will be\n      used (the ""GO"" symbol), and all other decoder inputs will be generated by:\n        next = embedding_lookup(embedding, argmax(previous_output)),\n      In effect, this implements a greedy decoder. It can also be used\n      during training to emulate http://arxiv.org/abs/1506.03099.\n      If False, decoder_inputs are used as given (the standard decoder case).\n    update_embedding_for_previous: Boolean; if False and feed_previous=True,\n      only the embedding for the first symbol of decoder_inputs (the ""GO""\n      symbol) will be updated by back propagation. Embeddings for the symbols\n      generated from the decoder itself remain unchanged. This parameter has\n      no effect if feed_previous=False.\n    dtype: The dtype to use for the RNN initial states (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""embedding_attention_decoder"".\n    initial_state_attention: If False (default), initial attentions are zero.\n      If True, initialize the attentions from the initial state and attention\n      states -- useful when we wish to resume decoding from a previously\n      stored decoder state and attention states.\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing the generated outputs.\n      state: The state of each decoder cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n\n  Raises:\n    ValueError: When output_projection has the wrong shape.\n  """"""\n  if output_size is None:\n    output_size = cell.output_size\n  if output_projection is not None:\n    proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n    proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n  with variable_scope.variable_scope(\n      scope or ""embedding_attention_decoder"", dtype=dtype) as scope:\n\n    embedding = variable_scope.get_variable(""embedding"",\n                                            [num_symbols, embedding_size])\n    loop_function = _extract_argmax_and_embed(\n        embedding, output_projection,\n        update_embedding_for_previous) if feed_previous else None\n    emb_inp = [\n        embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs]\n    return attention_decoder(\n        emb_inp,\n        initial_state,\n        attention_states,\n        cell,\n        output_size=output_size,\n        num_heads=num_heads,\n        loop_function=loop_function,\n        initial_state_attention=initial_state_attention,\n        scope=scope)\n\n\ndef embedding_attention_seq2seq(encoder_inputs,\n                                decoder_inputs,\n                                cell,\n                                num_encoder_symbols,\n                                num_decoder_symbols,\n                                embedding_size,\n                                num_heads=1,\n                                output_projection=None,\n                                feed_previous=False,\n                                dtype=None,\n                                scope=None,\n                                initial_state_attention=False):\n\n  with variable_scope.variable_scope(\n      scope or ""embedding_attention_seq2seq"", dtype=dtype) as scope:\n    dtype = scope.dtype\n    # Encoder.\n    encoder_cell = rnn_cell.EmbeddingWrapper(cell, embedding_classes=num_encoder_symbols,\n                                             embedding_size=embedding_size)\n    encoder_outputs, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n\n    # First calculate a concatenation of encoder outputs to put attention on.\n    top_states = [array_ops.reshape(e, [-1, 1, cell.output_size])\n                  for e in encoder_outputs]\n    attention_states = array_ops.concat(1, top_states)\n\n    # Decoder.\n    output_size = None\n    if output_projection is None:\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n      output_size = num_decoder_symbols\n\n    if isinstance(feed_previous, bool):\n      outputs, state = embedding_attention_decoder(\n          decoder_inputs,\n          encoder_state,\n          attention_states,\n          cell,\n          num_decoder_symbols,\n          embedding_size,\n          num_heads=num_heads,\n          output_size=output_size,\n          output_projection=output_projection,\n          feed_previous=feed_previous,\n          initial_state_attention=initial_state_attention,\n          scope=scope)\n      return outputs, state, encoder_state\n\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n    def decoder(feed_previous_bool):\n      reuse = None if feed_previous_bool else True\n      with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=reuse) as scope:\n        outputs, state = embedding_attention_decoder(\n            decoder_inputs,\n            encoder_state,\n            attention_states,\n            cell,\n            num_decoder_symbols,\n            embedding_size,\n            num_heads=num_heads,\n            output_size=output_size,\n            output_projection=output_projection,\n            feed_previous=feed_previous_bool,\n            update_embedding_for_previous=False,\n            initial_state_attention=initial_state_attention,\n            scope=scope)\n        state_list = [state]\n        if nest.is_sequence(state):\n          state_list = nest.flatten(state)\n        return outputs + state_list\n\n    outputs_and_state = control_flow_ops.cond(feed_previous,\n                                              lambda: decoder(True),\n                                              lambda: decoder(False))\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n    state_list = outputs_and_state[outputs_len:]\n    state = state_list[0]\n    if nest.is_sequence(encoder_state):\n      state = nest.pack_sequence_as(structure=encoder_state,\n                                    flat_sequence=state_list)\n    return outputs_and_state[:outputs_len], state, encoder_state\n\n\ndef one2many_rnn_seq2seq(encoder_inputs,\n                         decoder_inputs_dict,\n                         cell,\n                         num_encoder_symbols,\n                         num_decoder_symbols_dict,\n                         embedding_size,\n                         feed_previous=False,\n                         dtype=None,\n                         scope=None):\n  """"""One-to-many RNN sequence-to-sequence model (multi-task).\n\n  This is a multi-task sequence-to-sequence model with one encoder and multiple\n  decoders. Reference to multi-task sequence-to-sequence learning can be found\n  here: http://arxiv.org/abs/1511.06114\n\n  Args:\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    decoder_inputs_dict: A dictionany mapping decoder name (string) to\n      the corresponding decoder_inputs; each decoder_inputs is a list of 1D\n      Tensors of shape [batch_size]; num_decoders is defined as\n      len(decoder_inputs_dict).\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    num_encoder_symbols: Integer; number of symbols on the encoder side.\n    num_decoder_symbols_dict: A dictionary mapping decoder name (string) to an\n      integer specifying number of symbols for the corresponding decoder;\n      len(num_decoder_symbols_dict) must be equal to num_decoders.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first of\n      decoder_inputs will be used (the ""GO"" symbol), and all other decoder\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\n      If False, decoder_inputs are used as given (the standard decoder case).\n    dtype: The dtype of the initial state for both the encoder and encoder\n      rnn cells (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""one2many_rnn_seq2seq""\n\n  Returns:\n    A tuple of the form (outputs_dict, state_dict), where:\n      outputs_dict: A mapping from decoder name (string) to a list of the same\n        length as decoder_inputs_dict[name]; each element in the list is a 2D\n        Tensors with shape [batch_size x num_decoder_symbol_list[name]]\n        containing the generated outputs.\n      state_dict: A mapping from decoder name (string) to the final state of the\n        corresponding decoder RNN; it is a 2D Tensor of shape\n        [batch_size x cell.state_size].\n  """"""\n  outputs_dict = {}\n  state_dict = {}\n\n  with variable_scope.variable_scope(\n      scope or ""one2many_rnn_seq2seq"", dtype=dtype) as scope:\n    dtype = scope.dtype\n\n    # Encoder.\n    encoder_cell = rnn_cell.EmbeddingWrapper(\n        cell, embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n\n    # Decoder.\n    for name, decoder_inputs in decoder_inputs_dict.items():\n      num_decoder_symbols = num_decoder_symbols_dict[name]\n\n      with variable_scope.variable_scope(""one2many_decoder_"" + str(\n          name)) as scope:\n        decoder_cell = rnn_cell.OutputProjectionWrapper(cell,\n                                                        num_decoder_symbols)\n        if isinstance(feed_previous, bool):\n          outputs, state = embedding_rnn_decoder(\n              decoder_inputs, encoder_state, decoder_cell, num_decoder_symbols,\n              embedding_size, feed_previous=feed_previous)\n        else:\n          # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n          def filled_embedding_rnn_decoder(feed_previous):\n            """"""The current decoder with a fixed feed_previous parameter.""""""\n            # pylint: disable=cell-var-from-loop\n            reuse = None if feed_previous else True\n            vs = variable_scope.get_variable_scope()\n            with variable_scope.variable_scope(vs, reuse=reuse):\n              outputs, state = embedding_rnn_decoder(\n                  decoder_inputs, encoder_state, decoder_cell,\n                  num_decoder_symbols, embedding_size,\n                  feed_previous=feed_previous)\n            # pylint: enable=cell-var-from-loop\n            state_list = [state]\n            if nest.is_sequence(state):\n              state_list = nest.flatten(state)\n            return outputs + state_list\n\n          outputs_and_state = control_flow_ops.cond(\n              feed_previous,\n              lambda: filled_embedding_rnn_decoder(True),\n              lambda: filled_embedding_rnn_decoder(False))\n          # Outputs length is the same as for decoder inputs.\n          outputs_len = len(decoder_inputs)\n          outputs = outputs_and_state[:outputs_len]\n          state_list = outputs_and_state[outputs_len:]\n          state = state_list[0]\n          if nest.is_sequence(encoder_state):\n            state = nest.pack_sequence_as(structure=encoder_state,\n                                          flat_sequence=state_list)\n      outputs_dict[name] = outputs\n      state_dict[name] = state\n\n  return outputs_dict, state_dict\n\n\ndef sequence_loss_by_example(logits, targets, weights,\n                             average_across_timesteps=True,\n                             softmax_loss_function=None, name=None):\n  """"""Weighted cross-entropy loss for a sequence of logits (per example).\n\n  Args:\n    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n    name: Optional name for this operation, default: ""sequence_loss_by_example"".\n\n  Returns:\n    1D batch-sized float Tensor: The log-perplexity for each sequence.\n\n  Raises:\n    ValueError: If len(logits) is different from len(targets) or len(weights).\n  """"""\n  if len(targets) != len(logits) or len(weights) != len(logits):\n    raise ValueError(""Lengths of logits, weights, and targets must be the same ""\n                     ""%d, %d, %d."" % (len(logits), len(weights), len(targets)))\n  with ops.name_scope(name, ""sequence_loss_by_example"",\n                      logits + targets + weights):\n    log_perp_list = []\n    for logit, target, weight in zip(logits, targets, weights):\n      if softmax_loss_function is None:\n        # TODO(irving,ebrevdo): This reshape is needed because\n        # sequence_loss_by_example is called with scalars sometimes, which\n        # violates our general scalar strictness policy.\n        target = array_ops.reshape(target, [-1])\n        crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(\n            logit, target)\n      else:\n        crossent = softmax_loss_function(logit, target)\n      log_perp_list.append(crossent * weight)\n    log_perps = math_ops.add_n(log_perp_list)\n    if average_across_timesteps:\n      total_size = math_ops.add_n(weights)\n      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n      log_perps /= total_size\n  return log_perps\n\n\ndef sequence_loss(logits, targets, weights,\n                  average_across_timesteps=True, average_across_batch=True,\n                  softmax_loss_function=None, name=None):\n  """"""Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\n\n  Args:\n    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    average_across_batch: If set, divide the returned cost by the batch size.\n    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n    name: Optional name for this operation, defaults to ""sequence_loss"".\n\n  Returns:\n    A scalar float Tensor: The average log-perplexity per symbol (weighted).\n\n  Raises:\n    ValueError: If len(logits) is different from len(targets) or len(weights).\n  """"""\n  with ops.name_scope(name, ""sequence_loss"", logits + targets + weights):\n    cost = math_ops.reduce_sum(sequence_loss_by_example(\n        logits, targets, weights,\n        average_across_timesteps=average_across_timesteps,\n        softmax_loss_function=softmax_loss_function))\n    if average_across_batch:\n      batch_size = array_ops.shape(targets[0])[0]\n      return cost / math_ops.cast(batch_size, cost.dtype)\n    else:\n      return cost\n\n\ndef model_with_buckets(encoder_inputs, decoder_inputs, targets, weights,\n                       buckets, seq2seq, softmax_loss_function=None,\n                       per_example_loss=False, name=None):\n  if len(encoder_inputs) < buckets[-1][0]:\n    raise ValueError(""Length of encoder_inputs (%d) must be at least that of la""\n                     ""st bucket (%d)."" % (len(encoder_inputs), buckets[-1][0]))\n  if len(targets) < buckets[-1][1]:\n    raise ValueError(""Length of targets (%d) must be at least that of last""\n                     ""bucket (%d)."" % (len(targets), buckets[-1][1]))\n  if len(weights) < buckets[-1][1]:\n    raise ValueError(""Length of weights (%d) must be at least that of last""\n                     ""bucket (%d)."" % (len(weights), buckets[-1][1]))\n\n  all_inputs = encoder_inputs + decoder_inputs + targets + weights\n  losses = []\n  outputs = []\n  encoder_states = []\n  with ops.name_scope(name, ""model_with_buckets"", all_inputs):\n    for j, bucket in enumerate(buckets):\n      with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                         reuse=True if j > 0 else None):\n        bucket_outputs, decoder_states, encoder_state = seq2seq(encoder_inputs[:bucket[0]],\n                                    decoder_inputs[:bucket[1]])\n        outputs.append(bucket_outputs)\n        #print(""bucket outputs: %s"" %bucket_outputs)\n        encoder_states.append(encoder_state)\n        if per_example_loss:\n          losses.append(sequence_loss_by_example(\n              outputs[-1], targets[:bucket[1]], weights[:bucket[1]],\n              softmax_loss_function=softmax_loss_function))\n        else:\n          losses.append(sequence_loss(\n              outputs[-1], targets[:bucket[1]], weights[:bucket[1]],\n              softmax_loss_function=softmax_loss_function))\n\n  return outputs, losses, encoder_states\n'"
