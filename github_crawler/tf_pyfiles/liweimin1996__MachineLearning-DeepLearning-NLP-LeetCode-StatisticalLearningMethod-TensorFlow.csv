file_path,api_count,code
深度学习 吴恩达 Python代码实现/Keras_test.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Thu May 30 11:42:53 2019\n\n@author: Gift\n""""""\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\n\n# \xe7\x94\x9f\xe6\x88\x90\xe8\x99\x9a\xe6\x8b\x9f\xe6\x95\xb0\xe6\x8d\xae\nimport numpy as np\nx_train = np.random.random((1000, 20))\ny_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\nx_test = np.random.random((100, 20))\ny_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n\nmodel = Sequential()\n# Dense(64) \xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xb7\xe6\x9c\x89 64 \xe4\xb8\xaa\xe9\x9a\x90\xe8\x97\x8f\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\x9a\x84\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe3\x80\x82\n# \xe5\x9c\xa8\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe5\xbf\x85\xe9\xa1\xbb\xe6\x8c\x87\xe5\xae\x9a\xe6\x89\x80\xe6\x9c\x9f\xe6\x9c\x9b\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe5\xb0\xba\xe5\xaf\xb8\xef\xbc\x9a\n# \xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xef\xbc\x8c\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa 20 \xe7\xbb\xb4\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\xe3\x80\x82\nmodel.add(Dense(64, activation=\'relu\', input_dim=20))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation=\'relu\'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation=\'softmax\'))\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=sgd,\n              metrics=[\'accuracy\'])\n\nmodel.fit(x_train, y_train,\n          epochs=20,\n          batch_size=128)\nscore = model.evaluate(x_test, y_test, batch_size=128)\n\n# \xe7\x94\x9f\xe6\x88\x90\xe8\x99\x9a\xe6\x8b\x9f\xe6\x95\xb0\xe6\x8d\xae\nx_train = np.random.random((1000, 20))\ny_train = np.random.randint(2, size=(1000, 1))\nx_test = np.random.random((100, 20))\ny_test = np.random.randint(2, size=(100, 1))\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=20, activation=\'relu\'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation=\'relu\'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation=\'sigmoid\'))\n\nmodel.compile(loss=\'binary_crossentropy\',\n              optimizer=\'rmsprop\',\n              metrics=[\'accuracy\'])\n\nmodel.fit(x_train, y_train,\n          epochs=20,\n          batch_size=128)\nscore = model.evaluate(x_test, y_test, batch_size=128)\n\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nimport numpy as np\n\ndata_dim = 16\ntimesteps = 8\nnum_classes = 10\nbatch_size = 32\n\n# \xe6\x9c\x9f\xe6\x9c\x9b\xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe5\xb0\xba\xe5\xaf\xb8: (batch_size, timesteps, data_dim)\n# \xe8\xaf\xb7\xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xbf\x85\xe9\xa1\xbb\xe6\x8f\x90\xe4\xbe\x9b\xe5\xae\x8c\xe6\x95\xb4\xe7\x9a\x84 batch_input_shape\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe7\xbd\x91\xe7\xbb\x9c\xe6\x98\xaf\xe6\x9c\x89\xe7\x8a\xb6\xe6\x80\x81\xe7\x9a\x84\xe3\x80\x82\n# \xe7\xac\xac k \xe6\x89\xb9\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe7\xac\xac i \xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe6\x98\xaf\xe7\xac\xac k-1 \xe6\x89\xb9\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe7\xac\xac i \xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe5\x90\x8e\xe7\xbb\xad\xe3\x80\x82\nmodel = Sequential()\nmodel.add(LSTM(32, return_sequences=True, stateful=True,\n               batch_input_shape=(batch_size, timesteps, data_dim)))\nmodel.add(LSTM(32, return_sequences=True, stateful=True))\nmodel.add(LSTM(32, stateful=True))\nmodel.add(Dense(10, activation=\'softmax\'))\n\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=\'rmsprop\',\n              metrics=[\'accuracy\'])\n\n# \xe7\x94\x9f\xe6\x88\x90\xe8\x99\x9a\xe6\x8b\x9f\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\nx_train = np.random.random((batch_size * 10, timesteps, data_dim))\ny_train = np.random.random((batch_size * 10, num_classes))\n\n# \xe7\x94\x9f\xe6\x88\x90\xe8\x99\x9a\xe6\x8b\x9f\xe9\xaa\x8c\xe8\xaf\x81\xe6\x95\xb0\xe6\x8d\xae\nx_val = np.random.random((batch_size * 3, timesteps, data_dim))\ny_val = np.random.random((batch_size * 3, num_classes))\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size, epochs=5, shuffle=False,\n          validation_data=(x_val, y_val))\n\n\n\n\n'"
深度学习 吴恩达 Python代码实现/TensorFlow_test.py,6,"b'""""""\nCreated on 2019-5-18 22:26:57\n\n@author: liweimin\n""""""\n\nimport random\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf \n\nw=tf.Variable(0,dtype=tf.float32)\nx = tf.placeholder(tf.float32,[3,1])\n#coefficients = np.array([[1.],[-10.],[25.]])\ncoefficients = np.array([[1.],[-20.],[100.]])\n#cost=tf.add(tf.add(w**2,tf.multiply(-10.,w)),25)\n#cost=w**2-10*w+25\ncost = x[0][0]*w**2 +x[1][0]*w + x[2][0]\n\ntrain=tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n\ninit=tf.global_variables_initializer()\nsession=tf.Session()\nsession.run(init)\nprint(session.run(w))\n\nfor i in range(1000):\n    session.run(train,feed_dict = {x:coefficients})\nprint(session.run(w))\n\n\n\n\n\n'"
TensorFlow实践 Google Python代码实现/第2讲 python代码/a.py,0,b'123\n'
TensorFlow实践 Google Python代码实现/第2讲 python代码/animal.py,0,"b'class Animals():\n    def breathe(self):\n        print "" breathing""\n    def move(self):\n        print ""moving""\n    def eat (self):\n        print ""eating food""\nclass Mammals(Animals):\n    def breastfeed(self):\n        print ""feeding young""\nclass Cats(Mammals):\n    def __init__(self, spots):\n        self.spots = spots\n    def catch_mouse(self):\n        print ""catch mouse""\n    def left_foot_forward(self):\n        print ""left foot forward""\n    def left_foot_backward(self):\n        print ""left foot backward""\n    def dance(self):\n        self.left_foot_forward()\n        self.left_foot_backward()\n        self.left_foot_forward()\n        self.left_foot_backward()\nkitty=Cats(10)\nprint kitty.spots\nkitty.dance()\nkitty.breastfeed()\nkitty.move()\n'"
TensorFlow实践 Google Python代码实现/第2讲 python代码/b.py,0,"b'#coding:utf-8\nage=input(""\xe8\xbe\x93\xe5\x85\xa5\xe4\xbd\xa0\xe7\x9a\x84\xe5\xb9\xb4\xe9\xbe\x84\\n"")\nif age>18:\n    print ""\xe5\xa4\xa7\xe4\xba\x8e\xe5\x8d\x81\xe5\x85\xab\xe5\xb2\x81""\n    print ""\xe4\xbd\xa0\xe6\x88\x90\xe5\xb9\xb4\xe4\xba\x86""\nelse:\n    print ""\xe5\xb0\x8f\xe4\xba\x8e\xe7\xad\x89\xe4\xba\x8e\xe5\x8d\x81\xe5\x85\xab\xe5\xb2\x81""\n    print ""\xe8\xbf\x98\xe6\x9c\xaa\xe6\x88\x90\xe5\xb9\xb4""\n\n'"
TensorFlow实践 Google Python代码实现/第2讲 python代码/c.py,0,"b'#coding:utf-8\nnum=input(""please input your class number:"")\nif num==1 or num==2:\n    print ""class room 302""\nelif num==3:\n    print ""class room 303""\nelif num==4:\n    print ""class room 304""\nelse:\n    print ""class room 305""\n'"
TensorFlow实践 Google Python代码实现/第2讲 python代码/tf3_1.py,2,"b'import tensorflow as tf\na=tf.constant([1.0,2.0])\nb=tf.constant([3.0,4.0])\nresult=a+b\nprint result\n'"
机器学习 吴恩达 Python代码实现/ex1-linear regression 作业1 线性回归/ML-Exercise1.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Mar 16 14:38:43 2019\n\n@author: liweimin\n""""""\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\npath =  \'ex1data1.txt\'\ndata = pd.read_csv(path, header=None, names=[\'Population\', \'Profit\'])\ndata.head()\n\ndata.describe()\ndata.plot(kind=\'scatter\', x=\'Population\', y=\'Profit\', figsize=(12,8))\nplt.show()\n\ndef computeCost(X, y, theta):\n    inner = np.power(((X * theta.T) - y), 2)\n    return np.sum(inner) / (2 * len(X))\n\ndata.insert(0, \'Ones\', 1)\n\ncols = data.shape[1]\nX = data.iloc[:,0:cols-1]#X\xe6\x98\xaf\xe6\x89\x80\xe6\x9c\x89\xe8\xa1\x8c\xef\xbc\x8c\xe5\x8e\xbb\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\ny = data.iloc[:,cols-1:cols]#X\xe6\x98\xaf\xe6\x89\x80\xe6\x9c\x89\xe8\xa1\x8c\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\n\nX.head()#head()\xe6\x98\xaf\xe8\xa7\x82\xe5\xaf\x9f\xe5\x89\x8d5\xe8\xa1\x8c\n\ny.head()\n\nX = np.matrix(X.values)\ny = np.matrix(y.values)\ntheta = np.matrix(np.array([0,0]))\n\ntheta\nX.shape, theta.shape, y.shape\ncomputeCost(X, y, theta)\n\ndef gradientDescent(X, y, theta, alpha, iters):\n    temp = np.matrix(np.zeros(theta.shape))\n    parameters = int(theta.ravel().shape[1])\n    cost = np.zeros(iters)\n    \n    for i in range(iters):\n        error = (X * theta.T) - y\n        \n        for j in range(parameters):\n            term = np.multiply(error, X[:,j])\n            temp[0,j] = theta[0,j] - ((alpha / len(X)) * np.sum(term))\n            \n        theta = temp\n        cost[i] = computeCost(X, y, theta)\n        \n    return theta, cost\n\nalpha = 0.01\niters = 1000\n\ng, cost = gradientDescent(X, y, theta, alpha, iters)\ng\ncomputeCost(X, y, g)\n\nx = np.linspace(data.Population.min(), data.Population.max(), 100)\nf = g[0, 0] + (g[0, 1] * x)\n\nfig, ax = plt.subplots(figsize=(12,8))\nax.plot(x, f, \'r\', label=\'Prediction\')\nax.scatter(data.Population, data.Profit, label=\'Traning Data\')\nax.legend(loc=2)\nax.set_xlabel(\'Population\')\nax.set_ylabel(\'Profit\')\nax.set_title(\'Predicted Profit vs. Population Size\')\nplt.show()\n\nfig, ax = plt.subplots(figsize=(12,8))\nax.plot(np.arange(iters), cost, \'r\')\nax.set_xlabel(\'Iterations\')\nax.set_ylabel(\'Cost\')\nax.set_title(\'Error vs. Training Epoch\')\nplt.show()\n\npath =  \'ex1data2.txt\'\ndata2 = pd.read_csv(path, header=None, names=[\'Size\', \'Bedrooms\', \'Price\'])\ndata2.head()\n\ndata2 = (data2 - data2.mean()) / data2.std()\ndata2.head()\n\n# add ones column\ndata2.insert(0, \'Ones\', 1)\n\n# set X (training data) and y (target variable)\ncols = data2.shape[1]\nX2 = data2.iloc[:,0:cols-1]\ny2 = data2.iloc[:,cols-1:cols]\n\n# convert to matrices and initialize theta\nX2 = np.matrix(X2.values)\ny2 = np.matrix(y2.values)\ntheta2 = np.matrix(np.array([0,0,0]))\n\n# perform linear regression on the data set\ng2, cost2 = gradientDescent(X2, y2, theta2, alpha, iters)\n\n# get the cost (error) of the model\ncomputeCost(X2, y2, g2)\n\nfig, ax = plt.subplots(figsize=(12,8))\nax.plot(np.arange(iters), cost2, \'r\')\nax.set_xlabel(\'Iterations\')\nax.set_ylabel(\'Cost\')\nax.set_title(\'Error vs. Training Epoch\')\nplt.show()\n\nfrom sklearn import linear_model\nmodel = linear_model.LinearRegression()\nmodel.fit(X, y)\n\nx = np.array(X[:, 1].A1)\nf = model.predict(X).flatten()\n\nfig, ax = plt.subplots(figsize=(12,8))\nax.plot(x, f, \'r\', label=\'Prediction\')\nax.scatter(data.Population, data.Profit, label=\'Traning Data\')\nax.legend(loc=2)\nax.set_xlabel(\'Population\')\nax.set_ylabel(\'Profit\')\nax.set_title(\'Predicted Profit vs. Population Size\')\nplt.show()\n\n# \xe6\xad\xa3\xe8\xa7\x84\xe6\x96\xb9\xe7\xa8\x8b\ndef normalEqn(X, y):\n    theta = np.linalg.inv(X.T@X)@X.T@y#X.T@X\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8eX.T.dot(X)\n    return theta\n\nfinal_theta2=normalEqn(X, y)#\xe6\x84\x9f\xe8\xa7\x89\xe5\x92\x8c\xe6\x89\xb9\xe9\x87\x8f\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe7\x9a\x84theta\xe7\x9a\x84\xe5\x80\xbc\xe6\x9c\x89\xe7\x82\xb9\xe5\xb7\xae\xe8\xb7\x9d\nprint(final_theta2)\n\n'"
机器学习 吴恩达 Python代码实现/ex1-linear regression 作业1 线性回归/linear_regreesion.py,11,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Mar  9 20:53:12 2019\n\n@author: liweimin\n""""""\n# ex1_1linear regreesion\xe5\xbd\x93\xe6\x9c\x89\xe6\x8e\xa8\xe5\xaf\xbc\xe5\xa5\xbd\xe7\x9a\x84\xe5\x85\xac\xe5\xbc\x8f\xe5\x90\x8e\xef\xbc\x8c\xe6\x9c\x89\xe5\xbe\x88\xe5\xa4\x9a\xe4\xb8\x9c\xe8\xa5\xbf\xe7\x9b\xb4\xe6\x8e\xa5\xe7\x85\xa7\xe7\x9d\x80\xe5\x85\xac\xe5\xbc\x8f\xe5\x86\x99\xe4\xbb\xa3\xe7\xa0\x81\xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xba\x86\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x8d\xe6\x98\xaf\xe5\xbe\x88\xe9\x9a\xbe\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\n\ndf = pd.read_csv(\'ex1data1.txt\', names=[\'population\', \'profit\'])#\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xe5\xb9\xb6\xe8\xb5\x8b\xe4\xba\x88\xe5\x88\x97\xe5\x90\x8d\nprint(df.head())#\xe6\x9f\xa5\xe7\x9c\x8b\xe5\x89\x8d5\xe8\xa1\x8c\nprint(df.info())#\xe6\x9f\xa5\xe7\x9c\x8b\xe7\x9b\xb8\xe5\x85\xb3\xe4\xbf\xa1\xe6\x81\xaf\nprint(df.describe())#\xe6\x9f\xa5\xe7\x9c\x8b\xe6\x95\xb0\xe6\x8d\xae\xe5\x9f\xba\xe6\x9c\xac\xe7\x89\xb9\xe5\xbe\x81\nsns.lmplot(\'population\', \'profit\', df, size=6, fit_reg=False)\nplt.show()#\xe7\x94\xbb\xe5\x87\xba\xe7\x9b\xb8\xe5\x85\xb3\xe6\x95\xb0\xe6\x8d\xae\n\ndef get_X(df):#\xe8\xaf\xbb\xe5\x8f\x96x\xe7\x89\xb9\xe5\xbe\x81\n    ones = pd.DataFrame({\'ones\': np.ones(len(df))})#ones\xe6\x98\xafm\xe8\xa1\x8c1\xe5\x88\x97\xe7\x9a\x84dataframe\n    data = pd.concat([ones, df], axis=1)  # \xe5\x90\x88\xe5\xb9\xb6\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xae\xe5\x88\x97\xe5\x90\x88\xe5\xb9\xb6\n    return data.iloc[:, :-1].as_matrix()  # \xe8\xbf\x99\xe4\xb8\xaa\xe6\x93\x8d\xe4\xbd\x9c\xe8\xbf\x94\xe5\x9b\x9e ndarray,\xe4\xb8\x8d\xe6\x98\xaf\xe7\x9f\xa9\xe9\x98\xb5\n\n\ndef get_y(df):#\xe8\xaf\xbb\xe5\x8f\x96y\xe6\xa0\x87\xe7\xad\xbe\n#\xe9\xbb\x98\xe8\xae\xa4\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\xe6\x98\xaf\xe7\x9b\xae\xe6\xa0\x87\xe5\x80\xbc\n    return np.array(df.iloc[:, -1])#df.iloc[:, -1]\xe6\x98\xaf\xe6\x8c\x87df\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\n\n\ndef normalize_feature(df):\n#\xe6\xb2\xbfDataFrame\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe8\xbd\xb4(\xe9\xbb\x98\xe8\xae\xa4\xe5\x80\xbc\xe4\xb8\xba0)\xe5\xba\x94\xe7\x94\xa8\xe5\x87\xbd\xe6\x95\xb0\n    return df.apply(lambda column: (column - column.mean()) / column.std())#\xe7\x89\xb9\xe5\xbe\x81\xe7\xbc\xa9\xe6\x94\xbe\n\ndef linear_regression(X_data, y_data, alpha, epoch, optimizer=tf.train.GradientDescentOptimizer):# \xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe6\x97\xa7\xe9\x87\x91\xe5\xb1\xb1\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xa4\xa7\xe7\xa5\x9eLucas Shen\xe5\x86\x99\xe7\x9a\x84\n      # \xe5\x9b\xbe\xe8\xbe\x93\xe5\x85\xa5\xe5\x8d\xa0\xe4\xbd\x8d\xe7\xac\xa6\n    X = tf.placeholder(tf.float32, shape=X_data.shape)\n    y = tf.placeholder(tf.float32, shape=y_data.shape)\n\n    # \xe6\x9e\x84\xe5\xbb\xba\xe5\x9b\xbe\n    with tf.variable_scope(\'linear-regression\'):\n        W = tf.get_variable(""weights"",\n                            (X_data.shape[1], 1),\n                            initializer=tf.constant_initializer())  # n*1\n\n        y_pred = tf.matmul(X, W)  # m*n @ n*1 -> m*1\n\n        loss = 1 / (2 * len(X_data)) * tf.matmul((y_pred - y), (y_pred - y), transpose_a=True)  # (m*1).T @ m*1 = 1*1\n        #\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\xae\x9a\xe4\xb9\x89\n    opt = optimizer(learning_rate=alpha)#\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n    opt_operation = opt.minimize(loss)#\xe6\x9c\x80\xe5\xb0\x8f\xe5\x8c\x96\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n\n    # \xe8\xbf\x90\xe8\xa1\x8csession\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        loss_data = []\n\n        for i in range(epoch):\n            _, loss_val, W_val = sess.run([opt_operation, loss, W], feed_dict={X: X_data, y: y_data})\n            loss_data.append(loss_val[0, 0])  # \xe5\x9b\xa0\xe4\xb8\xba\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\xe9\x83\xbd\xe6\x98\xaf1*1 ndarray\n\n            if len(loss_data) > 1 and np.abs(loss_data[-1] - loss_data[-2]) < 10 ** -9:  # \xe5\xbd\x93\xe6\xbb\xa1\xe8\xb6\xb3\xe6\x94\xb6\xe6\x95\x9b\xe6\x9d\xa1\xe4\xbb\xb6\xe6\x97\xb6\xe8\xb7\xb3\xe5\x87\xba\n                break\n\n    # \xe6\xb8\x85\xe7\x90\x86\xe5\x9b\xbe\n    tf.reset_default_graph()\n    return {\'loss\': loss_data, \'parameters\': W_val}  # \xe5\x8f\xaa\xe6\x83\xb3\xe4\xbb\xa5\xe8\xa1\x8c\xe5\x90\x91\xe9\x87\x8f\xe6\xa0\xbc\xe5\xbc\x8f\xe8\xbf\x94\xe5\x9b\x9e\n\n\ndata = pd.read_csv(\'ex1data1.txt\', names=[\'population\', \'profit\'])#\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\xb9\xb6\xe8\xb5\x8b\xe4\xba\x88\xe5\x88\x97\xe5\x90\x8d\n\nprint(data.head())#\xe7\x9c\x8b\xe4\xb8\x8b\xe6\x95\xb0\xe6\x8d\xae\xe5\x89\x8d5\xe8\xa1\x8c\nX = get_X(data)\nprint(X.shape, type(X))\n\ny = get_y(data)\nprint(y.shape, type(y))\n#\xe7\x9c\x8b\xe4\xb8\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\xb4\xe5\xba\xa6\ntheta = np.zeros(X.shape[1])#X.shape[1]=2,\xe4\xbb\xa3\xe8\xa1\xa8\xe7\x89\xb9\xe5\xbe\x81\xe6\x95\xb0n\ndef lr_cost(theta, X, y):\n#     """"""\n#     X: R(m*n), m \xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0, n \xe7\x89\xb9\xe5\xbe\x81\xe6\x95\xb0\n#     y: R(m)\n#     theta : R(n), \xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n#     """"""\n    m = X.shape[0]#m\xe4\xb8\xba\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\n\n    inner = X @ theta - y  # R(m*1)\xef\xbc\x8cX @ theta\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8eX.dot(theta)\n\n    # 1*m @ m*1 = 1*1 \xe7\x9f\xa9\xe9\x98\xb5\xe4\xb9\x98\xe6\xb3\x95\n    # numpy\xe6\xb2\xa1\xe6\x9c\x89\xe5\x9c\xa81d\xe6\x95\xb0\xe7\xbb\x84\xe4\xb8\xad\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe7\xbd\xae\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\n    # \xe5\x90\x91\xe9\x87\x8f\xe5\x86\x85\xe7\xa7\xaf\n    square_sum = inner.T @ inner\n    cost = square_sum / (2 * m)\n\n    return cost\nprint(lr_cost(theta, X, y))#\xe8\xbf\x94\xe5\x9b\x9etheta\xe7\x9a\x84\xe5\x80\xbc\ndef gradient(theta, X, y):\n    m = X.shape[0]\n\n    inner = X.T @ (X @ theta - y)  # (m,n).T @ (m, 1) -> (n, 1)\xef\xbc\x8cX @ theta\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8eX.dot(theta)\n\n    return inner / m\ndef batch_gradient_decent(theta, X, y, epoch, alpha=0.01):\n#   \xe6\x8b\x9f\xe5\x90\x88\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe5\x8f\x82\xe6\x95\xb0\xe5\x92\x8c\xe4\xbb\xa3\xe4\xbb\xb7\n#     epoch: \xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe8\xbd\xae\xe6\x95\xb0\n#     """"""\n    cost_data = [lr_cost(theta, X, y)]\n    _theta = theta.copy()  # \xe6\x8b\xb7\xe8\xb4\x9d\xe4\xb8\x80\xe4\xbb\xbd\xef\xbc\x8c\xe4\xb8\x8d\xe5\x92\x8c\xe5\x8e\x9f\xe6\x9d\xa5\xe7\x9a\x84theta\xe6\xb7\xb7\xe6\xb7\x86\n\n    for _ in range(epoch):\n        _theta = _theta - alpha * gradient(_theta, X, y)\n        cost_data.append(lr_cost(_theta, X, y))\n\n    return _theta, cost_data\n#\xe6\x89\xb9\xe9\x87\x8f\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe5\x87\xbd\xe6\x95\xb0\nepoch = 500\nfinal_theta, cost_data = batch_gradient_decent(theta, X, y, epoch)\nprint (final_theta)\n#\xe6\x9c\x80\xe7\xbb\x88\xe7\x9a\x84theta\nprint (cost_data)\n# \xe7\x9c\x8b\xe4\xb8\x8b\xe4\xbb\xa3\xe4\xbb\xb7\xe6\x95\xb0\xe6\x8d\xae\n# \xe8\xae\xa1\xe7\xae\x97\xe6\x9c\x80\xe7\xbb\x88\xe7\x9a\x84\xe4\xbb\xa3\xe4\xbb\xb7\nprint (lr_cost(final_theta, X, y))\nax = sns.tsplot(cost_data, time=np.arange(epoch+1))\nax.set_xlabel(\'epoch\')\nax.set_ylabel(\'cost\')\nplt.show()\n#\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe4\xbb\x8e\xe7\xac\xac\xe4\xba\x8c\xe8\xbd\xae\xe4\xbb\xa3\xe4\xbb\xb7\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x98\xe6\x8d\xa2\xe5\xbe\x88\xe5\xa4\xa7\xef\xbc\x8c\xe6\x8e\xa5\xe4\xb8\x8b\xe6\x9d\xa5\xe5\xb9\xb3\xe7\xa8\xb3\xe4\xba\x86\nb = final_theta[0] # intercept\xef\xbc\x8cY\xe8\xbd\xb4\xe4\xb8\x8a\xe7\x9a\x84\xe6\x88\xaa\xe8\xb7\x9d\nm = final_theta[1] # slope\xef\xbc\x8c\xe6\x96\x9c\xe7\x8e\x87\n\nplt.scatter(data.population, data.profit, label=""Training data"")\nplt.plot(data.population, data.population*m + b, label=""Prediction"")\nplt.legend(loc=2)\nplt.show()\n'"
机器学习 吴恩达 Python代码实现/ex1-linear regression 作业1 线性回归/linear_regreesion_pro.py,16,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Mar  9 22:02:25 2019\n\n@author: liweimin\n""""""\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\n\nraw_data = pd.read_csv(\'ex1data2.txt\', names=[\'square\', \'bedrooms\', \'price\'])\nraw_data.head()\n\ndef normalize_feature(df):\n    return df.apply(lambda column: (column - column.mean()) / column.std())\n\ndef get_X(df):#\xe8\xaf\xbb\xe5\x8f\x96\xe7\x89\xb9\xe5\xbe\x81\n    ones = pd.DataFrame({\'ones\': np.ones(len(df))})#ones\xe6\x98\xafm\xe8\xa1\x8c1\xe5\x88\x97\xe7\x9a\x84dataframe\n    data = pd.concat([ones, df], axis=1)  # \xe5\x90\x88\xe5\xb9\xb6\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xae\xe5\x88\x97\xe5\x90\x88\xe5\xb9\xb6\n    return data.iloc[:, :-1].as_matrix()  # \xe8\xbf\x99\xe4\xb8\xaa\xe6\x93\x8d\xe4\xbd\x9c\xe8\xbf\x94\xe5\x9b\x9e ndarray,\xe4\xb8\x8d\xe6\x98\xaf\xe7\x9f\xa9\xe9\x98\xb5\n\n\ndef get_y(df):#\xe8\xaf\xbb\xe5\x8f\x96\xe6\xa0\x87\xe7\xad\xbe\n    return np.array(df.iloc[:, -1])#df.iloc[:, -1]\xe6\x98\xaf\xe6\x8c\x87df\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\n\ndef lr_cost(theta, X, y):\n#     """"""\n#     X: R(m*n), m \xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0, n \xe7\x89\xb9\xe5\xbe\x81\xe6\x95\xb0\n#     y: R(m)\n#     theta : R(n), \xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n#     """"""\n    m = X.shape[0]#m\xe4\xb8\xba\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\n\n    inner = X @ theta - y  # R(m*1)\xef\xbc\x8cX @ theta\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8eX.dot(theta)\n    square_sum = inner.T @ inner\n    cost = square_sum / (2 * m)\n    return cost\n\ndef gradient(theta, X, y):\n    m = X.shape[0]\n\n    inner = X.T @ (X @ theta - y)  # (m,n).T @ (m, 1) -> (n, 1)\xef\xbc\x8cX @ theta\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8eX.dot(theta)\n\n    return inner / m\n\ndef batch_gradient_decent(theta, X, y, epoch, alpha=0.01):\n#   \xe6\x8b\x9f\xe5\x90\x88\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe5\x8f\x82\xe6\x95\xb0\xe5\x92\x8c\xe4\xbb\xa3\xe4\xbb\xb7\n#     epoch: \xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe8\xbd\xae\xe6\x95\xb0\n    cost_data = [lr_cost(theta, X, y)]\n    _theta = theta.copy()  # \xe6\x8b\xb7\xe8\xb4\x9d\xe4\xb8\x80\xe4\xbb\xbd\xef\xbc\x8c\xe4\xb8\x8d\xe5\x92\x8c\xe5\x8e\x9f\xe6\x9d\xa5\xe7\x9a\x84theta\xe6\xb7\xb7\xe6\xb7\x86\n\n    for _ in range(epoch):\n        _theta = _theta - alpha * gradient(_theta, X, y)\n        cost_data.append(lr_cost(_theta, X, y))\n\n    return _theta, cost_data\n#\xe6\x89\xb9\xe9\x87\x8f\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe5\x87\xbd\xe6\x95\xb0\n\ndef linear_regression(X_data, y_data, alpha, epoch, optimizer=tf.train.GradientDescentOptimizer):# \xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe6\x97\xa7\xe9\x87\x91\xe5\xb1\xb1\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xa4\xa7\xe7\xa5\x9eLucas Shen\xe5\x86\x99\xe7\x9a\x84\n    X = tf.placeholder(tf.float32, shape=X_data.shape)\n    y = tf.placeholder(tf.float32, shape=y_data.shape)\n    \n    with tf.variable_scope(\'linear-regression\'):\n        W = tf.get_variable(""weights"",\n                            (X_data.shape[1], 1),\n                            initializer=tf.constant_initializer())  # n*1\n\n        y_pred = tf.matmul(X, W)  # m*n @ n*1 -> m*1\n\n        loss = 1 / (2 * len(X_data)) * tf.matmul((y_pred - y), (y_pred - y), transpose_a=True)  # (m*1).T @ m*1 = 1*1\n\n    opt = optimizer(learning_rate=alpha)\n    opt_operation = opt.minimize(loss)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        loss_data = []\n\n        for i in range(epoch):\n            _, loss_val, W_val = sess.run([opt_operation, loss, W], feed_dict={X: X_data, y: y_data})\n            loss_data.append(loss_val[0, 0])  \n\n            if len(loss_data) > 1 and np.abs(loss_data[-1] - loss_data[-2]) < 10 ** -9:  \n                break\n    tf.reset_default_graph()\n    return {\'loss\': loss_data, \'parameters\': W_val}  # just want to return in row vector format\n\ndata = normalize_feature(raw_data)\ndata.head()\nX = get_X(data)\nprint(X.shape, type(X))\n\ny = get_y(data)\nprint(y.shape, type(y))#\xe7\x9c\x8b\xe4\xb8\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe5\x92\x8c\xe7\xb1\xbb\xe5\x9e\x8b\n\nalpha = 0.01#\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\ntheta = np.zeros(X.shape[1])#X.shape[1]\xef\xbc\x9a\xe7\x89\xb9\xe5\xbe\x81\xe6\x95\xb0n\nepoch = 500#\xe8\xbd\xae\xe6\x95\xb0\n\nfinal_theta, cost_data = batch_gradient_decent(theta, X, y, epoch, alpha=alpha)#\xe4\xb8\x8d\xe6\x98\x8ebug\n\nsns.tsplot(time=np.arange(len(cost_data)), data = cost_data)\nplt.xlabel(\'epoch\', fontsize=18)\nplt.ylabel(\'cost\', fontsize=18)\nplt.show()\n\nfinal_theta\nbase = np.logspace(-1, -5, num=4)\ncandidate = np.sort(np.concatenate((base, base*3)))\nprint(candidate)\nepoch=50\n\nfig, ax = plt.subplots(figsize=(16, 9))\n\nfor alpha in candidate:\n    _, cost_data = batch_gradient_decent(theta, X, y, epoch, alpha=alpha)\n    ax.plot(np.arange(epoch+1), cost_data, label=alpha)\n\nax.set_xlabel(\'epoch\', fontsize=18)\nax.set_ylabel(\'cost\', fontsize=18)\nax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nax.set_title(\'learning rate\', fontsize=18)\nplt.show()\n# \xe6\xad\xa3\xe8\xa7\x84\xe6\x96\xb9\xe7\xa8\x8b\ndef normalEqn(X, y):\n    theta = np.linalg.inv(X.T@X)@X.T@y#X.T@X\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8eX.T.dot(X)\n    return theta\nfinal_theta2=normalEqn(X, y)#\xe6\x84\x9f\xe8\xa7\x89\xe5\x92\x8c\xe6\x89\xb9\xe9\x87\x8f\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe7\x9a\x84theta\xe7\x9a\x84\xe5\x80\xbc\xe6\x9c\x89\xe7\x82\xb9\xe5\xb7\xae\xe8\xb7\x9d\nfinal_theta2\n\nX_data = get_X(data)\nprint(X_data.shape, type(X_data))\n\ny_data = get_y(data).reshape(len(X_data), 1)  # special treatment for tensorflow input data\nprint(y_data.shape, type(y_data))\n\nepoch = 2000\nalpha = 0.01\n\noptimizer_dict={\'GD\': tf.train.GradientDescentOptimizer,\n                \'Adagrad\': tf.train.AdagradOptimizer,\n                \'Adam\': tf.train.AdamOptimizer,\n                \'Ftrl\': tf.train.FtrlOptimizer,\n                \'RMS\': tf.train.RMSPropOptimizer\n               }\nresults = []\nfor name in optimizer_dict:\n    res = linear_regression(X_data, y_data, alpha, epoch, optimizer=optimizer_dict[name])\n    res[\'name\'] = name\n    results.append(res)\n    \nfig, ax = plt.subplots(figsize=(16, 9))\n\nfor res in results: \n    loss_data = res[\'loss\']\n    ax.plot(np.arange(len(loss_data)), loss_data, label=res[\'name\'])\n\nax.set_xlabel(\'epoch\', fontsize=18)\nax.set_ylabel(\'cost\', fontsize=18)\nax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nax.set_title(\'different optimizer\', fontsize=18)\nplt.show()\n\n'"
机器学习 吴恩达 Python代码实现/ex2-logistic regression 作业2 logistic回归/IrisClassification_byLogisticRegression_v2.py,0,"b'# !/usr/bin/env python\n# encoding: utf-8\n#author :\'liweimin\'\n# Version : IrisClassification_byLogisticRegression_new3\n\n# 1.Choosing a proper method to classify the IRIS dataset.\n# 2.Visualizing raw data and classified data, respectively.\n# 3.Submitting with executable codes.\n# 4.Submitting a document with the algorithm description and results in detail.\n# 5.Submiting to email: XMUML2018@163.com\n# 6.The deadline is 2018.04.16.\n\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport itertools as it\nimport matplotlib as mpl\nfrom matplotlib import colors\n\n# LogisticRegression\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe4\xbc\xa0\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x88\xe5\x8c\x85\xe6\x8b\xac\xe7\x89\xb9\xe5\xbe\x81\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x8a\xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x89\xef\xbc\x8c\xe7\xbb\x93\xe6\x9e\x9c\xe8\xbf\x94\xe5\x9b\x9e\xe8\xae\xad\xe7\xbb\x83\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0 W\ndef LogRegressionAlgorithm(datas,labels):\n    kinds = list(set(labels))  # 3\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\xe5\x88\x97\xe8\xa1\xa8\n    means=datas.mean(axis=0) #\xe5\x90\x84\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\n    stds=datas.std(axis=0) #\xe5\x90\x84\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\n    N,M= datas.shape[0],datas.shape[1]+1  #N\xe6\x98\xaf\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xef\xbc\x8cM\xe6\x98\xaf\xe5\x8f\x82\xe6\x95\xb0\xe5\x90\x91\xe9\x87\x8f\xe7\x9a\x84\xe7\xbb\xb4\n    K=3 #k=3\xe6\x98\xaf\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\n\n    data=np.ones((N,M))\n    data[:,1:]=(datas-means)/stds #\xe5\xaf\xb9\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\n\n    W=np.zeros((K-1,M))  #\xe5\xad\x98\xe5\x82\xa8\xe5\x8f\x82\xe6\x95\xb0\xe7\x9f\xa9\xe9\x98\xb5\n    priorEs=np.array([1.0/N*np.sum(data[labels==kinds[i]],axis=0) for i in range(K-1)]) #\xe5\x90\x84\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe5\x85\x88\xe9\xaa\x8c\xe6\x9c\x9f\xe6\x9c\x9b\xe5\x80\xbc\n\n    liklist=[]\n    for it in range(1000):\n        lik=0 #\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84\xe5\xaf\xb9\xe6\x95\xb0\xe4\xbc\xbc\xe7\x84\xb6\xe5\x87\xbd\xe6\x95\xb0\xe5\x80\xbc\n        for k in range(K-1): #\xe4\xbc\xbc\xe7\x84\xb6\xe5\x87\xbd\xe6\x95\xb0\xe5\x80\xbc\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\n            lik -= np.sum(np.dot(W[k],data[labels==kinds[k]].transpose()))\n        lik +=1.0/N *np.sum(np.log(np.sum(np.exp(np.dot(W,data.transpose())),axis=0)+1)) #\xe4\xbc\xbc\xe7\x84\xb6\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe7\xac\xac\xe4\xba\x8c\xe9\x83\xa8\xe5\x88\x86\n        liklist.append(lik)\n\n        wx=np.exp(np.dot(W,data.transpose()))\n        probs=np.divide(wx,1+np.sum(wx,axis=0).transpose()) # K-1 *N\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\n        posteriorEs=1.0/N*np.dot(probs,data) #\xe5\x90\x84\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe5\x90\x8e\xe9\xaa\x8c\xe6\x9c\x9f\xe6\x9c\x9b\xe5\x80\xbc\n        gradients=posteriorEs - priorEs +1.0/100 *W #\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe9\xa1\xb9\xe6\x98\xaf\xe9\xab\x98\xe6\x96\xaf\xe9\xa1\xb9\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\n        W -= gradients #\xe5\xaf\xb9\xe5\x8f\x82\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xbf\xae\xe6\xad\xa3\n    print(""\xe8\xbe\x93\xe5\x87\xbaW\xe4\xb8\xba\xef\xbc\x9a"",W)\n    return W\n\n#\xe6\xa0\xb9\xe6\x8d\xae\xe8\xae\xad\xe7\xbb\x83\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0W\xe5\x92\x8c\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe3\x80\x82\xe8\xbe\x93\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x92\x8c\xe7\x94\xb1LogisticRegression\xe7\xae\x97\xe6\xb3\x95\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0W\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xe4\xb8\xba\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x80\xbc\ndef predict_fun(datas,W):\n    N, M = datas.shape[0], datas.shape[1] + 1  # N\xe6\x98\xaf\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xef\xbc\x8cM\xe6\x98\xaf\xe5\x8f\x82\xe6\x95\xb0\xe5\x90\x91\xe9\x87\x8f\xe7\x9a\x84\xe7\xbb\xb4\n    K = 3  # k=3\xe6\x98\xaf\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\n    data = np.ones((N, M))\n    means = datas.mean(axis=0)  # \xe5\x90\x84\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\n    stds = datas.std(axis=0)  # \xe5\x90\x84\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\n    data[:, 1:] = (datas - means) / stds  # \xe5\xaf\xb9\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\n\n    # probM\xe6\xaf\x8f\xe8\xa1\x8c\xe4\xb8\x89\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xef\xbc\x8c\xe5\x88\x86\xe5\x88\xab\xe8\xa1\xa8\xe7\xa4\xbadata\xe4\xb8\xad\xe5\xaf\xb9\xe5\xba\x94\xe6\xa0\xb7\xe6\x9c\xac\xe8\xa2\xab\xe5\x88\xa4\xe7\xbb\x99\xe4\xb8\x89\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\n    probM = np.ones((N, K))\n    print(""data.shape:"", data.shape)\n    print(""datas.shape:"", datas.shape)\n    print(""W.shape:"", W.shape)\n    print(""probM.shape:"", probM.shape)\n    probM[:, :-1] = np.exp(np.dot(data, W.transpose()))\n    probM /= np.array([np.sum(probM, axis=1)]).transpose()  # \xe5\xbe\x97\xe5\x88\xb0\xe6\xa6\x82\xe7\x8e\x87\n\n    predict = np.argmax(probM, axis=1).astype(int)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe6\xa6\x82\xe7\x8e\x87\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\n    print(""\xe8\xbe\x93\xe5\x87\xbapredict\xe4\xb8\xba\xef\xbc\x9a"", predict)\n    return predict\n\n\nif __name__ == \'__main__\':\n    # data_visualization()\n    # LogRegressionAlgorithm()\n    # rights, predict, kinds=LogRegressionAlgorithm()\n    # confusion_matrix(rights, predict, kinds)\n    attributes=[\'SepalLength\',\'SepalWidth\',\'PetalLength\',\'PetalWidth\'] #\xe9\xb8\xa2\xe5\xb0\xbe\xe8\x8a\xb1\xe7\x9a\x84\xe5\x9b\x9b\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe5\x90\x8d\n\n    datas=[]\n    labels=[]\n\n    # with open(\'IRIS_dataset.txt\',\'r\') as f:\n    #     for line in f:\n    #         linedata=line.split(\',\')\n    #         datas.append(linedata[:-1]) #\xe5\x89\x8d4\xe5\x88\x97\xe6\x98\xaf4\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe5\x80\xbc\n    #         labels.append(linedata[-1].replace(\'\\n\',\'\')) #\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\xe6\x98\xaf\xe7\xb1\xbb\xe5\x88\xab\n\n    #\xe8\xaf\xbb\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x9a\n    data_file=open(\'IRIS_dataset.txt\',\'r\')\n    for line in data_file.readlines():\n        # print(line)\n        linedata = line.split(\',\')\n        # datas.append(linedata[:-1])  # \xe5\x89\x8d4\xe5\x88\x97\xe6\x98\xaf4\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe5\x80\xbc(\xe8\xaf\xaf\xe5\x88\xa4\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\xba\xef\xbc\x9a7\n        datas.append(linedata[:-3])  # \xe5\x89\x8d2\xe5\x88\x97\xe6\x98\xaf2\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe5\x80\xbc(\xe8\xaf\xaf\xe5\x88\xa4\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\xba\xef\xbc\x9a30\n        labels.append(linedata[-1].replace(\'\\n\', \'\'))  # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\xe6\x98\xaf\xe7\xb1\xbb\xe5\x88\xab\n\n    datas=np.array(datas)\n    datas=datas.astype(float) #\xe5\xb0\x86\xe4\xba\x8c\xe7\xbb\xb4\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe6\x95\xb0\xe7\xbb\x84\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe6\xb5\xae\xe7\x82\xb9\xe6\x95\xb0\xe6\x95\xb0\xe7\xbb\x84\n    labels=np.array(labels)\n    kinds=list(set(labels)) #3\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\xe5\x88\x97\xe8\xa1\xa8\n\n    #\xe9\x80\x9a\xe8\xbf\x87LogisticRegression\xe7\xae\x97\xe6\xb3\x95\xe5\xbe\x97\xe5\x88\xb0\xe5\x8f\x82\xe6\x95\xb0 W\n    W=LogRegressionAlgorithm(datas,labels)\n\n    #\xe9\x80\x9a\xe8\xbf\x87\xe9\xa2\x84\xe6\xb5\x8b\xe5\x87\xbd\xe6\x95\xb0predict_fun\xef\xbc\x88\xef\xbc\x89\xe5\x87\xbd\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\n    predict=predict_fun(datas,W)\n\n    # rights \xe5\x88\x97\xe8\xa1\xa8\xe5\x82\xa8\xe5\xad\x98\xe4\xbb\xa3\xe8\xa1\xa8\xe5\x8e\x9f\xe5\xa7\x8b\xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xaelabels \xe6\x95\xb0\xe6\x8d\xae\xe7\x94\x9f\xe6\x88\x90\n    N = datas.shape[0]\n    rights = np.zeros(N)\n    rights[labels == kinds[1]] = 1\n    rights[labels == kinds[2]] = 2\n    rights = rights.astype(int)\n    # \xe8\xaf\xaf\xe5\x88\xa4\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\n    print(""\xe8\xaf\xaf\xe5\x88\xa4\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\xba\xef\xbc\x9a%d\\n"" % np.sum(predict != rights))\n    # print(""LR\xe5\x88\x86\xe7\xb1\xbb\xe5\x99\xa8\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\xba\xef\xbc\x9a%f\\n""%(int(np.sum(predict !=rights)/int(N))))\n\n\n    # \xe7\xbb\x98\xe5\x88\xb6\xe5\x9b\xbe\xe5\x83\x8f-------------------------------------------------------\n    # 1.\xe7\xa1\xae\xe5\xae\x9a\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe8\x8c\x83\xe5\x9b\xb4\xef\xbc\x8cx\xef\xbc\x8cy\xe8\xbd\xb4\xe5\x88\x86\xe5\x88\xab\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81\n    x1_min, x1_max = datas[:, 0].min(), datas[:, 0].max()  # \xe7\xac\xac0\xe5\x88\x97\xe7\x9a\x84\xe8\x8c\x83\xe5\x9b\xb4\n    x2_min, x2_max = datas[:, 1].min(), datas[:, 1].max()  # \xe7\xac\xac1\xe5\x88\x97\xe7\x9a\x84\xe8\x8c\x83\xe5\x9b\xb4\n    x1, x2 = np.mgrid[x1_min:x1_max:150j, x2_min:x2_max:150j]  # \xe7\x94\x9f\xe6\x88\x90\xe7\xbd\x91\xe6\xa0\xbc\xe9\x87\x87\xe6\xa0\xb7\xe7\x82\xb9\xef\xbc\x8c\xe6\xa8\xaa\xe8\xbd\xb4\xe4\xb8\xba\xe5\xb1\x9e\xe6\x80\xa7x1\xef\xbc\x8c\xe7\xba\xb5\xe8\xbd\xb4\xe4\xb8\xba\xe5\xb1\x9e\xe6\x80\xa7x2\n    grid_test = np.stack((x1.flat, x2.flat), axis=1)  # \xe6\xb5\x8b\xe8\xaf\x95\xe7\x82\xb9\n    #.flat \xe5\x87\xbd\xe6\x95\xb0\xe5\xb0\x86\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x9f\xa9\xe9\x98\xb5\xe9\x83\xbd\xe5\x8f\x98\xe6\x88\x90\xe4\xb8\xa4\xe4\xb8\xaa\xe4\xb8\x80\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x8c\xe8\xb0\x83\xe7\x94\xa8stack\xe5\x87\xbd\xe6\x95\xb0\xe7\xbb\x84\xe5\x90\x88\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe4\xba\x8c\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\n    print(""grid_test = \\n"", grid_test)\n\n    grid_hat = predict_fun(grid_test,W)  # \xe9\xa2\x84\xe6\xb5\x8b\xe5\x88\x86\xe7\xb1\xbb\xe5\x80\xbc\n    grid_hat = grid_hat.reshape(x1.shape)  # \xe4\xbd\xbf\xe4\xb9\x8b\xe4\xb8\x8e\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe7\x9b\xb8\xe5\x90\x8c\n    #grid_hat\xe6\x9c\xac\xe6\x9d\xa5\xe6\x98\xaf\xe4\xb8\x80\xe5\x94\xaf\xe7\x9a\x84\xef\xbc\x8c\xe8\xb0\x83\xe7\x94\xa8reshape()\xe5\x87\xbd\xe6\x95\xb0\xe4\xbf\xae\xe6\x94\xb9\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x8c\xe5\xb0\x86\xe5\x85\xb6grid_hat\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x88\xe9\x95\xbf\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\xef\xbc\x89\n    print(""grid_hat = \\n"", grid_hat)\n    print(""grid_hat.shape: = \\n"", grid_hat.shape) # (150, 150)\n    # 2.\xe6\x8c\x87\xe5\xae\x9a\xe9\xbb\x98\xe8\xae\xa4\xe5\xad\x97\xe4\xbd\x93\n    mpl.rcParams[\'font.sans-serif\'] = [u\'SimHei\']\n    mpl.rcParams[\'axes.unicode_minus\'] = False\n\n    # 3.\xe7\xbb\x98\xe5\x88\xb6\xe5\x9b\xbe\xe5\x83\x8f\n    cm_light = mpl.colors.ListedColormap([\'#A0FFA0\', \'#FFA0A0\', \'#A0A0FF\'])\n    cm_dark = mpl.colors.ListedColormap([\'g\', \'r\', \'b\'])\n\n    alpha = 0.5\n\n    plt.pcolormesh(x1, x2, grid_hat, cmap=plt.cm.Paired)  # \xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\xe7\x9a\x84\xe6\x98\xbe\xe7\xa4\xba\n    # \xe8\xb0\x83\xe7\x94\xa8pcolormesh()\xe5\x87\xbd\xe6\x95\xb0\xe5\xb0\x86x1\xe3\x80\x81x2\xe4\xb8\xa4\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe7\x9f\xa9\xe9\x98\xb5\xe5\x92\x8c\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9cgrid_hat\xe7\xbb\x98\xe5\x88\xb6\xe5\x9c\xa8\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8a\n    # \xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8f\x91\xe7\x8e\xb0\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\xba\xe4\xb8\x89\xe4\xb8\xaa\xe9\xa2\x9c\xe8\x89\xb2\xe5\x8c\xba\xe5\x9d\x97\xef\xbc\x8c\xe5\x88\x86\xe5\xb8\x83\xe8\xa1\xa8\xe7\xa4\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe4\xb8\x89\xe7\xb1\xbb\xe5\x8c\xba\xe5\x9f\x9f\xe3\x80\x82cmap=plt.cm.Paired/cmap=cm_light\xe8\xa1\xa8\xe7\xa4\xba\xe7\xbb\x98\xe5\x9b\xbe\xe6\xa0\xb7\xe5\xbc\x8f\xe9\x80\x89\xe6\x8b\xa9Paired\xe4\xb8\xbb\xe9\xa2\x98\n    # plt.scatter(datas[:, 0], datas[:, 1], c=labels, edgecolors=\'k\', s=50, cmap=cm_dark)  # \xe6\xa0\xb7\xe6\x9c\xac\n    plt.plot(datas[:, 0], datas[:, 1], \'o\', alpha=alpha, color=\'blue\', markeredgecolor=\'k\')\n    ##\xe7\xbb\x98\xe5\x88\xb6\xe6\x95\xa3\xe7\x82\xb9\xe5\x9b\xbe\n    plt.scatter(datas[:, 0], datas[:, 1], s=120, facecolors=\'none\', zorder=10)  # \xe5\x9c\x88\xe4\xb8\xad\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe6\xa0\xb7\xe6\x9c\xac\n    plt.xlabel(u\'\xe8\x8a\xb1\xe8\x90\xbc\xe9\x95\xbf\xe5\xba\xa6\', fontsize=13)  #X\xe8\xbd\xb4\xe6\xa0\x87\xe7\xad\xbe\n    plt.ylabel(u\'\xe8\x8a\xb1\xe8\x90\xbc\xe5\xae\xbd\xe5\xba\xa6\', fontsize=13)  #Y\xe8\xbd\xb4\xe6\xa0\x87\xe7\xad\xbe\n    plt.xlim(x1_min, x1_max) # x \xe8\xbd\xb4\xe8\x8c\x83\xe5\x9b\xb4\n    plt.ylim(x2_min, x2_max) # y \xe8\xbd\xb4\xe8\x8c\x83\xe5\x9b\xb4\n    plt.title(u\'\xe9\xb8\xa2\xe5\xb0\xbe\xe8\x8a\xb1LogisticRegression\xe4\xba\x8c\xe7\x89\xb9\xe5\xbe\x81\xe5\x88\x86\xe7\xb1\xbb\', fontsize=15)\n    # plt.legend(loc=2)  # \xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe7\xbb\x98\xe5\x88\xb6\xe5\x9b\xbe\xe6\xa0\x87\n    # plt.grid()\n    plt.show()\n    # -------------------------------------------------------\n\n\n\n##\xe6\xb3\xa8\xef\xbc\x9a\n# 1\xe3\x80\x81scatter\n# \xe7\x94\xa8\xe6\x9d\xa5\xe7\x94\xbb\xe6\x95\xa3\xe7\x82\xb9\xe5\x9b\xbe\xe7\x9a\x84\xef\xbc\x8c\xe5\xaf\xb9\xe6\xa0\xb7\xe6\x9c\xac\xe7\x82\xb9\xe7\x9d\x80\xe8\x89\xb2\xe3\x80\x82\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9aX\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaan*2\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x8c\xe4\xbb\xa3\xe8\xa1\xa8n\xe4\xb8\xaa2\xe7\xbb\xb4\xe6\xa0\xb7\xe6\x9c\xac\xe7\x82\xb9\xef\xbc\x8c\xe4\xb8\x94\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x82\xb9\xe5\xaf\xb9\xe5\xba\x94\xe4\xb8\x80\xe4\xb8\xaalabel y\xef\xbc\x8c\xe7\x94\xa8y\xe6\x9d\xa5\xe5\xaf\xb9\xe9\xa2\x9c\xe8\x89\xb2\xe5\x8f\x98\xe9\x87\x8fc\xe8\xb5\x8b\xe5\x80\xbc\xe6\x9d\xa5\xe5\x8c\xba\xe5\x88\x86\xe9\xa2\x9c\xe8\x89\xb2\xef\xbc\x8c\xe6\x8c\x89\xe7\x85\xa7cmap\xe6\x9d\xa5\xe5\xb8\x83\xe5\xb1\x80\xe3\x80\x82\n# plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired)\n\n# 2\xe3\x80\x81pcolormesh\n# \xe7\x94\xa8\xe6\xb3\x95\xef\xbc\x9a\xe7\xb1\xbb\xe4\xbc\xbcnp.pcolor \xef\xbc\x8c\xe6\x98\xaf\xe5\xaf\xb9\xe5\x9d\x90\xe6\xa0\x87\xe7\x82\xb9\xe7\x9d\x80\xe8\x89\xb2\xe3\x80\x82\n# np.pcolormesh(X, Y, C, **kwargs)\n# \xe4\xbe\x8b\xe5\xa6\x82\xe6\x9c\x89\xe6\xa0\xb7\xe6\x9c\xac\xe7\x82\xb9\xef\xbc\x88X[i\xef\xbc\x8cj] , Y[i\xef\xbc\x8cj]\xef\xbc\x89\xef\xbc\x8c\xe5\xaf\xb9\xe6\xa0\xb7\xe6\x9c\xac\xe5\x91\xa8\xe5\x9b\xb4\xef\xbc\x88\xe5\x8c\x85\xe6\x8b\xac\xe6\xa0\xb7\xe6\x9c\xac\xe6\x89\x80\xe5\x9c\xa8\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x89\xe7\x9a\x84\xe5\x9b\x9b\xe4\xb8\xaa\xe5\x9d\x90\xe6\xa0\x87\xe7\x82\xb9\xe8\xbf\x9b\xe8\xa1\x8c\xe7\x9d\x80\xe8\x89\xb2\xef\xbc\x8cC\xe4\xbb\xa3\xe8\xa1\xa8\xe7\x9d\x80\xe8\x89\xb2\xe6\x96\xb9\xe6\xa1\x88\xef\xbc\x8ckwargs\xe9\x87\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xbe\xe7\xbd\xae\xe7\x9d\x80\xe8\x89\xb2\xe9\x85\x8d\xe7\xbd\xae\xe3\x80\x82\n# (X[i,   j],   Y[i,   j]),\n# (X[i,   j+1], Y[i,   j+1]),\n# (X[i+1, j],   Y[i+1, j]),\n# (X[i+1, j+1], Y[i+1, j+1]).\n# \xe6\xa0\xb7\xe4\xbe\x8b\xef\xbc\x9aplt.pcolormesh(XX, YY, Z>0, cmap=plt.cm.Paired)\n\n#\xe7\xbb\x98\xe5\x88\xb6\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe4\xbb\xa3\xe7\xa0\x81\xe5\x8f\xaa\xe9\x80\x82\xe7\x94\xa8\xe4\xba\x8e\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x8c\xe7\x89\xb9\xe5\xbe\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xe3\x80\x82\n#\xe5\xa6\x82\xe6\x9e\x9c\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8\xe5\x9b\x9b\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe5\x88\x99\xe9\x9c\x80\xe8\xa6\x81\xe5\xb0\x86\xe2\x80\x9c\xe7\xbb\x98\xe5\x88\xb6\xe5\x9b\xbe\xe5\x83\x8f\xe2\x80\x9d\xe4\xbb\xa3\xe7\xa0\x81\xe6\xb3\xa8\xe9\x87\x8a\xe6\x8e\x89\xe6\x89\x8d\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xbf\x90\xe8\xa1\x8c\xe6\x88\x90\xe5\x8a\x9f\xe3\x80\x82'"
机器学习实战 Python代码实现/RBF信用预测/RBF_test315.py,0,"b'""""""\nCreated on Sat Mar  9 22:02:25 2019\n\n@author: liweimin\n""""""\n\n#import turtle\n#import random\n#import time\n#import jieba\n#import numpy as np\n#import matplotlib.pyplot as plt\n#import pandas as pd\n#import sklearn\n#import seaborn as sns \n#import tensorflow as tf \n#\n#\n#A=np.array([[56,0.0,4.4,68.0],\n#           [1.2,104,52,8],\n#           [1.8,135,99,0.9]])\n#\n#print(A)\n#\n#\n#cal=A.sum(axis=0)\n#print(cal)\n#\n#per=100*A/cal\n#print(per)\n\n\n\n\nfrom scipy import *\nfrom scipy.linalg import norm, pinv\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.cross_validation import train_test_split\nimport pandas as pd\n\nclass RBF:\n    def __init__(self, indim, numCenters, outdim): \n        self.indim = indim\n        self.outdim = outdim\n        self.numCenters = numCenters\n        self.centers = [random.uniform(-1, 1, indim) for i in range(numCenters)]\n        self.beta = 8\n        self.W = random.random((self.numCenters, self.outdim))\n\n    def _basisfunc(self, c, d):\n        assert len(d) == self.indim\n        return exp(-self.beta * norm(c-d)**2)\n\n    def _calcAct(self, X):\n # calculate activations of RBFs\n        G = zeros((X.shape[0], self.numCenters), float)\n        for ci, c in enumerate(self.centers):\n            for xi, x in enumerate(X):\n                G[xi,ci] = self._basisfunc(c, x)\n        return G\n\n    def train(self, X, Y):\n        """""" X: matrix of dimensions n x indim\n        y: column vector of dimension n x 1 """"""\n # choose random center vectors from training set\n        rnd_idx = random.permutation(X.shape[0])[:self.numCenters]\n        self.centers = [X[i,:] for i in rnd_idx]\n        print(""center"", self.centers)\n # calculate activations of RBFs\n        G = self._calcAct(X)\n        print(G)\n # calculate output weights (pseudoinverse)\n        self.W = dot(pinv(G), Y)\n\n    def test(self, X):\n        """""" X: matrix of dimensions n x indim """"""\n        rnd_idx = random.permutation(X.shape[0])[:self.numCenters]\n        self.centers = [X[i,:] for i in rnd_idx]\n        G = self._calcAct(X)\n        Y = dot(G, self.W)\n        return Y\n#x = np.array([[0,0],[0,1],[1,0],[1,1]])\n#y = np.array([[0],[1],[1],[0]])\ndata = pd.read_table(\'german.data-numeric\', header=None, sep=\'\\s+\')\ndata = data.as_matrix()\nx = data[:,0:24]\ny = data[:,-1:]\n#\xe5\xaf\xb9\xe4\xbf\xa1\xe7\x94\xa8\xe5\xa5\xbd\xe5\x9d\x8f\xe8\xbf\x9b\xe8\xa1\x8c 0 \xe5\x92\x8c 1 \xe5\xa4\x84\xe7\x90\x86\ny[y==1]=0\ny[y==2]=1\n#\xe5\xaf\xb9 x \xe7\x9a\x84\xe5\x8f\x96\xe5\x80\xbc\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\n#x = (x - np.mean(x,axis=0))/np.std(x,axis=0)\nx = (x - np.min(x,axis=0))/(np.max(x,axis=0) - np.min(x,axis=0))\n#\xe6\x8b\x86\xe5\x88\x86\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8e\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\n#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\ninnums,indim = x_train.shape\n#centernum = 24\ncenternum = 36\noutnums, outdim = y_train.shape\nrbf = RBF(indim, centernum, outdim)\nrbf.train(x_train, y_train)\nresult = rbf.test(x_test)\n#\xe7\xbb\x93\xe6\x9e\x9c\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba y_test \xe5\xaf\xb9\xe6\xaf\x94\nresult.astype(int)\ncount = 0\nfor i in range(len(y_test)):\n    if y_test[i] == result.astype(int)[i]:\n        count += 1\nprint(\'accuracy rate is \', count*100/len(y_test),\'%\')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#a=np.random.rand(1000000)\n#b=np.random.rand(1000000)\n#\n#start_time=time.perf_counter()\n#\n#c=np.dot(a,b)\n#\n#end_time=time.perf_counter()\n#\n#time1=end_time-start_time\n#\n#print(time1)\n#print(c)\n#\n#\n#start_time2=time.perf_counter()\n#\n#\n#c=0\n#for i in range(1000000):\n#    c+=a[i]*b[i]\n#\n#end_time2=time.perf_counter()\n#\n#time2=end_time2-start_time2\n#\n#print(time2)\n#print(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\n\n\n\n\n\n\n    \n\n\n\n       \n    \n    \n\n'"
机器学习实战 Python代码实现/SVM/SVCSMO.py,0,"b'from __future__ import division, print_function\nfrom numpy import linalg\nimport os\nimport numpy as np\nimport random as rnd\nfilepath = os.path.dirname(os.path.abspath(__file__))\n\nclass SVCSMO():\n    """"""\n        Simple implementation of a Support Vector Classification using the\n        Sequential Minimal Optimization (SMO) algorithm for training.\n    """"""\n    def __init__(self, max_iter=10000, kernel_type=\'linear\', C=1.0, epsilon=0.001, sigma=5.0):\n        """"""\n        :param max_iter: maximum iteration\n        :param kernel_type: Kernel type to use in training.\n                        \'linear\' use linear kernel function.\n                        \'quadratic\' use quadratic kernel function.\n                        \'gaussian\' use gaussian kernel function\n        :param C: Value of regularization parameter C\n        :param epsilon: Convergence value.\n        :param sigma: parameter for gaussian kernel\n        """"""\n        self.kernels = {\n            \'linear\' : self.kernel_linear,\n            \'quadratic\' : self.kernel_quadratic,\n            \'gaussian\' : self.kernel_gaussian\n        }\n        self.max_iter = max_iter\n        self.kernel_type = kernel_type\n        self.C = C\n        self.epsilon = epsilon\n        self.sigma = sigma\n    def fit(self, X, y):\n        # Initialization\n        n, d = X.shape[0], X.shape[1]\n        alpha = np.zeros((n))\n        kernel = self.kernels[self.kernel_type]\n        count = 0\n        while True:\n            count += 1\n            alpha_prev = np.copy(alpha)\n            for j in range(0, n):\n                i = self.get_rnd_int(0, n-1, j) # Get random int i~=j\n                x_i, x_j, y_i, y_j = X[i,:], X[j,:], y[i], y[j]\n                k_ij = kernel(x_i, x_i) + kernel(x_j, x_j) - 2 * kernel(x_i, x_j)\n                if k_ij == 0:\n                    continue\n                alpha_prime_j, alpha_prime_i = alpha[j], alpha[i]\n                (L, H) = self.compute_L_H(self.C, alpha_prime_j, alpha_prime_i, y_j, y_i)\n\n                # Compute model parameters\n                self.w = self.calc_w(alpha, y, X)\n                self.b = self.calc_b(X, y, self.w)\n\n                # Compute E_i, E_j\n                E_i = self.E(x_i, y_i, self.w, self.b)\n                E_j = self.E(x_j, y_j, self.w, self.b)\n\n                # Set new alpha values\n                alpha[j] = alpha_prime_j + float(y_j * (E_i - E_j))/k_ij\n                alpha[j] = max(alpha[j], L)\n                alpha[j] = min(alpha[j], H)\n\n                alpha[i] = alpha_prime_i + y_i*y_j * (alpha_prime_j - alpha[j])\n\n            # Check convergence\n            diff = np.linalg.norm(alpha - alpha_prev)\n            if diff < self.epsilon:\n                break\n\n            if count >= self.max_iter:\n                print(""Iteration number exceeded the max of %d iterations"" % (self.max_iter))\n                return\n        # Compute final model parameters\n        self.b = self.calc_b(X, y, self.w)\n        if self.kernel_type == \'linear\':\n            self.w = self.calc_w(alpha, y, X)\n        # Get support vectors\n        alpha_idx = np.where(alpha > 0)[0]\n        support_vectors = X[alpha_idx, :]\n        return support_vectors, count\n    def predict(self, X):\n        return self.h(X, self.w, self.b)\n    def calc_b(self, X, y, w):\n        b_tmp = y - np.dot(w.T, X.T)\n        return np.mean(b_tmp)\n    def calc_w(self, alpha, y, X):\n        return np.dot(alpha * y, X)\n    # Prediction\n    def h(self, X, w, b):\n        return np.sign(np.dot(w.T, X.T) + b).astype(int)\n    # Prediction error\n    def E(self, x_k, y_k, w, b):\n        return self.h(x_k, w, b) - y_k\n    def compute_L_H(self, C, alpha_prime_j, alpha_prime_i, y_j, y_i):\n        if(y_i != y_j):\n            return (max(0, alpha_prime_j - alpha_prime_i), min(C, C - alpha_prime_i + alpha_prime_j))\n        else:\n            return (max(0, alpha_prime_i + alpha_prime_j - C), min(C, alpha_prime_i + alpha_prime_j))\n    def get_rnd_int(self, a,b,z):\n        i = z\n        while i == z:\n            i = rnd.randint(a,b)\n        return i\n    # Define kernels\n    def kernel_linear(self, x1, x2):\n        return np.dot(x1, x2.T)\n    def kernel_quadratic(self, x1, x2):\n        return (np.dot(x1, x2.T) ** 2)\n    def kernel_gaussian(self, x1, x2, sigma=5.0):\n        if self.sigma:\n            sigma = self.sigma\n        return np.exp(-linalg.norm(x1-x2)**2 / (2 * (sigma ** 2)))\n'"
机器学习实战 Python代码实现/SVM/svm-simple.py,0,"b'# -*- coding:UTF-8 -*-\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\n""""""\n\xe5\x87\xbd\xe6\x95\xb0\xe8\xaf\xb4\xe6\x98\x8e:\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\n\nParameters:\n    fileName - \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\nReturns:\n    dataMat - \xe6\x95\xb0\xe6\x8d\xae\xe7\x9f\xa9\xe9\x98\xb5\n    labelMat - \xe6\x95\xb0\xe6\x8d\xae\xe6\xa0\x87\xe7\xad\xbe\nAuthor:\n    Jack Cui\nBlog:\n    http://blog.csdn.net/c406495762\nZhihu:\n    https://www.zhihu.com/people/Jack--Cui/\nModify:\n    2017-09-21\n""""""\ndef loadDataSet(fileName):\n\tdataMat = []; labelMat = []\n\tfr = open(fileName)\n\tfor line in fr.readlines():                                     #\xe9\x80\x90\xe8\xa1\x8c\xe8\xaf\xbb\xe5\x8f\x96\xef\xbc\x8c\xe6\xbb\xa4\xe9\x99\xa4\xe7\xa9\xba\xe6\xa0\xbc\xe7\xad\x89\n\t\tlineArr = line.strip().split(\'\\t\')\n\t\tdataMat.append([float(lineArr[0]), float(lineArr[1])])      #\xe6\xb7\xbb\xe5\x8a\xa0\xe6\x95\xb0\xe6\x8d\xae\n\t\tlabelMat.append(float(lineArr[2]))                          #\xe6\xb7\xbb\xe5\x8a\xa0\xe6\xa0\x87\xe7\xad\xbe\n\treturn dataMat,labelMat\n\n\n""""""\n\xe5\x87\xbd\xe6\x95\xb0\xe8\xaf\xb4\xe6\x98\x8e:\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9alpha\n\nParameters:\n    i - alpha_i\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\xe5\x80\xbc\n    m - alpha\xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xaa\xe6\x95\xb0\nReturns:\n    j - alpha_j\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\xe5\x80\xbc\nAuthor:\n    Jack Cui\nBlog:\n    http://blog.csdn.net/c406495762\nZhihu:\n    https://www.zhihu.com/people/Jack--Cui/\nModify:\n    2017-09-21\n""""""\ndef selectJrand(i, m):\n\tj = i                                 #\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8d\xe7\xad\x89\xe4\xba\x8ei\xe7\x9a\x84j\n\twhile (j == i):\n\t\tj = int(random.uniform(0, m))\n\treturn j\n\n""""""\n\xe5\x87\xbd\xe6\x95\xb0\xe8\xaf\xb4\xe6\x98\x8e:\xe4\xbf\xae\xe5\x89\xaaalpha\n\nParameters:\n    aj - alpha_j\xe5\x80\xbc\n    H - alpha\xe4\xb8\x8a\xe9\x99\x90\n    L - alpha\xe4\xb8\x8b\xe9\x99\x90\nReturns:\n    aj - alpah\xe5\x80\xbc\nAuthor:\n    Jack Cui\nBlog:\n    http://blog.csdn.net/c406495762\nZhihu:\n    https://www.zhihu.com/people/Jack--Cui/\nModify:\n    2017-09-21\n""""""\ndef clipAlpha(aj,H,L):\n\tif aj > H: \n\t\taj = H\n\tif L > aj:\n\t\taj = L\n\treturn aj\n\n""""""\n\xe5\x87\xbd\xe6\x95\xb0\xe8\xaf\xb4\xe6\x98\x8e:\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n\nParameters:\n    dataMat - \xe6\x95\xb0\xe6\x8d\xae\xe7\x9f\xa9\xe9\x98\xb5\n    labelMat - \xe6\x95\xb0\xe6\x8d\xae\xe6\xa0\x87\xe7\xad\xbe\nReturns:\n    \xe6\x97\xa0\nAuthor:\n    Jack Cui\nBlog:\n    http://blog.csdn.net/c406495762\nZhihu:\n    https://www.zhihu.com/people/Jack--Cui/\nModify:\n    2017-09-21\n""""""\ndef showDataSet(dataMat, labelMat):\n\tdata_plus = []                                  #\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\n\tdata_minus = []                                 #\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\n\tfor i in range(len(dataMat)):\n\t\tif labelMat[i] > 0:\n\t\t\tdata_plus.append(dataMat[i])\n\t\telse:\n\t\t\tdata_minus.append(dataMat[i])\n\tdata_plus_np = np.array(data_plus)              #\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbanumpy\xe7\x9f\xa9\xe9\x98\xb5\n\tdata_minus_np = np.array(data_minus)            #\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbanumpy\xe7\x9f\xa9\xe9\x98\xb5\n\tplt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1])   #\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xa3\xe7\x82\xb9\xe5\x9b\xbe\n\tplt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1]) #\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xa3\xe7\x82\xb9\xe5\x9b\xbe\n\tplt.show()\n\n\n""""""\n\xe5\x87\xbd\xe6\x95\xb0\xe8\xaf\xb4\xe6\x98\x8e:\xe7\xae\x80\xe5\x8c\x96\xe7\x89\x88SMO\xe7\xae\x97\xe6\xb3\x95\n\nParameters:\n    dataMatIn - \xe6\x95\xb0\xe6\x8d\xae\xe7\x9f\xa9\xe9\x98\xb5\n    classLabels - \xe6\x95\xb0\xe6\x8d\xae\xe6\xa0\x87\xe7\xad\xbe\n    C - \xe6\x9d\xbe\xe5\xbc\x9b\xe5\x8f\x98\xe9\x87\x8f\n    toler - \xe5\xae\xb9\xe9\x94\x99\xe7\x8e\x87\n    maxIter - \xe6\x9c\x80\xe5\xa4\xa7\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\nReturns:\n    \xe6\x97\xa0\nAuthor:\n    Jack Cui\nBlog:\n    http://blog.csdn.net/c406495762\nZhihu:\n    https://www.zhihu.com/people/Jack--Cui/\nModify:\n    2017-09-23\n""""""\ndef smoSimple(dataMatIn, classLabels, C, toler, maxIter):\n\t#\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbanumpy\xe7\x9a\x84mat\xe5\xad\x98\xe5\x82\xa8\n\tdataMatrix = np.mat(dataMatIn); labelMat = np.mat(classLabels).transpose()\n\t#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96b\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe7\xbb\x9f\xe8\xae\xa1dataMatrix\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\n\tb = 0; m,n = np.shape(dataMatrix)\n\t#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96alpha\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xae\xbe\xe4\xb8\xba0\n\talphas = np.mat(np.zeros((m,1)))\n\t#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\n\titer_num = 0\n\t#\xe6\x9c\x80\xe5\xa4\x9a\xe8\xbf\xad\xe4\xbb\xa3matIter\xe6\xac\xa1\n\twhile (iter_num < maxIter):\n\t\talphaPairsChanged = 0\n\t\tfor i in range(m):\n\t\t\t#\xe6\xad\xa5\xe9\xaa\xa41\xef\xbc\x9a\xe8\xae\xa1\xe7\xae\x97\xe8\xaf\xaf\xe5\xb7\xaeEi\n\t\t\tfXi = float(np.multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b\n\t\t\tEi = fXi - float(labelMat[i])\n\t\t\t#\xe4\xbc\x98\xe5\x8c\x96alpha\xef\xbc\x8c\xe8\xae\xbe\xe5\xae\x9a\xe4\xb8\x80\xe5\xae\x9a\xe7\x9a\x84\xe5\xae\xb9\xe9\x94\x99\xe7\x8e\x87\xe3\x80\x82\n\t\t\tif ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):\n\t\t\t\t#\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe5\x8f\xa6\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8ealpha_i\xe6\x88\x90\xe5\xaf\xb9\xe4\xbc\x98\xe5\x8c\x96\xe7\x9a\x84alpha_j\n\t\t\t\tj = selectJrand(i,m)\n\t\t\t\t#\xe6\xad\xa5\xe9\xaa\xa41\xef\xbc\x9a\xe8\xae\xa1\xe7\xae\x97\xe8\xaf\xaf\xe5\xb7\xaeEj\n\t\t\t\tfXj = float(np.multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b\n\t\t\t\tEj = fXj - float(labelMat[j])\n\t\t\t\t#\xe4\xbf\x9d\xe5\xad\x98\xe6\x9b\xb4\xe6\x96\xb0\xe5\x89\x8d\xe7\x9a\x84aplpha\xe5\x80\xbc\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe6\xb7\xb1\xe6\x8b\xb7\xe8\xb4\x9d\n\t\t\t\talphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();\n\t\t\t\t#\xe6\xad\xa5\xe9\xaa\xa42\xef\xbc\x9a\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x8a\xe4\xb8\x8b\xe7\x95\x8cL\xe5\x92\x8cH\n\t\t\t\tif (labelMat[i] != labelMat[j]):\n\t\t\t\t    L = max(0, alphas[j] - alphas[i])\n\t\t\t\t    H = min(C, C + alphas[j] - alphas[i])\n\t\t\t\telse:\n\t\t\t\t    L = max(0, alphas[j] + alphas[i] - C)\n\t\t\t\t    H = min(C, alphas[j] + alphas[i])\n\t\t\t\tif L==H: print(""L==H""); continue\n\t\t\t\t#\xe6\xad\xa5\xe9\xaa\xa43\xef\xbc\x9a\xe8\xae\xa1\xe7\xae\x97eta\n\t\t\t\teta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T\n\t\t\t\tif eta >= 0: print(""eta>=0""); continue\n\t\t\t\t#\xe6\xad\xa5\xe9\xaa\xa44\xef\xbc\x9a\xe6\x9b\xb4\xe6\x96\xb0alpha_j\n\t\t\t\talphas[j] -= labelMat[j]*(Ei - Ej)/eta\n\t\t\t\t#\xe6\xad\xa5\xe9\xaa\xa45\xef\xbc\x9a\xe4\xbf\xae\xe5\x89\xaaalpha_j\n\t\t\t\talphas[j] = clipAlpha(alphas[j],H,L)\n\t\t\t\tif (abs(alphas[j] - alphaJold) < 0.00001): print(""alpha_j\xe5\x8f\x98\xe5\x8c\x96\xe5\xa4\xaa\xe5\xb0\x8f""); continue\n\t\t\t\t#\xe6\xad\xa5\xe9\xaa\xa46\xef\xbc\x9a\xe6\x9b\xb4\xe6\x96\xb0alpha_i\n\t\t\t\talphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])\n\t\t\t\t#\xe6\xad\xa5\xe9\xaa\xa47\xef\xbc\x9a\xe6\x9b\xb4\xe6\x96\xb0b_1\xe5\x92\x8cb_2\n\t\t\t\tb1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T\n\t\t\t\tb2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T\n\t\t\t\t#\xe6\xad\xa5\xe9\xaa\xa48\xef\xbc\x9a\xe6\xa0\xb9\xe6\x8d\xaeb_1\xe5\x92\x8cb_2\xe6\x9b\xb4\xe6\x96\xb0b\n\t\t\t\tif (0 < alphas[i]) and (C > alphas[i]): b = b1\n\t\t\t\telif (0 < alphas[j]) and (C > alphas[j]): b = b2\n\t\t\t\telse: b = (b1 + b2)/2.0\n\t\t\t\t#\xe7\xbb\x9f\xe8\xae\xa1\xe4\xbc\x98\xe5\x8c\x96\xe6\xac\xa1\xe6\x95\xb0\n\t\t\t\talphaPairsChanged += 1\n\t\t\t\t#\xe6\x89\x93\xe5\x8d\xb0\xe7\xbb\x9f\xe8\xae\xa1\xe4\xbf\xa1\xe6\x81\xaf\n\t\t\t\tprint(""\xe7\xac\xac%d\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3 \xe6\xa0\xb7\xe6\x9c\xac:%d, alpha\xe4\xbc\x98\xe5\x8c\x96\xe6\xac\xa1\xe6\x95\xb0:%d"" % (iter_num,i,alphaPairsChanged))\n\t\t#\xe6\x9b\xb4\xe6\x96\xb0\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\n\t\tif (alphaPairsChanged == 0): iter_num += 1\n\t\telse: iter_num = 0\n\t\tprint(""\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0: %d"" % iter_num)\n\treturn b,alphas\n\n""""""\n\xe5\x87\xbd\xe6\x95\xb0\xe8\xaf\xb4\xe6\x98\x8e:\xe5\x88\x86\xe7\xb1\xbb\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n\nParameters:\n\tdataMat - \xe6\x95\xb0\xe6\x8d\xae\xe7\x9f\xa9\xe9\x98\xb5\n    w - \xe7\x9b\xb4\xe7\xba\xbf\xe6\xb3\x95\xe5\x90\x91\xe9\x87\x8f\n    b - \xe7\x9b\xb4\xe7\xba\xbf\xe8\xa7\xa3\xe5\x86\xb3\nReturns:\n    \xe6\x97\xa0\nAuthor:\n    Jack Cui\nBlog:\n    http://blog.csdn.net/c406495762\nZhihu:\n    https://www.zhihu.com/people/Jack--Cui/\nModify:\n    2017-09-23\n""""""\ndef showClassifer(dataMat, w, b):\n\t#\xe7\xbb\x98\xe5\x88\xb6\xe6\xa0\xb7\xe6\x9c\xac\xe7\x82\xb9\n\tdata_plus = []                                  #\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\n\tdata_minus = []                                 #\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\n\tfor i in range(len(dataMat)):\n\t\tif labelMat[i] > 0:\n\t\t\tdata_plus.append(dataMat[i])\n\t\telse:\n\t\t\tdata_minus.append(dataMat[i])\n\tdata_plus_np = np.array(data_plus)              #\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbanumpy\xe7\x9f\xa9\xe9\x98\xb5\n\tdata_minus_np = np.array(data_minus)            #\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbanumpy\xe7\x9f\xa9\xe9\x98\xb5\n\tplt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1], s=30, alpha=0.7)   #\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xa3\xe7\x82\xb9\xe5\x9b\xbe\n\tplt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1], s=30, alpha=0.7) #\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xa3\xe7\x82\xb9\xe5\x9b\xbe\n\t#\xe7\xbb\x98\xe5\x88\xb6\xe7\x9b\xb4\xe7\xba\xbf\n\tx1 = max(dataMat)[0]\n\tx2 = min(dataMat)[0]\n\ta1, a2 = w\n\tb = float(b)\n\ta1 = float(a1[0])\n\ta2 = float(a2[0])\n\ty1, y2 = (-b- a1*x1)/a2, (-b - a1*x2)/a2\n\tplt.plot([x1, x2], [y1, y2])\n\t#\xe6\x89\xbe\xe5\x87\xba\xe6\x94\xaf\xe6\x8c\x81\xe5\x90\x91\xe9\x87\x8f\xe7\x82\xb9\n\tfor i, alpha in enumerate(alphas):\n\t\tif abs(alpha) > 0:\n\t\t\tx, y = dataMat[i]\n\t\t\tplt.scatter([x], [y], s=150, c=\'none\', alpha=0.7, linewidth=1.5, edgecolor=\'red\')\n\tplt.show()\n\n\n""""""\n\xe5\x87\xbd\xe6\x95\xb0\xe8\xaf\xb4\xe6\x98\x8e:\xe8\xae\xa1\xe7\xae\x97w\n\nParameters:\n\tdataMat - \xe6\x95\xb0\xe6\x8d\xae\xe7\x9f\xa9\xe9\x98\xb5\n    labelMat - \xe6\x95\xb0\xe6\x8d\xae\xe6\xa0\x87\xe7\xad\xbe\n    alphas - alphas\xe5\x80\xbc\nReturns:\n    \xe6\x97\xa0\nAuthor:\n    Jack Cui\nBlog:\n    http://blog.csdn.net/c406495762\nZhihu:\n    https://www.zhihu.com/people/Jack--Cui/\nModify:\n    2017-09-23\n""""""\ndef get_w(dataMat, labelMat, alphas):\n    alphas, dataMat, labelMat = np.array(alphas), np.array(dataMat), np.array(labelMat)\n    w = np.dot((np.tile(labelMat.reshape(1, -1).T, (1, 2)) * dataMat).T, alphas)\n    return w.tolist()\n\n\nif __name__ == \'__main__\':\n\tdataMat, labelMat = loadDataSet(\'testSet.txt\')\n\tb,alphas = smoSimple(dataMat, labelMat, 0.6, 0.001, 40)\n\tw = get_w(dataMat, labelMat, alphas)\n\tshowClassifer(dataMat, w, b)\n\t'"
机器学习实战 Python代码实现/SVM_by_SMO/SVCSMO.py,0,"b'from __future__ import division, print_function\nfrom numpy import linalg\nimport os\nimport numpy as np\nimport random as rnd\nfilepath = os.path.dirname(os.path.abspath(__file__))\n\nclass SVCSMO():\n    """"""\n    \xe6\x94\xaf\xe6\x8c\x81\xe5\x90\x91\xe9\x87\x8f\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe7\xae\x80\xe5\x8d\x95\xe5\xae\x9e\xe7\x8e\xb0\n    \xe7\x94\xa8\xe4\xba\x8e\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xe6\x9c\x80\xe5\xb0\x8f\xe4\xbc\x98\xe5\x8c\x96\xef\xbc\x88SMO\xef\xbc\x89\xe7\xae\x97\xe6\xb3\x95\xe3\x80\x82\n    """"""\n    def __init__(self, max_iter=10000, kernel_type=\'linear\', C=1.0, epsilon=0.001, sigma=5.0):\n        """"""\n        :param max_iter: maximum iteration\xe6\x9c\x80\xe5\xa4\xa7\xe8\xbf\xad\xe4\xbb\xa3\n        :param kernel_type: Kernel type to use in training.\xe7\x94\xa8\xe4\xba\x8e\xe5\x9f\xb9\xe8\xae\xad\xe7\x9a\x84\xe5\x86\x85\xe6\xa0\xb8\xe7\xb1\xbb\xe5\x9e\x8b\xe3\x80\x82\n                        \'linear\' use linear kernel function.\xe4\xbd\xbf\xe7\x94\xa8\xe7\xba\xbf\xe6\x80\xa7\xe6\xa0\xb8\xe5\x87\xbd\xe6\x95\xb0\n                        \'quadratic\' use quadratic kernel function.\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x8c\xe6\xac\xa1\xe6\xa0\xb8\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\n                        \'gaussian\' use gaussian kernel function\xe4\xbd\xbf\xe7\x94\xa8\xe9\xab\x98\xe6\x96\xaf\xe6\xa0\xb8\xe5\x87\xbd\xe6\x95\xb0\n        :param C: Value of regularization parameter C\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe5\x8f\x82\xe6\x95\xb0c\xe5\x80\xbc\n        :param epsilon: Convergence value.\xe6\x94\xb6\xe6\x95\x9b\xe5\x80\xbc\xe3\x80\x82\n        :param sigma: parameter for gaussian kernel\xe9\xab\x98\xe6\x96\xaf\xe6\xa0\xb8\xe5\x8f\x82\xe6\x95\xb0\n        """"""\n        self.kernels = {\n            \'linear\' : self.kernel_linear,\n            \'quadratic\' : self.kernel_quadratic,\n            \'gaussian\' : self.kernel_gaussian\n        }\n        self.max_iter = max_iter\n        self.kernel_type = kernel_type\n        self.C = C\n        self.epsilon = epsilon\n        self.sigma = sigma\n    def fit(self, X, y):\n        # Initialization\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        n, d = X.shape[0], X.shape[1]\n        alpha = np.zeros((n))\n        kernel = self.kernels[self.kernel_type]\n        count = 0\n        while True:\n            count += 1\n            alpha_prev = np.copy(alpha)\n            for j in range(0, n):\n                i = self.get_rnd_int(0, n-1, j) # Get random int i~=j  \xe5\xbe\x97\xe5\x88\xb0\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb4\xe6\x95\xb0\n                x_i, x_j, y_i, y_j = X[i,:], X[j,:], y[i], y[j]\n                k_ij = kernel(x_i, x_i) + kernel(x_j, x_j) - 2 * kernel(x_i, x_j)\n                if k_ij == 0:\n                    continue\n                alpha_prime_j, alpha_prime_i = alpha[j], alpha[i]\n                (L, H) = self.compute_L_H(self.C, alpha_prime_j, alpha_prime_i, y_j, y_i)\n\n                # Compute model parameters\xe8\xae\xa1\xe7\xae\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n                self.w = self.calc_w(alpha, y, X)\n                self.b = self.calc_b(X, y, self.w)\n\n                # Compute E_i, E_j\n                E_i = self.E(x_i, y_i, self.w, self.b)\n                E_j = self.E(x_j, y_j, self.w, self.b)\n\n                # Set new alpha values\xe8\xae\xbe\xe7\xbd\xae\xe6\x96\xb0\xe7\x9a\x84alpha\xe5\x80\xbc\n                alpha[j] = alpha_prime_j + float(y_j * (E_i - E_j))/k_ij\n                alpha[j] = max(alpha[j], L)\n                alpha[j] = min(alpha[j], H)\n\n                alpha[i] = alpha_prime_i + y_i*y_j * (alpha_prime_j - alpha[j])\n\n            # Check convergence\xe6\xa3\x80\xe6\x9f\xa5\xe6\x94\xb6\xe6\x95\x9b\xe6\x80\xa7\n            diff = np.linalg.norm(alpha - alpha_prev)\n            if diff < self.epsilon:\n                break\n\n            if count >= self.max_iter:\n                print(""Iteration number exceeded the max of %d iterations"" % (self.max_iter))\n                return\n        # Compute final model parameters\n        self.b = self.calc_b(X, y, self.w)\n        if self.kernel_type == \'linear\':\n            self.w = self.calc_w(alpha, y, X)\n        # Get support vectors\n        alpha_idx = np.where(alpha > 0)[0]\n        support_vectors = X[alpha_idx, :]\n        return support_vectors, count\n    def predict(self, X):\n        return self.h(X, self.w, self.b)\n    def calc_b(self, X, y, w):\n        b_tmp = y - np.dot(w.T, X.T)\n        return np.mean(b_tmp)\n    def calc_w(self, alpha, y, X):\n        return np.dot(alpha * y, X)\n    # Prediction\n    def h(self, X, w, b):\n        return np.sign(np.dot(w.T, X.T) + b).astype(int)\n    # Prediction error\xe9\xa2\x84\xe6\xb5\x8b\xe8\xaf\xaf\xe5\xb7\xae\n    def E(self, x_k, y_k, w, b):\n        return self.h(x_k, w, b) - y_k\n    def compute_L_H(self, C, alpha_prime_j, alpha_prime_i, y_j, y_i):\n        if(y_i != y_j):\n            return (max(0, alpha_prime_j - alpha_prime_i), min(C, C - alpha_prime_i + alpha_prime_j))\n        else:\n            return (max(0, alpha_prime_i + alpha_prime_j - C), min(C, alpha_prime_i + alpha_prime_j))\n    def get_rnd_int(self, a,b,z):\n        i = z\n        while i == z:\n            i = rnd.randint(a,b)\n        return i\n    # Define kernels\xe5\xae\x9a\xe4\xb9\x89\xe6\xa0\xb8\xe5\x87\xbd\xe6\x95\xb0\n    def kernel_linear(self, x1, x2):\n        return np.dot(x1, x2.T)\n    def kernel_quadratic(self, x1, x2):\n        return (np.dot(x1, x2.T) ** 2)\n    def kernel_gaussian(self, x1, x2, sigma=5.0):\n        if self.sigma:\n            sigma = self.sigma\n        return np.exp(-linalg.norm(x1-x2)**2 / (2 * (sigma ** 2)))\n'"
机器学习实战 Python代码实现/SVM_by_SMO/testSVM-SMO.py,0,"b'from __future__ import division, print_function\nimport csv, os, sys\nimport numpy as np\nfrom SVCSMO import SVCSMO\nfilepath = os.path.dirname(os.path.abspath(__file__))\n\ndef readData(filename, header=True):\n    data, header = [], None\n    with open(filename, \'rt\') as csvfile:#\xe5\xba\x94\xe8\xaf\xa5\xe4\xbd\xbf\xe7\x94\xa8rt\xef\xbc\x88text\xef\xbc\x89\xef\xbc\x8crb\xef\xbc\x88bit\xef\xbc\x89\xe4\xbc\x9a\xe6\x8a\xa5\xe9\x94\x99\n        spamreader = csv.reader(csvfile, delimiter=\',\')\n        if header:\n            header = spamreader.next()\n        for row in spamreader:\n            data.append(row)\n    return (np.array(data), np.array(header))\n\ndef calc_acc(y, y_hat):\n    idx = np.where(y_hat == 1)\n    TP = np.sum(y_hat[idx] == y[idx])\n    idx = np.where(y_hat == -1)\n    TN = np.sum(y_hat[idx] == y[idx])\n    return float(TP + TN)/len(y)\n\ndef calc_mse(y, y_hat):\n    return np.nanmean(((y - y_hat) ** 2))\n\ndef test_main(filename=\'data/iris-virginica.txt\', C=1.0, kernel_type=\'linear\', epsilon=0.001):\n    # Load data\n    (data, _) = readData(\'%s/%s\' % (filepath, filename), header=False)\n    data = data.astype(float)\n\n    # Split data\xe5\x88\x86\xe8\xa3\x82\xe6\x95\xb0\xe6\x8d\xae\n    X, y = data[:,0:-1], data[:,-1].astype(int)\n\n    # Initialize model\n    model = SVCSMO()\n\n    # Fit model\n    support_vectors, iterations = model.fit(X, y)\n\n    # Support vector count\xe6\x94\xaf\xe6\x8c\x81\xe5\x90\x91\xe9\x87\x8f\xe8\xae\xa1\xe6\x95\xb0\n    sv_count = support_vectors.shape[0]\n\n    # Make prediction\n    y_hat = model.predict(X)\n\n    # Calculate accuracy\n    acc = calc_acc(y, y_hat)\n    mse = calc_mse(y, y_hat)\n\n    print(""Support vector count: %d"" % (sv_count))\n    print(""bias:\\t\\t%.3f"" % (model.b))\n    print(""w:\\t\\t"" + str(model.w))\n    print(""accuracy:\\t%.3f"" % (acc))\n    print(""mse:\\t%.3f"" % (mse))\n    print(""Converged after %d iterations"" % (iterations))\n\nif __name__ == \'__main__\':\n    param = {}\n    param[\'filename\'] = \'./small_data/iris-slwc.txt\'\n    param[\'C\'] = 0.1\n    param[\'kernel_type\'] = \'linear\'\n    param[\'epsilon\'] = 0.001\n\n\n    test_main(**param)\n\n'"
机器学习实战 Python代码实现/多层感知机/keras_iris.py,0,"b'""""""\nCreated on Sat Mar  9 22:02:25 2019\n\n@author: liweimin\n""""""\n\n#import turtle\n#import random\n#import time\n#import jieba\n#import numpy as np\n#import matplotlib.pyplot as plt\n#import pandas as pd\n#import sklearn\n#from sklearn.datasets import load_boston\n#from sklearn.datasets import load_iris\n#\n#\n#boston=load_boston()\n#print(boston.data.shape)\n#\n#iris=load_iris()\n#print(iris.target_names)\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import SGD\nfrom keras.models import load_model\nimport matplotlib.pyplot as plt\n\n\n# \xe4\xbf\x9d\xe5\xad\x98loss\xe5\x92\x8cacc\xef\xbc\x8c\xe7\x94\xbb\xe5\x9b\xbe\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = {\'batch\': [], \'epoch\': []}\n        self.accuracy = {\'batch\': [], \'epoch\': []}\n        self.val_loss = {\'batch\': [], \'epoch\': []}\n        self.val_acc = {\'batch\': [], \'epoch\': []}\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses[\'batch\'].append(logs.get(\'loss\'))\n        self.accuracy[\'batch\'].append(logs.get(\'acc\'))\n        self.val_loss[\'batch\'].append(logs.get(\'val_loss\'))\n        self.val_acc[\'batch\'].append(logs.get(\'val_acc\'))\n\n    def on_epoch_end(self, batch, logs={}):\n        self.losses[\'epoch\'].append(logs.get(\'loss\'))\n        self.accuracy[\'epoch\'].append(logs.get(\'acc\'))\n        self.val_loss[\'epoch\'].append(logs.get(\'val_loss\'))\n        self.val_acc[\'epoch\'].append(logs.get(\'val_acc\'))\n\n    def loss_plot(self, loss_type):\n        iters = range(len(self.losses[loss_type]))\n        plt.figure()\n        # train acc \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 accuracy\n        plt.plot(iters, self.accuracy[loss_type], \'r\', label=\'train acc\')\n        # loss\n        plt.plot(iters, self.losses[loss_type], \'g\', label=\'train loss\')\n        if loss_type == \'epoch\':\n            # val acc \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 validation \n            plt.plot(iters, self.val_acc[loss_type], \'b\', label=\'val acc\')\n            # val loss \n            plt.plot(iters, self.val_loss[loss_type], \'k\', label=\'val loss\')\n        plt.grid(True)\n        plt.xlabel(loss_type)\n        plt.ylabel(\'acc-loss\')\n        plt.legend(loc=""upper left"")\n        plt.show()\n\n\n# \xe8\x8e\xb7\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\ndata = load_iris()\n# print(data)\n# print(type(data))\nx = data[\'data\']\n# print(x[1])\ny = data[\'target\']\n# \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe5\x88\x92\xe5\x88\x86 | random_state\xef\xbc\x9a\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe7\xa7\x8d\xe5\xad\x90\nx_train, x_test, y_init_train, y_init_test = train_test_split(x, y, test_size=0.2, random_state=1)\n# \xe6\x9f\xa5\xe7\x9c\x8b\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\nprint(x_test[:1])\nprint(y_init_test[:1])\n\nprint(x_train.shape)\n# one hot\xe7\xbc\x96\xe7\xa0\x81\ny_train = keras.utils.to_categorical(y_init_train, num_classes=3)\nprint(y_train.shape)\ny_test = keras.utils.to_categorical(y_init_test, num_classes=3)\nprint(y_test[:1])\n\n\'\'\'\n\xe5\x9f\xba\xe4\xba\x8eMLP\xef\xbc\x88Multi-layer Perceptron\xef\xbc\x89\xe5\xa4\x9a\xe5\xb1\x82\xe6\x84\x9f\xe7\x9f\xa5\xe5\x99\xa8\xe7\x9a\x84softmax\xe5\xa4\x9a\xe5\x88\x86\xe7\xb1\xbb\n\'\'\'\nmodel = Sequential()\n# Dense(128) is a fully-connected layer with 128 hidden units.\n# in the first layer, you must specify the expected input data shape:\n# here, 4-dimensional vectors.\nmodel.add(Dense(128, activation=\'relu\', input_dim=4))\n# Dropout\xe9\x9a\x8f\xe6\x9c\xba\xe5\xa4\xb1\xe6\xb4\xbb\xef\xbc\x8c\xe5\xb8\xb8\xe7\x94\xa8\xe4\xba\x8e\xe5\x9b\xbe\xe5\x83\x8f\xe8\xaf\x86\xe5\x88\xab\xe4\xb8\xad\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\n# model.add(Dropout(0.2))\nmodel.add(Dense(64, activation=\'relu\'))\nmodel.add(Dense(64, activation=\'relu\'))\nmodel.add(Dense(3, activation=\'softmax\'))\n\n# lr\xe8\xa1\xa8\xe7\xa4\xba\xe5\xad\xa6\xe4\xb9\xa0\xe9\x80\x9f\xe7\x8e\x87\xef\xbc\x8cmomentum\xe8\xa1\xa8\xe7\xa4\xba\xe5\x8a\xa8\xe9\x87\x8f\xe9\xa1\xb9\xef\xbc\x8cdecay\xe6\x98\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe9\x80\x9f\xe7\x8e\x87\xe7\x9a\x84\xe8\xa1\xb0\xe5\x87\x8f\xe7\xb3\xbb\xe6\x95\xb0(\xe6\xaf\x8f\xe4\xb8\xaaepoch\xe8\xa1\xb0\xe5\x87\x8f\xe4\xb8\x80\xe6\xac\xa1)\n# Nesterov\xe7\x9a\x84\xe5\x80\xbc\xe6\x98\xafFalse\xe6\x88\x96\xe8\x80\x85True\xef\xbc\x8c\xe8\xa1\xa8\xe7\xa4\xba\xe4\xbd\xbf\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8Nesterov momentum\n# SGD\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\n# \xe7\xbc\x96\xe8\xaf\x91\xe6\xa8\xa1\xe5\x9e\x8b\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=sgd,\n              metrics=[\'accuracy\'])\n\n# \xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\xae\x9e\xe4\xbe\x8bhistory\nhistory = LossHistory()\n\n# \xe8\xae\xad\xe7\xbb\x83\nmodel.fit(x_train, y_train,\n          epochs=30,  # \xe6\x95\xb0\xe6\x8d\xae\xe8\xa2\xab\xe8\xbd\xae30\xe6\xac\xa1\n          batch_size=128,\n          validation_data=(x_test, y_test),\n          callbacks=[history])\n# \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n# model.save(\'iris.h5\')\n# \xe8\xaf\xbb\xe5\x8f\x96\xe6\xa8\xa1\xe5\x9e\x8b\n# model = load_model(\'iris.h5\')\n\nscore = model.evaluate(x_test, y_test, verbose=0, batch_size=128)  # \xe4\xb8\x8d\xe5\x86\x99\xe9\xbb\x98\xe8\xae\xa4\xe6\x98\xafverbose=1\xef\xbc\x8c\xe6\x89\x93\xe5\x8d\xb0\xe8\xbf\x9b\xe5\xba\xa6\xe6\x9d\xa1\xe8\xae\xb0\xe5\xbd\x95\xef\xbc\x8c0\xe4\xb8\x8d\xe6\x89\x93\xe5\x8d\xb0\xe3\x80\x82\nprint(\'Test loss:\', score[0])\nprint(\'Test accuracy:\', score[1])\n# p_pred = model.predict(x_test)\n# print(""p_pred:\\n"", p_pred)\nlabel_pred = model.predict_classes(x_test, verbose=0)\nprint(""label_pred4test:\\n"", label_pred)\nprint(""label_init4test:\\n"", y_init_test)\nlabel_pred4train = model.predict_classes(x_train, verbose=0)\nprint(""label_pred4train:\\n"", label_pred4train)\nprint(""label_init4train:\\n"", y_init_train)\n\n# \xe7\xbb\x98\xe5\x88\xb6acc-loss\xe6\x9b\xb2\xe7\xba\xbf\nhistory.loss_plot(\'epoch\')\n\n\n\n\n\n\n    \n\n\n\n       \n    \n    \n\n'"
深度学习 吴恩达 Python代码实现/Class 1 Week 2 assignment2_1/assignment2_1.py,0,"b'""""""\nCreated on Sat Mar  9 22:02:25 2019\n\n@author: liweimin\n""""""\n\nimport turtle\nimport random\nimport time\nimport jieba\nimport math\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sklearn\nimport seaborn as sns \nimport tensorflow as tf \n\n\ndef sigmoidm(x):\n    s=1/(1+math.exp(-x))\n    return s\n\ndef sigmoid(x):\n    s=1/(1+np.exp(-x))\n    return s\n\ndef sigmoid_derivative(x):\n    s = 1 / (1 + np.exp(-x))\n    ds=s*(1-s)\n    return ds\n\ndef image2vector(image):\n    """"""\n    Argument:\n    image -- a numpy array of shape (length, height, depth)\n\n    Returns:\n    v -- a vector of shape (length*height*depth, 1)\n    """"""\n\n    ### START CODE HERE ### (\xe2\x89\x88 1 line of code)\n    v = image.reshape((image.shape[0] * image.shape[1] * image.shape[2], 1))\n    ### END CODE HERE ###\n\n    return v\n\ndef normalizeRows(x):\n    x_norm=np.linalg.norm(x,ord=2,axis=1,keepdims=True)\n    x=x/x_norm\n    return x\n\n\ns = np.array([1, 2, 3])\nprint(sigmoid_derivative(s))\n\n\nx=np.array([[0,3,4],\n            [1,4,6]])   \nprint(normalizeRows(x))\n    \ndef softmax(x):\n    x_exp = np.exp(x)\n    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n    s=x/x_sum\n    return s\n    \nsmax = np.array([[9, 2, 5, 0, 0],\n                [7, 5, 0, 0 ,0]])\nprint(softmax(smax))\n    \n\n\nx1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\nx2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n  \n\n### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ### \nstart=time.perf_counter()\ndot = 0\nfor i in range(len(x1)):\n    dot+= x1[i]*x2[i]\n    \n\nend=time.perf_counter()  \n    \n    \na_time=end -start\nprint(dot)\nprint (1000*a_time)\n    \n### CLASSIC OUTER PRODUCT IMPLEMENTATION ###  \nstart=time.perf_counter()\nouter = np.zeros((len(x1),len(x2))) \nfor i in range(len(x1)):\n    for j in range(len(x2)):\n        outer[i,j] = x1[i]*x2[j]\n\nend=time.perf_counter()  \na_time=end -start\nprint(outer)\nprint (1000*a_time)\n    \n### CLASSIC ELEMENTWISE IMPLEMENTATION ###\nstart=time.perf_counter()\nmul = np.zeros(len(x1))\nfor i in range(len(x1)):\n    mul[i] = x1[i]*x2[i]\nend=time.perf_counter()  \n\nend=time.perf_counter()  \na_time=end -start\nprint(mul)\nprint (1000*a_time)\n\n## CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\nW = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\nstart=time.perf_counter()\ngdot = np.zeros(W.shape[0])\nfor i in range(W.shape[0]):\n    for j in range(len(x1)):\n        gdot[i] += W[i,j]*x1[j]\nend=time.perf_counter()  \na_time=end -start\nprint(gdot)\nprint (1000*a_time)\n\n\n### VECTORIZED DOT PRODUCT OF VECTORS ###\n\nstart=time.perf_counter()\ndot=np.dot(x1,x2)\n\nend=time.perf_counter()  \na_time=end -start\nprint(dot)\nprint (1000*a_time)\n\n### VECTORIZED OUTER PRODUCT ###\n\nstart=time.perf_counter()\nouter=np.outer(x1,x2)\n\nend=time.perf_counter()  \na_time=end -start\nprint(outer)\nprint (1000*a_time)\n\n### VECTORIZED ELEMENTWISE MULTIPLICATION ###\nstart=time.perf_counter()\nmul=np.multiply(x1,x2)\n\nend=time.perf_counter()  \na_time=end -start\nprint(mul)\nprint (1000*a_time)\n\n### VECTORIZED GENERAL DOT PRODUCT ###\nstart=time.perf_counter()\ngdot=np.dot(W,x1)\n\nend=time.perf_counter()  \na_time=end -start\nprint(gdot)\nprint (1000*a_time)\n\n\n# GRADED FUNCTION: L1\ndef L1(y_hat,y):\n    loss=np.sum(np.abs(y-y_hat))\n    return loss\n\ny_hat=np.array([.9,0.2,0.1,.4,.9])\ny=np.array([1,0,0,1,1])\nprint(L1(y_hat,y))\n\n# GRADED FUNCTION: L2\n\ndef L2(y_hat, y):\n    loss=np.sum(np.dot(y-y_hat,y-y_hat))\n    #\xe8\xbf\x99\xe4\xb8\xaa\xe9\x9c\x80\xe8\xa6\x81\xe4\xbb\x94\xe7\xbb\x86\xe7\x9c\x8b\xe4\xbd\x9c\xe4\xb8\x9a\xe7\x9a\x84\xe6\x8f\x90\xe7\xa4\xba\n    return loss\nyhat = np.array([.9, 0.2, 0.1, .4, .9])\ny = np.array([1, 0, 0, 1, 1])\nprint(L2(yhat,y))\n    \n\n\n\n\n\n\n\n\n\n\n'"
深度学习 吴恩达 Python代码实现/Class 1 Week 2 assignment2_2/assignment2_2.py,0,"b'""""""\nCreated on 2019-4-19 18:23:23\n\n@author: liweimin\n""""""\n\nimport turtle\nimport random\nimport time\nimport jieba\nimport math\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sklearn\nimport seaborn as sns \nimport tensorflow as tf \nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom lr_utils import load_dataset\n\n\n# \xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae (cat/non-cat)\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n# \xe4\xbe\x8b\xe5\xad\x90\nindex = 25\n#\n#plt.imshow(train_set_x_orig[index])\nprint (""y = "" + str(train_set_y[:, index]) + "", it\'s a \'"" + classes[np.squeeze(train_set_y[:, index])].decode(""utf-8"") +  ""\' picture."")\n\nm_train = train_set_x_orig.shape[0]\nm_test = test_set_x_orig.shape[0]\nnum_px = train_set_x_orig[0].shape[0]\n\nprint (""Number of training examples: m_train = "" + str(m_train))\nprint (""Number of testing examples: m_test = "" + str(m_test))\nprint (""Height/Width of each image: num_px = "" + str(num_px))\nprint (""Each image is of size: ("" + str(num_px) + "", "" + str(num_px) + "", 3)"")\nprint (""train_set_x shape: "" + str(train_set_x_orig.shape))\nprint (""train_set_y shape: "" + str(train_set_y.shape))\nprint (""test_set_x shape: "" + str(test_set_x_orig.shape))\nprint (""test_set_y shape: "" + str(test_set_y.shape))\n\n\n# Reshape the training and test examples\n\n# train_set_x_flatten = train_set_x_orig.reshape(num_px * num_px * 3, -1)\n# test_set_x_flatten = test_set_x_orig.reshape(num_px * num_px * 3, -1)\ntrain_set_x_flatten = train_set_x_orig.reshape(m_train, -1).T\ntest_set_x_flatten = test_set_x_orig.reshape(m_test, -1).T\n\nprint (""train_set_x_flatten shape: "" + str(train_set_x_flatten.shape))\nprint (""train_set_y shape: "" + str(train_set_y.shape))\nprint (""test_set_x_flatten shape: "" + str(test_set_x_flatten.shape))\nprint (""test_set_y shape: "" + str(test_set_y.shape))\nprint (""sanity check after reshaping: "" + str(train_set_x_flatten[0:5,0]))\n#\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96Standardize\ntrain_set_x = train_set_x_flatten/255.\ntest_set_x = test_set_x_flatten/255.\n\n\ndef sigmoid(z):\n    s=1/(1+np.exp(-z))\n    return s\n\nprint (""sigmoid([0, 2]) = "" + str(sigmoid(np.array([0,2]))))\n\ndef initialize_with_zeros(dim):\n    w=np.zeros((dim,1))\n    b=0\n    assert (w.shape==(dim,1))\n    assert (isinstance(b, float) or isinstance(b, int))\n    return w,b\n\ndim = 2\nw, b = initialize_with_zeros(dim)\nprint (""w = "" + str(w))\nprint (""b = "" + str(b))\n\n# GRADED FUNCTION: propagate\n\ndef propagate(w, b, X, Y):\n    m=X.shape[1]\n    A=sigmoid(np.dot(w.T,X)+b)\n    #cost = -1 / m * np.sum(np.multiply(Y, np.log(A)) + np.multiply(1-Y, np.log(1-A))) \n    cost= -1/m *np.sum(np.multiply(Y,np.log(A))+np.multiply(1-Y,np.log(1-A)))\n    dw=1/m * np.dot(X,(A-Y).T)\n    db=1/m *np.sum(A-Y)\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    grads = {""dw"": dw,\n             ""db"": db}\n    return grads, cost\n\n\nw, b, X, Y = np.array([[1],[2]]), 2, np.array([[1,2],[3,4]]), np.array([[1,0]])\ngrads, cost = propagate(w, b, X, Y)\nprint (""dw = "" + str(grads[""dw""]))\nprint (""db = "" + str(grads[""db""]))\nprint (""cost = "" + str(cost))\n\n\n# GRADED FUNCTION: optimize\ndef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    costs = []\n    for i in range(num_iterations):\n        grads,cost=propagate(w,b,X,Y)\n        dw=grads[\'dw\']\n        db=grads[\'db\']\n        \n        w=w-learning_rate*dw\n        b=b-learning_rate*db\n        \n        if i%100==0:\n            costs.append(cost)\n            \n        if print_cost and i % 100 == 0:\n            print(\'cost after iteration %i :% f\' % (i,cost))\n    params = {""w"": w,\n              ""b"": b}\n\n    grads = {""dw"": dw,\n             ""db"": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint (""w = "" + str(params[""w""]))\nprint (""b = "" + str(params[""b""]))\nprint (""dw = "" + str(grads[""dw""]))\nprint (""db = "" + str(grads[""db""]))\n\n# GRADED FUNCTION: predict\ndef predict(w, b, X):\n     #\xe6\x9e\x84\xe9\x80\xa0\xe6\x95\xb0\xe7\xbb\x84\n     m = X.shape[1]\n     Y_prediction=np.zeros((1,m))\n     w = w.reshape(X.shape[0], 1)\n     A = sigmoid(np.dot(w.T, X) + b)\n     for i in range(A.shape[1]):\n         if(A[0,i]>0.5):\n              Y_prediction[0,i]=1\n         else:#if \xe4\xb8\x80\xe5\xae\x9a\xe8\xa6\x81\xe5\xaf\xb9\xe9\xbd\x90\xef\xbc\x8c\xe7\xa8\x8b\xe5\xba\x8f\xe8\x83\xbd\xe8\xbf\x90\xe8\xa1\x8c\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x8d\xe5\xaf\xb9\n              Y_prediction[0,i]=0\n     \n     assert(Y_prediction.shape == (1, m))\n     return Y_prediction\n   \nprint (""predictions = "" + str(predict(w, b, X)))\n\n# GRADED FUNCTION: model\ndef model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    #\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba0\n    w, b = np.zeros((X_train.shape[0], 1)), 0\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n    w = parameters[""w""]\n    b = parameters[""b""]\n    Y_prediction_train = predict(w, b, X_train)\n    Y_prediction_test = predict(w, b, X_test)\n    \n    # Print train/test Errors\n    print(""train accuracy: {} %"".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(""test accuracy: {} %"".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n    \n    d = {""costs"": costs,\n         ""Y_prediction_test"": Y_prediction_test, \n         ""Y_prediction_train"" : Y_prediction_train, \n         ""w"" : w, \n         ""b"" : b,\n         ""learning_rate"" : learning_rate,\n         ""num_iterations"": num_iterations}\n    return d\n\nd = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)\n\n# Example of a picture that was wrongly classified.\n#index = 1\n#plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\n#print (""y = "" + str(test_set_y[0,index]) + "", you predicted that it is a \\"""" + classes[int(d[""Y_prediction_test""][0,index])].decode(""utf-8"") +  ""\\"" picture."")\n\n\n# Plot learning curve (with costs)\xe6\xb3\xa8\xe6\x84\x8f\xe5\x9c\xa8Spyder\xe4\xb8\xad\xe5\x89\x8d\xe9\x9d\xa2\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe6\x9c\x89\xe6\x97\xb6\xe4\xbc\x9a\xe4\xba\x92\xe7\x9b\xb8\xe8\xa6\x86\xe7\x9b\x96 \ncosts = np.squeeze(d[\'costs\'])\nplt.plot(costs)\nplt.ylabel(\'cost\')\nplt.xlabel(\'iterations (per hundreds)\')\nplt.title(""Learning rate ="" + str(d[""learning_rate""]))\nplt.show()     \n     \nlearning_rates=[0.01,0.001,0.0001]\nmodels={}\nfor i in learning_rates:\n    print(\'learning rate is\'+str(i))\n    models[str(i)]=model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n     \n     \nfor i in learning_rates:\n    plt.plot(np.squeeze(models[str(i)][""costs""]), label= str(models[str(i)][""learning_rate""]))\n\nplt.ylabel(\'cost\')\nplt.xlabel(\'iterations\')\n\nlegend = plt.legend(loc=\'upper center\', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor(\'0.90\')\nplt.show()     \n    \n#\xe9\x9a\x8f\xe4\xbe\xbf\xe7\x99\xbe\xe5\xba\xa6\xe4\xb8\x80\xe5\xbc\xa0\xe7\x8c\xab\xe6\x88\x96\xe8\x80\x85\xe7\x8b\x97\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe6\x94\xbe\xe5\x9c\xa8image\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe9\x87\x8c\xef\xbc\x8c\xe6\x87\x92\xe7\x9a\x84\xe5\x86\x99for\xe5\xbe\xaa\xe7\x8e\xaf\xe4\xba\x86\nmy_image = ""cat2.jpg""\nfname = ""images/"" + my_image\nimage1 = np.array(ndimage.imread(fname, flatten=False))\nmy_image = scipy.misc.imresize(image1, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\nmy_predicted_image = predict(d[""w""], d[""b""], my_image)\nplt.subplot(1,2,1)\nplt.imshow(image1)\n#plt.show()\nprint(""y = "" + str(np.squeeze(my_predicted_image)) + "", your algorithm predicts a \\"""" + classes[int(np.squeeze(my_predicted_image)),].decode(""utf-8"") +  ""\\"" picture."")\n \n\nmy_image = ""cat3.jpg""\nfname = ""images/"" + my_image\nimage2 = np.array(ndimage.imread(fname, flatten=False))\nmy_image = scipy.misc.imresize(image2, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\nmy_predicted_image = predict(d[""w""], d[""b""], my_image)\nplt.subplot(1,2,2)\nplt.imshow(image2)\n#plt.show()\nprint(""y = "" + str(np.squeeze(my_predicted_image)) + "", your algorithm predicts a \\"""" + classes[int(np.squeeze(my_predicted_image)),].decode(""utf-8"") +  ""\\"" picture."")\n \n#\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb9\x9f\xe5\xb0\xb170%\xe5\xb7\xa6\xe5\x8f\xb3\xef\xbc\x88\xe4\xb8\x8d\xe5\x88\xb0\xef\xbc\x89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
深度学习 吴恩达 Python代码实现/Class 1 Week 2 assignment2_2/lr_utils.py,0,"b'import numpy as np\nimport h5py\n    \n    \ndef load_dataset():\n    train_dataset = h5py.File(\'datasets/train_catvnoncat.h5\', ""r"")\n    train_set_x_orig = np.array(train_dataset[""train_set_x""][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[""train_set_y""][:]) # your train set labels\n\n    test_dataset = h5py.File(\'datasets/test_catvnoncat.h5\', ""r"")\n    test_set_x_orig = np.array(test_dataset[""test_set_x""][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[""test_set_y""][:]) # your test set labels\n\n    classes = np.array(test_dataset[""list_classes""][:]) # the list of classes\n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes'"
深度学习 吴恩达 Python代码实现/Class 1 Week 3 assignment3/assignment3.py,0,"b'""""""\nCreated on 2019-5-2 14:58:45\n\n@author: liweimin\n""""""\n\nimport random\nimport time\nimport jieba\nimport math\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom testCases import *\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\nfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n\nnp.random.seed(1) # set a seed so that the results are consistent\n#\xe5\xae\x9a\xe4\xb9\x89\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n#\xe5\xbe\xaa\xe7\x8e\xaf\xef\xbc\x9a\n#\xe8\xae\xa1\xe7\xae\x97\xe6\xad\xa3\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n#\xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n#\xe8\xae\xa1\xe7\xae\x97\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe6\x9d\xa5\xe5\xbe\x97\xe5\x88\xb0grad\n#\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n\nX, Y = load_planar_dataset() \nplt.scatter(X[0, :], X[1, :], c=Y.reshape(X[0,:].shape), s=40, cmap=plt.cm.Spectral);\n#\xe5\x8e\x9f\xe4\xbb\xa3\xe7\xa0\x81c=Y\xef\xbc\x8c\xe4\xbc\x9a\xe6\x8a\xa5\xe9\x94\x99\nshape_X = X.shape\nshape_Y = Y.shape\nm = shape_X[1]  # training set size\n\nprint (\'The shape of X is: \' + str(shape_X))\nprint (\'The shape of Y is: \' + str(shape_Y))\nprint (\'I have m = %d training examples!\' % (m))\n\n\n# GRADED FUNCTION: layer_sizes\n\ndef layer_sizes(X, Y):\n    n_x=X.shape[0]\n    n_h=4\n    n_y=Y.shape[0]  \n    \n    return (n_x, n_h, n_y)\n\nX_assess, Y_assess = layer_sizes_test_case()\n(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)\nprint(""The size of the input layer is: n_x = "" + str(n_x))\nprint(""The size of the hidden layer is: n_h = "" + str(n_h))\nprint(""The size of the output layer is: n_y = "" + str(n_y))\n\n# GRADED FUNCTION: initialize_parameters\n\ndef initialize_parameters(n_x, n_h, n_y):\n    np.random.seed(2)\n    W1 = np.random.randn(n_h, n_x) * 0.01\n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h) * 0.01\n    b2 = np.zeros((n_y, 1)\n    assert (W1.shape == (n_h, n_x))#\xe6\x9c\x89\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe5\xbe\x85\xe8\xa7\xa3\xe5\x86\xb3\n    assert (b1.shape == (n_h, 1))\n    assert (W2.shape == (n_y, n_h))\n    assert (b2.shape == (n_y, 1))\n\n    parameters = {""W1"": W1,\n                  ""b1"": b1,\n                  ""W2"": W2,\n                  ""b2"": b2}\n    return parameters\n\n\n\n\n\n\n\n\n'"
深度学习 吴恩达 Python代码实现/Class 1 Week 3 assignment3/planar_utils.py,0,"b'import matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\n\ndef plot_decision_boundary(model, X, y):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    Z = model(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.ylabel(\'x2\')\n    plt.xlabel(\'x1\')\n    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n    \n\ndef sigmoid(x):\n    """"""\n    Compute the sigmoid of x\n\n    Arguments:\n    x -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(x)\n    """"""\n    s = 1/(1+np.exp(-x))\n    return s\n\ndef load_planar_dataset():\n    np.random.seed(1)\n    m = 400 # number of examples\n    N = int(m/2) # number of points per class\n    D = 2 # dimensionality\n    X = np.zeros((m,D)) # data matrix where each row is a single example\n    Y = np.zeros((m,1), dtype=\'uint8\') # labels vector (0 for red, 1 for blue)\n    a = 4 # maximum ray of the flower\n\n    for j in range(2):\n        ix = range(N*j,N*(j+1))\n        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n        Y[ix] = j\n        \n    X = X.T\n    Y = Y.T\n\n    return X, Y\n\ndef load_extra_datasets():  \n    N = 200\n    noisy_circles = sklearn.datasets.make_circles(n_samples=N, factor=.5, noise=.3)\n    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)\n    blobs = sklearn.datasets.make_blobs(n_samples=N, random_state=5, n_features=2, centers=6)\n    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)\n    \n    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure'"
深度学习 吴恩达 Python代码实现/Class 1 Week 3 assignment3/testCases.py,0,"b""import numpy as np\n\ndef layer_sizes_test_case():\n    np.random.seed(1)\n    X_assess = np.random.randn(5, 3)\n    Y_assess = np.random.randn(2, 3)\n    return X_assess, Y_assess\n\ndef initialize_parameters_test_case():\n    n_x, n_h, n_y = 2, 4, 1\n    return n_x, n_h, n_y\n\ndef forward_propagation_test_case():\n    np.random.seed(1)\n    X_assess = np.random.randn(2, 3)\n\n    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n        [-0.02136196,  0.01640271],\n        [-0.01793436, -0.00841747],\n        [ 0.00502881, -0.01245288]]),\n     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n     'b1': np.array([[ 0.],\n        [ 0.],\n        [ 0.],\n        [ 0.]]),\n     'b2': np.array([[ 0.]])}\n\n    return X_assess, parameters\n\ndef compute_cost_test_case():\n    np.random.seed(1)\n    Y_assess = np.random.randn(1, 3)\n    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n        [-0.02136196,  0.01640271],\n        [-0.01793436, -0.00841747],\n        [ 0.00502881, -0.01245288]]),\n     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n     'b1': np.array([[ 0.],\n        [ 0.],\n        [ 0.],\n        [ 0.]]),\n     'b2': np.array([[ 0.]])}\n\n    a2 = (np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]))\n    \n    return a2, Y_assess, parameters\n\ndef backward_propagation_test_case():\n    np.random.seed(1)\n    X_assess = np.random.randn(2, 3)\n    Y_assess = np.random.randn(1, 3)\n    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n        [-0.02136196,  0.01640271],\n        [-0.01793436, -0.00841747],\n        [ 0.00502881, -0.01245288]]),\n     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n     'b1': np.array([[ 0.],\n        [ 0.],\n        [ 0.],\n        [ 0.]]),\n     'b2': np.array([[ 0.]])}\n\n    cache = {'A1': np.array([[-0.00616578,  0.0020626 ,  0.00349619],\n         [-0.05225116,  0.02725659, -0.02646251],\n         [-0.02009721,  0.0036869 ,  0.02883756],\n         [ 0.02152675, -0.01385234,  0.02599885]]),\n  'A2': np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]),\n  'Z1': np.array([[-0.00616586,  0.0020626 ,  0.0034962 ],\n         [-0.05229879,  0.02726335, -0.02646869],\n         [-0.02009991,  0.00368692,  0.02884556],\n         [ 0.02153007, -0.01385322,  0.02600471]]),\n  'Z2': np.array([[ 0.00092281, -0.00056678,  0.00095853]])}\n    return parameters, cache, X_assess, Y_assess\n\ndef update_parameters_test_case():\n    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n        [-0.02311792,  0.03137121],\n        [-0.0169217 , -0.01752545],\n        [ 0.00935436, -0.05018221]]),\n 'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n 'b1': np.array([[ -8.97523455e-07],\n        [  8.15562092e-06],\n        [  6.04810633e-07],\n        [ -2.54560700e-06]]),\n 'b2': np.array([[  9.14954378e-05]])}\n\n    grads = {'dW1': np.array([[ 0.00023322, -0.00205423],\n        [ 0.00082222, -0.00700776],\n        [-0.00031831,  0.0028636 ],\n        [-0.00092857,  0.00809933]]),\n 'dW2': np.array([[ -1.75740039e-05,   3.70231337e-03,  -1.25683095e-03,\n          -2.55715317e-03]]),\n 'db1': np.array([[  1.05570087e-07],\n        [ -3.81814487e-06],\n        [ -1.90155145e-07],\n        [  5.46467802e-07]]),\n 'db2': np.array([[ -1.08923140e-05]])}\n    return parameters, grads\n\ndef nn_model_test_case():\n    np.random.seed(1)\n    X_assess = np.random.randn(2, 3)\n    Y_assess = np.random.randn(1, 3)\n    return X_assess, Y_assess\n\ndef predict_test_case():\n    np.random.seed(1)\n    X_assess = np.random.randn(2, 3)\n    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n        [-0.02311792,  0.03137121],\n        [-0.0169217 , -0.01752545],\n        [ 0.00935436, -0.05018221]]),\n     'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n     'b1': np.array([[ -8.97523455e-07],\n        [  8.15562092e-06],\n        [  6.04810633e-07],\n        [ -2.54560700e-06]]),\n     'b2': np.array([[  9.14954378e-05]])}\n    return parameters, X_assess\n"""
深度学习 吴恩达 Python代码实现/vector test/vec_test1.py,0,"b'""""""\nCreated on Sat Mar  9 22:02:25 2019\n\n@author: liweimin\n""""""\n\nimport turtle\nimport random\nimport time\nimport jieba\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sklearn\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_boston\nfrom sklearn.datasets import load_iris\n\n\n\na=np.random.rand(1000000)\nb=np.random.rand(1000000)\n\nstart_time=time.perf_counter()\n\nc=np.dot(a,b)\n\nend_time=time.perf_counter()\n\ntime1=end_time-start_time\n\nprint(time1)\nprint(c)\n\n\nstart_time2=time.perf_counter()\n\n\nc=0\nfor i in range(1000000):\n    c+=a[i]*b[i]\n\nend_time2=time.perf_counter()\n\ntime2=end_time2-start_time2\n\nprint(time2)\nprint(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\n\n\n\n\n\n\n    \n\n\n\n       \n    \n    \n\n'"
机器学习实战 Python代码实现/logistic逻辑回归/鸢尾花分类/IrisLogisticRegression.py,0,"b'# encoding: utf-8\n#author :\'liweimin\'\n#2019-3-26 17:09:11\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools as it\nimport matplotlib as mpl\nfrom matplotlib import colors\nfrom sklearn import datasets\nfrom sklearn import model_selection\nimport random\nimport pandas\nfrom sklearn.linear_model import LogisticRegression #\xe7\xba\xbf\xe6\x80\xa7\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe7\x9a\x84\xe9\x80\xbb\xe8\xbe\x91\xe5\x9b\x9e\xe5\xbd\x92  \xe7\x94\xa8\xe4\xba\x8e\xe5\xaf\xb9\xe6\xaf\x94\xe5\x8f\x82\xe7\x85\xa7\nfrom sklearn.metrics import confusion_matrix #\xe8\xae\xa1\xe7\xae\x97\xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x9d\xa5\xe8\xaf\x84\xe4\xbc\xb0\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe6\x80\xa7\nfrom sklearn.metrics import accuracy_score #\xe8\xae\xa1\xe7\xae\x97\xe7\xb2\xbe\xe5\xba\xa6\xe5\xbe\x97\xe5\x88\x86\nfrom sklearn.metrics import classification_report  #\xe5\xb0\x86\xe4\xb8\xbb\xe8\xa6\x81\xe5\x88\x86\xe7\xb1\xbb\xe6\x8c\x87\xe6\xa0\x87\xe4\xbb\xa5\xe6\x96\x87\xe6\x9c\xac\xe8\xbe\x93\xe5\x87\xba\n\n#\xe5\xae\x9a\xe4\xb9\x89Sigmoid\xe6\x9b\xb2\xe7\xba\xbf\ndef Sigmoid(x):  \n    return 1.0 / (1.0 + np.exp(-x))  \n\n# \xe9\x80\xbb\xe8\xbe\x91\xe5\x9b\x9e\xe5\xbd\x92\xe7\xae\x97\xe6\xb3\x95\ndef LogReg(datas,labels):\n    kinds = list(set(labels))  # 3\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\n    means=datas.mean(axis=0) #\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\n    stds=datas.std(axis=0) #\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\n\n    N,M= datas.shape[0],datas.shape[1]+1  #N\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xef\xbc\x8cM\xe5\x8f\x82\xe6\x95\xb0\xe5\x90\x91\xe9\x87\x8f\xe7\x9a\x84\xe7\xbb\xb4\n    K=3 #\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\n\n    data=np.ones((N,M))\n    data[:,1:]=(datas-means)/stds #\xe5\xaf\xb9\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\n\n    W=np.zeros((K-1,M))  #\xe5\xad\x98\xe5\x82\xa8\xe5\x8f\x82\xe6\x95\xb0\xe7\x9f\xa9\xe9\x98\xb5\n    priorEs=np.array([1.0/N*np.sum(data[labels==kinds[i]],axis=0) for i in range(K-1)]) #\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe5\x85\x88\xe9\xaa\x8c\xe6\x9c\x9f\xe6\x9c\x9b\xe5\x80\xbc\n\n    liklist=[]\n    for it in range(1000):\n        lik=0 #\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84\xe5\xaf\xb9\xe6\x95\xb0\xe4\xbc\xbc\xe7\x84\xb6\xe5\x87\xbd\xe6\x95\xb0\xe5\x80\xbc\n        for k in range(K-1): #\xe4\xbc\xbc\xe7\x84\xb6\xe5\x87\xbd\xe6\x95\xb0\xe5\x80\xbc\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\n            lik -= np.sum(np.dot(W[k],data[labels==kinds[k]].transpose()))\n        lik +=1.0/N *np.sum(np.log(np.sum(np.exp(np.dot(W,data.transpose())),axis=0)+1)) #\xe4\xbc\xbc\xe7\x84\xb6\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe7\xac\xac\xe4\xba\x8c\xe9\x83\xa8\xe5\x88\x86\n        liklist.append(lik)\n\n        wx=np.exp(np.dot(W,data.transpose()))\n        probs=np.divide(wx,1+np.sum(wx,axis=0).transpose()) # K-1 *N\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\n        posteriorEs=1.0/N*np.dot(probs,data) #\xe5\x90\x84\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe5\x90\x8e\xe9\xaa\x8c\xe6\x9c\x9f\xe6\x9c\x9b\xe5\x80\xbc\n        gradients=posteriorEs - priorEs +1.0/100 *W #\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe9\xa1\xb9\xe6\x98\xaf\xe9\xab\x98\xe6\x96\xaf\xe9\xa1\xb9\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\n        W -= gradients #\xe5\xaf\xb9\xe5\x8f\x82\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xbf\xae\xe6\xad\xa3\n    print(""\xe8\xbe\x93\xe5\x87\xbaW\xe4\xb8\xba\xef\xbc\x9a"",W)\n    return W\n\n#\xe6\xa0\xb9\xe6\x8d\xae\xe8\xae\xad\xe7\xbb\x83\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0W\xe5\x92\x8c\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe3\x80\x82\xe8\xbe\x93\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x92\x8c\xe7\x94\xb1LogReg\xe7\xae\x97\xe6\xb3\x95\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0W\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xe4\xb8\xba\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x80\xbc\ndef iris_predict(datas,W):\n    N, M = datas.shape[0], datas.shape[1] + 1  # N\xe6\x98\xaf\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xef\xbc\x8cM\xe6\x98\xaf\xe5\x8f\x82\xe6\x95\xb0\xe5\x90\x91\xe9\x87\x8f\xe7\x9a\x84\xe7\xbb\xb4\n    K = 3  # k=3\xe6\x98\xaf\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\n    data = np.ones((N, M))\n    means = datas.mean(axis=0)  # \xe5\x90\x84\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\n    stds = datas.std(axis=0)  # \xe5\x90\x84\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\n    data[:, 1:] = (datas - means) / stds  # \xe5\xaf\xb9\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\n\n    # probM\xe6\xaf\x8f\xe8\xa1\x8c\xe4\xb8\x89\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xef\xbc\x8c\xe5\x88\x86\xe5\x88\xab\xe8\xa1\xa8\xe7\xa4\xbadata\xe4\xb8\xad\xe5\xaf\xb9\xe5\xba\x94\xe6\xa0\xb7\xe6\x9c\xac\xe8\xa2\xab\xe5\x88\xa4\xe7\xbb\x99\xe4\xb8\x89\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\n    probM = np.ones((N, K))\n    print(""data.shape:"", data.shape)\n    print(""datas.shape:"", datas.shape)\n    print(""W.shape:"", W.shape)\n    print(""probM.shape:"", probM.shape)\n    probM[:, :-1] = np.exp(np.dot(data, W.transpose()))\n    probM /= np.array([np.sum(probM, axis=1)]).transpose()  # \xe5\xbe\x97\xe5\x88\xb0\xe6\xa6\x82\xe7\x8e\x87\n\n    predict = np.argmax(probM, axis=1).astype(int)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe6\xa6\x82\xe7\x8e\x87\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\n    print(""\xe8\xbe\x93\xe5\x87\xbapredict\xe4\xb8\xba\xef\xbc\x9a"", predict)\n    return predict\n\n\nif __name__ == \'__main__\':\n    \n    attributes=[\'sepal_length\',\'sepal_width\',\'petal_length\',\'petal_width\'] #\xe9\xb8\xa2\xe5\xb0\xbe\xe8\x8a\xb1\xe5\xb1\x9e\xe6\x80\xa7\xe5\x90\x8d\n\n    datas=[]\n    labels=[]\n    datas_test=[]\n    labels_test=[]\n\nwith open(\'IRIS.txt\', \'r\') as f_iris:\n    lines = f_iris.readlines()\n\nwith open(\'train.txt\', \'w\') as ftr, open(\'test.txt\', \'w\') as fte:\n    for i in range(105):   #70%\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c30%\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xb5\x8b\xe8\xaf\x95\n        ftr.write(lines.pop(random.randint(0, len(lines) - 1)))\n    fte.writelines(lines)\ndata_file=open(\'train.txt\',\'r\')\ndata_file_test=open(\'test.txt\',\'r\')\n\nfor line in data_file.readlines():\n    linedata = line.split(\',\')\n    datas.append(linedata[:-1])  # \xe9\x80\x89\xe5\x8f\x96\xe5\x89\x8d4\xe5\x88\x97\xe6\x98\xaf4\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe5\x80\xbc\n    labels.append(linedata[-1].replace(\'\\n\', \'\'))  # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\xe6\x98\xaf\xe7\xb1\xbb\xe5\x88\xab\nfor line in data_file_test.readlines():\n    linedata = line.split(\',\')\n    datas_test.append(linedata[:-1])  # \xe9\x80\x89\xe5\x8f\x96\xe5\x89\x8d4\xe5\x88\x97\xe6\x98\xaf4\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe7\x9a\x84\xe5\x80\xbc\n    labels_test.append(linedata[-1].replace(\'\\n\', \'\'))  # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\xe6\x98\xaf\xe7\xb1\xbb\xe5\x88\xab\n\ndatas=np.array(datas)\ndatas_test=np.array(datas_test)\ndatas=datas.astype(float) #\xe5\xb0\x86\xe4\xba\x8c\xe7\xbb\xb4\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe6\x95\xb0\xe7\xbb\x84\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe6\xb5\xae\xe7\x82\xb9\xe6\x95\xb0\xe6\x95\xb0\xe7\xbb\x84\ndatas_test=datas_test.astype(float)\nlabels=np.array(labels)\nlabels_test=np.array(labels_test)\nkinds=list(set(labels)) #3\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\nkinds_test=list(set(labels_test))\n\n#\xe9\x80\x9a\xe8\xbf\x87LogReg\xe7\xae\x97\xe6\xb3\x95\xe5\xbe\x97\xe5\x88\xb0\xe5\x8f\x82\xe6\x95\xb0 W\nW=LogReg(datas,labels)\n\n#\xe9\x80\x9a\xe8\xbf\x87iris_predict\xef\xbc\x88\xef\xbc\x89\xe5\x87\xbd\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\npredict=iris_predict(datas_test,W)\n\n# rights\xe5\x88\x97\xe8\xa1\xa8\xe5\x82\xa8\xe5\xad\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xaelabels\xe6\x95\xb0\xe6\x8d\xae\xe7\x94\x9f\xe6\x88\x90\nN = datas_test.shape[0]\nrights = np.zeros(N)\nrights[labels_test == kinds_test[1]] = 1\nrights[labels_test == kinds_test[2]] = 2\nrights = rights.astype(int)\n\n\nerr=np.sum(predict != rights)\nright=45-err\nprint(""\xe9\xa2\x84\xe6\xb5\x8b\xe6\xad\xa3\xe7\xa1\xae\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\xba\xef\xbc\x9a%d\\n"" % right)\nprint(""\xe9\xa2\x84\xe6\xb5\x8b\xe9\x94\x99\xe8\xaf\xaf\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\xba\xef\xbc\x9a%d\\n"" % err)\nacc=(45-err)/45\nprint(\'logistic\xe5\x9b\x9e\xe5\xbd\x92\xe5\x87\x86\xe7\xa1\xae\xe5\xba\xa6\xe4\xb8\xba:%f\' % acc)\n\n\n#Python \xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84logistic\xe7\xae\x97\xe6\xb3\x95(95%\xe5\xb7\xa6\xe5\x8f\xb3)\xef\xbc\x8c\xe5\xaf\xb9\xe6\xaf\x94\xe6\x89\x8b\xe5\x86\x99\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8898%\xe5\xb7\xa6\xe5\x8f\xb3\xef\xbc\x89\ndata = ""iris.data""\nnames = [\'sepal-length\', \'sepal-width\', \'petal-length\', \'petal-width\', \'class\']\ndataset = pandas.read_csv(data, names=names) #\xe8\xaf\xbb\xe5\x8f\x96csv\xe6\x95\xb0\xe6\x8d\xae\n \narray = dataset.values #\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe6\x95\xb0\xe7\xbb\x84\xe5\xbd\xa2\xe5\xbc\x8f\nX = array[:,0:4] #\xe5\x8f\x96\xe5\x89\x8d\xe5\x9b\x9b\xe5\x88\x97\xef\xbc\x8c\xe5\x8d\xb3\xe5\xb1\x9e\xe6\x80\xa7\xe6\x95\xb0\xe5\x80\xbc\nY = array[:,4] #\xe5\x8f\x96\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\xef\xbc\x8c\xe7\xa7\x8d\xe7\xb1\xbb\nvalidation_size = 0.30 #\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe8\xa7\x84\xe6\xa8\xa1\nseed = 2019\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed) #\xe5\x88\x86\xe5\x89\xb2\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86 \n\nseed = 2019\nscoring = \'accuracy\'\n\nlog = LogisticRegression()\nlog.fit(X_train, Y_train) #log\xe6\x8b\x9f\xe5\x90\x88\xe5\xba\x8f\xe5\x88\x97\xe9\x9b\x86\npredictions = log.predict(X_validation) #\xe9\xa2\x84\xe6\xb5\x8b\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\nprint(\'log\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\xb2\xbe\xe5\xba\xa6\xe5\xbe\x97\xe5\x88\x86:%f\' % accuracy_score(Y_validation, predictions)) \nprint(\'\xe5\x88\x86\xe7\xb1\xbb\xe9\xa2\x84\xe6\xb5\x8b\xe6\x8a\xa5\xe5\x91\x8a\' +classification_report(Y_validation, predictions)) \n\n\n'"
机器学习实战 Python代码实现/logistic逻辑回归/鸢尾花分类/irisTest.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Tue Mar 26 11:17:33 2019\n\n@author: Gift\n""""""\n\n# Load libraries\nimport pandas\nfrom pandas.tools.plotting import scatter_matrix #\xe5\xaf\xbc\xe5\x85\xa5\xe6\x95\xa3\xe7\x82\xb9\xe5\x9b\xbe\xe7\x9f\xa9\xe9\x98\xb5\xe5\x8c\x85\nimport matplotlib.pyplot as plt  \nfrom sklearn import model_selection  #\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xaf\x94\xe8\xbe\x83\xe5\x92\x8c\xe9\x80\x89\xe6\x8b\xa9\xe5\x8c\x85\nfrom sklearn.metrics import classification_report  #\xe5\xb0\x86\xe4\xb8\xbb\xe8\xa6\x81\xe5\x88\x86\xe7\xb1\xbb\xe6\x8c\x87\xe6\xa0\x87\xe4\xbb\xa5\xe6\x96\x87\xe6\x9c\xac\xe8\xbe\x93\xe5\x87\xba\nfrom sklearn.metrics import confusion_matrix #\xe8\xae\xa1\xe7\xae\x97\xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x9d\xa5\xe8\xaf\x84\xe4\xbc\xb0\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe6\x80\xa7\nfrom sklearn.metrics import accuracy_score #\xe8\xae\xa1\xe7\xae\x97\xe7\xb2\xbe\xe5\xba\xa6\xe5\xbe\x97\xe5\x88\x86\nfrom sklearn.linear_model import LogisticRegression #\xe7\xba\xbf\xe6\x80\xa7\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe7\x9a\x84\xe9\x80\xbb\xe8\xbe\x91\xe5\x9b\x9e\xe5\xbd\x92\nfrom sklearn.tree import DecisionTreeClassifier #\xe6\xa0\x91\xe7\xae\x97\xe6\xb3\x95\xe4\xb8\xad\xe7\x9a\x84\xe5\x86\xb3\xe7\xad\x96\xe6\xa0\x91\xe5\x88\x86\xe7\xb1\xbb\xe5\x8c\x85\nfrom sklearn.neighbors import KNeighborsClassifier #\xe5\xaf\xbc\xe5\x85\xa5\xe6\x9c\x80\xe8\xbf\x91\xe9\x82\xbb\xe7\xae\x97\xe6\xb3\x95\xe4\xb8\xad\xe7\x9a\x84KNN\xe6\x9c\x80\xe8\xbf\x91\xe9\x82\xbb\xe5\x88\x86\xe7\xb1\xbb\xe5\x8c\x85\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis #\xe5\x88\xa4\xe5\x88\xab\xe5\x88\x86\xe6\x9e\x90\xe7\xae\x97\xe6\xb3\x95\xe4\xb8\xad\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe5\x88\xa4\xe5\x88\xab\xe5\x88\x86\xe6\x9e\x90\xe5\x8c\x85\nfrom sklearn.naive_bayes import GaussianNB #\xe6\x9c\xb4\xe7\xb4\xa0\xe8\xb4\x9d\xe5\x8f\xb6\xe6\x96\xaf\xe4\xb8\xad\xe7\x9a\x84\xe9\xab\x98\xe6\x96\xaf\xe6\x9c\xb4\xe7\xb4\xa0\xe8\xb4\x9d\xe5\x8f\xb6\xe6\x96\xaf\xe5\x8c\x85\nfrom sklearn.svm import SVC  #\xe6\x94\xaf\xe6\x8c\x81\xe5\x90\x91\xe9\x87\x8f\xe6\x9c\xba\xe7\xae\x97\xe6\xb3\x95\xe4\xb8\xad\xe7\x9a\x84\xe6\x94\xaf\xe6\x8c\x81\xe5\x90\x91\xe9\x87\x8f\xe5\x88\x86\xe7\xb1\xbb\xe5\x8c\x85\nfrom sklearn import datasets\n# Load dataset\n\n# Load dataset\ndata = ""iris.data""\nnames = [\'sepal-length\', \'sepal-width\', \'petal-length\', \'petal-width\', \'class\']\ndataset = pandas.read_csv(data, names=names) #\xe8\xaf\xbb\xe5\x8f\x96csv\xe6\x95\xb0\xe6\x8d\xae\n\n# shape\nprint(dataset.shape)\n\n# head\nprint(dataset.head(20))\n\nprint(dataset.describe())\n\n# class distribution\nprint(dataset.groupby(\'class\').size())\n\n# histograms\ndataset.hist()\nplt.show()\n\n# Split-out validation dataset\narray = dataset.values #\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe6\x95\xb0\xe7\xbb\x84\xe5\xbd\xa2\xe5\xbc\x8f\nX = array[:,0:4] #\xe5\x8f\x96\xe5\x89\x8d\xe5\x9b\x9b\xe5\x88\x97\xef\xbc\x8c\xe5\x8d\xb3\xe5\xb1\x9e\xe6\x80\xa7\xe6\x95\xb0\xe5\x80\xbc\nY = array[:,4] #\xe5\x8f\x96\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\xef\xbc\x8c\xe7\xa7\x8d\xe7\xb1\xbb\nvalidation_size = 0.30 #\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe8\xa7\x84\xe6\xa8\xa1\nseed = 7\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed) #\xe5\x88\x86\xe5\x89\xb2\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n\n# Test options and evaluation metric\nseed = 7\nscoring = \'accuracy\'\n\n# Spot Check Algorithms\nmodels = [] #\xe5\xbb\xba\xe7\xab\x8b\xe5\x88\x97\xe8\xa1\xa8\nmodels.append((\'LR\', LogisticRegression())) #\xe5\xbe\x80maodels\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x85\x83\xe7\xbb\x84\xef\xbc\x88\xe7\xae\x97\xe6\xb3\x95\xe5\x90\x8d\xe7\xa7\xb0\xef\xbc\x8c\xe7\xae\x97\xe6\xb3\x95\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x89\nmodels.append((\'LDA\', LinearDiscriminantAnalysis()))\nmodels.append((\'KNN\', KNeighborsClassifier()))\nmodels.append((\'CART\', DecisionTreeClassifier()))\nmodels.append((\'NB\', GaussianNB()))\nmodels.append((\'SVM\', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models: #\xe5\xb0\x86\xe7\xae\x97\xe6\xb3\x95\xe5\x90\x8d\xe7\xa7\xb0\xe4\xb8\x8e\xe5\x87\xbd\xe6\x95\xb0\xe5\x88\x86\xe5\x88\xab\xe8\xaf\xbb\xe5\x8f\x96\n\tkfold = model_selection.KFold(n_splits=10, random_state=seed) #\xe5\xbb\xba\xe7\xab\x8b10\xe5\x80\x8d\xe4\xba\xa4\xe5\x8f\x89\xe9\xaa\x8c\xe8\xaf\x81\n\tcv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring) #\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x97\xe6\xb3\x95\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbd\x9c\xe4\xb8\xba\xe5\x85\xb6\xe4\xb8\xad\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe4\xb8\x80\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe7\xb2\xbe\xe5\xba\xa6\xe5\xbe\x97\xe5\x88\x86\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = ""%s: %f (%f)"" % (name, cv_results.mean(), cv_results.std()) \n\tprint(msg) \n    \n\n# Compare Algorithms\nfig = plt.figure()\nfig.suptitle(\'Algorithm Comparison\')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()\n\n# Make predictions on validation dataset\nknn = KNeighborsClassifier()\nknn.fit(X_train, Y_train) #knn\xe6\x8b\x9f\xe5\x90\x88\xe5\xba\x8f\xe5\x88\x97\xe9\x9b\x86\npredictions = knn.predict(X_validation) #\xe9\xa2\x84\xe6\xb5\x8b\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\nprint(accuracy_score(Y_validation, predictions)) #\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\xb2\xbe\xe5\xba\xa6\xe5\xbe\x97\xe5\x88\x86\nprint(confusion_matrix(Y_validation, predictions)) #\xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\nprint(classification_report(Y_validation, predictions)) #\xe5\x88\x86\xe7\xb1\xbb\xe9\xa2\x84\xe6\xb5\x8b\xe6\x8a\xa5\xe5\x91\x8a\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
机器学习实战 Python代码实现/logistic逻辑回归/鸢尾花分类/perceptron.py,0,"b'import numpy as np\nimport math\nimport random\nimport pandas\nfrom sklearn import datasets\nfrom sklearn.model_selection import  train_test_split\n\n#sigmoid\xe5\x87\xbd\xe6\x95\xb0\ndef sigmoid(x):  \n        return 1.0 / (1.0 + np.exp(-x))  \n    \ndef sample_mean(data):\n    return sum(data) / len(data)\n\n\ndef sample_variance(data, mean=None):\n    if mean is None:\n        mean = sample_mean(data)\n\n    return sum([(x - mean) ** 2 for x in data]) / len(data)\n\n\ndef get_covariance_matrix(sample_input):\n    row, column = sample_input.shape\n    means = np.zeros((column))\n\n    for i in range(column):\n        for j in range(row):\n            means.itemset(i, means.item(i) + sample_input.item((j, i))/float(row))\n\n    result = np.asmatrix(np.zeros((column, column)))\n\n    for i in range(column):\n        for j in range(i+1):\n            total = 0.0\n            for k in range(row):\n                total += (sample_input.item((k, i)) - means.item(i))*(sample_input.item((k, j)) - means.item(j))\n\n            total /= float(row - 1)\n            result.itemset((i, j), total)\n            result.itemset((j, i), total)\n    return result\n\n\ndef __init__(self, k, d):\n        self.k = k\n        self.d = d\n        self.learning_rate = 0.01\n        self.weights = np.random.rand(k, d + 1)\n#\xe8\xae\xad\xe7\xbb\x83\xe7\xae\x97\xe6\xb3\x95\ndef train(self, input_data, output_data):\n        for outer in range(1000):\n            error = 0.0\n            delta = np.zeros((self.k, self.d + 1))\n\n            for t in range(input_data.shape[0]):\n                outputs = [0.0] * self.k\n                positive_count = 0\n                for i in range(self.k):\n                    for j in range(self.d + 1):\n                        outputs[i] += self.weights.item((i, j)) * input_data.item((t, j))\n\n                    if outputs[i] > 0:\n                        positive_count += 1\n\n                y = [0.0] * self.k\n                total = 0.0\n\n                for i in range(self.k):\n                    y[i] = math.pow(math.e, outputs[i])\n                    total += y[i]\n\n               log_index = 0\n                for i in range(self.k):\n                    y[i] /= total\n                    if y[i] > y[m_index]:\n                       log_index = i\n\n                for i in range(self.k):\n                    expected_result = output_data.item((t, i))\n                    if (expected_result == 1 and outputs[i] <= 0.0) or (expected_result == 0 and outputs[i] >= 0.0):\n                        error += 1\n                        for j in range(self.d + 1):\n                            delta.itemset((i, j), delta.item((i, j)) + (output_data.item((t, i)) - y[i]) * input_data.item((t, j)))\n\n            if error == 0.0:\n                break\n\n            for i in range(self.k):\n                for j in range(self.d + 1):\n                    self.weights.itemset((i, j), self.weights.item((i, j)) + self.learning_rate * delta.item((i, j)))\n#\xe5\x8f\x8d\xe9\xa6\x88\ndef response(self, input_data):\n        outputs = [0.0] * self.k\n        for i in range(self.k):\n            for j in range(self.d + 1):\n                outputs[i] += self.weights.item((i, j)) * input_data.item((j,))\n\n        y = [0.0] * self.k\n        total = 0.0\n\n        for i in range(self.k):\n            y[i] = math.pow(math.e, outputs[i])\n            total += y[i]\n\n       log_index = 0\n        for i in range(self.k):\n            y[i] /= total\n            if y[i] > y[m_index]:\n               log_index = i\n\n        returnlog_index\n    \n\nif __name__ == \'__main__\':\n\n    datas=[] \n    labels=[]\n    datas_test=[]\n    labels_test=[]\n\nwith open(\'IRIS.txt\', \'r\') as f_iris:\n    lines = f_iris.readlines()\n\nwith open(\'train.txt\', \'w\') as ftr, open(\'test.txt\', \'w\') as fte:\n    for i in range(105):   #70%\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c30%\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe6\x9c\x89\xe4\xb8\x80\xe7\x82\xb9\xe5\xb0\x8f\xe9\x97\xae\xe9\xa2\x98\n        ftr.write(lines.pop(random.randint(0, len(lines) - 1)))\n    fte.writelines(lines)\ndata_file=open(\'train.txt\',\'r\')\ndata_file_test=open(\'test.txt\',\'r\')\n\nfor line in data_file.readlines():\n    linedata = line.split(\',\')\n    datas.append(linedata[:-1])  \n    labels.append(linedata[-1].replace(\'\\n\', \'\'))  \nfor line in data_file_test.readlines():\n    linedata = line.split(\',\')\n    datas_test.append(linedata[:-1])  \n    labels_test.append(linedata[-1].replace(\'\\n\', \'\'))  \n        \niris=datasets.load_iris() # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n\nX = iris.data # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe9\x83\xa8\xe5\x88\x86\nY = iris.target # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe6\xa0\x87\xe7\xad\xbe\n\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 0)\n\n    \ntrain(x_train,x_test,\'output2.txt\')\n\n# rights\xe5\x88\x97\xe8\xa1\xa8\xe5\x82\xa8\xe5\xad\x98\xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\nN = datas_test.shape[0]\nrights = np.zeros(N)\nrights[labels_test == kinds_test[1]] = 1\nrights[labels_test == kinds_test[2]] = 2\nrights = rights.astype(int)\n\n\nerr=np.sum(predict != rights)\nright=45-err\nprint(""\xe9\xa2\x84\xe6\xb5\x8b\xe6\xad\xa3\xe7\xa1\xae\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\xba\xef\xbc\x9a%d\\n"" % right)\nprint(""\xe9\xa2\x84\xe6\xb5\x8b\xe9\x94\x99\xe8\xaf\xaf\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\xba\xef\xbc\x9a%d\\n"" % err)\nacc=(45-err)/45\nprint(\'logistic\xe5\x9b\x9e\xe5\xbd\x92\xe5\x87\x86\xe7\xa1\xae\xe5\xba\xa6\xe4\xb8\xba:%f\' % acc)\n\n\n#Python \xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84logistic\xe7\xae\x97\xe6\xb3\x95(95%\xe5\xb7\xa6\xe5\x8f\xb3)\xef\xbc\x8c\xe5\xaf\xb9\xe6\xaf\x94\xe6\x89\x8b\xe5\x86\x99\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8898%\xe5\xb7\xa6\xe5\x8f\xb3\xef\xbc\x89\ndata = ""iris.data""\nnames = [\'sepal-length\', \'sepal-width\', \'petal-length\', \'petal-width\', \'class\']\ndataset = pandas.read_csv(data, names=names) \n \narray = dataset.values \nX = array[:,0:4] \nY = array[:,4] \nvalidation_size = 0.30 \nseed = 2019\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed) #\xe5\x88\x86\xe5\x89\xb2\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86 \n\nseed = 2019\nscoring = \'accuracy\'\n\nlog = LogisticRegression()\nlog.fit(X_train, Y_train) #log\xe6\x8b\x9f\xe5\x90\x88\xe5\xba\x8f\xe5\x88\x97\xe9\x9b\x86\npredictions = log.predict(X_validation) #\xe9\xa2\x84\xe6\xb5\x8b\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\nprint(\'log\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\xb2\xbe\xe5\xba\xa6\xe5\xbe\x97\xe5\x88\x86:%f\' % accuracy_score(Y_validation, predictions)) \nprint(\'\xe5\x88\x86\xe7\xb1\xbb\xe9\xa2\x84\xe6\xb5\x8b\xe6\x8a\xa5\xe5\x91\x8a\' +classification_report(Y_validation, predictions)) '"
