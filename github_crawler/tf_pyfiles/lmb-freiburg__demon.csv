file_path,api_count,code
datasets/generate_sun3d_train_datasets.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport os\nimport sys\nimport math\nimport pickle\nimport argparse\nimport itertools\nimport h5py\nfrom multiprocessing import Pool\ndatasets_dir = os.path.dirname(__file__)\nsys.path.insert(0, os.path.join(datasets_dir, \'..\', \'python\'))\n\nfrom depthmotionnet.dataset_tools.sun3d_utils import *\nfrom depthmotionnet.dataset_tools.view_tools import *\nfrom depthmotionnet.dataset_tools.view_io import *\n\n\ndef create_train_file(outfile, sun3d_data_path, seq_name, baseline_range, seq_sharpness_dict):\n    """"""Creates a h5 file with training samples with a specific baseline range\n\n    outfile: str\n        Output file\n\n    sun3d_data_path: str\n        The path to the sun3d data directory\n\n    seq_name: str\n        sequence name\n\n    baseline_range: tuple(float, float)\n        Minimum and maximum baseline\n\n    seq_sharpness_dict: dict\n        Dictionary with the sharpness score of all sequences.\n        key: str with sequence name\n        value: numpy.ndarray with sharpness scores\n\n    """"""\n    created_groups = 0\n    with h5py.File(outfile,\'w\') as f:\n        created_groups += create_samples_from_sequence(f, sun3d_data_path, seq_name, baseline_range, seq_sharpness_dict[seq_name])\n    return created_groups\n\n\ndef merge_h5files(outfile, files):\n    """"""Merges multiple h5 files into a new file and removes the input files afterwards.\n\n    outfile: str\n        Output file\n\n    files: list of str\n        List of files to merge\n    """"""\n    with h5py.File(outfile,\'w\') as dst:\n        for f in files:\n            print(\'copy\', f, \'to\', outfile)\n            with h5py.File(f,\'r\') as src:\n                for group_name in src:\n                    src.copy(source=group_name, dest=dst)\n    for f in files:\n        os.remove(f)\n\n\n\n\ndef main():\n\n    print(\n""""""================================================================================\n\n This script runs for about 1 day on a computer with 16 threads and requires\n up to 50GB of disk space in the output directory!\n\n================================================================================"""""")\n\n    parser = argparse.ArgumentParser(description=""Generates the sun3d training datasets."")\n    parser.add_argument(""--sun3d_path"", type=str, required=True, help=""The path to the sun3d data directory"")\n    parser.add_argument(""--outputdir"", type=str, default=\'training_data\', help=""Output directory for the generated h5 files"")\n    parser.add_argument(""--threads"", type=int, default=16, help=""Number of threads"")\n\n    args = None\n    try:\n        args = parser.parse_args()\n        print(args)\n    except:\n        return 1\n\n    sun3d_data_path = args.sun3d_path\n    outputdir = args.outputdir\n    os.makedirs(outputdir, exist_ok=True)\n    threads = args.threads\n\n    # read txt file with the train sequence names\n    with open(\'sun3d_train_sequences.txt\', \'r\') as f:\n        sequences = f.read().splitlines()\n\n    # compute the sharpness scores for all sequences and images\n    if os.path.isfile(\'sun3d_seq_sharpness_dict.pkl\'):\n        print(\'Reading sequence sharpness file seq_sharpness_dict.pkl\')\n        with open(\'sun3d_seq_sharpness_dict.pkl\',\'rb\') as f:\n            seq_sharpness_dict = pickle.load(f)\n    else:\n        print(\'Computing sharpness for all images. This could take a while.\')\n        with Pool(threads) as pool:\n            args = [(sun3d_data_path, seq,) for seq in sequences]    \n            sequence_sharpness = pool.starmap(compute_sharpness, args, chunksize=1)\n\n        seq_sharpness_dict = dict(zip(sequences, sequence_sharpness))\n\n        with open(\'sun3d_seq_sharpness_dict.pkl\',\'wb\') as f:\n            pickle.dump(seq_sharpness_dict, f)\n\n\n    # baseline ranges from 1cm-10cm to 1.6m-inf\n    baseline_ranges = [(0.01,0.10), (0.10,0.20), (0.20,0.40), (0.40,0.80), (0.80,1.60), (1.60, float(\'inf\'))]\n\n    with Pool(threads) as pool:\n\n        # create temporary h5 files for each baseline and sequence combination\n        baseline_range_files_dict = {b:[] for b in baseline_ranges}\n        args = []\n        for i, base_range_seq_name in enumerate(itertools.product(baseline_ranges, sequences)):\n            base_range, seq_name = base_range_seq_name\n            #print(base_range, seq_name)\n            outfile = os.path.join(outputdir,""{0}.h5"".format(i))\n            args.append((outfile, sun3d_data_path, seq_name, base_range, seq_sharpness_dict))\n            baseline_range_files_dict[base_range].append(outfile)\n\n        created_groups = pool.starmap(create_train_file, args, chunksize=1)\n\n    # merge temporary files by creating one file per baseline range\n    for base_range in baseline_ranges:\n        outfile = os.path.join(outputdir, \'sun3d_train_{0}m_to_{1}m.h5\'.format(*base_range))\n        merge_h5files(outfile, baseline_range_files_dict[base_range])\n\n\n    print(\'created\', sum(created_groups), \'groups\')\n\n    return 0\n\n\n    \n    \n\nif __name__ == ""__main__"":\n    sys.exit(main())\n\n'"
examples/create_dataset_and_use_readerop.py,2,"b'################################################################################\n# Create a new dataset h5 file that could be used for training\n#\nimport os\nimport sys\nimport numpy as np\nfrom PIL import Image\nimport h5py\n\nexamples_dir = os.path.dirname(__file__)\nsys.path.insert(0, os.path.join(examples_dir, \'..\', \'python\'))\nfrom depthmotionnet.dataset_tools import *\n\n\n# intrinsics supported by DeMoN\nnormalized_intrinsics = [0.89115971, 1.18821287, 0.5, 0.5]\n\n# unique group name not starting with \'.\'\ngroup_name = \'sculpture-0001\'\n\n# write a new dataset with a single group and two views\nwith h5py.File(\'dataset.h5\',\'w\') as f:\n    \n    for i in range(2):\n        img = Image.open(\'sculpture{0}.png\'.format(i+1))\n        Rt = np.loadtxt(\'sculpture_Rt{0}.txt\'.format(i+1))\n        depth = np.load(\'sculpture_depth{0}.npy\'.format(i+1))\n        K = np.eye(3)\n        K[0,0] = normalized_intrinsics[0] * img.size[0]\n        K[1,1] = normalized_intrinsics[1] * img.size[1]\n        K[0,2] = normalized_intrinsics[2] * img.size[0]\n        K[1,2] = normalized_intrinsics[2] * img.size[1]\n\n        # create a View tuple\n        view = View(R=Rt[:,:3], t=Rt[:,3], K=K, image=img, depth=depth, depth_metric=\'camera_z\')\n\n        # write view to the h5 file\n        # view enumeration must start with 0 (\'v0\')\n        view_group = f.require_group(group_name+\'/frames/t0/v{0}\'.format(i))\n        write_view(view_group, view)\n\n    # write valid image pair combinations to the group t0\n    viewpoint_pairs = np.array([0, 1, 1, 0], dtype=np.int32)\n    time_group = f[group_name][\'frames/t0\']\n    time_group.attrs[\'viewpoint_pairs\'] = viewpoint_pairs\n\n\n################################################################################\n# Use the reader op to read the created h5 file\n#\nfrom depthmotionnet.datareader import *\nimport json\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\n\n\n# keys for the requested output tensors. \n# These keys will be passed to the data reader op.\ndata_tensors_keys = (\'IMAGE_PAIR\', \'MOTION\', \'DEPTH\', \'INTRINSICS\')\n\n# the following parameters are just an example and are not optimized for training\nreader_params = {\n     \'batch_size\': 1,\n     \'test_phase\': False,\n     \'builder_threads\': 1,\n     \'inverse_depth\': True,\n     \'motion_format\': \'ANGLEAXIS6\',\n     \'norm_trans_scale_depth\': True,\n     # downsampling of image and depth is supported\n     \'scaled_height\': 96,\n     \'scaled_width\': 128,\n     \'scene_pool_size\': 5, # for actual training this should be around 500\n     \'augment_rot180\': 0,\n     \'augment_mirror_x\': 0,\n     \'top_output\': data_tensors_keys, # request data tensors\n     \'source\': [{\'path\': \'dataset.h5\', \'weight\': [{\'t\': 0, \'v\': 1.0}]},],\n    }\n\nreader_tensors = multi_vi_h5_data_reader(len(data_tensors_keys), json.dumps(reader_params))\n# create a dict to make the distinct data tensors accessible via keys\ndata_dict = dict(zip(data_tensors_keys,reader_tensors[2]))\n\ngpu_options = tf.GPUOptions()\ngpu_options.per_process_gpu_memory_fraction=0.8 # leave some memory to other processes\nsession = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))\n\nresult =  session.run(data_dict)\n\n# show the depth ground truth.\n# Note that the data reader op replaces invalid depth values with nan.\nplt.imshow(result[\'DEPTH\'].squeeze(), cmap=\'Greys\')\nplt.show()\n\n# visualize the data as point cloud if vtk is available\ntry:\n    from depthmotionnet.vis import *\n    visualize_prediction(\n        inverse_depth=result[\'DEPTH\'], \n        image=result[\'IMAGE_PAIR\'][0,0:3], \n        rotation=result[\'MOTION\'][0,0:3], \n        translation=result[\'MOTION\'][0,3:])\nexcept ImportError as err:\n    print(""Cannot visualize as pointcloud."", err)\n\n\n\n'"
examples/evaluation.py,11,"b'#\n# This script computes the depth and motion errors for the network predictions.\n#\n# Note that numbers are not identical to the values reported in the paper, due \n# to implementation differences between the caffe and tensorflow version.\n#\n# Running this script requires about 4gb of disk space.\n#\n# This script expects the test datasets in the folder ../datasets\n# Use the provided script in ../datasets for downloading the data.\n#\nimport os\nimport sys\nimport json\nimport h5py\nimport xarray\nimport numpy as np\nimport lmbspecialops as sops\nimport tensorflow as tf\n\nexamples_dir = os.path.dirname(__file__)\nweights_dir = os.path.join(examples_dir,\'..\',\'weights\')\nsys.path.insert(0, os.path.join(examples_dir, \'..\', \'python\'))\n\nfrom depthmotionnet.datareader import *\nfrom depthmotionnet.networks_original import *\nfrom depthmotionnet.helpers import convert_NCHW_to_NHWC, convert_NHWC_to_NCHW\nfrom depthmotionnet.evaluation import *\n\n\ndef create_ground_truth_file(dataset, dataset_dir):\n    """"""Creates a hdf5 file with the ground truth test data\n    \n    dataset: str\n        name of the dataset\n    dataset_dir: str\n        path to the directory containing the datasets\n\n    Returns the path to the created file\n    """"""\n    ds = dataset\n    # destination file\n    ground_truth_file = \'{0}_ground_truth.h5\'.format(ds)\n    \n    if os.path.isfile(ground_truth_file):\n        return ground_truth_file # skip existing files\n    \n    print(\'creating {0}\'.format(ground_truth_file))\n    \n    # data types requested from the reader op\n    data_tensors_keys = (\'IMAGE_PAIR\', \'MOTION\', \'DEPTH\', \'INTRINSICS\')\n\n    reader_params = {\n         \'batch_size\': 1,\n         \'test_phase\': True,   # deactivates randomization\n         \'builder_threads\': 1, # must be 1 in test phase\n         \'inverse_depth\': True,\n         \'motion_format\': \'ANGLEAXIS6\',\n         # True is also possible here. If set to True we store ground truth with \n         # precomputed normalization. False keeps the original information.\n         \'norm_trans_scale_depth\': False,\n         # original data resolution\n         \'scaled_height\': 480,\n         \'scaled_width\': 640,\n         \'scene_pool_size\': 5, \n         # no augmentation\n         \'augment_rot180\': 0,\n         \'augment_mirror_x\': 0,\n         \'top_output\': data_tensors_keys,\n         \'source\': [{\'path\': os.path.join(dataset_dir,\'{0}_test.h5\'.format(ds))}],\n        }\n\n    reader_tensors = multi_vi_h5_data_reader(len(data_tensors_keys), json.dumps(reader_params))\n    \n    # create a dict to make the distinct data tensors accessible via keys\n    data_dict = dict(zip(data_tensors_keys,reader_tensors[2]))\n    info_tensor = reader_tensors[0]\n    sample_ids_tensor = reader_tensors[1]\n    rotation_tensor, translation_tensor = tf.split(data_dict[\'MOTION\'], 2, axis=1)\n\n    flow_tensor = sops.depth_to_flow(data_dict[\'DEPTH\'], data_dict[\'INTRINSICS\'], rotation_tensor, translation_tensor, inverse_depth=True, normalize_flow=True)\n\n    gpu_options = tf.GPUOptions()\n    gpu_options.per_process_gpu_memory_fraction=0.8 # leave some memory to other processes\n    session = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))\n\n\n    fetch_dict = {\'INFO\': info_tensor, \'SAMPLE_IDS\': sample_ids_tensor, \'FLOW\': flow_tensor}\n    fetch_dict.update(data_dict)\n\n    with h5py.File(ground_truth_file) as f:\n\n        number_of_test_iterations = 1 # will be set to the correct value in the while loop\n        iteration = 0\n        while iteration < number_of_test_iterations:\n            \n            data =  session.run(fetch_dict)\n            \n            # get number of iterations from the info vector\n            number_of_test_iterations = int(data[\'INFO\'][0])\n\n            # write ground truth data to the file\n            group = f.require_group(str(iteration))\n            group[\'image_pair\'] = data[\'IMAGE_PAIR\'][0]\n            group[\'depth\'] = data[\'DEPTH\'][0]\n            group[\'motion\'] = data[\'MOTION\'][0]\n            group[\'flow\'] = data[\'FLOW\'][0]\n            group[\'intrinsics\'] = data[\'INTRINSICS\'][0]\n            \n            # save sample id as attribute of the group.\n            # the evaluation code will use this to check if prediction and ground truth match.\n            sample_id = (\'\'.join(map(chr, data[\'SAMPLE_IDS\']))).strip()\n            group.attrs[\'sample_id\'] = np.string_(sample_id)\n            iteration += 1\n            \n    del session\n    tf.reset_default_graph()\n    return ground_truth_file\n\n\n\ndef create_prediction_file(dataset, dataset_dir):\n    """"""Creates a hdf5 file with the predictions\n    \n    dataset: str\n        name of the dataset\n    dataset_dir: str\n        path to the directory containing the datasets\n\n    Returns the path to the created file\n    """"""\n  \n    if tf.test.is_gpu_available(True):\n        data_format=\'channels_first\'\n    else: # running on cpu requires channels_last data format\n        data_format=\'channels_last\'\n    print(\'Using data_format ""{0}""\'.format(data_format))\n\n    ds = dataset\n    # destination file\n    prediction_file = \'{0}_prediction.h5\'.format(ds)\n\n    # data types requested from the reader op\n    data_tensors_keys = (\'IMAGE_PAIR\', \'MOTION\', \'DEPTH\', \'INTRINSICS\')\n\n    reader_params = {\n             \'batch_size\': 1,\n             \'test_phase\': True,   # deactivates randomization\n             \'builder_threads\': 1, # must be 1 in test phase\n             \'inverse_depth\': True,\n             \'motion_format\': \'ANGLEAXIS6\',\n             \'norm_trans_scale_depth\': True,\n             # inpu resolution for demon\n             \'scaled_height\': 192,\n             \'scaled_width\': 256,\n             \'scene_pool_size\': 5, \n             # no augmentation\n             \'augment_rot180\': 0,\n             \'augment_mirror_x\': 0,\n             \'top_output\': data_tensors_keys,\n             \'source\': [{\'path\': os.path.join(dataset_dir,\'{0}_test.h5\'.format(ds))}],\n            }\n\n    reader_tensors = multi_vi_h5_data_reader(len(data_tensors_keys), json.dumps(reader_params))\n    \n    # create a dict to make the distinct data tensors accessible via keys\n    data_dict = dict(zip(data_tensors_keys,reader_tensors[2]))\n    info_tensor = reader_tensors[0]\n    sample_ids_tensor = reader_tensors[1]\n    image1, image2 = tf.split(data_dict[\'IMAGE_PAIR\'],2,axis=1)\n    \n    # downsample second image\n    image2_2 = sops.median3x3_downsample(sops.median3x3_downsample(image2))\n\n    gpu_options = tf.GPUOptions()\n    gpu_options.per_process_gpu_memory_fraction=0.8 # leave some memory to other processes\n    session = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))\n\n    \n    # init networks\n    bootstrap_net = BootstrapNet(session, data_format)\n    iterative_net = IterativeNet(session, data_format)\n    refine_net = RefinementNet(session, data_format)\n\n    session.run(tf.global_variables_initializer())\n\n    # load weights\n    saver = tf.train.Saver()\n    saver.restore(session,os.path.join(weights_dir,\'demon_original\'))\n\n    fetch_dict = {\n        \'INFO\': info_tensor,\n        \'SAMPLE_IDS\': sample_ids_tensor,\n        \'image1\': image1,\n        \'image2_2\': image2_2,\n    }\n    fetch_dict.update(data_dict)\n\n    if data_format == \'channels_last\':\n        for k in (\'image1\', \'image2_2\', \'IMAGE_PAIR\',):\n            fetch_dict[k] = convert_NCHW_to_NHWC(fetch_dict[k])\n    \n    with h5py.File(prediction_file, \'w\') as f:\n\n        number_of_test_iterations = 1 # will be set to the correct value in the while loop\n        test_iteration = 0\n        while test_iteration < number_of_test_iterations:\n            \n            data =  session.run(fetch_dict)\n            \n            # get number of iterations from the info vector\n            number_of_test_iterations = int(data[\'INFO\'][0])\n\n            # create group for the current test sample and save the sample id.\n            group = f.require_group(\'snapshot_1/{0}\'.format(test_iteration))\n            sample_id = (\'\'.join(map(chr, data[\'SAMPLE_IDS\']))).strip()\n            group.attrs[\'sample_id\'] = np.string_(sample_id)\n            \n            # save intrinsics\n            group[\'intrinsics\'] = data[\'INTRINSICS\']\n            \n            # run the network and save outputs for each network iteration \'i\'.\n            # iteration 0 corresponds to the bootstrap network.\n            # we also store the refined depth for each iteration.\n            for i in range(4):\n                if i == 0:\n                    result = bootstrap_net.eval(data[\'IMAGE_PAIR\'], data[\'image2_2\'])      \n                else:\n                    result = iterative_net.eval(\n                        data[\'IMAGE_PAIR\'], \n                        data[\'image2_2\'], \n                        result[\'predict_depth2\'], \n                        result[\'predict_normal2\'], \n                        result[\'predict_rotation\'], \n                        result[\'predict_translation\']\n                    )\n                # write predictions\n                if data_format == \'channels_last\':\n                    group[\'predicted_flow/{0}\'.format(i)] = result[\'predict_flow2\'][0].transpose([2,0,1])\n                    group[\'predicted_depth/{0}\'.format(i)] = result[\'predict_depth2\'][0,:,:,0]\n                else:\n                    group[\'predicted_flow/{0}\'.format(i)] = result[\'predict_flow2\'][0]\n                    group[\'predicted_depth/{0}\'.format(i)] = result[\'predict_depth2\'][0,0]\n                    \n                predict_motion = np.concatenate((result[\'predict_rotation\'],result[\'predict_translation\']),axis=1)\n                group[\'predicted_motion/{0}\'.format(i)] = predict_motion[0]\n                \n                # run refinement network\n                result_refined = refine_net.eval(data[\'image1\'],result[\'predict_depth2\'])\n                \n                # write refined depth prediction\n                if data_format == \'channels_last\':\n                    group[\'predicted_depth/{0}_refined\'.format(i)] = result_refined[\'predict_depth0\'][0,:,:,0]\n                else:\n                    group[\'predicted_depth/{0}_refined\'.format(i)] = result_refined[\'predict_depth0\'][0,0]\n                \n            test_iteration += 1\n            \n    del session\n    tf.reset_default_graph()\n    return prediction_file\n\ndef main():\n\n    # list the test datasets names for evaluation\n    datasets = (\'mvs\', \'scenes11\', \'rgbd\', \'sun3d\', \'nyu2\')\n    dataset_dir = os.path.join(\'..\', \'datasets\')\n\n\n\n    # creating the ground truth and prediction files requires about 11gb of disk space\n    for dataset in datasets:\n        gt_file = create_ground_truth_file(dataset, dataset_dir)\n        \n        print(\'creating predictions for\', dataset)\n        pr_file = create_prediction_file(dataset, dataset_dir)\n\n        # compute errors\n        # the evaluate function expects the path to a prediction and the corresponding\n        # ground truth file.\n        print(\'computing errors for\', dataset)\n\n        # compute errors for comparison with single image depth methods\n        eval_result = evaluate(pr_file, gt_file, depthmask=False, eigen_crop_gt_and_pred=True)\n        # save evaluation results to disk\n        write_xarray_json(eval_result, \'{0}_eval_crop_allpix.json\'.format(dataset))\n        \n        if dataset != \'nyu2\':\n            # depthmask=True will compute depth errors only for pixels visible in both images.\n            eval_result = evaluate(pr_file, gt_file, depthmask=True)\n            # save evaluation results to disk\n            write_xarray_json(eval_result, \'{0}_eval.json\'.format(dataset))\n            \n\n\n    # print errors\n    for dataset in datasets:\n        \n        # In the following eval_result is a 5D array with the following dimensions:\n        #  - snapshots: stores results of different network training states\n        #  - iteration: network iterations \'0\' stores the result of the bootstrap network.\n        #               \'3\' stores the results after bootstrap + 3 times iterative network.\n        #               \'3_refined\' stores the result after the refinement network.\n        #  - sample: the sample number.\n        #  - errors: stores the different error metrics.\n        #  - scaled: is a boolean dimension used for storing errors after optimal scaling \n        #            the prediction with a scalar factor. This was meant as an alternative\n        #            to scale invariant error measures. Just set this to False and ignore.\n        # \n        # The following prints the error metrics as used in the paper.\n\n        depth_errors = [\'depth_l1_inverse\',\'depth_scale_invariant\',\'depth_abs_relative\']\n        motion_errors = [\'rot_err\',\'tran_angle_err\']\n        print(\'======================================\')\n        print(\'dataset: \', dataset)\n        if dataset != \'nyu2\':\n            eval_result = read_xarray_json(\'{0}_eval.json\'.format(dataset))\n            print(\'  depth\', eval_result[0].loc[\'3_refined\',:,depth_errors,False].mean(\'sample\').to_pandas().to_string())\n            print(\'  motion\', eval_result[0].loc[\'3\',:,motion_errors,False].mean(\'sample\').to_pandas().to_string())\n        eval_result = read_xarray_json(\'{0}_eval_crop_allpix.json\'.format(dataset))\n        print(\'  depth cropped+all pixels\', eval_result[0].loc[\'3_refined\',:,[\'depth_scale_invariant\'],False].mean(\'sample\').to_pandas().to_string())\n        \n\nif __name__ == ""__main__"":\n    main()\n\n\n'"
examples/example.py,5,"b'import tensorflow as tf\nimport numpy as np\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport os\nimport sys\n\nexamples_dir = os.path.dirname(__file__)\nweights_dir = os.path.join(examples_dir,\'..\',\'weights\')\nsys.path.insert(0, os.path.join(examples_dir, \'..\', \'python\'))\n\nfrom depthmotionnet.networks_original import *\n\n\ndef prepare_input_data(img1, img2, data_format):\n    """"""Creates the arrays used as input from the two images.""""""\n    # scale images if necessary\n    if img1.size[0] != 256 or img1.size[1] != 192:\n        img1 = img1.resize((256,192))\n    if img2.size[0] != 256 or img2.size[1] != 192:\n        img2 = img2.resize((256,192))\n    img2_2 = img2.resize((64,48))\n        \n    # transform range from [0,255] to [-0.5,0.5]\n    img1_arr = np.array(img1).astype(np.float32)/255 -0.5\n    img2_arr = np.array(img2).astype(np.float32)/255 -0.5\n    img2_2_arr = np.array(img2_2).astype(np.float32)/255 -0.5\n    \n    if data_format == \'channels_first\':\n        img1_arr = img1_arr.transpose([2,0,1])\n        img2_arr = img2_arr.transpose([2,0,1])\n        img2_2_arr = img2_2_arr.transpose([2,0,1])\n        image_pair = np.concatenate((img1_arr,img2_arr), axis=0)\n    else:\n        image_pair = np.concatenate((img1_arr,img2_arr),axis=-1)\n    \n    result = {\n        \'image_pair\': image_pair[np.newaxis,:],\n        \'image1\': img1_arr[np.newaxis,:], # first image\n        \'image2_2\': img2_2_arr[np.newaxis,:], # second image with (w=64,h=48)\n    }\n    return result\n\n\nif tf.test.is_gpu_available(True):\n    data_format=\'channels_first\'\nelse: # running on cpu requires channels_last data format\n    data_format=\'channels_last\'\n\n# \n# DeMoN has been trained for specific internal camera parameters.\n#\n# If you use your own images try to adapt the intrinsics by cropping\n# to match the following normalized intrinsics:\n#\n#  K = (0.89115971  0           0.5)\n#      (0           1.18821287  0.5)\n#      (0           0           1  ),\n#  where K(1,1), K(2,2) are the focal lengths for x and y direction.\n#  and (K(1,3), K(2,3)) is the principal point.\n#  The parameters are normalized such that the image height and width is 1.\n#\n\n# read data\nimg1 = Image.open(os.path.join(examples_dir,\'sculpture1.png\'))\nimg2 = Image.open(os.path.join(examples_dir,\'sculpture2.png\'))\n\ninput_data = prepare_input_data(img1,img2,data_format)\n\ngpu_options = tf.GPUOptions()\ngpu_options.per_process_gpu_memory_fraction=0.8\nsession = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))\n\n# init networks\nbootstrap_net = BootstrapNet(session, data_format)\niterative_net = IterativeNet(session, data_format)\nrefine_net = RefinementNet(session, data_format)\n\nsession.run(tf.global_variables_initializer())\n\n# load weights\nsaver = tf.train.Saver()\nsaver.restore(session,os.path.join(weights_dir,\'demon_original\'))\n\n\n# run the network\nresult = bootstrap_net.eval(input_data[\'image_pair\'], input_data[\'image2_2\'])\nfor i in range(3):\n    result = iterative_net.eval(\n        input_data[\'image_pair\'], \n        input_data[\'image2_2\'], \n        result[\'predict_depth2\'], \n        result[\'predict_normal2\'], \n        result[\'predict_rotation\'], \n        result[\'predict_translation\']\n    )\nrotation = result[\'predict_rotation\']\ntranslation = result[\'predict_translation\']\nresult = refine_net.eval(input_data[\'image1\'],result[\'predict_depth2\'])\n\n\nplt.imshow(result[\'predict_depth0\'].squeeze(), cmap=\'Greys\')\nplt.show()\n\n# try to visualize the point cloud\ntry:\n    from depthmotionnet.vis import *\n    visualize_prediction(\n        inverse_depth=result[\'predict_depth0\'], \n        image=input_data[\'image_pair\'][0,0:3] if data_format==\'channels_first\' else input_data[\'image_pair\'].transpose([0,3,1,2])[0,0:3], \n        rotation=rotation, \n        translation=translation)\nexcept ImportError as err:\n    print(""Cannot visualize as pointcloud."", err)\n\n'"
examples/example_v2.py,5,"b'import tensorflow as tf\nimport numpy as np\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport os\nimport sys\nimport argparse\n\nexamples_dir = os.path.dirname(__file__)\nweights_dir = os.path.join(examples_dir,\'..\',\'weights\')\nsys.path.insert(0, os.path.join(examples_dir, \'..\', \'python\'))\n\nfrom depthmotionnet.v2.networks import *\n\nparser = argparse.ArgumentParser(description=""Runs the v2 network on the example image pair."")\nparser.add_argument(""--checkpoint"", type=str, required=True, help=""Path to the checkpoint without the file extension"")\nargs = parser.parse_args()\n\n\ndef prepare_input_data(img1, img2, data_format):\n    """"""Creates the arrays used as input from the two images.""""""\n    # scale images if necessary\n    if img1.size[0] != 256 or img1.size[1] != 192:\n        img1 = img1.resize((256,192))\n    if img2.size[0] != 256 or img2.size[1] != 192:\n        img2 = img2.resize((256,192))\n    img2_2 = img2.resize((64,48))\n        \n    # transform range from [0,255] to [-0.5,0.5]\n    img1_arr = np.array(img1).astype(np.float32)/255 -0.5\n    img2_arr = np.array(img2).astype(np.float32)/255 -0.5\n    img2_2_arr = np.array(img2_2).astype(np.float32)/255 -0.5\n    \n    if data_format == \'channels_first\':\n        img1_arr = img1_arr.transpose([2,0,1])\n        img2_arr = img2_arr.transpose([2,0,1])\n        img2_2_arr = img2_2_arr.transpose([2,0,1])\n        image_pair = np.concatenate((img1_arr,img2_arr), axis=0)\n    else:\n        image_pair = np.concatenate((img1_arr,img2_arr),axis=-1)\n    \n    result = {\n        \'image_pair\': image_pair[np.newaxis,:],\n        \'image1\': img1_arr[np.newaxis,:], # first image\n        \'image2_2\': img2_2_arr[np.newaxis,:], # second image with (w=64,h=48)\n    }\n    return result\n\n\nif tf.test.is_gpu_available(True):\n    data_format=\'channels_first\'\nelse: # running on cpu requires channels_last data format\n    print(\'Running this example requires a GPU\')\n    sys.exit(1)\n\n# \n# DeMoN has been trained for specific internal camera parameters.\n#\n# If you use your own images try to adapt the intrinsics by cropping\n# to match the following normalized intrinsics:\n#\n#  K = (0.89115971  0           0.5)\n#      (0           1.18821287  0.5)\n#      (0           0           1  ),\n#  where K(1,1), K(2,2) are the focal lengths for x and y direction.\n#  and (K(1,3), K(2,3)) is the principal point.\n#  The parameters are normalized such that the image height and width is 1.\n#\n\n# read data\nimg1 = Image.open(os.path.join(examples_dir,\'sculpture1.png\'))\nimg2 = Image.open(os.path.join(examples_dir,\'sculpture2.png\'))\n\ninput_data = prepare_input_data(img1,img2,data_format)\n\ngpu_options = tf.GPUOptions()\ngpu_options.per_process_gpu_memory_fraction=0.8\nsession = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))\n\n# init networks\nbootstrap_net = BootstrapNet(session)\niterative_net = IterativeNet(session)\nrefine_net = RefinementNet(session)\n\nsession.run(tf.global_variables_initializer())\n\n# load weights\nsaver = tf.train.Saver()\nsaver.restore(session, args.checkpoint)\n\n\n# run the network\nresult = bootstrap_net.eval(input_data[\'image_pair\'], input_data[\'image2_2\'])\nfor i in range(3):\n    result = iterative_net.eval(\n        input_data[\'image_pair\'], \n        input_data[\'image2_2\'], \n        result[\'predict_depth2\'], \n        result[\'predict_normal2\'], \n        result[\'predict_rotation\'], \n        result[\'predict_translation\']\n    )\nrotation = result[\'predict_rotation\']\ntranslation = result[\'predict_translation\']\nresult = refine_net.eval(input_data[\'image1\'],result[\'predict_depth2\'], result[\'predict_normal2\'])\n\n\nplt.imshow(result[\'predict_depth0\'].squeeze(), cmap=\'Greys\')\nplt.show()\n\n# try to visualize the point cloud\ntry:\n    from depthmotionnet.vis import *\n    visualize_prediction(\n        inverse_depth=result[\'predict_depth0\'], \n        image=input_data[\'image_pair\'][0,0:3] if data_format==\'channels_first\' else input_data[\'image_pair\'].transpose([0,3,1,2])[0,0:3], \n        rotation=rotation, \n        translation=translation)\nexcept ImportError as err:\n    print(""Cannot visualize as pointcloud."", err)\n\n'"
python/depthmotionnet/__init__.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n'"
python/depthmotionnet/blocks_original.py,35,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport tensorflow as tf\nfrom .helpers import *\nimport lmbspecialops as sops\n\n\ndef _predict_flow_caffe_padding(inp, predict_confidence=False, **kwargs ):\n    """"""Generates a tensor for optical flow prediction\n    \n    inp: Tensor\n\n    predict_confidence: bool\n        If True the output tensor has 4 channels instead of 2.\n        The last two channels are the x and y flow confidence.\n    """"""\n\n    tmp = convrelu_caffe_padding(\n        inputs=inp,\n        num_outputs=24,\n        kernel_size=3,\n        strides=1,\n        name=""conv1"",\n        **kwargs,\n    )\n    \n    output = conv2d_caffe_padding(\n        inputs=tmp,\n        num_outputs=4 if predict_confidence else 2,\n        kernel_size=3,\n        strides=1,\n        name=""conv2"",\n        **kwargs,\n    )\n    \n    return output\n\n\ndef _upsample_prediction(inp, num_outputs, **kwargs ):\n    """"""Upconvolution for upsampling predictions\n    \n    inp: Tensor \n        Tensor with the prediction\n        \n    num_outputs: int\n        Number of output channels. \n        Usually this should match the number of channels in the predictions\n    """"""\n    output = tf.layers.conv2d_transpose(\n        inputs=inp,\n        filters=num_outputs,\n        kernel_size=4,\n        strides=2,\n        padding=\'same\',\n        activation=None,\n        kernel_initializer=default_weights_initializer(),\n        name=""upconv"",\n        **kwargs,\n    )\n    return output\n\n\n\ndef _refine_caffe_padding(inp, num_outputs, data_format, upsampled_prediction=None, features_direct=None, **kwargs):\n    """""" Generates the concatenation of \n         - the previous features used to compute the flow/depth\n         - the upsampled previous flow/depth\n         - the direct features that already have the correct resolution\n\n    inp: Tensor\n        The features that have been used before to compute flow/depth\n\n    num_outputs: int \n        number of outputs for the upconvolution of \'features\'\n\n    upsampled_prediction: Tensor\n        The upsampled flow/depth prediction\n\n    features_direct: Tensor\n        The direct features which already have the spatial output resolution\n    """"""\n    tmp = tf.layers.conv2d_transpose(\n        inputs=inp,\n        filters=num_outputs,\n        kernel_size=4,\n        strides=2,\n        padding=\'VALID\',\n        activation=myLeakyRelu,\n        kernel_initializer=default_weights_initializer(),\n        data_format=data_format,\n        name=""upconv"",\n        **kwargs,\n    )\n    target_shape = features_direct.get_shape().as_list()\n    upsampled_features = tf.slice(tmp, [0,0,1,1] if data_format==\'channels_first\' else [0,1,1,0], target_shape)\n    inputs = [upsampled_features, features_direct, upsampled_prediction]\n    concat_inputs = [ x for x in inputs if not x is None ]\n    \n    if data_format == \'channels_first\':\n        return tf.concat(concat_inputs, axis=1)\n    else: # NHWC\n        return tf.concat(concat_inputs, axis=3)\n\n\n\ndef flow_block_demon_original(image_pair, image2_2=None, intrinsics=None, prev_predictions=None, data_format=\'channels_first\'):\n    """"""Creates a flow network\n    \n    image_pair: Tensor\n        Image pair concatenated along the channel axis.\n\n    image2_2: Tensor\n        Second image at resolution level 2 (downsampled two times)\n        \n    intrinsics: Tensor \n        The normalized intrinsic parameters\n\n    prev_predictions: dict of Tensor\n        Predictions from the previous depth block\n    \n    Returns a dict with the predictions\n    """"""\n    conv_params = {\'data_format\':data_format}\n\n    # contracting part\n    conv1 = convrelu2_caffe_padding(name=\'conv1\', inputs=image_pair, num_outputs=32, kernel_size=9, stride=2, **conv_params)\n\n    if prev_predictions is None:\n        conv2 = convrelu2_caffe_padding(name=\'conv2\', inputs=conv1, num_outputs=64, kernel_size=7, stride=2, **conv_params)\n        conv2_1 = convrelu2_caffe_padding(name=\'conv2_1\', inputs=conv2, num_outputs=64, kernel_size=3, stride=1, **conv_params)\n    else:\n        conv2 = convrelu2_caffe_padding(name=\'conv2\', inputs=conv1, num_outputs=32, kernel_size=7, stride=2, **conv_params)\n\n        # create warped input\n        if data_format==\'channels_first\':\n            prev_depth_nchw = prev_predictions[\'predict_depth2\']\n        else:\n            prev_depth_nchw = convert_NHWC_to_NCHW(prev_predictions[\'predict_depth2\'])\n\n        _flow_from_depth_motion = sops.depth_to_flow(\n            intrinsics = intrinsics,\n            depth = prev_depth_nchw,\n            rotation = prev_predictions[\'predict_rotation\'],\n            translation = prev_predictions[\'predict_translation\'],\n            inverse_depth = True,\n            normalize_flow = True,\n            )\n        # set flow vectors to zero if the motion is too large.\n        # this also eliminates nan values which can be produced by very bad camera parameters\n        flow_from_depth_motion_norm = tf.norm(_flow_from_depth_motion, axis=1, keep_dims=True)\n        flow_from_depth_motion_norm = tf.concat((flow_from_depth_motion_norm, flow_from_depth_motion_norm),axis=1)\n        tmp_zeros = tf.zeros_like(_flow_from_depth_motion,dtype=tf.float32)\n        flow_from_depth_motion =  tf.where( flow_from_depth_motion_norm < 1.0, _flow_from_depth_motion, tmp_zeros)\n\n\n        image2_2_warped = sops.warp2d(\n            input = image2_2 if data_format==\'channels_first\' else convert_NHWC_to_NCHW(image2_2),\n            displacements = flow_from_depth_motion,\n            normalized = True,\n            border_mode = \'value\',\n            )\n        if data_format==\'channels_last\':\n            flow_from_depth_motion = convert_NCHW_to_NHWC(flow_from_depth_motion)\n            image2_2_warped = convert_NCHW_to_NHWC(image2_2_warped)\n        extra_inputs = (image2_2_warped, flow_from_depth_motion, prev_predictions[\'predict_depth2\'], prev_predictions[\'predict_normal2\'])\n\n        # stop gradient here\n        extra_inputs_concat = tf.stop_gradient(tf.concat(extra_inputs, axis=1 if data_format==\'channels_first\' else 3))\n\n        conv_extra_inputs = convrelu2_caffe_padding(name=\'conv2_extra_inputs\', inputs=extra_inputs_concat, num_outputs=32, kernel_size=3, stride=1, **conv_params)\n        conv2_concat = tf.concat((conv2,conv_extra_inputs), axis=1 if data_format==\'channels_first\' else 3)\n        conv2_1 = convrelu2_caffe_padding(name=\'conv2_1\', inputs=conv2_concat, num_outputs=64, kernel_size=3, stride=1, **conv_params)\n    \n    \n    conv3 = convrelu2_caffe_padding(name=\'conv3\', inputs=conv2_1, num_outputs=128, kernel_size=5, stride=2, **conv_params)\n    conv3_1 = convrelu2_caffe_padding(name=\'conv3_1\', inputs=conv3, num_outputs=128, kernel_size=3, stride=1, **conv_params)\n    \n    conv4 = convrelu2_caffe_padding(name=\'conv4\', inputs=conv3_1, num_outputs=256, kernel_size=5, stride=2, **conv_params)\n    conv4_1 = convrelu2_caffe_padding(name=\'conv4_1\', inputs=conv4, num_outputs=256, kernel_size=3, stride=1, **conv_params)\n    \n    conv5 = convrelu2_caffe_padding(name=\'conv5\', inputs=conv4_1, num_outputs=512, kernel_size=5, stride=2, **conv_params)\n    conv5_1 = convrelu2_caffe_padding(name=\'conv5_1\', inputs=conv5, num_outputs=512, kernel_size=3, stride=1, **conv_params)\n    \n    \n    # expanding part\n    with tf.variable_scope(\'predict_flow5\'):\n        predict_flowconf5 = _predict_flow_caffe_padding(conv5_1, predict_confidence=True, **conv_params)\n    \n    with tf.variable_scope(\'upsample_flow5to4\'):\n        predict_flowconf5to4 = _upsample_prediction(predict_flowconf5, 2, **conv_params)\n   \n    with tf.variable_scope(\'refine4\'):\n        concat4 = _refine_caffe_padding(\n            inp=conv5_1, \n            num_outputs=256, \n            upsampled_prediction=predict_flowconf5to4, \n            features_direct=conv4_1,\n            **conv_params,\n        )\n\n    with tf.variable_scope(\'refine3\'):\n        concat3 = _refine_caffe_padding(\n            inp=concat4, \n            num_outputs=128, \n            features_direct=conv3_1,\n            **conv_params,\n        )\n\n    with tf.variable_scope(\'refine2\'):\n        concat2 = _refine_caffe_padding(\n            inp=concat3, \n            num_outputs=64, \n            features_direct=conv2_1,\n            **conv_params,\n        )\n\n    with tf.variable_scope(\'predict_flow2\'):\n        predict_flowconf2 = _predict_flow_caffe_padding(concat2, predict_confidence=True, **conv_params)\n \n    return { \'predict_flowconf5\': predict_flowconf5, \'predict_flowconf2\': predict_flowconf2 }\n\n\ndef _predict_depthnormal_caffe_padding(inp, predicted_scale=None, predict_normals=True, intermediate_num_outputs=24, data_format=\'channels_first\', **kwargs):\n    """"""Generates the ops for depth and normal prediction\n    \n    inp: Tensor\n\n    predicted_scale: Tensor\n        The predicted scale for scaling the depth values\n\n    predict_normals: bool\n        If True the output tensor has 4 channels instead of 1.\n        The last three channels are the normals.\n\n    intermediate_num_outputs: Tensor\n        Number of filters for the intermediate feature blob\n\n    Returns the depth prediction and the normal predictions separately\n    """"""\n\n    tmp = convrelu_caffe_padding(\n        inputs=inp,\n        num_outputs=intermediate_num_outputs,\n        kernel_size=3,\n        strides=1,\n        name=""conv1"",\n        data_format=data_format,\n        **kwargs,\n    )\n    \n    tmp2 = conv2d_caffe_padding(\n        inputs=tmp,\n        num_outputs=4 if predict_normals else 1,\n        kernel_size=3,\n        strides=1,\n        name=""conv2"",\n        data_format=data_format,\n        **kwargs,\n    )\n\n    if predict_normals:\n\n        predicted_unscaled_depth, predicted_normal = tf.split(value=tmp2, num_or_size_splits=[1,3], axis=1 if data_format==\'channels_first\' else 3) \n\n        if not predicted_scale is None:\n            batch_size = predicted_scale.get_shape().as_list()[0]\n            s = tf.reshape(predicted_scale, [batch_size,1,1,1])\n            predicted_depth = s*predicted_unscaled_depth\n        else:\n            predicted_depth = predicted_unscaled_depth\n\n        return predicted_depth, predicted_normal\n    else:\n        if not predicted_scale is None:\n            predicted_depth = predicted_scale*tmp2\n        else:\n            predicted_depth = tmp2\n\n        return predicted_depth\n\n\n\n\ndef depthmotion_block_demon_original(image_pair, image2_2, prev_flow2, prev_flowconf2, prev_rotation=None, prev_translation=None, intrinsics=None, data_format=\'channels_first\'):\n    """"""Creates a depth and motion network\n    \n    image_pair: Tensor\n        Image pair concatenated along the channel axis.\n        The tensor format is NCHW with C == 6.\n\n    image2_2: Tensor\n        Second image at resolution level 2\n\n    prev_flow2: Tensor\n        The output of the flow network. Contains only the flow (2 channels)\n\n    prev_flowconf2: Tensor\n        The output of the flow network. Contains flow and flow confidence (4 channels)\n\n    prev_rotation: Tensor\n        The previously predicted rotation.\n        \n    prev_translaion: Tensor\n        The previously predicted translation.\n\n    intrinsics: Tensor\n        Tensor with the intrinsic camera parameters\n        Only required if prev_rotation and prev_translation is not None.\n        \n    Returns a dictionary with the predictions for depth, normals and motion\n    """"""\n    conv_params = {\'data_format\':data_format}\n    fc_params = {}\n    \n    # contracting part\n    conv1 = convrelu2_caffe_padding(name=\'conv1\', inputs=image_pair, num_outputs=32, kernel_size=9, stride=2, **conv_params)\n    \n    conv2 = convrelu2_caffe_padding(name=\'conv2\', inputs=conv1, num_outputs=32, kernel_size=7, stride=2, **conv_params)\n    # create extra inputs\n    if data_format==\'channels_first\':\n        image2_2_warped = sops.warp2d(image2_2, prev_flow2, normalized=True, border_mode=\'value\')\n    else:\n        prev_flow2_nchw = convert_NHWC_to_NCHW(prev_flow2)\n        image2_2_warped = convert_NCHW_to_NHWC(sops.warp2d(convert_NHWC_to_NCHW(image2_2), prev_flow2_nchw, normalized=True, border_mode=\'value\'))\n        \n    extra_inputs = [image2_2_warped, prev_flowconf2]\n    if (not prev_rotation is None) and (not prev_translation is None) and (not intrinsics is None):\n        if data_format==\'channels_first\':\n            depth_from_flow = sops.flow_to_depth(\n                flow=prev_flow2, \n                intrinsics=intrinsics,\n                rotation=prev_rotation,\n                translation=prev_translation,\n                normalized_flow=True,\n                inverse_depth=True,\n                )\n        else:\n            depth_from_flow = convert_NCHW_to_NHWC(sops.flow_to_depth(\n                flow=prev_flow2_nchw, \n                intrinsics=intrinsics,\n                rotation=prev_rotation,\n                translation=prev_translation,\n                normalized_flow=True,\n                inverse_depth=True,\n                ))\n            \n        extra_inputs.append(depth_from_flow)\n\n    concat_extra_inputs = tf.stop_gradient(tf.concat(extra_inputs, axis=1 if data_format==\'channels_first\' else 3))\n    conv_extra_inputs = convrelu2_caffe_padding(name=\'conv2_extra_inputs\', inputs=concat_extra_inputs, num_outputs=32, kernel_size=3, stride=1, **conv_params)\n    conv2_concat = tf.concat((conv2,conv_extra_inputs),axis=1 if data_format==\'channels_first\' else 3)\n    conv2_1 = convrelu2_caffe_padding(name=\'conv2_1\', inputs=conv2_concat, num_outputs=64, kernel_size=3, stride=1, **conv_params)\n    \n    conv3 = convrelu2_caffe_padding(name=\'conv3\', inputs=conv2_1, num_outputs=128, kernel_size=5, stride=2, **conv_params)\n    conv3_1 = convrelu2_caffe_padding(name=\'conv3_1\', inputs=conv3, num_outputs=128, kernel_size=3, stride=1, **conv_params)\n    \n    conv4 = convrelu2_caffe_padding(name=\'conv4\', inputs=conv3_1, num_outputs=256, kernel_size=5, stride=2, **conv_params)\n    conv4_1 = convrelu2_caffe_padding(name=\'conv4_1\', inputs=conv4, num_outputs=256, kernel_size=3, stride=1, **conv_params)\n    \n    conv5 = convrelu2_caffe_padding(name=\'conv5\', inputs=conv4_1, num_outputs=512, kernel_size=3, stride=2, **conv_params)\n    conv5_1 = convrelu2_caffe_padding(name=\'conv5_1\', inputs=conv5, num_outputs=512, kernel_size=3, stride=1, **conv_params)\n    \n    \n    # motion prediction part\n    motion_conv1 = convrelu_caffe_padding(\n        name=\'motion_conv1\',\n        inputs=conv5_1,\n        num_outputs=128,\n        kernel_size=3,\n        strides=1,\n        **conv_params,\n    )\n    if data_format==\'channels_last\':\n        motion_conv1 = convert_NHWC_to_NCHW(motion_conv1)\n    motion_fc1 = tf.layers.dense(\n        name=\'motion_fc1\',\n        inputs=tf.contrib.layers.flatten(motion_conv1),\n        units=1024,\n        activation=myLeakyRelu,\n        **fc_params,\n    )\n    motion_fc2 = tf.layers.dense(\n        name=\'motion_fc2\',\n        inputs=motion_fc1,\n        units=128,\n        activation=myLeakyRelu,\n        **fc_params,\n    )\n    predict_motion_scale = tf.layers.dense(\n        name=\'motion_fc3\',\n        inputs=motion_fc2,\n        units=7,\n        activation=None,\n        **fc_params,\n    )\n\n    predict_rotation, predict_translation, predict_scale = tf.split(value=predict_motion_scale, num_or_size_splits=[3,3,1], axis=1)\n    \n    # expanding part\n    with tf.variable_scope(\'refine4\'):\n        concat4 = _refine_caffe_padding(\n            inp=conv5_1, \n            num_outputs=256, \n            features_direct=conv4_1,\n            **conv_params,\n        )\n\n    with tf.variable_scope(\'refine3\'):\n        concat3 = _refine_caffe_padding(\n            inp=concat4, \n            num_outputs=128, \n            features_direct=conv3_1,\n            **conv_params,\n        )\n\n    with tf.variable_scope(\'refine2\'):\n        concat2 = _refine_caffe_padding(\n            inp=concat3, \n            num_outputs=64, \n            features_direct=conv2_1,\n            **conv_params,\n        )\n\n    with tf.variable_scope(\'predict_depthnormal2\'):\n        predict_depth2, predict_normal2 = _predict_depthnormal_caffe_padding(concat2, predicted_scale=predict_scale, **conv_params)\n \n    return { \n        \'predict_depth2\': predict_depth2,  \n        \'predict_normal2\': predict_normal2,  \n        \'predict_rotation\': predict_rotation,  \n        \'predict_translation\': predict_translation,  \n        \'predict_scale\': predict_scale,\n        }\n    \n\n\ndef depth_refine_block_demon_original(image1, depthmotion_predictions, data_format=\'channels_first\' ):\n    """"""Creates a refinement network for the depth and normal predictions\n    \n    image1: Tensor\n        The reference image at full resolution.\n\n    depthmotion_predictions: dict of Tensors\n        The output of the depthmotion network.\n\n    Returns a dictionary with the predictions\n    """"""\n    conv_params = {\'data_format\': data_format}\n\n    # upsample the predicted depth and normals to the same size as the reference image \n    if data_format==\'channels_first\':\n        original_image_size = image1.get_shape().as_list()[-2:]\n    else:\n        original_image_size = image1.get_shape().as_list()[1:3]\n    depth2 = depthmotion_predictions[\'predict_depth2\']\n    if data_format==\'channels_first\':\n        depth2_nhwc = convert_NCHW_to_NHWC(depth2) \n    else:\n        depth2_nhwc = depth2\n    depth2_nhwc_orig_size = tf.image.resize_nearest_neighbor(depth2_nhwc, original_image_size)\n\n    if data_format==\'channels_first\':\n        depth2_orig_size = convert_NHWC_to_NCHW(depth2_nhwc_orig_size)\n    else:\n        depth2_orig_size = depth2_nhwc_orig_size\n\n    net_inputs = tf.concat((image1, depth2_orig_size), axis=1 if data_format==\'channels_first\' else 3)\n\n    # contracting part\n    conv0 = convrelu_caffe_padding(name=\'conv0\', inputs=net_inputs, num_outputs=32, kernel_size=3, strides=1, **conv_params)\n    \n    conv1 = convrelu_caffe_padding(name=\'conv1\', inputs=conv0, num_outputs=64, kernel_size=3, strides=2, **conv_params)\n    conv1_1 = convrelu_caffe_padding(name=\'conv1_1\', inputs=conv1, num_outputs=64, kernel_size=3, strides=1, **conv_params)\n    \n    conv2 = convrelu_caffe_padding(name=\'conv2\', inputs=conv1_1, num_outputs=128, kernel_size=3, strides=2, **conv_params)\n    conv2_1 = convrelu_caffe_padding(name=\'conv2_1\', inputs=conv2, num_outputs=128, kernel_size=3, strides=1, **conv_params)\n\n    # expanding part\n    with tf.variable_scope(\'refine1\'):\n        concat1 = _refine_caffe_padding(\n            inp=conv2_1, \n            num_outputs=64, \n            features_direct=conv1_1,\n            **conv_params,\n        )\n    \n    with tf.variable_scope(\'refine0\'):\n        concat0 = _refine_caffe_padding(\n            inp=concat1, \n            num_outputs=32, \n            features_direct=conv0,\n            **conv_params,\n        )\n    \n    with tf.variable_scope(\'predict_depth0\'):\n        predict_depth0 = _predict_depthnormal_caffe_padding(concat0, predict_normals=False, intermediate_num_outputs=16, **conv_params)\n\n    return { \'predict_depth0\': predict_depth0, }\n\n\n'"
python/depthmotionnet/helpers.py,11,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport tensorflow as tf\nimport lmbspecialops as sops\nimport numpy as np\n \ndef convert_NCHW_to_NHWC(inp):\n    """"""Convert the tensor from caffe format NCHW into tensorflow format NHWC\n        \n        inp: tensor \n    """"""\n    return tf.transpose(inp,[0,2,3,1])\n\ndef convert_NHWC_to_NCHW(inp):\n    """"""Convert the tensor from tensorflow format NHWC into caffe format NCHW \n        \n        inp: tensor \n    """"""\n    return tf.transpose(inp,[0,3,1,2])\n\n\ndef angleaxis_to_rotation_matrix(aa):\n    """"""Converts the 3 element angle axis representation to a 3x3 rotation matrix\n    \n    aa: numpy.ndarray with 1 dimension and 3 elements\n\n    Returns a 3x3 numpy.ndarray\n    """"""\n    angle = np.sqrt(aa.dot(aa))\n\n    if angle > 1e-6:\n        c = np.cos(angle);\n        s = np.sin(angle);\n        u = np.array([aa[0]/angle, aa[1]/angle, aa[2]/angle]);\n\n        R = np.empty((3,3))\n        R[0,0] = c+u[0]*u[0]*(1-c);      R[0,1] = u[0]*u[1]*(1-c)-u[2]*s; R[0,2] = u[0]*u[2]*(1-c)+u[1]*s;\n        R[1,0] = u[1]*u[0]*(1-c)+u[2]*s; R[1,1] = c+u[1]*u[1]*(1-c);      R[1,2] = u[1]*u[2]*(1-c)-u[0]*s;\n        R[2,0] = u[2]*u[0]*(1-c)-u[1]*s; R[2,1] = u[2]*u[1]*(1-c)+u[0]*s; R[2,2] = c+u[2]*u[2]*(1-c);\n    else:\n        R = np.eye(3)\n    return R\n\n\ndef myLeakyRelu(x):\n    """"""Leaky ReLU with leak factor 0.1""""""\n    # return tf.maximum(0.1*x,x)\n    return sops.leaky_relu(x, leak=0.1)\n\n\ndef default_weights_initializer():\n    return tf.contrib.layers.variance_scaling_initializer()\n\n\ndef conv2d_caffe_padding(inputs, num_outputs, kernel_size, data_format, **kwargs):\n    """"""Convolution with \'same\' padding as in caffe""""""\n    if isinstance(kernel_size,(tuple,list)):\n        kernel_ysize = kernel_size[0]\n        kernel_xsize = kernel_size[1]\n    else:\n        kernel_ysize = kernel_size\n        kernel_xsize = kernel_size\n    pad_y = kernel_ysize//2\n    pad_x = kernel_xsize//2\n\n    if data_format==\'channels_first\':\n        paddings = [[0,0], [0,0], [pad_y, pad_y], [pad_x,pad_x]]\n    else:\n        paddings = [[0,0], [pad_y, pad_y], [pad_x,pad_x], [0,0]]\n    padded_input = tf.pad(inputs, paddings=paddings)\n    return tf.layers.conv2d(\n        inputs=padded_input,\n        filters=num_outputs,\n        kernel_size=kernel_size,\n        kernel_initializer=default_weights_initializer(),\n        padding=\'valid\',\n        data_format=data_format,\n        **kwargs,\n        )\n\n\ndef convrelu_caffe_padding(inputs, num_outputs, kernel_size, data_format, **kwargs):\n    """"""Shortcut for a single convolution+relu \n    \n    See tf.layers.conv2d for a description of remaining parameters\n    """"""\n    return conv2d_caffe_padding(inputs, num_outputs, kernel_size, data_format, activation=myLeakyRelu, **kwargs)\n\n\ndef convrelu2_caffe_padding(inputs, num_outputs, kernel_size, name, stride, data_format, **kwargs):\n    """"""Shortcut for two convolution+relu with 1D filter kernels and \'same\' padding as in caffe\n    \n    num_outputs: int or (int,int)\n        If num_outputs is a tuple then the first element is the number of\n        outputs for the 1d filter in y direction and the second element is\n        the final number of outputs.\n    """"""\n    if isinstance(num_outputs,(tuple,list)):\n        num_outputs_y = num_outputs[0]\n        num_outputs_x = num_outputs[1]\n    else:\n        num_outputs_y = num_outputs\n        num_outputs_x = num_outputs\n\n    pad = kernel_size//2\n\n    if data_format==\'channels_first\':\n        paddings_y = [[0,0], [0,0], [pad, pad], [0,0]]\n        paddings_x = [[0,0], [0,0], [0,0], [pad, pad]]\n    else:\n        paddings_y = [[0,0], [pad, pad], [0,0], [0,0]]\n        paddings_x = [[0,0], [0,0], [pad, pad], [0,0]]\n    padded_input = tf.pad(inputs, paddings=paddings_y)\n\n    tmp_y = tf.layers.conv2d(\n        inputs=padded_input,\n        filters=num_outputs_y,\n        kernel_size=[kernel_size,1],\n        strides=[stride,1],\n        padding=\'valid\',\n        activation=myLeakyRelu,\n        kernel_initializer=default_weights_initializer(),\n        data_format=data_format,\n        name=name+\'y\',\n        **kwargs,\n    )\n    return tf.layers.conv2d(\n        inputs=tf.pad(tmp_y, paddings=paddings_x),\n        filters=num_outputs_x,\n        kernel_size=[1,kernel_size],\n        strides=[1,stride],\n        padding=\'valid\',\n        activation=myLeakyRelu,\n        kernel_initializer=default_weights_initializer(),\n        data_format=data_format,\n        name=name+\'x\',\n        **kwargs,\n    )\n\n'"
python/depthmotionnet/networks_original.py,31,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom .blocks_original import *\n\n\n\nclass BootstrapNet:\n    def __init__(self, session, data_format=\'channels_first\', batch_size=1):\n        """"""Creates the network\n\n        session: tf.Session\n            Tensorflow session\n\n        data_format: str\n            Either \'channels_first\' or \'channels_last\'.\n            Running on the cpu requires \'channels_last\'.\n\n        batch_size: int\n            The batch size\n        """"""\n        self.session = session\n        if data_format==\'channels_first\':\n            self.placeholder_image_pair = tf.placeholder(dtype=tf.float32, shape=(batch_size,6,192,256))\n            self.placeholder_image2_2 = tf.placeholder(dtype=tf.float32, shape=(batch_size,3,48,64))\n        else:\n            self.placeholder_image_pair = tf.placeholder(dtype=tf.float32, shape=(batch_size,192,256,6))\n            self.placeholder_image2_2 = tf.placeholder(dtype=tf.float32, shape=(batch_size,48,64,3))\n\n        with tf.variable_scope(\'netFlow1\'):\n            netFlow1_result = flow_block_demon_original(self.placeholder_image_pair, data_format=data_format )\n            self.netFlow1_result = netFlow1_result\n            self.predict_flow5, self.predict_conf5 = tf.split(value=netFlow1_result[\'predict_flowconf5\'], num_or_size_splits=2, axis=1 if data_format==\'channels_first\' else 3)\n            self.predict_flow2, self.predict_conf2 = tf.split(value=netFlow1_result[\'predict_flowconf2\'], num_or_size_splits=2, axis=1 if data_format==\'channels_first\' else 3)\n\n        with tf.variable_scope(\'netDM1\'):\n            self.netDM1_result = depthmotion_block_demon_original(\n                    image_pair=self.placeholder_image_pair, \n                    image2_2=self.placeholder_image2_2, \n                    prev_flow2=self.predict_flow2, \n                    prev_flowconf2=self.netFlow1_result[\'predict_flowconf2\'], \n                    data_format=data_format\n                    )\n\n\n    def eval(self, image_pair, image2_2):\n        """"""Runs the bootstrap network\n        \n        image_pair: numpy.ndarray\n            Array with shape [N,6,192,256] if data_format==\'channels_first\'\n            \n            Image pair in the range [-0.5, 0.5]\n\n        image2_2: numpy.ndarray\n            Second image at resolution level 2 (downsampled two times)\n\n            The shape for data_format==\'channels_first\' is [1,3,48,64]\n\n        Returns a dict with the preditions of the bootstrap net\n        """"""\n        \n        fetches = {\n                \'predict_flow5\': self.predict_flow5,\n                \'predict_flow2\': self.predict_flow2,\n                \'predict_depth2\': self.netDM1_result[\'predict_depth2\'],\n                \'predict_normal2\': self.netDM1_result[\'predict_normal2\'],\n                \'predict_rotation\': self.netDM1_result[\'predict_rotation\'],\n                \'predict_translation\': self.netDM1_result[\'predict_translation\'],\n                }\n        feed_dict = {\n                self.placeholder_image_pair: image_pair,\n                self.placeholder_image2_2: image2_2,\n                }\n        return self.session.run(fetches, feed_dict=feed_dict)\n\n\n\nclass IterativeNet:\n    def __init__(self, session, data_format=\'channels_first\', batch_size=1):\n        """"""Creates the network\n\n        session: tf.Session\n            Tensorflow session\n\n        data_format: str\n            Either \'channels_first\' or \'channels_last\'.\n            Running on the cpu requires \'channels_last\'.\n\n        batch_size: int\n            The batch size\n        """"""\n        self.session = session\n\n        intrinsics = np.broadcast_to(np.array([[0.89115971, 1.18821287, 0.5, 0.5]]),(batch_size,4))\n        self.intrinsics = tf.constant(intrinsics, dtype=tf.float32)\n\n        if data_format == \'channels_first\':\n            self.placeholder_image_pair = tf.placeholder(dtype=tf.float32, shape=(batch_size,6,192,256))\n            self.placeholder_image2_2 = tf.placeholder(dtype=tf.float32, shape=(batch_size,3,48,64))\n            self.placeholder_depth2 = tf.placeholder(dtype=tf.float32, shape=(batch_size,1,48,64))\n            self.placeholder_normal2 = tf.placeholder(dtype=tf.float32, shape=(batch_size,3,48,64))\n        else:\n            self.placeholder_image_pair = tf.placeholder(dtype=tf.float32, shape=(batch_size,192,256,6))\n            self.placeholder_image2_2 = tf.placeholder(dtype=tf.float32, shape=(batch_size,48,64,3))\n            self.placeholder_depth2 = tf.placeholder(dtype=tf.float32, shape=(batch_size,48,64,1))\n            self.placeholder_normal2 = tf.placeholder(dtype=tf.float32, shape=(batch_size,48,64,3))\n\n        self.placeholder_rotation = tf.placeholder(dtype=tf.float32, shape=(batch_size,3))\n        self.placeholder_translation = tf.placeholder(dtype=tf.float32, shape=(batch_size,3))\n\n        with tf.variable_scope(\'netFlow2\'):\n            netFlow2_result = flow_block_demon_original(\n                    image_pair=self.placeholder_image_pair,\n                    image2_2=self.placeholder_image2_2,\n                    intrinsics=self.intrinsics,\n                    prev_predictions={\n                        \'predict_depth2\': self.placeholder_depth2,\n                        \'predict_normal2\': self.placeholder_normal2,\n                        \'predict_rotation\': self.placeholder_rotation,\n                        \'predict_translation\': self.placeholder_translation,\n                        },\n                    data_format=data_format,\n                )\n            self.netFlow2_result = netFlow2_result\n            self.predict_flow5, self.predict_conf5 = tf.split(value=netFlow2_result[\'predict_flowconf5\'], num_or_size_splits=2, axis=1 if data_format==\'channels_first\' else 3)\n            self.predict_flow2, self.predict_conf2 = tf.split(value=netFlow2_result[\'predict_flowconf2\'], num_or_size_splits=2, axis=1 if data_format==\'channels_first\' else 3)\n\n        with tf.variable_scope(\'netDM2\'):\n            self.netDM2_result = depthmotion_block_demon_original(\n                    image_pair=self.placeholder_image_pair,\n                    image2_2=self.placeholder_image2_2, \n                    prev_flow2=self.predict_flow2, \n                    prev_flowconf2=self.netFlow2_result[\'predict_flowconf2\'],\n                    prev_rotation=self.placeholder_rotation,\n                    prev_translation=self.placeholder_translation,\n                    intrinsics=self.intrinsics,\n                    data_format=data_format,\n                    )\n\n    def eval(self, image_pair, image2_2, depth2, normal2, rotation, translation ):\n        """"""Runs the iterative network\n        \n        image_pair: numpy.ndarray\n            Array with shape [N,6,192,256] if data_format==\'channels_first\'\n            \n            Image pair in the range [-0.5, 0.5]\n\n        image2_2: numpy.ndarray\n            Second image at resolution level 2 (downsampled two times)\n\n            The shape for data_format==\'channels_first\' is [1,3,48,64]\n\n        depth2: numpy.ndarray\n            Depth prediction at resolution level 2\n\n        normal2: numpy.ndarray\n            Normal prediction at resolution level 2\n\n        rotation: numpy.ndarray\n            Rotation prediction in 3 element angle axis format\n\n        translation: numpy.ndarray\n            Translation prediction\n\n        Returns a dict with the preditions of the iterative net\n        """"""\n\n        fetches = {\n                \'predict_flow5\': self.predict_flow5,\n                \'predict_flow2\': self.predict_flow2,\n                \'predict_depth2\': self.netDM2_result[\'predict_depth2\'],\n                \'predict_normal2\': self.netDM2_result[\'predict_normal2\'],\n                \'predict_rotation\': self.netDM2_result[\'predict_rotation\'],\n                \'predict_translation\': self.netDM2_result[\'predict_translation\'],\n                }\n        feed_dict = {\n                self.placeholder_image_pair: image_pair,\n                self.placeholder_image2_2: image2_2,\n                self.placeholder_depth2: depth2,\n                self.placeholder_normal2: normal2,\n                self.placeholder_rotation: rotation,\n                self.placeholder_translation: translation,\n                }\n        return self.session.run(fetches, feed_dict=feed_dict)\n\n\n\nclass RefinementNet:\n\n    def __init__(self, session, data_format=\'channels_first\', batch_size=1):\n        """"""Creates the network\n\n        session: tf.Session\n            Tensorflow session\n\n        data_format: str\n            Either \'channels_first\' or \'channels_last\'.\n            Running on the cpu requires \'channels_last\'.\n\n        batch_size: int\n            The batch size\n        """"""\n        self.session = session\n\n        if data_format == \'channels_first\':\n            self.placeholder_image1 = tf.placeholder(dtype=tf.float32, shape=(batch_size,3,192,256))\n            self.placeholder_depth2 = tf.placeholder(dtype=tf.float32, shape=(batch_size,1,48,64))\n        else:\n            self.placeholder_image1 = tf.placeholder(dtype=tf.float32, shape=(batch_size,192,256,3))\n            self.placeholder_depth2 = tf.placeholder(dtype=tf.float32, shape=(batch_size,48,64,1))\n\n\n        with tf.variable_scope(\'netRefine\'):\n            self.netRefine_result = depth_refine_block_demon_original(\n                    image1=self.placeholder_image1, \n                    depthmotion_predictions={\n                        \'predict_depth2\': self.placeholder_depth2,\n                        },\n                    data_format=data_format,\n                    )\n\n    def eval(self, image1, depth2):\n        """"""Runs the refinement network\n        \n        image1: numpy.ndarray\n            Array with the first image with shape [N,3,192,256] if data_format==\'channels_first\'\n\n        depth2: numpy.ndarray\n            Depth prediction at resolution level 2\n\n        Returns a dict with the preditions of the refinement net\n        """"""\n\n        fetches = {\n                \'predict_depth0\': self.netRefine_result[\'predict_depth0\'],\n                }\n        feed_dict = {\n                self.placeholder_image1: image1,\n                self.placeholder_depth2: depth2,\n                }\n        return self.session.run(fetches, feed_dict=feed_dict)\n\n'"
python/depthmotionnet/vis.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport pyximport; pyximport.install()\nimport numpy as np\nfrom .helpers import angleaxis_to_rotation_matrix\n\n\ndef compute_point_cloud_from_depthmap( depth, K, R, t, normals=None, colors=None ):\n    """"""Creates a point cloud numpy array and optional normals and colors arrays\n\n    depth: numpy.ndarray \n        2d array with depth values\n\n    K: numpy.ndarray\n        3x3 matrix with internal camera parameters\n\n    R: numpy.ndarray\n        3x3 rotation matrix\n\n    t: numpy.ndarray\n        3d translation vector\n\n    normals: numpy.ndarray\n        optional array with normal vectors\n\n    colors: numpy.ndarray\n        optional RGB image with the same dimensions as the depth map.\n        The shape is (3,h,w) with type uint8\n\n    """"""\n    from .vis_cython import compute_point_cloud_from_depthmap as _compute_point_cloud_from_depthmap\n    return _compute_point_cloud_from_depthmap(depth, K, R, t, normals, colors)\n\n\ndef create_camera_polydata(R, t, only_polys=False):\n    """"""Creates a vtkPolyData object with a camera mesh""""""\n    import vtk\n    cam_points = np.array([ \n        [0, 0, 0],\n        [-1,-1, 1.5],\n        [ 1,-1, 1.5],\n        [ 1, 1, 1.5],\n        [-1, 1, 1.5],\n        [-0.5, 1, 1.5],\n        [ 0.5, 1, 1.5],\n        [ 0,1.2,1.5],\n        [ 1,-0.5,1.5],\n        [ 1, 0.5,1.5],\n        [ 1.2, 0, 1.5]]\n    ) \n    cam_points = (0.25*cam_points - t).dot(R)\n\n    vpoints = vtk.vtkPoints()\n    vpoints.SetNumberOfPoints(cam_points.shape[0])\n    for i in range(cam_points.shape[0]):\n        vpoints.SetPoint(i, cam_points[i])\n    vpoly = vtk.vtkPolyData()\n    vpoly.SetPoints(vpoints)\n    \n    poly_cells = vtk.vtkCellArray()\n\n    if not only_polys:\n        line_cells = vtk.vtkCellArray()\n        \n        line_cells.InsertNextCell( 5 );\n        line_cells.InsertCellPoint( 1 );\n        line_cells.InsertCellPoint( 2 );\n        line_cells.InsertCellPoint( 3 );\n        line_cells.InsertCellPoint( 4 );\n        line_cells.InsertCellPoint( 1 );\n\n        line_cells.InsertNextCell( 3 );\n        line_cells.InsertCellPoint( 1 );\n        line_cells.InsertCellPoint( 0 );\n        line_cells.InsertCellPoint( 2 );\n\n        line_cells.InsertNextCell( 3 );\n        line_cells.InsertCellPoint( 3 );\n        line_cells.InsertCellPoint( 0 );\n        line_cells.InsertCellPoint( 4 );\n\n        # x-axis indicator\n        line_cells.InsertNextCell( 3 );\n        line_cells.InsertCellPoint( 8 );\n        line_cells.InsertCellPoint( 10 );\n        line_cells.InsertCellPoint( 9 );\n        vpoly.SetLines(line_cells)\n    else:\n        # left\n        poly_cells.InsertNextCell( 3 );\n        poly_cells.InsertCellPoint( 0 );\n        poly_cells.InsertCellPoint( 1 );\n        poly_cells.InsertCellPoint( 4 );\n\n        # right\n        poly_cells.InsertNextCell( 3 );\n        poly_cells.InsertCellPoint( 0 );\n        poly_cells.InsertCellPoint( 3 );\n        poly_cells.InsertCellPoint( 2 );\n\n        # top\n        poly_cells.InsertNextCell( 3 );\n        poly_cells.InsertCellPoint( 0 );\n        poly_cells.InsertCellPoint( 4 );\n        poly_cells.InsertCellPoint( 3 );\n\n        # bottom\n        poly_cells.InsertNextCell( 3 );\n        poly_cells.InsertCellPoint( 0 );\n        poly_cells.InsertCellPoint( 2 );\n        poly_cells.InsertCellPoint( 1 );\n\n        # x-axis indicator\n        poly_cells.InsertNextCell( 3 );\n        poly_cells.InsertCellPoint( 8 );\n        poly_cells.InsertCellPoint( 10 );\n        poly_cells.InsertCellPoint( 9 );\n\n    # up vector (y-axis)\n    poly_cells.InsertNextCell( 3 );\n    poly_cells.InsertCellPoint( 5 );\n    poly_cells.InsertCellPoint( 6 );\n    poly_cells.InsertCellPoint( 7 );\n\n    vpoly.SetPolys(poly_cells)\n\n    return vpoly\n\n\ndef create_camera_actor(R, t):\n    """"""Creates a vtkActor object with a camera mesh""""""\n    import vtk\n    vpoly = create_camera_polydata(R, t)\n    mapper = vtk.vtkPolyDataMapper()\n    mapper.SetInputData(vpoly)\n\n    actor = vtk.vtkActor()\n    actor.SetMapper(mapper)\n    actor.GetProperty().LightingOff()\n    actor.GetProperty().SetLineWidth(2)\n\n    return actor\n\n\ndef create_pointcloud_polydata(points, colors=None):\n    """"""Creates a vtkPolyData object with the point cloud from numpy arrays\n    \n    points: numpy.ndarray\n        pointcloud with shape (n,3)\n    \n    colors: numpy.ndarray\n        uint8 array with colors for each point. shape is (n,3)\n\n    Returns vtkPolyData object\n    """"""\n    import vtk\n    vpoints = vtk.vtkPoints()\n    vpoints.SetNumberOfPoints(points.shape[0])\n    for i in range(points.shape[0]):\n        vpoints.SetPoint(i, points[i])\n    vpoly = vtk.vtkPolyData()\n    vpoly.SetPoints(vpoints)\n    \n    if not colors is None:\n        vcolors = vtk.vtkUnsignedCharArray()\n        vcolors.SetNumberOfComponents(3)\n        vcolors.SetName(""Colors"")\n        vcolors.SetNumberOfTuples(points.shape[0])\n        for i in range(points.shape[0]):\n            vcolors.SetTuple3(i ,colors[i,0],colors[i,1], colors[i,2])\n        vpoly.GetPointData().SetScalars(vcolors)\n\n    vcells = vtk.vtkCellArray()\n    \n    for i in range(points.shape[0]):\n        vcells.InsertNextCell(1)\n        vcells.InsertCellPoint(i)\n        \n    vpoly.SetVerts(vcells)\n    \n    return vpoly\n\n\n\ndef create_pointcloud_actor(points, colors=None):\n    """"""Creates a vtkActor with the point cloud from numpy arrays\n    \n    points: numpy.ndarray\n        pointcloud with shape (n,3)\n    \n    colors: numpy.ndarray\n        uint8 array with colors for each point. shape is (n,3)\n\n    Returns vtkActor object\n    """"""\n    import vtk\n    vpoly = create_pointcloud_polydata(points, colors)\n    mapper = vtk.vtkPolyDataMapper()\n    mapper.SetInputData(vpoly)\n\n    actor = vtk.vtkActor()\n    actor.SetMapper(mapper)\n    actor.GetProperty().SetPointSize(3)\n\n    return actor\n\n\ndef visualize_prediction( inverse_depth, intrinsics=None, normals=None, rotation=None, translation=None, image=None ):\n    """"""Visualizes the network predictions\n\n    inverse_depth: numpy.ndarray\n        2d array with the inverse depth values with shape (h,w)\n\n    intrinsics: numpy.ndarray\n        4 element vector with the normalized intrinsic parameters with shape\n        (4,)\n\n    normals: numpy.ndarray\n        normal map with shape (3,h,w)\n\n    rotation: numpy.ndarray\n        rotation in axis angle format with 3 elements with shape (3,)\n\n    translation: numpy.ndarray\n        translation vector with shape (3,)\n\n    image: numpy.ndarray\n        Image with shape (3,h,w) in the range [-0.5,0.5].\n    """"""\n    import vtk\n    depth = (1/inverse_depth).squeeze()\n\n    w = depth.shape[-1]\n    h = depth.shape[-2]\n\n    if intrinsics is None:\n        intrinsics = np.array([0.89115971, 1.18821287, 0.5, 0.5]) # sun3d intrinsics\n\n    K = np.eye(3)\n    K[0,0] = intrinsics[0]*w\n    K[1,1] = intrinsics[1]*h\n    K[0,2] = intrinsics[2]*w\n    K[1,2] = intrinsics[3]*h\n\n    R1 = np.eye(3)\n    t1 = np.zeros((3,))\n\n    if not rotation is None and not translation is None:\n        R2 = angleaxis_to_rotation_matrix(rotation.squeeze())\n        t2 = translation.squeeze()\n    else:\n        R2 = np.eye(3)\n        t2 = np.zeros((3,))\n\n    if not normals is None:\n        n = normals.squeeze()\n    else:\n        n = None\n\n    if not image is None:\n        img = ((image+0.5)*255).astype(np.uint8)\n    else:\n        img = None\n\n    pointcloud = compute_point_cloud_from_depthmap(depth, K, R1, t1, n, img)\n\n    renderer = vtk.vtkRenderer()\n    renderer.SetBackground(0, 0, 0)\n\n    pointcloud_actor = create_pointcloud_actor(\n        points=pointcloud[\'points\'], \n        colors=pointcloud[\'colors\'] if \'colors\' in pointcloud else None,\n        )\n    renderer.AddActor(pointcloud_actor)\n\n    cam1_actor = create_camera_actor(R1,t1)\n    renderer.AddActor(cam1_actor)\n    \n    cam2_actor = create_camera_actor(R2,t2)\n    renderer.AddActor(cam2_actor)\n    \n    axes = vtk.vtkAxesActor()\n    axes.GetXAxisCaptionActor2D().SetHeight(0.05)\n    axes.GetYAxisCaptionActor2D().SetHeight(0.05)\n    axes.GetZAxisCaptionActor2D().SetHeight(0.05)\n    axes.SetCylinderRadius(0.03)\n    axes.SetShaftTypeToCylinder()\n    renderer.AddActor(axes)\n\n    renwin = vtk.vtkRenderWindow()\n    renwin.SetWindowName(""Point Cloud Viewer"")\n    renwin.SetSize(800,600)\n    renwin.AddRenderer(renderer)\n    \n \n    # An interactor\n    interactor = vtk.vtkRenderWindowInteractor()\n    interstyle = vtk.vtkInteractorStyleTrackballCamera()\n    interactor.SetInteractorStyle(interstyle)\n    interactor.SetRenderWindow(renwin)\n \n    # Start\n    interactor.Initialize()\n    interactor.Start()\n    \n\ndef export_prediction_to_ply( output_prefix, inverse_depth, intrinsics=None, normals=None, rotation=None, translation=None, image=None ):\n    """"""Exports the network predictions to ply files meant for external visualization\n\n    inverse_depth: numpy.ndarray\n        2d array with the inverse depth values with shape (h,w)\n\n    intrinsics: numpy.ndarray\n        4 element vector with the normalized intrinsic parameters with shape\n        (4,)\n\n    normals: numpy.ndarray\n        normal map with shape (3,h,w)\n\n    rotation: numpy.ndarray\n        rotation in axis angle format with 3 elements with shape (3,)\n\n    translation: numpy.ndarray\n        translation vector with shape (3,)\n\n    image: numpy.ndarray\n        Image with shape (3,h,w) in the range [-0.5,0.5].\n    """"""\n    import vtk\n    depth = (1/inverse_depth).squeeze()\n\n    w = depth.shape[-1]\n    h = depth.shape[-2]\n\n    if intrinsics is None:\n        intrinsics = np.array([0.89115971, 1.18821287, 0.5, 0.5]) # sun3d intrinsics\n\n    K = np.eye(3)\n    K[0,0] = intrinsics[0]*w\n    K[1,1] = intrinsics[1]*h\n    K[0,2] = intrinsics[2]*w\n    K[1,2] = intrinsics[3]*h\n\n    R1 = np.eye(3)\n    t1 = np.zeros((3,))\n\n    if not rotation is None and not translation is None:\n        R2 = angleaxis_to_rotation_matrix(rotation.squeeze())\n        t2 = translation.squeeze()\n    else:\n        R2 = np.eye(3)\n        t2 = np.zeros((3,))\n\n    if not normals is None:\n        n = normals.squeeze()\n    else:\n        n = None\n\n    if not image is None:\n        img = ((image+0.5)*255).astype(np.uint8)\n    else:\n        img = None\n\n    pointcloud = compute_point_cloud_from_depthmap(depth, K, R1, t1, n, img)\n\n    pointcloud_polydata = create_pointcloud_polydata( \n        points=pointcloud[\'points\'], \n        colors=pointcloud[\'colors\'] if \'colors\' in pointcloud else None,\n        )\n    plywriter = vtk.vtkPLYWriter()\n    plywriter.SetFileName(output_prefix + \'points.ply\')\n    plywriter.SetInputData(pointcloud_polydata)\n    plywriter.SetArrayName(\'Colors\')\n    plywriter.Write()\n    \n    cam1_polydata = create_camera_polydata(R1,t1, True)\n    plywriter = vtk.vtkPLYWriter()\n    plywriter.SetFileName(output_prefix + \'cam1.ply\')\n    plywriter.SetInputData(cam1_polydata)\n    plywriter.Write()\n    \n    cam2_polydata = create_camera_polydata(R2,t2, True)\n    plywriter = vtk.vtkPLYWriter()\n    plywriter.SetFileName(output_prefix + \'cam2.ply\')\n    plywriter.SetInputData(cam2_polydata)\n    plywriter.Write()\n\n\n\ndef transform_pointcloud_points(points, T):\n    """"""Transforms the pointcloud with T\n    \n    points: numpy.ndarray\n        pointcloud with shape (n,3)\n\n    T: numpy.ndarray\n        The 4x4 transformation\n\n    Returns the transformed points\n    """"""\n    tmp = np.empty((points.shape[0],points.shape[1]+1),dtype=points.dtype)\n    tmp[:,0:3] = points\n    tmp[:,3] = 1\n    return T.dot(tmp.transpose())[0:3].transpose()\n\n'"
training/v2/training.py,69,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport tensorflow as tf\nimport numpy as np\nfrom tfutils import *\nimport os\nimport sys\nimport glob\nimport json\n\nimport depthmotionnet.datareader as datareader\nfrom depthmotionnet.v2.blocks import *\nfrom depthmotionnet.v2.losses import *\n\n#\n# global parameters\n#\n_num_gpus = get_gpu_count()\n_num_gpus=1 # multi gpu training is not well tested\n\n# the train dir stores all checkpoints and summaries. The dir name is the name of this file without .py\n_train_dir = os.path.splitext(os.path.basename(__file__))[0]\n\n# set the path to the training h5 files here\n_data_dir = \'../../datasets/traindata\'\n\n\n# The training procedure has several stages, which we call evolutions.\n# The training parameters and the network graph change with each evolution.\n_evolutions = (\'0_flow1\', \'1_dm1\', \'2_flow2\', \'3_dm2\', \'4_iterative\', \'5_refine\')\n_k = 1000\n_max_iter_dict = {\n        \'0_flow1\': 1000*_k,\n        \'1_dm1\': 1000*_k,\n        \'2_flow2\': 250*_k,\n        \'3_dm2\': 250*_k,\n        \'4_iterative\': 1500*_k,\n        \'5_refine\': 250*_k,\n        }\n\n_base_lr_dict = {\n        \'0_flow1\': 0.00025,\n        \'1_dm1\': 0.0002,\n        \'2_flow2\': 0.00015,\n        \'3_dm2\': 0.00015,\n        \'4_iterative\': 0.00015,\n        \'5_refine\': 0.0002,\n        }\n\n_simulated_iterations = 4\n_flow_loss_weight = 0.5*1000\n_flow_grad_loss_weight = 0.25*1000\n_flow_conf_loss_weight = 0.5*100*0.5\n_flow_conf_grad_loss_weight = 0.25*100\n_depth_loss_weight = 0.5*300\n_depth_grad_loss_weight = 0.25*1500\n_normal_loss_weight = 0.5*50\n_rotation_loss_weight = 160\n_translation_loss_weight = 15*3\n_kernel_regularizer = tf.contrib.layers.l2_regularizer(0.0004)\n\n\ndef main(argv=None):\n\n    # Setup the session and the EvolutionTrainer.\n    # The trainer object manages snapshots, multiple evolutions and provides \n    # a mainloop for training.\n    gpu_options = tf.GPUOptions()\n    session = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))\n    trainer = EvolutionTrainer(session, _train_dir, _evolutions)\n    max_iter = _max_iter_dict[trainer.current_evo.name()]\n\n    global_stepf = tf.to_float(trainer.global_step())\n\n    top_output = (\'IMAGE_PAIR\', \'MOTION\', \'DEPTH\', \'INTRINSICS\')\n\n    batch_size = 32\n    if trainer.current_evo >= \'4_iterative\':\n        batch_size = 8\n\n    reader_params = {\n        \'batch_size\': batch_size,\n        \'test_phase\': False,\n        \'motion_format\': \'ANGLEAXIS6\',\n        \'inverse_depth\': True,\n        \'builder_threads\': 1,\n        \'scaled_width\': 256,\n        \'scaled_height\': 192,\n        \'norm_trans_scale_depth\': True,        \n        \'top_output\': top_output,\n        \'scene_pool_size\': 650,\n        \'builder_threads\': 8,\n    }\n\n    # add data sources\n    reader_params = datareader.add_sources(reader_params, glob.glob(os.path.join(_data_dir,\'sun3d_train*.h5\')), 0.8)\n    reader_params = datareader.add_sources(reader_params, glob.glob(os.path.join(_data_dir,\'rgbd_*_train.h5\')), 0.2)\n    reader_params = datareader.add_sources(reader_params, glob.glob(os.path.join(_data_dir,\'mvs_breisach.h5\')), 0.3)\n    reader_params = datareader.add_sources(reader_params, glob.glob(os.path.join(_data_dir,\'mvs_citywall.h5\')), 0.3)\n    reader_params = datareader.add_sources(reader_params, glob.glob(os.path.join(_data_dir,\'mvs_achteck_turm.h5\')), 0.003)\n    reader_params = datareader.add_sources(reader_params, glob.glob(os.path.join(_data_dir,\'scenes11_train.h5\')), 0.2)\n\n    learning_rate = ease_in_quad(\n        global_stepf-_max_iter_dict[trainer.current_evo.name()]/3,\n        _base_lr_dict[trainer.current_evo.name()], \n        1e-6-_base_lr_dict[trainer.current_evo.name()], \n        float(2*_max_iter_dict[trainer.current_evo.name()]/3) )\n    tf.summary.scalar(\'LearningRate\',learning_rate)\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=1e-6, use_locking=False)\n\n\n    with tf.name_scope(""datareader""):\n        reader_tensors = datareader.multi_vi_h5_data_reader(len(top_output), json.dumps(reader_params))\n        data_tensors = reader_tensors[2]\n        data_dict_all = dict(zip(top_output, data_tensors))\n        num_test_iterations, current_batch_buffer, max_batch_buffer, current_read_buffer, max_read_buffer = tf.unstack(reader_tensors[0])\n        tf.summary.scalar(""datareader/batch_buffer"",current_batch_buffer)\n        tf.summary.scalar(""datareader/read_buffer"",current_read_buffer)\n\n        # split the data for the individual towers\n        data_dict_split = {}\n        for k,v in data_dict_all.items():\n            if k == \'INFO\':\n                continue # skip info vector\n            if _num_gpus > 1:\n                tmp = tf.split(v, num_or_size_splits=_num_gpus)\n            else:\n                tmp = [v]\n            data_dict_split[k] = tmp\n\n    tower_grads = []  # list of list of tuple(gradient, variable)\n    tower_losses = [] # list of dict with loss_name: loss\n    tower_total_losses = [] # list of tensors with the total loss for each tower\n\n    # We use queues to collect output form the iterative part and then add this\n    # output to the next mini batch.\n    # Each tower uses its own queue, we store the queues and ops for all towers\n    # in the following lists.\n    iterative_net_queues = []\n    iterative_net_queues_enqueue_ops = []\n    iterative_net_queues_enqueue_ops_initialization = []\n    for gpu_id in range(_num_gpus):\n        with tf.device(\'/gpu:{0}\'.format(gpu_id)), tf.name_scope(\'tower_{0}\'.format(gpu_id)) as tower:\n            reuse = gpu_id != 0\n            \n            data_dict = {}\n            for k,v in data_dict_split.items():\n                data_dict[k] = v[gpu_id]\n\n            # dict of the losses of the current tower\n            loss_dict = {}\n\n            # data preprocessing\n            with tf.name_scope(""data_preprocess""):\n                rotation, translation = tf.split(value=data_dict[\'MOTION\'], num_or_size_splits=2, axis=1)\n                ground_truth = prepare_ground_truth_tensors(\n                    data_dict[\'DEPTH\'],\n                    rotation,\n                    translation,\n                    data_dict[\'INTRINSICS\'],\n                )\n                image1, image2 = tf.split(value=data_dict[\'IMAGE_PAIR\'], num_or_size_splits=2, axis=1)\n                image2_2 = tf.transpose(tf.image.resize_area(tf.transpose(image2,perm=[0,2,3,1]), (48,64)), perm=[0,3,1,2])\n                if trainer.current_evo >= \'5_refine\':\n                    data_dict[\'image1\'] = image1\n                data_dict[\'image2_2\'] = image2_2\n                ground_truth[\'rotation\'] = rotation\n                ground_truth[\'translation\'] = translation\n\n\n            #\n            # netFlow1\n            #\n            with tf.variable_scope(\'netFlow1\', reuse=reuse):\n                netFlow1_result = flow_block(\n                    image_pair=data_dict[\'IMAGE_PAIR\'], \n                    kernel_regularizer=_kernel_regularizer,\n                    )\n                predict_flow5, predict_conf5 = tf.split(value=netFlow1_result[\'predict_flowconf5\'], num_or_size_splits=2, axis=1)\n                predict_flow2, predict_conf2 = tf.split(value=netFlow1_result[\'predict_flowconf2\'], num_or_size_splits=2, axis=1)\n\n            # losses for netFlow1\n            if trainer.current_evo == \'0_flow1\':\n                with tf.name_scope(\'netFlow1_losses\'):\n                    # slowly increase the weights for the scale invariant gradient losses\n                    flow_sig_weight = ease_out_quad(global_stepf, 0, _flow_grad_loss_weight, float(max_iter//3))\n                    conf_sig_weight = ease_out_quad(global_stepf, 0, _flow_conf_grad_loss_weight, float(max_iter//3))\n                    # slowly decrase the importance of the losses on the smaller resolution\n                    level5_factor = ease_in_quad(global_stepf, 1, -1, float(max_iter//3))\n\n\n                    losses = flow_loss_block(\n                        gt_flow2=ground_truth[\'flow2\'], \n                        gt_flow5=ground_truth[\'flow5\'], \n                        gt_flow2_sig=ground_truth[\'flow2_sig\'], \n                        pr_flow2=predict_flow2, \n                        pr_flow5=predict_flow5, \n                        pr_conf2=predict_conf2, \n                        pr_conf5=predict_conf5, \n                        flow_weight=_flow_loss_weight, \n                        conf_weight=_flow_conf_loss_weight, \n                        flow_sig_weight=flow_sig_weight,\n                        conf_sig_weight=conf_sig_weight,\n                        conf_diff_scale=10,\n                        level5_factor=level5_factor,\n                        loss_prefix=\'netFlow1_\'\n                        )\n                    loss_dict.update(losses) # add to the loss dict of the current tower\n\n                    # add selected losses to the \'losses\' collection. the remaining losses are only used for summaries.\n                    selected_losses = (\'loss_flow5\', \'loss_flow2\', \'loss_flow2_sig\', \'loss_conf5\', \'loss_conf2\', \'loss_conf2_sig\')\n                    for l in selected_losses:\n                        tf.losses.add_loss(losses[\'netFlow1_\'+l])\n\n\n            #\n            # netDM1\n            #\n            if trainer.current_evo >= \'1_dm1\':\n                with tf.variable_scope(\'netDM1\', reuse=reuse):\n                    netDM1_result = depthmotion_block(\n                            image_pair=data_dict[\'IMAGE_PAIR\'],\n                            image2_2=data_dict[\'image2_2\'], \n                            prev_flow2=predict_flow2, \n                            prev_flowconf2=netFlow1_result[\'predict_flowconf2\'], \n                            kernel_regularizer=_kernel_regularizer\n                            )\n\n            # losses for netDM1\n            if trainer.current_evo in (\'1_dm1\',):\n\n                with tf.name_scope(\'netDM1_losses\'), tf.variable_scope(\'netDM1\'):\n                    # slowly increase the weights for the scale invariant gradient losses\n                    depth_sig_weight = ease_out_quad(global_stepf, 0, _depth_grad_loss_weight, float(2000000))\n\n                    losses = depthnormal_loss_block(\n                        gt_depth2=ground_truth[\'depth2\'],\n                        gt_depth2_sig=ground_truth[\'depth2_sig\'],\n                        gt_normal2=ground_truth[\'normal2\'],\n                        gt_rotation=ground_truth[\'rotation\'],\n                        gt_translation=ground_truth[\'translation\'],\n                        pr_depth2=netDM1_result[\'predict_depth2\'],\n                        pr_normal2=netDM1_result[\'predict_normal2\'],\n                        pr_rotation=netDM1_result[\'predict_rotation\'],\n                        pr_translation=netDM1_result[\'predict_translation\'],\n                        depth_weight=_depth_loss_weight,\n                        depth_sig_weight=depth_sig_weight, \n                        normal_weight=_normal_loss_weight, \n                        rotation_weight=_rotation_loss_weight,\n                        translation_weight=_translation_loss_weight,\n                        translation_factor=1,\n                        loss_prefix=\'netDM1_\',\n                        )\n                    loss_dict.update(losses) # add to the loss dict of the current tower\n\n                    # add selected losses to the \'losses\' collection. the remaining losses are only used for summaries.\n                    selected_losses = (\'loss_depth2\', \'loss_depth2_sig\', \'loss_normal2\', \'loss_rotation\', \'loss_translation\')\n                    for l in selected_losses:\n                        tf.losses.add_loss(losses[\'netDM1_\'+l])\n\n\n            if trainer.current_evo >= \'4_iterative\':\n                dtypes = [ v.dtype for k, v in data_dict.items() ]\n                dtypes += [ v.dtype for k, v in ground_truth.items() ]\n                dtypes += [ v.dtype for k, v in netDM1_result.items() ]\n                names = [ k for k in data_dict ]\n                names += [ k for k in ground_truth ]\n                names += [ k for k in netDM1_result ]\n                shapes = []\n                for k,v in data_dict.items():\n                    shapes.append([v.shape.as_list()[0]*(_simulated_iterations-1)] + v.shape.as_list()[1:])\n                for k,v in ground_truth.items():\n                    shapes.append([v.shape.as_list()[0]*(_simulated_iterations-1)] + v.shape.as_list()[1:])\n                for k,v in netDM1_result.items():\n                    shapes.append([v.shape.as_list()[0]*(_simulated_iterations-1)] + v.shape.as_list()[1:])\n\n                queue = tf.FIFOQueue(\n                    capacity=2,\n                    dtypes=dtypes,\n                    shapes=shapes,\n                    names=names,\n                    )\n                iterative_net_queues.append(queue)\n\n                enqueue_data_dict_initialization = {}\n                for k in data_dict:\n                    enqueue_data_dict_initialization[k] = tf.concat((_simulated_iterations-1)*[data_dict[k]], axis=0)\n                for k in ground_truth:\n                    enqueue_data_dict_initialization[k] = tf.concat((_simulated_iterations-1)*[ground_truth[k]], axis=0)\n                for k in netDM1_result:\n                    enqueue_data_dict_initialization[k] = tf.concat((_simulated_iterations-1)*[netDM1_result[k]], axis=0)\n                #for k,v in enqueue_data_dict_initialization.items():\n                    #print(k,v.shape.as_list())\n                enqueue_op_initialization = queue.enqueue(enqueue_data_dict_initialization)\n                iterative_net_queues_enqueue_ops_initialization.append(enqueue_op_initialization)\n\n                data_from_queue = queue.dequeue()\n                \n                for k in data_dict:\n                    data_dict[k] = tf.concat((data_dict[k],data_from_queue[k]),axis=0)\n                for k in ground_truth:\n                    ground_truth[k] = tf.concat((ground_truth[k],data_from_queue[k]),axis=0)\n                for k in netDM1_result:\n                    netDM1_result[k] = tf.concat((netDM1_result[k],data_from_queue[k]),axis=0)\n\n\n\n            #\n            # netFlow2\n            #\n            if trainer.current_evo >= \'2_flow2\':\n                with tf.variable_scope(\'netFlow2\', reuse=reuse):\n                    netFlow2_result = flow_block(\n                        image_pair=data_dict[\'IMAGE_PAIR\'],\n                        image2_2=data_dict[\'image2_2\'],\n                        intrinsics=data_dict[\'INTRINSICS\'],\n                        prev_predictions=netDM1_result,\n                        kernel_regularizer=_kernel_regularizer\n                        )\n                    predict_flow5, predict_conf5 = tf.split(value=netFlow2_result[\'predict_flowconf5\'], num_or_size_splits=2, axis=1)\n                    predict_flow2, predict_conf2 = tf.split(value=netFlow2_result[\'predict_flowconf2\'], num_or_size_splits=2, axis=1)\n\n            # losses for netFlow2\n            if trainer.current_evo in (\'2_flow2\',\'4_iterative\'):\n                with tf.name_scope(\'netFlow2_losses\'):\n                    if trainer.current_evo == \'2_flow2\':\n                        # slowly increase the weights for the scale invariant gradient losses\n                        flow_sig_weight = ease_out_quad(global_stepf, 0, _flow_grad_loss_weight, float(max_iter//3))\n                        conf_sig_weight = ease_out_quad(global_stepf, 0, _flow_conf_grad_loss_weight, float(max_iter//3))\n                        # slowly decrase the importance of the losses on the smaller resolution\n                        level5_factor = ease_in_quad(global_stepf, 1, -1, float(max_iter//3))\n                    else:\n                        flow_sig_weight = _flow_grad_loss_weight\n                        conf_sig_weight = _flow_conf_grad_loss_weight\n                        level5_factor = 0\n\n\n                    losses = flow_loss_block(\n                        gt_flow2=ground_truth[\'flow2\'], \n                        gt_flow5=ground_truth[\'flow5\'], \n                        gt_flow2_sig=ground_truth[\'flow2_sig\'], \n                        pr_flow2=predict_flow2, \n                        pr_flow5=predict_flow5, \n                        pr_conf2=predict_conf2, \n                        pr_conf5=predict_conf5, \n                        flow_weight=_flow_loss_weight, \n                        conf_weight=_flow_conf_loss_weight, \n                        flow_sig_weight=flow_sig_weight,\n                        conf_sig_weight=conf_sig_weight,\n                        conf_diff_scale=10,\n                        level5_factor=level5_factor,\n                        loss_prefix=\'netFlow2_\'\n                        )\n                    loss_dict.update(losses) # add to the loss dict of the current tower\n\n                    # add selected losses to the \'losses\' collection. the remaining losses are only used for summaries.\n                    selected_losses = (\'loss_flow5\', \'loss_flow2\', \'loss_flow2_sig\', \'loss_conf5\', \'loss_conf2\', \'loss_conf2_sig\')\n                    for l in selected_losses:\n                        tf.losses.add_loss(losses[\'netFlow2_\'+l])\n\n\n            #\n            # netDM2\n            #\n            if trainer.current_evo >= \'3_dm2\':\n                with tf.variable_scope(\'netDM2\', reuse=reuse):\n                    netDM2_result = depthmotion_block(\n                            image_pair=data_dict[\'IMAGE_PAIR\'], \n                            image2_2=data_dict[\'image2_2\'], \n                            prev_flow2=predict_flow2, \n                            prev_flowconf2=netFlow2_result[\'predict_flowconf2\'], \n                            intrinsics=data_dict[\'INTRINSICS\'],\n                            prev_rotation=netDM1_result[\'predict_rotation\'],\n                            prev_translation=netDM1_result[\'predict_translation\'],\n                            kernel_regularizer=_kernel_regularizer\n                            )\n\n            # losses for netDM2\n            if trainer.current_evo in (\'3_dm2\', \'4_iterative\'):\n\n                with tf.name_scope(\'netDM2_losses\'), tf.variable_scope(\'netDM2\'):\n                    # slowly increase the weights for the scale invariant gradient losses\n                    if trainer.current_evo == \'3_dm2\':\n                        depth_sig_weight = ease_out_quad(global_stepf, 0, _depth_grad_loss_weight, float(max_iter))\n                    else:\n                        depth_sig_weight = _depth_grad_loss_weight\n\n                    losses = depthnormal_loss_block(\n                        gt_depth2=ground_truth[\'depth2\'],\n                        gt_depth2_sig=ground_truth[\'depth2_sig\'],\n                        gt_normal2=ground_truth[\'normal2\'],\n                        gt_rotation=ground_truth[\'rotation\'],\n                        gt_translation=ground_truth[\'translation\'],\n                        pr_depth2=netDM2_result[\'predict_depth2\'],\n                        pr_normal2=netDM2_result[\'predict_normal2\'],\n                        pr_rotation=netDM2_result[\'predict_rotation\'],\n                        pr_translation=netDM2_result[\'predict_translation\'],\n                        depth_weight=_depth_loss_weight,\n                        depth_sig_weight=depth_sig_weight, \n                        normal_weight=_normal_loss_weight, \n                        rotation_weight=_rotation_loss_weight,\n                        translation_weight=_translation_loss_weight,\n                        translation_factor=1,\n                        loss_prefix=\'netDM2_\',\n                        )\n                    loss_dict.update(losses) # add to the loss dict of the current tower\n\n                    # add selected losses to the \'losses\' collection. the remaining losses are only used for summaries.\n                    selected_losses = (\'loss_depth2\', \'loss_depth2_sig\', \'loss_normal2\', \'loss_rotation\', \'loss_translation\')\n                    for l in selected_losses:\n                        tf.losses.add_loss(losses[\'netDM2_\'+l])\n\n            \n            if trainer.current_evo == \'5_refine\':\n                with tf.variable_scope(\'netRefine\', reuse=reuse):\n                    netRefine_result = depth_refine_block(\n                            image1=data_dict[\'image1\'], \n                            depthmotion_predictions=netDM2_result,\n                            kernel_regularizer=_kernel_regularizer\n                            )\n                    \n                with tf.name_scope(\'netRefine_losses\'), tf.variable_scope(\'netRefine\'):\n                    # slowly increase the weights for the scale invariant gradient losses\n                    depth_sig_weight = ease_out_quad(global_stepf, 0, 0.5*_depth_grad_loss_weight, float(max_iter))\n\n                    losses = depth_refine_loss_block(\n                        gt_depth0=ground_truth[\'depth0\'],\n                        gt_depth0_sig=ground_truth[\'depth0_sig\'],\n                        gt_normal0=ground_truth[\'normal0\'],\n                        pr_depth0=netRefine_result[\'predict_depth0\'],\n                        pr_normal0=netRefine_result[\'predict_normal0\'],\n                        depth_weight=_depth_loss_weight,\n                        depth_sig_weight=depth_sig_weight, \n                        normal_weight=_normal_loss_weight, \n                        loss_prefix=\'netRefine_\',\n                        )\n                    loss_dict.update(losses) # add to the loss dict of the current tower\n\n                    # add selected losses to the \'losses\' collection. the remaining losses are only used for summaries.\n                    selected_losses = (\'loss_depth0\', \'loss_depth0_sig\', \'loss_normal0\', )\n                    for l in selected_losses:\n                        tf.losses.add_loss(losses[\'netRefine_\'+l])\n\n\n\n            \n            if trainer.current_evo >= \'4_iterative\':\n                # split the data and enqueue the \'newer\' parts\n                enqueue_data_dict = {}\n                # dicts with data required by the next iteration\n                dicts = (data_dict,ground_truth,netDM2_result) \n                for d in dicts:\n                    for k in d:\n                        shape = d[k].shape.as_list()\n                        num =  (_simulated_iterations-1)*shape[0]//_simulated_iterations\n                        slice_size = [num] + shape[1:]\n                        slice_begin = len(slice_size)*[0]\n                        enqueue_data_dict[k] = tf.slice(d[k], begin=slice_begin, size=slice_size)\n                    \n                enqueue_op = queue.enqueue(enqueue_data_dict)\n                iterative_net_queues_enqueue_ops.append(enqueue_op)\n                \n\n                \n            \n\n            # generate a summary for all the individual losses in this tower\n            if _num_gpus > 1:\n                for name, loss in loss_dict.items():\n                    tf.summary.scalar(name,loss)\n\n            tower_losses.append(loss_dict)\n\n            # compute loss for this tower\n            losses = tf.losses.get_losses(scope=tower)\n            regularization_losses = []\n            if gpu_id == 0:\n                regularization_losses = tf.losses.get_regularization_losses(scope=tower)\n            tower_total_loss = tf.add_n(losses+regularization_losses)\n            tower_total_losses.append(tower_total_loss)\n            tf.summary.scalar(\'TotalLoss\',tower_total_loss)\n\n            # define which variables to train\n            train_vars = []\n            if trainer.current_evo <=\'0_flow1\':\n                train_vars.extend(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \'netFlow1\'))\n\n            if trainer.current_evo in (\'1_dm1\', ):\n                train_vars.extend(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \'netDM1\'))\n    \n            if trainer.current_evo in (\'2_flow2\', \'4_iterative\'):\n                train_vars.extend(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \'netFlow2\'))\n    \n            if trainer.current_evo in (\'3_dm2\', \'4_iterative\'):\n                train_vars.extend(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \'netDM2\'))\n\n            if trainer.current_evo in (\'5_refine\',):\n                train_vars.extend(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \'netRefine\'))\n    \n    \n            grads_and_vars = optimizer.compute_gradients(loss=tower_total_loss, var_list=train_vars, colocate_gradients_with_ops=False)\n            clipped_grads_and_vars = []\n            with tf.name_scope(\'clip_gradients\'):\n                for g, v in grads_and_vars:\n                    if not g is None:\n                        clipped_g = tf.clip_by_value(g, clip_value_min=-100, clip_value_max=100)\n                        #clipped_g = g\n                        clipped_grads_and_vars.append((clipped_g,v))\n                    else:\n                        clipped_grads_and_vars.append((g,v))\n\n            tower_grads.append(clipped_grads_and_vars)\n\n\n    combined_losses = combine_loss_dicts(tower_losses)\n    with tf.name_scope(\'CombinedLosses\'):\n        for name, loss in combined_losses.items():\n            tf.summary.scalar(name, loss)\n        if _num_gpus > 1:\n            total_loss = tf.add_n(tower_total_losses, \'TotalLoss\')\n        else:\n            total_loss = tower_total_losses[0]\n        tf.summary.scalar(\'TotalLoss\', total_loss)\n        \n\n    # combine gradients from all towers\n    avg_grads = average_gradients(tower_grads)\n\n    optimize_op = optimizer.apply_gradients(grads_and_vars=avg_grads, global_step=trainer.global_step())\n\n\n\n    train_op = optimize_op\n\n    summary_op = tf.summary.merge_all()\n\n    train_var_summary_op_list = []\n    for g, v in clipped_grads_and_vars:\n        train_var_summary_op_list.append( tf.summary.histogram(v.name,  v, collections=\'TRAIN_VARS_SUMMARIES\') )\n        if not g is None:\n            train_var_summary_op_list.append( tf.summary.histogram(v.name+\'_grad\',  g, collections=\'TRAIN_VARS_SUMMARIES\') )\n    train_var_summary_op = tf.summary.merge(train_var_summary_op_list)\n\n    check_numeric_ops = []\n    for x in train_vars:\n        check_numeric_ops.append(tf.check_numerics(x, \'train var check\'))\n    check = tf.group(*check_numeric_ops)\n\n\n\n    # init all vars\n    init_op = tf.global_variables_initializer()\n    session.run(init_op)\n\n\n    # restore weights from checkpoint\n    trainer.load_checkpoint()  \n\n    if trainer.current_evo >= \'4_iterative\':\n        # init the queues\n        session.run(iterative_net_queues_enqueue_ops_initialization)\n        # make sure that enqueue ops run each iteration\n        train_op = tf.group(optimize_op, *iterative_net_queues_enqueue_ops)\n\n    # define which variables to save\n    save_var_dict = create_save_var_dict() # adds global_step and all trainable variables\n\n\n    # train\n    status = trainer.mainloop(\n            max_iter=max_iter, \n            train_ops=[train_op], \n            saver_interval=100000, \n            saver_var_list=save_var_dict,\n            summary_int_ops=[(_k//2,summary_op), (5*_k,train_var_summary_op)],\n            display_str_ops=[(\'total_loss\',tf.check_numerics(total_loss, \'total_loss\')), (\'learning_rate\', learning_rate)],\n            display_interval=100,\n            custom_int_ops=[(1*_k,check)], \n            recovery_saver_interval=10,\n            )\n\n    trainer.coordinator().raise_requested_exception()\n\n    return status\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n\n\n'"
python/depthmotionnet/datareader/__init__.py,0,"b""#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport os\nimport tensorflow\nfrom .helpers import *\n\n# try to import the multivih5datareaderop from the 'build' directory\nif 'MULTIVIH5DATAREADEROP_LIB' in os.environ:\n    _readerlib_path = os.environ['MULTIVIH5DATAREADEROP_LIB']\nelse:\n    _readerlib_path = os.path.abspath(os.path.join(os.path.split(__file__)[0], '..', '..', '..', 'build','multivih5datareaderop', 'multivih5datareaderop.so'))\n\nreaderlib = None\nmulti_vi_h5_data_reader = None\nif os.path.isfile(_readerlib_path):\n    readerlib = tensorflow.load_op_library(_readerlib_path)\n    print('Using {0}'.format(_readerlib_path))\n    multi_vi_h5_data_reader = readerlib.multi_vi_h5_data_reader\n\n"""
python/depthmotionnet/datareader/helpers.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n\ndef add_sources(params, dataset_files, weight, normalize=True, concatenate=False):\n    """"""Add sources to the parameters for the multi_vi_h5_data_reader op.\n\n    params: dict\n        dict with the parameters for the multi_vi_h5_data_reader op.\n\n    dataset_files: list of str\n        List of h5 file paths to be added as sources.\n\n    weight: float\n        The sampling importance. \n        Higher values means the reader op samples more often from these files.\n\n    normalize: bool\n        If True the weight for each file will be divided by the number of files.\n        If concatenate is True this parameter has no effect.\n\n    concatenate: bool\n        If True adds only a single source that contains all files.\n\n    """"""\n    if not \'source\' in params:\n        params[\'source\'] = []\n\n    if concatenate:\n        # generate a single source with all paths\n        source = {\'path\': \';\'.join(dataset_files)}\n        params[\'source\'].append(source)\n\n    else:\n        # generate for each path a new source\n        for item in dataset_files:\n            w = weight\n            if normalize:\n                w /= len(dataset_files)\n\n            source = {\'path\': item, \'weight\': w}\n            params[\'source\'].append(source)\n\n    return params\n'"
python/depthmotionnet/dataset_tools/__init__.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom .view import View\nfrom .view_io import *\nfrom .view_tools import *\n'"
python/depthmotionnet/dataset_tools/helpers.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport numpy as np\nfrom PIL import Image\nfrom scipy.ndimage.filters import laplace\n\n\ndef measure_sharpness(img):\n    """"""Measures the sharpeness of an image using the variance of the laplacian\n\n    img: PIL.Image\n\n    Returns the variance of the laplacian. Higher values mean a sharper image\n    """"""\n    img_gray = np.array(img.convert(\'L\'), dtype=np.float32)\n    return np.var(laplace(img_gray))\n\n\ndef concat_images_vertical(images):\n    """"""Concatenates a list of PIL.Image in vertical direction\n\n    images: list of PIL.Image\n\n    Returns the concatenated image\n    """"""\n    total_height = 0\n    total_width = 0\n    for img in images:\n        total_width = max(total_width, img.size[0])\n        total_height += img.size[1]\n    result = Image.new(\'RGB\',(total_width,total_height))\n    ypos = 0\n    for img in images:\n        result.paste(img,(0,ypos))\n        ypos += img.size[1]\n    return result\n\n\ndef concat_images_horizontal(images):\n    """"""Concatenates a list of PIL.Image in horizontal direction\n\n    images: list of PIL.Image\n\n    Returns the concatenated image\n    """"""\n    total_height = 0\n    total_width = 0\n    for img in images:\n        total_height = max(total_height, img.size[1])\n        total_width += img.size[0]\n    result = Image.new(\'RGB\',(total_width,total_height))\n    xpos = 0\n    for img in images:\n        result.paste(img,(xpos,0))\n        xpos += img.size[0]\n    return result\n\n\ndef safe_crop_image(image, box, fill_value):\n    """"""crops an image and adds a border if necessary\n    \n    image: PIL.Image\n\n    box: 4 tuple\n        (x0,y0,x1,y1) tuple\n\n    fill_value: color value, scalar or tuple\n\n    Returns the cropped image\n    """"""\n    x0, y0, x1, y1 = box\n    if x0 >=0 and y0 >= 0 and x1 < image.width and y1 < image.height:\n        return image.crop(box)\n    else:\n        crop_width = x1-x0\n        crop_height = y1-y0\n        tmp = Image.new(image.mode, (crop_width, crop_height), fill_value)\n        safe_box = (\n            max(0,min(x0,image.width-1)),\n            max(0,min(y0,image.height-1)),\n            max(0,min(x1,image.width)),\n            max(0,min(y1,image.height)),\n            )\n        img_crop = image.crop(safe_box)\n        x = -x0 if x0 < 0 else 0\n        y = -y0 if y0 < 0 else 0\n        tmp.paste(image, (x,y))\n        return tmp\n\n\ndef safe_crop_array2d(arr, box, fill_value):\n    """"""crops an array and adds a border if necessary\n    \n    arr: numpy.ndarray with 2 dims\n\n    box: 4 tuple\n        (x0,y0,x1,y1) tuple. x is the column and y is the row!\n\n    fill_value: scalar\n\n    Returns the cropped array\n    """"""\n    x0, y0, x1, y1 = box\n    if x0 >=0 and y0 >= 0 and x1 < arr.shape[1] and y1 < arr.shape[0]:\n        return arr[y0:y1,x0:x1]\n    else:\n        crop_width = x1-x0\n        crop_height = y1-y0\n        tmp = np.full((crop_height, crop_width), fill_value, dtype=arr.dtype)\n        safe_box = (\n            max(0,min(x0,arr.shape[1]-1)),\n            max(0,min(y0,arr.shape[0]-1)),\n            max(0,min(x1,arr.shape[1])),\n            max(0,min(y1,arr.shape[0])),\n            )\n        x = -x0 if x0 < 0 else 0\n        y = -y0 if y0 < 0 else 0\n        safe_width = safe_box[2]-safe_box[0]\n        safe_height = safe_box[3]-safe_box[1]\n        tmp[y:y+safe_height,x:x+safe_width] = arr[safe_box[1]:safe_box[3],safe_box[0]:safe_box[2]]\n        return tmp\n\n'"
python/depthmotionnet/dataset_tools/lz4.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom ctypes import *\nimport os\n\n# try the version used by the multivih5datareaderop first\ntry:\n    _lib_path = os.path.abspath(os.path.join(os.path.split(__file__)[0], \'..\', \'..\', \'..\', \'build\',\'lz4\',\'src\',\'lz4-build\',\'contrib\', \'cmake_unofficial\', \'liblz4.so\'))\n    liblz4 = CDLL(_lib_path)\nexcept:\n    # try system version\n    try:\n        liblz4 = CDLL(\'liblz4.so\')\n    except:\n        raise RuntimeError(\'Cannot load liblz4.so\')\n\n\ndef lz4_uncompress(input_data, expected_decompressed_size):\n    """"""decompresses the LZ4 compressed data\n    \n    input_data: bytes\n        byte string of the input data\n\n    expected_decompressed_size: int\n        size of the decompressed output data\n\n    returns the decompressed data as bytes or None on error\n    """"""\n    assert isinstance(input_data,bytes), ""input_data must be of type bytes""\n    assert isinstance(expected_decompressed_size,int), ""expected_decompressed_size must be of type int""\n\n    dst_buf = create_string_buffer(expected_decompressed_size)\n    status = liblz4.LZ4_decompress_safe(input_data,dst_buf,len(input_data),expected_decompressed_size)\n    if status != expected_decompressed_size:\n        return None\n    else:\n        return dst_buf.raw\n\n\n\ndef lz4_compress_bound(input_size):\n    """"""Returns the maximum size needed for compressing data with the given input_size""""""\n    assert isinstance(input_size,int), ""input_size must be of type int""\n    \n    result = liblz4.LZ4_compressBound(c_int(input_size))\n    return result\n\n\n\ndef lz4_compress_HC(src):\n    """"""Compresses the input bytes with LZ4 high compression algorithm.\n\n    Returns the compressed bytes array or an empty array on error\n    """"""\n    assert isinstance(src,bytes), ""src must be of type bytes""\n    max_compressed_size = lz4_compress_bound(len(src))\n    dst_buf = create_string_buffer(max_compressed_size)\n    # written_size = liblz4.LZ4_compress_HC(src, dst_buf, len(src), max_compressed_size, c_int(0)) # new signature. TODO update liblz4\n    written_size = liblz4.LZ4_compressHC(src, dst_buf, len(src))\n    return dst_buf.raw[:written_size]\n    \n'"
python/depthmotionnet/dataset_tools/sun3d_utils.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport os\nimport math\nimport itertools\nimport h5py\nimport numpy as np\nfrom PIL import Image\nfrom scipy.ndimage import maximum_filter1d, minimum_filter1d\n\nfrom .view import View\nfrom .view_io import *\nfrom .view_tools import *\nfrom .helpers import measure_sharpness\n\n\ndef read_frameid_timestamp(files):\n    """"""Get frameids and timestamps from the sun3d filenames\n    \n    files: list of str\n        a list of the filenames\n        \n    Returns the frameid and timestamp as numpy.array\n    """"""\n    frameids = []\n    timestamps = []\n    for f in files:\n        id_timestamp = f[:-4].split(\'-\')\n        frameids.append( int(id_timestamp[0]) )\n        timestamps.append( int(id_timestamp[1]) )\n    return np.asarray(frameids), np.asarray(timestamps)\n\n\ndef read_image(filename):\n    """"""Read image from a file\n    \n    filename: str\n    \n    Returns image as PIL Image\n    """"""\n    image = Image.open(filename)\n    image.load()\n    return image\n\ndef read_depth(filename):\n    """"""Read depth from a sun3d depth file\n    \n    filename: str\n    \n    Returns depth as np.float32 array\n    """"""\n    depth_pil = Image.open(filename)\n    depth_arr = np.array(depth_pil)\n    depth_uint16 = depth_arr.astype(np.uint16)\n    depth_shifted = (depth_uint16 >> 3) | (depth_uint16 << 13)\n    depth_float = (depth_shifted/1000).astype(np.float32)\n    return depth_float\n\ndef read_Rt(extrinsics, frame):\n    """"""Read camera extrinsics at certain frame\n    \n    extrinsics: np array with size (totalframe*3, 4)\n    \n    frame: int (starts from 0)\n    \n    Returns the rotation and translation \n    """"""\n    Rt = extrinsics[3*frame:3*frame+3]\n    R_arr = Rt[0:3,0:3]\n    t_arr = Rt[0:3,3]\n    R = R_arr.transpose()\n    t = -np.dot(R,t_arr)\n    return R, t\n\n\ndef compute_sharpness(sun3d_data_path, seq_name):\n    """"""Returns a numpy array with the sharpness score of all images in the sequence.\n\n    sun3d_data_path: str\n        base path to the sun3d data\n\n    seq_name: str\n        the name of the sequence e.g. ""mit_32_d463/d463_1""\n\n    """"""\n    seq_path = os.path.join(sun3d_data_path,seq_name)\n    image_files = [f for f in sorted(os.listdir(os.path.join(seq_path,\'image\'))) if f.endswith(\'.jpg\')]\n\n    sharpness = []\n    for img_file in image_files:\n        img = read_image(os.path.join(seq_path,\'image\',img_file))\n        sharpness.append(measure_sharpness(img))\n\n    return np.asarray(sharpness)\n\n\ndef create_samples_from_sequence(h5file, sun3d_data_path, seq_name, baseline_range, sharpness, sharpness_window=30, max_views_num=10):\n    """"""Read a sun3d sequence and write samples to the h5file\n    \n    h5file: h5py.File handle\n    \n    sun3d_data_path: str\n        base path to the sun3d data\n\n    seq_name: str\n        the name of the sequence e.g. ""mit_32_d463/d463_1""\n\n    baseline_range: tuple(float,float)\n        The allowed baseline range\n\n    sharpness: numpy.ndarray 1D\n        Array with the sharpness score for each image\n\n    sharpness_window: int\n        Window for detecting sharp images\n\n    Returns the number of generated groups\n    """"""\n    generated_groups = 0\n    seq_path = os.path.join(sun3d_data_path,seq_name)\n    group_prefix = seq_name.replace(\'/\',\'.\')\n    if not os.path.exists(os.path.join(seq_path, \'extrinsics\')):\n       return 0\n\n    # file list\n    image_files = [f for f in sorted(os.listdir(os.path.join(seq_path,\'image\'))) if f.endswith(\'.jpg\')]\n    depth_files = [f for f in sorted(os.listdir(os.path.join(seq_path,\'depthTSDF\'))) if f.endswith(\'.png\')]\n    extrinsics_files = [f for f in sorted(os.listdir(os.path.join(seq_path,\'extrinsics\'))) if f.endswith(\'.txt\')]\n\n    # read intrinsics\n    intrinsics = np.loadtxt(os.path.join(seq_path,\'intrinsics.txt\'))\n\n    # read extrinsics params\n    extrinsics = np.loadtxt(os.path.join(seq_path,\'extrinsics\',extrinsics_files[-1]))\n\n    # read time stamp\n    img_ids, img_timestamps = read_frameid_timestamp(image_files)\n    _, depth_timestamps = read_frameid_timestamp(depth_files)\n\n    # find a depth for each image\n    idx_img2depth = []\n    for img_timestamp in img_timestamps:\n        idx_img2depth.append(np.argmin(abs(depth_timestamps[:] - img_timestamp)))\n\n\n    # find sharp images with nonmaximum suppression\n    assert sharpness.size == len(image_files)\n    sharpness_maxfilter = maximum_filter1d(np.asarray(sharpness), size=sharpness_window, mode=\'constant\', cval=0)\n    sharp_images_index = np.where( sharpness == sharpness_maxfilter )[0]\n\n    used_views = set()\n    for i1, frame_idx1 in enumerate(sharp_images_index):\n        if i1 in used_views:\n            continue\n            \n        R1, t1 = read_Rt(extrinsics, frame_idx1)\n        i2 = i1+1\n        \n        depth_file = os.path.join(seq_path,\'depthTSDF\', depth_files[idx_img2depth[frame_idx1]])\n        depth1 = read_depth(depth_file)\n        \n        if np.count_nonzero(np.isfinite(depth1) & (depth1 > 0)) < 0.5*depth1.size:\n            continue\n        \n        image1 = read_image(os.path.join(seq_path,\'image\',image_files[frame_idx1]))\n        view1 = View(R=R1, t=t1, K=intrinsics, image=image1, depth=depth1, depth_metric=\'camera_z\')\n        \n        views = [view1]\n        used_views.add(i1)\n        \n        for i2 in range(i1+1, sharp_images_index.size):\n            frame_idx2 = sharp_images_index[i2]\n            R2, t2 = read_Rt(extrinsics, frame_idx2)\n            baseline = np.linalg.norm( (-R1.transpose().dot(t1)) - (-R2.transpose().dot(t2))) # unit is meters\n            if baseline < baseline_range[0] or baseline > baseline_range[1]:\n                continue\n            \n            cosine = np.dot(R1[2,:],R2[2,:])\n            if cosine < math.cos(math.radians(70)):\n                continue\n                \n            depth_file = os.path.join(seq_path,\'depthTSDF\', depth_files[idx_img2depth[frame_idx2]])\n            depth2 = read_depth(depth_file)\n            \n            if np.count_nonzero(np.isfinite(depth2) & (depth2 > 0)) < 0.5*depth2.size:\n                continue\n\n            view2 = View(R=R2, t=t2, K=intrinsics, image=None, depth=depth2, depth_metric=\'camera_z\')\n            check_params = {\'min_valid_threshold\': 0.4, \'min_depth_consistent\': 0.7 }\n            if check_depth_consistency(view1, [view2],**check_params) and check_depth_consistency(view2, [view1], **check_params):\n                image2 = read_image(os.path.join(seq_path,\'image\',image_files[frame_idx2]))\n                view2 = view2._replace(image=image2)\n                views.append(view2)\n                used_views.add(i2)\n                # print(baseline, cosine)\n            if len(views) > max_views_num:\n                break\n            \n        if len(views) > 1:\n            group_name = group_prefix+\'-{:07d}\'.format(img_ids[i1])\n            print(\'writing\', group_name)\n\n            view_pairs = []\n            for pair in itertools.product(range(len(views)),repeat=2):\n                if pair[0] != pair[1]:\n                    baseline = np.linalg.norm(views[pair[0]].t-views[pair[1]].t)\n                    if baseline >= baseline_range[0] or baseline <= baseline_range[1]:\n                        view_pairs.extend(pair)\n            for i, v in enumerate(views):\n                view_group = h5file.require_group(group_name+\'/frames/t0/v{0}\'.format(i))\n                write_view(view_group, v)\n\n            # write valid image pair combinations to the group t0\n            viewpoint_pairs = np.array(view_pairs, dtype=np.int32)\n            time_group = h5file[group_name][\'frames/t0\']\n            time_group.attrs[\'viewpoint_pairs\'] = viewpoint_pairs\n            generated_groups += 1\n\n    return generated_groups\n                \n    \n\n'"
python/depthmotionnet/dataset_tools/view.py,0,"b""#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom collections import namedtuple\n\n# depth always stores the absolute depth values (not inverse depth)\n# image is a PIL.Image with the same dimensions as depth\n# depth_metric should always be 'camera_z'\n# K corresponds to the width and height of image/depth\n# R, t is the world to camera transform\nView = namedtuple('View',['R','t','K','image','depth','depth_metric'])\n\n"""
python/depthmotionnet/dataset_tools/view_io.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport numpy as np\nfrom PIL import Image\nfrom io import BytesIO\nfrom .lz4 import lz4_uncompress, lz4_compress_HC\nfrom .webp import webp_encode_array, webp_encode_image\n\nfrom .view import View\n\n\ndef read_webp_image(h5_dataset):\n    """"""Reads a dataset that stores an image compressed as webp\n    \n    h5_dataset : hdf5 dataset object\n\n    Returns the image as PIL Image\n    """"""\n    data = h5_dataset[:].tobytes()\n    img_bytesio = BytesIO(data)\n    pil_img = Image.open(img_bytesio,\'r\')\n    return pil_img\n\n\ndef write_webp_image(h5_group, image, dsname=""image""):\n    """"""Writes the image as webp to a new dataset\n\n    h5_group: hdf5 group\n        The group that shall contain the newly created dataset\n\n    image: PIL.Image or rgb numpy array\n        The image\n    """"""\n    if isinstance(image,np.ndarray):\n        compressed_data = webp_encode_array(image)\n    else:\n        compressed_data = webp_encode_image(image)\n    image_compressed = np.frombuffer(compressed_data,dtype=np.int8)\n    ds = h5_group.create_dataset(dsname, data=image_compressed)\n    ds.attrs[\'format\'] = np.string_(""webp"")\n\n\n\ndef read_lz4half_depth(h5_dataset):\n    """"""Reads a dataset that stores a depth map in lz4 compressed float16 format\n    \n    h5_dataset : hdf5 dataset object\n\n    Returns the depth map as numpy array with float32\n    """"""\n    extents = h5_dataset.attrs[\'extents\']\n    num_pixel = extents[0]*extents[1]\n    expected_size = 2*num_pixel\n    data = h5_dataset[:].tobytes()\n    depth_raw_data = lz4_uncompress(data,int(expected_size))\n    depth = np.fromstring(depth_raw_data,dtype=np.float16)\n    depth = depth.astype(np.float32)\n    depth = depth.reshape((extents[0],extents[1]))\n    return depth\n\n\ndef write_lz4half_depth(h5_group, depth, depth_metric, dsname=""depth""):\n    """"""Writes the depth as 16bit lz4 compressed char array to the given path\n\n    h5_group: hdf5 group\n        The group that shall contain the newly created dataset\n\n    depth: numpy array with float32\n    """"""\n    assert isinstance(depth, np.ndarray), ""depth must be a numpy array""\n    assert depth.dtype == np.float32, ""depth must be a float32 array""\n    assert len(depth.shape) == 2, ""depth must be a 2d array""\n    assert depth_metric in (\'camera_z\', \'ray_length\'), ""depth metric must be either \'camera_z\' or \'ray_length\'""\n    height = depth.shape[0]\n    width = depth.shape[1]\n    depth16 = depth.astype(np.float16)\n    depth_raw_data = depth16.tobytes()\n    compressed_data = lz4_compress_HC(depth_raw_data)\n    depth_compressed = np.frombuffer(compressed_data,dtype=np.int8)\n    ds = h5_group.create_dataset(dsname, data=depth_compressed)\n    ds.attrs[\'format\'] = np.string_(""lz4half"")\n    ds.attrs[\'extents\'] = np.array([height, width], dtype=np.int32)\n    ds.attrs[\'depth_metric\'] = np.string_(depth_metric)\n\n\ndef read_camera_params(h5_dataset):\n    """"""Reads a dataset that stores camera params in float64\n    \n    h5_dataset : hdf5 dataset object\n\n    Returns K,R,t as numpy array with float64\n    """"""\n    fx = h5_dataset[0]\n    fy = h5_dataset[1]\n    skew = h5_dataset[2]\n    cx = h5_dataset[3]\n    cy = h5_dataset[4]\n    K = np.array([[fx, skew, cx],\n                 [0, fy, cy],\n                 [0, 0, 1]], dtype=np.float64)\n    R = np.array([[h5_dataset[5], h5_dataset[8], h5_dataset[11]], \n                  [h5_dataset[6], h5_dataset[9], h5_dataset[12]], \n                  [h5_dataset[7], h5_dataset[10], h5_dataset[13]]], dtype=np.float64)\n    t = np.array([h5_dataset[14], h5_dataset[15], h5_dataset[16]], dtype=np.float64)   \n    return K,R,t\n\n\ndef write_camera_params(h5_group, K, R, t, dsname=""camera""):\n    """"""Writes the camera params as float64 to the given path\n\n    h5_group: hdf5 group\n        The group that shall contain the newly created dataset\n\n    K, R, t: numpy array with float64\n    """"""\n    data = np.array([K[0,0], K[1,1], K[0,1], K[0,2], K[1,2], \n                    R[0,0], R[1,0], R[2,0], R[0,1], R[1,1], R[2,1], R[0,2], R[1,2], R[2,2], \n                    t[0], t[1], t[2]], dtype=np.float64)\n    ds = h5_group.create_dataset(dsname, data=data)\n    ds.attrs[\'format\'] = ""pinhole"".encode(\'ascii\')\n\n\ndef read_view(h5_group):\n    """"""Reads the view group and returns it as a View tuple\n    \n    h5_group: hdf5 group\n        The group for reading the view\n\n    Returns the View tuple\n    """"""\n    img = read_webp_image(h5_group[\'image\'])\n    depth = read_lz4half_depth(h5_group[\'depth\'])\n    depth_metric = h5_group[\'depth\'].attrs[\'depth_metric\'].decode(\'ascii\')\n    K_arr,R_arr,t_arr = read_camera_params(h5_group[\'camera\'])\n    return View(image=img, depth=depth, depth_metric=depth_metric, K=K_arr, R=R_arr, t=t_arr)\n\n\ndef write_view(h5_group, view):\n    """"""Writes the View tuple to the group\n\n    h5_group: hdf5 group\n        The group for storing the view\n\n    view: View namedtuple\n        The tuple storing the view\n    \n    """"""\n    for ds in (\'image\', \'depth\', \'camera\'):\n        if ds in h5_group:\n            del h5_group[ds]\n\n    write_webp_image(h5_group, view.image)\n    write_lz4half_depth(h5_group, view.depth, view.depth_metric)\n    write_camera_params(h5_group, view.K, view.R, view.t)\n\n\n'"
python/depthmotionnet/dataset_tools/view_tools.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport pyximport; pyximport.install()\nimport numpy as np\n\nfrom .view import View\n\ndef compute_visible_points_mask( view1, view2, borderx=0, bordery=0 ):\n    """"""Computes a mask of the pixels in view1 that are visible in view2\n\n    view1: View namedtuple\n        First view\n\n    view2: View namedtuple\n        Second view\n\n    borderx: int\n        border in x direction. Points in the border are considered invalid\n\n    bordery: int\n        border in y direction. Points in the border are considered invalid\n\n    Returns a mask of valid points\n    """"""\n    from .view_tools_cython import compute_visible_points_mask as _compute_visible_points_mask\n    assert view1.depth_metric == \'camera_z\', ""Depth metric must be \'camera_z\'""\n    return _compute_visible_points_mask( view1, view2, borderx, bordery )\n\n\ndef compute_depth_ratios( view1, view2 ):\n    """"""Projects each point defined in view1 to view2 and computes the ratio of \n    the depth value of the projected point and the stored depth value in view2.\n\n\n    view1: View namedtuple\n        First view\n\n    view2: View namedtuple\n        Second view\n\n    Returns the scale value for view2 relative to view1\n    """"""\n    from .view_tools_cython import compute_depth_ratios as _compute_depth_ratios\n    return _compute_depth_ratios(view1, view2)\n\n\ndef check_depth_consistency( view, rest_of_the_views, depth_ratio_threshold=0.9, min_valid_threshold=0.5, min_depth_consistent=0.7 ):\n    """"""Checks if the depth of view is consistent with the rest_of_the_views\n    \n    view: View namedtuple\n        Reference view\n\n    rest_of_the_views: list of View namedtuple\n        List of the rest of the views\n\n    depth_ratio_threshold: float\n        The allowed minimum depth ratio\n\n    min_valid_threshold: float\n        ratio of pixels that should have consistent depth values with the rest_of_the_views\n\n    min_depth_consistent: float\n        ratio of depth consistent pixels with respect to the number of valid depth ratios\n\n    Returns True if the depth is consistent\n    """"""\n    min_ratio_threshold = min(depth_ratio_threshold, 1/depth_ratio_threshold)\n    max_ratio_threshold = max(depth_ratio_threshold, 1/depth_ratio_threshold)\n    for v in rest_of_the_views:\n        dr = compute_depth_ratios(view, v)\n        valid_dr = dr[np.isfinite(dr)]\n        if valid_dr.size / dr.size < min_valid_threshold:\n            return False\n\n        num_consistent = np.count_nonzero((valid_dr > min_ratio_threshold) & (valid_dr < max_ratio_threshold))\n        if num_consistent / valid_dr.size < min_depth_consistent:\n            return False\n\n    return True\n\n\ndef adjust_intrinsics(view, K_new, width_new, height_new):\n    """"""Creates a new View with the specified intrinsics and image dimensions.\n    The skew parameter K[0,1] will be ignored.\n    \n    view: View namedtuple\n        The view tuple\n        \n    K_new: numpy.ndarray\n        3x3 calibration matrix with the new intrinsics\n        \n    width_new: int\n        The new image width\n        \n    height_new: int\n        The new image height\n        \n    Returns a View tuple with adjusted image, depth and intrinsics\n    """"""\n    from PIL import Image\n    from skimage.transform import resize\n    from .helpers import safe_crop_image, safe_crop_array2d\n\n    #original parameters\n    fx = view.K[0,0]\n    fy = view.K[1,1]\n    cx = view.K[0,2]\n    cy = view.K[1,2]\n    width = view.image.width\n    height = view.image.height\n    \n    #target param\n    fx_new = K_new[0,0]\n    fy_new = K_new[1,1]\n    cx_new = K_new[0,2]\n    cy_new = K_new[1,2]\n    \n    scale_x = fx_new/fx\n    scale_y = fy_new/fy\n    \n    #resize to get the right focal length\n    width_resize = int(width*scale_x)\n    height_resize = int(height*scale_y)\n    # principal point position in the resized image\n    cx_resize = cx*scale_x\n    cy_resize = cy*scale_y\n    \n    img_resize = view.image.resize((width_resize, height_resize), Image.BILINEAR if scale_x > 1 else Image.LANCZOS)\n    if not view.depth is None:\n        max_depth    = np.max(view.depth)\n        depth_resize = view.depth / max_depth\n        depth_resize[depth_resize < 0.] = 0.\n        depth_resize = resize(depth_resize, (height_resize,width_resize), 0,mode=\'constant\') * max_depth\n    else:\n        depth_resize = None\n    \n    #crop to get the right principle point and resolution\n    x0 = int(round(cx_resize - cx_new))\n    y0 = int(round(cy_resize - cy_new))\n    x1 = x0 + int(width_new)\n    y1 = y0 + int(height_new)\n\n    if x0 < 0 or y0 < 0 or x1 > width_resize or y1 > height_resize:\n        print(\'Warning: Adjusting intrinsics adds a border to the image\')\n        img_new = safe_crop_image(img_resize,(x0,y0,x1,y1),(127,127,127))\n        if not depth_resize is None:\n            depth_new = safe_crop_array2d(depth_resize,(x0,y0,x1,y1),0).astype(np.float32)\n        else:\n            depth_new = None\n    else:\n        img_new = img_resize.crop((x0,y0,x1,y1))\n        if not depth_resize is None:\n            depth_new = depth_resize[y0:y1,x0:x1].astype(np.float32)\n        else:\n            depth_new = None\n    \n    return View(R=view.R, t=view.t, K=K_new, image=img_new, depth=depth_new, depth_metric=view.depth_metric)\n\n\ndef resize_view(view, width_new, height_new):\n    """"""Creates a new View with the new size.\n    The intrinsics will be adjusted to match the new image size\n    \n    view: View namedtuple\n        The view tuple\n\n    width_new: int\n        The new image width\n        \n    height_new: int\n        The new image height\n\n    Returns a View tuple with adjusted image, depth and intrinsics\n    """"""\n    from PIL import Image\n    from skimage.transform import resize\n\n    if view.image.width == width_new and view.image.height == height_new:\n        return View(*view)\n\n    #original param\n    fx = view.K[0,0]\n    fy = view.K[1,1]\n    cx = view.K[0,2]\n    cy = view.K[1,2]\n    width = view.image.width\n    height = view.image.height\n\n    #target param\n    fx_new = width_new*fx/width\n    fy_new = height_new*fy/height\n    cx_new = width_new*cx/width\n    cy_new = height_new*cy/height\n\n    K_new = np.array([fx_new, 0, cx_new, 0, fy_new, cy_new, 0, 0, 1],dtype=np.float64).reshape((3,3))\n\n    img_resize = view.image.resize((width_new, height_new), Image.BILINEAR if width_new > width else Image.LANCZOS)\n    max_depth = view.depth.max()\n    depth_resize = max_depth*resize(view.depth/max_depth, (height_new, width_new), order=0, mode=\'constant\')\n    depth_resize = depth_resize.astype(view.depth.dtype)\n    return View(R=view.R, t=view.t, K=K_new, image=img_resize, depth=depth_resize, depth_metric=view.depth_metric)\n\n\ndef compute_view_distances( views ):\n    """"""Computes the spatial distances between views\n\n    views: List of View namedtuple\n\n    Returns the spatial distance as distance matrix\n    """"""\n    from scipy.spatial.distance import pdist, squareform\n    positions = np.empty((len(views),3))\n    for i, view in enumerate(views):\n        C = -view.R.transpose().dot(view.t)\n        positions[i] = C\n    return squareform(pdist(positions,\'euclidean\'))\n\n\ndef compute_view_angle( view1, view2 ):\n    """"""Computes the viewing direction angle between two views\n\n    view1: View namedtuple\n        First view\n\n    view2: View namedtuple\n        Second view\n\n    Returns the angle in radians\n    """"""\n    dot = np.clip(view1.R[2,:].dot(view2.R[2,:]), -1, 1)\n    return np.arccos(dot)\n\n\ndef create_image_overview( views ):\n    """"""Creates a small overview image showing the RGB images of all views\n    \n    views: list of View  or  list of list of View\n\n    Returns a PIL.Image\n    """"""\n    assert isinstance(views, list)\n    from .helpers import concat_images_vertical, concat_images_horizontal\n    max_height = 100 # maximum height of individual images\n\n    def resize_image(img):\n        if img.size[1] > max_height:\n            new_width = int(img.size[0]*(max_height/img.size[1]))\n            return img.resize((new_width,max_height))\n        else:\n            return img\n\n    column_images = []\n    for col in views:\n        if isinstance(col,list):\n            tmp_images = []\n            for row in col:\n                tmp_images.append(resize_image(row.image))\n            col_img = concat_images_vertical(tmp_images)\n            column_images.append(col_img)\n        elif isinstance(col,View):\n            column_images.append(resize_image(col.image))\n    return concat_images_horizontal(column_images)\n\n\ndef visualize_views( views ):\n    """"""Visualizes views\n\n    views: list of View namedtuple\n\n    Opens a vtk window with the visualization\n    """"""\n    import vtk\n    from .. import vis\n\n\n    renderer = vtk.vtkRenderer()\n    renderer.SetBackground(0, 0, 0)\n\n    axes = vtk.vtkAxesActor()\n    axes.GetXAxisCaptionActor2D().SetHeight(0.05)\n    axes.GetYAxisCaptionActor2D().SetHeight(0.05)\n    axes.GetZAxisCaptionActor2D().SetHeight(0.05)\n    axes.SetCylinderRadius(0.03)\n    axes.SetShaftTypeToCylinder()\n    renderer.AddActor(axes)\n\n    renwin = vtk.vtkRenderWindow()\n    renwin.SetWindowName(""Viewer (press \'m\' to change colors, use \'.\' and \',\' to adjust opacity)"")\n    renwin.SetSize(800,600)\n    renwin.AddRenderer(renderer)\n    \n \n    # An interactor\n    interactor = vtk.vtkRenderWindowInteractor()\n    interstyle = vtk.vtkInteractorStyleTrackballCamera()\n    interactor.SetInteractorStyle(interstyle)\n    interactor.SetRenderWindow(renwin)\n\n    colors = ((1,0,0), (0,0,1), (0,1,1), (1,0,1), (1,1,0), (1,1,1), (0,1,0))\n \n    pointcloud_polydatas = []\n    pointcloud_actors = []\n    for idx, view in enumerate(views):\n\n        img_arr = None\n        if not view.image is None:\n            img_arr = np.array(view.image).transpose([2,0,1])\n\n\n        pointcloud = vis.compute_point_cloud_from_depthmap(view.depth, view.K, view.R, view.t, colors=img_arr)\n        pointcloud_polydata = vis.create_pointcloud_polydata( \n            points=pointcloud[\'points\'], \n            colors=pointcloud[\'colors\'] if \'colors\' in pointcloud else None,\n        )\n        pointcloud_polydatas.append(pointcloud_polydata)\n\n        pc_mapper = vtk.vtkPolyDataMapper()\n        pc_mapper.SetInputData(pointcloud_polydata)\n\n        pc_actor = vtk.vtkActor()\n        pointcloud_actors.append(pc_actor)\n        pc_actor.SetMapper(pc_mapper)\n        pc_actor.GetProperty().SetPointSize(2)\n        \n\n        color = colors[idx%len(colors)]\n\n        pc_actor.GetProperty().SetColor(*color)\n        renderer.AddActor(pc_actor)\n\n        cam_actor = vis.create_camera_actor(view.R,view.t)\n        cam_actor.GetProperty().SetColor(*color)\n        renderer.AddActor(cam_actor)\n\n\n\n    def change_point_properties(obj, ev):\n        if change_point_properties.current_active_scalars == ""Colors"":\n            change_point_properties.current_active_scalars = """"\n        else:\n            change_point_properties.current_active_scalars = ""Colors""\n\n        if ""m"" == obj.GetKeySym():\n            for polydata in pointcloud_polydatas:\n                polydata.GetPointData().SetActiveScalars(change_point_properties.current_active_scalars)\n\n        if ""period"" == obj.GetKeySym():\n            for actor in pointcloud_actors:\n                opacity = actor.GetProperty().GetOpacity()\n                opacity = min(1.0, opacity - 0.1)\n                \n                actor.GetProperty().SetOpacity(opacity)\n        if ""comma"" == obj.GetKeySym():\n            for actor in pointcloud_actors:\n                opacity = actor.GetProperty().GetOpacity()\n                opacity = max(0.0, opacity + 0.1)\n                actor.GetProperty().SetOpacity(opacity)\n        renwin.Render()\n            \n    change_point_properties.current_active_scalars = ""Colors""\n\n    interactor.AddObserver(\'KeyReleaseEvent\', change_point_properties)\n    \n    # Start\n    interactor.Initialize()\n    interactor.Start()\n\n    interactor.RemoveAllObservers()\n    del change_point_properties\n    \n\n'"
python/depthmotionnet/dataset_tools/webp.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom ctypes import *\nfrom PIL import Image\nimport numpy as np\nimport os\n\n# try the version used by the multivih5datareaderop first\ntry:\n    _lib_path = os.path.abspath(os.path.join(os.path.split(__file__)[0], \'..\', \'..\', \'..\', \'build\',\'webp\',\'src\',\'webp-build\', \'src\', \'.libs\', \'libwebp.so\'))\n    libwebp = CDLL(_lib_path)\nexcept:\n    # try system version\n    try:\n        libwebp = CDLL(\'libwebp.so\')\n    except:\n        raise RuntimeError(\'Cannot load libwebp.so\')\n\ndef webp_encode_array(array, quality=90.0):\n    """"""encode the array as webp and return as bytes.\n\n    array: uint8 numpy array\n        array with the following shape [height,width,3] or [3,height,width]\n\n    Returns the compressed bytes array or None on error\n    """"""\n    assert isinstance(array, np.ndarray), ""array must be a numpy array""\n    assert array.dtype == np.uint8, ""array must be a uint8 array""\n    assert len(array.shape) == 3, ""array must be a 3d array""\n    assert array.shape[0] == 3 or array.shape[-1] == 3, ""array must have 3 color channels""\n    \n    if array.shape[0] != array.shape[-1] and array.shape[0] == 3:\n        array_rgb = array.transpose([2,0,1])\n    else:\n        array_rgb = array\n    data = array_rgb.tobytes()\n\n    width = c_int(array_rgb.shape[1])\n    height = c_int(array_rgb.shape[0])\n    stride = c_int(array_rgb.shape[1]*3)\n    output = POINTER(c_char)()\n    size = libwebp.WebPEncodeRGB(data, width, height, stride, c_float(quality), pointer(output))\n    if size == 0:\n        return None\n\n    webp_img = output[:size]\n    libwebp.WebPFree(output)\n    # libc.free(output)\n    return webp_img\n\n    \n\n\ndef webp_encode_image(image):\n    """"""encode the image as webp and return as bytes\n\n    image: PIL.Image\n        Image to encode\n    """"""\n    arr = np.array(image)\n    return webp_encode_array(arr)\n'"
python/depthmotionnet/evaluation/__init__.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom .evaluate_to_xarray import evaluate, write_xarray_json, read_xarray_json\n\n'"
python/depthmotionnet/evaluation/evaluate_to_xarray.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom .metrics import compute_motion_errors,evaluate_depth,compute_flow_epe\nimport h5py\nimport xarray\nimport numpy as np\nimport re\nimport math\nimport json\nimport scipy.misc\nimport time\nimport skimage.transform\n\n\'\'\'\nFunctions to evaluate DeMoN results stored as hdf5 files. The results are stored as xarray DataArray converted to json\n\'\'\'\n\ndef write_xarray_json(data, out_file):\n    """"""Writes xarray as json to a file""""""\n    with open(out_file, \'w\') as f:\n        json.dump(data.to_dict(), f)\n        \ndef read_xarray_json(in_file):\n    """"""Reads xarray from a json file""""""\n    with open(in_file, \'r\') as f:\n        return xarray.DataArray.from_dict(json.load(f))\n           \ndef get_metainfo(data_file):\n    """"""Checks a hdf5 data file for its format and dimensions. \n\n    data_file: str\n        Path to the hdf5 file generated with the test_iterative.py script.\n\n    returns a dictionary with the following keys:\n        iterative:  bool, if the file is from an iterative net\n        snapshots:  list of str, names od snapshots in the file contain\n        iterations: list of net_iterations\n        samples:    list of samples\n    """"""\n    \n    re_iteration = re.compile(\'.*_(\\d+)(\\.caffemodel\\.h5)?\')\n    with h5py.File(data_file,\'r\') as f:\n        group_name = list(f.keys())[0]\n        iterative_net = bool(re_iteration.match(group_name))\n        if iterative_net:\n            snapshots = list(f.keys())\n            snapshots.sort(key=lambda x: int(re_iteration.match(x).group(1)))\n            snapshot_iters = [int(re_iteration.match(x).group(1)) for x in snapshots]\n            snapshot_group = f[snapshots[0]]\n            samples = list(snapshot_group.keys())\n            samples.sort(key=int)\n            sample_group = snapshot_group[samples[0]]\n            # collect iterations from all prediction datasets\n            iterations = set()\n            for prediction in (\'predicted_depth\', \'predicted_normal\', \'predicted_motion\', \'predicted_flow\', \'predicted_conf\'):\n                if prediction in sample_group:\n                    iterations.update( list(sample_group[prediction]) )\n            iterations = list(iterations)\n            iterations.sort(key=lambda x: (int(x.split(\'_\')[0]),len(x.split(\'_\'))))\n        else:\n            snapshots = [\'snapshot\']\n            snapshot_iters = [-1]\n            iterations = [\'0\']\n            samples = list(f.keys())\n            samples.sort(key=int)\n\n    metainfo = {\n            \'iterative\':iterative_net, \n            \'snapshots\': snapshots, \n            \'iterations\': iterations, \n            \'samples\':samples, \n            \'snapshot_iters\': snapshot_iters, \n            \'input_file\': data_file,\n            }\n    return metainfo\n\n\ndef invalidate_points_not_visible_in_second_image(depth, motion, intrinsics):\n    """"""Sets the depth values for the points not visible in the second view to nan\n\n    depth: numpy.ndarray\n        array with inverse depth values as stored in the test output h5 files\n\n    motion: numpy.ndarray\n        the 6 element motion vector (ANGLEAXIS6)\n\n    intrinsics: numpy.ndarray or None\n        the normalized intrinsics vector\n        If None we assume intrinsics as in sun3d\n    """"""\n    from .helpers import motion_vector_to_Rt, intrinsics_vector_to_K\n    from ..dataset_tools.view import View\n    from ..dataset_tools.view_tools import compute_visible_points_mask\n    #from matplotlib import pyplot as plt\n    abs_depth = 1/depth\n    R, t = motion_vector_to_Rt(motion.squeeze())\n\n    if intrinsics is None:\n        intrinsics = np.array([[0.891, 1.188, 0.5, 0.5]], dtype=np.float32) # sun3d intrinsics\n    intrinsics = intrinsics.squeeze()\n    K = intrinsics_vector_to_K(intrinsics, depth.shape[-1], depth.shape[-2])\n    view1 = View(R=np.eye(3), t=np.zeros((3,)), K=K, image=None, depth=abs_depth, depth_metric=\'camera_z\')\n    view2 = View(R=R, t=t, K=K, image=None, depth=abs_depth, depth_metric=\'camera_z\')\n    invalid_points = compute_visible_points_mask(view1, view2) == 0\n    # tmp = depth.copy()\n    depth[invalid_points] = np.nan\n    # plt.imshow(np.concatenate((tmp,depth),axis=1))\n    # plt.show(block=True)\n    \n        \n\n\n\ndef get_data(iterative, results_h5_file, snap, sample, net_iter, gt_h5_file=None, depthmask=False, eigen_crop_gt_and_pred=False):\n    """"""Helper function to read data from the h5 files\n    \n    iterative: bool\n        If true the hdf5 file stores results from multiple iterations.\n\n    results_h5_file: h5py.File\n        The file with the network predictions\n\n    snap: str\n        Name of the snapshot\n\n    sample: str\n        Sample number as string\n\n    net_iter: int\n        network iteration\n\n    gt_h5_file: h5py.File\n        ground truth h5 file.\n\n    depthmask: bool\n        If True the depth values for points not visible in the second image will be masked out\n        \n    eigen_crop_gt_and_pred: bool\n        If true crops images and depth maps to match the evaluation for NYU in Eigen\'s paper.\n\n    Returns a dictionary with ground truth and predictions for depth, motion and flow.\n    """"""\n    data_types = [\'motion\', \'depth\', \'flow\', \'normals\', \'intrinsics\']\n    data = {}\n    # get ground truth\n    if iterative and (gt_h5_file is None):\n        sample_group = results_h5_file[snap][sample]\n    else:\n        if gt_h5_file is None:\n            sample_group = results_h5_file[sample]\n        else:\n            sample_group = gt_h5_file[sample]\n            gt_sample_id = sample_group.attrs[\'sample_id\']\n\n    for dt in data_types:\n        if dt in sample_group:\n            data[dt + \'_gt\'] = sample_group[dt][:]    \n            \n    # get predictions\n    if iterative:\n        sample_group = results_h5_file[snap][sample]\n        pr_sample_id = sample_group.attrs[\'sample_id\']\n        assert gt_sample_id == pr_sample_id, ""sample ids do not match: prediction id=\'{0}\', ground truth id=\'{1}\'"".format(pr_sample_id,gt_sample_id)\n        for dt in data_types:\n            if \'predicted_{0}/{1}\'.format(dt,net_iter) in sample_group:\n                data[dt + \'_pred\'] = sample_group[\'predicted_\'+dt][net_iter][:]\n    else:\n        sample_group = results_h5_file[sample]\n        for dt in data_types:\n            if (\'predicted_\'+dt) in sample_group:\n                data[dt + \'_pred\'] = sample_group[\'predicted_\'+dt][:]\n        \n    for key in data:    \n        data[key] = np.squeeze(data[key])\n        \n    if (\'depth_pred\' in data) and (data[\'depth_pred\'].shape == (109,147)):\n        print(\'\\n >>> Eigen and Fergus detected, cropping the ground truth <<<\\n\')\n        assert(data[\'depth_gt\'].shape == (480,640))\n        data[\'depth_gt\'] = data[\'depth_gt\'][23:23+436,27:27+588]\n        \n    if depthmask and (\'motion_gt\' in data) and (\'depth_gt\' in data):\n        intrinsics = data[\'intrinsics\'] if \'intrinsics\' in data else None\n        invalidate_points_not_visible_in_second_image(data[\'depth_gt\'], data[\'motion_gt\'], intrinsics)\n    \n    # reshape the predictions to GT size if necessary\n    if (\'depth_gt\' in data) and (\'depth_pred\' in data) and (not (data[\'depth_gt\'].shape == data[\'depth_pred\'].shape)):\n        data[\'depth_pred\'] = skimage.transform.resize(data[\'depth_pred\'], data[\'depth_gt\'].shape, order=0, mode=\'constant\', preserve_range=True)\n    if (\'flow_gt\' in data) and (\'flow_pred\' in data) and (not (data[\'flow_gt\'].shape == data[\'flow_pred\'].shape)):\n        data[\'flow_pred\'] = np.transpose(skimage.transform.resize(\\\n                                np.transpose(data[\'flow_pred\'],(1,2,0)), data[\'depth_gt\'].shape, order=0, mode=\'constant\', preserve_range=True),(2,0,1))\n        \n    if eigen_crop_gt_and_pred and data[\'depth_gt\'].shape != (436,588):\n        assert(data[\'depth_gt\'].shape == (480,640))\n        assert(data[\'depth_pred\'].shape == (480,640))\n        data[\'depth_gt\'] = data[\'depth_gt\'][23:23+436,27:27+588]\n        data[\'depth_pred\'] = data[\'depth_pred\'][23:23+436,27:27+588]\n    \n    return data\n        \n\ndef evaluate(results_file, gt_file, depthmask=False, eigen_crop_gt_and_pred=False, depth_scaling=\'abs\'):\n    \'\'\'\n    Compute different error measures given a hdf5 result (prediction) file, and output them as an xarray.\n    results_file: str\n        Path to the network results (prediction) in hdf5 format.\n\n    gt_file: str\n        Path to the hdf5 file with ground truth data stored in the simple test output format\n\n    depthmask: bool\n        If True the depth values for points not visible in the second image will be masked out\n\n    eigen_crop_gt_and_pred: bool\n        If true crops images and depth maps to match the evaluation for NYU in Eigen\'s paper.\n\n    depth_scaling: str\n        selects a scaling method for the scaled results. E.g. \'abs\' scales such that the \n        least squares error for the absolute depth values is minimized.\n        \n    \'\'\'\n    depth_pred_max=np.inf\n\n    depth_errors_to_compute = [\'l1\',\n                               \'l1_inverse\',\n                               \'scale_invariant\',\n                               \'abs_relative\',\n                               \'sq_relative\',\n                               \'avg_log10\',\n                               \'rmse_log\',\n                               \'rmse\',\n                               \'ratio_threshold_1.25\',\n                               \'ratio_threshold_1.5625\',\n                               \'ratio_threshold_1.953125\']\n    \n    errors_to_compute = [\'rot_err\', \'tran_err\', \'tran_angle_err\'] + \\\n                       [\'depth_\' + e for e in depth_errors_to_compute] + \\\n                       [\'flow_epe\', \'camera_baseline\']\n    \n    metainfo = get_metainfo(results_file)\n    results = xarray.DataArray(np.zeros((len(metainfo[\'snapshots\']), len(metainfo[\'iterations\']), len(metainfo[\'samples\']), len(errors_to_compute), 2)), \n                             [(\'snapshot\', metainfo[\'snapshots\']),\n                              (\'iteration\', metainfo[\'iterations\']),\n                              (\'sample\', metainfo[\'samples\']),\n                              (\'errors\', errors_to_compute),\n                              (\'scaled\', [False,True])])\n    results[:] = np.nan\n    \n    # save metainfo and evaluation options\n    for key,val in metainfo.items():\n        results.attrs[key] = val\n    results.attrs[\'gt_file\'] = gt_file\n    results.attrs[\'depthmask\'] = depthmask\n    results.attrs[\'depth_scaling\'] = depth_scaling\n    results.attrs[\'depth_pred_max\'] = str(depth_pred_max)\n\n           \n    with h5py.File(results_file,\'r\') as results_f:\n        if gt_file:\n            gt_f = h5py.File(gt_file,\'r\')\n        else:\n            gt_f = None\n\n        t0 = 0\n        for nsnap,snap in enumerate(metainfo[\'snapshots\']):\n            for nsample,sample in enumerate(metainfo[\'samples\']):            \n                for niter,net_iter in enumerate(metainfo[\'iterations\']):\n                    if time.time() - t0 > 5:\n                        t0 = time.time()\n                        print(\'Processing snapshot %d/%d. sample %d/%d\' % \\\n                                    (nsnap+1, len(metainfo[\'snapshots\']), nsample+1, len(metainfo[\'samples\'])))\n                    data = get_data(metainfo[\'iterative\'], results_f, snap, sample, net_iter, gt_h5_file=gt_f, depthmask=depthmask, eigen_crop_gt_and_pred=eigen_crop_gt_and_pred)\n                    \n                    if (\'depth_gt\' in data) and (\'depth_pred\' in data): \n                        #print(data[\'depth_pred\'].dtype, data[\'depth_pred\'][:3,:3], data[\'depth_gt\'].dtype, data[\'depth_gt\'][:3,:3])\n                        if \'motion_gt\' in data and (not np.any(np.isnan(data[\'motion_gt\']))):\n                            translation_gt = data[\'motion_gt\'][-3:]\n                            results.loc[snap,net_iter,sample,\'camera_baseline\'] = np.linalg.norm(translation_gt)\n                        else:\n                            translation_gt = np.array([1.,0.,0.])                    \n                        depth_errs, depth_errs_pred_scaled = evaluate_depth(translation_gt, data[\'depth_gt\'], data[\'depth_pred\'], \n                                                                distances_to_compute=depth_errors_to_compute, inverse_gt=True, inverse_pred=True, \n                                                                depth_scaling=depth_scaling, depth_pred_max=depth_pred_max)\n                    \n                        for dist in depth_errors_to_compute:\n                            results.loc[snap,net_iter,sample,\'depth_\' + dist,False] = depth_errs[dist]\n                            results.loc[snap,net_iter,sample,\'depth_\' + dist,True] = depth_errs_pred_scaled[dist]\n                    \n                    if (\'motion_gt\' in data) and (\'motion_pred\' in data):\n                        normalize_translation = True\n                        rot_err, tran_err, tran_angle_err = compute_motion_errors(data[\'motion_pred\'], data[\'motion_gt\'], normalize_translation)\n                        results.loc[snap,net_iter,sample,\'rot_err\'] = rot_err\n                        results.loc[snap,net_iter,sample,\'tran_err\'] = tran_err\n                        results.loc[snap,net_iter,sample,\'tran_angle_err\'] = tran_angle_err\n                    \n                    if (\'flow_gt\' in data) and (\'flow_pred\' in data):\n                        flow_epe = compute_flow_epe(data[\'flow_pred\'],data[\'flow_gt\'])\n                        results.loc[snap,net_iter,sample,\'flow_epe\'] = flow_epe\n        if gt_file:\n            gt_f.close()\n             \n    return results\n    \n        \n\n\n'"
python/depthmotionnet/evaluation/helpers.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport numpy as np\nfrom minieigen import Matrix3, Vector3, Vector2, Quaternion\n\n\ndef angleaxis_to_angle_axis(aa, epsilon=1e-6):\n    """"""Converts the angle axis vector with angle encoded as magnitude to \n    the angle axis representation with seperate angle and axis.\n\n    aa: minieigen.Vector3\n        axis angle with angle as vector magnitude\n\n    epsilon: minimum angle in rad\n        If the angle is smaller than epsilon\n        then 0,(1,0,0) will be returned\n\n    returns the tuple (angle,axis)\n    """"""\n    angle = aa.norm()\n    if angle < epsilon:\n        angle = 0\n        axis = Vector3(1,0,0)\n    else:\n        axis = aa.normalized()\n    return angle, axis\n\n\ndef angleaxis_to_quaternion(aa, epsilon=1e-6):\n    """"""Converts the angle axis vector with angle encoded as magnitude to \n    the quaternion representation.\n\n    aa: minieigen.Vector3\n        axis angle with angle as vector magnitude\n\n    epsilon: minimum angle in rad\n        If the angle is smaller than epsilon\n        then 0,(1,0,0) will be returned\n\n    returns the unit quaternion\n    """"""\n    angle, axis = angleaxis_to_angle_axis(aa,epsilon)\n    return Quaternion(angle,axis)\n\n\n\ndef angleaxis_to_rotation_matrix(aa, epsilon=1e-6):\n    """"""Converts the angle axis vector with angle encoded as magnitude to \n    the rotation matrix representation.\n\n    aa: minieigen.Vector3\n        axis angle with angle as vector magnitude\n\n    epsilon: minimum angle in rad\n        If the angle is smaller than epsilon\n        then 0,(1,0,0) will be returned\n\n    returns the 3x3 rotation matrix as numpy.ndarray\n    """"""\n    q = angleaxis_to_quaternion(aa,epsilon)\n    tmp = q.toRotationMatrix()\n    return np.array(tmp)\n\n\n\ndef motion_vector_to_Rt(motion, epsilon=1e-6):\n    """"""Converts the motion vector to the rotation matrix R and translation t\n\n    motion: np.ndarray\n        array with 6 elements. The motions is given as [aa1, aa2, aa3, tx, ty, tz].\n        aa1,aa2,aa3 is an angle axis representation. The angle is the norm of the axis.\n        [tx, ty, tz] is a 3d translation.\n\n\n    epsilon: minimum angle in rad\n        If the angle is smaller than epsilon\n        then 0,(1,0,0) will be returned\n\n    returns the 3x3 rotation matrix and the 3d translation vector\n    """"""\n    pass\n    tmp = motion.squeeze().astype(np.float64)\n    t = tmp[3:].copy()\n    R = angleaxis_to_rotation_matrix(Vector3(tmp[0:3]),epsilon)\n    return R, t\n\n\ndef intrinsics_vector_to_K(intrinsics, width, height):\n    """"""Converts the normalized intrinsics vector to the calibration matrix K\n\n    intrinsics: np.ndarray\n        4 element vector with normalized intrinsics [fx, fy, cx, cy]\n\n    width: int\n        image width in pixels\n\n    height: int \n        image height in pixels\n\n    returns the calibration matrix K as numpy.ndarray\n    """"""\n    tmp = intrinsics.squeeze().astype(np.float64)\n    K = np.array([tmp[0]*width, 0, tmp[2]*width, 0, tmp[1]*height, tmp[3]*height, 0, 0, 1], dtype=np.float64).reshape((3,3))\n    \n    return K\n'"
python/depthmotionnet/evaluation/metrics.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport numpy as np\nimport math\nfrom minieigen import Quaternion, Vector3\n\n# implements error metrics from Eigen et al. https://arxiv.org/pdf/1406.2283.pdf\n\ndef compute_valid_depth_mask(d1, d2=None):\n    """"""Computes the mask of valid values for one or two depth maps\n    \n    Returns a valid mask that only selects values that are valid depth value \n    in both depth maps (if d2 is given).\n    Valid depth values are >0 and finite.\n    """"""\n    if d2 is None:\n        valid_mask = np.isfinite(d1)\n        valid_mask[valid_mask] = (d1[valid_mask] > 0)\n    else:\n        valid_mask = np.isfinite(d1) & np.isfinite(d2)\n        valid_mask[valid_mask] = (d1[valid_mask] > 0) & (d2[valid_mask] > 0)\n    return valid_mask\n\n\ndef l1(depth1,depth2):\n    """"""\n    Computes the l1 errors between the two depth maps.\n    Takes preprocessed depths (no nans, infs and non-positive values)\n\n    depth1:  one depth map\n    depth2:  another depth map\n\n    Returns: \n        L1(log)\n\n    """"""\n    assert(np.all(np.isfinite(depth1) & np.isfinite(depth2) & (depth1 > 0) & (depth2 > 0)))\n    diff = depth1 - depth2\n    num_pixels = float(diff.size)\n    \n    if num_pixels == 0:\n        return np.nan\n    else:\n        return np.sum(np.absolute(diff)) / num_pixels\n    \n\ndef l1_inverse(depth1,depth2):\n    """"""\n    Computes the l1 errors between inverses of two depth maps.\n    Takes preprocessed depths (no nans, infs and non-positive values)\n\n    depth1:  one depth map\n    depth2:  another depth map\n\n    Returns: \n        L1(log)\n\n    """"""\n    assert(np.all(np.isfinite(depth1) & np.isfinite(depth2) & (depth1 > 0) & (depth2 > 0)))\n    diff = np.reciprocal(depth1) - np.reciprocal(depth2)\n    num_pixels = float(diff.size)\n    \n    if num_pixels == 0:\n        return np.nan\n    else:\n        return np.sum(np.absolute(diff)) / num_pixels\n\n\ndef rmse_log(depth1,depth2):\n    """"""\n    Computes the root min square errors between the logs of two depth maps.\n    Takes preprocessed depths (no nans, infs and non-positive values)\n\n    depth1:  one depth map\n    depth2:  another depth map\n\n    Returns: \n        RMSE(log)\n\n    """"""\n    assert(np.all(np.isfinite(depth1) & np.isfinite(depth2) & (depth1 > 0) & (depth2 > 0)))\n    log_diff = np.log(depth1) - np.log(depth2)\n    num_pixels = float(log_diff.size)\n    \n    if num_pixels == 0:\n        return np.nan\n    else:\n        return np.sqrt(np.sum(np.square(log_diff)) / num_pixels)\n    \n\ndef rmse(depth1,depth2):\n    """"""\n    Computes the root min square errors between the two depth maps.\n    Takes preprocessed depths (no nans, infs and non-positive values)\n\n    depth1:  one depth map\n    depth2:  another depth map\n\n    Returns: \n        RMSE(log)\n\n    """"""\n    assert(np.all(np.isfinite(depth1) & np.isfinite(depth2) & (depth1 > 0) & (depth2 > 0)))\n    diff = depth1 - depth2\n    num_pixels = float(diff.size)\n    \n    if num_pixels == 0:\n        return np.nan\n    else:\n        return np.sqrt(np.sum(np.square(diff)) / num_pixels)\n    \n\ndef scale_invariant(depth1,depth2):\n    """"""\n    Computes the scale invariant loss based on differences of logs of depth maps.\n    Takes preprocessed depths (no nans, infs and non-positive values)\n\n    depth1:  one depth map\n    depth2:  another depth map\n\n    Returns: \n        scale_invariant_distance\n\n    """"""\n    # sqrt(Eq. 3)\n    assert(np.all(np.isfinite(depth1) & np.isfinite(depth2) & (depth1 > 0) & (depth2 > 0)))\n    log_diff = np.log(depth1) - np.log(depth2)\n    num_pixels = float(log_diff.size)\n    \n    if num_pixels == 0:\n        return np.nan\n    else:\n        return np.sqrt(np.sum(np.square(log_diff)) / num_pixels - np.square(np.sum(log_diff)) / np.square(num_pixels))\n\n\ndef abs_relative(depth_pred,depth_gt):\n    """"""\n    Computes relative absolute distance.\n    Takes preprocessed depths (no nans, infs and non-positive values)\n\n    depth_pred:  depth map prediction\n    depth_gt:    depth map ground truth\n\n    Returns: \n        abs_relative_distance\n\n    """"""\n    assert(np.all(np.isfinite(depth_pred) & np.isfinite(depth_gt) & (depth_pred > 0) & (depth_gt > 0)))\n    diff = depth_pred - depth_gt\n    num_pixels = float(diff.size)\n\n    if num_pixels == 0:\n        return np.nan\n    else:\n        return np.sum(np.absolute(diff) / depth_gt) / num_pixels\n\n\ndef avg_log10(depth1,depth2):\n    """"""\n    Computes average log_10 error (Liu, Neural Fields, 2015).\n    Takes preprocessed depths (no nans, infs and non-positive values)\n\n    depth1:  one depth map\n    depth2:  another depth map\n\n    Returns:\n        abs_relative_distance\n\n    """"""\n    assert(np.all(np.isfinite(depth1) & np.isfinite(depth2) & (depth1 > 0) & (depth2 > 0)))\n    log_diff = np.log10(depth1) - np.log10(depth2)\n    num_pixels = float(log_diff.size)\n\n    if num_pixels == 0:\n        return np.nan\n    else:\n        return np.sum(np.absolute(log_diff)) / num_pixels\n    \n\ndef sq_relative(depth_pred,depth_gt):\n    """"""\n    Computes relative squared distance.\n    Takes preprocessed depths (no nans, infs and non-positive values)\n\n    depth_pred:  depth map prediction\n    depth_gt:    depth map ground truth\n\n    Returns: \n        squared_relative_distance\n\n    """"""\n    assert(np.all(np.isfinite(depth_pred) & np.isfinite(depth_gt) & (depth_pred > 0) & (depth_gt > 0)))\n    diff = depth_pred - depth_gt\n    num_pixels = float(diff.size)\n    \n    if num_pixels == 0:\n        return np.nan\n    else:\n        return np.sum(np.square(diff) / depth_gt) / num_pixels\n    \n\ndef ratio_threshold(depth1, depth2, threshold):\n    """"""\n    Computes the percentage of pixels for which the ratio of the two depth maps is less than a given threshold.\n    Takes preprocessed depths (no nans, infs and non-positive values)\n\n    depth1:  one depth map\n    depth2:  another depth map\n\n    Returns: \n        percentage of pixels with ratio less than the threshold\n\n    """"""\n    assert(threshold > 0.)\n    assert(np.all(np.isfinite(depth1) & np.isfinite(depth2) & (depth1 > 0) & (depth2 > 0)))\n    log_diff = np.log(depth1) - np.log(depth2)\n    num_pixels = float(log_diff.size)\n    \n    if num_pixels == 0:\n        return np.nan\n    else:\n        return float(np.sum(np.absolute(log_diff) < np.log(threshold))) / num_pixels\n    \n\ndef compute_errors(depth_pred, depth_gt, distances_to_compute=None):\n    """"""\n    Computes different distance measures between two depth maps.\n\n    depth_pred:           depth map prediction\n    depth_gt:             depth map ground truth\n    distances_to_compute: which distances to compute\n\n    Returns: \n        a dictionary with computed distances, and the number of valid pixels\n\n    """"""\n\n    valid_mask = compute_valid_depth_mask(depth_pred, depth_gt)\n    depth_pred = depth_pred[valid_mask]\n    depth_gt   = depth_gt[valid_mask]\n    num_valid  = np.sum(valid_mask)\n    \n    if distances_to_compute is None:\n        distances_to_compute = [\'l1\',\n                                \'l1_inverse\',\n                                \'scale_invariant\',\n                                \'abs_relative\',\n                                \'sq_relative\',\n                                \'avg_log10\',\n                                \'rmse_log\',\n                                \'rmse\',\n                                \'ratio_threshold_1.25\',\n                                \'ratio_threshold_1.5625\',\n                                \'ratio_threshold_1.953125\']\n    \n    results = {\'num_valid\': num_valid}\n    for dist in distances_to_compute:\n        if dist.startswith(\'ratio_threshold\'):\n            threshold = float(dist.split(\'_\')[-1])\n            results[dist] = ratio_threshold(depth_pred,depth_gt,threshold)\n        else:\n            results[dist] = globals()[dist](depth_pred,depth_gt)\n        \n    return results\n\n\ndef compute_depth_scale_factor(depth1, depth2, depth_scaling=\'abs\'):\n    """"""\n    Computes the scale factor for depth1 to minimize the least squares error to depth2\n    """"""\n    \n    assert(np.all(np.isfinite(depth1) & np.isfinite(depth2) & (depth1 > 0) & (depth2 > 0)))\n    \n    \n    if depth_scaling == \'abs\':\n        # minimize MSE on depth\n        d1d1 = np.multiply(depth1,depth1)\n        d1d2 = np.multiply(depth1,depth2)\n        mask = compute_valid_depth_mask(d1d2)\n        sum_d1d1 = np.sum(d1d1[mask])\n        sum_d1d2 = np.sum(d1d2[mask])\n        if sum_d1d1 > 0.:\n            scale = sum_d1d2/sum_d1d1\n        else:\n            print(\'compute_depth_scale_factor: Norm=0 during scaling\')\n            scale = 1.\n    elif depth_scaling == \'log\':    \n        # minimize MSE on log depth\n        log_diff = np.log(depth2) - np.log(depth1)\n        scale = np.exp(np.mean(log_diff))\n    elif depth_scaling == \'inv\':    \n        # minimize MSE on inverse depth\n        d1d1 = np.multiply(np.reciprocal(depth1),np.reciprocal(depth1))\n        d1d2 = np.multiply(np.reciprocal(depth1),np.reciprocal(depth2))\n        mask = compute_valid_depth_mask(d1d2)\n        sum_d1d1 = np.sum(d1d1[mask])\n        sum_d1d2 = np.sum(d1d2[mask])\n        if sum_d1d1 > 0.:\n            scale = np.reciprocal(sum_d1d2/sum_d1d1)\n        else:\n            print(\'compute_depth_scale_factor: Norm=0 during scaling\')\n            scale = 1.\n    else:\n        raise Exception(\'Unknown depth scaling method\')\n        \n    return scale\n\n\ndef evaluate_depth( translation_gt, depth_gt_in, depth_pred_in, \n    distances_to_compute=None, inverse_gt=True, inverse_pred=True, \n    depth_scaling=\'abs\', depth_pred_max=np.inf ):\n    """"""\n    Computes different error measures for the inverse depth map without scaling and with scaling.\n\n    translation_gt: 1d numpy array with [tx,ty,tz]\n        The translation that corresponds to the ground truth depth\n\n    depth_gt: 2d numpy array\n        This is the ground truth depth\n        \n    depth_pred: depth prediction being evaluated\n    \n    distances_to_compute: which distances to compute\n\n    returns (err, err_after_scaling)\n        errs is the dictionary of errors without optimally scaling the prediction\n\n        errs_pred_scaled is the dictionary of errors after minimizing \n        the least squares error by scaling the prediction\n    """"""\n    \n    valid_mask = compute_valid_depth_mask(depth_pred_in, depth_gt_in)\n    depth_pred = depth_pred_in[valid_mask]\n    depth_gt   = depth_gt_in[valid_mask]\n    if inverse_gt:\n        depth_gt   = np.reciprocal(depth_gt)\n    if inverse_pred:\n        depth_pred = np.reciprocal(depth_pred)\n    \n    #if depth_pred_max < np.inf:\n        #depth_pred[depth_pred>depth_pred_max] = depth_pred_max\n    \n    # we need to scale the ground truth depth if the translation is not normalized\n    translation_norm = np.sqrt(translation_gt.dot(translation_gt))\n    scale_gt_depth = not np.isclose(1.0, translation_norm)\n    if scale_gt_depth:\n        depth_gt_scaled = depth_gt / translation_norm\n    else:\n        depth_gt_scaled = depth_gt\n            \n    errs = compute_errors(depth_pred,depth_gt_scaled,distances_to_compute)\n            \n    # minimize the least squares error and compute the errors again\n    scale = compute_depth_scale_factor(depth_pred,depth_gt_scaled, depth_scaling=depth_scaling)\n    depth_pred_scaled = depth_pred*scale\n            \n    errs_pred_scaled = compute_errors(depth_pred_scaled,depth_gt_scaled,distances_to_compute)\n            \n    return errs, errs_pred_scaled\n\n\ndef compute_flow_epe(flow1, flow2):\n    """"""Computes the average endpoint error between the two flow fields""""""\n    diff = flow1 - flow2\n    epe = np.sqrt(diff[0,:,:]**2 + diff[1,:,:]**2)\n    # mask out invalid epe values\n    valid_mask = compute_valid_depth_mask(epe) \n    epe = epe[valid_mask]\n    if epe.size > 0:\n        return np.mean(epe)\n    else:\n        return np.nan\n\n\ndef compute_motion_errors(predicted_motion, gt_motion, normalize_translations):\n    """"""\n    Computes the error of the motion.\n\n    predicted_motion: 1d numpy array with 6 elements\n        the motions as [aa1, aa2, aa3, tx, ty, tz]\n        aa1,aa2,aa3 is an angle axis representation.\n        The angle is the norm of the axis\n\n    gt_motion: 1d numpy array with 6 elements\n        ground truth motion in the same format as the predicted motion\n\n    normalize_translations: bool\n        If True then translations will be normalized before computing the error\n\n    Returns\n     rotation angular distance in radian\n     tranlation distance of the normalized translations\n     tranlation angular distance in radian\n    """"""\n    def _numpy_to_Vector3(arr):\n        tmp = arr.astype(np.float64)\n        return Vector3(tmp[0],tmp[1],tmp[2])\n\n    gt_axis = _numpy_to_Vector3(gt_motion[0:3])\n    gt_angle = gt_axis.norm()\n    if gt_angle < 1e-6:\n        gt_angle = 0\n        gt_axis = Vector3(1,0,0)\n    else:\n        gt_axis.normalize()\n    gt_q = Quaternion(gt_angle,gt_axis)\n\n    predicted_axis = _numpy_to_Vector3(predicted_motion[0:3])\n    predicted_angle = predicted_axis.norm()\n    if predicted_angle < 1e-6:\n        predicted_angle = 0\n        predicted_axis = Vector3(1,0,0)\n    else:\n        predicted_axis.normalize()\n    predicted_axis.normalize()\n    predicted_q =  Quaternion(predicted_angle,predicted_axis)\n\n    rotation_angle_dist = gt_q.angularDistance(predicted_q)\n    \n    gt_trans = _numpy_to_Vector3(gt_motion[3:6])\n    if normalize_translations:\n        gt_trans.normalize()\n    predicted_trans = _numpy_to_Vector3(predicted_motion[3:6])\n    if normalize_translations and predicted_trans.norm() > 1e-6:\n        predicted_trans.normalize()\n    translation_dist = (gt_trans-predicted_trans).norm()\n    \n    translation_angle_diff = math.acos(np.clip(gt_trans.dot(predicted_trans),-1,1))\n    \n    return np.rad2deg(rotation_angle_dist), translation_dist, np.rad2deg(translation_angle_diff)\n\n\n'"
python/depthmotionnet/v2/__init__.py,0,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n\n'"
python/depthmotionnet/v2/blocks.py,42,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport os, sys\nimport tensorflow as tf\nfrom .helpers import *\nimport lmbspecialops as sops\n\n\ndef _predict_flow(inp, predict_confidence=False, **kwargs ):\n    """"""Generates a tensor for optical flow prediction\n    \n    inp: Tensor\n\n    predict_confidence: bool\n        If True the output tensor has 4 channels instead of 2.\n        The last two channels are the x and y flow confidence.\n    """"""\n\n    tmp = convrelu(\n        inputs=inp,\n        num_outputs=24,\n        kernel_size=3,\n        strides=1,\n        name=""conv1"",\n        **kwargs,\n    )\n    \n    output = conv2d(\n        inputs=tmp,\n        num_outputs=4 if predict_confidence else 2,\n        kernel_size=3,\n        strides=1,\n        name=""conv2"",\n        **kwargs,\n    )\n    \n    return output\n\n\ndef _upsample_prediction(inp, num_outputs, **kwargs ):\n    """"""Upconvolution for upsampling predictions\n    \n    inp: Tensor \n        Tensor with the prediction\n        \n    num_outputs: int\n        Number of output channels. \n        Usually this should match the number of channels in the predictions\n    """"""\n    output = tf.layers.conv2d_transpose(\n        inputs=inp,\n        filters=num_outputs,\n        kernel_size=4,\n        strides=2,\n        padding=\'same\',\n        activation=None,\n        kernel_initializer=default_weights_initializer(),\n        name=""upconv"",\n        **kwargs,\n    )\n    return output\n\n\n\ndef _refine(inp, num_outputs, data_format, upsampled_prediction=None, features_direct=None, **kwargs):\n    """""" Generates the concatenation of \n         - the previous features used to compute the flow/depth\n         - the upsampled previous flow/depth\n         - the direct features that already have the correct resolution\n\n    inp: Tensor\n        The features that have been used before to compute flow/depth\n\n    num_outputs: int \n        number of outputs for the upconvolution of \'features\'\n\n    upsampled_prediction: Tensor\n        The upsampled flow/depth prediction\n\n    features_direct: Tensor\n        The direct features which already have the spatial output resolution\n    """"""\n    upsampled_features = tf.layers.conv2d_transpose(\n        inputs=inp,\n        filters=num_outputs,\n        kernel_size=4,\n        strides=2,\n        padding=\'same\',\n        activation=myLeakyRelu,\n        kernel_initializer=default_weights_initializer(),\n        data_format=data_format,\n        name=""upconv"",\n        **kwargs,\n    )\n    inputs = [upsampled_features, features_direct, upsampled_prediction]\n    concat_inputs = [ x for x in inputs if not x is None ]\n    \n    if data_format == \'channels_first\':\n        return tf.concat(concat_inputs, axis=1)\n    else: # NHWC\n        return tf.concat(concat_inputs, axis=3)\n\n\n\ndef flow_block(image_pair, image2_2=None, intrinsics=None, prev_predictions=None, data_format=\'channels_first\', kernel_regularizer=None):\n    """"""Creates a flow network\n    \n    image_pair: Tensor\n        Image pair concatenated along the channel axis.\n\n    image2_2: Tensor\n        Second image at resolution level 2 (downsampled two times)\n        \n    intrinsics: Tensor \n        The normalized intrinsic parameters\n\n    prev_predictions: dict of Tensor\n        Predictions from the previous depth block\n    \n    Returns a dict with the predictions\n    """"""\n    conv_params = {\'data_format\':data_format, \'kernel_regularizer\':kernel_regularizer}\n\n    # contracting part\n    conv1 = convrelu2(name=\'conv1\', inputs=image_pair, num_outputs=(24,32), kernel_size=9, stride=2, **conv_params)\n\n    if prev_predictions is None:\n        conv2 = convrelu2(name=\'conv2\', inputs=conv1, num_outputs=(48,64), kernel_size=7, stride=2, **conv_params)\n        conv2_1 = convrelu2(name=\'conv2_1\', inputs=conv2, num_outputs=64, kernel_size=3, stride=1, **conv_params)\n    else:\n        conv2 = convrelu2(name=\'conv2\', inputs=conv1, num_outputs=32, kernel_size=7, stride=2, **conv_params)\n\n        # create warped input\n        if data_format==\'channels_first\':\n            prev_depth_nchw = prev_predictions[\'predict_depth2\']\n        else:\n            prev_depth_nchw = convert_NHWC_to_NCHW(prev_predictions[\'predict_depth2\'])\n\n        _flow_from_depth_motion = sops.depth_to_flow(\n            intrinsics = intrinsics,\n            depth = prev_depth_nchw,\n            rotation = prev_predictions[\'predict_rotation\'],\n            translation = prev_predictions[\'predict_translation\'],\n            inverse_depth = True,\n            normalize_flow = True,\n            )\n        # set flow vectors to zero if the motion is too large.\n        # this also eliminates nan values which can be produced by very bad camera parameters\n        flow_from_depth_motion_norm = tf.norm(_flow_from_depth_motion, axis=1, keep_dims=True)\n        flow_from_depth_motion_norm = tf.concat((flow_from_depth_motion_norm, flow_from_depth_motion_norm),axis=1)\n        tmp_zeros = tf.zeros_like(_flow_from_depth_motion,dtype=tf.float32)\n        flow_from_depth_motion =  tf.where( flow_from_depth_motion_norm < 1.0, _flow_from_depth_motion, tmp_zeros)\n\n\n        image2_2_warped = sops.warp2d(\n            input = image2_2 if data_format==\'channels_first\' else convert_NHWC_to_NCHW(image2_2),\n            displacements = flow_from_depth_motion,\n            normalized = True,\n            border_mode = \'value\',\n            )\n        if data_format==\'channels_last\':\n            flow_from_depth_motion = convert_NCHW_to_NHWC(flow_from_depth_motion)\n            image2_2_warped = convert_NCHW_to_NHWC(image2_2_warped)\n        extra_inputs = (image2_2_warped, flow_from_depth_motion, prev_predictions[\'predict_depth2\'], prev_predictions[\'predict_normal2\'])\n\n        # stop gradient here\n        extra_inputs_concat = tf.stop_gradient(tf.concat(extra_inputs, axis=1 if data_format==\'channels_first\' else 3))\n\n        conv_extra_inputs = convrelu2(name=\'conv2_extra_inputs\', inputs=extra_inputs_concat, num_outputs=32, kernel_size=3, stride=1, **conv_params)\n        conv2_concat = tf.concat((conv2,conv_extra_inputs), axis=1 if data_format==\'channels_first\' else 3)\n        conv2_1 = convrelu2(name=\'conv2_1\', inputs=conv2_concat, num_outputs=64, kernel_size=3, stride=1, **conv_params)\n    \n    \n    conv3 = convrelu2(name=\'conv3\', inputs=conv2_1, num_outputs=(96,128), kernel_size=5, stride=2, **conv_params)\n    conv3_1 = convrelu2(name=\'conv3_1\', inputs=conv3, num_outputs=128, kernel_size=3, stride=1, **conv_params)\n    \n    conv4 = convrelu2(name=\'conv4\', inputs=conv3_1, num_outputs=(192,256), kernel_size=5, stride=2, **conv_params)\n    conv4_1 = convrelu2(name=\'conv4_1\', inputs=conv4, num_outputs=256, kernel_size=3, stride=1, **conv_params)\n    \n    conv5 = convrelu2(name=\'conv5\', inputs=conv4_1, num_outputs=384, kernel_size=5, stride=2, **conv_params)\n    conv5_1 = convrelu2(name=\'conv5_1\', inputs=conv5, num_outputs=384, kernel_size=3, stride=1, **conv_params)\n\n    dense_slice_shape = conv5_1.get_shape().as_list()\n    if data_format == \'channels_first\':\n        dense_slice_shape[1] = 96\n    else:\n        dense_slice_shape[-1] = 96\n    units = 1\n    for i in range(1,len(dense_slice_shape)):\n        units *= dense_slice_shape[i]\n    dense5 = tf.layers.dense(\n            tf.contrib.layers.flatten(tf.slice(conv5_1, [0,0,0,0], dense_slice_shape)),\n            units=units,\n            activation=myLeakyRelu,\n            kernel_initializer=default_weights_initializer(),\n            kernel_regularizer=kernel_regularizer,\n            name=\'dense5\'\n            )\n    print(dense5)\n    conv5_1_dense5 = tf.concat((conv5_1,tf.reshape(dense5, dense_slice_shape)),  axis=1 if data_format==\'channels_first\' else 3)\n\n    \n    # expanding part\n    with tf.variable_scope(\'predict_flow5\'):\n        predict_flowconf5 = _predict_flow(conv5_1_dense5, predict_confidence=True, **conv_params)\n    \n    with tf.variable_scope(\'upsample_flow5to4\'):\n        predict_flowconf5to4 = _upsample_prediction(predict_flowconf5, 2, **conv_params)\n   \n    with tf.variable_scope(\'refine4\'):\n        concat4 = _refine(\n            inp=conv5_1_dense5, \n            num_outputs=256, \n            upsampled_prediction=predict_flowconf5to4, \n            features_direct=conv4_1,\n            **conv_params,\n        )\n\n    with tf.variable_scope(\'refine3\'):\n        concat3 = _refine(\n            inp=concat4, \n            num_outputs=128, \n            features_direct=conv3_1,\n            **conv_params,\n        )\n\n    with tf.variable_scope(\'refine2\'):\n        concat2 = _refine(\n            inp=concat3, \n            num_outputs=64, \n            features_direct=conv2_1,\n            **conv_params,\n        )\n\n    with tf.variable_scope(\'predict_flow2\'):\n        predict_flowconf2 = _predict_flow(concat2, predict_confidence=True, **conv_params)\n \n    return { \'predict_flowconf5\': predict_flowconf5, \'predict_flowconf2\': predict_flowconf2 }\n\n\ndef _predict_depthnormal(inp, predicted_scale=None, predict_normals=True, intermediate_num_outputs=24, data_format=\'channels_first\', **kwargs):\n    """"""Generates the ops for depth and normal prediction\n    \n    inp: Tensor\n\n    predicted_scale: Tensor\n        The predicted scale for scaling the depth values\n\n    predict_normals: bool\n        If True the output tensor has 4 channels instead of 1.\n        The last three channels are the normals.\n\n    intermediate_num_outputs: Tensor\n        Number of filters for the intermediate feature blob\n\n    Returns the depth prediction and the normal predictions separately\n    """"""\n\n    tmp = convrelu(\n        inputs=inp,\n        num_outputs=intermediate_num_outputs,\n        kernel_size=3,\n        strides=1,\n        name=""conv1"",\n        data_format=data_format,\n        **kwargs,\n    )\n    \n    tmp2 = conv2d(\n        inputs=tmp,\n        num_outputs=4 if predict_normals else 1,\n        kernel_size=3,\n        strides=1,\n        name=""conv2"",\n        data_format=data_format,\n        **kwargs,\n    )\n\n    if predict_normals:\n\n        predicted_unscaled_depth, predicted_normal = tf.split(value=tmp2, num_or_size_splits=[1,3], axis=1 if data_format==\'channels_first\' else 3) \n\n        if not predicted_scale is None:\n            batch_size = predicted_scale.get_shape().as_list()[0]\n            s = tf.reshape(predicted_scale, [batch_size,1,1,1])\n            predicted_depth = s*predicted_unscaled_depth\n        else:\n            predicted_depth = predicted_unscaled_depth\n\n        return predicted_depth, predicted_normal\n    else:\n        if not predicted_scale is None:\n            predicted_depth = predicted_scale*tmp2\n        else:\n            predicted_depth = tmp2\n\n        return predicted_depth\n\n\n\n\ndef depthmotion_block(image_pair, image2_2, prev_flow2, prev_flowconf2, prev_rotation=None, prev_translation=None, intrinsics=None, data_format=\'channels_first\', kernel_regularizer=None):\n    """"""Creates a depth and motion network\n    \n    image_pair: Tensor\n        Image pair concatenated along the channel axis.\n        The tensor format is NCHW with C == 6.\n\n    image2_2: Tensor\n        Second image at resolution level 2\n\n    prev_flow2: Tensor\n        The output of the flow network. Contains only the flow (2 channels)\n\n    prev_flowconf2: Tensor\n        The output of the flow network. Contains flow and flow confidence (4 channels)\n\n    prev_rotation: Tensor\n        The previously predicted rotation.\n        \n    prev_translaion: Tensor\n        The previously predicted translation.\n\n    intrinsics: Tensor\n        Tensor with the intrinsic camera parameters\n        Only required if prev_rotation and prev_translation is not None.\n        \n    Returns a dictionary with the predictions for depth, normals and motion\n    """"""\n    conv_params = {\'data_format\':data_format, \'kernel_regularizer\':kernel_regularizer}\n    fc_params = {}\n    \n    # contracting part\n    conv1 = convrelu2(name=\'conv1\', inputs=image_pair, num_outputs=(24,32), kernel_size=9, stride=2, **conv_params)\n    \n    conv2 = convrelu2(name=\'conv2\', inputs=conv1, num_outputs=32, kernel_size=7, stride=2, **conv_params)\n    # create extra inputs\n    if data_format==\'channels_first\':\n        image2_2_warped = sops.warp2d(image2_2, prev_flow2, normalized=True, border_mode=\'value\')\n    else:\n        prev_flow2_nchw = convert_NHWC_to_NCHW(prev_flow2)\n        image2_2_warped = convert_NCHW_to_NHWC(sops.warp2d(convert_NHWC_to_NCHW(image2_2), prev_flow2_nchw, normalized=True, border_mode=\'value\'))\n        \n    extra_inputs = [image2_2_warped, prev_flowconf2]\n    if (not prev_rotation is None) and (not prev_translation is None) and (not intrinsics is None):\n        if data_format==\'channels_first\':\n            depth_from_flow = sops.flow_to_depth2(\n                flow=prev_flow2, \n                intrinsics=intrinsics,\n                rotation=prev_rotation,\n                translation=prev_translation,\n                normalized_flow=True,\n                inverse_depth=True,\n                )\n        else:\n            depth_from_flow = convert_NCHW_to_NHWC(sops.flow_to_depth2(\n                flow=prev_flow2_nchw, \n                intrinsics=intrinsics,\n                rotation=prev_rotation,\n                translation=prev_translation,\n                normalized_flow=True,\n                inverse_depth=True,\n                ))\n        depth_from_flow = tf.clip_by_value(depth_from_flow, 0.0, 50.0)    \n\n        extra_inputs.append(depth_from_flow)\n\n    concat_extra_inputs = tf.stop_gradient(tf.concat(extra_inputs, axis=1 if data_format==\'channels_first\' else 3))\n    conv_extra_inputs = convrelu2(name=\'conv2_extra_inputs\', inputs=concat_extra_inputs, num_outputs=32, kernel_size=3, stride=1, **conv_params)\n    conv2_concat = tf.concat((conv2,conv_extra_inputs),axis=1 if data_format==\'channels_first\' else 3)\n    conv2_1 = convrelu2(name=\'conv2_1\', inputs=conv2_concat, num_outputs=64, kernel_size=3, stride=1, **conv_params)\n    \n    conv3 = convrelu2(name=\'conv3\', inputs=conv2_1, num_outputs=(96,128), kernel_size=5, stride=2, **conv_params)\n    conv3_1 = convrelu2(name=\'conv3_1\', inputs=conv3, num_outputs=128, kernel_size=3, stride=1, **conv_params)\n    \n    conv4 = convrelu2(name=\'conv4\', inputs=conv3_1, num_outputs=(192,256), kernel_size=5, stride=2, **conv_params)\n    conv4_1 = convrelu2(name=\'conv4_1\', inputs=conv4, num_outputs=256, kernel_size=3, stride=1, **conv_params)\n    \n    conv5 = convrelu2(name=\'conv5\', inputs=conv4_1, num_outputs=384, kernel_size=3, stride=2, **conv_params)\n    conv5_1 = convrelu2(name=\'conv5_1\', inputs=conv5, num_outputs=384, kernel_size=3, stride=1, **conv_params)\n    \n    dense_slice_shape = conv5_1.get_shape().as_list()\n    if data_format == \'channels_first\':\n        dense_slice_shape[1] = 96\n    else:\n        dense_slice_shape[-1] = 96\n    units = 1\n    for i in range(1,len(dense_slice_shape)):\n        units *= dense_slice_shape[i]\n    dense5 = tf.layers.dense(\n            tf.contrib.layers.flatten(tf.slice(conv5_1, [0,0,0,0], dense_slice_shape)),\n            units=units,\n            activation=myLeakyRelu,\n            kernel_initializer=default_weights_initializer(),\n            kernel_regularizer=kernel_regularizer,\n            name=\'dense5\'\n            )\n    print(dense5)\n    conv5_1_dense5 = tf.concat((conv5_1,tf.reshape(dense5, dense_slice_shape)),  axis=1 if data_format==\'channels_first\' else 3)\n    \n    # motion prediction part\n    motion_conv3 = convrelu2(name=\'motion_conv3\', inputs=conv2_1, num_outputs=64, kernel_size=5, stride=2, **conv_params)\n    motion_conv4 = convrelu2(name=\'motion_conv4\', inputs=motion_conv3, num_outputs=64, kernel_size=5, stride=2, **conv_params)\n    motion_conv5a = convrelu2(name=\'motion_conv5a\', inputs=motion_conv4, num_outputs=64, kernel_size=3, stride=2, **conv_params)\n\n    motion_conv5b = convrelu(\n        name=\'motion_conv5b\',\n        inputs=conv5_1_dense5,\n        num_outputs=64,\n        kernel_size=3,\n        strides=1,\n        **conv_params,\n    )\n    motion_conv5_1 = tf.concat((motion_conv5a, motion_conv5b), axis=1 if data_format==\'channels_first\' else 3)\n\n    if data_format==\'channels_last\':\n        motion_conv5_1 = convert_NHWC_to_NCHW(motion_conv5_1)\n    motion_fc1 = tf.layers.dense(\n        name=\'motion_fc1\',\n        inputs=tf.contrib.layers.flatten(motion_conv5_1),\n        units=1024,\n        activation=myLeakyRelu,\n        kernel_regularizer=kernel_regularizer,\n        **fc_params,\n    )\n    motion_fc2 = tf.layers.dense(\n        name=\'motion_fc2\',\n        inputs=motion_fc1,\n        units=128,\n        activation=myLeakyRelu,\n        kernel_regularizer=kernel_regularizer,\n        **fc_params,\n    )\n    predict_motion_scale = tf.layers.dense(\n        name=\'motion_fc3\',\n        inputs=motion_fc2,\n        units=7,\n        activation=None,\n        kernel_regularizer=kernel_regularizer,\n        **fc_params,\n    )\n\n    predict_rotation, predict_translation, predict_scale = tf.split(value=predict_motion_scale, num_or_size_splits=[3,3,1], axis=1)\n    \n    # expanding part\n    with tf.variable_scope(\'refine4\'):\n        concat4 = _refine(\n            inp=conv5_1, \n            num_outputs=256, \n            features_direct=conv4_1,\n            **conv_params,\n        )\n\n    with tf.variable_scope(\'refine3\'):\n        concat3 = _refine(\n            inp=concat4, \n            num_outputs=128, \n            features_direct=conv3_1,\n            **conv_params,\n        )\n\n    with tf.variable_scope(\'refine2\'):\n        concat2 = _refine(\n            inp=concat3, \n            num_outputs=64, \n            features_direct=conv2_1,\n            **conv_params,\n        )\n\n    with tf.variable_scope(\'predict_depthnormal2\'):\n        predict_depth2, predict_normal2 = _predict_depthnormal(concat2, predicted_scale=predict_scale, **conv_params)\n \n    return { \n        \'predict_depth2\': predict_depth2,  \n        \'predict_normal2\': predict_normal2,  \n        \'predict_rotation\': predict_rotation,  \n        \'predict_translation\': predict_translation,  \n        \'predict_scale\': predict_scale,\n        }\n    \n\n\n\ndef depth_refine_block(image1, depthmotion_predictions, data_format=\'channels_first\', kernel_regularizer=None ):\n    """"""Creates a refinement network for the depth and normal predictions\n    \n    image1: Tensor\n        The reference image at full resolution.\n\n    depthmotion_predictions: dict of Tensors\n        The output of the depthmotion network.\n\n    Returns a dictionary with the predictions\n    """"""\n    conv_params = {\'data_format\': data_format, \'kernel_regularizer\':kernel_regularizer}\n\n    # upsample the predicted depth and normals to the same size as the reference image \n    if data_format==\'channels_first\':\n        original_image_size = image1.get_shape().as_list()[-2:]\n    else:\n        original_image_size = image1.get_shape().as_list()[1:3]\n    depth2 = depthmotion_predictions[\'predict_depth2\']\n    if data_format==\'channels_first\':\n        depth2_nhwc = convert_NCHW_to_NHWC(depth2) \n    else:\n        depth2_nhwc = depth2\n    depth2_nhwc_orig_size = tf.image.resize_nearest_neighbor(depth2_nhwc, original_image_size)\n\n    if data_format==\'channels_first\':\n        depth2_orig_size = convert_NHWC_to_NCHW(depth2_nhwc_orig_size)\n    else:\n        depth2_orig_size = depth2_nhwc_orig_size\n\n    net_inputs = tf.concat((image1, depth2_orig_size), axis=1 if data_format==\'channels_first\' else 3)\n\n    # contracting part\n    conv0 = convrelu(name=\'conv0\', inputs=net_inputs, num_outputs=32, kernel_size=3, strides=1, **conv_params)\n    \n    conv1 = convrelu(name=\'conv1\', inputs=conv0, num_outputs=64, kernel_size=3, strides=2, **conv_params)\n    conv1_1 = convrelu(name=\'conv1_1\', inputs=conv1, num_outputs=64, kernel_size=3, strides=1, **conv_params)\n    \n    conv2 = convrelu(name=\'conv2\', inputs=conv1_1, num_outputs=128, kernel_size=3, strides=2, **conv_params)\n    conv2_1 = convrelu(name=\'conv2_1\', inputs=conv2, num_outputs=128, kernel_size=3, strides=1, **conv_params)\n\n    # expanding part\n    with tf.variable_scope(\'refine1\'):\n        concat1 = _refine(\n            inp=conv2_1, \n            num_outputs=64, \n            features_direct=conv1_1,\n            **conv_params,\n        )\n    \n    with tf.variable_scope(\'refine0\'):\n        concat0 = _refine(\n            inp=concat1, \n            num_outputs=32, \n            features_direct=conv0,\n            **conv_params,\n        )\n    \n    with tf.variable_scope(\'predict_depth0\'):\n        predict_depth0, predict_normal0 = _predict_depthnormal(concat0, predict_normals=True, intermediate_num_outputs=16, **conv_params)\n\n    return { \'predict_depth0\': predict_depth0, \'predict_normal0\': predict_normal0 }\n\n\n'"
python/depthmotionnet/v2/helpers.py,4,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport tensorflow as tf\nimport lmbspecialops as sops\nimport numpy as np\n\nfrom depthmotionnet.helpers import *\n \ndef conv2d(inputs, num_outputs, kernel_size, data_format, **kwargs):\n    """"""Convolution with \'same\' padding""""""\n\n    return tf.layers.conv2d(\n        inputs=inputs,\n        filters=num_outputs,\n        kernel_size=kernel_size,\n        kernel_initializer=default_weights_initializer(),\n        padding=\'same\',\n        data_format=data_format,\n        **kwargs,\n        )\n\n\ndef convrelu(inputs, num_outputs, kernel_size, data_format, **kwargs):\n    """"""Shortcut for a single convolution+relu \n    \n    See tf.layers.conv2d for a description of remaining parameters\n    """"""\n    return conv2d(inputs, num_outputs, kernel_size, data_format, activation=myLeakyRelu, **kwargs)\n\n\ndef convrelu2(inputs, num_outputs, kernel_size, name, stride, data_format, **kwargs):\n    """"""Shortcut for two convolution+relu with 1D filter kernels \n    \n    num_outputs: int or (int,int)\n        If num_outputs is a tuple then the first element is the number of\n        outputs for the 1d filter in y direction and the second element is\n        the final number of outputs.\n    """"""\n    if isinstance(num_outputs,(tuple,list)):\n        num_outputs_y = num_outputs[0]\n        num_outputs_x = num_outputs[1]\n    else:\n        num_outputs_y = num_outputs\n        num_outputs_x = num_outputs\n\n    if isinstance(kernel_size,(tuple,list)):\n        kernel_size_y = kernel_size[0]\n        kernel_size_x = kernel_size[1]\n    else:\n        kernel_size_y = kernel_size\n        kernel_size_x = kernel_size\n\n    tmp_y = tf.layers.conv2d(\n        inputs=inputs,\n        filters=num_outputs_y,\n        kernel_size=[kernel_size_y,1],\n        strides=[stride,1],\n        padding=\'same\',\n        activation=myLeakyRelu,\n        kernel_initializer=default_weights_initializer(),\n        data_format=data_format,\n        name=name+\'y\',\n        **kwargs,\n    )\n    return tf.layers.conv2d(\n        inputs=tmp_y,\n        filters=num_outputs_x,\n        kernel_size=[1,kernel_size_x],\n        strides=[1,stride],\n        padding=\'same\',\n        activation=myLeakyRelu,\n        kernel_initializer=default_weights_initializer(),\n        data_format=data_format,\n        name=name+\'x\',\n        **kwargs,\n    )\n\n\ndef recursive_median_downsample(inp, iterations):\n    """"""Recursively downsamples the input using a 3x3 median filter""""""\n    result = []\n    for i in range(iterations):\n        if not result:\n            tmp_inp = inp\n        else:\n            tmp_inp = result[-1]\n        result.append(sops.median3x3_downsample(tmp_inp))\n    return tuple(result)\n\n\n\n'"
python/depthmotionnet/v2/losses.py,10,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport tensorflow as tf\nimport lmbspecialops as sops\nfrom .helpers import *\n\n\ndef l1_loss(x, epsilon):\n    """"""L1 loss\n\n    Returns a scalar tensor with the loss\n    """"""\n    with tf.name_scope(""l1_loss""):\n        return tf.reduce_sum(tf.sqrt(x**2 + epsilon))\n\n\ndef pointwise_l2_loss(inp, gt, epsilon, data_format=\'NCHW\'):\n    """"""Computes the pointwise unsquared l2 loss.\n    The input tensors must use the format NCHW. \n    This loss ignores nan values. \n    The loss is normalized by the number of pixels.\n    \n    inp: Tensor\n        This is the prediction.\n        \n    gt: Tensor\n        The ground truth with the same shape as \'inp\'\n        \n    epsilon: float\n        The epsilon value to avoid division by zero in the gradient computation\n    """"""\n    with tf.name_scope(\'pointwise_l2_loss\'):\n        gt_ = tf.stop_gradient(gt)\n        diff = sops.replace_nonfinite(inp-gt_)\n        if data_format == \'NCHW\':\n            return tf.reduce_mean(tf.sqrt(tf.reduce_sum(diff**2, axis=1)+epsilon))\n        else: # NHWC\n            return tf.reduce_mean(tf.sqrt(tf.reduce_sum(diff**2, axis=3)+epsilon))\n\n\n\ndef scale_invariant_gradient( inp, deltas, weights, epsilon=0.001):\n    """"""Computes the scale invariant gradient images\n    \n    inp: Tensor\n        \n    deltas: list of int\n      The pixel delta for the difference. \n      This vector must be the same length as weight.\n\n    weights: list of float\n      The weight factor for each difference.\n      This vector must be the same length as delta.\n\n    epsilon: float\n      epsilon value for avoiding division by zero\n        \n    """"""\n    assert len(deltas)==len(weights)\n\n    sig_images = []\n    for delta, weight in zip(deltas,weights):\n        sig_images.append(sops.scale_invariant_gradient(inp, deltas=[delta], weights=[weight], epsilon=epsilon))\n    return tf.concat(sig_images,axis=1)\n\n\n\ndef scale_invariant_gradient_loss( inp, gt, epsilon ):\n    """"""Computes the scale invariant gradient loss\n\n    inp: Tensor\n        Tensor with the scale invariant gradient images computed on the prediction\n\n    gt: Tensor\n        Tensor with the scale invariant gradient images computed on the ground truth\n\n    epsilon: float\n      epsilon value for avoiding division by zero\n    """"""\n    num_channels_inp = inp.get_shape().as_list()[1]\n    num_channels_gt = gt.get_shape().as_list()[1]\n    assert num_channels_inp%2==0\n    assert num_channels_inp==num_channels_gt\n\n    tmp = []\n    for i in range(num_channels_inp//2):\n        tmp.append(pointwise_l2_loss(inp[:,i*2:i*2+2,:,:], gt[:,i*2:i*2+2,:,:], epsilon))\n\n    return tf.add_n(tmp)\n\n\n\n\ndef flow_loss_block(\n    gt_flow2, \n    gt_flow5, \n    gt_flow2_sig, \n    pr_flow2, \n    pr_flow5, \n    pr_conf2, \n    pr_conf5, \n    flow_weight, \n    conf_weight, \n    flow_sig_weight, \n    conf_sig_weight, \n    conf_diff_scale=1,\n    level5_factor=0.5,\n    loss_prefix=\'\',\n    ):\n    """"""Adds loss operations to the flow outputs\n\n    gt_flow2: ground truth flow at resolution level 2\n    gt_flow5: ground truth flow at resolution level 5\n    gt_flow2_sig: the scale invariant gradient of the ground truth flow at resolution level 2\n    pr_flow2: predicted flow at resolution level 2\n    pr_flow5: predicted flow at resolution level 5\n    pr_conf2: predicted confidence at resolution level 2\n    pr_conf5: predicted confidence at resolution level 5\n    flow_weight: the weight for the \'absolute\' loss on the flows\n    conf_weight: the weight for the \'absolute\' loss on the flow confidence\n    flow_sig_weight: the weight for the loss on the scale invariant gradient images of the flow\n    conf_sig_weight: the weight for the loss on the scale invariant gradient images of the confidence\n    conf_diff_scale: scale factor for the absolute differences in the conf map computation\n    level5_factor: factor for the losses at the smaller resolution level 5. affects losses on pr_flow5 and pr_conf5.\n    loss_prefix: prefix name for the loss in the returned dict e.g. \'netFlow1_\'\n\n    Returns a dictionary with the losses\n    """"""\n    losses = {}\n    epsilon = 0.00001\n\n    loss_flow5 = (level5_factor*flow_weight) * pointwise_l2_loss(pr_flow5, gt_flow5, epsilon=epsilon)\n    losses[\'loss_flow5\'] = loss_flow5\n    loss_flow2 = (flow_weight) * pointwise_l2_loss(pr_flow2, gt_flow2, epsilon=epsilon)\n    losses[\'loss_flow2\'] = loss_flow2\n\n    loss_flow5_unscaled = pointwise_l2_loss(pr_flow5, gt_flow5, epsilon=0)\n    losses[\'loss_flow5_unscaled\'] = loss_flow5_unscaled\n    loss_flow2_unscaled = pointwise_l2_loss(pr_flow2, gt_flow2, epsilon=0)\n    losses[\'loss_flow2_unscaled\'] = loss_flow2_unscaled\n\n    # ground truth confidence maps\n    conf2 = compute_confidence_map(pr_flow2, gt_flow2, conf_diff_scale)\n    conf5 = compute_confidence_map(pr_flow5, gt_flow5, conf_diff_scale)\n\n    if not pr_conf5 is None: \n        loss_conf5 = (level5_factor*conf_weight) * pointwise_l2_loss(pr_conf5, conf5, epsilon=epsilon)\n        losses[\'loss_conf5\'] = loss_conf5  \n        loss_conf5_unscaled = pointwise_l2_loss(pr_conf5, conf5, epsilon=0)\n        losses[\'loss_conf5_unscaled\'] = loss_conf5_unscaled  \n    if not pr_conf2 is None:\n        loss_conf2 = conf_weight * pointwise_l2_loss(pr_conf2, conf2, epsilon=epsilon)\n        losses[\'loss_conf2\'] = loss_conf2  \n        loss_conf2_unscaled = pointwise_l2_loss(pr_conf2, conf2, epsilon=0)\n        losses[\'loss_conf2_unscaled\'] = loss_conf2_unscaled  \n\n\n    sig_params = {\'deltas\':[1,2,4,8,16], \'weights\':[1,1,1,1,1], \'epsilon\': 0.001}\n\n    if not flow_sig_weight is None:\n        pr_flow2_sig = scale_invariant_gradient(pr_flow2, **sig_params)\n        loss_flow2_sig = flow_sig_weight * pointwise_l2_loss(pr_flow2_sig, gt_flow2_sig, epsilon=epsilon)\n        losses[\'loss_flow2_sig\'] = loss_flow2_sig  \n        loss_flow2_sig_unscaled = pointwise_l2_loss(pr_flow2_sig, gt_flow2_sig, epsilon=0)\n        losses[\'loss_flow2_sig_unscaled\'] = loss_flow2_sig_unscaled  \n\n    if not conf_sig_weight is None and not pr_conf2 is None:\n        pr_conf2_sig = scale_invariant_gradient(pr_conf2, **sig_params)\n        conf2_sig = scale_invariant_gradient(conf2, **sig_params)\n        loss_conf2_sig = conf_sig_weight * pointwise_l2_loss(pr_conf2_sig, conf2_sig, epsilon=epsilon)\n        losses[\'loss_conf2_sig\'] = loss_conf2_sig  \n        loss_conf2_sig_unscaled = pointwise_l2_loss(pr_conf2_sig, conf2_sig, epsilon=0)\n        losses[\'loss_conf2_sig_unscaled\'] = loss_conf2_sig_unscaled  \n\n    # add prefix and return\n    return { loss_prefix+k: losses[k] for k in losses }\n        \n\n\n\n\ndef depthnormal_loss_block(\n    gt_depth2, \n    gt_depth2_sig, \n    gt_normal2,\n    gt_rotation,\n    gt_translation,\n    pr_depth2,\n    pr_normal2,\n    pr_rotation,\n    pr_translation,\n    depth_weight, \n    depth_sig_weight, \n    normal_weight, \n    rotation_weight,\n    translation_weight,\n    translation_factor,\n    loss_prefix=\'\' ):\n    """"""Adds loss operations to the flow outputs\n\n    gt_depth2: ground truth depth at resolution level 2\n    gt_depth2_sig: the scale invariant gradient of the ground truth depth at resolution level 2\n    gt_normal2: ground truth normals at resolution level 2\n    pr_depth2: predicted depth at resolution level 2\n    pr_normal2: predicted normals at resolution level 2\n    depth_weight: the weight for the \'absolute\' loss on the depth\n    depth_sig_weight: the weight for the loss on the scale invariant gradient image of the depth\n    normal_weight: the weight for the loss on the normals\n    rotation_weight: the weight for the loss on the rotation\n    translation_weight: the weight for the loss on the translation\n    translation_factor: additional factor on the translation loss\n    loss_prefix: prefix name for the loss in the summary e.g. \'netDM1\'\n\n    Returns a dictionary with the losses\n    """"""\n    losses = {}\n    batch_size = pr_depth2.get_shape().as_list()[0]\n    epsilon = 0.00001\n    sig_params = {\'deltas\':[1,2,4,8,16], \'weights\':[1,1,1,1,1], \'epsilon\':0.01}\n\n    loss_depth2 = depth_weight* pointwise_l2_loss(pr_depth2, gt_depth2, epsilon=epsilon)\n\n    pr_depth2_sig = scale_invariant_gradient(pr_depth2, **sig_params)\n    loss_depth2_sig = depth_sig_weight* pointwise_l2_loss(pr_depth2_sig, gt_depth2_sig, epsilon=epsilon)\n    loss_depth2_sig_unscaled = pointwise_l2_loss(pr_depth2_sig, gt_depth2_sig, epsilon=0)\n\n\n    loss_normal2 = normal_weight* pointwise_l2_loss(pr_normal2, gt_normal2, epsilon=epsilon)\n\n    losses[\'loss_depth2\'] = loss_depth2\n    losses[\'loss_depth2_sig\'] = loss_depth2_sig\n    losses[\'loss_depth2_sig_unscaled\'] = loss_depth2_sig_unscaled\n    losses[\'loss_normal2\'] = loss_normal2\n\n    # motion losses\n    loss_rotation = (rotation_weight/batch_size)*l1_loss(pr_rotation-gt_rotation, epsilon=epsilon)\n    loss_translation_no_factor = (translation_weight/batch_size)*l1_loss(pr_translation-gt_translation, epsilon=epsilon)\n    loss_translation = translation_factor*loss_translation_no_factor\n    rot_transl_loss_ratio = loss_rotation/loss_translation_no_factor\n\n    losses[\'loss_rotation\'] = loss_rotation\n    losses[\'loss_translation\'] = loss_translation\n    losses[\'loss_translation_no_factor\'] = loss_translation_no_factor\n    losses[\'rot_transl_loss_ratio\'] = rot_transl_loss_ratio\n    \n    # add prefix and return\n    return { loss_prefix+k: losses[k] for k in losses }\n\n\ndef depth_refine_loss_block(\n    gt_depth0, \n    gt_depth0_sig, \n    gt_normal0,\n    pr_depth0,\n    pr_normal0,\n    depth_weight, \n    depth_sig_weight, \n    normal_weight, \n    loss_prefix=\'\' ):\n    """"""Adds loss operations to the flow outputs\n\n    gt_depth0: ground truth depth at resolution level 0\n    gt_depth0_sig: the scale invariant gradient of the ground truth depth at resolution level 0\n    gt_normal0: ground truth normals at resolution level 0\n    pr_depth0: predicted depth at resolution level 0\n    pr_normal0: predicted normals at resolution level 0\n    depth_weight: the weight for the \'absolute\' loss on the depth\n    depth_sig_weight: the weight for the loss on the scale invariant gradient image of the depth\n    normal_weight: the weight for the loss on the normals\n    loss_prefix: prefix name for the loss in the summary e.g. \'netRefine\'\n\n    Returns a dictionary with the losses\n    """"""\n    losses = {}\n    epsilon = 0.00001\n    sig_params = {\'deltas\':[1,2,4,8,16], \'weights\':[1,1,1,1,1], \'epsilon\':0.01}\n\n    loss_depth0 = depth_weight* pointwise_l2_loss(pr_depth0, gt_depth0, epsilon=epsilon)\n\n    pr_depth0_sig = scale_invariant_gradient(pr_depth0, **sig_params)\n    loss_depth0_sig = depth_sig_weight* pointwise_l2_loss(pr_depth0_sig, gt_depth0_sig, epsilon=epsilon)\n    loss_depth0_sig_unscaled = pointwise_l2_loss(pr_depth0_sig, gt_depth0_sig, epsilon=0)\n\n\n    loss_normal0 = normal_weight* pointwise_l2_loss(pr_normal0, gt_normal0, epsilon=epsilon)\n\n    losses[\'loss_depth0\'] = loss_depth0\n    losses[\'loss_depth0_sig\'] = loss_depth0_sig\n    losses[\'loss_depth0_sig_unscaled\'] = loss_depth0_sig_unscaled\n    losses[\'loss_normal0\'] = loss_normal0\n\n    # add prefix and return\n    return { loss_prefix+k: losses[k] for k in losses }\n\n\n\ndef prepare_ground_truth_tensors(depth, rotation, translation, intrinsics):\n    """"""Computes ground truth tensors at lower resolution and scale invariant gradient (sig)\n    images of some ground truth tensors.\n    \n    depth: Tensor\n        depth map with inverse depth values\n        \n    rotation: Tensor\n        rotation in angle axis format with 3 elements\n\n    translation: Tensor\n        the camera translation\n\n    intrinsics: Tensor\n        Tensor with the intrinsic camera parameters\n\n    Returns a dictionary with ground truth data for depth, normal and flow for\n    different resolutions.\n    """"""\n    depth1, depth2, depth3, depth4, depth5 = recursive_median_downsample(depth,5)\n    flow0 = sops.depth_to_flow(depth, intrinsics, rotation, translation, inverse_depth=True, normalize_flow=True, name=\'DepthToFlow0\')\n    flow2 = sops.depth_to_flow(depth2, intrinsics, rotation, translation, inverse_depth=True, normalize_flow=True, name=\'DepthToFlow2\')\n    flow5 = sops.depth_to_flow(depth5, intrinsics, rotation, translation, inverse_depth=True, normalize_flow=True, name=\'DepthToFlow5\')\n    \n    normal0 = sops.depth_to_normals(depth, intrinsics, inverse_depth=True)\n    normal2 = sops.depth_to_normals(depth2, intrinsics, inverse_depth=True)\n    \n    sig_params = {\'deltas\':[1,2,4,8,16], \'weights\':[1,1,1,1,1], \'epsilon\': 0.001}\n\n    depth0_sig = scale_invariant_gradient(depth, **sig_params)\n    depth2_sig = scale_invariant_gradient(depth2, **sig_params)\n    flow2_sig = scale_invariant_gradient(flow2, **sig_params)\n    \n    return {\n            \'depth0\': depth, \n            \'depth0_sig\': depth0_sig, \n            \'depth2\': depth2, \n            \'depth2_sig\': depth2_sig, \n            \'flow0\': flow0, \n            \'flow2\': flow2, \n            \'flow2_sig\': flow2_sig, \n            \'flow5\': flow5, \n            \'normal0\': normal0,\n            \'normal2\': normal2,\n            }\n\n\n\ndef compute_confidence_map(predicted_flow, gt_flow, scale=1):\n    """"""Computes the ground truth confidence map as c_gt = exp(-s|f_pr-f_gt|) \n    \n    predict_flow: Tensor\n        The predicted flow\n        \n    gt_flow: Tensor\n        The ground truth flow\n\n    scale: float\n        Scale factor for the absolute differences\n    """"""\n    with tf.name_scope(\'compute_confidence_map\'):\n        return tf.exp(-scale*tf.abs(predicted_flow - gt_flow))\n   \n\n'"
python/depthmotionnet/v2/networks.py,27,"b'#\n#  DeMoN - Depth Motion Network\n#  Copyright (C) 2017  Benjamin Ummenhofer, Huizhong Zhou\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom .blocks import *\n\nclass BootstrapNet:\n    def __init__(self, session):\n        """"""Creates the bootstrap network\n\n        session: tf.Session\n            Tensorflow session\n\n        """"""\n        self.session = session\n        self.placeholder_image_pair = tf.placeholder(dtype=tf.float32, shape=(1,6,192,256))\n        self.placeholder_image2_2 = tf.placeholder(dtype=tf.float32, shape=(1,3,48,64))\n\n\n        with tf.variable_scope(\'netFlow1\'):\n            netFlow1_result = flow_block(self.placeholder_image_pair )\n            self.netFlow1_result = netFlow1_result\n            self.predict_flow5, self.predict_conf5 = tf.split(value=netFlow1_result[\'predict_flowconf5\'], num_or_size_splits=2, axis=1)\n            self.predict_flow2, self.predict_conf2 = tf.split(value=netFlow1_result[\'predict_flowconf2\'], num_or_size_splits=2, axis=1)\n\n        with tf.variable_scope(\'netDM1\'):\n            self.netDM1_result = depthmotion_block(\n                    image_pair=self.placeholder_image_pair, \n                    image2_2=self.placeholder_image2_2, \n                    prev_flow2=self.predict_flow2, \n                    prev_flowconf2=self.netFlow1_result[\'predict_flowconf2\'], \n                    )\n\n\n    def eval(self, image_pair, image2_2):\n        """"""Runs the bootstrap network\n        \n        image_pair: numpy.ndarray\n            Array with shape [1,6,192,256] if data_format==\'channels_first\'\n            \n            Image pair in the range [-0.5, 0.5]\n\n        image2_2: numpy.ndarray\n            Second image at resolution level 2 (downsampled two times)\n\n            The shape for data_format==\'channels_first\' is [1,3,48,64]\n\n        Returns a dict with the preditions of the bootstrap net\n        """"""\n        fetches = {\n                \'predict_flow5\': self.predict_flow5,\n                \'predict_flow2\': self.predict_flow2,\n                \'predict_depth2\': self.netDM1_result[\'predict_depth2\'],\n                \'predict_normal2\': self.netDM1_result[\'predict_normal2\'],\n                \'predict_rotation\': self.netDM1_result[\'predict_rotation\'],\n                \'predict_translation\': self.netDM1_result[\'predict_translation\'],\n                }\n        feed_dict = {\n                self.placeholder_image_pair: image_pair,\n                self.placeholder_image2_2: image2_2,\n                }\n        return self.session.run(fetches, feed_dict=feed_dict)\n\n\n\n\nclass IterativeNet:\n    def __init__(self, session):\n        """"""Creates the bootstrap network\n\n        session: tf.Session\n            Tensorflow session\n\n        """"""\n        self.session = session\n\n        self.intrinsics = tf.constant([[0.89115971, 1.18821287, 0.5, 0.5]], dtype=tf.float32)\n\n        self.placeholder_image_pair = tf.placeholder(dtype=tf.float32, shape=(1,6,192,256))\n        self.placeholder_image2_2 = tf.placeholder(dtype=tf.float32, shape=(1,3,48,64))\n\n        self.placeholder_depth2 = tf.placeholder(dtype=tf.float32, shape=(1,1,48,64))\n        self.placeholder_normal2 = tf.placeholder(dtype=tf.float32, shape=(1,3,48,64))\n        self.placeholder_rotation = tf.placeholder(dtype=tf.float32, shape=(1,3))\n        self.placeholder_translation = tf.placeholder(dtype=tf.float32, shape=(1,3))\n\n        with tf.variable_scope(\'netFlow2\'):\n            netFlow2_result = flow_block(\n                image_pair=self.placeholder_image_pair,\n                image2_2=self.placeholder_image2_2,\n                intrinsics=self.intrinsics,\n                prev_predictions={\n                    \'predict_depth2\': self.placeholder_depth2,\n                    \'predict_normal2\': self.placeholder_normal2,\n                    \'predict_rotation\': self.placeholder_rotation,\n                    \'predict_translation\': self.placeholder_translation,\n                    },\n                )\n            self.netFlow2_result = netFlow2_result\n            self.predict_flow5, self.predict_conf5 = tf.split(value=netFlow2_result[\'predict_flowconf5\'], num_or_size_splits=2, axis=1)\n            self.predict_flow2, self.predict_conf2 = tf.split(value=netFlow2_result[\'predict_flowconf2\'], num_or_size_splits=2, axis=1)\n\n        with tf.variable_scope(\'netDM2\'):\n            self.netDM2_result = depthmotion_block(\n                    image_pair=self.placeholder_image_pair,\n                    image2_2=self.placeholder_image2_2, \n                    prev_flow2=self.predict_flow2, \n                    prev_flowconf2=self.netFlow2_result[\'predict_flowconf2\'], \n                    intrinsics=self.intrinsics,\n                    prev_rotation=self.placeholder_rotation,\n                    prev_translation=self.placeholder_translation,\n                    )\n\n    def eval(self, image_pair, image2_2, depth2, normal2, rotation, translation ):\n        """"""Runs the iterative network\n        \n        image_pair: numpy.ndarray\n            Array with shape [1,6,192,256]\n            \n            Image pair in the range [-0.5, 0.5]\n\n        image2_2: numpy.ndarray\n            Second image at resolution level 2 (downsampled two times)\n\n            The shape is [1,3,48,64]\n\n        depth2: numpy.ndarray\n            Depth prediction at resolution level 2\n\n        normal2: numpy.ndarray\n            Normal prediction at resolution level 2\n\n        rotation: numpy.ndarray\n            Rotation prediction in 3 element angle axis format\n\n        translation: numpy.ndarray\n            Translation prediction\n\n        Returns a dict with the preditions of the iterative net\n        """"""\n\n\n        fetches = {\n                \'predict_flow5\': self.predict_flow5,\n                \'predict_flow2\': self.predict_flow2,\n                \'predict_depth2\': self.netDM2_result[\'predict_depth2\'],\n                \'predict_normal2\': self.netDM2_result[\'predict_normal2\'],\n                \'predict_rotation\': self.netDM2_result[\'predict_rotation\'],\n                \'predict_translation\': self.netDM2_result[\'predict_translation\'],\n                }\n        feed_dict = {\n                self.placeholder_image_pair: image_pair,\n                self.placeholder_image2_2: image2_2,\n                self.placeholder_depth2: depth2,\n                self.placeholder_normal2: normal2,\n                self.placeholder_rotation: rotation,\n                self.placeholder_translation: translation,\n                }\n        return self.session.run(fetches, feed_dict=feed_dict)\n\n\n\n\nclass RefinementNet:\n\n    def __init__(self, session):\n        """"""Creates the network\n\n        session: tf.Session\n            Tensorflow session\n\n        """"""\n\n        self.session = session\n        self.placeholder_image_pair = tf.placeholder(dtype=tf.float32, shape=(1,6,192,256))\n        self.placeholder_image1 = tf.placeholder(dtype=tf.float32, shape=(1,3,192,256))\n        self.placeholder_depth2 = tf.placeholder(dtype=tf.float32, shape=(1,1,48,64))\n        self.placeholder_normal2 = tf.placeholder(dtype=tf.float32, shape=(1,3,48,64))\n        self.placeholder_rotation = tf.placeholder(dtype=tf.float32, shape=(1,3))\n        self.placeholder_translation = tf.placeholder(dtype=tf.float32, shape=(1,3))\n\n        with tf.variable_scope(\'netRefine\'):\n            self.netRefine_result = depth_refine_block(\n                    image1=self.placeholder_image1, \n                    depthmotion_predictions={\n                        \'predict_depth2\': self.placeholder_depth2,\n                        \'predict_normal2\': self.placeholder_normal2,\n                        },\n                    )\n\n    def eval(self, image1, depth2, normal2):\n        """"""Runs the refinement network\n        \n        image1: numpy.ndarray\n            Array with the first image with shape [1,3,192,256]\n\n        depth2: numpy.ndarray\n            Depth prediction at resolution level 2\n\n        normal2: numpy.ndarray\n            normal prediction at resolution level 2\n\n        Returns a dict with the preditions of the refinement net\n        """"""\n\n        fetches = {\n                \'predict_depth0\': self.netRefine_result[\'predict_depth0\'],\n                \'predict_normal0\': self.netRefine_result[\'predict_normal0\'],\n                }\n        feed_dict = {\n                self.placeholder_image1: image1,\n                self.placeholder_depth2: depth2,\n                self.placeholder_normal2: normal2,\n                }\n        return self.session.run(fetches, feed_dict=feed_dict)\n\n\n\n\n'"
