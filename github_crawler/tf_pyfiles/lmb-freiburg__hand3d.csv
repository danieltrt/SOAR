file_path,api_count,code
create_binary_db.py,0,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n""""""\n    Script to convert Rendered Handpose Dataset into binary files,\n    which allows for much faster reading than plain image files.\n\n    Set ""path_to_db"" and ""set"" accordingly.\n\n    In order to use this file you need to download and unzip the dataset first.\n""""""\nfrom __future__ import print_function, unicode_literals\n\nimport pickle\nimport os\nimport scipy.misc\nimport struct\n\n# SET THIS to where RHD is located on your machine\npath_to_db = \'./RHD_published_v2/\'\n\n# chose if you want to create a binary for training or evaluation set\n# set = \'training\'\nset = \'evaluation\'\n\n### No more changes below this line ###\n\n\n# function to write the binary file\ndef write_to_binary(file_handle, image, mask, kp_coord_xyz, kp_coord_uv, kp_visible, K_mat):\n    """""""" Writes records to an open binary file. """"""\n    bytes_written = 0\n    # 1. write kp_coord_xyz\n    for coord in kp_coord_xyz:\n        file_handle.write(struct.pack(\'f\', coord[0]))\n        file_handle.write(struct.pack(\'f\', coord[1]))\n        file_handle.write(struct.pack(\'f\', coord[2]))\n    bytes_written += 4*kp_coord_xyz.shape[0]*kp_coord_xyz.shape[1]\n\n    # 2. write kp_coord_uv\n    for coord in kp_coord_uv:\n        file_handle.write(struct.pack(\'f\', coord[0]))\n        file_handle.write(struct.pack(\'f\', coord[1]))\n    bytes_written += 4*kp_coord_uv.shape[0]*kp_coord_uv.shape[1]\n\n    # 3. write camera intrinsic matrix\n    for K_row in K_mat:\n        for K_element in K_row:\n            file_handle.write(struct.pack(\'f\', K_element))\n    bytes_written += 4*9\n\n    file_handle.write(struct.pack(\'B\', 255))\n    file_handle.write(struct.pack(\'B\', 255))\n    bytes_written += 2\n\n    # 4. write image\n    for x in range(image.shape[0]):\n        for y in range(image.shape[1]):\n            file_handle.write(struct.pack(\'B\', image[x, y, 0]))\n            file_handle.write(struct.pack(\'B\', image[x, y, 1]))\n            file_handle.write(struct.pack(\'B\', image[x, y, 2]))\n    bytes_written += 4*image.shape[0]*image.shape[1]*image.shape[2]\n\n    # 5. write mask\n    for x in range(mask.shape[0]):\n        for y in range(mask.shape[1]):\n            file_handle.write(struct.pack(\'B\', mask[x, y]))\n    bytes_written += 4*mask.shape[0]*mask.shape[1]\n\n    # 6. write visibility\n    for x in range(kp_visible.shape[0]):\n        file_handle.write(struct.pack(\'B\', kp_visible[x]))\n    bytes_written += kp_visible.shape[0]\n\n    # print(\'bytes_written\', bytes_written)\n\n# binary file we will write\nfile_name_out = \'./data/bin/rhd_%s.bin\' % set\n\nif not os.path.exists(\'./data/bin\'):\n    os.mkdir(\'./data/bin\')\n\n# load annotations of this set\nwith open(os.path.join(path_to_db, set, \'anno_%s.pickle\' % set), \'rb\') as fi:\n    anno_all = pickle.load(fi)\n\n# iterate samples of the set and write to binary file\nwith open(file_name_out, \'wb\') as fo:\n    num_samples = len(anno_all.items())\n    for sample_id, anno in anno_all.items():\n        # load data\n        image = scipy.misc.imread(os.path.join(path_to_db, set, \'color\', \'%.5d.png\' % sample_id))\n        mask = scipy.misc.imread(os.path.join(path_to_db, set, \'mask\', \'%.5d.png\' % sample_id))\n\n        # get info from annotation dictionary\n        kp_coord_uv = anno[\'uv_vis\'][:, :2]  # u, v coordinates of 42 hand keypoints, pixel\n        kp_visible = anno[\'uv_vis\'][:, 2] == 1  # visibility of the keypoints, boolean\n        kp_coord_xyz = anno[\'xyz\']  # x, y, z coordinates of the keypoints, in meters\n        camera_intrinsic_matrix = anno[\'K\']  # matrix containing intrinsic parameters\n\n        write_to_binary(fo, image, mask, kp_coord_xyz, kp_coord_uv, kp_visible, camera_intrinsic_matrix)\n\n        if (sample_id % 100) == 0:\n            print(\'%d / %d images done: %.3f percent\' % (sample_id, num_samples, sample_id*100.0/num_samples))\n'"
eval2d.py,6,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n"""""" Script for evaluation of HandSegNet + PoseNet on full scale images.\n\n    To reproduce row 3 from Table 1 of the paper set USE_RETRAINED = False, use_wrist_coord=True, scale_to_size=True):\n    Net R-val AUC=0.663 EPE median=5.833 EPE mean=17.041\n\n    Using the correct evaluation setting (use_wrist_coord=False) leads to:\n    Net R-val AUC=0.679 EPE median=5.275 EPE mean=16.561\n\n    Using the correct evaluation setting and reporting results in the 320x320 frame of RHD\n    (use_wrist_coord=False, scale_to_size=True) frame leads to:\n    Net R-val AUC=0.635 EPE median=6.745 EPE mean=18.741\n""""""\nfrom __future__ import print_function, unicode_literals\nimport tensorflow as tf\nimport numpy as np\n\nfrom data.BinaryDbReader import *\nfrom nets.ColorHandPose3DNetwork import ColorHandPose3DNetwork\nfrom utils.general import detect_keypoints, trafo_coords, EvalUtil, load_weights_from_snapshot\n\n# flag that allows to load a retrained snapshot(original weights used in the paper are used otherwise)\nUSE_RETRAINED = False\nPATH_TO_POSENET_SNAPSHOTS = \'./snapshots_posenet/\'  # only used when USE_RETRAINED is true\nPATH_TO_HANDSEGNET_SNAPSHOTS = \'./snapshots_handsegnet/\'  # only used when USE_RETRAINED is true\n\n# get dataset\ndataset = BinaryDbReader(mode=\'evaluation\', shuffle=False, use_wrist_coord=True, scale_to_size=True)\n\n# build network graph\ndata = dataset.get()\n\n# build network\nnet = ColorHandPose3DNetwork()\n\n# scale input to common size for evaluation\nimage_scaled = tf.image.resize_images(data[\'image\'], (240, 320))\ns = data[\'image\'].get_shape().as_list()\nscale = (240.0/s[1], 320.0/s[2])\n\n# feed trough network\nkeypoints_scoremap, _, scale_crop, center = net.inference2d(image_scaled)\n\n# Start TF\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\ntf.train.start_queue_runners(sess=sess)\n\n# initialize network weights\nif USE_RETRAINED:\n    # retrained version: HandSegNet\n    last_cpt = tf.train.latest_checkpoint(PATH_TO_HANDSEGNET_SNAPSHOTS)\n    assert last_cpt is not None, ""Could not locate snapshot to load. Did you already train the network and set the path accordingly?""\n    load_weights_from_snapshot(sess, last_cpt, discard_list=[\'Adam\', \'global_step\', \'beta\'])\n\n    # retrained version: PoseNet\n    last_cpt = tf.train.latest_checkpoint(PATH_TO_POSENET_SNAPSHOTS)\n    assert last_cpt is not None, ""Could not locate snapshot to load. Did you already train the network and set the path accordingly?""\n    load_weights_from_snapshot(sess, last_cpt, discard_list=[\'Adam\', \'global_step\', \'beta\'])\nelse:\n    # load weights used in the paper\n    net.init(sess, weight_files=[\'./weights/handsegnet-rhd.pickle\',\n                                 \'./weights/posenet-rhd-stb.pickle\'], exclude_var_list=[\'PosePrior\', \'ViewpointNet\'])\n\nutil = EvalUtil()\n# iterate dataset\nfor i in range(dataset.num_samples):\n    # get prediction\n    keypoints_scoremap_v,\\\n    scale_crop_v, center_v, kp_uv21_gt, kp_vis = sess.run([keypoints_scoremap, scale_crop, center, data[\'keypoint_uv21\'], data[\'keypoint_vis21\']])\n\n    keypoints_scoremap_v = np.squeeze(keypoints_scoremap_v)\n    kp_uv21_gt = np.squeeze(kp_uv21_gt)\n    kp_vis = np.squeeze(kp_vis)\n\n    # detect keypoints\n    coord_hw_pred_crop = detect_keypoints(np.squeeze(keypoints_scoremap_v))\n    coord_hw_pred = trafo_coords(coord_hw_pred_crop, center_v, scale_crop_v, 256)\n    coord_uv_pred = np.stack([coord_hw_pred[:, 1], coord_hw_pred[:, 0]], 1)\n\n    # scale pred to image size of the dataset (to match with stored coordinates)\n    coord_uv_pred[:, 1] /= scale[0]\n    coord_uv_pred[:, 0] /= scale[1]\n\n    # some datasets are already stored with downsampled resolution\n    scale2orig_res = 1.0\n    if hasattr(dataset, \'resolution\'):\n        scale2orig_res = dataset.resolution\n\n    util.feed(kp_uv21_gt/scale2orig_res, kp_vis, coord_uv_pred/scale2orig_res)\n\n    if (i % 100) == 0:\n        print(\'%d / %d images done: %.3f percent\' % (i, dataset.num_samples, i*100.0/dataset.num_samples))\n\n# Output results\nmean, median, auc, _, _ = util.get_measures(0.0, 30.0, 20)\nprint(\'Evaluation results:\')\nprint(\'Average mean EPE: %.3f pixels\' % mean)\nprint(\'Average median EPE: %.3f pixels\' % median)\nprint(\'Area under curve: %.3f\' % auc)'"
eval2d_gt_cropped.py,6,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n"""""" Script for isolated evaluation of PoseNet on hand cropped images.\n    Ground truth keypoint annotations are used for crop generation.\n\n    This allows to reproduce row 1 from Table 1 of the paper:\n    GT R-val AUC=0.724 EPE median=5.001 EPE mean=9.135\n""""""\nfrom __future__ import print_function, unicode_literals\nimport tensorflow as tf\nimport numpy as np\n\nfrom data.BinaryDbReader import *\nfrom nets.ColorHandPose3DNetwork import ColorHandPose3DNetwork\nfrom utils.general import detect_keypoints, EvalUtil, load_weights_from_snapshot\n\n# flag that allows to load a retrained snapshot(original weights used in the paper are used otherwise)\nUSE_RETRAINED = False\nPATH_TO_SNAPSHOTS = \'./snapshots_posenet/\'  # only used when USE_RETRAINED is true\n\n# get dataset\ndataset = BinaryDbReader(mode=\'evaluation\', shuffle=False, hand_crop=True, use_wrist_coord=False)\n\n# build network graph\ndata = dataset.get()\n\n# build network\nevaluation = tf.placeholder_with_default(True, shape=())\nnet = ColorHandPose3DNetwork()\nkeypoints_scoremap = net.inference_pose2d(data[\'image_crop\'])\nkeypoints_scoremap = keypoints_scoremap[-1]\n\n# upscale to original size\ns = data[\'image_crop\'].get_shape().as_list()\nkeypoints_scoremap = tf.image.resize_images(keypoints_scoremap, (s[1], s[2]))\n\n# Start TF\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\ntf.train.start_queue_runners(sess=sess)\n\n# initialize network weights\nif USE_RETRAINED:\n    # retrained version\n    last_cpt = tf.train.latest_checkpoint(PATH_TO_SNAPSHOTS)\n    assert last_cpt is not None, ""Could not locate snapshot to load. Did you already train the network and set the path accordingly?""\n    load_weights_from_snapshot(sess, last_cpt, discard_list=[\'Adam\', \'global_step\', \'beta\'])\nelse:\n    # load weights used in the paper\n    net.init(sess, weight_files=[\'./weights/posenet-rhd-stb.pickle\'], exclude_var_list=[\'PosePrior\', \'ViewpointNet\'])\n\nutil = EvalUtil()\n# iterate dataset\nfor i in range(dataset.num_samples):\n    # get prediction\n    crop_scale, keypoints_scoremap_v, kp_uv21_gt, kp_vis = sess.run([data[\'crop_scale\'], keypoints_scoremap, data[\'keypoint_uv21\'], data[\'keypoint_vis21\']])\n\n    keypoints_scoremap_v = np.squeeze(keypoints_scoremap_v)\n    kp_uv21_gt = np.squeeze(kp_uv21_gt)\n    kp_vis = np.squeeze(kp_vis)\n    crop_scale = np.squeeze(crop_scale)\n\n    # detect keypoints\n    coord_hw_pred_crop = detect_keypoints(np.squeeze(keypoints_scoremap_v))\n    coord_uv_pred_crop = np.stack([coord_hw_pred_crop[:, 1], coord_hw_pred_crop[:, 0]], 1)\n\n    util.feed(kp_uv21_gt/crop_scale, kp_vis, coord_uv_pred_crop/crop_scale)\n\n    if (i % 100) == 0:\n        print(\'%d / %d images done: %.3f percent\' % (i, dataset.num_samples, i*100.0/dataset.num_samples))\n\nmean, median, auc, _, _ = util.get_measures(0.0, 30.0, 20)\nprint(\'Evaluation results:\')\nprint(\'Average mean EPE: %.3f pixels\' % mean)\nprint(\'Average median EPE: %.3f pixels\' % median)\nprint(\'Area under curve: %.3f\' % auc)\n'"
eval3d.py,5,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n"""""" Script for evaluation of different Lifting variants on full scale images.\n\n    This allows to reproduce Table 2 of the paper R-val ""Average median error"":\n    Method      | Number in the paper                   | Our result with TF 1.3\n\n    Direct      | 20.9                                  | 20.848 mm\n    Bottleneck  | Number in the paper is *not* correct  | 21.907 mm\n    Local       | 39.1                                  | 39.121 mm\n    Proposed    | 18.8                                  | 18.840 mm\n\n\n    Also there is one new variant that was not included in the paper as it is more current work.\n    Its the like local, but with the loss in xyz coordinate frame, which seems to work better:\n    Local with XYZ Loss  21.950 mm\n""""""""""\nfrom __future__ import print_function, unicode_literals\nimport tensorflow as tf\nimport numpy as np\n\nfrom data.BinaryDbReader import *\nfrom nets.PosePriorNetwork import PosePriorNetwork\nfrom utils.general import EvalUtil, load_weights_from_snapshot\n\n# Chose which variant to evaluate\nUSE_RETRAINED = False\nVARIANT = \'direct\'\n# VARIANT = \'bottleneck\'\n# VARIANT = \'local\'\n# VARIANT = \'local_w_xyz_loss\'\n# VARIANT = \'proposed\'\n\n# get dataset\ndataset = BinaryDbReader(mode=\'evaluation\', shuffle=False, hand_crop=True, use_wrist_coord=False)\n\n# build network graph\ndata = dataset.get()\n\n# build network\nnet = PosePriorNetwork(VARIANT)\n\n# feed through network\nevaluation = tf.placeholder_with_default(True, shape=())\ncoord3d_pred, _, _ = net.inference(data[\'scoremap\'], data[\'hand_side\'], evaluation)\n\ncoord3d_gt = data[\'keypoint_xyz21\']\n\n# Start TF\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\ntf.train.start_queue_runners(sess=sess)\n\n# initialize network with weights used in the paper\nif USE_RETRAINED:\n    # retrained version: HandSegNet\n    last_cpt = tf.train.latest_checkpoint(\'./snapshots_lifting_%s_drop/\' % VARIANT)\n    assert last_cpt is not None, ""Could not locate snapshot to load. Did you already train the network?""\n    load_weights_from_snapshot(sess, last_cpt, discard_list=[\'Adam\', \'global_step\', \'beta\'])\nelse:\n    net.init(sess, weight_files=[\'./weights/lifting-%s.pickle\' % VARIANT])\n\nutil = EvalUtil()\n# iterate dataset\nfor i in range(dataset.num_samples):\n    # get prediction\n    keypoint_xyz21, keypoint_scale, coord3d_pred_v = sess.run([data[\'keypoint_xyz21\'], data[\'keypoint_scale\'], coord3d_pred, ])\n\n    keypoint_xyz21 = np.squeeze(keypoint_xyz21)\n    keypoint_scale = np.squeeze(keypoint_scale)\n    coord3d_pred_v = np.squeeze(coord3d_pred_v)\n\n    # rescale to meters\n    coord3d_pred_v *= keypoint_scale\n\n    # center gt\n    keypoint_xyz21 -= keypoint_xyz21[0, :]\n\n    kp_vis = np.ones_like(keypoint_xyz21[:, 0])\n    util.feed(keypoint_xyz21, kp_vis, coord3d_pred_v)\n\n    if (i % 100) == 0:\n        print(\'%d / %d images done: %.3f percent\' % (i, dataset.num_samples, i*100.0/dataset.num_samples))\n\n# Output results\nmean, median, auc, _, _ = util.get_measures(0.0, 0.050, 20)\nprint(\'Evaluation results for %s:\' % VARIANT)\nprint(\'Average mean EPE: %.3f mm\' % (mean*1000))\nprint(\'Average median EPE: %.3f mm\' % (median*1000))\nprint(\'Area under curve: %.3f\' % auc)\n'"
eval_full.py,5,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n"""""" Script for evaluation of the full pipeline.\n\n    This allows to reproduce results of Figure 9. But in order to do so you need to get the STB dataset.\n    See README for more information.\n\n    Results for STB-e:\n    Average mean EPE: 12.210 mm\n    Average median EPE: 9.405 mm\n    Area under curve: 0.764 (from 0mm to 50mm)\n    Area under curve: 0.941 (from 30mm to 50mm)\n\n    Results for RHD-e (not in the paper, but a possible baseline for future approaches):\n    Average mean EPE: 35.606 mm\n    Average median EPE: 28.686 mm\n    Area under curve: 0.424 (from 0mm to 50mm)\n    Area under curve: 0.603 (from 30mm to 50mm)\n""""""""""\nfrom __future__ import print_function, unicode_literals\nimport tensorflow as tf\nimport numpy as np\n\nfrom data.BinaryDbReader import *\nfrom data.BinaryDbReaderSTB import *\nfrom nets.ColorHandPose3DNetwork import ColorHandPose3DNetwork\nfrom utils.general import EvalUtil, get_stb_ref_curves, calc_auc\n\n# get dataset\n# dataset = BinaryDbReader(mode=\'evaluation\', shuffle=False, use_wrist_coord=False)\ndataset = BinaryDbReaderSTB(mode=\'evaluation\', shuffle=False, use_wrist_coord=False)\n\n# build network graph\ndata = dataset.get()\nimage_scaled = tf.image.resize_images(data[\'image\'], (240, 320))\n\n# build network\nnet = ColorHandPose3DNetwork()\n\n# feed through network\nevaluation = tf.placeholder_with_default(True, shape=())\n_, _, _, _, _, coord3d_pred = net.inference(image_scaled, data[\'hand_side\'], evaluation)\ncoord3d_gt = data[\'keypoint_xyz21\']\n\n# Start TF\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\ntf.train.start_queue_runners(sess=sess)\n\n# initialize network with weights used in the paper\nnet.init(sess, weight_files=[\'./weights/handsegnet-rhd.pickle\',\n                             \'./weights/posenet3d-rhd-stb.pickle\'])\n\nutil = EvalUtil()\n# iterate dataset\nfor i in range(dataset.num_samples):\n    # get prediction\n    keypoint_xyz21, keypoint_vis21, keypoint_scale, coord3d_pred_v = sess.run([data[\'keypoint_xyz21\'], data[\'keypoint_vis21\'], data[\'keypoint_scale\'], coord3d_pred])\n\n    keypoint_xyz21 = np.squeeze(keypoint_xyz21)\n    keypoint_vis21 = np.squeeze(keypoint_vis21)\n    coord3d_pred_v = np.squeeze(coord3d_pred_v)\n    keypoint_scale = np.squeeze(keypoint_scale)\n\n    # rescale to meters\n    coord3d_pred_v *= keypoint_scale\n\n    # center gt\n    keypoint_xyz21 -= keypoint_xyz21[0, :]\n\n    util.feed(keypoint_xyz21, keypoint_vis21, coord3d_pred_v)\n\n    if (i % 100) == 0:\n        print(\'%d / %d images done: %.3f percent\' % (i, dataset.num_samples, i*100.0/dataset.num_samples))\n\n# Output results\nmean, median, auc, pck_curve_all, threshs = util.get_measures(0.0, 0.050, 20)  # rainier: Should lead to 0.764 / 9.405 / 12.210\nprint(\'Evaluation results\')\nprint(\'Average mean EPE: %.3f mm\' % (mean*1000))\nprint(\'Average median EPE: %.3f mm\' % (median*1000))\nprint(\'Area under curve between 0mm - 50mm: %.3f\' % auc)\n\n# only use subset that lies in 20mm .. 50mm\npck_curve_all, threshs = pck_curve_all[8:], threshs[8:]*1000.0\nauc_subset = calc_auc(threshs, pck_curve_all)\nprint(\'Area under curve between 20mm - 50mm: %.3f\' % auc_subset)\n\n# Show Figure 9 from the paper\nif type(dataset) == BinaryDbReaderSTB:\n\n    import matplotlib.pyplot as plt\n    curve_list = get_stb_ref_curves()\n    curve_list.append((threshs, pck_curve_all, \'Ours (AUC=%.3f)\' % auc_subset))\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    for t, v, name in curve_list:\n        ax.plot(t, v, label=name)\n    ax.set_xlabel(\'threshold in mm\')\n    ax.set_ylabel(\'PCK\')\n    plt.legend(loc=\'lower right\')\n    plt.show()\n'"
run.py,5,"b""#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom __future__ import print_function, unicode_literals\n\nimport tensorflow as tf\nimport numpy as np\nimport scipy.misc\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom nets.ColorHandPose3DNetwork import ColorHandPose3DNetwork\nfrom utils.general import detect_keypoints, trafo_coords, plot_hand, plot_hand_3d\n\nif __name__ == '__main__':\n    # images to be shown\n    image_list = list()\n    image_list.append('./data/img.png')\n    image_list.append('./data/img2.png')\n    image_list.append('./data/img3.png')\n    image_list.append('./data/img4.png')\n    image_list.append('./data/img5.png')\n\n    # network input\n    image_tf = tf.placeholder(tf.float32, shape=(1, 240, 320, 3))\n    hand_side_tf = tf.constant([[1.0, 0.0]])  # left hand (true for all samples provided)\n    evaluation = tf.placeholder_with_default(True, shape=())\n\n    # build network\n    net = ColorHandPose3DNetwork()\n    hand_scoremap_tf, image_crop_tf, scale_tf, center_tf,\\\n    keypoints_scoremap_tf, keypoint_coord3d_tf = net.inference(image_tf, hand_side_tf, evaluation)\n\n    # Start TF\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n\n    # initialize network\n    net.init(sess)\n\n    # Feed image list through network\n    for img_name in image_list:\n        image_raw = scipy.misc.imread(img_name)\n        image_raw = scipy.misc.imresize(image_raw, (240, 320))\n        image_v = np.expand_dims((image_raw.astype('float') / 255.0) - 0.5, 0)\n\n        hand_scoremap_v, image_crop_v, scale_v, center_v,\\\n        keypoints_scoremap_v, keypoint_coord3d_v = sess.run([hand_scoremap_tf, image_crop_tf, scale_tf, center_tf,\n                                                             keypoints_scoremap_tf, keypoint_coord3d_tf],\n                                                            feed_dict={image_tf: image_v})\n\n        hand_scoremap_v = np.squeeze(hand_scoremap_v)\n        image_crop_v = np.squeeze(image_crop_v)\n        keypoints_scoremap_v = np.squeeze(keypoints_scoremap_v)\n        keypoint_coord3d_v = np.squeeze(keypoint_coord3d_v)\n\n        # post processing\n        image_crop_v = ((image_crop_v + 0.5) * 255).astype('uint8')\n        coord_hw_crop = detect_keypoints(np.squeeze(keypoints_scoremap_v))\n        coord_hw = trafo_coords(coord_hw_crop, center_v, scale_v, 256)\n\n        # visualize\n        fig = plt.figure(1)\n        ax1 = fig.add_subplot(221)\n        ax2 = fig.add_subplot(222)\n        ax3 = fig.add_subplot(223)\n        ax4 = fig.add_subplot(224, projection='3d')\n        ax1.imshow(image_raw)\n        plot_hand(coord_hw, ax1)\n        ax2.imshow(image_crop_v)\n        plot_hand(coord_hw_crop, ax2)\n        ax3.imshow(np.argmax(hand_scoremap_v, 2))\n        plot_hand_3d(keypoint_coord3d_v, ax4)\n        ax4.view_init(azim=-90.0, elev=-90.0)  # aligns the 3d coord with the camera view\n        ax4.set_xlim([-3, 3])\n        ax4.set_ylim([-3, 1])\n        ax4.set_zlim([-3, 3])\n        plt.show()\n"""
training_handsegnet.py,11,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom __future__ import print_function, unicode_literals\n\nimport tensorflow as tf\nimport os\nimport sys\n\nfrom nets.ColorHandPose3DNetwork import ColorHandPose3DNetwork\nfrom data.BinaryDbReader import BinaryDbReader\nfrom utils.general import LearningRateScheduler, load_weights_from_snapshot\n\n# training parameters\ntrain_para = {\'lr\': [1e-5, 1e-6, 1e-7],\n              \'lr_iter\': [20000, 30000],\n              \'max_iter\': 40000,\n              \'show_loss_freq\': 1000,\n              \'snapshot_freq\': 5000,\n              \'snapshot_dir\': \'snapshots_handsegnet\'}\n\n# get dataset\ndataset = BinaryDbReader(mode=\'training\',\n                         batch_size=8, shuffle=True,\n                         hue_aug=True, random_crop_to_size=True)\n\n# build network graph\ndata = dataset.get()\n\n# build network\nevaluation = tf.placeholder_with_default(True, shape=())\nnet = ColorHandPose3DNetwork()\nhand_mask_pred = net.inference_detection(data[\'image\'], train=True)\n\n# Start TF\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\ntf.train.start_queue_runners(sess=sess)\n\n# Loss\nloss = 0.0\ns = data[\'hand_mask\'].get_shape().as_list()\nfor i, pred_item in enumerate(hand_mask_pred):\n    gt = tf.reshape(data[\'hand_mask\'], [s[0]*s[1]*s[2], -1])\n    pred = tf.reshape(hand_mask_pred, [s[0]*s[1]*s[2], -1])\n    loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=gt))\n\n# Solver\nglobal_step = tf.Variable(0, trainable=False, name=""global_step"")\nlr_scheduler = LearningRateScheduler(values=train_para[\'lr\'], steps=train_para[\'lr_iter\'])\nlr = lr_scheduler.get_lr(global_step)\nopt = tf.train.AdamOptimizer(lr)\ntrain_op = opt.minimize(loss)\n\n# init weights\nsess.run(tf.global_variables_initializer())\nsaver = tf.train.Saver(max_to_keep=1, keep_checkpoint_every_n_hours=4.0)\n\nrename_dict = {\'CPM/PersonNet\': \'HandSegNet\',\n               \'_CPM\': \'\'}\nload_weights_from_snapshot(sess, \'./weights/cpm-model-mpii\', [\'PoseNet\', \'Mconv\', \'conv6\'], rename_dict)\n\n# snapshot dir\nif not os.path.exists(train_para[\'snapshot_dir\']):\n    os.mkdir(train_para[\'snapshot_dir\'])\n    print(\'Created snapshot dir:\', train_para[\'snapshot_dir\'])\n\n# Training loop\nfor i in range(train_para[\'max_iter\']):\n    _, loss_v = sess.run([train_op, loss])\n\n    if (i % train_para[\'show_loss_freq\']) == 0:\n        print(\'Iteration %d\\t Loss %.1e\' % (i, loss_v))\n        sys.stdout.flush()\n\n    if (i % train_para[\'snapshot_freq\']) == 0:\n        saver.save(sess, ""%s/model"" % train_para[\'snapshot_dir\'], global_step=i)\n        print(\'Saved a snapshot.\')\n        sys.stdout.flush()\n\n\nprint(\'Training finished. Saving final snapshot.\')\nsaver.save(sess, ""%s/model"" % train_para[\'snapshot_dir\'], global_step=train_para[\'max_iter\'])\n'"
training_lifting.py,13,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom __future__ import print_function, unicode_literals\n\nimport tensorflow as tf\nimport os\nimport sys\n\nfrom nets.PosePriorNetwork import PosePriorNetwork\nfrom data.BinaryDbReader import BinaryDbReader\nfrom utils.general import LearningRateScheduler\n\n# Chose which variant to evaluate\n# VARIANT = \'direct\'\n# VARIANT = \'bottleneck\'\n# VARIANT = \'local\'\n# VARIANT = \'local_w_xyz_loss\'\nVARIANT = \'proposed\'\n\n# training parameters\ntrain_para = {\'lr\': [1e-5, 1e-6],\n              \'lr_iter\': [60000],\n              \'max_iter\': 80000,\n              \'show_loss_freq\': 1000,\n              \'snapshot_freq\': 5000,\n              \'snapshot_dir\': \'snapshots_lifting_%s\' % VARIANT}\n\n# get dataset\ndataset = BinaryDbReader(mode=\'training\',\n                         batch_size=8, shuffle=True, hand_crop=True, use_wrist_coord=False,\n                         coord_uv_noise=True, crop_center_noise=True, crop_offset_noise=True, crop_scale_noise=True)\n\n# build network graph\ndata = dataset.get()\n\n# build network\nnet = PosePriorNetwork(VARIANT)\n\n# feed trough network\nevaluation = tf.placeholder_with_default(True, shape=())\n_, coord3d_pred, R = net.inference(data[\'scoremap\'], data[\'hand_side\'], evaluation)\n\n# Start TF\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\ntf.train.start_queue_runners(sess=sess)\n\n# Loss\nloss = 0.0\nif (VARIANT == \'direct\') or (VARIANT == \'bottleneck\'):\n    loss = tf.reduce_mean(tf.square(coord3d_pred - data[\'keypoint_xyz21_normed\']))\nelif VARIANT == \'local\':\n    loss += tf.reduce_mean(tf.square(coord3d_pred - data[\'keypoint_xyz21_local\']))\nelif VARIANT == \'local_w_xyz_loss\':\n    from utils.relative_trafo import bone_rel_trafo_inv\n\n    # transform the local coordinates back into xyz for the loss\n    coord3d_pred_xyz = bone_rel_trafo_inv(coord3d_pred)\n    loss += tf.reduce_mean(tf.square(coord3d_pred_xyz - data[\'keypoint_xyz21_normed\']))\nelif VARIANT == \'proposed\':\n    loss += tf.reduce_mean(tf.square(coord3d_pred - data[\'keypoint_xyz21_can\']))\n    loss += tf.reduce_mean(tf.square(R - data[\'rot_mat\']))\n\n# Solver\nglobal_step = tf.Variable(0, trainable=False, name=""global_step"")\nlr_scheduler = LearningRateScheduler(values=train_para[\'lr\'], steps=train_para[\'lr_iter\'])\nlr = lr_scheduler.get_lr(global_step)\nopt = tf.train.AdamOptimizer(lr)\ntrain_op = opt.minimize(loss)\n\n# init weights\nsess.run(tf.global_variables_initializer())\nsaver = tf.train.Saver(max_to_keep=1, keep_checkpoint_every_n_hours=4.0)\n\n# snapshot dir\nif not os.path.exists(train_para[\'snapshot_dir\']):\n    os.mkdir(train_para[\'snapshot_dir\'])\n    print(\'Created snapshot dir:\', train_para[\'snapshot_dir\'])\n\n# Training loop\nprint(\'Starting to train ...\')\nfor i in range(train_para[\'max_iter\']):\n    _, loss_v = sess.run([train_op, loss])\n\n    if (i % train_para[\'show_loss_freq\']) == 0:\n        print(\'Iteration %d\\t Loss %.1e\' % (i, loss_v))\n        sys.stdout.flush()\n\n    if (i % train_para[\'snapshot_freq\']) == 0:\n        saver.save(sess, ""%s/model"" % train_para[\'snapshot_dir\'], global_step=i)\n        print(\'Saved a snapshot.\')\n        sys.stdout.flush()\n\n\nprint(\'Training finished. Saving final snapshot.\')\nsaver.save(sess, ""%s/model"" % train_para[\'snapshot_dir\'], global_step=train_para[\'max_iter\'])\n'"
training_posenet.py,11,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom __future__ import print_function, unicode_literals\n\nimport tensorflow as tf\nimport os\nimport sys\n\nfrom nets.ColorHandPose3DNetwork import ColorHandPose3DNetwork\nfrom data.BinaryDbReader import BinaryDbReader\nfrom utils.general import LearningRateScheduler, load_weights_from_snapshot\n\n# training parameters\ntrain_para = {\'lr\': [1e-4, 1e-5, 1e-6],\n              \'lr_iter\': [10000, 20000],\n              \'max_iter\': 30000,\n              \'show_loss_freq\': 1000,\n              \'snapshot_freq\': 5000,\n              \'snapshot_dir\': \'snapshots_posenet\'}\n\n# get dataset\ndataset = BinaryDbReader(mode=\'training\',\n                         batch_size=8, shuffle=True, use_wrist_coord=False,\n                         hand_crop=True, coord_uv_noise=True, crop_center_noise=True)\n\n# build network graph\ndata = dataset.get()\n\n# build network\nevaluation = tf.placeholder_with_default(True, shape=())\nnet = ColorHandPose3DNetwork()\nkeypoints_scoremap = net.inference_pose2d(data[\'image_crop\'], train=True)\ns = data[\'scoremap\'].get_shape().as_list()\nkeypoints_scoremap = [tf.image.resize_images(x, (s[1], s[2])) for x in keypoints_scoremap]\n\n# Start TF\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\ntf.train.start_queue_runners(sess=sess)\n\n# Loss\nloss = 0.0\ns = data[\'scoremap\'].get_shape().as_list()\nvis = tf.cast(tf.reshape(data[\'keypoint_vis21\'], [s[0], s[3]]), tf.float32)\nfor i, pred_item in enumerate(keypoints_scoremap):\n    loss += tf.reduce_sum(vis * tf.sqrt(tf.reduce_mean(tf.square(pred_item - data[\'scoremap\']), [1, 2]))) / (tf.reduce_sum(vis) + 0.001)\n\n# Solver\nglobal_step = tf.Variable(0, trainable=False, name=""global_step"")\nlr_scheduler = LearningRateScheduler(values=train_para[\'lr\'], steps=train_para[\'lr_iter\'])\nlr = lr_scheduler.get_lr(global_step)\nopt = tf.train.AdamOptimizer(lr)\ntrain_op = opt.minimize(loss)\n\n# init weights\nsess.run(tf.global_variables_initializer())\nsaver = tf.train.Saver(max_to_keep=1, keep_checkpoint_every_n_hours=4.0)\n\nrename_dict = {\'CPM/PoseNet\': \'PoseNet2D\',\n               \'_CPM\': \'\'}\nload_weights_from_snapshot(sess, \'./weights/cpm-model-mpii\', [\'PersonNet\', \'PoseNet/Mconv\', \'conv5_2_CPM\'], rename_dict)\n\n# snapshot dir\nif not os.path.exists(train_para[\'snapshot_dir\']):\n    os.mkdir(train_para[\'snapshot_dir\'])\n    print(\'Created snapshot dir:\', train_para[\'snapshot_dir\'])\n\n# Training loop\nprint(\'Starting to train ...\')\nfor i in range(train_para[\'max_iter\']):\n    _, loss_v = sess.run([train_op, loss])\n\n    if (i % train_para[\'show_loss_freq\']) == 0:\n        print(\'Iteration %d\\t Loss %.1e\' % (i, loss_v))\n        sys.stdout.flush()\n\n    if (i % train_para[\'snapshot_freq\']) == 0:\n        saver.save(sess, ""%s/model"" % train_para[\'snapshot_dir\'], global_step=i)\n        print(\'Saved a snapshot.\')\n        sys.stdout.flush()\n\n\nprint(\'Training finished. Saving final snapshot.\')\nsaver.save(sess, ""%s/model"" % train_para[\'snapshot_dir\'], global_step=train_para[\'max_iter\'])\n'"
data/BinaryDbReader.py,114,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom __future__ import print_function, unicode_literals\nimport os\n\nimport tensorflow as tf\n\nfrom utils.general import crop_image_from_xy\nfrom utils.canonical_trafo import canonical_trafo, flip_right_hand\nfrom utils.relative_trafo import bone_rel_trafo\n\n\nclass BinaryDbReader(object):\n    """"""\n        Reads data from a binary dataset created by create_binary_db.py\n    """"""\n    def __init__(self, mode=None, batch_size=1, shuffle=True, use_wrist_coord=True, sigma=25.0, hand_crop=False,\n                 random_crop_to_size=False,\n                 scale_to_size=False,\n                 hue_aug=False,\n                 coord_uv_noise=False,\n                 crop_center_noise=False, crop_scale_noise=False, crop_offset_noise=False,\n                 scoremap_dropout=False):\n        """""" Inputs:\n                mode: string, Indicates which binary file to read. Can be \'training\' or \'evaluation\'\n                batch_size: int, Number of samples forming a batch\n                shuffle: boolean, If true samples of binary file are shuffled while reading\n                use_wrist_coord: boolean, When true keypoint #0 is the wrist, palm center otherwise\n                hand_crop: boolean, When true calculates a tight hand crop using the gt keypoint annotations\n                                    Updates/Sets the following output items: image_crop, crop_scale, scoremap, cam_mat, keypoint_uv21\n                sigma: float, Size of the ground truth scoremaps\n                random_crop_to_size: boolean, Takes randomly sampled crops from the image & the mask\n                scale_to_size: boolean, Scales down image and keypoints\n                hue_aug: boolean, Random hue augmentation\n                coord_uv_noise: boolean, Adds some gaussian noise on the gt uv coordinate\n                crop_center_noise: boolean, Adds gaussian noise on the hand crop center location (all keypoints still lie within the crop)\n                crop_scale_noise: boolean, Adds gaussian noise on the hand crop size\n                crop_offset_noise: boolean, Offsets the hand crop center randomly (keypoints can lie outside the crop)\n                scoremap_dropout: boolean, Randomly drop scoremap channels\n        """"""\n        self.path_to_db = \'./data/bin/\'\n\n        self.num_samples = 0\n        if mode == \'training\':\n            self.path_to_db += \'rhd_training.bin\'\n            self.num_samples = 41258\n        elif mode == \'evaluation\':\n            self.path_to_db += \'rhd_evaluation.bin\'\n            self.num_samples = 2728\n        else:\n            assert 0, ""Unknown dataset mode.""\n\n        assert os.path.exists(self.path_to_db), ""Could not find the binary data file!""\n\n        # general parameters\n        self.batch_size = batch_size\n        self.sigma = sigma\n        self.shuffle = shuffle\n        self.use_wrist_coord = use_wrist_coord\n        self.random_crop_to_size = random_crop_to_size\n        self.random_crop_size = 256\n        self.scale_to_size = scale_to_size\n        self.scale_target_size = (240, 320)  # size its scaled down to if scale_to_size=True\n\n        # data augmentation parameters\n        self.hue_aug = hue_aug\n        self.hue_aug_max = 0.1\n\n        self.hand_crop = hand_crop\n        self.coord_uv_noise = coord_uv_noise\n        self.coord_uv_noise_sigma = 2.5  # std dev in px of noise on the uv coordinates\n        self.crop_center_noise = crop_center_noise\n        self.crop_center_noise_sigma = 20.0  # std dev in px: this moves what is in the ""center"", but the crop always contains all keypoints\n\n        self.crop_scale_noise = crop_scale_noise\n        self.crop_offset_noise = crop_offset_noise\n        self.crop_offset_noise_sigma = 10.0  # translates the crop after size calculation (this can move keypoints outside)\n        self.scoremap_dropout = scoremap_dropout\n        self.scoremap_dropout_prob = 0.8\n\n        # these are constants of the dataset and therefore must not be changed\n        self.image_size = (320, 320)\n        self.crop_size = 256\n        self.num_kp = 42\n\n    def get(self):\n        """""" Provides input data to the graph. """"""\n        # calculate size of each record (this lists what is contained in the db and how many bytes are occupied)\n        record_bytes = 2\n\n        encoding_bytes = 4\n        kp_xyz_entries = 3 * self.num_kp\n        record_bytes += encoding_bytes*kp_xyz_entries\n\n        encoding_bytes = 4\n        kp_uv_entries = 2 * self.num_kp\n        record_bytes += encoding_bytes*kp_uv_entries\n\n        cam_matrix_entries = 9\n        record_bytes += encoding_bytes*cam_matrix_entries\n\n        image_bytes = self.image_size[0] * self.image_size[1] * 3\n        record_bytes += image_bytes\n\n        hand_parts_bytes = self.image_size[0] * self.image_size[1]\n        record_bytes += hand_parts_bytes\n\n        kp_vis_bytes = self.num_kp\n        record_bytes += kp_vis_bytes\n\n        """""" READ DATA ITEMS""""""\n        # Start reader\n        reader = tf.FixedLengthRecordReader(header_bytes=0, record_bytes=record_bytes)\n        _, value = reader.read(tf.train.string_input_producer([self.path_to_db]))\n\n        # decode to floats\n        bytes_read = 0\n        data_dict = dict()\n        record_bytes_float32 = tf.decode_raw(value, tf.float32)\n\n        # 1. Read keypoint xyz\n        keypoint_xyz = tf.reshape(tf.slice(record_bytes_float32, [bytes_read//4], [kp_xyz_entries]), [self.num_kp, 3])\n        bytes_read += encoding_bytes*kp_xyz_entries\n\n        # calculate palm coord\n        if not self.use_wrist_coord:\n            palm_coord_l = tf.expand_dims(0.5*(keypoint_xyz[0, :] + keypoint_xyz[12, :]), 0)\n            palm_coord_r = tf.expand_dims(0.5*(keypoint_xyz[21, :] + keypoint_xyz[33, :]), 0)\n            keypoint_xyz = tf.concat([palm_coord_l, keypoint_xyz[1:21, :], palm_coord_r, keypoint_xyz[-20:, :]], 0)\n\n        data_dict[\'keypoint_xyz\'] = keypoint_xyz\n\n        # 2. Read keypoint uv\n        keypoint_uv = tf.cast(tf.reshape(tf.slice(record_bytes_float32, [bytes_read//4], [kp_uv_entries]), [self.num_kp, 2]), tf.int32)\n        bytes_read += encoding_bytes*kp_uv_entries\n\n        keypoint_uv = tf.cast(keypoint_uv, tf.float32)\n\n        # calculate palm coord\n        if not self.use_wrist_coord:\n            palm_coord_uv_l = tf.expand_dims(0.5*(keypoint_uv[0, :] + keypoint_uv[12, :]), 0)\n            palm_coord_uv_r = tf.expand_dims(0.5*(keypoint_uv[21, :] + keypoint_uv[33, :]), 0)\n            keypoint_uv = tf.concat([palm_coord_uv_l, keypoint_uv[1:21, :], palm_coord_uv_r, keypoint_uv[-20:, :]], 0)\n\n        if self.coord_uv_noise:\n            noise = tf.truncated_normal([42, 2], mean=0.0, stddev=self.coord_uv_noise_sigma)\n            keypoint_uv += noise\n\n        data_dict[\'keypoint_uv\'] = keypoint_uv\n\n        # 3. Camera intrinsics\n        cam_mat = tf.reshape(tf.slice(record_bytes_float32, [bytes_read//4], [cam_matrix_entries]), [3, 3])\n        bytes_read += encoding_bytes*cam_matrix_entries\n        data_dict[\'cam_mat\'] = cam_mat\n\n        # decode to uint8\n        bytes_read += 2\n        record_bytes_uint8 = tf.decode_raw(value, tf.uint8)\n\n        # 4. Read image\n        image = tf.reshape(tf.slice(record_bytes_uint8, [bytes_read], [image_bytes]),\n                               [self.image_size[0], self.image_size[1], 3])\n        image = tf.cast(image, tf.float32)\n        bytes_read += image_bytes\n\n        # subtract mean\n        image = image / 255.0 - 0.5\n        if self.hue_aug:\n            image = tf.image.random_hue(image, self.hue_aug_max)\n        data_dict[\'image\'] = image\n\n        # 5. Read mask\n        hand_parts_mask = tf.reshape(tf.slice(record_bytes_uint8, [bytes_read], [hand_parts_bytes]),\n                               [self.image_size[0], self.image_size[1]])\n        hand_parts_mask = tf.cast(hand_parts_mask, tf.int32)\n        bytes_read += hand_parts_bytes\n        data_dict[\'hand_parts\'] = hand_parts_mask\n        hand_mask = tf.greater(hand_parts_mask, 1)\n        bg_mask = tf.logical_not(hand_mask)\n        data_dict[\'hand_mask\'] = tf.cast(tf.stack([bg_mask, hand_mask], 2), tf.int32)\n\n        # 6. Read visibilty\n        keypoint_vis = tf.reshape(tf.slice(record_bytes_uint8, [bytes_read], [kp_vis_bytes]),\n                               [self.num_kp])\n        keypoint_vis = tf.cast(keypoint_vis, tf.bool)\n        bytes_read += kp_vis_bytes\n\n        # calculate palm visibility\n        if not self.use_wrist_coord:\n            palm_vis_l = tf.expand_dims(tf.logical_or(keypoint_vis[0], keypoint_vis[12]), 0)\n            palm_vis_r = tf.expand_dims(tf.logical_or(keypoint_vis[21], keypoint_vis[33]), 0)\n            keypoint_vis = tf.concat([palm_vis_l, keypoint_vis[1:21], palm_vis_r, keypoint_vis[-20:]], 0)\n        data_dict[\'keypoint_vis\'] = keypoint_vis\n\n        assert bytes_read == record_bytes, ""Doesnt add up.""\n\n        """""" DEPENDENT DATA ITEMS: SUBSET of 21 keypoints""""""\n        # figure out dominant hand by analysis of the segmentation mask\n        one_map, zero_map = tf.ones_like(hand_parts_mask), tf.zeros_like(hand_parts_mask)\n        cond_l = tf.logical_and(tf.greater(hand_parts_mask, one_map), tf.less(hand_parts_mask, one_map*18))\n        cond_r = tf.greater(hand_parts_mask, one_map*17)\n        hand_map_l = tf.where(cond_l, one_map, zero_map)\n        hand_map_r = tf.where(cond_r, one_map, zero_map)\n        num_px_left_hand = tf.reduce_sum(hand_map_l)\n        num_px_right_hand = tf.reduce_sum(hand_map_r)\n\n        # PRODUCE the 21 subset using the segmentation masks\n        # We only deal with the more prominent hand for each frame and discard the second set of keypoints\n        kp_coord_xyz_left = keypoint_xyz[:21, :]\n        kp_coord_xyz_right = keypoint_xyz[-21:, :]\n\n        cond_left = tf.logical_and(tf.cast(tf.ones_like(kp_coord_xyz_left), tf.bool), tf.greater(num_px_left_hand, num_px_right_hand))\n        kp_coord_xyz21 = tf.where(cond_left, kp_coord_xyz_left, kp_coord_xyz_right)\n\n        hand_side = tf.where(tf.greater(num_px_left_hand, num_px_right_hand),\n                             tf.constant(0, dtype=tf.int32),\n                             tf.constant(1, dtype=tf.int32))  # left hand = 0; right hand = 1\n        data_dict[\'hand_side\'] = tf.one_hot(hand_side, depth=2, on_value=1.0, off_value=0.0, dtype=tf.float32)\n\n        data_dict[\'keypoint_xyz21\'] = kp_coord_xyz21\n\n        # make coords relative to root joint\n        kp_coord_xyz_root = kp_coord_xyz21[0, :] # this is the palm coord\n        kp_coord_xyz21_rel = kp_coord_xyz21 - kp_coord_xyz_root  # relative coords in metric coords\n        index_root_bone_length = tf.sqrt(tf.reduce_sum(tf.square(kp_coord_xyz21_rel[12, :] - kp_coord_xyz21_rel[11, :])))\n        data_dict[\'keypoint_scale\'] = index_root_bone_length\n        data_dict[\'keypoint_xyz21_normed\'] = kp_coord_xyz21_rel / index_root_bone_length  # normalized by length of 12->11\n\n        # calculate local coordinates\n        kp_coord_xyz21_local = bone_rel_trafo(data_dict[\'keypoint_xyz21_normed\'])\n        kp_coord_xyz21_local = tf.squeeze(kp_coord_xyz21_local)\n        data_dict[\'keypoint_xyz21_local\'] = kp_coord_xyz21_local\n\n        # calculate viewpoint and coords in canonical coordinates\n        kp_coord_xyz21_rel_can, rot_mat = canonical_trafo(data_dict[\'keypoint_xyz21_normed\'])\n        kp_coord_xyz21_rel_can, rot_mat = tf.squeeze(kp_coord_xyz21_rel_can), tf.squeeze(rot_mat)\n        kp_coord_xyz21_rel_can = flip_right_hand(kp_coord_xyz21_rel_can, tf.logical_not(cond_left))\n        data_dict[\'keypoint_xyz21_can\'] = kp_coord_xyz21_rel_can\n        data_dict[\'rot_mat\'] = tf.matrix_inverse(rot_mat)\n\n        # Set of 21 for visibility\n        keypoint_vis_left = keypoint_vis[:21]\n        keypoint_vis_right = keypoint_vis[-21:]\n        keypoint_vis21 = tf.where(cond_left[:, 0], keypoint_vis_left, keypoint_vis_right)\n        data_dict[\'keypoint_vis21\'] = keypoint_vis21\n\n        # Set of 21 for UV coordinates\n        keypoint_uv_left = keypoint_uv[:21, :]\n        keypoint_uv_right = keypoint_uv[-21:, :]\n        keypoint_uv21 = tf.where(cond_left[:, :2], keypoint_uv_left, keypoint_uv_right)\n        data_dict[\'keypoint_uv21\'] = keypoint_uv21\n\n        """""" DEPENDENT DATA ITEMS: HAND CROP """"""\n        if self.hand_crop:\n            crop_center = keypoint_uv21[12, ::-1]\n\n            # catch problem, when no valid kp available (happens almost never)\n            crop_center = tf.cond(tf.reduce_all(tf.is_finite(crop_center)), lambda: crop_center,\n                                  lambda: tf.constant([0.0, 0.0]))\n            crop_center.set_shape([2, ])\n\n            if self.crop_center_noise:\n                noise = tf.truncated_normal([2], mean=0.0, stddev=self.crop_center_noise_sigma)\n                crop_center += noise\n\n            crop_scale_noise = tf.constant(1.0)\n            if self.crop_scale_noise:\n                    crop_scale_noise = tf.squeeze(tf.random_uniform([1], minval=1.0, maxval=1.2))\n\n            # select visible coords only\n            kp_coord_h = tf.boolean_mask(keypoint_uv21[:, 1], keypoint_vis21)\n            kp_coord_w = tf.boolean_mask(keypoint_uv21[:, 0], keypoint_vis21)\n            kp_coord_hw = tf.stack([kp_coord_h, kp_coord_w], 1)\n\n            # determine size of crop (measure spatial extend of hw coords first)\n            min_coord = tf.maximum(tf.reduce_min(kp_coord_hw, 0), 0.0)\n            max_coord = tf.minimum(tf.reduce_max(kp_coord_hw, 0), self.image_size)\n\n            # find out larger distance wrt the center of crop\n            crop_size_best = 2*tf.maximum(max_coord - crop_center, crop_center - min_coord)\n            crop_size_best = tf.reduce_max(crop_size_best)\n            crop_size_best = tf.minimum(tf.maximum(crop_size_best, 50.0), 500.0)\n\n            # catch problem, when no valid kp available\n            crop_size_best = tf.cond(tf.reduce_all(tf.is_finite(crop_size_best)), lambda: crop_size_best,\n                                  lambda: tf.constant(200.0))\n            crop_size_best.set_shape([])\n\n            # calculate necessary scaling\n            scale = tf.cast(self.crop_size, tf.float32) / crop_size_best\n            scale = tf.minimum(tf.maximum(scale, 1.0), 10.0)\n            scale *= crop_scale_noise\n            data_dict[\'crop_scale\'] = scale\n\n            if self.crop_offset_noise:\n                noise = tf.truncated_normal([2], mean=0.0, stddev=self.crop_offset_noise_sigma)\n                crop_center += noise\n\n            # Crop image\n            img_crop = crop_image_from_xy(tf.expand_dims(image, 0), crop_center, self.crop_size, scale)\n            data_dict[\'image_crop\'] = tf.squeeze(img_crop)\n\n            # Modify uv21 coordinates\n            crop_center_float = tf.cast(crop_center, tf.float32)\n            keypoint_uv21_u = (keypoint_uv21[:, 0] - crop_center_float[1]) * scale + self.crop_size // 2\n            keypoint_uv21_v = (keypoint_uv21[:, 1] - crop_center_float[0]) * scale + self.crop_size // 2\n            keypoint_uv21 = tf.stack([keypoint_uv21_u, keypoint_uv21_v], 1)\n            data_dict[\'keypoint_uv21\'] = keypoint_uv21\n\n            # Modify camera intrinsics\n            scale = tf.reshape(scale, [1, ])\n            scale_matrix = tf.dynamic_stitch([[0], [1], [2],\n                                              [3], [4], [5],\n                                              [6], [7], [8]], [scale, [0.0], [0.0],\n                                                               [0.0], scale, [0.0],\n                                                               [0.0], [0.0], [1.0]])\n            scale_matrix = tf.reshape(scale_matrix, [3, 3])\n\n            crop_center_float = tf.cast(crop_center, tf.float32)\n            trans1 = crop_center_float[0] * scale - self.crop_size // 2\n            trans2 = crop_center_float[1] * scale - self.crop_size // 2\n            trans1 = tf.reshape(trans1, [1, ])\n            trans2 = tf.reshape(trans2, [1, ])\n            trans_matrix = tf.dynamic_stitch([[0], [1], [2],\n                                              [3], [4], [5],\n                                              [6], [7], [8]], [[1.0], [0.0], -trans2,\n                                                               [0.0], [1.0], -trans1,\n                                                               [0.0], [0.0], [1.0]])\n            trans_matrix = tf.reshape(trans_matrix, [3, 3])\n\n            data_dict[\'cam_mat\'] = tf.matmul(trans_matrix, tf.matmul(scale_matrix, cam_mat))\n\n        """""" DEPENDENT DATA ITEMS: Scoremap from the SUBSET of 21 keypoints""""""\n        # create scoremaps from the subset of 2D annoataion\n        keypoint_hw21 = tf.stack([keypoint_uv21[:, 1], keypoint_uv21[:, 0]], -1)\n\n        scoremap_size = self.image_size\n        \n        if self.hand_crop:\n            scoremap_size = (self.crop_size, self.crop_size)\n\n        scoremap = self.create_multiple_gaussian_map(keypoint_hw21,\n                                                     scoremap_size,\n                                                     self.sigma,\n                                                     valid_vec=keypoint_vis21)\n        \n        if self.scoremap_dropout:\n            scoremap = tf.nn.dropout(scoremap, self.scoremap_dropout_prob,\n                                        noise_shape=[1, 1, 21])\n            scoremap *= self.scoremap_dropout_prob\n\n        data_dict[\'scoremap\'] = scoremap\n\n        if self.scale_to_size:\n            image, keypoint_uv21, keypoint_vis21 = data_dict[\'image\'], data_dict[\'keypoint_uv21\'], data_dict[\'keypoint_vis21\']\n            s = image.get_shape().as_list()\n            image = tf.image.resize_images(image, self.scale_target_size)\n            scale = (self.scale_target_size[0]/float(s[0]), self.scale_target_size[1]/float(s[1]))\n            keypoint_uv21 = tf.stack([keypoint_uv21[:, 0] * scale[1],\n                                      keypoint_uv21[:, 1] * scale[0]], 1)\n\n            data_dict = dict()  # delete everything else because the scaling makes the data invalid anyway\n            data_dict[\'image\'] = image\n            data_dict[\'keypoint_uv21\'] = keypoint_uv21\n            data_dict[\'keypoint_vis21\'] = keypoint_vis21\n\n        elif self.random_crop_to_size:\n            tensor_stack = tf.concat([data_dict[\'image\'],\n                                      tf.expand_dims(tf.cast(data_dict[\'hand_parts\'], tf.float32), -1),\n                                      tf.cast(data_dict[\'hand_mask\'], tf.float32)], 2)\n            s = tensor_stack.get_shape().as_list()\n            tensor_stack_cropped = tf.random_crop(tensor_stack,\n                                                  [self.random_crop_size, self.random_crop_size, s[2]])\n            data_dict = dict()  # delete everything else because the random cropping makes the data invalid anyway\n            data_dict[\'image\'], data_dict[\'hand_parts\'], data_dict[\'hand_mask\'] = tensor_stack_cropped[:, :, :3],\\\n                                                                                  tf.cast(tensor_stack_cropped[:, :, 3], tf.int32),\\\n                                                                                  tf.cast(tensor_stack_cropped[:, :, 4:], tf.int32)\n\n        names, tensors = zip(*data_dict.items())\n\n        if self.shuffle:\n            tensors = tf.train.shuffle_batch_join([tensors],\n                                                  batch_size=self.batch_size,\n                                                  capacity=100,\n                                                  min_after_dequeue=50,\n                                                  enqueue_many=False)\n        else:\n            tensors = tf.train.batch_join([tensors],\n                                          batch_size=self.batch_size,\n                                          capacity=100,\n                                          enqueue_many=False)\n\n        return dict(zip(names, tensors))\n\n\n\n    @staticmethod\n    def create_multiple_gaussian_map(coords_uv, output_size, sigma, valid_vec=None):\n        """""" Creates a map of size (output_shape[0], output_shape[1]) at (center[0], center[1])\n            with variance sigma for multiple coordinates.""""""\n        with tf.name_scope(\'create_multiple_gaussian_map\'):\n            sigma = tf.cast(sigma, tf.float32)\n            assert len(output_size) == 2\n            s = coords_uv.get_shape().as_list()\n            coords_uv = tf.cast(coords_uv, tf.int32)\n            if valid_vec is not None:\n                valid_vec = tf.cast(valid_vec, tf.float32)\n                valid_vec = tf.squeeze(valid_vec)\n                cond_val = tf.greater(valid_vec, 0.5)\n            else:\n                cond_val = tf.ones_like(coords_uv[:, 0], dtype=tf.float32)\n                cond_val = tf.greater(cond_val, 0.5)\n\n            cond_1_in = tf.logical_and(tf.less(coords_uv[:, 0], output_size[0]-1), tf.greater(coords_uv[:, 0], 0))\n            cond_2_in = tf.logical_and(tf.less(coords_uv[:, 1], output_size[1]-1), tf.greater(coords_uv[:, 1], 0))\n            cond_in = tf.logical_and(cond_1_in, cond_2_in)\n            cond = tf.logical_and(cond_val, cond_in)\n\n            coords_uv = tf.cast(coords_uv, tf.float32)\n\n            # create meshgrid\n            x_range = tf.expand_dims(tf.range(output_size[0]), 1)\n            y_range = tf.expand_dims(tf.range(output_size[1]), 0)\n\n            X = tf.cast(tf.tile(x_range, [1, output_size[1]]), tf.float32)\n            Y = tf.cast(tf.tile(y_range, [output_size[0], 1]), tf.float32)\n\n            X.set_shape((output_size[0], output_size[1]))\n            Y.set_shape((output_size[0], output_size[1]))\n\n            X = tf.expand_dims(X, -1)\n            Y = tf.expand_dims(Y, -1)\n\n            X_b = tf.tile(X, [1, 1, s[0]])\n            Y_b = tf.tile(Y, [1, 1, s[0]])\n\n            X_b -= coords_uv[:, 0]\n            Y_b -= coords_uv[:, 1]\n\n            dist = tf.square(X_b) + tf.square(Y_b)\n\n            scoremap = tf.exp(-dist / tf.square(sigma)) * tf.cast(cond, tf.float32)\n\n            return scoremap\n\n\n'"
data/BinaryDbReaderSTB.py,91,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom __future__ import print_function, unicode_literals\nimport os\n\nimport tensorflow as tf\n\nfrom utils.general import crop_image_from_xy\nfrom utils.canonical_trafo import canonical_trafo, flip_right_hand\nfrom utils.relative_trafo import bone_rel_trafo\n\n\nclass BinaryDbReaderSTB(object):\n    """"""\n        Reads data from the STB Dataset\n    """"""\n    def __init__(self, mode=None, batch_size=1, shuffle=True, use_wrist_coord=True, sigma=25.0, hand_crop=False,\n                 random_crop_to_size=False,\n                 hue_aug=False,\n                 coord_uv_noise=False,\n                 crop_center_noise=False, crop_scale_noise=False, crop_offset_noise=False,\n                 scoremap_dropout=False):\n        """""" Inputs:\n                mode: string, Indicates which binary file to read. Can be \'training\' or \'evaluation\'\n                batch_size: int, Number of samples forming a batch\n                shuffle: boolean, If true samples of binary file are shuffled while reading\n                use_wrist_coord: boolean, When true keypoint #0 is the wrist, palm center otherwise\n                hand_crop: boolean, When true calculates a tight hand crop using the gt keypoint annotations\n                                    Updates/Sets the following output items: image_crop, crop_scale, scoremap, cam_mat, keypoint_uv21\n                sigma: float, Size of the ground truth scoremaps\n                random_crop_to_size: boolean, Takes randomly sampled crops from the image & the mask\n                hue_aug: boolean, Random hue augmentation\n                coord_uv_noise: boolean, Adds some gaussian noise on the gt uv coordinate\n                crop_center_noise: boolean, Adds gaussian noise on the hand crop center location (all keypoints still lie within the crop)\n                crop_scale_noise: boolean, Adds gaussian noise on the hand crop size\n                crop_offset_noise: boolean, Offsets the hand crop center randomly (keypoints can lie outside the crop)\n                scoremap_dropout: boolean, Randomly drop scoremap channels\n        """"""\n        self.num_samples = 0\n        if mode == \'training\':\n            # self.path_to_db = \'./data/stb_train_shuffled.bin\'\n            self.num_samples = 30000\n            assert 0, ""This set is not for training!""\n        elif mode == \'evaluation\':\n            self.path_to_db = \'./data/stb/stb_eval.bin\'\n            self.num_samples = 6000\n        else:\n            assert 0, ""Unknown dataset mode.""\n\n        assert os.path.exists(self.path_to_db), ""Could not find the binary data file!""\n\n        # general parameters\n        self.batch_size = batch_size\n        self.sigma = sigma\n        self.shuffle = shuffle\n        self.use_wrist_coord = use_wrist_coord\n        self.random_crop_to_size = random_crop_to_size\n        self.random_crop_size = 256\n\n        # data augmentation parameters\n        self.hue_aug = hue_aug\n        self.hue_aug_max = 0.1\n\n        self.hand_crop = hand_crop\n        self.coord_uv_noise = coord_uv_noise\n        self.coord_uv_noise_sigma = 2.5  # std dev in px of noise on the uv coordinates\n        self.crop_center_noise = crop_center_noise\n        self.crop_center_noise_sigma = 20.0  # std dev in px: this moves what is in the ""center"", but the crop always contains all keypoints\n\n        self.crop_scale_noise = crop_scale_noise\n        self.crop_offset_noise = crop_offset_noise\n        self.crop_offset_noise_sigma = 10.0  # translates the crop after size calculation (this can move keypoints outside)\n        self.scoremap_dropout = scoremap_dropout\n        self.scoremap_dropout_prob = 0.8\n\n        # these are constants of the dataset and therefore must not be changed\n        self.image_size = (480, 640)\n        self.crop_size = 256\n        self.num_kp = 21\n\n    def get(self):\n        """""" Provides input data to the graph. """"""\n        # calculate size of each record (this lists what is contained in the db and how many bytes are occupied)\n        record_bytes = 0\n\n        encoding_bytes = 4\n        kp_xyz_entries = 3 * self.num_kp\n        record_bytes += encoding_bytes*kp_xyz_entries\n\n        encoding_bytes = 4\n        kp_uv_entries = 2 * self.num_kp\n        record_bytes += encoding_bytes*kp_uv_entries\n\n        kp_vis_entries = self.num_kp\n        record_bytes += encoding_bytes*kp_vis_entries\n\n        image_bytes = self.image_size[0] * self.image_size[1] * 3\n        record_bytes += image_bytes\n\n        """""" READ DATA ITEMS""""""\n        # Start reader\n        reader = tf.FixedLengthRecordReader(header_bytes=0, record_bytes=record_bytes)\n        _, value = reader.read(tf.train.string_input_producer([self.path_to_db]))\n\n        # decode to floats\n        bytes_read = 0\n        data_dict = dict()\n        record_bytes_float32 = tf.decode_raw(value, tf.float32)\n\n        # 1. Read keypoint xyz\n        keypoint_xyz21 = tf.reshape(tf.slice(record_bytes_float32, [bytes_read//4], [kp_xyz_entries]), [self.num_kp, 3])\n        bytes_read += encoding_bytes*kp_xyz_entries\n        keypoint_xyz21 /= 1000.0  # scale to meters\n        keypoint_xyz21 = self.convert_kp(keypoint_xyz21)\n\n        # calculate wrist coord\n        if self.use_wrist_coord:\n            wrist_xyz = keypoint_xyz21[16, :] + 2.0*(keypoint_xyz21[0, :] - keypoint_xyz21[16, :])\n            keypoint_xyz21 = tf.concat([tf.expand_dims(wrist_xyz, 0),\n                                        keypoint_xyz21[1:, :]], 0)\n\n        data_dict[\'keypoint_xyz21\'] = keypoint_xyz21\n\n        # 2. Read keypoint uv AND VIS\n        keypoint_uv_vis21 = tf.reshape(tf.slice(record_bytes_float32, [bytes_read//4], [kp_uv_entries+kp_vis_entries]), [self.num_kp, 3])\n        bytes_read += encoding_bytes*(kp_uv_entries+kp_vis_entries)\n        keypoint_uv_vis21 = self.convert_kp(keypoint_uv_vis21)\n        keypoint_uv21 = keypoint_uv_vis21[:, :2]\n        keypoint_vis21 = tf.equal(keypoint_uv_vis21[:, 2], 1.0)\n\n        # calculate wrist vis\n        if self.use_wrist_coord:\n            wrist_vis = tf.logical_or(keypoint_vis21[16], keypoint_vis21[0])\n            keypoint_vis21 = tf.concat([tf.expand_dims(wrist_vis, 0),\n                                        keypoint_vis21[1:]], 0)\n\n            wrist_uv = keypoint_uv21[16, :] + 2.0*(keypoint_uv21[0, :] - keypoint_uv21[16, :])\n            keypoint_uv21 = tf.concat([tf.expand_dims(wrist_uv, 0),\n                                       keypoint_uv21[1:, :]], 0)\n\n        data_dict[\'keypoint_vis21\'] = keypoint_vis21\n\n        if self.coord_uv_noise:\n            noise = tf.truncated_normal([42, 2], mean=0.0, stddev=self.coord_uv_noise_sigma)\n            keypoint_uv21 += noise\n\n        data_dict[\'keypoint_uv21\'] = keypoint_uv21\n\n        # decode to uint8\n        record_bytes_uint8 = tf.decode_raw(value, tf.uint8)\n\n        # 4. Read image\n        image = tf.reshape(tf.slice(record_bytes_uint8, [bytes_read], [image_bytes]),\n                               [self.image_size[0], self.image_size[1], 3])\n        image = tf.cast(image, tf.float32)\n        bytes_read += image_bytes\n\n        # subtract mean\n        image = image / 255.0 - 0.5\n        if self.hue_aug:\n            image = tf.image.random_hue(image, self.hue_aug_max)\n        data_dict[\'image\'] = image\n\n        """""" CONSTANTS """"""\n        # Camera intrinsics\n        sx = 822.79041\n        sy = 822.79041\n        tx = 318.47345\n        ty = 250.31296\n        data_dict[\'cam_mat\'] = tf.constant([[sx, 0.0, tx], [0.0, sy, ty], [0.0, 0.0, 1.0]])\n\n        # Hand side: this dataset only contains left hands\n        data_dict[\'hand_side\'] = tf.one_hot(tf.constant(0, dtype=tf.int32), depth=2, on_value=1.0, off_value=0.0, dtype=tf.float32)\n\n        assert bytes_read == record_bytes, ""Doesnt add up.""\n\n        """""" DEPENDENT DATA ITEMS: XYZ represenations. """"""\n        # make coords relative to root joint\n        kp_coord_xyz_root = keypoint_xyz21[0, :] # this is the palm coord\n        kp_coord_xyz21_rel = keypoint_xyz21 - kp_coord_xyz_root  # relative coords in metric coords\n        index_root_bone_length = tf.sqrt(tf.reduce_sum(tf.square(kp_coord_xyz21_rel[12, :] - kp_coord_xyz21_rel[11, :])))\n        data_dict[\'keypoint_scale\'] = index_root_bone_length\n        data_dict[\'keypoint_xyz21_normed\'] = kp_coord_xyz21_rel / index_root_bone_length  # normalized by length of 12->11\n\n        # calculate local coordinates\n        kp_coord_xyz21_local = bone_rel_trafo(data_dict[\'keypoint_xyz21_normed\'])\n        kp_coord_xyz21_local = tf.squeeze(kp_coord_xyz21_local)\n        data_dict[\'keypoint_xyz21_local\'] = kp_coord_xyz21_local\n\n        # calculate viewpoint and coords in canonical coordinates\n        kp_coord_xyz21_rel_can, rot_mat = canonical_trafo(data_dict[\'keypoint_xyz21_normed\'])\n        kp_coord_xyz21_rel_can, rot_mat = tf.squeeze(kp_coord_xyz21_rel_can), tf.squeeze(rot_mat)\n        data_dict[\'keypoint_xyz21_can\'] = kp_coord_xyz21_rel_can\n        data_dict[\'rot_mat\'] = tf.matrix_inverse(rot_mat)\n\n        """""" DEPENDENT DATA ITEMS: HAND CROP """"""\n        if self.hand_crop:\n            crop_center = keypoint_uv21[12, ::-1]\n\n            # catch problem, when no valid kp available (happens almost never)\n            crop_center = tf.cond(tf.reduce_all(tf.is_finite(crop_center)), lambda: crop_center,\n                                  lambda: tf.constant([0.0, 0.0]))\n            crop_center.set_shape([2, ])\n\n            if self.crop_center_noise:\n                noise = tf.truncated_normal([2], mean=0.0, stddev=self.crop_center_noise_sigma)\n                crop_center += noise\n\n            crop_scale_noise = tf.constant(1.0)\n            if self.crop_scale_noise:\n                    crop_scale_noise = tf.squeeze(tf.random_uniform([1], minval=1.0, maxval=1.2))\n\n            if not self.use_wrist_coord:\n                wrist_uv = keypoint_uv21[16, :] + 2.0*(keypoint_uv21[0, :] - keypoint_uv21[16, :])\n                keypoint_uv21 = tf.concat([tf.expand_dims(wrist_uv, 0),\n                                           keypoint_uv21[1:, :]], 0)\n\n            # select visible coords only\n            kp_coord_h = tf.boolean_mask(keypoint_uv21[:, 1], keypoint_vis21)\n            kp_coord_w = tf.boolean_mask(keypoint_uv21[:, 0], keypoint_vis21)\n            kp_coord_hw = tf.stack([kp_coord_h, kp_coord_w], 1)\n\n            # determine size of crop (measure spatial extend of hw coords first)\n            min_coord = tf.maximum(tf.reduce_min(kp_coord_hw, 0), 0.0)\n            max_coord = tf.minimum(tf.reduce_max(kp_coord_hw, 0), self.image_size)\n\n            # find out larger distance wrt the center of crop\n            crop_size_best = 2*tf.maximum(max_coord - crop_center, crop_center - min_coord)\n            crop_size_best = tf.reduce_max(crop_size_best)\n            crop_size_best = tf.minimum(tf.maximum(crop_size_best, 50.0), 500.0)\n\n            # catch problem, when no valid kp available\n            crop_size_best = tf.cond(tf.reduce_all(tf.is_finite(crop_size_best)), lambda: crop_size_best,\n                                  lambda: tf.constant(200.0))\n            crop_size_best.set_shape([])\n\n            # calculate necessary scaling\n            scale = tf.cast(self.crop_size, tf.float32) / crop_size_best\n            scale = tf.minimum(tf.maximum(scale, 1.0), 10.0)\n            scale *= crop_scale_noise\n            data_dict[\'crop_scale\'] = scale\n\n            if self.crop_offset_noise:\n                noise = tf.truncated_normal([2], mean=0.0, stddev=self.crop_offset_noise_sigma)\n                crop_center += noise\n\n            # Crop image\n            img_crop = crop_image_from_xy(tf.expand_dims(image, 0), crop_center, self.crop_size, scale)\n            data_dict[\'image_crop\'] = tf.squeeze(img_crop)\n\n            # Modify uv21 coordinates\n            crop_center_float = tf.cast(crop_center, tf.float32)\n            keypoint_uv21_u = (data_dict[\'keypoint_uv21\'][:, 0] - crop_center_float[1]) * scale + self.crop_size // 2\n            keypoint_uv21_v = (data_dict[\'keypoint_uv21\'][:, 1] - crop_center_float[0]) * scale + self.crop_size // 2\n            keypoint_uv21 = tf.stack([keypoint_uv21_u, keypoint_uv21_v], 1)\n            data_dict[\'keypoint_uv21\'] = keypoint_uv21\n\n            # Modify camera intrinsics\n            scale = tf.reshape(scale, [1, ])\n            scale_matrix = tf.dynamic_stitch([[0], [1], [2],\n                                              [3], [4], [5],\n                                              [6], [7], [8]], [scale, [0.0], [0.0],\n                                                               [0.0], scale, [0.0],\n                                                               [0.0], [0.0], [1.0]])\n            scale_matrix = tf.reshape(scale_matrix, [3, 3])\n\n            crop_center_float = tf.cast(crop_center, tf.float32)\n            trans1 = crop_center_float[0] * scale - self.crop_size // 2\n            trans2 = crop_center_float[1] * scale - self.crop_size // 2\n            trans1 = tf.reshape(trans1, [1, ])\n            trans2 = tf.reshape(trans2, [1, ])\n            trans_matrix = tf.dynamic_stitch([[0], [1], [2],\n                                              [3], [4], [5],\n                                              [6], [7], [8]], [[1.0], [0.0], -trans2,\n                                                               [0.0], [1.0], -trans1,\n                                                               [0.0], [0.0], [1.0]])\n            trans_matrix = tf.reshape(trans_matrix, [3, 3])\n\n            data_dict[\'cam_mat\'] = tf.matmul(trans_matrix, tf.matmul(scale_matrix, data_dict[\'cam_mat\']))\n\n        """""" DEPENDENT DATA ITEMS: Scoremap from the SUBSET of 21 keypoints""""""\n        # create scoremaps from the subset of 2D annoataion\n        keypoint_hw21 = tf.stack([keypoint_uv21[:, 1], keypoint_uv21[:, 0]], -1)\n\n        scoremap_size = self.image_size\n        \n        if self.hand_crop:\n            scoremap_size = (self.crop_size, self.crop_size)\n\n        scoremap = self.create_multiple_gaussian_map(keypoint_hw21,\n                                                     scoremap_size,\n                                                     self.sigma,\n                                                     valid_vec=keypoint_vis21)\n        \n        if self.scoremap_dropout:\n            scoremap = tf.nn.dropout(scoremap, self.scoremap_dropout_prob,\n                                        noise_shape=[1, 1, 21])\n            scoremap *= self.scoremap_dropout_prob\n\n        data_dict[\'scoremap\'] = scoremap\n\n        if self.random_crop_to_size:\n            tensor_stack = tf.concat([data_dict[\'image\'],\n                                      tf.expand_dims(tf.cast(data_dict[\'hand_parts\'], tf.float32), -1),\n                                      tf.cast(data_dict[\'hand_mask\'], tf.float32)], 2)\n            s = tensor_stack.get_shape().as_list()\n            tensor_stack_cropped = tf.random_crop(tensor_stack,\n                                                  [self.random_crop_size, self.random_crop_size, s[2]])\n            data_dict = dict()  # delete everything else because the random cropping makes the data invalid anyway\n            data_dict[\'image\'], data_dict[\'hand_parts\'], data_dict[\'hand_mask\'] = tensor_stack_cropped[:, :, :3],\\\n                                                                                  tf.cast(tensor_stack_cropped[:, :, 3], tf.int32),\\\n                                                                                  tf.cast(tensor_stack_cropped[:, :, 4:], tf.int32)\n\n        names, tensors = zip(*data_dict.items())\n\n        if self.shuffle:\n            tensors = tf.train.shuffle_batch_join([tensors],\n                                                  batch_size=self.batch_size,\n                                                  capacity=100,\n                                                  min_after_dequeue=50,\n                                                  enqueue_many=False)\n        else:\n            tensors = tf.train.batch_join([tensors],\n                                          batch_size=self.batch_size,\n                                          capacity=100,\n                                          enqueue_many=False)\n\n        return dict(zip(names, tensors))\n\n\n\n    @staticmethod\n    def create_multiple_gaussian_map(coords_uv, output_size, sigma, valid_vec=None):\n        """""" Creates a map of size (output_shape[0], output_shape[1]) at (center[0], center[1])\n            with variance sigma for multiple coordinates.""""""\n        with tf.name_scope(\'create_multiple_gaussian_map\'):\n            sigma = tf.cast(sigma, tf.float32)\n            assert len(output_size) == 2\n            s = coords_uv.get_shape().as_list()\n            coords_uv = tf.cast(coords_uv, tf.int32)\n            if valid_vec is not None:\n                valid_vec = tf.cast(valid_vec, tf.float32)\n                valid_vec = tf.squeeze(valid_vec)\n                cond_val = tf.greater(valid_vec, 0.5)\n            else:\n                cond_val = tf.ones_like(coords_uv[:, 0], dtype=tf.float32)\n                cond_val = tf.greater(cond_val, 0.5)\n\n            cond_1_in = tf.logical_and(tf.less(coords_uv[:, 0], output_size[0]-1), tf.greater(coords_uv[:, 0], 0))\n            cond_2_in = tf.logical_and(tf.less(coords_uv[:, 1], output_size[1]-1), tf.greater(coords_uv[:, 1], 0))\n            cond_in = tf.logical_and(cond_1_in, cond_2_in)\n            cond = tf.logical_and(cond_val, cond_in)\n\n            coords_uv = tf.cast(coords_uv, tf.float32)\n\n            # create meshgrid\n            x_range = tf.expand_dims(tf.range(output_size[0]), 1)\n            y_range = tf.expand_dims(tf.range(output_size[1]), 0)\n\n            X = tf.cast(tf.tile(x_range, [1, output_size[1]]), tf.float32)\n            Y = tf.cast(tf.tile(y_range, [output_size[0], 1]), tf.float32)\n\n            X.set_shape((output_size[0], output_size[1]))\n            Y.set_shape((output_size[0], output_size[1]))\n\n            X = tf.expand_dims(X, -1)\n            Y = tf.expand_dims(Y, -1)\n\n            X_b = tf.tile(X, [1, 1, s[0]])\n            Y_b = tf.tile(Y, [1, 1, s[0]])\n\n            X_b -= coords_uv[:, 0]\n            Y_b -= coords_uv[:, 1]\n\n            dist = tf.square(X_b) + tf.square(Y_b)\n\n            scoremap = tf.exp(-dist / tf.square(sigma)) * tf.cast(cond, tf.float32)\n\n            return scoremap\n\n    @staticmethod\n    def convert_kp(keypoints):\n        """""" Maps the keypoints into the right order. """"""\n\n        # mapping into my keypoint definition\n        kp_dict = {0: 0, 1: 20, 2: 19, 3: 18, 4: 17, 5: 16, 6: 15, 7: 14, 8: 13, 9: 12, 10: 11, 11: 10,\n                   12: 9, 13: 8, 14: 7, 15: 6, 16: 5, 17: 4, 18: 3, 19: 2, 20: 1}\n\n        keypoints_new = list()\n        for i in range(21):\n            if i in kp_dict.keys():\n                pos = kp_dict[i]\n                keypoints_new.append(keypoints[pos, :])\n\n        return tf.stack(keypoints_new, 0)\n\n\nif __name__ == \'__main__\':\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Test functionality: BASIC\n    dataset = BinaryDbReaderSTB(mode=\'evaluation\')\n    data = dataset.get()\n    session = tf.Session()\n    tf.train.start_queue_runners(sess=session)\n\n    for _ in range(10):\n        # get data from graph\n        image, cam_mat, scoremap, \\\n        keypoint_uv, keypoint_xyz, keypoint_vis = session.run([data[\'image\'], data[\'cam_mat\'], data[\'scoremap\'],\n                                                                      data[\'keypoint_uv21\'], data[\'keypoint_xyz21\'], data[\'keypoint_vis21\']])\n\n        keypoint_vis = np.squeeze(keypoint_vis)\n        keypoint_uv = np.squeeze(keypoint_uv)\n        keypoint_xyz = np.squeeze(keypoint_xyz)\n        cam_mat = np.squeeze(cam_mat)\n\n        # project into frame\n        keypoint_uv_proj = np.matmul(keypoint_xyz[:, :], np.transpose(cam_mat[:, :]))\n        keypoint_uv_proj = keypoint_uv_proj[:, :2] / keypoint_uv_proj[:, -1:]\n\n        # show results\n        fig = plt.figure(1)\n        ax1 = fig.add_subplot(\'221\')\n        ax2 = fig.add_subplot(\'222\')\n        ax3 = fig.add_subplot(\'223\')\n\n        image_rgb = ((np.squeeze(image) + 0.5) * 255.0).astype(np.uint8)\n        ax1.imshow(image_rgb)\n        ax1.plot(keypoint_uv[keypoint_vis, 0], keypoint_uv[keypoint_vis, 1], \'ro\')\n        ax2.imshow(image_rgb)\n        ax2.plot(keypoint_uv_proj[keypoint_vis, 0], keypoint_uv_proj[keypoint_vis, 1], \'bo\')\n        scoremap = np.max(scoremap[0, :, :, :] > 0.8, 2)\n        ax3.imshow(scoremap)\n        plt.show()\n\n    # # Test functionality: CROP\n    # dataset = BinaryDbReaderSHB(mode=\'training\', shuffle=False, hand_crop=True, crop_center_noise=True)\n    # data = dataset.get()\n    # session = tf.Session()\n    # tf.train.start_queue_runners(sess=session)\n    #\n    # for _ in range(5):\n    #     # get data from graph\n    #     image, \\\n    #     keypoint_uv21, keypoint_xyz21, keypoint_vis21, \\\n    #     cam_mat = session.run([data[\'image_crop\'],\n    #                              data[\'keypoint_uv21\'], data[\'keypoint_xyz21\'], data[\'keypoint_vis21\'],\n    #                              data[\'cam_mat\']])\n    #\n    #     keypoint_vis21 = np.squeeze(keypoint_vis21)\n    #     keypoint_uv21 = np.squeeze(keypoint_uv21)\n    #     keypoint_xyz21 = np.squeeze(keypoint_xyz21)\n    #     cam_mat = np.squeeze(cam_mat)\n    #\n    #     # project into frame\n    #     keypoint_uv_proj = np.matmul(keypoint_xyz21[:, :], np.transpose(cam_mat[:, :]))\n    #     keypoint_uv_proj = keypoint_uv_proj[:, :2] / keypoint_uv_proj[:, -1:]\n    #\n    #     # show results\n    #     fig = plt.figure(1)\n    #     ax1 = fig.add_subplot(\'111\')\n    #\n    #     image_rgb = ((np.squeeze(image) + 0.5) * 255.0).astype(np.uint8)\n    #     ax1.imshow(image_rgb)\n    #     ax1.plot(keypoint_uv_proj[keypoint_vis21, 0], keypoint_uv_proj[keypoint_vis21, 1], \'ro\')\n    #     ax1.plot(keypoint_uv21[keypoint_vis21, 0], keypoint_uv21[keypoint_vis21, 1], \'g+\')\n    #     plt.show()\n'"
nets/ColorHandPose3DNetwork.py,58,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom __future__ import print_function, unicode_literals\n\nimport tensorflow as tf\nimport os\n\nfrom utils.general import *\n\nops = NetworkOps\n\n\nclass ColorHandPose3DNetwork(object):\n    """""" Network performing 3D pose estimation of a human hand from a single color image. """"""\n    def __init__(self):\n        self.crop_size = 256\n        self.num_kp = 21\n\n    def init(self, session, weight_files=None, exclude_var_list=None):\n        """""" Initializes weights from pickled python dictionaries.\n\n            Inputs:\n                session: tf.Session, Tensorflow session object containing the network graph\n                weight_files: list of str, Paths to the pickle files that are used to initialize network weights\n                exclude_var_list: list of str, Weights that should not be loaded\n        """"""\n        if exclude_var_list is None:\n            exclude_var_list = list()\n\n        import pickle\n\n        if weight_files is None:\n            weight_files = [\'./weights/handsegnet-rhd.pickle\', \'./weights/posenet3d-rhd-stb-slr-finetuned.pickle\']\n\n        # Initialize with weights\n        for file_name in weight_files:\n            assert os.path.exists(file_name), ""File not found.""\n            with open(file_name, \'rb\') as fi:\n                weight_dict = pickle.load(fi)\n                weight_dict = {k: v for k, v in weight_dict.items() if not any([x in k for x in exclude_var_list])}\n                if len(weight_dict) > 0:\n                    init_op, init_feed = tf.contrib.framework.assign_from_values(weight_dict)\n                    session.run(init_op, init_feed)\n                    print(\'Loaded %d variables from %s\' % (len(weight_dict), file_name))\n\n    def inference(self, image, hand_side, evaluation):\n        """""" Full pipeline: HandSegNet + PoseNet + PosePrior.\n\n            Inputs:\n                image: [B, H, W, 3] tf.float32 tensor, Image with mean subtracted\n                hand_side: [B, 2] tf.float32 tensor, One hot encoding if the image is showing left or right side\n                evaluation: [] tf.bool tensor, True while evaluation false during training (controls dropout)\n\n            Outputs:\n                hand_scoremap: [B, H, W, 2] tf.float32 tensor, Scores for background and hand class\n                image_crop: [B, 256, 256, 3] tf.float32 tensor, Hand cropped input image\n                scale_crop: [B, 1] tf.float32 tensor, Scaling between input image and image_crop\n                center: [B, 1] tf.float32 tensor, Center of image_crop wrt to image\n                keypoints_scoremap: [B, 256, 256, 21] tf.float32 tensor, Scores for the hand keypoints\n                keypoint_coord3d: [B, 21, 3] tf.float32 tensor, Normalized 3D coordinates\n        """"""\n        # use network for hand segmentation for detection\n        hand_scoremap = self.inference_detection(image)\n        hand_scoremap = hand_scoremap[-1]\n\n        # Intermediate data processing\n        hand_mask = single_obj_scoremap(hand_scoremap)\n        center, _, crop_size_best = calc_center_bb(hand_mask)\n        crop_size_best *= 1.25\n        scale_crop = tf.minimum(tf.maximum(self.crop_size / crop_size_best, 0.25), 5.0)\n        image_crop = crop_image_from_xy(image, center, self.crop_size, scale=scale_crop)\n\n        # detect keypoints in 2D\n        keypoints_scoremap = self.inference_pose2d(image_crop)\n        keypoints_scoremap = keypoints_scoremap[-1]\n\n        # estimate most likely 3D pose\n        keypoint_coord3d = self._inference_pose3d(keypoints_scoremap, hand_side, evaluation)\n\n        # upsample keypoint scoremap\n        s = image_crop.get_shape().as_list()\n        keypoints_scoremap = tf.image.resize_images(keypoints_scoremap, (s[1], s[2]))\n\n        return hand_scoremap, image_crop, scale_crop, center, keypoints_scoremap, keypoint_coord3d\n\n    def inference2d(self, image):\n        """""" Only 2D part of the pipeline: HandSegNet + PoseNet.\n\n            Inputs:\n                image: [B, H, W, 3] tf.float32 tensor, Image with mean subtracted\n\n            Outputs:\n                image_crop: [B, 256, 256, 3] tf.float32 tensor, Hand cropped input image\n                scale_crop: [B, 1] tf.float32 tensor, Scaling between input image and image_crop\n                center: [B, 1] tf.float32 tensor, Center of image_crop wrt to image\n                keypoints_scoremap: [B, 256, 256, 21] tf.float32 tensor, Scores for the hand keypoints\n        """"""\n        # use network for hand segmentation for detection\n        hand_scoremap = self.inference_detection(image)\n        hand_scoremap = hand_scoremap[-1]\n\n        # Intermediate data processing\n        hand_mask = single_obj_scoremap(hand_scoremap)\n        center, _, crop_size_best = calc_center_bb(hand_mask)\n        crop_size_best *= 1.25\n        scale_crop = tf.minimum(tf.maximum(self.crop_size / crop_size_best, 0.25), 5.0)\n        image_crop = crop_image_from_xy(image, center, self.crop_size, scale=scale_crop)\n\n        # detect keypoints in 2D\n        s = image_crop.get_shape().as_list()\n        keypoints_scoremap = self.inference_pose2d(image_crop)\n        keypoints_scoremap = keypoints_scoremap[-1]\n        keypoints_scoremap = tf.image.resize_images(keypoints_scoremap, (s[1], s[2]))\n        return keypoints_scoremap, image_crop, scale_crop, center\n\n    @staticmethod\n    def inference_detection(image, train=False):\n        """""" HandSegNet: Detects the hand in the input image by segmenting it.\n\n            Inputs:\n                image: [B, H, W, 3] tf.float32 tensor, Image with mean subtracted\n                train: bool, True in case weights should be trainable\n\n            Outputs:\n                scoremap_list_large: list of [B, 256, 256, 2] tf.float32 tensor, Scores for the hand segmentation classes\n        """"""\n        with tf.variable_scope(\'HandSegNet\'):\n            scoremap_list = list()\n            layers_per_block = [2, 2, 4, 4]\n            out_chan_list = [64, 128, 256, 512]\n            pool_list = [True, True, True, False]\n\n            # learn some feature representation, that describes the image content well\n            x = image\n            for block_id, (layer_num, chan_num, pool) in enumerate(zip(layers_per_block, out_chan_list, pool_list), 1):\n                for layer_id in range(layer_num):\n                    x = ops.conv_relu(x, \'conv%d_%d\' % (block_id, layer_id+1), kernel_size=3, stride=1, out_chan=chan_num, trainable=train)\n                if pool:\n                    x = ops.max_pool(x, \'pool%d\' % block_id)\n\n            x = ops.conv_relu(x, \'conv5_1\', kernel_size=3, stride=1, out_chan=512, trainable=train)\n            encoding = ops.conv_relu(x, \'conv5_2\', kernel_size=3, stride=1, out_chan=128, trainable=train)\n\n            # use encoding to detect initial scoremap\n            x = ops.conv_relu(encoding, \'conv6_1\', kernel_size=1, stride=1, out_chan=512, trainable=train)\n            scoremap = ops.conv(x, \'conv6_2\', kernel_size=1, stride=1, out_chan=2, trainable=train)\n            scoremap_list.append(scoremap)\n\n            # upsample to full size\n            s = image.get_shape().as_list()\n            scoremap_list_large = [tf.image.resize_images(x, (s[1], s[2])) for x in scoremap_list]\n\n        return scoremap_list_large\n\n    def inference_pose2d(self, image_crop, train=False):\n        """""" PoseNet: Given an image it detects the 2D hand keypoints.\n            The image should already contain a rather tightly cropped hand.\n\n            Inputs:\n                image: [B, H, W, 3] tf.float32 tensor, Image with mean subtracted\n                train: bool, True in case weights should be trainable\n\n            Outputs:\n                scoremap_list_large: list of [B, 256, 256, 21] tf.float32 tensor, Scores for the hand keypoints\n        """"""\n        with tf.variable_scope(\'PoseNet2D\'):\n            scoremap_list = list()\n            layers_per_block = [2, 2, 4, 2]\n            out_chan_list = [64, 128, 256, 512]\n            pool_list = [True, True, True, False]\n\n            # learn some feature representation, that describes the image content well\n            x = image_crop\n            for block_id, (layer_num, chan_num, pool) in enumerate(zip(layers_per_block, out_chan_list, pool_list), 1):\n                for layer_id in range(layer_num):\n                    x = ops.conv_relu(x, \'conv%d_%d\' % (block_id, layer_id+1), kernel_size=3, stride=1, out_chan=chan_num, trainable=train)\n                if pool:\n                    x = ops.max_pool(x, \'pool%d\' % block_id)\n\n            x = ops.conv_relu(x, \'conv4_3\', kernel_size=3, stride=1, out_chan=256, trainable=train)\n            x = ops.conv_relu(x, \'conv4_4\', kernel_size=3, stride=1, out_chan=256, trainable=train)\n            x = ops.conv_relu(x, \'conv4_5\', kernel_size=3, stride=1, out_chan=256, trainable=train)\n            x = ops.conv_relu(x, \'conv4_6\', kernel_size=3, stride=1, out_chan=256, trainable=train)\n            encoding = ops.conv_relu(x, \'conv4_7\', kernel_size=3, stride=1, out_chan=128, trainable=train)\n\n            # use encoding to detect initial scoremap\n            x = ops.conv_relu(encoding, \'conv5_1\', kernel_size=1, stride=1, out_chan=512, trainable=train)\n            scoremap = ops.conv(x, \'conv5_2\', kernel_size=1, stride=1, out_chan=self.num_kp, trainable=train)\n            scoremap_list.append(scoremap)\n\n            # iterate recurrent part a couple of times\n            layers_per_recurrent_unit = 5\n            num_recurrent_units = 2\n            for pass_id in range(num_recurrent_units):\n                x = tf.concat([scoremap_list[-1], encoding], 3)\n                for rec_id in range(layers_per_recurrent_unit):\n                    x = ops.conv_relu(x, \'conv%d_%d\' % (pass_id+6, rec_id+1), kernel_size=7, stride=1, out_chan=128, trainable=train)\n                x = ops.conv_relu(x, \'conv%d_6\' % (pass_id+6), kernel_size=1, stride=1, out_chan=128, trainable=train)\n                scoremap = ops.conv(x, \'conv%d_7\' % (pass_id+6), kernel_size=1, stride=1, out_chan=self.num_kp, trainable=train)\n                scoremap_list.append(scoremap)\n\n            scoremap_list_large = scoremap_list\n\n        return scoremap_list_large\n\n    def _inference_pose3d(self, keypoints_scoremap, hand_side, evaluation, train=False):\n        """""" PosePrior + Viewpoint: Estimates the most likely normalized 3D pose given 2D detections and hand side.\n\n            Inputs:\n                keypoints_scoremap: [B, 32, 32, 21] tf.float32 tensor, Scores for the hand keypoints\n                hand_side: [B, 2] tf.float32 tensor, One hot encoding if the image is showing left or right side\n                evaluation: [] tf.bool tensor, True while evaluation false during training (controls dropout)\n                train: bool, True in case weights should be trainable\n\n            Outputs:\n                coord_xyz_rel_normed: [B, 21, 3] tf.float32 tensor, Normalized 3D coordinates\n        """"""\n        # infer coordinates in the canonical frame\n        coord_can = self._inference_pose3d_can(keypoints_scoremap, hand_side, evaluation, train=train)\n\n        # infer viewpoint\n        rot_mat = self._inference_viewpoint(keypoints_scoremap, hand_side, evaluation, train=train)\n\n        # flip hand according to hand side\n        cond_right = tf.equal(tf.argmax(hand_side, 1), 1)\n        cond_right_all = tf.tile(tf.reshape(cond_right, [-1, 1, 1]), [1, self.num_kp, 3])\n        coord_xyz_can_flip = self._flip_right_hand(coord_can, cond_right_all)\n\n        # rotate view back\n        coord_xyz_rel_normed = tf.matmul(coord_xyz_can_flip, rot_mat)\n\n        return coord_xyz_rel_normed\n\n    def _inference_pose3d_can(self, keypoints_scoremap, hand_side, evaluation, train=False):\n        """""" Inference of canonical coordinates. """"""\n        with tf.variable_scope(\'PosePrior\'):\n            # use encoding to detect relative, normed 3d coords\n            x = keypoints_scoremap  # this is 28x28x21\n            s = x.get_shape().as_list()\n            out_chan_list = [32, 64, 128]\n            for i, out_chan in enumerate(out_chan_list):\n                x = ops.conv_relu(x, \'conv_pose_%d_1\' % i, kernel_size=3, stride=1, out_chan=out_chan, trainable=train)\n                x = ops.conv_relu(x, \'conv_pose_%d_2\' % i, kernel_size=3, stride=2, out_chan=out_chan, trainable=train) # in the end this will be 4x4xC\n\n            # Estimate relative 3D coordinates\n            out_chan_list = [512, 512]\n            x = tf.reshape(x, [s[0], -1])\n            x = tf.concat([x, hand_side], 1)\n            for i, out_chan in enumerate(out_chan_list):\n                x = ops.fully_connected_relu(x, \'fc_rel%d\' % i, out_chan=out_chan, trainable=train)\n                x = ops.dropout(x, 0.8, evaluation)\n            coord_xyz_rel = ops.fully_connected(x, \'fc_xyz\', out_chan=self.num_kp*3, trainable=train)\n\n            # reshape stuff\n            coord_xyz_rel = tf.reshape(coord_xyz_rel, [s[0], self.num_kp, 3])\n\n            return coord_xyz_rel\n\n    def _inference_viewpoint(self, keypoints_scoremap, hand_side, evaluation, train=False):\n        """""" Inference of the viewpoint. """"""\n        with tf.variable_scope(\'ViewpointNet\'):\n            # estimate rotation\n            ux, uy, uz = self._rotation_estimation(keypoints_scoremap, hand_side, evaluation, train=train)\n\n            # assemble rotation matrix\n            rot_mat = self._get_rot_mat(ux, uy, uz)\n\n            return rot_mat\n\n    @staticmethod\n    def _rotation_estimation(scoremap2d, hand_side, evaluation, train=False):\n        """""" Estimates the rotation from canonical coords to realworld xyz. """"""\n        # conv down scoremap to some reasonable length\n        x = tf.concat([scoremap2d], 3)\n        s = x.get_shape().as_list()\n        out_chan_list = [64, 128, 256]\n        for i, out_chan in enumerate(out_chan_list):\n            x = ops.conv_relu(x, \'conv_vp_%d_1\' % i, kernel_size=3, stride=1, out_chan=out_chan, trainable=train)\n            x = ops.conv_relu(x, \'conv_vp_%d_2\' % i, kernel_size=3, stride=2, out_chan=out_chan, trainable=train) # in the end this will be 4x4x128\n\n        # flatten\n        x = tf.reshape(x, [s[0], -1])  # this is Bx2048\n        x = tf.concat([x, hand_side], 1)\n\n        # Estimate Viewpoint --> 3 params\n        out_chan_list = [256, 128]\n        for i, out_chan in enumerate(out_chan_list):\n            x = ops.fully_connected_relu(x, \'fc_vp%d\' % i, out_chan=out_chan, trainable=train)\n            x = ops.dropout(x, 0.75, evaluation)\n\n        ux = ops.fully_connected(x, \'fc_vp_ux\', out_chan=1, trainable=train)\n        uy = ops.fully_connected(x, \'fc_vp_uy\', out_chan=1, trainable=train)\n        uz = ops.fully_connected(x, \'fc_vp_uz\', out_chan=1, trainable=train)\n        return ux, uy, uz\n\n    def _get_rot_mat(self, ux_b, uy_b, uz_b):\n        """""" Returns a rotation matrix from axis and (encoded) angle.""""""\n        with tf.name_scope(\'get_rot_mat\'):\n            u_norm = tf.sqrt(tf.square(ux_b) + tf.square(uy_b) + tf.square(uz_b) + 1e-8)\n            theta = u_norm\n\n            # some tmp vars\n            st_b = tf.sin(theta)\n            ct_b = tf.cos(theta)\n            one_ct_b = 1.0 - tf.cos(theta)\n\n            st = st_b[:, 0]\n            ct = ct_b[:, 0]\n            one_ct = one_ct_b[:, 0]\n            norm_fac = 1.0 / u_norm[:, 0]\n            ux = ux_b[:, 0] * norm_fac\n            uy = uy_b[:, 0] * norm_fac\n            uz = uz_b[:, 0] * norm_fac\n\n            trafo_matrix = self._stitch_mat_from_vecs([ct+ux*ux*one_ct, ux*uy*one_ct-uz*st, ux*uz*one_ct+uy*st,\n                                                       uy*ux*one_ct+uz*st, ct+uy*uy*one_ct, uy*uz*one_ct-ux*st,\n                                                       uz*ux*one_ct-uy*st, uz*uy*one_ct+ux*st, ct+uz*uz*one_ct])\n\n            return trafo_matrix\n\n    @staticmethod\n    def _flip_right_hand(coords_xyz_canonical, cond_right):\n        """""" Flips the given canonical coordinates, when cond_right is true. Returns coords unchanged otherwise.\n            The returned coordinates represent those of a left hand.\n\n            Inputs:\n                coords_xyz_canonical: Nx3 matrix, containing the coordinates for each of the N keypoints\n        """"""\n        with tf.variable_scope(\'flip-right-hand\'):\n            expanded = False\n            s = coords_xyz_canonical.get_shape().as_list()\n            if len(s) == 2:\n                coords_xyz_canonical = tf.expand_dims(coords_xyz_canonical, 0)\n                cond_right = tf.expand_dims(cond_right, 0)\n                expanded = True\n\n            # mirror along y axis\n            coords_xyz_canonical_mirrored = tf.stack([coords_xyz_canonical[:, :, 0], coords_xyz_canonical[:, :, 1], -coords_xyz_canonical[:, :, 2]], -1)\n\n            # select mirrored in case it was a right hand\n            coords_xyz_canonical_left = tf.where(cond_right, coords_xyz_canonical_mirrored, coords_xyz_canonical)\n\n            if expanded:\n                coords_xyz_canonical_left = tf.squeeze(coords_xyz_canonical_left, [0])\n\n            return coords_xyz_canonical_left\n\n    @staticmethod\n    def _stitch_mat_from_vecs(vector_list):\n        """""" Stitches a given list of vectors into a 3x3 matrix.\n\n            Input:\n                vector_list: list of 9 tensors, which will be stitched into a matrix. list contains matrix elements\n                    in a row-first fashion (m11, m12, m13, m21, m22, m23, m31, m32, m33). Length of the vectors has\n                    to be the same, because it is interpreted as batch dimension.\n        """"""\n\n        assert len(vector_list) == 9, ""There have to be exactly 9 tensors in vector_list.""\n        batch_size = vector_list[0].get_shape().as_list()[0]\n        vector_list = [tf.reshape(x, [1, batch_size]) for x in vector_list]\n\n        trafo_matrix = tf.dynamic_stitch([[0], [1], [2],\n                                          [3], [4], [5],\n                                          [6], [7], [8]], vector_list)\n\n        trafo_matrix = tf.reshape(trafo_matrix, [3, 3, batch_size])\n        trafo_matrix = tf.transpose(trafo_matrix, [2, 0, 1])\n\n        return trafo_matrix\n'"
nets/PosePriorNetwork.py,29,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom __future__ import print_function, unicode_literals\n\nimport tensorflow as tf\nimport os\n\nfrom utils.general import *\nfrom utils.canonical_trafo import *\nfrom utils.relative_trafo import *\n\nops = NetworkOps\n\n\nclass PosePriorNetwork(object):\n    """""" Network containing different variants for lifting 2D predictions into 3D. """"""\n    def __init__(self, variant):\n        self.num_kp = 21\n        self.variant = variant\n\n    def init(self, session, weight_files=None, exclude_var_list=None):\n        """""" Initializes weights from pickled python dictionaries.\n\n            Inputs:\n                session: tf.Session, Tensorflow session object containing the network graph\n                weight_files: list of str, Paths to the pickle files that are used to initialize network weights\n                exclude_var_list: list of str, Weights that should not be loaded\n        """"""\n        if exclude_var_list is None:\n            exclude_var_list = list()\n\n        import pickle\n        # Initialize with weights\n        for file_name in weight_files:\n            assert os.path.exists(file_name), ""File not found.""\n            with open(file_name, \'rb\') as fi:\n                weight_dict = pickle.load(fi)\n                weight_dict = {k: v for k, v in weight_dict.items() if not any([x in k for x in exclude_var_list])}\n                if len(weight_dict) > 0:\n                    init_op, init_feed = tf.contrib.framework.assign_from_values(weight_dict)\n                    session.run(init_op, init_feed)\n                    print(\'Loaded %d variables from %s\' % (len(weight_dict), file_name))\n\n    def inference(self, scoremap, hand_side, evaluation):\n        """""" Infere 3D coordinates from 2D scoremaps. """"""\n        scoremap_pooled = tf.nn.avg_pool(scoremap, ksize=[1, 8, 8, 1], strides=[1, 8, 8, 1], padding=\'SAME\')\n\n        coord3d, R = None, None\n        if self.variant == \'direct\':\n            coord_xyz_rel_normed = self._inference_pose3d(scoremap_pooled, hand_side, evaluation, train=True)\n            coord3d = coord_xyz_rel_normed\n        elif self.variant == \'bottleneck\':\n            coord_xyz_rel_normed = self._inference_pose3d(scoremap_pooled, hand_side, evaluation, train=True, bottleneck=True)\n            coord3d = coord_xyz_rel_normed\n        elif (self.variant == \'local\') or (self.variant == \'local_w_xyz_loss\'):\n            coord_xyz_rel_loc = self._inference_pose3d(scoremap_pooled, hand_side, evaluation, train=True)\n            coord3d = coord_xyz_rel_loc\n\n            # assemble to real coords\n            coord_xyz_rel_normed = bone_rel_trafo_inv(coord_xyz_rel_loc)\n        elif self.variant == \'proposed\':\n            # infer coordinates in the canonical frame\n            coord_can = self._inference_pose3d(scoremap_pooled, hand_side, evaluation, train=True)\n            coord3d = coord_can\n\n            # infer viewpoint\n            rot_mat = self._inference_viewpoint(scoremap_pooled, hand_side, evaluation, train=True)\n            R = rot_mat\n\n            # flip hand according to hand side\n            cond_right = tf.equal(tf.argmax(hand_side, 1), 1)\n            cond_right_all = tf.tile(tf.reshape(cond_right, [-1, 1, 1]), [1, self.num_kp, 3])\n            coord_xyz_can_flip = self._flip_right_hand(coord_can, cond_right_all)\n\n            # rotate view back\n            coord_xyz_rel_normed = tf.matmul(coord_xyz_can_flip, rot_mat)\n        else:\n            assert 0, ""Unknown variant.""\n\n        return coord_xyz_rel_normed, coord3d, R\n\n    def _inference_pose3d(self, keypoints_scoremap, hand_side, evaluation, train=False, bottleneck=False):\n        """""" Inference of canonical coordinates. """"""\n        with tf.variable_scope(\'PosePrior\'):\n            # use encoding to detect relative, normed 3d coords\n            x = keypoints_scoremap  # this is 28x28x21\n            s = x.get_shape().as_list()\n            out_chan_list = [32, 64, 128]\n            for i, out_chan in enumerate(out_chan_list):\n                x = ops.conv_relu(x, \'conv_pose_%d_1\' % i, kernel_size=3, stride=1, out_chan=out_chan, trainable=train)\n                x = ops.conv_relu(x, \'conv_pose_%d_2\' % i, kernel_size=3, stride=2, out_chan=out_chan, trainable=train) # in the end this will be 4x4xC\n\n            # Estimate relative 3D coordinates\n            out_chan_list = [512, 512]\n            x = tf.reshape(x, [s[0], -1])\n            x = tf.concat([x, hand_side], 1)\n            for i, out_chan in enumerate(out_chan_list):\n                x = ops.fully_connected_relu(x, \'fc_rel%d\' % i, out_chan=out_chan, trainable=train)\n                x = ops.dropout(x, 0.8, evaluation)\n            if bottleneck:\n                x = ops.fully_connected(x, \'fc_bottleneck\', out_chan=30)\n            coord_xyz_rel = ops.fully_connected(x, \'fc_xyz\', out_chan=self.num_kp*3, trainable=train)\n\n            # reshape stuff\n            coord_xyz_rel = tf.reshape(coord_xyz_rel, [s[0], self.num_kp, 3])\n\n            return coord_xyz_rel\n\n    def _inference_viewpoint(self, keypoints_scoremap, hand_side, evaluation, train=False):\n        """""" Inference of the viewpoint. """"""\n        with tf.variable_scope(\'ViewpointNet\'):\n            # estimate rotation\n            ux, uy, uz = self._rotation_estimation(keypoints_scoremap, hand_side, evaluation, train=train)\n\n            # assemble rotation matrix\n            rot_mat = self._get_rot_mat(ux, uy, uz)\n\n            return rot_mat\n\n    @staticmethod\n    def _rotation_estimation(scoremap2d, hand_side, evaluation, train=False):\n        """""" Estimates the rotation from canonical coords to realworld xyz. """"""\n        # conv down scoremap to some reasonable length\n        x = tf.concat([scoremap2d], 3)\n        s = x.get_shape().as_list()\n        out_chan_list = [64, 128, 256]\n        for i, out_chan in enumerate(out_chan_list):\n            x = ops.conv_relu(x, \'conv_vp_%d_1\' % i, kernel_size=3, stride=1, out_chan=out_chan, trainable=train)\n            x = ops.conv_relu(x, \'conv_vp_%d_2\' % i, kernel_size=3, stride=2, out_chan=out_chan, trainable=train) # in the end this will be 4x4x128\n\n        # flatten\n        x = tf.reshape(x, [s[0], -1])  # this is Bx2048\n        x = tf.concat([x, hand_side], 1)\n\n        # Estimate Viewpoint --> 3 params\n        out_chan_list = [256, 128]\n        for i, out_chan in enumerate(out_chan_list):\n            x = ops.fully_connected_relu(x, \'fc_vp%d\' % i, out_chan=out_chan, trainable=train)\n            x = ops.dropout(x, 0.75, evaluation)\n\n        ux = ops.fully_connected(x, \'fc_vp_ux\', out_chan=1, trainable=train)\n        uy = ops.fully_connected(x, \'fc_vp_uy\', out_chan=1, trainable=train)\n        uz = ops.fully_connected(x, \'fc_vp_uz\', out_chan=1, trainable=train)\n        return ux, uy, uz\n\n    def _get_rot_mat(self, ux_b, uy_b, uz_b):\n        """""" Returns a rotation matrix from axis and (encoded) angle.""""""\n        with tf.name_scope(\'get_rot_mat\'):\n            u_norm = tf.sqrt(tf.square(ux_b) + tf.square(uy_b) + tf.square(uz_b) + 1e-8)\n            theta = u_norm\n\n            # some tmp vars\n            st_b = tf.sin(theta)\n            ct_b = tf.cos(theta)\n            one_ct_b = 1.0 - tf.cos(theta)\n\n            st = st_b[:, 0]\n            ct = ct_b[:, 0]\n            one_ct = one_ct_b[:, 0]\n            norm_fac = 1.0 / u_norm[:, 0]\n            ux = ux_b[:, 0] * norm_fac\n            uy = uy_b[:, 0] * norm_fac\n            uz = uz_b[:, 0] * norm_fac\n\n            trafo_matrix = self._stitch_mat_from_vecs([ct+ux*ux*one_ct, ux*uy*one_ct-uz*st, ux*uz*one_ct+uy*st,\n                                                       uy*ux*one_ct+uz*st, ct+uy*uy*one_ct, uy*uz*one_ct-ux*st,\n                                                       uz*ux*one_ct-uy*st, uz*uy*one_ct+ux*st, ct+uz*uz*one_ct])\n\n            return trafo_matrix\n\n    @staticmethod\n    def _flip_right_hand(coords_xyz_canonical, cond_right):\n        """""" Flips the given canonical coordinates, when cond_right is true. Returns coords unchanged otherwise.\n            The returned coordinates represent those of a left hand.\n\n            Inputs:\n                coords_xyz_canonical: Nx3 matrix, containing the coordinates for each of the N keypoints\n        """"""\n        with tf.variable_scope(\'flip-right-hand\'):\n            expanded = False\n            s = coords_xyz_canonical.get_shape().as_list()\n            if len(s) == 2:\n                coords_xyz_canonical = tf.expand_dims(coords_xyz_canonical, 0)\n                cond_right = tf.expand_dims(cond_right, 0)\n                expanded = True\n\n            # mirror along y axis\n            coords_xyz_canonical_mirrored = tf.stack([coords_xyz_canonical[:, :, 0], coords_xyz_canonical[:, :, 1], -coords_xyz_canonical[:, :, 2]], -1)\n\n            # select mirrored in case it was a right hand\n            coords_xyz_canonical_left = tf.where(cond_right, coords_xyz_canonical_mirrored, coords_xyz_canonical)\n\n            if expanded:\n                coords_xyz_canonical_left = tf.squeeze(coords_xyz_canonical_left, [0])\n\n            return coords_xyz_canonical_left\n\n    @staticmethod\n    def _stitch_mat_from_vecs(vector_list):\n        """""" Stitches a given list of vectors into a 3x3 matrix.\n\n            Input:\n                vector_list: list of 9 tensors, which will be stitched into a matrix. list contains matrix elements\n                    in a row-first fashion (m11, m12, m13, m21, m22, m23, m31, m32, m33). Length of the vectors has\n                    to be the same, because it is interpreted as batch dimension.\n        """"""\n\n        assert len(vector_list) == 9, ""There have to be exactly 9 tensors in vector_list.""\n        batch_size = vector_list[0].get_shape().as_list()[0]\n        vector_list = [tf.reshape(x, [1, batch_size]) for x in vector_list]\n\n        trafo_matrix = tf.dynamic_stitch([[0], [1], [2],\n                                          [3], [4], [5],\n                                          [6], [7], [8]], vector_list)\n\n        trafo_matrix = tf.reshape(trafo_matrix, [3, 3, batch_size])\n        trafo_matrix = tf.transpose(trafo_matrix, [2, 0, 1])\n\n        return trafo_matrix\n'"
utils/canonical_trafo.py,32,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nimport tensorflow as tf\n\n\ndef atan2(y, x):\n    """""" My implementation of atan2 in tensorflow.  Returns in -pi .. pi.""""""\n    tan = tf.atan(y / (x + 1e-8))  # this returns in -pi/2 .. pi/2\n\n    one_map = tf.ones_like(tan)\n\n    # correct quadrant error\n    correction = tf.where(tf.less(x + 1e-8, 0.0), 3.141592653589793*one_map, 0.0*one_map)\n    tan_c = tan + correction  # this returns in -pi/2 .. 3pi/2\n\n    # bring to positive values\n    correction = tf.where(tf.less(tan_c, 0.0), 2*3.141592653589793*one_map, 0.0*one_map)\n    tan_zero_2pi = tan_c + correction  # this returns in 0 .. 2pi\n\n    # make symmetric\n    correction = tf.where(tf.greater(tan_zero_2pi, 3.141592653589793), -2*3.141592653589793*one_map, 0.0*one_map)\n    tan_final = tan_zero_2pi + correction  # this returns in -pi .. pi\n    return tan_final\n\n\ndef _stitch_mat_from_vecs(vector_list):\n    """""" Stitches a given list of vectors into a 3x3 matrix.\n\n        Input:\n            vector_list: list of 9 tensors, which will be stitched into a matrix. list contains matrix elements\n                in a row-first fashion (m11, m12, m13, m21, m22, m23, m31, m32, m33). Length of the vectors has\n                to be the same, because it is interpreted as batch dimension.\n    """"""\n\n    assert len(vector_list) == 9, ""There have to be exactly 9 tensors in vector_list.""\n    batch_size = vector_list[0].get_shape().as_list()[0]\n    vector_list = [tf.reshape(x, [1, batch_size]) for x in vector_list]\n\n    trafo_matrix = tf.dynamic_stitch([[0], [1], [2],\n                                      [3], [4], [5],\n                                      [6], [7], [8]], vector_list)\n\n    trafo_matrix = tf.reshape(trafo_matrix, [3, 3, batch_size])\n    trafo_matrix = tf.transpose(trafo_matrix, [2, 0, 1])\n\n    return trafo_matrix\n\n\ndef _get_rot_mat_x(angle):\n    """""" Returns a 3D rotation matrix. """"""\n    one_vec = tf.ones_like(angle)\n    zero_vec = one_vec*0.0\n    trafo_matrix = _stitch_mat_from_vecs([one_vec, zero_vec, zero_vec,\n                                          zero_vec, tf.cos(angle), tf.sin(angle),\n                                          zero_vec, -tf.sin(angle), tf.cos(angle)])\n    return trafo_matrix\n\n\ndef _get_rot_mat_y(angle):\n    """""" Returns a 3D rotation matrix. """"""\n    one_vec = tf.ones_like(angle)\n    zero_vec = one_vec*0.0\n    trafo_matrix = _stitch_mat_from_vecs([tf.cos(angle), zero_vec, -tf.sin(angle),\n                                          zero_vec, one_vec, zero_vec,\n                                          tf.sin(angle), zero_vec, tf.cos(angle)])\n    return trafo_matrix\n\n\ndef _get_rot_mat_z(angle):\n    """""" Returns a 3D rotation matrix. """"""\n    one_vec = tf.ones_like(angle)\n    zero_vec = one_vec*0.0\n    trafo_matrix = _stitch_mat_from_vecs([tf.cos(angle), tf.sin(angle), zero_vec,\n                                          -tf.sin(angle), tf.cos(angle), zero_vec,\n                                          zero_vec, zero_vec, one_vec])\n    return trafo_matrix\n\n\ndef canonical_trafo(coords_xyz):\n    """""" Transforms the given real xyz coordinates into some canonical frame.\n        Within that frame the hands of all frames are nicely aligned, which\n        should help the network to learn reasonable shape priors.\n\n        Inputs:\n            coords_xyz: BxNx3 matrix, containing the coordinates for each of the N keypoints\n    """"""\n    with tf.variable_scope(\'canonical-trafo\'):\n        coords_xyz = tf.reshape(coords_xyz, [-1, 21, 3])\n\n        ROOT_NODE_ID = 0  # Node that will be at 0/0/0: 0=palm keypoint (root)\n        ALIGN_NODE_ID = 12  # Node that will be at 0/-D/0: 12=beginning of middle finger\n        ROT_NODE_ID = 20  # Node that will be at z=0, x>0; 20: Beginning of pinky\n\n        # 1. Translate the whole set s.t. the root kp is located in the origin\n        trans = tf.expand_dims(coords_xyz[:, ROOT_NODE_ID, :], 1)\n        coords_xyz_t = coords_xyz - trans\n\n        # 2. Rotate and scale keypoints such that the root bone is of unit length and aligned with the y axis\n        p = coords_xyz_t[:, ALIGN_NODE_ID, :]  # thats the point we want to put on (0/1/0)\n\n        # Rotate point into the yz-plane\n        alpha = atan2(p[:, 0], p[:, 1])\n        rot_mat = _get_rot_mat_z(alpha)\n        coords_xyz_t_r1 = tf.matmul(coords_xyz_t, rot_mat)\n        total_rot_mat = rot_mat\n\n        # Rotate point within the yz-plane onto the xy-plane\n        p = coords_xyz_t_r1[:, ALIGN_NODE_ID, :]\n        beta = -atan2(p[:, 2], p[:, 1])\n        rot_mat = _get_rot_mat_x(beta + 3.141592653589793)\n        coords_xyz_t_r2 = tf.matmul(coords_xyz_t_r1, rot_mat)\n        total_rot_mat = tf.matmul(total_rot_mat, rot_mat)\n\n        # 3. Rotate keypoints such that rotation along the y-axis is defined\n        p = coords_xyz_t_r2[:, ROT_NODE_ID, :]\n        gamma = atan2(p[:, 2], p[:, 0])\n        rot_mat = _get_rot_mat_y(gamma)\n        coords_xyz_normed = tf.matmul(coords_xyz_t_r2, rot_mat)\n        total_rot_mat = tf.matmul(total_rot_mat, rot_mat)\n\n        return coords_xyz_normed, total_rot_mat\n\n\ndef flip_right_hand(coords_xyz_canonical, cond_right):\n    """""" Flips the given canonical coordinates, when cond_right is true. Returns coords unchanged otherwise.\n        The returned coordinates represent those of a left hand.\n\n        Inputs:\n            coords_xyz_canonical: Nx3 matrix, containing the coordinates for each of the N keypoints\n    """"""\n    with tf.variable_scope(\'flip-right-hand\'):\n        expanded = False\n        s = coords_xyz_canonical.get_shape().as_list()\n        if len(s) == 2:\n            coords_xyz_canonical = tf.expand_dims(coords_xyz_canonical, 0)\n            cond_right = tf.expand_dims(cond_right, 0)\n            expanded = True\n\n        # mirror along y axis\n        coords_xyz_canonical_mirrored = tf.stack([coords_xyz_canonical[:, :, 0], coords_xyz_canonical[:, :, 1], -coords_xyz_canonical[:, :, 2]], -1)\n\n        # select mirrored in case it was a right hand\n        coords_xyz_canonical_left = tf.where(cond_right, coords_xyz_canonical_mirrored, coords_xyz_canonical)\n\n        if expanded:\n            coords_xyz_canonical_left = tf.squeeze(coords_xyz_canonical_left, [0])\n\n        return coords_xyz_canonical_left'"
utils/general.py,110,"b'#\n#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image\n#  Copyright (C) 2017  Christian Zimmermann\n#  \n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 2 of the License, or\n#  (at your option) any later version.\n#  \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#  \n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom __future__ import print_function, unicode_literals\n\nimport tensorflow as tf\nfrom tensorflow.python import pywrap_tensorflow\nimport numpy as np\nimport math\n\n\nclass NetworkOps(object):\n    """""" Operations that are frequently used within networks. """"""\n    neg_slope_of_relu = 0.01\n\n    @classmethod\n    def leaky_relu(cls, tensor, name=\'relu\'):\n        out_tensor = tf.maximum(tensor, cls.neg_slope_of_relu*tensor, name=name)\n        return out_tensor\n\n    @classmethod\n    def conv(cls, in_tensor, layer_name, kernel_size, stride, out_chan, trainable=True):\n        with tf.variable_scope(layer_name):\n            in_size = in_tensor.get_shape().as_list()\n\n            strides = [1, stride, stride, 1]\n            kernel_shape = [kernel_size, kernel_size, in_size[3], out_chan]\n\n            # conv\n            kernel = tf.get_variable(\'weights\', kernel_shape, tf.float32,\n                                     tf.contrib.layers.xavier_initializer_conv2d(), trainable=trainable, collections=[\'wd\', \'variables\', \'filters\'])\n            tmp_result = tf.nn.conv2d(in_tensor, kernel, strides, padding=\'SAME\')\n\n            # bias\n            biases = tf.get_variable(\'biases\', [kernel_shape[3]], tf.float32,\n                                     tf.constant_initializer(0.0001), trainable=trainable, collections=[\'wd\', \'variables\', \'biases\'])\n            out_tensor = tf.nn.bias_add(tmp_result, biases, name=\'out\')\n\n            return out_tensor\n\n    @classmethod\n    def conv_relu(cls, in_tensor, layer_name, kernel_size, stride, out_chan, trainable=True):\n        tensor = cls.conv(in_tensor, layer_name, kernel_size, stride, out_chan, trainable)\n        out_tensor = cls.leaky_relu(tensor, name=\'out\')\n        return out_tensor\n\n    @classmethod\n    def max_pool(cls, bottom, name=\'pool\'):\n        pooled = tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                                padding=\'VALID\', name=name)\n        return pooled\n\n    @classmethod\n    def upconv(cls, in_tensor, layer_name, output_shape, kernel_size, stride, trainable=True):\n        with tf.variable_scope(layer_name):\n            in_size = in_tensor.get_shape().as_list()\n\n            kernel_shape = [kernel_size, kernel_size, in_size[3], in_size[3]]\n            strides = [1, stride, stride, 1]\n\n            # conv\n            kernel = cls.get_deconv_filter(kernel_shape, trainable)\n            tmp_result = tf.nn.conv2d_transpose(value=in_tensor, filter=kernel, output_shape=output_shape,\n                                                strides=strides, padding=\'SAME\')\n\n            # bias\n            biases = tf.get_variable(\'biases\', [kernel_shape[2]], tf.float32,\n                                     tf.constant_initializer(0.0), trainable=trainable, collections=[\'wd\', \'variables\', \'biases\'])\n            out_tensor = tf.nn.bias_add(tmp_result, biases)\n            return out_tensor\n\n    @classmethod\n    def upconv_relu(cls, in_tensor, layer_name, output_shape, kernel_size, stride, trainable=True):\n        tensor = cls.upconv(in_tensor, layer_name, output_shape, kernel_size, stride, trainable)\n        out_tensor = cls.leaky_relu(tensor, name=\'out\')\n        return out_tensor\n\n    @staticmethod\n    def get_deconv_filter(f_shape, trainable):\n        width = f_shape[0]\n        height = f_shape[1]\n        f = math.ceil(width/2.0)\n        c = (2 * f - 1 - f % 2) / (2.0 * f)\n        bilinear = np.zeros([f_shape[0], f_shape[1]])\n        for x in range(width):\n            for y in range(height):\n                value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n                bilinear[x, y] = value\n        weights = np.zeros(f_shape)\n        for i in range(f_shape[2]):\n            weights[:, :, i, i] = bilinear\n\n        init = tf.constant_initializer(value=weights,\n                                       dtype=tf.float32)\n        return tf.get_variable(name=""weights"", initializer=init,\n                               shape=weights.shape, trainable=trainable, collections=[\'wd\', \'variables\', \'filters\'])\n\n    @staticmethod\n    def fully_connected(in_tensor, layer_name, out_chan, trainable=True):\n        with tf.variable_scope(layer_name):\n            in_size = in_tensor.get_shape().as_list()\n            assert len(in_size) == 2, \'Input to a fully connected layer must be a vector.\'\n            weights_shape = [in_size[1], out_chan]\n\n            # weight matrix\n            weights = tf.get_variable(\'weights\', weights_shape, tf.float32,\n                                     tf.contrib.layers.xavier_initializer(), trainable=trainable)\n            weights = tf.check_numerics(weights, \'weights: %s\' % layer_name)\n\n            # bias\n            biases = tf.get_variable(\'biases\', [out_chan], tf.float32,\n                                     tf.constant_initializer(0.0001), trainable=trainable)\n            biases = tf.check_numerics(biases, \'biases: %s\' % layer_name)\n\n            out_tensor = tf.matmul(in_tensor, weights) + biases\n            return out_tensor\n\n    @classmethod\n    def fully_connected_relu(cls, in_tensor, layer_name, out_chan, trainable=True):\n        tensor = cls.fully_connected(in_tensor, layer_name, out_chan, trainable)\n        out_tensor = tf.maximum(tensor, cls.neg_slope_of_relu*tensor, name=\'out\')\n        return out_tensor\n\n    @staticmethod\n    def dropout(in_tensor, keep_prob, evaluation):\n        """""" Dropout: Each neuron is dropped independently. """"""\n        with tf.variable_scope(\'dropout\'):\n            tensor_shape = in_tensor.get_shape().as_list()\n            out_tensor = tf.cond(evaluation,\n                                 lambda: tf.nn.dropout(in_tensor, 1.0,\n                                                       noise_shape=tensor_shape),\n                                 lambda: tf.nn.dropout(in_tensor, keep_prob,\n                                                       noise_shape=tensor_shape))\n            return out_tensor\n\n    @staticmethod\n    def spatial_dropout(in_tensor, keep_prob, evaluation):\n        """""" Spatial dropout: Not each neuron is dropped independently, but feature map wise. """"""\n        with tf.variable_scope(\'spatial_dropout\'):\n            tensor_shape = in_tensor.get_shape().as_list()\n            out_tensor = tf.cond(evaluation,\n                                 lambda: tf.nn.dropout(in_tensor, 1.0,\n                                                       noise_shape=tensor_shape),\n                                 lambda: tf.nn.dropout(in_tensor, keep_prob,\n                                                       noise_shape=[tensor_shape[0], 1, 1, tensor_shape[3]]))\n            return out_tensor\n\n\ndef crop_image_from_xy(image, crop_location, crop_size, scale=1.0):\n    """"""\n    Crops an image. When factor is not given does an central crop.\n\n    Inputs:\n        image: 4D tensor, [batch, height, width, channels] which will be cropped in height and width dimension\n        crop_location: tensor, [batch, 2] which represent the height and width location of the crop\n        crop_size: int, describes the extension of the crop\n    Outputs:\n        image_crop: 4D tensor, [batch, crop_size, crop_size, channels]\n    """"""\n    with tf.name_scope(\'crop_image_from_xy\'):\n        s = image.get_shape().as_list()\n        assert len(s) == 4, ""Image needs to be of shape [batch, width, height, channel]""\n        scale = tf.reshape(scale, [-1])\n        crop_location = tf.cast(crop_location, tf.float32)\n        crop_location = tf.reshape(crop_location, [s[0], 2])\n        crop_size = tf.cast(crop_size, tf.float32)\n\n        crop_size_scaled = crop_size / scale\n        y1 = crop_location[:, 0] - crop_size_scaled//2\n        y2 = y1 + crop_size_scaled\n        x1 = crop_location[:, 1] - crop_size_scaled//2\n        x2 = x1 + crop_size_scaled\n        y1 /= s[1]\n        y2 /= s[1]\n        x1 /= s[2]\n        x2 /= s[2]\n        boxes = tf.stack([y1, x1, y2, x2], -1)\n\n        crop_size = tf.cast(tf.stack([crop_size, crop_size]), tf.int32)\n        box_ind = tf.range(s[0])\n        image_c = tf.image.crop_and_resize(tf.cast(image, tf.float32), boxes, box_ind, crop_size, name=\'crop\')\n        return image_c\n\n\ndef find_max_location(scoremap):\n    """""" Returns the coordinates of the given scoremap with maximum value. """"""\n    with tf.variable_scope(\'find_max_location\'):\n        s = scoremap.get_shape().as_list()\n        if len(s) == 4:\n            scoremap = tf.squeeze(scoremap, [3])\n        if len(s) == 2:\n            scoremap = tf.expand_dims(scoremap, 0)\n\n        s = scoremap.get_shape().as_list()\n        assert len(s) == 3, ""Scoremap must be 3D.""\n        assert (s[0] < s[1]) and (s[0] < s[2]), ""Scoremap must be [Batch, Width, Height]""\n\n        # my meshgrid\n        x_range = tf.expand_dims(tf.range(s[1]), 1)\n        y_range = tf.expand_dims(tf.range(s[2]), 0)\n        X = tf.tile(x_range, [1, s[2]])\n        Y = tf.tile(y_range, [s[1], 1])\n\n        x_vec = tf.reshape(X, [-1])\n        y_vec = tf.reshape(Y, [-1])\n        scoremap_vec = tf.reshape(scoremap, [s[0], -1])\n        max_ind_vec = tf.cast(tf.argmax(scoremap_vec, dimension=1), tf.int32)\n\n        xy_loc = list()\n        for i in range(s[0]):\n            x_loc = tf.reshape(x_vec[max_ind_vec[i]], [1])\n            y_loc = tf.reshape(y_vec[max_ind_vec[i]], [1])\n            xy_loc.append(tf.concat([x_loc, y_loc], 0))\n\n        xy_loc = tf.stack(xy_loc, 0)\n        return xy_loc\n\n\ndef single_obj_scoremap(scoremap):\n    """""" Applies my algorithm to figure out the most likely object from a given segmentation scoremap. """"""\n    with tf.variable_scope(\'single_obj_scoremap\'):\n        filter_size = 21\n        s = scoremap.get_shape().as_list()\n        assert len(s) == 4, ""Scoremap must be 4D.""\n\n        scoremap_softmax = tf.nn.softmax(scoremap)  #B, H, W, C --> normalizes across last dimension\n        scoremap_fg = tf.reduce_max(scoremap_softmax[:, :, :, 1:], 3) # B, H, W\n        detmap_fg = tf.round(scoremap_fg) # B, H, W\n\n        # find maximum in the fg scoremap\n        max_loc = find_max_location(scoremap_fg)\n\n        # use maximum to start ""growing"" our objectmap\n        objectmap_list = list()\n        kernel_dil = tf.ones((filter_size, filter_size, 1)) / float(filter_size*filter_size)\n        for i in range(s[0]):\n            # create initial objectmap (put a one at the maximum)\n            sparse_ind = tf.reshape(max_loc[i, :], [1, 2])  # reshape that its one point with 2dim)\n            objectmap = tf.sparse_to_dense(sparse_ind, [s[1], s[2]], 1.0)\n\n            # grow the map by dilation and pixelwise and\n            num_passes = max(s[1], s[2]) // (filter_size//2) # number of passes needes to make sure the map can spread over the whole image\n            for j in range(num_passes):\n                objectmap = tf.reshape(objectmap, [1, s[1], s[2], 1])\n                objectmap_dil = tf.nn.dilation2d(objectmap, kernel_dil, [1, 1, 1, 1], [1, 1, 1, 1], \'SAME\')\n                objectmap_dil = tf.reshape(objectmap_dil, [s[1], s[2]])\n                objectmap = tf.round(tf.multiply(detmap_fg[i, :, :], objectmap_dil))\n\n            objectmap = tf.reshape(objectmap, [s[1], s[2], 1])\n            objectmap_list.append(objectmap)\n\n        objectmap = tf.stack(objectmap_list)\n\n        return objectmap\n\n\ndef calc_center_bb(binary_class_mask):\n    """""" Returns the center of mass coordinates for the given binary_class_mask. """"""\n    with tf.variable_scope(\'calc_center_bb\'):\n        binary_class_mask = tf.cast(binary_class_mask, tf.int32)\n        binary_class_mask = tf.equal(binary_class_mask, 1)\n        s = binary_class_mask.get_shape().as_list()\n        if len(s) == 4:\n            binary_class_mask = tf.squeeze(binary_class_mask, [3])\n\n        s = binary_class_mask.get_shape().as_list()\n        assert len(s) == 3, ""binary_class_mask must be 3D.""\n        assert (s[0] < s[1]) and (s[0] < s[2]), ""binary_class_mask must be [Batch, Width, Height]""\n\n        # my meshgrid\n        x_range = tf.expand_dims(tf.range(s[1]), 1)\n        y_range = tf.expand_dims(tf.range(s[2]), 0)\n        X = tf.tile(x_range, [1, s[2]])\n        Y = tf.tile(y_range, [s[1], 1])\n\n        bb_list = list()\n        center_list = list()\n        crop_size_list = list()\n        for i in range(s[0]):\n            X_masked = tf.cast(tf.boolean_mask(X, binary_class_mask[i, :, :]), tf.float32)\n            Y_masked = tf.cast(tf.boolean_mask(Y, binary_class_mask[i, :, :]), tf.float32)\n\n            x_min = tf.reduce_min(X_masked)\n            x_max = tf.reduce_max(X_masked)\n            y_min = tf.reduce_min(Y_masked)\n            y_max = tf.reduce_max(Y_masked)\n\n            start = tf.stack([x_min, y_min])\n            end = tf.stack([x_max, y_max])\n            bb = tf.stack([start, end], 1)\n            bb_list.append(bb)\n\n            center_x = 0.5*(x_max + x_min)\n            center_y = 0.5*(y_max + y_min)\n            center = tf.stack([center_x, center_y], 0)\n\n            center = tf.cond(tf.reduce_all(tf.is_finite(center)), lambda: center,\n                                  lambda: tf.constant([160.0, 160.0]))\n            center.set_shape([2])\n            center_list.append(center)\n\n            crop_size_x = x_max - x_min\n            crop_size_y = y_max - y_min\n            crop_size = tf.expand_dims(tf.maximum(crop_size_x, crop_size_y), 0)\n            crop_size = tf.cond(tf.reduce_all(tf.is_finite(crop_size)), lambda: crop_size,\n                                  lambda: tf.constant([100.0]))\n            crop_size.set_shape([1])\n            crop_size_list.append(crop_size)\n\n        bb = tf.stack(bb_list)\n        center = tf.stack(center_list)\n        crop_size = tf.stack(crop_size_list)\n\n        return center, bb, crop_size\n\n\ndef detect_keypoints(scoremaps):\n    """""" Performs detection per scoremap for the hands keypoints. """"""\n    if len(scoremaps.shape) == 4:\n        scoremaps = np.squeeze(scoremaps)\n    s = scoremaps.shape\n    assert len(s) == 3, ""This function was only designed for 3D Scoremaps.""\n    assert (s[2] < s[1]) and (s[2] < s[0]), ""Probably the input is not correct, because [H, W, C] is expected.""\n\n    keypoint_coords = np.zeros((s[2], 2))\n    for i in range(s[2]):\n        v, u = np.unravel_index(np.argmax(scoremaps[:, :, i]), (s[0], s[1]))\n        keypoint_coords[i, 0] = v\n        keypoint_coords[i, 1] = u\n    return keypoint_coords\n\n\ndef trafo_coords(keypoints_crop_coords, centers, scale, crop_size):\n    """""" Transforms coords into global image coordinates. """"""\n    keypoints_coords = np.copy(keypoints_crop_coords)\n\n    keypoints_coords -= crop_size // 2\n\n    keypoints_coords /= scale\n\n    keypoints_coords += centers\n\n    return keypoints_coords\n\n\ndef plot_hand(coords_hw, axis, color_fixed=None, linewidth=\'1\'):\n    """""" Plots a hand stick figure into a matplotlib figure. """"""\n    colors = np.array([[0., 0., 0.5],\n                       [0., 0., 0.73172906],\n                       [0., 0., 0.96345811],\n                       [0., 0.12745098, 1.],\n                       [0., 0.33137255, 1.],\n                       [0., 0.55098039, 1.],\n                       [0., 0.75490196, 1.],\n                       [0.06008855, 0.9745098, 0.90765338],\n                       [0.22454143, 1., 0.74320051],\n                       [0.40164453, 1., 0.56609741],\n                       [0.56609741, 1., 0.40164453],\n                       [0.74320051, 1., 0.22454143],\n                       [0.90765338, 1., 0.06008855],\n                       [1., 0.82861293, 0.],\n                       [1., 0.63979666, 0.],\n                       [1., 0.43645606, 0.],\n                       [1., 0.2476398, 0.],\n                       [0.96345811, 0.0442992, 0.],\n                       [0.73172906, 0., 0.],\n                       [0.5, 0., 0.]])\n\n    # define connections and colors of the bones\n    bones = [((0, 4), colors[0, :]),\n             ((4, 3), colors[1, :]),\n             ((3, 2), colors[2, :]),\n             ((2, 1), colors[3, :]),\n\n             ((0, 8), colors[4, :]),\n             ((8, 7), colors[5, :]),\n             ((7, 6), colors[6, :]),\n             ((6, 5), colors[7, :]),\n\n             ((0, 12), colors[8, :]),\n             ((12, 11), colors[9, :]),\n             ((11, 10), colors[10, :]),\n             ((10, 9), colors[11, :]),\n\n             ((0, 16), colors[12, :]),\n             ((16, 15), colors[13, :]),\n             ((15, 14), colors[14, :]),\n             ((14, 13), colors[15, :]),\n\n             ((0, 20), colors[16, :]),\n             ((20, 19), colors[17, :]),\n             ((19, 18), colors[18, :]),\n             ((18, 17), colors[19, :])]\n\n    for connection, color in bones:\n        coord1 = coords_hw[connection[0], :]\n        coord2 = coords_hw[connection[1], :]\n        coords = np.stack([coord1, coord2])\n        if color_fixed is None:\n            axis.plot(coords[:, 1], coords[:, 0], color=color, linewidth=linewidth)\n        else:\n            axis.plot(coords[:, 1], coords[:, 0], color_fixed, linewidth=linewidth)\n\n\ndef plot_hand_3d(coords_xyz, axis, color_fixed=None, linewidth=\'1\'):\n    """""" Plots a hand stick figure into a matplotlib figure. """"""\n    colors = np.array([[0., 0., 0.5],\n                       [0., 0., 0.73172906],\n                       [0., 0., 0.96345811],\n                       [0., 0.12745098, 1.],\n                       [0., 0.33137255, 1.],\n                       [0., 0.55098039, 1.],\n                       [0., 0.75490196, 1.],\n                       [0.06008855, 0.9745098, 0.90765338],\n                       [0.22454143, 1., 0.74320051],\n                       [0.40164453, 1., 0.56609741],\n                       [0.56609741, 1., 0.40164453],\n                       [0.74320051, 1., 0.22454143],\n                       [0.90765338, 1., 0.06008855],\n                       [1., 0.82861293, 0.],\n                       [1., 0.63979666, 0.],\n                       [1., 0.43645606, 0.],\n                       [1., 0.2476398, 0.],\n                       [0.96345811, 0.0442992, 0.],\n                       [0.73172906, 0., 0.],\n                       [0.5, 0., 0.]])\n\n    # define connections and colors of the bones\n    bones = [((0, 4), colors[0, :]),\n             ((4, 3), colors[1, :]),\n             ((3, 2), colors[2, :]),\n             ((2, 1), colors[3, :]),\n\n             ((0, 8), colors[4, :]),\n             ((8, 7), colors[5, :]),\n             ((7, 6), colors[6, :]),\n             ((6, 5), colors[7, :]),\n\n             ((0, 12), colors[8, :]),\n             ((12, 11), colors[9, :]),\n             ((11, 10), colors[10, :]),\n             ((10, 9), colors[11, :]),\n\n             ((0, 16), colors[12, :]),\n             ((16, 15), colors[13, :]),\n             ((15, 14), colors[14, :]),\n             ((14, 13), colors[15, :]),\n\n             ((0, 20), colors[16, :]),\n             ((20, 19), colors[17, :]),\n             ((19, 18), colors[18, :]),\n             ((18, 17), colors[19, :])]\n\n    for connection, color in bones:\n        coord1 = coords_xyz[connection[0], :]\n        coord2 = coords_xyz[connection[1], :]\n        coords = np.stack([coord1, coord2])\n        if color_fixed is None:\n            axis.plot(coords[:, 0], coords[:, 1], coords[:, 2], color=color, linewidth=linewidth)\n        else:\n            axis.plot(coords[:, 0], coords[:, 1], coords[:, 2], color_fixed, linewidth=linewidth)\n\n    axis.view_init(azim=-90., elev=90.)\n\n\nclass LearningRateScheduler:\n    """"""\n        Provides scalar tensors at certain iteration as is needed for a multistep learning rate schedule.\n    """"""\n    def __init__(self, steps, values):\n        self.steps = steps\n        self.values = values\n\n        assert len(steps)+1 == len(values), ""There must be one more element in value as step.""\n\n    def get_lr(self, global_step):\n        with tf.name_scope(\'lr_scheduler\'):\n\n            if len(self.values) == 1: #1 value -> no step\n                learning_rate = tf.constant(self.values[0])\n            elif len(self.values) == 2: #2 values -> one step\n                cond = tf.greater(global_step, self.steps[0])\n                learning_rate = tf.where(cond, self.values[1], self.values[0])\n            else: # n values -> n-1 steps\n                cond_first = tf.less(global_step, self.steps[0])\n\n                cond_between = list()\n                for ind, step in enumerate(range(0, len(self.steps)-1)):\n                    cond_between.append(tf.logical_and(tf.less(global_step, self.steps[ind+1]),\n                                                       tf.greater_equal(global_step, self.steps[ind])))\n\n                cond_last = tf.greater_equal(global_step, self.steps[-1])\n\n                cond_full = [cond_first]\n                cond_full.extend(cond_between)\n                cond_full.append(cond_last)\n\n                cond_vec = tf.stack(cond_full)\n                lr_vec = tf.stack(self.values)\n\n                learning_rate = tf.where(cond_vec, lr_vec, tf.zeros_like(lr_vec))\n\n                learning_rate = tf.reduce_sum(learning_rate)\n\n            return learning_rate\n\n\nclass EvalUtil:\n    """""" Util class for evaluation networks.\n    """"""\n    def __init__(self, num_kp=21):\n        # init empty data storage\n        self.data = list()\n        self.num_kp = num_kp\n        for _ in range(num_kp):\n            self.data.append(list())\n\n    def feed(self, keypoint_gt, keypoint_vis, keypoint_pred):\n        """""" Used to feed data to the class. Stores the euclidean distance between gt and pred, when it is visible. """"""\n        keypoint_gt = np.squeeze(keypoint_gt)\n        keypoint_pred = np.squeeze(keypoint_pred)\n        keypoint_vis = np.squeeze(keypoint_vis).astype(\'bool\')\n\n        assert len(keypoint_gt.shape) == 2\n        assert len(keypoint_pred.shape) == 2\n        assert len(keypoint_vis.shape) == 1\n\n        # calc euclidean distance\n        diff = keypoint_gt - keypoint_pred\n        euclidean_dist = np.sqrt(np.sum(np.square(diff), axis=1))\n\n        num_kp = keypoint_gt.shape[0]\n        for i in range(num_kp):\n            if keypoint_vis[i]:\n                self.data[i].append(euclidean_dist[i])\n\n    def _get_pck(self, kp_id, threshold):\n        """""" Returns pck for one keypoint for the given threshold. """"""\n        if len(self.data[kp_id]) == 0:\n            return None\n\n        data = np.array(self.data[kp_id])\n        pck = np.mean((data <= threshold).astype(\'float\'))\n        return pck\n\n    def _get_epe(self, kp_id):\n        """""" Returns end point error for one keypoint. """"""\n        if len(self.data[kp_id]) == 0:\n            return None, None\n\n        data = np.array(self.data[kp_id])\n        epe_mean = np.mean(data)\n        epe_median = np.median(data)\n        return epe_mean, epe_median\n\n    def get_measures(self, val_min, val_max, steps):\n        """""" Outputs the average mean and median error as well as the pck score. """"""\n        thresholds = np.linspace(val_min, val_max, steps)\n        thresholds = np.array(thresholds)\n        norm_factor = np.trapz(np.ones_like(thresholds), thresholds)\n\n        # init mean measures\n        epe_mean_all = list()\n        epe_median_all = list()\n        auc_all = list()\n        pck_curve_all = list()\n\n        # Create one plot for each part\n        for part_id in range(self.num_kp):\n            # mean/median error\n            mean, median = self._get_epe(part_id)\n\n            if mean is None:\n                # there was no valid measurement for this keypoint\n                continue\n\n            epe_mean_all.append(mean)\n            epe_median_all.append(median)\n\n            # pck/auc\n            pck_curve = list()\n            for t in thresholds:\n                pck = self._get_pck(part_id, t)\n                pck_curve.append(pck)\n\n            pck_curve = np.array(pck_curve)\n            pck_curve_all.append(pck_curve)\n            auc = np.trapz(pck_curve, thresholds)\n            auc /= norm_factor\n            auc_all.append(auc)\n\n        epe_mean_all = np.mean(np.array(epe_mean_all))\n        epe_median_all = np.mean(np.array(epe_median_all))\n        auc_all = np.mean(np.array(auc_all))\n        pck_curve_all = np.mean(np.array(pck_curve_all), 0)  # mean only over keypoints\n\n        return epe_mean_all, epe_median_all, auc_all, pck_curve_all, thresholds\n\n\ndef load_weights_from_snapshot(session, checkpoint_path, discard_list=None, rename_dict=None):\n        """""" Loads weights from a snapshot except the ones indicated with discard_list. Others are possibly renamed. """"""\n        reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path)\n        var_to_shape_map = reader.get_variable_to_shape_map()\n\n        # Remove everything from the discard list\n        if discard_list is not None:\n            num_disc = 0\n            var_to_shape_map_new = dict()\n            for k, v in var_to_shape_map.items():\n                good = True\n                for dis_str in discard_list:\n                    if dis_str in k:\n                        good = False\n\n                if good:\n                    var_to_shape_map_new[k] = v\n                else:\n                    num_disc += 1\n            var_to_shape_map = dict(var_to_shape_map_new)\n            print(\'Discarded %d items\' % num_disc)\n\n        # rename everything according to rename_dict\n        num_rename = 0\n        var_to_shape_map_new = dict()\n        for name in var_to_shape_map.keys():\n            new_name = name\n            if rename_dict is not None:\n                for rename_str in rename_dict.keys():\n                    if rename_str in name:\n                        new_name = new_name.replace(rename_str, rename_dict[rename_str])\n                        num_rename += 1\n            var_to_shape_map_new[new_name] = reader.get_tensor(name)\n        var_to_shape_map = dict(var_to_shape_map_new)\n\n        init_op, init_feed = tf.contrib.framework.assign_from_values(var_to_shape_map)\n        session.run(init_op, init_feed)\n        print(\'Initialized %d variables from %s.\' % (len(var_to_shape_map), checkpoint_path))\n\n\ndef calc_auc(x, y):\n    """""" Given x and y values it calculates the approx. integral and normalizes it: area under curve""""""\n    integral = np.trapz(y, x)\n    norm = np.trapz(np.ones_like(y), x)\n\n    return integral / norm\n\n\ndef get_stb_ref_curves():\n    """"""\n        Returns results of various baseline methods on the Stereo Tracking Benchmark Dataset reported by:\n        Zhang et al., \xe2\x80\x983d Hand Pose Tracking and Estimation Using Stereo Matching\xe2\x80\x99, 2016\n    """"""\n    curve_list = list()\n    thresh_mm = np.array([20.0, 25, 30, 35, 40, 45, 50])\n    pso_b1 = np.array([0.32236842,  0.53947368,  0.67434211,  0.75657895,  0.80921053, 0.86513158,  0.89473684])\n    curve_list.append((thresh_mm, pso_b1, \'PSO (AUC=%.3f)\' % calc_auc(thresh_mm, pso_b1)))\n    icppso_b1 = np.array([ 0.51973684,  0.64473684,  0.71710526,  0.77302632,  0.80921053, 0.84868421,  0.86842105])\n    curve_list.append((thresh_mm, icppso_b1, \'ICPPSO (AUC=%.3f)\' % calc_auc(thresh_mm, icppso_b1)))\n    chpr_b1 = np.array([ 0.56578947,  0.71710526,  0.82236842,  0.88157895,  0.91447368, 0.9375,  0.96052632])\n    curve_list.append((thresh_mm, chpr_b1, \'CHPR (AUC=%.3f)\' % calc_auc(thresh_mm, chpr_b1)))\n    return curve_list\n'"
utils/relative_trafo.py,50,"b'import tensorflow as tf\n\n\ndef _stitch_mat_from_vecs(vector_list):\n    """""" Stitches a given list of vectors into a 4x4 matrix.\n\n        Input:\n            vector_list: list of 16 tensors, which will be stitched into a matrix. list contains matrix elements\n                in a row-first fashion (m11, m12, m13, m14, m21, m22, m23, m24, ...). Length of the vectors has\n                to be the same, because it is interpreted as batch dimension.\n    """"""\n\n    assert len(vector_list) == 16, ""There have to be exactly 16 tensors in vector_list.""\n    batch_size = vector_list[0].get_shape().as_list()[0]\n    vector_list = [tf.reshape(x, [1, batch_size]) for x in vector_list]\n\n    trafo_matrix = tf.dynamic_stitch([[0], [1], [2], [3],\n                                      [4], [5], [6], [7],\n                                      [8], [9], [10], [11],\n                                      [12], [13], [14], [15]], vector_list)\n\n    trafo_matrix = tf.reshape(trafo_matrix, [4, 4, batch_size])\n    trafo_matrix = tf.transpose(trafo_matrix, [2, 0, 1])\n\n    return trafo_matrix\n\n\ndef _atan2(y, x):\n    """""" My implementation of atan2 in tensorflow.  Returns in -pi .. pi.""""""\n    tan = tf.atan(y / (x + 1e-8))  # this returns in -pi/2 .. pi/2\n\n    one_map = tf.ones_like(tan)\n\n    # correct quadrant error\n    correction = tf.where(tf.less(x + 1e-8, 0.0), 3.141592653589793*one_map, 0.0*one_map)\n    tan_c = tan + correction  # this returns in -pi/2 .. 3pi/2\n\n    # bring to positive values\n    correction = tf.where(tf.less(tan_c, 0.0), 2*3.141592653589793*one_map, 0.0*one_map)\n    tan_zero_2pi = tan_c + correction  # this returns in 0 .. 2pi\n\n    # make symmetric\n    correction = tf.where(tf.greater(tan_zero_2pi, 3.141592653589793), -2*3.141592653589793*one_map, 0.0*one_map)\n    tan_final = tan_zero_2pi + correction  # this returns in -pi .. pi\n    return tan_final\n\n\ndef _get_rot_mat_x_hom(angle):\n    """""" Returns a 3D rotation matrix in homogeneous coords.  """"""\n    one_vec = tf.ones_like(angle)\n    zero_vec = one_vec*0.0\n    trafo_matrix = _stitch_mat_from_vecs([one_vec, zero_vec, zero_vec, zero_vec,\n                                          zero_vec, tf.cos(angle), -tf.sin(angle), zero_vec,\n                                          zero_vec, tf.sin(angle), tf.cos(angle), zero_vec,\n                                          zero_vec, zero_vec, zero_vec, one_vec])\n    return trafo_matrix\n\n\ndef _get_rot_mat_y_hom(angle):\n    """""" Returns a 3D rotation matrix in homogeneous coords.  """"""\n    one_vec = tf.ones_like(angle)\n    zero_vec = one_vec*0.0\n    trafo_matrix = _stitch_mat_from_vecs([tf.cos(angle), zero_vec, tf.sin(angle), zero_vec,\n                                          zero_vec, one_vec, zero_vec, zero_vec,\n                                          -tf.sin(angle), zero_vec, tf.cos(angle), zero_vec,\n                                          zero_vec, zero_vec, zero_vec, one_vec])\n    return trafo_matrix\n\n\ndef _get_rot_mat_z_hom(angle):\n    """""" Returns a 3D rotation matrix in homogeneous coords. """"""\n    one_vec = tf.ones_like(angle)\n    zero_vec = one_vec*0.0\n    trafo_matrix = _stitch_mat_from_vecs([tf.cos(angle), -tf.sin(angle), zero_vec, zero_vec,\n                                          tf.sin(angle), tf.cos(angle), zero_vec, zero_vec,\n                                          zero_vec, zero_vec, one_vec, zero_vec,\n                                          zero_vec, zero_vec, zero_vec, one_vec])\n    return trafo_matrix\n\n\ndef _get_trans_mat_hom(trans):\n    """""" Returns a 3D translation matrix in homogeneous coords. """"""\n    one_vec = tf.ones_like(trans)\n    zero_vec = one_vec*0.0\n    trafo_matrix = _stitch_mat_from_vecs([one_vec, zero_vec, zero_vec, zero_vec,\n                                          zero_vec, one_vec, zero_vec, zero_vec,\n                                          zero_vec, zero_vec, one_vec, trans,\n                                          zero_vec, zero_vec, zero_vec, one_vec])\n    return trafo_matrix\n\n\ndef _to_hom(vector):\n    s = vector.get_shape().as_list()\n    vector = tf.reshape(vector, [s[0], -1, 1])\n    vector = tf.concat([vector, tf.ones((s[0], 1, 1))], 1)\n    return vector\n\n\ndef _from_hom(vector):\n    s = vector.get_shape().as_list()\n    vector = tf.reshape(vector, [s[0], -1, 1])\n    return vector[:, :-1, :]\n\n\ndef _forward(length, angle_x, angle_y, T):\n    """""" Given a articulations it calculates the update to the coord matrix and the location of the end point in global coords. """"""\n    # update current transformation from local -> new local\n    T_this = tf.matmul(_get_trans_mat_hom(-length), tf.matmul(_get_rot_mat_x_hom(-angle_x), _get_rot_mat_y_hom(-angle_y)))\n\n    # trafo from global -> new local\n    T = tf.matmul(T_this, T)\n\n    # calculate global location of this point\n    # x0 = tf.constant([[0.0], [0.0], [0.0], [1.0]])\n    s = length.get_shape().as_list()\n    x0 = _to_hom(tf.zeros((s[0], 3, 1)))\n    x = tf.matmul(tf.matrix_inverse(T), x0)\n    return x, T\n\n\ndef _backward(delta_vec, T):\n    """""" Given a vector it calculates the articulated angles and updates the current coord matrix. """"""\n    # calculate length directly\n    length = tf.sqrt(delta_vec[:, 0, 0]**2 + delta_vec[:, 1, 0]**2 + delta_vec[:, 2, 0]**2)\n\n    # calculate y rotation\n    angle_y = _atan2(delta_vec[:, 0, 0], delta_vec[:, 2, 0])\n\n    # this vector is an intermediate result and always has x=0\n    delta_vec_tmp = tf.matmul(_get_rot_mat_y_hom(-angle_y), delta_vec)\n\n    # calculate x rotation\n    angle_x = _atan2(-delta_vec_tmp[:, 1, 0], delta_vec_tmp[:, 2, 0])\n\n    # update current transformation from local -> new local\n    T_this = tf.matmul(_get_trans_mat_hom(-length), tf.matmul(_get_rot_mat_x_hom(-angle_x), _get_rot_mat_y_hom(-angle_y)))\n\n    # trafo from global -> new local\n    T = tf.matmul(T_this, T)\n\n    # make them all batched scalars\n    length = tf.reshape(length, [-1])\n    angle_x = tf.reshape(angle_x, [-1])\n    angle_y = tf.reshape(angle_y, [-1])\n    return length, angle_x, angle_y, T\n\n# Encodes how the kinematic chain goes; Is a mapping from child -> parent: dict[child] = parent\nkinematic_chain_dict = {0: \'root\',\n\n                        4: \'root\',\n                        3: 4,\n                        2: 3,\n                        1: 2,\n\n                        8: \'root\',\n                        7: 8,\n                        6: 7,\n                        5: 6,\n\n                        12: \'root\',\n                        11: 12,\n                        10: 11,\n                        9: 10,\n\n                        16: \'root\',\n                        15: 16,\n                        14: 15,\n                        13: 14,\n\n                        20: \'root\',\n                        19: 20,\n                        18: 19,\n                        17: 18}\n\n# order in which we will calculate stuff\nkinematic_chain_list = [0,\n                        4, 3, 2, 1,\n                        8, 7, 6, 5,\n                        12, 11, 10, 9,\n                        16, 15, 14, 13,\n                        20, 19, 18, 17]\n\n\ndef bone_rel_trafo(coords_xyz):\n    """""" Transforms the given real xyz coordinates into a bunch of relative frames.\n        The frames are set up according to the kinematic chain. Each parent of the chain\n        is the origin for the location of the next bone, where the z-axis is aligned with the bone\n        and articulation is measured as rotations along the x- and y- axes.\n\n        Inputs:\n            coords_xyz: BxNx3 matrix, containing the coordinates for each of the N keypoints\n    """"""\n    with tf.variable_scope(\'bone_rel_transformation\'):\n        coords_xyz = tf.reshape(coords_xyz, [-1, 21, 3])\n\n        # list of results\n        trafo_list = [None for _ in kinematic_chain_list]\n        coords_rel_list = [0.0 for _ in kinematic_chain_list]\n\n        # Iterate kinematic chain list (from root --> leaves)\n        for bone_id in kinematic_chain_list:\n\n            # get parent of current bone\n            parent_id = kinematic_chain_dict[bone_id]\n\n            if parent_id == \'root\':\n\n                # if there is no parent global = local\n                delta_vec = _to_hom(tf.expand_dims(coords_xyz[:, bone_id, :], 1))\n                T = _get_trans_mat_hom(tf.zeros_like(coords_xyz[:, 0, 0]))\n\n                # get articulation angles from bone vector\n                results = _backward(delta_vec, T)\n\n                # save results\n                coords_rel_list[bone_id] = tf.stack(results[:3], 1)\n                trafo_list[bone_id] = results[3]\n\n            else:\n                T = trafo_list[parent_id]  #by sticking to the order defined in kinematic_chain_list its ensured, that this is avail\n                assert T is not None, \'Something went wrong.\'\n\n                # calculate coords in local system\n                x_local_parent = tf.matmul(T, _to_hom(tf.expand_dims(coords_xyz[:, parent_id, :], 1)))\n                x_local_child = tf.matmul(T, _to_hom(tf.expand_dims(coords_xyz[:, bone_id, :], 1)))\n\n                # calculate bone vector in local coords\n                delta_vec = x_local_child - x_local_parent\n                delta_vec = _to_hom(tf.expand_dims(delta_vec[:, :3, :], 1))\n\n                # get articulation angles from bone vector\n                results = _backward(delta_vec, T)\n\n                # save results\n                coords_rel_list[bone_id] = tf.stack(results[:3], 1)\n                trafo_list[bone_id] = results[3]\n\n        coords_rel = tf.stack(coords_rel_list, 1)\n\n        return coords_rel\n\n\ndef bone_rel_trafo_inv(coords_rel):\n    """""" Assembles relative coords back to xyz coords. Inverse operation to bone_rel_trafo().\n\n        Inputs:\n            coords_rel: BxNx3 matrix, containing the coordinates for each of the N keypoints [length, angle_x, angle_y]\n    """"""\n    with tf.variable_scope(\'assemble_bone_rel\'):\n        s = coords_rel.get_shape().as_list()\n        if len(s) == 2:\n            coords_rel = tf.expand_dims(coords_rel, 0)\n            s = coords_rel.get_shape().as_list()\n        assert len(s) == 3, ""Has to be a batch of coords.""\n\n        # list of results\n        trafo_list = [None for _ in kinematic_chain_list]\n        coords_xyz_list = [0.0 for _ in kinematic_chain_list]\n\n        # Iterate kinematic chain list (from root --> leaves)\n        for bone_id in kinematic_chain_list:\n\n            # get parent of current bone\n            parent_id = kinematic_chain_dict[bone_id]\n\n            if parent_id == \'root\':\n                # if there is no parent global = local\n                T = _get_trans_mat_hom(tf.zeros_like(coords_rel[:, 0, 0]))\n\n                # get articulation angles from bone vector\n                x, T = _forward(length=coords_rel[:, bone_id, 0],\n                                angle_x=coords_rel[:, bone_id, 1],\n                                angle_y=coords_rel[:, bone_id, 2],\n                                T=T)\n\n                # save results\n                coords_xyz_list[bone_id] = tf.squeeze(_from_hom(x), [2])\n                trafo_list[bone_id] = T\n\n            else:\n                T = trafo_list[parent_id]  #by sticking to the order defined in kinematic_chain_list its ensured, that this is avail\n                assert T is not None, \'Something went wrong.\'\n\n                # get articulation angles from bone vector\n                x, T = _forward(length=coords_rel[:, bone_id, 0],\n                                angle_x=coords_rel[:, bone_id, 1],\n                                angle_y=coords_rel[:, bone_id, 2],\n                                T=T)\n\n                # save results\n                coords_xyz_list[bone_id] = tf.squeeze(_from_hom(x), [2])\n                trafo_list[bone_id] = T\n\n        coords_xyz = tf.stack(coords_xyz_list, 1)\n        return coords_xyz'"
