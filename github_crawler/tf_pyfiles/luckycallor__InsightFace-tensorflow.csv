file_path,api_count,code
evaluate.py,7,"b'import io\nimport os\nimport yaml\nimport pickle\nimport argparse\nimport numpy as np\nimport tensorflow as tf\n\nfrom scipy import misc\n\nfrom model import get_embd\nfrom eval.utils import calculate_roc, calculate_tar\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--mode\', type=str, default=\'build\', help=\'model mode: build\')\n    parser.add_argument(\'--config_path\', type=str, default=\'./configs/config_ms1m_100.yaml\', help=\'config path, used when mode is build\')\n    parser.add_argument(\'--model_path\', type=str, default=\'/data/hhd/InsightFace-tensorflow/output/20190116-130753/checkpoints/ckpt-m-116000\', help=\'model path\')\n    parser.add_argument(\'--val_data\', type=str, default=\'\', help=\'val data, a dict with key as data name, value as data path\')\n    parser.add_argument(\'--train_mode\', type=int, default=0, help=\'whether set train phase to True when getting embds. zero means False, one means True\')\n    parser.add_argument(\'--target_far\', type=float, default=1e-3, help=\'target far when calculate tar\')\n\n    return parser.parse_args()\n\n\ndef load_bin(path, image_size):\n    print(\'reading %s\' % path)\n    bins, issame_list = pickle.load(open(path, \'rb\'), encoding=\'bytes\')\n    num = len(bins)\n    images = np.zeros(shape=[num, image_size, image_size, 3], dtype=np.float32)\n    images_f = np.zeros(shape=[num, image_size, image_size, 3], dtype=np.float32)\n    # m = config[\'augment_margin\']\n    # s = int(m/2)\n    cnt = 0\n    for bin in bins:\n        img = misc.imread(io.BytesIO(bin))\n        img = misc.imresize(img, [image_size, image_size])\n        # img = img[s:s+image_size, s:s+image_size, :]\n        img_f = np.fliplr(img)\n        img = img/127.5-1.0\n        img_f = img_f/127.5-1.0\n        images[cnt] = img\n        images_f[cnt] = img_f\n        cnt += 1\n    print(\'done!\')\n    return (images, images_f, issame_list)\n\n\n\ndef evaluate(embeddings, actual_issame, far_target=1e-3, distance_metric=0, nrof_folds=10):\n    thresholds = np.arange(0, 4, 0.01)\n    if distance_metric == 1:\n        thresholdes = np.arange(0, 1, 0.0025)\n    embeddings1 = embeddings[0::2]\n    embeddings2 = embeddings[1::2]\n    tpr, fpr, accuracy = calculate_roc(thresholds, embeddings1, embeddings2, np.asarray(actual_issame), distance_metric=distance_metric, nrof_folds=nrof_folds)\n    tar, tar_std, far = calculate_tar(thresholds, embeddings1, embeddings2, np.asarray(actual_issame), far_target=far_target, distance_metric=distance_metric, nrof_folds=nrof_folds)\n    acc_mean = np.mean(accuracy)\n    acc_std = np.std(accuracy)\n    return tpr, fpr, acc_mean, acc_std, tar, tar_std, far\n\n\ndef run_embds(sess, images, batch_size, image_size, train_mode, embds_ph, image_ph, train_ph_dropout, train_ph_bn):\n    if train_mode >= 1:\n        train = True\n    else:\n        train = False\n    batch_num = len(images)//batch_size\n    left = len(images)%batch_size\n    embds = []\n    for i in range(batch_num):\n        image_batch = images[i*batch_size: (i+1)*batch_size]\n        cur_embd = sess.run(embds_ph, feed_dict={image_ph: image_batch, train_ph_dropout: train, train_ph_bn: train})\n        embds += list(cur_embd)\n        print(\'%d/%d\' % (i, batch_num), end=\'\\r\')\n    if left > 0:\n        image_batch = np.zeros([batch_size, image_size, image_size, 3])\n        image_batch[:left, :, :, :] = images[-left:]\n        cur_embd = sess.run(embds_ph, feed_dict={image_ph: image_batch, train_ph_dropout: train, train_ph_bn: train})\n        embds += list(cur_embd)[:left]\n    print()\n    print(\'done!\')\n    return np.array(embds)\n\n\nif __name__ == \'__main__\':\n    args = get_args()\n    if args.mode == \'build\':\n        print(\'building...\')\n        config = yaml.load(open(args.config_path))\n        images = tf.placeholder(dtype=tf.float32, shape=[None, config[\'image_size\'], config[\'image_size\'], 3], name=\'input_image\')\n        train_phase_dropout = tf.placeholder(dtype=tf.bool, shape=None, name=\'train_phase\')\n        train_phase_bn = tf.placeholder(dtype=tf.bool, shape=None, name=\'train_phase_last\')\n        embds, _ = get_embd(images, train_phase_dropout, train_phase_bn, config)\n        print(\'done!\')\n        tf_config = tf.ConfigProto(allow_soft_placement=True)\n        tf_config.gpu_options.allow_growth = True\n        with tf.Session(config=tf_config) as sess:\n            tf.global_variables_initializer().run()\n            print(\'loading...\')\n            saver = tf.train.Saver()\n            saver.restore(sess, args.model_path)\n            print(\'done!\')\n\n            batch_size = config[\'batch_size\']\n            # batch_size = 32\n            print(\'evaluating...\')\n            val_data = {}\n            if args.val_data == \'\':\n                val_data = config[\'val_data\']\n            else:\n                val_data[os.path.basename(args.val_data)] = args.val_data\n            for k, v in val_data.items():\n                imgs, imgs_f, issame = load_bin(v, config[\'image_size\'])\n                print(\'forward running...\')\n                embds_arr = run_embds(sess, imgs, batch_size, config[\'image_size\'], args.train_mode, embds, images, train_phase_dropout, train_phase_bn)\n                embds_f_arr = run_embds(sess, imgs_f, batch_size, config[\'image_size\'], args.train_mode, embds, images, train_phase_dropout, train_phase_bn)\n                embds_arr = embds_arr/np.linalg.norm(embds_arr, axis=1, keepdims=True)+embds_f_arr/np.linalg.norm(embds_f_arr, axis=1, keepdims=True)\n                print(\'done!\')\n                tpr, fpr, acc_mean, acc_std, tar, tar_std, far = evaluate(embds_arr, issame, far_target=args.target_far, distance_metric=0)\n                print(\'eval on %s: acc--%1.5f+-%1.5f, tar--%1.5f+-%1.5f@far=%1.5f\' % (k, acc_mean, acc_std, tar, tar_std, far))\n            print(\'done!\')\n    else:\n        raise ValueError(""Invalid value for --mode."")\n\n'"
finetune_softmax.py,48,"b'import os\nimport time\nimport pickle\nimport argparse\nimport numpy as np\n\nimport io\nimport yaml\nfrom scipy import misc\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom datetime import datetime\n\nfrom losses.logit_loss import get_logits\nfrom data.classificationDataTool import ClassificationImageData\nfrom model import get_embd\nfrom utils import average_gradients, check_folders, analyze_vars\nfrom evaluate import load_bin, evaluate\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--config_path\', type=str, help=\'path to config file\', default=\'./configs/config_finetune.yaml\')\n\n    return parser.parse_args()\n\n\ndef inference(images, labels, is_training_dropout, is_training_bn, config):\n    embds, end_points = get_embd(images, is_training_dropout, is_training_bn, config)\n    logits = get_logits(embds, labels, config)\n    end_points[\'logits\'] = logits\n    return embds, logits, end_points\n\n\nclass Trainer:\n    def __init__(self, config):\n        self.config = config\n        subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n        self.output_dir = os.path.join(config[\'output_dir\'], subdir)\n        self.model_dir = os.path.join(self.output_dir, \'models\')\n        self.log_dir = os.path.join(self.output_dir, \'log\')\n        self.checkpoint_dir = os.path.join(self.output_dir, \'checkpoints\')\n        self.debug_dir = os.path.join(self.output_dir, \'debug\')\n        check_folders([self.output_dir, self.model_dir, self.log_dir, self.checkpoint_dir, self.debug_dir])\n        self.val_log = os.path.join(self.output_dir, \'val_log.txt\')\n\n        self.batch_size = config[\'batch_size\']\n        self.gpu_num = config[\'gpu_num\']\n        if self.batch_size % self.gpu_num != 0:\n            raise ValueError(\'batch_size must be a multiple of gpu_num\')\n        self.image_size = config[\'image_size\']\n        self.epoch_num = config[\'epoch_num\']\n        self.step_per_epoch = config[\'step_per_epoch\']\n        self.val_freq = config[\'val_freq\']\n        self.val_data = config[\'val_data\']\n        self.val_bn_train = config[\'val_bn_train\']\n        # for k, v in config[\'val_data\'].items():\n        #     self.val_data[k] = load_bin(v, self.image_size)\n        #     imgs = self.val_data[k][0]\n        #     np.save(os.path.join(self.debug_dir, k+\'.npy\'), imgs[:100])\n\n        with open(os.path.join(self.output_dir, \'config.yaml\'), \'w\') as f:\n            f.write(yaml.dump(self.config))\n\n\n    def build(self):\n        self.train_phase_dropout = tf.placeholder(dtype=tf.bool, shape=None, name=\'train_phase_dropout\')\n        self.train_phase_bn = tf.placeholder(dtype=tf.bool, shape=None, name=\'train_phase_bn\')\n        self.global_step = tf.Variable(name=\'global_step\', initial_value=0, trainable=False)\n        self.inc_op = tf.assign_add(self.global_step, 1, name=\'increment_global_step\')\n        scale = int(512.0/self.batch_size)\n        lr_steps = [scale*s for s in self.config[\'lr_steps\']]\n        lr_values = [v/scale for v in self.config[\'lr_values\']]\n        # lr_steps = self.config[\'lr_steps\']\n        self.lr = tf.train.piecewise_constant(self.global_step, boundaries=lr_steps, values=lr_values, name=\'lr_schedule\')\n\n        cid = ClassificationImageData(img_size=self.image_size, augment_flag=self.config[\'augment_flag\'], augment_margin=self.config[\'augment_margin\'])\n        train_dataset = cid.read_TFRecord(self.config[\'train_data\']).shuffle(10000).repeat().batch(self.batch_size)\n        train_iterator = train_dataset.make_one_shot_iterator()\n        self.train_images, self.train_labels = train_iterator.get_next()\n        self.train_images = tf.identity(self.train_images, \'input_images\')\n        self.train_labels = tf.identity(self.train_labels, \'labels\')\n        if self.gpu_num <= 1:\n            self.embds, self.logits, self.end_points = inference(self.train_images, self.train_labels, self.train_phase_dropout, self.train_phase_bn, self.config)\n            self.embds = tf.identity(self.embds, \'embeddings\')\n            self.inference_loss = slim.losses.sparse_softmax_cross_entropy(logits=self.logits, labels=self.train_labels)\n            self.wd_loss = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n            self.train_loss = self.inference_loss+self.wd_loss\n            pred = tf.arg_max(tf.nn.softmax(self.logits), dimension=-1, output_type=tf.int64)\n            self.train_acc = tf.reduce_mean(tf.cast(tf.equal(pred, self.train_labels), tf.float32))\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            vars_softmax = [v for v in tf.trainable_variables() if \'embd_extractor\' not in v.name]\n            with tf.control_dependencies(update_ops):\n                self.train_op = tf.train.MomentumOptimizer(learning_rate=self.lr, momentum=self.config[\'momentum\']).minimize(self.train_loss)\n                self.train_op_softmax = tf.train.MomentumOptimizer(learning_rate=self.lr, momentum=self.config[\'momentum\']).minimize(self.train_loss, var_list=vars_softmax)\n        else:\n            self.embds = []\n            self.logits = []\n            self.inference_loss = []\n            self.wd_loss = []\n            self.train_loss = []\n            pred = []\n            tower_grads = []\n            tower_grads_softmax = []\n            update_ops = []\n            opt = tf.train.MomentumOptimizer(learning_rate=self.lr, momentum=self.config[\'momentum\'])\n            train_images = tf.split(self.train_images, self.gpu_num)\n            train_labels = tf.split(self.train_labels, self.gpu_num)\n            for i in range(self.gpu_num):\n                sub_train_images = train_images[i]\n                sub_train_labels = train_labels[i]\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.variable_scope(tf.get_variable_scope(), reuse=(i > 0)):\n                        embds, logits, end_points = inference(sub_train_images, sub_train_labels, self.train_phase_dropout, self.train_phase_bn, self.config)\n                        inference_loss = slim.losses.sparse_softmax_cross_entropy(logits=logits, labels=sub_train_labels)\n                        wd_loss = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n                        train_loss = inference_loss+wd_loss\n                        pred.append(tf.arg_max(tf.nn.softmax(logits), dimension=-1, output_type=tf.int64))\n                        vars_softmax = [v for v in tf.trainable_variables() if \'embd_extractor\' not in v.name]\n                        tower_grads.append(opt.compute_gradients(train_loss))\n                        tower_grads_softmax.append(opt.compute_gradients(train_loss, var_list=vars_softmax))\n                        update_ops.append(tf.get_collection(tf.GraphKeys.UPDATE_OPS))\n                        self.embds.append(embds)\n                        self.logits.append(logits)\n                        self.inference_loss.append(inference_loss)\n                        self.wd_loss.append(wd_loss)\n                        self.train_loss.append(train_loss)\n            self.embds = tf.concat(self.embds, axis=0)\n            self.logits = tf.concat(self.logits, axis=0)\n            self.inference_loss = tf.add_n(self.inference_loss)/self.gpu_num\n            self.wd_loss = tf.add_n(self.wd_loss)/self.gpu_num\n            self.train_loss = tf.add_n(self.train_loss)/self.gpu_num\n            pred = tf.concat(pred, axis=0)\n            self.train_acc = tf.reduce_mean(tf.cast(tf.equal(pred, self.train_labels), tf.float32))\n            train_ops = [opt.apply_gradients(average_gradients(tower_grads))]\n            train_ops_softmax = [opt.apply_gradients(average_gradients(tower_grads_softmax))]\n            train_ops.extend(update_ops)\n            train_ops_softmax.extend(update_ops)\n            self.train_op = tf.group(*train_ops)\n            self.train_op_softmax = tf.group(*train_ops_softmax)\n\n\n        self.train_summary = tf.summary.merge([\n            tf.summary.scalar(\'inference_loss\', self.inference_loss),\n            tf.summary.scalar(\'wd_loss\', self.wd_loss),\n            tf.summary.scalar(\'train_loss\', self.train_loss),\n            tf.summary.scalar(\'train_acc\', self.train_acc)\n        ])\n\n    def run_embds(self, sess, images):\n        batch_num = len(images)//self.batch_size\n        left = len(images)%self.batch_size\n        embds = []\n        for i in range(batch_num):\n            cur_embd = sess.run(self.embds, feed_dict={self.train_images: images[i*self.batch_size: (i+1)*self.batch_size], self.train_phase_dropout: False, self.train_phase_bn: self.val_bn_train})\n            embds += list(cur_embd)\n        if left > 0:\n            image_batch = np.zeros([self.batch_size, self.image_size, self.image_size, 3])\n            image_batch[:left, :, :, :] = images[-left:]\n            cur_embd = sess.run(self.embds, feed_dict={self.train_images: image_batch, self.train_phase_dropout: False, self.train_phase_bn: self.val_bn_train})\n            embds += list(cur_embd)[:left]\n        return np.array(embds)\n\n    def save_image_label(self, images, labels, step):\n        save_dir = os.path.join(self.debug_dir, \'image_by_label\')\n        for i in range(len(labels)):\n            if(labels[i] < 10):\n                cur_save_dir = os.path.join(save_dir, str(labels[i]))\n                check_folders(cur_save_dir)\n                misc.imsave(os.path.join(cur_save_dir, \'%d_%d.jpg\' % (step, i)), images[i])\n\n\n    def train(self):\n        self.build()\n        analyze_vars(tf.trainable_variables(), os.path.join(self.output_dir, \'model_vars.txt\'))\n        with open(os.path.join(self.output_dir, \'regularizers.txt\'), \'w\') as f:\n            for v in tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES):\n                f.write(v.name+\'\\n\')\n        # exit(-1)\n        tf_config = tf.ConfigProto(allow_soft_placement=True)\n        tf_config.gpu_options.allow_growth = True\n        with tf.Session(config=tf_config) as sess:\n            tf.global_variables_initializer().run()\n            saver_ckpt = tf.train.Saver()\n            saver_best = tf.train.Saver()\n            saver_embd = tf.train.Saver(var_list=[v for v in tf.trainable_variables() if \'embd_extractor\' in v.name])\n            if config[\'pretrained_model\'] != \'\':\n                saver_embd.restore(sess, config[\'pretrained_model\'])\n            summary_writer = tf.summary.FileWriter(self.log_dir, sess.graph)\n            start_time = time.time()\n            best_acc = 0\n            counter = 0\n            debug = True\n            for i in range(self.epoch_num):\n                if i < config[\'fixed_epoch_num\']:\n                    cur_train_op = self.train_op_softmax\n                else:\n                    cur_train_op = self.train_op\n                for j in range(self.step_per_epoch):\n                    _, l, l_wd, l_inf, acc, s, _ = sess.run([cur_train_op, self.train_loss, self.wd_loss, self.inference_loss, self.train_acc, self.train_summary, self.inc_op], feed_dict={self.train_phase_dropout: True, self.train_phase_bn: True})\n                    counter += 1\n                    \n                    print(""Epoch: [%2d/%2d] [%6d/%6d] time: %.2f, loss: %.3f (inference: %.3f, wd: %.3f), acc: %.3f"" % (i, self.epoch_num, j, self.step_per_epoch, time.time() - start_time, l, l_inf, l_wd, acc))\n                    start_time = time.time()\n                    if counter % self.val_freq == 0:\n                        saver_ckpt.save(sess, os.path.join(self.checkpoint_dir, \'ckpt-m\'), global_step=counter)\n                        acc = []\n                        with open(self.val_log, \'a\') as f:\n                            f.write(\'step: %d\\n\' % counter)\n                            for k, v in self.val_data.items():\n                                imgs, imgs_f, issame = load_bin(v, self.image_size)\n                                embds = self.run_embds(sess, imgs)\n                                embds_f = self.run_embds(sess, imgs_f)\n                                embds = embds/np.linalg.norm(embds, axis=1, keepdims=True)+embds_f/np.linalg.norm(embds_f, axis=1, keepdims=True)\n                                tpr, fpr, acc_mean, acc_std, tar, tar_std, far = evaluate(embds, issame, far_target=1e-3, distance_metric=0)\n                                f.write(\'eval on %s: acc--%1.5f+-%1.5f, tar--%1.5f+-%1.5f@far=%1.5f\\n\' % (k, acc_mean, acc_std, tar, tar_std, far))\n                                acc.append(acc_mean)\n                            acc = np.mean(np.array(acc))\n                            if acc > best_acc:\n                                saver_best.save(sess, os.path.join(self.model_dir, \'best-m\'), global_step=counter)\n                                best_acc = acc\n\n                        \nif __name__ == \'__main__\':\n    args = parse_args()\n    config = yaml.load(open(args.config_path))\n    trainer = Trainer(config)\n    trainer.train()\n\n\n                    '"
get_embd.py,7,"b""import io\nimport os\nimport yaml\nimport pickle\nimport argparse\nimport numpy as np\nimport tensorflow as tf\n\nfrom scipy import misc\n\nfrom model import get_embd\nfrom eval.utils import calculate_roc, calculate_tar\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--mode', type=str, default='build', help='model mode: build')\n    parser.add_argument('--config_path', type=str, default='./configs/config_ms1m_100.yaml', help='config path, used when mode is build')\n    parser.add_argument('--model_path', type=str, default='/data/hhd/InsightFace-tensorflow/output/20190116-130753/checkpoints/ckpt-m-116000', help='model path')\n    parser.add_argument('--read_path', type=str, default='', help='path to image file or directory to images')\n    parser.add_argument('--save_path', type=str, default='embds.pkl', help='path to save embds')\n    parser.add_argument('--train_mode', type=int, default=0, help='whether set train phase to True when getting embds. zero means False, one means True')\n\n    return parser.parse_args()\n\n\ndef load_image(path, image_size):\n    print('reading %s' % path)\n    if os.path.isdir(path):\n        paths = list(os.listdir(path))\n    else:\n        paths = [path]\n    images = []\n    images_f = []\n    for path in paths:\n        img = misc.imread(path)\n        img = misc.imresize(img, [image_size, image_size])\n        # img = img[s:s+image_size, s:s+image_size, :]\n        img_f = np.fliplr(img)\n        img = img/127.5-1.0\n        img_f = img_f/127.5-1.0\n        images.append(img)\n        images_f.append(img_f)\n    fns = [os.path.basename(p) for p in paths]\n    print('done!')\n    return (np.array(images), np.array(images_f), fns)\n\n\n\ndef evaluate(embeddings, actual_issame, far_target=1e-3, distance_metric=0, nrof_folds=10):\n    thresholds = np.arange(0, 4, 0.01)\n    if distance_metric == 1:\n        thresholdes = np.arange(0, 1, 0.0025)\n    embeddings1 = embeddings[0::2]\n    embeddings2 = embeddings[1::2]\n    tpr, fpr, accuracy = calculate_roc(thresholds, embeddings1, embeddings2, np.asarray(actual_issame), distance_metric=distance_metric, nrof_folds=nrof_folds)\n    tar, tar_std, far = calculate_tar(thresholds, embeddings1, embeddings2, np.asarray(actual_issame), far_target=far_target, distance_metric=distance_metric, nrof_folds=nrof_folds)\n    acc_mean = np.mean(accuracy)\n    acc_std = np.std(accuracy)\n    return tpr, fpr, acc_mean, acc_std, tar, tar_std, far\n\n\ndef run_embds(sess, images, batch_size, image_size, train_mode, embds_ph, image_ph, train_ph_dropout, train_ph_bn):\n    if train_mode >= 1:\n        train = True\n    else:\n        train = False\n    batch_num = len(images)//batch_size\n    left = len(images)%batch_size\n    embds = []\n    for i in range(batch_num):\n        image_batch = images[i*batch_size: (i+1)*batch_size]\n        cur_embd = sess.run(embds_ph, feed_dict={image_ph: image_batch, train_ph_dropout: train, train_ph_bn: train})\n        embds += list(cur_embd)\n        print('%d/%d' % (i, batch_num), end='\\r')\n    if left > 0:\n        image_batch = np.zeros([batch_size, image_size, image_size, 3])\n        image_batch[:left, :, :, :] = images[-left:]\n        cur_embd = sess.run(embds_ph, feed_dict={image_ph: image_batch, train_ph_dropout: train, train_ph_bn: train})\n        embds += list(cur_embd)[:left]\n    print()\n    print('done!')\n    return np.array(embds)\n\n\nif __name__ == '__main__':\n    args = get_args()\n    if args.mode == 'build':\n        print('building...')\n        config = yaml.load(open(args.config_path))\n        images = tf.placeholder(dtype=tf.float32, shape=[None, config['image_size'], config['image_size'], 3], name='input_image')\n        train_phase_dropout = tf.placeholder(dtype=tf.bool, shape=None, name='train_phase')\n        train_phase_bn = tf.placeholder(dtype=tf.bool, shape=None, name='train_phase_last')\n        embds, _ = get_embd(images, train_phase_dropout, train_phase_bn, config)\n        print('done!')\n        tf_config = tf.ConfigProto(allow_soft_placement=True)\n        tf_config.gpu_options.allow_growth = True\n        with tf.Session(config=tf_config) as sess:\n            tf.global_variables_initializer().run()\n            print('loading...')\n            saver = tf.train.Saver(var_list=tf.trainable_variables())\n            saver.restore(sess, args.model_path)\n            print('done!')\n\n            batch_size = config['batch_size']\n            imgs, imgs_f, fns = load_image(args.read_path, config['image_size'])\n            print('forward running...')\n            embds_arr = run_embds(sess, imgs, batch_size, config['image_size'], args.train_mode, embds, images, train_phase_dropout, train_phase_bn)\n            embds_f_arr = run_embds(sess, imgs_f, batch_size, config['image_size'], args.train_mode, embds, images, train_phase_dropout, train_phase_bn)\n            embds_arr = embds_arr/np.linalg.norm(embds_arr, axis=1, keepdims=True)+embds_f_arr/np.linalg.norm(embds_f_arr, axis=1, keepdims=True)\n            embds_arr = embds_arr/np.linalg.norm(embds_arr, axis=1, keepdims=True)\n            print('done!')\n            print('saving...')\n            embds_dict = dict(*zip(fns, list(embds_arr)))\n            pickle.dump(embds_dict, open(args.save_path, 'wb'))\n            print('done!')\n\n"""
model.py,1,"b""import tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom backbones import modifiedResNet_v2, ResNet_v2\n\n\ndef get_embd(inputs, is_training_dropout, is_training_bn, config, reuse=False, scope='embd_extractor'):\n    with tf.variable_scope(scope, reuse=reuse):\n        net = inputs\n        end_points = {}\n        if config['backbone_type'].startswith('resnet_v2_m'):\n            arg_sc = modifiedResNet_v2.resnet_arg_scope(weight_decay=config['weight_decay'], batch_norm_decay=config['bn_decay'])\n            with slim.arg_scope(arg_sc):\n                if config['backbone_type'] == 'resnet_v2_m_50':\n                    net, end_points = modifiedResNet_v2.resnet_v2_m_50(net, is_training=is_training_bn, return_raw=True)\n                elif config['backbone_type'] == 'resnet_v2_m_101':\n                    net, end_points = modifiedResNet_v2.resnet_v2_m_101(net, is_training=is_training_bn, return_raw=True)\n                elif config['backbone_type'] == 'resnet_v2_m_152':\n                    net, end_points = modifiedResNet_v2.resnet_v2_m_152(net, is_training=is_training_bn, return_raw=True)\n                elif config['backbone_type'] == 'resnet_v2_m_200':\n                    net, end_points = modifiedResNet_v2.resnet_v2_m_200(net, is_training=is_training_bn, return_raw=True)\n                else:\n                    raise ValueError('Invalid backbone type.')\n        elif config['backbone_type'].startswith('resnet_v2'):\n            arg_sc = ResNet_v2.resnet_arg_scope(weight_decay=config['weight_decay'], batch_norm_decay=config['bn_decay'])\n            with slim.arg_scope(arg_sc):\n                if config['backbone_type'] == 'resnet_v2_50':\n                    net, end_points = ResNet_v2.resnet_v2_50(net, is_training=is_training_bn, return_raw=True)\n                elif config['backbone_type'] == 'resnet_v2_101':\n                    net, end_points = ResNet_v2.resnet_v2_101(net, is_training=is_training_bn, return_raw=True)\n                elif config['backbone_type'] == 'resnet_v2_152':\n                    net, end_points = ResNet_v2.resnet_v2_152(net, is_training=is_training_bn, return_raw=True)\n                elif config['backbone_type'] == 'resnet_v2_200':\n                    net, end_points = ResNet_v2.resnet_v2_200(net, is_training=is_training_bn, return_raw=True)\n        else:\n            raise ValueError('Invalid backbone type.')\n\n        if config['out_type'] == 'E':\n            with slim.arg_scope(arg_sc):\n                net = slim.batch_norm(net, activation_fn=None, is_training=is_training_bn)\n                net = slim.dropout(net, keep_prob=config['keep_prob'], is_training=is_training_dropout)\n                net = slim.flatten(net)\n                net = slim.fully_connected(net, config['embd_size'], normalizer_fn=None, activation_fn=None)\n                net = slim.batch_norm(net, scale=False, activation_fn=None, is_training=is_training_bn)\n                end_points['embds'] = net\n        else:\n            raise ValueError('Invalid out type.')\n        \n        return net, end_points\n\n\n        \n        """
train_softmax.py,44,"b'import os\nimport time\nimport pickle\nimport argparse\nimport numpy as np\n\nimport io\nimport yaml\nfrom scipy import misc\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom datetime import datetime\n\nfrom losses.logit_loss import get_logits\nfrom data.classificationDataTool import ClassificationImageData\nfrom model import get_embd\nfrom utils import average_gradients, check_folders, analyze_vars\nfrom evaluate import load_bin, evaluate\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--config_path\', type=str, help=\'path to config file\', default=\'./configs/config_ms1m_100.yaml\')\n\n    return parser.parse_args()\n\n\ndef inference(images, labels, is_training_dropout, is_training_bn, config):\n    embds, end_points = get_embd(images, is_training_dropout, is_training_bn, config)\n    logits = get_logits(embds, labels, config)\n    end_points[\'logits\'] = logits\n    return embds, logits, end_points\n\n\nclass Trainer:\n    def __init__(self, config):\n        self.config = config\n        subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n        self.output_dir = os.path.join(config[\'output_dir\'], subdir)\n        self.model_dir = os.path.join(self.output_dir, \'models\')\n        self.log_dir = os.path.join(self.output_dir, \'log\')\n        self.checkpoint_dir = os.path.join(self.output_dir, \'checkpoints\')\n        self.debug_dir = os.path.join(self.output_dir, \'debug\')\n        check_folders([self.output_dir, self.model_dir, self.log_dir, self.checkpoint_dir, self.debug_dir])\n        self.val_log = os.path.join(self.output_dir, \'val_log.txt\')\n\n        self.batch_size = config[\'batch_size\']\n        self.gpu_num = config[\'gpu_num\']\n        if self.batch_size % self.gpu_num != 0:\n            raise ValueError(\'batch_size must be a multiple of gpu_num\')\n        self.image_size = config[\'image_size\']\n        self.epoch_num = config[\'epoch_num\']\n        self.step_per_epoch = config[\'step_per_epoch\']\n        self.val_freq = config[\'val_freq\']\n        self.val_data = config[\'val_data\']\n        self.val_bn_train = config[\'val_bn_train\']\n        # for k, v in config[\'val_data\'].items():\n        #     self.val_data[k] = load_bin(v, self.image_size)\n        #     imgs = self.val_data[k][0]\n        #     np.save(os.path.join(self.debug_dir, k+\'.npy\'), imgs[:100])\n\n        with open(os.path.join(self.output_dir, \'config.yaml\'), \'w\') as f:\n            f.write(yaml.dump(self.config))\n\n\n    def build(self):\n        self.train_phase_dropout = tf.placeholder(dtype=tf.bool, shape=None, name=\'train_phase_dropout\')\n        self.train_phase_bn = tf.placeholder(dtype=tf.bool, shape=None, name=\'train_phase_bn\')\n        self.global_step = tf.Variable(name=\'global_step\', initial_value=0, trainable=False)\n        self.inc_op = tf.assign_add(self.global_step, 1, name=\'increment_global_step\')\n        scale = int(512.0/self.batch_size)\n        lr_steps = [scale*s for s in self.config[\'lr_steps\']]\n        lr_values = [v/scale for v in self.config[\'lr_values\']]\n        # lr_steps = self.config[\'lr_steps\']\n        self.lr = tf.train.piecewise_constant(self.global_step, boundaries=lr_steps, values=lr_values, name=\'lr_schedule\')\n\n        cid = ClassificationImageData(img_size=self.image_size, augment_flag=self.config[\'augment_flag\'], augment_margin=self.config[\'augment_margin\'])\n        train_dataset = cid.read_TFRecord(self.config[\'train_data\']).shuffle(10000).repeat().batch(self.batch_size)\n        train_iterator = train_dataset.make_one_shot_iterator()\n        self.train_images, self.train_labels = train_iterator.get_next()\n        self.train_images = tf.identity(self.train_images, \'input_images\')\n        self.train_labels = tf.identity(self.train_labels, \'labels\')\n        if self.gpu_num <= 1:\n            self.embds, self.logits, self.end_points = inference(self.train_images, self.train_labels, self.train_phase_dropout, self.train_phase_bn, self.config)\n            self.embds = tf.identity(self.embds, \'embeddings\')\n            self.inference_loss = slim.losses.sparse_softmax_cross_entropy(logits=self.logits, labels=self.train_labels)\n            self.wd_loss = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n            self.train_loss = self.inference_loss+self.wd_loss\n            pred = tf.arg_max(tf.nn.softmax(self.logits), dimension=-1, output_type=tf.int64)\n            self.train_acc = tf.reduce_mean(tf.cast(tf.equal(pred, self.train_labels), tf.float32))\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                self.train_op = tf.train.MomentumOptimizer(learning_rate=self.lr, momentum=self.config[\'momentum\']).minimize(self.train_loss)\n        else:\n            self.embds = []\n            self.logits = []\n            self.inference_loss = []\n            self.wd_loss = []\n            self.train_loss = []\n            pred = []\n            tower_grads = []\n            update_ops = []\n            opt = tf.train.MomentumOptimizer(learning_rate=self.lr, momentum=self.config[\'momentum\'])\n            train_images = tf.split(self.train_images, self.gpu_num)\n            train_labels = tf.split(self.train_labels, self.gpu_num)\n            for i in range(self.gpu_num):\n                sub_train_images = train_images[i]\n                sub_train_labels = train_labels[i]\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.variable_scope(tf.get_variable_scope(), reuse=(i > 0)):\n                        embds, logits, end_points = inference(sub_train_images, sub_train_labels, self.train_phase_dropout, self.train_phase_bn, self.config)\n                        inference_loss = slim.losses.sparse_softmax_cross_entropy(logits=logits, labels=sub_train_labels)\n                        wd_loss = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n                        train_loss = inference_loss+wd_loss\n                        pred.append(tf.arg_max(tf.nn.softmax(logits), dimension=-1, output_type=tf.int64))\n                        tower_grads.append(opt.compute_gradients(train_loss))\n                        update_ops.append(tf.get_collection(tf.GraphKeys.UPDATE_OPS))\n                        self.embds.append(embds)\n                        self.logits.append(logits)\n                        self.inference_loss.append(inference_loss)\n                        self.wd_loss.append(wd_loss)\n                        self.train_loss.append(train_loss)\n            self.embds = tf.concat(self.embds, axis=0)\n            self.logits = tf.concat(self.logits, axis=0)\n            self.inference_loss = tf.add_n(self.inference_loss)/self.gpu_num\n            self.wd_loss = tf.add_n(self.wd_loss)/self.gpu_num\n            self.train_loss = tf.add_n(self.train_loss)/self.gpu_num\n            pred = tf.concat(pred, axis=0)\n            self.train_acc = tf.reduce_mean(tf.cast(tf.equal(pred, self.train_labels), tf.float32))\n            train_ops = [opt.apply_gradients(average_gradients(tower_grads))]\n            train_ops.extend(update_ops)\n            self.train_op = tf.group(*train_ops)\n\n\n        self.train_summary = tf.summary.merge([\n            tf.summary.scalar(\'inference_loss\', self.inference_loss),\n            tf.summary.scalar(\'wd_loss\', self.wd_loss),\n            tf.summary.scalar(\'train_loss\', self.train_loss),\n            tf.summary.scalar(\'train_acc\', self.train_acc)\n        ])\n\n    def run_embds(self, sess, images):\n        batch_num = len(images)//self.batch_size\n        left = len(images)%self.batch_size\n        embds = []\n        for i in range(batch_num):\n            cur_embd = sess.run(self.embds, feed_dict={self.train_images: images[i*self.batch_size: (i+1)*self.batch_size], self.train_phase_dropout: False, self.train_phase_bn: self.val_bn_train})\n            embds += list(cur_embd)\n        if left > 0:\n            image_batch = np.zeros([self.batch_size, self.image_size, self.image_size, 3])\n            image_batch[:left, :, :, :] = images[-left:]\n            cur_embd = sess.run(self.embds, feed_dict={self.train_images: image_batch, self.train_phase_dropout: False, self.train_phase_bn: self.val_bn_train})\n            embds += list(cur_embd)[:left]\n        return np.array(embds)\n\n    def save_image_label(self, images, labels, step):\n        save_dir = os.path.join(self.debug_dir, \'image_by_label\')\n        for i in range(len(labels)):\n            if(labels[i] < 10):\n                cur_save_dir = os.path.join(save_dir, str(labels[i]))\n                check_folders(cur_save_dir)\n                misc.imsave(os.path.join(cur_save_dir, \'%d_%d.jpg\' % (step, i)), images[i])\n\n\n    def train(self):\n        self.build()\n        analyze_vars(tf.trainable_variables(), os.path.join(self.output_dir, \'model_vars.txt\'))\n        with open(os.path.join(self.output_dir, \'regularizers.txt\'), \'w\') as f:\n            for v in tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES):\n                f.write(v.name+\'\\n\')\n        # exit(-1)\n        tf_config = tf.ConfigProto(allow_soft_placement=True)\n        tf_config.gpu_options.allow_growth = True\n        with tf.Session(config=tf_config) as sess:\n            tf.global_variables_initializer().run()\n            saver_ckpt = tf.train.Saver()\n            saver_best = tf.train.Saver()\n            summary_writer = tf.summary.FileWriter(self.log_dir, sess.graph)\n            start_time = time.time()\n            best_acc = 0\n            counter = 0\n            if config[\'pretrained_model\'] != \'\':\n                saver_ckpt.restore(sess, config[\'pretrained_model\'])\n                step = int(os.path.basename(config[\'pretrained_model\']).split(\'.\')[0].split(\'-\')[-1])\n                sess.run(tf.assign(self.global_step, step))\n                counter = self.global_step.eval(sess)\n                print(\'start step: %d\' % counter)\n            debug = True\n            for i in range(self.epoch_num):\n                for j in range(self.step_per_epoch):\n                    _, l, l_wd, l_inf, acc, s, _ = sess.run([self.train_op, self.train_loss, self.wd_loss, self.inference_loss, self.train_acc, self.train_summary, self.inc_op], feed_dict={self.train_phase_dropout: True, self.train_phase_bn: True})\n                    counter += 1\n\n                    # debug\n                    # self.save_image_label(train_img, train_lbl, counter)\n                    # if(debug):\n                    #     if(len(train_imgs) < 100):\n                    #         train_imgs.append(train_img[0])\n                    #     else:\n                    #         np.save(os.path.join(self.debug_dir, \'train_imgs.npy\'), np.array(train_imgs))\n                    #         debug=False\n                    \n                    print(""Epoch: [%2d/%2d] [%6d/%6d] time: %.2f, loss: %.3f (inference: %.3f, wd: %.3f), acc: %.3f"" % (i, self.epoch_num, j, self.step_per_epoch, time.time() - start_time, l, l_inf, l_wd, acc))\n                    start_time = time.time()\n                    if counter % self.val_freq == 0:\n                        saver_ckpt.save(sess, os.path.join(self.checkpoint_dir, \'ckpt-m\'), global_step=counter)\n                        acc = []\n                        with open(self.val_log, \'a\') as f:\n                            f.write(\'step: %d\\n\' % counter)\n                            for k, v in self.val_data.items():\n                                imgs, imgs_f, issame = load_bin(v, self.image_size)\n                                embds = self.run_embds(sess, imgs)\n                                embds_f = self.run_embds(sess, imgs_f)\n                                embds = embds/np.linalg.norm(embds, axis=1, keepdims=True)+embds_f/np.linalg.norm(embds_f, axis=1, keepdims=True)\n                                tpr, fpr, acc_mean, acc_std, tar, tar_std, far = evaluate(embds, issame, far_target=1e-3, distance_metric=0)\n                                f.write(\'eval on %s: acc--%1.5f+-%1.5f, tar--%1.5f+-%1.5f@far=%1.5f\\n\' % (k, acc_mean, acc_std, tar, tar_std, far))\n                                acc.append(acc_mean)\n                            acc = np.mean(np.array(acc))\n                            if acc > best_acc:\n                                saver_best.save(sess, os.path.join(self.model_dir, \'best-m\'), global_step=counter)\n                                best_acc = acc\n\n                        \nif __name__ == \'__main__\':\n    args = parse_args()\n    config = yaml.load(open(args.config_path))\n    trainer = Trainer(config)\n    trainer.train()\n\n\n                    '"
utils.py,4,"b'import os\n\nimport tensorflow as tf\n\n\ndef check_folders(paths):\n    if isinstance(paths, str):\n        paths = [paths]\n    for path in paths:\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n        tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n        List of pairs of (gradient, variable) where the gradient has been averaged\n        across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef tensor_description(var):\n    """"""Returns a compact and informative string about a tensor.\n    Args:\n        var: A tensor variable.\n    Returns:\n        a string with type and size, e.g.: (float32 1x8x8x1024).\n    """"""\n    description = \'(\' + str(var.dtype.name) + \' \'\n    sizes = var.get_shape()\n    for i, size in enumerate(sizes):\n        description += str(size)\n        if i < len(sizes) - 1:\n            description += \'x\'\n    description += \')\'\n    return description\n\n\ndef analyze_vars(variables, path):\n    """"""Prints the names and shapes of the variables.\n    Args:\n        variables: list of variables, for example tf.global_variables().\n        print_info: Optional, if true print variables and their shape.\n    Returns:\n        (total size of the variables, total bytes of the variables)\n    """"""\n    f = open(path, \'w\')\n    f.write(\'---------\\n\')\n    f.write(\'Variables: name (type shape) [size]\\n\')\n    f.write(\'---------\\n\')\n    total_size = 0\n    total_bytes = 0\n    for var in variables:\n        # if var.num_elements() is None or [] assume size 0.\n        var_size = var.get_shape().num_elements() or 0\n        var_bytes = var_size * var.dtype.size\n        total_size += var_size\n        total_bytes += var_bytes\n        f.write(var.name+\' \'+tensor_description(var)+\' \'+\'[%d, bytes: %d]\\n\' % (var_size, var_bytes))\n    f.write(\'Total size of variables: %d\\n\' % total_size)\n    f.write(\'Total bytes of variables: %d\\n\' % total_bytes)\n    return total_size, total_bytes'"
backbones/ResNet_v1.py,8,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom backbones import utils\n\n\nresnet_arg_scope = utils.resnet_arg_scope\n\n\nclass NoOpScope(object):\n    """"""No-op context manager.""""""\n\n    def __enter__(self):\n        return None\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        return False\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs,\n               depth,\n               depth_bottleneck,\n               stride,\n               rate=1,\n               outputs_collections=None,\n               scope=None,\n               use_bounded_activations=False):\n    with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n        depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n\n        if depth == depth_in:\n            shortcut = utils.subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride, activation_fn=tf.nn.relu6 if use_bounded_activations else None, scope=\'shortcut\')\n        \n        residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1, scope=\'conv1\')\n        residual = utils.conv2d_same(residual, depth_bottleneck, 3, stride, rate=rate, scope=\'conv2\')\n        residual = slim.conv2d(residual, depth, [1, 1], stride=1, activation_fn=None, scope=\'conv3\')\n\n        if use_bounded_activations:\n            # Use clip_by_value to simulate bandpass activation.\n            residual = tf.clip_by_value(residual, -6.0, 6.0)\n            output = tf.nn.relu6(shortcut + residual)\n        else:\n            output = tf.nn.relu(shortcut + residual)\n\n        return slim.utils.collect_named_outputs(outputs_collections, sc.name, output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              store_non_strided_activations=False,\n              reuse=None,\n              scope=None):\n    with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n        end_points_collection = sc.original_name_scope + \'_end_points\'\n        with slim.arg_scope([slim.conv2d, bottleneck, utils.stack_blocks_dense], outputs_collections=end_points_collection):\n            with (slim.arg_scope([slim.batch_norm], is_training=is_training) if is_training is not None else NoOpScope()):\n                net = inputs\n                if include_root_block:\n                    if output_stride is not None:\n                        if output_stride % 4 != 0:\n                            raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n                        output_stride /= 4\n                    net = utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n                    net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n                net = utils.stack_blocks_dense(net, blocks, output_stride, store_non_strided_activations)\n\n                end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n\n                if global_pool:\n                    net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n                    end_points[\'global_pool\'] = net\n                if num_classes:\n                    net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, scope=\'logits\')\n                    end_points[sc.name + \'/logits\'] = net\n                    if spatial_squeeze:\n                        net = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n                        end_points[sc.name + \'/spatial_squeeze\'] = net\n                    end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n                return net, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_block(scope, base_depth, num_units, stride):\n    return utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n    }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n    }])\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 store_non_strided_activations=False,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n    """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n        resnet_v1_block(\'block3\', base_depth=256, num_units=6, stride=2),\n        resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n    ]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                    global_pool=global_pool, output_stride=output_stride,\n                    include_root_block=True, spatial_squeeze=spatial_squeeze,\n                    store_non_strided_activations=store_non_strided_activations,\n                    reuse=reuse, scope=scope)\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  store_non_strided_activations=False,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n    """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n        resnet_v1_block(\'block3\', base_depth=256, num_units=23, stride=2),\n        resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n    ]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                    global_pool=global_pool, output_stride=output_stride,\n                    include_root_block=True, spatial_squeeze=spatial_squeeze,\n                    store_non_strided_activations=store_non_strided_activations,\n                    reuse=reuse, scope=scope)\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  store_non_strided_activations=False,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n    """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v1_block(\'block2\', base_depth=128, num_units=8, stride=2),\n        resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n        resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n    ]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                    global_pool=global_pool, output_stride=output_stride,\n                    include_root_block=True, spatial_squeeze=spatial_squeeze,\n                    store_non_strided_activations=store_non_strided_activations,\n                    reuse=reuse, scope=scope)\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  store_non_strided_activations=False,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n    """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v1_block(\'block2\', base_depth=128, num_units=24, stride=2),\n        resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n        resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n    ]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                    global_pool=global_pool, output_stride=output_stride,\n                    include_root_block=True, spatial_squeeze=spatial_squeeze,\n                    store_non_strided_activations=store_non_strided_activations,\n                    reuse=reuse, scope=scope)\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n'"
backbones/ResNet_v2.py,6,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom backbones import utils\n\nresnet_arg_scope = utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n    with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n        depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n        preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n        if depth == depth_in:\n            shortcut = utils.subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride, normalizer_fn=None, activation_fn=None, scope=\'shortcut\')\n\n        residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1, scope=\'conv1\')\n        residual = utils.conv2d_same(residual, depth_bottleneck, 3, stride, rate=rate, scope=\'conv2\')\n        residual = slim.conv2d(residual, depth, [1, 1], stride=1, normalizer_fn=None, activation_fn=None, scope=\'conv3\')\n\n        output = shortcut + residual\n\n        return slim.utils.collect_named_outputs(outputs_collections, sc.name, output)\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              return_raw=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              reuse=None,\n              scope=None):\n    with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n        end_points_collection = sc.original_name_scope + \'_end_points\'\n        with slim.arg_scope([slim.conv2d, bottleneck, utils.stack_blocks_dense], outputs_collections=end_points_collection):\n            with slim.arg_scope([slim.batch_norm], is_training=is_training):\n                net = inputs\n                if include_root_block:\n                    if output_stride is not None:\n                        if output_stride % 4 != 0:\n                            raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n                        output_stride /= 4\n                    with slim.arg_scope([slim.conv2d], activation_fn=None, normalizer_fn=None):\n                        net = utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n                    net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n                net = utils.stack_blocks_dense(net, blocks, output_stride)\n                end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n                if return_raw:\n                    return net, end_points\n\n                net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n                end_points[sc.name + \'/postnorm\'] = net\n\n                if global_pool:\n                    net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n                    end_points[\'global_pool\'] = net\n\n                if num_classes:\n                    net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, scope=\'logits\')\n                    end_points[sc.name + \'/logits\'] = net\n                    if spatial_squeeze:\n                        net = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n                        end_points[sc.name + \'/spatial_squeeze\'] = net\n                    end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n                return net, end_points\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_block(scope, base_depth, num_units, stride):\n    return utils.Block(scope, bottleneck, [{\n        \'depth\': base_depth * 4,\n        \'depth_bottleneck\': base_depth,\n        \'stride\': 1\n    }] * (num_units - 1) + [{\n        \'depth\': base_depth * 4,\n        \'depth_bottleneck\': base_depth,\n        \'stride\': stride\n    }])\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 return_raw=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n    """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n    blocks = [\n        resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n        resnet_v2_block(\'block3\', base_depth=256, num_units=6, stride=2),\n        resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n    ]\n    return resnet_v2(inputs, blocks, num_classes, is_training=is_training, return_raw=return_raw, global_pool=global_pool, output_stride=output_stride, include_root_block=True, spatial_squeeze=spatial_squeeze, reuse=reuse, scope=scope)\nresnet_v2_50.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  return_raw=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n    """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n    blocks = [\n        resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n        resnet_v2_block(\'block3\', base_depth=256, num_units=23, stride=2),\n        resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n    ]\n    return resnet_v2(inputs, blocks, num_classes, is_training=is_training, return_raw=return_raw, global_pool=global_pool, output_stride=output_stride, include_root_block=True, spatial_squeeze=spatial_squeeze, reuse=reuse, scope=scope)\nresnet_v2_101.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  return_raw=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n    """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n    blocks = [\n        resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v2_block(\'block2\', base_depth=128, num_units=8, stride=2),\n        resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n        resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n    ]\n    return resnet_v2(inputs, blocks, num_classes, is_training=is_training, return_raw=return_raw, global_pool=global_pool, output_stride=output_stride, include_root_block=True, spatial_squeeze=spatial_squeeze, reuse=reuse, scope=scope)\nresnet_v2_152.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  return_raw=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n    """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n    blocks = [\n        resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v2_block(\'block2\', base_depth=128, num_units=24, stride=2),\n        resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n        resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n    ]\n    return resnet_v2(inputs, blocks, num_classes, is_training=is_training, return_raw=return_raw, global_pool=global_pool, output_stride=output_stride, include_root_block=True, spatial_squeeze=spatial_squeeze, reuse=reuse, scope=scope)\nresnet_v2_200.default_image_size = resnet_v2.default_image_size\n'"
backbones/modifiedResNet_v2.py,8,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom backbones import utils\n\nresnet_arg_scope = utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1, outputs_collections=None, scope=None):\n    with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n        depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n        preact = slim.batch_norm(inputs, activation_fn=tf.nn.leaky_relu, scope=\'preact\')\n        if depth == depth_in:\n            shortcut = utils.subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride, normalizer_fn=None, activation_fn=None, scope=\'shortcut\')\n\n        residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1, scope=\'conv1\')\n        residual = utils.conv2d_same(residual, depth_bottleneck, 3, stride, rate=rate, scope=\'conv2\')\n        residual = slim.conv2d(residual, depth, [1, 1], stride=1, normalizer_fn=None, activation_fn=None, scope=\'conv3\')\n\n        output = shortcut + residual\n\n        return slim.utils.collect_named_outputs(outputs_collections, sc.name, output)\n\n\n@slim.add_arg_scope\ndef block(inputs, depth, stride, rate=1, outputs_collections=None, scope=None):\n    with tf.variable_scope(scope, \'block_v2\', [inputs]) as sc:\n        depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n        preact = slim.batch_norm(inputs, activation_fn=tf.nn.leaky_relu, scope=\'preact\')\n        if depth == depth_in:\n            shortcut = utils.subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride, normalizer_fn=None, activation_fn=None, scope=\'shortcut\')\n\n        residual = utils.conv2d_same(preact, depth, 3, stride, rate=rate, scope=\'conv1\')\n        residual = slim.conv2d(residual, depth, [3, 3], stride=1, normalizer_fn=None, activation_fn=None, scope=\'conv2\')\n        # residual = slim.conv2d(residual, depth, [1, 1], stride=1, normalizer_fn=None, activation_fn=None, scope=\'conv3\')\n\n        output = shortcut + residual\n\n        return slim.utils.collect_named_outputs(outputs_collections, sc.name, output)\n\n\ndef resnet_v2_m(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              return_raw=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              reuse=None,\n              scope=None):\n    with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n        end_points_collection = sc.original_name_scope + \'_end_points\'\n        with slim.arg_scope([slim.conv2d, bottleneck, utils.stack_blocks_dense], outputs_collections=end_points_collection):\n            with slim.arg_scope([slim.batch_norm], is_training=is_training):\n                net = inputs\n                if include_root_block:\n                    if output_stride is not None:\n                        if output_stride % 4 != 0:\n                            raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n                        output_stride /= 4\n                    with slim.arg_scope([slim.conv2d], activation_fn=None, normalizer_fn=None):\n                        net = utils.conv2d_same(net, 64, 3, stride=1, scope=\'conv1\')\n                    # net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n                net = utils.stack_blocks_dense(net, blocks, output_stride)\n                end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n                if return_raw:\n                    return net, end_points\n                net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n                end_points[sc.name + \'/postnorm\'] = net\n\n                if global_pool:\n                    net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n                    end_points[\'global_pool\'] = net\n\n                if num_classes:\n                    net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, scope=\'logits\')\n                    end_points[sc.name + \'/logits\'] = net\n                    if spatial_squeeze:\n                        net = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n                        end_points[sc.name + \'/spatial_squeeze\'] = net\n                    end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n                return net, end_points\nresnet_v2_m.default_image_size = 224\n\n\ndef resnet_v2_bottleneck(scope, base_depth, num_units, stride):\n    return utils.Block(scope, bottleneck, [{\n        \'depth\': base_depth * 4,\n        \'depth_bottleneck\': base_depth,\n        \'stride\': stride\n    }] + (num_units - 1) * [{\n        \'depth\': base_depth * 4,\n        \'depth_bottleneck\': base_depth,\n        \'stride\': 1\n    }])\nresnet_v2_m.default_image_size = 224\n\n\ndef resnet_v2_block(scope, base_depth, num_units, stride):\n    return utils.Block(scope, block, [{\n        \'depth\': base_depth * 4,\n        \'stride\': stride\n    }] + (num_units - 1) * [{\n        \'depth\': base_depth * 4,\n        \'stride\': 1\n    }])\nresnet_v2_m.default_image_size = 224\n\n\ndef resnet_v2_m_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 return_raw=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n    """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n    blocks = [\n        resnet_v2_block(\'block1\', base_depth=16, num_units=3, stride=2),\n        resnet_v2_block(\'block2\', base_depth=32, num_units=4, stride=2),\n        resnet_v2_block(\'block3\', base_depth=64, num_units=14, stride=2),\n        resnet_v2_block(\'block4\', base_depth=128, num_units=3, stride=2),\n    ]\n    return resnet_v2_m(inputs, blocks, num_classes, is_training=is_training, return_raw=return_raw, global_pool=global_pool, output_stride=output_stride, include_root_block=True, spatial_squeeze=spatial_squeeze, reuse=reuse, scope=scope)\nresnet_v2_m_50.default_image_size = resnet_v2_m.default_image_size\n\n\ndef resnet_v2_m_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  return_raw=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n    """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n    blocks = [\n        resnet_v2_bottleneck(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v2_bottleneck(\'block2\', base_depth=128, num_units=4, stride=2),\n        resnet_v2_bottleneck(\'block3\', base_depth=256, num_units=23, stride=2),\n        resnet_v2_bottleneck(\'block4\', base_depth=512, num_units=3, stride=2),\n    ]\n    return resnet_v2_m(inputs, blocks, num_classes, is_training=is_training, return_raw=return_raw, global_pool=global_pool, output_stride=output_stride, include_root_block=True, spatial_squeeze=spatial_squeeze, reuse=reuse, scope=scope)\nresnet_v2_m_101.default_image_size = resnet_v2_m.default_image_size\n\n\ndef resnet_v2_m_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  return_raw=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n    """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n    blocks = [\n        resnet_v2_bottleneck(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v2_bottleneck(\'block2\', base_depth=128, num_units=8, stride=2),\n        resnet_v2_bottleneck(\'block3\', base_depth=256, num_units=36, stride=2),\n        resnet_v2_bottleneck(\'block4\', base_depth=512, num_units=3, stride=2),\n    ]\n    return resnet_v2_m(inputs, blocks, num_classes, is_training=is_training, return_raw=return_raw, global_pool=global_pool, output_stride=output_stride, include_root_block=True, spatial_squeeze=spatial_squeeze, reuse=reuse, scope=scope)\nresnet_v2_m_152.default_image_size = resnet_v2_m.default_image_size\n\n\ndef resnet_v2_m_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  return_raw=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n    """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n    blocks = [\n        resnet_v2_bottleneck(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v2_bottleneck(\'block2\', base_depth=128, num_units=24, stride=2),\n        resnet_v2_bottleneck(\'block3\', base_depth=256, num_units=36, stride=2),\n        resnet_v2_bottleneck(\'block4\', base_depth=512, num_units=3, stride=2),\n    ]\n    return resnet_v2_m(inputs, blocks, num_classes, is_training=is_training, return_raw=return_raw, global_pool=global_pool, output_stride=output_stride, include_root_block=True, spatial_squeeze=spatial_squeeze, reuse=reuse, scope=scope)\nresnet_v2_m_200.default_image_size = resnet_v2_m.default_image_size\n'"
backbones/utils.py,6,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import namedtuple\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n\nclass Block(namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n    """"""A named tuple describing a ResNet block.\n\n    Its parts are:\n        scope: The scope of the `Block`.\n        unit_fn: The ResNet unit function which takes as input a `Tensor` and returns another `Tensor` with the output of the ResNet unit.\n        args: A list of length equal to the number of units in the `Block`. The list contains one (depth, depth_bottleneck, stride) tuple for each unit in the block to serve as argument to unit_fn.\n    """"""\n\n\ndef subsample(inputs, factor, scope=None):\n    if factor == 1:\n        return inputs\n    else:\n        return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)  # padding=\'VALID\'\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n    if stride == 1:\n        return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate, padding=\'SAME\', scope=scope)\n    else:\n        kernel_size_effective = kernel_size+(kernel_size-1)*(rate-1)\n        pad_total = kernel_size_effective-1\n        pad_beg = pad_total//2\n        pad_end = pad_total-pad_beg\n        inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])  # zero padding\n        return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride, rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None, store_non_strided_activations=False, outputs_collections=None):\n    current_stride = 1\n    rate = 1\n\n    for block in blocks:\n        with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n            block_stride = 1\n            for i, unit in enumerate(block.args):\n                if store_non_strided_activations and i == len(block.args)-1:\n                    block_stride = unit.get(\'stride\', 1)\n                    unit = dict(unit, stride=1)\n                with tf.variable_scope(\'unit_%d\' % (i+1), values=[net]):\n                    if output_stride is not None and current_stride == output_stride:\n                        net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))\n                        rate *= unit.get(\'stride\', 1)\n                    else:\n                        net = block.unit_fn(net, rate=1, **unit)\n                        current_stride *= unit.get(\'stride\', 1)\n                        if output_stride is not None and current_stride > output_stride:\n                            raise ValueError(\'The target output_stride cannot be reached.\')\n            net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n            if output_stride is not None and current_stride == output_stride:\n                rate *= block_stride\n            else:\n                net = subsample(net, block_stride)\n                current_stride *= block_stride\n                if output_stride is not None and current_stride > output_stride:\n                    raise ValueError(\'The target output_stride cannot be reached.\')\n    if output_stride is not None and current_stride != output_stride:\n        raise ValueError(\'The target output_stride cannot be reached.\')\n    return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.9,\n                     batch_norm_epsilon=2e-5,\n                     batch_norm_scale=True,\n                     activation_fn=tf.nn.leaky_relu,\n                     use_batch_norm=True,\n                     batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS):\n    batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': batch_norm_updates_collections,\n      \'fused\': None,  # Use fused batch norm if possible.\n      \'param_regularizers\': {\'gamma\': slim.l2_regularizer(weight_decay)},\n    }\n    \n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_regularizer=slim.l2_regularizer(weight_decay),\n        weights_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n        activation_fn=activation_fn,\n        normalizer_fn=slim.batch_norm if use_batch_norm else None,\n        normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n            with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n                return arg_sc'"
data/classificationDataTool.py,20,"b'import os\nimport tensorflow as tf\nfrom scipy import misc\nimport numpy as np\nimport random\nimport sys\nimport io\n\n\ndef to_rgb(img):\n    if img.ndim < 3:\n        h, w = img.shape\n        ret = np.empty((h, w, 3), dtype=np.uint8)\n        ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\n        return ret\n    else:\n        return img\n\n\ndef augmentation(image, aug_img_size):\n    ori_image_shape = tf.shape(image)\n    image = tf.image.random_flip_left_right(image)\n    # image = tf.image.resize_images(image, [aug_img_size, aug_img_size])\n    # image = tf.random_crop(image, ori_image_shape)\n    return image\n\n\nclass ClassificationImageData:\n\n    def __init__(self, img_size=112, augment_flag=True, augment_margin=16):\n        self.img_size = img_size\n        self.augment_flag = augment_flag\n        self.augment_margin = augment_margin\n\n\n    def get_path_label(self, root):\n        ids = list(os.listdir(root))\n        ids.sort()\n        self.cat_num = len(ids)\n        id_dict = dict(zip(ids, list(range(self.cat_num))))\n        paths = []\n        labels = []\n        for i in ids:\n            cur_dir = os.path.join(root, i)\n            fns = os.listdir(cur_dir)\n            paths += [os.path.join(cur_dir, fn) for fn in fns]\n            labels += [id_dict[i]]*len(fns)\n        return paths, labels\n\n\n    def image_processing(self, img):\n        img.set_shape([None, None, 3])\n        img = tf.image.resize_images(img, [self.img_size, self.img_size])\n\n        if self.augment_flag :\n            augment_size = self.img_size + self.augment_margin\n            img = augmentation(img, augment_size)\n        \n        img = tf.cast(img, tf.float32) / 127.5 - 1\n\n        return img\n\n\n    def add_record(self, img, label, writer):\n        img = to_rgb(img)\n        img = misc.imresize(img, [self.img_size, self.img_size]).astype(np.uint8)\n        shape = img.shape\n        tf_features = tf.train.Features(feature={\n            ""img"": tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.tostring()])),\n            ""shape"": tf.train.Feature(int64_list=tf.train.Int64List(value=list(shape))),\n            ""label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n        })\n        tf_example = tf.train.Example(features = tf_features)\n        tf_serialized = tf_example.SerializeToString()\n        writer.write(tf_serialized)\n    \n    \n    def write_tfrecord_from_folders(self, read_dir, write_path):\n        print(\'write tfrecord from folders...\')\n        writer = tf.python_io.TFRecordWriter(write_path, options=None)\n        paths, labels = self.get_path_label(read_dir)\n        assert(len(paths) == len(labels))\n        total = len(paths)\n        cnt = 0\n        for p, l in zip(paths, labels):\n            img = misc.imread(p).astype(np.uint8)\n            self.add_record(img, l, writer)\n            cnt += 1\n            print(\'%d/%d\' % (cnt, total), end=\'\\r\')\n        writer.close()\n        print(\'done![%d/%d]\' % (cnt, total))\n        print(\'class num: %d\' % self.cat_num)\n\n\n    def write_tfrecord_from_mxrec(self, read_dir, write_path):\n        import mxnet as mx\n        print(\'write tfrecord from mxrec...\')\n        idx_path = os.path.join(read_dir, \'train.idx\')\n        bin_path = os.path.join(read_dir, \'train.rec\')\n        imgrec = mx.recordio.MXIndexedRecordIO(idx_path, bin_path, \'r\')\n        s = imgrec.read_idx(0)\n        header, _ = mx.recordio.unpack(s)\n        imgidx = list(range(1, int(header.label[0])))\n        writer = tf.python_io.TFRecordWriter(write_path, options=None)\n        total = len(imgidx)\n        cnt = 0\n        labels = []\n        for i in imgidx:\n            img_info = imgrec.read_idx(i)\n            header, img = mx.recordio.unpack(img_info)\n            l = int(header.label)\n            labels.append(l)\n            img = io.BytesIO(img)\n            img = misc.imread(img).astype(np.uint8)\n            self.add_record(img, l, writer)\n            cnt += 1\n            print(\'%d/%d\' % (cnt, total), end=\'\\r\')\n        writer.close()\n        self.cat_num = len(set(labels))\n        print(\'done![%d/%d]\' % (cnt, total))\n        print(\'class num: %d\' % self.cat_num)\n\n\n    def parse_function(self, example_proto):\n        dics = {\n            \'img\': tf.FixedLenFeature(shape=(), dtype=tf.string),\n            \'shape\': tf.FixedLenFeature(shape=(3,), dtype=tf.int64),\n            \'label\': tf.FixedLenFeature(shape=(), dtype=tf.int64)\n        }\n        parsed_example = tf.parse_single_example(example_proto, dics)\n        parsed_example[\'img\'] = tf.decode_raw(parsed_example[\'img\'], tf.uint8)\n        parsed_example[\'img\'] = tf.reshape(parsed_example[\'img\'], parsed_example[\'shape\'])\n        return self.image_processing(parsed_example[\'img\']), parsed_example[\'label\']\n\n\n    def read_TFRecord(self, filenames):\n        dataset = tf.data.TFRecordDataset(filenames, buffer_size=256<<20)\n        return dataset.map(self.parse_function, num_parallel_calls=8)\n\n'"
data/generateTFRecord.py,0,"b'import argparse\n\nfrom classificationDataTool import ClassificationImageData\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--mode\', type=str, help=\'from which to generate TFRecord, folders or mxrec\', default=\'mxrec\')\n    parser.add_argument(\'--image_size\', type=int, help=\'image size\', default=112)\n    parser.add_argument(\'--read_dir\', type=str, help=\'directory to read data\', default=\'\')\n    parser.add_argument(\'--save_path\', type=str, help=\'path to save TFRecord file\', default=\'\')\n\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    args = get_args()\n    cid = ClassificationImageData(img_size=args.image_size)\n    if args.mode == \'folders\':\n        cid.write_tfrecord_from_folders(args.read_dir, args.save_path)\n    elif args.mode == \'mxrec\':\n        cid.write_tfrecord_from_mxrec(args.read_dir, args.save_path)\n    else:\n        raise(\'ERROR: wrong mode (only folders and mxrec are supported)\')\n'"
data/mxrec2folders.py,0,"b""import mxnet as mx\nimport io\nimport os\nfrom scipy import misc\n\nimport numpy as np\n\nread_dir = r'F:\\FaceDataset\\faces_webface_112x112'\nsave_dir = r'F:\\FaceDataset\\faces_webface_112x112_folders'\n\nidx_path = os.path.join(read_dir, 'train.idx')\nbin_path = os.path.join(read_dir, 'train.rec')\nimgrec = mx.recordio.MXIndexedRecordIO(idx_path, bin_path, 'r')\ns = imgrec.read_idx(0)\nheader, _ = mx.recordio.unpack(s)\nimgidx = list(range(1, int(header.label[0])))\ntotal = len(imgidx)\ncnt = 0\nfor i in imgidx:\n    img_info = imgrec.read_idx(i)\n    header, img = mx.recordio.unpack(img_info)\n    l = int(header.label)\n    img = io.BytesIO(img)\n    img = misc.imread(img)\n    cur_save_dir = os.path.join(save_dir, str(l))\n    if not os.path.exists(cur_save_dir):\n        os.makedirs(cur_save_dir)\n    misc.imsave(os.path.join(cur_save_dir, str(cnt)+'.jpg'), img)\n    cnt += 1\n    print('%d/%d' % (cnt, total), end='\\r')\n    # if cnt >= 10:\n    #     break"""
data/tmp.py,0,"b""import mxnet as mx\nimport io\nimport os\nfrom scipy import misc\n\nimport numpy as np\n\nread_dir = r'F:\\FaceDataset\\faces_webface_112x112'\nsave_dir = r'F:\\FaceDataset\\faces_webface_112x112\\img_sample'\n\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\nidx_path = os.path.join(read_dir, 'train.idx')\nbin_path = os.path.join(read_dir, 'train.rec')\nimgrec = mx.recordio.MXIndexedRecordIO(idx_path, bin_path, 'r')\ns = imgrec.read_idx(0)\nheader, _ = mx.recordio.unpack(s)\nimgidx = list(range(1, int(header.label[0])))\ntotal = len(imgidx)\ncnt = 0\nfor i in imgidx:\n    img_info = imgrec.read_idx(i)\n    header, img = mx.recordio.unpack(img_info)\n    l = int(header.label)\n    img = io.BytesIO(img)\n    img = misc.imread(img)\n    print('============================================')\n    print(img.dtype)\n    print(np.max(img))\n    print(np.min(img))\n    print('============================================')\n    misc.imsave(os.path.join(save_dir, str(cnt)+'.jpg'), img)\n    cnt += 1\n    if cnt >= 100:\n        break"""
data/tmp1.py,0,"b""import io\nimport os\nfrom scipy import misc\n\nimport numpy as np\nimport pickle\n\n\nread_path = r'F:\\FaceDataset\\faces_vgg_112x112\\lfw.bin'\nsave_dir = r'F:\\FaceDataset\\faces_vgg_112x112\\lfw_img_sample'\n\n\nbins, issame_list = pickle.load(open(read_path, 'rb'), encoding='bytes')\ncnt = 0\nfor bin in bins:\n    img = misc.imread(io.BytesIO(bin))\n    print('============================================')\n    print(img.dtype)\n    print(np.max(img))\n    print(np.min(img))\n    print('============================================')\n    misc.imsave(os.path.join(save_dir, str(cnt)+'.jpg'), img)\n    cnt += 1\n    if cnt >= 10:\n        break"""
eval/utils.py,0,"b""import os\nimport math\nimport numpy as np\n\nfrom sklearn.model_selection import KFold\nfrom scipy import interpolate\n\n\ndef distance(embeddings1, embeddings2, distance_metric=0):\n    if distance_metric==0:\n        # Euclidian distance\n        embeddings1 = embeddings1/np.linalg.norm(embeddings1, axis=1, keepdims=True)\n        embeddings2 = embeddings2/np.linalg.norm(embeddings2, axis=1, keepdims=True)\n        diff = np.subtract(embeddings1, embeddings2)\n        dist = np.sum(np.square(diff),1)\n    elif distance_metric==1:\n        # Distance based on cosine similarity\n        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n        similarity = dot/norm\n        dist = np.arccos(similarity) / math.pi\n    else:\n        raise 'Undefined distance metric %d' % distance_metric \n        \n    return dist\n\n\ndef calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, distance_metric=0, nrof_folds=10):\n    assert(embeddings1.shape[0] == embeddings2.shape[0])\n    assert(embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n    \n    tprs = np.zeros((nrof_folds,nrof_thresholds))\n    fprs = np.zeros((nrof_folds,nrof_thresholds))\n    accuracy = np.zeros((nrof_folds))\n    \n    dist = distance(embeddings1, embeddings2, distance_metric)\n    indices = np.arange(nrof_pairs)\n    \n    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n        \n        # Find the best threshold for the fold\n        acc_train = np.zeros((nrof_thresholds))\n        for threshold_idx, threshold in enumerate(thresholds):\n            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n        best_threshold_index = np.argmax(acc_train)\n        for threshold_idx, threshold in enumerate(thresholds):\n            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n          \n    tpr = np.mean(tprs,0)\n    fpr = np.mean(fprs,0)\n    return tpr, fpr, accuracy\n\ndef calculate_accuracy(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n  \n    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n    acc = float(tp+tn)/dist.size\n    return tpr, fpr, acc\n\n\ndef calculate_tar_far(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    n_same = np.sum(actual_issame)\n    n_diff = np.sum(np.logical_not(actual_issame))\n    tar = float(true_accept) / float(n_same)\n    far = float(false_accept) / float(n_diff)\n    return tar, far\n\n\ndef calculate_tar(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False):\n    assert(embeddings1.shape[0] == embeddings2.shape[0])\n    assert(embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n    \n    tar = np.zeros(nrof_folds)\n    far = np.zeros(nrof_folds)\n    \n    indices = np.arange(nrof_pairs)\n    \n    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n        if subtract_mean:\n            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n        else:\n            mean = 0.0\n        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n      \n        # Find the threshold that gives FAR = far_target\n        far_train = np.zeros(nrof_thresholds)\n        for threshold_idx, threshold in enumerate(thresholds):\n            _, far_train[threshold_idx] = calculate_tar_far(threshold, dist[train_set], actual_issame[train_set])\n        if np.max(far_train)>=far_target:\n            f = interpolate.interp1d(far_train, thresholds, kind='slinear')\n            threshold = f(far_target)\n        else:\n            threshold = 0.0\n\n        tar[fold_idx], far[fold_idx] = calculate_tar_far(threshold, dist[test_set], actual_issame[test_set])\n\n    tar_mean = np.mean(tar)\n    far_mean = np.mean(far)\n    tar_std = np.std(tar)\n    return tar_mean, tar_std, far_mean"""
losses/logit_loss.py,16,"b""import tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nimport math\n\n\nW_INIT = tf.contrib.layers.xavier_initializer(uniform=False)\n\n\ndef get_logits(embds, labels, config, w_init=W_INIT, reuse=False, scope='logits'):\n    with tf.variable_scope(scope, reuse=reuse):\n        weights = tf.get_variable(name='classify_weight', shape=[embds.get_shape().as_list()[-1], config['class_num']], dtype=tf.float32, initializer=w_init, regularizer=slim.l2_regularizer(config['weight_decay']), trainable=True)\n        if config['loss_type'] == 'arcface':\n            return calculate_arcface_logits(embds, weights, labels, config['class_num'], config['logits_scale'], config['logits_margin'])\n        elif config['loss_type'] == 'softmax':\n            return slim.fully_connected(embds, num_outputs=config['class_num'], activation_fn=None, normalizer_fn=None, weights_initializer=w_init, weights_regularizer=slim.l2_regularizer(config['weight_decay']))\n        else:\n            raise ValueError('Invalid loss type.')\n\n\ndef calculate_arcface_logits(embds, weights, labels, class_num, s, m):\n    embds = tf.nn.l2_normalize(embds, axis=1, name='normed_embd')\n    weights = tf.nn.l2_normalize(weights, axis=0)\n\n    cos_m = math.cos(m)\n    sin_m = math.sin(m)\n\n    mm = sin_m * m\n\n    threshold = math.cos(math.pi - m)\n\n    cos_t = tf.matmul(embds, weights, name='cos_t')\n\n    cos_t2 = tf.square(cos_t, name='cos_2')\n    sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n    sin_t = tf.sqrt(sin_t2, name='sin_t')\n    cos_mt = s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n    cond_v = cos_t - threshold\n    cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n    keep_val = s*(cos_t - mm)\n    cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n    mask = tf.one_hot(labels, depth=class_num, name='one_hot_mask')\n    inv_mask = tf.subtract(1., mask, name='inverse_mask')\n    s_cos_t = tf.multiply(s, cos_t, name='scalar_cos_t')\n    output = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_logits')\n    return output\n\n"""
