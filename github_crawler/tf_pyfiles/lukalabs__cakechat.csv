file_path,api_count,code
bin/cakechat_server.py,0,"b""import os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom cakechat.utils.env import set_keras_tf_session\n\ngpu_memory_fraction = os.environ.get('GPU_MEMORY_FRACTION', 0.1)\nset_keras_tf_session(gpu_memory_fraction)\n\nfrom cakechat.api.v1.server import app\n\nif __name__ == '__main__':\n    # runs development server\n    app.run(host='0.0.0.0', port=8080)\n"""
cakechat/__init__.py,0,b''
cakechat/config.py,0,"b'import os\n\nfrom cakechat.utils.data_structures import create_namedtuple_instance\nfrom cakechat.utils.env import is_dev_env\n\nMODEL_NAME = \'cakechat_v2.0_keras_tf\'\n\nINTX = \'uint16\'  # use unsigined 16-bits int representation for memory efficiency\nRANDOM_SEED = 42  # Fix the random seed to a certain value to make everything reproducible\n\n# AWS S3 params\nS3_MODELS_BUCKET_NAME = \'cake-chat-data-v2\'  # S3 bucket with all the data\nS3_NN_MODEL_REMOTE_DIR = \'nn_models\'  # S3 remote directory with models itself\nS3_TOKENS_IDX_REMOTE_DIR = \'tokens_index\'  # S3 remote directory with tokens index\nS3_CONDITIONS_IDX_REMOTE_DIR = \'conditions_index\'  # S3 remote directory with conditions index\nS3_W2V_REMOTE_DIR = \'w2v_models\'  # S3 remote directory with pre-trained w2v models\n\n# train datasets\nDATA_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), \'data\')\nPROCESSED_CORPUS_DIR = os.path.join(DATA_PATH, \'corpora_processed\')\nTOKEN_INDEX_DIR = os.path.join(DATA_PATH, \'tokens_index\')  # Path to prepared tokens index directory\nCONDITION_IDS_INDEX_DIR = os.path.join(DATA_PATH, \'conditions_index\')  # Path to prepared conditions index directory\n\n# train & val data params\nBASE_CORPUS_NAME = \'processed_dialogs\'  # Basic corpus name prefix\nTRAIN_CORPUS_NAME = \'train_\' + BASE_CORPUS_NAME  # Training dataset filename prefix\nCONTEXT_SENSITIVE_VAL_CORPUS_NAME = \'val_\' + BASE_CORPUS_NAME  # Validation dataset filename prefix for intermediate\nCONTEXT_SENSITIVE_TEST_CORPUS_NAME = \'test_\' + BASE_CORPUS_NAME  # Testing dataset for final metrics calculation\nMAX_VAL_LINES_NUM = 10000  # Max lines number from validation set to be used for metrics calculation\n\n# test datasets\nTEST_DATA_DIR = os.path.join(DATA_PATH, \'quality\')\nCONTEXT_FREE_VAL_CORPUS_NAME = \'context_free_validation_set\'  # Context-free validation set path\nTEST_CORPUS_NAME = \'context_free_test_set\'  # Context-free test set path\nQUESTIONS_CORPUS_NAME = \'context_free_questions\'  # Context-free questions only path\n\n# directory to store model wights and calcualted metrics\nRESULTS_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), \'results\')  # Directory to store training results\nTENSORBOARD_LOG_DIR = os.path.join(RESULTS_PATH, \'tensorboard\')  # Path to tensorboard logs directory\n\n# word embeddings params\nUSE_PRETRAINED_W2V_EMBEDDINGS_LAYER = True  # Whether to use word2vec to pre-train weights for the embedding layer\nTRAIN_WORD_EMBEDDINGS_LAYER = True  # Allow fine-tuning of the word embedding layer during the model training\nW2V_MODEL_DIR = os.path.join(DATA_PATH, \'w2v_models\')  # Path to store & load trained word2vec models\nWORD_EMBEDDING_DIMENSION = 128  # word2vec embedding dimension\nW2V_WINDOW_SIZE = 10  # word2vec window size, used during the w2v pre-training\nUSE_SKIP_GRAM = True  # Use skip-gram word2vec mode. When False, CBOW is used\nTOKEN_REPRESENTATION_SIZE = 256\nMIN_WORD_FREQ = 1  # Minimum frequency of a word to be used in word2vec pre-calculation\nVOCABULARY_MAX_SIZE = 50000  # Maximum vocabulary size in tokens\nMAX_CONDITIONS_NUM = 5  # Maximum conditions num\n\n# condition inputs. We use five major emotions to condition our model\'s predictions\nEMOTIONS_TYPES = create_namedtuple_instance(\n    \'EMOTIONS_TYPES\', neutral=\'neutral\', anger=\'anger\', joy=\'joy\', fear=\'fear\', sadness=\'sadness\')\nDEFAULT_CONDITION = EMOTIONS_TYPES.neutral  # Default condition to be used during the prediction (if not specified)\nCONDITION_EMBEDDING_DIMENSION = 128  # Conditions embedding layer dimension to be trained.\n\n# NN architecture params\nHIDDEN_LAYER_DIMENSION = 768  # Dimension for the recurrent layer\nDENSE_DROPOUT_RATIO = 0.2  # Use dropout with the given ratio before decoder\'s output\nUSE_CUDNN = bool(os.environ.get(\'CUDA_VISIBLE_DEVICES\'))  # True by default for GPU-enable machines (provides ~25% inference\n# speed up) and False on CPU-only machines since they does not support CuDNN\n\n# training params\nEPOCHS_NUM = 2  # Total epochs num\nBATCH_SIZE = 196  # Number of samples to be used for gradient estimation on each train step. In case of using multiple\n# GPUs for train, each worker will have this number of samples on each step.\nSHUFFLE_TRAINING_BATCHES = True  # Shuffle training batches in the dataset each epoch\n\nINPUT_SEQUENCE_LENGTH = 30  # Input sequence length for the model during the training;\nINPUT_CONTEXT_SIZE = 3  # Maximum depth of the conversational history to be used in encoder (at least 1)\nOUTPUT_SEQUENCE_LENGTH = 32  # Output sequence length. Better to keep as INPUT_SEQUENCE_LENGTH+2 for start/end tokens\n\nGRAD_CLIP = 5.0  # Gradient clipping param passed to optimizer\nLEARNING_RATE = 6.0  # Learning rate for Adadelta optimzer\nLOG_RUN_METADATA = False  # Set \'True\' to profile memory consumption and computation time on tensorboard\nAUTOENCODER_MODE = False  # Set \'True\' to switch seq2seq (x -> y) into autoencoder (x -> x). Used for debugging\n\n# predictions params\nMAX_PREDICTIONS_LENGTH = 40  # Max. number of tokens which can be generated on the prediction step\nPREDICTION_MODES = create_namedtuple_instance(\n    \'PREDICTION_MODES\',\n    beamsearch=\'beamsearch\',\n    beamsearch_reranking=\'beamsearch_reranking\',\n    sampling=\'sampling\',\n    sampling_reranking=\'sampling_reranking\')\nPREDICTION_MODE_FOR_TESTS = PREDICTION_MODES.sampling  # Default prediction mode used in metrics computation\nPREDICTION_DISTINCTNESS_NUM_TOKENS = 50000  # Number of tokens which should be generated to compute distinctness metric\n\n# Prediction probabilities modifiers\nREPETITION_PENALIZE_COEFFICIENT = 10.0  # Divide the probabilities of the tokens already have been used during decoding\nNON_PENALIZABLE_TOKENS = [\'a\', \'an\', \'the\', \'*\', \'.\', \',\', \'?\', \'!\', \'\\\'\', \'""\', \'^\', \'`\']  # Exclude these tokens from\n# repetition penalization modifier\n\n# Options for sampling and sampling-reranking options\nDEFAULT_TEMPERATURE = 0.5  # Default softmax temperature used for sampling\n\n# Options for beamsearch and sampling-reranking:\nBEAM_SIZE = 10  # Size of the beam (beamsearch only)\nSAMPLES_NUM_FOR_RERANKING = 10  # Number of samples used in reranking (sampling-reranking only)\nMMI_REVERSE_MODEL_SCORE_WEIGHT = 1.0  # Weight for MMI reranking reverse-model score, see the paper:\n# 0.0 - scoring is performing using completely the default model, 1.0 - using completely the reverse model\n\n# Logging params\nLOG_CANDIDATES_NUM = 3  # Number of candidates to be printed to output during the logging\nSCREEN_LOG_NUM_TEST_LINES = 10  # Number of first test lines to use when logging outputs on screen\nEVAL_STATE_PER_BATCHES = 500  # How many batches to train until next metrics computed for TensorBoard\n\n# Use reduced params values for development\nif is_dev_env():\n    # train & val data params\n    MAX_VAL_LINES_NUM = 10\n\n    # word embeddings params\n    USE_PRETRAINED_W2V_EMBEDDINGS_LAYER = True\n    TRAIN_WORD_EMBEDDINGS_LAYER = True\n    WORD_EMBEDDING_DIMENSION = 64\n    VOCABULARY_MAX_SIZE = 1000\n    MAX_CONDITIONS_NUM = 5\n\n    # condition inputs\n    CONDITION_EMBEDDING_DIMENSION = 1\n\n    # NN architecture params\n    HIDDEN_LAYER_DIMENSION = 128\n    DENSE_DROPOUT_RATIO = 0.2\n    USE_CUDNN = False\n\n    # training params\n    INPUT_SEQUENCE_LENGTH = 3\n    INPUT_CONTEXT_SIZE = 1\n    OUTPUT_SEQUENCE_LENGTH = 5\n    BATCH_SIZE = 4\n    SHUFFLE_TRAINING_BATCHES = False\n    EPOCHS_NUM = 4\n    LEARNING_RATE = 1.0\n    LOG_RUN_METADATA = False\n    AUTOENCODER_MODE = False\n\n    # predictions params\n    MAX_PREDICTIONS_LENGTH = 4\n\n    # options for beamsearch and sampling-reranking:\n    SAMPLES_NUM_FOR_RERANKING = 5\n    BEAM_SIZE = 5\n\n    # logging params\n    LOG_CANDIDATES_NUM = 3\n    SCREEN_LOG_NUM_TEST_LINES = 4\n    EVAL_STATE_PER_BATCHES = 5\n'"
tools/distributed_train.py,0,"b""import os\nimport sys\nimport argparse\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom cakechat.utils.env import run_horovod_train\n\n\ndef parse_args():\n    argparser = argparse.ArgumentParser()\n    argparser.add_argument('-g', '--gpu-ids', action='store', nargs='+', required=True)\n    argparser.add_argument('-s', '--train-subset-size', action='store', type=int)\n    args = argparser.parse_args()\n\n    return args\n\n\nif __name__ == '__main__':\n    args = parse_args()\n\n    train_cmd = 'python tools/train.py'\n    if args.train_subset_size:\n        train_cmd += ' -s {}'.format(args.train_subset_size)\n\n    run_horovod_train(train_cmd, args.gpu_ids)\n"""
tools/fetch.py,0,"b'#!/usr/bin/env python\n""""""\nGets trained model and warms it up (i.e. compiles and dumps corresponding prediction functions)\n""""""\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom cakechat.utils.env import init_keras\n\ninit_keras()\n\nfrom cakechat.dialog_model.factory import get_trained_model\nfrom cakechat.utils.logger import get_tools_logger\nfrom cakechat.utils.w2v.model import get_w2v_model\n\n_logger = get_tools_logger(__file__)\n\n\ndef parse_args():\n    argparser = argparse.ArgumentParser()\n    argparser.add_argument(\n        \'-m\',\n        \'--model\',\n        action=\'store\',\n        choices=[\'default\', \'reverse\', \'w2v\', \'all\'],\n        help=\'Fetch models from s3 to disk\',\n        default=\'all\')\n    args = argparser.parse_args()\n\n    return args\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    if args.model in {\'default\', \'all\'}:\n        get_trained_model(fetch_from_s3=True)\n\n    if args.model in {\'reverse\', \'all\'}:\n        get_trained_model(fetch_from_s3=True, is_reverse_model=True)\n\n    if args.model in {\'w2v\', \'all\'}:\n        get_w2v_model(fetch_from_s3=True)\n'"
tools/generate_predictions.py,0,"b'import argparse\nimport os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom cakechat.utils.env import init_cuda_env\n\ninit_cuda_env()\n\nfrom cakechat.dialog_model.factory import get_reverse_model\nfrom cakechat.dialog_model.model import CakeChatModel\nfrom cakechat.dialog_model.model_utils import transform_contexts_to_token_ids, lines_to_context\nfrom cakechat.dialog_model.quality import log_predictions, calculate_and_log_val_metrics\nfrom cakechat.utils.files_utils import is_non_empty_file\nfrom cakechat.utils.logger import get_tools_logger\nfrom cakechat.utils.data_types import ModelParam\nfrom cakechat.utils.dataset_loader import get_tokenized_test_lines, load_context_free_val, \\\n    load_context_sensitive_val, get_validation_data_id, get_validation_sets_names\nfrom cakechat.utils.text_processing import get_index_to_token_path, load_index_to_item, get_index_to_condition_path\nfrom cakechat.utils.w2v.model import get_w2v_model_id\nfrom cakechat.config import BASE_CORPUS_NAME, QUESTIONS_CORPUS_NAME, INPUT_SEQUENCE_LENGTH, INPUT_CONTEXT_SIZE, \\\n    PREDICTION_MODES, PREDICTION_MODE_FOR_TESTS, RESULTS_PATH, DEFAULT_TEMPERATURE, TRAIN_CORPUS_NAME, \\\n    USE_PRETRAINED_W2V_EMBEDDINGS_LAYER\n\n_logger = get_tools_logger(__file__)\n\n\ndef _save_test_results(test_dataset, predictions_filename, nn_model, prediction_mode, **kwargs):\n    context_sensitive_val = load_context_sensitive_val(nn_model.token_to_index, nn_model.condition_to_index)\n    context_free_val = load_context_free_val(nn_model.token_to_index)\n    calculate_and_log_val_metrics(nn_model, context_sensitive_val, context_free_val, prediction_mode,\n                                  calculate_ngram_distance=False)\n\n    test_dataset_ids = transform_contexts_to_token_ids(\n        list(lines_to_context(test_dataset)), nn_model.token_to_index, INPUT_SEQUENCE_LENGTH, INPUT_CONTEXT_SIZE)\n    log_predictions(predictions_filename, test_dataset_ids, nn_model, prediction_modes=[prediction_mode], **kwargs)\n\n\ndef predict(model_path,\n            tokens_index_path=None,\n            conditions_index_path=None,\n            default_predictions_path=None,\n            reverse_model_weights=None,\n            temperatures=None,\n            prediction_mode=None):\n    if not tokens_index_path:\n        tokens_index_path = get_index_to_token_path(BASE_CORPUS_NAME)\n    if not conditions_index_path:\n        conditions_index_path = get_index_to_condition_path(BASE_CORPUS_NAME)\n    if not temperatures:\n        temperatures = [DEFAULT_TEMPERATURE]\n    if not prediction_mode:\n        prediction_mode = PREDICTION_MODE_FOR_TESTS\n\n    # Construct list of parameters values for all possible combinations of passed parameters\n    prediction_params = [dict()]\n    if reverse_model_weights:\n        prediction_params = [\n            dict(params, mmi_reverse_model_score_weight=w)\n            for params in prediction_params\n            for w in reverse_model_weights\n        ]\n    if temperatures:\n        prediction_params = [dict(params, temperature=t) for params in prediction_params for t in temperatures]\n\n    if not is_non_empty_file(tokens_index_path):\n        _logger.warning(\'Couldn\\\'t find tokens_index file:\\n{}. \\nExiting...\'.format(tokens_index_path))\n        return\n\n    index_to_token = load_index_to_item(tokens_index_path)\n    index_to_condition = load_index_to_item(conditions_index_path)\n    w2v_model_id = get_w2v_model_id() if USE_PRETRAINED_W2V_EMBEDDINGS_LAYER else None\n\n    nn_model = CakeChatModel(\n        index_to_token,\n        index_to_condition,\n        training_data_param=ModelParam(value=None, id=TRAIN_CORPUS_NAME),\n        validation_data_param=ModelParam(value=None, id=get_validation_data_id(get_validation_sets_names())),\n        w2v_model_param=ModelParam(value=None, id=w2v_model_id),\n        model_init_path=model_path,\n        reverse_model=get_reverse_model(prediction_mode))\n\n    nn_model.init_model()\n    nn_model.resolve_model()\n\n    if not default_predictions_path:\n        default_predictions_path = os.path.join(RESULTS_PATH, \'results\', \'predictions_\' + nn_model.model_name)\n\n    # Get path for each combination of parameters\n    predictions_paths = []\n    # Add suffix to the filename only for parameters that have a specific value passed as an argument\n    # If no parameters were specified, no suffix is added\n    if len(prediction_params) > 1:\n        for cur_params in prediction_params:\n            cur_path = \'{base_path}_{params_str}.tsv\'.format(\n                base_path=default_predictions_path,\n                params_str=\'_\'.join([\'{}_{}\'.format(k, v) for k, v in cur_params.items()]))\n            predictions_paths.append(cur_path)\n    else:\n        predictions_paths = [default_predictions_path + \'.tsv\']\n\n    _logger.info(\'Model for prediction: {}\'.format(nn_model.model_path))\n    _logger.info(\'Tokens index: {}\'.format(tokens_index_path))\n    _logger.info(\'File with questions: {}\'.format(QUESTIONS_CORPUS_NAME))\n    _logger.info(\'Files to dump responses: {}\'.format(\'\\n\'.join(predictions_paths)))\n    _logger.info(\'Prediction parameters {}\'.format(\'\\n\'.join([str(x) for x in prediction_params])))\n\n    processed_test_set = get_tokenized_test_lines(QUESTIONS_CORPUS_NAME, set(index_to_token.values()))\n\n    for cur_params, cur_path in zip(prediction_params, predictions_paths):\n        _logger.info(\'Predicting with the following params: {}\'.format(cur_params))\n        _save_test_results(processed_test_set, cur_path, nn_model, prediction_mode, **cur_params)\n\n\ndef parse_args():\n    argparser = argparse.ArgumentParser()\n\n    argparser.add_argument(\n        \'-p\', \'--prediction-mode\', action=\'store\', help=\'Prediction mode\', choices=PREDICTION_MODES, default=None)\n\n    argparser.add_argument(\n        \'-m\',\n        \'--model\',\n        action=\'store\',\n        help=\'Path to the file with your model. \'\n        \'Be careful, model parameters are inferred from config, not from the filename\',\n        default=None)\n\n    argparser.add_argument(\n        \'-i\',\n        \'--tokens_index\',\n        action=\'store\',\n        help=\'Path to the json file with index_to_token dictionary.\',\n        default=None)\n\n    argparser.add_argument(\n        \'-c\',\n        \'--conditions_index\',\n        action=\'store\',\n        help=\'Path to the json file with index_to_condition dictionary.\',\n        default=None)\n\n    argparser.add_argument(\n        \'-o\',\n        \'--output\',\n        action=\'store\',\n        help=\'Path to the file to dump predictions.\'\n        \'Be careful, file extension "".tsv"" is appended to the filename automatically\',\n        default=None)\n\n    argparser.add_argument(\n        \'-r\',\n        \'--reverse-model-weights\',\n        action=\'append\',\n        type=float,\n        help=\'Reverse model score weight for prediction with MMI-reranking objective. Used only in *-reranking modes\',\n        default=None)\n\n    argparser.add_argument(\'-t\', \'--temperatures\', action=\'append\', help=\'temperature values\', default=None, type=float)\n\n    args = argparser.parse_args()\n\n    # Extra params validation\n    reranking_modes = [PREDICTION_MODES.beamsearch_reranking, PREDICTION_MODES.sampling_reranking]\n    if args.reverse_model_weights and args.prediction_mode not in reranking_modes:\n        raise Exception(\'--reverse-model-weights param can be specified only for *-reranking prediction modes.\')\n\n    return args\n\n\nif __name__ == \'__main__\':\n    args = vars(parse_args())\n    predict(args.pop(\'model\'), args.pop(\'tokens_index\'), args.pop(\'conditions_index\'), args.pop(\'output\'), **args)\n'"
tools/generate_predictions_for_condition.py,0,"b""import argparse\nimport os\nimport sys\n\nimport numpy as np\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n\nfrom cakechat.utils.env import init_cuda_env\n\ninit_cuda_env()\n\nfrom cakechat.config import QUESTIONS_CORPUS_NAME, INPUT_SEQUENCE_LENGTH, INPUT_CONTEXT_SIZE, \\\n    PREDICTION_MODES, PREDICTION_MODE_FOR_TESTS, DEFAULT_CONDITION, RANDOM_SEED, INTX\nfrom cakechat.utils.text_processing import get_tokens_sequence, replace_out_of_voc_tokens\nfrom cakechat.utils.dataset_loader import get_tokenized_test_lines\nfrom cakechat.dialog_model.model_utils import transform_context_token_ids_to_sentences, \\\n    transform_contexts_to_token_ids, lines_to_context\nfrom cakechat.dialog_model.inference import get_nn_responses\nfrom cakechat.dialog_model.factory import get_trained_model\n\nnp.random.seed(seed=RANDOM_SEED)\n\n\ndef load_corpus(nn_model, corpus_name):\n    return get_tokenized_test_lines(corpus_name, set(nn_model.index_to_token.values()))\n\n\ndef process_text(nn_model, text):\n    tokenized_line = get_tokens_sequence(text)\n    return [replace_out_of_voc_tokens(tokenized_line, nn_model.token_to_index)]\n\n\ndef transform_lines_to_contexts_token_ids(tokenized_lines, nn_model):\n    return transform_contexts_to_token_ids(\n        list(lines_to_context(tokenized_lines)), nn_model.token_to_index, INPUT_SEQUENCE_LENGTH, INPUT_CONTEXT_SIZE)\n\n\ndef predict_for_condition_id(nn_model, contexts, condition_id, prediction_mode=PREDICTION_MODE_FOR_TESTS):\n    condition_ids = np.array([condition_id] * contexts.shape[0], dtype=INTX)\n    responses = get_nn_responses(\n        contexts, nn_model, mode=prediction_mode, output_candidates_num=1, condition_ids=condition_ids)\n    return [candidates[0] for candidates in responses]\n\n\ndef print_predictions(nn_model, contexts_token_ids, condition, prediction_mode=PREDICTION_MODE_FOR_TESTS):\n    x_sents = transform_context_token_ids_to_sentences(contexts_token_ids, nn_model.index_to_token)\n    y_sents = predict_for_condition_id(\n        nn_model, contexts_token_ids, nn_model.condition_to_index[condition], prediction_mode=prediction_mode)\n\n    for x, y in zip(x_sents, y_sents):\n        print('condition: {}; context: {}'.format(condition, x))\n        print('response: {}'.format(y))\n        print()\n\n\ndef parse_args():\n    argparser = argparse.ArgumentParser()\n\n    argparser.add_argument(\n        '-p',\n        '--prediction-mode',\n        action='store',\n        help='Prediction mode',\n        choices=PREDICTION_MODES,\n        default=PREDICTION_MODE_FOR_TESTS)\n\n    argparser.add_argument('-d', '--data', action='store', help='Corpus name', default=QUESTIONS_CORPUS_NAME)\n    argparser.add_argument('-t', '--text', action='store', help='Context message that feed to the model', default=None)\n    argparser.add_argument('-c', '--condition', action='store', help='Condition', default=DEFAULT_CONDITION)\n\n    return argparser.parse_args()\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    nn_model = get_trained_model()\n\n    if args.text:\n        tokenized_lines = process_text(nn_model, args.text)\n    else:\n        tokenized_lines = load_corpus(nn_model, args.data)\n\n    contexts_token_ids = transform_lines_to_contexts_token_ids(tokenized_lines, nn_model)\n\n    print_predictions(nn_model, contexts_token_ids, args.condition, prediction_mode=args.prediction_mode)\n"""
tools/prepare_index_files.py,0,"b'import sys\nimport os\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom collections import Counter\nimport json\n\nfrom tqdm import tqdm\n\nfrom cakechat.utils.files_utils import is_non_empty_file, ensure_dir\nfrom cakechat.utils.text_processing import get_tokens_sequence, get_processed_corpus_path, get_index_to_token_path, \\\n    get_index_to_condition_path, load_processed_dialogs_from_json, FileTextLinesIterator, SPECIAL_TOKENS\nfrom cakechat.config import BASE_CORPUS_NAME, TRAIN_CORPUS_NAME, DEFAULT_CONDITION, VOCABULARY_MAX_SIZE, MAX_CONDITIONS_NUM\n\nTEXT_FIELD_NAME = \'text\'\nCONDITION_FIELD_NAME = \'condition\'\nSIMPLE_TOKENIZE = False  # Set to True if you want to use str.split() instead of get_tokens_sequence() for\n# tokenization\n\n\ndef build_index_mappings(corpus_path, max_tokens_num=VOCABULARY_MAX_SIZE, max_conditions_num=MAX_CONDITIONS_NUM,\n                         simple_tokenize=SIMPLE_TOKENIZE):\n    if not is_non_empty_file(corpus_path):\n        raise ValueError(\'Test corpus file doesn\\\'t exist: {}\'.format(corpus_path))\n\n    dialogs = load_processed_dialogs_from_json(\n        FileTextLinesIterator(corpus_path), text_field_name=TEXT_FIELD_NAME, condition_field_name=CONDITION_FIELD_NAME)\n\n    tokens_counter = Counter()\n    conditions_counter = Counter()\n\n    for dialog in tqdm(dialogs):\n        for utterance in dialog:\n            tokens = utterance[TEXT_FIELD_NAME].split() if simple_tokenize else \\\n                get_tokens_sequence(utterance[TEXT_FIELD_NAME])\n\n            tokens_counter.update(tokens)\n            conditions_counter[utterance[CONDITION_FIELD_NAME]] += 1\n\n    # Build the tokens list\n    vocab = list(SPECIAL_TOKENS) + \\\n            [token for token, _ in tokens_counter.most_common(max_tokens_num - len(SPECIAL_TOKENS))]\n\n    # Build the conditions list\n    conditions = [condition for condition, _ in conditions_counter.most_common(max_conditions_num)]\n\n    # Validate the condition list\n    if DEFAULT_CONDITION not in conditions:\n        raise Exception(\'No default condition ""{}"" found in the dataset condition list.\'.format(DEFAULT_CONDITION))\n\n    # Return index_to_token and index_to_condition mappings\n    return dict(enumerate(vocab)), dict(enumerate(conditions))\n\n\ndef dump_index_to_item(index_to_item, path):\n    ensure_dir(os.path.dirname(path))\n    with open(path, \'w\', encoding=\'utf-8\') as fh:\n        json.dump(index_to_item, fh, ensure_ascii=False)\n\n\nif __name__ == \'__main__\':\n    processed_train_corpus_path = get_processed_corpus_path(TRAIN_CORPUS_NAME)\n    index_to_token_path = get_index_to_token_path(BASE_CORPUS_NAME)\n    index_to_condition_path = get_index_to_condition_path(BASE_CORPUS_NAME)\n\n    index_to_token, index_to_condition = build_index_mappings(processed_train_corpus_path)\n    dump_index_to_item(index_to_token, index_to_token_path)\n    dump_index_to_item(index_to_condition, index_to_condition_path)\n'"
tools/telegram_bot.py,0,"b""import os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport argparse\nfrom collections import deque\n\nfrom cakechat.utils.env import init_cuda_env\n\ninit_cuda_env()\n\nfrom cakechat.api.response import get_response\nfrom cakechat.config import INPUT_CONTEXT_SIZE, DEFAULT_CONDITION\nfrom cakechat.utils.telegram_bot_client import TelegramBot, AbstractTelegramChatSession\n\n\nclass CakeChatTelegramChatSession(AbstractTelegramChatSession):\n    def __init__(self, *args, **kwargs):\n        super(CakeChatTelegramChatSession, self).__init__(*args, **kwargs)\n        self._context = deque(maxlen=INPUT_CONTEXT_SIZE)\n\n    def handle_text_message(self, msg_text, msg):\n        self._context.append(msg_text.strip())\n        response = get_response(self._context, DEFAULT_CONDITION)\n        self._context.append(response)\n        self._send_text(response)\n\n    def default_handle_message(self, msg):\n        self._send_text('Sorry bruh, text only')\n\n\nif __name__ == '__main__':\n    argparser = argparse.ArgumentParser()\n    argparser.add_argument('-t', '--token', help='Bot token')\n    args = argparser.parse_args()\n\n    TelegramBot(token=args.token).run(CakeChatTelegramChatSession)\n"""
tools/test_api.py,0,"b'import argparse\nimport os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport requests\n\nfrom cakechat.config import DEFAULT_CONDITION\n\n_HOST_FQDN = \'127.0.0.1\'\n_SERVER_PORT = \'8080\'\n\n\ndef parse_args():\n    argparser = argparse.ArgumentParser()\n    argparser.add_argument(\'-f\', \'--fqdn\', action=\'store\', default=_HOST_FQDN)\n    argparser.add_argument(\'-p\', \'--port\', action=\'store\', default=_SERVER_PORT)\n    argparser.add_argument(\'-c\', \'--context\', action=\'append\', help=\'set ""-c your_dialog_context""\', required=True)\n    argparser.add_argument(\'-e\', \'--emotion\', action=\'store\', default=DEFAULT_CONDITION)\n\n    return argparser.parse_args()\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    url = \'http://{}:{}/cakechat_api/v1/actions/get_response\'.format(args.fqdn, args.port)\n    body = {\'context\': args.context, \'emotion\': args.emotion}\n\n    response = requests.post(url, json=body)\n    print(response.json())\n'"
tools/train.py,1,"b""import argparse\nimport os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport random\nimport numpy\nimport tensorflow as tf\n\nfrom cakechat.utils.env import init_keras, try_import_horovod\n\nhvd = try_import_horovod()\ninit_keras(hvd)\n\n# fix random seeds for experiments reproducibility\nrandom.seed(42)\nnumpy.random.seed(42)\ntf.set_random_seed(42)\n\nfrom cakechat.config import BASE_CORPUS_NAME, TRAIN_CORPUS_NAME, CONTEXT_SENSITIVE_VAL_CORPUS_NAME, \\\n    USE_PRETRAINED_W2V_EMBEDDINGS_LAYER, S3_MODELS_BUCKET_NAME, S3_NN_MODEL_REMOTE_DIR, PREDICTION_MODE_FOR_TESTS\nfrom cakechat.dialog_model.factory import get_reverse_model\nfrom cakechat.dialog_model.model import CakeChatModel\nfrom cakechat.utils.data_types import ModelParam\nfrom cakechat.utils.dataset_loader import get_validation_data_id, get_validation_sets_names, \\\n    get_validation_dataset_name_to_data, get_training_dataset\nfrom cakechat.utils.files_utils import is_non_empty_file, FileNotFoundException\nfrom cakechat.utils.logger import get_tools_logger\nfrom cakechat.utils.s3 import S3FileResolver\nfrom cakechat.utils.text_processing import get_processed_corpus_path, get_index_to_token_path, \\\n    get_index_to_condition_path, load_index_to_item\nfrom cakechat.utils.w2v.model import get_w2v_model_id, get_w2v_model\n\n_logger = get_tools_logger(__file__)\n\n\ndef _look_for_saved_files(files_paths):\n    for f_path in files_paths:\n        if not is_non_empty_file(f_path):\n            raise FileNotFoundException('\\nCould not find the following file or it\\'s empty: {0}'.format(f_path))\n\n\ndef train(model_init_path=None,\n          is_reverse_model=False,\n          train_subset_size=None,\n          use_pretrained_w2v=USE_PRETRAINED_W2V_EMBEDDINGS_LAYER,\n          train_corpus_name=TRAIN_CORPUS_NAME,\n          context_sensitive_val_corpus_name=CONTEXT_SENSITIVE_VAL_CORPUS_NAME,\n          base_corpus_name=BASE_CORPUS_NAME,\n          s3_models_bucket_name=S3_MODELS_BUCKET_NAME,\n          s3_nn_model_remote_dir=S3_NN_MODEL_REMOTE_DIR,\n          prediction_mode_for_tests=PREDICTION_MODE_FOR_TESTS):\n    processed_train_corpus_path = get_processed_corpus_path(train_corpus_name)\n    processed_val_corpus_path = get_processed_corpus_path(context_sensitive_val_corpus_name)\n    index_to_token_path = get_index_to_token_path(base_corpus_name)\n    index_to_condition_path = get_index_to_condition_path(base_corpus_name)\n\n    # check the existence of all necessary files before compiling the model\n    _look_for_saved_files(files_paths=[processed_train_corpus_path, processed_val_corpus_path, index_to_token_path])\n\n    # load essentials for building model and training\n    index_to_token = load_index_to_item(index_to_token_path)\n    index_to_condition = load_index_to_item(index_to_condition_path)\n    token_to_index = {v: k for k, v in index_to_token.items()}\n    condition_to_index = {v: k for k, v in index_to_condition.items()}\n\n    training_data_param = ModelParam(\n        value=get_training_dataset(train_corpus_name, token_to_index, condition_to_index, is_reverse_model,\n                                   train_subset_size),\n        id=train_corpus_name)\n\n    val_sets_names = get_validation_sets_names()\n    validation_data_param = ModelParam(\n        value=get_validation_dataset_name_to_data(val_sets_names, token_to_index, condition_to_index, is_reverse_model),\n        id=get_validation_data_id(val_sets_names))\n\n    w2v_model_param = ModelParam(value=get_w2v_model(), id=get_w2v_model_id()) if use_pretrained_w2v \\\n        else ModelParam(value=None, id=None)\n\n    model_resolver_factory = S3FileResolver.init_resolver(\n        bucket_name=s3_models_bucket_name, remote_dir=s3_nn_model_remote_dir)\n\n    reverse_model = get_reverse_model(prediction_mode_for_tests) if not is_reverse_model else None\n\n    # build CakeChatModel\n    cakechat_model = CakeChatModel(\n        index_to_token,\n        index_to_condition,\n        training_data_param=training_data_param,\n        validation_data_param=validation_data_param,\n        w2v_model_param=w2v_model_param,\n        model_init_path=model_init_path,\n        model_resolver=model_resolver_factory,\n        is_reverse_model=is_reverse_model,\n        reverse_model=reverse_model,\n        horovod=hvd)\n\n    # train model\n    cakechat_model.train_model()\n\n\ndef parse_args():\n    argparser = argparse.ArgumentParser()\n\n    argparser.add_argument(\n        '-r', '--reverse', action='store_true', help='Pass this flag if you want to train reverse model.')\n    argparser.add_argument(\n        '-i',\n        '--init_weights',\n        help='Path to the file with weights that should be used for the model\\'s initialisation')\n    argparser.add_argument('-s', '--train-subset-size', action='store', type=int)\n\n    return argparser.parse_args()\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    train(model_init_path=args.init_weights, is_reverse_model=args.reverse, train_subset_size=args.train_subset_size)\n"""
tools/train_w2v.py,0,"b""import os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom cakechat.utils.text_processing import get_processed_corpus_path, load_processed_dialogs_from_json, \\\n    FileTextLinesIterator, get_dialog_lines_and_conditions, ProcessedLinesIterator, get_flatten_dialogs\nfrom cakechat.utils.w2v.model import _get_w2v_model as get_w2v_model\nfrom cakechat.config import TRAIN_CORPUS_NAME, VOCABULARY_MAX_SIZE, WORD_EMBEDDING_DIMENSION, W2V_WINDOW_SIZE, \\\n    USE_SKIP_GRAM\n\nif __name__ == '__main__':\n    processed_corpus_path = get_processed_corpus_path(TRAIN_CORPUS_NAME)\n\n    dialogs = load_processed_dialogs_from_json(\n        FileTextLinesIterator(processed_corpus_path), text_field_name='text', condition_field_name='condition')\n\n    training_dialogs_lines_for_w2v, _ = get_dialog_lines_and_conditions(\n        get_flatten_dialogs(dialogs), text_field_name='text', condition_field_name='condition')\n\n    tokenized_training_lines = ProcessedLinesIterator(training_dialogs_lines_for_w2v, processing_callbacks=[str.split])\n\n    get_w2v_model(\n        tokenized_lines=tokenized_training_lines,\n        corpus_name=TRAIN_CORPUS_NAME,\n        voc_size=VOCABULARY_MAX_SIZE,\n        vec_size=WORD_EMBEDDING_DIMENSION,\n        window_size=W2V_WINDOW_SIZE,\n        skip_gram=USE_SKIP_GRAM)\n"""
cakechat/api/__init__.py,0,b''
cakechat/api/config.py,0,"b""from cakechat.config import PREDICTION_MODES\n\n# Prediction mode used in API\nPREDICTION_MODE = PREDICTION_MODES.sampling_reranking\n\n# In case of PREDICTION_MODES.{beamsearch, beamsearch-reranking, sampling-reranking} choose random non-offensive\n# response out of K best candidates proposed by the algorithm.\nNUM_BEST_CANDIDATES_TO_PICK_FROM = 3\n\n# In case of PREDICTION_MODES.sampling generate samples one-by-one until a non-offensive sample occurs. This parameter\n# defines max number of samples will be generated until succeed.\nSAMPLING_ATTEMPTS_NUM = 10\n\n# Default response text in case we weren't able to produce appropriate response.\nDEFAULT_RESPONSE = '\xf0\x9f\x99\x8a'\n"""
cakechat/api/response.py,0,"b'import random\n\nfrom cakechat.api.config import PREDICTION_MODE, NUM_BEST_CANDIDATES_TO_PICK_FROM, SAMPLING_ATTEMPTS_NUM, \\\n    DEFAULT_RESPONSE\nfrom cakechat.config import INPUT_CONTEXT_SIZE, INPUT_SEQUENCE_LENGTH, PREDICTION_MODES\nfrom cakechat.dialog_model.factory import get_trained_model, get_reverse_model\nfrom cakechat.dialog_model.inference import get_nn_responses, warmup_predictor\nfrom cakechat.dialog_model.model_utils import transform_contexts_to_token_ids, transform_conditions_to_ids\nfrom cakechat.utils.offense_detector import OffenseDetector\nfrom cakechat.utils.offense_detector.config import OFFENSIVE_PHRASES_PATH\nfrom cakechat.utils.text_processing import get_tokens_sequence, prettify_response\n\n_offense_detector = OffenseDetector(OFFENSIVE_PHRASES_PATH)\n_cakechat_model = get_trained_model(reverse_model=get_reverse_model(PREDICTION_MODE))\nwarmup_predictor(_cakechat_model, PREDICTION_MODE)\n\n\ndef _is_appropriate_response(response):\n    return response != \'\' and not _offense_detector.has_offensive_ngrams(response)\n\n\ndef _get_non_offensive_response_using_fast_sampling(context_tokens_ids, condition_id):\n    for _ in range(SAMPLING_ATTEMPTS_NUM):\n        response = get_nn_responses(\n            context_tokens_ids, _cakechat_model, PREDICTION_MODES.sampling, condition_ids=condition_id)[0][0]\n\n        if _is_appropriate_response(response):\n            return prettify_response(response)\n\n    return DEFAULT_RESPONSE\n\n\ndef _get_non_offensive_response(context_tokens_ids, condition_id):\n    responses = get_nn_responses(\n        context_tokens_ids,\n        _cakechat_model,\n        PREDICTION_MODE,\n        output_candidates_num=NUM_BEST_CANDIDATES_TO_PICK_FROM,\n        condition_ids=condition_id)[0]\n\n    responses = list(filter(_is_appropriate_response, responses))\n    if responses:\n        selected_response = random.choice(responses)\n        return prettify_response(selected_response)\n\n    return DEFAULT_RESPONSE\n\n\ndef get_response(dialog_context, emotion):\n    """"""\n    :param dialog_context: list of dialog utterances\n    :param emotion: emotion to condition response\n    :return: dialog response conditioned on input emotion\n    """"""\n    tokenized_dialog_context = list(map(get_tokens_sequence, dialog_context))\n    tokenized_dialog_contexts = [tokenized_dialog_context]\n    context_tokens_ids = transform_contexts_to_token_ids(tokenized_dialog_contexts, _cakechat_model.token_to_index,\n                                                         INPUT_SEQUENCE_LENGTH, INPUT_CONTEXT_SIZE)\n\n    condition_ids = transform_conditions_to_ids([emotion], _cakechat_model.condition_to_index, n_dialogs=1)\n\n    if PREDICTION_MODE == PREDICTION_MODES.sampling:  # Different strategy here for better performance.\n        return _get_non_offensive_response_using_fast_sampling(context_tokens_ids, condition_ids)\n    else:\n        return _get_non_offensive_response(context_tokens_ids, condition_ids)\n'"
cakechat/api/utils.py,0,"b""from flask import jsonify\n\n\ndef get_api_error_response(message, code, logger):\n    logger.error(message)\n    return jsonify({'message': message}), code\n\n\ndef _is_list_of_unicode_strings(data):\n    return data and isinstance(data, (list, tuple)) and all(isinstance(s, str) for s in data)\n\n\ndef parse_dataset_param(params, param_name, required=True):\n    if not required and params.get(param_name) is None:\n        return None\n\n    dataset = params[param_name]\n    if not _is_list_of_unicode_strings(dataset):\n        raise ValueError('`{}` should be non-empty list of unicode strings'.format(param_name))\n    if not all(dataset):\n        raise ValueError('`{}` should not contain empty strings'.format(param_name))\n\n    return dataset\n"""
cakechat/dialog_model/__init__.py,0,b''
cakechat/dialog_model/abstract_callbacks.py,0,"b'""""""\nEssentials for using training callbacks together with AbstractKerasModel.\n\nTL;DR\n1. If you are implementing your own callback, please inherit it from `AbstractKerasModelCallback`.\n2. If you are using a stock (keras) callback, please wrap it with `ParametrizedCallback`\n""""""\nimport abc\n\nimport keras\n\n\nclass _KerasCallbackAdapter(keras.callbacks.Callback):\n    """"""\n    Class that adapts `AbstractKerasModelCallback`-based callback to the native keras one. Not assumed to be used\n    directly in the client code.\n    """"""\n\n    def __init__(self, callback):\n        """"""\n        :param callback: instance of `AbstractKerasModelCallback`\n        """"""\n        super(_KerasCallbackAdapter, self).__init__()\n        self.__callback = callback\n\n    def on_epoch_begin(self, epoch, logs=None):\n        return self.__callback.on_epoch_begin(epoch, logs)\n\n    def on_epoch_end(self, epoch, logs=None):\n        return self.__callback.on_epoch_end(epoch, logs)\n\n    def on_batch_begin(self, batch, logs=None):\n        return self.__callback.on_batch_begin(batch, logs)\n\n    def on_batch_end(self, batch, logs=None):\n        return self.__callback.on_batch_end(batch, logs)\n\n    def on_train_begin(self, logs=None):\n        return self.__callback.on_train_begin(logs)\n\n    def on_train_end(self, logs=None):\n        return self.__callback.on_train_end(logs)\n\n\nclass _AbstractCallback(object, metaclass=abc.ABCMeta):\n    """"""\n    Common interface for training callbacks used with AbstractKerasModel models\n    """"""\n\n    @property\n    @abc.abstractmethod\n    def callback_params(self):\n        """"""\n        :return dict of params that affect the resulting model\n        """"""\n        pass\n\n    @property\n    @abc.abstractmethod\n    def runs_only_on_main_worker(self):\n        """"""\n        :return True, if this callback runs only on main worker (in case of distributed training), False otherwise\n        """"""\n        pass\n\n\nclass AbstractKerasModelCallback(_AbstractCallback, metaclass=abc.ABCMeta):\n    """"""\n    Base callback class that is compatible with `AbstractKerasModel` (so it can be used within keras via\n    `_KerasCallbackAdapter`). If you are implementing your own callback that utilizes `AbstractKerasModel`, please\n    inherit from this class and not from `keras.callbacks.Callback` in order to isolate your callback\'s variables and\n    methods from the ones that belong to `keras.callbacks.Callback`.\n    See `EvaluateAndSaveBestIntermediateModelCallback` as an example.\n    """"""\n\n    def __init__(self, model):\n        """"""\n        :param model: instance of `AbstractKerasModel`\n        """"""\n        super(AbstractKerasModelCallback, self).__init__()\n        self._model = model\n\n    def on_epoch_begin(self, epoch, logs=None):\n        pass\n\n    def on_epoch_end(self, epoch, logs=None):\n        pass\n\n    def on_batch_begin(self, batch, logs=None):\n        pass\n\n    def on_batch_end(self, batch, logs=None):\n        pass\n\n    def on_train_begin(self, logs=None):\n        pass\n\n    def on_train_end(self, logs=None):\n        pass\n\n\nclass ParametrizedCallback(_AbstractCallback):\n    """"""\n    Provides `_AbstractCallback` interface for arbitrary callback object.\n    If you are going to use one of the stock keras callbacks, please choose one of the following options:\n        1. Use this class to instantiate a AbstractKerasModel-friendly callback object. Specify `callback_params` if the callback\n            affects the resulting model by some parameters\n        2. Create class <YourCallbackName>(ParametrizedCallback) and pass the original callback object, as well as\n            `callback_params` if the callback affects the resulting model. Recommended, if you are going to publish your\n            callback in the repository.\n    See `AbstractKerasModel#_create_essential_callbacks` as an example.\n    """"""\n\n    def __init__(self, callback, runs_only_on_main_worker, callback_params=None):\n        """"""\n        :param callback: arbitrary callback object (e.g. instance of `keras.callbacks.Callback`)\n        :param runs_only_on_main_worker: True, if this callback runs only on main worker (in case of distributed\n                training), False otherwise\n        :param callback_params: dict of params that affect the resulting model\n        :return:\n        """"""\n        super(ParametrizedCallback, self).__init__()\n        self._callback = callback\n        self._callback_params = callback_params or {}\n        self._runs_only_on_main_worker = runs_only_on_main_worker\n\n    @property\n    def callback_params(self):\n        return self._callback_params\n\n    @property\n    def runs_only_on_main_worker(self):\n        return self._runs_only_on_main_worker\n\n    @property\n    def callback(self):\n        return self._callback\n'"
cakechat/dialog_model/abstract_model.py,0,"b'import abc\nimport hashlib\nimport json\nimport os\n\nfrom cakechat.dialog_model.quality.metrics.utils import MetricsSerializer\nfrom cakechat.utils.files_utils import DummyFileResolver\nfrom cakechat.utils.logger import WithLogger\n\n\nclass AbstractModel(WithLogger, metaclass=abc.ABCMeta):\n    # Model resources default values\n    _MODEL_RESOURCE_NAME = \'model\'\n    _METRICS_RESOURCE_NAME = \'metrics\'\n\n    def __init__(self, model_resolver_factory=None, metrics_serializer=None):\n        """"""\n        :param model_resolver_factory: a factory of `cakechat.utils.files_utils.AbstractFileResolver` that\n            takes model path and returns a file resolver object\n        :param metrics_serializer: an instance compatible with the interface of\n        `cakechat.dialog_model.quality.metrics.utils.MetricsSerializer`\n        :return:\n        """"""\n        super(AbstractModel, self).__init__()\n\n        self._model = None\n        self.__model_resolver_factory = model_resolver_factory if model_resolver_factory else DummyFileResolver\n\n        self._model_init_path = None\n\n        self._metrics = None\n        self._metrics_serializer = metrics_serializer if metrics_serializer else MetricsSerializer()\n\n    @property\n    def model(self):\n        return self._model\n\n    @property\n    def metrics(self):\n        return self._metrics\n\n    @property\n    def _model_params_str(self):\n        return json.dumps(self.model_params, sort_keys=True)\n\n    @property\n    def model_id(self):\n        if not hasattr(self, \'__uniq_id\'):\n            self.__uniq_id = hashlib.md5(self._model_params_str.encode()).hexdigest()\n\n        # it\'s enough to use only the first 12 characters of the hash to avoid collisions\n        # see: http://stackoverflow.com/a/18134919\n        short_id = self.__uniq_id[:12]\n        return \'{}_{}\'.format(self.model_name, short_id)\n\n    @property\n    def model_path(self):\n        return os.path.join(self._model_dir, self.model_id)\n\n    @property\n    def _model_resource_path(self):\n        return os.path.join(self.model_path, self._MODEL_RESOURCE_NAME)\n\n    @property\n    def _metrics_resource_path(self):\n        return os.path.join(self.model_path, self._METRICS_RESOURCE_NAME)\n\n    @property\n    @abc.abstractmethod\n    def model_name(self):\n        """"""\n        Returns human-readable model name\n\n        :return:\n        """"""\n        pass\n\n    @property\n    @abc.abstractmethod\n    def model_params(self):\n        """"""\n        Returns a dict with model params. Note that these params are used to compute a unique model id, so they should\n        reflect the full model state (including the data (or its id) on which the model is trained/validated).\n\n        :return:\n        """"""\n        pass\n\n    @property\n    @abc.abstractmethod\n    def _model_dir(self):\n        pass\n\n    @abc.abstractmethod\n    def train_model(self, *args, **kwargs):\n        """"""\n        Trains the model. Put just for method name unification\n\n        :param args:\n        :param kwargs:\n        :return:\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def _load_model(self, fresh_model, model_resource_path):\n        """"""\n        Fills given fresh_model by parameters stored in model_resource_path\n\n        :param fresh_model: model object\n        :param model_resource_path:\n        :return: loaded model\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def _save_model(self, model_resource_path):\n        pass\n\n    @abc.abstractmethod\n    def _evaluate(self):\n        """"""\n        Evaluates model on validation set/sets\n\n        :return: { dataset_name : { metric_name : metric_value } }\n        """"""\n        pass\n\n    def resolve_model(self):\n        self._logger.info(\'Looking for the previously trained model\')\n        self._logger.info(\'Model params str: {}\'.format(self._model_params_str))\n        model_path = self._model_init_path or self._model_resource_path\n\n        if not self._model_init_path and not self.__model_resolver_factory(self.model_path).resolve():\n            err_msg = \'Can\\\'t find previously trained model in {}\'.format(self.model_path)\n            self._logger.error(err_msg)\n            raise ValueError(err_msg)\n\n        self._logger.info(\'Loading previously calculated model\')\n        self._model = self._load_model(self._model, model_path)\n        self._logger.info(\'Loaded model: {}\'.format(model_path))\n'"
cakechat/dialog_model/callbacks.py,0,"b'import time\n\nfrom cakechat.config import SCREEN_LOG_NUM_TEST_LINES, PREDICTION_MODE_FOR_TESTS, LOG_CANDIDATES_NUM, LOG_RUN_METADATA, \\\n    EVAL_STATE_PER_BATCHES\nfrom cakechat.dialog_model.inference import get_nn_responses\nfrom cakechat.dialog_model.keras_model import EvaluateAndSaveBestIntermediateModelCallback\nfrom cakechat.dialog_model.model_utils import transform_context_token_ids_to_sentences\nfrom cakechat.utils.dataset_loader import load_questions_set\nfrom cakechat.utils.logger import laconic_logger\n\n\nclass CakeChatEvaluatorCallback(EvaluateAndSaveBestIntermediateModelCallback):\n    def __init__(self,\n                 model,\n                 index_to_token,\n                 batch_size,\n                 batches_num_per_epoch,\n                 eval_state_per_batches=EVAL_STATE_PER_BATCHES,\n                 prediction_mode_for_tests=PREDICTION_MODE_FOR_TESTS,\n                 log_run_metadata=LOG_RUN_METADATA,\n                 screen_log_num_test_lines=SCREEN_LOG_NUM_TEST_LINES,\n                 log_candidates_num=LOG_CANDIDATES_NUM):\n        """"""\n        :param model: CakeChatModel object\n        :param eval_state_per_batches: run model evaluation each `eval_state_per_batches` steps\n        """"""\n        super(CakeChatEvaluatorCallback, self).__init__(model, eval_state_per_batches)\n\n        self._index_to_token = index_to_token\n        self._token_to_index = {v: k for k, v in index_to_token.items()}\n\n        self._val_contexts_tokens_ids = load_questions_set(self._token_to_index).x[:screen_log_num_test_lines]\n        self._val_contexts = \\\n            transform_context_token_ids_to_sentences(self._val_contexts_tokens_ids, self._index_to_token)\n\n        self._batch_size = batch_size\n        self._batches_num = batches_num_per_epoch\n\n        self._cur_batch_id = 0\n        self._cur_loss = 0\n\n        self._batch_start_time = None\n        self._total_training_time = 0\n\n        # logging params\n        self._prediction_mode_for_tests = prediction_mode_for_tests\n        self._log_run_metadata = log_run_metadata\n        self._log_candidates_num = log_candidates_num\n\n    def on_batch_begin(self, batch, logs=None):\n        self._batch_start_time = time.time()\n\n    def on_batch_end(self, batch, logs=None):\n        self._total_training_time += time.time() - self._batch_start_time\n\n        if batch % self._eval_state_per_batches == 0:\n            self._cur_loss = logs.get(\'loss\')\n            self._log_train_statistics()\n            self._log_sample_answers()\n            self._eval_and_save_current_model(batch)\n            self._log_metrics({\'train\': {\'loss\': self._cur_loss}})\n\n            if self._log_run_metadata:\n                self._model.metrics_plotter.log_run_metadata(self._model.model_id, self._model.run_metadata)\n                self._logger.info(\'Logged run_metadata to tensorboard\')\n\n        self._cur_batch_id += 1\n\n    def _log_train_statistics(self):\n        total_time = time.time() - self._training_start_time\n\n        progress = self._cur_batch_id / self._batches_num  # may be more than 100% if epochs num is more than 1\n        avr_time_per_batch = total_time / (self._cur_batch_id + 1)\n        expected_time_per_epoch = avr_time_per_batch * self._batches_num\n        total_training_time_in_percent = self._total_training_time / total_time\n\n        batches_per_sec = self._cur_batch_id / total_time\n        samples_per_sec = self._cur_batch_id * self._batch_size / total_time\n\n        self._logger.info(\'Train statistics:\\n\')\n\n        laconic_logger.info(\'batch:\\t{batch_id} / {batches_num} ({progress:.1%})\'.format(\n            batch_id=self._cur_batch_id, batches_num=self._batches_num, progress=progress))\n\n        laconic_logger.info(\'loss:\\t{loss:.2f}\'.format(loss=self._cur_loss))\n\n        laconic_logger.info(\'time:\\tepoch estimate {epoch_time:.2f} h | total {total_time:.2f} h | \'\n                            \'train {training_time:.2f} h ({training_time_percent:.1%})\'.format(\n                                epoch_time=expected_time_per_epoch / 3600,  # in hours\n                                total_time=total_time / 3600,  # in hours\n                                training_time=self._total_training_time / 3600,  # in hours\n                                training_time_percent=total_training_time_in_percent))\n\n        laconic_logger.info(\'speed:\\t{batches_per_hour:.0f} batches/h, {samples_per_sec:.0f} samples/sec\\n\'.format(\n            batches_per_hour=batches_per_sec * 3600, samples_per_sec=samples_per_sec))\n\n    def _log_sample_answers(self):\n        self._logger.info(\'Sample responses for {} mode:\'.format(self._prediction_mode_for_tests))\n\n        responses = get_nn_responses(\n            self._val_contexts_tokens_ids,\n            self._model,\n            self._prediction_mode_for_tests,\n            output_candidates_num=self._log_candidates_num)\n\n        for context, response_candidates in zip(self._val_contexts, responses):\n            laconic_logger.info(\'\')  # for better readability\n\n            for i, response in enumerate(response_candidates):\n                laconic_logger.info(\'{0: <35}\\t #{1: <2d} --> \\t{2}\'.format(context, i + 1, response))\n\n        laconic_logger.info(\'\')  # for better readability\n'"
cakechat/dialog_model/factory.py,0,"b'import os\n\nfrom cachetools import cached\n\nfrom cakechat.config import BASE_CORPUS_NAME, S3_MODELS_BUCKET_NAME, S3_TOKENS_IDX_REMOTE_DIR, \\\n    S3_NN_MODEL_REMOTE_DIR, S3_CONDITIONS_IDX_REMOTE_DIR, PREDICTION_MODES, TRAIN_CORPUS_NAME, \\\n    USE_PRETRAINED_W2V_EMBEDDINGS_LAYER\nfrom cakechat.dialog_model.inference_model import InferenceCakeChatModel\nfrom cakechat.utils.data_types import ModelParam\nfrom cakechat.utils.dataset_loader import get_validation_data_id, get_validation_sets_names\nfrom cakechat.utils.files_utils import FileNotFoundException\nfrom cakechat.utils.s3 import S3FileResolver, get_s3_model_resolver\nfrom cakechat.utils.text_processing import get_index_to_token_path, load_index_to_item, get_index_to_condition_path\nfrom cakechat.utils.w2v.model import get_w2v_model_id\n\n\ndef _get_index_to_token(fetch_from_s3):\n    index_to_token_path = get_index_to_token_path(BASE_CORPUS_NAME)\n    file_name = os.path.basename(index_to_token_path)\n    if fetch_from_s3:\n        tokens_idx_resolver = S3FileResolver(index_to_token_path, S3_MODELS_BUCKET_NAME, S3_TOKENS_IDX_REMOTE_DIR)\n        if not tokens_idx_resolver.resolve():\n            raise FileNotFoundException(\'No such file on S3: {}\'.format(file_name))\n    else:\n        if not os.path.exists(index_to_token_path):\n            raise FileNotFoundException(\'No such file: {}\'.format(file_name) +\n                                        \'Run ""python tools/fetch.py"" first to get all necessary files.\')\n\n    return load_index_to_item(index_to_token_path)\n\n\ndef _get_index_to_condition(fetch_from_s3):\n    index_to_condition_path = get_index_to_condition_path(BASE_CORPUS_NAME)\n    if fetch_from_s3:\n        index_to_condition_resolver = S3FileResolver(index_to_condition_path, S3_MODELS_BUCKET_NAME,\n                                                     S3_CONDITIONS_IDX_REMOTE_DIR)\n        if not index_to_condition_resolver.resolve():\n            raise FileNotFoundException(\'Can\\\'t get index_to_condition because file does not exist on S3\')\n    else:\n        if not os.path.exists(index_to_condition_path):\n            raise FileNotFoundException(\'Can\\\'t get index_to_condition because file does not exist. \'\n                                        \'Run tools/fetch.py first to get all required files or construct \'\n                                        \'it yourself.\')\n\n    return load_index_to_item(index_to_condition_path)\n\n\n@cached(cache={})\ndef get_trained_model(is_reverse_model=False, reverse_model=None, fetch_from_s3=True):\n    """"""\n    Get a trained model, direct or reverse.\n    :param is_reverse_model: boolean, if True, a reverse trained model will be returned to be used during inference\n    in direct model in *_reranking modes; if False, a direct trained model is returned\n    :param reverse_model: object of a reverse model to be used in direct model in *_reranking inference modes\n    :param fetch_from_s3: boolean, if True, download trained model from Amazon S3 if the the model is not found locally;\n    if False, the model presence will be checked only locally\n    :return:\n    """"""\n    if fetch_from_s3:\n        resolver_factory = get_s3_model_resolver(S3_MODELS_BUCKET_NAME, S3_NN_MODEL_REMOTE_DIR)\n    else:\n        resolver_factory = None\n\n    w2v_model_id = get_w2v_model_id() if USE_PRETRAINED_W2V_EMBEDDINGS_LAYER else None\n\n    model = InferenceCakeChatModel(\n        index_to_token=_get_index_to_token(fetch_from_s3),\n        index_to_condition=_get_index_to_condition(fetch_from_s3),\n        training_data_param=ModelParam(value=None, id=TRAIN_CORPUS_NAME),\n        validation_data_param=ModelParam(value=None, id=get_validation_data_id(get_validation_sets_names())),\n        w2v_model_param=ModelParam(value=None, id=w2v_model_id),\n        model_resolver=resolver_factory,\n        is_reverse_model=is_reverse_model,\n        reverse_model=reverse_model)\n\n    model.init_model()\n    model.resolve_model()\n    return model\n\n\ndef get_reverse_model(prediction_mode):\n    reranking_modes = [PREDICTION_MODES.beamsearch_reranking, PREDICTION_MODES.sampling_reranking]\n    return get_trained_model(is_reverse_model=True) if prediction_mode in reranking_modes else None\n'"
cakechat/dialog_model/inference_model.py,0,"b'from cakechat.dialog_model.keras_model import KerasTFModelIsolator\nfrom cakechat.dialog_model.model import CakeChatModel\n\n\nclass InferenceCakeChatModel(CakeChatModel, KerasTFModelIsolator):\n    """"""\n    Inference-aimed extension of CakeChatModel, which supports isolation of underlying Keras (Tensorflow) model\n    to fit multi-model multi-threaded run-time environments\n    """"""\n\n    def __init__(self,\n                 index_to_token,\n                 index_to_condition,\n                 training_data_param=None,\n                 validation_data_param=None,\n                 w2v_model_param=None,\n                 model_init_path=None,\n                 model_resolver=None,\n                 is_reverse_model=False,\n                 reverse_model=None):\n        KerasTFModelIsolator.__init__(self)\n\n        self.init_model = self._isolate_func(self.init_model)\n        self.resolve_model = self._isolate_func(self.resolve_model)\n        self.print_weights_summary = self._isolate_func(self.print_weights_summary)\n        self.train_model = self._isolate_func(self.train_model)\n\n        self.get_utterance_encoding = self._isolate_func(self.get_utterance_encoding)\n        self.get_thought_vectors = self._isolate_func(self.get_thought_vectors)\n        self.predict_prob = self._isolate_func(self.predict_prob)\n        self.predict_prob_by_thought_vector = self._isolate_func(self.predict_prob_by_thought_vector)\n        self.predict_prob_one_step = self._isolate_func(self.predict_prob_one_step)\n        self.predict_log_prob = self._isolate_func(self.predict_log_prob)\n        self.predict_log_prob_one_step = self._isolate_func(self.predict_log_prob_one_step)\n        self.predict_sequence_score = self._isolate_func(self.predict_sequence_score)\n        self.predict_sequence_score_by_thought_vector = self._isolate_func(\n            self.predict_sequence_score_by_thought_vector)\n\n        super(InferenceCakeChatModel, self).__init__(\n            index_to_token=index_to_token,\n            index_to_condition=index_to_condition,\n            training_data_param=training_data_param,\n            validation_data_param=validation_data_param,\n            w2v_model_param=w2v_model_param,\n            model_init_path=model_init_path,\n            model_resolver=model_resolver,\n            is_reverse_model=is_reverse_model,\n            reverse_model=reverse_model)\n'"
cakechat/dialog_model/keras_model.py,2,"b'import abc\nimport os\nimport time\nfrom datetime import timedelta\n\nfrom keras import backend as K\n\n\nfrom cakechat.dialog_model.abstract_callbacks import AbstractKerasModelCallback, ParametrizedCallback, \\\n    _KerasCallbackAdapter\nfrom cakechat.dialog_model.abstract_model import AbstractModel\nfrom cakechat.dialog_model.quality.metrics.plotters import DummyMetricsPlotter\nfrom cakechat.utils.env import is_main_horovod_worker, set_horovod_worker_random_seed\nfrom cakechat.utils.files_utils import is_non_empty_file\nfrom cakechat.utils.logger import WithLogger\n\n\nclass KerasTFModelIsolator(object):\n    def __init__(self):\n        # Use global keras (tensorflow) session config here\n        keras_session_config = K.get_session()._config\n\n        self._keras_isolated_graph = K.tf.Graph()\n        self._keras_isolated_session = K.tf.Session(graph=self._keras_isolated_graph, config=keras_session_config)\n\n    def _isolate_func(self, func):\n        def wrapper(*args, **kwargs):\n            with self._keras_isolated_graph.as_default():\n                with self._keras_isolated_session.as_default():\n                    return func(*args, **kwargs)\n\n        return wrapper\n\n\nclass EvaluateAndSaveBestIntermediateModelCallback(AbstractKerasModelCallback, WithLogger):\n    def __init__(self, model, eval_state_per_batches):\n        """"""\n        :param model: AbstractKerasModel object\n        :param eval_state_per_batches: run model evaluation each `eval_state_per_batches` steps\n        """"""\n        super(EvaluateAndSaveBestIntermediateModelCallback, self).__init__(model)\n        WithLogger.__init__(self)\n\n        self._eval_state_per_batches = eval_state_per_batches\n        self._training_start_time = None\n        self._cur_epoch_start_time = None\n\n    @property\n    def callback_params(self):\n        return {\'eval_state_per_batches\': self._eval_state_per_batches}\n\n    @property\n    def runs_only_on_main_worker(self):\n        return True\n\n    @staticmethod\n    def _get_formatted_time(seconds):\n        return str(timedelta(seconds=int(seconds)))\n\n    def _log_metrics(self, dataset_name_to_metrics):\n        for dataset_name, metrics in dataset_name_to_metrics.items():\n            for metric_name, metric_value in metrics.items():\n                self._logger.info(\'{} {} = {}\'.format(dataset_name, metric_name, metric_value))\n                self._model.metrics_plotter.plot(self._model.model_id, \'{}/{}\'.format(dataset_name, metric_name),\n                                                 metric_value)\n\n    def _eval_and_save_current_model(self, batch_num=None):\n        total_elapsed_time = time.time() - self._training_start_time\n        self._logger.info(\'Total elapsed time: {}\'.format(self._get_formatted_time(total_elapsed_time)))\n\n        if batch_num:\n            elapsed_time_per_batch = (time.time() - self._cur_epoch_start_time) / batch_num\n            self._logger.info(\'Cur batch num: {}; Train time per batch: {:.2f} seconds\'.format(\n                batch_num, elapsed_time_per_batch))\n\n        dataset_name_to_metrics = self._model._evaluate()\n        self._log_metrics(dataset_name_to_metrics)\n\n        if not os.path.exists(self._model.model_path):\n            os.makedirs(self._model.model_path)\n\n        if self._model.metrics is None or self._model._is_better_model(dataset_name_to_metrics, self._model.metrics):\n            self._logger.info(\'Obtained new best model. Saving it to {}\'.format(self._model._model_resource_path))\n            self._model._save_model(self._model._model_resource_path)\n            self._model._metrics_serializer.save_metrics(self._model._metrics_resource_path, dataset_name_to_metrics)\n            self._model._metrics = dataset_name_to_metrics\n\n        self._model._save_model(self._model._model_progress_resource_path)\n\n    def on_train_begin(self, logs=None):\n        self._logger.info(\'Start training\')\n        self._training_start_time = time.time()\n\n    def on_train_end(self, logs=None):\n        self._logger.info(\'Stop training and compute final model metrics\')\n        self._eval_and_save_current_model()\n\n    def on_batch_end(self, batch_num, logs=None):\n        if batch_num > 0 and batch_num % self._eval_state_per_batches == 0:\n            self._eval_and_save_current_model(batch_num)\n\n    def on_epoch_begin(self, epoch_num, logs=None):\n        cur_epoch_num = epoch_num + 1\n        self._logger.info(\'Starting epoch {}\'.format(cur_epoch_num))\n        self._cur_epoch_start_time = time.time()\n\n    def on_epoch_end(self, epoch_num, logs=None):\n        cur_epoch_num = epoch_num + 1\n        cur_epoch_time = time.time() - self._cur_epoch_start_time\n        self._logger.info(\'For epoch {} elapsed time: {}\'.format(cur_epoch_num,\n                                                                 self._get_formatted_time(cur_epoch_time)))\n\n\nclass AbstractKerasModel(AbstractModel, metaclass=abc.ABCMeta):\n    # Model resources default values\n    _MODEL_PROGRESS_RESOURCE_NAME = \'model.current\'\n\n    def __init__(self, metrics_plotter=None, horovod=None, training_callbacks=None, *args, **kwargs):\n        """"""\n        :param metrics_plotter: object that plots training and validation metrics (see `TensorboardMetricsPlotter`)\n        :param horovod: horovod module initialized for training on multiple GPUs. If None, uses single GPU, or CPU\n        :param training_callbacks: list of instances of `AbstractKerasModelCallback`/`ParametrizedCallback` or None.\n            In subclasses, please call `_create_essential_callbacks` to get essential callbacks, and/or put your own\n            ones in this argument.\n        """"""\n        super(AbstractKerasModel, self).__init__(*args, **kwargs)\n\n        self._metrics_plotter = metrics_plotter if metrics_plotter else DummyMetricsPlotter()\n        self._horovod = horovod\n\n        self._class_weight = None\n        self._callbacks = training_callbacks or []\n\n    @staticmethod\n    def _create_essential_callbacks(model, horovod=None, eval_state_per_batches=None):\n        """"""\n        :param model: a model object, typically `self`\n        :param horovod: if not None, adds callback for model params broadcasting between workers\n        :param eval_state_per_batches: if not None, adds callback to evaluate the model every `eval_state_per_batches`\n                batches\n        :return: a list of callbacks\n        """"""\n        callbacks = []\n\n        if horovod:\n            callbacks.append(\n                ParametrizedCallback(\n                    horovod.callbacks.BroadcastGlobalVariablesCallback(0), runs_only_on_main_worker=False))\n\n        if eval_state_per_batches:\n            callbacks.append(EvaluateAndSaveBestIntermediateModelCallback(model, eval_state_per_batches))\n\n        return callbacks\n\n    def _get_worker_callbacks(self):\n        if is_main_horovod_worker(self._horovod):\n            # all callbacks should be run on main worker\n            return self._callbacks\n\n        # but not all callbacks should be run on a not main worker\n        return [callback for callback in self._callbacks if not callback.runs_only_on_main_worker]\n\n    @staticmethod\n    def _to_keras_callbacks(callbacks):\n        """"""\n        Casts AbstractKerasModel callbacks (see `cakechat.dialog_model.callbacks`) to the keras-based ones (instances of\n            `keras.callbacks.Callback`)\n        :param callbacks:\n        :return:\n        """"""\n        keras_callbacks = []\n        for custom_callback in callbacks:\n            if isinstance(custom_callback, AbstractKerasModelCallback):\n                keras_callback = _KerasCallbackAdapter(custom_callback)\n            elif isinstance(custom_callback, ParametrizedCallback):\n                keras_callback = custom_callback.callback\n            else:\n                raise ValueError(\'Unsupported callback type: {}\'.format(type(custom_callback)))\n\n            keras_callbacks.append(keras_callback)\n\n        return keras_callbacks\n\n    def _set_class_weight(self, class_weight):\n        self._class_weight = class_weight\n\n    @property\n    @abc.abstractmethod\n    def _model_params(self):\n        pass\n\n    @property\n    def model_params(self):\n        params = {\n            \'training_callbacks\': {\n                cb.__class__.__name__: cb.callback_params for cb in self._callbacks if cb.callback_params\n            }\n        }\n        params.update(self._model_params)\n        return params\n\n    @property\n    def _model_progress_resource_path(self):\n        return os.path.join(self.model_path, self._MODEL_PROGRESS_RESOURCE_NAME)\n\n    @property\n    def model(self):\n        self.init_model()\n        return self._model\n\n    @property\n    def metrics_plotter(self):\n        return self._metrics_plotter\n\n    @abc.abstractmethod\n    def _get_training_model(self):\n        pass\n\n    @abc.abstractmethod\n    def _build_model(self):\n        pass\n\n    @abc.abstractmethod\n    def _is_better_model(self, new_metrics, old_metrics):\n        pass\n\n    @abc.abstractmethod\n    def _get_training_batch_generator(self):\n        """"""\n        :return: generator with (inputs, targets) or (inputs, targets, sample_weights) tuples.\n        The generator is expected to loop over its data indefinitely.\n        An epoch finishes when epoch_batches_num batches have been seen by the training worker.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def _get_epoch_batches_num(self):\n        pass\n\n    def _save_model(self, model_file_path):\n        self._model.save(model_file_path, overwrite=True)\n        self._logger.info(\'Saved model weights to {}\'.format(model_file_path))\n\n    def _load_model(self, fresh_model, model_file_path):\n        fresh_model.load_weights(model_file_path, by_name=True)\n        self._logger.info(\'Restored model weights from {}\'.format(model_file_path))\n        return fresh_model\n\n    def _load_model_if_exists(self):\n        if is_non_empty_file(self._model_progress_resource_path):\n            self._model = self._load_model(self._model, self._model_progress_resource_path)\n            self._metrics = self._metrics_serializer.load_metrics(self._metrics_resource_path)\n            return\n\n        self._logger.info(\'Could not find saved model at {}\\nModel will be trained from scratch.\\n\'\n                          .format(self._model_progress_resource_path))\n\n    def print_weights_summary(self):\n        summary = \'\\n\\nModel weights summary:\'\n        summary += \'\\n\\t{0:<80} {1:<20} {2:}\\n\'.format(\'layer name\', \'output shape:\', \'size:\')\n\n        weights_names = [weight.name for layer in self._model.layers for weight in layer.weights]\n        weights = self._model.get_weights()\n\n        total_network_size = 0\n        for name, weight in zip(weights_names, weights):\n            param_size = weight.nbytes / 1024 / 1024\n            summary += \'\\n\\t{0:<80} {1:20} {2:<.2f}Mb\'.format(name, str(weight.shape), param_size)\n            total_network_size += param_size\n\n        summary += \'\\n\\nTotal network size: {0:.1f} Mb\\n\'.format(total_network_size)\n        self._logger.info(summary)\n\n    def init_model(self):\n        if not self._model:\n            self._logger.info(\'Initializing NN model\')\n            self._model = self._build_model()\n            self._logger.info(\'NN model is initialized\\n\')\n            self.print_weights_summary()\n\n    def train_model(self):\n        self.init_model()\n        self._load_model_if_exists()\n\n        set_horovod_worker_random_seed(self._horovod)\n        training_batch_generator = self._get_training_batch_generator()\n\n        epoch_batches_num = self._get_epoch_batches_num()\n        workers_num = self._horovod.size() if self._horovod else 1\n\n        self._logger.info(\'Total epochs num = {}; Total batches per epochs = {}; Total workers for train = {}\'.format(\n            self.model_params[\'epochs_num\'], epoch_batches_num, workers_num))\n\n        worker_callbacks = self._get_worker_callbacks()\n        training_model = self._get_training_model()\n        training_model.fit_generator(\n            training_batch_generator,\n            steps_per_epoch=epoch_batches_num // workers_num,\n            callbacks=self._to_keras_callbacks(worker_callbacks),\n            epochs=self.model_params[\'epochs_num\'],\n            class_weight=self._class_weight,\n            verbose=0,\n            workers=0)\n\n        # reload model with the best quality\n        if is_main_horovod_worker(self._horovod):\n            self._model = self._load_model(self._model, self._model_resource_path)\n'"
cakechat/dialog_model/layers.py,0,"b'from keras.layers import K, RepeatVector, Lambda\n\n\ndef repeat_vector(inputs):\n    """"""\n    Temporary solution:\n    Use this function within a Lambda layer to get a repeated layer with a variable 1-st dimension (seq_len).\n    May be useful to further feed it to a Concatenate layer.\n\n    inputs == (layer_for_repeat, layer_for_getting_rep_num):\n        layer_for_repeat:           shape == (batch_size, vector_dim)\n        layer_for_getting_rep_num:  shape == (batch_size, seq_len, ...)\n    :return:\n        repeated layer_for_repeat, shape == (batch_size, seq_len, vector_dim)\n    """"""\n    layer_for_repeat, layer_for_getting_rep_num = inputs\n    repeated_vector = RepeatVector(\n        n=K.shape(layer_for_getting_rep_num)[1], name=\'custom_repeat_vector\')(layer_for_repeat)\n    # shape == (batch_size, seq_len, vector_dim)\n    return repeated_vector\n\n\ndef softmax_with_temperature(logits, temperature):\n    """"""\n    :param logits:      shape == (batch_size, seq_len, vocab_size), float32\n    :param temperature: shape == (batch_size, 1), float32\n    :return:\n        transformed tokens probs, shape == (batch_size, seq_len, vocab_size), float32\n    """"""\n\n    def softmax_with_temp(args):\n        logits, temperature = args\n        repeat_num = K.shape(logits)[1]\n        temperature_repeated = RepeatVector(repeat_num)(temperature)\n        # shape == (batch_size, seq_len)\n        scaled_logits = logits / temperature_repeated\n        # shape == (batch_size, seq_len, vocab_size)\n\n        # for numerical stability (e.g. for low temperatures):\n        scaled_logits = scaled_logits - K.max(scaled_logits, axis=2, keepdims=True)\n        # shape == (batch_size, seq_len, vocab_size)\n        transformed_probs = K.softmax(scaled_logits)\n        # shape == (batch_size, seq_len, vocab_size)\n        return transformed_probs\n\n    # wrap transformation in Lambda to turn the result to Keras layer\n    transformed_probs = Lambda(\n        function=softmax_with_temp,\n        mask=lambda inputs, inputs_masks: inputs_masks[0],  # function to get mask of the first input\n        name=\'softmax_with_temperature\')([logits, temperature])\n    # output shape == (batch_size, seq_len, vocab_size)\n\n    return transformed_probs\n'"
cakechat/dialog_model/model.py,6,"b'import math\nimport os\nfrom functools import partial\n\nimport numpy as np\nimport tensorflow as tf\nfrom keras import Input, Model, optimizers\nfrom keras.layers import K, Bidirectional, Embedding, Concatenate, Dense, Dropout, TimeDistributed, \\\n    Reshape, Lambda, CuDNNGRU, GRU\n\nfrom cakechat.config import HIDDEN_LAYER_DIMENSION, GRAD_CLIP, LEARNING_RATE, TRAIN_WORD_EMBEDDINGS_LAYER, \\\n    WORD_EMBEDDING_DIMENSION, DENSE_DROPOUT_RATIO, CONDITION_EMBEDDING_DIMENSION, MODEL_NAME, BASE_CORPUS_NAME, \\\n    INPUT_CONTEXT_SIZE, INPUT_SEQUENCE_LENGTH, OUTPUT_SEQUENCE_LENGTH, BATCH_SIZE, LOG_RUN_METADATA, \\\n    TENSORBOARD_LOG_DIR, EPOCHS_NUM, SHUFFLE_TRAINING_BATCHES, RANDOM_SEED, RESULTS_PATH, USE_CUDNN\nfrom cakechat.dialog_model.callbacks import CakeChatEvaluatorCallback\nfrom cakechat.dialog_model.keras_model import AbstractKerasModel\nfrom cakechat.dialog_model.layers import repeat_vector, softmax_with_temperature\nfrom cakechat.dialog_model.model_utils import get_training_batch\nfrom cakechat.dialog_model.quality.metrics.perplexity import calculate_model_mean_perplexity\nfrom cakechat.dialog_model.quality.metrics.plotters import TensorboardMetricsPlotter\nfrom cakechat.utils.data_structures import create_namedtuple_instance\nfrom cakechat.utils.logger import WithLogger\nfrom cakechat.utils.text_processing import SPECIAL_TOKENS\nfrom cakechat.utils.w2v.utils import get_token_vector\n\n\nclass CakeChatModel(AbstractKerasModel, WithLogger):\n    def __init__(self,\n                 index_to_token,\n                 index_to_condition,\n                 training_data_param,\n                 validation_data_param,\n                 w2v_model_param,\n                 model_init_path=None,\n                 model_resolver=None,\n                 model_name=MODEL_NAME,\n                 corpus_name=BASE_CORPUS_NAME,\n                 skip_token=SPECIAL_TOKENS.PAD_TOKEN,\n                 token_embedding_dim=WORD_EMBEDDING_DIMENSION,\n                 train_token_embedding=TRAIN_WORD_EMBEDDINGS_LAYER,\n                 condition_embedding_dim=CONDITION_EMBEDDING_DIMENSION,\n                 input_seq_len=INPUT_SEQUENCE_LENGTH,\n                 input_context_size=INPUT_CONTEXT_SIZE,\n                 output_seq_len=OUTPUT_SEQUENCE_LENGTH,\n                 hidden_layer_dim=HIDDEN_LAYER_DIMENSION,\n                 use_cudnn=USE_CUDNN,\n                 dense_dropout_ratio=DENSE_DROPOUT_RATIO,\n                 is_reverse_model=False,\n                 reverse_model=None,\n                 learning_rate=LEARNING_RATE,\n                 grad_clip=GRAD_CLIP,\n                 batch_size=BATCH_SIZE,\n                 epochs_num=EPOCHS_NUM,\n                 horovod=None,\n                 tensorboard_log_dir=TENSORBOARD_LOG_DIR,\n                 log_run_metadata=LOG_RUN_METADATA):\n        """"""\n        :param index_to_token: Dict with mapping: tokens indices to tokens\n        :param index_to_condition: Dict with mapping: conditions indicies to conditions values\n        :param training_data_param: Instance of ModelParam, tuple (value, id) where value is a dataset used for training\n        and id is a name this dataset\n        :param validation_data_param: Instance of ModelParam, tuple (value, id) where value is a dataset used for\n        metrics calculation and id is a concatenation of these datasets\' names\n        :param w2v_model_param: Instance of ModelParam, tuple (value, id) where value is a word2vec matrix of shape\n        (vocab_size, token_embedding_dim) with float values, used for initializing token embedding layers, and id is\n        the name of word2vec model\n        :param model_init_path: Path to a file with model\'s saved weights for layers intialization\n        :param model_resolver: Factory that takes model path and returns a file resolver object\n        :param model_name: String prefix that is prepended to automatically generated model\'s name. The prefix helps\n         distinguish the current experiment from other experiments with similar params.\n        :param corpus_name: File name of the training dataset (included into automatically generated model\'s name)\n        :param skip_token: Token to skip with masking, usually _pad_ token. Id of this token is inferred from\n        index_to_token dictionary\n        :param token_embedding_dim:  Vectors dimensionality of tokens embeddings\n        :param train_token_embedding: Bool value indicating whether to train token embeddings along with other model\'s\n        weights or keep them freezed during training\n        :param condition_embedding_dim: Vectors dimensionality of conditions embeddings\n        :param input_seq_len: Max number of tokens in the context sentences\n        :param input_context_size: Max number of sentences in the context\n        :param output_seq_len: Max number of tokens in the output sentences\n        :param hidden_layer_dim: Vectors dimensionality of hidden layers in GRU and Dense layers\n        :param dense_dropout_ratio: Float value between 0 and 1, indicating the ratio of neurons that will be randomly\n        deactivated during training to prevent model\'s overfitting\n        :param is_reverse_model: Bool value indicating the type of model:\n        False (regular model) - predicts response for the given context\n        True (reverse model) - predicts context for the given response (actually, predict the last context sentence for\n        given response and the beginning of the context) - used for calculating Maximim Mutual Information metric\n        :param reverse_model: Trained reverse model used to generate predictions in *_reranking modes\n        :param learning_rate: Learning rate of the optimization algorithm\n        :param grad_clip: Clipping parameter of the optimization algorithm, used to prevent gradient explosion\n        :param batch_size: Number of samples to be used for gradient estimation on each train step\n        :param epochs_num: Number of full dataset passes during train\n        :param horovod: Initialized horovod module used for multi-gpu training. Trains on single gpu if horovod=None\n        :param tensorboard_log_dir: Path to tensorboard logs directory\n        :param log_run_metadata: Set \'True\' to profile memory consumption and computation time on tensorboard\n        """"""\n        # Calculate batches number in each epoch.\n        # The last batch which may be smaller than batch size is included in this number\n        batches_num_per_epoch = math.ceil(training_data_param.value.x.shape[0] / batch_size) \\\n            if training_data_param.value else None\n\n        # Create callbacks\n        callbacks = self._create_essential_callbacks(self, horovod)\n        callbacks.extend([\n            # Custom callback for metrics calculation\n            CakeChatEvaluatorCallback(self, index_to_token, batch_size, batches_num_per_epoch)\n        ])\n\n        super(CakeChatModel, self).__init__(\n            model_resolver_factory=model_resolver,\n            metrics_plotter=TensorboardMetricsPlotter(tensorboard_log_dir),\n            horovod=horovod,\n            training_callbacks=callbacks)\n        WithLogger.__init__(self)\n\n        self._model_name = \'reverse_{}\'.format(model_name) if is_reverse_model else model_name\n        self._rnn_class = CuDNNGRU if use_cudnn else partial(GRU, reset_after=True)\n\n        # tokens params\n        self._index_to_token = index_to_token\n        self._token_to_index = {v: k for k, v in index_to_token.items()}\n        self._vocab_size = len(self._index_to_token)\n        self._skip_token_id = self._token_to_index[skip_token]\n\n        self._token_embedding_dim = token_embedding_dim\n        self._train_token_embedding = train_token_embedding\n        self._W_init_embedding = \\\n            self._build_embedding_matrix(self._token_to_index, w2v_model_param.value, token_embedding_dim) \\\n                if w2v_model_param.value else None\n\n        # condition params\n        self._index_to_condition = index_to_condition\n        self._condition_to_index = {v: k for k, v in index_to_condition.items()}\n        self._condition_embedding_dim = condition_embedding_dim\n\n        # data params\n        self._training_data = training_data_param.value\n        self._validation_data = validation_data_param.value\n\n        # train params\n        self._batches_num_per_epoch = batches_num_per_epoch\n        self._model_init_path = model_init_path\n        self._horovod = horovod\n\n        self._optimizer = optimizers.Adadelta(lr=learning_rate, clipvalue=grad_clip)\n        if self._horovod:\n            self._optimizer = horovod.DistributedOptimizer(self._optimizer)\n\n        # gather model\'s params that define the experiment setting\n        self._params = create_namedtuple_instance(\n            name=\'Params\',\n            corpus_name=corpus_name,\n            input_context_size=input_context_size,\n            input_seq_len=input_seq_len,\n            output_seq_len=output_seq_len,\n            token_embedding_dim=token_embedding_dim,\n            train_batch_size=batch_size,\n            hidden_layer_dim=hidden_layer_dim,\n            w2v_model=w2v_model_param.id,\n            is_reverse_model=is_reverse_model,\n            dense_dropout_ratio=dense_dropout_ratio,\n            voc_size=len(self._token_to_index),\n            training_data=training_data_param.id,\n            validation_data=validation_data_param.id,\n            epochs_num=epochs_num,\n            optimizer=self._optimizer.get_config())\n\n        # profiling params\n        self._run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) if log_run_metadata else None\n        self._run_metadata = tf.RunMetadata() if log_run_metadata else None\n\n        # parts of computational graph\n        self._models = None\n\n        # get trained reverse model used for inference\n        self._reverse_model = reverse_model\n\n    @property\n    def model_name(self):\n        return self._model_name\n\n    @property\n    def run_metadata(self):\n        return self._run_metadata\n\n    @property\n    def token_to_index(self):\n        return self._token_to_index\n\n    @property\n    def index_to_token(self):\n        return self._index_to_token\n\n    @property\n    def condition_to_index(self):\n        return self._condition_to_index\n\n    @property\n    def index_to_condition(self):\n        return self._index_to_condition\n\n    @property\n    def vocab_size(self):\n        return self._vocab_size\n\n    @property\n    def skip_token_id(self):\n        return self._skip_token_id\n\n    @property\n    def hidden_layer_dim(self):\n        return self._params.hidden_layer_dim\n\n    @property\n    def decoder_depth(self):\n        return self._decoder_depth\n\n    @property\n    def is_reverse_model(self):\n        return self._params.is_reverse_model\n\n    @property\n    def reverse_model(self):\n        return self._reverse_model\n\n    @property\n    def _model_dir(self):\n        return os.path.join(RESULTS_PATH, \'nn_models\')\n\n    @property\n    def _model_params(self):\n        return self._params._asdict()\n\n    @property\n    def _model_progress_resource_path(self):\n        return os.path.join(self.model_path, self._MODEL_PROGRESS_RESOURCE_NAME)\n\n    def _build_model(self):\n        # embeddings\n        x_tokens_emb_model = self._tokens_embedding_model(name=\'x_token_embedding\')\n        y_tokens_emb_model = self._tokens_embedding_model(name=\'y_token_embedding\')\n        with tf.variable_scope(\'condition_embedding_scope\', reuse=True):\n            condition_emb_model = self._condition_embedding_model()\n\n        # encoding\n        with tf.variable_scope(\'utterance_scope\', reuse=True):\n            utterance_enc_model = self._utterance_encoder(x_tokens_emb_model)\n        with tf.variable_scope(\'encoder_scope\', reuse=True):\n            context_enc_model = self._context_encoder(utterance_enc_model)\n\n        # decoding\n        with tf.variable_scope(\'decoder_scope\', reuse=True):\n            decoder_training_model, decoder_model = self._decoder(y_tokens_emb_model, condition_emb_model)\n\n        # seq2seq\n        seq2seq_training_model, seq2seq_model = self._seq2seq(context_enc_model, decoder_training_model, decoder_model)\n\n        self._models = dict(\n            utterance_encoder=utterance_enc_model,\n            context_encoder=context_enc_model,\n            decoder=decoder_model,\n            seq2seq=seq2seq_model,\n            seq2seq_training=seq2seq_training_model)\n\n        return self._models[\'seq2seq_training\']\n\n    def _get_training_model(self):\n        def sparse_categorical_crossentropy_logits(y_true, y_pred):\n            return K.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n\n        self._logger.info(\'Compiling seq2seq for train...\')\n\n        self._models[\'seq2seq_training\'].compile(\n            loss=sparse_categorical_crossentropy_logits,\n            optimizer=self._optimizer,\n            options=self._run_options,\n            run_metadata=self._run_metadata)\n\n        return self._models[\'seq2seq_training\']\n\n    def _tokens_embedding_model(self, name=\'token_embedding\'):\n        self._logger.info(\'Building tokens_embedding_model...\')\n\n        tokens_ids = Input(shape=(None, ), dtype=\'int32\', name=name + \'_input\')\n        # output shape == (batch_size, seq_len)\n\n        tokens_embeddings = Embedding(\n            input_dim=self._vocab_size,\n            output_dim=self._token_embedding_dim,\n            trainable=self._train_token_embedding,\n            name=name,\n            weights=None if self._W_init_embedding is None else [self._W_init_embedding])(tokens_ids)\n        # output shape == (batch_size, seq_len, token_emb_size)\n\n        return Model(tokens_ids, tokens_embeddings, name=name + \'_model\')\n\n    def _condition_embedding_model(self):\n        self._logger.info(\'Building condition_embedding_model...\')\n\n        condition_id = Input(shape=(1, ), dtype=\'int32\', name=\'condition_input\')\n        # output shape == (batch_size, 1)\n\n        condition_emb = Embedding(\n            input_dim=len(self._condition_to_index),\n            output_dim=self._condition_embedding_dim,\n            name=\'condition_embedding\')(condition_id)\n        # output shape == (batch_size, 1, condition_emb_size)\n\n        condition_emb_reshaped = Reshape(\n            target_shape=(self._condition_embedding_dim, ), name=\'condition_embedding_reshaped\')(condition_emb)\n        # output shape == (batch_size, condition_emb_size)\n\n        return Model(condition_id, condition_emb_reshaped, name=\'condition_emb_model\')\n\n    def _utterance_encoder(self, tokens_emb_model):\n        self._logger.info(\'Building utterance_encoder...\')\n\n        tokens_ids = tokens_emb_model.inputs[0]\n        # output shape == (batch_size, seq_len)\n        tokens_embeddings = tokens_emb_model(tokens_ids)\n        # output shape == (batch_size, seq_len, token_emb_size)\n\n        bidir_enc = Bidirectional(\n            layer=self._rnn_class(\n                units=self._params.hidden_layer_dim, return_sequences=True, name=\'encoder\'),\n            name=\'bidir_utterance_encoder\')(tokens_embeddings)\n        # output shape == (batch_size, seq_len, 2 * hidden_layer_dim)\n\n        utterance_encoding = self._rnn_class(\n            units=self._params.hidden_layer_dim, name=\'utterance_encoder_final\')(bidir_enc)\n        # output shape == (batch_size, hidden_layer_dim)\n\n        return Model(tokens_ids, utterance_encoding, name=\'utterance_encoder_model\')\n\n    def _context_encoder(self, utterance_enc_model):\n        self._logger.info(\'Building context_encoder...\')\n        context_tokens_ids = Input(\n            shape=(self._params.input_context_size, self._params.input_seq_len),\n            dtype=\'int32\',\n            name=\'context_tokens_ids\')\n        # output shape == (batch_size, context_size, seq_len)\n\n        context_utterance_embeddings = TimeDistributed(\n            layer=utterance_enc_model, input_shape=(self._params.input_context_size,\n                                                    self._params.input_seq_len))(context_tokens_ids)\n        # output shape == (batch_size, context_size, utterance_encoding_dim)\n\n        context_encoding = self._rnn_class(\n            units=self._params.hidden_layer_dim, name=\'context_encoder\')(context_utterance_embeddings)\n        # output shape == (batch_size, hidden_layer_dim)\n\n        return Model(context_tokens_ids, context_encoding, name=\'encoder_model\')\n\n    def _decoder(self, tokens_emb_model, condition_emb_model):\n        self._logger.info(\'Building decoder...\')\n\n        thought_vector = Input(shape=(self._params.hidden_layer_dim, ), dtype=K.floatx(), name=\'dec_thought_vector\')\n        # output shape == (batch_size, hidden_layer_dim)\n        response_tokens_ids = tokens_emb_model.inputs[0]\n        # output shape == (batch_size, seq_len)\n        condition_id = condition_emb_model.inputs[0]\n        # output shape == (batch_size, 1)\n        temperature = Input(shape=(1, ), dtype=\'float32\', name=\'dec_temperature\')\n        # output shape == (batch_size, 1)\n\n        # hardcode decoder\'s depth here: the general solution for any number of stacked rnn layers hs num is too bulky\n        # and we don\'t need it, so keep it simple, stupid\n        self._decoder_depth = 2\n        # keep inputs for rnn decoder hidden states globally accessible for all model layers that are using them\n        # otherwise you may encounter a keras bug that affects rnn stateful models\n        # related discussion: https://github.com/keras-team/keras/issues/9385#issuecomment-365464721\n        self._dec_hs_input = Input(\n            shape=(self._decoder_depth, self._params.hidden_layer_dim), dtype=K.floatx(), name=\'dec_hs\')\n        # shape == (batch_size, dec_depth, hidden_layer_dim)\n\n        response_tokens_embeddings = tokens_emb_model(response_tokens_ids)\n        # output shape == (batch_size, seq_len, token_emb_size)\n        condition_embedding = condition_emb_model(condition_id)\n        # output shape == (batch_size, cond_emb_size)\n        conditioned_tv = Concatenate(name=\'conditioned_tv\')([thought_vector, condition_embedding])\n        # output shape == (batch_size, hidden_layer_dim + cond_emb_size)\n\n        # Temporary solution:\n        # use a custom lambda function for layer repeating and manually set output_shape\n        # otherwise the consequent Concatenate layer won\'t work\n        repeated_conditioned_tv = Lambda(\n            function=repeat_vector,\n            mask=lambda inputs, inputs_masks: inputs_masks[0],  # function to get mask of the first input\n            output_shape=(None, self._params.hidden_layer_dim + self._condition_embedding_dim),\n            name=\'repeated_conditioned_tv\')([conditioned_tv, response_tokens_ids])\n        # output shape == (batch_size, seq_len, hidden_layer_dim + cond_emb_size)\n\n        decoder_input = Concatenate(name=\'concat_emb_cond_tv\')([response_tokens_embeddings, repeated_conditioned_tv])\n        # output shape == (batch_size, seq_len, token_emb_size + hidden_layer_dim + cond_emb_size)\n\n        # unpack hidden states to tensors\n        dec_hs_0 = Lambda(\n            function=lambda x: x[:, 0, :], output_shape=(self._params.hidden_layer_dim, ),\n            name=\'dec_hs_0\')(self._dec_hs_input)\n\n        dec_hs_1 = Lambda(\n            function=lambda x: x[:, 1, :], output_shape=(self._params.hidden_layer_dim, ),\n            name=\'dec_hs_1\')(self._dec_hs_input)\n\n        outputs_seq_0, updated_hs_seq_0 = self._rnn_class(\n            units=self._params.hidden_layer_dim, return_sequences=True, return_state=True, name=\'decoder_0\')\\\n            (decoder_input, initial_state=dec_hs_0)\n        # outputs_seq_0 and updated_hs_seq_0 shapes == (batch_size, seq_len, hidden_layer_dim)\n\n        outputs_seq_1, updated_hs_seq_1 = self._rnn_class(\n            units=self._params.hidden_layer_dim, return_sequences=True, return_state=True, name=\'decoder_1\')\\\n            (outputs_seq_0, initial_state=dec_hs_1)\n        # outputs_seq_1 and updated_hs_seq_1 shapes == (batch_size, seq_len, hidden_layer_dim)\n\n        outputs_dropout = Dropout(rate=self._params.dense_dropout_ratio)(outputs_seq_1)\n        # output shape == (batch_size, seq_len, hidden_layer_dim)\n        tokens_logits = Dense(self._vocab_size)(outputs_dropout)\n        # output shape == (batch_size, seq_len, vocab_size)\n        tokens_probs = softmax_with_temperature(tokens_logits, temperature)\n        # output shape == (batch_size, seq_len, vocab_size)\n\n        # pack updated hidden states into one tensor\n        updated_hs = Concatenate(\n            axis=1, name=\'updated_hs\')([\n                Reshape((1, self._params.hidden_layer_dim))(updated_hs_seq_0),\n                Reshape((1, self._params.hidden_layer_dim))(updated_hs_seq_1)\n            ])\n\n        decoder_training_model = Model(\n            inputs=[thought_vector, response_tokens_ids, condition_id, self._dec_hs_input],\n            outputs=[tokens_logits],\n            name=\'decoder_training_model\')\n\n        decoder_model = Model(\n            inputs=[thought_vector, response_tokens_ids, condition_id, self._dec_hs_input, temperature],\n            outputs=[tokens_probs, updated_hs],\n            name=\'decoder_model\')\n\n        return decoder_training_model, decoder_model\n\n    def _seq2seq(self, context_encoder, decoder_training, decoder):\n        self._logger.info(\'Building seq2seq...\')\n\n        context_tokens_ids = context_encoder.inputs[0]\n        # output shape == (batch_size, context_size, input_seq_len)\n        response_tokens_ids = decoder.inputs[1]\n        # output shape == (batch_size, output_seq_len - 1)\n        condition_id = decoder.inputs[2]\n        # output shape == (batch_size, 1)\n        temperature = decoder.inputs[4]\n        # output shape == (batch_size, 1)\n\n        context_encoding = context_encoder(inputs=[context_tokens_ids])\n        # output shape == (batch_size, hidden_layer_dim)\n\n        tokens_logits = decoder_training(\n            inputs=[context_encoding, response_tokens_ids, condition_id, self._dec_hs_input])\n        # tokens_probs shape == (batch_size, output_seq_len - 1, vocab_size)\n\n        tokens_probs, _ = decoder(\n            inputs=[context_encoding, response_tokens_ids, condition_id, self._dec_hs_input, temperature])\n        # tokens_probs shape == (batch_size, output_seq_len - 1, vocab_size)\n\n        training_model = Model(\n            inputs=[context_tokens_ids, response_tokens_ids, condition_id, self._dec_hs_input],\n            outputs=[tokens_logits],\n            name=\'seq2seq_training_model\')\n\n        model = Model(\n            inputs=[context_tokens_ids, response_tokens_ids, condition_id, self._dec_hs_input, temperature],\n            outputs=[tokens_probs],\n            name=\'seq2seq_model\')\n\n        return training_model, model\n\n    def _get_training_batch_generator(self):\n        # set unique random seed for different workers to correctly process batches in multi-gpu training\n        horovod_seed = self._horovod.rank() if self._horovod else 0\n        epoch_id = 0\n\n        while True:  # inifinite batches generator\n            epoch_id += 1\n\n            for train_batch in get_training_batch(\n                    self._training_data,\n                    self._params.train_batch_size,\n                    random_permute=SHUFFLE_TRAINING_BATCHES,\n                    random_seed=RANDOM_SEED * epoch_id + horovod_seed):\n\n                context_tokens_ids, response_tokens_ids, condition_id = train_batch\n                # response tokens are wraped with _start_ and _end_ tokens\n                # output shape == (batch_size, seq_len)\n\n                # get input response ids by removing last sequence token (_end_)\n                input_response_tokens_ids = response_tokens_ids[:, :-1]\n                # output shape == (batch_size, seq_len - 1)\n\n                # get target response ids by removing the first (_start_) token of the sequence\n                target_response_tokens_ids = response_tokens_ids[:, 1:]\n                # output shape == (batch_size, seq_len - 1)\n\n                # workaround for using sparse_categorical_crossentropy loss\n                # see https://github.com/tensorflow/tensorflow/issues/17150#issuecomment-399776510\n                target_response_tokens_ids = np.expand_dims(target_response_tokens_ids, axis=-1)\n                # output shape == (batch_size, seq_len - 1, 1)\n\n                init_dec_hs = np.zeros(\n                    shape=(context_tokens_ids.shape[0], self._decoder_depth, self._params.hidden_layer_dim),\n                    dtype=K.floatx())\n\n                yield [context_tokens_ids, input_response_tokens_ids, condition_id,\n                       init_dec_hs], target_response_tokens_ids\n\n    def _get_epoch_batches_num(self):\n        return self._batches_num_per_epoch\n\n    def get_utterance_encoding(self, utterance_tokens_ids):\n        """"""\n        :param utterance_tokens_ids:   shape == (batch_size, seq_len), int32\n        :return: utterance_encoding    shape == (batch_size, hidden_layer_dim), float32\n        """"""\n        return self._models[\'utterance_encoder\'](utterance_tokens_ids)\n\n    def get_thought_vectors(self, context_tokens_ids):\n        """"""\n        :param context_tokens_ids,   shape == (batch_size, context_size, seq_len), int32\n        :return: context_encoding,   shape == (batch_size, hidden_layer_dim), float32\n        """"""\n        return self._models[\'context_encoder\'].predict(context_tokens_ids)\n\n    def predict_prob(self, context_tokens_ids, response_tokens_ids, condition_id, temperature=1.0):\n        """"""\n        :param context_tokens_ids:      shape == (batch_size, context_size, seq_len), int32\n        :param response_tokens_ids:     shape == (batch_size, seq_len), int32\n        :param condition_id:            shape == (batch_size, 1), int32\n        :param temperature:             float32\n        :return:\n            tokens_probs:               shape == (batch_size, seq_len, vocab_size), float32\n        """"""\n        # remove last token, but keep first token to match seq2seq decoder input\'s shape\n        response_tokens_ids = response_tokens_ids[:, :-1]\n        # shape == (batch_size, seq_len - 1)\n\n        init_dec_hs = np.zeros(\n            shape=(context_tokens_ids.shape[0], self._decoder_depth, self._params.hidden_layer_dim), dtype=K.floatx())\n        # shape == (batch_size, decoder_depth, hidden_layer_dim)\n\n        temperature = np.full_like(condition_id, temperature, dtype=np.float32)\n        # shape == (batch_size, 1)\n\n        tokens_probs = self._models[\'seq2seq\'].predict(\n            [context_tokens_ids, response_tokens_ids, condition_id, init_dec_hs, temperature])\n        # shape == (batch_size, seq_len - 1, vocab_size)\n        return tokens_probs\n\n    def predict_prob_by_thought_vector(self, thought_vector, response_tokens_ids, condition_id, temperature=1.0):\n        """"""\n        :param thought_vector:          shape == (batch_size, hidden_layer_dim), float32\n        :param response_tokens_ids:     shape == (batch_size, seq_len), int32\n        :param condition_id:            shape == (batch_size, 1), int32\n        :param temperature:             float32\n        :return:\n            tokens_probs:               shape == (batch_size, seq_len, vocab_size), float32\n        """"""\n        # remove last token, but keep first token to match seq2seq decoder input\'s shape\n        response_tokens_ids = response_tokens_ids[:, :-1]\n        # output shape == (batch_size, seq_len - 1)\n\n        init_dec_hs = \\\n            np.zeros((thought_vector.shape[0], self._decoder_depth, self._params.hidden_layer_dim), dtype=K.floatx())\n        # shape == (batch_size, decoder_depth, hidden_layer_dim)\n\n        temperature = np.full_like(condition_id, temperature, dtype=np.float32)\n        # shape == (batch_size, 1)\n\n        tokens_probs, _ = self._models[\'decoder\'].predict(\n            [thought_vector, response_tokens_ids, condition_id, init_dec_hs, temperature])\n        # shape == (batch_size, seq_len - 1, vocab_size)\n        return tokens_probs\n\n    def predict_prob_one_step(self, thought_vector, prev_hidden_states, prev_tokens_id, condition_id, temperature=1.0):\n        """"""\n        :param thought_vector:          shape == (batch_size, hidden_layer_dim), float32\n        :param prev_hidden_states:      shape == (batch_size, 2, hidden_layer_dim), float32\n        :param prev_tokens_id:          shape == (batch_size, 1), int32\n        :param condition_id:            shape == (batch_size, 1), int32\n        :param temperature:             float32\n        :return:\n            updated_hidden_states:      shape == (batch_size, 2, hidden_layer_dim), float32\n            transformed_token_prob:     shape == (batch_size, vocab_size), float32\n        """"""\n        temperature = np.full_like(prev_tokens_id, temperature, dtype=np.float32)\n        # shape == (batch_size, 1)\n\n        token_prob, updated_hidden_states = self._models[\'decoder\'].predict(\n            [thought_vector, prev_tokens_id, condition_id, prev_hidden_states, temperature])\n        return updated_hidden_states, token_prob\n\n    def predict_log_prob(self, context_tokens_ids, response_tokens_ids, condition_id, temperature=1.0):\n        """"""\n        :param context_tokens_ids:      shape == (batch_size, context_size, seq_len), int32\n        :param response_tokens_ids:     shape == (batch_size, seq_len), int32\n        :param condition_id:            shape == (batch_size, 1), int32\n        :param temperature:             float32\n        :return:\n            tokens_probs:               shape == (batch_size, seq_len, vocab_size), float32\n        """"""\n        tokens_probs = self.predict_prob(context_tokens_ids, response_tokens_ids, condition_id, temperature)\n\n        tokens_log_probs = np.log(tokens_probs)\n        return tokens_log_probs\n\n    def predict_log_prob_one_step(self,\n                                  thought_vector,\n                                  prev_hidden_states,\n                                  prev_tokens_id,\n                                  condition_id,\n                                  temperature=1.0):\n        """"""\n        :param thought_vector:          shape == (batch_size, hidden_layer_dim), float32\n        :param prev_hidden_states:      shape == (batch_size, 2 * hidden_layer_dim), float32\n        :param prev_tokens_id:          shape == (batch_size, 1), int32\n        :param condition_id:            shape == (batch_size, 1), int32\n        :param temperature:             float32\n        :return:\n            updated_hidden_states:      shape == (batch_size, 2 * hidden_layer_dim), float32\n            transformed_token_prob:     shape == (batch_size, vocab_size), float32\n        """"""\n        updated_hidden_states, token_prob = self.predict_prob_one_step(thought_vector, prev_hidden_states,\n                                                                       prev_tokens_id, condition_id, temperature)\n\n        token_log_prob = np.log(token_prob)\n        return updated_hidden_states, token_log_prob\n\n    def _compute_sequence_score(self, tokens_ids, tokens_probs):\n        """"""\n        :param tokens_ids:      shape == (batch_size, seq_len), int32\n        :param tokens_probs:    shape == (batch_size, seq_len - 1, vocab_size), float32\n        :return:\n        """"""\n        mask = tokens_ids != self._skip_token_id\n        # shape == (batch_size, seq_len)\n\n        # All shapes are symbolic and are evaluated on run-time only after input tensors are supplied\n        batch_size, truncated_seq_len, vocab_size = tokens_probs.shape\n        total_tokens_num = batch_size * truncated_seq_len\n\n        # We need to reshape here for effective slicing without any loops or scans\n        probs_long = tokens_probs.reshape((total_tokens_num, vocab_size))\n        # shape == (batch_size * (seq_len - 1), vocab_size), float32\n\n        # Do not use first tokens for likelihood computation:\n        # these are _start_ tokens, we don\'t have probabilities for them\n        output_ids = tokens_ids[:, 1:]\n        # shape == (batch_size, seq_len - 1)\n        mask = mask[:, 1:]\n        # shape == (batch_size, seq_len - 1)\n\n        token_ids_flattened = output_ids.reshape((total_tokens_num, ))\n        # shape == (batch_size * (seq_len - 1))\n\n        # Select probabilities of only observed tokens and reshape back\n        observed_tokens_probs = probs_long[np.arange(total_tokens_num), token_ids_flattened]\n        # shape == (batch_size * (seq_len - 1), )\n        observed_tokens_log_probs = np.log(observed_tokens_probs)\n        # shape == (batch_size * (seq_len - 1), )\n        nonpad_observed_tokens_log_probs = observed_tokens_log_probs.reshape((batch_size, truncated_seq_len)) * mask\n        # shape == (batch_size, seq_len - 1)\n\n        batch_scores = nonpad_observed_tokens_log_probs.sum(axis=1)\n        # shape == (batch_size, )\n        return batch_scores\n\n    def predict_sequence_score(self, context_tokens_ids, response_tokens_ids, condition_id):\n        """"""\n        :param context_tokens_ids:      shape == (batch_size, context_size, seq_len), int32\n        :param response_tokens_ids:     shape == (batch_size, seq_len), int32\n        :param condition_id:            shape == (batch_size, 1), int32\n        :return:\n            sequence_score:             shape == (batch_size, 1), float32\n        """"""\n        response_tokens_probs = self.predict_prob(context_tokens_ids, response_tokens_ids, condition_id)\n        # output shape == (batch_size, seq_len - 1, vocab_size)\n\n        return self._compute_sequence_score(response_tokens_ids, response_tokens_probs)\n\n    def predict_sequence_score_by_thought_vector(self, thought_vector, response_tokens_ids, condition_id):\n        """"""\n        :param thought_vector:          shape == (batch_size, hidden_layer_dim), float32\n        :param response_tokens_ids:     shape == (batch_size, seq_len), int32\n        :param condition_id:            shape == (batch_size, 1), int32\n        :return:\n            sequence_score:             shape == (batch_size, 1), float32\n        """"""\n        response_tokens_probs = self.predict_prob_by_thought_vector(thought_vector, response_tokens_ids, condition_id)\n        # output shape == (batch_size, seq_len - 1, vocab_size)\n\n        return self._compute_sequence_score(response_tokens_ids, response_tokens_probs)\n\n    def _evaluate(self):\n        self._logger.info(\'Evaluating model\\\'s perplexity...\')\n        metrics = {}\n\n        for dataset_name, dataset in self._validation_data.items():\n            perplexity = calculate_model_mean_perplexity(self, dataset)\n            metrics[dataset_name] = {\'perplexity\': perplexity}\n\n        return metrics\n\n    @staticmethod\n    def _build_embedding_matrix(token_to_index, w2v_model, embedding_dim):\n        embedding_matrix = np.zeros((len(token_to_index), embedding_dim))\n        for token, index in token_to_index.items():\n            embedding_matrix[index] = get_token_vector(token, w2v_model, embedding_dim)\n\n        return embedding_matrix\n\n    @staticmethod\n    def _get_metric_mean(metrics, metric_name):\n        return np.mean([metrics[dataset_name][metric_name] for dataset_name in metrics])\n\n    def _is_better_model(self, new_metrics, old_metrics):\n        return self._get_metric_mean(new_metrics, metric_name=\'perplexity\') < \\\n               self._get_metric_mean(old_metrics, metric_name=\'perplexity\')\n\n    def _load_model_if_exists(self):\n        if self._model_init_path:\n            self._model = self._load_model(self._model, self._model_init_path)\n            return\n\n        # proceed with usual process of weights loading if no _model_init_path is passed\n        super(CakeChatModel, self)._load_model_if_exists()\n'"
cakechat/dialog_model/model_utils.py,0,"b'from itertools import islice\n\nimport numpy as np\n\nfrom cakechat.config import INPUT_CONTEXT_SIZE, INPUT_SEQUENCE_LENGTH, DEFAULT_CONDITION, OUTPUT_SEQUENCE_LENGTH, \\\n    AUTOENCODER_MODE, RANDOM_SEED, INTX\nfrom cakechat.utils.data_types import Dataset\nfrom cakechat.utils.logger import get_logger\nfrom cakechat.utils.tee_file import file_buffered_tee\nfrom cakechat.utils.text_processing import SPECIAL_TOKENS\n\n_logger = get_logger(__name__)\n\n\nclass ModelLoaderException(Exception):\n    pass\n\n\ndef transform_conditions_to_ids(conditions, condition_to_index, n_dialogs):\n    condition_ids_iterator = map(\n        lambda condition: condition_to_index.get(condition, condition_to_index[DEFAULT_CONDITION]), conditions)\n    condition_ids = np.full(n_dialogs, condition_to_index[DEFAULT_CONDITION], dtype=INTX)\n    # shape == (n_dialogs, )\n    for sample_idx, condition_id in enumerate(condition_ids_iterator):\n        condition_ids[sample_idx] = condition_id\n\n    # shape == (n_dialogs, 1)\n    return condition_ids\n\n\ndef lines_to_context(tokenized_lines):\n    for line in tokenized_lines:\n        yield [line]\n\n\ndef transform_contexts_to_token_ids(tokenized_contexts,\n                                    token_to_index,\n                                    max_line_len,\n                                    max_context_len=1,\n                                    max_contexts_num=None,\n                                    add_start_end=False):\n    """"""\n    Transforms contexts of lines of text to matrix of indices of tokens to be used in training/predicting.\n    Uses only first max_lines_num lines of tokenized_lines. Also clips each line to max_line_len tokens.\n    if length of a line is less that max_line_len, it\'s padded with token_to_index[PAD_TOKEN].\n\n    :param tokenized_contexts: iterable of lists (contexts) of lists (utterances) of tokens to transform to ids\n    :param token_to_index: dict that maps each token to its id\n    :param max_line_len: maximum number of tokens in a line\n    :param max_context_len: maximum context length\n    :param max_contexts_num: maximum number of contexts\n    :param add_start_end: add start/end tokens to sequence\n    :return: X -- numpy array, dtype=INTX, shape = (max_lines_num, max_context_len, max_line_len).\n    """"""\n\n    if max_contexts_num is None:\n        if not isinstance(tokenized_contexts, list):\n            raise TypeError(\'tokenized_lines should has list type if max_lines_num is not specified\')\n        max_contexts_num = len(tokenized_contexts)\n\n    X = np.full((max_contexts_num, max_context_len, max_line_len), token_to_index[SPECIAL_TOKENS.PAD_TOKEN], dtype=INTX)\n\n    for context_idx, context in enumerate(tokenized_contexts):\n        if context_idx >= max_contexts_num:\n            break\n\n        # take last max_content_len utterances\n        context = context[-max_context_len:]\n\n        # fill utterances to the end of context, keep first empty utterances padded.\n        utterance_offset = max_context_len - len(context)\n        for utterance_idx, utterance in enumerate(context):\n            if add_start_end:\n                utterance = [SPECIAL_TOKENS.START_TOKEN] + utterance + [SPECIAL_TOKENS.EOS_TOKEN]\n\n            for token_idx, token in enumerate(utterance[:max_line_len]):\n                X[context_idx, utterance_offset + utterance_idx, token_idx] = token_to_index[token] \\\n                    if token in token_to_index else token_to_index[SPECIAL_TOKENS.UNKNOWN_TOKEN]\n\n    return X\n\n\ndef transform_lines_to_token_ids(tokenized_lines, token_to_index, max_line_len, max_lines_num=None,\n                                 add_start_end=False):\n    """"""\n    Transforms lines of text to matrix of indices of tokens to be used in training/predicting.\n    Uses only first max_lines_num lines of tokenized_lines. Also clips each line to max_line_len tokens.\n    if length of a line is less that max_line_len, it\'s padded with token_to_index[PAD_TOKEN].\n\n    :param tokenized_lines: iterable of lists (utterances) of tokens to transform to ids\n    :param token_to_index: dict that maps each token to its id\n    :param max_line_len: maximum number of tokens in a lineh\n    :param max_lines_num: maximum number of lines\n    :param add_start_end: add start/end tokens to sequence\n    :return: X -- numpy array, dtype=INTX, shape = (max_lines_num, max_line_len).\n    """"""\n\n    if max_lines_num is None:\n        if not isinstance(tokenized_lines, list):\n            raise TypeError(\'tokenized_lines should has list type if max_lines_num is not specified\')\n        max_lines_num = len(tokenized_lines)\n\n    X = np.full((max_lines_num, max_line_len), token_to_index[SPECIAL_TOKENS.PAD_TOKEN], dtype=INTX)\n\n    for line_idx, line in enumerate(tokenized_lines):\n        if line_idx >= max_lines_num:\n            break\n\n        if add_start_end:\n            line = [SPECIAL_TOKENS.START_TOKEN] + line + [SPECIAL_TOKENS.EOS_TOKEN]\n\n        for token_idx, token in enumerate(line[:max_line_len]):\n            X[line_idx, token_idx] = token_to_index[token] \\\n                if token in token_to_index else token_to_index[SPECIAL_TOKENS.UNKNOWN_TOKEN]\n\n    return X\n\n\ndef transform_token_ids_to_sentences(y_ids, index_to_token):\n    """"""\n    Transforms batch of token ids into list of joined strings (sentences)\n    Transformation of each sentence ends when the end_token occurred.\n    Skips start tokens.\n\n    :param y_ids: numpy array of integers, shape (lines_num, tokens_num), represents token ids\n    :param index_to_token: dictionary to be used for transforming\n    :return: list of strings, list length = lines_num\n    """"""\n    n_responses, n_tokens = y_ids.shape\n\n    responses = []\n    for resp_idx in range(n_responses):\n        response_tokens = []\n        for token_idx in range(n_tokens):\n            token_to_add = index_to_token[y_ids[resp_idx, token_idx]]\n\n            if token_to_add in [SPECIAL_TOKENS.EOS_TOKEN, SPECIAL_TOKENS.PAD_TOKEN]:\n                break\n            if token_to_add == SPECIAL_TOKENS.START_TOKEN:\n                continue\n            response_tokens.append(token_to_add)\n\n        response_str = \' \'.join(response_tokens)\n        responses.append(response_str)\n    return responses\n\n\ndef transform_context_token_ids_to_sentences(x_ids, index_to_token):\n    """"""\n    Transforms batch of token ids into list of joined strings (sentences)\n    Transformation of each sentence ends when the end_token occurred.\n    Skips start tokens.\n\n    :param x_ids: context token ids, numpy array of shape (batch_size, context_len, tokens_num)\n    :param index_to_token:\n    :return:\n    """"""\n    n_samples, n_contexts, n_tokens = x_ids.shape\n\n    samples = []\n    for sample_idx in range(n_samples):\n        context_samples = []\n        for cont_idx in range(n_contexts):\n            sample_tokens = []\n            for token_idx in range(n_tokens):\n                token_to_add = index_to_token[x_ids[sample_idx, cont_idx, token_idx]]\n\n                if token_to_add == SPECIAL_TOKENS.EOS_TOKEN or token_to_add == SPECIAL_TOKENS.PAD_TOKEN:\n                    break\n                if token_to_add == SPECIAL_TOKENS.START_TOKEN:\n                    continue\n\n                sample_tokens.append(token_to_add)\n\n            sample_str = \' \'.join(sample_tokens)\n            context_samples.append(sample_str)\n        samples.append(\' / \'.join(context_samples))\n    return samples\n\n\ndef _get_token_vector(token, w2v_model):\n    if token in w2v_model.wv.vocab:\n        return np.array(w2v_model[token])\n    elif token == SPECIAL_TOKENS.PAD_TOKEN:\n        return np.zeros(w2v_model.vector_size)\n    else:\n        _logger.warning(\'Can\\\'t find token [{}] in w2v dict\'.format(token))\n        if not hasattr(_get_token_vector, \'unk_vector\'):\n            if SPECIAL_TOKENS.UNKNOWN_TOKEN in w2v_model.wv.vocab:\n                _get_token_vector.unk_vector = np.array(w2v_model[SPECIAL_TOKENS.UNKNOWN_TOKEN])\n            else:\n                _get_token_vector.unk_vector = np.mean([w2v_model[x] for x in w2v_model.wv.vocab], axis=0)\n        return _get_token_vector.unk_vector\n\n\ndef get_training_batch(inputs, batch_size, random_permute=False, random_seed=RANDOM_SEED):\n    """"""\n    Generator that yields data in batches. The last batch may be incomplete, yield it as well.\n    :param inputs: tuple of numpy arrays, for example (contexts_ids, responses_ids, conditions_ids)\n    :param batch_size: length of numpy arrays to be yielded for each input\n    :param random_permute: if True input arrays data will be synchronously shuffled before yielding\n    :param random_seed: seed to ensure the identical shuffling of input data for experiments reproducibility\n    :return: generator that yields tuples of numpy arrays with batch_size length\n    """"""\n    n_samples = inputs[0].shape[0]\n    n_batches = n_samples // batch_size\n    batches_seq = np.arange(n_batches)\n    samples_seq = np.arange(n_samples)\n\n    if random_permute:\n        np.random.seed(random_seed)\n        np.random.shuffle(samples_seq)\n\n    for i in batches_seq:\n        start = i * batch_size\n        end = (i + 1) * batch_size\n        samples_ids = samples_seq[start:end]\n        # yield batch_size selected sequences of x and y ids\n        yield tuple(inp[samples_ids] for inp in inputs)\n\n    seen_samples_num = len(batches_seq) * batch_size\n\n    if seen_samples_num < n_samples:\n        samples_ids = samples_seq[seen_samples_num:]\n        # yield the rest of x and y sequences\n        yield tuple(inp[samples_ids] for inp in inputs)\n\n\ndef reverse_nn_input(dataset, service_tokens):\n    """"""\n    Swaps the last utterance of x with y for each x-y pair in the dataset.\n    To handle different length of sequences, everything is filled with pads\n    to the length of longest sequence.\n    """"""\n    # Swap last utterance of x with y, while padding with start- and eos-tokens\n    y_output = np.full(dataset.y.shape, service_tokens.pad_token_id, dtype=dataset.y.dtype)\n    for y_output_sample, x_input_sample in zip(y_output, dataset.x[:, -1]):\n        # Write start token at the first index\n        y_output_sample[0] = service_tokens.start_token_id\n        y_output_token_index = 1\n        for value in x_input_sample:\n            # We should stop at pad tokens in the input sample\n            if value == service_tokens.pad_token_id:\n                break\n            # We should keep last token index with pad, so we can replace it futher with eos-token\n            if y_output_token_index == y_output_sample.shape[-1] - 1:\n                break\n            y_output_sample[y_output_token_index] = value\n            y_output_token_index += 1\n        # Write eos token right after the last non-pad token in the sample\n        y_output_sample[y_output_token_index] = service_tokens.eos_token_id\n\n    # Use utterances from y in x while truncating start- and eos-tokens\n    x_output = np.full(dataset.x.shape, service_tokens.pad_token_id, dtype=dataset.x.dtype)\n    for x_output_sample, x_input_sample, y_input_sample in zip(x_output, dataset.x[:, :-1], dataset.y):\n        # Copy all the context utterances except the last one right to the output\n        x_output_sample[:-1] = x_input_sample\n        x_output_token_index = 0\n        for value in y_input_sample:\n            # Skip start- and eos-tokens from the input sample because we don\'t need them in X\n            if value in {service_tokens.start_token_id, service_tokens.eos_token_id}:\n                continue\n            # Stop if we already reached the end of output sample (in case the input sample is longer than output)\n            if x_output_token_index == x_output_sample.shape[-1]:\n                break\n            # Fill the tokens of the last utterance in dialog context\n            x_output_sample[-1, x_output_token_index] = value\n            x_output_token_index += 1\n\n    return Dataset(x=x_output, y=y_output, condition_ids=dataset.condition_ids)\n\n\ndef transform_conditions_to_nn_input(dialog_conditions, condition_to_index, num_dialogs):\n    y_conditions_iterator = islice(dialog_conditions, 1, None, 2)\n\n    _logger.info(\'Iterating through conditions of output list\')\n    return transform_conditions_to_ids(y_conditions_iterator, condition_to_index, num_dialogs)\n\n\ndef _get_x_data_iterator_with_context(x_data_iterator, y_data_iterator, context_size=INPUT_CONTEXT_SIZE):\n    context = []\n\n    last_y_line = None\n    for x_line, y_line in zip(x_data_iterator, y_data_iterator):\n        if x_line != last_y_line:\n            context = []  # clear context if last response != current dialog context (new dialog)\n\n        context.append(x_line)\n        yield context[-context_size:]  # yield list of tokenized lines\n        last_y_line = y_line\n\n\ndef transform_lines_to_nn_input(tokenized_dialog_lines, token_to_index, autoencoder_mode=AUTOENCODER_MODE):\n    """"""\n    Splits lines (IterableSentences) and generates numpy arrays of token ids suitable for training.\n    Doesn\'t store all lines in memory.\n    """"""\n    x_data_iterator, y_data_iterator, iterator_for_len_calc = file_buffered_tee(tokenized_dialog_lines, 3)\n\n    _logger.info(\'Iterating through lines to get number of elements in the dataset\')\n    n_dialogs = sum(1 for _ in iterator_for_len_calc)\n\n    if not autoencoder_mode:\n        # seq2seq mode\n        x_data_iterator = islice(x_data_iterator, 0, None, 2)\n        y_data_iterator = islice(y_data_iterator, 1, None, 2)\n        n_dialogs //= 2\n\n    y_data_iterator, y_data_iterator_for_context = file_buffered_tee(y_data_iterator)\n    x_data_iterator = _get_x_data_iterator_with_context(x_data_iterator, y_data_iterator_for_context)\n\n    _logger.info(\'Iterating through lines to get input matrix\')\n    x_ids = transform_contexts_to_token_ids(\n        x_data_iterator, token_to_index, INPUT_SEQUENCE_LENGTH, INPUT_CONTEXT_SIZE, max_contexts_num=n_dialogs)\n\n    _logger.info(\'Iterating through lines to get output matrix\')\n    y_ids = transform_lines_to_token_ids(\n        y_data_iterator, token_to_index, OUTPUT_SEQUENCE_LENGTH, n_dialogs, add_start_end=True)\n    return x_ids, y_ids, n_dialogs\n'"
cakechat/utils/__init__.py,0,b''
cakechat/utils/data_structures.py,0,"b'import itertools\nimport collections\n\n\ndef flatten(xs, constructor=list):\n    return constructor(itertools.chain.from_iterable(xs))\n\n\ndef create_namedtuple_instance(name, **kwargs):\n    return collections.namedtuple(name, kwargs.keys())(**kwargs)\n'"
cakechat/utils/data_types.py,0,"b""from collections import namedtuple\n\nDataset = namedtuple('Dataset', ['x', 'y', 'condition_ids'])\nModelParam = namedtuple('ModelParam', ['value', 'id'])\n"""
cakechat/utils/dataset_loader.py,0,"b""import os\nfrom itertools import islice\n\nimport numpy as np\n\nfrom cakechat.config import TEST_DATA_DIR, CONTEXT_FREE_VAL_CORPUS_NAME, QUESTIONS_CORPUS_NAME, \\\n    CONTEXT_SENSITIVE_VAL_CORPUS_NAME, INPUT_SEQUENCE_LENGTH, INPUT_CONTEXT_SIZE, DEFAULT_CONDITION, RANDOM_SEED, \\\n    MAX_VAL_LINES_NUM, CONTEXT_SENSITIVE_TEST_CORPUS_NAME\nfrom cakechat.dialog_model.inference import ServiceTokensIDs\nfrom cakechat.dialog_model.model_utils import lines_to_context, transform_contexts_to_token_ids, \\\n    transform_conditions_to_nn_input, transform_lines_to_nn_input, reverse_nn_input\nfrom cakechat.utils.data_structures import create_namedtuple_instance\nfrom cakechat.utils.data_types import Dataset\nfrom cakechat.utils.files_utils import load_file, is_non_empty_file\nfrom cakechat.utils.logger import get_logger\nfrom cakechat.utils.profile import timer\nfrom cakechat.utils.text_processing import get_tokens_sequence, replace_out_of_voc_tokens, \\\n    get_processed_corpus_path, load_processed_dialogs_from_json, FileTextLinesIterator, \\\n    get_dialog_lines_and_conditions, ProcessedLinesIterator, get_alternated_dialogs_lines\n\n_logger = get_logger(__name__)\n\n\ndef get_tokenized_test_lines(corpus_name, tokens_voc):\n    corpus_path = os.path.join(TEST_DATA_DIR, '{}.txt'.format(corpus_name))\n    if not is_non_empty_file(corpus_path):\n        raise ValueError('Test corpus file doesn\\'t exist: {}'.format(corpus_path))\n    test_lines = load_file(corpus_path)\n    result = []\n    for line in test_lines:\n        tokenized_line = get_tokens_sequence(line)\n        tokenized_line = replace_out_of_voc_tokens(tokenized_line, tokens_voc)\n        result.append(tokenized_line)\n\n    return result\n\n\ndef _load_dataset_without_responses(corpus_name, token_to_index):\n    tokenized_lines = get_tokenized_test_lines(corpus_name, set(token_to_index.keys()))\n    context_tokens_ids = transform_contexts_to_token_ids(\n        lines_to_context(tokenized_lines),\n        token_to_index,\n        INPUT_SEQUENCE_LENGTH,\n        INPUT_CONTEXT_SIZE,\n        max_contexts_num=len(tokenized_lines))\n    return Dataset(x=context_tokens_ids, y=None, condition_ids=None)\n\n\ndef load_questions_set(token_to_index):\n    return _load_dataset_without_responses(QUESTIONS_CORPUS_NAME, token_to_index)\n\n\ndef get_validation_data_id(validation_sets_names):\n    return ','.join(sorted(validation_sets_names))\n\n\ndef get_validation_sets_names():\n    return [CONTEXT_FREE_VAL_CORPUS_NAME, CONTEXT_SENSITIVE_VAL_CORPUS_NAME]\n\n\ndef get_validation_dataset_name_to_data(validation_sets_names, token_to_index, condition_to_index, is_reverse_model):\n    _logger.info('Loading validations sets...')\n    factory = {\n        CONTEXT_FREE_VAL_CORPUS_NAME: lambda: load_context_free_val(token_to_index),\n        CONTEXT_SENSITIVE_VAL_CORPUS_NAME: lambda: load_context_sensitive_val(token_to_index, condition_to_index)\n    }\n    dataset_name_to_data = {val_set_name: factory[val_set_name]() for val_set_name in validation_sets_names}\n    _logger.info('Done loading validations sets')\n\n    if is_reverse_model:\n        _logger.info('Reversing validations sets...')\n        service_tokens = ServiceTokensIDs(token_to_index)\n        dataset_name_to_data = {\n            val_set_name: reverse_nn_input(val_set, service_tokens)\n            for val_set_name, val_set in dataset_name_to_data.items()\n        }\n        _logger.info('Done reversing validations sets')\n\n    return dataset_name_to_data\n\n\n@timer\ndef load_context_free_val(token_to_index):\n    _logger.info('Transform context free validation lines to matrix of indexes')\n    tokenized_validation_lines = get_tokenized_test_lines(CONTEXT_FREE_VAL_CORPUS_NAME, set(token_to_index.keys()))\n    tokenized_validation_lines = tokenized_validation_lines[:MAX_VAL_LINES_NUM]\n    x_validation, y_validation, _ = transform_lines_to_nn_input(tokenized_validation_lines, token_to_index)\n    return Dataset(x=x_validation, y=y_validation, condition_ids=None)\n\n\n@timer\ndef load_context_sensitive_val(token_to_index, condition_to_index):\n    processed_val_corpus_path = get_processed_corpus_path(CONTEXT_SENSITIVE_VAL_CORPUS_NAME)\n    context_sensitive_val_dialogs = load_processed_dialogs_from_json(\n        FileTextLinesIterator(processed_val_corpus_path), text_field_name='text', condition_field_name='condition')\n    context_sensitive_val_dialogs = islice(context_sensitive_val_dialogs, MAX_VAL_LINES_NUM)\n\n    alternated_context_sensitive_val_dialogs = \\\n        get_alternated_dialogs_lines(context_sensitive_val_dialogs)\n    alternated_context_sensitive_val_lines, alternated_context_sensitive_val_conditions = \\\n        get_dialog_lines_and_conditions(alternated_context_sensitive_val_dialogs,\n                                        text_field_name='text', condition_field_name='condition')\n    tokenized_alternated_context_sensitive_val_lines = ProcessedLinesIterator(\n        alternated_context_sensitive_val_lines, processing_callbacks=[get_tokens_sequence])\n\n    _logger.info('Transform context sensitive validation lines to tensor of indexes')\n    x_context_sensitive_val, y_context_sensitive_val, num_context_sensitive_val_dialogs = \\\n        transform_lines_to_nn_input(tokenized_alternated_context_sensitive_val_lines, token_to_index)\n    condition_ids_context_sensitive_val = transform_conditions_to_nn_input(\n        alternated_context_sensitive_val_conditions, condition_to_index, num_context_sensitive_val_dialogs)\n    return Dataset(\n        x=x_context_sensitive_val, y=y_context_sensitive_val, condition_ids=condition_ids_context_sensitive_val)\n\n\n@timer\ndef load_conditioned_dataset(corpus_name, token_to_index, condition_to_index, subset_size=None):\n    processed_corpus_path = get_processed_corpus_path(corpus_name)\n    dialogs = load_processed_dialogs_from_json(\n        FileTextLinesIterator(processed_corpus_path), text_field_name='text', condition_field_name='condition')\n    if subset_size:\n        _logger.info('Slicing dataset to the first {} entries'.format(subset_size))\n        dialogs = islice(dialogs, subset_size)\n    train_lines, train_conditions = get_dialog_lines_and_conditions(\n        get_alternated_dialogs_lines(dialogs), text_field_name='text', condition_field_name='condition')\n    tokenized_alternated_train_lines = ProcessedLinesIterator(train_lines, processing_callbacks=[get_tokens_sequence])\n\n    # prepare train set\n    x_train, y_train, n_dialogs = transform_lines_to_nn_input(tokenized_alternated_train_lines, token_to_index)\n\n    condition_ids_train = transform_conditions_to_nn_input(train_conditions, condition_to_index, n_dialogs)\n    return Dataset(x=x_train, y=y_train, condition_ids=condition_ids_train)\n\n\ndef get_training_dataset(train_corpus_name,\n                         token_to_index,\n                         condition_to_index,\n                         is_reverse_model,\n                         train_subset_size=None):\n    _logger.info('Loading training dataset...')\n    train_dataset = load_conditioned_dataset(train_corpus_name, token_to_index, condition_to_index, train_subset_size)\n\n    if is_reverse_model:\n        _logger.info('Reversing training dataset...')\n        service_tokens = ServiceTokensIDs(token_to_index)\n        train_dataset = reverse_nn_input(train_dataset, service_tokens)\n\n    return train_dataset\n\n\n@timer\ndef generate_subset(dataset, subset_size, random_seed=RANDOM_SEED):\n    # Fix random seed here so that we get the same subsets every time the function is called\n    np.random.seed(random_seed)\n    if subset_size > dataset.x.shape[0]:\n        raise ValueError('Error while generating subset of the validation data: '\n                         'dataset size ({}) is less than subset size ({})'.format(dataset.x.shape[0], subset_size))\n    sample_idx = np.random.choice(dataset.x.shape[0], size=subset_size, replace=False)\n    return Dataset(\n        x=dataset.x[sample_idx],\n        y=dataset.y[sample_idx] if dataset.y is not None else None,\n        condition_ids=dataset.condition_ids[sample_idx] if dataset.condition_ids is not None else None)\n\n\ndef load_datasets(token_to_index, condition_to_index, test_corpus_name=CONTEXT_SENSITIVE_TEST_CORPUS_NAME):\n    # load context_sensitive_test dataset\n    cs_test = load_conditioned_dataset(test_corpus_name, token_to_index, condition_to_index)\n    # load context_free_validation dataset\n    cf_validation = load_context_free_val(token_to_index)\n\n    # load context sensitive testset for one selected condition\n    condition_mask = cs_test.condition_ids != condition_to_index[DEFAULT_CONDITION]\n    conditioned_test = Dataset(\n        x=cs_test.x[condition_mask], y=cs_test.y[condition_mask], condition_ids=cs_test.condition_ids[condition_mask])\n\n    # get a subset of conditioned_test of the same size as cf_validation;\n    # if there are no so many samples in conditioned_test, use all of the available conditioned_test samples\n    cs_test_one_condition = \\\n        generate_subset(conditioned_test, subset_size=min(cf_validation.x.shape[0], conditioned_test.x.shape[0]))\n\n    return create_namedtuple_instance(\n        'EvalMetricsDatasets',\n        cf_validation=cf_validation,\n        cs_test=cs_test,\n        cs_test_one_condition=cs_test_one_condition)\n"""
cakechat/utils/env.py,4,"b'import os\nimport subprocess\n\nimport numpy as np\nimport tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\n\nfrom cakechat.utils.logger import get_logger\n\n_logger = get_logger(__name__)\n\n\ndef is_dev_env():\n    try:\n        is_dev = os.environ[\'IS_DEV\']\n        return bool(int(is_dev))\n    except (KeyError, ValueError):\n        return False\n\n\ndef init_cuda_env():\n    os.environ[\'PATH\'] += \':/usr/local/cuda/bin\'\n    os.environ[\'LD_LIBRARY_PATH\'] = \'/usr/local/cuda/lib64:/usr/local/nvidia/lib64/:/usr/local/cuda/extras/CUPTI/lib64\'\n    os.environ[\'LIBRARY_PATH\'] = \'/usr/local/share/cudnn\'\n    os.environ[\'CUDA_HOME\'] = \'/usr/local/cuda\'\n    os.environ[\'CUDA_DEVICE_ORDER\'] = \'PCI_BUS_ID\'\n\n\ndef try_import_horovod():\n    try:\n        import horovod.keras as hvd\n    except ImportError:\n        return None\n    else:\n        return hvd\n\n\ndef init_keras(hvd=None):\n    """"""\n    Set config for Horovod. Config params copied from official example:\n    https://github.com/uber/horovod/blob/master/examples/keras_mnist_advanced.py#L15\n\n    :param hvd: instance of horovod.keras\n    """"""\n\n    init_cuda_env()\n    config = tf.ConfigProto()\n\n    if hvd:\n        hvd.init()\n        config.gpu_options.allow_growth = True\n        config.gpu_options.visible_device_list = str(hvd.local_rank())\n\n    set_session(tf.Session(config=config))\n\n\ndef set_keras_tf_session(gpu_memory_fraction):\n    config = tf.ConfigProto()\n    config.gpu_options.per_process_gpu_memory_fraction = float(gpu_memory_fraction)  # pylint: disable=maybe-no-member\n    set_session(tf.Session(config=config))\n\n\ndef run_horovod_train(train_cmd, gpu_ids):\n    os.environ[\'CUDA_DEVICE_ORDER\'] = \'PCI_BUS_ID\'\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \',\'.join(gpu_ids)\n\n    cmd = \'mpirun -np {workers_nums} -H localhost:{workers_nums} {train_cmd}\'.format(\n        workers_nums=len(gpu_ids), train_cmd=train_cmd)\n    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\n    while process.poll() is None:\n        output = process.stdout.readline()\n        if output:\n            print(output.strip())\n\n\ndef is_main_horovod_worker(horovod):\n    return horovod is None or horovod.rank() == 0\n\n\ndef set_horovod_worker_random_seed(horovod):\n    seed = horovod.rank() if horovod else 0\n    np.random.seed(seed)\n\n'"
cakechat/utils/files_utils.py,0,"b'import os\nimport pickle\nimport tarfile\nfrom functools import partial\nfrom abc import abstractmethod, ABCMeta\n\nfrom cakechat.utils.logger import get_logger, WithLogger\n\n_logger = get_logger(__name__)\n\nDEFAULT_CSV_DELIMITER = \',\'\n\n\nclass AbstractFileResolver(object, metaclass=ABCMeta):\n    def __init__(self, file_path):\n        self._file_path = file_path\n\n    @property\n    def file_path(self):\n        return self._file_path\n\n    def resolve(self):\n        """"""\n        :return: True if file can be resolved, False otherwise\n        """"""\n        if os.path.exists(self._file_path):\n            return True\n\n        return self._resolve()\n\n    @abstractmethod\n    def _resolve(self):\n        """"""\n        Performs some actions if file does not exist locally. Should be defined in subclasses\n\n        :return: True if file can be resolved, False otherwise\n        """"""\n        pass\n\n\nclass DummyFileResolver(AbstractFileResolver):\n    """"""\n    Does nothing if file does not exist locally\n    """"""\n\n    def _resolve(self):\n        return False\n\n\nclass PackageResolver(WithLogger):\n    def __init__(self, package_path, package_file_resolver_factory, package_file_ext, package_extractor):\n        """"""\n        :param package_path:\n        :param package_file_resolver_factory: a factory creating package file resolver\n        :param package_file_ext: package file extension\n        :param package_extractor: a function taking package file, package path, and extracting contents to that path\n        :return:\n        """"""\n        WithLogger.__init__(self)\n\n        self._package_path = package_path\n        self._package_file_resolver_factory = package_file_resolver_factory\n        self._package_file_ext = package_file_ext\n        self._package_extractor = package_extractor\n\n    @staticmethod\n    def init_resolver(**kwargs):\n        """"""\n        Method helping to set once some parameters like package_file_resolver and package_extractor\n\n        :param kwargs:\n        :return: partially initialized class object\n        """"""\n        return partial(PackageResolver, **kwargs)\n\n    def resolve(self):\n        if os.path.exists(self._package_path):\n            return True\n\n        package_file_path = \'{}.{}\'.format(self._package_path, self._package_file_ext)\n        package_file_resolver = self._package_file_resolver_factory(package_file_path)\n        if package_file_resolver.resolve():\n            self._logger.info(\'Extracting package {}\'.format(package_file_resolver.file_path))\n            self._package_extractor(package_file_resolver.file_path, self._package_path)\n            return True\n        else:\n            return False\n\n\ndef load_file(file_path, filter_empty_lines=True):\n    with open(file_path, \'r\', encoding=\'utf-8\') as fh:\n        lines = [line.strip() for line in fh.readlines()]\n        if filter_empty_lines:\n            lines = list(filter(None, lines))\n\n        return lines\n\n\ndef ensure_dir(dir_name):\n    if dir_name and not os.path.exists(dir_name):\n        os.makedirs(dir_name)\n\n\ndef serialize(filename, data, protocol=pickle.HIGHEST_PROTOCOL):\n    ensure_dir(os.path.dirname(filename))\n    with open(filename, \'wb\') as f:\n        pickle.dump(data, f, protocol)\n\n\ndef deserialize(filename):\n    with open(filename, \'rb\') as f:\n        item = pickle.load(f)\n    return item\n\n\ndef get_persisted(factory, persisted_file_name, **kwargs):\n    """"""\n    Loads cache if exists, otherwise calls factory and stores the results in the specified cache file.\n    **kwargs are passed to the serialize() function\n    :param factory:\n    :param persisted_file_name:\n    :return:\n    """"""\n    if os.path.exists(persisted_file_name):\n        _logger.info(\'Loading {}\'.format(persisted_file_name))\n        cached = deserialize(persisted_file_name)\n        return cached\n\n    _logger.info(\'Creating {}\'.format(persisted_file_name))\n    data = factory()\n    serialize(persisted_file_name, data, **kwargs)\n    return data\n\n\ndef is_non_empty_file(file_path):\n    return os.path.isfile(file_path) and os.stat(file_path).st_size != 0\n\n\nclass FileNotFoundException(Exception):\n    pass\n\n\ndef extract_tar(source_path, destination_path, compression_type=\'gz\'):\n    """"""\n    :param source_path:\n    :param destination_path:\n    :param compression_type: None, gz or bzip2\n    :return:\n    """"""\n    mode = \'r:{}\'.format(compression_type if compression_type else \'r\')\n    with tarfile.open(source_path, mode) as fh:\n        fh.extractall(path=destination_path)\n\n\ndef ensure_file(file_name, mode, encoding=None):\n    ensure_dir(os.path.dirname(file_name))\n    return open(file_name, mode, encoding=encoding)\n\n\ndef get_cached(factory, cache_file_name, **kwargs):\n    """"""\n    Loads cache if exists, otherwise calls factory and stores the results in the specified cache file.\n    **kwargs are passed to the serialize() function\n    :param factory:\n    :param cache_file_name:\n    :return:\n    """"""\n    if os.path.exists(cache_file_name):\n        _logger.info(\'Loading {}\'.format(cache_file_name))\n        cached = deserialize(cache_file_name)\n        return cached\n\n    _logger.info(\'Creating {}\'.format(cache_file_name))\n    data = factory()\n    serialize(cache_file_name, data, **kwargs)\n    return data\n'"
cakechat/utils/logger.py,0,"b""import logging\nimport logging.config\n\n\nlogging.config.dictConfig({\n    'version': 1,\n    'disable_existing_loggers': False,\n    'handlers': {\n        'console': {\n            'class': 'cakechat.utils.logger_utils.FormattedStreamHandler',\n            'level': 'INFO'\n        },\n        'laconic': {\n            'class': 'cakechat.utils.logger_utils.LaconicStreamHandler',\n            'level': 'INFO'\n        }\n    },\n    'loggers': {\n        'cakechat': {\n            'handlers': ['console'],\n            'level': 'INFO',\n        },\n        'cakechat.laconic_logger': {\n            'handlers': ['laconic'],\n            'level': 'INFO',\n            'propagate': False\n        }\n    }\n})\n\n\ndef get_logger(name):\n    return logging.getLogger(name)\n\n\ndef get_tools_logger(name):\n    return logging.getLogger('cakechat.' + name)\n\n\ndef _get_laconic_logger():\n    return get_tools_logger('laconic_logger')\n\n\nclass WithLogger(object):\n    def __init__(self):\n        self._logger = logging.getLogger(self.__class__.__module__ + '.' + self.__class__.__name__)\n\n\nlaconic_logger = _get_laconic_logger()\n"""
cakechat/utils/logger_utils.py,0,"b""import logging\n\n\nclass DefaultFormatter(logging.Formatter):\n    _FMT = '[%(asctime)s.%(msecs)03d][%(levelname)s][%(process)s][%(name)s][%(lineno)d] %(message)s'\n    _DATEFMT = '%d.%m.%Y %H:%M:%S'\n\n    def __init__(self):\n        super(DefaultFormatter, self).__init__(fmt=self._FMT, datefmt=self._DATEFMT)\n\n\nclass FormattedStreamHandler(logging.StreamHandler):\n    def __init__(self, stream=None):\n        super(FormattedStreamHandler, self).__init__(stream)\n        self.formatter = DefaultFormatter()\n\n\nclass LaconicFormatter(logging.Formatter):\n    _FMT = '%(message)s'\n\n    def __init__(self):\n        super(LaconicFormatter, self).__init__(fmt=self._FMT)\n\n\nclass LaconicStreamHandler(logging.StreamHandler):\n    def __init__(self, stream=None):\n        super(LaconicStreamHandler, self).__init__(stream)\n        self.formatter = LaconicFormatter()\n"""
cakechat/utils/profile.py,0,"b'import time\nfrom functools import wraps\n\nfrom cakechat.utils.logger import get_logger\n\n_logger = get_logger(__name__)\n\n\ndef _execute_and_profile(fn, *args, **kwargs):\n    start_time = time.time()\n    fn_result = fn(*args, **kwargs)\n    execution_time = time.time() - start_time\n\n    _logger.info(\'Elapsed time for ""{}"": {}\'.format(fn.__name__, execution_time))\n    return execution_time, fn_result\n\n\ndef timer(fn):\n    """"""\n    Timer decorator. Logs execution time of the function.\n    """"""\n\n    @wraps(fn)\n    def _perform(*args, **kwargs):\n        _, fn_result = _execute_and_profile(fn, *args, **kwargs)\n        return fn_result\n\n    return _perform\n'"
cakechat/utils/tee_file.py,0,"b""import os\nimport pickle\nimport tempfile\n\n# Pickle on HIGHEST_PROTOCOL breaks on Python 3.6.5\n_PICKLE_PROTOCOL = 2\n\n\ndef _pickle_iterable(filename, iterable):\n    with open(filename, 'wb') as pickle_fh:\n        pklr = pickle.Pickler(pickle_fh, _PICKLE_PROTOCOL)\n        for entry in iterable:\n            pklr.dump(entry)\n            pklr.clear_memo()\n\n\ndef _open_pickle(filename):\n    return open(filename, 'rb')\n\n\ndef _unpickle_iterable(pickle_fh):\n    with pickle_fh:\n        unpklr = pickle.Unpickler(pickle_fh)\n        try:\n            while True:\n                yield unpklr.load()\n        except EOFError:\n            pass\n\n\ndef file_buffered_tee(iterable, n=2):\n    _, filename = tempfile.mkstemp()\n    try:\n        _pickle_iterable(filename, iterable)\n        return tuple(_unpickle_iterable(_open_pickle(filename)) for _ in range(n))\n    finally:\n        os.remove(filename)\n"""
cakechat/utils/telegram_bot_client.py,0,"b'from abc import ABCMeta, abstractmethod\n\nimport telepot\nimport telepot.loop\n\nfrom cakechat.utils.logger import WithLogger\n\n\nclass AbstractTelegramChatSession(WithLogger, metaclass=ABCMeta):\n    """"""\n    Specific implementations of a chat session should overload default\n    message handler `default_handle_message`, and possibly some of specific\n    handler like `handle_photo_message` or `handle_text_message`.\n\n    `_register_command` can be used to register a specific command to be\n    recognized by the bot along with a corresponding handler.\n\n    For example consider the following chat session that just echoes\n    the input in a reverse order\n\n    class ReversedChatSession(AbstractTelegramChatSession):\n        def __init__(self, *args, **kwargs):\n            super(ReversedChatSession, self).__init__(*args, **kwargs)\n            self._register_command(\'start\', self.greet)\n\n        def greet(self, arg):\n            self._send_text(\'!olleH\')\n\n        def handle_text_message(self, msg_text, msg):\n            self._send_text(msg_text[::-1])\n\n        def default_handle_message(self, msg):\n            self._send_text(\'Sorry, I can only reverse text\')\n\n    In order to run a telegram bot with a given ChatSession, create\n    an instance of TelegramBot and call the run method.\n\n    TelegramBot(token).run(ReversedChatSession)\n    """"""\n\n    def __init__(self, bot, chat_id):\n        super(AbstractTelegramChatSession, self).__init__()\n        self._bot = bot\n        self._chat_id = chat_id\n\n        self._command_to_handler = {}\n        # Default no-op /start command handler\n        self._register_command(command=\'start\', handler=lambda *args: None, description=\'Start or restart a session\')\n        self._register_command(\n            command=\'help\', handler=self._send_bot_help, description=\'Show bot info and a list of available commands\')\n\n    def _send_text(self, text, *args, **kwargs):\n        self._bot.sendMessage(self._chat_id, text, *args, **kwargs)\n\n    def _send_photo(self, photo, *args, **kwargs):\n        self._bot.sendPhoto(self._chat_id, photo, *args, **kwargs)\n\n    def handle_text_message(self, text, msg):\n        self._logger.info(\'Received text message ({}), falling back to the default message handler\'.format(text))\n        return self.default_handle_message(msg)\n\n    def handle_photo_message(self, photo_url, msg):\n        self._logger.info(\'Received photo (url={}), falling back to the default message handler\'.format(photo_url))\n        return self.default_handle_message(msg)\n\n    @abstractmethod\n    def default_handle_message(self, msg):\n        pass\n\n    @staticmethod\n    def _bot_info():\n        return \'This is Replika CakeChat telegram bot.\'\n\n    def _send_bot_help(self, _):\n        help_lines = [self._bot_info(), \'\', \'List of available commands:\']\n        for command, (_, description) in self._command_to_handler.items():\n            help_lines.append(\'/{} - {}\'.format(command, description))\n\n        return self._send_text(\'\\n\'.join(help_lines))\n\n    def handle_command(self, command, arg):\n        if command not in self._command_to_handler:\n            self._send_text(\'Unknown command {}\'.format(command))\n            return self._send_bot_help(arg)\n\n        handler, _ = self._command_to_handler[command]\n        return handler(arg)\n\n    def _register_command(self, command, handler, description=\'???\'):\n        self._command_to_handler[command] = (handler, description)\n\n\nclass TelegramBot(WithLogger):\n    """"""\n    Interface for telegram bot API. Each bot maintains many\n    ChatSessions that are handled by classes\n    """"""\n\n    def __init__(self, token):\n        """"""\n\n        :param token: a bot authorization token, can be obtained from the @BotFather bot.\n        """"""\n        super(TelegramBot, self).__init__()\n\n        self._token = token\n        self._bot = telepot.Bot(token)\n        self._chat_id_to_session = {}\n\n    @staticmethod\n    def _parse_command(msg_text):\n        """"""\n        Parse message text as /<command> [argument]\n        Further parsing of `argument` is left for specific\n        implementations of `AbstractTelegramChatSession.handle_command`\n        """"""\n        if not msg_text.startswith(\'/\'):\n            raise ValueError(\'The command must start with /\')\n\n        command_and_arg = msg_text[1:].strip().split(\' \', 1)\n        command = command_and_arg[0]\n        arg = command_and_arg[1] if len(command_and_arg) > 1 else \'\'\n        return command, arg\n\n    def _extract_photo_url(self, photo_sizes):\n        # Telegram prepares several resized versions of the image,\n        # we chose the biggest, assuming it\'s the original one\n        photo_id = max(photo_sizes, key=lambda x: x[\'width\'] * x[\'height\'])[\'file_id\']\n        photo_path = self._bot.getFile(photo_id)[\'file_path\']\n        return \'https://api.telegram.org/file/bot{token}/{path}\'.format(token=self._token, path=photo_path)\n\n    def _init_chat_session(self, chat_id, session_class, **session_kwargs):\n        session = session_class(self._bot, chat_id, **session_kwargs)\n        action = \'Started new\' if chat_id not in self._chat_id_to_session else \'Restarted existing\'\n        self._logger.info(\'{} chat session {}, currently opened: {}\'.format(action, chat_id,\n                                                                            len(self._chat_id_to_session)))\n        return session\n\n    def run(self, session_class, **session_kwargs):\n        """"""\n        :param session_class: subclass of AbstractTelegramChat\n        """"""\n        self._logger.info(\'Started a new chat bot {}\'.format(self._bot.getMe()))\n\n        def _handler(msg):\n            content_type, chat_type, chat_id = telepot.glance(msg)\n\n            if chat_id not in self._chat_id_to_session:\n                self._chat_id_to_session[chat_id] = self._init_chat_session(chat_id, session_class, **session_kwargs)\n\n            session = self._chat_id_to_session[chat_id]\n\n            if content_type == \'text\' and msg[\'text\'].startswith(\'/\'):\n                command, arg = self._parse_command(msg[\'text\'])\n                if command == \'start\':\n                    session = self._chat_id_to_session[chat_id] = self._init_chat_session(\n                        chat_id, session_class, **session_kwargs)\n\n                return session.handle_command(command, arg)\n\n            if content_type == \'text\':\n                return session.handle_text_message(msg[\'text\'], msg)\n\n            if content_type == \'photo\':\n                photo_url = self._extract_photo_url(msg[\'photo\'])\n                return session.handle_photo_message(photo_url, msg)\n\n            return session.default_handle_message(msg)\n\n        telepot.loop.MessageLoop(self._bot, _handler).run_forever()\n'"
tools/quality/condition_quality.py,0,"b'import os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom cakechat.utils.env import init_cuda_env\n\ninit_cuda_env()\n\nfrom cakechat.utils.dataset_loader import load_datasets\nfrom cakechat.utils.data_types import Dataset\nfrom cakechat.utils.logger import get_tools_logger\nfrom cakechat.dialog_model.factory import get_trained_model\nfrom cakechat.dialog_model.model_utils import transform_token_ids_to_sentences\nfrom cakechat.dialog_model.inference import get_nn_responses\nfrom cakechat.dialog_model.quality import calculate_model_mean_perplexity, get_tfidf_vectorizer, \\\n    calculate_lexical_similarity\nfrom cakechat.config import PREDICTION_MODE_FOR_TESTS, DEFAULT_CONDITION\n\n_logger = get_tools_logger(__file__)\n\n\ndef _make_non_conditioned(dataset):\n    return Dataset(x=dataset.x, y=dataset.y, condition_ids=None)\n\n\ndef _slice_condition_data(dataset, condition_id):\n    condition_mask = (dataset.condition_ids == condition_id)\n    return Dataset(\n        x=dataset.x[condition_mask], y=dataset.y[condition_mask], condition_ids=dataset.condition_ids[condition_mask])\n\n\ndef calc_perplexity_metrics(nn_model, eval_datasets):\n    return {\n        \'ppl_cs_test\':\n            calculate_model_mean_perplexity(nn_model, eval_datasets.cs_test),\n        \'ppl_cs_test_not_conditioned\':\n            calculate_model_mean_perplexity(nn_model, _make_non_conditioned(eval_datasets.cs_test)),\n        \'ppl_cs_test_one_condition\':\n            calculate_model_mean_perplexity(nn_model, eval_datasets.cs_test_one_condition),\n        \'ppl_cs_test_one_condition_not_conditioned\':\n            calculate_model_mean_perplexity(nn_model, _make_non_conditioned(eval_datasets.cs_test_one_condition)),\n        \'ppl_cf_validation\':\n            calculate_model_mean_perplexity(nn_model, eval_datasets.cf_validation)\n    }\n\n\ndef calc_perplexity_for_conditions(nn_model, dataset):\n    cond_to_ppl_conditioned, cond_to_ppl_not_conditioned = {}, {}\n\n    for condition, condition_id in nn_model.condition_to_index.items():\n        if condition == DEFAULT_CONDITION:\n            continue\n\n        dataset_with_conditions = _slice_condition_data(dataset, condition_id)\n\n        if not dataset_with_conditions.x.size:\n            _logger.warning(\n                \'No dataset samples found with the given condition ""{}"", skipping metrics.\'.format(condition))\n            continue\n\n        cond_to_ppl_conditioned[condition] = \\\n            calculate_model_mean_perplexity(nn_model, _make_non_conditioned(dataset_with_conditions))\n\n        cond_to_ppl_not_conditioned[condition] = \\\n            calculate_model_mean_perplexity(nn_model, dataset_with_conditions)\n\n    return cond_to_ppl_conditioned, cond_to_ppl_not_conditioned\n\n\ndef predict_for_condition_id(nn_model, x_val, condition_id=None):\n    responses = get_nn_responses(x_val, nn_model, mode=PREDICTION_MODE_FOR_TESTS, condition_ids=condition_id)\n    return [candidates[0] for candidates in responses]\n\n\ndef calc_lexical_similarity_metrics(nn_model, testset, tfidf_vectorizer):\n    """"""\n    For each condition calculate lexical similarity between ground-truth responses and\n    generated conditioned responses. Similarly compare ground-truth responses with non-conditioned generated responses.\n    If lex_sim(gt, cond_resp) > lex_sim(gt, non_cond_resp), the conditioning on extra information proves to be useful.\n    :param nn_model: trained model to evaluate\n    :param testset: context-sensitive testset, instance of Dataset\n    :param tfidf_vectorizer: instance of scikit-learn TfidfVectorizer, calculates lexical similariry for documents\n    according to TF-IDF metric\n    :return: two dictionaries:\n        {condition: lex_sim(gt, cond_resp)},\n        {condition: lex_sim(gt, non_cond_resp)}\n    """"""\n    gt_vs_cond_lex_sim, gt_vs_non_cond_lex_sim = {}, {}\n\n    for condition, condition_id in nn_model.condition_to_index.items():\n        sample_mask_for_condition = testset.condition_ids == condition_id\n        contexts_for_condition = testset.x[sample_mask_for_condition]\n        responses_for_condition = testset.y[sample_mask_for_condition]\n\n        if not responses_for_condition.size:\n            _logger.warning(\'No dataset samples found for condition ""{}"", skip it.\'.format(condition))\n            continue\n\n        gt_responses = transform_token_ids_to_sentences(responses_for_condition, nn_model.index_to_token)\n        conditioned_responses = predict_for_condition_id(nn_model, contexts_for_condition, condition_id)\n        non_conditioned_responses = predict_for_condition_id(nn_model, contexts_for_condition, condition_id=None)\n\n        gt_vs_cond_lex_sim[condition] = \\\n            calculate_lexical_similarity(gt_responses, conditioned_responses, tfidf_vectorizer)\n\n        gt_vs_non_cond_lex_sim[condition] = \\\n            calculate_lexical_similarity(gt_responses, non_conditioned_responses, tfidf_vectorizer)\n\n    return gt_vs_cond_lex_sim, gt_vs_non_cond_lex_sim\n\n\nif __name__ == \'__main__\':\n    nn_model = get_trained_model()\n    eval_datasets = load_datasets(nn_model.token_to_index, nn_model.condition_to_index)\n\n    print(\'\\nPerplexity on datasets:\')\n    for dataset, perplexity in calc_perplexity_metrics(nn_model, eval_datasets).items():\n        print(\'\\t{}: \\t{:.1f}\'.format(dataset, perplexity))\n\n    cond_to_ppl_conditioned, cond_to_ppl_not_conditioned = \\\n        calc_perplexity_for_conditions(nn_model, eval_datasets.cs_test)\n\n    print(\'\\nPerplexity on conditioned testset for conditions:\')\n    for condition, perplexity in cond_to_ppl_conditioned.items():\n        print(\'\\t{}: \\t{:.1f}\'.format(condition, perplexity))\n\n    print(\'\\nPerplexity on non-conditioned testset for conditions:\')\n    for condition, perplexity in cond_to_ppl_not_conditioned.items():\n        print(\'\\t{}: \\t{:.1f}\'.format(condition, perplexity))\n\n    gt_vs_cond_lex_sim, gt_vs_non_cond_lex_sim = \\\n        calc_lexical_similarity_metrics(nn_model, eval_datasets.cs_test, get_tfidf_vectorizer())\n\n    print(\'\\nLexical similarity, ground-truth vs. conditioned responses:\')\n    for condition, lex_sim in gt_vs_cond_lex_sim.items():\n        print(\'\\t{}: \\t{:.2f}\'.format(condition, lex_sim))\n\n    print(\'\\nLexical similarity, ground-truth vs. non-conditioned responses:\')\n    for condition, lex_sim in gt_vs_non_cond_lex_sim.items():\n        print(\'\\t{}: \\t{:.2f}\'.format(condition, lex_sim))\n'"
tools/quality/prediction_distinctness.py,0,"b""import argparse\nimport os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nimport numpy as np\n\nfrom cakechat.utils.env import init_cuda_env\n\ninit_cuda_env()\n\nfrom cakechat.dialog_model.factory import get_trained_model\nfrom cakechat.dialog_model.quality import calculate_response_ngram_distinctness\nfrom cakechat.utils.dataset_loader import load_datasets, load_questions_set\nfrom cakechat.utils.logger import get_tools_logger\nfrom cakechat.config import PREDICTION_MODES, PREDICTION_MODE_FOR_TESTS\n\n_logger = get_tools_logger(__file__)\n\n\ndef log_distinct_metrics(nn_model, x, condition_ids=None, samples_num=1, ngram_lengths=(1, 2, 3)):\n    for ngram_length in ngram_lengths:\n        scores = [\n            calculate_response_ngram_distinctness(x, nn_model, ngram_len=ngram_length, condition_ids=condition_ids)\n            for _ in range(samples_num)\n        ]\n        scores_mean = np.mean(scores)\n        scores_std = np.std(scores)\n        result = 'distinct {}-gram = {:.5f}'.format(ngram_length, scores_mean)\n        if samples_num > 1:\n            result += ' (std: {:.5f})'.format(scores_std)\n        print(result)\n\n\ndef evaluate_distinctness(args):\n    if args.sample_size > 1 and PREDICTION_MODE_FOR_TESTS == PREDICTION_MODES.beamsearch:\n        _logger.waring('Using sample_size > 1 is meaningless with prediction_mode=\\'beamsearch\\' because there\\'s no '\n                       'randomness in the prediction. Use sample_size=1 instead.')\n\n    nn_model = get_trained_model()\n\n    if args.validation_only:\n        validation = load_questions_set(nn_model.token_to_index)\n        validation_set_name = 'context free questions'\n    else:\n        eval_datasets = load_datasets(nn_model.token_to_index, nn_model.condition_to_index)\n        validation = eval_datasets.cf_validation\n        cs_test = eval_datasets.cs_test\n        cs_test_one_condition = eval_datasets.cs_test_one_condition\n\n        validation_set_name = 'validation set without conditions'\n\n        _logger.info('Evaluating distinctness for context sensitive testset without conditions')\n        log_distinct_metrics(nn_model, cs_test.x, samples_num=args.sample_size)\n\n        _logger.info('Evaluating distinctness for context sensitive testset with conditions')\n        log_distinct_metrics(nn_model, cs_test.x, cs_test.condition_ids, samples_num=args.sample_size)\n\n        _logger.info('Evaluating distinctness for defined-conditions-subset without conditions')\n        log_distinct_metrics(nn_model, cs_test_one_condition.x, samples_num=args.sample_size)\n\n        _logger.info('Evaluating distinctness for defined-conditions-subset with conditions')\n        log_distinct_metrics(\n            nn_model, cs_test_one_condition.x, cs_test_one_condition.condition_ids, samples_num=args.sample_size)\n\n    _logger.info('Evaluating distinctness for {}'.format(validation_set_name))\n    log_distinct_metrics(nn_model, validation.x, samples_num=args.sample_size)\n\n\nif __name__ == '__main__':\n    argparser = argparse.ArgumentParser()\n\n    argparser.add_argument(\n        '-s', '--sample_size', action='store', default=1, type=int, help='Number of samples to average over')\n\n    argparser.add_argument(\n        '-v',\n        '--validation_only',\n        action='store_true',\n        help='Evaluate on the validation set only (useful if you are impatient)')\n\n    args = argparser.parse_args()\n\n    evaluate_distinctness(args)\n"""
tools/quality/ranking_quality.py,0,"b""import os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom cakechat.utils.env import init_cuda_env\n\ninit_cuda_env()\n\nfrom collections import defaultdict\n\nfrom cakechat.config import INPUT_SEQUENCE_LENGTH, INPUT_CONTEXT_SIZE, OUTPUT_SEQUENCE_LENGTH, TEST_CORPUS_NAME, \\\n    TEST_DATA_DIR\nfrom cakechat.dialog_model.inference.utils import get_sequence_score\nfrom cakechat.dialog_model.quality import compute_retrieval_metric_mean, compute_average_precision, compute_recall_k\nfrom cakechat.dialog_model.model_utils import transform_lines_to_token_ids, transform_contexts_to_token_ids\nfrom cakechat.dialog_model.factory import get_trained_model\nfrom cakechat.utils.text_processing import get_tokens_sequence\nfrom cakechat.utils.files_utils import load_file\nfrom cakechat.utils.data_structures import flatten\n\n\ndef _read_testset():\n    corpus_path = os.path.join(TEST_DATA_DIR, '{}.txt'.format(TEST_CORPUS_NAME))\n    test_lines = load_file(corpus_path)\n\n    testset = defaultdict(set)\n    for i in range(0, len(test_lines) - 1, 2):\n        context = test_lines[i].strip()\n        response = test_lines[i + 1].strip()\n        testset[context].add(response)\n\n    return testset\n\n\ndef _get_context_to_weighted_responses(nn_model, testset, all_utterances):\n    token_to_index = nn_model.token_to_index\n\n    all_utterances_ids = transform_lines_to_token_ids(\n        list(map(get_tokens_sequence, all_utterances)), token_to_index, OUTPUT_SEQUENCE_LENGTH, add_start_end=True)\n\n    context_to_weighted_responses = {}\n\n    for context in testset:\n        context_tokenized = get_tokens_sequence(context)\n        repeated_context_ids = transform_contexts_to_token_ids(\n            [[context_tokenized]] * len(all_utterances), token_to_index, INPUT_SEQUENCE_LENGTH, INPUT_CONTEXT_SIZE)\n\n        scores = get_sequence_score(nn_model, repeated_context_ids, all_utterances_ids)\n\n        context_to_weighted_responses[context] = dict(zip(all_utterances, scores))\n\n    return context_to_weighted_responses\n\n\ndef _compute_metrics(model, testset):\n    all_utterances = list(flatten(testset.values(), set))  # Get all unique responses\n    context_to_weighted_responses = _get_context_to_weighted_responses(model, testset, all_utterances)\n\n    test_set_size = len(all_utterances)\n    metrics = {\n        'mean_ap':\n            compute_retrieval_metric_mean(\n                compute_average_precision, testset, context_to_weighted_responses, top_count=test_set_size),\n        'mean_recall@10':\n            compute_retrieval_metric_mean(compute_recall_k, testset, context_to_weighted_responses, top_count=10),\n        'mean_recall@25%':\n            compute_retrieval_metric_mean(\n                compute_recall_k, testset, context_to_weighted_responses, top_count=test_set_size // 4)\n    }\n\n    print('Test set size = {}'.format(test_set_size))\n    for metric_name, metric_value in metrics.items():\n        print('{} = {}'.format(metric_name, metric_value))\n\n\nif __name__ == '__main__':\n    nn_model = get_trained_model()\n    testset = _read_testset()\n    _compute_metrics(nn_model, testset)\n"""
cakechat/api/v1/__init__.py,0,b''
cakechat/api/v1/server.py,0,"b'from flask import Flask, request, jsonify\n\nfrom cakechat.api.response import get_response\nfrom cakechat.api.utils import get_api_error_response, parse_dataset_param\nfrom cakechat.config import EMOTIONS_TYPES, DEFAULT_CONDITION\nfrom cakechat.utils.logger import get_logger\n\n_logger = get_logger(__name__)\n\napp = Flask(__name__)\n\n\n@app.route(\'/cakechat_api/v1/actions/get_response\', methods=[\'POST\'])\ndef get_model_response():\n    params = request.get_json()\n    _logger.info(\'request params: {}\'.format(params))\n\n    try:\n        dialog_context = parse_dataset_param(params, param_name=\'context\')\n    except KeyError as e:\n        return get_api_error_response(\'Malformed request, no ""{}"" param was found\'.format(e), 400, _logger)\n    except ValueError as e:\n        return get_api_error_response(\'Malformed request: {}\'.format(e), 400, _logger)\n\n    emotion = params.get(\'emotion\', DEFAULT_CONDITION)\n    if emotion not in EMOTIONS_TYPES:\n        return get_api_error_response(\n            \'Malformed request, emotion param ""{}"" is not in emotion list {}\'.format(emotion, list(EMOTIONS_TYPES)),\n            400, _logger)\n\n    response = get_response(dialog_context, emotion)\n    _logger.info(\'Given response: ""{}"" for context: {}; emotion ""{}""\'.format(response, dialog_context, emotion))\n\n    return jsonify({\'response\': response}), 200\n\n\n@app.errorhandler(Exception)\ndef on_exception(exception):\n    return get_api_error_response(\'Can\\\'t process request: {}\'.format(exception), 500, _logger)\n'"
cakechat/dialog_model/inference/__init__.py,0,"b'from cakechat.dialog_model.inference.utils import get_sequence_log_probs, get_sequence_score_by_thought_vector, \\\n    get_sequence_score\nfrom cakechat.dialog_model.inference.predict import get_nn_response_ids, get_nn_responses, warmup_predictor\nfrom cakechat.dialog_model.inference.service_tokens import ServiceTokensIDs\n'"
cakechat/dialog_model/inference/factory.py,0,"b'from cakechat.config import PREDICTION_MODES\nfrom cakechat.dialog_model.inference.candidates import BeamsearchCandidatesGenerator, SamplingCandidatesGenerator\nfrom cakechat.dialog_model.inference.predictor import Predictor\nfrom cakechat.dialog_model.inference.reranking import DummyReranker, MMIReranker\n\n\ndef predictor_factory(nn_model, mode, config):\n    """"""\n\n    :param nn_model: Model used for predicting\n    :param mode: Prediction mode: \'sampling\', \'sampling-reranking\' or \'candidates\'\n    :param config: All additional prediction parameters. See PredictionConfig for the details.\n    :return: BasePredictor descendant with predict_response() method implemented.\n    """"""\n    if mode not in PREDICTION_MODES:\n        raise ValueError(\'Unknown prediction mode {}. Use one of the following: {}.\'.format(\n            mode, list(PREDICTION_MODES)))\n\n    if mode in [PREDICTION_MODES.beamsearch, PREDICTION_MODES.beamsearch_reranking]:\n        candidates_generator = BeamsearchCandidatesGenerator(nn_model, config[\'beam_size\'],\n                                                             config[\'repetition_penalization_coefficient\'])\n    else:\n        candidates_generator = SamplingCandidatesGenerator(nn_model, config[\'temperature\'], config[\'samples_num\'],\n                                                           config[\'repetition_penalization_coefficient\'])\n\n    if mode in [PREDICTION_MODES.beamsearch_reranking, PREDICTION_MODES.sampling_reranking]:\n        if config[\'mmi_reverse_model_score_weight\'] <= 0:\n            raise ValueError(\'mmi_reverse_model_score_weight should be > 0 for reranking mode\')\n\n        reranker = MMIReranker(nn_model, nn_model.reverse_model, config[\'mmi_reverse_model_score_weight\'],\n                               config[\'repetition_penalization_coefficient\'])\n    else:\n        reranker = DummyReranker()\n\n    return Predictor(nn_model, candidates_generator, reranker)\n'"
cakechat/dialog_model/inference/predict.py,0,"b'from cakechat.config import MAX_PREDICTIONS_LENGTH, BEAM_SIZE, MMI_REVERSE_MODEL_SCORE_WEIGHT, DEFAULT_TEMPERATURE, \\\n    SAMPLES_NUM_FOR_RERANKING, PREDICTION_MODES, REPETITION_PENALIZE_COEFFICIENT\nfrom cakechat.dialog_model.inference.factory import predictor_factory\nfrom cakechat.dialog_model.model_utils import transform_token_ids_to_sentences\nfrom cakechat.utils.logger import get_logger\n\n_logger = get_logger(__name__)\n\n\nclass PredictionConfig(object):\n    def __init__(self, mode, **kwargs):\n        self.mode = mode\n        self.repetition_penalization_coefficient = \\\n            kwargs.get(\'repetition_penalization_coefficient\', REPETITION_PENALIZE_COEFFICIENT)\n\n        if self.mode == PREDICTION_MODES.sampling:\n            self.temperature = kwargs.get(\'temperature\', DEFAULT_TEMPERATURE)\n            self.samples_num = kwargs.get(\'samples_num\', 1)\n        elif self.mode == PREDICTION_MODES.beamsearch:\n            self.beam_size = kwargs.get(\'beam_size\', BEAM_SIZE)\n        elif self.mode == PREDICTION_MODES.sampling_reranking:\n            self.temperature = kwargs.get(\'temperature\', DEFAULT_TEMPERATURE)\n            self.samples_num = kwargs.get(\'samples_num\', SAMPLES_NUM_FOR_RERANKING)\n            self.mmi_reverse_model_score_weight = kwargs.get(\'mmi_reverse_model_score_weight\',\n                                                             MMI_REVERSE_MODEL_SCORE_WEIGHT)\n        elif self.mode == PREDICTION_MODES.beamsearch_reranking:\n            self.beam_size = kwargs.get(\'beam_size\', BEAM_SIZE)\n            self.mmi_reverse_model_score_weight = kwargs.get(\'mmi_reverse_model_score_weight\',\n                                                             MMI_REVERSE_MODEL_SCORE_WEIGHT)\n\n    def get_options_dict(self):\n        return self.__dict__\n\n    def __str__(self):\n        return str(self.__dict__)\n\n\ndef warmup_predictor(nn_model, prediction_mode):\n    if prediction_mode in {PREDICTION_MODES.beamsearch_reranking, PREDICTION_MODES.sampling_reranking}:\n        prediction_config = PredictionConfig(prediction_mode)\n        predictor_factory(nn_model, prediction_mode, prediction_config.get_options_dict())\n\n\ndef get_nn_response_ids(context_token_ids,\n                        nn_model,\n                        mode,\n                        output_candidates_num=1,\n                        output_seq_len=MAX_PREDICTIONS_LENGTH,\n                        condition_ids=None,\n                        **kwargs):\n    """"""\n    Predicts several responses for every context.\n\n    :param context_token_ids: np.array; shape (batch_size, context_size, context_len); dtype=int\n        Represents all tokens ids to use for predicting\n    :param nn_model: CakeChatModel\n    :param mode: one of PREDICTION_MODES mode\n    :param output_candidates_num: Number of candidates to generate.\n        When mode is either \'beamsearch\', \'beamsearch-reranking\'  or \'sampling-reranking\', the candidates with the\n        highest score are returned. When mode is \'sampling\', the candidates_num of samples are generated independently.\n    :param condition_ids: List with ids of conditions responding for each context.\n    :param output_seq_len: Number of tokens to generate.\n    :param kwargs: Other prediction parameters, passed into predictor constructor.\n        Might be different depending on mode. See PredictionConfig for the details.\n    :return: np.array; shape (batch_size, output_candidates_num, output_seq_len); dtype=int\n        Generated predictions.\n    """"""\n    if mode == PREDICTION_MODES.sampling:\n        kwargs[\'samples_num\'] = output_candidates_num\n\n    prediction_config = PredictionConfig(mode, **kwargs)\n    _logger.debug(\'Generating predicted response for the following params: {}\'.format(prediction_config))\n\n    predictor = predictor_factory(nn_model, mode, prediction_config.get_options_dict())\n    responses = predictor.predict_responses(context_token_ids, output_seq_len, condition_ids, output_candidates_num)\n\n    return responses\n\n\ndef get_nn_responses(context_token_ids,\n                     nn_model,\n                     mode,\n                     output_candidates_num=1,\n                     output_seq_len=MAX_PREDICTIONS_LENGTH,\n                     condition_ids=None,\n                     **kwargs):\n    """"""\n    Predicts output_candidates_num responses for every context and returns them in form of strings.\n    See get_nn_response_ids for the details.\n\n    :param context_token_ids: numpy array of integers, shape (contexts_num, INPUT_CONTEXT_SIZE, INPUT_SEQUENCE_LENGTH)\n    :param nn_model: trained model\n    :param mode: prediction mode, see const PREDICTION_MODES\n    :param output_candidates_num: number of responses to be generated for each context\n    :param output_seq_len: max length of generated responses\n    :param condition_ids: extra info to be taken into account while generating response (emotion, for example)\n\n    :return: list of lists of strings, shape (contexts_num, output_candidates_num)\n    """"""\n\n    response_tokens_ids = get_nn_response_ids(context_token_ids, nn_model, mode, output_candidates_num, output_seq_len,\n                                              condition_ids, **kwargs)\n    # shape (contexts_num, output_candidates_num, output_seq_len), numpy array of integers\n\n    responses = [\n        transform_token_ids_to_sentences(response_candidates_tokens_ids, nn_model.index_to_token)\n        for response_candidates_tokens_ids in response_tokens_ids\n    ]\n    # responses shape (contexts_num, output_candidates_num), list of lists of strings\n\n    return responses\n'"
cakechat/dialog_model/inference/predictor.py,0,"b'import numpy as np\n\nfrom cakechat.config import INTX\n\n\nclass Predictor(object):\n    def __init__(self, nn_model, candidates_generator, reranker):\n        self._nn_model = nn_model\n        self._generator = candidates_generator\n        self._reranker = reranker\n\n    @staticmethod\n    def _select_best_candidates(reranked_candidates, candidates_num):\n        """"""\n        We need this complicated implementation to handle different number of generated candidates for each sample.\n        If for some context we generated less then candidates_num candidates, we fill this responses with pads.\n        """"""\n        batch_size = len(reranked_candidates)\n        # reranked_candidates is a list of lists (we need too keep it this way because we can have different number\n        # of candidates for each context), so we can\'t just write reranked_candidates.shape[2]\n        output_seq_len = reranked_candidates[0][0].size\n        result = np.zeros((batch_size, candidates_num, output_seq_len), dtype=INTX)\n        # Loop here instead of slices because number of candidates for each context may vary here\n        for i in range(batch_size):\n            for j, candidate in enumerate(reranked_candidates[i]):\n                if j >= candidates_num:\n                    break\n                result[i][j] = reranked_candidates[i][j]\n        return result\n\n    def predict_responses(self, context_token_ids, output_seq_len, condition_ids=None, candidates_num=1):\n        all_candidates = self._generator.generate_candidates(context_token_ids, condition_ids, output_seq_len)\n        reranked_candidates = self._reranker.rerank_candidates(context_token_ids, all_candidates, condition_ids)\n        selected_responses = self._select_best_candidates(reranked_candidates, candidates_num)\n        return selected_responses\n'"
cakechat/dialog_model/inference/reranking.py,0,"b'from abc import ABCMeta, abstractmethod\nfrom itertools import zip_longest\n\nimport numpy as np\n\nfrom cakechat.dialog_model.inference.service_tokens import ServiceTokensIDs\nfrom cakechat.dialog_model.inference.utils import get_sequence_score_by_thought_vector, get_sequence_score, \\\n    get_thought_vectors\nfrom cakechat.dialog_model.model_utils import reverse_nn_input\nfrom cakechat.utils.data_types import Dataset\nfrom cakechat.utils.logger import get_logger\nfrom cakechat.utils.profile import timer\n\n_logger = get_logger(__name__)\n\n\nclass AbstractCandidatesReranker(object, metaclass=ABCMeta):\n    @abstractmethod\n    def rerank_candidates(self, contexts, all_candidates, condition_ids):\n        pass\n\n\nclass DummyReranker(AbstractCandidatesReranker):\n    def rerank_candidates(self, contexts, all_candidates, condition_ids):\n        return all_candidates\n\n\nclass MMIReranker(AbstractCandidatesReranker):\n    """"""\n    Ranks candidates based on the MMI-score:\n     score = (1 - \\lambda) log p(y|x) + \\lambda log p(x|y) - \\beta R_y,\n    where\n     - x is dialogue context;\n     - y is a candidate response;\n     - R_y is the number of repeated tokens used in a candidate response\n     - \\lambda, \\beta - hyperparameters. \\beta = log(REPETITION_PENALIZE_COEFFICIENT)\n\n    Score formula is based on (9) https://arxiv.org/pdf/1510.03055v3.pdf\n    """"""\n\n    def __init__(self, nn_model, reverse_model, mmi_reverse_model_score_weight, repetition_penalization_coefficient):\n        self._nn_model = nn_model\n        if mmi_reverse_model_score_weight != 0.0 and reverse_model is None:\n            raise ValueError(\'Reverse model has to be supplied to MMI-reranker. \'\n                             \'If you don\\\'t have one, set mmi_reverse_model_score_weight to 0.\')\n        self._reverse_model_score_weight = mmi_reverse_model_score_weight\n        self._reverse_model = reverse_model\n        self._service_tokens_ids = ServiceTokensIDs(nn_model.token_to_index)\n        self._log_repetition_penalization_coefficient = np.log(repetition_penalization_coefficient)\n\n    def _compute_likelihood_of_output_given_input(self, thought_vector, candidates, condition_id):\n        # Repeat to get same thought vector for each candidate\n        thoughts_batch = np.repeat(thought_vector, candidates.shape[0], axis=0)\n        return get_sequence_score_by_thought_vector(self._nn_model, thoughts_batch, candidates, condition_id)\n\n    def _compute_likelihood_of_input_given_output(self, context, candidates, condition_id):\n        # Repeat to get same context for each candidate\n        repeated_context = np.repeat(context, candidates.shape[0], axis=0)\n        reversed_dataset = reverse_nn_input(\n            Dataset(x=repeated_context, y=candidates, condition_ids=None), self._service_tokens_ids)\n        return get_sequence_score(self._reverse_model, reversed_dataset.x, reversed_dataset.y, condition_id)\n\n    def _compute_num_repetitions(self, candidates):\n        skip_tokens_ids = \\\n            self._service_tokens_ids.special_tokens_ids + self._service_tokens_ids.non_penalizable_tokens_ids\n        result = []\n        for candidate in candidates:\n            penalizable_tokens = candidate[~np.in1d(candidate, skip_tokens_ids)]  # All tokens not in skip_tokens_ids\n            num_repetitions = penalizable_tokens.size - np.unique(penalizable_tokens).size\n            result.append(num_repetitions)\n        return np.array(result)\n\n    def _compute_candidates_scores(self, context, candidates, condition_id):\n        context = context[np.newaxis, :]  # from (seq_len,) to (1 x seq_len)\n        thought_vector = get_thought_vectors(self._nn_model, context)\n\n        candidates_num_repetitions = self._compute_num_repetitions(candidates)\n\n        if self._reverse_model_score_weight == 0.0:\n            candidates_scores = self._compute_likelihood_of_output_given_input(thought_vector, candidates, condition_id)\n        elif self._reverse_model_score_weight == 1.0:  # Don\'t compute the likelihood in this case for performance\n            candidates_scores = self._compute_likelihood_of_input_given_output(context, candidates, condition_id)\n        else:\n            candidates_likelihood = self._compute_likelihood_of_output_given_input(thought_vector, candidates,\n                                                                                   condition_id)\n            candidates_reverse_likelihood = self._compute_likelihood_of_input_given_output(\n                context, candidates, condition_id)\n            candidates_scores = (1 - self._reverse_model_score_weight) * candidates_likelihood + \\\n                                self._reverse_model_score_weight * candidates_reverse_likelihood\n\n        candidates_scores -= self._log_repetition_penalization_coefficient * candidates_num_repetitions\n        return candidates_scores\n\n    @timer\n    def rerank_candidates(self, contexts, all_candidates, condition_ids):\n        condition_ids = [] if condition_ids is None else condition_ids  # For izip_lingest\n        candidates_scores = [\n            self._compute_candidates_scores(context, candidates, condition_id)\n            for context, candidates, condition_id in zip_longest(contexts, all_candidates, condition_ids)\n        ]\n        scores_order = [np.argsort(-np.array(scores)) for scores in candidates_scores]\n        batch_size = len(contexts)\n        # reranked_candidates[i][j] = j-th best response for i-th question\n        reranked_candidates = [\n            [all_candidates[i][j] for j in scores_order[i]] for i in range(batch_size)  # yapf: disable\n        ]\n        return reranked_candidates\n'"
cakechat/dialog_model/inference/service_tokens.py,0,"b'from cakechat.config import NON_PENALIZABLE_TOKENS\nfrom cakechat.utils.offense_detector.config import OFFENSIVE_PHRASES_PATH\nfrom cakechat.utils.offense_detector import OffenseDetector\nfrom cakechat.utils.text_processing import SPECIAL_TOKENS\n\n_offense_detector = OffenseDetector(OFFENSIVE_PHRASES_PATH)\n\n\nclass ServiceTokensIDs(object):\n    """"""\n    Handles computation of indices of all special tokens needed for predicting and reranking of responses\n    """"""\n\n    def __init__(self, token_to_index):\n        self.start_token_id = token_to_index[SPECIAL_TOKENS.START_TOKEN]\n        self.eos_token_id = token_to_index[SPECIAL_TOKENS.EOS_TOKEN]\n        self.pad_token_id = token_to_index[SPECIAL_TOKENS.PAD_TOKEN]\n        self.unk_token_id = token_to_index[SPECIAL_TOKENS.UNKNOWN_TOKEN]\n        self.special_tokens_ids = [self.start_token_id, self.eos_token_id, self.pad_token_id, self.unk_token_id]\n\n        # Get first token for each offensive ngram\n        offensive_tokens = [ngram[0] for ngram in _offense_detector.offensive_ngrams if len(ngram) == 1]\n        # We don\'t penalize for repeating these tokens:\n        self.non_penalizable_tokens_ids = [token_to_index[w] for w in NON_PENALIZABLE_TOKENS if w in token_to_index]\n        # These tokens are banned during the prediction:\n        offensive_tokens_ids = [token_to_index[w] for w in offensive_tokens if w in token_to_index]\n        self.banned_tokens_ids = offensive_tokens_ids + [self.unk_token_id]\n'"
cakechat/dialog_model/inference/utils.py,0,"b'import numpy as np\n\nfrom cakechat.config import BATCH_SIZE, DEFAULT_CONDITION, INTX\nfrom cakechat.dialog_model.model_utils import get_training_batch\n\n\ndef _predict_batch_by_batch(predict_fn, batched_inputs, non_batched_inputs=None, batch_size=BATCH_SIZE, num_outputs=1):\n    """"""\n    Splits prediction for big dataset in order to save GPU memory.\n    Equivalent to predict_fn(*batched_inputs + non_batched_inputs).\n\n    predict_fn: compiled keras predict function.\n    batched_inputs:\n        Inputs that we split into batches. On each iteration, we only pass one batch of this data into predict_fn.\n    non_batched_inputs:\n        Inputs that we do not split into batches. These inputs are the same for each call of predict_fn\n    batch_size: int\n        Size of each batch that we split our batched_inputs into\n    num_ouputs: int, default=1\n        Number of items returned on each call of predict_fn.\n    """"""\n    if non_batched_inputs is None:\n        non_batched_inputs = []\n\n    results = [[] for _ in range(num_outputs)]\n\n    for inputs_batch in get_training_batch(batched_inputs, batch_size):\n        args = list(inputs_batch) + non_batched_inputs\n        cur_result = predict_fn(*args)\n        if num_outputs > 1:\n            for i in range(num_outputs):\n                results[i].append(cur_result[i])\n        else:\n            results[0].append(cur_result)\n\n    if num_outputs > 1:\n        return tuple(np.concatenate(results[i]) for i in range(num_outputs))\n    else:\n        return np.concatenate(results[0])\n\n\ndef _handle_condition_ids(condition_ids, condition_to_index, num_responses):\n    """"""\n    Returns condition_ids preprocessed to match the shape of responses batch.\n    Specifically:\n        If condition_ids is None it is replaced with default condition index repeated num_responses times.\n        If condition_ids is an one index, it is repeated num_responses times.\n        If condition_ids is an array, assert that the shape is right.\n    """"""\n    if condition_ids is None:\n        return np.array([condition_to_index[DEFAULT_CONDITION]] * num_responses, dtype=INTX)\n\n    condition_ids = np.array(condition_ids, dtype=INTX)\n    if len(condition_ids.shape) == 0:  # If condition_ids is one number\n        return np.repeat(condition_ids[np.newaxis], num_responses, axis=0)\n    elif condition_ids.shape != (num_responses, ):\n        raise ValueError(\'Shape of condition_ids is {} and is not equal to (num_resonses, )={}\'.format(\n            condition_ids.shape, (num_responses, )))\n    else:\n        return condition_ids\n\n\ndef _predict_one_step(predict_fn,\n                      condition_to_index,\n                      thought_vectors,\n                      prev_hidden_states,\n                      prev_tokens_ids,\n                      condition_ids=None,\n                      temperature=1.0):\n    condition_ids = _handle_condition_ids(condition_ids, condition_to_index, thought_vectors.shape[0])\n    # We need newaxis to match the expected shape of an argument passed to predict_fn function\n    prev_tokens_ids = prev_tokens_ids[:, np.newaxis]\n\n    hidden_states, token_scores = \\\n        _predict_batch_by_batch(\n            predict_fn,\n            batched_inputs=[thought_vectors, prev_hidden_states, prev_tokens_ids, condition_ids],\n            non_batched_inputs=[temperature],\n            num_outputs=2)\n\n    # token_scores is batch_size x num_tokens x vocab_size.\n    # num_tokens is always 1, so we slice out the corresponding dimensionality.\n    return hidden_states, token_scores[:, 0, :]\n\n\ndef get_sequence_score_by_thought_vector(nn_model, thought_vectors, y_ids, condition_ids=None):\n    num_responses = thought_vectors.shape[0]\n    condition_ids = _handle_condition_ids(condition_ids, nn_model.condition_to_index, num_responses)\n    return _predict_batch_by_batch(\n        nn_model.predict_sequence_score_by_thought_vector, batched_inputs=[thought_vectors, y_ids, condition_ids])\n\n\ndef get_sequence_score(nn_model, x_ids, y_ids, condition_ids=None):\n    num_responses = x_ids.shape[0]\n    condition_ids = _handle_condition_ids(condition_ids, nn_model.condition_to_index, num_responses)\n    return _predict_batch_by_batch(nn_model.predict_sequence_score, batched_inputs=[x_ids, y_ids, condition_ids])\n\n\ndef get_sequence_log_probs(nn_model, x_ids, y_ids, condition_ids=None):\n    num_responses = x_ids.shape[0]\n    condition_ids = _handle_condition_ids(condition_ids, nn_model.condition_to_index, num_responses)\n    return _predict_batch_by_batch(nn_model.predict_log_prob, batched_inputs=[x_ids, y_ids, condition_ids])\n\n\ndef get_thought_vectors(nn_model, x_ids):\n    return _predict_batch_by_batch(nn_model.get_thought_vectors, batched_inputs=[x_ids])\n\n\ndef get_next_token_prob_one_step(nn_model, thoughts_batch, hidden_states_batch, prev_tokens_ids, condition_ids,\n                                 **kwargs):\n    return _predict_one_step(nn_model.predict_prob_one_step, nn_model.condition_to_index, thoughts_batch,\n                             hidden_states_batch, prev_tokens_ids, condition_ids, **kwargs)\n\n\ndef get_next_token_log_prob_one_step(nn_model, thoughts_batch, hidden_states_batch, prev_tokens_ids, condition_ids,\n                                     **kwargs):\n    return _predict_one_step(nn_model.predict_log_prob_one_step, nn_model.condition_to_index, thoughts_batch,\n                             hidden_states_batch, prev_tokens_ids, condition_ids, **kwargs)\n'"
cakechat/dialog_model/quality/__init__.py,0,"b'from cakechat.dialog_model.quality.metrics.distinctness import calculate_response_ngram_distinctness\nfrom cakechat.dialog_model.quality.metrics.lexical_simlarity import calculate_lexical_similarity, get_tfidf_vectorizer\nfrom cakechat.dialog_model.quality.metrics.perplexity import calculate_model_mean_perplexity\nfrom cakechat.dialog_model.quality.metrics.ranking import compute_average_precision, compute_recall_k, \\\n    compute_retrieval_metric_mean\nfrom cakechat.dialog_model.quality.logging import calculate_and_log_val_metrics, log_predictions\n'"
cakechat/dialog_model/quality/logging.py,0,"b'import csv\nimport os\nfrom datetime import datetime\n\nimport pandas as pd\n\nfrom cakechat.config import PREDICTION_MODE_FOR_TESTS, MAX_PREDICTIONS_LENGTH\nfrom cakechat.dialog_model.inference import get_nn_responses\nfrom cakechat.dialog_model.model_utils import transform_context_token_ids_to_sentences\nfrom cakechat.dialog_model.quality import calculate_model_mean_perplexity, calculate_response_ngram_distinctness\nfrom cakechat.utils.files_utils import ensure_dir\nfrom cakechat.utils.logger import get_logger\n\n_logger = get_logger(__name__)\n\n\ndef calculate_and_log_val_metrics(nn_model,\n                                  context_sensitive_val,\n                                  context_free_val,\n                                  prediction_mode=PREDICTION_MODE_FOR_TESTS,\n                                  calculate_ngram_distance=True):\n    metric_name_to_value = {\n        \'context_free_perplexity\': calculate_model_mean_perplexity(nn_model, context_free_val),\n        \'context_sensitive_perplexity\': calculate_model_mean_perplexity(nn_model, context_sensitive_val)\n    }\n\n    if calculate_ngram_distance:\n        for metric_name, ngram_len in [(\'unigram_distinctness\', 1), (\'bigram_distinctness\', 2)]:\n            metric_name_to_value[metric_name] = calculate_response_ngram_distinctness(\n                context_sensitive_val.x,\n                nn_model,\n                ngram_len=ngram_len,\n                mode=prediction_mode,\n                condition_ids=context_sensitive_val.condition_ids)\n\n    for metric_name, metric_value in metric_name_to_value.items():\n        _logger.info(\'Val set {}: {:.3f}\'.format(metric_name, metric_value))\n\n    return metric_name_to_value\n\n\ndef _init_csv_writer(predictions_path, output_seq_len, model_name):\n    with open(predictions_path, \'w\', encoding=\'utf-8\') as fh:\n        csv_writer = csv.writer(fh, delimiter=\'\\t\')\n        csv_writer.writerow([model_name])\n        csv_writer.writerow([\'date: {}\'.format(datetime.now().strftime(\'%Y-%m-%d %H:%M\'))])\n        csv_writer.writerow([\'{} maximum tokens in the response\'.format(output_seq_len)])\n        csv_writer.writerow([\'\'])  # empty row for better readability\n\n\ndef log_predictions(predictions_path,\n                    contexts_token_ids,\n                    nn_model,\n                    prediction_modes,\n                    output_seq_len=MAX_PREDICTIONS_LENGTH,\n                    **kwargs):\n    """"""\n    Generate responses for provided contexts and save the results on the disk. For a given context\n    several responses will be generated - one for each mode from the prediction_modes list.\n\n    :param predictions_path: Generated responses will be saved to this file\n    :param contexts_token_ids: contexts token ids, numpy array of shape (batch_size, context_len, INPUT_SEQUENCE_LENGTH)\n    :param nn_model: instance of CakeChatModel class\n    :param prediction_modes: See PREDICTION_MODES for available options\n    :param output_seq_len: Max number of tokens in generated responses\n    """"""\n    _logger.info(\'Logging responses for test set\')\n\n    # Create all the directories for the prediction path in case they don\'t exist\n    ensure_dir(os.path.dirname(predictions_path))\n\n    _init_csv_writer(predictions_path, output_seq_len, nn_model.model_name)\n\n    contexts = transform_context_token_ids_to_sentences(contexts_token_ids, nn_model.index_to_token)\n    predictions_data = pd.DataFrame()\n    predictions_data[\'contexts\'] = contexts\n\n    for prediction_mode in prediction_modes:\n        predicted_responses = get_nn_responses(contexts_token_ids, nn_model, prediction_mode, **kwargs)\n        # list of lists of strings, shape (contexts_num, 1)\n        predicted_responses = [response[0] for response in predicted_responses]\n        # list of strings, shape (contexts_num)\n        predictions_data[prediction_mode] = predicted_responses\n\n    predictions_data.to_csv(predictions_path, sep=\'\\t\', index=False, encoding=\'utf-8\', mode=\'a\', float_format=\'%.2f\')\n\n    _logger.info(\'Dumped {} predicted responses to {}\'.format(len(contexts), predictions_path))\n'"
cakechat/utils/offense_detector/__init__.py,0,b'from cakechat.utils.offense_detector.detector import OffenseDetector\n'
cakechat/utils/offense_detector/config.py,0,"b""import os\nimport pkg_resources\n\nimport cakechat.utils.offense_detector\n\nOFFENSIVE_PHRASES_PATH = pkg_resources.resource_filename(cakechat.utils.offense_detector.__name__,\n                                                         os.path.join('data', 'offensive_phrases.csv'))\n\n"""
cakechat/utils/offense_detector/detector.py,0,"b'import nltk\n\nfrom cakechat.utils.data_structures import flatten\nfrom cakechat.utils.files_utils import load_file\nfrom cakechat.utils.text_processing import get_tokens_sequence\n\n\nclass OffenseDetector(object):\n    def __init__(self, offensive_phrases_path):\n        self._offensive_ngrams = self._build_offensive_ngrams(offensive_phrases_path)\n        self._max_ngram_len = max(map(len, self._offensive_ngrams))\n\n    @property\n    def offensive_ngrams(self):\n        return self._offensive_ngrams\n\n    @staticmethod\n    def _build_offensive_ngrams(offensive_phrases_path):\n        offensive_phrases = load_file(offensive_phrases_path)\n        offensive_ngrams = [tuple(get_tokens_sequence(offensive_phrase)) for offensive_phrase in offensive_phrases]\n        return set(offensive_ngrams)\n\n    def _get_ngrams(self, tokenized_line):\n        ngrams = [nltk.ngrams(tokenized_line, i) for i in range(1, self._max_ngram_len + 1)]\n        return flatten(ngrams, constructor=set)\n\n    def has_offensive_ngrams(self, text):\n        if not isinstance(text, str):\n            raise TypeError(\'""text"" variable must be a string\')\n        tokenized_text = get_tokens_sequence(text)\n        text_ngrams = self._get_ngrams(tokenized_text)\n\n        return bool(text_ngrams & self._offensive_ngrams)\n'"
cakechat/utils/s3/__init__.py,0,"b'from cakechat.utils.s3.bucket import S3Bucket\nfrom cakechat.utils.s3.resolver import S3FileResolver, get_s3_resource, get_s3_model_resolver\n'"
cakechat/utils/s3/bucket.py,0,"b'import os\n\nfrom cakechat.utils.files_utils import ensure_dir\nfrom cakechat.utils.logger import get_logger\n\n\nclass S3Bucket(object):\n    def __init__(self, bucket_client):\n        self._logger = get_logger(__name__)\n        self._bucket_client = bucket_client\n\n    def download(self, remote_file_name, local_file_name):\n        """"""\n        Download file from AWS S3 to the local one\n        """"""\n        # create dir if not exists for storing file from s3\n        ensure_dir(os.path.dirname(local_file_name))\n\n        self._logger.info(\'Getting file %s from AWS S3 and saving it as %s\' % (remote_file_name, local_file_name))\n        self._bucket_client.download_file(remote_file_name, local_file_name)\n        self._logger.info(\'Got file %s from S3\' % remote_file_name)\n\n    def upload(self, local_file_name, remote_file_name):\n        """"""\n        Upload local file to AWS S3 bucket\n        """"""\n        self._logger.info(\'Saving file {} in amazon S3 as {}\'.format(local_file_name, remote_file_name))\n        self._bucket_client.upload_file(local_file_name, remote_file_name)\n        self._logger.info(\'File %s saved to %s on S3\' % (local_file_name, remote_file_name))\n'"
cakechat/utils/s3/resolver.py,0,"b'import os\nfrom functools import partial\n\nimport boto3\nfrom botocore import UNSIGNED\nfrom botocore.client import Config\n\nfrom cakechat.utils.files_utils import AbstractFileResolver, PackageResolver, extract_tar\nfrom cakechat.utils.logger import WithLogger\nfrom cakechat.utils.s3 import S3Bucket\n\n\nclass S3FileResolver(AbstractFileResolver, WithLogger):\n    """"""\n    Tries to download file from AWS S3 if it does not exist locally\n    """"""\n\n    def __init__(self, file_path, bucket_name, remote_dir):\n        super(S3FileResolver, self).__init__(file_path)\n        WithLogger.__init__(self)\n\n        self._bucket_name = bucket_name\n        self._remote_dir = remote_dir\n\n    @staticmethod\n    def init_resolver(**kwargs):\n        """"""\n        Method helping to set once some parameters like bucket_name and remote_dir\n        :param kwargs:\n        :return: partially initialized class object\n        """"""\n        return partial(S3FileResolver, **kwargs)\n\n    def _get_remote_path(self):\n        return \'%s/%s\' % (self._remote_dir, os.path.basename(self._file_path))\n\n    def _resolve(self):\n        remote_path = self._get_remote_path()\n\n        try:\n            bucket = S3Bucket(get_s3_resource().Bucket(self._bucket_name))\n            bucket.download(remote_path, self._file_path)\n            return True\n        except Exception as e:\n            self._logger.warn(\'File can not be downloaded from AWS S3 because: %s\' % str(e))\n\n        return False\n\n\ndef get_s3_resource():\n    return boto3.resource(\'s3\', config=Config(signature_version=UNSIGNED))\n\n\ndef get_s3_model_resolver(bucket_name, remote_dir):\n    return PackageResolver.init_resolver(\n        package_file_resolver_factory=S3FileResolver.init_resolver(bucket_name=bucket_name, remote_dir=remote_dir),\n        package_file_ext=\'tar.gz\',\n        package_extractor=extract_tar)\n'"
cakechat/utils/text_processing/__init__.py,0,"b'from cakechat.utils.text_processing.str_processor import get_tokens_sequence, replace_out_of_voc_tokens, \\\n    prettify_response\nfrom cakechat.utils.text_processing.config import SPECIAL_TOKENS\nfrom cakechat.utils.text_processing.utils import get_processed_corpus_path, get_index_to_token_path, \\\n    get_index_to_condition_path, load_index_to_item\nfrom cakechat.utils.text_processing.dialog import get_flatten_dialogs, get_alternated_dialogs_lines, \\\n    get_dialog_lines_and_conditions, load_processed_dialogs_from_json\nfrom cakechat.utils.text_processing.corpus_iterator import FileTextLinesIterator, ProcessedLinesIterator\n'"
cakechat/utils/text_processing/config.py,0,"b""from cakechat.utils.data_structures import create_namedtuple_instance\n\nSPECIAL_TOKENS = create_namedtuple_instance(\n    'SPECIAL_TOKENS', PAD_TOKEN='_pad_', UNKNOWN_TOKEN='_unk_', START_TOKEN='_start_', EOS_TOKEN='_end_')\n\nDIALOG_TEXT_FIELD = 'text'\nDIALOG_CONDITION_FIELD = 'condition'\n"""
cakechat/utils/text_processing/corpus_iterator.py,0,"b'import json\nfrom copy import copy\n\nfrom cakechat.utils.logger import get_logger\n\n_logger = get_logger(__name__)\n\n\nclass FileTextLinesIterator(object):\n    def __init__(self, filename):\n        self._filename = filename\n\n    def __iter__(self):\n        for line in open(self._filename, \'r\', encoding=\'utf-8\'):\n            yield line.strip()\n\n    def __copy__(self):\n        return FileTextLinesIterator(self._filename)\n\n\nclass ProcessedLinesIterator(object):\n    def __init__(self, lines_iter, processing_callbacks=None):\n        self._lines_iter = lines_iter\n        self._processing_callbacks = processing_callbacks if processing_callbacks else []\n\n    def __iter__(self):\n        for line in self._lines_iter:\n            for callback in self._processing_callbacks:\n                line = callback(line)\n            yield line\n\n    def __copy__(self):\n        return ProcessedLinesIterator(copy(self._lines_iter), self._processing_callbacks)\n\n\nclass JsonTextLinesIterator(object):\n    def __init__(self, text_lines_iter):\n        self._text_lines_iter = text_lines_iter\n\n    def __iter__(self):\n        for line in self._text_lines_iter:\n            try:\n                yield json.loads(line.strip())\n            except ValueError:\n                _logger.warning(\'Skipped invalid json object: ""{}""\'.format(line.strip()))\n                continue\n\n    def __copy__(self):\n        return JsonTextLinesIterator(copy(self._text_lines_iter))\n'"
cakechat/utils/text_processing/dialog.py,0,"b'from operator import itemgetter\n\nfrom cakechat.utils.tee_file import file_buffered_tee\nfrom cakechat.utils.text_processing.config import DIALOG_CONDITION_FIELD, DIALOG_TEXT_FIELD\nfrom cakechat.utils.text_processing.corpus_iterator import JsonTextLinesIterator\n\n\ndef get_flatten_dialogs(dialogs):\n    for dialog in dialogs:\n        for dialog_line in dialog:\n            yield dialog_line\n\n\ndef get_alternated_dialogs_lines(dialogs):\n    for dialog in dialogs:\n        for first_dialog_line, second_dialog_line in zip(dialog, dialog[1:]):\n            yield first_dialog_line\n            yield second_dialog_line\n\n\ndef get_dialog_lines_and_conditions(dialog_lines, text_field_name, condition_field_name):\n    """"""\n    Splits one dialog_lines generator into two generators - one for conditions and one for dialog lines\n    """"""\n    conditions_iter, dialog_lines_iter = file_buffered_tee(\n        map(lambda line: [line[condition_field_name], line[text_field_name]], dialog_lines))\n    conditions_iter = map(itemgetter(0), conditions_iter)\n    dialog_lines_iter = map(itemgetter(1), dialog_lines_iter)\n    return dialog_lines_iter, conditions_iter\n\n\ndef load_processed_dialogs_from_json(lines, text_field_name, condition_field_name):\n    for line_json in JsonTextLinesIterator(lines):\n        yield [{\n            text_field_name: entry[DIALOG_TEXT_FIELD],\n            condition_field_name: entry[DIALOG_CONDITION_FIELD]\n        } for entry in line_json]\n'"
cakechat/utils/text_processing/str_processor.py,0,"b'import re\n\nimport nltk.tokenize\n\nfrom cakechat.utils.text_processing.config import SPECIAL_TOKENS\n\n_END_CHARS = \'.?!\'\n\n_tokenizer = nltk.tokenize.RegexpTokenizer(pattern=\'\\w+|[^\\w\\s]\')\n\n\ndef get_tokens_sequence(text, lower=True, check_unicode=True):\n    if check_unicode and not isinstance(text, str):\n        raise TypeError(\'Text object should be unicode type. Got instead ""{}"" of type {}\'.format(text, type(text)))\n\n    if not text.strip():\n        return []\n\n    if lower:\n        text = text.lower()\n\n    tokens = _tokenizer.tokenize(text)\n\n    return tokens\n\n\ndef replace_out_of_voc_tokens(tokens, tokens_voc):\n    return [t if t in tokens_voc else SPECIAL_TOKENS.UNKNOWN_TOKEN for t in tokens]\n\n\ndef _capitalize_first_chars(text):\n    if not text:\n        return text\n\n    chars_pos_to_capitalize = [0] + [m.end() - 1 for m in re.finditer(\'[{}] \\w\'.format(_END_CHARS), text)]\n\n    for char_pos in chars_pos_to_capitalize:\n        text = text[:char_pos] + text[char_pos].upper() + text[char_pos + 1:]\n\n    return text\n\n\ndef prettify_response(response):\n    """"""\n    Prettify chatbot\'s answer removing excessive characters and capitalizing first words of sentences.\n    Before: ""hello world ! nice to meet you , buddy . do you like me ? I \' ve been missing you for a while . . .""\n    After: ""Hello world! Nice to meet you, buddy. Do you like me? I\'ve been missing you for a while...""\n    """"""\n    response = response.replace(\' \\\' \', \'\\\'\')\n\n    for ch in set(_END_CHARS) | {\',\'}:\n        response = response.replace(\' \' + ch, ch)\n\n    response = _capitalize_first_chars(response)\n    response = response.strip()\n\n    return response\n'"
cakechat/utils/text_processing/utils.py,0,"b""import json\nimport os\n\nfrom cakechat.config import PROCESSED_CORPUS_DIR, TOKEN_INDEX_DIR, CONDITION_IDS_INDEX_DIR\n\n\ndef get_processed_corpus_path(corpus_name):\n    return os.path.join(PROCESSED_CORPUS_DIR, corpus_name + '.txt')\n\n\ndef get_index_to_token_path(processed_corpus_name):\n    return os.path.join(TOKEN_INDEX_DIR, 't_idx_{}.json'.format(processed_corpus_name))\n\n\ndef get_index_to_condition_path(processed_corpus_name):\n    return os.path.join(CONDITION_IDS_INDEX_DIR, 'c_idx_{}.json'.format(processed_corpus_name))\n\n\ndef load_index_to_item(items_index_path):\n    with open(items_index_path, 'r', encoding='utf-8') as item_index_fh:\n        index_to_item = json.load(item_index_fh)\n        index_to_item = {int(k): v for k, v in index_to_item.items()}\n\n    return index_to_item\n"""
cakechat/utils/w2v/__init__.py,0,"b'from cakechat.utils.w2v.utils import get_w2v_model_path, get_w2v_params_str, get_w2v_model_name\n'"
cakechat/utils/w2v/model.py,0,"b'import multiprocessing\nimport os\nfrom gensim.models import Word2Vec\n\nfrom cakechat.config import TRAIN_CORPUS_NAME, VOCABULARY_MAX_SIZE, WORD_EMBEDDING_DIMENSION, S3_MODELS_BUCKET_NAME, \\\n    S3_W2V_REMOTE_DIR, USE_SKIP_GRAM, MIN_WORD_FREQ, TOKEN_REPRESENTATION_SIZE, W2V_WINDOW_SIZE\nfrom cakechat.dialog_model.model_utils import ModelLoaderException\nfrom cakechat.utils.files_utils import DummyFileResolver, ensure_dir\nfrom cakechat.utils.logger import get_logger\nfrom cakechat.utils.s3 import S3FileResolver\nfrom cakechat.utils.tee_file import file_buffered_tee\nfrom cakechat.utils.w2v import get_w2v_model_name, get_w2v_params_str, get_w2v_model_path\n\n_WORKERS_NUM = multiprocessing.cpu_count()\n\n_logger = get_logger(__name__)\n\n\ndef _train_model(tokenized_lines, voc_size, vec_size, window_size, skip_gram):\n    _logger.info(\'Word2Vec model will be trained now. It can take long, so relax and have fun.\')\n\n    params_str = get_w2v_params_str(voc_size, vec_size, window_size, skip_gram)\n    _logger.info(\'Parameters for training: {}\'.format(params_str))\n\n    model = Word2Vec(\n        window=window_size,\n        size=vec_size,\n        max_vocab_size=voc_size,\n        min_count=MIN_WORD_FREQ,\n        workers=_WORKERS_NUM,\n        sg=skip_gram)\n\n    tokenized_lines_for_voc, tokenized_lines_for_train = file_buffered_tee(tokenized_lines)\n\n    model.build_vocab(tokenized_lines_for_voc)\n    model.train(tokenized_lines_for_train)\n\n    # forget the original vectors and only keep the normalized ones = saves lots of memory\n    # https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.init_sims\n    model.init_sims(replace=True)\n\n    return model\n\n\ndef _save_model(model, model_path):\n    _logger.info(\'Saving model to {}\'.format(model_path))\n    ensure_dir(os.path.dirname(model_path))\n    model.save(model_path, separately=[])\n    _logger.info(\'Model has been saved\')\n\n\ndef _load_model(model_path):\n    _logger.info(\'Loading model from {}\'.format(model_path))\n    model = Word2Vec.load(model_path, mmap=\'r\')\n    _logger.info(\'Model ""{}"" has been loaded.\'.format(os.path.basename(model_path)))\n    return model\n\n\ndef _get_w2v_model(corpus_name,\n                   voc_size,\n                   model_resolver_factory=None,\n                   tokenized_lines=None,\n                   vec_size=TOKEN_REPRESENTATION_SIZE,\n                   window_size=W2V_WINDOW_SIZE,\n                   skip_gram=USE_SKIP_GRAM):\n    _logger.info(\'Getting w2v model\')\n\n    model_path = get_w2v_model_path(corpus_name, voc_size, vec_size, window_size, skip_gram)\n    model_resolver = model_resolver_factory(model_path) if model_resolver_factory else DummyFileResolver(model_path)\n\n    if not model_resolver.resolve():\n        if not tokenized_lines:\n            raise ModelLoaderException(\n                \'Tokenized corpus ""{}"" was not provided, so w2v model can\\\'t be trained.\'.format(corpus_name))\n\n        # bin model is not present on the disk, so get it\n        model = _train_model(tokenized_lines, voc_size, vec_size, window_size, skip_gram)\n        _save_model(model, model_path)\n    else:\n        # bin model is on the disk, load it\n        model = _load_model(model_path)\n\n    _logger.info(\'Successfully got w2v model\\n\')\n\n    return model\n\ndef get_w2v_model(fetch_from_s3=False,\n                  corpus_name=TRAIN_CORPUS_NAME,\n                  voc_size=VOCABULARY_MAX_SIZE,\n                  vec_size=WORD_EMBEDDING_DIMENSION,\n                  window_size=W2V_WINDOW_SIZE,\n                  skip_gram=USE_SKIP_GRAM):\n    try:\n        model_resolver_factory = \\\n            S3FileResolver.init_resolver(bucket_name=S3_MODELS_BUCKET_NAME, remote_dir=S3_W2V_REMOTE_DIR) \\\n            if fetch_from_s3 else None\n\n        w2v_model = _get_w2v_model(\n            corpus_name=corpus_name,\n            voc_size=voc_size,\n            model_resolver_factory=model_resolver_factory,\n            vec_size=vec_size,\n            window_size=window_size,\n            skip_gram=skip_gram)\n\n    except ModelLoaderException:\n        raise ModelLoaderException(\'Word2vec model is absent. Please run `tools/train_w2v.py` to get the model.\'\n                                   \' WARNING: If you compare different dialog models, be sure that they\'\n                                   \' use the same w2v model (since each run of the w2v-trainer even with the same\'\n                                   \' parameters leads to different w2v models)\')\n\n    return w2v_model\n\n\ndef get_w2v_model_id(corpus_name=TRAIN_CORPUS_NAME,\n                     voc_size=VOCABULARY_MAX_SIZE,\n                     vec_size=WORD_EMBEDDING_DIMENSION,\n                     window_size=W2V_WINDOW_SIZE,\n                     skip_gram=USE_SKIP_GRAM):\n\n    return get_w2v_model_name(\n        corpus_name, voc_size=voc_size, vec_size=vec_size, window_size=window_size, skip_gram=skip_gram)\n'"
cakechat/utils/w2v/utils.py,0,"b'import os\nimport numpy as np\n\nfrom cakechat.config import W2V_MODEL_DIR, USE_SKIP_GRAM, W2V_WINDOW_SIZE, TOKEN_REPRESENTATION_SIZE\nfrom cakechat.utils.logger import get_logger\nfrom cakechat.utils.text_processing import SPECIAL_TOKENS\n\n_logger = get_logger(__name__)\n\n\ndef _get_w2v_model_name(corpus_name, voc_size, vec_size, window_size=W2V_WINDOW_SIZE, skip_gram=USE_SKIP_GRAM):\n    params_str = get_w2v_params_str(voc_size, vec_size, window_size, skip_gram)\n    model_name = \'{}_{}\'.format(corpus_name, params_str)\n    return model_name\n\n\ndef get_w2v_params_str(voc_size, vec_size, window_size=W2V_WINDOW_SIZE, skip_gram=USE_SKIP_GRAM):\n    params_str = \'window{window_size}_voc{voc_size}_vec{vec_size}_sg{skip_gram}\'\n    params_str = params_str.format(window_size=window_size, voc_size=voc_size, vec_size=vec_size, skip_gram=skip_gram)\n    return params_str\n\n\ndef get_w2v_model_path(corpus_name, voc_size, vec_size, window_size=W2V_WINDOW_SIZE, skip_gram=USE_SKIP_GRAM):\n    model_name = get_w2v_model_name(corpus_name, voc_size, vec_size, window_size, skip_gram)\n    model_path = os.path.join(W2V_MODEL_DIR, \'{}.bin\'.format(model_name))\n    return model_path\n\n\ndef get_w2v_model_name(corpus_name, voc_size, vec_size, window_size=W2V_WINDOW_SIZE, skip_gram=USE_SKIP_GRAM):\n    params_str = get_w2v_params_str(voc_size, vec_size, window_size, skip_gram)\n    model_name = \'{}_{}\'.format(corpus_name, params_str)\n    return model_name\n\n\ndef get_token_vector(token, model, token_vec_size=TOKEN_REPRESENTATION_SIZE):\n    if token in model.wv.vocab:\n        return np.array(model[token])\n\n    # generally we want have trained embeddings for all special tokens except the PAD one\n    if token != SPECIAL_TOKENS.PAD_TOKEN:\n        _logger.warn(\'Unknown embedding for token ""{}""\'.format(token))\n\n    return np.zeros(token_vec_size, dtype=np.float32)\n'"
cakechat/dialog_model/inference/candidates/__init__.py,0,b'from cakechat.dialog_model.inference.candidates.beamsearch import BeamsearchCandidatesGenerator\nfrom cakechat.dialog_model.inference.candidates.sampling import SamplingCandidatesGenerator\n'
cakechat/dialog_model/inference/candidates/abstract_generator.py,0,"b'from abc import ABCMeta, abstractmethod\n\n\nclass AbstractCandidatesGenerator(object, metaclass=ABCMeta):\n    @abstractmethod\n    def generate_candidates(self, context_token_ids, condition_ids, output_seq_len):\n        pass\n'"
cakechat/dialog_model/inference/candidates/beamsearch.py,0,"b'from itertools import zip_longest\n\nimport keras.backend as K\nimport numpy as np\n\nfrom cakechat.config import INTX\nfrom cakechat.dialog_model.inference.candidates.abstract_generator import AbstractCandidatesGenerator\nfrom cakechat.dialog_model.inference.service_tokens import ServiceTokensIDs\nfrom cakechat.dialog_model.inference.utils import get_next_token_log_prob_one_step, get_thought_vectors\nfrom cakechat.utils.profile import timer\n\n\nclass BeamsearchCandidatesGenerator(AbstractCandidatesGenerator):\n    def __init__(self, nn_model, beam_size, repetition_penalization_coefficient):\n        """"""\n        :param nn_model: NN model to use for predicting\n        :param beam_size: Size of beam\n        """"""\n        self._nn_model = nn_model\n        self._beam_size = min(beam_size, self._nn_model.vocab_size)\n        self._num_finished_candidates_to_keep = self._beam_size ** 2\n        self._log_repetition_penalization_coefficient = np.log(repetition_penalization_coefficient)\n        self._service_tokens_ids = ServiceTokensIDs(nn_model.token_to_index)\n\n    @staticmethod\n    def _get_k_max_elements_indices_and_scores(vec, k, mask=None):\n        if mask is None:\n            # We use argpartition here instead of argsort to achieve linear-time performance.\n            max_elements_indices = np.argpartition(-vec, k - 1)[:k]\n        else:\n            masked_vec = vec.copy()  # To avoid side-effects\n            masked_vec[~mask] = -np.inf\n            max_elements_indices = np.argpartition(-masked_vec, k - 1)[:k]\n        return max_elements_indices, vec[max_elements_indices]\n\n    def _init_hidden_states_and_candidates(self, output_seq_len):\n        # This array will contain beam_size candidates, each of which output_seq_len long.\n        # dtype=INTX, because this array stores ids of tokens which are integers.\n        self._cur_candidates = np.full(\n            (self._beam_size, output_seq_len), self._service_tokens_ids.pad_token_id, dtype=INTX)\n        # First, fill in first token of each candidate\n        self._cur_candidates[:, 0] = self._service_tokens_ids.start_token_id\n        # and prepare an array for score of each candidate\n        self._cur_candidates_scores = np.zeros(self._beam_size)\n\n        # Finished candidates are going to be concatenated here. Each candidate will be of shape=(1, output_seq_len),\n        # But now we have 0 candidates: that\'s why we need to initialize this array with shape=(0, output_seq_len)\n        self._finished_candidates = np.zeros((0, output_seq_len), dtype=INTX)\n        # Same story here: 0 candidates so long => shape=0.\n        self._finished_candidates_scores = np.zeros(0, dtype=K.floatx())\n\n        # For each candidate in the beam, for each layer of the decoder we need hidden_states_dim numbers to store\n        # this array\n        self._hidden_states_batch = np.zeros(\n            (self._beam_size, self._nn_model.decoder_depth, self._nn_model.hidden_layer_dim),\n            dtype=K.floatx())  # By default, numpy has dtype=np.float64, but this array is passed\n        # right into model\'s functions, so we need to have explicit type declaring here.\n\n    def _compute_thought_vectors(self, context_token_ids):\n        # thought_vector is (1 x thought_vector_dim);\n        # context_token_ids is (input_seq_len), but we need to make it 1 x input_seq_len, because model\'s functions\n        # require input_seq_len dimension to be the second one.\n        thought_vector = get_thought_vectors(self._nn_model, context_token_ids[np.newaxis, :])\n        # All model\'s functions process each sequence independently: every input sequence is matched to the\n        # corresponding output sequence. So if we want to get probabilities of all outputs given the saved inputs,\n        # we need to repeat the input <num_outputs> times. <num_outputs> = beam_size here.\n        self._thought_batch = np.repeat(thought_vector, self._beam_size, axis=0)\n\n    def _update_next_candidates_and_hidden_states(self, token_idx, best_non_finished_candidates_indices,\n                                                  expanded_beam_tokens):\n        """"""\n        Updates current state of candidates prediction process and fills in current candidates.\n\n        :param token_idx: position of current token\n        :param expanded_beam_tokens: np.array with shape (beam_size * beam_size,)\n            Tokens candidates for the next step.\n        :param best_non_finished_candidates_indices: np.array with shape (beam_size,)\n            Contains indexes of best K candidates in current K^2 sized expanded beam to use in the next beam.\n        """"""\n        # Separate arrays for the updated hidden states\n        next_hidden_states_batch = np.zeros_like(self._hidden_states_batch)\n        # and the candidates\n        next_step_candidates = np.full_like(self._cur_candidates, self._service_tokens_ids.pad_token_id, dtype=INTX)\n\n        for i, candidate_idx in enumerate(best_non_finished_candidates_indices):\n            # expanded_beam_tokens contains the last token for each of the beam_size^2 candidates in the expanded beam.\n            # We need to get which original candidate this token in the expanded beam corresponds to.\n            # (to fill in all the previous tokens from self._cur_candidates)\n            # Because all the candidates in the expanded beam were filled sequentially, we just use this formula:\n            original_candidate_idx = candidate_idx // self._beam_size\n\n            # Construct the candidates for the next step using self._cur_candidates and the last token:\n\n            # next_tokens is the last token for each new candidate here.\n            next_token = expanded_beam_tokens[candidate_idx]\n\n            # First, fill in all the preceding tokens for current candidate\n            next_step_candidates[i, :token_idx] = self._cur_candidates[original_candidate_idx, :token_idx]\n            # And put the last token of the current candidate on its position\n            next_step_candidates[i, token_idx] = next_token\n            # We also have to update the hidden states for the next step: we need to know which hidden states\n            # to use to continue decoding each candidate and we get them from the corresponding positions\n            next_hidden_states_batch[i] = self._hidden_states_batch[original_candidate_idx]\n\n        self._hidden_states_batch = next_hidden_states_batch\n        self._cur_candidates = next_step_candidates\n\n    def _update_finished_candidates(self, token_idx, best_finished_candidates_indices, expanded_beam_scores,\n                                    expanded_beam_tokens, output_seq_len):\n        n_finished_candidates = len(best_finished_candidates_indices)\n        if not n_finished_candidates:\n            return\n\n        # These are only finished candidates on the current step. We will further append this array to\n        # self._finished_candidates\n        # dtype=INTX, because this array stores ids of tokens which are integers.\n        cur_finished_candidates = \\\n            np.full((n_finished_candidates, output_seq_len), self._service_tokens_ids.pad_token_id, dtype=INTX)\n\n        cur_finished_candidates_scores = np.full(n_finished_candidates, 0, dtype=K.floatx())\n        for i, candidate_idx in enumerate(best_finished_candidates_indices):\n            # expanded_beam_tokens contains the last token for each of the beam_size^2 candidates in the expanded beam\n            # to get all the other tokens we need to get which original candidate this token in the expanded beam\n            # corresponds to. Because all the candidates in the expanded beam were filled sequentially, we can just\n            # use this formula:\n            original_candidate_idx = candidate_idx // self._beam_size\n\n            # Construct the candidates for the next step using self._cur_candidates and the last token:\n\n            # next_tokens is the last token for each new candidate here.\n            next_token = expanded_beam_tokens[candidate_idx]\n            # Also we do the same thing for scores\n            candidate_score = expanded_beam_scores[candidate_idx]\n\n            # First, fill in all the preceding tokens for current candidate\n            cur_finished_candidates[i, :token_idx] = self._cur_candidates[original_candidate_idx, :token_idx]\n            # And put the last token of the current candidate on its position\n            cur_finished_candidates[i, token_idx] = next_token\n            cur_finished_candidates_scores[i] = candidate_score\n\n        # Use concatenate to add sequences of the same length to the list of the sequences.\n        # Use axis=0 here, because 0-th dimension corresponds to the index of the candidate in the list\n        # And 1-st dimension enumerates the tokens within each sequence.\n        self._finished_candidates = np.concatenate((self._finished_candidates, cur_finished_candidates), axis=0)\n        self._finished_candidates_scores = np.concatenate(\n            (self._finished_candidates_scores, cur_finished_candidates_scores), axis=0)\n\n    def _penalize_by_repetition(self, next_token_log_probs_batch, used_tokens_ids_batch):\n        for i, used_tokens_ids in enumerate(used_tokens_ids_batch):\n            tokens_ids_to_penalize = [\n                x for x in used_tokens_ids if x not in self._service_tokens_ids.non_penalizable_tokens_ids\n            ]\n            next_token_log_probs_batch[i, tokens_ids_to_penalize] -= self._log_repetition_penalization_coefficient\n        return next_token_log_probs_batch\n\n    def _compute_next_token_score_batch(self, token_idx, condition_id):\n        # Get prediction of the model - p(T|S)\n        current_token_id_for_each_candidate = self._cur_candidates[:, token_idx - 1]\n        self._hidden_states_batch, next_token_score_batch = \\\n            get_next_token_log_prob_one_step(self._nn_model, self._thought_batch, self._hidden_states_batch,\n                                             current_token_id_for_each_candidate, condition_id)\n        # Candidates[:, :token_idx], as usual, means ""All tokens preceding token_idx for each candidate""\n        # We use all preceding tokens to compute counts and penalize current distribution using these counts.\n        next_token_score_batch = self._penalize_by_repetition(next_token_score_batch,\n                                                              self._cur_candidates[:, :token_idx])\n\n        # next_token_score_batch here is (num_candidates x vocab_size). For each candidate we find all the\n        # banned tokens and kill their scores to -inf.\n        next_token_score_batch[:, self._service_tokens_ids.banned_tokens_ids] = -np.inf\n        return next_token_score_batch\n\n    def _get_aggregated_scores_and_tokens_for_expanded_beam(self, next_token_score_batch):\n        # Expanded beam is beam_size x beam_size candidates that we consider on the next step.\n        # But we don\'t want to keep all the candidates themselves for better performance. So we just keep\n        # the last token and the total score.\n        expanded_beam_scores = np.zeros((self._beam_size * self._beam_size), dtype=K.floatx())\n        expanded_beam_tokens = np.zeros((self._beam_size * self._beam_size), dtype=INTX)\n\n        for candidate_idx in range(self._beam_size):\n            # Get beam_size candidates on each step\n            next_token_candidates, next_token_scores = \\\n                self._get_k_max_elements_indices_and_scores(next_token_score_batch[candidate_idx], self._beam_size)\n            # sequentially fill in the additive scores\n            expanded_beam_scores[candidate_idx * self._beam_size:(candidate_idx + 1) * self._beam_size] = \\\n                self._cur_candidates_scores[candidate_idx] + next_token_scores\n            # and the corresponding last tokens in the array\n            expanded_beam_tokens[candidate_idx * self._beam_size:(candidate_idx + 1) * self._beam_size] = \\\n                next_token_candidates\n        return expanded_beam_scores, expanded_beam_tokens\n\n    def _get_best_finished_and_nonfinished_candidates(self, expanded_beam_scores, expanded_beam_tokens):\n        """"""\n        Get top-k next tokens for each candidate.\n        Also updates aggregated scores according to the scores for chosen tokens.\n\n        :param expanded_beam_scores: Aggregated scores for each response in the extended beam\n        :param expanded_beam_tokens: Last tokens in each response in the extended beam\n        :return: best_non_finished_candidates, best_finished_candidates\n            best_non_finished_candidates are used for updating beam for the next step\n            best_finished_candidates are returned to dump on each step and rerank afterwards.\n        """"""\n        # This mask contains true if the corresponding candidate is finished with <EOS> and false otherwise\n        finished_candidates_mask = (expanded_beam_tokens == self._service_tokens_ids.eos_token_id)\n        # We select the best candidates among those who are not finished\n        best_non_finished_candidates, self._cur_candidates_scores = \\\n            self._get_k_max_elements_indices_and_scores(expanded_beam_scores, self._beam_size,\n                                                        ~finished_candidates_mask)\n        # And also return finished candidates that are good enough to fit in the new beam, but do not go there\n        high_quality_candidates_mask = (expanded_beam_scores > self._cur_candidates_scores[self._beam_size - 1] - 1e-6)\n        # We need [0] in the end of this line because np.nonzero returns tuple, but we only need indices\n        best_finished_candidates = np.nonzero(finished_candidates_mask & high_quality_candidates_mask)[0]\n        return best_non_finished_candidates, best_finished_candidates\n\n    def _generate_candidates_for_one_context(self, condition_id, output_seq_len):\n        # Fill the first beam_size candidates.\n        # We need to do it separately here, because the logic is a little bit different from what is going on for all\n        # the other steps. There we generate beam_size next tokens for each each current candidate in the beam and then\n        # select the beam_size best ones. But here we just compute the initial beam_size candidates according to\n        # the score of the 1-st token.\n        next_token_score_batch = self._compute_next_token_score_batch(1, condition_id)\n        self._cur_candidates[:, 1], self._cur_candidates_scores = self._get_k_max_elements_indices_and_scores(\n            next_token_score_batch[0], self._beam_size)\n\n        for token_idx in range(2, output_seq_len):  # Start from 2 because first token candidates are already filled.\n            # This array has shape beam_size x vocab_size. We use this scores to select best tokens for the beam\n            # on the next step.\n            next_token_score_batch = self._compute_next_token_score_batch(token_idx, condition_id)\n\n            # Select beam_size best tokens for each candidate in the beam.\n            # Also compute the score of the corresponding candidate.\n            expanded_beam_scores, expanded_beam_tokens = \\\n                self._get_aggregated_scores_and_tokens_for_expanded_beam(next_token_score_batch)\n\n            # Select the best candidates according to the scores computed prevuoisly\n            best_non_finished_candidates_indices, best_finished_candidates_indices = \\\n                self._get_best_finished_and_nonfinished_candidates(expanded_beam_scores, expanded_beam_tokens)\n\n            self._update_finished_candidates(token_idx, best_finished_candidates_indices, expanded_beam_scores,\n                                             expanded_beam_tokens, output_seq_len)\n            self._update_next_candidates_and_hidden_states(token_idx, best_non_finished_candidates_indices,\n                                                           expanded_beam_tokens)\n\n        # Pre-filter candidates based on intermidiate scores for better performance\n        output_candidates_num = min(self._beam_size, self._finished_candidates.shape[0])\n        idxs, _ = self._get_k_max_elements_indices_and_scores(self._finished_candidates_scores, output_candidates_num)\n        self._finished_candidates = self._finished_candidates[idxs]\n\n        return self._finished_candidates if self._finished_candidates.shape[0] > 0 else self._cur_candidates\n\n    @timer\n    def generate_candidates(self, context_token_ids, condition_ids, output_seq_len):\n        x_with_conditions_batch = zip_longest(context_token_ids, condition_ids if condition_ids is not None else [])\n        result = []\n        for x, condition_id in x_with_conditions_batch:\n            self._compute_thought_vectors(x)\n            self._init_hidden_states_and_candidates(output_seq_len)\n            result.append(self._generate_candidates_for_one_context(condition_id, output_seq_len))\n        return result\n'"
cakechat/dialog_model/inference/candidates/sampling.py,0,"b'import keras.backend as K\nimport numpy as np\n\nfrom cakechat.config import INTX\nfrom cakechat.dialog_model.inference.candidates.abstract_generator import AbstractCandidatesGenerator\nfrom cakechat.dialog_model.inference.service_tokens import ServiceTokensIDs\nfrom cakechat.dialog_model.inference.utils import get_next_token_prob_one_step, get_thought_vectors\n\n\nclass TokenSampler(object):\n    """"""\n    Class for sampling responses without banned tokens and repeating.\n\n    There has to be individual instance of TokenSampler for each run of sampling procedure\n    because it contains counters of tokens that have to be reset before sampling new responses.\n    """"""\n\n    def __init__(self, batch_size, banned_tokens_ids, non_penalizable_tokens_ids, repetition_penalization_coefficient):\n        self._batch_size = batch_size\n        self._banned_tokens_ids = banned_tokens_ids\n        self._non_penalizable_tokens_ids = non_penalizable_tokens_ids\n        self._used_tokens_ids = [[] for _ in range(batch_size)]\n        self._repetition_penalization_coefficient = repetition_penalization_coefficient\n\n    def sample(self, probabilities, sample_idx, temperature=1.0):\n        """"""\n        Sample using individual priors for each sample_idx.\n        Also updates individual priors for each sample in batch.\n        We need individual priors to prevent the model from repeating the same tokens over and over again in one\n        response.\n\n        probabilities: Probabilities of each token. The distribution given by these probabilities\n            is used by this function to sample the token.\n        sample_idx : Integer between 0 and batch_size-1. We need it to figure out which token_log_prior line to use.\n        temperature: Temperature for sampling. Temperature has to be a positive number.\n        :return: Index of sampled token\n        """"""\n        # To make the repetition penalization invariant to the original temperature we have to adjust the coefficient:\n        repetition_penalize_coefficient = np.exp(np.log(self._repetition_penalization_coefficient) / temperature)\n        # Back-up the array to avoid side-effects (otherwise the function will change the probabilities passed as an\n        # argument)\n        probabilities = np.copy(probabilities)\n\n        probabilities[self._banned_tokens_ids] = 0\n        probabilities[self._used_tokens_ids[sample_idx]] /= repetition_penalize_coefficient\n\n        probabilities /= np.sum(probabilities)\n        token_id = np.random.choice(probabilities.shape[0], replace=False, p=probabilities)\n\n        # Update used tokens list\n        if token_id not in self._non_penalizable_tokens_ids:\n            self._used_tokens_ids[sample_idx].append(token_id)\n\n        return token_id\n\n\nclass SamplingCandidatesGenerator(AbstractCandidatesGenerator):\n    def __init__(self, nn_model, temperature, samples_num, repetition_penalization_coefficient):\n        self._nn_model = nn_model\n        self._temperature = temperature\n        self._samples_num = samples_num\n        self._service_tokens_ids = ServiceTokensIDs(nn_model.token_to_index)\n        self._repetition_penalization_coefficient = repetition_penalization_coefficient\n\n    def _sample_response(self, thought_vectors, condition_ids, output_seq_len):\n        batch_size = thought_vectors.shape[0]\n        sampler = TokenSampler(batch_size, self._service_tokens_ids.banned_tokens_ids,\n                               self._service_tokens_ids.non_penalizable_tokens_ids,\n                               self._repetition_penalization_coefficient)\n        # For each candidate in the batch, for each layer of the decoder we need hidden_states_dim numbers to store\n        # this array\n        hidden_states_batch = np.zeros(\n            (batch_size, self._nn_model.decoder_depth, self._nn_model.hidden_layer_dim),\n            dtype=K.floatx())  # By default, numpy has dtype=np.float64, but this array is passed\n        # right into model\'s functions, so we need to have explicit type declaring here.\n\n        response_tokens_ids = np.full((batch_size, output_seq_len), self._service_tokens_ids.pad_token_id, dtype=INTX)\n\n        # Track finished responses to skip prediction step for them\n        is_response_finished = np.zeros(batch_size, dtype=np.bool)\n\n        # Fill in first tokens of each response in the batch:\n        response_tokens_ids[:, 0] = self._service_tokens_ids.start_token_id\n        for token_idx in range(1, output_seq_len):  # Starting with the second token\n            hidden_states_batch, next_token_probs_batch = \\\n                get_next_token_prob_one_step(self._nn_model, thought_vectors, hidden_states_batch,\n                                             response_tokens_ids[:, token_idx - 1],  # previous token for each response\n                                             condition_ids,\n                                             temperature=self._temperature)\n\n            for response_idx, next_token_probs in enumerate(next_token_probs_batch):\n                if is_response_finished[response_idx]:\n                    continue\n\n                next_token_id = sampler.sample(next_token_probs, response_idx, self._temperature)\n                response_tokens_ids[response_idx, token_idx] = next_token_id\n\n                if next_token_id in [self._service_tokens_ids.eos_token_id, self._service_tokens_ids.pad_token_id]:\n                    is_response_finished[response_idx] = True\n\n            # Stop if all responses are done\n            if np.all(is_response_finished):\n                break\n\n        return response_tokens_ids\n\n    def generate_candidates(self, context_tokens_ids, condition_ids, output_seq_len):\n        """"""\n        Predict answers for every sequence token by token until EOS_TOKEN occurred in the sequence\n        using sampling with temperature.\n        During the sampling procedure offensive and <unk> tokens are banned.\n        Probabilities of tokens that have already been used in a response are penalized\n        (divided by REPETITION_PENALIZE_COEFFICIENT).\n        All the rest of the sequence is filled with PAD_TOKENs.\n        """"""\n        thought_vectors = get_thought_vectors(self._nn_model, context_tokens_ids)\n        sampled_candidates = [\n            self._sample_response(thought_vectors, condition_ids, output_seq_len) for _ in range(self._samples_num)\n        ]\n\n        # Transpose the result: candidate_id x batch_size x seq_len -> batch_size x candidate_id x seq_len\n        return np.swapaxes(sampled_candidates, 0, 1)\n'"
cakechat/dialog_model/inference/tests/predict.py,0,"b'import os\nimport sys\nimport unittest\n\nimport keras.backend as K\nimport numpy as np\n\nsys.path.append(\n    os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))))\n\nfrom cakechat.utils.env import init_cuda_env\n\ninit_cuda_env()\n\nfrom cakechat.dialog_model.factory import get_trained_model\nfrom cakechat.dialog_model.inference import get_sequence_log_probs\nfrom cakechat.dialog_model.inference.utils import get_next_token_log_prob_one_step\nfrom cakechat.config import HIDDEN_LAYER_DIMENSION, RANDOM_SEED, INPUT_CONTEXT_SIZE, \\\n    INPUT_SEQUENCE_LENGTH, OUTPUT_SEQUENCE_LENGTH, INTX\n\nnp.random.seed(seed=RANDOM_SEED)\n\n\nclass TestPredict(unittest.TestCase):\n    @staticmethod\n    def _predict_log_probabilities_one_step(nn_model, x_batch, y_batch):\n        """"""\n        Predict answers for every sequence token by token until EOS_TOKEN occurred in the sequence using sampling with temperature.\n        All the rest of the sequence is filled with PAD_TOKENs.\n        """"""\n        thought_vectors_batch = nn_model.get_thought_vectors(x_batch)\n        hidden_states_batch = np.zeros(\n            (x_batch.shape[0], nn_model.decoder_depth, HIDDEN_LAYER_DIMENSION), dtype=K.floatx())\n\n        total_log_probs = np.zeros((y_batch.shape[0], y_batch.shape[1] - 1, nn_model.vocab_size))\n        for token_idx in range(1, y_batch.shape[1]):\n            hidden_states_batch, next_token_log_probs_batch = \\\n                get_next_token_log_prob_one_step(nn_model, thought_vectors_batch, hidden_states_batch,\n                                                 y_batch[:, token_idx - 1], condition_ids=None)\n            # total_log_probs has shape (batch_size x num_tokens x vocab_size)\n            total_log_probs[:, token_idx - 1, :] = next_token_log_probs_batch\n\n        return total_log_probs\n\n    def test_one_step_decoder(self):\n        nn_model = get_trained_model()\n\n        _EPS = 1e-5\n        batch_size = 1\n        # input batches shapes should correspond to the shapes of the trained model layers\n        context_size = INPUT_CONTEXT_SIZE\n        input_seq_len = INPUT_SEQUENCE_LENGTH\n        output_seq_len = OUTPUT_SEQUENCE_LENGTH\n\n        x = np.random.randint(0, nn_model.vocab_size, size=(batch_size, context_size, input_seq_len), dtype=INTX)\n        y = np.random.randint(0, nn_model.vocab_size, size=(batch_size, output_seq_len), dtype=INTX)\n\n        ground_truth_log_probabilities = get_sequence_log_probs(nn_model, x, y, condition_ids=None)\n        one_step_log_probabilities = self._predict_log_probabilities_one_step(nn_model, x, y)\n        mae = np.abs(one_step_log_probabilities - ground_truth_log_probabilities).mean()\n\n        self.assertTrue(mae < _EPS)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
cakechat/dialog_model/inference/tests/sampling.py,0,"b""import os\nimport sys\nimport unittest\n\nimport keras.backend as K\nimport numpy as np\nfrom scipy.stats import binom\n\nsys.path.append(\n    os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))))\n\nfrom cakechat.dialog_model.inference.candidates.sampling import TokenSampler\nfrom cakechat.config import REPETITION_PENALIZE_COEFFICIENT, RANDOM_SEED\n\nnp.random.seed(seed=RANDOM_SEED)\n\n# Type I error rate: probability that a test will fail even though everything is OK\n# The lower the probability is the more inaccurate (in terms of Type II error) the test becomes.\n# This is independent probability for every test in the TestCase.\n_CONFIDENCE_LEVEL = 1e-6\n# Number of samples for monte-carlo estimation or probabilities.\n# The bigger number of sample is, the more accurate tests become\n_SAMPLES_NUM = 10000\n\n\nclass TestSampling(unittest.TestCase):\n    def test_sample_list(self):\n        # Error rate is p(token1) + p(token2) = conf_level / 2 + conf_level / 2 = conf_level:\n        probs = [_CONFIDENCE_LEVEL / 2, _CONFIDENCE_LEVEL / 2, 1 - _CONFIDENCE_LEVEL]\n\n        token_sampler = TokenSampler(\n            batch_size=1,\n            banned_tokens_ids=[],\n            non_penalizable_tokens_ids=range(len(probs)),\n            repetition_penalization_coefficient=REPETITION_PENALIZE_COEFFICIENT)\n        expected_token_ids = np.array([2])\n        actual_token_ids = token_sampler.sample(probs, sample_idx=0)\n        self.assertEqual(expected_token_ids, actual_token_ids)\n\n    def test_sample_ndarray(self):\n        # Error rate is p(token1) + p(token2) = conf_level / 2 + conf_level / 2 = conf_level\n        probs = np.array([_CONFIDENCE_LEVEL / 2, _CONFIDENCE_LEVEL / 2, 1 - _CONFIDENCE_LEVEL], dtype=K.floatx())\n\n        token_sampler = TokenSampler(\n            batch_size=1,\n            banned_tokens_ids=[],\n            non_penalizable_tokens_ids=range(len(probs)),\n            repetition_penalization_coefficient=REPETITION_PENALIZE_COEFFICIENT)\n        expected_token_ids = np.array([2])\n        actual_token_ids = token_sampler.sample(probs, sample_idx=0)\n        self.assertEqual(expected_token_ids, actual_token_ids)\n\n    def test_sample_probs(self):\n        probs = [0.3, 0.6, 0.1]\n\n        token_sampler = TokenSampler(\n            batch_size=1,\n            banned_tokens_ids=[],\n            non_penalizable_tokens_ids=range(len(probs)),\n            repetition_penalization_coefficient=REPETITION_PENALIZE_COEFFICIENT)\n        adjusted_confidence_level = _CONFIDENCE_LEVEL / len(probs)  # bonferroni correction\n        confidence_intervals = [binom.interval(1 - adjusted_confidence_level, _SAMPLES_NUM, p) for p in probs]\n        est_probs_from, est_probs_to = zip(*confidence_intervals)\n        samples = np.array([token_sampler.sample(probs, 0) for _ in range(_SAMPLES_NUM)])\n        counts = {val: np.sum(samples == val) for val in np.unique(samples)}\n\n        for i, _ in enumerate(probs):\n            self.assertLessEqual(counts[i], est_probs_to[i])\n            self.assertGreaterEqual(counts[i], est_probs_from[i])\n\n    def test_sample_with_zeros(self):\n        probs = np.array([1.0, 0, 0], dtype=K.floatx())\n\n        token_sampler = TokenSampler(\n            batch_size=1,\n            banned_tokens_ids=[],\n            non_penalizable_tokens_ids=range(len(probs)),\n            repetition_penalization_coefficient=REPETITION_PENALIZE_COEFFICIENT)\n        expected_token_ids = np.array([0])\n        actual_token_ids = token_sampler.sample(probs, sample_idx=0)\n        self.assertEqual(expected_token_ids, actual_token_ids)\n\n    def test_sample_banned_tokens(self):\n        eps = _CONFIDENCE_LEVEL * 0.3\n        # Here we multiply the confidence level by 0.3 so that after removal of banned token and renormalization\n        # the probability of an error remains equal to _CONFIDENCE_LEVEL value.\n        probs = np.array([0.7, 0.3 - eps, eps], dtype=K.floatx())\n\n        token_sampler = TokenSampler(\n            batch_size=1,\n            banned_tokens_ids=[0],\n            non_penalizable_tokens_ids=range(len(probs)),\n            repetition_penalization_coefficient=REPETITION_PENALIZE_COEFFICIENT)\n        expected_token_ids = np.array([1])\n        actual_token_ids = token_sampler.sample(probs, sample_idx=0)\n        self.assertEqual(expected_token_ids, actual_token_ids)\n\n    def test_sample_banned_tokens_2(self):\n        eps = 1e-6\n        probs = np.array([1.0 - eps, eps, 0], dtype=K.floatx())\n\n        token_sampler = TokenSampler(\n            batch_size=1,\n            banned_tokens_ids=[0],\n            non_penalizable_tokens_ids=range(len(probs)),\n            repetition_penalization_coefficient=REPETITION_PENALIZE_COEFFICIENT)\n        # Token #1 has to be returned even though its probability is really small\n        expected_token_ids = np.array([1])\n        actual_token_ids = token_sampler.sample(probs, sample_idx=0)\n        self.assertEqual(expected_token_ids, actual_token_ids)\n\n    def test_repetition_penalization(self):\n        probs = [0.5, 0.5]\n\n        actual_num_nonequal_pairs = 0\n        for _ in range(_SAMPLES_NUM):\n            token_sampler = TokenSampler(\n                batch_size=1,\n                banned_tokens_ids=[],\n                non_penalizable_tokens_ids=[],\n                repetition_penalization_coefficient=REPETITION_PENALIZE_COEFFICIENT)\n            first_token = token_sampler.sample(probs, sample_idx=0)\n            second_token = token_sampler.sample(probs, sample_idx=0)\n            actual_num_nonequal_pairs += int(first_token != second_token)\n\n        # P(first != second) = P(first=0, second=1) + P(first=1, second=0) =\n        # = 0.5 * 0.5 * r / (0.5 + 0.5 * r) + 0.5 * 0.5 * r / (0.5 + 0.5 * r) = r / (1 + r)\n        expected_nonequal_pair_rate = REPETITION_PENALIZE_COEFFICIENT / (1 + REPETITION_PENALIZE_COEFFICIENT)\n        expected_nonequal_pair_rate_from, expected_nonequal_pair_rate_to = \\\n            binom.interval(1 - _CONFIDENCE_LEVEL, _SAMPLES_NUM, expected_nonequal_pair_rate)\n        self.assertLessEqual(actual_num_nonequal_pairs, expected_nonequal_pair_rate_to)\n        self.assertGreaterEqual(actual_num_nonequal_pairs, expected_nonequal_pair_rate_from)\n\n    def test_nonpenalizable_tokens(self):\n        probs = [0.5, 0.5]\n\n        actual_num_nonequal_pairs = 0\n        samples_generated = 0\n        while samples_generated < _SAMPLES_NUM:\n            token_sampler = TokenSampler(\n                batch_size=1,\n                banned_tokens_ids=[],\n                non_penalizable_tokens_ids=[0],\n                repetition_penalization_coefficient=REPETITION_PENALIZE_COEFFICIENT)\n            first_token = token_sampler.sample(probs, sample_idx=0)\n            if first_token == 0:\n                samples_generated += 1\n                second_token = token_sampler.sample(probs, sample_idx=0)\n                actual_num_nonequal_pairs += (first_token != second_token)\n\n        # When we don't penalize for token#0, P(first != second | first=0) = P(second=1 | first=0) = 0.5\n        expected_nonequal_pair_rate = 0.5\n        expected_nonequal_pair_rate_from, expected_nonequal_pair_rate_to = binom.interval(\n            1 - _CONFIDENCE_LEVEL, _SAMPLES_NUM, expected_nonequal_pair_rate)\n        self.assertLessEqual(actual_num_nonequal_pairs, expected_nonequal_pair_rate_to)\n        self.assertGreaterEqual(actual_num_nonequal_pairs, expected_nonequal_pair_rate_from)\n\n    def test_nonpenalizable_tokens_2(self):\n        probs = [0.5, 0.5]\n\n        actual_num_nonequal_pairs = 0\n        samples_generated = 0\n        while samples_generated < _SAMPLES_NUM:\n            token_sampler = TokenSampler(\n                batch_size=1,\n                banned_tokens_ids=[],\n                non_penalizable_tokens_ids=[1],\n                repetition_penalization_coefficient=REPETITION_PENALIZE_COEFFICIENT)\n            first_token = token_sampler.sample(probs, sample_idx=0)\n            if first_token == 0:\n                samples_generated += 1\n                second_token = token_sampler.sample(probs, sample_idx=0)\n                actual_num_nonequal_pairs += (first_token != second_token)\n\n        # When we penalize for token#0, P(first != second | first=0) = P(second=1 | first=0) = 0.5 * r / (0.5 + 0.5 * r) = r / (1 + r)\n        expected_nonequal_pair_rate = REPETITION_PENALIZE_COEFFICIENT / (1 + REPETITION_PENALIZE_COEFFICIENT)\n        expected_nonequal_pair_rate_from, expected_nonequal_pair_rate_to = binom.interval(\n            1 - _CONFIDENCE_LEVEL, _SAMPLES_NUM, expected_nonequal_pair_rate)\n        self.assertLessEqual(actual_num_nonequal_pairs, expected_nonequal_pair_rate_to)\n        self.assertGreaterEqual(actual_num_nonequal_pairs, expected_nonequal_pair_rate_from)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
cakechat/dialog_model/quality/metrics/__init__.py,0,b''
cakechat/dialog_model/quality/metrics/distinctness.py,0,"b'import numpy as np\n\nfrom cakechat.config import PREDICTION_MODE_FOR_TESTS, DEFAULT_TEMPERATURE, BEAM_SIZE, \\\n    PREDICTION_DISTINCTNESS_NUM_TOKENS\nfrom cakechat.dialog_model.inference import get_nn_response_ids, ServiceTokensIDs\nfrom cakechat.utils.profile import timer\n\n\ndef _calculate_distinct_ngrams(prediction_samples, ngram_len):\n    """"""\n    Takes a list of predicted token_ids and computes number of distinct ngrams\n    for a given ngram_length\n    """"""\n    ngrams = set()\n    for y in prediction_samples:\n        # Calculate all n-grams where n = ngram_len. (Get ngram_len cyclic shifts of y and transpose the result)\n        cur_ngrams = list(zip(*[y[i:] for i in range(ngram_len)]))  # yapf: disable\n\n        # Aggregate statistics\n        ngrams.update(cur_ngrams)\n\n    return len(ngrams)\n\n\n@timer\ndef calculate_response_ngram_distinctness(x,\n                                          nn_model,\n                                          ngram_len,\n                                          num_tokens_to_generate=PREDICTION_DISTINCTNESS_NUM_TOKENS,\n                                          mode=PREDICTION_MODE_FOR_TESTS,\n                                          condition_ids=None,\n                                          temperature=DEFAULT_TEMPERATURE,\n                                          beam_size=BEAM_SIZE):\n    """"""\n    Computes the distinct-n metric of predictions of model given context.\n     distinct-n = <number of unique n-grams> / <total number of n-grams>.\n\n    Metric was proposed in https://arxiv.org/pdf/1510.03055v3.pdf\n    """"""\n    service_tokens_ids = ServiceTokensIDs(nn_model.token_to_index)\n    num_tokens_left = num_tokens_to_generate\n\n    responses = []\n    while num_tokens_left > 0:\n        # Take the first sample for each x\n        responses_ids = get_nn_response_ids(\n            x,\n            nn_model,\n            mode=mode,\n            condition_ids=condition_ids,\n            temperature=temperature,\n            beam_size=beam_size,\n            candidates_num=1)[:, 0, :]\n\n        for y in responses_ids:\n            # mask out special tokens\n            mask = ~np.in1d(y, service_tokens_ids.special_tokens_ids)\n            y_masked = y[mask][:num_tokens_left]\n            responses.append(y_masked)\n\n            num_tokens_left -= len(y_masked)\n            if num_tokens_left == 0:\n                break\n\n    distinct_ngrams = _calculate_distinct_ngrams(responses, ngram_len)\n    return distinct_ngrams / num_tokens_to_generate\n'"
cakechat/dialog_model/quality/metrics/lexical_simlarity.py,0,"b'import os\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom cakechat.config import TRAIN_CORPUS_NAME, BASE_CORPUS_NAME, RESULTS_PATH\nfrom cakechat.utils.files_utils import get_persisted\nfrom cakechat.utils.text_processing import load_index_to_item, get_tokens_sequence, get_processed_corpus_path, \\\n    load_processed_dialogs_from_json, FileTextLinesIterator, get_dialog_lines_and_conditions, \\\n    get_alternated_dialogs_lines, get_index_to_token_path\n\n_TFIDF_VECTORIZER_FULL_PATH = os.path.join(RESULTS_PATH, \'tfidf_vectorizer.pickle\')\n\n\ndef _load_train_lines(corpus_name=TRAIN_CORPUS_NAME):\n    processed_corpus_path = get_processed_corpus_path(corpus_name)\n    dialogs = load_processed_dialogs_from_json(\n        FileTextLinesIterator(processed_corpus_path), text_field_name=\'text\', condition_field_name=\'condition\')\n    train_lines, _ = get_dialog_lines_and_conditions(\n        get_alternated_dialogs_lines(dialogs), text_field_name=\'text\', condition_field_name=\'condition\')\n    return train_lines\n\n\ndef _calculate_tfidf_vector(tfidf_vectorizer, y_sequence):\n    # tfidf-transformer works with list of items,\n    # therefore we should supply list of sequences and then slice the first element.\n    return tfidf_vectorizer.transform([y_sequence])[0]\n\n\ndef calculate_lexical_similarity(x_sequences, y_sequences, tfidf_vectorizer):\n    """"""\n    Computes lexical similarity between two lists of texts.\n\n    lexical_similarity = cos(x_vector, y_vector)\n        where x_vector and y_vector are tf-idf representations of texts.\n    """"""\n    x_sequence = \' \'.join(x_sequences)\n    y_sequence = \' \'.join(y_sequences)\n    x = _calculate_tfidf_vector(tfidf_vectorizer, x_sequence)\n    y = _calculate_tfidf_vector(tfidf_vectorizer, y_sequence)\n\n    # Compute dot-product between two 1xk-sparse matrices.\n    # We also need to slice [0, 0] because the result of .dot() operation is a 1x1 sparse matrix,\n    # but we want to extract a float number.\n    return x.dot(y.T)[0, 0]\n\n\ndef _calculate_tfidf_vectorizer(base_corpus_name=BASE_CORPUS_NAME):\n    index_to_token = load_index_to_item(get_index_to_token_path(base_corpus_name))\n    token_to_index = {v: k for k, v in index_to_token.items()}\n    train_lines = _load_train_lines()\n    tfidf_vectorizer = TfidfVectorizer(tokenizer=get_tokens_sequence, vocabulary=token_to_index)\n    tfidf_vectorizer.fit(train_lines)\n    return tfidf_vectorizer\n\n\ndef get_tfidf_vectorizer():\n    return get_persisted(_calculate_tfidf_vectorizer, _TFIDF_VECTORIZER_FULL_PATH)\n'"
cakechat/dialog_model/quality/metrics/perplexity.py,0,"b""import numpy as np\n\nfrom cakechat.dialog_model.inference import get_sequence_score\nfrom cakechat.dialog_model.quality.metrics.utils import MetricsException\nfrom cakechat.utils.logger import get_logger\nfrom cakechat.utils.text_processing import SPECIAL_TOKENS\n\n_logger = get_logger(__name__)\n\n\ndef _calculate_mean_perplexity(output_ids, output_scores, skip_token_id):\n    total_nonpad_tokens = np.sum(output_ids != skip_token_id, axis=1)\n\n    empty_sequences_mask = total_nonpad_tokens == 0\n    empty_sequences_num = np.sum(empty_sequences_mask)\n    non_empty_sequences_mask = ~empty_sequences_mask\n\n    if empty_sequences_num:\n        _logger.info('Got pads-only sequences while computing perplexity. '\n                     'Skipping these {} samples'.format(empty_sequences_num))\n    if np.all(empty_sequences_mask):\n        raise MetricsException('Got all pad-only sequences while computing perplexity')\n\n    output_scores = output_scores[non_empty_sequences_mask]\n    total_nonpad_tokens = total_nonpad_tokens[non_empty_sequences_mask]\n    sample_perplexities = np.exp(-output_scores / total_nonpad_tokens)\n\n    return np.mean(sample_perplexities)\n\n\ndef calculate_model_mean_perplexity(nn_model, dataset):\n    output_scores = get_sequence_score(nn_model, dataset.x, dataset.y, dataset.condition_ids)\n    if not np.all(dataset.y[:, 0] == nn_model.token_to_index[SPECIAL_TOKENS.START_TOKEN]):\n        raise MetricsException('All responses in the dataset have to start with start_token_id.'\n                               'Make sure there are start_token ids in the beginning of each sequence in dataset.y')\n\n    output_ids = dataset.y[:, 1:]  # remove start token\n\n    return _calculate_mean_perplexity(output_ids, output_scores, nn_model.skip_token_id)\n"""
cakechat/dialog_model/quality/metrics/plotters.py,2,"b'import os\nfrom collections import Counter\nimport tensorflow as tf\nfrom keras import backend as K\n\nfrom cakechat.utils.files_utils import get_cached, serialize\n\n\nclass DummyMetricsPlotter(object):\n    def plot(self, model_id, metric_name, metric_value):\n        pass\n\n\nclass TensorboardMetricsPlotter(object):\n    def __init__(self, log_dir):\n        self._log_dir = log_dir\n        self._writers = {}\n        self._steps_path = os.path.join(self._log_dir, \'steps\')\n        self._steps = get_cached(Counter, self._steps_path)\n\n    @staticmethod\n    def _get_model_specific_key(model_name, key):\n        """"""\n        Build unique identifier for (model_name, key_name) pair.\n        """"""\n        return \'{}_{}\'.format(model_name, key)\n\n    def _get_model_writer(self, model_name):\n        if model_name not in self._writers:\n            self._writers[model_name] = \\\n                tf.summary.FileWriter(os.path.join(self._log_dir, model_name), K.get_session().graph)\n\n        return self._writers[model_name]\n\n    def plot(self, model_name, metric_name, metric_value):\n        summary = tf.Summary()\n        summary.value.add(tag=metric_name, simple_value=metric_value)  # pylint: disable=maybe-no-member\n\n        writer = self._get_model_writer(model_name)\n        metric_model_key = self._get_model_specific_key(model_name, metric_name)\n        writer.add_summary(summary, self._steps[metric_model_key])\n        writer.flush()\n\n        self._steps[metric_model_key] += 1\n        serialize(self._steps_path, self._steps)\n\n    def log_run_metadata(self, model_name, run_metadata):\n        run_metadata_model_key = self._get_model_specific_key(model_name, key=\'run_metadata\')\n        run_tag = \'{}_{}\'.format(self._steps[run_metadata_model_key], run_metadata_model_key)\n\n        writer = self._get_model_writer(model_name)\n        writer.add_run_metadata(run_metadata, run_tag, self._steps[run_metadata_model_key])\n        writer.flush()\n\n        self._steps[run_metadata_model_key] += 1\n        serialize(self._steps_path, self._steps)\n\n    @property\n    def log_dir(self):\n        return self._log_dir\n'"
cakechat/dialog_model/quality/metrics/ranking.py,0,"b""import numpy as np\nfrom sklearn.metrics import average_precision_score\n\n\ndef compute_average_precision(expected_answers, weighted_actual_answers, top):\n    actual_responses, actual_weights = zip(*weighted_actual_answers.items())\n\n    expected_labels = [int(response in expected_answers) for response in actual_responses][:top]\n    actual_weights = actual_weights[:top]\n\n    if any(expected_labels):\n        score = average_precision_score(expected_labels, actual_weights)\n    else:\n        score = 0.0\n\n    return score\n\n\ndef compute_recall_k(expected_answers, weighted_actual_answers, k):\n    sorted_k_responses = sorted(\n        weighted_actual_answers.keys(), key=lambda response: weighted_actual_answers[response], reverse=True)[:k]\n\n    recall_k = len(set(sorted_k_responses) & set(expected_answers)) / len(expected_answers)\n    return recall_k\n\n\ndef compute_retrieval_metric_mean(metric_func, questions_answers, questions_to_weighted_actual_answers, top_count):\n    if top_count <= 0:\n        raise ValueError('top_count should be a natural number')\n\n    return np.mean([\n        metric_func(answers, questions_to_weighted_actual_answers[question], top_count)\n        for question, answers in questions_answers.items()\n    ])\n"""
cakechat/dialog_model/quality/metrics/utils.py,0,"b""import json\n\nfrom cakechat.utils.files_utils import ensure_file\nfrom cakechat.utils.logger import get_logger\n\n_logger = get_logger(__name__)\n\n\nclass MetricsException(Exception):\n    pass\n\n\nclass MetricsSerializer(object):\n    @staticmethod\n    def load_metrics(metrics_resource_name):\n        _logger.info('Restoring metrics from {}'.format(metrics_resource_name))\n        with open(metrics_resource_name, 'r', encoding='utf-8') as fh:\n            return json.load(fh)\n\n    @staticmethod\n    def save_metrics(metrics_resource_name, metrics):\n        _logger.info('Saving metrics to {}'.format(metrics_resource_name))\n        with ensure_file(metrics_resource_name, 'w', encoding='utf-8') as fh:\n            json.dump(metrics, fh, indent=2)\n"""
cakechat/dialog_model/quality/tests/__init__.py,0,b''
cakechat/dialog_model/quality/tests/metrics.py,0,"b""import os\nimport sys\nimport unittest\n\nimport numpy as np\n\nsys.path.append(\n    os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))))\n\nfrom cakechat.dialog_model.quality.metrics.perplexity import _calculate_mean_perplexity\nfrom cakechat.dialog_model.quality.metrics.utils import MetricsException\n\n_DELTA = 1e-3\n\n\nclass TestMetrics(unittest.TestCase):\n    def test_perplexity_inf(self):\n        output_probs = np.array([[0.2, 0.3, 0.5], [1.0, 0.0, 0.0]])\n        output_tokens = np.array([0, 1])\n        sequence_likelihood = np.log(output_probs[np.arange(len(output_probs)), output_tokens]).sum()\n\n        perp = _calculate_mean_perplexity(np.array([output_tokens]), np.array([sequence_likelihood]), 2)\n        perp_exp = np.inf\n\n        self.assertAlmostEqual(perp, perp_exp, delta=_DELTA)\n\n    def test_perplexity(self):\n        output_probs = np.array([[0.2, 0.3, 0.5], [1.0, 0.0, 0.0]])\n        output_tokens = np.array([0, 0])\n        sequence_likelihood = np.log(output_probs[np.arange(len(output_probs)), output_tokens]).sum()\n\n        perp = _calculate_mean_perplexity(np.array([output_tokens]), np.array([sequence_likelihood]), 2)\n        perp_exp = np.exp(-0.5 * np.log(0.2))\n\n        self.assertAlmostEqual(perp, perp_exp, delta=_DELTA)\n\n    def test_perplexity_with_all_skips(self):\n        output_probs = np.array([[0.2, 0.3, 0.5], [1.0, 0.0, 0.0]])\n        output_tokens = np.array([0, 0])\n        sequence_likelihood = np.log(output_probs[np.arange(len(output_probs)), output_tokens]).sum()\n\n        skip_token_id = 0\n\n        with self.assertRaises(MetricsException):\n            _calculate_mean_perplexity(np.array([output_tokens]), np.array([sequence_likelihood]), skip_token_id)\n\n    def test_perplexity_with_skip_token_exp(self):\n        output_probs = np.array([[0.2, 0.3, 0.5], [1.0, 0.0, 0.0]])\n        output_tokens = np.array([2, 1])\n        skip_token_id = 1\n\n        mask = output_tokens != skip_token_id\n        sequence_likelihood = np.log(output_probs[np.arange(len(output_probs[mask])), output_tokens[mask]]).sum()\n\n        perp_exp = np.exp(-np.log(0.5))\n        perp = _calculate_mean_perplexity(np.array([output_tokens]), np.array([sequence_likelihood]), skip_token_id)\n\n        self.assertAlmostEqual(perp, perp_exp, delta=_DELTA)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
