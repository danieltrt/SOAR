file_path,api_count,code
evaluate.py,0,"b'""""""\nThe file defines the evaluate process on target dataset.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom sklearn.metrics import multilabel_confusion_matrix\nfrom utils.helpers import *\nfrom utils.utils import load_image\nimport numpy as np\nimport argparse\nimport sys\nimport cv2\nimport os\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--dataset\', help=\'The path of the dataset.\', type=str, default=\'CamVid\')\nparser.add_argument(\'--crop_height\', help=\'The height to crop the image.\', type=int, default=256)\nparser.add_argument(\'--crop_width\', help=\'The width to crop the image.\', type=int, default=256)\nparser.add_argument(\'--predictions\', help=\'The path of predicted image.\', type=str, required=True)\n\nargs = parser.parse_args()\n\n# check related paths\npaths = check_related_path(os.getcwd())\n\n# get image and label file names for training and validation\n_, _, _, _, _, test_label_names = get_dataset_info(args.dataset)\n\n# get color info\ncsv_file = os.path.join(args.dataset, \'class_dict.csv\')\n\nclass_names, _ = get_colored_info(csv_file)\n\n# get the prediction file name list\nif not os.path.exists(args.predictions):\n    raise ValueError(\'the path of predictions does not exit.\')\n\nprediction_names = []\nfor file in sorted(os.listdir(args.predictions)):\n    prediction_names.append(os.path.join(args.predictions, file))\n\n# evaluated classes\nevaluated_classes = get_evaluated_classes(os.path.join(args.dataset, \'evaluated_classes.txt\'))\n\nnum_classes = len(class_names)\nclass_iou = dict()\nfor name in evaluated_classes:\n    class_iou[name] = list()\n\nclass_idx = dict(zip(class_names, range(num_classes)))\n\n# begin evaluate\nassert len(test_label_names) == len(prediction_names)\n\nfor i, (name1, name2) in enumerate(zip(test_label_names, prediction_names)):\n    sys.stdout.write(\'\\rRunning test image %d / %d\' % (i + 1, len(test_label_names)))\n    sys.stdout.flush()\n\n    label = np.array(cv2.resize(load_image(name1),\n                                dsize=(args.crop_width, args.crop_height), interpolation=cv2.INTER_NEAREST))\n    pred = np.array(cv2.resize(load_image(name2),\n                               dsize=(args.crop_width, args.crop_height), interpolation=cv2.INTER_NEAREST))\n\n    confusion_matrix = multilabel_confusion_matrix(label.flatten(), pred.flatten(), labels=list(class_idx.values()))\n    for eval_cls in evaluated_classes:\n        eval_idx = class_idx[eval_cls]\n        (tn, fp), (fn, tp) = confusion_matrix[eval_idx]\n\n        if tp + fn > 0:\n            class_iou[eval_cls].append(tp / (tp + fp + fn))\n\nprint(\'\\n****************************************\')\nprint(\'* The IoU of each class is as follows: *\')\nprint(\'****************************************\')\nfor eval_cls in evaluated_classes:\n    class_iou[eval_cls] = np.mean(class_iou[eval_cls])\n    print(\'{cls:}: {iou:.4f}\'.format(cls=eval_cls, iou=class_iou[eval_cls]))\n\nprint(\'\\n**********************************************\')\nprint(\'* The Mean IoU of all classes is as follows: *\')\nprint(\'**********************************************\')\nprint(\'Mean IoU: {mean_iou:.4f}\'.format(mean_iou=np.mean(list(class_iou.values()))))\n'"
predict.py,0,"b'""""""\nThe file defines the predict process of a single RGB image.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom utils.helpers import check_related_path, get_colored_info, color_encode\nfrom utils.utils import load_image, decode_one_hot\nfrom keras_applications import imagenet_utils\nfrom builders import builder\nfrom PIL import Image\nimport numpy as np\nimport argparse\nimport sys\nimport cv2\nimport os\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model\', help=\'Choose the semantic segmentation methods.\', type=str, required=True)\nparser.add_argument(\'--base_model\', help=\'Choose the backbone model.\', type=str, default=None)\nparser.add_argument(\'--csv_file\', help=\'The path of color code csv file.\', type=str, default=None)\nparser.add_argument(\'--num_classes\', help=\'The number of classes to be segmented.\', type=int, required=True)\nparser.add_argument(\'--crop_height\', help=\'The height to crop the image.\', type=int, default=256)\nparser.add_argument(\'--crop_width\', help=\'The width to crop the image.\', type=int, default=256)\nparser.add_argument(\'--weights\', help=\'The path of weights to be loaded.\', type=str, default=None)\nparser.add_argument(\'--image_path\', help=\'The path of predicted image.\', type=str, required=True)\nparser.add_argument(\'--color_encode\', help=\'Whether to color encode the prediction.\', type=str2bool, default=True)\n\nargs = parser.parse_args()\n\n# check related paths\npaths = check_related_path(os.getcwd())\n\n# check the image path\nif not os.path.exists(args.image_path):\n    raise ValueError(\'The path \\\'{image_path}\\\' does not exist the image file.\'.format(image_path=args.image_path))\n\n# build the model\nnet, base_model = builder(args.num_classes, (args.crop_height, args.crop_width), args.model, args.base_model)\n\n# load weights\nprint(\'Loading the weights...\')\nif args.weights is None:\n    net.load_weights(filepath=os.path.join(\n        paths[\'weigths_path\'], \'{model}_based_on_{base_model}.h5\'.format(model=args.model, base_model=base_model)))\nelse:\n    if not os.path.exists(args.weights):\n        raise ValueError(\'The weights file does not exist in \\\'{path}\\\'\'.format(path=args.weights))\n    net.load_weights(args.weights)\n\n# begin testing\nprint(""\\n***** Begin testing *****"")\nprint(""Model -->"", args.model)\nprint(""Base Model -->"", base_model)\nprint(""Crop Height -->"", args.crop_height)\nprint(""Crop Width -->"", args.crop_width)\nprint(""Num Classes -->"", args.num_classes)\n\nprint("""")\n\n# load_images\nimage_names=list()\nif os.path.isfile(args.image_path):\n    image_names.append(args.image_path)\nelse:\n    for f in os.listdir(args.image_path):\n        image_names.append(os.path.join(args.image_path, f))\n    image_names.sort()\n\n# get color info\nif args.color_csv is None:\n    csv_file = os.path.join(\'CamVid\', \'class_dict.csv\')\nelse:\n    csv_file = args.csv_file\n\n_, color_values = get_colored_info(csv_file)\n\nfor i, name in enumerate(image_names):\n    sys.stdout.write(\'\\rRunning test image %d / %d\'%(i+1, len(image_names)))\n    sys.stdout.flush()\n\n    image = cv2.resize(load_image(name),\n                       dsize=(args.crop_width, args.crop_height))\n    image = imagenet_utils.preprocess_input(image.astype(np.float32), data_format=\'channels_last\', mode=\'torch\')\n\n    # image processing\n    if np.ndim(image) == 3:\n        image = np.expand_dims(image, axis=0)\n    assert np.ndim(image) == 4\n\n    # get the prediction\n    prediction = net.predict(image)\n\n    if np.ndim(prediction) == 4:\n        prediction = np.squeeze(prediction, axis=0)\n\n    # decode one-hot\n    prediction = decode_one_hot(prediction)\n\n    # color encode\n    if args.color_encode:\n        prediction = color_encode(prediction, color_values)\n\n    # get PIL file\n    prediction = Image.fromarray(np.uint8(prediction))\n\n    # save the prediction\n    _, file_name = os.path.split(name)\n    prediction.save(os.path.join(paths[\'prediction_path\'], file_name))\n'"
test.py,1,"b'""""""\nThe file defines the testing process.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom utils.data_generator import ImageDataGenerator\nfrom utils.helpers import get_dataset_info, check_related_path\nfrom utils.losses import categorical_crossentropy_with_logits\nfrom utils.metrics import MeanIoU\nfrom builders import builder\nimport tensorflow as tf\nimport argparse\nimport os\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model\', help=\'Choose the semantic segmentation methods.\', type=str, required=True)\nparser.add_argument(\'--base_model\', help=\'Choose the backbone model.\', type=str, default=None)\nparser.add_argument(\'--dataset\', help=\'The path of the dataset.\', type=str, required=True)\nparser.add_argument(\'--num_classes\', help=\'The number of classes to be segmented.\', type=int, required=True)\nparser.add_argument(\'--crop_height\', help=\'The height to crop the image.\', type=int, default=256)\nparser.add_argument(\'--crop_width\', help=\'The width to crop the image.\', type=int, default=256)\nparser.add_argument(\'--batch_size\', help=\'The training batch size.\', type=int, default=5)\nparser.add_argument(\'--weights\', help=\'The path of weights to be loaded.\', type=str, default=None)\n\nargs = parser.parse_args()\n\n# check related paths\npaths = check_related_path(os.getcwd())\n\n# get image and label file names for training and validation\n_, _, _, _, test_image_names, test_label_names = get_dataset_info(args.dataset)\n\n# build the model\nnet, base_model = builder(args.num_classes, (args.crop_height, args.crop_width), args.model, args.base_model)\n\n# summary\nnet.summary()\n\n# load weights\nprint(\'Loading the weights...\')\nif args.weights is None:\n    net.load_weights(filepath=os.path.join(\n        paths[\'weigths_path\'], \'{model}_based_on_{base_model}.h5\'.format(model=args.model, base_model=base_model)))\nelse:\n    if not os.path.exists(args.weights):\n        raise ValueError(\'The weights file does not exist in \\\'{path}\\\'\'.format(path=args.weights))\n    net.load_weights(args.weights)\n\n# compile the model\nnet.compile(optimizer=tf.keras.optimizers.Adam(),\n            loss=categorical_crossentropy_with_logits,\n            metrics=[MeanIoU(args.num_classes)])\n# data generator\ntest_gen = ImageDataGenerator()\n\ntest_generator = test_gen.flow(images_list=test_image_names,\n                               labels_list=test_label_names,\n                               num_classes=args.num_classes,\n                               batch_size=args.batch_size,\n                               target_size=(args.crop_height, args.crop_width))\n\n# begin testing\nprint(""\\n***** Begin testing *****"")\nprint(""Dataset -->"", args.dataset)\nprint(""Model -->"", args.model)\nprint(""Base Model -->"", base_model)\nprint(""Crop Height -->"", args.crop_height)\nprint(""Crop Width -->"", args.crop_width)\nprint(""Batch Size -->"", args.batch_size)\nprint(""Num Classes -->"", args.num_classes)\n\nprint("""")\n\n# some other training parameters\nsteps = len(test_image_names) // args.batch_size\n\n# testing\nscores = net.evaluate_generator(test_generator, steps=steps, workers=os.cpu_count(), use_multiprocessing=False)\n\nprint(\'loss={loss:0.4f}, MeanIoU={mean_iou:0.4f}\'.format(loss=scores[0], mean_iou=scores[1]))\n'"
train.py,6,"b'""""""\nThe file defines the training process.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom utils.data_generator import ImageDataGenerator\nfrom utils.helpers import get_dataset_info, check_related_path\nfrom utils.callbacks import LearningRateScheduler\nfrom utils.optimizers import *\nfrom utils.losses import *\nfrom utils.learning_rate import *\nfrom utils.metrics import MeanIoU\nfrom utils import utils\nfrom builders import builder\nimport tensorflow as tf\nimport argparse\nimport os\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model\', help=\'Choose the semantic segmentation methods.\', type=str, required=True)\nparser.add_argument(\'--base_model\', help=\'Choose the backbone model.\', type=str, default=None)\nparser.add_argument(\'--dataset\', help=\'The path of the dataset.\', type=str, default=\'CamVid\')\nparser.add_argument(\'--loss\', help=\'The loss function for traing.\', type=str, default=None,\n                    choices=[\'ce\', \'focal_loss\', \'miou_loss\', \'self_balanced_focal_loss\'])\nparser.add_argument(\'--num_classes\', help=\'The number of classes to be segmented.\', type=int, default=32)\nparser.add_argument(\'--random_crop\', help=\'Whether to randomly crop the image.\', type=str2bool, default=False)\nparser.add_argument(\'--crop_height\', help=\'The height to crop the image.\', type=int, default=256)\nparser.add_argument(\'--crop_width\', help=\'The width to crop the image.\', type=int, default=256)\nparser.add_argument(\'--batch_size\', help=\'The training batch size.\', type=int, default=5)\nparser.add_argument(\'--valid_batch_size\', help=\'The validation batch size.\', type=int, default=1)\nparser.add_argument(\'--num_epochs\', help=\'The number of epochs to train for.\', type=int, default=100)\nparser.add_argument(\'--initial_epoch\', help=\'The initial epoch of training.\', type=int, default=0)\nparser.add_argument(\'--h_flip\', help=\'Whether to randomly flip the image horizontally.\', type=str2bool, default=False)\nparser.add_argument(\'--v_flip\', help=\'Whether to randomly flip the image vertically.\', type=str2bool, default=False)\nparser.add_argument(\'--brightness\', help=\'Randomly change the brightness (list).\', type=float, default=None, nargs=\'+\')\nparser.add_argument(\'--rotation\', help=\'The angle to randomly rotate the image.\', type=float, default=0.)\nparser.add_argument(\'--zoom_range\', help=\'The times for zooming the image.\', type=float, default=0., nargs=\'+\')\nparser.add_argument(\'--channel_shift\', help=\'The channel shift range.\', type=float, default=0.)\nparser.add_argument(\'--data_aug_rate\', help=\'The rate of data augmentation.\', type=float, default=0.)\nparser.add_argument(\'--checkpoint_freq\', help=\'How often to save a checkpoint.\', type=int, default=1)\nparser.add_argument(\'--validation_freq\', help=\'How often to perform validation.\', type=int, default=1)\nparser.add_argument(\'--num_valid_images\', help=\'The number of images used for validation.\', type=int, default=20)\nparser.add_argument(\'--data_shuffle\', help=\'Whether to shuffle the data.\', type=str2bool, default=True)\nparser.add_argument(\'--random_seed\', help=\'The random shuffle seed.\', type=int, default=None)\nparser.add_argument(\'--weights\', help=\'The path of weights to be loaded.\', type=str, default=None)\nparser.add_argument(\'--steps_per_epoch\', help=\'The training steps of each epoch\', type=int, default=None)\nparser.add_argument(\'--lr_scheduler\', help=\'The strategy to schedule learning rate.\', type=str, default=\'cosine_decay\',\n                    choices=[\'step_decay\', \'poly_decay\', \'cosine_decay\'])\nparser.add_argument(\'--lr_warmup\', help=\'Whether to use lr warm up.\', type=bool, default=False)\nparser.add_argument(\'--learning_rate\', help=\'The initial learning rate.\', type=float, default=3e-4)\nparser.add_argument(\'--optimizer\', help=\'The optimizer for training.\', type=str, default=\'adam\',\n                    choices=[\'sgd\', \'adam\', \'nadam\', \'adamw\', \'nadamw\', \'sgdw\'])\n\nargs = parser.parse_args()\n\n# check related paths\npaths = check_related_path(os.getcwd())\n\n# get image and label file names for training and validation\ntrain_image_names, train_label_names, valid_image_names, valid_label_names, _, _ = get_dataset_info(args.dataset)\n\n# build the model\nnet, base_model = builder(args.num_classes, (args.crop_height, args.crop_width), args.model, args.base_model)\n\n# summary\nnet.summary()\n\n# load weights\nif args.weights is not None:\n    print(\'Loading the weights...\')\n    net.load_weights(args.weights)\n\n# chose loss\nlosses = {\'ce\': categorical_crossentropy_with_logits,\n          \'focal_loss\': focal_loss(),\n          \'miou_loss\': miou_loss(num_classes=args.num_classes),\n          \'self_balanced_focal_loss\': self_balanced_focal_loss()}\nloss = losses[args.loss] if args.loss is not None else categorical_crossentropy_with_logits\n\n# chose optimizer\ntotal_iterations = len(train_image_names) * args.num_epochs // args.batch_size\nwd_dict = utils.get_weight_decays(net)\nordered_values = []\nweight_decays = utils.fill_dict_in_order(wd_dict, ordered_values)\n\noptimizers = {\'adam\': tf.keras.optimizers.Adam(learning_rate=args.learning_rate),\n              \'nadam\': tf.keras.optimizers.Nadam(learning_rate=args.learning_rate),\n              \'sgd\': tf.keras.optimizers.SGD(learning_rate=args.learning_rate, momentum=0.99),\n              \'adamw\': AdamW(learning_rate=args.learning_rate, batch_size=args.batch_size,\n                             total_iterations=total_iterations),\n              \'nadamw\': NadamW(learning_rate=args.learning_rate, batch_size=args.batch_size,\n                               total_iterations=total_iterations),\n              \'sgdw\': SGDW(learning_rate=args.learning_rate, momentum=0.99, batch_size=args.batch_size,\n                           total_iterations=total_iterations)}\n\n# lr schedule strategy\nif args.lr_warmup and args.num_epochs - 5 <= 0:\n    raise ValueError(\'num_epochs must be larger than 5 if lr warm up is used.\')\n\nlr_decays = {\'step_decay\': step_decay(args.learning_rate, args.num_epochs - 5 if args.lr_warmup else args.num_epochs,\n                                      warmup=args.lr_warmup),\n             \'poly_decay\': poly_decay(args.learning_rate, args.num_epochs - 5 if args.lr_warmup else args.num_epochs,\n                                      warmup=args.lr_warmup),\n             \'cosine_decay\': cosine_decay(args.num_epochs - 5 if args.lr_warmup else args.num_epochs,\n                                          args.learning_rate, warmup=args.lr_warmup)}\nlr_decay = lr_decays[args.lr_scheduler]\n\n# training and validation steps\nsteps_per_epoch = len(train_image_names) // args.batch_size if not args.steps_per_epoch else args.steps_per_epoch\nvalidation_steps = args.num_valid_images // args.valid_batch_size\n\n# compile the model\nnet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=args.learning_rate),\n            loss=loss,\n            metrics=[MeanIoU(args.num_classes)])\n# data generator\n# data augmentation setting\ntrain_gen = ImageDataGenerator(random_crop=args.random_crop,\n                               rotation_range=args.rotation,\n                               brightness_range=args.brightness,\n                               zoom_range=args.zoom_range,\n                               channel_shift_range=args.channel_shift,\n                               horizontal_flip=args.v_flip,\n                               vertical_flip=args.v_flip)\n\nvalid_gen = ImageDataGenerator()\n\ntrain_generator = train_gen.flow(images_list=train_image_names,\n                                 labels_list=train_label_names,\n                                 num_classes=args.num_classes,\n                                 batch_size=args.batch_size,\n                                 target_size=(args.crop_height, args.crop_width),\n                                 shuffle=args.data_shuffle,\n                                 seed=args.random_seed,\n                                 data_aug_rate=args.data_aug_rate)\n\nvalid_generator = valid_gen.flow(images_list=valid_image_names,\n                                 labels_list=valid_label_names,\n                                 num_classes=args.num_classes,\n                                 batch_size=args.valid_batch_size,\n                                 target_size=(args.crop_height, args.crop_width))\n\n# callbacks setting\n# checkpoint setting\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=os.path.join(paths[\'checkpoints_path\'],\n                          \'{model}_based_on_{base}_\'.format(model=args.model, base=base_model) +\n                          \'miou_{val_mean_io_u:04f}_\' + \'ep_{epoch:02d}.h5\'),\n    save_best_only=True, period=args.checkpoint_freq, monitor=\'val_mean_io_u\', mode=\'max\')\n# tensorboard setting\ntensorboard = tf.keras.callbacks.TensorBoard(log_dir=paths[\'logs_path\'])\n# learning rate scheduler setting\nlearning_rate_scheduler = LearningRateScheduler(lr_decay, args.learning_rate, args.lr_warmup, steps_per_epoch,\n                                                verbose=1)\n\ncallbacks = [model_checkpoint, tensorboard, learning_rate_scheduler]\n\n# begin training\nprint(""\\n***** Begin training *****"")\nprint(""Dataset -->"", args.dataset)\nprint(""Num Images -->"", len(train_image_names))\nprint(""Model -->"", args.model)\nprint(""Base Model -->"", base_model)\nprint(""Crop Height -->"", args.crop_height)\nprint(""Crop Width -->"", args.crop_width)\nprint(""Num Epochs -->"", args.num_epochs)\nprint(""Initial Epoch -->"", args.initial_epoch)\nprint(""Batch Size -->"", args.batch_size)\nprint(""Num Classes -->"", args.num_classes)\n\nprint(""Data Augmentation:"")\nprint(""\\tData Augmentation Rate -->"", args.data_aug_rate)\nprint(""\\tVertical Flip -->"", args.v_flip)\nprint(""\\tHorizontal Flip -->"", args.h_flip)\nprint(""\\tBrightness Alteration -->"", args.brightness)\nprint(""\\tRotation -->"", args.rotation)\nprint(""\\tZoom -->"", args.zoom_range)\nprint(""\\tChannel Shift -->"", args.channel_shift)\n\nprint("""")\n\n# training...\nnet.fit_generator(train_generator,\n                  steps_per_epoch=steps_per_epoch,\n                  epochs=args.num_epochs,\n                  callbacks=callbacks,\n                  validation_data=valid_generator,\n                  validation_steps=validation_steps,\n                  validation_freq=args.validation_freq,\n                  max_queue_size=10,\n                  workers=os.cpu_count(),\n                  use_multiprocessing=False,\n                  initial_epoch=args.initial_epoch)\n\n# save weights\nnet.save(filepath=os.path.join(\n    paths[\'weights_path\'], \'{model}_based_on_{base_model}.h5\'.format(model=args.model, base_model=base_model)))\n'"
base_models/__init__.py,0,b'from base_models.densenet import DenseNet\nfrom base_models.resnet import ResNet\nfrom base_models.vgg import VGG\nfrom base_models.xception import Xception\nfrom base_models.mobilenet import MobileNet'
base_models/densenet.py,2,"b'""""""\nThe implementation of DenseNet121/169/201/264 based on Tensorflow.\nSome codes are based on official tensorflow source codes.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom utils.layers import Concatenate\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nbackend = tf.keras.backend\n\n\nclass DenseNet(object):\n    def __init__(self, version=\'DenseNet121\', dilation=None, **kwargs):\n        """"""\n        The implementation of DenseNet based on Tensorflow.\n        :param version: \'DenseNet121\', \'DenseNet169\', \'DenseNet201\' or \'DenseNet264\'.\n        :param dilation: Whether to use dilation strategy.\n        :param kwargs: other parameters.\n        """"""\n        super(DenseNet, self).__init__(**kwargs)\n        params = {\'DenseNet121\': [6, 12, 24, 16],\n                  \'DenseNet169\': [6, 12, 32, 32],\n                  \'DenseNet201\': [6, 12, 48, 32],\n                  \'DenseNet264\': [6, 12, 64, 48]}\n        self.version = version\n        assert version in params\n        self.params = params[version]\n\n        if dilation is None:\n            self.dilation = [1, 1]\n        else:\n            self.dilation = dilation\n        assert len(self.dilation) == 2\n\n    def _dense_block(self, x, blocks, name, dilation=1):\n        """"""A dense block.\n\n        # Arguments\n            x: input tensor.\n            blocks: integer, the number of building blocks.\n            name: string, block label.\n\n        # Returns\n            output tensor for the block.\n        """"""\n        for i in range(blocks):\n            x = self._conv_block(x, 32, name=name + \'_block\' + str(i + 1), dilation=dilation)\n        return x\n\n    def _transition_block(self, x, reduction, name, dilation=1):\n        """"""A transition block.\n\n        # Arguments\n            x: input tensor.\n            reduction: float, compression rate at transition layers.\n            name: string, block label.\n\n        # Returns\n            output tensor for the block.\n        """"""\n        bn_axis = 3 if backend.image_data_format() == \'channels_last\' else 1\n        x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n                                      name=name + \'_bn\')(x)\n        x = layers.Activation(\'relu\', name=name + \'_relu\')(x)\n        x = layers.Conv2D(int(backend.int_shape(x)[bn_axis] * reduction), 1,\n                          use_bias=False,\n                          name=name + \'_conv\',\n                          dilation_rate=dilation)(x)\n        if dilation == 1:\n            x = layers.AveragePooling2D(2, strides=2, name=name + \'_pool\')(x)\n        return x\n\n    def _conv_block(self, x, growth_rate, name, dilation=1):\n        """"""A building block for a dense block.\n\n        # Arguments\n            x: input tensor.\n            growth_rate: float, growth rate at dense layers.\n            name: string, block label.\n\n        # Returns\n            Output tensor for the block.\n        """"""\n        _, h, w, _ = backend.int_shape(x)\n\n        bn_axis = 3 if backend.image_data_format() == \'channels_last\' else 1\n        x1 = layers.BatchNormalization(axis=bn_axis,\n                                       epsilon=1.001e-5,\n                                       name=name + \'_0_bn\')(x)\n        x1 = layers.Activation(\'relu\', name=name + \'_0_relu\')(x1)\n        x1 = layers.Conv2D(4 * growth_rate, 1,\n                           use_bias=False,\n                           name=name + \'_1_conv\')(x1)\n        x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n                                       name=name + \'_1_bn\')(x1)\n        x1 = layers.Activation(\'relu\', name=name + \'_1_relu\')(x1)\n        x1 = layers.Conv2D(growth_rate, 3,\n                           padding=\'same\',\n                           use_bias=False,\n                           name=name + \'_2_conv\',\n                           dilation_rate=dilation)(x1)\n        x = Concatenate(out_size=(h, w), axis=bn_axis, name=name + \'_concat\')([x, x1])\n        return x\n\n    def __call__(self, inputs, output_stages=\'c5\', **kwargs):\n        """"""\n        call for DenseNet.\n        :param inputs: a 4-D tensor.\n        :param output_stages: str or a list of str containing the output stages.\n        :param kwargs: other parameters.\n        :return: the output of different stages.\n        """"""\n        _, h, w, _ = backend.int_shape(inputs)\n\n        blocks = self.params\n        dilation = self.dilation\n        bn_axis = 3 if backend.image_data_format() == \'channels_last\' else 1\n\n        x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(inputs)\n        x = layers.Conv2D(64, 7, strides=2, use_bias=False, name=\'conv1/conv\')(x)\n        x = layers.BatchNormalization(\n            axis=bn_axis, epsilon=1.001e-5, name=\'conv1/bn\')(x)\n        x = layers.Activation(\'relu\', name=\'conv1/relu\')(x)\n        x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n        x = layers.MaxPooling2D(3, strides=2, name=\'pool1\')(x)\n        c1 = x\n\n        x = self._dense_block(x, blocks[0], name=\'conv2\')\n        x = self._transition_block(x, 0.5, name=\'pool2\')\n        c2 = x\n\n        x = self._dense_block(x, blocks[1], name=\'conv3\')\n        x = self._transition_block(x, 0.5, name=\'pool3\', dilation=dilation[0])\n        c3 = x\n\n        x = self._dense_block(x, blocks[2], name=\'conv4\', dilation=dilation[0])\n        x = self._transition_block(x, 0.5, name=\'pool4\', dilation=dilation[1])\n        c4 = x\n\n        x = self._dense_block(x, blocks[3], name=\'conv5\', dilation=dilation[1])\n        x = layers.BatchNormalization(\n            axis=bn_axis, epsilon=1.001e-5, name=\'bn\')(x)\n        x = layers.Activation(\'relu\', name=\'relu\')(x)\n        c5 = x\n\n        self.outputs = {\'c1\': c1,\n                        \'c2\': c2,\n                        \'c3\': c3,\n                        \'c4\': c4,\n                        \'c5\': c5}\n\n        if type(output_stages) is not list:\n            return self.outputs[output_stages]\n        else:\n            return [self.outputs[ci] for ci in output_stages]\n'"
base_models/mobilenet.py,2,"b'""""""\nThe implementation of MobileNetV1/V2 based on Tensorflow.\nSome codes are based on official tensorflow source codes.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nbackend = tf.keras.backend\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef _correct_pad(backend, inputs, kernel_size):\n    """"""Returns a tuple for zero-padding for 2D convolution with downsampling.\n\n    # Arguments\n        input_size: An integer or tuple/list of 2 integers.\n        kernel_size: An integer or tuple/list of 2 integers.\n\n    # Returns\n        A tuple.\n    """"""\n    img_dim = 2 if backend.image_data_format() == \'channels_first\' else 1\n    input_size = backend.int_shape(inputs)[img_dim:(img_dim + 2)]\n\n    if isinstance(kernel_size, int):\n        kernel_size = (kernel_size, kernel_size)\n\n    if input_size[0] is None:\n        adjust = (1, 1)\n    else:\n        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n\n    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n\n    return ((correct[0] - adjust[0], correct[0]),\n            (correct[1] - adjust[1], correct[1]))\n\n\nclass MobileNet(object):\n    def __init__(self, version=\'MobileNetV2\', dilation=None, **kwargs):\n        """"""\n        The implementation of MobileNetV1 and MobileNetV2 based on Tensorflow.\n        :param version: \'MobileNetV1\' or \'MobileNetV2\'\n        :param dilation: Whether to use dialtion strategy\n        :param kwargs: other parameters\n        """"""\n        super(MobileNet, self).__init__(**kwargs)\n        self.version = version\n        self.mobilenet = {\'MobileNetV1\': self._mobilenet_v1,\n                          \'MobileNetV2\': self._mobilenet_v2}\n        assert version in self.mobilenet\n\n        if dilation is None:\n            self.dilation = [1, 1]\n        else:\n            self.dilation = dilation\n        assert len(self.dilation) == 2\n\n    def __call__(self, inputs, output_stages=\'c5\', **kwargs):\n        """"""\n        call for MobileNetV1 or MobileNetV2.\n        :param inputs: a 4-D tensor\n        :param output_stages: str or a list of str indicating the output stages.\n        :param kwargs: other parameters\n        :return: a 4-D tensor\n        """"""\n        net = self.mobilenet[self.version]\n        c1, c2, c3, c4, c5 = net(inputs)\n\n        self.outputs = {\'c1\': c1,\n                        \'c2\': c2,\n                        \'c3\': c3,\n                        \'c4\': c4,\n                        \'c5\': c5}\n\n        if type(output_stages) is not list:\n            return self.outputs[output_stages]\n        else:\n            return [self.outputs[ci] for ci in output_stages]\n\n    def _inverted_res_block_v2(self, inputs, expansion, stride, alpha, filters, block_id, dilation=1):\n        """"""\n        inverted residual block in MobileNetV2.\n        :param inputs: a 4-D tensor\n        :param expansion: the expansion rate.\n        :param stride: stride for convolution\n        :param alpha: controls the width of the network.\n        :param filters: output filters\n        :param block_id: block id\n        :param dilation: dilation rate\n        :return: a 4-D tensor\n        """"""\n        channel_axis = 1 if backend.image_data_format() == \'channels_first\' else -1\n\n        in_channels = backend.int_shape(inputs)[channel_axis]\n        pointwise_conv_filters = int(filters * alpha)\n        pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n        x = inputs\n        prefix = \'block_{}_\'.format(block_id)\n\n        if block_id:\n            # Expand\n            x = layers.Conv2D(expansion * in_channels,\n                              kernel_size=1,\n                              padding=\'same\',\n                              use_bias=False,\n                              activation=None,\n                              name=prefix + \'expand\')(x)\n            x = layers.BatchNormalization(axis=channel_axis,\n                                          epsilon=1e-3,\n                                          momentum=0.999,\n                                          name=prefix + \'expand_BN\')(x)\n            x = layers.ReLU(6., name=prefix + \'expand_relu\')(x)\n        else:\n            prefix = \'expanded_conv_\'\n\n        # Depthwise\n        if stride == 2 and dilation == 1:\n            x = layers.ZeroPadding2D(padding=_correct_pad(backend, x, 3),\n                                     name=prefix + \'pad\')(x)\n        x = layers.DepthwiseConv2D(kernel_size=3,\n                                   strides=stride if dilation == 1 else 1,\n                                   activation=None,\n                                   use_bias=False,\n                                   padding=\'valid\' if stride == 2 and dilation == 1 else \'same\',\n                                   name=prefix + \'depthwise\',\n                                   dilation_rate=dilation)(x)\n        x = layers.BatchNormalization(axis=channel_axis,\n                                      epsilon=1e-3,\n                                      momentum=0.999,\n                                      name=prefix + \'depthwise_BN\')(x)\n\n        x = layers.ReLU(6., name=prefix + \'depthwise_relu\')(x)\n\n        # Project\n        x = layers.Conv2D(pointwise_filters,\n                          kernel_size=1,\n                          padding=\'same\',\n                          use_bias=False,\n                          activation=None,\n                          name=prefix + \'project\')(x)\n        x = layers.BatchNormalization(axis=channel_axis,\n                                      epsilon=1e-3,\n                                      momentum=0.999,\n                                      name=prefix + \'project_BN\')(x)\n\n        if in_channels == pointwise_filters and stride == 1:\n            return layers.Add(name=prefix + \'add\')([inputs, x])\n        return x\n\n    def _conv_block_v1(self, inputs, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n        """"""Adds an initial convolution layer (with batch normalization and relu6).\n\n        # Arguments\n            inputs: Input tensor of shape `(rows, cols, 3)`\n                (with `channels_last` data format) or\n                (3, rows, cols) (with `channels_first` data format).\n                It should have exactly 3 inputs channels,\n                and width and height should be no smaller than 32.\n                E.g. `(224, 224, 3)` would be one valid value.\n            filters: Integer, the dimensionality of the output space\n                (i.e. the number of output filters in the convolution).\n            alpha: controls the width of the network.\n                - If `alpha` < 1.0, proportionally decreases the number\n                    of filters in each layer.\n                - If `alpha` > 1.0, proportionally increases the number\n                    of filters in each layer.\n                - If `alpha` = 1, default number of filters from the paper\n                     are used at each layer.\n            kernel: An integer or tuple/list of 2 integers, specifying the\n                width and height of the 2D convolution window.\n                Can be a single integer to specify the same value for\n                all spatial dimensions.\n            strides: An integer or tuple/list of 2 integers,\n                specifying the strides of the convolution\n                along the width and height.\n                Can be a single integer to specify the same value for\n                all spatial dimensions.\n                Specifying any stride value != 1 is incompatible with specifying\n                any `dilation_rate` value != 1.\n\n        # Input shape\n            4D tensor with shape:\n            `(samples, channels, rows, cols)` if data_format=\'channels_first\'\n            or 4D tensor with shape:\n            `(samples, rows, cols, channels)` if data_format=\'channels_last\'.\n\n        # Output shape\n            4D tensor with shape:\n            `(samples, filters, new_rows, new_cols)`\n            if data_format=\'channels_first\'\n            or 4D tensor with shape:\n            `(samples, new_rows, new_cols, filters)`\n            if data_format=\'channels_last\'.\n            `rows` and `cols` values might have changed due to stride.\n\n        # Returns\n            Output tensor of block.\n        """"""\n        channel_axis = 1 if backend.image_data_format() == \'channels_first\' else -1\n        filters = int(filters * alpha)\n        x = layers.ZeroPadding2D(padding=((0, 1), (0, 1)), name=\'conv1_pad\')(inputs)\n        x = layers.Conv2D(filters, kernel,\n                          padding=\'valid\',\n                          use_bias=False,\n                          strides=strides,\n                          name=\'conv1\')(x)\n        x = layers.BatchNormalization(axis=channel_axis, name=\'conv1_bn\')(x)\n        return layers.ReLU(6., name=\'conv1_relu\')(x)\n\n    def _depthwise_conv_block_v1(self, inputs, pointwise_conv_filters, alpha,\n                                 depth_multiplier=1, strides=(1, 1), block_id=1, dilation=1):\n        """"""Adds a depthwise convolution block.\n\n        A depthwise convolution block consists of a depthwise conv,\n        batch normalization, relu6, pointwise convolution,\n        batch normalization and relu6 activation.\n\n        # Arguments\n            inputs: Input tensor of shape `(rows, cols, channels)`\n                (with `channels_last` data format) or\n                (channels, rows, cols) (with `channels_first` data format).\n            pointwise_conv_filters: Integer, the dimensionality of the output space\n                (i.e. the number of output filters in the pointwise convolution).\n            alpha: controls the width of the network.\n                - If `alpha` < 1.0, proportionally decreases the number\n                    of filters in each layer.\n                - If `alpha` > 1.0, proportionally increases the number\n                    of filters in each layer.\n                - If `alpha` = 1, default number of filters from the paper\n                     are used at each layer.\n            depth_multiplier: The number of depthwise convolution output channels\n                for each input channel.\n                The total number of depthwise convolution output\n                channels will be equal to `filters_in * depth_multiplier`.\n            strides: An integer or tuple/list of 2 integers,\n                specifying the strides of the convolution\n                along the width and height.\n                Can be a single integer to specify the same value for\n                all spatial dimensions.\n                Specifying any stride value != 1 is incompatible with specifying\n                any `dilation_rate` value != 1.\n            block_id: Integer, a unique identification designating\n                the block number.\n\n        # Input shape\n            4D tensor with shape:\n            `(batch, channels, rows, cols)` if data_format=\'channels_first\'\n            or 4D tensor with shape:\n            `(batch, rows, cols, channels)` if data_format=\'channels_last\'.\n\n        # Output shape\n            4D tensor with shape:\n            `(batch, filters, new_rows, new_cols)`\n            if data_format=\'channels_first\'\n            or 4D tensor with shape:\n            `(batch, new_rows, new_cols, filters)`\n            if data_format=\'channels_last\'.\n            `rows` and `cols` values might have changed due to stride.\n\n        # Returns\n            Output tensor of block.\n        """"""\n        channel_axis = 1 if backend.image_data_format() == \'channels_first\' else -1\n        pointwise_conv_filters = int(pointwise_conv_filters * alpha)\n\n        strides = (1, 1) if dilation > 1 else strides\n\n        if strides == (1, 1):\n            x = inputs\n        else:\n            x = layers.ZeroPadding2D(((0, 1), (0, 1)),\n                                     name=\'conv_pad_%d\' % block_id)(inputs)\n        x = layers.DepthwiseConv2D((3, 3),\n                                   padding=\'same\' if strides == (1, 1) else \'valid\',\n                                   depth_multiplier=depth_multiplier,\n                                   strides=strides,\n                                   use_bias=False,\n                                   name=\'conv_dw_%d\' % block_id,\n                                   dilation_rate=dilation)(x)\n        x = layers.BatchNormalization(\n            axis=channel_axis, name=\'conv_dw_%d_bn\' % block_id)(x)\n        x = layers.ReLU(6., name=\'conv_dw_%d_relu\' % block_id)(x)\n\n        x = layers.Conv2D(pointwise_conv_filters, (1, 1),\n                          padding=\'same\',\n                          use_bias=False,\n                          strides=(1, 1),\n                          name=\'conv_pw_%d\' % block_id)(x)\n        x = layers.BatchNormalization(axis=channel_axis,\n                                      name=\'conv_pw_%d_bn\' % block_id)(x)\n        return layers.ReLU(6., name=\'conv_pw_%d_relu\' % block_id)(x)\n\n    def _mobilenet_v1(self, inputs, alpha=1.0, depth_multiplier=1):\n        """"""\n        call for MobileNetV1.\n        :param inputs: a 4-D tensor.\n        :param alpha: controls the width of the network.\n        :param depth_multiplier: depth multiplier for depthwise convolution.\n        :return: .\n        """"""\n        dilation = self.dilation\n\n        x = self._conv_block_v1(inputs, 32, alpha, strides=(2, 2))\n        x = self._depthwise_conv_block_v1(x, 64, alpha, depth_multiplier, block_id=1)\n        c1 = x\n\n        x = self._depthwise_conv_block_v1(x, 128, alpha, depth_multiplier,\n                                          strides=(2, 2), block_id=2)\n        x = self._depthwise_conv_block_v1(x, 128, alpha, depth_multiplier, block_id=3)\n        c2 = x\n\n        x = self._depthwise_conv_block_v1(x, 256, alpha, depth_multiplier,\n                                          strides=(2, 2), block_id=4)\n        x = self._depthwise_conv_block_v1(x, 256, alpha, depth_multiplier, block_id=5)\n        c3 = x\n\n        x = self._depthwise_conv_block_v1(x, 512, alpha, depth_multiplier,\n                                          strides=(2, 2), block_id=6, dilation=dilation[0])\n        x = self._depthwise_conv_block_v1(x, 512, alpha, depth_multiplier, block_id=7, dilation=dilation[0])\n        x = self._depthwise_conv_block_v1(x, 512, alpha, depth_multiplier, block_id=8, dilation=dilation[0])\n        x = self._depthwise_conv_block_v1(x, 512, alpha, depth_multiplier, block_id=9, dilation=dilation[0])\n        x = self._depthwise_conv_block_v1(x, 512, alpha, depth_multiplier, block_id=10, dilation=dilation[0])\n        x = self._depthwise_conv_block_v1(x, 512, alpha, depth_multiplier, block_id=11, dilation=dilation[0])\n        c4 = x\n\n        x = self._depthwise_conv_block_v1(x, 1024, alpha, depth_multiplier,\n                                          strides=(2, 2), block_id=12, dilation=dilation[1])\n        x = self._depthwise_conv_block_v1(x, 1024, alpha, depth_multiplier, block_id=13, dilation=dilation[1])\n        c5 = x\n\n        return c1, c2, c3, c4, c5\n\n    def _mobilenet_v2(self, inputs, alpha=1.0):\n        """"""\n        call for MobileNetV2.\n        :param inputs: a 4-D tensor.\n        :param alpha: controls the width of the network.\n        :return: the output of different stages.\n        """"""\n        dilation = self.dilation\n        channel_axis = 1 if backend.image_data_format() == \'channels_first\' else -1\n\n        first_block_filters = _make_divisible(32 * alpha, 8)\n        x = layers.ZeroPadding2D(padding=_correct_pad(backend, inputs, 3),\n                                 name=\'Conv1_pad\')(inputs)\n        x = layers.Conv2D(first_block_filters,\n                          kernel_size=3,\n                          strides=(2, 2),\n                          padding=\'valid\',\n                          use_bias=False,\n                          name=\'Conv1\')(x)\n        x = layers.BatchNormalization(axis=channel_axis,\n                                      epsilon=1e-3,\n                                      momentum=0.999,\n                                      name=\'bn_Conv1\')(x)\n        x = layers.ReLU(6., name=\'Conv1_relu\')(x)\n\n        x = self._inverted_res_block_v2(x, filters=16, alpha=alpha, stride=1,\n                                        expansion=1, block_id=0)\n        c1 = x\n\n        x = self._inverted_res_block_v2(x, filters=24, alpha=alpha, stride=2,\n                                        expansion=6, block_id=1)\n        x = self._inverted_res_block_v2(x, filters=24, alpha=alpha, stride=1,\n                                        expansion=6, block_id=2)\n        c2 = x\n\n        x = self._inverted_res_block_v2(x, filters=32, alpha=alpha, stride=2,\n                                        expansion=6, block_id=3)\n        x = self._inverted_res_block_v2(x, filters=32, alpha=alpha, stride=1,\n                                        expansion=6, block_id=4)\n        x = self._inverted_res_block_v2(x, filters=32, alpha=alpha, stride=1,\n                                        expansion=6, block_id=5)\n        c3 = x\n\n        x = self._inverted_res_block_v2(x, filters=64, alpha=alpha, stride=2,\n                                        expansion=6, block_id=6, dilation=dilation[0])\n        x = self._inverted_res_block_v2(x, filters=64, alpha=alpha, stride=1,\n                                        expansion=6, block_id=7, dilation=dilation[0])\n        x = self._inverted_res_block_v2(x, filters=64, alpha=alpha, stride=1,\n                                        expansion=6, block_id=8, dilation=dilation[0])\n        x = self._inverted_res_block_v2(x, filters=64, alpha=alpha, stride=1,\n                                        expansion=6, block_id=9, dilation=dilation[0])\n\n        x = self._inverted_res_block_v2(x, filters=96, alpha=alpha, stride=1,\n                                        expansion=6, block_id=10, dilation=dilation[0])\n        x = self._inverted_res_block_v2(x, filters=96, alpha=alpha, stride=1,\n                                        expansion=6, block_id=11, dilation=dilation[0])\n        x = self._inverted_res_block_v2(x, filters=96, alpha=alpha, stride=1,\n                                        expansion=6, block_id=12, dilation=dilation[0])\n        c4 = x\n\n        x = self._inverted_res_block_v2(x, filters=160, alpha=alpha, stride=2,\n                                        expansion=6, block_id=13, dilation=dilation[1])\n        x = self._inverted_res_block_v2(x, filters=160, alpha=alpha, stride=1,\n                                        expansion=6, block_id=14, dilation=dilation[1])\n        x = self._inverted_res_block_v2(x, filters=160, alpha=alpha, stride=1,\n                                        expansion=6, block_id=15, dilation=dilation[1])\n\n        x = self._inverted_res_block_v2(x, filters=320, alpha=alpha, stride=1,\n                                        expansion=6, block_id=16, dilation=dilation[1])\n\n        # no alpha applied to last conv as stated in the paper:\n        # if the width multiplier is greater than 1 we\n        # increase the number of output channels\n        if alpha > 1.0:\n            last_block_filters = _make_divisible(1280 * alpha, 8)\n        else:\n            last_block_filters = 1280\n\n        x = layers.Conv2D(last_block_filters,\n                          kernel_size=1,\n                          use_bias=False,\n                          name=\'Conv_1\')(x)\n        x = layers.BatchNormalization(axis=channel_axis,\n                                      epsilon=1e-3,\n                                      momentum=0.999,\n                                      name=\'Conv_1_bn\')(x)\n        x = layers.ReLU(6., name=\'out_relu\')(x)\n        c5 = x\n\n        return c1, c2, c3, c4, c5\n'"
base_models/resnet.py,2,"b'""""""\nThe implementation of ResNet50/101/152 based on Tensorflow.\nSome codes are based on official tensorflow source codes.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nbackend = tf.keras.backend\n\n\nclass ResNet(object):\n    def __init__(self, version=\'ResNet50\', dilation=None, **kwargs):\n        """"""\n       The implementation of ResNet based on Tensorflow.\n       :param version: \'ResNet50\', \'ResNet101\' or \'ResNet152\'\n       :param dilation: Whether to use dilation strategy\n       :param kwargs: other parameters.\n       """"""\n        super(ResNet, self).__init__(**kwargs)\n        params = {\'ResNet50\': [2, 3, 5, 2],\n                  \'ResNet101\': [2, 3, 22, 2],\n                  \'ResNet152\': [2, 7, 35, 2]}\n        self.version = version\n        assert version in params\n        self.params = params[version]\n\n        if dilation is None:\n            self.dilation = [1, 1]\n        else:\n            self.dilation = dilation\n        assert len(self.dilation) == 2\n\n    def _identity_block(self, input_tensor, kernel_size, filters, stage, block, dilation=1):\n        """"""The identity block is the block that has no conv layer at shortcut.\n\n        # Arguments\n            input_tensor: input tensor\n            kernel_size: default 3, the kernel size of\n                middle conv layer at main path\n            filters: list of integers, the filters of 3 conv layer at main path\n            stage: integer, current stage label, used for generating layer names\n            block: \'a\',\'b\'..., current block label, used for generating layer names\n\n        # Returns\n            Output tensor for the block.\n        """"""\n        filters1, filters2, filters3 = filters\n        if backend.image_data_format() == \'channels_last\':\n            bn_axis = 3\n        else:\n            bn_axis = 1\n\n        if block > \'z\':\n            block = chr(ord(block) - ord(\'z\') + ord(\'A\') - 1)\n\n        conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n        bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n        x = layers.Conv2D(filters1, (1, 1),\n                          kernel_initializer=\'he_normal\',\n                          name=conv_name_base + \'2a\')(input_tensor)\n        x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2a\')(x)\n        x = layers.Activation(\'relu\')(x)\n\n        x = layers.Conv2D(filters2, kernel_size,\n                          padding=\'same\',\n                          kernel_initializer=\'he_normal\',\n                          name=conv_name_base + \'2b\',\n                          dilation_rate=dilation)(x)\n        x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2b\')(x)\n        x = layers.Activation(\'relu\')(x)\n\n        x = layers.Conv2D(filters3, (1, 1),\n                          kernel_initializer=\'he_normal\',\n                          name=conv_name_base + \'2c\')(x)\n        x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2c\')(x)\n\n        x = layers.add([x, input_tensor])\n        x = layers.Activation(\'relu\')(x)\n        return x\n\n    def _conv_block(self,\n                    input_tensor,\n                    kernel_size,\n                    filters,\n                    stage,\n                    block,\n                    strides=(2, 2),\n                    dilation=1):\n        """"""A block that has a conv layer at shortcut.\n\n        # Arguments\n            input_tensor: input tensor\n            kernel_size: default 3, the kernel size of\n                middle conv layer at main path\n            filters: list of integers, the filters of 3 conv layer at main path\n            stage: integer, current stage label, used for generating layer names\n            block: \'a\',\'b\'..., current block label, used for generating layer names\n            strides: Strides for the first conv layer in the block.\n\n        # Returns\n            Output tensor for the block.\n\n        Note that from stage 3,\n        the first conv layer at main path is with strides=(2, 2)\n        And the shortcut should have strides=(2, 2) as well\n        """"""\n        filters1, filters2, filters3 = filters\n        if backend.image_data_format() == \'channels_last\':\n            bn_axis = 3\n        else:\n            bn_axis = 1\n        conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n        bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n        strides = (1, 1) if dilation > 1 else strides\n\n        x = layers.Conv2D(filters1, (1, 1), strides=strides,\n                          kernel_initializer=\'he_normal\',\n                          name=conv_name_base + \'2a\')(input_tensor)\n        x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2a\')(x)\n        x = layers.Activation(\'relu\')(x)\n\n        x = layers.Conv2D(filters2, kernel_size, padding=\'same\',\n                          kernel_initializer=\'he_normal\',\n                          name=conv_name_base + \'2b\',\n                          dilation_rate=dilation)(x)\n        x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2b\')(x)\n        x = layers.Activation(\'relu\')(x)\n\n        x = layers.Conv2D(filters3, (1, 1),\n                          kernel_initializer=\'he_normal\',\n                          name=conv_name_base + \'2c\')(x)\n        x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2c\')(x)\n\n        shortcut = layers.Conv2D(filters3, (1, 1), strides=strides,\n                                 kernel_initializer=\'he_normal\',\n                                 name=conv_name_base + \'1\')(input_tensor)\n        shortcut = layers.BatchNormalization(\n            axis=bn_axis, name=bn_name_base + \'1\')(shortcut)\n\n        x = layers.add([x, shortcut])\n        x = layers.Activation(\'relu\')(x)\n        return x\n\n    def __call__(self, inputs, output_stages=\'c5\', **kwargs):\n        """"""\n        call for ResNet50, ResNet101 or ResNet152.\n        :param inputs: a 4-D tensor.\n        :param output_stages: str or a list of str containing the output stages.\n        :param kwargs: other parameters.\n        :return: the output of different stages.\n        """"""\n        if backend.image_data_format() == \'channels_last\':\n            bn_axis = 3\n        else:\n            bn_axis = 1\n\n        dilation = self.dilation\n\n        x = layers.ZeroPadding2D(padding=(3, 3), name=\'conv1_pad\')(inputs)\n        x = layers.Conv2D(64, (7, 7),\n                          strides=(2, 2),\n                          padding=\'valid\',\n                          kernel_initializer=\'he_normal\',\n                          name=\'conv1\')(x)\n        x = layers.BatchNormalization(axis=bn_axis, name=\'bn_conv1\')(x)\n        x = layers.Activation(\'relu\')(x)\n        x = layers.ZeroPadding2D(padding=(1, 1), name=\'pool1_pad\')(x)\n        x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n        c1 = x\n\n        x = self._conv_block(x, 3, [64, 64, 256], stage=2, block=\'a\', strides=(1, 1))\n        for i in range(self.params[0]):\n            x = self._identity_block(x, 3, [64, 64, 256], stage=2, block=chr(ord(\'b\') + i))\n        c2 = x\n\n        x = self._conv_block(x, 3, [128, 128, 512], stage=3, block=\'a\')\n        for i in range(self.params[1]):\n            x = self._identity_block(x, 3, [128, 128, 512], stage=3, block=chr(ord(\'b\') + i))\n        c3 = x\n\n        x = self._conv_block(x, 3, [256, 256, 1024], stage=4, block=\'a\', dilation=dilation[0])\n        for i in range(self.params[2]):\n            x = self._identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(ord(\'b\') + i), dilation=dilation[0])\n        c4 = x\n\n        x = self._conv_block(x, 3, [512, 512, 2048], stage=5, block=\'a\', dilation=dilation[1])\n        for i in range(self.params[3]):\n            x = self._identity_block(x, 3, [512, 512, 2048], stage=5, block=chr(ord(\'b\') + i), dilation=dilation[1])\n        c5 = x\n\n        self.outputs = {\'c1\': c1,\n                        \'c2\': c2,\n                        \'c3\': c3,\n                        \'c4\': c4,\n                        \'c5\': c5}\n\n        if type(output_stages) is not list:\n            return self.outputs[output_stages]\n        else:\n            return [self.outputs[ci] for ci in output_stages]\n'"
base_models/vgg.py,2,"b'""""""\nThe implementation of VGG16/VGG19 based on Tensorflow.\nSome codes are based on official tensorflow source codes.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nbackend = tf.keras.backend\n\n\nclass VGG(object):\n    def __init__(self, version=\'VGG16\', dilation=None, **kwargs):\n        """"""\n        The implementation of VGG16 and VGG19 based on Tensorflow.\n        :param version: \'VGG16\' or \'VGG19\'\n        :param dilation: Whether to use dilation strategy\n        :param kwargs: other parameters.\n        """"""\n        super(VGG, self).__init__(**kwargs)\n        params = {\'VGG16\': [2, 2, 3, 3, 3],\n                  \'VGG19\': [2, 2, 4, 4, 4]}\n        self.version = version\n        assert version in params\n        self.params = params[version]\n\n        if dilation is None:\n            self.dilation = [1, 1]\n        else:\n            self.dilation = dilation\n        assert len(self.dilation) == 2\n\n    def __call__(self, inputs, output_stages=\'c5\', **kwargs):\n        """"""\n        call for VGG16 or VGG19.\n        :param inputs: a 4-D tensor.\n        :param output_stages: str or a list of str containing the output stages.\n        :param kwargs: other parameters.\n        :return: the output of different stages.\n        """"""\n        dilation = self.dilation\n        _, h, w, _ = backend.int_shape(inputs)\n\n        # Block 1\n        for i in range(self.params[0]):\n            x = layers.Conv2D(64, (3, 3),\n                              activation=\'relu\',\n                              padding=\'same\',\n                              name=\'block1_conv\' + str(i + 1))(inputs)\n        x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=\'block1_pool\')(x)\n        c1 = x\n\n        # Block 2\n        for i in range(self.params[1]):\n            x = layers.Conv2D(128, (3, 3),\n                              activation=\'relu\',\n                              padding=\'same\',\n                              name=\'block2_conv\' + str(i + 1))(x)\n        x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=\'block2_pool\')(x)\n        c2 = x\n\n        # Block 3\n        for i in range(self.params[2]):\n            x = layers.Conv2D(256, (3, 3),\n                              activation=\'relu\',\n                              padding=\'same\',\n                              name=\'block3_conv\' + str(i + 1))(x)\n        x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=\'block3_pool\')(x)\n        c3 = x\n\n        # Block 4\n        for i in range(self.params[3]):\n            x = layers.Conv2D(512, (3, 3),\n                              activation=\'relu\',\n                              padding=\'same\',\n                              name=\'block4_conv\' + str(i + 1),\n                              dilation_rate=dilation[0])(x)\n        if dilation[0] == 1:\n            x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=\'block4_pool\')(x)\n        c4 = x\n\n        # Block 5\n        for i in range(self.params[4]):\n            x = layers.Conv2D(512, (3, 3),\n                              activation=\'relu\',\n                              padding=\'same\',\n                              name=\'block5_conv\' + str(i + 1),\n                              dilation_rate=dilation[1])(x)\n        if dilation[1] == 1:\n            x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=\'block5_pool\')(x)\n        c5 = x\n\n        self.outputs = {\'c1\': c1,\n                        \'c2\': c2,\n                        \'c3\': c3,\n                        \'c4\': c4,\n                        \'c5\': c5}\n\n        if type(output_stages) is not list:\n            return self.outputs[output_stages]\n        else:\n            return [self.outputs[ci] for ci in output_stages]\n'"
base_models/xception.py,2,"b'""""""\nThe implementation of Xception based on Tensorflow.\nSome codes are based on official tensorflow source codes.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nbackend = tf.keras.backend\n\n\nclass Xception(object):\n    def __init__(self, version=\'Xception\', dilation=None, **kwargs):\n        """"""\n        The implementation of Xception and Xception in DeepLabV3Plus based on Tensorflow.\n        :param version: \'Xception\' or \'Xception-DeepLab\'\n        :param dilation: Whether to use dilation strategy\n        :param kwargs: other parameters.\n        """"""\n        super(Xception, self).__init__(**kwargs)\n        self.version = version\n        if dilation is None:\n            self.strides = [2, 2]\n        else:\n            self.strides = [2 if dilation[0] == 1 else 1] + [2 if dilation[1] == 1 else 1]\n        assert len(self.strides) == 2\n        assert version in [\'Xception\', \'Xception-DeepLab\']\n\n    def __call__(self, inputs, output_stages=\'c5\', **kwargs):\n        """"""\n        call for Xception or Xception-DeepLab.\n        :param inputs: a 4-D tensor.\n        :param output_stages: str or a list of str containing the output stages.\n        :param kwargs: other parameters.\n        :return: the output of different stages.\n        """"""\n        strides = self.strides\n        if self.version == \'Xception-DeepLab\':\n            rm_pool = True\n            num_middle_flow = 16\n        else:\n            rm_pool = False\n            num_middle_flow = 8\n\n        channel_axis = 1 if backend.image_data_format() == \'channels_first\' else -1\n\n        x = layers.Conv2D(32, (3, 3),\n                          strides=(2, 2),\n                          use_bias=False,\n                          padding=\'same\',\n                          name=\'block1_conv1\')(inputs)\n        x = layers.BatchNormalization(axis=channel_axis, name=\'block1_conv1_bn\')(x)\n        x = layers.Activation(\'relu\', name=\'block1_conv1_act\')(x)\n        x = layers.Conv2D(64, (3, 3), use_bias=False, padding=\'same\', name=\'block1_conv2\')(x)\n        x = layers.BatchNormalization(axis=channel_axis, name=\'block1_conv2_bn\')(x)\n        x = layers.Activation(\'relu\', name=\'block1_conv2_act\')(x)\n\n        residual = layers.Conv2D(128, (1, 1),\n                                 strides=(2, 2),\n                                 padding=\'same\',\n                                 use_bias=False)(x)\n        residual = layers.BatchNormalization(axis=channel_axis)(residual)\n\n        x = layers.SeparableConv2D(128, (3, 3),\n                                   padding=\'same\',\n                                   use_bias=False,\n                                   name=\'block2_sepconv1\')(x)\n        x = layers.BatchNormalization(axis=channel_axis, name=\'block2_sepconv1_bn\')(x)\n        x = layers.Activation(\'relu\', name=\'block2_sepconv2_act\')(x)\n        x = layers.SeparableConv2D(128, (3, 3),\n                                   padding=\'same\',\n                                   use_bias=False,\n                                   name=\'block2_sepconv2\')(x)\n        x = layers.BatchNormalization(axis=channel_axis, name=\'block2_sepconv2_bn\')(x)\n\n        x = layers.MaxPooling2D((3, 3),\n                                strides=(2, 2),\n                                padding=\'same\',\n                                name=\'block2_pool\')(x)\n        x = layers.add([x, residual])\n        c1 = x\n\n        residual = layers.Conv2D(256, (1, 1), strides=(2, 2),\n                                 padding=\'same\', use_bias=False)(x)\n        residual = layers.BatchNormalization(axis=channel_axis)(residual)\n\n        x = layers.Activation(\'relu\', name=\'block3_sepconv1_act\')(x)\n        x = layers.SeparableConv2D(256, (3, 3),\n                                   padding=\'same\',\n                                   use_bias=False,\n                                   name=\'block3_sepconv1\')(x)\n        x = layers.BatchNormalization(axis=channel_axis, name=\'block3_sepconv1_bn\')(x)\n        x = layers.Activation(\'relu\', name=\'block3_sepconv2_act\')(x)\n        x = layers.SeparableConv2D(256, (3, 3),\n                                   padding=\'same\',\n                                   use_bias=False,\n                                   name=\'block3_sepconv2\')(x)\n        x = layers.BatchNormalization(axis=channel_axis, name=\'block3_sepconv2_bn\')(x)\n\n        if rm_pool:\n            x = layers.Activation(\'relu\', name=\'block3_sepconv3_act\')(x)\n            x = layers.SeparableConv2D(256, (3, 3),\n                                       strides=(2, 2),\n                                       padding=\'same\',\n                                       use_bias=False,\n                                       name=\'block3_sepconv3\')(x)\n            x = layers.BatchNormalization(axis=channel_axis, name=\'block3_sepconv3_bn\')(x)\n        else:\n            x = layers.MaxPooling2D((3, 3), strides=(2, 2),\n                                    padding=\'same\',\n                                    name=\'block3_pool\')(x)\n        x = layers.add([x, residual])\n        c2 = x\n\n        residual = layers.Conv2D(728, (1, 1),\n                                 strides=strides[0],\n                                 padding=\'same\',\n                                 use_bias=False)(x)\n        residual = layers.BatchNormalization(axis=channel_axis)(residual)\n\n        x = layers.Activation(\'relu\', name=\'block4_sepconv1_act\')(x)\n        x = layers.SeparableConv2D(728, (3, 3),\n                                   padding=\'same\',\n                                   use_bias=False,\n                                   name=\'block4_sepconv1\')(x)\n        x = layers.BatchNormalization(axis=channel_axis, name=\'block4_sepconv1_bn\')(x)\n        x = layers.Activation(\'relu\', name=\'block4_sepconv2_act\')(x)\n        x = layers.SeparableConv2D(728, (3, 3),\n                                   padding=\'same\',\n                                   use_bias=False,\n                                   name=\'block4_sepconv2\')(x)\n        x = layers.BatchNormalization(axis=channel_axis, name=\'block4_sepconv2_bn\')(x)\n\n        if rm_pool:\n            x = layers.Activation(\'relu\', name=\'block4_sepconv3_act\')(x)\n            x = layers.SeparableConv2D(728, (3, 3),\n                                       strides=strides[0],\n                                       padding=\'same\',\n                                       use_bias=False,\n                                       name=\'block4_sepconv3\')(x)\n            x = layers.BatchNormalization(axis=channel_axis, name=\'block4_sepconv3_bn\')(x)\n        else:\n            x = layers.MaxPooling2D((3, 3), strides=(2, 2),\n                                    padding=\'same\',\n                                    name=\'block4_pool\')(x)\n        x = layers.add([x, residual])\n        c3 = x\n\n        for i in range(num_middle_flow):\n            residual = x\n            prefix = \'block\' + str(i + 5)\n\n            x = layers.Activation(\'relu\', name=prefix + \'_sepconv1_act\')(x)\n            x = layers.SeparableConv2D(728, (3, 3),\n                                       padding=\'same\',\n                                       use_bias=False,\n                                       name=prefix + \'_sepconv1\')(x)\n            x = layers.BatchNormalization(axis=channel_axis,\n                                          name=prefix + \'_sepconv1_bn\')(x)\n            x = layers.Activation(\'relu\', name=prefix + \'_sepconv2_act\')(x)\n            x = layers.SeparableConv2D(728, (3, 3),\n                                       padding=\'same\',\n                                       use_bias=False,\n                                       name=prefix + \'_sepconv2\')(x)\n            x = layers.BatchNormalization(axis=channel_axis,\n                                          name=prefix + \'_sepconv2_bn\')(x)\n            x = layers.Activation(\'relu\', name=prefix + \'_sepconv3_act\')(x)\n            x = layers.SeparableConv2D(728, (3, 3),\n                                       padding=\'same\',\n                                       use_bias=False,\n                                       name=prefix + \'_sepconv3\')(x)\n            x = layers.BatchNormalization(axis=channel_axis,\n                                          name=prefix + \'_sepconv3_bn\')(x)\n\n            x = layers.add([x, residual])\n        c4 = x\n\n        residual = layers.Conv2D(1024, (1, 1), strides=strides[1],\n                                 padding=\'same\', use_bias=False)(x)\n        residual = layers.BatchNormalization(axis=channel_axis)(residual)\n\n        id = 5 + num_middle_flow\n        x = layers.Activation(\'relu\', name=\'block{id}_sepconv1_act\'.format(id=id))(x)\n        x = layers.SeparableConv2D(728, (3, 3),\n                                   padding=\'same\',\n                                   use_bias=False,\n                                   name=\'block{id}_sepconv1\'.format(id=id))(x)\n        x = layers.BatchNormalization(axis=channel_axis, name=\'block{id}_sepconv1_bn\'.format(id=id))(x)\n        x = layers.Activation(\'relu\', name=\'block{id}_sepconv2_act\'.format(id=id))(x)\n        x = layers.SeparableConv2D(1024, (3, 3),\n                                   padding=\'same\',\n                                   use_bias=False,\n                                   name=\'block{id}_sepconv2\'.format(id=id))(x)\n        x = layers.BatchNormalization(axis=channel_axis, name=\'block{id}_sepconv2_bn\'.format(id=id))(x)\n\n        if rm_pool:\n            x = layers.Activation(\'relu\', name=\'block{id}_sepconv3_act\'.format(id=id))(x)\n            x = layers.SeparableConv2D(1024, (3, 3),\n                                       strides=strides[1],\n                                       padding=\'same\',\n                                       use_bias=False,\n                                       name=\'block{id}_sepconv3\'.format(id=id))(x)\n            x = layers.BatchNormalization(axis=channel_axis, name=\'block{id}_sepconv3_bn\'.format(id=id))(x)\n        else:\n            x = layers.MaxPooling2D((3, 3),\n                                    strides=(2, 2),\n                                    padding=\'same\',\n                                    name=\'block{id}_pool\'.format(id=id))(x)\n        x = layers.add([x, residual])\n\n        x = layers.SeparableConv2D(1536, (3, 3),\n                                   padding=\'same\',\n                                   use_bias=False,\n                                   name=\'block{id}_sepconv1\'.format(id=id + 1))(x)\n        x = layers.BatchNormalization(axis=channel_axis, name=\'block{id}_sepconv1_bn\'.format(id=id + 1))(x)\n        x = layers.Activation(\'relu\', name=\'block{id}_sepconv1_act\'.format(id=id + 1))(x)\n\n        if self.version == \'Xception-DeepLab\':\n            x = layers.SeparableConv2D(1536, (3, 3),\n                                       padding=\'same\',\n                                       use_bias=False,\n                                       name=\'block{id}_sepconv1_1\'.format(id=id + 1))(x)\n            x = layers.BatchNormalization(axis=channel_axis, name=\'block{id}_sepconv1_1_bn\'.format(id=id + 1))(x)\n            x = layers.Activation(\'relu\', name=\'block{id}_sepconv1_1_act\'.format(id=id + 1))(x)\n\n        x = layers.SeparableConv2D(2048, (3, 3),\n                                   padding=\'same\',\n                                   use_bias=False,\n                                   name=\'block{id}_sepconv2\'.format(id=id + 1))(x)\n        x = layers.BatchNormalization(axis=channel_axis, name=\'block{id}_sepconv2_bn\'.format(id=id + 1))(x)\n        x = layers.Activation(\'relu\', name=\'block{id}_sepconv2_act\'.format(id=id + 1))(x)\n\n        c5 = x\n\n        self.outputs = {\'c1\': c1,\n                        \'c2\': c2,\n                        \'c3\': c3,\n                        \'c4\': c4,\n                        \'c5\': c5}\n\n        if type(output_stages) is not list:\n            return self.outputs[output_stages]\n        else:\n            return [self.outputs[ci] for ci in output_stages]\n'"
builders/__init__.py,0,b'from builders.model_builder import builder'
builders/model_builder.py,1,"b'""""""\nThe model builder to build different semantic segmentation models.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom models import *\nimport tensorflow as tf\n\nlayers = tf.keras.layers\n\n\ndef builder(num_classes, input_size=(256, 256), model=\'SegNet\', base_model=None):\n    models = {\'FCN-8s\': FCN,\n              \'FCN-16s\': FCN,\n              \'FCN-32s\': FCN,\n              \'UNet\': UNet,\n              \'SegNet\': SegNet,\n              \'Bayesian-SegNet\': SegNet,\n              \'PAN\': PAN,\n              \'PSPNet\': PSPNet,\n              \'RefineNet\': RefineNet,\n              \'DenseASPP\': DenseASPP,\n              \'DeepLabV3\': DeepLabV3,\n              \'DeepLabV3Plus\': DeepLabV3Plus,\n              \'BiSegNet\': BiSegNet}\n\n    assert model in models\n\n    net = models[model](num_classes, model, base_model)\n\n    inputs = layers.Input(shape=input_size+(3,))\n\n    return net(inputs), net.get_base_model()\n'"
models/__init__.py,0,b'from models.network import Network\nfrom models.fcn import FCN\nfrom models.pspnet import PSPNet\nfrom models.segnet import SegNet\nfrom models.unet import UNet\nfrom models.pan import PAN\nfrom models.deeplab_v3 import DeepLabV3\nfrom models.deeplab_v3_plus import DeepLabV3Plus\nfrom models.refinenet import RefineNet\nfrom models.denseaspp import DenseASPP\nfrom models.bisegnet import BiSegNet\n'
models/bisegnet.py,3,"b'""""""\nThe implementation of BiSegNet based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom utils import layers as custom_layers\nfrom models import Network\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nmodels = tf.keras.models\nbackend = tf.keras.backend\n\n\nclass BiSegNet(Network):\n    def __init__(self, num_classes, version=\'BiSegNet\', base_model=\'Xception\', **kwargs):\n        """"""\n        The initialization of BiSegNet.\n        :param num_classes: the number of predicted classes.\n        :param version: \'BiSegNet\'\n        :param base_model: the backbone model\n        :param kwargs: other parameters\n        """"""\n        base_model = \'Xception\' if base_model is None else base_model\n\n        assert version == \'BiSegNet\'\n        super(BiSegNet, self).__init__(num_classes, version, base_model, **kwargs)\n\n    def __call__(self, inputs=None, input_size=None, **kwargs):\n        assert inputs is not None or input_size is not None\n\n        if inputs is None:\n            assert isinstance(input_size, tuple)\n            inputs = layers.Input(shape=input_size+(3,))\n        return self._bisegnet(inputs)\n\n    def _conv_block(self, x, filters, kernel_size=3, strides=1):\n        x = layers.Conv2D(filters, kernel_size, strides, padding=\'same\', kernel_initializer=\'he_normal\')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n        return x\n\n    def _attention_refinement_module(self, x):\n        # Global average pooling\n        _, _, _, c = backend.int_shape(x)\n\n        glb = custom_layers.GlobalAveragePooling2D(keep_dims=True)(x)\n        glb = layers.Conv2D(c, 1, strides=1, kernel_initializer=\'he_normal\')(glb)\n        glb = layers.BatchNormalization()(glb)\n        glb = layers.Activation(activation=\'sigmoid\')(glb)\n\n        x = layers.Multiply()([x, glb])\n\n        return x\n\n    def _feature_fusion_module(self, input_1, input_2, filters):\n        inputs = layers.Concatenate()([input_1, input_2])\n        inputs = self._conv_block(inputs, filters=filters, kernel_size=3)\n\n        # Global average pooling\n        _, _, _, c = backend.int_shape(inputs)\n\n        glb = custom_layers.GlobalAveragePooling2D(keep_dims=True)(inputs)\n        glb = layers.Conv2D(filters, 1, strides=1, activation=\'relu\', kernel_initializer=\'he_normal\')(glb)\n        glb = layers.Conv2D(filters, 1, strides=1, activation=\'sigmoid\', kernel_initializer=\'he_normal\')(glb)\n\n        x = layers.Multiply()([inputs, glb])\n\n        return x\n\n    def _bisegnet(self, inputs):\n        num_classes = self.num_classes\n\n        # the spatial path\n        sx = self._conv_block(inputs, 64, 3, 2)\n        sx = self._conv_block(sx, 128, 3, 2)\n        sx = self._conv_block(sx, 256, 3, 2)\n\n        # the context path\n        if self.base_model in [\'VGG16\',\n                               \'VGG19\',\n                               \'ResNet50\',\n                               \'ResNet101\',\n                               \'ResNet152\',\n                               \'MobileNetV1\',\n                               \'MobileNetV2\',\n                               \'Xception\',\n                               \'Xception-DeepLab\']:\n            c4, c5 = self.encoder(inputs, output_stages=[\'c4\', \'c5\'])\n        else:\n            c4, c5 = self.encoder(inputs, output_stages=[\'c3\', \'c5\'])\n\n        c4 = self._attention_refinement_module(c4)\n        c5 = self._attention_refinement_module(c5)\n\n        glb = custom_layers.GlobalAveragePooling2D(keep_dims=True)(c5)\n        c5 = layers.Multiply()([c5, glb])\n\n        # combining the paths\n        c4 = layers.UpSampling2D(size=(2, 2), interpolation=\'bilinear\')(c4)\n        c5 = layers.UpSampling2D(size=(4, 4), interpolation=\'bilinear\')(c5)\n\n        cx = layers.Concatenate()([c4, c5])\n\n        x = self._feature_fusion_module(sx, cx, num_classes)\n\n        x = layers.UpSampling2D(size=(8, 8), interpolation=\'bilinear\')(x)\n        x = layers.Conv2D(num_classes, 1, 1, kernel_initializer=\'he_normal\')(x)\n\n        outputs = x\n\n        return models.Model(inputs, outputs, name=self.version)\n'"
models/deeplab_v3.py,3,"b'""""""\nThe implementation of DeepLabV3 based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom utils import layers as custom_layers\nfrom models import Network\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nmodels = tf.keras.models\nbackend = tf.keras.backend\n\n\nclass DeepLabV3(Network):\n    def __init__(self, num_classes, version=\'DeepLabV3\', base_model=\'ResNet50\', **kwargs):\n        """"""\n        The initialization of DeepLabV3.\n        :param num_classes: the number of predicted classes.\n        :param version: \'DeepLabV3\'\n        :param base_model: the backbone model\n        :param kwargs: other parameters\n        """"""\n        dilation = [1, 2]\n        base_model = \'ResNet50\' if base_model is None else base_model\n\n        assert version == \'DeepLabV3\'\n        assert base_model in [\'VGG16\',\n                              \'VGG19\',\n                              \'ResNet50\',\n                              \'ResNet101\',\n                              \'ResNet152\',\n                              \'DenseNet121\',\n                              \'DenseNet169\',\n                              \'DenseNet201\',\n                              \'DenseNet264\',\n                              \'MobileNetV1\',\n                              \'MobileNetV2\',\n                              \'Xception-DeepLab\']\n        super(DeepLabV3, self).__init__(num_classes, version, base_model, dilation, **kwargs)\n        self.dilation = dilation\n\n    def __call__(self, inputs=None, input_size=None, **kwargs):\n        assert inputs is not None or input_size is not None\n\n        if inputs is None:\n            assert isinstance(input_size, tuple)\n            inputs = layers.Input(shape=input_size + (3,))\n        return self._deeplabv3(inputs)\n\n    def _deeplabv3(self, inputs):\n        multi_grid = [1, 2, 4]\n        num_classes = self.num_classes\n        dilation = self.dilation\n\n        _, h, w, _ = backend.int_shape(inputs)\n        self.aspp_size = (h // 16, w // 16)\n\n        x = self.encoder(inputs, output_stages=\'c4\')\n\n        x = self._conv_block(x, 3, [512, 512, 2048], stage=5, block=\'a\', dilation=dilation[1])\n        for i in range(2):\n            x = self._identity_block(x, 3, [512, 512, 2048],\n                                     stage=5,\n                                     block=chr(ord(\'b\') + i),\n                                     dilation=dilation[1] * multi_grid[i])\n        x = self._aspp(x, 256)\n        x = layers.Conv2D(num_classes, 1, strides=1, kernel_initializer=\'he_normal\')(x)\n        x = layers.UpSampling2D(size=(16, 16), interpolation=\'bilinear\')(x)\n\n        outputs = x\n        return models.Model(inputs, outputs, name=self.version)\n\n    def _aspp(self, x, out_filters):\n        xs = list()\n        x1 = layers.Conv2D(out_filters, 1, strides=1, kernel_initializer=\'he_normal\')(x)\n        xs.append(x1)\n\n        for i in range(3):\n            xi = layers.Conv2D(out_filters, 3,\n                               strides=1,\n                               padding=\'same\',\n                               dilation_rate=6 * (i + 1))(x)\n            xs.append(xi)\n        img_pool = custom_layers.GlobalAveragePooling2D(keep_dims=True)(x)\n        img_pool = layers.Conv2D(out_filters, 1, 1, kernel_initializer=\'he_normal\')(img_pool)\n        img_pool = layers.UpSampling2D(size=self.aspp_size, interpolation=\'bilinear\')(img_pool)\n        xs.append(img_pool)\n\n        x = custom_layers.Concatenate(out_size=self.aspp_size)(xs)\n        x = layers.Conv2D(out_filters, 1, strides=1, kernel_initializer=\'he_normal\')(x)\n        x = layers.BatchNormalization()(x)\n\n        return x\n\n    def _identity_block(self, input_tensor, kernel_size, filters, stage, block, dilation=1):\n        """"""The identity block is the block that has no conv layer at shortcut.\n\n        # Arguments\n            input_tensor: input tensor\n            kernel_size: default 3, the kernel size of\n                middle conv layer at main path\n            filters: list of integers, the filters of 3 conv layer at main path\n            stage: integer, current stage label, used for generating layer names\n            block: \'a\',\'b\'..., current block label, used for generating layer names\n\n        # Returns\n            Output tensor for the block.\n        """"""\n        filters1, filters2, filters3 = filters\n        if backend.image_data_format() == \'channels_last\':\n            bn_axis = 3\n        else:\n            bn_axis = 1\n        conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n        bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n        x = layers.Conv2D(filters1, (1, 1),\n                          kernel_initializer=\'he_normal\',\n                          name=conv_name_base + \'2a\')(input_tensor)\n        x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2a\')(x)\n        x = layers.Activation(\'relu\')(x)\n\n        x = layers.Conv2D(filters2, kernel_size,\n                          padding=\'same\',\n                          kernel_initializer=\'he_normal\',\n                          name=conv_name_base + \'2b\',\n                          dilation_rate=dilation)(x)\n        x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2b\')(x)\n        x = layers.Activation(\'relu\')(x)\n\n        x = layers.Conv2D(filters3, (1, 1),\n                          kernel_initializer=\'he_normal\',\n                          name=conv_name_base + \'2c\')(x)\n        x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2c\')(x)\n\n        x = layers.add([x, input_tensor])\n        x = layers.Activation(\'relu\')(x)\n        return x\n\n    def _conv_block(self,\n                    input_tensor,\n                    kernel_size,\n                    filters,\n                    stage,\n                    block,\n                    strides=(2, 2),\n                    dilation=1):\n        """"""A block that has a conv layer at shortcut.\n\n        # Arguments\n            input_tensor: input tensor\n            kernel_size: default 3, the kernel size of\n                middle conv layer at main path\n            filters: list of integers, the filters of 3 conv layer at main path\n            stage: integer, current stage label, used for generating layer names\n            block: \'a\',\'b\'..., current block label, used for generating layer names\n            strides: Strides for the first conv layer in the block.\n\n        # Returns\n            Output tensor for the block.\n\n        Note that from stage 3,\n        the first conv layer at main path is with strides=(2, 2)\n        And the shortcut should have strides=(2, 2) as well\n        """"""\n        filters1, filters2, filters3 = filters\n        if backend.image_data_format() == \'channels_last\':\n            bn_axis = 3\n        else:\n            bn_axis = 1\n        conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n        bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n        strides = (1, 1) if dilation > 1 else strides\n\n        x = layers.Conv2D(filters1, (1, 1),\n                          strides=strides,\n                          name=conv_name_base + \'2a\',\n                          kernel_initializer=\'he_normal\')(input_tensor)\n        x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2a\')(x)\n        x = layers.Activation(\'relu\')(x)\n\n        x = layers.Conv2D(filters2, kernel_size,\n                          padding=\'same\',\n                          name=conv_name_base + \'2b\',\n                          kernel_initializer=\'he_normal\',\n                          dilation_rate=dilation)(x)\n        x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2b\')(x)\n        x = layers.Activation(\'relu\')(x)\n\n        x = layers.Conv2D(filters3, (1, 1),\n                          name=conv_name_base + \'2c\',\n                          kernel_initializer=\'he_normal\')(x)\n        x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2c\')(x)\n\n        shortcut = layers.Conv2D(filters3, (1, 1),\n                                 strides=strides,\n                                 name=conv_name_base + \'1\',\n                                 kernel_initializer=\'he_normal\')(input_tensor)\n        shortcut = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'1\')(shortcut)\n\n        x = layers.add([x, shortcut])\n        x = layers.Activation(\'relu\')(x)\n        return x\n'"
models/deeplab_v3_plus.py,3,"b'""""""\nThe implementation of DeepLabV3Plus based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom utils import layers as custom_layers\nfrom models import Network\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nmodels = tf.keras.models\nbackend = tf.keras.backend\n\n\nclass DeepLabV3Plus(Network):\n    def __init__(self, num_classes, version=\'DeepLabV3Plus\', base_model=\'Xception-DeepLab\', **kwargs):\n        """"""\n        The initialization of DeepLabV3Plus.\n        :param num_classes: the number of predicted classes.\n        :param version: \'DeepLabV3Plus\'\n        :param base_model: the backbone model\n        :param kwargs: other parameters\n        """"""\n        dilation = [1, 2]\n        base_model = \'Xception-DeepLab\' if base_model is None else base_model\n\n        assert version == \'DeepLabV3Plus\'\n        assert base_model in [\'VGG16\',\n                              \'VGG19\',\n                              \'ResNet50\',\n                              \'ResNet101\',\n                              \'ResNet152\',\n                              \'DenseNet121\',\n                              \'DenseNet169\',\n                              \'DenseNet201\',\n                              \'DenseNet264\',\n                              \'MobileNetV1\',\n                              \'MobileNetV2\',\n                              \'Xception-DeepLab\']\n        super(DeepLabV3Plus, self).__init__(num_classes, version, base_model, dilation, **kwargs)\n        self.dilation = dilation\n\n    def __call__(self, inputs=None, input_size=None, **kwargs):\n        assert inputs is not None or input_size is not None\n\n        if inputs is None:\n            assert isinstance(input_size, tuple)\n            inputs = layers.Input(shape=input_size + (3,))\n        return self._deeplab_v3_plus(inputs)\n\n    def _deeplab_v3_plus(self, inputs):\n        num_classes = self.num_classes\n        _, h, w, _ = backend.int_shape(inputs)\n        self.aspp_size = (h // 16, w // 16)\n\n        if self.base_model in [\'VGG16\',\n                               \'VGG19\',\n                               \'ResNet50\',\n                               \'ResNet101\',\n                               \'ResNet152\',\n                               \'MobileNetV1\',\n                               \'MobileNetV2\']:\n            c2, c5 = self.encoder(inputs, output_stages=[\'c2\', \'c5\'])\n        else:\n            c2, c5 = self.encoder(inputs, output_stages=[\'c1\', \'c5\'])\n\n        x = self._aspp(c5, 256)\n        x = layers.Dropout(rate=0.5)(x)\n\n        x = layers.UpSampling2D(size=(4, 4), interpolation=\'bilinear\')(x)\n        x = self._conv_bn_relu(x, 48, 1, strides=1)\n\n        x = custom_layers.Concatenate(out_size=self.aspp_size)([x, c2])\n        x = self._conv_bn_relu(x, 256, 3, 1)\n        x = layers.Dropout(rate=0.5)(x)\n\n        x = self._conv_bn_relu(x, 256, 3, 1)\n        x = layers.Dropout(rate=0.1)(x)\n\n        x = layers.Conv2D(num_classes, 1, strides=1)(x)\n        x = layers.UpSampling2D(size=(4, 4), interpolation=\'bilinear\')(x)\n\n        outputs = x\n        return models.Model(inputs, outputs, name=self.version)\n\n    def _conv_bn_relu(self, x, filters, kernel_size, strides=1):\n        x = layers.Conv2D(filters, kernel_size, strides=strides, padding=\'same\')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n        return x\n\n    def _aspp(self, x, out_filters):\n        xs = list()\n        x1 = layers.Conv2D(out_filters, 1, strides=1)(x)\n        xs.append(x1)\n\n        for i in range(3):\n            xi = layers.Conv2D(out_filters, 3, strides=1, padding=\'same\', dilation_rate=6 * (i + 1))(x)\n            xs.append(xi)\n        img_pool = custom_layers.GlobalAveragePooling2D(keep_dims=True)(x)\n        img_pool = layers.Conv2D(out_filters, 1, 1, kernel_initializer=\'he_normal\')(img_pool)\n        img_pool = layers.UpSampling2D(size=self.aspp_size, interpolation=\'bilinear\')(img_pool)\n        xs.append(img_pool)\n\n        x = custom_layers.Concatenate(out_size=self.aspp_size)(xs)\n        x = layers.Conv2D(out_filters, 1, strides=1, kernel_initializer=\'he_normal\')(x)\n        x = layers.BatchNormalization()(x)\n\n        return x\n'"
models/denseaspp.py,3,"b'""""""\nThe implementation of DenseASPP based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom utils import layers as custom_layers\nfrom models import Network\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nmodels = tf.keras.models\nbackend = tf.keras.backend\n\n\nclass DenseASPP(Network):\n    def __init__(self, num_classes, version=\'DenseASPP\', base_model=\'DenseNet121\', **kwargs):\n        """"""\n        The initialization of DenseASPP based.\n        :param num_classes: the number of predicted classes.\n        :param version: \'DenseASPP\'\n        :param base_model: the backbone model\n        :param kwargs: other parameters\n        """"""\n        dilation = [2, 4]\n        base_model = \'DenseNet121\' if base_model is None else base_model\n\n        assert version == \'DenseASPP\'\n        assert base_model in [\'VGG16\',\n                              \'VGG19\',\n                              \'ResNet50\',\n                              \'ResNet101\',\n                              \'ResNet152\',\n                              \'DenseNet121\',\n                              \'DenseNet169\',\n                              \'DenseNet201\',\n                              \'DenseNet264\',\n                              \'MobileNetV1\',\n                              \'MobileNetV2\',\n                              \'Xception-DeepLab\']\n        super(DenseASPP, self).__init__(num_classes, version, base_model, dilation, **kwargs)\n\n    def __call__(self, inputs=None, input_size=None, **kwargs):\n        assert inputs is not None or input_size is not None\n\n        if inputs is None:\n            assert isinstance(input_size, tuple)\n            inputs = layers.Input(shape=input_size + (3,))\n        return self._denseaspp(inputs)\n\n    def _dilated_conv_block(self, inputs, filters, kernel_size=3, rate=1):\n        x = layers.BatchNormalization()(inputs)\n        x = layers.ReLU()(x)\n        x = layers.Conv2D(filters, kernel_size,\n                          padding=\'same\',\n                          dilation_rate=rate,\n                          kernel_initializer=\'he_normal\')(x)\n        return x\n\n    def _denseaspp(self, inputs):\n        _, inputs_h, inputs_w, _ = backend.int_shape(inputs)\n        aspp_size = inputs_h // 8, inputs_w // 8\n        num_classes = self.num_classes\n\n        c5 = self.encoder(inputs, output_stages=\'c5\')\n\n        # First block rate=3\n        d3 = self._dilated_conv_block(c5, 256, 1)\n        d3 = self._dilated_conv_block(d3, 64, 3, rate=3)\n\n        # Second block rate=6\n        d4 = custom_layers.Concatenate(out_size=aspp_size)([c5, d3])\n        d4 = self._dilated_conv_block(d4, 256, 1)\n        d4 = self._dilated_conv_block(d4, 64, 3, rate=6)\n\n        # Third block rate=12\n        d5 = custom_layers.Concatenate(out_size=aspp_size)([c5, d3, d4])\n        d5 = self._dilated_conv_block(d5, 256, 1)\n        d5 = self._dilated_conv_block(d5, 64, 3, rate=12)\n\n        # Forth block rate=18\n        d6 = custom_layers.Concatenate(out_size=aspp_size)([c5, d3, d4, d5])\n        d6 = self._dilated_conv_block(d6, 256, 1)\n        d6 = self._dilated_conv_block(d6, 64, 3, rate=18)\n\n        # Fifth block rate=24\n        d7 = custom_layers.Concatenate(out_size=aspp_size)([c5, d3, d4, d5, d6])\n        d7 = self._dilated_conv_block(d7, 256, 1)\n        d7 = self._dilated_conv_block(d7, 64, 3, rate=24)\n\n        x = custom_layers.Concatenate(out_size=aspp_size)([c5, d3, d4, d5, d6, d7])\n        x = layers.Conv2D(num_classes, 1, strides=1, kernel_initializer=\'he_normal\')(x)\n        x = layers.UpSampling2D(size=(8, 8), interpolation=\'bilinear\')(x)\n\n        outputs = x\n        return models.Model(inputs, outputs, name=self.version)\n'"
models/fcn.py,3,"b'""""""\nThe implementation of FCN-8s/16s/32s based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom models import Network\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nmodels = tf.keras.models\nbackend = tf.keras.backend\n\n\nclass FCN(Network):\n    def __init__(self, num_classes, version=\'FCN-8s\', base_model=\'VGG16\', **kwargs):\n        """"""\n        The initialization of FCN-8s/16s/32s.\n        :param num_classes: the number of predicted classes.\n        :param version: \'FCN-8s\', \'FCN-16s\' or \'FCN-32s\'.\n        :param base_model: the backbone model\n        :param kwargs: other parameters\n        """"""\n        fcn = {\'FCN-8s\': self._fcn_8s,\n               \'FCN-16s\': self._fcn_16s,\n               \'FCN-32s\': self._fcn_32s}\n        base_model = \'VGG16\' if base_model is None else base_model\n\n        assert version in fcn\n        self.fcn = fcn[version]\n        super(FCN, self).__init__(num_classes, version, base_model, **kwargs)\n\n    def __call__(self, inputs=None, input_size=None, **kwargs):\n        assert inputs is not None or input_size is not None\n\n        if inputs is None:\n            assert isinstance(input_size, tuple)\n            inputs = layers.Input(shape=input_size + (3,))\n        return self.fcn(inputs)\n\n    def _conv_relu(self, x, filters, kernel_size=1):\n        x = layers.Conv2D(filters, kernel_size, padding=\'same\', kernel_initializer=\'he_normal\')(x)\n        x = layers.ReLU()(x)\n        return x\n\n    def _fcn_32s(self, inputs):\n        num_classes = self.num_classes\n\n        x = self.encoder(inputs)\n        x = self._conv_relu(x, 4096, 7)\n        x = layers.Dropout(rate=0.5)(x)\n        x = self._conv_relu(x, 4096, 1)\n        x = layers.Dropout(rate=0.5)(x)\n\n        x = layers.Conv2D(num_classes, 1, kernel_initializer=\'he_normal\')(x)\n        x = layers.Conv2DTranspose(num_classes, 64, strides=32, padding=\'same\', kernel_initializer=\'he_normal\')(x)\n\n        outputs = x\n        return models.Model(inputs, outputs, name=self.version)\n\n    def _fcn_16s(self, inputs):\n        num_classes = self.num_classes\n\n        if self.base_model in [\'DenseNet121\',\n                               \'DenseNet169\',\n                               \'DenseNet201\',\n                               \'DenseNet264\',\n                               \'Xception\',\n                               \'Xception-DeepLab\']:\n            c4, c5 = self.encoder(inputs, output_stages=[\'c3\', \'c5\'])\n        else:\n            c4, c5 = self.encoder(inputs, output_stages=[\'c4\', \'c5\'])\n\n        x = self._conv_relu(c5, 4096, 7)\n        x = layers.Dropout(rate=0.5)(x)\n        x = self._conv_relu(x, 4096, 1)\n        x = layers.Dropout(rate=0.5)(x)\n\n        x = layers.Conv2D(num_classes, 1, kernel_initializer=\'he_normal\')(x)\n        x = layers.Conv2DTranspose(num_classes, 4,\n                                   strides=2,\n                                   padding=\'same\',\n                                   kernel_initializer=\'he_normal\')(x)\n        c4 = layers.Conv2D(num_classes, 1, kernel_initializer=\'he_normal\')(c4)\n        x = layers.Add()([x, c4])\n\n        x = layers.Conv2DTranspose(num_classes, 32,\n                                   strides=16,\n                                   padding=\'same\',\n                                   kernel_initializer=\'he_normal\')(x)\n\n        outputs = x\n        return models.Model(inputs, outputs, name=self.version)\n\n    def _fcn_8s(self, inputs):\n        num_classes = self.num_classes\n\n        if self.base_model in [\'VGG16\',\n                               \'VGG19\',\n                               \'ResNet50\',\n                               \'ResNet101\',\n                               \'ResNet152\',\n                               \'MobileNetV1\',\n                               \'MobileNetV2\']:\n            c3, c4, c5 = self.encoder(inputs, output_stages=[\'c3\', \'c4\', \'c5\'])\n        else:\n            c3, c4, c5 = self.encoder(inputs, output_stages=[\'c2\', \'c3\', \'c5\'])\n\n        x = self._conv_relu(c5, 4096, 7)\n        x = layers.Dropout(rate=0.5)(x)\n        x = self._conv_relu(x, 4096, 1)\n        x = layers.Dropout(rate=0.5)(x)\n\n        x = layers.Conv2D(num_classes, 1, kernel_initializer=\'he_normal\')(x)\n        x = layers.Conv2DTranspose(num_classes, 4,\n                                   strides=2,\n                                   padding=\'same\',\n                                   kernel_initializer=\'he_normal\')(x)\n        c4 = layers.Conv2D(num_classes, 1)(c4)\n        x = layers.Add()([x, c4])\n\n        x = layers.Conv2DTranspose(num_classes, 4,\n                                   strides=2,\n                                   padding=\'same\',\n                                   kernel_initializer=\'he_normal\')(x)\n        c3 = layers.Conv2D(num_classes, 1)(c3)\n        x = layers.Add()([x, c3])\n\n        x = layers.Conv2DTranspose(num_classes, 16,\n                                   strides=8,\n                                   padding=\'same\',\n                                   kernel_initializer=\'he_normal\')(x)\n\n        outputs = x\n        return models.Model(inputs, outputs, name=self.version)\n'"
models/network.py,0,"b'""""""\nThe implementation of Network based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom base_models import *\n\n\nclass Network(object):\n    def __init__(self, num_classes, version=\'PAN\', base_model=\'ResNet50\', dilation=None, **kwargs):\n        super(Network, self).__init__(**kwargs)\n        if base_model in [\'VGG16\', \'VGG19\']:\n            self.encoder = VGG(base_model, dilation=dilation)\n        elif base_model in [\'ResNet50\', \'ResNet101\', \'ResNet152\']:\n            self.encoder = ResNet(base_model, dilation=dilation)\n        elif base_model in [\'DenseNet121\', \'DenseNet169\', \'DenseNet201\', \'DenseNet264\']:\n            self.encoder = DenseNet(base_model, dilation=dilation)\n        elif base_model in [\'Xception\', \'Xception-DeepLab\']:\n            self.encoder = Xception(base_model, dilation=dilation)\n        elif base_model in [\'MobileNetV1\', \'MobileNetV2\']:\n            self.encoder = MobileNet(base_model, dilation=dilation)\n        else:\n            raise ValueError(\'The base model {model} is not in the \'\n                             \'supported model list!!!\'.format(model=base_model))\n\n        self.num_classes = num_classes\n        self.version = version\n        self.base_model = base_model\n\n    def __call__(self, inputs, **kwargs):\n        return inputs\n\n    def get_version(self):\n        return self.version\n\n    def get_base_model(self):\n        return self.base_model\n'"
models/pan.py,3,"b'""""""\nThe implementation of PAN (Pyramid Attention Networks) based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom utils import layers as custom_layers\nfrom models import Network\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nmodels = tf.keras.models\nbackend = tf.keras.backend\n\n\nclass PAN(Network):\n    def __init__(self, num_classes, version=\'PAN\', base_model=\'ResNet50\', **kwargs):\n        """"""\n        The initialization of PAN.\n        :param num_classes: the number of predicted classes.\n        :param version: \'PAN\'\n        :param base_model: the backbone model\n        :param kwargs: other parameters\n        """"""\n        base_model = \'ResNet50\' if base_model is None else base_model\n        assert version == \'PAN\'\n\n        dilation = [2, 4]\n        if base_model in [\'VGG16\',\n                          \'VGG19\',\n                          \'MobileNetV1\',\n                          \'MobileNetV2\',\n                          \'ResNet50\',\n                          \'ResNet101\',\n                          \'ResNet152\']:\n            self.up_size = [(1, 1), (1, 1), (2, 2), (4, 4)]\n        elif base_model in [\'DenseNet121\',\n                            \'DenseNet169\',\n                            \'DenseNet201\',\n                            \'DenseNet264\',\n                            \'Xception-DeepLab\']:\n            self.up_size = [(1, 1), (1, 1), (1, 1), (8, 8)]\n        else:\n            raise ValueError(\'The base model \\\'{model}\\\' is not \'\n                             \'supported in PAN.\'.format(model=base_model))\n\n        super(PAN, self).__init__(num_classes, version, base_model, dilation, **kwargs)\n\n    def __call__(self, inputs=None, input_size=None, **kwargs):\n        assert inputs is not None or input_size is not None\n\n        if inputs is None:\n            assert isinstance(input_size, tuple)\n            inputs = layers.Input(shape=input_size + (3,))\n        return self._pan(inputs)\n\n    def _conv_bn_relu(self, x, filters, kernel_size, strides=1):\n        x = layers.Conv2D(filters, kernel_size, strides, padding=\'same\', kernel_initializer=\'he_normal\')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n        return x\n\n    def _fpa(self, x, out_filters):\n        _, h, w, _ = backend.int_shape(x)\n\n        # global average pooling\n        glb = custom_layers.GlobalAveragePooling2D(keep_dims=True)(x)\n        glb = layers.Conv2D(out_filters, 1, strides=1, kernel_initializer=\'he_normal\')(glb)\n\n        # down\n        down1 = layers.AveragePooling2D(pool_size=(2, 2))(x)\n        down1 = self._conv_bn_relu(down1, 1, 7, 1)\n\n        down2 = layers.AveragePooling2D(pool_size=(2, 2))(down1)\n        down2 = self._conv_bn_relu(down2, 1, 5, 1)\n\n        down3 = layers.AveragePooling2D(pool_size=(2, 2))(down2)\n        down3 = self._conv_bn_relu(down3, 1, 3, 1)\n\n        down1 = self._conv_bn_relu(down1, 1, 7, 1)\n        down2 = self._conv_bn_relu(down2, 1, 5, 1)\n        down3 = self._conv_bn_relu(down3, 1, 3, 1)\n\n        # up\n        up2 = layers.UpSampling2D(size=(2, 2))(down3)\n        up2 = layers.Add()([up2, down2])\n\n        up1 = layers.UpSampling2D(size=(2, 2))(up2)\n        up1 = layers.Add()([up1, down1])\n\n        up = layers.UpSampling2D(size=(2, 2))(up1)\n\n        x = layers.Conv2D(out_filters, 1, strides=1, kernel_initializer=\'he_normal\')(x)\n        x = layers.BatchNormalization()(x)\n\n        # multiply\n        x = layers.Multiply()([x, up])\n\n        # add\n        x = layers.Add()([x, glb])\n\n        return x\n\n    def _gau(self, x, y, out_filters, up_size=(2, 2)):\n        glb = custom_layers.GlobalAveragePooling2D(keep_dims=True)(y)\n        glb = layers.Conv2D(out_filters, 1, strides=1, activation=\'sigmoid\', kernel_initializer=\'he_normal\')(glb)\n\n        x = self._conv_bn_relu(x, out_filters, 3, 1)\n        x = layers.Multiply()([x, glb])\n\n        y = layers.UpSampling2D(size=up_size, interpolation=\'bilinear\')(y)\n\n        y = layers.Add()([x, y])\n\n        return y\n\n    def _pan(self, inputs):\n        num_classes = self.num_classes\n        up_size = self.up_size\n\n        c2, c3, c4, c5 = self.encoder(inputs, output_stages=[\'c2\', \'c3\', \'c4\', \'c5\'])\n\n        y = self._fpa(c5, num_classes)\n\n        y = self._gau(c4, y, num_classes, up_size[0])\n        y = self._gau(c3, y, num_classes, up_size[1])\n        y = self._gau(c2, y, num_classes, up_size[2])\n\n        y = layers.UpSampling2D(size=up_size[3], interpolation=\'bilinear\')(y)\n\n        outputs = y\n\n        return models.Model(inputs, outputs, name=self.version)\n'"
models/pspnet.py,3,"b'""""""\nThe implementation of PSPNet based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom utils import layers as custom_layers\nfrom models import Network\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nmodels = tf.keras.models\nbackend = tf.keras.backend\n\n\nclass PSPNet(Network):\n    def __init__(self, num_classes, version=\'PSPNet\', base_model=\'ResNet50\', **kwargs):\n        """"""\n        The initialization of PSPNet.\n        :param num_classes: the number of predicted classes.\n        :param version: \'PSPNet\'\n        :param base_model: the backbone model\n        :param kwargs: other parameters\n        """"""\n        dilation = [2, 4]\n        base_model = \'ResNet50\' if base_model is None else base_model\n\n        assert version == \'PSPNet\'\n        assert base_model in [\'VGG16\',\n                              \'VGG19\',\n                              \'ResNet50\',\n                              \'ResNet101\',\n                              \'ResNet152\',\n                              \'DenseNet121\',\n                              \'DenseNet169\',\n                              \'DenseNet201\',\n                              \'DenseNet264\',\n                              \'MobileNetV1\',\n                              \'MobileNetV2\',\n                              \'Xception-DeepLab\']\n        super(PSPNet, self).__init__(num_classes, version, base_model, dilation, **kwargs)\n\n    def __call__(self, inputs=None, input_size=None, **kwargs):\n        assert inputs is not None or input_size is not None\n\n        if inputs is None:\n            assert isinstance(input_size, tuple)\n            inputs = layers.Input(shape=input_size + (3,))\n        return self._pspnet(inputs)\n\n    def _pspnet(self, inputs):\n        num_classes = self.num_classes\n        _, inputs_h, inputs_w, _ = backend.int_shape(inputs)\n\n        h, w = inputs_h // 8, inputs_w // 8\n        x = self.encoder(inputs)\n\n        if not (h % 6 == 0 and w % 6 == 0):\n            raise ValueError(\'\\\'pyramid pooling\\\' size must be divided by 6, but received {size}\'.format(size=(h, w)))\n        pool_size = [(h, w),\n                     (h // 2, w // 2),\n                     (h // 3, w // 3),\n                     (h // 6, w // 6)]\n\n        # pyramid pooling\n        x1 = custom_layers.GlobalAveragePooling2D(keep_dims=True)(x)\n        x1 = layers.Conv2D(512, 1, strides=1, kernel_initializer=\'he_normal\')(x1)\n        x1 = layers.BatchNormalization()(x1)\n        x1 = layers.ReLU()(x1)\n        x1 = layers.UpSampling2D(size=pool_size[0])(x1)\n\n        x2 = layers.AveragePooling2D(pool_size=pool_size[1])(x)\n        x2 = layers.Conv2D(512, 1, strides=1, kernel_initializer=\'he_normal\')(x2)\n        x2 = layers.BatchNormalization()(x2)\n        x2 = layers.ReLU()(x2)\n        x2 = layers.UpSampling2D(size=pool_size[1])(x2)\n\n        x3 = layers.AveragePooling2D(pool_size=pool_size[2])(x)\n        x3 = layers.Conv2D(512, 1, strides=1, kernel_initializer=\'he_normal\')(x3)\n        x3 = layers.BatchNormalization()(x3)\n        x3 = layers.ReLU()(x3)\n        x3 = layers.UpSampling2D(size=pool_size[2])(x3)\n\n        x6 = layers.AveragePooling2D(pool_size=pool_size[3])(x)\n        x6 = layers.Conv2D(512, 1, strides=1, kernel_initializer=\'he_normal\')(x6)\n        x6 = layers.BatchNormalization()(x6)\n        x6 = layers.ReLU()(x6)\n        x6 = layers.UpSampling2D(size=pool_size[3])(x6)\n\n        x = layers.Concatenate()([x, x1, x2, x3, x6])\n\n        x = layers.Conv2D(512, 3, strides=1, padding=\'same\', kernel_initializer=\'he_normal\')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n\n        x = layers.Conv2D(num_classes, 1, strides=1, kernel_initializer=\'he_normal\')(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.UpSampling2D(size=(8, 8), interpolation=\'bilinear\')(x)\n\n        outputs = x\n\n        return models.Model(inputs, outputs, name=self.version)\n'"
models/refinenet.py,3,"b'""""""\nThe implementation of RefineNet based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom models import Network\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nmodels = tf.keras.models\nbackend = tf.keras.backend\n\n\nclass RefineNet(Network):\n    def __init__(self, num_classes, version=\'RefineNet\', base_model=\'ResNet50\', **kwargs):\n        """"""\n        The initialization of RefineNet.\n        :param num_classes: the number of predicted classes.\n        :param version: \'RefineNet\'\n        :param base_model: the backbone model\n        :param kwargs: other parameters\n        """"""\n        base_model = \'ResNet50\' if base_model is None else base_model\n\n        assert version == \'RefineNet\'\n        assert base_model in [\'VGG16\',\n                              \'VGG19\',\n                              \'ResNet50\',\n                              \'ResNet101\',\n                              \'ResNet152\',\n                              \'MobileNetV1\',\n                              \'MobileNetV2\']\n        super(RefineNet, self).__init__(num_classes, version, base_model, **kwargs)\n\n    def __call__(self, inputs=None, input_size=None, **kwargs):\n        assert inputs is not None or input_size is not None\n\n        if inputs is None:\n            assert isinstance(input_size, tuple)\n            inputs = layers.Input(shape=input_size + (3,))\n        return self._refinenet(inputs)\n\n    def _refinenet(self, inputs):\n        num_classes = self.num_classes\n\n        xs = self.encoder(inputs, output_stages=[\'c2\', \'c3\', \'c4\', \'c5\'])[::-1]\n\n        g = [None, None, None, None]\n        h = [None, None, None, None]\n\n        for i in range(4):\n            h[i] = layers.Conv2D(256, 1, strides=1, kernel_initializer=\'he_normal\')(xs[i])\n\n        g[0] = self._refine_block(high_inputs=None, low_inputs=h[0])\n        g[1] = self._refine_block(g[0], h[1])\n        g[2] = self._refine_block(g[1], h[2])\n        g[3] = self._refine_block(g[2], h[3])\n\n        x = layers.Conv2D(num_classes, 1, strides=1, kernel_initializer=\'he_normal\')(g[3])\n        x = layers.UpSampling2D(size=(4, 4), interpolation=\'bilinear\')(x)\n\n        outputs = x\n\n        return models.Model(inputs, outputs, name=self.version)\n\n    def _residual_conv_unit(self, inputs, features=256, kernel_size=3):\n        x = layers.ReLU()(inputs)\n        x = layers.Conv2D(features, kernel_size, padding=\'same\', kernel_initializer=\'he_normal\')(x)\n        x = layers.ReLU()(x)\n        x = layers.Conv2D(features, kernel_size, padding=\'same\', kernel_initializer=\'he_normal\')(x)\n        x = layers.Add()([inputs, x])\n        return x\n\n    def _chained_residual_pooling(self, inputs, features=256):\n        x_relu = layers.ReLU()(inputs)\n        x = layers.MaxPool2D((5, 5), strides=1, padding=\'same\')(x_relu)\n        x = layers.Conv2D(features, 3, padding=\'same\', kernel_initializer=\'he_normal\')(x)\n        x_sum_1 = layers.Add()([x, x_relu])\n\n        x = layers.MaxPool2D((5, 5), strides=1, padding=\'same\')(x_relu)\n        x = layers.Conv2D(features, 3, padding=\'same\', kernel_initializer=\'he_normal\')(x)\n        x_sum_2 = layers.Add()([x, x_sum_1])\n\n        return x_sum_2\n\n    def _multi_resolution_fusion(self, high_inputs=None, low_inputs=None, features=256):\n\n        if high_inputs is None:  # refineNet block 4\n            rcu_low_1 = low_inputs[0]\n            rcu_low_2 = low_inputs[1]\n\n            rcu_low_1 = layers.Conv2D(features, 3, padding=\'same\', kernel_initializer=\'he_normal\')(rcu_low_1)\n            rcu_low_2 = layers.Conv2D(features, 3, padding=\'same\', kernel_initializer=\'he_normal\')(rcu_low_2)\n\n            return layers.Add()([rcu_low_1, rcu_low_2])\n\n        else:\n            rcu_low_1 = low_inputs[0]\n            rcu_low_2 = low_inputs[1]\n\n            rcu_low_1 = layers.Conv2D(features, 3, padding=\'same\', kernel_initializer=\'he_normal\')(rcu_low_1)\n            rcu_low_2 = layers.Conv2D(features, 3, padding=\'same\', kernel_initializer=\'he_normal\')(rcu_low_2)\n\n            rcu_low = layers.Add()([rcu_low_1, rcu_low_2])\n\n            rcu_high_1 = high_inputs[0]\n            rcu_high_2 = high_inputs[1]\n\n            rcu_high_1 = layers.UpSampling2D(size=(2, 2), interpolation=\'bilinear\')(\n                layers.Conv2D(features, 3, padding=\'same\', kernel_initializer=\'he_normal\')(rcu_high_1))\n            rcu_high_2 = layers.UpSampling2D(size=(2, 2), interpolation=\'bilinear\')(\n                layers.Conv2D(features, 3, padding=\'same\', kernel_initializer=\'he_normal\')(rcu_high_2))\n\n            rcu_high = layers.Add()([rcu_high_1, rcu_high_2])\n\n            return layers.Add()([rcu_low, rcu_high])\n\n    def _refine_block(self, high_inputs=None, low_inputs=None):\n\n        if high_inputs is None:  # block 4\n            rcu_low_1 = self._residual_conv_unit(low_inputs, features=256)\n            rcu_low_2 = self._residual_conv_unit(low_inputs, features=256)\n            rcu_low = [rcu_low_1, rcu_low_2]\n\n            fuse = self._multi_resolution_fusion(high_inputs=None, low_inputs=rcu_low, features=256)\n            fuse_pooling = self._chained_residual_pooling(fuse, features=256)\n            output = self._residual_conv_unit(fuse_pooling, features=256)\n            return output\n        else:\n            rcu_low_1 = self._residual_conv_unit(low_inputs, features=256)\n            rcu_low_2 = self._residual_conv_unit(low_inputs, features=256)\n            rcu_low = [rcu_low_1, rcu_low_2]\n\n            rcu_high_1 = self._residual_conv_unit(high_inputs, features=256)\n            rcu_high_2 = self._residual_conv_unit(high_inputs, features=256)\n            rcu_high = [rcu_high_1, rcu_high_2]\n\n            fuse = self._multi_resolution_fusion(rcu_high, rcu_low, features=256)\n            fuse_pooling = self._chained_residual_pooling(fuse, features=256)\n            output = self._residual_conv_unit(fuse_pooling, features=256)\n            return output\n'"
models/segnet.py,3,"b'""""""\nThe implementation of SegNet and Bayesian-SegNet based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom models import Network\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nmodels = tf.keras.models\nbackend = tf.keras.backend\n\n\nclass SegNet(Network):\n    def __init__(self, num_classes, version=\'SegNet\', base_model=\'VGG16\', **kwargs):\n        """"""\n        The initialization of SegNet or Bayesian-SegNet.\n        :param num_classes: the number of predicted classes.\n        :param version: \'SegNet\' or \'Bayesian-SegNet\'.\n        :param base_model: the backbone model\n        :param kwargs: other parameters\n        """"""\n        base_model = \'VGG16\' if base_model is None else base_model\n        assert version in [\'SegNet\', \'Bayesian-SegNet\']\n        assert base_model in [\'VGG16\',\n                              \'VGG19\',\n                              \'ResNet50\',\n                              \'ResNet101\',\n                              \'ResNet152\',\n                              \'DenseNet121\',\n                              \'DenseNet169\',\n                              \'DenseNet201\',\n                              \'DenseNet269\',\n                              \'MobileNetV1\',\n                              \'MobileNetV2\',\n                              \'Xception\',\n                              \'Xception-DeepLab\']\n        super(SegNet, self).__init__(num_classes, version, base_model, **kwargs)\n\n    def __call__(self, inputs=None, input_size=None, **kwargs):\n        assert inputs is not None or input_size is not None\n\n        if inputs is None:\n            assert isinstance(input_size, tuple)\n            inputs = layers.Input(shape=input_size + (3,))\n        return self._segnet(inputs)\n\n    def _conv_bn_relu(self, x, filters, kernel_size=1, strides=1):\n        x = layers.Conv2D(filters, kernel_size,\n                          strides=strides,\n                          padding=\'same\',\n                          kernel_initializer=\'he_normal\')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n        return x\n\n    def _segnet(self, inputs):\n        num_classes = self.num_classes\n        dropout = True if self.version == \'Bayesian-SegNet\' else False\n\n        x = self.encoder(inputs)\n\n        if dropout:\n            x = layers.Dropout(rate=0.5)(x)\n        x = layers.UpSampling2D(size=(2, 2))(x)\n        x = self._conv_bn_relu(x, 512, 3, strides=1)\n        x = self._conv_bn_relu(x, 512, 3, strides=1)\n        x = self._conv_bn_relu(x, 512, 3, strides=1)\n\n        if dropout:\n            x = layers.Dropout(rate=0.5)(x)\n        x = layers.UpSampling2D(size=(2, 2))(x)\n        x = self._conv_bn_relu(x, 512, 3, strides=1)\n        x = self._conv_bn_relu(x, 512, 3, strides=1)\n        x = self._conv_bn_relu(x, 256, 3, strides=1)\n\n        if dropout:\n            x = layers.Dropout(rate=0.5)(x)\n        x = layers.UpSampling2D(size=(2, 2))(x)\n        x = self._conv_bn_relu(x, 256, 3, strides=1)\n        x = self._conv_bn_relu(x, 256, 3, strides=1)\n        x = self._conv_bn_relu(x, 128, 3, strides=1)\n\n        if dropout:\n            x = layers.Dropout(rate=0.5)(x)\n        x = layers.UpSampling2D(size=(2, 2))(x)\n        x = self._conv_bn_relu(x, 128, 3, strides=1)\n        x = self._conv_bn_relu(x, 64, 3, strides=1)\n\n        if dropout:\n            x = layers.Dropout(rate=0.5)(x)\n        x = layers.UpSampling2D(size=(2, 2))(x)\n        x = self._conv_bn_relu(x, 64, 3, strides=1)\n        x = layers.Conv2D(num_classes, 1,\n                          strides=1,\n                          kernel_initializer=\'he_normal\')(x)\n        x = layers.BatchNormalization()(x)\n\n        outputs = x\n        return models.Model(inputs, outputs, name=self.version)\n'"
models/unet.py,3,"b'""""""\nThe implementation of UNet based on Tensorflow.\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom models import Network\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nmodels = tf.keras.models\nbackend = tf.keras.backend\n\n\nclass UNet(Network):\n    def __init__(self, num_classes, version=\'UNet\', base_model=\'VGG16\', **kwargs):\n        """"""\n        The initialization of UNet.\n        :param num_classes: the number of predicted classes.\n        :param version: \'UNet\'\n        :param base_model: the backbone model\n        :param kwargs: other parameters\n        """"""\n        base_model = \'VGG16\' if base_model is None else base_model\n\n        assert version == \'UNet\'\n        assert base_model in [\'VGG16\',\n                              \'VGG19\',\n                              \'MobileNetV1\',\n                              \'MobileNetV2\']\n        super(UNet, self).__init__(num_classes, version, base_model, **kwargs)\n\n    def __call__(self, inputs=None, input_size=None, **kwargs):\n        assert inputs is not None or input_size is not None\n\n        if inputs is None:\n            assert isinstance(input_size, tuple)\n            inputs = layers.Input(shape=input_size + (3,))\n        return self._unet(inputs)\n\n    def _conv_bn_relu(self, x, filters, kernel_size=1, strides=1):\n        x = layers.Conv2D(filters, kernel_size,\n                          strides=strides,\n                          padding=\'same\',\n                          kernel_initializer=\'he_normal\')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n        return x\n\n    def _unet(self, inputs):\n        num_classes = self.num_classes\n\n        c1, c2, c3, c4, c5 = self.encoder(inputs, output_stages=[\'c1\', \'c2\', \'c3\', \'c4\', \'c5\'])\n\n        x = layers.Dropout(0.5)(c5)\n\n        x = layers.UpSampling2D(size=(2, 2))(x)\n        x = self._conv_bn_relu(x, 512, 2, strides=1)\n        x = layers.Concatenate()([x, c4])\n        x = self._conv_bn_relu(x, 512, 3, strides=1)\n        x = self._conv_bn_relu(x, 512, 3, strides=1)\n\n        x = layers.UpSampling2D(size=(2, 2))(x)\n        x = self._conv_bn_relu(x, 256, 2, strides=1)\n        x = layers.Concatenate()([x, c3])\n        x = self._conv_bn_relu(x, 256, 3, strides=1)\n        x = self._conv_bn_relu(x, 256, 3, strides=1)\n\n        x = layers.UpSampling2D(size=(2, 2))(x)\n        x = self._conv_bn_relu(x, 128, 2, strides=1)\n        x = layers.Concatenate()([x, c2])\n        x = self._conv_bn_relu(x, 128, 3, strides=1)\n        x = self._conv_bn_relu(x, 128, 3, strides=1)\n\n        x = layers.UpSampling2D(size=(2, 2))(x)\n        x = self._conv_bn_relu(x, 64, 2, strides=1)\n        x = layers.Concatenate()([x, c1])\n        x = self._conv_bn_relu(x, 64, 3, strides=1)\n        x = self._conv_bn_relu(x, 64, 3, strides=1)\n\n        x = layers.UpSampling2D(size=(2, 2))(x)\n        x = layers.Conv2D(num_classes, 1, strides=1,\n                          kernel_initializer=\'he_normal\')(x)\n        x = layers.BatchNormalization()(x)\n\n        outputs = x\n        return models.Model(inputs, outputs, name=self.version)\n'"
utils/__init__.py,0,b'from utils import *\n'
utils/callbacks.py,2,"b'""""""\nThe implementation of some callbacks based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nimport tensorflow as tf\nimport numpy as np\n\ncallbacks = tf.keras.callbacks\nbackend = tf.keras.backend\n\n\nclass LearningRateScheduler(callbacks.Callback):\n    def __init__(self,\n                 schedule,\n                 learning_rate=None,\n                 warmup=False,\n                 steps_per_epoch=None,\n                 verbose=0):\n        super(LearningRateScheduler, self).__init__()\n        self.learning_rate = learning_rate\n        self.schedule = schedule\n        self.verbose = verbose\n        self.warmup_epochs = 5 if warmup else 0\n        self.warmup_steps = int(steps_per_epoch) * self.warmup_epochs if warmup else 0\n        self.global_batch = 0\n\n        if warmup and learning_rate is None:\n            raise ValueError(\'learning_rate cannot be None if warmup is used.\')\n        if warmup and steps_per_epoch is None:\n            raise ValueError(\'steps_per_epoch cannot be None if warmup is used.\')\n\n    def on_train_batch_begin(self, batch, logs=None):\n        self.global_batch += 1\n        if self.global_batch < self.warmup_steps:\n            if not hasattr(self.model.optimizer, \'lr\'):\n                raise ValueError(\'Optimizer must have a ""lr"" attribute.\')\n            lr = self.learning_rate * self.global_batch / self.warmup_steps\n            backend.set_value(self.model.optimizer.lr, lr)\n            if self.verbose > 0:\n                print(\'\\nBatch %05d: LearningRateScheduler warming up learning \'\n                      \'rate to %s.\' % (self.global_batch, lr))\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if not hasattr(self.model.optimizer, \'lr\'):\n            raise ValueError(\'Optimizer must have a ""lr"" attribute.\')\n        lr = float(backend.get_value(self.model.optimizer.lr))\n\n        if epoch >= self.warmup_epochs:\n            try:  # new API\n                lr = self.schedule(epoch - self.warmup_epochs, lr)\n            except TypeError:  # Support for old API for backward compatibility\n                lr = self.schedule(epoch - self.warmup_epochs)\n            if not isinstance(lr, (float, np.float32, np.float64)):\n                raise ValueError(\'The output of the ""schedule"" function \'\n                                 \'should be float.\')\n            backend.set_value(self.model.optimizer.lr, lr)\n\n            if self.verbose > 0:\n                print(\'\\nEpoch %05d: LearningRateScheduler reducing learning \'\n                      \'rate to %s.\' % (epoch + 1, lr))\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs[\'lr\'] = backend.get_value(self.model.optimizer.lr)\n'"
utils/data_generator.py,1,"b'""""""\nThe implementation of Data Generator based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom tensorflow.python.keras.preprocessing.image import Iterator\nfrom keras_applications import imagenet_utils\nfrom utils.utils import *\nimport tensorflow as tf\nimport numpy as np\n\nkeras_utils = tf.keras.utils\n\n\nclass DataIterator(Iterator):\n    def __init__(self,\n                 image_data_generator,\n                 images_list,\n                 labels_list,\n                 num_classes,\n                 batch_size,\n                 target_size,\n                 shuffle=True,\n                 seed=None,\n                 data_aug_rate=0.):\n        num_images = len(images_list)\n\n        self.image_data_generator = image_data_generator\n        self.images_list = images_list\n        self.labels_list = labels_list\n        self.num_classes = num_classes\n        self.target_size = target_size\n        self.data_aug_rate = data_aug_rate\n\n        super(DataIterator, self).__init__(num_images, batch_size, shuffle, seed)\n\n    def _get_batches_of_transformed_samples(self, index_array):\n        batch_x = np.zeros(shape=(len(index_array),) + self.target_size + (3,))\n        batch_y = np.zeros(shape=(len(index_array),) + self.target_size + (self.num_classes,))\n\n        for i, idx in enumerate(index_array):\n            image, label = load_image(self.images_list[idx]), load_image(self.labels_list[idx])\n            # random crop\n            if self.image_data_generator.random_crop:\n                image, label = random_crop(image, label, self.target_size)\n            else:\n                image, label = resize_image(image, label, self.target_size)\n            # data augmentation\n            if np.random.uniform(0., 1.) < self.data_aug_rate:\n                # random vertical flip\n                if np.random.randint(2):\n                    image, label = random_vertical_flip(image, label, self.image_data_generator.vertical_flip)\n                # random horizontal flip\n                if np.random.randint(2):\n                    image, label = random_horizontal_flip(image, label, self.image_data_generator.horizontal_flip)\n                # random brightness\n                if np.random.randint(2):\n                    image, label = random_brightness(image, label, self.image_data_generator.brightness_range)\n                # random rotation\n                if np.random.randint(2):\n                    image, label = random_rotation(image, label, self.image_data_generator.rotation_range)\n                # random channel shift\n                if np.random.randint(2):\n                    image, label = random_channel_shift(image, label, self.image_data_generator.channel_shift_range)\n                # random zoom\n                if np.random.randint(2):\n                    image, label = random_zoom(image, label, self.image_data_generator.zoom_range)\n\n            image = imagenet_utils.preprocess_input(image.astype(\'float32\'), data_format=\'channels_last\',\n                                                    mode=\'torch\')\n            label = one_hot(label, self.num_classes)\n\n            batch_x[i], batch_y[i] = image, label\n\n        return batch_x, batch_y\n\n\nclass ImageDataGenerator(object):\n    def __init__(self,\n                 random_crop=False,\n                 rotation_range=0,\n                 brightness_range=None,\n                 zoom_range=0.0,\n                 channel_shift_range=0.0,\n                 horizontal_flip=False,\n                 vertical_flip=False):\n        self.random_crop = random_crop\n        self.rotation_range = rotation_range\n        self.brightness_range = brightness_range\n        self.zoom_range = zoom_range\n        self.channel_shift_range = channel_shift_range\n        self.horizontal_flip = horizontal_flip\n        self.vertical_flip = vertical_flip\n\n    def flow(self,\n             images_list,\n             labels_list,\n             num_classes,\n             batch_size,\n             target_size,\n             shuffle=True,\n             seed=None,\n             data_aug_rate=0.):\n        return DataIterator(image_data_generator=self,\n                            images_list=images_list,\n                            labels_list=labels_list,\n                            num_classes=num_classes,\n                            batch_size=batch_size,\n                            target_size=target_size,\n                            shuffle=shuffle,\n                            seed=seed,\n                            data_aug_rate=data_aug_rate)\n'"
utils/helpers.py,0,"b'""""""\nThe implementation of some helpers.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nimport numpy as np\nimport warnings\nimport csv\nimport os\n\n\ndef get_dataset_info(dataset_path):\n    image_label_paths = check_dataset_path(dataset_path)\n    image_label_names = list()\n\n    for i, path in enumerate(image_label_paths):\n        names = list()\n        if path is not None:\n            files = sorted(os.listdir(path))\n            for file in files:\n                names.append(os.path.join(path, file))\n        image_label_names.append(names)\n\n    assert len(image_label_names[0]) == len(image_label_names[1])\n    assert len(image_label_names[2]) == len(image_label_names[3])\n\n    return image_label_names\n\n\ndef check_dataset_path(dataset_path):\n    primary_directory = [\'train\', \'valid\', \'test\']\n    secondary_directory = [\'images\', \'labels\']\n\n    if not os.path.exists(dataset_path):\n        raise ValueError(\'The path of the dataset does not exist.\')\n    else:\n        train_path = os.path.join(dataset_path, primary_directory[0])\n        valid_path = os.path.join(dataset_path, primary_directory[1])\n        test_path = os.path.join(dataset_path, primary_directory[2])\n        if not os.path.exists(train_path):\n            raise ValueError(\'The path of the training data does not exist.\')\n        if not os.path.exists(valid_path):\n            raise ValueError(\'The path of the validation data does not exist.\')\n        if not os.path.exists(test_path):\n            warnings.warn(\'The path of the test data does not exist. \')\n\n        train_image_path = os.path.join(train_path, secondary_directory[0])\n        train_label_path = os.path.join(train_path, secondary_directory[1])\n        if not os.path.exists(train_image_path) or not os.path.exists(train_label_path):\n            raise ValueError(\'The path of images or labels for training does not exist.\')\n\n        valid_image_path = os.path.join(valid_path, secondary_directory[0])\n        valid_label_path = os.path.join(valid_path, secondary_directory[1])\n        if not os.path.exists(valid_image_path) or not os.path.exists(valid_label_path):\n            raise ValueError(\'The path of images or labels for validation does not exist.\')\n\n        test_image_path = os.path.join(test_path, secondary_directory[0])\n        test_label_path = os.path.join(test_path, secondary_directory[1])\n        if not os.path.exists(test_image_path) or not os.path.exists(test_label_path):\n            warnings.warn(\'The path of images or labels for test does not exist.\')\n            test_image_path = None\n            test_label_path = None\n\n        return train_image_path, train_label_path, valid_image_path, valid_label_path, test_image_path, test_label_path\n\n\ndef check_related_path(current_path):\n    assert os.path.exists(current_path)\n\n    checkpoints_path = os.path.join(current_path, \'checkpoints\')\n    logs_path = os.path.join(checkpoints_path, \'logs\')\n    weights_path = os.path.join(current_path, \'weights\')\n    prediction_path = os.path.join(current_path, \'predictions\')\n\n    if not os.path.exists(checkpoints_path):\n        os.mkdir(checkpoints_path)\n    if not os.path.exists(logs_path):\n        os.mkdir(logs_path)\n    if not os.path.exists(weights_path):\n        os.mkdir(weights_path)\n    if not os.path.exists(prediction_path):\n        os.mkdir(prediction_path)\n\n    paths = {\'checkpoints_path\': checkpoints_path,\n             \'logs_path\': logs_path,\n             \'weights_path\': weights_path,\n             \'prediction_path\': prediction_path}\n    return paths\n\n\ndef get_colored_info(csv_path):\n    if not os.path.exists(csv_path):\n        raise ValueError(\'The path \\\'{path:}\\\' of csv file does not exist!\'.format(path=csv_path))\n\n    filename, file_extension = os.path.splitext(csv_path)\n    if not file_extension == \'.csv\':\n        raise ValueError(\'File is not a CSV!\')\n\n    class_names = []\n    label_values = []\n    with open(csv_path, \'r\') as csv_file:\n        file_reader = csv.reader(csv_file, delimiter=\',\')\n        header = next(file_reader)\n        for row in file_reader:\n            class_names.append(row[0])\n            label_values.append([int(row[1]), int(row[2]), int(row[3])])\n    return class_names, label_values\n\n\ndef get_evaluated_classes(file_path):\n    if not os.path.exists(file_path):\n        raise ValueError(\'The path of evaluated classes file does not exist!\')\n\n    with open(file_path, \'r\') as file:\n        evaluated_classes = list(map(lambda z: z.strip(), file.readlines()))\n\n    return evaluated_classes\n\n\ndef color_encode(image, color_values):\n    color_codes = np.array(color_values)\n    x = color_codes[image.astype(int)]\n\n    return x\n'"
utils/layers.py,7,"b'""""""\nThe implementation of some layers based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nimport tensorflow as tf\n\nlayers = tf.keras.layers\nbackend = tf.keras.backend\n\n\nclass GlobalAveragePooling2D(layers.GlobalAveragePooling2D):\n    def __init__(self, keep_dims=False, **kwargs):\n        super(GlobalAveragePooling2D, self).__init__(**kwargs)\n        self.keep_dims = keep_dims\n\n    def call(self, inputs):\n        if self.keep_dims is False:\n            return super(GlobalAveragePooling2D, self).call(inputs)\n        else:\n            return backend.mean(inputs, axis=[1, 2], keepdims=True)\n\n    def compute_output_shape(self, input_shape):\n        if self.keep_dims is False:\n            return super(GlobalAveragePooling2D, self).compute_output_shape(input_shape)\n        else:\n            input_shape = tf.TensorShape(input_shape).as_list()\n            return tf.TensorShape([input_shape[0], 1, 1, input_shape[3]])\n\n    def get_config(self):\n        config = super(GlobalAveragePooling2D, self).get_config()\n        config[\'keep_dim\'] = self.keep_dims\n        return config\n\n\nclass Concatenate(layers.Concatenate):\n    def __init__(self, out_size=None, axis=-1, name=None):\n        super(Concatenate, self).__init__(axis=axis, name=name)\n        self.out_size = out_size\n\n    def call(self, inputs):\n        return backend.concatenate(inputs, self.axis)\n\n    def build(self, input_shape):\n        pass\n\n    def compute_output_shape(self, input_shape):\n        if self.out_size is None:\n            return super(Concatenate, self).compute_output_shape(input_shape)\n        else:\n            if not isinstance(input_shape, list):\n                raise ValueError(\'A `Concatenate` layer should be called \'\n                                 \'on a list of inputs.\')\n            input_shapes = input_shape\n            output_shape = list(input_shapes[0])\n            for shape in input_shapes[1:]:\n                if output_shape[self.axis] is None or shape[self.axis] is None:\n                    output_shape[self.axis] = None\n                    break\n                output_shape[self.axis] += shape[self.axis]\n            return tuple([output_shape[0]] + list(self.out_size) + [output_shape[-1]])\n\n    def get_config(self):\n        config = super(Concatenate, self).get_config()\n        config[\'out_size\'] = self.out_size\n        return config\n\n\nclass PixelShuffle(layers.Layer):\n    def __init__(self, block_size=2, **kwargs):\n        super(PixelShuffle, self).__init__(**kwargs)\n        if isinstance(block_size, int):\n            self.block_size = block_size\n        elif isinstance(block_size, (list, tuple)):\n            assert len(block_size) == 2 and block_size[0] == block_size[1]\n            self.block_size = block_size[0]\n        else:\n            raise ValueError(\'error \\\'block_size\\\'.\')\n\n    def build(self, input_shape):\n        pass\n\n    def call(self, inputs, **kwargs):\n        return tf.nn.depth_to_space(inputs, self.block_size)\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape).as_list()\n\n        _, h, w, c = input_shape\n\n        new_h = h * self.block_size\n        new_w = w * self.block_size\n        new_c = c / self.block_size ** 2\n\n        return tf.TensorShape([input_shape[0], new_h, new_w, new_c])\n\n    def get_config(self):\n        config = super(PixelShuffle, self).get_config()\n        config[\'block_size\'] = self.block_size\n        return config\n'"
utils/learning_rate.py,0,"b'""""""\nThe implementation of learning rate scheduler.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nimport numpy as np\n\n\ndef step_decay(lr=3e-4, max_epochs=100, warmup=False):\n    """"""\n    step decay.\n    :param lr: initial lr\n    :param max_epochs: max epochs\n    :param warmup: warm up or not\n    :return: current lr\n    """"""\n    drop = 0.1\n    max_epochs = max_epochs - 5 if warmup else max_epochs\n\n    def decay(epoch):\n        lrate = lr * np.power(drop, np.floor((1 + epoch) / max_epochs))\n        return lrate\n\n    return decay\n\n\ndef poly_decay(lr=3e-4, max_epochs=100, warmup=False):\n    """"""\n    poly decay.\n    :param lr: initial lr\n    :param max_epochs: max epochs\n    :param warmup: warm up or not\n    :return: current lr\n    """"""\n    max_epochs = max_epochs - 5 if warmup else max_epochs\n\n    def decay(epoch):\n        lrate = lr * (1 - np.power(epoch / max_epochs, 0.9))\n        return lrate\n\n    return decay\n\n\ndef cosine_decay(max_epochs, max_lr, min_lr=1e-7, warmup=False):\n    """"""\n    cosine annealing scheduler.\n    :param max_epochs: max epochs\n    :param max_lr: max lr\n    :param min_lr: min lr\n    :param warmup: warm up or not\n    :return: current lr\n    """"""\n    max_epochs = max_epochs - 5 if warmup else max_epochs\n\n    def decay(epoch):\n        lrate = min_lr + (max_lr - min_lr) * (\n                1 + np.cos(np.pi * epoch / max_epochs)) / 2\n        return lrate\n\n    return decay\n'"
utils/losses.py,3,"b'""""""\nThe implementation of some losses based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nimport tensorflow as tf\n\nbackend = tf.keras.backend\n\n\ndef categorical_crossentropy_with_logits(y_true, y_pred):\n    # compute cross entropy\n    cross_entropy = backend.categorical_crossentropy(y_true, y_pred, from_logits=True)\n\n    # compute loss\n    loss = backend.mean(backend.sum(cross_entropy, axis=[1, 2]))\n    return loss\n\n\ndef focal_loss(alpha=0.25, gamma=2.0):\n    def loss(y_true, y_pred):\n        y_pred = backend.softmax(y_pred)\n        # compute ce loss\n        cross_entropy = backend.categorical_crossentropy(y_true, y_pred, from_logits=False)\n        # compute weights\n        weights = backend.sum(alpha * backend.pow(1 - y_pred, gamma) * y_true, axis=-1)\n        return backend.mean(backend.sum(weights * cross_entropy, axis=[1, 2]))\n\n    return loss\n\n\ndef miou_loss(weights=None, num_classes=2):\n    if weights is not None:\n        assert len(weights) == num_classes\n        weights = tf.convert_to_tensor(weights)\n    else:\n        weights = tf.convert_to_tensor([1.] * num_classes)\n\n    def loss(y_true, y_pred):\n        y_pred = backend.softmax(y_pred)\n\n        inter = y_pred * y_true\n        inter = backend.sum(inter, axis=[1, 2])\n\n        union = y_pred + y_true - (y_pred * y_true)\n        union = backend.sum(union, axis=[1, 2])\n\n        return -backend.mean((weights * inter) / (weights * union + 1e-8))\n\n    return loss\n\n\ndef self_balanced_focal_loss(alpha=3, gamma=2.0):\n    """"""\n    Original by Yang Lu:\n\n    This is an improvement of Focal Loss, which has solved the problem\n    that the factor in Focal Loss failed in semantic segmentation.\n    It can adaptively adjust the weights of different classes in semantic segmentation\n    without introducing extra supervised information.\n\n    :param alpha: The factor to balance different classes in semantic segmentation.\n    :param gamma: The factor to balance different samples in semantic segmentation.\n    :return:\n    """"""\n\n    def loss(y_true, y_pred):\n        # cross entropy loss\n        y_pred = backend.softmax(y_pred, -1)\n        cross_entropy = backend.categorical_crossentropy(y_true, y_pred)\n\n        # sample weights\n        sample_weights = backend.max(backend.pow(1.0 - y_pred, gamma) * y_true, axis=-1)\n\n        # class weights\n        pixel_rate = backend.sum(y_true, axis=[1, 2], keepdims=True) / backend.sum(backend.ones_like(y_true),\n                                                                                   axis=[1, 2], keepdims=True)\n        class_weights = backend.max(backend.pow(backend.ones_like(y_true) * alpha, pixel_rate) * y_true, axis=-1)\n\n        # final loss\n        final_loss = class_weights * sample_weights * cross_entropy\n        return backend.mean(backend.sum(final_loss, axis=[1, 2]))\n\n    return loss\n'"
utils/metrics.py,3,"b'""""""\nThe implementation of some metrics based on Tensorflow.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nimport tensorflow as tf\n\n\nclass MeanIoU(tf.keras.metrics.MeanIoU):\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        # sparse code\n        y_true = tf.argmax(y_true, axis=-1)\n        y_pred = tf.argmax(y_pred, axis=-1)\n        return super(MeanIoU, self).update_state(y_true, y_pred, sample_weight)\n'"
utils/optimizers.py,2,"b'""""""\nThe implementation of some optimizers based on Tensorflow.\n\n@Author: OverLordGoldDragon\n@Project: https://github.com/OverLordGoldDragon/keras-adamw\n\n""""""\nfrom termcolor import colored\nimport tensorflow as tf\nimport numpy as np\n\nK = tf.keras.backend\nOptimizer = tf.keras.optimizers.Optimizer\n\n\ndef warn_str():\n    return colored(\'WARNING: \', \'red\')\n\n\nclass AdamW(Optimizer):\n    """"""AdamW optimizer.\n    Default parameters follow those provided in the original paper.\n    # Arguments\n        learning_rate: float >= 0. Learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 1.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n            algorithm from the paper ""On the Convergence of Adam and Beyond"".\n        batch_size:       int >= 1. Train input batch size; used for normalization\n        total_iterations: int >= 0. Total expected iterations / weight updates\n                          throughout training, used for normalization; <1>\n        weight_decays:    dict / None. Name-value pairs specifying weight decays,\n                          as {<weight matrix name>:<weight decay value>}; <2>\n        lr_multipliers:   dict / None. Name-value pairs specifying per-layer lr\n                          multipliers, as {<layer name>:<multiplier value>}; <2>\n        use_cosine_annealing: bool. If True, multiplies lr each train iteration\n                              as a function of eta_min, eta_max, total_iterations,\n                              and t_cur (current); [2]-Appendix, 2\n        eta_min, eta_max: int, int. Min & max values of cosine annealing\n                          lr multiplier; [2]-Appendix, 2\n        t_cur: int. Value to initialize t_cur to - used for \'warm restarts\'.\n               To be used together with use_cosine_annealing==True\n        init_verbose: bool. If True, print weight-name--weight-decay, and\n                      lr-multiplier--layer-name value pairs set during\n                      optimizer initialization (recommended)\n    # <1> - if using \'warm restarts\', then refers to total expected iterations\n            for a given restart; can be an estimate, and training won\'t stop\n            at iterations == total_iterations. [2]-Appendix, pg 1\n    # <2> - [AdamW Keras Implementation - Github repository]\n            (https://github.com/OverLordGoldDragon/keras_adamw)\n    # References\n        - [1][Adam - A Method for Stochastic Optimization]\n             (http://arxiv.org/abs/1412.6980v8)\n        - [2][Fixing Weight Decay Regularization in Adam]\n             (https://arxiv.org/abs/1711.05101)\n    """"""\n\n    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n                 amsgrad=False, batch_size=32, total_iterations=0,\n                 weight_decays=None, lr_multipliers=None,\n                 use_cosine_annealing=False, eta_min=0, eta_max=1,\n                 t_cur=0, init_verbose=True, name=\'AdamW\', **kwargs):\n        self.initial_decay = kwargs.pop(\'decay\', 0.0)\n        self.epsilon = kwargs.pop(\'epsilon\', K.epsilon())\n        learning_rate = kwargs.pop(\'lr\', learning_rate)\n        eta_t = kwargs.pop(\'eta_t\', 1.)\n        super(AdamW, self).__init__(name, **kwargs)\n\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype=\'int64\', name=\'iterations\')\n            self.learning_rate = K.variable(learning_rate, name=\'learning_rate\')\n            self.beta_1 = K.variable(beta_1, name=\'beta_1\')\n            self.beta_2 = K.variable(beta_2, name=\'beta_2\')\n            self.decay = K.variable(self.initial_decay, name=\'decay\')\n            self.batch_size = K.variable(batch_size, dtype=\'int64\',\n                                         name=\'batch_size\')\n            self.total_iterations = K.variable(total_iterations, dtype=\'int64\',\n                                               name=\'total_iterations\')\n            self.eta_min = K.constant(eta_min, name=\'eta_min\')\n            self.eta_max = K.constant(eta_max, name=\'eta_max\')\n            self.eta_t = K.variable(eta_t, dtype=\'float32\', name=\'eta_t\')\n            self.t_cur = K.variable(t_cur, dtype=\'int64\', name=\'t_cur\')\n\n        self.amsgrad = amsgrad\n        self.lr_multipliers = lr_multipliers\n        self.weight_decays = weight_decays\n        self.init_verbose = init_verbose\n        self.use_cosine_annealing = use_cosine_annealing\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        self.updates.append(K.update_add(self.t_cur, 1))\n\n        lr = self.learning_rate\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                      K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n                     (1. - K.pow(self.beta_1, t)))\n\n        ms = [K.zeros(K.int_shape(p),\n                      dtype=K.dtype(p),\n                      name=\'m_\' + str(i))\n              for (i, p) in enumerate(params)]\n        vs = [K.zeros(K.int_shape(p),\n                      dtype=K.dtype(p),\n                      name=\'v_\' + str(i))\n              for (i, p) in enumerate(params)]\n\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p),\n                             dtype=K.dtype(p),\n                             name=\'vhat_\' + str(i))\n                     for (i, p) in enumerate(params)]\n        else:\n            vhats = [K.zeros(1, name=\'vhat_\' + str(i))\n                     for i in range(len(params))]\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        total_iterations = K.get_value(self.total_iterations)\n        if total_iterations == 0:\n            print(warn_str() + ""\'total_iterations\'==0, must be !=0 to use ""\n                  + ""cosine annealing and/or weight decays; ""\n                  + ""proceeding without either"")\n        # Schedule multiplier\n        if self.use_cosine_annealing and total_iterations != 0:\n            t_frac = K.cast(self.t_cur / (self.total_iterations + 1), \'float32\')\n            self.eta_t = self.eta_min + 0.5 * (self.eta_max - self.eta_min) * \\\n                         (1 + K.cos(np.pi * t_frac))\n            if self.init_verbose:\n                print(\'Using cosine annealing learning rates\')\n        self.lr_t = lr * self.eta_t  # for external tracking\n\n        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n            # Learning rate multipliers\n            multiplier_name = None\n            if self.lr_multipliers:\n                multiplier_name = [mult_name for mult_name in self.lr_multipliers\n                                   if mult_name in p.name]\n            new_lr = lr_t\n            if multiplier_name:\n                new_lr = new_lr * self.lr_multipliers[multiplier_name[0]]\n                if self.init_verbose:\n                    print(\'{} learning rate set for {} -- {}\'.format(\n                        \'%.e\' % K.get_value(new_lr), p.name.split(\'/\')[0], new_lr))\n            elif not multiplier_name and self.init_verbose:\n                print(\'No change in learning rate {} -- {}\'.format(\n                    p.name, K.get_value(new_lr)))\n\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n                self.updates.append(K.update(vhat, vhat_t))\n            else:\n                p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n\n            # Weight decays\n            if p.name in self.weight_decays.keys() and total_iterations != 0:\n                wd = self.weight_decays[p.name]\n                wd_normalized = wd * K.cast(\n                    K.sqrt(self.batch_size / self.total_iterations), \'float32\')\n                p_t = p_t - self.eta_t * wd_normalized * p\n                if self.init_verbose:\n                    print(\'{} weight decay set for {}\'.format(\n                        K.get_value(wd_normalized), p.name))\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, \'constraint\', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {\'learning_rate\': float(K.get_value(self.learning_rate)),\n                  \'beta_1\': float(K.get_value(self.beta_1)),\n                  \'beta_2\': float(K.get_value(self.beta_2)),\n                  \'decay\': float(K.get_value(self.decay)),\n                  \'batch_size\': int(K.get_value(self.batch_size)),\n                  \'total_iterations\': int(K.get_value(self.total_iterations)),\n                  \'weight_decays\': self.weight_decays,\n                  \'lr_multipliers\': self.lr_multipliers,\n                  \'use_cosine_annealing\': self.use_cosine_annealing,\n                  \'t_cur\': int(K.get_value(self.t_cur)),\n                  \'eta_t\': int(K.get_value(self.eta_t)),\n                  \'eta_min\': int(K.get_value(self.eta_min)),\n                  \'eta_max\': int(K.get_value(self.eta_max)),\n                  \'init_verbose\': self.init_verbose,\n                  \'epsilon\': self.epsilon,\n                  \'amsgrad\': self.amsgrad}\n        base_config = super(AdamW, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass NadamW(Optimizer):\n    """"""Nesterov Adam optimizer.\n    Much like Adam is essentially RMSprop with momentum,\n    Nadam is Adam RMSprop with Nesterov momentum.\n    Default parameters follow those provided in the paper.\n    It is recommended to leave the parameters of this optimizer\n    at their default values.\n    # Arguments\n        lr: float >= 0. Learning rate.\n        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n    # Arguments (other): see AdamW\n    # References\n        - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n        - [On the importance of initialization and momentum in deep learning]\n          (http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n    """"""\n\n    def __init__(self, learning_rate=0.002, beta_1=0.9, beta_2=0.999,\n                 batch_size=32, total_iterations=0,\n                 weight_decays=None, lr_multipliers=None,\n                 use_cosine_annealing=False, eta_min=0, eta_max=1,\n                 t_cur=0, init_verbose=True, name=\'NadamW\', **kwargs):\n        self.schedule_decay = kwargs.pop(\'schedule_decay\', 0.004)\n        self.epsilon = kwargs.pop(\'epsilon\', K.epsilon())\n        learning_rate = kwargs.pop(\'lr\', learning_rate)\n        eta_t = kwargs.pop(\'eta_t\', 1.)\n        super(NadamW, self).__init__(name, **kwargs)\n\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype=\'int64\', name=\'iterations\')\n            self.m_schedule = K.variable(1., name=\'m_schedule\')\n            self.learning_rate = K.variable(learning_rate, name=\'learning_rate\')\n            self.beta_1 = K.variable(beta_1, name=\'beta_1\')\n            self.beta_2 = K.variable(beta_2, name=\'beta_2\')\n            self.batch_size = K.variable(batch_size, dtype=\'int64\',\n                                         name=\'batch_size\')\n            self.total_iterations = K.variable(total_iterations, dtype=\'int64\',\n                                               name=\'total_iterations\')\n            self.eta_min = K.constant(eta_min, name=\'eta_min\')\n            self.eta_max = K.constant(eta_max, name=\'eta_max\')\n            self.eta_t = K.variable(eta_t, dtype=\'float32\', name=\'eta_t\')\n            self.t_cur = K.variable(t_cur, dtype=\'int64\', name=\'t_cur\')\n\n        self.lr_multipliers = lr_multipliers\n        self.weight_decays = weight_decays\n        self.use_cosine_annealing = use_cosine_annealing\n        self.init_verbose = init_verbose\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        self.updates.append(K.update_add(self.t_cur, 1))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n\n        # Due to the recommendations in [2], i.e. warming momentum schedule\n        momentum_cache_t = self.beta_1 * (1. - 0.5 * (\n            K.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))\n        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (\n            K.pow(K.cast_to_floatx(0.96), (t + 1) * self.schedule_decay)))\n        m_schedule_new = self.m_schedule * momentum_cache_t\n        m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n        self.updates.append((self.m_schedule, m_schedule_new))\n\n        shapes = [K.int_shape(p) for p in params]\n        ms = [K.zeros(shape, name=\'m_\' + str(i))\n              for (i, shape) in enumerate(shapes)]\n        vs = [K.zeros(shape, name=\'v_\' + str(i))\n              for (i, shape) in enumerate(shapes)]\n\n        self.weights = [self.iterations, self.m_schedule] + ms + vs\n\n        total_iterations = K.get_value(self.total_iterations)\n        if total_iterations == 0:\n            print(warn_str() + ""\'total_iterations\'==0, must be !=0 to use ""\n                  + ""cosine annealing and/or weight decays; ""\n                  + ""proceeding without either"")\n        # Schedule multiplier\n        if self.use_cosine_annealing and total_iterations != 0:\n            t_frac = K.cast(self.t_cur / (self.total_iterations + 1), \'float32\')\n            self.eta_t = self.eta_min + 0.5 * (self.eta_max - self.eta_min) * \\\n                         (1 + K.cos(np.pi * t_frac))\n            if self.init_verbose:\n                print(\'Using cosine annealing learning rates\')\n        self.lr_t = self.learning_rate * self.eta_t  # for external tracking\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n            # Learning rate multipliers\n            multiplier_name = None\n            if self.lr_multipliers:\n                multiplier_name = [mult_name for mult_name in self.lr_multipliers\n                                   if mult_name in p.name]\n            new_lr = self.learning_rate\n            if multiplier_name:\n                new_lr = new_lr * self.lr_multipliers[multiplier_name[0]]\n                if self.init_verbose:\n                    print(\'{} learning rate set for {} -- {}\'.format(\n                        \'%.e\' % K.get_value(new_lr), p.name.split(\'/\')[0], new_lr))\n            elif not multiplier_name and self.init_verbose:\n                print(\'No change in learning rate {} -- {}\'.format(\n                    p.name, K.get_value(new_lr)))\n\n            # the following equations given in [1]\n            g_prime = g / (1. - m_schedule_new)\n            m_t = self.beta_1 * m + (1. - self.beta_1) * g\n            m_t_prime = m_t / (1. - m_schedule_next)\n            v_t = self.beta_2 * v + (1. - self.beta_2) * K.square(g)\n            v_t_prime = v_t / (1. - K.pow(self.beta_2, t))\n            m_t_bar = (1. - momentum_cache_t) * g_prime + (\n                    momentum_cache_t_1 * m_t_prime)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            p_t = p - self.eta_t * new_lr * m_t_bar / (\n                    K.sqrt(v_t_prime) + self.epsilon)\n\n            # Weight decays\n            if p.name in self.weight_decays.keys() and total_iterations != 0:\n                wd = self.weight_decays[p.name]\n                wd_normalized = wd * K.cast(\n                    K.sqrt(self.batch_size / self.total_iterations), \'float32\')\n                p_t = p_t - self.eta_t * wd_normalized * p\n                if self.init_verbose:\n                    print(\'{} weight decay set for {}\'.format(\n                        K.get_value(wd_normalized), p.name))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, \'constraint\', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def set_weights(self, weights):\n        params = self.weights\n        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer\n        # since it does not include m_schedule at head of the weight list. Set\n        # m_schedule to 1.\n        if len(params) == len(weights) + 1:\n            weights = [weights[0]] + [np.array(1.)] + weights[1:]\n        super(NadamW, self).set_weights(weights)\n\n    def get_config(self):\n        config = {\'learning_rate\': float(K.get_value(self.learning_rate)),\n                  \'beta_1\': float(K.get_value(self.beta_1)),\n                  \'beta_2\': float(K.get_value(self.beta_2)),\n                  \'epsilon\': self.epsilon,\n                  \'schedule_decay\': self.schedule_decay,\n                  \'batch_size\': int(K.get_value(self.batch_size)),\n                  \'total_iterations\': int(K.get_value(self.total_iterations)),\n                  \'weight_decays\': self.weight_decays,\n                  \'lr_multipliers\': self.lr_multipliers,\n                  \'use_cosine_annealing\': self.use_cosine_annealing,\n                  \'t_cur\': int(K.get_value(self.t_cur)),\n                  \'eta_t\': int(K.get_value(self.eta_t)),\n                  \'eta_min\': int(K.get_value(self.eta_min)),\n                  \'eta_max\': int(K.get_value(self.eta_max)),\n                  \'init_verbose\': self.init_verbose}\n        base_config = super(NadamW, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SGDW(Optimizer):\n    """"""Stochastic gradient descent optimizer.\n    Includes support for momentum,\n    learning rate decay, and Nesterov momentum.\n    # Arguments\n        lr: float >= 0. Learning rate.\n        momentum: float >= 0. Parameter that accelerates SGD\n            in the relevant direction and dampens oscillations.\n        decay: float >= 0. Learning rate decay over each update.\n        nesterov: boolean. Whether to apply Nesterov momentum.\n    # Arguments (other): see AdamW\n    """"""\n\n    def __init__(self, learning_rate=0.01, momentum=0., nesterov=False,\n                 batch_size=32, total_iterations=0,\n                 weight_decays=None, lr_multipliers=None,\n                 use_cosine_annealing=False, eta_min=0, eta_max=1,\n                 t_cur=0, init_verbose=True, name=\'SGDW\', **kwargs):\n        self.initial_decay = kwargs.pop(\'decay\', 0.0)\n        learning_rate = kwargs.pop(\'lr\', learning_rate)\n        eta_t = kwargs.pop(\'eta_t\', 1.)\n        super(SGDW, self).__init__(name, **kwargs)\n\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype=\'int64\', name=\'iterations\')\n            self.learning_rate = K.variable(learning_rate, name=\'learning_rate\')\n            self.momentum = K.variable(momentum, name=\'momentum\')\n            self.decay = K.variable(self.initial_decay, name=\'decay\')\n            self.batch_size = K.variable(batch_size, dtype=\'int64\',\n                                         name=\'batch_size\')\n            self.total_iterations = K.variable(total_iterations, dtype=\'int64\',\n                                               name=\'total_iterations\')\n            self.eta_min = K.constant(eta_min, name=\'eta_min\')\n            self.eta_max = K.constant(eta_max, name=\'eta_max\')\n            self.eta_t = K.variable(eta_t, dtype=\'float32\', name=\'eta_t\')\n            self.t_cur = K.variable(t_cur, dtype=\'int64\', name=\'t_cur\')\n\n        self.nesterov = nesterov\n        self.lr_multipliers = lr_multipliers\n        self.weight_decays = weight_decays\n        self.init_verbose = init_verbose\n        self.use_cosine_annealing = use_cosine_annealing\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        self.updates.append(K.update_add(self.t_cur, 1))\n\n        lr = self.learning_rate\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                      K.dtype(self.decay))))\n        # momentum\n        shapes = [K.int_shape(p) for p in params]\n        moments = [K.zeros(shape, name=\'moment_\' + str(i))\n                   for (i, shape) in enumerate(shapes)]\n        self.weights = [self.iterations] + moments\n\n        total_iterations = K.get_value(self.total_iterations)\n        if total_iterations == 0:\n            print(warn_str() + ""\'total_iterations\'==0, must be !=0 to use ""\n                  + ""cosine annealing and/or weight decays; ""\n                  + ""proceeding without either"")\n        # Schedule multiplier\n        if self.use_cosine_annealing and total_iterations != 0:\n            t_frac = K.cast(self.t_cur / (self.total_iterations + 1), \'float32\')\n            self.eta_t = self.eta_min + 0.5 * (self.eta_max - self.eta_min) * \\\n                         (1 + K.cos(np.pi * t_frac))\n            if self.init_verbose:\n                print(\'Using cosine annealing learning rates\')\n        self.lr_t = lr * self.eta_t  # for external tracking\n\n        for p, g, m in zip(params, grads, moments):\n            # Learning rate multipliers\n            multiplier_name = None\n            if self.lr_multipliers:\n                multiplier_name = [mult_name for mult_name in self.lr_multipliers\n                                   if mult_name in p.name]\n            new_lr = self.learning_rate\n            if multiplier_name:\n                new_lr = new_lr * self.lr_multipliers[multiplier_name[0]]\n                if self.init_verbose:\n                    print(\'{} learning rate set for {} -- {}\'.format(\n                        \'%.e\' % K.get_value(new_lr), p.name.split(\'/\')[0], new_lr))\n            elif not multiplier_name and self.init_verbose:\n                print(\'No change in learning rate {} -- {}\'.format(\n                    p.name, K.get_value(new_lr)))\n\n            v = self.momentum * m - self.eta_t * new_lr * g  # velocity\n            self.updates.append(K.update(m, v))\n\n            if self.nesterov:\n                new_p = p + self.momentum * v - self.eta_t * new_lr * g\n            else:\n                new_p = p + v\n\n            # Weight decays\n            if p.name in self.weight_decays.keys() and total_iterations != 0:\n                wd = self.weight_decays[p.name]\n                wd_normalized = wd * K.cast(\n                    K.sqrt(self.batch_size / self.total_iterations), \'float32\')\n                new_p = new_p - self.eta_t * wd_normalized * p\n                if self.init_verbose:\n                    print(\'{} weight decay set for {}\'.format(\n                        K.get_value(wd_normalized), p.name))\n            # Apply constraints.\n            if getattr(p, \'constraint\', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {\'learning_rate\': float(K.get_value(self.learning_rate)),\n                  \'momentum\': float(K.get_value(self.momentum)),\n                  \'decay\': float(K.get_value(self.decay)),\n                  \'nesterov\': self.nesterov,\n                  \'batch_size\': int(K.get_value(self.batch_size)),\n                  \'total_iterations\': int(K.get_value(self.total_iterations)),\n                  \'weight_decays\': self.weight_decays,\n                  \'lr_multipliers\': self.lr_multipliers,\n                  \'use_cosine_annealing\': self.use_cosine_annealing,\n                  \'t_cur\': int(K.get_value(self.t_cur)),\n                  \'eta_t\': int(K.get_value(self.eta_t)),\n                  \'eta_min\': int(K.get_value(self.eta_min)),\n                  \'eta_max\': int(K.get_value(self.eta_max)),\n                  \'init_verbose\': self.init_verbose}\n        base_config = super(SGDW, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
utils/utils.py,0,"b'""""""\nThe implementation of some utils.\n\n@Author: Yang Lu\n@Github: https://github.com/luyanger1799\n@Project: https://github.com/luyanger1799/amazing-semantic-segmentation\n\n""""""\nfrom keras_preprocessing import image as keras_image\nfrom PIL import Image\nimport numpy as np\nimport cv2\n\n\ndef load_image(name):\n    img = Image.open(name)\n    return np.array(img)\n\n\ndef resize_image(image, label, target_size=None):\n    if target_size is not None:\n        image = cv2.resize(image, dsize=target_size[::-1])\n        label = cv2.resize(label, dsize=target_size[::-1], interpolation=cv2.INTER_NEAREST)\n    return image, label\n\n\ndef random_crop(image, label, crop_size):\n    h, w = image.shape[0:2]\n    crop_h, crop_w = crop_size\n\n    if h < crop_h or w < crop_w:\n        image = cv2.resize(image, (max(w, crop_w), max(h, crop_h)))\n        label = cv2.resize(label, (max(w, crop_w), max(h, crop_h)), interpolation=cv2.INTER_NEAREST)\n\n    h, w = image.shape[0:2]\n    h_beg = np.random.randint(h - crop_h)\n    w_beg = np.random.randint(w - crop_w)\n\n    cropped_image = image[h_beg:h_beg + crop_h, w_beg:w_beg + crop_w]\n    cropped_label = label[h_beg:h_beg + crop_h, w_beg:w_beg + crop_w]\n\n    return cropped_image, cropped_label\n\n\ndef random_zoom(image, label, zoom_range):\n    if np.ndim(label) == 2:\n        label = np.expand_dims(label, axis=-1)\n    assert np.ndim(label) == 3\n\n    if np.isscalar(zoom_range):\n        zx, zy = np.random.uniform(1 - zoom_range, 1 + zoom_range, 2)\n    elif len(zoom_range) == 2:\n        zx, zy = np.random.uniform(zoom_range[0], zoom_range[1], 2)\n    else:\n        raise ValueError(\'`zoom_range` should be a float or \'\n                         \'a tuple or list of two floats. \'\n                         \'Received: %s\' % (zoom_range,))\n\n    image = keras_image.apply_affine_transform(image, zx=zx, zy=zy, fill_mode=\'nearest\')\n    label = keras_image.apply_affine_transform(label, zx=zx, zy=zy, fill_mode=\'nearest\')\n\n    return image, label\n\n\ndef random_brightness(image, label, brightness_range):\n    if np.ndim(label) == 2:\n        label = np.expand_dims(label, axis=-1)\n    assert np.ndim(label) == 3\n\n    if brightness_range is not None:\n        if isinstance(brightness_range, (tuple, list)) and len(brightness_range) == 2:\n            brightness = np.random.uniform(brightness_range[0], brightness_range[1])\n        else:\n            raise ValueError(\'`brightness_range` should be \'\n                             \'a tuple or list of two floats. \'\n                             \'Received: %s\' % (brightness_range,))\n        image = keras_image.apply_brightness_shift(image, brightness)\n    return image, label\n\n\ndef random_horizontal_flip(image, label, h_flip):\n    if h_flip:\n        image = cv2.flip(image, 1)\n        label = cv2.flip(label, 1)\n    return image, label\n\n\ndef random_vertical_flip(image, label, v_flip):\n    if v_flip:\n        image = cv2.flip(image, 0)\n        label = cv2.flip(label, 0)\n    return image, label\n\n\ndef random_rotation(image, label, rotation_range):\n    if np.ndim(label) == 2:\n        label = np.expand_dims(label, axis=-1)\n    assert np.ndim(label) == 3\n\n    if rotation_range > 0.:\n        theta = np.random.uniform(-rotation_range, rotation_range)\n        # rotate it!\n        image = keras_image.apply_affine_transform(image, theta=theta, fill_mode=\'nearest\')\n        label = keras_image.apply_affine_transform(label, theta=theta, fill_mode=\'nearest\')\n    return image, label\n\n\ndef random_channel_shift(image, label, channel_shift_range):\n    if np.ndim(label) == 2:\n        label = np.expand_dims(label, axis=-1)\n    assert np.ndim(label) == 3\n\n    if channel_shift_range > 0:\n        channel_shift_intensity = np.random.uniform(-channel_shift_range, channel_shift_range)\n        image = keras_image.apply_channel_shift(image, channel_shift_intensity, channel_axis=2)\n    return image, label\n\n\ndef one_hot(label, num_classes):\n    if np.ndim(label) == 3:\n        label = np.squeeze(label, axis=-1)\n    assert np.ndim(label) == 2\n\n    heat_map = np.ones(shape=label.shape[0:2] + (num_classes,))\n    for i in range(num_classes):\n        heat_map[:, :, i] = np.equal(label, i).astype(\'float32\')\n    return heat_map\n\n\ndef decode_one_hot(one_hot_map):\n    return np.argmax(one_hot_map, axis=-1)\n\n\n########################################################################################################################\n# adamw utils\ndef get_weight_decays(model, verbose=1):\n    wd_dict = {}\n    for layer in model.layers:\n        layer_l2regs = _get_layer_l2regs(layer)\n        if layer_l2regs:\n            for layer_l2 in layer_l2regs:\n                weight_name, weight_l2 = layer_l2\n                wd_dict.update({weight_name: weight_l2})\n                if weight_l2 != 0 and verbose:\n                    print((""WARNING: {} l2-regularization = {} - should be ""\n                           ""set 0 before compiling model"").format(\n                        weight_name, weight_l2))\n    return wd_dict\n\n\ndef fill_dict_in_order(_dict, _list_of_vals):\n    for idx, key in enumerate(_dict.keys()):\n        _dict[key] = _list_of_vals[idx]\n    return _dict\n\n\ndef _get_layer_l2regs(layer):\n    if hasattr(layer, \'layer\') or hasattr(layer, \'cell\'):\n        return _rnn_l2regs(layer)\n    else:\n        l2_lambda_kb = []\n        for weight_name in [\'kernel\', \'bias\']:\n            _lambda = getattr(layer, weight_name + \'_regularizer\', None)\n            if _lambda is not None:\n                l2_lambda_kb.append([getattr(layer, weight_name).name,\n                                     float(_lambda.l2)])\n        return l2_lambda_kb\n\n\ndef _rnn_l2regs(layer):\n    _layer = layer.layer if \'backward_layer\' in layer.__dict__ else layer\n    cell = _layer.cell\n\n    l2_lambda_krb = []\n    if hasattr(cell, \'kernel_regularizer\') or \\\n            hasattr(cell, \'recurrent_regularizer\') or hasattr(cell, \'bias_regularizer\'):\n        for weight_name in [\'kernel\', \'recurrent\', \'bias\']:\n            _lambda = getattr(cell, weight_name + \'_regularizer\', None)\n            if _lambda is not None:\n                weight_name = weight_name if \'recurrent\' not in weight_name \\\n                    else \'recurrent_kernel\'\n                l2_lambda_krb.append([getattr(cell, weight_name).name,\n                                      float(_lambda.l2)])\n    return l2_lambda_krb\n'"
