file_path,api_count,code
__init__.py,0,b''
config.py,0,"b'\ndef load_parameters():\n    """"""\n    Loads the defined hyperparameters.\n    :return parameters: Dictionary of loaded parameters.\n    """"""\n\n    # Input data params\n    TASK_NAME = \'EuTrans\'                           # Task name.\n    DATASET_NAME = TASK_NAME                        # Dataset name.\n    SRC_LAN = \'es\'                                  # Language of the source text.\n    TRG_LAN = \'en\'                                  # Language of the target text.\n    DATA_ROOT_PATH = \'examples/%s/\' % DATASET_NAME  # Path where data is stored.\n\n    # SRC_LAN or TRG_LAN will be added to the file names.\n    TEXT_FILES = {\'train\': \'training.\',             # Data files.\n                  \'val\': \'dev.\',\n                  \'test\': \'test.\'}\n\n    GLOSSARY = None                               # Glossary location. If not None, it overwrites translations according to this glossary file\n\n    # Dataset class parameters\n    INPUTS_IDS_DATASET = [\'source_text\', \'state_below\']        # Corresponding inputs of the dataset.\n    OUTPUTS_IDS_DATASET = [\'target_text\']                      # Corresponding outputs of the dataset.\n    INPUTS_IDS_MODEL = [\'source_text\', \'state_below\']          # Corresponding inputs of the built model.\n    OUTPUTS_IDS_MODEL = [\'target_text\']                        # Corresponding outputs of the built model.\n    INPUTS_TYPES_DATASET = [\'text-features\', \'text-features\']  # Corresponding types of the data. \'text\' or \'text-features\' allowed.\n    OUTPUTS_TYPES_DATASET = [\'text-features\']                  # They are equivalent, only differ on how the data is loaded.\n\n    # Evaluation params\n    METRICS = [\'sacrebleu\', \'perplexity\']         # Metric used for evaluating the model.\n    KERAS_METRICS = [\'perplexity\']                # Metrics to be logged by Keras during training (in addition to the loss).\n    EVAL_ON_SETS = [\'val\']                        # Possible values: \'train\', \'val\' and \'test\' (external evaluator).\n    START_EVAL_ON_EPOCH = 1                       # First epoch to start the model evaluation.\n    EVAL_EACH_EPOCHS = True                       # Select whether evaluate between N epochs or N updates.\n    EVAL_EACH = 1                                 # Sets the evaluation frequency (epochs or updates).\n\n    # Search parameters\n    SAMPLING = \'max_likelihood\'                   # Possible values: multinomial or max_likelihood (recommended).\n    TEMPERATURE = 1                               # Multinomial sampling parameter.\n    BEAM_SEARCH = True                            # Switches on-off the beam search procedure.\n    BEAM_SIZE = 6                                 # Beam size (in case of BEAM_SEARCH == True).\n    OPTIMIZED_SEARCH = True                       # Compute annotations only a single time per sample.\n    SEARCH_PRUNING = False                        # Apply pruning strategies to the beam search method.\n                                                  # It will likely increase decoding speed, but decrease quality.\n    MAXLEN_GIVEN_X = True                         # Generate translations of similar length to the source sentences.\n    MAXLEN_GIVEN_X_FACTOR = 2                     # The hypotheses will have (as maximum) the number of words of the\n                                                  # source sentence * LENGTH_Y_GIVEN_X_FACTOR.\n    MINLEN_GIVEN_X = True                         # Generate translations of similar length to the source sentences.\n    MINLEN_GIVEN_X_FACTOR = 3                     # The hypotheses will have (as minimum) the number of words of the\n                                                  # source sentence / LENGTH_Y_GIVEN_X_FACTOR.\n\n    # Apply length and coverage decoding normalizations.\n    # See Section 7 from Wu et al. (2016) (https://arxiv.org/abs/1609.08144).\n    LENGTH_PENALTY = False                        # Apply length penalty.\n    LENGTH_NORM_FACTOR = 0.2                      # Length penalty factor.\n    COVERAGE_PENALTY = False                      # Apply source coverage penalty.\n    COVERAGE_NORM_FACTOR = 0.2                    # Coverage penalty factor.\n\n    # Alternative (simple) length normalization.\n    NORMALIZE_SAMPLING = False                    # Normalize hypotheses scores according to their length:\n    ALPHA_FACTOR = .6                             # Normalization according to |h|**ALPHA_FACTOR.\n\n    # Sampling params: Show some samples during training.\n    SAMPLE_ON_SETS = [\'train\', \'val\']             # Possible values: \'train\', \'val\' and \'test\'.\n    N_SAMPLES = 5                                 # Number of samples generated.\n    START_SAMPLING_ON_EPOCH = 1                   # First epoch where to start the sampling counter.\n    SAMPLE_EACH_UPDATES = 300                     # Sampling frequency (always in #updates).\n\n    # Unknown words treatment\n    POS_UNK = True                                # Enable POS_UNK strategy for unknown words.\n    HEURISTIC = 0                                 # Heuristic to follow:\n                                                  #     0: Replace the UNK by the correspondingly aligned source.\n                                                  #     1: Replace the UNK by the translation (given by an external\n                                                  #        dictionary) of the correspondingly aligned source.\n                                                  #     2: Replace the UNK by the translation (given by an external\n                                                  #        dictionary) of the correspondingly aligned source only if it\n                                                  #        starts with a lowercase. Otherwise, copies the source word.\n    ALIGN_FROM_RAW = True                         # Align using the full vocabulary or the short_list.\n\n    # Source -- Target pkl mapping (used for heuristics 1--2). See utils/build_mapping_file.sh for further info.\n    MAPPING = DATA_ROOT_PATH + \'/mapping.%s_%s.pkl\' % (SRC_LAN, TRG_LAN)\n\n    # Word representation params\n    TOKENIZATION_METHOD = \'tokenize_none\'         # Select which tokenization we\'ll apply.\n                                                  # See Dataset class (from stager_keras_wrapper) for more info.\n    BPE_CODES_PATH = DATA_ROOT_PATH + \'/training_codes.joint\'    # If TOKENIZATION_METHOD = \'tokenize_bpe\',\n                                                  # sets the path to the learned BPE codes.\n    DETOKENIZATION_METHOD = \'detokenize_none\'     # Select which de-tokenization method we\'ll apply.\n\n    APPLY_DETOKENIZATION = False                  # Wheter we apply a detokenization method.\n\n    TOKENIZE_HYPOTHESES = True   \t\t          # Whether we tokenize the hypotheses using the\n                                                  # previously defined tokenization method.\n    TOKENIZE_REFERENCES = True                    # Whether we tokenize the references using the\n                                                  # previously defined tokenization method.\n\n    # Input image parameters\n    DATA_AUGMENTATION = False                     # Apply data augmentation on input data (still unimplemented for text inputs).\n\n    # Text parameters\n    FILL = \'end\'                                  # Whether we pad the \'end\', the \'start\' of  the sentence with 0s. We can also \'center\' it.\n    PAD_ON_BATCH = True                           # Whether we take as many timesteps as the longest sequence of\n                                                  # the batch or a fixed size (MAX_OUTPUT_TEXT_LEN).\n    # Input text parameters\n    INPUT_VOCABULARY_SIZE = 0                     # Size of the input vocabulary. Set to 0 for using all,\n                                                  # otherwise it will be truncated to these most frequent words.\n    MIN_OCCURRENCES_INPUT_VOCAB = 0               # Minimum number of occurrences allowed for the words in the input vocabulary.\n                                                  # Set to 0 for using them all.\n    MAX_INPUT_TEXT_LEN = 50                       # Maximum length of the input sequence.\n\n    # Output text parameters\n    OUTPUT_VOCABULARY_SIZE = 0                    # Size of the input vocabulary. Set to 0 for using all,\n                                                  # otherwise it will be truncated to these most frequent words.\n    MIN_OCCURRENCES_OUTPUT_VOCAB = 0              # Minimum number of occurrences allowed for the words in the output vocabulary.\n    MAX_OUTPUT_TEXT_LEN = 50                      # Maximum length of the output sequence.\n                                                  # set to 0 if we want to use the whole answer as a single class.\n    MAX_OUTPUT_TEXT_LEN_TEST = MAX_OUTPUT_TEXT_LEN * 3  # Maximum length of the output sequence during test time.\n\n    # Optimizer parameters (see model.compile() function).\n    LOSS = \'categorical_crossentropy\'\n    CLASSIFIER_ACTIVATION = \'softmax\'\n    SAMPLE_WEIGHTS = True                         # Select whether we use a weights matrix (mask) for the data outputs\n    LABEL_SMOOTHING = 0.                          # Epsilon value for label smoothing. Only valid for \'categorical_crossentropy\' loss. See arxiv.org/abs/1512.00567.\n\n    OPTIMIZER = \'Adam\'                            # Optimizer. Supported optimizers: SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam.\n    LR = 0.001                                    # Learning rate. Recommended values - Adam 0.0002 - Adadelta 1.0.\n    CLIP_C = 5.                                   # During training, clip L2 norm of gradients to this value (0. means deactivated).\n    CLIP_V = 0.                                   # During training, clip absolute value of gradients to this value (0. means deactivated).\n    USE_TF_OPTIMIZER = True                       # Use native Tensorflow\'s optimizer (only for the Tensorflow backend).\n\n    # Advanced parameters for optimizers. Default values are usually effective.\n    MOMENTUM = 0.                                 # Momentum value (for SGD optimizer).\n    NESTEROV_MOMENTUM = False                     # Use Nesterov momentum (for SGD optimizer).\n    RHO = 0.9                                     # Rho value (for Adadelta and RMSprop optimizers).\n    BETA_1 = 0.9                                  # Beta 1 value (for Adam, Adamax Nadam optimizers).\n    BETA_2 = 0.999                                # Beta 2 value (for Adam, Adamax Nadam optimizers).\n    AMSGRAD = False                               # Whether to apply the AMSGrad variant of Adam (see https://openreview.net/pdf?id=ryQu7f-RZ).\n    EPSILON = 1e-8                                # Optimizers epsilon value.\n    ACCUMULATE_GRADIENTS = 1                      # Accumulate gradients for this number of batches. Currently only implemented for Adam.\n\n    # Learning rate schedule\n    LR_DECAY = None                               # Frequency (number of epochs or updates) between LR annealings. Set to None for not decay the learning rate.\n    LR_GAMMA = 0.8                                # Multiplier used for decreasing the LR.\n    LR_REDUCE_EACH_EPOCHS = False                 # Reduce each LR_DECAY number of epochs or updates.\n    LR_START_REDUCTION_ON_EPOCH = 0               # Epoch to start the reduction.\n    LR_REDUCER_TYPE = \'exponential\'               # Function to reduce. \'linear\' and \'exponential\' implemented.\n                                                  # Linear reduction: new_lr = lr * LR_GAMMA\n                                                  # Exponential reduction: new_lr = lr * LR_REDUCER_EXP_BASE ** (current_nb / LR_HALF_LIFE) * LR_GAMMA\n                                                  # Noam reduction: new_lr = lr * min(current_nb ** LR_REDUCER_EXP_BASE, current_nb * LR_HALF_LIFE ** WARMUP_EXP)\n    LR_REDUCER_EXP_BASE = -0.5                    # Base for the exponential decay.\n    LR_HALF_LIFE = 100                            # Factor/warmup steps for exponenital/noam decay.\n    WARMUP_EXP = -1.5                             # Warmup steps for noam decay.\n    MIN_LR = 1e-9                                 # Minimum value allowed for the decayed LR\n\n    # Training parameters\n    MAX_EPOCH = 500                               # Stop when computed this number of epochs.\n    BATCH_SIZE = 50                               # Size of each minibatch.\n    N_GPUS = 1                                    # Number of GPUs to use. Only for Tensorflow backend. Each GPU will receive mini-batches of BATCH_SIZE / N_GPUS.\n\n    HOMOGENEOUS_BATCHES = False                   # Use batches with homogeneous output lengths (Dangerous!!).\n    JOINT_BATCHES = 4                             # When using homogeneous batches, get this number of batches to sort.\n    PARALLEL_LOADERS = 1                          # Parallel data batch loaders. Somewhat untested if > 1.\n    EPOCHS_FOR_SAVE = 1                           # Number of epochs between model saves.\n    WRITE_VALID_SAMPLES = True                    # Write valid samples in file.\n    SAVE_EACH_EVALUATION = True                   # Save each time we evaluate the model.\n\n    # Early stop parameters\n    EARLY_STOP = True                             # Turns on/off the early stop protocol.\n    PATIENCE = 10                                 # We\'ll stop if the val STOP_METRIC does not improve after this.\n                                                  # number of evaluations.\n    STOP_METRIC = \'Bleu_4\'                        # Metric for the stop.\n    MIN_DELTA = 0.                                # Minimum change in the monitored quantity to consider it as an improvement.\n\n    # Model parameters\n    MODEL_TYPE = \'AttentionRNNEncoderDecoder\'     # Model to train. See model_zoo.py for more info.\n                                                  # Supported architectures: \'AttentionRNNEncoderDecoder\' and \'Transformer\'.\n\n    # Common hyperparameters for all models\n    TRAINABLE_ENCODER = True                      # Whether the encoder\'s weights should be modified during training.\n    TRAINABLE_DECODER = True                      # Whether the decoder\'s weights should be modified during training.\n\n    # Initializers (see keras/initializations.py).\n    INIT_FUNCTION = \'glorot_uniform\'              # General initialization function for matrices.\n    INNER_INIT = \'orthogonal\'                     # Initialization function for inner RNN matrices.\n    INIT_ATT = \'glorot_uniform\'                   # Initialization function for attention mechism matrices\n\n    SOURCE_TEXT_EMBEDDING_SIZE = 32              # Source language word embedding size.\n    SRC_PRETRAINED_VECTORS = None                 # Path to pretrained vectors (e.g.: DATA_ROOT_PATH + \'/DATA/word2vec.%s.npy\' % SRC_LAN).\n                                                  # Set to None if you don\'t want to use pretrained vectors.\n                                                  # When using pretrained word embeddings. this parameter must match with the word embeddings size\n    SRC_PRETRAINED_VECTORS_TRAINABLE = True       # Finetune or not the target word embedding vectors.\n\n    TARGET_TEXT_EMBEDDING_SIZE = 32               # Source language word embedding size.\n    TRG_PRETRAINED_VECTORS = None                 # Path to pretrained vectors. (e.g. DATA_ROOT_PATH + \'/DATA/word2vec.%s.npy\' % TRG_LAN)\n                                                  # Set to None if you don\'t want to use pretrained vectors.\n                                                  # When using pretrained word embeddings, the size of the pretrained word embeddings must match with the word embeddings size.\n    TRG_PRETRAINED_VECTORS_TRAINABLE = True       # Finetune or not the target word embedding vectors.\n\n    SCALE_SOURCE_WORD_EMBEDDINGS = False          # Scale source word embeddings by Sqrt(SOURCE_TEXT_EMBEDDING_SIZE).\n    SCALE_TARGET_WORD_EMBEDDINGS = False          # Scale target word embeddings by Sqrt(TARGET_TEXT_EMBEDDING_SIZE).\n\n    TIE_EMBEDDINGS = False                        # Use the same embeddings for source and target language.\n\n    N_LAYERS_ENCODER = 1                          # Stack this number of encoding layers.\n    N_LAYERS_DECODER = 1                          # Stack this number of decoding layers.\n\n    # Additional Fully-Connected layers applied before softmax.\n    #       Here we should specify the activation function and the output dimension.\n    #       (e.g DEEP_OUTPUT_LAYERS = [(\'tanh\', 600), (\'relu\', 400), (\'relu\', 200)])\n    DEEP_OUTPUT_LAYERS = [(\'linear\', TARGET_TEXT_EMBEDDING_SIZE)]\n\n    # AttentionRNNEncoderDecoder model hyperparameters\n    ENCODER_RNN_TYPE = \'LSTM\'                     # Encoder\'s RNN unit type (\'LSTM\' and \'GRU\' supported).\n    USE_CUDNN = False                             # Use CuDNN\'s implementation of GRU and LSTM (only for Tensorflow backend).\n    GRU_RESET_AFTER = True                        # GRU convention (whether to apply reset gate after or before matrix multiplication).\n                                                  # False = ""before"", True = ""after"" (CuDNN compatible).\n\n\n    DECODER_RNN_TYPE = \'ConditionalLSTM\'          # Decoder\'s RNN unit type.\n                                                  # (\'LSTM\', \'GRU\', \'ConditionalLSTM\' and \'ConditionalGRU\' supported).\n    ATTENTION_MODE = \'add\'                        # Attention mode. \'add\' (Bahdanau-style), \'dot\' (Luong-style) or \'scaled-dot\'.\n\n    # Encoder configuration\n    ENCODER_HIDDEN_SIZE = 32                      # For models with RNN encoder.\n    BIDIRECTIONAL_ENCODER = True                  # Use bidirectional encoder.\n    BIDIRECTIONAL_DEEP_ENCODER = True             # Use bidirectional encoder in all encoding layers.\n    BIDIRECTIONAL_MERGE_MODE = \'concat\'           # Merge function for bidirectional layers.\n\n    # Fully-Connected layers for initializing the first decoder RNN state.\n    #       Here we should only specify the activation function of each layer (as they have a potentially fixed size)\n    #       (e.g INIT_LAYERS = [\'tanh\', \'relu\'])\n    INIT_LAYERS = [\'tanh\']\n\n    # Decoder configuration\n    DECODER_HIDDEN_SIZE = 32                      # For models with RNN decoder.\n    ATTENTION_SIZE = DECODER_HIDDEN_SIZE\n\n    # Skip connections parameters\n    SKIP_VECTORS_HIDDEN_SIZE = TARGET_TEXT_EMBEDDING_SIZE     # Hidden size.\n    ADDITIONAL_OUTPUT_MERGE_MODE = \'Add\'          # Merge mode for the skip-connections (see keras.layers.merge.py).\n    SKIP_VECTORS_SHARED_ACTIVATION = \'tanh\'       # Activation for the skip vectors.\n\n    # Transformer model hyperparameters\n    MODEL_SIZE = 32                               # Transformer model size (d_{model} in de paper).\n    MULTIHEAD_ATTENTION_ACTIVATION = \'linear\'     # Activation the input projections in the Multi-Head Attention blocks.\n    FF_SIZE = MODEL_SIZE * 4                      # Size of the feed-forward layers of the Transformer model.\n    N_HEADS = 8                                   # Number of parallel attention layers of the Transformer model.\n\n    # Regularizers\n    REGULARIZATION_FN = \'L2\'                      # Regularization function. \'L1\', \'L2\' and \'L1_L2\' supported.\n    WEIGHT_DECAY = 1e-4                           # Regularization coefficient.\n    RECURRENT_WEIGHT_DECAY = 0.                   # Regularization coefficient in recurrent layers.\n\n    DROPOUT_P = 0.                                # Percentage of units to drop (0 means no dropout).\n    RECURRENT_INPUT_DROPOUT_P = 0.                # Percentage of units to drop in input cells of recurrent layers.\n    RECURRENT_DROPOUT_P = 0.                      # Percentage of units to drop in recurrent layers.\n    ATTENTION_DROPOUT_P = 0.                      # Percentage of units to drop in attention layers (0 means no dropout).\n\n    USE_NOISE = False                              # Use gaussian noise during training.\n    NOISE_AMOUNT = 0.01                           # Amount of noise.\n\n    USE_BATCH_NORMALIZATION = True                # If True it is recommended to deactivate Dropout.\n    BATCH_NORMALIZATION_MODE = 1                  # See documentation in Keras\' BN.\n\n    USE_PRELU = False                             # use PReLU activations as regularizer.\n    USE_L1 = False                                # L1 normalization on the features.\n    USE_L2 = False                                # L2 normalization on the features.\n\n    DOUBLE_STOCHASTIC_ATTENTION_REG = 0.0         # Doubly stochastic attention (Eq. 14 from arXiv:1502.03044).\n\n    # Results plot and models storing parameters.\n    EXTRA_NAME = \'\'                               # This will be appended to the end of the model name.\n    if MODEL_TYPE == \'AttentionRNNEncoderDecoder\':\n        MODEL_NAME = TASK_NAME + \'_\' + SRC_LAN + TRG_LAN + \'_\' + MODEL_TYPE + \\\n                 \'_src_emb_\' + str(SOURCE_TEXT_EMBEDDING_SIZE) + \\\n                 \'_bidir_\' + str(BIDIRECTIONAL_ENCODER) + \\\n                 \'_enc_\' + ENCODER_RNN_TYPE + \'_\' + str(ENCODER_HIDDEN_SIZE) + \\\n                 \'_dec_\' + DECODER_RNN_TYPE + \'_\' + str(DECODER_HIDDEN_SIZE) + \\\n                 \'_deepout_\' + \'_\'.join([layer[0] for layer in DEEP_OUTPUT_LAYERS]) + \\\n                 \'_trg_emb_\' + str(TARGET_TEXT_EMBEDDING_SIZE) + \\\n                 \'_\' + OPTIMIZER + \'_\' + str(LR)\n    elif MODEL_TYPE == \'Transformer\':\n        MODEL_NAME = TASK_NAME + \'_\' + SRC_LAN + TRG_LAN + \'_\' + MODEL_TYPE + \\\n                 \'_model_size_\' + str(MODEL_SIZE) + \\\n                 \'_ff_size_\' + str(FF_SIZE) + \\\n                 \'_num_heads_\' + str(N_HEADS) + \\\n                 \'_encoder_blocks_\' + str(N_LAYERS_ENCODER) + \\\n                 \'_decoder_blocks_\' + str(N_LAYERS_DECODER) + \\\n                 \'_deepout_\' + \'_\'.join([layer[0] for layer in DEEP_OUTPUT_LAYERS]) + \\\n                 \'_\' + OPTIMIZER + \'_\' + str(LR)\n    else:\n        MODEL_NAME = TASK_NAME + \'_\' + SRC_LAN + TRG_LAN + \'_\' +\\\n                     MODEL_TYPE + \'_\' + OPTIMIZER + \'_\' + str(LR)\n\n    MODEL_NAME += EXTRA_NAME\n\n    STORE_PATH = \'trained_models/\' + MODEL_NAME + \'/\'  # Models and evaluation results will be stored here.\n    DATASET_STORE_PATH = \'datasets/\'                   # Dataset instance will be stored here.\n\n    # Tensorboard configuration. Only if the backend is Tensorflow. Otherwise, it will be ignored.\n    TENSORBOARD = True                                 # Switches On/Off the tensorboard callback.\n    LOG_DIR = \'tensorboard_logs\'                       # Directory to store teh model. Will be created inside STORE_PATH.\n    EMBEDDINGS_FREQ = 1                                # Frequency (in epochs) at which selected embedding layers will be saved.\n\n    SAMPLING_SAVE_MODE = \'list\'                        # \'list\': Store in a text file, one sentence per line.\n    PLOT_EVALUATION = False                            # If True, the evaluation will be plotted into the model folder.\n    MAX_PLOT_Y = 1. if \'coco\' in METRICS else 100.     # Max value of axis Y in the plot.\n\n    VERBOSE = 1                                        # Verbosity level.\n    RELOAD = 0                                         # If 0 start training from scratch, otherwise the model.\n                                                       # Saved on epoch \'RELOAD\' will be used.\n    RELOAD_EPOCH = True                                # Select whether we reload epoch or update number.\n\n    REBUILD_DATASET = True                             # Build again or use stored instance.\n    MODE = \'training\'                                  # \'training\' or \'sampling\' (if \'sampling\' then RELOAD must\n                                                       # be greater than 0 and EVAL_ON_SETS will be used).\n\n    # Extra parameters for special trainings. In most cases, they should be set to `False`\n    TRAIN_ON_TRAINVAL = False                          # train the model on both training and validation sets combined.\n    FORCE_RELOAD_VOCABULARY = False                    # force building a new vocabulary from the training samples\n                                                       # applicable if RELOAD > 1\n    # ================================================ #\n    parameters = locals().copy()\n    return parameters\n'"
main.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport argparse\nimport ast\nimport logging\n\nfrom keras_wrapper.extra.read_write import pkl2dict\nfrom config import load_parameters\nfrom utils.utils import update_parameters\nfrom nmt_keras import check_params\nfrom nmt_keras.training import train_model\nlogging.basicConfig(level=logging.INFO, format=\'[%(asctime)s] %(message)s\', datefmt=\'%d/%m/%Y %H:%M:%S\')\nlogger = logging.getLogger(__name__)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(""Train or sample NMT models"")\n    parser.add_argument(""-c"", ""--config"", required=False, help=""Config pkl for loading the model configuration. ""\n                                                               ""If not specified, hyperparameters ""\n                                                               ""are read from config.py"")\n    parser.add_argument(""-ds"", ""--dataset"", required=False, help=""Dataset instance with data"")\n    parser.add_argument(""changes"", nargs=""*"", help=""Changes to config. ""\n                                                   ""Following the syntax Key=Value"",\n                        default="""")\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    parameters = load_parameters()\n    if args.config is not None:\n        parameters = update_parameters(parameters, pkl2dict(args.config))\n    try:\n        for arg in args.changes:\n            try:\n                k, v = arg.split(\'=\')\n            except ValueError:\n                print (\'Overwritten arguments must have the form key=Value. \\n Currently are: %s\' % str(args.changes))\n                exit(1)\n            try:\n                parameters[k] = ast.literal_eval(v)\n            except ValueError:\n                parameters[k] = v\n    except ValueError:\n        print (\'Error processing arguments: (\', k, "","", v, "")"")\n        exit(2)\n\n    parameters = check_params(parameters)\n    if parameters[\'MODE\'] == \'training\':\n        logger.info(\'Running training.\')\n        train_model(parameters, args.dataset)\n    elif parameters[\'MODE\'] == \'sampling\':\n        logger.error(\'Depecrated function. For sampling from a trained model, please run sample_ensemble.py.\')\n        exit(2)\n    logger.info(\'Done!\')\n'"
sample_ensemble.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport argparse\nimport logging\nimport ast\nfrom keras_wrapper.extra.read_write import pkl2dict\nfrom nmt_keras import check_params\nfrom nmt_keras.apply_model import sample_ensemble\n\nlogging.basicConfig(level=logging.INFO, format=\'[%(asctime)s] %(message)s\', datefmt=\'%d/%m/%Y %H:%M:%S\')\nlogger = logging.getLogger(__name__)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(""Use several translation models for obtaining predictions from a source text file."")\n    parser.add_argument(""-ds"", ""--dataset"", required=True, help=""Dataset instance with data"")\n    parser.add_argument(""-t"", ""--text"", required=True, help=""Text file with source sentences"")\n    parser.add_argument(""-s"", ""--splits"", nargs=\'+\', required=False, default=[\'val\'], help=""Splits to sample. ""\n                                                                                           ""Should be already included""\n                                                                                           ""into the dataset object."")\n    parser.add_argument(""-d"", ""--dest"", required=False, help=""File to save translations in. If not specified, ""\n                                                             ""translations are outputted in STDOUT."")\n    parser.add_argument(""-v"", ""--verbose"", required=False, default=0, type=int, help=""Verbosity level"")\n    parser.add_argument(""-c"", ""--config"", required=False, help=""Config pkl for loading the model configuration. ""\n                                                               ""If not specified, hyperparameters ""\n                                                               ""are read from config.py"")\n    parser.add_argument(""-n"", ""--n-best"", action=""store_true"", default=False, help=""Write n-best list (n = beam size)"")\n    parser.add_argument(""-w"", ""--weights"", nargs=""*"", help=""Weight given to each model in the ensemble. You should provide the same number of weights than models.""\n                                                           ""By default, it applies the same weight to each model (1/N)."", default=[])\n    parser.add_argument(""-g"", ""--glossary"", required=False, help=""Glossary file for overwriting translations."")\n    parser.add_argument(""-m"", ""--models"", nargs=""+"", required=True, help=""Path to the models"")\n    parser.add_argument(""-ch"", ""--changes"", nargs=""*"", help=""Changes to the config. Following the syntax Key=Value"",\n                        default="""")\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n\n    args = parse_args()\n    if args.config is None:\n        logger.info(""Reading parameters from config.py"")\n        from config import load_parameters\n        params = load_parameters()\n    else:\n        logger.info(""Loading parameters from %s"" % str(args.config))\n        params = pkl2dict(args.config)\n    try:\n        for arg in args.changes:\n            try:\n                k, v = arg.split(\'=\')\n            except ValueError:\n                print (\'Overwritten arguments must have the form key=Value. \\n Currently are: %s\' % str(args.changes))\n                exit(1)\n            try:\n                params[k] = ast.literal_eval(v)\n            except ValueError:\n                params[k] = v\n    except ValueError:\n        print (\'Error processing arguments: (\', k, "","", v, "")"")\n        exit(2)\n    params = check_params(params)\n    sample_ensemble(args, params)\n'"
score.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport argparse\nimport logging\nimport ast\nfrom keras_wrapper.extra.read_write import pkl2dict\nfrom nmt_keras import check_params\nfrom nmt_keras.apply_model import score_corpus\n\nlogging.basicConfig(level=logging.INFO, format=\'[%(asctime)s] %(message)s\', datefmt=\'%d/%m/%Y %H:%M:%S\')\nlogger = logging.getLogger(__name__)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(""Use several translation models for scoring source--target pairs"")\n    parser.add_argument(""-ds"", ""--dataset"", required=True, help=""Dataset instance with data"")\n    parser.add_argument(""-src"", ""--source"", required=True, help=""Text file with source sentences"")\n    parser.add_argument(""-trg"", ""--target"", required=True, help=""Text file with target sentences"")\n    parser.add_argument(""-s"", ""--splits"", nargs=\'+\', required=False, default=[\'val\'], help=""Splits to sample. ""\n                                                                                           ""Should be already included ""\n                                                                                           ""into the dataset object."")\n    parser.add_argument(""-d"", ""--dest"", required=False, help=""File to save scores in"")\n    parser.add_argument(""-v"", ""--verbose"", required=False, action=\'store_true\', default=False, help=""Be verbose"")\n    parser.add_argument(""-w"", ""--weights"", nargs=""*"", help=""Weight given to each model in the ensemble. ""\n                                                           ""You should provide the same number of weights than models. ""\n                                                           ""By default, it applies the same weight to each model (1/N)."", default=[])\n    parser.add_argument(""-c"", ""--config"", required=False, help=""Config pkl for loading the model configuration. ""\n                                                               ""If not specified, hyperparameters ""\n                                                               ""are read from config.py"")\n    parser.add_argument(""--models"", nargs=\'+\', required=True, help=""path to the models"")\n    parser.add_argument(""-ch"", ""--changes"", nargs=""*"", help=""Changes to the config. Following the syntax Key=Value"",\n                        default="""")\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n\n    args = parse_args()\n    if args.config is None:\n        logger.info(""Reading parameters from config.py"")\n        from config import load_parameters\n        params = load_parameters()\n    else:\n        logger.info(""Loading parameters from %s"" % str(args.config))\n        params = pkl2dict(args.config)\n    try:\n        for arg in args.changes:\n            try:\n                k, v = arg.split(\'=\')\n            except ValueError:\n                print (\'Overwritten arguments must have the form key=Value. \\n Currently are: %s\' % str(args.changes))\n                exit(1)\n            try:\n                params[k] = ast.literal_eval(v)\n            except ValueError:\n                params[k] = v\n    except ValueError:\n        print (\'Error processing arguments: (\', k, "","", v, "")"")\n        exit(2)\n    params = check_params(params)\n    score_corpus(args, params)\n'"
setup.py,0,"b'# -*- coding: utf-8 -*-\nfrom setuptools import setup\n\nsetup(name=\'nmt_keras\',\n      version=\'0.6\',\n      description=\'Neural Machine Translation with Keras (Theano and Tensorflow).\',\n      author=\'Marc Bola\xc3\xb1os - Alvaro Peris\',\n      author_email=\'lvapeab@gmail.com\',\n      url=\'https://github.com/lvapeab/nmt-keras\',\n      download_url=\'https://github.com/lvapeab/nmt-keras/archive/master.zip\',\n      license=\'MIT\',\n      classifiers=[\n          \'Intended Audience :: Developers\',\n          \'Intended Audience :: Education\',\n          \'Intended Audience :: Science/Research\',\n          \'Programming Language :: Python :: 2\',\n          \'Programming Language :: Python :: 2.7\',\n          \'Programming Language :: Python :: 3\',\n          \'Programming Language :: Python :: 3.6\',\n          \'Programming Language :: Python :: 3.7\',\n          \'Topic :: Software Development :: Libraries\',\n          \'Topic :: Software Development :: Libraries :: Python Modules\',\n          ""License :: OSI Approved :: MIT License""\n      ],\n      install_requires=[\n          \'cloudpickle\',\n          \'future\',\n          \'keras @ https://github.com/MarcBS/keras/archive/master.zip\',\n          \'keras_applications\',\n          \'keras_preprocessing\',\n          \'h5py\',\n          \'matplotlib\',\n          \'multimodal-keras-wrapper\',\n          \'numpy\',\n          \'scikit-image\',\n          \'scikit-learn\',\n          \'six\',\n          \'tables\',\n          \'numpy\',\n          \'pandas\',\n          \'sacrebleu\',\n          \'sacremoses\',\n          \'scipy\',\n          \'tensorflow<2\'\n      ],\n      package_dir={\'nmt_keras\': \'.\',\n                   \'nmt_keras.utils\': \'utils\',\n                   \'nmt_keras.data_engine\': \'data_engine\',\n                   \'nmt_keras.nmt_keras\': \'nmt_keras\',\n                   \'nmt_keras.demo-web\': \'demo-web\',\n                   },\n      packages=[\'nmt_keras\',\n                \'nmt_keras.utils\',\n                \'nmt_keras.data_engine\',\n                \'nmt_keras.nmt_keras\',\n                \'nmt_keras.demo-web\'\n                ],\n      package_data={\n          \'nmt_keras\': [\'examples/*\']\n      }\n      )\n'"
data_engine/__init__.py,0,"b""__author__ = 'lvapeab'\n"""
data_engine/prepare_data.py,0,"b'import logging\nimport os\nfrom keras_wrapper.dataset import Dataset, saveDataset, loadDataset\n\nlogging.basicConfig(level=logging.INFO, format=\'[%(asctime)s] %(message)s\', datefmt=\'%d/%m/%Y %H:%M:%S\')\nlogger = logging.getLogger(__name__)\n\n\ndef update_dataset_from_file(ds,\n                             input_text_filename,\n                             params,\n                             splits=None,\n                             output_text_filename=None,\n                             remove_outputs=False,\n                             compute_state_below=False,\n                             recompute_references=False):\n    """"""\n    Updates the dataset instance from a text file according to the given params.\n    Used for sampling\n\n    :param ds: Dataset instance\n    :param input_text_filename: Source language sentences\n    :param params: Parameters for building the dataset\n    :param splits: Splits to sample\n    :param output_text_filename: Target language sentences\n    :param remove_outputs: Remove outputs from dataset (if True, will ignore the output_text_filename parameter)\n    :param compute_state_below: Compute state below input (shifted target text for professor teaching)\n    :param recompute_references: Whether we should rebuild the references of the dataset or not.\n\n    :return: Dataset object with the processed data\n    """"""\n    if splits is None:\n        splits = [\'val\']\n\n    if output_text_filename is None:\n        recompute_references = False\n\n    for split in splits:\n        if split == \'train\':\n            output_type = params.get(\'OUTPUTS_TYPES_DATASET\', [\'dense-text\'] if \'sparse\' in params[\'LOSS\'] else [\'text\'])[0]\n        else:\n            # Type of val/test outuput is always \'text\' or \'dense-text\'\n            output_type = \'dense-text\' if \'sparse\' in params[\'LOSS\'] else \'text\'\n\n        if remove_outputs:\n            ds.removeOutput(split,\n                            id=params[\'OUTPUTS_IDS_DATASET\'][0])\n            recompute_references = False\n\n        elif output_text_filename is not None:\n            ds.setOutput(output_text_filename,\n                         split,\n                         type=output_type,\n                         id=params[\'OUTPUTS_IDS_DATASET\'][0],\n                         tokenization=params.get(\'TOKENIZATION_METHOD\', \'tokenize_none\'),\n                         build_vocabulary=False,\n                         pad_on_batch=params.get(\'PAD_ON_BATCH\', True),\n                         fill=params.get(\'FILL\', \'end\'),\n                         sample_weights=params.get(\'SAMPLE_WEIGHTS\', True),\n                         max_text_len=params.get(\'MAX_OUTPUT_TEXT_LEN\', 100),\n                         max_words=params.get(\'OUTPUT_VOCABULARY_SIZE\', 0),\n                         min_occ=params.get(\'MIN_OCCURRENCES_OUTPUT_VOCAB\', 0),\n                         bpe_codes=params.get(\'BPE_CODES_PATH\', None),\n                         label_smoothing=params.get(\'LABEL_SMOOTHING\', 0.),\n                         overwrite_split=True)\n\n        # INPUT DATA\n        ds.setInput(input_text_filename,\n                    split,\n                    type=params.get(\'INPUTS_TYPES_DATASET\', [\'text\', \'text\'])[0],\n                    id=params[\'INPUTS_IDS_DATASET\'][0],\n                    tokenization=params.get(\'TOKENIZATION_METHOD\', \'tokenize_none\'),\n                    build_vocabulary=False,\n                    pad_on_batch=params.get(\'PAD_ON_BATCH\', True),\n                    fill=params.get(\'FILL\', \'end\'),\n                    max_text_len=params.get(\'MAX_INPUT_TEXT_LEN\', 100),\n                    max_words=params.get(\'INPUT_VOCABULARY_SIZE\', 0),\n                    min_occ=params.get(\'MIN_OCCURRENCES_INPUT_VOCAB\', 0),\n                    bpe_codes=params.get(\'BPE_CODES_PATH\', None),\n                    overwrite_split=True)\n\n        if compute_state_below and output_text_filename is not None:\n            # INPUT DATA\n            ds.setInput(output_text_filename,\n                        split,\n                        type=params.get(\'INPUTS_TYPES_DATASET\', [\'text\', \'text\'])[1],\n                        id=params[\'INPUTS_IDS_DATASET\'][1],\n                        pad_on_batch=params.get(\'PAD_ON_BATCH\', True),\n                        tokenization=params.get(\'TOKENIZATION_METHOD\', \'tokenize_none\'),\n                        build_vocabulary=False,\n                        offset=1,\n                        fill=params.get(\'FILL\', \'end\'),\n                        max_text_len=params.get(\'MAX_OUTPUT_TEXT_LEN\', 100),\n                        max_words=params.get(\'OUTPUT_VOCABULARY_SIZE\', 0),\n                        min_occ=params.get(\'MIN_OCCURRENCES_OUTPUT_VOCAB\', 0),\n                        bpe_codes=params.get(\'BPE_CODES_PATH\', None),\n                        overwrite_split=True)\n        else:\n            ds.setInput(None,\n                        split,\n                        type=\'ghost\',\n                        id=params[\'INPUTS_IDS_DATASET\'][-1],\n                        required=False,\n                        overwrite_split=True)\n\n        if params[\'ALIGN_FROM_RAW\']:\n            ds.setRawInput(input_text_filename,\n                           split,\n                           type=\'file-name\',\n                           id=\'raw_\' + params[\'INPUTS_IDS_DATASET\'][0],\n                           overwrite_split=True)\n\n        # If we had multiple references per sentence\n        if recompute_references:\n            prepare_references(ds, repeat=1, n=1, set_names=params[\'EVAL_ON_SETS\'])\n\n    return ds\n\n\ndef build_dataset(params):\n    """"""\n    Builds (or loads) a Dataset instance.\n    :param params: Parameters specifying Dataset options\n    :return: Dataset object\n    """"""\n\n    if params[\'REBUILD_DATASET\']:  # We build a new dataset instance\n        if params[\'VERBOSE\'] > 0:\n            silence = False\n            logger.info(\'Building \' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \' dataset\')\n        else:\n            silence = True\n\n        base_path = params[\'DATA_ROOT_PATH\']\n        name = params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\']\n        ds = Dataset(name, base_path, silence=silence)\n\n        # OUTPUT DATA\n        # Load the train, val and test splits of the target language sentences (outputs). The files include a sentence per line.\n        ds.setOutput(os.path.join(base_path, params[\'TEXT_FILES\'][\'train\'] + params[\'TRG_LAN\']),\n                     \'train\',\n                     type=params.get(\'OUTPUTS_TYPES_DATASET\',\n                                     [\'dense-text\'] if \'sparse\' in params[\'LOSS\'] else [\'text\'])[0],\n                     id=params[\'OUTPUTS_IDS_DATASET\'][0],\n                     tokenization=params.get(\'TOKENIZATION_METHOD\', \'tokenize_none\'),\n                     build_vocabulary=True,\n                     pad_on_batch=params.get(\'PAD_ON_BATCH\', True),\n                     sample_weights=params.get(\'SAMPLE_WEIGHTS\', True),\n                     fill=params.get(\'FILL\', \'end\'),\n                     max_text_len=params.get(\'MAX_OUTPUT_TEXT_LEN\', 70),\n                     max_words=params.get(\'OUTPUT_VOCABULARY_SIZE\', 0),\n                     min_occ=params.get(\'MIN_OCCURRENCES_OUTPUT_VOCAB\', 0),\n                     bpe_codes=params.get(\'BPE_CODES_PATH\', None),\n                     label_smoothing=params.get(\'LABEL_SMOOTHING\', 0.))\n\n        for split in [\'val\', \'test\']:\n            if params[\'TEXT_FILES\'].get(split) is not None:\n                ds.setOutput(os.path.join(base_path, params[\'TEXT_FILES\'][split] + params[\'TRG_LAN\']),\n                             split,\n                             type=\'text\',  # The type of the references should be always \'text\'\n                             id=params[\'OUTPUTS_IDS_DATASET\'][0],\n                             pad_on_batch=params.get(\'PAD_ON_BATCH\', True),\n                             tokenization=params.get(\'TOKENIZATION_METHOD\', \'tokenize_none\'),\n                             sample_weights=params.get(\'SAMPLE_WEIGHTS\', True),\n                             max_text_len=params.get(\'MAX_OUTPUT_TEXT_LEN\', 70),\n                             max_words=params.get(\'OUTPUT_VOCABULARY_SIZE\', 0),\n                             bpe_codes=params.get(\'BPE_CODES_PATH\', None),\n                             label_smoothing=0.)\n\n        # INPUT DATA\n        # We must ensure that the \'train\' split is the first (for building the vocabulary)\n        for split in params[\'TEXT_FILES\']:\n            build_vocabulary = split == \'train\'\n            ds.setInput(os.path.join(base_path, params[\'TEXT_FILES\'][split] + params[\'SRC_LAN\']),\n                        split,\n                        type=params.get(\'INPUTS_TYPES_DATASET\', [\'text\', \'text\'])[0],\n                        id=params[\'INPUTS_IDS_DATASET\'][0],\n                        pad_on_batch=params.get(\'PAD_ON_BATCH\', True),\n                        tokenization=params.get(\'TOKENIZATION_METHOD\', \'tokenize_none\'),\n                        build_vocabulary=build_vocabulary,\n                        fill=params.get(\'FILL\', \'end\'),\n                        max_text_len=params.get(\'MAX_INPUT_TEXT_LEN\', 70),\n                        max_words=params.get(\'INPUT_VOCABULARY_SIZE\', 0),\n                        min_occ=params.get(\'MIN_OCCURRENCES_INPUT_VOCAB\', 0),\n                        bpe_codes=params.get(\'BPE_CODES_PATH\', None))\n\n            if len(params[\'INPUTS_IDS_DATASET\']) > 1:\n                if \'train\' in split:\n                    ds.setInput(os.path.join(base_path, params[\'TEXT_FILES\'][split] + params[\'TRG_LAN\']),\n                                split,\n                                type=params.get(\'INPUTS_TYPES_DATASET\', [\'text\', \'text\'])[1],\n                                id=params[\'INPUTS_IDS_DATASET\'][1],\n                                required=False,\n                                tokenization=params.get(\'TOKENIZATION_METHOD\', \'tokenize_none\'),\n                                pad_on_batch=params.get(\'PAD_ON_BATCH\', True),\n                                build_vocabulary=params[\'OUTPUTS_IDS_DATASET\'][0],\n                                offset=1,\n                                fill=params.get(\'FILL\', \'end\'),\n                                max_text_len=params.get(\'MAX_OUTPUT_TEXT_LEN\', 70),\n                                max_words=params.get(\'OUTPUT_VOCABULARY_SIZE\', 0),\n                                bpe_codes=params.get(\'BPE_CODES_PATH\', None))\n                    if params.get(\'TIE_EMBEDDINGS\', False):\n                        ds.merge_vocabularies([params[\'INPUTS_IDS_DATASET\'][1], params[\'INPUTS_IDS_DATASET\'][0]])\n                else:\n                    ds.setInput(None,\n                                split,\n                                type=\'ghost\',\n                                id=params[\'INPUTS_IDS_DATASET\'][-1],\n                                required=False)\n            if params.get(\'ALIGN_FROM_RAW\', True) and not params.get(\'HOMOGENEOUS_BATCHES\', False):\n                ds.setRawInput(os.path.join(base_path, params[\'TEXT_FILES\'][split] + params[\'SRC_LAN\']),\n                               split,\n                               type=\'file-name\',\n                               id=\'raw_\' + params[\'INPUTS_IDS_DATASET\'][0])\n        if params.get(\'POS_UNK\', False):\n            if params.get(\'HEURISTIC\', 0) > 0:\n                ds.loadMapping(params[\'MAPPING\'])\n        # Prepare references\n        prepare_references(ds,\n                           repeat=1,\n                           n=1,\n                           set_names=params[\'EVAL_ON_SETS\'])\n\n        # We have finished loading the dataset, now we can store it for using it in the future\n        saveDataset(ds, params[\'DATASET_STORE_PATH\'])\n\n    else:\n        # We can easily recover it with a single line\n        ds = loadDataset(os.path.join(params[\'DATASET_STORE_PATH\'],\n                                      \'Dataset_\' + params[\'DATASET_NAME\'] +\n                                      \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\'))\n\n        # Prepare references\n        prepare_references(ds,\n                           repeat=1,\n                           n=1,\n                           set_names=params[\'EVAL_ON_SETS\'])\n\n    return ds\n\n\ndef prepare_references(ds, repeat, n=1, set_names=None):\n    """"""\n    Keeps only n captions per image and stores the rest in dictionaries for a later evaluation\n    :param ds: Dataset object\n    :param repeat: Number of input samples per output\n    :param n: Number of outputs to keep.\n    :param set_names: Set name.\n    :return:\n    """"""\n\n    if set_names is None:\n        set_names = [\'val\', \'test\']\n    for s in set_names:\n        logger.info(\'Keeping \' + str(n) + \' captions per input on the \' + str(s) + \' set.\')\n\n        ds.extra_variables[s] = dict()\n        n_samples = getattr(ds, \'len_\' + s)\n        # Process inputs\n        for id_in in ds.ids_inputs:\n            new_X = []\n            if id_in in ds.optional_inputs:\n                try:\n                    X = getattr(ds, \'X_\' + s)\n                    for i in range(0, n_samples, repeat):\n                        for j in range(n):\n                            new_X.append(X[id_in][i + j])\n                    setattr(ds, \'X_\' + s + \'[\' + id_in + \']\', new_X)\n                except Exception:\n                    pass\n            else:\n                X = getattr(ds, \'X_\' + s)\n                for i in range(0, n_samples, repeat):\n                    for j in range(n):\n                        new_X.append(X[id_in][i + j])\n                aux_list = getattr(ds, \'X_\' + s)\n                aux_list[id_in] = new_X\n                setattr(ds, \'X_\' + s, aux_list)\n                del aux_list\n        # Process outputs\n        for id_out in ds.ids_outputs:\n            new_Y = []\n            Y = getattr(ds, \'Y_\' + s)\n            dict_Y = dict()\n            count_samples = 0\n            for i in range(0, n_samples, repeat):\n                dict_Y[count_samples] = []\n                for j in range(repeat):\n                    if j < n:\n                        new_Y.append(Y[id_out][i + j])\n                    dict_Y[count_samples].append(Y[id_out][i + j])\n                count_samples += 1\n\n            aux_list = getattr(ds, \'Y_\' + s)\n            aux_list[id_out] = new_Y\n            setattr(ds, \'Y_\' + s, aux_list)\n            del aux_list\n\n            # store dictionary with img_pos -> [cap1, cap2, cap3, ..., capN]\n            ds.extra_variables[s][id_out] = dict_Y\n\n        new_len = len(new_Y)\n        setattr(ds, \'len_\' + s, new_len)\n\n        logger.info(\'Samples reduced to \' + str(new_len) + \' in \' + s + \' set.\')\n\n# Backwards compatibility:\nkeep_n_captions = prepare_references\n'"
data_engine/rebuild_dataset_from_config.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\n\nimport argparse\nimport logging\nimport ast\nfrom prepare_data import build_dataset\nfrom keras_wrapper.extra.read_write import pkl2dict\n\nlogging.basicConfig(level=logging.INFO, format=\'[%(asctime)s] %(message)s\', datefmt=\'%d/%m/%Y %H:%M:%S\')\nlogger = logging.getLogger(__name__)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(""Rebuilds a dataset object from a given config instance."")\n    parser.add_argument(""-c"", ""--config"", required=False, help=""Config pkl for loading the model configuration. ""\n                                                               ""If not specified, hyperparameters ""\n                                                               ""are read from config.py"")\n    parser.add_argument(""-ch"", ""--changes"", nargs=""*"", help=""Changes to the config. Following the syntax Key=Value"",\n                        default="""")\n    return parser.parse_args()\n\nif __name__ == ""__main__"":\n\n    args = parse_args()\n    if args.config is None:\n        logger.info(""Reading parameters from config.py"")\n        from config import load_parameters\n        params = load_parameters()\n    else:\n        logger.info(""Loading parameters from %s"" % str(args.config))\n        params = pkl2dict(args.config)\n    try:\n        for arg in args.changes:\n            try:\n                k, v = arg.split(\'=\')\n            except ValueError:\n                print (\'Overwritten arguments must have the form key=Value. \\n Currently are: %s\' % str(args.changes))\n                exit(1)\n            try:\n                params[k] = ast.literal_eval(v)\n            except ValueError:\n                params[k] = v\n    except ValueError:\n        print (\'Error processing arguments: (\', k, "","", v, "")"")\n        exit(2)\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n'"
demo-web/__init__.py,0,b''
demo-web/config_online.py,0,"b'\ndef load_parameters():\n    """"""\n    Loads the defined hyperparameters\n    :return parameters: Dictionary of loaded parameters\n    """"""\n    # Optimizer parameters (see model.compile() function)\n    CLASSIFIER_ACTIVATION = \'softmax\'\n    LOSS = \'categorical_crossentropy\'\n    OPTIMIZER = \'adadelta\'                        # Optimizer\n    LR = 0.1                                      # Learning rate.\n    # PAS-like params\n    C = 0.1                                       # Constant parameter for PAS and PPAS optimizer.\n    D = 0.1                                       # Constant parameter for PAS and PPAS optimizer.\n    K = 1                                         # Number of iterations to perform per sample\n    USE_CUSTOM_LOSS = False if \'categorical_crossentropy\' in LOSS else True\n    N_BEST_OPTIMIZER = False                      # Use N-Best list-based optimization\n    OPTIMIZER_REGULARIZER = \'\'                    # Metric to optimize (BLEU or TER)\n\n    # General params\n    CLIP_C = 5.                                   # During training, clip gradients to this norm\n    LR_DECAY = None                               # Minimum number of epochs before the next LR decay. Set to None if don\'t want to decay the learning rate\n    LR_GAMMA = 0.8                                # Multiplier used for decreasing the LR\n\n    # Training parameters\n    MAX_EPOCH = 1                                 # Stop when computed this number of epochs\n\n    EPOCHS_FOR_SAVE = 1                           # Number of epochs between model saves\n    WRITE_VALID_SAMPLES = True                    # Write valid samples in file\n    SAVE_EACH_EVALUATION = True                   # Save each time we evaluate the model\n\n    # Early stop parameters\n    EARLY_STOP = False                            # Turns on/off the early stop protocol\n\n    # Model parameters\n    MODEL_TYPE = \'AttentionRNNEncoderDecoder\'     # Model to train. See model_zoo() for the supported architectures\n    NOISE_AMOUNT = 0.0                            # Amount of noise\n\n    STORE_PATH = \'trained_models/retrained_model2/\'  # Models and evaluation results will be stored here\n\n    # ================================================ #\n    parameters = locals().copy()\n    return parameters\n'"
demo-web/sample_server.py,0,"b'# -*- coding: utf-8 -*-\n# !/usr/bin/env python\n\nfrom __future__ import print_function\n\ntry:\n    import itertools.imap as map\nexcept ImportError:\n    pass\nimport argparse\nimport ast\nimport logging\nimport time\nimport sys\nimport os\nimport copy\nfrom http.server import HTTPServer, BaseHTTPRequestHandler\nimport urllib\nfrom collections import OrderedDict\nsys.path.append(os.path.dirname(os.path.realpath(__file__)) + \'/../\')\nfrom keras_wrapper.model_ensemble import InteractiveBeamSearchSampler\nfrom keras_wrapper.cnn_model import loadModel, updateModel\nfrom keras_wrapper.dataset import loadDataset\nfrom keras_wrapper.extra.isles_utils import *\nfrom keras_wrapper.extra.read_write import pkl2dict, list2file\nfrom keras_wrapper.online_trainer import OnlineTrainer\nfrom keras_wrapper.utils import decode_predictions_beam_search, flatten_list_of_lists\nfrom nmt_keras.model_zoo import TranslationModel\n# from online_models import build_online_models\nfrom utils.utils import update_parameters\nfrom config_online import load_parameters as load_parameters_online\nfrom config import load_parameters\nlogger = logging.getLogger(__name__)\n\n\nclass NMTHandler(BaseHTTPRequestHandler):\n    def do_GET(self):\n        do_GET_start_time = time.time()\n        args = self.path.split(\'?\')[1]\n        args = args.split(\'&\')\n        source_sentence = None\n        validated_prefix = None\n        learn = False\n        beam_size = 6\n        length_norm = 0.\n        coverage_norm = 0.\n        alpha_norm = 1.\n        args_processing_start_time = time.time()\n        print (args)\n        for aa in args:\n            cc = aa.split(\'=\')\n            if cc[0] == \'source\':\n                source_sentence = urllib.parse.unquote_plus(cc[1])\n\n            if cc[0] == \'prefix\':\n                validated_prefix = cc[1]\n                validated_prefix = urllib.parse.unquote_plus(validated_prefix)\n\n            if cc[0] == \'learn\':\n                learn = cc[1]\n                learn = urllib.parse.unquote_plus(learn)\n                learn = eval(learn)\n\n            if cc[0] == \'beam_size\':\n                beam_size = cc[1]\n                beam_size = urllib.parse.unquote_plus(beam_size)\n                beam_size = int(beam_size)\n                self.server.sampler.params_prediction[\'beam_size\'] = beam_size\n\n            if cc[0] == \'length_norm\':\n                length_norm = cc[1]\n                length_norm = urllib.parse.unquote_plus(length_norm)\n                length_norm = float(length_norm)\n                self.server.sampler.params_prediction[\'length_norm_factor\'] = length_norm\n\n            if cc[0] == \'coverage_norm\':\n                coverage_norm = cc[1]\n                coverage_norm = urllib.parse.unquote_plus(coverage_norm)\n                coverage_norm = float(coverage_norm)\n                self.server.sampler.params_prediction[\'coverage_norm_factor\'] = coverage_norm\n\n            if cc[0] == \'alpha_norm\':\n                alpha_norm = cc[1]\n                alpha_norm = urllib.parse.unquote_plus(alpha_norm)\n                alpha_norm = float(alpha_norm)\n                self.server.sampler.params_prediction[\'alpha_factor\'] = alpha_norm\n\n        if source_sentence is None:\n            self.send_response(400)  # 400: (\'Bad Request\', \'Bad request syntax or unsupported method\')\n            return\n        source_sentence = urllib.parse.unquote_plus(source_sentence)\n        args_processing_end_time = time.time()\n        logger.log(2, \'args_processing time: %.6f\' % (args_processing_end_time - args_processing_start_time))\n\n        generate_sample_start_time = time.time()\n        if learn and validated_prefix is not None and source_sentence is not None:\n            self.server.sampler.learn_from_sample(source_sentence, validated_prefix)\n            self.send_response(200)  # 200: (\'OK\', \'Request fulfilled, document follows\')\n        else:\n            hypothesis = self.server.sampler.generate_sample(source_sentence, validated_prefix=validated_prefix)\n            response = hypothesis + u\'\\n\'\n            generate_sample_end_time = time.time()\n            logger.log(2, \'args_processing time: %.6f\' % (generate_sample_end_time - generate_sample_start_time))\n            send_response_start_time = time.time()\n            self.send_response(200)  # 200: (\'OK\', \'Request fulfilled, document follows\')\n            self.send_header(""Content-type"", ""text/html"")\n            self.end_headers()\n            self.wfile.write(response.encode(\'utf-8\'))\n            send_response_end_time = time.time()\n            logger.log(2, \'send_response time: %.6f\' % (send_response_end_time - send_response_start_time))\n            do_GET_end_time = time.time()\n            logger.log(2, \'do_GET time: %.6f\' % (do_GET_end_time - do_GET_start_time))\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(""Interactive neural machine translation server."")\n    parser.add_argument(""-ds"", ""--dataset"", required=True, help=""Dataset instance"")\n    parser.add_argument(""-v"", ""--verbose"", required=False, default=0, type=int, help=""Verbosity level"")\n    parser.add_argument(""-c"", ""--config"", required=False, help=""Config pkl for loading the model configuration. ""\n                                                               ""If not specified, hyperparameters ""\n                                                               ""are read from config.py"")\n    parser.add_argument(""-m"", ""--models"", nargs=""+"", required=True, help=""Path to the models"")\n    parser.add_argument(""-ch"", ""--changes"", nargs=""*"", help=""Changes to the config. Following the syntax Key=Value"",\n                        default="""")\n    parser.add_argument(""-o"", ""--online"",\n                        action=\'store_true\', default=False, required=False,\n                        help=""Online training mode after postedition. "")\n    parser.add_argument(""-a"", ""--address"", help=""Server address"", type=str, default=\'\')\n    parser.add_argument(""-p"", ""--port"", help=""Port to use"", type=int, default=6542)\n    parser.add_argument(""-l"", ""--logging-level"", help=""Logging level: \\t 0: Only info messages.""\n                                                      ""\\t 1: Debug messages.""\n                                                      ""\\t 2: Time monitoring messages."", type=int, default=0)\n    parser.add_argument(""-eos"", ""--eos-symbol"", help=""End-of-sentence symbol"", type=str, default=\'/\')\n\n    return parser.parse_args()\n\n\nclass NMTSampler:\n    def __init__(self, models, dataset, params, params_prediction, params_training, model_tokenize_f, model_detokenize_f, general_tokenize_f,\n                 general_detokenize_f, mapping=None, word2index_x=None, word2index_y=None, index2word_y=None,\n                 excluded_words=None, unk_id=1, eos_symbol=\'/\', online=False, verbose=0):\n        """"""\n        Builds an NMTSampler: An object containing models and dataset, for the interactive-predictive and adaptive framework.\n        :param models:\n        :param dataset:\n        :param dict params: All hyperparameters of the model.\n        :param dict params_prediction: Hyperparameters regarding prediction and search.\n        :param dict params_training:  Hyperparamters regarding incremental training.\n        :param function model_tokenize_f: Function used for tokenizing the input sentence. E.g. BPE.\n        :param function model_detokenize_f: Function used for detokenizing the output sentence. E.g. BPE revert.\n        :param function general_tokenize_f: Function used for tokenizing the input sentence. E.g. Moses tokenizer.\n        :param function general_detokenize_f: Function used for detokenizing the output sentence. E.g. Moses detokenizer.\n        :param dict mapping: Source-target dictionary (for unk_replace heuristics 1 and 2).\n        :param dict word2index_x: Mapping from word strings into indices for the source language.\n        :param dict word2index_y: Mapping from word strings into indices for the target language.\n        :param dict index2word_y: Mapping from indices into word strings for the target language.\n        :param dict excluded_words: words that won\'t be generated in the middle of two isles. Currenly unused.\n        :param int unk_id: Unknown word index.\n        :param str eos_symbol: End-of-sentence symbol.\n        :param bool online: Whether apply online learning after accepting each hypothesis.\n        :param int verbose: Verbosity level.\n        """"""\n\n        self.models = models\n        self.dataset = dataset\n        self.params = params\n        self.params_prediction = params_prediction\n        self.params_training = params_training\n        self.model_tokenize_f = model_tokenize_f\n        self.model_detokenize_f = model_detokenize_f\n        self.general_tokenize_f = general_tokenize_f\n        self.general_detokenize_f = general_detokenize_f\n        self.mapping = mapping\n        self.excluded_words = excluded_words\n        self.verbose = verbose\n        self.eos_symbol = eos_symbol\n        self.word2index_x = word2index_x if word2index_x is not None else \\\n            dataset.vocabulary[params_prediction[\'INPUTS_IDS_DATASET\'][0]][\'words2idx\']\n        self.index2word_y = index2word_y if index2word_y is not None else \\\n            dataset.vocabulary[params_prediction[\'OUTPUTS_IDS_DATASET\'][0]][\'idx2words\']\n        self.word2index_y = word2index_y if word2index_y is not None else \\\n            dataset.vocabulary[params_prediction[\'OUTPUTS_IDS_DATASET\'][0]][\'words2idx\']\n        self.unk_id = unk_id\n\n        self.interactive_beam_searcher = InteractiveBeamSearchSampler(self.models,\n                                                                      self.dataset,\n                                                                      self.params_prediction,\n                                                                      excluded_words=self.excluded_words,\n                                                                      verbose=self.verbose)\n\n        # Compile sampling function by generating a fake sample.\n        # TODO: Find a better way of doing this\n        logger.info(\'Compiling sampler...\')\n        self.generate_sample(\'i\')\n        logger.info(\'Done.\')\n\n        self.online = online\n        if self.online:\n            self.online_trainer = OnlineTrainer(self.models, self.dataset, None,  # Sampler\n                                                None,  # Params prediction\n                                                params_training,\n                                                verbose=self.verbose)\n            for i, nmt_model in enumerate(self.models):\n                logger.info(\'Compiling model %d...\' % i)\n                nmt_model.model._make_train_function()\n            logger.info(\'Done.\')\n\n        else:\n            self.online_trainer = None\n\n    def generate_sample(self, source_sentence, validated_prefix=None, max_N=5, isle_indices=None,\n                        filtered_idx2word=None, unk_indices=None, unk_words=None):\n        """"""\n        Generate sample via constrained search. Options labeled with <<isles>> are untested\n        and likely require some modifications to correctly work.\n        :param source_sentence: Source sentence.\n        :param validated_prefix: Prefix to keep in the output.\n        :param max_N: Maximum number of words to generate between validated segments. <<isles>>\n        :param isle_indices: Indices of the validated segments. <<isles>>\n        :param filtered_idx2word: List of candidate words to be the next one to generate (after generating fixed_words).\n        :param unk_indices: Positions of the unknown words.\n        :param unk_words: Unknown words.\n        :return:\n        """"""\n        logger.log(2, \'Beam size: %d\' % (self.params_prediction[\'beam_size\']))\n        generate_sample_start_time = time.time()\n        if unk_indices is None:\n            unk_indices = []\n        if unk_words is None:\n            unk_words = []\n\n        tokenization_start_time = time.time()\n        tokenized_input = self.general_tokenize_f(source_sentence, escape=False)\n        tokenized_input = self.model_tokenize_f(tokenized_input)\n        tokenization_end_time = time.time()\n        logger.log(2, \'tokenization time: %.6f\' % (tokenization_end_time - tokenization_start_time))\n        parse_input_start_time = time.time()\n        # Go from text to indices\n        src_seq = self.dataset.loadText([tokenized_input],\n                                        vocabularies=self.dataset.vocabulary[self.params[\'INPUTS_IDS_DATASET\'][0]],\n                                        max_len=self.params[\'MAX_INPUT_TEXT_LEN\'],\n                                        offset=0,\n                                        fill=self.dataset.fill_text[self.params[\'INPUTS_IDS_DATASET\'][0]],\n                                        pad_on_batch=self.dataset.pad_on_batch[self.params[\'INPUTS_IDS_DATASET\'][0]],\n                                        words_so_far=False,\n                                        loading_X=True)[0][0]\n\n        parse_input_end_time = time.time()\n        logger.log(2, \'parse_input time: %.6f\' % (parse_input_end_time - parse_input_start_time))\n\n        fixed_words_user = OrderedDict()\n        unk_words_dict = OrderedDict()\n        # If the user provided some feedback...\n        if validated_prefix is not None:\n            next_correction = validated_prefix[-1]\n            if next_correction == self.eos_symbol:\n                return validated_prefix[:-1].decode(\'utf-8\')\n\n            # 2.2.4 Tokenize the prefix properly (possibly applying BPE)\n            #  TODO: Here we are tokenizing the target language with the source language tokenizer\n            prefix_tokenization_start_time = time.time()\n            tokenized_validated_prefix = self.general_tokenize_f(validated_prefix, escape=False)\n            tokenized_validated_prefix = self.model_tokenize_f(tokenized_validated_prefix)\n            prefix_tokenization_end_time = time.time()\n            logger.log(2, \'prefix_tokenization time: %.6f\' % (prefix_tokenization_end_time - prefix_tokenization_start_time))\n\n            # 2.2.5 Validate words\n            word_validation_start_time = time.time()\n            for pos, word in enumerate(tokenized_validated_prefix.split()):\n                fixed_words_user[pos] = self.word2index_y.get(word, self.unk_id)\n                if self.word2index_y.get(word) is None:\n                    unk_words_dict[pos] = word\n            word_validation_end_time = time.time()\n            logger.log(2, \'word_validation time: %.6f\' % (word_validation_end_time - word_validation_start_time))\n\n            # 2.2.6 Constrain search for the last word\n            constrain_search_start_time = time.time()\n            last_user_word_pos = list(fixed_words_user.keys())[-1]\n            if next_correction != u\' \':\n                last_user_word = tokenized_validated_prefix.split()[-1]\n                filtered_idx2word = dict((self.word2index_y[candidate_word], candidate_word)\n                                         for candidate_word in self.word2index_y if candidate_word[:len(last_user_word)] == last_user_word)\n\n                if filtered_idx2word != dict():\n                    del fixed_words_user[last_user_word_pos]\n                    if last_user_word_pos in list(unk_words_dict.keys()):\n                        del unk_words_dict[last_user_word_pos]\n            else:\n                filtered_idx2word = dict()\n            constrain_search_end_time = time.time()\n            logger.log(2, \'constrain_search_end_time time: %.6f\' % (constrain_search_end_time - constrain_search_start_time))\n\n        sample_beam_search_start_time = time.time()\n        trans_indices, costs, alphas = \\\n            self.interactive_beam_searcher.sample_beam_search_interactive(src_seq,\n                                                                          fixed_words=copy.copy(fixed_words_user),\n                                                                          max_N=max_N,\n                                                                          isles=isle_indices,\n                                                                          valid_next_words=filtered_idx2word,\n                                                                          idx2word=self.index2word_y)\n        sample_beam_search_end_time = time.time()\n        logger.log(2, \'sample_beam_search time: %.6f\' % (sample_beam_search_end_time - sample_beam_search_start_time))\n\n        if False and self.params_prediction[\'pos_unk\']:\n            alphas = [alphas]\n            sources = [tokenized_input]\n            heuristic = self.params_prediction[\'heuristic\']\n        else:\n            alphas = None\n            heuristic = None\n            sources = None\n\n        # 1.2 Decode hypothesis\n        decoding_predictions_start_time = time.time()\n        hypothesis = decode_predictions_beam_search([trans_indices],\n                                                    self.index2word_y,\n                                                    alphas=alphas,\n                                                    x_text=sources,\n                                                    heuristic=heuristic,\n                                                    mapping=self.mapping,\n                                                    pad_sequences=True,\n                                                    verbose=0)[0]\n        decoding_predictions_end_time = time.time()\n        logger.log(2, \'decoding_predictions time: %.6f\' % (decoding_predictions_end_time - decoding_predictions_start_time))\n\n        # UNK words management\n        unk_management_start_time = time.time()\n        unk_indices = list(unk_words_dict)\n        unk_words = list(unk_words_dict.values())\n        if len(unk_indices) > 0:  # If we added some UNK word\n            hypothesis = hypothesis.split()\n            if len(hypothesis) < len(unk_indices):  # The full hypothesis will be made up UNK words:\n                for i, index in enumerate(range(0, len(hypothesis))):\n                    hypothesis[index] = unk_words[unk_indices[i]]\n                for ii in range(i + 1, len(unk_words)):\n                    hypothesis.append(unk_words[ii])\n            else:  # We put each unknown word in the corresponding gap\n                for i, index in enumerate(unk_indices):\n                    if index < len(hypothesis):\n                        hypothesis[index] = unk_words[i]\n                    else:\n                        hypothesis.append(unk_words[i])\n            hypothesis = u\' \'.join(hypothesis)\n        unk_management_end_time = time.time()\n        logger.log(2, \'unk_management time: %.6f\' % (unk_management_end_time - unk_management_start_time))\n\n        hypothesis_detokenization_start_time = time.time()\n        hypothesis = self.model_detokenize_f(hypothesis)\n        hypothesis = self.general_detokenize_f(hypothesis, unescape=False)\n        hypothesis_detokenization_end_time = time.time()\n        logger.log(2, \'hypothesis_detokenization time: %.6f\' % (hypothesis_detokenization_end_time - hypothesis_detokenization_start_time))\n        generate_sample_end_time = time.time()\n        logger.log(2, \'generate_sample time: %.6f\' % (generate_sample_end_time - generate_sample_start_time))\n        return hypothesis\n\n    def learn_from_sample(self, source_sentence, target_sentence):\n        """"""\n        Incrementally adapt the model with the validated sample.\n        :param source_sentence: Source sentence (x).\n        :param target_sentence: Target sentence (y).\n        :return:\n        """"""\n        # Tokenize input\n        tokenized_input = self.general_tokenize_f(source_sentence, escape=False)\n        tokenized_input = self.model_tokenize_f(tokenized_input)\n        src_seq = self.dataset.loadText([tokenized_input],\n                                        vocabularies=self.dataset.vocabulary[self.params[\'INPUTS_IDS_DATASET\'][0]],\n                                        max_len=self.params[\'MAX_INPUT_TEXT_LEN\'],\n                                        offset=0,\n                                        fill=self.dataset.fill_text[self.params[\'INPUTS_IDS_DATASET\'][0]],\n                                        pad_on_batch=self.dataset.pad_on_batch[self.params[\'INPUTS_IDS_DATASET\'][0]],\n                                        words_so_far=False,\n                                        loading_X=True)[0][0]\n        # Tokenize output\n        tokenized_reference = self.general_tokenize_f(target_sentence, escape=False)\n        tokenized_reference = self.model_tokenize_f(tokenized_reference)\n\n        # Build inputs/outpus of the system\n        state_below = self.dataset.loadText([tokenized_reference],\n                                            vocabularies=self.dataset.vocabulary[self.params[\'OUTPUTS_IDS_DATASET\'][0]],\n                                            max_len=self.params[\'MAX_OUTPUT_TEXT_LEN_TEST\'],\n                                            offset=1,\n                                            fill=self.dataset.fill_text[self.params[\'INPUTS_IDS_DATASET\'][-1]],\n                                            pad_on_batch=self.dataset.pad_on_batch[self.params[\'INPUTS_IDS_DATASET\'][-1]],\n                                            words_so_far=False,\n                                            loading_X=True)[0]\n\n        # 4.1.3 Ground truth sample -> Interactively translated sentence\n        # TODO: Load dense-text if necessary\n        trg_seq = self.dataset.loadTextOneHot([tokenized_reference],\n                                              vocabularies=self.dataset.vocabulary[self.params[\'OUTPUTS_IDS_DATASET\'][0]],\n                                              vocabulary_len=self.dataset.vocabulary_len[self.params[\'OUTPUTS_IDS_DATASET\'][0]],\n                                              max_len=self.params[\'MAX_OUTPUT_TEXT_LEN_TEST\'],\n                                              offset=0,\n                                              fill=self.dataset.fill_text[self.params[\'OUTPUTS_IDS_DATASET\'][0]],\n                                              pad_on_batch=self.dataset.pad_on_batch[self.params[\'OUTPUTS_IDS_DATASET\'][0]],\n                                              words_so_far=False,\n                                              sample_weights=self.params[\'SAMPLE_WEIGHTS\'],\n                                              loading_X=False)\n        # 4.2 Train online!\n        if self.online_trainer is not None:\n            self.online_trainer.train_online([np.asarray([src_seq]), state_below], trg_seq, trg_words=[target_sentence])\n        else:\n            logger.warning(\'Online learning is disabled.\')\n\n\ndef main():\n    args = parse_args()\n    server_address = (args.address, args.port)\n    httpd = HTTPServer(server_address, NMTHandler)\n    logger.setLevel(args.logging_level)\n    parameters = load_parameters()\n    if args.config is not None:\n        logger.info(""Loading parameters from %s"" % str(args.config))\n        parameters = update_parameters(parameters, pkl2dict(args.config))\n\n    if args.online:\n        online_parameters = load_parameters_online()\n        parameters = update_parameters(parameters, online_parameters)\n\n    try:\n        for arg in args.changes:\n            try:\n                k, v = arg.split(\'=\')\n            except ValueError:\n                print(\'Overwritten arguments must have the form key=Value. \\n Currently are: %s\' % str(args.changes))\n                exit(1)\n            try:\n                parameters[k] = ast.literal_eval(v)\n            except ValueError:\n                parameters[k] = v\n    except ValueError:\n        print(\'Error processing arguments: (\', k, "","", v, "")"")\n        exit(2)\n    dataset = loadDataset(args.dataset)\n\n    # For converting predictions into sentences\n    # Dataset backwards compatibility\n    bpe_separator = dataset.BPE_separator if hasattr(dataset,\n                                                     ""BPE_separator"") and dataset.BPE_separator is not None else \'@@\'\n    # Build BPE tokenizer if necessary\n    if \'bpe\' in parameters[\'TOKENIZATION_METHOD\'].lower():\n        logger.info(\'Building BPE\')\n        if not dataset.BPE_built:\n            dataset.build_bpe(parameters.get(\'BPE_CODES_PATH\', parameters[\'DATA_ROOT_PATH\'] + \'/training_codes.joint\'),\n                              separator=bpe_separator)\n    # Build tokenization function\n    tokenize_f = eval(\'dataset.\' + parameters.get(\'TOKENIZATION_METHOD\', \'tokenize_bpe\'))\n    detokenize_function = eval(\'dataset.\' + parameters.get(\'DETOKENIZATION_METHOD\', \'detokenize_bpe\'))\n    dataset.build_moses_tokenizer(language=parameters[\'SRC_LAN\'])\n    dataset.build_moses_detokenizer(language=parameters[\'TRG_LAN\'])\n    tokenize_general = dataset.tokenize_moses\n    detokenize_general = dataset.detokenize_moses\n\n    # Prediction parameters\n    params_prediction = dict()\n    params_prediction[\'max_batch_size\'] = parameters.get(\'BATCH_SIZE\', 20)\n    params_prediction[\'n_parallel_loaders\'] = parameters.get(\'PARALLEL_LOADERS\', 1)\n    params_prediction[\'beam_size\'] = parameters.get(\'BEAM_SIZE\', 6)\n    params_prediction[\'maxlen\'] = parameters.get(\'MAX_OUTPUT_TEXT_LEN_TEST\', 100)\n    params_prediction[\'optimized_search\'] = parameters[\'OPTIMIZED_SEARCH\']\n    params_prediction[\'model_inputs\'] = parameters[\'INPUTS_IDS_MODEL\']\n    params_prediction[\'model_outputs\'] = parameters[\'OUTPUTS_IDS_MODEL\']\n    params_prediction[\'dataset_inputs\'] = parameters[\'INPUTS_IDS_DATASET\']\n    params_prediction[\'dataset_outputs\'] = parameters[\'OUTPUTS_IDS_DATASET\']\n    params_prediction[\'search_pruning\'] = parameters.get(\'SEARCH_PRUNING\', False)\n    params_prediction[\'normalize_probs\'] = True\n    params_prediction[\'alpha_factor\'] = parameters.get(\'ALPHA_FACTOR\', 1.0)\n    params_prediction[\'coverage_penalty\'] = True\n    params_prediction[\'length_penalty\'] = True\n    params_prediction[\'length_norm_factor\'] = parameters.get(\'LENGTH_NORM_FACTOR\', 0.0)\n    params_prediction[\'coverage_norm_factor\'] = parameters.get(\'COVERAGE_NORM_FACTOR\', 0.0)\n    params_prediction[\'pos_unk\'] = parameters.get(\'POS_UNK\', False)\n    params_prediction[\'heuristic\'] = parameters.get(\'HEURISTIC\', 0)\n    params_prediction[\'state_below_index\'] = -1\n    params_prediction[\'output_text_index\'] = 0\n    params_prediction[\'state_below_maxlen\'] = -1 if parameters.get(\'PAD_ON_BATCH\', True) else parameters.get(\'MAX_OUTPUT_TEXT_LEN\', 50)\n    params_prediction[\'output_max_length_depending_on_x\'] = parameters.get(\'MAXLEN_GIVEN_X\', True)\n    params_prediction[\'output_max_length_depending_on_x_factor\'] = parameters.get(\'MAXLEN_GIVEN_X_FACTOR\', 3)\n    params_prediction[\'output_min_length_depending_on_x\'] = parameters.get(\'MINLEN_GIVEN_X\', True)\n    params_prediction[\'output_min_length_depending_on_x_factor\'] = parameters.get(\'MINLEN_GIVEN_X_FACTOR\', 2)\n    params_prediction[\'attend_on_output\'] = parameters.get(\'ATTEND_ON_OUTPUT\', \'transformer\' in parameters[\'MODEL_TYPE\'].lower())\n\n    # Manage pos_unk strategies\n    if parameters[\'POS_UNK\']:\n        mapping = None if dataset.mapping == dict() else dataset.mapping\n    else:\n        mapping = None\n\n    if \'transformer\' in parameters[\'MODEL_TYPE\'].lower():\n        params_prediction[\'pos_unk\'] = False\n        params_prediction[\'coverage_penalty\'] = False\n\n    # Training parameters\n    parameters_training = dict()\n    if args.online:\n        logger.info(\'Loading models from %s\' % str(args.models))\n        parameters_training = {  # Traning parameters\n            \'n_epochs\': parameters[\'MAX_EPOCH\'],\n            \'shuffle\': False,\n            \'loss\': parameters.get(\'LOSS\', \'categorical_crossentropy\'),\n            \'batch_size\': parameters.get(\'BATCH_SIZE\', 1),\n            \'homogeneous_batches\': False,\n            \'optimizer\': parameters.get(\'OPTIMIZER\', \'SGD\'),\n            \'lr\': parameters.get(\'LR\', 0.1),\n            \'lr_decay\': parameters.get(\'LR_DECAY\', None),\n            \'lr_gamma\': parameters.get(\'LR_GAMMA\', 1.),\n            \'epochs_for_save\': -1,\n            \'verbose\': args.verbose,\n            \'eval_on_sets\': parameters.get(\'EVAL_ON_SETS_KERAS\', None),\n            \'n_parallel_loaders\': parameters[\'PARALLEL_LOADERS\'],\n            \'extra_callbacks\': [],  # callbacks,\n            \'reload_epoch\': parameters[\'RELOAD\'],\n            \'epoch_offset\': parameters[\'RELOAD\'],\n            \'data_augmentation\': parameters[\'DATA_AUGMENTATION\'],\n            \'patience\': parameters.get(\'PATIENCE\', 0),\n            \'metric_check\': parameters.get(\'STOP_METRIC\', None),\n            \'eval_on_epochs\': parameters.get(\'EVAL_EACH_EPOCHS\', True),\n            \'each_n_epochs\': parameters.get(\'EVAL_EACH\', 1),\n            \'start_eval_on_epoch\': parameters.get(\'START_EVAL_ON_EPOCH\', 0),\n            \'additional_training_settings\': {\'k\': parameters.get(\'K\', 1),\n                                             \'tau\': parameters.get(\'TAU\', 1),\n                                             \'lambda\': parameters.get(\'LAMBDA\', 0.5),\n                                             \'c\': parameters.get(\'C\', 0.5),\n                                             \'d\': parameters.get(\'D\', 0.5)\n                                             }\n        }\n        model_instances = [TranslationModel(parameters,\n                                            model_type=parameters[\'MODEL_TYPE\'],\n                                            verbose=parameters[\'VERBOSE\'],\n                                            model_name=parameters[\'MODEL_NAME\'] + \'_\' + str(i),\n                                            vocabularies=dataset.vocabulary,\n                                            store_path=parameters[\'STORE_PATH\'],\n                                            set_optimizer=False)\n                           for i in range(len(args.models))]\n        models = [updateModel(model, path, -1, full_path=True) for (model, path) in zip(model_instances, args.models)]\n    else:\n        models = [loadModel(m, -1, full_path=True) for m in args.models]\n\n    for nmt_model in models:\n        nmt_model.setParams(parameters)\n        nmt_model.setOptimizer()\n\n    parameters[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[parameters[\'INPUTS_IDS_DATASET\'][0]]\n    parameters[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[parameters[\'OUTPUTS_IDS_DATASET\'][0]]\n\n    # Get word2index and index2word dictionaries\n    index2word_y = dataset.vocabulary[parameters[\'OUTPUTS_IDS_DATASET\'][0]][\'idx2words\']\n    word2index_y = dataset.vocabulary[parameters[\'OUTPUTS_IDS_DATASET\'][0]][\'words2idx\']\n    index2word_x = dataset.vocabulary[parameters[\'INPUTS_IDS_DATASET\'][0]][\'idx2words\']\n    word2index_x = dataset.vocabulary[parameters[\'INPUTS_IDS_DATASET\'][0]][\'words2idx\']\n\n    excluded_words = None\n    interactive_beam_searcher = NMTSampler(models, dataset, parameters, params_prediction, parameters_training,\n                                           tokenize_f, detokenize_function,\n                                           tokenize_general, detokenize_general,\n                                           mapping=mapping, word2index_x=word2index_x, word2index_y=word2index_y,\n                                           index2word_y=index2word_y, eos_symbol=args.eos_symbol,\n                                           excluded_words=excluded_words, online=args.online, verbose=args.verbose)\n\n    httpd.sampler = interactive_beam_searcher\n\n    logger.info(\'Server starting at %s\' % str(server_address))\n    httpd.serve_forever()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
nmt_keras/__init__.py,0,"b'__author__ = \'lvapeab\'\n\nimport logging\n\nlogging.basicConfig(level=logging.INFO, format=\'[%(asctime)s] %(message)s\', datefmt=\'%d/%m/%Y %H:%M:%S\')\nlogger = logging.getLogger(__name__)\n\n\ndef check_params(params):\n    """"""\n    Checks some typical parameters and warns if something wrong was specified.\n    :param params: Model instance on which to apply the callback.\n    :return: None\n    """"""\n\n    if params[\'SRC_PRETRAINED_VECTORS\'] and params[\'SRC_PRETRAINED_VECTORS\'][-4:] != \'.npy\':\n        logger.warning(\'It seems that the pretrained word vectors provided for the target text are not in npy format.\'\n                       \'You should preprocess the word embeddings with the ""utils/preprocess_*_word_vectors.py script.\')\n\n    if params[\'TRG_PRETRAINED_VECTORS\'] and params[\'TRG_PRETRAINED_VECTORS\'][-4:] != \'.npy\':\n        logger.warning(\'It seems that the pretrained word vectors provided for the target text are not in npy format.\'\n                       \'You should preprocess the word embeddings with the ""utils/preprocess_*_word_vectors.py script.\')\n    if not params[\'PAD_ON_BATCH\']:\n        logger.warning(\'It is HIGHLY recommended to set the option ""PAD_ON_BATCH = True.""\')\n\n    if params[\'MODEL_TYPE\'].lower() == \'transformer\':\n\n        assert params[\'MODEL_SIZE\'] == params[\'TARGET_TEXT_EMBEDDING_SIZE\'], \'When using the Transformer model, \' \\\n                                                                             \'dimensions of ""MODEL_SIZE"" and ""TARGET_TEXT_EMBEDDING_SIZE"" must match. \' \\\n                                                                             \'Currently, they are: %d and %d, respectively.\' % (\n                                                                             params[\'MODEL_SIZE\'],\n                                                                             params[\'TARGET_TEXT_EMBEDDING_SIZE\'])\n        assert params[\'MODEL_SIZE\'] == params[\'SOURCE_TEXT_EMBEDDING_SIZE\'], \'When using the Transformer model, \' \\\n                                                                             \'dimensions of ""MODEL_SIZE"" and ""SOURCE_TEXT_EMBEDDING_SIZE"" must match. \' \\\n                                                                             \'Currently, they are: %d and %d, respectively.\' % (\n                                                                             params[\'MODEL_SIZE\'],\n                                                                             params[\'SOURCE_TEXT_EMBEDDING_SIZE\'])\n\n        if params[\'POS_UNK\']:\n            logger.warn(\'The ""POS_UNK"" option is still unimplemented for the ""Transformer"" model. Setting it to False.\')\n            params[\'POS_UNK\'] = False\n        assert params[\'MODEL_SIZE\'] % params[\'N_HEADS\'] == 0, \\\n            \'""MODEL_SIZE"" should be a multiple of ""N_HEADS"". \' \\\n            \'Currently: mod(%d, %d) == %d.\' % (\n            params[\'MODEL_SIZE\'], params[\'N_HEADS\'], params[\'MODEL_SIZE\'] % params[\'N_HEADS\'])\n\n    if params[\'POS_UNK\']:\n        if not params[\'OPTIMIZED_SEARCH\']:\n            logger.warn(\n                \'Unknown words replacement requires to use the optimized search (""OPTIMIZED_SEARCH"" parameter). Setting ""POS_UNK"" to False.\')\n            params[\'POS_UNK\'] = False\n\n    if params[\'COVERAGE_PENALTY\']:\n        assert params[\'OPTIMIZED_SEARCH\'], \'The application of ""COVERAGE_PENALTY"" requires \' \\\n                                           \'to use the optimized search (""OPTIMIZED_SEARCH"" parameter).\'\n\n    if \'from_logits\' in params.get(\'LOSS\', \'categorical_crossentropy\'):\n        if params.get(\'CLASSIFIER_ACTIVATION\', \'softmax\'):\n            params[\'CLASSIFIER_ACTIVATION\'] = None\n\n    if params.get(\'LABEL_SMOOTHING\', 0.) and \'sparse\' in params.get(\'LOSS\', \'categorical_crossentropy\'):\n        logger.warn(\'Label smoothing with sparse outputs is still unimplemented\')\n\n    if params.get(\'TRAIN_ONLY_LAST_LAYER\'):\n        logger.info(\'Training only last layer.\')\n        params[\'TRAINABLE_ENCODER\'] = False\n        params[\'TRAINABLE_DECODER\'] = False\n\n    return params\n'"
nmt_keras/apply_model.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\n\ntry:\n    import itertools.imap as map\nexcept ImportError:\n    pass\nimport logging\nfrom keras_wrapper.extra.read_write import list2file, nbest2file, list2stdout, numpy2file, pkl2dict\n\nlogging.basicConfig(level=logging.INFO, format=\'[%(asctime)s] %(message)s\', datefmt=\'%d/%m/%Y %H:%M:%S\')\nlogger = logging.getLogger(__name__)\n\n\ndef sample_ensemble(args, params):\n    """"""\n    Use several translation models for obtaining predictions from a source text file.\n\n    :param argparse.Namespace args: Arguments given to the method:\n\n                      * dataset: Dataset instance with data.\n                      * text: Text file with source sentences.\n                      * splits: Splits to sample. Should be already included in the dataset object.\n                      * dest: Output file to save scores.\n                      * weights: Weight given to each model in the ensemble. You should provide the same number of weights than models. By default, it applies the same weight to each model (1/N).\n                      * n_best: Write n-best list (n = beam size).\n                      * config: Config .pkl for loading the model configuration. If not specified, hyperparameters are read from config.py.\n                      * models: Path to the models.\n                      * verbose: Be verbose or not.\n\n    :param params: parameters of the translation model.\n    """"""\n    from data_engine.prepare_data import update_dataset_from_file\n    from keras_wrapper.model_ensemble import BeamSearchEnsemble\n    from keras_wrapper.cnn_model import loadModel\n    from keras_wrapper.dataset import loadDataset\n    from keras_wrapper.utils import decode_predictions_beam_search\n\n    logger.info(""Using an ensemble of %d models"" % len(args.models))\n    models = [loadModel(m, -1, full_path=True) for m in args.models]\n    dataset = loadDataset(args.dataset)\n    dataset = update_dataset_from_file(dataset, args.text, params, splits=args.splits, remove_outputs=True)\n\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    # For converting predictions into sentences\n    index2word_y = dataset.vocabulary[params[\'OUTPUTS_IDS_DATASET\'][0]][\'idx2words\']\n\n    if params.get(\'APPLY_DETOKENIZATION\', False):\n        detokenize_function = eval(\'dataset.\' + params[\'DETOKENIZATION_METHOD\'])\n\n    params_prediction = dict()\n    params_prediction[\'max_batch_size\'] = params.get(\'BATCH_SIZE\', 20)\n    params_prediction[\'n_parallel_loaders\'] = params.get(\'PARALLEL_LOADERS\', 1)\n    params_prediction[\'beam_size\'] = params.get(\'BEAM_SIZE\', 6)\n    params_prediction[\'maxlen\'] = params.get(\'MAX_OUTPUT_TEXT_LEN_TEST\', 100)\n    params_prediction[\'optimized_search\'] = params[\'OPTIMIZED_SEARCH\']\n    params_prediction[\'model_inputs\'] = params[\'INPUTS_IDS_MODEL\']\n    params_prediction[\'model_outputs\'] = params[\'OUTPUTS_IDS_MODEL\']\n    params_prediction[\'dataset_inputs\'] = params[\'INPUTS_IDS_DATASET\']\n    params_prediction[\'dataset_outputs\'] = params[\'OUTPUTS_IDS_DATASET\']\n    params_prediction[\'search_pruning\'] = params.get(\'SEARCH_PRUNING\', False)\n    params_prediction[\'normalize_probs\'] = params.get(\'NORMALIZE_SAMPLING\', False)\n    params_prediction[\'alpha_factor\'] = params.get(\'ALPHA_FACTOR\', 1.0)\n    params_prediction[\'coverage_penalty\'] = params.get(\'COVERAGE_PENALTY\', False)\n    params_prediction[\'length_penalty\'] = params.get(\'LENGTH_PENALTY\', False)\n    params_prediction[\'length_norm_factor\'] = params.get(\'LENGTH_NORM_FACTOR\', 0.0)\n    params_prediction[\'coverage_norm_factor\'] = params.get(\'COVERAGE_NORM_FACTOR\', 0.0)\n    params_prediction[\'pos_unk\'] = params.get(\'POS_UNK\', False)\n    params_prediction[\'state_below_maxlen\'] = -1 if params.get(\'PAD_ON_BATCH\', True) \\\n        else params.get(\'MAX_OUTPUT_TEXT_LEN\', 50)\n    params_prediction[\'output_max_length_depending_on_x\'] = params.get(\'MAXLEN_GIVEN_X\', True)\n    params_prediction[\'output_max_length_depending_on_x_factor\'] = params.get(\'MAXLEN_GIVEN_X_FACTOR\', 3)\n    params_prediction[\'output_min_length_depending_on_x\'] = params.get(\'MINLEN_GIVEN_X\', True)\n    params_prediction[\'output_min_length_depending_on_x_factor\'] = params.get(\'MINLEN_GIVEN_X_FACTOR\', 2)\n    params_prediction[\'attend_on_output\'] = params.get(\'ATTEND_ON_OUTPUT\',\n                                                       \'transformer\' in params[\'MODEL_TYPE\'].lower())\n    params_prediction[\'glossary\'] = params.get(\'GLOSSARY\', None)\n\n    heuristic = params.get(\'HEURISTIC\', 0)\n    mapping = None if dataset.mapping == dict() else dataset.mapping\n    model_weights = args.weights\n\n    if args.glossary is not None:\n        glossary = pkl2dict(args.glossary)\n    elif params_prediction[\'glossary\'] is not None:\n        glossary = pkl2dict(params_prediction[\'glossary\'])\n    else:\n        glossary = None\n\n    if model_weights:\n        assert len(model_weights) == len(\n            models), \'You should give a weight to each model. You gave %d models and %d weights.\' % (\n            len(models), len(model_weights))\n        model_weights = list(map(float, model_weights))\n        if len(model_weights) > 1:\n            logger.info(\'Giving the following weights to each model: %s\' % str(model_weights))\n\n    for s in args.splits:\n        # Apply model predictions\n        params_prediction[\'predict_on_sets\'] = [s]\n        beam_searcher = BeamSearchEnsemble(models,\n                                           dataset,\n                                           params_prediction,\n                                           model_weights=model_weights,\n                                           n_best=args.n_best,\n                                           verbose=args.verbose)\n        predictions = beam_searcher.predictBeamSearchNet()[s]\n        samples = predictions[\'samples\']\n        alphas = predictions[\'alphas\'] if params_prediction[\'pos_unk\'] else None\n\n        if params_prediction[\'pos_unk\']:\n            sources = [x.strip() for x in open(args.text, \'r\').read().split(\'\\n\')]\n            sources = sources[:-1] if len(sources[-1]) == 0 else sources\n        else:\n            sources = None\n\n        decoded_predictions = decode_predictions_beam_search(samples,\n                                                             index2word_y,\n                                                             glossary=glossary,\n                                                             alphas=alphas,\n                                                             x_text=sources,\n                                                             heuristic=heuristic,\n                                                             mapping=mapping,\n                                                             verbose=args.verbose)\n        # Apply detokenization function if needed\n        if params.get(\'APPLY_DETOKENIZATION\', False):\n            decoded_predictions = list(map(detokenize_function, decoded_predictions))\n\n        if args.n_best:\n            n_best_predictions = []\n            for i, (n_best_preds, n_best_scores, n_best_alphas) in enumerate(predictions[\'n_best\']):\n                n_best_sample_score = []\n                for n_best_pred, n_best_score, n_best_alpha in zip(n_best_preds, n_best_scores, n_best_alphas):\n                    pred = decode_predictions_beam_search([n_best_pred],\n                                                          index2word_y,\n                                                          glossary=glossary,\n                                                          alphas=[n_best_alpha] if params_prediction[\n                                                              \'pos_unk\'] else None,\n                                                          x_text=[sources[i]] if params_prediction[\'pos_unk\'] else None,\n                                                          heuristic=heuristic,\n                                                          mapping=mapping,\n                                                          verbose=args.verbose)\n                    # Apply detokenization function if needed\n                    if params.get(\'APPLY_DETOKENIZATION\', False):\n                        pred = list(map(detokenize_function, pred))\n\n                    n_best_sample_score.append([i, pred, n_best_score])\n                n_best_predictions.append(n_best_sample_score)\n        # Store result\n        if args.dest is not None:\n            filepath = args.dest  # results file\n            if params.get(\'SAMPLING_SAVE_MODE\', \'list\'):\n                list2file(filepath, decoded_predictions)\n                if args.n_best:\n                    nbest2file(filepath + \'.nbest\', n_best_predictions)\n            else:\n                raise Exception(\'Only ""list"" is allowed in ""SAMPLING_SAVE_MODE""\')\n        else:\n            list2stdout(decoded_predictions)\n            if args.n_best:\n                logger.info(\'Storing n-best sentences in ./\' + s + \'.nbest\')\n                nbest2file(\'./\' + s + \'.nbest\', n_best_predictions)\n        logger.info(\'Sampling finished\')\n\n\ndef score_corpus(args, params):\n    """"""\n    Use one or several translation models for scoring source--target pairs-\n\n    :param argparse.Namespace args: Arguments given to the method:\n\n                                * dataset: Dataset instance with data.\n                                * source: Text file with source sentences.\n                                * target: Text file with target sentences.\n                                * splits: Splits to sample. Should be already included in the dataset object.\n                                * dest: Output file to save scores.\n                                * weights: Weight given to each model in the ensemble. You should provide the same number of weights than models. By default, it applies the same weight to each model (1/N).\n                                * verbose: Be verbose or not.\n                                * config: Config .pkl for loading the model configuration. If not specified, hyperparameters are read from config.py.\n                                * models: Path to the models.\n    :param dict params: parameters of the translation model.\n    """"""\n\n    from data_engine.prepare_data import update_dataset_from_file\n    from keras_wrapper.dataset import loadDataset\n    from keras_wrapper.cnn_model import loadModel\n    from keras_wrapper.model_ensemble import BeamSearchEnsemble\n\n    logger.info(""Using an ensemble of %d models"" % len(args.models))\n    models = [loadModel(m, -1, full_path=True) for m in args.models]\n    dataset = loadDataset(args.dataset)\n    dataset = update_dataset_from_file(dataset,\n                                       args.source,\n                                       params,\n                                       splits=args.splits,\n                                       output_text_filename=args.target,\n                                       compute_state_below=True)\n\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    # Apply scoring\n    extra_vars = dict()\n    extra_vars[\'tokenize_f\'] = eval(\'dataset.\' + params[\'TOKENIZATION_METHOD\'])\n\n    model_weights = args.weights\n    if model_weights:\n        assert len(model_weights) == len(\n            models), \'You should give a weight to each model. You gave %d models and %d weights.\' % (\n            len(models), len(model_weights))\n        model_weights = list(map(float, model_weights))\n        if len(model_weights) > 1:\n            logger.info(\'Giving the following weights to each model: %s\' % str(model_weights))\n\n    for s in args.splits:\n        # Apply model predictions\n        params_prediction = {\'max_batch_size\': params[\'BATCH_SIZE\'],\n                             \'n_parallel_loaders\': params[\'PARALLEL_LOADERS\'],\n                             \'predict_on_sets\': [s]}\n\n        if params[\'BEAM_SEARCH\']:\n            params_prediction[\'beam_size\'] = params[\'BEAM_SIZE\']\n            params_prediction[\'maxlen\'] = params[\'MAX_OUTPUT_TEXT_LEN_TEST\']\n            params_prediction[\'optimized_search\'] = params[\'OPTIMIZED_SEARCH\']\n            params_prediction[\'model_inputs\'] = params[\'INPUTS_IDS_MODEL\']\n            params_prediction[\'model_outputs\'] = params[\'OUTPUTS_IDS_MODEL\']\n            params_prediction[\'dataset_inputs\'] = params[\'INPUTS_IDS_DATASET\']\n            params_prediction[\'dataset_outputs\'] = params[\'OUTPUTS_IDS_DATASET\']\n            params_prediction[\'normalize_probs\'] = params.get(\'NORMALIZE_SAMPLING\', False)\n            params_prediction[\'alpha_factor\'] = params.get(\'ALPHA_FACTOR\', 1.0)\n            params_prediction[\'coverage_penalty\'] = params.get(\'COVERAGE_PENALTY\', False)\n            params_prediction[\'length_penalty\'] = params.get(\'LENGTH_PENALTY\', False)\n            params_prediction[\'length_norm_factor\'] = params.get(\'LENGTH_NORM_FACTOR\', 0.0)\n            params_prediction[\'coverage_norm_factor\'] = params.get(\'COVERAGE_NORM_FACTOR\', 0.0)\n            params_prediction[\'pos_unk\'] = params.get(\'POS_UNK\', False)\n            params_prediction[\'state_below_maxlen\'] = -1 if params.get(\'PAD_ON_BATCH\', True) \\\n                else params.get(\'MAX_OUTPUT_TEXT_LEN\', 50)\n            params_prediction[\'output_max_length_depending_on_x\'] = params.get(\'MAXLEN_GIVEN_X\', True)\n            params_prediction[\'output_max_length_depending_on_x_factor\'] = params.get(\'MAXLEN_GIVEN_X_FACTOR\', 3)\n            params_prediction[\'output_min_length_depending_on_x\'] = params.get(\'MINLEN_GIVEN_X\', True)\n            params_prediction[\'output_min_length_depending_on_x_factor\'] = params.get(\'MINLEN_GIVEN_X_FACTOR\', 2)\n            params_prediction[\'attend_on_output\'] = params.get(\'ATTEND_ON_OUTPUT\',\n                                                               \'transformer\' in params[\'MODEL_TYPE\'].lower())\n            beam_searcher = BeamSearchEnsemble(models,\n                                               dataset,\n                                               params_prediction,\n                                               model_weights=model_weights,\n                                               verbose=args.verbose)\n            scores = beam_searcher.scoreNet()[s]\n\n        # Store result\n        if args.dest is not None:\n            filepath = args.dest  # results file\n            if params[\'SAMPLING_SAVE_MODE\'] == \'list\':\n                list2file(filepath, scores)\n            elif params[\'SAMPLING_SAVE_MODE\'] == \'numpy\':\n                numpy2file(filepath, scores)\n            else:\n                raise Exception(\'The sampling mode \' + params[\'SAMPLING_SAVE_MODE\'] + \' is not currently supported.\')\n        else:\n            print(scores)\n'"
nmt_keras/build_callbacks.py,0,"b'# -*- coding: utf-8 -*-\nfrom keras_wrapper.extra.callbacks import *\n\n\ndef buildCallbacks(params, model, dataset):\n    """"""\n    Builds the selected set of callbacks run during the training of the model:\n        * EvalPerformance: Evaluates the model in the validation set given a number of epochs/updates.\n        * SampleEachNUpdates: Shows several translation samples during training.\n\n\n    :param dict params: Dictionary of network hyperparameters.\n    :param Model_Wrapper model: Model instance on which to apply the callback.\n    :param Dataset dataset: Dataset instance on which to apply the callback.\n    :return: list of callbacks to pass to the Keras\' training.\n    """"""\n\n    callbacks = []\n\n    if params[\'METRICS\'] or params[\'SAMPLE_ON_SETS\']:\n        # Evaluate training\n        extra_vars = {\'language\': params.get(\'TRG_LAN\', \'en\'),\n                      \'n_parallel_loaders\': params[\'PARALLEL_LOADERS\'],\n                      \'tokenize_f\': eval(\'dataset.\' + params.get(\'TOKENIZATION_METHOD\', \'tokenize_none\')),\n                      \'detokenize_f\': eval(\'dataset.\' + params.get(\'DETOKENIZATION_METHOD\', \'detokenize_none\')),\n                      \'apply_detokenization\': params.get(\'APPLY_DETOKENIZATION\', False),\n                      \'tokenize_hypotheses\': params.get(\'TOKENIZE_HYPOTHESES\', True),\n                      \'tokenize_references\': params.get(\'TOKENIZE_REFERENCES\', True)\n                      }\n\n        input_text_id = params[\'INPUTS_IDS_DATASET\'][0]\n        vocab_x = dataset.vocabulary[input_text_id][\'idx2words\']\n        vocab_y = dataset.vocabulary[params[\'OUTPUTS_IDS_DATASET\'][0]][\'idx2words\']\n        if params[\'BEAM_SEARCH\']:\n            extra_vars[\'beam_size\'] = params.get(\'BEAM_SIZE\', 6)\n            extra_vars[\'state_below_index\'] = params.get(\'BEAM_SEARCH_COND_INPUT\', -1)\n            extra_vars[\'maxlen\'] = params.get(\'MAX_OUTPUT_TEXT_LEN_TEST\', 30)\n            extra_vars[\'optimized_search\'] = params.get(\'OPTIMIZED_SEARCH\', True)\n            extra_vars[\'model_inputs\'] = params[\'INPUTS_IDS_MODEL\']\n            extra_vars[\'model_outputs\'] = params[\'OUTPUTS_IDS_MODEL\']\n            extra_vars[\'dataset_inputs\'] = params[\'INPUTS_IDS_DATASET\']\n            extra_vars[\'dataset_outputs\'] = params[\'OUTPUTS_IDS_DATASET\']\n            extra_vars[\'search_pruning\'] = params.get(\'SEARCH_PRUNING\', False)\n            extra_vars[\'normalize_probs\'] = params.get(\'NORMALIZE_SAMPLING\', False)\n            extra_vars[\'alpha_factor\'] = params.get(\'ALPHA_FACTOR\', 1.)\n            extra_vars[\'coverage_penalty\'] = params.get(\'COVERAGE_PENALTY\', False)\n            extra_vars[\'length_penalty\'] = params.get(\'LENGTH_PENALTY\', False)\n            extra_vars[\'length_norm_factor\'] = params.get(\'LENGTH_NORM_FACTOR\', 0.0)\n            extra_vars[\'coverage_norm_factor\'] = params.get(\'COVERAGE_NORM_FACTOR\', 0.0)\n            extra_vars[\'state_below_maxlen\'] = -1 if params.get(\'PAD_ON_BATCH\', True) \\\n                else params.get(\'MAX_OUTPUT_TEXT_LEN\', 50)\n            extra_vars[\'pos_unk\'] = params[\'POS_UNK\']\n            extra_vars[\'output_max_length_depending_on_x\'] = params.get(\'MAXLEN_GIVEN_X\', True)\n            extra_vars[\'output_max_length_depending_on_x_factor\'] = params.get(\'MAXLEN_GIVEN_X_FACTOR\', 3)\n            extra_vars[\'output_min_length_depending_on_x\'] = params.get(\'MINLEN_GIVEN_X\', True)\n            extra_vars[\'output_min_length_depending_on_x_factor\'] = params.get(\'MINLEN_GIVEN_X_FACTOR\', 2)\n            extra_vars[\'attend_on_output\'] = params.get(\'ATTEND_ON_OUTPUT\', \'transformer\' in params[\'MODEL_TYPE\'].lower())\n            extra_vars[\'glossary\'] = None if params.get(\'GLOSSARY\', None) is None else pkl2dict(params.get(\'GLOSSARY\'))\n\n            if params[\'POS_UNK\']:\n                extra_vars[\'heuristic\'] = params[\'HEURISTIC\']\n                if params[\'HEURISTIC\'] > 0:\n                    extra_vars[\'mapping\'] = dataset.mapping\n\n        if params[\'METRICS\']:\n            for s in params[\'EVAL_ON_SETS\']:\n                extra_vars[s] = dict()\n                extra_vars[s][\'references\'] = dataset.extra_variables[s][params[\'OUTPUTS_IDS_DATASET\'][0]]\n            callback_metric = EvalPerformance(model,\n                                              dataset,\n                                              gt_id=params[\'OUTPUTS_IDS_DATASET\'][0],\n                                              metric_name=params[\'METRICS\'],\n                                              set_name=params[\'EVAL_ON_SETS\'],\n                                              batch_size=params[\'BATCH_SIZE\'],\n                                              each_n_epochs=params[\'EVAL_EACH\'],\n                                              extra_vars=extra_vars,\n                                              reload_epoch=params[\'RELOAD\'],\n                                              is_text=True,\n                                              input_text_id=input_text_id,\n                                              index2word_y=vocab_y,\n                                              index2word_x=vocab_x,\n                                              sampling_type=params[\'SAMPLING\'],\n                                              beam_search=params[\'BEAM_SEARCH\'],\n                                              save_path=model.model_path,\n                                              start_eval_on_epoch=params[\'START_EVAL_ON_EPOCH\'],\n                                              write_samples=True,\n                                              write_type=params[\'SAMPLING_SAVE_MODE\'],\n                                              eval_on_epochs=params[\'EVAL_EACH_EPOCHS\'],\n                                              save_each_evaluation=params[\'SAVE_EACH_EVALUATION\'],\n                                              do_plot=params.get(\'PLOT_EVALUATION\', False),\n                                              max_plot=params.get(\'MAX_PLOT_Y\', None),\n                                              verbose=params[\'VERBOSE\'])\n\n            callbacks.append(callback_metric)\n\n        if params[\'SAMPLE_ON_SETS\']:\n            callback_sampling = SampleEachNUpdates(model,\n                                                   dataset,\n                                                   gt_id=params[\'OUTPUTS_IDS_DATASET\'][0],\n                                                   set_name=params[\'SAMPLE_ON_SETS\'],\n                                                   n_samples=params[\'N_SAMPLES\'],\n                                                   each_n_updates=params[\'SAMPLE_EACH_UPDATES\'],\n                                                   extra_vars=extra_vars,\n                                                   reload_epoch=params[\'RELOAD\'],\n                                                   batch_size=params[\'BATCH_SIZE\'],\n                                                   is_text=True,\n                                                   index2word_x=vocab_x,\n                                                   index2word_y=vocab_y,\n                                                   print_sources=True,\n                                                   in_pred_idx=params[\'INPUTS_IDS_DATASET\'][0],\n                                                   sampling_type=params[\'SAMPLING\'],\n                                                   beam_search=params[\'BEAM_SEARCH\'],\n                                                   start_sampling_on_epoch=params[\'START_SAMPLING_ON_EPOCH\'],\n                                                   verbose=params[\'VERBOSE\'])\n            callbacks.append(callback_sampling)\n    return callbacks\n'"
nmt_keras/model_zoo.py,6,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom six import iteritems\ntry:\n    import itertools.zip as zip\nexcept ImportError:\n    pass\n\nimport logging\nimport os\nimport sys\n\nfrom keras.layers import *\nfrom keras.models import model_from_json, Model\nfrom keras.utils import multi_gpu_model\nfrom keras.optimizers import *\nfrom keras.regularizers import l2, AlphaRegularizer\nfrom keras_wrapper.cnn_model import Model_Wrapper\nfrom keras_wrapper.extra.regularize import Regularize\nlogging.basicConfig(level=logging.INFO, format=\'[%(asctime)s] %(message)s\', datefmt=\'%d/%m/%Y %H:%M:%S\')\nlogger = logging.getLogger(__name__)\n\n\ndef getPositionalEncodingWeights(input_dim, output_dim, name=\'\', verbose=True):\n    """"""\n    Obtains fixed sinusoidal embeddings for obtaining the positional encoding.\n\n    :param int input_dim: Input dimension of the embeddings (i.e. vocabulary size).\n    :param int output_dim: Embeddings dimension.\n    :param str name: Name of the layer\n    :param int verbose: Be verbose\n    :return: A list with sinusoidal embeddings.\n    """"""\n\n    if verbose > 0:\n        logger.info(""<<< Obtaining positional encodings of layer "" + name + "" >>>"")\n    position_enc = np.array([[pos / np.power(10000, 2. * i / output_dim) for i in range(output_dim)] for pos in range(input_dim)])\n    position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n    position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n    return [position_enc]\n\n\nclass TranslationModel(Model_Wrapper):\n    """"""\n    Translation model class. Instance of the Model_Wrapper class (see staged_keras_wrapper).\n\n    :param dict params: all hyperparameters of the model.\n    :param str model_type: network name type (corresponds to any method defined in the section \'MODELS\' of this class).\n                 Only valid if \'structure_path\' == None.\n    :param int verbose: set to 0 if you don\'t want the model to output informative messages\n    :param str structure_path: path to a Keras\' model json file.\n                          If we speficy this parameter then \'type\' will be only an informative parameter.\n    :param str weights_path: path to the pre-trained weights file (if None, then it will be initialized according to params)\n    :param str model_name: optional name given to the network (if None, then it will be assigned to current time as its name)\n    :param dict vocabularies: vocabularies used for word embedding\n    :param str store_path: path to the folder where the temporal model packups will be stored\n    :param bool set_optimizer: Compile optimizer or not.\n    :param bool clear_dirs: Clean model directories or not.\n    """"""\n\n    def __init__(self,\n                 params,\n                 model_type=\'Translation_Model\',\n                 verbose=1,\n                 structure_path=None,\n                 weights_path=None,\n                 model_name=None,\n                 vocabularies=None,\n                 store_path=None,\n                 set_optimizer=True,\n                 clear_dirs=True):\n        """"""\n        Translation_Model object constructor.\n\n        :param params: all hyperparams of the model.\n        :param model_type: network name type (corresponds to any method defined in the section \'MODELS\' of this class).\n                     Only valid if \'structure_path\' == None.\n        :param verbose: set to 0 if you don\'t want the model to output informative messages\n        :param structure_path: path to a Keras\' model json file.\n                              If we speficy this parameter then \'type\' will be only an informative parameter.\n        :param weights_path: path to the pre-trained weights file (if None, then it will be randomly initialized)\n        :param model_name: optional name given to the network (if None, then it will be assigned to current time as its name)\n        :param vocabularies: vocabularies used for word embedding\n        :param store_path: path to the folder where the temporal model packups will be stored\n        :param set_optimizer: Compile optimizer or not.\n        :param clear_dirs: Clean model directories or not.\n\n        """"""\n        super(TranslationModel, self).__init__(model_type=model_type,\n                                               model_name=model_name,\n                                               silence=verbose == 0,\n                                               models_path=store_path,\n                                               inheritance=True)\n\n        self.__toprint = [\'_model_type\', \'name\', \'model_path\', \'verbose\']\n\n        self.verbose = verbose\n        self._model_type = model_type\n        self.params = params\n        self.vocabularies = vocabularies\n        self.ids_inputs = params[\'INPUTS_IDS_MODEL\']\n        self.ids_outputs = params[\'OUTPUTS_IDS_MODEL\']\n        self.return_alphas = params[\'COVERAGE_PENALTY\'] or params[\'POS_UNK\']\n        # Sets the model name and prepares the folders for storing the models\n        self.setName(model_name, models_path=store_path, clear_dirs=clear_dirs)\n\n        self.use_CuDNN = \'CuDNN\' if K.backend() == \'tensorflow\' and params.get(\'USE_CUDNN\', True) else \'\'\n\n        # Prepare source word embedding\n        if params[\'SRC_PRETRAINED_VECTORS\'] is not None:\n            if self.verbose > 0:\n                logger.info(""<<< Loading pretrained word vectors from:  "" + params[\'SRC_PRETRAINED_VECTORS\'] + "" >>>"")\n            src_word_vectors = np.load(os.path.join(params[\'SRC_PRETRAINED_VECTORS\']), allow_pickle=True).item()\n            self.src_embedding_weights = np.random.rand(params[\'INPUT_VOCABULARY_SIZE\'],\n                                                        params[\'SOURCE_TEXT_EMBEDDING_SIZE\'])\n            for word, index in iteritems(self.vocabularies[self.ids_inputs[0]][\'words2idx\']):\n                if src_word_vectors.get(word) is not None:\n                    self.src_embedding_weights[index, :] = src_word_vectors[word]\n            self.src_embedding_weights = [self.src_embedding_weights]\n            self.src_embedding_weights_trainable = params[\'SRC_PRETRAINED_VECTORS_TRAINABLE\'] and params.get(\'TRAINABLE_ENCODER\', True)\n            del src_word_vectors\n\n        else:\n            self.src_embedding_weights = None\n            self.src_embedding_weights_trainable = params.get(\'TRAINABLE_ENCODER\', True)\n\n        # Prepare target word embedding\n        if params[\'TRG_PRETRAINED_VECTORS\'] is not None:\n            if self.verbose > 0:\n                logger.info(""<<< Loading pretrained word vectors from: "" + params[\'TRG_PRETRAINED_VECTORS\'] + "" >>>"")\n            trg_word_vectors = np.load(os.path.join(params[\'TRG_PRETRAINED_VECTORS\']), allow_pickle=True).item()\n            self.trg_embedding_weights = np.random.rand(params[\'OUTPUT_VOCABULARY_SIZE\'],\n                                                        params[\'TARGET_TEXT_EMBEDDING_SIZE\'])\n            for word, index in iteritems(self.vocabularies[self.ids_outputs[0]][\'words2idx\']):\n                if trg_word_vectors.get(word) is not None:\n                    self.trg_embedding_weights[index, :] = trg_word_vectors[word]\n            self.trg_embedding_weights = [self.trg_embedding_weights]\n            self.trg_embedding_weights_trainable = params[\'TRG_PRETRAINED_VECTORS_TRAINABLE\'] and params.get(\'TRAINABLE_DECODER\', True)\n            del trg_word_vectors\n        else:\n            self.trg_embedding_weights = None\n            self.trg_embedding_weights_trainable = params.get(\'TRAINABLE_DECODER\', True)\n\n        # Prepare model\n        if structure_path:\n            # Load a .json model\n            if self.verbose > 0:\n                logger.info(""<<< Loading model structure from file "" + structure_path + "" >>>"")\n            self.model = model_from_json(open(structure_path).read())\n        else:\n            # Build model from scratch\n            if hasattr(self, model_type):\n                if self.verbose > 0:\n                    logger.info(""<<< Building "" + model_type + "" Translation_Model >>>"")\n                eval(\'self.\' + model_type + \'(params)\')\n            else:\n                raise Exception(\'Translation_Model model_type ""\' + model_type + \'"" is not implemented.\')\n\n        # Load weights from file\n        if weights_path:\n            if self.verbose > 0:\n                logger.info(""<<< Loading weights from file "" + weights_path + "" >>>"")\n            self.model.load_weights(weights_path)\n\n        # Print information of self\n        if verbose > 0:\n            print(str(self))\n            self.model.summary()\n            sys.stdout.flush()\n\n        if set_optimizer:\n            self.setOptimizer()\n\n    def setParams(self, params):\n        self.params = params\n\n    def setOptimizer(self, **kwargs):\n        """"""\n        Sets and compiles a new optimizer for the Translation_Model.\n        The configuration is read from Translation_Model.params.\n        :return: None\n        """"""\n        if int(self.params.get(\'ACCUMULATE_GRADIENTS\', 1)) > 1 and self.params[\'OPTIMIZER\'].lower() != \'adam\':\n            logger.warning(\'Gradient accumulate is only implemented for the Adam optimizer. Setting ""ACCUMULATE_GRADIENTS"" to 1.\')\n            self.params[\'ACCUMULATE_GRADIENTS\'] = 1\n\n        optimizer_str = \'\\t LR: \' + str(self.params.get(\'LR\', 0.01)) + \\\n                        \'\\n\\t LOSS: \' + str(self.params.get(\'LOSS\', \'categorical_crossentropy\'))\n\n        if self.params.get(\'USE_TF_OPTIMIZER\', False) and K.backend() == \'tensorflow\':\n            if self.params[\'OPTIMIZER\'].lower() not in [\'sgd\', \'adagrad\', \'adadelta\', \'rmsprop\', \'adam\']:\n                logger.warning(\'The optimizer %s is not natively implemented in Tensorflow. Using the Keras version.\' % (str(self.params[\'OPTIMIZER\'])))\n            if self.params.get(\'LR_DECAY\') is not None:\n                logger.warning(\'The learning rate decay is not natively implemented in native Tensorflow optimizers. Using the Keras version.\')\n                self.params[\'USE_TF_OPTIMIZER\'] = False\n            if self.params.get(\'ACCUMULATE_GRADIENTS\', 1) > 1:\n                logger.warning(\'The gradient accumulation is not natively implemented in native Tensorflow optimizers. Using the Keras version.\')\n                self.params[\'USE_TF_OPTIMIZER\'] = False\n\n        if self.params.get(\'USE_TF_OPTIMIZER\', False) and K.backend() == \'tensorflow\' and self.params[\'OPTIMIZER\'].lower() in [\'sgd\', \'adagrad\', \'adadelta\', \'rmsprop\', \'adam\']:\n            import tensorflow as tf\n            if self.params[\'OPTIMIZER\'].lower() == \'sgd\':\n                if self.params.get(\'MOMENTUM\') is None:\n                    optimizer = TFOptimizer(tf.train.GradientDescentOptimizer(self.params.get(\'LR\', 0.01)))\n                else:\n                    optimizer = TFOptimizer(tf.train.MomentumOptimizer(self.params.get(\'LR\', 0.01),\n                                                                       self.params.get(\'MOMENTUM\', 0.0),\n                                                                       use_nesterov=self.params.get(\'NESTEROV_MOMENTUM\', False)))\n                    optimizer_str += \'\\n\\t MOMENTUM: \' + str(self.params.get(\'MOMENTUM\', 0.0)) + \\\n                                     \'\\n\\t NESTEROV: \' + str(self.params.get(\'NESTEROV_MOMENTUM\', False))\n\n            elif self.params[\'OPTIMIZER\'].lower() == \'adam\':\n                optimizer = TFOptimizer(tf.train.AdamOptimizer(learning_rate=self.params.get(\'LR\', 0.01),\n                                                               beta1=self.params.get(\'BETA_1\', 0.9),\n                                                               beta2=self.params.get(\'BETA_2\', 0.999),\n                                                               epsilon=self.params.get(\'EPSILON\', 1e-7)))\n                optimizer_str += \'\\n\\t BETA_1: \' + str(self.params.get(\'BETA_1\', 0.9)) + \\\n                                 \'\\n\\t BETA_2: \' + str(self.params.get(\'BETA_2\', 0.999)) + \\\n                                 \'\\n\\t EPSILON: \' + str(self.params.get(\'EPSILON\', 1e-7))\n\n            elif self.params[\'OPTIMIZER\'].lower() == \'adagrad\':\n                optimizer = TFOptimizer(tf.train.AdagradOptimizer(self.params.get(\'LR\', 0.01)))\n\n            elif self.params[\'OPTIMIZER\'].lower() == \'rmsprop\':\n                optimizer = TFOptimizer(tf.train.RMSPropOptimizer(self.params.get(\'LR\', 0.01),\n                                                                  decay=self.params.get(\'LR_OPTIMIZER_DECAY\', 0.0),\n                                                                  momentum=self.params.get(\'MOMENTUM\', 0.0),\n                                                                  epsilon=self.params.get(\'EPSILON\', 1e-7)))\n                optimizer_str += \'\\n\\t MOMENTUM: \' + str(self.params.get(\'MOMENTUM\', 0.9)) + \\\n                                 \'\\n\\t EPSILON: \' + str(self.params.get(\'EPSILON\', 1e-7))\n\n            elif self.params[\'OPTIMIZER\'].lower() == \'adadelta\':\n                optimizer = TFOptimizer(tf.train.AdadeltaOptimizer(learning_rate=self.params.get(\'LR\', 0.01),\n                                                                   rho=self.params.get(\'RHO\', 0.95),\n                                                                   epsilon=self.params.get(\'EPSILON\', 1e-7)))\n                optimizer_str += \'\\n\\t RHO: \' + str(self.params.get(\'RHO\', 0.9)) + \\\n                                 \'\\n\\t EPSILON: \' + str(self.params.get(\'EPSILON\', 1e-7))\n\n            else:\n                raise Exception(\'\\tThe chosen optimizer is not implemented.\')\n        else:\n            if self.params[\'OPTIMIZER\'].lower() == \'sgd\':\n                optimizer = SGD(lr=self.params.get(\'LR\', 0.01),\n                                momentum=self.params.get(\'MOMENTUM\', 0.0),\n                                decay=self.params.get(\'LR_OPTIMIZER_DECAY\', 0.0),\n                                nesterov=self.params.get(\'NESTEROV_MOMENTUM\', False),\n                                clipnorm=self.params.get(\'CLIP_C\', 0.),\n                                clipvalue=self.params.get(\'CLIP_V\', 0.))\n                optimizer_str += \'\\n\\t MOMENTUM: \' + str(self.params.get(\'MOMENTUM\', 0.0)) + \\\n                                 \'\\n\\t NESTEROV: \' + str(self.params.get(\'NESTEROV_MOMENTUM\', False))\n\n            elif self.params[\'OPTIMIZER\'].lower() == \'rsmprop\':\n                optimizer = RMSprop(lr=self.params.get(\'LR\', 0.001),\n                                    rho=self.params.get(\'RHO\', 0.9),\n                                    decay=self.params.get(\'LR_OPTIMIZER_DECAY\', 0.0),\n                                    clipnorm=self.params.get(\'CLIP_C\', 0.),\n                                    clipvalue=self.params.get(\'CLIP_V\', 0.),\n                                    epsilon=self.params.get(\'EPSILON\', 1e-7))\n                optimizer_str += \'\\n\\t RHO: \' + str(self.params.get(\'RHO\', 0.9)) + \\\n                                 \'\\n\\t EPSILON: \' + str(self.params.get(\'EPSILON\', 1e-7))\n\n            elif self.params[\'OPTIMIZER\'].lower() == \'adagrad\':\n                optimizer = Adagrad(lr=self.params.get(\'LR\', 0.01),\n                                    decay=self.params.get(\'LR_OPTIMIZER_DECAY\', 0.0),\n                                    clipnorm=self.params.get(\'CLIP_C\', 0.),\n                                    clipvalue=self.params.get(\'CLIP_V\', 0.),\n                                    epsilon=self.params.get(\'EPSILON\', 1e-7))\n                optimizer_str += \'\\n\\t EPSILON: \' + str(self.params.get(\'EPSILON\', 1e-7))\n\n            elif self.params[\'OPTIMIZER\'].lower() == \'adadelta\':\n                optimizer = Adadelta(lr=self.params.get(\'LR\', 1.0),\n                                     rho=self.params.get(\'RHO\', 0.9),\n                                     decay=self.params.get(\'LR_OPTIMIZER_DECAY\', 0.0),\n                                     clipnorm=self.params.get(\'CLIP_C\', 0.),\n                                     clipvalue=self.params.get(\'CLIP_V\', 0.),\n                                     epsilon=self.params.get(\'EPSILON\', 1e-7))\n                optimizer_str += \'\\n\\t RHO: \' + str(self.params.get(\'RHO\', 0.9)) + \\\n                                 \'\\n\\t EPSILON: \' + str(self.params.get(\'EPSILON\', 1e-7))\n\n            elif self.params[\'OPTIMIZER\'].lower() == \'adam\':\n                if self.params.get(\'ACCUMULATE_GRADIENTS\', 1) > 1:\n                    optimizer = AdamAccumulate(lr=self.params.get(\'LR\', 0.001),\n                                               beta_1=self.params.get(\'BETA_1\', 0.9),\n                                               beta_2=self.params.get(\'BETA_2\', 0.999),\n                                               amsgrad=self.params.get(\'AMSGRAD\', False),\n                                               decay=self.params.get(\'LR_OPTIMIZER_DECAY\', 0.0),\n                                               clipnorm=self.params.get(\'CLIP_C\', 0.),\n                                               clipvalue=self.params.get(\'CLIP_V\', 0.),\n                                               epsilon=self.params.get(\'EPSILON\', 1e-7),\n                                               accum_iters=self.params.get(\'ACCUMULATE_GRADIENTS\'))\n                    optimizer_str += \'\\n\\t BETA_1: \' + str(self.params.get(\'BETA_1\', 0.9)) + \\\n                                     \'\\n\\t BETA_2: \' + str(self.params.get(\'BETA_2\', 0.999)) + \\\n                                     \'\\n\\t AMSGRAD: \' + str(self.params.get(\'AMSGRAD\', False)) + \\\n                                     \'\\n\\t ACCUMULATE_GRADIENTS: \' + str(self.params.get(\'ACCUMULATE_GRADIENTS\')) + \\\n                                     \'\\n\\t EPSILON: \' + str(self.params.get(\'EPSILON\', 1e-7))\n                else:\n                    optimizer = Adam(lr=self.params.get(\'LR\', 0.001),\n                                     beta_1=self.params.get(\'BETA_1\', 0.9),\n                                     beta_2=self.params.get(\'BETA_2\', 0.999),\n                                     amsgrad=self.params.get(\'AMSGRAD\', False),\n                                     decay=self.params.get(\'LR_OPTIMIZER_DECAY\', 0.0),\n                                     clipnorm=self.params.get(\'CLIP_C\', 0.),\n                                     clipvalue=self.params.get(\'CLIP_V\', 0.),\n                                     epsilon=self.params.get(\'EPSILON\', 1e-7))\n                    optimizer_str += \'\\n\\t BETA_1: \' + str(self.params.get(\'BETA_1\', 0.9)) + \\\n                                     \'\\n\\t BETA_2: \' + str(self.params.get(\'BETA_2\', 0.999)) + \\\n                                     \'\\n\\t AMSGRAD: \' + str(self.params.get(\'AMSGRAD\', False)) + \\\n                                     \'\\n\\t EPSILON: \' + str(self.params.get(\'EPSILON\', 1e-7))\n\n            elif self.params[\'OPTIMIZER\'].lower() == \'adamax\':\n                optimizer = Adamax(lr=self.params.get(\'LR\', 0.002),\n                                   beta_1=self.params.get(\'BETA_1\', 0.9),\n                                   beta_2=self.params.get(\'BETA_2\', 0.999),\n                                   decay=self.params.get(\'LR_OPTIMIZER_DECAY\', 0.0),\n                                   clipnorm=self.params.get(\'CLIP_C\', 0.),\n                                   clipvalue=self.params.get(\'CLIP_V\', 0.),\n                                   epsilon=self.params.get(\'EPSILON\', 1e-7))\n                optimizer_str += \'\\n\\t BETA_1: \' + str(self.params.get(\'BETA_1\', 0.9)) + \\\n                                 \'\\n\\t BETA_2: \' + str(self.params.get(\'BETA_2\', 0.999)) + \\\n                                 \'\\n\\t EPSILON: \' + str(self.params.get(\'EPSILON\', 1e-7))\n            elif self.params[\'OPTIMIZER\'].lower() == \'nadam\':\n                optimizer = Nadam(lr=self.params.get(\'LR\', 0.002),\n                                  beta_1=self.params.get(\'BETA_1\', 0.9),\n                                  beta_2=self.params.get(\'BETA_2\', 0.999),\n                                  schedule_decay=self.params.get(\'LR_OPTIMIZER_DECAY\', 0.0),\n                                  clipnorm=self.params.get(\'CLIP_C\', 0.),\n                                  clipvalue=self.params.get(\'CLIP_V\', 0.),\n                                  epsilon=self.params.get(\'EPSILON\', 1e-7))\n                optimizer_str += \'\\n\\t BETA_1: \' + str(self.params.get(\'BETA_1\', 0.9)) + \\\n                                 \'\\n\\t BETA_2: \' + str(self.params.get(\'BETA_2\', 0.999)) + \\\n                                 \'\\n\\t EPSILON: \' + str(self.params.get(\'EPSILON\', 1e-7))\n\n            elif self.params[\'OPTIMIZER\'].lower() == \'sgdhd\':\n                optimizer = SGDHD(lr=self.params.get(\'LR\', 0.002),\n                                  clipnorm=self.params.get(\'CLIP_C\', 10.),\n                                  clipvalue=self.params.get(\'CLIP_V\', 0.),\n                                  hypergrad_lr=self.params.get(\'HYPERGRAD_LR\', 0.001))\n                optimizer_str += \'\\n\\t HYPERGRAD_LR: \' + str(self.params.get(\'HYPERGRAD_LR\', 0.001))\n\n            elif self.params[\'OPTIMIZER\'].lower() == \'qhsgd\':\n                optimizer = QHSGD(lr=self.params.get(\'LR\', 0.002),\n                                  momentum=self.params.get(\'MOMENTUM\', 0.0),\n                                  quasi_hyperbolic_momentum=self.params.get(\'QUASI_HYPERBOLIC_MOMENTUM\', 0.0),\n                                  decay=self.params.get(\'LR_OPTIMIZER_DECAY\', 0.0),\n                                  nesterov=self.params.get(\'NESTEROV_MOMENTUM\', False),\n                                  dampening=self.params.get(\'DAMPENING\', 0.),\n                                  clipnorm=self.params.get(\'CLIP_C\', 10.),\n                                  clipvalue=self.params.get(\'CLIP_V\', 0.))\n                optimizer_str += \'\\n\\t MOMENTUM: \' + str(self.params.get(\'MOMENTUM\', 0.0)) + \\\n                                 \'\\n\\t QUASI_HYPERBOLIC_MOMENTUM: \' + str(self.params.get(\'QUASI_HYPERBOLIC_MOMENTUM\', 0.0)) + \\\n                                 \'\\n\\t DAMPENING: \' + str(self.params.get(\'DAMPENING\', 0.0)) + \\\n                                 \'\\n\\t NESTEROV: \' + str(self.params.get(\'NESTEROV_MOMENTUM\', False))\n\n            elif self.params[\'OPTIMIZER\'].lower() == \'qhsgdhd\':\n                optimizer = QHSGDHD(lr=self.params.get(\'LR\', 0.002),\n                                    momentum=self.params.get(\'MOMENTUM\', 0.0),\n                                    quasi_hyperbolic_momentum=self.params.get(\'QUASI_HYPERBOLIC_MOMENTUM\', 0.0),\n                                    dampening=self.params.get(\'DAMPENING\', 0.),\n                                    hypergrad_lr=self.params.get(\'HYPERGRAD_LR\', 0.001),\n                                    decay=self.params.get(\'LR_OPTIMIZER_DECAY\', 0.0),\n                                    nesterov=self.params.get(\'NESTEROV_MOMENTUM\', False),\n                                    clipnorm=self.params.get(\'CLIP_C\', 10.),\n                                    clipvalue=self.params.get(\'CLIP_V\', 0.))\n                optimizer_str += \'\\n\\t MOMENTUM: \' + str(self.params.get(\'MOMENTUM\', 0.0)) + \\\n                                 \'\\n\\t QUASI_HYPERBOLIC_MOMENTUM: \' + str(self.params.get(\'QUASI_HYPERBOLIC_MOMENTUM\', 0.0)) + \\\n                                 \'\\n\\t HYPERGRAD_LR: \' + str(self.params.get(\'HYPERGRAD_LR\', 0.001)) + \\\n                                 \'\\n\\t DAMPENING: \' + str(self.params.get(\'DAMPENING\', 0.0)) + \\\n                                 \'\\n\\t NESTEROV: \' + str(self.params.get(\'NESTEROV_MOMENTUM\', False))\n\n            elif self.params[\'OPTIMIZER\'].lower() == \'adamhd\':\n                optimizer = AdamHD(lr=self.params.get(\'LR\', 0.002),\n                                   hypergrad_lr=self.params.get(\'HYPERGRAD_LR\', 0.001),\n                                   beta_1=self.params.get(\'BETA_1\', 0.9),\n                                   beta_2=self.params.get(\'BETA_2\', 0.999),\n                                   decay=self.params.get(\'LR_OPTIMIZER_DECAY\', 0.0),\n                                   clipnorm=self.params.get(\'CLIP_C\', 10.),\n                                   clipvalue=self.params.get(\'CLIP_V\', 0.),\n                                   epsilon=self.params.get(\'EPSILON\', 1e-7))\n                optimizer_str += \'\\n\\t HYPERGRAD_LR: \' + str(self.params.get(\'HYPERGRAD_LR\', 0.001)) + \\\n                                 \'\\n\\t BETA_1: \' + str(self.params.get(\'BETA_1\', 0.9)) + \\\n                                 \'\\n\\t BETA_2: \' + str(self.params.get(\'BETA_2\', 0.999)) + \\\n                                 \'\\n\\t EPSILON: \' + str(self.params.get(\'EPSILON\', 1e-7))\n            else:\n                logger.info(\'\\tWARNING: The modification of the LR is not implemented for the chosen optimizer.\')\n                optimizer = eval(self.params[\'OPTIMIZER\'])\n\n            optimizer_str += \'\\n\\t CLIP_C \' + str(self.params.get(\'CLIP_C\', 0.)) + \\\n                             \'\\n\\t CLIP_V \' + str(self.params.get(\'CLIP_V\', 0.)) + \\\n                             \'\\n\\t LR_OPTIMIZER_DECAY \' + str(self.params.get(\'LR_OPTIMIZER_DECAY\', 0.0)) + \\\n                             \'\\n\\t ACCUMULATE_GRADIENTS \' + str(self.params.get(\'ACCUMULATE_GRADIENTS\', 1)) + \'\\n\'\n        if self.verbose > 0:\n            logger.info(""Preparing optimizer and compiling. Optimizer configuration: \\n"" + optimizer_str)\n\n        if hasattr(self, \'multi_gpu_model\') and self.multi_gpu_model is not None:\n            model_to_compile = self.multi_gpu_model\n        else:\n            model_to_compile = self.model\n\n        model_to_compile.compile(optimizer=optimizer,\n                                 loss=self.params[\'LOSS\'],\n                                 metrics=self.params.get(\'KERAS_METRICS\', []),\n                                 loss_weights=self.params.get(\'LOSS_WEIGHTS\', None),\n                                 sample_weight_mode=\'temporal\' if self.params[\'SAMPLE_WEIGHTS\'] else None,\n                                 weighted_metrics=self.params.get(\'KERAS_METRICS_WEIGHTS\', None),\n                                 target_tensors=self.params.get(\'TARGET_TENSORS\'))\n\n    def __str__(self):\n        """"""\n        Plots basic model information.\n\n        :return: String containing model information.\n        """"""\n        obj_str = \'-----------------------------------------------------------------------------------\\n\'\n        class_name = self.__class__.__name__\n        obj_str += \'\\t\\t\' + class_name + \' instance\\n\'\n        obj_str += \'-----------------------------------------------------------------------------------\\n\'\n\n        # Print pickled attributes\n        for att in self.__toprint:\n            obj_str += att + \': \' + str(self.__dict__[att])\n            obj_str += \'\\n\'\n\n        obj_str += \'\\n\'\n        obj_str += \'Params:\\n\\t\'\n        obj_str += ""\\n\\t"".join([str(key) + "": "" + str(self.params[key]) for key in sorted(self.params.keys())])\n        obj_str += \'\\n\'\n        obj_str += \'-----------------------------------------------------------------------------------\'\n\n        return obj_str\n\n    # ------------------------------------------------------- #\n    #       PREDEFINED MODELS\n    # ------------------------------------------------------- #\n\n    def AttentionRNNEncoderDecoder(self, params):\n        """"""\n        Neural machine translation with:\n            * BRNN encoder\n            * Attention mechansim on input sequence of annotations\n            * Conditional RNN for decoding\n            * Deep output layers:\n            * Context projected to output\n            * Last word projected to output\n            * Possibly deep encoder/decoder\n        See:\n            * `Neural Machine Translation by Jointly Learning to Align and Translate`_.\n            * `Nematus\\: a Toolkit for Neural Machine Translation`_.\n\n        .. _Neural Machine Translation by Jointly Learning to Align and Translate: https://arxiv.org/abs/1409.0473\n        .. _Nematus\\: a Toolkit for Neural Machine Translation: https://arxiv.org/abs/1703.04357\n\n        :param int params: Dictionary of hyper-params (see config.py)\n        :return: None\n        """"""\n\n        # 1. Source text input\n        src_text = Input(name=self.ids_inputs[0], batch_shape=tuple([None, None]), dtype=\'int32\')\n        # 2. Encoder\n        # 2.1. Source word embedding\n        embedding = Embedding(params[\'INPUT_VOCABULARY_SIZE\'], params[\'SOURCE_TEXT_EMBEDDING_SIZE\'],\n                              name=\'source_word_embedding\',\n                              embeddings_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                              embeddings_initializer=params[\'INIT_FUNCTION\'],\n                              trainable=self.src_embedding_weights_trainable,\n                              weights=self.src_embedding_weights,\n                              mask_zero=True)\n        src_embedding = embedding(src_text)\n\n        if params.get(\'SCALE_SOURCE_WORD_EMBEDDINGS\', False):\n            src_embedding = SqrtScaling(params[\'SOURCE_TEXT_EMBEDDING_SIZE\'])(src_embedding)\n\n        src_embedding = Regularize(src_embedding, params, name=\'src_embedding\')\n\n        # Get mask of source embeddings (CuDNN RNNs don\'t accept masks)\n        src_embedding_mask = GetMask(name=\'source_text_mask\')(src_embedding)\n        src_embedding = RemoveMask()(src_embedding)\n\n        if params[\'RECURRENT_INPUT_DROPOUT_P\'] > 0.:\n            src_embedding = Dropout(params[\'RECURRENT_INPUT_DROPOUT_P\'])(src_embedding)\n\n        # 2.2. BRNN encoder (GRU/LSTM)\n        if params[\'BIDIRECTIONAL_ENCODER\']:\n            annotations = Bidirectional(eval(self.use_CuDNN + params[\'ENCODER_RNN_TYPE\'])(params[\'ENCODER_HIDDEN_SIZE\'],\n                                                                                          kernel_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                                          recurrent_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                                          bias_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                                          kernel_initializer=params[\'INIT_FUNCTION\'],\n                                                                                          recurrent_initializer=params[\'INNER_INIT\'],\n                                                                                          trainable=params.get(\'TRAINABLE_ENCODER\', True),\n                                                                                          reset_after=params.get(\'GRU_RESET_AFTER\', False),\n                                                                                          return_sequences=True),\n                                        trainable=params.get(\'TRAINABLE_ENCODER\', True),\n                                        name=\'bidirectional_encoder_\' + params[\'ENCODER_RNN_TYPE\'],\n                                        merge_mode=params.get(\'BIDIRECTIONAL_MERGE_MODE\', \'concat\'))(src_embedding)\n        else:\n            annotations = eval(self.use_CuDNN + params[\'ENCODER_RNN_TYPE\'])(params[\'ENCODER_HIDDEN_SIZE\'],\n                                                                            kernel_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                            recurrent_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                            bias_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                            kernel_initializer=params[\'INIT_FUNCTION\'],\n                                                                            recurrent_initializer=params[\'INNER_INIT\'],\n                                                                            reset_after=params.get(\'GRU_RESET_AFTER\', False),\n                                                                            trainable=params.get(\'TRAINABLE_ENCODER\', True),\n                                                                            return_sequences=True,\n                                                                            name=\'encoder_\' + params[\'ENCODER_RNN_TYPE\'])(src_embedding)\n        annotations = Regularize(annotations, params, name=\'annotations\')\n        # 2.3. Potentially deep encoder\n        for n_layer in range(1, params[\'N_LAYERS_ENCODER\']):\n\n            if params[\'RECURRENT_INPUT_DROPOUT_P\'] > 0.:\n                annotations = Dropout(params[\'RECURRENT_INPUT_DROPOUT_P\'])(annotations)\n\n            if params[\'BIDIRECTIONAL_DEEP_ENCODER\']:\n                current_annotations = Bidirectional(eval(self.use_CuDNN + params[\'ENCODER_RNN_TYPE\'])(params[\'ENCODER_HIDDEN_SIZE\'],\n                                                                                                      kernel_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                                                      recurrent_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                                                      bias_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                                                      kernel_initializer=params[\'INIT_FUNCTION\'],\n                                                                                                      recurrent_initializer=params[\'INNER_INIT\'],\n                                                                                                      trainable=params.get(\'TRAINABLE_ENCODER\', True),\n                                                                                                      return_sequences=True),\n                                                    merge_mode=params.get(\'BIDIRECTIONAL_MERGE_MODE\', \'concat\'),\n                                                    trainable=params.get(\'TRAINABLE_ENCODER\', True),\n                                                    name=\'bidirectional_encoder_\' + str(n_layer))(annotations)\n                current_annotations = Regularize(current_annotations, params, name=\'annotations_\' + str(n_layer))\n                annotations = current_annotations if n_layer == 1 and not params[\'BIDIRECTIONAL_ENCODER\'] else Add()([annotations, current_annotations])\n            else:\n                current_annotations = eval(self.use_CuDNN + params[\'ENCODER_RNN_TYPE\'])(params[\'ENCODER_HIDDEN_SIZE\'],\n                                                                                        kernel_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                                        recurrent_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                                        bias_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                                        kernel_initializer=params[\'INIT_FUNCTION\'],\n                                                                                        recurrent_initializer=params[\'INNER_INIT\'],\n                                                                                        return_sequences=True,\n                                                                                        trainable=params.get(\'TRAINABLE_ENCODER\', True),\n                                                                                        name=\'encoder_\' + str(n_layer))(annotations)\n\n                current_annotations = Regularize(current_annotations, params, name=\'annotations_\' + str(n_layer))\n                annotations = current_annotations if n_layer == 1 and params[\'BIDIRECTIONAL_ENCODER\'] else Add()([annotations, current_annotations])\n\n        # 3. Decoder\n        # 3.1.1. Previously generated words as inputs for training -> Teacher forcing\n        next_words = Input(name=self.ids_inputs[1], batch_shape=tuple([None, None]), dtype=\'int32\')\n        # 3.1.2. Target word embedding\n        if params.get(\'TIE_EMBEDDINGS\', False):\n            state_below = embedding(next_words)\n        else:\n            state_below = Embedding(params[\'OUTPUT_VOCABULARY_SIZE\'], params[\'TARGET_TEXT_EMBEDDING_SIZE\'],\n                                    name=\'target_word_embedding\',\n                                    embeddings_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                    embeddings_initializer=params[\'INIT_FUNCTION\'],\n                                    trainable=self.trg_embedding_weights_trainable,\n                                    weights=self.trg_embedding_weights,\n                                    mask_zero=True)(next_words)\n\n        if params.get(\'SCALE_TARGET_WORD_EMBEDDINGS\', False):\n            state_below = SqrtScaling(params[\'TARGET_TEXT_EMBEDDING_SIZE\'])(state_below)\n        state_below = Regularize(state_below, params, name=\'state_below\')\n\n        # 3.2. Decoder\'s RNN initialization perceptrons with ctx mean\n        annotations = ApplyMask(name=\'annotations\')([annotations, src_embedding_mask])  # We may want the padded annotations\n        ctx_mean = MaskedMean(name=\'ctx_mean\')(annotations)\n\n        if len(params[\'INIT_LAYERS\']) > 0:\n            for n_layer_init in range(len(params[\'INIT_LAYERS\']) - 1):\n                ctx_mean = Dense(params[\'DECODER_HIDDEN_SIZE\'], name=\'init_layer_%d\' % n_layer_init,\n                                 kernel_initializer=params[\'INIT_FUNCTION\'],\n                                 kernel_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                 bias_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                 trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                 activation=params[\'INIT_LAYERS\'][n_layer_init]\n                                 )(ctx_mean)\n                ctx_mean = Regularize(ctx_mean, params, name=\'ctx\' + str(n_layer_init))\n\n            initial_state = Dense(params[\'DECODER_HIDDEN_SIZE\'], name=\'initial_state\',\n                                  kernel_initializer=params[\'INIT_FUNCTION\'],\n                                  kernel_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                  bias_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                  trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                  activation=params[\'INIT_LAYERS\'][-1]\n                                  )(ctx_mean)\n            initial_state = Regularize(initial_state, params, name=\'initial_state\')\n            input_attentional_decoder = [state_below, annotations, initial_state]\n\n            if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n                initial_memory = Dense(params[\'DECODER_HIDDEN_SIZE\'], name=\'initial_memory\',\n                                       kernel_initializer=params[\'INIT_FUNCTION\'],\n                                       kernel_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                       bias_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                       trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                       activation=params[\'INIT_LAYERS\'][-1])(ctx_mean)\n                initial_memory = Regularize(initial_memory, params, name=\'initial_memory\')\n                input_attentional_decoder.append(initial_memory)\n        else:\n            # Initialize to zeros vector\n            input_attentional_decoder = [state_below, annotations]\n            initial_state = ZeroesLayer(params[\'DECODER_HIDDEN_SIZE\'])(ctx_mean)\n            input_attentional_decoder.append(initial_state)\n            if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n                input_attentional_decoder.append(initial_state)\n\n        # 3.3. Attentional decoder\n        sharedAttRNNCond = eval(\'Att\' + params[\'DECODER_RNN_TYPE\'] + \'Cond\')(params[\'DECODER_HIDDEN_SIZE\'],\n                                                                             attention_mode=params.get(\'ATTENTION_MODE\', \'add\'),\n                                                                             att_units=params.get(\'ATTENTION_SIZE\', 0),\n                                                                             kernel_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                             recurrent_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                             conditional_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                             bias_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                                                                             attention_context_wa_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                                                             attention_recurrent_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                                                             attention_context_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                                                             bias_ba_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                                                             dropout=params[\'RECURRENT_INPUT_DROPOUT_P\'],\n                                                                             recurrent_dropout=params[\'RECURRENT_DROPOUT_P\'],\n                                                                             conditional_dropout=params[\'RECURRENT_INPUT_DROPOUT_P\'],\n                                                                             attention_dropout=params.get(\'ATTENTION_DROPOUT_P\', 0.),\n                                                                             kernel_initializer=params[\'INIT_FUNCTION\'],\n                                                                             recurrent_initializer=params[\'INNER_INIT\'],\n                                                                             attention_context_initializer=params[\'INIT_ATT\'],\n                                                                             trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                                                             return_sequences=True,\n                                                                             return_extra_variables=True,\n                                                                             return_states=True,\n                                                                             num_inputs=len(input_attentional_decoder),\n                                                                             name=\'decoder_Att\' + params[\'DECODER_RNN_TYPE\'] + \'Cond\')\n\n        rnn_output = sharedAttRNNCond(input_attentional_decoder)\n        proj_h = rnn_output[0]\n        x_att = rnn_output[1]\n        alphas = rnn_output[2]\n        h_state = rnn_output[3]\n        if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n            h_memory = rnn_output[4]\n        shared_Lambda_Permute = PermuteGeneral((1, 0, 2))\n\n        if params[\'DOUBLE_STOCHASTIC_ATTENTION_REG\'] > 0:\n            alpha_regularizer = AlphaRegularizer(alpha_factor=params[\'DOUBLE_STOCHASTIC_ATTENTION_REG\'])(alphas)\n\n        [proj_h, shared_reg_proj_h] = Regularize(proj_h, params, shared_layers=True, name=\'proj_h0\')\n\n        # 3.4. Possibly deep decoder\n        shared_proj_h_list = []\n        shared_reg_proj_h_list = []\n\n        h_states_list = [h_state]\n        if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n            h_memories_list = [h_memory]\n\n        for n_layer in range(1, params[\'N_LAYERS_DECODER\']):\n            current_rnn_input = [proj_h, shared_Lambda_Permute(x_att), initial_state]\n            if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n                current_rnn_input.append(initial_memory)\n            shared_proj_h_list.append(eval(params[\'DECODER_RNN_TYPE\'].replace(\'Conditional\', \'\') + \'Cond\')(\n                params[\'DECODER_HIDDEN_SIZE\'],\n                kernel_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                recurrent_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                conditional_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                bias_regularizer=l2(params[\'RECURRENT_WEIGHT_DECAY\']),\n                dropout=params[\'RECURRENT_DROPOUT_P\'],\n                recurrent_dropout=params[\'RECURRENT_INPUT_DROPOUT_P\'],\n                conditional_dropout=params[\'RECURRENT_INPUT_DROPOUT_P\'],\n                kernel_initializer=params[\'INIT_FUNCTION\'],\n                recurrent_initializer=params[\'INNER_INIT\'],\n                return_sequences=True,\n                return_states=True,\n                trainable=params.get(\'TRAINABLE_DECODER\', True),\n                num_inputs=len(current_rnn_input),\n                name=\'decoder_\' + params[\'DECODER_RNN_TYPE\'].replace(\'Conditional\', \'\') + \'Cond\' + str(n_layer)))\n\n            current_rnn_output = shared_proj_h_list[-1](current_rnn_input)\n            current_proj_h = current_rnn_output[0]\n            h_states_list.append(current_rnn_output[1])\n            if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n                h_memories_list.append(current_rnn_output[2])\n            [current_proj_h, shared_reg_proj_h] = Regularize(current_proj_h, params, shared_layers=True,\n                                                             name=\'proj_h\' + str(n_layer))\n            shared_reg_proj_h_list.append(shared_reg_proj_h)\n\n            proj_h = Add()([proj_h, current_proj_h])\n\n        # 3.5. Skip connections between encoder and output layer\n        shared_FC_mlp = TimeDistributed(Dense(params[\'SKIP_VECTORS_HIDDEN_SIZE\'],\n                                              kernel_initializer=params[\'INIT_FUNCTION\'],\n                                              kernel_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                              bias_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                              trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                              activation=\'linear\'),\n                                        trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                        name=\'logit_lstm\')\n        out_layer_mlp = shared_FC_mlp(proj_h)\n        shared_FC_ctx = TimeDistributed(Dense(params[\'SKIP_VECTORS_HIDDEN_SIZE\'],\n                                              kernel_initializer=params[\'INIT_FUNCTION\'],\n                                              kernel_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                              bias_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                              trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                              activation=\'linear\'),\n                                        trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                        name=\'logit_ctx\')\n        out_layer_ctx = shared_FC_ctx(x_att)\n        out_layer_ctx = shared_Lambda_Permute(out_layer_ctx)\n        shared_FC_emb = TimeDistributed(Dense(params[\'SKIP_VECTORS_HIDDEN_SIZE\'],\n                                              kernel_initializer=params[\'INIT_FUNCTION\'],\n                                              kernel_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                              bias_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                              trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                              activation=\'linear\'),\n                                        trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                        name=\'logit_emb\')\n        out_layer_emb = shared_FC_emb(state_below)\n\n        [out_layer_mlp, shared_reg_out_layer_mlp] = Regularize(out_layer_mlp, params,\n                                                               shared_layers=True, name=\'out_layer_mlp\')\n        [out_layer_ctx, shared_reg_out_layer_ctx] = Regularize(out_layer_ctx, params,\n                                                               shared_layers=True, name=\'out_layer_ctx\')\n        [out_layer_emb, shared_reg_out_layer_emb] = Regularize(out_layer_emb, params,\n                                                               shared_layers=True, name=\'out_layer_emb\')\n\n        shared_additional_output_merge = eval(params[\'ADDITIONAL_OUTPUT_MERGE_MODE\'])(name=\'additional_input\')\n        additional_output = shared_additional_output_merge([out_layer_mlp, out_layer_ctx, out_layer_emb])\n        shared_activation = Activation(params.get(\'SKIP_VECTORS_SHARED_ACTIVATION\', \'tanh\'))\n\n        out_layer = shared_activation(additional_output)\n\n        shared_deep_list = []\n        shared_reg_deep_list = []\n        # 3.6 Optional deep ouput layer\n        for i, (activation, dimension) in enumerate(params[\'DEEP_OUTPUT_LAYERS\']):\n            shared_deep_list.append(TimeDistributed(Dense(dimension, activation=activation,\n                                                          kernel_initializer=params[\'INIT_FUNCTION\'],\n                                                          kernel_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                                          bias_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                                          trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                                          ),\n                                                    trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                                    name=activation + \'_%d\' % i))\n            out_layer = shared_deep_list[-1](out_layer)\n            [out_layer, shared_reg_out_layer] = Regularize(out_layer,\n                                                           params, shared_layers=True,\n                                                           name=\'out_layer_\' + str(activation) + \'_%d\' % i)\n            shared_reg_deep_list.append(shared_reg_out_layer)\n\n        # 3.7. Output layer: Softmax\n        shared_FC_soft = TimeDistributed(Dense(params[\'OUTPUT_VOCABULARY_SIZE\'],\n                                               activation=params[\'CLASSIFIER_ACTIVATION\'],\n                                               kernel_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                               bias_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                               trainable=(params.get(\'TRAINABLE_DECODER\', True) or params.get(\'TRAIN_ONLY_LAST_LAYER\', True)),\n                                               ),\n                                         trainable=(params.get(\'TRAINABLE_DECODER\', True) or params.get(\'TRAIN_ONLY_LAST_LAYER\', True)),\n                                         name=self.ids_outputs[0])\n        softout = shared_FC_soft(out_layer)\n\n        self.model = Model(inputs=[src_text, next_words],\n                           outputs=softout,\n                           name=self.name + \'_training\')\n\n        if params[\'DOUBLE_STOCHASTIC_ATTENTION_REG\'] > 0.:\n            self.model.add_loss(alpha_regularizer)\n\n        if params.get(\'N_GPUS\', 1) > 1:\n            self.multi_gpu_model = multi_gpu_model(self.model, gpus=params[\'N_GPUS\'])\n        else:\n            self.multi_gpu_model = None\n\n        ##################################################################\n        #                         SAMPLING MODEL                         #\n        ##################################################################\n        # Now that we have the basic training model ready, let\'s prepare the model for applying decoding\n        # The beam-search model will include all the minimum required set of layers (decoder stage) which offer the\n        # possibility to generate the next state in the sequence given a pre-processed input (encoder stage)\n        # First, we need a model that outputs the preprocessed input + initial h state\n        # for applying the initial forward pass\n        model_init_input = [src_text, next_words]\n        model_init_output = [softout, annotations] + h_states_list\n        if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n            model_init_output += h_memories_list\n        if self.return_alphas:\n            model_init_output.append(alphas)\n        self.model_init = Model(inputs=model_init_input,\n                                outputs=model_init_output,\n                                name=self.name + \'_model_init\')\n\n        # Store inputs and outputs names for model_init\n        self.ids_inputs_init = self.ids_inputs\n        ids_states_names = [\'next_state_\' + str(i) for i in range(len(h_states_list))]\n\n        # first output must be the output probs.\n        self.ids_outputs_init = self.ids_outputs + [\'preprocessed_input\'] + ids_states_names\n        if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n            ids_memories_names = [\'next_memory_\' + str(i) for i in range(len(h_memories_list))]\n            self.ids_outputs_init += ids_memories_names\n        # Second, we need to build an additional model with the capability to have the following inputs:\n        #   - preprocessed_input\n        #   - prev_word\n        #   - prev_state\n        # and the following outputs:\n        #   - softmax probabilities\n        #   - next_state\n        preprocessed_size = params[\'ENCODER_HIDDEN_SIZE\'] * 2 if \\\n            (params[\'BIDIRECTIONAL_ENCODER\'] and params[\'N_LAYERS_ENCODER\'] == 1) or (params[\'BIDIRECTIONAL_DEEP_ENCODER\'] and params[\'N_LAYERS_ENCODER\'] > 1) \\\n            else params[\'ENCODER_HIDDEN_SIZE\']\n        # Define inputs\n        n_deep_decoder_layer_idx = 0\n        preprocessed_annotations = Input(name=\'preprocessed_input\', shape=tuple([None, preprocessed_size]))\n        prev_h_states_list = [Input(name=\'prev_state_\' + str(i),\n                                    shape=tuple([params[\'DECODER_HIDDEN_SIZE\']]))\n                              for i in range(len(h_states_list))]\n\n        input_attentional_decoder = [state_below, preprocessed_annotations,\n                                     prev_h_states_list[n_deep_decoder_layer_idx]]\n\n        if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n            prev_h_memories_list = [Input(name=\'prev_memory_\' + str(i),\n                                          shape=tuple([params[\'DECODER_HIDDEN_SIZE\']]))\n                                    for i in range(len(h_memories_list))]\n\n            input_attentional_decoder.append(prev_h_memories_list[n_deep_decoder_layer_idx])\n        # Apply decoder\n        rnn_output = sharedAttRNNCond(input_attentional_decoder)\n        proj_h = rnn_output[0]\n        x_att = rnn_output[1]\n        alphas = rnn_output[2]\n        h_states_list = [rnn_output[3]]\n        if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n            h_memories_list = [rnn_output[4]]\n        for reg in shared_reg_proj_h:\n            proj_h = reg(proj_h)\n\n        for (rnn_decoder_layer, proj_h_reg) in zip(shared_proj_h_list, shared_reg_proj_h_list):\n            n_deep_decoder_layer_idx += 1\n            input_rnn_decoder_layer = [proj_h, shared_Lambda_Permute(x_att),\n                                       prev_h_states_list[n_deep_decoder_layer_idx]]\n            if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n                input_rnn_decoder_layer.append(prev_h_memories_list[n_deep_decoder_layer_idx])\n\n            current_rnn_output = rnn_decoder_layer(input_rnn_decoder_layer)\n            current_proj_h = current_rnn_output[0]\n            h_states_list.append(current_rnn_output[1])  # h_state\n            if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n                h_memories_list.append(current_rnn_output[2])  # h_memory\n            for reg in proj_h_reg:\n                current_proj_h = reg(current_proj_h)\n            proj_h = Add()([proj_h, current_proj_h])\n        out_layer_mlp = shared_FC_mlp(proj_h)\n        out_layer_ctx = shared_FC_ctx(x_att)\n        out_layer_ctx = shared_Lambda_Permute(out_layer_ctx)\n        out_layer_emb = shared_FC_emb(state_below)\n\n        for (reg_out_layer_mlp, reg_out_layer_ctx, reg_out_layer_emb) in zip(shared_reg_out_layer_mlp,\n                                                                             shared_reg_out_layer_ctx,\n                                                                             shared_reg_out_layer_emb):\n            out_layer_mlp = reg_out_layer_mlp(out_layer_mlp)\n            out_layer_ctx = reg_out_layer_ctx(out_layer_ctx)\n            out_layer_emb = reg_out_layer_emb(out_layer_emb)\n\n        additional_output = shared_additional_output_merge([out_layer_mlp, out_layer_ctx, out_layer_emb])\n        out_layer = shared_activation(additional_output)\n\n        for (deep_out_layer, reg_list) in zip(shared_deep_list, shared_reg_deep_list):\n            out_layer = deep_out_layer(out_layer)\n            for reg in reg_list:\n                out_layer = reg(out_layer)\n\n        # Softmax\n        softout = shared_FC_soft(out_layer)\n        model_next_inputs = [next_words, preprocessed_annotations] + prev_h_states_list\n        model_next_outputs = [softout, preprocessed_annotations] + h_states_list\n        if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n            model_next_inputs += prev_h_memories_list\n            model_next_outputs += h_memories_list\n\n        if self.return_alphas:\n            model_next_outputs.append(alphas)\n\n        self.model_next = Model(inputs=model_next_inputs,\n                                outputs=model_next_outputs,\n                                name=self.name + \'_model_next\')\n        # Store inputs and outputs names for model_next\n        # first input must be previous word\n        self.ids_inputs_next = [self.ids_inputs[1]] + [\'preprocessed_input\']\n        # first output must be the output probs.\n        self.ids_outputs_next = self.ids_outputs + [\'preprocessed_input\']\n        # Input -> Output matchings from model_init to model_next and from model_next to model_next\n        self.matchings_init_to_next = {\'preprocessed_input\': \'preprocessed_input\'}\n        self.matchings_next_to_next = {\'preprocessed_input\': \'preprocessed_input\'}\n        # append all next states and matchings\n\n        for n_state in range(len(prev_h_states_list)):\n            self.ids_inputs_next.append(\'prev_state_\' + str(n_state))\n            self.ids_outputs_next.append(\'next_state_\' + str(n_state))\n            self.matchings_init_to_next[\'next_state_\' + str(n_state)] = \'prev_state_\' + str(n_state)\n            self.matchings_next_to_next[\'next_state_\' + str(n_state)] = \'prev_state_\' + str(n_state)\n\n        if \'LSTM\' in params[\'DECODER_RNN_TYPE\']:\n            for n_memory in range(len(prev_h_memories_list)):\n                self.ids_inputs_next.append(\'prev_memory_\' + str(n_memory))\n                self.ids_outputs_next.append(\'next_memory_\' + str(n_memory))\n                self.matchings_init_to_next[\'next_memory_\' + str(n_memory)] = \'prev_memory_\' + str(n_memory)\n                self.matchings_next_to_next[\'next_memory_\' + str(n_memory)] = \'prev_memory_\' + str(n_memory)\n\n    def Transformer(self, params):\n        """"""\n        Neural machine translation consisting in stacking blocks of:\n            * Multi-head attention.\n            * Dropout.\n            * Residual connection.\n            * Normalization.\n            * Position-wise feed-forward networks.\n\n        Positional information is injected to the model via embeddings with positional encoding.\n\n        See:\n            * `Attention Is All You Need`_.\n\n        .. _Attention Is All You Need: https://arxiv.org/abs/1706.03762\n\n        :param int params: Dictionary of params (see config.py)\n        :return: None\n        """"""\n\n        # 1. Source text input\n        src_text = Input(name=self.ids_inputs[0], batch_shape=tuple([None, None]), dtype=\'int32\')\n        src_positions = PositionLayer(name=\'position_layer_src_text\')(src_text)\n\n        # 2. Encoder\n        # 2.1. Source word embedding\n        embedding = Embedding(params[\'INPUT_VOCABULARY_SIZE\'], params[\'SOURCE_TEXT_EMBEDDING_SIZE\'],\n                              name=\'source_word_embedding\',\n                              embeddings_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                              embeddings_initializer=params[\'INIT_FUNCTION\'],\n                              trainable=self.src_embedding_weights_trainable,\n                              weights=self.src_embedding_weights,\n                              mask_zero=True)\n        src_embedding = embedding(src_text)\n\n        if params.get(\'SCALE_SOURCE_WORD_EMBEDDINGS\', False):\n            src_embedding = SqrtScaling(params[\'MODEL_SIZE\'])(src_embedding)\n        if params[\'TARGET_TEXT_EMBEDDING_SIZE\'] == params[\'SOURCE_TEXT_EMBEDDING_SIZE\']:\n            max_len = max(params[\'MAX_INPUT_TEXT_LEN\'], params[\'MAX_OUTPUT_TEXT_LEN\'], params[\'MAX_OUTPUT_TEXT_LEN_TEST\'])\n        else:\n            max_len = params[\'MAX_INPUT_TEXT_LEN\']\n\n        positional_embedding = Embedding(max_len,\n                                         params[\'SOURCE_TEXT_EMBEDDING_SIZE\'],\n                                         name=\'positional_src_word_embedding\',\n                                         trainable=False,\n                                         weights=getPositionalEncodingWeights(max_len,\n                                                                              params[\'SOURCE_TEXT_EMBEDDING_SIZE\'],\n                                                                              name=\'positional_src_word_embedding\',\n                                                                              verbose=self.verbose))\n        positional_src_embedding = positional_embedding(src_positions)\n        src_residual_multihead = Add(name=\'add_src_embedding_positional_src_embedding\')([src_embedding, positional_src_embedding])\n\n        # Regularize\n        src_residual_multihead = Dropout(params[\'DROPOUT_P\'])(src_residual_multihead)\n\n        prev_src_residual_multihead = src_residual_multihead\n\n        # Left tranformer block (encoder)\n        for n_block in range(params[\'N_LAYERS_ENCODER\']):\n            src_multihead = MultiHeadAttention(params[\'N_HEADS\'],\n                                               params[\'MODEL_SIZE\'],\n                                               activation=params.get(\'MULTIHEAD_ATTENTION_ACTIVATION\', \'relu\'),\n                                               use_bias=True,\n                                               dropout=params.get(\'ATTENTION_DROPOUT_P\', 0.),\n                                               name=\'src_MultiHeadAttention_\' + str(n_block))([src_residual_multihead,\n                                                                                               src_residual_multihead])\n            # Regularize\n            src_multihead = Dropout(params[\'DROPOUT_P\'])(src_multihead)\n            # Add\n            src_multihead = Add(name=\'src_Residual_MultiHeadAttention_\' + str(n_block))([src_multihead, prev_src_residual_multihead])\n\n            # And norm\n            src_multihead = BatchNormalization(mode=1, name=\'src_Normalization_MultiHeadAttention_\' + str(n_block))(src_multihead)\n\n            # FF\n            ff_src_multihead = TimeDistributed(PositionwiseFeedForwardDense(params[\'FF_SIZE\']))(src_multihead)\n            # Regularize\n            ff_src_multihead = Dropout(params[\'DROPOUT_P\'])(ff_src_multihead)\n\n            # Add\n            src_multihead = Add(name=\'src_Residual_FF_\' + str(n_block))([ff_src_multihead, src_multihead])\n            # And norm\n            src_multihead = BatchNormalization(mode=1, name=\'src_Normalization_FF_\' + str(n_block))(src_multihead)\n\n            prev_src_residual_multihead = src_multihead\n            src_residual_multihead = src_multihead\n\n        masked_src_multihead = MaskLayer()(prev_src_residual_multihead)  # We may want the padded annotations\n\n        # 3.1.1. Previously generated words as inputs for training -> Teacher forcing\n        next_words = Input(name=self.ids_inputs[1], batch_shape=tuple([None, None]), dtype=\'int32\')\n        next_words_positions = PositionLayer(name=\'position_layer_next_words\')(next_words)\n\n        # 3.1.2. Target word embedding\n        if params.get(\'TIE_EMBEDDINGS\', False):\n            state_below = embedding(next_words)\n        else:\n            state_below = Embedding(params[\'OUTPUT_VOCABULARY_SIZE\'], params[\'TARGET_TEXT_EMBEDDING_SIZE\'],\n                                    name=\'target_word_embedding\',\n                                    embeddings_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                    embeddings_initializer=params[\'INIT_FUNCTION\'],\n                                    trainable=self.trg_embedding_weights_trainable,\n                                    weights=self.trg_embedding_weights,\n                                    mask_zero=True)(next_words)\n\n        if params.get(\'SCALE_TARGET_WORD_EMBEDDINGS\', False):\n            state_below = SqrtScaling(params[\'MODEL_SIZE\'])(state_below)\n\n        if params[\'TARGET_TEXT_EMBEDDING_SIZE\'] == params[\'SOURCE_TEXT_EMBEDDING_SIZE\']:\n            positional_embedding_trg = positional_embedding\n        else:\n            max_len = max(params[\'MAX_OUTPUT_TEXT_LEN\'], params[\'MAX_OUTPUT_TEXT_LEN_TEST\'])\n\n            positional_embedding_trg = Embedding(max_len,\n                                                 params[\'TARGET_TEXT_EMBEDDING_SIZE\'],\n                                                 name=\'positional_trg_word_embedding\',\n                                                 trainable=False,\n                                                 weights=getPositionalEncodingWeights(max_len,\n                                                                                      params[\'TARGET_TEXT_EMBEDDING_SIZE\'],\n                                                                                      name=\'positional_trg_word_embedding\',\n                                                                                      verbose=self.verbose))\n\n        positional_trg_embedding = positional_embedding_trg(next_words_positions)\n\n        state_below = Add()([state_below, positional_trg_embedding])\n\n        # Regularize\n        state_below = Dropout(params[\'DROPOUT_P\'])(state_below)\n\n        shared_trg_multihead_list = []\n        shared_trg_dropout_multihead_list = []\n        shared_trg_add_multihead_list = []\n        shared_trg_norm_multihead_list = []\n\n        shared_src_trg_multihead_list = []\n        shared_src_trg_dropout_multihead_list = []\n        shared_src_trg_add_multihead_list = []\n        shared_src_trg_norm_multihead_list = []\n\n        shared_ff_list = []\n        shared_dropout_ff_list = []\n        shared_add_ff_list = []\n        shared_norm_ff_list = []\n\n        prev_state_below = state_below\n\n        # Right tranformer block (decoder)\n        for n_block in range(params[\'N_LAYERS_DECODER\']):\n\n            # Declare shared layers of each block\n\n            # Masked Multi-Head Attention block\n            shared_trg_multihead = MultiHeadAttention(params[\'N_HEADS\'],\n                                                      params[\'MODEL_SIZE\'],\n                                                      activation=params.get(\'MULTIHEAD_ATTENTION_ACTIVATION\', \'relu\'),\n                                                      use_bias=True,\n                                                      dropout=params.get(\'ATTENTION_DROPOUT_P\', 0.),\n                                                      mask_future=True,  # Avoid attending on future sequences\n                                                      name=\'trg_MultiHeadAttention_\' + str(n_block))\n            shared_trg_multihead_list.append(shared_trg_multihead)\n\n            # Regularize\n            shared_trg_multihead_dropout = Dropout(params[\'DROPOUT_P\'])\n            shared_trg_dropout_multihead_list.append(shared_trg_multihead_dropout)\n\n            # Add\n            shared_trg_multihead_add = Add(name=\'trg_Residual_MultiHeadAttention_\' + str(n_block))\n            shared_trg_add_multihead_list.append(shared_trg_multihead_add)\n\n            # And norm\n            shared_trg_multihead_norm = BatchNormalization(mode=1, name=\'trg_Normalization_MultiHeadAttention_\' + str(n_block))\n            shared_trg_norm_multihead_list.append(shared_trg_multihead_norm)\n\n            # Second Multi-Head Attention block\n            shared_src_trg_multihead = MultiHeadAttention(params[\'N_HEADS\'],\n                                                          params[\'MODEL_SIZE\'],\n                                                          activation=params.get(\'MULTIHEAD_ATTENTION_ACTIVATION\', \'relu\'),\n                                                          use_bias=True,\n                                                          dropout=params.get(\'ATTENTION_DROPOUT_P\', 0.),\n                                                          name=\'src_trg_MultiHeadAttention_\' + str(n_block))\n            shared_src_trg_multihead_list.append(shared_src_trg_multihead)\n\n            # Regularize\n            shared_src_trg_multihead_dropout = Dropout(params[\'DROPOUT_P\'])\n            shared_src_trg_dropout_multihead_list.append(shared_src_trg_multihead_dropout)\n\n            # Add\n            shared_src_trg_multihead_add = Add(name=\'src_trg_Residual_MultiHeadAttention_\' + str(n_block))\n            shared_src_trg_add_multihead_list.append(shared_src_trg_multihead_add)\n\n            # And norm\n            shared_src_trg_multihead_norm = BatchNormalization(mode=1, name=\'src_trg_Normalization_MultiHeadAttention_\' + str(n_block))\n            shared_src_trg_norm_multihead_list.append(shared_src_trg_multihead_norm)\n\n            # FF\n            shared_ff_src_trg_multihead = TimeDistributed(PositionwiseFeedForwardDense(params[\'FF_SIZE\'],\n                                                                                       name=\'src_trg_PositionwiseFeedForward_\' + str(n_block)),\n                                                          name=\'src_trg_TimeDistributedPositionwiseFeedForward_\' + str(n_block))\n            shared_ff_list.append(shared_ff_src_trg_multihead)\n\n            # Regularize\n            shared_ff_src_trg_multihead_dropout = Dropout(params[\'DROPOUT_P\'])\n            shared_dropout_ff_list.append(shared_ff_src_trg_multihead_dropout)\n\n            # Add\n            shared_ff_src_trg_multihead_add = Add(name=\'src_trg_Residual_FF_\' + str(n_block))\n            shared_add_ff_list.append(shared_ff_src_trg_multihead_add)\n\n            # And norm\n            shared_ff_src_trg_multihead_norm = BatchNormalization(mode=1, name=\'src_trg_Normalization_FF_\' + str(n_block))\n            shared_norm_ff_list.append(shared_ff_src_trg_multihead_norm)\n\n            # Apply shared layers\n            # Masked Multi-Head Attention block\n            trg_multihead = shared_trg_multihead_list[n_block]([prev_state_below, prev_state_below])\n\n            # Regularize\n            trg_multihead_dropout = shared_trg_dropout_multihead_list[n_block](trg_multihead)\n\n            # Add\n            trg_multihead_add = shared_trg_add_multihead_list[n_block]([prev_state_below, trg_multihead_dropout])\n\n            # And norm\n            trg_multihead_norm = shared_trg_norm_multihead_list[n_block](trg_multihead_add)\n\n            # Second Multi-Head Attention block\n            src_trg_multihead = shared_src_trg_multihead_list[n_block]([trg_multihead_norm,   # Queries from the previous decoder layer.\n                                                                        masked_src_multihead  # Keys and values from the output of the encoder.\n                                                                        ])\n\n            # Regularize\n            src_trg_multihead_dropout = shared_src_trg_dropout_multihead_list[n_block](src_trg_multihead)\n\n            # Add\n            src_trg_multihead_add = shared_src_trg_add_multihead_list[n_block]([src_trg_multihead_dropout, trg_multihead_norm])\n\n            # And norm\n            src_trg_multihead_norm = shared_src_trg_norm_multihead_list[n_block](src_trg_multihead_add)\n\n            # FF\n            ff_src_trg_multihead = shared_ff_list[n_block](src_trg_multihead_norm)\n\n            # Regularize\n            ff_src_trg_multihead_dropout = shared_dropout_ff_list[n_block](ff_src_trg_multihead)\n\n            # Add\n            ff_src_trg_multihead_add = shared_add_ff_list[n_block]([ff_src_trg_multihead_dropout,\n                                                                    src_trg_multihead_norm])\n\n            # And norm\n            ff_src_trg_multihead_norm = shared_norm_ff_list[n_block](ff_src_trg_multihead_add)\n\n            prev_state_below = ff_src_trg_multihead_norm\n\n        out_layer = prev_state_below\n        shared_deep_list = []\n        shared_reg_deep_list = []\n        # 3.6 Optional deep ouput layer\n        for i, (activation, dimension) in enumerate(params[\'DEEP_OUTPUT_LAYERS\']):\n            shared_deep_list.append(TimeDistributed(Dense(dimension,\n                                                          activation=activation,\n                                                          kernel_initializer=params[\'INIT_FUNCTION\'],\n                                                          kernel_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                                          bias_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                                          trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                                          ),\n                                                    trainable=params.get(\'TRAINABLE_DECODER\', True),\n                                                    name=activation + \'_%d\' % i))\n            out_layer = shared_deep_list[-1](out_layer)\n            [out_layer, shared_reg_out_layer] = Regularize(out_layer,\n                                                           params, shared_layers=True,\n                                                           name=\'out_layer_\' + str(activation) + \'_%d\' % i)\n            shared_reg_deep_list.append(shared_reg_out_layer)\n\n        # 3.7. Output layer: Softmax\n        shared_FC_soft = TimeDistributed(Dense(params[\'OUTPUT_VOCABULARY_SIZE\'],\n                                               activation=params[\'CLASSIFIER_ACTIVATION\'],\n                                               kernel_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                               bias_regularizer=l2(params[\'WEIGHT_DECAY\']),\n                                               trainable=(params.get(\'TRAINABLE_DECODER\', True) or params.get(\'TRAIN_ONLY_LAST_LAYER\', True)),\n                                               ),\n                                         trainable=(params.get(\'TRAINABLE_DECODER\', True) or params.get(\'TRAIN_ONLY_LAST_LAYER\', True)),\n                                         name=self.ids_outputs[0])\n        softout = shared_FC_soft(out_layer)\n        self.model = Model(inputs=[src_text, next_words],\n                           outputs=softout,\n                           name=self.name + \'_training\')\n\n        if params.get(\'N_GPUS\', 1) > 1:\n            self.multi_gpu_model = multi_gpu_model(self.model, gpus=params[\'N_GPUS\'])\n        else:\n            self.multi_gpu_model = None\n\n        ##################################################################\n        #                         SAMPLING MODEL                         #\n        ##################################################################\n        # Now that we have the basic training model ready, let\'s prepare the model for applying decoding\n        # The beam-search model will include all the minimum required set of layers (decoder stage) which offer the\n        # possibility to generate the next state in the sequence given a pre-processed input (encoder stage)\n        # First, we need a model that outputs the preprocessed input\n        # for applying the initial forward pass\n\n        model_init_input = [src_text, next_words]\n        model_init_output = [softout, masked_src_multihead]\n\n        self.model_init = Model(inputs=model_init_input,\n                                outputs=model_init_output,\n                                name=self.name + \'_model_init\')\n\n        # Store inputs and outputs names for model_init\n        self.ids_inputs_init = self.ids_inputs\n\n        # first output must be the output probs.\n        self.ids_outputs_init = self.ids_outputs + [\'preprocessed_input\']\n\n        # Second, we need to build an additional model with the capability to have the following inputs:\n        #   - preprocessed_input\n        #   - prev_word\n        # and the following outputs:\n        #   - softmax probabilities\n\n        preprocessed_size = params[\'MODEL_SIZE\']\n\n        # Define inputs\n        preprocessed_annotations = Input(name=\'preprocessed_input\',\n                                         shape=tuple([None, preprocessed_size]),\n                                         dtype=\'float32\')\n        # Apply decoder\n        prev_state_below = state_below\n\n        # RIGHT TRANSFORMER BLOCK\n        for n_block in range(params[\'N_LAYERS_DECODER\']):\n            # Masked Multi-Head Attention block\n            trg_multihead = shared_trg_multihead_list[n_block]([prev_state_below, prev_state_below])\n\n            # Regularize\n            trg_multihead_dropout = shared_trg_dropout_multihead_list[n_block](trg_multihead)\n\n            # Add\n            trg_multihead_add = shared_trg_add_multihead_list[n_block]([prev_state_below, trg_multihead_dropout])\n\n            # And norm\n            trg_multihead_norm = shared_trg_norm_multihead_list[n_block](trg_multihead_add)\n\n            # Second Multi-Head Attention block\n            src_trg_multihead = shared_src_trg_multihead_list[n_block]([trg_multihead_norm,\n                                                                        preprocessed_annotations])\n\n            # Regularize\n            src_trg_multihead_dropout = shared_src_trg_dropout_multihead_list[n_block](src_trg_multihead)\n\n            # Add\n            src_trg_multihead_add = shared_src_trg_add_multihead_list[n_block]([src_trg_multihead_dropout,\n                                                                                trg_multihead_norm])\n\n            # And norm\n            src_trg_multihead_norm = shared_src_trg_norm_multihead_list[n_block](src_trg_multihead_add)\n\n            # FF\n            ff_src_trg_multihead = shared_ff_list[n_block](src_trg_multihead_norm)\n\n            # Regularize\n            ff_src_trg_multihead_dropout = shared_dropout_ff_list[n_block](ff_src_trg_multihead)\n\n            # Add\n            ff_src_trg_multihead_add = shared_add_ff_list[n_block]([ff_src_trg_multihead_dropout,\n                                                                    src_trg_multihead_norm])\n\n            # And norm\n            ff_src_trg_multihead_norm = shared_norm_ff_list[n_block](ff_src_trg_multihead_add)\n\n            prev_state_below = ff_src_trg_multihead_norm\n\n        out_layer = prev_state_below\n\n        for (deep_out_layer, reg_list) in zip(shared_deep_list, shared_reg_deep_list):\n            out_layer = deep_out_layer(out_layer)\n            for reg in reg_list:\n                out_layer = reg(out_layer)\n\n        # Softmax\n        softout = shared_FC_soft(out_layer)\n\n        model_next_inputs = [next_words, preprocessed_annotations]\n        model_next_outputs = [softout, preprocessed_annotations]\n\n        # if self.return_alphas:\n        #     model_next_outputs.append(alphas)\n\n        self.model_next = Model(inputs=model_next_inputs,\n                                outputs=model_next_outputs,\n                                name=self.name + \'_model_next\')\n\n        # Store inputs and outputs names for model_next\n        # first input must be previous word\n        self.ids_inputs_next = [self.ids_inputs[1]] + [\'preprocessed_input\']\n        # first output must be the output probs.\n        self.ids_outputs_next = self.ids_outputs + [\'preprocessed_input\']\n        # Input -> Output matchings from model_init to model_next and from model_next to model_next\n        self.matchings_init_to_next = {\'preprocessed_input\': \'preprocessed_input\'}\n        self.matchings_next_to_next = {\'preprocessed_input\': \'preprocessed_input\'}\n\n    # Backwards compatibility.\n    GroundHogModel = AttentionRNNEncoderDecoder\n'"
nmt_keras/training.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom six import iteritems\nfrom timeit import default_timer as timer\nimport logging\n\nimport os\n\nlogging.basicConfig(level=logging.INFO, format=\'[%(asctime)s] %(message)s\', datefmt=\'%d/%m/%Y %H:%M:%S\')\nlogger = logging.getLogger(__name__)\n\nfrom data_engine.prepare_data import build_dataset, update_dataset_from_file\nfrom keras_wrapper.cnn_model import updateModel\nfrom keras_wrapper.dataset import loadDataset, saveDataset\nfrom keras_wrapper.extra.read_write import dict2pkl\nfrom nmt_keras.model_zoo import TranslationModel\nfrom nmt_keras.build_callbacks import buildCallbacks\n\n\ndef train_model(params, load_dataset=None):\n    """"""\n    Training function.\n\n    Sets the training parameters from params.\n\n    Build or loads the model and launches the training.\n\n    :param dict params: Dictionary of network hyperparameters.\n    :param str load_dataset: Load dataset from file or build it from the parameters.\n    :return: None\n    """"""\n\n    if params[\'RELOAD\'] > 0:\n        logger.info(\'Resuming training.\')\n        # Load data\n        if load_dataset is None:\n            if params[\'REBUILD_DATASET\']:\n                logger.info(\'Rebuilding dataset.\')\n                dataset = build_dataset(params)\n            else:\n                logger.info(\'Updating dataset.\')\n                dataset = loadDataset(\n                    os.path.join(\n                        params[\'DATASET_STORE_PATH\'],\n                        \'Dataset_\' +\n                        params[\'DATASET_NAME\'] + \'_\' +\n                        params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n                )\n\n                epoch_offset = 0 if dataset.len_train == 0 else int(\n                    params[\'RELOAD\'] * params[\'BATCH_SIZE\'] / dataset.len_train)\n                params[\'EPOCH_OFFSET\'] = params[\'RELOAD\'] if params[\'RELOAD_EPOCH\'] else epoch_offset\n\n                for split, filename in iteritems(params[\'TEXT_FILES\']):\n                    dataset = update_dataset_from_file(dataset,\n                                                       os.path.join(params[\'DATA_ROOT_PATH\'],\n                                                                    filename + params[\'SRC_LAN\']),\n                                                       params,\n                                                       splits=list([split]),\n                                                       output_text_filename=os.path.join(params[\'DATA_ROOT_PATH\'],\n                                                                                         filename + params[\'TRG_LAN\']),\n                                                       remove_outputs=False,\n                                                       compute_state_below=True,\n                                                       recompute_references=True)\n                    dataset.name = params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\']\n                saveDataset(dataset, params[\'DATASET_STORE_PATH\'])\n\n        else:\n            logger.info(\'Reloading and using dataset.\')\n            dataset = loadDataset(load_dataset)\n    else:\n        # Load data\n        if load_dataset is None:\n            dataset = build_dataset(params)\n        else:\n            dataset = loadDataset(load_dataset)\n\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n\n    # Build model\n    set_optimizer = True if params[\'RELOAD\'] == 0 else False\n    clear_dirs = True if params[\'RELOAD\'] == 0 else False\n\n    # build new model\n    nmt_model = TranslationModel(params,\n                                 model_type=params[\'MODEL_TYPE\'],\n                                 verbose=params[\'VERBOSE\'],\n                                 model_name=params[\'MODEL_NAME\'],\n                                 vocabularies=dataset.vocabulary,\n                                 store_path=params[\'STORE_PATH\'],\n                                 set_optimizer=set_optimizer,\n                                 clear_dirs=clear_dirs)\n\n    # Define the inputs and outputs mapping from our Dataset instance to our model\n    inputMapping = dict()\n    for i, id_in in enumerate(params[\'INPUTS_IDS_DATASET\']):\n        pos_source = dataset.ids_inputs.index(id_in)\n        id_dest = nmt_model.ids_inputs[i]\n        inputMapping[id_dest] = pos_source\n    nmt_model.setInputsMapping(inputMapping)\n\n    outputMapping = dict()\n    for i, id_out in enumerate(params[\'OUTPUTS_IDS_DATASET\']):\n        pos_target = dataset.ids_outputs.index(id_out)\n        id_dest = nmt_model.ids_outputs[i]\n        outputMapping[id_dest] = pos_target\n    nmt_model.setOutputsMapping(outputMapping)\n\n    if params[\'RELOAD\'] > 0:\n        nmt_model = updateModel(nmt_model, params[\'STORE_PATH\'], params[\'RELOAD\'], reload_epoch=params[\'RELOAD_EPOCH\'])\n        nmt_model.setParams(params)\n        nmt_model.setOptimizer()\n        if params.get(\'EPOCH_OFFSET\') is None:\n            params[\'EPOCH_OFFSET\'] = params[\'RELOAD\'] if params[\'RELOAD_EPOCH\'] else \\\n                int(params[\'RELOAD\'] * params[\'BATCH_SIZE\'] / dataset.len_train)\n\n    # Store configuration as pkl\n    dict2pkl(params, os.path.join(params[\'STORE_PATH\'], \'config\'))\n\n    # Callbacks\n    callbacks = buildCallbacks(params, nmt_model, dataset)\n\n    # Training\n    total_start_time = timer()\n\n    logger.debug(\'Starting training!\')\n    training_params = {\'n_epochs\': params[\'MAX_EPOCH\'],\n                       \'batch_size\': params[\'BATCH_SIZE\'],\n                       \'homogeneous_batches\': params[\'HOMOGENEOUS_BATCHES\'],\n                       \'maxlen\': params[\'MAX_OUTPUT_TEXT_LEN\'],\n                       \'joint_batches\': params[\'JOINT_BATCHES\'],\n                       \'lr_decay\': params.get(\'LR_DECAY\', None),  # LR decay parameters\n                       \'initial_lr\': params.get(\'LR\', 1.0),\n                       \'reduce_each_epochs\': params.get(\'LR_REDUCE_EACH_EPOCHS\', True),\n                       \'start_reduction_on_epoch\': params.get(\'LR_START_REDUCTION_ON_EPOCH\', 0),\n                       \'lr_gamma\': params.get(\'LR_GAMMA\', 0.9),\n                       \'lr_reducer_type\': params.get(\'LR_REDUCER_TYPE\', \'linear\'),\n                       \'lr_reducer_exp_base\': params.get(\'LR_REDUCER_EXP_BASE\', 0),\n                       \'lr_half_life\': params.get(\'LR_HALF_LIFE\', 50000),\n                       \'lr_warmup_exp\': params.get(\'WARMUP_EXP\', -1.5),\n                       \'min_lr\': params.get(\'MIN_LR\', 1e-9),\n                       \'epochs_for_save\': params[\'EPOCHS_FOR_SAVE\'],\n                       \'verbose\': params[\'VERBOSE\'],\n                       \'eval_on_sets\': None,  # Unsupported for autorreggressive models\n                       \'n_parallel_loaders\': params[\'PARALLEL_LOADERS\'],\n                       \'extra_callbacks\': callbacks,\n                       \'reload_epoch\': params[\'RELOAD\'],\n                       \'epoch_offset\': params.get(\'EPOCH_OFFSET\', 0),\n                       \'data_augmentation\': params[\'DATA_AUGMENTATION\'],\n                       \'patience\': params.get(\'PATIENCE\', 0),  # early stopping parameters\n                       \'metric_check\': params.get(\'STOP_METRIC\', None) if params.get(\'EARLY_STOP\', False) else None,\n                       \'min_delta\': params.get(\'MIN_DELTA\', 0.),\n                       \'eval_on_epochs\': params.get(\'EVAL_EACH_EPOCHS\', True),\n                       \'each_n_epochs\': params.get(\'EVAL_EACH\', 1),\n                       \'start_eval_on_epoch\': params.get(\'START_EVAL_ON_EPOCH\', 0),\n                       \'n_gpus\': params.get(\'N_GPUS\', 1),\n                       \'tensorboard\': params.get(\'TENSORBOARD\', False),\n                       \'tensorboard_params\':\n                           {\n                               \'log_dir\': params.get(\'LOG_DIR\', \'tensorboard_logs\'),\n                               \'histogram_freq\': params.get(\'HISTOGRAM_FREQ\', 0),\n                               \'batch_size\': params.get(\'TENSORBOARD_BATCH_SIZE\', params[\'BATCH_SIZE\']),\n                               \'write_graph\': params.get(\'WRITE_GRAPH\', True),\n                               \'write_grads\': params.get(\'WRITE_GRADS\', False),\n                               \'write_images\': params.get(\'WRITE_IMAGES\', False),\n                               \'embeddings_freq\': None,\n                               \'embeddings_layer_names\': None,\n                               \'embeddings_metadata\': None,\n                               \'word_embeddings_labels\': None,\n                               \'update_freq\': params.get(\'UPDATE_FREQ\', \'epoch\')}\n                       }\n    nmt_model.trainNet(dataset, training_params)\n\n    total_end_time = timer()\n    time_difference = total_end_time - total_start_time\n    logger.info(\'In total is {0:.2f}s = {1:.2f}m\'.format(time_difference, time_difference / 60.0))\n'"
tests/__init__.py,0,b''
tests/test_config.py,0,"b'import tempfile\nimport shutil\n\n\ndef load_tests_params():\n    """"""\n    Loads the hyperparameters for tests.\n    :return parameters: Dictionary of loaded parameters.\n    """"""\n\n    # Input data params\n    TASK_NAME = \'EuTrans\'  # Task name.\n    DATASET_NAME = TASK_NAME  # Dataset name.\n    SRC_LAN = \'es\'  # Language of the source text.\n    TRG_LAN = \'en\'  # Language of the target text.\n    DATA_ROOT_PATH = \'examples/%s/\' % DATASET_NAME  # Path where data is stored.\n\n    # SRC_LAN or TRG_LAN will be added to the file names.\n    TEXT_FILES = {\'train\': \'dev.\',  # Data files.\n                  \'val\': \'dev.\',\n                  \'test\': \'dev.\'}\n\n    GLOSSARY = None  # Glossary location. If not None, it overwrites translations according to this glossary file\n\n    # Dataset class parameters\n    INPUTS_IDS_DATASET = [\'source_text\', \'state_below\']  # Corresponding inputs of the dataset.\n    OUTPUTS_IDS_DATASET = [\'target_text\']  # Corresponding outputs of the dataset.\n    INPUTS_IDS_MODEL = [\'source_text\', \'state_below\']  # Corresponding inputs of the built model.\n    OUTPUTS_IDS_MODEL = [\'target_text\']  # Corresponding outputs of the built model.\n    INPUTS_TYPES_DATASET = [\'text-features\',\n                            \'text-features\']  # Corresponding types of the data. \'text\' or \'text-features\' allowed.\n    OUTPUTS_TYPES_DATASET = [\'text-features\']  # They are equivalent, only differ on how the data is loaded.\n\n    # Evaluation params\n    METRICS = [\'sacrebleu\', \'perplexity\']  # Metric used for evaluating the model.\n    KERAS_METRICS = [\'perplexity\']  # Metrics to be logged by Keras during training (in addition to the loss).\n    EVAL_ON_SETS = [\'val\']  # Possible values: \'train\', \'val\' and \'test\' (external evaluator).\n    START_EVAL_ON_EPOCH = 0  # First epoch to start the model evaluation.\n    EVAL_EACH_EPOCHS = True  # Select whether evaluate between N epochs or N updates.\n    EVAL_EACH = 1  # Sets the evaluation frequency (epochs or updates).\n\n    # Search parameters\n    SAMPLING = \'max_likelihood\'  # Possible values: multinomial or max_likelihood (recommended).\n    TEMPERATURE = 1  # Multinomial sampling parameter.\n    BEAM_SEARCH = True  # Switches on-off the beam search procedure.\n    BEAM_SIZE = 6  # Beam size (in case of BEAM_SEARCH == True).\n    OPTIMIZED_SEARCH = True  # Compute annotations only a single time per sample.\n    SEARCH_PRUNING = False  # Apply pruning strategies to the beam search method.\n    # It will likely increase decoding speed, but decrease quality.\n    MAXLEN_GIVEN_X = False  # Generate translations of similar length to the source sentences.\n    MAXLEN_GIVEN_X_FACTOR = 2  # The hypotheses will have (as maximum) the number of words of the\n    # source sentence * LENGTH_Y_GIVEN_X_FACTOR.\n    MINLEN_GIVEN_X = False  # Generate translations of similar length to the source sentences.\n    MINLEN_GIVEN_X_FACTOR = 3  # The hypotheses will have (as minimum) the number of words of the\n    # source sentence / LENGTH_Y_GIVEN_X_FACTOR.\n\n    # Apply length and coverage decoding normalizations.\n    # See Section 7 from Wu et al. (2016) (https://arxiv.org/abs/1609.08144).\n    LENGTH_PENALTY = False  # Apply length penalty.\n    LENGTH_NORM_FACTOR = 0.2  # Length penalty factor.\n    COVERAGE_PENALTY = False  # Apply source coverage penalty.\n    COVERAGE_NORM_FACTOR = 0.2  # Coverage penalty factor.\n\n    # Alternative (simple) length normalization.\n    NORMALIZE_SAMPLING = False  # Normalize hypotheses scores according to their length:\n    ALPHA_FACTOR = .6  # Normalization according to |h|**ALPHA_FACTOR.\n\n    # Sampling params: Show some samples during training.\n    SAMPLE_ON_SETS = [\'train\', \'val\']  # Possible values: \'train\', \'val\' and \'test\'.\n    N_SAMPLES = 5  # Number of samples generated.\n    START_SAMPLING_ON_EPOCH = 1  # First epoch where to start the sampling counter.\n    SAMPLE_EACH_UPDATES = 300  # Sampling frequency (always in #updates).\n\n    # Unknown words treatment\n    POS_UNK = False  # Enable POS_UNK strategy for unknown words.\n    HEURISTIC = 0  # Heuristic to follow:\n    #     0: Replace the UNK by the correspondingly aligned source.\n    #     1: Replace the UNK by the translation (given by an external\n    #        dictionary) of the correspondingly aligned source.\n    #     2: Replace the UNK by the translation (given by an external\n    #        dictionary) of the correspondingly aligned source only if it\n    #        starts with a lowercase. Otherwise, copies the source word.\n    ALIGN_FROM_RAW = True  # Align using the full vocabulary or the short_list.\n\n    # Source -- Target pkl mapping (used for heuristics 1--2). See utils/build_mapping_file.sh for further info.\n    MAPPING = DATA_ROOT_PATH + \'/mapping.%s_%s.pkl\' % (SRC_LAN, TRG_LAN)\n\n    # Word representation params\n    TOKENIZATION_METHOD = \'tokenize_none\'  # Select which tokenization we\'ll apply.\n    # See Dataset class (from stager_keras_wrapper) for more info.\n    BPE_CODES_PATH = DATA_ROOT_PATH + \'/training_codes.joint\'  # If TOKENIZATION_METHOD = \'tokenize_bpe\',\n    # sets the path to the learned BPE codes.\n    DETOKENIZATION_METHOD = \'detokenize_none\'  # Select which de-tokenization method we\'ll apply.\n\n    APPLY_DETOKENIZATION = False  # Wheter we apply a detokenization method.\n\n    TOKENIZE_HYPOTHESES = True  # Whether we tokenize the hypotheses using the\n    # previously defined tokenization method.\n    TOKENIZE_REFERENCES = True  # Whether we tokenize the references using the\n    # previously defined tokenization method.\n\n    # Input image parameters\n    DATA_AUGMENTATION = False  # Apply data augmentation on input data (still unimplemented for text inputs).\n\n    # Text parameters\n    FILL = \'end\'  # Whether we pad the \'end\', the \'start\' of  the sentence with 0s. We can also \'center\' it.\n    PAD_ON_BATCH = True  # Whether we take as many timesteps as the longest sequence of\n    # the batch or a fixed size (MAX_OUTPUT_TEXT_LEN).\n    # Input text parameters\n    INPUT_VOCABULARY_SIZE = 0  # Size of the input vocabulary. Set to 0 for using all,\n    # otherwise it will be truncated to these most frequent words.\n    MIN_OCCURRENCES_INPUT_VOCAB = 0  # Minimum number of occurrences allowed for the words in the input vocabulary.\n    # Set to 0 for using them all.\n    MAX_INPUT_TEXT_LEN = 50  # Maximum length of the input sequence.\n\n    # Output text parameters\n    OUTPUT_VOCABULARY_SIZE = 0  # Size of the input vocabulary. Set to 0 for using all,\n    # otherwise it will be truncated to these most frequent words.\n    MIN_OCCURRENCES_OUTPUT_VOCAB = 0  # Minimum number of occurrences allowed for the words in the output vocabulary.\n    MAX_OUTPUT_TEXT_LEN = 10  # Maximum length of the output sequence.\n    # set to 0 if we want to use the whole answer as a single class.\n    MAX_OUTPUT_TEXT_LEN_TEST = MAX_OUTPUT_TEXT_LEN  # Maximum length of the output sequence during test time.\n\n    # Optimizer parameters (see model.compile() function).\n    LOSS = \'categorical_crossentropy\'\n    CLASSIFIER_ACTIVATION = \'softmax\'\n    SAMPLE_WEIGHTS = True  # Select whether we use a weights matrix (mask) for the data outputs\n    LABEL_SMOOTHING = 0.  # Epsilon value for label smoothing. Only valid for \'categorical_crossentropy\' loss. See arxiv.org/abs/1512.00567.\n\n    OPTIMIZER = \'Adam\'  # Optimizer. Supported optimizers: SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam.\n    LR = 0.001  # Learning rate. Recommended values - Adam 0.0002 - Adadelta 1.0.\n    CLIP_C = 5.  # During training, clip L2 norm of gradients to this value (0. means deactivated).\n    CLIP_V = 0.  # During training, clip absolute value of gradients to this value (0. means deactivated).\n    USE_TF_OPTIMIZER = True  # Use native Tensorflow\'s optimizer (only for the Tensorflow backend).\n\n    # Advanced parameters for optimizers. Default values are usually effective.\n    MOMENTUM = 0.  # Momentum value (for SGD optimizer).\n    NESTEROV_MOMENTUM = False  # Use Nesterov momentum (for SGD optimizer).\n    RHO = 0.9  # Rho value (for Adadelta and RMSprop optimizers).\n    BETA_1 = 0.9  # Beta 1 value (for Adam, Adamax Nadam optimizers).\n    BETA_2 = 0.999  # Beta 2 value (for Adam, Adamax Nadam optimizers).\n    AMSGRAD = False  # Whether to apply the AMSGrad variant of Adam (see https://openreview.net/pdf?id=ryQu7f-RZ).\n    EPSILON = 1e-8  # Optimizers epsilon value.\n    ACCUMULATE_GRADIENTS = 1  # Accumulate gradients for this number of batches. Currently only implemented for Adam.\n\n    # Learning rate schedule\n    LR_DECAY = None  # Frequency (number of epochs or updates) between LR annealings. Set to None for not decay the learning rate.\n    LR_GAMMA = 0.8  # Multiplier used for decreasing the LR.\n    LR_REDUCE_EACH_EPOCHS = False  # Reduce each LR_DECAY number of epochs or updates.\n    LR_START_REDUCTION_ON_EPOCH = 0  # Epoch to start the reduction.\n    LR_REDUCER_TYPE = \'exponential\'  # Function to reduce. \'linear\' and \'exponential\' implemented.\n    # Linear reduction: new_lr = lr * LR_GAMMA\n    # Exponential reduction: new_lr = lr * LR_REDUCER_EXP_BASE ** (current_nb / LR_HALF_LIFE) * LR_GAMMA\n    # Noam reduction: new_lr = lr * min(current_nb ** LR_REDUCER_EXP_BASE, current_nb * LR_HALF_LIFE ** WARMUP_EXP)\n    LR_REDUCER_EXP_BASE = -0.5  # Base for the exponential decay.\n    LR_HALF_LIFE = 100  # Factor/warmup steps for exponenital/noam decay.\n    WARMUP_EXP = -1.5  # Warmup steps for noam decay.\n    MIN_LR = 1e-9  # Minimum value allowed for the decayed LR\n\n    # Training parameters\n    MAX_EPOCH = 1  # Stop when computed this number of epochs.\n    BATCH_SIZE = 10  # Size of each minibatch.\n    N_GPUS = 1  # Number of GPUs to use. Only for Tensorflow backend. Each GPU will receive mini-batches of BATCH_SIZE / N_GPUS.\n\n    HOMOGENEOUS_BATCHES = False  # Use batches with homogeneous output lengths (Dangerous!!).\n    JOINT_BATCHES = 4  # When using homogeneous batches, get this number of batches to sort.\n    PARALLEL_LOADERS = 1  # Parallel data batch loaders. Somewhat untested if > 1.\n    EPOCHS_FOR_SAVE = 1  # Number of epochs between model saves.\n    WRITE_VALID_SAMPLES = True  # Write valid samples in file.\n    SAVE_EACH_EVALUATION = True  # Save each time we evaluate the model.\n\n    # Early stop parameters\n    EARLY_STOP = True  # Turns on/off the early stop protocol.\n    PATIENCE = 10  # We\'ll stop if the val STOP_METRIC does not improve after this.\n    # number of evaluations.\n    STOP_METRIC = \'Bleu_4\'  # Metric for the stop.\n    MIN_DELTA = 0.  # Minimum change in the monitored quantity to consider it as an improvement.\n\n    # Model parameters\n    MODEL_TYPE = \'AttentionRNNEncoderDecoder\'  # Model to train. See model_zoo.py for more info.\n    # Supported architectures: \'AttentionRNNEncoderDecoder\' and \'Transformer\'.\n\n    # Common hyperparameters for all models\n    # # # # # # # # # # # # # # # # # # # # # # # #\n    TRAINABLE_ENCODER = True  # Whether the encoder\'s weights should be modified during training.\n    TRAINABLE_DECODER = True  # Whether the decoder\'s weights should be modified during training.\n\n    # Initializers (see keras/initializations.py).\n    INIT_FUNCTION = \'glorot_uniform\'  # General initialization function for matrices.\n    INNER_INIT = \'orthogonal\'  # Initialization function for inner RNN matrices.\n    INIT_ATT = \'glorot_uniform\'  # Initialization function for attention mechism matrices\n\n    SOURCE_TEXT_EMBEDDING_SIZE = 8  # Source language word embedding size.\n    SRC_PRETRAINED_VECTORS = None  # Path to pretrained vectors (e.g.: DATA_ROOT_PATH + \'/DATA/word2vec.%s.npy\' % SRC_LAN).\n    # Set to None if you don\'t want to use pretrained vectors.\n    # When using pretrained word embeddings. this parameter must match with the word embeddings size\n    SRC_PRETRAINED_VECTORS_TRAINABLE = True  # Finetune or not the target word embedding vectors.\n\n    TARGET_TEXT_EMBEDDING_SIZE = 8  # Source language word embedding size.\n    TRG_PRETRAINED_VECTORS = None  # Path to pretrained vectors. (e.g. DATA_ROOT_PATH + \'/DATA/word2vec.%s.npy\' % TRG_LAN)\n    # Set to None if you don\'t want to use pretrained vectors.\n    # When using pretrained word embeddings, the size of the pretrained word embeddings must match with the word embeddings size.\n    TRG_PRETRAINED_VECTORS_TRAINABLE = True  # Finetune or not the target word embedding vectors.\n\n    SCALE_SOURCE_WORD_EMBEDDINGS = False  # Scale source word embeddings by Sqrt(SOURCE_TEXT_EMBEDDING_SIZE).\n    SCALE_TARGET_WORD_EMBEDDINGS = False  # Scale target word embeddings by Sqrt(TARGET_TEXT_EMBEDDING_SIZE).\n\n    TIE_EMBEDDINGS = False  # Use the same embeddings for source and target language.\n\n    N_LAYERS_ENCODER = 1  # Stack this number of encoding layers.\n    N_LAYERS_DECODER = 1  # Stack this number of decoding layers.\n\n    # Additional Fully-Connected layers applied before softmax.\n    #       Here we should specify the activation function and the output dimension.\n    #       (e.g DEEP_OUTPUT_LAYERS = [(\'tanh\', 600), (\'relu\', 400), (\'relu\', 200)])\n    DEEP_OUTPUT_LAYERS = [(\'linear\', TARGET_TEXT_EMBEDDING_SIZE)]\n    # # # # # # # # # # # # # # # # # # # # # # # #\n\n    # AttentionRNNEncoderDecoder model hyperparameters\n    # # # # # # # # # # # # # # # # # # # # # # # #\n    ENCODER_RNN_TYPE = \'LSTM\'  # Encoder\'s RNN unit type (\'LSTM\' and \'GRU\' supported).\n    USE_CUDNN = False  # Use CuDNN\'s implementation of GRU and LSTM (only for Tensorflow backend).\n\n    DECODER_RNN_TYPE = \'ConditionalLSTM\'  # Decoder\'s RNN unit type.\n    # (\'LSTM\', \'GRU\', \'ConditionalLSTM\' and \'ConditionalGRU\' supported).\n    ATTENTION_MODE = \'add\'  # Attention mode. \'add\' (Bahdanau-style), \'dot\' (Luong-style) or \'scaled-dot\'.\n\n    # Encoder configuration\n    ENCODER_HIDDEN_SIZE = 4  # For models with RNN encoder.\n    BIDIRECTIONAL_ENCODER = True  # Use bidirectional encoder.\n    BIDIRECTIONAL_DEEP_ENCODER = True  # Use bidirectional encoder in all encoding layers.\n    BIDIRECTIONAL_MERGE_MODE = \'concat\'  # Merge function for bidirectional layers.\n\n    # Fully-Connected layers for initializing the first decoder RNN state.\n    #       Here we should only specify the activation function of each layer (as they have a potentially fixed size)\n    #       (e.g INIT_LAYERS = [\'tanh\', \'relu\'])\n    INIT_LAYERS = [\'tanh\']\n\n    # Decoder configuration\n    DECODER_HIDDEN_SIZE = 4  # For models with RNN decoder.\n    ATTENTION_SIZE = DECODER_HIDDEN_SIZE\n\n    # Skip connections parameters\n    SKIP_VECTORS_HIDDEN_SIZE = TARGET_TEXT_EMBEDDING_SIZE  # Hidden size.\n    ADDITIONAL_OUTPUT_MERGE_MODE = \'Add\'  # Merge mode for the skip-connections (see keras.layers.merge.py).\n    SKIP_VECTORS_SHARED_ACTIVATION = \'tanh\'  # Activation for the skip vectors.\n    # # # # # # # # # # # # # # # # # # # # # # # #\n\n    # Transformer model hyperparameters\n    # # # # # # # # # # # # # # # # # # # # # # # #\n    MODEL_SIZE = 32  # Transformer model size (d_{model} in de paper).\n    MULTIHEAD_ATTENTION_ACTIVATION = \'linear\'  # Activation the input projections in the Multi-Head Attention blocks.\n    FF_SIZE = MODEL_SIZE * 4  # Size of the feed-forward layers of the Transformer model.\n    N_HEADS = 8  # Number of parallel attention layers of the Transformer model.\n    # # # # # # # # # # # # # # # # # # # # # # # #\n\n    # Regularizers\n    REGULARIZATION_FN = \'L2\'  # Regularization function. \'L1\', \'L2\' and \'L1_L2\' supported.\n    WEIGHT_DECAY = 1e-4  # Regularization coefficient.\n    RECURRENT_WEIGHT_DECAY = 1e-4  # Regularization coefficient in recurrent layers.\n\n    DROPOUT_P = 0.01  # Percentage of units to drop (0 means no dropout).\n    RECURRENT_INPUT_DROPOUT_P = 0.01  # Percentage of units to drop in input cells of recurrent layers.\n    RECURRENT_DROPOUT_P = 0.01  # Percentage of units to drop in recurrent layers.\n    ATTENTION_DROPOUT_P = 0.  # Percentage of units to drop in attention layers (0 means no dropout).\n\n    USE_NOISE = True  # Use gaussian noise during training.\n    NOISE_AMOUNT = 0.01  # Amount of noise.\n\n    USE_BATCH_NORMALIZATION = True  # If True it is recommended to deactivate Dropout.\n    BATCH_NORMALIZATION_MODE = 1  # See documentation in Keras\' BN.\n\n    USE_PRELU = False  # use PReLU activations as regularizer.\n    USE_L1 = False  # L1 normalization on the features.\n    USE_L2 = False  # L2 normalization on the features.\n\n    DOUBLE_STOCHASTIC_ATTENTION_REG = 0.7  # Doubly stochastic attention (Eq. 14 from arXiv:1502.03044).\n\n    # Results plot and models storing parameters.\n    EXTRA_NAME = \'\'  # This will be appended to the end of the model name.\n    if MODEL_TYPE == \'AttentionRNNEncoderDecoder\':\n        MODEL_NAME = TASK_NAME + \'_\' + SRC_LAN + TRG_LAN + \'_\' + MODEL_TYPE + \\\n                     \'_src_emb_\' + str(SOURCE_TEXT_EMBEDDING_SIZE) + \\\n                     \'_bidir_\' + str(BIDIRECTIONAL_ENCODER) + \\\n                     \'_enc_\' + ENCODER_RNN_TYPE + \'_\' + str(ENCODER_HIDDEN_SIZE) + \\\n                     \'_dec_\' + DECODER_RNN_TYPE + \'_\' + str(DECODER_HIDDEN_SIZE) + \\\n                     \'_deepout_\' + \'_\'.join([layer[0] for layer in DEEP_OUTPUT_LAYERS]) + \\\n                     \'_trg_emb_\' + str(TARGET_TEXT_EMBEDDING_SIZE) + \\\n                     \'_\' + OPTIMIZER + \'_\' + str(LR)\n    elif MODEL_TYPE == \'Transformer\':\n        MODEL_NAME = TASK_NAME + \'_\' + SRC_LAN + TRG_LAN + \'_\' + MODEL_TYPE + \\\n                     \'_model_size_\' + str(MODEL_SIZE) + \\\n                     \'_ff_size_\' + str(FF_SIZE) + \\\n                     \'_num_heads_\' + str(N_HEADS) + \\\n                     \'_encoder_blocks_\' + str(N_LAYERS_ENCODER) + \\\n                     \'_decoder_blocks_\' + str(N_LAYERS_DECODER) + \\\n                     \'_deepout_\' + \'_\'.join([layer[0] for layer in DEEP_OUTPUT_LAYERS]) + \\\n                     \'_\' + OPTIMIZER + \'_\' + str(LR)\n    else:\n        MODEL_NAME = TASK_NAME + \'_\' + SRC_LAN + TRG_LAN + \'_\' + \\\n                     MODEL_TYPE + \'_\' + OPTIMIZER + \'_\' + str(LR)\n\n    MODEL_NAME += EXTRA_NAME\n\n    STORE_PATH = tempfile.mkdtemp()  # Models and evaluation results will be stored here.\n    DATASET_STORE_PATH = tempfile.mkdtemp()  # Dataset instance will be stored here.\n\n    # Tensorboard configuration. Only if the backend is Tensorflow. Otherwise, it will be ignored.\n    TENSORBOARD = True  # Switches On/Off the tensorboard callback.\n    LOG_DIR = \'tensorboard_logs\'  # Directory to store teh model. Will be created inside STORE_PATH.\n    EMBEDDINGS_FREQ = 1  # Frequency (in epochs) at which selected embedding layers will be saved.\n\n    SAMPLING_SAVE_MODE = \'list\'  # \'list\': Store in a text file, one sentence per line.\n    PLOT_EVALUATION = False  # If True, the evaluation will be plotted into the model folder.\n\n    VERBOSE = 1  # Verbosity level.\n    RELOAD = 0  # If 0 start training from scratch, otherwise the model.\n    # Saved on epoch \'RELOAD\' will be used.\n    RELOAD_EPOCH = True  # Select whether we reload epoch or update number.\n\n    REBUILD_DATASET = True  # Build again or use stored instance.\n    MODE = \'training\'  # \'training\' or \'sampling\' (if \'sampling\' then RELOAD must\n    # be greater than 0 and EVAL_ON_SETS will be used).\n\n    # Extra parameters for special trainings. In most cases, they should be set to `False`\n    TRAIN_ON_TRAINVAL = False  # train the model on both training and validation sets combined.\n    FORCE_RELOAD_VOCABULARY = False  # force building a new vocabulary from the training samples\n    # applicable if RELOAD > 1\n    # ================================================ #\n    parameters = locals().copy()\n    return parameters\n\n\ndef load_transformer_test_params():\n    params = load_tests_params()\n    params[\'MODEL_TYPE\'] = \'Transformer\'\n    params[\'N_LAYERS_ENCODER\'] = 2\n    params[\'N_LAYERS_DECODER\'] = 2\n    params[\'MULTIHEAD_ATTENTION_ACTIVATION\'] = \'relu\'\n    params[\'MODEL_SIZE\'] = 8\n    params[\'FF_SIZE\'] = params[\'MODEL_SIZE\'] * 4\n    params[\'N_HEADS\'] = 2\n    params[\'REBUILD_DATASET\'] = True\n    params[\'OPTIMIZED_SEARCH\'] = True\n    params[\'POS_UNK\'] = False\n    return params\n\n\ndef clean_dirs(params):\n    shutil.rmtree(params[\'STORE_PATH\'])\n    shutil.rmtree(params[\'DATASET_STORE_PATH\'])\n'"
tests/test_load_params.py,0,"b""import pytest\nfrom config import load_parameters\n\n\ndef test_load_params():\n    params = load_parameters()\n    assert isinstance(params, dict)\n\n\ndef test_params_exist():\n    params = load_parameters()\n    assert 'DATASET_NAME' in list(params)\n    assert 'SRC_LAN' in list(params)\n    assert 'TRG_LAN' in list(params)\n    assert 'DATA_ROOT_PATH' in list(params)\n    assert 'TEXT_FILES' in list(params)\n    assert 'INPUTS_IDS_DATASET' in list(params)\n    assert 'OUTPUTS_IDS_DATASET' in list(params)\n    assert 'INPUTS_IDS_MODEL' in list(params)\n    assert 'OUTPUTS_IDS_MODEL' in list(params)\n    assert 'METRICS' in list(params)\n    assert 'EVAL_ON_SETS' in list(params)\n    assert 'START_EVAL_ON_EPOCH' in list(params)\n    assert 'EVAL_EACH_EPOCHS' in list(params)\n    assert 'EVAL_EACH' in list(params)\n    assert 'SAMPLING' in list(params)\n    assert 'TEMPERATURE' in list(params)\n    assert 'BEAM_SEARCH' in list(params)\n    assert 'BEAM_SIZE' in list(params)\n    assert 'OPTIMIZED_SEARCH' in list(params)\n    assert 'LENGTH_PENALTY' in list(params)\n    assert 'LENGTH_NORM_FACTOR' in list(params)\n    assert 'COVERAGE_PENALTY' in list(params)\n    assert 'COVERAGE_NORM_FACTOR' in list(params)\n    assert 'NORMALIZE_SAMPLING' in list(params)\n    assert 'ALPHA_FACTOR' in list(params)\n    assert 'SAMPLE_ON_SETS' in list(params)\n    assert 'N_SAMPLES' in list(params)\n    assert 'START_SAMPLING_ON_EPOCH' in list(params)\n    assert 'SAMPLE_EACH_UPDATES' in list(params)\n    assert 'POS_UNK' in list(params)\n    assert 'HEURISTIC' in list(params)\n    assert 'ALIGN_FROM_RAW' in list(params)\n    assert 'MAPPING' in list(params)\n    assert 'TOKENIZATION_METHOD' in list(params)\n    assert 'DETOKENIZATION_METHOD' in list(params)\n    assert 'APPLY_DETOKENIZATION' in list(params)\n    assert 'TOKENIZE_HYPOTHESES' in list(params)\n    assert 'TOKENIZE_REFERENCES' in list(params)\n    assert 'DATA_AUGMENTATION' in list(params)\n    assert 'FILL' in list(params)\n    assert 'PAD_ON_BATCH' in list(params)\n    assert 'INPUT_VOCABULARY_SIZE' in list(params)\n    assert 'MIN_OCCURRENCES_INPUT_VOCAB' in list(params)\n    assert 'MAX_INPUT_TEXT_LEN' in list(params)\n    assert 'OUTPUT_VOCABULARY_SIZE' in list(params)\n    assert 'MIN_OCCURRENCES_OUTPUT_VOCAB' in list(params)\n    assert 'MAX_OUTPUT_TEXT_LEN' in list(params)\n    assert 'MAX_OUTPUT_TEXT_LEN_TEST' in list(params)\n    assert 'LOSS' in list(params)\n    assert 'CLASSIFIER_ACTIVATION' in list(params)\n    assert 'OPTIMIZER' in list(params)\n    assert 'LR' in list(params)\n    assert 'CLIP_C' in list(params)\n    assert 'CLIP_V' in list(params)\n    assert 'SAMPLE_WEIGHTS' in list(params)\n    assert 'LR_DECAY' in list(params)\n    assert 'LR_GAMMA' in list(params)\n    assert 'MAX_EPOCH' in list(params)\n    assert 'BATCH_SIZE' in list(params)\n    assert 'HOMOGENEOUS_BATCHES' in list(params)\n    assert 'JOINT_BATCHES' in list(params)\n    assert 'PARALLEL_LOADERS' in list(params)\n    assert 'EPOCHS_FOR_SAVE' in list(params)\n    assert 'WRITE_VALID_SAMPLES' in list(params)\n    assert 'SAVE_EACH_EVALUATION' in list(params)\n    assert 'EARLY_STOP' in list(params)\n    assert 'PATIENCE' in list(params)\n    assert 'STOP_METRIC' in list(params)\n    assert 'MIN_DELTA' in list(params)\n    assert 'MODEL_TYPE' in list(params)\n    assert 'ENCODER_RNN_TYPE' in list(params)\n    assert 'DECODER_RNN_TYPE' in list(params)\n    assert 'INIT_FUNCTION' in list(params)\n    assert 'SOURCE_TEXT_EMBEDDING_SIZE' in list(params)\n    assert 'SRC_PRETRAINED_VECTORS' in list(params)\n    assert 'SRC_PRETRAINED_VECTORS_TRAINABLE' in list(params)\n    assert 'TARGET_TEXT_EMBEDDING_SIZE' in list(params)\n    assert 'TRG_PRETRAINED_VECTORS' in list(params)\n    assert 'TRG_PRETRAINED_VECTORS_TRAINABLE' in list(params)\n    assert 'ENCODER_HIDDEN_SIZE' in list(params)\n    assert 'BIDIRECTIONAL_ENCODER' in list(params)\n    assert 'N_LAYERS_ENCODER' in list(params)\n    assert 'BIDIRECTIONAL_DEEP_ENCODER' in list(params)\n    assert 'DECODER_HIDDEN_SIZE' in list(params)\n    assert 'N_LAYERS_DECODER' in list(params)\n    assert 'ADDITIONAL_OUTPUT_MERGE_MODE' in list(params)\n    assert 'ATTENTION_SIZE' in list(params)\n    assert 'SKIP_VECTORS_HIDDEN_SIZE' in list(params)\n    assert 'INIT_LAYERS' in list(params)\n    assert 'DEEP_OUTPUT_LAYERS' in list(params)\n    assert 'WEIGHT_DECAY' in list(params)\n    assert 'RECURRENT_WEIGHT_DECAY' in list(params)\n    assert 'DROPOUT_P' in list(params)\n    assert 'RECURRENT_INPUT_DROPOUT_P' in list(params)\n    assert 'RECURRENT_DROPOUT_P' in list(params)\n    assert 'USE_NOISE' in list(params)\n    assert 'NOISE_AMOUNT' in list(params)\n    assert 'USE_BATCH_NORMALIZATION' in list(params)\n    assert 'BATCH_NORMALIZATION_MODE' in list(params)\n    assert 'USE_PRELU' in list(params)\n    assert 'USE_L2' in list(params)\n    assert 'EXTRA_NAME' in list(params)\n    assert 'MODEL_NAME' in list(params)\n    assert 'STORE_PATH' in list(params)\n    assert 'DATASET_STORE_PATH' in list(params)\n    assert 'SAMPLING_SAVE_MODE' in list(params)\n    assert 'VERBOSE' in list(params)\n    assert 'RELOAD' in list(params)\n    assert 'RELOAD_EPOCH' in list(params)\n    assert 'REBUILD_DATASET' in list(params)\n    assert 'MODE' in list(params)\n    assert 'TRAIN_ON_TRAINVAL' in list(params)\n    assert 'FORCE_RELOAD_VOCABULARY' in list(params)\n\n\ndef test_params_type():\n    params = load_parameters()\n    assert isinstance(params['DATASET_NAME'], str)\n    assert isinstance(params['SRC_LAN'], str)\n    assert isinstance(params['TRG_LAN'], str)\n    assert isinstance(params['DATA_ROOT_PATH'], str)\n    assert isinstance(params['TEXT_FILES'], dict)\n    assert isinstance(params['INPUTS_IDS_DATASET'], list)\n    assert isinstance(params['OUTPUTS_IDS_DATASET'], list)\n    assert isinstance(params['INPUTS_IDS_MODEL'], list)\n    assert isinstance(params['OUTPUTS_IDS_MODEL'], list)\n    assert isinstance(params['METRICS'], list)\n    assert isinstance(params['EVAL_ON_SETS'], list)\n    assert isinstance(params['START_EVAL_ON_EPOCH'], int)\n    assert isinstance(params['EVAL_EACH_EPOCHS'], bool)\n    assert isinstance(params['EVAL_EACH'], int)\n    assert isinstance(params['SAMPLING'], str)\n    assert isinstance(params['TEMPERATURE'], int)\n    assert isinstance(params['BEAM_SEARCH'], bool)\n    assert isinstance(params['BEAM_SIZE'], int)\n    assert isinstance(params['OPTIMIZED_SEARCH'], bool)\n    assert isinstance(params['LENGTH_PENALTY'], bool)\n    assert isinstance(params['LENGTH_NORM_FACTOR'], float)\n    assert isinstance(params['COVERAGE_PENALTY'], bool)\n    assert isinstance(params['COVERAGE_NORM_FACTOR'], float)\n    assert isinstance(params['NORMALIZE_SAMPLING'], bool)\n    assert isinstance(params['ALPHA_FACTOR'], float)\n    assert isinstance(params['SAMPLE_ON_SETS'], list)\n    assert isinstance(params['N_SAMPLES'], int)\n    assert isinstance(params['START_SAMPLING_ON_EPOCH'], int)\n    assert isinstance(params['SAMPLE_EACH_UPDATES'], int)\n    assert isinstance(params['POS_UNK'], bool)\n    assert isinstance(params['HEURISTIC'], int)\n    assert isinstance(params['ALIGN_FROM_RAW'], bool)\n    assert isinstance(params['MAPPING'], str)\n    assert isinstance(params['TOKENIZATION_METHOD'], str)\n    assert isinstance(params['DETOKENIZATION_METHOD'], str)\n    assert isinstance(params['APPLY_DETOKENIZATION'], bool)\n    assert isinstance(params['TOKENIZE_HYPOTHESES'], bool)\n    assert isinstance(params['TOKENIZE_REFERENCES'], bool)\n    assert isinstance(params['DATA_AUGMENTATION'], bool)\n    assert isinstance(params['FILL'], str)\n    assert isinstance(params['PAD_ON_BATCH'], bool)\n    assert isinstance(params['INPUT_VOCABULARY_SIZE'], int)\n    assert isinstance(params['MIN_OCCURRENCES_INPUT_VOCAB'], int)\n    assert isinstance(params['MAX_INPUT_TEXT_LEN'], int)\n    assert isinstance(params['OUTPUT_VOCABULARY_SIZE'], int)\n    assert isinstance(params['MIN_OCCURRENCES_OUTPUT_VOCAB'], int)\n    assert isinstance(params['MAX_OUTPUT_TEXT_LEN'], int)\n    assert isinstance(params['MAX_OUTPUT_TEXT_LEN_TEST'], int)\n    assert isinstance(params['LOSS'], str)\n    assert isinstance(params['CLASSIFIER_ACTIVATION'], str)\n    assert isinstance(params['OPTIMIZER'], str)\n    assert isinstance(params['LR'], float)\n    assert isinstance(params['CLIP_C'], float)\n    assert isinstance(params['CLIP_V'], float)\n    assert isinstance(params['SAMPLE_WEIGHTS'], bool)\n    assert params['LR_DECAY'] is None\n    assert isinstance(params['LR_GAMMA'], float)\n    assert isinstance(params['MAX_EPOCH'], int)\n    assert isinstance(params['BATCH_SIZE'], int)\n    assert isinstance(params['HOMOGENEOUS_BATCHES'], bool)\n    assert isinstance(params['JOINT_BATCHES'], int)\n    assert isinstance(params['PARALLEL_LOADERS'], int)\n    assert isinstance(params['EPOCHS_FOR_SAVE'], int)\n    assert isinstance(params['WRITE_VALID_SAMPLES'], bool)\n    assert isinstance(params['SAVE_EACH_EVALUATION'], bool)\n    assert isinstance(params['EARLY_STOP'], bool)\n    assert isinstance(params['PATIENCE'], int)\n    assert isinstance(params['STOP_METRIC'], str)\n    assert isinstance(params['MIN_DELTA'], float)\n    assert isinstance(params['MODEL_TYPE'], str)\n    assert isinstance(params['ENCODER_RNN_TYPE'], str)\n    assert isinstance(params['DECODER_RNN_TYPE'], str)\n    assert isinstance(params['INIT_FUNCTION'], str)\n    assert isinstance(params['SOURCE_TEXT_EMBEDDING_SIZE'], int)\n    assert params['SRC_PRETRAINED_VECTORS'] is None\n    assert isinstance(params['SRC_PRETRAINED_VECTORS_TRAINABLE'], bool)\n    assert isinstance(params['TARGET_TEXT_EMBEDDING_SIZE'], int)\n    assert params['TRG_PRETRAINED_VECTORS'] is None\n    assert isinstance(params['ENCODER_HIDDEN_SIZE'], int)\n    assert isinstance(params['BIDIRECTIONAL_ENCODER'], bool)\n    assert isinstance(params['N_LAYERS_ENCODER'], int)\n    assert isinstance(params['BIDIRECTIONAL_DEEP_ENCODER'], bool)\n    assert isinstance(params['DECODER_HIDDEN_SIZE'], int)\n    assert isinstance(params['N_LAYERS_DECODER'], int)\n    assert isinstance(params['ATTENTION_SIZE'], int)\n    assert isinstance(params['ADDITIONAL_OUTPUT_MERGE_MODE'], str)\n    assert isinstance(params['SKIP_VECTORS_HIDDEN_SIZE'], int)\n    assert isinstance(params['INIT_LAYERS'], list)\n    assert isinstance(params['DEEP_OUTPUT_LAYERS'], list)\n    assert isinstance(params['WEIGHT_DECAY'], float)\n    assert isinstance(params['RECURRENT_WEIGHT_DECAY'], float)\n    assert isinstance(params['DROPOUT_P'], float)\n    assert isinstance(params['RECURRENT_INPUT_DROPOUT_P'], float)\n    assert isinstance(params['RECURRENT_DROPOUT_P'], float)\n    assert isinstance(params['USE_NOISE'], bool)\n    assert isinstance(params['NOISE_AMOUNT'], float)\n    assert isinstance(params['USE_BATCH_NORMALIZATION'], bool)\n    assert isinstance(params['BATCH_NORMALIZATION_MODE'], int)\n    assert isinstance(params['USE_PRELU'], bool)\n    assert isinstance(params['USE_L2'], bool)\n    assert isinstance(params['EXTRA_NAME'], str)\n    assert isinstance(params['MODEL_NAME'], str)\n    assert isinstance(params['STORE_PATH'], str)\n    assert isinstance(params['DATASET_STORE_PATH'], str)\n    assert isinstance(params['SAMPLING_SAVE_MODE'], str)\n    assert isinstance(params['VERBOSE'], int)\n    assert isinstance(params['RELOAD'], int)\n    assert isinstance(params['RELOAD_EPOCH'], bool)\n    assert isinstance(params['REBUILD_DATASET'], bool)\n    assert isinstance(params['MODE'], str)\n    assert isinstance(params['TRAIN_ON_TRAINVAL'], bool)\n    assert isinstance(params['FORCE_RELOAD_VOCABULARY'], bool)\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
utils/__init__.py,0,b''
utils/average_models.py,0,"b'import argparse\nimport logging\nimport sys\nimport os\nfrom keras_wrapper.utils import average_models\nsys.path.insert(1, os.path.abspath("".""))\nsys.path.insert(0, os.path.abspath(""../""))\n\nlogging.basicConfig(level=logging.INFO, format=\'[%(asctime)s] %(message)s\', datefmt=\'%d/%m/%Y %H:%M:%S\')\nlogger = logging.getLogger(__name__)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(""Averages models"")\n\n    parser.add_argument(""-d"", ""--dest"",\n                        default=\'./model\',\n                        required=False,\n                        help=""Path to the averaged model. If not specified, the model is saved in \'./model\'."")\n    parser.add_argument(""-v"", ""--verbose"", required=False, default=0, type=int, help=""Verbosity level"")\n    parser.add_argument(""-w"", ""--weights"", nargs=""*"", help=""Weight given to each model in the averaging. You should provide the same number of weights than models.""\n                                                           ""By default, it applies the same weight to each model (1/N)."", default=[])\n    parser.add_argument(""-m"", ""--models"", nargs=""+"", required=True, help=""Path to the models"")\n    return parser.parse_args()\n\n\ndef weighted_average(args):\n    """"""\n    Apply a weighted average to the models.\n    :param args: Options for the averaging function:\n              * models: Path to the models.\n              * dest: Path to the averaged model. If unspecified, the model is saved in \'./model\'\n              * weights: Weight given to each model in the averaging. Should be the same number of weights than models.\n                         If unspecified, it applies the same weight to each model (1/N).\n    :return:\n    """"""\n    logger.info(""Averaging %d models"" % len(args.models))\n    average_models(args.models, args.dest, weights=args.weights)\n    logger.info(\'Averaging finished.\')\n\n\nif __name__ == ""__main__"":\n\n    args = parse_args()\n    weighted_average(args)\n'"
utils/build_glossary.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom keras_wrapper.extra.read_write import dict2pkl\nimport argparse\n\n# Preprocess a glossary file with the format\n#   word <separator> desired_replacement\n# and stores them in a suitable format (.pkl)\n\n\ndef build_glossary(glossary_text_file, dest_filename, separator=\'\\t\'):\n    """"""\n    Preprocess a glossary file with the format\n        word <separator> desired_replacement\n    and stores them in a suitable format (.pkl)\n\n    :param glossary_text_file: Path to the glossary file.\n    :param dest_filename: Output filename.\n    :param separator: Separator between words and replacements\n    """"""\n    glossary = dict()\n    print (""Reading glossary from %s"" % glossary_text_file)\n    for glossary_line in open(glossary_text_file).read().splitlines():\n        split_line = glossary_line.split(separator)\n        glossary[split_line[0]] = \' \'.join(split_line[1:])\n    print (""Done. Saving glossary into %s"" % dest_filename)\n    dict2pkl(glossary, dest_filename)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(""Process a glossary text file. A glossary file is a mapping of words specifying ""\n                                     ""its translation."")\n    parser.add_argument(""-g"", ""--glossary"", required=True, help=""Glossary text file. A word mapping per line."")\n    parser.add_argument(""-d"", ""--destination"", required=True, help=""Destination file."", default=\'word2vec.en\')\n    parser.add_argument(""-s"", ""--separator"", required=False, default=\' \', help=""Separator of the glossary file"")\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    build_glossary(args.glossary, args.destination, separator=args.separator)\n'"
utils/config_pkl2py.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom six import iteritems\nimport argparse\nimport ast\nimport sys\nfrom keras_wrapper.extra.read_write import pkl2dict\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(""Rebuilds a python file (like config.py) from a given config instance."")\n    parser.add_argument(""-c"", ""--config"", required=False, help=""Config pkl for loading the model configuration. ""\n                                                               ""If not specified, hyperparameters ""\n                                                               ""are read from config.py"")\n    parser.add_argument(""-d"", ""--dest"", required=False, type=str,\n                        default=None, help=""Destination file. If unspecidied, standard output"")\n    parser.add_argument(""-ch"", ""--changes"", nargs=""*"", help=""Changes to the config. Following the syntax Key=Value"",\n                        default="""")\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n\n    args = parse_args()\n    if args.config is None:\n        from config import load_parameters\n\n        params = load_parameters()\n    else:\n        params = pkl2dict(args.config)\n    try:\n        for arg in args.changes:\n            try:\n                k, v = arg.split(\'=\')\n            except ValueError:\n                print (\'Overwritten arguments must have the form key=Value. \\n Currently are: %s\' % str(args.changes))\n                exit(1)\n            try:\n                params[k] = ast.literal_eval(v)\n            except ValueError:\n                params[k] = v\n    except ValueError:\n        print (\'Error processing arguments: (\', k, "","", v, "")"")\n        exit(2)\n\n    if args.dest is not None:\n        print (args.dest)\n        output = open(args.dest, \'w\')\n    else:\n        output = sys.stdout\n\n    # Print header\n    output.write(\'def load_parameters():\\n\')\n    output.write(\'\\t""""""\\n\')\n    output.write(\'\\tLoads the defined hyperparameters\\n\')\n    output.write(\'\\t:return parameters: Dictionary of loaded parameters\\n\')\n    output.write(\'\\t""""""\\n\')\n    for key, value in iteritems(params):\n        output.write(\'\\t\' + key + \'=\' + str(value) + \'\\n\')\n    # Print ending\n    output.write(\'\\t# ================================================ #\\n\')\n    output.write(\'\\tparameters = locals().copy()\\n\')\n    output.write(\'\\treturn parameters\\n\')\n    if args.dest is not None:\n        output.close()\n'"
utils/format_corpus_for_aligner.py,0,"b'# Convert a tokenized parallel corpus into a format suitable for fast_align\n# Code partially taken from:\n#    https://github.com/sebastien-j/LV_groundhog/blob/master/experiments/nmt/utils/format_fast_align.py\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--source"", type=str)  # Use the tokenized text files # Source\nparser.add_argument(""--target"", type=str)  # Target\nparser.add_argument(""--dest"", type=str)\nparser.add_argument(""--aligner"", type=str, default=\'fast_align\')\n\nargs = parser.parse_args()\nif args.aligner == \'fast_align\':\n    with open(args.source, \'r\') as left:\n        with open(args.target, \'r\') as right:\n            with open(args.dest, \'w\') as final:\n                while True:\n                    lline = left.readline()\n                    rline = right.readline()\n                    if (lline == \'\') or (rline == \'\'):\n                        break\n                    assert (lline[-1] == \'\\n\')\n                    assert (rline[-1] == \'\\n\')\n                    if (lline != \'\\n\') and (rline != \'\\n\'):\n                        final.write(lline[:-1] + \' ||| \' + rline)\nelif args.aligner == \'giza\':\n    raise NotImplementedError(\'Giza alignments still not supported\')\nelse:\n    raise AttributeError(\'Option %s not supported\' % args.aligner)\n'"
utils/preprocess_binary_word_vectors.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport os\nimport numpy as np\nimport argparse\nfrom os.path import basename, dirname\n\n\n# Preprocess pretrained binary vectors\n# and stores them in a suitable format (.npy)\n\ndef word2vec2npy(v_path, base_path_save, dest_filename):\n    """"""\n    Preprocess pretrained binary vectors and stores them in a suitable format.\n    :param v_path: Path to the binary vectors file.\n    :param base_path_save: Path where the formatted vectors will be stored.\n    :param dest_filename: Filename of the formatted vectors.\n    """"""\n    word_vecs = dict()\n    print (""Loading vectors from %s"" % v_path)\n    with open(v_path, ""rb"") as f:\n        header = f.readline()\n        vocab_size, layer1_size = map(int, header.split())\n        binary_len = np.dtype(\'float32\').itemsize * layer1_size\n        i = 0\n        print (""Vector length:"", layer1_size)\n        for _ in range(vocab_size):\n            word = []\n            while True:\n                ch = f.read(1)\n                if ch == \' \':\n                    word = \'\'.join(word)\n                    break\n                if ch != \'\\n\':\n                    word.append(ch)\n            word_vecs[word] = np.fromstring(f.read(binary_len),\n                                            dtype=\'float32\')\n            i += 1\n            if i % 1000 == 0:\n                print (""Processed %d vectors (%.2f %%)\\r"" % (i, 100 * float(i) / vocab_size),)\n\n    # Store dict\n    print ("""")\n    output_path = os.path.join(base_path_save, dest_filename + \'.npy\')\n    print (""Saving word vectors in %s"" % output_path)\n    np.save(output_path, word_vecs)\n    print("""")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(""Preprocess pre-trained word embeddings."")\n    parser.add_argument(""-v"", ""--vectors"", required=True, help=""Pre-trained word embeddings file."",\n                        default=""GoogleNews-vectors-negative300.bin"")\n    parser.add_argument(""-d"", ""--destination"", required=True, help=""Destination file."", default=\'word2vec.en\')\n    parser.add_argument(""-s"", ""--split_symbol"", required=False,\n                        help=""Split symbol."", default=\'word2vec.en\')\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    dest_file = basename(args.destination)\n    base_path = dirname(args.destination)\n\n    word2vec2npy(args.vectors, base_path, dest_file)\n'"
utils/preprocess_text_word_vectors.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport numpy as np\nimport argparse\nfrom os.path import basename, dirname\n\n\n# Preprocess pretrained text vectors\n# and stores them in a suitable format (.npy)\n\ndef txtvec2npy(v_path, base_path_save, dest_filename):\n    """"""\n    Preprocess pretrained text vectors and stores them in a suitable format\n    :param v_path: Path to the text vectors file.\n    :param base_path_save: Path where the formatted vectors will be stored.\n    :param dest_filename: Filename of the formatted vectors.\n    """"""\n    word_vecs = dict()\n    print (""Loading vectors from %s"" % v_path)\n    vectors = [x[:-1] for x in open(v_path).readlines()]\n    if len(vectors[0].split()) == 2:\n        signature = vectors.pop(0).split()\n        dimension = int(signature[1])\n        n_vecs = len(vectors)\n        assert int(signature[0]) == n_vecs, \'The number of read vectors does not match with the expected one (read %d, expected %d)\' % (n_vecs, int(signature[0]))\n    else:\n        n_vecs = len(vectors)\n        dimension = len(vectors[0].split()) - 1\n\n    print (""Found %d vectors of dimension %d in %s"" % (n_vecs, dimension, v_path))\n    i = 0\n    for vector in vectors:\n        v = vector.split()\n        vec = np.asarray(v[-dimension:], dtype=\'float32\')\n        word = \' \'.join(v[0: len(v) - dimension])\n        word_vecs[word] = vec\n        i += 1\n        if i % 1000 == 0:\n            print (""Processed %d vectors (%.2f %%)\\r"" % (i, 100 * float(i) / n_vecs),)\n\n    print ("""")\n    # Store dict\n    print (""Saving word vectors in %s"" % (base_path_save + dest_filename + \'.npy\'))\n    np.save(base_path_save + dest_filename + \'.npy\', word_vecs)\n    print ("""")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(""Preprocess pre-trained word embeddings."")\n    parser.add_argument(""-v"", ""--vectors"", required=True, help=""Pre-trained word embeddings file."",\n                        default=""GoogleNews-vectors-negative300.txt"")\n    parser.add_argument(""-d"", ""--destination"", required=True, help=""Destination file."", default=\'word2vec.en\')\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    dest_file = basename(args.destination)\n    base_path = dirname(args.destination)\n    txtvec2npy(args.vectors, base_path, dest_file)\n'"
utils/ttables_to_dict.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\n\n# Uses T-tables made by Chris Dyer\'s Fast Align\n# Code adapted from: https://github.com/sebastien-j/LV_groundhog/blob/master/experiments/nmt/utils/convert_Ttables.py\n\nimport numpy as np\nfrom keras_wrapper.extra.read_write import dict2pkl\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--fname"", type=str)  # T-tables\nparser.add_argument(""--dest"", type=str)\nparser.add_argument(""--verbose"", type=int)\nparser.add_argument(""--keep-probs"", required=False, type=int, default=0, help=""Keep alignments with probs."")\n\nargs = parser.parse_args()\n\nd = {}\ntmp_dict = dict()\nwith open(args.fname, \'r\') as f:\n    i = -1\n    cur_source = -1\n    for line in f:\n        line = line.split()\n        if line[0] != cur_source:\n            i += 1\n            if (i % 1000) == 0 and args.verbose > 0:\n                print (i)\n            if cur_source != -1:\n                d[cur_source] = tmp_dict  # Set dict for previous word\n            cur_source = line[0]\n            tmp_dict = dict()\n            tmp_dict[line[1]] = pow(np.e, float(line[2]))\n        else:\n            tmp_dict[line[1]] = pow(np.e, float(line[2]))\nd[cur_source] = tmp_dict\ndel tmp_dict\n\n\nif args.keep_probs:\n    f1 = d\nelse:\n    e = {}\n    j = 0\n    for elt in d:\n        if (j % 1000) == 0 and args.verbose > 0:\n            print (j)\n        j += 1\n        e[elt] = sorted(d[elt], key=d[elt].get)[::-1]\n\n    f1 = {}\n    j = 0\n    for elt in e:\n        if (j % 1000) == 0 and args.verbose > 0:\n            print (j)\n        j += 1\n        f1[elt] = e[elt][0]\n\ndict2pkl(f1, args.dest)\n'"
utils/utils.py,0,"b'from six import iteritems\n\n\ndef update_parameters(params, updates, restrict=False):\n    """"""\n    Updates the parameters from params with the ones specified in updates\n    :param params: Parameters dictionary to update\n    :param updates: Updater dictionary\n    :param restrict: If True, parameters from the original dict are not overwritten.\n    :return:\n    """"""\n    for new_param_key, new_param_value in iteritems(updates):\n        if restrict and params.get(new_param_key) is not None:\n            params[new_param_key] = new_param_value\n        else:\n            params[new_param_key] = new_param_value\n\n    return params\n'"
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../../\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\nimport sphinx_rtd_theme\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.githubpages\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'ntemplates\']\n\nedit_on_github_project = \'lvapeab/nmt-keras\'\nedit_on_github_branch = \'master\'\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'NMT-Keras\'\ncopyright = u\'2017, \xc3\x81lvaro Peris\'\nauthor = u\'\xc3\x81lvaro Peris\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = u\'0.2\'\n# The full version, including alpha/beta/rc tags.\nrelease = u\'0.2\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#\n# today = \'\'\n#\n# Else, today_fmt is used as the format for a strftime call.\n#\n# today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#\n# default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n# keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': False,\n    \'navigation_depth\': 3,\n}\n\nhtml_context = {\n    \'display_github\': True,\n    \'github_repo\': ""nmt-keras"",\n    \'github_user\': ""lvapeab"",\n    \'github_version\': ""master"",\n    \'conf_py_path\': ""/docs/source/""\n}\n\n# Add any paths that contain custom themes here, relative to this directory.\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# The name for this set of Sphinx documents.\n# ""<project> v<release> documentation"" by default.\n#\nhtml_title = u\'NMT-Keras\'\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#\nhtml_short_title = u\'NMT-Keras documentation\'\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#\n# html_logo = None\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'nstatic\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#\n# html_extra_path = []\n\n# If not None, a \'Last updated on:\' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to \'%b %d, %Y\'.\n#\n# html_last_updated_fmt = None\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n#\nhtml_domain_indices = True\n\n# If false, no index is generated.\n#\nhtml_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#\nhtml_show_sourcelink = False\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#\n# html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#\n# html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n# html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\', \'zh\'\n#\n# html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# \'ja\' uses this config value.\n# \'zh\' user can custom change `jieba` dictionary path.\n#\n# html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#\n# html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'NMT-Kerasdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n     # The paper size (\'letterpaper\' or \'a4paper\').\n     #\n     # \'papersize\': \'letterpaper\',\n\n     # The font size (\'10pt\', \'11pt\' or \'12pt\').\n     #\n     # \'pointsize\': \'10pt\',\n\n     # Additional stuff for the LaTeX preamble.\n     #\n     # \'preamble\': \'\',\n\n     # Latex figure (float) alignment\n     #\n     # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'NMT-Keras.tex\', u\'NMT-Keras Documentation\',\n     u\'\xc3\x81lvaro Peris\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#\n# latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n#\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#\n# latex_appendices = []\n\n# It false, will not define \\strong, \\code, \titleref, \\crossref ... but only\n# \\sphinxstrong, ..., \\sphinxtitleref, ... To help avoid clash with user added\n# packages.\n#\n# latex_keep_old_macro_names = True\n\n# If false, no module index is generated.\n#\n# latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'nmt-keras\', u\'NMT-Keras Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'NMT-Keras\', u\'NMT-Keras Documentation\',\n     author, \'NMT-Keras\', \'Neural Machine Translation with Keras\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n#\n# texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#\n# texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#\n# texinfo_no_detailmenu = False\n'"
examples/configs/config_rnn.py,0,"b'\ndef load_parameters():\n    """"""\n    Loads the defined hyperparameters\n    :return parameters: Dictionary of loaded parameters\n    """"""\n\n    # Input data params\n    TASK_NAME = \'my_task\'                           # Task name\n    DATASET_NAME = TASK_NAME                        # Dataset name\n    SRC_LAN = \'fr\'                                  # Language of the source text\n    TRG_LAN = \'en\'                                  # Language of the target text\n    DATA_ROOT_PATH = \'/DATA/%s/%s/joint_bpe/\' % (TASK_NAME, SRC_LAN+TRG_LAN)  # Path where data is stored\n\n    # SRC_LAN or TRG_LAN will be added to the file names\n    TEXT_FILES = {\'train\': \'training.\',        # Data files\n                  \'val\': \'dev.\',\n                  \'test\': \'test.\'}\n\n    # Dataset class parameters\n    INPUTS_IDS_DATASET = [\'source_text\', \'state_below\']     # Corresponding inputs of the dataset\n    OUTPUTS_IDS_DATASET = [\'target_text\']                   # Corresponding outputs of the dataset\n    INPUTS_IDS_MODEL = [\'source_text\', \'state_below\']       # Corresponding inputs of the built model\n    OUTPUTS_IDS_MODEL = [\'target_text\']                     # Corresponding outputs of the built model\n\n    # Evaluation params\n    METRICS = [\'coco\']                            # Metric used for evaluating the model\n    EVAL_ON_SETS = [\'val\']                        # Possible values: \'train\', \'val\' and \'test\' (external evaluator)\n    EVAL_ON_SETS_KERAS = []                       # Possible values: \'train\', \'val\' and \'test\' (Keras\' evaluator). Untested.\n    START_EVAL_ON_EPOCH = 1                       # First epoch to start the model evaluation\n    EVAL_EACH_EPOCHS = False                      # Select whether evaluate between N epochs or N updates\n    EVAL_EACH = 3750                              # Sets the evaluation frequency (epochs or updates)\n\n    # Search parameters\n    SAMPLING = \'max_likelihood\'                   # Possible values: multinomial or max_likelihood (recommended)\n    TEMPERATURE = 1                               # Multinomial sampling parameter\n    BEAM_SEARCH = True                            # Switches on-off the beam search procedure\n    BEAM_SIZE = 6                                 # Beam size (in case of BEAM_SEARCH == True)\n    OPTIMIZED_SEARCH = True                       # Compute annotations only a single time per sample\n    SEARCH_PRUNING = False                        # Apply pruning strategies to the beam search method.\n                                                  # It will likely increase decoding speed, but decrease quality.\n    MAXLEN_GIVEN_X = True                         # Generate translations of similar length to the source sentences\n    MAXLEN_GIVEN_X_FACTOR = 1.7                   # The hypotheses will have (as maximum) the number of words of the\n                                                  # source sentence * LENGTH_Y_GIVEN_X_FACTOR\n    MINLEN_GIVEN_X = True                         # Generate translations of similar length to the source sentences\n    MINLEN_GIVEN_X_FACTOR = 2                     # The hypotheses will have (as minimum) the number of words of the\n                                                  # source sentence / LENGTH_Y_GIVEN_X_FACTOR\n\n    # Apply length and coverage decoding normalizations.\n    # See Section 7 from Wu et al. (2016) (https://arxiv.org/abs/1609.08144)\n    LENGTH_PENALTY = False                        # Apply length penalty\n    LENGTH_NORM_FACTOR = 0.2                      # Length penalty factor\n    COVERAGE_PENALTY = False                      # Apply source coverage penalty\n    COVERAGE_NORM_FACTOR = 0.2                    # Coverage penalty factor\n\n    # Alternative (simple) length normalization.\n    NORMALIZE_SAMPLING = False                    # Normalize hypotheses scores according to their length:\n    ALPHA_FACTOR = .6                             # Normalization according to |h|**ALPHA_FACTOR\n\n    # Sampling params: Show some samples during training\n    SAMPLE_ON_SETS = [\'train\', \'val\']             # Possible values: \'train\', \'val\' and \'test\'\n    N_SAMPLES = 5                                 # Number of samples generated\n    START_SAMPLING_ON_EPOCH = 2                   # First epoch where to start the sampling counter\n    SAMPLE_EACH_UPDATES = 10000                     # Sampling frequency (always in #updates)\n\n    # Unknown words treatment\n    POS_UNK = True                               # Enable POS_UNK strategy for unknown words\n    HEURISTIC = 0                                 # Heuristic to follow:\n                                                  #     0: Replace the UNK by the correspondingly aligned source\n                                                  #     1: Replace the UNK by the translation (given by an external\n                                                  #        dictionary) of the correspondingly aligned source\n                                                  #     2: Replace the UNK by the translation (given by an external\n                                                  #        dictionary) of the correspondingly aligned source only if it\n                                                  #        starts with a lowercase. Otherwise, copies the source word.\n    ALIGN_FROM_RAW = True                         # Align using the full vocabulary or the short_list\n\n    MAPPING = DATA_ROOT_PATH + \'/mapping.%s_%s.pkl\' % (SRC_LAN, TRG_LAN) # Source -- Target pkl mapping (used for heuristics 1--2)\n\n    # Word representation params\n    TOKENIZATION_METHOD = \'tokenize_none\'         # Select which tokenization we\'ll apply.\n                                                  # See Dataset class (from stager_keras_wrapper) for more info.\n    BPE_CODES_PATH = DATA_ROOT_PATH + \'/training_codes.joint\'    # If TOKENIZATION_METHOD = \'tokenize_bpe\',\n                                                  # sets the path to the learned BPE codes.\n    DETOKENIZATION_METHOD = \'detokenize_bpe\'     # Select which de-tokenization method we\'ll apply\n\n    APPLY_DETOKENIZATION = True                   # Wheter we apply a detokenization method\n\n    TOKENIZE_HYPOTHESES = True   \t\t          # Whether we tokenize the hypotheses using the\n                                                  # previously defined tokenization method\n    TOKENIZE_REFERENCES = True                    # Whether we tokenize the references using the\n                                                  # previously defined tokenization method\n\n    # Input image parameters\n    DATA_AUGMENTATION = False                     # Apply data augmentation on input data (still unimplemented for text inputs)\n\n    # Text parameters\n    FILL = \'end\'                                  # Whether we pad the \'end\' or the \'start\' of the sentence with 0s\n    PAD_ON_BATCH = True                           # Whether we take as many timesteps as the longest sequence of\n                                                  # the batch or a fixed size (MAX_OUTPUT_TEXT_LEN)\n    # Input text parameters\n    INPUT_VOCABULARY_SIZE = 0                     # Size of the input vocabulary. Set to 0 for using all,\n                                                  # otherwise it will be truncated to these most frequent words.\n    MIN_OCCURRENCES_INPUT_VOCAB = 0               # Minimum number of occurrences allowed for the words in the input vocabulary.\n                                                  # Set to 0 for using them all.\n    MAX_INPUT_TEXT_LEN = 70                       # Maximum length of the input sequence\n\n    # Output text parameters\n    OUTPUT_VOCABULARY_SIZE = 0                    # Size of the input vocabulary. Set to 0 for using all,\n                                                  # otherwise it will be truncated to these most frequent words.\n    MIN_OCCURRENCES_OUTPUT_VOCAB = 0              # Minimum number of occurrences allowed for the words in the output vocabulary.\n    MAX_OUTPUT_TEXT_LEN = 70                      # Maximum length of the output sequence\n                                                  # set to 0 if we want to use the whole answer as a single class\n    MAX_OUTPUT_TEXT_LEN_TEST = MAX_OUTPUT_TEXT_LEN * 3  # Maximum length of the output sequence during test time\n\n    # Optimizer parameters (see model.compile() function).\n    LOSS = \'categorical_crossentropy\'\n    CLASSIFIER_ACTIVATION = \'softmax\'\n    SAMPLE_WEIGHTS = True                         # Select whether we use a weights matrix (mask) for the data outputs\n    LABEL_SMOOTHING = 0.05                        # Epsilon value for label smoothing. Only valid for \'categorical_crossentropy\' loss. See arxiv.org/abs/1512.00567.\n\n    OPTIMIZER = \'Adam\'                            # Optimizer. Supported optimizers: SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam.\n    LR = 0.0002                                    # Learning rate. Recommended values - Adam 0.0002 - Adadelta 1.0.\n    CLIP_C = 5.                                   # During training, clip L2 norm of gradients to this value (0. means deactivated).\n    CLIP_V = 0.                                   # During training, clip absolute value of gradients to this value (0. means deactivated).\n    USE_TF_OPTIMIZER = True                       # Use native Tensorflow\'s optimizer (only for the Tensorflow backend).\n\n    # Advanced parameters for optimizers. Default values are usually effective.\n    MOMENTUM = 0.                                 # Momentum value (for SGD optimizer).\n    NESTEROV_MOMENTUM = False                     # Use Nesterov momentum (for SGD optimizer).\n    RHO = 0.9                                     # Rho value (for Adadelta and RMSprop optimizers).\n    BETA_1 = 0.9                                  # Beta 1 value (for Adam, Adamax Nadam optimizers).\n    BETA_2 = 0.999                                # Beta 2 value (for Adam, Adamax Nadam optimizers).\n    AMSGRAD = False                               # Whether to apply the AMSGrad variant of Adam (see https://openreview.net/pdf?id=ryQu7f-RZ).\n    EPSILON = 1e-7                                # Optimizers epsilon value.\n\n    # Learning rate annealing\n    LR_DECAY = None                               # Frequency (number of epochs or updates) between LR annealings. Set to None for not decay the learning rate\n    LR_GAMMA = 0.8                                # Multiplier used for decreasing the LR\n    LR_REDUCE_EACH_EPOCHS = False                 # Reduce each LR_DECAY number of epochs or updates\n    LR_START_REDUCTION_ON_EPOCH = 0               # Epoch to start the reduction\n    LR_REDUCER_TYPE = \'exponential\'               # Function to reduce. \'linear\' and \'exponential\' implemented.\n                                                  # Linear reduction: new_lr = lr * LR_GAMMA\n                                                  # Exponential reduction: new_lr = lr * LR_REDUCER_EXP_BASE ** (current_nb / LR_HALF_LIFE) * LR_GAMMA\n                                                  # Noam reduction: new_lr = lr * min(current_nb ** LR_REDUCER_EXP_BASE, current_nb * LR_HALF_LIFE ** WARMUP_EXP)\n    LR_REDUCER_EXP_BASE = -0.5                     # Base for the exponential decay.\n    LR_HALF_LIFE = 100                           # Factor/warmup steps for exponenital/noam decay.\n    WARMUP_EXP = -1.5                             # Warmup steps for noam decay.\n\n    # Training parameters\n    MAX_EPOCH = 500                               # Stop when computed this number of epochs.\n    BATCH_SIZE = 50                               # Size of each minibatch.\n    N_GPUS = 1                                    # Number of GPUs to use. Only for Tensorflow backend. Each GPU will receive mini-batches of BATCH_SIZE / N_GPUS.\n\n    HOMOGENEOUS_BATCHES = False                   # Use batches with homogeneous output lengths (Dangerous!!).\n    JOINT_BATCHES = 4                             # When using homogeneous batches, get this number of batches to sort.\n    PARALLEL_LOADERS = 1                          # Parallel data batch loaders. Somewhat untested if > 1.\n    EPOCHS_FOR_SAVE = 1                           # Number of epochs between model saves.\n    WRITE_VALID_SAMPLES = True                    # Write valid samples in file.\n    SAVE_EACH_EVALUATION = True                   # Save each time we evaluate the model.\n\n    # Early stop parameters\n    EARLY_STOP = True                             # Turns on/off the early stop protocol.\n    PATIENCE = 10                                 # We\'ll stop if the val STOP_METRIC does not improve after this.\n                                                  # number of evaluations.\n    STOP_METRIC = \'Bleu_4\'                        # Metric for the stop.\n\n    # Model parameters\n    MODEL_TYPE = \'AttentionRNNEncoderDecoder\'     # Model to train. See model_zoo.py for more info.\n                                                  # Supported architectures: \'AttentionRNNEncoderDecoder\' and \'Transformer\'.\n\n    # Hyperparameters common to all models\n    # # # # # # # # # # # # # # # # # # # # # # # #\n    TRAINABLE_ENCODER = True                      # Whether the encoder\'s weights should be modified during training.\n    TRAINABLE_DECODER = True                      # Whether the decoder\'s weights should be modified during training.\n\n    # Initializers (see keras/initializations.py).\n    INIT_FUNCTION = \'glorot_uniform\'              # General initialization function for matrices.\n    INNER_INIT = \'orthogonal\'                     # Initialization function for inner RNN matrices.\n    INIT_ATT = \'glorot_uniform\'                   # Initialization function for attention mechism matrices\n\n    SOURCE_TEXT_EMBEDDING_SIZE = 512              # Source language word embedding size.\n    SRC_PRETRAINED_VECTORS = None                 # Path to pretrained vectors (e.g.: DATA_ROOT_PATH + \'/DATA/word2vec.%s.npy\' % SRC_LAN).\n                                                  # Set to None if you don\'t want to use pretrained vectors.\n                                                  # When using pretrained word embeddings. this parameter must match with the word embeddings size\n    SRC_PRETRAINED_VECTORS_TRAINABLE = True       # Finetune or not the target word embedding vectors.\n\n    TARGET_TEXT_EMBEDDING_SIZE = 512              # Source language word embedding size.\n    TRG_PRETRAINED_VECTORS = None                 # Path to pretrained vectors. (e.g. DATA_ROOT_PATH + \'/DATA/word2vec.%s.npy\' % TRG_LAN)\n                                                  # Set to None if you don\'t want to use pretrained vectors.\n                                                  # When using pretrained word embeddings, the size of the pretrained word embeddings must match with the word embeddings size.\n    TRG_PRETRAINED_VECTORS_TRAINABLE = True       # Finetune or not the target word embedding vectors.\n\n    SCALE_SOURCE_WORD_EMBEDDINGS = False          # Scale source word embeddings by Sqrt(SOURCE_TEXT_EMBEDDING_SIZE)\n    SCALE_TARGET_WORD_EMBEDDINGS = False          # Scale target word embeddings by Sqrt(TARGET_TEXT_EMBEDDING_SIZE)\n\n    N_LAYERS_ENCODER = 1                          # Stack this number of encoding layers.\n    N_LAYERS_DECODER = 1                          # Stack this number of decoding layers.\n\n    # Additional Fully-Connected layers applied before softmax.\n    #       Here we should specify the activation function and the output dimension.\n    #       (e.g DEEP_OUTPUT_LAYERS = [(\'tanh\', 600), (\'relu\', 400), (\'relu\', 200)])\n    DEEP_OUTPUT_LAYERS = [(\'linear\', TARGET_TEXT_EMBEDDING_SIZE)]\n    # # # # # # # # # # # # # # # # # # # # # # # #\n\n    # AttentionRNNEncoderDecoder model hyperparameters\n    # # # # # # # # # # # # # # # # # # # # # # # #\n    ENCODER_RNN_TYPE = \'LSTM\'                     # Encoder\'s RNN unit type (\'LSTM\' and \'GRU\' supported).\n    USE_CUDNN = True                              # Use CuDNN\'s implementation of GRU and LSTM (only for Tensorflow backend).\n\n    DECODER_RNN_TYPE = \'ConditionalLSTM\'          # Decoder\'s RNN unit type.\n                                                  # (\'LSTM\', \'GRU\', \'ConditionalLSTM\' and \'ConditionalGRU\' supported).\n    ATTENTION_MODE = \'add\'                        # Attention mode. \'add\' (Bahdanau-style) or \'dot\' (Luong-style).\n\n    # Encoder configuration\n    ENCODER_HIDDEN_SIZE = 512                      # For models with RNN encoder.\n    BIDIRECTIONAL_ENCODER = True                  # Use bidirectional encoder.\n    BIDIRECTIONAL_DEEP_ENCODER = True             # Use bidirectional encoder in all encoding layers.\n    BIDIRECTIONAL_MERGE_MODE = \'concat\'           # Merge function for bidirectional layers.\n\n    # Fully-Connected layers for initializing the first decoder RNN state.\n    #       Here we should only specify the activation function of each layer (as they have a potentially fixed size)\n    #       (e.g INIT_LAYERS = [\'tanh\', \'relu\'])\n    INIT_LAYERS = [\'tanh\']\n\n    # Decoder configuration\n    DECODER_HIDDEN_SIZE = 512                      # For models with RNN decoder.\n    ATTENTION_SIZE = DECODER_HIDDEN_SIZE\n\n    # Skip connections parameters\n    SKIP_VECTORS_HIDDEN_SIZE = TARGET_TEXT_EMBEDDING_SIZE     # Hidden size.\n    ADDITIONAL_OUTPUT_MERGE_MODE = \'Add\'          # Merge mode for the skip-connections (see keras.layers.merge.py).\n    SKIP_VECTORS_SHARED_ACTIVATION = \'tanh\'       # Activation for the skip vectors.\n    # # # # # # # # # # # # # # # # # # # # # # # #\n\n    # Transformer model hyperparameters\n    # # # # # # # # # # # # # # # # # # # # # # # #\n    MODEL_SIZE = 512                               # Transformer model size (d_{model} in de paper).\n    MULTIHEAD_ATTENTION_ACTIVATION = \'linear\'     # Activation the input projections in the Multi-Head Attention blocks.\n    FF_SIZE = MODEL_SIZE * 4                      # Size of the feed-forward layers of the Transformer model.\n    N_HEADS = 8                                   # Number of parallel attention layers of the Transformer model.\n    # # # # # # # # # # # # # # # # # # # # # # # #\n\n    # Regularizers\n    REGULARIZATION_FN = \'L2\'                      # Regularization function. \'L1\', \'L2\' and \'L1_L2\' supported.\n    WEIGHT_DECAY = 1e-4                           # L2 regularization\n    RECURRENT_WEIGHT_DECAY = 0.                   # L2 regularization in recurrent layers\n\n    DROPOUT_P = 0.1                               # Percentage of units to drop (0 means no dropout).\n    RECURRENT_INPUT_DROPOUT_P = 0.                # Percentage of units to drop in input cells of recurrent layers.\n    RECURRENT_DROPOUT_P = 0.                      # Percentage of units to drop in recurrent layers.\n    ATTENTION_DROPOUT_P = 0.1                     # Percentage of units to drop in attention layers (0 means no dropout).\n\n    USE_NOISE = True                              # Use gaussian noise during training\n    NOISE_AMOUNT = 0.01                           # Amount of noise\n\n    USE_BATCH_NORMALIZATION = True                # If True it is recommended to deactivate Dropout\n    BATCH_NORMALIZATION_MODE = 1                  # See documentation in Keras\' BN\n\n    USE_PRELU = False                             # use PReLU activations as regularizer.\n    USE_L1 = False                                # L1 normalization on the features.\n    USE_L2 = False                                # L2 normalization on the features.\n\n    DOUBLE_STOCHASTIC_ATTENTION_REG = 0.0         # Doubly stochastic attention (Eq. 14 from arXiv:1502.03044)\n\n    # Results plot and models storing parameters.\n    EXTRA_NAME = \'\'                               # This will be appended to the end of the model name.\n    if MODEL_TYPE == \'AttentionRNNEncoderDecoder\':\n        MODEL_NAME = TASK_NAME + \'_\' + SRC_LAN + TRG_LAN + \'_\' + MODEL_TYPE + \\\n                 \'_src_emb_\' + str(SOURCE_TEXT_EMBEDDING_SIZE) + \\\n                 \'_bidir_\' + str(BIDIRECTIONAL_ENCODER) + \\\n                 \'_enc_\' + ENCODER_RNN_TYPE + \'_\' + str(ENCODER_HIDDEN_SIZE) + \\\n                 \'_dec_\' + DECODER_RNN_TYPE + \'_\' + str(DECODER_HIDDEN_SIZE) + \\\n                 \'_deepout_\' + \'_\'.join([layer[0] for layer in DEEP_OUTPUT_LAYERS]) + \\\n                 \'_trg_emb_\' + str(TARGET_TEXT_EMBEDDING_SIZE) + \\\n                 \'_\' + OPTIMIZER + \'_\' + str(LR)\n    elif MODEL_TYPE == \'Transformer\':\n        MODEL_NAME = TASK_NAME + \'_\' + SRC_LAN + TRG_LAN + \'_\' + MODEL_TYPE + \\\n                 \'_model_size_\' + str(MODEL_SIZE) + \\\n                 \'_ff_size_\' + str(FF_SIZE) + \\\n                 \'_num_heads_\' + str(N_HEADS) + \\\n                 \'_encoder_blocks_\' + str(N_LAYERS_ENCODER) + \\\n                 \'_decoder_blocks_\' + str(N_LAYERS_DECODER) + \\\n                 \'_deepout_\' + \'_\'.join([layer[0] for layer in DEEP_OUTPUT_LAYERS]) + \\\n                 \'_\' + OPTIMIZER + \'_\' + str(LR)\n    else:\n        MODEL_NAME = TASK_NAME + \'_\' + SRC_LAN + TRG_LAN + \'_\' +\\\n                     MODEL_TYPE + \'_\' + OPTIMIZER + \'_\' + str(LR)\n\n    MODEL_NAME += EXTRA_NAME\n\n    STORE_PATH = \'trained_models/\' + MODEL_NAME + \'/\'  # Models and evaluation results will be stored here.\n    DATASET_STORE_PATH = \'datasets/\'                   # Dataset instance will be stored here.\n\n    # Tensorboard configuration. Only if the backend is Tensorflow. Otherwise, it will be ignored.\n    TENSORBOARD = True                       # Switches On/Off the tensorboard callback.\n    LOG_DIR = \'tensorboard_logs\'             # Directory to store teh model. Will be created inside STORE_PATH.\n    EMBEDDINGS_FREQ = 1                      # Frequency (in epochs) at which selected embedding layers will be saved.\n    SAMPLING_SAVE_MODE = \'list\'                        # \'list\': Store in a text file, one sentence per line.\n    VERBOSE = 1                                        # Verbosity level.\n    RELOAD = 0                                         # If 0 start training from scratch, otherwise the model.\n                                                       # Saved on epoch \'RELOAD\' will be used.\n    RELOAD_EPOCH = True                                # Select whether we reload epoch or update number.\n\n    REBUILD_DATASET = True                             # Build again or use stored instance.\n    MODE = \'training\'                                  # \'training\' or \'sampling\' (if \'sampling\' then RELOAD must\n                                                       # be greater than 0 and EVAL_ON_SETS will be used).\n\n    # Extra parameters for special trainings. In most cases, they should be set to `False`\n    TRAIN_ON_TRAINVAL = False                          # train the model on both training and validation sets combined.\n    FORCE_RELOAD_VOCABULARY = False                    # force building a new vocabulary from the training samples\n                                                       # applicable if RELOAD > 1\n\n    # ================================================ #\n    parameters = locals().copy()\n    return parameters\n'"
examples/configs/config_transformer.py,0,"b'\ndef load_parameters():\n    """"""\n    Loads the defined hyperparameters\n    :return parameters: Dictionary of loaded parameters\n    """"""\n\n    # Input data params\n    TASK_NAME = \'my_task\'                           # Task name\n    DATASET_NAME = TASK_NAME                        # Dataset name\n    SRC_LAN = \'fr\'                                  # Language of the source text\n    TRG_LAN = \'en\'                                  # Language of the target text\n    DATA_ROOT_PATH = \'/DATA/%s/%s/joint_bpe/\' % (TASK_NAME, SRC_LAN+TRG_LAN)  # Path where data is stored\n\n    # SRC_LAN or TRG_LAN will be added to the file names\n    TEXT_FILES = {\'train\': \'training.\',        # Data files\n                  \'val\': \'dev.\',\n                  \'test\': \'test.\'}\n\n    # Dataset class parameters\n    INPUTS_IDS_DATASET = [\'source_text\', \'state_below\']     # Corresponding inputs of the dataset\n    OUTPUTS_IDS_DATASET = [\'target_text\']                   # Corresponding outputs of the dataset\n    INPUTS_IDS_MODEL = [\'source_text\', \'state_below\']       # Corresponding inputs of the built model\n    OUTPUTS_IDS_MODEL = [\'target_text\']                     # Corresponding outputs of the built model\n\n    # Evaluation params\n    METRICS = [\'coco\']                            # Metric used for evaluating the model\n    EVAL_ON_SETS = [\'val\']                        # Possible values: \'train\', \'val\' and \'test\' (external evaluator)\n    EVAL_ON_SETS_KERAS = []                       # Possible values: \'train\', \'val\' and \'test\' (Keras\' evaluator). Untested.\n    START_EVAL_ON_EPOCH = 1                       # First epoch to start the model evaluation\n    EVAL_EACH_EPOCHS = False                      # Select whether evaluate between N epochs or N updates\n    EVAL_EACH = 3750                              # Sets the evaluation frequency (epochs or updates)\n\n    # Search parameters\n    SAMPLING = \'max_likelihood\'                   # Possible values: multinomial or max_likelihood (recommended)\n    TEMPERATURE = 1                               # Multinomial sampling parameter\n    BEAM_SEARCH = True                            # Switches on-off the beam search procedure\n    BEAM_SIZE = 6                                 # Beam size (in case of BEAM_SEARCH == True)\n    OPTIMIZED_SEARCH = True                       # Compute annotations only a single time per sample\n    SEARCH_PRUNING = False                        # Apply pruning strategies to the beam search method.\n                                                  # It will likely increase decoding speed, but decrease quality.\n    MAXLEN_GIVEN_X = True                         # Generate translations of similar length to the source sentences\n    MAXLEN_GIVEN_X_FACTOR = 1.7                   # The hypotheses will have (as maximum) the number of words of the\n                                                  # source sentence * LENGTH_Y_GIVEN_X_FACTOR\n    MINLEN_GIVEN_X = True                         # Generate translations of similar length to the source sentences\n    MINLEN_GIVEN_X_FACTOR = 2                     # The hypotheses will have (as minimum) the number of words of the\n                                                  # source sentence / LENGTH_Y_GIVEN_X_FACTOR\n\n    # Apply length and coverage decoding normalizations.\n    # See Section 7 from Wu et al. (2016) (https://arxiv.org/abs/1609.08144)\n    LENGTH_PENALTY = False                        # Apply length penalty\n    LENGTH_NORM_FACTOR = 0.2                      # Length penalty factor\n    COVERAGE_PENALTY = False                      # Apply source coverage penalty\n    COVERAGE_NORM_FACTOR = 0.2                    # Coverage penalty factor\n\n    # Alternative (simple) length normalization.\n    NORMALIZE_SAMPLING = False                    # Normalize hypotheses scores according to their length:\n    ALPHA_FACTOR = .6                             # Normalization according to |h|**ALPHA_FACTOR\n\n    # Sampling params: Show some samples during training\n    SAMPLE_ON_SETS = [\'train\', \'val\']             # Possible values: \'train\', \'val\' and \'test\'\n    N_SAMPLES = 5                                 # Number of samples generated\n    START_SAMPLING_ON_EPOCH = 2                   # First epoch where to start the sampling counter\n    SAMPLE_EACH_UPDATES = 10000                     # Sampling frequency (always in #updates)\n\n    # Unknown words treatment\n    POS_UNK = True                               # Enable POS_UNK strategy for unknown words\n    HEURISTIC = 0                                 # Heuristic to follow:\n                                                  #     0: Replace the UNK by the correspondingly aligned source\n                                                  #     1: Replace the UNK by the translation (given by an external\n                                                  #        dictionary) of the correspondingly aligned source\n                                                  #     2: Replace the UNK by the translation (given by an external\n                                                  #        dictionary) of the correspondingly aligned source only if it\n                                                  #        starts with a lowercase. Otherwise, copies the source word.\n    ALIGN_FROM_RAW = True                         # Align using the full vocabulary or the short_list\n\n    MAPPING = DATA_ROOT_PATH + \'/mapping.%s_%s.pkl\' % (SRC_LAN, TRG_LAN) # Source -- Target pkl mapping (used for heuristics 1--2)\n\n    # Word representation params\n    TOKENIZATION_METHOD = \'tokenize_none\'         # Select which tokenization we\'ll apply.\n                                                  # See Dataset class (from stager_keras_wrapper) for more info.\n    BPE_CODES_PATH = DATA_ROOT_PATH + \'/training_codes.joint\'    # If TOKENIZATION_METHOD = \'tokenize_bpe\',\n                                                  # sets the path to the learned BPE codes.\n    DETOKENIZATION_METHOD = \'detokenize_bpe\'     # Select which de-tokenization method we\'ll apply\n\n    APPLY_DETOKENIZATION = True                   # Wheter we apply a detokenization method\n\n    TOKENIZE_HYPOTHESES = True   \t\t          # Whether we tokenize the hypotheses using the\n                                                  # previously defined tokenization method\n    TOKENIZE_REFERENCES = True                    # Whether we tokenize the references using the\n                                                  # previously defined tokenization method\n\n    # Input image parameters\n    DATA_AUGMENTATION = False                     # Apply data augmentation on input data (still unimplemented for text inputs)\n\n    # Text parameters\n    FILL = \'end\'                                  # Whether we pad the \'end\' or the \'start\' of the sentence with 0s\n    PAD_ON_BATCH = True                           # Whether we take as many timesteps as the longest sequence of\n                                                  # the batch or a fixed size (MAX_OUTPUT_TEXT_LEN)\n    # Input text parameters\n    INPUT_VOCABULARY_SIZE = 0                     # Size of the input vocabulary. Set to 0 for using all,\n                                                  # otherwise it will be truncated to these most frequent words.\n    MIN_OCCURRENCES_INPUT_VOCAB = 0               # Minimum number of occurrences allowed for the words in the input vocabulary.\n                                                  # Set to 0 for using them all.\n    MAX_INPUT_TEXT_LEN = 70                       # Maximum length of the input sequence\n\n    # Output text parameters\n    OUTPUT_VOCABULARY_SIZE = 0                    # Size of the input vocabulary. Set to 0 for using all,\n                                                  # otherwise it will be truncated to these most frequent words.\n    MIN_OCCURRENCES_OUTPUT_VOCAB = 0              # Minimum number of occurrences allowed for the words in the output vocabulary.\n    MAX_OUTPUT_TEXT_LEN = 70                      # Maximum length of the output sequence\n                                                  # set to 0 if we want to use the whole answer as a single class\n    MAX_OUTPUT_TEXT_LEN_TEST = MAX_OUTPUT_TEXT_LEN * 3  # Maximum length of the output sequence during test time\n\n    # Optimizer parameters (see model.compile() function).\n    LOSS = \'categorical_crossentropy\'\n    CLASSIFIER_ACTIVATION = \'softmax\'\n    SAMPLE_WEIGHTS = True                         # Select whether we use a weights matrix (mask) for the data outputs\n    LABEL_SMOOTHING = 0.1                        # Epsilon value for label smoothing. Only valid for \'categorical_crossentropy\' loss. See arxiv.org/abs/1512.00567.\n\n    OPTIMIZER = \'Adam\'                            # Optimizer. Supported optimizers: SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam.\n    LR = 0.004                                    # Learning rate. Recommended values - Adam 0.0002 - Adadelta 1.0.\n    CLIP_C = 5.                                   # During training, clip L2 norm of gradients to this value (0. means deactivated).\n    CLIP_V = 0.                                   # During training, clip absolute value of gradients to this value (0. means deactivated).\n    USE_TF_OPTIMIZER = True                       # Use native Tensorflow\'s optimizer (only for the Tensorflow backend).\n\n    # Advanced parameters for optimizers. Default values are usually effective.\n    MOMENTUM = 0.                                 # Momentum value (for SGD optimizer).\n    NESTEROV_MOMENTUM = False                     # Use Nesterov momentum (for SGD optimizer).\n    RHO = 0.9                                     # Rho value (for Adadelta and RMSprop optimizers).\n    BETA_1 = 0.9                                  # Beta 1 value (for Adam, Adamax Nadam optimizers).\n    BETA_2 = 0.999                                # Beta 2 value (for Adam, Adamax Nadam optimizers).\n    AMSGRAD = False                               # Whether to apply the AMSGrad variant of Adam (see https://openreview.net/pdf?id=ryQu7f-RZ).\n    EPSILON = 1e-7                                # Optimizers epsilon value.\n\n    # Learning rate annealing\n    LR_DECAY = 1                                  # Frequency (number of epochs or updates) between LR annealings. Set to None for not decay the learning rate\n    LR_GAMMA = 1                                  # Multiplier used for decreasing the LR\n    LR_REDUCE_EACH_EPOCHS = False                 # Reduce each LR_DECAY number of epochs or updates\n    LR_START_REDUCTION_ON_EPOCH = 0               # Epoch to start the reduction\n    LR_REDUCER_TYPE = \'noam\'               # Function to reduce. \'linear\' and \'exponential\' implemented.\n                                                  # Linear reduction: new_lr = lr * LR_GAMMA\n                                                  # Exponential reduction: new_lr = lr * LR_REDUCER_EXP_BASE ** (current_nb / LR_HALF_LIFE) * LR_GAMMA\n                                                  # Noam reduction: new_lr = lr * min(current_nb ** LR_REDUCER_EXP_BASE, current_nb * LR_HALF_LIFE ** WARMUP_EXP)\n    LR_REDUCER_EXP_BASE = -0.5                     # Base for the exponential decay.\n    LR_HALF_LIFE = 4000                           # Factor/warmup steps for exponenital/noam decay.\n    WARMUP_EXP = -1.5                             # Warmup steps for noam decay.\n\n    # Training parameters\n    MAX_EPOCH = 500                               # Stop when computed this number of epochs.\n    BATCH_SIZE = 50                               # Size of each minibatch.\n    N_GPUS = 1                                    # Number of GPUs to use. Only for Tensorflow backend. Each GPU will receive mini-batches of BATCH_SIZE / N_GPUS.\n\n    HOMOGENEOUS_BATCHES = False                   # Use batches with homogeneous output lengths (Dangerous!!).\n    JOINT_BATCHES = 4                             # When using homogeneous batches, get this number of batches to sort.\n    PARALLEL_LOADERS = 1                          # Parallel data batch loaders. Somewhat untested if > 1.\n    EPOCHS_FOR_SAVE = 1                           # Number of epochs between model saves.\n    WRITE_VALID_SAMPLES = True                    # Write valid samples in file.\n    SAVE_EACH_EVALUATION = True                   # Save each time we evaluate the model.\n\n    # Early stop parameters\n    EARLY_STOP = True                             # Turns on/off the early stop protocol.\n    PATIENCE = 10                                 # We\'ll stop if the val STOP_METRIC does not improve after this.\n                                                  # number of evaluations.\n    STOP_METRIC = \'Bleu_4\'                        # Metric for the stop.\n\n    # Model parameters\n    MODEL_TYPE = \'Transformer\'     # Model to train. See model_zoo.py for more info.\n                                                  # Supported architectures: \'AttentionRNNEncoderDecoder\' and \'Transformer\'.\n\n    # Hyperparameters common to all models\n    # # # # # # # # # # # # # # # # # # # # # # # #\n    TRAINABLE_ENCODER = True                      # Whether the encoder\'s weights should be modified during training.\n    TRAINABLE_DECODER = True                      # Whether the decoder\'s weights should be modified during training.\n\n    # Initializers (see keras/initializations.py).\n    INIT_FUNCTION = \'glorot_uniform\'              # General initialization function for matrices.\n    INNER_INIT = \'orthogonal\'                     # Initialization function for inner RNN matrices.\n    INIT_ATT = \'glorot_uniform\'                   # Initialization function for attention mechism matrices\n\n    SOURCE_TEXT_EMBEDDING_SIZE = 512              # Source language word embedding size.\n    SRC_PRETRAINED_VECTORS = None                 # Path to pretrained vectors (e.g.: DATA_ROOT_PATH + \'/DATA/word2vec.%s.npy\' % SRC_LAN).\n                                                  # Set to None if you don\'t want to use pretrained vectors.\n                                                  # When using pretrained word embeddings. this parameter must match with the word embeddings size\n    SRC_PRETRAINED_VECTORS_TRAINABLE = True       # Finetune or not the target word embedding vectors.\n\n    TARGET_TEXT_EMBEDDING_SIZE = 512              # Source language word embedding size.\n    TRG_PRETRAINED_VECTORS = None                 # Path to pretrained vectors. (e.g. DATA_ROOT_PATH + \'/DATA/word2vec.%s.npy\' % TRG_LAN)\n                                                  # Set to None if you don\'t want to use pretrained vectors.\n                                                  # When using pretrained word embeddings, the size of the pretrained word embeddings must match with the word embeddings size.\n    TRG_PRETRAINED_VECTORS_TRAINABLE = True       # Finetune or not the target word embedding vectors.\n\n    SCALE_SOURCE_WORD_EMBEDDINGS = True           # Scale source word embeddings by Sqrt(SOURCE_TEXT_EMBEDDING_SIZE)\n    SCALE_TARGET_WORD_EMBEDDINGS = True           # Scale target word embeddings by Sqrt(TARGET_TEXT_EMBEDDING_SIZE)\n\n    N_LAYERS_ENCODER = 6                          # Stack this number of encoding layers.\n    N_LAYERS_DECODER = 6                          # Stack this number of decoding layers.\n\n    # Additional Fully-Connected layers applied before softmax.\n    #       Here we should specify the activation function and the output dimension.\n    #       (e.g DEEP_OUTPUT_LAYERS = [(\'tanh\', 600), (\'relu\', 400), (\'relu\', 200)])\n    DEEP_OUTPUT_LAYERS = [(\'linear\', TARGET_TEXT_EMBEDDING_SIZE)]\n    # # # # # # # # # # # # # # # # # # # # # # # #\n\n    # AttentionRNNEncoderDecoder model hyperparameters\n    # # # # # # # # # # # # # # # # # # # # # # # #\n    ENCODER_RNN_TYPE = \'LSTM\'                     # Encoder\'s RNN unit type (\'LSTM\' and \'GRU\' supported).\n    USE_CUDNN = True                              # Use CuDNN\'s implementation of GRU and LSTM (only for Tensorflow backend).\n\n    DECODER_RNN_TYPE = \'ConditionalLSTM\'          # Decoder\'s RNN unit type.\n                                                  # (\'LSTM\', \'GRU\', \'ConditionalLSTM\' and \'ConditionalGRU\' supported).\n    ATTENTION_MODE = \'add\'                        # Attention mode. \'add\' (Bahdanau-style) or \'dot\' (Luong-style).\n\n    # Encoder configuration\n    ENCODER_HIDDEN_SIZE = 512                      # For models with RNN encoder.\n    BIDIRECTIONAL_ENCODER = True                  # Use bidirectional encoder.\n    BIDIRECTIONAL_DEEP_ENCODER = True             # Use bidirectional encoder in all encoding layers.\n    BIDIRECTIONAL_MERGE_MODE = \'concat\'           # Merge function for bidirectional layers.\n\n    # Fully-Connected layers for initializing the first decoder RNN state.\n    #       Here we should only specify the activation function of each layer (as they have a potentially fixed size)\n    #       (e.g INIT_LAYERS = [\'tanh\', \'relu\'])\n    INIT_LAYERS = [\'tanh\']\n\n    # Decoder configuration\n    DECODER_HIDDEN_SIZE = 512                      # For models with RNN decoder.\n    ATTENTION_SIZE = DECODER_HIDDEN_SIZE\n\n    # Skip connections parameters\n    SKIP_VECTORS_HIDDEN_SIZE = TARGET_TEXT_EMBEDDING_SIZE     # Hidden size.\n    ADDITIONAL_OUTPUT_MERGE_MODE = \'Add\'          # Merge mode for the skip-connections (see keras.layers.merge.py).\n    SKIP_VECTORS_SHARED_ACTIVATION = \'tanh\'       # Activation for the skip vectors.\n    # # # # # # # # # # # # # # # # # # # # # # # #\n\n    # Transformer model hyperparameters\n    # # # # # # # # # # # # # # # # # # # # # # # #\n    MODEL_SIZE = 512                               # Transformer model size (d_{model} in de paper).\n    MULTIHEAD_ATTENTION_ACTIVATION = \'linear\'     # Activation the input projections in the Multi-Head Attention blocks.\n    FF_SIZE = MODEL_SIZE * 4                      # Size of the feed-forward layers of the Transformer model.\n    N_HEADS = 8                                   # Number of parallel attention layers of the Transformer model.\n    # # # # # # # # # # # # # # # # # # # # # # # #\n\n    # Regularizers\n    REGULARIZATION_FN = \'L2\'                      # Regularization function. \'L1\', \'L2\' and \'L1_L2\' supported.\n    WEIGHT_DECAY = 1e-4                           # L2 regularization\n    RECURRENT_WEIGHT_DECAY = 0.                   # L2 regularization in recurrent layers\n\n    DROPOUT_P = 0.1                               # Percentage of units to drop (0 means no dropout).\n    RECURRENT_INPUT_DROPOUT_P = 0.                # Percentage of units to drop in input cells of recurrent layers.\n    RECURRENT_DROPOUT_P = 0.                      # Percentage of units to drop in recurrent layers.\n    ATTENTION_DROPOUT_P = 0.1                     # Percentage of units to drop in attention layers (0 means no dropout).\n\n    USE_NOISE = True                              # Use gaussian noise during training\n    NOISE_AMOUNT = 0.01                           # Amount of noise\n\n    USE_BATCH_NORMALIZATION = True                # If True it is recommended to deactivate Dropout\n    BATCH_NORMALIZATION_MODE = 1                  # See documentation in Keras\' BN\n\n    USE_PRELU = False                             # use PReLU activations as regularizer.\n    USE_L1 = False                                # L1 normalization on the features.\n    USE_L2 = False                                # L2 normalization on the features.\n\n    DOUBLE_STOCHASTIC_ATTENTION_REG = 0.0         # Doubly stochastic attention (Eq. 14 from arXiv:1502.03044)\n\n    # Results plot and models storing parameters.\n    EXTRA_NAME = \'\'                               # This will be appended to the end of the model name.\n    if MODEL_TYPE == \'AttentionRNNEncoderDecoder\':\n        MODEL_NAME = TASK_NAME + \'_\' + SRC_LAN + TRG_LAN + \'_\' + MODEL_TYPE + \\\n                 \'_src_emb_\' + str(SOURCE_TEXT_EMBEDDING_SIZE) + \\\n                 \'_bidir_\' + str(BIDIRECTIONAL_ENCODER) + \\\n                 \'_enc_\' + ENCODER_RNN_TYPE + \'_\' + str(ENCODER_HIDDEN_SIZE) + \\\n                 \'_dec_\' + DECODER_RNN_TYPE + \'_\' + str(DECODER_HIDDEN_SIZE) + \\\n                 \'_deepout_\' + \'_\'.join([layer[0] for layer in DEEP_OUTPUT_LAYERS]) + \\\n                 \'_trg_emb_\' + str(TARGET_TEXT_EMBEDDING_SIZE) + \\\n                 \'_\' + OPTIMIZER + \'_\' + str(LR)\n    elif MODEL_TYPE == \'Transformer\':\n        MODEL_NAME = TASK_NAME + \'_\' + SRC_LAN + TRG_LAN + \'_\' + MODEL_TYPE + \\\n                 \'_model_size_\' + str(MODEL_SIZE) + \\\n                 \'_ff_size_\' + str(FF_SIZE) + \\\n                 \'_num_heads_\' + str(N_HEADS) + \\\n                 \'_encoder_blocks_\' + str(N_LAYERS_ENCODER) + \\\n                 \'_decoder_blocks_\' + str(N_LAYERS_DECODER) + \\\n                 \'_deepout_\' + \'_\'.join([layer[0] for layer in DEEP_OUTPUT_LAYERS]) + \\\n                 \'_\' + OPTIMIZER + \'_\' + str(LR)\n    else:\n        MODEL_NAME = TASK_NAME + \'_\' + SRC_LAN + TRG_LAN + \'_\' +\\\n                     MODEL_TYPE + \'_\' + OPTIMIZER + \'_\' + str(LR)\n\n    MODEL_NAME += EXTRA_NAME\n\n    STORE_PATH = \'trained_models/\' + MODEL_NAME + \'/\'  # Models and evaluation results will be stored here.\n    DATASET_STORE_PATH = \'datasets/\'                   # Dataset instance will be stored here.\n\n    # Tensorboard configuration. Only if the backend is Tensorflow. Otherwise, it will be ignored.\n    TENSORBOARD = True                       # Switches On/Off the tensorboard callback.\n    LOG_DIR = \'tensorboard_logs\'             # Directory to store teh model. Will be created inside STORE_PATH.\n    EMBEDDINGS_FREQ = 1                      # Frequency (in epochs) at which selected embedding layers will be saved.\n\n    SAMPLING_SAVE_MODE = \'list\'                        # \'list\': Store in a text file, one sentence per line.\n    VERBOSE = 1                                        # Verbosity level.\n    RELOAD = 0                                         # If 0 start training from scratch, otherwise the model.\n                                                       # Saved on epoch \'RELOAD\' will be used.\n    RELOAD_EPOCH = True                                # Select whether we reload epoch or update number.\n\n    REBUILD_DATASET = True                             # Build again or use stored instance.\n    MODE = \'training\'                                  # \'training\' or \'sampling\' (if \'sampling\' then RELOAD must\n                                                       # be greater than 0 and EVAL_ON_SETS will be used).\n\n    # Extra parameters for special trainings. In most cases, they should be set to `False`\n    TRAIN_ON_TRAINVAL = False                          # train the model on both training and validation sets combined.\n    FORCE_RELOAD_VOCABULARY = False                    # force building a new vocabulary from the training samples\n                                                       # applicable if RELOAD > 1\n\n    # ================================================ #\n    parameters = locals().copy()\n    return parameters\n'"
meta-optimizers/spearmint/__init__.py,0,b''
meta-optimizers/spearmint/spearmint_opt.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport logging\nimport subprocess\nimport os\nimport sys\n\n# sys.path.append(""../../"") # Adds higher directory to python modules path.\nsys.path.insert(1, os.path.abspath("".""))\nsys.path.insert(0, os.path.abspath(""../../""))\nfrom config import load_parameters\nfrom main import check_params, train_model\n\nlogging.basicConfig(level=logging.INFO, format=\'[%(asctime)s] %(message)s\', datefmt=\'%d/%m/%Y %H:%M:%S\')\nlogger = logging.getLogger(__name__)\nmetric_name = \'Bleu_4\'\nmaximize = True  # Select whether we want to maximize the metric or minimize it\nd = dict(os.environ.copy())\nd[\'LC_NUMERIC\'] = \'en_US.utf-8\'\n\n\ndef invoke_model(parameters):\n    """"""\n    Loads a model, trains it and evaluates it.\n    :param parameters: Model parameters\n    :return: Metric to minimize value.\n    """"""\n\n    model_params = load_parameters()\n    model_name = model_params[""MODEL_TYPE""]\n    for parameter in list(parameters):\n        model_params[parameter] = parameters[parameter][0]\n        logger.debug(""Assigning to %s the value %s"" % (str(parameter), parameters[parameter][0]))\n        model_name += \'_\' + str(parameter) + \'_\' + str(parameters[parameter][0])\n    model_params[""MODEL_NAME""] = model_name\n    # models and evaluation results will be stored here\n    model_params[""STORE_PATH""] = os.path.join(\'trained_models\', model_params[""MODEL_NAME""])\n    check_params(model_params)\n    assert model_params[\'MODE\'] == \'training\', \'You can only launch Spearmint when training!\'\n    logger.info(\'Running training.\')\n    train_model(model_params)\n\n    results_path = os.path.join(model_params[\'STORE_PATH\'],\n                                model_params[\'EVAL_ON_SETS\'][0] + \'.\' + model_params[\'METRICS\'][0])\n\n    # Recover the highest metric score\n    metric_pos_cmd = ""head -n 1 "" + results_path + \\\n                     "" |awk -v metric="" + metric_name + \\\n                     "" \'BEGIN{FS=\\"",\\""}"" \\\n                     ""{for (i=1; i<=NF; i++) if ($i == metric) print i;}\'""\n    metric_pos = subprocess.Popen(metric_pos_cmd,\n                                  stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True).communicate()[0][:-1]\n    cmd = ""tail -n +2 "" + results_path + \\\n          "" |awk -v m_pos="" + str(metric_pos) + \\\n          "" \'BEGIN{FS=\\"",\\""}{print $m_pos}\'|sort -gr|head -n 1""\n    ps = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True, env=d)\n    metric_value = float(ps.communicate()[0])\n    print (""Best %s: %f"" % (metric_name, metric_value))\n\n    return 1. - metric_value if maximize else metric_value  # Spearmint minimizes a function\n\n\ndef main(job_id, params):\n    """"""\n    Launches the spearmint job\n    :param job_id: Job identifier.\n    :param params: Model parameters.\n    :return: Metric to minimize value.\n    """"""\n    print (params)\n    return invoke_model(params)\n\n\nif __name__ == ""__main__"":\n    # Testing function\n    params = {\'SOURCE_TEXT_EMBEDDING_SIZE\': [1],\n              \'ENCODER_HIDDEN_SIZE\': [2],\n              \'TARGET_TEXT_EMBEDDING_SIZE\': [1],\n              \'DECODER_HIDDEN_SIZE\': [2],\n              \'MAX_EPOCH\': [2],\n              \'START_EVAL_ON_EPOCH\': [1]}\n    main(1, params)\n'"
tests/NMT_architectures/attention_ConditionalGRU.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_ConditionalGRU_add():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'GRU\'\n    params[\'DECODER_RNN_TYPE\'] = \'ConditionalGRU\'\n    params[\'N_LAYERS_DECODER\'] = 1\n    params[\'ATTENTION_MODE\'] = \'add\'\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_ConditionalGRU_dot():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'GRU\'\n    params[\'DECODER_RNN_TYPE\'] = \'ConditionalGRU\'\n    params[\'N_LAYERS_DECODER\'] = 1\n    params[\'ATTENTION_MODE\'] = \'dot\'\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_ConditionalGRU_scaled():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'GRU\'\n    params[\'DECODER_RNN_TYPE\'] = \'ConditionalGRU\'\n    params[\'N_LAYERS_DECODER\'] = 1\n    params[\'ATTENTION_MODE\'] = \'scaled-dot\'\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/NMT_architectures/attention_ConditionalLSTM.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_ConditionalLSTM_add():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'DECODER_RNN_TYPE\'] = \'ConditionalLSTM\'\n    params[\'N_LAYERS_DECODER\'] = 1\n    params[\'ATTENTION_MODE\'] = \'add\'\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + params[\'ATTENTION_MODE\'] + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_ConditionalLSTM_dot():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'DECODER_RNN_TYPE\'] = \'ConditionalLSTM\'\n    params[\'N_LAYERS_DECODER\'] = 1\n    params[\'ATTENTION_MODE\'] = \'dot\'\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + params[\'ATTENTION_MODE\'] + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_ConditionalLSTM_scaled():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'DECODER_RNN_TYPE\'] = \'ConditionalLSTM\'\n    params[\'N_LAYERS_DECODER\'] = 1\n    params[\'ATTENTION_MODE\'] = \'scaled-dot\'\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + params[\'ATTENTION_MODE\'] + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/NMT_architectures/attention_GRU.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_GRU_add():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'GRU\'\n    params[\'DECODER_RNN_TYPE\'] = \'GRU\'\n    params[\'N_LAYERS_DECODER\'] = 1\n    params[\'ATTENTION_MODE\'] = \'add\'\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + params[\'ATTENTION_MODE\'] + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_ConditionalGRU_dot():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'GRU\'\n    params[\'DECODER_RNN_TYPE\'] = \'GRU\'\n    params[\'N_LAYERS_DECODER\'] = 1\n    params[\'ATTENTION_MODE\'] = \'dot\'\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + params[\'ATTENTION_MODE\'] + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_ConditionalGRU_scaled():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'GRU\'\n    params[\'DECODER_RNN_TYPE\'] = \'GRU\'\n    params[\'N_LAYERS_DECODER\'] = 1\n    params[\'ATTENTION_MODE\'] = \'scaled-dot\'\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + params[\'ATTENTION_MODE\'] + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/NMT_architectures/attention_LSTM.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_LSTM_add():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'DECODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'N_LAYERS_DECODER\'] = 1\n    params[\'ATTENTION_MODE\'] = \'add\'\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + params[\'ATTENTION_MODE\'] + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_LSTM_dot():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'DECODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'N_LAYERS_DECODER\'] = 1\n    params[\'ATTENTION_MODE\'] = \'dot\'\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + params[\'ATTENTION_MODE\'] + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_LSTM_scaled():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'DECODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'N_LAYERS_DECODER\'] = 1\n    params[\'ATTENTION_MODE\'] = \'scaled-dot\'\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + params[\'ATTENTION_MODE\'] + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/NMT_architectures/bidir_deep_GRU_GRU.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_NMT_Bidir_deep_GRU_GRU():\n    params = load_tests_params()\n\n    # Current test params: Two-layered GRU - GRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 2\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'GRU\'\n    params[\'DECODER_RNN_TYPE\'] = \'GRU\'\n    params[\'N_LAYERS_DECODER\'] = 2\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/NMT_architectures/bidir_deep_LSTM_GRU.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_NMT_Bidir_deep_LSTM_GRU():\n    params = load_tests_params()\n\n    # Current test params: Two-layered LSTM - GRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 2\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = True\n    params[\'ENCODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'DECODER_RNN_TYPE\'] = \'GRU\'\n    params[\'N_LAYERS_DECODER\'] = 2\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/NMT_architectures/shallow_GRU_ConditionalLSTM.py,0,"b'import argparse\nimport os\nimport pytest\n\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_NMT_Bidir_GRU_ConditionalLSTM():\n    params = load_tests_params()\n\n    # Current test params: Single layered GRU - ConditionalLSTM\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = False\n    params[\'ENCODER_RNN_TYPE\'] = \'GRU\'\n    params[\'DECODER_RNN_TYPE\'] = \'ConditionalLSTM\'\n    params[\'N_LAYERS_DECODER\'] = 1\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_NMT_Unidir_GRU_ConditionalLSTM():\n    params = load_tests_params()\n\n    # Current test params: Single layered GRU - ConditionalLSTM\n    params[\'BIDIRECTIONAL_ENCODER\'] = False\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = False\n    params[\'ENCODER_RNN_TYPE\'] = \'GRU\'\n    params[\'DECODER_RNN_TYPE\'] = \'ConditionalLSTM\'\n    params[\'N_LAYERS_DECODER\'] = 1\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/NMT_architectures/shallow_GRU_LSTM.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_NMT_Bidir_GRU_LSTM():\n    params = load_tests_params()\n\n    # Current test params: Single layered GRU - LSTM\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = False\n    params[\'ENCODER_RNN_TYPE\'] = \'GRU\'\n    params[\'DECODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'N_LAYERS_DECODER\'] = 1\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_NMT_Unidir_GRU_LSTM():\n    params = load_tests_params()\n\n    # Current test params: Single layered GRU - LSTM\n    params[\'BIDIRECTIONAL_ENCODER\'] = False\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = False\n    params[\'ENCODER_RNN_TYPE\'] = \'GRU\'\n    params[\'DECODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'N_LAYERS_DECODER\'] = 1\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/NMT_architectures/shallow_LSTM_ConditionalGRU.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_NMT_Bidir_LSTM_ConditionalGRU():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = False\n    params[\'ENCODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'DECODER_RNN_TYPE\'] = \'ConditionalGRU\'\n    params[\'N_LAYERS_DECODER\'] = 1\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_NMT_Unidir_LSTM_ConditionalGRU():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - ConditionalGRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = False\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = False\n    params[\'ENCODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'DECODER_RNN_TYPE\'] = \'ConditionalGRU\'\n    params[\'N_LAYERS_DECODER\'] = 1\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/NMT_architectures/shallow_LSTM_LSTM.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_NMT_Bidir_LSTM_LSTM():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - LSTM\n    params[\'BIDIRECTIONAL_ENCODER\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = False\n    params[\'ENCODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'DECODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'N_LAYERS_DECODER\'] = 1\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_NMT_Unidir_LSTM_LSTM():\n    params = load_tests_params()\n\n    # Current test params: Single layered LSTM - LSTM\n    params[\'BIDIRECTIONAL_ENCODER\'] = False\n    params[\'N_LAYERS_ENCODER\'] = 1\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = False\n    params[\'ENCODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'DECODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'N_LAYERS_DECODER\'] = 1\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/NMT_architectures/unidir_deep_GRU_ConditionalLSTM.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_NMT_Unidir_deep_GRU_ConditionalLSTM():\n    params = load_tests_params()\n\n    # Current test params: Single layered GRU - GRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = False\n    params[\'N_LAYERS_ENCODER\'] = 2\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = False\n    params[\'ENCODER_RNN_TYPE\'] = \'GRU\'\n    params[\'DECODER_RNN_TYPE\'] = \'ConditionalLSTM\'\n    params[\'N_LAYERS_DECODER\'] = 2\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/NMT_architectures/unidir_deep_LSTM_ConditionalLSTM.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_NMT_Unidir_deep_LSTM_ConditionalLSTM():\n    params = load_tests_params()\n\n    # Current test params: Single layered GRU - GRU\n    params[\'BIDIRECTIONAL_ENCODER\'] = False\n    params[\'N_LAYERS_ENCODER\'] = 2\n    params[\'BIDIRECTIONAL_DEEP_ENCODER\'] = False\n    params[\'ENCODER_RNN_TYPE\'] = \'LSTM\'\n    params[\'DECODER_RNN_TYPE\'] = \'ConditionalLSTM\'\n    params[\'N_LAYERS_DECODER\'] = 2\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/NMT_architectures/unidir_deep_transformer.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_transformer():\n    params = load_tests_params()\n\n    # Current test params: Transformer\n    params[\'MODEL_TYPE\'] = \'Transformer\'\n    params[\'N_LAYERS_ENCODER\'] = 2\n    params[\'N_LAYERS_DECODER\'] = 2\n    params[\'MULTIHEAD_ATTENTION_ACTIVATION\'] = \'relu\'\n    params[\'MODEL_SIZE\'] = 8\n    params[\'FF_SIZE\'] = params[\'MODEL_SIZE\'] * 4\n    params[\'N_HEADS\'] = 2\n    params[\'REBUILD_DATASET\'] = True\n    params[\'OPTIMIZED_SEARCH\'] = True\n    params[\'POS_UNK\'] = False\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_model_size_\' + str(params[\'MODEL_SIZE\']) + \\\n        \'_ff_size_\' + str(params[\'FF_SIZE\']) + \\\n        \'_num_heads_\' + str(params[\'N_HEADS\']) + \\\n        \'_encoder_blocks_\' + str(params[\'N_LAYERS_ENCODER\']) + \\\n        \'_decoder_blocks_\' + str(params[\'N_LAYERS_DECODER\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/NMT_architectures/unidir_deep_transformer_tied_embeddings.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_transformer():\n    params = load_tests_params()\n\n    # Current test params: Transformer\n    params[\'MODEL_TYPE\'] = \'Transformer\'\n    params[\'TIED_EMBEDDINGS\'] = True\n    params[\'N_LAYERS_ENCODER\'] = 2\n    params[\'N_LAYERS_DECODER\'] = 2\n    params[\'MULTIHEAD_ATTENTION_ACTIVATION\'] = \'relu\'\n    params[\'MODEL_SIZE\'] = 8\n    params[\'FF_SIZE\'] = params[\'MODEL_SIZE\'] * 4\n    params[\'N_HEADS\'] = 2\n    params[\'REBUILD_DATASET\'] = True\n    params[\'OPTIMIZED_SEARCH\'] = True\n    params[\'POS_UNK\'] = False\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_model_size_\' + str(params[\'MODEL_SIZE\']) + \\\n        \'_ff_size_\' + str(params[\'FF_SIZE\']) + \\\n        \'_num_heads_\' + str(params[\'N_HEADS\']) + \\\n        \'_encoder_blocks_\' + str(params[\'N_LAYERS_ENCODER\']) + \\\n        \'_decoder_blocks_\' + str(params[\'N_LAYERS_DECODER\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/data_engine/test_data_types.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import clean_dirs, load_transformer_test_params\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_text_features_none():\n    params = load_transformer_test_params()\n\n    # Current test params:\n    params[\'INPUTS_TYPES_DATASET\'] = [\'text\', \'text\']\n    params[\'OUTPUTS_TYPES_DATASET\'] = [\'text\']\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_text_features_target_text():\n    params = load_transformer_test_params()\n\n    # Current test params:\n    params[\'INPUTS_TYPES_DATASET\'] = [\'text\', \'text\']\n    params[\'OUTPUTS_TYPES_DATASET\'] = [\'text-features\']\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_text_features_state_below():\n    params = load_transformer_test_params()\n\n    # Current test params:\n    params[\'INPUTS_TYPES_DATASET\'] = [\'text\', \'text-features\']\n    params[\'OUTPUTS_TYPES_DATASET\'] = [\'text\']\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_text_features_src():\n    params = load_transformer_test_params()\n\n    # Current test params:\n    params[\'INPUTS_TYPES_DATASET\'] = [\'text-features\', \'text\']\n    params[\'OUTPUTS_TYPES_DATASET\'] = [\'text\']\n\n    params[\'REBUILD_DATASET\'] = True\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = params[\'STORE_PATH\'] + \'/config.pkl\'\n    parser.models = [params[\'STORE_PATH\'] + \'/epoch_\' + str(1)]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/data_engine/test_prepare_data.py,0,"b""import pytest\nimport os\n\nimport copy\nfrom config import load_parameters\nfrom data_engine.prepare_data import build_dataset, update_dataset_from_file, prepare_references\nfrom keras_wrapper.dataset import Dataset, loadDataset\n\n\ndef test_build_datset():\n    params = load_parameters()\n    for verbose in range(2):\n        params['REBUILD_DATASET'] = True\n        params['VERBOSE'] = verbose\n        ds = build_dataset(params)\n        assert isinstance(ds, Dataset)\n        len_splits = [('train', 9900), ('val', 100), ('test', 2996)]\n        for split, len_split in len_splits:\n            assert eval('ds.len_' + split) == len_split\n            assert eval('all(ds.loaded_' + split + ')')\n            assert len(eval('ds.X_' + split + str([params['INPUTS_IDS_DATASET'][0]]))) == len_split\n            assert len(eval('ds.Y_' + split + str([params['OUTPUTS_IDS_DATASET'][0]]))) == len_split\n\n\ndef test_load_dataset():\n    params = load_parameters()\n    ds = loadDataset(os.path.join('datasets',\n                                  'Dataset_' + params['DATASET_NAME'] + '_' + params['SRC_LAN'] + params[\n                                      'TRG_LAN'] + '.pkl'))\n    assert isinstance(ds, Dataset)\n    assert isinstance(ds.vocabulary, dict)\n    assert len(list(ds.vocabulary)) >= 3\n    for voc in ds.vocabulary:\n        assert len(list(ds.vocabulary[voc])) == 2\n\n\ndef test_update_dataset_from_file():\n    params = load_parameters()\n    for rebuild_dataset in [True, False]:\n        params['REBUILD_DATASET'] = rebuild_dataset\n        for splits in [[], None, ['val']]:\n            ds = build_dataset(params)\n            assert isinstance(ds, Dataset)\n            for output_text_filename in [None,\n                                         os.path.join(params['DATA_ROOT_PATH'],\n                                                      params['TEXT_FILES']['test'] + params['TRG_LAN'])]:\n                for remove_outputs in [True, False]:\n                    for compute_state_below in [True, False]:\n                        for recompute_references in [True, False]:\n                            ds2 = update_dataset_from_file(copy.deepcopy(ds),\n                                                           os.path.join(params['DATA_ROOT_PATH'],\n                                                                        params['TEXT_FILES']['test'] + params[\n                                                                            'SRC_LAN']),\n                                                           params,\n                                                           splits=splits,\n                                                           output_text_filename=output_text_filename,\n                                                           remove_outputs=remove_outputs,\n                                                           compute_state_below=compute_state_below,\n                                                           recompute_references=recompute_references)\n                            assert isinstance(ds2, Dataset)\n\n    # Final check: We update the val set with the test data. We check that dimensions match.\n    split = 'val'\n    len_test = 2996\n    ds2 = update_dataset_from_file(copy.deepcopy(ds),\n                                   params['DATA_ROOT_PATH'] + params['TEXT_FILES']['test'] + params['SRC_LAN'],\n                                   params,\n                                   splits=[split],\n                                   output_text_filename=os.path.join(params['DATA_ROOT_PATH'],\n                                                                     params['TEXT_FILES']['test'] + params['TRG_LAN']),\n                                   remove_outputs=False,\n                                   compute_state_below=True,\n                                   recompute_references=True)\n    assert isinstance(ds2, Dataset)\n    assert eval('ds2.len_' + split) == len_test\n    assert eval('all(ds2.loaded_' + split + ')')\n    assert len(eval('ds2.X_' + split + str([params['INPUTS_IDS_DATASET'][0]]))) == len_test\n    assert len(eval('ds2.Y_' + split + str([params['OUTPUTS_IDS_DATASET'][0]]))) == len_test\n\n\ndef test_keep_n_captions():\n    params = load_parameters()\n    params['REBUILD_DATASET'] = True\n    ds = build_dataset(params)\n    len_splits = {'train': 9900, 'val': 100, 'test': 2996}\n\n    for splits in [[], None, ['val'], ['val', 'test']]:\n        prepare_references(ds, 1, n=1, set_names=splits)\n        if splits is not None:\n            for split in splits:\n                len_split = len_splits[split]\n                assert eval('ds.len_' + split) == len_split\n                assert eval('all(ds.loaded_' + split + ')')\n                assert len(eval('ds.X_' + split + str([params['INPUTS_IDS_DATASET'][0]]))) == len_split\n                assert len(eval('ds.Y_' + split + str([params['OUTPUTS_IDS_DATASET'][0]]))) == len_split\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/encodings/test_sampling.py,0,"b'import pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\n\n\ndef test_sampling_maxlikelihood():\n    params = load_tests_params()\n\n    params[\'REBUILD_DATASET\'] = True\n    params[\'INPUT_VOCABULARY_SIZE\'] = 550\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = 550\n\n    params[\'POS_UNK\'] = True\n    params[\'HEURISTIC\'] = 0\n    params[\'ALIGN_FROM_RAW\'] = True\n\n    # Sampling params: Show some samples during training.\n    params[\'SAMPLE_ON_SETS\'] = [\'train\', \'val\']\n    params[\'N_SAMPLES\'] = 10\n    params[\'START_SAMPLING_ON_EPOCH\'] = 0\n    params[\'SAMPLE_EACH_UPDATES\'] = 50\n    params[\'SAMPLING\'] = \'max_likelihood\'\n\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_sampling_multinomial():\n    params = load_tests_params()\n\n    params[\'REBUILD_DATASET\'] = True\n    params[\'INPUT_VOCABULARY_SIZE\'] = 550\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = 550\n\n    params[\'POS_UNK\'] = True\n    params[\'HEURISTIC\'] = 0\n    params[\'ALIGN_FROM_RAW\'] = True\n\n    # Sampling params: Show some samples during training.\n    params[\'SAMPLE_ON_SETS\'] = [\'train\', \'val\']\n    params[\'N_SAMPLES\'] = 10\n    params[\'START_SAMPLING_ON_EPOCH\'] = 0\n    params[\'SAMPLE_EACH_UPDATES\'] = 50\n    params[\'SAMPLING\'] = \'multinomial\'\n\n    dataset = build_dataset(params)\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/encodings/test_unk_replace.py,0,"b'import argparse\nimport os\nimport pytest\nfrom tests.test_config import load_tests_params, clean_dirs\nfrom data_engine.prepare_data import build_dataset\nfrom nmt_keras.training import train_model\nfrom nmt_keras.apply_model import sample_ensemble, score_corpus\n\n\ndef test_unk_replace_0():\n    params = load_tests_params()\n\n    params[\'REBUILD_DATASET\'] = True\n    params[\'INPUT_VOCABULARY_SIZE\'] = 0\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = 50\n\n    params[\'POS_UNK\'] = True\n    params[\'HEURISTIC\'] = 0\n    params[\'ALIGN_FROM_RAW\'] = True\n\n    dataset = build_dataset(params)\n\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = os.path.join(params[\'STORE_PATH\'], \'config.pkl\')\n    parser.models = [os.path.join(params[\'STORE_PATH\'], \'epoch_\' + str(1))]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_unk_replace_1():\n    params = load_tests_params()\n\n    params[\'REBUILD_DATASET\'] = True\n    params[\'INPUT_VOCABULARY_SIZE\'] = 0\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = 50\n\n    params[\'POS_UNK\'] = True\n    params[\'HEURISTIC\'] = 1\n    params[\'ALIGN_FROM_RAW\'] = True\n    params[\'MAPPING\'] = os.path.join(params[\'DATA_ROOT_PATH\'], \'mapping.%s_%s.pkl\' % (params[\'SRC_LAN\'], params[\'TRG_LAN\']))\n\n    dataset = build_dataset(params)\n\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = os.path.join(params[\'STORE_PATH\'], \'config.pkl\')\n    parser.models = [os.path.join(params[\'STORE_PATH\'], \'epoch_\' + str(1))]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\ndef test_unk_replace_2():\n    params = load_tests_params()\n\n    params[\'REBUILD_DATASET\'] = True\n    params[\'INPUT_VOCABULARY_SIZE\'] = 0\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = 50\n\n    params[\'POS_UNK\'] = True\n    params[\'HEURISTIC\'] = 2\n    params[\'ALIGN_FROM_RAW\'] = True\n\n    dataset = build_dataset(params)\n    params[\'MAPPING\'] = os.path.join(params[\'DATA_ROOT_PATH\'], \'mapping.%s_%s.pkl\' % (params[\'SRC_LAN\'], params[\'TRG_LAN\']))\n\n    params[\'INPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'INPUTS_IDS_DATASET\'][0]]\n    params[\'OUTPUT_VOCABULARY_SIZE\'] = dataset.vocabulary_len[params[\'OUTPUTS_IDS_DATASET\'][0]]\n    params[\'MODEL_NAME\'] = \\\n        params[\'TASK_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'_\' + params[\'MODEL_TYPE\'] + \\\n        \'_src_emb_\' + str(params[\'SOURCE_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_bidir_\' + str(params[\'BIDIRECTIONAL_ENCODER\']) + \\\n        \'_enc_\' + params[\'ENCODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_ENCODER\']) + \'_\' + str(\n            params[\'ENCODER_HIDDEN_SIZE\']) + \\\n        \'_dec_\' + params[\'DECODER_RNN_TYPE\'] + \'_*\' + str(params[\'N_LAYERS_DECODER\']) + \'_\' + str(\n            params[\'DECODER_HIDDEN_SIZE\']) + \\\n        \'_deepout_\' + \'_\'.join([layer[0] for layer in params[\'DEEP_OUTPUT_LAYERS\']]) + \\\n        \'_trg_emb_\' + str(params[\'TARGET_TEXT_EMBEDDING_SIZE\']) + \\\n        \'_\' + params[\'OPTIMIZER\'] + \'_\' + str(params[\'LR\'])\n\n    # Test several NMT-Keras utilities: train, sample, sample_ensemble, score_corpus...\n    print(""Training model"")\n    train_model(params)\n    params[\'RELOAD\'] = 1\n    print(""Done"")\n\n    parser = argparse.ArgumentParser(\'Parser for unit testing\')\n    parser.dataset = os.path.join(\n        params[\'DATASET_STORE_PATH\'],\n        \'Dataset_\' + params[\'DATASET_NAME\'] + \'_\' + params[\'SRC_LAN\'] + params[\'TRG_LAN\'] + \'.pkl\')\n\n    parser.text = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.splits = [\'val\']\n    parser.config = os.path.join(params[\'STORE_PATH\'], \'config.pkl\')\n    parser.models = [os.path.join(params[\'STORE_PATH\'], \'epoch_\' + str(1))]\n    parser.verbose = 0\n    parser.dest = None\n    parser.source = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'SRC_LAN\'])\n    parser.target = os.path.join(params[\'DATA_ROOT_PATH\'], params[\'TEXT_FILES\'][\'val\'] + params[\'TRG_LAN\'])\n    parser.weights = []\n    parser.glossary = None\n\n    for n_best in [True, False]:\n        parser.n_best = n_best\n        print(""Sampling with n_best = %s "" % str(n_best))\n        sample_ensemble(parser, params)\n        print(""Done"")\n\n    print(""Scoring corpus"")\n    score_corpus(parser, params)\n    print(""Done"")\n    clean_dirs(params)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/utils/test_nmt_utils.py,0,"b""import pytest\nfrom six import iteritems\nfrom config import load_parameters\nfrom utils.utils import update_parameters\n\n\ndef test_update_parameters():\n    params = load_parameters()\n    updates = {\n        'ENCODER_HIDDEN_SIZE': 0,\n        'BIDIRECTIONAL_ENCODER': False,\n        'NEW_PARAMETER': 'new_value',\n        'ADDITIONAL_OUTPUT_MERGE_MODE': 'Concat'\n    }\n    new_params = update_parameters(params, updates, restrict=False)\n    for k, new_val in iteritems(updates):\n        assert new_params[k] == updates[k]\n\n    new_params = update_parameters(params, updates, restrict=True)\n    for k, _ in iteritems(updates):\n        assert new_params[k] == params.get(k, 'new_value')\n    assert new_params['NEW_PARAMETER'] == updates['NEW_PARAMETER']\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/utils/test_process_word_vectors.py,0,"b'import inspect\nimport os\nimport pytest\nimport numpy as np\nfrom subprocess import call\nfrom utils.preprocess_text_word_vectors import txtvec2npy\n\n\ndef test_text_word2vec2npy():\n    # check whether files are present in folder\n    vectors_name = \'wiki.fiu_vro.vec\'\n    path = os.path.dirname(inspect.getfile(inspect.currentframe()))\n    if not os.path.exists(path + \'/\' + vectors_name):\n        call([""wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/"" + vectors_name + "" -O "" +\n              path + ""/"" + vectors_name],\n             shell=True)\n    txtvec2npy(path + \'/\' + vectors_name, \'./\', vectors_name[:-4])\n    vectors = np.load(\'./\' + vectors_name[:-4] + \'.npy\', allow_pickle=True).item()\n\n    assert len(list(vectors)) == 8769\n    assert vectors[\'kihlkunnan\'].shape[0] == 300\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
