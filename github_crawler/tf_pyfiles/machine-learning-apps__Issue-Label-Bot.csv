file_path,api_count,code
fairing/LabelPrediction.py,1,"b'# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Use Append builder and deploy the model.\n\nThe file needs to be named the same as the class containing the predict method.\nThis is how Seldon works.\nhttps://docs.seldon.io/projects/seldon-core/en/latest/python/python_wrapping.html\n\nThis is the Seldon interface class.\n""""""\n\nimport app\nimport logging\nimport tensorflow as tf\n\nclass LabelPrediction(object):\n  def __init__(self):\n    """"""""""""\n    self.graph = None\n    self.issue_labeler = None\n\n  def predict(self, X, feature_names):\n    """"""Predict using the model for given ndarray.""""""\n\n    if self.issue_labeler is None:\n      # Load the model.\n      logging.info(""Creating the issue labeler and initializing TF graph."")\n      self.graph = tf.get_default_graph()\n      self.issue_labeler = app.init_issue_labeler()\n\n    probabilities = self.issue_labeler.get_probabilities(body=X[1],\n                                                    title=X[0])\n\n    logging.info(""Probability keys: %s"", probabilities.keys())\n\n    p = [0] * 3\n    for i, k in enumerate([""bug"", ""feature_request"", ""question""]):\n      if not k in probabilities:\n        continue\n      p[i] = probabilities[k]\n\n    return [p]\n'"
fairing/deploy_with_fairing.py,0,"b'import argparse\nimport logging\nimport fairing\nfrom fairing.builders.append import append\nimport fnmatch\nimport numpy as np\nimport os\nimport shutil\nimport tempfile\n\ndef deploy(registry, base_image):\n  logging.getLogger().setLevel(logging.INFO)\n  fairing.config.set_builder(\'append\', registry=registry, base_image=base_image)\n\n  # Add a common label.\n  labels = {\n    ""app"": ""mlapp"",\n  }\n  fairing.config.set_deployer(\'serving\', serving_class=""LabelPrediction"",\n                              labels=labels)\n\n  # Get the list of all the python files\n  this_dir = os.path.dirname(__file__)\n  base_dir = os.path.abspath(os.path.join(this_dir, ""..""))\n  flask_dir = os.path.join(base_dir, ""flask_app"")\n  input_files = []\n\n  # Context gymnastics.\n  # Create a directory with the desired layout for app.\n  # We need to flatten things because Seldon expects the interface module\n  # to be a top level module.\n  #\n  # TODO(https://github.com/SeldonIO/seldon-core/issues/465): Seldon\n  # can\'t handle the module being nested; it needs to be top level module.\n\n  context_dir = tempfile.mkdtemp()\n\n  logging.info(""Using context dir %s"", context_dir)\n\n  for dir_to_copy in [flask_dir, this_dir]:\n    for root, dirs, files in os.walk(dir_to_copy, topdown=False):\n      for name in files:\n        if not fnmatch.fnmatch(name, ""*.py""):\n          continue\n        shutil.copyfile(os.path.join(root, name),\n                        os.path.join(context_dir, name))\n        input_files.append(name)\n\n  # Need to change to context_dir so that paths are added at the correct\n  # location in the context .tar.gz\n  os.chdir(context_dir)\n  fairing.config.set_preprocessor(\'python\', input_files=input_files)\n  fairing.config.run()\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n    ""--registry"", default="""", type=str,\n    help=(""The registry where your images should be pushed""))\n\n  parser.add_argument(\n    ""--base_image"", default="""", type=str,\n    help=(""The base image to use""))\n\n  args = parser.parse_args()\n  deploy(args.registry, args.base_image)'"
flask_app/app.py,1,"b'import os\nimport logging\nimport json\nfrom collections import defaultdict\nimport hmac\nfrom flask import (abort, Flask, session, render_template,\n                   session, redirect, url_for, request,\n                   flash, jsonify)\nfrom flask_session import Session\nfrom sqlalchemy import desc\nfrom mlapp import GitHubApp\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import get_file\nfrom utils import IssueLabeler\nimport dill as dpickle\nfrom urllib.request import urlopen\nfrom sql_models import db, Issues, Predictions\nimport tensorflow as tf\nimport requests\nimport traceback\nimport yaml\nimport random\nfrom forward_utils import get_forwarded_repos\nfrom forward_utils import publish_message\nfrom forward_utils import create_topic_if_not_exists\n\napp = Flask(__name__)\napp_url = os.getenv(\'APP_URL\')\n\n# Configure session to use filesystem. Hamel: BOILERPLATE.\napp.config[""SESSION_PERMANENT""] = False\nSession(app)\n\n# Bind database to flask app\napp.config[""SQLALCHEMY_DATABASE_URI""] = os.getenv(""DATABASE_URL"")\napp.config[""SQLALCHEMY_TRACK_MODIFICATIONS""] = False\ndb.init_app(app)\n\n# Additional Setup inspired by https://github.com/bradshjg/flask-githubapp/blob/master/flask_githubapp/core.py\napp.webhook_secret = os.getenv(\'WEBHOOK_SECRET\')\nLOG = logging.getLogger(__name__)\n\n# set the prediction threshold for everything except for the label question which has a different threshold\nprediction_threshold = defaultdict(lambda: .52)\nprediction_threshold[\'question\'] = .60\n\n# set the project id and topic name for GCP pubsub\nPUBSUB_PROJECT_ID = os.environ[\'GCP_PROJECT_ID\']\nPUBSUB_TOPIC_NAME = os.environ[\'GCP_PUBSUB_TOPIC_NAME\']\n\n# get repos that should possibly be forwarded\n# dict: {repo_owner/repo_name: proportion}\nforwarded_repos = get_forwarded_repos(os.getenv(""LABEL_BOT_CONFIG"",\n                                                ""forwarded_repo.yaml""))\n\ndef init_issue_labeler():\n    ""Load all necessary artifacts to make predictions.""\n    title_pp_url = ""https://storage.googleapis.com/codenet/issue_labels/issue_label_model_files/title_pp.dpkl""\n    body_pp_url = \'https://storage.googleapis.com/codenet/issue_labels/issue_label_model_files/body_pp.dpkl\'\n    model_url = \'https://storage.googleapis.com/codenet/issue_labels/issue_label_model_files/Issue_Label_v1_best_model.hdf5\'\n    model_filename = \'downloaded_model.hdf5\'\n\n\n    with urlopen(title_pp_url) as f:\n        title_pp = dpickle.load(f)\n\n    with urlopen(body_pp_url) as f:\n        body_pp = dpickle.load(f)\n\n    model_path = get_file(fname=model_filename, origin=model_url)\n    model = load_model(model_path)\n\n    logging.info(f""Forwarded repo config:\\n{forwarded_repos}"")\n    return IssueLabeler(body_text_preprocessor=body_pp,\n                        title_text_preprocessor=title_pp,\n                        model=model)\n\ndef init():\n    ""Load all necessary artifacts to make predictions.""\n    logging.info(f""Initializing the app"")\n    logging.info(f""Forwarded repo config:\\n{forwarded_repos}"")\n    app.graph = tf.get_default_graph()\n    app.issue_labeler = init_issue_labeler()\n    create_topic_if_not_exists(PUBSUB_PROJECT_ID, PUBSUB_TOPIC_NAME)\n\n# this redirects http to https\n# from https://stackoverflow.com/a/53501072/1518630\n# @app.before_request\n# def before_request():\n#     if not request.is_secure and app.env != ""development"":\n#         url = request.url.replace(""http://"", ""https://"", 1)\n#         code = 301\n#         return redirect(url, code=code)\n\n# Webpage for app\n@app.route(""/"", methods=[""GET""])\ndef index():\n    ""Landing page""\n    results = db.engine.execute(""SELECT * FROM (SELECT distinct repo, username FROM issues a JOIN predictions b on a.issue_id=b.issue_id WHERE username != \'hamelsmu\' LIMIT 200) as t ORDER BY random() LIMIT 25"").fetchall()\n    num_active_users = f\'{db.engine.execute(""SELECT count(distinct username) FROM issues"").fetchall()[0][0]:,}\'\n    num_predictions = f\'{db.engine.execute(""SELECT count(*) FROM predictions"").fetchall()[0][0]:,}\'\n    num_repos = f\'{db.engine.execute(""select count(*) from (select distinct username, repo from issues) as t"").fetchall()[0][0]:,}\'\n    return render_template(""index.html"",\n                           results=results,\n                           num_active_users=num_active_users,\n                           num_repos=num_repos,\n                           num_predictions=num_predictions)\n\n# smee by default sends things to /event_handler route\n@app.route(""/event_handler"", methods=[""POST""])\ndef bot():\n    ""Handle payload""\n\n    logging.debug(""Request Data:\\n%s"", request.data)\n    if not request.json:\n        logging.error(""Request is not a json request. Please fix."")\n        # TODO(jlewi): What is the proper code invalid request?\n        abort(400)\n\n    logging.debug(""Handling request with action: %s"",\n                  request.json.get(\'action\', \'None\'))\n    # authenticate webhook to make sure it is from GitHub\n    verify_webhook(request)\n\n    # Check if payload corresponds to an issue being opened\n    if \'issue\' not in request.json:\n        logging.warning(""Event is not for an issue with action opened."")\n        return \'ok\'\n\n    # get metadata\n    installation_id = request.json[\'installation\'][\'id\']\n    issue_num = request.json[\'issue\'][\'number\']\n    private = request.json[\'repository\'][\'private\']\n    username, repo = request.json[\'repository\'][\'full_name\'].split(\'/\')\n    title = request.json[\'issue\'][\'title\']\n    body = request.json[\'issue\'][\'body\']\n\n    # don\'t do anything if repo is private.\n    if private:\n        logging.info(f""Recieved a private issue which is being skipped"")\n        return \'ok\'\n\n    logging.info(f""Recieved {username}/{repo}#{issue_num}"")\n    try:\n        # forward some issues of specific repos and select by their given forwarded proportion\n        # TODO(jlewi): We could probably simplify this because at this point\n        # for any repo/org that we are forwarding the issues we should probably\n        # always forward the issues.\n        forward_probability = None\n        repo_spec = f\'{username}/{repo}\'\n        if username in forwarded_repos.get(""orgs"", {}):\n            forward_probability = forwarded_repos[""orgs""][username]\n        elif repo_spec in forwarded_repos.get(""repos"", {}):\n            forward_probability = forwarded_repos[""repos""][repo_spec]\n\n        if forward_probability:\n            if random.random() <= forward_probability:\n                logging.info(f""Publishing {username}/{repo}#{issue_num} to ""\n                             f""projects/{PUBSUB_PROJECT_ID}/topics/{PUBSUB_TOPIC_NAME}"")\n                # send the event to pubsub\n                publish_message(PUBSUB_PROJECT_ID, PUBSUB_TOPIC_NAME,\n                                installation_id, username, repo, issue_num)\n                return f\'Labeling of {username}/{repo}/issues/{issue_num} delegated to microservice via pubsub.\'\n            else:\n                logging.info(f""{username}/{repo}#{issue_num} not selected for ""\n                             f""publishing to ""\n                             f""projects/{PUBSUB_PROJECT_ID}/topics/{PUBSUB_TOPIC_NAME}"")\n    except Exception as e:\n        logging.error(f""Exception occured while handling issue ""\n                      f""{username}/{repo}#{issue_num}\\n Exception: {e}\\n""\n                      f""{traceback.format_exc()}"")\n\n    # Check if payload corresponds to an issue being opened\n    if \'action\' not in request.json or request.json[\'action\'] != \'opened\':\n        logging.warning(""Event is not for an issue with action opened."")\n        return \'ok\'\n\n    # write the issue to the database using ORM\n    issue_db_obj = Issues(repo=repo,\n                          username=username,\n                          issue_num=issue_num,\n                          title=title,\n                          body=body)\n\n    db.session.add(issue_db_obj)\n    db.session.commit()\n\n    # make predictions with the model\n    with app.graph.as_default():\n        predictions = app.issue_labeler.get_probabilities(body=body, title=title)\n    #log to console\n    LOG.warning(f\'issue opened by {username} in {repo} #{issue_num}: {title} \\nbody:\\n {body}\\n\')\n    LOG.warning(f\'predictions: {str(predictions)}\')\n\n    # get the most confident prediction\n    argmax = max(predictions, key=predictions.get)\n\n    # get the isssue handle\n    issue = get_issue_handle(installation_id, username, repo, issue_num)\n\n\n    labeled = True\n    threshold = prediction_threshold[argmax]\n\n    # take an action if the prediction is confident enough\n    if (predictions[argmax] >= threshold):\n        # initialize the label name to = the argmax\n        label_name = argmax\n\n        # handle the yaml file\n        yaml = get_yaml(owner=username, repo=repo)\n        if yaml and \'label-alias\' in yaml:\n            if  argmax in yaml[\'label-alias\']:\n                LOG.warning(\'User has custom names: \', yaml[\'label-alias\'])\n                new_name = yaml[\'label-alias\'][argmax]\n                if new_name:\n                    label_name = new_name\n\n        # create message\n        message = f\'Issue-Label Bot is automatically applying the label `{label_name}` to this issue, with a confidence of {predictions[argmax]:.2f}. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \\n\\n Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard]({app_url}data/{username}/{repo}) and [code](https://github.com/hamelsmu/MLapp) for this bot.\'\n        # label the issue using the GitHub api\n        issue.add_labels(label_name)\n\n    else:\n        message = f\'Issue Label Bot is not confident enough to auto-label this issue. See [dashboard]({app_url}data/{username}/{repo}) for more details.\'\n        LOG.warning(f\'Not confident enough to label this issue: # {str(issue_num)}\')\n        labeled = False\n\n    # Make a comment using the GitHub api\n    comment = issue.create_comment(message)\n\n    # log the event to the database using ORM\n    issue_db_obj.add_prediction(comment_id=comment.id,\n                                prediction=argmax,\n                                probability=predictions[argmax],\n                                logs=str(predictions),\n                                threshold=threshold,\n                                labeled=labeled)\n    return \'ok\'\n\n@app.route(""/repos/<string:username>"", methods=[""GET""])\ndef get_repos(username):\n    ""Get repos actively installed in.""\n    ghapp = get_app()\n    app = ghapp.get_app()\n    try:\n        install_id =  app.app_installation_for_user(f\'{username}\').id\n    except:\n        return f\'No current installations for {username} found.\'\n\n    url = f\'https://api.github.com/installation/repositories\'\n    headers = {\'Authorization\': f\'token {ghapp.get_installation_access_token(install_id)}\',\n               \'Accept\': \'application/vnd.github.machine-man-preview+json\'}\n\n    response = requests.get(url=url, headers=headers, params={\'per_page\':100})\n    if response.status_code == 200:\n        repos = response.json()[\'repositories\']\n        repos_with_preds = [x.repo for x in Issues.query.filter(Issues.username == username and Issues.predictions != None).distinct(Issues.repo).all()]\n        return render_template(\'repos.html\', repos=repos, username=username, repos_with_preds=repos_with_preds)\n\n    else:\n        return response.status_code\n\n@app.route(\'/users\')\ndef show_users():\n    users = get_users()\n    users_with_preds = [x.username for x in Issues.query.filter(Issues.predictions != None).distinct(Issues.username).all()]\n\n    users = [{\'name\':a, \'is_pred\':b} for a,b in sorted([(x, x in users_with_preds) for x in users], key=lambda x: ~x[1])]\n\n    return render_template(\'users.html\', users=users, users_with_preds=users_with_preds)\n\n@app.route(""/data/<string:owner>/<string:repo>"", methods=[""GET"", ""POST""])\ndef data(owner, repo):\n    ""Route where users can see the Bot\'s recent predictions for a repo""\n    installed = app_installation_exists(owner=owner, repo=repo)\n    alert = None\n    if not installed:\n        alert = \'Warning: The app is no longer installed on this repo. Will not be able to update feedback, but you can still view predictions.\'\n\n    if not is_public(owner, repo):\n        return render_template(""data.html"",\n                               results=[],\n                               num_issues=0,\n                               owner=owner,\n                               repo=repo,\n                               is_public=False,\n                               error=f\'<span style=""font-weight:bold"">{owner}/{repo}</span> is a private repo or does not exist.\')\n\n    issues = Issues.query.filter(Issues.username == owner, Issues.repo == repo).all()\n    issue_numbers = [x.issue_id for x in issues]\n\n    if request.method == \'POST\':\n        if installed:\n            update_feedback(owner=owner, repo=repo)\n        else:\n            return render_template(""data.html"",\n                               results=[],\n                               num_issues=0,\n                               owner=owner,\n                               repo=repo,\n                               is_public=True,\n                               error=f\'App is no longer installed for <span style=""font-weight:bold"">{owner}/{repo}</span>. Cannot fetch feedback.\')\n\n    # get the 50 most recent predictions.\n    predictions = (Predictions.query.filter(Predictions.issue_id.in_(issue_numbers))\n                    .order_by(desc(Predictions.issue_id))\n                    .limit(50)\n                    .all())\n\n    num_issues = len(issues)\n    num_predictions = len(predictions)\n\n    return render_template(""data.html"",\n                           results=predictions,\n                           num_issues=num_issues,\n                           num_predictions=num_predictions,\n                           owner=owner,\n                           repo=repo,\n                           alert=alert,\n                           is_public=True,\n                           installed=installed)\n\n@app.route(""/health_check"", methods=[""GET""])\ndef health_check():\n    ""route for load balancer.""\n    return jsonify({\'success\':True}), 200, {\'ContentType\':\'application/json\'}\n\ndef update_feedback(owner, repo):\n    ""Update feedback for predicted labels for an owner/repo""\n    # authenticate webhook to make sure it is from GitHub\n    issues = Issues.query.filter(Issues.username == owner, Issues.repo == repo).order_by(Issues.issue_num.desc()).limit(50).all()\n    issue_numbers = [x.issue_id for x in issues]\n\n    # only update last 100 things to prevent edge cases on repos with large number of issues.\n    predictions = (Predictions.query.filter(Predictions.issue_id.in_(issue_numbers))\n                   .limit(100)\n                   .all())\n\n    # we only want to get the installation token once for the list of predictions.\n    ghapp = get_app()\n    installation_id = ghapp.get_installation_id(owner=owner, repo=repo)\n    installation_access_token = ghapp.get_installation_access_token(installation_id)\n\n    # grab all the reactions and update the statistics in the database.\n    for prediction in predictions:\n        try:\n            reactions = ghapp.get_reactions(owner=owner,\n                                            repo=repo,\n                                            comment_id=prediction.comment_id,\n                                            iat=installation_access_token)\n            prediction.likes = reactions[\'+1\']\n            prediction.dislikes = reactions[\'-1\']\n        except:\n            continue\n    db.session.commit()\n    print(f\'Successfully updated feedback based on reactions for {len(predictions)} predictions in {owner}/{repo}.\')\n\n\ndef get_app():\n    ""grab a fresh instance of the app handle.""\n    app_id = os.getenv(\'APP_ID\')\n    if not app_id:\n        raise ValueError(""APP_ID environment variable must be set."")\n    key_file_path = os.getenv(""GITHUB_APP_PEM_KEY"")\n    if not key_file_path:\n        raise ValueError(""GITHUB_APP_PEM_KEY environment variable must be set."")\n    ghapp = GitHubApp(pem_path=key_file_path, app_id=app_id)\n    return ghapp\n\ndef get_users():\n    ""git list of users.""\n    ghapp = get_app()\n    app = ghapp.get_app()\n    return [x.account[\'login\'] for x in list(app.app_installations())]\n\ndef app_installation_exists(owner, repo):\n    ""check if app is installed on the repo.""\n    ghapp = get_app()\n    try:\n        ghapp.get_installation_id(owner=owner, repo=repo)\n        return True\n    except:\n        return False\n\ndef get_issue_handle(installation_id, username, repository, number):\n    ""get an issue object.""\n    ghapp = get_app()\n    install = ghapp.get_installation(installation_id)\n    return install.issue(username, repository, number)\n\ndef get_yaml(owner, repo):\n    """"""\n    Looks for the yaml file in a /.github directory.\n\n    yaml file must be named issue_label_bot.yaml\n    """"""\n    ghapp = get_app()\n    try:\n        # get the app installation handle\n        inst_id = ghapp.get_installation_id(owner=owner, repo=repo)\n        inst = ghapp.get_installation(installation_id=inst_id)\n        # get the repo handle, which allows you got get the file contents\n        repo = inst.repository(owner=owner, repository=repo)\n        results = repo.file_contents(\'.github/issue_label_bot.yaml\').decoded\n\n    except:\n        return None\n\n    return yaml.safe_load(results)\n\nSIGNATURE_HEADER = \'X-Hub-Signature\'\n\ndef verify_webhook(request):\n    ""Make sure request is from GitHub.com""\n\n    # if we are testing, don\'t bother checking the payload\n    if os.getenv(\'DEVELOPMENT_FLAG\'): return True\n\n    # Inspired by https://github.com/bradshjg/flask-githubapp/blob/master/flask_githubapp/core.py#L191-L198\n    if SIGNATURE_HEADER not in request.headers:\n        logging.error(""Request is missing header %s"", SIGNATURE_HEADER)\n\n    signature = request.headers[SIGNATURE_HEADER].split(\'=\')[1]\n\n    mac = hmac.new(str.encode(app.webhook_secret), msg=request.data, digestmod=\'sha1\')\n\n    if not hmac.compare_digest(mac.hexdigest(), signature):\n        LOG.warning(\'GitHub hook signature verification failed.\')\n        abort(400)\n\ndef is_public(owner, repo):\n    ""Verify repo is public.""\n    try:\n        return requests.head(f\'https://github.com/{owner}/{repo}\').status_code == 200\n    except:\n        return False\n\nif __name__ == ""__main__"":\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    init()\n    with app.app_context():\n        # create tables if they do not exist\n        db.create_all()\n\n    # make sure things reload\n    app.jinja_env.auto_reload = True\n    app.config[\'TEMPLATES_AUTO_RELOAD\'] = True\n    app.run(debug=True, host=\'0.0.0.0\', port=os.getenv(\'PORT\'))\n'"
flask_app/forward_utils.py,0,"b'import os\nimport yaml\nfrom google.cloud import pubsub\n\ndef get_forwarded_repos(yaml_path=\'forwarded_repo.yaml\'):\n    with open(yaml_path, \'r\') as f:\n        config = yaml.safe_load(f)\n    return config\n\ndef check_topic_path_exists(project_id, topic_path):\n    """"""\n    Check if the topic path exists in the project.\n    Args:\n      project_id: project id on GCP\n      topic_path: topic path in pubsub\n\n    Return\n    ------\n    bool\n        topic_path exists or not\n    """"""\n    publisher = pubsub.PublisherClient()\n    project_path = publisher.project_path(project_id)\n    for existing_topic_path in publisher.list_topics(project_path):\n        if existing_topic_path.name == topic_path:\n            return True\n    return False\n\ndef create_topic_if_not_exists(project_id, topic_name):\n    """"""\n    Create a new pubsub topic if the topic name does not exist in the project.\n    Args:\n      project_id: project id on GCP\n      topic_name: topic name used to create topic path\n    """"""\n    publisher = pubsub.PublisherClient()\n    topic_path = publisher.topic_path(project_id, topic_name)\n    if check_topic_path_exists(project_id, topic_path):\n        return\n    publisher.create_topic(topic_path)\n\ndef publish_message(project_id, topic_name, installation_id,\n                    repo_owner, repo_name, issue_num):\n    """"""\n    Publish a message to a pubsub topic.\n    Args:\n      project_id: project id on GCP\n      topic_name: topic name used to create topic path\n      installation_id: repo installation id\n      repo_owner: repo owner\n      repo_name: repo name\n      issue_num: issue index\n    """"""\n    publisher = pubsub.PublisherClient()\n    topic_path = publisher.topic_path(project_id, topic_name)\n    # all attributes being published to pubsub must be sent as text strings\n    publisher.publish(topic_path,\n                      b\'New issue.\',\n                      installation_id=str(installation_id),\n                      repo_owner=repo_owner,\n                      repo_name=repo_name,\n                      issue_num=str(issue_num))\n'"
flask_app/mlapp.py,0,"b'from collections import namedtuple, Counter\nfrom github3 import GitHub\nfrom pathlib import Path\nfrom cryptography.hazmat.backends import default_backend\nimport time\nimport json\nimport jwt\nimport requests\nfrom tqdm import tqdm\nfrom typing import List\n\nclass GitHubApp(GitHub):\n    """"""\n    This is a small wrapper around the github3.py library\n    \n    Provides some convenience functions for testing purposes.\n    """"""\n    \n    def __init__(self, pem_path, app_id):\n        super().__init__()\n        \n        self.path = Path(pem_path)\n        self.app_id = app_id\n        \n        if not self.path.is_file():\n            raise ValueError(f\'argument: `pem_path` must be a valid filename. {pem_path} was not found.\')        \n    \n    def get_app(self):\n        with open(self.path, \'rb\') as key_file:\n            client = GitHub()\n            client.login_as_app(private_key_pem=key_file.read(),\n                        app_id=self.app_id)\n        return client\n    \n    def get_installation(self, installation_id):\n        ""login as app installation without requesting previously gathered data.""\n        with open(self.path, \'rb\') as key_file:\n            client = GitHub()\n            client.login_as_app_installation(private_key_pem=key_file.read(),\n                                             app_id=self.app_id,\n                                             installation_id=installation_id)\n        return client\n        \n    def get_test_installation_id(self):\n        ""Get a sample test_installation id.""\n        client = self.get_app()\n        return next(client.app_installations()).id\n        \n    def get_test_installation(self):\n        ""login as app installation with the first installation_id retrieved.""\n        return self.get_installation(self.get_test_installation_id())\n    \n    def get_test_repo(self):\n        repo = self.get_all_repos(self.get_test_installation_id())[0]\n        appInstallation = self.get_test_installation()\n        owner, name = repo[\'full_name\'].split(\'/\')\n        return appInstallation.repository(owner, name)\n        \n    def get_test_issue(self):\n        test_repo = self.get_test_repo()\n        return next(test_repo.issues())\n        \n    def get_jwt(self):\n        """"""\n        This is needed to retrieve the installation access token (for debugging). \n        \n        Useful for debugging purposes.  Must call .decode() on returned object to get string.\n        """"""\n        now = self._now_int()\n        payload = {\n            ""iat"": now,\n            ""exp"": now + (60),\n            ""iss"": self.app_id\n        }\n        with open(self.path, \'rb\') as key_file:\n            private_key = default_backend().load_pem_private_key(key_file.read(), None)\n            return jwt.encode(payload, private_key, algorithm=\'RS256\')\n    \n    def get_installation_id(self, owner, repo):\n        ""https://developer.github.com/v3/apps/#find-repository-installation""\n        url = f\'https://api.github.com/repos/{owner}/{repo}/installation\'\n\n        headers = {\'Authorization\': f\'Bearer {self.get_jwt().decode()}\',\n                   \'Accept\': \'application/vnd.github.machine-man-preview+json\'}\n        \n        response = requests.get(url=url, headers=headers)\n        if response.status_code != 200:\n            raise Exception(f\'Status code : {response.status_code}, {response.json()}\')\n        return response.json()[\'id\']\n\n    def get_installation_access_token(self, installation_id):\n        ""Get the installation access token for debugging.""\n        \n        url = f\'https://api.github.com/app/installations/{installation_id}/access_tokens\'\n        headers = {\'Authorization\': f\'Bearer {self.get_jwt().decode()}\',\n                   \'Accept\': \'application/vnd.github.machine-man-preview+json\'}\n        \n        response = requests.post(url=url, headers=headers)\n        if response.status_code != 201:\n            raise Exception(f\'Status code : {response.status_code}, {response.json()}\')\n        return response.json()[\'token\']\n\n    def _extract(self, d, keys):\n        ""extract selected keys from a dict.""\n        return dict((k, d[k]) for k in keys if k in d)\n    \n    def _now_int(self):\n        return int(time.time())\n\n    def get_all_repos(self, installation_id):\n        """"""Get all repos that this installation has access to.\n        \n        Useful for testing and debugging.\n        """"""\n        url = \'https://api.github.com/installation/repositories\'\n        headers={\'Authorization\': f\'token {self.get_installation_access_token(installation_id)}\',\n                 \'Accept\': \'application/vnd.github.machine-man-preview+json\'}\n        \n        response = requests.get(url=url, headers=headers)\n        \n        if response.status_code >= 400:\n            raise Exception(f\'Status code : {response.status_code}, {response.json()}\')\n        \n        fields = [\'name\', \'full_name\', \'id\']\n        return [self._extract(x, fields) for x in response.json()[\'repositories\']]\n    \n    def get_reactions(self, owner: str, repo: str, comment_id: int, iat: str):\n        """"""Get a list of reactions.\n\n        https://developer.github.com/v3/reactions/#list-reactions-for-a-commit-comment\n        """"""\n        url = f\'https://api.github.com/repos/{owner}/{repo}/issues/comments/{comment_id}/reactions\'\n        # installation_id = self.get_installation_id(owner, repo)\n        # headers={\'Authorization\': f\'token {self.get_installation_access_token(installation_id)}\',\n        #          \'Accept\': \'application/vnd.github.squirrel-girl-preview+json\'}\n        headers={\'Authorization\': f\'token {iat}\',\n                 \'Accept\': \'application/vnd.github.squirrel-girl-preview+json\'}\n        \n        response = requests.get(url=url, headers=headers)\n        \n        if response.status_code >= 400:\n            raise Exception(f\'Status code : {response.status_code}, {response.json()}\')\n        \n        results = [self._extract(x, [\'content\']) for x in response.json()]\n        # count the reactions\n        return Counter([x[\'content\'] for x in results])\n\n\n    @staticmethod\n    def unpack_issues(client, owner, repo, label_only=True):\n        """"""\n        extract relevant data from issues.\n\n        returns a list of namedtuples which contains the following fields:\n            title: str\n            number: int\n            body: str\n            labels: list\n            url: str\n\n        """"""\n        Issue = namedtuple(\'Issue\', [\'title\', \'number\', \'body\', \'labels\', \'url\'])\n\n        issue_data = []\n        issues = list(client.issues_on(owner, repo))\n        for issue in tqdm(issues, total=len(issues)):\n            labels=[label.name for label in issue.labels()]\n            \n            # if there are no labels, then optionally skip\n            if label_only and not labels:\n                continue\n    \n            issue_data.append(Issue(title=issue.title,\n                                    number=issue.number,\n                                    body=issue.body,\n                                    labels=[label.name for label in issue.labels()],\n                                    url=issue.html_url)\n                             )\n        return issue_data\n\n    def generate_installation_curl(self, endpoint):\n        iat = self.get_installation_access_token()\n        print(f\'curl -i -H ""Authorization: token {iat}"" -H ""Accept: application/vnd.github.machine-man-preview+json"" https://api.github.com{endpoint}\')'"
flask_app/sql_models.py,0,"b'import os\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\n\ndb = SQLAlchemy()\n\nclass Issues(db.Model):\n    __tablename__ = ""issues""\n    issue_id = db.Column(db.Integer, primary_key=True)\n    repo = db.Column(db.String, nullable=False)\n    username = db.Column(db.String, nullable=False)\n    issue_num = db.Column(db.Integer, nullable=False)\n    title = db.Column(db.String, nullable=False)\n    body = db.Column(db.String, nullable=True)\n    # the below statement allows you to call `Predictions.issue` to refer back to the issue\n    predictions = db.relationship(\'Predictions\', backref=\'issue\', lazy=True)\n\n    def add_prediction(self, comment_id, prediction, probability, logs, threshold, labeled, prediction_type=\'issue label\'):\n        p = Predictions(issue_id = self.issue_id,\n                        comment_id=comment_id,\n                        prediction=prediction,\n                        probability=probability,\n                        likes=None,\n                        dislikes=None,\n                        prediction_type=prediction_type,\n                        logs=logs,\n                        threshold=threshold,\n                        labeled=labeled)\n        db.session.add(p)\n        db.session.commit()\n\n\nclass Predictions(db.Model):\n    __tablename__ = ""predictions""\n    prediction_id = db.Column(db.Integer, primary_key=True)\n    issue_id = db.Column(db.Integer, db.ForeignKey(""issues.issue_id""), nullable=False)\n    comment_id = db.Column(db.BigInteger, nullable=False)\n    prediction = db.Column(db.String, nullable=False)\n    probability = db.Column(db.Float, nullable=False)\n    likes = db.Column(db.Integer, nullable=True)\n    dislikes = db.Column(db.Integer, nullable=True)\n    prediction_type = db.Column(db.String, nullable=False)\n    logs = db.Column(db.String, nullable=True)\n    threshold = db.Column(db.Float, nullable=False)\n    labeled = db.Column(db.Boolean, nullable=False)\n\n    def update_feedback(self, likes, dislikes):\n        p = Predictions.get(self.prediction_id)\n        p.likes = likes\n        p.dislikes = dislikes'"
flask_app/utils.py,0,"b'import numpy as np\n\n# Because of error when using a virutal env\n# https://markhneedham.com/blog/2018/05/04/python-runtime-error-osx-matplotlib-not-installed-as-framework-mac/\nimport matplotlib\nmatplotlib.use(\'TkAgg\')\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve\nfrom sklearn.utils.multiclass import unique_labels\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    """"""\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    """"""\n    if not title:\n        if normalize:\n            title = \'Normalized confusion matrix\'\n        else:\n            title = \'Confusion matrix, without normalization\'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n        print(""Normalized confusion matrix"")\n    else:\n        print(\'Confusion matrix, without normalization\')\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation=\'nearest\', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel=\'True label\',\n           xlabel=\'Predicted label\')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",\n             rotation_mode=""anchor"")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = \'.2f\' if normalize else \'d\'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=""center"", va=""center"",\n                    color=""white"" if cm[i, j] > thresh else ""black"")\n    fig.tight_layout()\n    return ax\n\n\nclass IssueLabeler:\n    def __init__(self, \n                 body_text_preprocessor, \n                 title_text_preprocessor, \n                 model, \n                 class_names=[\'bug\', \'feature_request\', \'question\']):\n        """"""\n        Parameters\n        ----------\n        body_text_preprocessor: ktext.preprocess.processor\n            the text preprocessor trained on issue bodies\n        title_text_preprocessor: ktext.preprocess.processor\n            text preprocessor trained on issue titles\n        model: tensorflow.keras.models\n            a keras model that takes as input two tensors: vectorized \n            issue body and issue title.\n        class_names: list\n            class names as they correspond to the integer indices supplied to the model. \n        """"""\n        self.body_pp = body_text_preprocessor\n        self.title_pp = title_text_preprocessor\n        self.model = model\n        self.class_names = class_names\n        \n    \n    def get_probabilities(self, body:str, title:str):\n        """"""\n        Get probabilities for the each class. \n        \n        Parameters\n        ----------\n        body: str\n           the issue body\n        title: str\n            the issue title\n            \n        Returns\n        ------\n        Dict[str:float]\n        \n        Example\n        -------\n        >>> issue_labeler = IssueLabeler(body_pp, title_pp, model)\n        >>> issue_labeler.get_probabilities(\'hello world\', \'hello world\')\n        {\'bug\': 0.08372017741203308,\n         \'feature\': 0.6401631832122803,\n         \'question\': 0.2761166989803314}\n        """"""\n        #transform raw text into array of ints\n        vec_body = self.body_pp.transform([body])\n        vec_title = self.title_pp.transform([title])\n        \n        # get predictions\n        probs = self.model.predict(x=[vec_body, vec_title]).tolist()[0]\n        \n        return {k:v for k,v in zip(self.class_names, probs)}\n\n\ndef plot_precision_recall_vs_threshold(y, y_hat, class_names, precision_threshold):\n    ""plot precision recall curves focused on precision.""\n    # credit: https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb\n    assert len(class_names)-1 <= y_hat.shape[-1], \'number of class names must equal number of classes in the data\'\n    assert y.shape == y_hat.shape, \'shape of ground_truth and predictions must be the same.\'\n    \n    for class_name in class_names:\n        class_int = class_names.index(class_name)\n        precisions, recalls, thresholds = precision_recall_curve(y[:, class_int], y_hat[:, class_int])\n        \n        # get the first index of the precision that meets the threshold\n        precision_idx = np.argmax(precisions >= precision_threshold)\n        # find the exact probability at that threshold\n        prob_thresh = thresholds[precision_idx]\n        # find the exact recall at that threshold\n        recall_at_thresh = recalls[precision_idx]\n        \n        plt.figure(figsize=(8, 4))\n        plt.plot(thresholds, precisions[:-1], ""b--"", label=""Precision"", linewidth=2)\n        plt.plot(thresholds, recalls[:-1], ""g-"", label=""Recall"", linewidth=2)\n        plt.axhline(y=precision_threshold, label=f\'{precision_threshold:.2f}\', linewidth=1)\n        plt.xlabel(""Threshold"", fontsize=11)\n        plt.legend(loc=""lower left"", fontsize=10)\n        plt.title(f\'Precision vs. Recall For Label: {class_name}\')\n        plt.ylim([0, 1])\n        plt.xlim([0, 1])\n        plt.show()\n        print(f\'Label ""{class_name}"" @ {precision_threshold:.2f} precision:\')\n        print(f\'  Cutoff: {prob_thresh:.2f}\')\n        print(f\'  Recall: {recall_at_thresh:.2f}\')\n        print(\'\\n\')'"
notebooks/mlapp.py,0,"b'from collections import namedtuple, Counter\nfrom github3 import GitHub\nfrom pathlib import Path\nfrom cryptography.hazmat.backends import default_backend\nimport time\nimport json\nimport jwt\nimport requests\nfrom tqdm import tqdm\nfrom typing import List\n\nclass GitHubApp(GitHub):\n    """"""\n    This is a small wrapper around the github3.py library\n    \n    Provides some convenience functions for testing purposes.\n    """"""\n    \n    def __init__(self, pem_path, app_id):\n        super().__init__()\n        \n        self.path = Path(pem_path)\n        self.app_id = app_id\n        \n        if not self.path.is_file():\n            raise ValueError(f\'argument: `pem_path` must be a valid filename. {pem_path} was not found.\')        \n    \n    def get_app(self):\n        with open(self.path, \'rb\') as key_file:\n            client = GitHub()\n            client.login_as_app(private_key_pem=key_file.read(),\n                        app_id=self.app_id)\n        return client\n    \n    def get_installation(self, installation_id):\n        ""login as app installation without requesting previously gathered data.""\n        with open(self.path, \'rb\') as key_file:\n            client = GitHub()\n            client.login_as_app_installation(private_key_pem=key_file.read(),\n                                             app_id=self.app_id,\n                                             installation_id=installation_id)\n        return client\n        \n    def get_test_installation_id(self):\n        ""Get a sample test_installation id.""\n        client = self.get_app()\n        return next(client.app_installations()).id\n        \n    def get_test_installation(self):\n        ""login as app installation with the first installation_id retrieved.""\n        return self.get_installation(self.get_test_installation_id())\n    \n    def get_test_repo(self):\n        repo = self.get_all_repos(self.get_test_installation_id())[0]\n        appInstallation = self.get_test_installation()\n        owner, name = repo[\'full_name\'].split(\'/\')\n        return appInstallation.repository(owner, name)\n        \n    def get_test_issue(self):\n        test_repo = self.get_test_repo()\n        return next(test_repo.issues())\n        \n    def get_jwt(self):\n        """"""\n        This is needed to retrieve the installation access token (for debugging). \n        \n        Useful for debugging purposes.  Must call .decode() on returned object to get string.\n        """"""\n        now = self._now_int()\n        payload = {\n            ""iat"": now,\n            ""exp"": now + (60),\n            ""iss"": self.app_id\n        }\n        with open(self.path, \'rb\') as key_file:\n            private_key = default_backend().load_pem_private_key(key_file.read(), None)\n            return jwt.encode(payload, private_key, algorithm=\'RS256\')\n    \n    def get_installation_id(self, owner, repo):\n        ""https://developer.github.com/v3/apps/#find-repository-installation""\n        url = f\'https://api.github.com/repos/{owner}/{repo}/installation\'\n\n        headers = {\'Authorization\': f\'Bearer {self.get_jwt().decode()}\',\n                   \'Accept\': \'application/vnd.github.machine-man-preview+json\'}\n        \n        response = requests.get(url=url, headers=headers)\n        if response.status_code != 200:\n            raise Exception(f\'Status code : {response.status_code}, {response.json()}\')\n        return response.json()[\'id\']\n\n    def get_installation_access_token(self, installation_id):\n        ""Get the installation access token for debugging.""\n        \n        url = f\'https://api.github.com/app/installations/{installation_id}/access_tokens\'\n        headers = {\'Authorization\': f\'Bearer {self.get_jwt().decode()}\',\n                   \'Accept\': \'application/vnd.github.machine-man-preview+json\'}\n        \n        response = requests.post(url=url, headers=headers)\n        if response.status_code != 201:\n            raise Exception(f\'Status code : {response.status_code}, {response.json()}\')\n        return response.json()[\'token\']\n\n    def _extract(self, d, keys):\n        ""extract selected keys from a dict.""\n        return dict((k, d[k]) for k in keys if k in d)\n    \n    def _now_int(self):\n        return int(time.time())\n\n    def get_all_repos(self, installation_id):\n        """"""Get all repos that this installation has access to.\n        \n        Useful for testing and debugging.\n        """"""\n        url = \'https://api.github.com/installation/repositories\'\n        headers={\'Authorization\': f\'token {self.get_installation_access_token(installation_id)}\',\n                 \'Accept\': \'application/vnd.github.machine-man-preview+json\'}\n        \n        response = requests.get(url=url, headers=headers)\n        \n        if response.status_code >= 400:\n            raise Exception(f\'Status code : {response.status_code}, {response.json()}\')\n        \n        fields = [\'name\', \'full_name\', \'id\']\n        return [self._extract(x, fields) for x in response.json()[\'repositories\']]\n    \n    def get_reactions(self, owner: str, repo: str, comment_id: int, iat: str):\n        """"""Get a list of reactions.\n\n        https://developer.github.com/v3/reactions/#list-reactions-for-a-commit-comment\n        """"""\n        url = f\'https://api.github.com/repos/{owner}/{repo}/issues/comments/{comment_id}/reactions\'\n        # installation_id = self.get_installation_id(owner, repo)\n        # headers={\'Authorization\': f\'token {self.get_installation_access_token(installation_id)}\',\n        #          \'Accept\': \'application/vnd.github.squirrel-girl-preview+json\'}\n        headers={\'Authorization\': f\'token {iat}\',\n                 \'Accept\': \'application/vnd.github.squirrel-girl-preview+json\'}\n        \n        response = requests.get(url=url, headers=headers)\n        \n        if response.status_code >= 400:\n            raise Exception(f\'Status code : {response.status_code}, {response.json()}\')\n        \n        results = [self._extract(x, [\'content\']) for x in response.json()]\n        # count the reactions\n        return Counter([x[\'content\'] for x in results])\n\n\n    @staticmethod\n    def unpack_issues(client, owner, repo, label_only=True):\n        """"""\n        extract relevant data from issues.\n\n        returns a list of namedtuples which contains the following fields:\n            title: str\n            number: int\n            body: str\n            labels: list\n            url: str\n\n        """"""\n        Issue = namedtuple(\'Issue\', [\'title\', \'number\', \'body\', \'labels\', \'url\'])\n\n        issue_data = []\n        issues = list(client.issues_on(owner, repo))\n        for issue in tqdm(issues, total=len(issues)):\n            labels=[label.name for label in issue.labels()]\n            \n            # if there are no labels, then optionally skip\n            if label_only and not labels:\n                continue\n    \n            issue_data.append(Issue(title=issue.title,\n                                    number=issue.number,\n                                    body=issue.body,\n                                    labels=[label.name for label in issue.labels()],\n                                    url=issue.html_url)\n                             )\n        return issue_data\n\n    def generate_installation_curl(self, endpoint):\n        iat = self.get_installation_access_token()\n        print(f\'curl -i -H ""Authorization: token {iat}"" -H ""Accept: application/vnd.github.machine-man-preview+json"" https://api.github.com{endpoint}\')'"
notebooks/utils.py,0,"b'import numpy as np\n\n# Because of error when using a virutal env\n# https://markhneedham.com/blog/2018/05/04/python-runtime-error-osx-matplotlib-not-installed-as-framework-mac/\nimport matplotlib\nmatplotlib.use(\'TkAgg\')\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve\nfrom sklearn.utils.multiclass import unique_labels\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    """"""\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    """"""\n    if not title:\n        if normalize:\n            title = \'Normalized confusion matrix\'\n        else:\n            title = \'Confusion matrix, without normalization\'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n        print(""Normalized confusion matrix"")\n    else:\n        print(\'Confusion matrix, without normalization\')\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation=\'nearest\', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel=\'True label\',\n           xlabel=\'Predicted label\')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",\n             rotation_mode=""anchor"")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = \'.2f\' if normalize else \'d\'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=""center"", va=""center"",\n                    color=""white"" if cm[i, j] > thresh else ""black"")\n    fig.tight_layout()\n    return ax\n\n\nclass IssueLabeler:\n    def __init__(self, \n                 body_text_preprocessor, \n                 title_text_preprocessor, \n                 model, \n                 class_names=[\'bug\', \'feature_request\', \'question\']):\n        """"""\n        Parameters\n        ----------\n        body_text_preprocessor: ktext.preprocess.processor\n            the text preprocessor trained on issue bodies\n        title_text_preprocessor: ktext.preprocess.processor\n            text preprocessor trained on issue titles\n        model: tensorflow.keras.models\n            a keras model that takes as input two tensors: vectorized \n            issue body and issue title.\n        class_names: list\n            class names as they correspond to the integer indices supplied to the model. \n        """"""\n        self.body_pp = body_text_preprocessor\n        self.title_pp = title_text_preprocessor\n        self.model = model\n        self.class_names = class_names\n        \n    \n    def get_probabilities(self, body:str, title:str):\n        """"""\n        Get probabilities for the each class. \n        \n        Parameters\n        ----------\n        body: str\n           the issue body\n        title: str\n            the issue title\n            \n        Returns\n        ------\n        Dict[str:float]\n        \n        Example\n        -------\n        >>> issue_labeler = IssueLabeler(body_pp, title_pp, model)\n        >>> issue_labeler.get_probabilities(\'hello world\', \'hello world\')\n        {\'bug\': 0.08372017741203308,\n         \'feature\': 0.6401631832122803,\n         \'question\': 0.2761166989803314}\n        """"""\n        #transform raw text into array of ints\n        vec_body = self.body_pp.transform([body])\n        vec_title = self.title_pp.transform([title])\n        \n        # get predictions\n        probs = self.model.predict(x=[vec_body, vec_title]).tolist()[0]\n        \n        return {k:v for k,v in zip(self.class_names, probs)}\n\n\ndef plot_precision_recall_vs_threshold(y, y_hat, class_names, precision_threshold):\n    ""plot precision recall curves focused on precision.""\n    # credit: https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb\n    assert len(class_names)-1 <= y_hat.shape[-1], \'number of class names must equal number of classes in the data\'\n    assert y.shape == y_hat.shape, \'shape of ground_truth and predictions must be the same.\'\n    \n    for class_name in class_names:\n        class_int = class_names.index(class_name)\n        precisions, recalls, thresholds = precision_recall_curve(y[:, class_int], y_hat[:, class_int])\n        \n        # get the first index of the precision that meets the threshold\n        precision_idx = np.argmax(precisions >= precision_threshold)\n        # find the exact probability at that threshold\n        prob_thresh = thresholds[precision_idx]\n        # find the exact recall at that threshold\n        recall_at_thresh = recalls[precision_idx]\n        \n        plt.figure(figsize=(8, 4))\n        plt.plot(thresholds, precisions[:-1], ""b--"", label=""Precision"", linewidth=2)\n        plt.plot(thresholds, recalls[:-1], ""g-"", label=""Recall"", linewidth=2)\n        plt.axhline(y=precision_threshold, label=f\'{precision_threshold:.2f}\', linewidth=1)\n        plt.xlabel(""Threshold"", fontsize=11)\n        plt.legend(loc=""lower left"", fontsize=10)\n        plt.title(f\'Precision vs. Recall For Label: {class_name}\')\n        plt.ylim([0, 1])\n        plt.xlim([0, 1])\n        plt.show()\n        print(f\'Label ""{class_name}"" @ {precision_threshold:.2f} precision:\')\n        print(f\'  Cutoff: {prob_thresh:.2f}\')\n        print(f\'  Recall: {recall_at_thresh:.2f}\')\n        print(\'\\n\')'"
script/create_secrets.py,0,"b'#!/usr/bin/python\n""""""A script to create the required secrets in one namespace by copying them\nfrom another namespace\n""""""\n\nimport base64\nimport fire\nfrom google.cloud import storage\nfrom kubernetes import client as k8s_client\nfrom kubernetes import config as k8s_config\nfrom kubernetes.client import rest\nimport logging\nimport yaml\nimport os\nimport re\nimport subprocess\n\nDEV_ENVIRONMENT = ""dev""\nPROD_ENVIRONMENT = ""prod""\n\n# The namespace for the dev environment.\nDEV_NAMESPACE = ""label-bot-dev""\nPROD_NAMESPACE = ""label-bot-prod""\n\nGCS_REGEX = re.compile(""gs://([^/]*)(/.*)?"")\n\ndef split_gcs_uri(gcs_uri):\n  """"""Split a GCS URI into bucket and path.""""""\n  m = GCS_REGEX.match(gcs_uri)\n  bucket = m.group(1)\n  path = """"\n  if m.group(2):\n    path = m.group(2).lstrip(""/"")\n  return bucket, path\n\ndef secret_exists(namespace, name, client):\n  api = k8s_client.CoreV1Api(client)\n\n  try:\n    api.read_namespaced_secret(name, namespace)\n    return True\n  except rest.ApiException as e:\n    if e.status != 404:\n      raise\n\n  return False\n\ndef _read_gcs_path(gcs_path):\n  bucket_name, blob_name = split_gcs_uri(gcs_path)\n\n  storage_client = storage.Client()\n\n  bucket = storage_client.bucket(bucket_name)\n  blob = bucket.blob(blob_name)\n  contents = blob.download_as_string().decode()\n\n  return contents\n\nclass SecretCreator:\n\n  @staticmethod\n  def _secret_from_gcs(secret_name, gcs_path):\n    bucket_name, blob_name = split_gcs_uri(gcs_path)\n\n    storage_client = storage.Client()\n\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n    contents = blob.download_as_string().decode()\n\n    file_name = os.path.basename(blob_name)\n    namespace, name = secret_name.split(""/"", 1)\n    subprocess.check_call([""kubectl"", ""-n"", namespace, ""create"",\n                           ""secret"", ""generic"",\n                           name,\n                           f""--from-literal=f{file_name}={contents}""])\n  @staticmethod\n  def copy_secret(source, dest):\n    """"""Create the dev version of the secrets.\n\n    Args:\n      source: {namespace}/{secret name}\n      dest: {namespace}/{secret name}\n    """"""\n    src_namespace, src_name = source.split(""/"", 1)\n    dest_namespace, dest_name = dest.split(""/"", 1)\n\n    data = subprocess.check_output([""kubectl"", ""-n"", src_namespace, ""get"",\n                                    ""secrets"", src_name, ""-o"",\n                                    ""yaml""])\n\n    encoded = yaml.load(data)\n    decoded = {}\n\n    for k, v in encoded[""data""].items():\n      decoded[k] = base64.b64decode(v).decode()\n\n    command = [""kubectl"", ""create"", ""-n"", dest_namespace, ""secret"",\n               ""generic"", dest_name]\n\n    for k, v in decoded.items():\n      command.append(f""--from-literal={k}={v}"")\n\n    subprocess.check_call(command)\n\n  @staticmethod\n  def create(env):\n    """"""Create the secrets for the dev environment.\n\n    Args:\n      env: The environment to create the secrets in.\n    """"""\n\n    if env == DEV_ENVIRONMENT:\n      namespace = DEV_NAMESPACE\n      github_app_pem_key = (""gs://issue-label-bot-dev_secrets/""\n                           ""kf-label-bot-dev.2019-12-30.private-key.pem"")\n      webhook_gcs = (""gs://issue-label-bot-dev_secrets/""\n                     ""kf-label-bot-dev.webhook.secret"")\n    elif env == PROD_ENVIRONMENT:\n      namespace = PROD_NAMESPACE\n      github_app_pem_key = (""gs://github-probots_secrets/""\n                            ""issue-label-bot-github-app.private-key.pem"")\n      webhook_gcs = (""gs://github-probots_secrets/""\n                     ""issue-label-bot-prod.webhook.secret"")\n    else:\n      raise ValueError(f""env={env} is not an allowed value; must be ""\n                       f""{DEV_ENVIRONMENT} or {PROD_ENVIRONMENT}"")\n\n    k8s_config.load_kube_config(persist_config=False)\n\n    client = k8s_client.ApiClient()\n\n    if secret_exists(namespace, ""user-gcp-sa"", client):\n      logging.warning(f""Secret {namespace}/user-gcp-sa already exists; ""\n                      f""Not recreating it."")\n    else:\n      # We get a GCP secret by copying it from the kubeflow namespace.\n      SecretCreator.copy_secret(""kubeflow/user-gcp-sa"",\n                                f""{namespace}/user-gcp-sa"")\n\n    if secret_exists(namespace, ""github-app"", client):\n      logging.warning(f""Secret {namespace}/github-app already exists; ""\n                      f""Not recreating it."")\n    else:\n      # Create the secret containing the PEM key for the github app\n      SecretCreator._secret_from_gcs(f""{namespace}/github-app"", github_app_pem_key)\n\n    # Create the inference secret containing the postgres database with\n    # postgres secret and the webhook secret\n    inference_secret = ""ml-app-inference-secret""\n    if secret_exists(namespace, inference_secret, client):\n      logging.warning(f""Secret {namespace}/{inference_secret} already exists; ""\n                      f""Not recreating it."")\n    else:\n      postgres = _read_gcs_path(""gs://issue-label-bot-dev_secrets/""\n                                ""issue-label-bot.postgres"")\n      webhook = _read_gcs_path(webhook_gcs)\n\n      subprocess.check_call([""kubectl"", ""-n"", namespace, ""create"",\n                             ""secret"", ""generic"",\n                             inference_secret,\n                             f""--from-literal=DATABASE_URL={postgres}"",\n                             f""--from-literal=WEBHOOK_SECRET={webhook}""])\n\n\nif __name__ == \'__main__\':\n  logging.basicConfig(level=logging.INFO,\n                      format=(\'%(levelname)s|%(asctime)s\'\n                              \'|%(message)s|%(pathname)s|%(lineno)d|\'),\n                      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                      )\n\n  fire.Fire(SecretCreator)\n'"
script/send_request.py,0,"b'""""""A helper script to send requests to test the label bot.""""""\nimport base64\nimport fire\nimport hmac\nimport json\nimport logging\nimport requests\nimport subprocess\n\nclass SendRequest:\n  @staticmethod\n  def send(url=""https://label-bot-dev.mlbot.net/event_handler""):\n    # Get the webhook secret\n    secret = subprocess.check_output([""kubectl"", ""get"", ""secret"",\n                                      ""ml-app-inference-secret"",\n                                      ""-o"", ""jsonpath=\'{.data.WEBHOOK_SECRET}\'""])\n\n    secret_decoded = base64.b64decode(secret).decode()\n\n    if url != ""https://label-bot-dev.mlbot.net/event_handler"":\n      logging.error(""You aren\'t using the dev label bot webhook but ""\n                    ""send_request.py currently hard codes the GitHub App ""\n                    ""Install ID to kf-label-bot-dev on kubeflow/code-intelligence ""\n                    ""we need to change that in order to be able to allow ""\n                    ""worker to actually write to the repo."")\n    # TODO(jlewi): We should allow specificing a specific issue.\n    payload = {\n      ""action"": ""opened"",\n      # Installation corresponding to kf-label-bot-dev on\n      # kubeflow/code-intelligence\n      ""installation"": {\n        # TODO(jlewi): This is the installation id of the kf-label-bot-dev\n        ""id"": 5980888,\n      },\n      ""issue"": {\n        ""number"": 104,\n        ""title"": ""Test kf-label bot-dev this is a bug"",\n        ""body"": (""Test whether events are correctly routed to the dev instance.""\n                 ""If not then there is a bug in the setup"")\n      },\n      ""repository"": {\n        ""full_name"": ""kubeflow/code-intelligence"",\n        ""private"": False,\n      }\n    }\n\n    data = str.encode(json.dumps(payload))\n    # See: https://developer.github.com/webhooks/securing/\n    # We need to compute the signature of the payload using the secret\n    mac = hmac.new(str.encode(secret_decoded), msg=data, digestmod=\'sha1\')\n\n    headers = {\n     ""X-Hub-Signature"": ""="" + mac.hexdigest(),\n     ""Content-Type"": ""application/json"",\n    }\n\n    # We use data and not json because we need to compute the hash of the\n    # data to match the signature\n    logging.info(f""Send url: {url}"")\n    response = requests.post(url, data=data, headers=headers)\n\n    logging.info(f""Response {response}"")\n\nif __name__ == ""__main__"":\n  logging.basicConfig(level=logging.INFO,\n                      format=(\'%(levelname)s|%(asctime)s\'\n                              \'|%(message)s|%(pathname)s|%(lineno)d|\'),\n                      datefmt=\'%Y-%m-%dT%H:%M:%S\',\n                      )\n\n  fire.Fire(SendRequest)\n\n'"
argo/src/preprocess.py,0,"b'import pandas as pd\nimport dask.dataframe as df\nfrom dask_ml.preprocessing import OneHotEncoder\nimport numpy as np\nfrom keras.utils.np_utils import to_categorical\nfrom dask.distributed import Client\nimport time\nimport json\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom typing import Callable, List\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom dask import array as da\nfrom textacy.preprocess import preprocess_text\nimport dask.multiprocessing\nfrom pathos.multiprocessing import cpu_count\nfrom collections import Counter\nfrom collections import defaultdict\nimport h5py\n\nclient = Client(os.getenv(""DASK_SCHEDULER_ADDRESS""))\n\nstart_time = time.time()\n\noutput_dir = ""/data/""\n\nbase_url = \'https://storage.googleapis.com/codenet/issue_labels/\'\ndd = df.from_pandas(pd.concat([pd.read_csv(base_url+f\'00000000000{i}.csv.gz\') for i in range(10)]), npartitions=1024)\n\ndef textacy_cleaner(text: str) -> str:\n    """"""a\n    Defines the default function for cleaning text.\n\n    This function operates over a list.\n    """"""\n    return preprocess_text(text,\n                           fix_unicode=True,\n                           lowercase=True,\n                           transliterate=True,\n                           no_urls=True,\n                           no_emails=True,\n                           no_phone_numbers=True,\n                           no_numbers=True,\n                           no_currency_symbols=True,\n                           no_punct=True,\n                           no_contractions=False,\n                           no_accents=True)\n\n\ndef process_document(doc: str) -> List[str]:\n    if doc and len(doc) > 20000:\n        return [""_start_"", """", ""_end_""]\n    doc = text_to_word_sequence(textacy_cleaner(doc))\n    if len(doc) > 1000:\n        return [""_start_"", """", ""_end_""]\n    return [""_start_""] + doc + [""_end_""]\n\n\ntest_data = \'hello world 314-903-3072, hamel.husain@gmail.com wee woo\'\nassert process_document(test_data) == [\'_start_\', \'hello\', \'world\', \'phone\', \'email\', \'wee\', \'woo\', \'_end_\']\n\n\nbodies_parsed = dd[""body""].apply(process_document)\ntitles_parsed = dd[""title""].apply(process_document)\n\nnow = time.time() - start_time\nprint(f""tokenized {now}"")\n\ndef to_one_hot(df):\n    return to_categorical(df.values, num_classes=3)\n\ntargets = dd[""class_int""].to_frame().map_partitions(to_one_hot)\n\nbody_quant = int(bodies_parsed.apply(len).quantile(q=0.85).compute())\ntitle_quant = int(titles_parsed.apply(len).quantile(q=0.85).compute())\n\nprint(f""Quantiles title-{title_quant} body-{body_quant} "")\n\ndef drop_long_docs(doc, max_len):\n    if len(doc) > max_len:\n        return doc[:max_len]\n    return doc\n\nbodies_parsed = bodies_parsed.apply(drop_long_docs, max_len=body_quant)\ntitles_parsed = titles_parsed.apply(drop_long_docs, max_len=title_quant)\n\ndef count_words(partition):\n    c = Counter()\n    def count(p):\n        c.update(p)\n        return c\n    ct = Counter()\n    ct.update(dict(partition.apply(count).iloc[0].most_common(n=8000)))\n    return ct\n\n\nnow = time.time() - start_time\nprint(f""quantiles done {now}"")\n\n\nbody_counts = bodies_parsed.map_partitions(count_words).compute()\nnow = time.time() - start_time\nprint(f""body counts computed {now}"")\nbody_counts = sum(body_counts.tolist(), Counter())\nnow = time.time() - start_time\nprint(f""body-counts done {now}"")\ntitle_counts = titles_parsed.map_partitions(count_words).compute()\ntitle_counts = sum(title_counts.tolist(), Counter())\n\nnow = time.time() - start_time\nprint(f""counting words body {now}"")\n\nwords_to_keep_body = body_counts.most_common(n=8000)\nbody_vocab = defaultdict(lambda: 1)\nbody_vocab.update({x:i+2 for i, x in enumerate([x[0] for x in words_to_keep_body])})\n\nnow = time.time() - start_time\nprint(f""counting words title {now}"")\nwords_to_keep_title = title_counts.most_common(n=4500)\ntitles_vocab = defaultdict(lambda: 1)\ntitles_vocab.update({x:i+2 for i, x in enumerate([x[0] for x in words_to_keep_title])})\n\nnow = time.time() - start_time\nprint(f""words counted {now}"")\n\nnumer_bodies = bodies_parsed.apply(lambda x: [body_vocab[w] for w in x])\nnumer_titles = titles_parsed.apply(lambda x: [titles_vocab[w] for w in x])\n\ndef pad_partition(numerized_doc, max_len):\n    if type(numerized_doc) != list:\n        return\n    return pad_sequences([numerized_doc], maxlen=max_len, truncating=\'post\')[0]\n\nprocessed_bodies = numer_bodies.apply(pad_partition, max_len=body_quant)\nprocessed_titles = numer_titles.apply(pad_partition, max_len=title_quant)\n\nnow = time.time() - start_time\nprint(f""saving {now}"")\n\nprocessed_titles = np.stack(processed_titles.values.compute())\nprocessed_bodies = np.stack(processed_bodies.values.compute())\n\nnow = time.time() - start_time\nprint(f""creating hdf5 {now}"")\n\n\nf = h5py.File(\'/data/dataset.hdf5\', \'w\')\nf.create_dataset(\'/titles\', data=processed_titles)\nf.create_dataset(\'/bodies\', data=processed_bodies)\nf.create_dataset(\'/targets\', data=targets)\nf.close()\n\nwith open(""/data/metadata.json"", ""w"") as f:\n    meta = {\n        \'body_vocab_size\': len(body_vocab),\n        \'title_vocab_size\': len(titles_vocab),\n        \'issue_body_doc_length\': body_quant,\n        \'issue_title_doc_length\': title_quant,\n    }\n    f.write(json.dumps(meta))\n\n\nnow = time.time() - start_time\nprint(f""saved {now}"")'"
argo/src/train.py,0,"b'import numpy as np\nimport dill as dpickle\nimport h5py\nimport json\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import multi_gpu_model\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, GRU, Dense, Embedding, Conv1D, Bidirectional, BatchNormalization, Dot, Flatten, Concatenate\nfrom tensorflow.keras.optimizers import Nadam\nfrom tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n\n\n\n\ninput_dir = ""/data/""\nout_dir = ""/output/""\ndataset = h5py.File(\'/data/dataset.hdf5\', \'r\')\nwith open(""/data/metadata.json"", ""r"") as f:\n    meta = json.loads(f.read())\n\n\ntrain_bodies, test_bodies, train_titles, test_titles, train_targets, test_targets = train_test_split(\n    np.array(dataset[\'bodies\']),\n    np.array(dataset[\'titles\']),\n    np.array(dataset[\'targets\']),\n    test_size=0.25\n)\n\nbody_emb_size = 50\ntitle_emb_size = 50\n\nnum_classes = 3\n\nbody_input = Input(shape=(meta[\'issue_body_doc_length\'],), name=\'Body-Input\')\ntitle_input = Input(shape=(meta[\'issue_title_doc_length\'],), name=\'Title-Input\')\n\nb_i = Embedding(meta[\'body_vocab_size\']+1, body_emb_size, name=\'Body-Embedding\', mask_zero=False)(body_input)\nb_t = Embedding(meta[\'title_vocab_size\']+1, title_emb_size, name=\'Title-Embedding\', mask_zero=False)(title_input)\n\nb_i = BatchNormalization()(b_i)\nb_i = Bidirectional(GRU(100, name=\'Body-Encoder\'))(b_i)\n\nb_t = BatchNormalization()(b_t)\nb_t = GRU(75, name=\'Title-Encoder\')(b_t)\n\nb = Concatenate(name=\'Concat\')([b_i, b_t])\nb = BatchNormalization()(b)\nout = Dense(num_classes, activation=\'softmax\')(b)\n\nparallel_model = Model([body_input, title_input], out)\nparallel_model.compile(optimizer=Nadam(lr=0.001), loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\n\nscript_name_base = \'IssueLabeler\'\ncsv_logger = CSVLogger(out_dir + \'{:}.log\'.format(script_name_base))\nmodel_checkpoint = ModelCheckpoint(out_dir + \'{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5\'.format(script_name_base),\n                                   save_best_only=True)\n\nbatch_size = 900\nepochs = 4\nhistory = parallel_model.fit(x=[train_bodies, train_titles], \n                             y=train_targets,\n                             batch_size=batch_size,\n                             epochs=epochs,\n                             validation_split=0.10, \n                             callbacks=[csv_logger, model_checkpoint])\n'"
