file_path,api_count,code
data_utils.py,7,"b""'''\nThis script contains several functions used for data processing.\n'''\n\n#############################################################################\n# Import here useful libraries\n#############################################################################\nfrom nltk.tokenize import word_tokenize\nimport tensorflow as tf\nimport pandas as pd\nimport pickle\nimport random\nimport glob\nimport nltk\nimport re\n\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt')\n\n\ndef imdb2tfrecords(path_data='datasets/aclImdb/', min_word_frequency=5,\n                   max_words_review=700):\n    '''\n    This script processes the data and saves it in the default TensorFlow \n    file format: tfrecords.\n    \n    Args:\n        path_data: the path where the imdb data is stored.\n        min_word_frequency: the minimum frequency of a word, to keep it\n                            in the vocabulary.\n        max_words_review: the maximum number of words allowed in a review.\n    '''\n    # Get the filenames of the positive/negative reviews we will use\n    # for training the RNN\n    train_pos_files = glob.glob(path_data + 'train/pos/*')\n    train_neg_files = glob.glob(path_data + 'train/neg/*')\n\n    # Concatenate both positive and negative reviews filenames\n    train_files = train_pos_files + train_neg_files\n    \n    # List with all the reviews in the train dataset\n    reviews = [open(train_files[i],'r').read() for i in range(len(train_files))]\n    \n    # Remove HTML tags\n    reviews = [re.sub(r'<[^>]+>', ' ', review) for review in reviews]\n        \n    # Tokenize each review in part\n    reviews = [word_tokenize(review) for review in reviews]\n    \n    # Compute the length of each review\n    len_reviews = [len(review) for review in reviews]\n    pickle.dump(len_reviews, open(path_data + 'length_reviews.pkl', 'wb'))\n\n    # Flatten nested list\n    reviews = [word for review in reviews for word in review]\n    \n    # Compute the frequency of each word\n    word_frequency = pd.value_counts(reviews)\n    \n    # Keep only words with frequency higher than minimum\n    vocabulary = word_frequency[word_frequency>=min_word_frequency].index.tolist()\n    \n    # Add Unknown, Start and End token. \n    extra_tokens = ['Unknown_token', 'End_token']\n    vocabulary += extra_tokens\n    \n    # Create a word2idx dictionary\n    word2idx = {vocabulary[i]: i for i in range(len(vocabulary))}\n    \n    # Write word vocabulary to disk\n    pickle.dump(word2idx, open(path_data + 'word2idx.pkl', 'wb'))\n        \n    def text2tfrecords(filenames, writer, vocabulary, word2idx,\n                       max_words_review):\n        '''\n        Function to parse each review in part and write to disk\n        as a tfrecord.\n        \n        Args:\n            filenames: the paths of the review files.\n            writer: the writer object for tfrecords.\n            vocabulary: list with all the words included in the vocabulary.\n            word2idx: dictionary of words and their corresponding indexes.\n        '''\n        # Shuffle filenames\n        random.shuffle(filenames)\n        for filename in filenames:\n            review = open(filename, 'r').read()\n            review = re.sub(r'<[^>]+>', ' ', review)\n            review = word_tokenize(review)\n            # Reduce review to max words\n            review = review[-max_words_review:]\n            # Replace words with their equivalent index from word2idx\n            review = [word2idx[word] if word in vocabulary else \n                      word2idx['Unknown_token'] for word in review]\n            indexed_review = review + [word2idx['End_token']]\n            sequence_length = len(indexed_review)\n            target = 1 if filename.split('/')[-2]=='pos' else 0\n            # Create a Sequence Example to store our data in\n            ex = tf.train.SequenceExample()\n            # Add non-sequential features to our example\n            ex.context.feature['sequence_length'].int64_list.value.append(sequence_length)\n            ex.context.feature['target'].int64_list.value.append(target)\n            # Add sequential feature\n            token_indexes = ex.feature_lists.feature_list['token_indexes']\n            for token_index in indexed_review:\n                token_indexes.feature.add().int64_list.value.append(token_index)\n            writer.write(ex.SerializeToString())\n    \n    ##########################################################################     \n    # Write train data to tfrecords.This might take a while (~10 minutes)\n    ##########################################################################\n    train_writer = tf.python_io.TFRecordWriter(path_data + 'train.tfrecords')\n    text2tfrecords(train_files, train_writer, vocabulary, word2idx, \n                   max_words_review)\n\n    ##########################################################################\n    # Get the filenames of the reviews we will use for testing the RNN \n    ##########################################################################\n    test_pos_files = glob.glob(path_data + 'test/pos/*')\n    test_neg_files = glob.glob(path_data + 'test/neg/*')\n    test_files = test_pos_files + test_neg_files\n\n    ##########################################################################\n    # Write test data to tfrecords (~10 minutes)\n    ##########################################################################\n    test_writer = tf.python_io.TFRecordWriter('datasets/aclImdb/test.tfrecords')\n    text2tfrecords(test_files, test_writer, vocabulary, word2idx,\n                   max_words_review)\n\n\ndef parse_imdb_sequence(record):\n    '''\n    Script to parse imdb tfrecords.\n    \n    Returns:\n        token_indexes: sequence of token indexes present in the review.\n        target: the target of the movie review.\n        sequence_length: the length of the sequence.\n    '''\n    context_features = {\n        'sequence_length': tf.FixedLenFeature([], dtype=tf.int64),\n        'target': tf.FixedLenFeature([], dtype=tf.int64),\n        }\n    sequence_features = {\n        'token_indexes': tf.FixedLenSequenceFeature([], dtype=tf.int64),\n        }\n    context_parsed, sequence_parsed = tf.parse_single_sequence_example(record, \n        context_features=context_features, sequence_features=sequence_features)\n        \n    return (sequence_parsed['token_indexes'], context_parsed['target'],\n            context_parsed['sequence_length'])\n\n     \n        \n"""
