file_path,api_count,code
04-WebApp/main.py,0,"b'#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\n### General imports ###\nfrom __future__ import division\nimport numpy as np\nimport pandas as pd\nimport time\nimport re\nimport os\nfrom collections import Counter\nimport altair as alt\n\n### Flask imports\nimport requests\nfrom flask import Flask, render_template, session, request, redirect, flash, Response\n\n### Audio imports ###\nfrom library.speech_emotion_recognition import *\n\n### Video imports ###\nfrom library.video_emotion_recognition import *\n\n### Text imports ###\nfrom library.text_emotion_recognition import *\nfrom library.text_preprocessor import *\nfrom nltk import *\nfrom tika import parser\nfrom werkzeug.utils import secure_filename\nimport tempfile\n\n\n\n# Flask config\napp = Flask(__name__)\napp.secret_key = b\'(\\xee\\x00\\xd4\\xce""\\xcf\\xe8@\\r\\xde\\xfc\\xbdJ\\x08W\'\napp.config[\'UPLOAD_FOLDER\'] = \'/Upload\'\n\n################################################################################\n################################## INDEX #######################################\n################################################################################\n\n# Home page\n@app.route(\'/\', methods=[\'GET\'])\ndef index():\n    return render_template(\'index.html\')\n\n################################################################################\n################################## RULES #######################################\n################################################################################\n\n# Rules of the game\n@app.route(\'/rules\')\ndef rules():\n    return render_template(\'rules.html\')\n\n################################################################################\n############################### VIDEO INTERVIEW ################################\n################################################################################\n\n# Read the overall dataframe before the user starts to add his own data\ndf = pd.read_csv(\'static/js/db/histo.txt\', sep="","")\n\n# Video interview template\n@app.route(\'/video\', methods=[\'POST\'])\ndef video() :\n    # Display a warning message\n    flash(\'You will have 45 seconds to discuss the topic mentioned above. Due to restrictions, we are not able to redirect you once the video is over. Please move your URL to /video_dash instead of /video_1 once over. You will be able to see your results then.\')\n    return render_template(\'video.html\')\n\n# Display the video flow (face, landmarks, emotion)\n@app.route(\'/video_1\', methods=[\'POST\'])\ndef video_1() :\n    try :\n        # Response is used to display a flow of information\n        return Response(gen(),mimetype=\'multipart/x-mixed-replace; boundary=frame\')\n    #return Response(stream_template(\'video.html\', gen()))\n    except :\n        return None\n\n# Dashboard\n@app.route(\'/video_dash\', methods=(""POST"", ""GET""))\ndef video_dash():\n    \n    # Load personal history\n    df_2 = pd.read_csv(\'static/js/db/histo_perso.txt\')\n\n\n    def emo_prop(df_2) :\n        return [int(100*len(df_2[df_2.density==0])/len(df_2)),\n                    int(100*len(df_2[df_2.density==1])/len(df_2)),\n                    int(100*len(df_2[df_2.density==2])/len(df_2)),\n                    int(100*len(df_2[df_2.density==3])/len(df_2)),\n                    int(100*len(df_2[df_2.density==4])/len(df_2)),\n                    int(100*len(df_2[df_2.density==5])/len(df_2)),\n                    int(100*len(df_2[df_2.density==6])/len(df_2))]\n\n    emotions = [""Angry"", ""Disgust"", ""Fear"",  ""Happy"", ""Sad"", ""Surprise"", ""Neutral""]\n    emo_perso = {}\n    emo_glob = {}\n\n    for i in range(len(emotions)) :\n        emo_perso[emotions[i]] = len(df_2[df_2.density==i])\n        emo_glob[emotions[i]] = len(df[df.density==i])\n\n    df_perso = pd.DataFrame.from_dict(emo_perso, orient=\'index\')\n    df_perso = df_perso.reset_index()\n    df_perso.columns = [\'EMOTION\', \'VALUE\']\n    df_perso.to_csv(\'static/js/db/hist_vid_perso.txt\', sep="","", index=False)\n\n    df_glob = pd.DataFrame.from_dict(emo_glob, orient=\'index\')\n    df_glob = df_glob.reset_index()\n    df_glob.columns = [\'EMOTION\', \'VALUE\']\n    df_glob.to_csv(\'static/js/db/hist_vid_glob.txt\', sep="","", index=False)\n\n    emotion = df_2.density.mode()[0]\n    emotion_other = df.density.mode()[0]\n\n    def emotion_label(emotion) :\n        if emotion == 0 :\n            return ""Angry""\n        elif emotion == 1 :\n            return ""Disgust""\n        elif emotion == 2 :\n            return ""Fear""\n        elif emotion == 3 :\n            return ""Happy""\n        elif emotion == 4 :\n            return ""Sad""\n        elif emotion == 5 :\n            return ""Surprise""\n        else :\n            return ""Neutral""\n\n    ### Altair Plot\n    df_altair = pd.read_csv(\'static/js/db/prob.csv\', header=None, index_col=None).reset_index()\n    df_altair.columns = [\'Time\', \'Angry\', \'Disgust\', \'Fear\', \'Happy\', \'Sad\', \'Surprise\', \'Neutral\']\n\n    \n    angry = alt.Chart(df_altair).mark_line(color=\'orange\', strokeWidth=2).encode(\n       x=\'Time:Q\',\n       y=\'Angry:Q\',\n       tooltip=[""Angry""]\n    )\n\n    disgust = alt.Chart(df_altair).mark_line(color=\'red\', strokeWidth=2).encode(\n        x=\'Time:Q\',\n        y=\'Disgust:Q\',\n        tooltip=[""Disgust""])\n\n\n    fear = alt.Chart(df_altair).mark_line(color=\'green\', strokeWidth=2).encode(\n        x=\'Time:Q\',\n        y=\'Fear:Q\',\n        tooltip=[""Fear""])\n\n\n    happy = alt.Chart(df_altair).mark_line(color=\'blue\', strokeWidth=2).encode(\n        x=\'Time:Q\',\n        y=\'Happy:Q\',\n        tooltip=[""Happy""])\n\n\n    sad = alt.Chart(df_altair).mark_line(color=\'black\', strokeWidth=2).encode(\n        x=\'Time:Q\',\n        y=\'Sad:Q\',\n        tooltip=[""Sad""])\n\n\n    surprise = alt.Chart(df_altair).mark_line(color=\'pink\', strokeWidth=2).encode(\n        x=\'Time:Q\',\n        y=\'Surprise:Q\',\n        tooltip=[""Surprise""])\n\n\n    neutral = alt.Chart(df_altair).mark_line(color=\'brown\', strokeWidth=2).encode(\n        x=\'Time:Q\',\n        y=\'Neutral:Q\',\n        tooltip=[""Neutral""])\n\n\n    chart = (angry + disgust + fear + happy + sad + surprise + neutral).properties(\n    width=1000, height=400, title=\'Probability of each emotion over time\')\n\n    chart.save(\'static/CSS/chart.html\')\n    \n    return render_template(\'video_dash.html\', emo=emotion_label(emotion), emo_other = emotion_label(emotion_other), prob = emo_prop(df_2), prob_other = emo_prop(df))\n\n\n################################################################################\n############################### AUDIO INTERVIEW ################################\n################################################################################\n\n# Audio Index\n@app.route(\'/audio_index\', methods=[\'POST\'])\ndef audio_index():\n\n    # Flash message\n    flash(""After pressing the button above, you will have 15sec to answer the question."")\n    \n    return render_template(\'audio.html\', display_button=False)\n\n# Audio Recording\n@app.route(\'/audio_recording\', methods=(""POST"", ""GET""))\ndef audio_recording():\n\n    # Instanciate new SpeechEmotionRecognition object\n    SER = speechEmotionRecognition()\n\n    # Voice Recording\n    rec_duration = 16 # in sec\n    rec_sub_dir = os.path.join(\'tmp\',\'voice_recording.wav\')\n    SER.voice_recording(rec_sub_dir, duration=rec_duration)\n\n    # Send Flash message\n    flash(""The recording is over! You now have the opportunity to do an analysis of your emotions. If you wish, you can also choose to record yourself again."")\n\n    return render_template(\'audio.html\', display_button=True)\n\n\n# Audio Emotion Analysis\n@app.route(\'/audio_dash\', methods=(""POST"", ""GET""))\ndef audio_dash():\n\n    # Sub dir to speech emotion recognition model\n    model_sub_dir = os.path.join(\'Models\', \'audio.hdf5\')\n\n    # Instanciate new SpeechEmotionRecognition object\n    SER = speechEmotionRecognition(model_sub_dir)\n\n    # Voice Record sub dir\n    rec_sub_dir = os.path.join(\'tmp\',\'voice_recording.wav\')\n\n    # Predict emotion in voice at each time step\n    step = 1 # in sec\n    sample_rate = 16000 # in kHz\n    emotions, timestamp = SER.predict_emotion_from_file(rec_sub_dir, chunk_step=step*sample_rate)\n\n    # Export predicted emotions to .txt format\n    SER.prediction_to_csv(emotions, os.path.join(""static/js/db"", ""audio_emotions.txt""), mode=\'w\')\n    SER.prediction_to_csv(emotions, os.path.join(""static/js/db"", ""audio_emotions_other.txt""), mode=\'a\')\n\n    # Get most common emotion during the interview\n    major_emotion = max(set(emotions), key=emotions.count)\n\n    # Calculate emotion distribution\n    emotion_dist = [int(100 * emotions.count(emotion) / len(emotions)) for emotion in SER._emotion.values()]\n\n    # Export emotion distribution to .csv format for D3JS\n    df = pd.DataFrame(emotion_dist, index=SER._emotion.values(), columns=[\'VALUE\']).rename_axis(\'EMOTION\')\n    df.to_csv(os.path.join(\'static/js/db\',\'audio_emotions_dist.txt\'), sep=\',\')\n\n    # Get most common emotion of other candidates\n    df_other = pd.read_csv(os.path.join(""static/js/db"", ""audio_emotions_other.txt""), sep="","")\n\n    # Get most common emotion during the interview for other candidates\n    major_emotion_other = df_other.EMOTION.mode()[0]\n\n    # Calculate emotion distribution for other candidates\n    emotion_dist_other = [int(100 * len(df_other[df_other.EMOTION==emotion]) / len(df_other)) for emotion in SER._emotion.values()]\n\n    # Export emotion distribution to .csv format for D3JS\n    df_other = pd.DataFrame(emotion_dist_other, index=SER._emotion.values(), columns=[\'VALUE\']).rename_axis(\'EMOTION\')\n    df_other.to_csv(os.path.join(\'static/js/db\',\'audio_emotions_dist_other.txt\'), sep=\',\')\n\n    # Sleep\n    time.sleep(0.5)\n\n    return render_template(\'audio_dash.html\', emo=major_emotion, emo_other=major_emotion_other, prob=emotion_dist, prob_other=emotion_dist_other)\n\n\n################################################################################\n############################### TEXT INTERVIEW #################################\n################################################################################\n\nglobal df_text\n\ntempdirectory = tempfile.gettempdir()\n\n@app.route(\'/text\', methods=[\'POST\'])\ndef text() :\n    return render_template(\'text.html\')\n\ndef get_personality(text):\n    try:\n        pred = predict().run(text, model_name = ""Personality_traits_NN"")\n        return pred\n    except KeyError:\n        return None\n\ndef get_text_info(text):\n    text = text[0]\n    words = wordpunct_tokenize(text)\n    common_words = FreqDist(words).most_common(100)\n    counts = Counter(words)\n    num_words = len(text.split())\n    return common_words, num_words, counts\n\ndef preprocess_text(text):\n    preprocessed_texts = NLTKPreprocessor().transform([text])\n    return preprocessed_texts\n\n@app.route(\'/text_1\', methods=[\'POST\'])\ndef text_1():\n    \n    text = request.form.get(\'text\')\n    traits = [\'Extraversion\', \'Neuroticism\', \'Agreeableness\', \'Conscientiousness\', \'Openness\']\n    probas = get_personality(text)[0].tolist()\n    \n    df_text = pd.read_csv(\'static/js/db/text.txt\', sep="","")\n    df_new = df_text.append(pd.DataFrame([probas], columns=traits))\n    df_new.to_csv(\'static/js/db/text.txt\', sep="","", index=False)\n    \n    perso = {}\n    perso[\'Extraversion\'] = probas[0]\n    perso[\'Neuroticism\'] = probas[1]\n    perso[\'Agreeableness\'] = probas[2]\n    perso[\'Conscientiousness\'] = probas[3]\n    perso[\'Openness\'] = probas[4]\n    \n    df_text_perso = pd.DataFrame.from_dict(perso, orient=\'index\')\n    df_text_perso = df_text_perso.reset_index()\n    df_text_perso.columns = [\'Trait\', \'Value\']\n    \n    df_text_perso.to_csv(\'static/js/db/text_perso.txt\', sep=\',\', index=False)\n    \n    means = {}\n    means[\'Extraversion\'] = np.mean(df_new[\'Extraversion\'])\n    means[\'Neuroticism\'] = np.mean(df_new[\'Neuroticism\'])\n    means[\'Agreeableness\'] = np.mean(df_new[\'Agreeableness\'])\n    means[\'Conscientiousness\'] = np.mean(df_new[\'Conscientiousness\'])\n    means[\'Openness\'] = np.mean(df_new[\'Openness\'])\n    \n    probas_others = [np.mean(df_new[\'Extraversion\']), np.mean(df_new[\'Neuroticism\']), np.mean(df_new[\'Agreeableness\']), np.mean(df_new[\'Conscientiousness\']), np.mean(df_new[\'Openness\'])]\n    probas_others = [int(e*100) for e in probas_others]\n    \n    df_mean = pd.DataFrame.from_dict(means, orient=\'index\')\n    df_mean = df_mean.reset_index()\n    df_mean.columns = [\'Trait\', \'Value\']\n    \n    df_mean.to_csv(\'static/js/db/text_mean.txt\', sep=\',\', index=False)\n    trait_others = df_mean.loc[df_mean[\'Value\'].idxmax()][\'Trait\']\n    \n    probas = [int(e*100) for e in probas]\n    \n    data_traits = zip(traits, probas)\n    \n    session[\'probas\'] = probas\n    session[\'text_info\'] = {}\n    session[\'text_info\'][""common_words""] = []\n    session[\'text_info\'][""num_words""] = []\n    \n    preprocessed_text = preprocess_text(text)\n    common_words, num_words, counts = get_text_info(preprocessed_text)\n    \n    session[\'text_info\'][""common_words""].append(common_words)\n    session[\'text_info\'][""num_words""].append(num_words)\n    \n    trait = traits[probas.index(max(probas))]\n    \n    with open(""static/js/db/words_perso.txt"", ""w"") as d:\n        d.write(""WORDS,FREQ"" + \'\\n\')\n        for line in counts :\n            d.write(line + "","" + str(counts[line]) + \'\\n\')\n        d.close()\n    \n    with open(""static/js/db/words_common.txt"", ""a"") as d:\n        for line in counts :\n            d.write(line + "","" + str(counts[line]) + \'\\n\')\n        d.close()\n\n    df_words_co = pd.read_csv(\'static/js/db/words_common.txt\', sep=\',\', error_bad_lines=False)\n    df_words_co.FREQ = df_words_co.FREQ.apply(pd.to_numeric)\n    df_words_co = df_words_co.groupby(\'WORDS\').sum().reset_index()\n    df_words_co.to_csv(\'static/js/db/words_common.txt\', sep="","", index=False)\n    common_words_others = df_words_co.sort_values(by=[\'FREQ\'], ascending=False)[\'WORDS\'][:15]\n\n    df_words_perso = pd.read_csv(\'static/js/db/words_perso.txt\', sep=\',\', error_bad_lines=False)\n    common_words_perso = df_words_perso.sort_values(by=[\'FREQ\'], ascending=False)[\'WORDS\'][:15]\n\n    return render_template(\'text_dash.html\', traits = probas, trait = trait, trait_others = trait_others, probas_others = probas_others, num_words = num_words, common_words = common_words_perso, common_words_others=common_words_others)\n\nALLOWED_EXTENSIONS = set([\'pdf\'])\n\ndef allowed_file(filename):\n    return \'.\' in filename and filename.rsplit(\'.\', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n@app.route(\'/text_pdf\', methods=[\'POST\'])\ndef text_pdf():\n    f = request.files[\'file\']\n    f.save(secure_filename(f.filename))\n    \n    text = parser.from_file(f.filename)[\'content\']\n    traits = [\'Extraversion\', \'Neuroticism\', \'Agreeableness\', \'Conscientiousness\', \'Openness\']\n    probas = get_personality(text)[0].tolist()\n    \n    df_text = pd.read_csv(\'static/js/db/text.txt\', sep="","")\n    df_new = df_text.append(pd.DataFrame([probas], columns=traits))\n    df_new.to_csv(\'static/js/db/text.txt\', sep="","", index=False)\n    \n    perso = {}\n    perso[\'Extraversion\'] = probas[0]\n    perso[\'Neuroticism\'] = probas[1]\n    perso[\'Agreeableness\'] = probas[2]\n    perso[\'Conscientiousness\'] = probas[3]\n    perso[\'Openness\'] = probas[4]\n    \n    df_text_perso = pd.DataFrame.from_dict(perso, orient=\'index\')\n    df_text_perso = df_text_perso.reset_index()\n    df_text_perso.columns = [\'Trait\', \'Value\']\n    \n    df_text_perso.to_csv(\'static/js/db/text_perso.txt\', sep=\',\', index=False)\n    \n    means = {}\n    means[\'Extraversion\'] = np.mean(df_new[\'Extraversion\'])\n    means[\'Neuroticism\'] = np.mean(df_new[\'Neuroticism\'])\n    means[\'Agreeableness\'] = np.mean(df_new[\'Agreeableness\'])\n    means[\'Conscientiousness\'] = np.mean(df_new[\'Conscientiousness\'])\n    means[\'Openness\'] = np.mean(df_new[\'Openness\'])\n    \n    probas_others = [np.mean(df_new[\'Extraversion\']), np.mean(df_new[\'Neuroticism\']), np.mean(df_new[\'Agreeableness\']), np.mean(df_new[\'Conscientiousness\']), np.mean(df_new[\'Openness\'])]\n    probas_others = [int(e*100) for e in probas_others]\n    \n    df_mean = pd.DataFrame.from_dict(means, orient=\'index\')\n    df_mean = df_mean.reset_index()\n    df_mean.columns = [\'Trait\', \'Value\']\n    \n    df_mean.to_csv(\'static/js/db/text_mean.txt\', sep=\',\', index=False)\n    trait_others = df_mean.ix[df_mean[\'Value\'].idxmax()][\'Trait\']\n    \n    probas = [int(e*100) for e in probas]\n    \n    data_traits = zip(traits, probas)\n    \n    session[\'probas\'] = probas\n    session[\'text_info\'] = {}\n    session[\'text_info\'][""common_words""] = []\n    session[\'text_info\'][""num_words""] = []\n    \n    preprocessed_text = preprocess_text(text)\n    common_words, num_words, counts = get_text_info(preprocessed_text)\n    \n    session[\'text_info\'][""common_words""].append(common_words)\n    session[\'text_info\'][""num_words""].append(num_words)\n    \n    trait = traits[probas.index(max(probas))]\n    \n    with open(""static/js/db/words_perso.txt"", ""w"") as d:\n        d.write(""WORDS,FREQ"" + \'\\n\')\n        for line in counts :\n            d.write(line + "","" + str(counts[line]) + \'\\n\')\n        d.close()\n    \n    with open(""static/js/db/words_common.txt"", ""a"") as d:\n        for line in counts :\n            d.write(line + "","" + str(counts[line]) + \'\\n\')\n        d.close()\n\n    df_words_co = pd.read_csv(\'static/js/db/words_common.txt\', sep=\',\', error_bad_lines=False)\n    df_words_co.FREQ = df_words_co.FREQ.apply(pd.to_numeric)\n    df_words_co = df_words_co.groupby(\'WORDS\').sum().reset_index()\n    df_words_co.to_csv(\'static/js/db/words_common.txt\', sep="","", index=False)\n    common_words_others = df_words_co.sort_values(by=[\'FREQ\'], ascending=False)[\'WORDS\'][:15]\n\n    df_words_perso = pd.read_csv(\'static/js/db/words_perso.txt\', sep=\',\', error_bad_lines=False)\n    common_words_perso = df_words_perso.sort_values(by=[\'FREQ\'], ascending=False)[\'WORDS\'][:15]\n\n    return render_template(\'text_dash.html\', traits = probas, trait = trait, trait_others = trait_others, probas_others = probas_others, num_words = num_words, common_words = common_words_perso, common_words_others=common_words_others)\n\nif __name__ == \'__main__\':\n    app.run(debug=True)\n'"
02-Text/Python/get_text.py,0,"b'#!/usr/bin/python3\n# -*- coding: utf-8 -*-\nimport numpy as np\nfrom bs4 import BeautifulSoup\nfrom json import loads\nfrom urllib.request import urlopen\nfrom urllib.parse import urlencode\nimport ssl\nimport re\nimport urllib\nfrom flask import Flask, render_template, session, request, redirect, flash, url_for\nfrom predict import *\nfrom nltk import *\n\n\ndef get_personality(text):\n    try:\n        pred = predict().run(text, model_name = ""Personality_traits_NN"")\n        return pred\n    except KeyError:\n        return None\n\ndef get_text_info(text):\n    # Retrieve some info on the text data\n    words = tokenize.word_tokenize(text)\n    common_words = FreqDist(words).most_common(100)\n    num_words = len(text.split())\n    return common_words, num_words\n\napp = Flask(__name__)\napp.secret_key = ""motdepasse""\n\n@app.route(\'/\', methods=[\'GET\'])\ndef index():\n    return render_template(\'index.html\')\n\n@app.route(\'/text\', methods=[\'POST\'])\ndef text():\n    text = request.form.get(\'text\')\n    traits = [\'Extraversion\', \'Neuroticism\', \'Agreeableness\', \'Conscientiousness\', \'Openness\']\n    probas = get_personality(text)[0].tolist()\n    data_traits = zip(traits, probas)\n    session[\'probas\'] = probas\n    session[\'text_info\'] = {}\n    session[\'text_info\'][""common_words""] = []\n    session[\'text_info\'][""num_words""] = []\n    common_words, num_words = get_text_info(text)\n    session[\'text_info\'][""common_words""].append(common_words)\n    session[\'text_info\'][""num_words""].append(num_words)\n    trait = traits[probas.index(max(probas))]\n    #flash(\'Your dominant personality trait is : {}\'.format(str(trait)))\n    return render_template(\'result.html\', traits = data_traits, trait = trait, num_words = num_words, common_words = common_words)\n\n\nif __name__ == \'__main__\':\n    app.run(debug=True)\n\n\n'"
02-Text/Python/load_data.py,0,"b'import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk import *\n\nclass load_data:\n\n    def __init__(self, path = \'/Users/raphaellederman/Desktop/Fil_Rouge_Local/Text/Data/essays.csv\'):\n        self.path = path\n\n    def run(self):\n        self.data_essays = pd.read_csv(self.path, encoding = ""ISO-8859-1"")\n        self.data_essays[\'cEXT\'] = np.where(self.data_essays[\'cEXT\']==\'y\', 1, 0)\n        self.data_essays[\'cNEU\'] = np.where(self.data_essays[\'cNEU\']==\'y\', 1, 0)\n        self.data_essays[\'cAGR\'] = np.where(self.data_essays[\'cAGR\']==\'y\', 1, 0)\n        self.data_essays[\'cCON\'] = np.where(self.data_essays[\'cCON\']==\'y\', 1, 0)\n        self.data_essays[\'cOPN\'] = np.where(self.data_essays[\'cOPN\']==\'y\', 1, 0)\n        self.X_essays = self.data_essays[\'TEXT\'].tolist()\n        self.y_essays = self.data_essays[[\'cEXT\', \'cNEU\', \'cAGR\', \'cCON\', \'cOPN\']]\n        self.labels = [\'cEXT\', \'cNEU\', \'cAGR\', \'cCON\', \'cOPN\']\n        return self.data_essays, self.X_essays, self.y_essays, self.labels\n\n\n\n\n\n'"
02-Text/Python/predict.py,0,"b'from gensim.models import KeyedVectors\nfrom gensim.models import word2vec\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport datetime\nfrom operator import itemgetter\nfrom random import randint\nimport seaborn as sns\n\nimport os\nimport time\nimport string\nimport dill\nimport pickle\n\nfrom nltk import *\nfrom nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\nfrom nltk.corpus import stopwords as sw, wordnet as wn\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, FunctionTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import precision_score, accuracy_score, confusion_matrix, classification_report as clsr\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\nfrom sklearn.model_selection import GridSearchCV, train_test_split as tts\nfrom sklearn.manifold import TSNE\nfrom sklearn.multiclass import OneVsRestClassifier\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, LSTM, SpatialDropout1D, Activation, Conv1D, MaxPooling1D, Input, concatenate\nfrom keras.utils.np_utils import to_categorical\n\nclass predict:\n\n    def __init__(self):\n        self.max_sentence_len = 300\n        self.max_features = 300\n        self.embed_dim = 300\n        self.NLTKPreprocessor = self.NLTKPreprocessor()\n\n\n    class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n        """"""\n        Transforms input data by using NLTK tokenization, POS tagging, lemmatization and vectorization.\n        """"""\n\n        def __init__(self, max_sentence_len = 300, stopwords=None, punct=None, lower=True, strip=True):\n            """"""\n            Instantiates the preprocessor.\n            """"""\n            self.lower = lower\n            self.strip = strip\n            self.stopwords = set(stopwords) if stopwords else set(sw.words(\'english\'))\n            self.punct = set(punct) if punct else set(string.punctuation)\n            self.lemmatizer = WordNetLemmatizer()\n            self.max_sentence_len = max_sentence_len\n\n        def fit(self, X, y=None):\n            """"""\n            Fit simply returns self.\n            """"""\n            return self\n\n        def inverse_transform(self, X):\n            """"""\n            No inverse transformation.\n            """"""\n            return X\n\n        def transform(self, X):\n            """"""\n            Actually runs the preprocessing on each document.\n            """"""\n            output = np.array([(self.tokenize(doc)) for doc in X])\n            return output\n\n        def tokenize(self, document):\n            """"""\n            Returns a normalized, lemmatized list of tokens from a document by\n            applying segmentation, tokenization, and part of speech tagging.\n            Uses the part of speech tags to look up the lemma in WordNet, and returns the lowercase\n            version of all the words, removing stopwords and punctuation.\n            """"""\n            lemmatized_tokens = []\n\n            # Clean the text\n            document = re.sub(r""[^A-Za-z0-9^,!.\\/\'+-=]"", "" "", document)\n            document = re.sub(r""what\'s"", ""what is "", document)\n            document = re.sub(r""\\\'s"", "" "", document)\n            document = re.sub(r""\\\'ve"", "" have "", document)\n            document = re.sub(r""can\'t"", ""cannot "", document)\n            document = re.sub(r""n\'t"", "" not "", document)\n            document = re.sub(r""i\'m"", ""i am "", document)\n            document = re.sub(r""\\\'re"", "" are "", document)\n            document = re.sub(r""\\\'d"", "" would "", document)\n            document = re.sub(r""\\\'ll"", "" will "", document)\n            document = re.sub(r""(\\d+)(k)"", r""\\g<1>000"", document)\n\n            # Break the document into sentences\n            for sent in sent_tokenize(document):\n\n                # Break the sentence into part of speech tagged tokens\n                for token, tag in pos_tag(wordpunct_tokenize(sent)):\n\n                    # Apply preprocessing to the token\n                    token = token.lower() if self.lower else token\n                    token = token.strip() if self.strip else token\n                    token = token.strip(\'_\') if self.strip else token\n                    token = token.strip(\'*\') if self.strip else token\n\n                    # If punctuation or stopword, ignore token and continue\n                    if token in self.stopwords or all(char in self.punct for char in token):\n                        continue\n\n                    # Lemmatize the token\n                    lemma = self.lemmatize(token, tag)\n                    lemmatized_tokens.append(lemma)\n\n            doc = \' \'.join(lemmatized_tokens)\n            tokenized_document = self.vectorize(np.array(doc)[np.newaxis])\n            return tokenized_document\n\n\n        def vectorize(self, doc):\n            """"""\n            Returns a vectorized padded version of sequences.\n            """"""\n            save_path = ""/Users/raphaellederman/Desktop/Fil_Rouge/Text/Data/padding.pickle""\n            with open(save_path, \'rb\') as f:\n                tokenizer = pickle.load(f)\n            doc_pad = tokenizer.texts_to_sequences(doc)\n            doc_pad = pad_sequences(doc_pad, padding=\'pre\', truncating=\'pre\', maxlen=self.max_sentence_len)\n            return np.squeeze(doc_pad)\n\n        def lemmatize(self, token, tag):\n            """"""\n            Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n            tag to perform WordNet lemmatization.\n            """"""\n            tag = {\n                \'N\': wn.NOUN,\n                \'V\': wn.VERB,\n                \'R\': wn.ADV,\n                \'J\': wn.ADJ\n            }.get(tag[0], wn.NOUN)\n\n            return self.lemmatizer.lemmatize(token, tag)\n\n\n    class MyRNNTransformer(BaseEstimator, TransformerMixin):\n        """"""\n        Transformer allowing our Keras model to be included in our pipeline\n        """"""\n        def __init__(self, classifier):\n            self.classifier = classifier\n\n        def fit(self, X, y):\n            batch_size = 32\n            num_epochs = 35\n            batch_size = batch_size\n            epochs = num_epochs\n            self.classifier.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=2)\n            return self\n\n        def transform(self, X):\n            self.pred = self.classifier.predict(X)\n            self.classes = [[0 if el < 0.2 else 1 for el in item] for item in self.pred]\n            return self.pred\n\n\n    def run(self, X, model_name):\n        """"""\n        Returns the predictions from the pipeline including our NLTKPreprocessor and Keras classifier.\n        """"""\n        def build(classifier):\n            """"""\n            Inner build function that builds a pipeline including a preprocessor and a classifier.\n            """"""\n            model = Pipeline([\n                    (\'preprocessor\', self.NLTKPreprocessor),\n                    (\'classifier\', classifier)\n                ])\n            return model\n\n        save_path = \'/Users/raphaellederman/Desktop/Fil_Rouge/Text/Models/\'\n        json_file = open(save_path + model_name + \'.json\', \'r\')\n        classifier = model_from_json(json_file.read())\n        classifier.load_weights(save_path + model_name + \'.h5\')\n        classifier.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n        json_file.close()\n        model = build(self.MyRNNTransformer(classifier))\n        y_pred = model.transform([X])\n\n        return y_pred\n\n\n'"
02-Text/Python/predict_svm.py,0,"b'from gensim.models import KeyedVectors\nfrom gensim.models import word2vec\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport datetime\nfrom operator import itemgetter\nfrom random import randint\nimport seaborn as sns\n\nimport os\nimport time\nimport string\nimport dill\nimport pickle\n\nfrom nltk import *\nfrom nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\nfrom nltk.corpus import stopwords as sw, wordnet as wn\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, FunctionTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import precision_score, accuracy_score, confusion_matrix, classification_report as clsr\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\nfrom sklearn.model_selection import GridSearchCV, train_test_split as tts\nfrom sklearn.manifold import TSNE\nfrom sklearn.multiclass import OneVsRestClassifier\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, LSTM, SpatialDropout1D, Activation, Conv1D, MaxPooling1D, Input, concatenate\nfrom keras.utils.np_utils import to_categorical\n\n\nclass predict_svm:\n\n    def __init__(self):\n        self.max_sentence_len = 300\n        self.NLTKPreprocessor = self.NLTKPreprocessor()\n        #self.MyRNNTransformer = self.MyRNNTransformer()\n\n\n    class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n        """"""\n        Transforms input data by using NLTK tokenization, POS tagging, lemmatization and vectorization.\n        """"""\n\n        def __init__(self, max_sentence_len = 300, stopwords=None, punct=None, lower=True, strip=True):\n            """"""\n            Instantiates the preprocessor.\n            """"""\n            self.lower = lower\n            self.strip = strip\n            self.stopwords = set(stopwords) if stopwords else set(sw.words(\'english\'))\n            self.punct = set(punct) if punct else set(string.punctuation)\n            self.lemmatizer = WordNetLemmatizer()\n            self.max_sentence_len = max_sentence_len\n\n        def fit(self, X, y=None):\n            """"""\n            Fit simply returns self.\n            """"""\n            return self\n\n        def inverse_transform(self, X):\n            """"""\n            No inverse transformation.\n            """"""\n            return X\n\n        def transform(self, X):\n            """"""\n            Actually runs the preprocessing on each document.\n            """"""\n            output = np.array([(self.tokenize(doc)) for doc in X])\n            return output\n\n        def tokenize(self, document):\n            """"""\n            Returns a normalized, lemmatized list of tokens from a document by\n            applying segmentation, tokenization, and part of speech tagging.\n            Uses the part of speech tags to look up the lemma in WordNet, and returns the lowercase\n            version of all the words, removing stopwords and punctuation.\n            """"""\n            lemmatized_tokens = []\n\n            # Clean the text\n            document = re.sub(r""[^A-Za-z0-9^,!.\\/\'+-=]"", "" "", document)\n            document = re.sub(r""what\'s"", ""what is "", document)\n            document = re.sub(r""\\\'s"", "" "", document)\n            document = re.sub(r""\\\'ve"", "" have "", document)\n            document = re.sub(r""can\'t"", ""cannot "", document)\n            document = re.sub(r""n\'t"", "" not "", document)\n            document = re.sub(r""i\'m"", ""i am "", document)\n            document = re.sub(r""\\\'re"", "" are "", document)\n            document = re.sub(r""\\\'d"", "" would "", document)\n            document = re.sub(r""\\\'ll"", "" will "", document)\n            document = re.sub(r""(\\d+)(k)"", r""\\g<1>000"", document)\n\n            # Break the document into sentences\n            for sent in sent_tokenize(document):\n\n                # Break the sentence into part of speech tagged tokens\n                for token, tag in pos_tag(wordpunct_tokenize(sent)):\n\n                    # Apply preprocessing to the token\n                    token = token.lower() if self.lower else token\n                    token = token.strip() if self.strip else token\n                    token = token.strip(\'_\') if self.strip else token\n                    token = token.strip(\'*\') if self.strip else token\n\n                    # If punctuation or stopword, ignore token and continue\n                    if token in self.stopwords or all(char in self.punct for char in token):\n                        continue\n\n                    # Lemmatize the token\n                    lemma = self.lemmatize(token, tag)\n                    lemmatized_tokens.append(lemma)\n\n            doc = \' \'.join(lemmatized_tokens)\n            tokenized_document = self.vectorize(np.array(doc)[np.newaxis])\n            return tokenized_document\n\n\n        def vectorize(self, doc):\n            """"""\n            Returns a vectorized padded version of sequences.\n            """"""\n            save_path = ""Data/padding.pickle""\n            with open(save_path, \'rb\') as f:\n                tokenizer = pickle.load(f)\n            doc_pad = tokenizer.texts_to_sequences(doc)\n            doc_pad = pad_sequences(doc_pad, padding=\'pre\', truncating=\'pre\', maxlen=self.max_sentence_len)\n            return np.squeeze(doc_pad)\n\n        def lemmatize(self, token, tag):\n            """"""\n            Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n            tag to perform WordNet lemmatization.\n            """"""\n            tag = {\n                \'N\': wn.NOUN,\n                \'V\': wn.VERB,\n                \'R\': wn.ADV,\n                \'J\': wn.ADJ\n            }.get(tag[0], wn.NOUN)\n\n            return self.lemmatizer.lemmatize(token, tag)\n\n\n    class MyRNNTransformer(BaseEstimator, TransformerMixin):\n        """"""\n        Transformer allowing our Keras model to be included in our pipeline\n        """"""\n        def __init__(self, classifier):\n            self.classifier = classifier\n\n        def fit(self, X, y):\n            batch_size = 32\n            num_epochs = 35\n            batch_size = batch_size\n            epochs = num_epochs\n            self.classifier.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=2)\n            return self\n\n        def transform(self, X):\n            self.pred = self.classifier.predict_proba(X)\n            self.classes = [[0 if el < 0.2 else 1 for el in item] for item in self.pred]\n            return self.pred\n\n\n    def identity(self, arg):\n        """"""\n        Simple identity function works as a passthrough.\n        """"""\n        return arg\n\n\n    def reshape_a_feature_column(self, series):\n        return np.reshape(np.asarray(series), (len(series), 1))\n\n\n    def pipelinize_feature(self, function, active=True):\n        def list_comprehend_a_function(list_or_series, active=True):\n            if active:\n                processed = [function(i) for i in list_or_series]\n                processed = self.reshape_a_feature_column(processed)\n                return processed\n            else:\n                return self.reshape_a_feature_column(np.zeros(len(list_or_series)))\n\n\n    def get_text_length(self, text):\n        return len(text)\n\n\n    def multiclass_accuracy(self,predictions, target):\n        ""Returns the multiclass accuracy of the classifier\'s predictions""\n        score = []\n        for j in range(0, 5):\n            count = 0\n            for i in range(len(predictions)):\n                if predictions[i][j] == target[i][j]:\n                    count += 1\n            score.append(count / len(predictions))\n        return score\n\n\n    def run(self, X, model_name):\n        """"""\n        Returns the predictions from the pipeline including our NLTKPreprocessor and Keras classifier.\n        """"""\n        def build(classifier):\n            """"""\n            Inner build function that builds a pipeline including a preprocessor and a classifier.\n            """"""\n            model = Pipeline([\n                (\'preprocessor\', self.NLTKPreprocessor),\n                (\'features\', FeatureUnion([\n                    (\'vectorizer\', TfidfVectorizer(tokenizer=self.identity, preprocessor=None, lowercase=False)),\n                    (\'text_length\', self.pipelinize_feature(self.get_text_length, active=True))\n                ])),\n                (\'classifier\', classifier)\n            ])\n            return model\n\n        save_path = ""Models/""\n        with open(save_path + model_name, \'rb\') as f:\n            model = dill.load(f)\n        y_pred = model.predict(X)\n        return y_pred'"
02-Text/Python/test.py,0,"b'from gensim.models import KeyedVectors\nfrom gensim.models import word2vec\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport datetime\nfrom operator import itemgetter\nfrom random import randint\nimport seaborn as sns\n\nimport os\nimport time\nimport string\nimport dill\nimport pickle\n\nfrom nltk import *\nfrom nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\nfrom nltk.corpus import stopwords as sw, wordnet as wn\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, FunctionTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import precision_score, accuracy_score, confusion_matrix, classification_report as clsr\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\nfrom sklearn.model_selection import GridSearchCV, train_test_split as tts\nfrom sklearn.manifold import TSNE\nfrom sklearn.multiclass import OneVsRestClassifier\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, LSTM, SpatialDropout1D, Activation, Conv1D, MaxPooling1D, Input, concatenate\nfrom keras.utils.np_utils import to_categorical\n\nclass test:\n\n    def __init__(self):\n        self.max_sentence_len = 300\n        self.max_features = 300\n        self.embed_dim = 300\n        self.NLTKPreprocessor = self.NLTKPreprocessor()\n        #self.MyRNNTransformer = self.MyRNNTransformer()\n\n\n    class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n        """"""\n        Transforms input data by using NLTK tokenization, POS tagging, lemmatization and vectorization.\n        """"""\n\n        def __init__(self, max_sentence_len = 300, stopwords=None, punct=None, lower=True, strip=True):\n            """"""\n            Instantiates the preprocessor.\n            """"""\n            self.lower = lower\n            self.strip = strip\n            self.stopwords = set(stopwords) if stopwords else set(sw.words(\'english\'))\n            self.punct = set(punct) if punct else set(string.punctuation)\n            self.lemmatizer = WordNetLemmatizer()\n            self.max_sentence_len = max_sentence_len\n\n        def fit(self, X, y=None):\n            """"""\n            Fit simply returns self.\n            """"""\n            return self\n\n        def inverse_transform(self, X):\n            """"""\n            No inverse transformation.\n            """"""\n            return X\n\n        def transform(self, X):\n            """"""\n            Actually runs the preprocessing on each document.\n            """"""\n            output = np.array([(self.tokenize(doc)) for doc in X])\n            return output\n\n        def tokenize(self, document):\n            """"""\n            Returns a normalized, lemmatized list of tokens from a document by\n            applying segmentation, tokenization, and part of speech tagging.\n            Uses the part of speech tags to look up the lemma in WordNet, and returns the lowercase\n            version of all the words, removing stopwords and punctuation.\n            """"""\n            lemmatized_tokens = []\n\n            # Clean the text\n            document = re.sub(r""[^A-Za-z0-9^,!.\\/\'+-=]"", "" "", document)\n            document = re.sub(r""what\'s"", ""what is "", document)\n            document = re.sub(r""\\\'s"", "" "", document)\n            document = re.sub(r""\\\'ve"", "" have "", document)\n            document = re.sub(r""can\'t"", ""cannot "", document)\n            document = re.sub(r""n\'t"", "" not "", document)\n            document = re.sub(r""i\'m"", ""i am "", document)\n            document = re.sub(r""\\\'re"", "" are "", document)\n            document = re.sub(r""\\\'d"", "" would "", document)\n            document = re.sub(r""\\\'ll"", "" will "", document)\n            document = re.sub(r""(\\d+)(k)"", r""\\g<1>000"", document)\n\n            # Break the document into sentences\n            for sent in sent_tokenize(document):\n\n                # Break the sentence into part of speech tagged tokens\n                for token, tag in pos_tag(wordpunct_tokenize(sent)):\n\n                    # Apply preprocessing to the token\n                    token = token.lower() if self.lower else token\n                    token = token.strip() if self.strip else token\n                    token = token.strip(\'_\') if self.strip else token\n                    token = token.strip(\'*\') if self.strip else token\n\n                    # If punctuation or stopword, ignore token and continue\n                    if token in self.stopwords or all(char in self.punct for char in token):\n                        continue\n\n                    # Lemmatize the token\n                    lemma = self.lemmatize(token, tag)\n                    lemmatized_tokens.append(lemma)\n\n            doc = \' \'.join(lemmatized_tokens)\n            tokenized_document = self.vectorize(np.array(doc)[np.newaxis])\n            return tokenized_document\n\n\n        def vectorize(self, doc):\n            """"""\n            Returns a vectorized padded version of sequences.\n            """"""\n            save_path = ""Data/padding.pickle""\n            with open(save_path, \'rb\') as f:\n                tokenizer = pickle.load(f)\n            doc_pad = tokenizer.texts_to_sequences(doc)\n            doc_pad = pad_sequences(doc_pad, padding=\'pre\', truncating=\'pre\', maxlen=self.max_sentence_len)\n            return np.squeeze(doc_pad)\n\n        def lemmatize(self, token, tag):\n            """"""\n            Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n            tag to perform WordNet lemmatization.\n            """"""\n            tag = {\n                \'N\': wn.NOUN,\n                \'V\': wn.VERB,\n                \'R\': wn.ADV,\n                \'J\': wn.ADJ\n            }.get(tag[0], wn.NOUN)\n\n            return self.lemmatizer.lemmatize(token, tag)\n\n\n    class MyRNNTransformer(BaseEstimator, TransformerMixin):\n        """"""\n        Transformer allowing our Keras model to be included in our pipeline\n        """"""\n        def __init__(self, classifier):\n            self.classifier = classifier\n\n        def fit(self, X, y):\n            batch_size = 32\n            num_epochs = 35\n            batch_size = batch_size\n            epochs = num_epochs\n            self.classifier.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=2)\n            return self\n\n        def transform(self, X):\n            self.pred = self.classifier.predict(X)\n            self.classes = [[0 if el < 0.2 else 1 for el in item] for item in self.pred]\n            return self.pred\n\n    def multiclass_accuracy(self,predictions, target):\n        ""Returns the multiclass accuracy of the classifier\'s predictions""\n        score = []\n        for j in range(0, 5):\n            count = 0\n            for i in range(len(predictions)):\n                if predictions[i][j] == target[i][j]:\n                    count += 1\n            score.append(count / len(predictions))\n        return score\n\n\n    def run(self, X, y, model_name):\n        """"""\n        Returns the predictions from the pipeline including our NLTKPreprocessor and Keras classifier.\n        """"""\n        def build(classifier):\n            """"""\n            Inner build function that builds a pipeline including a preprocessor and a classifier.\n            """"""\n            model = Pipeline([\n                    (\'preprocessor\', self.NLTKPreprocessor),\n                    (\'classifier\', classifier)\n                ])\n            return model\n\n        save_path = \'Models/\'\n        json_file = open(save_path + model_name + \'.json\', \'r\')\n        classifier = model_from_json(json_file.read())\n        classifier.load_weights(save_path + model_name + \'.h5\')\n        classifier.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n        json_file.close()\n        model = build(self.MyRNNTransformer(classifier))\n        y_pred = model.transform(X)\n        y_pred_classes = [[0 if el < 0.2 else 1 for el in item] for item in y_pred]\n        print(self.multiclass_accuracy(y.values.tolist(), y_pred_classes))\n\n        return y_pred\n\n\n'"
02-Text/Python/test_svm.py,0,"b'from gensim.models import KeyedVectors\nfrom gensim.models import word2vec\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport datetime\nfrom operator import itemgetter\nfrom random import randint\nimport seaborn as sns\n\nimport os\nimport time\nimport string\nimport dill\nimport pickle\n\nfrom nltk import *\nfrom nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\nfrom nltk.corpus import stopwords as sw, wordnet as wn\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, FunctionTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import precision_score, accuracy_score, confusion_matrix, classification_report as clsr\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\nfrom sklearn.model_selection import GridSearchCV, train_test_split as tts\nfrom sklearn.manifold import TSNE\nfrom sklearn.multiclass import OneVsRestClassifier\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, LSTM, SpatialDropout1D, Activation, Conv1D, MaxPooling1D, Input, concatenate\nfrom keras.utils.np_utils import to_categorical\n\n\nclass test_svm:\n\n    def __init__(self):\n        self.max_sentence_len = 300\n        self.NLTKPreprocessor = self.NLTKPreprocessor()\n        #self.MyRNNTransformer = self.MyRNNTransformer()\n\n\n    class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n        """"""\n        Transforms input data by using NLTK tokenization, POS tagging, lemmatization and vectorization.\n        """"""\n\n        def __init__(self, max_sentence_len = 300, stopwords=None, punct=None, lower=True, strip=True):\n            """"""\n            Instantiates the preprocessor.\n            """"""\n            self.lower = lower\n            self.strip = strip\n            self.stopwords = set(stopwords) if stopwords else set(sw.words(\'english\'))\n            self.punct = set(punct) if punct else set(string.punctuation)\n            self.lemmatizer = WordNetLemmatizer()\n            self.max_sentence_len = max_sentence_len\n\n        def fit(self, X, y=None):\n            """"""\n            Fit simply returns self.\n            """"""\n            return self\n\n        def inverse_transform(self, X):\n            """"""\n            No inverse transformation.\n            """"""\n            return X\n\n        def transform(self, X):\n            """"""\n            Actually runs the preprocessing on each document.\n            """"""\n            output = np.array([(self.tokenize(doc)) for doc in X])\n            return output\n\n        def tokenize(self, document):\n            """"""\n            Returns a normalized, lemmatized list of tokens from a document by\n            applying segmentation, tokenization, and part of speech tagging.\n            Uses the part of speech tags to look up the lemma in WordNet, and returns the lowercase\n            version of all the words, removing stopwords and punctuation.\n            """"""\n            lemmatized_tokens = []\n\n            # Clean the text\n            document = re.sub(r""[^A-Za-z0-9^,!.\\/\'+-=]"", "" "", document)\n            document = re.sub(r""what\'s"", ""what is "", document)\n            document = re.sub(r""\\\'s"", "" "", document)\n            document = re.sub(r""\\\'ve"", "" have "", document)\n            document = re.sub(r""can\'t"", ""cannot "", document)\n            document = re.sub(r""n\'t"", "" not "", document)\n            document = re.sub(r""i\'m"", ""i am "", document)\n            document = re.sub(r""\\\'re"", "" are "", document)\n            document = re.sub(r""\\\'d"", "" would "", document)\n            document = re.sub(r""\\\'ll"", "" will "", document)\n            document = re.sub(r""(\\d+)(k)"", r""\\g<1>000"", document)\n\n            # Break the document into sentences\n            for sent in sent_tokenize(document):\n\n                # Break the sentence into part of speech tagged tokens\n                for token, tag in pos_tag(wordpunct_tokenize(sent)):\n\n                    # Apply preprocessing to the token\n                    token = token.lower() if self.lower else token\n                    token = token.strip() if self.strip else token\n                    token = token.strip(\'_\') if self.strip else token\n                    token = token.strip(\'*\') if self.strip else token\n\n                    # If punctuation or stopword, ignore token and continue\n                    if token in self.stopwords or all(char in self.punct for char in token):\n                        continue\n\n                    # Lemmatize the token\n                    lemma = self.lemmatize(token, tag)\n                    lemmatized_tokens.append(lemma)\n\n            doc = \' \'.join(lemmatized_tokens)\n            tokenized_document = self.vectorize(np.array(doc)[np.newaxis])\n            return tokenized_document\n\n\n        def vectorize(self, doc):\n            """"""\n            Returns a vectorized padded version of sequences.\n            """"""\n            save_path = ""Data/padding.pickle""\n            with open(save_path, \'rb\') as f:\n                tokenizer = pickle.load(f)\n            doc_pad = tokenizer.texts_to_sequences(doc)\n            doc_pad = pad_sequences(doc_pad, padding=\'pre\', truncating=\'pre\', maxlen=self.max_sentence_len)\n            return np.squeeze(doc_pad)\n\n        def lemmatize(self, token, tag):\n            """"""\n            Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n            tag to perform WordNet lemmatization.\n            """"""\n            tag = {\n                \'N\': wn.NOUN,\n                \'V\': wn.VERB,\n                \'R\': wn.ADV,\n                \'J\': wn.ADJ\n            }.get(tag[0], wn.NOUN)\n\n            return self.lemmatizer.lemmatize(token, tag)\n\n\n    class MyRNNTransformer(BaseEstimator, TransformerMixin):\n        """"""\n        Transformer allowing our Keras model to be included in our pipeline\n        """"""\n        def __init__(self, classifier):\n            self.classifier = classifier\n\n        def fit(self, X, y):\n            batch_size = 32\n            num_epochs = 35\n            batch_size = batch_size\n            epochs = num_epochs\n            self.classifier.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=2)\n            return self\n\n        def transform(self, X):\n            self.pred = self.classifier.predict_proba(X)\n            self.classes = [[0 if el < 0.2 else 1 for el in item] for item in self.pred]\n            return self.pred\n\n\n    def identity(self, arg):\n        """"""\n        Simple identity function works as a passthrough.\n        """"""\n        return arg\n\n\n    def reshape_a_feature_column(self, series):\n        return np.reshape(np.asarray(series), (len(series), 1))\n\n\n    def pipelinize_feature(self, function, active=True):\n        def list_comprehend_a_function(list_or_series, active=True):\n            if active:\n                processed = [function(i) for i in list_or_series]\n                processed = self.reshape_a_feature_column(processed)\n                return processed\n            else:\n                return self.reshape_a_feature_column(np.zeros(len(list_or_series)))\n\n\n    def get_text_length(self, text):\n        return len(text)\n\n\n    def multiclass_accuracy(self,predictions, target):\n        ""Returns the multiclass accuracy of the classifier\'s predictions""\n        score = []\n        for j in range(0, 5):\n            count = 0\n            for i in range(len(predictions)):\n                if predictions[i][j] == target[i][j]:\n                    count += 1\n            score.append(count / len(predictions))\n        return score\n\n\n    def run(self, X, y, model_name):\n        """"""\n        Returns the predictions from the pipeline including our NLTKPreprocessor and Keras classifier.\n        """"""\n        def build(classifier):\n            """"""\n            Inner build function that builds a pipeline including a preprocessor and a classifier.\n            """"""\n            model = Pipeline([\n                (\'preprocessor\', self.NLTKPreprocessor),\n                (\'features\', FeatureUnion([\n                    (\'vectorizer\', TfidfVectorizer(tokenizer=self.identity, preprocessor=None, lowercase=False)),\n                    (\'text_length\', self.pipelinize_feature(self.get_text_length, active=True))\n                ])),\n                (\'classifier\', classifier)\n            ])\n            return model\n\n        save_path = ""Models/""\n        with open(save_path + model_name, \'rb\') as f:\n            model = dill.load(f)\n        y_pred = model.predict(X)\n        y_pred_classes = [[0 if el < 0.2 else 1 for el in item] for item in y_pred]\n        print(self.multiclass_accuracy(y.values.tolist(), y_pred_classes))\n\n        return y_pred'"
02-Text/Python/train.py,0,"b'from gensim.models import KeyedVectors\nfrom gensim.models import word2vec\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport datetime\nfrom operator import itemgetter\nfrom random import randint\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport wget\n\nimport os\nimport time\nimport string\nimport dill\nimport pickle\n\nfrom nltk import *\nfrom nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\nfrom nltk.corpus import stopwords as sw, wordnet as wn\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, FunctionTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import precision_score, accuracy_score, confusion_matrix, classification_report as clsr\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\nfrom sklearn.model_selection import GridSearchCV, train_test_split as tts\nfrom sklearn.manifold import TSNE\nfrom sklearn.multiclass import OneVsRestClassifier\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, LSTM, SpatialDropout1D, Activation, Conv1D, MaxPooling1D, Input, concatenate\nfrom keras.utils.np_utils import to_categorical\n\n\nclass train:\n\n    def __init__(self, corpus):\n        self.max_sentence_len = 300\n        self.max_features = 300\n        self.embed_dim = 300\n        self.lstm_out = 180\n        self.dropout_lstm = 0.3\n        self.recurrent_dropout_lstm = 0.3\n        self.dropout = 0.3\n        self.conv_nfilters = 128\n        self.conv_kernel_size = 8\n        self.max_pool_size = 2\n        self.NLTKPreprocessor = self.NLTKPreprocessor(corpus)\n        #self.MyRNNTransformer = self.MyRNNTransformer()\n\n\n    class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n        """"""\n        Transforms input data by using NLTK tokenization, POS tagging, lemmatization and vectorization.\n        """"""\n\n        def __init__(self, corpus, max_sentence_len = 300, stopwords=None, punct=None, lower=True, strip=True):\n            """"""\n            Instantiates the preprocessor.\n            """"""\n            self.lower = lower\n            self.strip = strip\n            self.stopwords = set(stopwords) if stopwords else set(sw.words(\'english\'))\n            self.punct = set(punct) if punct else set(string.punctuation)\n            self.lemmatizer = WordNetLemmatizer()\n            self.corpus = corpus\n            self.max_sentence_len = max_sentence_len\n\n        def fit(self, X, y=None):\n            """"""\n            Fit simply returns self.\n            """"""\n            return self\n\n        def inverse_transform(self, X):\n            """"""\n            No inverse transformation.\n            """"""\n            return X\n\n        def transform(self, X):\n            """"""\n            Actually runs the preprocessing on each document.\n            """"""\n            output = np.array([(self.tokenize(doc)) for doc in X])\n            return output\n\n        def tokenize(self, document):\n            """"""\n            Returns a normalized, lemmatized list of tokens from a document by\n            applying segmentation, tokenization, and part of speech tagging.\n            Uses the part of speech tags to look up the lemma in WordNet, and returns the lowercase\n            version of all the words, removing stopwords and punctuation.\n            """"""\n            lemmatized_tokens = []\n\n            # Clean the text\n            document = re.sub(r""[^A-Za-z0-9^,!.\\/\'+-=]"", "" "", document)\n            document = re.sub(r""what\'s"", ""what is "", document)\n            document = re.sub(r""\\\'s"", "" "", document)\n            document = re.sub(r""\\\'ve"", "" have "", document)\n            document = re.sub(r""can\'t"", ""cannot "", document)\n            document = re.sub(r""n\'t"", "" not "", document)\n            document = re.sub(r""i\'m"", ""i am "", document)\n            document = re.sub(r""\\\'re"", "" are "", document)\n            document = re.sub(r""\\\'d"", "" would "", document)\n            document = re.sub(r""\\\'ll"", "" will "", document)\n            document = re.sub(r""(\\d+)(k)"", r""\\g<1>000"", document)\n\n            # Break the document into sentences\n            for sent in sent_tokenize(document):\n\n                # Break the sentence into part of speech tagged tokens\n                for token, tag in pos_tag(wordpunct_tokenize(sent)):\n\n                    # Apply preprocessing to the token\n                    token = token.lower() if self.lower else token\n                    token = token.strip() if self.strip else token\n                    token = token.strip(\'_\') if self.strip else token\n                    token = token.strip(\'*\') if self.strip else token\n\n                    # If punctuation or stopword, ignore token and continue\n                    if token in self.stopwords or all(char in self.punct for char in token):\n                        continue\n\n                    # Lemmatize the token\n                    lemma = self.lemmatize(token, tag)\n                    lemmatized_tokens.append(lemma)\n\n            doc = \' \'.join(lemmatized_tokens)\n            tokenized_document = self.vectorize(np.array(doc)[np.newaxis])\n            return tokenized_document\n\n\n        def vectorize(self, doc):\n            """"""\n            Returns a vectorized padded version of sequences.\n            """"""\n            save_path = ""Data/padding.pickle""\n            with open(save_path, \'rb\') as f:\n                tokenizer = pickle.load(f)\n            doc_pad = tokenizer.texts_to_sequences(doc)\n            doc_pad = pad_sequences(doc_pad, padding=\'pre\', truncating=\'pre\', maxlen=self.max_sentence_len)\n            return np.squeeze(doc_pad)\n\n        def lemmatize(self, token, tag):\n            """"""\n            Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n            tag to perform WordNet lemmatization.\n            """"""\n            tag = {\n                \'N\': wn.NOUN,\n                \'V\': wn.VERB,\n                \'R\': wn.ADV,\n                \'J\': wn.ADJ\n            }.get(tag[0], wn.NOUN)\n\n            return self.lemmatizer.lemmatize(token, tag)\n\n\n    class MyRNNTransformer(BaseEstimator, TransformerMixin):\n        """"""\n        Transformer allowing our Keras model to be included in our pipeline\n        """"""\n        def __init__(self, classifier):\n            self.classifier = classifier\n\n        def fit(self, X, y):\n            batch_size = 32\n            num_epochs = 135\n            batch_size = batch_size\n            epochs = num_epochs\n            self.classifier.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=2)\n            return self\n\n        def transform(self, X):\n            self.pred = self.classifier.predict(X)\n            self.classes = [[0 if el < 0.2 else 1 for el in item] for item in self.pred]\n            return self.classes\n\n\n    def multiclass_accuracy(self,predictions, target):\n        ""Returns the multiclass accuracy of the classifier\'s predictions""\n        score = []\n        for j in range(0, 5):\n            count = 0\n            for i in range(len(predictions)):\n                if predictions[i][j] == target[i][j]:\n                    count += 1\n            score.append(count / len(predictions))\n        return score\n\n\n    def load_google_vec(self):\n        url = \'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\'\n        #wget.download(url, \'Data/GoogleNews-vectors.bin.gz\')\n        return KeyedVectors.load_word2vec_format(\n            \'Data/GoogleNews-vectors.bin.gz\',\n            binary=True)\n\n    def lemmatize_token(self, token, tag):\n        tag = {\n            \'N\': wn.NOUN,\n            \'V\': wn.VERB,\n            \'R\': wn.ADV,\n            \'J\': wn.ADJ\n        }.get(tag[0], wn.NOUN)\n        return WordNetLemmatizer().lemmatize(token, tag)\n\n\n    def get_preprocessed_corpus(self, X_corpus):\n        """"""\n        Returns a preprocessed version of a full corpus (ie. tokenization and lemmatization using POS taggs)\n        """"""\n        X = \' \'.join(X_corpus)\n        lemmatized_tokens = []\n\n        # Break the document into sentences\n        for sent in sent_tokenize(X):\n\n            # Break the sentence into part of speech tagged tokens\n            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n\n                # Apply preprocessing to the token\n                token = token.lower()\n                token = token.strip()\n                token = token.strip(\'_\')\n                token = token.strip(\'*\')\n\n                # If punctuation or stopword, ignore token and continue\n                if token in set(sw.words(\'english\')) or all(char in set(string.punctuation) for char in token):\n                    continue\n\n                # Lemmatize the token and yield\n                lemma = self.lemmatize_token(token, tag)\n                lemmatized_tokens.append(lemma)\n\n        doc = \' \'.join(lemmatized_tokens)\n        return doc\n\n\n    def prepare_embedding(self, X):\n        """"""\n        Returns the embedding weights matrix, the word index, and the word-vector dictionnary corresponding\n        to the training corpus set of words.\n        """"""\n        # Load Word2Vec vectors\n        word2vec = self.load_google_vec()\n\n        # Fit and apply an NLTK tokenizer on the preprocessed training corpus to obtain sequences.\n        tokenizer = Tokenizer(num_words=self.max_features)\n        X_pad = self.get_preprocessed_corpus(X)\n        tokenizer.fit_on_texts(pd.Series(X_pad))\n        X_pad = tokenizer.texts_to_sequences(pd.Series(X_pad))\n\n        # Pad the sequences\n        X_pad = pad_sequences(X_pad, maxlen=self.max_sentence_len, padding=\'post\', truncating=\'post\')\n\n        # Retrieve the word index\n        train_word_index = tokenizer.word_index\n\n        # Construct the embedding weights matrix and word-vector dictionnary\n        train_embedding_weights = np.zeros((len(train_word_index) + 1, self.embed_dim))\n        for word, index in train_word_index.items():\n            train_embedding_weights[index, :] = word2vec[word] if word in word2vec else np.random.rand(self.embed_dim)\n        word_vector_dict = dict(zip(pd.Series(list(train_word_index.keys())),\n                                    pd.Series(list(train_word_index.keys())).apply(\n                                        lambda x: train_embedding_weights[train_word_index[x]])))\n        return train_embedding_weights, train_word_index, word_vector_dict\n\n\n    def run(self, X, y, model_name=None, pretrained_weights_path = None, pretrained_model_path = None, verbose=True):\n        """"""\n        Builds a classifer for the given list of documents and targets\n\n        """"""\n\n        def build(classifier, X, y, embedding_dict, corpus):\n            """"""\n            Inner build function that builds a pipeline including a preprocessor and a classifier.\n            """"""\n            model = Pipeline([\n                (\'preprocessor\', self.NLTKPreprocessor),\n                (\'classifier\', classifier)\n            ])\n            return model.fit(X, y)\n\n        # Label encode the targets\n        y_trans = y\n\n        # Prepare the embedding\n        train_embedding_weights, train_word_index, wv_dict = self.prepare_embedding(X)\n\n        # Begin evaluation\n        if verbose: print(""Building for evaluation"")\n        indices = range(len(y))\n\n        # Keras model definition\n        Input_words = Input(shape=(300,), name=\'input1\')\n        x = Embedding(len(train_word_index) + 1, self.embed_dim, weights=[train_embedding_weights],\n                      input_length=self.max_sentence_len, trainable=True)(Input_words)\n        # classifier.add(Embedding(30000, 300,input_length = 350))\n        x = Conv1D(filters=self.conv_nfilters, kernel_size= self.conv_kernel_size, padding=\'same\', activation=\'relu\')(x)\n        x = MaxPooling1D(pool_size=self.max_pool_size)(x)\n        x = SpatialDropout1D(self.dropout)(x)\n        x = BatchNormalization()(x)\n        x = Conv1D(filters=(self.conv_nfilters)*2, kernel_size= self.conv_kernel_size, padding=\'same\', activation=\'relu\')(x)\n        x = MaxPooling1D(pool_size=self.max_pool_size)(x)\n        x = SpatialDropout1D(self.dropout)(x)\n        x = BatchNormalization()(x)\n        x = Conv1D(filters=(self.conv_nfilters)*3, kernel_size= self.conv_kernel_size, padding=\'same\', activation=\'relu\')(x)\n        x = MaxPooling1D(pool_size=self.max_pool_size)(x)\n        x = SpatialDropout1D(self.dropout)(x)\n        x = BatchNormalization()(x)\n        x = LSTM(self.lstm_out, return_sequences=True, dropout=self.dropout_lstm, recurrent_dropout=self.recurrent_dropout_lstm)(x)\n        x = LSTM(self.lstm_out, return_sequences=True, dropout=self.dropout_lstm, recurrent_dropout=self.recurrent_dropout_lstm)(x)\n        x = LSTM(self.lstm_out, dropout=self.dropout_lstm, recurrent_dropout=self.recurrent_dropout_lstm)(x)\n        x = Dense(128, activation=\'softmax\')(x)\n        out = Dense(5, activation=\'softmax\')(x)\n        classifier = Model(inputs=Input_words, outputs=[out])\n        classifier.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n        print(classifier.summary())\n\n        # Loading pretrained model for transfer learning\n        if pretrained_weights_path and pretrained_model_path:\n            json_file = open(pretrained_model_path, \'r\')\n            classifier = model_from_json(json_file.read())\n            classifier.load_weights(pretrained_weights_path)\n            classifier.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n            json_file.close()\n            model = build(self.MyRNNTransformer(classifier), X, y_trans, wv_dict, corpus=X)\n            \n        # Train on the whole set from scratch\n        if verbose: \n            print(""Building complete model and saving ..."")\n            model= build(self.MyRNNTransformer(classifier), X, y_trans, wv_dict, corpus=X)\n\n        # Save the model\n        if model_name:\n            outpath = \'Models/\'\n            classifier.save_weights(outpath + model_name + \'.h5\')\n            with open(outpath + model_name + \'.json\', \'w\') as json_file:\n                json_file.write(classifier.to_json())\n            print(""Model written out to {}"".format(model_name))\n        else:\n            print(\'Please provide model name for saving\')\n        \n        return model\n\n'"
02-Text/Python/train_svm.py,0,"b'from gensim.models import KeyedVectors\nfrom gensim.models import word2vec\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport datetime\nfrom operator import itemgetter\nfrom random import randint\nimport seaborn as sns\nimport wget\n\nimport os\nimport time\nimport string\nimport dill\nimport pickle\n\nfrom nltk import *\nfrom nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\nfrom nltk.corpus import stopwords as sw, wordnet as wn\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, FunctionTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import precision_score, accuracy_score, confusion_matrix, classification_report as clsr\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\nfrom sklearn.model_selection import GridSearchCV, train_test_split as tts\nfrom sklearn.manifold import TSNE\nfrom sklearn.multiclass import OneVsRestClassifier\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, LSTM, SpatialDropout1D, Activation, Conv1D, MaxPooling1D, Input, concatenate\nfrom keras.utils.np_utils import to_categorical\n\nclass train_svm:\n\n    def __init__(self, corpus):\n        self.max_sentence_len = 300\n        self.max_features = 300\n        self.embed_dim = 300\n        self.NLTKPreprocessor = self.NLTKPreprocessor(corpus)\n        #self.MyRNNTransformer = self.MyRNNTransformer()\n\n\n    class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n        """"""\n        Transforms input data by using NLTK tokenization, POS tagging, lemmatization and vectorization.\n        """"""\n\n        def __init__(self, corpus, max_sentence_len = 300, stopwords=None, punct=None, lower=True, strip=True):\n            """"""\n            Instantiates the preprocessor.\n            """"""\n            self.lower = lower\n            self.strip = strip\n            self.stopwords = set(stopwords) if stopwords else set(sw.words(\'english\'))\n            self.punct = set(punct) if punct else set(string.punctuation)\n            self.lemmatizer = WordNetLemmatizer()\n            self.corpus = corpus\n            self.max_sentence_len = max_sentence_len\n\n        def fit(self, X, y=None):\n            """"""\n            Fit simply returns self.\n            """"""\n            return self\n\n        def inverse_transform(self, X):\n            """"""\n            No inverse transformation.\n            """"""\n            return X\n\n        def transform(self, X):\n            """"""\n            Actually runs the preprocessing on each document.\n            """"""\n            output = np.array([(self.tokenize(doc)) for doc in X])\n            return output\n\n        def tokenize(self, document):\n            """"""\n            Returns a normalized, lemmatized list of tokens from a document by\n            applying segmentation, tokenization, and part of speech tagging.\n            Uses the part of speech tags to look up the lemma in WordNet, and returns the lowercase\n            version of all the words, removing stopwords and punctuation.\n            """"""\n            lemmatized_tokens = []\n\n            # Clean the text\n            document = re.sub(r""[^A-Za-z0-9^,!.\\/\'+-=]"", "" "", document)\n            document = re.sub(r""what\'s"", ""what is "", document)\n            document = re.sub(r""\\\'s"", "" "", document)\n            document = re.sub(r""\\\'ve"", "" have "", document)\n            document = re.sub(r""can\'t"", ""cannot "", document)\n            document = re.sub(r""n\'t"", "" not "", document)\n            document = re.sub(r""i\'m"", ""i am "", document)\n            document = re.sub(r""\\\'re"", "" are "", document)\n            document = re.sub(r""\\\'d"", "" would "", document)\n            document = re.sub(r""\\\'ll"", "" will "", document)\n            document = re.sub(r""(\\d+)(k)"", r""\\g<1>000"", document)\n\n            # Break the document into sentences\n            for sent in sent_tokenize(document):\n\n                # Break the sentence into part of speech tagged tokens\n                for token, tag in pos_tag(wordpunct_tokenize(sent)):\n\n                    # Apply preprocessing to the token\n                    token = token.lower() if self.lower else token\n                    token = token.strip() if self.strip else token\n                    token = token.strip(\'_\') if self.strip else token\n                    token = token.strip(\'*\') if self.strip else token\n\n                    # If punctuation or stopword, ignore token and continue\n                    if token in self.stopwords or all(char in self.punct for char in token):\n                        continue\n\n                    # Lemmatize the token\n                    lemma = self.lemmatize(token, tag)\n                    lemmatized_tokens.append(lemma)\n\n            doc = \' \'.join(lemmatized_tokens)\n            tokenized_document = self.vectorize(np.array(doc)[np.newaxis])\n            return tokenized_document\n\n\n        def vectorize(self, doc):\n            """"""\n            Returns a vectorized padded version of sequences.\n            """"""\n            save_path = ""Data/padding.pickle""\n            with open(save_path, \'rb\') as f:\n                tokenizer = pickle.load(f)\n            doc_pad = tokenizer.texts_to_sequences(doc)\n            doc_pad = pad_sequences(doc_pad, padding=\'pre\', truncating=\'pre\', maxlen=self.max_sentence_len)\n            return np.squeeze(doc_pad)\n\n        def lemmatize(self, token, tag):\n            """"""\n            Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n            tag to perform WordNet lemmatization.\n            """"""\n            tag = {\n                \'N\': wn.NOUN,\n                \'V\': wn.VERB,\n                \'R\': wn.ADV,\n                \'J\': wn.ADJ\n            }.get(tag[0], wn.NOUN)\n\n            return self.lemmatizer.lemmatize(token, tag)\n\n\n    class MyRNNTransformer(BaseEstimator, TransformerMixin):\n        """"""\n        Transformer allowing our Keras model to be included in our pipeline\n        """"""\n        def __init__(self, classifier):\n            self.classifier = classifier\n\n        def fit(self, X, y):\n            batch_size = 32\n            num_epochs = 35\n            batch_size = batch_size\n            epochs = num_epochs\n            self.classifier.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=2)\n            return self\n\n        def transform(self, X):\n            self.pred = self.classifier.predict_proba(X)\n            self.classes = [[0 if el < 0.2 else 1 for el in item] for item in self.pred]\n            return self.classes\n\n\n    class TfidfEmbeddingVectorizer(object):\n        def __init__(self, word2vec):\n            self.word2vec = word2vec\n            self.word2weight = None\n            self.dim = len(word2vec.values())\n\n        def fit(self, X, y):\n            tfidf = TfidfVectorizer(analyzer=lambda x: x)\n            tfidf.fit(X)\n            # if a word was never seen - it must be at least as infrequent\n            # as any of the known words. So the default idf is the max of\n            # known idf\'s\n            max_idf = max(tfidf.idf_)\n            self.word2weight = defaultdict(\n                lambda: max_idf,\n                [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n\n            return self\n\n        def transform(self, X):\n            return np.array([\n                np.mean([self.word2vec[w] * self.word2weight[w]\n                         for w in words if w in self.word2vec] or\n                        [np.zeros(self.dim)], axis=0)\n                for words in X\n            ])\n\n\n    def identity(self, arg):\n        """"""\n        Simple identity function works as a passthrough.\n        """"""\n        return arg\n\n\n    def reshape_a_feature_column(self, series):\n        return np.reshape(np.asarray(series), (len(series), 1))\n\n\n    def pipelinize_feature(self, function, active=True):\n        def list_comprehend_a_function(list_or_series, active=True):\n            if active:\n                processed = [function(i) for i in list_or_series]\n                processed = self.reshape_a_feature_column(processed)\n                return processed\n            else:\n                return self.reshape_a_feature_column(np.zeros(len(list_or_series)))\n\n\n    def get_text_length(self, text):\n        return len(text)\n\n    def load_google_vec(self):\n        url = \'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\'\n        wget.download(url, \'Data/GoogleNews-vectors.bin.gz\')\n        return KeyedVectors.load_word2vec_format(\n            \'Data/GoogleNews-vectors.bin.gz\',\n            binary=True)\n\n\n    def lemmatize_token(self, token, tag):\n        tag = {\n            \'N\': wn.NOUN,\n            \'V\': wn.VERB,\n            \'R\': wn.ADV,\n            \'J\': wn.ADJ\n        }.get(tag[0], wn.NOUN)\n        return WordNetLemmatizer().lemmatize(token, tag)\n\n\n    def get_preprocessed_corpus(self, X_corpus):\n        """"""\n        Returns a preprocessed version of a full corpus (ie. tokenization and lemmatization using POS taggs)\n        """"""\n        X = \' \'.join(X_corpus)\n        lemmatized_tokens = []\n\n        # Break the document into sentences\n        for sent in sent_tokenize(X):\n\n            # Break the sentence into part of speech tagged tokens\n            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n\n                # Apply preprocessing to the token\n                token = token.lower()\n                token = token.strip()\n                token = token.strip(\'_\')\n                token = token.strip(\'*\')\n\n                # If punctuation or stopword, ignore token and continue\n                if token in set(sw.words(\'english\')) or all(char in set(string.punctuation) for char in token):\n                    continue\n\n                # Lemmatize the token and yield\n                lemma = self.lemmatize_token(token, tag)\n                lemmatized_tokens.append(lemma)\n\n        doc = \' \'.join(lemmatized_tokens)\n        return doc\n\n\n    def prepare_embedding(self, X):\n        """"""\n        Returns the embedding weights matrix, the word index, and the word-vector dictionnary corresponding\n        to the training corpus set of words.\n        """"""\n        # Load Word2Vec vectors\n        word2vec = self.load_google_vec()\n\n        # Fit and apply an NLTK tokenizer on the preprocessed training corpus to obtain sequences.\n        tokenizer = Tokenizer(num_words=self.max_features)\n        X_pad = self.get_preprocessed_corpus(X)\n        tokenizer.fit_on_texts(pd.Series(X_pad))\n        X_pad = tokenizer.texts_to_sequences(pd.Series(X_pad))\n\n        # Pad the sequences\n        X_pad = pad_sequences(X_pad, maxlen=self.max_sentence_len, padding=\'post\', truncating=\'post\')\n\n        # Retrieve the word index\n        train_word_index = tokenizer.word_index\n\n        # Construct the embedding weights matrix and word-vector dictionnary\n        train_embedding_weights = np.zeros((len(train_word_index) + 1, self.embed_dim))\n        for word, index in train_word_index.items():\n            train_embedding_weights[index, :] = word2vec[word] if word in word2vec else np.random.rand(self.embed_dim)\n        word_vector_dict = dict(zip(pd.Series(list(train_word_index.keys())),\n                                    pd.Series(list(train_word_index.keys())).apply(\n                                        lambda x: train_embedding_weights[train_word_index[x]])))\n        return train_embedding_weights, train_word_index, word_vector_dict\n\n\n\n    def multiclass_accuracy(self,predictions, target):\n        ""Returns the multiclass accuracy of the classifier\'s predictions""\n        score = []\n        for j in range(0, 5):\n            count = 0\n            for i in range(len(predictions)):\n                if predictions[i][j] == target[i][j]:\n                    count += 1\n            score.append(count / len(predictions))\n        return score\n\n\n    def run(self,X, y, classifier=SGDClassifier, model_name=None,\n                           verbose=True):\n        """"""\n        Builds a classifer for the given list of documents and targets\n        """"""\n        def build(classifier, X, y, embedding_dict, corpus):\n            """"""\n            Inner build function that builds a single model.\n            """"""\n            classifier = OneVsRestClassifier(classifier(), n_jobs=-1)\n            model = Pipeline([\n                (\'preprocessor\', self.NLTKPreprocessor),\n                (""wordVectz"", self.TfidfEmbeddingVectorizer(embedding_dict)),\n                (\'clf\', classifier)\n            ])\n            return model.fit(X, y)\n\n        y_trans = y\n\n        # Prepare the embedding\n        train_embedding_weights, train_word_index, wv_dict = self.prepare_embedding(X)\n\n        # Begin evaluation\n        if verbose: print(""Building complete model and saving ..."")\n        model = build(classifier, X, y_trans, wv_dict, corpus=X)\n        \n        # Save the model\n        if model_name:\n            outpath = \'Models/\'\n            with open(outpath + model_name, \'wb\') as f:\n                dill.dump(model, f)\n            print(""Model written out to {}"".format(model_name))\n\n        return model\n'"
02-Text/Python/visualize.py,0,"b'from Text.Python.load_data import *\nfrom Text.Python.train import *\n\nfrom nltk.corpus import movie_reviews as reviews\nfrom sklearn.datasets import fetch_20newsgroups\nfrom gensim.models import KeyedVectors\nfrom gensim.models import word2vec\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport datetime\nfrom operator import itemgetter\nfrom random import randint\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n\nimport os\nimport time\nimport string\nimport pickle\n\nfrom nltk import *\nfrom nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\nfrom nltk.corpus import stopwords as sw, wordnet as wn\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, FunctionTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import precision_score, accuracy_score, confusion_matrix, classification_report as clsr\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\nfrom sklearn.model_selection import GridSearchCV, train_test_split as tts\nfrom sklearn.manifold import TSNE\nfrom sklearn.multiclass import OneVsRestClassifier\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, LSTM, SpatialDropout1D, Activation, Conv1D, MaxPooling1D, Input, concatenate\nfrom keras.utils.np_utils import to_categorical\n\nclass visualize:\n\n    def __init__(self, complete_dataset, X, labels_list):\n        self.data = complete_dataset\n        self.X = X\n        self.labels_list = labels_list\n\n    def textlength_vs_labels_histogram(self):\n        # Visualization of histograms of text length vs. label\n        for label in self.labels_list:\n            g = sns.FacetGrid(data=self.data, col=label)\n            g.map(plt.hist, \'text length\', bins=50)\n        plt.show()\n\n    def textlength_vs_labels_boxplot(self):\n        # Visualization of boxplots of text length vs. label\n        for i, label in enumerate(self.labels_list):\n            plt.figure(i)\n            sns.boxplot(x=label, y=\'text length\', data=self.data)\n        plt.show()\n\n    def most_frequent_words(self):\n        # Visualization of the most frequent words\n        complete_corpus = \' \'.join(self.X)\n        words = tokenize.word_tokenize(complete_corpus)\n        fdist = FreqDist(words)\n        print(""List of 100 most frequent words/counts"")\n        print(fdist.most_common(100))\n        fdist.plot(40)\n\n    def most_frequent_words_preprocessed(self):\n        # Visualization of the most frequent words\n        if not hasattr(self, \'X_preprocess\'):\n            preprocessor = train(corpus = self.X).NLTKPreprocessor\n            self.X_preprocess = prep.transform(self.X).tolist()\n        complete_corpus = \' \'.join(self.X_preprocess)\n        words = tokenize.word_tokenize(complete_corpus)\n        fdist = FreqDist(words)\n        print(""List of 100 most frequent words/counts"")\n        print(fdist.most_common(100))\n        fdist.plot(40)\n\n    def get_corpus_statistics(self):\n        # Retrieve some info on the text data\n        numWords = []\n        for text in self.X:\n                counter = len(text.split())\n                numWords.append(counter)  \n        numFiles = len(numWords)\n        print(\'The total number of essays is\', numFiles)\n        print(\'The total number of words in all essays is\', sum(numWords))\n        print(\'The average number of words in each essay is\', sum(numWords)/len(numWords))\n\n    def get_preprocessed_corpus_statistics(self):\n        # Retrieve some info on the preprocessed text data\n        if not hasattr(self, \'X_preprocess\'):\n            preprocessor = train(corpus = self.X).NLTKPreprocessor\n            self.X_preprocess = prep.transform(self.X).tolist()\n        len_list = [np.count_nonzero(self.X_preprocess[i]) for i in range(len(self.X))]\n        print(\'The average number of words in each preprocessed essay is\', np.mean(len_list))\n        print(\'The standard deviation of the number of words in each preprocessed essay is\', np.std(len_list))\n        print(\'The average number of words in each preprocessed essay plus 2 standard deviations is\', np.mean(len_list) + 2 * np.std(len_list))\n\nclass tsne:\n    \n    def __init__(self, X, max_features = 30000, max_sentence_len = 300, embed_dim = 300,  n_elements = 100):\n        self.X = X\n        self.max_features =max_features\n        self.max_sentence_len = max_sentence_len\n        self.embed_dim = embed_dim\n        self.n_elements = n_elements\n        self.vectors, self.words, self.dic =  self.prepare_embedding(self.X)\n\n    def load_google_vec(self):\n        url = \'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\'\n        #wget.download(url, \'Data/GoogleNews-vectors.bin.gz\')\n        return KeyedVectors.load_word2vec_format(\n            \'Data/GoogleNews-vectors.bin.gz\',\n            binary=True)\n\n    def lemmatize_token(self, token, tag):\n        tag = {\n            \'N\': wn.NOUN,\n            \'V\': wn.VERB,\n            \'R\': wn.ADV,\n            \'J\': wn.ADJ\n        }.get(tag[0], wn.NOUN)\n        return WordNetLemmatizer().lemmatize(token, tag)\n\n\n    def get_preprocessed_corpus(self, X_corpus):\n        """"""\n        Returns a preprocessed version of a full corpus (ie. tokenization and lemmatization using POS taggs)\n        """"""\n        X = \' \'.join(X_corpus)\n        lemmatized_tokens = []\n\n        # Break the document into sentences\n        for sent in sent_tokenize(X):\n\n            # Break the sentence into part of speech tagged tokens\n            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n\n                # Apply preprocessing to the token\n                token = token.lower()\n                token = token.strip()\n                token = token.strip(\'_\')\n                token = token.strip(\'*\')\n\n                # If punctuation or stopword, ignore token and continue\n                if token in set(sw.words(\'english\')) or all(char in set(string.punctuation) for char in token):\n                    continue\n\n                # Lemmatize the token and yield\n                lemma = self.lemmatize_token(token, tag)\n                lemmatized_tokens.append(lemma)\n\n        doc = \' \'.join(lemmatized_tokens)\n        return doc\n\n\n    def prepare_embedding(self, X):\n        """"""\n        Returns the embedding weights matrix, the word index, and the word-vector dictionnary corresponding\n        to the training corpus set of words.\n        """"""\n        # Load Word2Vec vectors\n        word2vec = self.load_google_vec()\n\n        # Fit and apply an NLTK tokenizer on the preprocessed training corpus to obtain sequences.\n        tokenizer = Tokenizer(num_words=self.max_features)\n        X_pad = self.get_preprocessed_corpus(X)\n        tokenizer.fit_on_texts(pd.Series(X_pad))\n        X_pad = tokenizer.texts_to_sequences(pd.Series(X_pad))\n\n        # Pad the sequences\n        X_pad = pad_sequences(X_pad, maxlen=self.max_sentence_len, padding=\'post\', truncating=\'post\')\n\n        # Retrieve the word index\n        train_word_index = tokenizer.word_index\n\n        # Construct the embedding weights matrix and word-vector dictionnary\n        train_embedding_weights = np.zeros((len(train_word_index) + 1, self.embed_dim))\n        for word, index in train_word_index.items():\n            train_embedding_weights[index, :] = word2vec[word] if word in word2vec else np.random.rand(self.embed_dim)\n        word_vector_dict = dict(zip(pd.Series(list(train_word_index.keys())),\n                                    pd.Series(list(train_word_index.keys())).apply(\n                                        lambda x: train_embedding_weights[train_word_index[x]])))\n        return train_embedding_weights, train_word_index, word_vector_dict\n\n\n    def plot(self):\n        labels = []\n        tokens = []\n\n        l_bound = 0\n        u_bound = len(self.words)\n        step = int(len(self.words)/self.n_elements)\n\n        #for index in range(l_bound,u_bound, step):\n        for index in random.sample(range(l_bound,u_bound), self.n_elements):\n            tokens.append(self.vectors[index])\n            labels.append(self.words[index])\n\n        tsne_model = TSNE(perplexity=40, n_components=2, init=\'pca\', n_iter=2500, random_state=23)\n        new_values = tsne_model.fit_transform(tokens)\n\n        xx = []\n        yy = []\n        for value in new_values:\n            xx.append(value[0])\n            yy.append(value[1])\n\n        plt.figure(figsize=(16, 16))\n        for i in range(len(xx)):\n            plt.scatter(xx[i],yy[i])\n            plt.annotate(labels[i],\n                         xy=(xx[i], yy[i]),\n                         xytext=(5, 2),\n                         textcoords=\'offset points\',\n                         ha=\'right\',\n                         va=\'bottom\')\n        plt.show()\n\n\nclass NLTKPreprocessor(BaseEstimator, TransformerMixin):\n    """"""\n    Transforms input data by using NLTK tokenization, POS tagging, lemmatization and vectorization.\n    """"""\n\n    def __init__(self, corpus, max_sentence_len = 300, stopwords=None, punct=None, lower=True, strip=True):\n        """"""\n        Instantiates the preprocessor.\n        """"""\n        self.lower = lower\n        self.strip = strip\n        self.stopwords = set(stopwords) if stopwords else set(sw.words(\'english\'))\n        self.punct = set(punct) if punct else set(string.punctuation)\n        self.lemmatizer = WordNetLemmatizer()\n        self.corpus = corpus\n        self.max_sentence_len = max_sentence_len\n\n    def fit(self, X, y=None):\n        """"""\n        Fit simply returns self.\n        """"""\n        return self\n\n    def inverse_transform(self, X):\n        """"""\n        No inverse transformation.\n        """"""\n        return X\n\n    def transform(self, X):\n        """"""\n        Actually runs the preprocessing on each document.\n        """"""\n        output = np.array([(self.tokenize(doc)) for doc in X])\n        return output\n\n    def tokenize(self, document):\n        """"""\n        Returns a normalized, lemmatized list of tokens from a document by\n        applying segmentation, tokenization, and part of speech tagging.\n        Uses the part of speech tags to look up the lemma in WordNet, and returns the lowercase\n        version of all the words, removing stopwords and punctuation.\n        """"""\n        lemmatized_tokens = []\n\n        # Clean the text\n        document = re.sub(r""[^A-Za-z0-9^,!.\\/\'+-=]"", "" "", document)\n        document = re.sub(r""what\'s"", ""what is "", document)\n        document = re.sub(r""\\\'s"", "" "", document)\n        document = re.sub(r""\\\'ve"", "" have "", document)\n        document = re.sub(r""can\'t"", ""cannot "", document)\n        document = re.sub(r""n\'t"", "" not "", document)\n        document = re.sub(r""i\'m"", ""i am "", document)\n        document = re.sub(r""\\\'re"", "" are "", document)\n        document = re.sub(r""\\\'d"", "" would "", document)\n        document = re.sub(r""\\\'ll"", "" will "", document)\n        document = re.sub(r""(\\d+)(k)"", r""\\g<1>000"", document)\n\n        # Break the document into sentences\n        for sent in sent_tokenize(document):\n\n            # Break the sentence into part of speech tagged tokens\n            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n\n                # Apply preprocessing to the token\n                token = token.lower() if self.lower else token\n                token = token.strip() if self.strip else token\n                token = token.strip(\'_\') if self.strip else token\n                token = token.strip(\'*\') if self.strip else token\n\n                # If punctuation or stopword, ignore token and continue\n                if token in self.stopwords or all(char in self.punct for char in token):\n                    continue\n\n                # Lemmatize the token\n                lemma = self.lemmatize(token, tag)\n                lemmatized_tokens.append(lemma)\n\n        doc = \' \'.join(lemmatized_tokens)\n        tokenized_document = self.vectorize(np.array(doc)[np.newaxis])\n        return tokenized_document\n\n\n    def vectorize(self, doc):\n        """"""\n        Returns a vectorized padded version of sequences.\n        """"""\n        save_path = ""Data/padding.pickle""\n        with open(save_path, \'rb\') as f:\n            tokenizer = pickle.load(f)\n        doc_pad = tokenizer.texts_to_sequences(doc)\n        doc_pad = pad_sequences(doc_pad, padding=\'pre\', truncating=\'pre\', maxlen=self.max_sentence_len)\n        return np.squeeze(doc_pad)\n\n    def lemmatize(self, token, tag):\n        """"""\n        Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n        tag to perform WordNet lemmatization.\n        """"""\n        tag = {\n            \'N\': wn.NOUN,\n            \'V\': wn.VERB,\n            \'R\': wn.ADV,\n            \'J\': wn.ADJ\n        }.get(tag[0], wn.NOUN)\n\n        return self.lemmatizer.lemmatize(token, tag)\n'"
03-Video/Python/live_face.py,0,"b'### General imports ###\nfrom __future__ import division\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nfrom time import time\nfrom time import sleep\nimport re\nimport os\n\nimport argparse\nfrom collections import OrderedDict\n\n### Image processing ###\nfrom scipy.ndimage import zoom\nfrom scipy.spatial import distance\nimport imutils\nfrom scipy import ndimage\n\nimport dlib\n\nfrom tensorflow.keras.models import load_model\nfrom imutils import face_utils\n\nimport requests\n\nglobal shape_x\nglobal shape_y\nglobal input_shape\nglobal nClasses\n\ndef show_webcam() :\n\n    shape_x = 48\n    shape_y = 48\n    input_shape = (shape_x, shape_y, 1)\n    nClasses = 7\n\n    thresh = 0.25\n    frame_check = 20\n\n    def eye_aspect_ratio(eye):\n        A = distance.euclidean(eye[1], eye[5])\n        B = distance.euclidean(eye[2], eye[4])\n        C = distance.euclidean(eye[0], eye[3])\n        ear = (A + B) / (2.0 * C)\n        return ear\n\n    def detect_face(frame):\n        \n        #Cascade classifier pre-trained model\n        cascPath = \'Models/face_landmarks.dat\'\n        faceCascade = cv2.CascadeClassifier(cascPath)\n        \n        #BGR -> Gray conversion\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        \n        #Cascade MultiScale classifier\n        detected_faces = faceCascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=6,\n                                                      minSize=(shape_x, shape_y),\n                                                      flags=cv2.CASCADE_SCALE_IMAGE)\n        coord = []\n                                                      \n        for x, y, w, h in detected_faces :\n            if w > 100 :\n                sub_img=frame[y:y+h,x:x+w]\n                cv2.rectangle(frame,(x,y),(x+w,y+h),(0, 255,255),1)\n                coord.append([x,y,w,h])\n\n        return gray, detected_faces, coord\n\n    def extract_face_features(faces, offset_coefficients=(0.075, 0.05)):\n        gray = faces[0]\n        detected_face = faces[1]\n        \n        new_face = []\n        \n        for det in detected_face :\n            #Region dans laquelle la face est d\xc3\xa9tect\xc3\xa9e\n            x, y, w, h = det\n            #X et y correspondent \xc3\xa0 la conversion en gris par gray, et w, h correspondent \xc3\xa0 la hauteur/largeur\n            \n            #Offset coefficient, np.floor takes the lowest integer (delete border of the image)\n            horizontal_offset = np.int(np.floor(offset_coefficients[0] * w))\n            vertical_offset = np.int(np.floor(offset_coefficients[1] * h))\n            \n            #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            #gray transforme l\'image\n            extracted_face = gray[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n            \n            #Zoom sur la face extraite\n            new_extracted_face = zoom(extracted_face, (shape_x / extracted_face.shape[0],shape_y / extracted_face.shape[1]))\n            #cast type float\n            new_extracted_face = new_extracted_face.astype(np.float32)\n            #scale\n            new_extracted_face /= float(new_extracted_face.max())\n            #print(new_extracted_face)\n            \n            new_face.append(new_extracted_face)\n        \n        return new_face\n\n\n    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""left_eye""]\n    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""right_eye""]\n    \n    (nStart, nEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""nose""]\n    (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""mouth""]\n    (jStart, jEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""jaw""]\n\n    (eblStart, eblEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""left_eyebrow""]\n    (ebrStart, ebrEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""right_eyebrow""]\n\n    model = load_model(\'Models/video.h5\')\n    face_detect = dlib.get_frontal_face_detector()\n    predictor_landmarks  = dlib.shape_predictor(""Models/face_landmarks.dat"")\n    \n    #Lancer la capture video\n    video_capture = cv2.VideoCapture(0)\n\n    while True:\n        # Capture frame-by-frame\n        ret, frame = video_capture.read()\n        \n        face_index = 0\n        \n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        rects = face_detect(gray, 1)\n        #gray, detected_faces, coord = detect_face(frame)\n\n        for (i, rect) in enumerate(rects):\n\n            shape = predictor_landmarks(gray, rect)\n            shape = face_utils.shape_to_np(shape)\n            \n            # Identify face coordinates\n            (x, y, w, h) = face_utils.rect_to_bb(rect)\n            face = gray[y:y+h,x:x+w]\n            \n            #Zoom on extracted face\n            face = zoom(face, (shape_x / face.shape[0],shape_y / face.shape[1]))\n            \n            #Cast type float\n            face = face.astype(np.float32)\n            \n            #Scale\n            face /= float(face.max())\n            face = np.reshape(face.flatten(), (1, 48, 48, 1))\n            \n            #Make Prediction\n            prediction = model.predict(face)\n            prediction_result = np.argmax(prediction)\n            \n            # Rectangle around the face\n            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        \n            cv2.putText(frame, ""Face #{}"".format(i + 1), (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n     \n            for (j, k) in shape:\n                cv2.circle(frame, (j, k), 1, (0, 0, 255), -1)\n            \n            # 1. Add prediction probabilities\n            cv2.putText(frame, ""----------------"",(40,100 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n            cv2.putText(frame, ""Emotional report : Face #"" + str(i+1),(40,120 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n            cv2.putText(frame, ""Angry : "" + str(round(prediction[0][0],3)),(40,140 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n            cv2.putText(frame, ""Disgust : "" + str(round(prediction[0][1],3)),(40,160 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n            cv2.putText(frame, ""Fear : "" + str(round(prediction[0][2],3)),(40,180 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n            cv2.putText(frame, ""Happy : "" + str(round(prediction[0][3],3)),(40,200 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n            cv2.putText(frame, ""Sad : "" + str(round(prediction[0][4],3)),(40,220 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n            cv2.putText(frame, ""Surprise : "" + str(round(prediction[0][5],3)),(40,240 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n            cv2.putText(frame, ""Neutral : "" + str(round(prediction[0][6],3)),(40,260 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n            \n            # 2. Annotate main image with a label\n            if prediction_result == 0 :\n                cv2.putText(frame, ""Angry"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            elif prediction_result == 1 :\n                cv2.putText(frame, ""Disgust"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            elif prediction_result == 2 :\n                cv2.putText(frame, ""Fear"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            elif prediction_result == 3 :\n                cv2.putText(frame, ""Happy"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            elif prediction_result == 4 :\n                cv2.putText(frame, ""Sad"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            elif prediction_result == 5 :\n                cv2.putText(frame, ""Surprise"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            else :\n                cv2.putText(frame, ""Neutral"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            \n            # 3. Eye Detection and Blink Count\n            leftEye = shape[lStart:lEnd]\n            rightEye = shape[rStart:rEnd]\n            \n            # Compute Eye Aspect Ratio\n            leftEAR = eye_aspect_ratio(leftEye)\n            rightEAR = eye_aspect_ratio(rightEye)\n            ear = (leftEAR + rightEAR) / 2.0\n            \n            # And plot its contours\n            leftEyeHull = cv2.convexHull(leftEye)\n            rightEyeHull = cv2.convexHull(rightEye)\n            cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n            cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n            \n            # 4. Detect Nose\n            nose = shape[nStart:nEnd]\n            noseHull = cv2.convexHull(nose)\n            cv2.drawContours(frame, [noseHull], -1, (0, 255, 0), 1)\n\n            # 5. Detect Mouth\n            mouth = shape[mStart:mEnd]\n            mouthHull = cv2.convexHull(mouth)\n            cv2.drawContours(frame, [mouthHull], -1, (0, 255, 0), 1)\n            \n            # 6. Detect Jaw\n            jaw = shape[jStart:jEnd]\n            jawHull = cv2.convexHull(jaw)\n            cv2.drawContours(frame, [jawHull], -1, (0, 255, 0), 1)\n            \n            # 7. Detect Eyebrows\n            ebr = shape[ebrStart:ebrEnd]\n            ebrHull = cv2.convexHull(ebr)\n            cv2.drawContours(frame, [ebrHull], -1, (0, 255, 0), 1)\n            ebl = shape[eblStart:eblEnd]\n            eblHull = cv2.convexHull(ebl)\n            cv2.drawContours(frame, [eblHull], -1, (0, 255, 0), 1)\n        \n        cv2.putText(frame,\'Number of Faces : \' + str(len(rects)),(40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, 155, 1)\n        cv2.imshow(\'Video\', frame)\n        \n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n\n    # When everything is done, release the capture\n    video_capture.release()\n    cv2.destroyAllWindows()\n\ndef main():\n    show_webcam()\n\nif __name__ == ""__main__"":\n    main()\n'"
04-WebApp/library/speech_emotion_recognition.py,0,"b'## Basics ##\nimport time\nimport os\nimport numpy as np\n\n## Audio Preprocessing ##\nimport pyaudio\nimport wave\nimport librosa\nfrom scipy.stats import zscore\n\n## Time Distributed CNN ##\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Activation, TimeDistributed\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten\nfrom tensorflow.keras.layers import LSTM\n\n\n\'\'\'\nSpeech Emotion Recognition\n\'\'\'\nclass speechEmotionRecognition:\n\n    \'\'\'\n    Voice recording function\n    \'\'\'\n    def __init__(self, subdir_model=None):\n\n        # Load prediction model\n        if subdir_model is not None:\n            self._model = self.build_model()\n            self._model.load_weights(subdir_model)\n\n        # Emotion encoding\n        self._emotion = {0:\'Angry\', 1:\'Disgust\', 2:\'Fear\', 3:\'Happy\', 4:\'Neutral\', 5:\'Sad\', 6:\'Surprise\'}\n\n\n    \'\'\'\n    Voice recording function\n    \'\'\'\n    def voice_recording(self, filename, duration=5, sample_rate=16000, chunk=1024, channels=1):\n\n        # Start the audio recording stream\n        p = pyaudio.PyAudio()\n        stream = p.open(format=pyaudio.paInt16,\n                        channels=channels,\n                        rate=sample_rate,\n                        input=True,\n                        frames_per_buffer=chunk)\n\n        # Create an empty list to store audio recording\n        frames = []\n\n        # Determine the timestamp of the start of the response interval\n        print(\'* Start Recording *\')\n        stream.start_stream()\n        start_time = time.time()\n        current_time = time.time()\n\n        # Record audio until timeout\n        while (current_time - start_time) < duration:\n\n            # Record data audio data\n            data = stream.read(chunk)\n\n            # Add the data to a buffer (a list of chunks)\n            frames.append(data)\n\n            # Get new timestamp\n            current_time = time.time()\n\n        # Close the audio recording stream\n        stream.stop_stream()\n        stream.close()\n        p.terminate()\n        print(\'* End Recording * \')\n\n        # Export audio recording to wav format\n        wf = wave.open(filename, \'w\')\n        wf.setnchannels(channels)\n        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n        wf.setframerate(sample_rate)\n        wf.writeframes(b\'\'.join(frames))\n        wf.close()\n\n\n    \'\'\'\n    Mel-spectogram computation\n    \'\'\'\n    def mel_spectrogram(self, y, sr=16000, n_fft=512, win_length=256, hop_length=128, window=\'hamming\', n_mels=128, fmax=4000):\n\n        # Compute spectogram\n        mel_spect = np.abs(librosa.stft(y, n_fft=n_fft, window=window, win_length=win_length, hop_length=hop_length)) ** 2\n\n        # Compute mel spectrogram\n        mel_spect = librosa.feature.melspectrogram(S=mel_spect, sr=sr, n_mels=n_mels, fmax=fmax)\n\n        # Compute log-mel spectrogram\n        mel_spect = librosa.power_to_db(mel_spect, ref=np.max)\n\n        return np.asarray(mel_spect)\n\n\n    \'\'\'\n    Audio framing\n    \'\'\'\n    def frame(self, y, win_step=64, win_size=128):\n\n        # Number of frames\n        nb_frames = 1 + int((y.shape[2] - win_size) / win_step)\n\n        # Framming\n        frames = np.zeros((y.shape[0], nb_frames, y.shape[1], win_size)).astype(np.float16)\n        for t in range(nb_frames):\n            frames[:,t,:,:] = np.copy(y[:,:,(t * win_step):(t * win_step + win_size)]).astype(np.float16)\n\n        return frames\n\n\n    \'\'\'\n    Time distributed Convolutional Neural Network model\n    \'\'\'\n    def build_model(self):\n\n        # Clear Keras session\n        K.clear_session()\n\n        # Define input\n        input_y = Input(shape=(5, 128, 128, 1), name=\'Input_MELSPECT\')\n\n        # First LFLB (local feature learning block)\n        y = TimeDistributed(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding=\'same\'), name=\'Conv_1_MELSPECT\')(input_y)\n        y = TimeDistributed(BatchNormalization(), name=\'BatchNorm_1_MELSPECT\')(y)\n        y = TimeDistributed(Activation(\'elu\'), name=\'Activ_1_MELSPECT\')(y)\n        y = TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\'same\'), name=\'MaxPool_1_MELSPECT\')(y)\n        y = TimeDistributed(Dropout(0.2), name=\'Drop_1_MELSPECT\')(y)\n\n        # Second LFLB (local feature learning block)\n        y = TimeDistributed(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding=\'same\'), name=\'Conv_2_MELSPECT\')(y)\n        y = TimeDistributed(BatchNormalization(), name=\'BatchNorm_2_MELSPECT\')(y)\n        y = TimeDistributed(Activation(\'elu\'), name=\'Activ_2_MELSPECT\')(y)\n        y = TimeDistributed(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding=\'same\'), name=\'MaxPool_2_MELSPECT\')(y)\n        y = TimeDistributed(Dropout(0.2), name=\'Drop_2_MELSPECT\')(y)\n\n        # Third LFLB (local feature learning block)\n        y = TimeDistributed(Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding=\'same\'), name=\'Conv_3_MELSPECT\')(y)\n        y = TimeDistributed(BatchNormalization(), name=\'BatchNorm_3_MELSPECT\')(y)\n        y = TimeDistributed(Activation(\'elu\'), name=\'Activ_3_MELSPECT\')(y)\n        y = TimeDistributed(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding=\'same\'), name=\'MaxPool_3_MELSPECT\')(y)\n        y = TimeDistributed(Dropout(0.2), name=\'Drop_3_MELSPECT\')(y)\n\n        # Fourth LFLB (local feature learning block)\n        y = TimeDistributed(Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding=\'same\'), name=\'Conv_4_MELSPECT\')(y)\n        y = TimeDistributed(BatchNormalization(), name=\'BatchNorm_4_MELSPECT\')(y)\n        y = TimeDistributed(Activation(\'elu\'), name=\'Activ_4_MELSPECT\')(y)\n        y = TimeDistributed(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding=\'same\'), name=\'MaxPool_4_MELSPECT\')(y)\n        y = TimeDistributed(Dropout(0.2), name=\'Drop_4_MELSPECT\')(y)\n\n        # Flat\n        y = TimeDistributed(Flatten(), name=\'Flat_MELSPECT\')(y)\n\n        # LSTM layer\n        y = LSTM(256, return_sequences=False, dropout=0.2, name=\'LSTM_1\')(y)\n\n        # Fully connected\n        y = Dense(7, activation=\'softmax\', name=\'FC\')(y)\n\n        # Build final model\n        model = Model(inputs=input_y, outputs=y)\n\n        return model\n\n\n    \'\'\'\n    Predict speech emotion over time from an audio file\n    \'\'\'\n    def predict_emotion_from_file(self, filename, chunk_step=16000, chunk_size=49100, predict_proba=False, sample_rate=16000):\n\n        # Read audio file\n        y, sr = librosa.core.load(filename, sr=sample_rate, offset=0.5)\n\n        # Split audio signals into chunks\n        chunks = self.frame(y.reshape(1, 1, -1), chunk_step, chunk_size)\n\n        # Reshape chunks\n        chunks = chunks.reshape(chunks.shape[1],chunks.shape[-1])\n\n        # Z-normalization\n        y = np.asarray(list(map(zscore, chunks)))\n\n        # Compute mel spectrogram\n        mel_spect = np.asarray(list(map(self.mel_spectrogram, y)))\n\n        # Time distributed Framing\n        mel_spect_ts = self.frame(mel_spect)\n\n        # Build X for time distributed CNN\n        X = mel_spect_ts.reshape(mel_spect_ts.shape[0],\n                                    mel_spect_ts.shape[1],\n                                    mel_spect_ts.shape[2],\n                                    mel_spect_ts.shape[3],\n                                    1)\n\n        # Predict emotion\n        if predict_proba is True:\n            predict = self._model.predict(X)\n        else:\n            predict = np.argmax(self._model.predict(X), axis=1)\n            predict = [self._emotion.get(emotion) for emotion in predict]\n\n        # Clear Keras session\n        K.clear_session()\n\n        # Predict timestamp\n        timestamp = np.concatenate([[chunk_size], np.ones((len(predict) - 1)) * chunk_step]).cumsum()\n        timestamp = np.round(timestamp / sample_rate)\n\n        return [predict, timestamp]\n\n    \'\'\'\n    Export emotions predicted to csv format\n    \'\'\'\n    def prediction_to_csv(self, predictions, filename, mode=\'w\'):\n\n        # Write emotion in filename\n        with open(filename, mode) as f:\n            if mode == \'w\':\n                f.write(""EMOTIONS""+\'\\n\')\n            for emotion in predictions:\n                f.write(str(emotion)+\'\\n\')\n            f.close()\n'"
04-WebApp/library/text_emotion_recognition.py,0,"b'from gensim.models import KeyedVectors\nfrom gensim.models import word2vec\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport datetime\nfrom operator import itemgetter\nfrom random import randint\nimport seaborn as sns\n\nimport os\nimport time\nimport string\nimport dill\nimport pickle\n\nfrom nltk import *\n\nfrom nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\nfrom nltk.corpus import stopwords as sw, wordnet as wn\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, FunctionTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import precision_score, accuracy_score, confusion_matrix, classification_report as clsr\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\nfrom sklearn.model_selection import GridSearchCV, train_test_split as tts\nfrom sklearn.manifold import TSNE\nfrom sklearn.multiclass import OneVsRestClassifier\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, LSTM, SpatialDropout1D, Activation, Conv1D, MaxPooling1D, Input, concatenate\nfrom keras.utils.np_utils import to_categorical\nfrom keras import backend as K\n\n# Do some code, e.g. train and save model\n\nclass predict:\n    \n    def __init__(self):\n        self.max_sentence_len = 300\n        self.max_features = 300\n        self.embed_dim = 300\n        self.NLTKPreprocessor = self.NLTKPreprocessor()\n    \n    \n    class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n        """"""\n        Transforms input data by using NLTK tokenization, POS tagging\n        lemmatization and vectorization.\n        """"""\n        \n        def __init__(self, max_sentence_len = 300, stopwords=None, punct=None, lower=True, strip=True):\n            """"""\n            Instantiates the preprocessor.\n            """"""\n            self.lower = lower\n            self.strip = strip\n            self.stopwords = set(stopwords) if stopwords else set(sw.words(\'english\'))\n            self.punct = set(punct) if punct else set(string.punctuation)\n            self.lemmatizer = WordNetLemmatizer()\n            self.max_sentence_len = max_sentence_len\n        \n        def fit(self, X, y=None):\n            """"""\n                Fit simply returns self.\n                """"""\n            return self\n        \n        def inverse_transform(self, X):\n            """"""\n                No inverse transformation.\n                """"""\n            return X\n        \n        def transform(self, X):\n            """"""\n                Actually runs the preprocessing on each document.\n                """"""\n            \n            print(str(X))\n            \n            output = np.array([(self.tokenize(doc)) for doc in X])\n            return output\n        \n        def tokenize(self, document):\n            """"""\n                Returns a normalized, lemmatized list of tokens from a document by\n                applying segmentation, tokenization, and part of speech tagging.\n                Uses the part of speech tags to look up the lemma in WordNet, and returns the lowercase\n                version of all the words, removing stopwords and punctuation.\n                """"""\n            lemmatized_tokens = []\n            \n            # Clean the text\n            document = re.sub(r""[^A-Za-z0-9^,!.\\/\'+-=]"", "" "", document)\n            document = re.sub(r""what\'s"", ""what is "", document)\n            document = re.sub(r""\\\'s"", "" "", document)\n            document = re.sub(r""\\\'ve"", "" have "", document)\n            document = re.sub(r""can\'t"", ""cannot "", document)\n            document = re.sub(r""n\'t"", "" not "", document)\n            document = re.sub(r""i\'m"", ""i am "", document)\n            document = re.sub(r""\\\'re"", "" are "", document)\n            document = re.sub(r""\\\'d"", "" would "", document)\n            document = re.sub(r""\\\'ll"", "" will "", document)\n            document = re.sub(r""(\\d+)(k)"", r""\\g<1>000"", document)\n            \n            # Break the document into sentences\n            for sent in sent_tokenize(document):\n                \n                # Break the sentence into part of speech tagged tokens\n                for token, tag in pos_tag(wordpunct_tokenize(sent)):\n                    \n                    # Apply preprocessing to the token\n                    token = token.lower() if self.lower else token\n                    token = token.strip() if self.strip else token\n                    token = token.strip(\'_\') if self.strip else token\n                    token = token.strip(\'*\') if self.strip else token\n                    \n                    # If punctuation or stopword, ignore token and continue\n                    if token in self.stopwords or all(char in self.punct for char in token):\n                        continue\n                    \n                    # Lemmatize the token\n                    lemma = self.lemmatize(token, tag)\n                    lemmatized_tokens.append(lemma)\n            \n            doc = \' \'.join(lemmatized_tokens)\n            tokenized_document = self.vectorize(np.array(doc)[np.newaxis])\n            return tokenized_document\n        \n        \n        def vectorize(self, doc):\n            """"""\n                Returns a vectorized padded version of sequences.\n                """"""\n            save_path = ""Models/padding.pickle""\n            with open(save_path, \'rb\') as f:\n                tokenizer = pickle.load(f)\n            doc_pad = tokenizer.texts_to_sequences(doc)\n            doc_pad = pad_sequences(doc_pad, padding=\'pre\', truncating=\'pre\', maxlen=self.max_sentence_len)\n            return np.squeeze(doc_pad)\n        \n        def lemmatize(self, token, tag):\n            """"""\n                Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n                tag to perform WordNet lemmatization.\n                """"""\n            tag = {\n                \'N\': wn.NOUN,\n                \'V\': wn.VERB,\n                \'R\': wn.ADV,\n                \'J\': wn.ADJ\n            }.get(tag[0], wn.NOUN)\n            \n            return self.lemmatizer.lemmatize(token, tag)\n\n\n    class MyRNNTransformer(BaseEstimator, TransformerMixin):\n        """"""\n        Transformer allowing our Keras model to be included in our pipeline\n        """"""\n        def __init__(self, classifier):\n            self.classifier = classifier\n        \n        def fit(self, X, y):\n            batch_size = 32\n            num_epochs = 35\n            batch_size = batch_size\n            epochs = num_epochs\n            self.classifier.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=2)\n            return self\n        \n        def transform(self, X):\n            self.pred = self.classifier.predict(X)\n            self.classes = [[0 if el < 0.2 else 1 for el in item] for item in self.pred]\n            return self.pred\n\n\n    def run(self, X, model_name):\n        """"""\n        Returns the predictions from the pipeline including our NLTKPreprocessor and Keras classifier.\n        """"""\n        def build(classifier):\n            """"""\n            Inner build function that builds a pipeline including a preprocessor and a classifier.\n            """"""\n            model = Pipeline([\n                              (\'preprocessor\', self.NLTKPreprocessor),\n                              (\'classifier\', classifier)\n                              ])\n            return model\n        \n        save_path = \'Models/\'\n        json_file = open(save_path + model_name + \'.json\', \'r\')\n        classifier = model_from_json(json_file.read())\n        classifier.load_weights(save_path + model_name + \'.h5\')\n        classifier.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n        json_file.close()\n        model = build(self.MyRNNTransformer(classifier))\n        y_pred = model.transform([X])\n        \n        K.clear_session()\n        \n        return y_pred\n'"
04-WebApp/library/text_preprocessor.py,0,"b'from gensim.models import KeyedVectors\nfrom gensim.models import word2vec\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport datetime\nfrom operator import itemgetter\nfrom random import randint\nimport seaborn as sns\n\nimport os\nimport time\nimport string\nimport dill\nimport pickle\n\nfrom nltk import *\n\nfrom nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\nfrom nltk.corpus import stopwords as sw, wordnet as wn\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, FunctionTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import precision_score, accuracy_score, confusion_matrix, classification_report as clsr\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\nfrom sklearn.model_selection import GridSearchCV, train_test_split as tts\nfrom sklearn.manifold import TSNE\nfrom sklearn.multiclass import OneVsRestClassifier\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, LSTM, SpatialDropout1D, Activation, Conv1D, MaxPooling1D, Input, concatenate\nfrom keras.utils.np_utils import to_categorical\nfrom keras import backend as K\n\nclass NLTKPreprocessor(BaseEstimator, TransformerMixin):\n    """"""\n        Transforms input data by using NLTK tokenization, POS tagging, lemmatization and vectorization.\n        """"""\n    \n    def __init__(self, max_sentence_len = 300, stopwords=None, punct=None, lower=True, strip=True):\n        """"""\n            Instantiates the preprocessor.\n            """"""\n        self.lower = lower\n        self.strip = strip\n        self.stopwords = set(stopwords) if stopwords else set(sw.words(\'english\'))\n        self.punct = set(punct) if punct else set(string.punctuation)\n        self.lemmatizer = WordNetLemmatizer()\n        self.max_sentence_len = max_sentence_len\n    \n    def fit(self, X, y=None):\n        """"""\n            Fit simply returns self.\n            """"""\n        return self\n    \n    def inverse_transform(self, X):\n        """"""\n            No inverse transformation.\n            """"""\n        return X\n    \n    def transform(self, X):\n        """"""\n            Actually runs the preprocessing on each document.\n            """"""\n        output = [(self.tokenize(doc)) for doc in X]\n        return output\n    \n    def tokenize(self, document):\n        """"""\n            Returns a normalized, lemmatized list of tokens from a document by\n            applying segmentation, tokenization, and part of speech tagging.\n            Uses the part of speech tags to look up the lemma in WordNet, and returns the lowercase\n            version of all the words, removing stopwords and punctuation.\n            """"""\n        lemmatized_tokens = []\n        \n        # Clean the text\n        document = re.sub(r""[^A-Za-z0-9^,!.\\/\'+-=]"", "" "", document)\n        document = re.sub(r""what\'s"", ""what is "", document)\n        document = re.sub(r""\\\'s"", "" "", document)\n        document = re.sub(r""\\\'ve"", "" have "", document)\n        document = re.sub(r""can\'t"", ""cannot "", document)\n        document = re.sub(r""n\'t"", "" not "", document)\n        document = re.sub(r""i\'m"", ""i am "", document)\n        document = re.sub(r""\\\'re"", "" are "", document)\n        document = re.sub(r""\\\'d"", "" would "", document)\n        document = re.sub(r""\\\'ll"", "" will "", document)\n        document = re.sub(r""(\\d+)(k)"", r""\\g<1>000"", document)\n        \n        # Break the document into sentences\n        for sent in sent_tokenize(document):\n            \n            # Break the sentence into part of speech tagged tokens\n            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n                \n                # Apply preprocessing to the token\n                token = token.lower() if self.lower else token\n                token = token.strip() if self.strip else token\n                token = token.strip(\'_\') if self.strip else token\n                token = token.strip(\'*\') if self.strip else token\n                \n                # If punctuation or stopword, ignore token and continue\n                if token in self.stopwords or all(char in self.punct for char in token):\n                    continue\n                \n                # Lemmatize the token\n                lemma = self.lemmatize(token, tag)\n                lemmatized_tokens.append(lemma)\n        \n        doc = \' \'.join(lemmatized_tokens)\n        #tokenized_document = self.vectorize(np.array(doc)[np.newaxis])\n        return doc\n\n\n    def vectorize(self, doc):\n        """"""\n            Returns a vectorized padded version of sequences.\n            """"""\n        save_path = ""Models/padding.pickle""\n        with open(save_path, \'rb\') as f:\n            tokenizer = pickle.load(f)\n\n        doc_pad = tokenizer.texts_to_sequences(doc)\n        return np.squeeze(doc_pad)\n\n    def lemmatize(self, token, tag):\n        """"""\n        Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n        tag to perform WordNet lemmatization.\n        """"""\n        tag = {\n        \'N\': wn.NOUN,\n        \'V\': wn.VERB,\n        \'R\': wn.ADV,\n        \'J\': wn.ADJ\n        }.get(tag[0], wn.NOUN)\n\n        return self.lemmatizer.lemmatize(token, tag)\n'"
04-WebApp/library/video_emotion_recognition.py,0,"b'### General imports ###\nfrom __future__ import division\nimport numpy as np\nimport pandas as pd\nimport time\nfrom time import sleep\nimport re\nimport os\nimport requests\nimport argparse\nfrom collections import OrderedDict\n\n### Image processing ###\nimport cv2\nfrom scipy.ndimage import zoom\nfrom scipy.spatial import distance\nimport imutils\nfrom scipy import ndimage\nimport dlib\nfrom imutils import face_utils\n\n### Model ###\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras import backend as K\n\ndef gen():\n    """"""\n    Video streaming generator function.\n    """"""\n    \n    # Start video capute. 0 = Webcam, 1 = Video file, -1 = Webcam for Web\n    video_capture = cv2.VideoCapture(0)\n    \n    # Image shape\n    shape_x = 48\n    shape_y = 48\n    input_shape = (shape_x, shape_y, 1)\n    \n    # We have 7 emotions\n    nClasses = 7\n    \n    # Timer until the end of the recording\n    end = 0\n    \n    # Count number of eye blinks (not used in model prediction)\n    def eye_aspect_ratio(eye):\n        \n        A = distance.euclidean(eye[1], eye[5])\n        B = distance.euclidean(eye[2], eye[4])\n        C = distance.euclidean(eye[0], eye[3])\n        ear = (A + B) / (2.0 * C)\n        \n        return ear\n    \n    # Detect facial landmarks and return coordinates (not used in model prediction but in visualization)\n    def detect_face(frame):\n        \n        #Cascade classifier pre-trained model\n        cascPath = \'Models/face_landmarks.dat\'\n        faceCascade = cv2.CascadeClassifier(cascPath)\n        \n        #BGR -> Gray conversion\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        \n        #Cascade MultiScale classifier\n        detected_faces = faceCascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=6,\n                                                      minSize=(shape_x, shape_y),\n                                                      flags=cv2.CASCADE_SCALE_IMAGE)\n        coord = []\n                                                      \n        for x, y, w, h in detected_faces :\n            if w > 100 :\n                # Square around the landmarks\n                sub_img=frame[y:y+h,x:x+w]\n                # Put a rectangle around the face\n                cv2.rectangle(frame,(x,y),(x+w,y+h),(0, 255,255),1)\n                coord.append([x,y,w,h])\n                                                          \n        return gray, detected_faces, coord\n    \n    #  Zoom on the face of the person\n    def extract_face_features(faces, offset_coefficients=(0.075, 0.05)):\n        \n        # Each face identified\n        gray = faces[0]\n        \n        # ID of each face identifies\n        detected_face = faces[1]\n        \n        new_face = []\n        \n        for det in detected_face :\n            # Region in which the face is detected\n            # x, y represent the starting point, w the width (moving right) and h the height (moving up)\n            x, y, w, h = det\n            \n            #Offset coefficient (margins), np.floor takes the lowest integer (delete border of the image)\n            horizontal_offset = np.int(np.floor(offset_coefficients[0] * w))\n            vertical_offset = np.int(np.floor(offset_coefficients[1] * h))\n            \n            # Coordinates of the extracted face\n            extracted_face = gray[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n            \n            #Zoom on the extracted face\n            new_extracted_face = zoom(extracted_face, (shape_x / extracted_face.shape[0],shape_y / extracted_face.shape[1]))\n            \n            # Cast type to float\n            new_extracted_face = new_extracted_face.astype(np.float32)\n            \n            # Scale the new image\n            new_extracted_face /= float(new_extracted_face.max())\n            \n            # Append the face to the list\n            new_face.append(new_extracted_face)\n        \n        return new_face\n    \n    # Initiate Landmarks\n    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""left_eye""]\n    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""right_eye""]\n    \n    (nStart, nEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""nose""]\n    (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""mouth""]\n    (jStart, jEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""jaw""]\n    \n    (eblStart, eblEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""left_eyebrow""]\n    (ebrStart, ebrEnd) = face_utils.FACIAL_LANDMARKS_IDXS[""right_eyebrow""]\n    \n    # Load the pre-trained X-Ception model\n    model = load_model(\'Models/video.h5\')\n    \n    # Load the face detector\n    face_detect = dlib.get_frontal_face_detector()\n    \n    # Load the facial landmarks predictor\n    predictor_landmarks  = dlib.shape_predictor(""Models/face_landmarks.dat"")\n\n    # Prediction vector\n    predictions = []\n    \n    # Timer\n    global k\n    k = 0\n    max_time = 15\n    start = time.time()\n    \n    angry_0 = []\n    disgust_1 = []\n    fear_2 = []\n    happy_3 = []\n    sad_4 = []\n    surprise_5 = []\n    neutral_6 = []\n\n    # Record for 45 seconds\n    while end - start < max_time :\n        \n        k = k+1\n        end = time.time()\n        \n        # Capture frame-by-frame the video_capture initiated above\n        ret, frame = video_capture.read()\n        \n        # Face index, face by face\n        face_index = 0\n        \n        # Image to gray scale\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        \n        # All faces detected\n        rects = face_detect(gray, 1)\n        \n        #gray, detected_faces, coord = detect_face(frame)\n        \n        \n        # For each detected face\n        for (i, rect) in enumerate(rects):\n            \n            # Identify face coordinates\n            (x, y, w, h) = face_utils.rect_to_bb(rect)\n            face = gray[y:y+h,x:x+w]\n            \n            # Identify landmarks and cast to numpy\n            shape = predictor_landmarks(gray, rect)\n            shape = face_utils.shape_to_np(shape)\n            \n            # Zoom on extracted face\n            face = zoom(face, (shape_x / face.shape[0],shape_y / face.shape[1]))\n            \n            # Cast type float\n            face = face.astype(np.float32)\n            \n            # Scale the face\n            face /= float(face.max())\n            face = np.reshape(face.flatten(), (1, 48, 48, 1))\n            \n            # Make Emotion prediction on the face, outputs probabilities\n            prediction = model.predict(face)\n            \n            # For plotting purposes with Altair\n            angry_0.append(prediction[0][0].astype(float))\n            disgust_1.append(prediction[0][1].astype(float))\n            fear_2.append(prediction[0][2].astype(float))\n            happy_3.append(prediction[0][3].astype(float))\n            sad_4.append(prediction[0][4].astype(float))\n            surprise_5.append(prediction[0][5].astype(float))\n            neutral_6.append(prediction[0][6].astype(float))\n            \n            # Most likely emotion\n            prediction_result = np.argmax(prediction)\n            \n            # Append the emotion to the final list\n            predictions.append(str(prediction_result))\n            \n            # Draw rectangle around the face\n            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n            \n            # Top left : Put the ID of the face\n            cv2.putText(frame, ""Face #{}"".format(i + 1), (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n            \n            # Draw all the landmarks dots\n            for (j, k) in shape:\n                cv2.circle(frame, (j, k), 1, (0, 0, 255), -1)\n        \n            # Add prediction probabilities on the top-left report\n            cv2.putText(frame, ""----------------"",(40,100 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n            cv2.putText(frame, ""Emotional report : Face #"" + str(i+1),(40,120 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n            cv2.putText(frame, ""Angry : "" + str(round(prediction[0][0],3)),(40,140 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n            cv2.putText(frame, ""Disgust : "" + str(round(prediction[0][1],3)),(40,160 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n            cv2.putText(frame, ""Fear : "" + str(round(prediction[0][2],3)),(40,180 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n            cv2.putText(frame, ""Happy : "" + str(round(prediction[0][3],3)),(40,200 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n            cv2.putText(frame, ""Sad : "" + str(round(prediction[0][4],3)),(40,220 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n            cv2.putText(frame, ""Surprise : "" + str(round(prediction[0][5],3)),(40,240 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n            cv2.putText(frame, ""Neutral : "" + str(round(prediction[0][6],3)),(40,260 + 180*i), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n            \n            # Annotate main image with the emotion label\n            if prediction_result == 0 :\n                cv2.putText(frame, ""Angry"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            elif prediction_result == 1 :\n                cv2.putText(frame, ""Disgust"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            elif prediction_result == 2 :\n                cv2.putText(frame, ""Fear"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            elif prediction_result == 3 :\n                cv2.putText(frame, ""Happy"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            elif prediction_result == 4 :\n                cv2.putText(frame, ""Sad"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            elif prediction_result == 5 :\n                cv2.putText(frame, ""Surprise"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            else :\n                cv2.putText(frame, ""Neutral"",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            \n            # Eye Detection and Blink Count\n            leftEye = shape[lStart:lEnd]\n            rightEye = shape[rStart:rEnd]\n            \n            # Compute Eye Aspect Ratio\n            leftEAR = eye_aspect_ratio(leftEye)\n            rightEAR = eye_aspect_ratio(rightEye)\n            ear = (leftEAR + rightEAR) / 2.0\n            \n            # And plot its contours\n            leftEyeHull = cv2.convexHull(leftEye)\n            rightEyeHull = cv2.convexHull(rightEye)\n            cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n            cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n            \n            # Detect Nose and draw its contours\n            nose = shape[nStart:nEnd]\n            noseHull = cv2.convexHull(nose)\n            cv2.drawContours(frame, [noseHull], -1, (0, 255, 0), 1)\n            \n            # Detect Mouth and draw its contours\n            mouth = shape[mStart:mEnd]\n            mouthHull = cv2.convexHull(mouth)\n            cv2.drawContours(frame, [mouthHull], -1, (0, 255, 0), 1)\n            \n            # Detect Jaw and draw its contours\n            jaw = shape[jStart:jEnd]\n            jawHull = cv2.convexHull(jaw)\n            cv2.drawContours(frame, [jawHull], -1, (0, 255, 0), 1)\n            \n            # Detect Eyebrows and draw its contours\n            ebr = shape[ebrStart:ebrEnd]\n            ebrHull = cv2.convexHull(ebr)\n            cv2.drawContours(frame, [ebrHull], -1, (0, 255, 0), 1)\n\n            ebl = shape[eblStart:eblEnd]\n            eblHull = cv2.convexHull(ebl)\n            cv2.drawContours(frame, [eblHull], -1, (0, 255, 0), 1)\n    \n        # Show number of faces captured\n        cv2.putText(frame,\'Number of Faces : \' + str(len(rects)),(40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, 155, 1)\n        \n        # For flask, save image as t.jpg (rewritten at each step)\n        cv2.imwrite(\'tmp/t.jpg\', frame)\n        \n        # Yield the image at each step\n        yield (b\'--frame\\r\\n\'\n               b\'Content-Type: image/jpeg\\r\\n\\r\\n\' + open(\'tmp/t.jpg\', \'rb\').read() + b\'\\r\\n\')\n        \n        # Emotion mapping\n        #emotion = {0:\'Angry\', 1:\'Disgust\', 2:\'Fear\', 3:\'Happy\', 4:\'Neutral\', 5:\'Sad\', 6:\'Surprise\'}\n        \n        # Once reaching the end, write the results to the personal file and to the overall file\n        if end-start > max_time - 1 :\n            with open(""static/js/db/histo_perso.txt"", ""w"") as d:\n                d.write(""density""+\'\\n\')\n                for val in predictions :\n                    d.write(str(val)+\'\\n\')\n                \n            with open(""static/js/db/histo.txt"", ""a"") as d:\n                for val in predictions :\n                    d.write(str(val)+\'\\n\')\n               \n    \n            rows = zip(angry_0,disgust_1,fear_2,happy_3,sad_4,surprise_5,neutral_6)\n\n            import csv\n            with open(""static/js/db/prob.csv"", ""w"") as d:\n                writer = csv.writer(d)\n                for row in rows:\n                    writer.writerow(row)\n         \n\n            with open(""static/js/db/prob_tot.csv"", ""a"") as d:\n                writer = csv.writer(d)\n                for row in rows:\n                    writer.writerow(row)\n          \n            K.clear_session()\n            break\n\n    video_capture.release()\n# Clear session to allow user to do another test afterwards\n#K.clear_session()\n\n\n    # d.write(\',\'.join(str(i) for i in angry_0)+\'\\n\')\n    # d.write(\',\'.join(str(i) for i in disgust_1)+\'\\n\')\n    #d.write(\',\'.join(str(i) for i in fear_2)+\'\\n\')\n    # d.write(\',\'.join(str(i) for i in happy_3)+\'\\n\')\n    #  d.write(\',\'.join(str(i) for i in sad_4)+\'\\n\')\n    #  d.write(\',\'.join(str(i) for i in surprise_5)+\'\\n\')\n#  d.write(\',\'.join(str(i) for i in neutral_6)+\'\\n\')\n'"
01-Audio/Python/CNN-LSTM/SpeechEmotionRecognition.py,0,"b'## Basics ##\nimport time\nimport os\nimport numpy as np\n\n## Audio Preprocessing ##\nimport pyaudio\nimport wave\nimport librosa\nfrom scipy.stats import zscore\n\n## Time Distributed CNN ##\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Activation, TimeDistributed\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten\nfrom tensorflow.keras.layers import LSTM\n\n\n\'\'\'\nSpeech Emotion Recognition\n\'\'\'\nclass speechEmotionRecognition:\n\n    \'\'\'\n    Voice recording function\n    \'\'\'\n    def __init__(self, subdir_model=None):\n\n        # Load prediction model\n        if subdir_model is not None:\n            self._model = self.build_model()\n            self._model.load_weights(subdir_model)\n\n        # Emotion encoding\n        self._emotion = {0:\'Angry\', 1:\'Disgust\', 2:\'Fear\', 3:\'Happy\', 4:\'Neutral\', 5:\'Sad\', 6:\'Surprise\'}\n\n\n    \'\'\'\n    Voice recording function\n    \'\'\'\n    def voice_recording(self, filename, duration=5, sample_rate=16000, chunk=1024, channels=1):\n\n        # Start the audio recording stream\n        p = pyaudio.PyAudio()\n        stream = p.open(format=pyaudio.paInt16,\n                        channels=channels,\n                        rate=sample_rate,\n                        input=True,\n                        frames_per_buffer=chunk)\n\n        # Create an empty list to store audio recording\n        frames = []\n\n        # Determine the timestamp of the start of the response interval\n        print(\'* Start Recording *\')\n        stream.start_stream()\n        start_time = time.time()\n        current_time = time.time()\n\n        # Record audio until timeout\n        while (current_time - start_time) < duration:\n\n            # Record data audio data\n            data = stream.read(chunk)\n\n            # Add the data to a buffer (a list of chunks)\n            frames.append(data)\n\n            # Get new timestamp\n            current_time = time.time()\n\n        # Close the audio recording stream\n        stream.stop_stream()\n        stream.close()\n        p.terminate()\n        print(\'* End Recording * \')\n\n        # Export audio recording to wav format\n        wf = wave.open(filename, \'w\')\n        wf.setnchannels(channels)\n        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n        wf.setframerate(sample_rate)\n        wf.writeframes(b\'\'.join(frames))\n        wf.close()\n\n\n    \'\'\'\n    Mel-spectogram computation\n    \'\'\'\n    def mel_spectrogram(self, y, sr=16000, n_fft=512, win_length=256, hop_length=128, window=\'hamming\', n_mels=128, fmax=4000):\n\n        # Compute spectogram\n        mel_spect = np.abs(librosa.stft(y, n_fft=n_fft, window=window, win_length=win_length, hop_length=hop_length)) ** 2\n\n        # Compute mel spectrogram\n        mel_spect = librosa.feature.melspectrogram(S=mel_spect, sr=sr, n_mels=n_mels, fmax=fmax)\n\n        # Compute log-mel spectrogram\n        mel_spect = librosa.power_to_db(mel_spect, ref=np.max)\n\n        return np.asarray(mel_spect)\n\n\n    \'\'\'\n    Audio framing\n    \'\'\'\n    def frame(self, y, win_step=64, win_size=128):\n\n        # Number of frames\n        nb_frames = 1 + int((y.shape[2] - win_size) / win_step)\n\n        # Framming\n        frames = np.zeros((y.shape[0], nb_frames, y.shape[1], win_size)).astype(np.float16)\n        for t in range(nb_frames):\n            frames[:,t,:,:] = np.copy(y[:,:,(t * win_step):(t * win_step + win_size)]).astype(np.float16)\n\n        return frames\n\n\n    \'\'\'\n    Time distributed Convolutional Neural Network model\n    \'\'\'\n    def build_model(self):\n\n        # Clear Keras session\n        K.clear_session()\n\n        # Define input\n        input_y = Input(shape=(5, 128, 128, 1), name=\'Input_MELSPECT\')\n\n        # First LFLB (local feature learning block)\n        y = TimeDistributed(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding=\'same\'), name=\'Conv_1_MELSPECT\')(input_y)\n        y = TimeDistributed(BatchNormalization(), name=\'BatchNorm_1_MELSPECT\')(y)\n        y = TimeDistributed(Activation(\'elu\'), name=\'Activ_1_MELSPECT\')(y)\n        y = TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\'same\'), name=\'MaxPool_1_MELSPECT\')(y)\n        y = TimeDistributed(Dropout(0.2), name=\'Drop_1_MELSPECT\')(y)\n\n        # Second LFLB (local feature learning block)\n        y = TimeDistributed(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding=\'same\'), name=\'Conv_2_MELSPECT\')(y)\n        y = TimeDistributed(BatchNormalization(), name=\'BatchNorm_2_MELSPECT\')(y)\n        y = TimeDistributed(Activation(\'elu\'), name=\'Activ_2_MELSPECT\')(y)\n        y = TimeDistributed(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding=\'same\'), name=\'MaxPool_2_MELSPECT\')(y)\n        y = TimeDistributed(Dropout(0.2), name=\'Drop_2_MELSPECT\')(y)\n\n        # Third LFLB (local feature learning block)\n        y = TimeDistributed(Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding=\'same\'), name=\'Conv_3_MELSPECT\')(y)\n        y = TimeDistributed(BatchNormalization(), name=\'BatchNorm_3_MELSPECT\')(y)\n        y = TimeDistributed(Activation(\'elu\'), name=\'Activ_3_MELSPECT\')(y)\n        y = TimeDistributed(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding=\'same\'), name=\'MaxPool_3_MELSPECT\')(y)\n        y = TimeDistributed(Dropout(0.2), name=\'Drop_3_MELSPECT\')(y)\n\n        # Fourth LFLB (local feature learning block)\n        y = TimeDistributed(Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding=\'same\'), name=\'Conv_4_MELSPECT\')(y)\n        y = TimeDistributed(BatchNormalization(), name=\'BatchNorm_4_MELSPECT\')(y)\n        y = TimeDistributed(Activation(\'elu\'), name=\'Activ_4_MELSPECT\')(y)\n        y = TimeDistributed(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding=\'same\'), name=\'MaxPool_4_MELSPECT\')(y)\n        y = TimeDistributed(Dropout(0.2), name=\'Drop_4_MELSPECT\')(y)\n\n        # Flat\n        y = TimeDistributed(Flatten(), name=\'Flat_MELSPECT\')(y)\n\n        # LSTM layer\n        y = LSTM(256, return_sequences=False, dropout=0.2, name=\'LSTM_1\')(y)\n\n        # Fully connected\n        y = Dense(7, activation=\'softmax\', name=\'FC\')(y)\n\n        # Build final model\n        model = Model(inputs=input_y, outputs=y)\n\n        return model\n\n\n    \'\'\'\n    Predict speech emotion over time from an audio file\n    \'\'\'\n    def predict_emotion_from_file(self, filename, chunk_step=16000, chunk_size=49100, predict_proba=False, sample_rate=16000):\n\n        # Read audio file\n        y, sr = librosa.core.load(filename, sr=sample_rate, offset=0.5)\n\n        # Split audio signals into chunks\n        chunks = self.frame(y.reshape(1, 1, -1), chunk_step, chunk_size)\n\n        # Reshape chunks\n        chunks = chunks.reshape(chunks.shape[1],chunks.shape[-1])\n\n        # Z-normalization\n        y = np.asarray(list(map(zscore, chunks)))\n\n        # Compute mel spectrogram\n        mel_spect = np.asarray(list(map(self.mel_spectrogram, y)))\n\n        # Time distributed Framing\n        mel_spect_ts = self.frame(mel_spect)\n\n        # Build X for time distributed CNN\n        X = mel_spect_ts.reshape(mel_spect_ts.shape[0],\n                                    mel_spect_ts.shape[1],\n                                    mel_spect_ts.shape[2],\n                                    mel_spect_ts.shape[3],\n                                    1)\n\n        # Predict emotion\n        if predict_proba is True:\n            predict = self._model.predict(X)\n        else:\n            predict = np.argmax(self._model.predict(X), axis=1)\n            predict = [self._emotion.get(emotion) for emotion in predict]\n\n        # Clear Keras session\n        K.clear_session()\n\n        # Predict timestamp\n        timestamp = np.concatenate([[chunk_size], np.ones((len(predict) - 1)) * chunk_step]).cumsum()\n        timestamp = np.round(timestamp / sample_rate)\n\n        return [predict, timestamp]\n\n    \'\'\'\n    Export emotions predicted to csv format\n    \'\'\'\n    def prediction_to_csv(self, predictions, filename, mode=\'w\'):\n\n        # Write emotion in filename\n        with open(filename, mode) as f:\n            if mode == \'w\':\n                f.write(""EMOTIONS""+\'\\n\')\n            for emotion in predictions:\n                f.write(str(emotion)+\'\\n\')\n            f.close()\n'"
01-Audio/Python/SVM/AudioEmotionRecognition.py,0,"b'import pickle\nfrom AudioLibrary.AudioSignal import *\nfrom AudioLibrary.AudioFeatures import *\n\n\nclass AudioEmotionRecognition:\n\n    def __init__(self, model_path):\n\n        # Load classifier\n        self._clf = pickle.load(open(os.path.join(model_path, \'MODEL_CLF.p\'), \'rb\'))\n\n        # Load features parameters\n        self._features_param = pickle.load(open(os.path.join(model_path, \'MODEL_PARAM.p\'), \'rb\'))\n\n        # Load feature scaler parametrs (mean and std)\n        self._features_mean, self._features_std = pickle.load(open(os.path.join(model_path, \'MODEL_SCALER.p\'), \'rb\'))\n\n        # Load PCA\n        self._pca = pickle.load(open(os.path.join(model_path, \'MODEL_PCA.p\'), \'rb\'))\n\n        # Load label encoder\n        self._encoder = pickle.load(open(os.path.join(model_path, \'MODEL_ENCODER.p\'), \'rb\'))\n\n    \'\'\'\n    Function to scale audio features\n    \'\'\'\n    def scale_features(self, features):\n\n        # Scaled features\n        scaled_features = (features - self._features_mean) / self._features_std\n\n        # Return scaled features\n        return scaled_features\n\n    \'\'\'\n    Function to predict speech emotion from an audio signals\n    \'\'\'\n    def predict_emotion(self, audio_signal, predict_proba=False, decode=True):\n\n        # Extract audio features\n        audio_features = AudioFeatures(audio_signal, float(self._features_param.get(""win_size"")),\n                                       float(self._features_param.get(""win_step"")))\n        features, features_names = audio_features.global_feature_extraction(stats=self._features_param.get(""stats""),\n                                                                            features_list=self._features_param.get(\n                                                                                ""features_list""),\n                                                                            nb_mfcc=self._features_param.get(""nb_mfcc""),\n                                                                            diff=self._features_param.get(""diff""))\n        # Scale features\n        features = self.scale_features(features)\n\n        # Apply feature dimension reduction\n        if self._features_param.get(""PCA"") is True:\n            features = self._pca.transform(features)\n\n        # Make prediction\n        if predict_proba is True:\n            prediction = self._clf.predict_proba(features.reshape(1, -1))\n        else:\n            prediction = self._clf.predict(features.reshape(1, -1))\n\n        # Decode label emotion\n        if decode is True:\n            prediction = (self._encoder.inverse_transform((prediction.astype(int).flatten())))\n\n        # Remove gender recognition\n        prediction = prediction[0][2:]\n\n        return prediction\n\n    \'\'\'\n    Function to predict speech emotion over time from video\n    \'\'\'\n    def predict_emotion_from_file(self, filename, sample_rate, chunk_size=0, chunk_step=0, predict_proba=False,\n                                  decode=True):\n\n        # Initialize Audio Basic object\n        audio_signal = AudioSignal(sample_rate, filename=filename)\n\n        # Split audio signals into chunks\n        if chunk_size > 0:\n            chunks = audio_signal.framing(chunk_size, chunk_step)\n\n            # Initialize time stamp\n            timestamp = []\n\n            # Emotion prediction for each chunks\n            prediction = []\n            for signal in chunks:\n                if len(timestamp) == 0:\n                    timestamp.append(chunk_size)\n                else:\n                    timestamp.append(timestamp[-1] + chunk_step)\n                prediction.append(self.predict_emotion(signal, predict_proba=predict_proba, decode=decode))\n\n            # Return emotion prediction and related timestamp\n            return prediction, timestamp\n        else:\n\n            # Emotion prediction\n            prediction = self.predict_emotion(audio_signal, predict_proba=predict_proba, decode=decode)\n\n            # Return emotion prediction\n            return prediction\n'"
01-Audio/Python/SVM/AudioFeatures.py,0,"b'import numpy\nfrom scipy.fftpack.realtransforms import dct\nfrom scipy.stats import kurtosis, skew\nfrom AudioLibrary.AudioSignal import *\n\n\nclass AudioFeatures:\n\n    def __init__(self, audio_signal, win_size, win_step):\n\n        # Audio Signal\n        self._audio_signal = audio_signal\n\n        # Short time features window size\n        self._win_size = win_size\n\n        # Short time features window step\n        self._win_step = win_step\n\n    \'\'\'\n    Global statistics features extraction from an audio signals\n    \'\'\'\n    def global_feature_extraction(self, stats=[\'mean\', \'std\'], features_list=[], nb_mfcc=12, nb_filter=40, diff=0, hamming=True):\n\n        # Extract short term audio features\n        st_features, f_names = self.short_time_feature_extraction(features_list, nb_mfcc, nb_filter, hamming)\n\n        # Number of short term features\n        nb_feats = st_features.shape[1]\n\n        # Number of statistics\n        nb_stats = len(stats)\n\n        # Global statistics feature names\n        feature_names = ["""" for x in range(nb_feats * nb_stats)]\n        for i in range(nb_feats):\n            for j in range(nb_stats):\n                feature_names[i + j * nb_feats] = f_names[i] + ""_d"" + str(diff) + ""_"" + stats[j]\n\n        # Calculate global statistics features\n        features = numpy.zeros((nb_feats * nb_stats))\n        for i in range(nb_feats):\n\n            # Get features series\n            feat = st_features[:, i]\n\n            # Compute first or second order difference\n            if diff > 0:\n                feat = feat[diff:] - feat[:-diff]\n\n            # Global statistics\n            for j in range(nb_stats):\n                features[i + j * nb_feats] = self.compute_statistic(feat, stats[j])\n\n        return features, feature_names\n\n    \'\'\'\n    Short-time features extraction from an audio signals\n    \'\'\'\n    def short_time_feature_extraction(self, features=[], nb_mfcc=12, nb_filter=40, hamming=True):\n\n        # Copy features list to compute\n        features_list = list(features)\n\n        # MFFCs features names\n        mfcc_feature_names = []\n        if \'mfcc\' in features_list:\n            mfcc_feature_names = [""mfcc_{0:d}"".format(i) for i in range(1, nb_mfcc + 1)]\n            features_list.remove(\'mfcc\')\n\n        # Filter banks features names\n        fbank_features_names = []\n        if \'filter_banks\' in features_list:\n            fbank_features_names = [""fbank_{0:d}"".format(i) for i in range(1, nb_filter + 1)]\n            features_list.remove(\'filter_banks\')\n\n        # All Features names\n        feature_names = features_list + mfcc_feature_names + fbank_features_names\n\n        # Number of features\n        nb_features = len(feature_names)\n\n        # Framming signal\n        frames = self._audio_signal.framing(self._win_size, self._win_step, hamming=hamming)\n\n        # Number of frame\n        nb_frames = len(frames)\n\n        # Compute features on each frame\n        features = numpy.zeros((nb_frames, nb_features))\n        cur_pos = 0\n        for el in frames:\n\n            # Get signal of the frame\n            signal = el._signal\n\n            # Compute the normalize magnitude of the spectrum (Discrete Fourier Transform)\n            dft = el.dft(norm=True)\n\n            # Return the first half of the spectrum\n            dft = dft[:int((self._win_size * self._audio_signal._sample_rate) / 2)]\n            if cur_pos == 0:\n                dft_prev = dft\n\n            # Compute features on frame\n            for idx, f in enumerate(features_list):\n                features[cur_pos, idx] = self.compute_st_features(f, signal, dft, dft_prev,\n                                                                  self._audio_signal._sample_rate)\n\n            # Compute MFCCs and Filter Banks\n            if len(mfcc_feature_names) > 0:\n                features[cur_pos, len(features_list):len(features_list) + len(mfcc_feature_names) + len(fbank_features_names)] = self.mfcc(signal, self._audio_signal._sample_rate,\n                                                                   nb_coeff=nb_mfcc, nb_filt=nb_filter, return_fbank=len(fbank_features_names) > 0)\n            # Compute Filter Banks\n            elif len(fbank_features_names) > 0:\n                features[cur_pos, len(features_list) + len(mfcc_feature_names):] = self.filter_banks_coeff(signal, self._audio_signal._sample_rate, nb_filt=nb_filter)\n\n            # Keep previous Discrete Fourier Transform coefficients\n            dft_prev = dft\n            cur_pos = cur_pos + 1\n\n        return features, feature_names\n\n    \'\'\'\n    Computes zero crossing rate of a signal\n    \'\'\'\n    @staticmethod\n    def zcr(signal):\n        zcr = numpy.sum(numpy.abs(numpy.diff(numpy.sign(signal))))\n        zcr = zcr / (2 * numpy.float64(len(signal) - 1.0))\n        return zcr\n\n    \'\'\'\n    Computes signal energy of frame\n    \'\'\'\n    @staticmethod\n    def energy(signal):\n        energy = numpy.sum(signal ** 2) / numpy.float64(len(signal))\n        return energy\n\n    \'\'\'\n    Computes entropy of energy\n    \'\'\'\n    @staticmethod\n    def energy_entropy(signal, n_short_blocks=10, eps=10e-8):\n\n        # Total frame energy\n        energy = numpy.sum(signal ** 2)\n        sub_win_len = int(numpy.floor(len(signal) / n_short_blocks))\n\n        # Length of sub-frame\n        if len(signal) != sub_win_len * n_short_blocks:\n            signal = signal[0:sub_win_len * n_short_blocks]\n\n        # Get sub windows\n        sub_wins = signal.reshape(sub_win_len, n_short_blocks, order=\'F\').copy()\n\n        # Compute normalized sub-frame energies:\n        sub_energies = numpy.sum(sub_wins ** 2, axis=0) / (energy + eps)\n\n        # Compute entropy of the normalized sub-frame energies:\n        entropy = -numpy.sum(sub_energies * numpy.log2(sub_energies + eps))\n\n        return entropy\n\n    \'\'\'\n    Computes spectral centroid of frame\n    \'\'\'\n    @staticmethod\n    def spectral_centroid_spread(fft, fs, eps=10e-8):\n\n        # Sample range\n        sr = (numpy.arange(1, len(fft) + 1)) * (fs / (2.0 * len(fft)))\n\n        # Normalize fft coefficients by the max value\n        norm_fft = fft / (fft.max() + eps)\n\n        # Centroid:\n        C = numpy.sum(sr * norm_fft) / (numpy.sum(norm_fft) + eps)\n\n        # Spread:\n        S = numpy.sqrt(numpy.sum(((sr - C) ** 2) * norm_fft) / (numpy.sum(norm_fft) + eps))\n\n        # Normalize:\n        C = C / (fs / 2.0)\n        S = S / (fs / 2.0)\n\n        return C, S\n\n    \'\'\'\n    Computes the spectral flux feature\n    \'\'\'\n    @staticmethod\n    def spectral_flux(fft, fft_prev, eps=10e-8):\n\n        # Sum of fft coefficients\n        sum_fft = numpy.sum(fft + eps)\n\n        # Sum of previous fft coefficients\n        sum_fft_prev = numpy.sum(fft_prev + eps)\n\n        # Compute the spectral flux as the sum of square distances\n        flux = numpy.sum((fft / sum_fft - fft_prev / sum_fft_prev) ** 2)\n\n        return flux\n\n    \'\'\'\n    Computes the spectral roll off\n    \'\'\'\n    @staticmethod\n    def spectral_rolloff(fft, c=0.90, eps=10e-8):\n\n        # Total energy\n        energy = numpy.sum(fft ** 2)\n\n        # Roll off threshold\n        threshold = c * energy\n\n        # Compute cumulative energy\n        cum_energy = numpy.cumsum(fft ** 2) + eps\n\n        # Find the spectral roll off as the frequency position\n        [roll_off, ] = numpy.nonzero(cum_energy > threshold)\n\n        # Normalize\n        if len(roll_off) > 0:\n            roll_off = numpy.float64(roll_off[0]) / (float(len(fft)))\n        else:\n            roll_off = 0.0\n\n        return roll_off\n\n    \'\'\'\n    Computes the Filter Bank coefficients\n    \'\'\'\n    @staticmethod\n    def filter_banks_coeff(signal, sample_rate, nb_filt=40, nb_fft=512):\n\n        # Magnitude of the FFT\n        mag_frames = numpy.absolute(numpy.fft.rfft(signal, nb_fft))\n\n        # Power Spectrum\n        pow_frames = ((1.0 / nb_fft) * (mag_frames ** 2))\n        low_freq_mel = 0\n\n        # Convert Hz to Mel\n        high_freq_mel = (2595 * numpy.log10(1 + (sample_rate / 2) / 700))\n\n        # Equally spaced in Mel scale\n        mel_points = numpy.linspace(low_freq_mel, high_freq_mel, nb_filt + 2)\n\n        # Convert Mel to Hz\n        hz_points = (700 * (10 ** (mel_points / 2595) - 1))\n        bin = numpy.floor((nb_fft + 1) * hz_points / sample_rate)\n\n        # Calculate filter banks\n        fbank = numpy.zeros((nb_filt, int(numpy.floor(nb_fft / 2 + 1))))\n        for m in range(1, nb_filt + 1):\n\n            # left\n            f_m_minus = int(bin[m - 1])\n\n            # center\n            f_m = int(bin[m])\n\n            # right\n            f_m_plus = int(bin[m + 1])\n\n            for k in range(f_m_minus, f_m):\n                fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n            for k in range(f_m, f_m_plus):\n                fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n        filter_banks = numpy.dot(pow_frames, fbank.T)\n\n        # Numerical Stability\n        filter_banks = numpy.where(filter_banks == 0, numpy.finfo(float).eps, filter_banks)\n\n        # dB\n        filter_banks = 20 * numpy.log10(filter_banks)\n\n        return filter_banks\n\n    \'\'\'\n    Computes the MFCCs\n    \'\'\'\n    def mfcc(self, signal, sample_rate, nb_coeff=12, nb_filt=40, nb_fft=512, return_fbank=False):\n\n        # Apply filter bank on spectogram\n        filter_banks = self.filter_banks_coeff(signal, sample_rate, nb_filt=nb_filt, nb_fft=nb_fft)\n\n        # Compute MFCC coefficients\n        mfcc = dct(filter_banks, type=2, axis=-1, norm=\'ortho\')[1: (nb_coeff + 1)]\n\n        # Return MFFCs and Filter banks coefficients\n        if return_fbank is True:\n            return numpy.concatenate((mfcc, filter_banks))\n        else:\n            return mfcc\n\n    \'\'\'\n    Compute statistics on short time features\n    \'\'\'\n    @staticmethod\n    def compute_statistic(seq, statistic):\n        if statistic == \'mean\':\n            S = numpy.mean(seq)\n        elif statistic == \'med\':\n            S = numpy.median(seq)\n        elif statistic == \'std\':\n            S = numpy.std(seq)\n        elif statistic == \'kurt\':\n            S = kurtosis(seq)\n        elif statistic == \'skew\':\n            S = skew(seq)\n        elif statistic == \'min\':\n            S = numpy.min(seq)\n        elif statistic == \'max\':\n            S = numpy.max(seq)\n        elif statistic == \'q1\':\n            S = numpy.percentile(seq, 1)\n        elif statistic == \'q99\':\n            S = numpy.percentile(seq, 99)\n        elif statistic == \'range\':\n            S = numpy.abs(numpy.percentile(seq, 99) - numpy.percentile(seq, 1))\n        return S\n\n    \'\'\'\n    Compute short time features on signal\n    \'\'\'\n    def compute_st_features(self, feature, signal, dft, dft_prev, sample_rate):\n        if feature == \'zcr\':\n            F = self.zcr(signal)\n        elif feature == \'energy\':\n            F = self.energy(signal)\n        elif feature == \'energy_entropy\':\n            F = self.energy_entropy(signal)\n        elif feature == \'spectral_centroid\':\n            [F, FF] = self.spectral_centroid_spread(dft, sample_rate)\n        elif feature == \'spectral_spread\':\n            [FF, F] = self.spectral_centroid_spread(dft, sample_rate)\n        elif feature == \'spectral_entropy\':\n            F = self.energy_entropy(dft)\n        elif feature == \'spectral_flux\':\n            F = self.spectral_flux(dft, dft_prev)\n        elif feature == \'sprectral_rolloff\':\n            F = self.spectral_rolloff(dft)\n        return F\n'"
01-Audio/Python/SVM/AudioSignal.py,0,"b'import os\nimport numpy\nfrom pydub import AudioSegment\nfrom scipy.fftpack import fft\n\n\nclass AudioSignal(object):\n\n    def __init__(self, sample_rate, signal=None, filename=None):\n\n        # Set sample rate\n        self._sample_rate = sample_rate\n\n        if signal is None:\n\n            # Get file name and file extension\n            file, file_extension = os.path.splitext(filename)\n\n            # Check if file extension if audio format\n            if file_extension in [\'.mp3\', \'.wav\']:\n\n                # Read audio file\n                self._signal = self.read_audio_file(filename)\n\n            # Check if file extension if video format\n            elif file_extension in [\'.mp4\', \'.mkv\', \'avi\']:\n\n                # Extract audio from video\n                new_filename = self.extract_audio_from_video(filename)\n\n                # read audio file from extracted audio file\n                self._signal = self.read_audio_file(new_filename)\n\n            # Case file extension is not supported\n            else:\n                print(""Error: file not found or file extension not supported."")\n\n        elif filename is None:\n\n            # Cast signal to array\n            self._signal = signal\n\n        else:\n\n            print(""Error : argument missing in AudioSignal() constructor."")\n\n    \'\'\'\n    Function to extract audio from a video\n    \'\'\'\n    def extract_audio_from_video(self, filename):\n\n        # Get video file name and extension\n        file, file_extension = os.path.splitext(filename)\n\n        # Extract audio (.wav) from video\n        os.system(\'ffmpeg -i \' + file + file_extension + \' \' + \'-ar \' + str(self._sample_rate) + \' \' + file + \'.wav\')\n        print(""Sucessfully converted {} into audio!"".format(filename))\n\n        # Return audio file name created\n        return file + \'.wav\'\n\n    \'\'\'\n    Function to read audio file and to return audio samples of a specified WAV file\n    \'\'\'\n    def read_audio_file(self, filename):\n\n        # Get audio signal\n        audio_file = AudioSegment.from_file(filename)\n\n        # Resample audio signal\n        audio_file = audio_file.set_frame_rate(self._sample_rate)\n\n        # Cast to integer\n        if audio_file.sample_width == 2:\n            data = numpy.fromstring(audio_file._data, numpy.int16)\n        elif audio_file.sample_width == 4:\n            data = numpy.fromstring(audio_file._data, numpy.int32)\n\n        # Merge audio channels\n        audio_signal = []\n        for chn in list(range(audio_file.channels)):\n            audio_signal.append(data[chn::audio_file.channels])\n        audio_signal = numpy.array(audio_signal).T\n\n        # Flat signals\n        if audio_signal.ndim == 2:\n            if audio_signal.shape[1] == 1:\n                audio_signal = audio_signal.flatten()\n\n        # Convert stereo to mono\n        audio_signal = self.stereo_to_mono(audio_signal)\n\n        # Return sample rate and audio signal\n        return audio_signal\n\n    \'\'\'\n    Function to convert an input signal from stereo to mono\n    \'\'\'\n    @staticmethod\n    def stereo_to_mono(audio_signal):\n\n        # Check if signal is stereo and convert to mono\n        if isinstance(audio_signal, int):\n            return -1\n        if audio_signal.ndim == 1:\n            return audio_signal\n        elif audio_signal.ndim == 2:\n            if audio_signal.shape[1] == 1:\n                return audio_signal.flatten()\n            else:\n                if audio_signal.shape[1] == 2:\n                    return (audio_signal[:, 1] / 2) + (audio_signal[:, 0] / 2)\n                else:\n                    return -1\n\n    \'\'\'\n    Function to split the input signal into windows of same size\n    \'\'\'\n    def framing(self, size, step, hamming=False):\n\n        # Rescale windows step and size\n        win_size = int(size * self._sample_rate)\n        win_step = int(step * self._sample_rate)\n\n        # Number of frames\n        nb_frames = 1 + int((len(self._signal) - win_size) / win_step)\n\n        # Build Hamming function\n        if hamming is True:\n            ham = numpy.hamming(win_size)\n        else:\n            ham = numpy.ones(win_size)\n\n        # Split signals (and multiply each windows signals by Hamming functions)\n        frames = []\n        for t in range(nb_frames):\n            sub_signal = AudioSignal(self._sample_rate, signal=self._signal[(t * win_step): (t * win_step + win_size)] * ham)\n            frames.append(sub_signal)\n        return frames\n\n    \'\'\'\n    Function to compute the magnitude of the Discrete Fourier Transform coefficient\n    \'\'\'\n    def dft(self, norm=False):\n\n        # Commpute the magnitude of the spectrum (and normalize by the number of sample)\n        if norm is True:\n            dft = abs(fft(self._signal)) / len(self._signal)\n        else:\n            dft = abs(fft(self._signal))\n        return dft\n\n    \'\'\'\n    Function to apply pre-emphasis filter on signal\n    \'\'\'\n    def pre_emphasis(self, alpha =0.97):\n\n        # Emphasized signal\n        emphasized_signal = numpy.append(self._signal[0], self._signal[1:] - alpha * self._signal[:-1])\n\n        return emphasized_signal\n'"
01-Audio/Notebook/SVM/AudioLibrary/AudioEmotionRecognition.py,0,"b'import pickle\nfrom AudioLibrary.AudioSignal import *\nfrom AudioLibrary.AudioFeatures import *\n\n\nclass AudioEmotionRecognition:\n\n    def __init__(self, model_path):\n\n        # Load classifier\n        self._clf = pickle.load(open(os.path.join(model_path, \'MODEL_CLF.p\'), \'rb\'))\n\n        # Load features parameters\n        self._features_param = pickle.load(open(os.path.join(model_path, \'MODEL_PARAM.p\'), \'rb\'))\n\n        # Load feature scaler parametrs (mean and std)\n        self._features_mean, self._features_std = pickle.load(open(os.path.join(model_path, \'MODEL_SCALER.p\'), \'rb\'))\n\n        # Load PCA\n        self._pca = pickle.load(open(os.path.join(model_path, \'MODEL_PCA.p\'), \'rb\'))\n\n        # Load label encoder\n        self._encoder = pickle.load(open(os.path.join(model_path, \'MODEL_ENCODER.p\'), \'rb\'))\n\n    \'\'\'\n    Function to scale audio features\n    \'\'\'\n    def scale_features(self, features):\n\n        # Scaled features\n        scaled_features = (features - self._features_mean) / self._features_std\n\n        # Return scaled features\n        return scaled_features\n\n    \'\'\'\n    Function to predict speech emotion from an audio signals\n    \'\'\'\n    def predict_emotion(self, audio_signal, predict_proba=False, decode=True):\n\n        # Extract audio features\n        audio_features = AudioFeatures(audio_signal, float(self._features_param.get(""win_size"")),\n                                       float(self._features_param.get(""win_step"")))\n        features, features_names = audio_features.global_feature_extraction(stats=self._features_param.get(""stats""),\n                                                                            features_list=self._features_param.get(\n                                                                                ""features_list""),\n                                                                            nb_mfcc=self._features_param.get(""nb_mfcc""),\n                                                                            diff=self._features_param.get(""diff""))\n        # Scale features\n        features = self.scale_features(features)\n\n        # Apply feature dimension reduction\n        if self._features_param.get(""PCA"") is True:\n            features = self._pca.transform(features)\n\n        # Make prediction\n        if predict_proba is True:\n            prediction = self._clf.predict_proba(features.reshape(1, -1))\n        else:\n            prediction = self._clf.predict(features.reshape(1, -1))\n\n        # Decode label emotion\n        if decode is True:\n            prediction = (self._encoder.inverse_transform((prediction.astype(int).flatten())))\n\n        # Remove gender recognition\n        prediction = prediction[0][2:]\n\n        return prediction\n\n    \'\'\'\n    Function to predict speech emotion over time from video\n    \'\'\'\n    def predict_emotion_from_file(self, filename, sample_rate, chunk_size=0, chunk_step=0, predict_proba=False,\n                                  decode=True):\n\n        # Initialize Audio Basic object\n        audio_signal = AudioSignal(sample_rate, filename=filename)\n\n        # Split audio signals into chunks\n        if chunk_size > 0:\n            chunks = audio_signal.framing(chunk_size, chunk_step)\n\n            # Initialize time stamp\n            timestamp = []\n\n            # Emotion prediction for each chunks\n            prediction = []\n            for signal in chunks:\n                if len(timestamp) == 0:\n                    timestamp.append(chunk_size)\n                else:\n                    timestamp.append(timestamp[-1] + chunk_step)\n                prediction.append(self.predict_emotion(signal, predict_proba=predict_proba, decode=decode))\n\n            # Return emotion prediction and related timestamp\n            return prediction, timestamp\n        else:\n\n            # Emotion prediction\n            prediction = self.predict_emotion(audio_signal, predict_proba=predict_proba, decode=decode)\n\n            # Return emotion prediction\n            return prediction\n'"
01-Audio/Notebook/SVM/AudioLibrary/AudioFeatures.py,0,"b'import numpy\nfrom scipy.fftpack.realtransforms import dct\nfrom scipy.stats import kurtosis, skew\nfrom AudioLibrary.AudioSignal import *\n\n\nclass AudioFeatures:\n\n    def __init__(self, audio_signal, win_size, win_step):\n\n        # Audio Signal\n        self._audio_signal = audio_signal\n\n        # Short time features window size\n        self._win_size = win_size\n\n        # Short time features window step\n        self._win_step = win_step\n\n    \'\'\'\n    Global statistics features extraction from an audio signals\n    \'\'\'\n    def global_feature_extraction(self, stats=[\'mean\', \'std\'], features_list=[], nb_mfcc=12, nb_filter=40, diff=0, hamming=True):\n\n        # Extract short term audio features\n        st_features, f_names = self.short_time_feature_extraction(features_list, nb_mfcc, nb_filter, hamming)\n\n        # Number of short term features\n        nb_feats = st_features.shape[1]\n\n        # Number of statistics\n        nb_stats = len(stats)\n\n        # Global statistics feature names\n        feature_names = ["""" for x in range(nb_feats * nb_stats)]\n        for i in range(nb_feats):\n            for j in range(nb_stats):\n                feature_names[i + j * nb_feats] = f_names[i] + ""_d"" + str(diff) + ""_"" + stats[j]\n\n        # Calculate global statistics features\n        features = numpy.zeros((nb_feats * nb_stats))\n        for i in range(nb_feats):\n\n            # Get features series\n            feat = st_features[:, i]\n\n            # Compute first or second order difference\n            if diff > 0:\n                feat = feat[diff:] - feat[:-diff]\n\n            # Global statistics\n            for j in range(nb_stats):\n                features[i + j * nb_feats] = self.compute_statistic(feat, stats[j])\n\n        return features, feature_names\n\n    \'\'\'\n    Short-time features extraction from an audio signals\n    \'\'\'\n    def short_time_feature_extraction(self, features=[], nb_mfcc=12, nb_filter=40, hamming=True):\n\n        # Copy features list to compute\n        features_list = list(features)\n\n        # MFFCs features names\n        mfcc_feature_names = []\n        if \'mfcc\' in features_list:\n            mfcc_feature_names = [""mfcc_{0:d}"".format(i) for i in range(1, nb_mfcc + 1)]\n            features_list.remove(\'mfcc\')\n\n        # Filter banks features names\n        fbank_features_names = []\n        if \'filter_banks\' in features_list:\n            fbank_features_names = [""fbank_{0:d}"".format(i) for i in range(1, nb_filter + 1)]\n            features_list.remove(\'filter_banks\')\n\n        # All Features names\n        feature_names = features_list + mfcc_feature_names + fbank_features_names\n\n        # Number of features\n        nb_features = len(feature_names)\n\n        # Framming signal\n        frames = self._audio_signal.framing(self._win_size, self._win_step, hamming=hamming)\n\n        # Number of frame\n        nb_frames = len(frames)\n\n        # Compute features on each frame\n        features = numpy.zeros((nb_frames, nb_features))\n        cur_pos = 0\n        for el in frames:\n\n            # Get signal of the frame\n            signal = el._signal\n\n            # Compute the normalize magnitude of the spectrum (Discrete Fourier Transform)\n            dft = el.dft(norm=True)\n\n            # Return the first half of the spectrum\n            dft = dft[:int((self._win_size * self._audio_signal._sample_rate) / 2)]\n            if cur_pos == 0:\n                dft_prev = dft\n\n            # Compute features on frame\n            for idx, f in enumerate(features_list):\n                features[cur_pos, idx] = self.compute_st_features(f, signal, dft, dft_prev,\n                                                                  self._audio_signal._sample_rate)\n\n            # Compute MFCCs and Filter Banks\n            if len(mfcc_feature_names) > 0:\n                features[cur_pos, len(features_list):len(features_list) + len(mfcc_feature_names) + len(fbank_features_names)] = self.mfcc(signal, self._audio_signal._sample_rate,\n                                                                   nb_coeff=nb_mfcc, nb_filt=nb_filter, return_fbank=len(fbank_features_names) > 0)\n            # Compute Filter Banks\n            elif len(fbank_features_names) > 0:\n                features[cur_pos, len(features_list) + len(mfcc_feature_names):] = self.filter_banks_coeff(signal, self._audio_signal._sample_rate, nb_filt=nb_filter)\n\n            # Keep previous Discrete Fourier Transform coefficients\n            dft_prev = dft\n            cur_pos = cur_pos + 1\n\n        return features, feature_names\n\n    \'\'\'\n    Computes zero crossing rate of a signal\n    \'\'\'\n    @staticmethod\n    def zcr(signal):\n        zcr = numpy.sum(numpy.abs(numpy.diff(numpy.sign(signal))))\n        zcr = zcr / (2 * numpy.float64(len(signal) - 1.0))\n        return zcr\n\n    \'\'\'\n    Computes signal energy of frame\n    \'\'\'\n    @staticmethod\n    def energy(signal):\n        energy = numpy.sum(signal ** 2) / numpy.float64(len(signal))\n        return energy\n\n    \'\'\'\n    Computes entropy of energy\n    \'\'\'\n    @staticmethod\n    def energy_entropy(signal, n_short_blocks=10, eps=10e-8):\n\n        # Total frame energy\n        energy = numpy.sum(signal ** 2)\n        sub_win_len = int(numpy.floor(len(signal) / n_short_blocks))\n\n        # Length of sub-frame\n        if len(signal) != sub_win_len * n_short_blocks:\n            signal = signal[0:sub_win_len * n_short_blocks]\n\n        # Get sub windows\n        sub_wins = signal.reshape(sub_win_len, n_short_blocks, order=\'F\').copy()\n\n        # Compute normalized sub-frame energies:\n        sub_energies = numpy.sum(sub_wins ** 2, axis=0) / (energy + eps)\n\n        # Compute entropy of the normalized sub-frame energies:\n        entropy = -numpy.sum(sub_energies * numpy.log2(sub_energies + eps))\n\n        return entropy\n\n    \'\'\'\n    Computes spectral centroid of frame\n    \'\'\'\n    @staticmethod\n    def spectral_centroid_spread(fft, fs, eps=10e-8):\n\n        # Sample range\n        sr = (numpy.arange(1, len(fft) + 1)) * (fs / (2.0 * len(fft)))\n\n        # Normalize fft coefficients by the max value\n        norm_fft = fft / (fft.max() + eps)\n\n        # Centroid:\n        C = numpy.sum(sr * norm_fft) / (numpy.sum(norm_fft) + eps)\n\n        # Spread:\n        S = numpy.sqrt(numpy.sum(((sr - C) ** 2) * norm_fft) / (numpy.sum(norm_fft) + eps))\n\n        # Normalize:\n        C = C / (fs / 2.0)\n        S = S / (fs / 2.0)\n\n        return C, S\n\n    \'\'\'\n    Computes the spectral flux feature\n    \'\'\'\n    @staticmethod\n    def spectral_flux(fft, fft_prev, eps=10e-8):\n\n        # Sum of fft coefficients\n        sum_fft = numpy.sum(fft + eps)\n\n        # Sum of previous fft coefficients\n        sum_fft_prev = numpy.sum(fft_prev + eps)\n\n        # Compute the spectral flux as the sum of square distances\n        flux = numpy.sum((fft / sum_fft - fft_prev / sum_fft_prev) ** 2)\n\n        return flux\n\n    \'\'\'\n    Computes the spectral roll off\n    \'\'\'\n    @staticmethod\n    def spectral_rolloff(fft, c=0.90, eps=10e-8):\n\n        # Total energy\n        energy = numpy.sum(fft ** 2)\n\n        # Roll off threshold\n        threshold = c * energy\n\n        # Compute cumulative energy\n        cum_energy = numpy.cumsum(fft ** 2) + eps\n\n        # Find the spectral roll off as the frequency position\n        [roll_off, ] = numpy.nonzero(cum_energy > threshold)\n\n        # Normalize\n        if len(roll_off) > 0:\n            roll_off = numpy.float64(roll_off[0]) / (float(len(fft)))\n        else:\n            roll_off = 0.0\n\n        return roll_off\n\n    \'\'\'\n    Computes the Filter Bank coefficients\n    \'\'\'\n    @staticmethod\n    def filter_banks_coeff(signal, sample_rate, nb_filt=40, nb_fft=512):\n\n        # Magnitude of the FFT\n        mag_frames = numpy.absolute(numpy.fft.rfft(signal, nb_fft))\n\n        # Power Spectrum\n        pow_frames = ((1.0 / nb_fft) * (mag_frames ** 2))\n        low_freq_mel = 0\n\n        # Convert Hz to Mel\n        high_freq_mel = (2595 * numpy.log10(1 + (sample_rate / 2) / 700))\n\n        # Equally spaced in Mel scale\n        mel_points = numpy.linspace(low_freq_mel, high_freq_mel, nb_filt + 2)\n\n        # Convert Mel to Hz\n        hz_points = (700 * (10 ** (mel_points / 2595) - 1))\n        bin = numpy.floor((nb_fft + 1) * hz_points / sample_rate)\n\n        # Calculate filter banks\n        fbank = numpy.zeros((nb_filt, int(numpy.floor(nb_fft / 2 + 1))))\n        for m in range(1, nb_filt + 1):\n\n            # left\n            f_m_minus = int(bin[m - 1])\n\n            # center\n            f_m = int(bin[m])\n\n            # right\n            f_m_plus = int(bin[m + 1])\n\n            for k in range(f_m_minus, f_m):\n                fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n            for k in range(f_m, f_m_plus):\n                fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n        filter_banks = numpy.dot(pow_frames, fbank.T)\n\n        # Numerical Stability\n        filter_banks = numpy.where(filter_banks == 0, numpy.finfo(float).eps, filter_banks)\n\n        # dB\n        filter_banks = 20 * numpy.log10(filter_banks)\n\n        return filter_banks\n\n    \'\'\'\n    Computes the MFCCs\n    \'\'\'\n    def mfcc(self, signal, sample_rate, nb_coeff=12, nb_filt=40, nb_fft=512, return_fbank=False):\n\n        # Apply filter bank on spectogram\n        filter_banks = self.filter_banks_coeff(signal, sample_rate, nb_filt=nb_filt, nb_fft=nb_fft)\n\n        # Compute MFCC coefficients\n        mfcc = dct(filter_banks, type=2, axis=-1, norm=\'ortho\')[1: (nb_coeff + 1)]\n\n        # Return MFFCs and Filter banks coefficients\n        if return_fbank is True:\n            return numpy.concatenate((mfcc, filter_banks))\n        else:\n            return mfcc\n\n    \'\'\'\n    Compute statistics on short time features\n    \'\'\'\n    @staticmethod\n    def compute_statistic(seq, statistic):\n        if statistic == \'mean\':\n            S = numpy.mean(seq)\n        elif statistic == \'med\':\n            S = numpy.median(seq)\n        elif statistic == \'std\':\n            S = numpy.std(seq)\n        elif statistic == \'kurt\':\n            S = kurtosis(seq)\n        elif statistic == \'skew\':\n            S = skew(seq)\n        elif statistic == \'min\':\n            S = numpy.min(seq)\n        elif statistic == \'max\':\n            S = numpy.max(seq)\n        elif statistic == \'q1\':\n            S = numpy.percentile(seq, 1)\n        elif statistic == \'q99\':\n            S = numpy.percentile(seq, 99)\n        elif statistic == \'range\':\n            S = numpy.abs(numpy.percentile(seq, 99) - numpy.percentile(seq, 1))\n        return S\n\n    \'\'\'\n    Compute short time features on signal\n    \'\'\'\n    def compute_st_features(self, feature, signal, dft, dft_prev, sample_rate):\n        if feature == \'zcr\':\n            F = self.zcr(signal)\n        elif feature == \'energy\':\n            F = self.energy(signal)\n        elif feature == \'energy_entropy\':\n            F = self.energy_entropy(signal)\n        elif feature == \'spectral_centroid\':\n            [F, FF] = self.spectral_centroid_spread(dft, sample_rate)\n        elif feature == \'spectral_spread\':\n            [FF, F] = self.spectral_centroid_spread(dft, sample_rate)\n        elif feature == \'spectral_entropy\':\n            F = self.energy_entropy(dft)\n        elif feature == \'spectral_flux\':\n            F = self.spectral_flux(dft, dft_prev)\n        elif feature == \'sprectral_rolloff\':\n            F = self.spectral_rolloff(dft)\n        return F\n'"
01-Audio/Notebook/SVM/AudioLibrary/AudioSignal.py,0,"b'import os\nimport numpy\nfrom pydub import AudioSegment\nfrom scipy.fftpack import fft\n\n\nclass AudioSignal(object):\n\n    def __init__(self, sample_rate, signal=None, filename=None):\n\n        # Set sample rate\n        self._sample_rate = sample_rate\n\n        if signal is None:\n\n            # Get file name and file extension\n            file, file_extension = os.path.splitext(filename)\n\n            # Check if file extension if audio format\n            if file_extension in [\'.mp3\', \'.wav\']:\n\n                # Read audio file\n                self._signal = self.read_audio_file(filename)\n\n            # Check if file extension if video format\n            elif file_extension in [\'.mp4\', \'.mkv\', \'avi\']:\n\n                # Extract audio from video\n                new_filename = self.extract_audio_from_video(filename)\n\n                # read audio file from extracted audio file\n                self._signal = self.read_audio_file(new_filename)\n\n            # Case file extension is not supported\n            else:\n                print(""Error: file not found or file extension not supported."")\n\n        elif filename is None:\n\n            # Cast signal to array\n            self._signal = signal\n\n        else:\n\n            print(""Error : argument missing in AudioSignal() constructor."")\n\n    \'\'\'\n    Function to extract audio from a video\n    \'\'\'\n    def extract_audio_from_video(self, filename):\n\n        # Get video file name and extension\n        file, file_extension = os.path.splitext(filename)\n\n        # Extract audio (.wav) from video\n        os.system(\'ffmpeg -i \' + file + file_extension + \' \' + \'-ar \' + str(self._sample_rate) + \' \' + file + \'.wav\')\n        print(""Sucessfully converted {} into audio!"".format(filename))\n\n        # Return audio file name created\n        return file + \'.wav\'\n\n    \'\'\'\n    Function to read audio file and to return audio samples of a specified WAV file\n    \'\'\'\n    def read_audio_file(self, filename):\n\n        # Get audio signal\n        audio_file = AudioSegment.from_file(filename)\n\n        # Resample audio signal\n        audio_file = audio_file.set_frame_rate(self._sample_rate)\n\n        # Cast to integer\n        if audio_file.sample_width == 2:\n            data = numpy.fromstring(audio_file._data, numpy.int16)\n        elif audio_file.sample_width == 4:\n            data = numpy.fromstring(audio_file._data, numpy.int32)\n\n        # Merge audio channels\n        audio_signal = []\n        for chn in list(range(audio_file.channels)):\n            audio_signal.append(data[chn::audio_file.channels])\n        audio_signal = numpy.array(audio_signal).T\n\n        # Flat signals\n        if audio_signal.ndim == 2:\n            if audio_signal.shape[1] == 1:\n                audio_signal = audio_signal.flatten()\n\n        # Convert stereo to mono\n        audio_signal = self.stereo_to_mono(audio_signal)\n\n        # Return sample rate and audio signal\n        return audio_signal\n\n    \'\'\'\n    Function to convert an input signal from stereo to mono\n    \'\'\'\n    @staticmethod\n    def stereo_to_mono(audio_signal):\n\n        # Check if signal is stereo and convert to mono\n        if isinstance(audio_signal, int):\n            return -1\n        if audio_signal.ndim == 1:\n            return audio_signal\n        elif audio_signal.ndim == 2:\n            if audio_signal.shape[1] == 1:\n                return audio_signal.flatten()\n            else:\n                if audio_signal.shape[1] == 2:\n                    return (audio_signal[:, 1] / 2) + (audio_signal[:, 0] / 2)\n                else:\n                    return -1\n\n    \'\'\'\n    Function to split the input signal into windows of same size\n    \'\'\'\n    def framing(self, size, step, hamming=False):\n\n        # Rescale windows step and size\n        win_size = int(size * self._sample_rate)\n        win_step = int(step * self._sample_rate)\n\n        # Number of frames\n        nb_frames = 1 + int((len(self._signal) - win_size) / win_step)\n\n        # Build Hamming function\n        if hamming is True:\n            ham = numpy.hamming(win_size)\n        else:\n            ham = numpy.ones(win_size)\n\n        # Split signals (and multiply each windows signals by Hamming functions)\n        frames = []\n        for t in range(nb_frames):\n            sub_signal = AudioSignal(self._sample_rate, signal=self._signal[(t * win_step): (t * win_step + win_size)] * ham)\n            frames.append(sub_signal)\n        return frames\n\n    \'\'\'\n    Function to compute the magnitude of the Discrete Fourier Transform coefficient\n    \'\'\'\n    def dft(self, norm=False):\n\n        # Commpute the magnitude of the spectrum (and normalize by the number of sample)\n        if norm is True:\n            dft = abs(fft(self._signal)) / len(self._signal)\n        else:\n            dft = abs(fft(self._signal))\n        return dft\n\n    \'\'\'\n    Function to apply pre-emphasis filter on signal\n    \'\'\'\n    def pre_emphasis(self, alpha =0.97):\n\n        # Emphasized signal\n        emphasized_signal = numpy.append(self._signal[0], self._signal[1:] - alpha * self._signal[:-1])\n\n        return emphasized_signal\n'"
