file_path,api_count,code
dataset.py,46,"b""from PIL import Image\nfrom glob import glob\nimport pickle\nimport tensorflow as tf\nimport numpy as np\n\nIMG_SIZE = 120\nFLAGS = None\nDEBUG = None\n\n\ndef batch_masks(global_step, height, width, min_opacity, max_opacity):\n    return tf.concat([\n        tf.expand_dims(create_mask(\n            global_step, height, width, min_opacity, max_opacity), 0)\n        for _ in range(FLAGS.batch_size)], 0)\n\n\ndef create_mask(global_step, height, width, min_opacity, max_opacity):\n    if global_step:\n        min_opacity = tf.train.polynomial_decay(\n            max_opacity, global_step,\n            decay_steps=60000, end_learning_rate=min_opacity)\n\n    mask_h = tf.random_uniform([], int(height * .7), int(height * .9), tf.int32)\n    mask_w = tf.random_uniform([], int(width * .1), int(width * .3), tf.int32)\n    opacity = tf.random_uniform([], min_opacity, max_opacity, tf.float32)\n    max_angle = tf.random_uniform([], -1.5, 1.5, tf.float32)\n\n    mask = tf.ones([mask_h, mask_w]) * opacity\n    mask *= tf.cast(tf.random_uniform([], 0, 2, tf.int32) * 2 - 1, tf.float32)\n    y_pos = tf.random_uniform([], 0, height - mask_h, tf.int32)\n    x_pos = tf.random_uniform([], 0, width - mask_w, tf.int32)\n    mask = tf.pad(mask, [[y_pos, height - mask_h - y_pos],\n                         [x_pos, width - mask_w - x_pos]])  # Costly\n    mask = tf.expand_dims(mask, 2)\n    mask.set_shape([height, width, 1])\n    mask = tf.contrib.image.rotate(\n        mask, tf.random_uniform([], -max_angle, max_angle, tf.float32))  # Costly\n    return mask\n\n\ndef dataset_paths(paths):\n    def load_image(path):\n        image = tf.image.decode_image(tf.read_file(path))\n        image = tf.cast(image, tf.float32) / 255\n        image = tf.image.resize_image_with_crop_or_pad(image, 300, 300)\n        image.set_shape([300, 300, 3])\n        return image\n    dataset = tf.Dataset.from_tensor_slices(tf.constant(paths))\n    dataset = dataset.map(load_image)\n    dataset = dataset.batch(FLAGS.batch_size)\n    dataset = dataset.shuffle(buffer_size=5000)\n    dataset = dataset.repeat()\n    iterator = dataset.make_initializable_iterator()\n    next_element = iterator.get_next()\n    tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)\n    return next_element, lambda _: None\n\n\ndef get_records():\n    records = glob('data/*.tfrecords')\n    if not records:\n        convert_to_record()\n        records = glob('data/*.tfrecords')\n    return records\n\n\ndef dataset_split(dataset_fn, split):\n    records = get_records()\n    split = int(len(records) * split)\n    train, val = dataset_fn(records[:split]), dataset_fn(records[split:])\n    iterator = tf.data.Iterator.from_structure(\n        train.output_types, train.output_shapes)\n    next_element = iterator.get_next()\n    return (next_element,\n            [iterator.make_initializer(x) for x in [train, val]])\n\n\ndef dataset_voc2012():\n    records = get_records()\n    dataset = dataset_voc2012_rec(records)\n    iterator = dataset.make_initializable_iterator()\n    next_element = iterator.get_next()\n    tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)\n    return next_element, lambda x: None\n\n\ndef dataset_voc2012_rec(records):\n\n    def parse_function(serialized):\n        features = tf.parse_single_example(serialized, features=dict(\n            height=tf.FixedLenFeature([], tf.int64),\n            width=tf.FixedLenFeature([], tf.int64),\n            image_raw=tf.FixedLenFeature([], tf.string)))\n        image = tf.reshape(tf.decode_raw(features['image_raw'], tf.uint8),\n                           [tf.cast(features['height'], tf.int32),\n                            tf.cast(features['width'], tf.int32),\n                            3])\n        image = tf.cast(image, tf.float32) / 255\n        image = tf.cond(\n            tf.logical_and(tf.greater(tf.shape(image)[0], IMG_SIZE),\n                           tf.greater(tf.shape(image)[1], IMG_SIZE)),\n            lambda: tf.random_crop(image, [IMG_SIZE, IMG_SIZE, 3]),\n            lambda: tf.image.resize_image_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE),)\n        image.set_shape([IMG_SIZE, IMG_SIZE, 3])\n        image = tf.image.random_flip_left_right(image)\n        return image\n\n    dataset = tf.data.TFRecordDataset(records)\n    dataset = dataset.map(parse_function)\n    dataset = dataset.batch(FLAGS.batch_size)\n    dataset = dataset.filter(\n        lambda batch: tf.equal(tf.shape(batch)[0], FLAGS.batch_size))\n    dataset = dataset.shuffle(buffer_size=10000)\n    dataset = dataset.repeat()\n    return dataset\n\n\ndef dataset_cifar():\n    images = standardize(get_images())\n    images_placeholder = tf.placeholder(tf.float32, images.shape)\n    dataset = tf.data.Dataset.from_tensor_slices(images_placeholder)\n    dataset = dataset.map(tf.image.random_flip_left_right)\n    dataset = dataset.batch(FLAGS.batch_size)\n    dataset = dataset.filter(\n        lambda batch: tf.equal(tf.shape(batch)[0], FLAGS.batch_size))\n    dataset = dataset.shuffle(buffer_size=10000)\n    dataset = dataset.repeat()\n    iterator = dataset.make_initializable_iterator()\n    next_element = iterator.get_next()\n\n    def init(sess):\n        sess.run(iterator.initializer, feed_dict={images_placeholder: images})\n    return next_element, init\n\n\ndef get_images():\n    data = None\n    for path in glob('cifar-10-batches-py/data_batch_*'):\n        with open(path, 'rb') as fo:\n            binary = pickle.load(fo, encoding='bytes')\n        if data is None:\n            data = binary[b'data']\n        else:\n            data = np.concatenate((data, binary[b'data']))\n        if DEBUG:\n            break\n    data = data.reshape((-1, 3, 32, 32)).transpose((0, 2, 3, 1))\n    return data\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef convert_to_record():\n    print('Creating TFRecords')\n    filenames = glob('data/VOCdevkit/VOC2012/JPEGImages/*.jpg')\n    if not filenames:\n        print('Source images are missing!')\n    writer = None\n    for i, filename in enumerate(filenames):\n        if i % 200 == 0:\n            print(i)\n            if writer:\n                writer.close()\n            # Recommanded size is 100mb\n            writer = tf.python_io.TFRecordWriter('data/voc-%s.tfrecords' % i)\n        image = np.array(Image.open(filename))\n        example = tf.train.Example(features=tf.train.Features(\n            feature=dict(\n                height=_int64_feature(image.shape[0]),\n                width=_int64_feature(image.shape[1]),\n                image_raw=_bytes_feature(image.tostring()))))\n        writer.write(example.SerializeToString())\n    writer.close()\n\n\ndef standardize(batch):\n    return (batch.astype(np.float32) / 255)\n\n\ndef unstandardize(batch):\n    return (batch * 255).astype(np.uint8)\n"""
tests.py,4,"b""import tensorflow as tf\nimport numpy as np\nimport os\nimport watermarks as w\nfrom glob import glob\nfrom datetime import datetime\n\nw.DEBUG = True\n\n\nclass WatermarkTest(tf.test.TestCase):\n\n    def setUp(self):\n        w.FLAGS.logdir = '/tmp/tensorflow_log/%s' % datetime.now()\n        w.FLAGS.logdir = '/tmp/tensorflow_log'\n        w.FLAGS.batch_size = 4\n        w.FLAGS.learning_rate = 1e-1\n        [os.remove(x) for x in glob(w.FLAGS.logdir + '/events*')]\n\n    ########\n    # Data #\n    ########\n\n    def test_dataset_paths(self):\n        w.FLAGS.batch_size = 2\n        with self.test_session() as sess:\n            np.set_printoptions(threshold=np.nan)\n            x, _ = w.dataset_paths(['assets/cat.png'])\n            sess.run(tf.tables_initializer())\n            self.assertTupleEqual(x.eval().shape, (1, 300, 300, 4))\n\n    def test_dataset_mask(self):\n        w.FLAGS.batch_size = 2\n        with self.test_session():\n            np.set_printoptions(threshold=np.nan)\n            mask = w.batch_masks(None, 32, 32, .1, .4)\n            self.assertTupleEqual(mask.eval().shape, (2, 32, 32, 1))\n\n    def test_dataset_cifar(self):\n        w.FLAGS.batch_size = 2\n        with self.test_session() as sess:\n            np.set_printoptions(threshold=np.nan)\n            x, init = w.dataset_cifar()\n            init(sess)\n            self.assertTupleEqual(x.eval().shape, (2, 32, 32, 3))\n\n    def test_dataset_split(self):\n        w.FLAGS.batch_size = 2\n        with self.test_session() as sess:\n            x, iterator_inits = w.dataset_split(w.dataset_voc2012_rec, .8)\n            sess.run(iterator_inits[0])\n            self.assertTupleEqual(x.eval().shape, (2, 120, 120, 3))\n\n    def test_dataset_voc2012(self):\n        w.FLAGS.batch_size = 2\n        with self.test_session() as sess:\n            x, init = w.dataset_voc2012()\n            sess.run(tf.tables_initializer())\n            self.assertTupleEqual(x.eval().shape, (2, 120, 120, 3))\n\n    ############\n    # Pipeline #\n    ############\n\n    def test_training(self):\n        with self.test_session() as sess:\n            w.train(sess, w.dataset_voc2012_rec)\n\n    def test_inference_voc(self):\n        with self.test_session() as sess:\n            dv = w.dataset_voc2012\n            results = w.inference(sess, dv, 1)\n            self.assertTupleEqual(results[0].shape, (4, 120, 120, 3))\n\n    def test_inference_other(self):\n        with self.test_session() as sess:\n            def d_cherry(): return w.dataset_paths(['assets/cat.png', ])\n\n            def dm(): return w.dataset_paths(['assets/empty.png', ])\n\n            def ds(): return w.dataset_paths(['assets/cat-selection.png', ])\n            results = w.inference(sess, d_cherry, 1, dm, ds)\n            self.assertTupleEqual(results[0].shape, (1, 300, 300, 3))\n\n\nif __name__ == '__main__':\n    tf.test.main()\n"""
watermarks.py,65,"b'from dataset import batch_masks, unstandardize, dataset_paths, dataset_voc2012, dataset_split, dataset_voc2012_rec  # noqa\nfrom io import BytesIO\nfrom time import time\nimport IPython.display\nimport PIL.Image\nimport dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport tensorflow as tf\n\ntf.flags.DEFINE_string(\'logdir\', None, \'Log directory\')\ntf.flags.DEFINE_integer(""batch_size"", 32, ""Batch size"")\ntf.flags.DEFINE_float(""learning_rate"", .005, ""Learning rate"")\ntf.flags.DEFINE_string(""dataset"", \'dataset_voc2012_rec\', ""Dataset to use"")\ntf.flags.DEFINE_string(\'image\', None, \'Image with watermark\')\ntf.flags.DEFINE_string(\'selection\', None, \'Where to do the removal\')\n\nFLAGS = tf.flags.FLAGS\ndataset.FLAGS = FLAGS\nDEBUG = False\ndataset.DEBUG = DEBUG\n\n#########\n# Model #\n#########\n\n\ndef dense_block(net, growth_rate, channels_init, layers, training):\n    # Dense block https://github.com/liuzhuang13/DenseNet/blob/master/models/DenseConnectLayer.lua\n    # Receptive field: l * (k - 1) + k\n    for i, channels in enumerate(\n            [channels_init + i * growth_rate for i in range(layers)]):\n        # 3x3 convolution\n        previous_input = net\n        net = tf.layers.batch_normalization(net, training=training)\n        net = tf.nn.relu(net)\n\n        # Bottleneck\n        if channels > 4 * growth_rate:\n            net = tf.layers.conv2d(\n                net,\n                4 * growth_rate, 1,\n                padding=\'same\',\n                activation=None,)\n            net = tf.layers.batch_normalization(net, training=training)\n            net = tf.nn.relu(net)\n\n        net = tf.layers.conv2d(\n            net,\n            growth_rate, 3,\n            padding=\'same\',\n            activation=None,)\n        net = tf.concat([previous_input, net], axis=3)\n    return net\n\n\ndef selection_margin(masks, margin):\n    selection = tf.nn.conv2d(masks, tf.ones([\n        margin * 2 + 1, margin * 2 + 1, 1, 1]), [1, 1, 1, 1], \'SAME\')\n    selection = tf.clip_by_value(tf.abs(tf.ceil(selection)), 0, 1)\n    return selection\n\n\ndef atrous_conv2d(inputs, filters, kernel_size, rate):\n    shape = [kernel_size, kernel_size, inputs.shape.as_list()[-1], filters]\n    initializer = tf.contrib.layers.xavier_initializer()\n    weight = tf.Variable(initializer(shape))\n    conv = tf.nn.atrous_conv2d(inputs, weight, rate, \'SAME\')\n    return conv\n\n\ndef model(images, training):\n    growth_rate = 16\n    channels_init = growth_rate * 2\n    bottleneck_channels = 32\n\n    net = tf.layers.conv2d(images, channels_init, 3,\n                           padding=\'same\', activation=None,)\n\n    net = dense_block(net, growth_rate, channels_init, 4, training)\n    net = tf.layers.batch_normalization(net, training=training)\n    net = tf.nn.relu(net)\n\n    # Bottleneck\n    net = tf.layers.conv2d(\n        net, bottleneck_channels, 1, padding=\'same\', activation=None,)\n    net = tf.layers.batch_normalization(net, training=training)\n    net = tf.nn.relu(net)\n\n    # Dilation layers to increase the receptive field\n    # http://vladlen.info/papers/DRN.pdf\n    net = atrous_conv2d(net, bottleneck_channels, 3, 2)\n    net = tf.layers.batch_normalization(net, training=training)\n    net = tf.nn.relu(net)\n\n    net = atrous_conv2d(net, bottleneck_channels, 3, 4)\n    net = tf.layers.batch_normalization(net, training=training)\n    net = tf.nn.relu(net)\n\n    net = atrous_conv2d(net, bottleneck_channels, 3, 2)\n    net = tf.layers.batch_normalization(net, training=training)\n    net = tf.nn.relu(net)\n\n    net = tf.layers.conv2d(net, 3, 3, padding=\'same\', activation=None)\n    return net\n\n#############\n# Inference #\n#############\n\n\ndef inference(sess, dataset, passes=1,\n              dataset_mask=False, dataset_selection=False,\n              min_opacity=.15, max_opacity=.4):\n    # Data sources\n    next_image, iterator_init = dataset()\n    image_shape = next_image.shape.as_list()\n    if dataset_mask:\n        next_mask, iterator_init = dataset_mask()\n        next_mask = next_mask[:, :, :, 0:1]\n    else:\n        next_mask = batch_masks(\n            None, image_shape[1], image_shape[2], min_opacity, max_opacity)\n    if dataset_selection:\n        next_selection, iterator_init = dataset_selection()\n\n    # Model\n    images_p = tf.placeholder(tf.float32, shape=[None] + image_shape[1:])\n    mask_p = tf.placeholder(tf.float32, shape=[None] + image_shape[1:3] + [1])\n    selection_p = tf.placeholder(tf.float32, shape=[None] + image_shape[1:3] + [1])\n\n    image_w = tf.clip_by_value(images_p - mask_p, 0, 1)\n    selection_conv = selection_margin(mask_p, 4)\n    predictions = model(image_w, False) * selection_p\n    gen_mask = tf.clip_by_value(tf.abs(predictions), 0, 1)\n    reconstruction = tf.clip_by_value(image_w + predictions, 0, 1)\n    accuracy = get_accuracy(reconstruction, images_p)\n\n    # Inference\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.tables_initializer())\n    iterator_init(sess)\n\n    try:\n        saver = tf.train.Saver()\n        saver.restore(sess, ""/tmp/model.ckpt"")\n    except Exception as e:\n        print(\'An error occurred when trying to restore : %s\' % e)\n\n    # Pass 1\n    batch = sess.run([next_image, next_mask])\n    if dataset_selection:\n        selection = sess.run(tf.expand_dims(next_selection[:, :, :, 0], 3))\n    else:\n        selection = sess.run(selection_conv, feed_dict={mask_p: batch[1]})\n\n    feed_dict = {images_p: batch[0][:, :, :, 0:3],\n                 mask_p: batch[1][:batch[0].shape[0]],\n                 selection_p: selection[:batch[0].shape[0]]}\n    images = sess.run([image_w, reconstruction, gen_mask, accuracy], feed_dict=feed_dict)\n    results = [images[0], images[1]]\n    print(\'Mean accuracy %.3f%%\' % (images[3] * 100))\n\n    # Pass 2\n    for _ in range(1, passes):\n        reconstruction1 = images[1]\n        feed_dict = {images_p: reconstruction1,\n                     mask_p: np.zeros(list(reconstruction1.shape[:-1]) + [1])}\n        images = sess.run([image_w, reconstruction, gen_mask], feed_dict=feed_dict)\n        results += [images[1]]\n\n    # Reformat\n    results += [images[2]]\n    images = [unstandardize(x) for x in results]\n    return images\n\n############\n# Training #\n############\n\n\ndef train(sess, dataset, min_opacity=.15, max_opacity=.4):\n    global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n    training = tf.placeholder(tf.bool, shape=[])\n    with tf.device(\'/cpu:0\'):\n        next_image, iterator_inits = dataset_split(dataset, .8)\n\n    masks = batch_masks(\n        global_step, next_image.shape.as_list()[1], next_image.shape.as_list()[2],\n        min_opacity, max_opacity)\n    image_w = tf.clip_by_value(next_image - masks, 0, 1)\n    predictions = model(image_w, training) * selection_margin(masks, 4)\n    tf.summary.image(\'masks\', predictions)\n\n    # Define loss\n    image_mask = -(image_w - next_image)  # Mask after application on the image\n    abs_loss = tf.losses.absolute_difference(\n        predictions, image_mask, loss_collection=None)**.5\n    tf.losses.add_loss(abs_loss)\n    loss = tf.losses.get_total_loss(True)\n    tf.summary.scalar(\'loss\', loss)\n\n    # Optimizer\n    learning_rate = tf.train.polynomial_decay(\n        FLAGS.learning_rate, global_step,\n        decay_steps=60000, end_learning_rate=.0005)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_op = tf.train.AdamOptimizer(learning_rate).minimize(\n            loss,\n            global_step=global_step)\n\n    # Training loop\n    sess.run(tf.global_variables_initializer())\n    sess.run(iterator_inits[0])\n\n    saver = tf.train.Saver()\n    summaries = tf.summary.merge_all()\n    train_writer = tf.summary.FileWriter(os.path.join(FLAGS.logdir, \'train\'), sess.graph)\n    val_writer = tf.summary.FileWriter(os.path.join(FLAGS.logdir, \'val\'), sess.graph)\n\n    for i in range(1, 2 if DEBUG else int(1e6)):\n        if DEBUG:\n            start_time = time()\n            _, loss_, predictions_ = sess.run([train_op, loss, predictions],\n                                              feed_dict={training: True})\n            batch_time = 1000 * (time() - start_time) / FLAGS.batch_size\n            print(\'Time %dms, Loss %f\' % (batch_time, loss_))\n            continue\n\n        _, summaries_, global_step_ = sess.run(\n            [train_op, summaries, global_step], feed_dict={training: True})\n        train_writer.add_summary(summaries_, global_step_)\n\n        # Save model\n        if i % 2000 == 0:\n            path = saver.save(sess, ""/tmp/model.ckpt"")\n            print(i, \'Saving at\', path)\n            sess.run(iterator_inits[1])  # switch to validation dataset\n            while True:\n                try:\n                    _, summaries_ = sess.run([loss, summaries],\n                                             feed_dict={training: False})\n                    val_writer.add_summary(summaries_, global_step_)\n                except tf.errors.OutOfRangeError:\n                    break\n            sess.run(iterator_inits[0])\n    return\n\n\n###########\n# Helpers #\n###########\n\ndef get_accuracy(prediction, target):\n    diff = tf.reduce_mean(tf.abs(prediction - target) / 255, [1, 2, 3])\n    return (1 - tf.reduce_mean(diff))\n\n\ndef show_array(a, fmt=\'png\'):\n    a = np.uint8(a)\n    f = BytesIO()\n    PIL.Image.fromarray(a).save(f, fmt)\n    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n\n\ndef show_images(images):\n    for i in range(images.shape[0]):\n        if images.shape[1] <= 32:\n            plt.figure()\n            plt.imshow(images[i], interpolation=\'nearest\')\n        else:\n            show_array(images[i])\n\n#######\n# Run #\n#######\n\n\ndef main(_):\n    with tf.Session() as sess:\n        if FLAGS.image and FLAGS.selection:\n            images = inference(\n                sess,\n                lambda: dataset_paths([FLAGS.image]),\n                0,\n                lambda: dataset_paths([\'assets/empty.png\']),\n                lambda: dataset_paths([FLAGS.selection]))\n            image = np.squeeze(images[1])\n            PIL.Image.fromarray(image).save(\'output.png\')\n        else:\n            train(sess, globals()[FLAGS.dataset])\n    return\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
