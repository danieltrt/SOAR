file_path,api_count,code
tensorflow_model/convnet.py,59,"b'# needed libraries\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nlogs_path = \'/tmp/tensorflow_logs/convnet\'\n\n# mnist.train = 55,000 input data\n# mnist.test = 10,000 input data\n# mnist.validate = 5,000 input data\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n\n# Implementing Convnet with TF\ndef weight_variable(shape, name=None):\n    # break simmetry\n    if name:\n        w = tf.truncated_normal(shape, stddev=0.1, name=name)\n    else:\n        w = tf.truncated_normal(shape, stddev=0.1)\n\n    return tf.Variable(w)\n\n\ndef bias_variable(shape, name=None):\n    # avoid dead neurons\n    if name:\n        b = tf.constant(0.1, shape=shape, name=name)\n    else:\n        b = tf.constant(0.1, shape=shape)\n    return tf.Variable(b)\n\n\n# pool\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1], padding=\'SAME\')\n\ndef new_conv_layer(x, w):\n\treturn tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n# our network!!!\n\ng = tf.Graph()\n\nwith g.as_default():\n\n\t# input data\n\tx = tf.placeholder(tf.float32, shape=[None, 28*28], name=\'input_data\')\n\tx_image = tf.reshape(x, [-1, 28, 28, 1])\n\t# correct labels\n\ty_ = tf.placeholder(tf.float32, shape=[None, 10], name=\'correct_labels\')\n\n\t# fist conv layer\n\twith tf.name_scope(\'convLayer1\'):\n\t\tw1 = weight_variable([5, 5, 1, 32])\n\t\tb1 = bias_variable([32])\n\t\tconvlayer1 = tf.nn.relu(new_conv_layer(x_image, w1) + b1)\n\t\tmax_pool1 = max_pool_2x2(convlayer1)\n\n\t# second conv layer\n\twith tf.name_scope(\'convLayer2\'):\n\t\tw2 = weight_variable([5, 5, 32, 64])\n\t\tb2 = bias_variable([64])\n\t\tconvlayer2 = tf.nn.relu(new_conv_layer(max_pool1, w2) + b2)\n\t\tmax_pool2 = max_pool_2x2(convlayer2)\n\n\t# flat layer\n\twith tf.name_scope(\'flattenLayer\'):\n\t\tflat_layer = tf.reshape(max_pool2, [-1, 7 * 7 * 64])\n\n\t# fully connected layer\n\twith tf.name_scope(\'FullyConnectedLayer\'):\n\t\twfc1 = weight_variable([7 * 7 * 64, 1024])\n\t\tbfc1 = bias_variable([1024])\n\t\tfc1 = tf.nn.relu(tf.matmul(flat_layer, wfc1) + bfc1)\n\n\t# DROPOUT\n\twith tf.name_scope(\'Dropout\'):\n\t\tkeep_prob = tf.placeholder(tf.float32)\n\t\tdrop_layer = tf.nn.dropout(fc1, keep_prob)\n\n\t# final layer\n\twith tf.name_scope(\'FinalLayer\'):\n\t\tw_f = weight_variable([1024, 10])\n\t\tb_f = bias_variable([10])\n\t\ty_f = tf.matmul(drop_layer, w_f) + b_f\n\t\ty_f_softmax = tf.nn.softmax(y_f)\n\n\t# loss\n\tloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  logits=y_f))\n\n\t# train step\n\ttrain_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n\n\t# accuracy\n\tcorrect_prediction = tf.equal(tf.argmax(y_f_softmax, 1), tf.argmax(y_, 1))\n\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n\t# Create a summary to monitor loss tensor\n\ttf.summary.scalar(""loss"", loss)\n\t# Create a summary to monitor accuracy tensor\n\ttf.summary.scalar(""accuracy"", accuracy)\n\t# Merge all summaries into a single op\n\tmerged_summary_op = tf.summary.merge_all()\n\n\t# init\n\tinit = tf.global_variables_initializer()\n\n\t# Running the graph\n\n\tnum_steps = 3000\n\tbatch_size = 16\n\ttest_size = 10000\n\ttest_accuracy = 0.0\n\n\tsess = tf.Session()\n\n\tsess.run(init)\n\t# op to write logs to Tensorboard\n\tsummary_writer = tf.summary.FileWriter(logs_path,\n\t\t\t\t\t\t\t\t\t\t   graph=tf.get_default_graph())\n\n\tfor step in range(num_steps):\n\t\tbatch = mnist.train.next_batch(batch_size)\n\n\t\tts, error, acc, summary = sess.run([train_step, loss, accuracy,\n\t\t\t\t\t\t\t\t\t\t\tmerged_summary_op],\n\t\t\t\t\t\t\t\t\t\t   feed_dict={x: batch[0],\n\t\t\t\t\t\t\t\t\t\t\t\t\t  y_: batch[1],\n\t\t\t\t\t\t\t\t\t\t\t\t\t  keep_prob: 0.5})\n\t\tif step % 100 == 0:\n\t\t\ttrain_accuracy = accuracy.eval({\n\t\t\t\tx: batch[0], y_: batch[1], keep_prob: 1.0}, sess)\n\t\t\tprint(\'step %d, training accuracy %f\' % (step, train_accuracy))\n        \'\'\'\n\tprint \'Done!\'\n\tprint \'Evaluating...\'\n\tfor i in xrange(test_size/50):\n\t\tbatch = mnist.test.next_batch(50)\n\t\tacc = accuracy.eval({x: batch[0], y_: batch[1],\n\t\t\t\t\t\t\t\t\t   keep_prob: 1.0}, sess)\n\t\tif i % 10 == 0:\n\t\t\tprint(\'%d: test accuracy %f\' % (i, acc))\n\t\ttest_accuracy += acc\n\tprint \'avg test accuracy:\', test_accuracy/(test_size/50.0)\n        \'\'\'\n\n# copying variables as constants to export graph\n_w1 = w1.eval(sess)\n_b1 = b1.eval(sess)\n_w2 = w2.eval(sess)\n_b2 = b2.eval(sess)\n_wfc1 = wfc1.eval(sess)\n_bfc1 = bfc1.eval(sess)\n_w_f = w_f.eval(sess)\n_b_f = b_f.eval(sess)\n\nsess.close()\n\ng2 = tf.Graph()\nwith g2.as_default():\n\n\t# input data\n\tx2 = tf.placeholder(tf.float32, shape=[None, 28*28], name=\'input\')\n\tx2_image = tf.reshape(x2, [-1, 28, 28, 1])\n\t# correct labels\n\ty2_ = tf.placeholder(tf.float32, shape=[None, 10])\n\n\tw1_2 = tf.constant(_w1)\n\tb1_2 = tf.constant(_b1)\n\tconvlayer1_2 = tf.nn.relu(new_conv_layer(x2_image, w1_2) + b1_2)\n\tmax_pool1_2 = max_pool_2x2(convlayer1_2)\n\n\tw2_2 = tf.constant(_w2)\n\tb2_2 = tf.constant(_b2)\n\tconvlayer2_2 = tf.nn.relu(new_conv_layer(max_pool1_2, w2_2) + b2_2)\n\tmax_pool2_2 = max_pool_2x2(convlayer2_2)\n\n\t# flat layer\n\tflat_layer_2 = tf.reshape(max_pool2_2, [-1, 7 * 7 * 64])\n\n\t# fully connected layer\n\twfc1_2 = tf.constant(_wfc1)\n\tbfc1_2 = tf.constant(_bfc1)\n\tfc1_2 = tf.nn.relu(tf.matmul(flat_layer_2, wfc1_2) + bfc1_2)\n\n\t# no dropout layer\n\n\t# final layer\n\tw_f_2 = tf.constant(_w_f)\n\tb_f_2 = tf.constant(_b_f)\n\ty_f_2 = tf.matmul(fc1_2, w_f_2) + b_f_2\n\ty_f_softmax_2 = tf.nn.softmax(y_f_2, name=\'output\')\n\n\t# init\n\tinit_2 = tf.global_variables_initializer()\n\n\tsess_2 = tf.Session()\n        init_2 = tf.initialize_all_variables()\n        sess_2.run(init_2)\n\n        graph_def = g2.as_graph_def()\n        tf.train.write_graph(graph_def, \'\', \'graph.pb\', as_text=False)\n\n'"
