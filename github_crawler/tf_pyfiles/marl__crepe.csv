file_path,api_count,code
setup.py,0,"b'import bz2\nimport imp\nimport os\nimport sys\n\nimport pkg_resources\nfrom setuptools import setup, find_packages\n\ntry:\n    from urllib.request import urlretrieve\nexcept ImportError:\n    from urllib import urlretrieve\n\nmodel_capacities = [\'tiny\', \'small\', \'medium\', \'large\', \'full\']\nweight_files = [\'model-{}.h5\'.format(cap) for cap in model_capacities]\nbase_url = \'https://github.com/marl/crepe/raw/models/\'\n\nif len(sys.argv) > 1 and sys.argv[1] == \'sdist\':\n    # exclude the weight files in sdist\n    weight_files = []\nelse:\n    # in all other cases, decompress the weights file if necessary\n    for weight_file in weight_files:\n        weight_path = os.path.join(\'crepe\', weight_file)\n        if not os.path.isfile(weight_path):\n            compressed_file = weight_file + \'.bz2\'\n            compressed_path = os.path.join(\'crepe\', compressed_file)\n            if not os.path.isfile(compressed_file):\n                print(\'Downloading weight file {} ...\'.format(compressed_file))\n                urlretrieve(base_url + compressed_file, compressed_path)\n            print(\'Decompressing ...\')\n            with bz2.BZ2File(compressed_path, \'rb\') as source:\n                with open(weight_path, \'wb\') as target:\n                    target.write(source.read())\n            print(\'Decompression complete\')\n\nversion = imp.load_source(\'crepe.version\', os.path.join(\'crepe\', \'version.py\'))\n\nwith open(\'README.md\') as file:\n    long_description = file.read()\n\nsetup(\n    name=\'crepe\',\n    version=version.version,\n    description=\'CREPE pitch tracker\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    url=\'https://github.com/marl/crepe\',\n    author=\'Jong Wook Kim and Justin Salamon\',\n    author_email=\'jongwook@nyu.edu\',\n    packages=find_packages(),\n    entry_points = {\n        \'console_scripts\': [\'crepe=crepe.cli:main\'],\n    },\n    license=\'MIT\',\n    classifiers=[\n        \'Development Status :: 3 - Alpha\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Topic :: Multimedia :: Sound/Audio :: Analysis\',\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n    ],\n    keywords=\'tfrecord\',\n    project_urls={\n        \'Source\': \'https://github.com/marl/crepe\',\n        \'Tracker\': \'https://github.com/marl/crepe/issues\'\n    },\n    install_requires=[\n        str(requirement)\n        for requirement in pkg_resources.parse_requirements(\n            open(os.path.join(os.path.dirname(__file__), ""requirements.txt""))\n        )\n    ],\n    package_data={\n        \'crepe\': weight_files\n    },\n)\n'"
crepe/__init__.py,0,"b'from .version import version as __version__\nfrom .core import get_activation, predict, process_file\n'"
crepe/__main__.py,0,b'from .cli import main\n\n# call the CLI handler when the module is executed as `python -m crepe`\nmain()\n'
crepe/cli.py,0,"b'from __future__ import print_function\n\nimport os\nimport sys\nfrom argparse import ArgumentParser, RawDescriptionHelpFormatter\nfrom argparse import ArgumentTypeError\n\nfrom .core import process_file\n\n\ndef run(filename, output=None, model_capacity=\'full\', viterbi=False,\n        save_activation=False, save_plot=False, plot_voicing=False,\n        no_centering=False, step_size=10, verbose=True):\n    """"""\n    Collect the WAV files to process and run the model\n\n    Parameters\n    ----------\n    filename : list\n        List containing paths to WAV files or folders containing WAV files to\n        be analyzed.\n    output : str or None\n        Path to directory for saving output files. If None, output files will\n        be saved to the directory containing the input file.\n    model_capacity : \'tiny\', \'small\', \'medium\', \'large\', or \'full\'\n        String specifying the model capacity; see the docstring of\n        :func:`~crepe.core.build_and_load_model`\n    viterbi : bool\n        Apply viterbi smoothing to the estimated pitch curve. False by default.\n    save_activation : bool\n        Save the output activation matrix to an .npy file. False by default.\n    save_plot: bool\n        Save a plot of the output activation matrix to a .png file. False by\n        default.\n    plot_voicing : bool\n        Include a visual representation of the voicing activity detection in\n        the plot of the output activation matrix. False by default, only\n        relevant if save_plot is True.\n    no_centering : bool\n        Don\'t pad the signal, meaning frames will begin at their timestamp\n        instead of being centered around their timestamp (which is the\n        default). CAUTION: setting this option can result in CREPE\'s output\n        being misaligned with respect to the output of other audio processing\n        tools and is generally not recommended.\n    step_size : int\n        The step size in milliseconds for running pitch estimation.\n    verbose : bool\n        Print status messages and keras progress (default=True).\n    """"""\n\n    files = []\n    for path in filename:\n        if os.path.isdir(path):\n            found = ([file for file in os.listdir(path) if\n                      file.lower().endswith(\'.wav\')])\n            if len(found) == 0:\n                print(\'CREPE: No WAV files found in directory {}\'.format(path),\n                      file=sys.stderr)\n            files += [os.path.join(path, file) for file in found]\n        elif os.path.isfile(path):\n            if not path.lower().endswith(\'.wav\'):\n                print(\'CREPE: Expecting WAV file(s) but got {}\'.format(path),\n                      file=sys.stderr)\n            files.append(path)\n        else:\n            print(\'CREPE: File or directory not found: {}\'.format(path),\n                  file=sys.stderr)\n\n    if len(files) == 0:\n        print(\'CREPE: No WAV files found in {}, aborting.\'.format(filename))\n        sys.exit(-1)\n\n    for i, file in enumerate(files):\n        if verbose:\n            print(\'CREPE: Processing {} ... ({}/{})\'.format(\n                file, i+1, len(files)), file=sys.stderr)\n        process_file(file, output=output,\n                     model_capacity=model_capacity,\n                     viterbi=viterbi,\n                     center=(not no_centering),\n                     save_activation=save_activation,\n                     save_plot=save_plot,\n                     plot_voicing=plot_voicing,\n                     step_size=step_size,\n                     verbose=verbose)\n\n\ndef positive_int(value):\n    """"""An argparse type method for accepting only positive integers""""""\n    ivalue = int(value)\n    if ivalue <= 0:\n        raise ArgumentTypeError(\'expected a positive integer\')\n    return ivalue\n\n\ndef main():\n    """"""\n    This is a script for running the pre-trained pitch estimation model, CREPE,\n    by taking WAV files(s) as input. For each input WAV, a CSV file containing:\n\n        time, frequency, confidence\n        0.00, 424.24, 0.42\n        0.01, 422.42, 0.84\n        ...\n\n    is created as the output, where the first column is a timestamp in seconds,\n    the second column is the estimated frequency in Hz, and the third column is\n    a value between 0 and 1 indicating the model\'s voicing confidence (i.e.\n    confidence in the presence of a pitch for every frame).\n\n    The script can also optionally save the output activation matrix of the\n    model to an npy file, where the matrix dimensions are (n_frames, 360) using\n    a hop size of 10 ms (there are 360 pitch bins covering 20 cents each).\n    The script can also output a plot of the activation matrix, including an\n    optional visual representation of the model\'s voicing detection.\n    """"""\n\n    parser = ArgumentParser(sys.argv[0], description=main.__doc__,\n                            formatter_class=RawDescriptionHelpFormatter)\n\n    parser.add_argument(\'filename\', nargs=\'+\',\n                        help=\'path to one ore more WAV file(s) to analyze OR \'\n                             \'can be a directory\')\n    parser.add_argument(\'--output\', \'-o\', default=None,\n                        help=\'directory to save the ouptut file(s), must \'\n                             \'already exist; if not given, the output will be \'\n                             \'saved to the same directory as the input WAV \'\n                             \'file(s)\')\n    parser.add_argument(\'--model-capacity\', \'-c\', default=\'full\',\n                        choices=[\'tiny\', \'small\', \'medium\', \'large\', \'full\'],\n                        help=\'String specifying the model capacity; smaller \'\n                             \'models are faster to compute, but may yield \'\n                             \'less accurate pitch estimation\')\n    parser.add_argument(\'--viterbi\', \'-V\', action=\'store_true\',\n                        help=\'perform Viterbi decoding to smooth the pitch \'\n                             \'curve\')\n    parser.add_argument(\'--save-activation\', \'-a\', action=\'store_true\',\n                        help=\'save the output activation matrix to a .npy \'\n                             \'file\')\n    parser.add_argument(\'--save-plot\', \'-p\', action=\'store_true\',\n                        help=\'save a plot of the activation matrix to a .png \'\n                             \'file\')\n    parser.add_argument(\'--plot-voicing\', \'-v\', action=\'store_true\',\n                        help=\'Plot the voicing prediction on top of the \'\n                             \'output activation matrix plot\')\n    parser.add_argument(\'--no-centering\', \'-n\', action=\'store_true\',\n                        help=""Don\'t pad the signal, meaning frames will begin ""\n                             ""at their timestamp instead of being centered ""\n                             ""around their timestamp (which is the default). ""\n                             ""CAUTION: setting this option can result in ""\n                             ""CREPE\'s output being misaligned with respect to ""\n                             ""the output of other audio processing tools and ""\n                             ""is generally not recommended."")\n    parser.add_argument(\'--step-size\', \'-s\', default=10, type=positive_int,\n                        help=\'The step size in milliseconds for running \'\n                             \'pitch estimation. The default is 10 ms.\')\n    parser.add_argument(\'--quiet\', \'-q\', default=False,\n                        action=\'store_true\',\n                        help=\'Suppress all non-error printouts (e.g. progress \'\n                             \'bar).\')\n\n    args = parser.parse_args()\n\n    run(args.filename,\n        output=args.output,\n        model_capacity=args.model_capacity,\n        viterbi=args.viterbi,\n        save_activation=args.save_activation,\n        save_plot=args.save_plot,\n        plot_voicing=args.plot_voicing,\n        no_centering=args.no_centering,\n        step_size=args.step_size,\n        verbose=not args.quiet)\n'"
crepe/core.py,0,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\nimport sys\n\nfrom scipy.io import wavfile\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\n# store as a global variable, since we only support a few models for now\nmodels = {\n    \'tiny\': None,\n    \'small\': None,\n    \'medium\': None,\n    \'large\': None,\n    \'full\': None\n}\n\n# the model is trained on 16kHz audio\nmodel_srate = 16000\n\n\ndef build_and_load_model(model_capacity):\n    """"""\n    Build the CNN model and load the weights\n\n    Parameters\n    ----------\n    model_capacity : \'tiny\', \'small\', \'medium\', \'large\', or \'full\'\n        String specifying the model capacity, which determines the model\'s\n        capacity multiplier to 4 (tiny), 8 (small), 16 (medium), 24 (large),\n        or 32 (full). \'full\' uses the model size specified in the paper,\n        and the others use a reduced number of filters in each convolutional\n        layer, resulting in a smaller model that is faster to evaluate at the\n        cost of slightly reduced pitch estimation accuracy.\n\n    Returns\n    -------\n    model : tensorflow.keras.models.Model\n        The pre-trained keras model loaded in memory\n    """"""\n    from tensorflow.keras.layers import Input, Reshape, Conv2D, BatchNormalization\n    from tensorflow.keras.layers import MaxPool2D, Dropout, Permute, Flatten, Dense\n    from tensorflow.keras.models import Model\n\n    if models[model_capacity] is None:\n        capacity_multiplier = {\n            \'tiny\': 4, \'small\': 8, \'medium\': 16, \'large\': 24, \'full\': 32\n        }[model_capacity]\n\n        layers = [1, 2, 3, 4, 5, 6]\n        filters = [n * capacity_multiplier for n in [32, 4, 4, 4, 8, 16]]\n        widths = [512, 64, 64, 64, 64, 64]\n        strides = [(4, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n\n        x = Input(shape=(1024,), name=\'input\', dtype=\'float32\')\n        y = Reshape(target_shape=(1024, 1, 1), name=\'input-reshape\')(x)\n\n        for l, f, w, s in zip(layers, filters, widths, strides):\n            y = Conv2D(f, (w, 1), strides=s, padding=\'same\',\n                       activation=\'relu\', name=""conv%d"" % l)(y)\n            y = BatchNormalization(name=""conv%d-BN"" % l)(y)\n            y = MaxPool2D(pool_size=(2, 1), strides=None, padding=\'valid\',\n                          name=""conv%d-maxpool"" % l)(y)\n            y = Dropout(0.25, name=""conv%d-dropout"" % l)(y)\n\n        y = Permute((2, 1, 3), name=""transpose"")(y)\n        y = Flatten(name=""flatten"")(y)\n        y = Dense(360, activation=\'sigmoid\', name=""classifier"")(y)\n\n        model = Model(inputs=x, outputs=y)\n\n        package_dir = os.path.dirname(os.path.realpath(__file__))\n        filename = ""model-{}.h5"".format(model_capacity)\n        model.load_weights(os.path.join(package_dir, filename))\n        model.compile(\'adam\', \'binary_crossentropy\')\n\n        models[model_capacity] = model\n\n    return models[model_capacity]\n\n\ndef output_path(file, suffix, output_dir):\n    """"""\n    return the output path of an output file corresponding to a wav file\n    """"""\n    path = re.sub(r""(?i).wav$"", suffix, file)\n    if output_dir is not None:\n        path = os.path.join(output_dir, os.path.basename(path))\n    return path\n\n\ndef to_local_average_cents(salience, center=None):\n    """"""\n    find the weighted average cents near the argmax bin\n    """"""\n\n    if not hasattr(to_local_average_cents, \'cents_mapping\'):\n        # the bin number-to-cents mapping\n        to_local_average_cents.cents_mapping = (\n                np.linspace(0, 7180, 360) + 1997.3794084376191)\n\n    if salience.ndim == 1:\n        if center is None:\n            center = int(np.argmax(salience))\n        start = max(0, center - 4)\n        end = min(len(salience), center + 5)\n        salience = salience[start:end]\n        product_sum = np.sum(\n            salience * to_local_average_cents.cents_mapping[start:end])\n        weight_sum = np.sum(salience)\n        return product_sum / weight_sum\n    if salience.ndim == 2:\n        return np.array([to_local_average_cents(salience[i, :]) for i in\n                         range(salience.shape[0])])\n\n    raise Exception(""label should be either 1d or 2d ndarray"")\n\n\ndef to_viterbi_cents(salience):\n    """"""\n    Find the Viterbi path using a transition prior that induces pitch\n    continuity.\n    """"""\n    from hmmlearn import hmm\n\n    # uniform prior on the starting pitch\n    starting = np.ones(360) / 360\n\n    # transition probabilities inducing continuous pitch\n    xx, yy = np.meshgrid(range(360), range(360))\n    transition = np.maximum(12 - abs(xx - yy), 0)\n    transition = transition / np.sum(transition, axis=1)[:, None]\n\n    # emission probability = fixed probability for self, evenly distribute the\n    # others\n    self_emission = 0.1\n    emission = (np.eye(360) * self_emission + np.ones(shape=(360, 360)) *\n                ((1 - self_emission) / 360))\n\n    # fix the model parameters because we are not optimizing the model\n    model = hmm.MultinomialHMM(360, starting, transition)\n    model.startprob_, model.transmat_, model.emissionprob_ = \\\n        starting, transition, emission\n\n    # find the Viterbi path\n    observations = np.argmax(salience, axis=1)\n    path = model.predict(observations.reshape(-1, 1), [len(observations)])\n\n    return np.array([to_local_average_cents(salience[i, :], path[i]) for i in\n                     range(len(observations))])\n\n\ndef get_activation(audio, sr, model_capacity=\'full\', center=True, step_size=10,\n                   verbose=1):\n    """"""\n\n    Parameters\n    ----------\n    audio : np.ndarray [shape=(N,) or (N, C)]\n        The audio samples. Multichannel audio will be downmixed.\n    sr : int\n        Sample rate of the audio samples. The audio will be resampled if\n        the sample rate is not 16 kHz, which is expected by the model.\n    model_capacity : \'tiny\', \'small\', \'medium\', \'large\', or \'full\'\n        String specifying the model capacity; see the docstring of\n        :func:`~crepe.core.build_and_load_model`\n    center : boolean\n        - If `True` (default), the signal `audio` is padded so that frame\n          `D[:, t]` is centered at `audio[t * hop_length]`.\n        - If `False`, then `D[:, t]` begins at `audio[t * hop_length]`\n    step_size : int\n        The step size in milliseconds for running pitch estimation.\n    verbose : int\n        Set the keras verbosity mode: 1 (default) will print out a progress bar\n        during prediction, 0 will suppress all non-error printouts.\n\n    Returns\n    -------\n    activation : np.ndarray [shape=(T, 360)]\n        The raw activation matrix\n    """"""\n    model = build_and_load_model(model_capacity)\n\n    if len(audio.shape) == 2:\n        audio = audio.mean(1)  # make mono\n    audio = audio.astype(np.float32)\n    if sr != model_srate:\n        # resample audio if necessary\n        from resampy import resample\n        audio = resample(audio, sr, model_srate)\n\n    # pad so that frames are centered around their timestamps (i.e. first frame\n    # is zero centered).\n    if center:\n        audio = np.pad(audio, 512, mode=\'constant\', constant_values=0)\n\n    # make 1024-sample frames of the audio with hop length of 10 milliseconds\n    hop_length = int(model_srate * step_size / 1000)\n    n_frames = 1 + int((len(audio) - 1024) / hop_length)\n    frames = as_strided(audio, shape=(1024, n_frames),\n                        strides=(audio.itemsize, hop_length * audio.itemsize))\n    frames = frames.transpose().copy()\n\n    # normalize each frame -- this is expected by the model\n    frames -= np.mean(frames, axis=1)[:, np.newaxis]\n    frames /= np.std(frames, axis=1)[:, np.newaxis]\n\n    # run prediction and convert the frequency bin weights to Hz\n    return model.predict(frames, verbose=verbose)\n\n\ndef predict(audio, sr, model_capacity=\'full\',\n            viterbi=False, center=True, step_size=10, verbose=1):\n    """"""\n    Perform pitch estimation on given audio\n\n    Parameters\n    ----------\n    audio : np.ndarray [shape=(N,) or (N, C)]\n        The audio samples. Multichannel audio will be downmixed.\n    sr : int\n        Sample rate of the audio samples. The audio will be resampled if\n        the sample rate is not 16 kHz, which is expected by the model.\n    model_capacity : \'tiny\', \'small\', \'medium\', \'large\', or \'full\'\n        String specifying the model capacity; see the docstring of\n        :func:`~crepe.core.build_and_load_model`\n    viterbi : bool\n        Apply viterbi smoothing to the estimated pitch curve. False by default.\n    center : boolean\n        - If `True` (default), the signal `audio` is padded so that frame\n          `D[:, t]` is centered at `audio[t * hop_length]`.\n        - If `False`, then `D[:, t]` begins at `audio[t * hop_length]`\n    step_size : int\n        The step size in milliseconds for running pitch estimation.\n    verbose : int\n        Set the keras verbosity mode: 1 (default) will print out a progress bar\n        during prediction, 0 will suppress all non-error printouts.\n\n    Returns\n    -------\n    A 4-tuple consisting of:\n\n        time: np.ndarray [shape=(T,)]\n            The timestamps on which the pitch was estimated\n        frequency: np.ndarray [shape=(T,)]\n            The predicted pitch values in Hz\n        confidence: np.ndarray [shape=(T,)]\n            The confidence of voice activity, between 0 and 1\n        activation: np.ndarray [shape=(T, 360)]\n            The raw activation matrix\n    """"""\n    activation = get_activation(audio, sr, model_capacity=model_capacity,\n                                center=center, step_size=step_size,\n                                verbose=verbose)\n    confidence = activation.max(axis=1)\n\n    if viterbi:\n        cents = to_viterbi_cents(activation)\n    else:\n        cents = to_local_average_cents(activation)\n\n    frequency = 10 * 2 ** (cents / 1200)\n    frequency[np.isnan(frequency)] = 0\n\n    time = np.arange(confidence.shape[0]) * step_size / 1000.0\n\n    return time, frequency, confidence, activation\n\n\ndef process_file(file, output=None, model_capacity=\'full\', viterbi=False,\n                 center=True, save_activation=False, save_plot=False,\n                 plot_voicing=False, step_size=10, verbose=True):\n    """"""\n    Use the input model to perform pitch estimation on the input file.\n\n    Parameters\n    ----------\n    file : str\n        Path to WAV file to be analyzed.\n    output : str or None\n        Path to directory for saving output files. If None, output files will\n        be saved to the directory containing the input file.\n    model_capacity : \'tiny\', \'small\', \'medium\', \'large\', or \'full\'\n        String specifying the model capacity; see the docstring of\n        :func:`~crepe.core.build_and_load_model`\n    viterbi : bool\n        Apply viterbi smoothing to the estimated pitch curve. False by default.\n    center : boolean\n        - If `True` (default), the signal `audio` is padded so that frame\n          `D[:, t]` is centered at `audio[t * hop_length]`.\n        - If `False`, then `D[:, t]` begins at `audio[t * hop_length]`\n    save_activation : bool\n        Save the output activation matrix to an .npy file. False by default.\n    save_plot : bool\n        Save a plot of the output activation matrix to a .png file. False by\n        default.\n    plot_voicing : bool\n        Include a visual representation of the voicing activity detection in\n        the plot of the output activation matrix. False by default, only\n        relevant if save_plot is True.\n    step_size : int\n        The step size in milliseconds for running pitch estimation.\n    verbose : bool\n        Print status messages and keras progress (default=True).\n\n    Returns\n    -------\n\n    """"""\n    try:\n        sr, audio = wavfile.read(file)\n    except ValueError:\n        print(""CREPE: Could not read %s"" % file, file=sys.stderr)\n        raise\n\n    time, frequency, confidence, activation = predict(\n        audio, sr,\n        model_capacity=model_capacity,\n        viterbi=viterbi,\n        center=center,\n        step_size=step_size,\n        verbose=1 * verbose)\n\n    # write prediction as TSV\n    f0_file = output_path(file, "".f0.csv"", output)\n    f0_data = np.vstack([time, frequency, confidence]).transpose()\n    np.savetxt(f0_file, f0_data, fmt=[\'%.3f\', \'%.3f\', \'%.6f\'], delimiter=\',\',\n               header=\'time,frequency,confidence\', comments=\'\')\n    if verbose:\n        print(""CREPE: Saved the estimated frequencies and confidence values ""\n              ""at {}"".format(f0_file))\n\n    # save the salience file to a .npy file\n    if save_activation:\n        activation_path = output_path(file, "".activation.npy"", output)\n        np.save(activation_path, activation)\n        if verbose:\n            print(""CREPE: Saved the activation matrix at {}"".format(\n                activation_path))\n\n    # save the salience visualization in a PNG file\n    if save_plot:\n        import matplotlib.cm\n        from imageio import imwrite\n\n        plot_file = output_path(file, "".activation.png"", output)\n        # to draw the low pitches in the bottom\n        salience = np.flip(activation, axis=1)\n        inferno = matplotlib.cm.get_cmap(\'inferno\')\n        image = inferno(salience.transpose())\n\n        if plot_voicing:\n            # attach a soft and hard voicing detection result under the\n            # salience plot\n            image = np.pad(image, [(0, 20), (0, 0), (0, 0)], mode=\'constant\')\n            image[-20:-10, :, :] = inferno(confidence)[np.newaxis, :, :]\n            image[-10:, :, :] = (\n                inferno((confidence > 0.5).astype(np.float))[np.newaxis, :, :])\n\n        imwrite(plot_file, (255 * image).astype(np.uint8))\n        if verbose:\n            print(""CREPE: Saved the salience plot at {}"".format(plot_file))\n\n'"
crepe/version.py,0,"b""version = '0.0.11'\n"""
tests/test_sweep.py,0,"b'import os\nimport numpy as np\nimport crepe\n\n# this data contains a sine sweep\nfile = os.path.join(os.path.dirname(__file__), \'sweep.wav\')\nf0_file = os.path.join(os.path.dirname(__file__), \'sweep.f0.csv\')\n\n\ndef verify_f0():\n    result = np.loadtxt(f0_file, delimiter=\',\', skiprows=1)\n\n    # it should be confident enough about the presence of pitch in every frame\n    assert np.mean(result[:, 2] > 0.5) > 0.98\n\n    # the frequencies should be linear\n    assert np.corrcoef(result[:, 1]) > 0.99\n\n    os.remove(f0_file)\n\n\ndef test_sweep():\n    crepe.process_file(file)\n    verify_f0()\n\n\ndef test_sweep_cli():\n    assert os.system(""crepe {}"".format(file)) == 0\n    verify_f0()\n'"
