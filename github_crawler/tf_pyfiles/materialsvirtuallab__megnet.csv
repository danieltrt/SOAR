file_path,api_count,code
setup.py,0,"b'from setuptools import setup\nfrom setuptools import find_packages\nimport os\nimport re\n\nthis_dir = os.path.abspath(os.path.dirname(__file__))\n\nwith open(os.path.join(this_dir, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\n\nwith open(\'megnet/__init__.py\', encoding=\'utf-8\') as fd:\n    try:\n        lines = \'\'\n        for item in fd.readlines():\n            item = item\n            lines += item + \'\\n\'\n    except Exception as exc:\n        raise Exception(\'Caught exception {}\'.format(exc))\n\n\nversion = re.search(\'__version__ = ""(.*)""\', lines).group(1)\n\n\nsetup(\n    name=\'megnet\',\n    version=version,\n    description=\'MatErials Graph Networks for machine learning of molecules and crystals.\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    author=\'Chi Chen\',\n    author_email=\'chc273@eng.ucsd.edu\',\n    download_url=\'https://github.com/materialsvirtuallab/megnet\',\n    license=\'BSD\',\n    install_requires=[\'numpy\', ""scikit-learn"",\n                      \'pymatgen>=2019.10.4\', \'monty\'],\n    extras_require={\n        \'model_saving\': [\'h5py\'],\n        \'molecules\': [\'openbabel\', \'rdkit\'],\n        \'tensorflow\': [\'tensorflow>=2.1\'],\n        \'tensorflow with gpu\': [\'tensorflow-gpu>=2.1\'],\n    },\n    packages=find_packages(),\n    package_data={\n        ""megnet"": [""*.json"", ""*.md""],\n    },\n    keywords=[""materials"", ""science"", ""machine"", ""learning"", ""deep"", ""graph"", ""networks"", ""neural""],\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.5"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Development Status :: 4 - Beta"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: BSD License"",\n        ""Operating System :: OS Independent"",\n        ""Topic :: Scientific/Engineering :: Information Analysis"",\n        ""Topic :: Scientific/Engineering :: Physics"",\n        ""Topic :: Scientific/Engineering :: Chemistry"",\n        ""Topic :: Software Development :: Libraries :: Python Modules""\n    ],\n    entry_points={\n        \'console_scripts\': [\n            \'meg = megnet.cli.meg:main\',\n        ]\n    }\n)\n'"
tasks.py,0,"b'""""""\nPyinvoke tasks.py file for automating releases and admin stuff.\n\nAuthor: Shyue Ping Ong\n""""""\n\n\nfrom invoke import task\nimport os\nimport json\nimport requests\nimport re\nimport subprocess\nimport megnet\nimport glob\nfrom monty.os import cd\n\n\nNEW_VER = megnet.__version__\n\n\n@task\ndef make_doc(ctx):\n    with cd(""docs_rst""):\n        ctx.run(""sphinx-apidoc --separate -d 6 -o . -f ../megnet"")\n        for f in glob.glob(""*.rst""):\n            if f.startswith(\'megnet\') and f.endswith(\'rst\'):\n                newoutput = []\n                suboutput = []\n                subpackage = False\n                with open(f, \'r\') as fid:\n                    for line in fid:\n                        clean = line.strip()\n                        if clean == ""Subpackages"":\n                            subpackage = True\n                        if not subpackage and not clean.endswith(""tests""):\n                            newoutput.append(line)\n                        else:\n                            if not clean.endswith(""tests""):\n                                suboutput.append(line)\n                            if clean.startswith(\n                                    ""megnet"") and not clean.endswith(""tests""):\n                                newoutput.extend(suboutput)\n                                subpackage = False\n                                suboutput = []\n\n                with open(f, \'w\') as fid:\n                    fid.write("""".join(newoutput))\n        ctx.run(""make html"")\n\n        # ctx.run(""cp _static/* ../docs/html/_static"")\n\n    with cd(""docs""):\n        ctx.run(""cp -r html/* ."")\n        ctx.run(""rm -r html"")\n        ctx.run(""rm -r doctrees"")\n        ctx.run(""rm -r _sources"")\n\n        # This makes sure pymatgen.org works to redirect to the Gihub page\n        # ctx.run(""echo \\""pymatgen.org\\"" > CNAME"")\n        # Avoid the use of jekyll so that _dir works as intended.\n        ctx.run(""touch .nojekyll"")\n\n\n@task\ndef publish(ctx):\n    ctx.run(""rm dist/*.*"", warn=True)\n    ctx.run(""python setup.py sdist bdist_wheel"")\n    ctx.run(""twine upload dist/*"")\n\n\n@task\ndef release_github(ctx):\n    with open(""CHANGES.md"") as f:\n        contents = f.read()\n    toks = re.split(r""\\#+"", contents)\n    desc = toks[1].strip()\n    payload = {\n        ""tag_name"": ""v"" + NEW_VER,\n        ""target_commitish"": ""master"",\n        ""name"": ""v"" + NEW_VER,\n        ""body"": desc,\n        ""draft"": False,\n        ""prerelease"": False\n    }\n    response = requests.post(\n        ""https://api.github.com/repos/materialsvirtuallab/megnet/releases"",\n        data=json.dumps(payload),\n        headers={""Authorization"": ""token "" + os.environ[""GITHUB_RELEASES_TOKEN""]})\n    print(response.text)\n\n\n@task\ndef update_changelog(ctx):\n    output = subprocess.check_output([""git"", ""log"", ""--pretty=format:%s"",\n                                      ""v%s..HEAD"" % CURRENT_VER])\n    lines = [""* "" + l for l in output.decode(""utf-8"").strip().split(""\\n"")]\n    with open(""CHANGES.rst"") as f:\n        contents = f.read()\n    l = ""==========""\n    toks = contents.split(l)\n    head = ""\\n\\nv%s\\n"" % NEW_VER + ""-"" * (len(NEW_VER) + 1) + ""\\n""\n    toks.insert(-1, head + ""\\n"".join(lines))\n    with open(""CHANGES.rst"", ""w"") as f:\n        f.write(toks[0] + l + """".join(toks[1:]))\n    ctx.run(""open CHANGES.rst"")\n\n\n@task\ndef release(ctx, notest=False):\n    ctx.run(""rm -r dist build megnet.egg-info"", warn=True)\n    if not notest:\n        ctx.run(""pytest megnet"")\n    publish(ctx)\n    release_github(ctx)\n'"
docs_rst/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# pymatgen documentation build configuration file, created by\n# sphinx-quickstart on Tue Nov 15 00:13:52 2011.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\'.\'))\nsys.path.insert(0, os.path.dirname(\'..\'))\nsys.path.insert(0, os.path.dirname(\'../megnet\'))\nsys.path.insert(0, os.path.dirname(\'../..\'))\n\nfrom megnet import __version__\n\n# -- General configuration -----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named \'sphinx.ext.*\') or your custom ones.\nextensions = [\'sphinx.ext.autodoc\', \'sphinx.ext.napoleon\', \'sphinx.ext.viewcode\']\nexclude_patterns = [\'../**/tests*\']\nexclude_dirnames = [\'../**/tests*\']\nautoclass_content = \'both\'\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix of source filenames.\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'megnet\'\ncopyright = u\'2019, Materials Virtual Lab\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = __version__\n# The full version, including alpha/beta/rc tags.\nrelease = __version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'_build\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\nadd_module_names = False\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nsys.path.append(os.path.abspath(\'_themes\'))\nhtml_theme_path = [\'_themes\']\nhtml_theme = \'kr\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = ["".""]\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = ""favicon.ico""\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'montydoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n  (\'index\', \'monty.tex\', u\'Monty Documentation\', \'Materials Virtual Lab\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (\'index\', \'megnet\', u\'megnet Documentation\',\n     [\'Materials Virtual Lab\'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  (\'index\', \'megnet\', u\'megnet Documentation\',\n   \'Materials Virtual Lab\', \'megnet\', \'One line description of project.\',\n   \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n\n# -- Options for Epub output ---------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = u\'megnet\'\nepub_author = \'Materials Virtual Lab\'\nepub_publisher = u\'Shyue Ping Ong\'\nepub_copyright = copyright\n\n# The language of the text. It defaults to the language option\n# or en if the language is not set.\n#epub_language = \'\'\n\n# The scheme of the identifier. Typical schemes are ISBN or URL.\n#epub_scheme = \'\'\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#epub_identifier = \'\'\n\n# A unique identification for the text.\n#epub_uid = \'\'\n\n# A tuple containing the cover image and cover page html template filenames.\n#epub_cover = ()\n\n# HTML files that should be inserted before the pages created by sphinx.\n# The format is a list of tuples containing the path and title.\n#epub_pre_files = []\n\n# HTML files shat should be inserted after the pages created by sphinx.\n# The format is a list of tuples containing the path and title.\n#epub_post_files = []\n\n# A list of files that should not be packed into the epub file.\n#epub_exclude_files = []\n\n# The depth of the table of contents in toc.ncx.\n#epub_tocdepth = 3\n\n# Allow duplicate toc entries.\n#epub_tocdup = True\n'"
megnet/__init__.py,0,"b'__version__ = ""1.1.8""\n'"
megnet/activations.py,0,"b'from typing import Callable, Any\n\nimport tensorflow.keras.backend as kb\nfrom tensorflow.keras.activations import get as keras_get\nfrom tensorflow.keras.activations import deserialize, serialize  # noqa\n\nfrom megnet.utils.typing import OptStrOrCallable\n\n\ndef softplus2(x):\n    """"""\n    out = log(exp(x)+1) - log(2)\n    softplus function that is 0 at x=0, the implementation aims at avoiding overflow\n\n    Args:\n        x: (Tensor) input tensor\n\n    Returns:\n         (Tensor) output tensor\n    """"""\n    return kb.relu(x) + kb.log(0.5*kb.exp(-kb.abs(x)) + 0.5)\n\n\ndef get(identifier: OptStrOrCallable = None) -> Callable[..., Any]:\n    """"""\n    Get activations by identifier\n\n    Args:\n        identifier (str or callable): the identifier of activations\n\n    Returns:\n        callable activation\n\n    """"""\n    try:\n        return keras_get(identifier)\n    except ValueError:\n        if isinstance(identifier, str):\n            return deserialize(identifier, custom_objects=globals())\n        else:\n            raise ValueError(\'Could not interpret:\',  identifier)\n'"
megnet/callbacks.py,0,"b'import re\nimport logging\nimport os\nimport warnings\nfrom glob import glob\nfrom collections import deque\nimport numpy as np\nfrom tensorflow.keras.callbacks import Callback\nimport tensorflow.keras.backend as kb\nfrom megnet.utils.metrics import mae, accuracy\nfrom megnet.utils.preprocessing import DummyScaler, Scaler\n\nfrom typing import Iterable, Dict\n\nlogging.basicConfig()\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n\nclass ModelCheckpointMAE(Callback):\n    """"""\n    Save the best MAE model with target scaler\n\n    Args:\n        filepath (string): path to save the model file with format. For example\n            `weights.{epoch:02d}-{val_mae:.6f}.hdf5` will save the corresponding epoch and\n            val_mae in the filename\n        monitor (string): quantity to monitor, default to ""val_mae""\n        verbose (int): 0 for no training log, 1 for only epoch-level log and 2 for batch-level log\n        save_best_only (bool): whether to save only the best model\n        save_weights_only (bool): whether to save the weights only excluding model structure\n        val_gen (generator): validation generator\n        steps_per_val (int): steps per epoch for validation generator\n        target_scaler (object): exposing inverse_transform method to scale the output\n        period (int): number of epoch interval for this callback\n        mode: (string) choose from ""min"", ""max"" or ""auto""\n    """"""\n\n    def __init__(self,\n                 filepath: str = \'./callback/val_mae_{epoch:05d}_{val_mae:.6f}.hdf5\',\n                 monitor: str = \'val_mae\',\n                 verbose: int = 0,\n                 save_best_only: bool = True,\n                 save_weights_only: bool = False,\n                 val_gen: Iterable = None,\n                 steps_per_val: int = None,\n                 target_scaler: Scaler = None,\n                 period: int = 1,\n                 mode: str = \'auto\'):\n        super().__init__()\n        if val_gen is None:\n            raise ValueError(\'No validation data is provided!\')\n        self.verbose = verbose\n        if self.verbose > 0:\n            logging.basicConfig(level=logging.INFO)\n        self.filepath = filepath\n        self.save_best_only = save_best_only\n        self.save_weights_only = save_weights_only\n        self.period = period\n        self.epochs_since_last_save = 0\n        self.val_gen = val_gen\n        self.steps_per_val = steps_per_val\n        self.target_scaler = target_scaler\n        if self.target_scaler is None:\n            self.target_scaler = DummyScaler()\n\n        if monitor == \'val_mae\':\n            self.metric = mae\n            self.monitor = \'val_mae\'\n        elif monitor == \'val_acc\':\n            self.metric = accuracy\n            self.filepath = self.filepath.replace(\'val_mae\', \'val_acc\')\n            self.monitor = \'val_acc\'\n\n        if mode == \'min\':\n            self.monitor_op = np.less\n            self.best = np.Inf\n        elif mode == \'max\':\n            self.monitor_op = np.greater\n            self.best = -np.Inf\n        else:\n            if \'acc\' in self.monitor or self.monitor.startswith(\'fmeasure\'):\n                self.monitor_op = np.greater\n                self.best = -np.Inf\n            else:\n                self.monitor_op = np.less\n                self.best = np.Inf\n\n    def on_epoch_end(self, epoch: int, logs: Dict = None) -> None:\n        """"""\n        Codes called by the callback at the end of epoch\n        Args:\n            epoch (int): epoch id\n            logs (dict): logs of training\n\n        Returns:\n            None\n        """"""\n        self.epochs_since_last_save += 1\n        if self.epochs_since_last_save >= self.period:\n            self.epochs_since_last_save = 0\n            val_pred = []\n            val_y = []\n            for i in range(self.steps_per_val):\n                val_data = self.val_gen[i]\n                nb_atom = _count(np.array(val_data[0][-2]))\n                stop_training = self.model.stop_training  # save stop_trainings state\n                pred_ = self.model.predict(val_data[0])\n                self.model.stop_training = stop_training\n                val_pred.append(self.target_scaler.inverse_transform(pred_[0, :, :],\n                                                                     nb_atom[:, None]))\n                val_y.append(self.target_scaler.inverse_transform(val_data[1][0, :, :],\n                                                                  nb_atom[:, None]))\n            current = self.metric(np.concatenate(val_y, axis=0), np.concatenate(val_pred, axis=0))\n            filepath = self.filepath.format(**{""epoch"": epoch + 1, self.monitor: current})\n\n            if self.save_best_only:\n                if current is None:\n                    warnings.warn(\'Can save best model only with %s available, \'\n                                  \'skipping.\' % self.monitor, RuntimeWarning)\n                else:\n                    if self.monitor_op(current, self.best):\n                        logger.info(\'\\nEpoch %05d: %s improved from %0.5f to %0.5f,\'\n                                    \' saving model to %s\' % (epoch + 1, self.monitor, self.best, current, filepath))\n                        self.best = current\n                        if self.save_weights_only:\n                            self.model.save_weights(filepath, overwrite=True)\n                        else:\n                            self.model.save(filepath, overwrite=True)\n                    else:\n                        if self.verbose > 0:\n                            logger.info(\'\\nEpoch %05d: %s did not improve from %0.5f\' %\n                                        (epoch + 1, self.monitor, self.best))\n            else:\n                logger.info(\'\\nEpoch %05d: saving model to %s\' % (epoch + 1, filepath))\n                if self.save_weights_only:\n                    self.model.save_weights(filepath, overwrite=True)\n                else:\n                    self.model.save(filepath, overwrite=True)\n\n\nclass ManualStop(Callback):\n    """"""\n    Stop the training manually by putting a ""STOP"" file in the directory\n    """"""\n\n    def on_batch_end(self, epoch: int, logs: Dict = None) -> None:\n        """"""\n        Codes called at the end of a batch\n        Args:\n            epoch (int): epoch id\n            logs (Dict): log dict\n\n        Returns: None\n\n        """"""\n        if os.path.isfile(\'STOP\'):\n            self.model.stop_training = True\n\n\nclass ReduceLRUponNan(Callback):\n    """"""\n    This callback function solves a problem that when doing regression,\n    an nan loss may occur, or the loss suddenly shoot up.\n    If such things happen, the model will reduce the learning rate\n    and load the last best model during the training process.\n    It has an extra function that patience for early stopping.\n    This will move to indepedent callback in the future.\n\n    Args:\n        filepath (str): filepath for saved model checkpoint, should be consistent with\n            checkpoint callback\n        factor (float): a value < 1 for scaling the learning rate\n        verbose (bool): whether to show the loading event\n        patience (int): number of steps that the val mae does not change.\n            It is a criteria for early stopping\n        monitor (str): target metric to monitor\n        mode (str): min, max or auto\n    """"""\n\n    def __init__(self,\n                 filepath: str = \'./callback/val_mae_{epoch:05d}_{val_mae:.6f}.hdf5\',\n                 factor: float = 0.5,\n                 verbose: bool = True,\n                 patience: int = 500,\n                 monitor: str = \'val_mae\',\n                 mode: str = \'auto\'):\n        self.filepath = filepath\n        self.verbose = verbose\n        self.factor = factor\n        self.losses = deque([], maxlen=10)\n        self.patience = patience\n        self.monitor = monitor\n        super().__init__()\n\n        if mode == \'min\':\n            self.monitor_op = np.argmin\n        elif mode == \'max\':\n            self.monitor_op = np.argmax\n        else:\n            if \'acc\' in self.monitor:\n                self.monitor_op = np.argmax\n            else:\n                self.monitor_op = np.argmin\n\n        # get variable name\n        variable_name_pattern = r\'{(.+?)}\'\n        self.variable_names = re.findall(variable_name_pattern, filepath)\n        self.variable_names = [i.split(\':\')[0] for i in self.variable_names]\n        if self.monitor not in self.variable_names:\n            raise ValueError(""The monitored metric should be in the name pattern"")\n\n    def on_epoch_end(self, epoch: int, logs: Dict = None):\n        """"""\n        Check the loss value at the end of an epoch\n        Args:\n            epoch (int): epoch id\n            logs (dict): log history\n\n        Returns: None\n\n        """"""\n        logs = logs or {}\n        loss = logs.get(\'loss\')\n        last_saved_epoch, last_metric, last_file = self._get_checkpoints()\n        if last_saved_epoch is not None:\n            if last_saved_epoch + self.patience <= epoch:\n                self.model.stop_training = True\n                logger.info(\'%s does not improve after %d, stopping \'\n                            \'the fitting...\' % (self.monitor, self.patience))\n\n        if loss is not None:\n            self.losses.append(loss)\n            if np.isnan(loss) or np.isinf(loss):\n                if self.verbose:\n                    logger.info(""Nan loss found!"")\n                self._reduce_lr_and_load(last_file)\n                if self.verbose:\n                    logger.info(""Now lr is %s."" % float(\n                        kb.eval(self.model.optimizer.lr)))\n            else:\n                if len(self.losses) > 1:\n                    if self.losses[-1] > (self.losses[-2] * 100):\n                        self._reduce_lr_and_load(last_file)\n                        if self.verbose:\n                            logger.info(\n                                ""Loss shot up from %.3f to %.3f! Reducing lr "" % (\n                                    self.losses[-1], self.losses[-2]))\n                            logger.info(""Now lr is %s."" % float(\n                                kb.eval(self.model.optimizer.lr)))\n\n    def _reduce_lr_and_load(self, last_file):\n        old_value = float(kb.eval(self.model.optimizer.lr))\n        self.model.reset_states()\n        self.model.optimizer.lr = old_value * self.factor\n\n        opt_dict = self.model.optimizer.get_config()\n        self.model.compile(self.model.optimizer.__class__(**opt_dict), self.model.loss)\n        if last_file is not None:\n            self.model.load_weights(last_file)\n            if self.verbose:\n                logger.info(""Load weights %s"" % last_file)\n        else:\n            logger.info(""No weights were loaded"")\n\n    def _get_checkpoints(self):\n        file_pattern = re.sub(r\'{(.+?)}\', r\'([0-9\\.]+)\', self.filepath)\n        glob_pattern = re.sub(r\'{(.+?)}\', r\'*\', self.filepath)\n        all_check_points = glob(glob_pattern)\n\n        if len(all_check_points) > 0:\n            metric_index = self.variable_names.index(self.monitor)\n            epoch_index = self.variable_names.index(\'epoch\')\n            metric_values = []\n            epochs = []\n            for i in all_check_points:\n                metrics = re.findall(file_pattern, i)[0]\n                metric_values.append(float(metrics[metric_index]))\n                epochs.append(int(metrics[epoch_index]))\n            ind = self.monitor_op(metric_values)\n            return epochs[ind], metric_values[ind], all_check_points[ind]\n        else:\n            return None, None, None\n\n\ndef _count(a: np.ndarray) -> np.ndarray:\n    """"""\n    count number of appearance for each element in a\n\n    Args:\n        a: (np.array)\n\n    Returns:\n        (np.array) number of appearance of each element in a\n    """"""\n    a = a.ravel()\n    a = np.r_[a[0], a, np.Inf]\n    z = np.where(np.abs(np.diff(a)) > 0)[0]\n    z = np.r_[0, z]\n    return np.diff(z)\n'"
megnet/config.py,6,"b'""""""Data types""""""\nimport numpy as np\nimport tensorflow as tf\n\n\nDTYPES = {\'float32\': {\'numpy\': np.float32, \'tf\': tf.float32},\n          \'float16\': {\'numpy\': np.float16, \'tf\': tf.float16},\n          \'int32\': {\'numpy\': np.int32, \'tf\': tf.int32},\n          \'int16\': {\'numpy\': np.int16, \'tf\': tf.int16}}\n\n\nclass DataType:\n    np_float = np.float32\n    np_int = np.int32\n    tf_float = tf.float32\n    tf_int = tf.int32\n\n    @classmethod\n    def set_dtype(cls, data_type: str) -> None:\n        """"""\n        Class method to set the data types\n        Args:\n            data_type (str): \'16\' or \'32\'\n        """"""\n        if data_type.endswith(\'32\'):\n            float_key = \'float32\'\n            int_key = \'int32\'\n        elif data_type.endswith(\'16\'):\n            float_key = \'float16\'\n            int_key = \'int16\'\n        else:\n            raise ValueError(""Data type not known, choose \'16\' or \'32\'"")\n\n        cls.np_float = DTYPES[float_key][\'numpy\']\n        cls.tf_float = DTYPES[float_key][\'tf\']\n        cls.np_int = DTYPES[int_key][\'numpy\']\n        cls.tf_int = DTYPES[int_key][\'tf\']\n\n\ndef set_global_dtypes(data_type) -> None:\n    """"""\n    Function to set the data types\n    Args:\n        data_type (str): \'16\' or \'32\'\n    Returns:\n\n    """"""\n    DataType.set_dtype(data_type)\n'"
megnet/losses.py,0,"b'import tensorflow.keras.backend as kb\n\n\ndef mean_squared_error_with_scale(y_true, y_pred, scale=10000):\n    """"""\n    Keras default log for tracking progress shows two decimal points,\n    here we multiply the mse by a factor to fully show the loss in progress bar\n\n    Args:\n        y_true: (tensor) training y\n        y_pred: (tensor) predicted y\n        scale: (int or float) factor to multiply with mse\n\n    Returns:\n        scaled mse (float)\n    """"""\n    return kb.mean(kb.square(y_pred - y_true), axis=-1) * scale\n\n\nmse_scale = mean_squared_error_with_scale\n'"
docs_rst/_themes/flask_theme_support.py,0,"b'# flasky extensions.  flasky pygments style based on tango style\nfrom pygments.style import Style\nfrom pygments.token import Keyword, Name, Comment, String, Error, \\\n     Number, Operator, Generic, Whitespace, Punctuation, Other, Literal\n\n\nclass FlaskyStyle(Style):\n    background_color = ""#f8f8f8""\n    default_style = """"\n\n    styles = {\n        # No corresponding class for the following:\n        #Text:                     """", # class:  \'\'\n        Whitespace:                ""underline #f8f8f8"",      # class: \'w\'\n        Error:                     ""#a40000 border:#ef2929"", # class: \'err\'\n        Other:                     ""#000000"",                # class \'x\'\n\n        Comment:                   ""italic #8f5902"", # class: \'c\'\n        Comment.Preproc:           ""noitalic"",       # class: \'cp\'\n\n        Keyword:                   ""bold #004461"",   # class: \'k\'\n        Keyword.Constant:          ""bold #004461"",   # class: \'kc\'\n        Keyword.Declaration:       ""bold #004461"",   # class: \'kd\'\n        Keyword.Namespace:         ""bold #004461"",   # class: \'kn\'\n        Keyword.Pseudo:            ""bold #004461"",   # class: \'kp\'\n        Keyword.Reserved:          ""bold #004461"",   # class: \'kr\'\n        Keyword.Type:              ""bold #004461"",   # class: \'kt\'\n\n        Operator:                  ""#582800"",   # class: \'o\'\n        Operator.Word:             ""bold #004461"",   # class: \'ow\' - like keywords\n\n        Punctuation:               ""bold #000000"",   # class: \'p\'\n\n        # because special names such as Name.Class, Name.Function, etc.\n        # are not recognized as such later in the parsing, we choose them\n        # to look the same as ordinary variables.\n        Name:                      ""#000000"",        # class: \'n\'\n        Name.Attribute:            ""#c4a000"",        # class: \'na\' - to be revised\n        Name.Builtin:              ""#004461"",        # class: \'nb\'\n        Name.Builtin.Pseudo:       ""#3465a4"",        # class: \'bp\'\n        Name.Class:                ""#000000"",        # class: \'nc\' - to be revised\n        Name.Constant:             ""#000000"",        # class: \'no\' - to be revised\n        Name.Decorator:            ""#888"",           # class: \'nd\' - to be revised\n        Name.Entity:               ""#ce5c00"",        # class: \'ni\'\n        Name.Exception:            ""bold #cc0000"",   # class: \'ne\'\n        Name.Function:             ""#000000"",        # class: \'nf\'\n        Name.Property:             ""#000000"",        # class: \'py\'\n        Name.Label:                ""#f57900"",        # class: \'nl\'\n        Name.Namespace:            ""#000000"",        # class: \'nn\' - to be revised\n        Name.Other:                ""#000000"",        # class: \'nx\'\n        Name.Tag:                  ""bold #004461"",   # class: \'nt\' - like a keyword\n        Name.Variable:             ""#000000"",        # class: \'nv\' - to be revised\n        Name.Variable.Class:       ""#000000"",        # class: \'vc\' - to be revised\n        Name.Variable.Global:      ""#000000"",        # class: \'vg\' - to be revised\n        Name.Variable.Instance:    ""#000000"",        # class: \'vi\' - to be revised\n\n        Number:                    ""#990000"",        # class: \'m\'\n\n        Literal:                   ""#000000"",        # class: \'l\'\n        Literal.Date:              ""#000000"",        # class: \'ld\'\n\n        String:                    ""#4e9a06"",        # class: \'s\'\n        String.Backtick:           ""#4e9a06"",        # class: \'sb\'\n        String.Char:               ""#4e9a06"",        # class: \'sc\'\n        String.Doc:                ""italic #8f5902"", # class: \'sd\' - like a comment\n        String.Double:             ""#4e9a06"",        # class: \'s2\'\n        String.Escape:             ""#4e9a06"",        # class: \'se\'\n        String.Heredoc:            ""#4e9a06"",        # class: \'sh\'\n        String.Interpol:           ""#4e9a06"",        # class: \'si\'\n        String.Other:              ""#4e9a06"",        # class: \'sx\'\n        String.Regex:              ""#4e9a06"",        # class: \'sr\'\n        String.Single:             ""#4e9a06"",        # class: \'s1\'\n        String.Symbol:             ""#4e9a06"",        # class: \'ss\'\n\n        Generic:                   ""#000000"",        # class: \'g\'\n        Generic.Deleted:           ""#a40000"",        # class: \'gd\'\n        Generic.Emph:              ""italic #000000"", # class: \'ge\'\n        Generic.Error:             ""#ef2929"",        # class: \'gr\'\n        Generic.Heading:           ""bold #000080"",   # class: \'gh\'\n        Generic.Inserted:          ""#00A000"",        # class: \'gi\'\n        Generic.Output:            ""#888"",           # class: \'go\'\n        Generic.Prompt:            ""#745334"",        # class: \'gp\'\n        Generic.Strong:            ""bold #000000"",   # class: \'gs\'\n        Generic.Subheading:        ""bold #800080"",   # class: \'gu\'\n        Generic.Traceback:         ""bold #a40000"",   # class: \'gt\'\n    }\n'"
megnet/cli/__init__.py,0,b''
megnet/cli/meg.py,0,"b'#!/usr/bin/env python\n# coding: utf-8\n# Copyright (c) Pymatgen Development Team.\n# Distributed under the terms of the MIT License.\n\n""""""\nA master convenience script with many tools for vasp and structure analysis.\n""""""\n\nimport argparse\nimport sys\nfrom difflib import SequenceMatcher\nfrom pathlib import Path\n\nfrom tabulate import tabulate\nfrom pymatgen import Structure\nfrom megnet.utils.models import MEGNetModel\n\nDEFAULT_MODEL_PATH = Path(__file__).parent / "".."" / "".."" / ""mvl_models"" / ""mp-2019.4.1""\nDEFAULT_MODELS = [str(f) for f in DEFAULT_MODEL_PATH.glob(""*.hdf5"")]\n\n\ndef predict(args):\n    """"""\n    Handle view commands.\n\n    :param args: Args from command.\n    """"""\n    headers = [""Filename""]\n    output = []\n    models = []\n    prefix = """"\n    for i, mn in enumerate(args.models):\n        model = MEGNetModel.from_file(mn)\n        models.append(model)\n        if i == 0:\n            prefix = mn\n        else:\n            sm = SequenceMatcher(None, prefix, mn)\n            match = sm.find_longest_match(0, len(prefix), 0, len(mn))\n            prefix = prefix[0: match.size]\n        headers.append(""%s (%s)"" % (mn, str(model.metadata.get(""unit"", """")).strip(""log10"")))\n    headers = [h.lstrip(prefix) for h in headers]\n\n    for fn in args.structures:\n        structure = Structure.from_file(fn)\n        row = [fn]\n        for model in models:\n            val = model.predict_structure(structure).ravel()\n            if ""log10"" in str(model.metadata.get(""unit"", """")):\n                val = 10 ** val\n            row.append(val)\n        output.append(row)\n    print(tabulate(output, headers=headers))\n\n\ndef main():\n    """"""\n    Handle main.\n    """"""\n    parser = argparse.ArgumentParser(\n        description=""""""\n    meg is command-line interface to useful MEGNet tasks, e.g., prediction\n    using a built model, etc. To see the options for the\n    sub-commands, type ""meg sub-command -h"".""""""\n    )\n\n    subparsers = parser.add_subparsers()\n\n    parser_predict = subparsers.add_parser(\n        ""predict"", help=""Predict property using MEGNET."")\n\n    parser_predict.add_argument(""-s"", ""--structures"", dest=""structures"",\n                                type=str, nargs=""+"",\n                                help=""Structures to process"")\n    parser_predict.add_argument(\n        ""-m"", ""--models"", dest=""models"", type=str, nargs=""+"", default=DEFAULT_MODELS,\n        help=""Models to run."")\n    parser_predict.set_defaults(func=predict)\n\n    args = parser.parse_args()\n\n    try:\n        getattr(args, ""func"")\n    except AttributeError:\n        parser.print_help()\n        sys.exit(0)\n    args.func(args)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
megnet/data/__init__.py,0,b''
megnet/data/crystal.py,0,"b'from megnet.data.graph import StructureGraph, StructureGraphFixedRadius\nimport numpy as np\nfrom megnet.data.graph import GaussianDistance, Converter\nfrom monty.serialization import loadfn\nfrom pathlib import Path\nfrom copy import deepcopy\nfrom pymatgen import Element\nfrom pymatgen.analysis.local_env import NearNeighbors\nfrom pymatgen import Structure\n\nfrom typing import Union, List, Dict\n\n\nMODULE_DIR = Path(__file__).parent.absolute()\n\n\nclass CrystalGraph(StructureGraphFixedRadius):\n    """"""\n    Convert a crystal into a graph with z as atomic feature and distance as bond feature\n    one can optionally include state features\n    """"""\n\n    def __init__(self,\n                 nn_strategy: Union[str, NearNeighbors] = \'MinimumDistanceNNAll\',\n                 atom_converter: Converter = None,\n                 bond_converter: Converter = None,\n                 cutoff: float = 5.0\n                 ):\n        self.cutoff = cutoff\n        super().__init__(nn_strategy=nn_strategy, atom_converter=atom_converter,\n                         bond_converter=bond_converter, cutoff=self.cutoff)\n\n\nclass CrystalGraphWithBondTypes(StructureGraph):\n    """"""\n    Overwrite the bond attributes with bond types, defined simply by\n    the metallicity of the atoms forming the bond. Three types of\n    scenario is considered, nonmetal-nonmetal (type 0), metal-nonmetal (type 1), and\n    metal-metal (type 2)\n\n    """"""\n\n    def __init__(self,\n                 nn_strategy: Union[str, NearNeighbors] = \'VoronoiNN\',\n                 atom_converter: Converter = None,\n                 bond_converter: Converter = None):\n        super().__init__(nn_strategy=nn_strategy, atom_converter=atom_converter,\n                         bond_converter=bond_converter)\n\n    def convert(self, structure: Structure, state_attributes: List = None) -> Dict:\n        graph = super().convert(structure, state_attributes=state_attributes)\n        return self._get_bond_type(graph)\n\n    @staticmethod\n    def _get_bond_type(graph) -> Dict:\n        new_graph = deepcopy(graph)\n        elements = [Element.from_Z(i) for i in graph[\'atom\']]\n        for k, (i, j) in enumerate(zip(graph[\'index1\'], graph[\'index2\'])):\n            new_graph[\'bond\'][k] = elements[i].is_metal + elements[j].is_metal\n        return new_graph\n\n\ndef get_elemental_embeddings() -> Dict:\n    """"""\n    Provides the pre-trained elemental embeddings using formation energies,\n    which can be used to speed up the training of other models. The embeddings\n    are also extremely useful elemental descriptors that encode chemical\n    similarity that may be used in other ways. See\n\n    ""Graph Networks as a Universal Machine Learning Framework for Molecules\n    and Crystals"", https://arxiv.org/abs/1812.05055\n\n    :return: Dict of elemental embeddings as {symbol: length 16 string}\n    """"""\n    return loadfn(MODULE_DIR / ""resources"" /\n                  ""elemental_embedding_1MEGNet_layer.json"")\n'"
megnet/data/graph.py,0,"b'""""""Abstract classes and utility operations for building graph representations and\ndata loaders (known as Sequence objects in Keras).\nMost users will not need to interact with this module.""""""\nfrom abc import abstractmethod\nfrom operator import itemgetter\nfrom tensorflow.keras.utils import Sequence\n\nimport numpy as np\nfrom megnet.utils.general import expand_1st, to_list\nfrom megnet.utils.data import get_graphs_within_cutoff\nfrom monty.json import MSONable\nfrom megnet.data import local_env\nfrom inspect import signature\nfrom pymatgen.analysis.local_env import NearNeighbors\nfrom pymatgen import Structure\n\nfrom typing import Union, Dict, List, Any\n\n\nclass Converter(MSONable):\n    """"""\n    Base class for atom or bond converter\n    """"""\n    def convert(self, d: Any) -> Any:\n        raise NotImplementedError\n\n\nclass StructureGraph(MSONable):\n    """"""\n    This is a base class for converting converting structure into graphs or model inputs\n    Methods to be implemented are follows:\n        1. convert(self, structure)\n            This is to convert a structure into a graph dictionary\n        2. get_input(self, structure)\n            This method convert a structure directly to a model input\n        3. get_flat_data(self, graphs, targets)\n            This method process graphs and targets pairs and output model input list.\n    """"""\n\n    # TODO (wardlt): Consider making ""num_*_features"" funcs to simplify making a MEGNet model\n\n    def __init__(self,\n                 nn_strategy: Union[str, NearNeighbors] = None,\n                 atom_converter: Converter = None,\n                 bond_converter: Converter = None,\n                 **kwargs):\n\n        if isinstance(nn_strategy, str):\n            strategy = local_env.get(nn_strategy)\n            parameters = signature(strategy).parameters\n            param_dict = {i: j.default for i, j in parameters.items()}\n            for i, j in kwargs.items():\n                if i in param_dict:\n                    setattr(self, i, j)\n                    param_dict.update({i: j})\n            self.nn_strategy = strategy(**param_dict)\n        elif isinstance(nn_strategy, NearNeighbors):\n            self.nn_strategy = nn_strategy\n        elif nn_strategy is None:\n            self.nn_strategy = None\n        else:\n            raise RuntimeError(""Strategy not valid"")\n\n        self.atom_converter = atom_converter\n        self.bond_converter = bond_converter\n        if self.atom_converter is None:\n            self.atom_converter = self._get_dummy_converter()\n        if self.bond_converter is None:\n            self.bond_converter = self._get_dummy_converter()\n\n    def convert(self, structure: Structure, state_attributes: List = None) -> Dict:\n        """"""\n        Take a pymatgen structure and convert it to a index-type graph representation\n        The graph will have node, distance, index1, index2, where node is a vector of Z number\n        of atoms in the structure, index1 and index2 mark the atom indices forming the bond and separated by\n        distance.\n        For state attributes, you can set structure.state = [[xx, xx]] beforehand or the algorithm would\n        take default [[0, 0]]\n        Args:\n            state_attributes: (list) state attributes\n            structure: (pymatgen structure)\n            (dictionary)\n        """"""\n        state_attributes = state_attributes or getattr(structure, \'state\', None) or \\\n            np.array([[0.0, 0.0]], dtype=\'float32\')\n        index1 = []\n        index2 = []\n        bonds = []\n        if self.nn_strategy is None:\n            raise RuntimeError(""NearNeighbor strategy is not provided!"")\n        for n, neighbors in enumerate(self.nn_strategy.get_all_nn_info(structure)):\n            index1.extend([n] * len(neighbors))\n            for neighbor in neighbors:\n                index2.append(neighbor[\'site_index\'])\n                bonds.append(neighbor[\'weight\'])\n        atoms = self.get_atom_features(structure)\n        if np.size(np.unique(index1)) < len(atoms):\n            raise RuntimeError(""Isolated atoms found in the structure"")\n        else:\n            return {\'atom\': atoms,\n                    \'bond\': bonds,\n                    \'state\': state_attributes,\n                    \'index1\': index1,\n                    \'index2\': index2\n                    }\n\n    @staticmethod\n    def get_atom_features(structure) -> List[int]:\n        """"""\n        Get atom features from structure, may be overwritten\n        Args:\n            structure: (Pymatgen.Structure) pymatgen structure\n        Returns:\n            List of atomic numbers\n        """"""\n        return np.array([i.specie.Z for i in structure],\n                        dtype=\'int32\').tolist()\n\n    def __call__(self, structure: Structure) -> Dict:\n        return self.convert(structure)\n\n    def get_input(self, structure: Structure) -> List[np.ndarray]:\n        """"""\n        Turns a structure into model input\n        """"""\n        graph = self.convert(structure)\n        return self.graph_to_input(graph)\n\n    def graph_to_input(self, graph: Dict) -> List[np.ndarray]:\n        """"""\n        Turns a graph into model input\n        Args:\n            (dict): Dictionary description of the graph\n        Return:\n            ([np.ndarray]): Inputs in the form needed by MEGNet\n        """"""\n        gnode = [0] * len(graph[\'atom\'])\n        gbond = [0] * len(graph[\'index1\'])\n\n        return [expand_1st(self.atom_converter.convert(graph[\'atom\'])),\n                expand_1st(self.bond_converter.convert(graph[\'bond\'])),\n                expand_1st(np.array(graph[\'state\'])),\n                expand_1st(np.array(graph[\'index1\'], dtype=np.int32)),\n                expand_1st(np.array(graph[\'index2\'], dtype=np.int32)),\n                expand_1st(np.array(gnode, dtype=np.int32)),\n                expand_1st(np.array(gbond, dtype=np.int32))]\n\n    def get_flat_data(self, graphs: List[Dict], targets: List = None) -> tuple:\n        """"""\n        Expand the graph dictionary to form a list of features and targets tensors.\n        This is useful when the model is trained on assembled graphs on the fly.\n        Args:\n            graphs: (list of dictionary) list of graph dictionary for each structure\n            targets: (list of float or list) Optional: corresponding target\n                values for each structure\n        Returns:\n            tuple(node_features, edges_features, global_values, index1, index2, targets)\n        """"""\n\n        output = []  # Will be a list of arrays\n\n        # Convert the graphs to matrices\n        for feature in [\'atom\', \'bond\', \'state\', \'index1\', \'index2\']:\n            output.append([np.array(x[feature]) for x in graphs])\n\n        # If needed, add the targets\n        if targets is not None:\n            output.append([to_list(t) for t in targets])\n\n        return tuple(output)\n\n    @staticmethod\n    def _get_dummy_converter() -> \'DummyConverter\':\n        return DummyConverter()\n\n    def as_dict(self) -> Dict:\n        all_dict = super().as_dict()\n        if \'nn_strategy\' in all_dict:\n            nn_strategy = all_dict.pop(\'nn_strategy\')\n            all_dict.update({\'nn_strategy\': local_env.serialize(nn_strategy)})\n        return all_dict\n\n    @classmethod\n    def from_dict(cls, d: Dict) -> \'StructureGraph\':\n        if \'nn_strategy\' in d:\n            nn_strategy = d.pop(\'nn_strategy\')\n            nn_strategy_obj = local_env.deserialize(nn_strategy)\n            d.update({\'nn_strategy\': nn_strategy_obj})\n            return super().from_dict(d)\n        return super().from_dict(d)\n\n\nclass StructureGraphFixedRadius(StructureGraph):\n    """"""\n    This one uses a short cut to call find_points_in_spheres cython function in\n    pymatgen. It is orders of magnitude faster than previous implementations\n    """"""\n\n    def convert(self, structure: Structure, state_attributes: List = None) -> Dict:\n        """"""\n        Take a pymatgen structure and convert it to a index-type graph representation\n        The graph will have node, distance, index1, index2, where node is a vector of Z number\n        of atoms in the structure, index1 and index2 mark the atom indices forming the bond and separated by\n        distance.\n        For state attributes, you can set structure.state = [[xx, xx]] beforehand or the algorithm would\n        take default [[0, 0]]\n        Args:\n            state_attributes: (list) state attributes\n            structure: (pymatgen structure)\n            (dictionary)\n        """"""\n        state_attributes = state_attributes or getattr(structure, \'state\', None) or np.array(\n                [[0.0, 0.0]], dtype=\'float32\')\n        atoms = self.get_atom_features(structure)\n        index1, index2, _, bonds = get_graphs_within_cutoff(structure, self.nn_strategy.cutoff)\n\n        if np.size(np.unique(index1)) < len(atoms):\n            raise RuntimeError(""Isolated atoms found in the structure"")\n        else:\n            return {\'atom\': atoms,\n                    \'bond\': bonds,\n                    \'state\': state_attributes,\n                    \'index1\': index1,\n                    \'index2\': index2\n                    }\n\n    @classmethod\n    def from_structure_graph(cls, structure_graph: StructureGraph) -> \'StructureGraphFixedRadius\':\n        return cls(nn_strategy=structure_graph.nn_strategy,\n                   atom_converter=structure_graph.atom_converter,\n                   bond_converter=structure_graph.bond_converter)\n\n\nclass DummyConverter(Converter):\n    """"""\n    Dummy converter as a placeholder\n    """"""\n\n    def convert(self, d: Any) -> Any:\n        return d\n\n\nclass EmbeddingMap(Converter):\n    """"""\n    Convert an integer to a row vector in a feature matrix\n    Args:\n        feature_matrix: (np.ndarray) A matrix of shape (N, M)\n    """"""\n\n    def __init__(self, feature_matrix: np.ndarray):\n        self.feature_matrix = np.array(feature_matrix)\n\n    def convert(self, int_array: np.ndarray) -> np.ndarray:\n        """"""\n        convert atomic number to row vectors in the feature_matrix\n        Args:\n            int_array: (1d array) number array of length L\n        Returns\n            (matrix) L*M matrix with N the length of d and M the length of centers\n        """"""\n        return self.feature_matrix[int_array]\n\n\nclass GaussianDistance(Converter):\n    """"""\n    Expand distance with Gaussian basis sit at centers and with width 0.5.\n    Args:\n        centers: (np.array)\n        width: (float)\n    """"""\n\n    def __init__(self, centers: np.ndarray = np.linspace(0, 5, 100), width=0.5):\n        self.centers = centers\n        self.width = width\n\n    def convert(self, d: np.ndarray) -> np.ndarray:\n        """"""\n        expand distance vector d with given parameters\n        Args:\n            d: (1d array) distance array\n        Returns\n            (matrix) N*M matrix with N the length of d and M the length of centers\n        """"""\n        d = np.array(d)\n        return np.exp(-(d[:, None] - self.centers[None, :]) ** 2 / self.width ** 2)\n\n\nclass BaseGraphBatchGenerator(Sequence):\n    """"""Base class for classes that generate batches of training data for MEGNet.\n    Based on the Sequence class, which is the data loader equivalent for Keras.\n    Implementations of this base class must implement the :meth:`_generate_inputs`,\n    which generates the lists of graph descriptions for a batch.\n    The :meth:`process_atom_features` function and related functions are used to modify\n    the features for each atom, bond, and global features when creating a batch.\n    """"""\n\n    def __init__(self, dataset_size: int, targets: np.ndarray,\n                 batch_size: int = 128, shuffle: bool = True):\n        """"""\n        Args:\n            dataset_size (int): Number of entries in dataset\n            targets (ndarray): Feature to be predicted for each network\n            batch_size (int): Maximum batch size\n            shuffle (bool): Whether to shuffle the data after each step\n        """"""\n        if targets is not None:\n            self.targets = np.array(targets)\n        else:\n            self.targets = None\n        self.batch_size = batch_size\n        self.total_n = dataset_size\n        self.is_shuffle = shuffle\n        self.max_step = int(np.ceil(self.total_n / batch_size))\n        self.mol_index = np.arange(self.total_n)\n        if self.is_shuffle:\n            self.mol_index = np.random.permutation(self.mol_index)\n\n    def __len__(self) -> int:\n        return self.max_step\n\n    def _combine_graph_data(self,\n                            feature_list_temp: List[np.ndarray],\n                            connection_list_temp: List[np.ndarray],\n                            global_list_temp: List[np.ndarray],\n                            index1_temp: List[np.ndarray],\n                            index2_temp: List[np.ndarray]) -> List:\n        """"""Compile the matrices describing each graph into single matrices for the entire graph\n        Beyond concatenating the graph descriptions, this operation updates the indices of each\n        node to be sequential across all graphs so they are not duplicated between graphs\n        Args:\n            feature_list_temp ([ndarray]): List of features for each node\n            connection_list_temp ([ndarray]): List of features for each connection\n            global_list_temp ([ndarray]): List of global state for each graph\n            index1_temp ([ndarray]): List of indices for the start of each bond\n            index2_temp ([ndarray]): List of indices for the end of each bond\n        Returns:\n            (tuple): Input arrays describing the entire batch of networks:\n                - ndarray: Features for each node\n                - ndarray: Features for each connection\n                - ndarray: Global state for each graph\n                - ndarray: Indices for the start of each bond\n                - ndarray: Indices for the end of each bond\n                - ndarray: Index of graph associated with each node\n                - ndarray: Index of graph associated with each connection\n        """"""\n        # get atom\'s structure id\n        gnode = []\n        for i, j in enumerate(feature_list_temp):\n            gnode += [i] * len(j)\n        # get bond features from a batch of structures\n        # get bond\'s structure id\n        gbond = []\n        for i, j in enumerate(connection_list_temp):\n            gbond += [i] * len(j)\n\n        # assemble atom features together\n        feature_list_temp = np.concatenate(feature_list_temp, axis=0)\n        feature_list_temp = self.process_atom_feature(feature_list_temp)\n\n        # assemble bond feature together\n        connection_list_temp = np.concatenate(connection_list_temp, axis=0)\n        connection_list_temp = self.process_bond_feature(connection_list_temp)\n\n        # assemble state feature together\n        global_list_temp = np.concatenate(global_list_temp, axis=0)\n        global_list_temp = self.process_state_feature(global_list_temp)\n\n        # assemble bond indices\n        index1 = []\n        index2 = []\n        offset_ind = 0\n        for ind1, ind2 in zip(index1_temp, index2_temp):\n            index1 += [i + offset_ind for i in ind1]\n            index2 += [i + offset_ind for i in ind2]\n            offset_ind += (max(ind1) + 1)\n        # Compile the inputs in needed order\n        inputs = [expand_1st(feature_list_temp),\n                  expand_1st(connection_list_temp),\n                  expand_1st(global_list_temp),\n                  expand_1st(np.array(index1, dtype=np.int32)),\n                  expand_1st(np.array(index2, dtype=np.int32)),\n                  expand_1st(np.array(gnode, dtype=np.int32)),\n                  expand_1st(np.array(gbond, dtype=np.int32))]\n        return inputs\n\n    def on_epoch_end(self):\n        if self.is_shuffle:\n            self.mol_index = np.random.permutation(self.mol_index)\n\n    def process_atom_feature(self, x: np.ndarray) -> np.ndarray:\n        return x\n\n    def process_bond_feature(self, x: np.ndarray) -> np.ndarray:\n        return x\n\n    def process_state_feature(self, x: np.ndarray) -> np.ndarray:\n        return x\n\n    def __getitem__(self, index: int) -> tuple:\n        # Get the indices for this batch\n        batch_index = self.mol_index[index * self.batch_size:(index + 1) * self.batch_size]\n\n        # Get the inputs for each batch\n        inputs = self._generate_inputs(batch_index)\n\n        # Make the graph data\n        inputs = self._combine_graph_data(*inputs)\n\n        # Return the batch\n        if self.targets is None:\n            return inputs\n        else:\n            # get targets\n            it = itemgetter(*batch_index)\n            target_temp = itemgetter_list(self.targets, batch_index)\n            target_temp = np.atleast_2d(target_temp)\n\n            return inputs, expand_1st(target_temp)\n\n    @abstractmethod\n    def _generate_inputs(self, batch_index: int) -> tuple:\n        """"""Get the graph descriptions for each batch\n        Args:\n             batch_index ([int]): List of indices for training batch\n        Returns:\n            (tuple): Input arrays describing each network:\n                - [ndarray]: List of features for each node\n                - [ndarray]: List of features for each connection\n                - [ndarray]: List of global state for each graph\n                - [ndarray]: List of indices for the start of each bond\n                - [ndarray]: List of indices for the end of each bond\n        """"""\n        pass\n\n\nclass GraphBatchGenerator(BaseGraphBatchGenerator):\n    """"""\n    A generator class that assembles several structures (indicated by\n    batch_size) and form (x, y) pairs for model training.\n    Args:\n        atom_features: (list of np.array) list of atom feature matrix,\n        bond_features: (list of np.array) list of bond features matrix\n        state_features: (list of np.array) list of [1, G] state features,\n            where G is the global state feature dimension\n        index1_list: (list of integer) list of (M, ) one side atomic index of the bond,\n        M is different for different structures\n        index2_list: (list of integer) list of (M, ) the other side atomic\n            index of the bond, M is different for different structures,\n            but it has to be the same as the corresponding index1.\n        targets: (numpy array), N*1, where N is the number of structures\n        batch_size: (int) number of samples in a batch\n    """"""\n\n    def __init__(self,\n                 atom_features: List[np.ndarray],\n                 bond_features: List[np.ndarray],\n                 state_features: List[np.ndarray],\n                 index1_list: List[int],\n                 index2_list: List[int],\n                 targets: np.ndarray = None,\n                 batch_size: int = 128,\n                 is_shuffle: bool = True):\n        super().__init__(len(atom_features), targets, batch_size, is_shuffle)\n        self.atom_features = atom_features\n        self.bond_features = bond_features\n        self.state_features = state_features\n        self.index1_list = index1_list\n        self.index2_list = index2_list\n\n    def _generate_inputs(self, batch_index: int) -> tuple:\n        """"""Get the graph descriptions for each batch\n        Args:\n             batch_index ([int]): List of indices for training batch\n        Returns:\n            (tuple): Input arrays describe each network:\n                - [ndarray]: List of features for each nodes\n                - [ndarray]: List of features for each connection\n                - [ndarray]: List of global state for each graph\n                - [ndarray]: List of indices for the start of each bond\n                - [ndarray]: List of indices for the end of each bond\n        """"""\n\n        # Get the features and connectivity lists for this batch\n        feature_list_temp = itemgetter_list(self.atom_features, batch_index)\n        connection_list_temp = itemgetter_list(self.bond_features, batch_index)\n        global_list_temp = itemgetter_list(self.state_features, batch_index)\n        index1_temp = itemgetter_list(self.index1_list, batch_index)\n        index2_temp = itemgetter_list(self.index2_list, batch_index)\n\n        return feature_list_temp, connection_list_temp, global_list_temp, index1_temp, index2_temp\n\n\nclass GraphBatchDistanceConvert(GraphBatchGenerator):\n    """"""\n    Generate batch of structures with bond distance being expanded using a Expansor\n    Args:\n        atom_features: (list of np.array) list of atom feature matrix,\n        bond_features: (list of np.array) list of bond features matrix\n        state_features: (list of np.array) list of [1, G] state features, where G is the global state feature dimension\n        index1_list: (list of integer) list of (M, ) one side atomic index of the bond, M is different for different\n            structures\n        index2_list: (list of integer) list of (M, ) the other side atomic index of the bond, M is different for\n            different structures, but it has to be the same as the correponding index1.\n        targets: (numpy array), N*1, where N is the number of structures\n        batch_size: (int) number of samples in a batch\n        is_shuffle: (bool) whether to shuffle the structure, default to True\n        distance_converter: (bool) converter for processing the distances\n    """"""\n\n    def __init__(self,\n                 atom_features: List[np.ndarray],\n                 bond_features: List[np.ndarray],\n                 state_features: List[np.ndarray],\n                 index1_list: List[int],\n                 index2_list: List[int],\n                 targets: np.ndarray = None,\n                 batch_size: int = 128,\n                 is_shuffle: bool = True,\n                 distance_converter: Converter = None):\n        super().__init__(atom_features=atom_features,\n                         bond_features=bond_features,\n                         state_features=state_features,\n                         index1_list=index1_list,\n                         index2_list=index2_list,\n                         targets=targets,\n                         batch_size=batch_size,\n                         is_shuffle=is_shuffle)\n        self.distance_converter = distance_converter\n\n    def process_bond_feature(self, x) -> np.ndarray:\n        return self.distance_converter.convert(x)\n\n\ndef itemgetter_list(l, indices: List) -> tuple:\n    """"""\n    Get indices of l and return a tuple\n    Args:\n        l:  (list)\n        indices: (list) indices\n    Returns:\n        (tuple)\n    """"""\n    it = itemgetter(*indices)\n    if np.size(indices) == 1:\n        return it(l),\n    else:\n        return it(l)\n'"
megnet/data/local_env.py,0,"b'""""""\nVarious NearNeighbors strategies to define local environments\nof sites in structure/molecule. Most of them are directly\nfrom pymatgen.analysis.local_env. The suitable NearNeighbors\nshould have get_nn_info method implemented and this method\nneeds to return a list of dict with each entry having following keys\n[\'site\', \'image\', \'weight\', \'site_index\']\n\nthe weight will be used as the bond attributes in subsequent graph\nconstruction\n\n""""""\nfrom inspect import getfullargspec\nfrom typing import Dict, List, Union\n\nfrom pymatgen import Structure, Molecule\nfrom pymatgen.analysis import local_env\nfrom pymatgen.analysis.local_env import (\n    NearNeighbors,\n    VoronoiNN, JmolNN, MinimumDistanceNN, OpenBabelNN,\n    CovalentBondNN, MinimumVIRENN, MinimumOKeeffeNN,\n    BrunnerNN_reciprocal, BrunnerNN_real, BrunnerNN_relative,\n    EconNN, CrystalNN, CutOffDictNN, Critic2NN)\n\n\nclass MinimumDistanceNNAll(NearNeighbors):\n    """"""\n    Determine bonded sites by fixed cutoff\n\n    Args:.\n        cutoff (float): cutoff radius in Angstrom to look for trial\n            near-neighbor sites (default: 4.0).\n    """"""\n\n    def __init__(self, cutoff: float = 4.0):\n        self.cutoff = cutoff\n\n    def get_nn_info(self, structure: Structure,\n                    n: int) -> List[Dict]:\n        """"""\n        Get all near-neighbor sites as well as the associated image locations\n        and weights of the site with index n using the closest neighbor\n        distance-based method.\n\n        Args:\n            structure (Structure): input structure.\n            n (integer): index of site for which to determine near\n                neighbors.\n\n        Returns:\n            siw (list of tuples (Site, array, float)): tuples, each one\n                of which represents a neighbor site, its image location,\n                and its weight.\n        """"""\n\n        site = structure[n]\n        neighs_dists = structure.get_neighbors(site, self.cutoff)\n\n        siw = []\n        for nn in neighs_dists:\n            siw.append({\'site\': nn,\n                        \'image\': self._get_image(structure, nn),\n                        \'weight\': nn.nn_distance,\n                        \'site_index\': self._get_original_site(structure, nn)})\n        return siw\n\n\nclass AllAtomPairs(NearNeighbors):\n    """"""\n    Get all combinations of atoms as bonds in a molecule\n    """"""\n\n    def get_nn_info(self, molecule: Molecule, n: int) -> List[Dict]:\n        site = molecule[n]\n        siw = []\n        for i, s in enumerate(molecule):\n            if i != n:\n                siw.append({\'site\': s,\n                            \'image\': None,\n                            \'weight\': site.distance(s),\n                            \'site_index\': i})\n        return siw\n\n\ndef serialize(identifier: Union[str, NearNeighbors]):\n    """"""\n    Serialize the local env objects to a dictionary\n    Args:\n        identifier: (NearNeighbors object/str/None)\n\n    Returns: dictionary or None\n\n    """"""\n    if isinstance(identifier, str):\n        return identifier\n    elif isinstance(identifier, NearNeighbors):\n        args = getfullargspec(identifier.__class__.__init__).args\n        d = {""@module"": identifier.__class__.__module__,\n             ""@class"": identifier.__class__.__name__}\n        for arg in args:\n            if arg == \'self\':\n                continue\n            try:\n                a = identifier.__getattribute__(arg)\n                d[arg] = a\n            except AttributeError:\n                raise ValueError(""Cannot find the argument"")\n        if hasattr(identifier, ""kwargs""):\n            d.update(**identifier.kwargs)\n        return d\n    elif identifier is None:\n        return None\n    else:\n        raise ValueError(\'Unknown identifier for local environment \', identifier)\n\n\ndef deserialize(config: Dict):\n    """"""\n    Deserialize the config dict to object\n    Args:\n        config: (dict) nn_strategy config dict from seralize function\n\n    Returns: object\n\n    """"""\n    if config is None:\n        return None\n    if (\'@module\' not in config) or (\'@class\' not in config):\n        raise ValueError(""The config dict cannot be loaded"")\n    modname = config[\'@module\']\n    classname = config[\'@class\']\n    mod = __import__(modname, globals(), locals(), [classname])\n    cls_ = getattr(mod, classname)\n    data = {k: v for k, v in config.items() if not k.startswith(\'@\')}\n    return cls_(**data)\n\n\nNNDict = {i.__name__.lower(): i for i in [\n    NearNeighbors, VoronoiNN, JmolNN, MinimumDistanceNN,\n    OpenBabelNN, CovalentBondNN, MinimumVIRENN, MinimumOKeeffeNN,\n    BrunnerNN_reciprocal, BrunnerNN_real, BrunnerNN_relative,\n    EconNN, CrystalNN, CutOffDictNN, Critic2NN,\n    MinimumDistanceNNAll, AllAtomPairs]}\n\n\ndef get(identifier: Union[str, Dict, NearNeighbors]) -> NearNeighbors:\n    """"""\n    Deserialize the NearNeighbors\n    Args:\n        identifier (str, dict or NearNeighbors): target for deserialize\n\n    Returns: NearNeighbors instance\n\n    """"""\n    # deserialize NearNeighbor from str\n    if isinstance(identifier, str):\n        if identifier.lower() in NNDict:\n            return NNDict.get(identifier.lower())\n        # try pymatgen\'s local_env module\n        nn = getattr(local_env, identifier, None)\n        if nn is not None:\n            return nn\n\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n\n    if isinstance(identifier, NearNeighbors):\n        return identifier\n\n    raise ValueError(""%s not identified"" % str(identifier))\n'"
megnet/data/molecule.py,0,"b'""""""\nTools for creating graph inputs from molecule data\n""""""\n\nimport os\nimport sys\nimport itertools\nfrom functools import partial\nfrom collections import deque\nfrom multiprocessing import Pool\n\nimport numpy as np\nfrom pymatgen import Molecule, Element\nfrom pymatgen.io.babel import BabelMolAdaptor\nfrom pymatgen.analysis.local_env import NearNeighbors\n\nfrom megnet.utils.general import fast_label_binarize\nfrom megnet.data.graph import (StructureGraph, GaussianDistance,\n                               BaseGraphBatchGenerator, GraphBatchGenerator, Converter)\nfrom .qm9 import ring_to_vector\n\n\ntry:\n    import pybel\nexcept ImportError:\n    pybel = None\n\ntry:\n    from rdkit import Chem\nexcept ImportError:\n    Chem = None\n\nfrom typing import List, Dict, Union\n\n__date__ = \'12/01/2018\'\n\n# List of features to use by default for each atom\n_ATOM_FEATURES = [\'element\', \'chirality\', \'formal_charge\', \'ring_sizes\',\n                  \'hybridization\', \'donor\', \'acceptor\', \'aromatic\']\n\n# List of features to use by default for each bond\n_BOND_FEATURES = [\'bond_type\', \'same_ring\', \'spatial_distance\', \'graph_distance\']\n\n# List of elements in library to use by default\n_ELEMENTS = [\'H\', \'C\', \'N\', \'O\', \'F\']\n\n\nclass SimpleMolGraph(StructureGraph):\n    """"""\n    Default using all atom pairs as bonds. The distance between atoms are used\n    as bond features. By default the distance is expanded using a Gaussian\n    expansion with centers at np.linspace(0, 4, 20) and width of 0.5\n    """"""\n    def __init__(self,\n                 nn_strategy: Union[str, NearNeighbors] = \'AllAtomPairs\',\n                 atom_converter: Converter = None,\n                 bond_converter: Converter = None,\n                 ):\n        if bond_converter is None:\n            bond_converter = GaussianDistance(np.linspace(0, 4, 20), 0.5)\n        super().__init__(nn_strategy=nn_strategy, atom_converter=atom_converter,\n                         bond_converter=bond_converter)\n\n\nclass MolecularGraph(StructureGraph):\n    """"""Class for generating the graph inputs from a molecule\n\n    Computes many different features for the atoms and bonds in a molecule, and prepares them\n    in a form compatible with MEGNet models. The :meth:`convert` method takes a OpenBabel molecule\n    and, besides computing features, also encodes them in a form compatible with machine learning.\n    Namely, the `convert` method one-hot encodes categorical variables and concatenates\n    the atomic features\n\n    ## Atomic Features\n\n    This class can compute the following features for each atom\n\n    - `atomic_num`: The atomic number\n    - `element`: (categorical) Element identity. (Unlike `atomic_num`, element is one-hot-encoded)\n    - `chirality`: (categorical) R, S, or not a Chiral center (one-hot encoded).\n    - `formal_charge`: Formal charge of the atom\n    - `ring_sizes`: For rings with 9 or fewer atoms, how many unique rings\n    of each size include this atom\n    - `hybridization`: (categorical) Hybridization of atom: sp, sp2, sp3, sq.\n    planer, trig, octahedral, or hydrogen\n    - `donor`: (boolean) Whether the atom is a hydrogen bond donor\n    - `acceptor`: (boolean) Whether the atom is a hydrogen bond acceptor\n    - `aromatic`: (boolean) Whether the atom is part of an aromatic system\n\n    ## Atom Pair Features\n\n    The class also computes features for each pair of atoms\n\n    - `bond_type`: (categorical) Whether the pair are unbonded, or in a single, double, triple, or aromatic bond\n    - `same_ring`: (boolean) Whether the atoms are in the same aromatic ring\n    - `graph_distance`: Distance of shortest path between atoms on the bonding graph\n    - `spatial_distance`: Euclidean distance between the atoms. By default, this distance is expanded into\n        a vector of 20 different values computed using the `GaussianDistance` converter\n\n    """"""\n    def __init__(self,\n                 atom_features: List[str] = None,\n                 bond_features: List[str] = None,\n                 distance_converter: Converter = None,\n                 known_elements: List[str] = None):\n        """"""\n        Args:\n            atom_features ([str]): List of atom features to compute\n            bond_features ([str]): List of bond features to compute\n            distance_converter (DistanceCovertor): Tool used to expand distances\n                from a single scalar vector to an array of values\n            known_elements ([str]): List of elements expected to be in dataset. Used only if the\n                feature `element` is used to describe each atom\n        """"""\n\n        # Check if openbabel and RDKit are installed\n        if Chem is None or pybel is None:\n            raise RuntimeError(\'RDKit and openbabel must be installed\')\n\n        super().__init__()\n        if bond_features is None:\n            bond_features = _BOND_FEATURES\n        if atom_features is None:\n            atom_features = _ATOM_FEATURES\n        if distance_converter is None:\n            distance_converter = GaussianDistance(np.linspace(0, 4, 20), 0.5)\n        if known_elements is None:\n            known_elements = _ELEMENTS\n\n        # Check if all feature names are valid\n        if any(i not in _ATOM_FEATURES for i in atom_features):\n            bad_features = set(atom_features).difference(_ATOM_FEATURES)\n            raise ValueError(\'Unrecognized atom features: {}\'.format(\', \'.join(bad_features)))\n        self.atom_features = atom_features\n        if any(i not in _BOND_FEATURES for i in bond_features):\n            bad_features = set(bond_features).difference(_BOND_FEATURES)\n            raise ValueError(\'Unrecognized bond features: {}\'.format(\', \'.join(bad_features)))\n        self.bond_features = bond_features\n        self.known_elements = known_elements\n        self.distance_converter = distance_converter\n\n    def convert(self,\n                mol,  # type: ignore\n                state_attributes: List = None,\n                full_pair_matrix: bool = True) -> Dict:\n        """"""\n        Compute the representation for a molecule\n\n        Args\xef\xbc\x9a\n            mol (pybel.Molecule): Molecule to generate features for\n            state_attributes (list): State attributes. Uses average mass and number of bonds per atom as default\n            full_pair_matrix (bool): Whether to generate info for all atom pairs, not just bonded ones\n        Returns:\n            (dict): Dictionary of features\n        """"""\n\n        # Get the features features for all atoms and bonds\n        atom_features = []\n        atom_pairs = []\n        for idx, atom in enumerate(mol.atoms):\n            f = self.get_atom_feature(mol, atom)\n            atom_features.append(f)\n        atom_features = sorted(atom_features, key=lambda x: x[""coordid""])\n        num_atoms = mol.OBMol.NumAtoms()\n        for i, j in itertools.combinations(range(0, num_atoms), 2):\n            bond_feature = self.get_pair_feature(mol, i, j, full_pair_matrix)\n            if bond_feature:\n                atom_pairs.append(bond_feature)\n            else:\n                continue\n\n        # Compute the graph distance, if desired\n        if \'graph_distance\' in self.bond_features:\n            graph_dist = self._dijkstra_distance(atom_pairs)\n            for i in atom_pairs:\n                i.update({\'graph_distance\': graph_dist[i[\'a_idx\'], i[\'b_idx\']]})\n\n        # Generate the state attributes (that describe the whole network)\n        state_attributes = state_attributes or [\n            [mol.molwt / num_atoms,\n             len([i for i in atom_pairs if i[\'bond_type\'] > 0]) / num_atoms]\n        ]\n\n        # Get the atom features in the order they are requested by the user as a 2D array\n        atoms = []\n        for atom in atom_features:\n            atoms.append(self._create_atom_feature_vector(atom))\n\n        # Get the bond features in the order request by the user\n        bonds = []\n        index1_temp = []\n        index2_temp = []\n        for bond in atom_pairs:\n            # Store the index of each bond\n            index1_temp.append(bond.pop(\'a_idx\'))\n            index2_temp.append(bond.pop(\'b_idx\'))\n\n            # Get the desired bond features\n            bonds.append(self._create_pair_feature_vector(bond))\n\n        # Given the bonds (i,j), make it so (i,j) == (j, i)\n        index1 = index1_temp + index2_temp\n        index2 = index2_temp + index1_temp\n        bonds = bonds + bonds\n\n        # Sort the arrays by the beginning index\n        sorted_arg = np.argsort(index1)\n        index1 = np.array(index1)[sorted_arg].tolist()\n        index2 = np.array(index2)[sorted_arg].tolist()\n        bonds = np.array(bonds)[sorted_arg].tolist()\n\n        return {\'atom\': atoms,\n                \'bond\': bonds,\n                \'state\': state_attributes,\n                \'index1\': index1,\n                \'index2\': index2}\n\n    def _create_pair_feature_vector(self, bond: Dict) -> List[float]:\n        """"""Generate the feature vector from the bond feature dictionary\n\n        Handles the binarization of categorical variables, and performing the distance conversion\n\n        Args:\n            bond (dict): Features for a certain pair of atoms\n        Returns:\n            ([float]) Values converted to a vector\n            """"""\n        bond_temp = []\n        for i in self.bond_features:\n            # Some features require conversion (e.g., binarization)\n            if i in bond:\n                if i == ""bond_type"":\n                    bond_temp.extend(fast_label_binarize(bond[i], [0, 1, 2, 3, 4]))\n                elif i == ""same_ring"":\n                    bond_temp.append(int(bond[i]))\n                elif i == ""spatial_distance"":\n                    expanded = self.distance_converter.convert([bond[i]])[0]\n                    if isinstance(expanded, np.ndarray):\n                        # If we use a distance expansion\n                        bond_temp.extend(expanded.tolist())\n                    else:\n                        # If not\n                        bond_temp.append(expanded)\n                else:\n                    bond_temp.append(bond[i])\n        return bond_temp\n\n    def _create_atom_feature_vector(self, atom: dict) -> List[int]:\n        """"""Generate the feature vector from the atomic feature dictionary\n\n        Handles the binarization of categorical variables, and transforming the ring_sizes to a list\n\n        Args:\n            atom (dict): Dictionary of atomic features\n        Returns:\n            ([int]): Atomic feature vector\n        """"""\n        atom_temp = []\n        for i in self.atom_features:\n            if i == \'chirality\':\n                atom_temp.extend(fast_label_binarize(atom[i], [0, 1, 2]))\n            elif i == \'element\':\n                atom_temp.extend(fast_label_binarize(atom[i], self.known_elements))\n            elif i in [\'aromatic\', \'donor\', \'acceptor\']:\n                atom_temp.append(int(atom[i]))\n            elif i == \'hybridization\':\n                atom_temp.extend(fast_label_binarize(atom[i], [1, 2, 3, 4, 5, 6]))\n            elif i == \'ring_sizes\':\n                atom_temp.extend(ring_to_vector(atom[i]))\n            else:  # It is a scalar\n                atom_temp.append(atom[i])\n        return atom_temp\n\n    def _dijkstra_distance(self, pairs: List[Dict]) -> List[int]:\n        """"""\n        Compute the graph distance between each pair of atoms,\n        using the network defined by the bonded atoms.\n\n        Args:\n            pairs ([dict]): List of bond information\n        Returns:\n            ([int]) Distance for each pair of bonds\n        """"""\n        bonds = []\n        for p in pairs:\n            if p[\'bond_type\'] > 0:\n                bonds.append([p[\'a_idx\'], p[\'b_idx\']])\n        return dijkstra_distance(bonds)\n\n    def get_atom_feature(self,\n                         mol,  # type: ignore\n                         atom) -> Dict:  # type: ignore\n        """"""\n        Generate all features of a particular atom\n\n        Args:\n            mol (pybel.Molecule): Molecule being evaluated\n            atom (pybel.Atom): Specific atom being evaluated\n        Return:\n            (dict): All features for that atom\n        """"""\n\n        # Get the link to the OpenBabel representation of the atom\n        obatom = atom.OBAtom\n        atom_idx = atom.idx - 1  # (pybel atoms indices start from 1)\n\n        # Get the element\n        element = Element.from_Z(obatom.GetAtomicNum()).symbol\n\n        # Get the fast-to-compute properties\n        output = {""element"": element,\n                  ""atomic_num"": obatom.GetAtomicNum(),\n                  ""formal_charge"": obatom.GetFormalCharge(),\n                  ""hybridization"": 6 if element == \'H\' else obatom.GetHyb(),\n                  ""acceptor"": obatom.IsHbondAcceptor(),\n                  ""donor"": obatom.IsHbondDonorH() if atom.type == \'H\' else obatom.IsHbondDonor(),\n                  ""aromatic"": obatom.IsAromatic(),\n                  ""coordid"": atom.coordidx}\n\n        # Get the chirality, if desired\n        if \'chirality\' in self.atom_features:\n            # Determine whether the molecule has chiral centers\n            chiral_cc = self._get_chiral_centers(mol)\n            if atom_idx not in chiral_cc:\n                output[\'chirality\'] = 0\n            else:\n                # 1 --> \'R\', 2 --> \'S\'\n                output[\'chirality\'] = 1 if chiral_cc[atom_idx] == \'R\' else 2\n\n        # Find the rings, if desired\n        if \'ring_sizes\' in self.atom_features:\n            rings = mol.OBMol.GetSSSR()  # OpenBabel caches ring computation internally, no need to cache ourselves\n            output[\'ring_sizes\'] = [r.Size() for r in rings if r.IsInRing(atom.idx)]\n\n        return output\n\n    def create_bond_feature(self, mol, bid: int, eid: int):\n        """"""\n        Create information for a bond for a pair of atoms that are not actually bonded\n\n        Args:\n            mol (pybel.Molecule): Molecule being featurized\n            bid (int): Index of atom beginning of the bond\n            eid (int): Index of atom at the end of the bond\n        """"""\n        a1 = mol.OBMol.GetAtom(bid + 1)\n        a2 = mol.OBMol.GetAtom(eid + 1)\n        same_ring = mol.OBMol.AreInSameRing(a1, a2)\n        return {""a_idx"": bid,\n                ""b_idx"": eid,\n                ""bond_type"": 0,\n                ""same_ring"": True if same_ring else False,\n                ""spatial_distance"": a1.GetDistance(a2)}\n\n    def get_pair_feature(self, mol, bid: int,\n                         eid: int, full_pair_matrix: bool):\n        """"""\n        Get the features for a certain bond\n\n        Args:\n            mol (pybel.Molecule): Molecule being featurized\n            bid (int): Index of atom beginning of the bond\n            eid (int): Index of atom at the end of the bond\n            full_pair_matrix (bool): Whether to compute the matrix for every atom - even those that\n                are not actually bonded\n        """"""\n        # Find the bonded pair of atoms\n        bond = mol.OBMol.GetBond(bid + 1, eid + 1)\n        if not bond:  # If the bond is ordered in the other direction\n            bond = mol.OBMol.GetBond(eid + 1, bid + 1)\n\n        # If the atoms are not bonded\n        if not bond:\n            if full_pair_matrix:\n                return self.create_bond_feature(mol, bid, eid)\n            else:\n                return None\n\n        # Compute bond features\n        a1 = mol.OBMol.GetAtom(bid + 1)\n        a2 = mol.OBMol.GetAtom(eid + 1)\n        same_ring = mol.OBMol.AreInSameRing(a1, a2)\n        return {""a_idx"": bid,\n                ""b_idx"": eid,\n                ""bond_type"": 4 if bond.IsAromatic() else bond.GetBondOrder(),\n                ""same_ring"": True if same_ring else False,\n                ""spatial_distance"": a1.GetDistance(a2)}\n\n    def _get_rdk_mol(self, mol, format: str = \'smiles\'):\n        """"""\n        Return: RDKit Mol (w/o H)\n        """"""\n        if format == \'pdb\':\n            return Chem.rdmolfiles.MolFromPDBBlock(mol.write(""pdb""))\n        elif format == \'smiles\':\n            return Chem.rdmolfiles.MolFromSmiles(mol.write(""smiles""))\n\n    def _get_chiral_centers(self, mol):\n        """"""\n        Use RDKit to find the chiral centers with CIP(R/S) label\n\n        This provides the absolute stereochemistry.  The chiral label obtained\n        from pybabel and rdkit.mol.getchiraltag is relative positions of the bonds as provided\n\n        Args:\n            mol (Molecule): Molecule to asses\n        Return:\n            (dict): Keys are the atom index and values are the CIP label\n        """"""\n        mol_rdk = self._get_rdk_mol(mol, \'smiles\')\n        if mol_rdk is None:\n            # Conversion to RDKit has failed\n            return {}\n        else:\n            chiral_cc = Chem.FindMolChiralCenters(mol_rdk)\n            return dict(chiral_cc)\n\n\ndef dijkstra_distance(bonds: List[List[int]]) -> np.ndarray:\n    """"""\n    Compute the graph distance based on the dijkstra algorithm\n\n    Args:\n        bonds: (list of list), for example [[0, 1], [1, 2]] means two bonds formed by atom 0, 1 and atom 1, 2\n\n    Returns:\n        full graph distance matrix\n    """"""\n    nb_atom = max(itertools.chain(*bonds)) + 1\n    graph_dist = np.ones((nb_atom, nb_atom), dtype=np.int32) * np.infty\n    for bond in bonds:\n        graph_dist[bond[0], bond[1]] = 1\n        graph_dist[bond[1], bond[0]] = 1\n\n    queue = deque()  # Queue used in all loops\n    visited = set()  # Used in all loops\n    for i in range(nb_atom):\n        graph_dist[i, i] = 0\n        visited.clear()\n        queue.append(i)\n        while queue:\n            s = queue.pop()\n            visited.add(s)\n\n            for k in np.where(graph_dist[s, :] == 1)[0]:\n                if k not in visited:\n                    queue.append(k)\n                    graph_dist[i, k] = min(graph_dist[i, k],\n                                           graph_dist[i, s] + 1)\n                    graph_dist[k, i] = graph_dist[i, k]\n    return graph_dist\n\n\ndef mol_from_smiles(smiles: str):\n    mol = pybel.readstring(format=\'smi\', string=smiles)\n    mol.make3D()\n    return mol\n\n\ndef mol_from_pymatgen(mol: Molecule):\n    """"""\n    Args:\n        mol(Molecule)\n    """"""\n    mol = pybel.Molecule(BabelMolAdaptor(mol).openbabel_mol)\n    mol.make3D()\n    return mol\n\n\ndef mol_from_file(file_path: str, file_format: str = \'xyz\'):\n    """"""\n    Args:\n        file_path(str)\n        file_format(str): allow formats that open babel supports\n    """"""\n    mol = [r for r in pybel.readfile(format=file_format,\n                                     filename=file_path)][0]\n    return mol\n\n\ndef _convert_mol(mol: str, molecule_format: str, converter: MolecularGraph) -> Dict:\n    """"""Convert a molecule from string to its graph features\n\n    Utility function used in the graph generator.\n\n    The parse and convert operations are both in this function due to Pybel objects\n    not being serializable. By not using the Pybel representation of each molecule\n    as an input to this function, we can use multiprocessing to parallelize conversion\n    over molecules as strings can be passed as pickle objects to the worker threads but\n    but Pybel objects cannot.\n\n    Args:\n        mol (str): String representation of a molecule\n        molecule_format (str): Format of the string representation\n        converter (MolecularGraph): Tool used to generate graph representation\n    Returns:\n        (dict): Graph representation of the molecule\n    """"""\n\n    # Convert molecule into pybel format\n    if molecule_format == \'smiles\':\n        mol = mol_from_smiles(mol)  # Used to generate 3D coordinates/H atoms\n    else:\n        mol = pybel.readstring(molecule_format, mol)\n\n    return converter.convert(mol)\n\n\nclass MolecularGraphBatchGenerator(BaseGraphBatchGenerator):\n    """"""Generator that creates batches of molecular data by computing graph properties on demand\n\n    If your dataset is small enough that the descriptions of the whole dataset fit in memory,\n    we recommend using :class:`megnet.data.graph.GraphBatchGenerator` instead to avoid\n    the computational cost of dynamically computing graphs.""""""\n\n    def __init__(self,\n                 mols: List[str],\n                 targets: List[np.ndarray] = None,\n                 converter: MolecularGraph = None,\n                 molecule_format: str = \'xyz\',\n                 batch_size: int = 128,\n                 shuffle: bool = True,\n                 n_jobs: int = 1):\n        """"""\n        Args:\n            mols ([str]): List of the string reprensetations of each molecule\n            targets ([ndarray]): Properties of each molecule to be predicted\n            converter (MolecularGraph): Converter used to generate graph features\n            molecule_format (str): Format of each of the string representations in `mols`\n            batch_size (int): Target size for each batch\n            shuffle (bool): Whether to shuffle the training data after each epoch\n            n_jobs (int): Number of worker threads (None to use all threads).\n        """"""\n\n        super().__init__(len(mols), targets, batch_size, shuffle)\n        self.mols = np.array(mols)\n        if converter is None:\n            converter = MolecularGraph()\n        self.converter = converter\n        self.molecule_format = molecule_format\n        self.n_jobs = n_jobs\n\n        def mute():\n            sys.stdout = open(os.devnull, \'w\')\n            sys.stderr = open(os.devnull, \'w\')\n        self.pool = Pool(self.n_jobs, initializer=mute) if self.n_jobs != 1 else None\n\n    def __del__(self):\n        if self.pool is not None:\n            self.pool.close()  # Kill thread pool if generator is deleted\n\n    def _generate_inputs(self, batch_index: int) -> np.ndarray:\n        # Get the molecules for this batch\n        mols = self.mols[batch_index]\n\n        # Generate the graphs\n        graphs = self._generate_graphs(mols)\n\n        # Return them as flattened into array format\n        return self.converter.get_flat_data(graphs)\n\n    def _generate_graphs(self, mols: List[str]) -> List[Dict]:\n        """"""Generate graphs for a certain collection of molecules\n\n        Args:\n            mols ([string]): Molecules to process\n        Returns:\n            ([dict]): Graphs for all of the molecules\n        """"""\n        if self.pool is None:\n            graphs = [_convert_mol(m, self.molecule_format, self.converter) for m in mols]\n        else:\n            func = partial(_convert_mol, molecule_format=self.molecule_format,\n                           converter=self.converter)\n            graphs = self.pool.map(func, mols)\n        return graphs\n\n    def create_cached_generator(self) -> GraphBatchGenerator:\n        """"""Generates features for all of the molecules and stores them in memory\n\n        Returns:\n            (GraphBatchGenerator) Graph genereator that relies on having the graphs in memory\n        """"""\n\n        # Make all the graphs\n        graphs = self._generate_graphs(self.mols)\n\n        # Turn them into a fat array\n        inputs = self.converter.get_flat_data(graphs, self.targets)\n\n        return GraphBatchGenerator(*inputs, is_shuffle=self.is_shuffle,\n                                   batch_size=self.batch_size)\n'"
megnet/data/qm9.py,0,"b'""""""\nSimple qm9 utils, kept here for historical reasons\n""""""\nfrom monty.json import MSONable\n\nATOMNUM2TYPE = {""1"": 1, ""6"": 2, ""7"": 4, ""8"": 6, ""9"": 8}\n\n\nclass AtomNumberToTypeConverter(MSONable):\n    """"""\n    Convert atomic number Z into the atomic type in the QM9 dataset\n    """"""\n    def __init__(self, mapping=ATOMNUM2TYPE):\n        self.mapping = mapping\n\n    def convert(self, l):\n        return [self.mapping[str(i)] for i in l]\n\n\ndef ring_to_vector(l):\n    """"""\n    Convert the ring sizes vector to a fixed length vector\n    For example, l can be [3, 5, 5], meaning that the atom is involved\n    in 1 3-sized ring and 2 5-sized ring. This function will convert it into\n    [ 0, 0, 1, 0, 2, 0, 0, 0, 0, 0].\n    Args:\n        l: (list of integer) ring_sizes attributes\n    Returns:\n        (list of integer) fixed size list with the i-1 th element indicates number of\n            i-sized ring this atom is involved in.\n    """"""\n    return_l = [0] * 9\n    if l:\n        for i in l:\n            return_l[i - 1] += 1\n    return return_l\n'"
megnet/layers/__init__.py,0,"b'from megnet.layers.graph import MEGNetLayer, CrystalGraphLayer, InteractionLayer\nfrom megnet.layers.readout import Set2Set, LinearWithIndex\nfrom tensorflow.keras.layers import deserialize as keras_layer_deserialize\nfrom megnet.losses import mean_squared_error_with_scale\nfrom megnet.activations import softplus2\nfrom megnet.layers.featurizer import GaussianExpansion\n\n\n_CUSTOM_OBJECTS = globals()\n\n\n__all__ = [\n    ""MEGNetLayer"", ""CrystalGraphLayer"", ""InteractionLayer"",\n    ""Set2Set"", ""LinearWithIndex"",\n    ""GaussianExpansion"",\n    ""keras_layer_deserialize"", ""mean_squared_error_with_scale"",\n    ""softplus2""\n]\n'"
megnet/models/__init__.py,0,"b'""""""\nModels package, this package contains various graph-based models\n""""""\nfrom .base import GraphModel\nfrom .megnet import MEGNetModel\n\n__all__ = [\n    ""GraphModel"",\n    ""MEGNetModel""\n]\n'"
megnet/models/base.py,0,"b'""""""\nImplements basic GraphModels.\n""""""\n\nimport os\nfrom warnings import warn\nfrom typing import Dict, List, Union\n\nimport numpy as np\nfrom monty.serialization import dumpfn, loadfn\n\nfrom tensorflow.keras.backend import int_shape\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.models import Model\n\nfrom megnet.callbacks import ModelCheckpointMAE, ManualStop, ReduceLRUponNan\nfrom megnet.data.graph import GraphBatchDistanceConvert, GraphBatchGenerator, StructureGraph\nfrom megnet.utils.preprocessing import DummyScaler, Scaler\n\nfrom pymatgen import Structure\n\n\nclass GraphModel:\n    """"""\n    Composition of keras model and converter class for transfering structure\n    object to input tensors. We add methods to train the model from\n    (structures, targets) pairs\n    """"""\n\n    def __init__(self,\n                 model: Model,\n                 graph_converter: StructureGraph,\n                 target_scaler: Scaler = DummyScaler(),\n                 metadata: Dict = None,\n                 **kwargs):\n        """"""\n        Args:\n            model: (keras model)\n            graph_converter: (object) a object that turns a structure to a graph,\n                check `megnet.data.crystal`\n            target_scaler: (object) a scaler object for converting targets, check\n                `megnet.utils.preprocessing`\n            metadata: (dict) An optional dict of metadata associated with the model.\n                Recommended to incorporate some basic information such as units,\n                MAE performance, etc.\n        """"""\n        self.model = model\n        self.graph_converter = graph_converter\n        self.target_scaler = target_scaler\n        self.metadata = metadata or {}\n\n    def __getattr__(self, p):\n        return getattr(self.model, p)\n\n    def train(self,\n              train_structures: List[Structure],\n              train_targets: List[float],\n              validation_structures: List[Structure] = None,\n              validation_targets: List[float] = None,\n              epochs: int = 1000,\n              batch_size: int = 128,\n              verbose: int = 1,\n              callbacks: List[Callback] = None,\n              scrub_failed_structures: bool = False,\n              prev_model: str = None,\n              save_checkpoint: bool = True,\n              automatic_correction: bool = True,\n              lr_scaling_factor: float = 0.5,\n              patience: int = 500,\n              **kwargs) -> ""GraphModel"":\n        """"""\n        Args:\n            train_structures: (list) list of pymatgen structures\n            train_targets: (list) list of target values\n            validation_structures: (list) list of pymatgen structures as validation\n            validation_targets: (list) list of validation targets\n            epochs: (int) number of epochs\n            batch_size: (int) training batch size\n            verbose: (int) keras fit verbose, 0 no progress bar, 1 only at the epoch end and 2 every batch\n            callbacks: (list) megnet or keras callback functions for training\n            scrub_failed_structures: (bool) whether to scrub structures with failed graph computation\n            prev_model: (str) file name for previously saved model\n            save_checkpoint: (bool) whether to save checkpoint\n            automatic_correction: (bool) correct nan errors\n            lr_scaling_factor: (float, less than 1) scale the learning rate down when nan loss encountered\n            patience: (int) patience for early stopping\n            **kwargs:\n        """"""\n        train_graphs, train_targets = self.get_all_graphs_targets(train_structures, train_targets,\n                                                                  scrub_failed_structures=scrub_failed_structures)\n        if validation_structures is not None:\n            val_graphs, validation_targets = self.get_all_graphs_targets(\n                validation_structures, validation_targets, scrub_failed_structures=scrub_failed_structures)\n        else:\n            val_graphs = None\n\n        self.train_from_graphs(train_graphs,\n                               train_targets,\n                               validation_graphs=val_graphs,\n                               validation_targets=validation_targets,\n                               epochs=epochs,\n                               batch_size=batch_size,\n                               verbose=verbose,\n                               callbacks=callbacks,\n                               prev_model=prev_model,\n                               lr_scaling_factor=lr_scaling_factor,\n                               patience=patience,\n                               save_checkpoint=save_checkpoint,\n                               automatic_correction=automatic_correction,\n                               **kwargs\n                               )\n        return self\n\n    def train_from_graphs(self,\n                          train_graphs: List[Dict],\n                          train_targets: List[float],\n                          validation_graphs: List[Dict] = None,\n                          validation_targets: List[float] = None,\n                          epochs: int = 1000,\n                          batch_size: int = 128,\n                          verbose: int = 1,\n                          callbacks: List[Callback] = None,\n                          prev_model: str = None,\n                          lr_scaling_factor: float = 0.5,\n                          patience: int = 500,\n                          save_checkpoint: bool = True,\n                          automatic_correction: bool = True,\n                          **kwargs\n                          ) -> ""GraphModel"":\n        """"""\n        Args:\n            train_graphs: (list) list of graph dictionaries\n            train_targets: (list) list of target values\n            validation_graphs: (list) list of graphs as validation\n            validation_targets: (list) list of validation targets\n            epochs: (int) number of epochs\n            batch_size: (int) training batch size\n            verbose: (int) keras fit verbose, 0 no progress bar, 1 only at the epoch end and 2 every batch\n            callbacks: (list) megnet or keras callback functions for training\n            prev_model: (str) file name for previously saved model\n            lr_scaling_factor: (float, less than 1) scale the learning rate down when nan loss encountered\n            patience: (int) patience for early stopping\n            save_checkpoint: (bool) whether to save checkpoint\n            automatic_correction: (bool) correct nan errors\n            **kwargs:\n        """"""\n        # load from saved model\n        if prev_model:\n            self.load_weights(prev_model)\n        is_classification = \'entropy\' in self.model.loss\n        monitor = \'val_acc\' if is_classification else \'val_mae\'\n        mode = \'max\' if is_classification else \'min\'\n        dirname = kwargs.pop(\'dirname\', \'callback\')\n        if not os.path.isdir(dirname):\n            os.makedirs(dirname)\n        if callbacks is None:\n            # with this call back you can stop the model training by `touch STOP`\n            callbacks = [ManualStop()]\n        train_nb_atoms = [len(i[\'atom\']) for i in train_graphs]\n        train_targets = [self.target_scaler.transform(i, j) for i, j in zip(train_targets, train_nb_atoms)]\n\n        if validation_graphs is not None:\n            filepath = os.path.join(dirname, \'%s_{epoch:05d}_{%s:.6f}.hdf5\' % (monitor, monitor))\n            val_nb_atoms = [len(i[\'atom\']) for i in validation_graphs]\n            validation_targets = [self.target_scaler.transform(i, j) for i, j in zip(validation_targets, val_nb_atoms)]\n            val_inputs = self.graph_converter.get_flat_data(validation_graphs, validation_targets)\n\n            val_generator = self._create_generator(*val_inputs,\n                                                   batch_size=batch_size)\n            steps_per_val = int(np.ceil(len(validation_graphs) / batch_size))\n            if automatic_correction:\n                callbacks.extend([ReduceLRUponNan(filepath=filepath,\n                                                  monitor=monitor,\n                                                  mode=mode,\n                                                  factor=lr_scaling_factor,\n                                                  patience=patience,\n                                                  )])\n            if save_checkpoint:\n                callbacks.extend([ModelCheckpointMAE(filepath=filepath,\n                                                     monitor=monitor,\n                                                     mode=mode,\n                                                     save_best_only=True,\n                                                     save_weights_only=False,\n                                                     val_gen=val_generator,\n                                                     steps_per_val=steps_per_val,\n                                                     target_scaler=self.target_scaler)])\n                # avoid running validation twice in an epoch\n                val_generator = None\n                steps_per_val = None\n        else:\n            val_generator = None\n            steps_per_val = None\n        train_inputs = self.graph_converter.get_flat_data(train_graphs, train_targets)\n        # check dimension match\n        self.check_dimension(train_graphs[0])\n        train_generator = self._create_generator(*train_inputs, batch_size=batch_size)\n        steps_per_train = int(np.ceil(len(train_graphs) / batch_size))\n        self.fit(train_generator, steps_per_epoch=steps_per_train,\n                 validation_data=val_generator, validation_steps=steps_per_val,\n                 epochs=epochs, verbose=verbose, callbacks=callbacks, **kwargs)\n        return self\n\n    def check_dimension(self, graph: Dict) -> bool:\n        """"""\n        Check the model dimension against the graph converter dimension\n        Args:\n            graph: structure graph\n\n        Returns:\n\n        """"""\n        test_inp = self.graph_converter.graph_to_input(graph)\n        input_shapes = [i.shape for i in test_inp]\n\n        model_input_shapes = [int_shape(i) for i in self.model.inputs]\n\n        def _check_match(real_shape, tensor_shape):\n            if len(real_shape) != len(tensor_shape):\n                return False\n            matched = True\n            for i, j in zip(real_shape, tensor_shape):\n                if j is None:\n                    continue\n                else:\n                    if i == j:\n                        continue\n                    else:\n                        matched = False\n            return matched\n\n        for i, j, k in zip([\'atom features\', \'bond features\', \'state features\'],\n                           input_shapes[:3], model_input_shapes[:3]):\n            matched = _check_match(j, k)\n            if not matched:\n                raise ValueError(""The data dimension for %s is %s and does not match model ""\n                                 ""required shape of %s"" % (i, str(j), str(k)))\n\n    def get_all_graphs_targets(self, structures: List[Structure],\n                               targets: List[float],\n                               scrub_failed_structures: bool = False) -> tuple:\n        """"""\n        Compute the graphs from structures and spit out (graphs, targets) with options to\n        automatically remove structures with failed graph computations\n\n        Args:\n            structures: (list) pymatgen structure list\n            targets: (list) target property list\n            scrub_failed_structures: (bool) whether to scrub those failed structures\n\n        Returns:\n            graphs, targets\n\n        """"""\n        graphs_valid = []\n        targets_valid = []\n\n        for i, (s, t) in enumerate(zip(structures, targets)):\n            try:\n                graph = self.graph_converter.convert(s)\n                graphs_valid.append(graph)\n                targets_valid.append(t)\n            except Exception as e:\n                if scrub_failed_structures:\n                    warn(""structure with index %d failed the graph computations"" % i,\n                         UserWarning)\n                    continue\n                else:\n                    raise RuntimeError(str(e))\n        return graphs_valid, targets_valid\n\n    def predict_structure(self, structure: Structure) -> np.ndarray:\n        """"""\n        Predict property from structure\n\n        Args:\n            structure: pymatgen structure or molecule\n\n        Returns:\n            predicted target value\n        """"""\n        graph = self.graph_converter.convert(structure)\n        return self.predict_graph(graph)\n\n    def predict_graph(self, graph: Dict) -> np.ndarray:\n        """"""\n        Predict property from graph\n\n        Args:\n            graph: a graph dictionary, see megnet.data.graph\n\n        Returns:\n            predicted target value\n\n        """"""\n        inp = self.graph_converter.graph_to_input(graph)\n        pred = self.predict(inp)  # direct prediction, shape [1, 1, m]\n        return self.target_scaler.inverse_transform(pred[0, 0],\n                                                    len(graph[\'atom\']))\n\n    def _create_generator(self, *args, **kwargs) -> \\\n            Union[GraphBatchDistanceConvert, GraphBatchGenerator]:\n        if hasattr(self.graph_converter, \'bond_converter\'):\n            kwargs.update({\'distance_converter\': self.graph_converter.bond_converter})\n            return GraphBatchDistanceConvert(*args, **kwargs)\n        else:\n            return GraphBatchGenerator(*args, **kwargs)\n\n    def save_model(self, filename: str) -> None:\n        """"""\n        Save the model to a keras model hdf5 and a json config for additional\n        converters\n\n        Args:\n            filename: (str) output file name\n\n        Returns:\n            None\n        """"""\n        self.model.save(filename)\n        dumpfn(\n            {\n                \'graph_converter\': self.graph_converter,\n                \'target_scaler\': self.target_scaler,\n                \'metadata\': self.metadata\n            },\n            filename + \'.json\'\n        )\n\n    @classmethod\n    def from_file(cls, filename: str) -> \'GraphModel\':\n        """"""\n        Class method to load model from\n            filename for keras model\n            filename.json for additional converters\n\n        Args:\n            filename: (str) model file name\n\n        Returns\n            GraphModel\n        """"""\n        configs = loadfn(filename + \'.json\')\n        from tensorflow.keras.models import load_model\n        from megnet.layers import _CUSTOM_OBJECTS\n        model = load_model(filename, custom_objects=_CUSTOM_OBJECTS)\n        configs.update({\'model\': model})\n        return GraphModel(**configs)\n'"
megnet/models/megnet.py,0,"b'""""""\nImplements megnet models.\n""""""\n\nfrom typing import Dict, List, Callable\n\nimport numpy as np\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Input, Concatenate, Add, Embedding, Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.models import Model\n\nfrom megnet.layers import MEGNetLayer, Set2Set, GaussianExpansion\nfrom megnet.activations import softplus2\nfrom megnet.data.graph import GaussianDistance, StructureGraph\nfrom megnet.data.crystal import CrystalGraph\nfrom megnet.utils.preprocessing import DummyScaler, Scaler\nfrom megnet.config import DataType\nfrom .base import GraphModel\n\n\nclass MEGNetModel(GraphModel):\n    """"""\n    Construct a graph network model with or without explicit atom features\n    if n_feature is specified then a general graph model is assumed,\n    otherwise a crystal graph model with z number as atom feature is assumed.\n    """"""\n\n    def __init__(self,\n                 nfeat_edge: int = None,\n                 nfeat_global: int = None,\n                 nfeat_node: int = None,\n                 nblocks: int = 3,\n                 lr: float = 1e-3,\n                 n1: int = 64,\n                 n2: int = 32,\n                 n3: int = 16,\n                 nvocal: int = 95,\n                 embedding_dim: int = 16,\n                 nbvocal: int = None,\n                 bond_embedding_dim: int = None,\n                 ngvocal: int = None,\n                 global_embedding_dim: int = None,\n                 npass: int = 3,\n                 ntarget: int = 1,\n                 act: Callable = softplus2,\n                 is_classification: bool = False,\n                 loss: str = ""mse"",\n                 metrics: List[str] = None,\n                 l2_coef: float = None,\n                 dropout: float = None,\n                 graph_converter: StructureGraph = None,\n                 target_scaler: Scaler = DummyScaler(),\n                 optimizer_kwargs: Dict = None,\n                 dropout_on_predict: bool = False,\n                 **kwargs\n                 ):\n        """"""\n        Args:\n            nfeat_edge: (int) number of bond features\n            nfeat_global: (int) number of state features\n            nfeat_node: (int) number of atom features\n            nblocks: (int) number of MEGNetLayer blocks\n            lr: (float) learning rate\n            n1: (int) number of hidden units in layer 1 in MEGNetLayer\n            n2: (int) number of hidden units in layer 2 in MEGNetLayer\n            n3: (int) number of hidden units in layer 3 in MEGNetLayer\n            nvocal: (int) number of total element\n            embedding_dim: (int) number of embedding dimension\n            nbvocal: (int) number of bond types if bond attributes are types\n            bond_embedding_dim: (int) number of bond embedding dimension\n            ngvocal: (int) number of global types if global attributes are types\n            global_embedding_dim: (int) number of global embedding dimension\n            npass: (int) number of recurrent steps in Set2Set layer\n            ntarget: (int) number of output targets\n            act: (object) activation function\n            l2_coef: (float or None) l2 regularization parameter\n            is_classification: (bool) whether it is a classification task\n            loss: (object or str) loss function\n            metrics: (list or dict) List or dictionary of Keras metrics to be evaluated by the model during training\n                and testing\n            dropout: (float) dropout rate\n            graph_converter: (object) object that exposes a ""convert"" method for structure to graph conversion\n            target_scaler: (object) object that exposes a ""transform"" and ""inverse_transform"" methods for transforming\n                the target values\n            optimizer_kwargs (dict): extra keywords for optimizer, for example clipnorm and clipvalue\n            kwargs (dict): in the case where bond inputs are pure distances (not the expanded distances nor integers\n                for embedding, i.e., nfeat_edge=None and bond_embedding_dim=None),\n                kwargs can take additional inputs for expand the distance using Gaussian basis.\n                centers (np.ndarray): array for defining the Gaussian expansion centers\n                width (float): width for the Gaussian basis\n        """"""\n\n        # Build the MEG Model\n        model = make_megnet_model(nfeat_edge=nfeat_edge,\n                                  nfeat_global=nfeat_global,\n                                  nfeat_node=nfeat_node,\n                                  nblocks=nblocks,\n                                  n1=n1,\n                                  n2=n2,\n                                  n3=n3,\n                                  nvocal=nvocal,\n                                  embedding_dim=embedding_dim,\n                                  nbvocal=nbvocal,\n                                  bond_embedding_dim=bond_embedding_dim,\n                                  ngvocal=ngvocal,\n                                  global_embedding_dim=global_embedding_dim,\n                                  npass=npass,\n                                  ntarget=ntarget,\n                                  act=act,\n                                  is_classification=is_classification,\n                                  l2_coef=l2_coef,\n                                  dropout=dropout,\n                                  dropout_on_predict=dropout_on_predict,\n                                  **kwargs)\n\n        # Compile the model with the optimizer\n        loss = \'binary_crossentropy\' if is_classification else loss\n\n        opt_params = {\'lr\': lr}\n        if optimizer_kwargs is not None:\n            opt_params.update(optimizer_kwargs)\n        model.compile(Adam(**opt_params), loss, metrics=metrics)\n\n        if graph_converter is None:\n            graph_converter = CrystalGraph(cutoff=4,\n                                           bond_converter=GaussianDistance(np.linspace(0, 5, 100), 0.5))\n\n        super().__init__(model=model, target_scaler=target_scaler, graph_converter=graph_converter)\n\n    @classmethod\n    def from_url(cls, url: str) -> \'MEGNetModel\':\n        """"""\n        Download and load a model from a URL. E.g.\n        https://github.com/materialsvirtuallab/megnet/blob/master/mvl_models/mp-2019.4.1/formation_energy.hdf5\n\n        Args:\n            url: (str) url link of the model\n\n        Returns:\n            GraphModel\n        """"""\n        import urllib.request\n        fname = url.split(""/"")[-1]\n        urllib.request.urlretrieve(url, fname)\n        urllib.request.urlretrieve(url + "".json"", fname + "".json"")\n        return cls.from_file(fname)\n\n    @classmethod\n    def from_mvl_models(cls, name: str) -> \'MEGNetModel\':\n        from megnet.utils.models import load_model\n        return load_model(name)\n\n\ndef make_megnet_model(nfeat_edge: int = None,\n                      nfeat_global: int = None,\n                      nfeat_node: int = None,\n                      nblocks: int = 3,\n                      n1: int = 64,\n                      n2: int = 32,\n                      n3: int = 16,\n                      nvocal: int = 95,\n                      embedding_dim: int = 16,\n                      nbvocal: int = None,\n                      bond_embedding_dim: int = None,\n                      ngvocal: int = None,\n                      global_embedding_dim: int = None,\n                      npass: int = 3,\n                      ntarget: int = 1,\n                      act: Callable = softplus2,\n                      is_classification: bool = False,\n                      l2_coef: float = None,\n                      dropout: float = None,\n                      dropout_on_predict: bool = False,\n                      **kwargs\n                      ) -> Model:\n    """"""Make a MEGNet Model\n    Args:\n        nfeat_edge: (int) number of bond features\n        nfeat_global: (int) number of state features\n        nfeat_node: (int) number of atom features\n        nblocks: (int) number of MEGNetLayer blocks\n        n1: (int) number of hidden units in layer 1 in MEGNetLayer\n        n2: (int) number of hidden units in layer 2 in MEGNetLayer\n        n3: (int) number of hidden units in layer 3 in MEGNetLayer\n        nvocal: (int) number of total element\n        embedding_dim: (int) number of embedding dimension\n        nbvocal: (int) number of bond types if bond attributes are types\n        bond_embedding_dim: (int) number of bond embedding dimension\n        ngvocal: (int) number of global types if global attributes are types\n        global_embedding_dim: (int) number of global embedding dimension\n        npass: (int) number of recurrent steps in Set2Set layer\n        ntarget: (int) number of output targets\n        act: (object) activation function\n        l2_coef: (float or None) l2 regularization parameter\n        is_classification: (bool) whether it is a classification task\n        dropout: (float) dropout rate\n        dropout_on_predict (bool): Whether to use dropout during prediction and training\n        kwargs (dict): in the case where bond inputs are pure distances (not the expanded\n                distances nor integers for embedding, i.e., nfeat_edge=None and bond_embedding_dim=None),\n                kwargs can take additional inputs for expand the distance using Gaussian basis.\n\n                centers (np.ndarray): array for defining the Gaussian expansion centers\n                width (float): width for the Gaussian basis\n    Returns:\n        (Model) Keras model, ready to run\n    """"""\n\n    # Get the setting for the training kwarg of Dropout\n    dropout_training = True if dropout_on_predict else None\n\n    # atom inputs\n\n    if nfeat_node is None:\n        # only z as feature\n        x1 = Input(shape=(None,), dtype=DataType.tf_int, name=\'atom_int_input\')\n        x1_ = Embedding(nvocal, embedding_dim, name=\'atom_embedding\')(x1)\n    else:\n        x1 = Input(shape=(None, nfeat_node), name=\'atom_feature_input\')\n        x1_ = x1\n\n    # bond inputs\n    if nfeat_edge is None:\n        if bond_embedding_dim is not None:\n            # bond attributes are integers for embedding\n            x2 = Input(shape=(None,), dtype=DataType.tf_int, name=\'bond_int_input\')\n            x2_ = Embedding(nbvocal, bond_embedding_dim, name=\'bond_embedding\')(x2)\n        else:\n            # the bond attributes are float distance\n            x2 = Input(shape=(None, ), dtype=DataType.tf_float, name=\'bond_float_input\')\n            centers = kwargs.get(\'centers\', None)\n            width = kwargs.get(\'width\', None)\n            if centers is None and width is None:\n                raise ValueError(""If the bond attributes are single float values, ""\n                                 ""we expect the value to be expanded before passing ""\n                                 ""to the models. Therefore, `centers` and `width` for ""\n                                 ""Gaussian basis expansion are needed"")\n            x2_ = GaussianExpansion(centers=centers, width=width)(x2)  # type: ignore\n    else:\n        x2 = Input(shape=(None, nfeat_edge), name=\'bond_feature_input\')\n        x2_ = x2\n\n    # state inputs\n    if nfeat_global is None:\n        if global_embedding_dim is not None:\n            # global state inputs are embedding integers\n            x3 = Input(shape=(None,), dtype=DataType.tf_int, name=\'state_int_input\')\n            x3_ = Embedding(ngvocal, global_embedding_dim, name=\'state_embedding\')(x3)\n        else:\n            # take default vector of two zeros\n            x3 = Input(shape=(None, 2), dtype=DataType.tf_float, name=\'state_default_input\')\n            x3_ = x3\n    else:\n        x3 = Input(shape=(None, nfeat_global), name=\'state_feature_input\')\n        x3_ = x3\n    x4 = Input(shape=(None,), dtype=DataType.tf_int, name=\'bond_index_1_input\')\n    x5 = Input(shape=(None,), dtype=DataType.tf_int, name=\'bond_index_2_input\')\n    x6 = Input(shape=(None,), dtype=DataType.tf_int, name=\'atom_graph_index_input\')\n    x7 = Input(shape=(None,), dtype=DataType.tf_int, name=\'bond_graph_index_input\')\n\n    if l2_coef is not None:\n        reg = l2(l2_coef)\n    else:\n        reg = None\n\n    # two feedforward layers\n    def ff(x, n_hiddens=[n1, n2], name_prefix=None):\n        if name_prefix is None:\n            name_prefix = \'FF\'\n        out = x\n        for k, i in enumerate(n_hiddens):\n            out = Dense(i, activation=act, kernel_regularizer=reg, name=\'%s_%d\' % (name_prefix, k))(out)\n        return out\n\n    # a block corresponds to two feedforward layers + one MEGNetLayer layer\n    # Note the first block does not contain the feedforward layer since\n    # it will be explicitly added before the block\n    def one_block(a, b, c, has_ff=True, block_index=0):\n        if has_ff:\n            x1_ = ff(a, name_prefix=\'block_%d_atom_ff\' % block_index)\n            x2_ = ff(b, name_prefix=\'block_%d_bond_ff\' % block_index)\n            x3_ = ff(c, name_prefix=\'block_%d_state_ff\' % block_index)\n        else:\n            x1_ = a\n            x2_ = b\n            x3_ = c\n        out = MEGNetLayer(\n            [n1, n1, n2], [n1, n1, n2], [n1, n1, n2],\n            pool_method=\'mean\', activation=act, kernel_regularizer=reg, name=\'megnet_%d\' % block_index)(\n            [x1_, x2_, x3_, x4, x5, x6, x7])\n\n        x1_temp = out[0]\n        x2_temp = out[1]\n        x3_temp = out[2]\n        if dropout:\n            x1_temp = Dropout(dropout, name=\'dropout_atom_%d\' % block_index)(x1_temp, training=dropout_training)\n            x2_temp = Dropout(dropout, name=\'dropout_bond_%d\' % block_index)(x2_temp, training=dropout_training)\n            x3_temp = Dropout(dropout, name=\'dropout_state_%d\' % block_index)(x3_temp, training=dropout_training)\n        return x1_temp, x2_temp, x3_temp\n\n    x1_ = ff(x1_, name_prefix=\'preblock_atom\')\n    x2_ = ff(x2_, name_prefix=\'preblock_bond\')\n    x3_ = ff(x3_, name_prefix=\'preblock_state\')\n    for i in range(nblocks):\n        if i == 0:\n            has_ff = False\n        else:\n            has_ff = True\n        x1_1 = x1_\n        x2_1 = x2_\n        x3_1 = x3_\n        x1_1, x2_1, x3_1 = one_block(x1_1, x2_1, x3_1, has_ff, block_index=i)\n        # skip connection\n        x1_ = Add(name=\'block_%d_add_atom\' % i)([x1_, x1_1])\n        x2_ = Add(name=\'block_%d_add_bond\' % i)([x2_, x2_1])\n        x3_ = Add(name=\'block_%d_add_state\' % i)([x3_, x3_1])\n\n    # print(Set2Set(T=npass, n_hidden=n3, kernel_regularizer=reg, name=\'set2set_atom\'\n    #             ).compute_output_shape([i.shape for i in [x1_, x6]]))\n    # set2set for both the atom and bond\n    node_vec = Set2Set(T=npass, n_hidden=n3, kernel_regularizer=reg, name=\'set2set_atom\')([x1_, x6])\n    # print(\'Node vec\', node_vec)\n    edge_vec = Set2Set(T=npass, n_hidden=n3, kernel_regularizer=reg, name=\'set2set_bond\')([x2_, x7])\n    # concatenate atom, bond, and global\n    final_vec = Concatenate(axis=-1)([node_vec, edge_vec, x3_])\n    if dropout:\n        final_vec = Dropout(dropout, name=\'dropout_final\')(final_vec, training=dropout_training)\n    # final dense layers\n    final_vec = Dense(n2, activation=act, kernel_regularizer=reg, name=\'readout_0\')(final_vec)\n    final_vec = Dense(n3, activation=act, kernel_regularizer=reg, name=\'readout_1\')(final_vec)\n    if is_classification:\n        final_act = \'sigmoid\'\n    else:\n        final_act = None\n    out = Dense(ntarget, activation=final_act, name=\'readout_2\')(final_vec)\n    model = Model(inputs=[x1, x2, x3, x4, x5, x6, x7], outputs=out)\n    return model\n'"
megnet/tests/test_activations.py,0,"b""import unittest\nfrom megnet.activations import softplus2\nimport numpy as np\n\n\ndef softplus_np(x):\n    return np.log(np.exp(x) + 1) - np.log(2.)\n\n\nclass TestSP(unittest.TestCase):\n    def test_softplus(self):\n        x = 10.0\n        self.assertAlmostEqual(softplus2(x).numpy(), softplus_np(x), places=5)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
megnet/tests/test_callbacks.py,0,"b'import os\nimport unittest\n\nimport numpy as np\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nimport tensorflow.keras.backend as kb\nfrom tensorflow.keras.utils import Sequence\n\nfrom monty.tempfile import ScratchDir\n\nfrom megnet.callbacks import ModelCheckpointMAE, ManualStop, ReduceLRUponNan\nfrom megnet.layers import MEGNetLayer\n\n\nclass Generator(Sequence):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def __len__(self):\n        return 1\n\n    def __getitem__(self, index):\n        return self.x, self.y\n\n\nclass TestCallBack(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.n_feature = 5\n        cls.n_bond_features = 6\n        cls.n_global_features = 2\n        cls.inp = [\n            Input(shape=(None, cls.n_feature)),\n            Input(shape=(None, cls.n_bond_features)),\n            Input(shape=(None, cls.n_global_features)),\n            Input(shape=(None,), dtype=\'int32\'),\n            Input(shape=(None,), dtype=\'int32\'),\n            Input(shape=(None,), dtype=\'int32\'),\n            Input(shape=(None,), dtype=\'int32\'),\n        ]\n        units_v = [2, 2]\n        units_e = [2, 2]\n        units_u = [2, ]\n        layer = MEGNetLayer(units_v, units_e, units_u)\n        out = layer(cls.inp)\n        cls.out = Dense(1)(out[2])\n        cls.model = Model(inputs=cls.inp, outputs=cls.out)\n        cls.model.compile(loss=\'mse\', optimizer=\'adam\')\n        cls.x = [np.random.normal(size=(1, 4, cls.n_feature)),\n                 np.random.normal(size=(1, 6, cls.n_bond_features)),\n                 np.random.normal(size=(1, 2, cls.n_global_features)),\n                 np.array([[0, 0, 1, 1, 2, 3]]),\n                 np.array([[1, 1, 0, 0, 3, 2]]),\n                 np.array([[0, 0, 1, 1]]),\n                 np.array([[0, 0, 0, 0, 1, 1]]),\n                 ]\n        cls.y = np.random.normal(size=(1, 2, 1))\n        cls.train_gen = Generator(cls.x, cls.y)\n\n    def test_manual_stop(self):\n        with ScratchDir("".""):\n            callbacks = [ManualStop()]\n            epoch_count = 0\n            for i in range(3):\n                if not getattr(self.model, ""stop_training"", False):\n                    self.model.fit(self.train_gen, steps_per_epoch=1, epochs=1,\n                                   callbacks=callbacks, verbose=0)\n                    epoch_count += 1\n            self.assertEqual(epoch_count, 3)\n            open(\'STOP\', \'a\').close()\n            for i in range(3):\n                if not getattr(self.model, \'stop_training\', False):\n                    self.model.fit(self.train_gen, steps_per_epoch=1, epochs=1,\n                                   callbacks=callbacks, verbose=0)\n                    epoch_count += 1\n            self.assertEqual(epoch_count, 4)\n            os.remove(\'STOP\')\n\n    def test_reduce_lr_upon_nan(self):\n        with ScratchDir(\'.\'):\n            callbacks = [ReduceLRUponNan(patience=100)]\n            self.assertAlmostEqual(float(kb.get_value(self.model.optimizer.lr)), 1e-3)\n            gen = Generator(self.x, np.array([1, np.nan]).reshape((1, 2, 1)))\n            self.model.fit(gen, steps_per_epoch=1, epochs=1, callbacks=callbacks, verbose=0)\n            self.assertAlmostEqual(float(kb.get_value(self.model.optimizer.lr)), 0.5e-3)\n\n            inp = [\n                Input(shape=(None, self.n_feature)),\n                Input(shape=(None, self.n_bond_features)),\n                Input(shape=(None, self.n_global_features)),\n                Input(shape=(None,), dtype=\'int32\'),\n                Input(shape=(None,), dtype=\'int32\'),\n                Input(shape=(None,), dtype=\'int32\'),\n                Input(shape=(None,), dtype=\'int32\'),\n            ]\n\n            units_v = [2, 2]\n            units_e = [2, 2]\n            units_u = [2, ]\n\n            layer = MEGNetLayer(units_v, units_e, units_u)\n            out = layer(inp)\n            out = Dense(1)(out[2])\n            model = Model(inputs=inp, outputs=out)\n            model.compile(loss=\'mse\', optimizer=\'adam\')\n            x = [np.random.normal(size=(1, 4, self.n_feature)),\n                 np.random.normal(size=(1, 6, self.n_bond_features)),\n                 np.random.normal(size=(1, 2, self.n_global_features)),\n                 np.array([[0, 0, 1, 1, 2, 3]]),\n                 np.array([[1, 1, 0, 0, 3, 2]]),\n                 np.array([[0, 0, 1, 1]]),\n                 np.array([[0, 0, 0, 0, 1, 1]]),\n                 ]\n            y = np.random.normal(size=(1, 2, 1))\n            train_gen = Generator(x, y)\n\n            callbacks = [ReduceLRUponNan(filepath=\'./val_mae_{epoch:05d}_{val_mae:.6f}.hdf5\',\n                                         patience=100),\n                         ModelCheckpointMAE(filepath=\'./val_mae_{epoch:05d}_{val_mae:.6f}.hdf5\',\n                                            val_gen=train_gen,\n                                            steps_per_val=1)\n                         ]\n            # 1. involve training and saving\n            model.fit(train_gen, steps_per_epoch=1, epochs=2, callbacks=callbacks, verbose=1)\n            # 2. throw nan loss, trigger ReduceLRUponNan\n            model.fit(gen, steps_per_epoch=1, epochs=1, callbacks=callbacks, verbose=1)\n            model.fit(gen, steps_per_epoch=1, epochs=1, callbacks=callbacks, verbose=1)\n            # 3. Normal training, recover saved model from 1\n            model.fit(train_gen, steps_per_epoch=1, epochs=2, callbacks=callbacks, verbose=1)\n\n            self.assertAlmostEqual(float(kb.get_value(model.optimizer.lr)), 0.25e-3)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/tests/test_config.py,2,"b'""""""\nTest the data types\n""""""\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom megnet.config import DataType, set_global_dtypes\n\n\nclass TestDataType(unittest.TestCase):\n    def test_set_16(self):\n        DataType.set_dtype(""16"")\n        self.assertTrue(DataType.np_int, np.int16)\n        self.assertTrue(DataType.np_float, np.float16)\n        self.assertTrue(DataType.tf_int, tf.int16)\n        self.assertTrue(DataType.tf_float, tf.float16)\n        set_global_dtypes(""32"")\n        self.assertTrue(DataType.np_int, np.int32)\n\n    def test_set_32(self):\n        DataType.set_dtype(""32"")\n        self.assertTrue(DataType.np_int, np.int32)\n        self.assertTrue(DataType.np_float, np.float32)\n\n    def test_wrong(self):\n        self.assertRaises(ValueError, DataType.set_dtype, ""64"")\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/tests/test_losses.py,0,"b""import unittest\nimport numpy as np\n\nfrom megnet.losses import mean_squared_error_with_scale\n\n\nclass TestLosses(unittest.TestCase):\n    def test_mse(self):\n        x = np.array([0.1, 0.2, 0.3])\n        y = np.array([0.05, 0.15, 0.25])\n        loss = mean_squared_error_with_scale(x, y, scale=100)\n        self.assertAlmostEqual(loss.numpy(), np.mean((x-y)**2) * 100)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
megnet/tests/test_models.py,0,"b'import unittest\nimport numpy as np\n\nfrom megnet.models import MEGNetModel, GraphModel\nfrom megnet.callbacks import ModelCheckpointMAE, ManualStop\nfrom megnet.data.graph import GaussianDistance\nfrom megnet.data.crystal import CrystalGraph\nfrom glob import glob\nimport os\nfrom pymatgen import Structure, Lattice\nimport shutil\nfrom monty.tempfile import ScratchDir\nfrom tensorflow.keras.utils import Sequence\nfrom pymatgen.util.testing import PymatgenTest\n\ncwd = os.path.dirname(os.path.abspath(__file__))\n\n\nclass TestModel(PymatgenTest):\n    @classmethod\n    def setUpClass(cls):\n        cls.n_feature = 3\n        cls.n_bond_features = 10\n        cls.n_global_features = 2\n\n        class Generator(Sequence):\n            def __init__(self, x, y):\n                self.x = x\n                self.y = y\n            def __len__(self):\n                return 10\n            def __getitem__(self, index):\n                return  self.x, self.y\n\n        x_crystal = [np.array([1, 2, 3, 4]).reshape((1, -1)),\n                     np.random.normal(size=(1, 6, cls.n_bond_features)),\n                     np.random.normal(size=(1, 2, cls.n_global_features)),\n                     np.array([[0, 0, 1, 1, 2, 3]]),\n                     np.array([[1, 1, 0, 0, 3, 2]]),\n                     np.array([[0, 0, 1, 1]]),\n                     np.array([[0, 0, 0, 0, 1, 1]]),\n                     ]\n\n        y = np.random.normal(size=(1, 2, 1))\n        cls.train_gen_crystal = Generator(x_crystal, y)\n        x_mol = [np.random.normal(size=(1, 4, cls.n_feature)),\n                 np.random.normal(size=(1, 6, cls.n_bond_features)),\n                 np.random.normal(size=(1, 2, cls.n_global_features)),\n                 np.array([[0, 0, 1, 1, 2, 3]]),\n                 np.array([[1, 1, 0, 0, 3, 2]]),\n                 np.array([[0, 0, 1, 1]]),\n                 np.array([[0, 0, 0, 0, 1, 1]]),\n                 ]\n        y = np.random.normal(size=(1, 2, 1))\n        cls.train_gen_mol = Generator(x_mol, y)\n\n        cls.model = MEGNetModel(10, 2, nblocks=1, lr=1e-2,\n                                n1=4, n2=4, n3=4, npass=1, ntarget=1,\n                                graph_converter=CrystalGraph(bond_converter=GaussianDistance(np.linspace(0, 5, 10), 0.5)),\n                                )\n        cls.model2 = MEGNetModel(10, 2, nblocks=1, lr=1e-2,\n                                 n1=4, n2=4, n3=4, npass=1, ntarget=2,\n                                 graph_converter=CrystalGraph(bond_converter=GaussianDistance(np.linspace(0, 5, 10), 0.5)),\n                                 )\n\n    def test_train_pred(self):\n        s = Structure.from_file(os.path.join(cwd, \'../data/tests/cifs/BaTiO3_mp-2998_computed.cif\'))\n        structures = [s.copy(), s.copy(), s.copy(), s.copy()]\n        targets = [0.1, 0.1, 0.1, 0.1]\n        with ScratchDir(\'.\'):\n            self.model.train(structures,\n                             targets,\n                             validation_structures=structures[:2],\n                             validation_targets=[0.1, 0.1],\n                             batch_size=2,\n                             epochs=1,\n                             verbose=2)\n            preds = self.model.predict_structure(structures[0])\n\n            # isolated atom error\n            for s in structures[3:]:\n                s.apply_strain(3)\n            with self.assertRaises(RuntimeError) as context:\n                self.model.train(structures,\n                                 targets,\n                                 epochs=1,\n                                 verbose=2,\n                                 scrub_failed_structures=False)\n                self.assertTrue(\'Isolated atoms found\' in str(context.exception))\n\n            with self.assertRaises(Exception) as context:\n                self.model.train(structures,\n                                 targets,\n                                 epochs=1,\n                                 verbose=2,\n                                 scrub_failed_structures=True)\n                self.assertTrue(\'structure with index\' in str(context.exception))\n\n            if os.path.isdir(\'callback\'):\n                shutil.rmtree(\'callback\')\n            self.assertTrue(np.size(preds) == 1)\n\n    def test_single_atom_structure(self):\n        s = Structure(Lattice.cubic(3), [\'Si\'], [[0, 0, 0]])\n        with ScratchDir(\'.\'):\n            # initialize the model\n            self.model.train([s, s], [0.1, 0.1], epochs=1)\n            pred = self.model.predict_structure(s)\n            self.assertEqual(len(pred.ravel()), 1)\n\n    def test_two_targets(self):\n        s = Structure(Lattice.cubic(3), [\'Si\'], [[0, 0, 0]])\n        with ScratchDir(\'.\'):\n            # initialize the model\n            self.model2.train([s, s], [[0.1, 0.2], [0.1, 0.2]], epochs=1)\n            pred = self.model2.predict_structure(s)\n            self.assertEqual(len(pred.ravel()), 2)\n\n    def test_save_and_load(self):\n        weights1 = self.model.get_weights()\n        with ScratchDir(\'.\'):\n            self.model.metadata = {""units"": ""eV""} # This is just a random\n            self.model.save_model(\'test.hdf5\')\n            model2 = GraphModel.from_file(\'test.hdf5\')\n            self.assertEqual(model2.metadata, {""units"": ""eV""})\n        weights2 = model2.get_weights()\n        self.assertTrue(np.allclose(weights1[0], weights2[0]))\n\n    def test_check_dimension(self):\n        gc = CrystalGraph(bond_converter=GaussianDistance(np.linspace(0, 5, 20), 0.5))\n        s = Structure(Lattice.cubic(3), [\'Si\'], [[0, 0, 0]])\n        graph = gc.convert(s)\n        model = MEGNetModel(10, 2, nblocks=1, lr=1e-2,\n                            n1=4, n2=4, n3=4, npass=1, ntarget=1,\n                            graph_converter=gc,\n                            )\n        with self.assertRaises(Exception) as context:\n            model.check_dimension(graph)\n            self.assertTrue(\'The data dimension for bond\' in str(context.exception))\n\n    def test_crystal_model(self):\n        callbacks = [ModelCheckpointMAE(filepath=\'./val_mae_{epoch:05d}_{val_mae:.6f}.hdf5\',\n                                        save_best_only=True,\n                                        val_gen=self.train_gen_crystal,\n                                        steps_per_val=1),\n                     ManualStop()]\n        with ScratchDir(\'.\'):\n            self.model.fit(self.train_gen_crystal, steps_per_epoch=1, epochs=2, verbose=1,\n                           callbacks=callbacks)\n            model_files = glob(\'val_mae*.hdf5\')\n            self.assertGreater(len(model_files), 0)\n            for i in model_files:\n                os.remove(i)\n\n    def test_crystal_model_v2(self):\n        cg = CrystalGraph()\n        s = Structure(Lattice.cubic(3), [\'Si\'], [[0, 0, 0]])\n        with ScratchDir(\'.\'):\n            model = MEGNetModel(nfeat_edge=None, nfeat_global=2, nblocks=1, lr=1e-2,\n                                n1=4, n2=4, n3=4, npass=1, ntarget=1,\n                                graph_converter=cg,\n                                centers=np.linspace(0, 4, 10),\n                                width=0.5\n                                )\n            model = model.train([s, s], [0.1, 0.1], epochs=2)\n            t = model.predict_structure(s)\n            self.assertTrue(t.shape == (1, ))\n\n    def test_from_url(self):\n        with ScratchDir("".""):\n            model = MEGNetModel.from_url(""https://github.com/materialsvirtuallab/megnet/raw/master/mvl_models/mp-2019.4.1/formation_energy.hdf5"")\n            li2o = self.get_structure(""Li2O"")\n            self.assertAlmostEqual(float(model.predict_structure(li2o)),\n                                   -2.0152957439422607, places=4)\n\n    def test_from_mvl_models(self):\n        with ScratchDir("".""):\n            model = MEGNetModel.from_mvl_models(\'Eform_MP_2019\')\n            li2o = self.get_structure(""Li2O"")\n            self.assertAlmostEqual(float(model.predict_structure(li2o)),\n                                   -2.0152957439422607, places=4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/utils/__init__.py,0,b''
megnet/utils/data.py,0,"b'""""""\nData utitlities\n""""""\nfrom typing import Tuple\n\nimport numpy as np\nfrom pymatgen.optimization.neighbors import find_points_in_spheres\nfrom pymatgen import Structure, Molecule\n\nfrom megnet.utils.typing import StructureOrMolecule\nfrom megnet.config import DataType\n\n\ndef get_graphs_within_cutoff(structure: StructureOrMolecule,\n                             cutoff: float = 5.0,\n                             numerical_tol: float = 1e-8) \\\n        -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    """"""\n    Get graph representations from structure within cutoff\n    Args:\n        structure (pymatgen Structure or molecule)\n        cutoff (float): cutoff radius\n        numerical_tol (float): numerical tolerance\n\n    Returns:\n        center_indices, neighbor_indices, images, distances\n    """"""\n    if isinstance(structure, Structure):\n        lattice_matrix = np.ascontiguousarray(\n            np.array(structure.lattice.matrix), dtype=float)\n        pbc = np.array([1, 1, 1], dtype=int)\n    elif isinstance(structure, Molecule):\n        lattice_matrix = np.array(\n            [[1000.0, 0., 0.],\n             [0., 1000., 0.],\n             [0., 0., 1000.]], dtype=float)\n        pbc = np.array([0, 0, 0], dtype=int)\n    else:\n        raise ValueError(\'structure type not supported\')\n    r = float(cutoff)\n    cart_coords = np.ascontiguousarray(\n        np.array(structure.cart_coords), dtype=float)\n    center_indices, neighbor_indices, images, distances = \\\n        find_points_in_spheres(cart_coords, cart_coords, r=r, pbc=pbc,\n                               lattice=lattice_matrix, tol=numerical_tol)\n    center_indices = center_indices.astype(DataType.np_int)\n    neighbor_indices = neighbor_indices.astype(DataType.np_int)\n    images = images.astype(DataType.np_int)\n    distances = distances.astype(DataType.np_float)\n    exclude_self = (center_indices != neighbor_indices) | (distances > numerical_tol)\n    return center_indices[exclude_self], neighbor_indices[exclude_self], \\\n        images[exclude_self], distances[exclude_self]\n'"
megnet/utils/descriptor.py,0,"b'""""""\nThis module implements atom/bond/structure-wise descriptor calculated from\npretrained megnet model\n""""""\n\nimport os\nfrom typing import Union\n\nimport numpy as np\nfrom tensorflow.keras.models import Model\n\nfrom megnet.models import MEGNetModel, GraphModel\nfrom megnet.utils.typing import StructureOrMolecule\n\n\nDEFAULT_MODEL = os.path.join(\n    os.path.dirname(__file__),\n    \'../../mvl_models/mp-2019.4.1/formation_energy.hdf5\')\n\n\nclass MEGNetDescriptor:\n    """"""\n    MEGNet descriptors. This class takes a trained model and\n    then compute the intermediate outputs as structure features\n\n    Args:\n        model_name (str or MEGNetModel): trained model. If it is\n            str, then only models in mvl_models are used.\n        use_cache (bool): whether to use cache for structure\n            graph calculations\n    """"""\n    def __init__(self,\n                 model_name: Union[str, GraphModel,\n                                   MEGNetModel] = DEFAULT_MODEL,\n                 use_cache: bool = True):\n        if isinstance(model_name, str):\n            model = MEGNetModel.from_file(model_name)\n        elif isinstance(model_name, GraphModel):\n            model = model_name\n        else:\n            raise ValueError(\'model_name only support str \'\n                             \'or GraphModel object\')\n\n        layers = model.layers\n        important_prefix = [\'meg\', \'set\', \'concatenate\']\n\n        all_names = [i.name for i in layers\n                     if any([i.name.startswith(j) for j in important_prefix])]\n\n        if any([i.startswith(\'megnet\') for i in all_names]):\n            self.version = \'v2\'\n        else:\n            self.version = \'v1\'\n\n        valid_outputs = [i.output for i in layers\n                         if any([i.name.startswith(j) for j in important_prefix])]\n\n        outputs = []\n        valid_names = []\n        for i, j in zip(all_names, valid_outputs):\n            if isinstance(j, list):\n                for k, l in enumerate(j):\n                    valid_names.append(i + \'_%d\' % k)\n                    outputs.append(l)\n            else:\n                valid_names.append(i)\n                outputs.append(j)\n\n        full_model = Model(inputs=model.inputs, outputs=outputs)\n        model.model = full_model\n        self.model = model\n        self.valid_names = valid_names\n        self._cache = {}\n        self.use_cache = use_cache\n\n    def _predict_structure(self, structure: StructureOrMolecule) -> np.ndarray:\n        graph = self.model.graph_converter.convert(structure)\n        inp = self.model.graph_converter.graph_to_input(graph)\n        return self.model.predict(inp)\n\n    def _predict_feature(self, structure: StructureOrMolecule) -> np.ndarray:\n        if not self.use_cache:\n            return self._predict_structure(structure)\n\n        s = str(structure)\n        if s in self._cache:\n            return self._cache[s]\n        else:\n            result = self._predict_structure(structure)\n            self._cache[s] = result\n            return result\n\n    def _get_features(self,\n                      structure: StructureOrMolecule,\n                      prefix: str,\n                      level: int,\n                      index: int = None) -> np.ndarray:\n        name = prefix\n        if level is not None:\n            name = prefix + ""_%d"" % level\n        if index is not None:\n            name += \'_%d\' % index\n\n        if name not in self.valid_names:\n            raise ValueError(""%s not in original megnet model"" % name)\n        ind = self.valid_names.index(name)\n        out_all = self._predict_feature(structure)\n        return out_all[ind][0]\n\n    def _get_updated_prefix_level(self, prefix: str, level: int):\n        mapping = {\'meg_net_layer\': [""megnet"", level-1],\n                   ""set2_set"": [""set2set_atom"" if level == 1 else ""set2set_bond"", None],\n                   ""concatenate"": [""concatenate"", None]}\n        if self.version == ""v2"":\n            return mapping[prefix][0], mapping[prefix][1]\n        return prefix, level\n\n    def get_atom_features(self, structure: StructureOrMolecule,\n                          level: int = 3) -> np.ndarray:\n        """"""\n        Get megnet atom features from structure\n        Args:\n            structure: pymatgen structure or molecule\n            level: int, indicating the block number of megnet, starting\n                from 1\n\n        Returns:\n            nxm atomic feature matrix\n\n        """"""\n        prefix, level = self._get_updated_prefix_level(\'meg_net_layer\', level)\n        return self._get_features(structure, prefix=prefix,\n                                  level=level, index=0)\n\n    def get_bond_features(self, structure: StructureOrMolecule,\n                          level: int = 3) -> np.ndarray:\n        """"""\n        Get bond features at megnet block level\n        Args:\n            structure: pymatgen structure\n            level: int\n\n        Returns:\n            n_bond x m bond feature matrix\n\n        """"""\n        prefix, level = self._get_updated_prefix_level(\'meg_net_layer\', level)\n        return self._get_features(structure, prefix=prefix,\n                                  level=level, index=1)\n\n    def get_global_features(self, structure: StructureOrMolecule,\n                            level: int = 2) -> np.ndarray:\n        """"""\n        Get state features at megnet block level\n        Args:\n            structure: pymatgen structure or molecule\n            level: int\n\n        Returns:\n            1 x m_g global feature vector\n\n        """"""\n        prefix, level = self._get_updated_prefix_level(\'meg_net_layer\', level)\n        return self._get_features(structure,\n                                  prefix=prefix, level=level, index=2)\n\n    def get_set2set(self, structure: StructureOrMolecule,\n                    ftype: str = \'atom\') -> np.ndarray:\n        """"""\n        Get set2set output as features\n        Args:\n            structure (StructureOrMolecule): pymatgen structure\n                or molecule\n            ftype (str): atom or bond\n\n        Returns:\n            feature matrix, each row is a vector for an atom\n            or bond\n\n        """"""\n        mapping = {\'atom\': 1, \'bond\': 2}\n        prefix, level = self._get_updated_prefix_level(\'set2_set\', level=mapping[ftype])\n        return self._get_features(structure, prefix=prefix, level=level)\n\n    def get_structure_features(self, structure: StructureOrMolecule) -> np.ndarray:\n        """"""\n        Get structure level feature vector\n        Args:\n            structure (StructureOrMolecule): pymatgen structure\n                or molecule\n\n        Returns:\n            one feature vector for the structure\n\n        """"""\n        prefix, level = self._get_updated_prefix_level(\'concatenate\', level=1)\n        return self._get_features(structure, prefix=prefix, level=level)\n'"
megnet/utils/general.py,0,"b'""""""\nOperation utilities on lists and arrays\n""""""\nfrom collections import Iterable\nfrom typing import Union, List, Sequence, Optional\n\nimport numpy as np\n\n\ndef to_list(x: Union[Iterable, np.ndarray]) -> List:\n    """"""\n    If x is not a list, convert it to list\n    """"""\n    if isinstance(x, Iterable):\n        return list(x)\n    elif isinstance(x, np.ndarray):\n        return x.tolist()  # noqa\n    else:\n        return [x]\n\n\ndef expand_1st(x: np.ndarray) -> np.ndarray:\n    """"""\n    Adding an extra first dimension\n\n    Args:\n        x: (np.array)\n    Returns:\n         (np.array)\n    """"""\n    return np.expand_dims(x, axis=0)\n\n\ndef fast_label_binarize(value: List, labels: List) -> List[int]:\n    """"""Faster version of label binarize\n\n    `label_binarize` from scikit-learn is slow when run 1 label at a time.\n    `label_binarize` also is efficient for large numbers of classes, which is not\n    common in `megnet`\n\n    Args:\n        value: Value to encode\n        labels (list): Possible class values\n    Returns:\n        ([int]): List of integers\n    """"""\n\n    if len(labels) == 2:\n        return [int(value == labels[0])]\n    else:\n        output = [0] * len(labels)\n        if value in labels:\n            output[labels.index(value)] = 1\n        return output\n\n\ndef check_shape(array: Optional[np.ndarray], shape: Sequence) -> bool:\n    """"""\n    Check if array complies with shape. Shape is a sequence of\n    integer that may end with None. If None is at the end of shape,\n    then any shapes in array after that dimension will match with shape.\n\n    Example: array with shape [10, 20, 30, 40] matches with [10, 20, None], but\n        does not match with shape [10, 20, 30, 20]\n\n    Args:\n        array (np.ndarray or None): array to be checked\n        shape (Sequence): integer array shape, it may ends with None\n    Returns: bool\n    """"""\n    if array is None:\n        return True\n    if all([i is None for i in shape]):\n        return True\n\n    array_shape = array.shape\n    valid_dims = [i for i in shape if i is not None]\n    n_for_check = len(valid_dims)\n    return all([i == j for i, j in zip(array_shape[:n_for_check], valid_dims)])\n\n\ndef reshape(array: np.ndarray, shape: Sequence) -> np.ndarray:\n    """"""\n    Take an array and reshape it according to shape. Here shape may contain\n    None field at the end.\n\n    if array shape is [3, 4] and shape is [3, 4, None], then array is shaped\n    to [3, 4, 1]. If the two shapes do not match then report an error\n\n    Args:\n        array (np.ndarray): array to be reshaped\n        shape (Sequence): shape dimensions\n\n    Returns: np.ndarray, reshaped array\n    """"""\n    if not check_shape(array, shape):\n        raise ValueError(""array cannot be reshaped due to mismatch"")\n\n    if array.ndim >= len(shape):\n        return array\n    shape_r = [i if i is not None else 1 for i in shape]\n    missing_dim = range(len(array.shape), len(shape_r))\n    array_r = np.expand_dims(array, axis=list(missing_dim))\n    tiles = [i // j for i, j in zip(shape_r, array_r.shape)]\n    return np.tile(array_r, tiles)\n'"
megnet/utils/layer.py,13,"b'""""""\nTensorflow layer utilities\n""""""\nimport numpy as np  # noqa\nimport tensorflow as tf\n\n\ndef _repeat(x: tf.Tensor, n: tf.Tensor, axis: int = 1) -> tf.Tensor:\n    """"""\n    Given an tensor x (N*M*K), repeat the middle axis (axis=1)\n    according to repetition indicator n (M, )\n    for example, if M = 3, axis=1, and n = Tensor([3, 1, 2]),\n    and the final tensor would have the shape (N*6*3) with the\n    first one in M repeated 3 times,\n    second 1 time and third 2 times.\n\n     Args:\n        x: (3d Tensor) tensor to be augmented\n        n: (1d Tensor) number of repetition for each row\n        axis: (int) axis for repetition\n\n    Returns:\n        (3d Tensor) tensor after repetition\n    """"""\n    # get maximum repeat length in x\n    assert len(n.shape) == 1\n    maxlen = tf.reduce_max(input_tensor=n)\n    x_shape = tf.shape(input=x)\n    x_dim = len(x.shape)\n    # get the length of x\n    xlen = tf.shape(input=n)[0]\n    # create a range with the length of x\n    shape = [1] * (x_dim + 1)\n    shape[axis + 1] = maxlen\n    # tile it to the maximum repeat length, it should be of shape\n    # [xlen, maxlen] now\n    x_tiled = tf.tile(tf.expand_dims(x, axis + 1), tf.stack(shape))\n\n    new_shape = tf.unstack(x_shape)\n    new_shape[axis] = -1\n    new_shape[-1] = x.shape[-1]\n    x_tiled = tf.reshape(x_tiled, new_shape)\n    # create a sequence mask using x\n    # this will create a boolean matrix of shape [xlen, maxlen]\n    # where result[i,j] is true if j < x[i].\n    mask = tf.sequence_mask(n, maxlen)\n    mask = tf.reshape(mask, (-1,))\n    # mask the elements based on the sequence mask\n    return tf.boolean_mask(tensor=x_tiled, mask=mask, axis=axis)\n\n\ndef repeat_with_index(x: tf.Tensor, index: tf.Tensor, axis: int = 1):\n    """"""\n    Given an tensor x (N*M*K), repeat the middle axis (axis=1)\n    according to the index tensor index (G, )\n    for example, if axis=1 and n = Tensor([0, 0, 0, 1, 2, 2])\n    then M = 3 (3 unique values),\n    and the final tensor would have the shape (N*6*3) with the\n    first one in M repeated 3 times,\n    second 1 time and third 2 times.\n\n     Args:\n        x: (3d Tensor) tensor to be augmented\n        index: (1d Tensor) repetition tensor\n        axis: (int) axis for repetition\n    Returns:\n        (3d Tensor) tensor after repetition\n    """"""\n    index = tf.reshape(index, (-1,))\n    _, _, n = tf.unique_with_counts(index)\n    return _repeat(x, n, axis)\n'"
megnet/utils/metrics.py,0,"b'""""""metrics for evaluating datasets""""""\nimport numpy as np\n\n\ndef mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    """"""\n    Simple mean absolute error calculations\n\n    Args:\n        y_true: (numpy array) ground truth\n        y_pred: (numpy array) predicted values\n    Returns:\n         (float) mean absolute error\n    """"""\n    return np.mean(np.abs(y_true - y_pred)).item()\n\n\ndef accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    """"""\n    Simple accuracy calculation\n\n    Args:\n        y_true: numpy array of 0 and 1\'s\n        y_pred: numpy array of predict sigmoid\n    Returns:\n        (float) accuracy\n    """"""\n    y_pred = y_pred > 0.5\n    return np.sum(y_true == y_pred) / len(y_pred)\n'"
megnet/utils/models.py,0,"b'""""""\nModel utilities, mainly for model loading and download\n""""""\nimport os\nfrom glob import glob\nfrom zipfile import ZipFile\nimport logging\n\nfrom megnet.models import MEGNetModel, GraphModel\n\nlogging.basicConfig()\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n\nCWD = os.path.dirname(os.path.abspath(__file__))\nTEMP_PATH = os.path.join(CWD, ""./mvl_models.zip"")\nLOCAL_MODEL_PATH = os.path.join(CWD, ""./mvl_models"")\n\nMODEL_PATH = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n                          \'../../mvl_models\')\n\nMODEL_MAPPING = {\'Eform_MP_2019\': \'mp-2019.4.1/formation_energy.hdf5\',\n                 \'Eform_MP_2018\': \'mp-2018.6.1/formation_energy.hdf5\',\n                 \'Efermi_MP_2019\': \'mp-2019.4.1/efermi.hdf5\',\n                 \'Bandgap_classifier_MP_2018\':\n                     \'mp-2018.6.1/band_classification.hdf5\',\n                 \'Bandgap_MP_2018\':\n                     \'mp-2018.6.1/band_gap_regression.hdf5\',\n                 \'logK_MP_2018\': \'mp-2018.6.1/log10K.hdf5\',\n                 \'logG_MP_2018\': \'mp-2018.6.1/log10G.hdf5\',\n                 \'logK_MP_2019\': \'mp-2019.4.1/log10K.hdf5\',\n                 \'logG_MP_2019\': \'mp-2019.4.1/log10G.hdf5\'}\n\nqm9_models = glob(os.path.join(MODEL_PATH, \'qm9-2018.6.1/*.hdf5\'))\n\nMODEL_MAPPING.update({\'QM9_%s_2018\' % i: \'qm9-2018.6.1/%s.hdf5\' % i for i in\n                      [j.split(\'/\')[-1].split(\'.\')[0] for j in qm9_models]})\n\n\nAVAILABLE_MODELS = list(MODEL_MAPPING.keys())\n\n\ndef load_model(model_name: str) -> GraphModel:\n    """"""\n    load the model by user friendly name as in megnet.utils.models.AVAILABEL_MODELS\n\n    Args:\n        model_name: str model name string\n\n    Returns: GraphModel\n\n    """"""\n\n    if model_name in AVAILABLE_MODELS:\n        mvl_path = os.path.join(MODEL_PATH, MODEL_MAPPING[model_name])\n        if os.path.isfile(mvl_path):\n            return MEGNetModel.from_file(mvl_path)\n\n        logger.info(""Package-level mvl_models not included, trying ""\n                    ""temperary mvl_models downloads.."")\n        local_mvl_path = os.path.join(LOCAL_MODEL_PATH, MODEL_MAPPING[model_name])\n        if os.path.isfile(local_mvl_path):\n            logger.info(""Model found in local mvl_models path"")\n            return MEGNetModel.from_file(local_mvl_path)\n        _download_models()\n        return load_model(model_name)\n    else:\n        raise ValueError(\'model name %s not in available model list %s\' %\n                         (model_name, AVAILABLE_MODELS))\n\n\ndef _download_models(url: str = ""https://ndownloader.figshare.com/files/22291785"",\n                     file_path: str = TEMP_PATH):\n    """"""\n    Download machine learning model files\n\n    Args:\n        url: (str) url link for the models\n    """"""\n\n    logger.info(""Fetching {} from {} to {}"".format(\n        os.path.basename(file_path), url, file_path))\n\n    import urllib.request\n\n    urllib.request.urlretrieve(url, file_path)\n\n    logger.info(""Start extracting models..."")\n    with ZipFile(file_path, \'r\') as zip_obj:\n        zip_obj.extractall(os.path.dirname(file_path))\n'"
megnet/utils/molecule.py,0,"b'""""""\nMolecule utility, mainly using openbabel\n""""""\nimport logging\n\nfrom pymatgen import Molecule\nfrom monty.dev import requires\ntry:\n    import pybel as pb\n    import openbabel as ob\nexcept ImportError:\n    logging.warning(""Openbabel is needed for molecule models, ""\n                    ""try \'conda install -c openbabel openbabel\' ""\n                    ""to install it"")\n    pb = None\n    ob = None\n\n\n@requires(pb is not None, ""openbabel is needed to run convert smiles"")\ndef get_pmg_mol_from_smiles(smiles: str) -> Molecule:\n    """"""\n    Get a pymatgen molecule from smiles representation\n    Args:\n        smiles: (str) smiles representation of molecule\n\n    Returns:\n        pymatgen Molecule\n    """"""\n    b_mol = pb.readstring(\'smi\', smiles)  # noqa\n    b_mol.make3D()\n    b_mol = b_mol.OBMol\n    sp = []\n    coords = []\n    for atom in ob.OBMolAtomIter(b_mol):\n        sp.append(atom.GetAtomicNum())\n        coords.append([atom.GetX(), atom.GetY(), atom.GetZ()])\n    return Molecule(sp, coords)\n'"
megnet/utils/preprocessing.py,0,"b'""""""\nPreprocessing codes\n""""""\n\nfrom typing import List\n\nimport numpy as np\nfrom monty.json import MSONable\n\nfrom .typing import StructureOrMolecule, VectorLike\n\n\nclass Scaler(MSONable):\n    """"""\n    Base Scaler class. It implements transform and\n    inverse_transform. Both methods will take number\n    of atom as the second parameter in addition to\n    the target property\n    """"""\n    def transform(self, target: float, n: int = 1) -> float:\n        """"""\n        Transform the target values into new target values\n        Args:\n            target (float): target numerical value\n            n (int): number of atoms\n        Returns:\n            scaled target\n\n        """"""\n        raise NotImplementedError\n\n    def inverse_transform(self, transformed_target: float,\n                          n: int = 1) -> float:\n        """"""\n        Inverse transform of the target\n\n        Args:\n            transformed_target (float): transformed target\n            n (int): number of atoms\n\n        Returns:\n            target\n        """"""\n        raise NotImplementedError\n\n\nclass StandardScaler(Scaler):\n    """"""\n    Standard scaler with consideration of extensive/intensive quantity\n    For intensive quantity, the mean is just the mean of training data,\n    and std is the std of training data\n    For extensive quantity, the mean is the mean of target/atom, and\n    std is the std for target/atom\n\n    Args:\n        mean (float): mean value of target\n        std (float): standard deviation of target\n        is_intensive (bool): whether the target is already an intensive\n            property\n\n    Methods:\n        transform(self, target, n=1): standard scaling the target and\n    """"""\n\n    def __init__(self, mean: float = 0.0, std: float = 1.0,\n                 is_intensive: bool = True):\n        self.mean = mean\n        if np.abs(std) < np.finfo(float).eps:\n            std = 1.0\n        self.std = std\n        self.is_intensive = is_intensive\n\n    def transform(self, target: float, n: int = 1) -> float:\n        """"""\n        Transform numeric values according the mean and std, plus a factor n\n\n        Args:\n            target (float): target numerical value\n            n (int): number of atoms\n        Returns:\n            scaled target\n        """"""\n        if self.is_intensive:\n            n = 1\n        return (target / n - self.mean) / self.std\n\n    def inverse_transform(self, transformed_target: float,\n                          n: int = 1) -> float:\n        """"""\n        Inverse transform of the target\n\n        Args:\n            transformed_target (float): transformed target\n            n (int): number of atoms\n\n        Returns:\n            original target\n        """"""\n        if self.is_intensive:\n            n = 1\n        return n * (transformed_target * self.std + self.mean)\n\n    @classmethod\n    def from_training_data(cls, structures: List[StructureOrMolecule],\n                           targets: VectorLike, is_intensive: bool = True) \\\n            -> \'StandardScaler\':\n        """"""\n        Generate a target scaler from a list of input structures/molecules,\n        a target value vector and an indicator for intensiveness of the\n        property\n\n        Args:\n            structures (list): list of structures/molecules\n            targets (list): vector of target properties\n            is_intensive (bool): whether the target is intensive\n\n        Returns: new instance\n\n        """"""\n        if is_intensive:\n            new_targets = targets\n        else:\n            new_targets = [i / len(j) for i, j in zip(targets, structures)]\n        mean = np.mean(new_targets).item()\n        std = np.std(new_targets).item()\n        return cls(mean, std, is_intensive)\n\n    def __str__(self):\n        return ""StandardScaler(mean=%.3f, std=%.3f, is_intensive=%d)"" % \\\n               (self.mean, self.std, self.is_intensive)\n\n    def __repr__(self):\n        return str(self)\n\n\nclass DummyScaler(MSONable):\n    """"""\n    Dummy scaler does nothing\n    """"""\n    def transform(self, target: float, n: int = 1) -> float:\n        """"""\n        Args:\n            target (float): target numerical value\n            n (int): number of atoms\n        Returns:\n            target\n        """"""\n        return target\n\n    def inverse_transform(self, transformed_target: float,\n                          n: int = 1) -> float:\n        """"""\n        return as it is\n\n        Args:\n            transformed_target (float): transformed target\n            n (int): number of atoms\n\n        Returns:\n            transformed_target\n        """"""\n        return transformed_target\n\n    @classmethod\n    def from_training_data(cls, structures: List[StructureOrMolecule],\n                           targets: VectorLike,\n                           is_intensive: bool = True):\n        """"""\n        Args:\n            structures (list): list of structures/molecules\n            targets (list): vector of target properties\n            is_intensive (bool): whether the target is intensive\n\n        Returns: DummyScaler\n\n        """"""\n        return cls()\n'"
megnet/utils/typing.py,0,"b'""""""Define several typing for convenient use""""""\n\nfrom typing import Union, Callable, Optional, Any, List\n\nimport numpy as np\nfrom pymatgen.core import Structure, Molecule\n\nOptStrOrCallable = Optional[Union[str, Callable[..., Any]]]\nStructureOrMolecule = Union[Structure, Molecule]\nVectorLike = Union[List[float], np.ndarray]\n'"
megnet/data/tests/test_crystal.py,0,"b'import unittest\nfrom megnet.data.graph import GaussianDistance\nfrom megnet.utils.general import to_list\nfrom megnet.data.crystal import CrystalGraph, get_elemental_embeddings, CrystalGraphWithBondTypes\nfrom pymatgen import Structure\nimport os\nimport numpy as np\n\nmodule_dir = os.path.dirname(os.path.abspath(__file__))\n\n\nclass TestGraph(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.structures = [\n            Structure.from_file(os.path.join(module_dir, ""cifs"", ""LiFePO4_mp-19017_computed.cif"")),\n            Structure.from_file(os.path.join(module_dir, ""cifs"", ""BaTiO3_mp-2998_computed.cif""))]\n\n    def test_crystalgraph(self):\n        cg = CrystalGraph(cutoff=4)\n        graph = cg.convert(self.structures[0])\n        self.assertEqual(cg.cutoff, 4)\n        keys = set(graph.keys())\n        self.assertSetEqual({""bond"", ""atom"", ""index1"", ""index2"", ""state""}, keys)\n        cg2 = CrystalGraph(cutoff=6)\n        self.assertEqual(cg2.cutoff, 6)\n        graph2 = cg2.convert(self.structures[0])\n        self.assertListEqual(to_list(graph2[\'state\'][0]), [0, 0])\n        graph3 = cg(self.structures[0])\n        np.testing.assert_almost_equal(graph[\'atom\'], graph3[\'atom\'])\n\n    def test_crystal_graph_with_bond_types(self):\n        graph = {\'atom\': [11, 8, 8],\n                 \'index1\': [0, 0, 1, 1, 2, 2],\n                 \'index2\': [0, 1, 2, 2, 1, 1],\n                 \'bond\': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n                 \'state\': [[0, 0]]}\n        cgbt = CrystalGraphWithBondTypes(nn_strategy=\'VoronoiNN\')\n        new_graph = cgbt._get_bond_type(graph)\n        self.assertListEqual(to_list(new_graph[\'bond\']), [2, 1, 0, 0, 0, 0])\n\n\n    def test_convert(self):\n        cg = CrystalGraph(cutoff=4)\n        graph = cg.convert(self.structures[0])\n        self.assertListEqual(to_list(graph[\'atom\']), [i.specie.Z for i in self.structures[0]])\n\n    def test_get_input(self):\n        cg = CrystalGraph(cutoff=4, bond_converter=GaussianDistance(np.linspace(0, 5, 100), 0.5))\n        inp = cg.get_input(self.structures[0])\n        self.assertEqual(len(inp), 7)\n        shapes = [i.shape for i in inp]\n        true_shapes = [(1, 28), (1, 704, 100), (1, 1, 2), (1, 704), (1, 704), (1, 28), (1, 704)]\n        for i, j in zip(shapes, true_shapes):\n            self.assertListEqual(list(i), list(j))\n\n    def test_get_flat_data(self):\n        cg = CrystalGraph(cutoff=4)\n        graphs = [cg.convert(i) for i in self.structures]\n        targets = [0.1, 0.2]\n        inp = cg.get_flat_data(graphs, targets)\n        self.assertListEqual([len(i) for i in inp], [2] * 6)\n\n    def test_get_elemental_embeddings(self):\n        data = get_elemental_embeddings()\n        for k, v in data.items():\n            self.assertTrue(len(v) == 16)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/data/tests/test_graph.py,0,"b'import unittest\nfrom megnet.data.graph import GaussianDistance, GraphBatchGenerator, GraphBatchDistanceConvert,\\\n    EmbeddingMap\nimport numpy as np\n\n\nclass TestGraph(unittest.TestCase):\n    def test_gaussian_expansion(self):\n        x = np.random.normal(size=(10, ))\n        centers = np.linspace(0, 5, 20)\n        width = 0.5\n        gd = GaussianDistance(centers, width)\n        out = gd.convert(x)\n        self.assertListEqual(list(out.shape), [10, 20])\n\n    def test_graph_generator(self):\n        feature = [np.random.normal(size=(3, 4)), np.random.normal(size=(2, 4))]\n        bond = [np.random.normal(size=(2, 5)), np.random.normal(size=(1, 5))]\n        glob_features = [np.random.normal(size=(1, 2)), np.random.normal(size=(1, 2))]\n        index1 = [np.array([0, 1]), np.array([0])]\n        index2 = [np.array([1, 2]), np.array([1])]\n        targets = np.random.normal(size=(2, 1))\n        gen = GraphBatchGenerator(feature, bond, glob_features, index1, index2, targets,\n                                  batch_size=2)\n        data = gen[0]\n        self.assertListEqual(list(data[0][0].shape), [1, 5, 4])\n        self.assertListEqual(list(data[0][1].shape), [1, 3, 5])\n        self.assertListEqual(list(data[0][2].shape), [1, 2, 2])\n        self.assertListEqual(list(data[0][3].shape), [1, 3])\n        self.assertListEqual(list(data[0][4].shape), [1, 3])\n        self.assertListEqual(list(data[1].shape), [1, 2, 1])\n\n        # Make sure it still functions if a target is not provided\n        gen = GraphBatchGenerator(feature, bond, glob_features, index1, index2, batch_size=2)\n\n        data = gen[0]\n        self.assertEqual(7, len(data))  # Should only be the inputs\n        self.assertListEqual(list(data[0].shape), [1, 5, 4])\n\n        # when bonds are one dimension arrays\n\n        bond = [np.random.normal(size=(2,)), np.random.normal(size=(1,))]\n        gen = GraphBatchGenerator(feature, bond, glob_features, index1, index2, targets,\n                                  batch_size=2)\n        data = gen[0]\n        self.assertListEqual(list(data[0][1].shape), [1, 3])\n\n    def test_graph_batch_distance_converter(self):\n        feature = [np.random.normal(size=(3, 4)), np.random.normal(size=(2, 4))]\n        bond = [np.random.normal(size=(2, )), np.random.normal(size=(1, ))]\n        glob_features = [np.random.normal(size=(1, 2)),\n                         np.random.normal(size=(1, 2))]\n        index1 = [np.array([0, 1]), np.array([0])]\n        index2 = [np.array([1, 2]), np.array([1])]\n        targets = np.random.normal(size=(2, 1))\n        centers = np.linspace(0, 5, 20)\n        width = 0.5\n        gen = GraphBatchDistanceConvert(feature, bond, glob_features, index1, index2, targets, batch_size=2,\n                                        distance_converter=GaussianDistance(centers, width))\n        data = gen[0]\n        self.assertListEqual(list(data[0][0].shape), [1, 5, 4])\n        self.assertListEqual(list(data[0][1].shape), [1, 3, 20])\n        self.assertListEqual(list(data[0][2].shape), [1, 2, 2])\n        self.assertListEqual(list(data[0][3].shape), [1, 3])\n        self.assertListEqual(list(data[0][4].shape), [1, 3])\n        self.assertListEqual(list(data[1].shape), [1, 2, 1])\n\n    def test_embedding_map(self):\n        m = EmbeddingMap(np.array([[1, 2], [3, 4]]))\n        res = m.convert(np.array([0, 0, 1, 1]))\n        self.assertListEqual(res[0].tolist(), [1, 2])\n        self.assertListEqual(res[2].tolist(), [3, 4])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/data/tests/test_graph_types.py,0,b''
megnet/data/tests/test_local_env.py,0,"b'import unittest\nfrom megnet.data.local_env import MinimumDistanceNNAll, AllAtomPairs, serialize, deserialize, get\nfrom pymatgen import Structure, Molecule\nimport os\n\nMODULE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\ndef _equal(x, y):\n    if isinstance(x, list):\n        return all([_equal(i, j) for i, j in zip(x, y)])\n    elif isinstance(x, dict):\n        return all(_equal(x[i], y[i]) for i in x.keys())\n    else:\n        if x == y:\n            return True\n        else:\n            print(x, y)\n            return False\n\n\ndef _sort_neighbors(neighbors):\n    out = []\n    for n in neighbors:\n        out.append([sorted(n, key=lambda x: (x[\'weight\'], x[\'site_index\']))])\n    return out\n\n\nclass TestLocalEnv(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.structure = Structure.from_file(os.path.join(MODULE_DIR, \'cifs\', \'BaTiO3_mp-2998_computed.cif\'))\n        cls.molecule = Molecule([\'C\', \'O\', \'O\'], [[0, 0, 0], [-1, 0, 0], [1, 0, 0]])\n        cls.mall = MinimumDistanceNNAll(4)\n        cls.aapair = AllAtomPairs()\n\n    def test_minimal_distance(self):\n        neighbors1 = self.mall.get_all_nn_info(self.structure)\n        neighbors2 = [self.mall.get_nn_info(self.structure, i) for i in range(len(self.structure))]\n        self.assertTrue(_equal(_sort_neighbors(neighbors1), _sort_neighbors(neighbors2)))\n\n    def test_all_atom_pairs(self):\n        mol_pairs = self.aapair.get_all_nn_info(self.molecule)\n        self.assertEqual(len(mol_pairs[0]), 2)\n\n    def test_serialization(self):\n        mall = MinimumDistanceNNAll(4)\n        config = serialize(mall)\n        self.assertDictEqual(config, {\'@module\': \'megnet.data.local_env\',\n                                      \'@class\': \'MinimumDistanceNNAll\',\n                                      \'cutoff\': 4})\n        self.assertTrue(serialize(None) is None)\n\n        mall2 = deserialize(config)\n        self.assertTrue(isinstance(mall2, MinimumDistanceNNAll))\n        self.assertTrue(mall2.cutoff == 4)\n\n    def test_get(self):\n        voronoi = get(\'VoronoiNN\')\n        self.assertTrue(voronoi.__name__ == \'VoronoiNN\')\n\n\nif __name__ == ""__main__"":\n    unittest.main()'"
megnet/data/tests/test_molecule.py,0,"b'import unittest\nimport os\nimport json\n\nfrom megnet.data.molecule import SimpleMolGraph\nfrom megnet.data.graph import DummyConverter\nfrom megnet.utils.general import to_list\nfrom pymatgen import Molecule\nimport numpy as np\n\nfrom megnet.data.molecule import MolecularGraph, MolecularGraphBatchGenerator,\\\n    pybel, mol_from_smiles, ring_to_vector\n\nif pybel is None:\n    import_failed = True\nelse:\n    import_failed = False\n\nmodule_dir = os.path.dirname(os.path.abspath(__file__))\n\n\ndef equal(x, y):\n    if isinstance(x, list):\n        return all([i == j for i, j in zip(x, y)])\n    if isinstance(x, float):\n        return abs(x-y) < 0.01\n    else:\n        return x == y\n\n\nclass QM9Test(unittest.TestCase):\n    def test_simple_molecule_graph(self):\n        mol = Molecule([\'C\', \'H\', \'O\'], [[0, 0, 0], [1, 0, 0], [2, 0, 0]])\n        graph = SimpleMolGraph().convert(mol)\n        self.assertListEqual(to_list(graph[\'atom\']), [6, 1, 8])\n        self.assertTrue(np.allclose(graph[\'bond\'], [1, 2, 1, 1, 2, 1]))\n        self.assertListEqual(to_list(graph[\'index1\']), [0, 0, 1, 1, 2, 2])\n        self.assertListEqual(to_list(graph[\'index2\']), [1, 2, 0, 2, 0, 1])\n\n    def test_ring_to_vector(self):\n        x = [2, 2, 3]\n        expected = [0, 2, 1, 0, 0, 0, 0, 0, 0]\n        self.assertListEqual(expected, ring_to_vector(x))\n\n\nclass MolecularGraphTest(unittest.TestCase):\n\n    @classmethod\n    @unittest.skipIf(import_failed, ""molecule package relies on openbabel"")\n    def setUpClass(cls):\n        with open(os.path.join(module_dir, \'qm9\', \'000001.json\'), \'r\') as f:\n            cls.qm9_000001 = json.load(f)\n        cls.mol = mol_from_smiles(cls.qm9_000001[\'smiles\'])\n\n    @unittest.skipIf(import_failed, ""molecule package relies on openbabel"")\n    def setUp(self) -> None:\n        self.mg = MolecularGraph()\n\n    @unittest.skipIf(import_failed, ""molecule package relies on openbabel"")\n    def test_featurizer(self):\n        mg = MolecularGraph()\n        mol_graph = mg.convert(self.mol)\n        self.assertEqual(len(mol_graph[\'index1\']), 20)  # 20 bonds, including double counting\n        self.assertEqual(len(mol_graph[\'atom\']), 5)  # 5 atoms\n        self.assertAlmostEqual(mol_graph[\'state\'][0][0], 3.2, places=1)\n        self.assertAlmostEqual(mol_graph[\'state\'][0][1], 0.8, places=1)\n        mol_graph = mg.convert(self.mol, state_attributes=[[1, 2]])\n        self.assertListEqual(mol_graph[\'state\'][0], [1, 2])\n\n    @unittest.skipIf(import_failed, ""molecule package relies on openbabel"")\n    def test_atom_features(self):\n        """"""Detailed test of get_atom_feature""""""\n\n        # Test on Methane (atom 0 is an H)\n        feat = self.mg.get_atom_feature(self.mol, self.mol.atoms[0])\n        self.assertEqual(feat[\'element\'], \'H\')\n        self.assertEqual(feat[\'atomic_num\'], 1)\n        self.assertEqual(feat[\'chirality\'], 0)\n        self.assertEqual(feat[\'formal_charge\'], 0)\n        self.assertEqual(feat[\'ring_sizes\'], [])\n        self.assertEqual(feat[\'hybridization\'], 6)\n        self.assertEqual(feat[\'acceptor\'], False)\n        self.assertEqual(feat[\'donor\'], False)\n        self.assertEqual(feat[\'aromatic\'], False)\n\n        # Make sure it gets the hybridization of the C correctly\n        feat = self.mg.get_atom_feature(self.mol, self.mol.atoms[1])\n        self.assertEqual(feat[\'element\'], \'C\')\n        self.assertEqual(feat[\'atomic_num\'], 6)\n        self.assertEqual(feat[\'chirality\'], 0)\n        self.assertEqual(feat[\'formal_charge\'], 0)\n        self.assertEqual(feat[\'ring_sizes\'], [])\n        self.assertEqual(feat[\'hybridization\'], 3)\n\n        # Test chirality using L/D-alanine\n        la = pybel.readstring(\'smiles\', \'N[C@@H](C)C(=O)O\')\n        feat = self.mg.get_atom_feature(la, la.atoms[1])\n        self.assertEqual(feat[\'element\'], \'C\')\n        self.assertEqual(feat[\'chirality\'], 2)\n\n        da = pybel.readstring(\'smiles\', \'N[C@H](C)C(=O)O\')\n        feat = self.mg.get_atom_feature(da, da.atoms[1])\n        self.assertEqual(feat[\'element\'], \'C\')\n        self.assertEqual(feat[\'chirality\'], 1)\n\n        # Test formal charge\n        proton = pybel.readstring(\'smiles\', \'[H+]\')\n        feat = self.mg.get_atom_feature(proton, proton.atoms[0])\n        self.assertEqual(feat[\'element\'], \'H\')\n        self.assertEqual(feat[\'formal_charge\'], 1)\n\n        # Test ring sizes\n        naph = pybel.readstring(\'smiles\', \'C1=CC=C2C=CC=CC2=C1\')\n        ring_sizes = [self.mg.get_atom_feature(naph, a)[\'ring_sizes\'] for a in naph.atoms]\n        self.assertEqual(ring_sizes.count([6]), 8)\n        self.assertEqual(ring_sizes.count([6, 6]), 2)\n\n        # Test aromicity\n        aromicity = [self.mg.get_atom_feature(naph, a)[\'aromatic\'] for a in naph.atoms]\n        self.assertTrue(all(aromicity))\n\n        # Test hydrogen bond acceptor\n        ammonia = pybel.readstring(\'smiles\', \'N\')\n        ammonia.addh()\n        feat = self.mg.get_atom_feature(ammonia, ammonia.atoms[1])\n        self.assertEqual(feat[\'element\'], \'H\')\n        self.assertTrue(feat[\'donor\'])\n        self.assertFalse(feat[\'acceptor\'])\n\n        # Test hydrogen bond donor\n        water = pybel.readstring(\'smiles\', \'O\')\n        feat = self.mg.get_atom_feature(water, water.atoms[0])\n        self.assertTrue(feat[\'acceptor\'])\n\n    @unittest.skipIf(import_failed, ""molecule package relies on openbabel"")\n    def test_atom_feature_vector(self):\n        """"""Test the code that transforms feature dict to a list""""""\n\n        # Make feature dictionary with complicated molecule\n        naph = pybel.readstring(\'smiles\', \'C1=CC=C2C=CC=CC2=C1\')\n        feat = self.mg.get_atom_feature(naph, naph.atoms[3])\n\n        # Run with the default features\n        vec = self.mg._create_atom_feature_vector(feat)\n        self.assertEqual(27, len(vec))\n\n        # Check the on-hot-encoding for elements\n        self.mg.atom_features = [\'element\']\n        vec = self.mg._create_atom_feature_vector(feat)\n        self.assertEqual([0, 1, 0, 0, 0], vec)\n\n        # Check with only atomic number and formal charge\n        self.mg.atom_features = [\'atomic_num\', \'formal_charge\']\n        vec = self.mg._create_atom_feature_vector(feat)\n        self.assertEqual([6, 0], vec)\n\n        # Make sure it obeys user-defined order\n        self.mg.atom_features = [\'formal_charge\', \'atomic_num\']\n        vec = self.mg._create_atom_feature_vector(feat)\n        self.assertEqual([0, 6], vec)\n\n        # Run the chirality binarization\n        self.mg.atom_features = [\'chirality\']\n        vec = self.mg._create_atom_feature_vector(feat)\n        self.assertEqual([1, 0, 0], vec)\n\n        # Run the ring size calculation (it is in 2 6-member rings)\n        self.mg.atom_features = [\'ring_sizes\']\n        vec = self.mg._create_atom_feature_vector(feat)\n        self.assertEqual([0, 0, 0, 0, 0, 2, 0, 0, 0], vec)\n\n        # Run the hybridization test\n        self.mg.atom_features = [\'hybridization\']\n        vec = self.mg._create_atom_feature_vector(feat)\n        self.assertEqual([0, 1, 0, 0, 0, 0], vec)\n\n        # Test donor, acceptor, aromatic\n        self.mg.atom_features = [\'donor\', \'acceptor\', \'aromatic\']\n        vec = self.mg._create_atom_feature_vector(feat)\n        self.assertEqual([0, 0, 1], vec)\n\n    @unittest.skipIf(import_failed, ""molecule package relies on openbabel"")\n    def test_bond_features(self):\n        """"""Detailed tests for bond features""""""\n\n        # Test C-H bonds on the methane molecule\n        feat = self.mg.get_pair_feature(self.mol, 0, 1, True)\n        self.assertEqual(0, feat[\'a_idx\'])\n        self.assertEqual(1, feat[\'b_idx\'])\n        self.assertEqual(1, feat[\'bond_type\'])\n        self.assertEqual(False, feat[\'same_ring\'])\n        self.assertAlmostEqual(1.0921, feat[\'spatial_distance\'], places=3)\n\n        feat = self.mg.get_pair_feature(self.mol, 1, 0, True)\n        self.assertEqual(1, feat[\'a_idx\'])\n        self.assertEqual(0, feat[\'b_idx\'])\n        self.assertEqual(1, feat[\'bond_type\'])\n        self.assertEqual(False, feat[\'same_ring\'])\n        self.assertAlmostEqual(1.0921, feat[\'spatial_distance\'], places=3)\n\n        # Test atoms that are not bonded\n        feat = self.mg.get_pair_feature(self.mol, 0, 2, True)\n        self.assertEqual(0, feat[\'a_idx\'])\n        self.assertEqual(2, feat[\'b_idx\'])\n        self.assertEqual(0, feat[\'bond_type\'])\n        self.assertEqual(False, feat[\'same_ring\'])\n        self.assertAlmostEqual(1.7835, feat[\'spatial_distance\'], places=3)\n\n        feat = self.mg.get_pair_feature(self.mol, 0, 2, False)\n        self.assertIsNone(feat)\n\n        # Test an aromatic bond\n        benzene = pybel.readstring(\'smiles\', \'C1=CC=CC=C1\')\n        feat = self.mg.get_pair_feature(benzene, 0, 1, True)\n        self.assertEqual(4, feat[\'bond_type\'])\n        self.assertEqual(True, feat[\'same_ring\'])\n\n    @unittest.skipIf(import_failed, ""molecule package relies on openbabel"")\n    def test_bond_feature_vec(self):\n        # Test the full list\n        feat = self.mg.get_pair_feature(self.mol, 0, 1, True)\n        self.assertEqual(26, len(self.mg._create_pair_feature_vector(feat)))\n\n        # Test the bond type\n        self.mg.bond_features = [\'bond_type\']\n        self.assertEqual([0, 1, 0, 0, 0], self.mg._create_pair_feature_vector(feat))\n\n        # Test the ring encoding\n        self.mg.bond_features = [\'same_ring\']\n        self.assertEqual([0], self.mg._create_pair_feature_vector(feat))\n\n        # Test the spatial distance\n        self.mg.bond_features = [\'spatial_distance\']\n        self.assertEqual(20, len(self.mg._create_pair_feature_vector(feat)))\n\n        # Test the spatial distance without the expansion\n        self.mg.distance_converter = DummyConverter()\n        self.assertAlmostEqual(1.0921, self.mg._create_pair_feature_vector(feat)[0], places=3)\n\n    @unittest.skipIf(import_failed, ""molecule package relies on openbabel"")\n    def test_mol_generator(self):\n        mols = [\'c\', \'C\', \'cc\', \'ccn\']\n        gen = MolecularGraphBatchGenerator(mols, range(4), batch_size=2, molecule_format=\'smiles\')\n\n        # Make a batch, check it has the correct sizes\n        batch = gen[0]\n        self.assertEqual(2, len(batch))\n        self.assertEqual((1, 1, 2), np.shape(batch[1]))  # Should be 2 targets\n        self.assertEqual(7, len(batch[0]))  # Should have 7 different arrays for inputs\n\n        # Test the generator with 2 threads\n        gen = MolecularGraphBatchGenerator(mols, range(4), batch_size=2,\n                                           molecule_format=\'smiles\', n_jobs=2)\n        batch = gen[0]\n        self.assertEqual(2, len(batch))\n\n        # Create the cached generator, amke sure it creates properly-sized inputs\n        cached = gen.create_cached_generator()\n\n        batch = cached[0]\n        self.assertEqual(2, len(batch))\n        self.assertEqual(2, np.size(batch[1]))  # Should be 2 targets\n        self.assertEqual(7, len(batch[0]))  # Should have 7 different arrays for inputs\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/layers/featurizer/__init__.py,0,"b'""""""\nFeaturizers used after the graph generation and before the graph convolution\n""""""\nfrom ._gaussian_expansion import GaussianExpansion\n'"
megnet/layers/featurizer/_gaussian_expansion.py,3,"b'""""""\nGaussian expansion of distances\n""""""\nimport numpy as np\n\nfrom tensorflow.keras.layers import Layer\nimport tensorflow as tf\n\n\nclass GaussianExpansion(Layer):\n    """"""\n    Simple Gaussian expansion.\n    A vector of distance [d1, d2, d3, ..., dn] is expanded to a\n    matrix of shape [n, m], where m is the number of Gaussian basis centers\n\n    Args:\n        centers (np.ndarray): Gaussian basis centers\n        width (float): width of the Gaussian basis\n    """"""\n    def __init__(self, centers, width, **kwargs):\n        self.centers = np.array(centers).ravel()\n        self.width = width\n        super().__init__(**kwargs)\n\n    def build(self, input_shape):\n        """"""\n        build the layer\n        Args:\n            input_shape (tuple): tuple of int for the input shape\n        """"""\n        self.built = True\n\n    def call(self, inputs, masks=None):\n        """"""\n        The core logic function\n\n        Args:\n            inputs (tf.Tensor): input distance tensor, with shape [None, n]\n            masks (tf.Tensor): bool tensor, not used here\n        """"""\n        return tf.math.exp(-(inputs[:, :, None] - self.centers[None, None, :])**2 / self.width**2)\n\n    def compute_output_shape(self, input_shape):\n        """"""\n        Compute the output shape, used in older keras API\n        """"""\n        return input_shape[0], input_shape[1], len(self.centers)\n\n    def get_config(self):\n        """"""\n        Get layer configurations\n        """"""\n        base_config = super().get_config()\n        config = {\'centers\': self.centers.tolist(), \'width\': self.width}\n        return dict(list(base_config.items()) + list(config.items()))\n'"
megnet/layers/graph/__init__.py,0,b'from .base import GraphNetworkLayer\nfrom .megnet import MEGNetLayer\nfrom .cgcnn import CrystalGraphLayer\nfrom .schnet import InteractionLayer\n'
megnet/layers/graph/base.py,12,"b'""""""\nA full GN block has the following computation steps\n 1. Compute updated edge attributes\n 2. Aggregate edge attributes per node\n 3. Compute updated node attributes\n 4. Aggregate edge attributes globally\n 5. Aggregate node attributes globally\n 6. Compute updated global attribute\n\n[1] https://arxiv.org/pdf/1806.01261.pdf\n""""""\nfrom typing import Dict, Sequence\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import regularizers, constraints, initializers\n\nfrom megnet import activations\nfrom megnet.utils.typing import OptStrOrCallable\n\n\nclass GraphNetworkLayer(Layer):\n    """"""\n    Implementation of a graph network layer. Current implementation is based on\n    neural networks for each update function, and sum or mean for each\n    aggregation function\n\n    Args:\n        activation (str): Default: None. The activation function used for each\n            sub-neural network. Examples include \'relu\', \'softmax\', \'tanh\',\n            \'sigmoid\' and etc.\n        use_bias (bool): Default: True. Whether to use the bias term in the\n            neural network.\n        kernel_initializer (str): Default: \'glorot_uniform\'. Initialization\n            function for the layer kernel weights,\n        bias_initializer (str): Default: \'zeros\'\n        activity_regularizer (str): Default: None. The regularization function\n            for the output\n        kernel_constraint (str): Default: None. Keras constraint for kernel\n            values\n        bias_constraint (str): Default: None .Keras constraint for bias values\n\n    Method:\n        call(inputs, mask=None): the logic of the layer, returns the final graph\n        compute_output_shape(input_shape): compute static output shapes, returns list of tuple shapes\n        build(input_shape): initialize the weights and biases for each function\n        phi_e(inputs): update function for bonds and returns updated bond attribute e_p\n        rho_e_v(e_p, inputs): aggregate updated bonds e_p to per atom attributes, b_e_p\n        phi_v(b_e_p, inputs): update the atom attributes by the results from previous step b_e_p and all the inputs\n            returns v_p.\n        rho_e_u(e_p, inputs): aggregate bonds to global attribute\n        rho_v_u(v_p, inputs): aggregate atom to global attributes\n        get_config(): part of keras interface for serialization\n\n    """"""\n\n    def __init__(self,\n                 activation: OptStrOrCallable = None,\n                 use_bias: bool = True,\n                 kernel_initializer: OptStrOrCallable = \'glorot_uniform\',\n                 bias_initializer: OptStrOrCallable = \'zeros\',\n                 kernel_regularizer: OptStrOrCallable = None,\n                 bias_regularizer: OptStrOrCallable = None,\n                 activity_regularizer: OptStrOrCallable = None,\n                 kernel_constraint: OptStrOrCallable = None,\n                 bias_constraint: OptStrOrCallable = None,\n                 **kwargs):\n        if \'input_shape\' not in kwargs and \'input_dim\' in kwargs:\n            kwargs[\'input_shape\'] = (kwargs.pop(\'input_dim\'),)\n        self.activation = activations.get(activation)  # noqa\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        super().__init__(**kwargs)\n\n    def call(self, inputs: Sequence, mask=None) -> Sequence:\n        e_p = self.phi_e(inputs)\n        b_ei_p = self.rho_e_v(e_p, inputs)\n        v_p = self.phi_v(b_ei_p, inputs)\n        b_e_p = self.rho_e_u(e_p, inputs)\n        b_v_p = self.rho_v_u(v_p, inputs)\n        u_p = self.phi_u(b_e_p, b_v_p, inputs)\n        return [v_p, e_p, u_p]\n\n    def phi_e(self, inputs: Sequence) -> tf.Tensor:\n        """"""\n        This is for updating the edge attributes\n        ek\' = phi_e(ek, vrk, vsk, u)\n\n        Args:\n            inputs (Sequence): list or tuple for the graph inputs\n\n        Returns:\n            updated edge/bond attributes\n        """"""\n        raise NotImplementedError\n\n    def rho_e_v(self, e_p: tf.Tensor, inputs: Sequence) -> tf.Tensor:\n        """"""\n        This is for step 2, aggregate edge attributes per node\n        Ei\' = {(ek\', rk, sk)} with rk =i, k=1:Ne\n\n        Args:\n            e_p (tf.Tensor): the updated edge attributes\n            inputs (Sequence): list or tuple for the graph inputs\n        Returns:\n            edge/bond to node/atom aggregated tensor\n        """"""\n        raise NotImplementedError\n\n    def phi_v(self, b_ei_p: tf.Tensor, inputs: Sequence):\n        """"""\n        Step 3. Compute updated node attributes\n        v_i\' = phi_v(\\bar e_i, vi, u)\n\n        Args:\n            b_ei_p (tf.Tensor): edge-to-node aggregated tensor\n            inputs (Sequence): list or tuple for the graph inputs\n        Returns:\n            updated node/atom attributes\n        """"""\n        raise NotImplementedError\n\n    def rho_e_u(self, e_p: tf.Tensor, inputs: Sequence) -> tf.Tensor:\n        """"""\n        let V\' = {v\'} i = 1:Nv\n        let E\' = {(e_k\', rk, sk)} k = 1:Ne\n        \\bar e\' = rho_e_u(E\')\n\n        Args:\n            e_p (tf.Tensor): updated edge/bond attributes\n            inputs (Sequence): list or tuple for the graph inputs\n        Returns:\n            edge/bond to global/state aggregated tensor\n        """"""\n        raise NotImplementedError\n\n    def rho_v_u(self, v_p: tf.Tensor, inputs: Sequence) -> tf.Tensor:\n        """"""\n        \\bar v\' = rho_v_u(V\')\n\n        Args:\n            v_p (tf.Tensor): updated atom/node attributes\n            inputs (Sequence): list or tuple for the graph inputs\n        Returns:\n            atom/node to global/state aggregated tensor\n        """"""\n        raise NotImplementedError\n\n    def phi_u(self, b_e_p: tf.Tensor, b_v_p: tf.Tensor, inputs: Sequence) -> tf.Tensor:\n        """"""\n        u\' = phi_u(\\bar e\', \\bar v\', u)\n        Args:\n            b_e_p (tf.Tensor): edge/bond to global aggregated tensor\n            b_v_p (tf.Tensor): node/atom to global aggregated tensor\n            inputs (Sequence): list or tuple for the graph inputs\n        Returns:\n            updated globa/state attributes\n        """"""\n        raise NotImplementedError\n\n    def get_config(self) -> Dict:\n        """"""\n        Part of keras layer interface, where the signature is converted into a dict\n        Returns:\n            configurational dictionary\n        """"""\n        config = {\n            \'activation\': activations.serialize(self.activation),\n            \'use_bias\': self.use_bias,\n            \'kernel_initializer\': initializers.serialize(\n                self.kernel_initializer),\n            \'bias_initializer\': initializers.serialize(\n                self.bias_initializer),\n            \'kernel_regularizer\': regularizers.serialize(\n                self.kernel_regularizer),\n            \'bias_regularizer\': regularizers.serialize(\n                self.bias_regularizer),\n            \'activity_regularizer\': regularizers.serialize(\n                self.activity_regularizer),\n            \'kernel_constraint\': constraints.serialize(\n                self.kernel_constraint),\n            \'bias_constraint\': constraints.serialize(self.bias_constraint)\n        }\n\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))  # noqa\n'"
megnet/layers/graph/cgcnn.py,7,"b'from megnet.layers.graph.base import GraphNetworkLayer\nimport tensorflow.keras.backend as kb\n\nimport tensorflow as tf\n\n\nclass CrystalGraphLayer(GraphNetworkLayer):\n    """"""\n    The CGCNN graph implementation as described in the paper\n\n    Xie et al. PHYSICAL REVIEW LETTERS 120, 145301 (2018)\n\n    Args:\n        activation (str): Default: None. The activation function used for each sub-neural network. Examples include\n            \'relu\', \'softmax\', \'tanh\', \'sigmoid\' and etc.\n        use_bias (bool): Default: True. Whether to use the bias term in the neural network.\n        kernel_initializer (str): Default: \'glorot_uniform\'. Initialization function for the layer kernel weights,\n        bias_initializer (str): Default: \'zeros\'\n        activity_regularizer (str): Default: None. The regularization function for the output\n        kernel_constraint (str): Default: None. Keras constraint for kernel values\n        bias_constraint (str): Default: None .Keras constraint for bias values\n\n    Methods:\n        call(inputs, mask=None): the logic of the layer, returns the final graph\n        compute_output_shape(input_shape): compute static output shapes, returns list of tuple shapes\n        build(input_shape): initialize the weights and biases for each function\n        phi_e(inputs): update function for bonds and returns updated bond attribute e_p\n        rho_e_v(e_p, inputs): aggregate updated bonds e_p to per atom attributes, b_e_p\n        phi_v(b_e_p, inputs): update the atom attributes by the results from previous step b_e_p and all the inputs\n            returns v_p.\n        rho_e_u(e_p, inputs): aggregate bonds to global attribute\n        rho_v_u(v_p, inputs): aggregate atom to global attributes\n        get_config(): part of keras interface for serialization\n\n    """"""\n\n    def __init__(self,\n                 activation=\'relu\',\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n\n        super().__init__(activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n\n    def build(self, input_shapes):\n        vdim = input_shapes[0][2]\n        edim = input_shapes[1][2]\n\n        with kb.name_scope(self.name):\n            with kb.name_scope(\'phi_v\'):\n                v_shapes = [[2 * vdim + edim, vdim]] * 2\n                self.phi_v_weights = [self.add_weight(shape=i,\n                                                      initializer=self.kernel_initializer,\n                                                      name=\'weight_v_%d\' % j,\n                                                      regularizer=self.kernel_regularizer,\n                                                      constraint=self.kernel_constraint)\n                                      for j, i in enumerate(v_shapes)]\n                if self.use_bias:\n                    self.phi_v_biases = [self.add_weight(shape=(i[-1],),\n                                                         initializer=self.bias_initializer,\n                                                         name=\'bias_v_%d\' % j,\n                                                         regularizer=self.bias_regularizer,\n                                                         constraint=self.bias_constraint)\n                                         for j, i in enumerate(v_shapes)]\n                else:\n                    self.phi_v_biases = None\n        self.built = True\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def phi_e(self, inputs):\n        nodes, edges, u, index1, index2, gnode, gbond = inputs\n        return edges\n\n    def rho_e_v(self, e_p, inputs):\n        """"""\n        Reduce edge attributes to node attribute, eqn 5 in the paper\n        Args:\n            e_p: updated bond\n            inputs: the whole input list\n\n        Returns: summed tensor\n\n        """"""\n        nodes, edges, u, index1, index2, gnode, gbond = inputs\n\n        index1 = tf.reshape(index1, (-1,))\n        index2 = tf.reshape(index2, (-1,))\n        fs = tf.gather(nodes, index1, axis=1)\n        fr = tf.gather(nodes, index2, axis=1)\n        concated = tf.concat([fs, fr, edges], axis=-1)\n        z1 = self._mlp(concated, self.phi_v_weights[0], self.phi_v_biases[0])\n        z2 = self._mlp(concated, self.phi_v_weights[1], self.phi_v_biases[1])\n        summed = tf.nn.sigmoid(z1) * self.activation(z2)\n        return tf.transpose(a=tf.math.segment_sum(tf.transpose(a=summed, perm=[1, 0, 2]), index1), perm=[1, 0, 2])\n\n    def phi_v(self, b_ei_p, inputs):\n        nodes, edges, u, index1, index2, gnode, gbond = inputs\n        return nodes + b_ei_p\n\n    def rho_e_u(self, e_p, inputs):\n        return 0\n\n    def rho_v_u(self, v_p, inputs):\n        return 0\n\n    def phi_u(self, b_e_p, b_v_p, inputs):\n        return inputs[2]\n\n    def _mlp(self, input_, weights, bias):\n        output = kb.dot(input_, weights) + bias\n        return output\n\n    def get_config(self):\n        base_config = super().get_config()\n        return dict(list(base_config.items()))\n'"
megnet/layers/graph/megnet.py,18,"b'import tensorflow.keras.backend as kb\n\nfrom megnet.layers.graph import GraphNetworkLayer\nfrom megnet.utils.layer import repeat_with_index\n\nimport tensorflow as tf\n\n__author__ = ""Chi Chen""\n__copyright__ = ""Copyright 2018, Materials Virtual Lab ""\n__version__ = ""0.1""\n__date__ = ""Dec 1, 2018""\n\n\nclass MEGNetLayer(GraphNetworkLayer):\n    """"""\n    The MEGNet graph implementation as described in the paper\n\n    Chen, Chi; Ye, Weike Ye; Zuo, Yunxing; Zheng, Chen; Ong, Shyue Ping.\n    Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals,\n    2018, arXiv preprint. [arXiv:1812.05055](https://arxiv.org/abs/1812.05055)\n\n    Args:\n        units_v (list of integers): the hidden layer sizes for node update neural network\n        units_e (list of integers): the hidden layer sizes for edge update neural network\n        units_u (list of integers): the hidden layer sizes for state update neural network\n        pool_method (str): \'mean\' or \'sum\', determines how information is gathered to nodes from neighboring edges\n        activation (str): Default: None. The activation function used for each sub-neural network. Examples include\n            \'relu\', \'softmax\', \'tanh\', \'sigmoid\' and etc.\n        use_bias (bool): Default: True. Whether to use the bias term in the neural network.\n        kernel_initializer (str): Default: \'glorot_uniform\'. Initialization function for the layer kernel weights,\n        bias_initializer (str): Default: \'zeros\'\n        activity_regularizer (str): Default: None. The regularization function for the output\n        kernel_constraint (str): Default: None. Keras constraint for kernel values\n        bias_constraint (str): Default: None .Keras constraint for bias values\n\n    Methods:\n        call(inputs, mask=None): the logic of the layer, returns the final graph\n        compute_output_shape(input_shape): compute static output shapes, returns list of tuple shapes\n        build(input_shape): initialize the weights and biases for each function\n        phi_e(inputs): update function for bonds and returns updated bond attribute e_p\n        rho_e_v(e_p, inputs): aggregate updated bonds e_p to per atom attributes, b_e_p\n        phi_v(b_e_p, inputs): update the atom attributes by the results from previous step b_e_p and all the inputs\n            returns v_p.\n        rho_e_u(e_p, inputs): aggregate bonds to global attribute\n        rho_v_u(v_p, inputs): aggregate atom to global attributes\n        get_config(): part of keras interface for serialization\n\n    """"""\n\n    def __init__(self,\n                 units_v,\n                 units_e,\n                 units_u,\n                 pool_method=\'mean\',\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n\n        super().__init__(activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n        self.units_v = units_v\n        self.units_e = units_e\n        self.units_u = units_u\n        self.pool_method = pool_method\n        if pool_method == \'mean\':\n            self.reduce_method = tf.reduce_mean\n            self.seg_method = tf.math.segment_mean\n        elif pool_method == \'sum\':\n            self.reduce_method = tf.reduce_sum\n            self.seg_method = tf.math.segment_sum\n        else:\n            raise ValueError(\'Pool method: \' + pool_method + \' not understood!\')\n\n    def build(self, input_shapes):\n        vdim = input_shapes[0][2]\n        edim = input_shapes[1][2]\n        udim = input_shapes[2][2]\n\n        with kb.name_scope(self.name):\n            with kb.name_scope(\'phi_v\'):\n                v_shapes = [self.units_e[-1] + vdim + udim] + self.units_v\n                v_shapes = list(zip(v_shapes[:-1], v_shapes[1:]))\n                self.phi_v_weights = [self.add_weight(shape=i,\n                                                      initializer=self.kernel_initializer,\n                                                      name=\'weight_v_%d\' % j,\n                                                      regularizer=self.kernel_regularizer,\n                                                      constraint=self.kernel_constraint)\n                                      for j, i in enumerate(v_shapes)]\n                if self.use_bias:\n                    self.phi_v_biases = [self.add_weight(shape=(i[-1],),\n                                                         initializer=self.bias_initializer,\n                                                         name=\'bias_v_%d\' % j,\n                                                         regularizer=self.bias_regularizer,\n                                                         constraint=self.bias_constraint)\n                                         for j, i in enumerate(v_shapes)]\n                else:\n                    self.phi_v_biases = None\n\n            with kb.name_scope(\'phi_e\'):\n                e_shapes = [2 * vdim + edim + udim] + self.units_e\n                e_shapes = list(zip(e_shapes[:-1], e_shapes[1:]))\n                self.phi_e_weights = [self.add_weight(shape=i,\n                                                      initializer=self.kernel_initializer,\n                                                      name=\'weight_e_%d\' % j,\n                                                      regularizer=self.kernel_regularizer,\n                                                      constraint=self.kernel_constraint)\n                                      for j, i in enumerate(e_shapes)]\n                if self.use_bias:\n                    self.phi_e_biases = [self.add_weight(shape=(i[-1],),\n                                                         initializer=self.bias_initializer,\n                                                         name=\'bias_e_%d\' % j,\n                                                         regularizer=self.bias_regularizer,\n                                                         constraint=self.bias_constraint)\n                                         for j, i in enumerate(e_shapes)]\n                else:\n                    self.phi_e_biases = None\n\n            with kb.name_scope(\'phi_u\'):\n                u_shapes = [self.units_e[-1] + self.units_v[\n                    -1] + udim] + self.units_u\n                u_shapes = list(zip(u_shapes[:-1], u_shapes[1:]))\n                self.phi_u_weights = [self.add_weight(shape=i,\n                                                      initializer=self.kernel_initializer,\n                                                      name=\'weight_u_%d\' % j,\n                                                      regularizer=self.kernel_regularizer,\n                                                      constraint=self.kernel_constraint)\n                                      for j, i in enumerate(u_shapes)]\n                if self.use_bias:\n                    self.phi_u_biases = [self.add_weight(shape=(i[-1],),\n                                                         initializer=self.bias_initializer,\n                                                         name=\'bias_u_%d\' % j,\n                                                         regularizer=self.bias_regularizer,\n                                                         constraint=self.bias_constraint)\n                                         for j, i in enumerate(u_shapes)]\n                else:\n                    self.phi_u_biases = None\n        self.built = True\n\n    def compute_output_shape(self, input_shape):\n        node_feature_shape = input_shape[0]\n        edge_feature_shape = input_shape[1]\n        state_feature_shape = input_shape[2]\n        output_shape = [\n            (node_feature_shape[0], node_feature_shape[1], self.units_v[-1]),\n            (edge_feature_shape[0], edge_feature_shape[1], self.units_e[-1]),\n            (state_feature_shape[0], state_feature_shape[1], self.units_u[-1])]\n        return output_shape\n\n    def phi_e(self, inputs):\n        nodes, edges, u, index1, index2, gnode, gbond = inputs\n        index1 = tf.reshape(index1, (-1,))\n        index2 = tf.reshape(index2, (-1,))\n        fs = tf.gather(nodes, index1, axis=1)\n        fr = tf.gather(nodes, index2, axis=1)\n        concate_node = tf.concat([fs, fr], axis=-1)\n        u_expand = repeat_with_index(u, gbond, axis=1)\n        concated = tf.concat([concate_node, edges, u_expand], axis=-1)\n        return self._mlp(concated, self.phi_e_weights, self.phi_e_biases)\n\n    def rho_e_v(self, e_p, inputs):\n        node, edges, u, index1, index2, gnode, gbond = inputs\n        index1 = tf.reshape(index1, (-1,))\n        return tf.expand_dims(self.seg_method(tf.squeeze(e_p), index1), axis=0)\n\n    def phi_v(self, b_ei_p, inputs):\n        nodes, edges, u, index1, index2, gnode, gbond = inputs\n        u_expand = repeat_with_index(u, gnode, axis=1)\n        concated = tf.concat([b_ei_p, nodes, u_expand], axis=-1)\n        return self._mlp(concated, self.phi_v_weights, self.phi_v_biases)\n\n    def rho_e_u(self, e_p, inputs):\n        nodes, edges, u, index1, index2, gnode, gbond = inputs\n        gbond = tf.reshape(gbond, (-1,))\n        return tf.expand_dims(self.seg_method(tf.squeeze(e_p), gbond), axis=0)\n\n    def rho_v_u(self, v_p, inputs):\n        nodes, edges, u, index1, index2, gnode, gbond = inputs\n        gnode = tf.reshape(gnode, (-1,))\n        return tf.expand_dims(self.seg_method(tf.squeeze(v_p, axis=0), gnode), axis=0)\n\n    def phi_u(self, b_e_p, b_v_p, inputs):\n        concated = tf.concat([b_e_p, b_v_p, inputs[2]], axis=-1)\n        return self._mlp(concated, self.phi_u_weights, self.phi_u_biases)\n\n    def _mlp(self, input_, weights, biases):\n        if biases is None:\n            biases = [0] * len(weights)\n        act = input_\n        for w, b in zip(weights, biases):\n            output = kb.dot(act, w) + b\n            act = self.activation(output)\n        return output\n\n    def get_config(self):\n        config = {\n            \'units_e\': self.units_e,\n            \'units_v\': self.units_v,\n            \'units_u\': self.units_u,\n            \'pool_method\': self.pool_method\n        }\n\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
megnet/layers/graph/schnet.py,4,"b'import tensorflow.keras.backend as kb\n\nfrom megnet.layers.graph.base import GraphNetworkLayer\nfrom megnet.activations import softplus2\n\nimport tensorflow as tf\n\n\nclass InteractionLayer(GraphNetworkLayer):\n    """"""\n    The Continuous filter InteractionLayer in Schnet\n\n    Sch\xc3\xbctt et al. SchNet: A continuous-filter convolutional neural network for modeling quantum interactions\n\n    Args:\n        activation (str): Default: None. The activation function used for each sub-neural network. Examples include\n            \'relu\', \'softmax\', \'tanh\', \'sigmoid\' and etc.\n        use_bias (bool): Default: True. Whether to use the bias term in the neural network.\n        kernel_initializer (str): Default: \'glorot_uniform\'. Initialization function for the layer kernel weights,\n        bias_initializer (str): Default: \'zeros\'\n        activity_regularizer (str): Default: None. The regularization function for the output\n        kernel_constraint (str): Default: None. Keras constraint for kernel values\n        bias_constraint (str): Default: None .Keras constraint for bias values\n\n    Methods:\n        call(inputs, mask=None): the logic of the layer, returns the final graph\n        compute_output_shape(input_shape): compute static output shapes, returns list of tuple shapes\n        build(input_shape): initialize the weights and biases for each function\n        phi_e(inputs): update function for bonds and returns updated bond attribute e_p\n        rho_e_v(e_p, inputs): aggregate updated bonds e_p to per atom attributes, b_e_p\n        phi_v(b_e_p, inputs): update the atom attributes by the results from previous step b_e_p and all the inputs\n            returns v_p.\n        rho_e_u(e_p, inputs): aggregate bonds to global attribute\n        rho_v_u(v_p, inputs): aggregate atom to global attributes\n        get_config(): part of keras interface for serialization\n\n    """"""\n\n    def __init__(self,\n                 activation=softplus2,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__(activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n\n    def build(self, input_shapes):\n        vdim = input_shapes[0][2]\n        edim = input_shapes[1][2]\n\n        with kb.name_scope(self.name):\n            with kb.name_scope(\'phi_e\'):\n                e_shapes = [[edim, vdim]] + [[vdim, vdim]] * 2\n                self.phi_e_weights = [self.add_weight(shape=i,\n                                                      initializer=self.kernel_initializer,\n                                                      name=\'weight_v_%d\' % j,\n                                                      regularizer=self.kernel_regularizer,\n                                                      constraint=self.kernel_constraint)\n                                      for j, i in enumerate(e_shapes)]\n                if self.use_bias:\n                    self.phi_e_biases = [self.add_weight(shape=(i[-1],),\n                                                         initializer=self.bias_initializer,\n                                                         name=\'bias_v_%d\' % j,\n                                                         regularizer=self.bias_regularizer,\n                                                         constraint=self.bias_constraint)\n                                         for j, i in enumerate(e_shapes)]\n                else:\n                    self.phi_e_biases = None\n\n        with kb.name_scope(self.name):\n            with kb.name_scope(\'phi_v\'):\n\n                v_shapes = [[vdim, vdim]] + [[vdim, vdim]] * 2\n                self.phi_v_weights = [self.add_weight(shape=i,\n                                                      initializer=self.kernel_initializer,\n                                                      name=\'weight_v_%d\' % j,\n                                                      regularizer=self.kernel_regularizer,\n                                                      constraint=self.kernel_constraint)\n                                      for j, i in enumerate(v_shapes)]\n                if self.use_bias:\n                    self.phi_v_biases = [self.add_weight(shape=(i[-1],),\n                                                         initializer=self.bias_initializer,\n                                                         name=\'bias_v_%d\' % j,\n                                                         regularizer=self.bias_regularizer,\n                                                         constraint=self.bias_constraint)\n                                         for j, i in enumerate(v_shapes)]\n                else:\n                    self.phi_v_biases = None\n        self.built = True\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def phi_e(self, inputs):\n        nodes, edges, u, index1, index2, gnode, gbond = inputs\n        return edges\n\n    def rho_e_v(self, e_p, inputs):\n        """"""\n        Reduce edge attributes to node attribute, eqn 5 in the paper\n        Args:\n            e_p: updated bond\n            inputs: the whole input list\n\n        Returns: summed tensor\n\n        """"""\n        nodes, edges, u, index1, index2, gnode, gbond = inputs\n\n        atomwise1 = self._mlp(nodes, self.phi_v_weights[0], self.phi_v_biases[0])\n\n        cfconv1 = self.activation(self._mlp(edges, self.phi_e_weights[0], self.phi_e_biases[0]))\n        cfconv2 = self.activation(self._mlp(cfconv1, self.phi_e_weights[1], self.phi_e_biases[1]))\n        cfconv_out = self._mlp(cfconv2, self.phi_e_weights[2], self.phi_e_biases[2])\n\n        index1 = tf.reshape(index1, (-1,))\n        index2 = tf.reshape(index2, (-1,))\n        fr = tf.gather(atomwise1, index2, axis=1)\n\n        after_cfconv = atomwise1 + \\\n            tf.transpose(a=tf.math.segment_sum(tf.transpose(a=fr * cfconv_out, perm=[1, 0, 2]), index1), perm=[1, 0, 2])\n\n        atomwise2 = self.activation(self._mlp(after_cfconv, self.phi_v_weights[1], self.phi_v_biases[1]))\n        atomwise3 = self._mlp(atomwise2, self.phi_v_weights[2], self.phi_v_biases[2])\n        return atomwise3\n\n    def phi_v(self, b_ei_p, inputs):\n        nodes, edges, u, index1, index2, gnode, gbond = inputs\n        return nodes + b_ei_p\n\n    def rho_e_u(self, e_p, inputs):\n        return 0\n\n    def rho_v_u(self, v_p, inputs):\n        return 0\n\n    def phi_u(self, b_e_p, b_v_p, inputs):\n        return inputs[2]\n\n    def _mlp(self, input_, weights, bias):\n        output = kb.dot(input_, weights) + bias\n        return output\n\n    def get_config(self):\n        base_config = super().get_config()\n        return base_config\n'"
megnet/layers/readout/__init__.py,0,b'from .set2set import Set2Set\nfrom .linear import LinearWithIndex\n'
megnet/layers/readout/linear.py,8,"b'from tensorflow.keras.layers import Layer\n\nimport tensorflow as tf\n\nMAPPING = {\'mean\': tf.math.segment_mean,\n           \'sum\': tf.math.segment_sum,\n           \'max\': tf.math.segment_max,\n           \'min\': tf.math.segment_min,\n           \'prod\': tf.math.segment_prod}\n\n\nclass LinearWithIndex(Layer):\n    """"""\n    Sum or average the node/edge attributes to get a structure-level vector\n\n    Args:\n        mode: (str) \'mean\', \'sum\', \'max\', \'mean\' or \'prod\'\n    """"""\n    def __init__(self, mode=\'mean\', **kwargs):\n        super(LinearWithIndex, self).__init__(**kwargs)\n        self.mode = mode\n        self.reduce_method = MAPPING.get(mode, None)\n        if self.reduce_method is None:\n            raise ValueError(\'mode not supported\')\n\n    def build(self, input_shape):\n        self.built = True\n\n    def call(self, inputs, mask=None):\n        prop, index = inputs\n        index = tf.reshape(index, (-1,))\n        prop = tf.transpose(a=prop, perm=[1, 0, 2])\n        out = self.reduce_method(prop, index)\n        out = tf.transpose(a=out, perm=[1, 0, 2])\n        return out\n\n    def compute_output_shape(self, input_shape):\n        prop_shape = input_shape[0]\n        return prop_shape[0], None, prop_shape[-1]\n\n    def get_config(self):\n        config = {\'mode\': self.mode}\n        base_config = super(LinearWithIndex, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
megnet/layers/readout/set2set.py,16,"b'from tensorflow.keras.layers import Layer\nimport tensorflow.keras.backend as kb\nfrom tensorflow.keras import activations, initializers, regularizers, constraints\n\nfrom megnet.utils.layer import repeat_with_index\n\nimport tensorflow as tf\n\n\nclass Set2Set(Layer):\n    """"""\n    For a set of vectors, the set2set neural network maps it to a single vector.\n    The order invariance is acheived by a attention mechanism.\n    See Vinyals, Oriol, Samy Bengio, and Manjunath Kudlur.\n    ""Order matters: Sequence to sequence for sets."" arXiv preprint\n    arXiv:1511.06391 (2015).\n\n    Args:\n        T: (int) recurrent step\n        n_hidden: (int) number of hidden units\n        activation: (str or object) activation function\n        activation_lstm: (str or object) activation function for lstm\n        recurrent_activation: (str or object) activation function for recurrent step\n        kernel_initializer: (str or object) initializer for kernel weights\n        recurrent_initializer: (str or object) initializer for recurrent weights\n        bias_initializer: (str or object) initializer for biases\n        use_bias: (bool) whether to use biases\n        unit_forget_bias: (bool) whether to use basis in forget gate\n        kernel_regularizer: (str or object) regularizer for kernel weights\n        recurrent_regularizer: (str or object) regularizer for recurrent weights\n        bias_regularizer: (str or object) regularizer for biases\n        kernel_constraint: (str or object) constraint for kernel weights\n        recurrent_constraint: (str or object) constraint for recurrent weights\n        bias_constraint:(str or object) constraint for biases\n        kwargs: other inputs for keras Layer class\n\n    """"""\n\n    def __init__(self,\n                 T=3,\n                 n_hidden=512,\n                 activation=None,\n                 activation_lstm=\'tanh\',\n                 recurrent_activation=\'hard_sigmoid\',\n                 kernel_initializer=\'glorot_uniform\',\n                 recurrent_initializer=\'orthogonal\',\n                 bias_initializer=\'zeros\',\n                 use_bias=True,\n                 unit_forget_bias=True,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n\n        super().__init__(**kwargs)\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.activation_lstm = activations.get(activation_lstm)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.unit_forget_bias = unit_forget_bias\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.T = T\n        self.n_hidden = n_hidden\n\n    def build(self, input_shape):\n\n        feature_shape, index_shape = input_shape\n        self.m_weight = self.add_weight(\n            shape=(feature_shape[-1], self.n_hidden),\n            initializer=self.kernel_initializer,\n            name=""x_to_m_weight"",\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.m_bias = self.add_weight(shape=(self.n_hidden,),\n                                          initializer=self.bias_initializer,\n                                          name=\'x_to_m_bias\',\n                                          regularizer=self.bias_regularizer,\n                                          constraint=self.bias_constraint)\n        else:\n            self.m_bias = None\n\n        self.recurrent_kernel = self.add_weight(\n            shape=(2 * self.n_hidden, 4 * self.n_hidden),\n            name=\'recurrent_kernel\',\n            initializer=self.recurrent_initializer,\n            regularizer=self.recurrent_regularizer,\n            constraint=self.recurrent_constraint)\n        if self.use_bias:\n            if self.unit_forget_bias:\n                def bias_initializer(_, *args, **kwargs):\n                    return kb.concatenate([self.bias_initializer(\n                        (self.n_hidden,), *args, **kwargs),\n                                          initializers.Ones()((self.n_hidden,),\n                                                              *args, **kwargs),\n                                          self.bias_initializer(\n                                              (self.n_hidden * 2,), *args,\n                                              **kwargs)])\n            else:\n                bias_initializer = self.bias_initializer\n            self.recurrent_bias = self.add_weight(shape=(self.n_hidden * 4,),\n                                                  name=\'recurrent_bias\',\n                                                  initializer=bias_initializer,\n                                                  regularizer=self.bias_regularizer,\n                                                  constraint=self.bias_constraint)\n        else:\n            self.recurrent_bias = None\n        self.built = True\n\n    def compute_output_shape(self, input_shape):\n        feature_shape, index_shape = input_shape\n        return feature_shape[0], None, 2 * self.n_hidden\n\n    def call(self, inputs, mask=None):\n        features, feature_graph_index = inputs\n        feature_graph_index = tf.reshape(feature_graph_index, (-1,))\n        _, _, count = tf.unique_with_counts(feature_graph_index)\n        m = kb.dot(features, self.m_weight)\n        if self.use_bias:\n            m += self.m_bias\n\n        self.h = tf.zeros(tf.stack(\n            [tf.shape(input=features)[0], tf.shape(input=count)[0], self.n_hidden]))\n        self.c = tf.zeros(tf.stack(\n            [tf.shape(input=features)[0], tf.shape(input=count)[0], self.n_hidden]))\n        q_star = tf.zeros(tf.stack(\n            [tf.shape(input=features)[0], tf.shape(input=count)[0], 2 * self.n_hidden]))\n        for i in range(self.T):\n            self.h, c = self._lstm(q_star, self.c)\n            e_i_t = tf.reduce_sum(\n                input_tensor=m * repeat_with_index(self.h, feature_graph_index), axis=-1)\n            exp = tf.exp(e_i_t)\n            # print(\'exp shape \', exp.shape)\n            seg_sum = tf.transpose(\n                a=tf.math.segment_sum(\n                    tf.transpose(a=exp, perm=[1, 0]),\n                    feature_graph_index),\n                perm=[1, 0])\n            seg_sum = tf.expand_dims(seg_sum, axis=-1)\n            # print(\'seg_sum shape\', seg_sum.shape)\n            interm = repeat_with_index(seg_sum, feature_graph_index)\n            # print(\'interm shape\', interm.shape)\n            a_i_t = exp / interm[..., 0]\n            # print(a_i_t.shape)\n            r_t = tf.transpose(a=tf.math.segment_sum(\n                tf.transpose(a=tf.multiply(m, a_i_t[:, :, None]), perm=[1, 0, 2]),\n                feature_graph_index), perm=[1, 0, 2])\n            q_star = kb.concatenate([self.h, r_t], axis=-1)\n        return q_star\n\n    def _lstm(self, h, c):\n        # lstm implementation here\n        z = kb.dot(h, self.recurrent_kernel)\n        if self.use_bias:\n            z += self.recurrent_bias\n        z0 = z[:, :, :self.n_hidden]\n        z1 = z[:, :, self.n_hidden:2 * self.n_hidden]\n        z2 = z[:, :, 2 * self.n_hidden:3 * self.n_hidden]\n        z3 = z[:, :, 3 * self.n_hidden:]\n        i = self.recurrent_activation(z0)\n        f = self.recurrent_activation(z1)\n        # print(z.shape, f.shape, c.shape, z2.shape)\n        c = f * c + i * self.activation_lstm(z2)\n        o = self.recurrent_activation(z3)\n        h = o * self.activation_lstm(c)\n        return h, c\n\n    def get_config(self):\n        config = {""T"": self.T,\n                  ""n_hidden"": self.n_hidden,\n                  ""activation"": activations.serialize(self.activation),\n                  ""activation_lstm"": activations.serialize(\n                      self.activation_lstm),\n                  ""recurrent_activation"": activations.serialize(\n                      self.recurrent_activation),\n                  ""kernel_initializer"": initializers.serialize(\n                      self.kernel_initializer),\n                  ""recurrent_initializer"": initializers.serialize(\n                      self.recurrent_initializer),\n                  ""bias_initializer"": initializers.serialize(\n                      self.bias_initializer),\n                  ""use_bias"": self.use_bias,\n                  ""unit_forget_bias"": self.unit_forget_bias,\n                  ""kernel_regularizer"": regularizers.serialize(\n                      self.kernel_regularizer),\n                  ""recurrent_regularizer"": regularizers.serialize(\n                      self.recurrent_regularizer),\n                  ""bias_regularizer"": regularizers.serialize(\n                      self.bias_regularizer),\n                  ""kernel_constraint"": constraints.serialize(\n                      self.kernel_constraint),\n                  ""recurrent_constraint"": constraints.serialize(\n                      self.recurrent_constraint),\n                  ""bias_constraint"": constraints.serialize(self.bias_constraint)\n\n                  }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
megnet/utils/tests/test_data.py,0,"b'import unittest\nfrom pymatgen import Structure, Lattice\nfrom megnet.utils.data import get_graphs_within_cutoff\n\n\nclass TestGeneralUtils(unittest.TestCase):\n    def test_model_load(self):\n        s = Structure(Lattice.cubic(3.6), [\'Mo\', \'Mo\'],\n                      [[0.5, 0.5, 0.5], [0, 0, 0]])\n        center_indices, neighbor_indices, images, distances = \\\n            get_graphs_within_cutoff(s, 4)\n        self.assertListEqual(center_indices.tolist(),\n                             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                              0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n                              1, 1, 1, 1, 1, 1, 1, 1])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n\n'"
megnet/utils/tests/test_descriptor.py,0,"b'import unittest\nfrom pymatgen import Structure, Lattice\n\nfrom megnet.utils.descriptor import MEGNetDescriptor, DEFAULT_MODEL\nfrom tensorflow.keras.models import Model\n\n\nclass TestGeneralUtils(unittest.TestCase):\n    def test_model_load(self):\n        model = MEGNetDescriptor(model_name=DEFAULT_MODEL)\n        self.assertTrue(model.model, Model)\n        s = Structure(Lattice.cubic(3.6), [\'Mo\', \'Mo\'], [[0.5, 0.5, 0.5], [0, 0, 0]])\n        atom_features = model.get_atom_features(s)\n        bond_features = model.get_bond_features(s)\n        glob_features = model.get_global_features(s)\n        atom_set2set = model.get_set2set(s, ftype=\'atom\')\n        bond_set2set = model.get_set2set(s, ftype=\'bond\')\n        s_features = model.get_structure_features(s)\n        self.assertListEqual(list(atom_features.shape), [2, 32])\n        self.assertListEqual(list(bond_features.shape), [28, 32])\n        self.assertListEqual(list(glob_features.shape), [1, 32])\n        self.assertListEqual(list(atom_set2set.shape), [1, 32])\n        self.assertListEqual(list(bond_set2set.shape), [1, 32])\n        self.assertListEqual(list(s_features.shape), [1, 96])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/utils/tests/test_general.py,0,"b'import unittest\nimport numpy as np\n\nfrom megnet.utils.general import (\n    expand_1st, to_list, check_shape, reshape, fast_label_binarize)\n\n\nclass TestGeneralUtils(unittest.TestCase):\n    def test_expand_dim(self):\n        x = np.array([1, 2, 3])\n        self.assertListEqual(list(expand_1st(x).shape), [1, 3])\n\n    def test_to_list(self):\n        x = 1\n        y = [1]\n        z = tuple([1, 2, 3])\n        v = np.array([1, 2, 3])\n        k = np.array([[1, 2], [3, 4]])\n        for k in [x, y, z, v, k]:\n            self.assertTrue(type(to_list(k)), list)\n\n    def test_fast_label_binarize(self):\n        binaries = fast_label_binarize(1, [0, 1])\n        self.assertListEqual(binaries, [0])\n        binaries = fast_label_binarize(1, [0, 1, 2])\n        self.assertListEqual(binaries, [0, 1, 0])\n\n    def test_check_shape(self):\n        x = np.random.normal(size=(10, 20))\n        self.assertTrue(check_shape(x, [10, 20, None]))\n        self.assertTrue(check_shape(x, [10, 20]))\n        self.assertFalse(check_shape(x, [10, 10]))\n        self.assertTrue(check_shape(None, [10, 20]))\n\n    def test_reshape(self):\n        x = np.random.normal(size=(10, 20))\n        self.assertEqual(reshape(x, [10, 20, None]).shape, (10, 20, 1))\n        self.assertEqual(reshape(x, [10, 20, 20, None]).shape, (10, 20, 20, 1))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/utils/tests/test_layer.py,3,"b'import unittest\nfrom megnet.utils.layer import repeat_with_index, _repeat\nimport tensorflow as tf\n\n\nclass TestCase(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.x = tf.random.normal(shape=(1, 3, 4))\n        cls.index = tf.convert_to_tensor(value=[0, 0, 0, 1, 1, 2])\n        cls.n = tf.convert_to_tensor(value=[3, 2, 1])\n\n    def test_repeat(self):\n        repeat_result = _repeat(self.x, self.n, axis=1).numpy()\n        self.assertListEqual(list(repeat_result.shape), [1, 6, 4])\n        self.assertEqual(repeat_result[0, 0, 0], repeat_result[0, 1, 0])\n        self.assertEqual(repeat_result[0, 0, 0], repeat_result[0, 2, 0])\n        self.assertNotEqual(repeat_result[0, 0, 0], repeat_result[0, 3, 0])\n        self.assertEqual(repeat_result[0, 3, 0], repeat_result[0, 4, 0])\n\n    def test_repeat_with_index(self):\n        repeat_result = repeat_with_index(self.x, self.index, axis=1).numpy()\n        self.assertListEqual(list(repeat_result.shape), [1, 6, 4])\n        self.assertEqual(repeat_result[0, 0, 0], repeat_result[0, 1, 0])\n        self.assertEqual(repeat_result[0, 0, 0], repeat_result[0, 2, 0])\n        self.assertNotEqual(repeat_result[0, 0, 0], repeat_result[0, 3, 0])\n        self.assertEqual(repeat_result[0, 3, 0], repeat_result[0, 4, 0])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/utils/tests/test_metrics.py,0,"b'import unittest\nimport numpy as np\n\nfrom megnet.utils.metrics import mae, accuracy\n\n\nclass TestCase(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.y_true = np.array([0, 1, 2, 3, 4])\n        cls.y_pred = np.array([0.1, 1.1, 2.1, 3.1, 4.1])\n\n        cls.y_true_acc = np.array([0, 1, 1, 0])\n        cls.y_pred_acc = np.array([0.1, 0.6, 0.4, 0.4])\n\n    def test_mae(self):\n        mae_result = mae(self.y_true, self.y_pred)\n        self.assertAlmostEqual(mae_result, 0.1)\n\n    def test_accuracy(self):\n        acc = accuracy(self.y_true_acc, self.y_pred_acc)\n        self.assertAlmostEqual(acc, 0.75)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/utils/tests/test_models_util.py,0,"b'import unittest\n\nfrom megnet.utils.models import load_model\nfrom megnet.models import GraphModel\n\n\nclass TestLoadModel(unittest.TestCase):\n\n    def test_load_crystal(self):\n        model = load_model(\'Eform_MP_2019\')\n        self.assertIsInstance(model, GraphModel)\n        with self.assertRaises(ValueError):\n            _ = load_model(\'Eform_MP_2020\')\n\n    def test_load_qm9(self):\n        model = load_model(\'QM9_G_2018\')\n        self.assertIsInstance(model, GraphModel)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/utils/tests/test_molecule_util.py,0,"b'import os\nimport unittest\nfrom pymatgen import Molecule\n\nfrom megnet.utils.molecule import get_pmg_mol_from_smiles, pb\nfrom megnet.models import MEGNetModel\n\n\nCWD = os.path.dirname(os.path.abspath(__file__))\n\n\nclass TestMolecule(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.molecule = Molecule([\'C\', \'O\', \'O\'], [[0, 0, 0], [-1, 0, 0], [1, 0, 0]])\n        cls.model = MEGNetModel.from_file(os.path.join(\n            CWD, \'../../../mvl_models/mp-2019.4.1/formation_energy.hdf5\'))\n\n    def test_mol(self):\n        pred = self.model.predict_structure(self.molecule)\n        self.assertAlmostEqual(float(pred), 0.39973044, 5)\n\n    @unittest.skipIf(pb is None, ""Openbabel is not installed"")\n    def test_get_pmg_mol_from_smiles(self):\n        mol = get_pmg_mol_from_smiles(\'C\')\n        self.assertTrue(isinstance(mol, Molecule))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/utils/tests/test_preprocessing.py,0,"b'import unittest\n\nfrom megnet.utils.preprocessing import StandardScaler\n\n\nclass TestPreprocessing(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # Since only len(structure) is called, we will\n        # use a dummy list as structure\n        cls.structures = [[0, 1], [0, 1, 2], [0, 1, 2, 3]]\n        cls.targets = [2, 3, 4]\n\n    def test_from_training(self):\n        scaler = StandardScaler.from_training_data(self.structures,\n                                                   self.targets, is_intensive=False)\n        self.assertEqual(scaler.mean, 1)\n        self.assertEqual(scaler.std, 1)\n\n    def test_transform_inverse_transform(self):\n        scaler = StandardScaler.from_training_data(self.structures,\n                                                   self.targets, is_intensive=False)\n        transformed_target = scaler.transform(100, 1)\n        orig_target = scaler.inverse_transform(transformed_target, 1)\n        self.assertAlmostEqual(100, orig_target)\n        scaler = StandardScaler.from_training_data(self.structures,\n                                                   self.targets, is_intensive=True)\n        transformed_target = scaler.transform(100, 1)\n        orig_target = scaler.inverse_transform(transformed_target, 1)\n        self.assertAlmostEqual(100, orig_target)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n\n'"
megnet/layers/featurizer/tests/test_gaussian.py,0,"b'import unittest\n\nimport numpy as np\n\nfrom megnet.layers.featurizer import GaussianExpansion\n\nclass TestGaussian(unittest.TestCase):\n    def test_gaussian(self):\n        x = np.random.normal(size=(1, 10))\n        centers = np.linspace(0, 6, 100)\n        width = 0.5\n\n        ge = GaussianExpansion(centers=centers, width=width)\n        self.assertTrue(ge(x).shape == (1, 10, 100))\n        \n        np.testing.assert_array_equal(centers, ge.get_config()[\'centers\'])\n        \n        self.assertTrue(ge.built)\n        self.assertTrue(ge.compute_output_shape((1, 10)) == (1, 10, 100))\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/layers/graph/tests/test_cgcnn.py,0,"b'import unittest\n\nimport numpy as np\n\nfrom megnet.layers import CrystalGraphLayer\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\n\n\nclass TestLayer(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.n_features = 5\n        cls.n_bond_features = 6\n        cls.n_global_features = 2\n        cls.x = [\n            Input(shape=(None, cls.n_features)),\n            Input(shape=(None, cls.n_bond_features)),\n            Input(shape=(None, cls.n_global_features)),\n            Input(shape=(None, ), dtype=\'int32\'),\n            Input(shape=(None, ), dtype=\'int32\'),\n            Input(shape=(None, ), dtype=\'int32\'),\n            Input(shape=(None, ), dtype=\'int32\'),\n        ]\n\n    def test_layer(self):\n        layer = CrystalGraphLayer()\n        out = layer(self.x)\n        self.assertListEqual([i.shape[-1] for i in out],\n                             [self.n_features, self.n_bond_features, self.n_global_features])\n        new_layer = CrystalGraphLayer.from_config(layer.get_config())\n        out2 = new_layer(self.x)\n        self.assertListEqual([i.shape[-1] for i in out2],\n                             [self.n_features, self.n_bond_features, self.n_global_features])\n\n        int32 = \'int32\'\n        x1 = np.random.rand(1, 5, 10)\n        x2 = np.random.rand(1, 6, 5)\n        x3 = np.random.rand(1, 2, 20)\n        x4 = np.array([0, 1, 2, 3, 3, 4]).reshape([1, -1])\n        x5 = np.array([1, 0, 3, 2, 4, 3]).reshape([1, -1])\n        x6 = np.array([[0, 0, 1, 1, 1]])\n        x7 = np.array([[0, 0, 1, 1, 1, 1]])\n        x1_ = Input(shape=(None, 10))\n        x2_ = Input(shape=(None, 5))\n        x3_ = Input(shape=(None, 20))\n        x4_ = Input(shape=(None,), dtype=int32)\n        x5_ = Input(shape=(None,), dtype=int32)\n        x6_ = Input(shape=(None,), dtype=int32)\n        x7_ = Input(shape=(None,), dtype=int32)\n        out = CrystalGraphLayer()(\n            [x1_, x2_, x3_, x4_, x5_, x6_, x7_])\n        model = Model(inputs=[x1_, x2_, x3_, x4_, x5_, x6_, x7_], outputs=out)\n        model.compile(\'adam\', \'mse\')\n        ans = model.predict([x1, x2, x3, x4, x5, x6, x7])\n        self.assertEqual(ans[0].shape, (1, 5, 10))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/layers/graph/tests/test_megnet.py,0,"b'import unittest\n\nimport numpy as np\n\nfrom megnet.layers import MEGNetLayer\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\n\n\nclass TestLayer(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.n_features = 5\n        cls.n_bond_features = 6\n        cls.n_global_features = 2\n        cls.x = [\n            Input(shape=(None, cls.n_features)),\n            Input(shape=(None, cls.n_bond_features)),\n            Input(shape=(None, cls.n_global_features)),\n            Input(shape=(None, ), dtype=\'int32\'),\n            Input(shape=(None, ), dtype=\'int32\'),\n            Input(shape=(None, ), dtype=\'int32\'),\n            Input(shape=(None, ), dtype=\'int32\'),\n        ]\n\n    def test_megnet(self):\n        units_v = [13, 14, 16]\n        units_e = [16, 16, 17]\n        units_u = [13, 14, 18]\n        layer = MEGNetLayer(units_v, units_e, units_u)\n        out = layer(self.x)\n        self.assertListEqual([i.shape[-1] for i in out], [units_v[-1], units_e[-1], units_u[-1]])\n        new_layer = MEGNetLayer.from_config(layer.get_config())\n        out2 = new_layer(self.x)\n        self.assertListEqual([i.shape[-1] for i in out2], [units_v[-1], units_e[-1], units_u[-1]])\n\n        int32 = \'int32\'\n        x1 = np.random.rand(1, 5, 10)\n        x2 = np.random.rand(1, 6, 5)\n        x3 = np.random.rand(1, 2, 20)\n        x4 = np.array([0, 1, 2, 3, 3, 4]).reshape([1, -1])\n        x5 = np.array([1, 0, 3, 2, 4, 3]).reshape([1, -1])\n        x6 = np.array([[0, 0, 1, 1, 1]])\n        x7 = np.array([[0, 0, 1, 1, 1, 1]])\n        x1_ = Input(shape=(None, 10))\n        x2_ = Input(shape=(None, 5))\n        x3_ = Input(shape=(None, 20))\n        x4_ = Input(shape=(None,), dtype=int32)\n        x5_ = Input(shape=(None,), dtype=int32)\n        x6_ = Input(shape=(None,), dtype=int32)\n        x7_ = Input(shape=(None,), dtype=int32)\n        out = MEGNetLayer([10, 5], [20, 4], [30, 3])(\n            [x1_, x2_, x3_, x4_, x5_, x6_, x7_])\n        model = Model(inputs=[x1_, x2_, x3_, x4_, x5_, x6_, x7_], outputs=out)\n        model.compile(\'adam\', \'mse\')\n        ans = model.predict([x1, x2, x3, x4, x5, x6, x7])\n        self.assertEqual(ans[0].shape, (1, 5, 5))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/layers/graph/tests/test_schnet.py,0,"b'import unittest\n\nimport numpy as np\n\nfrom megnet.layers import InteractionLayer\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\n\n\nclass TestLayer(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.n_features = 10\n        cls.n_bond_features = 6\n        cls.n_global_features = 2\n        cls.x = [\n            Input(shape=(None, cls.n_features)),\n            Input(shape=(None, cls.n_bond_features)),\n            Input(shape=(None, cls.n_global_features)),\n            Input(shape=(None, ), dtype=\'int32\'),\n            Input(shape=(None, ), dtype=\'int32\'),\n            Input(shape=(None, ), dtype=\'int32\'),\n            Input(shape=(None, ), dtype=\'int32\'),\n        ]\n\n    def test_layer(self):\n        layer = InteractionLayer()\n        out = layer(self.x)\n        self.assertListEqual([i.shape[-1] for i in out], [self.n_features, self.n_bond_features, self.n_global_features])\n        new_layer = InteractionLayer.from_config(layer.get_config())\n        out2 = new_layer(self.x)\n        self.assertListEqual([i.shape[-1] for i in out2], [self.n_features, self.n_bond_features, self.n_global_features])\n\n        int32 = \'int32\'\n        x1 = np.random.rand(1, 5, 10)\n        x2 = np.random.rand(1, 6, 5)\n        x3 = np.random.rand(1, 2, 20)\n        x4 = np.array([0, 1, 2, 3, 3, 4]).reshape([1, -1])\n        x5 = np.array([1, 0, 3, 2, 4, 3]).reshape([1, -1])\n        x6 = np.array([[0, 0, 1, 1, 1]])\n        x7 = np.array([[0, 0, 1, 1, 1, 1]])\n        x1_ = Input(shape=(None, 10))\n        x2_ = Input(shape=(None, 5))\n        x3_ = Input(shape=(None, 20))\n        x4_ = Input(shape=(None,), dtype=int32)\n        x5_ = Input(shape=(None,), dtype=int32)\n        x6_ = Input(shape=(None,), dtype=int32)\n        x7_ = Input(shape=(None,), dtype=int32)\n        out = InteractionLayer()(\n            [x1_, x2_, x3_, x4_, x5_, x6_, x7_])\n        model = Model(inputs=[x1_, x2_, x3_, x4_, x5_, x6_, x7_], outputs=out)\n        model.compile(\'adam\', \'mse\')\n        ans = model.predict([x1, x2, x3, x4, x5, x6, x7])\n        self.assertEqual(ans[0].shape, (1, 5, 10))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/layers/readout/tests/test_linear.py,0,"b'from megnet.layers import LinearWithIndex\nimport numpy as np\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nimport unittest\n\n\nclass TestLayer(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.x = Input(shape=(None, 6))\n        cls.index = Input(shape=(None,), dtype=\'int32\')\n\n    def test_linear(self):\n        linear = LinearWithIndex(mode=\'mean\')\n        out = linear([self.x, self.index])\n        shapes = linear.compute_output_shape([self.x.shape, self.index.shape])\n        self.assertEqual(shapes[-1], 6)\n        model = Model(inputs=[self.x, self.index], outputs=out)\n        model.compile(loss=\'mse\', optimizer=\'adam\')\n        x = np.random.normal(size=(1, 5, 6))\n        expected_output = np.concatenate([np.mean(x[0, :3, :], axis=0, keepdims=True),\n                                          np.mean(x[0, 3:, :], axis=0, keepdims=True)], axis=0)\n        index = np.array([[0, 0, 0, 1, 1]])\n        result = model.predict([x, index])\n        diff = np.linalg.norm(result[0, :, :] - expected_output)\n        self.assertListEqual(list(result.shape), [1, 2, 6])\n        self.assertTrue(diff < 1e-5)\n\n        linear = LinearWithIndex(mode=\'sum\')\n        out = linear([self.x, self.index])\n        model = Model(inputs=[self.x, self.index], outputs=out)\n        model.compile(loss=\'mse\', optimizer=\'adam\')\n        x = np.random.normal(size=(1, 5, 6))\n        expected_output = np.concatenate([np.sum(x[0, :3, :], axis=0, keepdims=True),\n                                          np.sum(x[0, 3:, :], axis=0, keepdims=True)], axis=0)\n        index = np.array([[0, 0, 0, 1, 1]])\n        result = model.predict([x, index])\n        diff = np.linalg.norm(result[0, :, :] - expected_output)\n        self.assertListEqual(list(result.shape), [1, 2, 6])\n        self.assertTrue(diff < 1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
megnet/layers/readout/tests/test_set2set.py,0,"b'from megnet.layers import Set2Set\nimport numpy as np\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nimport unittest\n\n\nclass TestLayer(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.x = Input(shape=(None, 6))\n        cls.index = Input(shape=(None,), dtype=\'int32\')\n\n    def test_set2set(self):\n        n_hidden = 10\n        set2set = Set2Set(n_hidden=n_hidden)\n        out = set2set([self.x, self.index])\n        shapes = set2set.compute_output_shape([self.x.shape, self.index.shape])\n        self.assertEqual(shapes[-1], n_hidden*2)\n\n        model = Model(inputs=[self.x, self.index], outputs=out)\n        model.compile(loss=\'mse\', optimizer=\'adam\')\n        x = np.random.normal(size=(1, 5, 6))\n        index = np.array([[0, 0, 0, 1, 1]])\n        result = model.predict([x, index])\n        self.assertListEqual(list(result.shape), [1, 2, n_hidden*2])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
