file_path,api_count,code
run_fetch.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n__author__ = 'maxim'\n\n\nimport poloniex\nimport util\n\n\ndef main():\n  tickers, periods, _ = util.parse_command_line(default_periods=poloniex.AVAILABLE_PERIODS,\n                                                default_targets=[])\n  poloniex.update_selected(tickers, periods)\n\n\nif __name__ == '__main__':\n  main()\n"""
run_predict.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n__author__ = \'maxim\'\n\nimport poloniex\nfrom predict import *\nfrom train import *\nfrom util import *\n\ndef main():\n  tickers, periods, targets = parse_command_line(default_tickers=[\'BTC_ETH\'],\n                                                 default_periods=[\'day\'],\n                                                 default_targets=[\'high\'])\n\n  for ticker in tickers:\n    for period in periods:\n      for target in targets:\n        job = JobInfo(\'_data\', \'_zoo\', name=\'%s_%s\' % (ticker, period), target=target)\n        raw_df = poloniex.get_latest_data(ticker, period=period, depth=100)\n        result_df = predict_multiple(job, raw_df=raw_df, rows_to_predict=1)\n\n        raw_df.set_index(\'date\', inplace=True)\n        result_df.rename(columns={\'True\': \'Current-Truth\'}, inplace=True)\n\n        info(\'Latest chart info:\', raw_df.tail(2), \'\', sep=\'\\n\')\n        info(\'Prediction for ""%s"":\' % target, result_df, \'\', sep=\'\\n\')\n\nif __name__ == \'__main__\':\n  main()\n'"
run_train.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n__author__ = 'maxim'\n\n\nimport numpy as np\n\nfrom models import *\nfrom train import JobRunner, JobInfo\nimport util\n\n\ndef iterate_neural(job_info, job_runner, iterations=10, k_lim=21):\n  if NeuralNetworkModel is None:\n    return\n\n  job_runner.iterate(iterations, params_fun=lambda: {\n    'target': job_info.target,\n    'k': np.random.randint(1, k_lim),\n    'model_class': NeuralNetworkModel,\n    'model_params': {\n      'batch_size': np.random.choice([500, 1000, 2000, 4000]),\n      'epochs': 100,\n      'learning_rate': 10 ** np.random.uniform(-10, -2),\n      'init_sigma': 10 ** np.random.uniform(-10, -3),\n      'layers': _random_layers(np.random.randint(1, 4)),\n      'cost_func': np.random.choice(['l1', 'l2']),\n      'lambda': 10 ** np.random.uniform(-8, -2),\n    }\n  })\n\n\ndef _random_layers(num):\n  return [{\n    'size': np.random.randint(50, 200),\n    'batchnorm': np.random.choice([True, False]),\n    'activation_func': np.random.choice(['relu', 'elu', 'sigmoid', 'leaky_relu', 'prelu']),\n    'dropout': np.random.uniform(0.1, 0.95),\n  } for _ in range(num)]\n\n\ndef iterate_rnn(job_info, job_runner, iterations=10):\n  if RecurrentModel is None:\n    return\n\n  job_runner.iterate(iterations, params_fun=lambda: {\n    'target': job_info.target,\n    'k': np.random.choice([24, 32, 48, 64, 96]),\n    'model_class': RecurrentModel,\n    'model_params': {\n      'batch_size': np.random.choice([1000, 2000, 4000]),\n      'epochs': 100,\n      'learning_rate': 10 ** np.random.uniform(-4, -2),\n      'layers': [np.random.choice([32, 64, 96]) for _ in range(np.random.randint(1, 4))],\n      'cell_type': np.random.choice(['lstm', 'gru']),\n      'double_state': np.random.choice([True, False]),\n      'dropout': np.random.uniform(0.0, 1.0),\n      'cost_func': np.random.choice(['l1', 'l2']),\n      'lambda': 10 ** np.random.uniform(-10, -6),\n    }\n  })\n\n\ndef iterate_cnn(job_info, job_runner, iterations=10):\n  if ConvModel is None:\n    return\n\n  job_runner.iterate(iterations, params_fun=lambda: {\n    'target': job_info.target,\n    'k': np.random.choice([24, 32, 48, 64]),\n    'model_class': ConvModel,\n    'model_params': {\n      'batch_size': np.random.choice([1000, 2000, 4000]),\n      'epochs': 150,\n      'learning_rate': 10 ** np.random.uniform(-4, -2),\n      'layers': [(np.random.choice([32, 64, 96, 128]),\n                  np.random.randint(2, 6))\n                 for _ in range(np.random.randint(1, 4))],\n      'dropout': np.random.uniform(0.0, 0.7),\n      'cost_func': np.random.choice(['l1', 'l2']),\n      'lambda': 10 ** np.random.uniform(-10, -6),\n    }\n  })\n\n\ndef iterate_linear(job_info, job_runner, k_lim=25):\n  if LinearModel is None:\n    return\n\n  for k in range(1, k_lim):\n    job_runner.single_run(**{\n      'target': job_info.target,\n      'k': k,\n      'model_class': LinearModel,\n      'model_params': {}\n    })\n\n\ndef iterate_xgb(job_info, job_runner, iterations=10, k_lim=21):\n  if XgbModel is None:\n    return\n\n  job_runner.iterate(iterations, params_fun=lambda: {\n    'target': job_info.target,\n    'k': np.random.randint(1, k_lim),\n    'model_class': XgbModel,\n    'model_params': {\n      'max_depth': np.random.randint(3, 8),\n      'n_estimators': np.random.randint(100, 300),\n      'learning_rate': 10 ** np.random.uniform(-2, -0.5),\n      'gamma': np.random.uniform(0, 0.1),\n      'subsample': np.random.uniform(0.5, 1),\n    }\n  })\n\n\ndef main():\n  tickers, periods, targets = util.parse_command_line(default_periods=['day'],\n                                                      default_targets=['high'])\n  while True:\n    for ticker in tickers:\n      for period in periods:\n        for target in targets:\n          job_info = JobInfo('_data', '_zoo', name='%s_%s' % (ticker, period), target=target)\n          job_runner = JobRunner(job_info, limit=np.median)\n          iterate_linear(job_info, job_runner)\n          iterate_neural(job_info, job_runner)\n          iterate_xgb(job_info, job_runner)\n          iterate_rnn(job_info, job_runner)\n          iterate_cnn(job_info, job_runner)\n          job_runner.print_result()\n\n\nif __name__ == '__main__':\n  main()\n"""
run_visual.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n__author__ = 'maxim'\n\nimport matplotlib.pyplot as plt\n\nfrom predict import predict_multiple\nfrom train import JobInfo\nfrom util import parse_command_line, read_df\n\nplt.style.use('ggplot')\n\ndef main():\n  train_date = None\n  tickers, periods, targets = parse_command_line(default_tickers=['BTC_ETH', 'BTC_LTC'],\n                                                 default_periods=['day'],\n                                                 default_targets=['high'])\n\n  for ticker in tickers:\n    for period in periods:\n      for target in targets:\n        job = JobInfo('_data', '_zoo', name='%s_%s' % (ticker, period), target=target)\n        result_df = predict_multiple(job, raw_df=read_df(job.get_source_name()), rows_to_predict=120)\n        result_df.index.names = ['']\n        result_df.plot(title=job.name)\n\n        if train_date is not None:\n          x = train_date\n          y = result_df['True'].min()\n          plt.axvline(x, color='k', linestyle='--')\n          plt.annotate('Training stop', xy=(x, y), xytext=(result_df.index.min(), y), color='k',\n                       arrowprops={'arrowstyle': '->', 'connectionstyle': 'arc3', 'color': 'k'})\n\n  plt.show()\n\n\nif __name__ == '__main__':\n  main()\n"""
models/__init__.py,0,"b""#!/usr/bin/env python\nfrom __future__ import absolute_import\n\n__author__ = 'maxim'\n\ntry:\n  from .cnn_model import ConvModel\nexcept ImportError:\n  ConvModel = None\n\ntry:\n  from .linear_model import LinearModel\nexcept ImportError:\n  LinearModel = None\n\ntry:\n  from .rnn_model import RecurrentModel\nexcept ImportError:\n  RecurrentModel = None\n\ntry:\n  from .nn_model import NeuralNetworkModel\nexcept ImportError:\n  NeuralNetworkModel = None\n\ntry:\n  from .xgboost_model import XgbModel\nexcept ImportError:\n  XgbModel = None\nexcept OSError:\n  XgbModel = None\n"""
models/cnn_model.py,15,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import print_function\n\n__author__ = 'maxim'\n\nimport tensorflow as tf\n\nfrom .nn_ops import COST_FUNCTIONS\nfrom .tensorflow_model import TensorflowModel\n\nclass ConvModel(TensorflowModel):\n  EXPECTS_TIME_PARAM = True\n\n  def __init__(self, **params):\n    super().__init__(**params)\n\n    self._time_steps = params.get('time_steps', 10)\n    self._features = int(self._features / self._time_steps)  # because features are unrolled\n    self._layers = params.get('layers', [])\n    self._dropout = params.get('dropout', 0.0)  # drop probability (not keep!)\n    self._lambda = params.get('lambda', 0.005)\n    self._cost_func = COST_FUNCTIONS[params.get('cost_func', 'l2')]\n\n    self._compile()\n\n  def _compile(self):\n    TensorflowModel._compile(self)\n\n    with tf.Graph().as_default() as self._graph:\n      x = tf.placeholder(tf.float32, shape=[None, self._time_steps * self._features], name='x')\n      y = tf.placeholder(tf.float32, shape=[None], name='y')\n      mode = tf.placeholder(tf.string, name='mode')\n\n      # unroll the features into time-series\n      layer = tf.reshape(x, [-1, self._time_steps, self._features])\n\n      for filters, kernel_size in self._layers:\n        layer = tf.layers.conv1d(layer,\n                                 filters=filters,\n                                 kernel_size=kernel_size,\n                                 strides=1,\n                                 activation=tf.nn.relu)\n      layer = tf.reduce_max(layer, axis=1)\n\n      layer = tf.layers.dropout(layer, rate=self._dropout, training=tf.equal(mode, 'train'))\n      output_layer = tf.layers.dense(layer, units=1,\n                                     activation=tf.nn.elu,\n                                     kernel_initializer=tf.glorot_uniform_initializer(),\n                                     kernel_regularizer=tf.contrib.layers.l2_regularizer(self._lambda),\n                                     name='dense')\n\n      cost = self._cost_func(output_layer, y)\n      optimizer = tf.train.AdamOptimizer(self._learning_rate).minimize(cost)\n\n      init = tf.global_variables_initializer()\n\n    self._x = x\n    self._y = y\n    self._mode = mode\n    self._output = output_layer\n    self._cost = cost\n    self._optimizer = optimizer\n    self._init = init\n"""
models/linear_model.py,0,"b""#!/usr/bin/env python\nfrom __future__ import absolute_import\n\n__author__ = 'maxim'\n\n\nimport os\nimport numpy as np\n\nfrom .model import Model\nfrom util import vlog\n\n\nclass LinearModel(Model):\n  DATA_WITH_BIAS = True\n\n  def __init__(self, **params):\n    Model.__init__(self, **params)\n    self._beta = None\n\n  def fit(self, train):\n    x, y = train.x, train.y\n    self._beta = np.linalg.pinv(x.T.dot(x)).dot(x.T).dot(y)\n\n  def predict(self, x):\n    return x.dot(self._beta)\n\n  def save(self, dest_dir):\n    path = os.path.join(dest_dir, 'beta.npy')\n    vlog('Saving the model to:', path)\n    np.save(path, self._beta)\n\n  def restore(self, source_dir):\n    path = os.path.join(source_dir, 'beta.npy')\n    vlog('Restoring the model from:', path)\n    self._beta = np.load(path)\n"""
models/model.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n__author__ = 'maxim'\n\n\nclass Model(object):\n  DATA_WITH_BIAS = False\n  EXPECTS_TIME_PARAM = False\n\n  def __init__(self, **params):\n    self._features = params['features']\n\n  def session(self):\n    class Dummy:\n      def __enter__(self): pass\n      def __exit__(self, exc_type, exc_val, exc_tb): pass\n    return Dummy()\n\n  def fit(self, train_set):\n    raise NotImplementedError\n\n  def predict(self, x):\n    raise NotImplementedError\n\n  def save(self, dest_dir):\n    pass\n\n  def restore(self, source_dir):\n    pass\n"""
models/nn_model.py,19,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\n__author__ = 'maxim'\n\n\nimport tensorflow as tf\n\nfrom .tensorflow_model import TensorflowModel\nfrom .nn_ops import ACTIVATIONS, COST_FUNCTIONS, dropout, batch_normalization\n\n\nclass NeuralNetworkModel(TensorflowModel):\n  def __init__(self, **params):\n    TensorflowModel.__init__(self, **params)\n\n    self._layers = params.get('layers', [])\n    self._init_sigma = params.get('init_sigma', 0.001)\n    self._lambda = params.get('lambda', 0.005)\n    self._cost_func = COST_FUNCTIONS[params.get('cost_func', 'l2')]\n\n    self._compile()\n\n\n  def _compile(self):\n    TensorflowModel._compile(self)\n\n    with tf.Graph().as_default() as self._graph:\n      x = tf.placeholder(tf.float32, shape=[None, self._features], name='x')\n      y = tf.placeholder(tf.float32, shape=[None], name='y')\n      mode = tf.placeholder(tf.string, name='mode')\n\n      rand_init = lambda shape: tf.random_normal(shape=shape, stddev=self._init_sigma)\n\n      layer = x\n      dimension = self._features\n      reg = 0\n      for idx, layer_params in enumerate(self._layers):\n        with tf.variable_scope('l_%d' % idx):\n          size = layer_params.get('size', 50)\n          W = tf.Variable(rand_init([dimension, size]), name='W%d' % idx)\n          b = tf.Variable(rand_init([size]), name='b%d' % idx)\n          layer = tf.matmul(layer, W) + b\n\n          batchnorm = layer_params.get('batchnorm', False)\n          if batchnorm:\n            batch_normalization(layer, tf.equal(mode, 'train'))\n\n          activation_func = ACTIVATIONS[layer_params.get('activation_func', 'relu')]\n          layer = activation_func(layer)\n\n          dropout_prob = layer_params.get('dropout', 0.5)\n          layer = dropout(layer, tf.equal(mode, 'train'), keep_prob=dropout_prob)\n\n          reg += self._lambda * tf.nn.l2_loss(W)\n          dimension = size\n\n      with tf.variable_scope('l_out'):\n        W_out = tf.Variable(rand_init([dimension, 1]), name='W')\n        b_out = tf.Variable(rand_init([1]), name='b')\n        output_layer = tf.matmul(layer, W_out) + b_out\n        reg += self._lambda * tf.nn.l2_loss(W_out)\n\n      cost = self._cost_func(output_layer, y) + reg\n      optimizer = tf.train.AdamOptimizer(self._learning_rate).minimize(cost)\n\n      init = tf.global_variables_initializer()\n\n    self._x = x\n    self._y = y\n    self._mode = mode\n    self._output = output_layer\n    self._cost = cost\n    self._optimizer = optimizer\n    self._init = init\n"""
models/nn_ops.py,18,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n__author__ = 'maxim'\n\n\nimport tensorflow as tf\nfrom tensorflow.python.training import moving_averages\n\n\ndef leaky_relu(x, alpha=0.1):\n  x = tf.nn.relu(x)\n  m_x = tf.nn.relu(-x)\n  x -= alpha * m_x\n  return x\n\n\ndef prelu(x):\n  shape = x.get_shape()\n  alpha = tf.Variable(initial_value=tf.zeros(shape=shape[1:]), name='alpha')\n  x = tf.nn.relu(x) + tf.multiply(alpha, (x - tf.abs(x))) * 0.5\n  return x\n\n\nACTIVATIONS = {'leaky_relu': leaky_relu, 'prelu': prelu}\nACTIVATIONS.update({name: getattr(tf, name) for name in ['tanh']})\nACTIVATIONS.update({name: getattr(tf.nn, name) for name in ['relu', 'elu', 'sigmoid']})\n\n\nCOST_FUNCTIONS = {\n  'l1': lambda output, y: tf.reduce_mean(tf.abs(output - y)),\n  'l2': lambda output, y: tf.reduce_mean(tf.pow(output - y, 2.0)),\n}\n\n\ndef dropout(incoming, is_training, keep_prob):\n  if keep_prob is None:\n    return incoming\n  return tf.cond(is_training, lambda: tf.nn.dropout(incoming, keep_prob), lambda: incoming)\n\n\ndef batch_normalization(incoming, is_training, beta=0.0, gamma=1.0, epsilon=1e-5, decay=0.9):\n  shape = incoming.get_shape()\n  dimensions_num = len(shape)\n  axis = list(range(dimensions_num - 1))\n\n  with tf.variable_scope('batchnorm'):\n    beta = tf.Variable(initial_value=tf.ones(shape=[shape[-1]]) * beta, name='beta')\n    gamma = tf.Variable(initial_value=tf.ones(shape=[shape[-1]]) * gamma, name='gamma')\n\n    moving_mean = tf.Variable(initial_value=tf.zeros(shape=shape[-1:]), trainable=False, name='moving_mean')\n    moving_variance = tf.Variable(initial_value=tf.zeros(shape=shape[-1:]), trainable=False, name='moving_variance')\n\n  def update_mean_var():\n    mean, variance = tf.nn.moments(incoming, axis)\n    update_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, decay)\n    update_moving_variance = moving_averages.assign_moving_average(moving_variance, variance, decay)\n    with tf.control_dependencies([update_moving_mean, update_moving_variance]):\n      return tf.identity(mean), tf.identity(variance)\n\n  mean, var = tf.cond(is_training, update_mean_var, lambda: (moving_mean, moving_variance))\n  inference = tf.nn.batch_normalization(incoming, mean, var, beta, gamma, epsilon)\n  inference.set_shape(shape)\n  return inference\n"""
models/rnn_model.py,17,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\n\n__author__ = 'maxim'\n\n\nimport tensorflow as tf\n\nfrom .nn_ops import COST_FUNCTIONS\nfrom .tensorflow_model import TensorflowModel\n\n\nCELL_TYPES = {\n  'lstm': tf.nn.rnn_cell.LSTMCell,\n  'gru': tf.nn.rnn_cell.GRUCell\n}\n\n\nclass RecurrentModel(TensorflowModel):\n  EXPECTS_TIME_PARAM = True\n\n  def __init__(self, **params):\n    TensorflowModel.__init__(self, **params)\n\n    self._time_steps = params.get('time_steps', 10)\n    self._features = int(self._features / self._time_steps)  # because features are unrolled\n    self._layers = params.get('layers', [])\n    self._cell_type = params.get('cell_type', 'lstm')\n    self._double_state = params.get('double_state', False)\n    self._dropout = params.get('dropout', 0.0)  # drop probability (not keep!)\n    self._lambda = params.get('lambda', 0.005)\n    self._cost_func = COST_FUNCTIONS[params.get('cost_func', 'l2')]\n\n    self._compile()\n\n\n  def _compile(self):\n    TensorflowModel._compile(self)\n\n    with tf.Graph().as_default() as self._graph:\n      x = tf.placeholder(tf.float32, shape=[None, self._time_steps * self._features], name='x')\n      y = tf.placeholder(tf.float32, shape=[None], name='y')\n      mode = tf.placeholder(tf.string, name='mode')\n\n      # unroll the features into time-series\n      x_series = tf.reshape(x, [-1, self._time_steps, self._features])\n\n      cells = []\n      cell_class = CELL_TYPES[self._cell_type]\n      for layer_size in self._layers:\n        cell = cell_class(num_units=layer_size)\n        cells.append(cell)\n      multi_cell = tf.nn.rnn_cell.MultiRNNCell(cells=cells)\n      outputs, states = tf.nn.dynamic_rnn(multi_cell, x_series, dtype=tf.float32)\n\n      if self._cell_type == 'lstm':\n        top_layer_h_state = states[-1].h\n        top_layer_c_state = states[-1].c\n        if self._double_state:\n          top_layer_state = tf.concat([top_layer_h_state, top_layer_c_state], axis=1)\n        else:\n          top_layer_state = top_layer_h_state\n      elif self._cell_type == 'gru':\n        top_layer_state = states[-1]\n        # double state is not supported for GRU\n      else:\n        # unexpected cell type, but let's expect the standard state value\n        top_layer_state = states[-1]\n\n      dropout_layer = tf.layers.dropout(top_layer_state, rate=self._dropout, training=tf.equal(mode, 'train'))\n      output_layer = tf.layers.dense(dropout_layer, units=1,\n                                     activation=tf.nn.elu,\n                                     kernel_initializer=tf.glorot_uniform_initializer(),\n                                     kernel_regularizer=tf.contrib.layers.l2_regularizer(self._lambda),\n                                     name='dense')\n\n      cost = self._cost_func(output_layer, y)\n      optimizer = tf.train.AdamOptimizer(self._learning_rate).minimize(cost)\n\n      init = tf.global_variables_initializer()\n\n    self._x = x\n    self._y = y\n    self._mode = mode\n    self._output = output_layer\n    self._cost = cost\n    self._optimizer = optimizer\n    self._init = init\n"""
models/tensorflow_model.py,4,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\n__author__ = 'maxim'\n\n\nimport os\n# Suppress initial logging, can be modified later\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport tensorflow as tf\n\nfrom .model import Model\nfrom util import *\n\n\nclass TensorflowModel(Model):\n  def __init__(self, **params):\n    Model.__init__(self, **params)\n\n    self._cpu_only = params.get('cpu_only', False)\n    self._tensorflow_log_level = params.get('tensorflow_log_level', 2)\n\n    self._epochs = params.get('epochs', 80)\n    self._batch_size = params.get('batch_size', 1024)\n    self._learning_rate = params.get('learning_rate', 0.001)\n\n    self._graph = None\n    self._session = None\n    self._init = None\n    self._x = None\n    self._y = None\n    self._mode = None\n    self._output = None\n    self._cost = None\n    self._optimizer = None\n    self._init = None\n\n\n  def _compile(self):\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(self._tensorflow_log_level)\n\n\n  def session(self):\n    assert self._graph is not None\n    config = tf.ConfigProto(device_count={'GPU': 0}) if self._cpu_only else tf.ConfigProto()\n    config.gpu_options.allow_growth = True  # https://github.com/vijayvee/Recursive-neural-networks-TensorFlow/issues/1\n    self._session = tf.Session(graph=self._graph, config=config)\n    return self._session\n\n\n  def fit(self, train):\n    assert self._session is not None\n    debug('Start training')\n    self._session.run(self._init)\n    while train.epochs_completed < self._epochs:\n      batch_x, batch_y = train.next_batch(self._batch_size)\n      _, cost_ = self._session.run([self._optimizer, self._cost],\n                                   feed_dict={self._x: batch_x, self._y: batch_y, self._mode: 'train'})\n      if train.just_completed and train.epochs_completed % 10 == 0:\n        info('Epoch: %2d cost=%.6f' % (train.epochs_completed, cost_))\n    debug('Training completed')\n\n\n  def predict(self, test_x):\n    return self._session.run(self._output, feed_dict={self._x: test_x, self._mode: 'test'}).reshape((-1,))\n\n\n  def save(self, dest_dir):\n    path = os.path.join(dest_dir, 'session.data')\n    saver = tf.train.Saver()\n    saver.save(self._session, path)\n\n\n  def restore(self, source_dir):\n    path = os.path.join(source_dir, 'session.data')\n    saver = tf.train.Saver()\n    saver.restore(self._session, path)\n"""
models/xgboost_model.py,0,"b""#!/usr/bin/env python\n__author__ = 'maxim'\n\n\nimport os\nfrom xgboost import XGBRegressor\nfrom sklearn.externals import joblib\n\nfrom .model import Model\nfrom util import vlog\n\n\nclass XgbModel(Model):\n  def __init__(self, **params):\n    Model.__init__(self, **params)\n    del params['features']\n    self._model = XGBRegressor(**params)\n\n  def fit(self, train):\n    self._model.fit(train.x, train.y)\n\n  def predict(self, x):\n    return self._model.predict(x)\n\n  def save(self, dest_dir):\n    path = os.path.join(dest_dir, 'model.joblib.dat')\n    vlog('Saving the model to:', path)\n    joblib.dump(self._model, path)\n\n  def restore(self, source_dir):\n    path = os.path.join(source_dir, 'model.joblib.dat')\n    vlog('Restoring the model from:', path)\n    self._model = joblib.load(path)\n"""
poloniex/__init__.py,0,"b""#!/usr/bin/env python\nfrom __future__ import absolute_import\n\n__author__ = 'maxim'\n\nfrom .api import AVAILABLE_PERIODS\nfrom .fetch_data import get_all_tickers_list, update_ticker, update_selected, update_all, get_latest_data\n"""
poloniex/api.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# See the full API here:\n# https://poloniex.com/support/api/\n\n__author__ = 'maxim'\n\n\nfrom six import string_types\nimport pandas as pd\n\nfrom util import *\n\n\nAVAILABLE_PERIODS = [300, 900, 1800, 7200, 14400, 86400]\n\n\ndef get_chart_data(pair, start_time, end_time, period):\n  url = 'https://poloniex.com/public?command=returnChartData&currencyPair=%s&start=%d&end=%d&period=%d' % \\\n        (pair, start_time, end_time, period_to_seconds(period))\n  info('Fetching %s: %s' % (pair, url))\n  df = pd.read_json(url, convert_dates=False)\n  info('Fetched %s (%s)' % (pair, period_to_human(period)))\n  return df\n\n\ndef get_24h_volume():\n  url = 'https://poloniex.com/public?command=return24hVolume'\n  info('Fetching %s' % url)\n  return pd.read_json(url)\n\n\ndef period_to_human(period):\n  if isinstance(period, string_types):\n    return period\n  if period == 300:\n    return '5m'\n  if period == 900:\n    return '15m'\n  if period == 1800:\n    return '30m'\n  if period == 7200:\n    return '2h'\n  if period == 14400:\n    return '4h'\n  if period == 86400:\n    return 'day'\n  return str(period)\n\n\ndef period_to_seconds(period):\n  if isinstance(period, int):\n    return period\n  if period == '5m':\n    return 300\n  if period == '15m':\n    return 900\n  if period == '30m':\n    return 1800\n  if period == '2h':\n    return 7200\n  if period == '4h':\n    return 14400\n  if period == 'day':\n    return 86400\n  return int(period)\n"""
poloniex/fetch_data.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\n__author__ = 'maxim'\n\n\nimport os\nimport time\nimport pandas as pd\n\nfrom . import api\nfrom util import *\n\n\nCOLUMNS = ['date', 'high', 'low', 'open', 'close', 'volume', 'quoteVolume', 'weightedAverage']\nPERIODS = ['5m', '15m', '30m', '2h', '4h', 'day']\n\n\ndef get_all_tickers_list():\n  df = api.get_24h_volume()\n  return [ticker for ticker in df.columns if not ticker.startswith('total')]\n\n\ndef update_ticker(ticker, period, dest_dir):\n  path = os.path.join(dest_dir, '%s_%s.csv' % (ticker, api.period_to_human(period)))\n  if os.path.exists(path):\n    existing_df = pd.read_csv(path)\n    start_time = existing_df.tail(2).date.min()\n    debug('Selected start_time=%d' % start_time)\n  else:\n    existing_df = None\n    start_time = 0\n  end_time = 2 ** 32\n\n  new_df = api.get_chart_data(ticker, start_time, end_time, period)\n  if new_df.date.iloc[-1] == 0:\n    warn('Error. No data for %s.' % ticker)\n    return\n\n  if existing_df is not None:\n    assert start_time > 0\n    df = pd.concat([existing_df[existing_df.date < start_time], new_df])\n    df.to_csv(path, index=False, columns=COLUMNS)\n    info('Data frame updated: %s' % path)\n  else:\n    new_df.to_csv(path, index=False, columns=COLUMNS)\n    info('Data frame saved to %s' % path)\n\n\ndef update_selected(tickers, periods=api.AVAILABLE_PERIODS, data_dir='_data', sleep=0.5):\n  if not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\n  info('Fetching the tickers: %s' % tickers)\n  info('Periods: %s' % [api.period_to_human(period) for period in periods])\n\n  for ticker in tickers:\n    for period in periods:\n      update_ticker(ticker, period, dest_dir=data_dir)\n      time.sleep(sleep)\n\n\ndef update_all(**kwargs):\n  tickers = get_all_tickers_list()\n  kwargs['tickers'] = tickers\n  update_selected(**kwargs)\n\n\ndef get_latest_data(ticker, period, depth):\n  now = time.time()\n  now_seconds = int(now)\n  start_time = now_seconds - (depth + 2) * api.period_to_seconds(period)\n  end_time = 2 ** 32\n\n  df = api.get_chart_data(ticker, start_time, end_time, period)\n  if df.date.iloc[-1] == 0:\n    warn('Error. No data for %s.' % ticker)\n    return\n\n  return df\n"""
predict/__init__.py,0,"b""#!/usr/bin/env python\nfrom __future__ import absolute_import\n\n__author__ = 'maxim'\n\nfrom .ensemble import Ensemble, predict_multiple\nfrom .model_io import get_model_info, ModelNotAvailable\n"""
predict/ensemble.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n__author__ = 'maxim'\n\ntry:\n  from itertools import izip as zip\nexcept ImportError:  # python 3.x\n  pass\nfrom itertools import count\nimport os\n\nimport numpy as np\nimport pandas as pd\n\nfrom train.job_info import parse_model_infos\nfrom util import *\nfrom .model_io import get_model_info\n\n\nclass Ensemble(object):\n  def __init__(self, models):\n    self._models = models\n\n  def predict_aggregated(self, df, last_rows=None, reducer=np.mean):\n    debug('Models=%d last_rows=%d' % (len(self._models), last_rows))\n    changes = [Ensemble.predict_changes_for_model(model_info, df, last_rows) for model_info in self._models]\n    changes = np.array(changes)\n    vlog('Predicted changes:', changes.shape)\n    vlog2('Predicted values:\\n', changes[:, :6])\n    return reducer(changes, axis=0)\n\n  @staticmethod\n  def predict_changes_for_model(model_info, df, last_rows=None):\n    run_params = model_info.run_params\n    model = model_info.model_class(**model_info.model_params)\n    x = to_dataset_for_prediction(df, run_params['k'], model_info.model_class.DATA_WITH_BIAS)\n\n    if last_rows is None:\n      assert run_params['k'] <= 100, 'One of the models is using k=%d. Set last rows manually' % run_params['k']\n      last_rows = df.shape[0] - 100   # 100 is max k\n    assert last_rows <= x.shape[0], 'Last rows is too large. Actual rows: %d' % x.shape[0]\n    x = x[-last_rows:]                # take only the last `last_rows` rows\n    vlog('Input for prediction:', x.shape)\n\n    with model.session():\n      model.restore(model_info.path)\n      predicted_changes = model.predict(x)\n      vlog('Predicted:', predicted_changes.shape, ' for model:', model_info.path)\n      vlog2('Predicted values:', predicted_changes[:20])\n      return predicted_changes\n\n  @staticmethod\n  def ensemble_top_models(job_info, top_n=5):\n    home_dir = os.path.join(job_info.zoo_dir, '%s_%s' % (job_info.ticker, job_info.period))\n    models = parse_model_infos(home_dir)\n    models.sort(key=lambda d: d['eval'])\n\n    model_paths = [os.path.join(home_dir, d['name']) for d in models]\n    models = [get_model_info(path, strict=False) for path in model_paths]\n    top_models = [model for model in models if model.is_available()][:top_n]\n    return Ensemble(top_models)\n\n\ndef predict_multiple(job_info, raw_df, rows_to_predict, top_models_num=5):\n  debug('Predicting %s target=%s' % (job_info.name, job_info.target))\n\n  raw_targets = raw_df[job_info.target][-(rows_to_predict + 1):].reset_index(drop=True)\n  changes_df = to_changes(raw_df)\n  target_changes = changes_df[job_info.target][-rows_to_predict:].reset_index(drop=True)\n  dates = changes_df.date[-rows_to_predict:].reset_index(drop=True)\n\n  df = changes_df[:-1]  # the data for models is shifted by one: the target for the last row is unknown\n\n  ensemble = Ensemble.ensemble_top_models(job_info, top_n=top_models_num)\n  predictions = ensemble.predict_aggregated(df, last_rows=rows_to_predict)\n\n  result = []\n  for idx, date, prediction_change, target_change in zip(count(), dates, predictions, target_changes):\n    debug('%%-change on %s: predict=%+.5f target=%+.5f' % (date, prediction_change, target_change))\n\n    # target_change is approx. raw_targets[idx + 1] / raw_targets[idx] - 1.0\n    raw_target = raw_targets[idx + 1]\n    raw_predicted = (1 + prediction_change) * raw_targets[idx]\n    debug('   value on %s: predict= %.5f target= %.5f' % (date, raw_predicted, raw_target))\n\n    result.append({'Time': date, 'Prediction': raw_predicted, 'True': raw_target})\n\n  result_df = pd.DataFrame(result)\n  result_df.set_index('Time', inplace=True)\n  return result_df\n"""
predict/model_io.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n__author__ = 'maxim'\n\n\nimport os\n\n# noinspection PyUnresolvedReferences\nfrom models import *\nfrom util import *\n\n\nclass ModelInfo(object):\n  def __init__(self, path, model_class, model_params, run_params):\n    self.path = path\n    self.model_class = model_class\n    self.model_params = model_params\n    self.run_params = run_params\n\n  def is_available(self):\n    return self.model_class is not None\n\n  def __repr__(self):\n    return repr({'path': self.path, 'class': self.model_class})\n\n\ndef get_model_info(path, strict=True):\n  model_params = _read_dict(os.path.join(path, 'model-params.txt'))\n  run_params = _read_dict(os.path.join(path, 'run-params.txt'))\n  model_class = run_params['model_class']\n  resolved_class = globals()[model_class]\n  if strict and resolved_class is None:\n    raise ModelNotAvailable(model_class)\n  return ModelInfo(path, resolved_class, model_params, run_params)\n\n\ndef _read_dict(path):\n  with open(path, 'r') as file_:\n    content = file_.read()\n    return str_to_obj(content)\n\n\nclass ModelNotAvailable(BaseException):\n  def __init__(self, model_class, *args):\n    super(ModelNotAvailable, self).__init__(*args)\n    self.model_class = model_class\n"""
train/__init__.py,0,"b""#!/usr/bin/env python\nfrom __future__ import absolute_import\n\n__author__ = 'maxim'\n\nfrom .job_info import JobInfo\nfrom .job_runner import JobRunner\n"""
train/evaluator.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n__author__ = 'maxim'\n\n\nimport numpy as np\nimport pandas as pd\n\n\nclass Evaluator(object):\n  def __init__(self, risk_factor=1.0):\n    super(Evaluator, self).__init__()\n    self._risk_factor = risk_factor\n\n  def eval(self, model, test_set):\n    prediction = model.predict(test_set.x)\n    stats = self._compute_stats(prediction, test_set.y)\n    return self._evaluate(stats), stats\n\n  def stats_str(self, stats):\n    return 'Mean absolute error: %.6f\\n' % stats['mae'] + \\\n           'SD absolute error:   %.6f\\n' % stats['stdae'] + \\\n           'Sign accuracy:       %.6f\\n' % stats['sign_accuracy'] + \\\n           'Mean squared error:  %.6f\\n' % stats['mse'] + \\\n           'Sqrt of MSE:         %.6f\\n' % stats['sqrt_mse'] + \\\n           'Mean error:          %.6f\\n' % stats['me'] + \\\n           'Residuals stats:     %s\\n' % _series_stats(stats['raw_residuals']) + \\\n           'Relative residuals:  %s\\n' % _series_stats(stats['rel_residuals'])\n\n  def _compute_stats(self, prediction, truth):\n    residuals = np.abs(prediction - truth)\n    return {\n      'mae': np.mean(residuals),\n      'stdae': np.std(residuals),\n      'sign_accuracy': np.mean(np.equal(np.sign(prediction), np.sign(truth))),\n      'mse': np.mean(np.power(prediction - truth, 2.0)),\n      'sqrt_mse': np.mean(np.power(prediction - truth, 2.0)) ** 0.5,\n      'me': np.mean(prediction - truth),\n      'raw_residuals': residuals,\n      'rel_residuals': residuals / np.maximum(np.abs(truth), 1e-3),\n    }\n\n  def _evaluate(self, stats):\n    return stats['mae'] + self._risk_factor * stats['stdae']\n\n\ndef _series_stats(series):\n  stats = pd.Series(series).describe(percentiles=[0.25, 0.5, 0.75, 0.9])\n  return 'mean=%.4f std=%.4f percentile=[0%%=%.4f 25%%=%.4f 50%%=%.4f 75%%=%.4f 90%%=%.4f 100%%=%.4f]' % \\\n         (stats['mean'], stats['std'], stats['min'], stats['25%'], stats['50%'], stats['75%'], stats['90%'], stats['max'])\n"""
train/job_info.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n__author__ = 'maxim'\n\n\nimport os\nimport re\n\n\nclass JobInfo(object):\n  def __init__(self, data_dir, zoo_dir, name, target):\n    self.data_dir = data_dir\n    self.zoo_dir = zoo_dir\n    self.name = name\n    self.ticker, self.period = _split_name(name)\n    self.target = target\n\n  def as_run_params(self):\n    return {\n      'name': self.name,\n      'ticker': self.ticker,\n      'period': self.period,\n      'target': self.target,\n    }\n\n  def get_source_name(self):\n    return os.path.join(self.data_dir, '%s.csv' % self.name)\n\n  def get_dest_name(self, eval_, k, id_=None):\n    suffix = '_' + id_ if id_ else ''\n    return os.path.join(self.zoo_dir, self.name, '%s_eval=%.4f_k=%d%s' % (self.target, eval_, k, suffix))\n\n  def get_current_eval_results(self):\n    directory = os.path.join(self.zoo_dir, self.name)\n    return parse_eval(directory, lambda info: info['target'] == self.target)\n\n\ndef _split_name(name):\n  idx = name.rindex('_')\n  return name[:idx], name[idx+1:]\n\n\ndef parse_model_infos(directory):\n  if os.path.exists(directory):\n    files = os.listdir(directory)\n    return [_parse_model_file(file_name) for file_name in files]\n  return []\n\n\ndef parse_eval(directory, accept):\n  infos = parse_model_infos(directory)\n  return [info['eval'] for info in infos if info if accept(info)]\n\n\ndef _parse_model_file(file_name):\n  if not '=' in file_name:\n    return {}\n\n  match = re.match('([a-z]+)_eval=([0-9.]+)_k=([0-9]+)(_\\w*)?', file_name)\n  if not match:\n    return {\n      'name': file_name,\n    }\n\n  return {\n    'name': file_name,\n    'target': match.group(1),\n    'eval': float(match.group(2)),\n    'k': int(match.group(3)),\n  }\n"""
train/job_runner.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\n__author__ = \'maxim\'\n\n\nimport os\n\nfrom .evaluator import Evaluator\nfrom util import *\n\n\nclass JobRunner(object):\n  def __init__(self, job_info, limit):\n    raw_df = read_df(job_info.get_source_name())\n    changes_df = to_changes(raw_df)\n\n    self._job_info = job_info\n    self._changes_df = changes_df\n    self._min_eval = _resolve_limit(limit, job_info)\n    self._min_params = None\n\n\n  def single_run(self, **params):\n    model_class = params[\'model_class\']\n    with_bias = model_class.DATA_WITH_BIAS\n    expects_k = model_class.EXPECTS_TIME_PARAM\n\n    data_set = to_dataset(self._changes_df, k=params[\'k\'], target_column=params[\'target\'], with_bias=with_bias)\n    train, test = split_dataset(data_set)\n\n    adapted = params.copy()\n    adapted[\'model_class\'] = params[\'model_class\'].__name__\n    info(\'Params=%s\' % smart_str(adapted))\n\n    model_params = params[\'model_params\']\n    model_params[\'features\'] = int(train.x.shape[1])\n    if expects_k:\n      model_params[\'time_steps\'] = params[\'k\']\n\n    run_params = {key: adapted[key] for key in [\'k\', \'model_class\']}\n    run_params.update(self._job_info.as_run_params())\n\n    model = model_class(**model_params)\n    evaluator = Evaluator(*params.get(\'eval_params\', {}))\n\n    with model.session():\n      model.fit(train)\n      train_eval, train_stats = evaluator.eval(model, train)\n      train_stats_str = evaluator.stats_str(train_stats)\n      debug(\'Train results:\\n\', train_stats_str)\n\n      test_eval, test_stats = evaluator.eval(model, test)\n      test_stats_str = evaluator.stats_str(test_stats)\n      is_record = test_eval < self._min_eval\n      marker = \' !!!\' if is_record else \'\'\n      info(\'Test results:\\n%sEval=%.6f%s\\n\' % (test_stats_str, test_eval, marker))\n\n      if is_record:\n        self._min_eval = test_eval\n        self._min_params = params\n\n        dest_dir = self._job_info.get_dest_name(test_eval, params[\'k\'])\n        while os.path.exists(dest_dir):\n          dest_dir = self._job_info.get_dest_name(test_eval, params[\'k\'], random_id(4))\n        os.makedirs(dest_dir)\n\n        model.save(dest_dir)\n        _save_to(dest_dir, \'stats.txt\', \'\\n\'.join([\'# Train results:\', train_stats_str, \'# Test results:\', test_stats_str]))\n        _save_to(dest_dir, \'model-params.txt\', smart_str(model_params))\n        _save_to(dest_dir, \'run-params.txt\', smart_str(run_params))\n        debug(\'Model saved to %s\' % dest_dir)\n\n\n  def iterate(self, iterations, params_fun):\n    for i in range(iterations):\n      info(\'Iteration %s: %s #%d\' % (self._job_info.name, self._job_info.target, i + 1))\n      params = params_fun()\n      self.single_run(**params)\n\n\n  def print_result(self):\n    if self._min_params is None:\n      info(\'Nothing found...\\n\')\n    else:\n      info(\'*** Best result: ***\\n\' + \\\n           \'Eval=%.5f\\n\' % self._min_eval + \\\n           \'Params=%s\\n\' % str(self._min_params))\n\n\ndef _save_to(dest_dir, name, data):\n  path = os.path.join(dest_dir, name)\n  with open(path, \'w\') as file_:\n    file_.write(data)\n    info(\'Data saved: %s\' % path)\n\n\ndef _resolve_limit(limit, job_info):\n  if not callable(limit):\n    info(\'Using the hard limit=%.5f\' % limit)\n    return limit\n\n  results = job_info.get_current_eval_results()\n  if results:\n    info(\'Auto-detected current model results for %s: %s\' % (job_info.name, results))\n    value = limit(results)\n    info(\'Using the limit=%.5f (computed by ""%s"" function)\' % (value, limit.__name__))\n    return value\n\n  info(\'Using the default limit=1.0\')\n  return 1.0\n\n\ndef random_id(size):\n  import string, random\n  chars = string.ascii_letters + string.digits\n  return \'\'.join(random.choice(chars) for _ in range(size))\n'"
util/__init__.py,0,"b""#!/usr/bin/env python\nfrom __future__ import absolute_import\n\n__author__ = 'maxim'\n\nfrom .cmdline import parse_command_line\nfrom .collection_util import smart_str, str_to_obj\nfrom .data_set import DataSet\nfrom .data_util import read_df, to_changes, to_dataset, to_dataset_for_prediction, split_dataset\nfrom .logging import debug, info, warn, vlog, vlog2, vlog3\n"""
util/cmdline.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n__author__ = \'maxim\'\n\n\nimport re\nimport sys\n\nfrom .logging import debug, info, warn\n\n\nDEFAULT_TICKERS = [\'BTC_ETH\', \'BTC_LTC\', \'BTC_XRP\', \'BTC_ZEC\']\nDEFAULT_PERIODS = [\'day\']\nDEFAULT_TARGETS = [\'high\']\n\n\ndef parse_command_line(default_tickers=DEFAULT_TICKERS,\n                       default_periods=DEFAULT_PERIODS,\n                       default_targets=DEFAULT_TARGETS):\n  args = sys.argv[1:]\n  options = [arg[2:].split(\'=\') for arg in args if arg.startswith(\'--\')]\n  args = [arg for arg in args if not arg.startswith(\'--\')]\n  debug(\'Parsing command line arguments:\', args)\n\n  # local import to avoid cyclic dependencies\n  from poloniex.fetch_data import COLUMNS, PERIODS\n  tickers = get_tickers(args, default_tickers)\n  periods = parse_option(\'period\', options, PERIODS, default_periods)\n  targets = parse_option(\'target\', options, COLUMNS, default_targets)\n  return tickers, periods, targets\n\n\ndef get_tickers(args, default=DEFAULT_TICKERS):\n  if not args:\n    if not default:\n      warn(\'No tickers provided. Example usage: ./runner.py BTC_ETH\')\n      return default\n    info(\'Hint: you can provide the tickers in the arguments, like ./runner.py BTC_ETH BTC_LTC\')\n    info(\'Using default tickers: %s\\n\' % pretty_list(default))\n    return default\n  for arg in args:\n    if not re.match(r\'[A-Z]{2,5}_[A-Z]{2,5}\', arg):\n      warn(\'Warning: the argument ""%s"" doesn\\\'t look like a ticker. Example: BTC_ETH\' % arg)\n  info(\'Using tickers: %s\\n\' % pretty_list(args))\n  return args\n\n\ndef parse_option(name, options, valid, default):\n  for option in options:\n    if len(option) == 2 and option[0] == name:\n      values = [value for value in option[1].split(\',\') if value in valid]\n      if values:\n        info(\'Using %s: %s\' % (name, pretty_list(values)))\n        return values\n      else:\n        warn(\'No valid %s specified. Using default: %s\' % (name, pretty_list(default)))\n  return default\n\n\ndef pretty_list(values):\n  if not values:\n    return \'<empty list>\'\n  return \', \'.join([\'""%s""\' % value for value in values])\n'"
util/collection_util.py,0,"b""#!/usr/bin/env python\n__author__ = 'maxim'\n\n\nimport numpy as np\n\n\ndef smart_str(val):\n  if type(val) in [float, np.float32, np.float64] and val:\n    return '%.6f' % val if abs(val) > 1e-6 else '%e' % val\n  if type(val) == dict:\n    return '{%s}' % ', '.join(['%s: %s' % (repr(k), smart_str(val[k])) for k in sorted(val.keys())])\n  if type(val) in [list, tuple]:\n    return '[%s]' % ', '.join(['%s' % smart_str(i) for i in val])\n  return repr(val)\n\n\ndef str_to_obj(s):\n  import ast\n  return ast.literal_eval(s)\n"""
util/data_set.py,0,"b'#!/usr/bin/env python\n__author__ = \'maxim\'\n\n\nimport numpy as np\n\nclass DataSet(object):\n  def __init__(self, x, y):\n    x = np.array(x)\n    y = np.array(y)\n    assert x.shape[0] == y.shape[0]\n\n    self.size = x.shape[0]\n    self.x = x\n    self.y = y\n    self.step = 0\n    self.epochs_completed = 0\n    self.index_in_epoch = 0\n    self.just_completed = False\n\n  @property\n  def index(self):\n    return self.epochs_completed * self.size + self.index_in_epoch\n\n  def reset_counters(self):\n    self.step = 0\n    self.epochs_completed = 0\n    self.index_in_epoch = 0\n\n  def next_batch(self, batch_size):\n    """"""Return the next `batch_size` examples from this data set.""""""\n    if self.just_completed:\n      permutation = np.arange(self.size)\n      np.random.shuffle(permutation)\n      self.x = self.x[permutation]\n      self.y = self.y[permutation]\n\n    self.step += 1\n    start = self.index_in_epoch\n    self.index_in_epoch += batch_size\n    end = min(self.index_in_epoch, self.size)\n    if self.index_in_epoch >= self.size:\n      self.index_in_epoch = 0\n    self.just_completed = end == self.size\n    self.epochs_completed += int(self.just_completed)\n    return self.x[start:end], self.y[start:end]\n'"
util/data_util.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n__author__ = 'maxim'\n\n\nimport datetime\nimport numpy as np\nimport pandas as pd\n\nfrom .data_set import DataSet\nfrom .logging import vlog\n\npd.set_option('display.expand_frame_repr', False)\n\n\ndef read_df(file_name):\n  df = pd.read_csv(file_name)\n  df.date = pd.to_datetime(df.date * 1000, unit='ms')\n  return df\n\n\ndef to_changes(raw):\n  if raw.date.dtype == np.int64:\n    raw.date = pd.to_datetime(raw.date * 1000, unit='ms')\n  return pd.DataFrame({\n    'date': raw.date,\n    'time': raw.date.astype(datetime.datetime).apply(lambda val: seconds(val) / (60*60*24)),\n    'high': raw.high.pct_change(),\n    'low': raw.low.pct_change(),\n    'open': raw.open.pct_change(),\n    'close': raw.close.pct_change(),\n    'vol': raw.volume.replace({0: 1e-5}).pct_change(),\n    'avg': raw.weightedAverage.pct_change(),\n  }, columns=['date', 'time', 'high', 'low', 'open', 'close', 'vol', 'avg'])\n\n\ndef seconds(datetime_):\n  return (datetime_ - datetime_.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds()\n\n\ndef to_dataset(df, k, target_column, with_bias):\n  df = df[1:].reset_index(drop=True)\n  df = df.drop(['date'], axis=1)\n  target = df[target_column]\n\n  n, cols = df.shape\n  windows_num = n - k  # effective window size, including the label, is k + 1\n\n  x = np.empty([windows_num, k * cols + int(with_bias)])\n  y = np.empty([windows_num])\n\n  for i in range(windows_num):\n    window = df[i:i+k]\n    row = window.as_matrix().reshape((-1,))\n    if with_bias:\n      row = np.insert(row, 0, 1)\n    x[i] = row\n    y[i] = target[i+k]\n\n  vlog('Data set: x=%s y=%s' % (x.shape, y.shape))\n  return DataSet(x, y)\n\n\ndef to_dataset_for_prediction(df, k, with_bias):\n  df = df[1:].reset_index(drop=True)\n  df = df.drop(['date'], axis=1)\n\n  n, cols = df.shape\n  windows_num = n - k + 1\n  x = np.empty([windows_num, k * cols + int(with_bias)])\n\n  for i in range(windows_num):\n    window = df[i:i+k]\n    row = window.as_matrix().reshape((-1,))\n    if with_bias:\n      row = np.insert(row, 0, 1)\n    x[i] = row\n\n  vlog('Data set for prediction:', x.shape)\n  return x\n\n\ndef split_dataset(dataset, ratio=None):\n  size = dataset.size\n  if ratio is None:\n    ratio = _choose_optimal_train_ratio(size)\n\n  mask = np.zeros(size, dtype=np.bool_)\n  train_size = int(size * ratio)\n  mask[:train_size] = True\n  np.random.shuffle(mask)\n\n  train_x = dataset.x[mask, :]\n  train_y = dataset.y[mask]\n\n  mask = np.invert(mask)\n  test_x = dataset.x[mask, :]\n  test_y = dataset.y[mask]\n\n  return DataSet(train_x, train_y), DataSet(test_x, test_y)\n\n\ndef _choose_optimal_train_ratio(size):\n  if size > 100000: return 0.95\n  if size > 50000:  return 0.9\n  if size > 20000:  return 0.875\n  if size > 10000:  return 0.85\n  if size > 7500:   return 0.825\n  if size > 5000:   return 0.8\n  if size > 3000:   return 0.775\n  if size > 2000:   return 0.75\n  if size > 1000:   return 0.7\n  return 0.7\n"""
util/logging.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import print_function\n\n__author__ = 'maxim'\n\n\nLOG_LEVEL = 1\n\ndef log(*msg, **kwargs):\n  import datetime\n  sep = kwargs.get('sep', ' ')\n  print('[%s]' % datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), sep.join([str(it) for it in msg]))\n\ndef set_silence():\n  global LOG_LEVEL\n  LOG_LEVEL = 10\n\ndef set_verbose(level=1):\n  global LOG_LEVEL\n  LOG_LEVEL = -level\n\ndef is_debug_logged():\n  return LOG_LEVEL <= 0\n\ndef is_info_logged():\n  return LOG_LEVEL <= 1\n\ndef debug(*msg, **kwargs):\n  log_at_level(0, *msg, **kwargs)\n\ndef info(*msg, **kwargs):\n  log_at_level(1, *msg, **kwargs)\n\ndef warn(*msg, **kwargs):\n  log_at_level(2, *msg, **kwargs)\n\ndef vlog(*msg, **kwargs):\n  log_at_level(-1, *msg, **kwargs)\n\ndef vlog2(*msg, **kwargs):\n  log_at_level(-2, *msg, **kwargs)\n\ndef vlog3(*msg, **kwargs):\n  log_at_level(-3, *msg, **kwargs)\n\ndef log_at_level(level, *msg, **kwargs):\n  if level >= LOG_LEVEL:\n    log(*msg, **kwargs)\n"""
