file_path,api_count,code
cnn.py,63,"b'# -*- coding: utf-8 -*-\n\n##########################################################\n#\n# Attention-based Convolutional Neural Network\n#   for Multi-label Multi-instance Learning\n#\n#\n#   Note: this implementation is mostly based on\n#   https://github.com/yuhaozhang/sentence-convnet/blob/master/model.py\n#\n##########################################################\n\nimport tensorflow as tf\n\n# model parameters\ntf.app.flags.DEFINE_integer(\'batch_size\', 100, \'Training batch size\')\ntf.app.flags.DEFINE_integer(\'emb_size\', 300, \'Size of word embeddings\')\ntf.app.flags.DEFINE_integer(\'num_kernel\', 100, \'Number of filters for each window size\')\ntf.app.flags.DEFINE_integer(\'min_window\', 3, \'Minimum size of filter window\')\ntf.app.flags.DEFINE_integer(\'max_window\', 5, \'Maximum size of filter window\')\ntf.app.flags.DEFINE_integer(\'vocab_size\', 40000, \'Vocabulary size\')\ntf.app.flags.DEFINE_integer(\'num_classes\', 10, \'Number of class to consider\')\ntf.app.flags.DEFINE_integer(\'sent_len\', 400, \'Input sentence length.\')\ntf.app.flags.DEFINE_float(\'l2_reg\', 1e-4, \'l2 regularization weight\')\ntf.app.flags.DEFINE_boolean(\'attention\', False, \'Whether use attention or not\')\ntf.app.flags.DEFINE_boolean(\'multi_label\', False, \'Multilabel or not\')\n\n\ndef _variable_on_cpu(name, shape, initializer):\n    with tf.device(\'/cpu:0\'):\n        var = tf.get_variable(name, shape, initializer=initializer)\n    return var\n\ndef _variable_with_weight_decay(name, shape, initializer, wd):\n    var = _variable_on_cpu(name, shape, initializer)\n    if wd is not None and wd != 0.:\n        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name=\'weight_loss\')\n    else:\n        weight_decay = tf.constant(0.0, dtype=tf.float32)\n    return var, weight_decay\n\n\ndef _auc_pr(true, prob, threshold):\n    pred = tf.where(prob > threshold, tf.ones_like(prob), tf.zeros_like(prob))\n    tp = tf.logical_and(tf.cast(pred, tf.bool), tf.cast(true, tf.bool))\n    fp = tf.logical_and(tf.cast(pred, tf.bool), tf.logical_not(tf.cast(true, tf.bool)))\n    fn = tf.logical_and(tf.logical_not(tf.cast(pred, tf.bool)), tf.cast(true, tf.bool))\n    pre = tf.truediv(tf.reduce_sum(tf.cast(tp, tf.int32)), tf.reduce_sum(tf.cast(tf.logical_or(tp, fp), tf.int32)))\n    rec = tf.truediv(tf.reduce_sum(tf.cast(tp, tf.int32)), tf.reduce_sum(tf.cast(tf.logical_or(tp, fn), tf.int32)))\n    return pre, rec\n\n\nclass Model(object):\n\n    def __init__(self, config, is_train=True):\n        self.is_train = is_train\n        self.emb_size = config[\'emb_size\']\n        self.batch_size = config[\'batch_size\']\n        self.num_kernel = config[\'num_kernel\']\n        self.min_window = config[\'min_window\']\n        self.max_window = config[\'max_window\']\n        self.vocab_size = config[\'vocab_size\']\n        self.num_classes = config[\'num_classes\']\n        self.sent_len = config[\'sent_len\']\n        self.l2_reg = config[\'l2_reg\']\n        self.multi_instance = config[\'attention\']\n        self.multi_label = config[\'multi_label\']\n        if is_train:\n            self.optimizer = config[\'optimizer\']\n            self.dropout = config[\'dropout\']\n        self.build_graph()\n\n    def build_graph(self):\n        """""" Build the computation graph. """"""\n        self._inputs = tf.placeholder(dtype=tf.int64, shape=[None, self.sent_len], name=\'input_x\')\n        self._labels = tf.placeholder(dtype=tf.float32, shape=[None, self.num_classes], name=\'input_y\')\n        self._attention = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\'attention\')\n        losses = []\n\n        # lookup layer\n        with tf.variable_scope(\'embedding\') as scope:\n            self._W_emb = _variable_on_cpu(name=\'embedding\', shape=[self.vocab_size, self.emb_size],\n                                           initializer=tf.random_uniform_initializer(minval=-1.0, maxval=1.0))\n            # sent_batch is of shape: (batch_size, sent_len, emb_size, 1), in order to use conv2d\n            sent_batch = tf.nn.embedding_lookup(params=self._W_emb, ids=self._inputs)\n            sent_batch = tf.expand_dims(sent_batch, -1)\n\n        # conv + pooling layer\n        pool_tensors = []\n        for k_size in range(self.min_window, self.max_window+1):\n            with tf.variable_scope(\'conv-%d\' % k_size) as scope:\n                kernel, wd = _variable_with_weight_decay(\n                    name=\'kernel-%d\' % k_size,\n                    shape=[k_size, self.emb_size, 1, self.num_kernel],\n                    initializer=tf.truncated_normal_initializer(stddev=0.01),\n                    wd=self.l2_reg)\n                losses.append(wd)\n                conv = tf.nn.conv2d(input=sent_batch, filter=kernel, strides=[1,1,1,1], padding=\'VALID\')\n                biases = _variable_on_cpu(name=\'bias-%d\' % k_size,\n                                          shape=[self.num_kernel],\n                                          initializer=tf.constant_initializer(0.0))\n                bias = tf.nn.bias_add(conv, biases)\n                activation = tf.nn.relu(bias, name=scope.name)\n                # shape of activation: [batch_size, conv_len, 1, num_kernel]\n                conv_len = activation.get_shape()[1]\n                pool = tf.nn.max_pool(activation, ksize=[1,conv_len,1,1], strides=[1,1,1,1], padding=\'VALID\')\n                # shape of pool: [batch_size, 1, 1, num_kernel]\n                pool_tensors.append(pool)\n\n        # Combine all pooled tensors\n        num_filters = self.max_window - self.min_window + 1\n        pool_size = num_filters * self.num_kernel\n        pool_layer = tf.concat(pool_tensors, num_filters, name=\'pool\')\n        pool_flat = tf.reshape(pool_layer, [-1, pool_size])\n\n        # drop out layer\n        if self.is_train and self.dropout > 0:\n            pool_dropout = tf.nn.dropout(pool_flat, 1 - self.dropout)\n        else:\n            pool_dropout = pool_flat\n\n        # fully-connected layer\n        with tf.variable_scope(\'output\') as scope:\n            W, wd = _variable_with_weight_decay(\'W\', shape=[pool_size, self.num_classes],\n                                                initializer=tf.truncated_normal_initializer(stddev=0.05),\n                                                wd=self.l2_reg)\n            losses.append(wd)\n            biases = _variable_on_cpu(\'bias\', shape=[self.num_classes],\n                                      initializer=tf.constant_initializer(0.01))\n            self.logits = tf.nn.bias_add(tf.matmul(pool_dropout, W), biases, name=\'logits\')\n\n        # loss\n        with tf.variable_scope(\'loss\') as scope:\n            if self.multi_label:\n                cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self._labels,\n                                                                        name=\'cross_entropy_per_example\')\n            else:\n                cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self._labels,\n                                                                        name=\'cross_entropy_per_example\')\n\n            if self.is_train and self.multi_instance:   # apply attention\n                cross_entropy_loss = tf.reduce_sum(tf.multiply(cross_entropy, self._attention),\n                                                   name=\'cross_entropy_loss\')\n            else:\n                cross_entropy_loss = tf.reduce_mean(cross_entropy, name=\'cross_entropy_loss\')\n\n            losses.append(cross_entropy_loss)\n            self._total_loss = tf.add_n(losses, name=\'total_loss\')\n\n        # eval with precision-recall\n        with tf.variable_scope(\'evaluation\') as scope:\n            precision = []\n            recall = []\n            for threshold in range(10, -1, -1):\n                pre, rec = _auc_pr(self._labels, tf.sigmoid(self.logits), threshold * 0.1)\n                precision.append(pre)\n                recall.append(rec)\n            self._eval_op = zip(precision, recall)\n\n            # f1 score on threshold=0.5\n            #self._f1_score = tf.truediv(tf.mul(tf.constant(2.0, dtype=tf.float64),\n            #                                 tf.mul(precision[5], recall[5])), tf.add(precision, recall))\n\n        # train on a batch\n        self._lr = tf.Variable(0.0, trainable=False)\n        if self.is_train:\n            if self.optimizer == \'adadelta\':\n                opt = tf.train.AdadeltaOptimizer(self._lr)\n            elif self.optimizer == \'adagrad\':\n                opt = tf.train.AdagradOptimizer(self._lr)\n            elif self.optimizer == \'adam\':\n                opt = tf.train.AdamOptimizer(self._lr)\n            elif self.optimizer == \'sgd\':\n                opt = tf.train.GradientDescentOptimizer(self._lr)\n            else:\n                raise ValueError(""Optimizer not supported."")\n            grads = opt.compute_gradients(self._total_loss)\n            self._train_op = opt.apply_gradients(grads)\n\n            for var in tf.trainable_variables():\n                tf.summary.histogram(var.op.name, var)\n        else:\n            self._train_op = tf.no_op()\n\n        return\n\n    @property\n    def inputs(self):\n        return self._inputs\n\n    @property\n    def labels(self):\n        return self._labels\n\n    @property\n    def attention(self):\n        return self._attention\n\n    @property\n    def lr(self):\n        return self._lr\n\n    @property\n    def train_op(self):\n        return self._train_op\n\n    @property\n    def total_loss(self):\n        return self._total_loss\n\n    @property\n    def eval_op(self):\n        return self._eval_op\n\n    @property\n    def scores(self):\n        return tf.sigmoid(self.logits)\n\n    @property\n    def W_emb(self):\n        return self._W_emb\n\n    def assign_lr(self, session, lr_value):\n        session.run(tf.assign(self.lr, lr_value))\n\n    def assign_embedding(self, session, pretrained):\n        session.run(tf.assign(self.W_emb, pretrained))\n'"
cnn_context.py,77,"b'# -*- coding: utf-8 -*-\n\n##########################################################\n#\n# Attention-based Convolutional Neural Network\n#   for Context-wise Learning\n#\n#\n#   Note: this implementation is mostly based on\n#   https://github.com/yuhaozhang/sentence-convnet/blob/master/model.py\n#\n##########################################################\n\nimport tensorflow as tf\n\n# model parameters\ntf.app.flags.DEFINE_integer(\'batch_size\', 100, \'Training batch size\')\ntf.app.flags.DEFINE_integer(\'emb_size\', 300, \'Size of word embeddings\')\ntf.app.flags.DEFINE_integer(\'num_kernel\', 100, \'Number of filters for each window size\')\ntf.app.flags.DEFINE_integer(\'min_window\', 3, \'Minimum size of filter window\')\ntf.app.flags.DEFINE_integer(\'max_window\', 5, \'Maximum size of filter window\')\ntf.app.flags.DEFINE_integer(\'vocab_size\', 40000, \'Vocabulary size\')\ntf.app.flags.DEFINE_integer(\'num_classes\', 10, \'Number of class to consider\')\ntf.app.flags.DEFINE_integer(\'sent_len\', 400, \'Input sentence length.\')\ntf.app.flags.DEFINE_float(\'l2_reg\', 1e-4, \'l2 regularization weight\')\ntf.app.flags.DEFINE_boolean(\'attention\', False, \'Whether use attention or not\')\ntf.app.flags.DEFINE_boolean(\'multi_label\', False, \'Multilabel or not\')\n\n\ndef _variable_on_cpu(name, shape, initializer):\n    with tf.device(\'/cpu:0\'):\n        var = tf.get_variable(name, shape, initializer=initializer)\n    return var\n\n\ndef _variable_with_weight_decay(name, shape, initializer, wd):\n    var = _variable_on_cpu(name, shape, initializer)\n    if wd is not None and wd != 0.:\n        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name=\'weight_loss\')\n    else:\n        weight_decay = tf.constant(0.0, dtype=tf.float32)\n    return var, weight_decay\n\n\ndef _auc_pr(true, prob, threshold):\n    pred = tf.where(prob > threshold, tf.ones_like(prob), tf.zeros_like(prob))\n    tp = tf.logical_and(tf.cast(pred, tf.bool), tf.cast(true, tf.bool))\n    fp = tf.logical_and(tf.cast(pred, tf.bool), tf.logical_not(tf.cast(true, tf.bool)))\n    fn = tf.logical_and(tf.logical_not(tf.cast(pred, tf.bool)), tf.cast(true, tf.bool))\n    pre = tf.truediv(tf.reduce_sum(tf.cast(tp, tf.int32)),\n                     tf.reduce_sum(tf.cast(tf.logical_or(tp, fp), tf.int32)))\n    rec = tf.truediv(tf.reduce_sum(tf.cast(tp, tf.int32)),\n                     tf.reduce_sum(tf.cast(tf.logical_or(tp, fn), tf.int32)))\n    return pre, rec\n\n\nclass Model(object):\n\n    def __init__(self, config, is_train=True):\n        self.is_train = is_train\n        self.emb_size = config[\'emb_size\']\n        self.batch_size = config[\'batch_size\']\n        self.num_kernel = config[\'num_kernel\']\n        self.min_window = config[\'min_window\']\n        self.max_window = config[\'max_window\']\n        self.vocab_size = config[\'vocab_size\']\n        self.num_classes = config[\'num_classes\']\n        self.sent_len = config[\'sent_len\']\n        self.l2_reg = config[\'l2_reg\']\n        self.multi_instance = config[\'attention\']\n        self.multi_label = config[\'multi_label\']\n        if is_train:\n            self.optimizer = config[\'optimizer\']\n            self.dropout = config[\'dropout\']\n        self.build_graph()\n\n    def conv_layer(self, input, context):\n        pool_tensors = []\n        losses = []\n        for k_size in range(self.min_window, self.max_window+1):\n            with tf.variable_scope(\'conv-%d-%s\' % (k_size, context)) as scope:\n                kernel, wd = _variable_with_weight_decay(\n                    name=\'kernel-%d-%s\' % (k_size, context),\n                    shape=[k_size, self.emb_size, 1, self.num_kernel],\n                    initializer=tf.truncated_normal_initializer(stddev=0.01),\n                    wd=self.l2_reg)\n                losses.append(wd)\n                conv = tf.nn.conv2d(input=input, filter=kernel, strides=[1,1,1,1], padding=\'VALID\')\n                biases = _variable_on_cpu(\'bias-%d-%s\' % (k_size, context),\n                                          [self.num_kernel], tf.constant_initializer(0.0))\n                bias = tf.nn.bias_add(conv, biases)\n                activation = tf.nn.relu(bias, name=scope.name)\n                # shape of activation: [batch_size, conv_len, 1, num_kernel]\n                conv_len = activation.get_shape()[1]\n                pool = tf.nn.max_pool(activation, ksize=[1,conv_len,1,1], strides=[1,1,1,1], padding=\'VALID\')\n                # shape of pool: [batch_size, 1, 1, num_kernel]\n                pool_tensors.append(pool)\n\n        # Combine pooled tensors\n        num_filters = self.max_window - self.min_window + 1\n        pool_size = num_filters * self.num_kernel   # 300\n        pool_layer = tf.concat(pool_tensors, num_filters, name=\'pool-%s\' % context)\n        pool_flat = tf.reshape(pool_layer, [-1, pool_size])\n\n        return losses, pool_flat\n\n    def build_graph(self):\n        """""" Build the computation graph. """"""\n        self._left = tf.placeholder(dtype=tf.int64, shape=[None, self.sent_len], name=\'input_left\')\n        self._middle = tf.placeholder(dtype=tf.int64, shape=[None, self.sent_len], name=\'input_middle\')\n        self._right = tf.placeholder(dtype=tf.int64, shape=[None, self.sent_len], name=\'input_right\')\n        self._labels = tf.placeholder(dtype=tf.float32, shape=[None, self.num_classes], name=\'input_y\')\n        self._attention = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\'attention\')\n        losses = []\n\n        with tf.variable_scope(\'embedding-left\') as scope:\n            self._W_emb_left = _variable_on_cpu(name=scope.name, shape=[self.vocab_size, self.emb_size],\n                                      initializer=tf.random_uniform_initializer(minval=-1.0, maxval=1.0))\n            sent_batch_left = tf.nn.embedding_lookup(params=self._W_emb_left, ids=self._left)\n            input_left = tf.expand_dims(sent_batch_left, -1)\n\n        with tf.variable_scope(\'embedding-middle\') as scope:\n            self._W_emb_middle = _variable_on_cpu(name=scope.name, shape=[self.vocab_size, self.emb_size],\n                                      initializer=tf.random_uniform_initializer(minval=-1.0, maxval=1.0))\n            sent_batch_middle = tf.nn.embedding_lookup(params=self._W_emb_middle, ids=self._middle)\n            input_middle = tf.expand_dims(sent_batch_middle, -1)\n\n        with tf.variable_scope(\'embedding-right\') as scope:\n            self._W_emb_right = _variable_on_cpu(name=scope.name, shape=[self.vocab_size, self.emb_size],\n                                      initializer=tf.random_uniform_initializer(minval=-1.0, maxval=1.0))\n            sent_batch_right = tf.nn.embedding_lookup(params=self._W_emb_right, ids=self._right)\n            input_right = tf.expand_dims(sent_batch_right, -1)\n\n        # conv + pooling layer\n        contexts = []\n        for contextwise_input, context in zip([input_left, input_middle, input_right],\n                                              [\'left\', \'middle\', \'right\']):\n            conv_losses, pool_flat = self.conv_layer(contextwise_input, context)\n            losses.extend(conv_losses)\n            contexts.append(pool_flat)\n        # Combine context tensors\n        num_filters = self.max_window - self.min_window + 1\n        pool_size = num_filters * self.num_kernel # 300\n        concat_context = tf.concat(contexts, 1, name=\'concat\')\n        flat_context = tf.reshape(concat_context, [-1, pool_size*3])\n\n        # drop out layer\n        if self.is_train and self.dropout > 0:\n            pool_dropout = tf.nn.dropout(flat_context, 1 - self.dropout)\n        else:\n            pool_dropout = flat_context\n\n        # fully-connected layer\n        with tf.variable_scope(\'output\') as scope:\n            W, wd = _variable_with_weight_decay(\'W\', shape=[pool_size*3, self.num_classes],\n                                initializer=tf.truncated_normal_initializer(stddev=0.05), wd=self.l2_reg)\n            losses.append(wd)\n            biases = _variable_on_cpu(\'bias\', shape=[self.num_classes],\n                                      initializer=tf.constant_initializer(0.01))\n            self.logits = tf.nn.bias_add(tf.matmul(pool_dropout, W), biases, name=\'logits\')\n\n        # loss\n        with tf.variable_scope(\'loss\') as scope:\n            if self.multi_label:\n                cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self._labels,\n                                                                        name=\'cross_entropy_per_example\')\n            else:\n                cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self._labels,\n                                                                        name=\'cross_entropy_per_example\')\n\n            if self.is_train and self.multi_instance: # apply attention\n                cross_entropy_loss = tf.reduce_sum(tf.multiply(cross_entropy, self._attention),\n                                                   name=\'cross_entropy_loss\')\n            else:\n                cross_entropy_loss = tf.reduce_mean(cross_entropy, name=\'cross_entropy_loss\')\n\n            losses.append(cross_entropy_loss)\n            self._total_loss = tf.add_n(losses, name=\'total_loss\')\n\n        # eval with auc-pr metric\n        with tf.variable_scope(\'evaluation\') as scope:\n            precision = []\n            recall = []\n            for threshold in range(10, -1, -1):\n                pre, rec = _auc_pr(self._labels, tf.sigmoid(self.logits), threshold * 0.1)\n                precision.append(pre)\n                recall.append(rec)\n            self._eval_op = zip(precision, recall)\n\n        # train on a batch\n        self._lr = tf.Variable(0.0, trainable=False)\n        if self.is_train:\n            if self.optimizer == \'adadelta\':\n                opt = tf.train.AdadeltaOptimizer(self._lr)\n            elif self.optimizer == \'adagrad\':\n                opt = tf.train.AdagradOptimizer(self._lr)\n            elif self.optimizer == \'adam\':\n                opt = tf.train.AdamOptimizer(self._lr)\n            elif self.optimizer == \'sgd\':\n                opt = tf.train.GradientDescentOptimizer(self._lr)\n            else:\n                raise ValueError(""Optimizer not supported."")\n            grads = opt.compute_gradients(self._total_loss)\n            self._train_op = opt.apply_gradients(grads)\n\n            for var in tf.trainable_variables():\n                tf.summary.histogram(var.op.name, var)\n        else:\n            self._train_op = tf.no_op()\n\n        return\n\n    @property\n    def left(self):\n        return self._left\n\n    @property\n    def middle(self):\n        return self._middle\n\n    @property\n    def right(self):\n        return self._right\n\n    @property\n    def labels(self):\n        return self._labels\n\n    @property\n    def attention(self):\n        return self._attention\n\n    @property\n    def lr(self):\n        return self._lr\n\n    @property\n    def train_op(self):\n        return self._train_op\n\n    @property\n    def total_loss(self):\n        return self._total_loss\n\n    @property\n    def eval_op(self):\n        return self._eval_op\n\n    @property\n    def scores(self):\n        return tf.sigmoid(self.logits)\n\n    @property\n    def W_emb_left(self):\n        return self._W_emb_left\n\n    @property\n    def W_emb_middle(self):\n        return self._W_emb_middle\n\n    @property\n    def W_emb_right(self):\n        return self._W_emb_right\n\n    def assign_lr(self, session, lr_value):\n        session.run(tf.assign(self.lr, lr_value))\n\n    def assign_embedding(self, session, pretrained):\n        session.run(tf.assign(self.W_emb_left, pretrained))\n        session.run(tf.assign(self.W_emb_middle, pretrained))\n        session.run(tf.assign(self.W_emb_right, pretrained))\n'"
distant_supervision.py,0,"b'# -*- coding: utf-8 -*-\n\n##########################################################\n#\n# Distant Supervision\n#\n###########################################################\n\nimport pandas as pd\nimport numpy as np\nfrom bs4 import BeautifulSoup as bs\n\n\nimport os\nimport re\nimport time\nimport requests\nimport urllib\nimport glob\nfrom codecs import open\nfrom itertools import combinations\nfrom collections import Counter\n\nimport util\n\nimport sys\nreload(sys)\nsys.setdefaultencoding(""utf-8"")\n\n\n# global variables\ndata_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), \'data\')\norig_dir = os.path.join(data_dir, \'orig\')\nner_dir = os.path.join(data_dir, \'ner\')\n\n\nner_path = ""/usr/local/Cellar/stanford-ner/3.5.2/libexec/""\nstanford_classifier = os.path.join(ner_path, \'classifiers\', \'english.all.3class.distsim.crf.ser.gz\')\nstanford_ner = os.path.join(ner_path, \'stanford-ner.jar\')\n\ntag_map = {\n    \'ORGANIZATION\': \'Q43229\',  # https://www.wikidata.org/wiki/Q43229\n    \'LOCATION\': \'Q17334923\',   # https://www.wikidata.org/wiki/Q17334923\n    \'PERSON\': \'Q5\'             # https://www.wikidata.org/wiki/Q5\n}\n\n# column names in DataFrame\ncol = [\'doc_id\', \'sent_id\', \'sent\', \'subj\', \'subj_begin\', \'subj_end\', \'subj_tag\',\n       \'rel\', \'obj\', \'obj_begin\', \'obj_end\', \'obj_tag\']\n\n\ndef sanitize(string):\n    """"""clean wikipedia article""""""\n    string = re.sub(r""\\[\\d{1,3}\\]"", "" "", string)\n    string = re.sub(r""\\[edit\\]"", "" "", string)\n    string = re.sub(r"" {2,}"", "" "", string)\n    return string.strip()\n\n\ndef download_wiki_articles(doc_id, limit=100, retry=False):\n    """"""download wikipedia article via Mediawiki API""""""\n    base_path = ""http://en.wikipedia.org/w/api.php?format=xml&action=query""\n    query = base_path + ""&list=random&rnnamespace=0&rnlimit=%d"" % limit\n    r = None\n    try:\n        r = urllib.urlopen(query).read()\n    except Exception as e:\n        if not retry:\n            download_wiki_articles(doc_id, limit, retry=True)\n        else:\n            print e.message\n            return None\n    pages = bs(r, ""html.parser"").findAll(\'page\')\n    if len(pages) < 1:\n        return None\n    docs = []\n    for page in pages:\n        if int(page[\'id\']) in doc_id:\n            continue\n\n        link = base_path + ""&prop=revisions&pageids=%s&rvprop=content&rvparse"" % page[\'id\']\n        content = urllib.urlopen(link).read()\n        content = bs(content, ""html.parser"").find(\'rev\').stripped_strings\n\n        # extract paragraph elements only\n        text = \'\'\n        for p in bs(\' \'.join(content), ""html.parser"").findAll(\'p\'):\n            text += \' \'.join(p.stripped_strings) + \'\\n\'\n        #text = text.encode(\'utf8\')\n        text = sanitize(text)\n\n        # save\n        if len(text) > 0:\n            title = re.sub(r""[ /]"", ""_"", page[\'title\'])\n            filename = page[\'id\'] + \'-\' + title + \'.txt\'\n            docs.append(filename)\n            with open(os.path.join(orig_dir, filename), \'w\', encoding=\'utf-8\') as f:\n                f.write(text)\n    return docs\n\n\ndef exec_ner(filenames):\n    """"""execute Stanford NER""""""\n    for filename in filenames:\n        in_path = os.path.join(orig_dir, filename)\n        out_path = os.path.join(ner_dir, filename)\n        cmd = \'java -mx700m -cp ""%s:"" edu.stanford.nlp.ie.crf.CRFClassifier\' % stanford_ner\n        cmd += \' -loadClassifier %s -outputFormat tabbedEntities\' % stanford_classifier\n        cmd += \' -textFile %s > %s\' % (in_path, out_path)\n        os.system(cmd)\n\n\ndef read_ner_output(filenames):\n    """"""read NER output files and store them in a pandas DataFrame""""""\n    rows = []\n    for filename in filenames:\n        path = os.path.join(ner_dir, filename)\n        if not os.path.exists(path):\n            continue\n        with open(path, \'r\', encoding=\'utf-8\') as f:\n            doc_id = filename.split(\'/\')[-1].split(\'-\', 1)[0]\n            counter = 0\n            tmp = []\n            for line in f.readlines():\n                if len(line.strip()) < 1 and len(tmp) > 2:\n                    ent = [i for i, t in enumerate(tmp) if t[1] in tag_map.keys()]\n                    for c in combinations(ent, 2):\n                        dic = {\'sent\': u\'\'}\n                        dic[\'doc_id\'] = doc_id\n                        dic[\'sent_id\'] = counter\n                        for j, t in enumerate(tmp):\n                            if j == c[0]:\n                                if len(dic[\'sent\']) > 0:\n                                    dic[\'subj_begin\'] = len(dic[\'sent\']) + 1\n                                else:\n                                    dic[\'subj_begin\'] = 0\n                                if len(dic[\'sent\']) > 0:\n                                    dic[\'subj_end\'] = len(dic[\'sent\']) + len(t[0].strip()) + 1\n                                else:\n                                    dic[\'subj_end\'] = len(t[0].strip())\n                                dic[\'subj\'] = t[0].strip()\n                                dic[\'subj_tag\'] = t[1].strip()\n                            elif j == c[1]:\n                                dic[\'obj_begin\'] = len(dic[\'sent\']) + 1\n                                dic[\'obj_end\'] = len(dic[\'sent\']) + len(t[0].strip()) + 1\n                                dic[\'obj\'] = t[0].strip()\n                                dic[\'obj_tag\'] = t[1].strip()\n\n                            if len(dic[\'sent\']) > 0:\n                                dic[\'sent\'] += \' \' + t[0].strip()\n                            else:\n                                dic[\'sent\'] += t[0].strip()\n                            if len(dic[\'sent\']) > 0:\n                                dic[\'sent\'] += \' \' + t[2].strip()\n                            else:\n                                dic[\'sent\'] += t[2].strip()\n                                #print \'""\'+dic[\'sent\']+\'""\', len(dic[\'sent\'])\n                        rows.append(dic)\n                        #print dic\n                    counter += 1\n                    tmp = []\n                elif len(line.strip()) < 1 and len(tmp) > 0 and len(tmp) <= 2:\n                    continue\n                elif len(line.strip()) > 0:\n                    e = line.split(\'\\t\')\n                    if len(e) == 1:\n                        e.insert(0, \'\')\n                        e.insert(0, \'\')\n                    if len(e) == 2 and e[1].strip() in tag_map.keys():\n                        e.append(\'\')\n                    if len(e) != 3:\n                        print e\n                        raise Exception\n                    tmp.append(e)\n                else:\n                    continue\n\n    return pd.DataFrame(rows)\n\n\ndef name2qid(name, tag, alias=False, retry=False):\n    """"""find QID (and Freebase ID if given) by name\n\n    >>> name2qid(\'Barack Obama\', \'PERSON\')        # perfect match\n    (u\'Q76\', u\'/m/02mjmr\')\n    >>> name2qid(\'Obama\', \'PERSON\', alias=True)   # alias match\n    (u\'Q76\', u\'/m/02mjmr\')\n    """"""\n\n    label = \'rdfs:label\'\n    if alias:\n        label = \'skos:altLabel\'\n\n    hpCharURL = \'https://query.wikidata.org/sparql?query=\\\n    SELECT DISTINCT ?item ?fid \\\n    WHERE {\\\n    ?item \'+label+\' ""\'+name+\'""@en.\\\n    ?item wdt:P31 ?_instanceOf.\\\n    ?_instanceOf wdt:P279* wd:\'+tag_map[tag]+\'.\\\n    OPTIONAL { ?item wdt:P646 ?fid. }\\\n    }\\\n    LIMIT 10\'\n    headers = {""Accept"": ""application/json""}\n\n    # check response\n    r = None\n    try:\n        r = requests.get(hpCharURL, headers=headers)\n    except requests.exceptions.ConnectionError:\n        if not retry:\n            time.sleep(60)\n            name2qid(name, tag, alias, retry=True)\n        else:\n            return None\n    except Exception as e:\n        print e.message\n        return None\n\n    # check json format\n    try:\n        response = r.json()\n    except ValueError:    # includes JSONDecodeError\n        return None\n\n    # parse results\n    results = []\n    for elm in response[\'results\'][\'bindings\']:\n        fid = \'\'\n        if elm.has_key(\'fid\'):\n            fid = elm[\'fid\'][\'value\']\n        results.append((elm[\'item\'][\'value\'].split(\'/\')[-1], fid))\n\n    if len(results) < 1:\n        return None\n    else:\n        return results[0]\n\n\ndef search_property(qid1, qid2, retry=False):\n    """"""find property (and schema.org relation if given)\n\n    >>> search_property(\'Q76\', \'Q30\') # Q76: Barack Obama, Q30: United States\n    [(u\'P27\', u\'country of citizenship\', u\'nationality\')]\n    """"""\n\n    hpCharURL = \'https://query.wikidata.org/sparql?query= \\\n    SELECT DISTINCT ?p ?l ?s \\\n    WHERE {\\\n    wd:\'+qid1+\' ?p wd:\'+qid2+\' .\\\n    ?property ?ref ?p .\\\n    ?property a wikibase:Property .\\\n    ?property rdfs:label ?l FILTER (lang(?l) = ""en"")\\\n    OPTIONAL { ?property wdt:P1628 ?s FILTER (SUBSTR(str(?s), 1, 18) = ""http://schema.org/""). }\\\n    }\\\n    LIMIT 10\'\n    headers = {""Accept"": ""application/json""}\n\n    # check response\n    r = None\n    try:\n        r = requests.get(hpCharURL, headers=headers)\n    except requests.exceptions.ConnectionError:\n        if not retry:\n            time.sleep(60)\n            search_property(qid1, qid2, retry=True)\n        else:\n            return None\n    except Exception as e:\n        print e.message\n        return None\n\n    # check json format\n    try:\n        response = r.json()\n    except ValueError:\n        return None\n\n    # parse results\n    results = []\n    for elm in response[\'results\'][\'bindings\']:\n        schema = \'\'\n        if elm.has_key(\'s\'):\n            schema = elm[\'s\'][\'value\'].split(\'/\')[-1]\n        results.append((elm[\'p\'][\'value\'].split(\'/\')[-1], elm[\'l\'][\'value\'], schema))\n\n    return results\n\n\ndef slot_filling(qid, pid, tag, retry=False):\n    """"""find slotfiller\n\n    >>> slot_filling(\'Q76\', \'P27\', \'LOCATION\') # Q76: Barack Obama, P27: country of citizenship\n    [(u\'United States\', u\'Q30\', u\'/m/09c7w0\')]\n    """"""\n\n    hpCharURL = \'https://query.wikidata.org/sparql?query=\\\n    SELECT DISTINCT ?item ?itemLabel ?fid \\\n    WHERE {\\\n    wd:\'+qid+\' wdt:\'+pid+\' ?item.\\\n    ?item wdt:P31 ?_instanceOf.\\\n    ?_instanceOf wdt:P279* wd:\'+tag_map[tag]+\'.\\\n    SERVICE wikibase:label { bd:serviceParam wikibase:language ""en"". }\\\n    OPTIONAL { ?item wdt:P646 ?fid. }\\\n    }\\\n    LIMIT 100\'\n    headers = {""Accept"": ""application/json""}\n\n    # check response\n    r = None\n    try:\n        r = requests.get(hpCharURL, headers=headers)\n    except requests.exceptions.ConnectionError:\n        if not retry:\n            time.sleep(60)\n            slot_filling(qid, pid, tag, retry=True)\n        else:\n            return None\n    except Exception as e:\n        print e.message\n        return None\n\n    # check json format\n    try:\n        response = r.json()\n    except ValueError:\n        return None\n\n    # parse results\n    results = []\n    for elm in response[\'results\'][\'bindings\']:\n        fid = \'\'\n        if elm.has_key(\'fid\'):\n            fid = elm[\'fid\'][\'value\']\n        results.append((elm[\'itemLabel\'][\'value\'], elm[\'item\'][\'value\'].split(\'/\')[-1], fid))\n\n    return results\n\n\ndef loop(step, doc_id, limit, entities, relations, counter):\n    """"""Distant Supervision Loop""""""\n    # Download wiki articles\n    print \'[1/4] Downloading wiki articles ...\'\n    docs = download_wiki_articles(doc_id, limit)\n    if docs is None:\n        return None\n\n    # Named Entity Recognition\n    print \'[2/4] Performing named entity recognition ...\'\n    exec_ner(docs)\n    wiki_data = read_ner_output(docs)\n    path = os.path.join(data_dir, \'candidates%d.tsv\' % step)\n    wiki_data.to_csv(path, sep=\'\\t\', encoding=\'utf-8\')\n    doc_id.extend([int(s) for s in wiki_data.doc_id.unique()])\n\n    # Prepare Containers\n    unique_entities = set([])\n    unique_entity_pairs = set([])\n    for idx, row in wiki_data.iterrows():\n        unique_entities.add((row[\'subj\'], row[\'subj_tag\']))\n        unique_entities.add((row[\'obj\'], row[\'obj_tag\']))\n        unique_entity_pairs.add((row[\'subj\'], row[\'obj\']))\n\n    # Entity Linkage\n    print \'[3/4] Linking entities ...\'\n    for name, tag in unique_entities:\n        if not entities.has_key(name) and tag in tag_map.keys():\n            e = name2qid(name, tag, alias=False)\n            if e is None:\n                e = name2qid(name, tag, alias=True)\n            entities[name] = e\n    util.dump_to_file(os.path.join(data_dir, ""entities.cPickle""), entities)\n\n    # Predicate Linkage\n    print \'[4/4] Linking predicates ...\'\n    for subj, obj in unique_entity_pairs:\n        if not relations.has_key((subj, obj)):\n            if entities[subj] is not None and entities[obj] is not None:\n                if (entities[subj][0] != entities[obj][0]) or (subj != obj):\n                    arg1 = entities[subj][0]\n                    arg2 = entities[obj][0]\n                    relations[(subj, obj)] = search_property(arg1, arg2)\n                    #TODO: alternative name relation\n                    #elif (entities[subj][0] == entities[obj][0]) and (subj != obj):\n                    #    relations[(subj, obj)] = \'P\'\n    util.dump_to_file(os.path.join(data_dir, ""relations.cPickle""), relations)\n\n    # Assign relation\n    wiki_data[\'rel\'] = pd.Series(index=wiki_data.index, dtype=str)\n    for idx, row in wiki_data.iterrows():\n        entity_pair = (row[\'subj\'], row[\'obj\'])\n\n        if relations.has_key(entity_pair):\n            rel = relations[entity_pair]\n            if rel is not None and len(rel) > 0:\n                counter += 1\n                wiki_data.set_value(idx, \'rel\', \', \'.join(set([s[0] for s in rel])))\n    # Save\n    path = os.path.join(data_dir, \'candidates%d.tsv\' % step)\n    wiki_data.to_csv(path, sep=\'\\t\', encoding=\'utf-8\')\n\n    # Cleanup\n    for f in glob.glob(os.path.join(orig_dir, \'*\')):\n        os.remove(f)\n    for f in glob.glob(os.path.join(ner_dir, \'*\')):\n        os.remove(f)\n\n    return doc_id, entities, relations, counter\n\n\ndef extract_relations(entities, relations):\n    """"""extract relations""""""\n    rows = []\n    for k, v in relations.iteritems():\n        if v is not None and len(v) > 0:\n            for r in v:\n                dic = {}\n                dic[\'subj_qid\'] = entities[k[0]][0]\n                dic[\'subj_fid\'] = entities[k[0]][1]\n                dic[\'subj\'] = k[0]\n                dic[\'obj_qid\'] = entities[k[1]][0]\n                dic[\'obj_fid\'] = entities[k[1]][1]\n                dic[\'obj\'] = k[1]\n                dic[\'rel_id\'] = r[0]\n                dic[\'rel\'] = r[1]\n                dic[\'rel_schema\'] = r[2]\n                #TODO: add number of mentions\n                #dic[\'wikidata_idx\'] = entity_pairs[k]\n                rows.append(dic)\n    return pd.DataFrame(rows)\n\n\ndef positive_examples():\n    entities = {}\n    relations = {}\n    counter = 0\n    limit = 1000\n    doc_id = []\n    step = 1\n\n    if not os.path.exists(orig_dir):\n        os.mkdir(orig_dir)\n    if not os.path.exists(ner_dir):\n        os.mkdir(ner_dir)\n\n    #for j in range(1, step):\n    #    wiki_data = pd.read_csv(os.path.join(data_dir, ""candidates%d.tsv"" % j), sep=\'\\t\', index_col=0)\n    #    doc_id.extend([int(s) for s in wiki_data.doc_id.unique()])\n    #    counter += int(wiki_data.rel.count())\n\n    while counter < 10000 and step < 100:\n        print \'===== step %d =====\' % step\n        ret = loop(step, doc_id, limit, entities, relations, counter)\n        if ret is not None:\n            doc_id, entities, relations, counter = ret\n\n        step += 1\n\n    # positive candidates\n    positive_data = []\n    for f in glob.glob(os.path.join(data_dir, \'candidates*.tsv\')):\n        pos = pd.read_csv(f, sep=\'\\t\', encoding=\'utf-8\', index_col=0)\n        positive_data.append(pos[pd.notnull(pos.rel)])\n    positive_df = pd.concat(positive_data, axis=0, ignore_index=True)\n    positive_df[col].to_csv(os.path.join(data_dir, \'positive_candidates.tsv\'), sep=\'\\t\', encoding=\'utf-8\')\n\n    # save relations\n    pos_rel = extract_relations(entities, relations)\n    pos_rel.to_csv(os.path.join(data_dir, \'positive_relations.tsv\'), sep=\'\\t\', encoding=\'utf-8\')\n\n\ndef negative_examples():\n    negative = {}\n\n    unique_pair = set([])\n    neg_candidates = []\n\n    #TODO: replace with positive_relations.tsv\n    entities = util.load_from_dump(os.path.join(data_dir, ""entities.cPickle""))\n    relations = util.load_from_dump(os.path.join(data_dir, ""relations.cPickle""))\n\n    rel_counter = Counter([u[0] for r in relations.values() if r is not None and len(r) > 0 for u in r])\n    most_common_rel = [r[0] for r in rel_counter.most_common(10)]\n\n\n    for data_path in glob.glob(os.path.join(data_dir, \'candidates*.tsv\')):\n        neg = pd.read_csv(data_path, sep=\'\\t\', encoding=\'utf-8\', index_col=0)\n        negative_df = neg[pd.isnull(neg.rel)]\n\n        # Assign relation\n        for idx, row in negative_df.iterrows():\n            if (entities.has_key(row[\'subj\']) and entities[row[\'subj\']] is not None \\\n                        and entities.has_key(row[\'obj\']) and entities[row[\'obj\']] is not None):\n                qid = entities[row[\'subj\']][0]\n                target = entities[row[\'obj\']][0]\n                candidates = []\n                for pid in most_common_rel:\n                    if (qid, pid) not in unique_pair:\n                        unique_pair.add((qid, pid))\n                        items = slot_filling(qid, pid, row[\'obj_tag\'])\n                        if items is not None and len(items) > 1:\n                            qids = [q[1] for q in items]\n                            if target not in qids:\n                                candidates.append(pid)\n\n                if len(candidates) > 0:\n                    row[\'rel\'] = \', \'.join(candidates)\n                    neg_candidates.append(row)\n\n\n    neg_examples = pd.DataFrame(neg_candidates)\n    neg_examples[col].to_csv(os.path.join(data_dir, \'negative_candidates.tsv\'), sep=\'\\t\', encoding=\'utf-8\')\n\n\n    # save relations\n    #pos_rel = extract_relations(entities, negative)\n    #pos_rel.to_csv(os.path.join(data_dir, \'positive_relations.tsv\'), sep=\'\\t\', encoding=\'utf-8\')\n\n\ndef load_gold_patterns():\n    def clean_str(string):\n        string = re.sub(r"", "", "" , "", string)\n        string = re.sub(r""\' "", "" \' "", string)\n        string = re.sub(r"" \\* "", "" .* "", string)\n        string = re.sub(r""\\("", ""-LRB-"", string)\n        string = re.sub(r""\\)"", ""-RRB-"", string)\n        string = re.sub(r"" {2,}"", "" "", string)\n        return string.strip()\n\n    g_patterns = []\n    g_labels = []\n    with open(os.path.join(data_dir, \'gold_patterns.tsv\'), \'r\') as f:\n        for line in f.readlines():\n            line = line.strip()\n            if len(line) > 0 and not line.startswith(\'#\'):\n                e = line.split(\'\\t\', 1)\n                if len(e) > 1:\n                    g_patterns.append(clean_str(e[1]))\n                    g_labels.append(e[0])\n                else:\n                    print e\n                    raise Exception(\'Process Error: %s\' % os.path.join(data_dir, \'gold_patterns.tsv\'))\n\n    return pd.DataFrame({\'pattern\': g_patterns, \'label\': g_labels})\n\n\ndef score_reliability(gold_patterns, sent, rel, subj, obj):\n    for name, group in gold_patterns.groupby(\'label\'):\n        if name in [r.strip() for r in rel.split(\',\')]:\n            for i, g in group.iterrows():\n                pattern = g[\'pattern\']\n                pattern = re.sub(r\'\\$ARG(0|1)\', subj, pattern, count=1)\n                pattern = re.sub(r\'\\$ARG(0|2)\', obj, pattern, count=1)\n                match = re.search(pattern, sent)\n                if match:\n                    return 1.0\n    return 0.0\n\n\ndef extract_positive():\n    if not os.path.exists(os.path.join(data_dir, \'mlmi\')):\n        os.mkdir(os.path.join(data_dir, \'mlmi\'))\n    if not os.path.exists(os.path.join(data_dir, \'er\')):\n        os.mkdir(os.path.join(data_dir, \'er\'))\n\n    # read gold patterns to extract attention\n    gold_patterns = load_gold_patterns()\n\n\n    #TODO: replace with negative_relations.tsv\n    entities = util.load_from_dump(os.path.join(data_dir, ""entities.cPickle""))\n    relations = util.load_from_dump(os.path.join(data_dir, ""relations.cPickle""))\n\n    # filter out the relations which occur less than 50 times\n    rel_c = Counter([u[0] for r in relations.values() if r is not None and len(r) > 0 for u in r])\n    rel_c_top = [k for k, v in rel_c.most_common(50) if v >= 50]\n\n    # positive examples\n    positive_df = pd.read_csv(os.path.join(data_dir, \'positive_candidates.tsv\'),\n                              sep=\'\\t\', encoding=\'utf-8\', index_col=0)\n\n    positive_df[\'right\'] = pd.Series(index=positive_df.index, dtype=str)\n    positive_df[\'middle\'] = pd.Series(index=positive_df.index, dtype=str)\n    positive_df[\'left\'] = pd.Series(index=positive_df.index, dtype=str)\n    positive_df[\'clean\'] = pd.Series(index=positive_df.index, dtype=str)\n    positive_df[\'label\'] = pd.Series(index=positive_df.index, dtype=str)\n    positive_df[\'attention\'] = pd.Series([0.0]*len(positive_df), index=positive_df.index, dtype=np.float32)\n\n    num_er = 0\n    with open(os.path.join(data_dir, \'er\', \'source.txt\'), \'w\', encoding=\'utf-8\') as f:\n        for idx, row in positive_df.iterrows():\n\n            # restore relation\n            rel = [\'<\' + l.strip() + \'>\' for l in row[\'rel\'].split(\',\') if l.strip() in rel_c_top]\n            if len(rel) > 0:\n\n                s = row[\'sent\']\n                subj = \'<\' + entities[row[\'subj\'].encode(\'utf-8\')][0] + \'>\'\n                obj = \'<\' + entities[row[\'obj\'].encode(\'utf-8\')][0] + \'>\'\n                left = s[:row[\'subj_begin\']] + subj\n                middle = s[row[\'subj_end\']:row[\'obj_begin\']]\n                right = obj + s[row[\'obj_end\']:]\n                text = left.strip() + \' \' + middle.strip() + \' \' + right.strip()\n\n                # check if begin-end position is correct\n                assert s[row[\'subj_begin\']:row[\'subj_end\']] == row[\'subj\']\n                assert s[row[\'obj_begin\']:row[\'obj_end\']] == row[\'obj\']\n\n                # MLMI dataset\n                # filter out too long sentences\n                if len(left.split()) < 100 and len(middle.split()) < 100 and len(right.split()) < 100:\n\n                    positive_df.set_value(idx, \'right\', right.strip())\n                    positive_df.set_value(idx, \'middle\', middle.strip())\n                    positive_df.set_value(idx, \'left\', left.strip())\n                    positive_df.set_value(idx, \'clean\', text.strip())\n\n                    # binarize label\n                    label = [\'0\'] * len(rel_c_top)\n                    for u in row[\'rel\'].split(\',\'):\n                        if u.strip() in rel_c_top:\n                            label[rel_c_top.index(u.strip())] = \'1\'\n                    positive_df.set_value(idx, \'label\', \' \'.join(label))\n\n                    # score reliability if positive\n                    reliability = score_reliability(gold_patterns, s, row[\'rel\'], row[\'subj\'], row[\'obj\'])\n                    positive_df.set_value(idx, \'attention\', reliability)\n\n                # ER dataset\n                for r in rel:\n                    num_er += 1\n                    f.write(subj + \' \' + r + \' \' + obj + \'\\n\')\n\n    with open(os.path.join(data_dir, \'er\', \'target.txt\'), \'w\', encoding=\'utf-8\') as f:\n        for _ in range(num_er):\n            f.write(\'1 0\\n\')\n\n    positive_df_valid = positive_df[pd.notnull(positive_df.clean)]\n    assert len(positive_df_valid[\'clean\']) == len(positive_df_valid[\'label\'])\n\n    positive_df_valid[\'right\'].to_csv(os.path.join(data_dir, \'mlmi\', \'source.right\'),\n                                      sep=\'\\t\', index=False, header=False, encoding=\'utf-8\')\n    positive_df_valid[\'middle\'].to_csv(os.path.join(data_dir, \'mlmi\', \'source.middle\'),\n                                       sep=\'\\t\', index=False, header=False, encoding=\'utf-8\')\n    positive_df_valid[\'left\'].to_csv(os.path.join(data_dir, \'mlmi\', \'source.left\'),\n                                     sep=\'\\t\', index=False, header=False, encoding=\'utf-8\')\n    positive_df_valid[\'clean\'].to_csv(os.path.join(data_dir, \'mlmi\', \'source.txt\'),\n                                      sep=\'\\t\', index=False, header=False, encoding=\'utf-8\')\n    positive_df_valid[\'label\'].to_csv(os.path.join(data_dir, \'mlmi\', \'target.txt\'),\n                                      sep=\'\\t\', index=False, header=False, encoding=\'utf-8\')\n    positive_df_valid[\'attention\'].to_csv(os.path.join(data_dir, \'mlmi\', \'source.att\'),\n                                          sep=\'\\t\', index=False, header=False, encoding=\'utf-8\')\n\n\ndef extract_negative():\n    entities = util.load_from_dump(os.path.join(data_dir, ""entities.cPickle""))\n\n    # negative examples\n    negative_df = pd.read_csv(os.path.join(data_dir, \'negative_candidates.tsv\'),\n                              sep=\'\\t\', encoding=\'utf-8\', index_col=0)\n\n    with open(os.path.join(data_dir, \'er\', \'source.txt\'), \'a\', encoding=\'utf-8\') as source_file:\n        with open(os.path.join(data_dir, \'er\', \'target.txt\'), \'a\', encoding=\'utf-8\') as target_file:\n            for idx, row in negative_df.iterrows():\n                s = row[\'sent\']\n\n                subj = \'<\' + entities[row[\'subj\'].encode(\'utf-8\')][0] + \'>\'\n                obj = \'<\' + entities[row[\'obj\'].encode(\'utf-8\')][0] + \'>\'\n                rel = [\'<\' + l.strip() + \'>\' for l in row[\'rel\'].split(\',\')]\n\n                assert s[row[\'subj_begin\']:row[\'subj_end\']] == row[\'subj\']\n                assert s[row[\'obj_begin\']:row[\'obj_end\']] == row[\'obj\']\n\n                if len(rel) > 0:\n                    for r in rel:\n                        source_file.write(subj + \' \' + r + \' \' + obj + \'\\n\')\n                        target_file.write(\'0 1\\n\')\n\ndef main():\n    # gather positive examples\n    if not os.path.exists(os.path.join(data_dir, \'positive_candidates.tsv\')):\n        positive_examples()\n    extract_positive()\n\n    # gather negative examples\n    if not os.path.exists(os.path.join(data_dir, \'negative_candidates.tsv\')):\n        negative_examples()\n    extract_negative()\n\n\n\n\nif __name__ == \'__main__\':\n    main()\n'"
eval.py,8,"b'# -*- coding: utf-8 -*-\n\n##########################################################\n#\n# Attention-based Convolutional Neural Network\n#   for Context-wise Learning\n#\n#\n#   Note: this implementation is mostly based on\n#   https://github.com/yuhaozhang/sentence-convnet/blob/master/eval.py\n#\n##########################################################\n\nfrom datetime import datetime\nimport os\nimport tensorflow as tf\nimport numpy as np\nimport util\n\n\nFLAGS = tf.app.flags.FLAGS\nthis_dir = os.path.abspath(os.path.dirname(__file__))\ntf.app.flags.DEFINE_string(\'train_dir\', os.path.join(this_dir, \'models\', \'er-cnn\'), \'Directory of the checkpoint files\')\n\n\ndef evaluate(eval_data, config):\n    """""" Build evaluation graph and run. """"""\n\n    with tf.Graph().as_default():\n        with tf.variable_scope(\'cnn\'):\n            if config.has_key(\'contextwise\') and config[\'contextwise\']:\n                import cnn_context\n                m = cnn_context.Model(config, is_train=False)\n            else:\n                import cnn\n                m = cnn.Model(config, is_train=False)\n        saver = tf.train.Saver(tf.global_variables())\n\n        with tf.Session() as sess:\n            ckpt = tf.train.get_checkpoint_state(config[\'train_dir\'])\n            if ckpt and ckpt.model_checkpoint_path:\n                saver.restore(sess, ckpt.model_checkpoint_path)\n            else:\n                raise IOError(""Loading checkpoint file failed!"")\n\n            print ""\\nStart evaluation on test set ...\\n""\n            if config.has_key(\'contextwise\') and config[\'contextwise\']:\n                left_batch, middle_batch, right_batch, y_batch, _ = zip(*eval_data)\n                feed = {m.left: np.array(left_batch),\n                        m.middle: np.array(middle_batch),\n                        m.right: np.array(right_batch),\n                        m.labels: np.array(y_batch)}\n            else:\n                x_batch, y_batch, _ = zip(*eval_data)\n                feed = {m.inputs: np.array(x_batch), m.labels: np.array(y_batch)}\n            loss, eval = sess.run([m.total_loss, m.eval_op], feed_dict=feed)\n            pre, rec = zip(*eval)\n\n            auc = util.calc_auc_pr(pre, rec)\n            f1 = (2.0 * pre[5] * rec[5]) / (pre[5] + rec[5])\n            print \'%s: loss = %.6f, p = %.4f, r = %4.4f, f1 = %.4f, auc = %.4f\' % (datetime.now(), loss,\n                                                                                   pre[5], rec[5], f1, auc)\n    return pre, rec\n\n\ndef main(argv=None):\n    restore_param = util.load_from_dump(os.path.join(FLAGS.train_dir, \'flags.cPickle\'))\n    restore_param[\'train_dir\'] = FLAGS.train_dir\n\n    if restore_param.has_key(\'contextwise\') and restore_param[\'contextwise\']:\n        source_path = os.path.join(restore_param[\'data_dir\'], ""ids"")\n        target_path = os.path.join(restore_param[\'data_dir\'], ""target.txt"")\n        _, data = util.read_data_contextwise(source_path, target_path, restore_param[\'sent_len\'],\n                                             train_size=restore_param[\'train_size\'])\n    else:\n        source_path = os.path.join(restore_param[\'data_dir\'], ""ids.txt"")\n        target_path = os.path.join(restore_param[\'data_dir\'], ""target.txt"")\n        _, data = util.read_data(source_path, target_path, restore_param[\'sent_len\'],\n                                 train_size=restore_param[\'train_size\'])\n\n    pre, rec = evaluate(data, restore_param)\n    util.dump_to_file(os.path.join(FLAGS.train_dir, \'results.cPickle\'), {\'precision\': pre, \'recall\': rec})\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
train.py,28,"b'# -*- coding: utf-8 -*-\n\nfrom datetime import datetime\nimport time\nimport os\nimport tensorflow as tf\nimport numpy as np\n\nimport cnn\nimport util\n\n\nFLAGS = tf.app.flags.FLAGS\n\n# train parameters\nthis_dir = os.path.abspath(os.path.dirname(__file__))\ntf.app.flags.DEFINE_string(\'data_dir\', os.path.join(this_dir, \'data\'), \'Directory of the data\')\ntf.app.flags.DEFINE_string(\'train_dir\', os.path.join(this_dir, \'train\'),\n                           \'Directory to save training checkpoint files\')\ntf.app.flags.DEFINE_integer(\'train_size\', 100000, \'Number of training examples\')\ntf.app.flags.DEFINE_integer(\'num_epochs\', 10, \'Number of epochs to run\')\ntf.app.flags.DEFINE_boolean(\'use_pretrain\', False, \'Use word2vec pretrained embeddings or not\')\n\ntf.app.flags.DEFINE_string(\'optimizer\', \'adam\',\n                           \'Optimizer to use. Must be one of ""sgd"", ""adagrad"", ""adadelta"" and ""adam""\')\ntf.app.flags.DEFINE_float(\'init_lr\', 1e-3, \'Initial learning rate\')\ntf.app.flags.DEFINE_float(\'lr_decay\', 0.95, \'LR decay rate\')\ntf.app.flags.DEFINE_integer(\'tolerance_step\', 500,\n                            \'Decay the lr after loss remains unchanged for this number of steps\')\ntf.app.flags.DEFINE_float(\'dropout\', 0.5, \'Dropout rate. 0 is no dropout.\')\n\n# logging\ntf.app.flags.DEFINE_integer(\'log_step\', 10, \'Display log to stdout after this step\')\ntf.app.flags.DEFINE_integer(\'summary_step\', 50,\n                            \'Write summary (evaluate model on dev set) after this step\')\ntf.app.flags.DEFINE_integer(\'checkpoint_step\', 100, \'Save model after this step\')\n\n\ndef train(train_data, test_data):\n    # train_dir\n    timestamp = str(int(time.time()))\n    out_dir = os.path.abspath(os.path.join(FLAGS.train_dir, timestamp))\n\n    # save flags\n    if not os.path.exists(out_dir):\n        os.mkdir(out_dir)\n    FLAGS._parse_flags()\n    config = dict(FLAGS.__flags.items())\n\n    # Window_size must not be larger than the sent_len\n    if config[\'sent_len\'] < config[\'max_window\']:\n        config[\'max_window\'] = config[\'sent_len\']\n\n    util.dump_to_file(os.path.join(out_dir, \'flags.cPickle\'), config)\n    print ""Parameters:""\n    for k, v in config.iteritems():\n        print \'%20s %r\' % (k, v)\n\n    num_batches_per_epoch = int(np.ceil(float(len(train_data))/FLAGS.batch_size))\n    max_steps = num_batches_per_epoch * FLAGS.num_epochs\n\n    with tf.Graph().as_default():\n        with tf.variable_scope(\'cnn\', reuse=None):\n            m = cnn.Model(config, is_train=True)\n        with tf.variable_scope(\'cnn\', reuse=True):\n            mtest = cnn.Model(config, is_train=False)\n\n        # checkpoint\n        saver = tf.train.Saver(tf.global_variables())\n        save_path = os.path.join(out_dir, \'model.ckpt\')\n        summary_op = tf.summary.merge_all()\n\n        # session\n        with tf.Session().as_default() as sess:\n            proj_config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n            embedding = proj_config.embeddings.add()\n            embedding.tensor_name = m.W_emb.name\n            embedding.metadata_path = os.path.join(FLAGS.data_dir, \'vocab.txt\')\n\n            train_summary_writer = tf.summary.FileWriter(os.path.join(out_dir, ""train""), graph=sess.graph)\n            dev_summary_writer = tf.summary.FileWriter(os.path.join(out_dir, ""dev""), graph=sess.graph)\n            tf.contrib.tensorboard.plugins.projector.visualize_embeddings(train_summary_writer, proj_config)\n            tf.contrib.tensorboard.plugins.projector.visualize_embeddings(dev_summary_writer, proj_config)\n\n            sess.run(tf.global_variables_initializer())\n\n            # assign pretrained embeddings\n            if FLAGS.use_pretrain:\n                print ""Initialize model with pretrained embeddings...""\n                pretrained_embedding = np.load(os.path.join(FLAGS.data_dir, \'emb.npy\'))\n                m.assign_embedding(sess, pretrained_embedding)\n\n            # initialize parameters\n            current_lr = FLAGS.init_lr\n            lowest_loss_value = float(""inf"")\n            decay_step_counter = 0\n            global_step = 0\n\n            # evaluate on dev set\n            def dev_step(mtest, sess):\n                dev_loss = []\n                dev_auc = []\n                dev_f1_score = []\n\n                # create batch\n                test_batches = util.batch_iter(test_data, batch_size=FLAGS.batch_size, num_epochs=1, shuffle=False)\n                for batch in test_batches:\n                    x_batch, y_batch, _ = zip(*batch)\n                    loss_value, eval_value = sess.run([mtest.total_loss, mtest.eval_op],\n                        feed_dict={mtest.inputs: np.array(x_batch), mtest.labels: np.array(y_batch)})\n                    dev_loss.append(loss_value)\n                    pre, rec = zip(*eval_value)\n                    dev_auc.append(util.calc_auc_pr(pre, rec))\n                    dev_f1_score.append((2.0 * pre[5] * rec[5]) / (pre[5] + rec[5])) # threshold = 0.5\n\n                return np.mean(dev_loss), np.mean(dev_auc), np.mean(dev_f1_score)\n\n            # train loop\n            print ""\\nStart training (save checkpoints in %s)\\n"" % out_dir\n            train_loss = []\n            train_auc = []\n            train_f1_score = []\n            train_batches = util.batch_iter(train_data, batch_size=FLAGS.batch_size, num_epochs=FLAGS.num_epochs)\n            for batch in train_batches:\n                batch_size = len(batch)\n\n                m.assign_lr(sess, current_lr)\n                global_step += 1\n\n                x_batch, y_batch, a_batch = zip(*batch)\n                feed = {m.inputs: np.array(x_batch), m.labels: np.array(y_batch)}\n                if FLAGS.attention:\n                    feed[m.attention] = np.array(a_batch)\n                start_time = time.time()\n                _, loss_value, eval_value = sess.run([m.train_op, m.total_loss, m.eval_op], feed_dict=feed)\n                proc_duration = time.time() - start_time\n                train_loss.append(loss_value)\n                pre, rec = zip(*eval_value)\n                auc = util.calc_auc_pr(pre, rec)\n                f1 = (2.0 * pre[5] * rec[5]) / (pre[5] + rec[5]) # threshold = 0.5\n                train_auc.append(auc)\n                train_f1_score.append(f1)\n\n                assert not np.isnan(loss_value), ""Model loss is NaN.""\n\n                # print log\n                if global_step % FLAGS.log_step == 0:\n                    examples_per_sec = batch_size / proc_duration\n                    format_str = \'%s: step %d/%d, f1 = %.4f, auc = %.4f, loss = %.4f \' + \\\n                                 \'(%.1f examples/sec; %.3f sec/batch), lr: %.6f\'\n                    print format_str % (datetime.now(), global_step, max_steps, f1, auc, loss_value,\n                                        examples_per_sec, proc_duration, current_lr)\n\n                # write summary\n                if global_step % FLAGS.summary_step == 0:\n                    summary_str = sess.run(summary_op)\n                    train_summary_writer.add_summary(summary_str, global_step)\n                    dev_summary_writer.add_summary(summary_str, global_step)\n\n                    # summary loss, f1\n                    train_summary_writer.add_summary(\n                        _summary_for_scalar(\'loss\', np.mean(train_loss)), global_step=global_step)\n                    train_summary_writer.add_summary(\n                        _summary_for_scalar(\'auc\', np.mean(train_auc)), global_step=global_step)\n                    train_summary_writer.add_summary(\n                        _summary_for_scalar(\'f1\', np.mean(train_f1_score)), global_step=global_step)\n\n                    dev_loss, dev_auc, dev_f1 = dev_step(mtest, sess)\n                    dev_summary_writer.add_summary(\n                        _summary_for_scalar(\'loss\', dev_loss), global_step=global_step)\n                    dev_summary_writer.add_summary(\n                        _summary_for_scalar(\'auc\', dev_auc), global_step=global_step)\n                    dev_summary_writer.add_summary(\n                        _summary_for_scalar(\'f1\', dev_f1), global_step=global_step)\n\n                    print ""\\n===== write summary =====""\n                    print ""%s: step %d/%d: train_loss = %.6f, train_auc = %.4f, train_f1 = %.4f"" \\\n                          % (datetime.now(), global_step, max_steps,\n                             np.mean(train_loss), np.mean(train_auc), np.mean(train_f1_score))\n                    print ""%s: step %d/%d:   dev_loss = %.6f,   dev_auc = %.4f,   dev_f1 = %.4f\\n"" \\\n                          % (datetime.now(), global_step, max_steps, dev_loss, dev_auc, dev_f1)\n\n                    # reset container\n                    train_loss = []\n                    train_auc = []\n                    train_f1_score = []\n\n                # decay learning rate if necessary\n                if loss_value < lowest_loss_value:\n                    lowest_loss_value = loss_value\n                    decay_step_counter = 0\n                else:\n                    decay_step_counter += 1\n                if decay_step_counter >= FLAGS.tolerance_step:\n                    current_lr *= FLAGS.lr_decay\n                    print \'%s: step %d/%d, Learning rate decays to %.5f\' % \\\n                          (datetime.now(), global_step, max_steps, current_lr)\n                    decay_step_counter = 0\n\n                # stop learning if learning rate is too low\n                if current_lr < 1e-5:\n                    break\n\n                # save checkpoint\n                if global_step % FLAGS.checkpoint_step == 0:\n                    saver.save(sess, save_path, global_step=global_step)\n            saver.save(sess, save_path, global_step=global_step)\n\n\ndef _summary_for_scalar(name, value):\n    return tf.Summary(value=[tf.Summary.Value(tag=name, simple_value=float(value))])\n\n\ndef main(argv=None):\n    if not os.path.exists(FLAGS.train_dir):\n        os.mkdir(FLAGS.train_dir)\n\n    # load dataset\n    source_path = os.path.join(FLAGS.data_dir, \'ids.txt\')\n    target_path = os.path.join(FLAGS.data_dir, \'target.txt\')\n    attention_path = None\n    if FLAGS.attention:\n        if os.path.exists(os.path.join(FLAGS.data_dir, \'source.att\')):\n            attention_path = os.path.join(FLAGS.data_dir, \'source.att\')\n        else:\n            raise ValueError(""Attention file %s not found."", os.path.join(FLAGS.data_dir, \'source.att\'))\n    train_data, test_data = util.read_data(source_path, target_path, FLAGS.sent_len,\n                                           attention_path=attention_path, train_size=FLAGS.train_size)\n    train(train_data, test_data)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
train_context.py,28,"b'# -*- coding: utf-8 -*-\n\nfrom datetime import datetime\nimport time\nimport os\nimport tensorflow as tf\nimport numpy as np\n\nimport cnn_context\nimport util\n\n\nFLAGS = tf.app.flags.FLAGS\n\n# train parameters\nthis_dir = os.path.abspath(os.path.dirname(__file__))\ntf.app.flags.DEFINE_string(\'data_dir\', os.path.join(this_dir, \'data\', \'mlmi\'), \'Directory of the data\')\ntf.app.flags.DEFINE_string(\'train_dir\', os.path.join(this_dir, \'train\'),\n                           \'Directory to save training checkpoint files\')\ntf.app.flags.DEFINE_integer(\'train_size\', 100000, \'Number of training examples\')\ntf.app.flags.DEFINE_integer(\'num_epochs\', 10, \'Number of epochs to run\')\ntf.app.flags.DEFINE_boolean(\'use_pretrain\', False, \'Use word2vec pretrained embeddings or not\')\n\ntf.app.flags.DEFINE_string(\'optimizer\', \'adam\',\n                           \'Optimizer to use. Must be one of ""sgd"", ""adagrad"", ""adadelta"" and ""adam""\')\ntf.app.flags.DEFINE_float(\'init_lr\', 1e-3, \'Initial learning rate\')\ntf.app.flags.DEFINE_float(\'lr_decay\', 0.95, \'LR decay rate\')\ntf.app.flags.DEFINE_integer(\'tolerance_step\', 500,\n                            \'Decay the lr after loss remains unchanged for this number of steps\')\ntf.app.flags.DEFINE_float(\'dropout\', 0.5, \'Dropout rate. 0 is no dropout.\')\n\n# logging\ntf.app.flags.DEFINE_integer(\'log_step\', 10, \'Display log to stdout after this step\')\ntf.app.flags.DEFINE_integer(\'summary_step\', 50,\n                            \'Write summary (evaluate model on dev set) after this step\')\ntf.app.flags.DEFINE_integer(\'checkpoint_step\', 100, \'Save model after this step\')\n\n\ndef train(train_data, test_data):\n    # train_dir\n    timestamp = str(int(time.time()))\n    out_dir = os.path.abspath(os.path.join(FLAGS.train_dir, timestamp))\n\n    # save flags\n    if not os.path.exists(out_dir):\n        os.mkdir(out_dir)\n    FLAGS._parse_flags()\n    config = dict(FLAGS.__flags.items())\n\n    # Window_size must not be larger than the sent_len\n    if config[\'sent_len\'] < config[\'max_window\']:\n        config[\'max_window\'] = config[\'sent_len\']\n\n    # flag to restore the contextwise model\n    config[\'contextwise\'] = True\n\n    # save flags\n    util.dump_to_file(os.path.join(out_dir, \'flags.cPickle\'), config)\n    print ""Parameters:""\n    for k, v in config.iteritems():\n        print \'%20s %r\' % (k, v)\n\n    # max number of steps\n    num_batches_per_epoch = int(np.ceil(float(len(train_data))/FLAGS.batch_size))\n    max_steps = num_batches_per_epoch * FLAGS.num_epochs\n\n    with tf.Graph().as_default():\n        with tf.variable_scope(\'cnn\', reuse=None):\n            m = cnn_context.Model(config, is_train=True)\n        with tf.variable_scope(\'cnn\', reuse=True):\n            mtest = cnn_context.Model(config, is_train=False)\n\n        # checkpoint\n        saver = tf.train.Saver(tf.global_variables())\n        save_path = os.path.join(out_dir, \'model.ckpt\')\n        summary_op = tf.summary.merge_all()\n\n        # session\n        with tf.Session().as_default() as sess:\n            proj_config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n            embedding_left = proj_config.embeddings.add()\n            embedding_middle = proj_config.embeddings.add()\n            embedding_right = proj_config.embeddings.add()\n            embedding_left.tensor_name = m.W_emb_left.name\n            embedding_middle.tensor_name = m.W_emb_middle.name\n            embedding_right.tensor_name = m.W_emb_right.name\n            embedding_left.metadata_path = os.path.join(FLAGS.data_dir, \'vocab.txt\')\n            embedding_middle.metadata_path = os.path.join(FLAGS.data_dir, \'vocab.txt\')\n            embedding_right.metadata_path = os.path.join(FLAGS.data_dir, \'vocab.txt\')\n\n            train_summary_writer = tf.summary.FileWriter(os.path.join(out_dir, ""train""), graph=sess.graph)\n            dev_summary_writer = tf.summary.FileWriter(os.path.join(out_dir, ""dev""), graph=sess.graph)\n            tf.contrib.tensorboard.plugins.projector.visualize_embeddings(train_summary_writer, proj_config)\n            tf.contrib.tensorboard.plugins.projector.visualize_embeddings(dev_summary_writer, proj_config)\n\n            sess.run(tf.global_variables_initializer())\n\n            # assign pretrained embeddings\n            if FLAGS.use_pretrain:\n                print ""Initialize model with pretrained embeddings...""\n                pretrained_embedding = np.load(os.path.join(FLAGS.data_dir, \'emb.npy\'))\n                m.assign_embedding(sess, pretrained_embedding)\n\n            # initialize parameters\n            current_lr = FLAGS.init_lr\n            lowest_loss_value = float(""inf"")\n            decay_step_counter = 0\n            global_step = 0\n\n            # evaluate on dev set\n            def dev_step(mtest, sess):\n                dev_loss = []\n                dev_auc = []\n                dev_f1_score = []\n\n                # create batch\n                test_batches = util.batch_iter(test_data, batch_size=FLAGS.batch_size, num_epochs=1, shuffle=False)\n                for batch in test_batches:\n                    left_batch, middle_batch, right_batch, y_batch, _ = zip(*batch)\n                    feed = {mtest.left: np.array(left_batch),\n                            mtest.middle: np.array(middle_batch),\n                            mtest.right: np.array(right_batch),\n                            mtest.labels: np.array(y_batch)}\n                    loss_value, eval_value = sess.run([mtest.total_loss, mtest.eval_op], feed_dict=feed)\n                    dev_loss.append(loss_value)\n                    pre, rec = zip(*eval_value)\n                    dev_auc.append(util.calc_auc_pr(pre, rec))\n                    dev_f1_score.append((2.0 * pre[5] * rec[5]) / (pre[5] + rec[5])) # threshold = 0.5\n\n                return np.mean(dev_loss), np.mean(dev_auc), np.mean(dev_f1_score)\n\n            # train loop\n            print ""\\nStart training (save checkpoints in %s)\\n"" % out_dir\n            train_loss = []\n            train_auc = []\n            train_f1_score = []\n            train_batches = util.batch_iter(train_data, batch_size=FLAGS.batch_size, num_epochs=FLAGS.num_epochs)\n            for batch in train_batches:\n                batch_size = len(batch)\n\n                m.assign_lr(sess, current_lr)\n                global_step += 1\n\n                left_batch, middle_batch, right_batch, y_batch, a_batch = zip(*batch)\n                feed = {m.left: np.array(left_batch),\n                        m.middle: np.array(middle_batch),\n                        m.right: np.array(right_batch),\n                        m.labels: np.array(y_batch)}\n                if FLAGS.attention:\n                    feed[m.attention] = np.array(a_batch)\n                start_time = time.time()\n                _, loss_value, eval_value = sess.run([m.train_op, m.total_loss, m.eval_op], feed_dict=feed)\n                proc_duration = time.time() - start_time\n                train_loss.append(loss_value)\n                pre, rec = zip(*eval_value)\n                auc = util.calc_auc_pr(pre, rec)\n                f1 = (2.0 * pre[5] * rec[5]) / (pre[5] + rec[5])    # threshold = 0.5\n                train_auc.append(auc)\n                train_f1_score.append(f1)\n\n                assert not np.isnan(loss_value), ""Model loss is NaN.""\n\n                # print log\n                if global_step % FLAGS.log_step == 0:\n                    examples_per_sec = batch_size / proc_duration\n                    format_str = \'%s: step %d/%d, f1 = %.4f, auc = %.4f, loss = %.4f \' + \\\n                                 \'(%.1f examples/sec; %.3f sec/batch), lr: %.6f\'\n                    print format_str % (datetime.now(), global_step, max_steps, f1, auc, loss_value,\n                                        examples_per_sec, proc_duration, current_lr)\n\n                # write summary\n                if global_step % FLAGS.summary_step == 0:\n                    summary_str = sess.run(summary_op)\n                    train_summary_writer.add_summary(summary_str, global_step)\n                    dev_summary_writer.add_summary(summary_str, global_step)\n\n                    # summary loss, f1\n                    train_summary_writer.add_summary(\n                        _summary_for_scalar(\'loss\', np.mean(train_loss)), global_step=global_step)\n                    train_summary_writer.add_summary(\n                        _summary_for_scalar(\'auc\', np.mean(train_auc)), global_step=global_step)\n                    train_summary_writer.add_summary(\n                        _summary_for_scalar(\'f1\', np.mean(train_f1_score)), global_step=global_step)\n\n                    dev_loss, dev_auc, dev_f1 = dev_step(mtest, sess)\n                    dev_summary_writer.add_summary(\n                        _summary_for_scalar(\'loss\', dev_loss), global_step=global_step)\n                    dev_summary_writer.add_summary(\n                        _summary_for_scalar(\'auc\', dev_auc), global_step=global_step)\n                    dev_summary_writer.add_summary(\n                        _summary_for_scalar(\'f1\', dev_f1), global_step=global_step)\n\n                    print ""\\n===== write summary =====""\n                    print ""%s: step %d/%d: train_loss = %.6f, train_auc = %.4f train_f1 = %.4f"" \\\n                          % (datetime.now(), global_step, max_steps,\n                             np.mean(train_loss), np.mean(train_auc), np.mean(train_f1_score))\n                    print ""%s: step %d/%d:   dev_loss = %.6f,   dev_auc = %.4f   dev_f1 = %.4f\\n"" \\\n                          % (datetime.now(), global_step, max_steps, dev_loss, dev_auc, dev_f1)\n\n                    # reset container\n                    train_loss = []\n                    train_auc = []\n                    train_f1_score = []\n\n                # decay learning rate if necessary\n                if loss_value < lowest_loss_value:\n                    lowest_loss_value = loss_value\n                    decay_step_counter = 0\n                else:\n                    decay_step_counter += 1\n                if decay_step_counter >= FLAGS.tolerance_step:\n                    current_lr *= FLAGS.lr_decay\n                    print \'%s: step %d/%d, Learning rate decays to %.5f\' % \\\n                          (datetime.now(), global_step, max_steps, current_lr)\n                    decay_step_counter = 0\n\n                # stop learning if learning rate is too low\n                if current_lr < 1e-5:\n                    break\n\n                # save checkpoint\n                if global_step % FLAGS.checkpoint_step == 0:\n                    saver.save(sess, save_path, global_step=global_step)\n            saver.save(sess, save_path, global_step=global_step)\n\n\ndef _summary_for_scalar(name, value):\n    return tf.Summary(value=[tf.Summary.Value(tag=name, simple_value=float(value))])\n\n\ndef main(argv=None):\n    if not os.path.exists(FLAGS.train_dir):\n        os.mkdir(FLAGS.train_dir)\n\n    # load contextwise dataset\n    source_path = os.path.join(FLAGS.data_dir, \'ids\')\n    target_path = os.path.join(FLAGS.data_dir, \'target.txt\')\n    attention_path = None\n    if FLAGS.attention:\n        if os.path.exists(os.path.join(FLAGS.data_dir, \'source.att\')):\n            attention_path = os.path.join(FLAGS.data_dir, \'source.att\')\n        else:\n            raise ValueError(""Attention file %s not found."", os.path.join(FLAGS.data_dir, \'source.att\'))\n    train_data, test_data = util.read_data_contextwise(source_path, target_path, FLAGS.sent_len,\n                                                       attention_path=attention_path, train_size=FLAGS.train_size)\n    train(train_data, test_data)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
util.py,0,"b'# -*- coding: utf-8 -*-\n\n##########################################################\n#\n# Helper functions to load data\n#\n###########################################################\n\nimport os\nimport re\nfrom codecs import open as codecs_open\nimport cPickle as pickle\nimport numpy as np\n\n\n# Special vocabulary symbols.\nPAD_TOKEN = \'<pad>\' # pad symbol\nUNK_TOKEN = \'<unk>\' # unknown word\nBOS_TOKEN = \'<bos>\' # begin-of-sentence symbol\nEOS_TOKEN = \'<eos>\' # end-of-sentence symbol\nNUM_TOKEN = \'<num>\' # numbers\n\n# we always put them at the start.\n_START_VOCAB = [PAD_TOKEN, UNK_TOKEN]\nPAD_ID = 0\nUNK_ID = 1\n\n# Regular expressions used to tokenize.\n_DIGIT_RE = re.compile(br""^\\d+$"")\n\n\nTHIS_DIR = os.path.abspath(os.path.dirname(__file__))\nRANDOM_SEED = 1234\n\n\ndef basic_tokenizer(sequence, bos=True, eos=True):\n    sequence = re.sub(r\'\\s{2}\', \' \' + EOS_TOKEN + \' \' + BOS_TOKEN + \' \', sequence)\n    if bos:\n        sequence = BOS_TOKEN + \' \' + sequence.strip()\n    if eos:\n        sequence = sequence.strip() + \' \' + EOS_TOKEN\n    return sequence.lower().split()\n\n\ndef create_vocabulary(vocabulary_path, data_path, max_vocabulary_size=40000, tokenizer=None, bos=True, eos=True):\n    """"""Create vocabulary file (if it does not exist yet) from data file.\n\n    Original taken from\n    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/data_utils.py\n    """"""\n    if not os.path.exists(vocabulary_path):\n        print(""Creating vocabulary %s from data %s"" % (vocabulary_path, data_path))\n        vocab = {}\n        with codecs_open(data_path, ""rb"", encoding=""utf-8"") as f:\n            for line in f.readlines():\n                tokens = tokenizer(line) if tokenizer else basic_tokenizer(line, bos, eos)\n                for w in tokens:\n                    word = re.sub(_DIGIT_RE, NUM_TOKEN, w)\n                    if word in vocab:\n                        vocab[word] += 1\n                    else:\n                        vocab[word] = 1\n            vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n            if len(vocab_list) > max_vocabulary_size:\n                print(""  %d words found. Truncate to %d."" % (len(vocab_list), max_vocabulary_size))\n                vocab_list = vocab_list[:max_vocabulary_size]\n            with codecs_open(vocabulary_path, ""wb"", encoding=""utf-8"") as vocab_file:\n                for w in vocab_list:\n                    vocab_file.write(w + b""\\n"")\n\n\ndef initialize_vocabulary(vocabulary_path):\n    """"""Initialize vocabulary from file.\n\n    Original taken from\n    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/data_utils.py\n    """"""\n    if os.path.exists(vocabulary_path):\n        rev_vocab = []\n        with codecs_open(vocabulary_path, ""rb"", encoding=""utf-8"") as f:\n            rev_vocab.extend(f.readlines())\n        rev_vocab = [line.strip() for line in rev_vocab]\n        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n        return vocab, rev_vocab\n    else:\n        raise ValueError(""Vocabulary file %s not found."", vocabulary_path)\n\n\ndef sentence_to_token_ids(sentence, vocabulary, tokenizer=None, bos=True, eos=True):\n    """"""Convert a string to list of integers representing token-ids.\n\n    Original taken from\n    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/data_utils.py\n    """"""\n    words = tokenizer(sentence) if tokenizer else basic_tokenizer(sentence, bos, eos)\n    return [vocabulary.get(re.sub(_DIGIT_RE, NUM_TOKEN, w), UNK_ID) for w in words]\n\n\ndef data_to_token_ids(data_path, target_path, vocabulary_path, tokenizer=None, bos=True, eos=True):\n    """"""Tokenize data file and turn into token-ids using given vocabulary file.\n\n    Original taken from\n    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/data_utils.py\n    """"""\n    if not os.path.exists(target_path):\n        print(""Vectorizing data in %s"" % data_path)\n        vocab, _ = initialize_vocabulary(vocabulary_path)\n        with codecs_open(data_path, ""rb"", encoding=""utf-8"") as data_file:\n            with codecs_open(target_path, ""wb"", encoding=""utf-8"") as tokens_file:\n                for line in data_file:\n                    token_ids = sentence_to_token_ids(line, vocab, tokenizer, bos, eos)\n                    tokens_file.write("" "".join([str(tok) for tok in token_ids]) + ""\\n"")\n\n\ndef shuffle_split(X, y, a=None, train_size=10000, shuffle=True):\n    """"""Shuffle and split data into train and test subset""""""\n    _X = np.array(X)\n    _y = np.array(y)\n    assert _X.shape[0] == _y.shape[0]\n\n    _a = [None] * _y.shape[0]\n    if a is not None and len(a) == len(y):\n        _a = np.array(a)\n        # compute softmax\n        _a = np.reshape(np.exp(_a) / np.sum(np.exp(_a)), (_y.shape[0], 1))\n        assert _a.shape[0] == _y.shape[0]\n\n    print ""Splitting data..."",\n    # split train-test\n    data = np.array(zip(_X, _y, _a))\n    data_size = _y.shape[0]\n    if train_size > data_size:\n        train_size = int(data_size * 0.9)\n    if shuffle:\n        np.random.seed(RANDOM_SEED)\n        shuffle_indices = np.random.permutation(np.arange(data_size))\n        shuffled_data = data[shuffle_indices]\n    else:\n        shuffled_data = data\n    print ""\\t%d for train, %d for test"" % (train_size, data_size - train_size)\n    return shuffled_data[:train_size], shuffled_data[train_size:]\n\n\ndef read_data(source_path, target_path, sent_len, attention_path=None, train_size=10000, shuffle=True):\n    """"""Read source(x), target(y) and attention if given.\n\n    Original taken from\n    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py\n    """"""\n    _X = []\n    _y = []\n    with codecs_open(source_path, mode=""r"", encoding=""utf-8"") as source_file:\n        with codecs_open(target_path, mode=""r"", encoding=""utf-8"") as target_file:\n            source, target = source_file.readline(), target_file.readline()\n            #counter = 0\n            print ""Loading data..."",\n            while source and target:\n                #counter += 1\n                #if counter % 1000 == 0:\n                #    print(""  reading data line %d"" % counter)\n                #    sys.stdout.flush()\n                source_ids = [np.int64(x.strip()) for x in source.split()]\n                if sent_len > len(source_ids):\n                    source_ids += [PAD_ID] * (sent_len - len(source_ids))\n                assert len(source_ids) == sent_len\n\n                #target = target.split(\'\\t\')[0].strip()\n                target_ids = [np.float32(y.strip()) for y in target.split()]\n\n                _X.append(source_ids)\n                _y.append(target_ids)\n                source, target = source_file.readline(), target_file.readline()\n\n    assert len(_X) == len(_y)\n    print ""\\t%d examples found."" % len(_y)\n\n    _a = None\n    if attention_path is not None:\n        with codecs_open(attention_path, mode=""r"", encoding=""utf-8"") as att_file:\n            _a = [np.float32(att.strip()) for att in att_file.readlines()]\n            assert len(_a) == len(_y)\n\n    return shuffle_split(_X, _y, a=_a, train_size=train_size, shuffle=shuffle)\n\n\ndef shuffle_split_contextwise(X, y, a=None, train_size=10000, shuffle=True):\n    """"""Shuffle and split data into train and test subset""""""\n\n    _left = np.array(X[\'left\'])\n    _middle = np.array(X[\'middle\'])\n    _right = np.array(X[\'right\'])\n    _y = np.array(y)\n\n    _a = [None] * _y.shape[0]\n    if a is not None and len(a) == len(y):\n        _a = np.array(a)\n        # compute softmax\n        _a = np.reshape(np.exp(_a) / np.sum(np.exp(_a)), (_y.shape[0], 1))\n        assert _a.shape[0] == _y.shape[0]\n\n    print ""Splitting data..."",\n    # split train-test\n    data = np.array(zip(_left, _middle, _right, _y, _a))\n    data_size = _y.shape[0]\n    if train_size > data_size:\n        train_size = int(data_size * 0.9)\n    if shuffle:\n        np.random.seed(RANDOM_SEED)\n        shuffle_indices = np.random.permutation(np.arange(data_size))\n        shuffled_data = data[shuffle_indices]\n    else:\n        shuffled_data = data\n    print ""\\t%d for train, %d for test"" % (train_size, data_size - train_size)\n    return shuffled_data[:train_size], shuffled_data[train_size:]\n\n\ndef read_data_contextwise(source_path, target_path, sent_len, attention_path=None, train_size=10000, shuffle=True):\n    """"""Read source file and pad the sequence to sent_len,\n       combine them with target (and attention if given).\n\n    Original taken from\n    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py\n    """"""\n    print ""Loading data..."",\n    _X = {\'left\': [], \'middle\': [], \'right\': []}\n    for context in _X.keys():\n        path = \'%s.%s\' % (source_path, context)\n        with codecs_open(path, mode=""r"", encoding=""utf-8"") as source_file:\n            for source in source_file.readlines():\n                source_ids = [np.int64(x.strip()) for x in source.split()]\n                if sent_len > len(source_ids):\n                    source_ids += [PAD_ID] * (sent_len - len(source_ids))\n                assert len(source_ids) == sent_len\n                _X[context].append(source_ids)\n    assert len(_X[\'left\']) == len(_X[\'middle\'])\n    assert len(_X[\'right\']) == len(_X[\'middle\'])\n\n    _y = []\n    with codecs_open(target_path, mode=""r"", encoding=""utf-8"") as target_file:\n        for target in target_file.readlines():\n            target_ids = [np.float32(y.strip()) for y in target.split()]\n            _y.append(target_ids)\n    assert len(_X[\'left\']) == len(_y)\n    print ""\\t%d examples found."" % len(_y)\n\n    _a = None\n    if attention_path is not None:\n        with codecs_open(attention_path, mode=""r"", encoding=""utf-8"") as att_file:\n            _a = [np.float32(att.strip()) for att in att_file.readlines()]\n            assert len(_a) == len(_y)\n\n    return shuffle_split_contextwise(_X, _y, a=_a, train_size=train_size, shuffle=shuffle)\n\n\ndef batch_iter(data, batch_size, num_epochs, shuffle=True):\n    """"""Generates a batch iterator.\n\n    Original taken from\n    https://github.com/dennybritz/cnn-text-classification-tf/blob/master/data_helpers.py\n    """"""\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int(np.ceil(float(data_size)/batch_size))\n    for epoch in range(num_epochs):\n        # Shuffle data at each epoch\n        if shuffle:\n            #np.random.seed(RANDOM_SEED)\n            shuffle_indices = np.random.permutation(np.arange(data_size))\n            shuffled_data = data[shuffle_indices]\n        else:\n            shuffled_data = data\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]\n\n\ndef dump_to_file(filename, obj):\n    with open(filename, \'wb\') as outfile:\n        pickle.dump(obj, file=outfile)\n    return\n\n\ndef load_from_dump(filename):\n    with open(filename, \'rb\') as infile:\n        obj = pickle.load(infile)\n    return obj\n\n\ndef _load_bin_vec(fname, vocab):\n    """"""\n    Loads 300x1 word vecs from Google (Mikolov) word2vec\n\n    Original taken from\n    https://github.com/yuhaozhang/sentence-convnet/blob/master/text_input.py\n    """"""\n    word_vecs = {}\n    with open(fname, ""rb"") as f:\n        header = f.readline()\n        vocab_size, layer1_size = map(int, header.split())\n        binary_len = np.dtype(\'float32\').itemsize * layer1_size\n        for line in xrange(vocab_size):\n            word = []\n            while True:\n                ch = f.read(1)\n                if ch == \' \':\n                    word = \'\'.join(word)\n                    break\n                if ch != \'\\n\':\n                    word.append(ch)\n            if word in vocab:\n                word_vecs[word] = np.fromstring(f.read(binary_len), dtype=\'float32\')\n            else:\n                f.read(binary_len)\n    return (word_vecs, layer1_size)\n\n\ndef _add_random_vec(word_vecs, vocab, emb_size=300):\n    for word in vocab:\n        if word not in word_vecs:\n            word_vecs[word] = np.random.uniform(-0.25,0.25,emb_size)\n    return word_vecs\n\n\ndef prepare_pretrained_embedding(fname, word2id):\n    print \'Reading pretrained word vectors from file ...\'\n    word_vecs, emb_size = _load_bin_vec(fname, word2id)\n    word_vecs = _add_random_vec(word_vecs, word2id, emb_size)\n    embedding = np.zeros([len(word2id), emb_size])\n    for w,idx in word2id.iteritems():\n        embedding[idx,:] = word_vecs[w]\n    print \'Generated embeddings with shape \' + str(embedding.shape)\n    return embedding\n\n\ndef offset(array, pre, post):\n    ret = np.array(array)\n    ret = np.insert(ret, 0, pre)\n    ret = np.append(ret, post)\n    return ret\n\n\ndef calc_auc_pr(precision, recall):\n    assert len(precision) == len(recall)\n    return np.trapz(offset(precision, 1, 0), x=offset(recall, 0, 1), dx=5)\n\n\ndef prepare_ids(data_dir, vocab_path):\n    for context in [\'left\', \'middle\', \'right\', \'txt\']:\n        data_path = os.path.join(data_dir, \'mlmi\', \'source.%s\' % context)\n        target_path = os.path.join(data_dir, \'mlmi\', \'ids.%s\' % context)\n        if context == \'left\':\n            bos, eos = True, False\n        elif context == \'middle\':\n            bos, eos = False, False\n        elif context == \'right\':\n            bos, eos = False, True\n        else:\n            bos, eos = True, True\n        data_to_token_ids(data_path, target_path, vocab_path, bos=bos, eos=eos)\n\n\ndef main():\n    data_dir = os.path.join(THIS_DIR, \'data\')\n\n    # multi-label multi-instance (MLMI-CNN) dataset\n    vocab_path = os.path.join(data_dir, \'mlmi\', \'vocab.txt\')\n    data_path = os.path.join(data_dir, \'mlmi\', \'source.txt\')\n    max_vocab_size = 36500\n    create_vocabulary(vocab_path, data_path, max_vocab_size)\n    prepare_ids(data_dir, vocab_path)\n\n    # pretrained embeddings\n    embedding_path = os.path.join(THIS_DIR, \'word2vec\', \'GoogleNews-vectors-negative300.bin\')\n    if os.path.exists(embedding_path):\n        word2id, _ = initialize_vocabulary(vocab_path)\n        embedding = prepare_pretrained_embedding(embedding_path, word2id)\n        np.save(os.path.join(data_dir, \'mlmi\', \'emb.npy\'), embedding)\n    else:\n        print ""Pretrained embeddings file %s not found."" % embedding_path\n\n    # single-label single-instance (ER-CNN) dataset\n    vocab_er = os.path.join(data_dir, \'er\', \'vocab.txt\')\n    data_er = os.path.join(data_dir, \'er\', \'source.txt\')\n    target_er = os.path.join(data_dir, \'er\', \'ids.txt\')\n    max_vocab_size = 11500\n    tokenizer = lambda x: x.split()\n    create_vocabulary(vocab_er, data_er, max_vocab_size, tokenizer=tokenizer)\n    data_to_token_ids(data_er, target_er, vocab_er, tokenizer=tokenizer)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
