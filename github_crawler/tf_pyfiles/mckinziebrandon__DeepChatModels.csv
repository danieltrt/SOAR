file_path,api_count,code
main.py,3,"b'#!/usr/bin/env python3\n\n""""""main.py: Train and/or chat with a bot. (work in progress).\n\nTypical use cases:\n    1.  Train a model specified by yaml config file, located at\n        path_to/my_config.yml, where paths are relative to project root:\n            ./main.py --config path_to/my_config.yml\n\n    2.  Train using mix of yaml config and cmd-line args, with\n        command-line args taking precedence over any values.\n            ./main.py \\\n                --config path_to/my_config.yml \\\n                --model_params ""{\'batch_size\': 32, \'optimizer\': \'RMSProp\'}""\n\n    3.  Load a pretrained model that was saved in path_to/pretrained_dir,\n        which is assumed to be relative to the project root.\n            ./main.py --pretrained_dir path_to/pretrained_dir\n\n""""""\n\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport os\n# Meaning of values:\n#   1: INFO messages are not printed.\n#   2: INFO, WARNING messages are not printed.\n# I\'m temporarily making the default \'2\' since the TF master\n# branch (as of May 6) is spewing warnings that are clearly\n# due to bugs on their side.\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nimport data\nimport chatbot\nimport logging\nimport tensorflow as tf\nfrom pydoc import locate\nfrom utils import io_utils\n\n# =============================================================================\n# FLAGS: Command line argument parser from TensorFlow.\n# =============================================================================\n\nflags = tf.app.flags\nflags.DEFINE_string(\n    flag_name=""pretrained_dir"",\n    default_value=None,\n    docstring=""relative path to a pretrained model directory.""\n              ""It is assumed that the model is one from this repository, and ""\n              "" thus has certain files that are generated after any training""\n              "" session (TL;DR: any ckpt_dir you\'ve trained previously)."")\nflags.DEFINE_string(\n    flag_name=""config"",\n    default_value=None,\n    docstring=""relative path to a valid yaml config file.""\n              "" For example: configs/example_cornell.yml"")\nflags.DEFINE_string(\n    flag_name=""debug"",\n    default_value=False,\n    docstring=""If true, increases output verbosity (log levels)."")\nflags.DEFINE_string(\n    flag_name=""model"",\n    default_value=""{}"",\n    docstring=""Options: chatbot.{DynamicBot,Simplebot,ChatBot}."")\nflags.DEFINE_string(\n    flag_name=""model_params"",\n    default_value=""{}"",\n    docstring=""Configuration dictionary, with supported keys specified by""\n              "" those in chatbot.globals.py."")\nflags.DEFINE_string(\n    flag_name=""dataset"",\n    default_value=""{}"",\n    docstring=""Name (capitalized) of dataset to use.""\n              "" Options: [data.]{Cornell,Ubuntu,Reddit}.""\n              "" - Legend: [optional] {Pick,One,Of,These}."")\nflags.DEFINE_string(\n    flag_name=""dataset_params"",\n    default_value=""{}"",\n    docstring=""Configuration dictionary, with supported keys specified by""\n              "" those in chatbot.globals.py."")\nFLAGS = flags.FLAGS\n\n\ndef start_training(dataset, bot):\n    """"""Train bot. \n    \n    Will expand this function later to aid interactivity/updates.\n    """"""\n    print(""Training bot. CTRL-C to stop training."")\n    bot.train(dataset)\n\n\ndef start_chatting(bot):\n    """"""Talk to bot. \n    \n    Will re-add teacher mode soon. Old implementation in _decode.py.""""""\n    print(""Initiating chat session."")\n    print(""Your bot has a temperature of %.2f."" % bot.temperature, end="" "")\n    if bot.temperature < 0.1:\n        print(""Not very adventurous, are we?"")\n    elif bot.temperature < 0.7:\n        print(""This should be interesting . . . "")\n    else:\n        print(""Enjoy your gibberish!"")\n    bot.chat()\n\n\ndef main(argv):\n\n    if FLAGS.debug:\n        # Setting to \'0\': all tensorflow messages are logged.\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'0\'\n        logging.basicConfig(level=logging.INFO)\n\n    # Extract the merged configs/dictionaries.\n    config = io_utils.parse_config(flags=FLAGS)\n    if config[\'model_params\'][\'decode\'] and config[\'model_params\'][\'reset_model\']:\n        print(""Woops! You passed {decode: True, reset_model: True}."" \n              "" You can\'t chat with a reset bot! I\'ll set reset to False."")\n        config[\'model_params\'][\'reset_model\'] = False\n\n    # If loading from pretrained, double-check that certain values are correct.\n    # (This is not something a user need worry about -- done automatically)\n    if FLAGS.pretrained_dir is not None:\n        assert config[\'model_params\'][\'decode\'] \\\n               and not config[\'model_params\'][\'reset_model\']\n\n    # Print out any non-default parameters given by user, so as to reassure\n    # them that everything is set up properly.\n    io_utils.print_non_defaults(config)\n\n    print(""Setting up %s dataset."" % config[\'dataset\'])\n    dataset_class = locate(config[\'dataset\']) or getattr(data, config[\'dataset\'])\n    dataset = dataset_class(config[\'dataset_params\'])\n    print(""Creating"", config[\'model\'], "". . . "")\n    bot_class = locate(config[\'model\']) or getattr(chatbot, config[\'model\'])\n    bot = bot_class(dataset, config)\n\n    if not config[\'model_params\'][\'decode\']:\n        start_training(dataset, bot)\n    else:\n        start_chatting(bot)\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(\'ERROR\')\n    tf.app.run()\n\n'"
setup.py,0,"b""from setuptools import setup\n\nsetup(name='DeepChatModels',\n        description='Conversation Models in TensorFlow',\n        url='http://github.com/mckinziebrandon/DeepChatModels',\n        author_email='mckinziebrandon@berkeley.edu',\n        license='MIT',\n        install_requires=[\n            'numpy',\n            'matplotlib',\n            'pandas',\n            'pyyaml',\n            ],\n        extras_require={'tensorflow': ['tensorflow'],\n            'tensorflow gpu': ['tensorflow-gpu']},\n        zip_safe=False)\n"""
chatbot/__init__.py,0,"b""from chatbot import globals\nfrom chatbot.components.base._rnn import  *\nfrom chatbot.components.bot_ops import dynamic_sampled_softmax_loss\nfrom chatbot.components.decoders import *\nfrom chatbot.components.embedder import *\nfrom chatbot.components.encoders import *\nfrom chatbot.dynamic_models import DynamicBot\nfrom chatbot.legacy.legacy_models import ChatBot, SimpleBot\n\n__all__ = ['Chatbot, SimpleBot', 'DynamicBot']\n"""
chatbot/_models.py,20,"b'""""""Abstract classes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport copy\nimport yaml\nimport random\nimport subprocess\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector\nfrom tensorflow.python.client import device_lib\nfrom utils import io_utils\nfrom chatbot.components import *\nfrom chatbot.globals import DEFAULT_FULL_CONFIG, OPTIMIZERS\n\n\ndef gpu_found():\n    """"""Returns True if tensorflow finds at least 1 GPU.""""""\n    devices = device_lib.list_local_devices()\n    return len([x.name for x in devices if x.device_type == \'GPU\']) > 0\n\n\nclass Model(object):\n    """"""Superclass of all subsequent model classes.\n    """"""\n\n    def __init__(self, logger, dataset, params):\n        """"""\n        Args:\n            logger: returned by getLogger & called by subclasses. Passed\n                    here so we know what object to use for info/warn/error.\n            dataset: object that inherits from data.Dataset.\n            params: (dict) user-specified params that override those in\n                           DEFAULT_FULL_CONFIG above.\n        """"""\n\n        self.log = logger\n        self.__dict__[\'__params\'] = Model.fill_params(dataset, params)\n\n        # Make particularly useful ckpt directories for website configurations.\n        if \'website_config\' in self.ckpt_dir:\n            self.ckpt_dir = Model._build_hparam_path(\n                ckpt_dir=self.ckpt_dir,\n                num_layers=self.num_layers,\n                max_seq_len=self.max_seq_len)\n            self.log.info(""New ckpt dir:"", self.ckpt_dir)\n\n        # Configure gpu options if we are using one.\n        if gpu_found():\n            self.log.info(""GPU Found. Setting allow_growth to True."")\n            gpu_config = tf.ConfigProto()\n            gpu_config.gpu_options.allow_growth = True\n            self.sess = tf.Session(config=gpu_config)\n        else:\n            self.log.warning(""GPU not found. Not recommended for training."")\n            self.sess = tf.Session()\n\n        with self.graph.name_scope(tf.GraphKeys.SUMMARIES):\n            self.global_step = tf.Variable(initial_value=0, trainable=False)\n            self.learning_rate = tf.constant(self.learning_rate)\n\n        # Create ckpt_dir if user hasn\'t already (if exists, has no effect).\n        subprocess.call([\'mkdir\', \'-p\', self.ckpt_dir])\n        self.projector_config = projector.ProjectorConfig()\n        # Good practice to set as None in constructor.\n        self.loss = None\n        self.file_writer = None\n        self.merged = None\n        self.train_op = None\n        self.saver = None\n\n    def compile(self):\n        """""" Configure training process and initialize model. Inspired by Keras.\n\n        Either restore model parameters or create fresh ones.\n            - Checks if we can both (1) find a checkpoint state, and (2) a\n            valid V1/V2 checkpoint path.\n            - If we can\'t, then just re-initialize model with fresh params.\n        """"""\n\n        self.log.info(""Checking for checkpoints . . ."")\n        checkpoint_state  = tf.train.get_checkpoint_state(self.ckpt_dir)\n\n        if not self.reset_model and checkpoint_state \\\n                and tf.train.checkpoint_exists(checkpoint_state.model_checkpoint_path):\n            print(""Reading model parameters from"",\n                  checkpoint_state.model_checkpoint_path)\n            self.file_writer = tf.summary.FileWriter(self.ckpt_dir)\n            self.saver = tf.train.Saver(tf.global_variables())\n            self.saver.restore(self.sess, checkpoint_state.model_checkpoint_path)\n        else:\n            print(""Created model with fresh parameters:\\n\\t"", self.ckpt_dir)\n            # Recursively delete all files in output but keep directories.\n            subprocess.call([\n                \'find\', self.ckpt_dir, \'-type\', \'f\', \'-exec\', \'rm\', \'{}\', \';\'\n            ])\n            self.file_writer = tf.summary.FileWriter(self.ckpt_dir)\n            # Add operation for calling all variable initializers.\n            init_op = tf.global_variables_initializer()\n            # Construct saver (adds save/restore ops to all).\n            self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=3)\n            # Add the fully-constructed graph to the event file.\n            self.file_writer.add_graph(self.sess.graph)\n            # Initialize all model variables.\n            self.sess.run(init_op)\n            # Store model config in ckpt dir for easy loading later.\n            with open(os.path.join(self.ckpt_dir, \'config.yml\'), \'w\') as f:\n                yaml.dump(getattr(self, ""params""), f, default_flow_style=False)\n\n    def save(self, summaries=None):\n        """"""\n        Args:\n            summaries: merged summary instance returned by session.run.\n        """"""\n\n        if self.saver is None:\n            raise ValueError(""Tried saving model before defining a saver."")\n\n        ckpt_fname = os.path.join(self.ckpt_dir, ""{}.ckpt"".format(self.data_name))\n        # Saves the state of all global variables in a ckpt file.\n        self.saver.save(self.sess, ckpt_fname, global_step=self.global_step)\n\n        if summaries is not None:\n            self.file_writer.add_summary(summaries, self.global_step.eval(self.sess))\n        else:\n            self.log.info(""Save called without summaries."")\n\n    def close(self, save_current=True):\n        """"""Call then when training session is terminated.\n            - Saves the current model/checkpoint state.\n            - Freezes the model into a protobuf file in self.ckpt_dir.\n            - Closes context managers for file_writing and session.\n        """"""\n        # First save the checkpoint as usual.\n        if save_current:\n            self.save()\n        # Freeze me, for I am infinite.\n        self.freeze()\n        # Be a responsible bot and close my file writer.\n        self.file_writer.close()\n        # Formally exit the session, farewell to all.\n        self.sess.close()\n\n    @property\n    def graph(self):\n        return self.sess.graph\n\n    @staticmethod\n    def fill_params(dataset, params):\n        """"""For now, essentially just returns (already parsed) params, \n        but placed here in case I want to customize later (likely).\n        """"""\n        # Replace (string) specification of dataset with the actual instance.\n        params[\'dataset\'] = dataset\n        params[\'dataset_params\'][\'data_name\'] = dataset.name\n        if params[\'model_params\'][\'ckpt_dir\'] == \'out\':\n            params[\'model_params\'][\'ckpt_dir\'] += \'/\'+dataset.name\n        # Define alias in case older models still use it.\n        params[\'model_params\'][\'is_chatting\'] = params[\'model_params\'][\'decode\']\n        return params\n\n    def freeze(self):\n        """"""Useful for e.g. deploying model on website.\n\n        Args: directory containing model ckpt files we\'d like to freeze.\n        """"""\n\n        if not tf.get_collection(\'freezer\'):\n            self.log.warning(\'No freezer found. Not saving a frozen model.\')\n            return\n\n        # Note: output_node_names is only used to tell tensorflow what is can\n        # throw away in the frozen graph (e.g. training ops).\n        output_node_names = "","".join(\n            [t.name.rstrip(\':0\') for t in tf.get_collection(\'freezer\')])\n        self.log.info(\'Output node names: %r\', output_node_names)\n\n        # Save a graph with only the bare necessities for chat sessions.\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\n            self.sess, self.graph.as_graph_def(), output_node_names.split(\',\'))\n\n        output_fname = os.path.join(self.ckpt_dir, ""frozen_model.pb"")\n        with tf.gfile.GFile(output_fname, \'wb\') as f:\n            f.write(output_graph_def.SerializeToString())\n        print(""%d ops in the final graph."" % len(output_graph_def.node))\n        subprocess.call([\'cp\', self.dataset.paths[\'vocab\'], self.ckpt_dir])\n\n    def __getattr__(self, name):\n        if name == \'params\':\n            camel_case = self.data_name.title().replace(\'_\', \'\')\n            replace_dict = {\'dataset\': ""data.""+camel_case}\n            return {**self.__dict__[\'__params\'], **replace_dict}\n        elif name in DEFAULT_FULL_CONFIG: # Requesting a top-level key.\n            return self.__dict__[\'__params\'][name]\n        else:\n            for k in DEFAULT_FULL_CONFIG.keys():\n                if not isinstance(self.__dict__[\'__params\'][k], dict):\n                    continue\n                if name in self.__dict__[\'__params\'][k]:\n                    return self.__dict__[\'__params\'][k][name]\n        raise AttributeError(name)\n\n    @staticmethod\n    def _build_hparam_path(ckpt_dir, **kwargs):\n        """"""Returns relative path build from args for descriptive checkpointing.\n\n        The new path becomes ckpt_dir appended with directories named by kwargs:\n            - If a given kwargs[key] is a string, that is set as the \n              appended dir name.\n            - Otherwise, it gets formatted, e.g. for key=\'learning_rate\' it \n              may become \'learning_rate_0_001\'\n\n        Returns:\n            ckpt_dir followed by sequentially appended directories, \n            named by kwargs.\n        """"""\n        kwargs = copy.deepcopy(kwargs)\n        new_ckpt_dir = ckpt_dir\n        for key in sorted(kwargs):\n            if not isinstance(kwargs[key], str):\n                dir_name = key + ""_"" + str(kwargs[key]).replace(\'.\', \'_\')\n            else:\n                dir_name = kwargs[key]\n            new_ckpt_dir = os.path.join(new_ckpt_dir, dir_name)\n        return new_ckpt_dir\n\n\nclass BucketModel(Model):\n    """"""Abstract class. Any classes that extend BucketModel just need to customize their\n        graph structure in __init__ and implement the step(...) function.\n        The real motivation for making this was to be able to use the true Model\n        abstract class for all classes in this directory, bucketed or not, r1.0 or r0.12.\n    """"""\n\n    def __init__(self, logger, buckets, dataset, params):\n        self.buckets = buckets\n        super(BucketModel, self).__init__(\n            logger=logger,\n            dataset=dataset,\n            params=params)\n\n    def compile(self):\n        """""" Configure training process. Name was inspired by Keras. <3 """"""\n\n        if self.losses is None:\n            raise ValueError(""Tried compiling model before defining losses."")\n\n        print(""Configuring training operations. This may take some time . . . "")\n        # Note: variables are trainable=True by default.\n        params = tf.trainable_variables()\n        # train_op will store the parameter (S)GD train_op.\n        self.apply_gradients = []\n        optimizer = OPTIMIZERS[self.optimizer](self.learning_rate)\n        for b in range(len(self.buckets)):\n            gradients = tf.gradients(self.losses[b], params)\n            # Gradient clipping is actually extremely simple, it basically just\n            # checks if L2Norm(gradients) > max_gradient, and if it is,\n            # it returns (gradients / L2Norm(gradients)) * max_grad.\n            clipped_gradients, _ = tf.clip_by_global_norm(\n                gradients, self.max_gradient)\n            self.apply_gradients.append(optimizer.apply_gradients(\n                zip(clipped_gradients, params),global_step=self.global_step))\n\n        super(BucketModel, self).compile()\n\n    def check_input_lengths(self, inputs, expected_lengths):\n        """"""\n        Raises:\n            ValueError: if length of encoder_inputs, decoder_inputs, or\n            target_weights disagrees with bucket size for the specified bucket_id.\n        """"""\n        for input, length in zip(inputs, expected_lengths):\n            if len(input) != length:\n                raise ValueError(""Input length doesn\'t match bucket size:""\n                                 "" %d != %d."" % (len(input), length))\n\n    def get_batch(self, data, bucket_id):\n        """"""Get a random batch of data from the specified bucket, prepare for step.\n\n        Args:\n          data: tuple of len(self.buckets). data[bucket_id] == [source_ids, target_ids]\n          bucket_id: integer, which bucket to get the batch for.\n\n        Returns:\n          The triple (encoder_inputs, decoder_inputs, target_weights) for\n          the constructed batch that has the proper format to call step(...) later.\n        """"""\n        encoder_size, decoder_size = self.buckets[bucket_id]\n        encoder_inputs, decoder_inputs = [], []\n\n        # Get a random batch of encoder and decoder inputs from data,\n        # pad them if needed, reverse encoder inputs and add GO to decoder.\n        for _ in range(self.batch_size):\n            encoder_input, decoder_input = random.choice(data[bucket_id])\n            # BasicEncoder inputs are padded and then reversed.\n            encoder_pad = [io_utils.PAD_ID] * (encoder_size - len(encoder_input))\n            encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n            # DynamicDecoder inputs get an extra ""GO"" symbol, and are padded then.\n            decoder_pad= [io_utils.PAD_ID] * (decoder_size - len(decoder_input) - 1)\n            decoder_inputs.append([io_utils.GO_ID] + decoder_input + decoder_pad)\n\n        # Define some small helper functions before we re-index & weight.\n        def inputs_to_unit(uid, inputs):\n            """""" Return re-indexed version of inputs array. Description in params below.\n            :param uid: index identifier for input timestep/unit/node of interest.\n            :param inputs:  single batch of data; inputs[i] is i\'th sentence.\n            :return:        re-indexed version of inputs as numpy array.\n            """"""\n            return np.array([inputs[i][uid] for i in range(self.batch_size)], dtype=np.int32)\n\n        batch_encoder_inputs = [inputs_to_unit(i, encoder_inputs) for i in range(encoder_size)]\n        batch_decoder_inputs = [inputs_to_unit(i, decoder_inputs) for i in range(decoder_size)]\n        batch_weights        = list(np.ones(shape=(decoder_size, self.batch_size), dtype=np.float32))\n\n        # Set weight for the final decoder unit to 0.0 for all batches.\n        for i in range(self.batch_size):\n            batch_weights[-1][i] = 0.0\n\n        # Also set any decoder-input-weights to 0 that have PAD\n        # as target decoder output.\n        for unit_id in range(decoder_size - 1):\n            ids_with_pad_target = [b for b in range(self.batch_size)\n                                   if decoder_inputs[b][unit_id+1] == io_utils.PAD_ID]\n            batch_weights[unit_id][ids_with_pad_target] = 0.0\n\n        return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n\n    def train(self, dataset):\n        """""" Train chatbot. """"""\n        from chatbot.legacy._train import train\n        train(self, dataset)\n\n    def decode(self):\n        """""" Create chat session between user & chatbot. """"""\n        from chatbot.legacy._decode import decode\n        decode(self)\n\n    def step(self, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only=False):\n        """"""Run a step of the model.\n\n        Args:\n          encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n          decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n          target_weights: list of numpy float vectors to feed as target weights.\n          bucket_id: which bucket of the model to use.\n        """"""\n        raise NotImplemented\n\n\n\n\n'"
chatbot/dynamic_models.py,27,"b'""""""Sequence-to-sequence models with dynamic unrolling and faster embedding \ntechniques.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport time\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom chatbot import components\nfrom chatbot.components import bot_ops, Embedder, InputPipeline\nfrom chatbot._models import Model\nfrom utils import io_utils\nfrom pydoc import locate\n\n\nclass DynamicBot(Model):\n    """""" General sequence-to-sequence model for conversations. \n    \n    Will eventually support beam search, and a wider variety of \n    cell options. At present, supports multi-layer encoder/decoders, \n    GRU/LSTM cells, attention, and dynamic unrolling (online decoding included). \n    \n    Additionally, will eventually support biologically inspired mechanisms for \n    learning, such as hebbian-based update rules.\n    """"""\n\n    def __init__(self, dataset, params):\n        """"""Build the model computation graph.\n        \n        Args:\n            dataset: any instance inheriting from data.DataSet.\n            params: dictionary of hyperparameters.\n                    For supported keys, see DEFAULT_FULL_CONFIG.\n                    (chatbot.globals.py)\n        """"""\n\n        self.log = logging.getLogger(\'DynamicBotLogger\')\n        # Let superclass handle common bookkeeping (saving/loading/dir paths).\n        super(DynamicBot, self).__init__(self.log, dataset, params)\n        # Build the model\'s structural components.\n        self.build_computation_graph(dataset)\n        # Configure training and evaluation.\n        # Note: this is distinct from build_computation_graph for historical\n        # reasons, and I plan on refactoring. Initially, I more or less followed\n        # the feel of Keras for setting up models, but after incorporating the\n        # YAML configuration files, this seems rather unnecessary.\n        self.compile()\n\n    def build_computation_graph(self, dataset):\n        """"""Create the TensorFlow model graph. Note that this only builds the \n        structural components, i.e. nothing related to training parameters,\n        optimization, etc. \n        \n        The main components to be built (in order): \n            1. InputPipeline\n            2. Embedder\n               - single object shared between encoder/decoder.\n               - creates distict embeddings for distinct variable scopes.\n            2. Encoder\n            3. Decoder\n        """"""\n\n        # Grab the model classes (Constructors) specified by user in params.\n        encoder_class = locate(getattr(self, \'encoder.class\')) \\\n                        or getattr(components, getattr(self, \'encoder.class\'))\n        decoder_class = locate(getattr(self, \'decoder.class\')) \\\n                        or getattr(components, getattr(self, \'decoder.class\'))\n\n        assert encoder_class is not None, ""Couldn\'t find requested %s."" % \\\n                                          self.model_params[\'encoder.class\']\n        assert decoder_class is not None, ""Couldn\'t find requested %s."" % \\\n                                          self.model_params[\'decoder.class\']\n\n        # Organize input pipeline inside single node for clean visualization.\n        self.pipeline = InputPipeline(\n            file_paths=dataset.paths,\n            batch_size=self.batch_size,\n            is_chatting=self.is_chatting)\n\n        # Grab the input feeds for encoder/decoder from the pipeline.\n        encoder_inputs = self.pipeline.encoder_inputs\n        self.decoder_inputs = self.pipeline.decoder_inputs\n\n        # Create embedder object -- handles all of your embedding needs!\n        # By passing scope to embedder calls, we can create distinct embeddings,\n        # while storing inside the same embedder object.\n        self.embedder = Embedder(\n            self.vocab_size,\n            self.embed_size,\n            l1_reg=self.l1_reg)\n\n        # Explicitly show required parameters for any subclass of\n        # chatbot.components.base.RNN (e.g. encoders/decoders).\n        # I do this for readability; you can easily tell below which additional\n        # params are needed, e.g. for a decoder.\n        rnn_params = {\n            \'state_size\': self.state_size,\n            \'embed_size\': self.embed_size,\n            \'num_layers\': self.num_layers,\n            \'dropout_prob\': self.dropout_prob,\n            \'base_cell\': self.base_cell}\n\n        with tf.variable_scope(\'encoder\'):\n            embedded_enc_inputs = self.embedder(encoder_inputs)\n            # For now, encoders require just the RNN params when created.\n            encoder = encoder_class(**rnn_params)\n            # Apply embedded inputs to encoder for the final (context) state.\n            encoder_outputs, encoder_state = encoder(embedded_enc_inputs)\n\n        with tf.variable_scope(""decoder""):\n            embedded_dec_inputs = self.embedder(self.decoder_inputs)\n            # Sneaky. Would be nice to have a ""cleaner"" way of doing this.\n            if getattr(self, \'attention_mechanism\', None) is not None:\n                rnn_params[\'attention_mechanism\'] = self.attention_mechanism\n            self.decoder = decoder_class(\n                encoder_outputs=encoder_outputs,\n                vocab_size=self.vocab_size,\n                max_seq_len=dataset.max_seq_len,\n                temperature=self.temperature,\n                **rnn_params)\n\n            # For decoder outpus, we want the full sequence (output sentence),\n            # not simply the last.\n            decoder_outputs, decoder_state = self.decoder(\n                embedded_dec_inputs,\n                initial_state=encoder_state,\n                is_chatting=self.is_chatting,\n                loop_embedder=self.embedder)\n\n        self.outputs = tf.identity(decoder_outputs, name=\'outputs\')\n        # Tag inputs and outputs by name should we want to freeze the model.\n        tf.add_to_collection(\'freezer\', encoder_inputs)\n        tf.add_to_collection(\'freezer\', self.outputs)\n        # Merge any summaries floating around in the aether into one object.\n        self.merged = tf.summary.merge_all()\n\n    def compile(self):\n        """""" TODO: perhaps merge this into __init__?\n        Originally, this function accepted training/evaluation specific \n        parameters. However, since moving the configuration parameters to .yaml \n        files and interfacing with the dictionary, no args are needed here, \n        and thus would mainly be a hassle to have to call before training. \n        \n        Will decide how to refactor this later.\n        """"""\n\n        if not self.is_chatting:\n            with tf.variable_scope(""evaluation"") as scope:\n                # Loss - target is to predict (as output) next decoder input.\n                # target_labels has shape [batch_size, dec_inp_seq_len - 1]\n                target_labels = self.decoder_inputs[:, 1:]\n                target_weights = tf.cast(target_labels > 0, target_labels.dtype)\n                preds = self.decoder.apply_projection(self.outputs)\n                reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n                l1 = tf.reduce_sum(tf.abs(reg_losses))\n\n                if self.sampled_loss:\n                    self.log.info(""Training with dynamic sampled softmax loss."")\n                    assert 0 < self.num_samples < self.vocab_size, \\\n                        ""num_samples is %d but should be between 0 and %d"" \\\n                        % (self.num_samples, self.vocab_size)\n\n                    self.loss = bot_ops.dynamic_sampled_softmax_loss(\n                        target_labels,\n                        self.outputs[:, :-1, :],\n                        self.decoder.get_projection_tensors(),\n                        self.vocab_size,\n                        num_samples=self.num_samples) + l1\n                else:\n                    self.loss = tf.losses.sparse_softmax_cross_entropy(\n                        labels=target_labels,\n                        logits=preds[:, :-1, :],\n                        weights=target_weights) + l1\n                    # New loss function I\'m experimenting with below:\n                    # I\'m suspicious that it may do the same stuff\n                    # under-the-hood as sparse_softmax_cross_entropy,\n                    # but I\'m doing speed tests/comparisons to make sure.\n                    #self.loss = bot_ops.cross_entropy_sequence_loss(\n                    #    labels=target_labels,\n                    #    logits=preds[:, :-1, :],\n                    #    weights=target_weights) + l1\n\n                self.log.info(""Optimizing with %s."", self.optimizer)\n                self.train_op = tf.contrib.layers.optimize_loss(\n                    loss=self.loss, global_step=self.global_step,\n                    learning_rate=self.learning_rate,\n                    optimizer=self.optimizer,\n                    clip_gradients=self.max_gradient,\n                    summaries=[\'gradients\'])\n\n                # Compute accuracy, ensuring we use fully projected outputs.\n                correct_pred = tf.equal(tf.argmax(preds[:, :-1, :], axis=2),\n                                        target_labels)\n                accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n                tf.summary.scalar(\'accuracy\', accuracy)\n                tf.summary.scalar(\'loss_train\', self.loss)\n                self.merged = tf.summary.merge_all()\n                # Note: Important not to merge in the validation loss, since\n                # we don\'t want to couple it with the training loss summary.\n                self.valid_summ = tf.summary.scalar(\'loss_valid\', self.loss)\n\n        super(DynamicBot, self).compile()\n\n    def step(self, forward_only=False):\n        """"""Run one step of the model, which can mean 1 of the following:\n            1. forward_only == False. \n               - This means we are training.\n               - We do a forward and a backward pass.\n            2. self.is_chatting. \n               - We are running a user\'s input sentence to generate a response.\n               - We only do a forward pass to get the response (word IDs).\n            3. Otherwise: inference (used for validation)\n               - Do a forward pass, but also compute loss(es) and summaries.\n\n        Args:\n            forward_only: if True, don\'t perform backward pass \n            (gradient updates).\n\n        Returns:\n            3-tuple: (summaries, step_loss, step_outputs).\n            \n            Qualifications/details for each of the 3 cases:\n            1. If forward_only == False: \n               - This is a training step: \'summaries\' are training summaries.\n               - step_outputs = None\n            2. else if self.is_chatting: \n               - summaries = step_loss = None\n               - step_outputs == the bot response tokens\n            3. else (validation):\n               - This is validation: \'summaries\' are validation summaries.\n               - step_outputs == None (to reduce computational cost).\n        """"""\n\n        if not forward_only:\n            fetches = [self.merged, self.loss, self.train_op]\n            summaries, step_loss, _ = self.sess.run(fetches)\n            return summaries, step_loss, None\n        elif self.is_chatting:\n            response = self.sess.run(\n                fetches=self.outputs,\n                feed_dict=self.pipeline.feed_dict)\n            return None, None, response\n        else:\n            fetches = [self.valid_summ, self.loss]  # , self.outputs]\n            summaries, step_loss = self.sess.run(fetches)\n            return summaries, step_loss, None\n\n    def train(self, dataset=None):\n        """"""Train bot on inputs until user types CTRL-C or queues run out of data.\n\n        Args:\n            dataset: (DEPRECATED) any instance of the Dataset class. \n            Will be removed soon.\n        """"""\n\n        def perplexity(loss):\n            return np.exp(float(loss)) if loss < 300 else float(""inf"")\n\n        if dataset is None:\n            dataset = self.dataset\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=self.sess, coord=coord)\n\n        # Tell embedder to coordinate with TensorBoard\'s ""Embedddings"" tab.\n        # This allows us to view words in 3D-projected embedding space.\n        self.embedder.assign_visualizers(\n            self.file_writer,\n            [\'encoder\', \'decoder\'],\n            dataset.paths[\'vocab\'])\n\n        # Note: Calling sleep allows sustained GPU utilization across training.\n        # Without it, GPU has to wait for data to be enqueued more often.\n        print(\'QUEUE RUNNERS RELEASED.\', end="" "")\n        for _ in range(3):\n            print(\'.\', end="" "");\n            time.sleep(1);\n            sys.stdout.flush()\n        print(\'GO!\')\n\n        try:\n            avg_loss = avg_step_time = 0.0\n            while not coord.should_stop():\n\n                i_step = self.sess.run(self.global_step)\n\n                start_time = time.time()\n                summaries, step_loss, _ = self.step()\n                # Calculate running averages.\n                avg_step_time += (time.time() - start_time) / self.steps_per_ckpt\n                avg_loss += step_loss / self.steps_per_ckpt\n\n                # Print updates in desired intervals (steps_per_ckpt).\n                if i_step % self.steps_per_ckpt == 0:\n                    # Display averged-training updates and save.\n                    print(""Step %d:"" % i_step, end="" "")\n                    print(""step time = %.3f"" % avg_step_time)\n                    print(""\\ttraining loss = %.3f"" % avg_loss, end=""; "")\n                    print(""training perplexity = %.2f"" % perplexity(avg_loss))\n                    self.save(summaries=summaries)\n\n                    # Toggle data switch and led the validation flow!\n                    self.pipeline.toggle_active()\n                    with self.graph.device(\'/cpu:0\'):\n                        summaries, eval_loss, _ = self.step(forward_only=True)\n                        self.save(summaries=summaries)\n                    self.pipeline.toggle_active()\n                    print(""\\tValidation loss = %.3f"" % eval_loss, end=""; "")\n                    print(""val perplexity = %.2f"" % perplexity(eval_loss))\n                    # Reset the running averages and exit checkpoint.\n                    avg_loss = avg_step_time = 0.0\n\n                if i_step >= self.max_steps:\n                    print(""Maximum step"", i_step, ""reached."")\n                    raise SystemExit\n\n        except (KeyboardInterrupt, SystemExit):\n            print(""Training halted. Cleaning up . . . "")\n            coord.request_stop()\n        except tf.errors.OutOfRangeError:\n            print(""OutOfRangeError. You have run out of data."")\n            coord.request_stop()\n        finally:\n            coord.join(threads)\n            self.close(save_current=False, rebuild_for_chat=True)\n\n    def decode(self):\n        """"""Sets up and manages chat session between bot and user (stdin).""""""\n        # Make sure params are set to chat values, just in case the user\n        # forgot to specify/doesn\'t know about such things.\n        self._set_chat_params()\n        # Decode from standard input.\n        print(""Type \\""exit\\"" to exit.\\n"")\n        sentence = io_utils.get_sentence()\n        while sentence != \'exit\':\n            response = self(sentence)\n            print(""Robot:"", response)\n            sentence = io_utils.get_sentence()\n        print(""Farewell, human."")\n\n    def __call__(self, sentence):\n        """"""This is how we talk to the bot interactively.\n        \n        While decode(self) above sets up/manages the chat session, \n        users can also use this directly to get responses from the bot, \n        given an input sentence. \n        \n        For example, one could do:\n            sentence = \'Hi, bot!\'\n            response = bot(sentence)\n        for a single input-to-response with the bot.\n\n        Args:\n            sentence: (str) Input sentence from user.\n\n        Returns:\n            response string from bot.\n        """"""\n        # Convert input sentence to token-ids.\n        encoder_inputs = io_utils.sentence_to_token_ids(\n            tf.compat.as_bytes(sentence), self.dataset.word_to_idx)\n\n        encoder_inputs = np.array([encoder_inputs[::-1]])\n        self.pipeline.feed_user_input(encoder_inputs)\n        # Get output sentence from the chatbot.\n        _, _, response = self.step(forward_only=True)\n        # response has shape [1, response_length].\n        # Its last element is the EOS_ID, which we don\'t show user.\n        response = self.dataset.as_words(response[0][:-1])\n        if \'UNK\' in response:\n            response = ""I don\'t know.""\n        return response\n\n    def chat(self):\n        """"""Alias for decode.""""""\n        self.decode()\n\n    def respond(self, sentence):\n        """"""Alias for __call__. (Suggestion)""""""\n        return self.__call__(sentence)\n\n    def close(self, save_current=True, rebuild_for_chat=True):\n        """"""Before closing, which will freeze our graph to a file,\n        rebuild it so that it\'s ready for chatting when unfreezed,\n        to make it easier for the user. Training can still be resumed\n        with no issue since it doesn\'t load frozen models, just ckpts.\n        """"""\n\n        if rebuild_for_chat:\n            lr_val = self.learning_rate.eval(session=self.sess)\n            tf.reset_default_graph()\n            # Gross. Am ashamed:\n            self.sess = tf.Session()\n            with self.graph.name_scope(tf.GraphKeys.SUMMARIES):\n                self.global_step    = tf.Variable(initial_value=0, trainable=False)\n                self.learning_rate  = tf.constant(lr_val)\n            self._set_chat_params()\n            self.build_computation_graph(self.dataset)\n            self.compile()\n        super(DynamicBot, self).close(save_current=save_current)\n\n    def _set_chat_params(self):\n        """"""Set training-specific param values to chatting-specific values.""""""\n        # TODO: use __setattr__ instead of this.\n        self.__dict__[\'__params\'][\'model_params\'][\'decode\'] = True\n        self.__dict__[\'__params\'][\'model_params\'][\'is_chatting\'] = True\n        self.__dict__[\'__params\'][\'model_params\'][\'batch_size\'] = 1\n        self.__dict__[\'__params\'][\'model_params\'][\'reset_model\'] = False\n        self.__dict__[\'__params\'][\'model_params\'][\'dropout_prob\'] = 0.0\n        assert self.is_chatting and self.decode and not self.reset_model\n\n'"
chatbot/globals.py,4,"b'""""""Place all default/global chatbot variables here.""""""\n\nimport tensorflow as tf\n\nOPTIMIZERS = {\n    \'Adagrad\':  tf.train.AdagradOptimizer,\n    \'Adam\':     tf.train.AdamOptimizer,\n    \'SGD\':      tf.train.GradientDescentOptimizer,\n    \'RMSProp\':  tf.train.RMSPropOptimizer,\n}\n\n# All allowed and/or used default configuration values, period.\nDEFAULT_FULL_CONFIG = {\n    ""model"": ""DynamicBot"",\n    ""dataset"": ""Cornell"",\n    ""model_params"": {\n        ""base_cell"": ""GRUCell"",\n        ""ckpt_dir"": ""out"",  # Directory to store training checkpoints.\n        ""decode"": False,\n        ""batch_size"": 256,\n        ""dropout_prob"": 0.2,  # Drop rate applied at encoder/decoders output.\n        ""decoder.class"": ""BasicDecoder"",\n        ""encoder.class"": ""BasicEncoder"",\n        ""embed_size"": 128,\n        ""learning_rate"": 0.002,\n        ""l1_reg"": 1.0e-6,  # L1 regularization applied to word embeddings.\n        ""lr_decay"": 0.98,\n        ""max_gradient"": 5.0,\n        ""max_steps"": int(1e6),  # Max number of training iterations.\n        ""num_layers"": 1,  # Num layers for each of encoder, decoder.\n        ""num_samples"": 512,  # IF sampled_loss is true, default sample size.\n        ""optimizer"": ""Adam"",  # Options are those in OPTIMIZERS above.\n        ""reset_model"": True,\n        ""sampled_loss"": False,  # Whether to do sampled softmax.\n        ""state_size"": 512,\n        ""steps_per_ckpt"": 200,\n        ""temperature"": 0.0,  # Response temp for chat sessions. (default argmax)\n    },\n    ""dataset_params"": {\n        ""data_dir"": None,  # Require user to specify.\n        ""vocab_size"": 40000,\n        ""max_seq_len"": 10,  # Maximum length of sentence used to train bot.\n        ""optimize_params"": True  # Reduce vocab size if exceeds num unique words\n    },\n}\n'"
data/__init__.py,0,"b""from __future__ import absolute_import\n\nfrom data import data_helper\nfrom data import _dataset\nfrom data import dataset_wrappers\n\nfrom data.data_helper import DataHelper\nfrom data._dataset import Dataset\nfrom data.dataset_wrappers import Cornell, Ubuntu, Reddit, TestData\n\n__all__ = ['Cornell', 'Reddit', 'Ubuntu', 'TestData']\n"""
data/_dataset.py,9,"b'""""""ABC for datasets. """"""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom utils import io_utils\nfrom abc import ABCMeta, abstractmethod, abstractproperty\n\nfrom chatbot.globals import DEFAULT_FULL_CONFIG\nDEFAULT_PARAMS = DEFAULT_FULL_CONFIG[\'dataset_params\']\n\n\nclass DatasetABC(metaclass=ABCMeta):\n\n    @abstractmethod\n    def convert_to_tf_records(self, *args):\n        """"""If not found in data dir, will create tfrecords data \n        files from text files.\n        """"""\n        pass\n\n    @abstractmethod\n    def train_generator(self, batch_size):\n        """"""Returns a generator function for batches of batch_size \n        train data.\n        """"""\n        pass\n\n    @abstractmethod\n    def valid_generator(self, batch_size):\n        """"""Returns a generator function for batches of batch_size \n        validation data.\n        """"""\n        pass\n\n    @abstractproperty\n    def word_to_idx(self):\n        """"""Return dictionary map from str -> int. """"""\n        pass\n\n    @abstractproperty\n    def idx_to_word(self):\n        """"""Return dictionary map from int -> str. """"""\n        pass\n\n    @abstractproperty\n    def name(self):\n        """"""Returns name of the dataset as a string.""""""\n        pass\n\n    @abstractproperty\n    def max_seq_len(self):\n        """"""Return the maximum allowed sentence length.""""""\n        pass\n\n\nclass Dataset(DatasetABC):\n\n    def __init__(self, dataset_params):\n        """"""Implements the general of subset of operations that all \n        dataset subclasses can use.\n\n        Args:\n            dataset_params: dictionary of configuration parameters. \n                See DEFAULT_FULL_CONFIG at top of file for supported keys.\n        """"""\n\n        self.__dict__[\'__params\'] = Dataset.fill_params(dataset_params)\n        # We query io_utils to ensure all data files are organized properly,\n        # and io_utils returns the paths to files of interest.\n        id_paths, vocab_path, vocab_size = io_utils.prepare_data(\n            data_dir=self.data_dir,\n            vocab_size=self.vocab_size,\n            optimize=dataset_params.get(\'optimize_params\'),\n            config_path=dataset_params.get(\'config_path\'))\n\n        if vocab_size != self.vocab_size:\n            self.log.info(""Updating vocab size from %d to %d"",\n                          self.vocab_size, vocab_size)\n            self.vocab_size = vocab_size\n            # Also update the input dict, in case it is used later/elsewhere.\n            dataset_params[\'vocab_size\'] = self.vocab_size\n\n        self.paths = dict()\n        self.paths = {\n            **id_paths,\n            \'vocab\': vocab_path,\n            \'train_tfrecords\': None,\n            \'valid_tfrecords\': None}\n        self._word_to_idx, self._idx_to_word = io_utils.get_vocab_dicts(\n            vocab_path)\n\n        # Create tfrecords file if not located in data_dir.\n        self.convert_to_tf_records(\'train\')\n        self.convert_to_tf_records(\'valid\')\n\n    def convert_to_tf_records(self, prefix=\'train\'):\n        """"""If can\'t find tfrecords \'prefix\' files, creates them.\n\n        Args:\n            prefix: \'train\' or \'valid\'. Determines which tfrecords to build.\n        """"""\n\n        from_path = self.paths[\'from_\'+prefix]\n        to_path = self.paths[\'to_\'+prefix]\n        tfrecords_fname = (prefix\n                           + \'voc%d_seq%d\' % (self.vocab_size, self.max_seq_len)\n                           + \'.tfrecords\')\n        output_path = os.path.join(self.data_dir, tfrecords_fname)\n        if os.path.isfile(output_path):\n            self.log.info(\'Using tfrecords file %s\' % output_path)\n            self.paths[prefix + \'_tfrecords\'] = output_path\n            return\n\n        def get_sequence_example(encoder_line, decoder_line):\n            space_needed = max(len(encoder_line.split()), len(decoder_line.split()))\n            if space_needed > self.max_seq_len:\n                return None\n\n            example  = tf.train.SequenceExample()\n            encoder_list = [int(x) for x in encoder_line.split()]\n            decoder_list = [io_utils.GO_ID] \\\n                           + [int(x) for x in decoder_line.split()] \\\n                           + [io_utils.EOS_ID]\n\n            # Why tensorflow . . . why . . .\n            example.context.feature[\'encoder_sequence_length\'].int64_list.value.append(\n                len(encoder_list))\n            example.context.feature[\'decoder_sequence_length\'].int64_list.value.append(\n                len(decoder_list))\n\n            encoder_sequence = example.feature_lists.feature_list[\'encoder_sequence\']\n            decoder_sequence = example.feature_lists.feature_list[\'decoder_sequence\']\n            for e in encoder_list:\n                encoder_sequence.feature.add().int64_list.value.append(e)\n            for d in decoder_list:\n                decoder_sequence.feature.add().int64_list.value.append(d)\n\n            return example\n\n        with tf.gfile.GFile(from_path, mode=""r"") as encoder_file:\n            with tf.gfile.GFile(to_path, mode=""r"") as decoder_file:\n                with tf.python_io.TFRecordWriter(output_path) as writer:\n                    encoder_line = encoder_file.readline()\n                    decoder_line = decoder_file.readline()\n                    while encoder_line and decoder_line:\n                        sequence_example = get_sequence_example(\n                            encoder_line,\n                            decoder_line)\n                        if sequence_example is not None:\n                            writer.write(sequence_example.SerializeToString())\n                        encoder_line = encoder_file.readline()\n                        decoder_line = decoder_file.readline()\n\n        self.log.info(""Converted text files %s and %s into tfrecords file %s"" \\\n                      % (os.path.basename(from_path),\n                         os.path.basename(to_path),\n                         os.path.basename(output_path)))\n        self.paths[prefix + \'_tfrecords\'] = output_path\n\n    def sentence_generator(self, prefix=\'from\'):\n        """"""Yields (as words) single sentences from training data, \n        for testing purposes.\n        """"""\n        self.log.info(""Generating sentences from %s"", self.paths[prefix+\'_train\'])\n        with tf.gfile.GFile(self.paths[prefix+\'_train\'], mode=""r"") as f:\n            sentence = self.as_words(\n                list(map(int, f.readline().strip().lower().split())))\n            while sentence:\n                yield sentence\n                sentence = self.as_words(\n                    list(map(int, f.readline().strip().lower().split())))\n\n    def pairs_generator(self, num_generate=None):\n        in_sentences = self.sentence_generator(\'from\')\n        in_sentences = [s for s in in_sentences]\n        out_sentences = self.sentence_generator(\'to\')\n        out_sentences = [s for s in out_sentences]\n\n        if num_generate is None:\n            num_generate = len(in_sentences)\n\n        count = 0\n        for in_sent, out_sent in zip(in_sentences, out_sentences):\n            yield in_sent, out_sent\n\n            count += 1\n            if count >= num_generate:\n                break\n\n    def train_generator(self, batch_size):\n        """"""[Note: not needed by DynamicBot since InputPipeline]""""""\n        return self._generator(\n            self.paths[\'from_train\'],\n            self.paths[\'to_train\'],\n            batch_size)\n\n    def valid_generator(self, batch_size):\n        """"""[Note: not needed by DynamicBot since InputPipeline]""""""\n        return self._generator(\n            self.paths[\'from_valid\'],\n            self.paths[\'to_valid\'],\n            batch_size)\n\n    def _generator(self, from_path, to_path, batch_size):\n        """"""(Used by BucketModels only). Returns a generator function that \n        reads data from file, and yields shuffled batches.\n\n        Args:\n            from_path: full path to file for encoder inputs.\n            to_path: full path to file for decoder inputs.\n            batch_size: number of samples to yield at once.\n        """"""\n\n        def longest_sentence(enc_list, dec_list):\n            max_enc_len = max([len(s) for s in enc_list])\n            max_dec_len = max([len(s) for s in dec_list])\n            return max(max_enc_len, max_dec_len)\n\n        def padded_batch(encoder_tokens, decoder_tokens):\n            max_sent_len = longest_sentence(encoder_tokens, decoder_tokens)\n            encoder_batch = np.array(\n                [s + [io_utils.PAD_ID] * (max_sent_len - len(s))\n                 for s in encoder_tokens])[:, ::-1]\n            decoder_batch = np.array(\n                [s + [io_utils.PAD_ID] * (max_sent_len - len(s))\n                 for s in decoder_tokens])\n            return encoder_batch, decoder_batch\n\n        encoder_tokens = []\n        decoder_tokens = []\n        with tf.gfile.GFile(from_path, mode=""r"") as source_file:\n            with tf.gfile.GFile(to_path, mode=""r"") as target_file:\n\n                source, target = source_file.readline(), target_file.readline()\n                while source and target:\n\n                    # Skip sentence pairs that are too long for specifications.\n                    space_needed = max(len(source.split()), len(target.split()))\n                    if space_needed > self.max_seq_len:\n                        source, target = source_file.readline(), target_file.readline()\n                        continue\n\n                    # Reformat token strings to token lists.\n                    # Note: GO_ID is prepended by the chat bot, since it\n                    # determines whether or not it\'s responsible for responding.\n                    encoder_tokens.append([int(x) for x in source.split()])\n                    decoder_tokens.append(\n                        [int(x) for x in target.split()] + [io_utils.EOS_ID])\n\n                    # Have we collected batch_size number of sentences?\n                    # If so, pad & yield.\n                    assert len(encoder_tokens) == len(decoder_tokens)\n                    if len(encoder_tokens) == batch_size:\n                        yield padded_batch(encoder_tokens, decoder_tokens)\n                        encoder_tokens = []\n                        decoder_tokens = []\n                    source, target = source_file.readline(), target_file.readline()\n\n                # Don\'t forget to yield the \'leftovers\'!\n                assert len(encoder_tokens) == len(decoder_tokens)\n                assert len(encoder_tokens) <= batch_size\n                if len(encoder_tokens) > 0:\n                    yield padded_batch(encoder_tokens, decoder_tokens)\n\n    @property\n    def word_to_idx(self):\n        """"""Return dictionary map from str -> int. """"""\n        return self._word_to_idx\n\n    @property\n    def idx_to_word(self):\n        """"""Return dictionary map from int -> str. """"""\n        return self._idx_to_word\n\n    def as_words(self, sentence):\n        """"""Convert list of integer tokens to a single sentence string.""""""\n\n        words = []\n        for token in sentence:\n            word = self.idx_to_word[token]\n            try:\n                word = tf.compat.as_str(word)\n            except UnicodeDecodeError:\n                logging.error(""UnicodeDecodeError on (token, word): ""\n                              ""(%r, %r)"", token, word)\n                word = str(word)\n            words.append(word)\n\n        words = "" "".join(words)\n        #words = "" "".join([tf.compat.as_str(self.idx_to_word[i]) for i in sentence])\n        words = words.replace(\' , \', \', \').replace(\' .\', \'.\').replace(\' !\', \'!\')\n        words = words.replace("" \' "", ""\'"").replace("" ?"", ""?"")\n        if len(words) < 2:\n            return words\n        return words[0].upper() + words[1:]\n\n    @property\n    def name(self):\n        """"""Returns name of the dataset as a string.""""""\n        return self._name\n\n    @property\n    def train_size(self):\n        raise NotImplemented\n\n    @property\n    def valid_size(self):\n        raise NotImplemented\n\n    @property\n    def max_seq_len(self):\n        return self._max_seq_len\n\n    @staticmethod\n    def fill_params(dataset_params):\n        """"""Assigns default values from DEFAULT_FULL_CONFIG \n        for keys not in dataset_params.""""""\n        if \'data_dir\' not in dataset_params:\n            raise ValueError(\'data directory not found in dataset_params.\')\n        return {**DEFAULT_PARAMS, **dataset_params}\n\n    def __getattr__(self, name):\n        if name not in self.__dict__[\'__params\']:\n            raise AttributeError(name)\n        else:\n            return self.__dict__[\'__params\'][name]\n\n'"
data/data_helper.py,0,"b'""""""Provides pre-processing functionality.\n\nAbstracts paths and filenames so we don\'t have to think about them. Currently,\nin use by Brandon, but will extend to general users in the future.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\nimport pdb\nimport sys\nimport json\nimport logging\nimport tempfile\nfrom pprint import pprint\nfrom subprocess import Popen, PIPE\n\nimport pandas as pd\nimport numpy as np\nfrom pympler.asizeof import asizeof          # for profiling memory usage\n\n# Absolute path to this file.\n_WORD_SPLIT = re.compile(r\'([.,!?\\""\\\':;)(])|\\s\')\nHERE = os.path.dirname(os.path.realpath(__file__))\nDATA_ROOTS = {\n    \'brandon\': \'/home/brandon/Datasets/reddit\',\n    \'ivan\': \'/Users/ivan/Documents/sp_17/reddit_data\',\n    \'mitch\': \'/Users/Mitchell/Documents/Chatbot/RedditData\',\n    \'george\': \'/Users/George/Documents/ChatbotData/reddit\'\n}\n# Maximum memory usage allowed (in GiB).\nMAX_MEM = 2.0\n\n\ndef prompt(text, default="""", required=False):\n    print(""%s (default=%r): "" % (text, default), end="""")\n    errors = 0\n    userinput = input()\n    while not userinput and required:\n        errors += 1\n        userinput = input(""C\'mon dude, be serious%s "" % (\n            \':\' if errors <= 1 else (\'!\' * errors)))\n    return userinput or default\n\n\nclass DataHelper:\n    """"""Manages file locations and computing resource during preprocessing.\n\n    This interacts directly with the user and double-checks their work; It makes it\n    harder for you to screw up.\n    """"""\n\n    def __init__(self, log_level=logging.INFO):\n        """""" Establish some baseline data with the user.\n        """"""\n        self.logfile = tempfile.NamedTemporaryFile(\n            mode=\'w\', prefix=\'data_helper\', delete=False)\n        self.logfile.close()\n        logging.basicConfig(filename=self.logfile.name, level=log_level)\n        print(""Using logfile:"", self.logfile.name)\n\n        self.file_counter = 0   # current file we\'re processing\n        self._word_freq = None  # temporary: for parallelizing frequency dict\n\n        print(""Hi, I\'m a DataHelper. For now, I help with the reddit dataset."")\n        print(""At any prompt, press ENTER if you want the default value."")\n\n        # 1. Get user name. We can associate info with a given user as we go.\n        user = prompt(""Username"", default=""brandon"").lower()\n        if user not in DATA_ROOTS:\n            print(""I don\'t recognize you, %s."" % user)\n            self.data_root = prompt(""Please give me the path to your data:"",\n                                    required=True)\n        else:\n            self.data_root = DATA_ROOTS[user]\n\n        print(""Hello, %s, I\'ve set your data root to %s"" % (user, self.data_root))\n\n        # 2. Get absolute paths to all data filenames in self.file_paths.\n        self.file_paths = []\n        years = prompt(""Years to process"", default=""2007,2008,2009"")\n        # Secretly supports passing a range too. Shhhh.\n        if \'-\' in years:\n            years = list(map(int, years.split(\'-\')))\n            years = list(range(years[0], years[1]+1))\n            years = list(map(str, years))\n        else:\n            years = years.split(\',\')\n        for y in years:\n            # The path is: $ROOT/raw_data/$YEAR\n            # Add the entirety of the directory to the file paths.\n            base_path = os.path.join(self.data_root, \'raw_data\', y)\n            rel_paths = os.listdir(base_path)\n            self.file_paths.extend([\n                os.path.join(base_path, f) for f in rel_paths \\\n                if not f.endswith("".bz2"")\n            ])\n\n        self._next_file_path = self.file_paths[0]\n        print(""These are the files I found:"")\n        pprint(self.file_paths)\n        print()\n\n        _max_mem = prompt(""Maximum memory to use (in GiB)"", ""%.2f"" % MAX_MEM)\n        try:\n            self.max_mem = float(_max_mem)\n        except ValueError:\n            print(""C\'mon dude, get it together!"")\n\n    def safe_load(self):\n        """""" Load data while keeping an eye on memory usage.""""""\n\n        if self.file_counter >= len(self.file_paths):\n            print(""No more files to load!"")\n            return None\n\n        # For in-place appending.\n        # S.O.: https://stackoverflow.com/questions/20906474/\n        list_ = []  # real descriptive :)\n        for i in range(self.file_counter, len(self.file_paths)):\n            # lines=True means ""read as json-object-per-line.""\n            list_.append(pd.read_json(self.file_paths[i], lines=True))\n\n            mem_usage = float(asizeof(list_)) / 1e9\n            logging.info(""Data list has size %.3f GiB"", mem_usage)\n            logging.info(""Most recent file loaded: %s"", self.file_paths[i])\n            print(""\\rLoaded file"", self.file_paths[i], end="""")\n            sys.stdout.flush()\n            if mem_usage > self.max_mem:\n                print(""\\nPast max capacity:"", mem_usage,\n                      ""Leaving data collection early."")\n                logging.warning(\'Terminated data loading after \'\n                                \'reading %d files.\', i + 1)\n                logging.info(\'Files read into df: %r\', self.file_paths[:i+1])\n                break\n        print()\n\n        # If the user decides they want to continue loading later\n        # (when memory frees up), we want the file_counter set so that it\n        # starts on the next file.\n        self.file_counter = i + 1\n        self._next_file_path = self.file_paths[self.file_counter]\n\n        df = pd.concat(list_).reset_index()\n        logging.info(""Number of lines in raw data file: %r"", len(df.index))\n        logging.info(""Column names from raw data file: %r"", df.columns)\n        logging.info(""DataHelper.safe_load: df.head() = %r"", df.head())\n        return df\n\n    def load_random(self, year=None):\n        """"""Load a random data file and return as a DataFrame.\n        \n        Args:\n            year: (int) If given, get a random file from this year.\n        """"""\n\n        files = self.file_paths\n        if year is not None:\n            files = list(filter(lambda f: str(year) in f, files))\n\n        rand_index = np.random.randint(low=0, high=len(files))\n        print(\'Returning data from file:\\n\', files[rand_index])\n        return pd.read_json(files[rand_index], lines=True)\n\n    def load_next(self):\n        if self.next_file_path is None:\n            logging.warning(\'Tried loading next file but no files remain.\')\n            return None\n\n        df = pd.read_json(self.next_file_path, lines=True)\n        self.file_counter += 1\n        if self.file_counter < len(self.file_paths):\n            self._next_file_path = self.file_paths[self.file_counter]\n        else:\n            self._next_file_path = None\n        return df\n\n    def set_word_freq(self, wf):\n        """"""Hacky (temporary) fix related to multiprocessing.Pool complaints\n        for the reddit preprocessing script.\n        """"""\n        self._word_freq = wf\n\n    @property\n    def word_freq(self):\n        return self._word_freq\n\n    @property\n    def next_file_path(self):\n        return self._next_file_path\n\n    def get_year_from_path(self, path):\n        year = path.strip(\'/\').split(\'/\')[-2]\n        try:\n            _ = int(year)\n        except ValueError:\n            logging.warning(""Couldn\'t get year from file path. Your directory""\n                            "" structure is unexpected."")\n            return None\n        logging.info(\'Extracted year %s\', year)\n        return year\n\n    def generate_files(self,\n                       from_file_path,\n                       to_file_path,\n                       root_to_children,\n                       comments_dict):\n        """"""Generates two files, [from_file_path] and [to_file_path] \n        of 1-1 comments.\n        """"""\n        from_file_path = os.path.join(self.data_root, from_file_path)\n        to_file_path = os.path.join(self.data_root, to_file_path)\n        print(""Writing data files:\\n"", from_file_path, ""\\n"", to_file_path)\n\n        with open(from_file_path, \'w\') as from_file:\n            with open(to_file_path, \'w\') as to_file:\n                for root_ID, child_IDs in root_to_children.items():\n                    for child_ID in child_IDs:\n                        try:\n                            from_file.write(comments_dict[root_ID].strip() + \'\\n\')\n                            to_file.write(comments_dict[child_ID].strip() + \'\\n\')\n                        except KeyError:\n                            pass\n\n        (num_samples, stderr) = Popen(\n            [\'wc\', \'-l\', from_file_path], stdout=PIPE).communicate()\n        num_samples = int(num_samples.strip().split()[0])\n\n        print(""Final processed file has %d samples total."" % num_samples)\n\n        # First make sure user has copy of bash script we\'re about to use.\n        # os.popen(\'cp %s %s\' % (os.path.join(HERE, \'split_into_n.sh\'), self.data_root))\n        # Split data into 90% training and 10% validation.\n        # os.popen(\'bash %s %d\' % (os.path.join(self.data_root, \'split_into_n.sh\'),\n        #                        0.1 * num_samples))\n\n    def df_generator(self):\n        """""" Generates df from single files at a time.""""""\n        for i in range(len(self.file_paths)):\n            df = pd.read_json(self.file_paths[i], lines=True)\n            init_num_rows = len(df.index)\n            logging.info(""Number of lines in raw data file: %r"" % init_num_rows)\n            logging.info(""Column names from raw data file: %r""  % df.columns)\n            yield df\n\n    @staticmethod\n    def random_rows_generator(num_rows_per_print, num_rows_total):\n        """""" Fun generator for viewing random comments (rows) in dataframes.""""""\n        num_iterations = num_rows_total // num_rows_per_print\n        shuffled_indices = np.arange(num_rows_per_print * num_iterations)\n        np.random.shuffle(shuffled_indices)\n        for batch in shuffled_indices.reshape(num_iterations, num_rows_per_print):\n            yield batch\n\n    @staticmethod\n    def word_tokenizer(sentences):\n        """""" Tokenizes sentence / list of sentences into word tokens.""""""\n        # Minor optimization: pre-create the list and fill it.\n        tokenized = [None for _ in range(len(sentences))]\n        for i in range(len(sentences)):\n            tokenized[i] = [\n                w for w in _WORD_SPLIT.split(sentences[i].strip()) if w\n            ]\n\n        return tokenized\n\n    @staticmethod\n    def df_to_json(df, target_file=None, orient=\'records\', lines=False, **kwargs):\n        """"""Converts dataframe to json object in the intuitive way, i.e.\n        each row is converted to a json object, where columns are properties. If\n        target_file is not None, then each such object is saved as a line in the\n        target_file. Helpful because pandas default args are NOT this behavior.\n        \n        Note: Setting lines=True can result in some problems when trying to reload\n        the file. Setting lines=False, while makes an essentially unreadable (for humans)\n        output file, it at least reproduces the saved dataframe upon loading via\n            df_reloaded = pd.read_json(target_file)\n        \n        Args:\n            df: Pandas DataFrame.\n            orient: \n            lines: whether or not to save rows on their own line or writing full file to\n                single line.\n            target_file: Where to save the json-converted df. \n                If None, just return the json object.\n            kwargs: any additional named params the user wishes to pass to df.to_json.\n        """"""\n\n        if target_file is None:\n            return df.to_json(orient=orient, lines=lines, **kwargs)\n        df.to_json(path_or_buf=target_file, orient=orient, lines=lines, **kwargs)\n'"
data/dataset_wrappers.py,0,"b'""""""Named data wrapper classes. No added functionality to dataset base class for now,\nbut preprocessing checks will be incorporated into each when it\'s time.\n""""""\n\nimport logging\nimport os\n\nfrom data._dataset import Dataset\n\n\ndef check_data(abs_path, name):\n    """"""All dataset wrappers call this as a quick sanity check.""""""\n\n    if abs_path is None:\n        raise ValueError(\'No data directory found in dataset_wrappers.check_data.\'\n                         \'Either specify data_dir or use io_utils.parse_config.\')\n\n    if os.path.basename(abs_path) != name:\n        print(""Data directory %s does not match dataset name %s."" % (abs_path, name))\n        propose_path = os.path.join(os.path.dirname(abs_path), name.lower())\n        print(""Would you like me to change data_dir to {}? [y/n] "".format(propose_path))\n        answer = input()\n        if answer == \'y\':\n            return propose_path\n        else:\n            raise ValueError(""Rejected path change. Terminating program."")\n    return abs_path\n\n\nclass Cornell(Dataset):\n    """"""Movie dialogs.""""""\n\n    def __init__(self, dataset_params):\n        self._name = ""cornell""\n        self.log = logging.getLogger(\'CornellLogger\')\n        dataset_params[\'data_dir\'] = check_data(\n            dataset_params.get(\'data_dir\'),\n            self.name)\n        super(Cornell, self).__init__(dataset_params)\n\n\nclass Ubuntu(Dataset):\n    """"""Technical support chat logs from IRC.""""""\n\n    def __init__(self, dataset_params):\n        self._name = ""ubuntu""\n        self.log = logging.getLogger(\'UbuntuLogger\')\n        dataset_params[\'data_dir\'] = check_data(\n            dataset_params.get(\'data_dir\'),\n            self.name)\n        super(Ubuntu, self).__init__(dataset_params)\n\n\nclass Reddit(Dataset):\n    """"""Reddit comments from 2007-2015.""""""\n\n    def __init__(self, dataset_params):\n        self._name = ""reddit""\n        self.log = logging.getLogger(\'RedditLogger\')\n        dataset_params[\'data_dir\'] = check_data(\n            dataset_params.get(\'data_dir\'),\n            self.name)\n        super(Reddit, self).__init__(dataset_params)\n\n\nclass TestData(Dataset):\n    """"""Mock dataset with a handful of sentences.""""""\n\n    def __init__(self, dataset_params):\n        self.log = logging.getLogger(\'TestDataLogger\')\n        self._name = ""test_data""\n        dataset_params[\'data_dir\'] = check_data(\n            dataset_params.get(\'data_dir\'),\n            self.name)\n        super(TestData, self).__init__(dataset_params)\n'"
data/reddit_preprocessor.py,0,"b'\xef\xbb\xbf""""""Reddit data preprocessing.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nfrom functools import wraps\nfrom itertools import chain\nfrom collections import Counter, defaultdict\nfrom multiprocessing import Pool\n\nimport numpy as np\nimport pandas as pd\nfrom data.data_helper import DataHelper\nfrom data.regex import regex_replace, contractions\nfrom nltk.corpus import wordnet\n\n\n# Global helper object that helps abstract away locations of\n# files & directories, and keeps an eye on memory usage.\nif __name__ == \'__main__\':\n    data_helper = DataHelper()\nelse:\n    data_helper = None\n# Max number of words in any saved sentence.\nMAX_SEQ_LEN = 20\n# Number of CPU cores available.\nNUM_CORES = 2\n# How many chunks we should split dataframes into at any given time.\nNUM_PARTITIONS = 64\n\n\ndef timed_function(*expected_args):\n    """"""Simple decorator to show how long the functions take to run.""""""\n    def decorator(fn):\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n            start_time = time.time()\n            res = fn(*args, **kwargs)\n            stop_time = time.time()\n            fname = expected_args[0]\n            print(""Time to run %s: %.3f seconds."" %\n                  (fname, stop_time - start_time))\n            return res\n        return wrapper\n    return decorator\n\n\n@timed_function(\'parallel_map_df\')\ndef parallel_map_df(fn, df):\n    """""" Based on great explanation from \'Pandas in Parallel\' (racketracer.com).\n    """"""\n    df = np.array_split(df, NUM_PARTITIONS)\n    pool = Pool(NUM_CORES)\n    df = pd.concat(pool.map(fn, df))\n    pool.close()\n    pool.join()\n    return df\n\n\n@timed_function(\'parallel_map_list\')\ndef parallel_map_list(fn, iterable):\n    """""" Based on great explanation from \'Pandas in Parallel\' (racketracer.com).\n    """"""\n    iterable = np.array_split(iterable, NUM_PARTITIONS)\n    pool = Pool(NUM_CORES)\n    iterable = np.concatenate(pool.map(fn, iterable))\n    pool.close()\n    pool.join()\n    return iterable\n\n\ndef sentence_score(sentences):\n    word_freq = data_helper.word_freq\n    scores = []\n    for sentence in sentences:\n        word_count = len(sentence) + 1e-20\n        sent_score = sum([1.0 / ((word_freq[w] + 1e-20) * word_count)\n                      for w in sentence if not wordnet.synsets(w)])\n        scores.append(sent_score)\n    return scores\n\n\ndef root_comments(df):\n    """""" Builds a list determining which rows of df are root comments.\n\n    Returns:\n        list of length equal to the number of rows in our data frame.\n    """"""\n    root_value = []\n    # Iterate over DataFrame rows as namedtuples,\n    # with index value as first element of the tuple.\n    for row in df.itertuples():\n        root_value.append(row.parent_id == row.link_id)\n    return root_value\n\n\n@timed_function(\'remove_extra_columns\')\ndef remove_extra_columns(df):\n    """"""Throw away columns we don\'t need and misc. style formatting.""""""\n    df[\'root\'] = root_comments(df)\n    df = df[[\'author\', \'body\', \'link_id\', \'parent_id\', \'name\', \'root\', \'subreddit\']]\n    df.style.set_properties(subset=[\'body\'], **{\'width\': \'500px\'})\n    df.style.set_properties(**{\'text-align\': \'left\'})\n    df.head()\n    return df\n\n\n@timed_function(\'regex_replacements\')\ndef regex_replacements(df):\n    # Remove comments that are \'[deleted]\'.\n    df = df.loc[df.body != \'[deleted]\'].reset_index(drop=True)\n    df.style.set_properties(subset=[\'body\'], **{\'width\': \'800px\'})\n\n    # Make all comments lowercase to help reduce vocab size.\n    df[\'body\'] = df[\'body\'].map(lambda s: s.strip().lower())\n\n    # Loop over regex replacements specified by modify_list.\n    for regex in regex_replace:\n        df[\'body\'].replace(\n            {regex: regex_replace[regex]},\n            regex=True,\n            inplace=True)\n\n    return df\n\n\n@timed_function(\'remove_large_comments\')\ndef remove_large_comments(max_len, df):\n    # Could probably do a regex find on spaces to make this faster.\n    df = df[df[\'body\'].map(lambda s: len(s.split())) < max_len].reset_index(drop=True)\n    return df\n\n\n@timed_function(\'expand_contractions\')\ndef expand_contractions(df):\n    """""" Replace all contractions with their expanded chat_form.\n    \n    Note: contractions is dict(contraction -> expanded form)\n    """"""\n    for c in contractions:\n        df[\'body\'].replace({c: contractions[c]}, regex=True, inplace=True)\n    return df\n\n\n@timed_function(\'children_dict\')\ndef children_dict(df):\n    """""" Returns a dictionary with keys being the root comments and\n    values being their immediate root_to_children. Assumes that df has \'root\' column.\n\n    Go through all comments. If it is a root, skip it since they wont have a parent_id\n    that corresponds to a comment.\n    """"""\n    children = defaultdict(list)\n    for row in df.itertuples():\n        if row.root == False:\n            children[row.parent_id].append(row.name)\n    return children\n\n\ndef main():\n    """"""Processes each file individually through the whole pipeline.\n    \n    This decision was made due to the very large (many larger than 5 Gb) files.\n    I have scripts that combine the output files, and I plan on making those\n    available soon (very basic). \n    """"""\n\n    current_file = data_helper.next_file_path\n    df = data_helper.load_next()\n    while df is not None:\n\n        # Execute preprocessing steps on current_file\'s dataframe.\n        df = remove_extra_columns(df)\n        df = regex_replacements(df)\n        df = remove_large_comments(max_len=MAX_SEQ_LEN, df=df)\n        df = expand_contractions(df)\n\n        sentences = parallel_map_list(fn=DataHelper.word_tokenizer, iterable=df.body.values)\n        data_helper.set_word_freq(Counter(chain.from_iterable(sentences)))\n\n        print(\'Bout to score!\')\n        df[\'score\'] = parallel_map_list(fn=sentence_score, iterable=sentences)\n        del sentences\n\n        # Keep the desired percentage of lowest-scored sentences. (low == good)\n        keep_best_percent = 0.8\n        df = df.loc[df[\'score\'] < df[\'score\'].quantile(keep_best_percent)]\n\n        print(\'Prepping for the grand finale.\')\n        comments_dict = pd.Series(df.body.values, index=df.name).to_dict()\n        root_to_children = children_dict(df)\n        file_basename = os.path.join(\'processed_data\',\n                                     data_helper.get_year_from_path(current_file),\n                                     os.path.basename(current_file))\n        data_helper.generate_files(\n            from_file_path=""{}_encoder.txt"".format(file_basename),\n            to_file_path=""{}_decoder.txt"".format(file_basename),\n            root_to_children=root_to_children,\n            comments_dict=comments_dict)\n\n        # Prep for next loop.\n        current_file = data_helper.next_file_path\n        df = data_helper.load_next()\n\nif __name__ == \'__main__\':\n    main()\n'"
data/regex.py,0,"b'regex_replace = {\n    (r""https?:\\/\\/""\n     r""(www\\.)?""\n     r""[^\\s\\.]+""\n     r""\\.\\S{2,}""): ""<link>"",             # Raw link.\n    r""\\[[^\\(\\)]*\\]\\(.*\\)"": ""<link>"",     # Markdown link.\n    r""\\r?\\n"": "" "",                       # Newlines.\n    r""\\d+"": ""<number>"",\n    r""\\.{2,}"": ""."",\n    r""(&gt;|\\*|)"": """",\n    r""[_-]+"": "" ""\n}\n\ncontractions = {\n    ""sha\'n\'t"": ""shall not"",\n    ""I\'ve"": ""I have"",\n    ""who\'s"": ""who has"",\n    ""you\'re"": ""you are"",\n    ""can\'t\'ve"": ""cannot have"",\n    ""could\'ve"": ""could have"",\n    ""shan\'t"": ""shall not"",\n    ""he\'d\'ve"": ""he would have"",\n    ""hadn\'t\'ve"": ""had not have"",\n    ""couldn\'t\'ve"": ""could not have"",\n    ""y\'all\'ve"": ""you all have"",\n    ""when\'ve"": ""when have"",\n    ""that\'d\'ve"": ""that would have"",\n    ""it\'ll"": ""it shall"",\n    ""oughtn\'t\'ve"": ""ought not have"",\n    ""you\'ll"": ""you shall"",\n    ""shouldn\'t\'ve"": ""should not have"",\n    ""shouldn\'t"": ""should not"",\n    ""we\'ve"": ""we have"",\n    ""who\'ve"": ""who have"",\n    ""why\'ve"": ""why have"",\n    ""needn\'t\'ve"": ""need not have"",\n    ""ma\'am"": ""madam"",\n    ""oughtn\'t"": ""ought not"",\n    ""mustn\'t\'ve"": ""must not have"",\n    ""they\'d\'ve"": ""they would have"",\n    ""isn\'t"": ""is not"",\n    ""y\'all\'re"": ""you all are"",\n    ""so\'s"": ""so as"",\n    ""he\'d"": ""he had"",\n    ""doesn\'t"": ""does not"",\n    ""he\'s"": ""he has"",\n    ""I\'m"": ""I am"",\n    ""mightn\'t\'ve"": ""might not have"",\n    ""hadn\'t"": ""had not"",\n    ""needn\'t"": ""need not"",\n    ""don\'t"": ""do not"",\n    ""he\'ll\'ve"": ""he shall have"",\n    ""we\'ll\'ve"": ""we will have"",\n    ""what\'ll"": ""what shall"",\n    ""that\'s"": ""that has"",\n    ""it\'d"": ""it had"",\n    ""how\'s"": ""how has"",\n    ""you\'ve"": ""you have"",\n    ""wouldn\'t"": ""would not"",\n    ""he\'ll"": ""he shall"",\n    ""we\'d"": ""we had"",\n    ""I\'ll"": ""I shall"",\n    ""when\'s"": ""when has"",\n    ""we\'ll"": ""we will"",\n    ""couldn\'t"": ""could not"",\n    ""you\'ll\'ve"": ""you shall have"",\n    ""will\'ve"": ""will have"",\n    ""there\'d\'ve"": ""there would have"",\n    ""they\'d"": ""they had"",\n    ""I\'d"": ""I had"", ""y\'all"": ""you all"",\n    ""won\'t\'ve"": ""will not have"",\n    ""aren\'t"": ""are not"",\n    ""haven\'t"": ""have not"",\n    ""mustn\'t"": ""must not"",\n    ""what\'ve"": ""what have"",\n    ""it\'s"": ""it has"",\n    ""she\'ll"": ""she shall"",\n    ""wasn\'t"": ""was not"",\n    ""they\'re"": ""they are"",\n    ""that\'d"": ""that would"",\n    ""how\'d\'y"": ""how do you"",\n    ""what\'s"": ""what has"",\n    ""there\'d"": ""there had"",\n    ""to\'ve"": ""to have"",\n    ""I\'ll\'ve"": ""I shall have"",\n    ""y\'all\'d"": ""you all would"",\n    ""would\'ve"": ""would have"",\n    ""how\'ll"": ""how will"",\n    ""she\'d"": ""she had"", ""what\'re"": ""what are"",\n    ""wouldn\'t\'ve"": ""would not have"",\n    ""might\'ve"": ""might have"", ""mayn\'t"": ""may not"", ""o\'clock"": ""of the clock"",\n    ""\'cause"": ""because"",\n    ""mightn\'t"": ""might not"",\n    ""didn\'t"": ""did not"",\n    ""they\'ll"": ""they shall"",\n    ""there\'s"": ""there has"",\n    ""we\'d\'ve"": ""we would have"", ""hasn\'t"": ""has not"",\n    ""let\'s"": ""let us"", ""she\'s"": ""she has"",\n    ""who\'ll"": ""who shall"",\n    ""shan\'t\'ve"": ""shall not have"",\n    ""won\'t"": ""will not"",\n    ""where\'ve"": ""where have"",\n    ""it\'ll\'ve"": ""it shall have"", ""where\'s"": ""where has"",\n    ""you\'d\'ve"": ""you would have"",\n    ""weren\'t"": ""were not"",\n    ""who\'ll\'ve"": ""who shall have"",\n    ""why\'s"": ""why has"",\n    ""how\'d"": ""how did"",\n    ""we\'re"": ""we are"",\n    ""she\'d\'ve"": ""she would have"",\n    ""ain\'t"": ""am not"",\n    ""y\'all\'d\'ve"": ""you all would have"",\n    ""I\'d\'ve"": ""I would have"", ""they\'ve"": ""they have"",\n    ""must\'ve"": ""must have"",\n    ""what\'ll\'ve"": ""what shall have"", ""she\'ll\'ve"": ""she shall have"",\n    ""where\'d"": ""where did"",\n    ""should\'ve"": ""should have"",\n    ""you\'d"": ""you had"",\n    ""can\'t"": ""cannot"",\n    ""it\'d\'ve"": ""it would have"",\n    ""so\'ve"": ""so have"", ""they\'ll\'ve"": ""they shall have""}\n\n'"
notebooks/__init__.py,0,b''
tests/__init__.py,0,b''
tests/test_config.py,1,"b'""""""Tests for various operations done on config (yaml) dictionaries in project.""""""\n\nimport os\nimport pydoc\nimport yaml\nimport logging\nimport unittest\nimport tensorflow as tf\nfrom utils import io_utils\nimport chatbot\nimport data\ndir = os.path.dirname(os.path.realpath(__file__))\nfrom tests.utils import *\n\n\nclass TestConfig(unittest.TestCase):\n    """"""Test behavior of tf.contrib.rnn after migrating to r1.0.""""""\n\n    def setUp(self):\n        with open(TEST_CONFIG_PATH) as f:\n            # So we can always reference default vals.\n            self.test_config = yaml.load(f)\n\n    def test_merge_params(self):\n        """"""Checks how parameters passed to TEST_FLAGS interact with\n        parameters from yaml files. Expected behavior is that any\n        params in TEST_FLAGS will override those from files, but that\n        all values from file will be used if not explicitly passed to\n        TEST_FLAGS.\n        """"""\n\n        config = io_utils.parse_config(flags=TEST_FLAGS)\n\n        # ==============================================================\n        # Easy tests.\n        # ==============================================================\n\n        # Change model in test_flags and ensure merged config uses that model.\n        config = update_config(config, model=\'ChatBot\')\n        self.assertEqual(config[\'model\'], \'ChatBot\')\n\n        # Also ensure that switching back works too.\n        config = update_config(config, model=\'DynamicBot\')\n        self.assertEqual(config[\'model\'], \'DynamicBot\')\n\n        # Do the same for changing the dataset.\n        config = update_config(config, dataset=\'TestData\')\n        self.assertEqual(config[\'dataset\'], \'TestData\')\n\n        # ==============================================================\n        # Medium tests.\n        # ==============================================================\n\n        # Ensure recursive merging works.\n        config = update_config(\n            config,\n            batch_size=123,\n            dropout_prob=0.8)\n        logging.info(config)\n        self.assertEqual(config[\'model\'], self.test_config[\'model\'])\n        self.assertEqual(config[\'dataset\'], self.test_config[\'dataset\'])\n        self.assertNotEqual(config[\'model_params\'], self.test_config[\'model_params\'])\n\n    def test_optimize(self):\n        """"""Ensure the new optimize config flag works. \n        \n        Right now, \'works\' means it correctly determiens the true vocab \n        size, updates it in the config file, and updates any assoc. file names.\n        """"""\n\n        config = io_utils.parse_config(flags=TEST_FLAGS)\n        logging.info(config)\n\n        # Manually set vocab size to huge (non-optimal for TestData) value.\n        config = io_utils.update_config(config=config, vocab_size=99999)\n        self.assertEqual(config[\'dataset_params\'][\'vocab_size\'], 99999)\n        self.assertEqual(config[\'dataset_params\'][\'config_path\'], TEST_CONFIG_PATH)\n\n        # Instantiate a new dataset.\n        # This where the \'optimize\' flag comes into play, since\n        # the dataset object is responsible for things like checking\n        # data file paths and unique words.\n        logging.info(""Setting up %s dataset."", config[\'dataset\'])\n        logging.info(""Passing %r for dataset_params"", config[\'dataset_params\'])\n        dataset_class = pydoc.locate(config[\'dataset\']) \\\n                        or getattr(data, config[\'dataset\'])\n        dataset = dataset_class(config[\'dataset_params\'])\n        self.assertIsInstance(dataset, data.TestData)\n        self.assertNotEqual(dataset.vocab_size, 99999)\n\n    def test_update_config(self):\n        """"""Test the new function in io_utils.py""""""\n\n        logging.info(os.getcwd())\n        config = io_utils.get_yaml_config(TEST_CONFIG_PATH)\n        config[\'model_params\'][\'ckpt_dir\'] = TEST_FLAGS.model_params[\'ckpt_dir\']\n        self.assertIsInstance(config, dict)\n        self.assertTrue(\'model\' in config)\n        self.assertTrue(\'dataset\' in config)\n        self.assertTrue(\'dataset_params\' in config)\n        self.assertTrue(\'model_params\' in config)\n\n        config = io_utils.update_config(\n            config_path=TEST_CONFIG_PATH,\n            return_config=True,\n            vocab_size=1234)\n\n        self.assertEqual(config[\'dataset_params\'][\'vocab_size\'], 1234)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n\n'"
tests/test_data.py,2,"b'import logging\nimport pdb\nimport sys\nsys.path.append("".."")\nimport os\nimport unittest\nimport tensorflow as tf\nfrom pydoc import locate\nimport chatbot\nfrom utils import io_utils\nimport data\nfrom chatbot.globals import DEFAULT_FULL_CONFIG\ndir = os.path.dirname(os.path.realpath(__file__))\nfrom tests.utils import *\n\n\nclass TestData(unittest.TestCase):\n    """"""Tests for the datsets.""""""\n\n    def setUp(self):\n        logging.basicConfig(level=logging.INFO)\n        tf.logging.set_verbosity(\'ERROR\')\n        self.supported_datasets = [\'Reddit\', \'Ubuntu\', \'Cornell\']\n        self.default_flags = {\n            \'pretrained_dir\': TEST_FLAGS.pretrained_dir,\n            \'config\': TEST_FLAGS.config,\n            \'model\': TEST_FLAGS.model,\n            \'debug\': TEST_FLAGS.debug}\n\n    def test_basic(self):\n        """"""Instantiate all supported datasets and check they satisfy basic conditions.\n        \n        THIS MAY TAKE A LONG TIME TO COMPLETE. Since we are testing that the \n        supported datasets can be instantiated successfully, it necessarily \n        means that the data must exist in proper format. Since the program\n        will generate the proper format(s) if not found, this will take \n        about 15 minutes if run from a completely fresh setup.\n        \n        Otherwise, a few seconds. :)\n        """"""\n\n        if os.getenv(\'DATA\') is None \\\n            and not os.path.exists(\'/home/brandon/Datasets\'):\n            print(\'To run this test, please enter the path to your datasets: \')\n            data_dir = input()\n        else:\n            data_dir = \'/home/brandon/Datasets\'\n\n        for dataset_name in self.supported_datasets:\n            logging.info(\'Testing %s\', dataset_name)\n\n            incomplete_params = {\n                \'vocab_size\': 40000,\n                \'max_seq_len\': 10}\n            self.assertIsNotNone(incomplete_params)\n            dataset_class = getattr(data, dataset_name)\n            # User must specify data_dir, which we have not done yet.\n            self.assertRaises(ValueError, dataset_class, incomplete_params)\n\n            config = io_utils.parse_config(flags=TEST_FLAGS)\n            dataset_params = config.get(\'dataset_params\')\n            dataset_params[\'data_dir\'] = os.path.join(\n                data_dir,\n                dataset_name.lower())\n            dataset = dataset_class(dataset_params)\n\n            # Ensure all params from DEFAULT_FULL_CONFIG[\'dataset_params\']\n            # are set to a value in our dataset object.\n            for default_key in DEFAULT_FULL_CONFIG[\'dataset_params\']:\n                self.assertIsNotNone(getattr(dataset, default_key))\n\n            # Check that all dataset properties exist.\n            self.assertIsNotNone(dataset.name)\n            self.assertIsNotNone(dataset.word_to_idx)\n            self.assertIsNotNone(dataset.idx_to_word)\n            self.assertIsNotNone(dataset.vocab_size)\n            self.assertIsNotNone(dataset.max_seq_len)\n\n            # Check that the properties satisfy basic expectations.\n            self.assertEqual(len(dataset.word_to_idx), len(dataset.idx_to_word))\n            self.assertEqual(len(dataset.word_to_idx), dataset.vocab_size)\n            self.assertEqual(len(dataset.idx_to_word), dataset.vocab_size)\n\n            incomplete_params.clear()\n            dataset_params.clear()\n\n    def test_cornell(self):\n        """"""Train a bot on cornell and display responses when given\n        training data as input -- a sanity check that the data is clean.\n        """"""\n\n        flags = Flags(\n            model_params=dict(\n                ckpt_dir=\'out/tests/test_cornell\',\n                reset_model=True,\n                steps_per_ckpt=50,\n                base_cell=\'GRUCell\',\n                num_layers=1,\n                state_size=128,\n                embed_size=64,\n                max_steps=50),\n            dataset_params=dict(\n                vocab_size=50000,\n                max_seq_len=8,\n                data_dir=\'/home/brandon/Datasets/cornell\'),\n            dataset=\'Cornell\',\n            **self.default_flags)\n\n        bot, dataset = create_bot(flags=flags, return_dataset=True)\n        bot.train()\n\n        del bot\n\n        # Recreate bot (its session is automatically closed after training).\n        flags.model_params[\'reset_model\'] = False\n        flags.model_params[\'decode\'] = True\n        bot, dataset = create_bot(flags, return_dataset=True)\n\n        for inp_sent, resp_sent in dataset.pairs_generator(100):\n            print(\'\\nHuman:\', inp_sent)\n            response = bot.respond(inp_sent)\n            if response == resp_sent:\n                print(\'Robot: %s\\nCorrect!\' % response)\n            else:\n                print(\'Robot: %s\\nExpected: %s\' % (\n                    response, resp_sent))\n\n\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(\'ERROR\')\n    unittest.main()\n'"
tests/test_dynamic_models.py,8,"b'""""""Trial runs on DynamicBot with the TestData Dataset.""""""\n\nimport time\nimport logging\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nimport pydoc\nfrom pydoc import locate\n\nimport data\nimport chatbot\nfrom utils import io_utils, bot_freezer\nfrom tests.utils import *\n\n\nclass TestDynamicModels(unittest.TestCase):\n\n    def setUp(self):\n        tf.logging.set_verbosity(\'ERROR\')\n\n    def test_create_bot(self):\n        """"""Ensure bot constructor is error-free.""""""\n        logging.info(""Creating bot . . . "")\n        bot = create_bot()\n        self.assertIsInstance(bot, chatbot.DynamicBot)\n\n    def test_save_bot(self):\n        """"""Ensure we can save to bot ckpt dir.""""""\n        bot = create_bot()\n        self.assertIsInstance(bot, chatbot.DynamicBot)\n\n    def test_save_bot(self):\n        """"""Ensure teardown operations are working.""""""\n        bot = create_bot()\n        self.assertIsInstance(bot, chatbot.DynamicBot)\n        logging.info(""Closing bot . . . "")\n        bot.close()\n\n    def test_train(self):\n        """"""Simulate a brief training session.""""""\n        flags = TEST_FLAGS\n        flags = flags._replace(model_params=dict(\n            **flags.model_params,\n            reset_model=True,\n            steps_per_ckpt=10))\n        bot = create_bot(flags)\n        self._quick_train(bot)\n\n    def test_base_methods(self):\n        """"""Call each method in chatbot._models.Model, checking for errors.""""""\n        bot = create_bot()\n        logging.info(\'Calling bot.save() . . . \')\n        bot.save()\n        logging.info(\'Calling bot.freeze() . . . \')\n        bot.freeze()\n        logging.info(\'Calling bot.close() . . . \')\n        bot.close()\n\n    def test_manual_freeze(self):\n        """"""Make sure we can freeze the bot, unfreeze, and still chat.""""""\n\n        # ================================================\n        # 1. Create & train bot.\n        # ================================================\n        flags = TEST_FLAGS\n        flags = flags._replace(model_params=dict(\n            ckpt_dir=os.path.join(TEST_DIR, \'out\'),\n            reset_model=True,\n            steps_per_ckpt=20,\n            max_steps=40))\n        bot = create_bot(flags)\n        self.assertEqual(bot.reset_model, True)\n        # Simulate small train sesh on bot.\n        bot.train()\n\n        # ================================================\n        # 2. Recreate a chattable bot.\n        # ================================================\n        # Recreate bot from scratch with decode set to true.\n        logging.info(""Resetting default graph . . . "")\n        tf.reset_default_graph()\n        flags = flags._replace(model_params={\n            **flags.model_params,\n            \'reset_model\': False,\n            \'decode\': True,\n            \'max_steps\': 100,\n            \'steps_per_ckpt\': 50})\n        self.assertTrue(flags.model_params.get(\'decode\'))\n        bot = create_bot(flags)\n        self.assertTrue(bot.is_chatting)\n        self.assertTrue(bot.decode)\n\n        print(""Testing quick chat sesh . . . "")\n        config = io_utils.parse_config(flags=flags)\n        dataset_class = pydoc.locate(config[\'dataset\']) \\\n                        or getattr(data, config[\'dataset\'])\n        dataset = dataset_class(config[\'dataset_params\'])\n        test_input = ""How\'s it going?""\n        encoder_inputs = io_utils.sentence_to_token_ids(\n            tf.compat.as_bytes(test_input),\n            dataset.word_to_idx)\n        encoder_inputs = np.array([encoder_inputs[::-1]])\n        bot.pipeline._feed_dict = {\n            bot.pipeline.user_input: encoder_inputs}\n\n        # Get output sentence from the chatbot.\n        _, _, response = bot.step(forward_only=True)\n        print(""Robot:"", dataset.as_words(response[0][:-1]))\n\n        # ================================================\n        # 3. Freeze the chattable bot.\n        # ================================================\n        logging.info(""Calling bot.freeze() . . . "")\n        bot.freeze()\n\n        # ================================================\n        # 4. Try to unfreeze and use it.\n        # ================================================\n        logging.info(""Resetting default graph . . . "")\n        tf.reset_default_graph()\n        logging.info(""Importing frozen graph into default . . . "")\n        frozen_graph = bot_freezer.load_graph(bot.ckpt_dir)\n\n        logging.info(""Extracting input/output tensors."")\n        tensors, frozen_graph = bot_freezer.unfreeze_bot(bot.ckpt_dir)\n        self.assertIsNotNone(tensors[\'inputs\'])\n        self.assertIsNotNone(tensors[\'outputs\'])\n\n        with tf.Session(graph=frozen_graph) as sess:\n            raw_input = ""How\'s it going?""\n            encoder_inputs  = io_utils.sentence_to_token_ids(\n                tf.compat.as_bytes(raw_input),\n                dataset.word_to_idx)\n            encoder_inputs = np.array([encoder_inputs[::-1]])\n            feed_dict = {tensors[\'inputs\'].name: encoder_inputs}\n            response = sess.run(tensors[\'outputs\'], feed_dict=feed_dict)\n            logging.info(\'Reponse: %s\', response)\n\n\n    def test_memorize(self):\n        """"""Train a bot to memorize (overfit) the small test data, and \n        show its responses to all train inputs when done.\n        """"""\n\n        flags = TEST_FLAGS\n        flags = flags._replace(model_params=dict(\n            ckpt_dir=\'out/test_data\',\n            reset_model=True,\n            steps_per_ckpt=300,\n            state_size=128,\n            embed_size=32,\n            max_steps=300))\n        flags = flags._replace(dataset_params=dict(\n            max_seq_len=20,\n            data_dir=TEST_DATA_DIR))\n        print(\'TEST_FLAGS\', flags.dataset)\n        bot, dataset = create_bot(flags=flags, return_dataset=True)\n        bot.train()\n\n        # Recreate bot (its session is automatically closed after training).\n        flags = flags._replace(model_params={\n            **flags.model_params,\n            \'reset_model\': False,\n            \'decode\': True})\n        bot, dataset = create_bot(flags, return_dataset=True)\n\n        for inp_sent, resp_sent in dataset.pairs_generator():\n            print(\'\\nHuman:\', inp_sent)\n            response = bot.respond(inp_sent)\n            if response == resp_sent:\n                print(\'Robot: %s\\nCorrect!\' % response)\n            else:\n                print(\'Robot: %s\\nExpected: %s\' % (\n                    response, resp_sent))\n\n\n    def _quick_train(self, bot, num_iter=10):\n        """"""Quickly train manually on some test data.""""""\n        coord   = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=bot.sess, coord=coord)\n        for _ in range(num_iter):\n            bot.step()\n        summaries, loss, _ = bot.step()\n        bot.save(summaries=summaries)\n        coord.request_stop()\n        coord.join(threads)\n\n'"
tests/test_legacy_models.py,1,"b'import os\nimport tensorflow as tf\nimport unittest\nimport logging\n\nimport sys\nfrom utils import io_utils\nimport data\nimport chatbot\n\nfrom tests.utils import TEST_FLAGS\n\nclass TestLegacyModels(unittest.TestCase):\n    """"""Test behavior of tf.contrib.rnn after migrating to r1.0.""""""\n\n    def setUp(self):\n        self.seq_len = 20\n        self.config = io_utils.parse_config(flags=TEST_FLAGS)\n        self.dataset = data.TestData(self.config[\'dataset_params\'])\n        self.batch_size = 2\n        logging.basicConfig(level=logging.INFO)\n        self.log = logging.getLogger(\'TestLegacyModels\')\n\n    def test_create(self):\n        """"""Test basic functionality of SimpleBot remains up-to-date with _models.""""""\n        simple_bot = chatbot.SimpleBot(\n            dataset=self.dataset,\n            params=self.config)\n        self.assertIsInstance(simple_bot, chatbot.SimpleBot)\n\n        chat_bot = chatbot.ChatBot(\n            buckets=[(10, 10)],\n            dataset=self.dataset,\n            params=self.config)\n        self.assertIsInstance(chat_bot, chatbot.ChatBot)\n\n    def test_compile(self):\n        """"""Test basic functionality of SimpleBot remains up-to-date with _models.""""""\n        buckets = [(10, 20)]\n\n        # SimpleBot\n        logging.info(""Creating/compiling SimpleBot . . . "")\n        bot = chatbot.SimpleBot(\n            dataset=self.dataset,\n            params=self.config)\n        bot.compile()\n\n        # ChatBot\n        logging.info(""Creating/compiling ChatBot . . . "")\n        bot = chatbot.ChatBot(\n            buckets=buckets,\n            dataset=self.dataset,\n            params=self.config)\n        bot.compile()\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/utils.py,1,"b'""""""Utility functions used by test modules.""""""\n\nimport logging\nimport data\nimport chatbot\n\nimport os\nfrom pydoc import locate\nimport pdb\nfrom utils import io_utils\nimport tensorflow as tf\nfrom chatbot.globals import DEFAULT_FULL_CONFIG\nfrom collections import namedtuple\n\nTEST_DIR = os.path.dirname(os.path.realpath(__file__))\nTEST_DATA_DIR = os.path.join(TEST_DIR, \'test_data\')\nTEST_CONFIG_PATH = os.path.join(TEST_DIR, \'test_config.yml\')\nlogging.basicConfig(level=logging.INFO)\n\n_flag_names = [""pretrained_dir"",\n               ""config"",\n               ""debug"",\n               ""model"",\n               ""model_params"",\n               ""dataset"",\n               ""dataset_params""]\nFlags = namedtuple(\'Flags\', _flag_names)\nTEST_FLAGS = Flags(pretrained_dir=None,\n                   config=TEST_CONFIG_PATH,\n                   debug=True,\n                   model=\'{}\',\n                   dataset=\'{}\',\n                   model_params={\'ckpt_dir\': os.path.join(TEST_DIR, \'out\')},\n                   dataset_params={\'data_dir\': TEST_DATA_DIR})\n\n\ndef create_bot(flags=TEST_FLAGS, return_dataset=False):\n    """"""Chatbot factory: Creates and returns a fresh bot. Nice for \n    testing specific methods quickly.\n    """"""\n    # Wipe the graph and update config if needed.\n    tf.reset_default_graph()\n    config = io_utils.parse_config(flags=flags)\n    io_utils.print_non_defaults(config)\n\n    # Instantiate a new dataset.\n    print(""Setting up"", config[\'dataset\'], ""dataset."")\n    dataset_class = locate(config[\'dataset\']) \\\n                    or getattr(data, config[\'dataset\'])\n    dataset = dataset_class(config[\'dataset_params\'])\n\n    # Instantiate a new chatbot.\n    print(""Creating"", config[\'model\'], "". . . "")\n    bot_class = locate(config[\'model\']) or getattr(chatbot, config[\'model\'])\n    bot = bot_class(dataset, config)\n\n    if return_dataset:\n        return bot, dataset\n    else:\n        return bot\n\n\ndef update_config(config, **kwargs):\n    new_config = {}\n    for key in DEFAULT_FULL_CONFIG:\n        for new_key in kwargs:\n            if new_key in DEFAULT_FULL_CONFIG[key]:\n                if new_config.get(key) is None:\n                    new_config[key] = {}\n                new_config[key][new_key] = kwargs[new_key]\n            elif new_key == key:\n                new_config[new_key] = kwargs[new_key]\n    return {**config, **new_config}\n\n'"
utils/__init__.py,0,b'from utils import io_utils\nfrom utils import bot_freezer\n'
utils/bot_freezer.py,11,"b'""""""Utilities for freezing and unfreezing model graphs and variables on the fly.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom utils import io_utils\nimport os\nimport re\nfrom pydoc import locate\n\n\ndef load_graph(frozen_model_dir):\n    """"""Load frozen tensorflow graph into the default graph.\n\n    Args:\n        frozen_model_dir: location of protobuf file containing frozen graph.\n\n    Returns:\n        tf.Graph object imported from frozen_model_path.\n    """"""\n\n    # Prase the frozen graph definition into a GraphDef object.\n    frozen_file = os.path.join(frozen_model_dir, ""frozen_model.pb"")\n    with tf.gfile.GFile(frozen_file, ""rb"") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    # Load the graph def into the default graph and return it.\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(\n            graph_def,\n            input_map=None,\n            return_elements=None,\n            op_dict=None,\n            producer_op_list=None\n        )\n    return graph\n\n\ndef unfreeze_bot(frozen_model_path):\n    """"""Restores the frozen graph from file and grabs input/output tensors needed to\n    interface with a bot for conversation.\n\n    Args:\n        frozen_model_path: location of protobuf file containing frozen graph.\n\n    Returns:\n        outputs: tensor that can be run in a session.\n    """"""\n\n    bot_graph   = load_graph(frozen_model_path)\n    tensors = {\'inputs\': bot_graph.get_tensor_by_name(\'import/input_pipeline/user_input:0\'),\n               \'outputs\': bot_graph.get_tensor_by_name(\'import/outputs:0\')}\n    return tensors, bot_graph\n\n\ndef unfreeze_and_chat(frozen_model_path):\n    """"""Summon a bot back from the dead and have a nice lil chat with it.""""""\n\n    tensor_dict, graph = unfreeze_bot(frozen_model_path)\n    config  = io_utils.parse_config(pretrained_dir=frozen_model_path)\n    word_to_idx, idx_to_word = get_frozen_vocab(config)\n\n    def as_words(sentence):\n        return "" "".join([tf.compat.as_str(idx_to_word[i]) for i in sentence])\n\n    with tf.Session(graph=graph) as sess:\n\n        def respond_to(sentence):\n            """"""Outputs response sentence (string) given input (string).""""""\n\n            # Convert input sentence to token-ids.\n            sentence_tokens = io_utils.sentence_to_token_ids(\n                tf.compat.as_bytes(sentence), word_to_idx)\n            sentence_tokens = np.array([sentence_tokens[::-1]])\n\n            # Get output sentence from the chatbot.\n            fetches = tensor_dict[\'outputs\']\n            feed_dict={tensor_dict[\'inputs\']: sentence_tokens}\n            response = sess.run(fetches=fetches, feed_dict=feed_dict)\n            return as_words(response[0][:-1])\n\n        sentence = io_utils.get_sentence()\n        while sentence != \'exit\':\n            resp = respond_to(sentence)\n            print(""Robot:"", resp)\n            sentence = io_utils.get_sentence()\n        print(""Farewell, human."")\n\n\ndef get_frozen_vocab(config):\n    """"""Helper function to get dictionaries for translating between tokens and words.""""""\n    data_dir    = config[\'dataset_params\'][\'data_dir\']\n    vocab_size  = config[\'dataset_params\'][\'vocab_size\']\n    vocab_path = os.path.join(data_dir, \'vocab{}.txt\'.format(vocab_size))\n    word_to_idx, idx_to_word = io_utils.get_vocab_dicts(vocab_path)\n    return word_to_idx, idx_to_word\n\n\nclass FrozenBot:\n\n    def __init__(self, frozen_model_dir, vocab_size):\n        print(frozen_model_dir)\n        print(type(frozen_model_dir))\n        self.tensor_dict, self.graph = unfreeze_bot(frozen_model_dir)\n        self.sess = tf.Session(graph=self.graph)\n\n        self.config = {\'dataset_params\': {\n            \'data_dir\': frozen_model_dir, \'vocab_size\': vocab_size}}\n        self.word_to_idx, self.idx_to_word = self.get_frozen_vocab()\n\n    def as_words(self, sentence):\n        return "" "".join([tf.compat.as_str(self.idx_to_word[i]) for i in sentence])\n\n    def __call__(self, sentence):\n        """"""Outputs response sentence (string) given input (string).""""""\n        # Convert input sentence to token-ids.\n        sentence_tokens = io_utils.sentence_to_token_ids(\n            tf.compat.as_bytes(sentence), self.word_to_idx)\n        sentence_tokens = np.array([sentence_tokens[::-1]])\n\n        # Get output sentence from the chatbot.\n        fetches = self.tensor_dict[\'outputs\']\n        feed_dict={self.tensor_dict[\'inputs\']: sentence_tokens}\n        response = self.sess.run(fetches=fetches, feed_dict=feed_dict)\n        return self.as_words(response[0][:-1])\n\n'"
utils/io_utils.py,6,"b'""""""Utilities for downloading data from various datasets, tokenizing, vocabularies.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\nimport sys\nimport yaml\nimport copy\nimport pandas as pd\nimport logging\n\nimport tensorflow as tf\nfrom collections import Counter, namedtuple\nfrom tensorflow.python.platform import gfile\nfrom subprocess import Popen, PIPE\nfrom chatbot.globals import DEFAULT_FULL_CONFIG\n\n\n# Special vocabulary symbols.\n_PAD = b""_PAD""      # Append to unused space for both encoder/decoder.\n_GO = b""_GO""       # Prepend to each decoder input.\n_EOS = b""_EOS""      # Append to outputs only. Stopping signal when decoding.\n_UNK = b""_UNK""      # For any symbols not in our vocabulary.\n_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n\n# Enumerations for ease of use by this and other files.\nPAD_ID = 0\nGO_ID = 1\nEOS_ID = 2\nUNK_ID = 3\n\n# Regular expressions used to tokenize.\n_WORD_SPLIT = re.compile(b""([.,!?\\""\':;)(])"")\n_DIGIT_RE = re.compile(br""\\d"")\n\n# Build mock FLAGS object for utils to wrap info around if needed.\n# This makes the API more user-friendly, since it takes care of\n# formatting data if the user doesn\'t do it exactly as expected.\n# Note: I did initially try this with an actual tf.app.flags object,\n# but it was a nightmare.\n_flag_names = [""pretrained_dir"",\n               ""config"",\n               ""debug"",\n               ""model"",\n               ""model_params"",\n               ""dataset"",\n               ""dataset_params""]\nFlags = namedtuple(\'Flags\', _flag_names)\n_FLAGS = Flags(pretrained_dir=None,\n               config=None,\n               debug=None,\n               model=\'{}\',\n               dataset=\'{}\',\n               model_params=\'{}\',\n               dataset_params=\'{}\')\n\n\ndef save_hyper_params(hyper_params, fname):\n    # Append to file if exists, else create.\n    df = pd.DataFrame(hyper_params)\n    with open(fname, \'a+\') as f:\n        df.to_csv(f, header=False)\n\n\ndef get_sentence(lower=True):\n    """"""Simple function to prompt user for input and return it w/o newline.\n    Frequently used in chat sessions, of course.\n    """"""\n    sys.stdout.write(""Human: "")\n    sys.stdout.flush()\n    sentence = input()\n    if lower:\n        return sentence.lower()\n    return sentence\n\n\ndef update_config(config=None,\n                  config_path=None,\n                  return_config=True,\n                  **kwargs):\n    """"""Update contents of a config file, overwriting any that \n    match those in kwargs.\n   \n    Args:\n        config: (dict) subset of DEFAULT_FULL_CONFIG.\n        config_path: (str) location of a yaml config file.\n        return_config: (bool) whether or not to return the config dictionary.\n        kwargs: key-value pairs to update in the config dictionary and/or file.\n         \n    At least one of {config, config_path} must be not None. If both are not\n    None, then we update the config dictionary with the kwargs, and then set\n    the file at config_path to match updated dictionary contents. \n    \n    In other words, if config is not None, we do won\'t consider the contents \n    of config_path when doing the updates.\n    """"""\n\n    if config is None and config_path is None:\n        raise ValueError(""Configuration info not given to update_config."")\n\n    if config is None:\n        # Grab the current config file contents into a dictionary.\n        config = get_yaml_config(config_path)\n        logging.info(""Updating config values %r for %s"",\n                     list(kwargs.keys()), config_path)\n\n    # Update its values with those in kwargs.\n    for top_level_key in DEFAULT_FULL_CONFIG:\n        for update_key in kwargs:\n            if update_key == top_level_key:\n                config[update_key] = kwargs[update_key]\n            elif update_key in DEFAULT_FULL_CONFIG[top_level_key]:\n                if config.get(top_level_key) is None:\n                    config[top_level_key] = {}\n                config[top_level_key][update_key] = kwargs[update_key]\n\n    # Rewrite the config file.\n    if config_path is not None:\n        with open(os.path.join(config_path), \'w\') as f:\n            yaml.dump(config, f, default_flow_style=False)\n\n    # Return the dictionary if requested.\n    if return_config:\n        return config\n\n\ndef get_yaml_config(path, save_path=True):\n    with open(path) as file:\n        config = yaml.load(file)\n        if save_path:\n            if config.get(\'dataset_params\') is not None:\n                config[\'dataset_params\'][\'config_path\'] = path\n    return config\n\n\ndef load_pretrained_config(pretrained_dir):\n    """"""Get the full configuration dictionary for a pretrained model.\n\n    Args:\n        pretrained_dir: path (relative to project root) that is assumed to contain:\n        - config.yml: full configuration file (automatically saved by all models).\n        - checkpoint(s) from training session (also saved automatically).\n\n    Returns:\n        config: dictionary loaded from config.yml, and with all training flags reset to\n                chat session flags, since the only time this is called is for chatting.\n    """"""\n    config_path = os.path.join(pretrained_dir, ""config.yml"")\n    config = get_yaml_config(config_path)\n    # The loaded config will have ""training"" values, so we need\n    # to set some of them to ""chatting"" values, instead of requiring\n    # user to specify them (since they are mandatory for any chat sesion).\n    config[\'model_params\'][\'decode\'] = True\n    config[\'model_params\'][\'is_chatting\'] = True  # alias\n    config[\'model_params\'][\'reset_model\'] = False\n    config[\'model_params\'][\'ckpt_dir\'] = pretrained_dir\n    return config\n\n\ndef print_non_defaults(config):\n    """"""Prints all values in config that aren\'t the default values in DEFAULT_FULL_CONFIG.\n    Args:\n        config: dict of parameters with same structure as DEFAULT_FULL_CONFIG.\n    """"""\n\n    print(""\\n---------- Your non-default parameters: ----------"")\n    if config[\'model\'] != DEFAULT_FULL_CONFIG[\'model\']:\n        print(""{}: {}"".format(\'model\', config[\'model\']))\n    if config[\'dataset\'] != DEFAULT_FULL_CONFIG[\'dataset\']:\n        print(""{}: {}"".format(\'dataset\', config[\'dataset\']))\n\n    for dict_id in [\'model_params\', \'dataset_params\']:\n        print(dict_id, end="":\\n"")\n        for key, val in config[dict_id].items():\n            # First check if key isn\'t even specified by defaults.\n            if key not in DEFAULT_FULL_CONFIG[dict_id]:\n                print(""\\t{}: {}"".format(key, val))\n            elif DEFAULT_FULL_CONFIG[dict_id][key] != val:\n                print(""\\t{}: {}"".format(key, val))\n    print(""--------------------------------------------------\\n"")\n\n\ndef flags_to_dict(flags):\n    """"""Builds and return a dictionary from flags keys, namely\n       \'model\', \'dataset\', \'model_params\', \'dataset_params\'.\n    """"""\n\n    if isinstance(flags, dict):\n        logging.warning(\'The `flags` object is already a dictionary!\')\n        return flags\n\n    if flags.pretrained_dir is not None:\n        config = load_pretrained_config(flags.pretrained_dir)\n        config[\'model_params\'] = {**config[\'model_params\'],\n                                  **yaml.load(getattr(flags, \'model_params\'))}\n        return config\n\n    flags_dict = {}\n    # Grab any values under supported keys defined in default config.\n    for stream in DEFAULT_FULL_CONFIG:\n\n        stream_attr = getattr(flags, stream)\n        if not isinstance(stream_attr, dict):\n            yaml_stream = yaml.load(getattr(flags, stream))\n        else:\n            yaml_stream = stream_attr\n\n        if yaml_stream:\n            flags_dict.update({stream: yaml_stream})\n        elif stream in [\'model_params\', \'dataset_params\']:\n            # Explicitly set it as empty for merging with default later.\n            flags_dict[stream] = {}\n\n    # If provided, incorporate yaml config file as well.\n    # Give preference to values in flags_dict, since those are\n    # values provided by user on command-line.\n    if flags.config is not None:\n        yaml_config = get_yaml_config(flags.config)\n        flags_dict = merge_dicts(\n            default_dict=yaml_config,\n            preference_dict=flags_dict)\n\n    return flags_dict\n\n\ndef merge_dicts(default_dict, preference_dict):\n    """"""Preferentially (and recursively) merge input dictionaries.\n    \n    Ensures that all values in preference dict are used, and\n    all other (i.e. unspecified) items are from default dict.\n    """"""\n\n    merged_dict = copy.deepcopy(default_dict)\n    for pref_key in preference_dict:\n        if isinstance(preference_dict[pref_key], dict) and pref_key in merged_dict:\n            # Dictionaries are expected to have the same type structure.\n            # So if any preference_dict[key] is a dict, then require default_dict[key]\n            # must also be a dict (if it exists, that is).\n            assert isinstance(merged_dict[pref_key], dict), \\\n                ""Expected default_dict[%r]=%r to have type dict."" % \\\n                (pref_key, merged_dict[pref_key])\n            # Since these are both dictionaries, can just recurse.\n            merged_dict[pref_key] = merge_dicts(merged_dict[pref_key],\n                                                preference_dict[pref_key])\n        else:\n            merged_dict[pref_key] = preference_dict[pref_key]\n    return merged_dict\n\n\ndef parse_config(flags=None, pretrained_dir=None, config_path=None):\n    """"""Get custom configuration dictionary from either a tensorflow flags \n    object, a path to a training directory, or a path to a yaml file. Only pass \n    one of these. See ""Args"" below for more details.\n    \n    The result is a dictionary of the same key-val structure as seen in\n    chatbot.globals.DEFAULT_FULL_CONFIG. For any key-value pair not found from\n    the (single) argument passed, it will be set to the default found in\n    DEFAULT_FULL_CONFIG.\n\n    Args:\n        flags: A tf.app.flags.FLAGS object. See FLAGS in main.py.\n        pretrained_dir: relative [to project root] path to a pretrained model \n            directory, i.e. a directory where a chatbot was previously \n            saved/trained (a ckpt_dir).\n        config_path: relative [to project root] path to a valid yaml \n            configuration file. For example: \'configs/my_config.yml\'.\n\n    Returns:\n        config: dictionary of merged config info, where precedence is given to\n        user-specified params on command-line (over .yml config files).\n    """"""\n\n    # Only pass one of the options!\n    assert sum(x is not None for x in [flags, pretrained_dir, config_path]) == 1\n\n    # Build a flags object from other params, if it doesn\'t exist.\n    if flags is None:\n\n        # Get the config_path from the pretrained directory.\n        if config_path is None:\n            config_path = os.path.join(pretrained_dir, \'config.yml\')\n        assert gfile.Exists(config_path), \\\n            ""Cannot parse from %s. No config.yml."" % config_path\n\n        # Wrap flags string inside an actual tf.app.flags object.\n        flags = _FLAGS\n        flags = flags._replace(config=config_path)\n    assert flags is not None\n\n    # Get configuration dictionary containing user-specified parameters.\n    config = flags_to_dict(flags)\n\n    # Sanity check: make sure we have values that don\'t have defaults.\n    if \'ckpt_dir\' not in config[\'model_params\']:\n        print(\'Robot: Please enter a directory for saving checkpoints:\')\n        config[\'model_params\'][\'ckpt_dir\'] = get_sentence(lower=False)\n    if \'data_dir\' not in config[\'dataset_params\']:\n        print(\'Robot: Please enter full path to directory containing data:\')\n        config[\'dataset_params\'][\'data_dir\'] = get_sentence(lower=False)\n\n    # Then, fill in any blanks with the full default config.\n    config = merge_dicts(default_dict=DEFAULT_FULL_CONFIG,\n                         preference_dict=config)\n    return config\n\n\ndef basic_tokenizer(sentence):\n    """"""Very basic tokenizer: split the sentence into a list of tokens.""""""\n    words = []\n    for space_separated_fragment in sentence.strip().lower().split():\n        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n    return [w for w in words if w]\n\n\ndef num_lines(file_path):\n    """"""Return the number of lines in file given by its absolute path.""""""\n    (num_samples, stderr) = Popen([\'wc\', \'-l\', file_path], stdout=PIPE).communicate()\n    return int(num_samples.strip().split()[0])\n\n\ndef get_word_freqs(path, counter, norm_digits=True):\n    """"""Extract word-frequency mapping from file given by path.\n    \n    Args:\n        path: data file of words we wish to extract vocab counts from.\n        counter: collections.Counter object for mapping word -> frequency.\n        norm_digits: Boolean; if true, all digits are replaced by 0s.\n    \n    Returns:\n        The counter (dict), updated with mappings from word -> frequency. \n    """"""\n\n    print(""Creating vocabulary for data"", path)\n    with gfile.GFile(path, mode=""rb"") as f:\n        for i, line in enumerate(f):\n            if (i + 1) % 100000 == 0:\n                print(""\\tProcessing line"", (i + 1))\n            line = tf.compat.as_bytes(line)\n            tokens = basic_tokenizer(line)\n            # Update word frequency counts in vocab counter dict.\n            for w in tokens:\n                word = _DIGIT_RE.sub(b""0"", w) if norm_digits else w\n                counter[word] += 1\n        return counter\n\n\ndef create_vocabulary(vocab_path, from_path, to_path, max_vocab_size, norm_digits=True):\n    """"""Create vocabulary file (if it does not exist yet) from data file.\n\n    Data file is assumed to contain one sentence per line. Each sentence is\n    tokenized and digits are normalized (if norm_digits is set).\n    Vocabulary contains the most-frequent tokens up to max_vocab_size.\n    We write it to vocabulary_path in a one-token-per-line format, so that later\n    token in the first line gets id=0, second line gets id=1, and so on.\n\n    Args:\n      vocab_path: path where the vocabulary will be created.\n      from_path: data file for encoder inputs.\n      to_path: data file for decoder inputs.\n      max_vocab_size: limit on the size of the created vocabulary.\n        norm_digits: Boolean; if true, all digits are replaced by 0s.\n    """"""\n\n    if gfile.Exists(vocab_path):\n        return num_lines(vocab_path)\n\n    vocab = Counter()\n    # Pool all data words together to reflect the data distribution well.\n    vocab = get_word_freqs(from_path, vocab, norm_digits)\n    vocab = get_word_freqs(to_path, vocab, norm_digits)\n\n    # Get sorted vocabulary, from most frequent to least frequent.\n    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n    vocab_list = vocab_list[:max_vocab_size]\n\n    # Write the list to a file.\n    with gfile.GFile(vocab_path, mode=""wb"") as vocab_file:\n        for w in vocab_list:\n            vocab_file.write(w + b""\\n"")\n\n    return len(vocab_list)\n\n\ndef get_vocab_dicts(vocabulary_path):\n    """"""Returns word_to_idx, idx_to_word dictionaries given vocabulary.\n\n    Args:\n      vocabulary_path: path to the file containing the vocabulary.\n\n    Returns:\n      a pair: the vocabulary (a dictionary mapping string to integers), and\n      the reversed vocabulary (a list, which reverses the vocabulary mapping).\n\n    Raises:\n      ValueError: if the provided vocabulary_path does not exist.\n    """"""\n    if gfile.Exists(vocabulary_path):\n        rev_vocab = []\n        with gfile.GFile(vocabulary_path, mode=""rb"") as f:\n            rev_vocab.extend(f.readlines())\n        rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\n        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n        return vocab, rev_vocab\n    else:\n        raise ValueError(""Vocabulary file %s not found."", vocabulary_path)\n\n\ndef sentence_to_token_ids(sentence, vocabulary, normalize_digits=True):\n    """"""Convert a string to list of integers representing token-ids.\n\n    For example, a sentence ""I have a dog"" may become tokenized into\n    [""I"", ""have"", ""a"", ""dog""] and with vocabulary {""I"": 1, ""have"": 2,\n    ""a"": 4, ""dog"": 7""} this function will return [1, 2, 4, 7].\n\n    Args:\n      sentence: the sentence in bytes format to convert to token-ids.\n      vocabulary: a dictionary mapping tokens to integers.\n      normalize_digits: Boolean; if true, all digits are replaced by 0s.\n\n    Returns:\n      a list of integers, the token-ids for the sentence.\n    """"""\n    words = basic_tokenizer(sentence)\n\n    if not normalize_digits:\n        return [vocabulary.get(w, UNK_ID) for w in words]\n\n    # Normalize digits by 0 before looking words up in the vocabulary.\n    return [vocabulary.get(_DIGIT_RE.sub(b""0"", w), UNK_ID) for w in words]\n\n\ndef data_to_token_ids(data_path, target_path, vocabulary_path, normalize_digits=True):\n    """"""Tokenize data file and turn into token-ids using given vocabulary file.\n\n    This function loads data line-by-line from data_path, calls the above\n    sentence_to_token_ids, and saves the result to target_path.\n\n    Args:\n      data_path: path to the data file in one-sentence-per-line format.\n      target_path: path where the file with token-ids will be created.\n      vocabulary_path: path to the vocabulary file.\n      normalize_digits: Boolean; if true, all digits are replaced by 0s.\n    """"""\n    if not gfile.Exists(target_path):\n        print(""Tokenizing data in %s"" % data_path)\n        vocab, _ = get_vocab_dicts(vocabulary_path=vocabulary_path)\n        with gfile.GFile(data_path, mode=""rb"") as data_file:\n            with gfile.GFile(target_path, mode=""w"") as tokens_file:\n                counter = 0\n                for line in data_file:\n                    counter += 1\n                    if counter % 100000 == 0:\n                        print(""  tokenizing line %d"" % counter)\n                    token_ids = sentence_to_token_ids(\n                        tf.compat.as_bytes(line), vocab, normalize_digits)\n                    tokens_file.write("" "".join([str(tok) for tok in token_ids]) + ""\\n"")\n\n\ndef prepare_data(data_dir,\n                 vocab_size,\n                 from_train_path=None,\n                 to_train_path=None,\n                 from_valid_path=None,\n                 to_valid_path=None,\n                 optimize=True,\n                 config_path=None):\n\n    """"""Prepare all necessary files that are required for the training.\n\n    Args:\n        data_dir: directory in which the data sets will be stored.\n        from_train_path: path to the file that includes ""from"" training samples.\n        to_train_path: path to the file that includes ""to"" training samples.\n        from_valid_path: path to the file that includes ""valid_from"" samples.\n        to_valid_path: path to the file that includes ""valid_to"" samples.\n        vocab_size: preferred number of words to use in vocabulary.\n        optimize: if True, allow program to rest this value if the actual\n            vocab_size (num unique words in data) < preferred vocab_size. \n            This would decrease computational cost, should the situation arise.\n        config_path: (required if optimize==True) location of config file.\n        \n    Note on optimize:\n    - It will only have an effect if the following conditions are ALL met:\n      - config_path is not None (and is a valid path)\n      - optimize == True (of course)\n      - true vocab size != [preferred] vocab_size\n\n    Returns:\n        Tuple of:\n        (1) path to the token-ids for ""from language"" training data-set,\n        (2) path to the token-ids for ""to language"" training data-set,\n        (3) path to the token-ids for ""from language"" development data-set,\n        (4) path to the token-ids for ""to language"" development data-set,\n        (5) path to the vocabulary file,\n        (6) the true vocabulary size (less than or equal to max allowed)\n    """"""\n\n    if optimize is None:\n        logging.warning(""You have not requested that your choice for ""\n                        ""vocab_size be optimized. This can lead to slower ""\n                        ""training times.\\nSet \'optimize_params: true\' under ""\n                        ""dataset_params in your yaml config to enable."")\n\n    def maybe_set_param(param, file_name):\n        if param is None:\n            param = os.path.join(data_dir, file_name)\n            logging.info(\'Set path from None to %s\', param)\n        return param\n\n    def get_vocab_path(vocab_size):\n        return os.path.join(data_dir, ""vocab%d.txt"" % vocab_size)\n\n    def append_to_paths(s, **paths):\n        return {name: path + s for name, path in paths.items()}\n\n    # Set any paths that are None to default values.\n    from_train_path = maybe_set_param(from_train_path, \'train_from.txt\')\n    to_train_path = maybe_set_param(to_train_path, \'train_to.txt\')\n    from_valid_path = maybe_set_param(from_valid_path, \'valid_from.txt\')\n    to_valid_path = maybe_set_param(to_valid_path, \'valid_to.txt\')\n\n    # Create vocabularies of the appropriate sizes.\n    vocab_path = get_vocab_path(vocab_size)\n    true_vocab_size = create_vocabulary(\n        vocab_path,\n        from_train_path,\n        to_train_path,\n        vocab_size)\n    assert true_vocab_size <= vocab_size\n\n    # User-permitted, we reset the config file\'s vocab size and rename the\n    # vocabulary path name to the optimal values.\n    should_optimize = config_path is not None\n    should_optimize = (vocab_size != true_vocab_size) and should_optimize\n    should_optimize = optimize and should_optimize\n    if should_optimize:\n        logging.info(\'Optimizing vocab size in config and renaming files.\')\n        # Necessary when we overestimate the number of unique words in the data.\n        # e.g. we set vocab_size = 40k but our data only has 5 unique words,\n        # it would be wasteful to train a model on 40k.\n        # Thus, we rename vocab filenames to have the true vocab size.\n        vocab_size = true_vocab_size\n        old_vocab_path = vocab_path\n        vocab_path = get_vocab_path(true_vocab_size)\n        if old_vocab_path != vocab_path:\n            Popen([\'mv\', old_vocab_path, vocab_path], stdout=PIPE).communicate()\n\n        # Reset the value of \'vocab_size\' in the configuration file, so that\n        # we won\'t need to regenerate everything again if the user wants to\n        # resume training/chat/etc.\n        update_config(config_path=config_path, vocab_size=true_vocab_size)\n\n    id_paths = append_to_paths(\n        \'.ids%d\' % vocab_size,\n        from_train=from_train_path,\n        to_train=to_train_path,\n        from_valid=from_valid_path,\n        to_valid=to_valid_path)\n\n    # Create token ids for all training and validation data.\n    for name in id_paths:\n        data_to_token_ids(\n            eval(name + \'_path\'),\n            id_paths[name],\n            vocab_path)\n\n    return id_paths, vocab_path, vocab_size\n'"
webpage/__init__.py,0,b''
webpage/config.py,0,"b""import os\nbasedir = os.path.abspath(os.path.dirname(__file__))\n\n\nclass Config:\n\n    DEFAULT_THEME = 'lumen'\n    # Boolean: True if you == Brandon McKinze; False otherwise :)\n    FLASK_PRACTICE_ADMIN = os.getenv('true')\n    # Activates the cross-site request forgery prevention.\n    WTF_CSRF_ENABLED = True\n    # Used to create cryptographic token used to valide a form.\n    SECRET_KEY = os.getenv('SECRET_KEY', 'not-really-a-secret-now')\n\n    # SQLAlchemy configuration.\n    SQLALCHEMY_TRACK_MODIFICATIONS = False\n    SQLALCHEMY_COMMIT_ON_TEARDOWN = True\n\n    # Username/password for flask admin access.\n    # COVER YOUR EYES - LOOK AWAY - NOTHING TO SEE HERE\n    BASIC_AUTH_USERNAME = os.getenv('BASIC_AUTH_USERNAME', 'admin')\n    BASIC_AUTH_PASSWORD = os.getenv('BASIC_AUTH_PASSWORD', 'password')\n\n    @staticmethod\n    def init_app(app):\n        pass\n\n\nclass DevelopmentConfig(Config):\n    DEBUG = True\n    # Path of our db file. Required by Flask-SQLAlchemy extension.\n    SQLALCHEMY_DATABASE_URI = 'sqlite:///' + os.path.join(basedir, 'data_dev.db')\n\n\nclass TestingConfig(Config):\n    TESTING = True\n    # Path of our db file. Required by Flask-SQLAlchemy extension.\n    SQLALCHEMY_DATABASE_URI = 'sqlite:///' + os.path.join(basedir, 'data_test.db')\n\n\nclass ProductionConfig(Config):\n    # Path of our db file. Required by Flask-SQLAlchemy extension.\n    SQLALCHEMY_DATABASE_URI = 'sqlite:///' + os.path.join(basedir, 'data.db')\n    SESSION_COOKIE_HTTPONLY = False\n    PREFERRED_URL_SCHEME = 'https'\n\nconfig = {\n    'development': DevelopmentConfig,\n    'testing': TestingConfig,\n    'production': ProductionConfig,\n    'default': DevelopmentConfig\n}\n\n"""
webpage/manage.py,0,"b'#!/usr/bin/env python3\n\n""""""manage.py: Start up the web server and the application.""""""\n\nimport os\nfrom deepchat import create_app, db\nfrom deepchat.models import User, Chatbot, Conversation, Turn\n\nfrom flask_script import Manager, Shell\nfrom flask_migrate import Migrate, MigrateCommand\n\n# First check if we are being called on the app engine.\nconfig_name = os.getenv(\'APPENGINE_CONFIG\')\n# If not, either set to my (Brandon) preference given by FLASK_CONFIG, or\n# set to default if not found (e.g. you != Brandon || haven\'t set FLASK_CONFIG)\nif config_name is None:\n    config_name = os.getenv(\'FLASK_CONFIG\', \'default\')\n\napp = create_app(config_name)\n# For better CLI.\nmanager = Manager(app)\n# Database tables can be created or upgraded with a single command:\n# python3 manage.py db upgrade\nmigrate = Migrate(app, db)\n\n\ndef make_shell_context():\n    """"""Automatic imports when we want to play in the shell.""""""\n    return dict(app=app,\n                db=db,\n                User=User,\n                Chatbot=Chatbot,\n                Conversation=Conversation,\n                Turn=Turn)\n\nmanager.add_command(""shell"", Shell(make_context=make_shell_context))\n\n# Give manager \'db\' command.\n# Now, \'manage.py db [options]\' runs the flask_migrate.Migrate method.\nmanager.add_command(\'db\', MigrateCommand)\n\n\n@manager.command\ndef test():\n    """"""Run the unit tests (see the tests package).\n\n    This can be run from the cmd line via \'python3 manage.py test\'.\n\n    Note: the decorator above allows us to define this as a custom method\n    for our manager object.\n    """"""\n    import unittest\n    tests = unittest.TestLoader().discover(\'tests\')\n    unittest.TextTestRunner(verbosity=2).run(tests)\n\n\n@manager.command\ndef deploy():\n    from flask_migrate import upgrade\n    # Migrate db to latest revision.\n    upgrade()\n\nif __name__ == \'__main__\':\n    manager.run()'"
chatbot/components/__init__.py,0,"b'from chatbot.components.embedder import Embedder\nfrom chatbot.components.input_pipeline import InputPipeline\nfrom chatbot.components.encoders import BasicEncoder, BidirectionalEncoder\nfrom chatbot.components.decoders import BasicDecoder, AttentionDecoder\n\n__all__ = [""InputPipeline"",\n           ""Embedder"",\n           ""BasicEncoder"",\n           ""BidirectionalEncoder"",\n           ""BasicDecoder"",\n           ""AttentionDecoder""]'"
chatbot/components/bot_ops.py,68,"b'""""""Custom TF \'ops\' as meant in the TensorFlow definition of ops.""""""\n\nimport numpy as np\nimport tensorflow as tf\nfrom utils import io_utils\nfrom tensorflow.python.util import nest\n\n\ndef dynamic_sampled_softmax_loss(labels, logits, output_projection, vocab_size,\n                                 from_scratch=False, num_samples=512, name=None):\n    """"""Sampled softmax loss function able to accept 3D Tensors as input,\n       as opposed to the official TensorFlow support for <= 2D. This is\n       dynamic because it can be applied across variable-length sequences,\n       which are unspecified at initialization with size \'None\'.\n\n       Args:\n        labels: 2D integer tensor of shape [batch_size, None] containing\n            the word ID labels for each individual rnn state from logits.\n        logits: 3D float tensor of shape [batch_size, None, state_size] as\n            ouput by a DynamicDecoder instance.\n        from_scratch: (bool) Whether to use the version I wrote from scratch, or to use\n                      the version I wrote that applies map_fn(sampled_softmax) across timeslices, which\n                      is probably less efficient. (Currently testing)\n        num\n        Returns:\n            loss as a scalar Tensor, computed as the mean over all batches and sequences.\n    """"""\n\n    if from_scratch:\n        return _dynamic_sampled_from_scratch(labels, logits, output_projection, vocab_size,\n                                             num_samples=num_samples, name=name)\n    else:\n        return _dynamic_sampled_map(labels, logits, output_projection, vocab_size,\n                                    num_samples=num_samples, name=name)\n\n\ndef _dynamic_sampled_map(labels, logits, output_projection, vocab_size,\n                                 num_samples=512, name=None):\n    """"""Sampled softmax loss function able to accept 3D Tensors as input,\n       as opposed to the official TensorFlow support for <= 2D. This is\n       dynamic because it can be applied across variable-length sequences,\n       which are unspecified at initialization with size \'None\'.\n\n       Args:\n           labels: 2D integer tensor of shape [batch_size, None] containing\n                the word ID labels for each individual rnn state from logits.\n            logits: 3D float tensor of shape [batch_size, None, state_size] as\n                ouput by a DynamicDecoder instance.\n\n        Returns:\n            loss as a scalar Tensor, computed as the mean over all batches and sequences.\n    """"""\n    with tf.name_scope(name, ""dynamic_sampled_softmax_loss"", [labels, logits, output_projection]):\n        seq_len = tf.shape(logits)[1]\n        st_size = tf.shape(logits)[2]\n        time_major_outputs = tf.reshape(logits, [seq_len, -1, st_size])\n        time_major_labels = tf.reshape(labels, [seq_len, -1])\n        # Reshape is apparently faster (dynamic) than transpose.\n        w_t = tf.reshape(output_projection[0], [vocab_size, -1])\n        b = output_projection[1]\n        def sampled_loss(elem):\n            logits, lab = elem\n            lab = tf.reshape(lab, [-1, 1])\n            # TODO: Figure out how this accurately gets loss without requiring weights,\n            # like sparse_softmax_cross_entropy requires.\n            return tf.reduce_mean(\n                tf.nn.sampled_softmax_loss(\n                    weights=w_t,\n                    biases=b,\n                    labels=lab,\n                    inputs=logits,\n                    num_sampled=num_samples,\n                    num_classes=vocab_size,\n                    partition_strategy=\'div\'))\n        batch_losses = tf.map_fn(sampled_loss,\n                                 (time_major_outputs, time_major_labels),\n                                 dtype=tf.float32)\n        loss = tf.reduce_mean(batch_losses)\n    return loss\n\n\ndef _dynamic_sampled_from_scratch(labels, logits, output_projection, vocab_size,\n                                  num_samples, name=None):\n    """"""Note: I closely follow the notation from Tensorflow\'s Candidate Sampling reference.\n       - Link: https://www.tensorflow.org/extras/candidate_sampling.pdf\n\n    Args:\n        output_projection: (tuple) returned by any DynamicDecoder.get_projections_tensors()\n            - output_projection[0] == w tensor. [state_size, vocab_size]\n            - output_projection[0] == b tensor. [vocab_size]\n        labels: 2D Integer tensor. [batch_size, None]\n        logits: 3D float Tensor [batch_size, None, state_size].\n            - In this project, usually is the decoder batch output sequence (NOT projected).\n        num_samples: number of classes out of vocab_size possible to use.\n        vocab_size: total number of classes.\n    """"""\n    with tf.name_scope(name, ""dynamic_sampled_from_scratch"", [labels, logits, output_projection]):\n        batch_size, seq_len, state_size  = tf.unstack(tf.shape(logits))\n        time_major_outputs  = tf.reshape(logits, [seq_len, batch_size, state_size])\n        time_major_labels   = tf.reshape(labels, [seq_len, batch_size])\n\n        weights = tf.transpose(output_projection[0])\n        biases = output_projection[1]\n        def sampled_loss_single_timestep(args):\n            """"""\n            Args: 2-tuple (because map_fn below)\n                targets: 1D tensor (sighs loudly) of shape [batch_size]\n                logits: 2D tensor (sighs intensify) of shape [batch_size, state_size].\n            """"""\n            logits, targets = args\n            with tf.name_scope(""compute_sampled_logits"", [weights, biases, logits, targets]):\n                targets = tf.cast(targets, tf.int64)\n                sampled_values = tf.nn.log_uniform_candidate_sampler(\n                    true_classes=tf.expand_dims(targets, -1),\n                    num_true=1,\n                    num_sampled=num_samples,\n                    unique=True,\n                    range_max=vocab_size)\n                S, Q_true, Q_samp = (tf.stop_gradient(s) for s in sampled_values)\n\n                # Get concatenated 1D tensor of shape [batch_size * None + num_samples],\n                all_ids = tf.concat([targets, S], 0)\n                _W = tf.nn.embedding_lookup(weights, all_ids, partition_strategy=\'div\')\n                _b = tf.nn.embedding_lookup(biases, all_ids)\n\n                W = {\'targets\': tf.slice(_W, begin=[0, 0], size=[batch_size, state_size]),\n                     \'samples\': tf.slice(_W, begin=[batch_size, 0], size=[num_samples, state_size])}\n                b = {\'targets\': tf.slice(_b, begin=[0], size=[batch_size]),\n                     \'samples\': tf.slice(_b, begin=[batch_size], size=[num_samples])}\n\n                true_logits  = tf.reduce_sum(tf.multiply(logits, W[\'targets\']), 1)\n                true_logits += b[\'targets\'] - tf.log(Q_true)\n\n                sampled_logits  = tf.matmul(logits, W[\'samples\'], transpose_b=True)\n                sampled_logits += b[\'samples\'] - tf.log(Q_samp)\n\n                F = tf.concat([true_logits, sampled_logits], 1)\n                def fn(s_i): return tf.where(targets == s_i, tf.ones_like(targets), tf.zeros_like(targets))\n                sample_labels = tf.transpose(tf.map_fn(fn, S))\n                out_targets = tf.concat([tf.ones_like(true_logits, dtype=tf.int64), sample_labels], 1)\n            return tf.losses.softmax_cross_entropy(out_targets, logits=F)\n\n        return tf.reduce_mean(tf.map_fn(sampled_loss_single_timestep,\n                                        (time_major_outputs, time_major_labels),\n                                        dtype=tf.float32))\n\n\ndef cross_entropy_sequence_loss(logits, labels, weights):\n    """"""My version of various tensorflow sequence loss implementations I\'ve \n    seen. They all seem to do the basic operations below, but in a much more\n    roundabout way. This version is able to be simpler because it assumes that\n    the inputs are coming from a chatbot.Model subclass.\n    """"""\n    with tf.name_scope(\'cross_entropy_sequence_loss\'):\n        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=logits, labels=labels)\n\n        # We can get the sequence lengths simply by casting all PAD labels\n        # with 0 and everything else with 1.\n        weights = tf.to_float(weights)\n        losses = tf.multiply(losses, weights)\n        return tf.reduce_sum(losses) / tf.reduce_sum(weights)\n\n\ndef dot_prod(x, y):\n    return tf.reduce_sum(tf.multiply(x, y))\n\n\ndef bahdanau_score(attention_dim, h_j, s_i):\n    state_size = tf.get_shape(h_j)[0]\n    h_proj = tf.get_variable(\'W_1\',\n                             [attention_dim, state_size],\n                             dtype=tf.float32)\n    s_proj = tf.get_variable(\'W_2\',\n                             [attention_dim, state_size],\n                             dtype=tf.float32)\n    v = tf.get_variable(\'v\',\n                        [attention_dim, state_size],\n                        dtype=tf.float32)\n    score = dot_prod(v, tf.tanh(h_proj + s_proj))\n    return score\n\n\ndef luong_score(attention_dim, h_j, s_i):\n    h_proj = tf.get_variable(\'W_1\',\n                             [attention_dim, tf.get_shape(h_j)[0]],\n                             dtype=tf.float32)\n    s_proj = tf.get_variable(\'W_2\',\n                             [attention_dim, tf.get_shape(s_i)[0]],\n                             dtype=tf.float32)\n    score = dot_prod(h_proj, s_proj)\n    return score\n\n\ndef linear_map(args, output_size, biases=None):\n    """"""Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n    \n    Basically, you pass in a bunch of vectors (ok you got me, 2D tensors because\n    batch dimensions) that you want added together but need their dimensions\n    to match. This function has you covered.\n\n    Args:\n        args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n        output_size: int, second dimension of W[i].\n        biases: tensor of shape [output_size] added to all in batch if not None.\n\n    Returns:\n        A 2D Tensor with shape [batch x output_size] equal to\n        sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n    """"""\n\n    if not nest.is_sequence(args):\n        args = [args]\n\n    # Calculate the total size of arguments on dimension 1.\n    total_arg_size = 0\n    shapes = [tf.shape(a)[1] for a in args]\n    for shape in shapes:\n        total_arg_size = tf.add(total_arg_size, shape)\n\n    dtype = args[0].dtype\n\n    # Now the computation.\n    scope = tf.get_variable_scope()\n    with tf.variable_scope(scope) as outer_scope:\n\n        weights = tf.get_variable(\'weights\',\n            [total_arg_size, output_size],\n            dtype=dtype)\n\n        if len(args) == 1:\n            res = tf.matmul(args[0], weights)\n        else:\n            res = tf.matmul(tf.concat(args, 1), weights)\n\n        return res if not biases else tf.nn.bias_add(res, biases)\n'"
chatbot/components/decoders.py,42,"b'import logging\nimport tensorflow as tf\nimport sys\n\n# Required due to TensorFlow\'s unreliable naming across versions . . .\ntry:\n    # r1.1\n    from tensorflow.contrib.seq2seq import DynamicAttentionWrapperState \\\n        as AttentionWrapperState\nexcept ImportError:\n    # master\n    from tensorflow.contrib.seq2seq import AttentionWrapperState\n\nfrom tensorflow.contrib.seq2seq import BahdanauAttention, LuongAttention\nfrom tensorflow.contrib.rnn import LSTMStateTuple, LSTMCell\nfrom chatbot.components.base._rnn import RNN, SimpleAttentionWrapper\nfrom utils import io_utils\n\n\nclass Decoder(RNN):\n    """"""Dynamic decoding (base) class that supports both training and inference without\n       requiring superfluous helper objects. With simple boolean parameters,\n       handles the decoder sub-graph construction dynamically in its entirety.\n    """"""\n\n    def __init__(self,\n                 base_cell,\n                 encoder_outputs,\n                 state_size,\n                 vocab_size,\n                 embed_size,\n                 dropout_prob,\n                 num_layers,\n                 temperature,\n                 max_seq_len,\n                 state_wrapper=None):\n        """"""\n        Args:\n            base_cell: (str) name of RNNCell class for underlying cell.\n            state_size: number of units in underlying rnn cell.\n            vocab_size: dimension of output space for projections.\n            embed_size: dimension size of word-embedding space.\n            dropout_prob: probability of a node being dropped.\n            num_layers: how many cells to include in the MultiRNNCell.\n            temperature: (float) determines randomness of outputs/responses.\n                - Some notable values (to get some intuition):\n                  - t -> 0: outputs approach simple argmax.\n                  - t = 1: same as sampling from softmax distribution over\n                    outputs, interpreting the softmax outputs as from a\n                    multinomial (probability) distribution.\n                  - t -> inf: outputs approach uniform random distribution.\n            state_wrapper: allow states to store their wrapper class. See the\n                wrapper method docstring below for more info.\n        """"""\n\n        self.encoder_outputs = encoder_outputs\n        if state_wrapper is None and base_cell == \'LSTMCell\':\n            state_wrapper = LSTMStateTuple\n\n        super(Decoder, self).__init__(\n            base_cell=base_cell,\n            state_size=state_size,\n            embed_size=embed_size,\n            dropout_prob=dropout_prob,\n            num_layers=num_layers,\n            state_wrapper=state_wrapper)\n\n        self.temperature = temperature\n        self.vocab_size = vocab_size\n        self.max_seq_len = max_seq_len\n        with tf.variable_scope(\'projection_tensors\'):\n            w = tf.get_variable(\n                name=""w"",\n                shape=[state_size, vocab_size],\n                dtype=tf.float32,\n                initializer=tf.contrib.layers.xavier_initializer())\n            b = tf.get_variable(\n                name=""b"",\n                shape=[vocab_size],\n                dtype=tf.float32,\n                initializer=tf.contrib.layers.xavier_initializer())\n            self._projection = (w, b)\n\n    def __call__(self,\n                 inputs,\n                 is_chatting,\n                 loop_embedder,\n                 cell,\n                 initial_state=None):\n        """"""Run the inputs on the decoder.\n\n        If we are chatting, then conduct dynamic sampling, which is the process\n        of generating a response given inputs == GO_ID.\n\n        Args:\n            inputs: Tensor with shape [batch_size, max_time, embed_size].\n                For training, inputs are the \'to\' sentence tokens (embedded).\n                For chatting, first input is <GO> and thereafter, the input is\n                the bot\'s previous output (looped around through embedding).\n            initial_state: Tensor with shape [batch_size, state_size].\n            is_chatting: (bool) Determines how we retrieve the outputs and the\n                         returned Tensor shape.\n            loop_embedder: required if is_chatting==True.\n                           Embedder instance needed to feed decoder outputs\n                           as next inputs.\n\n        Returns:\n            outputs: if not chatting, tensor of shape\n                [batch_size, max_time, vocab_size]. Otherwise, tensor of\n                response IDs with shape [batch_size, max_time].\n            state:   if not is_chatting, tensor of shape\n                [batch_size, state_size]. Otherwise, None.\n        """"""\n\n        self.rnn = tf.make_template(\'decoder_rnn\',\n                                    tf.nn.dynamic_rnn,\n                                    cell=cell,\n                                    dtype=tf.float32)\n\n        outputs, state = self.rnn(inputs=inputs,\n                                  initial_state=initial_state)\n\n        if not is_chatting:\n            return outputs, state\n\n        if loop_embedder is None:\n            raise ValueError(\n                ""Loop function required to feed outputs as inputs."")\n\n        def body(response, state):\n            """"""Input callable for tf.while_loop. See below.""""""\n            tf.get_variable_scope().reuse_variables()\n            decoder_input = loop_embedder(tf.reshape(response[-1], (1, 1)),\n                                          reuse=True)\n\n            outputs, state = self.rnn(inputs=decoder_input,\n                                      initial_state=state,\n                                      sequence_length=[1])\n\n            next_id = self.sample(self.apply_projection(outputs))\n            response = tf.concat([response, tf.stack([next_id])], axis=0)\n            return response, state\n\n        def cond(response, s):\n            """"""Input callable for tf.while_loop. See below.""""""\n            return tf.logical_and(\n                tf.not_equal(response[-1], io_utils.EOS_ID),\n                tf.less_equal(tf.size(response), self.max_seq_len))\n\n        # Project to full output state during inference time.\n        # Note: ""outputs"" at this point, at this exact line, is technically just\n        # a single output: the bot\'s first response token.\n        outputs = self.apply_projection(outputs)\n        # Begin the process of building the list of output tokens.\n        response = tf.stack([self.sample(outputs)])\n        # Reshape is needed so the while_loop ahead knows the shape of response.\n        # The comma after the 1 is intentional, it forces tf to believe us.\n        response = tf.reshape(response, [1,], name=\'response\')\n        tf.get_variable_scope().reuse_variables()\n\n        # ============== BEHOLD: The tensorflow while loop. ==================\n        # This allows us to sample dynamically. It also makes me happy!\n        # -- Repeat \'body\' while the \'cond\' returns true.\n        # -- \'cond\': callable returning a boolean scalar tensor.\n        # -- \'body\': callable returning a tuple of tensors of same\n        #            arity as loop_vars.\n        # -- \'loop_vars\': tuple of tensors that is passed to \'cond\' and \'body\'.\n        response, _ = tf.while_loop(\n            cond, body, (response, state),\n            shape_invariants=(tf.TensorShape([None]), cell.shape),\n            back_prop=False)\n        # =============== FAREWELL: The tensorflow while loop. =================\n\n        outputs = tf.expand_dims(response, 0)\n        return outputs, None\n\n    def apply_projection(self, outputs, scope=None):\n        """"""Defines & applies the affine transformation from state space\n        to output space.\n\n        Args:\n            outputs: Tensor of shape [batch_size, max_time, state_size]\n                returned by tf dynamic_rnn.\n            scope: (optional) variable scope for any created here.\n\n        Returns:\n            Tensor of shape [batch_size, max_time, vocab_size] representing the\n            projected outputs.\n        """"""\n\n        with tf.variable_scope(scope, ""proj_scope"", [outputs]):\n\n            # Swap 1st and 2nd indices to match expected input of map_fn.\n            seq_len = tf.shape(outputs)[1]\n            st_size = tf.shape(outputs)[2]\n            time_major_outputs = tf.reshape(outputs, [seq_len, -1, st_size])\n\n            # Project batch at single timestep from state space to output space.\n            def proj_op(batch):\n                return tf.matmul(batch, self._projection[0]) + self._projection[1]\n\n            # Get projected output states;\n            # 3D Tensor with shape [batch_size, seq_len, ouput_size].\n            projected_state = tf.map_fn(proj_op, time_major_outputs)\n        return tf.reshape(projected_state, [-1, seq_len, self.vocab_size])\n\n    def sample(self, projected_output):\n        """"""Return integer ID tensor representing the sampled word.\n        \n        Args:\n            projected_output: Tensor [1, 1, state_size], representing a single\n                decoder timestep output. \n        """"""\n        # TODO: We really need a tf.control_dependencies check here (for rank).\n        with tf.name_scope(\'decoder_sampler\', values=[projected_output]):\n\n            # Protect against extra size-1 dimensions; grab the 1D tensor\n            # of size state_size.\n            logits = tf.squeeze(projected_output)\n            if self.temperature < 0.02:\n                return tf.argmax(logits, axis=0)\n\n            # Convert logits to probability distribution.\n            probabilities = tf.div(logits, self.temperature)\n            projected_output = tf.div(\n                tf.exp(probabilities),\n                tf.reduce_sum(tf.exp(probabilities), axis=-1))\n\n            # Sample 1 time from the probability distribution.\n            sample_ID = tf.squeeze(\n                tf.multinomial(tf.expand_dims(probabilities, 0), 1))\n        return sample_ID\n\n    def get_projection_tensors(self):\n        """"""Returns the tuple (w, b) that decoder uses for projecting.\n        Required as argument to the sampled softmax loss.\n        """"""\n        return self._projection\n\n\nclass BasicDecoder(Decoder):\n    """"""Simple (but dynamic) decoder that is essentially just the base class.""""""\n\n    def __call__(self,\n                 inputs,\n                 initial_state=None,\n                 is_chatting=False,\n                 loop_embedder=None,\n                 cell=None):\n\n        return super(BasicDecoder, self).__call__(\n            inputs=inputs,\n            initial_state=initial_state,\n            is_chatting=is_chatting,\n            loop_embedder=loop_embedder,\n            cell=self.get_cell(\'decoder_cell\'))\n\n\nclass AttentionDecoder(Decoder):\n    """"""Dynamic decoder that applies an attention mechanism over the full\n    sequence of encoder outputs. Using Bahdanau for now (may change).\n    \n    TODO: Luong\'s paper mentions that they only use the *top* layer of \n    stacked LSTMs for attention-related computation. Since currently I\'m \n    only testing attention models with one-layer encoder/decoders, this\n    isn\'t an issue. However, in a couple days I should revisit this.\n    """"""\n\n    def __init__(self,\n                 encoder_outputs,\n                 base_cell,\n                 state_size,\n                 vocab_size,\n                 embed_size,\n                 attention_mechanism=\'BahdanauAttention\',\n                 dropout_prob=1.0,\n                 num_layers=1,\n                 temperature=0.0,\n                 max_seq_len=10):\n        """"""We need to explicitly call the constructor now, so we can:\n           - Specify we need the state wrapped in AttentionWrapperState.\n           - Specify our attention mechanism (will allow customization soon).\n        """"""\n\n        super(AttentionDecoder, self).__init__(\n            encoder_outputs=encoder_outputs,\n            base_cell=base_cell,\n            state_size=state_size,\n            vocab_size=vocab_size,\n            embed_size=embed_size,\n            dropout_prob=dropout_prob,\n            num_layers=num_layers,\n            temperature=temperature,\n            max_seq_len=max_seq_len,\n            state_wrapper=AttentionWrapperState)\n\n        _mechanism = getattr(tf.contrib.seq2seq, attention_mechanism)\n        self.attention_mechanism = _mechanism(num_units=state_size,\n                                              memory=encoder_outputs)\n        self.output_attention = True\n\n    def __call__(self,\n                 inputs,\n                 initial_state=None,\n                 is_chatting=False,\n                 loop_embedder=None,\n                 cell=None):\n        """"""\n        The only modifcation to the superclass is we pass in our own\n        cell that is wrapped with a custom attention class (specified in\n        base/_rnn.py). It is mostly the same as tensorflow\'s, but with minor\n        tweaks so that it could easily hang out with the other components of\n        the project.\n        """"""\n\n        if cell is None:\n            cell = self.get_cell(\'attn_cell\', initial_state)\n\n        return super(AttentionDecoder, self).__call__(\n            inputs=inputs,\n            is_chatting=is_chatting,\n            loop_embedder=loop_embedder,\n            cell=cell)\n\n    def get_cell(self, name, initial_state):\n        # Get the simple underlying cell first.\n        cell = super(AttentionDecoder, self).get_cell(name)\n        # Return the normal cell wrapped to support attention.\n        return SimpleAttentionWrapper(\n            cell=cell,\n            attention_mechanism=self.attention_mechanism,\n            initial_cell_state=initial_state)\n\n\n'"
chatbot/components/embedder.py,44,"b'import tensorflow as tf\nimport logging\nimport numpy as np\nfrom chatbot._models import Model\nfrom utils import io_utils\nimport time\n\n\nclass Embedder:\n    """"""Acts on tensors with integer elements, embedding them in a higher-dimensional\n    vector space. A single Embedder instance can embed both encoder and decoder by\n    associating them with distinct scopes. """"""\n\n    def __init__(self, vocab_size, embed_size, l1_reg=0.0):\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.l1_reg = l1_reg\n        self._scopes = dict()\n\n    def __call__(self, inputs, reuse=None):\n        """"""Embeds integers in inputs and returns the embedded inputs.\n\n        Args:\n          inputs: input tensor of shape [batch_size, max_time].\n\n        Returns:\n          Output tensor of shape [batch_size, max_time, embed_size]\n        """"""\n\n        # Ensure inputs has expected rank of 2.\n        assert len(inputs.shape) == 2, \\\n            ""Expected inputs rank 2 but found rank %r"" % len(inputs.shape)\n\n        scope = tf.get_variable_scope()\n        # Parse info from scope input needed for reliable reuse across model.\n        if scope is not None:\n            scope_name = scope if isinstance(scope, str) else scope.name\n            if scope_name not in self._scopes:\n                self._scopes[scope_name] = scope\n        else:\n            self._scopes[\'embedder_call\'] = tf.variable_scope(\'embedder_call\')\n\n        embed_tensor = tf.get_variable(\n            name=""embed_tensor"",\n            shape=[self.vocab_size, self.embed_size],\n            initializer=tf.contrib.layers.xavier_initializer(),\n            regularizer=tf.contrib.layers.l1_regularizer(self.l1_reg))\n        embedded_inputs = tf.nn.embedding_lookup(embed_tensor, inputs)\n        # Place any checks on inputs here before returning.\n        if not isinstance(embedded_inputs, tf.Tensor):\n            raise TypeError(""Embedded inputs should be of type Tensor."")\n        if len(embedded_inputs.shape) != 3:\n            raise ValueError(""Embedded sentence has incorrect shape."")\n        tf.summary.histogram(scope.name, embed_tensor)\n        return embedded_inputs\n\n    def assign_visualizers(self, writer, scope_names, metadata_path):\n        """"""Setup the tensorboard embedding visualizer.\n\n        Args:\n            writer: instance of tf.summary.FileWriter\n            scope_names: list of \n        """"""\n        assert writer is not None\n\n        if not isinstance(scope_names, list):\n            scope_names = [scope_names]\n\n        for scope_name in scope_names:\n            assert scope_name in self._scopes, \\\n                ""I don\'t have any embedding tensors for %s"" % scope_name\n            config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n            emb = config.embeddings.add()\n            emb.tensor_name = scope_name.rstrip(\'/\') + \'/embed_tensor:0\'\n            emb.metadata_path = metadata_path\n            tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n\n    def get_scope_basename(self, scope):\n        """"""\n        Args:\n            scope: tf.variable_scope.\n        """"""\n        return scope.name.strip(\'/\').split(\'/\')[-1]\n\n\nclass AutoEncoder(Model):\n    """"""[UNDER CONSTRUCTION]. AutoEncoder for unsupervised pretraining the\n    word embeddings for dynamic models.\n    """"""\n\n    def __init__(self, dataset, params):\n\n        self.log = logging.getLogger(\'AutoEncoderLogger\')\n        super(AutoEncoder, self).__init__(self.log, dataset, params)\n        self.build_computation_graph(dataset)\n        self.compile()\n\n    def build_computation_graph(self, dataset):\n        from chatbot.components.input_pipeline import InputPipeline\n        # Organize input pipeline inside single node for clean visualization.\n        self.pipeline = InputPipeline(\n            file_paths=dataset.paths,\n            batch_size=self.batch_size,\n            is_chatting=self.is_chatting)\n\n        self.encoder_inputs = self.pipeline.encoder_inputs\n\n        with tf.variable_scope(\'autoencoder_encoder\'):\n            embed_tensor = tf.get_variable(\n                name=""embed_tensor"",\n                shape=[self.vocab_size, self.embed_size])\n            _h = tf.nn.embedding_lookup(embed_tensor, self.encoder_inputs)\n            h = tf.contrib.keras.layers.Dense(self.embed_size, activation=\'relu\')(_h)\n\n        with tf.variable_scope(\'autoencoder_decoder\'):\n            w = tf.get_variable(\n                name=""w"",\n                shape=[self.embed_size, self.vocab_size],\n                dtype=tf.float32)\n            b = tf.get_variable(\n                name=""b"",\n                shape=[self.vocab_size],\n                dtype=tf.float32)\n\n            # Swap 1st and 2nd indices to match expected input of map_fn.\n            seq_len = tf.shape(h)[1]\n            st_size = tf.shape(h)[2]\n            time_major_outputs = tf.reshape(h, [seq_len, -1, st_size])\n            # Project batch at single timestep from state space to output space.\n            def proj_op(h_t):\n                return tf.matmul(h_t, w) + b\n            decoder_outputs = tf.map_fn(proj_op, time_major_outputs)\n            decoder_outputs = tf.reshape(decoder_outputs,\n                                         [-1, seq_len, self.vocab_size])\n\n        self.outputs = tf.identity(decoder_outputs, name=\'outputs\')\n        # Tag inputs and outputs by name should we want to freeze the model.\n        self.graph.add_to_collection(\'freezer\', self.encoder_inputs)\n        self.graph.add_to_collection(\'freezer\', self.outputs)\n        # Merge any summaries floating around in the aether into one object.\n        self.merged = tf.summary.merge_all()\n\n    def compile(self):\n\n        if not self.is_chatting:\n            with tf.variable_scope(""evaluation"") as scope:\n                target_labels = self.encoder_inputs[:, 1:]\n                target_weights = tf.cast(target_labels > 0, target_labels.dtype)\n                print(\'\\ntl\\n\', target_labels)\n                print(\'\\ntw\\n\', target_weights)\n                preds = self.outputs[:, :-1, :]\n                print(\'\\npreds\\n\', preds)\n\n                self.loss = tf.losses.sparse_softmax_cross_entropy(\n                    labels=target_labels,\n                    logits=preds,\n                    weights=target_weights)\n                print(self.loss)\n\n                self.train_op = tf.contrib.layers.optimize_loss(\n                    loss=self.loss, global_step=self.global_step,\n                    learning_rate=self.learning_rate,\n                    optimizer=\'Adam\',\n                    summaries=[\'gradients\'])\n\n                # Compute accuracy, ensuring we use fully projected outputs.\n                _preds = tf.argmax(self.outputs[:, :-1, :], axis=2)\n                correct_pred = tf.equal(\n                    _preds,\n                    target_labels)\n                accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n                tf.summary.scalar(\'accuracy\', accuracy)\n                tf.summary.scalar(\'loss_train\', self.loss)\n                self.merged = tf.summary.merge_all()\n        super(AutoEncoder, self).compile()\n\n    def step(self, forward_only=False):\n        if not forward_only:\n            return self.sess.run([self.merged, self.loss, self.train_op])\n        else:\n            return self.sess.run(fetches=tf.argmax(self.outputs[:, :-1, :], axis=2),\n                                 feed_dict=self.pipeline.feed_dict)\n\n    def train(self, close_when_done=True):\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=self.sess, coord=coord)\n        try:\n            avg_loss = avg_step_time = 0.0\n            while not coord.should_stop():\n\n                i_step = self.sess.run(self.global_step)\n                start_time = time.time()\n                summaries, step_loss,  _ = self.step()\n                avg_step_time += (time.time() - start_time) / self.steps_per_ckpt\n                avg_loss += step_loss / self.steps_per_ckpt\n\n                # Print updates in desired intervals (steps_per_ckpt).\n                if i_step % self.steps_per_ckpt == 0:\n                    print(\'loss:\', avg_loss)\n                    self.save(summaries=summaries)\n                    avg_loss = avg_step_time = 0.0\n\n                if i_step >= self.max_steps:\n                    print(""Maximum step"", i_step, ""reached."")\n                    raise SystemExit\n\n        except (KeyboardInterrupt, SystemExit):\n            print(""Training halted. Cleaning up . . . "")\n            coord.request_stop()\n        except tf.errors.OutOfRangeError:\n            print(""OutOfRangeError. You have run out of data."")\n            coord.request_stop()\n        finally:\n            coord.join(threads)\n            if close_when_done:\n                self.close()\n\n    def __call__(self, sentence):\n        encoder_inputs = io_utils.sentence_to_token_ids(\n            tf.compat.as_bytes(sentence), self.dataset.word_to_idx)\n        encoder_inputs = np.array([encoder_inputs[::-1]])\n        self.pipeline.feed_user_input(encoder_inputs)\n        # Get output sentence from the chatbot.\n        response = self.step(forward_only=True)\n        return self.dataset.as_words(response[0])\n\n'"
chatbot/components/encoders.py,8,"b'""""""Classes for the dynamic encoders.""""""\n\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import GRUCell\nfrom tensorflow.contrib.rnn import LSTMStateTuple, LSTMCell\nfrom chatbot.components.base._rnn import RNN\nfrom tensorflow.python.layers import core as layers_core\n\n\nclass BasicEncoder(RNN):\n    """"""Encoder architecture that is defined by its cell running \n    inside dynamic_rnn.\n    """"""\n\n    def __call__(self, inputs, initial_state=None):\n        """"""Run the inputs on the encoder and return the output(s).\n\n        Args:\n            inputs: Tensor with shape [batch_size, max_time, embed_size].\n            initial_state: (optional) Tensor with shape [batch_size, state_size] \n                to initialize decoder cell.\n\n        Returns:\n            outputs: (only if return_sequence is True)\n                     Tensor of shape [batch_size, max_time, state_size].\n            state:   The final encoder state; shape [batch_size, state_size].\n        """"""\n\n        cell = self.get_cell(""basic_enc_cell"")\n        _, state = tf.nn.dynamic_rnn(cell,\n                                     inputs,\n                                     initial_state=initial_state,\n                                     dtype=tf.float32)\n        return _, state\n\n\nclass BidirectionalEncoder(RNN):\n    """"""Encoder that concatenates two copies of its cell forward and backward and\n    feeds into a bidirectional_dynamic_rnn.\n\n    Outputs are concatenated before being returned. I may move this \n    functionality to an intermediate class layer that handles shape-matching \n    between encoder/decoder.\n    """"""\n\n    def __call__(self, inputs, initial_state=None):\n        """"""Run the inputs on the encoder and return the output(s).\n\n        Args:\n            inputs: Tensor with shape [batch_size, max_time, embed_size].\n\n        Returns:\n            outputs: Tensor of shape [batch_size, max_time, state_size].\n            state: The final encoder state; shape [batch_size, state_size].\n        """"""\n\n        cell_fw = self.get_cell(""cell_fw"")\n        cell_bw = self.get_cell(""cell_bw"")\n        outputs_tuple, final_state_tuple = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=cell_fw,\n            cell_bw=cell_bw,\n            inputs=inputs,\n            dtype=tf.float32)\n\n        # Create fully connected layer to help get us back to\n        # state size (from the dual state fw-bw).\n        layer = layers_core.Dense(units=self.state_size, use_bias=False)\n\n        def single_state(state):\n            """"""Reshape bidirectional state (via fully connected layer)\n            to state size.\n            """"""\n            if \'LSTM\' in self.base_cell:\n                bridged_state = LSTMStateTuple(\n                    c=layer(state[0]),\n                    h=layer(state[1]))\n            else:\n                bridged_state = layer(state)\n            return bridged_state\n\n        # Concatenate each of the tuples fw and bw dimensions.\n        # Now we are dealing with the concatenated ""states"" with dimension:\n        # [batch_size, max_time, state_size * 2].\n        # NOTE: Convention of LSTMCell is that outputs only contain the\n        # the hidden state (i.e. \'h\' only, no \'c\').\n        outputs = tf.concat(outputs_tuple, -1)\n        outputs = tf.map_fn(layer, outputs)\n\n        # Similarly, combine the tuple of final states, resulting in:\n        # [batch_size, state_size * 2].\n        final_state = tf.concat(final_state_tuple, -1)\n\n        if self.num_layers == 1:\n            final_state = single_state(final_state)\n        else:\n            final_state = tuple([single_state(fs)\n                                 for fs in tf.unstack(final_state)])\n\n        return outputs, final_state\n\n'"
chatbot/components/input_pipeline.py,26,"b'import logging\nimport tensorflow as tf\nfrom utils import io_utils\nfrom tensorflow.contrib.training import bucket_by_sequence_length\n\nLENGTHS = {\'encoder_sequence_length\': tf.FixedLenFeature([], dtype=tf.int64),\n           \'decoder_sequence_length\': tf.FixedLenFeature([], dtype=tf.int64)}\nSEQUENCES = {\'encoder_sequence\': tf.FixedLenSequenceFeature([], dtype=tf.int64),\n             \'decoder_sequence\': tf.FixedLenSequenceFeature([], dtype=tf.int64)}\n\n\nclass InputPipeline:\n    """"""TensorFlow-only input pipeline with parallel enqueuing,\n    dynamic bucketed-batching, and more.\n\n        Overview of pipeline construction:\n            1. Create ops for reading protobuf tfrecords line-by-line.\n            2. Enqueue raw outputs, attach to threads, and parse sequences.\n            3. Organize sequences into buckets of similar lengths, pad, and batch.\n    """"""\n\n    def __init__(self, file_paths, batch_size, capacity=None, is_chatting=False, scope=None):\n        """"""\n        Args:\n            file_paths: (dict) returned by instance of Dataset via Dataset.paths.\n            batch_size: number of examples returned by dequeue op.\n            capacity: maximum number of examples allowed in the input queue at a time.\n            is_chatting: (bool) determines whether we\'re feeding user input or file inputs.\n        """"""\n        with tf.name_scope(scope, \'input_pipeline\') as scope:\n            if capacity is None:\n                self.capacity = max(50 * batch_size, int(1e4))\n                logging.info(""Input capacity set to %d examples."" % self.capacity)\n            self.batch_size = batch_size\n            self.paths = file_paths\n            self.control = {\'train\': 0, \'valid\': 1}\n            self.active_data = tf.convert_to_tensor(self.control[\'train\'])\n            self.is_chatting = is_chatting\n            self._user_input = tf.placeholder(tf.int32, [1, None], name=\'user_input\')\n            self._feed_dict = None\n            self._scope = scope\n\n            if not is_chatting:\n                # Create tensors that will store input batches at runtime.\n                self._train_lengths, self.train_batches = self.build_pipeline(\'train\')\n                self._valid_lengths, self.valid_batches = self.build_pipeline(\'valid\')\n\n    def build_pipeline(self, name):\n        """"""Creates a new input subgraph composed of the following components:\n            - Reader queue that feeds protobuf data files.\n            - RandomShuffleQueue assigned parallel-thread queuerunners.\n            - Dynamic padded-bucketed-batching queue for organizing batches in a time and\n              space-efficient manner.\n\n        Args:\n            name: filename prefix for data. See Dataset class for naming conventions.\n\n        Returns:\n            2-tuple (lengths, sequences):\n                lengths: (dict) parsed context feature from protobuf file.\n                Supports keys in LENGTHS.\n                sequences: (dict) parsed feature_list from protobuf file.\n                Supports keys in SEQUENCES.\n        """"""\n        with tf.variable_scope(name + \'_pipeline\'):\n            proto_text = self._read_line(self.paths[name + \'_tfrecords\'])\n            context_pair, sequence_pair = self._assign_queue(proto_text)\n            input_length = tf.add(context_pair[\'encoder_sequence_length\'],\n                                  context_pair[\'decoder_sequence_length\'],\n                                  name=name + \'length_add\')\n            return self._padded_bucket_batches(input_length, sequence_pair)\n\n    @property\n    def encoder_inputs(self):\n        """"""Determines, via tensorflow control structures, which part of the pipeline to run\n           and retrieve inputs to a Model encoder component. """"""\n        if not self.is_chatting:\n            return self._cond_input(\'encoder\')\n        else:\n            return self._user_input\n\n    @property\n    def decoder_inputs(self):\n        """"""Determines, via tensorflow control structures, which part of the pipeline to run\n           and retrieve inputs to a Model decoder component. """"""\n        if not self.is_chatting:\n            return self._cond_input(\'decoder\')\n        else:\n            # In a chat session, we just give the bot the go-ahead to respond!\n            return tf.convert_to_tensor([[io_utils.GO_ID]])\n\n    @property\n    def user_input(self):\n        return self._user_input\n\n    @property\n    def feed_dict(self):\n        return self._feed_dict\n\n    def feed_user_input(self, user_input):\n        """"""Called by Model instances upon receiving input from stdin.""""""\n        self._feed_dict = {self._user_input.name: user_input}\n\n    def toggle_active(self):\n        """"""Simple callable that toggles active_data between training and validation.""""""\n        def to_valid(): return tf.constant(self.control[\'valid\'])\n        def to_train(): return tf.constant(self.control[\'train\'])\n        self.active_data = tf.cond(tf.equal(self.active_data, self.control[\'train\']),\n                                    to_valid, to_train)\n\n    def _cond_input(self, prefix):\n        with tf.name_scope(self._scope):\n            def train(): return self.train_batches[prefix + \'_sequence\']\n            def valid(): return self.valid_batches[prefix + \'_sequence\']\n            return tf.cond(tf.equal(self.active_data, self.control[\'train\']),\n                           train, valid, name=prefix + \'_cond_input\')\n\n    def _read_line(self, file):\n        """"""Create ops for extracting lines from files.\n\n        Returns:\n            Tensor that will contain the lines at runtime.\n        """"""\n        with tf.variable_scope(\'reader\'):\n            filename_queue = tf.train.string_input_producer([file])\n            reader = tf.TFRecordReader(name=\'tfrecord_reader\')\n            _, next_raw = reader.read(filename_queue, name=\'read_records\')\n        return next_raw\n\n    def _assign_queue(self, proto_text):\n        """"""\n        Args:\n            proto_text: object to be enqueued and managed by parallel threads.\n        """"""\n\n        with tf.variable_scope(\'shuffle_queue\'):\n            queue = tf.RandomShuffleQueue(\n                capacity=self.capacity,\n                min_after_dequeue=10*self.batch_size,\n                dtypes=tf.string, shapes=[()])\n\n            enqueue_op = queue.enqueue(proto_text)\n            example_dq = queue.dequeue()\n\n            qr = tf.train.QueueRunner(queue, [enqueue_op] * 4)\n            tf.train.add_queue_runner(qr)\n\n            _sequence_lengths, _sequences = tf.parse_single_sequence_example(\n                serialized=example_dq,\n                context_features=LENGTHS,\n                sequence_features=SEQUENCES)\n        return _sequence_lengths, _sequences\n\n    def _padded_bucket_batches(self, input_length, data):\n        with tf.variable_scope(\'bucket_batch\'):\n            lengths, sequences = bucket_by_sequence_length(\n                input_length=tf.to_int32(input_length),\n                tensors=data,\n                batch_size=self.batch_size,\n                bucket_boundaries=[8, 16, 32],\n                capacity=self.capacity,\n                dynamic_pad=True)\n        return lengths, sequences\n\n'"
chatbot/legacy/__init__.py,0,b''
chatbot/legacy/_decode.py,3,"b'""""""Used by legacy_models for decoding. Not needed by DynamicBot.""""""\n\nimport tensorflow as tf\nimport logging\nimport os\nimport sys\n\nfrom utils import io_utils\nfrom utils.io_utils import sentence_to_token_ids, get_vocab_dicts\nimport numpy as np\n\n\ndef decode(bot, dataset, teacher_mode=True):\n    """"""Runs a chat session between the given chatbot and user.""""""\n\n    # We decode one sentence at a time.\n    bot.batch_size = 1\n    # Decode from standard input.\n    print(""Type \\""exit\\"" to exit."")\n    print(""Write stuff after the \\"">\\"" below and I, your robot friend, will respond."")\n    sentence = io_utils.get_sentence()\n    while sentence:\n        # Convert input sentence to token-ids.\n        token_ids = sentence_to_token_ids(tf.compat.as_bytes(sentence), dataset.word_to_idx)\n        # Get output sentence from the chatbot.\n        outputs = decode_inputs(token_ids, dataset.idx_to_word, bot)\n        # Print the chatbot\'s response.\n        print(outputs)\n        if teacher_mode:\n            print(""What should I have said?"")\n            feedback = io_utils.get_sentence()\n            feedback_ids = sentence_to_token_ids(tf.compat.as_bytes(feedback), dataset.inputs_to_word)\n            outputs = train_on_feedback(bot, token_ids, feedback_ids, dataset.idx_to_word)\n            print(""Okay. Let me try again:\\n"", outputs)\n        # Wait for next input.\n        sentence = io_utils.get_sentence()\n        # Stop program if sentence == \'exit\\n\'.\n        if sentence == \'exit\':\n            print(""Fine, bye :("")\n            break\n\n\ndef decode_inputs(inputs, idx_to_word, chatbot):\n    # Which bucket does it belong to?\n    bucket_id = _assign_to_bucket(inputs, chatbot.buckets)\n    # Get a 1-element batch to feed the sentence to the chatbot.\n    data = {bucket_id: [(inputs, [])]}\n    encoder_inputs, decoder_inputs, target_weights = chatbot.get_batch(data, bucket_id)\n    # Get output logits for the sentence.\n    _, _, _, output_logits = chatbot.step(encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n    # Convert raw output to chat response & print.\n    return _logits_to_outputs(output_logits, chatbot.temperature, idx_to_word)\n\n\ndef train_on_feedback(chatbot, input_ids, feedback_ids, idx_to_outputs):\n    bucket_id = _assign_to_bucket(feedback_ids, chatbot.buckets)\n    data = {bucket_id: [(input_ids, feedback_ids)]}\n    enc_in, dec_in, weights = chatbot.get_batch(data, bucket_id)\n    # Jack up learning rate & make sure robot learned its lesson.\n    chatbot.sess.run(chatbot.learning_rate.assign(0.7))\n    for _ in range(10):\n        # LEARN YOU FOOL, LEARN. :)\n        chatbot.step(enc_in, dec_in, weights, bucket_id, False)\n    return decode_inputs(input_ids, idx_to_outputs, chatbot)\n\n\ndef _logits_to_outputs(output_logits, temperature, idx_word):\n    """"""\n    Args:\n        output_logits: shape is [output_length, [vocab_size]]\n    :return:\n    """"""\n    # outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n    outputs =  [_sample(l, temperature) for l in output_logits]\n    # If there is an EOS symbol in outputs, cut them at that point.\n    if io_utils.EOS_ID in outputs:\n        outputs = outputs[:outputs.index(io_utils.EOS_ID)]\n    outputs = "" "".join([tf.compat.as_str(idx_word[output]) for output in outputs]) + "".""\n    # Capitalize.\n    outputs = outputs[0].upper() + outputs[1:]\n    return outputs\n\n\ndef _sample(logits, temperature):\n    if temperature < 0.5:\n        return int(np.argmax(logits, axis=1))\n    logits = logits.flatten()\n    logits = logits / temperature\n    logits = np.exp(logits - np.max(logits))\n    logits = logits / np.sum(logits)\n    sampleID = np.argmax(np.random.multinomial(1, logits, 1))\n    while sampleID == io_utils.UNK_ID:\n        sampleID = np.argmax(np.random.multinomial(1, logits, 1))\n    return int(sampleID)\n\n\ndef _assign_to_bucket(token_ids, buckets):\n    """"""Find bucket large enough for token_ids, else warning.""""""\n    bucket_id = len(buckets) - 1\n    for i, bucket in enumerate(buckets):\n        if bucket[0] >= len(token_ids):\n            bucket_id = i\n            break\n    else:\n        logging.warning(""Sentence longer than  truncated: %s"", len(token_ids))\n    return bucket_id\n'"
chatbot/legacy/_train.py,0,"b'""""""Train seq2seq attention chatbot.\nNote: Only used for legacy_models.\nFor (better) DynamicBot implementation, please see dynamic_models.py and, for saving/restoring ops,\nthe base class of all models in _models.py.\n""""""\nimport time\nfrom utils import *\n\ndef train(bot, dataset):\n    """""" Train chatbot using dataset given by dataset.\n        chatbot: instance of ChatBot or SimpleBot.\n    """"""\n\n    # Get data as token-ids.\n    train_set, dev_set = io_utils.read_data(dataset,\n                                            bot.buckets)\n\n    # Interpret train_buckets_scale[i] as [cumulative] frac of samples in bucket i or below.\n    train_buckets_scale = _get_data_distribution(train_set, bot.buckets)\n\n    # This is the training loop.\n    i_step = 0\n    step_time, loss = 0.0, 0.0\n    previous_losses = []\n    try:\n        while True:\n            # Sample a random bucket index according to the data distribution,\n            # then get a batch of data from that bucket by calling chatbot.get_batch.\n            rand = np.random.random_sample()\n            bucket_id = min([i for i in range(len(train_buckets_scale)) if train_buckets_scale[i] > rand])\n\n            # Get a batch and make a step.\n            start_time = time.time()\n            summary, step_loss = run_train_step(bot, train_set, bucket_id, False)\n            step_time += (time.time() - start_time) / bot.steps_per_ckpt\n            loss      += step_loss / bot.steps_per_ckpt\n\n            # Once in a while, we save checkpoint, print statistics, and run evals.\n            if i_step % bot.steps_per_ckpt == 0:\n                run_checkpoint(bot, step_time, loss, previous_losses, dev_set)\n                step_time, loss = 0.0, 0.0\n            i_step += 1\n    except (KeyboardInterrupt, SystemExit):\n        print(""Training halted. Cleaning up . . . "")\n        # Store the model\'s graph in ckpt directory.\n        bot.saver.export_meta_graph(bot.ckpt_dir + dataset.name + \'.meta\')\n        bot.close()\n        print(""Done."")\n\n\ndef run_train_step(model, train_set, bucket_id, forward_only=False):\n    encoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\n    step_returns = model.step(encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only)\n    summary, _, losses, _ = step_returns\n    if not forward_only and summary is not None:\n        model.train_writer.add_summary(summary, model.global_step.eval(model.sess))\n    return summary, losses\n\n\ndef run_checkpoint(model, step_time, loss, previous_losses, dev_set):\n    # Print statistics for the previous epoch.\n    perplexity = np.exp(float(loss)) if loss < 300 else float(""inf"")\n    print(""\\nglobal step:"", model.global_step.eval(model.sess), end=""  "")\n    print(""learning rate: %.4f"" %  model.learning_rate.eval(session=model.sess), end=""  "")\n    print(""step time: %.2f"" % step_time, end=""  "")\n    print(""perplexity: %.2f"" % perplexity)\n\n    # Run evals on development set and print their perplexity.\n    for bucket_id in range(len(model.buckets)):\n        if len(dev_set[bucket_id]) == 0:\n            print(""  eval: empty bucket %d"" % (bucket_id))\n            continue\n        summary, eval_loss = run_train_step(model, dev_set, bucket_id, forward_only=True)\n        model.save(summaries=summary)\n        eval_ppx = np.exp(float(eval_loss)) if eval_loss < 300 else float(""inf"")\n        print(""  eval: bucket %d perplexity %.2f"" % (bucket_id, eval_ppx))\n    sys.stdout.flush()\n\n\ndef _get_data_distribution(train_set, buckets):\n    # Get number of samples for each bucket (i.e. train_bucket_sizes[1] == num-trn-samples-in-bucket-1).\n    train_bucket_sizes = [len(train_set[b]) for b in range(len(buckets))]\n    # The total number training samples, excluding the ones too long for our bucket choices.\n    train_total_size   = float(sum(train_bucket_sizes))\n\n    # Interpret as: train_buckets_scale[i] == [cumulative] fraction of samples in bucket i or below.\n    return [sum(train_bucket_sizes[:i + 1]) / train_total_size\n                     for i in range(len(train_bucket_sizes))]\n\n\n'"
chatbot/legacy/legacy_models.py,48,"b'""""""Sequence-to-sequence models.""""""\n\n# EDIT: Modified inheritance strucutre (see _models.py) so these *should* work again.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.legacy_seq2seq import embedding_attention_seq2seq\nfrom tensorflow.contrib.legacy_seq2seq import model_with_buckets\n#from tensorflow.contrib.rnn.python.ops import core_rnn\nfrom tensorflow.contrib.rnn.python.ops import core_rnn_cell\nfrom tensorflow.python.ops import embedding_ops\nfrom chatbot._models import BucketModel\n\n\nclass ChatBot(BucketModel):\n    """"""Sequence-to-sequence model with attention and for multiple buckets.\n\n    The input-to-output path can be thought of (on a high level) as follows:\n        1. Inputs:      Batches of integer lists, where each integer is a\n                        word ID to a pre-defined vocabulary.\n        2. Embedding:   each input integer is mapped to an embedding vector.\n                        Each embedding vector is of length \'layer_size\', an argument to __init__.\n                        The encoder and decoder have their own distinct embedding spaces.\n        3. Encoding:    The embedded batch vectors are fed to a multi-layer cell containing GRUs.\n        4. Attention:   At each timestep, the output of the multi-layer cell is saved, so that\n                        the decoder can access them in the manner specified in the paper on\n                        jointly learning to align and translate. (should give a link to paper...)\n        5. Decoding:    The decoder, the same type of embedded-multi-layer cell\n                        as the encoder, is initialized with the last output of the encoder,\n                        the ""context"". Thereafter, we either feed it a target sequence\n                        (when training) or we feed its previous output as its next input (chatting).\n    """"""\n\n    def __init__(self, buckets, dataset, params):\n\n        logging.basicConfig(level=logging.INFO)\n        logger = logging.getLogger(\'ChatBotLogger\')\n        super(ChatBot, self).__init__(\n            logger=logger,\n            buckets=buckets,\n            dataset=dataset,\n            params=params)\n\n        if len(buckets) > 1:\n            self.log.error(""ChatBot requires len(buckets) be 1 since tensorflow\'s""\n                           "" model_with_buckets function is now deprecated and BROKEN. The only""\n                           ""workaround is ensuring len(buckets) == 1. ChatBot apologizes.""\n                           ""ChatBot also wishes it didn\'t have to be this way. ""\n                           ""ChatBot is jealous that DynamicBot does not have these issues."")\n            raise ValueError(""Not allowed to pass buckets with len(buckets) > 1."")\n\n        # ==========================================================================================\n        # Define basic components: cell(s) state, encoder, decoder.\n        # ==========================================================================================\n\n        #cell =  tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(state_size)for _ in range(num_layers)])\n        cell = tf.contrib.rnn.GRUCell(self.state_size)\n        self.encoder_inputs = ChatBot._get_placeholder_list(""encoder"", buckets[-1][0])\n        self.decoder_inputs = ChatBot._get_placeholder_list(""decoder"", buckets[-1][1] + 1)\n        self.target_weights = ChatBot._get_placeholder_list(""weight"", buckets[-1][1] + 1, tf.float32)\n        target_outputs = [self.decoder_inputs[i + 1] for i in range(len(self.decoder_inputs) - 1)]\n\n        # If specified, sample from subset of full vocabulary size during training.\n        softmax_loss, output_proj = None, None\n        if 0 < self.num_samples < self.vocab_size:\n            softmax_loss, output_proj = ChatBot._sampled_loss(self.num_samples,\n                                                              self.state_size,\n                                                              self.vocab_size)\n\n        # ==========================================================================================\n        # Combine the components to construct desired model architecture.\n        # ==========================================================================================\n\n        # The seq2seq function: we use embedding for the input and attention.\n        def seq2seq_f(encoder_inputs, decoder_inputs):\n            # Note: the returned function uses separate embeddings for encoded/decoded sets.\n            #           Maybe try implementing same embedding for both.\n            # Question: the outputs are projected to vocab_size NO MATTER WHAT.\n            #           i.e. if output_proj is None, it uses its own OutputProjectionWrapper instead\n            #           --> How does this affect our model?? A bit misleading imo.\n            #with tf.variable_scope(scope or ""seq2seq2_f"") as seq_scope:\n            return embedding_attention_seq2seq(encoder_inputs, decoder_inputs, cell,\n                                               num_encoder_symbols=self.vocab_size,\n                                               num_decoder_symbols=self.vocab_size,\n                                               embedding_size=self.state_size,\n                                               output_projection=output_proj,\n                                               feed_previous=self.is_chatting,\n                                               dtype=tf.float32)\n\n        # Note that self.outputs and self.losses are lists of length len(buckets).\n        # This allows us to identify which outputs/losses to compute given a particular bucket.\n        # Furthermore, \\forall i < j, len(self.outputs[i])  < len(self.outputs[j]). (same for loss)\n        self.outputs, self.losses = model_with_buckets(\n            self.encoder_inputs, self.decoder_inputs,\n            target_outputs, self.target_weights,\n            buckets, seq2seq_f,\n            softmax_loss_function=softmax_loss)\n\n        # If decoding, append _projection to true output to the model.\n        if self.is_chatting and output_proj is not None:\n            self.outputs = ChatBot._get_projections(len(buckets), self.outputs, output_proj)\n\n        with tf.variable_scope(""summaries""):\n            self.summaries = {}\n            for i, loss in enumerate(self.losses):\n                name = ""loss{}"".format(i)\n                self.summaries[name] = tf.summary.scalar(""loss{}"".format(i), loss)\n\n    def step(self, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only=False):\n        """"""Run a step of the model.\n\n        Args:\n          encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n          decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n          target_weights: list of numpy float vectors to feed as target weights.\n          bucket_id: which bucket of the model to use.\n\n        Returns:\n            [summary, gradient_norms, loss, outputs]\n        """"""\n\n        encoder_size, decoder_size = self.buckets[bucket_id]\n        super(ChatBot, self).check_input_lengths(\n            [encoder_inputs, decoder_inputs, target_weights],\n            [encoder_size, decoder_size, decoder_size])\n\n        input_feed = {}\n        for l in range(encoder_size):\n            input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n        for l in range(decoder_size):\n            input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n            input_feed[self.target_weights[l].name] = target_weights[l]\n        input_feed[self.decoder_inputs[decoder_size].name] = np.zeros([self.batch_size],\n                                                                      dtype=np.int32)\n\n        if not forward_only:  # Not just for decoding; also for validating in training.\n            fetches = [self.summaries[""loss{}"".format(bucket_id)],\n                       self.apply_gradients[bucket_id],  # Update Op that does SGD.\n                       self.losses[bucket_id]]          # Loss for this batch.\n            outputs = self.sess.run(fetches=fetches, feed_dict=input_feed)\n            return outputs[0], None, outputs[2], None # Summary, no gradients, loss, outputs.\n        else:\n            fetches = [self.losses[bucket_id]]  # Loss for this batch.\n            for l in range(decoder_size):       # Output logits.\n                fetches.append(self.outputs[bucket_id][l])\n            outputs = self.sess.run(fetches=fetches, feed_dict=input_feed)\n            return None, None, outputs[0], outputs[1:] # No summary, no gradients, loss, outputs.\n\n    @staticmethod\n    def _sampled_loss(num_samples, hidden_size, vocab_size):\n        """"""Defines the samples softmax loss op and the associated output _projection.\n        Args:\n            num_samples:     (context: importance sampling) size of subset of outputs for softmax.\n            hidden_size:     number of units in the individual recurrent states.\n            vocab_size: number of unique output words.\n        Returns:\n            sampled_loss, apply_projection\n            - function: sampled_loss(labels, inputs)\n            - apply_projection: transformation to full vocab space, applied to decoder output.\n        """"""\n\n        assert(0 < num_samples < vocab_size)\n\n        # Define the standard affine-softmax transformation from hidden_size -> vocab_size.\n        # True output (for a given bucket) := tf.matmul(decoder_out, w) + b\n        w_t = tf.get_variable(""proj_w"", [vocab_size, hidden_size], dtype=tf.float32)\n        w = tf.transpose(w_t)\n        b = tf.get_variable(""proj_b"", [vocab_size], dtype=tf.float32)\n        output_projection = (w, b)\n\n        def sampled_loss(labels, inputs):\n            labels = tf.reshape(labels, [-1, 1])\n            return tf.nn.sampled_softmax_loss(\n                    weights=w_t,\n                    biases=b,\n                    labels=labels,\n                    inputs=inputs,\n                    num_sampled=num_samples,\n                    num_classes=vocab_size)\n\n        return sampled_loss, output_projection\n\n    @staticmethod\n    def _get_projections(num_buckets, unprojected_vals, projection_operator):\n        """"""Apply _projection operator to unprojected_vals, a tuple of length num_buckets.\n\n        :param num_buckets:         the number of projections that will be applied.\n        :param unprojected_vals:    tuple of length num_buckets.\n        :param projection_operator: (in the mathematical meaning) tuple of shape unprojected_vals.shape[-1].\n        :return: tuple of length num_buckets, with entries the same shape as entries in unprojected_vals, except for the last dimension.\n        """"""\n        projected_vals = unprojected_vals\n        for b in range(num_buckets):\n            projected_vals[b] = [tf.matmul(output, projection_operator[0]) + projection_operator[1]\n                                 for output in unprojected_vals[b]]\n        return projected_vals\n\n    @staticmethod\n    def _get_placeholder_list(name, length, dtype=tf.int32):\n        """"""\n        Args:\n            name: prefix of name of each tf.placeholder list item, where i\'th name is [name]i.\n            length: number of items (tf.placeholders) in the returned list.\n        Returns:\n            list of tensorflow placeholder of dtype=tf.int32 and unspecified shape.\n        """"""\n        return [tf.placeholder(dtype, shape=[None], name=name+str(i)) for i in range(length)]\n\n\nclass SimpleBot(BucketModel):\n    """"""Primitive implementation from scratch, for learning purposes.\n            1. Inputs: same as ChatBot.\n            2. Embedding: same as ChatBot.\n            3. BasicEncoder: Single GRUCell.\n            4. DynamicDecoder: Single GRUCell.\n    """"""\n\n    def __init__(self, dataset, params):\n\n        # SimpleBot allows user to not worry about making their own buckets.\n        # SimpleBot does that for you. SimpleBot cares.\n        max_seq_len = dataset.max_seq_len\n        buckets = [(max_seq_len // 2,  max_seq_len // 2), (max_seq_len, max_seq_len)]\n        logging.basicConfig(level=logging.INFO)\n        logger = logging.getLogger(\'SimpleBotLogger\')\n        super(SimpleBot, self).__init__(\n            logger=logger,\n            buckets=buckets,\n            dataset=dataset,\n            params=params)\n\n\n        # ==========================================================================================\n        # Create placeholder lists for encoder/decoder sequences.\n        # ==========================================================================================\n\n        with tf.variable_scope(""placeholders""):\n            self.encoder_inputs = [tf.placeholder(tf.int32, shape=[None], name=""encoder""+str(i))\n                                   for i in range(self.max_seq_len)]\n            self.decoder_inputs = [tf.placeholder(tf.int32, shape=[None], name=""decoder""+str(i))\n                                   for i in range(self.max_seq_len+1)]\n            self.target_weights = [tf.placeholder(tf.float32, shape=[None], name=""weight""+str(i))\n                                   for i in range(self.max_seq_len+1)]\n\n        # ==========================================================================================\n        # Before bucketing, need to define the underlying model(x, y) -> outputs, state(s).\n        # ==========================================================================================\n\n        def seq2seq(encoder_inputs, decoder_inputs, scope=None):\n            """"""Builds basic encoder-decoder model and returns list of (2D) output tensors.""""""\n            with tf.variable_scope(scope or ""seq2seq""):\n                encoder_cell = tf.contrib.rnn.GRUCell(self.state_size)\n                encoder_cell = tf.contrib.rnn.EmbeddingWrapper(encoder_cell, self.vocab_size, self.state_size)\n                # BasicEncoder(raw_inputs) -> Embed(raw_inputs) -> [be an RNN] -> encoder state.\n                _, encoder_state = tf.contrib.rnn.static_rnn(encoder_cell, encoder_inputs, dtype=tf.float32)\n                with tf.variable_scope(""decoder""):\n\n                    def loop_function(x):\n                        with tf.variable_scope(""loop_function""):\n                            params = tf.get_variable(""embed_tensor"", [self.vocab_size, self.state_size])\n                            return embedding_ops.embedding_lookup(params, tf.argmax(x, 1))\n\n                    _decoder_cell = tf.contrib.rnn.GRUCell(self.state_size)\n                    _decoder_cell = tf.contrib.rnn.EmbeddingWrapper(_decoder_cell, self.vocab_size, self.state_size)\n                    # Dear TensorFlow: you should replace the \'reuse\' param in\n                    # OutputProjectionWrapper with \'scope\' and just do scope.reuse in __init__.\n                    # sincerely, programming conventions.\n                    decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(\n                        _decoder_cell, self.vocab_size, reuse=tf.get_variable_scope().reuse)\n\n                    decoder_outputs = []\n                    prev = None\n                    decoder_state = None\n\n                    for i, dec_inp in enumerate(decoder_inputs):\n                        if self.is_chatting and prev is not None:\n                            dec_inp = loop_function(tf.reshape(prev, [1, 1]))\n                        if i == 0:\n                            output, decoder_state = decoder_cell(dec_inp, encoder_state,\n                                                                 scope=tf.get_variable_scope())\n                        else:\n                            tf.get_variable_scope().reuse_variables()\n                            output, decoder_state = decoder_cell(dec_inp, decoder_state,\n                                                                 scope=tf.get_variable_scope())\n                        decoder_outputs.append(output)\n                return decoder_outputs\n\n        # ====================================================================================\n        # Now we can build a simple bucketed seq2seq model.\n        # ====================================================================================\n\n        self.losses  = []\n        self.outputs = []\n        values  = self.encoder_inputs + self.decoder_inputs + self.decoder_inputs\n        with tf.name_scope(""simple_bucket_model"", values):\n            for idx_b, bucket in enumerate(buckets):\n                # Reminder: you should never explicitly set reuse=False. It\'s a no-no.\n                with tf.variable_scope(tf.get_variable_scope(), reuse=True if idx_b > 0 else None)\\\n                        as bucket_scope:\n                    # The outputs for this bucket are defined entirely by the seq2seq function.\n                    self.outputs.append(seq2seq(\n                        self.encoder_inputs[:bucket[0]],\n                        self.decoder_inputs[:bucket[1]],\n                        scope=bucket_scope))\n                    # Target outputs are just the inputs time-shifted by 1.\n                    target_outputs = [self.decoder_inputs[i + 1]\n                                      for i in range(len(self.decoder_inputs) - 1)]\n                    # Compute loss by comparing outputs and target outputs.\n                    self.losses.append(SimpleBot._simple_loss(self.batch_size,\n                                                              self.outputs[-1],\n                                                    target_outputs[:bucket[1]],\n                                                    self.target_weights[:bucket[1]]))\n\n        with tf.variable_scope(""summaries""):\n            self.summaries = {}\n            for i, loss in enumerate(self.losses):\n                name = ""loss{}"".format(i)\n                self.summaries[name] = tf.summary.scalar(""loss{}"".format(i), loss)\n\n    @staticmethod\n    def _simple_loss(batch_size, logits, targets, weights):\n        """"""Compute weighted cross-entropy loss on softmax(logits).""""""\n        # Note: name_scope only affects names of ops,\n        # while variable_scope affects both ops AND variables.\n        with tf.name_scope(""simple_loss"", values=logits+targets+weights):\n            log_perplexities = []\n            for l, t, w in zip(logits, targets, weights):\n                cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=t, logits=l)\n                log_perplexities.append(cross_entropy * w)\n        # Reduce via elementwise-sum.\n        log_perplexities = tf.add_n(log_perplexities)\n        # Get weighted-averge by dividing by sum of the weights.\n        log_perplexities /= tf.add_n(weights) + 1e-12\n        return tf.reduce_sum(log_perplexities) / tf.cast(batch_size, tf.float32)\n\n    def step(self, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only=False):\n        """"""Run a step of the model.\n\n        Args:\n          encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n          decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n          target_weights: list of numpy float vectors to feed as target weights.\n          bucket_id: which bucket of the model to use.\n\n        Returns:\n            [summary, gradient_norms, loss, outputs]:\n        """"""\n\n        encoder_size, decoder_size = self.buckets[bucket_id]\n        super(SimpleBot, self).check_input_lengths(\n            [encoder_inputs, decoder_inputs, target_weights],\n            [encoder_size, decoder_size, decoder_size])\n\n        input_feed = {}\n        for l in range(encoder_size):\n            input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n        for l in range(decoder_size):\n            input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n            input_feed[self.target_weights[l].name] = target_weights[l]\n        input_feed[self.decoder_inputs[decoder_size].name] = np.zeros([self.batch_size], dtype=np.int32)\n\n        # Fetches: the Operations/Tensors we want executed/evaluated during session.run(...).\n        if not forward_only: # Not just for decoding; also for validating in training.\n            fetches = [self.summaries[""loss{}"".format(bucket_id)],\n                       self.apply_gradients[bucket_id],  # Update Op that does SGD.\n                       self.losses[bucket_id]]          # Loss for this batch.\n            outputs = self.sess.run(fetches=fetches, feed_dict=input_feed)\n            return outputs[0], None, outputs[2], None  # summaries,  No gradient norm, loss, no outputs.\n        else:\n            fetches = [self.losses[bucket_id]]  # Loss for this batch.\n            for l in range(decoder_size):       # Output logits.\n                fetches.append(self.outputs[bucket_id][l])\n            outputs = self.sess.run(fetches=fetches, feed_dict=input_feed)\n            return None, None, outputs[0], outputs[1:]  #No summary,  No gradient norm, loss, outputs.\n'"
webpage/deepchat/__init__.py,0,"b'""""""deepchat/__init__.py: Initialize session objects.""""""\n\nimport os\nfrom flask import Flask\nfrom flask_wtf import CSRFProtect\nfrom flask_moment import Moment\nfrom flask_restful import Resource, Api\nfrom flask_basicauth import BasicAuth\nfrom flask_pagedown import PageDown\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_admin import Admin\nfrom config import config\n\ncsrf = CSRFProtect()\n# Initialize our database.\ndb = SQLAlchemy()\n# Nice thingy for displaying dates/times.\nmoment = Moment()\n# Client-sdie Markdown-to-HTML converter implemented in JS.\npagedown = PageDown()\n# Flask-restful api interface.\napi = Api()\n# Database visualizer.\n#name=os.getenv(\'APPENGINE_CONFIG\', \'Development\').title(),\nadmin = Admin(template_mode=\'bootstrap3\')\n# Basic authentication (mainly for using flask-admin).\nbasic_auth = BasicAuth()\n\nclass ReverseProxied(object):\n    \'\'\'Wrap the application in this middleware and configure the \n    front-end server to add these headers, to let you quietly bind \n    this to a URL other than / and to an HTTP scheme that is \n    different than what is used locally.\n\n    In nginx:\n    location /myprefix {\n        proxy_pass http://192.168.0.1:5001;\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Scheme $scheme;\n        proxy_set_header X-Script-Name /myprefix;\n        }\n\n    :param app: the WSGI application\n    \'\'\'\n    def __init__(self, app):\n        self.app = app\n\n    def __call__(self, environ, start_response):\n        script_name = environ.get(\'HTTP_X_SCRIPT_NAME\', \'\')\n        if script_name:\n            environ[\'SCRIPT_NAME\'] = script_name\n            path_info = environ[\'PATH_INFO\']\n            if path_info.startswith(script_name):\n                environ[\'PATH_INFO\'] = path_info[len(script_name):]\n\n        scheme = environ.get(\'HTTP_X_SCHEME\', \'\')\n        if scheme:\n            environ[\'wsgi.url_scheme\'] = scheme\n\n        server = environ.get(\'HTTP_X_FORWARDED_SERVER\', \'\')\n        if server:\n            environ[\'HTTP_HOST\'] = server\n\n        return self.app(environ, start_response)\n\n\ndef create_app(config_name):\n    """"""The application factory, which allows the app to be created at runtime. \n    This is in contrast to before, where it was created in the global scope \n    (i.e. no way to apply configuration changes dynamically).\n    \n    Returns:\n        app: the created application instance. Note that the app is still \n        missing routes and custom error page handlers, which will be handled \n        by blueprints.\n    """"""\n\n    from .main import main as main_blueprint\n\n    # Create flask application object, and\n    # read/use info in config.py.\n    app = Flask(__name__)\n    #if config_name == \'production\':\n    #    app.wsgi_app = ReverseProxied(app.wsgi_app)\n\n    csrf.init_app(app)\n    app.config.from_object(config[config_name])\n    config[config_name].init_app(app)\n\n    # Initialize our database.\n    db.init_app(app)\n    # Nice thingy for displaying dates/times.\n    moment.init_app(app)\n    # Client-sdie Markdown-to-HTML converter implemented in JS.\n    pagedown.init_app(app)\n    #\n    admin.name = config_name.title()\n    admin.init_app(app)\n\n    basic_auth.init_app(app)\n\n    api.init_app(app)\n    app.register_blueprint(main_blueprint)\n\n    return app\n\n\n\n'"
webpage/deepchat/models.py,0,"b'""""""app/models.py: Tutorial IV - Databases.\n\ndatabase models: collection of classes whose purpose is to represent the\n                 data that we will store in our database.\n\nThe ORM layer (SQLAlchemy) will do the translations required to map\nobjects created from these classes into rows in the proper database table.\n    - ORM: Object Relational Mapper; links b/w tables corresp. to objects.\n""""""\n\nfrom deepchat import db\nimport json\n\n\nclass User(db.Model):\n    """"""A model that represents our users.\n\n    Jargon/Parameters:\n        - primary key: unique id given to each user.\n        - varchar: a string.\n        - db.Column parameter info:\n            - index=True: allows for faster queries by associating a given column\n                          with its own index. Use for values frequently looked up.\n            - unique=True: don\'t allow duplicate values in this column.\n\n    Fields:\n        id: (db.Integer) primary_key for identifying a user in the table.\n        name: (str)\n        posts: (db.relationship)\n\n    """"""\n\n    # Fields are defined as class variables, but are used by super() in init.\n    # Pass boolean args to indicate which fields are unique/indexed.\n    # Note: \'unique\' here means [a given user] \'has only one\'.\n    id = db.Column(db.Integer, primary_key=True)\n    name = db.Column(db.String(64), index=True, unique=True)\n    # Relationships are not actual database fields (not shown on a db diagram).\n    # - backref: *defines* a field that will be added to the instances of\n    #             Posts that point back to this user.\n    # - lazy=\'dynamic\': ""Instead of loading the items, return another query\n    #                    object which we can refine before loading items.\n    conversations = db.relationship(\'Conversation\', backref=\'user\', lazy=\'dynamic\')\n\n    def __repr__(self):\n        return ""<User {0}>"".format(self.name)\n\n\nclass Chatbot(db.Model):\n    """"""Chatbot. Fields are the same as from yaml config files.""""""\n    id = db.Column(db.Integer, primary_key=True)\n    name = db.Column(db.String(64), index=True, unique=True)  # TODO: make unique?\n    dataset = db.Column(db.String(64))\n    base_cell = db.Column(db.String(64))\n    encoder = db.Column(db.String(64))\n    decoder = db.Column(db.String(64))\n    learning_rate = db.Column(db.Float)\n    num_layers = db.Column(db.Integer)\n    state_size = db.Column(db.Integer)\n    conversations = db.relationship(\'Conversation\', backref=\'chatbot\', lazy=\'dynamic\')\n\n    def __init__(self, name, **bot_kwargs):\n        self.name = (name or \'Unknown Bot\')\n        self.dataset = bot_kwargs[\'dataset\']\n        self.base_cell = bot_kwargs[\'base_cell\']\n        self.encoder = bot_kwargs[\'encoder\']\n        self.decoder = bot_kwargs[\'decoder\']\n        self.learning_rate = bot_kwargs[\'learning_rate\']\n        self.num_layers = bot_kwargs[\'num_layers\']\n        self.state_size = bot_kwargs[\'state_size\']\n\n\n    def __repr__(self):\n        return json.dumps(""<Chatbot {0}>"".format(self.name))\n\n\nclass Conversation(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    start_time = db.Column(db.DateTime, index=True, unique=True)\n    user_id = db.Column(db.Integer, db.ForeignKey(\'user.id\'))\n    chatbot_id = db.Column(db.Integer, db.ForeignKey(\'chatbot.id\'))\n    turns = db.relationship(\'Turn\', backref=\'conversation\', lazy=\'dynamic\')\n\n    def __repr__(self):\n        return \'<Conversation between {0} and {1}>\'.format(self.user_id, self.chatbot_id)\n\n\nclass Turn(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    user_message = db.Column(db.Text)\n    chatbot_message = db.Column(db.Text)\n    conversation_id = db.Column(db.Integer, db.ForeignKey(\'conversation.id\'))\n\n    def __repr__(self):\n        return \'User: {0}\\nChatBot: {1}\'.format(\n            self.user_message, self.chatbot_message)\n'"
webpage/deepchat/web_bot.py,11,"b'""""""Minimal subset of functions/methods from repo needed to run bot on Heroku.\nSee the main repository for better docs (all from utils and data directory).\n""""""\n\nimport os\nimport re\nimport numpy as np\nimport tensorflow as tf\nimport yaml\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'1\'\n\nUNK_ID  = 3\n# Regular expressions used to tokenize.\n_WORD_SPLIT = re.compile(b""([.,!?\\""\':;)(])"")\n_DIGIT_RE = re.compile(br""\\d"")\n_PRIMARY_KEYS = [\'model\', \'dataset\', \'model_params\', \'dataset_params\']\n\n\ndef basic_tokenizer(sentence):\n    words = []\n    for space_separated_fragment in sentence.strip().split():\n        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n    return [w for w in words if w]\n\n\ndef sentence_to_token_ids(sentence, vocabulary, normalize_digits=True):\n    words = basic_tokenizer(sentence)\n    if not normalize_digits:\n        return [vocabulary.get(w, UNK_ID) for w in words]\n    # Normalize digits by 0 before looking words up in the vocabulary.\n    return [vocabulary.get(_DIGIT_RE.sub(b""0"", w), UNK_ID) for w in words]\n\n\ndef get_vocab_dicts(vocabulary_path):\n    """"""Returns word_to_idx, idx_to_word dictionaries given vocabulary.""""""\n    if tf.gfile.Exists(vocabulary_path):\n        rev_vocab = []\n        with tf.gfile.GFile(vocabulary_path, mode=""rb"") as f:\n            rev_vocab.extend(f.readlines())\n        rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\n        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n        return vocab, rev_vocab\n    else:\n        raise ValueError(""Vocabulary file %s not found."" % vocabulary_path)\n\n\ndef load_graph(frozen_model_dir):\n    """"""Load frozen tensorflow graph into the default graph.\n\n    Args:\n        frozen_model_dir: location of protobuf file containing frozen graph.\n\n    Returns:\n        tf.Graph object imported from frozen_model_path.\n    """"""\n\n    # Prase the frozen graph definition into a GraphDef object.\n    frozen_file = os.path.join(frozen_model_dir, ""frozen_model.pb"")\n    with tf.gfile.GFile(frozen_file, ""rb"") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    # Load the graph def into the default graph and return it.\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(\n            graph_def,\n            input_map=None,\n            return_elements=None,\n            op_dict=None,\n            producer_op_list=None)\n    return graph\n\n\ndef unfreeze_bot(frozen_model_path):\n    """"""Restores the frozen graph from file and grabs input/output tensors needed to\n    interface with a cornell_bot for conversation.\n\n    Args:\n        frozen_model_path: location of protobuf file containing frozen graph.\n\n    Returns:\n        outputs: tensor that can be run in a session.\n    """"""\n\n    bot_graph = load_graph(frozen_model_path)\n    tensors = {\'inputs\': bot_graph.get_tensor_by_name(\'import/input_pipeline/user_input:0\'),\n               \'outputs\': bot_graph.get_tensor_by_name(\'import/outputs:0\')}\n    return tensors, bot_graph\n\n\nclass FrozenBot:\n    """"""The mouth and ears of a cornell_bot that\'s been serialized.""""""\n\n    def __init__(self, frozen_model_dir, is_testing=False):\n        """"""\n        Args:\n            is_testing: (bool) True for testing (while GPU is busy training).\n            In that case, just use a \'bot\' that returns inputs reversed.\n        """"""\n\n        # Get absolute path to model directory.\n        here = os.path.dirname(os.path.realpath(__file__))\n        assets_path = os.path.join(here, \'static\', \'assets\')\n        self.abs_model_dir = os.path.join(assets_path,\n                                     \'frozen_models\',\n                                     frozen_model_dir)\n        self.load_config(os.path.join(self.abs_model_dir, \'config.yml\'))\n        self.word_to_idx, self.idx_to_word = self.get_frozen_vocab(self.config)\n        self.is_testing = is_testing\n\n        # Setup tensorflow graph(s)/session(s) iff not testing.\n        if not is_testing:\n            self.unfreeze()\n\n    def load_config(self, config_path):\n        with open(config_path) as f:\n            config = yaml.load(f)\n        config[\'dataset_params\'][\'data_dir\'] = os.path.dirname(config_path)\n        self.__dict__[\'__params\'] = config\n\n    def __getattr__(self, name):\n        if name == \'config\':\n            return self.__dict__[\'__params\']\n        elif name in _PRIMARY_KEYS:\n            return self.__dict__[\'__params\'][name]\n        else:\n            for primary_key in _PRIMARY_KEYS:\n                if not isinstance(self.__dict__[\'__params\'][primary_key], dict):\n                    continue\n                elif name in self.__dict__[\'__params\'][primary_key]:\n                    return self.__dict__[\'__params\'][primary_key][name]\n        raise AttributeError(name)\n\n    def get_frozen_vocab(self, config):\n        """"""Helper function to get dictionaries between tokens and words.""""""\n        data_dir    = config[\'dataset_params\'][\'data_dir\']\n        vocab_size  = config[\'dataset_params\'][\'vocab_size\']\n        vocab_path = os.path.join(data_dir, \'vocab{}.txt\'.format(vocab_size))\n        word_to_idx, idx_to_word = get_vocab_dicts(vocab_path)\n        return word_to_idx, idx_to_word\n\n    def as_words(self, sentence):\n        words = []\n        for token in sentence:\n            word = self.idx_to_word[token]\n            try:\n                word = tf.compat.as_str(word)\n            except UnicodeDecodeError:\n                word = str(word)\n            words.append(word)\n\n        words = "" "".join(words)\n        words = words.replace(\' , \', \', \').replace(\' .\', \'.\').replace(\' !\', \'!\')\n        words = words.replace("" \' "", ""\'"").replace("" ?"", ""?"")\n        if len(words) < 2:\n            return words\n        return words[0].upper() + words[1:]\n\n\n    def __call__(self, sentence):\n        """"""Outputs response sentence (string) given input (string).""""""\n\n        if self.is_testing:\n            return sentence[::-1]\n\n        sentence = sentence.strip().lower()\n        print(\'User:\', sentence)\n        # Convert input sentence to token-ids.\n        sentence_tokens = sentence_to_token_ids(\n            tf.compat.as_bytes(sentence), self.word_to_idx)\n        sentence_tokens = np.array([sentence_tokens[::-1]])\n        # Get output sentence from the chatbot.\n        fetches = self.tensor_dict[\'outputs\']\n        feed_dict={self.tensor_dict[\'inputs\']: sentence_tokens}\n        response = self.sess.run(fetches=fetches, feed_dict=feed_dict)\n        response = self.as_words(response[0][:-1])\n        # Translate from confused-bot-language to English...\n        if \'UNK\' in response:\n            response = ""I don\'t know.""\n        print(""Bot:"", response)\n        return response\n\n    def unfreeze(self):\n        # Setup tensorflow graph(s)/session(s) iff not testing.\n        if not self.is_testing:\n            # Get bot graph and input/output tensors.\n            self.tensor_dict, graph = unfreeze_bot(self.abs_model_dir)\n            self.sess = tf.Session(graph=graph)\n\n    def freeze(self):\n        if not self.is_testing:\n            self.sess.close()\n            self.graph = self.tensor_dict = None\n\n\n'"
webpage/migrations/env.py,0,"b'from __future__ import with_statement\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom logging.config import fileConfig\nimport logging\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nfileConfig(config.config_file_name)\nlogger = logging.getLogger(\'alembic.env\')\n\n# add your model\'s MetaData object here\n# for \'autogenerate\' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\nfrom flask import current_app\nconfig.set_main_option(\'sqlalchemy.url\',\n                       current_app.config.get(\'SQLALCHEMY_DATABASE_URI\'))\ntarget_metadata = current_app.extensions[\'migrate\'].db.metadata\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(""my_important_option"")\n# ... etc.\n\n\ndef run_migrations_offline():\n    """"""Run migrations in \'offline\' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don\'t even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    """"""\n    url = config.get_main_option(""sqlalchemy.url"")\n    context.configure(url=url)\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online():\n    """"""Run migrations in \'online\' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    """"""\n\n    # this callback is used to prevent an auto-migration from being generated\n    # when there are no changes to the schema\n    # reference: http://alembic.readthedocs.org/en/latest/cookbook.html\n    def process_revision_directives(context, revision, directives):\n        if getattr(config.cmd_opts, \'autogenerate\', False):\n            script = directives[0]\n            if script.upgrade_ops.is_empty():\n                directives[:] = []\n                logger.info(\'No changes in schema detected.\')\n\n    engine = engine_from_config(config.get_section(config.config_ini_section),\n                                prefix=\'sqlalchemy.\',\n                                poolclass=pool.NullPool)\n\n    connection = engine.connect()\n    context.configure(connection=connection,\n                      target_metadata=target_metadata,\n                      process_revision_directives=process_revision_directives,\n                      **current_app.extensions[\'migrate\'].configure_args)\n\n    try:\n        with context.begin_transaction():\n            context.run_migrations()\n    finally:\n        connection.close()\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n'"
webpage/tests/__init__.py,0,b''
webpage/tests/test_database.py,0,"b'""""""Unit tests for the application.""""""\n\nfrom flask import current_app\nfrom flask import request\nfrom deepchat import create_app, db\nfrom deepchat.models import User, Conversation, Chatbot, Turn\n\nimport sys\nimport unittest\nimport sqlite3\nimport sqlalchemy\n\n\nclass TestDatabase(unittest.TestCase):\n\n    def setUp(self):\n        """"""Called before running a test.""""""\n        self.app = create_app(\'testing\')\n        self.app_context = self.app.app_context()\n        self.app_context.push()\n        db.create_all()\n\n    def tearDown(self):\n        """"""Called after running a test.""""""\n        db.session.remove()\n        db.drop_all()\n        self.app_context.pop()\n\n    def test_app_exists(self):\n        self.assertFalse(current_app is None)\n\n\n\n\n\n\n'"
webpage/tests/test_simple.py,0,"b'""""""Unit tests for the application.""""""\n\nimport unittest\nfrom flask import current_app\nfrom deepchat import create_app, db\n\n\nclass TestSimple(unittest.TestCase):\n    """"""Simple tests - ensuring the app can open/access database/etc.""""""\n\n    def setUp(self):\n        """"""Called before running a test.""""""\n        self.app = create_app(\'testing\')\n        self.app_context = self.app.app_context()\n        self.app_context.push()\n        db.create_all()\n\n    def tearDown(self):\n        """"""Called after running a test.""""""\n        db.session.remove()\n        db.drop_all()\n        self.app_context.pop()\n\n    def test_app_exists(self):\n        self.assertFalse(current_app is None)\n\n    def test_app_is_testing(self):\n        """"""Ensure we can access the right config specifications.""""""\n        self.assertTrue(current_app.config[\'TESTING\'])\n'"
chatbot/components/base/__init__.py,0,b''
chatbot/components/base/_rnn.py,31,"b'""""""Collection of base RNN classes and custom RNNCells.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom chatbot.components import bot_ops\nfrom tensorflow.contrib.rnn import RNNCell\nfrom tensorflow.contrib.rnn import GRUCell, MultiRNNCell, LSTMStateTuple\nfrom tensorflow.python.layers import core as layers_core\n\n# Required due to TensorFlow\'s unreliable naming across versions . . .\ntry:\n    # r1.1\n    from tensorflow.contrib.seq2seq import DynamicAttentionWrapper \\\n        as AttentionWrapper\n    from tensorflow.contrib.seq2seq import DynamicAttentionWrapperState \\\n        as AttentionWrapperState\nexcept ImportError:\n    # master\n    from tensorflow.contrib.seq2seq import AttentionWrapper\n    from tensorflow.contrib.seq2seq import AttentionWrapperState\n\n\nclass Cell(RNNCell):\n    """"""Simple wrapper class for any extensions I want to make to the\n    encoder/decoder rnn cells. For now, just Dropout+GRU.""""""\n\n    def __init__(self, state_size, num_layers, dropout_prob, base_cell):\n        """"""Define the cell by composing/wrapping with tf.contrib.rnn functions.\n        \n        Args:\n            state_size: number of units in the cell.\n            num_layers: how many cells to include in the MultiRNNCell.\n            dropout_prob: probability of a node being dropped.\n            base_cell: (str) name of underling cell to use (e.g. \'GRUCell\')\n        """"""\n\n        self._state_size = state_size\n        self._num_layers = num_layers\n        self._dropout_prob = dropout_prob\n        self._base_cell = base_cell\n\n        def single_cell():\n            """"""Convert cell name (str) to class, and create it.""""""\n            return getattr(tf.contrib.rnn, base_cell)(num_units=state_size)\n\n        if num_layers == 1:\n            self._cell = single_cell()\n        else:\n            self._cell = MultiRNNCell(\n                [single_cell() for _ in range(num_layers)])\n\n    @property\n    def state_size(self):\n        return self._cell.state_size\n\n    @property\n    def shape(self):\n        """"""Needed for shape_invariants arg for tf.while_loop.""""""\n        if self._num_layers == 1:\n            return self.single_layer_shape()\n        else:\n            return tuple(self.single_layer_shape()\n                         for _ in range(self._num_layers))\n\n    def single_layer_shape(self):\n        if \'LSTM\' in self._base_cell:\n            return LSTMStateTuple(c=tf.TensorShape([None, self._state_size]),\n                                  h=tf.TensorShape([None, self._state_size]))\n        else:\n            return tf.TensorShape([None, self._state_size])\n\n    @property\n    def output_size(self):\n        return self._cell.output_size\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Run this RNN cell on inputs, starting from the given state.\n\n        Args:\n            inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n            state: Either 2D Tensor or tuple of 2D tensors, determined by cases:\n                - `self.state_size` is int: `2-D Tensor` with shape\n                    `[batch_size x self.state_size]`.\n                - `self.state_size` is tuple: tuple with shapes\n                    `[batch_size x s] for s in self.state_size`.\n            scope: VariableScope for the created subgraph; \n                defaults to class name.\n\n        Returns:\n            A pair containing:\n            - Output: 2D tensor with shape [batch_size x self.output_size].\n            - New state: Either a single `2-D` tensor, or a tuple of tensors \n                matching the arity and shapes of `state`.\n        """"""\n        output, new_state = self._cell(inputs, state, scope)\n        output = tf.layers.dropout(output, rate=self._dropout_prob, name=""dropout"")\n        return output, new_state\n\n\nclass RNN(object):\n    """"""Base class for encoders/decoders. Has simple instance attributes and\n    an RNNCell object and getter.\n    """"""\n\n    def __init__(self,\n                 state_size,\n                 embed_size,\n                 dropout_prob,\n                 num_layers,\n                 base_cell=""GRUCell"",\n                 state_wrapper=None):\n        """"""\n        Args:\n            state_size: number of units in underlying rnn cell.\n            embed_size: dimension size of word-embedding space.\n            dropout_prob: probability of a node being dropped.\n            num_layers: how many cells to include in the MultiRNNCell.\n            base_cell: (str) name of underling cell to use (e.g. \'GRUCell\')\n            state_wrapper: allow states to store their wrapper class. See the\n                wrapper method docstring below for more info.\n        """"""\n        self.state_size = state_size\n        self.embed_size = embed_size\n        self.num_layers = num_layers\n        self.dropout_prob = dropout_prob\n        self.base_cell = base_cell\n        self._wrapper = state_wrapper\n\n    def get_cell(self, name):\n        """"""Returns a cell instance, defined by its name scope.""""""\n        with tf.name_scope(name, ""get_cell""):\n            return Cell(state_size=self.state_size,\n                        num_layers=self.num_layers,\n                        dropout_prob=self.dropout_prob,\n                        base_cell=self.base_cell)\n\n    def wrapper(self, state):\n        """"""Some RNN states are wrapped in namedtuples. \n        (TensorFlow decision, definitely not mine...). \n        \n        This is here for derived classes to specify their wrapper state. \n        Some examples: LSTMStateTuple and AttentionWrapperState.\n        \n        Args:\n            state: tensor state tuple, will be unpacked into the wrapper tuple.\n        """"""\n        if self._wrapper is None:\n            return state\n        else:\n            return self._wrapper(*state)\n\n    def __call__(self, *args):\n        raise NotImplemented\n\n\nclass SimpleAttentionWrapper(RNNCell):\n    """"""A simplified and tweaked version of TensorFlow\'s AttentionWrapper.\n    \n    It closely follows the implementation described by Luong et. al, 2015 in\n    `Effective Approaches to Attention-based Neural Machine Translation`.\n    """"""\n\n    def __init__(self,\n                 cell,\n                 attention_mechanism,\n                 initial_cell_state=None,\n                 name=None):\n        """"""Construct the wrapper.\n        \n        Main tweak is creating the attention_layer with a tanh activation \n        (Luong\'s choice) as opposed to linear (TensorFlow\'s choice). Also,\n        since I am sticking with Luong\'s approach, parameters that are in the\n        constructor of TensorFlow\'s AttentionWrapper have been removed, and \n        the corresponding values are set to how Luong\'s paper defined them.\n        \n        Args:\n            cell: instance of the Cell class above.\n            attention_mechanism: instance of tf AttentionMechanism.\n            initial_cell_state: The initial state value to use for the cell when\n                the user calls `zero_state()`.\n            name: Name to use when creating ops.\n        """"""\n\n        super(SimpleAttentionWrapper, self).__init__(name=name)\n\n        # Assume that \'cell\' is an instance of the custom \'Cell\' class above.\n        self._base_cell = cell._base_cell\n        self._num_layers = cell._num_layers\n        self._state_size = cell._state_size\n\n        self._attention_size = attention_mechanism.values.get_shape()[-1].value\n        self._attention_layer = layers_core.Dense(self._attention_size,\n                                                  activation=tf.nn.tanh,\n                                                  name=""attention_layer"",\n                                                  use_bias=False)\n\n        self._cell = cell\n        self._attention_mechanism = attention_mechanism\n        with tf.name_scope(name, ""AttentionWrapperInit""):\n            if initial_cell_state is None:\n                self._initial_cell_state = None\n            else:\n                final_state_tensor = nest.flatten(initial_cell_state)[-1]\n                state_batch_size = (\n                    final_state_tensor.shape[0].value\n                    or tf.shape(final_state_tensor)[0])\n                error_message = (\n                    ""Constructor AttentionWrapper %s: "" % self._base_name +\n                    ""Non-matching batch sizes between the memory ""\n                    ""(encoder output) and initial_cell_state."")\n                with tf.control_dependencies(\n                    [tf.assert_equal(state_batch_size,\n                        self._attention_mechanism.batch_size,\n                        message=error_message)]):\n                    self._initial_cell_state = nest.map_structure(\n                        lambda s: tf.identity(s, name=""check_initial_cell_state""),\n                        initial_cell_state)\n\n    def zero_state(self, batch_size, dtype):\n        with tf.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n            if self._initial_cell_state is not None:\n                cell_state = self._initial_cell_state\n            else:\n                cell_state = self._cell.zero_state(batch_size, dtype)\n            error_message = (\n                ""zero_state of AttentionWrapper %s: "" % self._base_name +\n                ""Non-matching batch sizes between the memory ""\n                ""(encoder output) and the requested batch size."")\n            with tf.control_dependencies(\n                [tf.assert_equal(batch_size,\n                    self._attention_mechanism.batch_size,\n                    message=error_message)]):\n                cell_state = nest.map_structure(\n                    lambda s: tf.identity(s, name=""checked_cell_state""),\n                    cell_state)\n            alignment_history = ()\n\n            _zero_state_tensors = rnn_cell_impl._zero_state_tensors\n            return AttentionWrapperState(\n                cell_state=cell_state,\n                time=tf.zeros([], dtype=tf.int32),\n                attention=_zero_state_tensors(self._attention_size, batch_size,\n                dtype),\n                alignments=self._attention_mechanism.initial_alignments(\n                    batch_size, dtype),\n                alignment_history=alignment_history)\n\n    def call(self, inputs, state):\n        """"""First computes the cell state and output in the usual way, \n        then works through the attention pipeline:\n            h --> a --> c --> h_tilde\n        using the naming/notation from Luong et. al, 2015.\n\n        Args:\n            inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n            state: An instance of `AttentionWrapperState` containing the \n                tensors from the prev timestep.\n     \n        Returns:\n            A tuple `(attention_or_cell_output, next_state)`, where:\n            - `attention_or_cell_output` depending on `output_attention`.\n            - `next_state` is an instance of `DynamicAttentionWrapperState`\n                containing the state calculated at this time step.\n        """"""\n\n        # Concatenate the previous h_tilde with inputs (input-feeding).\n        cell_inputs = tf.concat([inputs, state.attention], -1)\n\n        # 1. (hidden) Compute the hidden state (cell_output).\n        cell_output, next_cell_state = self._cell(cell_inputs,\n                                                  state.cell_state)\n\n        # 2. (align) Compute the normalized alignment scores. [B, L_enc].\n        # where L_enc is the max seq len in the encoder outputs for the (B)atch.\n        score = self._attention_mechanism(\n            cell_output, previous_alignments=state.alignments)\n        alignments = tf.nn.softmax(score)\n\n        # Reshape from [B, L_enc] to [B, 1, L_enc]\n        expanded_alignments = tf.expand_dims(alignments, 1)\n        # (Possibly projected) encoder outputs: [B, L_enc, state_size]\n        encoder_outputs = self._attention_mechanism.values\n        # 3 (context) Take inner prod. [B, 1, state size].\n        context = tf.matmul(expanded_alignments, encoder_outputs)\n        context = tf.squeeze(context, [1])\n\n        # 4 (h_tilde) Compute tanh(W [c, h]).\n        attention = self._attention_layer(\n            tf.concat([cell_output, context], -1))\n\n        next_state = AttentionWrapperState(\n            cell_state=next_cell_state,\n            attention=attention,\n            time=state.time + 1,\n            alignments=alignments,\n            alignment_history=())\n\n        return attention, next_state\n\n\n    @property\n    def output_size(self):\n        return self._attention_size\n\n    @property\n    def state_size(self):\n        return AttentionWrapperState(\n            cell_state=self._cell.state_size,\n            attention=self._attention_size,\n            time=tf.TensorShape([]),\n            alignments=self._attention_mechanism.alignments_size,\n            alignment_history=())\n\n    @property\n    def shape(self):\n        return AttentionWrapperState(\n            cell_state=self._cell.shape,\n            attention=tf.TensorShape([None, self._attention_size]),\n            time=tf.TensorShape(None),\n            alignments=tf.TensorShape([None, None]),\n            alignment_history=())\n\n\nclass BasicRNNCell(RNNCell):\n    """"""Same as tf.contrib.rnn.BasicRNNCell, rewritten for clarity.\n\n    For example, many TF implementations have leftover code debt from past \n    versions, so I wanted to show what is actually going on, with the fluff \n    removed. Also, I\'ve removed generally accepted values from parameters/args \n    in favor of just setting them.\n    """"""\n\n    def __init__(self, num_units, reuse=None):\n        self._num_units = num_units\n        self._reuse = reuse\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Most basic RNN. Define as:\n            output = new_state = act(W * input + U * state + B).\n        """"""\n        output = tf.tanh(bot_ops.linear_map(\n            args=[inputs, state],\n            output_size=self._num_units,\n            bias=True))\n        return output, output\n\n\n'"
webpage/deepchat/main/__init__.py,0,"b'""""""Package constructor file for creating blueprint(s).""""""\n\nfrom flask import Blueprint\nfrom flask_cors import CORS\n\n# Blueprint(<blueprint name>, <module/package where blueprint is located>).\n# Note: The <blueprint name> (main for us) defines the blueprint namespace.\nmain = Blueprint(\'main\', __name__)\nCORS(main, supports_credentials=True)\n\n# By importing views and errors here, we cause the routes and error handlers\n# to be associated with the blueprint.\nfrom . import views, errors\n'"
webpage/deepchat/main/errors.py,0,"b'""""""Routes for error pages.""""""\n\nfrom flask import render_template\nfrom . import main\n\n\n@main.app_errorhandler(404)\ndef page_not_found(e):\n    return render_template(\'404.html\'), 404\n\n\n@main.app_errorhandler(500)\ndef internal_server_error(e):\n    # TODO\n    return ""Server error. I should really make a template for this . . . ""\n'"
webpage/deepchat/main/forms.py,0,"b'""""""apps/forms.py: """"""\n\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, SubmitField, \\\n    TextField, TextAreaField, HiddenField\nfrom wtforms.validators import DataRequired, InputRequired\nfrom wtforms.validators import ValidationError\n\n\ndef bad_chars(form, string_field):\n    for c in r"";\'`"":\n        if c in string_field.data:\n            raise ValidationError(\'DONT TYPE DAT\')\n\n\nclass ChatForm(FlaskForm):\n    """"""Creates a chat_form for users to enter input.""""""\n    message = StringField(\'message\', validators=[DataRequired()])\n    submit = SubmitField(\'Submit\')\n\n\nclass UserForm(FlaskForm):\n    """"""Form for creating/editing a user.""""""\n    name = StringField(label=\'name\',\n                       id=\'user-name\',\n        validators=[DataRequired(), bad_chars])\n    submit = SubmitField(label=\'Submit\')\n\n\nclass SentencePairForm(FlaskForm):\n    input_sentence = StringField(\n        label=\'input-sentence\',\n        id=\'input-sentence\',\n        validators=[DataRequired()])\n    response_sentence = StringField(\n        label=\'response-sentence\',\n        id=\'response-sentence\',\n        validators=[DataRequired()])\n    submit = SubmitField(label=\'Submit\')\n\n\n'"
webpage/deepchat/main/views.py,0,"b'from datetime import datetime\nimport os\nimport yaml\nimport json\n\nfrom flask import make_response, flash\nfrom werkzeug.exceptions import HTTPException\nfrom flask_admin.contrib import sqla\n\nfrom . import main\nfrom .. import db, web_bot, admin, basic_auth, api\n\nfrom flask import redirect, current_app\nfrom flask import render_template\nfrom flask import session, url_for, request\nfrom flask_cors import cross_origin\nfrom flask_restful import Resource, fields\n\nfrom .forms import ChatForm, UserForm\nfrom ..models import User, Chatbot, Conversation, Turn\nfrom .. import models\nfrom pydoc import locate\n\n\n@main.context_processor\ndef inject_enumerate():\n    return dict(enumerate=enumerate)\n\n\n@main.before_app_first_request\ndef load_gloabal_data():\n    """"""Create the cornell_bot to be used for chat session.""""""\n    session[\'start_time\'] = None\n\n\n@main.route(\'/\')\n@main.route(\'/index\')\n@cross_origin()\ndef index():\n    # Create the (empty) forms that user can fill with info.\n    user_form = UserForm()\n    chat_form = ChatForm()\n    return render_template(\'index.html\',\n                           user=session.get(\'user\'),\n                           user_form=user_form,\n                           chat_form=chat_form)\n\n\n@main.route(\'/about\')\n@cross_origin()\ndef about():\n    return render_template(\'about.html\', user=session.get(\'user\', \'Anon\'))\n\n\n@main.route(\'/plots\')\ndef plots():\n    return render_template(\'plots.html\',\n                           user=session.get(\'user\', \'Anon\'))\n\n\ndef update_database(user_message, bot_response):\n    """"""Fill database (db) with new input-response, and associated data.""""""\n\n    # 1. Get the User db.Model.\n    user = get_database_model(\'User\', filter=session.get(\'user\', \'Anon\'))\n\n    # 2. Get the Chatbot db.Model.\n    bot_name = ChatAPI.bot_name\n    chatbot = get_database_model(\'Chatbot\', filter=bot_name)\n\n    # 3. Get the Conversation db.Model.\n    if session.get(\'start_time\') is None:\n        session[\'start_time\'] = datetime.utcnow()\n    conversation = get_database_model(\'Conversation\',\n                                      filter=session.get(\'start_time\'),\n                                      user=user,\n                                      chatbot=chatbot)\n\n    # 4. Get the Turn db.model. (called get adds it to the db if not there).\n    _ = get_database_model(\'Turn\',\n                           user_message=user_message,\n                           chatbot_message=bot_response,\n                           conversation=conversation)\n    db.session.commit()\n\n\ndef get_database_model(class_name, filter=None, **kwargs):\n    model_class = getattr(models, class_name)\n    assert model_class is not None, \'db_model for %s is None.\' % class_name\n\n    if filter is not None:\n        if class_name == \'Conversation\':\n            filter_kw = {\'start_time\': filter}\n        else:\n            filter_kw = {\'name\': filter}\n        db_model = model_class.query.filter_by(**filter_kw).first()\n    else:\n        db_model = None\n        filter_kw = {}\n\n    if db_model is None:\n        db_model = model_class(**filter_kw, **kwargs)\n        db.session.add(db_model)\n    return db_model\n\n\n# -------------------------------------------------------\n# APIs\n# -------------------------------------------------------\n\nclass UserAPI(Resource):\n\n    def post(self):\n        name = request.values.get(\'name\', \'Anon\')\n        session[\'user\'] = name\n        user_model = get_database_model(\'User\', filter=name)\n        return {\'name\': user_model.name}\n\n\nclass ChatAPI(Resource):\n    # Class attributes. This is convenient since we only want one active\n    # bot at any given time.\n    bot_name = \'Unk Bot\'\n    bot = None\n\n    def __init__(self, data_name):\n        if ChatAPI.bot_name != data_name:\n            ChatAPI.bot_name = data_name\n            ChatAPI.bot = web_bot.FrozenBot(frozen_model_dir=data_name,\n                                            is_testing=current_app.testing)\n            config = ChatAPI.bot.config\n            _ = get_database_model(\'Chatbot\',\n                                   filter=ChatAPI.bot_name,\n                                   dataset=config[\'dataset\'],\n                                   **config[\'model_params\'])\n            db.session.commit()\n            # TODO: delete this after refactor rest of file.\n            session[\'data_name\'] = data_name\n\n    def post(self):\n        print(\'post received\')\n        print(\'request:\', request)\n        user_message = request.values.get(\'user_message\')\n        print(\'user_message = \', user_message)\n        bot_response = self.bot(user_message)\n        print(\'resp:\', bot_response)\n        update_database(user_message, bot_response)\n        return {\'response\': bot_response,\n                \'bot_name\': ChatAPI.bot_name}\n\n\nclass RedditAPI(ChatAPI):\n    def __init__(self):\n        super(RedditAPI, self).__init__(\'reddit\')\n\n\nclass CornellAPI(ChatAPI):\n    def __init__(self):\n        super(CornellAPI, self).__init__(\'cornell\')\n\n\nclass UbuntuAPI(ChatAPI):\n    def __init__(self):\n        super(UbuntuAPI, self).__init__(\'ubuntu\')\n\napi.add_resource(UserAPI, \'/user/\')\napi.add_resource(RedditAPI, \'/chat/reddit/\')\napi.add_resource(CornellAPI, \'/chat/cornell/\')\napi.add_resource(UbuntuAPI, \'/chat/ubuntu/\')\n\n# -------------------------------------------------------\n# ADMIN: Authentication for the admin (me) on /admin.\n# -------------------------------------------------------\n\n\nclass AuthException(HTTPException):\n    def __init__(self, message):\n        super().__init__(message, make_response(\n            ""You could not be authenticated. Please refresh the page."", 401,\n            {\'WWW-Authenticate\': \'Basic realm=""Login Required""\'}))\n\n\nclass ModelView(sqla.ModelView):\n    def is_accessible(self):\n        if not basic_auth.authenticate():\n            raise AuthException(\'Not authenticated.\')\n        else:\n            return True\n\n    def inaccessible_callback(self, name, **kwargs):\n        return redirect(basic_auth.challenge())\n\nadmin.add_view(ModelView(User, db.session))\nadmin.add_view(ModelView(Chatbot, db.session))\nadmin.add_view(ModelView(Conversation, db.session))\nadmin.add_view(ModelView(Turn, db.session))\n'"
webpage/migrations/versions/236b966ecd2f_.py,0,"b'""""""empty message\n\nRevision ID: 236b966ecd2f\nRevises: \nCreate Date: 2017-05-03 14:27:37.853971\n\n""""""\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = \'236b966ecd2f\'\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\'chatbot\',\n    sa.Column(\'id\', sa.Integer(), nullable=False),\n    sa.Column(\'name\', sa.String(length=64), nullable=True),\n    sa.Column(\'dataset\', sa.String(length=64), nullable=True),\n    sa.Column(\'base_cell\', sa.String(length=64), nullable=True),\n    sa.Column(\'encoder\', sa.String(length=64), nullable=True),\n    sa.Column(\'decoder\', sa.String(length=64), nullable=True),\n    sa.Column(\'learning_rate\', sa.Float(), nullable=True),\n    sa.Column(\'num_layers\', sa.Integer(), nullable=True),\n    sa.Column(\'state_size\', sa.Integer(), nullable=True),\n    sa.PrimaryKeyConstraint(\'id\')\n    )\n    op.create_index(op.f(\'ix_chatbot_name\'), \'chatbot\', [\'name\'], unique=True)\n    op.create_table(\'user\',\n    sa.Column(\'id\', sa.Integer(), nullable=False),\n    sa.Column(\'name\', sa.String(length=64), nullable=True),\n    sa.PrimaryKeyConstraint(\'id\')\n    )\n    op.create_index(op.f(\'ix_user_name\'), \'user\', [\'name\'], unique=True)\n    op.create_table(\'conversation\',\n    sa.Column(\'id\', sa.Integer(), nullable=False),\n    sa.Column(\'start_time\', sa.DateTime(), nullable=True),\n    sa.Column(\'user_id\', sa.Integer(), nullable=True),\n    sa.Column(\'chatbot_id\', sa.Integer(), nullable=True),\n    sa.ForeignKeyConstraint([\'chatbot_id\'], [\'chatbot.id\'], ),\n    sa.ForeignKeyConstraint([\'user_id\'], [\'user.id\'], ),\n    sa.PrimaryKeyConstraint(\'id\')\n    )\n    op.create_index(op.f(\'ix_conversation_start_time\'), \'conversation\', [\'start_time\'], unique=True)\n    op.create_table(\'turn\',\n    sa.Column(\'id\', sa.Integer(), nullable=False),\n    sa.Column(\'user_message\', sa.Text(), nullable=True),\n    sa.Column(\'chatbot_message\', sa.Text(), nullable=True),\n    sa.Column(\'conversation_id\', sa.Integer(), nullable=True),\n    sa.ForeignKeyConstraint([\'conversation_id\'], [\'conversation.id\'], ),\n    sa.PrimaryKeyConstraint(\'id\')\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\'turn\')\n    op.drop_index(op.f(\'ix_conversation_start_time\'), table_name=\'conversation\')\n    op.drop_table(\'conversation\')\n    op.drop_index(op.f(\'ix_user_name\'), table_name=\'user\')\n    op.drop_table(\'user\')\n    op.drop_index(op.f(\'ix_chatbot_name\'), table_name=\'chatbot\')\n    op.drop_table(\'chatbot\')\n    # ### end Alembic commands ###\n'"
