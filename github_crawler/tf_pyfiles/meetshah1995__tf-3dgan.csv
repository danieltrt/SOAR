file_path,api_count,code
src/3dgan.py,70,"b'#!/usr/bin/env python\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nimport dataIO as d\n\nfrom tqdm import *\n\n\n\'\'\'\nGlobal Parameters\n\'\'\'\nn_epochs   = 10\nbatch_size = 64\ng_lr       = 0.0025\nd_lr       = 0.00001\nbeta       = 0.5\nalpha_d    = 0.0015\nalpha_g    = 0.000025\nd_thresh   = 0.8 \nz_size     = 100\nobj        = \'chair\' \n\ntrain_sample_directory = \'./train_sample/\'\nmodel_directory = \'./models/\'\nis_local = True\n\nweights, biases = {}, {}\n\ndef generator(z, batch_size=batch_size, phase_train=True, reuse=False):\n\n    strides    = [1,2,2,2,1]\n\n    g_1 = tf.add(tf.matmul(z, weights[\'wg1\']), biases[\'bg1\'])\n    g_1 = tf.reshape(g_1, [-1,4,4,4,512])\n    g_1 = tf.contrib.layers.batch_norm(g_1, is_training=phase_train)\n\n    g_2 = tf.nn.conv3d_transpose(g_1, weights[\'wg2\'], output_shape=[batch_size,8,8,8,256], strides=strides, padding=""SAME"")\n    g_2 = tf.nn.bias_add(g_2, biases[\'bg2\'])\n    g_2 = tf.contrib.layers.batch_norm(g_2, is_training=phase_train)\n    g_2 = tf.nn.relu(g_2)\n\n    g_3 = tf.nn.conv3d_transpose(g_2, weights[\'wg3\'], output_shape=[batch_size,16,16,16,128], strides=strides, padding=""SAME"")\n    g_3 = tf.nn.bias_add(g_3, biases[\'bg3\'])\n    g_3 = tf.contrib.layers.batch_norm(g_3, is_training=phase_train)\n    g_3 = tf.nn.relu(g_3)\n    \n    g_4 = tf.nn.conv3d_transpose(g_3, weights[\'wg4\'], output_shape=[batch_size,32,32,32,1], strides=strides, padding=""SAME"")\n    g_4 = tf.nn.bias_add(g_4, biases[\'bg4\'])                                   \n    g_4 = tf.nn.sigmoid(g_4)\n    \n    return g_4\n\n\ndef discriminator(inputs, phase_train=True, reuse=False):\n\n    strides    = [1,2,2,2,1]\n\n    d_1 = tf.nn.conv3d(inputs, weights[\'wd1\'], strides=strides, padding=""SAME"")\n    d_1 = tf.nn.bias_add(d_1, biases[\'bd1\'])\n    d_1 = tf.contrib.layers.batch_norm(d_1, is_training=phase_train)                               \n    d_1 = tf.nn.relu(d_1)\n\n    d_2 = tf.nn.conv3d(d_1, weights[\'wd2\'], strides=strides, padding=""SAME"") \n    d_2 = tf.nn.bias_add(d_2, biases[\'bd2\'])                                  \n    d_2 = tf.contrib.layers.batch_norm(d_2, is_training=phase_train)\n    d_2 = tf.nn.relu(d_2)\n    \n    d_3 = tf.nn.conv3d(d_2, weights[\'wd3\'], strides=strides, padding=""SAME"") \n    d_3 = tf.nn.bias_add(d_3, biases[\'bd3\'])                                  \n    d_3 = tf.contrib.layers.batch_norm(d_3, is_training=phase_train)\n    d_3 = tf.nn.relu(d_3) \n\n    d_4 = tf.nn.conv3d(d_3, weights[\'wd4\'], strides=strides, padding=""SAME"")     \n    d_4 = tf.nn.bias_add(d_4, biases[\'bd4\'])                              \n    d_4 = tf.contrib.layers.batch_norm(d_4, is_training=phase_train)\n    d_4 = tf.nn.relu(d_4) \n\n    shape = d_4.get_shape().as_list()\n    dim = np.prod(shape[1:])\n    d_5 = tf.reshape(d_4, shape=[-1, dim])\n    d_5 = tf.add(tf.matmul(d_5, weights[\'wd5\']), biases[\'bd5\'])\n    \n    return d_5\n\ndef initialiseWeights():\n\n    global weights\n    xavier_init = tf.contrib.layers.xavier_initializer()\n\n    # filter for deconv3d: A 5-D Tensor with the same type as value and shape [depth, height, width, output_channels, in_channels]\n    weights[\'wg1\'] = tf.get_variable(""wg1"", shape=[z_size, 4*4*4*512], initializer=xavier_init)\n    weights[\'wg2\'] = tf.get_variable(""wg2"", shape=[4, 4, 4, 256, 512], initializer=xavier_init)\n    weights[\'wg3\'] = tf.get_variable(""wg3"", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n    weights[\'wg4\'] = tf.get_variable(""wg4"", shape=[4, 4, 4, 1, 128  ], initializer=xavier_init)\n\n    weights[\'wd1\'] = tf.get_variable(""wd1"", shape=[4, 4, 4, 1, 32], initializer=xavier_init)\n    weights[\'wd2\'] = tf.get_variable(""wd2"", shape=[4, 4, 4, 32, 64], initializer=xavier_init)\n    weights[\'wd3\'] = tf.get_variable(""wd3"", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n    weights[\'wd4\'] = tf.get_variable(""wd4"", shape=[2, 2, 2, 128, 256], initializer=xavier_init)    \n    weights[\'wd5\'] = tf.get_variable(""wd5"", shape=[2* 2* 2* 256, 1 ], initializer=xavier_init)    \n\n    return weights\n\ndef initialiseBiases():\n    \n    global biases\n    zero_init = tf.zeros_initializer()\n\n    biases[\'bg1\'] = tf.get_variable(""bg1"", shape=[4*4*4*512], initializer=zero_init)\n    biases[\'bg2\'] = tf.get_variable(""bg2"", shape=[256], initializer=zero_init)\n    biases[\'bg3\'] = tf.get_variable(""bg3"", shape=[128], initializer=zero_init)\n    biases[\'bg4\'] = tf.get_variable(""bg4"", shape=[ 1 ], initializer=zero_init)\n\n    biases[\'bd1\'] = tf.get_variable(""bd1"", shape=[32], initializer=zero_init)\n    biases[\'bd2\'] = tf.get_variable(""bd2"", shape=[64], initializer=zero_init)\n    biases[\'bd3\'] = tf.get_variable(""bd3"", shape=[128], initializer=zero_init)\n    biases[\'bd4\'] = tf.get_variable(""bd4"", shape=[256], initializer=zero_init)    \n    biases[\'bd5\'] = tf.get_variable(""bd5"", shape=[1 ], initializer=zero_init) \n\n    return biases\n\ndef trainGAN():\n\n    weights, biases =  initialiseWeights(), initialiseBiases()\n\n    z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n    x_vector = tf.placeholder(shape=[batch_size,32,32,32,1],dtype=tf.float32) \n\n    net_g_train = generator(z_vector, phase_train=True, reuse=False) \n\n    d_output_x = discriminator(x_vector, phase_train=True, reuse=False)\n    d_output_x = tf.maximum(tf.minimum(d_output_x, 0.99), 0.01)\n    summary_d_x_hist = tf.summary.histogram(""d_prob_x"", d_output_x)\n\n    d_output_z = discriminator(net_g_train, phase_train=True, reuse=True)\n    d_output_z = tf.maximum(tf.minimum(d_output_z, 0.99), 0.01)\n    summary_d_z_hist = tf.summary.histogram(""d_prob_z"", d_output_z)\n\n    d_loss = -tf.reduce_mean(tf.log(d_output_x) + tf.log(1-d_output_z))\n    summary_d_loss = tf.summary.scalar(""d_loss"", d_loss)\n    \n    g_loss = -tf.reduce_mean(tf.log(d_output_z))\n    summary_g_loss = tf.summary.scalar(""g_loss"", g_loss)\n\n    net_g_test = generator(z_vector, phase_train=True, reuse=True)\n    para_g=list(np.array(tf.trainable_variables())[[0,1,4,5,8,9,12,13]])\n    para_d=list(np.array(tf.trainable_variables())[[14,15,16,17,20,21,24,25]])#,28,29]])\n\n    # only update the weights for the discriminator network\n    optimizer_op_d = tf.train.AdamOptimizer(learning_rate=alpha_d,beta1=beta).minimize(d_loss,var_list=para_d)\n    # only update the weights for the generator network\n    optimizer_op_g = tf.train.AdamOptimizer(learning_rate=alpha_g,beta1=beta).minimize(g_loss,var_list=para_g)\n\n    saver = tf.train.Saver(max_to_keep=50) \n\n    with tf.Session() as sess:  \n      \n        sess.run(tf.global_variables_initializer())        \n        z_sample = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n        volumes = d.getAll(obj=obj, train=True, is_local=True)\n        volumes = volumes[...,np.newaxis].astype(np.float) \n\n        for epoch in tqdm(range(n_epochs)):\n            \n            idx = np.random.randint(len(volumes), size=batch_size)\n            x = volumes[idx]\n            z = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n        \n            # Update the discriminator and generator\n            d_summary_merge = tf.summary.merge([summary_d_loss, summary_d_x_hist,summary_d_z_hist])\n\n            summary_d, discriminator_loss = sess.run([d_summary_merge,d_loss],feed_dict={z_vector:z, x_vector:x})\n            summary_g, generator_loss = sess.run([summary_g_loss,g_loss],feed_dict={z_vector:z})  \n            \n            if discriminator_loss <= 4.6*0.1: \n                sess.run([optimizer_op_g],feed_dict={z_vector:z})\n            elif generator_loss <= 4.6*0.1:\n                sess.run([optimizer_op_d],feed_dict={z_vector:z, x_vector:x})\n            else:\n                sess.run([optimizer_op_d],feed_dict={z_vector:z, x_vector:x})\n                sess.run([optimizer_op_g],feed_dict={z_vector:z})\n                            \n            print ""epoch: "",epoch,\', d_loss:\',discriminator_loss,\'g_loss:\',generator_loss\n\n            # output generated chairs\n            if epoch % 500 == 10:\n                g_chairs = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n                if not os.path.exists(train_sample_directory):\n                    os.makedirs(train_sample_directory)\n                g_chairs.dump(train_sample_directory+\'/\'+str(epoch))\n            \n            if epoch % 500 == 10:\n                if not os.path.exists(model_directory):\n                    os.makedirs(model_directory)      \n                saver.save(sess, save_path = model_directory + \'/\' + str(epoch) + \'.cptk\')\n\ndef testGAN():\n    ## TODO\n    pass\n\ndef visualize():\n    ## TODO\n    pass\n\ndef saveModel():\n    ## TODO\n    pass\n\nif __name__ == \'__main__\':\n    trainGAN()\n'"
src/3dgan_autoencoder.py,112,"b'#!/usr/bin/env python\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nimport dataIO as d\n\nfrom tqdm import *\nfrom utils import *\n\n\'\'\'\nGlobal Parameters\n\'\'\'\nn_epochs   = 10000\nn_ae_epochs= 1000\nbatch_size = 50\ng_lr       = 0.0025\nd_lr       = 0.00001\nae_lr      = 0.0001\nbeta       = 0.5\nd_thresh   = 0.8 \nz_size     = 200\nleak_value = 0.2\ncube_len   = 64\nobj_ratio  = 0.5\nreg_l2     = 0.001\ngan_inter  = 50\nae_inter   = 50\nobj        = \'chair\' \n\ntrain_sample_directory = \'./train_sample/\'\nmodel_directory = \'./models/\'\nis_local = False\n\nweights, biases = {}, {}\n\ndef generator(z, batch_size=batch_size, phase_train=True, reuse=False):\n\n    strides    = [1,2,2,2,1]\n\n    with tf.variable_scope(""gen""):\n        z = tf.reshape(z, (batch_size, 1, 1, 1, z_size))\n        g_1 = tf.nn.conv3d_transpose(z, weights[\'wg1\'], (batch_size,4,4,4,512), strides=[1,1,1,1,1], padding=""VALID"")\n        g_1 = tf.nn.bias_add(g_1, biases[\'bg1\'])                                  \n        g_1 = tf.contrib.layers.batch_norm(g_1, is_training=phase_train)\n        g_1 = tf.nn.relu(g_1)\n\n        g_2 = tf.nn.conv3d_transpose(g_1, weights[\'wg2\'], (batch_size,8,8,8,256), strides=strides, padding=""SAME"")\n        g_2 = tf.nn.bias_add(g_2, biases[\'bg2\'])\n        g_2 = tf.contrib.layers.batch_norm(g_2, is_training=phase_train)\n        g_2 = tf.nn.relu(g_2)\n\n        g_3 = tf.nn.conv3d_transpose(g_2, weights[\'wg3\'], (batch_size,16,16,16,128), strides=strides, padding=""SAME"")\n        g_3 = tf.nn.bias_add(g_3, biases[\'bg3\'])\n        g_3 = tf.contrib.layers.batch_norm(g_3, is_training=phase_train)\n        g_3 = tf.nn.relu(g_3)\n\n        g_4 = tf.nn.conv3d_transpose(g_3, weights[\'wg4\'], (batch_size,32,32,32,64), strides=strides, padding=""SAME"")\n        g_4 = tf.nn.bias_add(g_4, biases[\'bg4\'])\n        g_4 = tf.contrib.layers.batch_norm(g_4, is_training=phase_train)\n        g_4 = tf.nn.relu(g_4)\n        \n        g_5 = tf.nn.conv3d_transpose(g_4, weights[\'wg5\'], (batch_size,64,64,64,1), strides=strides, padding=""SAME"")\n        g_5 = tf.nn.bias_add(g_5, biases[\'bg5\'])\n        g_5 = tf.nn.sigmoid(g_5)\n\n    print g_1, \'g1\'\n    print g_2, \'g2\'\n    print g_3, \'g3\'\n    print g_4, \'g4\'\n    print g_5, \'g5\'\n    \n    return g_5\n\ndef encoder(inputs, phase_train=True, reuse=False):\n\n    strides    = [1,2,2,2,1]\n    with tf.variable_scope(""dis""):\n        d_1 = tf.nn.conv3d(inputs, weights[\'wd1\'], strides=strides, padding=""SAME"")\n        d_1 = tf.nn.bias_add(d_1, biases[\'bd1\'])\n        d_1 = tf.contrib.layers.batch_norm(d_1, is_training=phase_train)                               \n        d_1 = lrelu(d_1, leak_value)\n\n        d_2 = tf.nn.conv3d(d_1, weights[\'wd2\'], strides=strides, padding=""SAME"") \n        d_2 = tf.nn.bias_add(d_2, biases[\'bd2\'])\n        d_2 = tf.contrib.layers.batch_norm(d_2, is_training=phase_train)\n        d_2 = lrelu(d_2, leak_value)\n        \n        d_3 = tf.nn.conv3d(d_2, weights[\'wd3\'], strides=strides, padding=""SAME"")  \n        d_3 = tf.nn.bias_add(d_3, biases[\'bd3\'])\n        d_3 = tf.contrib.layers.batch_norm(d_3, is_training=phase_train)\n        d_3 = lrelu(d_3, leak_value) \n\n        d_4 = tf.nn.conv3d(d_3, weights[\'wd4\'], strides=strides, padding=""SAME"")     \n        d_4 = tf.nn.bias_add(d_4, biases[\'bd4\'])\n        d_4 = tf.contrib.layers.batch_norm(d_4, is_training=phase_train)\n        d_4 = lrelu(d_4)\n\n        d_5 = tf.nn.conv3d(d_4, weights[\'wae_d\'], strides=[1,1,1,1,1], padding=""VALID"")     \n        d_5 = tf.nn.bias_add(d_5, biases[\'bae_d\'])\n        d_5 = tf.nn.sigmoid(d_5)\n\n    print d_5, \'ae5\'\n\n    return d_5\n\ndef discriminator(inputs, phase_train=True, reuse=False):\n\n    strides    = [1,2,2,2,1]\n    with tf.variable_scope(""dis"", reuse=True):\n        d_1 = tf.nn.conv3d(inputs, weights[\'wd1\'], strides=strides, padding=""SAME"")\n        d_1 = tf.nn.bias_add(d_1, biases[\'bd1\'])\n        d_1 = tf.contrib.layers.batch_norm(d_1, is_training=phase_train)                               \n        d_1 = lrelu(d_1, leak_value)\n\n        d_2 = tf.nn.conv3d(d_1, weights[\'wd2\'], strides=strides, padding=""SAME"") \n        d_2 = tf.nn.bias_add(d_2, biases[\'bd2\'])\n        d_2 = tf.contrib.layers.batch_norm(d_2, is_training=phase_train)\n        d_2 = lrelu(d_2, leak_value)\n        \n        d_3 = tf.nn.conv3d(d_2, weights[\'wd3\'], strides=strides, padding=""SAME"")  \n        d_3 = tf.nn.bias_add(d_3, biases[\'bd3\'])\n        d_3 = tf.contrib.layers.batch_norm(d_3, is_training=phase_train)\n        d_3 = lrelu(d_3, leak_value) \n\n        d_4 = tf.nn.conv3d(d_3, weights[\'wd4\'], strides=strides, padding=""SAME"")     \n        d_4 = tf.nn.bias_add(d_4, biases[\'bd4\'])\n        d_4 = tf.contrib.layers.batch_norm(d_4, is_training=phase_train)\n        d_4 = lrelu(d_4)\n\n        d_5 = tf.nn.conv3d(d_4, weights[\'wd5\'], strides=[1,1,1,1,1], padding=""VALID"")     \n        d_5 = tf.nn.bias_add(d_5, biases[\'bd5\'])\n        d_5 = tf.contrib.layers.batch_norm(d_5, is_training=phase_train)\n        d_5 = tf.nn.sigmoid(d_5)\n\n    print d_1, \'d1\'\n    print d_2, \'d2\'\n    print d_3, \'d3\'\n    print d_4, \'d4\'\n    print d_5, \'d5\'\n\n    return d_5\n\ndef initialiseWeights():\n\n    global weights\n    xavier_init = tf.contrib.layers.xavier_initializer()\n\n    weights[\'wg1\'] = tf.get_variable(""wg1"", shape=[4, 4, 4, 512, 200], initializer=xavier_init)\n    weights[\'wg2\'] = tf.get_variable(""wg2"", shape=[4, 4, 4, 256, 512], initializer=xavier_init)\n    weights[\'wg3\'] = tf.get_variable(""wg3"", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n    weights[\'wg4\'] = tf.get_variable(""wg4"", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n    weights[\'wg5\'] = tf.get_variable(""wg5"", shape=[4, 4, 4, 1, 64], initializer=xavier_init)    \n\n    weights[\'wd1\'] = tf.get_variable(""wd1"", shape=[4, 4, 4, 1, 64], initializer=xavier_init)\n    weights[\'wd2\'] = tf.get_variable(""wd2"", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n    weights[\'wd3\'] = tf.get_variable(""wd3"", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n    weights[\'wd4\'] = tf.get_variable(""wd4"", shape=[4, 4, 4, 256, 512], initializer=xavier_init)    \n    weights[\'wd5\'] = tf.get_variable(""wd5"", shape=[4, 4, 4, 512, 1], initializer=xavier_init)    \n\n    return weights\n\ndef initialiseBiases():\n    \n    global biases\n    zero_init = tf.zeros_initializer()\n\n    biases[\'bg1\'] = tf.get_variable(""bg1"", shape=[512], initializer=zero_init)\n    biases[\'bg2\'] = tf.get_variable(""bg2"", shape=[256], initializer=zero_init)\n    biases[\'bg3\'] = tf.get_variable(""bg3"", shape=[128], initializer=zero_init)\n    biases[\'bg4\'] = tf.get_variable(""bg4"", shape=[64], initializer=zero_init)\n    biases[\'bg5\'] = tf.get_variable(""bg5"", shape=[1], initializer=zero_init)\n\n    biases[\'bd1\'] = tf.get_variable(""bd1"", shape=[64], initializer=zero_init)\n    biases[\'bd2\'] = tf.get_variable(""bd2"", shape=[128], initializer=zero_init)\n    biases[\'bd3\'] = tf.get_variable(""bd3"", shape=[256], initializer=zero_init)\n    biases[\'bd4\'] = tf.get_variable(""bd4"", shape=[512], initializer=zero_init)    \n    biases[\'bd5\'] = tf.get_variable(""bd5"", shape=[1], initializer=zero_init) \n\n    return biases\n\ndef trainGAN(is_dummy=False, exp_id=None):\n\n    weights, biases =  initialiseWeights(), initialiseBiases()\n    x_vector = tf.placeholder(shape=[batch_size,cube_len,cube_len,cube_len,1],dtype=tf.float32) \n    z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n\n    # Weights for autoencoder pretraining\n    xavier_init = tf.contrib.layers.xavier_initializer()\n    zero_init = tf.zeros_initializer()\n    weights[\'wae_d\'] = tf.get_variable(""wae_d"", shape=[4, 4, 4, 512, 200], initializer=xavier_init)\n    biases[\'bae_d\'] =  tf.get_variable(""bae_d"", shape=[200], initializer=zero_init)\n\n    encoded = encoder(x_vector, phase_train=True, reuse=False)\n    encoded = tf.maximum(tf.minimum(encoded, 0.99), 0.01)\n    decoded = generator(encoded, phase_train=True, reuse=False) \n\n    decoded_test = generator(tf.maximum(tf.minimum(encoder(x_vector, phase_train=False, reuse=False), 0.99), 0.01), phase_train=False, reuse=False)\n\n    # Round decoder output\n    decoded = threshold(decoded)\n    # Compute MSE Loss and L2 Loss\n    mse_loss = tf.reduce_mean(tf.pow(x_vector - decoded, 2))\n    para_ae = [var for var in tf.trainable_variables() if any(x in var.name for x in [\'wg\', \'wd\', \'wae\'])]\n    for var in tf.trainable_variables():\n        if \'wd5\' in var.name:\n            last_layer_dis = var\n    para_ae.remove(last_layer_dis)\n    # l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in para_ae])\n    # ae_loss = mse_loss + reg_l2 * l2_loss\n    ae_loss = mse_loss     \n\n    optimizer_ae = tf.train.AdamOptimizer(learning_rate=ae_lr,beta1=beta, name=""Adam_AE"").minimize(ae_loss)\n    # optimizer_ae = tf.train.RMSPropOptimizer(learning_rate=ae_lr, name=""RMS_AE"").minimize(ae_loss)\n\n\n    net_g_train = generator(z_vector, phase_train=True, reuse=False) \n\n    d_output_x = discriminator(x_vector, phase_train=True, reuse=False)\n    d_output_x = tf.maximum(tf.minimum(d_output_x, 0.99), 0.01)\n    summary_d_x_hist = tf.summary.histogram(""d_prob_x"", d_output_x)\n\n    d_output_z = discriminator(net_g_train, phase_train=True, reuse=True)\n    d_output_z = tf.maximum(tf.minimum(d_output_z, 0.99), 0.01)\n    summary_d_z_hist = tf.summary.histogram(""d_prob_z"", d_output_z)\n\n    # Compute the discriminator accuracy\n    n_p_x = tf.reduce_sum(tf.cast(d_output_x > 0.5, tf.int32))\n    n_p_z = tf.reduce_sum(tf.cast(d_output_z <= 0.5, tf.int32))\n    d_acc = tf.divide(n_p_x + n_p_z, 2 * batch_size)\n\n    # Compute the discriminator and generator loss\n    d_loss = -tf.reduce_mean(tf.log(d_output_x) + tf.log(1-d_output_z))\n    g_loss = -tf.reduce_mean(tf.log(d_output_z))\n    \n    summary_d_loss = tf.summary.scalar(""d_loss"", d_loss)\n    summary_g_loss = tf.summary.scalar(""g_loss"", g_loss)\n    summary_n_p_z = tf.summary.scalar(""n_p_z"", n_p_z)\n    summary_n_p_x = tf.summary.scalar(""n_p_x"", n_p_x)\n    summary_d_acc = tf.summary.scalar(""d_acc"", d_acc)\n\n    net_g_test = generator(z_vector, phase_train=False, reuse=True)\n\n    para_g = [var for var in tf.trainable_variables() if any(x in var.name for x in [\'wg\', \'bg\', \'gen\'])]\n    para_d = [var for var in tf.trainable_variables() if any(x in var.name for x in [\'wd\', \'bd\', \'dis\'])]\n\n    # only update the weights for the discriminator network\n    optimizer_op_d = tf.train.AdamOptimizer(learning_rate=d_lr,beta1=beta).minimize(d_loss,var_list=para_d)\n    # only update the weights for the generator network\n    optimizer_op_g = tf.train.AdamOptimizer(learning_rate=g_lr,beta1=beta).minimize(g_loss,var_list=para_g)\n\n    saver = tf.train.Saver(max_to_keep=50) \n\n    with tf.Session() as sess:  \n      \n        sess.run(tf.global_variables_initializer())        \n        z_sample = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n        if is_dummy:\n            volumes = np.random.randint(0,2,(batch_size,cube_len,cube_len,cube_len))\n            print \'Using Dummy Data\'\n        else:\n            volumes = d.getAll(obj=obj, train=True, is_local=is_local, obj_ratio=obj_ratio)\n            print \'Using \' + obj + \' Data\'\n        volumes = volumes[...,np.newaxis].astype(np.float) \n\n        for epoch in range(n_ae_epochs):\n            idx = np.random.randint(len(volumes), size=batch_size)\n            x = volumes[idx]\n\n            # Autoencoder pretraining\n            # ae_l, mse_l, l2_l, _ = sess.run([ae_loss, mse_loss, l2_loss, optimizer_ae],feed_dict={x_vector:x})\n            # print \'Autoencoder Training \', ""epoch: "",epoch, \'ae_loss:\', ae_l, \'mse_loss:\', mse_l, \'l2_loss:\', l2_l\n\n            ae_l, mse_l, _ = sess.run([ae_loss, mse_loss, optimizer_ae],feed_dict={x_vector:x})\n            print \'Autoencoder Training \', ""epoch: "",epoch, \'ae_loss:\', ae_l, \'mse_loss:\', mse_l\n\n            # output generated chairs\n            if epoch % ae_inter == 10:\n                idx = np.random.randint(len(volumes), size=batch_size)\n                x = volumes[idx]\n                decoded_chairs = sess.run(decoded_test, feed_dict={x_vector:x})\n                if not os.path.exists(train_sample_directory):\n                    os.makedirs(train_sample_directory)\n                decoded_chairs.dump(train_sample_directory+\'/ae_\' + exp_id +str(epoch))\n\n        for epoch in range(n_epochs):\n            \n            idx = np.random.randint(len(volumes), size=batch_size)\n            x = volumes[idx]\n            z = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n\n            # Update the discriminator and generator\n            d_summary_merge = tf.summary.merge([summary_d_loss,\n                                                summary_d_x_hist, \n                                                summary_d_z_hist,\n                                                summary_n_p_x,\n                                                summary_n_p_z,\n                                                summary_d_acc])\n\n            summary_d, discriminator_loss = sess.run([d_summary_merge,d_loss],feed_dict={z_vector:z, x_vector:x})\n            summary_g, generator_loss = sess.run([summary_g_loss,g_loss],feed_dict={z_vector:z})  \n            d_accuracy, n_x, n_z = sess.run([d_acc, n_p_x, n_p_z],feed_dict={z_vector:z, x_vector:x})\n            print n_x, n_z\n\n            if d_accuracy < d_thresh:\n                sess.run([optimizer_op_d],feed_dict={z_vector:z, x_vector:x})\n                print \'Discriminator Training \', ""epoch: "",epoch,\', d_loss:\',discriminator_loss,\'g_loss:\',generator_loss, ""d_acc: "", d_accuracy\n\n            sess.run([optimizer_op_g],feed_dict={z_vector:z})\n            print \'Generator Training \', ""epoch: "",epoch,\', d_loss:\',discriminator_loss,\'g_loss:\',generator_loss, ""d_acc: "", d_accuracy\n\n            # output generated chairs\n            if epoch % gan_inter == 10:\n                g_chairs = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n                if not os.path.exists(train_sample_directory):\n                    os.makedirs(train_sample_directory)\n                g_chairs.dump(train_sample_directory+\'/\'+str(epoch))\n            \n            if epoch % gan_inter == 10:\n                if not os.path.exists(model_directory):\n                    os.makedirs(model_directory)      \n                saver.save(sess, save_path = model_directory + \'/\' + str(epoch) + \'.cptk\')\n\nif __name__ == \'__main__\':\n    is_dummy = bool(int(sys.argv[1]))\n    exp_id = sys.argv[2]\n    trainGAN(is_dummy=is_dummy, exp_id=exp_id)\n'"
src/3dgan_feature_matching.py,99,"b'#!/usr/bin/env python\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nimport dataIO as d\n\nfrom tqdm import *\nfrom utils import *\nfrom operator import mul\n\n\'\'\'\nGlobal Parameters\n\'\'\'\nn_epochs   = 10000\nbatch_size = 32\ng_lr       = 0.0025\nd_lr       = 0.00001\nbeta       = 0.5\nd_thresh   = 0.8\nz_size     = 200\nleak_value = 0.2\ncube_len   = 64\nobj_ratio  = 0.5\nfeat_loss  = 1.0\nobj        = \'chair\' \n\ntrain_sample_directory = \'./train_sample/\'\nmodel_directory = \'./models/\'\nis_local = False\n\nweights, biases = {}, {}\n\ndef generator(z, batch_size=batch_size, phase_train=True, reuse=False):\n\n    strides    = [1,2,2,2,1]\n\n    with tf.variable_scope(""gen""):\n        z = tf.reshape(z, (batch_size, 1, 1, 1, z_size))\n        g_1 = tf.nn.conv3d_transpose(z, weights[\'wg1\'], (batch_size,4,4,4,512), strides=[1,1,1,1,1], padding=""VALID"")\n        g_1 = tf.nn.bias_add(g_1, biases[\'bg1\'])\n        g_1 = tf.contrib.layers.batch_norm(g_1, is_training=phase_train)\n        g_1 = tf.nn.relu(g_1)\n\n        g_2 = tf.nn.conv3d_transpose(g_1, weights[\'wg2\'], (batch_size,8,8,8,256), strides=strides, padding=""SAME"")\n        g_2 = tf.nn.bias_add(g_2, biases[\'bg2\'])\n        g_2 = tf.contrib.layers.batch_norm(g_2, is_training=phase_train)\n        g_2 = tf.nn.relu(g_2)\n\n        g_3 = tf.nn.conv3d_transpose(g_2, weights[\'wg3\'], (batch_size,16,16,16,128), strides=strides, padding=""SAME"")\n        g_3 = tf.nn.bias_add(g_3, biases[\'bg3\'])\n        g_3 = tf.contrib.layers.batch_norm(g_3, is_training=phase_train)\n        g_3 = tf.nn.relu(g_3)\n\n        g_4 = tf.nn.conv3d_transpose(g_3, weights[\'wg4\'], (batch_size,32,32,32,64), strides=strides, padding=""SAME"")\n        g_4 = tf.nn.bias_add(g_4, biases[\'bg4\'])\n        g_4 = tf.contrib.layers.batch_norm(g_4, is_training=phase_train)\n        g_4 = tf.nn.relu(g_4)\n        \n        g_5 = tf.nn.conv3d_transpose(g_4, weights[\'wg5\'], (batch_size,64,64,64,1), strides=strides, padding=""SAME"")\n        g_5 = tf.nn.bias_add(g_5, biases[\'bg5\'])\n        # g_5 = tf.nn.sigmoid(g_5)\n        g_5 = tf.nn.tanh(g_5)\n\n    print g_1, \'g1\'\n    print g_2, \'g2\'\n    print g_3, \'g3\'\n    print g_4, \'g4\'\n    print g_5, \'g5\'\n    \n    return g_5\n\n\ndef discriminator(inputs, phase_train=True, reuse=False):\n\n    strides    = [1,2,2,2,1]\n    with tf.variable_scope(""dis""):\n        d_1 = tf.nn.conv3d(inputs, weights[\'wd1\'], strides=strides, padding=""SAME"")\n        d_1 = tf.nn.bias_add(d_1, biases[\'bd1\'])\n        d_1 = tf.contrib.layers.batch_norm(d_1, is_training=phase_train)                               \n        d_1 = lrelu(d_1, leak_value)\n\n        d_2 = tf.nn.conv3d(d_1, weights[\'wd2\'], strides=strides, padding=""SAME"") \n        d_2 = tf.nn.bias_add(d_2, biases[\'bd2\'])\n        d_2 = tf.contrib.layers.batch_norm(d_2, is_training=phase_train)\n        d_2 = lrelu(d_2, leak_value)\n        \n        d_3 = tf.nn.conv3d(d_2, weights[\'wd3\'], strides=strides, padding=""SAME"")  \n        d_3 = tf.nn.bias_add(d_3, biases[\'bd3\'])\n        d_3 = tf.contrib.layers.batch_norm(d_3, is_training=phase_train)\n        d_3 = lrelu(d_3, leak_value) \n\n        d_4 = tf.nn.conv3d(d_3, weights[\'wd4\'], strides=strides, padding=""SAME"")     \n        d_4 = tf.nn.bias_add(d_4, biases[\'bd4\'])\n        d_4 = tf.contrib.layers.batch_norm(d_4, is_training=phase_train)\n        d_4 = lrelu(d_4)\n\n        d_5 = tf.nn.conv3d(d_4, weights[\'wd5\'], strides=[1,1,1,1,1], padding=""VALID"")     \n        d_5 = tf.nn.bias_add(d_5, biases[\'bd5\'])\n        shape_lst = list(d_5.get_shape()[1:])\n        flat_features = tf.reshape(d_5, [-1, reduce(mul, [int(l) for l in shape_lst], 1)])\n        d_5_no_sigmoid = d_5\n        d_5 = tf.nn.sigmoid(d_5)\n\n    print d_1, \'d1\'\n    print d_2, \'d2\'\n    print d_3, \'d3\'\n    print d_4, \'d4\'\n    print d_5, \'d5\'\n\n    return d_5, d_5_no_sigmoid, flat_features\n\ndef initialiseMITWeights(file_path=\'../../mit_weights.npy\'):\n    global weights\n\n    mit_weights = np.load(file_path)\n    layer_idx = [\'1\', \'4\', \'7\', \'10\', \'13\'] \n\n\n    weights[\'wg1\'] = tf.get_variable(""wg1"", initializer=tf.constant(mit_weights[()][\'1\'].transpose(2,3,4,1,0).astype(np.float32)))\n    weights[\'wg2\'] = tf.get_variable(""wg2"", initializer=tf.constant(mit_weights[()][\'4\'].transpose(2,3,4,1,0).astype(np.float32)))\n    weights[\'wg3\'] = tf.get_variable(""wg3"", initializer=tf.constant(mit_weights[()][\'7\'].transpose(2,3,4,1,0).astype(np.float32)))\n    weights[\'wg4\'] = tf.get_variable(""wg4"", initializer=tf.constant(mit_weights[()][\'10\'].transpose(2,3,4,1,0).astype(np.float32)))\n    weights[\'wg5\'] = tf.get_variable(""wg5"", initializer=tf.constant(mit_weights[()][\'13\'].transpose(2,3,4,1,0).astype(np.float32)))    \n\n    return weights\n\n\ndef initialiseWeights():\n\n    global weights\n    xavier_init = tf.contrib.layers.xavier_initializer()\n\n    weights[\'wg1\'] = tf.get_variable(""wg1"", shape=[4, 4, 4, 512, 200], initializer=xavier_init)\n    weights[\'wg2\'] = tf.get_variable(""wg2"", shape=[4, 4, 4, 256, 512], initializer=xavier_init)\n    weights[\'wg3\'] = tf.get_variable(""wg3"", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n    weights[\'wg4\'] = tf.get_variable(""wg4"", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n    weights[\'wg5\'] = tf.get_variable(""wg5"", shape=[4, 4, 4, 1, 64], initializer=xavier_init)    \n\n    weights[\'wd1\'] = tf.get_variable(""wd1"", shape=[4, 4, 4, 1, 64], initializer=xavier_init)\n    weights[\'wd2\'] = tf.get_variable(""wd2"", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n    weights[\'wd3\'] = tf.get_variable(""wd3"", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n    weights[\'wd4\'] = tf.get_variable(""wd4"", shape=[4, 4, 4, 256, 512], initializer=xavier_init)    \n    weights[\'wd5\'] = tf.get_variable(""wd5"", shape=[4, 4, 4, 512, 1], initializer=xavier_init)    \n\n    return weights\n\ndef initialiseBiases():\n    \n    global biases\n    zero_init = tf.zeros_initializer()\n\n    biases[\'bg1\'] = tf.get_variable(""bg1"", shape=[512], initializer=zero_init)\n    biases[\'bg2\'] = tf.get_variable(""bg2"", shape=[256], initializer=zero_init)\n    biases[\'bg3\'] = tf.get_variable(""bg3"", shape=[128], initializer=zero_init)\n    biases[\'bg4\'] = tf.get_variable(""bg4"", shape=[64], initializer=zero_init)\n    biases[\'bg5\'] = tf.get_variable(""bg5"", shape=[1], initializer=zero_init)\n\n    biases[\'bd1\'] = tf.get_variable(""bd1"", shape=[64], initializer=zero_init)\n    biases[\'bd2\'] = tf.get_variable(""bd2"", shape=[128], initializer=zero_init)\n    biases[\'bd3\'] = tf.get_variable(""bd3"", shape=[256], initializer=zero_init)\n    biases[\'bd4\'] = tf.get_variable(""bd4"", shape=[512], initializer=zero_init)    \n    biases[\'bd5\'] = tf.get_variable(""bd5"", shape=[1], initializer=zero_init) \n\n    return biases\n\n\ndef trainGAN(is_dummy=False):\n\n    weights =  initialiseWeights()\n    biases = initialiseBiases()\n\n    z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n    x_vector = tf.placeholder(shape=[batch_size,cube_len,cube_len,cube_len,1],dtype=tf.float32)\n\n    net_g_train = generator(z_vector, phase_train=True, reuse=False) \n\n    d_output_x, d_no_sigmoid_output_x, flat_feature_x = discriminator(x_vector, phase_train=True, reuse=False)\n    d_output_x = tf.maximum(tf.minimum(d_output_x, 0.99), 0.01)\n    summary_d_x_hist = tf.summary.histogram(""d_prob_x"", d_output_x)\n\n    d_output_z, d_no_sigmoid_output_z, flat_feature_z = discriminator(net_g_train, phase_train=True, reuse=True)\n    d_output_z = tf.maximum(tf.minimum(d_output_z, 0.99), 0.01)\n    summary_d_z_hist = tf.summary.histogram(""d_prob_z"", d_output_z)\n\n    # Compute the discriminator accuracy\n    n_p_x = tf.reduce_sum(tf.cast(d_output_x > 0.5, tf.int32))\n    n_p_z = tf.reduce_sum(tf.cast(d_output_z < 0.5, tf.int32))\n    d_acc = tf.divide(n_p_x + n_p_z, 2 * batch_size)\n\n    # Compute the discriminator and generator loss\n    # d_loss = -tf.reduce_mean(tf.log(d_output_x) + tf.log(1-d_output_z))\n    # g_loss = -tf.reduce_mean(tf.log(d_output_z))\n\n    d_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_x, labels=tf.ones_like(d_output_x)) + tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_z, labels=tf.zeros_like(d_output_z))\n    g_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_z, labels=tf.ones_like(d_output_z))\n    \n    d_loss = tf.reduce_mean(d_loss)\n    g_loss = tf.reduce_mean(g_loss)\n\n    # Feature Matching loss\n    feature_loss = tf.reduce_mean(abs(tf.reduce_mean(flat_feature_z, 0) - tf.reduce_mean(flat_feature_x, 0)))\n    g_loss = (1-feat_loss) * g_loss + feat_loss * feature_loss\n\n    summary_d_loss = tf.summary.scalar(""d_loss"", d_loss)\n    summary_g_loss = tf.summary.scalar(""g_loss"", g_loss)\n    summary_n_p_z = tf.summary.scalar(""n_p_z"", n_p_z)\n    summary_n_p_x = tf.summary.scalar(""n_p_x"", n_p_x)\n    summary_d_acc = tf.summary.scalar(""d_acc"", d_acc)\n\n    net_g_test = generator(z_vector, phase_train=False, reuse=True)\n\n    para_g = [var for var in tf.trainable_variables() if any(x in var.name for x in [\'wg\', \'bg\', \'gen\'])]\n    para_d = [var for var in tf.trainable_variables() if any(x in var.name for x in [\'wd\', \'bd\', \'dis\'])]\n\n    # only update the weights for the discriminator network\n    optimizer_op_d = tf.train.AdamOptimizer(learning_rate=d_lr,beta1=beta).minimize(d_loss,var_list=para_d)\n    # only update the weights for the generator network\n    optimizer_op_g = tf.train.AdamOptimizer(learning_rate=g_lr,beta1=beta).minimize(g_loss,var_list=para_g)\n\n    saver = tf.train.Saver(max_to_keep=50) \n\n    with tf.Session() as sess:  \n      \n        sess.run(tf.global_variables_initializer())       \n        z_sample = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n        if is_dummy:\n            volumes = np.random.randint(0,2,(batch_size,cube_len,cube_len,cube_len))\n            print \'Using Dummy Data\'\n        else:\n            volumes = d.getAll(obj=obj, train=True, is_local=is_local, obj_ratio=obj_ratio)\n            print \'Using \' + obj + \' Data\'\n        volumes = volumes[...,np.newaxis].astype(np.float)\n        # volumes *= 2.0\n        # volumes -= 1.0\n\n        for epoch in range(n_epochs):\n            \n            idx = np.random.randint(len(volumes), size=batch_size)\n            x = volumes[idx]\n            # z = np.random.normal(0, 1, size=[batch_size, z_size]).astype(np.float32)\n            z = np.random.uniform(0, 1, size=[batch_size, z_size]).astype(np.float32)\n\n            # Update the discriminator and generator\n            d_summary_merge = tf.summary.merge([summary_d_loss,\n                                                summary_d_x_hist, \n                                                summary_d_z_hist,\n                                                summary_n_p_x,\n                                                summary_n_p_z,\n                                                summary_d_acc])\n\n            summary_d, discriminator_loss = sess.run([d_summary_merge,d_loss],feed_dict={z_vector:z, x_vector:x})\n            summary_g, generator_loss = sess.run([summary_g_loss,g_loss],feed_dict={z_vector:z, x_vector:x}) \n            d_accuracy, n_x, n_z = sess.run([d_acc, n_p_x, n_p_z],feed_dict={z_vector:z, x_vector:x})\n            print n_x, n_z\n\n            if d_accuracy < d_thresh:\n                sess.run([optimizer_op_d],feed_dict={z_vector:z, x_vector:x})\n                print \'Discriminator Training \', ""epoch: "",epoch,\', d_loss:\',discriminator_loss,\'g_loss:\',generator_loss, ""d_acc: "", d_accuracy\n\n            sess.run([optimizer_op_g],feed_dict={z_vector:z, x_vector:x})\n            print \'Generator Training \', ""epoch: "",epoch,\', d_loss:\',discriminator_loss,\'g_loss:\',generator_loss, ""d_acc: "", d_accuracy\n\n            # output generated chairs\n            if epoch % 50 == 10:\n                g_chairs = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n                if not os.path.exists(train_sample_directory):\n                    os.makedirs(train_sample_directory)\n                g_chairs.dump(train_sample_directory+\'/biasfree_\'+str(epoch))\n            \n            if epoch % 50 == 10:\n                if not os.path.exists(model_directory):\n                    os.makedirs(model_directory)      \n                saver.save(sess, save_path = model_directory + \'/biasfree_\' + str(epoch) + \'.cptk\')\n\ndef testGAN():\n\n    weights =  initialiseMITWeights()\n\n    z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n    x_vector = tf.placeholder(shape=[batch_size,cube_len,cube_len,cube_len,1],dtype=tf.float32) \n\n    net_g_test = generator(z_vector, phase_train=False, reuse=True)\n\n    with tf.Session() as sess:\n      \n        sess.run(tf.global_variables_initializer())       \n        z_sample = np.random.uniform(0, 1, size=[batch_size, z_size]).astype(np.float32)\n\n        g_chairs = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n        if not os.path.exists(train_sample_directory):\n            os.makedirs(train_sample_directory)\n        g_chairs.dump(train_sample_directory+\'/biasfree_trained_\' + str(batch_size))\n\nif __name__ == \'__main__\':\n    is_dummy = bool(int(sys.argv[1]))\n    trainGAN(is_dummy=is_dummy)\n    # testGAN()\n'"
src/3dgan_mit.py,83,"b'#!/usr/bin/env python\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nimport dataIO as d\n\nfrom tqdm import *\nfrom utils import *\n\n\'\'\'\nGlobal Parameters\n\'\'\'\nn_epochs   = 10000\nbatch_size = 50\ng_lr       = 0.008\nd_lr       = 0.000001\nbeta       = 0.5\nd_thresh   = 0.8 \nz_size     = 200\nleak_value = 0.2\ncube_len   = 64\nobj_ratio  = 0.5\nobj        = \'chair\' \n\ntrain_sample_directory = \'./train_sample/\'\nmodel_directory = \'./models/\'\nis_local = False\n\nweights, biases = {}, {}\n\ndef generator(z, batch_size=batch_size, phase_train=True, reuse=False):\n\n    strides    = [1,2,2,2,1]\n\n    with tf.variable_scope(""gen""):\n        z = tf.reshape(z, (batch_size, 1, 1, 1, z_size))\n        g_1 = tf.nn.conv3d_transpose(z, weights[\'wg1\'], (batch_size,4,4,4,512), strides=[1,1,1,1,1], padding=""VALID"")\n        g_1 = tf.nn.bias_add(g_1, biases[\'bg1\'])                                  \n        g_1 = tf.contrib.layers.batch_norm(g_1, is_training=phase_train)\n        g_1 = tf.nn.relu(g_1)\n\n        g_2 = tf.nn.conv3d_transpose(g_1, weights[\'wg2\'], (batch_size,8,8,8,256), strides=strides, padding=""SAME"")\n        g_2 = tf.nn.bias_add(g_2, biases[\'bg2\'])\n        g_2 = tf.contrib.layers.batch_norm(g_2, is_training=phase_train)\n        g_2 = tf.nn.relu(g_2)\n\n        g_3 = tf.nn.conv3d_transpose(g_2, weights[\'wg3\'], (batch_size,16,16,16,128), strides=strides, padding=""SAME"")\n        g_3 = tf.nn.bias_add(g_3, biases[\'bg3\'])\n        g_3 = tf.contrib.layers.batch_norm(g_3, is_training=phase_train)\n        g_3 = tf.nn.relu(g_3)\n\n        g_4 = tf.nn.conv3d_transpose(g_3, weights[\'wg4\'], (batch_size,32,32,32,64), strides=strides, padding=""SAME"")\n        g_4 = tf.nn.bias_add(g_4, biases[\'bg4\'])\n        g_4 = tf.contrib.layers.batch_norm(g_4, is_training=phase_train)\n        g_4 = tf.nn.relu(g_4)\n        \n        g_5 = tf.nn.conv3d_transpose(g_4, weights[\'wg5\'], (batch_size,64,64,64,1), strides=strides, padding=""SAME"")\n        g_5 = tf.nn.bias_add(g_5, biases[\'bg5\'])\n        g_5 = tf.nn.sigmoid(g_5)\n\n    print g_1, \'g1\'\n    print g_2, \'g2\'\n    print g_3, \'g3\'\n    print g_4, \'g4\'\n    print g_5, \'g5\'\n    \n    return g_5\n\n\ndef discriminator(inputs, phase_train=True, reuse=False):\n\n    strides    = [1,2,2,2,1]\n    with tf.variable_scope(""dis""):\n        d_1 = tf.nn.conv3d(inputs, weights[\'wd1\'], strides=strides, padding=""SAME"")\n        d_1 = tf.nn.bias_add(d_1, biases[\'bd1\'])\n        d_1 = tf.contrib.layers.batch_norm(d_1, is_training=phase_train)                               \n        d_1 = lrelu(d_1, leak_value)\n\n        d_2 = tf.nn.conv3d(d_1, weights[\'wd2\'], strides=strides, padding=""SAME"") \n        d_2 = tf.nn.bias_add(d_2, biases[\'bd2\'])\n        d_2 = tf.contrib.layers.batch_norm(d_2, is_training=phase_train)\n        d_2 = lrelu(d_2, leak_value)\n        \n        d_3 = tf.nn.conv3d(d_2, weights[\'wd3\'], strides=strides, padding=""SAME"")  \n        d_3 = tf.nn.bias_add(d_3, biases[\'bd3\'])\n        d_3 = tf.contrib.layers.batch_norm(d_3, is_training=phase_train)\n        d_3 = lrelu(d_3, leak_value) \n\n        d_4 = tf.nn.conv3d(d_3, weights[\'wd4\'], strides=strides, padding=""SAME"")     \n        d_4 = tf.nn.bias_add(d_4, biases[\'bd4\'])\n        d_4 = tf.contrib.layers.batch_norm(d_4, is_training=phase_train)\n        d_4 = lrelu(d_4)\n\n        d_5 = tf.nn.conv3d(d_4, weights[\'wd5\'], strides=[1,1,1,1,1], padding=""VALID"")     \n        d_5 = tf.nn.bias_add(d_5, biases[\'bd5\'])\n        d_5 = tf.nn.sigmoid(d_5)\n\n    print d_1, \'d1\'\n    print d_2, \'d2\'\n    print d_3, \'d3\'\n    print d_4, \'d4\'\n    print d_5, \'d5\'\n\n    return d_5\n\ndef initialiseWeights():\n\n    global weights\n    xavier_init = tf.contrib.layers.xavier_initializer()\n\n    weights[\'wg1\'] = tf.get_variable(""wg1"", shape=[4, 4, 4, 512, 200], initializer=xavier_init)\n    weights[\'wg2\'] = tf.get_variable(""wg2"", shape=[4, 4, 4, 256, 512], initializer=xavier_init)\n    weights[\'wg3\'] = tf.get_variable(""wg3"", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n    weights[\'wg4\'] = tf.get_variable(""wg4"", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n    weights[\'wg5\'] = tf.get_variable(""wg5"", shape=[4, 4, 4, 1, 64], initializer=xavier_init)    \n\n    weights[\'wd1\'] = tf.get_variable(""wd1"", shape=[4, 4, 4, 1, 64], initializer=xavier_init)\n    weights[\'wd2\'] = tf.get_variable(""wd2"", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n    weights[\'wd3\'] = tf.get_variable(""wd3"", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n    weights[\'wd4\'] = tf.get_variable(""wd4"", shape=[4, 4, 4, 256, 512], initializer=xavier_init)    \n    weights[\'wd5\'] = tf.get_variable(""wd5"", shape=[4, 4, 4, 512, 1], initializer=xavier_init)    \n\n    return weights\n\ndef initialiseBiases():\n    \n    global biases\n    zero_init = tf.zeros_initializer()\n\n    biases[\'bg1\'] = tf.get_variable(""bg1"", shape=[512], initializer=zero_init)\n    biases[\'bg2\'] = tf.get_variable(""bg2"", shape=[256], initializer=zero_init)\n    biases[\'bg3\'] = tf.get_variable(""bg3"", shape=[128], initializer=zero_init)\n    biases[\'bg4\'] = tf.get_variable(""bg4"", shape=[64], initializer=zero_init)\n    biases[\'bg5\'] = tf.get_variable(""bg5"", shape=[1], initializer=zero_init)\n\n    biases[\'bd1\'] = tf.get_variable(""bd1"", shape=[64], initializer=zero_init)\n    biases[\'bd2\'] = tf.get_variable(""bd2"", shape=[128], initializer=zero_init)\n    biases[\'bd3\'] = tf.get_variable(""bd3"", shape=[256], initializer=zero_init)\n    biases[\'bd4\'] = tf.get_variable(""bd4"", shape=[512], initializer=zero_init)    \n    biases[\'bd5\'] = tf.get_variable(""bd5"", shape=[1], initializer=zero_init) \n\n    return biases\n\ndef trainGAN(is_dummy=False):\n\n    weights, biases =  initialiseWeights(), initialiseBiases()\n\n    z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n    x_vector = tf.placeholder(shape=[batch_size,cube_len,cube_len,cube_len,1],dtype=tf.float32) \n\n    net_g_train = generator(z_vector, phase_train=True, reuse=False) \n\n    d_output_x = discriminator(x_vector, phase_train=True, reuse=False)\n    d_output_x = tf.maximum(tf.minimum(d_output_x, 0.99), 0.01)\n    summary_d_x_hist = tf.summary.histogram(""d_prob_x"", d_output_x)\n\n    d_output_z = discriminator(net_g_train, phase_train=True, reuse=True)\n    d_output_z = tf.maximum(tf.minimum(d_output_z, 0.99), 0.01)\n    summary_d_z_hist = tf.summary.histogram(""d_prob_z"", d_output_z)\n\n    # Compute the discriminator accuracy\n    n_p_x = tf.reduce_sum(tf.cast(d_output_x > 0.5, tf.int32))\n    n_p_z = tf.reduce_sum(tf.cast(d_output_z <= 0.5, tf.int32))\n    d_acc = tf.divide(n_p_x + n_p_z, 2 * batch_size)\n\n    # Compute the discriminator and generator loss\n    d_loss = -tf.reduce_mean(tf.log(d_output_x) + tf.log(1-d_output_z))\n    g_loss = -tf.reduce_mean(tf.log(d_output_z))\n    \n    summary_d_loss = tf.summary.scalar(""d_loss"", d_loss)\n    summary_g_loss = tf.summary.scalar(""g_loss"", g_loss)\n    summary_n_p_z = tf.summary.scalar(""n_p_z"", n_p_z)\n    summary_n_p_x = tf.summary.scalar(""n_p_x"", n_p_x)\n    summary_d_acc = tf.summary.scalar(""d_acc"", d_acc)\n\n    net_g_test = generator(z_vector, phase_train=False, reuse=True)\n\n    para_g = [var for var in tf.trainable_variables() if any(x in var.name for x in [\'wg\', \'bg\', \'gen\'])]\n    para_d = [var for var in tf.trainable_variables() if any(x in var.name for x in [\'wd\', \'bd\', \'dis\'])]\n\n    # only update the weights for the discriminator network\n    optimizer_op_d = tf.train.AdamOptimizer(learning_rate=d_lr,beta1=beta).minimize(d_loss,var_list=para_d)\n    # only update the weights for the generator network\n    optimizer_op_g = tf.train.AdamOptimizer(learning_rate=g_lr,beta1=beta).minimize(g_loss,var_list=para_g)\n\n    saver = tf.train.Saver(max_to_keep=50) \n\n    with tf.Session() as sess:  \n      \n        sess.run(tf.global_variables_initializer())        \n        z_sample = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n        if is_dummy:\n            volumes = np.random.randint(0,2,(batch_size,cube_len,cube_len,cube_len))\n            print \'Using Dummy Data\'\n        else:\n            volumes = d.getAll(obj=obj, train=True, is_local=is_local, obj_ratio=obj_ratio)\n            print \'Using \' + obj + \' Data\'\n        volumes = volumes[...,np.newaxis].astype(np.float) \n\n        for epoch in range(n_epochs):\n            \n            idx = np.random.randint(len(volumes), size=batch_size)\n            x = volumes[idx]\n            z = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n\n            # Update the discriminator and generator\n            d_summary_merge = tf.summary.merge([summary_d_loss,\n                                                summary_d_x_hist, \n                                                summary_d_z_hist,\n                                                summary_n_p_x,\n                                                summary_n_p_z,\n                                                summary_d_acc])\n\n            summary_d, discriminator_loss = sess.run([d_summary_merge,d_loss],feed_dict={z_vector:z, x_vector:x})\n            summary_g, generator_loss = sess.run([summary_g_loss,g_loss],feed_dict={z_vector:z})  \n            d_accuracy, n_x, n_z = sess.run([d_acc, n_p_x, n_p_z],feed_dict={z_vector:z, x_vector:x})\n            print n_x, n_z\n\n            if d_accuracy < d_thresh:\n                sess.run([optimizer_op_d],feed_dict={z_vector:z, x_vector:x})\n                print \'Discriminator Training \', ""epoch: "",epoch,\', d_loss:\',discriminator_loss,\'g_loss:\',generator_loss, ""d_acc: "", d_accuracy\n\n            sess.run([optimizer_op_g],feed_dict={z_vector:z})\n            print \'Generator Training \', ""epoch: "",epoch,\', d_loss:\',discriminator_loss,\'g_loss:\',generator_loss, ""d_acc: "", d_accuracy\n\n            # output generated chairs\n            if epoch % 500 == 10:\n                g_chairs = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n                if not os.path.exists(train_sample_directory):\n                    os.makedirs(train_sample_directory)\n                g_chairs.dump(train_sample_directory+\'/\'+str(epoch))\n            \n            if epoch % 500 == 10:\n                if not os.path.exists(model_directory):\n                    os.makedirs(model_directory)      \n                saver.save(sess, save_path = model_directory + \'/\' + str(epoch) + \'.cptk\')\n\nif __name__ == \'__main__\':\n    is_dummy = bool(int(sys.argv[1]))\n    trainGAN(is_dummy=is_dummy)\n'"
src/3dgan_mit_biasfree.py,72,"b'#!/usr/bin/env python\nimport os\nimport sys\nimport visdom\n\nimport numpy as np\nimport tensorflow as tf\nimport dataIO as d\n\nfrom tqdm import *\nfrom utils import *\n\n\'\'\'\nGlobal Parameters\n\'\'\'\nn_epochs   = 10000\nbatch_size = 32\ng_lr       = 0.0025\nd_lr       = 0.00001\nbeta       = 0.5\nd_thresh   = 0.8\nz_size     = 200\nleak_value = 0.2\ncube_len   = 64\nobj_ratio  = 0.7\nobj        = \'chair\' \n\ntrain_sample_directory = \'./train_sample/\'\nmodel_directory = \'./models/\'\nis_local = False\n\nweights = {}\n\ndef generator(z, batch_size=batch_size, phase_train=True, reuse=False):\n\n    strides    = [1,2,2,2,1]\n\n    with tf.variable_scope(""gen"", reuse=reuse):\n        z = tf.reshape(z, (batch_size, 1, 1, 1, z_size))\n        g_1 = tf.nn.conv3d_transpose(z, weights[\'wg1\'], (batch_size,4,4,4,512), strides=[1,1,1,1,1], padding=""VALID"")\n        g_1 = tf.contrib.layers.batch_norm(g_1, is_training=phase_train)\n        g_1 = tf.nn.relu(g_1)\n\n        g_2 = tf.nn.conv3d_transpose(g_1, weights[\'wg2\'], (batch_size,8,8,8,256), strides=strides, padding=""SAME"")\n        g_2 = tf.contrib.layers.batch_norm(g_2, is_training=phase_train)\n        g_2 = tf.nn.relu(g_2)\n\n        g_3 = tf.nn.conv3d_transpose(g_2, weights[\'wg3\'], (batch_size,16,16,16,128), strides=strides, padding=""SAME"")\n        g_3 = tf.contrib.layers.batch_norm(g_3, is_training=phase_train)\n        g_3 = tf.nn.relu(g_3)\n\n        g_4 = tf.nn.conv3d_transpose(g_3, weights[\'wg4\'], (batch_size,32,32,32,64), strides=strides, padding=""SAME"")\n        g_4 = tf.contrib.layers.batch_norm(g_4, is_training=phase_train)\n        g_4 = tf.nn.relu(g_4)\n        \n        g_5 = tf.nn.conv3d_transpose(g_4, weights[\'wg5\'], (batch_size,64,64,64,1), strides=strides, padding=""SAME"")\n        # g_5 = tf.nn.sigmoid(g_5)\n        g_5 = tf.nn.tanh(g_5)\n\n    print g_1, \'g1\'\n    print g_2, \'g2\'\n    print g_3, \'g3\'\n    print g_4, \'g4\'\n    print g_5, \'g5\'\n    \n    return g_5\n\n\ndef discriminator(inputs, phase_train=True, reuse=False):\n\n    strides    = [1,2,2,2,1]\n    with tf.variable_scope(""dis"", reuse=reuse):\n        d_1 = tf.nn.conv3d(inputs, weights[\'wd1\'], strides=strides, padding=""SAME"")\n        d_1 = tf.contrib.layers.batch_norm(d_1, is_training=phase_train)                               \n        d_1 = lrelu(d_1, leak_value)\n\n        d_2 = tf.nn.conv3d(d_1, weights[\'wd2\'], strides=strides, padding=""SAME"") \n        d_2 = tf.contrib.layers.batch_norm(d_2, is_training=phase_train)\n        d_2 = lrelu(d_2, leak_value)\n        \n        d_3 = tf.nn.conv3d(d_2, weights[\'wd3\'], strides=strides, padding=""SAME"")  \n        d_3 = tf.contrib.layers.batch_norm(d_3, is_training=phase_train)\n        d_3 = lrelu(d_3, leak_value) \n\n        d_4 = tf.nn.conv3d(d_3, weights[\'wd4\'], strides=strides, padding=""SAME"")     \n        d_4 = tf.contrib.layers.batch_norm(d_4, is_training=phase_train)\n        d_4 = lrelu(d_4)\n\n        d_5 = tf.nn.conv3d(d_4, weights[\'wd5\'], strides=[1,1,1,1,1], padding=""VALID"")     \n        d_5_no_sigmoid = d_5\n        d_5 = tf.nn.sigmoid(d_5)\n\n    print d_1, \'d1\'\n    print d_2, \'d2\'\n    print d_3, \'d3\'\n    print d_4, \'d4\'\n    print d_5, \'d5\'\n\n    return d_5, d_5_no_sigmoid\n\ndef initialiseWeights():\n\n    global weights\n    xavier_init = tf.contrib.layers.xavier_initializer()\n\n    weights[\'wg1\'] = tf.get_variable(""wg1"", shape=[4, 4, 4, 512, 200], initializer=xavier_init)\n    weights[\'wg2\'] = tf.get_variable(""wg2"", shape=[4, 4, 4, 256, 512], initializer=xavier_init)\n    weights[\'wg3\'] = tf.get_variable(""wg3"", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n    weights[\'wg4\'] = tf.get_variable(""wg4"", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n    weights[\'wg5\'] = tf.get_variable(""wg5"", shape=[4, 4, 4, 1, 64], initializer=xavier_init)    \n\n    weights[\'wd1\'] = tf.get_variable(""wd1"", shape=[4, 4, 4, 1, 64], initializer=xavier_init)\n    weights[\'wd2\'] = tf.get_variable(""wd2"", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n    weights[\'wd3\'] = tf.get_variable(""wd3"", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n    weights[\'wd4\'] = tf.get_variable(""wd4"", shape=[4, 4, 4, 256, 512], initializer=xavier_init)    \n    weights[\'wd5\'] = tf.get_variable(""wd5"", shape=[4, 4, 4, 512, 1], initializer=xavier_init)    \n\n    return weights\n\n\ndef trainGAN(is_dummy=False, checkpoint=None):\n\n    weights =  initialiseWeights()\n\n    z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n    x_vector = tf.placeholder(shape=[batch_size,cube_len,cube_len,cube_len,1],dtype=tf.float32) \n\n    net_g_train = generator(z_vector, phase_train=True, reuse=False) \n\n    d_output_x, d_no_sigmoid_output_x = discriminator(x_vector, phase_train=True, reuse=False)\n    d_output_x = tf.maximum(tf.minimum(d_output_x, 0.99), 0.01)\n    summary_d_x_hist = tf.summary.histogram(""d_prob_x"", d_output_x)\n\n    d_output_z, d_no_sigmoid_output_z = discriminator(net_g_train, phase_train=True, reuse=True)\n    d_output_z = tf.maximum(tf.minimum(d_output_z, 0.99), 0.01)\n    summary_d_z_hist = tf.summary.histogram(""d_prob_z"", d_output_z)\n\n    # Compute the discriminator accuracy\n    n_p_x = tf.reduce_sum(tf.cast(d_output_x > 0.5, tf.int32))\n    n_p_z = tf.reduce_sum(tf.cast(d_output_z < 0.5, tf.int32))\n    d_acc = tf.divide(n_p_x + n_p_z, 2 * batch_size)\n\n    # Compute the discriminator and generator loss\n    # d_loss = -tf.reduce_mean(tf.log(d_output_x) + tf.log(1-d_output_z))\n    # g_loss = -tf.reduce_mean(tf.log(d_output_z))\n\n    d_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_x, labels=tf.ones_like(d_output_x)) + tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_z, labels=tf.zeros_like(d_output_z))\n    g_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_z, labels=tf.ones_like(d_output_z))\n    \n    d_loss = tf.reduce_mean(d_loss)\n    g_loss = tf.reduce_mean(g_loss)\n\n    summary_d_loss = tf.summary.scalar(""d_loss"", d_loss)\n    summary_g_loss = tf.summary.scalar(""g_loss"", g_loss)\n    summary_n_p_z = tf.summary.scalar(""n_p_z"", n_p_z)\n    summary_n_p_x = tf.summary.scalar(""n_p_x"", n_p_x)\n    summary_d_acc = tf.summary.scalar(""d_acc"", d_acc)\n\n    net_g_test = generator(z_vector, phase_train=False, reuse=True)\n\n    para_g = [var for var in tf.trainable_variables() if any(x in var.name for x in [\'wg\', \'bg\', \'gen\'])]\n    para_d = [var for var in tf.trainable_variables() if any(x in var.name for x in [\'wd\', \'bd\', \'dis\'])]\n\n    # only update the weights for the discriminator network\n    optimizer_op_d = tf.train.AdamOptimizer(learning_rate=d_lr,beta1=beta).minimize(d_loss,var_list=para_d)\n    # only update the weights for the generator network\n    optimizer_op_g = tf.train.AdamOptimizer(learning_rate=g_lr,beta1=beta).minimize(g_loss,var_list=para_g)\n\n    saver = tf.train.Saver() \n    vis = visdom.Visdom()\n\n\n    with tf.Session() as sess:  \n      \n        sess.run(tf.global_variables_initializer())        \n        if checkpoint is not None:\n            saver.restore(sess, checkpoint)        \n\n        if is_dummy:\n            volumes = np.random.randint(0,2,(batch_size,cube_len,cube_len,cube_len))\n            print \'Using Dummy Data\'\n        else:\n            volumes = d.getAll(obj=obj, train=True, is_local=is_local, obj_ratio=obj_ratio)\n            print \'Using \' + obj + \' Data\'\n        volumes = volumes[...,np.newaxis].astype(np.float)\n        # volumes *= 2.0\n        # volumes -= 1.0\n\n        for epoch in range(n_epochs):\n            \n            idx = np.random.randint(len(volumes), size=batch_size)\n            x = volumes[idx]\n            z_sample = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n            z = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n            # z = np.random.uniform(0, 1, size=[batch_size, z_size]).astype(np.float32)\n\n            # Update the discriminator and generator\n            d_summary_merge = tf.summary.merge([summary_d_loss,\n                                                summary_d_x_hist, \n                                                summary_d_z_hist,\n                                                summary_n_p_x,\n                                                summary_n_p_z,\n                                                summary_d_acc])\n\n            summary_d, discriminator_loss = sess.run([d_summary_merge,d_loss],feed_dict={z_vector:z, x_vector:x})\n            summary_g, generator_loss = sess.run([summary_g_loss,g_loss],feed_dict={z_vector:z})  \n            d_accuracy, n_x, n_z = sess.run([d_acc, n_p_x, n_p_z],feed_dict={z_vector:z, x_vector:x})\n            print n_x, n_z\n\n            if d_accuracy < d_thresh:\n                sess.run([optimizer_op_d],feed_dict={z_vector:z, x_vector:x})\n                print \'Discriminator Training \', ""epoch: "",epoch,\', d_loss:\',discriminator_loss,\'g_loss:\',generator_loss, ""d_acc: "", d_accuracy\n\n            sess.run([optimizer_op_g],feed_dict={z_vector:z})\n            print \'Generator Training \', ""epoch: "",epoch,\', d_loss:\',discriminator_loss,\'g_loss:\',generator_loss, ""d_acc: "", d_accuracy\n\n            # output generated chairs\n            if epoch % 200 == 0:\n                g_objects = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n                if not os.path.exists(train_sample_directory):\n                    os.makedirs(train_sample_directory)\n                g_objects.dump(train_sample_directory+\'/biasfree_\'+str(epoch))\n                id_ch = np.random.randint(0, batch_size, 4)\n                for i in range(4):\n                    if g_objects[id_ch[i]].max() > 0.5:\n    \t\t            d.plotVoxelVisdom(np.squeeze(g_objects[id_ch[i]]>0.5), vis, \'_\'.join(map(str,[epoch,i])))          \n            if epoch % 50 == 10:\n                if not os.path.exists(model_directory):\n                    os.makedirs(model_directory)      \n                saver.save(sess, save_path = model_directory + \'/biasfree_\' + str(epoch) + \'.cptk\')\n\n\ndef testGAN(trained_model_path=None, n_batches=40):\n\n    weights = initialiseWeights()\n\n    z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n    net_g_test = generator(z_vector, phase_train=True, reuse=True)\n\n    vis = visdom.Visdom()\n\n    sess = tf.Session()\n    saver = tf.train.Saver()\n    \n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        saver.restore(sess, trained_model_path) \n\n        # output generated chairs\n        for i in range(n_batches):\n            next_sigma = float(raw_input())\n            z_sample = np.random.normal(0, next_sigma, size=[batch_size, z_size]).astype(np.float32)\n            g_objects = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n            id_ch = np.random.randint(0, batch_size, 4)\n            for i in range(4):\n                print g_objects[id_ch[i]].max(), g_objects[id_ch[i]].min(), g_objects[id_ch[i]].shape\n                if g_objects[id_ch[i]].max() > 0.5:\n                    d.plotVoxelVisdom(np.squeeze(g_objects[id_ch[i]]>0.5), vis, \'_\'.join(map(str,[i])))\n\nif __name__ == \'__main__\':\n    test = bool(int(sys.argv[1]))\n    if test:\n        path = sys.argv[2]\n        testGAN(trained_model_path=path)\n    else:\n        ckpt = sys.argv[2]\n        if ckpt == \'0\':\n            trainGAN(is_dummy=False, checkpoint=None)\n        else:\n            trainGAN(is_dummy=False, checkpoint=ckpt)\n\n'"
src/3dgan_test.py,76,"b'#!/usr/bin/env python\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nimport dataIO as d\n\nfrom tqdm import *\nfrom utils import *\n\n\'\'\'\nGlobal Parameters\n\'\'\'\nn_epochs   = 10000\nbatch_size = 32\ng_lr       = 0.0025\nd_lr       = 0.00001\nbeta       = 0.5\nd_thresh   = 0.8\nz_size     = 200\nleak_value = 0.2\ncube_len   = 64\nobj_ratio  = 0.5\nobj        = \'chair\' \n\ntrain_sample_directory = \'./train_sample/\'\nmodel_directory = \'./models/\'\nis_local = False\n\nweights = {}\n\ndef generator(z, batch_size=batch_size, phase_train=True, reuse=False):\n\n    strides    = [1,2,2,2,1]\n\n    with tf.variable_scope(""gen""):\n        z = tf.reshape(z, (batch_size, 1, 1, 1, z_size))\n        g_1 = tf.nn.conv3d_transpose(z, weights[\'wg1\'], (batch_size,4,4,4,512), strides=[1,1,1,1,1], padding=""VALID"")\n        g_1 = tf.contrib.layers.batch_norm(g_1, is_training=phase_train)\n        g_1 = tf.nn.relu(g_1)\n\n        g_2 = tf.nn.conv3d_transpose(g_1, weights[\'wg2\'], (batch_size,8,8,8,256), strides=strides, padding=""SAME"")\n        g_2 = tf.contrib.layers.batch_norm(g_2, is_training=phase_train)\n        g_2 = tf.nn.relu(g_2)\n\n        g_3 = tf.nn.conv3d_transpose(g_2, weights[\'wg3\'], (batch_size,16,16,16,128), strides=strides, padding=""SAME"")\n        g_3 = tf.contrib.layers.batch_norm(g_3, is_training=phase_train)\n        g_3 = tf.nn.relu(g_3)\n\n        g_4 = tf.nn.conv3d_transpose(g_3, weights[\'wg4\'], (batch_size,32,32,32,64), strides=strides, padding=""SAME"")\n        g_4 = tf.contrib.layers.batch_norm(g_4, is_training=phase_train)\n        g_4 = tf.nn.relu(g_4)\n        \n        g_5 = tf.nn.conv3d_transpose(g_4, weights[\'wg5\'], (batch_size,64,64,64,1), strides=strides, padding=""SAME"")\n        g_5 = tf.nn.sigmoid(g_5)\n        # g_5 = tf.nn.tanh(g_5)\n\n    print g_1, \'g1\'\n    print g_2, \'g2\'\n    print g_3, \'g3\'\n    print g_4, \'g4\'\n    print g_5, \'g5\'\n    \n    return g_5\n\n\ndef discriminator(inputs, phase_train=True, reuse=False):\n\n    strides    = [1,2,2,2,1]\n    with tf.variable_scope(""dis""):\n        d_1 = tf.nn.conv3d(inputs, weights[\'wd1\'], strides=strides, padding=""SAME"")\n        d_1 = tf.contrib.layers.batch_norm(d_1, is_training=phase_train)                               \n        d_1 = lrelu(d_1, leak_value)\n\n        d_2 = tf.nn.conv3d(d_1, weights[\'wd2\'], strides=strides, padding=""SAME"") \n        d_2 = tf.contrib.layers.batch_norm(d_2, is_training=phase_train)\n        d_2 = lrelu(d_2, leak_value)\n        \n        d_3 = tf.nn.conv3d(d_2, weights[\'wd3\'], strides=strides, padding=""SAME"")  \n        d_3 = tf.contrib.layers.batch_norm(d_3, is_training=phase_train)\n        d_3 = lrelu(d_3, leak_value) \n\n        d_4 = tf.nn.conv3d(d_3, weights[\'wd4\'], strides=strides, padding=""SAME"")     \n        d_4 = tf.contrib.layers.batch_norm(d_4, is_training=phase_train)\n        d_4 = lrelu(d_4)\n\n        d_5 = tf.nn.conv3d(d_4, weights[\'wd5\'], strides=[1,1,1,1,1], padding=""VALID"")     \n        d_5 = tf.nn.sigmoid(d_5)\n\n    print d_1, \'d1\'\n    print d_2, \'d2\'\n    print d_3, \'d3\'\n    print d_4, \'d4\'\n    print d_5, \'d5\'\n\n    return d_5\n\ndef initialiseMITWeights(file_path=\'../../mit_weights.npy\'):\n    global weights\n\n    mit_weights = np.load(file_path)\n    layer_idx = [\'1\', \'4\', \'7\', \'10\', \'13\'] \n\n\n    weights[\'wg1\'] = tf.get_variable(""wg1"", initializer=tf.constant(mit_weights[()][\'1\'].transpose(2,3,4,1,0).astype(np.float32)))\n    weights[\'wg2\'] = tf.get_variable(""wg2"", initializer=tf.constant(mit_weights[()][\'4\'].transpose(2,3,4,1,0).astype(np.float32)))\n    weights[\'wg3\'] = tf.get_variable(""wg3"", initializer=tf.constant(mit_weights[()][\'7\'].transpose(2,3,4,1,0).astype(np.float32)))\n    weights[\'wg4\'] = tf.get_variable(""wg4"", initializer=tf.constant(mit_weights[()][\'10\'].transpose(2,3,4,1,0).astype(np.float32)))\n    weights[\'wg5\'] = tf.get_variable(""wg5"", initializer=tf.constant(mit_weights[()][\'13\'].transpose(2,3,4,1,0).astype(np.float32)))    \n\n    return weights\n\n\ndef initialiseWeights():\n\n    global weights\n    xavier_init = tf.contrib.layers.xavier_initializer()\n\n    weights[\'wg1\'] = tf.get_variable(""wg1"", shape=[4, 4, 4, 512, 200], initializer=xavier_init)\n    weights[\'wg2\'] = tf.get_variable(""wg2"", shape=[4, 4, 4, 256, 512], initializer=xavier_init)\n    weights[\'wg3\'] = tf.get_variable(""wg3"", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n    weights[\'wg4\'] = tf.get_variable(""wg4"", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n    weights[\'wg5\'] = tf.get_variable(""wg5"", shape=[4, 4, 4, 1, 64], initializer=xavier_init)    \n\n    weights[\'wd1\'] = tf.get_variable(""wd1"", shape=[4, 4, 4, 1, 64], initializer=xavier_init)\n    weights[\'wd2\'] = tf.get_variable(""wd2"", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n    weights[\'wd3\'] = tf.get_variable(""wd3"", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n    weights[\'wd4\'] = tf.get_variable(""wd4"", shape=[4, 4, 4, 256, 512], initializer=xavier_init)    \n    weights[\'wd5\'] = tf.get_variable(""wd5"", shape=[4, 4, 4, 512, 1], initializer=xavier_init)    \n\n    return weights\n\n\ndef trainGAN(is_dummy=False):\n\n    weights =  initialiseWeights()\n\n    z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n    x_vector = tf.placeholder(shape=[batch_size,cube_len,cube_len,cube_len,1],dtype=tf.float32) \n\n    net_g_train = generator(z_vector, phase_train=True, reuse=False) \n\n    d_output_x = discriminator(x_vector, phase_train=True, reuse=False)\n    d_output_x = tf.maximum(tf.minimum(d_output_x, 0.99), 0.01)\n    summary_d_x_hist = tf.summary.histogram(""d_prob_x"", d_output_x)\n\n    d_output_z = discriminator(net_g_train, phase_train=True, reuse=True)\n    d_output_z = tf.maximum(tf.minimum(d_output_z, 0.99), 0.01)\n    summary_d_z_hist = tf.summary.histogram(""d_prob_z"", d_output_z)\n\n    # Compute the discriminator accuracy\n    n_p_x = tf.reduce_sum(tf.cast(d_output_x > 0.5, tf.int32))\n    n_p_z = tf.reduce_sum(tf.cast(d_output_z < 0.5, tf.int32))\n    d_acc = tf.divide(n_p_x + n_p_z, 2 * batch_size)\n\n    # Compute the discriminator and generator loss\n    # d_loss = -tf.reduce_mean(tf.log(d_output_x) + tf.log(1-d_output_z))\n    # g_loss = -tf.reduce_mean(tf.log(d_output_z))\n\n    d_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=d_output_x, labels=tf.ones_like(d_output_x)) + tf.nn.sigmoid_cross_entropy_with_logits(logits=d_output_z, labels=tf.zeros_like(d_output_z))\n    g_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=d_output_z, labels=tf.ones_like(d_output_z))\n    \n    d_loss = tf.reduce_mean(d_loss)\n    g_loss = tf.reduce_mean(g_loss)\n\n    summary_d_loss = tf.summary.scalar(""d_loss"", d_loss)\n    summary_g_loss = tf.summary.scalar(""g_loss"", g_loss)\n    summary_n_p_z = tf.summary.scalar(""n_p_z"", n_p_z)\n    summary_n_p_x = tf.summary.scalar(""n_p_x"", n_p_x)\n    summary_d_acc = tf.summary.scalar(""d_acc"", d_acc)\n\n    net_g_test = generator(z_vector, phase_train=False, reuse=True)\n\n    para_g = [var for var in tf.trainable_variables() if any(x in var.name for x in [\'wg\', \'bg\', \'gen\'])]\n    para_d = [var for var in tf.trainable_variables() if any(x in var.name for x in [\'wd\', \'bd\', \'dis\'])]\n\n    # only update the weights for the discriminator network\n    optimizer_op_d = tf.train.AdamOptimizer(learning_rate=d_lr,beta1=beta).minimize(d_loss,var_list=para_d)\n    # only update the weights for the generator network\n    optimizer_op_g = tf.train.AdamOptimizer(learning_rate=g_lr,beta1=beta).minimize(g_loss,var_list=para_g)\n\n    saver = tf.train.Saver(max_to_keep=50) \n\n    with tf.Session() as sess:  \n      \n        sess.run(tf.global_variables_initializer())       \n        z_sample = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n        if is_dummy:\n            volumes = np.random.randint(0,2,(batch_size,cube_len,cube_len,cube_len))\n            print \'Using Dummy Data\'\n        else:\n            volumes = d.getAll(obj=obj, train=True, is_local=is_local, obj_ratio=obj_ratio)\n            print \'Using \' + obj + \' Data\'\n        volumes = volumes[...,np.newaxis].astype(np.float)\n        # volumes *= 2.0\n        # volumes -= 1.0\n\n        for epoch in range(n_epochs):\n            \n            idx = np.random.randint(len(volumes), size=batch_size)\n            x = volumes[idx]\n            # z = np.random.normal(0, 1, size=[batch_size, z_size]).astype(np.float32)\n            z = np.random.uniform(0, 1, size=[batch_size, z_size]).astype(np.float32)\n\n            # Update the discriminator and generator\n            d_summary_merge = tf.summary.merge([summary_d_loss,\n                                                summary_d_x_hist, \n                                                summary_d_z_hist,\n                                                summary_n_p_x,\n                                                summary_n_p_z,\n                                                summary_d_acc])\n\n            summary_d, discriminator_loss = sess.run([d_summary_merge,d_loss],feed_dict={z_vector:z, x_vector:x})\n            summary_g, generator_loss = sess.run([summary_g_loss,g_loss],feed_dict={z_vector:z})  \n            d_accuracy, n_x, n_z = sess.run([d_acc, n_p_x, n_p_z],feed_dict={z_vector:z, x_vector:x})\n            print n_x, n_z\n\n            if d_accuracy < d_thresh:\n                sess.run([optimizer_op_d],feed_dict={z_vector:z, x_vector:x})\n                print \'Discriminator Training \', ""epoch: "",epoch,\', d_loss:\',discriminator_loss,\'g_loss:\',generator_loss, ""d_acc: "", d_accuracy\n\n            sess.run([optimizer_op_g],feed_dict={z_vector:z})\n            print \'Generator Training \', ""epoch: "",epoch,\', d_loss:\',discriminator_loss,\'g_loss:\',generator_loss, ""d_acc: "", d_accuracy\n\n            # output generated chairs\n            if epoch % 50 == 10:\n                g_chairs = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n                if not os.path.exists(train_sample_directory):\n                    os.makedirs(train_sample_directory)\n                g_chairs.dump(train_sample_directory+\'/biasfree_\'+str(epoch))\n            \n            if epoch % 50 == 10:\n                if not os.path.exists(model_directory):\n                    os.makedirs(model_directory)      \n                saver.save(sess, save_path = model_directory + \'/biasfree_\' + str(epoch) + \'.cptk\')\n\ndef testGAN():\n\n    weights =  initialiseMITWeights()\n\n    z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n    x_vector = tf.placeholder(shape=[batch_size,cube_len,cube_len,cube_len,1],dtype=tf.float32) \n\n    net_g_test = generator(z_vector, phase_train=False, reuse=True)\n\n    with tf.Session() as sess:\n      \n        sess.run(tf.global_variables_initializer())       \n        z_sample = np.random.uniform(0, 1, size=[batch_size, z_size]).astype(np.float32)\n\n        g_chairs = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n        if not os.path.exists(train_sample_directory):\n            os.makedirs(train_sample_directory)\n        g_chairs.dump(train_sample_directory+\'/biasfree_trained_\' + str(batch_size))\n\nif __name__ == \'__main__\':\n    #is_dummy = bool(int(sys.argv[1]))\n    #trainGAN(is_dummy=is_dummy)\n    testGAN()\n'"
src/dataIO.py,0,"b""import sys\nimport os\n\nimport scipy.ndimage as nd\nimport scipy.io as io\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport skimage.measure as sk\n\nfrom mpl_toolkits import mplot3d\n\ntry:\n    import trimesh\n    from stl import mesh\nexcept:\n    pass\n    print 'All dependencies not loaded, some functionality may not work'\n\nLOCAL_PATH = '/home/meetshah1995/datasets/ModelNet/3DShapeNets/volumetric_data/'\nSERVER_PATH = '/home/gpu_users/meetshah/3dgan/volumetric_data/'\n\ndef getVF(path):\n    raw_data = tuple(open(path, 'r'))\n    header = raw_data[1].split()\n    n_vertices = int(header[0])\n    n_faces = int(header[1])\n    vertices = np.asarray([map(float,raw_data[i+2].split()) for i in range(n_vertices)])\n    faces = np.asarray([map(int,raw_data[i+2+n_vertices].split()) for i in range(n_faces)]) \n    return vertices, faces\n\ndef plotFromVF(vertices, faces):\n    input_vec = mesh.Mesh(np.zeros(faces.shape[0], dtype=mesh.Mesh.dtype))\n    for i, f in enumerate(faces):\n        for j in range(3):\n            input_vec.vectors[i][j] = vertices[f[j],:]\n    figure = plt.figure()\n    axes = mplot3d.Axes3D(figure)\n    axes.add_collection3d(mplot3d.art3d.Poly3DCollection(input_vec.vectors))\n    scale = input_vec.points.flatten(-1)\n    axes.auto_scale_xyz(scale, scale, scale)\n    plt.show()\n\ndef plotFromVoxels(voxels):\n    z,x,y = voxels.nonzero()\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(x, y, -z, zdir='z', c= 'red')\n    plt.show()\n\ndef getVFByMarchingCubes(voxels, threshold=0.5):\n    v, f =  sk.marching_cubes(voxels, level=threshold)\n    return v, f\n\ndef plotMeshFromVoxels(voxels, threshold=0.5):\n    v,f = getVFByMarchingCubes(voxels, threshold)\n    plotFromVF(v,f)\n\ndef plotVoxelVisdom(voxels, visdom, title):\n    v, f = getVFByMarchingCubes(voxels)\n    visdom.mesh(X=v, Y=f, opts=dict(opacity=0.5, title=title))\n\ndef plotFromVertices(vertices):\n    figure = plt.figure()\n    axes = mplot3d.Axes3D(figure)\n    axes.scatter(vertices.T[0,:],vertices.T[1,:],vertices.T[2,:])\n    plt.show()\n\ndef getVolumeFromOFF(path, sideLen=32):\n    mesh = trimesh.load(path)\n    volume = trimesh.voxel.Voxel(mesh, 0.5).raw\n    (x, y, z) = map(float, volume.shape)\n    volume = nd.zoom(volume.astype(float), \n                     (sideLen/x, sideLen/y, sideLen/z),\n                     order=1, \n                     mode='nearest')\n    volume[np.nonzero(volume)] = 1.0\n    return volume.astype(np.bool)\n\ndef getVoxelFromMat(path, cube_len=64):\n    voxels = io.loadmat(path)['instance']\n    voxels = np.pad(voxels,(1,1),'constant',constant_values=(0,0))\n    if cube_len != 32 and cube_len == 64:\n        voxels = nd.zoom(voxels, (2,2,2), mode='constant', order=0)\n    return voxels\n\ndef getAll(obj='airplane',train=True, is_local=False, cube_len=64, obj_ratio=1.0):\n    objPath = SERVER_PATH + obj + '/30/'\n    if is_local:\n        objPath = LOCAL_PATH + obj + '/30/'\n    objPath += 'train/' if train else 'test/'\n    fileList = [f for f in os.listdir(objPath) if f.endswith('.mat')]\n    fileList = fileList[0:int(obj_ratio*len(fileList))]\n    volumeBatch = np.asarray([getVoxelFromMat(objPath + f, cube_len) for f in fileList],dtype=np.bool)\n    return volumeBatch\n\n\nif __name__ == '__main__':\n    path = sys.argv[1]\n    volume = getVolumeFromOFF(path)\n    plotFromVoxels(volume)\n"""
src/utils.py,16,"b'#!/usr/bin/env python\n\n__author__ = ""Meet Shah""\n__license__ = ""MIT""\n\nimport tensorflow as tf\n\n\ndef init_weights(shape, name):\n    return tf.get_variable(name, shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n\n    \ndef init_biases(shape):\n    return tf.Variable(tf.zeros(shape))\n\n\ndef batchNorm(x, n_out, phase_train, scope=\'bn\'):\n    with tf.variable_scope(scope):\n        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),name=\'beta\', trainable=True)\n        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),name=\'gamma\', trainable=True)\n        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name=\'moments\')\n        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n\n        def mean_var_with_update():\n            ema_apply_op = ema.apply([batch_mean, batch_var])\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n\n        mean, var = tf.cond(phase_train,\n                            mean_var_with_update,\n                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n    return normed\n\n\nclass batch_norm(object):\n  \tdef __init__(self, epsilon=1e-5, momentum = 0.9, name=""batch_norm""):\n\t\twith tf.variable_scope(name):\n\t\t\tself.epsilon  = epsilon\n      \t\tself.momentum = momentum\n      \t\tself.name = name\n\n\tdef __call__(self, x, train=True):\n\t\treturn tf.contrib.layers.batch_norm(x,\n                      decay=self.momentum, \n                      updates_collections=None,\n                      epsilon=self.epsilon,\n                      scale=True,\n                      is_training=train,\n                      scope=self.name)\n\n\ndef threshold(x, val=0.5):\n    x = tf.clip_by_value(x,0.5,0.5001) - 0.5\n    x = tf.minimum(x * 10000,1) \n    return x\n\ndef lrelu(x, leak=0.2):\n    return tf.maximum(x, leak*x)\n\n# def lrelu(x, leak=0.2):\n#     f1 = 0.5 * (1 + leak)\n#     f2 = 0.5 * (1 - leak)\n#     return f1 * x + f2 * abs(x)'"
