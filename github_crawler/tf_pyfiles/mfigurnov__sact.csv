file_path,api_count,code
act.py,82,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions for adaptive computation time.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef adaptive_computation_time(halting_proba, eps=1e-2):\n  """"""Gets cost, number of steps and halting dist. for adaptive computation time.\n\n  See Alex Graves ""Adaptive Computation Time for Recurrent Neural Networks""\n  https://arxiv.org/pdf/1603.08983v4.pdf\n\n  Also see notes by Hugo Larochelle:\n  https://www.evernote.com/shard/s189/sh/fd165646-b630-48b7-844c-86ad2f07fcda/c9ab960af967ef847097f21d94b0bff7\n\n  This module makes several assumptions:\n  1) The maximum number of units is `max_units`.\n  2) We run all the units for each object during training and inference.\n    The unused units are simply ""masked"".\n\n  Args:\n    halting_proba: A 2-D `Tensor` of type `float32`. Probabilities\n      of halting the computation at a given unit for the object.\n      Shape is `[batch, max_units - 1]`.\n      The values need to be in the range [0, 1].\n    eps: A `float` in the range [0, 1]. Small number to ensure that\n      the computation can halt after the first unit.\n\n  Returns:\n    ponder_cost: An 1-D `Tensor` of type `float32`.\n      A differentiable upper bound on the number of units.\n    num_units: An 1-D `Tensor` of type `int32`.\n      Actual number of units that were actually executed.\n      num_units < ponder_cost.\n    halting_distribution: A 2-D `Tensor` of type `float32`.\n      Shape is `[batch, max_units]`. Halting probability distribution.\n      halting_distribution[i, j] is probability of computation for i-th object\n      to halt at j-th unit. Sum of every row should be close to one.\n  """"""\n  sh = halting_proba.get_shape().as_list()\n  batch = sh[0]\n  max_units = sh[1] + 1\n\n  zero_col = tf.zeros((batch, 1))\n\n  halting_padded = tf.concat([halting_proba, zero_col], 1)\n\n  halting_cumsum = tf.cumsum(halting_proba, axis=1)\n  halting_cumsum_padded = tf.concat([zero_col, halting_cumsum], 1)\n\n  # Does computation halt at this unit?\n  halt_flag = (halting_cumsum >= 1 - eps)\n  # Always halt at the final unit.\n  halt_flag_final = tf.concat([halt_flag, tf.fill([batch, 1], True)], 1)\n\n  # Halting iteration (zero-based), eqn. (7).\n  # Add a decaying value that ensures that the first true value is selected.\n  # The decay value is always less than one.\n  decay = 1. / (2. + tf.to_float(tf.range(max_units)))\n  halt_flag_final_with_decay = tf.to_float(halt_flag_final) + decay[None, :]\n  N = tf.to_int32(tf.argmax(halt_flag_final_with_decay, dimension=1))\n\n  N = tf.stop_gradient(N)\n\n  # Fancy indexing to obtain the value of the remainder. Eqn. (8).\n  N_indices = tf.range(batch) * max_units + N\n  remainder = 1 - tf.gather(tf.reshape(halting_cumsum_padded, [-1]), N_indices)\n\n  # Switch to one-based indexing here for num_units.\n  num_units = N + 1\n  ponder_cost = tf.to_float(num_units) + remainder\n\n  unit_index = tf.range(max_units)[None, :]\n  # Calculate the halting distribution, eqn. (6).\n  # Fill the first N steps with the halting probabilities.\n  # Next values are zero.\n  p = tf.where(tf.less(unit_index, N[:, None]),\n               halting_padded,\n               tf.zeros((batch, max_units)))\n  # Fill the (N+1)-st step with the remainder value.\n  p = tf.where(tf.equal(unit_index, N[:, None]),\n               tf.tile(remainder[:, None], tf.stack([1, max_units])),\n               p)\n  halting_distribution = p\n\n  return (ponder_cost, num_units, halting_distribution)\n\n\ndef run_units(inputs, unit, max_units, scope, reuse=False):\n  """"""Helper function for running units of the network.""""""\n  states = []\n  halting_probas = []\n  all_flops = []\n  with tf.variable_scope(scope, reuse=reuse):\n    state = inputs\n    for unit_idx in range(max_units):\n      state, halting_proba, flops = unit(state, unit_idx)\n      states.append(state)\n      halting_probas.append(halting_proba)\n      all_flops.append(flops)\n  return states, halting_probas, all_flops\n\n\ndef adaptive_computation_time_wrapper(inputs, unit, max_units,\n                                      eps=1e-2, scope=\'act\'):\n  """"""A wrapper of `adaptive_computation_time`.\n\n  Wraps `adaptive_computation_time` with an interface compatible with\n  `adaptive_computation_early_stopping`. Should do the same thing as\n  `adaptive_computation_early_stopping` but should work in cases when tf.cond\n  fails.\n  """"""\n  states, halting_probas, all_flops = run_units(inputs, unit,\n                                                max_units, scope)\n\n  (ponder_cost, num_units, halting_distribution) = \\\n      adaptive_computation_time(tf.concat(halting_probas[:-1], 1), eps=eps)\n\n  if states[0].get_shape().is_fully_defined():\n    sh = states[0].get_shape().as_list()\n  else:\n    sh = tf.shape(states[0])\n  batch = sh[0]\n  h = tf.reshape(halting_distribution, [batch, 1, max_units])\n  s = tf.reshape(tf.stack(states, axis=1), [batch, max_units, -1])\n  outputs = tf.matmul(h, s)\n  outputs = tf.reshape(outputs, sh)\n\n  flops_per_iteration = [\n      f * tf.to_int64(num_units > i) for (i, f) in enumerate(all_flops)\n  ]\n  flops = tf.add_n(flops_per_iteration)\n\n  return (ponder_cost, num_units, flops, halting_distribution, outputs)\n\n\ndef adaptive_computation_early_stopping(inputs, unit, max_units,\n                                        eps=1e-2, scope=\'act\'):\n  """"""Builds adaptive computation module with early stopping of computation.\n\n  `adaptive_computation_time` requires all units to be always\n  computed. This function stops the computation as soon as all objects in the\n  batch halt. However, if any object still needs calculation, the\n  unit is executed for all objects.\n\n  See `adaptive_computation_time` description for more information.\n\n  Args:\n    inputs: Input state at the first unit. Can have different shape from\n      state and output. Should have fully defined shape.\n    unit: A function which is called as follows:\n      `new_state, halting_proba, flops = unit(old_state, unit_idx)`\n      If `unit_idx==1`, then `old_state` is `inputs`.\n      Flops should be a 1-D `Tensor` of length batch_size of type `int64`.\n      It can perform different computation depending on `unit_idx`.\n      The function should not have any Python side-effects (due to `tf.cond`\n      implementation).\n\n      The function is called two times for each `unit_idx`.\n      1) Outside `tf.cond` to create the necessary variables with reuse=False.\n      2) Inside `tf.cond` with reuse=True.\n      For this reason, all variables should have static names.\n      Good: `w = tf.get_variable(\'weights\', [5, 3])`\n      Bad: `w = tf.Variable(tf.zeros([5, 3]))  # The name is auto-generated`\n    max_units: Maximum number of units.\n    eps: A `float` in the range [0, 1]. Small number to ensure that\n      the computation can halt after the first unit.\n    scope: variable scope or scope name in which the layers are created.\n      Defaults to \'act\'.\n\n  Returns:\n    ponder_cost: A 1-D `Tensor` of type `float32`.\n      A differentiable upper bound on the number of units.\n    num_units: A 1-D `Tensor` of type `int32`.\n      Actual number of units that took place. num_units < ponder_cost.\n    flops: A 1-D `Tensor` of type `int64`.\n      Number of floating point operations that took place.\n    halting_distribution: A 2-D `Tensor` of type `float`.\n      Shape is `[batch, max_units]`. Halting probability distribution.\n      halting_distribution[i, j] is probability of computation for i-th object\n      to halt at j-th unit. Sum of every row should be close to one.\n    outputs: A `Tensor` of shape [batch, ...]. Has same shape as states.\n      Outputs of the ACT module, intermediate states weighted\n      by the halting distribution for the units.\n  """"""\n  if inputs.get_shape().is_fully_defined():\n    sh = inputs.get_shape().as_list()\n  else:\n    sh = tf.shape(inputs)\n  batch = sh[0]\n  inputs_rank = len(sh)\n\n  def _body(unit_idx, state, halting_cumsum, elements_finished, remainder,\n            ponder_cost, num_units, flops, outputs):\n\n    (new_state, halting_proba, cur_flops) = unit(state, unit_idx)\n\n    # We always halt at the last unit.\n    if unit_idx < max_units - 1:\n      halting_proba = tf.reshape(halting_proba, [batch])\n    else:\n      halting_proba = tf.ones([batch])\n\n    halting_cumsum += halting_proba\n    cur_elements_finished = (halting_cumsum >= 1 - eps)\n    # Zero out halting_proba for the previously finished objects.\n    halting_proba = tf.where(cur_elements_finished,\n                             tf.zeros([batch]),\n                             halting_proba)\n    # Find objects which have halted at the current unit.\n    just_finished = tf.logical_and(tf.logical_not(elements_finished),\n                                   cur_elements_finished)\n    # For such objects, the halting distribution value is the remainder.\n    # For others, it is the halting_proba.\n    cur_halting_distrib = tf.where(just_finished,\n                                   remainder,\n                                   halting_proba)\n\n    # Update ponder_cost. Add 1 to objects which are still computed,\n    # remainder to the objects which have just halted and\n    # 0 to the previously halted objects.\n    ponder_cost += tf.where(\n        cur_elements_finished,\n        tf.where(just_finished, remainder, tf.zeros([batch])),\n        tf.ones([batch]))\n\n    # Add a unit to the objects that were active during this unit\n    # (not the ones that will be active the next unit).\n    evaluated_objects = tf.logical_not(elements_finished)\n    num_units += tf.to_int32(evaluated_objects)\n\n    # Update the FLOPS counters for the same objects.\n    flops += cur_flops * tf.to_int64(evaluated_objects)\n\n    # Add new state to the outputs weighted by the halting distribution.\n    outputs += new_state * tf.reshape(cur_halting_distrib,\n                                      [-1] + [1] * (inputs_rank - 1))\n\n    remainder -= halting_proba\n\n    return (new_state, halting_cumsum, cur_elements_finished, remainder,\n            ponder_cost, num_units, flops, cur_halting_distrib, outputs)\n\n  def _identity(unit_idx, state, halting_cumsum, elements_finished,\n                remainder, ponder_cost, num_units, flops, outputs):\n    return (state, halting_cumsum, elements_finished, remainder, ponder_cost,\n            num_units, flops, tf.zeros([batch]), outputs)\n\n  # Create all the variables and losses outside of tf.cond.\n  # Without this, regularization losses would not work correctly.\n  run_units(inputs, unit, max_units, scope)\n\n  state = inputs\n  halting_cumsum = tf.zeros([batch])\n  elements_finished = tf.fill([batch], False)\n  remainder = tf.ones([batch])\n  # Initialize ponder_cost with one to fix an off-by-one error.\n  ponder_cost = tf.ones([batch])\n  num_units = tf.zeros([batch], dtype=tf.int32)\n  flops = tf.zeros([batch], dtype=tf.int64)\n\n  # We don\'t know the shape of the outputs. Initialize it to scalar and\n  # run the first iteration outside tf.cond (it wants outputs of both\n  # branches to have the same shapes).\n  outputs = 0.\n\n  # Reuse the variables created above.\n  with tf.variable_scope(scope, reuse=True):\n    halting_distribs = []\n    for unit_idx in range(max_units):\n      finished = tf.reduce_all(elements_finished)\n      args = (unit_idx, state, halting_cumsum, elements_finished, remainder,\n              ponder_cost, num_units, flops, outputs)\n      if unit_idx == 0:\n        return_values = _body(*args)\n      else:\n        return_values = tf.cond(finished,\n                                lambda: _identity(*args),\n                                lambda: _body(*args))\n      (state, halting_cumsum, elements_finished, remainder, ponder_cost,\n       num_units, flops, cur_halting_distrib, outputs) = return_values\n\n      halting_distribs.append(tf.reshape(cur_halting_distrib, [batch, 1]))\n\n  halting_distribution = tf.concat(halting_distribs, 1)\n\n  return (ponder_cost, num_units, flops, halting_distribution, outputs)\n\n\ndef spatially_adaptive_computation_time(inputs, unit, max_units,\n                                        eps=1e-2, scope=\'act\'):\n  """"""Spatially adaptive computation time.\n\n  Each spatial position in the states tensor has its own halting distribution.\n  This allows to process different part of an image for a different number of\n  units.\n\n  The code is similar to `adaptive_computation_early_stopping`. The differences\n  are:\n  1) The states are expected to be 4-D tensors (Batch-Height-Width-Channels).\n    ACT is applied for first three dimensions.\n  2) unit should have a `residual_mask` argument. It is a `float32` mask\n    with 1\'s corresponding to the positions which need to be updated.\n    0\'s should be frozen. For ResNets this can be achieved by multiplying the\n    residual branch responses by `residual_mask`.\n  3) There is no tf.cond part so the computation is not actually saved.\n\n  Args:\n    inputs: Input states at the first unit, 4-D `Tensor` of type `float32`.\n    unit: A function. See `adaptive_computation_early_stopping` for\n      detailed explanation.\n    max_units: Maximum number of units.\n    eps: A `float` in the range [0, 1]. Small number to ensure that\n      the computation can halt after the first unit.\n    scope: variable scope or scope name in which the layers are created.\n      Defaults to \'act\'.\n\n  Returns:\n    ponder_cost: A 3-D `Tensor` of type `float32`.\n      Shape is [batch, height, width].\n      A differentiable upper bound on the number of units per spatial position.\n    num_units: A 3-D `Tensor` of type `int32`.\n      Shape is [batch, height, width].\n      Actual number of units per spatial position that were used.\n      num_units < ponder_cost.\n    flops: A 1-D `Tensor` of type `int64`.\n      Number of floating point operations that were used.\n    halting_distribution: A 4-D `Tensor` of type `float32`.\n      Shape is `[batch, height, width, max_units]`.\n      Halting probability distribution.\n      halting_distribution[i, h, w, j] is the probability of computation\n      for i-th object at the spatial position (h, w) to halt at j-th unit.\n      Sum over the last dimension should be close to one.\n    outputs: A 4-D `Tensor` of shape [batch, height, width, depth]. Outputs of\n      the ACT module, intermediate states weighted by the halting distribution\n      tensor.\n  """"""\n  with tf.variable_scope(scope):\n    halting_distribs = []\n    for unit_idx in range(max_units):\n\n      if not unit_idx:\n        (state, halting_proba, flops) = unit(\n            inputs, unit_idx, residual_mask=None)\n\n        # Initialize the variables which depend on the state shape.\n        state_shape_fully_defined = state.get_shape().is_fully_defined()\n        if state_shape_fully_defined:\n          sh = state.get_shape().as_list()\n          assert len(sh) == 4\n        else:\n          sh = tf.shape(state)\n        halting_cumsum = tf.zeros(sh[:3])\n        elements_finished = tf.fill(sh[:3], False)\n        remainder = tf.ones(sh[:3])\n        # Initialize ponder_cost with one to fix an off-by-one error.\n        ponder_cost = tf.ones(sh[:3])\n        num_units = tf.zeros(sh[:3], dtype=tf.int32)\n      else:\n        # Mask out the residual values for the not calculated outputs.\n        residual_mask = tf.to_float(tf.logical_not(elements_finished))\n        residual_mask = tf.expand_dims(residual_mask, 3)\n        (state, halting_proba, current_flops) = unit(\n            state, unit_idx, residual_mask=residual_mask)\n        flops += current_flops\n\n      # We always halt at the last unit.\n      if unit_idx < max_units - 1:\n        halting_proba = tf.reshape(halting_proba, sh[:3])\n      else:\n        halting_proba = tf.ones(sh[:3])\n\n      halting_cumsum += halting_proba\n      # Which objects are no longer calculated after this unit?\n      cur_elements_finished = (halting_cumsum >= 1 - eps)\n      # Zero out halting_proba for the previously finished positions.\n      halting_proba = tf.where(cur_elements_finished,\n                               tf.zeros(sh[:3]),\n                               halting_proba)\n      # Find positions which have halted at the current unit.\n      just_finished = tf.logical_and(tf.logical_not(elements_finished),\n                                     cur_elements_finished)\n      # For such positions, the halting distribution value is the remainder.\n      # For others, it is the halting_proba.\n      cur_halting_distrib = tf.where(just_finished,\n                                     remainder,\n                                     halting_proba)\n\n      # Update ponder_cost. Add 1 to positions which are still computed,\n      # remainder to the positions which have just halted and\n      # 0 to the previously halted positions.\n      ponder_cost += tf.where(\n          cur_elements_finished,\n          tf.where(just_finished, remainder, tf.zeros(sh[:3])),\n          tf.ones(sh[:3]))\n\n      # Add a unit to the positions that were active during this unit\n      # (not the ones that will be active the next unit).\n      num_units += tf.to_int32(tf.logical_not(elements_finished))\n\n      # Add new state to the outputs weighted by the halting distribution.\n      update = state * tf.expand_dims(cur_halting_distrib, 3)\n      if unit_idx:\n        outputs += update\n      else:\n        outputs = update\n\n      remainder -= halting_proba\n\n      elements_finished = cur_elements_finished\n\n      halting_distribs.append(cur_halting_distrib)\n\n  halting_distribution = tf.stack(halting_distribs, axis=3)\n\n  if not state_shape_fully_defined:\n    # Update static shape info. Faster RCNN code wants to know batch dimension\n    # statically.\n    outputs.set_shape(inputs.get_shape().as_list()[:1] + [None] * 3)\n\n  return (ponder_cost, num_units, flops, halting_distribution, outputs)\n'"
act_test.py,86,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for adaptive computation time.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nimport act\n\n\nclass ActTest(tf.test.TestCase):\n\n  def testOutputSize(self):\n    batch_size = 5\n    max_units = 8\n    h = tf.sigmoid(tf.random_normal(shape=[batch_size, max_units - 1]))\n    (cost, num_units, distrib) = act.adaptive_computation_time(h)\n    with self.test_session() as sess:\n      (cost_out, num_units_out, distrib_out) = sess.run(\n          (cost, num_units, distrib))\n      self.assertEqual(cost_out.shape, (batch_size,))\n      self.assertEqual(num_units_out.shape, (batch_size,))\n      self.assertEqual(distrib_out.shape, (batch_size, max_units))\n\n  def testEqualValuesInBatch(self):\n    batch_size = 2\n    max_units = 8\n    h = tf.sigmoid(tf.random_normal(shape=[1, max_units - 1]))\n    h = tf.tile(h, tf.stack([batch_size, 1]))\n    (cost, num_units, distrib) = act.adaptive_computation_time(h)\n    with self.test_session() as sess:\n      (cost_out, num_units_out, distrib_out) = sess.run(\n          (cost, num_units, distrib))\n      self.assertAlmostEqual(cost_out[0], cost_out[1])\n      self.assertEqual(num_units_out[0], num_units_out[1])\n      self.assertAllEqual(distrib_out[0], distrib_out[1])\n\n  def testStopsAtFirstUnit(self):\n    h = tf.constant([[0.999] * 4])\n    (cost, num_units, distrib) = act.adaptive_computation_time(h, eps=1e-2)\n    with self.test_session() as sess:\n      (cost_out, num_units_out, distrib_out) = sess.run(\n          (cost, num_units, distrib))\n      self.assertAllClose(cost_out, np.array([2.0]))\n      self.assertAllEqual(num_units_out, np.array([1]))\n      self.assertAllClose(distrib_out, np.array([[1.] + [0.] * 4]))\n\n  def testStopsAtMiddleUnit(self):\n    h = tf.constant([[0.01, 0.50, 0.60, 0.70]])\n    (cost, num_units, distrib) = act.adaptive_computation_time(h)\n    with self.test_session() as sess:\n      (cost_out, num_units_out, distrib_out) = sess.run(\n          (cost, num_units, distrib))\n      self.assertAllClose(cost_out, np.array([3.49]))\n      self.assertAllEqual(num_units_out, np.array([3]))\n      self.assertAllClose(distrib_out, np.array([[0.01, 0.50, 0.49, 0., 0.]]))\n\n  def testStopsAtLastUnit(self):\n    h = tf.constant([[0.01] * 4])\n    (cost, num_units, distrib) = act.adaptive_computation_time(h)\n    with self.test_session() as sess:\n      (cost_out, num_units_out, distrib_out) = sess.run(\n          (cost, num_units, distrib))\n      self.assertAllClose(cost_out, np.array([5.96]))\n      self.assertAllEqual(num_units_out, np.array([5]))\n      self.assertAllClose(distrib_out, np.array([[0.01] * 4 + [0.96]]))\n\n  def testCostGradientsStopsAtFirstUnit(self):\n    h = tf.constant([[0.999] * 4])\n    (cost, num_units, distrib) = act.adaptive_computation_time(h)\n    cost_grad = tf.gradients(cost, h)\n    with self.test_session() as sess:\n      cost_grad_out = sess.run(cost_grad)\n      self.assertAllClose(cost_grad_out, np.array([[[0.] * 4]]))\n\n  def testCostGradientsStopsAtMiddleUnit(self):\n    h = tf.constant([[0.01, 0.50, 0.60, 0.70]])\n    (cost, num_units, distrib) = act.adaptive_computation_time(h)\n    cost_grad = tf.gradients(cost, h)\n    with self.test_session() as sess:\n      cost_grad_out = sess.run(cost_grad)\n      self.assertAllClose(cost_grad_out, np.array([[[-1., -1., 0., 0.]]]))\n\n  def testCostGradientsStopsAtLastUnit(self):\n    h = tf.constant([[0.01] * 4])\n    (cost, num_units, distrib) = act.adaptive_computation_time(h)\n    cost_grad = tf.gradients(cost, h)\n    with self.test_session() as sess:\n      cost_grad_out = sess.run(cost_grad)\n      self.assertAllClose(cost_grad_out, np.array([[[-1.] * 4]]))\n\n\nclass ActWrapperTest(tf.test.TestCase):\n\n  def _runAct(self, unit_outputs, halting_probas):\n    self.assertEqual(len(unit_outputs), len(halting_probas))\n    batch = len(unit_outputs)\n\n    # halting_proba[i][-1] should not be used, but we still pass it here\n    # to be able to check that it does not affect anything.\n    for (l, h) in zip(unit_outputs, halting_probas):\n      self.assertEqual(len(l), len(h))\n    max_units = len(unit_outputs[0])\n\n    unit_outputs_tf = tf.constant(\n        unit_outputs, shape=[batch, max_units], dtype=tf.float32)\n    halting_probas_tf = tf.constant(\n        halting_probas, shape=[batch, max_units], dtype=tf.float32)\n    # Every unit for each object is two FLOPS.\n    flops_tf = tf.constant(2, shape=[batch, max_units], dtype=tf.int64)\n\n    def unit(x, unit_idx):\n      return (\n          tf.reshape(unit_outputs_tf[:, unit_idx], tf.stack([-1, 1])),\n          tf.reshape(halting_probas_tf[:, unit_idx], tf.stack([-1, 1])),\n          flops_tf[:, unit_idx])\n\n    inputs = tf.random_normal(shape=[batch, 1])\n    (cost, num_units, flops, distrib, outputs\n    ) = act.adaptive_computation_time_wrapper(inputs, unit, max_units)\n    cost_grad = tf.gradients(cost, halting_probas_tf)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      return sess.run((cost, num_units, flops, distrib, outputs, cost_grad))\n\n  def testEqualValuesInBatch(self):\n    (cost, num_units, flops, distrib, outputs, cost_grad) = self._runAct(\n        [list(range(5))] * 2, [[0.999] * 5] * 2)\n    self.assertAlmostEqual(cost[0], cost[1])\n    self.assertEqual(num_units[0], num_units[1])\n    self.assertEqual(flops[0], flops[1])\n    self.assertAllClose(distrib[0], distrib[1])\n    self.assertAllClose(outputs[0], outputs[1])\n    self.assertAllClose(cost_grad[0][0], cost_grad[0][1])\n\n  def testStopsAtFirstUnit(self):\n    (cost, num_units, flops, distrib, outputs, cost_grad) = self._runAct(\n        [list(range(5))], [[0.999] + [0.5] * 4])\n    self.assertAllClose(cost, [2.0])\n    self.assertAllEqual(num_units, [1])\n    self.assertAllEqual(flops, [2])\n    self.assertAllClose(distrib, [[1.0] + [0.0] * 4])\n    self.assertAllClose(outputs, [[0.0]])\n    self.assertAllClose(cost_grad, [[[0.0] * 5]])\n\n  def testStopsAtMiddleUnit(self):\n    (cost, num_units, flops, distrib, outputs, cost_grad) = self._runAct(\n        [list(range(5))], [[0.01, 0.5, 0.6, 0.7, 0.8]])\n    self.assertAllClose(cost, [3.49])\n    self.assertAllEqual(num_units, [3])\n    self.assertAllEqual(flops, [6])\n    self.assertAllClose(distrib, [[0.01, 0.50, 0.49, 0., 0.]])\n    self.assertAllClose(outputs, [[1.48]])\n    self.assertAllClose(cost_grad, [[[-1., -1., 0., 0., 0.]]])\n\n  def testStopsAtLastUnit(self):\n    (cost, num_units, flops, distrib, outputs, cost_grad) = self._runAct(\n        [list(range(5))], [[0.01] * 5])\n    self.assertAllClose(cost, [5.96])\n    self.assertAllEqual(num_units, [5])\n    self.assertAllEqual(flops, [10])\n    self.assertAllClose(distrib, [[0.01] * 4 + [0.96]])\n    self.assertAllClose(outputs, [[3.9]])\n    self.assertAllClose(cost_grad, [[[-1.] * 4 + [0.]]])\n\n  def testInputs(self):\n    inputs = tf.random_normal(shape=[2, 3])\n\n    def unit(x, unit_idx):\n      # First object runs for two units, second object for four units.\n      return (x, tf.constant(\n          [0.7, 0.3], shape=[2, 1]), tf.constant(\n              0, shape=[2], dtype=tf.int64))\n\n    (_, _, _, _, outputs) = act.adaptive_computation_time_wrapper(inputs,\n                                                                  unit, 5)\n    with self.test_session() as sess:\n      (inputs_out, outputs_out) = sess.run((inputs, outputs))\n      self.assertAllClose(inputs_out, outputs_out)\n\n  def testRegularization(self):\n    inputs = tf.random_normal(shape=[1, 3])\n\n    def unit(x, unit_idx):\n      with tf.variable_scope(\'{}\'.format(unit_idx)):\n        w = tf.get_variable(\n            \'test_variable\', [1, 1],\n            initializer=tf.constant_initializer(1.0),\n            regularizer=lambda _: 2.0 * tf.nn.l2_loss(_))\n      return (w, tf.constant(\n          1.0, shape=[1, 1]), tf.constant(\n              0, shape=[1], dtype=tf.int64))\n\n    (_, _, _, _, outputs) = act.adaptive_computation_time_wrapper(inputs,\n                                                                  unit, 5)\n    decay_cost = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      (_, decay_cost_out) = sess.run((outputs, decay_cost))\n      self.assertEqual(decay_cost_out, 5.0)\n\n\nclass ActEarlyStoppingTest(tf.test.TestCase):\n\n  def _runAct(self, unit_outputs, halting_probas):\n    self.assertEqual(len(unit_outputs), len(halting_probas))\n    batch = len(unit_outputs)\n\n    # halting_proba[i][-1] should not be used, but we still pass it here\n    # to be able to check that it does not affect anything.\n    for (l, h) in zip(unit_outputs, halting_probas):\n      self.assertEqual(len(l), len(h))\n    max_units = len(unit_outputs[0])\n\n    unit_outputs_tf = tf.constant(\n        unit_outputs, shape=[batch, max_units], dtype=tf.float32)\n    halting_probas_tf = tf.constant(\n        halting_probas, shape=[batch, max_units], dtype=tf.float32)\n    # Every unit for each object is two FLOPS.\n    flops_tf = tf.constant(2, shape=[batch, max_units], dtype=tf.int64)\n    unit_counter = tf.Variable(0, trainable=False)\n\n    def unit(x, unit_idx):\n      assign_op = unit_counter.assign_add(1)\n      with tf.control_dependencies([assign_op]):\n        return (\n            tf.reshape(unit_outputs_tf[:, unit_idx], tf.stack([-1, 1])),\n            tf.reshape(halting_probas_tf[:, unit_idx], tf.stack([-1, 1])),\n            flops_tf[:, unit_idx])\n\n    inputs = tf.random_normal(shape=[batch, 1])\n    (cost, num_units, flops, distrib, outputs\n    ) = act.adaptive_computation_early_stopping(inputs, unit, max_units)\n    cost_grad = tf.gradients(cost, halting_probas_tf)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      return sess.run((cost, num_units, flops, distrib, outputs, cost_grad,\n                       unit_counter))\n\n  def testEqualValuesInBatch(self):\n    (cost, num_units, flops, distrib, outputs, cost_grad,\n     unit_counter) = self._runAct([list(range(5))] * 2, [[0.999] * 5] * 2)\n    self.assertAlmostEqual(cost[0], cost[1])\n    self.assertEqual(num_units[0], num_units[1])\n    self.assertEqual(flops[0], flops[1])\n    self.assertAllClose(distrib[0], distrib[1])\n    self.assertAllClose(outputs[0], outputs[1])\n    self.assertAllClose(cost_grad[0][0], cost_grad[0][1])\n    self.assertEqual(unit_counter, 1)\n\n  def testStopsAtFirstUnit(self):\n    (cost, num_units, flops, distrib, outputs, cost_grad,\n     unit_counter) = self._runAct([list(range(5))], [[0.999] + [0.5] * 4])\n    self.assertAllClose(cost, [2.0])\n    self.assertAllEqual(num_units, [1])\n    self.assertAllEqual(flops, [2])\n    self.assertAllClose(distrib, [[1.0] + [0.0] * 4])\n    self.assertAllClose(outputs, [[0.0]])\n    self.assertAllClose(cost_grad, [[[0.0] * 5]])\n    self.assertEqual(unit_counter, 1)\n\n  def testStopsAtMiddleUnit(self):\n    (cost, num_units, flops, distrib, outputs, cost_grad,\n     unit_counter) = self._runAct([list(range(5))], [[0.01, 0.5, 0.6, 0.7, 0.8]])\n    self.assertAllClose(cost, [3.49])\n    self.assertAllEqual(num_units, [3])\n    self.assertAllEqual(flops, [6])\n    self.assertAllClose(distrib, [[0.01, 0.50, 0.49, 0., 0.]])\n    self.assertAllClose(outputs, [[1.48]])\n    self.assertAllClose(cost_grad, [[[-1., -1., 0., 0., 0.]]])\n    self.assertEqual(unit_counter, 3)\n\n  def testStopsAtLastUnit(self):\n    (cost, num_units, flops, distrib, outputs, cost_grad,\n     unit_counter) = self._runAct([list(range(5))], [[0.01] * 5])\n    self.assertAllClose(cost, [5.96])\n    self.assertAllEqual(num_units, [5])\n    self.assertAllEqual(flops, [10])\n    self.assertAllClose(distrib, [[0.01] * 4 + [0.96]])\n    self.assertAllClose(outputs, [[3.9]])\n    self.assertAllClose(cost_grad, [[[-1.] * 4 + [0.]]])\n    self.assertEqual(unit_counter, 5)\n\n  def testInputs(self):\n    inputs = tf.random_normal(shape=[2, 3])\n\n    def unit(x, unit_idx):\n      # First object runs for two units, second object for four units.\n      return (x, tf.constant(\n          [0.7, 0.3], shape=[2, 1]), tf.constant(\n              0, shape=[2], dtype=tf.int64))\n\n    (_, _, _, _, outputs) = act.adaptive_computation_early_stopping(inputs,\n                                                                    unit, 5)\n    with self.test_session() as sess:\n      (inputs_out, outputs_out) = sess.run((inputs, outputs))\n      self.assertAllClose(inputs_out, outputs_out)\n\n  def testRegularization(self):\n    inputs = tf.random_normal(shape=[1, 3])\n\n    def unit(x, unit_idx):\n      with tf.variable_scope(\'{}\'.format(unit_idx)):\n        w = tf.get_variable(\n            \'test_variable\', [1, 1],\n            initializer=tf.constant_initializer(1.0),\n            regularizer=lambda _: 2.0 * tf.nn.l2_loss(_))\n      return (w, tf.constant(\n          1.0, shape=[1, 1]), tf.constant(\n              0, shape=[1], dtype=tf.int64))\n\n    (_, _, _, _, outputs) = act.adaptive_computation_early_stopping(inputs,\n                                                                    unit, 5)\n    decay_cost = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      (outputs_out, decay_cost_out) = sess.run((outputs, decay_cost))\n      self.assertEqual(decay_cost_out, 5.0)\n\n\nclass SactTest(tf.test.TestCase):\n\n  def testSimple(self):\n    # Batch x Height x Width x Channels\n    sh = [1, 1, 2, 1]\n    unit_outputs = [\n        np.array([1.0, 2.0]).reshape(sh),\n        np.array([3.0, 4.0]).reshape(sh),\n        np.array([5.0, 6.0]).reshape(sh),\n    ]\n    halting_probas = [\n        np.array([0.9, 0.1]).reshape(sh),\n        np.array([0.5, 0.1]).reshape(sh),\n        np.array([0.8, 0.1]).reshape(sh),  # unused\n    ]\n    flops = [2, 2, 2]\n    max_units = 3\n    residual_masks = []\n\n    def unit(_, unit_idx, residual_mask):\n      residual_masks.append(residual_mask)\n      return (tf.constant(\n          unit_outputs[unit_idx], dtype=tf.float32), tf.constant(\n              halting_probas[unit_idx], dtype=tf.float32), tf.constant(\n                  flops[unit_idx], shape=[1], dtype=tf.int64))\n\n    inputs = tf.random_normal(shape=sh)\n    (cost, num_units, flops, distrib, outputs\n    ) = act.spatially_adaptive_computation_time(inputs, unit, max_units)\n    with self.test_session() as sess:\n      (cost_out, num_units_out, flops_out, distrib_out, outputs_out,\n       residual_masks_out) = sess.run(\n           (cost, num_units, flops, distrib, outputs, residual_masks[1:]))\n    # Batch x Height x Width\n    sh = [1, 1, 2]\n    self.assertAllClose(cost_out, np.array([2.1, 3.8]).reshape(sh))\n    self.assertAllEqual(num_units_out, np.array([2, 3]).reshape(sh))\n    self.assertAllEqual(flops_out, [6])\n    distrib_expected = np.array([[0.9, 0.1, 0.0], [0.1, 0.1, 0.8]])\n    self.assertAllClose(distrib_out, distrib_expected.reshape(sh + [3]))\n    outputs_expected = np.array([1.2, 5.4])\n    self.assertAllClose(outputs_out, outputs_expected.reshape(sh + [1]))\n    # Residual mask for the second unit\n    self.assertAllClose(residual_masks_out[0],\n                        np.array([1., 1.]).reshape(sh + [1]))\n    # Residual mask for the third unit\n    self.assertAllClose(residual_masks_out[1],\n                        np.array([0., 1.]).reshape(sh + [1]))\n\n  def testInputs(self):\n    max_units = 5\n    inputs = tf.random_normal(shape=[2, 5, 3, 3])\n    # Generate random probabilities for first four units that sum up to one.\n    # Fill in last unit with zeros.\n    probas = tf.random_normal(shape=[max_units - 1, 2, 5, 3])\n    probas = tf.reshape(probas, [max_units - 1, 2 * 5 * 3])\n    probas = tf.nn.softmax(probas)\n    probas = tf.reshape(probas, [max_units - 1, 2, 5, 3])\n    probas = tf.concat([probas, tf.zeros([1, 2, 5, 3])], 0)\n\n    def unit(x, unit_idx, residual_mask):\n      return (x, tf.reshape(probas[unit_idx, :, :, :], [2, 5, 3, 1]),\n              tf.zeros(\n                  [2], dtype=tf.int64))\n\n    (_, _, _, _, outputs) = act.spatially_adaptive_computation_time(\n        inputs, unit, max_units)\n    with self.test_session() as sess:\n      (inputs_out, outputs_out) = sess.run((inputs, outputs))\n      self.assertAllClose(inputs_out, outputs_out)\n\n  def testResidualMask(self):\n    # Batch x Height x Width x Channels\n    sh = [1, 1, 2, 1]\n    halting_probas = [\n        np.array([0.9, 0.1]).reshape(sh),\n        np.array([0.5, 0.1]).reshape(sh),\n        np.array([0.8, 0.1]).reshape(sh),  # unused\n    ]\n    max_units = 3\n\n    unit_outputs = []\n\n    def unit(x, unit_idx, residual_mask):\n      residual = tf.ones(sh)\n      if residual_mask is not None:\n        residual *= residual_mask\n      outputs = x + residual\n      unit_outputs.append(outputs)\n      return (outputs, tf.constant(\n          halting_probas[unit_idx], dtype=tf.float32), tf.zeros(\n              [2], dtype=tf.int64))\n\n    inputs = tf.zeros(sh)\n    (_, _, _, _, outputs) = act.spatially_adaptive_computation_time(\n        inputs, unit, max_units)\n    with self.test_session() as sess:\n      unit_outputs_out, final_outputs_out = sess.run(\n          (unit_outputs, outputs))\n\n    # First position runs for two iterations,\n    # second position for three iterations\n    self.assertAllClose(unit_outputs_out[0],\n                        np.array([1.0, 1.0]).reshape(sh))\n    self.assertAllClose(unit_outputs_out[1],\n                        np.array([2.0, 2.0]).reshape(sh))\n    self.assertAllClose(unit_outputs_out[2],\n                        np.array([2.0, 3.0]).reshape(sh))\n\n    self.assertAllClose(final_outputs_out, np.array([1.1, 2.7]).reshape(sh))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
cifar_data_provider.py,12,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Contains code for loading and preprocessing the CIFAR-10 data.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib import slim\nfrom tensorflow.contrib.slim import dataset_data_provider\n\nfrom external import datasets_cifar10\n\n\ndef provide_data(split_name, batch_size, dataset_dir=None):\n  """"""Provides batches of CIFAR data.\n\n  Args:\n    split_name: Either \'train\' or \'test\'.\n    batch_size: The number of images in each batch.\n    dataset_dir: Directory where the CIFAR-10 TFRecord files live.\n                 Defaults to ""~/tensorflow/data/cifar10""\n\n  Returns:\n    images: A `Tensor` of size [batch_size, 32, 32, 3]\n    images_not_whiten: A `Tensor` with the same size of `images`, unwhitened\n      images.\n    one_hot_labels: A `Tensor` of size [batch_size, num_classes], where\n      each row has a single element set to one and the rest set to zeros.\n    dataset.num_samples: The number of total samples in the dataset.\n    dataset.num_classes: The number of object classes in the dataset.\n\n  Raises:\n    ValueError: if the split_name is not either \'train\' or \'test\'.\n  """"""\n  with tf.device(\'/cpu:0\'):\n    is_train = split_name == \'train\'\n\n    if dataset_dir is None:\n      dataset_dir = os.path.expanduser(\'~/tensorflow/data/cifar10\')\n\n    dataset = datasets_cifar10.get_split(split_name, dataset_dir)\n    provider = dataset_data_provider.DatasetDataProvider(\n        dataset,\n        common_queue_capacity=5 * batch_size,\n        common_queue_min=batch_size,\n        shuffle=is_train)\n    [image, label] = provider.get([\'image\', \'label\'])\n    image = tf.to_float(image)\n\n    image_size = 32\n    if is_train:\n      num_threads = 4\n\n      image = tf.image.resize_image_with_crop_or_pad(image, image_size + 4,\n                                                     image_size + 4)\n      image = tf.random_crop(image, [image_size, image_size, 3])\n      image = tf.image.random_flip_left_right(image)\n      # Brightness/saturation/constrast provides small gains .2%~.5% on cifar.\n      # image = tf.image.random_brightness(image, max_delta=63. / 255.)\n      # image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      # image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n    else:\n      num_threads = 1\n\n      image = tf.image.resize_image_with_crop_or_pad(image, image_size,\n                                                     image_size)\n\n    image_not_whiten = image\n    image = tf.image.per_image_standardization(image)\n\n    # Creates a QueueRunner for the pre-fetching operation.\n    images, images_not_whiten, labels = tf.train.batch(\n        [image, image_not_whiten, label],\n        batch_size=batch_size,\n        num_threads=num_threads,\n        capacity=5 * batch_size)\n\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n\n  return (images, images_not_whiten, one_hot_labels, dataset.num_samples,\n          dataset.num_classes)\n'"
cifar_data_provider_test.py,2,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for cifar_data_provider.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport cifar_data_provider\n\n\nclass CifarDataProviderTest(tf.test.TestCase):\n\n  def _testCifar10(self, split_name, expected_num_samples):\n    data_tup = cifar_data_provider.provide_data(\n        split_name, 4, dataset_dir=\'testdata/cifar10\')\n    images, _, one_hot_labels, num_samples, num_classes = data_tup\n\n    self.assertEqual(num_samples, expected_num_samples)\n    self.assertEqual(num_classes, 10)\n    with self.test_session() as sess:\n      with slim.queues.QueueRunners(sess):\n        images_out, one_hot_labels_out = sess.run([images, one_hot_labels])\n        self.assertEqual(images_out.shape, (4, 32, 32, 3))\n        self.assertEqual(one_hot_labels_out.shape, (4, 10))\n\n  def testCifar10TrainSet(self):\n    self._testCifar10(\'train\', 50000)\n\n  def testCifar10TestSet(self):\n    self._testCifar10(\'test\', 10000)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
cifar_main.py,48,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Trains or evaluates a CIFAR ResNet ACT model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport cifar_data_provider\nimport cifar_model\nimport summary_utils\nimport training_utils\nimport utils\n\n\nFLAGS = tf.app.flags.FLAGS\n\n# General settings\ntf.app.flags.DEFINE_string(\'mode\', \'train\', \'One of ""train"" or ""eval"".\')\n\n# Training settings\ntf.app.flags.DEFINE_integer(\'batch_size\', 128,\n                            \'The number of images in each batch.\')\n\ntf.app.flags.DEFINE_string(\'master\', \'\',\n                           \'Name of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\'train_log_dir\', \'/tmp/resnet_act_cifar/\',\n                           \'Directory where to write event logs.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_summaries_secs\', 30,\n    \'The frequency with which summaries are saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_interval_secs\', 60,\n    \'The frequency with which the model is saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\'max_number_of_steps\', 100000,\n                            \'The maximum number of gradient steps.\')\n\ntf.app.flags.DEFINE_integer(\n    \'ps_tasks\', 0,\n    \'The number of parameter servers. If the value is 0, then the parameters \'\n    \'are handled locally by the worker.\')\n\ntf.app.flags.DEFINE_integer(\n    \'task\', 0,\n    \'The Task ID. This value is used when training with multiple workers to \'\n    \'identify each worker.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None,\n    \'Directory with CIFAR-10 data, should contain files \'\n    \'""cifar10_train.tfrecord"" and ""cifar10_test.tfrecord"".\')\n\n# Evaluation settings\ntf.app.flags.DEFINE_string(\'checkpoint_dir\', \'/tmp/resnet_act_cifar/\',\n                           \'Directory where the model was written to.\')\n\ntf.app.flags.DEFINE_string(\'eval_dir\', \'/tmp/resnet_act_cifar/\',\n                           \'Directory where the results are saved to.\')\n\ntf.app.flags.DEFINE_integer(\'eval_batch_size\', 100,\n                            \'The number of images in each batch for evaluation.\')\n\ntf.app.flags.DEFINE_integer(\n    \'eval_interval_secs\', 60,\n    \'The frequency, in seconds, with which evaluation is run.\')\n\ntf.app.flags.DEFINE_string(\'split_name\', \'test\', """"""Either \'train\' or \'test\'."""""")\n\ntf.app.flags.DEFINE_bool(\'evaluate_once\', False, \'Evaluate the model just once?\')\n\n# Model settings\ntf.app.flags.DEFINE_string(\n    \'model_type\', \'vanilla\',\n    \'Options: vanilla (basic ResNet model), act (Adaptive Computation Time), \'\n    \'act_early_stopping (act implementation which actually saves time), \'\n    \'sact (Spatially Adaptive Computation Time)\')\n\ntf.app.flags.DEFINE_float(\'tau\', 1.0, \'The value of tau (ponder relative cost).\')\n\ntf.app.flags.DEFINE_string(\n  \'model\',\n  \'5\',\n  \'An underscore separated string, number of residual units per block. \'\n  \'If only one number is provided, uses the same number of units in all blocks\')\n\ntf.app.flags.DEFINE_string(\'finetune_path\', \'\',\n                           \'Path for the initial checkpoint for finetuning.\')\n\n\ndef train():\n  if not tf.gfile.Exists(FLAGS.train_log_dir):\n    tf.gfile.MakeDirs(FLAGS.train_log_dir)\n\n  g = tf.Graph()\n  with g.as_default():\n    # If ps_tasks is zero, the local device is used. When using multiple\n    # (non-local) replicas, the ReplicaDeviceSetter distributes the variables\n    # across the different devices.\n    with tf.device(tf.train.replica_device_setter(FLAGS.ps_tasks)):\n      data_tuple = cifar_data_provider.provide_data(\n          \'train\', FLAGS.batch_size, dataset_dir=FLAGS.dataset_dir)\n      images, _, one_hot_labels, _, num_classes = data_tuple\n\n      # Define the model:\n      with slim.arg_scope(cifar_model.resnet_arg_scope(is_training=True)):\n        model = utils.split_and_int(FLAGS.model)\n        logits, end_points = cifar_model.resnet(\n            images,\n            model=model,\n            num_classes=num_classes,\n            model_type=FLAGS.model_type)\n\n        # Specify the loss function:\n        tf.losses.softmax_cross_entropy(\n            onehot_labels=one_hot_labels, logits=logits)\n        if FLAGS.model_type in (\'act\', \'act_early_stopping\', \'sact\'):\n          training_utils.add_all_ponder_costs(end_points, weights=FLAGS.tau)\n        total_loss = tf.losses.get_total_loss()\n        tf.summary.scalar(\'Total Loss\', total_loss)\n\n        metric_map = {}  # summary_utils.flops_metric_map(end_points, False)\n        if FLAGS.model_type in (\'act\', \'act_early_stopping\', \'sact\'):\n          metric_map.update(summary_utils.act_metric_map(end_points, False))\n        for name, value in metric_map.iteritems():\n          tf.summary.scalar(name, value)\n\n        if FLAGS.model_type == \'sact\':\n          summary_utils.add_heatmaps_image_summary(end_points)\n\n        init_fn = training_utils.finetuning_init_fn(FLAGS.finetune_path)\n\n        # Specify the optimization scheme:\n        global_step = slim.get_or_create_global_step()\n        # Original LR schedule\n        # boundaries = [40000, 60000, 80000]\n        # ""Longer"" LR schedule\n        boundaries = [60000, 75000, 90000]\n        boundaries = [tf.constant(x, dtype=tf.int64) for x in boundaries]\n        values = [0.1, 0.01, 0.001, 0.0001]\n        learning_rate = tf.train.piecewise_constant(global_step, boundaries,\n                                                    values)\n        tf.summary.scalar(\'Learning Rate\', learning_rate)\n        optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n\n        # Set up training.\n        train_op = slim.learning.create_train_op(total_loss, optimizer)\n\n        if FLAGS.train_log_dir:\n          logdir = FLAGS.train_log_dir\n        else:\n          logdir = None\n\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n\n        # Run training.\n        slim.learning.train(\n            train_op=train_op,\n            init_fn=init_fn,\n            logdir=logdir,\n            master=FLAGS.master,\n            number_of_steps=FLAGS.max_number_of_steps,\n            save_summaries_secs=FLAGS.save_summaries_secs,\n            save_interval_secs=FLAGS.save_interval_secs,\n            session_config=config)\n\n\ndef evaluate():\n  g = tf.Graph()\n  with g.as_default():\n    data_tuple = cifar_data_provider.provide_data(FLAGS.split_name,\n                                                  FLAGS.eval_batch_size,\n                                                  dataset_dir=FLAGS.dataset_dir)\n    images, _, one_hot_labels, num_samples, num_classes = data_tuple\n\n    # Define the model:\n    with slim.arg_scope(cifar_model.resnet_arg_scope(is_training=False)):\n      model = utils.split_and_int(FLAGS.model)\n      logits, end_points = cifar_model.resnet(\n          images,\n          model=model,\n          num_classes=num_classes,\n          model_type=FLAGS.model_type)\n\n      predictions = tf.argmax(logits, 1)\n\n      tf.losses.softmax_cross_entropy(\n          onehot_labels=one_hot_labels, logits=logits)\n      if FLAGS.model_type in (\'act\', \'act_early_stopping\', \'sact\'):\n        training_utils.add_all_ponder_costs(end_points, weights=FLAGS.tau)\n\n      loss = tf.losses.get_total_loss()\n\n      # Define the metrics:\n      labels = tf.argmax(one_hot_labels, 1)\n      metric_map = {\n          \'eval/Accuracy\':\n                tf.contrib.metrics.streaming_accuracy(predictions, labels),\n          \'eval/Mean Loss\':\n                tf.contrib.metrics.streaming_mean(loss),\n      }\n      metric_map.update(summary_utils.flops_metric_map(end_points, True))\n      if FLAGS.model_type in (\'act\', \'act_early_stopping\', \'sact\'):\n        metric_map.update(summary_utils.act_metric_map(end_points, True))\n      names_to_values, names_to_updates = tf.contrib.metrics.aggregate_metric_map(\n          metric_map)\n\n      for name, value in names_to_values.iteritems():\n        summ = tf.summary.scalar(name, value, collections=[])\n        summ = tf.Print(summ, [value], name)\n        tf.add_to_collection(tf.GraphKeys.SUMMARIES, summ)\n\n      if FLAGS.model_type == \'sact\':\n        summary_utils.add_heatmaps_image_summary(end_points)\n\n      # This ensures that we make a single pass over all of the data.\n      num_batches = math.ceil(num_samples / float(FLAGS.eval_batch_size))\n\n      if not FLAGS.evaluate_once:\n        eval_function = slim.evaluation.evaluation_loop\n        checkpoint_path = FLAGS.checkpoint_dir\n        eval_kwargs = {\'eval_interval_secs\': FLAGS.eval_interval_secs}\n      else:\n        eval_function = slim.evaluation.evaluate_once\n        checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n        assert checkpoint_path is not None\n        eval_kwargs = {}\n\n      config = tf.ConfigProto()\n      config.gpu_options.allow_growth = True\n\n      eval_function(\n          FLAGS.master,\n          checkpoint_path,\n          logdir=FLAGS.eval_dir,\n          num_evals=num_batches,\n          eval_op=names_to_updates.values(),\n          session_config=config,\n          **eval_kwargs)\n\n\ndef main(_):\n  if FLAGS.mode == \'train\':\n    train()\n  elif FLAGS.mode == \'eval\':\n    evaluate()\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
cifar_model.py,7,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n"""""" Adaptive computation time residual network for CIFAR-10.\n\nThe code is based on https://github.com/tensorflow/models/blob/master/resnet/resnet_model.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib import slim\nfrom tensorflow.contrib.slim.nets import resnet_utils\n\nimport flopsometer\nimport resnet_act\n\n\ndef lrelu(x, leakiness=0.1):\n  return tf.maximum(x, x * leakiness)\n\n\ndef residual(inputs,\n             depth,\n             stride,\n             activate_before_residual,\n             residual_mask=None,\n             scope=None):\n  with tf.variable_scope(scope, \'residual\', [inputs]):\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    preact = slim.batch_norm(inputs, scope=\'preact\')\n    if activate_before_residual:\n      shortcut = preact\n    else:\n      shortcut = inputs\n\n    if residual_mask is not None:\n      # Max-pooling trick only works correctly when stride is 1.\n      # We assume that stride=2 happens in the first layer where\n      # residual_mask is None.\n      assert stride == 1\n      diluted_residual_mask = slim.max_pool2d(\n          residual_mask, [3, 3], stride=1, padding=\'SAME\')\n    else:\n      diluted_residual_mask = None\n\n    flops = 0\n    conv_output, current_flops = flopsometer.conv2d(\n        preact,\n        depth,\n        3,\n        stride=stride,\n        padding=\'SAME\',\n        output_mask=diluted_residual_mask,\n        scope=\'conv1\')\n    flops += current_flops\n\n    conv_output, current_flops = flopsometer.conv2d(\n        conv_output,\n        depth,\n        3,\n        stride=1,\n        padding=\'SAME\',\n        activation_fn=None,\n        normalizer_fn=None,\n        output_mask=residual_mask,\n        scope=\'conv2\')\n    flops += current_flops\n\n    if depth_in != depth:\n      shortcut = slim.avg_pool2d(shortcut, stride, stride, padding=\'VALID\')\n      value = (depth - depth_in) // 2\n      shortcut = tf.pad(shortcut, [[0, 0], [0, 0], [0, 0], [value, value]])\n\n    if residual_mask is not None:\n      conv_output *= residual_mask\n\n    outputs = shortcut + conv_output\n\n    return outputs, flops\n\n\ndef resnet(inputs,\n           model,\n           num_classes,\n           model_type=\'vanilla\',\n           base_channels=16,\n           scope=\'resnet_residual\'):\n  """"""Builds a CIFAR-10 resnet model.""""""\n  num_blocks = 3\n  num_units = model\n  if len(num_units) == 1:\n    num_units *= num_blocks\n  assert len(num_units) == num_blocks\n\n  b = resnet_utils.Block\n  bc = base_channels\n  blocks = [\n    b(\'block_1\', residual,\n      [(bc, 1, True)] + [(bc, 1, False)] * (num_units[0] - 1)),\n    b(\'block_2\', residual,\n      [(2 * bc, 2, False)] + [(2 * bc, 1, False)] * (num_units[1] - 1)),\n    b(\'block_3\', residual,\n      [(4 * bc, 2, False)] + [(4 * bc, 1, False)] * (num_units[2] - 1))\n  ]\n\n  with tf.variable_scope(scope, [inputs]):\n    end_points = {\'inputs\': inputs}\n    end_points[\'flops\'] = 0\n    net = inputs\n    net, current_flops = flopsometer.conv2d(\n        net, bc, 3, activation_fn=None, normalizer_fn=None)\n    end_points[\'flops\'] += current_flops\n    net, end_points = resnet_act.stack_blocks(\n        net,\n        blocks,\n        model_type=model_type,\n        end_points=end_points)\n    net = tf.reduce_mean(net, [1, 2], keep_dims=True)\n    net = slim.batch_norm(net)\n    net, current_flops = flopsometer.conv2d(\n        net,\n        num_classes, [1, 1],\n        activation_fn=None,\n        normalizer_fn=None,\n        scope=\'logits\')\n    end_points[\'flops\'] += current_flops\n    net = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n\n    return net, end_points\n\n\ndef resnet_arg_scope(is_training=True):\n  """"""Sets up the default arguments for the CIFAR-10 resnet model.""""""\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'decay\': 0.9,\n      \'epsilon\': 0.001,\n      \'scale\': True,\n      # This forces batch_norm to compute the moving averages in-place\n      # instead of using a global collection which does not work with tf.cond.\n      # \'updates_collections\': None,\n  }\n\n  with slim.arg_scope([slim.conv2d, slim.batch_norm], activation_fn=lrelu):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_regularizer=slim.l2_regularizer(0.0002),\n        weights_initializer=slim.variance_scaling_initializer(),\n        normalizer_fn=slim.batch_norm,\n        normalizer_params=batch_norm_params):\n      with slim.arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n        return arg_sc\n'"
cifar_model_test.py,14,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for resnet_model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport cifar_model\nimport summary_utils\nimport training_utils\n\n\nclass CifarModelTest(tf.test.TestCase):\n\n  def _runBatch(self, is_training, model_type, model=[2]):\n    batch_size = 2\n    height, width = 32, 32\n    num_classes = 10\n\n    with slim.arg_scope(\n        cifar_model.resnet_arg_scope(is_training=is_training)):\n      with self.test_session() as sess:\n        images = tf.random_uniform((batch_size, height, width, 3))\n        logits, end_points = cifar_model.resnet(\n            images,\n            model=model,\n            num_classes=num_classes,\n            model_type=model_type,\n            base_channels=1)\n        if model_type in (\'act\', \'act_early_stopping\', \'sact\'):\n          metrics = summary_utils.act_metric_map(end_points,\n              not is_training)\n          metrics.update(summary_utils.flops_metric_map(end_points,\n              not is_training))\n        else:\n          metrics = {}\n\n        if is_training:\n          labels = tf.random_uniform(\n              (batch_size,), maxval=num_classes, dtype=tf.int32)\n          one_hot_labels = slim.one_hot_encoding(labels, num_classes)\n          tf.losses.softmax_cross_entropy(\n              onehot_labels=one_hot_labels, logits=logits)\n          if model_type in (\'act\', \'act_early_stopping\', \'sact\'):\n            training_utils.add_all_ponder_costs(end_points, weights=1.0)\n          total_loss = tf.losses.get_total_loss()\n          optimizer = tf.train.MomentumOptimizer(0.1, 0.9)\n          train_op = slim.learning.create_train_op(total_loss, optimizer)\n          sess.run(tf.global_variables_initializer())\n          sess.run((train_op, metrics))\n        else:\n          sess.run([tf.local_variables_initializer(),\n              tf.global_variables_initializer()])\n          logits_out, metrics_out = sess.run((logits, metrics))\n          self.assertEqual(logits_out.shape, (batch_size, num_classes))\n\n  def testTrainVanilla(self):\n    self._runBatch(is_training=True, model_type=\'vanilla\')\n\n  def testTrainAct(self):\n    self._runBatch(is_training=True, model_type=\'act\')\n\n  def testTrainSact(self):\n    self._runBatch(is_training=True, model_type=\'sact\')\n\n  def testTestVanilla(self):\n    self._runBatch(is_training=False, model_type=\'vanilla\')\n\n  def testTestVanillaResidualUnits(self):\n    self._runBatch(\n        is_training=False, model_type=\'vanilla\', model=[1, 2, 3])\n\n  def testTestAct(self):\n    self._runBatch(is_training=False, model_type=\'act\')\n\n  def testTestSact(self):\n    self._runBatch(is_training=False, model_type=\'sact\')\n\n  def testFlopsVanilla(self):\n    batch_size = 3\n    height, width = 32, 32\n    num_classes = 10\n\n    with slim.arg_scope(cifar_model.resnet_arg_scope(is_training=False)):\n      with self.test_session() as sess:\n        images = tf.random_uniform((batch_size, height, width, 3))\n        _, end_points = cifar_model.resnet(\n            images,\n            model=[18],\n            num_classes=num_classes,\n            model_type=\'vanilla\')\n        flops = sess.run(end_points[\'flops\'])\n        # TF graph_metrics value: 506307850 (0.1% difference)\n        expected_flops = 505775360\n        self.assertAllEqual(flops, [expected_flops] * 3)\n\n  def testVisualizationBasic(self):\n    batch_size = 3\n    height, width = 32, 32\n    num_classes = 10\n    is_training = False\n    num_images = 2\n    border = 5\n\n    with slim.arg_scope(cifar_model.resnet_arg_scope(is_training=is_training)):\n      with self.test_session() as sess:\n        images = tf.random_uniform((batch_size, height, width, 3))\n        logits, end_points = cifar_model.resnet(\n            images,\n            model=[2],\n            num_classes=num_classes,\n            model_type=\'sact\',\n            base_channels=1)\n\n        vis_ponder = summary_utils.sact_image_heatmap(\n            end_points,\n            \'ponder_cost\',\n            num_images=num_images,\n            alpha=0.75,\n            border=border)\n        vis_units = summary_utils.sact_image_heatmap(\n            end_points,\n            \'num_units\',\n            num_images=num_images,\n            alpha=0.75,\n            border=border)\n\n        sess.run(tf.global_variables_initializer())\n        vis_ponder_out, vis_units_out = sess.run(\n            [vis_ponder, vis_units])\n        self.assertEqual(vis_ponder_out.shape,\n                         (num_images, height, width * 2 + border, 3))\n        self.assertEqual(vis_units_out.shape,\n                         (num_images, height, width * 2 + border, 3))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
draw_ponder_maps.py,4,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n""""""Draws example ponder cost maps""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport h5py\nimport matplotlib\nmatplotlib.use(\'agg\')  # disables drawing to X\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'input_file\', None,\n                           \'An HDF5 file produced by imagenet_export.\')\n\ntf.app.flags.DEFINE_string(\'output_dir\', None,\n                           \'The directory to output the plotted ponder maps to.\')\n\n\ndef main(_):\n  f = h5py.File(FLAGS.input_file, \'r\')\n\n  num_images = f[\'images\'].shape[0]\n  ponder_cost = np.array(f[\'ponder_cost_map\'])\n  min_ponder = np.percentile(ponder_cost.ravel(), 0.1)\n  max_ponder = np.percentile(ponder_cost.ravel(), 99.9)\n  print(\'0.1st percentile of ponder cost {:.2f} \'.format(min_ponder))\n  print(\'99.9th percentile of ponder cost {:.2f} \'.format(max_ponder))\n\n  fig = plt.figure(figsize=(0.2, 2))\n  ax = fig.add_axes([0.0, 0.0, 1.0, 1.0])\n  cb = matplotlib.colorbar.ColorbarBase(\n      ax, cmap=\'viridis\',\n      norm=matplotlib.colors.Normalize(vmin=min_ponder, vmax=max_ponder))\n  ax.tick_params(labelsize=12)\n  filename = os.path.join(FLAGS.output_dir, \'colorbar.pdf\')\n  plt.savefig(filename, bbox_inches=\'tight\')\n\n  for i in range(num_images):\n    current_map = np.squeeze(f[\'ponder_cost_map\'][i])\n    mean_ponder = np.mean(current_map)\n    filename = \'{}/{:.2f}_{}_ponder.png\'.format(FLAGS.output_dir, mean_ponder, i)\n    matplotlib.image.imsave(\n        filename, current_map, cmap=\'viridis\', vmin=min_ponder, vmax=max_ponder)\n\n    im = f[\'images\'][i]\n    im = (im + 1.0) / 2.0\n    filename = \'{}/{:.2f}_{}_im.jpg\'.format(FLAGS.output_dir, mean_ponder, i)\n    matplotlib.image.imsave(filename, im)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
fake_cifar10.py,4,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Script to generate random data of the same format as CIFAR-10.\n\nCreates TFRecord files with the same fields as\ntensorflow/models/slim/datasets/downlod_and_convert_cifar10.py\nfor use in unit tests of the code that handles this data.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport StringIO\n\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\n\nfrom tensorflow_models.slim.datasets import dataset_utils\n\ntf.app.flags.DEFINE_string(\'out_directory\', \'testdata/cifar10\',\n                       \'Output directory for the test data.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\n_IMAGE_SIZE = 32\n\n\ndef create_fake_data(split_name, num_examples=4):\n  """"""Writes the fake TFRecords for one split of the dataset.\n\n  Args:\n    split_name: One of \'train\' or \'test\'.\n    num_examples: The number of random examples to generate and write to the\n                  output TFRecord file.\n  """"""\n  output_file = os.path.join(FLAGS.out_directory,\n                             \'cifar10_%s.tfrecord\' % split_name)\n  writer = tf.python_io.TFRecordWriter(output_file)\n  for _ in range(num_examples):\n    image = np.random.randint(256, size=(_IMAGE_SIZE, _IMAGE_SIZE, 3),\n                              dtype=np.uint8)\n    image = Image.fromarray(image)\n    image_buffer = StringIO.StringIO()\n    image.save(image_buffer, format=\'png\')\n    image_buffer = image_buffer.getvalue()\n\n    label = 0\n    example = dataset_utils.image_to_tfexample(\n        image_buffer, \'png\', _IMAGE_SIZE, _IMAGE_SIZE, label)\n    writer.write(example.SerializeToString())\n  writer.close()\n\n\ndef main(_):\n  create_fake_data(\'train\')\n  create_fake_data(\'test\')\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
fake_imagenet.py,4,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Script to generate random data of the same format as ImageNet.\n\nCreates TFRecord files with the same fields as\ntensorflow/models/inception/inception/build_imagenet_data\nfor use in unit tests of the code that handles this data.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport StringIO\n\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\n\nfrom inception.inception.data import build_imagenet_data\n\n\ntf.app.flags.DEFINE_string(\'out_directory\', \'testdata/imagenet\',\n                       \'Output directory for the test data.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _random_bounds(n):\n  x1, x2 = tuple(np.random.randint(n + 1, size=(2,)) / n)\n  return min(x1, x2), max(x1, x2)\n\n\ndef _random_bbox(image_width, image_height):\n  xmin, xmax = _random_bounds(image_width)\n  ymin, ymax = _random_bounds(image_height)\n  return [xmin, ymin, xmax, ymax]\n\n\ndef create_fake_data(split_name, image_width=640, image_height=480):\n  """"""Generates the fake data for a given ImageNet split.\n\n  Args:\n    split_name: One of \'train\' or \'valdiation\'.\n    image_width: The width of the random image to generate and write as an\n                 integer.\n    image_height: Integer height o fthe random image.\n  """"""\n  filename = \'/tmp/fake_%s.jpg\' % split_name\n\n  image = np.random.randint(256, size=(image_height, image_width, 3),\n                            dtype=np.uint8)\n  image = Image.fromarray(image)\n  image_buffer = StringIO.StringIO()\n  image.save(image_buffer, format=\'jpeg\')\n  image_buffer = image_buffer.getvalue()\n\n  bboxes = [_random_bbox(image_width, image_height)]\n\n  output_file = os.path.join(FLAGS.out_directory,\n                             \'%s-00000-of-00001\' % split_name)\n  writer = tf.python_io.TFRecordWriter(output_file)\n  # pylint: disable=protected-access\n  example = build_imagenet_data._convert_to_example(\n      filename, image_buffer, 0, \'n02110341\', \'dalmation\', bboxes,\n      image_height, image_width)\n  # pylint: enable=protected-access\n  writer.write(example.SerializeToString())\n  writer.close()\n\n\ndef main(_):\n  create_fake_data(\'train\')\n  create_fake_data(\'validation\')\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
flopsometer.py,8,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Measures FLOPS in convolution layers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib import slim\nfrom tensorflow.contrib.layers.python.layers import utils\n\n\ndef conv2d(inputs, num_outputs, kernel_size, *args, **kwargs):\n  """"""A wrapper/substitute for conv2d that counts the flops.\n\n  This counts the number of floating-point operations (flops) for a conv2d\n  layer, including one with a ""mask."" The optional keyword argument\n  `output_mask` specifies which of the position in the output response map need\n  actually be calculated, the rest can be discarded and are not counted in the\n  result.\n\n  Since this is a wrapper around slim.conv2d, see that function for details on\n  the inputs/outputs.\n\n  Args:\n    inputs:      The input response map to the convolution.\n    num_outputs: The number of output channels for the convolution.\n    kernel_size: Spatial size of the convolution kernel.\n    *args:       Additional position arguments forwarded to slim.conv2d.\n    **kwargs:    Additional keyword args forwarded to slim.conv2d.\n  Returns:\n    outputs:     The result of the convolution from slim.conv2d.\n    flops:       The operation count as a scalar integer tensor.\n  """"""\n  output_mask = kwargs.pop(\'output_mask\', None)\n\n  outputs = slim.conv2d(inputs, num_outputs, kernel_size, *args, **kwargs)\n\n  if inputs.get_shape().is_fully_defined():\n    inputs_shape = inputs.get_shape().as_list()\n    outputs_shape = outputs.get_shape().as_list()\n  else:\n    inputs_shape = tf.to_int64(tf.shape(inputs))\n    outputs_shape = tf.to_int64(tf.shape(outputs))\n  batch_size = outputs_shape[0]\n\n  num_filters_in = inputs_shape[3]\n  kernel_h, kernel_w = utils.two_element_tuple(kernel_size)\n  if output_mask is None:\n    num_spatial_positions = tf.fill(\n        # tf.fill does not support int64 dims :-|\n        dims=tf.to_int32(tf.stack([batch_size])),\n        value=outputs_shape[1] * outputs_shape[2])\n  else:\n    num_spatial_positions = tf.reduce_sum(output_mask, [1, 2])\n  num_spatial_positions = tf.to_int64(num_spatial_positions)\n\n  num_output_positions = num_spatial_positions * num_outputs\n  flops = 2 * num_output_positions * (kernel_h * kernel_w * num_filters_in)\n\n  # The numbers are slightly different than TensorFlow graph_metrics since we\n  # ignore biases. We do not try to mimic graph_metrics because it is\n  # inconsistent in the treatment of biases (batch_norm makes biases ""free"").\n  return outputs, flops\n\n\ndef conv2d_same(inputs,\n                num_outputs,\n                kernel_size,\n                stride,\n                rate=1,\n                output_mask=None,\n                scope=None):\n  """"""Version of TF-Slim resnet_utils.conv2d_same that uses the flopsometer.""""""\n  if stride == 1:\n    return conv2d(\n        inputs,\n        num_outputs,\n        kernel_size,\n        stride=1,\n        rate=rate,\n        padding=\'SAME\',\n        output_mask=output_mask,\n        scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return conv2d(\n        inputs,\n        num_outputs,\n        kernel_size,\n        stride=stride,\n        rate=rate,\n        padding=\'VALID\',\n        output_mask=output_mask,\n        scope=scope)\n'"
flopsometer_test.py,11,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for flopsometer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nimport flopsometer\n\n\nclass FlopsometerTest(tf.test.TestCase):\n\n  def testConv2d(self):\n    inputs = tf.zeros([2, 16, 16, 4])\n    _, flops = flopsometer.conv2d(\n        inputs, 8, [3, 3], stride=1, padding=\'SAME\', output_mask=None)\n    expected_flops = 2 * 16 * 16 * 3 * 3 * 8 * 4\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      flops_out = sess.run(flops)\n      self.assertAllEqual(flops_out, [expected_flops, expected_flops])\n\n  def testConv2dUnknownSize(self):\n    inputs = np.zeros([2, 16, 16, 4], dtype=np.float32)\n    inputs_tf = tf.placeholder(tf.float32, shape=(2, None, None, 4))\n    _, flops = flopsometer.conv2d(\n        inputs_tf, 8, [3, 3], stride=1, padding=\'SAME\', output_mask=None)\n    expected_flops = 2 * 16 * 16 * 3 * 3 * 8 * 4\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      flops_out = sess.run(flops, feed_dict={inputs_tf: inputs})\n      self.assertAllEqual(flops_out, [expected_flops, expected_flops])\n\n  def testConv2dStride(self):\n    inputs = tf.zeros([2, 16, 16, 4])\n    _, flops = flopsometer.conv2d(\n        inputs, 8, [3, 3], stride=2, padding=\'SAME\', output_mask=None)\n    output_positions = 8 * 8\n    expected_flops = 2 * output_positions * 3 * 3 * 8 * 4\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      flops_out = sess.run(flops)\n      self.assertAllEqual(flops_out, [expected_flops, expected_flops])\n\n  def testConv2dOutputMask(self):\n    inputs = tf.zeros([2, 16, 16, 4])\n    mask = np.random.random([2, 16, 16]) <= 0.6\n    mask_tf = tf.constant(np.float32(mask))\n    _, flops = flopsometer.conv2d(\n        inputs, 8, [3, 3], stride=1, padding=\'SAME\', output_mask=mask_tf)\n\n    per_position_flops = 2 * 3 * 3 * 8 * 4\n    num_positions = np.sum(np.sum(np.int32(mask), 2), 1)\n    expected_flops = [\n        per_position_flops * num_positions[0],\n        per_position_flops * num_positions[1]\n    ]\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      flops_out = sess.run(flops)\n      self.assertAllEqual(flops_out, expected_flops)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
imagenet_data_provider.py,4,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Contains code for loading and preprocessing the ImageNet data.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib import slim\nfrom tensorflow.contrib.slim import dataset_data_provider\n\nfrom external import inception_preprocessing\nfrom external import datasets_imagenet\n\n\ndef provide_data(split_name, batch_size, dataset_dir=None, is_training=False,\n                 num_readers=4, num_preprocessing_threads=4, image_size=224):\n  """"""Provides batches of Imagenet data.\n\n  Applies the processing in external/inception_preprocessing\n  to the TF-Slim ImageNet dataset class.\n\n  Args:\n    split_name: Either \'train\' or \'validation\'.\n    batch_size: The number of images in each batch.\n    dataset_dir: Directory where the ImageNet TFRecord files live.\n                 Defaults to ""~/tensorflow/data/imagenet""\n    is_training: Whether to apply data augmentation and shuffling.\n    num_readers: Number of parallel readers. Always set to one for evaluation.\n    num_preprocessing_threads: Number of preprocessing threads.\n\n  Returns:\n    images: A `Tensor` of size [batch_size, image_size, image_size, 3]\n    one_hot_labels: A `Tensor` of size [batch_size, num_classes], where\n      each row has a single element set to one and the rest set to zeros.\n    dataset.num_samples: The number of total samples in the dataset.\n    dataset.num_classes: The number of object classes in the dataset.\n\n  Raises:\n    ValueError: if the split_name is not either \'train\' or \'validation\'.\n  """"""\n\n  with tf.device(\'/cpu:0\'):\n    if dataset_dir is None:\n      dataset_dir = os.path.expanduser(\'~/tensorflow/data/imagenet\')\n\n    if not is_training:\n      num_readers = 1\n\n    dataset = datasets_imagenet.get_split(split_name, dataset_dir)\n    provider = dataset_data_provider.DatasetDataProvider(\n        dataset,\n        num_readers=num_readers,\n        shuffle=is_training,\n        common_queue_capacity=5 * batch_size,\n        common_queue_min=batch_size)\n\n    [image, bbox, label] = provider.get([\'image\', \'object/bbox\', \'label\'])\n    bbox = tf.expand_dims(bbox, 0)\n\n    image = inception_preprocessing.preprocess_image(\n      image, image_size, image_size, is_training, bbox, fast_mode=False)\n\n    images, labels = tf.train.batch(\n        [image, label],\n        batch_size=batch_size,\n        num_threads=num_preprocessing_threads,\n        capacity=5 * batch_size)\n\n    one_hot_labels = tf.one_hot(labels, dataset.num_classes)\n\n  return images, one_hot_labels, dataset.num_samples, dataset.num_classes\n'"
imagenet_data_provider_test.py,2,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for imagenet_data_provider.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport imagenet_data_provider\n\n\nclass ImagenetDataProviderTest(tf.test.TestCase):\n\n  def _testImageNet(self, split_name, is_training, expected_num_samples):\n    images, one_hot_labels, num_samples, num_classes = \\\n        imagenet_data_provider.provide_data(split_name, 1,\n                                            dataset_dir=\'testdata/imagenet\',\n                                            is_training=is_training)\n    self.assertEqual(num_samples, expected_num_samples)\n    self.assertEqual(num_classes, 1001)\n    with self.test_session() as sess:\n      with slim.queues.QueueRunners(sess):\n        images_out, one_hot_labels_out = sess.run([images, one_hot_labels])\n        self.assertEqual(images_out.shape, (1, 224, 224, 3))\n        self.assertEqual(one_hot_labels_out.shape, (1, 1001))\n\n  def testImageNetTrainSet(self):\n    self._testImageNet(\'train\', True, 1281167)\n\n  def testImageNetValidationSet(self):\n    self._testImageNet(\'validation\', False, 50000)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
imagenet_eval.py,27,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Evaluates a trained ResNet model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport imagenet_data_provider\nimport imagenet_model\nimport summary_utils\nimport utils\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'master\', \'\',\n                           \'Name of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\'checkpoint_dir\', \'/tmp/resnet/\',\n                           \'Directory where the model was written to.\')\n\ntf.app.flags.DEFINE_string(\'eval_dir\', \'/tmp/resnet/\',\n                           \'Directory where the results are saved to.\')\n\ntf.app.flags.DEFINE_string(\'dataset_dir\', None, \'Directory with Imagenet data.\')\n\ntf.app.flags.DEFINE_integer(\'eval_interval_secs\', 600,\n                            \'The frequency, in seconds, with which evaluation is run.\')\n\ntf.app.flags.DEFINE_integer(\'num_examples\', 50000,\n                            \'The number of examples to evaluate\')\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 32,\n    \'The number of examples to evaluate per evaluation iteration.\')\n\ntf.app.flags.DEFINE_string(\n    \'split_name\', \'validation\',\n    \'The name of the train/test split, either \\\'train\\\' or \\\'validation\\\'.\')\n\ntf.app.flags.DEFINE_float(\'moving_average_decay\', 0.9999,\n                          \'The decay to use for the moving average.\')\n\ntf.app.flags.DEFINE_integer(\'image_size\', 224,\n                            \'Image resolution for resize.\')\n\ntf.app.flags.DEFINE_string(\n    \'model\', \'101\',\n    \'Depth of the network to train (50, 101, 152, 200), or number of layers\'\n    \' in each block (e.g. 3_4_23_3).\')\n\ntf.app.flags.DEFINE_string(\n    \'model_type\', \'vanilla\',\n    \'Options: vanilla (basic ResNet model), act (Adaptive Computation Time), \'\n    \'act_early_stopping (act implementation which actually saves time), \'\n    \'sact (Spatially Adaptive Computation Time)\')\n\ntf.app.flags.DEFINE_float(\'tau\', 1.0, \'The value of tau (ponder relative cost).\')\n\ntf.app.flags.DEFINE_bool(\'evaluate_once\', False, \'Evaluate the model just once?\')\n\n\ndef main(_):\n  g = tf.Graph()\n  with g.as_default():\n    data_tuple = imagenet_data_provider.provide_data(\n        FLAGS.split_name,\n        FLAGS.batch_size,\n        dataset_dir=FLAGS.dataset_dir,\n        is_training=False,\n        image_size=FLAGS.image_size)\n    images, one_hot_labels, examples_per_epoch, num_classes = data_tuple\n\n    # Define the model:\n    with slim.arg_scope(imagenet_model.resnet_arg_scope(is_training=False)):\n      model = utils.split_and_int(FLAGS.model)\n      logits, end_points = imagenet_model.get_network(\n          images,\n          model,\n          num_classes,\n          model_type=FLAGS.model_type)\n\n      predictions = tf.argmax(end_points[\'predictions\'], 1)\n\n      # Define the metrics:\n      labels = tf.argmax(one_hot_labels, 1)\n      metric_map = {\n          \'eval/Accuracy\':\n              tf.contrib.metrics.streaming_accuracy(predictions, labels),\n          \'eval/Recall@5\':\n              tf.contrib.metrics.streaming_sparse_recall_at_k(\n                  end_points[\'predictions\'], tf.expand_dims(labels, 1), 5),\n      }\n      metric_map.update(summary_utils.flops_metric_map(end_points, True))\n      if FLAGS.model_type in [\'act\', \'act_early_stopping\', \'sact\']:\n        metric_map.update(summary_utils.act_metric_map(end_points, True))\n\n      names_to_values, names_to_updates = tf.contrib.metrics.aggregate_metric_map(\n          metric_map)\n\n      for name, value in names_to_values.iteritems():\n        summ = tf.summary.scalar(name, value, collections=[])\n        summ = tf.Print(summ, [value], name)\n        tf.add_to_collection(tf.GraphKeys.SUMMARIES, summ)\n\n      if FLAGS.model_type == \'sact\':\n        summary_utils.add_heatmaps_image_summary(end_points, border=10)\n\n      # This ensures that we make a single pass over all of the data.\n      num_batches = math.ceil(FLAGS.num_examples / float(FLAGS.batch_size))\n\n      if not FLAGS.evaluate_once:\n        eval_function = slim.evaluation.evaluation_loop\n        checkpoint_path = FLAGS.checkpoint_dir\n        kwargs = {\'eval_interval_secs\': FLAGS.eval_interval_secs}\n      else:\n        eval_function = slim.evaluation.evaluate_once\n        checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n        assert checkpoint_path is not None\n        kwargs = {}\n\n      eval_function(\n          FLAGS.master,\n          checkpoint_path,\n          logdir=FLAGS.eval_dir,\n          num_evals=num_batches,\n          eval_op=names_to_updates.values(),\n          **kwargs)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
imagenet_export.py,11,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Exports data about a trained ResNet-ACT/SACT model into a HDF5 file.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport imagenet_data_provider\nimport imagenet_model\nimport summary_utils\nimport utils\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_integer(\n    \'num_examples\', 1000,\n    \'The number of examples to evaluate\')\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 32,\n    \'The number of examples to evaluate per evaluation iteration.\')\n\ntf.app.flags.DEFINE_string(\n    \'split_name\', \'validation\',\n    \'The name of the train/test split, either \\\'train\\\' or \\\'validation\\\'.\')\n\ntf.app.flags.DEFINE_string(\n    \'model\', \'101\',\n    \'Depth of the network to train (50, 101, 152, 200), or number of layers\'\n    \' in each block (e.g. 3_4_23_3).\')\n\ntf.app.flags.DEFINE_string(\n    \'model_type\', \'vanilla\',\n    \'Options: act (Adaptive Computation Time), \'\n    \'act_early_stopping (act implementation which actually saves time), \'\n    \'sact (Spatially Adaptive Computation Time)\')\n\ntf.app.flags.DEFINE_string(\'checkpoint_dir\', \'\',\n                           \'Directory with the checkpoints.\')\n\ntf.app.flags.DEFINE_string(\'export_path\', \'\',\n                           \'Path to write the hdf5 file with exported data.\')\n\ntf.app.flags.DEFINE_string(\'dataset_dir\', None, \'Directory with Imagenet data.\')\n\n\ndef main(_):\n  assert FLAGS.model_type in (\'act\', \'act_early_stopping\', \'sact\')\n\n  g = tf.Graph()\n  with g.as_default():\n    data_tuple = imagenet_data_provider.provide_data(\n        FLAGS.split_name,\n        FLAGS.batch_size,\n        dataset_dir=FLAGS.dataset_dir,\n        is_training=False)\n    images, labels, _, num_classes = data_tuple\n\n    # Define the model:\n    with slim.arg_scope(imagenet_model.resnet_arg_scope(is_training=False)):\n      model = utils.split_and_int(FLAGS.model)\n      logits, end_points = imagenet_model.get_network(\n          images,\n          model,\n          num_classes,\n          model_type=FLAGS.model_type)\n\n      summary_utils.export_to_h5(FLAGS.checkpoint_dir, FLAGS.export_path,\n                                 images, end_points, FLAGS.num_examples,\n                                 FLAGS.batch_size, FLAGS.model_type==\'sact\')\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
imagenet_model.py,8,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Definition of Resnet-ACT model used for imagenet classification.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib import slim\nfrom tensorflow.contrib.slim.nets import resnet_utils\n\nimport act\nimport flopsometer\nimport resnet_act\n\n\ndef bottleneck(inputs,\n               depth,\n               depth_bottleneck,\n               stride,\n               rate=1,\n               residual_mask=None,\n               scope=None):\n  with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n    flops = 0\n\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut, current_flops = flopsometer.conv2d(\n          preact,\n          depth, [1, 1],\n          stride=stride,\n          normalizer_fn=None,\n          activation_fn=None,\n          scope=\'shortcut\')\n      flops += current_flops\n\n    if residual_mask is not None:\n      # Max-pooling trick only works correctly when stride is 1.\n      # We assume that stride=2 happens in the first layer where\n      # residual_mask is None.\n      assert stride == 1\n      diluted_residual_mask = slim.max_pool2d(\n          residual_mask, [3, 3], stride=1, padding=\'SAME\')\n    else:\n      diluted_residual_mask = None\n\n    residual, current_flops = flopsometer.conv2d(\n        preact,\n        depth_bottleneck, [1, 1],\n        stride=1,\n        output_mask=diluted_residual_mask,\n        scope=\'conv1\')\n    flops += current_flops\n\n    residual, current_flops = flopsometer.conv2d_same(\n        residual,\n        depth_bottleneck,\n        3,\n        stride,\n        rate=rate,\n        output_mask=residual_mask,\n        scope=\'conv2\')\n    flops += current_flops\n\n    residual, current_flops = flopsometer.conv2d(\n        residual,\n        depth, [1, 1],\n        stride=1,\n        normalizer_fn=None,\n        activation_fn=None,\n        output_mask=residual_mask,\n        scope=\'conv3\')\n    flops += current_flops\n\n    if residual_mask is not None:\n      residual *= residual_mask\n\n    outputs = shortcut + residual\n\n    return outputs, flops\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              global_pool=True,\n              model_type=\'vanilla\',\n              scope=None,\n              reuse=None,\n              end_points=None):\n  with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n    if end_points is None:\n      end_points = {}\n    end_points[\'inputs\'] = inputs\n    end_points[\'flops\'] = end_points.get(\'flops\', 0)\n    net = inputs\n    # We do not include batch normalization or activation functions in conv1\n    # because the first ResNet unit will perform these. Cf. Appendix of [2].\n    with slim.arg_scope([slim.conv2d], activation_fn=None, normalizer_fn=None):\n      net, current_flops = flopsometer.conv2d_same(\n          net, 64, 7, stride=2, scope=\'conv1\')\n      end_points[\'flops\'] += current_flops\n    net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n    # Early stopping is broken in distributed training.\n    net, end_points = resnet_act.stack_blocks(\n        net,\n        blocks,\n        model_type=model_type,\n        end_points=end_points)\n\n    if global_pool or num_classes is not None:\n      # This is needed because the pre-activation variant does not have batch\n      # normalization or activation functions in the residual unit output. See\n      # Appendix of [2].\n      net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n\n    if global_pool:\n      # Global average pooling.\n      net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n\n    if num_classes is not None:\n      net, current_flops = flopsometer.conv2d(\n          net,\n          num_classes, [1, 1],\n          activation_fn=None,\n          normalizer_fn=None,\n          scope=\'logits\')\n      end_points[\'flops\'] += current_flops\n      end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n    return net, end_points\n\n\ndef resnet_arg_scope(is_training=True):\n  return resnet_utils.resnet_arg_scope(is_training)\n  # with slim.arg_scope(resnet_utils.resnet_arg_scope(is_training)):\n    # # This forces batch_norm to compute the moving averages in-place\n    # # instead of using a global collection which does not work with tf.cond.\n    # with slim.arg_scope([slim.batch_norm], updates_collections=None) as arg_sc:\n    #   return arg_sc\n\n\ndef get_network(images,\n                model,\n                num_classes,\n                model_type=\'vanilla\',\n                global_pool=True,\n                base_channels=64,\n                scope=None,\n                reuse=None,\n                end_points=None):\n  # These settings are *not* compatible with Slim\'s ResNet v2.\n  # In ResNet Slim the downsampling is performed by the last layer of the\n  # current block. Here we perform downsampling in the first layer of the next\n  # block. This is consistent with the ResNet paper.\n  num_blocks = 4\n  if len(model) == 1:\n    standard_networks = {\n        50: [3, 4, 6, 3],\n        101: [3, 4, 23, 3],\n        152: [3, 8, 36, 3],\n        200: [3, 24, 36, 3],\n    }\n    num_units = standard_networks[model[0]]\n  else:\n    num_units = model\n  assert len(num_units) == num_blocks\n\n  b = resnet_utils.Block\n  bc = base_channels\n  blocks = [\n      b(\'block1\', bottleneck, [(4 * bc, bc, 1)] * num_units[0]),\n      b(\'block2\', bottleneck,\n        [(8 * bc, 2 * bc, 2)] + [(8 * bc, 2 * bc, 1)] * (num_units[1] - 1)),\n      b(\'block3\', bottleneck,\n        [(16 * bc, 4 * bc, 2)] + [(16 * bc, 4 * bc, 1)] * (num_units[2] - 1)),\n      b(\'block4\', bottleneck,\n        [(32 * bc, 8 * bc, 2)] + [(32 * bc, 8 * bc, 1)] * (num_units[3] - 1)),\n  ]\n\n  logits, end_points = resnet_v2(\n      images,\n      blocks,\n      num_classes,\n      global_pool=global_pool,\n      model_type=model_type,\n      scope=scope,\n      reuse=reuse,\n      end_points=end_points)\n\n  if num_classes is not None and global_pool:\n    logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n    end_points[\'predictions\'] = tf.squeeze(\n        end_points[\'predictions\'], [1, 2], name=\'SpatialSqueeze\')\n  return logits, end_points\n'"
imagenet_model_test.py,14,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for imagenet_model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport imagenet_model\nimport summary_utils\nimport training_utils\n\n\nclass ImagenetModelTest(tf.test.TestCase):\n\n  def _runBatch(self,\n                is_training,\n                model_type,\n                model=[2, 2, 2, 2]):\n    batch_size = 2\n    height, width = 128, 128\n    num_classes = 10\n\n    with self.test_session() as sess:\n      images = tf.random_uniform((batch_size, height, width, 3))\n      with slim.arg_scope(\n          imagenet_model.resnet_arg_scope(is_training=is_training)):\n        logits, end_points = imagenet_model.get_network(\n            images, model, num_classes, model_type=\'sact\', base_channels=1)\n        if model_type in (\'act\', \'act_early_stopping\', \'sact\'):\n          metrics = summary_utils.act_metric_map(end_points,\n              not is_training)\n          metrics.update(summary_utils.flops_metric_map(end_points,\n              not is_training))\n        else:\n          metrics = {}\n\n      if is_training:\n        labels = tf.random_uniform(\n            (batch_size,), maxval=num_classes, dtype=tf.int32)\n        one_hot_labels = slim.one_hot_encoding(labels, num_classes)\n        tf.losses.softmax_cross_entropy(\n            onehot_labels=one_hot_labels, logits=logits,\n            label_smoothing=0.1, weights=1.0)\n        if model_type in (\'act\', \'act_early_stopping\', \'sact\'):\n          training_utils.add_all_ponder_costs(end_points, weights=1.0)\n        total_loss = tf.losses.get_total_loss()\n        optimizer = tf.train.MomentumOptimizer(0.1, 0.9)\n        train_op = slim.learning.create_train_op(total_loss, optimizer)\n        sess.run(tf.global_variables_initializer())\n        sess.run((train_op, metrics))\n      else:\n        sess.run([tf.local_variables_initializer(),\n            tf.global_variables_initializer()])\n        logits_out, metrics_out = sess.run((logits, metrics))\n        self.assertEqual(logits_out.shape, (batch_size, num_classes))\n\n  def testTrainVanilla(self):\n    self._runBatch(is_training=True, model_type=\'vanilla\')\n\n  def testTrainAct(self):\n    self._runBatch(is_training=True, model_type=\'act\')\n\n  def testTrainSact(self):\n    self._runBatch(is_training=True, model_type=\'sact\')\n\n  def testTestVanilla(self):\n    self._runBatch(is_training=False, model_type=\'vanilla\')\n\n  def testTestAct(self):\n    self._runBatch(is_training=False, model_type=\'act\')\n\n  def testTestSact(self):\n    self._runBatch(is_training=False, model_type=\'sact\')\n\n  def testTestResNet50Model(self):\n    self._runBatch(is_training=False, model_type=\'vanilla\', model=[50])\n\n  def testFlopsVanilla(self):\n    batch_size = 3\n    height, width = 224, 224\n    num_classes = 1001\n\n    with self.test_session() as sess:\n      images = tf.random_uniform((batch_size, height, width, 3))\n      with slim.arg_scope(imagenet_model.resnet_arg_scope(is_training=False)):\n        _, end_points = imagenet_model.get_network(\n            images, [101], num_classes, \'vanilla\')\n        flops = sess.run(end_points[\'flops\'])\n        # TF graph_metrics value: 15614055401 (0.1% difference)\n        expected_flops = 15602814976\n        self.assertAllEqual(flops, [expected_flops] * 3)\n\n  def testVisualizationBasic(self):\n    batch_size = 5\n    height, width = 128, 128\n    num_classes = 10\n    is_training = False\n    num_images = 3\n    border = 5\n\n    with self.test_session() as sess:\n      images = tf.random_uniform((batch_size, height, width, 3))\n      with slim.arg_scope(imagenet_model.resnet_arg_scope(is_training=is_training)):\n        logits, end_points = imagenet_model.get_network(\n            images, [2, 2, 2, 2], num_classes, model_type=\'sact\',\n            base_channels=1)\n\n        vis_ponder = summary_utils.sact_image_heatmap(\n            end_points,\n            \'ponder_cost\',\n            num_images=num_images,\n            alpha=0.75,\n            border=border)\n        vis_units = summary_utils.sact_image_heatmap(\n            end_points,\n            \'num_units\',\n            num_images=num_images,\n            alpha=0.75,\n            border=border)\n\n        sess.run(tf.global_variables_initializer())\n        vis_ponder_out, vis_units_out = sess.run(\n            [vis_ponder, vis_units])\n        self.assertEqual(vis_ponder_out.shape,\n                         (num_images, height, width * 2 + border, 3))\n        self.assertEqual(vis_units_out.shape,\n                         (num_images, height, width * 2 + border, 3))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
imagenet_ponder_map.py,30,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Exports ponder cost maps for input images.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport math\nimport os\n\nimport matplotlib\nimport matplotlib.image\nmatplotlib.use(\'agg\')  # disables drawing to X\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport imagenet_model\nimport summary_utils\nimport utils\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'model\', \'101\',\n    \'Depth of the network to train (50, 101, 152, 200), or number of layers\'\n    \' in each block (e.g. 3_4_23_3).\')\n\ntf.app.flags.DEFINE_string(\'checkpoint_dir\', \'\',\n                           \'Directory with the checkpoints.\')\n\ntf.app.flags.DEFINE_string(\'images_pattern\', \'\',\n                           \'Pattern of the JPEG images to process.\')\n\ntf.app.flags.DEFINE_string(\'output_dir\', \'\',\n                           \'Directory to write the results to.\')\n\ntf.app.flags.DEFINE_integer(\n    \'image_size\', 0,\n    \'Resize the input image so that the longer edge has this many pixels.\'\n    \'Not resizing if set to zero (the default).\')\n\ndef preprocessing(image):\n  image = tf.subtract(image, 0.5)\n  image = tf.multiply(image, 2.0)\n  return image\n\n\ndef reverse_preprocessing(image):\n  image = tf.multiply(image, 0.5)\n  image = tf.add(image, 0.5)\n  return image\n\n\ndef main(_):\n  if not tf.gfile.Exists(FLAGS.output_dir):\n    tf.gfile.MakeDirs(FLAGS.output_dir)\n\n  num_classes = 1001\n\n  path = tf.placeholder(tf.string)\n  contents = tf.read_file(path)\n  image = tf.image.decode_jpeg(contents, channels=3)\n  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n  images = tf.expand_dims(image, 0)\n  images.set_shape([1, None, None, 3])\n\n  if FLAGS.image_size:\n    sh = tf.shape(image)\n    height, width = tf.to_float(sh[0]), tf.to_float(sh[1])\n    longer_size = tf.constant(FLAGS.image_size, dtype=tf.float32)\n\n    new_size = tf.cond(\n      height >= width,\n      lambda: (longer_size, (width / height) * longer_size),\n      lambda: ((height / width) * longer_size, longer_size))\n    images_resized = tf.image.resize_images(images,\n        size=tf.to_int32(tf.stack(new_size)),\n        method=tf.image.ResizeMethod.BICUBIC)\n  else:\n    images_resized = images\n\n  images_resized = preprocessing(images_resized)\n\n  # Define the model:\n  with slim.arg_scope(imagenet_model.resnet_arg_scope(is_training=False)):\n    model = utils.split_and_int(FLAGS.model)\n    logits, end_points = imagenet_model.get_network(\n        images_resized,\n        model,\n        num_classes,\n        model_type=\'sact\')\n    ponder_cost_map = summary_utils.sact_map(end_points, \'ponder_cost\')\n\n  checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n  assert checkpoint_path is not None\n\n  saver = tf.train.Saver()\n  sess = tf.Session()\n\n  saver.restore(sess, checkpoint_path)\n\n  for current_path in glob.glob(FLAGS.images_pattern):\n    print(\'Processing {}\'.format(current_path))\n\n    [image_resized_out, ponder_cost_map_out] = sess.run(\n        [tf.squeeze(reverse_preprocessing(images_resized), 0),\n         tf.squeeze(ponder_cost_map, [0, 3])],\n        feed_dict={path: current_path})\n\n    basename = os.path.splitext(os.path.basename(current_path))[0]\n    if FLAGS.image_size:\n      matplotlib.image.imsave(\n          os.path.join(FLAGS.output_dir, \'{}_im.jpg\'.format(basename)),\n          image_resized_out)\n    matplotlib.image.imsave(\n        os.path.join(FLAGS.output_dir, \'{}_ponder.jpg\'.format(basename)),\n        ponder_cost_map_out,\n        cmap=\'viridis\')\n\n    min_ponder = ponder_cost_map_out.min()\n    max_ponder = ponder_cost_map_out.max()\n    print(\'Minimum/maximum ponder cost {:.2f}/{:.2f}\'.format(\n        min_ponder, max_ponder))\n\n    fig = plt.figure(figsize=(0.2, 2))\n    ax = fig.add_axes([0.0, 0.0, 1.0, 1.0])\n    cb = matplotlib.colorbar.ColorbarBase(\n        ax, cmap=\'viridis\',\n        norm=matplotlib.colors.Normalize(vmin=min_ponder, vmax=max_ponder))\n    ax.tick_params(labelsize=12)\n    filename = os.path.join(FLAGS.output_dir, \'{}_colorbar.pdf\'.format(basename))\n    plt.savefig(filename, bbox_inches=\'tight\')\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
imagenet_train.py,34,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Trains a ResNet-ACT model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport imagenet_data_provider\nimport imagenet_model\nimport summary_utils\nimport training_utils\nimport utils\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'master\', \'\',\n                       \'Name of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\'train_log_dir\', \'/tmp/resnet/\',\n                       \'Directory where to write event logs.\')\n\ntf.app.flags.DEFINE_string(\n    \'split_name\', \'train\',\n    """"""The name of the train/test split, either \'train\' or \'validation\'."""""")\n\ntf.app.flags.DEFINE_integer(\'worker_replicas\', 1, \'Number of worker replicas.\')\n\ntf.app.flags.DEFINE_integer(\n    \'ps_tasks\', 0,\n    \'The number of parameter servers. If the value is 0, then the parameters \'\n    \'are handled locally by the worker.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_summaries_secs\', 600,\n    \'The frequency with which summaries are saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\'save_interval_secs\', 600,\n                       \'The frequency with which the model is saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\'startup_delay_steps\', 15,\n                       \'Number of training steps between replicas startup.\')\n\ntf.app.flags.DEFINE_integer(\'task\', 0, \'Task id of the replica running the training.\')\n\ntf.app.flags.DEFINE_string(\'dataset_dir\', None, \'Directory with ImageNet data.\')\n\n# Training parameters.\ntf.app.flags.DEFINE_integer(\'batch_size\', 32,\n                        \'The number of images in each batch.\')\n\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.05, """"""Initial learning rate."""""")\n\ntf.app.flags.DEFINE_float(\'momentum\', 0.9, """"""Momentum."""""")\n\ntf.app.flags.DEFINE_float(\'learning_rate_decay_factor\', 0.1,\n                      \'Learning rate decay factor.\')\n\ntf.app.flags.DEFINE_float(\'num_epochs_per_decay\', 30.0,\n                      \'Number of epochs after which learning rate decays.\')\n\ntf.app.flags.DEFINE_integer(\n    \'replicas_to_aggregate\', 1,\n    \'The Number of gradients to collect before updating params.\')\n\ntf.app.flags.DEFINE_float(\'moving_average_decay\', 0.9999,\n                     \'The decay to use for the moving average.\')\n\ntf.app.flags.DEFINE_integer(\'image_size\', 224,\n                            \'Image resolution for resize.\')\n\ntf.app.flags.DEFINE_string(\n    \'model\', \'101\',\n    \'Depth of the network to train (50, 101, 152, 200), or number of layers\'\n    \' in each block (e.g. 3_4_23_3).\')\n\ntf.app.flags.DEFINE_string(\n    \'model_type\', \'vanilla\',\n    \'Options: vanilla (basic ResNet model), act (Adaptive Computation Time), \'\n    \'act_early_stopping (act implementation which actually saves time), \'\n    \'sact (Spatially Adaptive Computation Time)\')\n\ntf.app.flags.DEFINE_float(\'tau\', 1.0, \'Target value of tau (ponder relative cost).\')\n\ntf.app.flags.DEFINE_string(\'finetune_path\', \'\',\n                       \'Path for the initial checkpoint for finetuning.\')\n\n\ndef main(_):\n  g = tf.Graph()\n  with g.as_default():\n    # If ps_tasks is zero, the local device is used. When using multiple\n    # (non-local) replicas, the ReplicaDeviceSetter distributes the variables\n    # across the different devices.\n    with tf.device(tf.train.replica_device_setter(\n        FLAGS.ps_tasks, merge_devices=True)):\n      data_tuple = imagenet_data_provider.provide_data(\n          FLAGS.split_name,\n          FLAGS.batch_size,\n          dataset_dir=FLAGS.dataset_dir,\n          is_training=True,\n          image_size=FLAGS.image_size)\n      images, labels, examples_per_epoch, num_classes = data_tuple\n\n      # Define the model:\n      with slim.arg_scope(imagenet_model.resnet_arg_scope(is_training=True)):\n        model = utils.split_and_int(FLAGS.model)\n        logits, end_points = imagenet_model.get_network(\n            images,\n            model,\n            num_classes,\n            model_type=FLAGS.model_type)\n\n        # Specify the loss function:\n        tf.losses.softmax_cross_entropy(\n            onehot_labels=labels, logits=logits, label_smoothing=0.1, weights=1.0)\n        if FLAGS.model_type in (\'act\', \'act_early_stopping\', \'sact\'):\n          training_utils.add_all_ponder_costs(end_points, weights=FLAGS.tau)\n        total_loss = tf.losses.get_total_loss()\n\n        # Configure the learning rate using an exponetial decay.\n        decay_steps = int(examples_per_epoch / FLAGS.batch_size *\n                          FLAGS.num_epochs_per_decay)\n\n        learning_rate = tf.train.exponential_decay(\n            FLAGS.learning_rate,\n            slim.get_or_create_global_step(),\n            decay_steps,\n            FLAGS.learning_rate_decay_factor,\n            staircase=True)\n\n        opt = tf.train.MomentumOptimizer(learning_rate, FLAGS.momentum)\n\n        init_fn = training_utils.finetuning_init_fn(FLAGS.finetune_path)\n\n        train_tensor = slim.learning.create_train_op(\n            total_loss,\n            optimizer=opt,\n            update_ops=tf.get_collection(tf.GraphKeys.UPDATE_OPS))\n\n        # Summaries:\n        tf.summary.scalar(\'losses/Total Loss\', total_loss)\n        tf.summary.scalar(\'training/Learning Rate\', learning_rate)\n\n        metric_map = {}  # summary_utils.flops_metric_map(end_points, False)\n        if FLAGS.model_type in (\'act\', \'act_early_stopping\', \'sact\'):\n          metric_map.update(summary_utils.act_metric_map(end_points, False))\n        for name, value in metric_map.iteritems():\n          tf.summary.scalar(name, value)\n\n        if FLAGS.model_type == \'sact\':\n          summary_utils.add_heatmaps_image_summary(end_points, border=10)\n\n        startup_delay_steps = FLAGS.task * FLAGS.startup_delay_steps\n\n        slim.learning.train(\n            train_tensor,\n            init_fn=init_fn,\n            logdir=FLAGS.train_log_dir,\n            master=FLAGS.master,\n            is_chief=(FLAGS.task == 0),\n            startup_delay_steps=startup_delay_steps,\n            save_summaries_secs=FLAGS.save_summaries_secs,\n            save_interval_secs=FLAGS.save_interval_secs)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
resnet_act.py,11,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Function for building ResNet with adaptive computation time (ACT).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom functools import partial\n\nimport h5py\nimport tensorflow as tf\n\nfrom tensorflow.contrib import slim\n\nimport act\nimport flopsometer\n\n\nSACT_KERNEL_SIZE = 3\nINIT_BIAS = -3.\n\n\ndef get_halting_proba(outputs):\n  with tf.variable_scope(\'halting_proba\'):\n    x = outputs\n    x = tf.reduce_mean(x, [1, 2], keep_dims=True)\n\n    x = slim.batch_norm(x, scope=\'global_bn\')\n    halting_proba, flops = flopsometer.conv2d(\n        x,\n        1,\n        1,\n        activation_fn=tf.nn.sigmoid,\n        normalizer_fn=None,\n        biases_initializer=tf.constant_initializer(INIT_BIAS),\n        scope=\'global_conv\')\n    halting_proba = tf.squeeze(halting_proba, [1, 2])\n\n    return halting_proba, flops\n\n\ndef get_halting_proba_conv(outputs, residual_mask=None):\n  with tf.variable_scope(\'halting_proba\'):\n    flops = 0\n\n    x = outputs\n\n    local_feature = slim.batch_norm(x, scope=\'local_bn\')\n    halting_logit, current_flops = flopsometer.conv2d(\n        local_feature,\n        1,\n        SACT_KERNEL_SIZE,\n        activation_fn=None,\n        normalizer_fn=None,\n        biases_initializer=tf.constant_initializer(INIT_BIAS),\n        output_mask=residual_mask,\n        scope=\'local_conv\')\n    flops += current_flops\n\n    # Add global halting logit.\n    global_feature = tf.reduce_mean(x, [1, 2], keep_dims=True)\n    global_feature = slim.batch_norm(global_feature, scope=\'global_bn\')\n    halting_logit_global, current_flops = flopsometer.conv2d(\n        global_feature,\n        1,\n        1,\n        activation_fn=None,\n        normalizer_fn=None,\n        biases_initializer=None,  # biases are already present in local logits\n        scope=\'global_conv\')\n    flops += current_flops\n\n    # Addition with broadcasting over spatial dimensions.\n    halting_logit += halting_logit_global\n\n    halting_proba = tf.sigmoid(halting_logit)\n\n    return halting_proba, flops\n\n\ndef unit_act(block,\n             inputs,\n             unit_idx,\n             skip_halting_proba=False,\n             sact=False,\n             residual_mask=None):\n  with tf.variable_scope(\'unit_%d\' % (unit_idx + 1), [inputs]):\n    outputs, flops = block.unit_fn(\n        inputs, *block.args[unit_idx], residual_mask=residual_mask)\n\n    if not skip_halting_proba and unit_idx < len(block.args) - 1:\n      if sact:\n        halting_proba, current_flops = get_halting_proba_conv(\n            outputs, residual_mask)\n        flops += current_flops\n      else:\n        halting_proba, current_flops = get_halting_proba(outputs)\n        flops += current_flops\n    else:\n      halting_proba = None\n\n    return outputs, halting_proba, flops\n\n\ndef stack_blocks(net, blocks, model_type, end_points=None):\n  """"""Utility function for assembling SACT models consisting of \'blocks.\'""""""\n  if end_points is None:\n    end_points = {}\n  end_points[\'flops\'] = end_points.get(\'flops\', 0)\n  end_points[\'block_scopes\'] = [block.scope for block in blocks]\n  end_points[\'block_num_units\'] = [len(block.args) for block in blocks]\n\n  assert model_type in (\'vanilla\', \'act\', \'act_early_stopping\', \'sact\')\n  model_type_to_func = {\n    \'act\': act.adaptive_computation_time_wrapper,\n    \'act_early_stopping\': act.adaptive_computation_early_stopping,\n    \'sact\': act.spatially_adaptive_computation_time,\n  }\n  act_func = model_type_to_func.get(model_type, None)\n\n  for block in blocks:\n    if act_func:\n      (ponder_cost, num_units, flops, halting_distribution, net) = act_func(\n          net,\n          partial(unit_act, block, sact=(model_type == \'sact\')),\n          len(block.args),\n          scope=block.scope)\n\n      end_points[\'{}/ponder_cost\'.format(block.scope)] = ponder_cost\n      end_points[\'{}/num_units\'.format(block.scope)] = num_units\n      end_points[\'{}/halting_distribution\'.format(\n          block.scope)] = halting_distribution\n    else:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        flops = 0\n        for unit_idx in range(len(block.args)):\n          net, _, current_flops = unit_act(\n              block, net, unit_idx, skip_halting_proba=True)\n          flops += current_flops\n\n    end_points[\'{}/flops\'.format(block.scope)] = flops\n    end_points[\'flops\'] += flops\n    end_points[block.scope] = net\n\n  return net, end_points\n'"
squeeze_model.py,13,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Removes the Momentum and Moving Average variables,\n  reducing the model size 2-3 times.\n  The provided pretrained models are squeezed.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nimport cifar_model\nimport imagenet_model\nimport utils\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'input_dir\', \'/tmp/resnet/\',\n                           \'Directory where the model was written to.\')\n\ntf.app.flags.DEFINE_string(\'output_dir\', \'/tmp/resnet2/\',\n                           \'Directory where the squeezed model will be written to.\')\n\ntf.app.flags.DEFINE_string(\n  \'model\',\n  None,\n  \'A description of the model.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_type\', None,\n    \'Options: vanilla (basic ResNet model), act (Adaptive Computation Time), \'\n    \'act_early_stopping (act implementation which actually saves time), \'\n    \'sact (Spatially Adaptive Computation Time)\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset\', None,\n    \'Options: imagenet, cifar\'\n)\n\n\ndef main(_):\n  if not tf.gfile.Exists(FLAGS.output_dir):\n    tf.gfile.MakeDirs(FLAGS.output_dir)\n\n  assert FLAGS.model is not None\n  assert FLAGS.model_type in (\'vanilla\', \'act\', \'act_early_stopping\', \'sact\')\n  assert FLAGS.dataset in (\'imagenet\', \'cifar\')\n\n  batch_size = 1\n\n  if FLAGS.dataset == \'imagenet\':\n    height, width = 224, 224\n    num_classes = 1001\n  elif FLAGS.dataset == \'cifar\':\n    height, width = 32, 32\n    num_classes = 10\n\n  images = tf.random_uniform((batch_size, height, width, 3))\n  model = utils.split_and_int(FLAGS.model)\n\n  # Define the model\n  if FLAGS.dataset == \'imagenet\':\n    with slim.arg_scope(imagenet_model.resnet_arg_scope(is_training=False)):\n      logits, end_points = imagenet_model.get_network(\n          images,\n          model,\n          num_classes,\n          model_type=FLAGS.model_type)\n  elif FLAGS.dataset == \'cifar\':\n    # Define the model:\n    with slim.arg_scope(cifar_model.resnet_arg_scope(is_training=False)):\n      logits, end_points = cifar_model.resnet(\n          images,\n          model=model,\n          num_classes=num_classes,\n          model_type=FLAGS.model_type)\n\n  tf_global_step = slim.get_or_create_global_step()\n\n  checkpoint_path = tf.train.latest_checkpoint(FLAGS.input_dir)\n  assert checkpoint_path is not None\n\n  saver = tf.train.Saver(write_version=2)\n\n  with tf.Session() as sess:\n    saver.restore(sess, checkpoint_path)\n    saver.save(sess, FLAGS.output_dir + \'/model\', global_step=tf_global_step)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
summary_utils.py,35,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Summary utility functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport h5py\nimport tensorflow as tf\n\nfrom tensorflow.contrib import slim\n\n\ndef moments_metric_map(x, name, mean_metric, delimiter=\'_\', do_shift=False):\n  if not mean_metric:\n    if do_shift:\n      shift = tf.reduce_mean(x)  # Seems to help numerical issues, but slower\n    else:\n      shift = None\n\n    mean, var = tf.nn.moments(x, list(range(len(x.get_shape().as_list()))),\n                              shift=shift)\n    std = tf.sqrt(tf.maximum(0.0, var))\n  else:\n    mean = tf.contrib.metrics.streaming_mean(x)\n    # Variance is estimated over the whole dataset, therefore it will\n    # generally be higher than during training.\n    var_value, var_update = tf.contrib.metrics.streaming_covariance(x, x)\n    std_value = tf.sqrt(tf.maximum(0.0, var_value))\n    std = std_value, var_update\n\n  metric_map = {\n      \'{}{}mean\'.format(name, delimiter): mean,\n      \'{}{}std\'.format(name, delimiter): std,\n  }\n\n  return metric_map\n\n\ndef act_metric_map(end_points, mean_metric):\n  """"""Assembles ACT-specific metrics into a map for use in tf.contrib.metrics.""""""\n  metric_map = {}\n\n  for block_scope in end_points[\'block_scopes\']:\n    name = \'{}/ponder_cost\'.format(block_scope)\n    ponder_cost = end_points[name]\n    ponder_map = moments_metric_map(ponder_cost, name, mean_metric)\n    metric_map.update(ponder_map)\n\n    name = \'{}/num_units\'.format(block_scope)\n    num_units = tf.to_float(end_points[name])\n    num_units_map = moments_metric_map(num_units, name, mean_metric)\n    metric_map.update(num_units_map)\n\n    if not mean_metric:\n      # Not sure how to make a streaming version of this metric,\n      # so tracking it only during training.\n      name = \'{}/num_units_max\'.format(block_scope)\n      metric_map[name] = tf.reduce_max(num_units)\n\n  return metric_map\n\n\ndef flops_metric_map(end_points, mean_metric, total_name=\'Total Flops\'):\n  """"""Assembles flops-count metrics into a map for use in tf.contrib.metrics.""""""\n  metric_map = {}\n  total_flops = tf.to_float(end_points[\'flops\'])\n  flops_map = moments_metric_map(total_flops, total_name, mean_metric,\n      delimiter=\'/\', do_shift=True)\n  metric_map.update(flops_map)\n\n  for block_scope in end_points[\'block_scopes\']:\n    name = \'{}/flops\'.format(block_scope)\n    flops = tf.to_float(end_points[name])\n    flops_map = moments_metric_map(flops, name, mean_metric, do_shift=True)\n    metric_map.update(flops_map)\n\n  return metric_map\n\n\ndef sact_image_heatmap(end_points,\n                           metric_name,\n                           num_images=5,\n                           alpha=0.75,\n                           border=5,\n                           normalize_images=True):\n  """"""Overlays a heatmap of the ponder cost onto the input image.""""""\n  assert metric_name in (\'ponder_cost\', \'num_units\')\n\n  images = end_points[\'inputs\']\n  if num_images is not None:\n    images = images[:num_images, :, :, :]\n  else:\n    num_images = tf.shape(images)[0]\n\n  # Normalize the images\n  if normalize_images:\n    images -= tf.reduce_min(images, [1, 2, 3], True)\n    images /= tf.reduce_max(images, [1, 2, 3], True)\n\n  resolution = tf.shape(images)[1:3]\n\n  max_value = sum(end_points[\'block_num_units\'])\n  if metric_name == \'ponder_cost\':\n    max_value += len(end_points[\'block_num_units\'])\n\n  heatmaps = []\n  for scope in end_points[\'block_scopes\']:\n    h = end_points[\'{}/{}\'.format(scope, metric_name)]\n    h = tf.to_float(h)\n    h = h[:num_images, :, :]\n    h = tf.expand_dims(h, 3)\n    # The metric maps can be lower resolution than the image.\n    # We simply resize the map to the image size.\n    h = tf.image.resize_nearest_neighbor(h, resolution, align_corners=False)\n    # Heatmap is in Red channel. Fill Blue and Green channels with zeros.\n    dimensions = tf.stack([num_images, resolution[0], resolution[1], 2])\n    h = tf.concat([h, tf.zeros(dimensions)], 3)\n    heatmaps.append(h)\n\n  im_heatmap = images * (1 - alpha) + tf.add_n(heatmaps) * (alpha / max_value)\n\n  # image, black border, image with overlayed heatmap\n  dimensions = tf.stack([num_images, resolution[0], border, 3])\n  ret = tf.concat([images, tf.zeros(dimensions), im_heatmap], 2)\n\n  return ret\n\n\ndef add_heatmaps_image_summary(end_points, num_images=3, alpha=0.75, border=5):\n  tf.summary.image(\n      \'heatmaps/ponder_cost\',\n      sact_image_heatmap(\n          end_points,\n          \'ponder_cost\',\n          num_images=num_images,\n          alpha=alpha,\n          border=border))\n  tf.summary.image(\n      \'heatmaps/num_units\',\n      sact_image_heatmap(\n          end_points,\n          \'num_units\',\n          num_images=num_images,\n          alpha=alpha,\n          border=border))\n\n\ndef sact_map(end_points, metric_name):\n  """"""Generates a heatmap of the ponder cost for visualization.""""""\n  assert metric_name in (\'ponder_cost\', \'num_units\')\n\n  inputs = end_points[\'inputs\']\n  if inputs.get_shape().is_fully_defined():\n    sh = inputs.get_shape().as_list()\n  else:\n    sh = tf.shape(inputs)\n  resolution = sh[1:3]\n\n  heatmaps = []\n  for scope in end_points[\'block_scopes\']:\n    h = end_points[\'{}/{}\'.format(scope, metric_name)]\n    h = tf.to_float(h)\n    h = tf.expand_dims(h, 3)\n    # The metric maps can be lower resolution than the image.\n    # We simply resize the map to the image size.\n    h = tf.image.resize_nearest_neighbor(h, resolution, align_corners=False)\n    heatmaps.append(h)\n\n  return tf.add_n(heatmaps)\n\n\ndef export_to_h5(checkpoint_dir, export_path, images, end_points, num_samples,\n                 batch_size, sact):\n  """"""Exports ponder cost maps and other useful info to an HDF5 file.""""""\n  output_file = h5py.File(export_path, \'w\')\n\n  output_file.attrs[\'block_scopes\'] = end_points[\'block_scopes\']\n  keys_to_tensors = {}\n  for block_scope in end_points[\'block_scopes\']:\n    for k in (\'{}/ponder_cost\'.format(block_scope),\n              \'{}/num_units\'.format(block_scope),\n              \'{}/halting_distribution\'.format(block_scope),\n              \'{}/flops\'.format(block_scope)):\n      keys_to_tensors[k] = end_points[k]\n  keys_to_tensors[\'images\'] = images\n  keys_to_tensors[\'flops\'] = end_points[\'flops\']\n\n  if sact:\n    keys_to_tensors[\'ponder_cost_map\'] = sact_map(end_points, \'ponder_cost\')\n    keys_to_tensors[\'num_units_map\'] = sact_map(end_points, \'num_units\')\n\n  keys_to_datasets = {}\n  for key, tensor in keys_to_tensors.iteritems():\n    sh = tensor.get_shape().as_list()\n    sh[0] = num_samples\n    print(key, sh)\n    keys_to_datasets[key] = output_file.create_dataset(\n        key, sh, compression=\'lzf\')\n\n  variables_to_restore = slim.get_model_variables()\n  checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n  assert checkpoint_path is not None\n  init_fn = slim.assign_from_checkpoint_fn(checkpoint_path,\n                                           variables_to_restore)\n\n  sv = tf.train.Supervisor(\n      graph=tf.get_default_graph(),\n      logdir=None,\n      summary_op=None,\n      summary_writer=None,\n      global_step=None,\n      saver=None)\n\n  assert num_samples % batch_size == 0\n  num_batches = num_samples // batch_size\n\n  with sv.managed_session(\'\', start_standard_services=False) as sess:\n    init_fn(sess)\n    sv.start_queue_runners(sess)\n\n    for i in range(num_batches):\n      tf.logging.info(\'Evaluating batch %d/%d\', i + 1, num_batches)\n      end_points_out = sess.run(keys_to_tensors)\n      for key, dataset in keys_to_datasets.iteritems():\n        dataset[i * batch_size:(i + 1) * batch_size, ...] = end_points_out[key]\n'"
summary_utils_test.py,4,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for summary_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nimport summary_utils\n\n\nclass SummaryUtilsTest(tf.test.TestCase):\n\n  def testSactImageHeatmap(self):\n    batch = 9\n    num_images = 5\n    height, width, channels = 32, 32, 3\n    border = 4\n    alpha = 0.75\n\n    end_points = {\n        \'inputs\': tf.ones([batch, height, width, channels]),\n        \'block_num_units\': [10],\n        \'block_scopes\': [\'block_1\'],\n        \'block_1/ponder_cost\': 5 * tf.ones([batch, height / 2, width / 2]),\n    }\n\n    heatmap = summary_utils.sact_image_heatmap(\n        end_points,\n        \'ponder_cost\',\n        num_images=num_images,\n        alpha=alpha,\n        border=border,\n        normalize_images=False)\n\n    with self.test_session() as sess:\n      inputs_out, heatmap_out = sess.run([end_points[\'inputs\'], heatmap])\n\n    self.assertEqual(heatmap_out.shape,\n                     (num_images, height, width * 2 + border, channels))\n    self.assertAllClose(heatmap_out[:, :, :width, :],\n                        inputs_out[:num_images, :, :, :])\n\n    expected_heatmap = 0.25 * inputs_out[:num_images, :, :, :]\n    expected_heatmap[:, :, :, 0] += 0.75 * (5.0 / 11.0)\n    self.assertAllClose(heatmap_out[:, :, width + border:, :], expected_heatmap)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
training_utils.py,6,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Training utility functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef add_all_ponder_costs(end_points, weights):\n  total_ponder_cost = 0.\n  for scope in end_points[\'block_scopes\']:\n    ponder_cost = end_points[\'{}/ponder_cost\'.format(scope)]\n    total_ponder_cost += tf.reduce_mean(ponder_cost)\n  tf.losses.add_loss(total_ponder_cost * weights)\n\n\ndef variables_to_str(variables):\n  return \', \'.join([var.op.name for var in variables])\n\n\ndef finetuning_init_fn(finetune_path):\n  """"""Sets up fine-tuning of a SACT model.""""""\n  if not finetune_path:\n    return None\n\n  tf.logging.warning(\'Finetuning from {}\'.format(finetune_path))\n  variables = tf.contrib.framework.get_model_variables()\n  variables_to_restore = [\n      var for var in variables if \'/halting_proba/\' not in var.op.name\n  ]\n  tf.logging.info(\'Restoring variables: {}\'.format(\n      variables_to_str(variables_to_restore)))\n  init_fn = tf.contrib.framework.assign_from_checkpoint_fn(\n      finetune_path, variables_to_restore)\n\n  return init_fn\n'"
utils.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Summary utility functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\ndef split_and_int(s):\n  return [int(x) for x in s.split(\'_\')]\n'"
external/__init__.py,0,b''
external/dataset_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains utilities for downloading and converting datasets.\n\nCopied from https://github.com/tensorflow/models/blob/master/slim/datasets/dataset_utils.py\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nLABELS_FILENAME = \'labels.txt\'\n\n\ndef int64_feature(values):\n  """"""Returns a TF-Feature of int64s.\n\n  Args:\n    values: A scalar or list of values.\n\n  Returns:\n    a TF-Feature.\n  """"""\n  if not isinstance(values, (tuple, list)):\n    values = [values]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\n\ndef bytes_feature(values):\n  """"""Returns a TF-Feature of bytes.\n\n  Args:\n    values: A string.\n\n  Returns:\n    a TF-Feature.\n  """"""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n\n\ndef image_to_tfexample(image_data, image_format, height, width, class_id):\n  return tf.train.Example(features=tf.train.Features(feature={\n      \'image/encoded\': bytes_feature(image_data),\n      \'image/format\': bytes_feature(image_format),\n      \'image/class/label\': int64_feature(class_id),\n      \'image/height\': int64_feature(height),\n      \'image/width\': int64_feature(width),\n  }))\n\n\ndef download_and_uncompress_tarball(tarball_url, dataset_dir):\n  """"""Downloads the `tarball_url` and uncompresses it locally.\n\n  Args:\n    tarball_url: The URL of a tarball file.\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = tarball_url.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n\n  def _progress(count, block_size, total_size):\n    sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n        filename, float(count * block_size) / float(total_size) * 100.0))\n    sys.stdout.flush()\n  filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)\n  print()\n  statinfo = os.stat(filepath)\n  print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef write_label_file(labels_to_class_names, dataset_dir,\n                     filename=LABELS_FILENAME):\n  """"""Writes a file with the list of class names.\n\n  Args:\n    labels_to_class_names: A map of (integer) labels to class names.\n    dataset_dir: The directory in which the labels file should be written.\n    filename: The filename where the class names are written.\n  """"""\n  labels_filename = os.path.join(dataset_dir, filename)\n  with tf.gfile.Open(labels_filename, \'w\') as f:\n    for label in labels_to_class_names:\n      class_name = labels_to_class_names[label]\n      f.write(\'%d:%s\\n\' % (label, class_name))\n\n\ndef has_labels(dataset_dir, filename=LABELS_FILENAME):\n  """"""Specifies whether or not the dataset directory contains a label map file.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    `True` if the labels file exists and `False` otherwise.\n  """"""\n  return tf.gfile.Exists(os.path.join(dataset_dir, filename))\n\n\ndef read_label_file(dataset_dir, filename=LABELS_FILENAME):\n  """"""Reads the labels file and returns a mapping from ID to class name.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    A map from a label (integer) to class name.\n  """"""\n  labels_filename = os.path.join(dataset_dir, filename)\n  with tf.gfile.Open(labels_filename, \'r\') as f:\n    lines = f.read().decode()\n  lines = lines.split(\'\\n\')\n  lines = filter(None, lines)\n\n  labels_to_class_names = {}\n  for line in lines:\n    index = line.index(\':\')\n    labels_to_class_names[int(line[:index])] = line[index+1:]\n  return labels_to_class_names\n'"
external/datasets_cifar10.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Cifar10 dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/slim/datasets/download_and_convert_cifar10.py\n\nCopied from https://github.com/tensorflow/models/blob/master/slim/datasets/cifar10.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom . import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'cifar10_%s.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 50000, \'test\': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [32 x 32 x 3] color image.\',\n    \'label\': \'A single integer between 0 and 9\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if not reader:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(shape=[32, 32, 3]),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
external/datasets_imagenet.py,20,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the ImageNet ILSVRC 2012 Dataset plus some bounding boxes.\n\nSome images have one or more bounding boxes associated with the label of the\nimage. See details here: http://image-net.org/download-bboxes\n\nImageNet is based upon WordNet 3.0. To uniquely identify a synset, we use\n""WordNet ID"" (wnid), which is a concatenation of POS ( i.e. part of speech )\nand SYNSET OFFSET of WordNet. For more information, please refer to the\nWordNet documentation[http://wordnet.princeton.edu/wordnet/documentation/].\n\n""There are bounding boxes for over 3000 popular synsets available.\nFor each synset, there are on average 150 images with bounding boxes.""\n\nWARNING: Don\'t use for object detection, in this case all the bounding boxes\nof the image belong to just one class.\n\nCopied from https://github.com/tensorflow/models/blob/master/slim/datasets/imagenet.py\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom . import dataset_utils\n\nslim = tf.contrib.slim\n\n# TODO(nsilberman): Add tfrecord file type once the script is updated.\n_FILE_PATTERN = \'%s-*\'\n\n_SPLITS_TO_SIZES = {\n    \'train\': 1281167,\n    \'validation\': 50000,\n}\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying height and width.\',\n    \'label\': \'The label id of the image, integer between 0 and 999\',\n    \'label_text\': \'The text of the label.\',\n    \'object/bbox\': \'A list of bounding boxes.\',\n    \'object/label\': \'A list of labels, one per each object.\',\n}\n\n_NUM_CLASSES = 1001\n\n\ndef create_readable_names_for_imagenet_labels():\n  """"""Create a dict mapping label id to human readable string.\n\n  Returns:\n      labels_to_names: dictionary where keys are integers from to 1000\n      and values are human-readable names.\n\n  We retrieve a synset file, which contains a list of valid synset labels used\n  by ILSVRC competition. There is one synset one per line, eg.\n          #   n01440764\n          #   n01443537\n  We also retrieve a synset_to_human_file, which contains a mapping from synsets\n  to human-readable names for every synset in Imagenet. These are stored in a\n  tsv format, as follows:\n          #   n02119247    black fox\n          #   n02119359    silver fox\n  We assign each synset (in alphabetical order) an integer, starting from 1\n  (since 0 is reserved for the background class).\n\n  Code is based on\n  https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py#L463\n  """"""\n\n  # pylint: disable=g-line-too-long\n  base_url = \'https://raw.githubusercontent.com/tensorflow/models/master/research/inception/inception/data/\'\n  synset_url = \'{}/imagenet_lsvrc_2015_synsets.txt\'.format(base_url)\n  synset_to_human_url = \'{}/imagenet_metadata.txt\'.format(base_url)\n\n  filename, _ = urllib.request.urlretrieve(synset_url)\n  synset_list = [s.strip() for s in open(filename).readlines()]\n  num_synsets_in_ilsvrc = len(synset_list)\n  assert num_synsets_in_ilsvrc == 1000\n\n  filename, _ = urllib.request.urlretrieve(synset_to_human_url)\n  synset_to_human_list = open(filename).readlines()\n  num_synsets_in_all_imagenet = len(synset_to_human_list)\n  assert num_synsets_in_all_imagenet == 21842\n\n  synset_to_human = {}\n  for s in synset_to_human_list:\n    parts = s.strip().split(\'\\t\')\n    assert len(parts) == 2\n    synset = parts[0]\n    human = parts[1]\n    synset_to_human[synset] = human\n\n  label_index = 1\n  labels_to_names = {0: \'background\'}\n  for synset in synset_list:\n    name = synset_to_human[synset]\n    labels_to_names[label_index] = name\n    label_index += 1\n\n  return labels_to_names\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading ImageNet.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature(\n          (), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature(\n          (), tf.string, default_value=\'jpeg\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], dtype=tf.int64, default_value=-1),\n      \'image/class/text\': tf.FixedLenFeature(\n          [], dtype=tf.string, default_value=\'\'),\n      \'image/object/bbox/xmin\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/ymin\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/xmax\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/ymax\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/class/label\': tf.VarLenFeature(\n          dtype=tf.int64),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n      \'label_text\': slim.tfexample_decoder.Tensor(\'image/class/text\'),\n      \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n          [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n      \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n  else:\n    labels_to_names = create_readable_names_for_imagenet_labels()\n    dataset_utils.write_label_file(labels_to_names, dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
external/download_and_convert_cifar10.py,15,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts cifar10 data to TFRecords of TF-Example protos.\n\nThis module downloads the cifar10 data, uncompresses it, reads the files\nthat make up the cifar10 data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take several minutes to run.\n\nCopied from https://github.com/tensorflow/models/blob/master/slim/datasets/download_and_convert_cifar10.py\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cPickle\nimport os\nimport sys\nimport tarfile\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\n\nimport dataset_utils\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\',\n    None,\n    \'The directory where the output TFRecords and temporary files are saved.\')\n\n# The URL where the CIFAR data can be downloaded.\n_DATA_URL = \'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\n\n# The number of training files.\n_NUM_TRAIN_FILES = 5\n\n# The height and width of each image.\n_IMAGE_SIZE = 32\n\n# The names of the classes.\n_CLASS_NAMES = [\n    \'airplane\',\n    \'automobile\',\n    \'bird\',\n    \'cat\',\n    \'deer\',\n    \'dog\',\n    \'frog\',\n    \'horse\',\n    \'ship\',\n    \'truck\',\n]\n\n\ndef _add_to_tfrecord(filename, tfrecord_writer, offset=0):\n  """"""Loads data from the cifar10 pickle files and writes files to a TFRecord.\n\n  Args:\n    filename: The filename of the cifar10 pickle file.\n    tfrecord_writer: The TFRecord writer to use for writing.\n    offset: An offset into the absolute number of images previously written.\n\n  Returns:\n    The new offset.\n  """"""\n  with tf.gfile.Open(filename, \'r\') as f:\n    data = cPickle.load(f)\n\n  images = data[\'data\']\n  num_images = images.shape[0]\n\n  images = images.reshape((num_images, 3, 32, 32))\n  labels = data[\'labels\']\n\n  with tf.Graph().as_default():\n    image_placeholder = tf.placeholder(dtype=tf.uint8)\n    encoded_image = tf.image.encode_png(image_placeholder)\n\n    with tf.Session(\'\') as sess:\n\n      for j in range(num_images):\n        sys.stdout.write(\'\\r>> Reading file [%s] image %d/%d\' % (\n            filename, offset + j + 1, offset + num_images))\n        sys.stdout.flush()\n\n        image = np.squeeze(images[j]).transpose((1, 2, 0))\n        label = labels[j]\n\n        png_string = sess.run(encoded_image,\n                              feed_dict={image_placeholder: image})\n\n        example = dataset_utils.image_to_tfexample(\n            png_string, \'png\', _IMAGE_SIZE, _IMAGE_SIZE, label)\n        tfrecord_writer.write(example.SerializeToString())\n\n  return offset + num_images\n\n\ndef _get_output_filename(dataset_dir, split_name):\n  """"""Creates the output filename.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n    split_name: The name of the train/test split.\n\n  Returns:\n    An absolute file path.\n  """"""\n  return \'%s/cifar10_%s.tfrecord\' % (dataset_dir, split_name)\n\n\ndef _download_and_uncompress_dataset(dataset_dir):\n  """"""Downloads cifar10 and uncompresses it locally.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n          filename, float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(_DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n    tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n  tf.gfile.Remove(filepath)\n\n  tmp_dir = os.path.join(dataset_dir, \'cifar-10-batches-py\')\n  tf.gfile.DeleteRecursively(tmp_dir)\n\n\ndef main(_):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  dataset_dir = FLAGS.dataset_dir\n\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  training_filename = _get_output_filename(dataset_dir, \'train\')\n  testing_filename = _get_output_filename(dataset_dir, \'test\')\n\n  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n\n  # First, process the training data:\n  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\n    offset = 0\n    for i in range(_NUM_TRAIN_FILES):\n      filename = os.path.join(dataset_dir,\n                              \'cifar-10-batches-py\',\n                              \'data_batch_%d\' % (i + 1))  # 1-indexed.\n      offset = _add_to_tfrecord(filename, tfrecord_writer, offset)\n\n  # Next, process the testing data:\n  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\n    filename = os.path.join(dataset_dir,\n                            \'cifar-10-batches-py\',\n                            \'test_batch\')\n    _add_to_tfrecord(filename, tfrecord_writer)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the Cifar10 dataset!\')\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
external/inception_preprocessing.py,62,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.\n\nCopied from https://github.com/tensorflow/models/blob/master/slim/preprocessing/inception_preprocessing.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n      image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # A large fraction of image datasets contain a human-annotated bounding\n    # box delineating the region of the image containing the object of interest.\n    # We choose to create a new bounding box for the object which is a randomly\n    # distorted version of the human-annotated bounding box that obeys an\n    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n    # bounding box. If no box is supplied, then we assume the bounding box is\n    # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n    return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True,\n                         scope=None):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n  with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                         dtype=tf.float32,\n                         shape=[1, 1, 4])\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                  bbox)\n    tf.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    image_with_distorted_box = tf.image.draw_bounding_boxes(\n        tf.expand_dims(image, 0), distorted_bbox)\n    tf.summary.image(\'images_with_distorted_bounding_box\',\n                     image_with_distorted_box)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [height, width], method=method),\n        num_cases=num_resize_cases)\n\n    tf.summary.image(\'cropped_resized_image\',\n                     tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors. There are 4 ways to do it.\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, ordering: distort_color(x, ordering, fast_mode),\n        num_cases=4)\n\n    tf.summary.image(\'final_distorted_image\',\n                     tf.expand_dims(distorted_image, 0))\n    distorted_image = tf.subtract(distorted_image, 0.5)\n    distorted_image = tf.multiply(distorted_image, 2.0)\n    return distorted_image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would cropt the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    return image\n\n\ndef preprocess_image(image, height, width,\n                     is_training=False,\n                     bbox=None,\n                     fast_mode=True):\n  """"""Pre-process one image for training or evaluation.\n\n  Args:\n    image: 3-D Tensor [height, width, channels] with the image.\n    height: integer, image expected height.\n    width: integer, image expected width.\n    is_training: Boolean. If true it would transform an image for train,\n      otherwise it would transform it for evaluation.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations.\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  """"""\n  if is_training:\n    return preprocess_for_train(image, height, width, bbox, fast_mode)\n  else:\n    return preprocess_for_eval(image, height, width)\n'"
