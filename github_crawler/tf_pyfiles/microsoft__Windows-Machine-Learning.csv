file_path,api_count,code
Tools/WinMLDashboard/deps/copy_netron.py,0,"b'#!/usr/bin/env python3\nimport os\nfrom pathlib import Path\nfrom unittest.mock import patch\nimport re\nimport shutil\nimport subprocess\n\n\n@patch(\'setuptools.setup\')\ndef get_package_data(mock_setup):\n    import Netron.setup as netron_setup\n    package_data = mock_setup.call_args[1][\'package_data\'][\'netron\']\n    return (package_data, netron_setup.node_dependencies[0][1])\n\n\ndef get_netron_static_scripts(src_path):\n    with open(src_path / \'view-browser.html\') as f:\n        scripts = []\n        regex = re.compile(""<script type=\'text/javascript\' src=\'(.*)\'></script>"")\n        for line in f.readlines():\n            match = re.match(regex, line)\n            if match:\n                scripts.append(match.group(1))\n        return scripts\n\n\ndef rebuild_needed(sources, destination):\n    try:\n        destination_mtime = os.path.getmtime(destination)\n    except FileNotFoundError:\n        return True\n    return any(os.path.getmtime(f) >= destination_mtime for f in sources)\n\n\ndef bundle_scripts(files):\n    bundle = []\n    for script in files:\n        with script.open(\'rb\') as f:\n            bundle.append(f.read())\n    return b\'\\n\'.join(bundle)\n\n\ndef minify(input_file, output_file):\n    subprocess.check_call([shutil.which(\'yarn\'), \'minify\', str(input_file), \'-o\', str(output_file)])\n\n\ndef main():\n    netron = Path(\'deps/Netron\')\n    src = netron / \'src\'\n    package_data, node_dependencies = get_package_data()\n    print(\'Netron package files:\\n{}\'.format(\' \'.join(package_data)))\n    print(\'Netron Node dependencies:\\n{}\'.format(\' \'.join(node_dependencies)))\n\n    static_scripts = get_netron_static_scripts(src)\n    print(\'These scripts will be bundled:\\n{}\'.format(\' \'.join(static_scripts)))\n    # Update script paths to point to paths before installation\n    for i, script in enumerate(static_scripts):\n        if script in package_data:\n            static_scripts[i] = src / script\n        else:\n            for filename in node_dependencies:\n                path = Path(filename)\n                if script == path.name:\n                    static_scripts[i] = netron / path\n\n    package_data = set(package_data) - set(static_scripts)\n\n    public = Path(\'public\')\n    ignored_extensions = [\'.css\', \'.html\', \'.ico\']\n    package_data = [src / filename for filename in package_data if not Path(filename).suffix in ignored_extensions]\n    for package_file in package_data:\n        try:\n            os.link(package_file, public / package_file.name)\n        except FileExistsError:\n            pass\n        except FileNotFoundError:\n            print(""Warning: Got FileNotFoundError linking {} -> {}. ""\n                  ""Netron\'s setup might be declaring files that are missing in their repository.""\n                  .format(public / package_file.name, package_file))\n\n    bundle_destination = public / \'netron_bundle.js\'\n    if rebuild_needed(static_scripts, bundle_destination):\n        import tempfile\n        with tempfile.NamedTemporaryFile() as f:\n            bundled_scripts = bundle_scripts(static_scripts)\n            f.write(bundled_scripts)\n            try:\n                minify(f.name, bundle_destination)\n            except:\n                import traceback\n                traceback.print_exc()\n                print(\'Minifying Netron failed! A non-minified build will be done instead\')\n                with open(bundle_destination, \'wb\') as f:\n                    f.write(bundled_scripts)\n    else:\n        print(\'Bundle is already up to date\')\n\nif __name__ == \'__main__\':\n    main()\n'"
Tools/WinMLDashboard/public/convert.py,2,"b""import argparse\nfrom pathlib import Path\n\nimport winmltools\nimport onnxmltools\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Convert model to ONNX.')\n    parser.add_argument('source', help='source  model')\n    parser.add_argument('framework', help='source framework model comes from')\n    parser.add_argument('ONNXVersion', help='which ONNX Version to convert to')\n    parser.add_argument('quantizationOption', help='on which OS to quantize' )\n    parser.add_argument('outputNames', help='names of output nodes')\n    parser.add_argument('destination', help='destination ONNX model (ONNX or prototxt extension)')\n    parser.add_argument('--name', default='WimMLDashboardConvertedModel', help='(ONNX output only) model name')\n    return parser.parse_args()\n\n\ndef get_extension(path):\n    return Path(path).suffix[1:].lower()\n\n\ndef save_onnx(onnx_model, destination):\n    destination_extension = get_extension(destination)\n    if destination_extension == 'onnx':\n        winmltools.utils.save_model(onnx_model, destination)\n    elif destination_extension == 'prototxt':\n        winmltools.utils.save_text(onnx_model, destination)\n    else:\n        raise RuntimeError('Conversion to extension {} is not supported'.format(destination_extension))\n\ndef get_opset(ONNXVersion):\n    if '1.2' == ONNXVersion:\n        return 7\n    elif '1.3' == ONNXVersion:\n        return 8\n    elif '1.5' == ONNXVersion:\n        return 10\n    else:\n        return 7\n\ndef get_useDequantize(quantizationOption):\n    if '19H1' == quantizationOption:\n        return True\n    else:\n        return False\n\ndef coreml_converter(args):\n    # When imported, CoreML tools checks for the current version of Keras and TF and prints warnings if they are\n    # outside its expected range. We don't want it to import these packages (since they are big and take seconds to\n    # load) and we don't want to clutter the console with unrelated Keras warnings when converting from CoreML.\n    import sys\n    sys.modules['keras'] = None\n    import coremltools\n    source_model = coremltools.utils.load_spec(args.source)\n    onnx_model = winmltools.convert_coreml(source_model, get_opset(args.ONNXVersion), args.name)\n    return onnx_model\n\n\ndef keras_converter(args):\n    from keras.models import load_model\n    source_model = load_model(args.source)\n    destination_extension = get_extension(args.destination)\n    onnx_model = winmltools.convert_keras(source_model, get_opset(args.ONNXVersion))\n    return onnx_model\n\ndef scikit_learn_converter(args):\n    from sklearn.externals import joblib\n    source_model = joblib.load(args.source) \n    from onnxmltools.convert.common.data_types import FloatTensorType\n    onnx_model = winmltools.convert_sklearn(source_model, get_opset(args.ONNXVersion),\n                                  initial_types=[('input', FloatTensorType(source_model.coef_.shape))])\n    return onnx_model\n\ndef xgboost_converter(args):\n    from sklearn.externals import joblib\n    source_model = joblib.load(args.source)\n    from onnxmltools.convert.common.data_types import FloatTensorType\n    onnx_model = winmltools.convert_xgboost(source_model, get_opset(args.ONNXVersion),\n                                initial_types=[('input', FloatTensorType(shape=[1, 'None']))])\n    return onnx_model\n\ndef libSVM_converter(args):\n    import svmutil\n    source_model = svmutil.svm_load_model(args.source)\n    from onnxmltools.convert.common.data_types import FloatTensorType\n    onnx_model = winmltools.convert_libsvm(source_model, get_opset(args.ONNXVersion),\n                                initial_types=[('input', FloatTensorType([1, 'None']))])\n    return onnx_model\n\ndef convert_tensorflow_file(filename, opset, output_names):\n    import winmltools\n    import tensorflow\n    import tf2onnx\n    from tensorflow.core.framework import graph_pb2\n    from tensorflow.python.tools import freeze_graph\n    import onnx\n    import tensorflow as tf\n\n    graph_def = graph_pb2.GraphDef()\n    with open(filename, 'rb') as file:\n        graph_def.ParseFromString(file.read())\n    g = tf.import_graph_def(graph_def, name='')\n    with tf.Session(graph=g) as sess:\n        converted_model = winmltools.convert_tensorflow(sess.graph, opset, continue_on_error=True, verbose=True, output_names=output_names)\n        onnx.checker.check_model(converted_model)\n    return converted_model\n\ndef tensorFlow_converter(args):\n    return convert_tensorflow_file(args.source, get_opset(args.ONNXVersion), args.outputNames.split())\n\ndef onnx_converter(args):\n    onnx_model = winmltools.load_model(args.source)\n    return onnx_model\n\nframework_converters = {\n    '': onnx_converter,\n    'coreml': coreml_converter,\n    'keras': keras_converter,\n    'scikit-learn': scikit_learn_converter,\n    'xgboost': xgboost_converter,\n    'libsvm': libSVM_converter,\n    'tensorflow': tensorFlow_converter\n}\n\nsuffix_converters = {\n    'h5': keras_converter,\n    'keras': keras_converter,\n    'mlmodel': coreml_converter,\n    'onnx': onnx_converter,\n}\n\n\ndef main(args):\n    # TODO: framework converter check.\n    source_extension = get_extension(args.source)\n    framework = args.framework.lower()\n    frame_converter = framework_converters.get(framework)\n    suffix_converter = suffix_converters.get(source_extension) \n\n    if not (frame_converter or suffix_converter):\n        raise RuntimeError('Conversion from extension {} is not supported'.format(source_extension))\n\n    if frame_converter and suffix_converter and (frame_converter != suffix_converter):\n        raise RuntimeError('model with extension {} do not come from {}'.format(source_extension, framework))\n\n    onnx_model = None\n    if frame_converter:\n        onnx_model = frame_converter(args)\n    else:\n        onnx_model = suffix_converter(args)\n    \n    if(args.quantizationOption and 'none' != args.quantizationOption):\n        onnx_model = winmltools.quantize(onnx_model, use_dequantize_linear=get_useDequantize(args.quantizationOption))\n    \n    \n    if 'tensorflow' == framework:\n        with open(args.destination, 'wb') as file:\n            file.write(onnx_model.SerializeToString())\n    else:\n        save_onnx(onnx_model, args.destination)\n\n\nif __name__ == '__main__':\n    main(parse_args())\n"""
Samples/CustomOperator/customize_model/scripts/debug_all_outputs.py,0,"b""# this script will interlace debug operators for every intermediate output of the onnx graph\r\n\r\nimport sys\r\nimport os.path as path\r\nimport onnx\r\nfrom onnx import helper\r\nimport argparse\r\n\r\ndef create_modified_model(args):\r\n    model_path = path.join(path.dirname(path.abspath(__file__)), args.model_path)\r\n    modified_path = path.join(path.dirname(path.abspath(__file__)), args.modified_path)\r\n    model = onnx.load(model_path)\r\n\r\n    intermediate_outputs = set()\r\n\r\n    # gather each unique intermediate output in the graph\r\n    for node in model.graph.node:\r\n        for output in node.output:\r\n            intermediate_outputs.add(output)\r\n\r\n    # remove final graph outputs (see the definition of graph outputs in the onnx standard: https://github.com/onnx/onnx/blob/master/docs/IR.md#user-content-graphs)\r\n    # it would be invalid to add debug operators after the final graph outputs since execution must be finished once all outputs are written to\r\n    for output in model.graph.output:\r\n        intermediate_outputs.remove(output.name)\r\n\r\n    # create a debug operator that consumes each intermediate output\r\n    # debug operator file path attribute is constructed by the name of the intermediate output\r\n    for output in intermediate_outputs:\r\n        inserted_node = model.graph.node.add()\r\n        valid_output_path = output.replace('/', '\\\\')\r\n        inserted_node.CopyFrom(helper.make_node('Debug', [output], ['unused_' + output], file_type=args.debug_file_type, file_path=path.join(valid_output_path,  valid_output_path)))\r\n\r\n    onnx.save(model, modified_path)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--model_path', help='source model path to add debug nodes', required=True)\r\n    parser.add_argument('--modified_path', help='output model path', required=True)\r\n    parser.add_argument('--debug_file_type', help='file_type attribute of Debug Operator inserted', choices=['png', 'text'], required=True)\r\n    args = parser.parse_args()\r\n    create_modified_model(args)"""
Samples/CustomOperator/customize_model/scripts/debug_single_output.py,0,"b""# this script add debug operators for the given intermediate output\r\n\r\nimport sys\r\nimport os.path as path\r\nimport onnx\r\nfrom onnx import helper\r\nimport argparse\r\n\r\ndef create_modified_model(args):\r\n    model_path = path.join(path.dirname(path.abspath(__file__)), args.model_path)\r\n    modified_path = path.join(path.dirname(path.abspath(__file__)), args.modified_path)\r\n    model = onnx.load(model_path)\r\n\r\n    #insert new debug operator node into graph with given attributes and input\r\n    inserted_node = model.graph.node.add()\r\n    valid_output_path = args.debug_file_path.replace('/', '\\\\')\r\n    inserted_node.CopyFrom(helper.make_node('Debug', [args.intermediate_output], ['unused_' + args.intermediate_output], file_type=args.debug_file_type, file_path=valid_output_path))\r\n\r\n    onnx.save(model, modified_path)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--model_path', help='source model path to add debug nodes', required=True)\r\n    parser.add_argument('--modified_path', help='output model path', required=True)\r\n    parser.add_argument('--debug_file_type', help='file_type attribute of Debug Operator inserted', choices=['png', 'text'], required=True)\r\n    parser.add_argument('--debug_file_path', help='file_path attribute of Debug Operator inserted', required=True)\r\n    parser.add_argument('--intermediate_output', help='the intermediate output this Debug Operator will consume', required=True)\r\n    args = parser.parse_args()\r\n    create_modified_model(args)"""
