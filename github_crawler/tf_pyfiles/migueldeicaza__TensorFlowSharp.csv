file_path,api_count,code
tests/TensorFlowSharp.Tests.CSharp/TestData/Adagrad/optimizer_lr_test.py,9,"b'# This script is used to create data file (expected.txt)\n# which is used to compare the output from TensorFlowSharp optimizer tests.\n\nimport tensorflow as tf\n\n# Training data\ntrain_x =[\n    3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n    7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1\n]\ntrain_y = [\n    1.7, 2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n        2.827,3.465,1.65,2.904,2.42,2.94,1.3\n]\nn_samples = len(train_x)\nlearning_rate = 0.01\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.constant(0.1), dtype=tf.float32)\nb = tf.Variable(tf.constant(0.1), dtype=tf.float32)\n\npred = tf.add(tf.multiply(X,W), b)\n\ncost = tf.divide(tf.reduce_sum(tf.pow(tf.subtract(pred, Y), 2.0)), tf.multiply(2.0, n_samples))\noptimizer = tf.train.AdagradOptimizer(learning_rate).minimize(cost, name = ""AdagradOptimizer"")\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(init)\n    for e in range(5):\n        for i in range(n_samples):\n            _, cost_v, W_v, b_v, pred_v = session.run([optimizer, cost, W, b, pred], feed_dict = {X: train_x[i], Y: train_y[i]})\n            print(f""loss: {cost_v:.4f}, W: {W_v:.4f}, b: {b_v:.4f}"")\n            #print(""Prediction: %f == Actual: %f"" % (pred_v, train_y[i]))'"
tests/TensorFlowSharp.Tests.CSharp/TestData/AdagradTimeDecay/optimizer_lr_test.py,11,"b'# This script is used to create data file (expected.txt)\n# which is used to compare the output from TensorFlowSharp optimizer tests.\n\nimport tensorflow as tf\n\n# Training data\ntrain_x =[\n    3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n    7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1\n]\ntrain_y = [\n    1.7, 2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n        2.827,3.465,1.65,2.904,2.42,2.94,1.3\n]\nn_samples = len(train_x)\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.constant(0.1), dtype=tf.float32)\nb = tf.Variable(tf.constant(0.1), dtype=tf.float32)\n\npred = tf.add(tf.multiply(X,W), b)\n\nglobal_step = tf.Variable(0, trainable=False)\nlearning_rate = 0.01\ndecay_rate = 0.5\ndecayed_learning_rate = learning_rate * (1. / (1. + decay_rate * tf.cast(global_step, tf.float32)))\n\ncost = tf.divide(tf.reduce_sum(tf.pow(tf.subtract(pred, Y), 2.0)), tf.multiply(2.0, n_samples))\noptimizer = tf.train.AdagradOptimizer(decayed_learning_rate).minimize(cost, global_step=global_step, name = ""AdagradOptimizer"")\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(init)\n    for e in range(5):\n        for i in range(n_samples):\n            _, cost_v, W_v, b_v, pred_v, lr_v, step_v = session.run([optimizer, cost, W, b, pred, decayed_learning_rate, global_step], feed_dict = {X: train_x[i], Y: train_y[i]})\n            print(f""step: {step_v:d}, loss: {cost_v:.4f}, W: {W_v:.4f}, b: {b_v:.4f}, lr: {lr_v:.8f}"")\n            #print(""Prediction: %f == Actual: %f"" % (pred_v, train_y[i]))'"
tests/TensorFlowSharp.Tests.CSharp/TestData/Adam/optimizer_lr_test.py,9,"b'# This script is used to create data file (expected.txt)\n# which is used to compare the output from TensorFlowSharp optimizer tests.\n\nimport tensorflow as tf\n\n# Training data\ntrain_x =[\n    3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n    7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1\n]\ntrain_y = [\n    1.7, 2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n        2.827,3.465,1.65,2.904,2.42,2.94,1.3\n]\nn_samples = len(train_x)\nlearning_rate = 0.01\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.constant(0.1), dtype=tf.float32)\nb = tf.Variable(tf.constant(0.1), dtype=tf.float32)\n\npred = tf.add(tf.multiply(X,W), b)\n\ncost = tf.divide(tf.reduce_sum(tf.pow(tf.subtract(pred, Y), 2.0)), tf.multiply(2.0, n_samples))\noptimizer = tf.train.AdamOptimizer(learning_rate, epsilon=1e-7).minimize(cost, name = ""AdamOptimizer"")\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(init)\n    for e in range(5):\n        for i in range(n_samples):\n            _, cost_v, W_v, b_v, pred_v = session.run([optimizer, cost, W, b, pred], feed_dict = {X: train_x[i], Y: train_y[i]})\n            print(f""loss: {cost_v:.4f}, W: {W_v:.4f}, b: {b_v:.4f}"")\n            #print(""Prediction: %f == Actual: %f"" % (pred_v, train_y[i]))'"
tests/TensorFlowSharp.Tests.CSharp/TestData/AdamTimeDecay/optimizer_lr_test.py,11,"b'# This script is used to create data file (expected.txt)\n# which is used to compare the output from TensorFlowSharp optimizer tests.\n\nimport tensorflow as tf\n\n# Training data\ntrain_x =[\n    3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n    7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1\n]\ntrain_y = [\n    1.7, 2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n        2.827,3.465,1.65,2.904,2.42,2.94,1.3\n]\nn_samples = len(train_x)\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.constant(0.1), dtype=tf.float32)\nb = tf.Variable(tf.constant(0.1), dtype=tf.float32)\n\npred = tf.add(tf.multiply(X,W), b)\n\nglobal_step = tf.Variable(0, trainable=False)\nlearning_rate = 0.01\ndecay_rate = 0.5\ndecayed_learning_rate = learning_rate * (1. / (1. + decay_rate * tf.cast(global_step, tf.float32)))\n\ncost = tf.divide(tf.reduce_sum(tf.pow(tf.subtract(pred, Y), 2.0)), tf.multiply(2.0, n_samples))\noptimizer = tf.train.AdamOptimizer(decayed_learning_rate, epsilon=1e-7).minimize(cost, global_step=global_step, name = ""AdamOptimizer"")\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(init)\n    for e in range(5):\n        for i in range(n_samples):\n            _, cost_v, W_v, b_v, pred_v = session.run([optimizer, cost, W, b, pred], feed_dict = {X: train_x[i], Y: train_y[i]})\n            print(f""loss: {cost_v:.4f}, W: {W_v:.4f}, b: {b_v:.4f}"")\n            #print(""Prediction: %f == Actual: %f"" % (pred_v, train_y[i]))'"
tests/TensorFlowSharp.Tests.CSharp/TestData/Momentum/optimizer_lr_test.py,9,"b'# This script is used to create data file (expected.txt)\n# which is used to compare the output from TensorFlowSharp optimizer tests.\n\nimport tensorflow as tf\n\n# Training data\ntrain_x =[\n    3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n    7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1\n]\ntrain_y = [\n    1.7, 2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n        2.827,3.465,1.65,2.904,2.42,2.94,1.3\n]\nn_samples = len(train_x)\nlearning_rate = 0.01\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.constant(0.1), dtype=tf.float32)\nb = tf.Variable(tf.constant(0.1), dtype=tf.float32)\n\npred = tf.add(tf.multiply(X,W), b)\n\ncost = tf.divide(tf.reduce_sum(tf.pow(tf.subtract(pred, Y), 2.0)), tf.multiply(2.0, n_samples))\noptimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(cost, name = ""MomentumOptimizer"")\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(init)\n    for e in range(2):\n        for i in range(n_samples):\n            _, cost_v, W_v, b_v, pred_v = session.run([optimizer, cost, W, b, pred], feed_dict = {X: train_x[i], Y: train_y[i]})\n            print(f""loss: {cost_v:.4f}, W: {W_v:.4f}, b: {b_v:.4f}"")\n            #print(""Prediction: %f == Actual: %f"" % (pred_v, train_y[i]))'"
tests/TensorFlowSharp.Tests.CSharp/TestData/MomentumNesterov/optimizer_lr_test.py,9,"b'# This script is used to create data file (expected.txt)\n# which is used to compare the output from TensorFlowSharp optimizer tests.\n\nimport tensorflow as tf\n\n# Training data\ntrain_x =[\n    3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n    7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1\n]\ntrain_y = [\n    1.7, 2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n        2.827,3.465,1.65,2.904,2.42,2.94,1.3\n]\nn_samples = len(train_x)\nlearning_rate = 0.01\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.constant(0.1), dtype=tf.float32)\nb = tf.Variable(tf.constant(0.1), dtype=tf.float32)\n\npred = tf.add(tf.multiply(X,W), b)\n\ncost = tf.divide(tf.reduce_sum(tf.pow(tf.subtract(pred, Y), 2.0)), tf.multiply(2.0, n_samples))\noptimizer = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True).minimize(cost, name = ""MomentumOptimizer"")\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(init)\n    for e in range(2):\n        for i in range(n_samples):\n            _, cost_v, W_v, b_v, pred_v = session.run([optimizer, cost, W, b, pred], feed_dict = {X: train_x[i], Y: train_y[i]})\n            print(f""loss: {cost_v:.4f}, W: {W_v:.4f}, b: {b_v:.4f}"")\n            #print(""Prediction: %f == Actual: %f"" % (pred_v, train_y[i]))'"
tests/TensorFlowSharp.Tests.CSharp/TestData/MomentumNesterovTimeDecay/optimizer_lr_test.py,12,"b'# This script is used to create data file (expected.txt)\n# which is used to compare the output from TensorFlowSharp optimizer tests.\n#\n# NOTE: This script is not used to generate the expected.txt file in this case \n# because of the tf.train.MomentumOptimizer implemention difference with decay.\n# The expected.txt is actually the output from the test itself.\nimport tensorflow as tf\n\n# Training data\ntrain_x =[\n    3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n    7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1\n]\ntrain_y = [\n    1.7, 2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n        2.827,3.465,1.65,2.904,2.42,2.94,1.3\n]\nn_samples = len(train_x)\nlearning_rate = 0.01\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.constant(0.1), dtype=tf.float32)\nb = tf.Variable(tf.constant(0.1), dtype=tf.float32)\n\npred = tf.add(tf.multiply(X,W), b)\n\nglobal_step = tf.Variable(0, trainable=False)\nlearning_rate = 0.01\ndecay_rate = 0.5\ndecayed_learning_rate = learning_rate * (1. / (1. + decay_rate * tf.cast(global_step, tf.float32)))\n\ncost = tf.divide(tf.reduce_sum(tf.pow(tf.subtract(pred, Y), 2.0)), tf.multiply(2.0, n_samples))\noptimizer = tf.train.MomentumOptimizer(decayed_learning_rate, 0.9, use_nesterov=True).minimize(cost, global_step=global_step, name = ""MomentumOptimizer"")\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(init)\n    for e in range(2):\n        for i in range(n_samples):\n            _, cost_v, W_v, b_v, pred_v, lr_v, step_v = session.run([optimizer, cost, W, b, pred, decayed_learning_rate, global_step], feed_dict = {X: train_x[i], Y: train_y[i]})\n            print(f""step: {step_v:d}, loss: {cost_v:.4f}, W: {W_v:.4f}, b: {b_v:.4f}, lr: {lr_v:.8f}"")\n            #print(""Prediction: %f == Actual: %f"" % (pred_v, train_y[i]))'"
tests/TensorFlowSharp.Tests.CSharp/TestData/MomentumTimeDecay/optimizer_lr_test.py,12,"b'# This script is used to create data file (expected.txt)\n# which is used to compare the output from TensorFlowSharp optimizer tests.\n#\n# NOTE: This script is not used to generate the expected.txt file in this case \n# because of the tf.train.MomentumOptimizer implemention difference with decay.\n# The expected.txt is actually the output from the test itself.\nimport tensorflow as tf\n\n# Training data\ntrain_x =[\n    3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n    7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1\n]\ntrain_y = [\n    1.7, 2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n        2.827,3.465,1.65,2.904,2.42,2.94,1.3\n]\nn_samples = len(train_x)\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.constant(0.1), dtype=tf.float32)\nb = tf.Variable(tf.constant(0.1), dtype=tf.float32)\n\npred = tf.add(tf.multiply(X,W), b)\n\nglobal_step = tf.Variable(0, trainable=False)\nlearning_rate = 0.01\ndecay_rate = 0.5\ndecayed_learning_rate = learning_rate * (1. / (1. + decay_rate * tf.cast(global_step, tf.float32)))\n\ncost = tf.divide(tf.reduce_sum(tf.pow(tf.subtract(pred, Y), 2.0)), tf.multiply(2.0, n_samples))\noptimizer = tf.train.MomentumOptimizer(decayed_learning_rate, 0.9).minimize(cost, global_step=global_step, name = ""MomentumOptimizer"")\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(init)\n    for e in range(2):\n        for i in range(n_samples):\n            _, cost_v, W_v, b_v, pred_v, lr_v, step_v = session.run([optimizer, cost, W, b, pred, decayed_learning_rate, global_step], feed_dict = {X: train_x[i], Y: train_y[i]})\n            print(f""step: {step_v:d}, loss: {cost_v:.4f}, W: {W_v:.4f}, b: {b_v:.4f}, lr: {lr_v:.8f}"")\n            #print(""Prediction: %f == Actual: %f"" % (pred_v, train_y[i]))'"
tests/TensorFlowSharp.Tests.CSharp/TestData/RMSProp/optimizer_lr_test.py,9,"b'# This script is used to create data file (expected.txt)\n# which is used to compare the output from TensorFlowSharp optimizer tests.\n\nimport tensorflow as tf\n\n# Training data\ntrain_x =[\n    3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n    7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1\n]\ntrain_y = [\n    1.7, 2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n        2.827,3.465,1.65,2.904,2.42,2.94,1.3\n]\nn_samples = len(train_x)\nlearning_rate = 0.01\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.constant(0.1), dtype=tf.float32)\nb = tf.Variable(tf.constant(0.1), dtype=tf.float32)\n\npred = tf.add(tf.multiply(X,W), b)\n\ncost = tf.divide(tf.reduce_sum(tf.pow(tf.subtract(pred, Y), 2.0)), tf.multiply(2.0, n_samples))\noptimizer = tf.train.RMSPropOptimizer(learning_rate, momentum=0.0,epsilon=1e-7).minimize(cost, name = ""RMSPropOptimizer"")\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(init)\n    for e in range(5):\n        for i in range(n_samples):\n            _, cost_v, W_v, b_v, pred_v = session.run([optimizer, cost, W, b, pred], feed_dict = {X: train_x[i], Y: train_y[i]})\n            print(f""loss: {cost_v:.4f}, W: {W_v:.4f}, b: {b_v:.4f}"")\n            #print(""Prediction: %f == Actual: %f"" % (pred_v, train_y[i]))'"
tests/TensorFlowSharp.Tests.CSharp/TestData/RMSPropTimeDecay/optimizer_lr_test.py,11,"b'# This script is used to create data file (expected.txt)\n# which is used to compare the output from TensorFlowSharp optimizer tests.\n\nimport tensorflow as tf\n\n# Training data\ntrain_x =[\n    3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n    7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1\n]\ntrain_y = [\n    1.7, 2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n        2.827,3.465,1.65,2.904,2.42,2.94,1.3\n]\nn_samples = len(train_x)\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.constant(0.1), dtype=tf.float32)\nb = tf.Variable(tf.constant(0.1), dtype=tf.float32)\n\npred = tf.add(tf.multiply(X,W), b)\n\nglobal_step = tf.Variable(0, trainable=False)\nlearning_rate = 0.01\ndecay_rate = 0.5\ndecayed_learning_rate = learning_rate * (1. / (1. + decay_rate * tf.cast(global_step, tf.float32)))\n\ncost = tf.divide(tf.reduce_sum(tf.pow(tf.subtract(pred, Y), 2.0)), tf.multiply(2.0, n_samples))\noptimizer = tf.train.RMSPropOptimizer(decayed_learning_rate, momentum=0.0,epsilon=1e-7).minimize(cost, global_step=global_step, name = ""AdagradOptimizer"")\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(init)\n    for e in range(5):\n        for i in range(n_samples):\n            _, cost_v, W_v, b_v, pred_v, lr_v, step_v = session.run([optimizer, cost, W, b, pred, decayed_learning_rate, global_step], feed_dict = {X: train_x[i], Y: train_y[i]})\n            print(f""step: {step_v:d}, loss: {cost_v:.4f}, W: {W_v:.4f}, b: {b_v:.4f}, lr: {lr_v:.8f}"")\n            #print(""Prediction: %f == Actual: %f"" % (pred_v, train_y[i]))'"
tests/TensorFlowSharp.Tests.CSharp/TestData/SGD/optimizer_lr_test.py,9,"b'# This script is used to create data file (expected.txt)\n# which is used to compare the output from TensorFlowSharp optimizer tests.\n\nimport tensorflow as tf\n\n# Training data\ntrain_x =[\n    3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n    7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1\n]\ntrain_y = [\n    1.7, 2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n        2.827,3.465,1.65,2.904,2.42,2.94,1.3\n]\nn_samples = len(train_x)\nlearning_rate = 0.01\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.constant(0.1), dtype=tf.float32)\nb = tf.Variable(tf.constant(0.1), dtype=tf.float32)\n\npred = tf.add(tf.multiply(X,W), b)\n\ncost = tf.divide(tf.reduce_sum(tf.pow(tf.subtract(pred, Y), 2.0)), tf.multiply(2.0, n_samples))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost, name = ""SGDOptimizer"")\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(init)\n    for e in range(5):\n        for i in range(n_samples):\n            _, cost_v, W_v, b_v, pred_v = session.run([optimizer, cost, W, b, pred], feed_dict = {X: train_x[i], Y: train_y[i]})\n            print(f""loss: {cost_v:.4f}, W: {W_v:.4f}, b: {b_v:.4f}"")\n            #print(""Prediction: %f == Actual: %f"" % (pred_v, train_y[i]))'"
tests/TensorFlowSharp.Tests.CSharp/TestData/SGDMnist/optimizer_lr_test.py,20,"b'# This script is used to create data file (expected.txt)\n# which is used to compare the output from TensorFlowSharp optimizer tests.\n#\n# NOTE: This script is not used to generate the expected.txt file in this case \n# because of the tf.train.MomentumOptimizer implemention difference with decay.\n# The expected.txt is actually the output from the test itself.\n\nimport tensorflow as tf\nfrom keras.utils.np_utils import to_categorical\nimport math\nimport tensorflow as tf\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\nx_train = x_train.reshape((-1,784))\nx_test = x_test.reshape((-1,784))\n\ny_train = to_categorical(y_train, num_classes=10)\ny_test = to_categorical(y_test, num_classes=10)\n\nn_samples = len(x_train)\nlearning_rate = 0.1\nX = tf.placeholder(tf.float32, shape=[None, 784])\nY = tf.placeholder(tf.float32, shape=[None, 10])\n\ntf.set_random_seed(1)\ninitB = 4 * math.sqrt(6) / math.sqrt(784 + 500)\nW1 = tf.Variable(tf.random_uniform([x_train.shape[1], 500], minval=-initB, maxval=initB)) \nb1 = tf.Variable(tf.constant(0., shape=[500], dtype=tf.float32))\nlayer1 = tf.nn.sigmoid(tf.add(tf.matmul(X,W1), b1))\n\ninitB = 4 * math.sqrt(6) / math.sqrt(500 + 100)\nW2 = tf.Variable(tf.random_uniform([500, 100], minval=-initB, maxval=initB))\nb2 = tf.Variable(tf.constant(0., shape=[100], dtype=tf.float32))\nlayer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1,W2), b2))\n\ninitB = 4 * math.sqrt(6) / math.sqrt(100 + 10)\nW3 = tf.Variable(tf.random_uniform([100, 10], minval=-initB, maxval=initB))\nb3 = tf.Variable(tf.constant(0., shape=[10], dtype=tf.float32))\nlayer3 = tf.add(tf.matmul(layer2,W3), b3)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=layer3))\noptimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9).minimize(cost, name = ""SGDOptimizer"")\n\nprediction = tf.nn.softmax(layer3, name = ""Prediction"")\naccuracy = tf.reduce_mean( tf.cast(tf.equal( tf.argmax(prediction,1), tf.argmax(Y, 1)), tf.float32), name = ""Accuracy"")\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    batch_size =100\n    total_batch = int(x_train.shape[0] / batch_size)\n    for epoch in range(5):\n        avg_loss = 0\n        avg_acc = 0\n        for batch_idx in range(0, x_train.shape[0], batch_size):\n            X_batch = x_train[batch_idx:batch_idx+batch_size]\n            Y_batch = y_train[batch_idx:batch_idx+batch_size]\n            _, loss_val, acc = sess.run([optimizer, cost, accuracy], feed_dict={X: X_batch, Y: Y_batch})\n            avg_loss += loss_val / total_batch\n            avg_acc += acc / total_batch\n        print(\'Epoch: \', \'%04d\' % (epoch+1), \'cost (cross-entropy) = %.4f , acc = %.4f\' % (avg_loss, avg_acc))'"
tests/TensorFlowSharp.Tests.CSharp/TestData/SGDTimeDecay/optimizer_lr_test.py,11,"b'# This script is used to create data file (expected.txt)\n# which is used to compare the output from TensorFlowSharp optimizer tests.\n\nimport tensorflow as tf\n\n# Training data\ntrain_x =[\n    3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n    7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1\n]\ntrain_y = [\n    1.7, 2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n        2.827,3.465,1.65,2.904,2.42,2.94,1.3\n]\nn_samples = len(train_x)\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.constant(0.1), dtype=tf.float32)\nb = tf.Variable(tf.constant(0.1), dtype=tf.float32)\n\npred = tf.add(tf.multiply(X,W), b)\n\nglobal_step = tf.Variable(0, trainable=False)\nlearning_rate = 0.01\ndecay_rate = 0.5\ndecayed_learning_rate = learning_rate * (1. / (1. + decay_rate * tf.cast(global_step, tf.float32)))\n\ncost = tf.divide(tf.reduce_sum(tf.pow(tf.subtract(pred, Y), 2.0)), tf.multiply(2.0, n_samples))\noptimizer = tf.train.GradientDescentOptimizer(decayed_learning_rate).minimize(cost, global_step=global_step, name = ""SGDOptimizer"")\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as session:\n    session.run(init)\n    for e in range(5):\n        for i in range(n_samples):\n            _, cost_v, W_v, b_v, pred_v, lr_v, step_v = session.run([optimizer, cost, W, b, pred, decayed_learning_rate, global_step], feed_dict = {X: train_x[i], Y: train_y[i]})\n            print(f""step: {step_v:d}, loss: {cost_v:.4f}, W: {W_v:.4f}, b: {b_v:.4f}, lr: {lr_v:.8f}"")\n            #print(""Prediction: %f == Actual: %f"" % (pred_v, train_y[i]))'"
