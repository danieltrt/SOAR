file_path,api_count,code
net.py,8,"b""import numpy as np\n\nfrom libs.ops import *\n\n\nclass DCGANGenerator(object):\n\n  def __init__(self, hidden_dim=128, batch_size=64, hidden_activation=tf.nn.relu, output_activation=tf.nn.tanh, use_batch_norm=True, z_distribution='normal', scope='generator', **kwargs):\n    self.hidden_dim = hidden_dim\n    self.batch_size = batch_size\n    self.hidden_activation = hidden_activation\n    self.output_activation = output_activation\n    self.use_batch_norm = use_batch_norm\n    self.z_distribution = z_distribution\n    self.scope = scope\n\n  def __call__(self, z, is_training=True, **kwargs):\n    with tf.variable_scope(self.scope):\n      if self.use_batch_norm:\n        l0  = self.hidden_activation(batch_norm(linear(z, 4 * 4 * 512, name='l0', stddev=0.02), name='bn0', is_training=is_training))\n        l0  = tf.reshape(l0, [self.batch_size, 4, 4, 512])\n        dc1 = self.hidden_activation(batch_norm(deconv2d( l0, [self.batch_size,  8,  8, 256], name='dc1', stddev=0.02), name='bn1', is_training=is_training))\n        dc2 = self.hidden_activation(batch_norm(deconv2d(dc1, [self.batch_size, 16, 16, 128], name='dc2', stddev=0.02), name='bn2', is_training=is_training))\n        dc3 = self.hidden_activation(batch_norm(deconv2d(dc2, [self.batch_size, 32, 32,  64], name='dc3', stddev=0.02), name='bn3', is_training=is_training))\n        dc4 = self.output_activation(deconv2d(dc3, [self.batch_size, 32, 32, 3], 3, 3, 1, 1, name='dc4', stddev=0.02))\n      else:\n        l0  = self.hidden_activation(linear(z, 4 * 4 * 512, name='l0', stddev=0.02))\n        l0  = tf.reshape(l0, [self.batch_size, 4, 4, 512])\n        dc1 = self.hidden_activation(deconv2d(l0, [self.batch_size, 8, 8, 256], name='dc1', stddev=0.02))\n        dc2 = self.hidden_activation(deconv2d(dc1, [self.batch_size, 16, 16, 128], name='dc2', stddev=0.02))\n        dc3 = self.hidden_activation(deconv2d(dc2, [self.batch_size, 32, 32, 64], name='dc3', stddev=0.02))\n        dc4 = self.output_activation(deconv2d(dc3, [self.batch_size, 32, 32, 3], 3, 3, 1, 1, name='dc4', stddev=0.02))\n      x = dc4\n    return x\n\n  def generate_noise(self):\n    if self.z_distribution == 'normal':\n      return np.random.randn(self.batch_size, self.hidden_dim).astype(np.float32)\n    elif self.z_distribution == 'uniform' :\n      return np.random.uniform(-1, 1, (self.batch_size, self.hidden_dim)).astype(np.float32)\n    else:\n      raise NotImplementedError\n\n\nclass SNDCGAN_Discrminator(object):\n\n  def __init__(self, batch_size=64, hidden_activation=lrelu, output_dim=1, scope='critic', **kwargs):\n    self.batch_size = batch_size\n    self.hidden_activation = hidden_activation\n    self.output_dim = output_dim\n    self.scope = scope\n\n  def __call__(self, x, update_collection=tf.GraphKeys.UPDATE_OPS, **kwargs):\n    with tf.variable_scope(self.scope):\n      c0_0 = self.hidden_activation(conv2d(   x,  64, 3, 3, 1, 1, spectral_normed=True, update_collection=update_collection, stddev=0.02, name='c0_0'))\n      c0_1 = self.hidden_activation(conv2d(c0_0, 128, 4, 4, 2, 2, spectral_normed=True, update_collection=update_collection, stddev=0.02, name='c0_1'))\n      c1_0 = self.hidden_activation(conv2d(c0_1, 128, 3, 3, 1, 1, spectral_normed=True, update_collection=update_collection, stddev=0.02, name='c1_0'))\n      c1_1 = self.hidden_activation(conv2d(c1_0, 256, 4, 4, 2, 2, spectral_normed=True, update_collection=update_collection, stddev=0.02, name='c1_1'))\n      c2_0 = self.hidden_activation(conv2d(c1_1, 256, 3, 3, 1, 1, spectral_normed=True, update_collection=update_collection, stddev=0.02, name='c2_0'))\n      c2_1 = self.hidden_activation(conv2d(c2_0, 512, 4, 4, 2, 2, spectral_normed=True, update_collection=update_collection, stddev=0.02, name='c2_1'))\n      c3_0 = self.hidden_activation(conv2d(c2_1, 512, 3, 3, 1, 1, spectral_normed=True, update_collection=update_collection, stddev=0.02, name='c3_0'))\n      c3_0 = tf.reshape(c3_0, [self.batch_size, -1])\n      l4 = linear(c3_0, self.output_dim, spectral_normed=True, update_collection=update_collection, stddev=0.02, name='l4')\n    return tf.reshape(l4, [-1])\n"""
test_sn_implementation.py,14,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nfrom libs.sn import spectral_normed_weight\nimport timeit\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\nSPECTRAL_NORM_UPDATE_OPS = ""spectral_norm_update_ops""\n\nx = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(np.random.normal(size=[784, 10], scale=0.02), name=\'W\', dtype=tf.float32)\nb = tf.Variable(tf.zeros([10]), name=\'b\', dtype=tf.float32)\nW_bar, sigma = spectral_normed_weight(W, num_iters=1, with_sigma=True, update_collection=SPECTRAL_NORM_UPDATE_OPS)\n\ny = tf.nn.softmax(tf.matmul(x, W_bar) + b)\n\ny_ = tf.placeholder(tf.float32, [None, 10])\n\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\ntrain_step = tf.train.GradientDescentOptimizer(0.001).minimize(cross_entropy)\n\ns, _, _ = tf.svd(W)\ns_bar, _, _ = tf.svd(W_bar)\nsess = tf.InteractiveSession()\ntf.global_variables_initializer().run()\nupdate_ops = tf.get_collection(SPECTRAL_NORM_UPDATE_OPS)\nfor _ in range(1000):\n  start = timeit.default_timer()\n  batch_xs, batch_ys = mnist.train.next_batch(100)\n  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n  sigma_, s_, s_bar_ = sess.run([sigma, s, s_bar])\n  # TESTING:\n  # Check for s_[0] (largest singular value) - sigma\n  # They are very close. Difference mostly around less than 5%\n  # Also, svd of W_bar is close to 1\n  # So I assume my implementation of singular value power iteration approximation is correct\n  for update_op in update_ops:\n    sess.run(update_op)\n  stop = timeit.default_timer()\n  print(\'Iteration:\', _, \'\\tW max SVD: \', s_[0], \'\\tW max SVD approx: \', sigma_, \'\\tPercentage difference: \',\n        abs(s_[0] - sigma_) / s_[0] * 100, \'\\tW_bar max SVD: \', s_bar_[0], \'\\tTime: \', stop - start)\ncorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n'"
train.py,20,"b'import timeit\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom libs.input_helper import Cifar10\nfrom libs.utils import save_images, mkdir\nfrom net import DCGANGenerator, SNDCGAN_Discrminator\nimport _pickle as pickle\nfrom libs.inception_score.model import get_inception_score\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_integer(\'batch_size\', 64, \'\')\nflags.DEFINE_integer(\'max_iter\', 100000, \'\')\nflags.DEFINE_integer(\'snapshot_interval\', 1000, \'interval of snapshot\')\nflags.DEFINE_integer(\'evaluation_interval\', 10000, \'interval of evalution\')\nflags.DEFINE_integer(\'display_interval\', 100, \'interval of displaying log to console\')\nflags.DEFINE_float(\'adam_alpha\', 0.0001, \'learning rate\')\nflags.DEFINE_float(\'adam_beta1\', 0.5, \'beta1 in Adam\')\nflags.DEFINE_float(\'adam_beta2\', 0.999, \'beta2 in Adam\')\nflags.DEFINE_integer(\'n_dis\', 1, \'n discrminator train\')\n\nmkdir(\'tmp\')\n\nINCEPTION_FILENAME = \'inception_score.pkl\'\nconfig = FLAGS.__flags\ngenerator = DCGANGenerator(**config)\ndiscriminator = SNDCGAN_Discrminator(**config)\ndata_set = Cifar10(batch_size=FLAGS.batch_size)\n\nglobal_step = tf.Variable(0, name=""global_step"", trainable=False)\nincrease_global_step = global_step.assign(global_step + 1)\nis_training = tf.placeholder(tf.bool, shape=())\nz = tf.placeholder(tf.float32, shape=[None, generator.generate_noise().shape[1]])\nx_hat = generator(z, is_training=is_training)\nx = tf.placeholder(tf.float32, shape=x_hat.shape)\n\nd_fake = discriminator(x_hat, update_collection=None)\n# Don\'t need to collect on the second call, put NO_OPS\nd_real = discriminator(x, update_collection=""NO_OPS"")\n# Softplus at the end as in the official code of author at chainer-gan-lib github repository\nd_loss = tf.reduce_mean(tf.nn.softplus(d_fake) + tf.nn.softplus(-d_real))\ng_loss = tf.reduce_mean(tf.nn.softplus(-d_fake))\nd_loss_summary_op = tf.summary.scalar(\'d_loss\', d_loss)\ng_loss_summary_op = tf.summary.scalar(\'g_loss\', g_loss)\nmerged_summary_op = tf.summary.merge_all()\nsummary_writer = tf.summary.FileWriter(\'snapshots\')\n\nd_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'critic\')\ng_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'generator\')\noptimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.adam_alpha, beta1=FLAGS.adam_beta1, beta2=FLAGS.adam_beta2)\nd_gvs = optimizer.compute_gradients(d_loss, var_list=d_vars)\ng_gvs = optimizer.compute_gradients(g_loss, var_list=g_vars)\nd_solver = optimizer.apply_gradients(d_gvs)\ng_solver = optimizer.apply_gradients(g_gvs)\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\nsess.run(tf.global_variables_initializer())\nsaver = tf.train.Saver()\nif tf.train.latest_checkpoint(\'snapshots\') is not None:\n  saver.restore(sess, tf.train.latest_checkpoint(\'snapshots\'))\n\nnp.random.seed(1337)\nsample_noise = generator.generate_noise()\nnp.random.seed()\niteration = sess.run(global_step)\nstart = timeit.default_timer()\n\nis_start_iteration = True\ninception_scores = []\nwhile iteration < FLAGS.max_iter:\n  _, g_loss_curr = sess.run([g_solver, g_loss], feed_dict={z: generator.generate_noise(), is_training: True})\n  for _ in range(FLAGS.n_dis):\n    _, d_loss_curr, summaries = sess.run([d_solver, d_loss, merged_summary_op],\n                                         feed_dict={x: data_set.get_next_batch(), z: generator.generate_noise(), is_training: True})\n  # increase global step after updating G and D\n  # before saving the model so that it will be written into the ckpt file\n  sess.run(increase_global_step)\n  if (iteration + 1) % FLAGS.display_interval == 0 and not is_start_iteration:\n    summary_writer.add_summary(summaries, global_step=iteration)\n    stop = timeit.default_timer()\n    print(\'Iter {}: d_loss = {:4f}, g_loss = {:4f}, time = {:2f}s\'.format(iteration, d_loss_curr, g_loss_curr, stop - start))\n    start = stop\n  if (iteration + 1) % FLAGS.snapshot_interval == 0 and not is_start_iteration:\n    saver.save(sess, \'snapshots/model.ckpt\', global_step=iteration)\n    sample_images = sess.run(x_hat, feed_dict={z: sample_noise, is_training: False})\n    save_images(sample_images, \'tmp/{:06d}.png\'.format(iteration))\n  if (iteration + 1) % FLAGS.evaluation_interval == 0:\n    sample_images = sess.run(x_hat, feed_dict={z: sample_noise, is_training: False})\n    save_images(sample_images, \'tmp/{:06d}.png\'.format(iteration))\n    # Sample 50000 images for evaluation\n    print(""Evaluating..."")\n    num_images_to_eval = 50000\n    eval_images = []\n    num_batches = num_images_to_eval // FLAGS.batch_size + 1\n    print(""Calculating Inception Score. Sampling {} images..."".format(num_images_to_eval))\n    np.random.seed(0)\n    for _ in range(num_batches):\n      images = sess.run(x_hat, feed_dict={z: generator.generate_noise(), is_training: False})\n      eval_images.append(images)\n    np.random.seed()\n    eval_images = np.vstack(eval_images)\n    eval_images = eval_images[:num_images_to_eval]\n    eval_images = np.clip((eval_images + 1.0) * 127.5, 0.0, 255.0).astype(np.uint8)\n    # Calc Inception score\n    eval_images = list(eval_images)\n    inception_score_mean, inception_score_std = get_inception_score(eval_images)\n    print(""Inception Score: Mean = {} \\tStd = {}."".format(inception_score_mean, inception_score_std))\n    inception_scores.append(dict(mean=inception_score_mean, std=inception_score_std))\n    with open(INCEPTION_FILENAME, \'wb\') as f:\n      pickle.dump(inception_scores, f)\n  iteration += 1\n  is_start_iteration = False\n'"
libs/input_helper.py,0,"b'from keras.datasets import cifar10\nimport numpy as np\n\nclass Cifar10(object):\n  def __init__(self, batch_size=64, test=False):\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    if test:\n      images = x_test\n    else:\n      images = x_train\n    self.images = (images - 127.5) / 127.5\n    self.batch_size = batch_size\n    self.num_samples = len(self.images)\n    self.shuffle_samples()\n    self.next_batch_pointer = 0\n\n  def shuffle_samples(self):\n    image_indices = np.random.permutation(np.arange(self.num_samples))\n    self.images = self.images[image_indices]\n\n  def get_next_batch(self):\n    num_samples_left = self.num_samples - self.next_batch_pointer\n    if num_samples_left >= self.batch_size:\n      batch = self.images[self.next_batch_pointer:self.next_batch_pointer + self.batch_size]\n      self.next_batch_pointer += self.batch_size\n    else:\n      partial_batch_1 = self.images[self.next_batch_pointer:self.num_samples]\n      self.shuffle_samples()\n      partial_batch_2 = self.images[0:self.batch_size - num_samples_left]\n      batch = np.vstack((partial_batch_1, partial_batch_2))\n      self.next_batch_pointer = self.batch_size - num_samples_left\n    return batch\n'"
libs/ops.py,25,"b'import numpy as np\nimport tensorflow as tf\n\nfrom libs.sn import spectral_normed_weight\n\n\ndef scope_has_variables(scope):\n  return len(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope.name)) > 0\n\n\ndef conv2d(input_, output_dim,\n           k_h=4, k_w=4, d_h=2, d_w=2, stddev=None,\n           name=""conv2d"", spectral_normed=False, update_collection=None, with_w=False, padding=""SAME""):\n  # Glorot intialization\n  # For RELU nonlinearity, it\'s sqrt(2./(n_in)) instead\n  fan_in = k_h * k_w * input_.get_shape().as_list()[-1]\n  fan_out = k_h * k_w * output_dim\n  if stddev is None:\n    stddev = np.sqrt(2. / (fan_in))\n\n  with tf.variable_scope(name) as scope:\n    if scope_has_variables(scope):\n      scope.reuse_variables()\n    w = tf.get_variable(""w"", [k_h, k_w, input_.get_shape()[-1], output_dim],\n                        initializer=tf.truncated_normal_initializer(stddev=stddev))\n    if spectral_normed:\n      conv = tf.nn.conv2d(input_, spectral_normed_weight(w, update_collection=update_collection),\n                          strides=[1, d_h, d_w, 1], padding=padding)\n    else:\n      conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=padding)\n\n    biases = tf.get_variable(""b"", [output_dim], initializer=tf.constant_initializer(0.0))\n    conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n\n    if with_w:\n      return conv, w, biases\n    else:\n      return conv\n\n\ndef deconv2d(input_, output_shape,\n             k_h=4, k_w=4, d_h=2, d_w=2, stddev=None,\n             name=""deconv2d"", spectral_normed=False, update_collection=None, with_w=False, padding=""SAME""):\n  # Glorot initialization\n  # For RELU nonlinearity, it\'s sqrt(2./(n_in)) instead\n  fan_in = k_h * k_w * input_.get_shape().as_list()[-1]\n  fan_out = k_h * k_w * output_shape[-1]\n  if stddev is None:\n    stddev = np.sqrt(2. / (fan_in))\n\n  with tf.variable_scope(name) as scope:\n    if scope_has_variables(scope):\n      scope.reuse_variables()\n    # filter : [height, width, output_channels, in_channels]\n    w = tf.get_variable(""w"", [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n                        initializer=tf.truncated_normal_initializer(stddev=stddev))\n    if spectral_normed:\n      deconv = tf.nn.conv2d_transpose(input_, spectral_normed_weight(w, update_collection=update_collection),\n                                      output_shape=output_shape,\n                                      strides=[1, d_h, d_w, 1], padding=padding)\n    else:\n      deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n                                      strides=[1, d_h, d_w, 1], padding=padding)\n\n    biases = tf.get_variable(""b"", [output_shape[-1]], initializer=tf.constant_initializer(0))\n    deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n    if with_w:\n      return deconv, w, biases\n    else:\n      return deconv\n\n\ndef lrelu(x, leak=0.1):\n  return tf.maximum(x, leak * x)\n\n\ndef linear(input_, output_size, name=""linear"", spectral_normed=False, update_collection=None, stddev=None, bias_start=0.0, with_biases=True,\n           with_w=False):\n  shape = input_.get_shape().as_list()\n\n  if stddev is None:\n    stddev = np.sqrt(1. / (shape[1]))\n  with tf.variable_scope(name) as scope:\n    if scope_has_variables(scope):\n      scope.reuse_variables()\n    weight = tf.get_variable(""w"", [shape[1], output_size], tf.float32,\n                             tf.truncated_normal_initializer(stddev=stddev))\n    if with_biases:\n      bias = tf.get_variable(""b"", [output_size],\n                             initializer=tf.constant_initializer(bias_start))\n    if spectral_normed:\n      mul = tf.matmul(input_, spectral_normed_weight(weight, update_collection=update_collection))\n    else:\n      mul = tf.matmul(input_, weight)\n    if with_w:\n      if with_biases:\n        return mul + bias, weight, bias\n      else:\n        return mul, weight, None\n    else:\n      if with_biases:\n        return mul + bias\n      else:\n        return mul\n\n\ndef batch_norm(input, is_training=True, momentum=0.9, epsilon=2e-5, in_place_update=True, name=""batch_norm""):\n  if in_place_update:\n    return tf.contrib.layers.batch_norm(input,\n                                        decay=momentum,\n                                        center=True,\n                                        scale=True,\n                                        epsilon=epsilon,\n                                        updates_collections=None,\n                                        is_training=is_training,\n                                        scope=name)\n  else:\n    return tf.contrib.layers.batch_norm(input,\n                                        decay=momentum,\n                                        center=True,\n                                        scale=True,\n                                        epsilon=epsilon,\n                                        is_training=is_training,\n                                        scope=name)\n'"
libs/sn.py,16,"b'import tensorflow as tf\nimport warnings\n\n\nNO_OPS = \'NO_OPS\'\n\n\ndef _l2normalize(v, eps=1e-12):\n  return v / (tf.reduce_sum(v ** 2) ** 0.5 + eps)\n\n\ndef spectral_normed_weight(W, u=None, num_iters=1, update_collection=None, with_sigma=False):\n  # Usually num_iters = 1 will be enough\n  W_shape = W.shape.as_list()\n  W_reshaped = tf.reshape(W, [-1, W_shape[-1]])\n  if u is None:\n    u = tf.get_variable(""u"", [1, W_shape[-1]], initializer=tf.truncated_normal_initializer(), trainable=False)\n  def power_iteration(i, u_i, v_i):\n    v_ip1 = _l2normalize(tf.matmul(u_i, tf.transpose(W_reshaped)))\n    u_ip1 = _l2normalize(tf.matmul(v_ip1, W_reshaped))\n    return i + 1, u_ip1, v_ip1\n  _, u_final, v_final = tf.while_loop(\n    cond=lambda i, _1, _2: i < num_iters,\n    body=power_iteration,\n    loop_vars=(tf.constant(0, dtype=tf.int32),\n               u, tf.zeros(dtype=tf.float32, shape=[1, W_reshaped.shape.as_list()[0]]))\n  )\n  if update_collection is None:\n    warnings.warn(\'Setting update_collection to None will make u being updated every W execution. This maybe undesirable\'\n                  \'. Please consider using a update collection instead.\')\n    sigma = tf.matmul(tf.matmul(v_final, W_reshaped), tf.transpose(u_final))[0, 0]\n    # sigma = tf.reduce_sum(tf.matmul(u_final, tf.transpose(W_reshaped)) * v_final)\n    W_bar = W_reshaped / sigma\n    with tf.control_dependencies([u.assign(u_final)]):\n      W_bar = tf.reshape(W_bar, W_shape)\n  else:\n    sigma = tf.matmul(tf.matmul(v_final, W_reshaped), tf.transpose(u_final))[0, 0]\n    # sigma = tf.reduce_sum(tf.matmul(u_final, tf.transpose(W_reshaped)) * v_final)\n    W_bar = W_reshaped / sigma\n    W_bar = tf.reshape(W_bar, W_shape)\n    # Put NO_OPS to not update any collection. This is useful for the second call of discriminator if the update_op\n    # has already been collected on the first call.\n    if update_collection != NO_OPS:\n      tf.add_to_collection(update_collection, u.assign(u_final))\n  if with_sigma:\n    return W_bar, sigma\n  else:\n    return W_bar\n'"
libs/utils.py,0,"b'""""""\nFrom official improved_gan_training github repository\nImage grid saver, based on color_grid_vis from github.com/Newmu\n""""""\n\nimport numpy as np\nimport scipy.misc\nfrom scipy.misc import imsave\nimport os\n\n\ndef save_images(X, save_path):\n  # [-1, 1] -> [0,255]\n  if isinstance(X.flatten()[0], np.floating):\n    X = ((X + 1.) * 127.5).astype(\'uint8\')\n\n  n_samples = X.shape[0]\n  rows = int(np.sqrt(n_samples))\n  while n_samples % rows != 0:\n    rows -= 1\n\n  nh, nw = rows, n_samples // rows\n\n  if X.ndim == 2:\n    X = np.reshape(X, (X.shape[0], int(np.sqrt(X.shape[1])), int(np.sqrt(X.shape[1]))))\n\n  if X.ndim == 4:\n    h, w = X[0].shape[:2]\n    img = np.zeros((h * nh, w * nw, 3))\n  elif X.ndim == 3:\n    h, w = X[0].shape[:2]\n    img = np.zeros((h * nh, w * nw))\n\n  for n, x in enumerate(X):\n    j = n // nw\n    i = n % nw\n    img[j * h:j * h + h, i * w:i * w + w] = x\n\n  imsave(save_path, img)\n\n\ndef mkdir(dir_name):\n  if not os.path.exists(dir_name):\n    os.makedirs(dir_name)\n'"
libs/inception_score/model.py,8,"b'# Code derived from tensorflow/tensorflow/models/image/imagenet/classify_image.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path\nimport sys\nimport tarfile\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\nimport glob\nimport scipy.misc\nimport math\nimport sys\n\nMODEL_DIR = \'/tmp/imagenet\'\nDATA_URL = \'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\'\nsoftmax = None\n\n# Call this function with list of images. Each of elements should be a\n# numpy array with values ranging from 0 to 255.\ndef get_inception_score(images, splits=10):\n  assert(type(images) == list)\n  assert(type(images[0]) == np.ndarray)\n  assert(len(images[0].shape) == 3)\n  assert(np.max(images[0]) > 10)\n  assert(np.min(images[0]) >= 0.0)\n  inps = []\n  for img in images:\n    img = img.astype(np.float32)\n    inps.append(np.expand_dims(img, 0))\n  bs = 100\n  with tf.Session() as sess:\n    preds = []\n    n_batches = int(math.ceil(float(len(inps)) / float(bs)))\n    for i in range(n_batches):\n        sys.stdout.write(""."")\n        sys.stdout.flush()\n        inp = inps[(i * bs):min((i + 1) * bs, len(inps))]\n        inp = np.concatenate(inp, 0)\n        pred = sess.run(softmax, {\'ExpandDims:0\': inp})\n        preds.append(pred)\n    preds = np.concatenate(preds, 0)\n    scores = []\n    for i in range(splits):\n      part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n      kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n      kl = np.mean(np.sum(kl, 1))\n      scores.append(np.exp(kl))\n    return np.mean(scores), np.std(scores)\n\n# This function is called automatically.\ndef _init_inception():\n  global softmax\n  if not os.path.exists(MODEL_DIR):\n    os.makedirs(MODEL_DIR)\n  filename = DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(MODEL_DIR, filename)\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n          filename, float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Succesfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  tarfile.open(filepath, \'r:gz\').extractall(MODEL_DIR)\n  with tf.gfile.FastGFile(os.path.join(\n      MODEL_DIR, \'classify_image_graph_def.pb\'), \'rb\') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    _ = tf.import_graph_def(graph_def, name=\'\')\n  # Works with an arbitrary minibatch size.\n  with tf.Session() as sess:\n    pool3 = sess.graph.get_tensor_by_name(\'pool_3:0\')\n    ops = pool3.graph.get_operations()\n    for op_idx, op in enumerate(ops):\n        for o in op.outputs:\n            shape = o.get_shape()\n            shape = [s.value for s in shape]\n            new_shape = []\n            for j, s in enumerate(shape):\n                if s == 1 and j == 0:\n                    new_shape.append(None)\n                else:\n                    new_shape.append(s)\n            o._shape = tf.TensorShape(new_shape)\n    w = sess.graph.get_operation_by_name(""softmax/logits/MatMul"").inputs[1]\n    logits = tf.matmul(tf.squeeze(pool3), w)\n    softmax = tf.nn.softmax(logits)\n\nif softmax is None:\n  _init_inception()'"
