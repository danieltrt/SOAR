file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\nlong_description = \'\'\'\nA simple Python package that wraps existing model fine-tuning and generation scripts for OpenAI GPT-2 text generation model (specifically the ""small"", 124M hyperparameter version). Additionally, this package allows easier generation of text, generating to a file for easy curation, allowing for prefixes to force the text to start with a given phrase.\n\n## Usage\n\nAn example for downloading the model to the local system, fineturning it on a dataset. and generating some text.\n\nWarning: the pretrained model, and thus any finetuned model, is 500 MB!\n\n```python\nimport gpt_2_simple as gpt2\n\ngpt2.download_gpt2()   # model is saved into current directory under /models/124M/\n\nsess = gpt2.start_tf_sess()\ngpt2.finetune(sess, \'shakespeare.txt\', steps=1000)   # steps is max number of training steps\n\ngpt2.generate(sess)\n```\n\nThe generated model checkpoints are by default in `/checkpoint/run1`. If you want to load a model from that folder and generate text from it:\n\n```python\nimport gpt_2_simple as gpt2\n\nsess = gpt2.start_tf_sess()\ngpt2.load_gpt2(sess)\n\ngpt2.generate(sess)\n```\n\nAs with textgenrnn, you can generate and save text for later use (e.g. an API or a bot) by using the `return_as_list` parameter.\n\n```python\nsingle_text = gpt2.generate(sess, return_as_list=True)[0]\nprint(single_text)\n```\n\nYou can pass a `run_name` parameter to `finetune` and `load_gpt2` if you want to store/load multiple models in a `checkpoint` folder.\n\nNB: *Restart the Python session first* if you want to finetune on another dataset or load another model.\n\'\'\'\n\n\nsetup(\n    name=\'gpt_2_simple\',\n    packages=[\'gpt_2_simple\'],  # this must be the same as the name above\n    version=\'0.7.1\',\n    description=""Python package to easily retrain OpenAI\'s GPT-2 "" \\\n    ""text-generating model on new texts."",\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    author=\'Max Woolf\',\n    author_email=\'max@minimaxir.com\',\n    url=\'https://github.com/minimaxir/gpt-2-simple\',\n    keywords=[\'deep learning\', \'tensorflow\', \'text generation\'],\n    classifiers=[],\n    license=\'MIT\',\n    entry_points={\n        \'console_scripts\': [\'gpt_2_simple=gpt_2_simple.gpt_2:cmd\'],\n    },\n    python_requires=\'>=3.5\',\n    include_package_data=True,\n    install_requires=[\'regex\', \'requests\', \'tqdm\', \'numpy\', \'toposort\']\n)\n'"
gpt_2_simple/__init__.py,0,b'from .gpt_2 import *'
gpt_2_simple/gpt_2.py,28,"b'import tarfile\nimport os\nimport json\nimport requests\nimport sys\nimport shutil\nimport re\nfrom tqdm import tqdm, trange\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.core.protobuf import rewriter_config_pb2\nfrom tensorflow.python.client import device_lib\nimport time\nfrom datetime import datetime\nimport csv\nimport argparse\n\n# if in Google Colaboratory\ntry:\n    from google.colab import drive\nexcept:\n    pass\n\nfrom gpt_2_simple.src import model, sample, encoder, memory_saving_gradients\nfrom gpt_2_simple.src.load_dataset import load_dataset, Sampler\nfrom gpt_2_simple.src.accumulate import AccumulatingOptimizer\n\nassert tf.__version__ < \'2.0.0\', ""gpt-2-simple currently does not support "" \\\n    ""TensorFlow 2.0. You\'ll need to use a virtualenv/cloud computer which "" \\\n    ""has Tensorflow 1.X on it.""\n\n\ndef download_file_with_progress(url_base, sub_dir, model_name, file_name):\n    """"""General utility for incrementally downloading files from the internet\n    with progress bar\n    from url_base / sub_dir / filename\n    to local file system sub_dir / filename\n\n    Parameters\n    ----------\n    file_name : str\n        name of file to get e.g. ""hparams.json""\n    sub_dir: str\n        subdirectory inside which to get and copy locally eg. ""models/124M"" \n        no trailing slash\n    url_base : str\n        Start of URL location specifying server and any base directories no \n        trailing slash\n        e.g. ""https://storage.googleapis.com/gpt-2""\n    """"""\n\n    # set to download 1MB at a time. This could be much larger with no issue\n    DOWNLOAD_CHUNK_SIZE = 1024 * 1024\n    r = requests.get(url_base + ""/models/"" + model_name + ""/"" + file_name, stream=True)\n    with open(os.path.join(sub_dir, file_name), \'wb\') as f:\n        file_size = int(r.headers[""content-length""])\n        with tqdm(ncols=100, desc=""Fetching "" + file_name,\n                  total=file_size, unit_scale=True) as pbar:\n            for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n                f.write(chunk)\n                pbar.update(DOWNLOAD_CHUNK_SIZE)\n   \n\ndef download_gpt2(model_dir=\'models\', model_name=\'124M\'):\n    """"""Downloads the GPT-2 model into the current directory\n    from Google Cloud Storage.\n\n    Parameters\n    ----------\n    model_dir : str\n        parent directory of model to download\n\n    model_name : str\n        name of the GPT-2 model to download. \n        As of 22 May 2019 one of ""124M"" or ""355M"" but may later include other \n        model sizes\n\n    Adapted from https://github.com/openai/gpt-2/blob/master/download_model.py\n    """"""\n\n    # create the <model_dir>/<model_name> subdirectory if not present\n    sub_dir = os.path.join(model_dir, model_name)\n    if not os.path.exists(sub_dir):\n        os.makedirs(sub_dir)\n    sub_dir = sub_dir.replace(\'\\\\\', \'/\')  # needed for Windows\n\n    for file_name in [\'checkpoint\', \'encoder.json\', \'hparams.json\',\n                      \'model.ckpt.data-00000-of-00001\', \'model.ckpt.index\',\n                      \'model.ckpt.meta\', \'vocab.bpe\']:\n        download_file_with_progress(url_base=""https://storage.googleapis.com/gpt-2"",\n                                    sub_dir=sub_dir,\n                                    model_name=model_name,\n                                    file_name=file_name)\n\n\ndef start_tf_sess(threads=-1, server=None):\n    """"""\n    Returns a tf.Session w/ config\n    """"""\n    config = tf.compat.v1.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF\n    if threads > 0:\n        config.intra_op_parallelism_threads = threads\n        config.inter_op_parallelism_threads = threads\n\n    if server is not None:\n        return tf.compat.v1.Session(target=server.target, config=config)\n    \n    return tf.compat.v1.Session(config=config)\n\n\ndef reset_session(sess, threads=-1, server=None):\n    """"""Resets the current TensorFlow session, to clear memory\n    or load another model.\n    """"""\n\n    tf.compat.v1.reset_default_graph()\n    sess.close()\n    sess = start_tf_sess(threads, server)\n    return sess\n\ndef get_available_gpus():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == \'GPU\']\n\ndef finetune(sess,\n             dataset,\n             steps=-1,\n             model_name=\'124M\',\n             model_dir=\'models\',\n             combine=50000,\n             batch_size=1,\n             learning_rate=0.0001,\n             accumulate_gradients=5,\n             restore_from=\'latest\',\n             run_name=\'run1\',\n             checkpoint_dir=\'checkpoint\',\n             sample_every=100,\n             sample_length=1023,\n             sample_num=1,\n             multi_gpu=False,\n             save_every=1000,\n             print_every=1,\n             max_checkpoints=1,\n             use_memory_saving_gradients=False,\n             only_train_transformer_layers=False,\n             optimizer=\'adam\',\n             overwrite=False):\n    """"""Finetunes the model on the given dataset.\n\n    Adapted from https://github.com/nshepperd/gpt-2/blob/finetuning/train.py.\n    See that file for parameter definitions.\n    """"""\n\n    # assert model_name not in [\'774M\', \'1558M\'] or multi_gpu, ""Currently, a modern single GPU cannot finetune the 774M GPT-2 model or larger.""\n\n    SAMPLE_DIR = \'samples\'\n\n    checkpoint_path = os.path.join(checkpoint_dir, run_name)\n\n    def maketree(path):\n        try:\n            os.makedirs(path)\n        except:\n            pass\n\n    maketree(checkpoint_path)\n    files = [f for f in os.listdir(checkpoint_path)]\n    for file in [\'hparams.json\', \'encoder.json\', \'vocab.bpe\']:\n        try:\n            shutil.copyfile(os.path.join(model_dir, model_name, file),\n                            os.path.join(checkpoint_path, file))\n        except FileNotFoundError as fnf_error:\n            print(""You need to download the GPT-2 model first via download_gpt2()"")\n            raise(fnf_error)\n\n    enc = encoder.get_encoder(checkpoint_path)\n    hparams = model.default_hparams()\n    with open(os.path.join(checkpoint_path, \'hparams.json\')) as f:\n        hparams.override_from_dict(json.load(f))\n\n    if sample_length > hparams.n_ctx:\n        raise ValueError(\n            ""Can\'t get samples longer than window size: %s"" % hparams.n_ctx)\n\n    if model_name not in [\'117M\', \'124M\']:\n        use_memory_saving_gradients = True\n        only_train_transformer_layers = True\n        accumulate_gradients = 1\n\n    context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n    gpus = []\n\n    if multi_gpu:\n        gpus = get_available_gpus()\n\n    output = model.model(hparams=hparams, X=context, gpus=gpus)\n    loss = tf.reduce_mean(\n        input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n            labels=context[:, 1:], logits=output[\'logits\'][:, :-1]))\n\n    tf_sample = sample.sample_sequence(\n        hparams=hparams,\n        length=sample_length,\n        context=context,\n        batch_size=batch_size,\n        temperature=1.0,\n        top_k=40)\n\n    all_vars = [v for v in tf.compat.v1.trainable_variables() if \'model\' in v.name]\n    train_vars = [v for v in all_vars if \'/h\' in v.name] if only_train_transformer_layers else all_vars\n\n    if optimizer == \'adam\':\n        opt = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n    elif optimizer == \'sgd\':\n        opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n\n    if accumulate_gradients > 1:\n        if use_memory_saving_gradients:\n            exit(""Memory saving gradients are not implemented for gradient accumulation yet."")\n        opt = AccumulatingOptimizer(\n            opt=opt,\n            var_list=train_vars)\n        opt_reset = opt.reset()\n        opt_compute = opt.compute_gradients(loss)\n        opt_apply = opt.apply_gradients()\n        summary_loss = tf.compat.v1.summary.scalar(\'loss\', opt_apply)\n    else:\n        if use_memory_saving_gradients:\n            opt_grads = memory_saving_gradients.gradients(loss, train_vars)\n        else:\n            opt_grads = tf.gradients(ys=loss, xs=train_vars)\n        opt_grads = list(zip(opt_grads, train_vars))\n        opt_apply = opt.apply_gradients(opt_grads)\n        summary_loss = tf.compat.v1.summary.scalar(\'loss\', loss)\n\n    summary_log = tf.compat.v1.summary.FileWriter(checkpoint_path)\n\n    saver = tf.compat.v1.train.Saver(\n        var_list=all_vars,\n        max_to_keep=max_checkpoints)\n    sess.run(tf.compat.v1.global_variables_initializer())\n\n    if restore_from == \'latest\':\n        ckpt = tf.train.latest_checkpoint(checkpoint_path)\n        if ckpt is None:\n            # Get fresh GPT weights if new run.\n            ckpt = tf.train.latest_checkpoint(\n                os.path.join(model_dir, model_name))\n    elif restore_from == \'fresh\':\n        ckpt = tf.train.latest_checkpoint(\n            os.path.join(model_dir, model_name))\n    else:\n        ckpt = tf.train.latest_checkpoint(restore_from)\n    print(\'Loading checkpoint\', ckpt)\n    saver.restore(sess, ckpt)\n\n    print(\'Loading dataset...\')\n    chunks = load_dataset(enc, dataset, combine)\n    data_sampler = Sampler(chunks)\n    print(\'dataset has\', data_sampler.total_size, \'tokens\')\n    print(\'Training...\')\n\n    counter = 1\n    counter_path = os.path.join(checkpoint_path, \'counter\')\n    if os.path.exists(counter_path) and restore_from == \'latest\':\n        # Load the step number if we\'re resuming a run\n        # Add 1 so we don\'t immediately try to save again\n        with open(counter_path, \'r\') as fp:\n            counter = int(fp.read()) + 1\n    counter_base = counter\n\n    def save():\n        maketree(checkpoint_path)\n        print(\n            \'Saving\',\n            os.path.join(checkpoint_path,\n                         \'model-{}\').format(counter-1))\n        saver.save(\n            sess,\n            os.path.join(checkpoint_path, \'model\'),\n            global_step=counter-1)\n        with open(counter_path, \'w\') as fp:\n            fp.write(str(counter-1) + \'\\n\')\n\n    def generate_samples():\n        context_tokens = data_sampler.sample(1)\n        all_text = []\n        index = 0\n        while index < sample_num:\n            out = sess.run(\n                tf_sample,\n                feed_dict={context: batch_size * [context_tokens]})\n            for i in range(min(sample_num - index, batch_size)):\n                text = enc.decode(out[i])\n                text = \'======== SAMPLE {} ========\\n{}\\n\'.format(\n                    index + 1, text)\n                all_text.append(text)\n                index += 1\n        print(text)\n        maketree(os.path.join(SAMPLE_DIR, run_name))\n        with open(\n                os.path.join(SAMPLE_DIR, run_name,\n                             \'samples-{}\').format(counter), \'w\') as fp:\n            fp.write(\'\\n\'.join(all_text))\n\n    def sample_batch():\n        return [data_sampler.sample(1024) for _ in range(batch_size)]\n\n    if overwrite and restore_from == \'latest\':\n        for file in files:\n            if file.startswith(\'model\') or file.startswith(\'events\'):\n                os.remove(os.path.join(checkpoint_path, file))\n        save()\n\n    avg_loss = (0.0, 0.0)\n    start_time = time.time()\n\n    if steps:\n        steps = int(steps)\n    \n    try:\n        while True:\n            if steps > 0 and counter == (counter_base + steps):\n                save()\n                return\n            if (counter - 1) % save_every == 0 and counter > 1:\n                save()\n            if (counter - 1) % sample_every == 0 and counter > 1:\n                generate_samples()\n\n            if accumulate_gradients > 1:\n                sess.run(opt_reset)\n                for _ in range(accumulate_gradients):\n                    sess.run(\n                        opt_compute, feed_dict={context: sample_batch()})\n                (v_loss, v_summary) = sess.run((opt_apply, summary_loss))\n            else:\n                (_, v_loss, v_summary) = sess.run(\n                    (opt_apply, loss, summary_loss),\n                    feed_dict={context: sample_batch()})\n\n            summary_log.add_summary(v_summary, counter)\n\n            if counter % print_every == 0:\n                avg_loss = (avg_loss[0] * 0.99 + v_loss,\n                            avg_loss[1] * 0.99 + 1.0)\n\n                print(\n                    \'[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}\'\n                    .format(\n                        counter=counter,\n                        time=time.time() - start_time,\n                        loss=v_loss,\n                        avg=avg_loss[0] / avg_loss[1]))\n\n            counter += 1\n    except KeyboardInterrupt:\n        print(\'interrupted\')\n        save()\n\n\ndef load_gpt2(sess,\n              checkpoint=\'latest\',\n              run_name=""run1"",\n              checkpoint_dir=""checkpoint"",\n              model_name=None,\n              model_dir=\'models\',\n              multi_gpu=False):\n    """"""Loads the model checkpoint or existing model into a TensorFlow session\n    for repeated predictions.\n    """"""\n\n    if model_name:\n        checkpoint_path = os.path.join(model_dir, model_name)\n    else:\n        checkpoint_path = os.path.join(checkpoint_dir, run_name)\n\n    hparams = model.default_hparams()\n    with open(os.path.join(checkpoint_path, \'hparams.json\')) as f:\n        hparams.override_from_dict(json.load(f))\n\n    context = tf.compat.v1.placeholder(tf.int32, [1, None])\n\n    gpus = []\n    if multi_gpu:\n        gpus = get_available_gpus()\n\n    output = model.model(hparams=hparams, X=context, gpus=gpus)\n\n    if checkpoint==\'latest\':\n        ckpt = tf.train.latest_checkpoint(checkpoint_path)\n    else:\n        ckpt = os.path.join(checkpoint_path,checkpoint)\n\n    saver = tf.compat.v1.train.Saver(allow_empty=True)\n    sess.run(tf.compat.v1.global_variables_initializer())\n\n    if model_name:\n        print(\'Loading pretrained model\', ckpt)\n    else:\n        print(\'Loading checkpoint\', ckpt)\n    saver.restore(sess, ckpt)\n\n\ndef generate(sess,\n             run_name=\'run1\',\n             checkpoint_dir=\'checkpoint\',\n             model_name=None,\n             model_dir=\'models\',\n             sample_dir=\'samples\',\n             return_as_list=False,\n             truncate=None,\n             destination_path=None,\n             sample_delim=\'=\' * 20 + \'\\n\',\n             prefix=None,\n             seed=None,\n             nsamples=1,\n             batch_size=1,\n             length=1023,\n             temperature=0.7,\n             top_k=0,\n             top_p=0.0,\n             include_prefix=True):\n    """"""Generates text from a model loaded into memory.\n\n    Adapted from https://github.com/openai/gpt-2/blob/master/src/interactive_conditional_samples.py\n    """"""\n\n    if batch_size is None:\n        batch_size = 1\n    assert nsamples % batch_size == 0\n\n    if nsamples == 1:\n        sample_delim = \'\'\n\n    if prefix == \'\':\n        prefix = None\n\n    if model_name:\n        checkpoint_path = os.path.join(model_dir, model_name)\n    else:\n        checkpoint_path = os.path.join(checkpoint_dir, run_name)\n\n    enc = encoder.get_encoder(checkpoint_path)\n    hparams = model.default_hparams()\n    with open(os.path.join(checkpoint_path, \'hparams.json\')) as f:\n        hparams.override_from_dict(json.load(f))\n\n    if prefix:\n        context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n        context_tokens = enc.encode(prefix)\n\n    np.random.seed(seed)\n    tf.compat.v1.set_random_seed(seed)\n\n    output = sample.sample_sequence(\n        hparams=hparams,\n        length=min(length, 1023 - (len(context_tokens) if prefix else 0)),\n        start_token=enc.encoder[\'<|endoftext|>\'] if not prefix else None,\n        context=context if prefix else None,\n        batch_size=batch_size,\n        temperature=temperature, top_k=top_k, top_p=top_p\n    )[:, 1:]\n\n    if destination_path:\n        f = open(destination_path, \'w\')\n    generated = 0\n    gen_texts = []\n    while generated < nsamples:\n        if not prefix:\n            out = sess.run(output)\n        else:\n            out = sess.run(output, feed_dict={\n                    context: batch_size * [context_tokens]\n                })\n        for i in range(batch_size):\n            generated += 1\n            gen_text = enc.decode(out[i])\n            if prefix:\n                gen_text = enc.decode(context_tokens[:1]) + gen_text\n            if truncate:\n                truncate_esc = re.escape(truncate)\n                if prefix and not include_prefix:\n                    prefix_esc = re.escape(prefix)\n                    pattern = \'(?:{})(.*?)(?:{})\'.format(prefix_esc,\n                                                         truncate_esc)\n                else:\n                    pattern = \'(.*?)(?:{})\'.format(truncate_esc)\n\n                trunc_text = re.search(pattern, gen_text, re.S)\n                if trunc_text:\n                    gen_text = trunc_text.group(1)\n            gen_text = gen_text.lstrip(\'\\n\')\n            if destination_path:\n                f.write(""{}\\n{}"".format(gen_text, sample_delim))\n            if not return_as_list and not destination_path:\n                print(""{}\\n{}"".format(gen_text, sample_delim), end=\'\')\n            gen_texts.append(gen_text)\n\n    if destination_path:\n        f.close()\n\n    if return_as_list:\n        return gen_texts\n\n\ndef generate_to_file(sess,\n                     run_name=\'run1\',\n                     checkpoint_dir=\'checkpoint\',\n                     model_name=None,\n                     model_dir=\'models\',\n                     truncate=None,\n                     destination_path=\'gpt_2_gen_texts.txt\',\n                     sample_delim=\'=\' * 20 + \'\\n\',\n                     prefix=None,\n                     seed=None,\n                     nsamples=1,\n                     batch_size=1,\n                     length=1023,\n                     temperature=0.7,\n                     top_k=0,\n                     top_p=0.0,\n                     include_prefix=True):\n    """"""Generates the texts to a file.\n\n    sample_delim separates texts: set to \'\' if each text is a small document.\n\n    Adapted from https://github.com/minimaxir/textgenrnn/blob/master/textgenrnn/textgenrnn.py\n    """"""\n\n    generate(sess=sess,\n             run_name=run_name,\n             checkpoint_dir=checkpoint_dir,\n             model_name=model_name,\n             model_dir=model_dir,\n             return_as_list=False,\n             truncate=truncate,\n             destination_path=destination_path,\n             sample_delim=sample_delim,\n             prefix=prefix,\n             seed=seed,\n             nsamples=nsamples,\n             batch_size=batch_size,\n             length=length,\n             temperature=temperature,\n             top_k=top_k,\n             top_p=top_p,\n             include_prefix=include_prefix)\n\n\ndef mount_gdrive():\n    """"""Mounts the user\'s Google Drive in Colaboratory.""""""\n    assert \'google.colab\' in sys.modules, ""You must be in Colaboratory to mount your Google Drive""\n\n    drive.mount(\'/content/drive\')\n\n\ndef is_mounted():\n    """"""Checks if the Google Drive is mounted.""""""\n    assert os.path.isdir(\'/content/drive\'), ""You must mount first using mount_gdrive()""\n\n\ndef get_tarfile_name(checkpoint_folder):\n    """"""Converts a folder path into a filename for a .tar archive""""""\n    tarfile_name = checkpoint_folder.replace(os.path.sep, \'_\') + \'.tar\'\n\n    return tarfile_name\n\n\ndef copy_checkpoint_to_gdrive(run_name=\'run1\', copy_folder=False):\n    """"""Copies the checkpoint folder to a mounted Google Drive.""""""\n    is_mounted()\n\n    checkpoint_folder = os.path.join(\'checkpoint\', run_name)\n\n    if copy_folder:\n        shutil.copytree(checkpoint_folder, ""/content/drive/My Drive/"" + checkpoint_folder)\n    else:\n        file_path = get_tarfile_name(checkpoint_folder)\n\n        # Reference: https://stackoverflow.com/a/17081026\n        with tarfile.open(file_path, \'w\') as tar:\n            tar.add(checkpoint_folder)\n\n        shutil.copyfile(file_path, ""/content/drive/My Drive/"" + file_path)\n\n\ndef copy_checkpoint_from_gdrive(run_name=\'run1\', copy_folder=False):\n    """"""Copies the checkpoint folder from a mounted Google Drive.""""""\n    is_mounted()\n\n    checkpoint_folder = os.path.join(\'checkpoint\', run_name)\n\n    if copy_folder:\n        shutil.copytree(""/content/drive/My Drive/"" + checkpoint_folder, checkpoint_folder)\n    else:\n        file_path = get_tarfile_name(checkpoint_folder)\n\n        shutil.copyfile(""/content/drive/My Drive/"" + file_path, file_path)\n\n        with tarfile.open(file_path, \'r\') as tar:\n            tar.extractall()\n\n\ndef copy_file_to_gdrive(file_path):\n    """"""Copies a file to a mounted Google Drive.""""""\n    is_mounted()\n\n    shutil.copyfile(file_path, ""/content/drive/My Drive/"" + file_path)\n\n\ndef copy_file_from_gdrive(file_path):\n    """"""Copies a file from a mounted Google Drive.""""""\n    is_mounted()\n\n    shutil.copyfile(""/content/drive/My Drive/"" + file_path, file_path)\n\n\ndef is_gpt2_downloaded(model_dir=\'models\', model_name=\'124M\'):\n    """"""Checks if the original model + associated files are present in folder.""""""\n\n    for filename in [\'checkpoint\', \'encoder.json\', \'hparams.json\',\n                     \'model.ckpt.data-00000-of-00001\', \'model.ckpt.index\',\n                     \'model.ckpt.meta\', \'vocab.bpe\']:\n        if not os.path.isfile(os.path.join(model_dir, model_name, filename)):\n            return False\n    return True\n\n\ndef encode_csv(csv_path, out_path=\'csv_encoded.txt\', header=True,\n               start_token=""<|startoftext|>"",\n               end_token=""<|endoftext|>""):\n    """"""Encodes a single-column CSV to a format suitable for gpt-2-simple.\n       Automatically adds the specified prefix and suffix tokens.\n    """"""\n\n    with open(csv_path, \'r\', encoding=\'utf8\', errors=\'ignore\') as f:\n        with open(out_path, \'w\', encoding=\'utf8\', errors=\'ignore\') as w:\n            if header:\n                f.readline()\n            reader = csv.reader(f)\n            for row in reader:\n                w.write(start_token + row[0] + end_token + ""\\n"")\n\n\ndef encode_dataset(file_path, model_dir=\'models\', out_path=\'text_encoded.npz\',\n                   model_name=""124M"",\n                   combine=50000):\n    """"""Preencodes a text document into chunks and compresses it,\n    saving time when generated.\n\n    Adapted from https://github.com/nshepperd/gpt-2/blob/finetuning/encode.py\n    """"""\n\n    model_path = os.path.join(model_dir, model_name)\n    enc = encoder.get_encoder(model_path)\n    print(\'Reading files\')\n    chunks = load_dataset(enc, file_path, combine)\n    print(\'Writing\', out_path)\n    np.savez_compressed(out_path, *chunks)\n\n\ndef cmd():\n    """"""Function called when invoking from the terminal.""""""\n\n    parser = argparse.ArgumentParser(\n        description=""Easily retrain OpenAI\'s GPT-2 text-generating model on new texts. (https://github.com/minimaxir/gpt-2-simple)""\n    )\n\n    # Explicit arguments\n    \n    parser.add_argument(\n        \'--mode\', help=\'Mode for using the CLI (either ""finetune"" or ""generate"") [Required]\', nargs=\'?\')\n    parser.add_argument(\n        \'--run_name\',  help=""[finetune/generate] Run number to save/load the model"",\n        nargs=\'?\', default=\'run1\')\n    parser.add_argument(\n        \'--checkpoint_dir\', help=""[finetune] Path of the checkpoint directory"",\n        nargs=\'?\', default=\'checkpoint\')\n    parser.add_argument(\n        \'--model_name\',  help=""[finetune] Name of the GPT-2 model to finetune"",\n        nargs=\'?\', default=\'124M\')\n    parser.add_argument(\n        \'--model_dir\', help=""[finetune] Path of directory of the GPT-2 model to finetune"",\n        nargs=\'?\', default=\'models\')\n    parser.add_argument(\n        \'--dataset\',  help=""[finetune] Path to the source text."",\n        nargs=\'?\', default=None)\n    parser.add_argument(\n        \'--steps\',  help=""[finetune] Number of steps to train (-1 for infinite)"",\n        nargs=\'?\', default=-1)\n    parser.add_argument(\n        \'--restore_from\',  help=""[finetune] Whether to load model \'fresh\' or from \'latest\' checkpoint."",\n        nargs=\'?\', default=\'latest\')\n    parser.add_argument(\n        \'--sample_every\',  help=""[finetune] After how many steps to print sample"",\n        nargs=\'?\', default=1000000, type=int)\n    parser.add_argument(\n        \'--save_every\',  help=""[finetune] After how many steps to save checkpoint"",\n        nargs=\'?\', default=100, type=int)\n    parser.add_argument(\n        \'--print_every\',  help=""[finetune] After how many steps to print progress"",\n        nargs=\'?\', default=10, type=int)\n    parser.add_argument(\n        \'--optimizer\',  help=""[finetune] Optimizer to use for finetuning (adam or sgd)"",\n        nargs=\'?\', default=\'adam\')\n    parser.add_argument(\n        \'--overwrite\',  help=""[finetune] Overwrite existing model when continuing training"",\n        nargs=\'?\', default=False, type=lambda x: (str(x).lower() == \'true\'))\n    parser.add_argument(\n        \'--nfiles\',  help=""[generate] How many files to generate."",\n        nargs=\'?\', default=1, type=int)\n    parser.add_argument(\n        \'--nsamples\',  help=""[generate] How many texts to generate."",\n        nargs=\'?\', default=1, type=int)\n    parser.add_argument(\n        \'--folder\',  help=""[generate] Folder to save the generated files"",\n        nargs=\'?\', default=""gen"", type=str)\n    parser.add_argument(\n        \'--length\',  help=""[generate] Length (tokens) of the generated texts"",\n        nargs=\'?\', default=1023, type=int)\n    parser.add_argument(\n        \'--temperature\',  help=""[generate] Temperature of the generated texts"",\n        nargs=\'?\', default=0.7, type=float)\n    parser.add_argument(\n        \'--top_k\',  help=""[generate] Sample only from top k tokens"",\n        nargs=\'?\', default=0, type=int)\n    parser.add_argument(\n        \'--top_p\',  help=""[generate] Sample from top p prob (overrides top_k if nonzero)"",\n        nargs=\'?\', default=0.0, type=float)\n    parser.add_argument(\n        \'--batch_size\',  help=""[generate] Batch size for generation (increase for GPUs)"",\n        nargs=\'?\', default=1, type=int)\n    parser.add_argument(\n        \'--prefix\',  help=""[generate] Prefix for generated texts"",\n        nargs=\'?\', default=None)\n    parser.add_argument(\n        \'--truncate\',  help=""[generate] Truncation for generated texts"",\n        nargs=\'?\', default=None)\n    # https://stackoverflow.com/a/46951029\n    parser.add_argument(\n        \'--include_prefix\',  help=""[generate] Include prefix when truncating."",\n        nargs=\'?\', default=True, type=lambda x: (str(x).lower() == \'true\'))\n    parser.add_argument(\n        \'--sample_delim\',  help=""[generate] Delimiter between each generated sample."",\n        nargs=\'?\', default=\'=\' * 20 + \'\\n\', type=str)\n    parser.add_argument(\n        \'--multi_gpu\',  help=""[generate/finetune] Attempt to allocate multiple GPUs for running."",\n        nargs=\'?\', default=True, type=lambda x: (str(x).lower() == \'true\'))\n\n    # Positional arguments\n    parser.add_argument(\'mode\', nargs=\'?\')\n    parser.add_argument(\'dataset\', nargs=\'?\')\n\n    args = parser.parse_args()\n    assert args.mode in [\'finetune\', \'generate\'], ""Mode must be \'finetune\' or \'generate\'""\n\n    if args.mode == \'finetune\':\n        assert args.dataset is not None, ""You need to provide a dataset.""\n\n        cmd_finetune(dataset=args.dataset, run_name=args.run_name,\n                     checkpoint_dir=args.checkpoint_dir,\n                     model_name=args.model_name,\n                     model_dir=args.model_dir,\n                     steps=args.steps, restore_from=args.restore_from,\n                     sample_every=args.sample_every,\n                     save_every=args.save_every,\n                     print_every=args.print_every,\n                     optimizer=args.optimizer,\n                     overwrite=args.overwrite,\n                     multi_gpu=args.multi_gpu)\n    if args.mode == ""generate"":\n        cmd_generate(nfiles=args.nfiles, nsamples=args.nsamples,\n                     folder=args.folder, length=args.length,\n                     temperature=args.temperature, batch_size=args.batch_size,\n                     prefix=args.prefix, truncate=args.truncate,\n                     include_prefix=args.include_prefix,\n                     sample_delim=args.sample_delim, run_name=args.run_name,\n                     checkpoint_dir=args.checkpoint_dir,\n                     top_k=args.top_k, top_p=args.top_p, multi_gpu=args.multi_gpu)\n\n\ndef cmd_finetune(dataset, run_name, checkpoint_dir, model_name, model_dir, steps,\n                 restore_from, sample_every,\n                 save_every, print_every, optimizer, overwrite, multi_gpu):\n    """"""Wrapper script for finetuning the model via the CLI.""""""\n\n    if not is_gpt2_downloaded(model_dir=model_dir, model_name=model_name):\n        download_gpt2(model_dir=model_dir, model_name=model_name)\n\n    sess = start_tf_sess()\n    finetune(sess, dataset=dataset, run_name=run_name,\n             checkpoint_dir=checkpoint_dir,\n             model_name=model_name,\n             model_dir=model_dir,\n             steps=steps, restore_from=restore_from,\n             sample_every=sample_every, save_every=save_every,\n             print_every=print_every,\n             optimizer=optimizer,\n             overwrite=overwrite,\n             multi_gpu=multi_gpu)\n\n\ndef cmd_generate(nfiles, nsamples, folder,\n                 length, temperature, batch_size,\n                 prefix, truncate, include_prefix,\n                 sample_delim, run_name,\n                 checkpoint_dir,\n                 top_k, top_p, multi_gpu):\n    """"""Wrapper script for generating text via the CLI.\n    The files are generated into a folder, which can be downloaded\n    recursively by downloading the entire folder.\n    """"""\n\n    sess = start_tf_sess()\n    load_gpt2(sess, run_name=run_name, checkpoint_dir=checkpoint_dir, multi_gpu=multi_gpu)\n\n    try:\n        os.mkdir(folder)\n    except:\n        shutil.rmtree(folder)\n        os.mkdir(folder)\n\n    for _ in trange(nfiles):\n        gen_file = os.path.join(folder,\n                    \'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt\'.format(datetime.utcnow()))\n\n        generate_to_file(sess,\n                         run_name=run_name,\n                         checkpoint_dir=checkpoint_dir,\n                         destination_path=gen_file,\n                         length=length,\n                         temperature=temperature,\n                         nsamples=nsamples,\n                         batch_size=batch_size,\n                         prefix=prefix,\n                         truncate=truncate,\n                         include_prefix=include_prefix,\n                         sample_delim=sample_delim,\n                         top_k=top_k,\n                         top_p=top_p\n                         )\n'"
gpt_2_simple/src/__init__.py,0,b''
gpt_2_simple/src/accumulate.py,11,"b'import argparse\nimport json\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport time\n\n\nclass AccumulatingOptimizer(object):\n    def __init__(self, opt, var_list):\n        self.opt = opt\n        self.var_list = var_list\n        self.accum_vars = {tv : tf.Variable(tf.zeros_like(tv.read_value()), trainable=False)\n                           for tv in var_list}\n        self.total_loss = tf.Variable(tf.zeros(shape=[], dtype=tf.float32))\n        self.count_loss = tf.Variable(tf.zeros(shape=[], dtype=tf.float32))\n\n    def reset(self):\n        updates = [tv.assign(tf.zeros_like(tv)) for tv in self.accum_vars.values()]\n        updates.append(self.total_loss.assign(tf.zeros(shape=[], dtype=tf.float32)))\n        updates.append(self.count_loss.assign(tf.zeros(shape=[], dtype=tf.float32)))\n        with tf.control_dependencies(updates):\n            return tf.no_op()\n\n    def compute_gradients(self, loss):\n        grads = self.opt.compute_gradients(loss, self.var_list)\n        updates = [self.accum_vars[v].assign_add(g) for (g,v) in grads]\n        updates.append(self.total_loss.assign_add(loss))\n        updates.append(self.count_loss.assign_add(1.0))\n        with tf.control_dependencies(updates):\n            return tf.no_op()\n\n    def apply_gradients(self):\n        grads = [(g,v) for (v,g) in self.accum_vars.items()]\n        with tf.control_dependencies([self.opt.apply_gradients(grads)]):\n            return self.total_loss / self.count_loss\n'"
gpt_2_simple/src/encoder.py,0,"b'""""""Byte pair encoding utilities""""""\n\nimport os\nimport json\nimport regex as re\nfrom functools import lru_cache\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you\'re at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""\n    bs = list(range(ord(""!""), ord(""~"")+1))+list(range(ord(""\xc2\xa1""), ord(""\xc2\xac"")+1))+list(range(ord(""\xc2\xae""), ord(""\xc3\xbf"")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass Encoder:\n    def __init__(self, encoder, bpe_merges, errors=\'replace\'):\n        self.encoder = encoder\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        self.errors = errors # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = re.compile(r""""""\'s|\'t|\'re|\'ve|\'m|\'ll|\'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"""""")\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            token = \'\'.join(self.byte_encoder[b] for b in token.encode(\'utf-8\'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\' \'))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \'\'.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\'utf-8\', errors=self.errors)\n        return text\n\ndef get_encoder(checkpoint_path):\n    with open(os.path.join(checkpoint_path, \'encoder.json\'), \'r\') as f:\n        encoder = json.load(f)\n    with open(os.path.join(checkpoint_path, \'vocab.bpe\'), \'r\', encoding=""utf-8"") as f:\n        bpe_data = f.read()\n    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\'\\n\')[1:-1]]\n    return Encoder(\n        encoder=encoder,\n        bpe_merges=bpe_merges,\n    )\n'"
gpt_2_simple/src/load_dataset.py,0,"b'import glob\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nimport tqdm\nimport csv\n\n\ndef load_dataset(enc, path, combine):\n    paths = []\n    if os.path.isfile(path):\n        # Simple file\n        paths.append(path)\n    elif os.path.isdir(path):\n        # Directory\n        for (dirpath, _, fnames) in os.walk(path):\n            for fname in fnames:\n                paths.append(os.path.join(dirpath, fname))\n    else:\n        # Assume glob\n        paths = glob.glob(path)\n\n    token_chunks = []\n    raw_text = \'\'\n    for path in tqdm.tqdm(paths):\n        if path.endswith(\'.npz\'):\n            # Pre-encoded\n            with np.load(path) as npz:\n                for item in npz.files:\n                    token_chunks.append(npz[item])\n        elif path.endswith(\'.csv\'):\n            start_token = ""<|startoftext|>""\n            end_token = ""<|endoftext|>""\n            with open(path, \'r\', encoding=\'utf8\', errors=\'ignore\') as fp:\n                fp.readline()   # skip header\n                reader = csv.reader(fp)\n                for row in reader:\n                    raw_text += start_token + row[0] + end_token + ""\\n""\n        else:\n            # Plain text\n            with open(path, \'r\', encoding=\'utf8\', errors=\'ignore\') as fp:\n                raw_text += fp.read()\n            if len(raw_text) >= combine:\n                tokens = np.stack(enc.encode(raw_text))\n                token_chunks.append(tokens)\n                raw_text = \'\'\n            else:\n                raw_text += \'<|endoftext|>\'\n    if raw_text:\n        tokens = np.stack(enc.encode(raw_text))\n        token_chunks.append(tokens)\n    return token_chunks\n\n\ndef binary_search(f, lo, hi):\n    if f(lo) or not f(hi):\n        return None\n    while hi > lo + 1:\n        mid = (lo + hi) // 2\n        if f(mid):\n            hi = mid\n        else:\n            lo = mid\n    return hi\n\n\nclass Sampler(object):\n    """"""Fairly samples a slice from a set of variable sized chunks.\n\n    \'Fairly\' means that the distribution is the same as sampling from one concatenated chunk,\n    but without crossing chunk boundaries.""""""\n\n    def __init__(self, chunks):\n        self.chunks = chunks\n        self.total_size = sum(chunk.shape[0] for chunk in chunks)\n        self.boundaries = [0]\n        for i in range(len(chunks)):\n            self.boundaries.append(self.boundaries[-1] + chunks[i].shape[0])\n\n    def sample(self, length):\n        assert length < self.total_size // len(\n            self.chunks\n        ), ""Dataset files are too small to sample {} tokens at a time"".format(\n            length)\n        while True:\n            index = random.randint(0, self.total_size - length - 1)\n            i = binary_search(lambda j: self.boundaries[j] > index, 0,\n                              len(self.boundaries) - 1) - 1\n            if self.boundaries[i + 1] > index + length:\n                within_chunk = index - self.boundaries[i]\n                return self.chunks[i][within_chunk:within_chunk + length]\n'"
gpt_2_simple/src/memory_saving_gradients.py,13,"b'from toposort import toposort\nimport contextlib\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.graph_editor as ge\nimport time\nimport sys\nsys.setrecursionlimit(10000)\n# refers back to current module if we decide to split helpers out\nutil = sys.modules[__name__]\n\n# getting rid of ""WARNING:tensorflow:VARIABLES collection name is deprecated""\nsetattr(tf.compat.v1.GraphKeys, ""VARIABLES"", ""variables"")\n\n# save original gradients since tf.gradient could be monkey-patched to point\n# to our version\nfrom tensorflow.python.ops import gradients as tf_gradients_lib\ntf_gradients = tf_gradients_lib.gradients\n\nMIN_CHECKPOINT_NODE_SIZE=1024    # use lower value during testing\n\n# specific versions we can use to do process-wide replacement of tf.gradients\ndef gradients_speed(ys, xs, grad_ys=None, **kwargs):\n    return gradients(ys, xs, grad_ys, checkpoints=\'speed\', **kwargs)\n\ndef gradients_memory(ys, xs, grad_ys=None, **kwargs):\n    return gradients(ys, xs, grad_ys, checkpoints=\'memory\', **kwargs)\n\ndef gradients_collection(ys, xs, grad_ys=None, **kwargs):\n    return gradients(ys, xs, grad_ys, checkpoints=\'collection\', **kwargs)\n\ndef gradients(ys, xs, grad_ys=None, checkpoints=\'collection\', **kwargs):\n    \'\'\'\n    Authors: Tim Salimans & Yaroslav Bulatov\n\n    memory efficient gradient implementation inspired by ""Training Deep Nets with Sublinear Memory Cost""\n    by Chen et al. 2016 (https://arxiv.org/abs/1604.06174)\n\n    ys,xs,grad_ys,kwargs are the arguments to standard tensorflow tf.gradients\n    (https://www.tensorflow.org/versions/r0.12/api_docs/python/train.html#gradients)\n\n    \'checkpoints\' can either be\n        - a list consisting of tensors from the forward pass of the neural net\n          that we should re-use when calculating the gradients in the backward pass\n          all other tensors that do not appear in this list will be re-computed\n        - a string specifying how this list should be determined. currently we support\n            - \'speed\':  checkpoint all outputs of convolutions and matmuls. these ops are usually the most expensive,\n                        so checkpointing them maximizes the running speed\n                        (this is a good option if nonlinearities, concats, batchnorms, etc are taking up a lot of memory)\n            - \'memory\': try to minimize the memory usage\n                        (currently using a very simple strategy that identifies a number of bottleneck tensors in the graph to checkpoint)\n            - \'collection\': look for a tensorflow collection named \'checkpoints\', which holds the tensors to checkpoint\n    \'\'\'\n\n    #    print(""Calling memsaving gradients with"", checkpoints)\n    if not isinstance(ys,list):\n        ys = [ys]\n    if not isinstance(xs,list):\n        xs = [xs]\n\n    bwd_ops = ge.get_backward_walk_ops([y.op for y in ys],\n                                       inclusive=True)\n\n    debug_print(""bwd_ops: %s"", bwd_ops)\n\n    # forward ops are all ops that are candidates for recomputation\n    fwd_ops = ge.get_forward_walk_ops([x.op for x in xs],\n                                      inclusive=True,\n                                      within_ops=bwd_ops)\n    debug_print(""fwd_ops: %s"", fwd_ops)\n\n    # exclude ops with no inputs\n    fwd_ops = [op for op in fwd_ops if op.inputs]\n\n    # don\'t recompute xs, remove variables\n    xs_ops = _to_ops(xs)\n    fwd_ops = [op for op in fwd_ops if not op in xs_ops]\n    fwd_ops = [op for op in fwd_ops if not \'/assign\' in op.name]\n    fwd_ops = [op for op in fwd_ops if not \'/Assign\' in op.name]\n    fwd_ops = [op for op in fwd_ops if not \'/read\' in op.name]\n    ts_all = ge.filter_ts(fwd_ops, True) # get the tensors\n    ts_all = [t for t in ts_all if \'/read\' not in t.name]\n    ts_all = set(ts_all) - set(xs) - set(ys)\n\n    # construct list of tensors to checkpoint during forward pass, if not\n    # given as input\n    if type(checkpoints) is not list:\n        if checkpoints == \'collection\':\n            checkpoints = tf.compat.v1.get_collection(\'checkpoints\')\n\n        elif checkpoints == \'speed\':\n            # checkpoint all expensive ops to maximize running speed\n            checkpoints = ge.filter_ts_from_regex(fwd_ops, \'conv2d|Conv|MatMul\')\n\n        elif checkpoints == \'memory\':\n\n            # remove very small tensors and some weird ops\n            def fixdims(t): # tf.Dimension values are not compatible with int, convert manually\n                try:\n                    return [int(e if e.value is not None else 64) for e in t]\n                except:\n                    return [0]  # unknown shape\n            ts_all = [t for t in ts_all if np.prod(fixdims(t.shape)) > MIN_CHECKPOINT_NODE_SIZE]\n            ts_all = [t for t in ts_all if \'L2Loss\' not in t.name]\n            ts_all = [t for t in ts_all if \'entropy\' not in t.name]\n            ts_all = [t for t in ts_all if \'FusedBatchNorm\' not in t.name]\n            ts_all = [t for t in ts_all if \'Switch\' not in t.name]\n            ts_all = [t for t in ts_all if \'dropout\' not in t.name]\n            # DV: FP16_FIX - need to add \'Cast\' layer here to make it work for FP16\n            ts_all = [t for t in ts_all if \'Cast\' not in t.name]\n\n            # filter out all tensors that are inputs of the backward graph\n            with util.capture_ops() as bwd_ops:\n                tf_gradients(ys, xs, grad_ys, **kwargs)\n\n            bwd_inputs = [t for op in bwd_ops for t in op.inputs]\n            # list of tensors in forward graph that is in input to bwd graph\n            ts_filtered = list(set(bwd_inputs).intersection(ts_all))\n            debug_print(""Using tensors %s"", ts_filtered)\n\n            # try two slightly different ways of getting bottlenecks tensors\n            # to checkpoint\n            for ts in [ts_filtered, ts_all]:\n\n                # get all bottlenecks in the graph\n                bottleneck_ts = []\n                for t in ts:\n                    b = set(ge.get_backward_walk_ops(t.op, inclusive=True, within_ops=fwd_ops))\n                    f = set(ge.get_forward_walk_ops(t.op, inclusive=False, within_ops=fwd_ops))\n                    # check that there are not shortcuts\n                    b_inp = set([inp for op in b for inp in op.inputs]).intersection(ts_all)\n                    f_inp = set([inp for op in f for inp in op.inputs]).intersection(ts_all)\n                    if not set(b_inp).intersection(f_inp) and len(b_inp)+len(f_inp) >= len(ts_all):\n                        bottleneck_ts.append(t)  # we have a bottleneck!\n                    else:\n                        debug_print(""Rejected bottleneck candidate and ops %s"", [t] + list(set(ts_all) - set(b_inp) - set(f_inp)))\n\n                # success? or try again without filtering?\n                if len(bottleneck_ts) >= np.sqrt(len(ts_filtered)): # yes, enough bottlenecks found!\n                    break\n\n            if not bottleneck_ts:\n                raise Exception(\'unable to find bottleneck tensors! please provide checkpoint nodes manually, or use checkpoints=""speed"".\')\n\n            # sort the bottlenecks\n            bottlenecks_sorted_lists = tf_toposort(bottleneck_ts, within_ops=fwd_ops)\n            sorted_bottlenecks = [t for ts in bottlenecks_sorted_lists for t in ts]\n\n            # save an approximately optimal number ~ sqrt(N)\n            N = len(ts_filtered)\n            if len(bottleneck_ts) <= np.ceil(np.sqrt(N)):\n                checkpoints = sorted_bottlenecks\n            else:\n                step = int(np.ceil(len(bottleneck_ts) / np.sqrt(N)))\n                checkpoints = sorted_bottlenecks[step::step]\n\n        else:\n            raise Exception(\'%s is unsupported input for ""checkpoints""\' % (checkpoints,))\n\n    checkpoints = list(set(checkpoints).intersection(ts_all))\n\n    # at this point automatic selection happened and checkpoints is list of nodes\n    assert isinstance(checkpoints, list)\n\n    debug_print(""Checkpoint nodes used: %s"", checkpoints)\n    # better error handling of special cases\n    # xs are already handled as checkpoint nodes, so no need to include them\n    xs_intersect_checkpoints = set(xs).intersection(set(checkpoints))\n    if xs_intersect_checkpoints:\n        debug_print(""Warning, some input nodes are also checkpoint nodes: %s"",\n                    xs_intersect_checkpoints)\n    ys_intersect_checkpoints = set(ys).intersection(set(checkpoints))\n    debug_print(""ys: %s, checkpoints: %s, intersect: %s"", ys, checkpoints,\n                ys_intersect_checkpoints)\n    # saving an output node (ys) gives no benefit in memory while creating\n    # new edge cases, exclude them\n    if ys_intersect_checkpoints:\n        debug_print(""Warning, some output nodes are also checkpoints nodes: %s"",\n              format_ops(ys_intersect_checkpoints))\n\n    # remove initial and terminal nodes from checkpoints list if present\n    checkpoints = list(set(checkpoints) - set(ys) - set(xs))\n\n    # check that we have some nodes to checkpoint\n    # if not checkpoints:\n    #     raise Exception(\'no checkpoints nodes found or given as input! \')\n\n    # disconnect dependencies between checkpointed tensors\n    checkpoints_disconnected = {}\n    for x in checkpoints:\n        if x.op and x.op.name is not None:\n            grad_node = tf.stop_gradient(x, name=x.op.name+""_sg"")\n        else:\n            grad_node = tf.stop_gradient(x)\n        checkpoints_disconnected[x] = grad_node\n\n    # partial derivatives to the checkpointed tensors and xs\n    ops_to_copy = fast_backward_ops(seed_ops=[y.op for y in ys],\n                                    stop_at_ts=checkpoints, within_ops=fwd_ops)\n    debug_print(""Found %s ops to copy within fwd_ops %s, seed %s, stop_at %s"",\n                    len(ops_to_copy), fwd_ops, [r.op for r in ys], checkpoints)\n    debug_print(""ops_to_copy = %s"", ops_to_copy)\n    debug_print(""Processing list %s"", ys)\n    copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(ops_to_copy), {})\n    for origin_op, op in info._transformed_ops.items():\n        op._set_device(origin_op.node_def.device)\n    copied_ops = info._transformed_ops.values()\n    debug_print(""Copied %s to %s"", ops_to_copy, copied_ops)\n    ge.reroute_ts(checkpoints_disconnected.values(), checkpoints_disconnected.keys(), can_modify=copied_ops)\n    debug_print(""Rewired %s in place of %s restricted to %s"",\n                checkpoints_disconnected.values(), checkpoints_disconnected.keys(), copied_ops)\n\n    # get gradients with respect to current boundary + original x\'s\n    copied_ys = [info._transformed_ops[y.op]._outputs[0] for y in ys]\n    boundary = list(checkpoints_disconnected.values())\n    dv = tf_gradients(ys=copied_ys, xs=boundary+xs, grad_ys=grad_ys, **kwargs)\n    debug_print(""Got gradients %s"", dv)\n    debug_print(""for %s"", copied_ys)\n    debug_print(""with respect to %s"", boundary+xs)\n\n    inputs_to_do_before = [y.op for y in ys]\n    if grad_ys is not None:\n        inputs_to_do_before += grad_ys\n    wait_to_do_ops = list(copied_ops) + [g.op for g in dv if g is not None]\n    my_add_control_inputs(wait_to_do_ops, inputs_to_do_before)\n\n    # partial derivatives to the checkpointed nodes\n    # dictionary of ""node: backprop"" for nodes in the boundary\n    d_checkpoints = {r: dr for r,dr in zip(checkpoints_disconnected.keys(),\n                                        dv[:len(checkpoints_disconnected)])}\n    # partial derivatives to xs (usually the params of the neural net)\n    d_xs = dv[len(checkpoints_disconnected):]\n\n    # incorporate derivatives flowing through the checkpointed nodes\n    checkpoints_sorted_lists = tf_toposort(checkpoints, within_ops=fwd_ops)\n    for ts in checkpoints_sorted_lists[::-1]:\n        debug_print(""Processing list %s"", ts)\n        checkpoints_other = [r for r in checkpoints if r not in ts]\n        checkpoints_disconnected_other = [checkpoints_disconnected[r] for r in checkpoints_other]\n\n        # copy part of the graph below current checkpoint node, stopping at\n        # other checkpoints nodes\n        ops_to_copy = fast_backward_ops(within_ops=fwd_ops, seed_ops=[r.op for r in ts], stop_at_ts=checkpoints_other)\n        debug_print(""Found %s ops to copy within %s, seed %s, stop_at %s"",\n                    len(ops_to_copy), fwd_ops, [r.op for r in ts],\n                    checkpoints_other)\n        debug_print(""ops_to_copy = %s"", ops_to_copy)\n        if not ops_to_copy: # we\'re done!\n            break\n        copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(ops_to_copy), {})\n        for origin_op, op in info._transformed_ops.items():\n            op._set_device(origin_op.node_def.device)\n        copied_ops = info._transformed_ops.values()\n        debug_print(""Copied %s to %s"", ops_to_copy, copied_ops)\n        ge.reroute_ts(checkpoints_disconnected_other, checkpoints_other, can_modify=copied_ops)\n        debug_print(""Rewired %s in place of %s restricted to %s"",\n                    checkpoints_disconnected_other, checkpoints_other, copied_ops)\n\n        # gradient flowing through the checkpointed node\n        boundary = [info._transformed_ops[r.op]._outputs[0] for r in ts]\n        substitute_backprops = [d_checkpoints[r] for r in ts]\n        dv = tf_gradients(boundary,\n                          checkpoints_disconnected_other+xs,\n                          grad_ys=substitute_backprops, **kwargs)\n        debug_print(""Got gradients %s"", dv)\n        debug_print(""for %s"", boundary)\n        debug_print(""with respect to %s"", checkpoints_disconnected_other+xs)\n        debug_print(""with boundary backprop substitutions %s"", substitute_backprops)\n\n        inputs_to_do_before = [d_checkpoints[r].op for r in ts]\n        wait_to_do_ops = list(copied_ops) + [g.op for g in dv if g is not None]\n        my_add_control_inputs(wait_to_do_ops, inputs_to_do_before)\n\n        # partial derivatives to the checkpointed nodes\n        for r, dr in zip(checkpoints_other, dv[:len(checkpoints_other)]):\n            if dr is not None:\n                if d_checkpoints[r] is None:\n                    d_checkpoints[r] = dr\n                else:\n                    d_checkpoints[r] += dr\n        def _unsparsify(x):\n            if not isinstance(x, tf.IndexedSlices):\n                return x\n            assert x.dense_shape is not None, ""memory_saving_gradients encountered sparse gradients of unknown shape""\n            indices = x.indices\n            while indices.shape.ndims < x.values.shape.ndims:\n                indices = tf.expand_dims(indices, -1)\n            return tf.scatter_nd(indices, x.values, x.dense_shape)\n\n        # partial derivatives to xs (usually the params of the neural net)\n        d_xs_new = dv[len(checkpoints_other):]\n        for j in range(len(xs)):\n            if d_xs_new[j] is not None:\n                if d_xs[j] is None:\n                    d_xs[j] = _unsparsify(d_xs_new[j])\n                else:\n                    d_xs[j] += _unsparsify(d_xs_new[j])\n\n\n    return d_xs\n\ndef tf_toposort(ts, within_ops=None):\n    all_ops = ge.get_forward_walk_ops([x.op for x in ts], within_ops=within_ops)\n\n    deps = {}\n    for op in all_ops:\n        for o in op.outputs:\n            deps[o] = set(op.inputs)\n    sorted_ts = toposort(deps)\n\n    # only keep the tensors from our original list\n    ts_sorted_lists = []\n    for l in sorted_ts:\n        keep = list(set(l).intersection(ts))\n        if keep:\n            ts_sorted_lists.append(keep)\n\n    return ts_sorted_lists\n\ndef fast_backward_ops(within_ops, seed_ops, stop_at_ts):\n    bwd_ops = set(ge.get_backward_walk_ops(seed_ops, stop_at_ts=stop_at_ts))\n    ops = bwd_ops.intersection(within_ops).difference([t.op for t in stop_at_ts])\n    return list(ops)\n\n@contextlib.contextmanager\ndef capture_ops():\n  """"""Decorator to capture ops created in the block.\n  with capture_ops() as ops:\n    # create some ops\n  print(ops) # => prints ops created.\n  """"""\n\n  micros = int(time.time()*10**6)\n  scope_name = str(micros)\n  op_list = []\n  with tf.compat.v1.name_scope(scope_name):\n    yield op_list\n\n  g = tf.compat.v1.get_default_graph()\n  op_list.extend(ge.select_ops(scope_name+""/.*"", graph=g))\n\ndef _to_op(tensor_or_op):\n  if hasattr(tensor_or_op, ""op""):\n    return tensor_or_op.op\n  return tensor_or_op\n\ndef _to_ops(iterable):\n  if not _is_iterable(iterable):\n    return iterable\n  return [_to_op(i) for i in iterable]\n\ndef _is_iterable(o):\n  try:\n    _ = iter(o)\n  except Exception:\n    return False\n  return True\n\nDEBUG_LOGGING=False\ndef debug_print(s, *args):\n  """"""Like logger.log, but also replaces all TensorFlow ops/tensors with their\n  names. Sensitive to value of DEBUG_LOGGING, see enable_debug/disable_debug\n\n  Usage:\n    debug_print(""see tensors %s for %s"", tensorlist, [1,2,3])\n  """"""\n\n  if DEBUG_LOGGING:\n    formatted_args = [format_ops(arg) for arg in args]\n    print(""DEBUG ""+s % tuple(formatted_args))\n\ndef format_ops(ops, sort_outputs=True):\n  """"""Helper method for printing ops. Converts Tensor/Operation op to op.name,\n  rest to str(op).""""""\n\n  if hasattr(ops, \'__iter__\') and not isinstance(ops, str):\n    l = [(op.name if hasattr(op, ""name"") else str(op)) for op in ops]\n    if sort_outputs:\n      return sorted(l)\n    return l\n  else:\n    return ops.name if hasattr(ops, ""name"") else str(ops)\n\ndef my_add_control_inputs(wait_to_do_ops, inputs_to_do_before):\n    for op in wait_to_do_ops:\n        ci = [i for i in inputs_to_do_before if op.control_inputs is None or i not in op.control_inputs]\n        ge.add_control_inputs(op, ci)\n'"
gpt_2_simple/src/model.py,56,"b'import numpy as np\nimport tensorflow as tf\n#from tensorflow.contrib.training import HParams\n\nclass HParams():\n    def __init__(self, n_vocab, n_ctx, n_embd, n_head, n_layer):\n        self.n_vocab = n_vocab\n        self.n_ctx = n_ctx\n        self.n_embd = n_embd\n        self.n_head = n_head\n        self.n_layer = n_layer\n        \n    def override_from_dict(self, param_dict):\n        try:\n            self.n_vocab = param_dict[\'n_vocab\']\n        except: \n            pass\n        try:\n            self.n_ctx = param_dict[\'n_ctx\']\n        except:\n            pass\n        try:\n            self.n_embd = param_dict[\'n_embd\']\n        except:\n            pass\n        try:\n            self.n_head = param_dict[\'n_head\']\n        except:\n            pass\n        try:\n            self.n_layer = param_dict[\'n_layer\']\n        except:\n            pass\n\ndef default_hparams():\n    return HParams(\n        n_vocab=0,\n        n_ctx=1024,\n        n_embd=768,\n        n_head=12,\n        n_layer=12,\n    )\n\ndef shape_list(x):\n    """"""Deal with dynamic shape in tensorflow cleanly.""""""\n    static = x.shape.as_list()\n    dynamic = tf.shape(input=x)\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n\ndef softmax(x, axis=-1):\n    x = x - tf.reduce_max(input_tensor=x, axis=axis, keepdims=True)\n    ex = tf.exp(x)\n    return ex / tf.reduce_sum(input_tensor=ex, axis=axis, keepdims=True)\n\ndef gelu(x):\n    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n\ndef norm(x, scope, *, axis=-1, epsilon=1e-5):\n    """"""Normalize to mean = 0, std = 1, then do a diagonal affine transform.""""""\n    with tf.compat.v1.variable_scope(scope):\n        n_state = x.shape[-1].value\n        g = tf.compat.v1.get_variable(\'g\', [n_state], initializer=tf.compat.v1.constant_initializer(1))\n        b = tf.compat.v1.get_variable(\'b\', [n_state], initializer=tf.compat.v1.constant_initializer(0))\n        u = tf.reduce_mean(input_tensor=x, axis=axis, keepdims=True)\n        s = tf.reduce_mean(input_tensor=tf.square(x-u), axis=axis, keepdims=True)\n        x = (x - u) * tf.math.rsqrt(s + epsilon)\n        x = x*g + b\n        return x\n\ndef split_states(x, n):\n    """"""Reshape the last dimension of x into [n, x.shape[-1]/n].""""""\n    *start, m = shape_list(x)\n    return tf.reshape(x, start + [n, m//n])\n\ndef merge_states(x):\n    """"""Smash the last two dimensions of x into a single dimension.""""""\n    *start, a, b = shape_list(x)\n    return tf.reshape(x, start + [a*b])\n\ndef conv1d(x, scope, nf, *, w_init_stdev=0.02):\n    with tf.compat.v1.variable_scope(scope):\n        *start, nx = shape_list(x)\n        w = tf.compat.v1.get_variable(\'w\', [1, nx, nf], initializer=tf.compat.v1.random_normal_initializer(stddev=w_init_stdev))\n        b = tf.compat.v1.get_variable(\'b\', [nf], initializer=tf.compat.v1.constant_initializer(0))\n        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n        return c\n\ndef attention_mask(nd, ns, *, dtype):\n    """"""1\'s in the lower triangle, counting from the lower right corner.\n\n    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn\'t produce garbage on TPUs.\n    """"""\n    i = tf.range(nd)[:,None]\n    j = tf.range(ns)\n    m = i >= j - ns + nd\n    return tf.cast(m, dtype)\n\n\ndef attn(x, scope, n_state, *, past, hparams):\n    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n    assert n_state % hparams.n_head == 0\n    if past is not None:\n        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n\n    def split_heads(x):\n        # From [batch, sequence, features] to [batch, heads, sequence, features]\n        return tf.transpose(a=split_states(x, hparams.n_head), perm=[0, 2, 1, 3])\n\n    def merge_heads(x):\n        # Reverse of split_heads\n        return merge_states(tf.transpose(a=x, perm=[0, 2, 1, 3]))\n\n    def mask_attn_weights(w):\n        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n        _, _, nd, ns = shape_list(w)\n        b = attention_mask(nd, ns, dtype=w.dtype)\n        b = tf.reshape(b, [1, 1, nd, ns])\n        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n        return w\n\n    def multihead_attn(q, k, v):\n        # q, k, v have shape [batch, heads, sequence, features]\n        w = tf.matmul(q, k, transpose_b=True)\n        w = w * tf.math.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n\n        w = mask_attn_weights(w)\n        w = softmax(w)\n        a = tf.matmul(w, v)\n        return a\n\n    with tf.compat.v1.variable_scope(scope):\n        c = conv1d(x, \'c_attn\', n_state*3)\n        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n        present = tf.stack([k, v], axis=1)\n        if past is not None:\n            pk, pv = tf.unstack(past, axis=1)\n            k = tf.concat([pk, k], axis=-2)\n            v = tf.concat([pv, v], axis=-2)\n        a = multihead_attn(q, k, v)\n        a = merge_heads(a)\n        a = conv1d(a, \'c_proj\', n_state)\n        return a, present\n\n\ndef mlp(x, scope, n_state, *, hparams):\n    with tf.compat.v1.variable_scope(scope):\n        nx = x.shape[-1].value\n        h = gelu(conv1d(x, \'c_fc\', n_state))\n        h2 = conv1d(h, \'c_proj\', nx)\n        return h2\n\n\ndef block(x, scope, *, past, hparams):\n    with tf.compat.v1.variable_scope(scope):\n        nx = x.shape[-1].value\n        a, present = attn(norm(x, \'ln_1\'), \'attn\', nx, past=past, hparams=hparams)\n        x = x + a\n        m = mlp(norm(x, \'ln_2\'), \'mlp\', nx*4, hparams=hparams)\n        x = x + m\n        return x, present\n\ndef past_shape(*, hparams, batch_size=None, sequence=None):\n    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n\ndef expand_tile(value, size):\n    """"""Add a new axis of given size.""""""\n    value = tf.convert_to_tensor(value=value, name=\'value\')\n    ndims = value.shape.ndims\n    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n\ndef positions_for(tokens, past_length):\n    batch_size = tf.shape(input=tokens)[0]\n    nsteps = tf.shape(input=tokens)[1]\n    return expand_tile(past_length + tf.range(nsteps), batch_size)\n\n\ndef model(hparams, X, past=None, scope=\'model\', gpus=[], reuse=False):\n    with tf.compat.v1.variable_scope(scope, reuse=reuse):\n        results = {}\n        batch, sequence = shape_list(X)\n\n        wpe = tf.compat.v1.get_variable(\'wpe\', [hparams.n_ctx, hparams.n_embd],\n                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.01))\n        wte = tf.compat.v1.get_variable(\'wte\', [hparams.n_vocab, hparams.n_embd],\n                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.02))\n        past_length = 0 if past is None else tf.shape(input=past)[-2]\n        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n\n        # Transformer\n        presents = []\n        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n        assert len(pasts) == hparams.n_layer\n        gpu_stack = np.floor(hparams.n_layer/len(gpus)) if len(gpus) > 0 else 0\n        d = 0\n        for layer, past in enumerate(pasts):\n            if gpu_stack < 1:\n                h, present = block(h, \'h%d\' % layer, past=past, hparams=hparams)\n                tf.compat.v1.add_to_collection(\'checkpoints\', h)\n                presents.append(present)\n            else:\n                if layer != 0 and layer % gpu_stack == 0 and d+1 != len(gpus):\n                    d += 1\n                with tf.device(gpus[d]):\n                    h, present = block(h, \'h%d\' % layer, past=past, hparams=hparams)\n                    tf.compat.v1.add_to_collection(\'checkpoints\', h)\n                    presents.append(present)\n        results[\'present\'] = tf.stack(presents, axis=1)\n        h = norm(h, \'ln_f\')\n\n        # Language model loss.  Do tokens <n predict token n?\n        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n        logits = tf.matmul(h_flat, wte, transpose_b=True)\n        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n        results[\'logits\'] = logits\n        return results\n'"
gpt_2_simple/src/sample.py,28,"b""import tensorflow as tf\n\nfrom gpt_2_simple.src import model\n\n\ndef top_k_logits(logits, k):\n    if k == 0:\n        # no truncation\n        return logits\n\n    def _top_k():\n        values, _ = tf.nn.top_k(logits, k=k)\n        min_values = values[:, -1, tf.newaxis]\n        return tf.compat.v1.where(\n            logits < min_values,\n            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n            logits,\n        )\n    return tf.cond(\n        pred=tf.equal(k, 0),\n        true_fn=lambda: logits,\n        false_fn=lambda: _top_k(),\n    )\n\n\ndef top_p_logits(logits, p):\n    with tf.compat.v1.variable_scope('top_p_logits'):\n        logits_sort = tf.sort(logits, direction='DESCENDING')\n        probs_sort = tf.nn.softmax(logits_sort)\n        probs_sums = tf.cumsum(probs_sort, axis=1, exclusive=True)\n        logits_masked = tf.compat.v1.where(probs_sums < p, logits_sort, tf.ones_like(\n            logits_sort)*1000)  # [batchsize, vocab]\n        min_logits = tf.reduce_min(input_tensor=logits_masked, axis=1, keepdims=True)  # [batchsize, 1]\n        return tf.compat.v1.where(\n            logits < min_logits,\n            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n            logits,\n        )\n\n\ndef sample_sequence(*, hparams, length, start_token=None,\n                    batch_size=None, context=None, temperature=1,\n                    top_k=0, top_p=0.0):\n    if start_token is None:\n        assert context is not None, 'Specify exactly one of start_token and context!'\n    else:\n        assert context is None, 'Specify exactly one of start_token and context!'\n        context = tf.fill([batch_size, 1], start_token)\n\n    def step(hparams, tokens, past=None):\n        lm_output = model.model(hparams=hparams, X=tokens,\n                                past=past, reuse=tf.compat.v1.AUTO_REUSE)\n\n        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n        presents = lm_output['present']\n        presents.set_shape(model.past_shape(\n            hparams=hparams, batch_size=batch_size))\n        return {\n            'logits': logits,\n            'presents': presents,\n        }\n\n    with tf.compat.v1.name_scope('sample_sequence'):\n        # Don't feed the last context token -- leave that to the loop below\n        # TODO: Would be slightly faster if we called step on the entire context,\n        # rather than leaving the last token transformer calculation to the while loop.\n        context_output = step(hparams, context[:, :-1])\n\n        def body(past, prev, output):\n            next_outputs = step(hparams, prev[:, tf.newaxis], past=past)\n            logits = next_outputs['logits'][:, -1, :] / tf.cast(temperature, tf.float32)\n            if top_p > 0.0:\n                logits = top_p_logits(logits, p=top_p)\n            else:\n                logits = top_k_logits(logits, k=top_k)\n            samples = tf.random.categorical(\n                logits, num_samples=1, dtype=tf.int32)\n            return [\n                tf.concat([past, next_outputs['presents']], axis=-2),\n                tf.squeeze(samples, axis=[1]),\n                tf.concat([output, samples], axis=1),\n            ]\n\n        def cond(*args):\n            return True\n\n        _, _, tokens = tf.while_loop(\n            cond=cond, body=body,\n            maximum_iterations=length,\n            loop_vars=[\n                context_output['presents'],\n                context[:, -1],\n                context,\n            ],\n            shape_invariants=[\n                tf.TensorShape(model.past_shape(\n                    hparams=hparams, batch_size=batch_size)),\n                tf.TensorShape([batch_size]),\n                tf.TensorShape([batch_size, None]),\n            ],\n            back_prop=False,\n        )\n\n        return tokens\n"""
