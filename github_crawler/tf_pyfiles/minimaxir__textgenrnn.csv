file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\n\nlong_description = '''\nEasily train your own text-generating neural network of\nany size and complexity on any text dataset with a few lines\nof code, or quickly train on a text using a pretrained model.\n\n- A modern neural network architecture which utilizes new techniques as\nattention-weighting and skip-embedding to accelerate training\nand improve model quality.\n- Able to train on and generate text at either the\ncharacter-level or word-level.\n- Able to configure RNN size, the number of RNN layers,\nand whether to use bidirectional RNNs.\n- Able to train on any generic input text file, including large files.\n- Able to train models on a GPU and then use them with a CPU.\n- Able to utilize a powerful CuDNN implementation of RNNs\nwhen trained on the GPU, which massively speeds up training time as\nopposed to normal LSTM implementations.\n- Able to train the model using contextual labels,\nallowing it to learn faster and produce better results in some cases.\n- Able to generate text interactively for customized stories.\n'''\n\n\nsetup(\n    name='textgenrnn',\n    packages=['textgenrnn'],  # this must be the same as the name above\n    version='2.0.0',\n    description='Easily train your own text-generating neural network ' \\\n    'of any size and complexity',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    author='Max Woolf',\n    author_email='max@minimaxir.com',\n    url='https://github.com/minimaxir/textgenrnn',\n    keywords=['deep learning', 'tensorflow', 'keras', 'text generation'],\n    classifiers=[],\n    license='MIT',\n    python_requires='>=3.5',\n    include_package_data=True,\n    install_requires=['h5py', 'scikit-learn', 'tqdm', 'tensorflow>=2.1.0']\n)\n"""
textgenrnn/AttentionWeightedAverage.py,0,"b'from tensorflow.keras.layers import Layer, InputSpec\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import initializers\n\n\nclass AttentionWeightedAverage(Layer):\n    """"""\n    Computes a weighted average of the different channels across timesteps.\n    Uses 1 parameter pr. channel to compute the attention value for\n    a single timestep.\n    """"""\n\n    def __init__(self, return_attention=False, **kwargs):\n        self.init = initializers.get(\'uniform\')\n        self.supports_masking = True\n        self.return_attention = return_attention\n        super(AttentionWeightedAverage, self).__init__(** kwargs)\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(ndim=3)]\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight(shape=(input_shape[2], 1),\n                                 name=\'{}_W\'.format(self.name),\n                                 trainable=True,\n                                 initializer=self.init)\n        super(AttentionWeightedAverage, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        # computes a probability distribution over the timesteps\n        # uses \'max trick\' for numerical stability\n        # reshape is done to avoid issue with Tensorflow\n        # and 1-dimensional weights\n        logits = K.dot(x, self.W)\n        x_shape = K.shape(x)\n        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n\n        # masked timesteps have zero weight\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            ai = ai * mask\n        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n        weighted_input = x * K.expand_dims(att_weights)\n        result = K.sum(weighted_input, axis=1)\n        if self.return_attention:\n            return [result, att_weights]\n        return result\n\n    def get_output_shape_for(self, input_shape):\n        return self.compute_output_shape(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        output_len = input_shape[2]\n        if self.return_attention:\n            return [(input_shape[0], output_len), (input_shape[0],\n                                                   input_shape[1])]\n        return (input_shape[0], output_len)\n\n    def compute_mask(self, input, input_mask=None):\n        if isinstance(input_mask, list):\n            return [None] * len(input_mask)\n        else:\n            return None\n'"
textgenrnn/__init__.py,0,b'from .textgenrnn import textgenrnn\n'
textgenrnn/model.py,0,"b""from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Bidirectional\nfrom tensorflow.keras.layers import concatenate, Reshape, SpatialDropout1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow import config as config\nfrom .AttentionWeightedAverage import AttentionWeightedAverage\n\n\ndef textgenrnn_model(num_classes, cfg, context_size=None,\n                     weights_path=None,\n                     dropout=0.0,\n                     optimizer=Adam(lr=4e-3)):\n    '''\n    Builds the model architecture for textgenrnn and\n    loads the specified weights for the model.\n    '''\n\n    input = Input(shape=(cfg['max_length'],), name='input')\n    embedded = Embedding(num_classes, cfg['dim_embeddings'],\n                         input_length=cfg['max_length'],\n                         name='embedding')(input)\n\n    if dropout > 0.0:\n        embedded = SpatialDropout1D(dropout, name='dropout')(embedded)\n\n    rnn_layer_list = []\n    for i in range(cfg['rnn_layers']):\n        prev_layer = embedded if i == 0 else rnn_layer_list[-1]\n        rnn_layer_list.append(new_rnn(cfg, i+1)(prev_layer))\n\n    seq_concat = concatenate([embedded] + rnn_layer_list, name='rnn_concat')\n    attention = AttentionWeightedAverage(name='attention')(seq_concat)\n    output = Dense(num_classes, name='output', activation='softmax')(attention)\n\n    if context_size is None:\n        model = Model(inputs=[input], outputs=[output])\n        if weights_path is not None:\n            model.load_weights(weights_path, by_name=True)\n        model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    else:\n        context_input = Input(\n            shape=(context_size,), name='context_input')\n        context_reshape = Reshape((context_size,),\n                                  name='context_reshape')(context_input)\n        merged = concatenate([attention, context_reshape], name='concat')\n        main_output = Dense(num_classes, name='context_output',\n                            activation='softmax')(merged)\n\n        model = Model(inputs=[input, context_input],\n                      outputs=[main_output, output])\n        if weights_path is not None:\n            model.load_weights(weights_path, by_name=True)\n        model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n                      loss_weights=[0.8, 0.2])\n\n    return model\n\n\n'''\nCreate a new LSTM layer per parameters. Unfortunately,\neach combination of parameters must be hardcoded.\n\nThe normal LSTMs use sigmoid recurrent activations\nfor parity with CuDNNLSTM:\nhttps://github.com/keras-team/keras/issues/8860\n'''\n\n'''\nFIXME\nFrom TensorFlow 2 you do not need to specify CuDNNLSTM.\nYou can just use LSTM with no activation function and it will\nautomatically use the CuDNN version.\nThis part can probably be cleaned up.\n'''\n\ndef new_rnn(cfg, layer_num):\n    use_cudnnlstm = K.backend() == 'tensorflow' and len(config.get_visible_devices('GPU')) > 0\n    if use_cudnnlstm:\n        if cfg['rnn_bidirectional']:\n            return Bidirectional(LSTM(cfg['rnn_size'],\n                                           return_sequences=True),\n                                 name='rnn_{}'.format(layer_num))\n\n        return LSTM(cfg['rnn_size'],\n                         return_sequences=True,\n                         name='rnn_{}'.format(layer_num))\n    else:\n        if cfg['rnn_bidirectional']:\n            return Bidirectional(LSTM(cfg['rnn_size'],\n                                      return_sequences=True,\n                                      recurrent_activation='sigmoid'),\n                                 name='rnn_{}'.format(layer_num))\n\n        return LSTM(cfg['rnn_size'],\n                    return_sequences=True,\n                    recurrent_activation='sigmoid',\n                    name='rnn_{}'.format(layer_num))\n"""
textgenrnn/model_training.py,0,"b""import numpy as np\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nfrom .utils import textgenrnn_encode_cat\n\n\ndef generate_sequences_from_texts(texts, indices_list,\n                                  textgenrnn, context_labels,\n                                  batch_size=128):\n    is_words = textgenrnn.config['word_level']\n    is_single = textgenrnn.config['single_text']\n    max_length = textgenrnn.config['max_length']\n    meta_token = textgenrnn.META_TOKEN\n\n    if is_words:\n        new_tokenizer = Tokenizer(filters='', char_level=True)\n        new_tokenizer.word_index = textgenrnn.vocab\n    else:\n        new_tokenizer = textgenrnn.tokenizer\n\n    while True:\n        np.random.shuffle(indices_list)\n\n        X_batch = []\n        Y_batch = []\n        context_batch = []\n        count_batch = 0\n\n        for row in range(indices_list.shape[0]):\n            text_index = indices_list[row, 0]\n            end_index = indices_list[row, 1]\n\n            text = texts[text_index]\n\n            if not is_single:\n                text = [meta_token] + list(text) + [meta_token]\n\n            if end_index > max_length:\n                x = text[end_index - max_length: end_index + 1]\n            else:\n                x = text[0: end_index + 1]\n            y = text[end_index + 1]\n\n            if y in textgenrnn.vocab:\n                x = process_sequence([x], textgenrnn, new_tokenizer)\n                y = textgenrnn_encode_cat([y], textgenrnn.vocab)\n\n                X_batch.append(x)\n                Y_batch.append(y)\n\n                if context_labels is not None:\n                    context_batch.append(context_labels[text_index])\n\n                count_batch += 1\n\n                if count_batch % batch_size == 0:\n                    X_batch = np.squeeze(np.array(X_batch))\n                    Y_batch = np.squeeze(np.array(Y_batch))\n                    context_batch = np.squeeze(np.array(context_batch))\n\n                    # print(X_batch.shape)\n\n                    if context_labels is not None:\n                        yield ([X_batch, context_batch], [Y_batch, Y_batch])\n                    else:\n                        yield (X_batch, Y_batch)\n                    X_batch = []\n                    Y_batch = []\n                    context_batch = []\n                    count_batch = 0\n\n\ndef process_sequence(X, textgenrnn, new_tokenizer):\n    X = new_tokenizer.texts_to_sequences(X)\n    X = sequence.pad_sequences(\n        X, maxlen=textgenrnn.config['max_length'])\n\n    return X\n"""
textgenrnn/textgenrnn.py,2,"b'import json\nimport re\n\nimport numpy as np\nimport tensorflow as tf\nimport tqdm\nfrom pkg_resources import resource_filename\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import LabelBinarizer\nfrom tensorflow import config as config\nfrom tensorflow.compat.v1.keras.backend import set_session\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n\nfrom .model import textgenrnn_model\nfrom .model_training import generate_sequences_from_texts\nfrom .utils import (\n    generate_after_epoch,\n    save_model_weights,\n    textgenrnn_encode_sequence,\n    textgenrnn_generate,\n    textgenrnn_texts_from_file,\n    textgenrnn_texts_from_file_context,\n)\n\n\nclass textgenrnn:\n    META_TOKEN = \'<s>\'\n    config = {\n        \'rnn_layers\': 2,\n        \'rnn_size\': 128,\n        \'rnn_bidirectional\': False,\n        \'max_length\': 40,\n        \'max_words\': 10000,\n        \'dim_embeddings\': 100,\n        \'word_level\': False,\n        \'single_text\': False\n    }\n    default_config = config.copy()\n\n    def __init__(self, weights_path=None,\n                 vocab_path=None,\n                 config_path=None,\n                 name=""textgenrnn"",\n                 allow_growth=None):\n\n        if weights_path is None:\n            weights_path = resource_filename(__name__,\n                                             \'textgenrnn_weights.hdf5\')\n\n        if vocab_path is None:\n            vocab_path = resource_filename(__name__,\n                                           \'textgenrnn_vocab.json\')\n\n        if allow_growth is not None:\n            c = tf.compat.v1.ConfigProto()\n            c.gpu_options.allow_growth = True\n            set_session(tf.compat.v1.Session(config=c))\n\n        if config_path is not None:\n            with open(config_path, \'r\',\n                      encoding=\'utf8\', errors=\'ignore\') as json_file:\n                self.config = json.load(json_file)\n\n        self.config.update({\'name\': name})\n        self.default_config.update({\'name\': name})\n\n        with open(vocab_path, \'r\',\n                  encoding=\'utf8\', errors=\'ignore\') as json_file:\n            self.vocab = json.load(json_file)\n\n        self.tokenizer = Tokenizer(filters=\'\', lower=False, char_level=True)\n        self.tokenizer.word_index = self.vocab\n        self.num_classes = len(self.vocab) + 1\n        self.model = textgenrnn_model(self.num_classes,\n                                      cfg=self.config,\n                                      weights_path=weights_path)\n        self.indices_char = dict((self.vocab[c], c) for c in self.vocab)\n\n    def generate(self, n=1, return_as_list=False, prefix=None,\n                 temperature=[1.0, 0.5, 0.2, 0.2],\n                 max_gen_length=300, interactive=False,\n                 top_n=3, progress=True):\n        gen_texts = []\n        iterable = tqdm.trange(n) if progress and n > 1 else range(n)\n        for _ in iterable:\n            gen_text, _ = textgenrnn_generate(self.model,\n                                              self.vocab,\n                                              self.indices_char,\n                                              temperature,\n                                              self.config[\'max_length\'],\n                                              self.META_TOKEN,\n                                              self.config[\'word_level\'],\n                                              self.config.get(\n                                                  \'single_text\', False),\n                                              max_gen_length,\n                                              interactive,\n                                              top_n,\n                                              prefix)\n            if not return_as_list:\n                print(""{}\\n"".format(gen_text))\n            gen_texts.append(gen_text)\n        if return_as_list:\n            return gen_texts\n\n    def generate_samples(self, n=3, temperatures=[0.2, 0.5, 1.0], **kwargs):\n        for temperature in temperatures:\n            print(\'#\'*20 + \'\\nTemperature: {}\\n\'.format(temperature) +\n                  \'#\'*20)\n            self.generate(n, temperature=temperature, progress=False, **kwargs)\n\n    def train_on_texts(self, texts, context_labels=None,\n                       batch_size=128,\n                       num_epochs=50,\n                       verbose=1,\n                       new_model=False,\n                       gen_epochs=1,\n                       train_size=1.0,\n                       max_gen_length=300,\n                       validation=True,\n                       dropout=0.0,\n                       via_new_model=False,\n                       save_epochs=0,\n                       multi_gpu=False,\n                       **kwargs):\n\n        if new_model and not via_new_model:\n            self.train_new_model(texts,\n                                 context_labels=context_labels,\n                                 num_epochs=num_epochs,\n                                 gen_epochs=gen_epochs,\n                                 train_size=train_size,\n                                 batch_size=batch_size,\n                                 dropout=dropout,\n                                 validation=validation,\n                                 save_epochs=save_epochs,\n                                 multi_gpu=multi_gpu,\n                                 **kwargs)\n            return\n\n        if context_labels:\n            context_labels = LabelBinarizer().fit_transform(context_labels)\n\n        if self.config[\'word_level\']:\n            # If training word level, must add spaces around each\n            # punctuation. https://stackoverflow.com/a/3645946/9314418\n            punct = \'!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\\\n\\\\t\\\'\xe2\x80\x98\xe2\x80\x99\xe2\x80\x9c\xe2\x80\x9d\xe2\x80\x99\xe2\x80\x93\xe2\x80\x94\xe2\x80\xa6\'\n            for i in range(len(texts)):\n                texts[i] = re.sub(\'([{}])\'.format(punct), r\' \\1 \', texts[i])\n                texts[i] = re.sub(\' {2,}\', \' \', texts[i])\n            texts = [text_to_word_sequence(text, filters=\'\') for text in texts]\n\n        # calculate all combinations of text indices + token indices\n        indices_list = [np.meshgrid(np.array(i), np.arange(\n            len(text) + 1)) for i, text in enumerate(texts)]\n        # indices_list = np.block(indices_list) # this hangs when indices_list is large enough\n        # FIX BEGIN ------\n        indices_list_o = np.block(indices_list[0])\n        for i in range(len(indices_list)-1):\n            tmp = np.block(indices_list[i+1])\n            indices_list_o = np.concatenate([indices_list_o, tmp])\n        indices_list = indices_list_o\n        # FIX END ------\n\n        # If a single text, there will be 2 extra indices, so remove them\n        # Also remove first sequences which use padding\n        if self.config[\'single_text\']:\n            indices_list = indices_list[self.config[\'max_length\']:-2, :]\n\n        indices_mask = np.random.rand(indices_list.shape[0]) < train_size\n\n        if multi_gpu:\n            num_gpus = len(config.get_visible_devices(\'GPU\'))\n            batch_size = batch_size * num_gpus\n\n        gen_val = None\n        val_steps = None\n        if train_size < 1.0 and validation:\n            indices_list_val = indices_list[~indices_mask, :]\n            gen_val = generate_sequences_from_texts(\n                texts, indices_list_val, self, context_labels, batch_size)\n            val_steps = max(\n                int(np.floor(indices_list_val.shape[0] / batch_size)), 1)\n\n        indices_list = indices_list[indices_mask, :]\n\n        num_tokens = indices_list.shape[0]\n        assert num_tokens >= batch_size, ""Fewer tokens than batch_size.""\n\n        level = \'word\' if self.config[\'word_level\'] else \'character\'\n        print(""Training on {:,} {} sequences."".format(num_tokens, level))\n\n        steps_per_epoch = max(int(np.floor(num_tokens / batch_size)), 1)\n\n        gen = generate_sequences_from_texts(\n            texts, indices_list, self, context_labels, batch_size)\n\n        base_lr = 4e-3\n\n        # scheduler function must be defined inline.\n        def lr_linear_decay(epoch):\n            return (base_lr * (1 - (epoch / num_epochs)))\n\n        \'\'\'\n        FIXME\n        This part is a bit messy as we need to initialize the model within\n        strategy.scope() when using multi-GPU. Can probably be cleaned up a bit.\n        \'\'\'\n\n        if context_labels is not None:\n            if new_model:\n                weights_path = None\n            else:\n                weights_path = ""{}_weights.hdf5"".format(self.config[\'name\'])\n                self.save(weights_path)\n\n\n            if multi_gpu:\n                from tensorflow import distribute as distribute\n                strategy = distribute.MirroredStrategy()\n                with strategy.scope():\n                    parallel_model = textgenrnn_model(self.num_classes,\n                                                      dropout=dropout,\n                                                      cfg=self.config,\n                                                      context_size=context_labels.shape[1],\n                                                      weights_path=weights_path)\n                    parallel_model.compile(loss=\'categorical_crossentropy\',\n                                           optimizer=Adam(lr=4e-3))\n                model_t = parallel_model\n                print(""Training on {} GPUs."".format(num_gpus))\n            else:\n                model_t = self.model\n        else:\n            if multi_gpu:\n                from tensorflow import distribute as distribute\n                if new_model:\n                    weights_path = None\n                else:\n                    weights_path = ""{}_weights.hdf5"".format(self.config[\'name\'])\n\n                strategy = distribute.MirroredStrategy()\n                with strategy.scope():\n                # Do not locate model/merge on CPU since sample sizes are small.\n                    parallel_model = textgenrnn_model(self.num_classes,\n                                                      cfg=self.config,\n                                                      weights_path=weights_path)\n                    parallel_model.compile(loss=\'categorical_crossentropy\',\n                                           optimizer=Adam(lr=4e-3))\n\n                model_t = parallel_model\n                print(""Training on {} GPUs."".format(num_gpus))\n            else:\n                model_t = self.model\n\n        model_t.fit(gen, steps_per_epoch=steps_per_epoch,\n                              epochs=num_epochs,\n                              callbacks=[\n                                  LearningRateScheduler(\n                                      lr_linear_decay),\n                                  generate_after_epoch(\n                                      self, gen_epochs,\n                                      max_gen_length),\n                                  save_model_weights(\n                                      self, num_epochs,\n                                      save_epochs)],\n                              verbose=verbose,\n                              max_queue_size=10,\n                              validation_data=gen_val,\n                              validation_steps=val_steps\n                              )\n\n        # Keep the text-only version of the model if using context labels\n        if context_labels is not None:\n            self.model = Model(inputs=self.model.input[0],\n                               outputs=self.model.output[1])\n\n    def train_new_model(self, texts, context_labels=None, num_epochs=50,\n                        gen_epochs=1, batch_size=128, dropout=0.0,\n                        train_size=1.0,\n                        validation=True, save_epochs=0,\n                        multi_gpu=False, **kwargs):\n        self.config = self.default_config.copy()\n        self.config.update(**kwargs)\n\n        print(""Training new model w/ {}-layer, {}-cell {}LSTMs"".format(\n            self.config[\'rnn_layers\'], self.config[\'rnn_size\'],\n            \'Bidirectional \' if self.config[\'rnn_bidirectional\'] else \'\'\n        ))\n\n        # Create text vocabulary for new texts\n        # if word-level, lowercase; if char-level, uppercase\n        self.tokenizer = Tokenizer(filters=\'\',\n                                   lower=self.config[\'word_level\'],\n                                   char_level=(not self.config[\'word_level\']))\n        self.tokenizer.fit_on_texts(texts)\n\n        # Limit vocab to max_words\n        max_words = self.config[\'max_words\']\n        self.tokenizer.word_index = {k: v for (\n            k, v) in self.tokenizer.word_index.items() if v <= max_words}\n\n        if not self.config.get(\'single_text\', False):\n            self.tokenizer.word_index[self.META_TOKEN] = len(\n                self.tokenizer.word_index) + 1\n        self.vocab = self.tokenizer.word_index\n        self.num_classes = len(self.vocab) + 1\n        self.indices_char = dict((self.vocab[c], c) for c in self.vocab)\n\n        # Create a new, blank model w/ given params\n        self.model = textgenrnn_model(self.num_classes,\n                                      dropout=dropout,\n                                      cfg=self.config)\n\n        # Save the files needed to recreate the model\n        with open(\'{}_vocab.json\'.format(self.config[\'name\']),\n                  \'w\', encoding=\'utf8\') as outfile:\n            json.dump(self.tokenizer.word_index, outfile, ensure_ascii=False)\n\n        with open(\'{}_config.json\'.format(self.config[\'name\']),\n                  \'w\', encoding=\'utf8\') as outfile:\n            json.dump(self.config, outfile, ensure_ascii=False)\n\n        self.train_on_texts(texts, new_model=True,\n                            via_new_model=True,\n                            context_labels=context_labels,\n                            num_epochs=num_epochs,\n                            gen_epochs=gen_epochs,\n                            train_size=train_size,\n                            batch_size=batch_size,\n                            dropout=dropout,\n                            validation=validation,\n                            save_epochs=save_epochs,\n                            multi_gpu=multi_gpu,\n                            **kwargs)\n\n    def save(self, weights_path=""textgenrnn_weights_saved.hdf5""):\n        self.model.save_weights(weights_path)\n\n    def load(self, weights_path):\n        self.model = textgenrnn_model(self.num_classes,\n                                      cfg=self.config,\n                                      weights_path=weights_path)\n\n    def reset(self):\n        self.config = self.default_config.copy()\n        self.__init__(name=self.config[\'name\'])\n\n    def train_from_file(self, file_path, header=True, delim=""\\n"",\n                        new_model=False, context=None,\n                        is_csv=False, **kwargs):\n\n        context_labels = None\n        if context:\n            texts, context_labels = textgenrnn_texts_from_file_context(\n                file_path)\n        else:\n            texts = textgenrnn_texts_from_file(file_path, header,\n                                               delim, is_csv)\n\n        print(""{:,} texts collected."".format(len(texts)))\n        if new_model:\n            self.train_new_model(\n                texts, context_labels=context_labels, **kwargs)\n        else:\n            self.train_on_texts(texts, context_labels=context_labels, **kwargs)\n\n    def train_from_largetext_file(self, file_path, new_model=True, **kwargs):\n        with open(file_path, \'r\', encoding=\'utf8\', errors=\'ignore\') as f:\n            texts = [f.read()]\n\n        if new_model:\n            self.train_new_model(\n                texts, single_text=True, **kwargs)\n        else:\n            self.train_on_texts(texts, single_text=True, **kwargs)\n\n    def generate_to_file(self, destination_path, **kwargs):\n        texts = self.generate(return_as_list=True, **kwargs)\n        with open(destination_path, \'w\', encoding=""utf-8"") as f:\n            for text in texts:\n                f.write(""{}\\n"".format(text))\n\n    def encode_text_vectors(self, texts, pca_dims=50, tsne_dims=None,\n                            tsne_seed=None, return_pca=False,\n                            return_tsne=False):\n\n        # if a single text, force it into a list:\n        if isinstance(texts, str):\n            texts = [texts]\n\n        vector_output = Model(inputs=self.model.input,\n                              outputs=self.model.get_layer(\'attention\').output)\n        encoded_vectors = []\n        maxlen = self.config[\'max_length\']\n        for text in texts:\n            if self.config[\'word_level\']:\n                text = text_to_word_sequence(text, filters=\'\')\n            text_aug = [self.META_TOKEN] + list(text[0:maxlen])\n            encoded_text = textgenrnn_encode_sequence(text_aug, self.vocab,\n                                                      maxlen)\n            encoded_vector = vector_output.predict(encoded_text)\n            encoded_vectors.append(encoded_vector)\n\n        encoded_vectors = np.squeeze(np.array(encoded_vectors), axis=1)\n        if pca_dims is not None:\n            assert len(texts) > 1, ""Must use more than 1 text for PCA""\n            pca = PCA(pca_dims)\n            encoded_vectors = pca.fit_transform(encoded_vectors)\n\n        if tsne_dims is not None:\n            tsne = TSNE(tsne_dims, random_state=tsne_seed)\n            encoded_vectors = tsne.fit_transform(encoded_vectors)\n\n        return_objects = encoded_vectors\n        if return_pca or return_tsne:\n            return_objects = [return_objects]\n        if return_pca:\n            return_objects.append(pca)\n        if return_tsne:\n            return_objects.append(tsne)\n\n        return return_objects\n\n    def similarity(self, text, texts, use_pca=True):\n        text_encoded = self.encode_text_vectors(text, pca_dims=None)\n        if use_pca:\n            texts_encoded, pca = self.encode_text_vectors(texts,\n                                                          return_pca=True)\n            text_encoded = pca.transform(text_encoded)\n        else:\n            texts_encoded = self.encode_text_vectors(texts, pca_dims=None)\n\n        cos_similairity = cosine_similarity(text_encoded, texts_encoded)[0]\n        text_sim_pairs = list(zip(texts, cos_similairity))\n        text_sim_pairs = sorted(text_sim_pairs, key=lambda x: -x[1])\n        return text_sim_pairs\n'"
textgenrnn/utils.py,0,"b'import csv\nimport re\nfrom random import shuffle\n\nimport numpy as np\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing import sequence\nfrom tqdm import trange\n\n\ndef textgenrnn_sample(preds, temperature, interactive=False, top_n=3):\n    \'\'\'\n    Samples predicted probabilities of the next character to allow\n    for the network to show ""creativity.""\n    \'\'\'\n\n    preds = np.asarray(preds).astype(\'float64\')\n\n    if temperature is None or temperature == 0.0:\n        return np.argmax(preds)\n\n    preds = np.log(preds + K.epsilon()) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n\n    if not interactive:\n        index = np.argmax(probas)\n\n        # prevent function from being able to choose 0 (placeholder)\n        # choose 2nd best index from preds\n        if index == 0:\n            index = np.argsort(preds)[-2]\n    else:\n        # return list of top N chars/words\n        # descending order, based on probability\n        index = (-preds).argsort()[:top_n]\n\n    return index\n\n\ndef textgenrnn_generate(model, vocab,\n                        indices_char, temperature=0.5,\n                        maxlen=40, meta_token=\'<s>\',\n                        word_level=False,\n                        single_text=False,\n                        max_gen_length=300,\n                        interactive=False,\n                        top_n=3,\n                        prefix=None,\n                        synthesize=False,\n                        stop_tokens=[\' \', \'\\n\']):\n    \'\'\'\n    Generates and returns a single text.\n    \'\'\'\n\n    collapse_char = \' \' if word_level else \'\'\n    end = False\n\n    # If generating word level, must add spaces around each punctuation.\n    # https://stackoverflow.com/a/3645946/9314418\n    if word_level and prefix:\n        punct = \'!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\\\n\\\\t\\\'\xe2\x80\x98\xe2\x80\x99\xe2\x80\x9c\xe2\x80\x9d\xe2\x80\x99\xe2\x80\x93\xe2\x80\x94\'\n        prefix = re.sub(\'([{}])\'.format(punct), r\' \\1 \', prefix)\n        prefix_t = [x.lower() for x in prefix.split()]\n\n    if not word_level and prefix:\n        prefix_t = list(prefix)\n\n    if single_text:\n        text = prefix_t if prefix else [\'\']\n        max_gen_length += maxlen\n    else:\n        text = [meta_token] + prefix_t if prefix else [meta_token]\n\n    if not isinstance(temperature, list):\n        temperature = [temperature]\n\n    if len(model.inputs) > 1:\n        model = Model(inputs=model.inputs[0], outputs=model.outputs[1])\n\n    while not end and len(text) < max_gen_length:\n        encoded_text = textgenrnn_encode_sequence(text[-maxlen:],\n                                                  vocab, maxlen)\n        next_temperature = temperature[(len(text) - 1) % len(temperature)]\n\n        if not interactive:\n            # auto-generate text without user intervention\n            next_index = textgenrnn_sample(\n                model.predict(encoded_text, batch_size=1)[0],\n                next_temperature)\n            next_char = indices_char[next_index]\n            text += [next_char]\n            if next_char == meta_token or len(text) >= max_gen_length:\n                end = True\n            gen_break = (next_char in stop_tokens or word_level or\n                         len(stop_tokens) == 0)\n            if synthesize and gen_break:\n                break\n        else:\n            # ask user what the next char/word should be\n            options_index = textgenrnn_sample(\n                model.predict(encoded_text, batch_size=1)[0],\n                next_temperature,\n                interactive=interactive,\n                top_n=top_n\n            )\n            options = [indices_char[idx] for idx in options_index]\n            print(\'Controls:\\n\\ts: stop.\\tx: backspace.\\to: write your own.\')\n            print(\'\\nOptions:\')\n\n            for i, option in enumerate(options, 1):\n                print(\'\\t{}: {}\'.format(i, option))\n\n            print(\'\\nProgress: {}\'.format(collapse_char.join(text)[3:]))\n            print(\'\\nYour choice?\')\n            user_input = input(\'> \')\n\n            try:\n                user_input = int(user_input)\n                next_char = options[user_input-1]\n                text += [next_char]\n            except ValueError:\n                if user_input == \'s\':\n                    next_char = \'<s>\'\n                    text += [next_char]\n                elif user_input == \'o\':\n                    other = input(\'> \')\n                    text += [other]\n                elif user_input == \'x\':\n                    try:\n                        del text[-1]\n                    except IndexError:\n                        pass\n                else:\n                    print(\'That\\\'s not an option!\')\n\n    # if single text, ignore sequences generated w/ padding\n    # if not single text, remove the <s> meta_tokens\n    if single_text:\n        text = text[maxlen:]\n    else:\n        text = text[1:]\n        if meta_token in text:\n            text.remove(meta_token)\n\n    text_joined = collapse_char.join(text)\n\n    # If word level, remove spaces around punctuation for cleanliness.\n    if word_level:\n        left_punct = ""!%),.:;?@\\]_}\\\\n\\\\t\'""\n        right_punct = ""$(\\[_\\\\n\\\\t\'""\n        punct = \'\\\\n\\\\t\'\n\n        text_joined = re.sub("" ([{}]) "".format(\n            punct), r\'\\1\', text_joined)\n        text_joined = re.sub("" ([{}])"".format(\n            left_punct), r\'\\1\', text_joined)\n        text_joined = re.sub(""([{}]) "".format(\n            right_punct), r\'\\1\', text_joined)\n        text_joined = re.sub(\'"" (.+?) ""\', \n            r\'""\\1""\', text_joined)\n\n    return text_joined, end\n\n\ndef textgenrnn_encode_sequence(text, vocab, maxlen):\n    \'\'\'\n    Encodes a text into the corresponding encoding for prediction with\n    the model.\n    \'\'\'\n\n    encoded = np.array([vocab.get(x, 0) for x in text])\n    return sequence.pad_sequences([encoded], maxlen=maxlen)\n\n\ndef textgenrnn_texts_from_file(file_path, header=True,\n                               delim=\'\\n\', is_csv=False):\n    \'\'\'\n    Retrieves texts from a newline-delimited file and returns as a list.\n    \'\'\'\n\n    with open(file_path, \'r\', encoding=\'utf8\', errors=\'ignore\') as f:\n        if header:\n            f.readline()\n        if is_csv:\n            texts = []\n            reader = csv.reader(f)\n            for row in reader:\n                if row:\n                    texts.append(row[0])\n        else:\n            texts = [line.rstrip(delim) for line in f]\n\n    return texts\n\n\ndef textgenrnn_texts_from_file_context(file_path, header=True):\n    \'\'\'\n    Retrieves texts+context from a two-column CSV.\n    \'\'\'\n\n    with open(file_path, \'r\', encoding=\'utf8\', errors=\'ignore\') as f:\n        if header:\n            f.readline()\n        texts = []\n        context_labels = []\n        reader = csv.reader(f)\n        for row in reader:\n            if row:\n                texts.append(row[0])\n                context_labels.append(row[1])\n\n    return (texts, context_labels)\n\n\ndef textgenrnn_encode_cat(chars, vocab):\n    \'\'\'\n    One-hot encodes values at given chars efficiently by preallocating\n    a zeros matrix.\n    \'\'\'\n\n    a = np.float32(np.zeros((len(chars), len(vocab) + 1)))\n    rows, cols = zip(*[(i, vocab.get(char, 0))\n                       for i, char in enumerate(chars)])\n    a[rows, cols] = 1\n    return a\n\n\ndef synthesize(textgens, n=1, return_as_list=False, prefix=\'\',\n               temperature=[0.5, 0.2, 0.2], max_gen_length=300,\n               progress=True, stop_tokens=[\' \', \'\\n\']):\n    """"""Synthesizes texts using an ensemble of input models.\n    """"""\n\n    gen_texts = []\n    iterable = trange(n) if progress and n > 1 else range(n)\n    for _ in iterable:\n        shuffle(textgens)\n        gen_text = prefix\n        end = False\n        textgen_i = 0\n        while not end:\n            textgen = textgens[textgen_i % len(textgens)]\n            gen_text, end = textgenrnn_generate(textgen.model,\n                                                textgen.vocab,\n                                                textgen.indices_char,\n                                                temperature,\n                                                textgen.config[\'max_length\'],\n                                                textgen.META_TOKEN,\n                                                textgen.config[\'word_level\'],\n                                                textgen.config.get(\n                                                    \'single_text\', False),\n                                                max_gen_length,\n                                                prefix=gen_text,\n                                                synthesize=True,\n                                                stop_tokens=stop_tokens)\n            textgen_i += 1\n        if not return_as_list:\n            print(""{}\\n"".format(gen_text))\n        gen_texts.append(gen_text)\n    if return_as_list:\n        return gen_texts\n\n\ndef synthesize_to_file(textgens, destination_path, **kwargs):\n    texts = synthesize(textgens, return_as_list=True, **kwargs)\n    with open(destination_path, \'w\') as f:\n        for text in texts:\n            f.write(""{}\\n"".format(text))\n\n\nclass generate_after_epoch(Callback):\n    def __init__(self, textgenrnn, gen_epochs, max_gen_length):\n        super().__init__()\n        self.textgenrnn = textgenrnn\n        self.gen_epochs = gen_epochs\n        self.max_gen_length = max_gen_length\n\n    def on_epoch_end(self, epoch, logs={}):\n        if self.gen_epochs > 0 and (epoch+1) % self.gen_epochs == 0:\n            self.textgenrnn.generate_samples(\n                max_gen_length=self.max_gen_length)\n\n\nclass save_model_weights(Callback):\n    def __init__(self, textgenrnn, num_epochs, save_epochs):\n        super().__init__()\n        self.textgenrnn = textgenrnn\n        self.weights_name = textgenrnn.config[\'name\']\n        self.num_epochs = num_epochs\n        self.save_epochs = save_epochs\n\n    def on_epoch_end(self, epoch, logs={}):\n        if len(self.textgenrnn.model.inputs) > 1:\n            self.textgenrnn.model = Model(inputs=self.model.input[0],\n                                          outputs=self.model.output[1])\n        if self.save_epochs > 0 and (epoch+1) % self.save_epochs == 0 and self.num_epochs != (epoch+1):\n            print(""Saving Model Weights \xe2\x80\x94 Epoch #{}"".format(epoch+1))\n            self.textgenrnn.model.save_weights(\n                ""{}_weights_epoch_{}.hdf5"".format(self.weights_name, epoch+1))\n        else:\n            self.textgenrnn.model.save_weights(\n                ""{}_weights.hdf5"".format(self.weights_name))\n'"
