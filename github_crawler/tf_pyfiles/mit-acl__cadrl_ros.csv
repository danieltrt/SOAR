file_path,api_count,code
scripts/__init__.py,0,b''
scripts/agent.py,0,"b'import numpy as np\nfrom network import Config\nimport util\nimport operator\n\nclass Agent():\n    def __init__(self, start_x, start_y, goal_x, goal_y, radius=0.5, pref_speed=1.0, initial_heading=0.0, id=0):\n\n        self.policy_type = ""A3C""\n\n        # Global Frame states\n        self.pos_global_frame = np.array([start_x, start_y], dtype=\'float64\')\n        self.goal_global_frame = np.array([goal_x, goal_y], dtype=\'float64\')\n        self.vel_global_frame = np.array([pref_speed, 0.0], dtype=\'float64\')\n        # self.vel_global_frame = np.array([0.0, 0.0], dtype=\'float64\')\n        self.speed_global_frame = 0.0 \n        self.heading_global_frame = initial_heading\n        \n        # Ego Frame states\n        self.speed_ego_frame = 0.0\n        self.heading_ego_frame = 0.0 \n        self.vel_ego_frame = np.array([0.0, 0.0])\n        self.goal_ego_frame = np.array([0.0, 0.0]) # xy coords of goal position\n        \n        # Store past selected actions\n        self.chosen_action_dict = {}\n        self.action_time_lag = 0.0\n\n        self.num_actions_to_store = 3\n        self.past_actions = np.zeros((self.num_actions_to_store,2))\n\n        # Other parameters\n        self.radius = radius\n        self.pref_speed = pref_speed\n        self.id = id\n        self.dist_to_goal = 0.0\n\n        self.num_nearby_agents = None\n\n        if Config.ROBOT_MODE:\n            self.time_remaining_to_reach_goal = np.inf\n        elif Config.EVALUATE_MODE or Config.PLAY_MODE:\n            self.time_remaining_to_reach_goal = 4*np.linalg.norm(self.pos_global_frame - self.goal_global_frame)/self.pref_speed\n        else:\n            self.time_remaining_to_reach_goal = 2*np.linalg.norm(self.pos_global_frame - self.goal_global_frame)/self.pref_speed\n        self.t = 0.0\n\n        self.is_at_goal = False\n        self.was_at_goal_already = False\n        self.was_in_collision_already = False\n        self.in_collision = False\n        self.ran_out_of_time = False\n\n        self.global_state_history = None\n        self.ego_state_history = None\n        self.update_state([0.0,0.0],0.0)\n\n        self.min_dist_to_other_agents = np.inf\n\n\n    def _check_if_at_goal(self):\n        near_goal_threshold = 0.2\n        is_near_goal = np.linalg.norm([self.pos_global_frame - self.goal_global_frame]) <= near_goal_threshold\n        # print ""Agent:"",self.id,""is_near_goal:"",is_near_goal,""was_at_goal_already:"", self.was_at_goal_already,""is_at_goal:"",self.is_at_goal\n        # if self.is_at_goal and is_near_goal:\n            # self.was_at_goal_already = True\n        self.is_at_goal = is_near_goal\n        # print ""was_at_goal_already:"", self.was_at_goal_already,""is_at_goal:"",self.is_at_goal\n\n    def update_state(self, action, dt):\n        if self.is_at_goal or self.ran_out_of_time or self.in_collision:\n            if self.is_at_goal: self.was_at_goal_already = True\n            if self.in_collision: self.was_in_collision_already = True\n            self.vel_global_frame = np.array([0.0, 0.0])\n            return\n\n        # self.past_actions = np.roll(self.past_actions,1,axis=0)\n        # self.past_actions[0,:] = action\n\n        if self.action_time_lag > 0:\n            # Store current action in dictionary, then look up the past action that should be executed this step\n            self.chosen_action_dict[self.t] = action\n            # print ""-------------""\n            # print ""Agent id: %i"" %self.id\n            # print ""Current t:"", self.t\n            # print ""Current action:"", action\n            timestamp_of_action_to_execute = self.t - self.action_time_lag\n            # print ""timestamp_of_action_to_execute:"", timestamp_of_action_to_execute\n            if timestamp_of_action_to_execute < 0:\n                # print ""storing up actions....""\n                action_to_execute = np.array([0.0,0.0])\n            else:\n                nearest_timestamp, _ = util.find_nearest(np.array(self.chosen_action_dict.keys()),timestamp_of_action_to_execute)\n                # print ""nearest_timestamp:"", nearest_timestamp\n                action_to_execute = self.chosen_action_dict[nearest_timestamp[0]]\n            # print ""action_to_execute:"", action_to_execute\n        else:\n            action_to_execute = action\n\n        selected_speed = action_to_execute[0]*self.pref_speed\n        selected_heading = util.wrap(action_to_execute[1] + self.heading_global_frame) # in global frame\n\n        dx = selected_speed * np.cos(selected_heading) * dt\n        dy = selected_speed * np.sin(selected_heading) * dt\n        self.pos_global_frame += np.array([dx, dy])\n        self.vel_global_frame[0] = selected_speed * np.cos(selected_heading)\n        self.vel_global_frame[1] = selected_speed * np.sin(selected_heading)\n        self.speed_global_frame = selected_speed\n        self.heading_global_frame = selected_heading\n\n        # Compute heading w.r.t. ref_prll, ref_orthog coordinate axes\n        self.ref_prll, self.ref_orth = self.get_ref()\n        ref_prll_angle_global_frame = np.arctan2(self.ref_prll[1], self.ref_prll[0])\n        self.heading_ego_frame = util.wrap(self.heading_global_frame - ref_prll_angle_global_frame)\n\n        # Compute velocity w.r.t. ref_prll, ref_orthog coordinate axes\n        cur_speed = np.linalg.norm(self.vel_global_frame)\n        v_prll = cur_speed * np.cos(self.heading_ego_frame)\n        v_orthog = cur_speed * np.sin(self.heading_ego_frame)    \n        self.vel_ego_frame = np.array([v_prll, v_orthog])\n\n        # Update time left so agent does not run around forever\n        self.time_remaining_to_reach_goal -= dt\n        self.t += dt\n        if self.time_remaining_to_reach_goal <= 0.0 and not Config.ROBOT_MODE:\n            self.ran_out_of_time = True\n\n        self._update_state_history()\n\n        self._check_if_at_goal()\n\n        return\n\n    def _update_state_history(self):\n        global_state, ego_state = self.to_vector()\n        if self.global_state_history is None or self.ego_state_history is None:\n            self.global_state_history = np.expand_dims(np.hstack([self.t, global_state]), axis=0)\n            self.ego_state_history = np.expand_dims(ego_state,axis=0)\n        else:\n            self.global_state_history = np.vstack([self.global_state_history, np.hstack([self.t, global_state])])\n            self.ego_state_history = np.vstack([self.ego_state_history, ego_state])\n\n    def print_agent_info(self):\n        print \'----------\'\n        print \'Global Frame:\'\n        print \'(px,py):\', self.pos_global_frame\n        print \'(vx,vy):\', self.vel_global_frame\n        print \'(gx, gy):\', self.goal_global_frame\n        print \'speed:\', self.speed_global_frame\n        print \'heading:\', self.heading_global_frame\n        print \'Body Frame:\'\n        print \'(vx,vy):\', self.vel_ego_frame\n        print \'heading:\', self.heading_ego_frame\n        print \'----------\'\n\n    def to_vector(self):\n        global_state = np.array([self.pos_global_frame[0], self.pos_global_frame[1], \\\n            self.goal_global_frame[0], self.goal_global_frame[1], self.radius, self.pref_speed, \\\n            self.vel_global_frame[0], self.vel_global_frame[1], self.speed_global_frame, self.heading_global_frame])\n        ego_state = np.array([self.dist_to_goal, self.heading_ego_frame])\n        return global_state, ego_state\n\n    def observe(self, agents):\n        #\n        # Observation vector is as follows;\n        # [<this_agent_info>, <other_agent_1_info>, <other_agent_2_info>, ... , <other_agent_n_info>] \n        # where <this_agent_info> = [id, dist_to_goal, heading (in ego frame)]\n        # where <other_agent_i_info> = [pos in this agent\'s ego parallel coord, pos in this agent\'s ego orthog coord]\n        #\n\n        obs = np.zeros((Config.FULL_LABELED_STATE_LENGTH))\n\n        # Own agent state (ID is removed before inputting to NN, num other agents is used to rearrange other agents into sequence by NN)\n        obs[0] = self.id \n        if Config.MULTI_AGENT_ARCH == \'RNN\':\n            obs[Config.AGENT_ID_LENGTH] = 0 \n        obs[Config.AGENT_ID_LENGTH+Config.FIRST_STATE_INDEX:Config.AGENT_ID_LENGTH+Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE] = \\\n                             self.dist_to_goal, self.heading_ego_frame, self.pref_speed, self.radius\n\n        other_agent_dists = {}\n        for i, other_agent in enumerate(agents):\n            if other_agent.id == self.id:\n                continue\n            # project other elements onto the new reference frame\n            rel_pos_to_other_global_frame = other_agent.pos_global_frame - self.pos_global_frame\n            dist_between_agent_centers = np.linalg.norm(rel_pos_to_other_global_frame)\n            dist_2_other = dist_between_agent_centers - self.radius - other_agent.radius\n            if dist_between_agent_centers > Config.SENSING_HORIZON:\n                # print ""Agent too far away""\n                continue\n            other_agent_dists[i] = dist_2_other\n        # print ""other_agent_dists:"", other_agent_dists\n        sorted_pairs = sorted(other_agent_dists.items(), key=operator.itemgetter(1))\n        sorted_inds = [ind for (ind,pair) in sorted_pairs]\n        sorted_inds.reverse()\n        clipped_sorted_inds = sorted_inds[-Config.MAX_NUM_OTHER_AGENTS_OBSERVED:]\n        clipped_sorted_agents = [agents[i] for i in clipped_sorted_inds]\n\n        self.num_nearby_agents = len(clipped_sorted_inds)\n        # print ""sorted_inds:"", sorted_inds\n        # print ""clipped_sorted_inds:"", clipped_sorted_inds\n        # print ""clipped_sorted_agents:"", clipped_sorted_agents\n\n        i = 0\n        for other_agent in clipped_sorted_agents:\n            if other_agent.id == self.id:\n                continue\n            # project other elements onto the new reference frame\n            rel_pos_to_other_global_frame = other_agent.pos_global_frame - self.pos_global_frame\n            p_parallel_ego_frame = np.dot(rel_pos_to_other_global_frame, self.ref_prll)\n            p_orthog_ego_frame = np.dot(rel_pos_to_other_global_frame, self.ref_orth)\n            v_parallel_ego_frame = np.dot(other_agent.vel_global_frame, self.ref_prll)\n            v_orthog_ego_frame = np.dot(other_agent.vel_global_frame, self.ref_orth)\n            dist_2_other = np.linalg.norm(rel_pos_to_other_global_frame) - self.radius - other_agent.radius\n            combined_radius = self.radius + other_agent.radius\n            is_on = 1\n\n            start_index = Config.AGENT_ID_LENGTH + Config.FIRST_STATE_INDEX + Config.HOST_AGENT_STATE_SIZE + Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i\n            end_index = Config.AGENT_ID_LENGTH + Config.FIRST_STATE_INDEX + Config.HOST_AGENT_STATE_SIZE + Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*(i+1)\n            \n            other_obs = np.array([p_parallel_ego_frame, p_orthog_ego_frame, v_parallel_ego_frame, v_orthog_ego_frame, other_agent.radius, \\\n                                    combined_radius, dist_2_other])\n            if Config.MULTI_AGENT_ARCH in [\'WEIGHT_SHARING\',\'VANILLA\']:\n                other_obs = np.hstack([other_obs, is_on])\n            obs[start_index:end_index] = other_obs\n            i += 1\n\n            \n        if Config.MULTI_AGENT_ARCH == \'RNN\':\n            obs[Config.AGENT_ID_LENGTH] = i # Will be used by RNN for seq_length\n        if Config.MULTI_AGENT_ARCH in [\'WEIGHT_SHARING\',\'VANILLA\']:\n            for j in range(i,Config.MAX_NUM_OTHER_AGENTS_OBSERVED):\n                start_index = Config.AGENT_ID_LENGTH + Config.FIRST_STATE_INDEX + Config.HOST_AGENT_STATE_SIZE + Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*j\n                end_index = Config.AGENT_ID_LENGTH + Config.FIRST_STATE_INDEX + Config.HOST_AGENT_STATE_SIZE + Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*(j+1)\n                other_obs[-1] = 0\n                obs[start_index:end_index] = other_obs\n\n        # past_actions = self.past_actions[1:3,:].flatten() # Only adds previous 1 action to state vector\n        # obs = np.hstack([obs, past_actions])\n\n        return obs\n\n    def get_ref(self):\n        #\n        # Using current and goal position of agent in global frame,\n        # compute coordinate axes of ego frame\n        #\n        # Returns:\n        # ref_prll: vector pointing from agent position -> goal\n        # ref_orthog: vector orthogonal to ref_prll\n        #\n        goal_direction = self.goal_global_frame - self.pos_global_frame\n        self.dist_to_goal = np.linalg.norm(goal_direction)\n        if self.dist_to_goal > 1e-8:\n            ref_prll = goal_direction / self.dist_to_goal\n        else:\n            ref_prll = goal_direction\n        ref_orth = np.array([-ref_prll[1], ref_prll[0]]) # rotate by 90 deg\n        return ref_prll, ref_orth'"
scripts/cadrl_node.py,0,"b'#!/usr/bin/env python\n\nimport rospy\nimport sys\nfrom std_msgs.msg import Float32, ColorRGBA, Int32\nfrom geometry_msgs.msg import PoseStamped, Twist, Vector3, Point\nfrom ford_msgs.msg import PedTrajVec, NNActions, PlannerMode, Clusters\nfrom visualization_msgs.msg import Marker, MarkerArray\n\nimport numpy as np\nimport numpy.matlib\nimport pickle\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nimport copy\nimport os\nimport time\nimport random\nimport math\n\nimport rospkg\n\nimport network\nimport agent\nimport util\n\nPED_RADIUS = 0.3\n# angle_1 - angle_2\n# contains direction in range [-3.14, 3.14]\ndef find_angle_diff(angle_1, angle_2):\n    angle_diff_raw = angle_1 - angle_2\n    angle_diff = (angle_diff_raw + np.pi) % (2 * np.pi) - np.pi\n    return angle_diff\n\nclass NN_jackal():\n    def __init__(self, veh_name, veh_data, nn, actions):\n        self.node_name = rospy.get_name()\n        self.prev_other_agents_state = []\n\n        # vehicle info\n        self.veh_name = veh_name\n        self.veh_data = veh_data\n\n        # self.agent = agent.Agent(0.0, 0.0, 100.0, 100.0, radius, pref_speed, initial_heading, id)\n        \n        # neural network\n        self.nn = nn\n        self.actions = actions\n        # self.value_net = value_net\n        self.operation_mode = PlannerMode()\n        self.operation_mode.mode = self.operation_mode.NN\n        \n        # for subscribers\n        self.pose = PoseStamped()\n        self.vel = Vector3()\n        self.psi = 0.0\n        self.ped_traj_vec = []\n        self.other_agents_state = []\n        self.feasible_actions = NNActions()\n\n        # for publishers\n        self.global_goal = PoseStamped()\n        self.goal = PoseStamped()\n        self.goal.pose.position.x = veh_data[\'goal\'][0]\n        self.goal.pose.position.y = veh_data[\'goal\'][1]\n        self.desired_position = PoseStamped()\n        self.desired_action = np.zeros((2,))\n\n        # handle obstacles close to vehicle\'s front\n        self.stop_moving_flag = False\n        self.d_min = 0.0\n        self.new_subgoal_received = False\n        self.new_global_goal_received = False\n\n        # visualization\n        self.path_marker = Marker()\n\n        # Clusters\n        self.prev_clusters = Clusters()\n        self.current_clusters = Clusters()\n\n        # subscribers and publishers\n        self.num_poses = 0\n        self.num_actions_computed = 0.0\n        self.pub_others = rospy.Publisher(\'~other_vels\',Vector3,queue_size=1)\n        self.pub_twist = rospy.Publisher(\'~nn_cmd_vel\',Twist,queue_size=1)\n        self.pub_pose_marker = rospy.Publisher(\'~pose_marker\',Marker,queue_size=1)\n        self.pub_agent_marker = rospy.Publisher(\'~agent_marker\',Marker,queue_size=1)\n        self.pub_agent_markers = rospy.Publisher(\'~agent_markers\',MarkerArray,queue_size=1)\n        self.pub_path_marker = rospy.Publisher(\'~path_marker\',Marker,queue_size=1)\n        self.pub_goal_path_marker = rospy.Publisher(\'~goal_path_marker\',Marker,queue_size=1)\n        self.sub_pose = rospy.Subscriber(\'~pose\',PoseStamped,self.cbPose)\n        self.sub_vel = rospy.Subscriber(\'~velocity\',Vector3,self.cbVel)\n        self.sub_nn_actions = rospy.Subscriber(\'~safe_actions\',NNActions,self.cbNNActions)\n        self.sub_mode = rospy.Subscriber(\'~mode\',PlannerMode, self.cbPlannerMode)\n        self.sub_global_goal = rospy.Subscriber(\'~goal\',PoseStamped, self.cbGlobalGoal)\n        \n        self.use_clusters = True\n        # self.use_clusters = False\n        if self.use_clusters:\n            self.sub_clusters = rospy.Subscriber(\'~clusters\',Clusters, self.cbClusters)\n        else:\n            self.sub_peds = rospy.Subscriber(\'~peds\',PedTrajVec, self.cbPeds)\n\n        # control timer\n        self.control_timer = rospy.Timer(rospy.Duration(0.01),self.cbControl)\n        self.nn_timer = rospy.Timer(rospy.Duration(0.1),self.cbComputeActionGA3C)\n\n    def cbGlobalGoal(self,msg):\n        self.new_global_goal_received = True\n        self.global_goal = msg\n        self.operation_mode.mode = self.operation_mode.SPIN_IN_PLACE\n\n        self.goal.pose.position.x = msg.pose.position.x\n        self.goal.pose.position.y = msg.pose.position.y\n        self.goal.header = msg.header\n        self.new_subgoal_received = True\n\n    def cbNNActions(self,msg):\n        # if msg.header.seq % 20 == 0:\n        #     self.goal.pose.position.x = msg.subgoal.x\n        #     self.goal.pose.position.y = msg.subgoal.y\n        #     self.goal.header = msg.header\n        #     self.new_subgoal_received = True\n        self.feasible_actions = msg\n\n    def cbPlannerMode(self, msg):\n        self.operation_mode = msg\n        self.operation_mode.mode = self.operation_mode.NN\n\n    def cbPose(self, msg):\n        self.num_poses += 1\n        q = msg.pose.orientation\n        self.psi = np.arctan2(2.0*(q.w*q.z + q.x*q.y), 1-2*(q.y*q.y+q.z*q.z)) # bounded by [-pi, pi]\n        self.pose = msg\n        self.visualize_pose(msg.pose.position,msg.pose.orientation)\n\n    def cbVel(self, msg):\n        self.vel = msg\n\n    def cbPeds(self, msg):\n        t_start = rospy.Time.now()\n        self.ped_traj_vec = [ped_traj for ped_traj in msg.ped_traj_vec if len(ped_traj.traj) > 0]\n        num_peds = len(self.ped_traj_vec)\n\n        # compute relative position with respect to the Jackal\n        rel_dist = np.zeros((num_peds, )) \n        rel_angle = np.zeros((num_peds, )) \n        # (rel_dist, angle)\n        for i, ped_traj in enumerate(self.ped_traj_vec):\n            rel_x = ped_traj.traj[-1].pose.x - self.pose.pose.position.x\n            rel_y = ped_traj.traj[-1].pose.y - self.pose.pose.position.y\n            rel_dist[i] = np.linalg.norm(np.array([rel_x, rel_y])) \n            rel_angle[i] = find_angle_diff(np.arctan2(rel_y, rel_x), self.psi)\n\n        # ignore people in the back of Jackal (60 deg cone)\n        valid_inds = np.where(abs(rel_angle)< 5.0 / 6.0 * np.pi)[0]\n\n        # get the n closest agents\n        self.other_agents_state = []\n        if len(valid_inds) == 0:\n            return\n        else:\n            if len(valid_inds) == 1:\n                # print ""valid_inds:"", valid_inds\n                # print ""self.ped_traj_vec:"", self.ped_traj_vec\n                valid_inds = valid_inds[0]\n                ped_traj_vec = [self.ped_traj_vec[valid_inds]]\n                rel_dist = np.array([rel_dist[valid_inds]])\n            elif len(valid_inds) > 1:\n                # print \'before\', len(self.ped_traj_vec)\n                # print \'valid_inds\', valid_inds\n                ped_traj_vec = [self.ped_traj_vec[tt] for tt in valid_inds]\n                # print \'after\', len(self.ped_traj_vec)\n                rel_dist = rel_dist[valid_inds]\n\n            # sort other agents by rel_dist\n            # num_neighbors = min(len(rel_dist), self.value_net.num_agents)\n            # print \'num_neighbors\', num_neighbors\n            # print \'rel_dist\', rel_dist\n            # neighbor_inds = np.argpartition(rel_dist, num_neighbors)[:num_neighbors]\n            if len(rel_dist) > self.value_net.num_agents-1:\n                num_neighbors = self.value_net.num_agents-1\n                neighbor_inds = np.argpartition(rel_dist, num_neighbors)[:num_neighbors]\n            else:\n                neighbor_inds = np.arange(len(rel_dist))\n            # agent state: [pos.x, pos.y, vel.x, vel.y, heading_angle, pref_speed, \\\n            #            goals[0].x, goals[0].y, radius, turning_dir]\n            for tt in neighbor_inds:\n                ped_traj = ped_traj_vec[tt]\n                # rel pos, rel vel, size\n                x = ped_traj.traj[-1].pose.x; y = ped_traj.traj[-1].pose.y\n                v_x = ped_traj.traj[-1].velocity.x; v_y = ped_traj.traj[-1].velocity.y\n                radius = PED_RADIUS;turning_dir = 0.0\n                # helper fields\n                heading_angle = np.arctan2(v_y, v_x)\n                pref_speed = np.linalg.norm(np.array([v_x, v_y]))\n                goal_x = x + 5.0; goal_y = y + 5.0\n                \n                # filter speed\n                alpha = 0.2\n                for prev_other_agent_state in self.prev_other_agents_state:\n                    pos_diff = np.linalg.norm(prev_other_agent_state[0:2] - np.array([x,y]))\n                    heading_diff_abs = abs(find_angle_diff(prev_other_agent_state[4], heading_angle))\n                    if pos_diff < 0.5 and heading_diff_abs < np.pi / 4.0:\n                        v_x = alpha * v_x + (1-alpha) * prev_other_agent_state[2]\n                        v_y = alpha * v_y + (1-alpha) * prev_other_agent_state[3]\n\n                        # TODO: find the best match rather than the first match\n                        break\n\n                if pref_speed < 0.2:\n                    pref_speed = 0; v_x = 0; v_y = 0\n                other_agent_state = np.array([x, y, v_x, v_y, heading_angle, pref_speed, \\\n                    goal_x, goal_y, radius, turning_dir])\n                self.other_agents_state.append(other_agent_state)\n\n            self.prev_other_agents_state = copy.deepcopy(self.other_agents_state)\n        t_end = rospy.Time.now()\n        # print ""cbPeds took:"", (t_end - t_start).to_sec(), ""sec""\n\n    def cbClusters(self, msg):\n        other_agents = []\n\n\n        xs = []; ys = []; radii = []; labels = []\n        num_clusters = len(msg.labels)\n        for i in range(num_clusters):\n            index = msg.labels[i]\n            x = msg.mean_points[i].x; y = msg.mean_points[i].y\n            v_x = msg.velocities[i].x; v_y = msg.velocities[i].y\n            # radius = PED_RADIUS\n            lower_r = np.linalg.norm(np.array([msg.mean_points[i].x-msg.min_points[i].x, msg.mean_points[i].y-msg.min_points[i].y]))\n            upper_r = np.linalg.norm(np.array([msg.mean_points[i].x-msg.max_points[i].x, msg.mean_points[i].y-msg.max_points[i].y]))\n            inflation_factor = 1.5\n            radius = max(PED_RADIUS, inflation_factor * max(upper_r, lower_r))\n\n\n            xs.append(x); ys.append(y); radii.append(radius); labels.append(index)\n            # self.visualize_other_agent(x,y,radius,msg.labels[i])\n            # helper fields\n            heading_angle = np.arctan2(v_y, v_x)\n            pref_speed = np.linalg.norm(np.array([v_x, v_y]))\n            goal_x = x + 5.0; goal_y = y + 5.0\n            \n            # # filter speed\n            # alpha = 0.2\n            # for prev_other_agent_state in self.prev_other_agents_state:\n            #     pos_diff = np.linalg.norm(prev_other_agent_state[0:2] - np.array([x,y]))\n            #     heading_diff_abs = abs(find_angle_diff(prev_other_agent_state[4], heading_angle))\n            #     if pos_diff < 0.5 and heading_diff_abs < np.pi / 4.0:\n            #         v_x = alpha * v_x + (1-alpha) * prev_other_agent_state[2]\n            #         v_y = alpha * v_y + (1-alpha) * prev_other_agent_state[3]\n\n            #         # TODO: find the best match rather than the first match\n            #         break\n            if pref_speed < 0.2:\n                pref_speed = 0; v_x = 0; v_y = 0\n            other_agents.append(agent.Agent(x, y, goal_x, goal_y, radius, pref_speed, heading_angle, index))\n        self.visualize_other_agents(xs, ys, radii, labels)\n        self.other_agents_state = other_agents\n\n    def stop_moving(self):\n        twist = Twist()\n        self.pub_twist.publish(twist)\n        # print \'Stop Moving.\'\n\n    def update_action(self, action):\n        # print \'update action\'\n        self.desired_action = action\n        self.desired_position.pose.position.x = self.pose.pose.position.x + 1*action[0]*np.cos(action[1])\n        self.desired_position.pose.position.y = self.pose.pose.position.y + 1*action[0]*np.sin(action[1])\n\n        twist = Twist()\n        twist.linear.x = action[0]\n        yaw_error = action[1] - self.psi\n        if yaw_error > np.pi:\n            yaw_error -= 2*np.pi\n        if yaw_error < -np.pi:\n            yaw_error += 2*np.pi\n        twist.angular.z = 2*yaw_error\n\n    def find_vmax(self, d_min, heading_diff):\n        # Calculate maximum linear velocity, as a function of error in\n        # heading and clear space in front of the vehicle\n        # (With nothing in front of vehicle, it\'s not important to\n        # track MPs perfectly; with an obstacle right in front, the\n        # vehicle must turn in place, then drive forward.)\n        d_min = max(0.0,d_min)\n        x = 0.3\n        margin = 0.3\n        # y = max(d_min - 0.3, 0.0)\n        y = max(d_min, 0.0)\n        # making sure x < y \n        if x > y:\n            x = 0\n        w_max = 1\n        # x^2 + y^2 = (v_max/w_max)^2\n        v_max = w_max * np.sqrt(x**2 + y**2)\n        v_max = np.clip(v_max,0.0,self.veh_data[\'pref_speed\'])\n        # print \'V_max, x, y, d_min\', v_max, x, y, d_min\n        if abs(heading_diff) < np.pi / 18:\n            return self.veh_data[\'pref_speed\']\n        return v_max\n\n    def cbControl(self, event):\n        if self.goal.header.stamp == rospy.Time(0) or self.stop_moving_flag \\\n            and not self.new_global_goal_received:\n            self.stop_moving()\n            return\n        elif self.operation_mode.mode==self.operation_mode.NN:\n            desired_yaw = self.desired_action[1]\n            yaw_error = desired_yaw - self.psi\n            if abs(yaw_error) > np.pi:\n                yaw_error -= np.sign(yaw_error)*2*np.pi\n            # print \'yaw_error:\',yaw_error\n            # max_yaw_error = 0.8\n            # yaw_error = self.desired_action[1]\n            gain = 2\n            vw = gain*yaw_error\n\n            use_d_min = False\n            if True: \n                use_d_min = True\n                # print ""vmax:"", self.find_vmax(self.d_min,yaw_error)\n                vx = min(self.desired_action[0], self.find_vmax(self.d_min,yaw_error))\n            else:\n                vx = self.desired_action[0]\n            # print ""vx:"", vx\n            # elif abs(yaw_error) < max_yaw_error:\n            #     vw = gain*yaw_error\n            # else:\n            #     vw = gain*max_yaw_error*np.sign(yaw_error)\n\n            twist = Twist()\n            twist.angular.z = vw\n            twist.linear.x = vx\n            self.pub_twist.publish(twist)\n            self.visualize_action(use_d_min)\n            return\n        elif self.operation_mode.mode == self.operation_mode.SPIN_IN_PLACE:\n            print \'Spinning in place.\'\n            self.stop_moving_flag = False\n            angle_to_goal = np.arctan2(self.global_goal.pose.position.y - self.pose.pose.position.y, \\\n                self.global_goal.pose.position.x - self.pose.pose.position.x) \n            global_yaw_error = self.psi - angle_to_goal\n            if abs(global_yaw_error) > 0.5:\n                vx = 0.0\n                vw = 1.0\n                twist = Twist()\n                twist.angular.z = vw\n                twist.linear.x = vx\n                self.pub_twist.publish(twist)\n            else:\n                print \'Done spinning in place\'\n                self.operation_mode.mode = self.operation_mode.NN\n                self.new_global_goal_received = False\n            return\n        else:\n            self.stop_moving()\n            return\n\n    def cbComputeActionGA3C(self, event):\n        if self.operation_mode.mode!=self.operation_mode.NN:\n            print \'Not in NN mode\'\n            print self.operation_mode.mode\n            return\n\n\n        # construct agent_state\n        x = self.pose.pose.position.x; y = self.pose.pose.position.y\n        v_x = self.vel.x; v_y = self.vel.y\n        radius = self.veh_data[\'radius\']; turning_dir = 0.0\n        heading_angle = self.psi\n        pref_speed = self.veh_data[\'pref_speed\']\n        goal_x = self.goal.pose.position.x; goal_y = self.goal.pose.position.y\n        # in case current speed is larger than desired speed\n        v = np.linalg.norm(np.array([v_x, v_y]))\n        if v > pref_speed:\n            v_x = v_x * pref_speed / v\n            v_y = v_y * pref_speed / v\n\n        host_agent = agent.Agent(x, y, goal_x, goal_y, radius, pref_speed, heading_angle, 0)\n        host_agent.vel_global_frame = np.array([v_x, v_y])\n        # host_agent.print_agent_info()\n\n        other_agents_state = copy.deepcopy(self.other_agents_state)\n        obs = host_agent.observe(other_agents_state)[1:]\n        obs = np.expand_dims(obs, axis=0)\n        # print ""obs:"", obs\n        predictions = self.nn.predict_p(obs, None)[0]\n        # print ""predictions:"", predictions\n        # print ""best action index:"", np.argmax(predictions)\n        raw_action = copy.deepcopy(self.actions[np.argmax(predictions)])\n        action = np.array([pref_speed*raw_action[0], util.wrap(raw_action[1] + self.psi)])\n        # print ""raw_action:"", raw_action\n        # print ""action:"", action\n\n        # if close to goal\n        kp_v = 0.5\n        kp_r = 1   \n\n        if host_agent.dist_to_goal < 2.0: # and self.percentComplete>=0.9:\n            # print ""somewhat close to goal""\n            pref_speed = max(min(kp_v * (host_agent.dist_to_goal-0.1), pref_speed), 0.0)\n            action[0] = min(raw_action[0], pref_speed)\n            turn_amount = max(min(kp_r * (host_agent.dist_to_goal-0.1), 1.0), 0.0) * raw_action[1]\n            action[1] = util.wrap(turn_amount + self.psi)\n        if host_agent.dist_to_goal < 0.3:\n            self.stop_moving_flag = True\n        else:\n            self.stop_moving_flag = False\n\n        # print \'chosen action (rel angle)\', action[0], action[1]\n        self.update_action(action)\n\n    def update_subgoal(self,subgoal):\n        self.goal.pose.position.x = subgoal[0]\n        self.goal.pose.position.y = subgoal[1]\n\n    def visualize_subgoal(self,subgoal, subgoal_options=None):\n        markers = MarkerArray()\n\n        # Display GREEN DOT at NN subgoal\n        marker = Marker()\n        marker.header.stamp = rospy.Time.now()\n        marker.header.frame_id = \'map\'\n        marker.ns = \'subgoal\'\n        marker.id = 0\n        marker.type = marker.CUBE\n        marker.action = marker.ADD\n        marker.pose.position.x = subgoal[0]\n        marker.pose.position.y = subgoal[1]\n        marker.scale = Vector3(x=0.4,y=0.4,z=0.2)\n        marker.color = ColorRGBA(g=1.0,a=1.0)\n        marker.lifetime = rospy.Duration(2.0)\n        self.pub_goal_path_marker.publish(marker)\n\n        if subgoal_options is not None:\n            for i in xrange(len(subgoal_options)):\n                marker = Marker()\n                marker.header.stamp = rospy.Time.now()\n                marker.header.frame_id = \'map\'\n                marker.ns = \'subgoal\'\n                marker.id = i+1\n                marker.type = marker.CUBE\n                marker.action = marker.ADD\n                marker.pose.position.x = subgoal_options[i][0]\n                marker.pose.position.y = subgoal_options[i][1]\n                marker.scale = Vector3(x=0.2,y=0.2,z=0.2)\n                marker.color = ColorRGBA(b=1.0,r=1.0,a=1.0)\n                marker.lifetime = rospy.Duration(1.0)\n                self.pub_goal_path_marker.publish(marker)\n\n\n\n\n    def visualize_pose(self,pos,orientation):\n        # Yellow Box for Vehicle\n        marker = Marker()\n        marker.header.stamp = rospy.Time.now()\n        marker.header.frame_id = \'map\'\n        marker.ns = \'agent\'\n        marker.id = 0\n        marker.type = marker.CUBE\n        marker.action = marker.ADD\n        marker.pose.position = pos\n        marker.pose.orientation = orientation\n        marker.scale = Vector3(x=0.7,y=0.42,z=1)\n        marker.color = ColorRGBA(r=1.0,g=1.0,a=1.0)\n        marker.lifetime = rospy.Duration(1.0)\n        self.pub_pose_marker.publish(marker)\n\n        # Red track for trajectory over time\n        marker = Marker()\n        marker.header.stamp = rospy.Time.now()\n        marker.header.frame_id = \'map\'\n        marker.ns = \'agent\'\n        marker.id = self.num_poses\n        marker.type = marker.CUBE\n        marker.action = marker.ADD\n        marker.pose.position = pos\n        marker.pose.orientation = orientation\n        marker.scale = Vector3(x=0.2,y=0.2,z=0.2)\n        marker.color = ColorRGBA(r=1.0,a=1.0)\n        marker.lifetime = rospy.Duration(10.0)\n        self.pub_pose_marker.publish(marker)\n\n    def visualize_other_agents(self,xs,ys,radii,labels):\n        markers = MarkerArray()\n        for i in range(len(xs)):\n            # Orange box for other agent\n            marker = Marker()\n            marker.header.stamp = rospy.Time.now()\n            marker.header.frame_id = \'map\'\n            marker.ns = \'other_agent\'\n            marker.id = labels[i]\n            marker.type = marker.CYLINDER\n            marker.action = marker.ADD\n            marker.pose.position.x = xs[i]\n            marker.pose.position.y = ys[i]\n            # marker.pose.orientation = orientation\n            marker.scale = Vector3(x=2*radii[i],y=2*radii[i],z=1)\n            marker.color = ColorRGBA(r=1.0,g=0.4,a=1.0)\n            marker.lifetime = rospy.Duration(0.1)\n            markers.markers.append(marker)\n\n        self.pub_agent_markers.publish(markers)\n\n    def visualize_action(self, use_d_min):\n        # Display BLUE ARROW from current position to NN desired position\n        marker = Marker()\n        marker.header.stamp = rospy.Time.now()\n        marker.header.frame_id = \'map\'\n        marker.ns = \'path_arrow\'\n        marker.id = 0\n        marker.type = marker.ARROW\n        marker.action = marker.ADD\n        marker.points.append(self.pose.pose.position)\n        marker.points.append(self.desired_position.pose.position)\n        marker.scale = Vector3(x=0.1,y=0.2,z=0.2)\n        marker.color = ColorRGBA(b=1.0,a=1.0)\n        marker.lifetime = rospy.Duration(0.5)\n        self.pub_goal_path_marker.publish(marker)\n\n        # Display BLUE DOT at NN desired position\n        marker = Marker()\n        marker.header.stamp = rospy.Time.now()\n        marker.header.frame_id = \'map\'\n        marker.ns = \'path_trail\'\n        marker.id = self.num_poses\n        marker.type = marker.CUBE\n        marker.action = marker.ADD\n        marker.pose.position = copy.deepcopy(self.desired_position.pose.position)\n        marker.scale = Vector3(x=0.2,y=0.2,z=0.2)\n        marker.color = ColorRGBA(b=1.0,a=0.1)\n        marker.lifetime = rospy.Duration(0.5)\n        if self.desired_action[0] == 0.0:\n            marker.pose.position.x += 2.0*np.cos(self.desired_action[1])\n            marker.pose.position.y += 2.0*np.sin(self.desired_action[1])\n        self.pub_goal_path_marker.publish(marker)\n\n        # Display RED LINE from along minimum clear distance in front\n        # marker = Marker()\n        # marker.header.stamp = rospy.Time.now()\n        # marker.header.frame_id = \'map\'\n        # marker.ns = \'clear_distance\'\n        # marker.id = 0\n        # marker.type = marker.LINE_LIST\n        # marker.lifetime = rospy.Duration(0.5)\n        # marker.scale = Vector3(x=0.08,y=0.08,z=0.08)\n        # if use_d_min:\n        #     marker.color = ColorRGBA(r=1.0,a=1.0)\n        # else:\n        #     marker.color = ColorRGBA(r=1.0,g=1.0,a=1.0)\n        # x_midpt = self.pose.pose.position.x + self.d_min*np.cos(self.psi)\n        # y_midpt = self.pose.pose.position.y + self.d_min*np.sin(self.psi)\n        # x_max = x_midpt - 1*np.sin(self.psi)\n        # x_min = x_midpt + 1*np.sin(self.psi)\n        # y_max = y_midpt + 1*np.cos(self.psi)\n        # y_min = y_midpt - 1*np.cos(self.psi)\n        # marker.points.append(Point(x=x_max,y=y_max))\n        # marker.points.append(Point(x=x_min,y=y_min))\n        # self.pub_goal_path_marker.publish(marker)\n\n\n    def on_shutdown(self):\n        rospy.loginfo(""[%s] Shutting down."" %(self.node_name))\n        self.stop_moving()\n        rospy.loginfo(""Stopped %s\'s velocity."" %(self.veh_name))\n\n\ndef run():\n    print \'hello world from cadrl_node.py\'\n    file_dir = os.path.dirname(os.path.realpath(__file__))\n    plt.rcParams.update({\'font.size\': 18})\n    rospack = rospkg.RosPack()\n\n    a = network.Actions()\n    actions = a.actions\n    num_actions = a.num_actions\n    nn = network.NetworkVP_rnn(network.Config.DEVICE, \'network\', num_actions)\n    nn.simple_load(rospack.get_path(\'cadrl_ros\')+\'/checkpoints/network_01900000\')\n\n    rospy.init_node(\'nn_jackal\',anonymous=False)\n    veh_name = \'JA01\'\n    pref_speed = rospy.get_param(""~jackal_speed"")\n    veh_data = {\'goal\':np.zeros((2,)),\'radius\':0.5,\'pref_speed\':pref_speed,\'kw\':10.0,\'kp\':1.0,\'name\':\'JA01\'}\n\n    print ""********\\n*******\\n*********\\nJackal speed:"", pref_speed, ""\\n**********\\n******""\n\n    nn_jackal = NN_jackal(veh_name, veh_data, nn, actions)\n    rospy.on_shutdown(nn_jackal.on_shutdown)\n\n    rospy.spin()\n\nif __name__ == \'__main__\':\n    run()\n'"
scripts/network.py,22,"b'import os\nimport re\nimport numpy as np\nimport tensorflow as tf\nimport time\n\nclass Actions():\n    # Define 11 choices of actions to be:\n    # [v_pref,      [-pi/6, -pi/12, 0, pi/12, pi/6]]\n    # [0.5*v_pref,  [-pi/6, 0, pi/6]]\n    # [0,           [-pi/6, 0, pi/6]]\n    def __init__(self):\n        self.actions = np.mgrid[1.0:1.1:0.5, -np.pi/6:np.pi/6+0.01:np.pi/12].reshape(2, -1).T\n        self.actions = np.vstack([self.actions,np.mgrid[0.5:0.6:0.5, -np.pi/6:np.pi/6+0.01:np.pi/6].reshape(2, -1).T])\n        self.actions = np.vstack([self.actions,np.mgrid[0.0:0.1:0.5, -np.pi/6:np.pi/6+0.01:np.pi/6].reshape(2, -1).T])\n        self.num_actions = len(self.actions)\n\nclass NetworkVPCore(object):\n    def __init__(self, device, model_name, num_actions):\n        self.device = device\n        self.model_name = model_name\n        self.num_actions = num_actions\n\n        self.graph = tf.Graph()\n        with self.graph.as_default() as g:\n            with tf.device(self.device):\n                self._create_graph()\n\n                self.sess = tf.Session(\n                    graph=self.graph,\n                    config=tf.ConfigProto(\n                        allow_soft_placement=True,\n                        log_device_placement=False,\n                        gpu_options=tf.GPUOptions(allow_growth=True)))\n                self.sess.run(tf.global_variables_initializer())\n\n                vars = tf.global_variables()\n                self.saver = tf.train.Saver({var.name: var for var in vars}, max_to_keep=0)\n\n    \n    def _create_graph_inputs(self):\n        self.x = tf.placeholder(tf.float32, [None, Config.NN_INPUT_SIZE], name=\'X\')\n \n    def _create_graph_outputs(self):\n        # FCN\n        self.fc1 = tf.layers.dense(inputs=self.final_flat, units = 256, use_bias = True, activation=tf.nn.relu, name = \'fullyconnected1\')\n\n        # Cost: p\n        self.logits_p = tf.layers.dense(inputs = self.fc1, units = self.num_actions, name = \'logits_p\', activation = None)\n        self.softmax_p = (tf.nn.softmax(self.logits_p) + Config.MIN_POLICY) / (1.0 + Config.MIN_POLICY * self.num_actions)\n\n        # Cost: v \n        self.logits_v = tf.squeeze(tf.layers.dense(inputs=self.fc1, units = 1, use_bias = True, activation=None, name = \'logits_v\'), axis=[1])\n\n    def predict_p(self, x):\n        return self.sess.run(self.softmax_p, feed_dict={self.x: x})\n\n    def predict_v(self, x):\n        return self.sess.run(self.logits_v, feed_dict={self.x: x})\n\n    def get_lstm_output(self, x):\n        return self.sess.run(self.rnn_output, feed_dict={self.x: x})\n\n    def predict_p_from_lstm_output(self, host_agent_vec, lstm_output):\n        return self.sess.run(nn.softmax_p, feed_dict={self.host_agent_vec: host_agent_vec, self.layer1_input: layer1_input})\n\n    def simple_load(self, filename=None):\n        if filename is None:\n            print ""[network.py] Didn\'t define simple_load filename""\n        self.saver.restore(self.sess, filename)\n\nclass NetworkVP_rnn(NetworkVPCore):\n    def __init__(self, device, model_name, num_actions):\n        super(self.__class__, self).__init__(device, model_name, num_actions)\n\n    def _create_graph(self):\n        # Use shared parent class to construct graph inputs\n        self._create_graph_inputs()\n\n        if Config.USE_REGULARIZATION:\n            regularizer = tf.contrib.layers.l2_regularizer(scale=0.0)\n        else:\n            regularizer = None\n\n        if Config.NORMALIZE_INPUT:\n            self.avg_vec = tf.constant(Config.NN_INPUT_AVG_VECTOR, dtype = tf.float32)\n            self.std_vec = tf.constant(Config.NN_INPUT_STD_VECTOR, dtype = tf.float32)\n            self.x_normalized = (self.x - self.avg_vec) / self.std_vec\n        else:\n            self.x_normalized = self.x\n\n\n        if Config.MULTI_AGENT_ARCH == \'RNN\':\n            num_hidden = 64\n            max_length = Config.MAX_NUM_OTHER_AGENTS_OBSERVED\n            self.num_other_agents = self.x[:,0]\n            self.host_agent_vec = self.x_normalized[:,Config.FIRST_STATE_INDEX:Config.HOST_AGENT_STATE_SIZE+Config.FIRST_STATE_INDEX:]\n            self.other_agent_vec = self.x_normalized[:,Config.HOST_AGENT_STATE_SIZE+Config.FIRST_STATE_INDEX:]\n            self.other_agent_seq = tf.reshape(self.other_agent_vec, [-1, max_length, Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH])\n            self.rnn_outputs, self.rnn_state = tf.nn.dynamic_rnn(tf.contrib.rnn.LSTMCell(num_hidden), self.other_agent_seq, dtype=tf.float32, sequence_length=self.num_other_agents)\n            self.rnn_output = self.rnn_state.h\n            self.layer1_input = tf.concat([self.host_agent_vec, self.rnn_output],1, name=\'layer1_input\')\n            self.layer1 = tf.layers.dense(inputs=self.layer1_input, units=256, activation=tf.nn.relu, kernel_regularizer=regularizer, name = \'layer1\')\n\n        self.layer2 = tf.layers.dense(inputs=self.layer1, units=256, activation=tf.nn.relu, name = \'layer2\')\n        self.final_flat = tf.contrib.layers.flatten(self.layer2)\n        \n        # Use shared parent class to construct graph outputs/objectives\n        self._create_graph_outputs()\n\n\nclass Config:\n    #########################################################################\n    # GENERAL PARAMETERS\n    NORMALIZE_INPUT     = True\n    USE_DROPOUT         = False\n    USE_REGULARIZATION  = True\n    ROBOT_MODE          = True\n    EVALUATE_MODE       = True\n\n    SENSING_HORIZON     = 8.0\n\n    MIN_POLICY = 1e-4\n\n    MAX_NUM_AGENTS_IN_ENVIRONMENT = 20\n    MULTI_AGENT_ARCH = \'RNN\'\n\n    DEVICE                        = \'/cpu:0\' # Device\n\n    HOST_AGENT_OBSERVATION_LENGTH = 4 # dist to goal, heading to goal, pref speed, radius\n    OTHER_AGENT_OBSERVATION_LENGTH = 7 # other px, other py, other vx, other vy, other radius, combined radius, distance between\n    RNN_HELPER_LENGTH = 1 # num other agents\n    AGENT_ID_LENGTH = 1 # id\n    IS_ON_LENGTH = 1 # 0/1 binary flag\n\n    HOST_AGENT_AVG_VECTOR = np.array([0.0, 0.0, 1.0, 0.5]) # dist to goal, heading to goal, pref speed, radius\n    HOST_AGENT_STD_VECTOR = np.array([5.0, 3.14, 1.0, 1.0]) # dist to goal, heading to goal, pref speed, radius\n    OTHER_AGENT_AVG_VECTOR = np.array([0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0]) # other px, other py, other vx, other vy, other radius, combined radius, distance between\n    OTHER_AGENT_STD_VECTOR = np.array([5.0, 5.0, 1.0, 1.0, 1.0, 5.0, 1.0]) # other px, other py, other vx, other vy, other radius, combined radius, distance between\n    RNN_HELPER_AVG_VECTOR = np.array([0.0])\n    RNN_HELPER_STD_VECTOR = np.array([1.0])\n    IS_ON_AVG_VECTOR = np.array([0.0])\n    IS_ON_STD_VECTOR = np.array([1.0])\n\n    if MAX_NUM_AGENTS_IN_ENVIRONMENT > 2:\n        if MULTI_AGENT_ARCH == \'RNN\':\n            # NN input:\n            # [num other agents, dist to goal, heading to goal, pref speed, radius, \n            #   other px, other py, other vx, other vy, other radius, dist btwn, combined radius,\n            #   other px, other py, other vx, other vy, other radius, dist btwn, combined radius,\n            #   other px, other py, other vx, other vy, other radius, dist btwn, combined radius]\n            MAX_NUM_OTHER_AGENTS_OBSERVED = 10\n            OTHER_AGENT_FULL_OBSERVATION_LENGTH = OTHER_AGENT_OBSERVATION_LENGTH\n            HOST_AGENT_STATE_SIZE = HOST_AGENT_OBSERVATION_LENGTH\n            FULL_STATE_LENGTH = RNN_HELPER_LENGTH + HOST_AGENT_OBSERVATION_LENGTH + MAX_NUM_OTHER_AGENTS_OBSERVED * OTHER_AGENT_FULL_OBSERVATION_LENGTH\n            FIRST_STATE_INDEX = 1\n\n            NN_INPUT_AVG_VECTOR = np.hstack([RNN_HELPER_AVG_VECTOR,HOST_AGENT_AVG_VECTOR,np.tile(OTHER_AGENT_AVG_VECTOR,MAX_NUM_OTHER_AGENTS_OBSERVED)])\n            NN_INPUT_STD_VECTOR = np.hstack([RNN_HELPER_STD_VECTOR,HOST_AGENT_STD_VECTOR,np.tile(OTHER_AGENT_STD_VECTOR,MAX_NUM_OTHER_AGENTS_OBSERVED)])\n            \n    FULL_LABELED_STATE_LENGTH = FULL_STATE_LENGTH + AGENT_ID_LENGTH\n    NN_INPUT_SIZE = FULL_STATE_LENGTH\n\n\n\nif __name__ == \'__main__\':\n    actions = Actions().actions\n    num_actions = Actions().num_actions\n    nn = NetworkVP_rnn(Config.DEVICE, \'network\', num_actions)\n    nn.simple_load()\n\n    obs = np.zeros((Config.FULL_STATE_LENGTH))\n    obs = np.expand_dims(obs, axis=0)\n\n    num_queries = 10000\n    t_start = time.time()\n    for i in range(num_queries):\n        obs[0,0] = 10 # num other agents\n        obs[0,1] = np.random.uniform(0.5, 10.0) # dist to goal\n        obs[0,2] = np.random.uniform(-np.pi, np.pi) # heading to goal\n        obs[0,3] = np.random.uniform(0.2, 2.0) # pref speed\n        obs[0,4] = np.random.uniform(0.2, 1.5) # radius\n        predictions = nn.predict_p(obs, None)[0]\n    t_end = time.time()\n    print ""avg query time:"", (t_end - t_start)/num_queries\n    print ""total time:"", t_end - t_start\n    # action = actions[np.argmax(predictions)]\n    # print ""action:"", action\n'"
scripts/util.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as ptch\nfrom network import Config\n\n# angle_1 - angle_2\n# contains direction in range [-3.14, 3.14]\ndef find_angle_diff(angle_1, angle_2):\n    angle_diff_raw = angle_1 - angle_2\n    angle_diff = (angle_diff_raw + np.pi) % (2 * np.pi) - np.pi\n    return angle_diff\n\n# keep angle between [-pi, pi]\ndef wrap(angle):\n    while angle >= np.pi:\n        angle -= 2*np.pi\n    while angle < -np.pi:\n        angle += 2*np.pi\n    return angle\n\ndef find_nearest(array,value):\n    # array is a 1D np array\n    # value is an scalar or 1D np array\n    tiled_value = np.tile(np.expand_dims(value,axis=0).transpose(), (1,np.shape(array)[0]))\n    idx = (np.abs(array-tiled_value)).argmin(axis=1)\n    return array[idx], idx\n\ndef convert_cadrl_state_to_state(cadrl_state):\n    # Convert the legacy cadrl format into this repo\'s state representation format \n    # for the host agent\n    number_examples, cadrl_state_size = np.shape(cadrl_state)\n    # print cadrl_state[0:5,:]\n\n    if Config.MAX_NUM_OTHER_AGENTS_OBSERVED in [3,4]:\n        # From CADRL README.txt\n        # agent_centric_state = 1 x 7 + n x 8 vector\n        #    [dist_to_goal, pref_speed, cur_speed, cur_heading, vx, vy, self_radius, \\\n        #     other_vx, other_vy, rel_pos_x, rel_pos_y, other_radius, self_radius+other_radius, dist_2_other, is_on]\n\n        agent_state = np.zeros([number_examples, Config.FULL_STATE_LENGTH])\n\n        agent_state[:,Config.FIRST_STATE_INDEX+0] = cadrl_state[:,0] # host agent distance to goal\n        agent_state[:,Config.FIRST_STATE_INDEX+1] = cadrl_state[:,3] # host agent heading in ego frame\n        agent_state[:,Config.FIRST_STATE_INDEX+2] = cadrl_state[:,1] # host agent pref_speed\n        agent_state[:,Config.FIRST_STATE_INDEX+3] = cadrl_state[:,6] # host agent radius\n        num_agents_on = np.zeros(number_examples)\n        for i in range(Config.MAX_NUM_OTHER_AGENTS_OBSERVED):\n            is_on_inds = np.where(cadrl_state[:,14+8*i] == 1.0)\n            agent_state[is_on_inds,Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+0+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i] = cadrl_state[is_on_inds,9+8*i] # other agent px\n            agent_state[is_on_inds,Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+1+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i] = cadrl_state[is_on_inds,10+8*i] # other agent py\n            agent_state[is_on_inds,Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+2+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i] = cadrl_state[is_on_inds,7+8*i] # other agent vx\n            agent_state[is_on_inds,Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+3+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i] = cadrl_state[is_on_inds,8+8*i] # other agent vy\n            agent_state[is_on_inds,Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+4+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i] = cadrl_state[is_on_inds,11+8*i] # other agent radius\n            agent_state[is_on_inds,Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+5+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i] = cadrl_state[is_on_inds,12+8*i] # combined radius\n            agent_state[is_on_inds,Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+6+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i] = cadrl_state[is_on_inds,13+8*i] # dist_2_other\n            if Config.MULTI_AGENT_ARCH in [\'WEIGHT_SHARING\', \'VANILLA\']:\n                agent_state[is_on_inds,Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+7+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i] = cadrl_state[is_on_inds,14+8*i] # is_on\n            num_agents_on[is_on_inds] += 1 # keep track of how many agents are ""on"", for the RNN\n        \n        if Config.MULTI_AGENT_ARCH == \'RNN\':\n            agent_state[:,0] = num_agents_on\n\n\n    elif Config.MAX_NUM_OTHER_AGENTS_OBSERVED == 2:\n        agent_state = np.zeros([number_examples, Config.FULL_STATE_LENGTH])\n        print cadrl_state[:3,:]\n        # CADRL README.txt is incorrect......\n        # [dist_to_goal, pref_speed, cur_speed, cur_heading, \\\n        #            other_vx, other_vy, rel_pos_x, rel_pos_y, self_radius, other_radius, self_radius+other_radius, vx, vy, dist_2_other]\n        agent_state[:,0] = cadrl_state[:,0] # host agent distance to goal\n        agent_state[:,1] = cadrl_state[:,5] # host agent heading in ego frame\n        agent_state[:,2] = cadrl_state[:,6] # host agent pref_speed\n        agent_state[:,3] = cadrl_state[:,7] # host agent radius\n        agent_state[:,4:9] = cadrl_state[:,9:14] # other agent px, py, vx, vy in body frame, radius\n        agent_state[:,9:11] = cadrl_state[:,14:16] # combined radius, dist btwn\n\n    else:\n        print ""[regression util.py] invalid number of agents""\n        assert(0)\n\n    return agent_state\n\ndef plot_current_state_ego_frame(state, figure_name=None, axes=None):\n    if axes is None:\n        if figure_name is None:\n            fig = plt.figure(figsize=(15, 6), frameon=False)\n        else:\n            fig = plt.figure(figure_name,figsize=(15, 6), frameon=False)\n            plt.clf()\n        ax = fig.add_subplot(1, 2, 1)\n    else:\n        ax = axes\n\n    plt_colors = []\n    plt_colors.append([0.8500, 0.3250, 0.0980]) # red\n    plt_colors.append([0.0, 0.4470, 0.7410]) # blue \n    plt_colors.append([0.4660, 0.6740, 0.1880]) # green \n    plt_colors.append([0.4940, 0.1840, 0.5560]) # purple\n    plt_colors.append([0.9290, 0.6940, 0.1250]) # orange \n    plt_colors.append([0.3010, 0.7450, 0.9330]) # cyan \n    plt_colors.append([0.6350, 0.0780, 0.1840]) # chocolate \n\n\n    ###############################\n    # state vector\n    ##############################\n    try:\n        state = np.squeeze(state)\n\n        host_dist_to_goal = state[Config.FIRST_STATE_INDEX+0]\n        host_heading = state[Config.FIRST_STATE_INDEX+1]\n        host_pref_speed = state[Config.FIRST_STATE_INDEX+2]\n        host_radius = state[Config.FIRST_STATE_INDEX+3]\n\n        other_pxs = []\n        other_pys = []\n        other_vxs = []\n        other_vys = []\n        other_radii = []\n\n        if Config.MULTI_AGENT_ARCH == \'RNN\':\n            num_others = int(state[0])\n        else:\n            num_others = int(sum([state[Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*(i+1)-1] for i in range(Config.MAX_NUM_OTHER_AGENTS_OBSERVED)]))\n\n        for i in range(num_others):\n            other_pxs.append(state[Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+0+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i])\n            other_pys.append(state[Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+1+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i])\n            other_vxs.append(state[Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+2+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i])\n            other_vys.append(state[Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+3+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i])\n            other_radii.append(state[Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+4+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i])\n\n    except:\n        return\n    ####################################\n\n    plt_colors_local = plt_colors\n    \n    ##############################################################################################\n    # first subfigure\n    # convert to representation that\'s easier to plot\n\n    circ1 = plt.Circle((-host_dist_to_goal, 0.0), radius=host_radius, fc=\'w\', ec=plt_colors_local[0])\n    ax.add_patch(circ1)\n    # goal\n    ax.plot(0.0, 0.0, c=plt_colors_local[0], marker=\'*\', markersize=20)\n\n    wedge = ptch.Wedge([-host_dist_to_goal, 0.0], 1.0, rad2deg(host_heading - np.pi/3), rad2deg(host_heading + np.pi/3), alpha=0.1)\n    ax.add_patch(wedge)\n    heading = ax.plot([-host_dist_to_goal, -host_dist_to_goal + np.cos(host_heading)], [0.0, np.sin(host_heading)], \'k--\')\n\n    # Other Agent\n    for i in range(len(other_pxs)): # plot all agents that are ""ON""\n        # circ = plt.Circle((-host_dist_to_goal + other_px, other_py), radius=0.5, fc=\'w\', ec=plt_colors_local[i+1])\n        circ = plt.Circle((-host_dist_to_goal + other_pxs[i], other_pys[i]), radius=other_radii[i], fc=\'w\', ec=plt_colors_local[i+1])\n        ax.add_patch(circ)\n        # other agent\'s speed\n        ax.arrow(-host_dist_to_goal + other_pxs[i], other_pys[i], other_vxs[i], other_vys[i], fc=plt_colors_local[i+1], \\\n            ec=plt_colors_local[i+1], head_width=0.05, head_length=0.1)\n\n    ax.set_xlabel(\'x (m)\')\n    ax.set_ylabel(\'y (m)\')\n    # plt.legend([vel_rvo, vel_SL], [\'RVO\', \'Selected\'])\n\n    ax.axis(\'equal\')\n    xlim = ax.get_xlim()\n    new_xlim = np.array((xlim[0], xlim[1]+0.5))\n    ax.set_xlim(new_xlim)\n    # plotting style (only show axis on bottom and left)\n    ax.spines[\'top\'].set_visible(False)\n    ax.spines[\'right\'].set_visible(False)\n    ax.yaxis.set_ticks_position(\'left\')\n    ax.xaxis.set_ticks_position(\'bottom\')\n\n    # plt.draw()\n    # plt.pause(0.0001)\n    return\n\ndef plot_snapshot(state, real_action_one_hot, real_value, possible_actions, probs, values, figure_name=None):\n    if figure_name is None:\n        fig = plt.figure(figsize=(15, 6), frameon=False)\n    else:\n        fig = plt.figure(figure_name,figsize=(15, 6), frameon=False)\n        plt.clf()\n\n    plt_colors = []\n    plt_colors.append([0.8500, 0.3250, 0.0980]) # red\n    plt_colors.append([0.0, 0.4470, 0.7410]) # blue \n    plt_colors.append([0.4660, 0.6740, 0.1880]) # green \n    plt_colors.append([0.4940, 0.1840, 0.5560]) # purple\n    plt_colors.append([0.9290, 0.6940, 0.1250]) # orange \n    plt_colors.append([0.3010, 0.7450, 0.9330]) # cyan \n    plt_colors.append([0.6350, 0.0780, 0.1840]) # chocolate \n\n\n    ###############################\n    # state vector\n    ##############################\n    try:\n        state = np.squeeze(state)\n        print state\n\n        host_dist_to_goal = state[Config.FIRST_STATE_INDEX+0]\n        host_heading = state[Config.FIRST_STATE_INDEX+1]\n        host_pref_speed = state[Config.FIRST_STATE_INDEX+2]\n        host_radius = state[Config.FIRST_STATE_INDEX+3]\n\n        other_pxs = []\n        other_pys = []\n        other_vxs = []\n        other_vys = []\n        other_radii = []\n\n        if Config.MULTI_AGENT_ARCH == \'RNN\':\n            num_others = int(state[0])\n        else:\n            num_others = int(sum([state[Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*(i+1)-1] for i in range(Config.MAX_NUM_OTHER_AGENTS_OBSERVED)]))\n\n        for i in range(num_others):\n            other_pxs.append(state[Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+0+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i])\n            other_pys.append(state[Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+1+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i])\n            other_vxs.append(state[Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+2+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i])\n            other_vys.append(state[Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+3+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i])\n            other_radii.append(state[Config.FIRST_STATE_INDEX+Config.HOST_AGENT_STATE_SIZE+4+Config.OTHER_AGENT_FULL_OBSERVATION_LENGTH*i])\n\n    except:\n        return\n    ####################################\n\n\n    plt_colors_local = plt_colors\n    \n    ##############################################################################################\n    # first subfigure\n    # convert to representation that\'s easier to plot\n    ax = fig.add_subplot(1, 2, 1)\n\n    circ1 = plt.Circle((-host_dist_to_goal, 0.0), radius=host_radius, fc=\'w\', ec=plt_colors_local[0])\n    ax.add_patch(circ1)\n    # goal\n    plt.plot(0.0, 0.0, c=plt_colors_local[0], marker=\'*\', markersize=20)\n\n    # find and plot best action \n    selected_action_ind = np.argmax(probs)\n    selected_action = possible_actions[selected_action_ind]\n    x_tmp = selected_action[0] * np.cos(selected_action[1]+host_heading) \n    y_tmp = selected_action[0] * np.sin(selected_action[1]+host_heading)\n    vel_SL = plt.arrow(-host_dist_to_goal, 0.0, x_tmp, y_tmp, fc=\'g\',\\\n        ec=\'g\', head_width=0.05, head_length=0.1)\n\n    if real_action_one_hot is not None:\n        real_action_ind = np.argmax(real_action_one_hot)\n        real_action = possible_actions[real_action_ind,:]\n        x_SL = real_action[0] * np.cos(real_action[1]+host_heading) \n        y_SL = real_action[0] * np.sin(real_action[1]+host_heading)\n        vel_rvo = plt.arrow(-host_dist_to_goal, 0.0, x_SL, y_SL, fc=\'y\',\\\n            ec=\'y\', head_width=0.05, head_length=0.1)\n\n    wedge = ptch.Wedge([-host_dist_to_goal, 0.0], 1.0, rad2deg(host_heading - np.pi/3), rad2deg(host_heading + np.pi/3), alpha=0.1)\n    ax.add_patch(wedge)\n    heading = plt.plot([-host_dist_to_goal, -host_dist_to_goal + np.cos(host_heading)], [0.0, np.sin(host_heading)], \'k--\')\n\n\n    # Other Agent\n    for i in range(len(other_pxs)): # plot all agents that are ""ON""\n        # circ = plt.Circle((-host_dist_to_goal + other_px, other_py), radius=0.5, fc=\'w\', ec=plt_colors_local[i+1])\n        circ = plt.Circle((-host_dist_to_goal + other_pxs[i], other_pys[i]), radius=other_radii[i], fc=\'w\', ec=plt_colors_local[i+1])\n        ax.add_patch(circ)\n        # other agent\'s speed\n        plt.arrow(-host_dist_to_goal + other_pxs[i], other_pys[i], other_vxs[i], other_vys[i], fc=plt_colors_local[i+1], \\\n            ec=plt_colors_local[i+1], head_width=0.05, head_length=0.1)\n        \n\n\n    plt.xlabel(\'x (m)\')\n    plt.ylabel(\'y (m)\')\n    plt.legend([vel_rvo, vel_SL], [\'RVO\', \'Selected\'])\n\n    ax.axis(\'equal\')\n    xlim = ax.get_xlim()\n    new_xlim = np.array((xlim[0], xlim[1]+0.5))\n    ax.set_xlim(new_xlim)\n    # plotting style (only show axis on bottom and left)\n    ax.spines[\'top\'].set_visible(False)\n    ax.spines[\'right\'].set_visible(False)\n    ax.yaxis.set_ticks_position(\'left\')\n    ax.xaxis.set_ticks_position(\'bottom\')\n\n    ##############################################################################################\n    # second subfigure\n    ax = fig.add_subplot(1, 2, 2)\n    \n    plot_x = host_pref_speed*possible_actions[:,0] * np.cos(possible_actions[:,1]+host_heading)\n    plot_y = host_pref_speed*possible_actions[:,0] * np.sin(possible_actions[:,1]+host_heading)\n    plot_z = np.squeeze(probs)\n\n    # Add dashed line between (0,0) and max speed forward\n    plt.plot([0.0, host_pref_speed*np.cos(host_heading)],[0.0, host_pref_speed*np.sin(host_heading)],\'k--\')\n    \n    \'\'\' plot using tripcolor (2D plot) \'\'\'\n    # triang = tri.Triangulation(plot_x, plot_y)\n    # color_min_inds = np.where(plot_z>0)[0]\n    # if len(color_min_inds) > 0:\n    #     color_min = np.amin(plot_z[color_min_inds]) - 0.05\n    # else:\n    #     color_min = 0.0\n    # color_max = max(np.amax(plot_z),0.0)\n    color_min = 0.0\n    color_max = 1.0\n    # plt.tripcolor(plot_x, plot_y, plot_z, shading=\'flat\', \\\n    #       cmap=plt.cm.rainbow, edgecolors=\'k\',vmin=color_min, vmax=color_max)\n    # plot_x = np.hstack((plot_x, plot_x, plot_x+0.05))\n    # plot_y = np.hstack((plot_y, plot_y+0.05, plot_y))\n    # plot_z = np.hstack((plot_z, plot_z, plot_z))\n    # plt.tripcolor(plot_x, plot_y, plot_z, shading=\'flat\', \\\n    #       cmap=plt.cm.rainbow, vmin=color_min, vmax=color_max)\n    plt.scatter(plot_x, plot_y, marker=\'+\', s=1000, linewidths=4, c=plot_z, cmap=plt.cm.rainbow, vmin=color_min, vmax=color_max)\n    for i, txt in enumerate(plot_z):\n        ax.annotate(round(txt,3), (plot_x[i], plot_y[i]))\n    plt.title(\'True value: %.3f, NN value: %.3f\' % (real_value, values[0]))\n    plt.xlabel(\'v_x (m/s)\')\n    plt.ylabel(\'v_y (m/s)\')\n    cbar = plt.colorbar()\n    cbar.set_ticks([color_min,(color_min+color_max)/2.0,color_max])\n    cbar.ax.set_yticklabels([\'%.3f\'%color_min, \\\n                        \'%.3f\'%((color_min+color_max)/2.0), \\\n                        \'%.3f\'%color_max])\n\n    # plotting style (only show axis on bottom and left)\n    ax.spines[\'top\'].set_visible(False)\n    ax.spines[\'right\'].set_visible(False)\n    ax.yaxis.set_ticks_position(\'left\')\n    ax.xaxis.set_ticks_position(\'bottom\')\n\n    plt.draw()\n    plt.pause(0.0001)\n    # raw_input()\n\ndef rad2deg(rad):\n    return rad*180/np.pi\n\ndef rgba2rgb(rgba):\n    # rgba is a list of 4 color elements btwn [0.0, 1.0]\n    # returns a list of rgb values between [0.0, 1.0] accounting for alpha and background color [1, 1, 1] == WHITE\n    alpha = rgba[3]\n    r = max(min((1 - alpha) * 1.0 + alpha * rgba[0],1.0),0.0)\n    g = max(min((1 - alpha) * 1.0 + alpha * rgba[1],1.0),0.0)\n    b = max(min((1 - alpha) * 1.0 + alpha * rgba[2],1.0),0.0)\n    return [r,g,b]'"
