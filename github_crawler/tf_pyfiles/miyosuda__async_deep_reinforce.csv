file_path,api_count,code
a3c.py,9,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport threading\nimport numpy as np\n\nimport signal\nimport random\nimport math\nimport os\nimport time\n\nfrom game_ac_network import GameACFFNetwork, GameACLSTMNetwork\nfrom a3c_training_thread import A3CTrainingThread\nfrom rmsprop_applier import RMSPropApplier\n\nfrom constants import ACTION_SIZE\nfrom constants import PARALLEL_SIZE\nfrom constants import INITIAL_ALPHA_LOW\nfrom constants import INITIAL_ALPHA_HIGH\nfrom constants import INITIAL_ALPHA_LOG_RATE\nfrom constants import MAX_TIME_STEP\nfrom constants import CHECKPOINT_DIR\nfrom constants import LOG_FILE\nfrom constants import RMSP_EPSILON\nfrom constants import RMSP_ALPHA\nfrom constants import GRAD_NORM_CLIP\nfrom constants import USE_GPU\nfrom constants import USE_LSTM\n\n\ndef log_uniform(lo, hi, rate):\n  log_lo = math.log(lo)\n  log_hi = math.log(hi)\n  v = log_lo * (1-rate) + log_hi * rate\n  return math.exp(v)\n\ndevice = ""/cpu:0""\nif USE_GPU:\n  device = ""/gpu:0""\n\ninitial_learning_rate = log_uniform(INITIAL_ALPHA_LOW,\n                                    INITIAL_ALPHA_HIGH,\n                                    INITIAL_ALPHA_LOG_RATE)\n\nglobal_t = 0\n\nstop_requested = False\n\nif USE_LSTM:\n  global_network = GameACLSTMNetwork(ACTION_SIZE, -1, device)\nelse:\n  global_network = GameACFFNetwork(ACTION_SIZE, -1, device)\n\n\ntraining_threads = []\n\nlearning_rate_input = tf.placeholder(""float"")\n\ngrad_applier = RMSPropApplier(learning_rate = learning_rate_input,\n                              decay = RMSP_ALPHA,\n                              momentum = 0.0,\n                              epsilon = RMSP_EPSILON,\n                              clip_norm = GRAD_NORM_CLIP,\n                              device = device)\n\nfor i in range(PARALLEL_SIZE):\n  training_thread = A3CTrainingThread(i, global_network, initial_learning_rate,\n                                      learning_rate_input,\n                                      grad_applier, MAX_TIME_STEP,\n                                      device = device)\n  training_threads.append(training_thread)\n\n# prepare session\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n                                        allow_soft_placement=True))\n\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# summary for tensorboard\nscore_input = tf.placeholder(tf.int32)\ntf.summary.scalar(""score"", score_input)\n\nsummary_op = tf.summary.merge_all()\nsummary_writer = tf.summary.FileWriter(LOG_FILE, sess.graph)\n\n# init or load checkpoint with saver\nsaver = tf.train.Saver()\ncheckpoint = tf.train.get_checkpoint_state(CHECKPOINT_DIR)\nif checkpoint and checkpoint.model_checkpoint_path:\n  saver.restore(sess, checkpoint.model_checkpoint_path)\n  print(""checkpoint loaded:"", checkpoint.model_checkpoint_path)\n  tokens = checkpoint.model_checkpoint_path.split(""-"")\n  # set global step\n  global_t = int(tokens[1])\n  print("">>> global step set: "", global_t)\n  # set wall time\n  wall_t_fname = CHECKPOINT_DIR + \'/\' + \'wall_t.\' + str(global_t)\n  with open(wall_t_fname, \'r\') as f:\n    wall_t = float(f.read())\nelse:\n  print(""Could not find old checkpoint"")\n  # set wall time\n  wall_t = 0.0\n\n\ndef train_function(parallel_index):\n  global global_t\n  \n  training_thread = training_threads[parallel_index]\n  # set start_time\n  start_time = time.time() - wall_t\n  training_thread.set_start_time(start_time)\n\n  while True:\n    if stop_requested:\n      break\n    if global_t > MAX_TIME_STEP:\n      break\n\n    diff_global_t = training_thread.process(sess, global_t, summary_writer,\n                                            summary_op, score_input)\n    global_t += diff_global_t\n    \n    \ndef signal_handler(signal, frame):\n  global stop_requested\n  print(\'You pressed Ctrl+C!\')\n  stop_requested = True\n  \ntrain_threads = []\nfor i in range(PARALLEL_SIZE):\n  train_threads.append(threading.Thread(target=train_function, args=(i,)))\n  \nsignal.signal(signal.SIGINT, signal_handler)\n\n# set start time\nstart_time = time.time() - wall_t\n\nfor t in train_threads:\n  t.start()\n\nprint(\'Press Ctrl+C to stop\')\nsignal.pause()\n\nprint(\'Now saving data. Please wait\')\n  \nfor t in train_threads:\n  t.join()\n\nif not os.path.exists(CHECKPOINT_DIR):\n  os.mkdir(CHECKPOINT_DIR)  \n\n# write wall time\nwall_t = time.time() - start_time\nwall_t_fname = CHECKPOINT_DIR + \'/\' + \'wall_t.\' + str(global_t)\nwith open(wall_t_fname, \'w\') as f:\n  f.write(str(wall_t))\n\nsaver.save(sess, CHECKPOINT_DIR + \'/\' + \'checkpoint\', global_step = global_t)\n\n'"
a3c_display.py,5,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport numpy as np\nimport random\n\nfrom game_state import GameState\nfrom game_ac_network import GameACFFNetwork, GameACLSTMNetwork\nfrom a3c_training_thread import A3CTrainingThread\nfrom rmsprop_applier import RMSPropApplier\n\nfrom constants import ACTION_SIZE\nfrom constants import PARALLEL_SIZE\nfrom constants import CHECKPOINT_DIR\nfrom constants import RMSP_EPSILON\nfrom constants import RMSP_ALPHA\nfrom constants import GRAD_NORM_CLIP\nfrom constants import USE_GPU\nfrom constants import USE_LSTM\n\ndef choose_action(pi_values):\n  return np.random.choice(range(len(pi_values)), p=pi_values)  \n\n# use CPU for display tool\ndevice = ""/cpu:0""\n\nif USE_LSTM:\n  global_network = GameACLSTMNetwork(ACTION_SIZE, -1, device)\nelse:\n  global_network = GameACFFNetwork(ACTION_SIZE, -1, device)\n\nlearning_rate_input = tf.placeholder(""float"")\n\ngrad_applier = RMSPropApplier(learning_rate = learning_rate_input,\n                              decay = RMSP_ALPHA,\n                              momentum = 0.0,\n                              epsilon = RMSP_EPSILON,\n                              clip_norm = GRAD_NORM_CLIP,\n                              device = device)\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nsaver = tf.train.Saver()\ncheckpoint = tf.train.get_checkpoint_state(CHECKPOINT_DIR)\nif checkpoint and checkpoint.model_checkpoint_path:\n  saver.restore(sess, checkpoint.model_checkpoint_path)\n  print(""checkpoint loaded:"", checkpoint.model_checkpoint_path)\nelse:\n  print(""Could not find old checkpoint"")\n\ngame_state = GameState(0, display=True, no_op_max=0)\n\nwhile True:\n  pi_values = global_network.run_policy(sess, game_state.s_t)\n\n  action = choose_action(pi_values)\n  game_state.process(action)\n\n  if game_state.terminal:\n    game_state.reset()\n  else:\n    game_state.update()\n'"
a3c_training_thread.py,2,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport numpy as np\nimport random\nimport time\nimport sys\n\nfrom game_state import GameState\nfrom game_state import ACTION_SIZE\nfrom game_ac_network import GameACFFNetwork, GameACLSTMNetwork\n\nfrom constants import GAMMA\nfrom constants import LOCAL_T_MAX\nfrom constants import ENTROPY_BETA\nfrom constants import USE_LSTM\n\nLOG_INTERVAL = 100\nPERFORMANCE_LOG_INTERVAL = 1000\n\nclass A3CTrainingThread(object):\n  def __init__(self,\n               thread_index,\n               global_network,\n               initial_learning_rate,\n               learning_rate_input,\n               grad_applier,\n               max_global_time_step,\n               device):\n\n    self.thread_index = thread_index\n    self.learning_rate_input = learning_rate_input\n    self.max_global_time_step = max_global_time_step\n\n    if USE_LSTM:\n      self.local_network = GameACLSTMNetwork(ACTION_SIZE, thread_index, device)\n    else:\n      self.local_network = GameACFFNetwork(ACTION_SIZE, thread_index, device)\n\n    self.local_network.prepare_loss(ENTROPY_BETA)\n\n    with tf.device(device):\n      var_refs = [v._ref() for v in self.local_network.get_vars()]\n      self.gradients = tf.gradients(\n        self.local_network.total_loss, var_refs,\n        gate_gradients=False,\n        aggregation_method=None,\n        colocate_gradients_with_ops=False)\n\n    self.apply_gradients = grad_applier.apply_gradients(\n      global_network.get_vars(),\n      self.gradients )\n      \n    self.sync = self.local_network.sync_from(global_network)\n    \n    self.game_state = GameState(113 * thread_index)\n    \n    self.local_t = 0\n\n    self.initial_learning_rate = initial_learning_rate\n\n    self.episode_reward = 0\n\n    # variable controling log output\n    self.prev_local_t = 0\n\n  def _anneal_learning_rate(self, global_time_step):\n    learning_rate = self.initial_learning_rate * (self.max_global_time_step - global_time_step) / self.max_global_time_step\n    if learning_rate < 0.0:\n      learning_rate = 0.0\n    return learning_rate\n\n  def choose_action(self, pi_values):\n    return np.random.choice(range(len(pi_values)), p=pi_values)\n\n  def _record_score(self, sess, summary_writer, summary_op, score_input, score, global_t):\n    summary_str = sess.run(summary_op, feed_dict={\n      score_input: score\n    })\n    summary_writer.add_summary(summary_str, global_t)\n    summary_writer.flush()\n    \n  def set_start_time(self, start_time):\n    self.start_time = start_time\n\n  def process(self, sess, global_t, summary_writer, summary_op, score_input):\n    states = []\n    actions = []\n    rewards = []\n    values = []\n\n    terminal_end = False\n\n    # copy weights from shared to local\n    sess.run( self.sync )\n\n    start_local_t = self.local_t\n\n    if USE_LSTM:\n      start_lstm_state = self.local_network.lstm_state_out\n    \n    # t_max times loop\n    for i in range(LOCAL_T_MAX):\n      pi_, value_ = self.local_network.run_policy_and_value(sess, self.game_state.s_t)\n      action = self.choose_action(pi_)\n\n      states.append(self.game_state.s_t)\n      actions.append(action)\n      values.append(value_)\n\n      if (self.thread_index == 0) and (self.local_t % LOG_INTERVAL == 0):\n        print(""pi={}"".format(pi_))\n        print("" V={}"".format(value_))\n\n      # process game\n      self.game_state.process(action)\n\n      # receive game result\n      reward = self.game_state.reward\n      terminal = self.game_state.terminal\n\n      self.episode_reward += reward\n\n      # clip reward\n      rewards.append( np.clip(reward, -1, 1) )\n\n      self.local_t += 1\n\n      # s_t1 -> s_t\n      self.game_state.update()\n      \n      if terminal:\n        terminal_end = True\n        print(""score={}"".format(self.episode_reward))\n\n        self._record_score(sess, summary_writer, summary_op, score_input,\n                           self.episode_reward, global_t)\n          \n        self.episode_reward = 0\n        self.game_state.reset()\n        if USE_LSTM:\n          self.local_network.reset_state()\n        break\n\n    R = 0.0\n    if not terminal_end:\n      R = self.local_network.run_value(sess, self.game_state.s_t)\n\n    actions.reverse()\n    states.reverse()\n    rewards.reverse()\n    values.reverse()\n\n    batch_si = []\n    batch_a = []\n    batch_td = []\n    batch_R = []\n\n    # compute and accmulate gradients\n    for(ai, ri, si, Vi) in zip(actions, rewards, states, values):\n      R = ri + GAMMA * R\n      td = R - Vi\n      a = np.zeros([ACTION_SIZE])\n      a[ai] = 1\n\n      batch_si.append(si)\n      batch_a.append(a)\n      batch_td.append(td)\n      batch_R.append(R)\n\n    cur_learning_rate = self._anneal_learning_rate(global_t)\n\n    if USE_LSTM:\n      batch_si.reverse()\n      batch_a.reverse()\n      batch_td.reverse()\n      batch_R.reverse()\n\n      sess.run( self.apply_gradients,\n                feed_dict = {\n                  self.local_network.s: batch_si,\n                  self.local_network.a: batch_a,\n                  self.local_network.td: batch_td,\n                  self.local_network.r: batch_R,\n                  self.local_network.initial_lstm_state: start_lstm_state,\n                  self.local_network.step_size : [len(batch_a)],\n                  self.learning_rate_input: cur_learning_rate } )\n    else:\n      sess.run( self.apply_gradients,\n                feed_dict = {\n                  self.local_network.s: batch_si,\n                  self.local_network.a: batch_a,\n                  self.local_network.td: batch_td,\n                  self.local_network.r: batch_R,\n                  self.learning_rate_input: cur_learning_rate} )\n      \n    if (self.thread_index == 0) and (self.local_t - self.prev_local_t >= PERFORMANCE_LOG_INTERVAL):\n      self.prev_local_t += PERFORMANCE_LOG_INTERVAL\n      elapsed_time = time.time() - self.start_time\n      steps_per_sec = global_t / elapsed_time\n      print(""### Performance : {} STEPS in {:.0f} sec. {:.0f} STEPS/sec. {:.2f}M STEPS/hour"".format(\n        global_t,  elapsed_time, steps_per_sec, steps_per_sec * 3600 / 1000000.))\n\n    # return advanced local step size\n    diff_local_t = self.local_t - start_local_t\n    return diff_local_t\n    \n'"
a3c_visualize.py,5,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport sys\nimport matplotlib.pyplot as plt\nimport random\n\nfrom game_state import GameState\nfrom game_ac_network import GameACFFNetwork, GameACLSTMNetwork\nfrom a3c_training_thread import A3CTrainingThread\nfrom rmsprop_applier import RMSPropApplier\n\nfrom constants import ACTION_SIZE\nfrom constants import PARALLEL_SIZE\nfrom constants import MAX_TIME_STEP\nfrom constants import CHECKPOINT_DIR\nfrom constants import RMSP_EPSILON\nfrom constants import RMSP_ALPHA\nfrom constants import GRAD_NORM_CLIP\nfrom constants import USE_GPU\nfrom constants import USE_LSTM\n\n# use CPU for weight visualize tool\ndevice = ""/cpu:0""\n\nif USE_LSTM:\n  global_network = GameACLSTMNetwork(ACTION_SIZE, -1, device)\nelse:\n  global_network = GameACFFNetwork(ACTION_SIZE, -1, device)\n\ntraining_threads = []\n\nlearning_rate_input = tf.placeholder(""float"")\n\ngrad_applier = RMSPropApplier(learning_rate = learning_rate_input,\n                              decay = RMSP_ALPHA,\n                              momentum = 0.0,\n                              epsilon = RMSP_EPSILON,\n                              clip_norm = GRAD_NORM_CLIP,\n                              device = device)\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nsaver = tf.train.Saver()\ncheckpoint = tf.train.get_checkpoint_state(CHECKPOINT_DIR)\nif checkpoint and checkpoint.model_checkpoint_path:\n  saver.restore(sess, checkpoint.model_checkpoint_path)\n  print(""checkpoint loaded:"", checkpoint.model_checkpoint_path)\nelse:\n  print(""Could not find old checkpoint"")\n  \nW_conv1 = sess.run(global_network.W_conv1)\n\n# show graph of W_conv1\nfig, axes = plt.subplots(4, 16, figsize=(12, 6),\n             subplot_kw={\'xticks\': [], \'yticks\': []})\nfig.subplots_adjust(hspace=0.1, wspace=0.1)\n\nfor ax,i in zip(axes.flat, range(4*16)):\n  inch = i//16\n  outch = i%16\n  img = W_conv1[:,:,inch,outch]\n  ax.imshow(img, cmap=plt.cm.gray, interpolation=\'nearest\')\n  ax.set_title(str(inch) + "","" + str(outch))\n\nplt.show()\n\n'"
constants.py,0,"b'# -*- coding: utf-8 -*-\n\nLOCAL_T_MAX = 20 # repeat step size\nRMSP_ALPHA = 0.99 # decay parameter for RMSProp\nRMSP_EPSILON = 0.1 # epsilon parameter for RMSProp\nCHECKPOINT_DIR = \'checkpoints\'\nLOG_FILE = \'tmp/a3c_log\'\nINITIAL_ALPHA_LOW = 1e-4    # log_uniform low limit for learning rate\nINITIAL_ALPHA_HIGH = 1e-2   # log_uniform high limit for learning rate\n\nPARALLEL_SIZE = 8 # parallel thread size\nROM = ""pong.bin""     # action size = 3\nACTION_SIZE = 3 # action size\n\nINITIAL_ALPHA_LOG_RATE = 0.4226 # log_uniform interpolate rate for learning rate (around 7 * 10^-4)\nGAMMA = 0.99 # discount factor for rewards\nENTROPY_BETA = 0.01 # entropy regurarlization constant\nMAX_TIME_STEP = 10 * 10**7\nGRAD_NORM_CLIP = 40.0 # gradient norm clipping\nUSE_GPU = True # To use GPU, set True\nUSE_LSTM = True # True for A3C LSTM, False for A3C FF\n'"
game_ac_network.py,46,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport numpy as np\n\n# Actor-Critic Network Base Class\n# (Policy network and Value network)\nclass GameACNetwork(object):\n  def __init__(self,\n               action_size,\n               thread_index, # -1 for global               \n               device=""/cpu:0""):\n    self._action_size = action_size\n    self._thread_index = thread_index\n    self._device = device    \n\n  def prepare_loss(self, entropy_beta):\n    with tf.device(self._device):\n      # taken action (input for policy)\n      self.a = tf.placeholder(""float"", [None, self._action_size])\n    \n      # temporary difference (R-V) (input for policy)\n      self.td = tf.placeholder(""float"", [None])\n\n      # avoid NaN with clipping when value in pi becomes zero\n      log_pi = tf.log(tf.clip_by_value(self.pi, 1e-20, 1.0))\n      \n      # policy entropy\n      entropy = -tf.reduce_sum(self.pi * log_pi, reduction_indices=1)\n      \n      # policy loss (output)  (Adding minus, because the original paper\'s objective function is for gradient ascent, but we use gradient descent optimizer.)\n      policy_loss = - tf.reduce_sum( tf.reduce_sum( tf.multiply( log_pi, self.a ), reduction_indices=1 ) * self.td + entropy * entropy_beta )\n\n      # R (input for value)\n      self.r = tf.placeholder(""float"", [None])\n      \n      # value loss (output)\n      # (Learning rate for Critic is half of Actor\'s, so multiply by 0.5)\n      value_loss = 0.5 * tf.nn.l2_loss(self.r - self.v)\n\n      # gradienet of policy and value are summed up\n      self.total_loss = policy_loss + value_loss\n\n  def run_policy_and_value(self, sess, s_t):\n    raise NotImplementedError()\n    \n  def run_policy(self, sess, s_t):\n    raise NotImplementedError()\n\n  def run_value(self, sess, s_t):\n    raise NotImplementedError()    \n\n  def get_vars(self):\n    raise NotImplementedError()\n\n  def sync_from(self, src_netowrk, name=None):\n    src_vars = src_netowrk.get_vars()\n    dst_vars = self.get_vars()\n\n    sync_ops = []\n\n    with tf.device(self._device):\n      with tf.name_scope(name, ""GameACNetwork"", []) as name:\n        for(src_var, dst_var) in zip(src_vars, dst_vars):\n          sync_op = tf.assign(dst_var, src_var)\n          sync_ops.append(sync_op)\n\n        return tf.group(*sync_ops, name=name)\n\n  # weight initialization based on muupan\'s code\n  # https://github.com/muupan/async-rl/blob/master/a3c_ale.py\n  def _fc_variable(self, weight_shape):\n    input_channels  = weight_shape[0]\n    output_channels = weight_shape[1]\n    d = 1.0 / np.sqrt(input_channels)\n    bias_shape = [output_channels]\n    weight = tf.Variable(tf.random_uniform(weight_shape, minval=-d, maxval=d))\n    bias   = tf.Variable(tf.random_uniform(bias_shape,   minval=-d, maxval=d))\n    return weight, bias\n\n  def _conv_variable(self, weight_shape):\n    w = weight_shape[0]\n    h = weight_shape[1]\n    input_channels  = weight_shape[2]\n    output_channels = weight_shape[3]\n    d = 1.0 / np.sqrt(input_channels * w * h)\n    bias_shape = [output_channels]\n    weight = tf.Variable(tf.random_uniform(weight_shape, minval=-d, maxval=d))\n    bias   = tf.Variable(tf.random_uniform(bias_shape,   minval=-d, maxval=d))\n    return weight, bias\n\n  def _conv2d(self, x, W, stride):\n    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = ""VALID"")\n\n# Actor-Critic FF Network\nclass GameACFFNetwork(GameACNetwork):\n  def __init__(self,\n               action_size,\n               thread_index, # -1 for global\n               device=""/cpu:0""):\n    GameACNetwork.__init__(self, action_size, thread_index, device)\n\n    scope_name = ""net_"" + str(self._thread_index)\n    with tf.device(self._device), tf.variable_scope(scope_name) as scope:\n      self.W_conv1, self.b_conv1 = self._conv_variable([8, 8, 4, 16])  # stride=4\n      self.W_conv2, self.b_conv2 = self._conv_variable([4, 4, 16, 32]) # stride=2\n\n      self.W_fc1, self.b_fc1 = self._fc_variable([2592, 256])\n\n      # weight for policy output layer\n      self.W_fc2, self.b_fc2 = self._fc_variable([256, action_size])\n\n      # weight for value output layer\n      self.W_fc3, self.b_fc3 = self._fc_variable([256, 1])\n\n      # state (input)\n      self.s = tf.placeholder(""float"", [None, 84, 84, 4])\n    \n      h_conv1 = tf.nn.relu(self._conv2d(self.s,  self.W_conv1, 4) + self.b_conv1)\n      h_conv2 = tf.nn.relu(self._conv2d(h_conv1, self.W_conv2, 2) + self.b_conv2)\n\n      h_conv2_flat = tf.reshape(h_conv2, [-1, 2592])\n      h_fc1 = tf.nn.relu(tf.matmul(h_conv2_flat, self.W_fc1) + self.b_fc1)\n\n      # policy (output)\n      self.pi = tf.nn.softmax(tf.matmul(h_fc1, self.W_fc2) + self.b_fc2)\n      # value (output)\n      v_ = tf.matmul(h_fc1, self.W_fc3) + self.b_fc3\n      self.v = tf.reshape( v_, [-1] )\n\n  def run_policy_and_value(self, sess, s_t):\n    pi_out, v_out = sess.run( [self.pi, self.v], feed_dict = {self.s : [s_t]} )\n    return (pi_out[0], v_out[0])\n\n  def run_policy(self, sess, s_t):\n    pi_out = sess.run( self.pi, feed_dict = {self.s : [s_t]} )\n    return pi_out[0]\n\n  def run_value(self, sess, s_t):\n    v_out = sess.run( self.v, feed_dict = {self.s : [s_t]} )\n    return v_out[0]\n\n  def get_vars(self):\n    return [self.W_conv1, self.b_conv1,\n            self.W_conv2, self.b_conv2,\n            self.W_fc1, self.b_fc1,\n            self.W_fc2, self.b_fc2,\n            self.W_fc3, self.b_fc3]\n\n# Actor-Critic LSTM Network\nclass GameACLSTMNetwork(GameACNetwork):\n  def __init__(self,\n               action_size,\n               thread_index, # -1 for global\n               device=""/cpu:0"" ):\n    GameACNetwork.__init__(self, action_size, thread_index, device)\n\n    scope_name = ""net_"" + str(self._thread_index)\n    with tf.device(self._device), tf.variable_scope(scope_name) as scope:\n      self.W_conv1, self.b_conv1 = self._conv_variable([8, 8, 4, 16])  # stride=4\n      self.W_conv2, self.b_conv2 = self._conv_variable([4, 4, 16, 32]) # stride=2\n      \n      self.W_fc1, self.b_fc1 = self._fc_variable([2592, 256])\n\n      # lstm\n      self.lstm = tf.contrib.rnn.BasicLSTMCell(256, state_is_tuple=True)\n\n      # weight for policy output layer\n      self.W_fc2, self.b_fc2 = self._fc_variable([256, action_size])\n\n      # weight for value output layer\n      self.W_fc3, self.b_fc3 = self._fc_variable([256, 1])\n\n      # state (input)\n      self.s = tf.placeholder(""float"", [None, 84, 84, 4])\n    \n      h_conv1 = tf.nn.relu(self._conv2d(self.s,  self.W_conv1, 4) + self.b_conv1)\n      h_conv2 = tf.nn.relu(self._conv2d(h_conv1, self.W_conv2, 2) + self.b_conv2)\n\n      h_conv2_flat = tf.reshape(h_conv2, [-1, 2592])\n      h_fc1 = tf.nn.relu(tf.matmul(h_conv2_flat, self.W_fc1) + self.b_fc1)\n      # h_fc1 shape=(5,256)\n\n      h_fc1_reshaped = tf.reshape(h_fc1, [1,-1,256])\n      # h_fc_reshaped = (1,5,256)\n\n      # place holder for LSTM unrolling time step size.\n      self.step_size = tf.placeholder(tf.float32, [1])\n\n      self.initial_lstm_state0 = tf.placeholder(tf.float32, [1, 256])\n      self.initial_lstm_state1 = tf.placeholder(tf.float32, [1, 256])\n      self.initial_lstm_state = tf.contrib.rnn.LSTMStateTuple(self.initial_lstm_state0,\n                                                              self.initial_lstm_state1)\n      \n      # Unrolling LSTM up to LOCAL_T_MAX time steps. (= 5time steps.)\n      # When episode terminates unrolling time steps becomes less than LOCAL_TIME_STEP.\n      # Unrolling step size is applied via self.step_size placeholder.\n      # When forward propagating, step_size is 1.\n      # (time_major = False, so output shape is [batch_size, max_time, cell.output_size])\n      lstm_outputs, self.lstm_state = tf.nn.dynamic_rnn(self.lstm,\n                                                        h_fc1_reshaped,\n                                                        initial_state = self.initial_lstm_state,\n                                                        sequence_length = self.step_size,\n                                                        time_major = False,\n                                                        scope = scope)\n\n      # lstm_outputs: (1,5,256) for back prop, (1,1,256) for forward prop.\n      \n      lstm_outputs = tf.reshape(lstm_outputs, [-1,256])\n\n      # policy (output)\n      self.pi = tf.nn.softmax(tf.matmul(lstm_outputs, self.W_fc2) + self.b_fc2)\n      \n      # value (output)\n      v_ = tf.matmul(lstm_outputs, self.W_fc3) + self.b_fc3\n      self.v = tf.reshape( v_, [-1] )\n\n      scope.reuse_variables()\n      self.W_lstm = tf.get_variable(""basic_lstm_cell/weights"")\n      self.b_lstm = tf.get_variable(""basic_lstm_cell/biases"")\n\n      self.reset_state()\n      \n  def reset_state(self):\n    self.lstm_state_out = tf.contrib.rnn.LSTMStateTuple(np.zeros([1, 256]),\n                                                        np.zeros([1, 256]))\n\n  def run_policy_and_value(self, sess, s_t):\n    # This run_policy_and_value() is used when forward propagating.\n    # so the step size is 1.\n    pi_out, v_out, self.lstm_state_out = sess.run( [self.pi, self.v, self.lstm_state],\n                                                   feed_dict = {self.s : [s_t],\n                                                                self.initial_lstm_state0 : self.lstm_state_out[0],\n                                                                self.initial_lstm_state1 : self.lstm_state_out[1],\n                                                                self.step_size : [1]} )\n    # pi_out: (1,3), v_out: (1)\n    return (pi_out[0], v_out[0])\n\n  def run_policy(self, sess, s_t):\n    # This run_policy() is used for displaying the result with display tool.    \n    pi_out, self.lstm_state_out = sess.run( [self.pi, self.lstm_state],\n                                            feed_dict = {self.s : [s_t],\n                                                         self.initial_lstm_state0 : self.lstm_state_out[0],\n                                                         self.initial_lstm_state1 : self.lstm_state_out[1],\n                                                         self.step_size : [1]} )\n                                            \n    return pi_out[0]\n\n  def run_value(self, sess, s_t):\n    # This run_value() is used for calculating V for bootstrapping at the \n    # end of LOCAL_T_MAX time step sequence.\n    # When next sequcen starts, V will be calculated again with the same state using updated network weights,\n    # so we don\'t update LSTM state here.\n    prev_lstm_state_out = self.lstm_state_out\n    v_out, _ = sess.run( [self.v, self.lstm_state],\n                         feed_dict = {self.s : [s_t],\n                                      self.initial_lstm_state0 : self.lstm_state_out[0],\n                                      self.initial_lstm_state1 : self.lstm_state_out[1],\n                                      self.step_size : [1]} )\n    \n    # roll back lstm state\n    self.lstm_state_out = prev_lstm_state_out\n    return v_out[0]\n\n  def get_vars(self):\n    return [self.W_conv1, self.b_conv1,\n            self.W_conv2, self.b_conv2,\n            self.W_fc1, self.b_fc1,\n            self.W_lstm, self.b_lstm,\n            self.W_fc2, self.b_fc2,\n            self.W_fc3, self.b_fc3]\n'"
game_state.py,0,"b""# -*- coding: utf-8 -*-\nimport sys\nimport numpy as np\nimport cv2\nfrom ale_python_interface import ALEInterface\n\nfrom constants import ROM\nfrom constants import ACTION_SIZE\n\nclass GameState(object):\n  def __init__(self, rand_seed, display=False, no_op_max=7):\n    self.ale = ALEInterface()\n    self.ale.setInt(b'random_seed', rand_seed)\n    self.ale.setFloat(b'repeat_action_probability', 0.0)\n    self.ale.setBool(b'color_averaging', True)\n    self.ale.setInt(b'frame_skip', 4)\n    self._no_op_max = no_op_max\n\n    if display:\n      self._setup_display()\n    \n    self.ale.loadROM(ROM.encode('ascii'))\n\n    # collect minimal action set\n    self.real_actions = self.ale.getMinimalActionSet()\n\n    # height=210, width=160\n    self._screen = np.empty((210, 160, 1), dtype=np.uint8)\n\n    self.reset()\n\n  def _process_frame(self, action, reshape):\n    reward = self.ale.act(action)\n    terminal = self.ale.game_over()\n\n    # screen shape is (210, 160, 1)\n    self.ale.getScreenGrayscale(self._screen)\n    \n    # reshape it into (210, 160)\n    reshaped_screen = np.reshape(self._screen, (210, 160))\n    \n    # resize to height=110, width=84\n    resized_screen = cv2.resize(reshaped_screen, (84, 110))\n    \n    x_t = resized_screen[18:102,:]\n    if reshape:\n      x_t = np.reshape(x_t, (84, 84, 1))\n    x_t = x_t.astype(np.float32)\n    x_t *= (1.0/255.0)\n    return reward, terminal, x_t\n    \n    \n  def _setup_display(self):\n    if sys.platform == 'darwin':\n      import pygame\n      pygame.init()\n      self.ale.setBool(b'sound', False)\n    elif sys.platform.startswith('linux'):\n      self.ale.setBool(b'sound', True)\n    self.ale.setBool(b'display_screen', True)\n\n  def reset(self):\n    self.ale.reset_game()\n    \n    # randomize initial state\n    if self._no_op_max > 0:\n      no_op = np.random.randint(0, self._no_op_max + 1)\n      for _ in range(no_op):\n        self.ale.act(0)\n\n    _, _, x_t = self._process_frame(0, False)\n    \n    self.reward = 0\n    self.terminal = False\n    self.s_t = np.stack((x_t, x_t, x_t, x_t), axis = 2)\n    \n  def process(self, action):\n    # convert original 18 action index to minimal action set index\n    real_action = self.real_actions[action]\n    \n    r, t, x_t1 = self._process_frame(real_action, True)\n\n    self.reward = r\n    self.terminal = t\n    self.s_t1 = np.append(self.s_t[:,:,1:], x_t1, axis = 2)    \n\n  def update(self):\n    self.s_t = self.s_t1\n"""
game_state_test.py,0,"b""import unittest\nimport numpy as np\n\nfrom game_state import GameState\n\nclass TestSequenceFunctions(unittest.TestCase):\n\n  def test_process(self):\n    game_state = GameState(0)\n    \n    before_s_t = np.array( game_state.s_t )\n    \n    for i in range(1000):\n      bef1 = game_state.s_t[:,:,1]\n      bef2 = game_state.s_t[:,:,2]\n      bef3 = game_state.s_t[:,:,3]\n\n      game_state.process(1)\n      game_state.update()\n      \n      aft0 = game_state.s_t[:,:,0]\n      aft1 = game_state.s_t[:,:,1]\n      aft2 = game_state.s_t[:,:,2]\n\n      # values should be shifted\n      self.assertTrue( (bef1.flatten() == aft0.flatten()).all() )\n      self.assertTrue( (bef2.flatten() == aft1.flatten()).all() )\n      self.assertTrue( (bef3.flatten() == aft2.flatten()).all() )\n\n      # all element should be less [0.0~1.0]\n      self.assertTrue( np.less_equal(bef1, 1.0).all() )\n      self.assertTrue( np.less_equal(bef2, 1.0).all() )\n      self.assertTrue( np.less_equal(bef3, 1.0).all() )\n      self.assertTrue( np.greater_equal(bef1, 0.0).all() )\n      self.assertTrue( np.greater_equal(bef2, 0.0).all() )\n      self.assertTrue( np.greater_equal(bef3, 0.0).all() )\n\n      self.assertTrue( np.less_equal(aft0, 1.0).all() )\n      self.assertTrue( np.less_equal(aft1, 1.0).all() )\n      self.assertTrue( np.less_equal(aft2, 1.0).all() )\n      self.assertTrue( np.greater_equal(aft0, 0.0).all() )\n      self.assertTrue( np.greater_equal(aft1, 0.0).all() )\n      self.assertTrue( np.greater_equal(aft2, 0.0).all() )\n\nif __name__ == '__main__':\n  unittest.main()\n"""
rmsprop_applier.py,11,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\n\nfrom tensorflow.python.training import training_ops\nfrom tensorflow.python.training import slot_creator\n\nclass RMSPropApplier(object):\n\n  def __init__(self,\n               learning_rate,\n               decay=0.9,\n               momentum=0.0,\n               epsilon=1e-10,\n               clip_norm=40.0,\n               device=""/cpu:0"",\n               name=""RMSPropApplier""):\n\n    self._name = name\n    self._learning_rate = learning_rate\n    self._decay = decay\n    self._momentum = momentum\n    self._epsilon = epsilon\n    self._clip_norm = clip_norm\n    self._device = device\n\n    # Tensors for learning rate and momentum.  Created in _prepare.\n    self._learning_rate_tensor = None\n    self._decay_tensor = None\n    self._momentum_tensor = None\n    self._epsilon_tensor = None\n\n    self._slots = {}\n\n  def _create_slots(self, var_list):\n    for v in var_list:\n      # \'val\' is Variable\'s intial value tensor.\n      val = tf.constant(1.0, dtype=v.dtype, shape=v.get_shape())\n      self._get_or_make_slot(v, val, ""rms"", self._name)\n      self._zeros_slot(v, ""momentum"", self._name)\n\n  def _prepare(self):\n      self._learning_rate_tensor = tf.convert_to_tensor(self._learning_rate,\n                                                      name=""learning_rate"")\n      self._decay_tensor = tf.convert_to_tensor(self._decay, name=""decay"")\n      self._momentum_tensor = tf.convert_to_tensor(self._momentum,\n                                                 name=""momentum"")\n      self._epsilon_tensor = tf.convert_to_tensor(self._epsilon,\n                                                name=""epsilon"")\n\n  def _slot_dict(self, slot_name):\n    named_slots = self._slots.get(slot_name, None)\n    if named_slots is None:\n      named_slots = {}\n      self._slots[slot_name] = named_slots\n    return named_slots\n\n  def _get_or_make_slot(self, var, val, slot_name, op_name):\n    named_slots = self._slot_dict(slot_name)\n    if var not in named_slots:\n      named_slots[var] = slot_creator.create_slot(var, val, op_name)\n    return named_slots[var]\n\n  def get_slot(self, var, name):\n    named_slots = self._slots.get(name, None)\n    if not named_slots:\n      return None\n    return named_slots.get(var, None)\n\n  def _zeros_slot(self, var, slot_name, op_name):\n    named_slots = self._slot_dict(slot_name)\n    if var not in named_slots:\n      named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\n    return named_slots[var]\n\n  # TODO: in RMSProp native code, memcpy() (for CPU) and\n  # cudaMemcpyAsync() (for GPU) are used when updating values,\n  # and values might tend to be overwritten with results from other threads.\n  # (Need to check the learning performance with replacing it)  \n  def _apply_dense(self, grad, var):\n    rms = self.get_slot(var, ""rms"")\n    mom = self.get_slot(var, ""momentum"")\n    return training_ops.apply_rms_prop(\n      var, rms, mom,\n      self._learning_rate_tensor,\n      self._decay_tensor,\n      self._momentum_tensor,\n      self._epsilon_tensor,\n      grad,\n      use_locking=False).op\n\n  # Apply accumulated gradients to var.\n  def apply_gradients(self, var_list, accum_grad_list, name=None):\n    update_ops = []\n\n    with tf.device(self._device):\n      with tf.control_dependencies(None):\n        self._create_slots(var_list)\n\n      with tf.name_scope(name, self._name, []) as name:\n        self._prepare()\n        for var, accum_grad in zip(var_list, accum_grad_list):\n          with tf.name_scope(""update_"" + var.op.name), tf.device(var.device):\n            clipped_accum_grad = tf.clip_by_norm(accum_grad, self._clip_norm)\n            update_ops.append(self._apply_dense(clipped_accum_grad, var))\n        return tf.group(*update_ops, name=name)\n'"
rmsprop_applier_test.py,6,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport math\nimport tensorflow as tf\nimport rmsprop_applier\n\nclass RMSPropApplierTest(tf.test.TestCase):\n  def testApply(self):\n    with self.test_session():\n      var = tf.Variable([1.0, 2.0])\n      \n      grad0 = tf.Variable([2.0, 4.0])\n      grad1 = tf.Variable([3.0, 6.0])\n      \n      opt = rmsprop_applier.RMSPropApplier(learning_rate=2.0,\n                                           decay=0.9,\n                                           momentum=0.0,\n                                           epsilon=1.0)\n      \n      apply_gradient0 = opt.apply_gradients([var], [grad0])\n      apply_gradient1 = opt.apply_gradients([var], [grad1])\n\n      tf.initialize_all_variables().run()\n\n      # grad0\xe3\x82\x92\xe5\x8f\x8d\xe6\x98\xa0\n      apply_gradient0.run()\n\n      ms_x = 1.0\n      ms_y = 1.0\n\n      x = 1.0\n      y = 2.0\n      dx = 2.0\n      dy = 4.0\n      ms_x = ms_x + (dx * dx - ms_x) * (1.0 - 0.9)\n      ms_y = ms_y + (dy * dy - ms_y) * (1.0 - 0.9)\n      x = x - (2.0 * dx / math.sqrt(ms_x+1.0))\n      y = y - (2.0 * dy / math.sqrt(ms_y+1.0))\n\n      self.assertAllClose(np.array([x, y]), var.eval())\n\n      # grad1\xe3\x82\x92\xe5\x8f\x8d\xe6\x98\xa0\n      apply_gradient1.run()\n\n      dx = 3.0\n      dy = 6.0\n      ms_x = ms_x + (dx * dx - ms_x) * (1.0 - 0.9)\n      ms_y = ms_y + (dy * dy - ms_y) * (1.0 - 0.9)\n      x = x - (2.0 * dx / math.sqrt(ms_x+1.0))\n      y = y - (2.0 * dy / math.sqrt(ms_y+1.0))\n      \n      self.assertAllClose(np.array([x, y]), var.eval())\n      \nif __name__ == ""__main__"":\n  tf.test.main()\n'"
