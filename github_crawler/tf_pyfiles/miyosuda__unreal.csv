file_path,api_count,code
display.py,5,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport os\nfrom collections import deque\nimport pygame\n\nfrom environment.environment import Environment\nfrom model.model import UnrealModel\nfrom train.experience import ExperienceFrame\nfrom options import get_options\n\nBLUE  = (128, 128, 255)\nRED   = (255, 192, 192)\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\n\n\n# get command line args\nflags = get_options(""display"")\n\n\nclass MovieWriter(object):\n  def __init__(self, file_name, frame_size, fps):\n    """"""\n    frame_size is (w, h)\n    """"""\n    self._frame_size = frame_size\n    fourcc = cv2.cv.CV_FOURCC(\'m\', \'p\', \'4\', \'v\')\n    self.vout = cv2.VideoWriter()\n    success = self.vout.open(file_name, fourcc, fps, frame_size, True)\n    if not success:\n      print(""Create movie failed: {0}"".format(file_name))\n\n  def add_frame(self, frame):\n    """"""\n    frame shape is (h, w, 3), dtype is np.uint8\n    """"""\n    self.vout.write(frame)\n\n  def close(self):\n    self.vout.release() \n    self.vout = None\n\n\nclass StateHistory(object):\n  def __init__(self):\n    self._states = deque(maxlen=3)\n\n  def add_state(self, state):\n    self._states.append(state)\n\n  @property\n  def is_full(self):\n    return len(self._states) >= 3\n\n  @property\n  def states(self):\n    return list(self._states)\n\n\nclass ValueHistory(object):\n  def __init__(self):\n    self._values = deque(maxlen=100)\n\n  def add_value(self, value):\n    self._values.append(value)\n\n  @property    \n  def is_empty(self):\n    return len(self._values) == 0\n\n  @property\n  def values(self):\n    return self._values\n\n\nclass Display(object):\n  def __init__(self, display_size):\n    pygame.init()\n    \n    self.surface = pygame.display.set_mode(display_size, 0, 24)\n    pygame.display.set_caption(\'UNREAL\')\n\n    self.action_size = Environment.get_action_size(flags.env_type, flags.env_name)\n    self.global_network = UnrealModel(self.action_size,\n                                      -1,\n                                      flags.use_pixel_change,\n                                      flags.use_value_replay,\n                                      flags.use_reward_prediction,\n                                      0.0,\n                                      0.0,\n                                      ""/cpu:0"",\n                                      for_display=True)\n    self.environment = Environment.create_environment(flags.env_type, flags.env_name)\n    self.font = pygame.font.SysFont(None, 20)\n    self.value_history = ValueHistory()\n    self.state_history = StateHistory()\n    self.episode_reward = 0\n\n  def update(self, sess):\n    self.surface.fill(BLACK)\n    self.process(sess)\n    pygame.display.update()\n\n  def choose_action(self, pi_values):\n    return np.random.choice(range(len(pi_values)), p=pi_values)\n\n  def scale_image(self, image, scale):\n    return image.repeat(scale, axis=0).repeat(scale, axis=1)\n\n  def draw_text(self, str, left, top, color=WHITE):\n    text = self.font.render(str, True, color, BLACK)\n    text_rect = text.get_rect()\n    text_rect.left = left    \n    text_rect.top = top\n    self.surface.blit(text, text_rect)  \n\n  def draw_center_text(self, str, center_x, top):\n    text = self.font.render(str, True, WHITE, BLACK)\n    text_rect = text.get_rect()\n    text_rect.centerx = center_x\n    text_rect.top = top\n    self.surface.blit(text, text_rect)\n\n  def show_pixel_change(self, pixel_change, left, top, rate, label):\n    """"""\n    Show pixel change\n    """"""    \n    pixel_change_ = np.clip(pixel_change * 255.0 * rate, 0.0, 255.0)\n    data = pixel_change_.astype(np.uint8)\n    data = np.stack([data for _ in range(3)], axis=2)\n    data = self.scale_image(data, 4)\n    image = pygame.image.frombuffer(data, (20*4,20*4), \'RGB\')\n    self.surface.blit(image, (left+8+4, top+8+4))\n    self.draw_center_text(label, left + 100/2, top + 100)\n    \n\n  def show_policy(self, pi):\n    """"""\n    Show action probability.\n    """"""\n    start_x = 10\n\n    y = 150\n  \n    for i in range(len(pi)):\n      width = pi[i] * 100\n      pygame.draw.rect(self.surface, WHITE, (start_x, y, width, 10))\n      y += 20\n    self.draw_center_text(""PI"", 50, y)\n  \n  def show_image(self, state):\n    """"""\n    Show input image\n    """"""\n    state_ = state * 255.0\n    data = state_.astype(np.uint8)\n    image = pygame.image.frombuffer(data, (84,84), \'RGB\')\n    self.surface.blit(image, (8, 8))\n    self.draw_center_text(""input"", 50, 100)\n\n  def show_value(self):\n    if self.value_history.is_empty:\n      return\n\n    min_v = float(""inf"")\n    max_v = float(""-inf"")\n\n    values = self.value_history.values\n\n    for v in values:\n      min_v = min(min_v, v)\n      max_v = max(max_v, v)\n\n    top = 150\n    left = 150\n    width = 100\n    height = 100\n    bottom = top + width\n    right = left + height\n\n    d = max_v - min_v\n    last_r = 0.0\n    for i,v in enumerate(values):\n      r = (v - min_v) / d\n      if i > 0:\n        x0 = i-1 + left\n        x1 = i   + left\n        y0 = bottom - last_r * height\n        y1 = bottom - r * height\n        pygame.draw.line(self.surface, BLUE, (x0, y0), (x1, y1), 1)\n      last_r = r\n\n    pygame.draw.line(self.surface, WHITE, (left,  top),    (left,  bottom), 1)\n    pygame.draw.line(self.surface, WHITE, (right, top),    (right, bottom), 1)\n    pygame.draw.line(self.surface, WHITE, (left,  top),    (right, top),    1)\n    pygame.draw.line(self.surface, WHITE, (left,  bottom), (right, bottom), 1)\n\n    self.draw_center_text(""V"", left + width/2, bottom+10)\n\n  def show_reward_prediction(self, rp_c, reward):\n    start_x = 310\n    reward_index = 0\n    if reward == 0:\n      reward_index = 0\n    elif reward > 0:\n      reward_index = 1\n    elif reward < 0:\n      reward_index = 2\n\n    y = 150\n\n    labels = [""0"", ""+"", ""-""]\n    \n    for i in range(len(rp_c)):\n      width = rp_c[i] * 100\n\n      if i == reward_index:\n        color = RED\n      else:\n        color = WHITE\n      pygame.draw.rect(self.surface, color, (start_x+15, y, width, 10))\n      self.draw_text(labels[i], start_x, y-1, color)\n      y += 20\n    \n    self.draw_center_text(""RP"", start_x + 100/2, y)\n\n  def show_reward(self):\n    self.draw_text(""REWARD: {}"".format(int(self.episode_reward)), 310, 10)\n\n  def process(self, sess):\n    last_action = self.environment.last_action\n    last_reward = np.clip(self.environment.last_reward, -1, 1)\n    last_action_reward = ExperienceFrame.concat_action_and_reward(last_action, self.action_size,\n                                                                  last_reward)\n    \n    if not flags.use_pixel_change:\n      pi_values, v_value = self.global_network.run_base_policy_and_value(sess,\n                                                                         self.environment.last_state,\n                                                                         last_action_reward)\n    else:\n      pi_values, v_value, pc_q = self.global_network.run_base_policy_value_pc_q(sess,\n                                                                                self.environment.last_state,\n                                                                                last_action_reward)\n    self.value_history.add_value(v_value)\n    \n    action = self.choose_action(pi_values)\n    state, reward, terminal, pixel_change = self.environment.process(action)\n    self.episode_reward += reward\n  \n    if terminal:\n      self.environment.reset()\n      self.episode_reward = 0\n      \n    self.show_image(state)\n    self.show_policy(pi_values)\n    self.show_value()\n    self.show_reward()\n    \n    if flags.use_pixel_change:\n      self.show_pixel_change(pixel_change, 100, 0, 3.0, ""PC"")\n      self.show_pixel_change(pc_q[:,:,action], 200, 0, 0.4, ""PC Q"")\n  \n    if flags.use_reward_prediction:\n      if self.state_history.is_full:\n        rp_c = self.global_network.run_rp_c(sess, self.state_history.states)\n        self.show_reward_prediction(rp_c, reward)\n  \n    self.state_history.add_state(state)\n\n  def get_frame(self):\n    data = self.surface.get_buffer().raw\n    return data\n\n\ndef main(args):\n  sess = tf.Session()\n  sess.run(tf.global_variables_initializer())\n  \n  display_size = (440, 400)\n  display = Display(display_size)\n  saver = tf.train.Saver()\n  checkpoint = tf.train.get_checkpoint_state(flags.checkpoint_dir)\n  if checkpoint and checkpoint.model_checkpoint_path:\n    saver.restore(sess, checkpoint.model_checkpoint_path)\n    print(""checkpoint loaded:"", checkpoint.model_checkpoint_path)\n  else:\n    print(""Could not find old checkpoint"")\n\n  clock = pygame.time.Clock()\n  \n  running = True\n  FPS = 15\n\n  if flags.recording:\n    writer = MovieWriter(""out.mov"", display_size, FPS)\n  \n  if flags.frame_saving:\n    frame_count = 0\n    if not os.path.exists(flags.frame_save_dir):\n      os.mkdir(flags.frame_save_dir)\n      \n  while running:\n    for event in pygame.event.get():\n      if event.type == pygame.QUIT:\n        running = False\n        \n    display.update(sess)\n    clock.tick(FPS)\n    \n    if flags.recording or flags.frame_saving:\n      frame_str = display.get_frame()\n      d = np.fromstring(frame_str, dtype=np.uint8)\n      d = d.reshape((display_size[1], display_size[0], 3))\n      if flags.recording:\n        writer.add_frame(d)\n      else:\n        frame_file_path = ""{0}/{1:06d}.png"".format(flags.frame_save_dir, frame_count)\n        cv2.imwrite(frame_file_path, d)\n        frame_count += 1\n  \n  if flags.recording:\n    writer.close()\n\n    \nif __name__ == \'__main__\':\n  tf.app.run()\n'"
main.py,11,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport threading\n\nimport signal\nimport math\nimport os\nimport time\n\nfrom environment.environment import Environment\nfrom model.model import UnrealModel\nfrom train.trainer import Trainer\nfrom train.rmsprop_applier import RMSPropApplier\nfrom options import get_options\n\nUSE_GPU = True # To use GPU, set True\n\n# get command line args\nflags = get_options(""training"")\n\ndef log_uniform(lo, hi, rate):\n  log_lo = math.log(lo)\n  log_hi = math.log(hi)\n  v = log_lo * (1-rate) + log_hi * rate\n  return math.exp(v)\n\n\nclass Application(object):\n  def __init__(self):\n    pass\n  \n  def train_function(self, parallel_index, preparing):\n    """""" Train each environment. """"""\n    \n    trainer = self.trainers[parallel_index]\n    if preparing:\n      trainer.prepare()\n    \n    # set start_time\n    trainer.set_start_time(self.start_time)\n  \n    while True:\n      if self.stop_requested:\n        break\n      if self.terminate_reqested:\n        trainer.stop()\n        break\n      if self.global_t > flags.max_time_step:\n        trainer.stop()\n        break\n      if parallel_index == 0 and self.global_t > self.next_save_steps:\n        # Save checkpoint\n        self.save()\n  \n      diff_global_t = trainer.process(self.sess,\n                                      self.global_t,\n                                      self.summary_writer,\n                                      self.summary_op,\n                                      self.score_input)\n      self.global_t += diff_global_t\n\n  def run(self):\n    device = ""/cpu:0""\n    if USE_GPU:\n      device = ""/gpu:0""\n    \n    initial_learning_rate = log_uniform(flags.initial_alpha_low,\n                                        flags.initial_alpha_high,\n                                        flags.initial_alpha_log_rate)\n    \n    self.global_t = 0\n    \n    self.stop_requested = False\n    self.terminate_reqested = False\n    \n    action_size = Environment.get_action_size(flags.env_type,\n                                              flags.env_name)\n    \n    self.global_network = UnrealModel(action_size,\n                                      -1,\n                                      flags.use_pixel_change,\n                                      flags.use_value_replay,\n                                      flags.use_reward_prediction,\n                                      flags.pixel_change_lambda,\n                                      flags.entropy_beta,\n                                      device)\n    self.trainers = []\n    \n    learning_rate_input = tf.placeholder(""float"")\n    \n    grad_applier = RMSPropApplier(learning_rate = learning_rate_input,\n                                  decay = flags.rmsp_alpha,\n                                  momentum = 0.0,\n                                  epsilon = flags.rmsp_epsilon,\n                                  clip_norm = flags.grad_norm_clip,\n                                  device = device)\n    \n    for i in range(flags.parallel_size):\n      trainer = Trainer(i,\n                        self.global_network,\n                        initial_learning_rate,\n                        learning_rate_input,\n                        grad_applier,\n                        flags.env_type,\n                        flags.env_name,\n                        flags.use_pixel_change,\n                        flags.use_value_replay,\n                        flags.use_reward_prediction,\n                        flags.pixel_change_lambda,\n                        flags.entropy_beta,\n                        flags.local_t_max,\n                        flags.gamma,\n                        flags.gamma_pc,\n                        flags.experience_history_size,\n                        flags.max_time_step,\n                        device)\n      self.trainers.append(trainer)\n    \n    # prepare session\n    config = tf.ConfigProto(log_device_placement=False,\n                            allow_soft_placement=True)\n    config.gpu_options.allow_growth = True\n    self.sess = tf.Session(config=config)\n    \n    self.sess.run(tf.global_variables_initializer())\n    \n    # summary for tensorboard\n    self.score_input = tf.placeholder(tf.int32)\n    tf.summary.scalar(""score"", self.score_input)\n    \n    self.summary_op = tf.summary.merge_all()\n    self.summary_writer = tf.summary.FileWriter(flags.log_file,\n                                                self.sess.graph)\n    \n    # init or load checkpoint with saver\n    self.saver = tf.train.Saver(self.global_network.get_vars())\n    \n    checkpoint = tf.train.get_checkpoint_state(flags.checkpoint_dir)\n    if checkpoint and checkpoint.model_checkpoint_path:\n      self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n      print(""checkpoint loaded:"", checkpoint.model_checkpoint_path)\n      tokens = checkpoint.model_checkpoint_path.split(""-"")\n      # set global step\n      self.global_t = int(tokens[1])\n      print("">>> global step set: "", self.global_t)\n      # set wall time\n      wall_t_fname = flags.checkpoint_dir + \'/\' + \'wall_t.\' + str(self.global_t)\n      with open(wall_t_fname, \'r\') as f:\n        self.wall_t = float(f.read())\n        self.next_save_steps = (self.global_t + flags.save_interval_step) // flags.save_interval_step * flags.save_interval_step\n        \n    else:\n      print(""Could not find old checkpoint"")\n      # set wall time\n      self.wall_t = 0.0\n      self.next_save_steps = flags.save_interval_step\n  \n    # run training threads\n    self.train_threads = []\n    for i in range(flags.parallel_size):\n      self.train_threads.append(threading.Thread(target=self.train_function, args=(i,True)))\n      \n    signal.signal(signal.SIGINT, self.signal_handler)\n  \n    # set start time\n    self.start_time = time.time() - self.wall_t\n  \n    for t in self.train_threads:\n      t.start()\n  \n    print(\'Press Ctrl+C to stop\')\n    signal.pause()\n\n  def save(self):\n    """""" Save checkpoint. \n    Called from therad-0.\n    """"""\n    self.stop_requested = True\n  \n    # Wait for all other threads to stop\n    for (i, t) in enumerate(self.train_threads):\n      if i != 0:\n        t.join()\n  \n    # Save\n    if not os.path.exists(flags.checkpoint_dir):\n      os.mkdir(flags.checkpoint_dir)\n  \n    # Write wall time\n    wall_t = time.time() - self.start_time\n    wall_t_fname = flags.checkpoint_dir + \'/\' + \'wall_t.\' + str(self.global_t)\n    with open(wall_t_fname, \'w\') as f:\n      f.write(str(wall_t))\n  \n    print(\'Start saving.\')\n    self.saver.save(self.sess,\n                    flags.checkpoint_dir + \'/\' + \'checkpoint\',\n                    global_step = self.global_t)\n    print(\'End saving.\')  \n    \n    self.stop_requested = False\n    self.next_save_steps += flags.save_interval_step\n  \n    # Restart other threads\n    for i in range(flags.parallel_size):\n      if i != 0:\n        thread = threading.Thread(target=self.train_function, args=(i,False))\n        self.train_threads[i] = thread\n        thread.start()\n    \n  def signal_handler(self, signal, frame):\n    print(\'You pressed Ctrl+C!\')\n    self.terminate_reqested = True\n\ndef main(argv):\n  app = Application()\n  app.run()\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
options.py,26,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef get_options(option_type):\n  """"""\n  option_type: string\n    \'training\' or \'display\' or \'visualize\'\n  """"""\n  # Common\n  tf.app.flags.DEFINE_string(""env_type"", ""lab"", ""environment type (lab or gym or maze)"")\n  tf.app.flags.DEFINE_string(""env_name"", ""nav_maze_static_01"",  ""environment name"")\n  tf.app.flags.DEFINE_boolean(""use_pixel_change"", True, ""whether to use pixel change"")\n  tf.app.flags.DEFINE_boolean(""use_value_replay"", True, ""whether to use value function replay"")\n  tf.app.flags.DEFINE_boolean(""use_reward_prediction"", True, ""whether to use reward prediction"")\n\n  tf.app.flags.DEFINE_string(""checkpoint_dir"", ""/tmp/unreal_checkpoints"", ""checkpoint directory"")\n\n  # For training\n  if option_type == \'training\':\n    tf.app.flags.DEFINE_integer(""parallel_size"", 8, ""parallel thread size"")\n    tf.app.flags.DEFINE_integer(""local_t_max"", 20, ""repeat step size"")\n    tf.app.flags.DEFINE_float(""rmsp_alpha"", 0.99, ""decay parameter for rmsprop"")\n    tf.app.flags.DEFINE_float(""rmsp_epsilon"", 0.1, ""epsilon parameter for rmsprop"")\n\n    tf.app.flags.DEFINE_string(""log_file"", ""/tmp/unreal_log/unreal_log"", ""log file directory"")\n    tf.app.flags.DEFINE_float(""initial_alpha_low"", 1e-4, ""log_uniform low limit for learning rate"")\n    tf.app.flags.DEFINE_float(""initial_alpha_high"", 5e-3, ""log_uniform high limit for learning rate"")\n    tf.app.flags.DEFINE_float(""initial_alpha_log_rate"", 0.5, ""log_uniform interpolate rate for learning rate"")\n    tf.app.flags.DEFINE_float(""gamma"", 0.99, ""discount factor for rewards"")\n    tf.app.flags.DEFINE_float(""gamma_pc"", 0.9, ""discount factor for pixel control"")\n    tf.app.flags.DEFINE_float(""entropy_beta"", 0.001, ""entropy regularization constant"")\n    tf.app.flags.DEFINE_float(""pixel_change_lambda"", 0.05, ""pixel change lambda"") # 0.05, 0.01 ~ 0.1 for lab, 0.0001 ~ 0.01 for gym\n    tf.app.flags.DEFINE_integer(""experience_history_size"", 2000, ""experience replay buffer size"")\n    tf.app.flags.DEFINE_integer(""max_time_step"", 10 * 10**7, ""max time steps"")\n    tf.app.flags.DEFINE_integer(""save_interval_step"", 100 * 1000, ""saving interval steps"")\n    tf.app.flags.DEFINE_boolean(""grad_norm_clip"", 40.0, ""gradient norm clipping"")\n\n  # For display\n  if option_type == \'display\':\n    tf.app.flags.DEFINE_string(""frame_save_dir"", ""/tmp/unreal_frames"", ""frame save directory"")\n    tf.app.flags.DEFINE_boolean(""recording"", False, ""whether to record movie"")\n    tf.app.flags.DEFINE_boolean(""frame_saving"", False, ""whether to save frames"")\n\n  return tf.app.flags.FLAGS\n'"
test.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\nimport train.experience_test\nimport train.rmsprop_applier_test\nimport environment.environment_test\nimport model.model_test\n\ndef get_suite():\n  suite = unittest.TestSuite()\n\n  suite.addTest(unittest.makeSuite(train.experience_test.TestExperience))\n  suite.addTest(unittest.makeSuite(train.rmsprop_applier_test.TestRMSPropApplier))    \n  suite.addTest(unittest.makeSuite(environment.environment_test.TestEnvironment))\n  suite.addTest(unittest.makeSuite(model.model_test.TestUnrealModel))\n  \n  return suite\n\ndef main():\n  suite = get_suite()\n  unittest.TextTestRunner().run(suite)\n\nif __name__ == '__main__':\n  main()\n"""
visualize.py,5,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom environment.environment import Environment\nfrom model.model import UnrealModel\nfrom options import get_options\n\n\n# get command line args\nflags = get_options(""visualize"")\n\n\ndef main(args):\n  action_size = Environment.get_action_size(flags.env_type, flags.env_name)\n  global_network = UnrealModel(action_size, -1,\n                               flags.use_pixel_change,\n                               flags.use_value_replay,\n                               flags.use_reward_prediction,\n                               0.0,\n                               0.0,\n                               ""/cpu:0"") # use CPU for weight visualize tool\n  \n  sess = tf.Session()\n  \n  init = tf.global_variables_initializer()\n  sess.run(init)\n  \n  saver = tf.train.Saver()\n  checkpoint = tf.train.get_checkpoint_state(flags.checkpoint_dir)\n  if checkpoint and checkpoint.model_checkpoint_path:\n    saver.restore(sess, checkpoint.model_checkpoint_path)\n    print(""checkpoint loaded:"", checkpoint.model_checkpoint_path)\n  else:\n    print(""Could not find old checkpoint"")\n  \n  vars = {}\n  var_list = global_network.get_vars()\n  for v in var_list:\n    vars[v.name] = v\n  \n  W_conv1 = sess.run(vars[\'net_-1/base_conv/W_base_conv1:0\'])\n  \n  # show graph of W_conv1\n  fig, axes = plt.subplots(3, 16, figsize=(12, 6),\n               subplot_kw={\'xticks\': [], \'yticks\': []})\n  fig.subplots_adjust(hspace=0.1, wspace=0.1)\n  \n  for ax,i in zip(axes.flat, range(3*16)):\n    inch = i//16\n    outch = i%16\n    img = W_conv1[:,:,inch,outch]\n    ax.imshow(img, cmap=plt.cm.gray, interpolation=\'nearest\')\n    ax.set_title(str(inch) + "","" + str(outch))\n  \n  plt.show()\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
environment/__init__.py,0,b''
environment/environment.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nclass Environment(object):\n  # cached action size\n  action_size = -1\n  \n  @staticmethod\n  def create_environment(env_type, env_name):\n    if env_type == \'maze\':\n      from . import maze_environment\n      return maze_environment.MazeEnvironment()\n    elif env_type == \'lab\':\n      from . import lab_environment\n      return lab_environment.LabEnvironment(env_name)\n    else:\n      from . import gym_environment\n      return gym_environment.GymEnvironment(env_name)\n  \n  @staticmethod\n  def get_action_size(env_type, env_name):\n    if Environment.action_size >= 0:\n      return Environment.action_size\n\n    if env_type == \'maze\':\n      from . import maze_environment\n      Environment.action_size = \\\n        maze_environment.MazeEnvironment.get_action_size()\n    elif env_type == ""lab"":\n      from . import lab_environment\n      Environment.action_size = \\\n        lab_environment.LabEnvironment.get_action_size(env_name)\n    else:\n      from . import gym_environment\n      Environment.action_size = \\\n        gym_environment.GymEnvironment.get_action_size(env_name)\n    return Environment.action_size\n\n  def __init__(self):\n    pass\n\n  def process(self, action):\n    pass\n\n  def reset(self):\n    pass\n\n  def stop(self):\n    pass  \n\n  def _subsample(self, a, average_width):\n    s = a.shape\n    sh = s[0]//average_width, average_width, s[1]//average_width, average_width\n    return a.reshape(sh).mean(-1).mean(1)  \n\n  def _calc_pixel_change(self, state, last_state):\n    d = np.absolute(state[2:-2,2:-2,:] - last_state[2:-2,2:-2,:])\n    # (80,80,3)\n    m = np.mean(d, 2)\n    c = self._subsample(m, 4)\n    return c\n'"
environment/environment_test.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\nimport numpy as np\n\nfrom environment.environment import Environment\n\n\nclass TestEnvironment(unittest.TestCase):\n  def test_lab(self):\n    has_lab = True\n    try:\n      import deepmind_lab\n    except ImportError:\n      has_lab = False\n      print(""Failed to import lab. Skipping lab environment testing."")\n\n    if has_lab:\n      env_type = ""lab""\n      env_name = ""nav_maze_static_01""\n      self.check_environment(env_type, env_name)\n\n  def test_gym(self):\n    env_type = ""gym""\n    env_name = ""MontezumaRevenge-v0""\n    self.check_environment(env_type, env_name)\n\n  def test_maze(self):\n    env_type = ""maze""\n    env_name = """"\n    self.check_environment(env_type, env_name)\n\n  def check_environment(self, env_type, env_name):\n    environment = Environment.create_environment(env_type, env_name)\n    # action_size = Environment.get_action_size(env_type, env_name) # Not used\n\n    for i in range(3):\n      state, reward, terminal, pixel_change = environment.process(0)\n\n      # Check shape\n      self.assertTrue( state.shape == (84,84,3) )\n      self.assertTrue( environment.last_state.shape == (84,84,3) )\n      self.assertTrue( pixel_change.shape == (20,20) )\n\n      # state and pixel_change value range should be [0,1]\n      self.assertTrue( np.amax(state) <= 1.0 )\n      self.assertTrue( np.amin(state) >= 0.0 )\n      self.assertTrue( np.amax(pixel_change) <= 1.0 )\n      self.assertTrue( np.amin(pixel_change) >= 0.0 )\n\n    environment.stop()    \n                       \n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
environment/gym_environment.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom multiprocessing import Process, Pipe\nimport numpy as np\nimport cv2\nimport gym\n\nfrom environment import environment\n\nCOMMAND_RESET     = 0\nCOMMAND_ACTION    = 1\nCOMMAND_TERMINATE = 2\n\n\ndef preprocess_frame(observation):\n  # observation shape = (210, 160, 3)\n  observation = observation.astype(np.float32)\n  resized_observation = cv2.resize(observation, (84, 84))\n  resized_observation = resized_observation / 255.0\n  return resized_observation\n\ndef worker(conn, env_name):\n  env = gym.make(env_name)\n  env.reset()\n  conn.send(0)\n  \n  while True:\n    command, arg = conn.recv()\n\n    if command == COMMAND_RESET:\n      obs = env.reset()\n      state = preprocess_frame(obs)\n      conn.send(state)\n    elif command == COMMAND_ACTION:\n      reward = 0\n      for i in range(4):\n        obs, r, terminal, _ = env.step(arg)\n        reward += r\n        if terminal:\n          break\n      state = preprocess_frame(obs)\n      conn.send([state, reward, terminal])\n    elif command == COMMAND_TERMINATE:\n      break\n    else:\n      print(""bad command: {}"".format(command))\n  env.close()\n  conn.send(0)\n  conn.close()\n\n\nclass GymEnvironment(environment.Environment):\n  @staticmethod\n  def get_action_size(env_name):\n    env = gym.make(env_name)\n    action_size = env.action_space.n\n    env.close()\n    return action_size\n  \n  def __init__(self, env_name):\n    environment.Environment.__init__(self)\n\n    self.conn, child_conn = Pipe()\n    self.proc = Process(target=worker, args=(child_conn, env_name))\n    self.proc.start()\n    self.conn.recv()\n    self.reset()\n\n  def reset(self):\n    self.conn.send([COMMAND_RESET, 0])\n    self.last_state = self.conn.recv()\n    \n    self.last_action = 0\n    self.last_reward = 0\n\n  def stop(self):\n    self.conn.send([COMMAND_TERMINATE, 0])\n    ret = self.conn.recv()\n    self.conn.close()\n    self.proc.join()\n    print(""gym environment stopped"")\n\n  def process(self, action):\n    self.conn.send([COMMAND_ACTION, action])\n    state, reward, terminal = self.conn.recv()\n    \n    pixel_change = self._calc_pixel_change(state, self.last_state)\n    self.last_state = state\n    self.last_action = action\n    self.last_reward = reward\n    return state, reward, terminal, pixel_change\n'"
environment/lab_environment.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom multiprocessing import Process, Pipe\nimport numpy as np\nimport deepmind_lab\n\nfrom environment import environment\n\nCOMMAND_RESET     = 0\nCOMMAND_ACTION    = 1\nCOMMAND_TERMINATE = 2\n\ndef worker(conn, env_name):\n  level = env_name\n  env = deepmind_lab.Lab(\n    level,\n    [\'RGB_INTERLACED\'],\n    config={\n      \'fps\': str(60),\n      \'width\': str(84),\n      \'height\': str(84)\n    })\n  conn.send(0)\n  \n  while True:\n    command, arg = conn.recv()\n\n    if command == COMMAND_RESET:\n      env.reset()\n      obs = env.observations()[\'RGB_INTERLACED\']\n      conn.send(obs)\n    elif command == COMMAND_ACTION:\n      reward = env.step(arg, num_steps=4)\n      terminal = not env.is_running()\n      if not terminal:\n        obs = env.observations()[\'RGB_INTERLACED\']\n      else:\n        obs = 0\n      conn.send([obs, reward, terminal])\n    elif command == COMMAND_TERMINATE:\n      break\n    else:\n      print(""bad command: {}"".format(command))\n  env.close()      \n  conn.send(0)\n  conn.close()\n\n\ndef _action(*entries):\n  return np.array(entries, dtype=np.intc)\n\n\nclass LabEnvironment(environment.Environment):\n  ACTION_LIST = [\n    _action(-20,   0,  0,  0, 0, 0, 0), # look_left\n    _action( 20,   0,  0,  0, 0, 0, 0), # look_right\n    #_action(  0,  10,  0,  0, 0, 0, 0), # look_up\n    #_action(  0, -10,  0,  0, 0, 0, 0), # look_down\n    _action(  0,   0, -1,  0, 0, 0, 0), # strafe_left\n    _action(  0,   0,  1,  0, 0, 0, 0), # strafe_right\n    _action(  0,   0,  0,  1, 0, 0, 0), # forward\n    _action(  0,   0,  0, -1, 0, 0, 0), # backward\n    #_action(  0,   0,  0,  0, 1, 0, 0), # fire\n    #_action(  0,   0,  0,  0, 0, 1, 0), # jump\n    #_action(  0,   0,  0,  0, 0, 0, 1)  # crouch\n  ]\n\n  @staticmethod\n  def get_action_size(env_name):\n    return len(LabEnvironment.ACTION_LIST)\n  \n  def __init__(self, env_name):\n    environment.Environment.__init__(self)\n    \n    self.conn, child_conn = Pipe()\n    self.proc = Process(target=worker, args=(child_conn, env_name))\n    self.proc.start()\n    self.conn.recv()\n    self.reset()\n\n  def reset(self):\n    self.conn.send([COMMAND_RESET, 0])\n    obs = self.conn.recv()\n    \n    self.last_state = self._preprocess_frame(obs)\n    self.last_action = 0\n    self.last_reward = 0\n\n  def stop(self):\n    self.conn.send([COMMAND_TERMINATE, 0])\n    ret = self.conn.recv()\n    self.conn.close()\n    self.proc.join()\n    print(""lab environment stopped"")\n    \n  def _preprocess_frame(self, image):\n    image = image.astype(np.float32)\n    image = image / 255.0\n    return image\n\n  def process(self, action):\n    real_action = LabEnvironment.ACTION_LIST[action]\n\n    self.conn.send([COMMAND_ACTION, real_action])\n    obs, reward, terminal = self.conn.recv()\n\n    if not terminal:\n      state = self._preprocess_frame(obs)\n    else:\n      state = self.last_state\n    \n    pixel_change = self._calc_pixel_change(state, self.last_state)\n    self.last_state = state\n    self.last_action = action\n    self.last_reward = reward\n    return state, reward, terminal, pixel_change\n'"
environment/maze_environment.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom environment import environment\n\nclass MazeEnvironment(environment.Environment):\n  @staticmethod\n  def get_action_size():\n    return 4\n  \n  def __init__(self):\n    environment.Environment.__init__(self)\n    \n    self._map_data = \\\n                     ""--+---G"" \\\n                     ""--+-+++"" \\\n                     ""S-+---+"" \\\n                     ""--+++--"" \\\n                     ""--+-+--"" \\\n                     ""--+----"" \\\n                     ""-----++"" \n    \n    self._setup()\n    self.reset()\n\n  def _setup(self):\n    image = np.zeros( (84, 84, 3), dtype=float )\n\n    start_pos = (-1, -1)\n    goal_pos  = (-1, -1)\n  \n    for y in range(7):\n      for x in range(7):\n        p = self._get_pixel(x,y)\n        if p == \'+\':\n          self._put_pixel(image, x, y, 0)\n        elif p == \'S\':\n          start_pos = (x, y)\n        elif p == \'G\':\n          goal_pos = (x, y)\n\n    self._maze_image = image\n    self._start_pos = start_pos\n    self._goal_pos = goal_pos\n    \n  def reset(self):\n    self.x = self._start_pos[0]\n    self.y = self._start_pos[1]\n    self.last_state = self._get_current_image()\n    self.last_action = 0\n    self.last_reward = 0    \n    \n  def _put_pixel(self, image, x, y, channel):\n    for i in range(12):\n      for j in range(12):\n        image[12*y + j, 12*x + i, channel] = 1.0\n        \n  def _get_pixel(self, x, y):\n    data_pos = y * 7 + x\n    return self._map_data[data_pos]\n\n  def _is_wall(self, x, y):\n    return self._get_pixel(x, y) == \'+\'\n\n  def _clamp(self, n, minn, maxn):\n    if n < minn:\n      return minn, True\n    elif n > maxn:\n      return maxn, True\n    return n, False\n  \n  def _move(self, dx, dy):\n    new_x = self.x + dx\n    new_y = self.y + dy\n\n    new_x, clamped_x = self._clamp(new_x, 0, 6)\n    new_y, clamped_y = self._clamp(new_y, 0, 6)\n\n    hit_wall = False\n\n    if self._is_wall(new_x, new_y):\n      new_x = self.x\n      new_y = self.y\n      hit_wall = True\n\n    hit = clamped_x or clamped_y or hit_wall\n    return new_x, new_y, hit\n\n  def _get_current_image(self):\n    image = np.array(self._maze_image)\n    self._put_pixel(image, self.x, self.y, 1)\n    return image\n\n  def process(self, action):\n    dx = 0\n    dy = 0\n    if action == 0: # UP\n      dy = -1\n    if action == 1: # DOWN\n      dy = 1\n    if action == 2: # LEFT\n      dx = -1\n    if action == 3: # RIGHT\n      dx = 1\n\n    self.x, self.y, hit = self._move(dx, dy)\n\n    image = self._get_current_image()\n    \n    terminal = (self.x == self._goal_pos[0] and\n                self.y == self._goal_pos[1])\n\n    if terminal:\n      reward = 1\n    elif hit:\n      reward = -1\n    else:\n      reward = 0\n\n    pixel_change = self._calc_pixel_change(image, self.last_state)\n    self.last_state = image\n    self.last_action = action\n    self.last_reward = reward\n    return image, reward, terminal, pixel_change\n'"
model/__init__.py,0,b''
model/model.py,77,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\n\n\n# weight initialization based on muupan\'s code\n# https://github.com/muupan/async-rl/blob/master/a3c_ale.py\ndef fc_initializer(input_channels, dtype=tf.float32):\n  def _initializer(shape, dtype=dtype, partition_info=None):\n    d = 1.0 / np.sqrt(input_channels)\n    return tf.random_uniform(shape, minval=-d, maxval=d)\n  return _initializer\n\n\ndef conv_initializer(kernel_width, kernel_height, input_channels, dtype=tf.float32):\n  def _initializer(shape, dtype=dtype, partition_info=None):\n    d = 1.0 / np.sqrt(input_channels * kernel_width * kernel_height)\n    return tf.random_uniform(shape, minval=-d, maxval=d)\n  return _initializer\n\n\nclass UnrealModel(object):\n  """"""\n  UNREAL algorithm network model.\n  """"""\n  def __init__(self,\n               action_size,\n               thread_index, # -1 for global\n               use_pixel_change,\n               use_value_replay,\n               use_reward_prediction,\n               pixel_change_lambda,\n               entropy_beta,\n               device,\n               for_display=False):\n    self._device = device\n    self._action_size = action_size\n    self._thread_index = thread_index\n    self._use_pixel_change = use_pixel_change\n    self._use_value_replay = use_value_replay\n    self._use_reward_prediction = use_reward_prediction\n    self._pixel_change_lambda = pixel_change_lambda\n    self._entropy_beta = entropy_beta\n    \n    self._create_network(for_display)\n    \n  def _create_network(self, for_display):\n    scope_name = ""net_{0}"".format(self._thread_index)\n    with tf.device(self._device), tf.variable_scope(scope_name) as scope:\n      # lstm\n      self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(256, state_is_tuple=True)\n      \n      # [base A3C network]\n      self._create_base_network()\n\n      # [Pixel change network]\n      if self._use_pixel_change:\n        self._create_pc_network()\n        if for_display:\n          self._create_pc_network_for_display()\n\n      # [Value replay network]\n      if self._use_value_replay:\n        self._create_vr_network()\n\n      # [Reward prediction network]\n      if self._use_reward_prediction:\n        self._create_rp_network()\n      \n      self.reset_state()\n\n      self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_name)\n\n\n  def _create_base_network(self):\n    # State (Base image input)\n    self.base_input = tf.placeholder(""float"", [None, 84, 84, 3])\n\n    # Last action and reward\n    self.base_last_action_reward_input = tf.placeholder(""float"", [None, self._action_size+1])\n    \n    # Conv layers\n    base_conv_output = self._base_conv_layers(self.base_input)\n    \n    # LSTM layer\n    self.base_initial_lstm_state0 = tf.placeholder(tf.float32, [1, 256])\n    self.base_initial_lstm_state1 = tf.placeholder(tf.float32, [1, 256])\n    \n    self.base_initial_lstm_state = tf.contrib.rnn.LSTMStateTuple(self.base_initial_lstm_state0,\n                                                                 self.base_initial_lstm_state1)\n\n    self.base_lstm_outputs, self.base_lstm_state = \\\n        self._base_lstm_layer(base_conv_output,\n                              self.base_last_action_reward_input,\n                              self.base_initial_lstm_state)\n\n    self.base_pi = self._base_policy_layer(self.base_lstm_outputs) # policy output\n    self.base_v  = self._base_value_layer(self.base_lstm_outputs)  # value output\n\n    \n  def _base_conv_layers(self, state_input, reuse=False):\n    with tf.variable_scope(""base_conv"", reuse=reuse) as scope:\n      # Weights\n      W_conv1, b_conv1 = self._conv_variable([8, 8, 3, 16],  ""base_conv1"")\n      W_conv2, b_conv2 = self._conv_variable([4, 4, 16, 32], ""base_conv2"")\n\n      # Nodes\n      h_conv1 = tf.nn.relu(self._conv2d(state_input, W_conv1, 4) + b_conv1) # stride=4\n      h_conv2 = tf.nn.relu(self._conv2d(h_conv1,     W_conv2, 2) + b_conv2) # stride=2\n      return h_conv2\n  \n  \n  def _base_lstm_layer(self, conv_output, last_action_reward_input, initial_state_input,\n                       reuse=False):\n    with tf.variable_scope(""base_lstm"", reuse=reuse) as scope:\n      # Weights\n      W_fc1, b_fc1 = self._fc_variable([2592, 256], ""base_fc1"")\n\n      # Nodes\n      conv_output_flat = tf.reshape(conv_output, [-1, 2592])\n      # (-1,9,9,32) -> (-1,2592)\n      conv_output_fc = tf.nn.relu(tf.matmul(conv_output_flat, W_fc1) + b_fc1)\n      # (unroll_step, 256)\n\n      step_size = tf.shape(conv_output_fc)[:1]\n\n      lstm_input = tf.concat([conv_output_fc, last_action_reward_input], 1)\n      # (unroll_step, 256+action_size+1)\n\n      lstm_input_reshaped = tf.reshape(lstm_input, [1, -1, 256+self._action_size+1])\n      # (1, unroll_step, 256+action_size+1)\n\n      lstm_outputs, lstm_state = tf.nn.dynamic_rnn(self.lstm_cell,\n                                                   lstm_input_reshaped,\n                                                   initial_state = initial_state_input,\n                                                   sequence_length = step_size,\n                                                   time_major = False,\n                                                   scope = scope)\n      \n      lstm_outputs = tf.reshape(lstm_outputs, [-1,256])\n      #(1,unroll_step,256) for back prop, (1,1,256) for forward prop.\n      return lstm_outputs, lstm_state\n\n\n  def _base_policy_layer(self, lstm_outputs, reuse=False):\n    with tf.variable_scope(""base_policy"", reuse=reuse) as scope:\n      # Weight for policy output layer\n      W_fc_p, b_fc_p = self._fc_variable([256, self._action_size], ""base_fc_p"")\n      # Policy (output)\n      base_pi = tf.nn.softmax(tf.matmul(lstm_outputs, W_fc_p) + b_fc_p)\n      return base_pi\n\n\n  def _base_value_layer(self, lstm_outputs, reuse=False):\n    with tf.variable_scope(""base_value"", reuse=reuse) as scope:\n      # Weight for value output layer\n      W_fc_v, b_fc_v = self._fc_variable([256, 1], ""base_fc_v"")\n      \n      # Value (output)\n      v_ = tf.matmul(lstm_outputs, W_fc_v) + b_fc_v\n      base_v = tf.reshape( v_, [-1] )\n      return base_v\n\n\n  def _create_pc_network(self):\n    # State (Image input) \n    self.pc_input = tf.placeholder(""float"", [None, 84, 84, 3])\n\n    # Last action and reward\n    self.pc_last_action_reward_input = tf.placeholder(""float"", [None, self._action_size+1])\n\n    # pc conv layers\n    pc_conv_output = self._base_conv_layers(self.pc_input, reuse=True)\n\n    # pc lastm layers\n    pc_initial_lstm_state = self.lstm_cell.zero_state(1, tf.float32)\n    # (Initial state is always reset.)\n    \n    pc_lstm_outputs, _ = self._base_lstm_layer(pc_conv_output,\n                                               self.pc_last_action_reward_input,\n                                               pc_initial_lstm_state,\n                                               reuse=True)\n    \n    self.pc_q, self.pc_q_max = self._pc_deconv_layers(pc_lstm_outputs)\n\n    \n  def _create_pc_network_for_display(self):\n    self.pc_q_disp, self.pc_q_max_disp = self._pc_deconv_layers(self.base_lstm_outputs, reuse=True)\n    \n  \n  def _pc_deconv_layers(self, lstm_outputs, reuse=False):\n    with tf.variable_scope(""pc_deconv"", reuse=reuse) as scope:    \n      # (Spatial map was written as 7x7x32, but here 9x9x32 is used to get 20x20 deconv result?)\n      # State (image input for pixel change)\n      W_pc_fc1, b_pc_fc1 = self._fc_variable([256, 9*9*32], ""pc_fc1"")\n        \n      W_pc_deconv_v, b_pc_deconv_v = self._conv_variable([4, 4, 1, 32],\n                                                         ""pc_deconv_v"", deconv=True)\n      W_pc_deconv_a, b_pc_deconv_a = self._conv_variable([4, 4, self._action_size, 32],\n                                                         ""pc_deconv_a"", deconv=True)\n      \n      h_pc_fc1 = tf.nn.relu(tf.matmul(lstm_outputs, W_pc_fc1) + b_pc_fc1)\n      h_pc_fc1_reshaped = tf.reshape(h_pc_fc1, [-1,9,9,32])\n      # Dueling network for V and Advantage\n      h_pc_deconv_v = tf.nn.relu(self._deconv2d(h_pc_fc1_reshaped,\n                                                W_pc_deconv_v, 9, 9, 2) +\n                                 b_pc_deconv_v)\n      h_pc_deconv_a = tf.nn.relu(self._deconv2d(h_pc_fc1_reshaped,\n                                                W_pc_deconv_a, 9, 9, 2) +\n                                 b_pc_deconv_a)\n      # Advantage mean\n      h_pc_deconv_a_mean = tf.reduce_mean(h_pc_deconv_a, reduction_indices=3, keep_dims=True)\n\n      # {Pixel change Q (output)\n      pc_q = h_pc_deconv_v + h_pc_deconv_a - h_pc_deconv_a_mean\n      #(-1, 20, 20, action_size)\n\n      # Max Q\n      pc_q_max = tf.reduce_max(pc_q, reduction_indices=3, keep_dims=False)\n      #(-1, 20, 20)\n\n      return pc_q, pc_q_max\n    \n\n  def _create_vr_network(self):\n    # State (Image input)\n    self.vr_input = tf.placeholder(""float"", [None, 84, 84, 3])\n\n    # Last action and reward\n    self.vr_last_action_reward_input = tf.placeholder(""float"", [None, self._action_size+1])\n\n    # VR conv layers\n    vr_conv_output = self._base_conv_layers(self.vr_input, reuse=True)\n\n    # pc lastm layers\n    vr_initial_lstm_state = self.lstm_cell.zero_state(1, tf.float32)\n    # (Initial state is always reset.)\n    \n    vr_lstm_outputs, _ = self._base_lstm_layer(vr_conv_output,\n                                               self.vr_last_action_reward_input,\n                                               vr_initial_lstm_state,\n                                               reuse=True)\n    # value output\n    self.vr_v  = self._base_value_layer(vr_lstm_outputs, reuse=True)\n\n    \n  def _create_rp_network(self):\n    self.rp_input = tf.placeholder(""float"", [3, 84, 84, 3])\n\n    # RP conv layers\n    rp_conv_output = self._base_conv_layers(self.rp_input, reuse=True)\n    rp_conv_output_reshaped = tf.reshape(rp_conv_output, [1,9*9*32*3])\n    \n    with tf.variable_scope(""rp_fc"") as scope:\n      # Weights\n      W_fc1, b_fc1 = self._fc_variable([9*9*32*3, 3], ""rp_fc1"")\n\n    # Reawrd prediction class output. (zero, positive, negative)\n    self.rp_c = tf.nn.softmax(tf.matmul(rp_conv_output_reshaped, W_fc1) + b_fc1)\n    # (1,3)\n\n  def _base_loss(self):\n    # [base A3C]\n    # Taken action (input for policy)\n    self.base_a = tf.placeholder(""float"", [None, self._action_size])\n    \n    # Advantage (R-V) (input for policy)\n    self.base_adv = tf.placeholder(""float"", [None])\n    \n    # Avoid NaN with clipping when value in pi becomes zero\n    log_pi = tf.log(tf.clip_by_value(self.base_pi, 1e-20, 1.0))\n    \n    # Policy entropy\n    entropy = -tf.reduce_sum(self.base_pi * log_pi, reduction_indices=1)\n    \n    # Policy loss (output)\n    policy_loss = -tf.reduce_sum( tf.reduce_sum( tf.multiply( log_pi, self.base_a ),\n                                                 reduction_indices=1 ) *\n                                  self.base_adv + entropy * self._entropy_beta)\n    \n    # R (input for value target)\n    self.base_r = tf.placeholder(""float"", [None])\n    \n    # Value loss (output)\n    # (Learning rate for Critic is half of Actor\'s, so multiply by 0.5)\n    value_loss = 0.5 * tf.nn.l2_loss(self.base_r - self.base_v)\n    \n    base_loss = policy_loss + value_loss\n    return base_loss\n\n  \n  def _pc_loss(self):\n    # [pixel change]\n    self.pc_a = tf.placeholder(""float"", [None, self._action_size])\n    pc_a_reshaped = tf.reshape(self.pc_a, [-1, 1, 1, self._action_size])\n\n    # Extract Q for taken action\n    pc_qa_ = tf.multiply(self.pc_q, pc_a_reshaped)\n    pc_qa = tf.reduce_sum(pc_qa_, reduction_indices=3, keep_dims=False)\n    # (-1, 20, 20)\n      \n    # TD target for Q\n    self.pc_r = tf.placeholder(""float"", [None, 20, 20])\n\n    pc_loss = self._pixel_change_lambda * tf.nn.l2_loss(self.pc_r - pc_qa)\n    return pc_loss\n\n  \n  def _vr_loss(self):\n    # R (input for value)\n    self.vr_r = tf.placeholder(""float"", [None])\n    \n    # Value loss (output)\n    vr_loss = tf.nn.l2_loss(self.vr_r - self.vr_v)\n    return vr_loss\n\n\n  def _rp_loss(self):\n    # reward prediction target. one hot vector\n    self.rp_c_target = tf.placeholder(""float"", [1,3])\n    \n    # Reward prediction loss (output)\n    rp_c = tf.clip_by_value(self.rp_c, 1e-20, 1.0)\n    rp_loss = -tf.reduce_sum(self.rp_c_target * tf.log(rp_c))\n    return rp_loss\n    \n    \n  def prepare_loss(self):\n    with tf.device(self._device):\n      loss = self._base_loss()\n      \n      if self._use_pixel_change:\n        pc_loss = self._pc_loss()\n        loss = loss + pc_loss\n\n      if self._use_value_replay:\n        vr_loss = self._vr_loss()\n        loss = loss + vr_loss\n\n      if self._use_reward_prediction:\n        rp_loss = self._rp_loss()\n        loss = loss + rp_loss\n      \n      self.total_loss = loss\n\n\n  def reset_state(self):\n    self.base_lstm_state_out = tf.contrib.rnn.LSTMStateTuple(np.zeros([1, 256]),\n                                                             np.zeros([1, 256]))\n\n  def run_base_policy_and_value(self, sess, s_t, last_action_reward):\n    # This run_base_policy_and_value() is used when forward propagating.\n    # so the step size is 1.\n    pi_out, v_out, self.base_lstm_state_out = sess.run( [self.base_pi, self.base_v, self.base_lstm_state],\n                                                        feed_dict = {self.base_input : [s_t],\n                                                                     self.base_last_action_reward_input : [last_action_reward],\n                                                                     self.base_initial_lstm_state0 : self.base_lstm_state_out[0],\n                                                                     self.base_initial_lstm_state1 : self.base_lstm_state_out[1]} )\n    # pi_out: (1,3), v_out: (1)\n    return (pi_out[0], v_out[0])\n\n  \n  def run_base_policy_value_pc_q(self, sess, s_t, last_action_reward):\n    # For display tool.\n    pi_out, v_out, self.base_lstm_state_out, q_disp_out, q_max_disp_out = \\\n        sess.run( [self.base_pi, self.base_v, self.base_lstm_state, self.pc_q_disp, self.pc_q_max_disp],\n                  feed_dict = {self.base_input : [s_t],\n                               self.base_last_action_reward_input : [last_action_reward],\n                               self.base_initial_lstm_state0 : self.base_lstm_state_out[0],\n                               self.base_initial_lstm_state1 : self.base_lstm_state_out[1]} )\n    \n    # pi_out: (1,3), v_out: (1), q_disp_out(1,20,20, action_size)\n    return (pi_out[0], v_out[0], q_disp_out[0])\n\n  \n  def run_base_value(self, sess, s_t, last_action_reward):\n    # This run_bae_value() is used for calculating V for bootstrapping at the \n    # end of LOCAL_T_MAX time step sequence.\n    # When next sequence starts, V will be calculated again with the same state using updated network weights,\n    # so we don\'t update LSTM state here.\n    v_out, _ = sess.run( [self.base_v, self.base_lstm_state],\n                         feed_dict = {self.base_input : [s_t],\n                                      self.base_last_action_reward_input : [last_action_reward],\n                                      self.base_initial_lstm_state0 : self.base_lstm_state_out[0],\n                                      self.base_initial_lstm_state1 : self.base_lstm_state_out[1]} )\n    return v_out[0]\n\n  \n  def run_pc_q_max(self, sess, s_t, last_action_reward):\n    q_max_out = sess.run( self.pc_q_max,\n                          feed_dict = {self.pc_input : [s_t],\n                                       self.pc_last_action_reward_input : [last_action_reward]} )\n    return q_max_out[0]\n\n  \n  def run_vr_value(self, sess, s_t, last_action_reward):\n    vr_v_out = sess.run( self.vr_v,\n                         feed_dict = {self.vr_input : [s_t],\n                                      self.vr_last_action_reward_input : [last_action_reward]} )\n    return vr_v_out[0]\n\n  \n  def run_rp_c(self, sess, s_t):\n    # For display tool\n    rp_c_out = sess.run( self.rp_c,\n                         feed_dict = {self.rp_input : s_t} )\n    return rp_c_out[0]\n\n  \n  def get_vars(self):\n    return self.variables\n  \n\n  def sync_from(self, src_network, name=None):\n    src_vars = src_network.get_vars()\n    dst_vars = self.get_vars()\n\n    sync_ops = []\n\n    with tf.device(self._device):\n      with tf.name_scope(name, ""UnrealModel"",[]) as name:\n        for(src_var, dst_var) in zip(src_vars, dst_vars):\n          sync_op = tf.assign(dst_var, src_var)\n          sync_ops.append(sync_op)\n\n        return tf.group(*sync_ops, name=name)\n      \n\n  def _fc_variable(self, weight_shape, name):\n    name_w = ""W_{0}"".format(name)\n    name_b = ""b_{0}"".format(name)\n    \n    input_channels  = weight_shape[0]\n    output_channels = weight_shape[1]\n    bias_shape = [output_channels]\n\n    weight = tf.get_variable(name_w, weight_shape, initializer=fc_initializer(input_channels))\n    bias   = tf.get_variable(name_b, bias_shape,   initializer=fc_initializer(input_channels))\n    return weight, bias\n\n  \n  def _conv_variable(self, weight_shape, name, deconv=False):\n    name_w = ""W_{0}"".format(name)\n    name_b = ""b_{0}"".format(name)\n    \n    w = weight_shape[0]\n    h = weight_shape[1]\n    if deconv:\n      input_channels  = weight_shape[3]\n      output_channels = weight_shape[2]\n    else:\n      input_channels  = weight_shape[2]\n      output_channels = weight_shape[3]\n    bias_shape = [output_channels]\n\n    weight = tf.get_variable(name_w, weight_shape,\n                             initializer=conv_initializer(w, h, input_channels))\n    bias   = tf.get_variable(name_b, bias_shape,\n                             initializer=conv_initializer(w, h, input_channels))\n    return weight, bias\n\n  \n  def _conv2d(self, x, W, stride):\n    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = ""VALID"")\n\n\n  def _get2d_deconv_output_size(self,\n                                input_height, input_width,\n                                filter_height, filter_width,\n                                stride, padding_type):\n    if padding_type == \'VALID\':\n      out_height = (input_height - 1) * stride + filter_height\n      out_width  = (input_width  - 1) * stride + filter_width\n      \n    elif padding_type == \'SAME\':\n      out_height = input_height * stride\n      out_width  = input_width  * stride\n    \n    return out_height, out_width\n\n\n  def _deconv2d(self, x, W, input_width, input_height, stride):\n    filter_height = W.get_shape()[0].value\n    filter_width  = W.get_shape()[1].value\n    out_channel   = W.get_shape()[2].value\n    \n    out_height, out_width = self._get2d_deconv_output_size(input_height,\n                                                           input_width,\n                                                           filter_height,\n                                                           filter_width,\n                                                           stride,\n                                                           \'VALID\')\n    batch_size = tf.shape(x)[0]\n    output_shape = tf.stack([batch_size, out_height, out_width, out_channel])\n    return tf.nn.conv2d_transpose(x, W, output_shape,\n                                  strides=[1, stride, stride, 1],\n                                  padding=\'VALID\')\n'"
model/model_test.py,2,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport math\nimport tensorflow as tf\nfrom model import UnrealModel\n\nclass TestUnrealModel(tf.test.TestCase):\n  def test_unreal_variable_size(self):\n    """""" Check total variable size with all options ON """"""\n    use_pixel_change = True\n    use_value_replay = True\n    use_reward_prediction = True\n\n    # base: conv=4, fc=2, lstm=2, policy_fc=2, value_fc=2\n    # pc:   fc=2, deconv_v=2, deconv_a=2\n    # rp:   fc=2\n    self.check_model_var_size( use_pixel_change,\n                               use_value_replay,\n                               use_reward_prediction,\n                               20 )\n\n  def test_pc_variable_size(self):\n    """""" Check total variable size with only pixel change ON """"""\n    use_pixel_change = True\n    use_value_replay = False\n    use_reward_prediction = False\n\n    # base: conv=4, fc=2, lstm=2, policy_fc=2, value_fc=2\n    # pc:   fc=2, deconv_v=2, deconv_a=2\n    self.check_model_var_size( use_pixel_change,\n                               use_value_replay,\n                               use_reward_prediction,\n                               18 )\n\n  def test_vr_variable_size(self):\n    """""" Check total variable size with only value funciton replay ON """"""\n    use_pixel_change = False\n    use_value_replay = True\n    use_reward_prediction = False\n\n    # base: conv=4, fc=2, lstm=2, policy_fc=2, value_fc=2\n    self.check_model_var_size( use_pixel_change,\n                               use_value_replay,\n                               use_reward_prediction,\n                               12 )\n\n  def test_rp_variable_size(self):\n    """""" Check total variable size with only reward prediction ON """"""\n    use_pixel_change = False\n    use_value_replay = False\n    use_reward_prediction = True\n\n    # base: conv=4, fc=2, lstm=2, policy_fc=2, value_fc=2\n    # rp:   fc=2\n    self.check_model_var_size( use_pixel_change,\n                               use_value_replay,\n                               use_reward_prediction,\n                               14 )\n    \n  def check_model_var_size(self,\n                           use_pixel_change,\n                           use_value_replay,\n                           use_reward_prediction,\n                           var_size):\n    """""" Check variable size of the model """"""\n    \n    model = UnrealModel(1,\n                        -1,\n                        use_pixel_change,\n                        use_value_replay,\n                        use_reward_prediction,\n                        1.0,\n                        1.0,\n                        ""/cpu:0"");\n    variables = model.get_vars()\n    self.assertEqual( len(variables), var_size )\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n  \n'"
train/__init__.py,0,b''
train/experience.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom collections import deque\n\n\nclass ExperienceFrame(object):\n  def __init__(self, state, reward, action, terminal, pixel_change, last_action, last_reward):\n    self.state = state\n    self.action = action # (Taken action with the \'state\')\n    self.reward = np.clip(reward, -1, 1) # Reward with the \'state\'. (Clipped)\n    self.terminal = terminal # (Whether terminated when \'state\' was inputted)\n    self.pixel_change = pixel_change\n    self.last_action = last_action # (After this last action was taken, agent move to the \'state\')\n    self.last_reward = np.clip(last_reward, -1, 1) # (After this last reward was received, agent move to the \'state\') (Clipped)\n\n  def get_last_action_reward(self, action_size):\n    """"""\n    Return one hot vectored last action + last reward.\n    """"""\n    return ExperienceFrame.concat_action_and_reward(self.last_action, action_size,\n                                                    self.last_reward)\n\n  def get_action_reward(self, action_size):\n    """"""\n    Return one hot vectored action + reward.\n    """"""\n    return ExperienceFrame.concat_action_and_reward(self.action, action_size,\n                                                    self.reward)\n\n  @staticmethod\n  def concat_action_and_reward(action, action_size, reward):\n    """"""\n    Return one hot vectored action and reward.\n    """"""\n    action_reward = np.zeros([action_size+1])\n    action_reward[action] = 1.0\n    action_reward[-1] = float(reward)\n    return action_reward\n  \n\nclass Experience(object):\n  def __init__(self, history_size):\n    self._history_size = history_size\n    self._frames = deque(maxlen=history_size)\n    # frame indices for zero rewards\n    self._zero_reward_indices = deque()\n    # frame indices for non zero rewards\n    self._non_zero_reward_indices = deque()\n    self._top_frame_index = 0\n\n\n  def add_frame(self, frame):\n    if frame.terminal and len(self._frames) > 0 and self._frames[-1].terminal:\n      # Discard if terminal frame continues\n      print(""Terminal frames continued."")\n      return\n\n    frame_index = self._top_frame_index + len(self._frames)\n    was_full = self.is_full()\n\n    # append frame\n    self._frames.append(frame)\n\n    # append index\n    if frame_index >= 3:\n      if frame.reward == 0:\n        self._zero_reward_indices.append(frame_index)\n      else:\n        self._non_zero_reward_indices.append(frame_index)\n    \n    if was_full:\n      self._top_frame_index += 1\n\n      cut_frame_index = self._top_frame_index + 3\n      # Cut frame if its index is lower than cut_frame_index.\n      if len(self._zero_reward_indices) > 0 and \\\n         self._zero_reward_indices[0] < cut_frame_index:\n        self._zero_reward_indices.popleft()\n        \n      if len(self._non_zero_reward_indices) > 0 and \\\n         self._non_zero_reward_indices[0] < cut_frame_index:\n        self._non_zero_reward_indices.popleft()\n\n\n  def is_full(self):\n    return len(self._frames) >= self._history_size\n\n\n  def sample_sequence(self, sequence_size):\n    # -1 for the case if start pos is the terminated frame.\n    # (Then +1 not to start from terminated frame.)\n    start_pos = np.random.randint(0, self._history_size - sequence_size -1)\n\n    if self._frames[start_pos].terminal:\n      start_pos += 1\n      # Assuming that there are no successive terminal frames.\n\n    sampled_frames = []\n    \n    for i in range(sequence_size):\n      frame = self._frames[start_pos+i]\n      sampled_frames.append(frame)\n      if frame.terminal:\n        break\n    \n    return sampled_frames\n\n  \n  def sample_rp_sequence(self):\n    """"""\n    Sample 4 successive frames for reward prediction.\n    """"""\n    if np.random.randint(2) == 0:\n      from_zero = True\n    else:\n      from_zero = False\n    \n    if len(self._zero_reward_indices) == 0:\n      # zero rewards container was empty\n      from_zero = False\n    elif len(self._non_zero_reward_indices) == 0:\n      # non zero rewards container was empty\n      from_zero = True\n\n    if from_zero:\n      index = np.random.randint(len(self._zero_reward_indices))\n      end_frame_index = self._zero_reward_indices[index]\n    else:\n      index = np.random.randint(len(self._non_zero_reward_indices))\n      end_frame_index = self._non_zero_reward_indices[index]\n\n    start_frame_index = end_frame_index-3\n    raw_start_frame_index = start_frame_index - self._top_frame_index\n\n    sampled_frames = []\n    \n    for i in range(4):\n      frame = self._frames[raw_start_frame_index+i]\n      sampled_frames.append(frame)\n\n    return sampled_frames\n'"
train/experience_test.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom train.experience import Experience, ExperienceFrame\n\n\nclass TestExperience(unittest.TestCase):\n  def _add_frame(self, experience, reward):\n    frame = ExperienceFrame(0, reward, 0, False, 0, 0, 0)\n    experience.add_frame(frame)\n    \n  def test_process(self):\n    experience = Experience(10)\n\n    for i in range(10):\n      if i == 5:\n        self._add_frame(experience, 1)\n      else:\n        self._add_frame(experience, 0)\n\n    self.assertTrue( experience.is_full() )\n    self.assertTrue( experience._top_frame_index == 0 )\n    \n    self._add_frame(experience, 0)\n\n    self.assertTrue( experience._top_frame_index == 1 )\n\n    for i in range(100):\n      frames = experience.sample_rp_sequence()\n      self.assertTrue( len(frames) == 4 )\n      # Reward should be shewed here.\n      #print(frames[3].reward)\n\nif __name__ == '__main__':\n  unittest.main()\n"""
train/rmsprop_applier.py,12,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.training import training_ops\nfrom tensorflow.python.training import slot_creator\n\n\nclass RMSPropApplier(object):\n\n  def __init__(self,\n               learning_rate,\n               decay=0.9,\n               momentum=0.0,\n               epsilon=1e-10,\n               clip_norm=40.0,\n               device=""/cpu:0"",\n               name=""RMSPropApplier""):\n\n    self._name = name\n    self._learning_rate = learning_rate\n    self._decay = decay\n    self._momentum = momentum\n    self._epsilon = epsilon\n    self._clip_norm = clip_norm\n    self._device = device\n\n    # Tensors for learning rate and momentum.  Created in _prepare.\n    self._learning_rate_tensor = None\n    self._decay_tensor = None\n    self._momentum_tensor = None\n    self._epsilon_tensor = None\n\n    self._slots = {}\n\n  def _create_slots(self, var_list):\n    for v in var_list:\n      # \'val\' is Variable\'s initial value tensor.\n      val = tf.constant(1.0, dtype=v.dtype, shape=v.get_shape())\n      self._get_or_make_slot(v, val, ""rms"", self._name)\n      self._zeros_slot(v, ""momentum"", self._name)\n\n  def _prepare(self):\n      self._learning_rate_tensor = tf.convert_to_tensor(self._learning_rate,\n                                                      name=""learning_rate"")\n      self._decay_tensor = tf.convert_to_tensor(self._decay, name=""decay"")\n      self._momentum_tensor = tf.convert_to_tensor(self._momentum,\n                                                 name=""momentum"")\n      self._epsilon_tensor = tf.convert_to_tensor(self._epsilon,\n                                                name=""epsilon"")\n\n  def _slot_dict(self, slot_name):\n    named_slots = self._slots.get(slot_name, None)\n    if named_slots is None:\n      named_slots = {}\n      self._slots[slot_name] = named_slots\n    return named_slots\n\n  def _get_or_make_slot(self, var, val, slot_name, op_name):\n    named_slots = self._slot_dict(slot_name)\n    if var not in named_slots:\n      named_slots[var] = slot_creator.create_slot(var, val, op_name)\n    return named_slots[var]\n\n  def get_slot(self, var, name):\n    named_slots = self._slots.get(name, None)\n    if not named_slots:\n      return None\n    return named_slots.get(var, None)\n\n  def _zeros_slot(self, var, slot_name, op_name):\n    named_slots = self._slot_dict(slot_name)\n    if var not in named_slots:\n      named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\n    return named_slots[var]\n\n  # TODO: in RMSProp native code, memcpy() (for CPU) and\n  # cudaMemcpyAsync() (for GPU) are used when updating values,\n  # and values might tend to be overwritten with results from other threads.\n  # (Need to check the learning performance with replacing it)  \n  def _apply_dense(self, grad, var):\n    rms = self.get_slot(var, ""rms"")\n    mom = self.get_slot(var, ""momentum"")\n    return training_ops.apply_rms_prop(\n      var, rms, mom,\n      self._learning_rate_tensor,\n      self._decay_tensor,\n      self._momentum_tensor,\n      self._epsilon_tensor,\n      grad,\n      use_locking=False).op\n\n  def minimize_local(self, loss, global_var_list, local_var_list):\n    """"""\n    minimize loss and apply gradients to global vars.\n    """"""\n    with tf.device(self._device):\n      var_refs = [v._ref() for v in local_var_list]\n      local_gradients = tf.gradients(\n        loss, var_refs,\n        gate_gradients=False,\n        aggregation_method=None,\n        colocate_gradients_with_ops=False)\n      return self._apply_gradients(global_var_list, local_gradients)\n\n  # Apply gradients to var.\n  def _apply_gradients(self, global_var_list, local_grad_list, name=None):\n    update_ops = []\n\n    with tf.control_dependencies(None):\n      self._create_slots(global_var_list)\n\n    # global gradient norm clipping\n    local_grad_list, _ =  tf.clip_by_global_norm(local_grad_list, self._clip_norm)\n\n    with tf.name_scope(name, self._name,[]) as name:\n      self._prepare()\n      for var, grad in zip(global_var_list, local_grad_list):\n        with tf.name_scope(""update_"" + var.op.name), tf.device(var.device):\n          update_ops.append(self._apply_dense(grad, var))\n      return tf.group(*update_ops, name=name)\n'"
train/rmsprop_applier_test.py,6,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport math\nimport tensorflow as tf\nimport rmsprop_applier\n\nclass TestRMSPropApplier(tf.test.TestCase):\n  def test_apply(self):\n    with self.test_session():\n      var = tf.Variable([1.0, 2.0])\n      \n      grad0 = tf.Variable([2.0, 4.0])\n      grad1 = tf.Variable([3.0, 6.0])\n      \n      opt = rmsprop_applier.RMSPropApplier(learning_rate=2.0,\n                                           decay=0.9,\n                                           momentum=0.0,\n                                           epsilon=1.0)\n      \n      apply_gradient0 = opt._apply_gradients([var], [grad0])\n      apply_gradient1 = opt._apply_gradients([var], [grad1])\n\n      tf.global_variables_initializer().run()\n\n      # apply grad0\n      apply_gradient0.run()\n\n      ms_x = 1.0\n      ms_y = 1.0\n\n      x = 1.0\n      y = 2.0\n      dx = 2.0\n      dy = 4.0\n      ms_x = ms_x + (dx * dx - ms_x) * (1.0 - 0.9)\n      ms_y = ms_y + (dy * dy - ms_y) * (1.0 - 0.9)\n      x = x - (2.0 * dx / math.sqrt(ms_x+1.0))\n      y = y - (2.0 * dy / math.sqrt(ms_y+1.0))\n\n      self.assertAllClose(np.array([x, y]), var.eval())\n\n      # apply grad1\n      apply_gradient1.run()\n\n      dx = 3.0\n      dy = 6.0\n      ms_x = ms_x + (dx * dx - ms_x) * (1.0 - 0.9)\n      ms_y = ms_y + (dy * dy - ms_y) * (1.0 - 0.9)\n      x = x - (2.0 * dx / math.sqrt(ms_x+1.0))\n      y = y - (2.0 * dy / math.sqrt(ms_y+1.0))\n      \n      self.assertAllClose(np.array([x, y]), var.eval())\n      \nif __name__ == ""__main__"":\n  tf.test.main()\n'"
train/trainer.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport time\n\nfrom environment.environment import Environment\nfrom model.model import UnrealModel\nfrom train.experience import Experience, ExperienceFrame\n\nLOG_INTERVAL = 100\nPERFORMANCE_LOG_INTERVAL = 1000\n\n\nclass Trainer(object):\n  def __init__(self,\n               thread_index,\n               global_network,\n               initial_learning_rate,\n               learning_rate_input,\n               grad_applier,\n               env_type,\n               env_name,\n               use_pixel_change,\n               use_value_replay,\n               use_reward_prediction,\n               pixel_change_lambda,\n               entropy_beta,\n               local_t_max,\n               gamma,\n               gamma_pc,\n               experience_history_size,\n               max_global_time_step,\n               device):\n\n    self.thread_index = thread_index\n    self.learning_rate_input = learning_rate_input\n    self.env_type = env_type\n    self.env_name = env_name\n    self.use_pixel_change = use_pixel_change\n    self.use_value_replay = use_value_replay\n    self.use_reward_prediction = use_reward_prediction\n    self.local_t_max = local_t_max\n    self.gamma = gamma\n    self.gamma_pc = gamma_pc\n    self.experience_history_size = experience_history_size\n    self.max_global_time_step = max_global_time_step\n    self.action_size = Environment.get_action_size(env_type, env_name)\n    \n    self.local_network = UnrealModel(self.action_size,\n                                     thread_index,\n                                     use_pixel_change,\n                                     use_value_replay,\n                                     use_reward_prediction,\n                                     pixel_change_lambda,\n                                     entropy_beta,\n                                     device)\n    self.local_network.prepare_loss()\n\n    self.apply_gradients = grad_applier.minimize_local(self.local_network.total_loss,\n                                                       global_network.get_vars(),\n                                                       self.local_network.get_vars())\n    \n    self.sync = self.local_network.sync_from(global_network)\n    self.experience = Experience(self.experience_history_size)\n    self.local_t = 0\n    self.initial_learning_rate = initial_learning_rate\n    self.episode_reward = 0\n    # For log output\n    self.prev_local_t = 0\n\n  def prepare(self):\n    self.environment = Environment.create_environment(self.env_type,\n                                                      self.env_name)\n\n  def stop(self):\n    self.environment.stop()\n    \n  def _anneal_learning_rate(self, global_time_step):\n    learning_rate = self.initial_learning_rate * (self.max_global_time_step - global_time_step) / self.max_global_time_step\n    if learning_rate < 0.0:\n      learning_rate = 0.0\n    return learning_rate\n\n  \n  def choose_action(self, pi_values):\n    return np.random.choice(range(len(pi_values)), p=pi_values)\n\n  \n  def _record_score(self, sess, summary_writer, summary_op, score_input, score, global_t):\n    summary_str = sess.run(summary_op, feed_dict={\n      score_input: score\n    })\n    summary_writer.add_summary(summary_str, global_t)\n    summary_writer.flush()\n\n    \n  def set_start_time(self, start_time):\n    self.start_time = start_time\n\n\n  def _fill_experience(self, sess):\n    """"""\n    Fill experience buffer until buffer is full.\n    """"""\n    prev_state = self.environment.last_state\n    last_action = self.environment.last_action\n    last_reward = self.environment.last_reward\n    last_action_reward = ExperienceFrame.concat_action_and_reward(last_action,\n                                                                  self.action_size,\n                                                                  last_reward)\n    \n    pi_, _ = self.local_network.run_base_policy_and_value(sess,\n                                                          self.environment.last_state,\n                                                          last_action_reward)\n    action = self.choose_action(pi_)\n    \n    new_state, reward, terminal, pixel_change = self.environment.process(action)\n    \n    frame = ExperienceFrame(prev_state, reward, action, terminal, pixel_change,\n                            last_action, last_reward)\n    self.experience.add_frame(frame)\n    \n    if terminal:\n      self.environment.reset()\n    if self.experience.is_full():\n      self.environment.reset()\n      print(""Replay buffer filled"")\n\n\n  def _print_log(self, global_t):\n    if (self.thread_index == 0) and (self.local_t - self.prev_local_t >= PERFORMANCE_LOG_INTERVAL):\n      self.prev_local_t += PERFORMANCE_LOG_INTERVAL\n      elapsed_time = time.time() - self.start_time\n      steps_per_sec = global_t / elapsed_time\n      print(""### Performance : {} STEPS in {:.0f} sec. {:.0f} STEPS/sec. {:.2f}M STEPS/hour"".format(\n        global_t,  elapsed_time, steps_per_sec, steps_per_sec * 3600 / 1000000.))\n    \n\n  def _process_base(self, sess, global_t, summary_writer, summary_op, score_input):\n    # [Base A3C]\n    states = []\n    last_action_rewards = []\n    actions = []\n    rewards = []\n    values = []\n\n    terminal_end = False\n\n    start_lstm_state = self.local_network.base_lstm_state_out\n\n    # t_max times loop\n    for _ in range(self.local_t_max):\n      # Prepare last action reward\n      last_action = self.environment.last_action\n      last_reward = self.environment.last_reward\n      last_action_reward = ExperienceFrame.concat_action_and_reward(last_action,\n                                                                    self.action_size,\n                                                                    last_reward)\n      \n      pi_, value_ = self.local_network.run_base_policy_and_value(sess,\n                                                                 self.environment.last_state,\n                                                                 last_action_reward)\n      \n      \n      action = self.choose_action(pi_)\n\n      states.append(self.environment.last_state)\n      last_action_rewards.append(last_action_reward)\n      actions.append(action)\n      values.append(value_)\n\n      if (self.thread_index == 0) and (self.local_t % LOG_INTERVAL == 0):\n        print(""pi={}"".format(pi_))\n        print("" V={}"".format(value_))\n\n      prev_state = self.environment.last_state\n\n      # Process game\n      new_state, reward, terminal, pixel_change = self.environment.process(action)\n      frame = ExperienceFrame(prev_state, reward, action, terminal, pixel_change,\n                              last_action, last_reward)\n\n      # Store to experience\n      self.experience.add_frame(frame)\n\n      self.episode_reward += reward\n\n      rewards.append( reward )\n\n      self.local_t += 1\n\n      if terminal:\n        terminal_end = True\n        print(""score={}"".format(self.episode_reward))\n\n        self._record_score(sess, summary_writer, summary_op, score_input,\n                           self.episode_reward, global_t)\n          \n        self.episode_reward = 0\n        self.environment.reset()\n        self.local_network.reset_state()\n        break\n\n    R = 0.0\n    if not terminal_end:\n      R = self.local_network.run_base_value(sess, new_state, frame.get_action_reward(self.action_size))\n\n    actions.reverse()\n    states.reverse()\n    rewards.reverse()\n    values.reverse()\n\n    batch_si = []\n    batch_a = []\n    batch_adv = []\n    batch_R = []\n\n    for(ai, ri, si, Vi) in zip(actions, rewards, states, values):\n      R = ri + self.gamma * R\n      adv = R - Vi\n      a = np.zeros([self.action_size])\n      a[ai] = 1.0\n\n      batch_si.append(si)\n      batch_a.append(a)\n      batch_adv.append(adv)\n      batch_R.append(R)\n\n    batch_si.reverse()\n    batch_a.reverse()\n    batch_adv.reverse()\n    batch_R.reverse()\n    \n    return batch_si, last_action_rewards, batch_a, batch_adv, batch_R, start_lstm_state\n\n  \n  def _process_pc(self, sess):\n    # [pixel change]\n    # Sample 20+1 frame (+1 for last next state)\n    pc_experience_frames = self.experience.sample_sequence(self.local_t_max+1)\n    # Reverse sequence to calculate from the last\n    pc_experience_frames.reverse()\n\n    batch_pc_si = []\n    batch_pc_a = []\n    batch_pc_R = []\n    batch_pc_last_action_reward = []\n    \n    pc_R = np.zeros([20,20], dtype=np.float32)\n    if not pc_experience_frames[1].terminal:\n      pc_R = self.local_network.run_pc_q_max(sess,\n                                             pc_experience_frames[0].state,\n                                             pc_experience_frames[0].get_last_action_reward(self.action_size))\n\n\n    for frame in pc_experience_frames[1:]:\n      pc_R = frame.pixel_change + self.gamma_pc * pc_R\n      a = np.zeros([self.action_size])\n      a[frame.action] = 1.0\n      last_action_reward = frame.get_last_action_reward(self.action_size)\n      \n      batch_pc_si.append(frame.state)\n      batch_pc_a.append(a)\n      batch_pc_R.append(pc_R)\n      batch_pc_last_action_reward.append(last_action_reward)\n\n    batch_pc_si.reverse()\n    batch_pc_a.reverse()\n    batch_pc_R.reverse()\n    batch_pc_last_action_reward.reverse()\n    \n    return batch_pc_si, batch_pc_last_action_reward, batch_pc_a, batch_pc_R\n\n  \n  def _process_vr(self, sess):\n    # [Value replay]\n    # Sample 20+1 frame (+1 for last next state)\n    vr_experience_frames = self.experience.sample_sequence(self.local_t_max+1)\n    # Reverse sequence to calculate from the last\n    vr_experience_frames.reverse()\n\n    batch_vr_si = []\n    batch_vr_R = []\n    batch_vr_last_action_reward = []\n\n    vr_R = 0.0\n    if not vr_experience_frames[1].terminal:\n      vr_R = self.local_network.run_vr_value(sess,\n                                             vr_experience_frames[0].state,\n                                             vr_experience_frames[0].get_last_action_reward(self.action_size))\n    \n    # t_max times loop\n    for frame in vr_experience_frames[1:]:\n      vr_R = frame.reward + self.gamma * vr_R\n      batch_vr_si.append(frame.state)\n      batch_vr_R.append(vr_R)\n      last_action_reward = frame.get_last_action_reward(self.action_size)\n      batch_vr_last_action_reward.append(last_action_reward)\n\n    batch_vr_si.reverse()\n    batch_vr_R.reverse()\n    batch_vr_last_action_reward.reverse()\n\n    return batch_vr_si, batch_vr_last_action_reward, batch_vr_R\n\n  \n  def _process_rp(self):\n    # [Reward prediction]\n    rp_experience_frames = self.experience.sample_rp_sequence()\n    # 4 frames\n\n    batch_rp_si = []\n    batch_rp_c = []\n    \n    for i in range(3):\n      batch_rp_si.append(rp_experience_frames[i].state)\n\n    # one hot vector for target reward\n    r = rp_experience_frames[3].reward\n    rp_c = [0.0, 0.0, 0.0]\n    if r == 0:\n      rp_c[0] = 1.0 # zero\n    elif r > 0:\n      rp_c[1] = 1.0 # positive\n    else:\n      rp_c[2] = 1.0 # negative\n    batch_rp_c.append(rp_c)\n    return batch_rp_si, batch_rp_c\n  \n  \n  def process(self, sess, global_t, summary_writer, summary_op, score_input):\n    # Fill experience replay buffer\n    if not self.experience.is_full():\n      self._fill_experience(sess)\n      return 0\n\n    start_local_t = self.local_t\n\n    cur_learning_rate = self._anneal_learning_rate(global_t)\n\n    # Copy weights from shared to local\n    sess.run( self.sync )\n\n    # [Base]\n    batch_si, batch_last_action_rewards, batch_a, batch_adv, batch_R, start_lstm_state = \\\n          self._process_base(sess,\n                             global_t,\n                             summary_writer,\n                             summary_op,\n                             score_input)\n    feed_dict = {\n      self.local_network.base_input: batch_si,\n      self.local_network.base_last_action_reward_input: batch_last_action_rewards,\n      self.local_network.base_a: batch_a,\n      self.local_network.base_adv: batch_adv,\n      self.local_network.base_r: batch_R,\n      self.local_network.base_initial_lstm_state: start_lstm_state,\n      # [common]\n      self.learning_rate_input: cur_learning_rate\n    }\n\n    # [Pixel change]\n    if self.use_pixel_change:\n      batch_pc_si, batch_pc_last_action_reward, batch_pc_a, batch_pc_R = self._process_pc(sess)\n\n      pc_feed_dict = {\n        self.local_network.pc_input: batch_pc_si,\n        self.local_network.pc_last_action_reward_input: batch_pc_last_action_reward,\n        self.local_network.pc_a: batch_pc_a,\n        self.local_network.pc_r: batch_pc_R\n      }\n      feed_dict.update(pc_feed_dict)\n\n    # [Value replay]\n    if self.use_value_replay:\n      batch_vr_si, batch_vr_last_action_reward, batch_vr_R = self._process_vr(sess)\n      \n      vr_feed_dict = {\n        self.local_network.vr_input: batch_vr_si,\n        self.local_network.vr_last_action_reward_input : batch_vr_last_action_reward,\n        self.local_network.vr_r: batch_vr_R\n      }\n      feed_dict.update(vr_feed_dict)\n\n    # [Reward prediction]\n    if self.use_reward_prediction:\n      batch_rp_si, batch_rp_c = self._process_rp()\n      rp_feed_dict = {\n        self.local_network.rp_input: batch_rp_si,\n        self.local_network.rp_c_target: batch_rp_c\n      }\n      feed_dict.update(rp_feed_dict)\n\n    # Calculate gradients and copy them to global network.\n    sess.run( self.apply_gradients, feed_dict=feed_dict )\n    \n    self._print_log(global_t)\n    \n    # Return advanced local step size\n    diff_local_t = self.local_t - start_local_t\n    return diff_local_t\n'"
