file_path,api_count,code
setup.py,0,"b'import os\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n\ndef read(file_name):\n    with open(os.path.join(os.path.dirname(__file__), file_name), \'r\') as f:\n        return f.readlines()\n\nsetup(\n    name=\'SimGAN\',\n    version=\'0.0.0\',\n    description=\'Implementation of Apple\\\'s Learning from Simulated and Unsupervised Images through Adversarial \'\n                \'Training.\',\n    # long_description=read(\'README.md\'),\n    url=\'https://github.com/wayaai/SimGAN.git\',\n    author=\'Michael Dietz\',\n    keywords=\'\',\n    packages=find_packages(exclude=[""tests.*"", ""tests""]))\n'"
sim-gan.py,6,"b'""""""\nImplementation of `3.1 Appearance-based Gaze Estimation` from\n[Learning from Simulated and Unsupervised Images through Adversarial Training](https://arxiv.org/pdf/1612.07828v1.pdf).\n\nNote: Only Python 3 support currently.\n""""""\n\nimport os\nimport sys\n\nfrom keras import applications\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\nfrom keras.preprocessing import image\nimport numpy as np\nimport tensorflow as tf\n\nfrom dlutils import plot_image_batch_w_labels\n\nfrom utils.image_history_buffer import ImageHistoryBuffer\n\n\n#\n# directories\n#\n\npath = os.path.dirname(os.path.abspath(__file__))\ncache_dir = os.path.join(path, \'cache\')\n\n#\n# image dimensions\n#\n\nimg_width = 55\nimg_height = 35\nimg_channels = 1\n\n#\n# training params\n#\n\nnb_steps = 10000\nbatch_size = 512\nk_d = 1  # number of discriminator updates per step\nk_g = 2  # number of generative network updates per step\nlog_interval = 100\n\n\ndef refiner_network(input_image_tensor):\n    """"""\n    The refiner network, R\xce\xb8, is a residual network (ResNet). It modifies the synthetic image on a pixel level, rather\n    than holistically modifying the image content, preserving the global structure and annotations.\n\n    :param input_image_tensor: Input tensor that corresponds to a synthetic image.\n    :return: Output tensor that corresponds to a refined synthetic image.\n    """"""\n    def resnet_block(input_features, nb_features=64, nb_kernel_rows=3, nb_kernel_cols=3):\n        """"""\n        A ResNet block with two `nb_kernel_rows` x `nb_kernel_cols` convolutional layers,\n        each with `nb_features` feature maps.\n\n        See Figure 6 in https://arxiv.org/pdf/1612.07828v1.pdf.\n\n        :param input_features: Input tensor to ResNet block.\n        :return: Output tensor from ResNet block.\n        """"""\n        y = layers.Convolution2D(nb_features, nb_kernel_rows, nb_kernel_cols, border_mode=\'same\')(input_features)\n        y = layers.Activation(\'relu\')(y)\n        y = layers.Convolution2D(nb_features, nb_kernel_rows, nb_kernel_cols, border_mode=\'same\')(y)\n\n        y = layers.merge([input_features, y], mode=\'sum\')\n        return layers.Activation(\'relu\')(y)\n\n    # an input image of size w \xc3\x97 h is convolved with 3 \xc3\x97 3 filters that output 64 feature maps\n    x = layers.Convolution2D(64, 3, 3, border_mode=\'same\', activation=\'relu\')(input_image_tensor)\n\n    # the output is passed through 4 ResNet blocks\n    for _ in range(4):\n        x = resnet_block(x)\n\n    # the output of the last ResNet block is passed to a 1 \xc3\x97 1 convolutional layer producing 1 feature map\n    # corresponding to the refined synthetic image\n    return layers.Convolution2D(img_channels, 1, 1, border_mode=\'same\', activation=\'tanh\')(x)\n\n\ndef discriminator_network(input_image_tensor):\n    """"""\n    The discriminator network, D\xcf\x86, contains 5 convolution layers and 2 max-pooling layers.\n\n    :param input_image_tensor: Input tensor corresponding to an image, either real or refined.\n    :return: Output tensor that corresponds to the probability of whether an image is real or refined.\n    """"""\n    x = layers.Convolution2D(96, 3, 3, border_mode=\'same\', subsample=(2, 2), activation=\'relu\')(input_image_tensor)\n    x = layers.Convolution2D(64, 3, 3, border_mode=\'same\', subsample=(2, 2), activation=\'relu\')(x)\n    x = layers.MaxPooling2D(pool_size=(3, 3), border_mode=\'same\', strides=(1, 1))(x)\n    x = layers.Convolution2D(32, 3, 3, border_mode=\'same\', subsample=(1, 1), activation=\'relu\')(x)\n    x = layers.Convolution2D(32, 1, 1, border_mode=\'same\', subsample=(1, 1), activation=\'relu\')(x)\n    x = layers.Convolution2D(2, 1, 1, border_mode=\'same\', subsample=(1, 1), activation=\'relu\')(x)\n\n    # here one feature map corresponds to `is_real` and the other to `is_refined`,\n    # and the custom loss function is then `tf.nn.sparse_softmax_cross_entropy_with_logits`\n    return layers.Reshape((-1, 2))(x)\n\n\ndef adversarial_training(synthesis_eyes_dir, mpii_gaze_dir, refiner_model_path=None, discriminator_model_path=None):\n    """"""Adversarial training of refiner network R\xce\xb8 and discriminator network D\xcf\x86.""""""\n    #\n    # define model input and output tensors\n    #\n\n    synthetic_image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n    refined_image_tensor = refiner_network(synthetic_image_tensor)\n\n    refined_or_real_image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n    discriminator_output = discriminator_network(refined_or_real_image_tensor)\n\n    #\n    # define models\n    #\n\n    refiner_model = models.Model(input=synthetic_image_tensor, output=refined_image_tensor, name=\'refiner\')\n    discriminator_model = models.Model(input=refined_or_real_image_tensor, output=discriminator_output,\n                                       name=\'discriminator\')\n\n    # combined must output the refined image along w/ the disc\'s classification of it for the refiner\'s self-reg loss\n    refiner_model_output = refiner_model(synthetic_image_tensor)\n    combined_output = discriminator_model(refiner_model_output)\n    combined_model = models.Model(input=synthetic_image_tensor, output=[refiner_model_output, combined_output],\n                                  name=\'combined\')\n\n    discriminator_model_output_shape = discriminator_model.output_shape\n\n    print(refiner_model.summary())\n    print(discriminator_model.summary())\n    print(combined_model.summary())\n\n    #\n    # define custom l1 loss function for the refiner\n    #\n\n    def self_regularization_loss(y_true, y_pred):\n        delta = 0.0001  # FIXME: need to figure out an appropriate value for this\n        return tf.multiply(delta, tf.reduce_sum(tf.abs(y_pred - y_true)))\n\n    #\n    # define custom local adversarial loss (softmax for each image section) for the discriminator\n    # the adversarial loss function is the sum of the cross-entropy losses over the local patches\n    #\n\n    def local_adversarial_loss(y_true, y_pred):\n        # y_true and y_pred have shape (batch_size, # of local patches, 2), but really we just want to average over\n        # the local patches and batch size so we can reshape to (batch_size * # of local patches, 2)\n        y_true = tf.reshape(y_true, (-1, 2))\n        y_pred = tf.reshape(y_pred, (-1, 2))\n        loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n\n        return tf.reduce_mean(loss)\n\n    #\n    # compile models\n    #\n\n    sgd = optimizers.SGD(lr=0.001)\n\n    refiner_model.compile(optimizer=sgd, loss=self_regularization_loss)\n    discriminator_model.compile(optimizer=sgd, loss=local_adversarial_loss)\n    discriminator_model.trainable = False\n    combined_model.compile(optimizer=sgd, loss=[self_regularization_loss, local_adversarial_loss])\n\n    #\n    # data generators\n    #\n\n    datagen = image.ImageDataGenerator(\n        preprocessing_function=applications.xception.preprocess_input,\n        dim_ordering=\'tf\')\n\n    flow_from_directory_params = {\'target_size\': (img_height, img_width),\n                                  \'color_mode\': \'grayscale\' if img_channels == 1 else \'rgb\',\n                                  \'class_mode\': None,\n                                  \'batch_size\': batch_size}\n\n    synthetic_generator = datagen.flow_from_directory(\n        directory=synthesis_eyes_dir,\n        **flow_from_directory_params\n    )\n\n    real_generator = datagen.flow_from_directory(\n        directory=mpii_gaze_dir,\n        **flow_from_directory_params\n    )\n\n    def get_image_batch(generator):\n        """"""keras generators may generate an incomplete batch for the last batch""""""\n        img_batch = generator.next()\n        if len(img_batch) != batch_size:\n            img_batch = generator.next()\n\n        assert len(img_batch) == batch_size\n\n        return img_batch\n\n    # the target labels for the cross-entropy loss layer are 0 for every yj (real) and 1 for every xi (refined)\n    y_real = np.array([[[1.0, 0.0]] * discriminator_model_output_shape[1]] * batch_size)\n    y_refined = np.array([[[0.0, 1.0]] * discriminator_model_output_shape[1]] * batch_size)\n    assert y_real.shape == (batch_size, discriminator_model_output_shape[1], 2)\n\n    if not refiner_model_path:\n        # we first train the R\xce\xb8 network with just self-regularization loss for 1,000 steps\n        print(\'pre-training the refiner network...\')\n        gen_loss = np.zeros(shape=len(refiner_model.metrics_names))\n\n        for i in range(1000):\n            synthetic_image_batch = get_image_batch(synthetic_generator)\n            gen_loss = np.add(refiner_model.train_on_batch(synthetic_image_batch, synthetic_image_batch), gen_loss)\n\n            # log every `log_interval` steps\n            if not i % log_interval:\n                figure_name = \'refined_image_batch_pre_train_step_{}.png\'.format(i)\n                print(\'Saving batch of refined images during pre-training at step: {}.\'.format(i))\n\n                synthetic_image_batch = get_image_batch(synthetic_generator)\n                plot_image_batch_w_labels.plot_batch(\n                    np.concatenate((synthetic_image_batch, refiner_model.predict_on_batch(synthetic_image_batch))),\n                    os.path.join(cache_dir, figure_name),\n                    label_batch=[\'Synthetic\'] * batch_size + [\'Refined\'] * batch_size)\n\n                print(\'Refiner model self regularization loss: {}.\'.format(gen_loss / log_interval))\n                gen_loss = np.zeros(shape=len(refiner_model.metrics_names))\n\n        refiner_model.save(os.path.join(cache_dir, \'refiner_model_pre_trained.h5\'))\n    else:\n        refiner_model.load_weights(refiner_model_path)\n\n    if not discriminator_model_path:\n        # and D\xcf\x86 for 200 steps (one mini-batch for refined images, another for real)\n        print(\'pre-training the discriminator network...\')\n        disc_loss = np.zeros(shape=len(discriminator_model.metrics_names))\n\n        for _ in range(100):\n            real_image_batch = get_image_batch(real_generator)\n            disc_loss = np.add(discriminator_model.train_on_batch(real_image_batch, y_real), disc_loss)\n\n            synthetic_image_batch = get_image_batch(synthetic_generator)\n            refined_image_batch = refiner_model.predict_on_batch(synthetic_image_batch)\n            disc_loss = np.add(discriminator_model.train_on_batch(refined_image_batch, y_refined), disc_loss)\n\n        discriminator_model.save(os.path.join(cache_dir, \'discriminator_model_pre_trained.h5\'))\n\n        # hard-coded for now\n        print(\'Discriminator model loss: {}.\'.format(disc_loss / (100 * 2)))\n    else:\n        discriminator_model.load_weights(discriminator_model_path)\n\n    # TODO: what is an appropriate size for the image history buffer?\n    image_history_buffer = ImageHistoryBuffer((0, img_height, img_width, img_channels), batch_size * 100, batch_size)\n\n    combined_loss = np.zeros(shape=len(combined_model.metrics_names))\n    disc_loss_real = np.zeros(shape=len(discriminator_model.metrics_names))\n    disc_loss_refined = np.zeros(shape=len(discriminator_model.metrics_names))\n\n    # see Algorithm 1 in https://arxiv.org/pdf/1612.07828v1.pdf\n    for i in range(nb_steps):\n        print(\'Step: {} of {}.\'.format(i, nb_steps))\n\n        # train the refiner\n        for _ in range(k_g * 2):\n            # sample a mini-batch of synthetic images\n            synthetic_image_batch = get_image_batch(synthetic_generator)\n\n            # update \xce\xb8 by taking an SGD step on mini-batch loss LR(\xce\xb8)\n            combined_loss = np.add(combined_model.train_on_batch(synthetic_image_batch,\n                                                                 [synthetic_image_batch, y_real]), combined_loss)\n\n        for _ in range(k_d):\n            # sample a mini-batch of synthetic and real images\n            synthetic_image_batch = get_image_batch(synthetic_generator)\n            real_image_batch = get_image_batch(real_generator)\n\n            # refine the synthetic images w/ the current refiner\n            refined_image_batch = refiner_model.predict_on_batch(synthetic_image_batch)\n\n            # use a history of refined images\n            half_batch_from_image_history = image_history_buffer.get_from_image_history_buffer()\n            image_history_buffer.add_to_image_history_buffer(refined_image_batch)\n\n            if len(half_batch_from_image_history):\n                refined_image_batch[:batch_size // 2] = half_batch_from_image_history\n\n            # update \xcf\x86 by taking an SGD step on mini-batch loss LD(\xcf\x86)\n            disc_loss_real = np.add(discriminator_model.train_on_batch(real_image_batch, y_real), disc_loss_real)\n            disc_loss_refined = np.add(discriminator_model.train_on_batch(refined_image_batch, y_refined),\n                                       disc_loss_refined)\n\n        if not i % log_interval:\n            # plot batch of refined images w/ current refiner\n            figure_name = \'refined_image_batch_step_{}.png\'.format(i)\n            print(\'Saving batch of refined images at adversarial step: {}.\'.format(i))\n\n            synthetic_image_batch = get_image_batch(synthetic_generator)\n            plot_image_batch_w_labels.plot_batch(\n                np.concatenate((synthetic_image_batch, refiner_model.predict_on_batch(synthetic_image_batch))),\n                os.path.join(cache_dir, figure_name),\n                label_batch=[\'Synthetic\'] * batch_size + [\'Refined\'] * batch_size)\n\n            # log loss summary\n            print(\'Refiner model loss: {}.\'.format(combined_loss / (log_interval * k_g * 2)))\n            print(\'Discriminator model loss real: {}.\'.format(disc_loss_real / (log_interval * k_d * 2)))\n            print(\'Discriminator model loss refined: {}.\'.format(disc_loss_refined / (log_interval * k_d * 2)))\n\n            combined_loss = np.zeros(shape=len(combined_model.metrics_names))\n            disc_loss_real = np.zeros(shape=len(discriminator_model.metrics_names))\n            disc_loss_refined = np.zeros(shape=len(discriminator_model.metrics_names))\n\n            # save model checkpoints\n            model_checkpoint_base_name = os.path.join(cache_dir, \'{}_model_step_{}.h5\')\n            refiner_model.save(model_checkpoint_base_name.format(\'refiner\', i))\n            discriminator_model.save(model_checkpoint_base_name.format(\'discriminator\', i))\n\n\ndef main(synthesis_eyes_dir, mpii_gaze_dir, refiner_model_path, discriminator_model_path):\n    adversarial_training(synthesis_eyes_dir, mpii_gaze_dir, refiner_model_path, discriminator_model_path)\n\n\nif __name__ == \'__main__\':\n    # TODO: if pre-trained models are passed in, we don\'t take the steps they\'ve been trained for into account\n    refiner_model_path = sys.argv[3] if len(sys.argv) >= 4 else None\n    discriminator_model_path = sys.argv[4] if len(sys.argv) >= 5 else None\n\n    main(sys.argv[1], sys.argv[2], refiner_model_path, discriminator_model_path)\n'"
utils/__init__.py,0,b''
utils/image_history_buffer.py,0,"b'""""""\nModule implementing the image history buffer described in `2.3. Updating Discriminator using a History of\nRefined Images` of https://arxiv.org/pdf/1612.07828v1.pdf.\n\n""""""\n\nimport numpy as np\n\n\nclass ImageHistoryBuffer(object):\n    def __init__(self, shape, max_size, batch_size):\n        """"""\n        Initialize the class\'s state.\n\n        :param shape: Shape of the data to be stored in the image history buffer\n                      (i.e. (0, img_height, img_width, img_channels)).\n        :param max_size: Maximum number of images that can be stored in the image history buffer.\n        :param batch_size: Batch size used to train GAN.\n        """"""\n        self.image_history_buffer = np.zeros(shape=shape)\n        self.max_size = max_size\n        self.batch_size = batch_size\n\n    def add_to_image_history_buffer(self, images, nb_to_add=None):\n        """"""\n        To be called during training of GAN. By default add batch_size // 2 images to the image history buffer each\n        time the generator generates a new batch of images.\n\n        :param images: Array of images (usually a batch) to be added to the image history buffer.\n        :param nb_to_add: The number of images from `images` to add to the image history buffer\n                          (batch_size / 2 by default).\n        """"""\n        if not nb_to_add:\n            nb_to_add = self.batch_size // 2\n\n        if len(self.image_history_buffer) < self.max_size:\n            np.append(self.image_history_buffer, images[:nb_to_add], axis=0)\n        elif len(self.image_history_buffer) == self.max_size:\n            self.image_history_buffer[:nb_to_add] = images[:nb_to_add]\n        else:\n            assert False\n\n        np.random.shuffle(self.image_history_buffer)\n\n    def get_from_image_history_buffer(self, nb_to_get=None):\n        """"""\n        Get a random sample of images from the history buffer.\n\n        :param nb_to_get: Number of images to get from the image history buffer (batch_size / 2 by default).\n        :return: A random sample of `nb_to_get` images from the image history buffer, or an empty np array if the image\n                 history buffer is empty.\n        """"""\n        if not nb_to_get:\n            nb_to_get = self.batch_size // 2\n\n        try:\n            return self.image_history_buffer[:nb_to_get]\n        except IndexError:\n            return np.zeros(shape=0)\n'"
utils/mpii_gaze_dataset_organize.py,0,"b'""""""\nThis script was used to organize mpii_gaze_dataset into something usable.\nIt is provided as-is and is kind of hacky but better than nothing...\n\n""""""\n\nimport glob\nimport os\nimport uuid\n\nimport numpy as np\nfrom PIL import Image\nimport scipy.io as sio\n\n\nsave_dir = \'/Users/mjdietzx/Downloads/MPIIGaze_Dataset\'\n\n\ndef butchered_mp_normalized_matlab_helper(mat_file_path):\n    """"""\n    Normalized data is provided in matlab files in MPIIGaze Dataset and these are tricky to load with Python.\n    This function was made with guessing and checking. Very frustrating.\n\n    :param mat_file_path: Full path to MPIIGaze Dataset matlab file.\n    :return: np array of images.\n    """"""\n    x = sio.loadmat(mat_file_path)\n    y = x.get(\'data\')\n    z = y[0, 0]\n\n    left_imgs = z[\'left\'][\'image\'][0, 0]\n    right_imgs = z[\'right\'][\'image\'][0, 0]\n\n    for img in np.concatenate((left_imgs, right_imgs)):\n        Image.fromarray(img).resize((55, 35), resample=Image.ANTIALIAS).save(\n            os.path.join(save_dir, \'{}.png\'.format(uuid.uuid4())))\n\n    return\n\nif __name__ == \'__main__\':\n    os.makedirs(save_dir)\n\n    for filename in glob.iglob(\'/Users/mjdietzx/Downloads/MPIIGaze/Data/Normalized/**/*.mat\', recursive=True):\n        print(filename)\n        butchered_mp_normalized_matlab_helper(filename)\n'"
