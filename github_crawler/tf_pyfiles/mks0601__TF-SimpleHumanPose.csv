file_path,api_count,code
lib/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n\n'
main/config.py,0,"b'import os\nimport os.path as osp\nimport sys\nimport numpy as np\n\nclass Config:\n    \n    ## dataset\n    dataset = \'COCO\' # \'COCO\', \'PoseTrack\', \'MPII\'\n    testset = \'val\' # train, test, val (there is no validation set for MPII)\n\n    ## directory\n    cur_dir = osp.dirname(os.path.abspath(__file__))\n    root_dir = osp.join(cur_dir, \'..\')\n    data_dir = osp.join(root_dir, \'data\')\n    output_dir = osp.join(root_dir, \'output\')\n    model_dump_dir = osp.join(output_dir, \'model_dump\', dataset)\n    vis_dir = osp.join(output_dir, \'vis\', dataset)\n    log_dir = osp.join(output_dir, \'log\', dataset)\n    result_dir = osp.join(output_dir, \'result\', dataset)\n \n    ## model setting\n    backbone = \'resnet50\' # \'resnet50\', \'resnet101\', \'resnet152\'\n    init_model = osp.join(data_dir, \'imagenet_weights\', \'resnet_v1_\' + backbone[6:] + \'.ckpt\')\n    \n    ## input, output\n    input_shape = (256, 192) # (256,192), (384,288)\n    output_shape = (input_shape[0]//4, input_shape[1]//4)\n    if output_shape[0] == 64:\n        sigma = 2\n    elif output_shape[0] == 96:\n        sigma = 3\n    pixel_means = np.array([[[123.68, 116.78, 103.94]]])\n\n    ## training config\n    lr_dec_epoch = [90, 120]\n    end_epoch = 140\n    lr = 5e-4\n    lr_dec_factor = 10\n    optimizer = \'adam\'\n    weight_decay = 1e-5\n    bn_train = True\n    batch_size = 32\n    scale_factor = 0.3\n    rotation_factor = 40\n\n    ## testing config\n    useGTbbox = False\n    flip_test = True\n    oks_nms_thr = 0.9\n    score_thr = 0.2\n    test_batch_size = 32\n\n    ## others\n    multi_thread_enable = True\n    num_thread = 10\n    gpu_ids = \'0\'\n    num_gpus = 1\n    continue_train = False\n    display = 1\n    \n    ## helper functions\n    def get_lr(self, epoch):\n        for e in self.lr_dec_epoch:\n            if epoch < e:\n                break\n        if epoch < self.lr_dec_epoch[-1]:\n            i = self.lr_dec_epoch.index(e)\n            return self.lr / (self.lr_dec_factor ** i)\n        else:\n            return self.lr / (self.lr_dec_factor ** len(self.lr_dec_epoch))\n    \n    def normalize_input(self, img):\n        return img - self.pixel_means\n    def denormalize_input(self, img):\n        return img + self.pixel_means\n\n    def set_args(self, gpu_ids, continue_train=False):\n        self.gpu_ids = gpu_ids\n        self.num_gpus = len(self.gpu_ids.split(\',\'))\n        self.continue_train = continue_train\n        os.environ[""CUDA_VISIBLE_DEVICES""] = self.gpu_ids\n        print(\'>>> Using /gpu:{}\'.format(self.gpu_ids))\n\ncfg = Config()\n\nsys.path.insert(0, osp.join(cfg.root_dir, \'lib\'))\nfrom tfflat.utils import add_pypath, make_dir\nadd_pypath(osp.join(cfg.data_dir))\nadd_pypath(osp.join(cfg.data_dir, cfg.dataset))\nmake_dir(cfg.model_dump_dir)\nmake_dir(cfg.vis_dir)\nmake_dir(cfg.log_dir)\nmake_dir(cfg.result_dir)\n\nfrom dataset import dbcfg\ncfg.num_kps = dbcfg.num_kps\ncfg.kps_names = dbcfg.kps_names\ncfg.kps_lines = dbcfg.kps_lines\ncfg.kps_symmetry = dbcfg.kps_symmetry\ncfg.img_path = dbcfg.img_path\ncfg.human_det_path = dbcfg.human_det_path\ncfg.vis_keypoints = dbcfg.vis_keypoints\n\n'"
main/gen_batch.py,0,"b""import os\nimport os.path as osp\nimport numpy as np\nimport cv2\nfrom config import cfg\nimport random\nimport time\nimport math\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\n\ndef get_affine_transform(center,\n                         scale,\n                         rot,\n                         output_size,\n                         shift=np.array([0, 0], dtype=np.float32),\n                         inv=0):\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n        print(scale)\n        scale = np.array([scale, scale])\n\n    src_w = scale[0]\n    dst_w = output_size[0]\n    dst_h = output_size[1]\n\n    rot_rad = np.pi * rot / 180\n    src_dir = get_dir([0, src_w * -0.5], rot_rad)\n    dst_dir = np.array([0, dst_w * -0.5], np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = center + scale * shift\n    src[1, :] = center + src_dir + scale * shift\n    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\n    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\n\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\ndef affine_transform(pt, t):\n    new_pt = np.array([pt[0], pt[1], 1.]).T\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2]\n\ndef get_dir(src_point, rot_rad):\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n\n    src_result = [0, 0]\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\n\n    return src_result\n\ndef get_3rd_point(a, b):\n    direct = a - b\n    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\n\ndef generate_batch(d, stage='train'):\n    \n    img = cv2.imread(os.path.join(cfg.img_path, d['imgpath']), cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n    if img is None:\n        print('cannot read ' + os.path.join(cfg.img_path, d['imgpath']))\n        assert 0\n\n    bbox = np.array(d['bbox']).astype(np.float32)\n    \n    x, y, w, h = bbox\n    aspect_ratio = cfg.input_shape[1]/cfg.input_shape[0]\n    center = np.array([x + w * 0.5, y + h * 0.5])\n    if w > aspect_ratio * h:\n        h = w / aspect_ratio\n    elif w < aspect_ratio * h:\n        w = h * aspect_ratio\n    scale = np.array([w,h]) * 1.25\n    rotation = 0\n\n    if stage == 'train':\n\n        joints = np.array(d['joints']).reshape(cfg.num_kps, 3).astype(np.float32)\n        \n        # data augmentation\n        scale = scale * np.clip(np.random.randn()*cfg.scale_factor + 1, 1-cfg.scale_factor, 1+cfg.scale_factor)\n        rotation = np.clip(np.random.randn()*cfg.rotation_factor, -cfg.rotation_factor*2, cfg.rotation_factor*2)\\\n                if random.random() <= 0.6 else 0\n        if random.random() <= 0.5:\n            img = img[:, ::-1, :]\n            center[0] = img.shape[1] - 1 - center[0]\n            joints[:,0] = img.shape[1] - 1 - joints[:,0]\n            for (q, w) in cfg.kps_symmetry:\n                joints_q, joints_w = joints[q,:].copy(), joints[w,:].copy()\n                joints[w,:], joints[q,:] = joints_q, joints_w\n\n        trans = get_affine_transform(center, scale, rotation, (cfg.input_shape[1], cfg.input_shape[0]))\n        cropped_img = cv2.warpAffine(img, trans, (cfg.input_shape[1], cfg.input_shape[0]), flags=cv2.INTER_LINEAR)\n        #cropped_img = cropped_img[:,:, ::-1]\n        cropped_img = cfg.normalize_input(cropped_img)\n        \n        for i in range(cfg.num_kps):\n            if joints[i,2] > 0:\n                joints[i,:2] = affine_transform(joints[i,:2], trans)\n                joints[i,2] *= ((joints[i,0] >= 0) & (joints[i,0] < cfg.input_shape[1]) & (joints[i,1] >= 0) & (joints[i,1] < cfg.input_shape[0]))\n        target_coord = joints[:,:2]\n        target_valid = joints[:,2]\n        \n        # for debug\n        vis = False\n        if vis:\n            filename = str(random.randrange(1,500))\n            tmpimg = cropped_img.astype(np.float32).copy()\n            tmpimg = cfg.denormalize_input(tmpimg)\n            tmpimg = tmpimg.astype(np.uint8).copy()\n            tmpkps = np.zeros((3,cfg.num_kps))\n            tmpkps[:2,:] = target_coord.transpose(1,0)\n            tmpkps[2,:] = target_valid\n            tmpimg = cfg.vis_keypoints(tmpimg, tmpkps)\n            cv2.imwrite(osp.join(cfg.vis_dir, filename + '_gt.jpg'), tmpimg)\n        \n        return [cropped_img,\n                target_coord, \n                (target_valid > 0)]\n\n    else:\n        trans = get_affine_transform(center, scale, rotation, (cfg.input_shape[1], cfg.input_shape[0]))\n        cropped_img = cv2.warpAffine(img, trans, (cfg.input_shape[1], cfg.input_shape[0]), flags=cv2.INTER_LINEAR)\n        #cropped_img = cropped_img[:,:, ::-1]\n        cropped_img = cfg.normalize_input(cropped_img)\n\n        crop_info = np.asarray([center[0]-scale[0]*0.5, center[1]-scale[1]*0.5, center[0]+scale[0]*0.5, center[1]+scale[1]*0.5])\n\n        return [cropped_img, crop_info]\n\n\n"""
main/model.py,19,"b""import tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport json\nimport math\nfrom functools import partial\n\nfrom config import cfg\nfrom tfflat.base import ModelDesc\n\nfrom nets.basemodel import resnet50, resnet101, resnet152, resnet_arg_scope, resnet_v1\nresnet_arg_scope = partial(resnet_arg_scope, bn_trainable=cfg.bn_train)\n\nclass Model(ModelDesc):\n     \n    def head_net(self, blocks, is_training, trainable=True):\n        \n        normal_initializer = tf.truncated_normal_initializer(0, 0.01)\n        msra_initializer = tf.contrib.layers.variance_scaling_initializer()\n        xavier_initializer = tf.contrib.layers.xavier_initializer()\n        \n        with slim.arg_scope(resnet_arg_scope(bn_is_training=is_training)):\n            \n            out = slim.conv2d_transpose(blocks[-1], 256, [4, 4], stride=2,\n                trainable=trainable, weights_initializer=normal_initializer,\n                padding='SAME', activation_fn=tf.nn.relu,\n                scope='up1')\n            out = slim.conv2d_transpose(out, 256, [4, 4], stride=2,\n                trainable=trainable, weights_initializer=normal_initializer,\n                padding='SAME', activation_fn=tf.nn.relu,\n                scope='up2')\n            out = slim.conv2d_transpose(out, 256, [4, 4], stride=2,\n                trainable=trainable, weights_initializer=normal_initializer,\n                padding='SAME', activation_fn=tf.nn.relu,\n                scope='up3')\n\n            out = slim.conv2d(out, cfg.num_kps, [1, 1],\n                    trainable=trainable, weights_initializer=msra_initializer,\n                    padding='SAME', normalizer_fn=None, activation_fn=None,\n                    scope='out')\n\n        return out\n   \n    def render_gaussian_heatmap(self, coord, output_shape, sigma):\n        \n        x = [i for i in range(output_shape[1])]\n        y = [i for i in range(output_shape[0])]\n        xx,yy = tf.meshgrid(x,y)\n        xx = tf.reshape(tf.to_float(xx), (1,*output_shape,1))\n        yy = tf.reshape(tf.to_float(yy), (1,*output_shape,1))\n              \n        x = tf.floor(tf.reshape(coord[:,:,0],[-1,1,1,cfg.num_kps]) / cfg.input_shape[1] * output_shape[1] + 0.5)\n        y = tf.floor(tf.reshape(coord[:,:,1],[-1,1,1,cfg.num_kps]) / cfg.input_shape[0] * output_shape[0] + 0.5)\n\n        heatmap = tf.exp(-(((xx-x)/tf.to_float(sigma))**2)/tf.to_float(2) -(((yy-y)/tf.to_float(sigma))**2)/tf.to_float(2))\n\n        return heatmap * 255.\n   \n    def make_network(self, is_train):\n        if is_train:\n            image = tf.placeholder(tf.float32, shape=[cfg.batch_size, *cfg.input_shape, 3])\n            target_coord = tf.placeholder(tf.float32, shape=[cfg.batch_size, cfg.num_kps, 2])\n            valid = tf.placeholder(tf.float32, shape=[cfg.batch_size, cfg.num_kps])\n            self.set_inputs(image, target_coord, valid)\n        else:\n            image = tf.placeholder(tf.float32, shape=[None, *cfg.input_shape, 3])\n            self.set_inputs(image)\n\n        backbone = eval(cfg.backbone)\n        resnet_fms = backbone(image, is_train, bn_trainable=True)\n        heatmap_outs = self.head_net(resnet_fms, is_train)\n        \n        if is_train:\n            gt_heatmap = tf.stop_gradient(self.render_gaussian_heatmap(target_coord, cfg.output_shape, cfg.sigma))\n            valid_mask = tf.reshape(valid, [cfg.batch_size, 1, 1, cfg.num_kps])\n            loss = tf.reduce_mean(tf.square(heatmap_outs - gt_heatmap) * valid_mask)\n            self.add_tower_summary('loss', loss)\n            self.set_loss(loss)\n        else:\n            self.set_outputs(heatmap_outs)\n\n"""
main/test.py,0,"b'import os\r\nimport os.path as osp\r\nimport numpy as np\r\nimport argparse\r\nfrom config import cfg\r\nimport cv2\r\nimport sys\r\nimport time\r\nimport json\r\nfrom PIL import Image\r\nimport matplotlib.pyplot as plt\r\nimport pickle\r\nfrom tqdm import tqdm\r\nimport math\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tfflat.base import Tester\r\nfrom tfflat.utils import mem_info\r\nfrom model import Model\r\n\r\nfrom gen_batch import generate_batch\r\nfrom dataset import Dataset\r\nfrom nms.nms import oks_nms\r\n\r\ndef test_net(tester, dets, det_range, gpu_id):\r\n\r\n    dump_results = []\r\n\r\n    start_time = time.time()\r\n\r\n    img_start = det_range[0]\r\n    img_id = 0\r\n    img_id2 = 0\r\n    pbar = tqdm(total=det_range[1] - img_start - 1, position=gpu_id)\r\n    pbar.set_description(""GPU %s"" % str(gpu_id))\r\n    while img_start < det_range[1]:\r\n        img_end = img_start + 1\r\n        im_info = dets[img_start]\r\n        while img_end < det_range[1] and dets[img_end][\'image_id\'] == im_info[\'image_id\']:\r\n            img_end += 1\r\n        \r\n        # all human detection results of a certain image\r\n        cropped_data = dets[img_start:img_end]\r\n\r\n        pbar.update(img_end - img_start)\r\n        img_start = img_end\r\n\r\n        kps_result = np.zeros((len(cropped_data), cfg.num_kps, 3))\r\n        area_save = np.zeros(len(cropped_data))\r\n\r\n        # cluster human detection results with test_batch_size\r\n        for batch_id in range(0, len(cropped_data), cfg.test_batch_size):\r\n            start_id = batch_id\r\n            end_id = min(len(cropped_data), batch_id + cfg.test_batch_size)\r\n             \r\n            imgs = []\r\n            crop_infos = []\r\n            for i in range(start_id, end_id):\r\n                img, crop_info = generate_batch(cropped_data[i], stage=\'test\')\r\n                imgs.append(img)\r\n                crop_infos.append(crop_info)\r\n            imgs = np.array(imgs)\r\n            crop_infos = np.array(crop_infos)\r\n            \r\n            # forward\r\n            heatmap = tester.predict_one([imgs])[0]\r\n            \r\n            if cfg.flip_test:\r\n                flip_imgs = imgs[:, :, ::-1, :]\r\n                flip_heatmap = tester.predict_one([flip_imgs])[0]\r\n               \r\n                flip_heatmap = flip_heatmap[:, :, ::-1, :]\r\n                for (q, w) in cfg.kps_symmetry:\r\n                    flip_heatmap_w, flip_heatmap_q = flip_heatmap[:,:,:,w].copy(), flip_heatmap[:,:,:,q].copy()\r\n                    flip_heatmap[:,:,:,q], flip_heatmap[:,:,:,w] = flip_heatmap_w, flip_heatmap_q\r\n                flip_heatmap[:,:,1:,:] = flip_heatmap.copy()[:,:,0:-1,:]\r\n                heatmap += flip_heatmap\r\n                heatmap /= 2\r\n            \r\n            # for each human detection from clustered batch\r\n            for image_id in range(start_id, end_id):\r\n               \r\n                for j in range(cfg.num_kps):\r\n                    hm_j = heatmap[image_id - start_id, :, :, j]\r\n                    idx = hm_j.argmax()\r\n                    y, x = np.unravel_index(idx, hm_j.shape)\r\n                    \r\n                    px = int(math.floor(x + 0.5))\r\n                    py = int(math.floor(y + 0.5))\r\n                    if 1 < px < cfg.output_shape[1]-1 and 1 < py < cfg.output_shape[0]-1:\r\n                        diff = np.array([hm_j[py][px+1] - hm_j[py][px-1],\r\n                                         hm_j[py+1][px]-hm_j[py-1][px]])\r\n                        diff = np.sign(diff)\r\n                        x += diff[0] * .25\r\n                        y += diff[1] * .25\r\n                    kps_result[image_id, j, :2] = (x * cfg.input_shape[1] / cfg.output_shape[1], y * cfg.input_shape[0] / cfg.output_shape[0])\r\n                    kps_result[image_id, j, 2] = hm_j.max() / 255 \r\n\r\n                vis=False\r\n                crop_info = crop_infos[image_id - start_id,:]\r\n                area = (crop_info[2] - crop_info[0]) * (crop_info[3] - crop_info[1])\r\n                if vis and np.any(kps_result[image_id,:,2]) > 0.9 and area > 96**2:\r\n                    tmpimg = imgs[image_id-start_id].copy()\r\n                    tmpimg = cfg.denormalize_input(tmpimg)\r\n                    tmpimg = tmpimg.astype(\'uint8\')\r\n                    tmpkps = np.zeros((3,cfg.num_kps))\r\n                    tmpkps[:2,:] = kps_result[image_id,:,:2].transpose(1,0)\r\n                    tmpkps[2,:] = kps_result[image_id,:,2]\r\n                    _tmpimg = tmpimg.copy()\r\n                    _tmpimg = cfg.vis_keypoints(_tmpimg, tmpkps)\r\n                    cv2.imwrite(osp.join(cfg.vis_dir, str(img_id) + \'_output.jpg\'), _tmpimg)\r\n                    img_id += 1\r\n\r\n                # map back to original images\r\n                for j in range(cfg.num_kps):\r\n                    kps_result[image_id, j, 0] = kps_result[image_id, j, 0] / cfg.input_shape[1] * (\\\r\n                    crop_infos[image_id - start_id][2] - crop_infos[image_id - start_id][0]) + crop_infos[image_id - start_id][0]\r\n                    kps_result[image_id, j, 1] = kps_result[image_id, j, 1] / cfg.input_shape[0] * (\\\r\n                    crop_infos[image_id - start_id][3] - crop_infos[image_id - start_id][1]) + crop_infos[image_id - start_id][1]\r\n                \r\n                area_save[image_id] = (crop_infos[image_id - start_id][2] - crop_infos[image_id - start_id][0]) * (crop_infos[image_id - start_id][3] - crop_infos[image_id - start_id][1])\r\n                \r\n        #vis\r\n        vis = False\r\n        if vis and np.any(kps_result[:,:,2] > 0.9):\r\n            tmpimg = cv2.imread(os.path.join(cfg.img_path, cropped_data[0][\'imgpath\']))\r\n            tmpimg = tmpimg.astype(\'uint8\')\r\n            for i in range(len(kps_result)):\r\n                tmpkps = np.zeros((3,cfg.num_kps))\r\n                tmpkps[:2,:] = kps_result[i, :, :2].transpose(1,0)\r\n                tmpkps[2,:] = kps_result[i, :, 2]\r\n                tmpimg = cfg.vis_keypoints(tmpimg, tmpkps)\r\n            cv2.imwrite(osp.join(cfg.vis_dir, str(img_id2) + \'.jpg\'), tmpimg)\r\n            img_id2 += 1\r\n        \r\n        score_result = np.copy(kps_result[:, :, 2])\r\n        kps_result[:, :, 2] = 1\r\n        kps_result = kps_result.reshape(-1,cfg.num_kps*3)\r\n       \r\n        # rescoring and oks nms\r\n        if cfg.dataset == \'COCO\':\r\n            rescored_score = np.zeros((len(score_result)))\r\n            for i in range(len(score_result)):\r\n                score_mask = score_result[i] > cfg.score_thr\r\n                if np.sum(score_mask) > 0:\r\n                    rescored_score[i] = np.mean(score_result[i][score_mask]) * cropped_data[i][\'score\']\r\n            score_result = rescored_score\r\n            keep = oks_nms(kps_result, score_result, area_save, cfg.oks_nms_thr)\r\n            if len(keep) > 0 :\r\n                kps_result = kps_result[keep,:]\r\n                score_result = score_result[keep]\r\n                area_save = area_save[keep]\r\n        elif cfg.dataset == \'PoseTrack\':\r\n            keep = oks_nms(kps_result, np.mean(score_result,axis=1), area_save, cfg.oks_nms_thr)\r\n            if len(keep) > 0 :\r\n                kps_result = kps_result[keep,:]\r\n                score_result = score_result[keep,:]\r\n                area_save = area_save[keep]\r\n        \r\n        # save result\r\n        for i in range(len(kps_result)):\r\n            if cfg.dataset == \'COCO\':\r\n                result = dict(image_id=im_info[\'image_id\'], category_id=1, score=float(round(score_result[i], 4)),\r\n                             keypoints=kps_result[i].round(3).tolist())\r\n            elif cfg.dataset == \'PoseTrack\':\r\n                result = dict(image_id=im_info[\'image_id\'], category_id=1, track_id=0, scores=score_result[i].round(4).tolist(),\r\n                              keypoints=kps_result[i].round(3).tolist())\r\n            elif cfg.dataset == \'MPII\':\r\n                result = dict(image_id=im_info[\'image_id\'], scores=score_result[i].round(4).tolist(),\r\n                              keypoints=kps_result[i].round(3).tolist())\r\n\r\n            dump_results.append(result)\r\n\r\n    return dump_results\r\n\r\n\r\ndef test(test_model):\r\n    \r\n    # annotation load\r\n    d = Dataset()\r\n    annot = d.load_annot(cfg.testset)\r\n    gt_img_id = d.load_imgid(annot)\r\n    \r\n    # human bbox load\r\n    if cfg.useGTbbox and cfg.testset in [\'train\', \'val\']:\r\n        if cfg.testset == \'train\':\r\n            dets = d.load_train_data(score=True)\r\n        else:\r\n            dets = d.load_val_data_with_annot()\r\n        dets.sort(key=lambda x: (x[\'image_id\']))\r\n    else:\r\n        with open(cfg.human_det_path, \'r\') as f:\r\n            dets = json.load(f)\r\n        dets = [i for i in dets if i[\'image_id\'] in gt_img_id]\r\n        dets = [i for i in dets if i[\'category_id\'] == 1]\r\n        dets = [i for i in dets if i[\'score\'] > 0]\r\n        dets.sort(key=lambda x: (x[\'image_id\'], x[\'score\']), reverse=True)\r\n    \r\n        img_id = []\r\n        for i in dets:\r\n            img_id.append(i[\'image_id\'])\r\n        imgname = d.imgid_to_imgname(annot, img_id, cfg.testset)\r\n        for i in range(len(dets)):\r\n            dets[i][\'imgpath\'] = imgname[i]\r\n\r\n    # job assign (multi-gpu)\r\n    from tfflat.mp_utils import MultiProc\r\n    img_start = 0\r\n    ranges = [0]\r\n    img_num = len(np.unique([i[\'image_id\'] for i in dets]))\r\n    images_per_gpu = int(img_num / len(args.gpu_ids.split(\',\'))) + 1\r\n    for run_img in range(img_num):\r\n        img_end = img_start + 1\r\n        while img_end < len(dets) and dets[img_end][\'image_id\'] == dets[img_start][\'image_id\']:\r\n            img_end += 1\r\n        if (run_img + 1) % images_per_gpu == 0 or (run_img + 1) == img_num:\r\n            ranges.append(img_end)\r\n        img_start = img_end\r\n\r\n    def func(gpu_id):\r\n        cfg.set_args(args.gpu_ids.split(\',\')[gpu_id])\r\n        tester = Tester(Model(), cfg)\r\n        tester.load_weights(test_model)\r\n        range = [ranges[gpu_id], ranges[gpu_id + 1]]\r\n        return test_net(tester, dets, range, gpu_id)\r\n\r\n    MultiGPUFunc = MultiProc(len(args.gpu_ids.split(\',\')), func)\r\n    result = MultiGPUFunc.work()\r\n\r\n    # evaluation\r\n    d.evaluation(result, annot, cfg.result_dir, cfg.testset)\r\n\r\nif __name__ == \'__main__\':\r\n    def parse_args():\r\n        parser = argparse.ArgumentParser()\r\n        parser.add_argument(\'--gpu\', type=str, dest=\'gpu_ids\')\r\n        parser.add_argument(\'--test_epoch\', type=str, dest=\'test_epoch\')\r\n        args = parser.parse_args()\r\n\r\n        # test gpus\r\n        if not args.gpu_ids:\r\n            args.gpu_ids = str(np.argmin(mem_info()))\r\n\r\n        if \'-\' in args.gpu_ids:\r\n            gpus = args.gpu_ids.split(\'-\')\r\n            gpus[0] = 0 if not gpus[0].isdigit() else int(gpus[0])\r\n            gpus[1] = len(mem_info()) if not gpus[1].isdigit() else int(gpus[1]) + 1\r\n            args.gpu_ids = \',\'.join(map(lambda x: str(x), list(range(*gpus))))\r\n        \r\n        assert args.test_epoch, \'Test epoch is required.\'\r\n        return args\r\n\r\n    global args\r\n    args = parse_args()\r\n    test(int(args.test_epoch))\r\n'"
main/train.py,0,"b""import tensorflow as tf\nimport argparse\nimport numpy as np\n\nfrom model import Model\nfrom config import cfg\nfrom tfflat.base import Trainer\nfrom tfflat.utils import mem_info\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu', type=str, dest='gpu_ids')\n    parser.add_argument('--continue', dest='continue_train', action='store_true')\n    args = parser.parse_args()\n\n    if not args.gpu_ids:\n        args.gpu_ids = str(np.argmin(mem_info()))\n\n    if '-' in args.gpu_ids:\n        gpus = args.gpu_ids.split('-')\n        gpus[0] = 0 if not gpus[0].isdigit() else int(gpus[0])\n        gpus[1] = len(mem_info()) if not gpus[1].isdigit() else int(gpus[1]) + 1\n        args.gpu_ids = ','.join(map(lambda x: str(x), list(range(*gpus))))\n\n    return args\nargs = parse_args()\n\ncfg.set_args(args.gpu_ids, args.continue_train)\ntrainer = Trainer(Model(), cfg)\ntrainer.train()\n\n\n\n"""
tool/mpii2coco.py,0,"b'from scipy.io import loadmat, savemat\nfrom PIL import Image\nimport os\nimport os.path as osp\nimport numpy as np\nimport json\n\n# run this code in the \'mpii_human_pose_v1_u12_2\' folder\n\ndef check_empty(list,name):\n    \n    try:\n        list[name]\n    except ValueError:\n        return True\n\n    if len(list[name]) > 0:\n        return False\n    else:\n        return True\n\n\ndb_type = \'train\' # train, test\nannot_file = loadmat(\'mpii_human_pose_v1_u12_1\')[\'RELEASE\']\nsave_path = \'../annotations/\' + db_type + \'.json\'\n\njoint_num = 16\nimg_num = len(annot_file[\'annolist\'][0][0][0])\n\naid = 0\ncoco = {\'images\': [], \'categories\': [], \'annotations\': []}\nfor img_id in range(img_num):\n    \n    if ((db_type == \'train\' and annot_file[\'img_train\'][0][0][0][img_id] == 1) or (db_type == \'test\' and annot_file[\'img_train\'][0][0][0][img_id] == 0)) and \\\n        check_empty(annot_file[\'annolist\'][0][0][0][img_id],\'annorect\') == False: #any person is annotated\n    \n        filename = \'images/\' + str(annot_file[\'annolist\'][0][0][0][img_id][\'image\'][0][0][0][0]) #filename\n        img = Image.open(osp.join(\'..\', filename))\n        w,h = img.size\n        img_dict = {\'id\': img_id, \'file_name\': filename, \'width\': w, \'height\': h}\n        coco[\'images\'].append(img_dict)\n\n        if db_type == \'test\':\n            continue\n        \n        person_num = len(annot_file[\'annolist\'][0][0][0][img_id][\'annorect\'][0]) #person_num\n        joint_annotated = np.zeros((person_num,joint_num))\n        for pid in range(person_num):\n            \n            if check_empty(annot_file[\'annolist\'][0][0][0][img_id][\'annorect\'][0][pid],\'annopoints\') == False: #kps is annotated\n                \n                bbox = np.zeros((4)) # xmin, ymin, w, h\n                kps = np.zeros((joint_num,3)) # xcoord, ycoord, vis\n\n                #kps\n                annot_joint_num = len(annot_file[\'annolist\'][0][0][0][img_id][\'annorect\'][0][pid][\'annopoints\'][\'point\'][0][0][0])\n                for jid in range(annot_joint_num):\n                    annot_jid = annot_file[\'annolist\'][0][0][0][img_id][\'annorect\'][0][pid][\'annopoints\'][\'point\'][0][0][0][jid][\'id\'][0][0]\n                    kps[annot_jid][0] = annot_file[\'annolist\'][0][0][0][img_id][\'annorect\'][0][pid][\'annopoints\'][\'point\'][0][0][0][jid][\'x\'][0][0]\n                    kps[annot_jid][1] = annot_file[\'annolist\'][0][0][0][img_id][\'annorect\'][0][pid][\'annopoints\'][\'point\'][0][0][0][jid][\'y\'][0][0]\n                    kps[annot_jid][2] = 1\n               \n                #bbox extract from annotated kps\n                annot_kps = kps[kps[:,2]==1,:].reshape(-1,3)\n                xmin = np.min(annot_kps[:,0])\n                ymin = np.min(annot_kps[:,1])\n                xmax = np.max(annot_kps[:,0])\n                ymax = np.max(annot_kps[:,1])\n                width = xmax - xmin - 1\n                height = ymax - ymin - 1\n                \n                # corrupted bounding box\n                if width <= 0 or height <= 0:\n                    continue\n                # 20% extend    \n                else:\n                    bbox[0] = (xmin + xmax)/2. - width/2*1.2\n                    bbox[1] = (ymin + ymax)/2. - height/2*1.2\n                    bbox[2] = width*1.2\n                    bbox[3] = height*1.2\n\n\n                person_dict = {\'id\': aid, \'image_id\': img_id, \'category_id\': 1, \'area\': bbox[2]*bbox[3], \'bbox\': bbox.tolist(), \'iscrowd\': 0, \'keypoints\': kps.reshape(-1).tolist(), \'num_keypoints\': int(np.sum(kps[:,2]==1))}\n                coco[\'annotations\'].append(person_dict)\n                aid += 1\n\ncategory = {\n    ""supercategory"": ""person"",\n    ""id"": 1,  # to be same as COCO, not using 0\n    ""name"": ""person"",\n    ""skeleton"": [[0,1],\n        [1,2], \n        [2,6], \n        [7,12], \n        [12,11], \n        [11,10], \n        [5,4], \n        [4,3], \n        [3,6], \n        [7,13], \n        [13,14], \n        [14,15], \n        [6,7], \n        [7,8], \n        [8,9]] ,\n    ""keypoints"": [""r_ankle"", ""r_knee"",""r_hip"", \n                    ""l_hip"", ""l_knee"", ""l_ankle"",\n                  ""pelvis"", ""throax"",\n                  ""upper_neck"", ""head_top"",\n                  ""r_wrist"", ""r_elbow"", ""r_shoulder"",\n                  ""l_shoulder"", ""l_elbow"", ""l_wrist""]}\n\ncoco[\'categories\'] = [category]\n\nwith open(save_path, \'w\') as f:\n    json.dump(coco, f)\n'"
tool/posetrack2coco.py,0,"b""import numpy as np\nimport json\nimport glob\nimport os\nimport os.path as osp\nfrom PIL import Image\nfrom pycocotools.coco import COCO\n\n# run this code in the 'posetrack_data' folder\n\ndb = 'test' #train, val, test\nannot_path = './annotations/' + db + '/'\nfilenames = glob.glob(annot_path + '*.json')\ncombined_annot = {'images': [], 'annotations': [], 'categories': []}\ncombined_annot_path = './combined_annotations/' + db + '2018.json'\n\nfor i in range(len(filenames)):\n\n    with open(filenames[i]) as f:\n        annot = json.load(f)\n    \n    for k,v in annot.items():\n        if k == 'categories':\n            combined_annot[k] = annot[k]\n\n        elif k == 'images':\n            for j in range(len(v)):\n                imgname = v[j]['file_name']\n                img = Image.open(osp.join('..', imgname))\n                w,h = img.size\n                annot[k][j]['width'] = w\n                annot[k][j]['height'] = h\n                annot[k][j]['coco_url'] = 'invalid'\n            combined_annot[k] += annot[k]\n\n        elif k == 'annotations':\n            if db == 'train' or db == 'val':\n                for j in range(len(v)):\n                    annot[k][j]['num_keypoints'] = sum(annot[k][j]['keypoints'][2::3])\n                    annot[k][j]['iscrowd'] = 0\n                    if annot[k][j]['num_keypoints'] == 0:\n                        annot[k][j]['bbox'] = [0,0,0,0]\n                    annot[k][j]['area'] = annot[k][j]['bbox'][2] * annot[k][j]['bbox'][3]\n            combined_annot[k] += annot[k]\n\n        else:\n            combined_annot[k] += annot[k]\n\n        \nwith open(combined_annot_path, 'w') as f:\n    json.dump(combined_annot, f)\n\n"""
data/COCO/dataset.py,0,"b'#!/usr/bin/python3\n# coding=utf-8\n\nimport os\nimport os.path as osp\nimport numpy as np\nimport cv2\nimport json\nimport pickle\nimport matplotlib.pyplot as plt\n\nimport sys\ncur_dir = os.path.dirname(__file__)\nsys.path.insert(0, osp.join(cur_dir, \'PythonAPI\'))\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nclass Dataset(object):\n    \n    dataset_name = \'COCO\'\n    num_kps = 17\n    kps_names = [\'nose\', \'l_eye\', \'r_eye\', \'l_ear\', \'r_ear\', \'l_shoulder\',\n    \'r_shoulder\', \'l_elbow\', \'r_elbow\', \'l_wrist\', \'r_wrist\',\n    \'l_hip\', \'r_hip\', \'l_knee\', \'r_knee\', \'l_ankle\', \'r_ankle\']\n    kps_symmetry = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16)]\n    kps_lines = [(1, 2), (0, 1), (0, 2), (2, 4), (1, 3), (6, 8), (8, 10), (5, 7), (7, 9), (12, 14), (14, 16), (11, 13), (13, 15), (5, 6), (11, 12)]\n\n    human_det_path = osp.join(\'..\', \'data\', dataset_name, \'dets\', \'human_detection.json\') # human detection result\n    img_path = osp.join(\'..\', \'data\', dataset_name, \'images\')\n    train_annot_path = osp.join(\'..\', \'data\', dataset_name, \'annotations\', \'person_keypoints_train2017.json\')\n    val_annot_path = osp.join(\'..\', \'data\', dataset_name, \'annotations\', \'person_keypoints_val2017.json\')\n    test_annot_path = osp.join(\'..\', \'data\', dataset_name, \'annotations\', \'image_info_test-dev2017.json\')\n\n    def load_train_data(self, score=False):\n        coco = COCO(self.train_annot_path)\n        train_data = []\n        for aid in coco.anns.keys():\n            ann = coco.anns[aid]\n            imgname = \'train2017/\' + coco.imgs[ann[\'image_id\']][\'file_name\']\n            joints = ann[\'keypoints\']\n \n            if (ann[\'image_id\'] not in coco.imgs) or ann[\'iscrowd\'] or (np.sum(joints[2::3]) == 0) or (ann[\'num_keypoints\'] == 0):\n                continue\n           \n            # sanitize bboxes\n            x, y, w, h = ann[\'bbox\']\n            img = coco.loadImgs(ann[\'image_id\'])[0]\n            width, height = img[\'width\'], img[\'height\']\n            x1 = np.max((0, x))\n            y1 = np.max((0, y))\n            x2 = np.min((width - 1, x1 + np.max((0, w - 1))))\n            y2 = np.min((height - 1, y1 + np.max((0, h - 1))))\n            if ann[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n                bbox = [x1, y1, x2-x1, y2-y1]\n            else:\n                continue\n            \n            if score:\n                data = dict(image_id = ann[\'image_id\'], imgpath = imgname, bbox=bbox, joints=joints, score=1)\n            else:\n                data = dict(image_id = ann[\'image_id\'], imgpath = imgname, bbox=bbox, joints=joints)\n\n            train_data.append(data)\n\n        return train_data\n    \n    def load_val_data_with_annot(self):\n        coco = COCO(self.val_annot_path)\n        val_data = []\n        for aid in coco.anns.keys():\n            ann = coco.anns[aid]\n            if ann[\'image_id\'] not in coco.imgs:\n                continue\n            imgname = \'val2017/\' + coco.imgs[ann[\'image_id\']][\'file_name\']\n            bbox = ann[\'bbox\']\n            joints = ann[\'keypoints\']\n            data = dict(image_id = ann[\'image_id\'], imgpath = imgname, bbox=bbox, joints=joints, score=1)\n            val_data.append(data)\n\n        return val_data\n\n    def load_annot(self, db_set):\n        if db_set == \'train\':\n            coco = COCO(self.train_annot_path)\n        elif db_set == \'val\':\n            coco = COCO(self.val_annot_path)\n        elif db_set == \'test\':\n            coco = COCO(self.test_annot_path)\n        else:\n            print(\'Unknown db_set\')\n            assert 0\n\n        return coco\n\n    def load_imgid(self, annot):\n        return annot.imgs\n\n    def imgid_to_imgname(self, annot, imgid, db_set):\n        imgs = annot.loadImgs(imgid)\n        imgname = [db_set + \'2017/\' + i[\'file_name\'] for i in imgs]\n        return imgname\n\n    def evaluation(self, result, gt, result_dir, db_set):\n        result_path = osp.join(result_dir, \'result.json\')\n        with open(result_path, \'w\') as f:\n            json.dump(result, f)\n\n        result = gt.loadRes(result_path)\n        cocoEval = COCOeval(gt, result, iouType=\'keypoints\')\n\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        cocoEval.summarize()\n\n        result_path = osp.join(result_dir, \'result.pkl\')\n        with open(result_path, \'wb\') as f:\n            pickle.dump(cocoEval, f, 2)\n            print(""Saved result file to "" + result_path)\n    \n    def vis_keypoints(self, img, kps, kp_thresh=0.4, alpha=1):\n\n        # Convert from plt 0-1 RGBA colors to 0-255 BGR colors for opencv.\n        cmap = plt.get_cmap(\'rainbow\')\n        colors = [cmap(i) for i in np.linspace(0, 1, len(self.kps_lines) + 2)]\n        colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]\n\n        # Perform the drawing on a copy of the image, to allow for blending.\n        kp_mask = np.copy(img)\n\n        # Draw mid shoulder / mid hip first for better visualization.\n        mid_shoulder = (\n            kps[:2, 5] +\n            kps[:2, 6]) / 2.0\n        sc_mid_shoulder = np.minimum(\n            kps[2, 5],\n            kps[2, 6])\n        mid_hip = (\n            kps[:2, 11] +\n            kps[:2, 12]) / 2.0\n        sc_mid_hip = np.minimum(\n            kps[2, 11],\n            kps[2, 12])\n        nose_idx = 0\n        if sc_mid_shoulder > kp_thresh and kps[2, nose_idx] > kp_thresh:\n            cv2.line(\n                kp_mask, tuple(mid_shoulder.astype(np.int32)), tuple(kps[:2, nose_idx].astype(np.int32)),\n                color=colors[len(self.kps_lines)], thickness=2, lineType=cv2.LINE_AA)\n        if sc_mid_shoulder > kp_thresh and sc_mid_hip > kp_thresh:\n            cv2.line(\n                kp_mask, tuple(mid_shoulder.astype(np.int32)), tuple(mid_hip.astype(np.int32)),\n                color=colors[len(self.kps_lines) + 1], thickness=2, lineType=cv2.LINE_AA)\n\n        # Draw the keypoints.\n        for l in range(len(self.kps_lines)):\n            i1 = self.kps_lines[l][0]\n            i2 = self.kps_lines[l][1]\n            p1 = kps[0, i1].astype(np.int32), kps[1, i1].astype(np.int32)\n            p2 = kps[0, i2].astype(np.int32), kps[1, i2].astype(np.int32)\n            if kps[2, i1] > kp_thresh and kps[2, i2] > kp_thresh:\n                cv2.line(\n                    kp_mask, p1, p2,\n                    color=colors[l], thickness=2, lineType=cv2.LINE_AA)\n            if kps[2, i1] > kp_thresh:\n                cv2.circle(\n                    kp_mask, p1,\n                    radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n            if kps[2, i2] > kp_thresh:\n                cv2.circle(\n                    kp_mask, p2,\n                    radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n\n        # Blend the keypoints.\n        return cv2.addWeighted(img, 1.0 - alpha, kp_mask, alpha, 0)\n\ndbcfg = Dataset()\n'"
data/MPII/dataset.py,0,"b'#!/usr/bin/python3\n# coding=utf-8\n\nimport os\nimport os.path as osp\nimport glob\nimport numpy as np\nimport cv2\nimport json\nimport pickle\nimport matplotlib.pyplot as plt\nfrom scipy.io import savemat\n\nimport sys\ncur_dir = os.path.dirname(__file__)\nsys.path.insert(0, osp.join(cur_dir, \'PythonAPI\'))\nfrom pycocotools.coco import COCO\n\nclass Dataset(object):\n    \n    dataset_name = \'MPII\'\n    num_kps = 16\n    kps_names = [""r_ankle"", ""r_knee"",""r_hip"", \n                    ""l_hip"", ""l_knee"", ""l_ankle"",\n                  ""pelvis"", ""throax"",\n                  ""upper_neck"", ""head_top"",\n                  ""r_wrist"", ""r_elbow"", ""r_shoulder"",\n                  ""l_shoulder"", ""l_elbow"", ""l_wrist""]\n    kps_symmetry = [(0, 5), (1, 4), (2, 3), (10, 15), (11, 14), (12, 13)]\n    kps_lines = [(0, 1), (1, 2), (2, 6), (7, 12), (12, 11), (11, 10), (5, 4), (4, 3), (3, 6), (7, 13), (13, 14), (14, 15), (6, 7), (7, 8), (8, 9)]\n\n    human_det_path = osp.join(\'..\', \'data\', dataset_name, \'dets\', \'human_detection.json\') # human detection result\n    img_path = osp.join(\'..\', \'data\', dataset_name)\n    train_annot_path = osp.join(\'..\', \'data\', dataset_name, \'annotations\', \'train.json\')\n    test_annot_path = osp.join(\'..\', \'data\', dataset_name, \'annotations\', \'test.json\')\n\n    def load_train_data(self, score=False):\n        coco = COCO(self.train_annot_path)\n        train_data = []\n        for aid in coco.anns.keys():\n            ann = coco.anns[aid]\n            imgname = coco.imgs[ann[\'image_id\']][\'file_name\']\n            joints = ann[\'keypoints\']\n\n            if (ann[\'image_id\'] not in coco.imgs) or ann[\'iscrowd\'] or (ann[\'num_keypoints\'] == 0):\n                continue\n\n            # sanitize bboxes\n            x, y, w, h = ann[\'bbox\']\n            img = coco.loadImgs(ann[\'image_id\'])[0]\n            width, height = img[\'width\'], img[\'height\']\n            x1 = np.max((0, x))\n            y1 = np.max((0, y))\n            x2 = np.min((width - 1, x1 + np.max((0, w - 1))))\n            y2 = np.min((height - 1, y1 + np.max((0, h - 1))))\n            if ann[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n                bbox = [x1, y1, x2-x1, y2-y1]\n            else:\n                continue\n            \n            if score:\n                data = dict(image_id = ann[\'image_id\'], imgpath = imgname, bbox=bbox, joints=joints, score=1)\n            else:\n                data = dict(image_id = ann[\'image_id\'], imgpath = imgname, bbox=bbox, joints=joints)\n            train_data.append(data)\n\n        return train_data\n    \n    def load_annot(self, db_set):\n        if db_set == \'train\':\n            coco = COCO(self.train_annot_path)\n        elif db_set == \'test\':\n            coco = COCO(self.test_annot_path)\n        else:\n            print(\'Unknown db_set\')\n            assert 0\n\n        return coco\n\n    def load_imgid(self, annot):\n        return annot.imgs\n\n    def imgid_to_imgname(self, annot, imgid, db_set):\n        imgs = annot.loadImgs(imgid)\n        imgname = [i[\'file_name\'] for i in imgs]\n        return imgname\n\n    def evaluation(self, result, annot, result_dir, db_set):\n        result_path = osp.join(result_dir, \'result.mat\')\n        savemat(result_path, mdict=result)\n\n    def vis_keypoints(self, img, kps, kp_thresh=0.4, alpha=1):\n\n        # Convert from plt 0-1 RGBA colors to 0-255 BGR colors for opencv.\n        cmap = plt.get_cmap(\'rainbow\')\n        colors = [cmap(i) for i in np.linspace(0, 1, len(self.kps_lines) + 2)]\n        colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]\n\n        # Perform the drawing on a copy of the image, to allow for blending.\n        kp_mask = np.copy(img)\n\n        # Draw the keypoints.\n        for l in range(len(self.kps_lines)):\n            i1 = self.kps_lines[l][0]\n            i2 = self.kps_lines[l][1]\n            p1 = kps[0, i1].astype(np.int32), kps[1, i1].astype(np.int32)\n            p2 = kps[0, i2].astype(np.int32), kps[1, i2].astype(np.int32)\n            if kps[2, i1] > kp_thresh and kps[2, i2] > kp_thresh:\n                cv2.line(\n                    kp_mask, p1, p2,\n                    color=colors[l], thickness=2, lineType=cv2.LINE_AA)\n            if kps[2, i1] > kp_thresh:\n                cv2.circle(\n                    kp_mask, p1,\n                    radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n            if kps[2, i2] > kp_thresh:\n                cv2.circle(\n                    kp_mask, p2,\n                    radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n\n        # Blend the keypoints.\n        return cv2.addWeighted(img, 1.0 - alpha, kp_mask, alpha, 0)\n\ndbcfg = Dataset()\n'"
data/PoseTrack/dataset.py,0,"b""#!/usr/bin/python3\n# coding=utf-8\n\nimport os\nimport os.path as osp\nimport glob\nimport numpy as np\nimport cv2\nimport json\nimport pickle\nimport matplotlib.pyplot as plt\n\nimport sys\ncur_dir = os.path.dirname(__file__)\nsys.path.insert(0, osp.join(cur_dir, 'PythonAPI'))\nfrom pycocotools.coco import COCO\n\nclass Dataset(object):\n    \n    dataset_name = 'PoseTrack'\n    num_kps = 17 \n    kps_names = ['nose', 'head_bottom', 'head_top', 'l_ear', 'r_ear', 'l_shoulder',\n    'r_shoulder', 'l_elbow', 'r_elbow', 'l_wrist', 'r_wrist',\n    'l_hip', 'r_hip', 'l_knee', 'r_knee', 'l_ankle', 'r_ankle'] # l_ear and r_ear are not annotated for PoseTrack\n    kps_symmetry = [(5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16)]\n    kps_lines = [(0, 1), (0, 2), (6, 8), (8, 10), (5, 7), (7, 9), (12, 14), (14, 16), (11, 13), (13, 15), (5, 6), (11, 12)]\n\n    human_det_path = osp.join('..', 'data', dataset_name, 'dets', 'human_detection.json') # human detection result\n\n    img_path = osp.join('..', 'data', dataset_name)\n    train_annot_path = osp.join('..', 'data', dataset_name, 'annotations', 'train2018.json')\n    val_annot_path = osp.join('..', 'data', dataset_name, 'annotations', 'val2018.json')\n    test_annot_path = osp.join('..', 'data', dataset_name, 'annotations', 'test2018.json')\n    original_annot_path = osp.join('..', 'data', dataset_name, 'original_annotations')\n\n    def load_train_data(self, score=False):\n        coco = COCO(self.train_annot_path)\n        train_data = []\n        for aid in coco.anns.keys():\n            ann = coco.anns[aid]\n            imgname = coco.imgs[ann['image_id']]['file_name']\n            joints = ann['keypoints']\n\n            if (ann['image_id'] not in coco.imgs) or ann['iscrowd'] or (ann['num_keypoints'] == 0):\n                continue\n            \n            # sanitize bboxes\n            x, y, w, h = ann['bbox']\n            img = coco.loadImgs(ann['image_id'])[0]\n            width, height = img['width'], img['height']\n            x1 = np.max((0, x))\n            y1 = np.max((0, y))\n            x2 = np.min((width - 1, x1 + np.max((0, w - 1))))\n            y2 = np.min((height - 1, y1 + np.max((0, h - 1))))\n            if ann['area'] > 0 and x2 >= x1 and y2 >= y1:\n                bbox = [x1, y1, x2-x1, y2-y1]\n            else:\n                continue\n\n            if score:\n                data = dict(image_id = ann['image_id'], imgpath = imgname, bbox=bbox, joints=joints, score=1)\n            else:\n                data = dict(image_id = ann['image_id'], imgpath = imgname, bbox=bbox, joints=joints)\n            train_data.append(data)\n\n        return train_data\n    \n    def load_val_data_with_annot(self):\n        coco = COCO(self.val_annot_path)\n        val_data = []\n        for aid in coco.anns.keys():\n            ann = coco.anns[aid]\n            if ann['image_id'] not in coco.imgs:\n                continue\n            imgname = coco.imgs[ann['image_id']]['file_name']\n            bbox = ann['bbox']\n            joints = ann['keypoints']\n            data = dict(image_id = ann['image_id'], imgpath = imgname, bbox=bbox, joints=joints, score=1)\n            val_data.append(data)\n\n        return val_data\n    \n    def load_annot(self, db_set):\n        if db_set == 'train':\n            coco = COCO(self.train_annot_path)\n        elif db_set == 'val':\n            coco = COCO(self.val_annot_path)\n        elif db_set == 'test':\n            coco = COCO(self.test_annot_path)\n        else:\n            print('Unknown db_set')\n            assert 0\n\n        return coco\n\n    def load_imgid(self, annot):\n        return annot.imgs\n\n    def imgid_to_imgname(self, annot, imgid, db_set):\n        imgs = annot.loadImgs(imgid)\n        imgname = [i['file_name'] for i in imgs]\n        return imgname\n\n    def evaluation(self, result, annot, result_dir, db_set):\n        # convert coco format to posetrack format\n        # evaluation is available by poseval (https://github.com/leonid-pishchulin/poseval)\n\n        print('Converting COCO format to PoseTrack format...')\n        filenames = glob.glob(osp.join(self.original_annot_path, db_set, '*.json'))\n        for i in range(len(filenames)):\n            \n            with open(filenames[i]) as f:\n                annot = json.load(f)\n            img_id_list = []\n            for ann in annot['images']:\n                img_id_list.append(ann['id'])\n\n            dump_result = {}\n            dump_result['images'] = annot['images']\n            dump_result['categories'] = annot['categories']\n            annot_from_result = []\n            for res in result:\n                if res['image_id'] in img_id_list:\n                    annot_from_result.append(res)\n            dump_result['annotations'] = annot_from_result\n            \n            result_path = osp.join(result_dir, filenames[i].split('/')[-1])\n            with open(result_path, 'w') as f:\n                json.dump(dump_result, f)\n\n    \n    def vis_keypoints(self, img, kps, kp_thresh=0.4, alpha=1):\n\n        # Convert from plt 0-1 RGBA colors to 0-255 BGR colors for opencv.\n        cmap = plt.get_cmap('rainbow')\n        colors = [cmap(i) for i in np.linspace(0, 1, len(self.kps_lines) + 2)]\n        colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]\n\n        # Perform the drawing on a copy of the image, to allow for blending.\n        kp_mask = np.copy(img)\n\n        # Draw mid shoulder / mid hip first for better visualization.\n        mid_shoulder = (\n            kps[:2, 5] +\n            kps[:2, 6]) / 2.0\n        sc_mid_shoulder = np.minimum(\n            kps[2, 5],\n            kps[2, 6])\n        mid_hip = (\n            kps[:2, 11] +\n            kps[:2, 12]) / 2.0\n        sc_mid_hip = np.minimum(\n            kps[2, 11],\n            kps[2, 12])\n        nose_idx = 0\n        if sc_mid_shoulder > kp_thresh and kps[2, nose_idx] > kp_thresh:\n            cv2.line(\n                kp_mask, tuple(mid_shoulder.astype(np.int32)), tuple(kps[:2, nose_idx].astype(np.int32)),\n                color=colors[len(self.kps_lines)], thickness=2, lineType=cv2.LINE_AA)\n        if sc_mid_shoulder > kp_thresh and sc_mid_hip > kp_thresh:\n            cv2.line(\n                kp_mask, tuple(mid_shoulder.astype(np.int32)), tuple(mid_hip.astype(np.int32)),\n                color=colors[len(self.kps_lines) + 1], thickness=2, lineType=cv2.LINE_AA)\n\n        # Draw the keypoints.\n        for l in range(len(self.kps_lines)):\n            i1 = self.kps_lines[l][0]\n            i2 = self.kps_lines[l][1]\n            p1 = kps[0, i1].astype(np.int32), kps[1, i1].astype(np.int32)\n            p2 = kps[0, i2].astype(np.int32), kps[1, i2].astype(np.int32)\n            if kps[2, i1] > kp_thresh and kps[2, i2] > kp_thresh:\n                cv2.line(\n                    kp_mask, p1, p2,\n                    color=colors[l], thickness=2, lineType=cv2.LINE_AA)\n            if kps[2, i1] > kp_thresh:\n                cv2.circle(\n                    kp_mask, p1,\n                    radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n            if kps[2, i2] > kp_thresh:\n                cv2.circle(\n                    kp_mask, p2,\n                    radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n\n        # Blend the keypoints.\n        return cv2.addWeighted(img, 1.0 - alpha, kp_mask, alpha, 0)\n\ndbcfg = Dataset()\n"""
lib/nets/__init__.py,0,b''
lib/nets/basemodel.py,9,"b""import tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\nfrom . import resnet_v1, resnet_utils\r\nfrom tensorflow.contrib.slim import arg_scope\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import nn_ops\r\nfrom tensorflow.contrib.layers.python.layers import regularizers, \\\r\n    initializers, layers\r\nfrom config import cfg\r\nimport numpy as np\r\n\r\ndef resnet_arg_scope(bn_is_training,\r\n                     bn_trainable,\r\n                     trainable=True,\r\n                     weight_decay=cfg.weight_decay,\r\n                     weight_init = initializers.variance_scaling_initializer(),\r\n                     batch_norm_decay=0.99,\r\n                     batch_norm_epsilon=1e-9,\r\n                     batch_norm_scale=True):\r\n    batch_norm_params = {\r\n        'is_training': bn_is_training,\r\n        'decay': batch_norm_decay,\r\n        'epsilon': batch_norm_epsilon,\r\n        'scale': batch_norm_scale,\r\n        'trainable': bn_trainable,\r\n        'updates_collections': ops.GraphKeys.UPDATE_OPS\r\n    }\r\n\r\n    with arg_scope(\r\n            [slim.conv2d, slim.conv2d_transpose],\r\n            weights_regularizer=regularizers.l2_regularizer(weight_decay),\r\n            weights_initializer=weight_init,\r\n            trainable=trainable,\r\n            activation_fn=nn_ops.relu,\r\n            normalizer_fn=layers.batch_norm,\r\n            normalizer_params=batch_norm_params):\r\n        with arg_scope([layers.batch_norm], **batch_norm_params) as arg_sc:\r\n            return arg_sc\r\n\r\ndef resnet50(inp, bn_is_training, bn_trainable):\r\n    bottleneck = resnet_v1.bottleneck\r\n    blocks = [\r\n        resnet_utils.Block('block1', bottleneck,\r\n                           [(256, 64, 1)] * 2 + [(256, 64, 1)]),\r\n        resnet_utils.Block('block2', bottleneck,\r\n                           [(512, 128, 2)] + [(512, 128, 1)] * 3),\r\n        resnet_utils.Block('block3', bottleneck,\r\n                           [(1024, 256, 2)] + [(1024, 256, 1)] * 5),\r\n        resnet_utils.Block('block4', bottleneck,\r\n                           [(2048, 512, 2)] + [(2048, 512, 1)] * 2)\r\n    ]   \r\n    \r\n    with slim.arg_scope(resnet_arg_scope(bn_is_training=bn_is_training, bn_trainable=bn_trainable)):\r\n\r\n        with tf.variable_scope('resnet_v1_50', 'resnet_v1_50'):\r\n            net = resnet_utils.conv2d_same(\r\n                    tf.concat(inp,axis=3), 64, 7, stride=2, scope='conv1')\r\n            \r\n            net = tf.pad(net, [[0, 0], [1, 1], [1, 1], [0, 0]])\r\n            net = slim.max_pool2d(\r\n                net, [3, 3], stride=2, padding='VALID', scope='pool1')\r\n        net, _ = resnet_v1.resnet_v1(                                  # trainable ?????\r\n            net, blocks[0:1],\r\n            global_pool=False, include_root_block=False,\r\n            scope='resnet_v1_50')\r\n    \r\n    with slim.arg_scope(resnet_arg_scope(bn_is_training=bn_is_training, bn_trainable=bn_trainable)):\r\n        net2, _ = resnet_v1.resnet_v1(\r\n            net, blocks[1:2],\r\n            global_pool=False, include_root_block=False,\r\n            scope='resnet_v1_50')\r\n\r\n    with slim.arg_scope(resnet_arg_scope(bn_is_training=bn_is_training, bn_trainable=bn_trainable)):\r\n        net3, _ = resnet_v1.resnet_v1(\r\n            net2, blocks[2:3],\r\n            global_pool=False, include_root_block=False,\r\n            scope='resnet_v1_50')\r\n\r\n    with slim.arg_scope(resnet_arg_scope(bn_is_training=bn_is_training, bn_trainable=bn_trainable)):\r\n        net4, _ = resnet_v1.resnet_v1(\r\n            net3, blocks[3:4],\r\n            global_pool=False, include_root_block=False,\r\n            scope='resnet_v1_50')\r\n\r\n    resnet_features = [net, net2, net3, net4]\r\n    return resnet_features\r\n\r\ndef resnet101(inp, bn_is_training, bn_trainable):\r\n    bottleneck = resnet_v1.bottleneck\r\n    blocks = [\r\n        resnet_utils.Block('block1', bottleneck,\r\n                           [(256, 64, 1)] * 2 + [(256, 64, 1)]),\r\n        resnet_utils.Block('block2', bottleneck,\r\n                           [(512, 128, 2)] + [(512, 128, 1)] * 3),\r\n        resnet_utils.Block('block3', bottleneck,\r\n                           [(1024, 256, 2)] + [(1024, 256, 1)] * 22),\r\n        resnet_utils.Block('block4', bottleneck,\r\n                           [(2048, 512, 2)] + [(2048, 512, 1)] * 2)\r\n    ]\r\n\r\n    with slim.arg_scope(resnet_arg_scope(bn_is_training=bn_is_training, bn_trainable=bn_trainable)):\r\n        with tf.variable_scope('resnet_v1_101', 'resnet_v1_101'):\r\n\r\n            net = resnet_utils.conv2d_same(\r\n                    tf.concat(inp,axis=3), 64, 7, stride=2, scope='conv1')\r\n            net = tf.pad(net, [[0, 0], [1, 1], [1, 1], [0, 0]])\r\n            net = slim.max_pool2d(\r\n                net, [3, 3], stride=2, padding='VALID', scope='pool1')\r\n        net, _ = resnet_v1.resnet_v1(                                  # trainable ?????\r\n            net, blocks[0:1],\r\n            global_pool=False, include_root_block=False,\r\n            scope='resnet_v1_101')\r\n    \r\n    with slim.arg_scope(resnet_arg_scope(bn_is_training=bn_is_training, bn_trainable=bn_trainable)):\r\n        net2, _ = resnet_v1.resnet_v1(\r\n            net, blocks[1:2],\r\n            global_pool=False, include_root_block=False,\r\n            scope='resnet_v1_101')\r\n    with slim.arg_scope(resnet_arg_scope(bn_is_training=bn_is_training, bn_trainable=bn_trainable)):\r\n        net3, _ = resnet_v1.resnet_v1(\r\n            net2, blocks[2:3],\r\n            global_pool=False, include_root_block=False,\r\n            scope='resnet_v1_101')\r\n    with slim.arg_scope(resnet_arg_scope(bn_is_training=bn_is_training, bn_trainable=bn_trainable)):\r\n        net4, _ = resnet_v1.resnet_v1(\r\n            net3, blocks[3:4],\r\n            global_pool=False, include_root_block=False,\r\n            scope='resnet_v1_101')\r\n\r\n    resnet_features = [net, net2, net3, net4]\r\n    return resnet_features\r\n\r\ndef resnet152(inp, bn_is_training, bn_trainable):\r\n    bottleneck = resnet_v1.bottleneck\r\n    blocks = [\r\n        resnet_utils.Block('block1', bottleneck,\r\n                           [(256, 64, 1)] * 2 + [(256, 64, 1)]),\r\n        resnet_utils.Block('block2', bottleneck,\r\n                           [(512, 128, 2)] + [(512, 128, 1)] * 7),\r\n        resnet_utils.Block('block3', bottleneck,\r\n                           [(1024, 256, 2)] + [(1024, 256, 1)] * 35),\r\n        resnet_utils.Block('block4', bottleneck,\r\n                           [(2048, 512, 2)] + [(2048, 512, 1)] * 2)\r\n    ]\r\n\r\n    with slim.arg_scope(resnet_arg_scope(bn_is_training=bn_is_training, bn_trainable=bn_trainable)):\r\n        with tf.variable_scope('resnet_v1_152', 'resnet_v1_152'):\r\n            net = resnet_utils.conv2d_same(\r\n                    tf.concat(inp,axis=3), 64, 7, stride=2, scope='conv1')\r\n            net = tf.pad(net, [[0, 0], [1, 1], [1, 1], [0, 0]])\r\n            net = slim.max_pool2d(\r\n                net, [3, 3], stride=2, padding='VALID', scope='pool1')\r\n        net, _ = resnet_v1.resnet_v1(                                  # trainable ?????\r\n            net, blocks[0:1],\r\n            global_pool=False, include_root_block=False,\r\n            scope='resnet_v1_152')\r\n    \r\n    with slim.arg_scope(resnet_arg_scope(bn_is_training=bn_is_training, bn_trainable=bn_trainable)):\r\n        net2, _ = resnet_v1.resnet_v1(\r\n            net, blocks[1:2],\r\n            global_pool=False, include_root_block=False,\r\n            scope='resnet_v1_152')\r\n    with slim.arg_scope(resnet_arg_scope(bn_is_training=bn_is_training, bn_trainable=bn_trainable)):\r\n        net3, _ = resnet_v1.resnet_v1(\r\n            net2, blocks[2:3],\r\n            global_pool=False, include_root_block=False,\r\n            scope='resnet_v1_152')\r\n    with slim.arg_scope(resnet_arg_scope(bn_is_training=bn_is_training, bn_trainable=bn_trainable)):\r\n        net4, _ = resnet_v1.resnet_v1(\r\n            net3, blocks[3:4],\r\n            global_pool=False, include_root_block=False,\r\n            scope='resnet_v1_152')\r\n\r\n    resnet_features = [net, net2, net3, net4]\r\n    return resnet_features\r\n\r\n"""
lib/nets/resnet_utils.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n""""""Contains building blocks for various versions of Residual Networks.\r\n\r\nResidual networks (ResNets) were proposed in:\r\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\r\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\r\n\r\nMore variants were introduced in:\r\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\r\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\r\n\r\nWe can obtain different ResNet variants by changing the network depth, width,\r\nand form of residual unit. This module implements the infrastructure for\r\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\r\nthe accompanying resnet_v1.py and resnet_v2.py modules.\r\n\r\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\r\nimplementation we subsample the output activations in the last residual unit of\r\neach block, instead of subsampling the input activations in the first residual\r\nunit of each block. The two implementations give identical results but our\r\nimplementation is more memory efficient.\r\n""""""\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport collections\r\n\r\nfrom tensorflow.contrib import layers as layers_lib\r\nfrom tensorflow.contrib.framework.python.ops import add_arg_scope\r\nfrom tensorflow.contrib.framework.python.ops import arg_scope\r\nfrom tensorflow.contrib.layers.python.layers import initializers\r\nfrom tensorflow.contrib.layers.python.layers import layers\r\nfrom tensorflow.contrib.layers.python.layers import regularizers\r\nfrom tensorflow.contrib.layers.python.layers import utils\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import nn_ops\r\nfrom tensorflow.python.ops import variable_scope\r\n\r\n\r\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\r\n  """"""A named tuple describing a ResNet block.\r\n\r\n  Its parts are:\r\n    scope: The scope of the `Block`.\r\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\r\n      returns another `Tensor` with the output of the ResNet unit.\r\n    args: A list of length equal to the number of units in the `Block`. The list\r\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\r\n      block to serve as argument to unit_fn.\r\n  """"""\r\n\r\n\r\ndef subsample(inputs, factor, scope=None):\r\n  """"""Subsamples the input along the spatial dimensions.\r\n\r\n  Args:\r\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\r\n    factor: The subsampling factor.\r\n    scope: Optional variable_scope.\r\n\r\n  Returns:\r\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\r\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\r\n  """"""\r\n  if factor == 1:\r\n    return inputs\r\n  else:\r\n    return layers.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\r\n\r\n\r\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, w_init=initializers.variance_scaling_initializer(), rate=1, scope=None):\r\n  """"""Strided 2-D convolution with \'SAME\' padding.\r\n\r\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\r\n  \'VALID\' padding.\r\n\r\n  Note that\r\n\r\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\r\n\r\n  is equivalent to\r\n\r\n     net = tf.contrib.layers.conv2d(inputs, num_outputs, 3, stride=1,\r\n     padding=\'SAME\')\r\n     net = subsample(net, factor=stride)\r\n\r\n  whereas\r\n\r\n     net = tf.contrib.layers.conv2d(inputs, num_outputs, 3, stride=stride,\r\n     padding=\'SAME\')\r\n\r\n  is different when the input\'s height or width is even, which is why we add the\r\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\r\n\r\n  Args:\r\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\r\n    num_outputs: An integer, the number of output filters.\r\n    kernel_size: An int with the kernel_size of the filters.\r\n    stride: An integer, the output stride.\r\n    rate: An integer, rate for atrous convolution.\r\n    scope: Scope.\r\n\r\n  Returns:\r\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\r\n      the convolution output.\r\n  """"""\r\n  if stride == 1:\r\n    return layers_lib.conv2d(\r\n        inputs,\r\n        num_outputs,\r\n        kernel_size,\r\n        stride=1,\r\n        rate=rate,\r\n        padding=\'SAME\',\r\n        weights_initializer=w_init,\r\n        scope=scope)\r\n  else:\r\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\r\n    pad_total = kernel_size_effective - 1\r\n    pad_beg = pad_total // 2\r\n    pad_end = pad_total - pad_beg\r\n    inputs = array_ops.pad(\r\n        inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\r\n    return layers_lib.conv2d(\r\n        inputs,\r\n        num_outputs,\r\n        kernel_size,\r\n        stride=stride,\r\n        rate=rate,\r\n        padding=\'VALID\',\r\n        weights_initializer=w_init,\r\n        scope=scope)\r\n\r\n\r\n@add_arg_scope\r\ndef stack_blocks_dense(net,\r\n                       blocks,\r\n                       output_stride=None,\r\n                       outputs_collections=None):\r\n  """"""Stacks ResNet `Blocks` and controls output feature density.\r\n\r\n  First, this function creates scopes for the ResNet in the form of\r\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\r\n\r\n  Second, this function allows the user to explicitly control the ResNet\r\n  output_stride, which is the ratio of the input to output spatial resolution.\r\n  This is useful for dense prediction tasks such as semantic segmentation or\r\n  object detection.\r\n\r\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\r\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\r\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\r\n  half the nominal network stride (e.g., output_stride=4), then we compute\r\n  responses twice.\r\n\r\n  Control of the output feature density is implemented by atrous convolution.\r\n\r\n  Args:\r\n    net: A `Tensor` of size [batch, height, width, channels].\r\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\r\n      element is a ResNet `Block` object describing the units in the `Block`.\r\n    output_stride: If `None`, then the output will be computed at the nominal\r\n      network stride. If output_stride is not `None`, it specifies the requested\r\n      ratio of input to output spatial resolution, which needs to be equal to\r\n      the product of unit strides from the start up to some level of the ResNet.\r\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\r\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\r\n      is equivalent to output_stride=24).\r\n    outputs_collections: Collection to add the ResNet block outputs.\r\n\r\n  Returns:\r\n    net: Output tensor with stride equal to the specified output_stride.\r\n\r\n  Raises:\r\n    ValueError: If the target output_stride is not valid.\r\n  """"""\r\n  # The current_stride variable keeps track of the effective stride of the\r\n  # activations. This allows us to invoke atrous convolution whenever applying\r\n  # the next residual unit would result in the activations having stride larger\r\n  # than the target output_stride.\r\n  current_stride = 1\r\n\r\n  # The atrous convolution rate parameter.\r\n  rate = 1\r\n\r\n  for block in blocks:\r\n    with variable_scope.variable_scope(block.scope, \'block\', [net]) as sc:\r\n      for i, unit in enumerate(block.args):\r\n        if output_stride is not None and current_stride > output_stride:\r\n          raise ValueError(\'The target output_stride cannot be reached.\')\r\n\r\n        with variable_scope.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\r\n          unit_depth, unit_depth_bottleneck, unit_stride = unit\r\n\r\n          # If we have reached the target output_stride, then we need to employ\r\n          # atrous convolution with stride=1 and multiply the atrous rate by the\r\n          # current unit\'s stride for use in subsequent layers.\r\n          if output_stride is not None and current_stride == output_stride:\r\n            net = block.unit_fn(\r\n                net,\r\n                depth=unit_depth,\r\n                depth_bottleneck=unit_depth_bottleneck,\r\n                stride=1,\r\n                rate=rate)\r\n            rate *= unit_stride\r\n\r\n          else:\r\n            net = block.unit_fn(\r\n                net,\r\n                depth=unit_depth,\r\n                depth_bottleneck=unit_depth_bottleneck,\r\n                stride=unit_stride,\r\n                rate=1)\r\n            current_stride *= unit_stride\r\n      net = utils.collect_named_outputs(outputs_collections, sc.name, net)\r\n\r\n  if output_stride is not None and current_stride != output_stride:\r\n    raise ValueError(\'The target output_stride cannot be reached.\')\r\n\r\n  return net\r\n\r\n\r\ndef resnet_arg_scope(is_training=True,\r\n                     weight_decay=0.0001,\r\n                     batch_norm_decay=0.997,\r\n                     batch_norm_epsilon=1e-5,\r\n                     batch_norm_scale=True):\r\n  """"""Defines the default ResNet arg scope.\r\n\r\n  TODO(gpapan): The batch-normalization related default values above are\r\n    appropriate for use in conjunction with the reference ResNet models\r\n    released at https://github.com/KaimingHe/deep-residual-networks. When\r\n    training ResNets from scratch, they might need to be tuned.\r\n\r\n  Args:\r\n    is_training: Whether or not we are training the parameters in the batch\r\n      normalization layers of the model.\r\n    weight_decay: The weight decay to use for regularizing the model.\r\n    batch_norm_decay: The moving average decay when estimating layer activation\r\n      statistics in batch normalization.\r\n    batch_norm_epsilon: Small constant to prevent division by zero when\r\n      normalizing activations by their variance in batch normalization.\r\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\r\n      activations in the batch normalization layer.\r\n\r\n  Returns:\r\n    An `arg_scope` to use for the resnet models.\r\n  """"""\r\n  batch_norm_params = {\r\n      \'is_training\': is_training,\r\n      \'decay\': batch_norm_decay,\r\n      \'epsilon\': batch_norm_epsilon,\r\n      \'scale\': batch_norm_scale,\r\n      \'updates_collections\': ops.GraphKeys.UPDATE_OPS,\r\n  }\r\n\r\n  with arg_scope(\r\n      [layers_lib.conv2d],\r\n      weights_regularizer=regularizers.l2_regularizer(weight_decay),\r\n      weights_initializer=initializers.variance_scaling_initializer(),\r\n      activation_fn=nn_ops.relu,\r\n      normalizer_fn=layers.batch_norm,\r\n      normalizer_params=batch_norm_params):\r\n    with arg_scope([layers.batch_norm], **batch_norm_params):\r\n      # The following implies padding=\'SAME\' for pool1, which makes feature\r\n      # alignment easier for dense prediction tasks. This is also used in\r\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\r\n      # code of \'Deep Residual Learning for Image Recognition\' uses\r\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\r\n      # tf.contrib.framework.arg_scope([tf.contrib.layers.max_pool2d], padding=\'VALID\').\r\n      with arg_scope([layers.max_pool2d], padding=\'SAME\') as arg_sc:\r\n        return arg_sc\r\n'"
lib/nets/resnet_v1.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n""""""Contains definitions for the original form of Residual Networks.\r\n\r\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\r\nby:\r\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\r\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\r\n\r\nOther variants were introduced in:\r\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\r\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\r\n\r\nThe networks defined in this module utilize the bottleneck building block of\r\n[1] with projection shortcuts only for increasing depths. They employ batch\r\nnormalization *after* every weight layer. This is the architecture used by\r\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\r\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\r\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\r\nnormalization *before* every weight layer in the so-called full pre-activation\r\nunits.\r\n\r\nTypical use:\r\n\r\n   from tensorflow.contrib.slim.python.slim.nets import\r\n   resnet_v1\r\n\r\nResNet-101 for image classification into 1000 classes:\r\n\r\n   # inputs has shape [batch, 224, 224, 3]\r\n   with slim.arg_scope(resnet_v1.resnet_arg_scope(is_training)):\r\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000)\r\n\r\nResNet-101 for semantic segmentation into 21 classes:\r\n\r\n   # inputs has shape [batch, 513, 513, 3]\r\n   with slim.arg_scope(resnet_v1.resnet_arg_scope(is_training)):\r\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\r\n                                                21,\r\n                                                global_pool=False,\r\n                                                output_stride=16)\r\n""""""\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom tensorflow.contrib import layers\r\nfrom tensorflow.contrib.framework.python.ops import add_arg_scope\r\nfrom tensorflow.contrib.framework.python.ops import arg_scope\r\nfrom tensorflow.contrib.layers.python.layers import layers as layers_lib\r\nfrom tensorflow.contrib.layers.python.layers import utils\r\n#from tensorflow.contrib.slim.python.slim.nets import resnet_utils\r\nfrom . import resnet_utils\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.ops import nn_ops\r\nfrom tensorflow.python.ops import variable_scope\r\n\r\nresnet_arg_scope = resnet_utils.resnet_arg_scope\r\n\r\n\r\n@add_arg_scope\r\ndef bottleneck(inputs,\r\n               depth,\r\n               depth_bottleneck,\r\n               stride,\r\n               rate=1,\r\n               outputs_collections=None,\r\n               scope=None):\r\n  """"""Bottleneck residual unit variant with BN after convolutions.\r\n\r\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\r\n  its definition. Note that we use here the bottleneck variant which has an\r\n  extra bottleneck layer.\r\n\r\n  When putting together two consecutive ResNet blocks that use this unit, one\r\n  should use stride = 2 in the last unit of the first block.\r\n\r\n  Args:\r\n    inputs: A tensor of size [batch, height, width, channels].\r\n    depth: The depth of the ResNet unit output.\r\n    depth_bottleneck: The depth of the bottleneck layers.\r\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\r\n      the units output compared to its input.\r\n    rate: An integer, rate for atrous convolution.\r\n    outputs_collections: Collection to add the ResNet unit output.\r\n    scope: Optional variable_scope.\r\n\r\n  Returns:\r\n    The ResNet unit\'s output.\r\n  """"""\r\n  with variable_scope.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\r\n    depth_in = utils.last_dimension(inputs.get_shape(), min_rank=4)\r\n    if depth == depth_in:\r\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\r\n    else:\r\n      shortcut = layers.conv2d(\r\n          inputs,\r\n          depth, [1, 1],\r\n          stride=stride,\r\n          activation_fn=None,\r\n          scope=\'shortcut\')\r\n\r\n    residual = layers.conv2d(\r\n        inputs, depth_bottleneck, [1, 1], stride=1, scope=\'conv1\')\r\n    residual = resnet_utils.conv2d_same(\r\n        residual, depth_bottleneck, 3, stride, rate=rate, scope=\'conv2\')\r\n    residual = layers.conv2d(\r\n        residual, depth, [1, 1], stride=1, activation_fn=None, scope=\'conv3\')\r\n\r\n    output = nn_ops.relu(shortcut + residual)\r\n\r\n    return utils.collect_named_outputs(outputs_collections, sc.name, output)\r\n\r\n\r\ndef resnet_v1(inputs,\r\n              blocks,\r\n              num_classes=None,\r\n              global_pool=True,\r\n              output_stride=None,\r\n              include_root_block=True,\r\n              reuse=None,\r\n              scope=None):\r\n  """"""Generator for v1 ResNet models.\r\n\r\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\r\n  methods for specific model instantiations, obtained by selecting different\r\n  block instantiations that produce ResNets of various depths.\r\n\r\n  Training for image classification on Imagenet is usually done with [224, 224]\r\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\r\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\r\n  However, for dense prediction tasks we advise that one uses inputs with\r\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\r\n  this case the feature maps at the ResNet output will have spatial shape\r\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\r\n  and corners exactly aligned with the input image corners, which greatly\r\n  facilitates alignment of the features to the image. Using as input [225, 225]\r\n  images results in [8, 8] feature maps at the output of the last ResNet block.\r\n\r\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\r\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\r\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\r\n  output_stride=16 in order to increase the density of the computed features at\r\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\r\n\r\n  Args:\r\n    inputs: A tensor of size [batch, height_in, width_in, channels].\r\n    blocks: A list of length equal to the number of ResNet blocks. Each element\r\n      is a resnet_utils.Block object describing the units in the block.\r\n    num_classes: Number of predicted classes for classification tasks. If None\r\n      we return the features before the logit layer.\r\n    global_pool: If True, we perform global average pooling before computing the\r\n      logits. Set to True for image classification, False for dense prediction.\r\n    output_stride: If None, then the output will be computed at the nominal\r\n      network stride. If output_stride is not None, it specifies the requested\r\n      ratio of input to output spatial resolution.\r\n    include_root_block: If True, include the initial convolution followed by\r\n      max-pooling, if False excludes it.\r\n    reuse: whether or not the network and its variables should be reused. To be\r\n      able to reuse \'scope\' must be given.\r\n    scope: Optional variable_scope.\r\n\r\n  Returns:\r\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\r\n      If global_pool is False, then height_out and width_out are reduced by a\r\n      factor of output_stride compared to the respective height_in and width_in,\r\n      else both height_out and width_out equal one. If num_classes is None, then\r\n      net is the output of the last ResNet block, potentially after global\r\n      average pooling. If num_classes is not None, net contains the pre-softmax\r\n      activations.\r\n    end_points: A dictionary from components of the network to the corresponding\r\n      activation.\r\n\r\n  Raises:\r\n    ValueError: If the target output_stride is not valid.\r\n  """"""\r\n  with variable_scope.variable_scope(\r\n      scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\r\n    end_points_collection = sc.original_name_scope + \'_end_points\'\r\n    with arg_scope(\r\n        [layers.conv2d, bottleneck, resnet_utils.stack_blocks_dense],\r\n        outputs_collections=end_points_collection):\r\n      net = inputs\r\n      if include_root_block:\r\n        if output_stride is not None:\r\n          if output_stride % 4 != 0:\r\n            raise ValueError(\'The output_stride needs to be a multiple of 4.\')\r\n          output_stride /= 4\r\n        net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\r\n        net = layers_lib.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\r\n      net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\r\n      if global_pool:\r\n        # Global average pooling.\r\n        net = math_ops.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\r\n      if num_classes is not None:\r\n        net = layers.conv2d(\r\n            net,\r\n            num_classes, [1, 1],\r\n            activation_fn=None,\r\n            normalizer_fn=None,\r\n            scope=\'logits\')\r\n      # Convert end_points_collection into a dictionary of end_points.\r\n      end_points = utils.convert_collection_to_dict(end_points_collection)\r\n      if num_classes is not None:\r\n        end_points[\'predictions\'] = layers_lib.softmax(net, scope=\'predictions\')\r\n      return net, end_points\r\n\r\n\r\nresnet_v1.default_image_size = 224\r\n\r\n\r\ndef resnet_v1_50(inputs,\r\n                 num_classes=None,\r\n                 global_pool=True,\r\n                 output_stride=None,\r\n                 reuse=None,\r\n                 scope=\'resnet_v1_50\'):\r\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\r\n  blocks = [\r\n      resnet_utils.Block(\'block1\', bottleneck,\r\n                         [(256, 64, 1)] * 2 + [(256, 64, 2)]),\r\n      resnet_utils.Block(\'block2\', bottleneck,\r\n                         [(512, 128, 1)] * 3 + [(512, 128, 2)]),\r\n      resnet_utils.Block(\'block3\', bottleneck,\r\n                         [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\r\n      resnet_utils.Block(\'block4\', bottleneck, [(2048, 512, 1)] * 3)\r\n  ]\r\n  return resnet_v1(\r\n      inputs,\r\n      blocks,\r\n      num_classes,\r\n      global_pool,\r\n      output_stride,\r\n      include_root_block=True,\r\n      reuse=reuse,\r\n      scope=scope)\r\n\r\n\r\ndef resnet_v1_101(inputs,\r\n                  num_classes=None,\r\n                  global_pool=True,\r\n                  output_stride=None,\r\n                  reuse=None,\r\n                  scope=\'resnet_v1_101\'):\r\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\r\n  blocks = [\r\n      resnet_utils.Block(\'block1\', bottleneck,\r\n                         [(256, 64, 1)] * 2 + [(256, 64, 2)]),\r\n      resnet_utils.Block(\'block2\', bottleneck,\r\n                         [(512, 128, 1)] * 3 + [(512, 128, 2)]),\r\n      resnet_utils.Block(\'block3\', bottleneck,\r\n                         [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\r\n      resnet_utils.Block(\'block4\', bottleneck, [(2048, 512, 1)] * 3)\r\n  ]\r\n  return resnet_v1(\r\n      inputs,\r\n      blocks,\r\n      num_classes,\r\n      global_pool,\r\n      output_stride,\r\n      include_root_block=True,\r\n      reuse=reuse,\r\n      scope=scope)\r\n\r\n\r\ndef resnet_v1_152(inputs,\r\n                  num_classes=None,\r\n                  global_pool=True,\r\n                  output_stride=None,\r\n                  reuse=None,\r\n                  scope=\'resnet_v1_152\'):\r\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\r\n  blocks = [\r\n      resnet_utils.Block(\'block1\', bottleneck,\r\n                         [(256, 64, 1)] * 2 + [(256, 64, 2)]),\r\n      resnet_utils.Block(\'block2\', bottleneck,\r\n                         [(512, 128, 1)] * 7 + [(512, 128, 2)]),\r\n      resnet_utils.Block(\'block3\', bottleneck,\r\n                         [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\r\n      resnet_utils.Block(\'block4\', bottleneck, [(2048, 512, 1)] * 3)\r\n  ]\r\n  return resnet_v1(\r\n      inputs,\r\n      blocks,\r\n      num_classes,\r\n      global_pool,\r\n      output_stride,\r\n      include_root_block=True,\r\n      reuse=reuse,\r\n      scope=scope)\r\n\r\n\r\ndef resnet_v1_200(inputs,\r\n                  num_classes=None,\r\n                  global_pool=True,\r\n                  output_stride=None,\r\n                  reuse=None,\r\n                  scope=\'resnet_v1_200\'):\r\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\r\n  blocks = [\r\n      resnet_utils.Block(\'block1\', bottleneck,\r\n                         [(256, 64, 1)] * 2 + [(256, 64, 2)]),\r\n      resnet_utils.Block(\'block2\', bottleneck,\r\n                         [(512, 128, 1)] * 23 + [(512, 128, 2)]),\r\n      resnet_utils.Block(\'block3\', bottleneck,\r\n                         [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\r\n      resnet_utils.Block(\'block4\', bottleneck, [(2048, 512, 1)] * 3)\r\n  ]\r\n  return resnet_v1(\r\n      inputs,\r\n      blocks,\r\n      num_classes,\r\n      global_pool,\r\n      output_stride,\r\n      include_root_block=True,\r\n      reuse=reuse,\r\n      scope=scope)\r\n'"
lib/nms/__init__.py,0,b''
lib/nms/nms.py,0,"b'# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Modified from py-faster-rcnn (https://github.com/rbgirshick/py-faster-rcnn)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom .cpu_nms import cpu_nms\nfrom .gpu_nms import gpu_nms\n\n\ndef py_nms_wrapper(thresh):\n    def _nms(dets):\n        return nms(dets, thresh)\n    return _nms\n\n\ndef cpu_nms_wrapper(thresh):\n    def _nms(dets):\n        return cpu_nms(dets, thresh)\n    return _nms\n\n\ndef gpu_nms_wrapper(thresh, device_id):\n    def _nms(dets):\n        return gpu_nms(dets, thresh, device_id)\n    return _nms\n\n\ndef nms(dets, thresh):\n    """"""\n    greedily select boxes with high confidence and overlap with current maximum <= thresh\n    rule out overlap >= thresh\n    :param dets: [[x1, y1, x2, y2 score]]\n    :param thresh: retain overlap < thresh\n    :return: indexes to keep\n    """"""\n    if dets.shape[0] == 0:\n        return []\n\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n\ndef oks_iou(g, d, a_g, a_d, sigmas=None, in_vis_thre=None):\n    if not isinstance(sigmas, np.ndarray):\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62, .62, 1.07, 1.07, .87, .87, .89, .89]) / 10.0\n    vars = (sigmas * 2) ** 2\n    xg = g[0::3]\n    yg = g[1::3]\n    vg = g[2::3]\n    ious = np.zeros((d.shape[0]))\n    for n_d in range(0, d.shape[0]):\n        xd = d[n_d, 0::3]\n        yd = d[n_d, 1::3]\n        vd = d[n_d, 2::3]\n        dx = xd - xg\n        dy = yd - yg\n        e = (dx ** 2 + dy ** 2) / vars / ((a_g + a_d[n_d]) / 2 + np.spacing(1)) / 2\n        if in_vis_thre is not None:\n            ind = list(vg > in_vis_thre) and list(vd > in_vis_thre)\n            e = e[ind]\n        ious[n_d] = np.sum(np.exp(-e)) / e.shape[0] if e.shape[0] != 0 else 0.0\n    return ious\n\ndef oks_nms(kpts, scores, areas, thresh, sigmas=None, in_vis_thre=None):\n    """"""\n    greedily select boxes with high confidence and overlap with current maximum <= thresh\n    rule out overlap >= thresh, overlap = oks\n    :param kpts_db\n    :param thresh: retain overlap < thresh\n    :return: indexes to keep\n    """"""\n    if len(kpts) == 0:\n        return []\n\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n\n        oks_ovr = oks_iou(kpts[i], kpts[order[1:]], areas[i], areas[order[1:]], sigmas, in_vis_thre)\n\n        inds = np.where(oks_ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n\n'"
lib/nms/setup.py,0,"b'# --------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Modified from py-faster-rcnn (https://github.com/rbgirshick/py-faster-rcnn)\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nfrom setuptools import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\nimport numpy as np\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # Adapted fom\n    # http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    Extension(\n        ""cpu_nms"",\n        [""cpu_nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\'gpu_nms\',\n        [\'nms_kernel.cu\', \'gpu_nms.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with\n        # gcc the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_35\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs = [numpy_include, CUDA[\'include\']]\n    ),\n]\n\nsetup(\n    name=\'nms\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
lib/tfflat/__init__.py,0,b''
lib/tfflat/base.py,33,"b'import tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nfrom collections import OrderedDict as dict\nimport setproctitle\nimport os\nimport os.path as osp\nimport glob\nimport abc\nimport math\n\nfrom .net_utils import average_gradients, aggregate_batch, get_optimizer, get_tower_summary_dict\nfrom .saver import load_model, Saver\nfrom .timer import Timer\nfrom .logger import colorlogger\nfrom .utils import approx_equal\n\nclass ModelDesc(object):\n    __metaclass__ = abc.ABCMeta\n    def __init__(self):\n        self._loss = None\n        self._inputs = []\n        self._outputs = []\n        self._tower_summary = []\n\n    def set_inputs(self, *vars):\n        self._inputs = vars\n\n    def set_outputs(self, *vars):\n        self._outputs = vars\n\n    def set_loss(self, var):\n        if not isinstance(var, tf.Tensor):\n            raise ValueError(""Loss must be an single tensor."")\n        # assert var.get_shape() == [], \'Loss tensor must be a scalar shape but got {} shape\'.format(var.get_shape())\n        self._loss = var\n\n    def get_loss(self, include_wd=False):\n        if self._loss is None:\n            raise ValueError(""Network doesn\'t define the final loss"")\n\n        if include_wd:\n            weight_decay = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            weight_decay = tf.add_n(weight_decay)\n            return self._loss + weight_decay\n        else:\n            return self._loss\n\n    def get_inputs(self):\n        if len(self._inputs) == 0:\n            raise ValueError(""Network doesn\'t define the inputs"")\n        return self._inputs\n\n    def get_outputs(self):\n        if len(self._outputs) == 0:\n            raise ValueError(""Network doesn\'t define the outputs"")\n        return self._outputs\n\n    def add_tower_summary(self, name, vars, reduced_method=\'mean\'):\n        assert reduced_method == \'mean\' or reduced_method == \'sum\', \\\n            ""Summary tensor only supports sum- or mean- reduced method""\n        if isinstance(vars, list):\n            for v in vars:\n                if vars.get_shape() == None:\n                    print(\'Summary tensor {} got an unknown shape.\'.format(name))\n                else:\n                    assert v.get_shape().as_list() == [], \\\n                        ""Summary tensor only supports scalar but got {}"".format(v.get_shape().as_list())\n                tf.add_to_collection(name, v)\n        else:\n            if vars.get_shape() == None:\n                print(\'Summary tensor {} got an unknown shape.\'.format(name))\n            else:\n                assert vars.get_shape().as_list() == [], \\\n                    ""Summary tensor only supports scalar but got {}"".format(vars.get_shape().as_list())\n            tf.add_to_collection(name, vars)\n        self._tower_summary.append([name, reduced_method])\n\n    @abc.abstractmethod\n    def make_network(self, is_train):\n        pass\n\n\nclass Base(object):\n    __metaclass__ = abc.ABCMeta\n    """"""\n    build graph:\n        _make_graph\n            make_inputs\n            make_network\n                add_tower_summary\n        get_summary\n    \n    train/test\n    """"""\n\n    def __init__(self, net, cfg, data_iter=None, log_name=\'logs.txt\'):\n        self._input_list = []\n        self._output_list = []\n        self._outputs = []\n        self.graph_ops = None\n\n        self.net = net\n        self.cfg = cfg\n\n        self.cur_epoch = 0\n\n        self.summary_dict = {}\n\n        # timer\n        self.tot_timer = Timer()\n        self.gpu_timer = Timer()\n        self.read_timer = Timer()\n\n        # logger\n        self.logger = colorlogger(cfg.log_dir, log_name=log_name)\n\n        # initialize tensorflow\n        tfconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n        tfconfig.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=tfconfig)\n\n        # build_graph\n        self.build_graph()\n\n        # get data iter\n        self._data_iter = data_iter\n\n    @abc.abstractmethod\n    def _make_data(self):\n        return\n\n    @abc.abstractmethod\n    def _make_graph(self):\n        return\n\n    def build_graph(self):\n        # all variables should be in the same graph and stored in cpu.\n        with tf.device(\'/device:CPU:0\'):\n            tf.set_random_seed(2333)\n            self.graph_ops = self._make_graph()\n            if not isinstance(self.graph_ops, list) and not isinstance(self.graph_ops, tuple):\n                self.graph_ops = [self.graph_ops]\n        self.summary_dict.update( get_tower_summary_dict(self.net._tower_summary) )\n\n    def load_weights(self, model=None):\n\n        if model == \'last_epoch\':\n            sfiles = os.path.join(self.cfg.model_dump_dir, \'snapshot_*.ckpt.meta\')\n            sfiles = glob.glob(sfiles)\n            if len(sfiles) > 0:\n                sfiles.sort(key=os.path.getmtime)\n                sfiles = [i[:-5] for i in sfiles if i.endswith(\'.meta\')]\n                model = sfiles[-1]\n            else:\n                self.logger.critical(\'No snapshot model exists.\')\n                return\n\n        if isinstance(model, int):\n            model = os.path.join(self.cfg.model_dump_dir, \'snapshot_%d.ckpt\' % model)\n\n        if isinstance(model, str) and (osp.exists(model + \'.meta\') or osp.exists(model)):\n            self.logger.info(\'Initialized model weights from {} ...\'.format(model))\n            load_model(self.sess, model)\n            if model.split(\'/\')[-1].startswith(\'snapshot_\'):\n                self.cur_epoch = int(model[model.find(\'snapshot_\')+9:model.find(\'.ckpt\')])\n                self.logger.info(\'Current epoch is %d.\' % self.cur_epoch)\n        else:\n            self.logger.critical(\'Load nothing. There is no model in path {}.\'.format(model))\n\n    def next_feed(self):\n        if self._data_iter is None:\n            raise ValueError(\'No input data.\')\n        feed_dict = dict()\n        for inputs in self._input_list:\n            blobs = next(self._data_iter)\n            for i, inp in enumerate(inputs):\n                inp_shape = inp.get_shape().as_list()\n                if None in inp_shape:\n                    feed_dict[inp] = blobs[i]\n                else:\n                    feed_dict[inp] = blobs[i].reshape(*inp_shape)\n        return feed_dict\n\nclass Trainer(Base):\n    def __init__(self, net, cfg, data_iter=None):\n        self.lr_eval = cfg.lr\n        self.lr = tf.Variable(cfg.lr, trainable=False)\n        self._optimizer = get_optimizer(self.lr, cfg.optimizer)\n\n        super(Trainer, self).__init__(net, cfg, data_iter, log_name=\'train_logs.txt\')\n\n        # make data\n        self._data_iter, self.itr_per_epoch = self._make_data()\n    \n    def _make_data(self):\n        from dataset import Dataset\n        from gen_batch import generate_batch\n\n        d = Dataset()\n        train_data = d.load_train_data()\n        \n        from tfflat.data_provider import DataFromList, MultiProcessMapDataZMQ, BatchData, MapData\n        data_load_thread = DataFromList(train_data)\n        if self.cfg.multi_thread_enable:\n            data_load_thread = MultiProcessMapDataZMQ(data_load_thread, self.cfg.num_thread, generate_batch, strict=True)\n        else:\n            data_load_thread = MapData(data_load_thread, generate_batch)\n        data_load_thread = BatchData(data_load_thread, self.cfg.batch_size)\n\n        data_load_thread.reset_state()\n        dataiter = data_load_thread.get_data()\n\n        return dataiter, math.ceil(len(train_data)/self.cfg.batch_size/self.cfg.num_gpus) \n\n    def _make_graph(self):\n        self.logger.info(""Generating training graph on {} GPUs ..."".format(self.cfg.num_gpus))\n\n        weights_initializer = slim.xavier_initializer()\n        biases_initializer = tf.constant_initializer(0.)\n        biases_regularizer = tf.no_regularizer\n        weights_regularizer = tf.contrib.layers.l2_regularizer(self.cfg.weight_decay)\n\n        tower_grads = []\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(self.cfg.num_gpus):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'tower_%d\' % i) as name_scope:\n                        # Force all Variables to reside on the CPU.\n                        with slim.arg_scope([slim.model_variable, slim.variable], device=\'/device:CPU:0\'):\n                            with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane, \\\n                                                 slim.conv2d_transpose, slim.separable_conv2d,\n                                                 slim.fully_connected],\n                                                weights_regularizer=weights_regularizer,\n                                                biases_regularizer=biases_regularizer,\n                                                weights_initializer=weights_initializer,\n                                                biases_initializer=biases_initializer):\n                                # loss over single GPU\n                                self.net.make_network(is_train=True)\n                                if i == self.cfg.num_gpus - 1:\n                                    loss = self.net.get_loss(include_wd=True)\n                                else:\n                                    loss = self.net.get_loss()\n                                self._input_list.append( self.net.get_inputs() )\n\n                        tf.get_variable_scope().reuse_variables()\n\n                        if i == 0:\n                            if self.cfg.num_gpus > 1 and self.cfg.bn_train is True:\n                                self.logger.warning(""BN is calculated only on single GPU."")\n                            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, name_scope)\n                            with tf.control_dependencies(extra_update_ops):\n                                grads = self._optimizer.compute_gradients(loss)\n                        else:\n                            grads = self._optimizer.compute_gradients(loss)\n                        final_grads = []\n                        with tf.variable_scope(\'Gradient_Mult\') as scope:\n                            for grad, var in grads:\n                                final_grads.append((grad, var))\n                        tower_grads.append(final_grads)\n\n        if len(tower_grads) > 1:\n            grads = average_gradients(tower_grads)\n        else:\n            grads = tower_grads[0]\n\n        apply_gradient_op = self._optimizer.apply_gradients(grads)\n        train_op = tf.group(apply_gradient_op, *extra_update_ops)\n\n        return train_op\n\n    def train(self):\n        \n        # saver\n        self.logger.info(\'Initialize saver ...\')\n        train_saver = Saver(self.sess, tf.global_variables(), self.cfg.model_dump_dir)\n\n        # initialize weights\n        self.logger.info(\'Initialize all variables ...\')\n        self.sess.run(tf.variables_initializer(tf.global_variables(), name=\'init\'))\n        self.load_weights(\'last_epoch\' if self.cfg.continue_train else self.cfg.init_model)\n\n        self.logger.info(\'Start training ...\')\n        start_itr = self.cur_epoch * self.itr_per_epoch + 1\n        end_itr = self.itr_per_epoch * self.cfg.end_epoch + 1\n        for itr in range(start_itr, end_itr):\n            self.tot_timer.tic()\n\n            self.cur_epoch = itr // self.itr_per_epoch\n            setproctitle.setproctitle(\'train epoch:\' + str(self.cur_epoch))\n\n            # apply current learning policy\n            cur_lr = self.cfg.get_lr(self.cur_epoch)\n            if not approx_equal(cur_lr, self.lr_eval):\n                print(self.lr_eval, cur_lr)\n                self.sess.run(tf.assign(self.lr, cur_lr))\n\n            # input data\n            self.read_timer.tic()\n            feed_dict = self.next_feed()\n            self.read_timer.toc()\n\n            # train one step\n            self.gpu_timer.tic()\n            _, self.lr_eval, *summary_res = self.sess.run(\n                [self.graph_ops[0], self.lr, *self.summary_dict.values()], feed_dict=feed_dict)\n            self.gpu_timer.toc()\n\n            itr_summary = dict()\n            for i, k in enumerate(self.summary_dict.keys()):\n                itr_summary[k] = summary_res[i]\n\n            screen = [\n                \'Epoch %d itr %d/%d:\' % (self.cur_epoch, itr, self.itr_per_epoch),\n                \'lr: %g\' % (self.lr_eval),\n                \'speed: %.2f(%.2fs r%.2f)s/itr\' % (\n                    self.tot_timer.average_time, self.gpu_timer.average_time, self.read_timer.average_time),\n                \'%.2fh/epoch\' % (self.tot_timer.average_time / 3600. * self.itr_per_epoch),\n                \' \'.join(map(lambda x: \'%s: %.4f\' % (x[0], x[1]), itr_summary.items())),\n            ]\n            \n\n            #TODO(display stall?)\n            if itr % self.cfg.display == 0:\n                self.logger.info(\' \'.join(screen))\n\n            if itr % self.itr_per_epoch == 0:\n                train_saver.save_model(self.cur_epoch)\n\n            self.tot_timer.toc()\n\nclass Tester(Base):\n    def __init__(self, net, cfg, data_iter=None):\n        super(Tester, self).__init__(net, cfg, data_iter, log_name=\'test_logs.txt\')\n\n    def next_feed(self, batch_data=None):\n        if self._data_iter is None and batch_data is None:\n            raise ValueError(\'No input data.\')\n        feed_dict = dict()\n        if batch_data is None:\n            for inputs in self._input_list:\n                blobs = next(self._data_iter)\n                for i, inp in enumerate(inputs):\n                    inp_shape = inp.get_shape().as_list()\n                    if None in inp_shape:\n                        feed_dict[inp] = blobs[i]\n                    else:\n                        feed_dict[inp] = blobs[i].reshape(*inp_shape)\n        else:\n            assert isinstance(batch_data, list) or isinstance(batch_data, tuple), ""Input data should be list-type.""\n            assert len(batch_data) == len(self._input_list[0]), ""Input data is incomplete.""\n\n            batch_size = self.cfg.batch_size\n            if self._input_list[0][0].get_shape().as_list()[0] is None:\n                # fill batch\n                for i in range(len(batch_data)):\n                    batch_size = (len(batch_data[i]) + self.cfg.num_gpus - 1) // self.cfg.num_gpus\n                    total_batches = batch_size * self.cfg.num_gpus\n                    left_batches = total_batches - len(batch_data[i])\n                    if left_batches > 0:\n                        batch_data[i] = np.append(batch_data[i], np.zeros((left_batches, *batch_data[i].shape[1:])), axis=0)\n                        self.logger.warning(""Fill some blanks to fit batch_size which wastes %d%% computation"" % (\n                            left_batches * 100. / total_batches))\n            else:\n                assert self.cfg.batch_size * self.cfg.num_gpus == len(batch_data[0]), \\\n                    ""Input batch doesn\'t fit placeholder batch.""\n\n            for j, inputs in enumerate(self._input_list):\n                for i, inp in enumerate(inputs):\n                    feed_dict[ inp ] = batch_data[i][j * batch_size: (j+1) * batch_size]\n\n            #@TODO(delete)\n            assert (j+1) * batch_size == len(batch_data[0]), \'check batch\'\n        return feed_dict, batch_size\n\n    def _make_graph(self):\n        self.logger.info(""Generating testing graph on {} GPUs ..."".format(self.cfg.num_gpus))\n\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(self.cfg.num_gpus):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'tower_%d\' % i) as name_scope:\n                        with slim.arg_scope([slim.model_variable, slim.variable], device=\'/device:CPU:0\'):\n                            self.net.make_network(is_train=False)\n                            self._input_list.append(self.net.get_inputs())\n                            self._output_list.append(self.net.get_outputs())\n\n                        tf.get_variable_scope().reuse_variables()\n\n        self._outputs = aggregate_batch(self._output_list)\n\n        # run_meta = tf.RunMetadata()\n        # opts = tf.profiler.ProfileOptionBuilder.float_operation()\n        # flops = tf.profiler.profile(self.sess.graph, run_meta=run_meta, cmd=\'op\', options=opts)\n        #\n        # opts = tf.profiler.ProfileOptionBuilder.trainable_variables_parameter()\n        # params = tf.profiler.profile(self.sess.graph, run_meta=run_meta, cmd=\'op\', options=opts)\n\n        # print(""{:,} --- {:,}"".format(flops.total_float_ops, params.total_parameters))\n        # from IPython import embed; embed()\n\n        return self._outputs\n\n    def predict_one(self, data=None):\n        # TODO(reduce data in limited batch)\n        assert len(self.summary_dict) == 0, ""still not support scalar summary in testing stage""\n        setproctitle.setproctitle(\'test epoch:\' + str(self.cur_epoch))\n\n        self.read_timer.tic()\n        feed_dict, batch_size = self.next_feed(data)\n        self.read_timer.toc()\n\n        self.gpu_timer.tic()\n        res = self.sess.run([*self.graph_ops, *self.summary_dict.values()], feed_dict=feed_dict)\n        self.gpu_timer.toc()\n\n        if data is not None and len(data[0]) < self.cfg.num_gpus * batch_size:\n            for i in range(len(res)):\n                res[i] = res[i][:len(data[0])]\n\n        return res\n\n    def test(self):\n        pass\n\n'"
lib/tfflat/data_provider.py,0,"b'# copy from tensorpack: https://github.com/ppwwyyxx/tensorpack\r\nimport numpy as np\r\nimport threading\r\nimport multiprocessing as mp\r\nimport weakref\r\nfrom contextlib import contextmanager\r\nfrom .serialize import loads, dumps\r\nimport errno\r\nimport uuid\r\nimport os\r\nimport zmq\r\nimport atexit\r\nfrom itertools import cycle\r\nfrom copy import copy\r\nfrom .utils import get_rng\r\nfrom setproctitle import setproctitle\r\n\r\ndef del_weakref(x):\r\n    o = x()\r\n    if o is not None:\r\n        o.__del__()\r\n\r\n@contextmanager\r\ndef _zmq_catch_error(name):\r\n    try:\r\n        yield\r\n    except zmq.ContextTerminated:\r\n        print(""[{}] Context terminated."".format(name))\r\n        raise Exception\r\n    except zmq.ZMQError as e:\r\n        if e.errno == errno.ENOTSOCK:       # socket closed\r\n            print(""[{}] Socket closed."".format(name))\r\n            raise Exception\r\n        else:\r\n            raise\r\n    except Exception:\r\n        raise\r\n\r\nclass DataFlowReentrantGuard(object):\r\n    """"""\r\n    A tool to enforce non-reentrancy.\r\n    Mostly used on DataFlow whose :meth:`get_data` is stateful,\r\n    so that multiple instances of the iterator cannot co-exist.\r\n    """"""\r\n    def __init__(self):\r\n        self._lock = threading.Lock()\r\n\r\n    def __enter__(self):\r\n        self._succ = self._lock.acquire(False)\r\n        if not self._succ:\r\n            raise threading.ThreadError(""This DataFlow is not reentrant!"")\r\n\r\n    def __exit__(self, exc_type, exc_val, exc_tb):\r\n        self._lock.release()\r\n        return False\r\n\r\nclass DataFromList(object):\r\n    def __init__(self, datalist, is_train=True, shuffle=True):\r\n        self.rng = get_rng()\r\n        self._datalist = datalist\r\n        self._shuffle = shuffle\r\n        self._is_train = is_train\r\n\r\n    def get_data(self):\r\n        if self._is_train:\r\n            while True:\r\n                idxs = np.arange(len(self._datalist))\r\n                if self._shuffle:\r\n                    self.rng.shuffle(idxs)\r\n                for i in idxs:\r\n                    yield self._datalist[i]\r\n        else:\r\n            idxs = np.arange(len(self._datalist))\r\n            if self._shuffle:\r\n                self.rng.shuffle(idxs)\r\n            for i in idxs:\r\n                yield self._datalist[i]\r\n\r\n    def reset_state(self):\r\n        self.rng = get_rng()\r\n\r\nclass _ParallelMapData(object):\r\n    def __init__(self, ds, buffer_size):\r\n        assert buffer_size > 0, buffer_size\r\n        self._buffer_size = buffer_size\r\n        self._buffer_occupancy = 0  # actual #elements in buffer\r\n\r\n        self.ds = ds\r\n\r\n    def _recv(self):\r\n        pass\r\n\r\n    def _send(self, dp):\r\n        pass\r\n\r\n    def _recv_filter_none(self):\r\n        ret = self._recv()\r\n        assert ret is not None, \\\r\n            ""[{}] Map function cannot return None when strict mode is used."".format(type(self).__name__)\r\n        return ret\r\n\r\n    def _fill_buffer(self, cnt=None):\r\n        if cnt is None:\r\n            cnt = self._buffer_size - self._buffer_occupancy\r\n        try:\r\n            for _ in range(cnt):\r\n                dp = next(self._iter)\r\n                self._send(dp)\r\n        except StopIteration:\r\n            print(\r\n                ""[{}] buffer_size cannot be larger than the size of the DataFlow!"".format(type(self).__name__))\r\n            raise\r\n        self._buffer_occupancy += cnt\r\n\r\n    def get_data_non_strict(self):\r\n        for dp in self._iter:\r\n            self._send(dp)\r\n            ret = self._recv()\r\n            if ret is not None:\r\n                yield ret\r\n\r\n        self._iter = self.ds.get_data()   # refresh\r\n        for _ in range(self._buffer_size):\r\n            self._send(next(self._iter))\r\n            ret = self._recv()\r\n            if ret is not None:\r\n                yield ret\r\n\r\n    def get_data_strict(self):\r\n        self._fill_buffer()\r\n        for dp in self._iter:\r\n            self._send(dp)\r\n            yield self._recv_filter_none()\r\n        self._iter = self.ds.get_data()   # refresh\r\n\r\n        # first clear the buffer, then fill\r\n        for k in range(self._buffer_size):\r\n            dp = self._recv_filter_none()\r\n            self._buffer_occupancy -= 1\r\n            if k == self._buffer_size - 1:\r\n                self._fill_buffer()\r\n            yield dp\r\n\r\nclass MapData(object):\r\n    """"""\r\n    Apply a mapper/filter on the DataFlow.\r\n\r\n    Note:\r\n        1. Please make sure func doesn\'t modify the components\r\n           unless you\'re certain it\'s safe.\r\n        2. If you discard some datapoints, ``ds.size()`` will be incorrect.\r\n    """"""\r\n\r\n    def __init__(self, ds, func):\r\n        """"""\r\n        Args:\r\n            ds (DataFlow): input DataFlow\r\n            func (datapoint -> datapoint | None): takes a datapoint and returns a new\r\n                datapoint. Return None to discard this datapoint.\r\n        """"""\r\n        self.ds = ds\r\n        self.func = func\r\n\r\n    def get_data(self):\r\n        for dp in self.ds.get_data():\r\n            ret = self.func(copy(dp))  # shallow copy the list\r\n            if ret is not None:\r\n                yield ret\r\n\r\n    def reset_state(self):\r\n        pass\r\n\r\nclass MultiProcessMapDataZMQ(_ParallelMapData):\r\n    """"""\r\n    Same as :class:`MapData`, but start processes to run the mapping function,\r\n    and communicate with ZeroMQ pipe.\r\n\r\n    Note:\r\n        1. Processes run in parallel and can take different time to run the\r\n           mapping function. Therefore the order of datapoints won\'t be\r\n           preserved, and datapoints from one pass of `df.get_data()` might get\r\n           mixed with datapoints from the next pass.\r\n\r\n           You can use **strict mode**, where `MultiProcessMapData.get_data()`\r\n           is guranteed to produce the exact set which `df.get_data()`\r\n           produces. Although the order of data still isn\'t preserved.\r\n    """"""\r\n    class _Worker(mp.Process):\r\n        def __init__(self, identity, map_func, pipename, hwm):\r\n            super(MultiProcessMapDataZMQ._Worker, self).__init__()\r\n            self.identity = identity\r\n            self.map_func = map_func\r\n            self.pipename = pipename\r\n            self.hwm = hwm\r\n\r\n        def run(self):\r\n            print(\'Start data provider {}-{}\'.format(self.pipename, self.identity))\r\n            setproctitle(\'data provider {}-{}\'.format(self.pipename, self.identity))\r\n            ctx = zmq.Context()\r\n            socket = ctx.socket(zmq.DEALER)\r\n            socket.setsockopt(zmq.IDENTITY, self.identity)\r\n            socket.set_hwm(self.hwm)\r\n            socket.connect(self.pipename)\r\n\r\n            while True:\r\n                dp = loads(socket.recv(copy=False).bytes)\r\n                dp = self.map_func(dp)\r\n                socket.send(dumps(dp), copy=False)\r\n\r\n    def __init__(self, ds, nr_proc, map_func, buffer_size=200, strict=False):\r\n        """"""\r\n        Args:\r\n            ds (DataFlow): the dataflow to map\r\n            nr_proc(int): number of threads to use\r\n            map_func (callable): datapoint -> datapoint | None\r\n            buffer_size (int): number of datapoints in the buffer\r\n            strict (bool): use ""strict mode"", see notes above.\r\n        """"""\r\n        _ParallelMapData.__init__(self, ds, buffer_size)\r\n        self.nr_proc = nr_proc\r\n        self.map_func = map_func\r\n        self._strict = strict\r\n        self._procs = []\r\n        self._guard = DataFlowReentrantGuard()\r\n\r\n        self._reset_done = False\r\n        self._procs = []\r\n\r\n    def _reset_once(self):\r\n        self.context = zmq.Context()\r\n        self.socket = self.context.socket(zmq.ROUTER)\r\n        self.socket.set_hwm(self._buffer_size * 2)\r\n        pipename = ""ipc://@{}-pipe-{}"".format(\'dataflow-map\', str(uuid.uuid1())[:8])\r\n\r\n        try:\r\n            self.socket.bind(pipename)\r\n        except zmq.ZMQError:\r\n            print(\r\n                ""ZMQError in socket.bind(). Perhaps you\'re \\\r\n                using pipes on a non-local file system. See documentation of PrefetchDataZMQ for more information."")\r\n            raise\r\n\r\n        self._proc_ids = [u\'{}\'.format(k).encode(\'utf-8\') for k in range(self.nr_proc)]\r\n        worker_hwm = int(self._buffer_size * 2 // self.nr_proc)\r\n        self._procs = [MultiProcessMapDataZMQ._Worker(\r\n            self._proc_ids[k], self.map_func, pipename, worker_hwm)\r\n            for k in range(self.nr_proc)]\r\n\r\n        self.ds.reset_state()\r\n        self._iter = self.ds.get_data()\r\n        self._iter_worker = cycle(iter(self._proc_ids))\r\n\r\n        for p in self._procs:\r\n            p.deamon = True\r\n            p.start()\r\n        self._fill_buffer()     # pre-fill the bufer\r\n\r\n    def reset_state(self):\r\n        if self._reset_done:\r\n            return\r\n        self._reset_done = True\r\n\r\n        # __del__ not guranteed to get called at exit\r\n        atexit.register(del_weakref, weakref.ref(self))\r\n\r\n        self._reset_once()  # build processes\r\n\r\n    def _send(self, dp):\r\n        # round-robin assignment\r\n        worker = next(self._iter_worker)\r\n        msg = [worker, dumps(dp)]\r\n        self.socket.send_multipart(msg, copy=False)\r\n\r\n    def _recv(self):\r\n        msg = self.socket.recv_multipart(copy=False)\r\n        dp = loads(msg[1].bytes)\r\n        return dp\r\n\r\n    def get_data(self):\r\n        with self._guard, _zmq_catch_error(\'MultiProcessMapData\'):\r\n            if self._strict:\r\n                for dp in self.get_data_strict():\r\n                    yield dp\r\n            else:\r\n                for dp in self.get_data_non_strict():\r\n                    yield dp\r\n\r\n    def __del__(self):\r\n        try:\r\n            if not self._reset_done:\r\n                return\r\n            if not self.context.closed:\r\n                self.socket.close(0)\r\n                self.context.destroy(0)\r\n            for x in self._procs:\r\n                x.terminate()\r\n                x.join(5)\r\n            print(""{} successfully cleaned-up."".format(type(self).__name__))\r\n        except Exception:\r\n            pass\r\n\r\nclass BatchData(object):\r\n    """"""\r\n    Stack datapoints into batches.\r\n    It produces datapoints of the same number of components as ``ds``, but\r\n    each component has one new extra dimension of size ``batch_size``.\r\n    The batch can be either a list of original components, or (by default)\r\n    a numpy array of original components.\r\n    """"""\r\n\r\n    def __init__(self, ds, batch_size, use_list=False):\r\n        """"""\r\n        Args:\r\n            ds (DataFlow): When ``use_list=False``, the components of ``ds``\r\n                must be either scalars or :class:`np.ndarray`, and have to be consistent in shapes.\r\n            batch_size(int): batch size\r\n            use_list (bool): if True, each component will contain a list\r\n                of datapoints instead of an numpy array of an extra dimension.\r\n        """"""\r\n        self.ds = ds\r\n        self.batch_size = int(batch_size)\r\n        self.use_list = use_list\r\n\r\n    def get_data(self):\r\n        """"""\r\n        Yields:\r\n            Batched data by stacking each component on an extra 0th dimension.\r\n        """"""\r\n        holder = []\r\n        for data in self.ds.get_data():\r\n            holder.append(data)\r\n            if len(holder) == self.batch_size:\r\n                yield BatchData._aggregate_batch(holder, self.use_list)\r\n                del holder[:]\r\n\r\n    @staticmethod\r\n    def _aggregate_batch(data_holder, use_list=False):\r\n        size = len(data_holder[0])\r\n        result = []\r\n        for k in range(size):\r\n            if use_list:\r\n                result.append(\r\n                    [x[k] for x in data_holder])\r\n            else:\r\n                dt = data_holder[0][k]\r\n                if type(dt) in [int, bool]:\r\n                    tp = \'int32\'\r\n                elif type(dt) == float:\r\n                    tp = \'float32\'\r\n                else:\r\n                    try:\r\n                        tp = dt.dtype\r\n                    except AttributeError:\r\n                        raise TypeError(""Unsupported type to batch: {}"".format(type(dt)))\r\n                try:\r\n                    result.append(\r\n                        np.asarray([x[k] for x in data_holder], dtype=tp))\r\n                except Exception as e:  # noqa\r\n                    logger.exception(""Cannot batch data. Perhaps they are of inconsistent shape?"")\r\n                    if isinstance(dt, np.ndarray):\r\n                        s = pprint.pformat([x[k].shape for x in data_holder])\r\n                        logger.error(""Shape of all arrays to be batched: "" + s)\r\n                    try:\r\n                        # open an ipython shell if possible\r\n                        import IPython as IP; IP.embed()    # noqa\r\n                    except ImportError:\r\n                        pass\r\n        return result\r\n\r\n    def reset_state(self):\r\n        self.ds.reset_state()\r\n\r\n'"
lib/tfflat/dpflow.py,0,"b""import zmq\r\nimport multiprocessing as mp\r\nfrom .serialize import loads, dumps\r\n\r\ndef data_sender(id, name, func_iter, *args):\r\n    context = zmq.Context()\r\n    sender = context.socket(zmq.PUSH)\r\n    sender.connect('ipc://@{}'.format(name))\r\n\r\n    print('start data provider {}-{}'.format(name, id))\r\n    while True:\r\n        data_iter = func_iter(id, *args)\r\n        for msg in data_iter:\r\n            # print(id)\r\n            sender.send( dumps([id, msg]) )\r\n\r\ndef provider(nr_proc, name, func_iter, *args):\r\n    proc_ids = [i for i in range(nr_proc)]\r\n\r\n    procs = []\r\n    for i in range(nr_proc):\r\n        w = mp.Process(target=data_sender, args=(proc_ids[i], name, func_iter, *args))\r\n        w.deamon = True\r\n        procs.append( w )\r\n\r\n    for p in procs:\r\n        p.start()\r\n\r\ndef receiver(name):\r\n    context = zmq.Context()\r\n\r\n    receiver = context.socket(zmq.PULL)\r\n    receiver.bind('ipc://@{}'.format(name))\r\n\r\n    while True:\r\n        id, msg = loads( receiver.recv() )\r\n        # print(id, end='')\r\n        yield msg\r\n"""
lib/tfflat/logger.py,0,"b'import logging\r\nimport os\r\n\r\nOK = \'\\033[92m\'\r\nWARNING = \'\\033[93m\'\r\nFAIL = \'\\033[91m\'\r\nEND = \'\\033[0m\'\r\n\r\nPINK = \'\\033[95m\'\r\nBLUE = \'\\033[94m\'\r\nGREEN = OK\r\nRED = FAIL\r\nWHITE = END\r\nYELLOW = WARNING\r\n\r\nclass colorlogger():\r\n    def __init__(self, log_dir, log_name=\'train_logs.txt\'):\r\n        # set log\r\n        self._logger = logging.getLogger(log_name)\r\n        self._logger.setLevel(logging.INFO)\r\n        log_file = os.path.join(log_dir, log_name)\r\n        if not os.path.exists(log_dir):\r\n            os.makedirs(log_dir)\r\n        file_log = logging.FileHandler(log_file, mode=\'a\')\r\n        file_log.setLevel(logging.INFO)\r\n        console_log = logging.StreamHandler()\r\n        console_log.setLevel(logging.INFO)\r\n        formatter = logging.Formatter(\r\n            ""{}%(asctime)s{} %(message)s"".format(GREEN, END),\r\n            ""%m-%d %H:%M:%S"")\r\n        file_log.setFormatter(formatter)\r\n        console_log.setFormatter(formatter)\r\n        self._logger.addHandler(file_log)\r\n        self._logger.addHandler(console_log)\r\n\r\n    def debug(self, msg):\r\n        self._logger.debug(str(msg))\r\n\r\n    def info(self, msg):\r\n        self._logger.info(str(msg))\r\n\r\n    def warning(self, msg):\r\n        self._logger.warning(WARNING + \'WRN: \' + str(msg) + END)\r\n\r\n    def critical(self, msg):\r\n        self._logger.critical(RED + \'CRI: \' + str(msg) + END)\r\n\r\n    def error(self, msg):\r\n        self._logger.error(RED + \'ERR: \' + str(msg) + END)\r\n'"
lib/tfflat/mp_utils.py,0,"b""import multiprocessing as mp\r\nimport numpy as np\r\n\r\nfrom .serialize import loads, dumps\r\nfrom .serialize import dump_pkl, load_pkl\r\nfrom .utils import del_file\r\n\r\n# reduce_method\r\nLIST = 0\r\nITEM = 1\r\nITEMS = 2\r\nITEMSLIST = 3\r\n\r\n# func type\r\nFUNC = 0\r\n# ITER = 1\r\n\r\n# dump & load\r\nQUEUE = 0\r\nPICKLE = 1\r\n\r\nclass Worker(mp.Process):\r\n    def __init__(self, id, queue, func, func_type, dump_method=PICKLE, *args, **kwargs):\r\n        super(Worker, self).__init__()\r\n        self.id = id\r\n        self._func = func\r\n        self._queue = queue\r\n        self.args = args\r\n        self.kwargs = kwargs\r\n        self._func_type = func_type\r\n        self._dump_method = dump_method\r\n\r\n    def run(self):\r\n        msg = self._func(self.id, *self.args, **self.kwargs)\r\n        if self._dump_method == QUEUE:\r\n            if self._func_type == FUNC:\r\n                self._queue.put( dumps([self.id, msg]) )\r\n            # elif self._func_type == ITER:\r\n            #     for i, msg in enumerate(func(self.id, *self.args, **self.kwargs)):\r\n            #         self._queue.put([self.id, i, dumps(msg)])\r\n            else:\r\n                raise ValueError('Invalid func type.')\r\n        else:\r\n            assert self._func_type == FUNC, 'dump by pickle supports only function that is executed one time.'\r\n            dump_pkl('tmp_result_{}'.format(self.id), [self.id, msg])\r\n            print('dump to temp_file: {}'.format('tmp_result_{}'.format(self.id)))\r\n\r\nclass MultiProc(object):\r\n    def __init__(self, nr_proc, func, func_type=FUNC, reduce_method=LIST, dump_method=PICKLE, *args, **kwargs):\r\n        self._queue = mp.Queue()\r\n        self.nr_proc = nr_proc\r\n        self._proc_ids = [i for i in range(self.nr_proc)]\r\n        self._func_type = func_type\r\n        self._reduce_method = reduce_method\r\n        self._dump_method = dump_method\r\n\r\n        self._procs = []\r\n        for i in range(self.nr_proc):\r\n            w = Worker(self._proc_ids[i], self._queue, func, func_type, dump_method=self._dump_method, *args, **kwargs)\r\n            w.deamon = True\r\n            self._procs.append( w )\r\n\r\n    def work(self):\r\n        for p in self._procs:\r\n            p.start()\r\n\r\n        ret = [[] for i in range(self.nr_proc)]\r\n        for i in range(self.nr_proc):\r\n            if self._dump_method == QUEUE:\r\n                id, msg = loads( self._queue.get(block=True, timeout=None) )\r\n                ret[id] = msg\r\n            elif self._dump_method == PICKLE:\r\n                pass\r\n            else:\r\n                raise ValueError('Invalid dump method')\r\n\r\n        for p in self._procs:\r\n            p.join()\r\n\r\n        if self._dump_method == PICKLE:\r\n            for i in range(self.nr_proc):\r\n                id, msg = load_pkl( 'tmp_result_{}'.format(self._proc_ids[i]) )\r\n                ret[id] = msg\r\n                del_file('tmp_result_{}.pkl'.format(self._proc_ids[i]))\r\n\r\n        result = []\r\n        if self._reduce_method == LIST:\r\n            for i in range(len(ret)):\r\n                result.extend(ret[i])\r\n        elif self._reduce_method == ITEM:\r\n            result = ret\r\n        elif self._reduce_method == ITEMS:\r\n            for i in range(len(ret[0])):\r\n                result.append( [ret[j][i] for j in range(len(ret))] )\r\n        elif self._reduce_method == ITEMSLIST:\r\n            for i in range(len(ret[0])):\r\n                tmp_res = []\r\n                for j in range(len(ret)):\r\n                    tmp_res.extend(ret[j][i])\r\n                result.append( tmp_res )\r\n        else:\r\n            raise ValueError('Invalid reduce method.')\r\n\r\n        return result\r\n\r\nif __name__ == '__main__':\r\n    test_ranges = [0, 100, 200, 300, 400, 500]\r\n    def test_net(id):\r\n        test_range = [test_ranges[id], test_ranges[id+1]]\r\n        x = []\r\n        for i in range(*test_range):\r\n            x.append(np.ones((10, 10)) * i)\r\n        print('finish {}'.format(id))\r\n        return x\r\n\r\n    x = MultiProc(5, test_net, reduce_method=LIST)\r\n    res = x.work()\r\n    from IPython import embed; embed()\r\n\r\n"""
lib/tfflat/net_utils.py,19,"b'import tensorflow as tf\r\nfrom collections import namedtuple\r\n\r\ndef average_gradients(tower_grads):\r\n    """"""Calculate the average gradient for each shared variable across all towers.\r\n    Note that this function provides a synchronization point across all towers.\r\n    Args:\r\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\r\n        is over individual gradients. The inner list is over the gradient\r\n        calculation for each tower.\r\n    Returns:\r\n       List of pairs of (gradient, variable) where the gradient has been averaged\r\n       across all towers.\r\n    """"""\r\n    average_grads = []\r\n    for grad_and_vars in zip(*tower_grads):\r\n        if grad_and_vars[0][0] is None:\r\n            print(\'No gradient on var {}\'.format(grad_and_vars[0][1].name))\r\n            continue\r\n        # Note that each grad_and_vars looks like the following:\r\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\r\n        grads = []\r\n        for g, _ in grad_and_vars:\r\n            # Add 0 dimension to the gradients to represent the tower.\r\n            expanded_g = tf.expand_dims(g, 0)\r\n\r\n            # Append on a \'tower\' dimension which we will average over below.\r\n            grads.append(expanded_g)\r\n\r\n        # Average over the \'tower\' dimension.\r\n        grad = tf.concat(axis=0, values=grads)\r\n        grad = tf.reduce_mean(grad, 0)\r\n\r\n        # Keep in mind that the Variables are redundant because they are shared\r\n        # across towers. So .. we will just return the first tower\'s pointer to\r\n        # the Variable.\r\n        v = grad_and_vars[0][1]\r\n        grad_and_var = (grad, v)\r\n        average_grads.append(grad_and_var)\r\n    return average_grads\r\n\r\ndef sum_gradients(tower_grads):\r\n    """"""Calculate the sum gradient for each shared variable across all towers.\r\n    Note that this function provides a synchronization point across all towers.\r\n    Args:\r\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\r\n        is over individual gradients. The inner list is over the gradient\r\n        calculation for each tower.\r\n    Returns:\r\n       List of pairs of (gradient, variable) where the gradient has been averaged\r\n       across all towers.\r\n    """"""\r\n    sum_grads = []\r\n    for grad_and_vars in zip(*tower_grads):\r\n        if grad_and_vars[0][0] is None:\r\n            print(\'No gradient on var {}\'.format(grad_and_vars[0][1].name))\r\n            continue\r\n        # Note that each grad_and_vars looks like the following:\r\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\r\n        grads = []\r\n        for g, _ in grad_and_vars:\r\n            # Add 0 dimension to the gradients to represent the tower.\r\n            if g is not None:\r\n                expanded_g = tf.expand_dims(g, 0)\r\n\r\n                # Append on a \'tower\' dimension which we will average over below.\r\n                grads.append(expanded_g)\r\n\r\n        # Average over the \'tower\' dimension.\r\n        grad = tf.concat(axis=0, values=grads)\r\n        grad = tf.reduce_sum(grad, 0)\r\n\r\n        # Keep in mind that the Variables are redundant because they are shared\r\n        # across towers. So .. we will just return the first tower\'s pointer to\r\n        # the Variable.\r\n        v = grad_and_vars[0][1]\r\n        grad_and_var = (grad, v)\r\n        sum_grads.append(grad_and_var)\r\n    return sum_grads\r\n\r\ndef aggregate_batch(data_holder):\r\n    results = []\r\n    if len(data_holder) == 1:\r\n        results = data_holder if isinstance(data_holder[0], tf.Tensor) else data_holder[0]\r\n    elif isinstance(data_holder[0], tf.Tensor):\r\n        results.append( tf.concat(data_holder, axis=0) )\r\n    else:\r\n        for i in range(len(data_holder[0])):\r\n            results.append(\r\n                tf.concat([data_holder[j][i] for j in range(len(data_holder))], axis=0))\r\n    return results\r\n\r\ndef get_optimizer(lr, optimizer=\'momentum\'):\r\n    if optimizer == \'sgd\':\r\n        optimizer = tf.train.GradientDescentOptimizer(lr)\r\n    elif optimizer == \'momentum\':\r\n        optimizer = tf.train.MomentumOptimizer(lr, 0.9)\r\n    elif optimizer == \'adam\':\r\n        optimizer = tf.train.AdamOptimizer(lr)\r\n    else:\r\n        raise ValueError(\'invalid optimizer\')\r\n    return optimizer\r\n\r\ndef get_tower_summary_dict(summary):\r\n    ret = dict()\r\n    for v, method in summary:\r\n        if len(tf.get_collection(v)) == 1:\r\n            ret[v] = tf.get_collection(v)[0]\r\n        elif len(tf.get_collection(v)) > 1:\r\n            if method == \'mean\':\r\n                ret[v] = tf.reduce_mean(tf.get_collection(v), axis=0)\r\n            elif method == \'sum\':\r\n                ret[v] = tf.reduce_sum(tf.get_collection(v), axis=0)\r\n            elif method == \'concat\':\r\n                ret[v] = tf.concat(tf.get_collection(v), axis=0)\r\n            else:\r\n                raise ValueError(\'Invalid summary reduced method: {}\'.format(method))\r\n    return ret\r\n'"
lib/tfflat/saver.py,3,"b'import tensorflow as tf\nfrom tensorflow.python import pywrap_tensorflow\nimport numpy as np\nfrom config import cfg\n\nimport os\nimport os.path as osp\n\ndef get_variables_in_checkpoint_file(file_name):\n    try:\n        reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n        var_to_shape_map = reader.get_variable_to_shape_map()\n        return reader, var_to_shape_map\n    except Exception as e:  # pylint: disable=broad-except\n        print(str(e))\n        if ""corrupted compressed block contents"" in str(e):\n            print(\n                ""It\'s likely that your checkpoint file has been compressed ""\n                ""with SNAPPY."")\n\nclass Saver(object):\n    def __init__(self, sess, var_list, model_dump_dir, name_prefix=\'snapshot\'):\n        self.sess = sess\n        self.var_list = var_list\n        self.model_dump_dir = model_dump_dir\n        self._name_prefix = name_prefix\n        \n        self.saver = tf.train.Saver(var_list=var_list, max_to_keep=100000)\n\n    def save_model(self, iter):\n        filename = \'{}_{:d}\'.format(self._name_prefix, iter) + \'.ckpt\'\n        if not os.path.exists(self.model_dump_dir):\n            os.makedirs(self.model_dump_dir)\n        filename = os.path.join(self.model_dump_dir, filename)\n        self.saver.save(self.sess, filename)\n        print(\'Wrote snapshot to: {:s}\'.format(filename))\n\ndef load_model(sess, model_path):\n    #TODO(global variables ?? how about _adam weights)\n    variables = tf.global_variables()\n    reader, var_keep_dic = get_variables_in_checkpoint_file(model_path)\n    if \'global_step\' in var_keep_dic:\n        var_keep_dic.pop(\'global_step\')\n    \n    # vis_var_keep_dic = []\n    variables_to_restore = {}\n    changed_variables = {}\n    for v in variables:\n        \n        v_name = v.name.split(\':\')[0]\n        if v_name in var_keep_dic:\n            # print(\'Varibles restored: %s\' % v.name)\n            #variables_to_restore.append(v)\n            variables_to_restore[v_name] = v\n            # vis_var_keep_dic.append(v.name.split(\':\')[0])\n        else:\n            # print(\'Unrestored Variables: %s\' % v.name)\n            pass\n    # print(\'Extra Variables in ckpt\', set(var_keep_dic) - set(vis_var_keep_dic))\n    \n    if len(variables_to_restore) > 0:\n        restorer = tf.train.Saver(variables_to_restore)\n        restorer.restore(sess, model_path)\n        \n    else:\n        print(\'No variables in {} fits the network\'.format(model_path))\n\n'"
lib/tfflat/serialize.py,0,"b'#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n### modified from https://github.com/ppwwyyxx/tensorpack\r\n\r\nimport os\r\nimport sys\r\nimport msgpack\r\nimport msgpack_numpy\r\nmsgpack_numpy.patch()\r\n\r\n# https://github.com/apache/arrow/pull/1223#issuecomment-359895666\r\nold_mod = sys.modules.get(\'torch\', None)\r\nsys.modules[\'torch\'] = None\r\ntry:\r\n    import pyarrow as pa\r\nexcept ImportError:\r\n    pa = None\r\nif old_mod is not None:\r\n    sys.modules[\'torch\'] = old_mod\r\nelse:\r\n    del sys.modules[\'torch\']\r\n\r\nimport pickle\r\n\r\n__all__ = [\'loads\', \'dumps\', \'dump_pkl\', \'load_pkl\']\r\n\r\n\r\ndef dumps_msgpack(obj):\r\n    """"""\r\n    Serialize an object.\r\n    Returns:\r\n        Implementation-dependent bytes-like object\r\n    """"""\r\n    return msgpack.dumps(obj, use_bin_type=True)\r\n\r\n\r\ndef loads_msgpack(buf):\r\n    """"""\r\n    Args:\r\n        buf: the output of `dumps`.\r\n    """"""\r\n    return msgpack.loads(buf, raw=False)\r\n\r\n\r\ndef dumps_pyarrow(obj):\r\n    """"""\r\n    Serialize an object.\r\n\r\n    Returns:\r\n        Implementation-dependent bytes-like object\r\n    """"""\r\n    return pa.serialize(obj).to_buffer()\r\n\r\n\r\ndef loads_pyarrow(buf):\r\n    """"""\r\n    Args:\r\n        buf: the output of `dumps`.\r\n    """"""\r\n    return pa.deserialize(buf)\r\n\r\n\r\ndef dump_pkl(name, obj):\r\n    with open(\'{}.pkl\'.format(name), \'wb\') as f:\r\n        pickle.dump( obj, f, pickle.HIGHEST_PROTOCOL )\r\n\r\ndef load_pkl(name):\r\n    with open(\'{}.pkl\'.format(name), \'rb\') as f:\r\n        ret = pickle.load( f )\r\n    return ret\r\n\r\nif pa is None:\r\n    loads = loads_msgpack\r\n    dumps = dumps_msgpack\r\nelse:\r\n    loads = loads_pyarrow\r\n    dumps = dumps_pyarrow\r\n\r\n'"
lib/tfflat/timer.py,0,"b'# --------------------------------------------------------\r\n# Fast R-CNN\r\n# Copyright (c) 2015 Microsoft\r\n# Licensed under The MIT License [see LICENSE for details]\r\n# Written by Ross Girshick\r\n# --------------------------------------------------------\r\n\r\nimport time\r\n\r\nclass Timer(object):\r\n    """"""A simple timer.""""""\r\n    def __init__(self):\r\n        self.total_time = 0.\r\n        self.calls = 0\r\n        self.start_time = 0.\r\n        self.diff = 0.\r\n        self.average_time = 0.\r\n        self.warm_up = 0\r\n\r\n    def tic(self):\r\n        # using time.time instead of time.clock because time time.clock\r\n        # does not normalize for multithreading\r\n        self.start_time = time.time()\r\n\r\n    def toc(self, average=True):\r\n        self.diff = time.time() - self.start_time\r\n        if self.warm_up < 10:\r\n            self.warm_up += 1\r\n            return self.diff\r\n        else:\r\n            self.total_time += self.diff\r\n            self.calls += 1\r\n            self.average_time = self.total_time / self.calls\r\n\r\n        if average:\r\n            return self.average_time\r\n        else:\r\n            return self.diff\r\n'"
lib/tfflat/utils.py,0,"b'#!/usr/bin/env python\r\n# -*- coding: UTF-8 -*-\r\n\r\nimport os\r\nimport sys\r\nimport numpy as np\r\nfrom datetime import datetime\r\n\r\ndef mem_info():\r\n    import subprocess\r\n    dev = subprocess.check_output(\r\n        ""nvidia-smi | grep MiB | awk -F \'|\' \'{print $3}\' | awk -F \'/\' \'{print $1}\' | grep -Eo \'[0-9]{1,10}\'"",\r\n        shell=True)\r\n    dev = dev.decode()\r\n    dev_mem = list(map(lambda x: int(x), dev.split(\'\\n\')[:-1]))\r\n    return dev_mem\r\n\r\ndef add_pypath(path):\r\n    if path not in sys.path:\r\n        sys.path.insert(0, path)\r\n\r\ndef make_link(dest_path, link_path):\r\n    if os.path.islink(link_path):\r\n        os.system(\'rm {}\'.format(link_path))\r\n    os.system(\'ln -s {} {}\'.format(dest_path, link_path))\r\n\r\ndef make_dir(path):\r\n    if os.path.exists(path) or os.path.islink(path):\r\n        return\r\n    os.makedirs(path)\r\n\r\ndef del_file(path, msg=\'{} deleted.\'):\r\n    if os.path.exists(path):\r\n        os.remove(path)\r\n        print(msg.format(path))\r\n\r\ndef approx_equal(a, b, eps=1e-9):\r\n    return np.fabs(a-b) < eps\r\n\r\ndef get_rng(obj=None):\r\n    """"""\r\n    Get a good RNG seeded with time, pid and the object.\r\n\r\n    Args:\r\n        obj: some object to use to generate random seed.\r\n    Returns:\r\n        np.random.RandomState: the RNG.\r\n    """"""\r\n    seed = (id(obj) + os.getpid() +\r\n            int(datetime.now().strftime(""%Y%m%d%H%M%S%f""))) % 4294967295\r\n    return np.random.RandomState(seed)\r\n\r\n'"
lib/utils/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n\n'
lib/utils/blob.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Blob helper functions.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\n\n\ndef im_list_to_blob(ims):\n  """"""Convert a list of images into a network input.\n\n  Assumes images are already prepared (means subtracted, BGR order, ...).\n  """"""\n  max_shape = np.array([im.shape for im in ims]).max(axis=0)\n  num_images = len(ims)\n  blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\n                  dtype=np.float32)\n  for i in range(num_images):\n    im = ims[i]\n    blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n\n  return blob\n\n\ndef prep_im_for_blob(im, pixel_means, target_size, max_size):\n  """"""Mean subtract and scale an image for use in a blob.""""""\n  im = im.astype(np.float32, copy=False)\n  im -= pixel_means\n  im_shape = im.shape\n  im_size_min = np.min(im_shape[0:2])\n  im_size_max = np.max(im_shape[0:2])\n  im_scale = float(target_size) / float(im_size_min)\n  # Prevent the biggest axis from being more than MAX_SIZE\n  if np.round(im_scale * im_size_max) > max_size:\n    im_scale = float(max_size) / float(im_size_max)\n  im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n                  interpolation=cv2.INTER_LINEAR)\n\n  return im, im_scale\n'"
lib/utils/boxes_grid.py,0,"b'# --------------------------------------------------------\n# Subcategory CNN\n# Copyright (c) 2015 CVGL Stanford\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Yu Xiang\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport math\nfrom config import cfg\n\n\ndef get_boxes_grid(image_height, image_width):\n  """"""\n  Return the boxes on image grid.\n  """"""\n\n  # height and width of the heatmap\n  if cfg.NET_NAME == \'CaffeNet\':\n    height = np.floor((image_height * max(cfg.TRAIN.SCALES) - 1) / 4.0 + 1)\n    height = np.floor((height - 1) / 2.0 + 1 + 0.5)\n    height = np.floor((height - 1) / 2.0 + 1 + 0.5)\n\n    width = np.floor((image_width * max(cfg.TRAIN.SCALES) - 1) / 4.0 + 1)\n    width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n    width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n  elif cfg.NET_NAME == \'VGGnet\':\n    height = np.floor(image_height * max(cfg.TRAIN.SCALES) / 2.0 + 0.5)\n    height = np.floor(height / 2.0 + 0.5)\n    height = np.floor(height / 2.0 + 0.5)\n    height = np.floor(height / 2.0 + 0.5)\n\n    width = np.floor(image_width * max(cfg.TRAIN.SCALES) / 2.0 + 0.5)\n    width = np.floor(width / 2.0 + 0.5)\n    width = np.floor(width / 2.0 + 0.5)\n    width = np.floor(width / 2.0 + 0.5)\n  else:\n    assert (1), \'The network architecture is not supported in utils.get_boxes_grid!\'\n\n  # compute the grid box centers\n  h = np.arange(height)\n  w = np.arange(width)\n  y, x = np.meshgrid(h, w, indexing=\'ij\')\n  centers = np.dstack((x, y))\n  centers = np.reshape(centers, (-1, 2))\n  num = centers.shape[0]\n\n  # compute width and height of grid box\n  area = cfg.TRAIN.KERNEL_SIZE * cfg.TRAIN.KERNEL_SIZE\n  aspect = cfg.TRAIN.ASPECTS  # height / width\n  num_aspect = len(aspect)\n  widths = np.zeros((1, num_aspect), dtype=np.float32)\n  heights = np.zeros((1, num_aspect), dtype=np.float32)\n  for i in range(num_aspect):\n    widths[0, i] = math.sqrt(area / aspect[i])\n    heights[0, i] = widths[0, i] * aspect[i]\n\n  # construct grid boxes\n  centers = np.repeat(centers, num_aspect, axis=0)\n  widths = np.tile(widths, num).transpose()\n  heights = np.tile(heights, num).transpose()\n\n  x1 = np.reshape(centers[:, 0], (-1, 1)) - widths * 0.5\n  x2 = np.reshape(centers[:, 0], (-1, 1)) + widths * 0.5\n  y1 = np.reshape(centers[:, 1], (-1, 1)) - heights * 0.5\n  y2 = np.reshape(centers[:, 1], (-1, 1)) + heights * 0.5\n\n  boxes_grid = np.hstack((x1, y1, x2, y2)) / cfg.TRAIN.SPATIAL_SCALE\n\n  return boxes_grid, centers[:, 0], centers[:, 1]\n'"
lib/utils/mask.py,0,"b'import json\nimport numpy as np\nimport cv2\nfrom pycocotools import mask as COCOmask\n\ndef showMask(img_obj):\n    img = cv2.imread(img_obj[\'fpath\'])\n    img_ori = img.copy()\n    gtmasks = img_obj[\'gtmasks\']\n    n = len(gtmasks)\n    print(img.shape)\n    for i, mobj in enumerate(gtmasks):\n        if not (type(mobj[\'mask\']) is list):\n            print(""Pass a RLE mask"")\n            continue\n        else:\n            pts = np.round(np.asarray(mobj[\'mask\'][0]))\n            pts = pts.reshape(pts.shape[0] // 2, 2)\n            pts = np.int32(pts)\n            color = np.uint8(np.random.rand(3) * 255).tolist()\n            cv2.fillPoly(img, [pts], color)\n    cv2.addWeighted(img, 0.5, img_ori, 0.5, 0, img)\n    cv2.imshow(""Mask"", img)\n    cv2.waitKey(0)\n\ndef get_seg(height, width, seg_ann):\n    label = np.zeros((height, width, 1))\n    if type(seg_ann) == list or type(seg_ann) == np.ndarray:\n        for s in seg_ann:\n            poly = np.array(s, np.int).reshape(len(s)//2, 2)\n            cv2.fillPoly(label, [poly], 1)\n    else:\n        if type(seg_ann[\'counts\']) == list:\n            rle = COCOmask.frPyObjects([seg_ann], label.shape[0], label.shape[1])\n        else:\n            rle = [seg_ann]\n        # we set the ground truth as one-hot\n        m = COCOmask.decode(rle) * 1\n        label[label == 0] = m[label == 0]\n    return label[:, :, 0]\n'"
lib/utils/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n        self.warm_up = 0\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        if self.warm_up < 100:\n            self.warm_up += 1\n            return self.diff\n        else:\n            self.total_time += self.diff\n            self.calls += 1\n            self.average_time = self.total_time / self.calls\n\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n'"
