file_path,api_count,code
build.py,0,"b'import os, sys\nimport subprocess\nimport argparse\nimport datetime\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--name\', help=\'name of docker container\', default=""ml-workspace"")\nparser.add_argument(\'--version\', help=\'version tag of docker container\', default=""latest"")\nparser.add_argument(\'--deploy\', help=\'deploy docker container to remote\', action=\'store_true\')\nparser.add_argument(\'--flavor\', help=\'flavor (full, light, minimal) used for docker container\', default=\'full\')\n\nREMOTE_IMAGE_PREFIX = ""mltooling/""\n\nargs, unknown = parser.parse_known_args()\nif unknown:\n    print(""Unknown arguments ""+str(unknown))\n\n# Wrapper to print out command\ndef call(command):\n    print(""Executing: ""+command)\n    return subprocess.call(command, shell=True)\n\n# calls build scripts in every module with same flags\ndef build(module="".""):\n    \n    if not os.path.isdir(module):\n        print(""Could not find directory for "" + module)\n        sys.exit(1)\n    \n    build_command = ""python build.py""\n\n    if args.version:\n        build_command += "" --version="" + str(args.version)\n\n    if args.deploy:\n        build_command += "" --deploy""\n \n    if args.flavor:\n        build_command += "" --flavor="" + str(args.flavor)\n\n    working_dir = os.path.dirname(os.path.realpath(__file__))\n    full_command = ""cd "" + module + "" && "" + build_command + "" && cd "" + working_dir\n    print(""Building "" + module + "" with: "" + full_command)\n    failed = call(full_command)\n    if failed:\n        print(""Failed to build module "" + module)\n        sys.exit(1)\n\nif not args.flavor:\n    args.flavor = ""full""\n\nargs.flavor = str(args.flavor).lower()\n\nif args.flavor == ""all"":\n    args.flavor = ""full""\n    build()\n    args.flavor = ""light""\n    build()\n    args.flavor = ""minimal""\n    build()\n    args.flavor = ""r""\n    build()\n    args.flavor = ""spark""\n    build()\n    args.flavor = ""gpu""\n    build()\n    sys.exit(0)\n\n# unknown flavor -> try to build from subdirectory\nif args.flavor not in [""full"", ""minimal"", ""light""]:\n    # assume that flavor has its own directory with build.py\n    build(args.flavor + ""-flavor"")\n    sys.exit(0)\n\nservice_name = os.path.basename(os.path.dirname(os.path.realpath(__file__)))\nif args.name:\n    service_name = args.name\n\n# Build full image without suffix if the flavor is not minimal or light\nif args.flavor in [""minimal"", ""light""]:\n    service_name += ""-"" + args.flavor\n\n# docker build\ngit_rev = ""unknown""\ntry:\n    git_rev = subprocess.check_output([""git"", ""rev-parse"", ""--short"", ""HEAD""]).decode(\'ascii\').strip()\nexcept:\n    pass\n\nbuild_date = datetime.datetime.utcnow().isoformat(""T"") + ""Z""\ntry:\n    build_date = subprocess.check_output([\'date\', \'-u\', \'+%Y-%m-%dT%H:%M:%SZ\']).decode(\'ascii\').strip()\nexcept:\n    pass\n\nvcs_ref_build_arg = "" --build-arg ARG_VCS_REF="" + str(git_rev)\nbuild_date_build_arg = "" --build-arg ARG_BUILD_DATE="" + str(build_date)\nflavor_build_arg = "" --build-arg ARG_WORKSPACE_FLAVOR="" + str(args.flavor)\nversion_build_arg = "" --build-arg ARG_WORKSPACE_VERSION="" + str(args.version)\n\nversioned_image = service_name+"":""+str(args.version)\nlatest_image = service_name+"":latest""\nfailed = call(""docker build -t ""+ versioned_image + "" -t "" + latest_image + "" "" \n            + version_build_arg + "" "" + flavor_build_arg+ "" "" + vcs_ref_build_arg + "" "" + build_date_build_arg + "" ./"")\n\nif failed:\n    print(""Failed to build container"")\n    sys.exit(1)\n\nremote_versioned_image = REMOTE_IMAGE_PREFIX + versioned_image\ncall(""docker tag "" + versioned_image + "" "" + remote_versioned_image)\n\nremote_latest_image = REMOTE_IMAGE_PREFIX + latest_image\ncall(""docker tag "" + latest_image + "" "" + remote_latest_image)\n\nif args.deploy:\n    call(""docker push "" + remote_versioned_image)\n\n    if ""SNAPSHOT"" not in args.version:\n    # do not push SNAPSHOT builds as latest version\n        call(""docker push "" + remote_latest_image)\n'"
gpu-flavor/build.py,0,"b'import os, sys\nimport subprocess\nimport argparse\nimport datetime\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--name\', help=\'base name of docker container\', default=""ml-workspace"")\nparser.add_argument(\'--version\', help=\'version tag of docker container\', default=""latest"")\nparser.add_argument(\'--deploy\', help=\'deploy docker container to remote\', action=\'store_true\')\nparser.add_argument(\'--flavor\', help=\'flavor (gpu) used for docker container\', default=\'gpu\')\n\nREMOTE_IMAGE_PREFIX = ""mltooling/""\n\nargs, unknown = parser.parse_known_args()\nif unknown:\n    print(""Unknown arguments ""+str(unknown))\n\n# Wrapper to print out command\ndef call(command):\n    print(""Executing: ""+command)\n    return subprocess.call(command, shell=True)\n\n# calls build scripts in every module with same flags\ndef build(module="".""):\n    \n    if not os.path.isdir(module):\n        print(""Could not find directory for "" + module)\n        sys.exit(1)\n    \n    build_command = ""python build.py""\n\n    if args.version:\n        build_command += "" --version="" + str(args.version)\n\n    if args.deploy:\n        build_command += "" --deploy""\n \n    if args.flavor:\n        build_command += "" --flavor="" + str(args.flavor)\n\n    working_dir = os.path.dirname(os.path.realpath(__file__))\n    full_command = ""cd "" + module + "" && "" + build_command + "" && cd "" + working_dir\n    print(""Building "" + module + "" with: "" + full_command)\n    failed = call(full_command)\n    if failed:\n        print(""Failed to build module "" + module)\n        sys.exit(1)\n\nif not args.flavor:\n    args.flavor = ""gpu""\n\nargs.flavor = str(args.flavor).lower()\n\nif args.flavor == ""all"":\n    args.flavor = ""gpu""\n    build()\n    sys.exit(0)\n\n# unknown flavor -> try to build from subdirectory\nif args.flavor not in [""gpu""]:\n    # assume that flavor has its own directory with build.py\n    build(args.flavor + ""-flavor"")\n    sys.exit(0)\n\nservice_name = os.path.basename(os.path.dirname(os.path.realpath(__file__)))\nif args.name:\n    service_name = args.name\n\n# add flavor to service name\nservice_name += ""-"" + args.flavor\n\n# docker build\ngit_rev = ""unknown""\ntry:\n    git_rev = subprocess.check_output([""git"", ""rev-parse"", ""--short"", ""HEAD""]).decode(\'ascii\').strip()\nexcept:\n    pass\n\nbuild_date = datetime.datetime.utcnow().isoformat(""T"") + ""Z""\ntry:\n    build_date = subprocess.check_output([\'date\', \'-u\', \'+%Y-%m-%dT%H:%M:%SZ\']).decode(\'ascii\').strip()\nexcept:\n    pass\n\nvcs_ref_build_arg = "" --build-arg ARG_VCS_REF="" + str(git_rev)\nbuild_date_build_arg = "" --build-arg ARG_BUILD_DATE="" + str(build_date)\nflavor_build_arg = "" --build-arg ARG_WORKSPACE_FLAVOR="" + str(args.flavor)\nversion_build_arg = "" --build-arg ARG_WORKSPACE_VERSION="" + str(args.version)\n\nversioned_image = service_name+"":""+str(args.version)\nlatest_image = service_name+"":latest""\nfailed = call(""docker build -t ""+ versioned_image + "" -t "" + latest_image + "" "" \n            + version_build_arg + "" "" + flavor_build_arg+ "" "" + vcs_ref_build_arg + "" "" + build_date_build_arg + "" ./"")\n\nif failed:\n    print(""Failed to build container"")\n    sys.exit(1)\n\nremote_versioned_image = REMOTE_IMAGE_PREFIX + versioned_image\ncall(""docker tag "" + versioned_image + "" "" + remote_versioned_image)\n\nremote_latest_image = REMOTE_IMAGE_PREFIX + latest_image\ncall(""docker tag "" + latest_image + "" "" + remote_latest_image)\n\nif args.deploy:\n    call(""docker push "" + remote_versioned_image)\n\n    if ""SNAPSHOT"" not in args.version:\n    # do not push SNAPSHOT builds as latest version\n        call(""docker push "" + remote_latest_image)\n'"
r-flavor/build.py,0,"b'import os, sys\nimport subprocess\nimport argparse\nimport datetime\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--name\', help=\'base name of docker container\', default=""ml-workspace"")\nparser.add_argument(\'--version\', help=\'version tag of docker container\', default=""latest"")\nparser.add_argument(\'--deploy\', help=\'deploy docker container to remote\', action=\'store_true\')\nparser.add_argument(\'--flavor\', help=\'flavor (r) used for docker container\', default=\'r\')\n\nREMOTE_IMAGE_PREFIX = ""mltooling/""\n\nargs, unknown = parser.parse_known_args()\nif unknown:\n    print(""Unknown arguments ""+str(unknown))\n\n# Wrapper to print out command\ndef call(command):\n    print(""Executing: ""+command)\n    return subprocess.call(command, shell=True)\n\n# calls build scripts in every module with same flags\ndef build(module="".""):\n    \n    if not os.path.isdir(module):\n        print(""Could not find directory for "" + module)\n        sys.exit(1)\n    \n    build_command = ""python build.py""\n\n    if args.version:\n        build_command += "" --version="" + str(args.version)\n\n    if args.deploy:\n        build_command += "" --deploy""\n \n    if args.flavor:\n        build_command += "" --flavor="" + str(args.flavor)\n\n    working_dir = os.path.dirname(os.path.realpath(__file__))\n    full_command = ""cd "" + module + "" && "" + build_command + "" && cd "" + working_dir\n    print(""Building "" + module + "" with: "" + full_command)\n    failed = call(full_command)\n    if failed:\n        print(""Failed to build module "" + module)\n        sys.exit(1)\n\nif not args.flavor:\n    args.flavor = ""r""\n\nargs.flavor = str(args.flavor).lower()\n\nif args.flavor == ""all"":\n    args.flavor = ""r""\n    build()\n    sys.exit(0)\n\n# unknown flavor -> try to build from subdirectory\nif args.flavor not in [""r""]:\n    # assume that flavor has its own directory with build.py\n    build(args.flavor + ""-flavor"")\n    sys.exit(0)\n\nservice_name = os.path.basename(os.path.dirname(os.path.realpath(__file__)))\nif args.name:\n    service_name = args.name\n\n# add flavor to service name\nservice_name += ""-"" + args.flavor\n\n# docker build\ngit_rev = ""unknown""\ntry:\n    git_rev = subprocess.check_output([""git"", ""rev-parse"", ""--short"", ""HEAD""]).decode(\'ascii\').strip()\nexcept:\n    pass\n\nbuild_date = datetime.datetime.utcnow().isoformat(""T"") + ""Z""\ntry:\n    build_date = subprocess.check_output([\'date\', \'-u\', \'+%Y-%m-%dT%H:%M:%SZ\']).decode(\'ascii\').strip()\nexcept:\n    pass\n\nvcs_ref_build_arg = "" --build-arg ARG_VCS_REF="" + str(git_rev)\nbuild_date_build_arg = "" --build-arg ARG_BUILD_DATE="" + str(build_date)\nflavor_build_arg = "" --build-arg ARG_WORKSPACE_FLAVOR="" + str(args.flavor)\nversion_build_arg = "" --build-arg ARG_WORKSPACE_VERSION="" + str(args.version)\n\nversioned_image = service_name+"":""+str(args.version)\nlatest_image = service_name+"":latest""\nfailed = call(""docker build -t ""+ versioned_image + "" -t "" + latest_image + "" "" \n            + version_build_arg + "" "" + flavor_build_arg+ "" "" + vcs_ref_build_arg + "" "" + build_date_build_arg + "" ./"")\n\nif failed:\n    print(""Failed to build container"")\n    sys.exit(1)\n\nremote_versioned_image = REMOTE_IMAGE_PREFIX + versioned_image\ncall(""docker tag "" + versioned_image + "" "" + remote_versioned_image)\n\nremote_latest_image = REMOTE_IMAGE_PREFIX + latest_image\ncall(""docker tag "" + latest_image + "" "" + remote_latest_image)\n\nif args.deploy:\n    call(""docker push "" + remote_versioned_image)\n\n    if ""SNAPSHOT"" not in args.version:\n    # do not push SNAPSHOT builds as latest version\n        call(""docker push "" + remote_latest_image)\n'"
resources/docker-entrypoint.py,0,"b'#!/usr/bin/python\n\n""""""\nMain Workspace Run Script\n""""""\n\nfrom subprocess import call\nimport os\nimport math\nimport sys\nfrom urllib.parse import quote\n\n# Enable logging\nimport logging\nlogging.basicConfig(\n    format=\'%(asctime)s [%(levelname)s] %(message)s\', \n    level=logging.INFO, \n    stream=sys.stdout)\n\nlog = logging.getLogger(__name__)\n\nlog.info(""Starting..."")\n\ndef set_env_variable(env_variable: str, value: str, ignore_if_set: bool = False):\n    if ignore_if_set and os.getenv(env_variable, None):\n        # if it is already set, do not set it to the new value\n        return\n    # TODO is export needed as well?\n    call(\'export \' + env_variable + \'=""\' + value + \'""\', shell=True)\n    os.environ[env_variable] = value\n\n# Manage base path dynamically\n\nENV_JUPYTERHUB_SERVICE_PREFIX = os.getenv(""JUPYTERHUB_SERVICE_PREFIX"")\n\nENV_NAME_WORKSPACE_BASE_URL = ""WORKSPACE_BASE_URL""\nbase_url = os.environ[ENV_NAME_WORKSPACE_BASE_URL]\n\nif not base_url:\n    base_url = """"\n\nif ENV_JUPYTERHUB_SERVICE_PREFIX:\n    # Installation with Jupyterhub\n    \n    # Base Url is not needed, Service prefix contains full path\n    # ENV_JUPYTERHUB_BASE_URL = os.getenv(""JUPYTERHUB_BASE_URL"")\n    # ENV_JUPYTERHUB_BASE_URL.rstrip(\'/\') + \n    base_url = ENV_JUPYTERHUB_SERVICE_PREFIX\n\n# Add leading slash\nif not base_url.startswith(""/""):\n    base_url = ""/"" + base_url\n\n# Remove trailing slash\nbase_url = base_url.rstrip(\'/\').strip()\n# always quote base url\nbase_url = quote(base_url, safe=""/%"")\n\nset_env_variable(ENV_NAME_WORKSPACE_BASE_URL, base_url)\n\n# Dynamically set MAX_NUM_THREADS\nENV_MAX_NUM_THREADS = os.getenv(""MAX_NUM_THREADS"", None)\nif ENV_MAX_NUM_THREADS:\n    # Determine the number of availabel CPU resources, but limit to a max number\n    if ENV_MAX_NUM_THREADS.lower() == ""auto"":\n        ENV_MAX_NUM_THREADS = str(math.ceil(os.cpu_count()))\n        try:\n            # read out docker information - if docker limits cpu quota\n            cpu_count = math.ceil(int(os.popen(\'cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us\').read().replace(\'\\n\', \'\')) / 100000)\n            if cpu_count > 0 and cpu_count < os.cpu_count():\n                ENV_MAX_NUM_THREADS = str(cpu_count)\n        except:\n            pass\n        if not ENV_MAX_NUM_THREADS or not ENV_MAX_NUM_THREADS.isnumeric() or ENV_MAX_NUM_THREADS == ""0"":\n            ENV_MAX_NUM_THREADS = ""4""\n        \n        if int(ENV_MAX_NUM_THREADS) > 8:\n            # there should be atleast one thread less compared to cores\n            ENV_MAX_NUM_THREADS = str(int(ENV_MAX_NUM_THREADS)-1)\n        \n        # set a maximum of 32, in most cases too many threads are adding too much overhead\n        if int(ENV_MAX_NUM_THREADS) > 32:\n            ENV_MAX_NUM_THREADS = ""32""\n    \n    # only set if it is not None or empty\n    # OMP_NUM_THREADS: Suggested value: vCPUs / 2 in which vCPUs is the number of virtual CPUs. \n    set_env_variable(""OMP_NUM_THREADS"", ENV_MAX_NUM_THREADS, ignore_if_set=True) # OpenMP\n    set_env_variable(""OPENBLAS_NUM_THREADS"", ENV_MAX_NUM_THREADS, ignore_if_set=True) # OpenBLAS\n    set_env_variable(""MKL_NUM_THREADS"", ENV_MAX_NUM_THREADS, ignore_if_set=True) # MKL\n    set_env_variable(""VECLIB_MAXIMUM_THREADS"", ENV_MAX_NUM_THREADS, ignore_if_set=True) # Accelerate\n    set_env_variable(""NUMEXPR_NUM_THREADS"", ENV_MAX_NUM_THREADS, ignore_if_set=True) # Numexpr\n    set_env_variable(""NUMEXPR_MAX_THREADS"", ENV_MAX_NUM_THREADS, ignore_if_set=True) # Numexpr - maximum\n    set_env_variable(""NUMBA_NUM_THREADS"", ENV_MAX_NUM_THREADS, ignore_if_set=True) # Numba\n    set_env_variable(""SPARK_WORKER_CORES"", ENV_MAX_NUM_THREADS, ignore_if_set=True) # Spark Worker\n    # TBB_NUM_THREADS\n    # GOTO_NUM_THREADS\n\nENV_RESOURCES_PATH = os.getenv(""RESOURCES_PATH"", ""/resources"")\nENV_WORKSPACE_HOME = os.getenv(\'WORKSPACE_HOME\', ""/workspace"")\n\n# pass all script arguments to next script\nscript_arguments = "" "" + \' \'.join(sys.argv[1:])\n\nEXECUTE_CODE = os.getenv(\'EXECUTE_CODE\', None)\nif EXECUTE_CODE:\n    # use workspace as working directory\n    sys.exit(call(""cd "" + ENV_WORKSPACE_HOME + "" && python "" + ENV_RESOURCES_PATH + ""/scripts/execute_code.py"" + script_arguments, shell=True))\n\nsys.exit(call(""python "" + ENV_RESOURCES_PATH + ""/scripts/run_workspace.py"" + script_arguments, shell=True))'"
spark-flavor/build.py,0,"b'import os, sys\nimport subprocess\nimport argparse\nimport datetime\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--name\', help=\'base name of docker container\', default=""ml-workspace"")\nparser.add_argument(\'--version\', help=\'version tag of docker container\', default=""latest"")\nparser.add_argument(\'--deploy\', help=\'deploy docker container to remote\', action=\'store_true\')\nparser.add_argument(\'--flavor\', help=\'flavor (spark) used for docker container\', default=\'spark\')\n\nREMOTE_IMAGE_PREFIX = ""mltooling/""\n\nargs, unknown = parser.parse_known_args()\nif unknown:\n    print(""Unknown arguments ""+str(unknown))\n\n# Wrapper to print out command\ndef call(command):\n    print(""Executing: ""+command)\n    return subprocess.call(command, shell=True)\n\n# calls build scripts in every module with same flags\ndef build(module="".""):\n    \n    if not os.path.isdir(module):\n        print(""Could not find directory for "" + module)\n        sys.exit(1)\n    \n    build_command = ""python build.py""\n\n    if args.version:\n        build_command += "" --version="" + str(args.version)\n\n    if args.deploy:\n        build_command += "" --deploy""\n \n    if args.flavor:\n        build_command += "" --flavor="" + str(args.flavor)\n\n    working_dir = os.path.dirname(os.path.realpath(__file__))\n    full_command = ""cd "" + module + "" && "" + build_command + "" && cd "" + working_dir\n    print(""Building "" + module + "" with: "" + full_command)\n    failed = call(full_command)\n    if failed:\n        print(""Failed to build module "" + module)\n        sys.exit(1)\n\nif not args.flavor:\n    args.flavor = ""spark""\n\nargs.flavor = str(args.flavor).lower()\n\nif args.flavor == ""all"":\n    args.flavor = ""spark""\n    build()\n    sys.exit(0)\n\n# unknown flavor -> try to build from subdirectory\nif args.flavor not in [""spark""]:\n    # assume that flavor has its own directory with build.py\n    build(args.flavor + ""-flavor"")\n    sys.exit(0)\n\nservice_name = os.path.basename(os.path.dirname(os.path.realpath(__file__)))\nif args.name:\n    service_name = args.name\n\n# add flavor to service name\nservice_name += ""-"" + args.flavor\n\n# docker build\ngit_rev = ""unknown""\ntry:\n    git_rev = subprocess.check_output([""git"", ""rev-parse"", ""--short"", ""HEAD""]).decode(\'ascii\').strip()\nexcept:\n    pass\n\nbuild_date = datetime.datetime.utcnow().isoformat(""T"") + ""Z""\ntry:\n    build_date = subprocess.check_output([\'date\', \'-u\', \'+%Y-%m-%dT%H:%M:%SZ\']).decode(\'ascii\').strip()\nexcept:\n    pass\n\nvcs_ref_build_arg = "" --build-arg ARG_VCS_REF="" + str(git_rev)\nbuild_date_build_arg = "" --build-arg ARG_BUILD_DATE="" + str(build_date)\nflavor_build_arg = "" --build-arg ARG_WORKSPACE_FLAVOR="" + str(args.flavor)\nversion_build_arg = "" --build-arg ARG_WORKSPACE_VERSION="" + str(args.version)\n\nversioned_image = service_name+"":""+str(args.version)\nlatest_image = service_name+"":latest""\nfailed = call(""docker build -t ""+ versioned_image + "" -t "" + latest_image + "" "" \n            + version_build_arg + "" "" + flavor_build_arg+ "" "" + vcs_ref_build_arg + "" "" + build_date_build_arg + "" ./"")\n\nif failed:\n    print(""Failed to build container"")\n    sys.exit(1)\n\nremote_versioned_image = REMOTE_IMAGE_PREFIX + versioned_image\ncall(""docker tag "" + versioned_image + "" "" + remote_versioned_image)\n\nremote_latest_image = REMOTE_IMAGE_PREFIX + latest_image\ncall(""docker tag "" + latest_image + "" "" + remote_latest_image)\n\nif args.deploy:\n    call(""docker push "" + remote_versioned_image)\n\n    if ""SNAPSHOT"" not in args.version:\n    # do not push SNAPSHOT builds as latest version\n        call(""docker push "" + remote_latest_image)\n'"
resources/jupyter/ipython_config.py,0,"b""# Make matplotlib output in Jupyter notebooks display correctly\nc = get_config()\n\nc.IPKernelApp.matplotlib = 'inline'"""
resources/jupyter/jupyter_notebook_config.py,0,"b'from jupyter_core.paths import jupyter_data_dir\nimport subprocess\nimport os\nimport psutil\nimport errno\nimport stat\n\nc = get_config()\n# https://jupyter-notebook.readthedocs.io/en/stable/config.html\nc.NotebookApp.ip = \'*\'\nc.NotebookApp.port = 8090\nc.NotebookApp.notebook_dir=""./""\nc.NotebookApp.open_browser = False\nc.NotebookApp.allow_root=True\n# https://forums.fast.ai/t/jupyter-notebook-enhancements-tips-and-tricks/17064/22\nc.NotebookApp.iopub_msg_rate_limit = 100000000\nc.NotebookApp.iopub_data_rate_limit=2147483647\nc.NotebookApp.port_retries=0\nc.NotebookApp.quit_button=False\nc.NotebookApp.allow_remote_access=True\nc.NotebookApp.disable_check_xsrf=True\nc.NotebookApp.allow_origin=\'*\'\nc.NotebookApp.trust_xheaders=True\n# c.NotebookApp.log_level=""WARN""\n\nc.JupyterApp.answer_yes = True\n\n# set base url if available\nbase_url = os.getenv(""WORKSPACE_BASE_URL"", ""/"")\nif base_url is not None and base_url is not ""/"":\n    c.NotebookApp.base_url=base_url\n\n# Do not delete files to trash: https://github.com/jupyter/notebook/issues/3130\nc.FileContentsManager.delete_to_trash=False\n\n# Always use inline for matplotlib\nc.IPKernelApp.matplotlib = \'inline\'\n\nshutdown_inactive_kernels = os.getenv(""SHUTDOWN_INACTIVE_KERNELS"", ""false"")\nif shutdown_inactive_kernels and shutdown_inactive_kernels.lower().strip() != ""false"":\n    cull_timeout = 172800 # default is 48 hours\n    try: \n        # see if env variable is set as timout integer\n        cull_timeout = int(shutdown_inactive_kernels)\n    except ValueError:\n        pass\n    \n    if cull_timeout > 0:\n        print(""Activating automatic kernel shutdown after "" + str(cull_timeout) + ""s of inactivity."")\n        # Timeout (in seconds) after which a kernel is considered idle and ready to be shutdown.\n        c.MappingKernelManager.cull_idle_timeout = cull_timeout\n        # Do not shutdown if kernel is busy (e.g on long-running kernel cells)\n        c.MappingKernelManager.cull_busy = False\n        # Do not shutdown kernels that are connected via browser, activate?\n        c.MappingKernelManager.cull_connected = False\n\nauthenticate_via_jupyter = os.getenv(""AUTHENTICATE_VIA_JUPYTER"", ""false"")\nif authenticate_via_jupyter and authenticate_via_jupyter.lower().strip() != ""false"":\n    # authentication via jupyter is activated\n\n    # Do not allow password change since it currently needs a server restart to accept the new password\n    c.NotebookApp.allow_password_change = False\n\n    if authenticate_via_jupyter.lower().strip() == ""<generated>"":\n        # dont do anything to let jupyter generate a token in print out on console\n        pass\n    # if true, do not set any token, authentication will be activate on another way (e.g. via JupyterHub)\n    elif authenticate_via_jupyter.lower().strip() != ""true"":\n        # if not true or false, set value as token\n        c.NotebookApp.token = authenticate_via_jupyter\nelse:\n    # Deactivate token -> no authentication\n    c.NotebookApp.token=""""\n\n# https://github.com/timkpaine/jupyterlab_iframe\ntry:\n    if not base_url.startswith(""/""):\n        base_url = ""/"" + base_url\n    # iframe plugin currently needs absolut URLS\n    c.JupyterLabIFrame.iframes = [base_url + \'tools/ungit\', base_url + \'tools/netdata\', base_url + \'tools/vnc\', base_url + \'tools/glances\', base_url + \'tools/vscode\']\nexcept:\n    pass\n\n# https://github.com/timkpaine/jupyterlab_templates\nWORKSPACE_HOME = os.getenv(""WORKSPACE_HOME"", ""/workspace"")\ntry:\n    if os.path.exists(WORKSPACE_HOME + \'/templates\'):\n        c.JupyterLabTemplates.template_dirs = [WORKSPACE_HOME + \'/templates\']\n    c.JupyterLabTemplates.include_default = False\nexcept:\n    pass\n\n# Set memory limits for resource use display: https://github.com/yuvipanda/nbresuse\ntry:\n    mem_limit = None\n    if os.path.isfile(""/sys/fs/cgroup/memory/memory.limit_in_bytes""):\n        with open(\'/sys/fs/cgroup/memory/memory.limit_in_bytes\', \'r\') as file:\n            mem_limit = file.read().replace(\'\\n\', \'\').strip()\n    \n    total_memory = psutil.virtual_memory().total\n\n    if not mem_limit:\n        mem_limit = total_memory\n    elif int(mem_limit) > int(total_memory):\n        # if mem limit from cgroup bigger than total memory -> use total memory\n        mem_limit = total_memory\n    \n    # Workaround -> round memory limit, otherwise the number is quite long\n    # TODO fix in nbresuse\n    mem_limit = round(int(mem_limit) / (1024 * 1024)) * (1024 * 1024)\n    c.ResourceUseDisplay.mem_limit = int(mem_limit)\n    c.ResourceUseDisplay.mem_warning_threshold=0.1\nexcept:\n    pass\n\n# Change default umask for all subprocesses of the notebook server if set in\n# the environment\nif \'NB_UMASK\' in os.environ:\n    os.umask(int(os.environ[\'NB_UMASK\'], 8))\n'"
resources/jupyter/tensorboard_notebook_patch.py,0,"b'""""""%tensorboard line magic that patches TensorBoard\'s implementation to make use of Jupyter \nTensorBoard server extension providing built-in proxying.\nUse:\n    %load_ext tensorboard.notebook\n    %tensorboard --logdir /logs\n""""""\n\nimport argparse\nimport uuid\n\nfrom IPython.display import display, HTML, Javascript\n\ndef _tensorboard_magic(line):\n    """"""Line magic function.\n    Makes an AJAX call to the Jupyter TensorBoard server extension and outputs\n    an IFrame displaying the TensorBoard instance.\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--logdir\', default=\'/workspace/environment/\')\n    args = parser.parse_args(line.split())\n    \n    iframe_id = \'tensorboard-\' + str(uuid.uuid4())\n        \n    html = """"""\n<!-- JUPYTER_TENSORBOARD_TEST_MARKER -->\n<script>\n    fetch(Jupyter.notebook.base_url + \'api/tensorboard\', {\n        method: \'POST\',\n        contentType: \'application/json\',\n        body: JSON.stringify({ \'logdir\': \'%s\' }),\n        headers: { \'Content-Type\': \'application/json\' }\n    })\n        .then(res => res.json())\n        .then(res => {\n            const iframe = document.getElementById(\'%s\');\n            iframe.src = Jupyter.notebook.base_url + \'tensorboard/\' + res.name;\n            iframe.style.display = \'block\';\n        });\n</script>\n<iframe\n    id=""%s""\n    style=""width: 100%%; height: 620px; display: none;""\n    frameBorder=""0"">\n</iframe>\n"""""" % (args.logdir, iframe_id, iframe_id)\n    \n    display(HTML(html))\n    \ndef load_ipython_extension(ipython):\n    """"""IPython extension entry point.""""""\n    ipython.register_magic_function(\n        _tensorboard_magic,\n        magic_kind=\'line\',\n        magic_name=\'tensorboard\',\n    )\n\ndef _load_ipython_extension(ipython):\n    """"""Load the TensorBoard notebook extension.\n    Intended to be called from `%load_ext tensorboard`. Do not invoke this\n    directly.\n    Args:\n      ipython: An `IPython.InteractiveShell` instance.\n    """"""\n    load_ipython_extension(ipython)'"
resources/scripts/backup_restore_config.py,0,"b'#!/usr/bin/python\n\n# System libraries\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport logging\nimport os\nimport random\nimport subprocess\nimport sys\nimport time\n\n# Enable logging\nlogging.basicConfig(\n    format=\'%(asctime)s [%(levelname)s] %(message)s\', \n    level=logging.INFO, \n    stream=sys.stdout)\n\nlog = logging.getLogger(__name__)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'mode\', type=str, default=""backup"", help=\'Either backup or restore the workspace configuration.\',\n                    choices=[""backup"", ""restore"", ""schedule""])\n\nargs, unknown = parser.parse_known_args()\nif unknown:\n    log.info(""Unknown arguments "" + str(unknown))\n\nWORKSPACE_HOME = os.getenv(\'WORKSPACE_HOME\')\nUSER_HOME = os.getenv(\'HOME\')\nRESOURCE_FOLDER = os.getenv(\'RESOURCES_PATH\')\nCONFIG_BACKUP_ENABLED = os.getenv(\'CONFIG_BACKUP_ENABLED\')\nCONFIG_BACKUP_FOLDER = WORKSPACE_HOME + ""/.workspace/backup/""\n\nif args.mode == ""restore"":\n    if CONFIG_BACKUP_ENABLED is None or CONFIG_BACKUP_ENABLED.lower() == ""false"" or CONFIG_BACKUP_ENABLED.lower() == ""off"":\n        log.info(""Configuration Backup is not activated. Restore process will not be started."")\n        sys.exit()\n\n    log.info(""Running config backup restore."")\n\n    if not os.path.exists(CONFIG_BACKUP_FOLDER) or len(os.listdir(CONFIG_BACKUP_FOLDER)) == 0:\n        log.info(""Nothing to restore. Config backup folder is empty."")\n        sys.exit()\n    \n    # set verbose? -v\n    rsync_restore =  ""rsync -a -r -t -z -E -X -A "" + CONFIG_BACKUP_FOLDER + "" "" + USER_HOME\n    log.debug(""Run rsync restore: "" + rsync_restore)\n    subprocess.call(rsync_restore, shell=True)\nelif args.mode == ""backup"":\n    if not os.path.exists(CONFIG_BACKUP_FOLDER):\n        os.makedirs(CONFIG_BACKUP_FOLDER)\n    \n    log.info(""Starting configuration backup."")\n    backup_selection = ""--include=\'/.config\' \\\n                        --include=\'/.config/xfce4/\' --include=\'/.config/xfce4/xfconf/***\' \\\n                        --include=\'/.config/Code/\' --include=\'/.config/Code/User/\' --include=\'/.config/Code/User/settings.json\' \\\n                        --include=\'/.config/gtk-3.0/\' --include=\'/.config/gtk-3.0/bookmarks\' \\\n                        --include=\'/.gitconfig\' \\\n                        --include=\'/filebrowser.db\' \\\n                        --include=\'/.local/\' --include=\'/.local/share/\' --include=\'/.local/share/jupyter/\' --include=\'/.local/share/jupyter/kernels/***\' \\\n                        --include=\'/.jupyter/***\'""\n    # Do not backup vscode extensions? --include=\'/.vscode/***\' \\\n    \n    # TODO configure selection via environemnt flag? \n    # set verbose? -v\n    rsync_backup =  ""rsync -a -r -t -z -E -X -A --delete-excluded --max-size=100m \\\n                        "" + backup_selection + "" \\\n                        --exclude=\'/.ssh/environment\' --include=\'/.ssh/***\' \\\n                        --exclude=\'*\' "" + USER_HOME + ""/ "" + CONFIG_BACKUP_FOLDER\n    log.debug(""Run rsync backup: "" + rsync_backup)\n    subprocess.call(rsync_backup, shell=True)\n\nelif args.mode == ""schedule"":\n    DEFAULT_CRON = ""0 * * * *""  # every hour\n\n    if CONFIG_BACKUP_ENABLED is None or CONFIG_BACKUP_ENABLED.lower() == ""false"" or CONFIG_BACKUP_ENABLED.lower() == ""off"":\n        log.info(""Configuration Backup is not activated."")\n        sys.exit()\n\n    if not os.path.exists(CONFIG_BACKUP_FOLDER):\n        os.makedirs(CONFIG_BACKUP_FOLDER)\n    \n    from crontab import CronTab, CronSlices\n\n    cron_schedule = DEFAULT_CRON\n    # env variable can also be a cron scheadule\n    if CronSlices.is_valid(CONFIG_BACKUP_ENABLED):\n        cron_schedule = CONFIG_BACKUP_ENABLED\n    \n    # Cron does not provide enviornment variables, source them manually\n    environment_file = os.path.join(RESOURCE_FOLDER, ""environment.sh"")\n    with open(environment_file, \'w\') as fp:\n        for env in os.environ:\n            if env != ""LS_COLORS"":\n                fp.write(""export "" + env + ""=\\"""" + os.environ[env] + ""\\""\\n"")\n\n    os.chmod(environment_file, 0o777)\n\n    script_file_path = os.path.realpath(__file__)\n    command = "". "" + environment_file + ""; "" + sys.executable + "" \'"" + script_file_path + ""\' backup> /proc/1/fd/1 2>/proc/1/fd/2""\n    cron = CronTab(user=True)\n\n    # remove all other backup tasks\n    cron.remove_all(command=command)\n\n    job = cron.new(command=command)\n    if CronSlices.is_valid(cron_schedule):\n        log.info(""Scheduling cron config backup task with with cron: "" + cron_schedule)\n        job.setall(cron_schedule)\n        job.enable()\n        cron.write()\n    else:\n        log.info(""Failed to schedule config backup. Cron is not valid."")\n\n    log.info(""Running cron jobs:"")\n    for job in cron:\n        log.info(job)\n'"
resources/scripts/check_xfdesktop_leak.py,0,"b'#!/usr/bin/python\n\n# System libraries\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport logging\nimport os\nimport random\nimport sys\nimport time\nimport psutil\nimport subprocess\n\n# Enable logging\nlogging.basicConfig(\n    format=\'%(asctime)s [%(levelname)s] %(message)s\', \n    level=logging.INFO, \n    stream=sys.stdout)\n\ndef get_process_id(name):\n    """"""Return process ids found by (partial) name.\n\n    >>> get_process_id(\'kthreadd\')\n    [2]\n    >>> get_process_id(\'watchdog\')\n    [10, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56, 61]  # ymmv\n    >>> get_process_id(\'non-existent process\')\n    []\n    """"""\n    return list(map(int,subprocess.check_output([""pidof"",name]).split()))\n\nlog = logging.getLogger(__name__)\n\n# Studio Libraries\nparser = argparse.ArgumentParser()\nparser.add_argument(\'mode\', type=str, default=""check"", help=\'Either check or schedule the xfdesktop leak check.\',\n                    choices=[""check"", ""schedule""])\n\nargs, unknown = parser.parse_known_args()\nif unknown:\n    log.info(""Unknown arguments "" + str(unknown))\n\nif args.mode == ""check"":\n    log.debug(""Starting xfdesktop leak check."")\n    \n    CHECKS = 10 # number of checks\n    CHECK_INTERVAL = 15 # seconds\n    MEMORY_THRESHOLD = 120000000 # == 120 MB\n    CPU_THRESHOLD = 1 # 1 % average - low value because probably relativ to cpus\n    MAX_MEMORY_THRESHOLD = 250000000 # == 250MB if it reaches this memory, kill anyways (regardless of CPU)\n    \n    # TODO generify for any process\n\n    # Get processes\n    try:\n        xfdesktop_pids = get_process_id(""xfdesktop"")\n        if len(xfdesktop_pids) > 1:\n            log.info(""Multiple processes found for xfdesktop: "" + str(xfdesktop_pids))\n        xfdesktop_process = psutil.Process(int(xfdesktop_pids[0]))\n\n        xfsettingsd_pids = get_process_id(""xfsettingsd"")\n        if len(xfsettingsd_pids) > 1:\n            log.info(""Multiple processes found for xfsettingsd: "" + str(xfsettingsd_pids))\n        xfsettingsd_process = psutil.Process(int(xfsettingsd_pids[0]))\n\n        xfce4panel_pids = get_process_id(""xfce4-panel"")\n        if len(xfce4panel_pids) > 1:\n            log.info(""Multiple processes found for xfce4-panel: "" + str(xfce4panel_pids))\n        xfce4panel_process = psutil.Process(int(xfce4panel_pids[0]))\n    except:\n        # could not find processes\n        log.info(""Failed to find xfdesktop, xfsettingsd, or xfce4-panel."")\n        sys.exit()\n\n    # Initial stats (on the first call those stats give wrong data):\n    xfdesktop_process.memory_info().rss\n    xfdesktop_process.cpu_percent()\n    xfsettingsd_process.memory_info().rss\n    xfsettingsd_process.cpu_percent()\n    xfce4panel_process.memory_info().rss\n    xfce4panel_process.cpu_percent()\n\n    xfdesktop_memory_sum = 0\n    xfdesktop_cpu_sum = 0\n    xfsettingsd_memory_sum = 0\n    xfsettingsd_cpu_sum = 0\n    xfce4panel_memory_sum = 0\n    xfce4panel_cpu_sum = 0\n\n    for i in range(CHECKS):\n        time.sleep(CHECK_INTERVAL) \n        # xfdesktop\n        xfdesktop_memory_sum += xfdesktop_process.memory_info().rss\n        # always call cpu percentage twice... otherwise it might be 0.0\n        xfdesktop_process.cpu_percent()\n        time.sleep(1) \n        xfdesktop_cpu_sum += xfdesktop_process.cpu_percent()\n        \n        # xfsettingsd\n        xfsettingsd_memory_sum += xfsettingsd_process.memory_info().rss\n        # always call cpu percentage twice... otherwise it might be 0.0\n        xfsettingsd_process.cpu_percent()\n        time.sleep(1)\n        xfsettingsd_cpu_sum += xfsettingsd_process.cpu_percent()\n\n        # xfce4panel\n        xfce4panel_memory_sum += xfce4panel_process.memory_info().rss\n        # always call cpu percentage twice... otherwise it might be 0.0\n        xfce4panel_process.cpu_percent()\n        time.sleep(1)\n        xfce4panel_cpu_sum += xfce4panel_process.cpu_percent()\n    \n    xfdesktop_memory_avg = xfdesktop_memory_sum/CHECKS\n    xfdesktop_cpu_avg = xfdesktop_cpu_sum/CHECKS\n    xfsettingsd_memory_avg = xfsettingsd_memory_sum/CHECKS\n    xfsettingsd_cpu_avg = xfsettingsd_cpu_sum/CHECKS\n    xfce4panel_memory_avg = xfce4panel_memory_sum/CHECKS\n    xfce4panel_cpu_avg = xfce4panel_cpu_sum/CHECKS\n\n    log.info(""Leak check: xfdesktop (mem="" + str(xfdesktop_memory_avg) + "" cpu="" + str(xfdesktop_cpu_avg) + ""); xfsettingsd (mem="" + str(xfsettingsd_memory_avg) + "" cpu="" + str(xfsettingsd_cpu_avg) + ""); xfce4-panel (mem="" + str(xfce4panel_memory_avg) + "" cpu="" + str(xfce4panel_cpu_avg) + "")"")\n\n    if (xfdesktop_memory_avg > MEMORY_THRESHOLD and xfdesktop_cpu_avg > CPU_THRESHOLD) or xfdesktop_memory_avg > MAX_MEMORY_THRESHOLD:\n        log.info(""xfdesktop process is leaking. Kill xfdesktop processes!"")\n        xfdesktop_process.kill()\n    \n    if (xfsettingsd_memory_avg > MEMORY_THRESHOLD and xfsettingsd_cpu_avg > CPU_THRESHOLD) or xfsettingsd_memory_avg > MAX_MEMORY_THRESHOLD:\n        log.info(""xfsettingsd process is leaking. Kill xfsettingsd processes!"")\n        xfsettingsd_process.kill()\n\n    if (xfce4panel_memory_avg > MEMORY_THRESHOLD and xfce4panel_cpu_avg > CPU_THRESHOLD) or xfce4panel_memory_avg > MAX_MEMORY_THRESHOLD:\n        log.info(""xfce4-panel process is leaking. Kill xfce4-panel processes!"")\n        xfce4panel_process.kill()\n        time.sleep(5)\n        log.info(""Start xfce4-panel again."")\n        log.info(""xfce4-panel started with exit code: "" + str(subprocess.call(""xfce4-panel"", shell=True)))\n\nelif args.mode == ""schedule"":\n    DEFAULT_CRON = ""0 * * * *""  # every hour\n    \n    from crontab import CronTab, CronSlices\n\n    cron_schedule = DEFAULT_CRON\n\n    script_file_path = os.path.realpath(__file__)\n    command = sys.executable + "" \'"" + script_file_path + ""\' check> /proc/1/fd/1 2>/proc/1/fd/2""\n\n    cron = CronTab(user=True)\n\n    # remove all other tasks\n    cron.remove_all(command=command)\n\n    job = cron.new(command=command)\n    if CronSlices.is_valid(cron_schedule):\n        log.info(""Scheduling cron check xfdesktop task with with cron: "" + cron_schedule)\n        job.setall(cron_schedule)\n        job.enable()\n        cron.write()\n    else:\n        log.info(""Failed to schedule check xfdesktop. Cron is not valid."")\n\n    log.info(""Running cron jobs:"")\n    for job in cron:\n        log.info(job)'"
resources/scripts/configure_cron_scripts.py,0,"b'#!/usr/bin/python\n\n""""""\nConfigure and run cron scripts\n""""""\n\nfrom subprocess import call\nimport os\nimport sys\n\n# Enable logging\nimport logging\nlogging.basicConfig(\n    format=\'%(asctime)s [%(levelname)s] %(message)s\', \n    level=logging.INFO, \n    stream=sys.stdout)\n\nlog = logging.getLogger(__name__)\n\nENV_RESOURCES_PATH = os.getenv(""RESOURCES_PATH"", ""/resources"")\n\n# start check xfdesktop leak process\ncall(""python "" + ENV_RESOURCES_PATH + ""/scripts/check_xfdesktop_leak.py schedule"", shell=True)\n\n# Conifg Backup \n\n# backup config directly on startup (e.g. ssh key)\ncall(""python "" + ENV_RESOURCES_PATH + ""/scripts/backup_restore_config.py backup"", shell=True)\n\n# start backup restore config process\ncall(""python "" + ENV_RESOURCES_PATH + ""/scripts/backup_restore_config.py schedule"", shell=True)'"
resources/scripts/configure_nginx.py,0,"b'#!/usr/bin/python\n\n""""""\nConfigure and start nginx service\n""""""\n\nfrom subprocess import call\nimport os\nimport sys\nfrom urllib.parse import quote, unquote\n\n# Enable logging\nimport logging\nlogging.basicConfig(\n    format=\'%(asctime)s [%(levelname)s] %(message)s\', \n    level=logging.INFO, \n    stream=sys.stdout)\n\nlog = logging.getLogger(__name__)\n\nENV_RESOURCES_PATH = os.getenv(""RESOURCES_PATH"", ""/resources"")\n\n# Basic Auth\n\nENV_NAME_SERVICE_USER = ""WORKSPACE_AUTH_USER""\nENV_NAME_SERVICE_PASSWORD = ""WORKSPACE_AUTH_PASSWORD""\nENV_SERVICE_USER = None\nENV_SERVICE_PASSWORD = None\n\nif ENV_NAME_SERVICE_USER in os.environ:\n    ENV_SERVICE_USER = os.environ[ENV_NAME_SERVICE_USER]\nif ENV_NAME_SERVICE_PASSWORD in os.environ:\n    ENV_SERVICE_PASSWORD = os.environ[ENV_NAME_SERVICE_PASSWORD]\n\nNGINX_FILE = ""/etc/nginx/nginx.conf""\n\n# Replace base url placeholders with actual base url -> should \ndecoded_base_url = unquote(os.getenv(""WORKSPACE_BASE_URL"", """").rstrip(\'/\'))\ncall(""sed -i \'s@{WORKSPACE_BASE_URL_DECODED}@"" + decoded_base_url + ""@g\' "" + NGINX_FILE, shell=True)\n# Set url escaped url\nencoded_base_url = quote(decoded_base_url, safe=""/%"")\ncall(""sed -i \'s@{WORKSPACE_BASE_URL_ENCODED}@"" + encoded_base_url + ""@g\' "" + NGINX_FILE, shell=True)\n\n# Activate or deactivate jupyter based authentication for tooling\ncall(""sed -i \'s@{AUTHENTICATE_VIA_JUPYTER}@"" + os.getenv(""AUTHENTICATE_VIA_JUPYTER"", ""false"").lower().strip() + ""@g\' "" + NGINX_FILE, shell=True)\n\ncall(""sed -i \'s@{SHARED_LINKS_ENABLED}@"" + os.getenv(""SHARED_LINKS_ENABLED"", ""false"").lower().strip() + ""@g\' "" + NGINX_FILE, shell=True)\n\n# Replace key hash with actual sha1 hash of key\ntry:\n    with open(os.getenv(""HOME"", ""/root"") + ""/.ssh/id_ed25519"", ""r"") as f:\n        private_key = f.read()\n\n    import hashlib\n    key_hasher = hashlib.sha1()\n    key_hasher.update(str.encode(str(private_key).lower().strip()))\n    key_hash = key_hasher.hexdigest()\n    \n    call(""sed -i \'s@{KEY_HASH}@"" + str(key_hash) + ""@g\' "" + NGINX_FILE, shell=True)\nexcept Exception as e:\n    log.error(""Error creating ssh key hash for nginx."", exc_info=True)\n\n# PREPARE SSL SERVING\nENV_NAME_SERVICE_SSL_ENABLED = ""WORKSPACE_SSL_ENABLED""\nif ENV_NAME_SERVICE_SSL_ENABLED in os.environ \\\n        and (os.environ[ENV_NAME_SERVICE_SSL_ENABLED] is True \\\n                or os.environ[ENV_NAME_SERVICE_SSL_ENABLED] == ""true"" \\\n                or os.environ[ENV_NAME_SERVICE_SSL_ENABLED] == ""on""):\n    ENV_SSL_RESOURCES_PATH =  os.getenv(""SSL_RESOURCES_PATH"", ""/resources/ssl"")\n\n    call(""sed -i \'s@#ssl_certificate_key@ssl_certificate_key "" + ENV_SSL_RESOURCES_PATH + ""/cert.key;@g\' "" + NGINX_FILE, shell=True)\n    call(""sed -i \'s@#ssl_certificate@ssl_certificate "" + ENV_SSL_RESOURCES_PATH + ""/cert.crt;@g\' "" + NGINX_FILE, shell=True)\n    # activate ssl in listen\n    call(""sed -i -r \'s/listen ([0-9]+);/listen \\\\1 ssl;/g\' "" + NGINX_FILE, shell=True)\n\n    # create / copy certificates -> only if SSL is enabled\n    call(ENV_RESOURCES_PATH + ""/scripts/setup-certs.sh"", shell=True)\n\n###\n\n# PREPARE BASIC AUTH\n# Basic Auth enablment is important for a standalone workspace deployment, as there the \n# /tools path is not protected by Jupyter\'s token!\nif ENV_SERVICE_USER and ENV_SERVICE_PASSWORD:\n\n    call(""sed -i \'s/#auth_basic /auth_basic /g\' "" + NGINX_FILE, shell=True)\n    call(""sed -i \'s/#auth_basic_user_file/auth_basic_user_file/g\' "" + NGINX_FILE, shell=True)\n\n    # create basic auth user\n    call(""echo \'"" + ENV_SERVICE_PASSWORD + ""\' | htpasswd -b -i -c /etc/nginx/.htpasswd \'""\\\n            + ENV_SERVICE_USER +""\'"", shell=True)\n###'"
resources/scripts/configure_ssh.py,0,"b'#!/usr/bin/python\n\n""""""\nConfigure ssh service\n""""""\n\nfrom subprocess import call\nimport os\nimport sys\n\n# Enable logging\nimport logging\nlogging.basicConfig(\n    format=\'%(asctime)s [%(levelname)s] %(message)s\', \n    level=logging.INFO, \n    stream=sys.stdout)\n\nlog = logging.getLogger(__name__)\n\nHOME = os.getenv(""HOME"", ""/root"")\nRESOURCE_FOLDER = os.getenv(\'RESOURCES_PATH\')\n\n# Export environment for ssh sessions\n#call(""printenv > $HOME/.ssh/environment"", shell=True)\nwith open(HOME + ""/.ssh/environment"", \'w\') as fp:\n    for env in os.environ:\n        if env == ""LS_COLORS"":\n            continue\n        # ignore most variables that get set by kubernetes if enableServiceLinks is not disabled\n        # https://github.com/kubernetes/kubernetes/pull/68754\n        if ""SERVICE_PORT"" in env.upper():\n            continue\n        if ""SERVICE_HOST"" in env.upper():\n            continue\n        if ""PORT"" in env.upper() and ""TCP"" in env.upper():\n            continue\n        fp.write(env + ""="" + str(os.environ[env]) + ""\\n"")\n\n### Generate SSH Key (for ssh access, also remote kernel access)\n# Generate a key pair without a passphrase (having the key should be enough) that can be used to ssh into the container\n# Add the public key to authorized_keys so someone with the public key can use it to ssh into the container\nSSH_KEY_NAME = ""id_ed25519"" # use default name instead of workspace_key\n# TODO add container and user information as a coment via -C\nif not os.path.isfile(HOME + ""/.ssh/""+SSH_KEY_NAME):\n    log.info(""Creating new SSH Key (""+ SSH_KEY_NAME + "")"")\n    # create ssh key if it does not exist yet\n    call(""ssh-keygen -f ~/.ssh/{} -t ed25519 -q -N \\""\\"" > /dev/null"".format(SSH_KEY_NAME), shell=True)\n\n# Copy public key to resources, otherwise nginx is not able to serve it\ncall(""/bin/cp -rf "" + HOME + ""/.ssh/id_ed25519.pub /resources/public-key.pub"", shell=True)\n\n# Make sure that knonw hosts and authorized keys exist\ncall(""touch "" + HOME + ""/.ssh/authorized_keys"", shell=True)\ncall(""touch "" + HOME + ""/.ssh/known_hosts"", shell=True)\n\n# echo """" >> ~/.ssh/authorized_keys will prepend a new line before the key is added to the file\ncall(""echo """" >> "" + HOME + ""/.ssh/authorized_keys"", shell=True)\n# only add to authrized key if it does not exist yet within the file\ncall(\'grep -qxF ""$(cat {home}/.ssh/{key_name}.pub)"" {home}/.ssh/authorized_keys || cat {home}/.ssh/{key_name}.pub >> {home}/.ssh/authorized_keys\'.format(home=HOME, key_name=SSH_KEY_NAME), shell=True)\n\n# Add identity to ssh agent -> e.g. can be used for git authorization\ncall(""eval \\""$(ssh-agent -s)\\"" && ssh-add "" + HOME + ""/.ssh/""+SSH_KEY_NAME + "" > /dev/null"", shell=True)\n\n# Fix permissions\n# https://superuser.com/questions/215504/permissions-on-private-key-in-ssh-folder\n# https://gist.github.com/grenade/6318301\n# https://help.ubuntu.com/community/SSH/OpenSSH/Keys\n\ncall(""chmod 700 ~/.ssh/"", shell=True)\ncall(""chmod 600 ~/.ssh/"" + SSH_KEY_NAME, shell=True)\ncall(""chmod 644 ~/.ssh/"" + SSH_KEY_NAME + "".pub"", shell=True)\n\n# TODO Config backup does not work when setting these:\n#call(""chmod 644 ~/.ssh/authorized_keys"", shell=True)\n#call(""chmod 644 ~/.ssh/known_hosts"", shell=True)\n#call(""chmod 644 ~/.ssh/config"", shell=True)\n# call(""chmod 700 ~/.ssh/"", shell=True)\n#call(""chmod -R 600 ~/.ssh/"", shell=True)\n#call(""chmod 644 ~/.ssh/authorized_keys"", shell=True)\n#call(""chmod 644 ~/.ssh/known_hosts"", shell=True)\n#call(""chmod 644 ~/.ssh/config"", shell=True)\n#call(""chmod 644 ~/.ssh/"" + SSH_KEY_NAME + "".pub"", shell=True)\n###'"
resources/scripts/configure_tools.py,0,"b'#!/usr/bin/python\n\n""""""\nConfigure and run tools\n""""""\n\nfrom subprocess import call\nimport os\nimport sys\n\n# Enable logging\nimport logging\nlogging.basicConfig(\n    format=\'%(asctime)s [%(levelname)s] %(message)s\', \n    level=logging.INFO, \n    stream=sys.stdout)\n\nlog = logging.getLogger(__name__)\n\nENV_RESOURCES_PATH = os.getenv(""RESOURCES_PATH"", ""/resources"")\nENV_WORKSPACE_HOME = os.getenv(""WORKSPACE_HOME"", ""/workspace"")\nHOME = os.getenv(""HOME"", ""/root"")\n\nDESKTOP_PATH = HOME + ""/Desktop""\n\n# Get jupyter token \nENV_AUTHENTICATE_VIA_JUPYTER = os.getenv(""AUTHENTICATE_VIA_JUPYTER"", ""false"")\n\ntoken_parameter = """"\nif ENV_AUTHENTICATE_VIA_JUPYTER.lower() == ""true"":\n    # Check if started via Jupyterhub -> JPY_API_TOKEN is set\n    ENV_JPY_API_TOKEN = os.getenv(""JPY_API_TOKEN"", None)\n    if ENV_JPY_API_TOKEN:\n        token_parameter = ""?token="" + ENV_JPY_API_TOKEN\nelif ENV_AUTHENTICATE_VIA_JUPYTER and ENV_AUTHENTICATE_VIA_JUPYTER.lower() != ""false"":\n    token_parameter = ""?token="" + ENV_AUTHENTICATE_VIA_JUPYTER\n\n# Create Jupyter Shortcut - at runtime since the jupyterhub token is needed\nurl = \'http://localhost:8092\' + token_parameter\nshortcut_metadata = \'[Desktop Entry]\\nVersion=1.0\\nType=Link\\nName=Jupyter Notebook\\nComment=\\nCategories=Development;\\nIcon=\' + ENV_RESOURCES_PATH + \'/icons/jupyter-icon.png\\nURL=\' + url\ncall(\'printf ""\' + shortcut_metadata + \'"" > \' + DESKTOP_PATH + \'/jupyter.desktop\', shell=True) # create a link on the Desktop to your Jupyter notebook server\ncall(\'chmod +x \' + DESKTOP_PATH + \'/jupyter.desktop\', shell=True) # Make executable\ncall(\'printf ""\' + shortcut_metadata + \'"" > /usr/share/applications/jupyter.desktop\', shell=True) # create a link in categories menu to your Jupyter notebook server\ncall(\'chmod +x /usr/share/applications/jupyter.desktop\', shell=True) # Make executable\n\n# Create Jupyter Lab Shortcut\nurl = \'http://localhost:8092\' + ""/lab"" + token_parameter\nshortcut_metadata = \'[Desktop Entry]\\nVersion=1.0\\nType=Link\\nName=Jupyter Lab\\nComment=\\nCategories=Development;\\nIcon=\' + ENV_RESOURCES_PATH + \'/icons/jupyterlab-icon.png\\nURL=\' + url\n\ncall(\'printf ""\' + shortcut_metadata + \'"" > /usr/share/applications/jupyterlab.desktop\', shell=True) # create a link in categories menu to your Jupyter Lab server\ncall(\'chmod +x /usr/share/applications/jupyterlab.desktop\', shell=True) # Make executable\n\n# Configure filebrowser - only if database file does not exist yet (e.g. isn\'t restored)\nif not os.path.exists(HOME + \'/filebrowser.db\'):\n    log.info(""Initialize filebrowser database."")\n    # Init filebrowser configuration - Surpress all output\n    call(\'filebrowser config init --database=\' + HOME + \'/filebrowser.db > /dev/null\', shell=True)\n\n    # Add admin user\n    import random, string\n    filebrowser_pwd = \'\'.join(random.sample(string.ascii_lowercase, 20))\n    log.info(""Create filebrowser admin with generated password: "" + filebrowser_pwd)\n    call(\'filebrowser users add admin \' + filebrowser_pwd + \' --perm.admin=true --database=\' + HOME + \'/filebrowser.db > /dev/null\', shell=True)\n\n    # Configure filebrowser\n    configure_filebrowser = \'filebrowser config set --root=""/"" --auth.method=proxy --auth.header=X-Token-Header \' \\\n                    + \' --branding.files=$RESOURCES_PATH""/filebrowser/"" --branding.name=""Filebrowser"" \' \\\n                    + \' --branding.disableExternal --signup=false --perm.admin=false --perm.create=false \' \\\n                    + \' --perm.delete=false --perm.download=true --perm.execute=false \' \\\n                    + \' --perm.admin=false --perm.create=false --perm.delete=false \' \\\n                    + \' --perm.modify=false --perm.rename=false --perm.share=false \' \\\n                    + \'  --database=\' + HOME + \'/filebrowser.db\'\n    # Port and base url is configured at startup - Surpress all output\n    call(configure_filebrowser + "" > /dev/null"", shell=True)\n\n# Tools are started via supervisor, see supervisor.conf'"
resources/scripts/execute_code.py,0,"b'#!/usr/bin/python\n\n""""""\nExecute code\n""""""\n\nfrom datetime import timedelta\nimport subprocess\nimport os\nimport sys\nimport time\n\n# Enable logging\nimport logging\nlogging.basicConfig(\n    format=\'%(asctime)s [%(levelname)s] %(message)s\', \n    level=logging.INFO, \n    stream=sys.stdout)\n\nlog = logging.getLogger(__name__)\n\n# Parse arguments\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--requirements-only\', help=\'Only install requirements without executing the code.\', action=\'store_true\')\nparser.add_argument(\'--code-only\', help=\'Only execute code without installing requirements.\', action=\'store_true\')\n\nargs, unknown = parser.parse_known_args()\nif unknown:\n    log.info(""Unknown arguments "" + str(unknown))\n\nstart_time = time.time()\n\nlog.info(""Execute Code..."")\n\n# Wrapper to print out command\ndef call(command):\n    log.info(""Executing: ""+command)\n    return subprocess.call(command, shell=True)\n\nRESOURCES_PATH = os.getenv(""RESOURCES_PATH"", ""/resources"")\nWORKSPACE_HOME = os.getenv(\'WORKSPACE_HOME\', ""/workspace"")\n\n# also allow command line argument as alternative to execute code (for workspace internal execution)\nEXECUTE_CODE = os.getenv(\'EXECUTE_CODE\', \' \'.join(sys.argv[1:]) if len(sys.argv) > 1 else None)\n\ncode_path = None \n\nif not EXECUTE_CODE:\n    log.info(""EXECUTE_CODE env variable is not set."")\n    sys.exit(1)\n\nif os.path.exists(EXECUTE_CODE):\n    code_path = EXECUTE_CODE\nelif EXECUTE_CODE.lower().startswith((""git+"", ""svn+"", ""hg+"", ""bzr+"")):\n    # Use pip to get vcs code: e.g. git+https://github.com/LukasMasuch/test-project.git#subdirectory=subdir\n    try:\n        from pip._internal.vcs import vcs\n        vcs_url = EXECUTE_CODE\n        vc_type, _ = vcs_url.split(\'+\', 1)\n\n        # Use tempfolder for cloning repository\n        import tempfile, shutil, atexit\n        code_path = tempfile.mkdtemp() \n        # automatically remove temp directory if process exits\n        def cleanup():\n            shutil.rmtree(code_path)\n        \n        atexit.register(cleanup)\n\n        vcs.get_backend(vc_type).export(code_path, url=vcs_url)\n        from pip._internal.models.link import Link\n        subdir = Link(vcs_url).subdirectory_fragment\n        if subdir:\n            code_path = os.path.join(code_path, subdir.lstrip(\'/\'))\n    except Exception as ex:\n        log.exception(""Failed to clone repository via pip internal."")\n\nif not code_path or not os.path.exists(code_path):\n    log.info(""No code artifacts could be found for "" + EXECUTE_CODE)\n    sys.exit(1)\n\n# code script is the file that actually executed -> if directory it needs a main module\nmain_script = code_path\n# Execute single script\nif os.path.isfile(code_path):\n    # set code path to the root folder of the script so it can also resolve conda env and requirements\n    code_path = os.path.dirname(os.path.realpath(code_path))\n\n# Execute code from folder -> this should always be a folder\nif os.path.isdir(code_path):\n    pip_runtime = ""pip""\n    python_runtime = ""python""\n    bash_runtime = ""/bin/bash""\n    \n    if not args.code_only:\n        log.info(""Searching requirements at path "" + main_script)\n        # Install requirements\n        # Check for conda environment file\n        conda_env_path = os.path.join(code_path, ""environment.yml"")\n        if os.path.isfile(conda_env_path):\n            conda_env_name = ""conda-env""\n            log.info(""Installing conda environment from "" + conda_env_path)\n            if call(""conda env create -n "" + conda_env_name + "" -f "" + conda_env_path) == 0:\n                # Set pip and python runtime to the conda environment\n                pip_runtime = ""/opt/conda/envs/"" + conda_env_name + ""/bin/pip""\n                python_runtime = ""/opt/conda/envs/"" + conda_env_name + ""/bin/python""\n                # put conda env into the bash runtime\n                bash_runtime = ""PATH=/opt/conda/envs/"" + conda_env_name + ""/bin/:$PATH /bin/bash""\n            else:\n                log.info(""Failed to install conda env from "" + conda_env_path)\n\n        # Check for setup.sh file - TODO should we execute this file after pip and conda?\n        setup_path = os.path.join(code_path, ""setup.sh"")\n        if os.path.isfile(setup_path):\n            log.info(""Running setup from "" + setup_path)\n            if call(bash_runtime + "" "" + setup_path) != 0:\n                log.info(""Failed to run setup.sh from "" + setup_path)\n    \n        # Check for requirements.txt file\n        requirements_path = os.path.join(code_path, ""requirements.txt"")\n        if os.path.isfile(requirements_path):\n            log.info(""Installing requirements from "" + requirements_path)\n            if call(pip_runtime + "" install --no-cache-dir -r "" + requirements_path) != 0:\n                log.info(""Failed to install requirements.txt from "" + requirements_path)\n    \n    if args.requirements_only:\n        log.info(""Finished installing requirements. Code execution is deactivated."")\n        sys.exit(0)\n    \n    log.info(""Executing python code at path "" + main_script)\n    # Run code: if it is a folder, it needs a main module (e.g. __main__.py)\n    exit_code = call(python_runtime + \' ""\' + main_script + \'""\')\n    if exit_code > 0:\n        log.info(""Execution failed with exit code: "" + str(exit_code))\n        if os.path.isdir(main_script):\n            log.info(""Please make sure that there is a main module (e.g. __main__.py) at this path: "" + main_script)\n    else:\n        log.info(""Code execution finished successfully."")\n    log.info(""Elapsed time: "" + str(timedelta(seconds=time.time() - start_time)))\n    sys.exit(exit_code)\n\nlog.info(""Something went wrong. This code should have never been reached."")\nsys.exit(1)'"
resources/scripts/run_custom_scripts.py,0,"b'#!/usr/bin/python\n\n""""""\nConfigure and run custom scripts\n""""""\n\nfrom subprocess import call\nimport os\nimport sys\n\n# Enable logging\nimport logging\nlogging.basicConfig(\n    format=\'%(asctime)s [%(levelname)s] %(message)s\', \n    level=logging.INFO, \n    stream=sys.stdout)\n\nlog = logging.getLogger(__name__)\n\nENV_RESOURCES_PATH = os.getenv(""RESOURCES_PATH"", ""/resources"")\n\n# Do nothing here, this file can be overwritten by containers that extend the workspace'"
resources/scripts/run_workspace.py,0,"b'#!/usr/bin/python\n\n""""""\nConfigure and run tools\n""""""\n\nfrom subprocess import call\nimport os\nimport sys\n\n# Enable logging\nimport logging\nlogging.basicConfig(\n    format=\'%(asctime)s [%(levelname)s] %(message)s\', \n    level=logging.INFO, \n    stream=sys.stdout)\n\nlog = logging.getLogger(__name__)\n\nlog.info(""Start Workspace"")\n\nENV_RESOURCES_PATH = os.getenv(""RESOURCES_PATH"", ""/resources"")\n\n# Include tutorials \nWORKSPACE_HOME = os.getenv(\'WORKSPACE_HOME\', ""/workspace"")\nINCLUDE_TUTORIALS = os.getenv(\'INCLUDE_TUTORIALS\', ""true"")\n\n# Only copy all content of tutorial folder to workspace folder if it is initialy empty\nif INCLUDE_TUTORIALS.lower() == ""true"" and os.path.exists(WORKSPACE_HOME) and len(os.listdir(WORKSPACE_HOME)) == 0:\n    log.info(""Copy tutorials to /workspace folder"")\n    from distutils.dir_util import copy_tree\n    # Copy all files within tutorials folder in resources to workspace home\n    copy_tree(os.path.join(ENV_RESOURCES_PATH, ""tutorials""), WORKSPACE_HOME)\n\n# restore config on startup - if CONFIG_BACKUP_ENABLED - it needs to run before other configuration \ncall(""python "" + ENV_RESOURCES_PATH + ""/scripts/backup_restore_config.py restore"", shell=True)\n\nlog.info(""Configure ssh service"")\ncall(""python "" + ENV_RESOURCES_PATH + ""/scripts/configure_ssh.py"", shell=True)\n\nlog.info(""Configure nginx service"")\ncall(""python "" + ENV_RESOURCES_PATH + ""/scripts/configure_nginx.py"", shell=True)\n\nlog.info(""Configure tools"")\ncall(""python "" + ENV_RESOURCES_PATH + ""/scripts/configure_tools.py"", shell=True)\n\nlog.info(""Configure cron scripts"")\ncall(""python "" + ENV_RESOURCES_PATH + ""/scripts/configure_cron_scripts.py"", shell=True)\n\nlog.info(""Configure and run custom scripts"")\ncall(""python "" + ENV_RESOURCES_PATH + ""/scripts/run_custom_scripts.py"", shell=True)\n\nstartup_custom_script = os.path.join(WORKSPACE_HOME, ""on_startup.sh"")\nif os.path.exists(startup_custom_script):\n    log.info(""Run on_startup.sh user script from workspace folder"")\n    # run startup script from workspace folder - can be used to run installation routines on workspace updates\n    call(""/bin/bash "" + startup_custom_script, shell=True)\n\n# Run supervisor process - main container process\ncall(\'supervisord -n -c /etc/supervisor/supervisord.conf\', shell=True)'"
resources/tests/test-code-execution.py,0,"b'#!/usr/bin/python\n\n""""""\nTest execute code functionality\n""""""\nimport subprocess\nimport os\nimport sys\nimport logging\n\nlogging.basicConfig(stream=sys.stdout, format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\nlog = logging.getLogger(__name__)\n\n# Wrapper to print out command\ndef call(command):\n    print(""Executing: ""+command)\n    return subprocess.call(command, shell=True)\n\nENV_RESOURCES_PATH = os.getenv(""RESOURCES_PATH"", ""/resources"")\n\nexit_code = call(ENV_RESOURCES_PATH + ""/scripts/execute_code.py "" + ENV_RESOURCES_PATH + ""/tests/ml-job/"")\n\nif exit_code == 0:\n    print(""Code execution test successfull."")\nelse:\n    print(""Code execution test failed."")\n    '"
resources/tests/test-installers.py,0,"b'#!/usr/bin/python\n\n""""""\nConfigure and run custom scripts\n""""""\nimport subprocess\nimport os\nimport sys\nimport logging\n\nlogging.basicConfig(stream=sys.stdout, format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\nlog = logging.getLogger(__name__)\n\n# Wrapper to print out command\ndef call(command):\n    print(""Executing: ""+command)\n    return subprocess.call(command, shell=True)\n\nENV_RESOURCES_PATH = os.getenv(""RESOURCES_PATH"", ""/resources"")\nTOOLS_DIR = os.path.join(ENV_RESOURCES_PATH, ""tools"")\nWORKSPACE_HOME = os.getenv(""WORKSPACE_HOME"", ""/workspace"")\n\ntest_results = {}\n\nfor filename in os.listdir(TOOLS_DIR):\n    if filename.endswith("".sh""):\n        script_path = os.path.join(TOOLS_DIR, filename)\n        print(""Testing "" + filename)\n        exit_code = call(script_path + "" --install"")\n        if exit_code == 0:\n            test_results[filename] = ""Installed successfully.""\n        else:\n            test_results[filename] = ""Failed to install.""\n        print(filename + "": "" + test_results[filename])\n\nprint(""###### TEST RESULTS ######"")\ntest_results_str = """"\nfor tool in test_results:\n    print(tool + "": "" + test_results[tool])\n    test_results_str += tool + "": "" + test_results[tool] + ""/n""\n\nos.makedirs(os.path.join(WORKSPACE_HOME, ""reports""), exist_ok=True)\nwith open(os.path.join(WORKSPACE_HOME, ""reports"", ""tool-installers-test.txt""),""w+"") as f:\n    f.write(test_results_str)\n'"
resources/tests/ml-job/__main__.py,0,b'from tqdm import tqdm\nimport time \n\nfor i in tqdm(range(5)):\n    time.sleep(1)'
resources/tests/ml-service/__main__.py,0,"b'#!/usr/local/bin/python3\n\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(""/"")\ndef read_root():\n    return {""Hello"": ""World""}\n\n\n@app.get(""/items/{item_id}"")\ndef read_item(item_id: int, q: str = None):\n    return {""item_id"": item_id, ""q"": q}\n\n\ndef patch_fastapi(app: FastAPI):\n    """"""\n    This patch is required to make fastapi fully functional with a relative url path.\n    This code snippet can be copy-pasted to any Fastapi application. \n    """"""\n    from fastapi.openapi.docs import get_swagger_ui_html\n    from starlette.requests import Request\n    from starlette.responses import HTMLResponse\n\n    async def swagger_ui_html(req: Request) -> HTMLResponse:\n        swagger_ui = get_swagger_ui_html(\n            openapi_url=""./""+app.openapi_url.lstrip(""/""),\n            title=app.title + "" - Swagger UI"",\n            oauth2_redirect_url=app.swagger_ui_oauth2_redirect_url,\n        )\n\n        # insert request interceptor to have all request run on relativ path\n        request_interceptor = \'requestInterceptor: (e)  => {\' \\\n            \'\\n\\t\\t\\tvar url = window.location.origin + window.location.pathname\' \\\n            \'\\n\\t\\t\\turl = url.substring( 0, url.lastIndexOf( ""/"" ) + 1);\' \\\n            \'\\n\\t\\t\\turl = e.url.replace(/http(s)?:\\/\\/[^/]*\\//i, url);\' \\\n            \'\\n\\t\\t\\te.contextUrl = url\' \\\n            \'\\n\\t\\t\\te.url = url\' \\\n            \'\\n\\t\\t\\treturn e;}\'\n\n        return HTMLResponse(swagger_ui.body.decode(""utf-8"").replace(""dom_id: \'#swagger-ui\',"", ""dom_id: \'#swagger-ui\',\\n\\t\\t"" + request_interceptor + "","") )\n\n    # remove old docs route and add our patched route\n    routes_new = []\n    for route in app.routes:\n        if route.path == ""/docs"":\n            continue\n        routes_new.append(route)\n\n    app.router.routes = routes_new\n    app.add_route(app.docs_url, swagger_ui_html, include_in_schema=False)\n\nprint(""Patch Fastapi to allow relative path resolution."")\npatch_fastapi(app)\n\nif __name__ == \'__main__\':\n    import uvicorn\n    uvicorn.run(app, host=""0.0.0.0"", port=8080, log_level=""info"", reload=True)\n'"
resources/jupyter/extensions/tooling-extension/setup.py,0,"b'import os, json\nfrom setuptools import setup\nfrom setuptools.command.install import install\n\nfrom notebook.nbextensions import install_nbextension\nfrom notebook.services.config import ConfigManager\nfrom jupyter_core.paths import jupyter_config_dir\n\nEXTENSION_NAME = ""jupyter_tooling""\nHANDLER_NAME = ""tooling_handler""\n\nOPEN_TOOLS_WIDGET = ""open-tools-widget""\nGIT_TREE_WIDGET = ""tooling-tree-widget""\nGIT_NOTEBOOK_WIDGET = ""tooling-notebook-widget""\n\nEXT_DIR = os.path.join(os.path.dirname(__file__), EXTENSION_NAME)\n\nclass InstallCommand(install):\n    def run(self):\n        open_tools_widget_path = EXTENSION_NAME+""/""+OPEN_TOOLS_WIDGET\n        git_tree_widget_path = EXTENSION_NAME+""/""+GIT_TREE_WIDGET\n        git_notebook_widget_path = EXTENSION_NAME+""/""+GIT_NOTEBOOK_WIDGET\n\n        # Install Python package\n        install.run(self)\n\n        # Install JavaScript extensions to ~/.local/jupyter/\n        install_nbextension(EXT_DIR, overwrite=True, user=True)\n\n        # Activate the JS extensions on the notebook, tree, and edit screens\n        js_cm = ConfigManager()\n        js_cm.update(\'tree\', {""load_extensions"": {open_tools_widget_path: True}})\n        js_cm.update(\'notebook\', {""load_extensions"": {open_tools_widget_path: True}})\n        js_cm.update(\'edit\', {""load_extensions"": {open_tools_widget_path: True}})\n\n        js_cm.update(\'notebook\', {""load_extensions"": {git_notebook_widget_path: True}})\n        js_cm.update(\'tree\', {""load_extensions"": {git_tree_widget_path: True}})\n\n        # Activate the Python server extension\n        server_extension_name = EXTENSION_NAME+"".""+HANDLER_NAME\n\n        jupyter_config_file = os.path.join(jupyter_config_dir(), ""jupyter_notebook_config.json"")\n        if not os.path.isfile(jupyter_config_file):\n            with open(jupyter_config_file, ""w"") as jsonFile:\n                initial_data = {\n                    ""NotebookApp"":{\n                        ""nbserver_extensions"": {},\n                        ""server_extensions"": []\n                    }\n                }\n                json.dump(initial_data, jsonFile, indent=4)\n\n        with open(jupyter_config_file, ""r"") as jsonFile:\n            data = json.load(jsonFile)\n            \n        if \'server_extensions\' not in data[\'NotebookApp\']:\n            data[\'NotebookApp\'][\'server_extensions\'] = []\n        \n        if \'nbserver_extensions\' not in data[\'NotebookApp\']:\n            data[\'NotebookApp\'][\'nbserver_extensions\'] = {}\n            \n        if server_extension_name not in data[\'NotebookApp\'][\'server_extensions\']:\n            data[\'NotebookApp\'][\'server_extensions\'] += [server_extension_name]\n        \n        data[\'NotebookApp\'][\'nbserver_extensions\'][server_extension_name] = True\n\n        with open(jupyter_config_file, ""w"") as jsonFile:\n            json.dump(data, jsonFile, indent=4)\n\n\nsetup(\n    name=\'Jupyter-Tooling-Extension\',\n    version=\'0.1\',\n    packages=[EXTENSION_NAME],\n    include_package_data=True,\n    cmdclass={\n        \'install\': InstallCommand\n    },\n    install_requires=[\n       \'GitPython\'\n    ]\n)\n\n\n'"
resources/jupyter/extensions/tooling-extension/jupyter_tooling/__init__.py,0,b''
resources/jupyter/extensions/tooling-extension/jupyter_tooling/tooling_handler.py,0,"b'import json\nimport glob\nimport os\nimport subprocess\nfrom subprocess import call\n\ntry:\n    from urllib.parse import unquote\nexcept ImportError:\n    from urllib import unquote\n\nimport warnings\nfrom datetime import datetime\n\nimport git\nimport tornado\nfrom notebook.base.handlers import IPythonHandler\nfrom notebook .utils import url_path_join\nfrom tornado import web\n\nSHARED_SSH_SETUP_PATH = ""/shared/ssh/setup""\nHOME = os.getenv(""HOME"", ""/root"")\nRESOURCES_PATH = os.getenv(""RESOURCES_PATH"", ""/resources"")\n\n# -------------- HANDLER -------------------------\nclass HelloWorldHandler(IPythonHandler):\n\n    def data_received(self, chunk):\n        pass\n\n    def get(self):\n        result = self.request.protocol + ""://"" + self.request.host\n        if \'base_url\' in self.application.settings:\n            result = result + ""   "" + self.application.settings[\'base_url\']\n        self.finish(result)\n\n\ndef handle_error(handler, status_code: int, error_msg: str = None, exception=None):\n    handler.set_status(status_code)\n\n    if not error_msg:\n        error_msg = """"\n\n    if exception:\n        if error_msg:\n            error_msg += ""\\nException: ""\n\n        error_msg += str(exception)\n\n    error = {\n        ""error"": error_msg\n    }\n    handler.finish(json.dumps(error))\n\n    log.info(""An error occurred ("" + str(status_code) + ""): "" + error_msg)\n\n\ndef send_data(handler, data):\n    handler.finish(json.dumps(data, sort_keys=True, indent=4))\n\nclass PingHandler(IPythonHandler):\n\n    @web.authenticated\n    def get(self):\n        # Used by Jupyterhub to test if user cookies are valid\n        self.finish(""Successful"")\n\nclass InstallToolHandler(IPythonHandler):\n\n    @web.authenticated\n    def get(self):\n        try:\n            workspace_installer_folder =  RESOURCES_PATH + \'/tools/\'\n            workspace_tool_installers = []\n            \n            # sort entries by name\n            for f in sorted(glob.glob(os.path.join(workspace_installer_folder, \'*.sh\'))):\n                tool_name = os.path.splitext(os.path.basename(f))[0].strip()\n                workspace_tool_installers.append({\n                            ""name"": tool_name,\n                            ""command"": ""/bin/bash "" + f})\n            \n            if not workspace_tool_installers:\n                log.warn(""No workspace tool installers found at path: "" + workspace_installer_folder)\n                # Backup if file does not exist\n                workspace_tool_installers.append({\n                            ""name"": ""none"",\n                            ""command"": ""No workspace tool installers found at path: "" + workspace_installer_folder})\n            self.finish(json.dumps(workspace_tool_installers))\n        except Exception as ex:\n            handle_error(self, 500, exception=ex)\n            return\n\nclass ToolingHandler(IPythonHandler):\n\n    @web.authenticated\n    def get(self):\n        try:\n            workspace_tooling_folder =  HOME + \'/.workspace/tools/\'\n            workspace_tools = []\n\n            def tool_is_duplicated(tool_array, tool):\n                """""" Tools with same ID should only be added once to the list""""""\n                for t in tool_array:\n                    if ""id"" in t and ""id"" in tool and tool[""id""] == t[""id""]:\n                        return True\n                return False\n            \n            # sort entries by name\n            for f in sorted(glob.glob(os.path.join(workspace_tooling_folder, \'*.json\'))):\n                try:\n                    with open(f, ""rb"") as tool_file:\n                        tool_data = json.load(tool_file)\n                        if not tool_data:\n                            continue\n                        if isinstance(tool_data, dict):\n                            if not tool_is_duplicated(workspace_tools, tool_data):\n                                workspace_tools.append(tool_data)\n                        else:\n                            # tool data is probably an array\n                            for tool in tool_data:\n                                if not tool_is_duplicated(workspace_tools, tool):\n                                    workspace_tools.append(tool)\n                except:\n                    log.warn(""Failed to load tools file: "" + f.name)\n                    continue\n\n            if not workspace_tools:\n                log.warn(""No workspace tools found at path: "" + workspace_tooling_folder)\n                # Backup if file does not exist\n                workspace_tools.append({""id"": ""vnc-link"",\n                            ""name"": ""VNC"",\n                            ""url_path"": ""/tools/vnc/?password=vncpassword"",\n                            ""description"": ""Desktop GUI for the workspace""})\n            self.finish(json.dumps(workspace_tools))\n        except Exception as ex:\n            handle_error(self, 500, exception=ex)\n            return\n\nclass GitCommitHandler(IPythonHandler):\n\n    @web.authenticated\n    def post(self):\n        data = self.get_json_body()\n\n        if data is None:\n            handle_error(self, 400, ""Please provide a valid file path and commit msg in body."")\n            return\n\n        if ""filePath"" not in data or not data[""filePath""]:\n            handle_error(self, 400, ""Please provide a valid filePath in body."")\n            return\n\n        file_path = _resolve_path(unquote(data[""filePath""]))\n\n        commit_msg = None\n        if ""commitMsg"" in data:\n            commit_msg = unquote(data[""commitMsg""])\n\n        try:\n            commit_file(file_path, commit_msg)\n        except Exception as ex:\n            handle_error(self, 500, exception=ex)\n            return\n\n\nclass GitInfoHandler(IPythonHandler):\n\n    @web.authenticated\n    def get(self):\n        try:\n            path = _resolve_path(self.get_argument(\'path\', None))\n            send_data(self, get_git_info(path))\n        except Exception as ex:\n            handle_error(self, 500, exception=ex)\n            return\n\n    @web.authenticated\n    def post(self):\n\n        path = _resolve_path(self.get_argument(\'path\', None))\n        data = self.get_json_body()\n\n        if data is None:\n            handle_error(self, 400, ""Please provide a valid name and email in body."")\n            return\n\n        if ""email"" not in data or not data[""email""]:\n            handle_error(self, 400, ""Please provide a valid email."")\n            return\n\n        email = data[""email""]\n\n        if ""name"" not in data or not data[""name""]:\n            handle_error(self, 400, ""Please provide a valid name."")\n            return\n\n        name = data[""name""]\n\n        try:\n            repo = get_repo(path)\n            set_user_email(email, repo)\n            set_user_name(name, repo)\n        except Exception as ex:\n            handle_error(self, 500, exception=ex)\n            return\n\nclass SSHScriptHandler(IPythonHandler):\n\n    @web.authenticated\n    def get(self):\n        try:\n            handle_ssh_script_request(self)\n        except Exception as ex:\n            handle_error(self, 500, exception=ex)\n            return\n\nclass SharedSSHHandler(IPythonHandler):\n     def get(self):\n        # authentication only via token\n        try:\n            sharing_enabled = os.environ.get(""SHARED_LINKS_ENABLED"", ""false"")\n            if sharing_enabled.lower() != ""true"":\n                handle_error(self, 401, error_msg=""Shared links are disabled. Please download and execute the SSH script manually."")\n                return\n            \n            token = self.get_argument(\'token\', None)\n            valid_token = generate_token(self.request.path)\n            if not token:\n                self.set_status(401)\n                self.finish(\'echo ""Please provide a token via get parameter.""\')\n                return\n            if token.lower().strip() != valid_token:\n                self.set_status(401)\n                self.finish(\'echo ""The provided token is not valid.""\')\n                return\n            \n            handle_ssh_script_request(self)\n        except Exception as ex:\n            handle_error(self, 500, exception=ex)\n            return\n\nclass SSHCommandHandler(IPythonHandler):\n\n    @web.authenticated\n    def get(self):\n        try:\n            sharing_enabled = os.environ.get(""SHARED_LINKS_ENABLED"", ""false"")\n            if sharing_enabled.lower() != ""true"":\n                self.finish(""Shared links are disabled. Please download and executen the SSH script manually."")\n                return\n            \n            # schema + host + port\n            origin = self.get_argument(\'origin\', None)\n            if not origin:\n                handle_error(self, 400, ""Please provide a valid origin (endpoint url) via get parameter."")\n                return\n            \n            host, port = parse_endpoint_origin(origin)\n            base_url = web_app.settings[\'base_url\'].rstrip(""/"") + SHARED_SSH_SETUP_PATH\n            setup_command = \'/bin/bash <(curl -s --insecure ""\' \\\n                            + origin + base_url \\\n                            + ""?token="" + generate_token(base_url) \\\n                            + ""&host="" + host \\\n                            + ""&port="" + port \\\n                            + \'"")\'\n            \n            self.finish(setup_command)\n        except Exception as ex:\n            handle_error(self, 500, exception=ex)\n            return\n\nclass SharedTokenHandler(IPythonHandler):\n    @web.authenticated\n    def get(self):\n        try:\n            sharing_enabled = os.environ.get(""SHARED_LINKS_ENABLED"", ""false"")\n            if sharing_enabled.lower() != ""true"":\n                handle_error(self, 400, error_msg=""Shared links are disabled."")\n                return\n            \n            path = self.get_argument(\'path\', None)\n            if path is None:\n                handle_error(self, 400, ""Please provide a valid path via get parameter."")\n                return\n            \n            self.finish(generate_token(path))\n        except Exception as ex:\n            handle_error(self, 500, exception=ex)\n            return\n\n\nclass SharedFilesHandler(IPythonHandler):\n    @web.authenticated\n    def get(self):\n        try:\n            sharing_enabled = os.environ.get(""SHARED_LINKS_ENABLED"", ""false"")\n            if sharing_enabled.lower() != ""true"":\n                self.finish(""Shared links are disabled. Please download and share the data manually."")\n                return\n            \n            path = _resolve_path(self.get_argument(\'path\', None))\n            if not path:\n                handle_error(self, 400, ""Please provide a valid path via get parameter."")\n                return\n            \n            if not os.path.exists(path):\n                handle_error(self, 400, ""The selected file or folder does not exist: "" + path)\n                return\n            \n            # schema + host + port\n            origin = self.get_argument(\'origin\', None)\n            if not origin:\n                handle_error(self, 400, ""Please provide a valid origin (endpoint url) via get parameter."")\n                return\n            \n            token = generate_token(path)\n\n            try:\n                # filebrowser needs to be stopped so that a user can be added\n                call(""supervisorctl stop filebrowser"", shell=True)\n\n                # Add new user with the given permissions and scope\n                add_user_command = ""filebrowser users add "" + token + "" "" + token \\\n                    + "" --perm.admin=false --perm.create=false --perm.delete=false"" \\\n                    + "" --perm.download=true --perm.execute=false --perm.modify=false"" \\\n                    + "" --perm.rename=false --perm.share=false --lockPassword=true"" \\\n                    + "" --database="" + HOME + ""/filebrowser.db --scope=\\"""" + path + ""\\""""\n\n                call(add_user_command, shell=True)\n            except:\n                pass\n            \n            call(""supervisorctl start filebrowser"", shell=True)\n\n            base_url = web_app.settings[\'base_url\'].rstrip(""/"") + ""/shared/filebrowser/""\n            setup_command = origin + base_url + ""?token="" + token\n            self.finish(setup_command)\n\n        except Exception as ex:\n            handle_error(self, 500, exception=ex)\n            return\n\n\n# ------------- GIT FUNCTIONS ------------------------\n\ndef execute_command(cmd: str):\n    return subprocess.check_output(cmd.split()).decode(\'utf-8\').replace(""\\n"", """")\n\n\ndef get_repo(directory: str):\n    if not directory:\n        return None\n\n    try:\n        return git.Repo(directory, search_parent_directories=True)\n    except:\n        return None\n\n\ndef set_user_email(email: str, repo=None):\n    if repo:\n        repo.config_writer().set_value(""user"", ""email"", email).release()\n    else:\n        exit_code = subprocess.call(\'git config --global user.email ""\' + email + \'""\', shell=True)\n        if exit_code > 0:\n            warnings.warn(""Global email configuration failed."")\n\n\ndef set_user_name(name: str, repo=None):\n    if repo:\n        repo.config_writer().set_value(""user"", ""name"", name).release()\n    else:\n        exit_code = subprocess.call(\'git config --global user.name ""\' + name + \'""\', shell=True)\n        if exit_code > 0:\n            warnings.warn(""Global name configuration failed."")\n\n\ndef commit_file(file_path: str, commit_msg: str = None, push: bool = True):\n    if not os.path.isfile(file_path):\n        raise Exception(""File does not exist: "" + file_path)\n\n    repo = get_repo(os.path.dirname(file_path))\n    if not repo:\n        raise Exception(""No git repo was found for file: "" + file_path)\n\n    # Always add file\n    repo.index.add([file_path])\n\n    if not get_user_name(repo):\n        raise Exception(\'Cannot push to remote. Please specify a name with: git config --global user.name ""YOUR NAME""\')\n\n    if not get_user_email(repo):\n        raise Exception(\n            \'Cannot push to remote. Please specify an email with: git config --global user.emails ""YOUR EMAIL""\')\n\n    if not commit_msg:\n        commit_msg = ""Updated "" + os.path.relpath(file_path, repo.working_dir)\n\n    try:\n        # fetch and merge newest state - fast-forward-only\n        repo.git.pull(\'--ff-only\')\n    except:\n        raise Exception(""The repo is not up-to-date or cannot be updated."")\n\n    try:\n        # Commit single file with commit message\n        repo.git.commit(file_path, m=commit_msg)\n    except git.GitCommandError as error:\n        if error.stdout and (\n                ""branch is up-to-date with"" in error.stdout or ""branch is up to date with"" in error.stdout):\n            # TODO better way to check if file has changed, e.g. has_file_changed\n            raise Exception(""File has not been changed: "" + file_path)\n        else:\n            raise error\n\n    if push:\n        # Push file to remote\n        try:\n            repo.git.push(""origin"", \'HEAD\')\n        except git.GitCommandError as error:\n            if error.stderr and ( ""No such device or address"" in error.stderr and ""could not read Username"" in error.stderr):\n                raise Exception(""User is not authenticated. Please use Ungit to login via HTTPS or use SSH authentication."")\n            else:\n                raise error\n\ndef get_config_value(key: str, repo=None):\n    try:\n        if repo:\n            return repo.git.config(key)\n        # no repo, look up global config\n        return execute_command(\'git config \' + key)\n    except:\n        return None\n\n\ndef get_user_name(repo=None):\n    return get_config_value(""user.name"", repo)\n\n\ndef get_user_email(repo=None):\n    return get_config_value(""user.email"", repo)\n\n\ndef get_active_branch(repo) -> str or None:\n    try:\n        return repo.active_branch.name\n    except:\n        return None\n\n\ndef get_last_commit(repo) -> str or None:\n    try:\n        return datetime.fromtimestamp(repo.head.commit.committed_date).strftime(""%d.%B %Y %I:%M:%S"")\n    except:\n        return None\n\n\ndef has_file_changed(repo, file_path: str):\n    # not working in all situations\n    changed_files = [item.a_path for item in repo.index.diff(None)]\n    return os.path.relpath(os.path.realpath(file_path), repo.working_dir) in (path for path in changed_files)\n\n\ndef get_git_info(directory: str):\n    repo = get_repo(directory)\n    git_info = {\n        ""userName"": get_user_name(repo),\n        ""userEmail"": get_user_email(repo),\n        ""repoRoot"": repo.working_dir if repo else None,\n        ""activeBranch"": get_active_branch(repo) if repo else None,\n        ""lastCommit"": get_last_commit(repo) if repo else None,\n        ""requestPath"": directory\n    }\n    return git_info\n\n\ndef _get_server_root() -> str:\n    return os.path.expanduser(web_app.settings[\'server_root_dir\'])\n\n\ndef _resolve_path(path: str) -> str or None:\n    if path:\n        # add jupyter server root directory\n        if path.startswith(""/""):\n            path = path[1:]\n\n        return os.path.join(_get_server_root(), path)\n    else:\n        return None\n\n\n# ------------- SSH Functions ------------------------\ndef handle_ssh_script_request(handler):\n    origin = handler.get_argument(\'origin\', None)\n    host = handler.get_argument(\'host\', None)\n    port = handler.get_argument(\'port\', None)\n\n    if not host and origin:\n        host, _ = parse_endpoint_origin(origin)\n    \n    if not port and origin:\n        _, port = parse_endpoint_origin(origin)\n    \n    if not host:\n        handle_error(handler, 400, ""Please provide a host via get parameter. Alternatively, you can also specify an origin with the full endpoint url."")\n        return\n\n    if not port:\n        handle_error(handler, 400, ""Please provide a port via get parameter. Alternatively, you can also specify an origin with the full endpoint url."")\n        return \n    \n    setup_script = get_setup_script(host, port)\n\n    download_script_flag = handler.get_argument(\'download\', None)\n    if download_script_flag and download_script_flag.lower().strip() == \'true\':\n        # Use host, otherwise it cannot be reconstructed in tooling plugin\n        \n        file_name = \'setup_ssh_{}-{}\'.format(host.lower().replace(""."", ""-""), port)\n        SSH_JUMPHOST_TARGET = os.environ.get(""SSH_JUMPHOST_TARGET"", """")\n        if SSH_JUMPHOST_TARGET:\n            # add name if variabl is set\n            file_name += ""-"" + SSH_JUMPHOST_TARGET.lower().replace(""."", ""-"")\n        file_name += "".sh""\n\n        handler.set_header(\'Content-Type\', \'application/octet-stream\')\n        handler.set_header(\'Content-Disposition\', \'attachment; filename=\' + file_name) # Hostname runtime\n        handler.write(setup_script)\n        handler.finish()\n    else:\n        handler.finish(setup_script)\n                \n\ndef parse_endpoint_origin(endpoint_url: str):\n    # get host and port from endpoint url\n    from urllib.parse import urlparse\n    endpoint_url = urlparse(endpoint_url)\n    hostname = endpoint_url.hostname\n    port = endpoint_url.port\n    if not port:\n        port = 80\n        if endpoint_url.scheme == ""https"":\n            port = 443\n    return hostname, str(port)\n    \ndef generate_token(base_url: str):\n    private_ssh_key_path = HOME + ""/.ssh/id_ed25519""\n    with open(private_ssh_key_path, ""r"") as f:\n        runtime_private_key = f.read()\n\n    import hashlib\n    key_hasher = hashlib.sha1()\n    key_hasher.update(str.encode(str(runtime_private_key).lower().strip()))\n    key_hash = key_hasher.hexdigest()\n\n    token_hasher = hashlib.sha1()\n    token_str = (key_hash+base_url).lower().strip()\n    token_hasher.update(str.encode(token_str))\n    return str(token_hasher.hexdigest())\n\ndef get_setup_script(hostname: str = None, port: str = None):\n    \n    private_ssh_key_path = HOME + ""/.ssh/id_ed25519""\n    with open(private_ssh_key_path, ""r"") as f:\n        runtime_private_key = f.read()\n\n    ssh_templates_path = os.path.dirname(os.path.abspath(__file__)) + ""/setup_templates""\n\n    with open(ssh_templates_path + \'/client_command.txt\', \'r\') as file:\n        client_command = file.read()\n    \n    SSH_JUMPHOST_TARGET = os.environ.get(""SSH_JUMPHOST_TARGET"", """")\n    is_runtime_manager_existing = False if SSH_JUMPHOST_TARGET == """" else True\n\n    RUNTIME_CONFIG_NAME = ""workspace-""\n    if is_runtime_manager_existing:\n        HOSTNAME_RUNTIME = SSH_JUMPHOST_TARGET\n        HOSTNAME_MANAGER = hostname\n        PORT_MANAGER = port\n        PORT_RUNTIME = os.getenv(""WORKSPACE_PORT"", ""8080"")\n\n        RUNTIME_CONFIG_NAME = RUNTIME_CONFIG_NAME + ""{}-{}-{}"".format(HOSTNAME_RUNTIME, HOSTNAME_MANAGER, PORT_MANAGER)\n                    \n        client_command = client_command \\\n            .replace(""{HOSTNAME_MANAGER}"", HOSTNAME_MANAGER) \\\n            .replace(""{PORT_MANAGER}"", str(PORT_MANAGER)) \\\n            .replace(""#ProxyCommand"", ""ProxyCommand"")\n                \n        local_keyscan_replacement = ""{}"".format(HOSTNAME_RUNTIME)\n    else:\n        HOSTNAME_RUNTIME = hostname\n        PORT_RUNTIME = port\n        RUNTIME_CONFIG_NAME = RUNTIME_CONFIG_NAME + ""{}-{}"".format(HOSTNAME_RUNTIME, PORT_RUNTIME)\n\n        local_keyscan_replacement = ""[{}]:{}"".format(HOSTNAME_RUNTIME, PORT_RUNTIME)            \n\n    # perform keyscan with localhost to get the runtime\'s keyscan result.\n    # Replace then the ""localhost"" part in the returning string with the actual RUNTIME_HOST_NAME\n    local_keyscan_entry = get_ssh_keyscan_results(""localhost"")\n    if local_keyscan_entry is not None:\n        local_keyscan_entry = local_keyscan_entry.replace(""localhost"", local_keyscan_replacement)\n\n    output = client_command \\\n        .replace(""{PRIVATE_KEY_RUNTIME}"", runtime_private_key) \\\n        .replace(""{HOSTNAME_RUNTIME}"", HOSTNAME_RUNTIME) \\\n        .replace(""{RUNTIME_KNOWN_HOST_ENTRY}"", local_keyscan_entry) \\\n        .replace(""{PORT_RUNTIME}"", str(PORT_RUNTIME)) \\\n        .replace(""{RUNTIME_CONFIG_NAME}"", RUNTIME_CONFIG_NAME) \\\n        .replace(""{RUNTIME_KEYSCAN_NAME}"", local_keyscan_replacement.replace(""["", ""\\["").replace(""]"", ""\\]""))\n\n    return output\n\ndef get_ssh_keyscan_results(host_name, host_port=22, key_format=""ecdsa""):\n    """"""\n    Perform the keyscan command to get the certicicate fingerprint (of specified format [e.g. rsa256, ecdsa, ...]) of the container.\n\n    # Arguments\n      - host_name (string): hostname which to scan for a key\n      - host_port (int): port which to scan for a key\n      - key_format (string): type of the key to return. the `ssh-keyscan` command usually lists the fingerprint in different formats (e.g. ecdsa-sha2-nistp256, ssh-rsa, ssh-ed25519, ...). The ssh-keyscan result is grepped for the key_format, so already a part could match. In that case, the last match is used.\n\n    # Returns\n      The keyscan entry which can be added to the known_hosts file. If `key_format` matches multiple results of `ssh-keyscan`, the last match is returned. If no match exists, it returns empty\n    """"""\n\n    keyscan_result = subprocess.run(\n        [\'ssh-keyscan\', \'-p\', str(host_port), host_name], stdout=subprocess.PIPE)\n    keys = keyscan_result.stdout.decode(""utf-8"").split(""\\n"")\n    keyscan_entry = """"\n    for key in keys:\n        if key_format in key:\n            keyscan_entry = key\n    return keyscan_entry\n\n\n# ------------- PLUGIN LOADER ------------------------\n\n\ndef load_jupyter_server_extension(nb_server_app) -> None:\n    # registers all handlers as a REST interface\n    global web_app\n    global log\n\n    web_app = nb_server_app.web_app\n    log = nb_server_app.log\n\n    host_pattern = \'.*$\'\n\n    # SharedSSHHandler\n\n    route_pattern = url_path_join(web_app.settings[\'base_url\'], \'/tooling/ping\')\n    web_app.add_handlers(host_pattern, [(route_pattern, PingHandler)])\n\n    route_pattern = url_path_join(web_app.settings[\'base_url\'], \'/tooling/tools\')\n    web_app.add_handlers(host_pattern, [(route_pattern, ToolingHandler)])\n\n    route_pattern = url_path_join(web_app.settings[\'base_url\'], \'/tooling/tool-installers\')\n    web_app.add_handlers(host_pattern, [(route_pattern, InstallToolHandler)])\n\n    route_pattern = url_path_join(web_app.settings[\'base_url\'], \'/tooling/token\')\n    web_app.add_handlers(host_pattern, [(route_pattern, SharedTokenHandler)])\n\n    route_pattern = url_path_join(web_app.settings[\'base_url\'], \'/tooling/git/info\')\n    web_app.add_handlers(host_pattern, [(route_pattern, GitInfoHandler)])\n\n    route_pattern = url_path_join(web_app.settings[\'base_url\'], \'/tooling/git/commit\')\n    web_app.add_handlers(host_pattern, [(route_pattern, GitCommitHandler)])\n\n    route_pattern = url_path_join(web_app.settings[\'base_url\'], \'/tooling/ssh/setup-script\')\n    web_app.add_handlers(host_pattern, [(route_pattern, SSHScriptHandler)])\n\n    route_pattern = url_path_join(web_app.settings[\'base_url\'], \'/tooling/ssh/setup-command\')\n    web_app.add_handlers(host_pattern, [(route_pattern, SSHCommandHandler)])\n\n    route_pattern = url_path_join(web_app.settings[\'base_url\'], ""/tooling/files/link"")\n    web_app.add_handlers(host_pattern, [(route_pattern, SharedFilesHandler)])\n\n    route_pattern = url_path_join(web_app.settings[\'base_url\'], SHARED_SSH_SETUP_PATH)\n    web_app.add_handlers(host_pattern, [(route_pattern, SharedSSHHandler)])\n\n    log.info(\'Extension jupyter-tooling-widget loaded successfully.\')\n\n\n# Test routine. Can be invoked manually\nif __name__ == ""__main__"":\n    application = tornado.web.Application([\n        (r\'/test\', HelloWorldHandler)\n    ])\n\n    application.listen(555)\n    tornado.ioloop.IOLoop.current().start()\n'"
