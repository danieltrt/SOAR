file_path,api_count,code
cartoonize.py,0,"b'import os\nimport PIL\nimport sys\nimport glob\nimport imageio\nimport logging\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom style_transfer.cartoongan import cartoongan\n\n\nSTYLES = [""shinkai"", ""hayao"", ""hosoda"", ""paprika""]\nVALID_EXTENSIONS = [\'jpg\', \'png\', \'gif\', \'JPG\']\n\n\nparser = argparse.ArgumentParser(description=""transform real world images to specified cartoon style(s)"")\nparser.add_argument(""--styles"", nargs=""+"", default=[STYLES[0]],\n                    help=""specify (multiple) cartoon styles which will be used to transform input images."")\nparser.add_argument(""--all_styles"", action=""store_true"",\n                    help=""set true if all styled results are desired"")\nparser.add_argument(""--input_dir"", type=str, default=""input_images"",\n                    help=""directory with images to be transformed"")\nparser.add_argument(""--output_dir"", type=str, default=""output_images"",\n                    help=""directory where transformed images are saved"")\nparser.add_argument(""--batch_size"", type=int, default=1,\n                    help=""number of images that will be transformed in parallel to speed up processing. ""\n                         ""higher value like 4 is recommended if there are gpus."")\nparser.add_argument(""--ignore_gif"", action=""store_true"",\n                    help=""transforming gif images can take long time. enable this when you want to ignore gifs"")\nparser.add_argument(""--overwrite"", action=""store_true"",\n                    help=""enable this if you want to regenerate outputs regardless of existing results"")\nparser.add_argument(""--skip_comparison"", action=""store_true"",\n                    help=""enable this if you only want individual style result and to save processing time"")\nparser.add_argument(""--comparison_view"", type=str, default=""smart"",\n                    choices=[""smart"", ""horizontal"", ""vertical"", ""grid""],\n                    help=""specify how input images and transformed images are concatenated for easier comparison"")\nparser.add_argument(""--gif_frame_frequency"", type=int, default=1,\n                    help=""how often should a frame in gif be transformed. freq=1 means that every frame ""\n                         ""in the gif will be transformed by default. set higher frequency can save processing ""\n                         ""time while make the transformed gif less smooth"")\nparser.add_argument(""--max_num_frames"", type=int, default=100,\n                    help=""max number of frames that will be extracted from a gif. set higher value if longer gif ""\n                         ""is needed"")\nparser.add_argument(""--keep_original_size"", action=""store_true"",\n                    help=""by default the input images will be resized to reasonable size to prevent potential large ""\n                         ""computation and to save file sizes. Enable this if you want the original image size."")\nparser.add_argument(""--max_resized_height"", type=int, default=300,\n                    help=""specify the max height of a image after resizing. the resized image will have the same""\n                         ""aspect ratio. Set higher value or enable `keep_original_size` if you want larger image."")\nparser.add_argument(""--convert_gif_to_mp4"", action=""store_true"",\n                    help=""convert transformed gif to mp4 which is much more smaller and easier to share. ""\n                         ""`ffmpeg` need to be installed at first."")\nparser.add_argument(""--logging_lvl"", type=str, default=""info"",\n                    choices=[""debug"", ""info"", ""warning"", ""error"", ""critical""],\n                    help=""logging level which decide how verbosely the program will be. set to `debug` if necessary"")\nparser.add_argument(""--debug"", action=""store_true"",\n                    help=""show the most detailed logging messages for debugging purpose"")\nparser.add_argument(""--show_tf_cpp_log"", action=""store_true"")\n\nargs = parser.parse_args()\n\nTEMPORARY_DIR = os.path.join(f""{args.output_dir}"", "".tmp"")\n\n\nlogger = logging.getLogger(""Cartoonizer"")\nlogger.propagate = False\nlog_lvl = {""debug"": logging.DEBUG, ""info"": logging.INFO,\n           ""warning"": logging.WARNING, ""error"": logging.ERROR,\n           ""critical"": logging.CRITICAL}\nif args.debug:\n    logger.setLevel(logging.DEBUG)\nelse:\n    logger.setLevel(log_lvl[args.logging_lvl])\nformatter = logging.Formatter(\n    ""[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s"", ""%Y-%m-%d %H:%M:%S"")\nstdhandler = logging.StreamHandler(sys.stdout)\nstdhandler.setFormatter(formatter)\nlogger.addHandler(stdhandler)\n\nif not args.show_tf_cpp_log:\n    os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""\n\n\ndef pre_processing(image_path, style, expand_dim=True):\n    input_image = PIL.Image.open(image_path).convert(""RGB"")\n\n    if not args.keep_original_size:\n        width, height = input_image.size\n        aspect_ratio = width / height\n        resized_height = min(height, args.max_resized_height)\n        resized_width = int(resized_height * aspect_ratio)\n        if width != resized_width:\n            logger.debug(f""resized ({width}, {height}) to: ({resized_width}, {resized_height})"")\n            input_image = input_image.resize((resized_width, resized_height))\n\n    input_image = np.asarray(input_image)\n    input_image = input_image.astype(np.float32)\n\n    input_image = input_image[:, :, [2, 1, 0]]\n\n    if expand_dim:\n        input_image = np.expand_dims(input_image, axis=0)\n    return input_image\n\n\ndef post_processing(transformed_image, style):\n    if not type(transformed_image) == np.ndarray:\n        transformed_image = transformed_image.numpy()\n    transformed_image = transformed_image[0]\n    transformed_image = transformed_image[:, :, [2, 1, 0]]\n    transformed_image = transformed_image * 0.5 + 0.5\n    transformed_image = transformed_image * 255\n    return transformed_image\n\n\ndef save_transformed_image(output_image, img_filename, save_dir):\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    transformed_image_path = os.path.join(save_dir, img_filename)\n\n    if output_image is not None:\n        image = PIL.Image.fromarray(output_image.astype(""uint8""))\n        image.save(transformed_image_path)\n\n    return transformed_image_path\n\n\ndef save_concatenated_image(image_paths, image_folder=""comparison"", num_columns=2):\n    images = [PIL.Image.open(i).convert(\'RGB\') for i in image_paths]\n    # pick the image which is the smallest, and resize the others to match it (can be arbitrary image shape here)\n    min_shape = sorted([(np.sum(i.size), i.size) for i in images])[0][1]\n    array = np.asarray([np.asarray(i.resize(min_shape)) for i in images])\n\n    view = args.comparison_view\n    if view == ""smart"":\n        width, height = min_shape[0], min_shape[1]\n        aspect_ratio = width / height\n        logger.debug(f""(width, height): ({width}, {height}), aspect_ratio: {aspect_ratio}"")\n        grid_suitable = (len(args.styles) + 1) % num_columns == 0\n        is_portrait = aspect_ratio <= 0.75\n        if grid_suitable and not is_portrait:\n            view = ""grid""\n        elif is_portrait:\n            view = ""horizontal""\n        else:\n            view = ""vertical""\n\n    if view == ""horizontal"":\n        images_comb = np.hstack(array)\n    elif view == ""vertical"":\n        images_comb = np.vstack(array)\n    elif view == ""grid"":\n        rows = np.split(array, num_columns)\n        rows = [np.hstack(row) for row in rows]\n        images_comb = np.vstack([row for row in rows])\n    else:\n        logger.debug(f""Wrong `comparison_view`: {args.comparison_view}"")\n\n    images_comb = PIL.Image.fromarray(images_comb)\n    file_name = image_paths[0].split(""/"")[-1]\n\n    if args.output_dir not in image_folder:\n        image_folder = os.path.join(args.output_dir, image_folder)\n    if not os.path.exists(image_folder):\n        os.makedirs(image_folder)\n\n    image_path = os.path.join(image_folder, file_name)\n    images_comb.save(image_path)\n    return image_path\n\n\ndef convert_gif_to_png(gif_path):\n    logger.debug(f""`{gif_path}` is a gif, extracting png images from it..."")\n    gif_filename = gif_path.split(""/"")[-1].replace("".gif"", """")\n    image = PIL.Image.open(gif_path)\n    palette = image.getpalette()\n    png_paths = list()\n    i = 0\n\n    png_dir = os.path.join(TEMPORARY_DIR, gif_filename)\n    if not os.path.exists(png_dir):\n        logger.debug(f""Creating temporary folder: {png_dir} for storing intermediate result..."")\n        os.makedirs(png_dir)\n\n    prev_generated_png_paths = glob.glob(png_dir + \'/*.png\')\n    if prev_generated_png_paths:\n        return prev_generated_png_paths\n\n    num_processed_frames = 0\n    logger.debug(""Generating png images..."")\n    try:\n        while num_processed_frames < args.max_num_frames:\n\n            image.putpalette(palette)\n            extracted_image = PIL.Image.new(""RGB"", image.size)\n            extracted_image.paste(image)\n\n            if not args.keep_original_size:\n                width, height = extracted_image.size\n                aspect_ratio = width / height\n                resized_height = min(height, args.max_resized_height)\n                resized_width = int(resized_height * aspect_ratio)\n                if width != resized_width:\n                    logger.debug(f""resized ({width}, {height}) to: ({resized_width}, {resized_height})"")\n                    extracted_image = extracted_image.resize((resized_width, resized_height))\n\n            if i % args.gif_frame_frequency == 0:\n                png_filename = f""{i + 1}.png""\n                png_path = os.path.join(png_dir, png_filename)\n                extracted_image.save(png_path)\n                png_paths.append(png_path)\n                num_processed_frames += 1\n\n            image.seek(image.tell() + 1)\n            i += 1\n\n    except EOFError:\n        pass  # end of sequence\n\n    logger.debug(f""Number of {len(png_paths)} png images were generated at {png_dir}."")\n    return png_paths\n\n\ndef transform_png_images(image_paths, model, style, return_existing_result=False):\n    transformed_image_paths = list()\n    save_dir = os.path.join(""/"".join(image_paths[0].split(""/"")[:-1]), style)\n    logger.debug(f""Transforming {len(image_paths)} images and saving them to {save_dir}...."")\n\n    if return_existing_result:\n        return glob.glob(os.path.join(save_dir, ""*.png""))\n\n    num_batch = int(np.ceil(len(image_paths) / args.batch_size))\n    image_paths = np.array_split(image_paths, num_batch)\n\n    logger.debug(f""Processing {num_batch} batches with batch_size={args.batch_size}..."")\n    for batch_image_paths in image_paths:\n        image_filenames = [path.split(""/"")[-1] for path in batch_image_paths]\n        input_images = [pre_processing(path, style=style, expand_dim=False) for path in batch_image_paths]\n        input_images = np.stack(input_images, axis=0)\n        transformed_images = model(input_images)\n        output_images = [post_processing(image, style=style)\n                         for image in np.split(transformed_images, transformed_images.shape[0])]\n        paths = [save_transformed_image(img, f, save_dir)\n                 for img, f in zip(output_images, image_filenames)]\n        transformed_image_paths.extend(paths)\n\n    return transformed_image_paths\n\n\ndef save_png_images_as_gif(image_paths, image_filename, style=""comparison""):\n\n    gif_dir = os.path.join(args.output_dir, style)\n    if not os.path.exists(gif_dir):\n        os.makedirs(gif_dir)\n    gif_path = os.path.join(gif_dir, image_filename)\n\n    with imageio.get_writer(gif_path, mode=\'I\') as writer:\n        file_names = sorted(image_paths, key=lambda x: int(x.split(\'/\')[-1].replace(\'.png\', \'\')))\n        logger.debug(f""Combining {len(file_names)} png images into {gif_path}..."")\n        for i, filename in enumerate(file_names):\n            image = imageio.imread(filename)\n            writer.append_data(image)\n    return gif_path\n\n\ndef convert_gif_to_mp4(gif_path, crf=25):\n\n    mp4_dir = os.path.join(os.path.dirname(gif_path), ""mp4"")\n    gif_file = gif_path.split(""/"")[-1]\n    if not os.path.exists(mp4_dir):\n        os.makedirs(mp4_dir)\n    mp4_path = os.path.join(mp4_dir, gif_file.replace("".gif"", "".mp4""))\n    cmd = ""ffmpeg -y -i {} -movflags faststart -pix_fmt yuv420p -vf \\""scale=trunc(iw/2)*2:trunc(ih/2)*2\\"" -crf {} {}""\n    cmd = cmd.replace(""My Drive"", ""My\\ Drive"")\n    os.system(cmd.format(gif_path, crf, mp4_path))\n\n\ndef result_exist(image_path, style):\n    return os.path.exists(os.path.join(args.output_dir, style, image_path.split(""/"")[-1]))\n\n\ndef main():\n\n    start = datetime.now()\n    logger.info(f""Transformed images will be saved to `{args.output_dir}` folder."")\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n\n    # create temporary folder which will be deleted after transformations\n    if not os.path.exists(TEMPORARY_DIR):\n        os.makedirs(TEMPORARY_DIR)\n\n    # decide what styles to used in this execution\n    styles = STYLES if args.all_styles else args.styles\n\n    models = list()\n    for style in styles:\n        models.append(cartoongan.load_model(style))\n\n    logger.info(f""Cartoonizing images using {\', \'.join(styles)} style..."")\n\n    image_paths = []\n    for ext in VALID_EXTENSIONS:\n        image_paths.extend(glob.glob(os.path.join(args.input_dir, f""*.{ext}"")))\n    logger.info(f""Preparing to transform {len(image_paths)} images from `{args.input_dir}` directory..."")\n\n    progress_bar = tqdm(image_paths, desc=\'Transforming\')\n    for image_path in progress_bar:\n        image_filename = image_path.split(""/"")[-1]\n        progress_bar.set_postfix(File=image_filename)\n\n        if image_filename.endswith("".gif"") and not args.ignore_gif:\n            png_paths = convert_gif_to_png(image_path)\n\n            png_paths_list = [png_paths]\n            num_images = len(png_paths)\n            for model, style in zip(models, styles):\n                return_existing_result = result_exist(image_path, style) or args.overwrite\n\n                transformed_png_paths = transform_png_images(png_paths, model, style,\n                                                             return_existing_result=return_existing_result)\n                png_paths_list.append(transformed_png_paths)\n\n                if not return_existing_result:\n                    gif_path = save_png_images_as_gif(transformed_png_paths, image_filename, style)\n                    if args.convert_gif_to_mp4:\n                        convert_gif_to_mp4(gif_path)\n\n            rearrange_paths_list = [[l[i] for l in png_paths_list] for i in range(num_images)]\n\n            save_dir = os.path.join(TEMPORARY_DIR, image_filename.replace("".gif"", """"), ""comparison"")\n\n            combined_image_paths = list()\n            for image_paths in rearrange_paths_list:\n                path = save_concatenated_image(image_paths, image_folder=save_dir)\n                combined_image_paths.append(path)\n\n            if not args.skip_comparison:\n                gif_path = save_png_images_as_gif(combined_image_paths, image_filename)\n                if args.convert_gif_to_mp4:\n                    convert_gif_to_mp4(gif_path)\n\n        else:\n            related_image_paths = [image_path]\n            for model, style in zip(models, styles):\n                input_image = pre_processing(image_path, style=style)\n                save_dir = os.path.join(args.output_dir, style)\n                return_existing_result = result_exist(image_path, style) and not args.overwrite\n\n                if not return_existing_result:\n                    transformed_image = model(input_image)\n                    output_image = post_processing(transformed_image, style=style)\n                    transformed_image_path = save_transformed_image(output_image, image_filename, save_dir)\n                else:\n                    transformed_image_path = save_transformed_image(None, image_filename, save_dir)\n\n                related_image_paths.append(transformed_image_path)\n\n            if not args.skip_comparison:\n                save_concatenated_image(related_image_paths)\n    progress_bar.close()\n\n    time_elapsed = datetime.now() - start\n    logger.info(f""Total processing time: {time_elapsed}"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
discriminator.py,3,"b'import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, LeakyReLU\nfrom keras_contrib.layers import InstanceNormalization\nfrom layers import ZeroPadding2D, ReflectionPadding2D, StridedConv\n\n\nclass Discriminator(Model):\n    def __init__(self,\n                 base_filters=32,\n                 lrelu_alpha=0.2,\n                 pad_type=""reflect"",\n                 norm_type=""batch""):\n        super(Discriminator, self).__init__(name=""Discriminator"")\n        if pad_type == ""reflect"":\n            self.flat_pad = ReflectionPadding2D()\n        elif pad_type == ""constant"":\n            self.flat_pad = ZeroPadding2D()\n        else:\n            raise ValueError(f""pad_type not recognized {pad_type}"")\n\n        self.flat_conv = Conv2D(base_filters, 3)\n        self.flat_lru = LeakyReLU(lrelu_alpha)\n        self.strided_conv1 = StridedConv(base_filters * 2,\n                                         lrelu_alpha,\n                                         pad_type,\n                                         norm_type)\n        self.strided_conv2 = StridedConv(base_filters * 4,\n                                         lrelu_alpha,\n                                         pad_type,\n                                         norm_type)\n        self.conv2 = Conv2D(base_filters * 8, 3)\n\n        if norm_type == ""instance"":\n            self.norm = InstanceNormalization()\n        elif norm_type == ""batch"":\n            self.norm = BatchNormalization()\n\n        self.lrelu = LeakyReLU(lrelu_alpha)\n\n        self.final_conv = Conv2D(1, 3)\n\n    def build(self, input_shape):\n        super(Discriminator, self).build(input_shape)\n\n    def call(self, x, training=False):\n        x = self.flat_pad(x)\n        x = self.flat_conv(x)\n        x = self.flat_lru(x)\n        x = self.strided_conv1(x, training=training)\n        x = self.strided_conv2(x, training=training)\n        x = self.conv2(x)\n        x = self.norm(x, training=training)\n        x = self.lrelu(x)\n        x = self.final_conv(x)\n        return x\n\n\nif __name__ == ""__main__"":\n    import numpy as np\n\n    shape = (1, 256, 256, 3)\n    nx = np.random.rand(*shape).astype(np.float32)\n    t = tf.keras.Input(shape=nx.shape[1:], batch_size=nx.shape[0])\n    tf.keras.backend.clear_session()\n    sc = StridedConv(t.shape[-1])\n    out = sc(t)\n    sc.summary()\n    print(f""Input  Shape: {t.shape}"")\n    print(f""Output Shape: {out.shape}"")\n    print(""\\n"" * 2)\n\n    tf.keras.backend.clear_session()\n    d = Discriminator()\n    out = d(t)\n    d.summary()\n    print(f""Input  Shape: {t.shape}"")\n    print(f""Output Shape: {out.shape}"")\n'"
export.py,4,"b'import os\nfrom subprocess import Popen\nimport tensorflow as tf\nfrom generator import Generator\nfrom logger import get_logger\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\ntf.get_logger().setLevel(40)\n\n\ndef main(m_path, out_dir, light):\n    logger = get_logger(""export"")\n    try:\n        g = Generator(light=light)\n        g.load_weights(tf.train.latest_checkpoint(m_path))\n        t = tf.keras.Input(shape=[None, None, 3], batch_size=None)\n        g(t, training=False)\n        g.summary()\n    except ValueError as e:\n        logger.error(e)\n        logger.error(""Failed to load specified weight."")\n        logger.error(""If you trained your model with --light, ""\n                     ""consider adding --light when executing this script; otherwise, ""\n                     ""do not add --light when executing this script."")\n        exit(1)\n    m_num = 0\n    smd = os.path.join(out_dir, ""SavedModel"")\n    tfmd = os.path.join(out_dir, ""tfjs_model"")\n    if light:\n        smd += ""Light""\n        tfmd += ""_light""\n    saved_model_dir = f""{smd}_{m_num:04d}""\n    tfjs_model_dir = f""{tfmd}_{m_num:04d}""\n    while os.path.exists(saved_model_dir):\n        m_num += 1\n        saved_model_dir = f""{smd}_{m_num:04d}""\n        tfjs_model_dir = f""{tfmd}_{m_num:04d}""\n    tf.saved_model.save(g, saved_model_dir)\n    cmd = [\'tensorflowjs_converter\', \'--input_format\', \'tf_saved_model\',\n           \'--output_format\', \'tfjs_graph_model\', saved_model_dir, tfjs_model_dir]\n    logger.info("" "".join(cmd))\n    exit_code = Popen(cmd).wait()\n    if exit_code == 0:\n        logger.info(f""Model converted to {saved_model_dir} and {tfjs_model_dir} successfully"")\n    else:\n        logger.error(""tfjs model conversion failed"")\n\n\nif __name__ == ""__main__"":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--m_path"", type=str, default=\'models\')\n    parser.add_argument(""--out_dir"", type=str, default=\'exported_models\')\n    parser.add_argument(""--light"", action=\'store_true\')\n    args = parser.parse_args()\n    main(args.m_path, args.out_dir, args.light)\n'"
generator.py,6,"b'import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv2D, Activation\nfrom layers import FlatConv, ConvBlock, ResBlock, UpSampleConv\nfrom layers import get_padding, DownShuffleUnitV2, BasicShuffleUnitV2\n\n\nclass Generator(Model):\n    def __init__(self,\n                 norm_type=""instance"",\n                 pad_type=""constant"",\n                 base_filters=64,\n                 num_resblocks=8,\n                 light=False):\n        super(Generator, self).__init__(name=""Generator"")\n        if light:\n            downconv = DownShuffleUnitV2\n            resblock = BasicShuffleUnitV2\n            base_filters += 32\n            end_ksize = 5\n        else:\n            downconv = ConvBlock\n            resblock = ResBlock\n            end_ksize = 7\n        upconv = UpSampleConv\n        self.flat_conv1 = FlatConv(filters=base_filters,\n                                   kernel_size=end_ksize,\n                                   norm_type=norm_type,\n                                   pad_type=pad_type)\n        self.down_conv1 = downconv(mid_filters=base_filters,\n                                   filters=base_filters * 2,\n                                   kernel_size=3,\n                                   stride=2,\n                                   norm_type=norm_type,\n                                   pad_type=pad_type)\n        self.down_conv2 = downconv(mid_filters=base_filters,\n                                   filters=base_filters * 4,\n                                   kernel_size=3,\n                                   stride=2,\n                                   norm_type=norm_type,\n                                   pad_type=pad_type)\n        self.residual_blocks = tf.keras.models.Sequential([\n            resblock(\n                filters=base_filters * 4,\n                kernel_size=3) for _ in range(num_resblocks)])\n        self.up_conv1 = upconv(filters=base_filters * 2,\n                               kernel_size=3,\n                               norm_type=norm_type,\n                               pad_type=pad_type,\n                               light=light)\n        self.up_conv2 = upconv(filters=base_filters,\n                               kernel_size=3,\n                               norm_type=norm_type,\n                               pad_type=pad_type,\n                               light=light)\n\n        end_padding = (end_ksize - 1) // 2\n        end_padding = (end_padding, end_padding)\n        self.final_conv = tf.keras.models.Sequential([\n            get_padding(pad_type, end_padding),\n            Conv2D(3, end_ksize)])\n        self.final_act = Activation(""tanh"")\n\n    def build(self, input_shape):\n        super(Generator, self).build(input_shape)\n\n    def call(self, x, training=False):\n        x = self.flat_conv1(x, training=training)\n        x = self.down_conv1(x, training=training)\n        x = self.down_conv2(x, training=training)\n        x = self.residual_blocks(x, training=training)\n        x = self.up_conv1(x, training=training)\n        x = self.up_conv2(x, training=training)\n        x = self.final_conv(x)\n        x = self.final_act(x)\n        return x\n\n    def compute_output_shape(self, input_shape):\n        return tf.TensorShape(input_shape)\n\n\nif __name__ == ""__main__"":\n    import numpy as np\n    f = 3\n    k = 3\n    s = (1, 64, 64, 3)\n    nx = np.random.rand(*s).astype(np.float32)\n\n    custom_layers = [\n        FlatConv(f, k),\n        ConvBlock(f, k),\n        ResBlock(f, k),\n        UpSampleConv(f, k)\n    ]\n\n    for layer in custom_layers:\n        tf.keras.backend.clear_session()\n        out = layer(nx)\n        layer.summary()\n        print(f""Input  Shape: {nx.shape}"")\n        print(f""Output Shape: {out.shape}"")\n        print(""\\n"" * 2)\n\n    tf.keras.backend.clear_session()\n    g = Generator()\n    shape = (1, 256, 256, 3)\n    nx = np.random.rand(*shape).astype(np.float32)\n    t = tf.keras.Input(shape=nx.shape[1:], batch_size=nx.shape[0])\n    out = g(t, training=False)\n    g.summary()\n    print(f""Input  Shape: {nx.shape}"")\n    print(f""Output Shape: {out.shape}"")\n    assert out.shape == shape, ""Output shape doesn\'t match input shape""\n    print(""Generator\'s output shape is exactly the same as shape of input."")\n'"
inference_with_ckpt.py,2,"b'""""""\nMinimum inference code\n""""""\nimport os\nimport numpy as np\nfrom imageio import imwrite\nfrom PIL import Image\nimport tensorflow as tf\nfrom generator import Generator\nfrom logger import get_logger\n\n\n# NOTE: TF warnings are too noisy without this\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\ntf.get_logger().setLevel(40)\n\n\ndef main(m_path, img_path, out_dir, light=False):\n    logger = get_logger(""inference"")\n    logger.info(f""generating image from {img_path}"")\n    try:\n        g = Generator(light=light)\n        g.load_weights(tf.train.latest_checkpoint(m_path))\n    except ValueError as e:\n        logger.error(e)\n        logger.error(""Failed to load specified weight."")\n        logger.error(""If you trained your model with --light, ""\n                     ""consider adding --light when executing this script; otherwise, ""\n                     ""do not add --light when executing this script."")\n        exit(1)\n    img = np.array(Image.open(img_path).convert(""RGB""))\n    img = np.expand_dims(img, 0).astype(np.float32) / 127.5 - 1\n    out = ((g(img).numpy().squeeze() + 1) * 127.5).astype(np.uint8)\n    if out_dir != """" and not os.path.isdir(out_dir):\n        os.makedirs(out_dir)\n    if out_dir == """":\n        out_dir = "".""\n    out_path = os.path.join(out_dir, os.path.split(img_path)[1])\n    imwrite(out_path, out)\n    logger.info(f""generated image saved to {out_path}"")\n\n\nif __name__ == ""__main__"":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--m_path"", type=str, default=""models"")\n    parser.add_argument(""--img_path"", type=str,\n                        default=os.path.join(""input_images"", ""temple.jpg""))\n    parser.add_argument(""--out_dir"", type=str, default=\'out\')\n    parser.add_argument(""--light"", action=\'store_true\')\n    args = parser.parse_args()\n    main(args.m_path, args.img_path, args.out_dir, args.light)\n'"
inference_with_saved_model.py,3,"b'""""""\nMinimum inference code\n""""""\nimport os\nimport numpy as np\nfrom imageio import imwrite\nfrom PIL import Image\nimport tensorflow as tf\nfrom logger import get_logger\n\n\n# NOTE: TF warnings are too noisy without this\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\ntf.get_logger().setLevel(40)\n\n\ndef main(m_path, img_path, out_dir):\n    logger = get_logger(""inference"")\n    logger.info(f""generating image from {img_path}"")\n    imported = tf.saved_model.load(m_path)\n    f = imported.signatures[""serving_default""]\n    img = np.array(Image.open(img_path).convert(""RGB""))\n    img = np.expand_dims(img, 0).astype(np.float32) / 127.5 - 1\n    out = f(tf.constant(img))[\'output_1\']\n    out = ((out.numpy().squeeze() + 1) * 127.5).astype(np.uint8)\n    if out_dir != """" and not os.path.isdir(out_dir):\n        os.makedirs(out_dir)\n    if out_dir == """":\n        out_dir = "".""\n    out_path = os.path.join(out_dir, os.path.split(img_path)[1])\n    imwrite(out_path, out)\n    logger.info(f""generated image saved to {out_path}"")\n\n\nif __name__ == ""__main__"":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--m_path"", type=str,\n                        default=os.path.join(""exported_models"", ""SavedModelLight_0000""))\n    parser.add_argument(""--img_path"", type=str,\n                        default=os.path.join(""input_images"", ""temple.jpg""))\n    parser.add_argument(""--out_dir"", type=str, default=\'out\')\n    args = parser.parse_args()\n    main(args.m_path, args.img_path, args.out_dir)\n'"
layers.py,15,"b'import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Layer, InputSpec, DepthwiseConv2D\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Add\nfrom tensorflow.keras.layers import ReLU, LeakyReLU, ZeroPadding2D\nfrom keras_contrib.layers import InstanceNormalization\n\n\ndef channel_shuffle_2(x):\n    dyn_shape = tf.shape(x)\n    h, w = dyn_shape[1], dyn_shape[2]\n    c = x.shape[3]\n    x = K.reshape(x, [-1, h, w, 2, c // 2])\n    x = K.permute_dimensions(x, [0, 1, 2, 4, 3])\n    x = K.reshape(x, [-1, h, w, c])\n    return x\n\n\nclass ReflectionPadding2D(Layer):\n    def __init__(self, padding=(1, 1), **kwargs):\n        super(ReflectionPadding2D, self).__init__(**kwargs)\n        padding = tuple(padding)\n        self.padding = ((0, 0), padding, padding, (0, 0))\n        self.input_spec = [InputSpec(ndim=4)]\n\n    def compute_output_shape(self, s):\n        """""" If you are using ""channels_last"" configuration""""""\n        return s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3]\n\n    def call(self, x):\n        return tf.pad(x, self.padding, ""REFLECT"")\n\n\ndef get_padding(pad_type, padding):\n    if pad_type == ""reflect"":\n        return ReflectionPadding2D(padding)\n    elif pad_type == ""constant"":\n        return ZeroPadding2D(padding)\n    else:\n        raise ValueError(f""Unrecognized pad_type {pad_type}"")\n\n\ndef get_norm(norm_type):\n    if norm_type == ""instance"":\n        return InstanceNormalization()\n    elif norm_type == \'batch\':\n        return BatchNormalization()\n    else:\n        raise ValueError(f""Unrecognized norm_type {norm_type}"")\n\n\nclass FlatConv(Model):\n    def __init__(self,\n                 filters,\n                 kernel_size,\n                 norm_type=""instance"",\n                 pad_type=""constant"",\n                 **kwargs):\n        super(FlatConv, self).__init__(name=""FlatConv"")\n        padding = (kernel_size - 1) // 2\n        padding = (padding, padding)\n        self.model = tf.keras.models.Sequential()\n        self.model.add(get_padding(pad_type, padding))\n        self.model.add(Conv2D(filters, kernel_size))\n        self.model.add(get_norm(norm_type))\n        self.model.add(ReLU())\n\n    def build(self, input_shape):\n        super(FlatConv, self).build(input_shape)\n\n    def call(self, x, training=False):\n        return self.model(x, training=training)\n\n\nclass BasicShuffleUnitV2(Model):\n    def __init__(self,\n                 filters,  # NOTE: will be filters // 2\n                 norm_type=""instance"",\n                 pad_type=""constant"",\n                 **kwargs):\n        super(BasicShuffleUnitV2, self).__init__(name=""BasicShuffleUnitV2"")\n        filters //= 2\n        self.model = tf.keras.models.Sequential([\n            Conv2D(filters, 1, use_bias=False),\n            get_norm(norm_type),\n            ReLU(),\n            DepthwiseConv2D(3, padding=\'same\', use_bias=False),\n            get_norm(norm_type),\n            Conv2D(filters, 1, use_bias=False),\n            get_norm(norm_type),\n            ReLU(),\n        ])\n\n    def build(self, input_shape):\n        super(BasicShuffleUnitV2, self).build(input_shape)\n\n    def call(self, x, training=False):\n        xl, xr = tf.split(x, 2, 3)\n        x = tf.concat((xl, self.model(xr)), 3)\n        return channel_shuffle_2(x)\n\n\nclass DownShuffleUnitV2(Model):\n    def __init__(self,\n                 filters,  # NOTE: will be filters // 2\n                 norm_type=""instance"",\n                 pad_type=""constant"",\n                 **kwargs):\n        super(DownShuffleUnitV2, self).__init__(name=""DownShuffleUnitV2"")\n        filters //= 2\n        self.r_model = tf.keras.models.Sequential([\n            Conv2D(filters, 1, use_bias=False),\n            get_norm(norm_type),\n            ReLU(),\n            DepthwiseConv2D(3, 2, \'same\', use_bias=False),\n            get_norm(norm_type),\n            Conv2D(filters, 1, use_bias=False),\n        ])\n        self.l_model = tf.keras.models.Sequential([\n            DepthwiseConv2D(3, 2, \'same\', use_bias=False),\n            get_norm(norm_type),\n            Conv2D(filters, 1, use_bias=False),\n        ])\n        self.bn_act = tf.keras.models.Sequential([\n            get_norm(norm_type),\n            ReLU(),\n        ])\n\n    def build(self, input_shape):\n        super(DownShuffleUnitV2, self).build(input_shape)\n\n    def call(self, x, training=False):\n        x = tf.concat((self.l_model(x), self.r_model(x)), 3)\n        x = self.bn_act(x)\n        return channel_shuffle_2(x)\n\n\nclass ConvBlock(Model):\n    def __init__(self,\n                 filters,\n                 kernel_size,\n                 stride=1,\n                 norm_type=""instance"",\n                 pad_type=""constant"",\n                 **kwargs):\n        super(ConvBlock, self).__init__(name=""ConvBlock"")\n        padding = (kernel_size - 1) // 2\n        padding = (padding, padding)\n\n        self.model = tf.keras.models.Sequential()\n        self.model.add(get_padding(pad_type, padding))\n        self.model.add(Conv2D(filters, kernel_size, stride))\n        self.model.add(get_padding(pad_type, padding))\n        self.model.add(Conv2D(filters, kernel_size))\n        self.model.add(get_norm(norm_type))\n        self.model.add(ReLU())\n\n    def build(self, input_shape):\n        super(ConvBlock, self).build(input_shape)\n\n    def call(self, x, training=False):\n        return self.model(x, training=training)\n\n\nclass ResBlock(Model):\n    def __init__(self,\n                 filters,\n                 kernel_size,\n                 norm_type=""instance"",\n                 pad_type=""constant"",\n                 **kwargs):\n        super(ResBlock, self).__init__(name=""ResBlock"")\n        padding = (kernel_size - 1) // 2\n        padding = (padding, padding)\n        self.model = tf.keras.models.Sequential()\n        self.model.add(get_padding(pad_type, padding))\n        self.model.add(Conv2D(filters, kernel_size))\n        self.model.add(get_norm(norm_type))\n        self.model.add(ReLU())\n        self.model.add(get_padding(pad_type, padding))\n        self.model.add(Conv2D(filters, kernel_size))\n        self.model.add(get_norm(norm_type))\n        self.add = Add()\n\n    def build(self, input_shape):\n        super(ResBlock, self).build(input_shape)\n\n    def call(self, x, training=False):\n        return self.add([self.model(x, training=training), x])\n\n\nclass UpSampleConv(Model):\n    def __init__(self,\n                 filters,\n                 kernel_size,\n                 norm_type=""instance"",\n                 pad_type=""constant"",\n                 light=False,\n                 **kwargs):\n        super(UpSampleConv, self).__init__(name=""UpSampleConv"")\n        if light:\n            self.model = tf.keras.models.Sequential([\n                Conv2D(filters, 1),\n                BasicShuffleUnitV2(filters, norm_type, pad_type)\n            ])\n        else:\n            self.model = ConvBlock(\n                filters, kernel_size, 1, norm_type, pad_type)\n\n    def build(self, input_shape):\n        super(UpSampleConv, self).build(input_shape)\n\n    def call(self, x, training=False):\n        x = tf.keras.backend.resize_images(x, 2, 2, ""channels_last"", \'bilinear\')\n        return self.model(x, training=training)\n\n\nclass StridedConv(Model):\n    def __init__(self,\n                 filters=64,\n                 lrelu_alpha=0.2,\n                 pad_type=""constant"",\n                 norm_type=""batch"",\n                 **kwargs):\n        super(StridedConv, self).__init__(name=""StridedConv"")\n\n        self.model = tf.keras.models.Sequential()\n        self.model.add(get_padding(pad_type, (1, 1)))\n        self.model.add(Conv2D(filters, 3, strides=(2, 2)))\n        self.model.add(LeakyReLU(lrelu_alpha))\n        self.model.add(get_padding(pad_type, (1, 1)))\n        self.model.add(Conv2D(filters * 2, 3))\n        self.model.add(get_norm(norm_type))\n        self.model.add(LeakyReLU(lrelu_alpha))\n\n    def build(self, input_shape):\n        super(StridedConv, self).build(input_shape)\n\n    def call(self, x, training=False):\n        return self.model(x, training=training)\n'"
logger.py,0,"b'import sys\nimport logging\n\n\ndef get_logger(name, log_file=None, debug=False):\n    lvl = logging.DEBUG if debug else logging.INFO\n    logging.basicConfig(level=lvl)\n    logger = logging.getLogger(name)\n    formatter = logging.Formatter(\n        ""[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s"",\n        ""%Y-%m-%d %H:%M:%S"")\n    stdhandler = logging.StreamHandler(sys.stdout)\n    stdhandler.setFormatter(formatter)\n    logger.addHandler(stdhandler)\n    if log_file is not None:\n        fhandler = logging.StreamHandler(open(log_file, ""a""))\n        fhandler.setFormatter(formatter)\n        logger.addHandler(fhandler)\n    logger.propagate = False\n    logger.setLevel(lvl)\n    return logger\n'"
to_pb.py,22,"b'""""""\nMinimal demonstration of tf1 compatibility\n""""""\nimport os\nfrom PIL import Image\n\ntry:\n    import tensorflow.compat.v1 as tf\n    tf.disable_v2_behavior()\nexcept (ImportError, AttributeError):\n    import tensorflow as tf\n\nfrom generator import Generator\nfrom logger import get_logger\n\n\n# NOTE: TF warnings are too noisy without this\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\ntf.logging.set_verbosity(tf.logging.ERROR)\n\n\ndef makedirs(path):\n    if not os.path.isdir(path):\n        os.makedirs(path)\n\n\ndef main(m_path, out_dir, light=False, test_out=True):\n    logger = get_logger(""tf1_export"", debug=test_out)\n    g = Generator(light=light)\n    t = tf.placeholder(tf.string, [])\n    x = tf.expand_dims(tf.image.decode_jpeg(tf.read_file(t), channels=3), 0)\n    x = (tf.cast(x, tf.float32) / 127.5) - 1\n    x = g(x, training=False)\n    out = tf.cast((tf.squeeze(x, 0) + 1) * 127.5, tf.uint8)\n    in_name, out_name = t.op.name, out.op.name\n    try:\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            g.load_weights(tf.train.latest_checkpoint(m_path))\n            in_graph_def = tf.get_default_graph().as_graph_def()\n            out_graph_def = tf.graph_util.convert_variables_to_constants(\n                sess, in_graph_def, [out_name])\n        tf.reset_default_graph()\n        tf.import_graph_def(out_graph_def, name=\'\')\n    except ValueError:\n        logger.error(""Failed to load specified weight."")\n        logger.error(""If you trained your model with --light, ""\n                     ""consider adding --light when executing this script; otherwise, ""\n                     ""do not add --light when executing this script."")\n        exit(1)\n    makedirs(out_dir)\n    m_cnt = 0\n    bpath = \'optimized_graph_light\' if light else \'optimized_graph\'\n    out_path = os.path.join(out_dir, f\'{bpath}_{m_cnt:04d}.pb\')\n    while os.path.exists(out_path):\n        m_cnt += 1\n        out_path = os.path.join(out_dir, f\'{bpath}_{m_cnt:04d}.pb\')\n    with tf.gfile.GFile(out_path, \'wb\') as f:\n        f.write(out_graph_def.SerializeToString())\n    if test_out:\n        with tf.Graph().as_default():\n            gd = tf.GraphDef()\n            with tf.gfile.GFile(out_path, \'rb\') as f:\n                gd.ParseFromString(f.read())\n            tf.import_graph_def(gd, name=\'\')\n            tf.get_default_graph().finalize()\n            t = tf.get_default_graph().get_tensor_by_name(f""{in_name}:0"")\n            out = tf.get_default_graph().get_tensor_by_name(f""{out_name}:0"")\n            from time import time\n            start = time()\n            with tf.Session() as sess:\n                img = Image.fromarray(sess.run(out, {t: ""input_images/temple.jpg""}))\n                img.show()\n            elapsed = time() - start\n            logger.debug(f""{elapsed} sec per img"")\n    logger.info(f""successfully exported ckpt to {out_path}"")\n    logger.info(f""input var name: {in_name}:0"")\n    logger.info(f""output var name: {out_name}:0"")\n\n\nif __name__ == ""__main__"":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--m_path"", type=str, default=""models"")\n    parser.add_argument(""--out_dir"", type=str, default=\'optimized_pbs\')\n    parser.add_argument(""--light"", action=\'store_true\')\n    parser.add_argument(""--not_test_out"", action=\'store_true\')\n    args = parser.parse_args()\n    main(args.m_path, args.out_dir, args.light, not args.not_test_out)\n'"
train.py,90,"b'import os\nimport gc\nfrom glob import glob\nfrom itertools import product\nfrom random import choice\n\nfrom imageio import imwrite\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom logger import get_logger\nfrom generator import Generator\nfrom discriminator import Discriminator\n\n\n@tf.function\ndef gram(x):\n    shape_x = tf.shape(x)\n    b = shape_x[0]\n    c = shape_x[3]\n    x = tf.reshape(x, [b, -1, c])\n    return tf.matmul(tf.transpose(x, [0, 2, 1]), x) / tf.cast((tf.size(x) // b), tf.float32)\n\n\nclass Trainer:\n    def __init__(\n        self,\n        dataset_name,\n        light,\n        source_domain,\n        target_domain,\n        gan_type,\n        epochs,\n        input_size,\n        multi_scale,\n        batch_size,\n        sample_size,\n        reporting_steps,\n        content_lambda,\n        style_lambda,\n        g_adv_lambda,\n        d_adv_lambda,\n        generator_lr,\n        discriminator_lr,\n        data_dir,\n        log_dir,\n        result_dir,\n        checkpoint_dir,\n        generator_checkpoint_prefix,\n        discriminator_checkpoint_prefix,\n        pretrain_checkpoint_prefix,\n        pretrain_model_dir,\n        model_dir,\n        disable_sampling,\n        ignore_vgg,\n        pretrain_learning_rate,\n        pretrain_epochs,\n        pretrain_saving_epochs,\n        pretrain_reporting_steps,\n        pretrain_generator_name,\n        generator_name,\n        discriminator_name,\n        debug,\n        **kwargs,\n    ):\n        self.debug = debug\n        self.ascii = os.name == ""nt""\n        self.dataset_name = dataset_name\n        self.light = light\n        self.source_domain = source_domain\n        self.target_domain = target_domain\n        self.gan_type = gan_type\n        self.epochs = epochs\n        self.input_size = input_size\n        self.multi_scale = multi_scale\n        self.batch_size = batch_size\n        self.sample_size = sample_size\n        self.reporting_steps = reporting_steps\n        self.content_lambda = float(content_lambda)\n        self.style_lambda = float(style_lambda)\n        self.g_adv_lambda = g_adv_lambda\n        self.d_adv_lambda = d_adv_lambda\n        self.generator_lr = generator_lr\n        self.discriminator_lr = discriminator_lr\n        self.data_dir = data_dir\n        self.log_dir = log_dir\n        self.result_dir = result_dir\n        self.checkpoint_dir = checkpoint_dir\n        self.generator_checkpoint_prefix = generator_checkpoint_prefix\n        self.discriminator_checkpoint_prefix = discriminator_checkpoint_prefix\n        self.pretrain_checkpoint_prefix = pretrain_checkpoint_prefix\n        self.pretrain_model_dir = pretrain_model_dir\n        self.model_dir = model_dir\n        self.disable_sampling = disable_sampling\n        self.ignore_vgg = ignore_vgg\n        self.pretrain_learning_rate = pretrain_learning_rate\n        self.pretrain_epochs = pretrain_epochs\n        self.pretrain_saving_epochs = pretrain_saving_epochs\n        self.pretrain_reporting_steps = pretrain_reporting_steps\n        self.pretrain_generator_name = pretrain_generator_name\n        self.generator_name = generator_name\n        self.discriminator_name = discriminator_name\n\n        self.logger = get_logger(""Trainer"", debug=debug)\n        # NOTE: just minimal demonstration of multi-scale training\n        self.sizes = [self.input_size - 32, self.input_size, self.input_size + 32]\n\n        if not self.ignore_vgg:\n            self.logger.info(""Setting up VGG19 for computing content loss..."")\n            from tensorflow.keras.applications import VGG19\n            from tensorflow.keras.layers import Conv2D\n            input_shape = (self.input_size, self.input_size, 3)\n            # download model using kwarg weights=""imagenet""\n            base_model = VGG19(weights=""imagenet"", include_top=False, input_shape=input_shape)\n            tmp_vgg_output = base_model.get_layer(""block4_conv3"").output\n            tmp_vgg_output = Conv2D(512, (3, 3), activation=\'linear\', padding=\'same\',\n                                    name=\'block4_conv4\')(tmp_vgg_output)\n            self.vgg = tf.keras.Model(inputs=base_model.input, outputs=tmp_vgg_output)\n            self.vgg.load_weights(os.path.expanduser(os.path.join(\n                ""~"", "".keras"", ""models"",\n                ""vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5"")), by_name=True)\n        else:\n            self.logger.info(""VGG19 will not be used. ""\n                             ""Content loss will simply imply pixel-wise difference."")\n            self.vgg = None\n\n        self.logger.info(f""Setting up objective functions and metrics using {self.gan_type}..."")\n        self.mae = tf.keras.losses.MeanAbsoluteError()\n        self.generator_loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n        if self.gan_type == ""gan"":\n            self.discriminator_loss_object = tf.keras.losses.BinaryCrossentropy(\n                from_logits=True)\n        elif self.gan_type == ""lsgan"":\n            self.discriminator_loss_object = tf.keras.losses.MeanSquaredError()\n        else:\n            wrong_msg = f""Non-recognized \'gan_type\': {self.gan_type}""\n            self.logger.critical(wrong_msg)\n            raise ValueError(wrong_msg)\n\n        self.g_total_loss_metric = tf.keras.metrics.Mean(""g_total_loss"", dtype=tf.float32)\n        self.g_adv_loss_metric = tf.keras.metrics.Mean(""g_adversarial_loss"", dtype=tf.float32)\n        if self.content_lambda != 0.:\n            self.content_loss_metric = tf.keras.metrics.Mean(""content_loss"", dtype=tf.float32)\n        if self.style_lambda != 0.:\n            self.style_loss_metric = tf.keras.metrics.Mean(""style_loss"", dtype=tf.float32)\n        self.d_total_loss_metric = tf.keras.metrics.Mean(""d_total_loss"", dtype=tf.float32)\n        self.d_real_loss_metric = tf.keras.metrics.Mean(""d_real_loss"", dtype=tf.float32)\n        self.d_fake_loss_metric = tf.keras.metrics.Mean(""d_fake_loss"", dtype=tf.float32)\n        self.d_smooth_loss_metric = tf.keras.metrics.Mean(""d_smooth_loss"", dtype=tf.float32)\n\n        self.metric_and_names = [\n            (self.g_total_loss_metric, ""g_total_loss""),\n            (self.g_adv_loss_metric, ""g_adversarial_loss""),\n            (self.d_total_loss_metric, ""d_total_loss""),\n            (self.d_real_loss_metric, ""d_real_loss""),\n            (self.d_fake_loss_metric, ""d_fake_loss""),\n            (self.d_smooth_loss_metric, ""d_smooth_loss""),\n        ]\n        if self.content_lambda != 0.:\n            self.metric_and_names.append((self.content_loss_metric, ""content_loss""))\n        if self.style_lambda != 0.:\n            self.metric_and_names.append((self.style_loss_metric, ""style_loss""))\n\n        self.logger.info(""Setting up checkpoint paths..."")\n        self.pretrain_checkpoint_prefix = os.path.join(\n            self.checkpoint_dir, ""pretrain"", self.pretrain_checkpoint_prefix)\n        self.generator_checkpoint_dir = os.path.join(\n            self.checkpoint_dir, self.generator_checkpoint_prefix)\n        self.generator_checkpoint_prefix = os.path.join(\n            self.generator_checkpoint_dir, self.generator_checkpoint_prefix)\n        self.discriminator_checkpoint_dir = os.path.join(\n            self.checkpoint_dir, self.discriminator_checkpoint_prefix)\n        self.discriminator_checkpoint_prefix = os.path.join(\n            self.discriminator_checkpoint_dir, self.discriminator_checkpoint_prefix)\n\n    def _save_generated_images(self, batch_x, image_name, nrow=2, ncol=4):\n        # NOTE: 0 <= batch_x <= 1, float32, numpy.ndarray\n        if not isinstance(batch_x, np.ndarray):\n            batch_x = batch_x.numpy()\n        n, h, w, c = batch_x.shape\n        out_arr = np.zeros([h * nrow, w * ncol, 3], dtype=np.uint8)\n        for (i, j), k in zip(product(range(nrow), range(ncol)), range(n)):\n            out_arr[(h * i):(h * (i+1)), (w * j):(w * (j+1))] = batch_x[k]\n        if not os.path.isdir(self.result_dir):\n            os.makedirs(self.result_dir)\n        imwrite(os.path.join(self.result_dir, image_name), out_arr)\n        gc.collect()\n        return out_arr\n\n    @tf.function\n    def random_resize(self, x):\n        size = choice(self.sizes)\n        return tf.image.resize(x, (size, size))\n\n    @tf.function\n    def image_processing(self, filename, is_train=True):\n        crop_size = self.input_size\n        if self.multi_scale and is_train:\n            crop_size += 32\n        x = tf.io.read_file(filename)\n        x = tf.image.decode_jpeg(x, channels=3)\n        if is_train:\n            sizes = tf.cast(\n                crop_size * tf.random.uniform([2], 0.9, 1.1), tf.int32)\n            shape = tf.shape(x)[:2]\n            sizes = tf.minimum(sizes, shape)\n            x = tf.image.random_crop(x, (sizes[0], sizes[1], 3))\n            x = tf.image.random_flip_left_right(x)\n        x = tf.image.resize(x, (crop_size, crop_size))\n        img = tf.cast(x, tf.float32) / 127.5 - 1\n        return img\n\n    def get_dataset(self, dataset_name, domain, _type, batch_size):\n        files = glob(os.path.join(self.data_dir, dataset_name, f""{_type}{domain}"", ""*""))\n        num_images = len(files)\n        self.logger.info(\n            f""Found {num_images} domain{domain} images in {_type}{domain} folder.""\n        )\n        ds = tf.data.Dataset.from_tensor_slices(files)\n        ds = ds.apply(tf.data.experimental.shuffle_and_repeat(num_images))\n\n        def fn(fname):\n            if self.multi_scale:\n                return self.random_resize(self.image_processing(fname, True))\n            else:\n                return self.image_processing(fname, True)\n\n        ds = ds.apply(tf.data.experimental.map_and_batch(fn, batch_size))\n        steps = int(np.ceil(num_images/batch_size))\n        # user iter(ds) to avoid generating iterator every epoch\n        return iter(ds), steps\n\n    @tf.function\n    def pass_to_vgg(self, tensor):\n        # NOTE: self.vgg should be fixed\n        if self.vgg is not None:\n            tensor = self.vgg(tensor)\n        return tensor\n\n    @tf.function\n    def content_loss(self, input_images, generated_images):\n        return self.mae(input_images, generated_images)\n\n    @tf.function\n    def style_loss(self, input_images, generated_images):\n        input_images = gram(input_images)\n        generated_images = gram(generated_images)\n        return self.mae(input_images, generated_images)\n\n    @tf.function\n    def discriminator_loss(self, real_output, fake_output, smooth_output):\n        real_loss = self.discriminator_loss_object(tf.ones_like(real_output), real_output)\n        fake_loss = self.discriminator_loss_object(tf.zeros_like(fake_output), fake_output)\n        smooth_loss = self.discriminator_loss_object(\n            tf.zeros_like(smooth_output), smooth_output)\n        total_loss = real_loss + fake_loss + smooth_loss\n        return real_loss, fake_loss, smooth_loss, total_loss\n\n    @tf.function\n    def generator_adversarial_loss(self, fake_output):\n        return self.generator_loss_object(tf.ones_like(fake_output), fake_output)\n\n    @tf.function\n    def pretrain_step(self, input_images, generator, optimizer):\n\n        with tf.GradientTape() as tape:\n            generated_images = generator(input_images, training=True)\n            c_loss = self.content_lambda * self.content_loss(\n                self.pass_to_vgg(input_images), self.pass_to_vgg(generated_images))\n\n        gradients = tape.gradient(c_loss, generator.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, generator.trainable_variables))\n\n        self.content_loss_metric(c_loss)\n\n    @tf.function\n    def train_step(self, source_images, target_images, smooth_images,\n                   generator, discriminator, g_optimizer, d_optimizer):\n\n        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n            real_output = discriminator(target_images, training=True)\n            generated_images = generator(source_images, training=True)\n            fake_output = discriminator(generated_images, training=True)\n            smooth_out = discriminator(smooth_images, training=True)\n            d_real_loss, d_fake_loss, d_smooth_loss, d_total_loss = \\\n                self.discriminator_loss(real_output, fake_output, smooth_out)\n\n            g_adv_loss = self.g_adv_lambda * self.generator_adversarial_loss(fake_output)\n            g_total_loss = g_adv_loss\n            # NOTE: self.*_lambdas are fixed\n            if self.content_lambda != 0. or self.style_lambda != 0.:\n                vgg_generated_images = self.pass_to_vgg(generated_images)\n                if self.content_lambda != 0.:\n                    c_loss = self.content_lambda * self.content_loss(\n                        self.pass_to_vgg(source_images), vgg_generated_images)\n                    g_total_loss = g_total_loss + c_loss\n                if self.style_lambda != 0.:\n                    s_loss = self.style_lambda * self.style_loss(\n                        self.pass_to_vgg(target_images[:vgg_generated_images.shape[0]]),\n                        vgg_generated_images)\n                    g_total_loss = g_total_loss + s_loss\n\n        d_grads = d_tape.gradient(d_total_loss, discriminator.trainable_variables)\n        g_grads = g_tape.gradient(g_total_loss, generator.trainable_variables)\n\n        d_optimizer.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n        g_optimizer.apply_gradients(zip(g_grads, generator.trainable_variables))\n\n        self.g_total_loss_metric(g_total_loss)\n        self.g_adv_loss_metric(g_adv_loss)\n        if self.content_lambda != 0.:\n            self.content_loss_metric(c_loss)\n        if self.style_lambda != 0.:\n            self.style_loss_metric(s_loss)\n        self.d_total_loss_metric(d_total_loss)\n        self.d_real_loss_metric(d_real_loss)\n        self.d_fake_loss_metric(d_fake_loss)\n        self.d_smooth_loss_metric(d_smooth_loss)\n\n    def pretrain_generator(self):\n        summary_writer = tf.summary.create_file_writer(os.path.join(self.log_dir, ""pretrain""))\n        self.logger.info(f""Starting to pretrain generator with {self.pretrain_epochs} epochs..."")\n        self.logger.info(\n            f""Building `{self.dataset_name}` dataset with domain `{self.source_domain}`...""\n        )\n        dataset, steps_per_epoch = self.get_dataset(dataset_name=self.dataset_name,\n                                                    domain=self.source_domain,\n                                                    _type=""train"",\n                                                    batch_size=self.batch_size)\n        if self.multi_scale:\n            self.logger.info(f""Initializing generator with ""\n                             f""batch_size: {self.batch_size}, input_size: multi-scale..."")\n        else:\n            self.logger.info(f""Initializing generator with ""\n                             f""batch_size: {self.batch_size}, input_size: {self.input_size}..."")\n        generator = Generator(base_filters=2 if self.debug else 64, light=self.light)\n        generator(tf.keras.Input(\n            shape=(self.input_size, self.input_size, 3),\n            batch_size=self.batch_size))\n        generator.summary()\n\n        self.logger.info(""Setting up optimizer to update generator\'s parameters..."")\n        optimizer = tf.keras.optimizers.Adam(\n            learning_rate=self.pretrain_learning_rate,\n            beta_1=0.5)\n\n        self.logger.info(f""Try restoring checkpoint: `{self.pretrain_checkpoint_prefix}`..."")\n        try:\n            checkpoint = tf.train.Checkpoint(generator=generator)\n            status = checkpoint.restore(tf.train.latest_checkpoint(\n                os.path.join(self.checkpoint_dir, ""pretrain"")))\n            status.assert_consumed()\n\n            self.logger.info(f""Previous checkpoints has been restored."")\n            trained_epochs = checkpoint.save_counter.numpy()\n            epochs = self.pretrain_epochs - trained_epochs\n            if epochs <= 0:\n                self.logger.info(f""Already trained {trained_epochs} epochs. ""\n                                 ""Set a larger `pretrain_epochs`..."")\n                return\n            else:\n                self.logger.info(f""Already trained {trained_epochs} epochs, ""\n                                 f""{epochs} epochs left to be trained..."")\n        except AssertionError:\n            self.logger.info(f""Checkpoint is not found, ""\n                             f""training from scratch with {self.pretrain_epochs} epochs..."")\n            trained_epochs = 0\n            epochs = self.pretrain_epochs\n\n        if not self.disable_sampling:\n            val_files = glob(os.path.join(\n                self.data_dir, self.dataset_name, f""test{self.source_domain}"", ""*""))\n            val_real_batch = tf.map_fn(\n                lambda fname: self.image_processing(fname, False),\n                tf.constant(val_files), tf.float32, back_prop=False)\n            real_batch = next(dataset)\n            while real_batch.shape[0] < self.sample_size:\n                real_batch = tf.concat((real_batch, next(dataset)), 0)\n            real_batch = real_batch[:self.sample_size]\n            with summary_writer.as_default():\n                img = np.expand_dims(self._save_generated_images(\n                    tf.cast((real_batch + 1) * 127.5, tf.uint8),\n                    image_name=""pretrain_sample_images.png""), 0,)\n                tf.summary.image(""pretrain_sample_images"", img, step=0)\n                img = np.expand_dims(self._save_generated_images(\n                    tf.cast((val_real_batch + 1) * 127.5, tf.uint8),\n                    image_name=""pretrain_val_sample_images.png""), 0,)\n                tf.summary.image(""pretrain_val_sample_images"", img, step=0)\n            gc.collect()\n        else:\n            self.logger.info(""Proceeding pretraining without sample images..."")\n\n        self.logger.info(""Starting pre-training loop, ""\n                         ""setting up summary writer to record progress on TensorBoard..."")\n\n        for epoch in range(epochs):\n            epoch_idx = trained_epochs + epoch + 1\n\n            for step in tqdm(\n                    range(1, steps_per_epoch + 1),\n                    desc=f""Pretrain Epoch {epoch + 1}/{epochs}""):\n                # NOTE: not following official ""for img in dataset"" example\n                #       since it generates new iterator every epoch and can\n                #       hardly be garbage-collected by python\n                image_batch = dataset.next()\n                self.pretrain_step(image_batch, generator, optimizer)\n\n                if step % self.pretrain_reporting_steps == 0:\n\n                    global_step = (epoch_idx - 1) * steps_per_epoch + step\n                    with summary_writer.as_default():\n                        tf.summary.scalar(\'content_loss\',\n                                          self.content_loss_metric.result(),\n                                          step=global_step)\n                        if not self.disable_sampling:\n                            fake_batch = tf.cast(\n                                (generator(real_batch, training=False) + 1) * 127.5, tf.uint8)\n                            img = np.expand_dims(self._save_generated_images(\n                                    fake_batch,\n                                    image_name=(f""pretrain_generated_images_at_epoch_{epoch_idx}""\n                                                f""_step_{step}.png"")),\n                                    0,\n                            )\n                            tf.summary.image(\'pretrain_generated_images\', img, step=global_step)\n                    self.content_loss_metric.reset_states()\n            with summary_writer.as_default():\n                if not self.disable_sampling:\n                    val_fake_batch = tf.cast(\n                        (generator(val_real_batch, training=False) + 1) * 127.5, tf.uint8)\n                    img = np.expand_dims(self._save_generated_images(\n                            val_fake_batch,\n                            image_name=(""pretrain_val_generated_images_at_epoch_""\n                                        f""{epoch_idx}_step_{step}.png"")),\n                            0,\n                    )\n                    tf.summary.image(\'pretrain_val_generated_images\', img, step=epoch)\n\n            if epoch % self.pretrain_saving_epochs == 0:\n                self.logger.info(f""Saving checkpoints after epoch {epoch_idx} ended..."")\n                checkpoint.save(file_prefix=self.pretrain_checkpoint_prefix)\n            gc.collect()\n        del dataset\n        gc.collect()\n\n    def train_gan(self):\n        self.logger.info(""Setting up summary writer to record progress on TensorBoard..."")\n        summary_writer = tf.summary.create_file_writer(self.log_dir)\n        self.logger.info(\n            f""Starting adversarial training with {self.epochs} epochs, ""\n            f""batch size: {self.batch_size}...""\n        )\n        self.logger.info(f""Building `{self.dataset_name}` ""\n                         ""datasets for source/target/smooth domains..."")\n        ds_source, steps_per_epoch = self.get_dataset(dataset_name=self.dataset_name,\n                                                      domain=self.source_domain,\n                                                      _type=""train"",\n                                                      batch_size=self.batch_size)\n        ds_target, _ = self.get_dataset(dataset_name=self.dataset_name,\n                                        domain=self.target_domain,\n                                        _type=""train"",\n                                        batch_size=self.batch_size)\n        ds_smooth, _ = self.get_dataset(dataset_name=self.dataset_name,\n                                        domain=f""{self.target_domain}_smooth"",\n                                        _type=""train"",\n                                        batch_size=self.batch_size)\n        self.logger.info(""Setting up optimizer to update generator and discriminator..."")\n        g_optimizer = tf.keras.optimizers.Adam(learning_rate=self.generator_lr, beta_1=.5)\n        d_optimizer = tf.keras.optimizers.Adam(learning_rate=self.discriminator_lr, beta_1=.5)\n        if self.multi_scale:\n            self.logger.info(f""Initializing generator with ""\n                             f""batch_size: {self.batch_size}, input_size: multi-scale..."")\n        else:\n            self.logger.info(f""Initializing generator with ""\n                             f""batch_size: {self.batch_size}, input_size: {self.input_size}..."")\n        generator = Generator(base_filters=2 if self.debug else 64, light=self.light)\n        generator(tf.keras.Input(\n            shape=(self.input_size, self.input_size, 3),\n            batch_size=self.batch_size))\n\n        self.logger.info(f""Searching existing checkpoints: `{self.generator_checkpoint_prefix}`..."")\n        try:\n            g_checkpoint = tf.train.Checkpoint(generator=generator)\n            g_checkpoint.restore(\n                tf.train.latest_checkpoint(\n                    self.generator_checkpoint_dir)).assert_existing_objects_matched()\n            self.logger.info(f""Previous checkpoints has been restored."")\n            trained_epochs = g_checkpoint.save_counter.numpy()\n            epochs = self.epochs - trained_epochs\n            if epochs <= 0:\n                self.logger.info(f""Already trained {trained_epochs} epochs. ""\n                                 ""Set a larger `epochs`..."")\n                return\n            else:\n                self.logger.info(f""Already trained {trained_epochs} epochs, ""\n                                 f""{epochs} epochs left to be trained..."")\n        except AssertionError as e:\n            self.logger.warning(e)\n            self.logger.warning(\n                ""Previous checkpoints are not found, trying to load checkpoints from pretraining...""\n            )\n\n            try:\n                g_checkpoint = tf.train.Checkpoint(generator=generator)\n                g_checkpoint.restore(tf.train.latest_checkpoint(\n                    os.path.join(\n                        self.checkpoint_dir, ""pretrain""))).assert_existing_objects_matched()\n                self.logger.info(""Successfully loaded ""\n                                 f""`{self.pretrain_checkpoint_prefix}`..."")\n            except AssertionError:\n                self.logger.warning(""specified pretrained checkpoint is not found, ""\n                                    ""training from scratch..."")\n\n            trained_epochs = 0\n            epochs = self.epochs\n\n        if self.multi_scale:\n            self.logger.info(f""Initializing discriminator with ""\n                             f""batch_size: {self.batch_size}, input_size: multi-scale..."")\n        else:\n            self.logger.info(f""Initializing discriminator with ""\n                             f""batch_size: {self.batch_size}, input_size: {self.input_size}..."")\n        if self.debug:\n            d_base_filters = 2\n        elif self.light:\n            d_base_filters = 24\n        else:\n            d_base_filters = 32\n        d = Discriminator(base_filters=d_base_filters)\n        d(tf.keras.Input(\n            shape=(self.input_size, self.input_size, 3),\n            batch_size=self.batch_size))\n\n        self.logger.info(""Searching existing checkpoints: ""\n                         f""`{self.discriminator_checkpoint_prefix}`..."")\n        try:\n            d_checkpoint = tf.train.Checkpoint(d=d)\n            d_checkpoint.restore(\n                tf.train.latest_checkpoint(\n                    self.discriminator_checkpoint_dir)).assert_existing_objects_matched()\n            self.logger.info(f""Previous checkpoints has been restored."")\n        except AssertionError:\n            self.logger.info(""specified checkpoint is not found, training from scratch..."")\n\n        if not self.disable_sampling:\n            val_files = glob(os.path.join(\n                self.data_dir, self.dataset_name, f""test{self.source_domain}"", ""*""))\n            val_real_batch = tf.map_fn(\n                lambda fname: self.image_processing(fname, False),\n                tf.constant(val_files), tf.float32, back_prop=False)\n            real_batch = next(ds_source)\n            while real_batch.shape[0] < self.sample_size:\n                real_batch = tf.concat((real_batch, next(ds_source)), 0)\n            real_batch = real_batch[:self.sample_size]\n            with summary_writer.as_default():\n                img = np.expand_dims(self._save_generated_images(\n                    tf.cast((real_batch + 1) * 127.5, tf.uint8),\n                    image_name=""gan_sample_images.png""), 0,)\n                tf.summary.image(""gan_sample_images"", img, step=0)\n                img = np.expand_dims(self._save_generated_images(\n                    tf.cast((val_real_batch + 1) * 127.5, tf.uint8),\n                    image_name=""gan_val_sample_images.png""), 0,)\n                tf.summary.image(""gan_val_sample_images"", img, step=0)\n            gc.collect()\n        else:\n            self.logger.info(""Proceeding training without sample images..."")\n\n        self.logger.info(""Starting training loop..."")\n\n        self.logger.info(f""Number of trained epochs: {trained_epochs}, ""\n                         f""epochs to be trained: {epochs}, ""\n                         f""batch size: {self.batch_size}"")\n        for epoch in range(epochs):\n            epoch_idx = trained_epochs + epoch + 1\n\n            for step in tqdm(\n                    range(1, steps_per_epoch + 1),\n                    desc=f\'Train {epoch + 1}/{epochs}\',\n                    total=steps_per_epoch):\n                source_images, target_images, smooth_images = (\n                    ds_source.next(), ds_target.next(), ds_smooth.next())\n                self.train_step(source_images, target_images, smooth_images,\n                                generator, d, g_optimizer, d_optimizer)\n\n                if step % self.reporting_steps == 0:\n\n                    global_step = (epoch_idx - 1) * steps_per_epoch + step\n                    with summary_writer.as_default():\n                        for metric, name in self.metric_and_names:\n                            tf.summary.scalar(name, metric.result(), step=global_step)\n                            metric.reset_states()\n                        if not self.disable_sampling:\n                            fake_batch = tf.cast(\n                                (generator(real_batch, training=False) + 1) * 127.5, tf.uint8)\n                            img = np.expand_dims(self._save_generated_images(\n                                    fake_batch,\n                                    image_name=(""gan_generated_images_at_epoch_""\n                                                f""{epoch_idx}_step_{step}.png"")),\n                                    0,\n                            )\n                            tf.summary.image(\'gan_generated_images\', img, step=global_step)\n\n                    self.logger.debug(f""Epoch {epoch_idx}, Step {step} finished, ""\n                                      f""{global_step * self.batch_size} images processed."")\n\n            with summary_writer.as_default():\n                if not self.disable_sampling:\n                    val_fake_batch = tf.cast(\n                        (generator(val_real_batch, training=False) + 1) * 127.5, tf.uint8)\n                    img = np.expand_dims(self._save_generated_images(\n                            val_fake_batch,\n                            image_name=(""gan_val_generated_images_at_epoch_""\n                                        f""{epoch_idx}_step_{step}.png"")),\n                            0,\n                    )\n                    tf.summary.image(\'gan_val_generated_images\', img, step=epoch)\n            self.logger.info(f""Saving checkpoints after epoch {epoch_idx} ended..."")\n            g_checkpoint.save(file_prefix=self.generator_checkpoint_prefix)\n            d_checkpoint.save(file_prefix=self.discriminator_checkpoint_prefix)\n\n            generator.save_weights(os.path.join(self.model_dir, ""generator""))\n            gc.collect()\n        del ds_source, ds_target, ds_smooth\n        gc.collect()\n\n\ndef main(**kwargs):\n    t = Trainer(**kwargs)\n\n    mode = kwargs[""mode""]\n    if mode == ""full"":\n        t.pretrain_generator()\n        gc.collect()\n        t.train_gan()\n    elif mode == ""pretrain"":\n        t.pretrain_generator()\n    elif mode == ""gan"":\n        t.train_gan()\n\n\nif __name__ == ""__main__"":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--mode"", type=str, default=""full"",\n                        choices=[""full"", ""pretrain"", ""gan""])\n    parser.add_argument(""--dataset_name"", type=str, default=""realworld2cartoon"")\n    parser.add_argument(""--light"", action=""store_true"")\n    parser.add_argument(""--input_size"", type=int, default=256)\n    parser.add_argument(""--multi_scale"", action=""store_true"")\n    parser.add_argument(""--batch_size"", type=int, default=1)\n    parser.add_argument(""--sample_size"", type=int, default=8)\n    parser.add_argument(""--source_domain"", type=str, default=""A"")\n    parser.add_argument(""--target_domain"", type=str, default=""B"")\n    parser.add_argument(""--gan_type"", type=str, default=""lsgan"", choices=[""gan"", ""lsgan""])\n    parser.add_argument(""--epochs"", type=int, default=100)\n    parser.add_argument(""--reporting_steps"", type=int, default=100)\n    parser.add_argument(""--content_lambda"", type=float, default=10)\n    parser.add_argument(""--style_lambda"", type=float, default=1.)\n    parser.add_argument(""--g_adv_lambda"", type=float, default=1)\n    parser.add_argument(""--d_adv_lambda"", type=float, default=1)\n    parser.add_argument(""--generator_lr"", type=float, default=1e-5)\n    parser.add_argument(""--discriminator_lr"", type=float, default=1e-5)\n    parser.add_argument(""--ignore_vgg"", action=""store_true"")\n    parser.add_argument(""--pretrain_learning_rate"", type=float, default=1e-5)\n    parser.add_argument(""--pretrain_epochs"", type=int, default=2)\n    parser.add_argument(""--pretrain_saving_epochs"", type=int, default=1)\n    parser.add_argument(""--pretrain_reporting_steps"", type=int, default=100)\n    parser.add_argument(""--data_dir"", type=str, default=""datasets"")\n    parser.add_argument(""--log_dir"", type=str, default=""runs"")\n    parser.add_argument(""--result_dir"", type=str, default=""result"")\n    parser.add_argument(""--checkpoint_dir"", type=str, default=""training_checkpoints"")\n    parser.add_argument(""--generator_checkpoint_prefix"", type=str, default=""generator"")\n    parser.add_argument(""--discriminator_checkpoint_prefix"", type=str, default=""discriminator"")\n    parser.add_argument(""--pretrain_checkpoint_prefix"", type=str, default=""pretrain_generator"")\n    parser.add_argument(""--pretrain_model_dir"", type=str, default=""models"")\n    parser.add_argument(""--model_dir"", type=str, default=""models"")\n    parser.add_argument(""--disable_sampling"", action=""store_true"")\n    # TODO: rearrange the order of options\n    parser.add_argument(\n        ""--pretrain_generator_name"", type=str, default=""pretrain_generator""\n    )\n    parser.add_argument(""--generator_name"", type=str, default=""generator"")\n    parser.add_argument(""--discriminator_name"", type=str, default=""discriminator"")\n    parser.add_argument(""--not_show_progress_bar"", action=""store_true"")\n    parser.add_argument(""--debug"", action=""store_true"")\n    parser.add_argument(""--show_tf_cpp_log"", action=""store_true"")\n\n    args = parser.parse_args()\n\n    if not args.show_tf_cpp_log:\n        os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""\n\n    args.show_progress = not args.not_show_progress_bar\n    kwargs = vars(args)\n    main(**kwargs)\n'"
scripts/smooth.py,0,"b'""""""\ncredit to https://github.com/taki0112/CartoonGAN-Tensorflow/blob/master/edge_smooth.py\nLICENSE for this script: https://github.com/taki0112/CartoonGAN-Tensorflow/blob/master/LICENSE\n""""""\nimport os\nimport numpy as np\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\n\ndef make_edge_smooth(path):\n    file_list = glob(os.path.expanduser(os.path.join(path, \'trainB\', \'*\')))\n    save_dir = os.path.expanduser(os.path.join(path, \'trainB_smooth\'))\n    if not os.path.isdir(save_dir):\n        os.makedirs(save_dir)\n\n    kernel_size = 5\n    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n    gauss = cv2.getGaussianKernel(kernel_size, 0)\n    gauss = gauss * gauss.transpose(1, 0)\n\n    for f in tqdm(file_list):\n        file_name = os.path.basename(f)\n\n        bgr_img = cv2.imread(f)\n        gray_img = cv2.imread(f, 0)\n        pad_img = np.pad(bgr_img, ((2, 2), (2, 2), (0, 0)), mode=\'reflect\')\n        edges = cv2.Canny(gray_img, 100, 200)\n        dilation = cv2.dilate(edges, kernel)\n\n        gauss_img = np.copy(bgr_img)\n        idx = np.where(dilation != 0)\n        for i in range(np.sum(dilation != 0)):\n            gauss_img[idx[0][i], idx[1][i], 0] = np.sum(np.multiply(\n                pad_img[idx[0][i]:idx[0][i] + kernel_size, idx[1][i]:idx[1][i] + kernel_size, 0],\n                gauss))\n            gauss_img[idx[0][i], idx[1][i], 1] = np.sum(np.multiply(\n                pad_img[idx[0][i]:idx[0][i] + kernel_size, idx[1][i]:idx[1][i] + kernel_size, 1],\n                gauss))\n            gauss_img[idx[0][i], idx[1][i], 2] = np.sum(np.multiply(\n                pad_img[idx[0][i]:idx[0][i] + kernel_size, idx[1][i]:idx[1][i] + kernel_size, 2],\n                gauss))\n\n        cv2.imwrite(os.path.join(save_dir, file_name), gauss_img)\n\n\ndef main(path):\n    make_edge_smooth(path)\n\n\nif __name__ == \'__main__\':\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--path"", type=str, help=\'path to your dataset\')\n    args = parser.parse_args()\n    main(args.path)\n'"
style_transfer/cartoongan/__init__.py,0,b''
style_transfer/cartoongan/cartoongan.py,29,"b'import os\nimport numpy as np\nimport tensorflow as tf\nfrom keras_contrib.layers import InstanceNormalization\nfrom tensorflow.keras.layers import Layer, InputSpec\n\nPRETRAINED_WEIGHT_DIR = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)), ""pretrained_weights"")\n\n\n# ref: https://stackoverflow.com/a/53349976/2447655\nclass ReflectionPadding2D(Layer):\n    def __init__(self, padding=(1, 1), **kwargs):\n        self.padding = tuple(padding)\n        self.input_spec = [InputSpec(ndim=4)]\n        super(ReflectionPadding2D, self).__init__(**kwargs)\n\n    def compute_output_shape(self, s):\n        """""" If you are using ""channels_last"" configuration""""""\n        return s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3]\n\n    def call(self, x):\n        w_pad, h_pad = self.padding\n        return tf.pad(x, [[0, 0], [h_pad, h_pad], [w_pad, w_pad], [0, 0]], \'REFLECT\')\n\n\ndef conv_layer(style, name, filters, kernel_size, strides=(1, 1), bias=True):\n    init_weight = np.load(f""{PRETRAINED_WEIGHT_DIR}/{style}/{name}.weight.npy"")\n    init_weight = np.transpose(init_weight, [2, 3, 1, 0])\n    init_bias = np.load(f""{PRETRAINED_WEIGHT_DIR}/{style}/{name}.bias.npy"")\n\n    if bias:\n        bias_initializer = tf.keras.initializers.constant(init_bias)\n    else:\n        bias_initializer = ""zeros""\n\n    layer = tf.keras.layers.Conv2D(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        kernel_initializer=tf.keras.initializers.constant(init_weight),\n        bias_initializer=bias_initializer\n    )\n    return layer\n\n\ndef instance_norm_layer(style, name, epsilon=1e-9):\n    init_beta = np.load(f""{PRETRAINED_WEIGHT_DIR}/{style}/{name}.shift.npy"")\n    init_gamma = np.load(f""{PRETRAINED_WEIGHT_DIR}/{style}/{name}.scale.npy"")\n\n    layer = InstanceNormalization(\n        axis=-1,\n        epsilon=epsilon,\n        beta_initializer=tf.keras.initializers.constant(init_beta),\n        gamma_initializer=tf.keras.initializers.constant(init_gamma)\n    )\n    return layer\n\n\ndef deconv_layers(style, name, filters, kernel_size, strides=(1, 1)):\n    init_weight = np.load(f""{PRETRAINED_WEIGHT_DIR}/{style}/{name}.weight.npy"")\n    init_weight = np.transpose(init_weight, [2, 3, 1, 0])\n    init_bias = np.load(f""{PRETRAINED_WEIGHT_DIR}/{style}/{name}.bias.npy"")\n\n    layers = list()\n    layers.append(tf.keras.layers.Conv2DTranspose(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        kernel_initializer=tf.keras.initializers.constant(init_weight),\n        bias_initializer=tf.keras.initializers.constant(init_bias)\n    ))\n\n    layers.append(tf.keras.layers.Cropping2D(cropping=((1, 0), (1, 0))))\n    return layers\n\n\ndef load_model(style):\n    inputs = tf.keras.Input(shape=(None, None, 3))\n\n    # y = tf.keras.layers.ZeroPadding2D(padding=(3, 3))(inputs)\n    y = ReflectionPadding2D(padding=(3, 3))(inputs)\n    y = conv_layer(style, ""conv01_1"", filters=64, kernel_size=7)(y)\n    y = instance_norm_layer(style, ""in01_1"")(y)\n    y = tf.keras.layers.Activation(""relu"")(y)\n\n    # y = tf.keras.layers.ZeroPadding2D(padding=(1, 1))(y)\n    y = ReflectionPadding2D(padding=(1, 1))(y)\n    y = conv_layer(style, ""conv02_1"", filters=128, kernel_size=3, strides=(2, 2))(y)\n    # y = tf.keras.layers.ZeroPadding2D(padding=(1, 1))(y)\n    y = ReflectionPadding2D(padding=(1, 1))(y)\n    y = conv_layer(style, ""conv02_2"", filters=128, kernel_size=3, strides=(1, 1))(y)\n    y = instance_norm_layer(style, ""in02_1"")(y)\n    y = tf.keras.layers.Activation(""relu"")(y)\n\n    # y = tf.keras.layers.ZeroPadding2D(padding=(1, 1))(y)\n    y = ReflectionPadding2D(padding=(1, 1))(y)\n    y = conv_layer(style, ""conv03_1"", filters=256, kernel_size=3, strides=(2, 2))(y)\n    # y = tf.keras.layers.ZeroPadding2D(padding=(1, 1))(y)\n    y = ReflectionPadding2D(padding=(1, 1))(y)\n    y = conv_layer(style, ""conv03_2"", filters=256, kernel_size=3, strides=(1, 1))(y)\n    y = instance_norm_layer(style, ""in03_1"")(y)\n\n    t_prev = tf.keras.layers.Activation(""relu"")(y)\n\n    for i in range(4, 12):\n        # y = tf.keras.layers.ZeroPadding2D(padding=(1, 1))(t_prev)\n        y = ReflectionPadding2D(padding=(1, 1))(t_prev)\n        y = conv_layer(style, ""conv%02d_1"" % i, filters=256, kernel_size=3)(y)\n        y = instance_norm_layer(style, ""in%02d_1"" % i)(y)\n        y = tf.keras.layers.Activation(""relu"")(y)\n\n        t = ReflectionPadding2D(padding=(1, 1))(y)\n        t = conv_layer(style, ""conv%02d_2"" % i, filters=256, kernel_size=3)(t)\n        t = instance_norm_layer(style, ""in%02d_2"" % i)(t)\n\n        t_prev = tf.keras.layers.Add()([t, t_prev])\n\n        if i == 11:\n            y = t_prev\n\n    layers = deconv_layers(style, ""deconv01_1"", filters=128, kernel_size=3, strides=(2, 2))\n    for layer in layers:\n        y = layer(y)\n    # y = tf.keras.layers.ZeroPadding2D(padding=(1, 1))(y)\n    y = ReflectionPadding2D(padding=(1, 1))(y)\n    y = conv_layer(style, ""deconv01_2"", filters=128, kernel_size=3)(y)\n    y = instance_norm_layer(style, ""in12_1"")(y)\n    y = tf.keras.layers.Activation(""relu"")(y)\n\n    layers = deconv_layers(style, ""deconv02_1"", filters=64, kernel_size=3, strides=(2, 2))\n    for layer in layers:\n        y = layer(y)\n    # y = tf.keras.layers.ZeroPadding2D(padding=(1, 1))(y)\n    y = ReflectionPadding2D(padding=(1, 1))(y)\n    y = conv_layer(style, ""deconv02_2"", filters=64, kernel_size=3)(y)\n    y = instance_norm_layer(style, ""in13_1"")(y)\n    y = tf.keras.layers.Activation(""relu"")(y)\n\n    # y = tf.keras.layers.ZeroPadding2D(padding=(3, 3))(y)\n    y = ReflectionPadding2D(padding=(3, 3))(y)\n    y = conv_layer(style, ""deconv03_1"", filters=3, kernel_size=7)(y)\n    y = tf.keras.layers.Activation(""tanh"")(y)\n\n    model = tf.keras.Model(inputs=inputs, outputs=y)\n\n    return model\n\n\nif __name__ == \'__main__\':\n    g = load_model(style=""shinkai"")\n    np.random.seed(9527)\n    nx = np.random.rand(1, 225, 225, 3).astype(np.float32)\n    out = g(nx)\n    tf_out = np.load(""tf_out.npy"")\n\n    diff = np.sqrt(np.mean((out - tf_out) ** 2))\n    assert diff < 1e-6\n'"
