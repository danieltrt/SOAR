file_path,api_count,code
src/__init__.py,0,b''
test/__init__.py,0,b''
src/reversi_zero/__init__.py,0,b''
src/reversi_zero/config.py,0,"b'import os\n\nfrom moke_config import ConfigBase\n\n\ndef _project_dir():\n    d = os.path.dirname\n    return d(d(d(os.path.abspath(__file__))))\n\n\ndef _data_dir():\n    return os.path.join(_project_dir(), ""data"")\n\n\nclass Config(ConfigBase):\n    def __init__(self):\n        self.type = ""default""\n        self.opts = Options()\n        self.resource = ResourceConfig()\n        self.gui = GuiConfig()\n        self.nboard = NBoardConfig()\n        self.model = ModelConfig()\n        self.play = PlayConfig()\n        self.play_data = PlayDataConfig()\n        self.trainer = TrainerConfig()\n        self.eval = EvaluateConfig()\n        self.play_with_human = PlayWithHumanConfig()\n\n\nclass Options(ConfigBase):\n    new = False\n\n\nclass ResourceConfig(ConfigBase):\n    def __init__(self):\n        self.project_dir = os.environ.get(""PROJECT_DIR"", _project_dir())\n        self.data_dir = os.environ.get(""DATA_DIR"", _data_dir())\n        self.model_dir = os.environ.get(""MODEL_DIR"", os.path.join(self.data_dir, ""model""))\n        self.model_best_config_path = os.path.join(self.model_dir, ""model_best_config.json"")\n        self.model_best_weight_path = os.path.join(self.model_dir, ""model_best_weight.h5"")\n\n        self.next_generation_model_dir = os.path.join(self.model_dir, ""next_generation"")\n        self.next_generation_model_dirname_tmpl = ""model_%s""\n        self.next_generation_model_config_filename = ""model_config.json""\n        self.next_generation_model_weight_filename = ""model_weight.h5""\n\n        self.play_data_dir = os.path.join(self.data_dir, ""play_data"")\n        self.play_data_filename_tmpl = ""play_%s.json""\n        self.self_play_ggf_data_dir = os.path.join(self.data_dir, ""self_play-ggf"")\n        self.ggf_filename_tmpl = ""self_play-%s.ggf""\n\n        self.log_dir = os.path.join(self.project_dir, ""logs"")\n        self.main_log_path = os.path.join(self.log_dir, ""main.log"")\n        self.tensorboard_log_dir = os.path.join(self.log_dir, \'tensorboard\')\n        self.self_play_log_dir = os.path.join(self.tensorboard_log_dir, ""self_play"")\n        self.force_learing_rate_file = os.path.join(self.data_dir, "".force-lr"")\n        self.force_simulation_num_file = os.path.join(self.data_dir, "".force-sim"")\n        self.self_play_game_idx_file = os.path.join(self.data_dir, "".self-play-game-idx"")\n\n    def create_directories(self):\n        dirs = [self.project_dir, self.data_dir, self.model_dir, self.play_data_dir, self.log_dir,\n                self.next_generation_model_dir, self.self_play_log_dir, self.self_play_ggf_data_dir]\n        for d in dirs:\n            if not os.path.exists(d):\n                os.makedirs(d)\n\n\nclass GuiConfig(ConfigBase):\n    def __init__(self):\n        self.window_size = (400, 440)\n        self.window_title = ""reversi-alpha-zero""\n\n\nclass PlayWithHumanConfig(ConfigBase):\n    def __init__(self):\n        self.parallel_search_num = 8\n        self.noise_eps = 0\n        self.change_tau_turn = 0\n        self.resign_threshold = None\n        self.use_newest_next_generation_model = True\n\n    def update_play_config(self, pc):\n        """"""\n\n        :param PlayConfig pc:\n        :return:\n        """"""\n        pc.noise_eps = self.noise_eps\n        pc.change_tau_turn = self.change_tau_turn\n        pc.parallel_search_num = self.parallel_search_num\n        pc.resign_threshold = self.resign_threshold\n        pc.use_newest_next_generation_model = self.use_newest_next_generation_model\n\n\nclass NBoardConfig(ConfigBase):\n    def __init__(self):\n        self.my_name = ""RAZ""\n        self.read_stdin_timeout = 0.1\n        self.simulation_num_per_depth_about = 20\n        self.hint_callback_per_sim = 10\n\n\nclass EvaluateConfig(ConfigBase):\n    def __init__(self):\n        self.game_num = 200  # 400\n        self.replace_rate = 0.55\n        self.play_config = PlayConfig()\n        self.play_config.simulation_num_per_move = 400\n        self.play_config.thinking_loop = 1\n        self.play_config.change_tau_turn = 0\n        self.play_config.noise_eps = 0\n        self.play_config.disable_resignation_rate = 0\n        self.evaluate_latest_first = True\n\n\nclass PlayDataConfig(ConfigBase):\n    def __init__(self):\n        # Max Training Data Size = nb_game_in_file * max_file_num * 8\n        self.multi_process_num = 16\n        self.nb_game_in_file = 2\n        self.max_file_num = 800\n        self.save_policy_of_tau_1 = True\n        self.enable_ggf_data = True\n        self.nb_game_in_ggf_file = 100\n        self.drop_draw_game_rate = 0\n\n\nclass PlayConfig(ConfigBase):\n    def __init__(self):\n        self.simulation_num_per_move = 200\n        self.share_mtcs_info_in_self_play = True\n        self.reset_mtcs_info_per_game = 1\n        self.thinking_loop = 10\n        self.required_visit_to_decide_action = 400\n        self.start_rethinking_turn = 8\n        self.c_puct = 1\n        self.noise_eps = 0.25\n        self.dirichlet_alpha = 0.5\n        self.change_tau_turn = 4\n        self.virtual_loss = 3\n        self.prediction_queue_size = 16\n        self.parallel_search_num = 8\n        self.prediction_worker_sleep_sec  = 0.0001\n        self.wait_for_expanding_sleep_sec = 0.00001\n        self.resign_threshold = -0.9\n        self.allowed_resign_turn = 20\n        self.disable_resignation_rate = 0.1\n        self.false_positive_threshold = 0.05\n        self.resign_threshold_delta = 0.01\n        self.policy_decay_turn = 60  # not used\n        self.policy_decay_power = 3\n\n        # Using a solver is a kind of cheating!\n        self.use_solver_turn = 50\n        self.use_solver_turn_in_simulation = 50\n\n        #\n        self.schedule_of_simulation_num_per_move = [\n            (0, 8),\n            (300, 50),\n            (2000, 200),\n        ]\n\n        # True means evaluating \'AlphaZero\' method (disable \'eval\' worker).\n        # Please change to False if you want to evaluate \'AlphaGo Zero\' method.\n        self.use_newest_next_generation_model = True\n\n\nclass TrainerConfig(ConfigBase):\n    def __init__(self):\n        self.wait_after_save_model_ratio = 1  # wait after saving model\n        self.batch_size = 256  # 2048\n        self.min_data_size_to_learn = 100000\n        self.epoch_to_checkpoint = 1\n        self.start_total_steps = 0\n        self.save_model_steps = 200\n        self.use_tensorboard = True\n        self.logging_per_steps = 100\n        self.delete_self_play_after_number_of_training = 0  # control ratio of train:self data.\n        self.lr_schedules = [\n            (0, 0.01),\n            (150000, 0.001),\n            (300000, 0.0001),\n        ]\n\n\nclass ModelConfig(ConfigBase):\n    def __init__(self):\n        self.cnn_filter_num = 256\n        self.cnn_filter_size = 3\n        self.res_layer_num = 10\n        self.l2_reg = 1e-4\n        self.value_fc_size = 256\n'"
src/reversi_zero/manager.py,0,"b'import argparse\n\nfrom logging import getLogger\n\nimport yaml\nfrom moke_config import create_config\n\nfrom .lib.logger import setup_logger\nfrom .config import Config\n\nlogger = getLogger(__name__)\n\nCMD_LIST = [\'self\', \'opt\', \'eval\', \'play_gui\', \'nboard\']\n\n\ndef create_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""cmd"", help=""what to do"", choices=CMD_LIST)\n    parser.add_argument(""-c"", help=""specify config yaml"", dest=""config_file"")\n    parser.add_argument(""--new"", help=""run from new best model"", action=""store_true"")\n    parser.add_argument(""--type"", help=""deprecated. Please use -c instead"")\n    parser.add_argument(""--total-step"", help=""set TrainerConfig.start_total_steps"", type=int)\n    return parser\n\n\ndef setup(config: Config, args):\n    config.opts.new = args.new\n    if args.total_step is not None:\n        config.trainer.start_total_steps = args.total_step\n    config.resource.create_directories()\n    setup_logger(config.resource.main_log_path)\n\n\ndef start():\n    parser = create_parser()\n    args = parser.parse_args()\n    if args.type:\n        print(""I\'m very sorry. --type option was deprecated. Please use -c option instead!"")\n        return 1\n\n    if args.config_file:\n        with open(args.config_file, ""rt"") as f:\n            config = create_config(Config, yaml.load(f))\n    else:\n        config = create_config(Config)\n    setup(config, args)\n\n    if args.cmd != ""nboard"":\n        logger.info(f""config type: {config.type}"")\n\n    if args.cmd == ""self"":\n        from .worker import self_play\n        return self_play.start(config)\n    elif args.cmd == \'opt\':\n        from .worker import optimize\n        return optimize.start(config)\n    elif args.cmd == \'eval\':\n        from .worker import evaluate\n        return evaluate.start(config)\n    elif args.cmd == \'play_gui\':\n        from .play_game import gui\n        return gui.start(config)\n    elif args.cmd == \'nboard\':\n        from .play_game import nboard\n        return nboard.start(config)\n'"
src/reversi_zero/run.py,0,"b'\nimport os\nimport sys\nfrom dotenv import load_dotenv, find_dotenv\n\nif find_dotenv():\n    load_dotenv(find_dotenv())\n\n_PATH_ = os.path.dirname(os.path.dirname(__file__))\n\nif _PATH_ not in sys.path:\n    sys.path.append(_PATH_)\n\n\nif __name__ == ""__main__"":\n    from reversi_zero import manager\n    manager.start()\n'"
src/spike/__init__.py,0,b''
src/spike/cy1.py,0,"b'from time import time\n\nimport pyximport\n\nfrom reversi_zero.env.reversi_env import Player\nfrom reversi_zero.lib.util import parse_to_bitboards\n\npyximport.install()\n\nimport timeit\n\n\ndef examples():\n    ret = [\n    \'\'\'\n    ##########\n    #OO      #\n    #XOO     #\n    #OXOOO   #\n    #  XOX   #\n    #   XXX  #\n    #  X     #\n    # X      #\n    #        #\n    ##########\n    \'\'\',\n    \'\'\'\n    ##########\n    #OOOOOXO #\n    #OOOOOXOO#\n    #OOOOOXOO#\n    #OXOXOXOO#\n    #OOXOXOXO#\n    #OOOOOOOO#\n    #XXXO   O#\n    #        #\n    ##########\n    \'\'\',\n    \'\'\'\n    ##########\n    #OOXXXXX #\n    #XOXXXXXX#\n    #XXXXXXXX#\n    #XOOXXXXX#\n    #OXXXOOOX#\n    #OXXOOOOX#\n    #OXXXOOOX#\n    # OOOOOOO#\n    ##########\n    \'\'\']\n    return ret\n\n\ndef test_find_correct_move():\n    import spike.bitboard_cython as f\n    import reversi_zero.lib.bitboard as b\n\n    for ex in examples():\n        black, white = parse_to_bitboards(ex)\n        assert f.find_correct_moves(black, white) == b.find_correct_moves(black, white)\n        cy = timeit.timeit(""f.find_correct_moves(black, white)"", globals=locals(), number=10000)\n        py = timeit.timeit(""b.find_correct_moves(black, white)"", globals=locals(), number=10000)\n        print(f""Cython={cy} : cPython={py}"")\n\n\ndef test_calc_flip():\n    import spike.bitboard_cython as f\n    import reversi_zero.lib.bitboard as b\n\n    for ex in examples():\n        black, white = parse_to_bitboards(ex)\n        assert f.find_correct_moves(black, white) == b.find_correct_moves(black, white)\n        legal_moves = f.find_correct_moves(black, white)\n        action_list = [idx for idx in range(64) if legal_moves & (1 << idx)]\n\n        for action in action_list:\n            assert f.calc_flip(action, black, white) == b.calc_flip(action, black, white)\n            cy = timeit.timeit(""f.calc_flip(action, black, white)"", globals=locals(), number=10000)\n            py = timeit.timeit(""b.calc_flip(action, black, white)"", globals=locals(), number=10000)\n            print(f""Cython={cy} : cPython={py}"")\n\n\ndef test_solve():\n    def q1():\n        import reversi_zero.lib.reversi_solver as p\n        import spike.reversi_solver_cython as c\n        board = \'\'\'\n        ##########\n        #XXXX    #\n        #XOXX    #\n        #XOXXOOOO#\n        #XOXOXOOO#\n        #XOXXOXOO#\n        #OOOOXOXO#\n        # OOOOOOO#\n        #  XXXXXO#\n        ##########\'\'\'\n        b, w = parse_to_bitboards(board)\n        print(""correct is (57, +2)"")\n\n        start_time = time()\n        ret = p.ReversiSolver().solve(b, w, next_player=Player.white, exactly=False)\n        print(f""{time()-start_time} sec: ret={ret}"")\n\n        start_time = time()\n        ret = c.ReversiSolver().solve(b, w, next_player=2, exactly=False)\n        print(f""{time()-start_time} sec: ret={ret}"")\n\n        # rr = p.ReversiSolver()\n        # print(rr.solve(b, w, Player.white, exactly=False))\n        # print(len(rr.cache))\n\n    def q2():\n        import reversi_zero.lib.reversi_solver as p\n        import spike.reversi_solver_cython as c\n        board = \'\'\'\n        ##########\n        #XXXX    #\n        #XXXX X  #\n        #XXXXXXOO#\n        #XXXXXXOO#\n        #XXXXOXOO#\n        #OXOOXOXO#\n        # OOOOOOO#\n        #OOOOOOOO#\n        ##########\'\'\'\n        b, w = parse_to_bitboards(board)\n\n        start_time = time()\n        ret = p.ReversiSolver().solve(b, w, next_player=Player.black, exactly=True)\n        print(f""{time()-start_time} sec: ret={ret}"")\n\n        start_time = time()\n        ret = c.ReversiSolver().solve(b, w, next_player=1, exactly=True)\n        print(f""{time()-start_time} sec: ret={ret}"")\n\n    def q3():\n        import reversi_zero.lib.reversi_solver as p\n        import spike.reversi_solver_cython as c\n        board = \'\'\'\n        ##########\n        #XXXXOOOX#\n        #XXXX XOX#\n        #XXXXXXOO#\n        #XXXXXOO #\n        #XXXXOXOO#\n        #OXOOXOXO#\n        # OOOOOOO#\n        #OOOOOOOO#\n        ##########\'\'\'\n        b, w = parse_to_bitboards(board)\n        start_time = time()\n        print(p.ReversiSolver().solve(b, w, next_player=Player.black, exactly=True))\n        ret = c.ReversiSolver().solve(b, w, next_player=1, exactly=True)\n        print(f""{time()-start_time} sec: ret={ret}"")\n\n    def q4():\n        import reversi_zero.lib.reversi_solver as p\n        import spike.reversi_solver_cython as c\n        board = \'\'\'\n        ##########\n        # X  XXXO#\n        #O XXXXXX#\n        #OOXOOOXX#\n        #OOOOOOXO#\n        #OOOOXOOO#\n        #OOOOXOOX#\n        # XXXOO  #\n        #   OOO  #\n        ##########\'\'\'\n        b, w = parse_to_bitboards(board)\n        start_time = time()\n        ret = p.ReversiSolver().solve(b, w, next_player=Player.black, exactly=True)\n        print(f""{time()-start_time} sec: ret={ret}"")\n\n        start_time = time()\n        ret = c.ReversiSolver().solve(b, w, next_player=1, exactly=True)\n        print(f""{time()-start_time} sec: ret={ret}"")\n\n    q4()\n\n\ndef test_bitcount():\n    import spike.bitboard_cython as c\n    import reversi_zero.lib.bitboard as p\n\n    x = 4242342758\n    assert p.bit_count(x) == c.bc_timeit(x)\n    print(timeit.timeit(""p.bit_count(x)"", number=100000, globals=locals()))\n    print(timeit.timeit(""c.bc_timeit(x)"", number=1, globals=locals()))\n\n\nif __name__ == \'__main__\':\n    # print(""find_correct_moves"")\n    # test_find_correct_move()\n    # print(""calc_flip"")\n    # test_calc_flip()\n    test_solve()\n    #test_bitcount()\n'"
test/agent/__init__.py,0,b''
test/agent/test_player.py,0,"b'from nose.tools.trivial import eq_, ok_\n\nimport numpy as np\n\n\nfrom reversi_zero.config import Config\nfrom reversi_zero.agent.player import ReversiPlayer\nfrom reversi_zero.lib.bitboard import bit_count\n\n\ndef test_add_data_to_move_buffer_with_8_symmetries():\n    config = Config()\n    player = ReversiPlayer(config, None)\n\n    """"""\n    board: p=0.2, q=0.8, O=own, X=enemy\n     01234567 - x\n    0O      q\n    1 O\n    2\n    3\n    4\n    5\n    6       X\n    7p      X\n    |\n    y\n    """"""\n\n    own = stone_bit(0, 0) | stone_bit(1, 1)\n    enemy = stone_bit(7, 6) | stone_bit(7, 7)\n    policy = np.zeros((64, ))\n    policy[idx(7, 0)] = 0.8\n    policy[idx(0, 7)] = 0.2\n    player.add_data_to_move_buffer_with_8_symmetries(own, enemy, policy)\n\n    # no transform\n    (o, e), p = player.moves[0]  # own, enemy, policy\n    eq_((bit_count(o), bit_count(e)), (2, 2))\n    ok_(check_bit(o, 0, 0))\n    ok_(check_bit(o, 1, 1))\n    ok_(check_bit(e, 7, 6))\n    ok_(check_bit(e, 7, 7))\n    eq_(p[idx(7, 0)], 0.8)\n    eq_(p[idx(0, 7)], 0.2)\n\n    # rotate right\n    (o, e), p = player.moves[1]  # own, enemy, policy\n    eq_((bit_count(o), bit_count(e)), (2, 2))\n    ok_(check_bit(o, 7, 0))\n    ok_(check_bit(o, 6, 1))\n    ok_(check_bit(e, 0, 7))\n    ok_(check_bit(e, 1, 7))\n    eq_(p[idx(7, 7)], 0.8)\n    eq_(p[idx(0, 0)], 0.2)\n\n    # rotate right twice\n    (o, e), p = player.moves[2]  # own, enemy, policy\n    eq_((bit_count(o), bit_count(e)), (2, 2))\n    ok_(check_bit(o, 7, 7))\n    ok_(check_bit(o, 6, 6))\n    ok_(check_bit(e, 0, 0))\n    ok_(check_bit(e, 0, 1))\n    eq_(p[idx(0, 7)], 0.8)\n    eq_(p[idx(7, 0)], 0.2)\n\n    # flip vertical -> rotate right\n    (o, e), p = player.moves[5]  # own, enemy, policy\n    eq_((bit_count(o), bit_count(e)), (2, 2))\n    ok_(check_bit(o, 0, 0))\n    ok_(check_bit(o, 1, 1))\n    ok_(check_bit(e, 6, 7))\n    ok_(check_bit(e, 7, 7))\n    eq_(p[idx(0, 7)], 0.8)\n    eq_(p[idx(7, 0)], 0.2)\n\n\ndef idx(x, y):\n    return y*8 + x\n\n\ndef stone_bit(x, y):\n    return 1 << idx(x, y)\n\n\ndef check_bit(bb, x, y):\n    return bb & stone_bit(x, y) != 0\n\n'"
test/lib/__init__.py,0,b''
test/lib/test_bitboard.py,0,"b'import numpy as np\n\nfrom nose.tools import assert_almost_equal\nfrom nose.tools.trivial import ok_, eq_\n\nfrom reversi_zero.lib.bitboard import find_correct_moves, board_to_string, bit_count, dirichlet_noise_of_mask, \\\n    bit_to_array\nfrom reversi_zero.lib.util import parse_to_bitboards\n\n\ndef test_find_correct_moves_1():\n    ex = \'\'\'\n##########\n#OO      #\n#XOO     #\n#OXOOO   #\n#  XOX   #\n#   XXX  #\n#  X     #\n# X      #\n#        #\n##########\'\'\'\n\n    expect = \'\'\'\n##########\n#OO      #\n#XOO     #\n#OXOOO   #\n#**XOX*  #\n# **XXX  #\n#  X**** #\n# X      #\n#        #\n##########\n\'\'\'\n    _flip_test(ex, expect)\n\n\ndef _flip_test(ex, expect, player_black=True):\n    b, w = parse_to_bitboards(ex)\n    moves = find_correct_moves(b, w) if player_black else find_correct_moves(w, b)\n    res = board_to_string(b, w, extra=moves)\n    eq_(res.strip(), expect.strip(), f""\\n{res}----{expect}"")\n\n\ndef test_find_correct_moves_2():\n    ex = \'\'\'\n##########\n#OOOOOXO #\n#OOOOOXOO#\n#OOOOOXOO#\n#OXOXOXOO#\n#OOXOXOXO#\n#OOOOOOOO#\n#XXXO   O#\n#        #\n##########\'\'\'\n\n    expect = \'\'\'\n##########\n#OOOOOXO*#\n#OOOOOXOO#\n#OOOOOXOO#\n#OXOXOXOO#\n#OOXOXOXO#\n#OOOOOOOO#\n#XXXO***O#\n#   *    #\n##########\'\'\'\n\n    _flip_test(ex, expect, player_black=False)\n\n\ndef test_find_correct_moves_3():\n    ex = \'\'\'\n##########\n#OOXXXXX #\n#XOXXXXXX#\n#XXXXXXXX#\n#XOOXXXXX#\n#OXXXOOOX#\n#OXXOOOOX#\n#OXXXOOOX#\n# OOOOOOO#\n##########\'\'\'\n\n    expect1 = \'\'\'\n##########\n#OOXXXXX #\n#XOXXXXXX#\n#XXXXXXXX#\n#XOOXXXXX#\n#OXXXOOOX#\n#OXXOOOOX#\n#OXXXOOOX#\n#*OOOOOOO#\n##########\'\'\'\n\n    expect2 = \'\'\'\n##########\n#OOXXXXX*#\n#XOXXXXXX#\n#XXXXXXXX#\n#XOOXXXXX#\n#OXXXOOOX#\n#OXXOOOOX#\n#OXXXOOOX#\n# OOOOOOO#\n##########\'\'\'\n\n    _flip_test(ex, expect1, player_black=False)\n    _flip_test(ex, expect2, player_black=True)\n\n\ndef test_dirichlet_noise_of_mask():\n    legal_moves = 47289423\n    bc = bit_count(legal_moves)\n    noise = dirichlet_noise_of_mask(legal_moves, 0.5)\n    assert_almost_equal(1, np.sum(noise))\n    eq_(bc, np.sum(noise > 0))\n    ary = bit_to_array(legal_moves, 64)\n    eq_(list(noise), list(noise * ary))\n'"
test/lib/test_ggf.py,0,"b'import numpy as np\n\nfrom nose.tools.trivial import eq_\n\nfrom reversi_zero.lib.bitboard import board_to_string\nfrom reversi_zero.lib.ggf import parse_ggf, convert_move_to_action, convert_action_to_move\nfrom reversi_zero.lib.util import parse_ggf_board_to_bitboard\n\nGGF_STR = \'(;GM[Othello]PC[NBoard]DT[2014-02-21 20:52:27 GMT]PB[./mEdax]PW[chris]RE[?]TI[15:00]TY[8]\' \\\n          \'BO[8 --*O-----------------------O*------*O--------------------------- *]\' \\\n          \'B[F5]W[F6]B[D3]W[C5]B[E6]W[F7]B[E7]W[F4];)\'\n\n\ndef test_parse_ggf():\n    ggf = parse_ggf(GGF_STR)\n    eq_(""8"", ggf.BO.board_type)\n    eq_(64, len(ggf.BO.square_cont))\n    eq_(""*"", ggf.BO.color)\n    eq_(8, len(ggf.MOVES))\n    eq_(""B"", ggf.MOVES[0].color)\n    eq_(""F5"", ggf.MOVES[0].pos)\n    eq_(""W"", ggf.MOVES[1].color)\n    eq_(""F6"", ggf.MOVES[1].pos)\n\n\ndef test_parse_ggf_board_to_bitboard():\n    ggf = parse_ggf(GGF_STR)\n    black, white = parse_ggf_board_to_bitboard(ggf.BO.square_cont)\n    eq_(EXPECTED1.strip(), board_to_string(black, white).strip())\n\n\ndef test_convert_move_to_action():\n    eq_(0, convert_move_to_action(""A1""))\n    eq_(63, convert_move_to_action(""H8""))\n    eq_(44, convert_move_to_action(""F5""))\n    eq_(None, convert_move_to_action(""PA""))\n\n\ndef test_convert_action_to_move():\n    eq_(""A1"", convert_action_to_move(0))\n    eq_(""H8"", convert_action_to_move(63))\n    eq_(""F5"", convert_action_to_move(44))\n    eq_(""PA"", convert_action_to_move(None))\n\n\nEXPECTED1 = \'\'\'\n##########\n#  OX    #\n#        #\n#        #\n#   XO   #\n#   OX   #\n#        #\n#        #\n#        #\n##########\n\'\'\''"
test/lib/test_util.py,0,"b'from nose.tools.trivial import eq_\n\nfrom reversi_zero.lib import util\nfrom reversi_zero.lib.bitboard import board_to_string\n\n\ndef test_parse_to_bitboards_init():\n    ex = \'\'\'\n    ##########\n    #        #\n    #        #\n    #        #\n    #   OX   #\n    #   XO   #\n    #        #\n    #        #\n    #        #\n    ##########\n    \'\'\'\n\n    black, white = util.parse_to_bitboards(ex)\n    eq_(black, 0b00001000 << 24 | 0b00010000 << 32, f""{ex}\\n-------\\n{board_to_string(black, white)}"")\n    eq_(white, 0b00010000 << 24 | 0b00001000 << 32, f""{ex}\\n-------\\n{board_to_string(black, white)}"")\n\n\ndef test_parse_to_bitboards():\n    ex = \'\'\'\n##########\n#OO      #\n#XOO     #\n#OXOOO   #\n#  XOX   #\n#   XXX  #\n#  X     #\n# X      #\n#       X#\n##########\'\'\'\n\n    black, white = util.parse_to_bitboards(ex)\n    eq_(ex.strip(), board_to_string(black, white).strip(), f""{ex}\\n-------\\n{board_to_string(black, white)}"")\n'"
test/worker/__init__.py,0,b''
test/worker/test_optimize.py,0,"b'from nose.tools import eq_\n\nfrom reversi_zero.config import Config\nfrom reversi_zero.worker.optimize import OptimizeWorker\n\n\ndef test_decide_learning_rate():\n    config = Config()\n    optimizer = OptimizeWorker(config)\n\n    config.trainer.lr_schedules = [\n            (0, 0.02),\n            (100000, 0.002),\n            (200000, 0.0002),\n    ]\n\n    eq_(0.02, optimizer.decide_learning_rate(100))\n    eq_(0.02, optimizer.decide_learning_rate(99999))\n    eq_(0.002, optimizer.decide_learning_rate(100001))\n    eq_(0.002, optimizer.decide_learning_rate(199999))\n    eq_(0.0002, optimizer.decide_learning_rate(200001))\n'"
src/reversi_zero/agent/__init__.py,0,b''
src/reversi_zero/agent/api.py,1,"b'import numpy as np\n\nfrom multiprocessing import Pipe, connection\nfrom threading import Thread\nfrom time import time\n\nfrom logging import getLogger\n\nfrom reversi_zero.agent.model import ReversiModel\nfrom reversi_zero.config import Config\n\nfrom reversi_zero.lib.model_helpler import reload_newest_next_generation_model_if_changed, load_best_model_weight, \\\n    save_as_best_model, reload_best_model_weight_if_changed\nimport tensorflow as tf\n\n\nlogger = getLogger(__name__)\n\n\nclass ReversiModelAPI:\n    def __init__(self, config: Config, agent_model):\n        """"""\n\n        :param config:\n        :param reversi_zero.agent.model.ReversiModel agent_model:\n        """"""\n        self.config = config\n        self.agent_model = agent_model\n\n    def predict(self, x):\n        assert x.ndim in (3, 4)\n        assert x.shape == (2, 8, 8) or x.shape[1:] == (2, 8, 8)\n        orig_x = x\n        if x.ndim == 3:\n            x = x.reshape(1, 2, 8, 8)\n\n        policy, value = self._do_predict(x)\n\n        if orig_x.ndim == 3:\n            return policy[0], value[0]\n        else:\n            return policy, value\n\n    def _do_predict(self, x):\n        return self.agent_model.model.predict_on_batch(x)\n\n\nclass MultiProcessReversiModelAPIServer:\n    # https://github.com/Akababa/Chess-Zero/blob/nohistory/src/chess_zero/agent/api_chess.py\n\n    def __init__(self, config: Config):\n        """"""\n\n        :param config:\n        """"""\n        self.config = config\n        self.model = None  # type: ReversiModel\n        self.connections = []\n\n    def get_api_client(self):\n        me, you = Pipe()\n        self.connections.append(me)\n        return MultiProcessReversiModelAPIClient(self.config, None, you)\n\n    def start_serve(self):\n        self.model = self.load_model()\n        # threading workaround: https://github.com/keras-team/keras/issues/5640\n        self.model.model._make_predict_function()\n        self.graph = tf.get_default_graph()\n\n        prediction_worker = Thread(target=self.prediction_worker, name=""prediction_worker"")\n        prediction_worker.daemon = True\n        prediction_worker.start()\n\n    def prediction_worker(self):\n        logger.debug(""prediction_worker started"")\n        average_prediction_size = []\n        last_model_check_time = time()\n        while True:\n            if last_model_check_time+60 < time():\n                self.try_reload_model()\n                last_model_check_time = time()\n                logger.debug(f""average_prediction_size={np.average(average_prediction_size)}"")\n                average_prediction_size = []\n            ready_conns = connection.wait(self.connections, timeout=0.001)  # type: list[Connection]\n            if not ready_conns:\n                continue\n            data = []\n            size_list = []\n            for conn in ready_conns:\n                x = conn.recv()\n                data.append(x)  # shape: (k, 2, 8, 8)\n                size_list.append(x.shape[0])  # save k\n            average_prediction_size.append(np.sum(size_list))\n            array = np.concatenate(data, axis=0)\n            policy_ary, value_ary = self.model.model.predict_on_batch(array)\n            idx = 0\n            for conn, s in zip(ready_conns, size_list):\n                conn.send((policy_ary[idx:idx+s], value_ary[idx:idx+s]))\n                idx += s\n\n    def load_model(self):\n        from reversi_zero.agent.model import ReversiModel\n        model = ReversiModel(self.config)\n        loaded = False\n        if not self.config.opts.new:\n            if self.config.play.use_newest_next_generation_model:\n                loaded = reload_newest_next_generation_model_if_changed(model) or load_best_model_weight(model)\n            else:\n                loaded = load_best_model_weight(model) or reload_newest_next_generation_model_if_changed(model)\n\n        if not loaded:\n            model.build()\n            save_as_best_model(model)\n        return model\n\n    def try_reload_model(self):\n        try:\n            logger.debug(""check model"")\n            if self.config.play.use_newest_next_generation_model:\n                reload_newest_next_generation_model_if_changed(self.model, clear_session=True)\n            else:\n                reload_best_model_weight_if_changed(self.model, clear_session=True)\n        except Exception as e:\n            logger.error(e)\n\n\nclass MultiProcessReversiModelAPIClient(ReversiModelAPI):\n    def __init__(self, config: Config, agent_model, conn):\n        """"""\n\n        :param config:\n        :param reversi_zero.agent.model.ReversiModel agent_model:\n        :param Connection conn:\n        """"""\n        super().__init__(config, agent_model)\n        self.connection = conn\n\n    def _do_predict(self, x):\n        self.connection.send(x)\n        return self.connection.recv()\n'"
src/reversi_zero/agent/model.py,0,"b'import hashlib\nimport json\nimport os\nfrom logging import getLogger\n# noinspection PyPep8Naming\nimport keras.backend as K\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.core import Activation, Dense, Flatten\nfrom keras.layers.merge import Add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.losses import mean_squared_error\nfrom keras.regularizers import l2\n\nfrom reversi_zero.config import Config\n\nlogger = getLogger(__name__)\n\n\nclass ReversiModel:\n    def __init__(self, config: Config):\n        self.config = config\n        self.model = None  # type: Model\n        self.digest = None\n\n    def build(self):\n        mc = self.config.model\n        in_x = x = Input((2, 8, 8))  # [own(8x8), enemy(8x8)]\n\n        # (batch, channels, height, width)\n        x = Conv2D(filters=mc.cnn_filter_num, kernel_size=mc.cnn_filter_size, padding=""same"",\n                   data_format=""channels_first"", kernel_regularizer=l2(mc.l2_reg))(x)\n        x = BatchNormalization(axis=1)(x)\n        x = Activation(""relu"")(x)\n\n        for _ in range(mc.res_layer_num):\n            x = self._build_residual_block(x)\n\n        res_out = x\n        # for policy output\n        x = Conv2D(filters=2, kernel_size=1, data_format=""channels_first"", kernel_regularizer=l2(mc.l2_reg))(res_out)\n        x = BatchNormalization(axis=1)(x)\n        x = Activation(""relu"")(x)\n        x = Flatten()(x)\n        # no output for \'pass\'\n        policy_out = Dense(8*8, kernel_regularizer=l2(mc.l2_reg), activation=""softmax"", name=""policy_out"")(x)\n\n        # for value output\n        x = Conv2D(filters=1, kernel_size=1, data_format=""channels_first"", kernel_regularizer=l2(mc.l2_reg))(res_out)\n        x = BatchNormalization(axis=1)(x)\n        x = Activation(""relu"")(x)\n        x = Flatten()(x)\n        x = Dense(mc.value_fc_size, kernel_regularizer=l2(mc.l2_reg), activation=""relu"")(x)\n        value_out = Dense(1, kernel_regularizer=l2(mc.l2_reg), activation=""tanh"", name=""value_out"")(x)\n\n        self.model = Model(in_x, [policy_out, value_out], name=""reversi_model"")\n\n    def _build_residual_block(self, x):\n        mc = self.config.model\n        in_x = x\n        x = Conv2D(filters=mc.cnn_filter_num, kernel_size=mc.cnn_filter_size, padding=""same"",\n                   data_format=""channels_first"", kernel_regularizer=l2(mc.l2_reg))(x)\n        x = BatchNormalization(axis=1)(x)\n        x = Activation(""relu"")(x)\n        x = Conv2D(filters=mc.cnn_filter_num, kernel_size=mc.cnn_filter_size, padding=""same"",\n                   data_format=""channels_first"", kernel_regularizer=l2(mc.l2_reg))(x)\n        x = BatchNormalization(axis=1)(x)\n        x = Add()([in_x, x])\n        x = Activation(""relu"")(x)\n        return x\n\n    @staticmethod\n    def fetch_digest(weight_path):\n        if os.path.exists(weight_path):\n            m = hashlib.sha256()\n            with open(weight_path, ""rb"") as f:\n                m.update(f.read())\n            return m.hexdigest()\n\n    def load(self, config_path, weight_path):\n        if os.path.exists(config_path) and os.path.exists(weight_path):\n            logger.debug(f""loading model from {config_path}"")\n            with open(config_path, ""rt"") as f:\n                self.model = Model.from_config(json.load(f))\n            self.model.load_weights(weight_path)\n            self.digest = self.fetch_digest(weight_path)\n            logger.debug(f""loaded model digest = {self.digest}"")\n            return True\n        else:\n            logger.debug(f""model files does not exist at {config_path} and {weight_path}"")\n            return False\n\n    def save(self, config_path, weight_path):\n        logger.debug(f""save model to {config_path}"")\n        with open(config_path, ""wt"") as f:\n            json.dump(self.model.get_config(), f)\n            self.model.save_weights(weight_path)\n        self.digest = self.fetch_digest(weight_path)\n        logger.debug(f""saved model digest {self.digest}"")\n\n\ndef objective_function_for_policy(y_true, y_pred):\n    # can use categorical_crossentropy??\n    return K.sum(-y_true * K.log(y_pred + K.epsilon()), axis=-1)\n\n\ndef objective_function_for_value(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred)\n'"
src/reversi_zero/agent/player.py,0,"b'from _asyncio import Future\nfrom asyncio.queues import Queue\nfrom collections import defaultdict, namedtuple\nfrom logging import getLogger\nimport asyncio\n\nimport numpy as np\nfrom numpy.random import random\n\nfrom reversi_zero.agent.api import ReversiModelAPI\nfrom reversi_zero.config import Config\nfrom reversi_zero.env.reversi_env import ReversiEnv, Player, Winner, another_player\nfrom reversi_zero.lib.bitboard import find_correct_moves, bit_to_array, flip_vertical, rotate90, dirichlet_noise_of_mask\n# from reversi_zero.lib.reversi_solver import ReversiSolver\nfrom reversi_zero.lib.alt.reversi_solver import ReversiSolver\n\n\nCounterKey = namedtuple(""CounterKey"", ""black white next_player"")\nQueueItem = namedtuple(""QueueItem"", ""state future"")\nHistoryItem = namedtuple(""HistoryItem"", ""action policy values visit enemy_values enemy_visit"")\nCallbackInMCTS = namedtuple(""CallbackInMCTS"", ""per_sim callback"")\nMCTSInfo = namedtuple(""MCTSInfo"", ""var_n var_w var_p"")\nActionWithEvaluation = namedtuple(""ActionWithEvaluation"", ""action n q"")\n\nlogger = getLogger(__name__)\n\n\nclass ReversiPlayer:\n    def __init__(self, config: Config, model, play_config=None, enable_resign=True, mtcs_info=None, api=None):\n        """"""\n\n        :param config:\n        :param reversi_zero.agent.model.ReversiModel|None model:\n        :param MCTSInfo mtcs_info:\n        :parameter ReversiModelAPI api:\n        """"""\n        self.config = config\n        self.model = model\n        self.play_config = play_config or self.config.play\n        self.enable_resign = enable_resign\n        self.api = api or ReversiModelAPI(self.config, self.model)\n\n        # key=(own, enemy, action)\n        mtcs_info = mtcs_info or self.create_mtcs_info()\n        self.var_n, self.var_w, self.var_p = mtcs_info\n\n        self.expanded = set(self.var_p.keys())\n        self.now_expanding = set()\n        self.prediction_queue = Queue(self.play_config.prediction_queue_size)\n        self.sem = asyncio.Semaphore(self.play_config.parallel_search_num)\n\n        self.moves = []\n        self.loop = asyncio.get_event_loop()\n        self.running_simulation_num = 0\n        self.callback_in_mtcs = None\n\n        self.thinking_history = {}  # for fun\n        self.resigned = False\n        self.requested_stop_thinking = False\n        self.solver = self.create_solver()\n\n    @staticmethod\n    def create_mtcs_info():\n        return MCTSInfo(defaultdict(lambda: np.zeros((64,))),\n                        defaultdict(lambda: np.zeros((64,))),\n                        defaultdict(lambda: np.zeros((64,))))\n\n    def var_q(self, key):\n        return self.var_w[key] / (self.var_n[key] + 1e-5)\n\n    def action(self, own, enemy, callback_in_mtcs=None):\n        """"""\n\n        :param own: BitBoard\n        :param enemy:  BitBoard\n        :param CallbackInMCTS callback_in_mtcs:\n        :return action=move pos=0 ~ 63 (0=top left, 7 top right, 63 bottom right)\n        """"""\n        action_with_eval = self.action_with_evaluation(own, enemy, callback_in_mtcs=callback_in_mtcs)\n        return action_with_eval.action\n\n    def action_with_evaluation(self, own, enemy, callback_in_mtcs=None):\n        """"""\n\n        :param own: BitBoard\n        :param enemy:  BitBoard\n        :param CallbackInMCTS callback_in_mtcs:\n        :rtype: ActionWithEvaluation\n        :return ActionWithEvaluation(\n                    action=move pos=0 ~ 63 (0=top left, 7 top right, 63 bottom right),\n                    n=N of the action,\n                    q=W/N of the action,\n                )\n        """"""\n        env = ReversiEnv().update(own, enemy, Player.black)\n        key = self.counter_key(env)\n        self.callback_in_mtcs = callback_in_mtcs\n        pc = self.play_config\n\n        if pc.use_solver_turn and env.turn >= pc.use_solver_turn:\n            ret = self.action_by_searching(key)\n            if ret:  # not save move as play data\n                return ret\n\n        for tl in range(self.play_config.thinking_loop):\n            if env.turn > 0:\n                self.search_moves(own, enemy)\n            else:\n                self.bypass_first_move(key)\n\n            policy = self.calc_policy(own, enemy)\n            action = int(np.random.choice(range(64), p=policy))\n            action_by_value = int(np.argmax(self.var_q(key) + (self.var_n[key] > 0)*100))\n            value_diff = self.var_q(key)[action] - self.var_q(key)[action_by_value]\n\n            if env.turn <= pc.start_rethinking_turn or self.requested_stop_thinking or \\\n                    (value_diff > -0.01 and self.var_n[key][action] >= pc.required_visit_to_decide_action):\n                break\n\n        # this is for play_gui, not necessary when training.\n        self.update_thinking_history(own, enemy, action, policy)\n\n        if self.play_config.resign_threshold is not None and\\\n                        np.max(self.var_q(key) - (self.var_n[key] == 0)*10) <= self.play_config.resign_threshold:\n            self.resigned = True\n            if self.enable_resign:\n                if env.turn >= self.config.play.allowed_resign_turn:\n                    return ActionWithEvaluation(None, 0, 0)  # means resign\n                else:\n                    logger.debug(f""Want to resign but disallowed turn {env.turn} < {self.config.play.allowed_resign_turn}"")\n\n        saved_policy = self.calc_policy_by_tau_1(key) if self.config.play_data.save_policy_of_tau_1 else policy\n        self.add_data_to_move_buffer_with_8_symmetries(own, enemy, saved_policy)\n        return ActionWithEvaluation(action=action, n=self.var_n[key][action], q=self.var_q(key)[action])\n\n    def update_thinking_history(self, black, white, action, policy):\n        key = CounterKey(black, white, Player.black.value)\n        next_key = self.get_next_key(black, white, action)\n        self.thinking_history[(black, white)] = \\\n            HistoryItem(action, policy, list(self.var_q(key)), list(self.var_n[key]),\n                        list(self.var_q(next_key)), list(self.var_n[next_key]))\n\n    def bypass_first_move(self, key):\n        legal_array = bit_to_array(find_correct_moves(key.black, key.white), 64)\n        action = np.argmax(legal_array)\n        self.var_n[key][action] = 1\n        self.var_w[key][action] = 0\n        self.var_p[key] = legal_array / np.sum(legal_array)\n\n    def action_by_searching(self, key):\n        action, score = self.solver.solve(key.black, key.white, Player(key.next_player), exactly=True)\n        if action is None:\n            return None\n        # logger.debug(f""action_by_searching: score={score}"")\n        policy = np.zeros(64)\n        policy[action] = 1\n        self.var_n[key][action] = 999\n        self.var_w[key][action] = np.sign(score) * 999\n        self.var_p[key] = policy\n        self.update_thinking_history(key.black, key.white, action, policy)\n        return ActionWithEvaluation(action=action, n=999, q=np.sign(score))\n\n    def stop_thinking(self):\n        self.requested_stop_thinking = True\n\n    def add_data_to_move_buffer_with_8_symmetries(self, own, enemy, policy):\n        for flip in [False, True]:\n            for rot_right in range(4):\n                own_saved, enemy_saved, policy_saved = own, enemy, policy.reshape((8, 8))\n                if flip:\n                    own_saved = flip_vertical(own_saved)\n                    enemy_saved = flip_vertical(enemy_saved)\n                    policy_saved = np.flipud(policy_saved)\n                if rot_right:\n                    for _ in range(rot_right):\n                        own_saved = rotate90(own_saved)\n                        enemy_saved = rotate90(enemy_saved)\n                    policy_saved = np.rot90(policy_saved, k=-rot_right)\n                self.moves.append([(own_saved, enemy_saved), list(policy_saved.reshape((64, )))])\n\n    def get_next_key(self, own, enemy, action):\n        env = ReversiEnv().update(own, enemy, Player.black)\n        env.step(action)\n        return self.counter_key(env)\n\n    def ask_thought_about(self, own, enemy) -> HistoryItem:\n        return self.thinking_history.get((own, enemy))\n\n    def search_moves(self, own, enemy):\n        loop = self.loop\n        self.running_simulation_num = 0\n        self.requested_stop_thinking = False\n\n        coroutine_list = []\n        for it in range(self.play_config.simulation_num_per_move):\n            cor = self.start_search_my_move(own, enemy)\n            coroutine_list.append(cor)\n\n        coroutine_list.append(self.prediction_worker())\n        loop.run_until_complete(asyncio.gather(*coroutine_list))\n\n    async def start_search_my_move(self, own, enemy):\n        self.running_simulation_num += 1\n        root_key = self.counter_key(ReversiEnv().update(own, enemy, Player.black))\n        with await self.sem:  # reduce parallel search number\n            if self.requested_stop_thinking:\n                self.running_simulation_num -= 1\n                return None\n            env = ReversiEnv().update(own, enemy, Player.black)\n            leaf_v = await self.search_my_move(env, is_root_node=True)\n            self.running_simulation_num -= 1\n            if self.callback_in_mtcs and self.callback_in_mtcs.per_sim > 0 and \\\n                    self.running_simulation_num % self.callback_in_mtcs.per_sim == 0:\n                self.callback_in_mtcs.callback(list(self.var_q(root_key)), list(self.var_n[root_key]))\n            return leaf_v\n\n    async def search_my_move(self, env: ReversiEnv, is_root_node=False):\n        """"""\n\n        Q, V is value for this Player(always black).\n        P is value for the player of next_player (black or white)\n        :param env:\n        :param is_root_node:\n        :return:\n        """"""\n        if env.done:\n            if env.winner == Winner.black:\n                return 1\n            elif env.winner == Winner.white:\n                return -1\n            else:\n                return 0\n\n        key = self.counter_key(env)\n        another_side_key = self.another_side_counter_key(env)\n\n        if self.config.play.use_solver_turn_in_simulation and \\\n                env.turn >= self.config.play.use_solver_turn_in_simulation:\n            action, score = self.solver.solve(key.black, key.white, Player(key.next_player), exactly=False)\n            if action:\n                score = score if env.next_player == Player.black else -score\n                leaf_v = np.sign(score)\n                leaf_p = np.zeros(64)\n                leaf_p[action] = 1\n                self.var_n[key][action] += 1\n                self.var_w[key][action] += leaf_v\n                self.var_p[key] = leaf_p\n                self.var_n[another_side_key][action] += 1\n                self.var_w[another_side_key][action] -= leaf_v\n                self.var_p[another_side_key] = leaf_p\n                return np.sign(score)\n\n        while key in self.now_expanding:\n            await asyncio.sleep(self.config.play.wait_for_expanding_sleep_sec)\n\n        # is leaf?\n        if key not in self.expanded:  # reach leaf node\n            leaf_v = await self.expand_and_evaluate(env)\n            if env.next_player == Player.black:\n                return leaf_v  # Value for black\n            else:\n                return -leaf_v  # Value for white == -Value for black\n\n        virtual_loss = self.config.play.virtual_loss\n        virtual_loss_for_w = virtual_loss if env.next_player == Player.black else -virtual_loss\n\n        action_t = self.select_action_q_and_u(env, is_root_node)\n        _, _ = env.step(action_t)\n\n        self.var_n[key][action_t] += virtual_loss\n        self.var_w[key][action_t] -= virtual_loss_for_w\n        leaf_v = await self.search_my_move(env)  # next move\n\n        # on returning search path\n        # update: N, W\n        self.var_n[key][action_t] += - virtual_loss + 1\n        self.var_w[key][action_t] += virtual_loss_for_w + leaf_v\n        # update another side info(flip color and player)\n        self.var_n[another_side_key][action_t] += 1\n        self.var_w[another_side_key][action_t] -= leaf_v  # must flip the sign.\n        return leaf_v\n\n    async def expand_and_evaluate(self, env):\n        """"""expand new leaf\n\n        update var_p, return leaf_v\n\n        :param ReversiEnv env:\n        :return: leaf_v\n        """"""\n\n        key = self.counter_key(env)\n        another_side_key = self.another_side_counter_key(env)\n        self.now_expanding.add(key)\n\n        black, white = env.board.black, env.board.white\n\n        # (di(p), v) = f\xce\xb8(di(sL))\n        # rotation and flip. flip -> rot.\n        is_flip_vertical = random() < 0.5\n        rotate_right_num = int(random() * 4)\n        if is_flip_vertical:\n            black, white = flip_vertical(black), flip_vertical(white)\n        for i in range(rotate_right_num):\n            black, white = rotate90(black), rotate90(white)  # rotate90: rotate bitboard RIGHT 1 time\n\n        black_ary = bit_to_array(black, 64).reshape((8, 8))\n        white_ary = bit_to_array(white, 64).reshape((8, 8))\n        state = [black_ary, white_ary] if env.next_player == Player.black else [white_ary, black_ary]\n        future = await self.predict(np.array(state))  # type: Future\n        await future\n        leaf_p, leaf_v = future.result()\n\n        # reverse rotate and flip about leaf_p\n        if rotate_right_num > 0 or is_flip_vertical:  # reverse rotation and flip. rot -> flip.\n            leaf_p = leaf_p.reshape((8, 8))\n            if rotate_right_num > 0:\n                leaf_p = np.rot90(leaf_p, k=rotate_right_num)  # rot90: rotate matrix LEFT k times\n            if is_flip_vertical:\n                leaf_p = np.flipud(leaf_p)\n            leaf_p = leaf_p.reshape((64, ))\n\n        self.var_p[key] = leaf_p  # P is value for next_player (black or white)\n        self.var_p[another_side_key] = leaf_p\n        self.expanded.add(key)\n        self.now_expanding.remove(key)\n        return float(leaf_v)\n\n    async def prediction_worker(self):\n        """"""For better performance, queueing prediction requests and predict together in this worker.\n\n        speed up about 45sec -> 15sec for example.\n        :return:\n        """"""\n        q = self.prediction_queue\n        margin = 10  # avoid finishing before other searches starting.\n        while self.running_simulation_num > 0 or margin > 0:\n            if q.empty():\n                if margin > 0:\n                    margin -= 1\n                await asyncio.sleep(self.config.play.prediction_worker_sleep_sec)\n                continue\n            item_list = [q.get_nowait() for _ in range(q.qsize())]  # type: list[QueueItem]\n            #logger.debug(f""predicting {len(item_list)} items"")\n            data = np.array([x.state for x in item_list])\n            policy_ary, value_ary = self.api.predict(data)  # shape=(N, 2, 8, 8)\n            #logger.debug(f""predicted {len(item_list)} items"")\n            for p, v, item in zip(policy_ary, value_ary, item_list):\n                item.future.set_result((p, v))\n\n    async def predict(self, x):\n        future = self.loop.create_future()\n        item = QueueItem(x, future)\n        await self.prediction_queue.put(item)\n        return future\n\n    def finish_game(self, z):\n        """"""\n\n        :param z: win=1, lose=-1, draw=0\n        :return:\n        """"""\n        for move in self.moves:  # add this game winner result to all past moves.\n            move += [z]\n\n    def calc_policy(self, own, enemy):\n        """"""calc \xcf\x80(a|s0)\n\n        :param own:\n        :param enemy:\n        :return:\n        """"""\n        pc = self.play_config\n        env = ReversiEnv().update(own, enemy, Player.black)\n        key = self.counter_key(env)\n        if env.turn < pc.change_tau_turn:\n            return self.calc_policy_by_tau_1(key)\n        else:\n            action = np.argmax(self.var_n[key])  # tau = 0\n            ret = np.zeros(64)\n            ret[action] = 1\n            return ret\n\n    def calc_policy_by_tau_1(self, key):\n        return self.var_n[key] / np.sum(self.var_n[key])  # tau = 1\n\n    @staticmethod\n    def counter_key(env: ReversiEnv):\n        return CounterKey(env.board.black, env.board.white, env.next_player.value)\n\n    @staticmethod\n    def another_side_counter_key(env: ReversiEnv):\n        return CounterKey(env.board.white, env.board.black, another_player(env.next_player).value)\n\n    def select_action_q_and_u(self, env, is_root_node):\n        key = self.counter_key(env)\n        if env.next_player == Player.black:\n            legal_moves = find_correct_moves(key.black, key.white)\n        else:\n            legal_moves = find_correct_moves(key.white, key.black)\n        # noinspection PyUnresolvedReferences\n        xx_ = np.sqrt(np.sum(self.var_n[key]))  # SQRT of sum(N(s, b); for all b)\n        xx_ = max(xx_, 1)  # avoid u_=0 if N is all 0\n        p_ = self.var_p[key]\n\n        # re-normalize in legal moves\n        p_ = p_ * bit_to_array(legal_moves, 64)\n        if np.sum(p_) > 0:\n            # decay policy gradually in the end phase\n            _pc = self.config.play\n            temperature = min(np.exp(1-np.power(env.turn/_pc.policy_decay_turn, _pc.policy_decay_power)), 1)\n            # normalize and decay policy\n            p_ = self.normalize(p_, temperature)\n\n        if is_root_node and self.play_config.noise_eps > 0:  # Is it correct?? -> (1-e)p + e*Dir(alpha)\n            noise = dirichlet_noise_of_mask(legal_moves, self.play_config.dirichlet_alpha)\n            p_ = (1 - self.play_config.noise_eps) * p_ + self.play_config.noise_eps * noise\n\n        u_ = self.play_config.c_puct * p_ * xx_ / (1 + self.var_n[key])\n        if env.next_player == Player.black:\n            v_ = (self.var_q(key) + u_ + 1000) * bit_to_array(legal_moves, 64)\n        else:\n            # When enemy\'s selecting action, flip Q-Value.\n            v_ = (-self.var_q(key) + u_ + 1000) * bit_to_array(legal_moves, 64)\n\n        # noinspection PyTypeChecker\n        action_t = int(np.argmax(v_))\n        return action_t\n\n    @staticmethod\n    def normalize(p, t=1):\n        pp = np.power(p, t)\n        return pp / np.sum(pp)\n\n    def create_solver(self):\n        return ReversiSolver()\n\n'"
src/reversi_zero/env/__init__.py,0,b''
src/reversi_zero/env/reversi_env.py,0,"b'import enum\n\nfrom logging import getLogger\n\nfrom reversi_zero.lib.bitboard import board_to_string, calc_flip, bit_count, find_correct_moves\n\nlogger = getLogger(__name__)\n# noinspection PyArgumentList\nPlayer = enum.Enum(""Player"", ""black white"")\n# noinspection PyArgumentList\nWinner = enum.Enum(""Winner"", ""black white draw"")\n\n\ndef another_player(player: Player):\n    return Player.white if player == Player.black else Player.black\n\n\nclass ReversiEnv:\n    def __init__(self):\n        self.board = None\n        self.next_player = None  # type: Player\n        self.turn = 0\n        self.done = False\n        self.winner = None  # type: Winner\n\n    def reset(self):\n        self.board = Board()\n        self.next_player = Player.black\n        self.turn = 0\n        self.done = False\n        self.winner = None\n        return self\n\n    def update(self, black, white, next_player):\n        self.board = Board(black, white)\n        self.next_player = next_player\n        self.turn = sum(self.board.number_of_black_and_white) - 4\n        self.done = False\n        self.winner = None\n        return self\n\n    def step(self, action):\n        """"""\n\n        :param int|None action: move pos=0 ~ 63 (0=top left, 7 top right, 63 bottom right), None is resign\n        :return:\n        """"""\n        assert action is None or 0 <= action <= 63, f""Illegal action={action}""\n\n        if action is None:\n            self._resigned()\n            return self.board, {}\n\n        own, enemy = self.get_own_and_enemy()\n\n        flipped = calc_flip(action, own, enemy)\n        if bit_count(flipped) == 0:\n            self.illegal_move_to_lose(action)\n            return self.board, {}\n        own ^= flipped\n        own |= 1 << action\n        enemy ^= flipped\n\n        self.set_own_and_enemy(own, enemy)\n        self.turn += 1\n\n        if bit_count(find_correct_moves(enemy, own)) > 0:  # there are legal moves for enemy.\n            self.change_to_next_player()\n        elif bit_count(find_correct_moves(own, enemy)) > 0:  # there are legal moves for me but enemy.\n            pass\n        else:  # there is no legal moves for me and enemy.\n            self._game_over()\n\n        return self.board, {}\n\n    def _game_over(self):\n        self.done = True\n        if self.winner is None:\n            black_num, white_num = self.board.number_of_black_and_white\n            if black_num > white_num:\n                self.winner = Winner.black\n            elif black_num < white_num:\n                self.winner = Winner.white\n            else:\n                self.winner = Winner.draw\n\n    def change_to_next_player(self):\n        self.next_player = another_player(self.next_player)\n\n    def illegal_move_to_lose(self, action):\n        logger.warning(f""Illegal action={action}, No Flipped!"")\n        self._win_another_player()\n        self._game_over()\n\n    def _resigned(self):\n        self._win_another_player()\n        self._game_over()\n\n    def _win_another_player(self):\n        win_player = another_player(self.next_player)  # type: Player\n        if win_player == Player.black:\n            self.winner = Winner.black\n        else:\n            self.winner = Winner.white\n\n    def get_own_and_enemy(self):\n        if self.next_player == Player.black:\n            own, enemy = self.board.black, self.board.white\n        else:\n            own, enemy = self.board.white, self.board.black\n        return own, enemy\n\n    def set_own_and_enemy(self, own, enemy):\n        if self.next_player == Player.black:\n            self.board.black, self.board.white = own, enemy\n        else:\n            self.board.white, self.board.black = own, enemy\n\n    def render(self):\n        b, w = self.board.number_of_black_and_white\n        print(f""next={self.next_player.name} turn={self.turn} B={b} W={w}"")\n        print(board_to_string(self.board.black, self.board.white, with_edge=True))\n\n    @property\n    def observation(self):\n        """"""\n\n        :rtype: Board\n        """"""\n        return self.board\n\n\nclass Board:\n    def __init__(self, black=None, white=None, init_type=0):\n        self.black = black or (0b00010000 << 24 | 0b00001000 << 32)\n        self.white = white or (0b00001000 << 24 | 0b00010000 << 32)\n\n        if init_type:\n            self.black, self.white = self.white, self.black\n\n    @property\n    def number_of_black_and_white(self):\n        return bit_count(self.black), bit_count(self.white)\n\n'"
src/reversi_zero/lib/__init__.py,0,b''
src/reversi_zero/lib/bitboard.py,0,"b'# http://primenumber.hatenadiary.jp/entry/2016/12/26/063226\nimport numpy as np\n\nBLACK_CHR = ""O""\nWHITE_CHR = ""X""\nEXTRA_CHR = ""*""\n\n\ndef board_to_string(black, white, with_edge=True, extra=None):\n    """"""\n     0  1  2  3  4  5  6  7\n     8  9 10 11 12 13 14 15\n    ..\n    56 57 58 59 60 61 62 63\n\n    0: Top Left, LSB\n    63: Bottom Right\n\n    :param black: bitboard\n    :param white: bitboard\n    :param with_edge:\n    :param extra: bitboard\n    :return:\n    """"""\n    array = ["" ""] * 64\n    extra = extra or 0\n    for i in range(64):\n        if black & 1:\n            array[i] = BLACK_CHR\n        elif white & 1:\n            array[i] = WHITE_CHR\n        elif extra & 1:\n            array[i] = EXTRA_CHR\n        black >>= 1\n        white >>= 1\n        extra >>= 1\n\n    ret = """"\n    if with_edge:\n        ret = ""#"" * 10 + ""\\n""\n    for y in range(8):\n        if with_edge:\n            ret += ""#""\n        ret += """".join(array[y * 8:y * 8 + 8])\n        if with_edge:\n            ret += ""#""\n        ret += ""\\n""\n    if with_edge:\n        ret += ""#"" * 10 + ""\\n""\n    return ret\n\n\ndef find_correct_moves(own, enemy):\n    """"""return legal moves""""""\n    left_right_mask = 0x7e7e7e7e7e7e7e7e  # Both most left-right edge are 0, else 1\n    top_bottom_mask = 0x00ffffffffffff00  # Both most top-bottom edge are 0, else 1\n    mask = left_right_mask & top_bottom_mask\n    mobility = 0\n    mobility |= search_offset_left(own, enemy, left_right_mask, 1)  # Left\n    mobility |= search_offset_left(own, enemy, mask, 9)  # Left Top\n    mobility |= search_offset_left(own, enemy, top_bottom_mask, 8)  # Top\n    mobility |= search_offset_left(own, enemy, mask, 7)  # Top Right\n    mobility |= search_offset_right(own, enemy, left_right_mask, 1)  # Right\n    mobility |= search_offset_right(own, enemy, mask, 9)  # Bottom Right\n    mobility |= search_offset_right(own, enemy, top_bottom_mask, 8)  # Bottom\n    mobility |= search_offset_right(own, enemy, mask, 7)  # Left bottom\n    return mobility\n\n\ndef calc_flip(pos, own, enemy):\n    """"""return flip stones of enemy by bitboard when I place stone at pos.\n\n    :param pos: 0~63\n    :param own: bitboard (0=top left, 63=bottom right)\n    :param enemy: bitboard\n    :return: flip stones of enemy when I place stone at pos.\n    """"""\n    assert 0 <= pos <= 63, f""pos={pos}""\n    f1 = _calc_flip_half(pos, own, enemy)\n    f2 = _calc_flip_half(63 - pos, rotate180(own), rotate180(enemy))\n    return f1 | rotate180(f2)\n\n\ndef _calc_flip_half(pos, own, enemy):\n    el = [enemy, enemy & 0x7e7e7e7e7e7e7e7e, enemy & 0x7e7e7e7e7e7e7e7e, enemy & 0x7e7e7e7e7e7e7e7e]\n    masks = [0x0101010101010100, 0x00000000000000fe, 0x0002040810204080, 0x8040201008040200]\n    masks = [b64(m << pos) for m in masks]\n    flipped = 0\n    for e, mask in zip(el, masks):\n        outflank = mask & ((e | ~mask) + 1) & own\n        flipped |= (outflank - (outflank != 0)) & mask\n    return flipped\n\n\ndef search_offset_left(own, enemy, mask, offset):\n    e = enemy & mask\n    blank = ~(own | enemy)\n    t = e & (own >> offset)\n    t |= e & (t >> offset)\n    t |= e & (t >> offset)\n    t |= e & (t >> offset)\n    t |= e & (t >> offset)\n    t |= e & (t >> offset)  # Up to six stones can be turned at once\n    return blank & (t >> offset)  # Only the blank squares can be started\n\n\ndef search_offset_right(own, enemy, mask, offset):\n    e = enemy & mask\n    blank = ~(own | enemy)\n    t = e & (own << offset)\n    t |= e & (t << offset)\n    t |= e & (t << offset)\n    t |= e & (t << offset)\n    t |= e & (t << offset)\n    t |= e & (t << offset)  # Up to six stones can be turned at once\n    return blank & (t << offset)  # Only the blank squares can be started\n\n\ndef flip_vertical(x):\n    k1 = 0x00FF00FF00FF00FF\n    k2 = 0x0000FFFF0000FFFF\n    x = ((x >> 8) & k1) | ((x & k1) << 8)\n    x = ((x >> 16) & k2) | ((x & k2) << 16)\n    x = (x >> 32) | b64(x << 32)\n    return x\n\n\ndef b64(x):\n    return x & 0xFFFFFFFFFFFFFFFF\n\n\ndef bit_count(x):\n    return bin(x).count(\'1\')\n\n\ndef bit_to_array(x, size):\n    """"""bit_to_array(0b0010, 4) -> array([0, 1, 0, 0])""""""\n    return np.array(list(reversed(((""0"" * size) + bin(x)[2:])[-size:])), dtype=np.uint8)\n\n\ndef flip_diag_a1h8(x):\n    k1 = 0x5500550055005500\n    k2 = 0x3333000033330000\n    k4 = 0x0f0f0f0f00000000\n    t = k4 & (x ^ b64(x << 28))\n    x ^= t ^ (t >> 28)\n    t = k2 & (x ^ b64(x << 14))\n    x ^= t ^ (t >> 14)\n    t = k1 & (x ^ b64(x << 7))\n    x ^= t ^ (t >> 7)\n    return x\n\n\ndef rotate90(x):\n    return flip_diag_a1h8(flip_vertical(x))\n\n\ndef rotate180(x):\n    return rotate90(rotate90(x))\n\n\ndef dirichlet_noise_of_mask(mask, alpha):\n    num_1 = bit_count(mask)\n    noise = list(np.random.dirichlet([alpha] * num_1))\n    ret_list = []\n    for i in range(64):\n        if (1 << i) & mask:\n            ret_list.append(noise.pop(0))\n        else:\n            ret_list.append(0)\n    return np.array(ret_list)\n'"
src/reversi_zero/lib/data_helper.py,0,"b'import json\nimport os\nfrom glob import glob\nfrom logging import getLogger\n\nfrom reversi_zero.config import ResourceConfig\n\nlogger = getLogger(__name__)\n\n\ndef get_game_data_filenames(rc: ResourceConfig):\n    pattern = os.path.join(rc.play_data_dir, rc.play_data_filename_tmpl % ""*"")\n    files = list(sorted(glob(pattern)))\n    return files\n\n\ndef get_next_generation_model_dirs(rc: ResourceConfig):\n    dir_pattern = os.path.join(rc.next_generation_model_dir, rc.next_generation_model_dirname_tmpl % ""*"")\n    dirs = list(sorted(glob(dir_pattern)))\n    return dirs\n\n\ndef write_game_data_to_file(path, data):\n    with open(path, ""wt"") as f:\n        json.dump(data, f)\n\n\ndef read_game_data_from_file(path):\n    with open(path, ""rt"") as f:\n        return json.load(f)\n'"
src/reversi_zero/lib/file_util.py,0,"b'import os\n\n\ndef read_as_int(filename):\n    if os.path.exists(filename):\n        try:\n            with open(filename, ""rt"") as f:\n                ret = int(str(f.read()).strip())\n                if ret:\n                    return ret\n        except ValueError:\n            pass\n'"
src/reversi_zero/lib/ggf.py,0,"b'import re\nfrom collections import namedtuple\n\nfrom datetime import datetime\n\nfrom reversi_zero.lib.util import parse_ggf_board_to_bitboard\n\nGGF = namedtuple(""GGF"", ""BO MOVES"")\nBO = namedtuple(""BO"", ""board_type, square_cont, color"")  # color: {O, *}  (O is white, * is black)\nMOVE = namedtuple(""MOVE"", ""color pos"")  # color={B, W} pos: like \'F5\'\n\n\ndef parse_ggf(ggf):\n    """"""https://skatgame.net/mburo/ggsa/ggf\n\n    :param ggf:\n    :rtype: GGF\n    """"""\n    tokens = re.split(r\'([a-zA-Z]+\\[[^\\]]+\\])\', ggf)\n    moves = []\n    bo = None\n    for token in tokens:\n        match = re.search(r\'([a-zA-Z]+)\\[([^\\]]+)\\]\', token)\n        if not match:\n            continue\n        key, value = re.search(r\'([a-zA-Z]+)\\[([^\\]]+)\\]\', token).groups()\n        key = key.upper()\n        if key == ""BO"":\n            bo = BO(*value.split("" ""))\n        elif key in (""B"", ""W""):\n            moves.append(MOVE(key, value))\n    return GGF(bo, moves)\n\n\ndef convert_move_to_action(move_str: str):\n    """"""\n\n    :param move_str: A1 -> 0, H8 -> 63\n    :return:\n    """"""\n    if move_str[:2].lower() == ""pa"":\n        return None\n    pos = move_str.lower()\n    y = ord(pos[0]) - ord(""a"")\n    x = int(pos[1]) - 1\n    return y * 8 + x\n\n\ndef convert_action_to_move(action):\n    """"""\n\n    :param int|None action:\n    :return:\n    """"""\n    if action is None:\n        return ""PA""\n    y = action // 8\n    x = action % 8\n    return chr(ord(""A"") + y) + str(x + 1)\n\n\ndef convert_to_bitboard_and_actions(ggf: GGF):\n    black, white = parse_ggf_board_to_bitboard(ggf.BO.square_cont)\n    actions = []\n    for move in ggf.MOVES:  # type: MOVE\n        actions.append(convert_move_to_action(move.pos))\n    return black, white, actions\n\n\ndef make_ggf_string(black_name=None, white_name=None, dt=None, moves=None, result=None, think_time_sec=60):\n    """"""\n\n    :param str black_name:\n    :param str white_name:\n    :param datetime|None dt:\n    :param str|None result:\n    :param list[str] moves:\n    :param int think_time_sec:\n    :return:\n    """"""\n    ggf = \'(;GM[Othello]PC[RAZSelf]DT[%(datetime)s]PB[%(black_name)s]PW[%(white_name)s]RE[%(result)s]TI[%(time)s]\' \\\n          \'TY[8]BO[8 ---------------------------O*------*O--------------------------- *]%(move_list)s;)\'\n    dt = dt or datetime.utcnow()\n\n    move_list = []\n    for i, move in enumerate(moves or []):\n        if i % 2 == 0:\n            move_list.append(f""B[{move}]"")\n        else:\n            move_list.append(f""W[{move}]"")\n\n    params = dict(\n        black_name=black_name or ""black"",\n        white_name=white_name or ""white"",\n        result=result or \'?\',\n        datetime=dt.strftime(""%Y.%m.%d_%H:%M:%S.%Z""),\n        time=f""{think_time_sec // 60}:{think_time_sec % 60}"",\n        move_list="""".join(move_list),\n    )\n    return ggf % params\n'"
src/reversi_zero/lib/logger.py,0,"b'from logging import StreamHandler, basicConfig, DEBUG, getLogger, Formatter\n\n\ndef setup_logger(log_filename):\n    format_str = \'%(asctime)s@%(name)s %(levelname)s # %(message)s\'\n    basicConfig(filename=log_filename, level=DEBUG, format=format_str)\n    stream_handler = StreamHandler()\n    stream_handler.setFormatter(Formatter(format_str))\n    getLogger().addHandler(stream_handler)\n\n\nif __name__ == \'__main__\':\n    setup_logger(""aa.log"")\n    logger = getLogger(""test"")\n    logger.info(""OK"")\n'"
src/reversi_zero/lib/model_helpler.py,0,"b'import os\nfrom logging import getLogger\nfrom time import sleep\n\nimport keras.backend as K\n\n\nlogger = getLogger(__name__)\n\n\ndef load_best_model_weight(model, clear_session=False):\n    """"""\n\n    :param reversi_zero.agent.model.ReversiModel model:\n    :param bool clear_session:\n    :return:\n    """"""\n    if clear_session:\n        K.clear_session()\n    return model.load(model.config.resource.model_best_config_path, model.config.resource.model_best_weight_path)\n\n\ndef save_as_best_model(model):\n    """"""\n\n    :param reversi_zero.agent.model.ReversiModel model:\n    :return:\n    """"""\n    return model.save(model.config.resource.model_best_config_path, model.config.resource.model_best_weight_path)\n\n\ndef reload_best_model_weight_if_changed(model, clear_session=False):\n    """"""\n\n    :param reversi_zero.agent.model.ReversiModel model:\n    :param bool clear_session:\n    :return:\n    """"""\n    logger.debug(f""start reload the best model if changed"")\n    digest = model.fetch_digest(model.config.resource.model_best_weight_path)\n    if digest != model.digest:\n        return load_best_model_weight(model, clear_session=clear_session)\n\n    logger.debug(f""the best model is not changed"")\n    return False\n\n\ndef reload_newest_next_generation_model_if_changed(model, clear_session=False):\n    """"""\n\n    :param reversi_zero.agent.model.ReversiModel model:\n    :param bool clear_session:\n    :return:\n    """"""\n    from reversi_zero.lib.data_helper import get_next_generation_model_dirs\n\n    rc = model.config.resource\n    dirs = get_next_generation_model_dirs(rc)\n    if not dirs:\n        logger.debug(""No next generation model exists."")\n        return False\n    model_dir = dirs[-1]\n    config_path = os.path.join(model_dir, rc.next_generation_model_config_filename)\n    weight_path = os.path.join(model_dir, rc.next_generation_model_weight_filename)\n    digest = model.fetch_digest(weight_path)\n    if digest and digest != model.digest:\n        logger.debug(f""Loading weight from {model_dir}"")\n        if clear_session:\n            K.clear_session()\n        for _ in range(5):\n            try:\n                return model.load(config_path, weight_path)\n            except Exception as e:\n                logger.warning(f""error in load model: #{e}"")\n                sleep(3)\n        raise RuntimeError(""Cannot Load Model!"")\n\n    else:\n        logger.debug(f""The newest model is not changed: digest={digest}"")\n        return False\n'"
src/reversi_zero/lib/nonblocking_stream_reader.py,0,"b'# idea from http://eyalarubas.com/python-subproc-nonblock.html\nfrom queue import Queue, Empty\nfrom threading import Thread\n\nfrom logging import getLogger\nlogger = getLogger(__name__)\n\n\nclass NonBlockingStreamReader:\n    def __init__(self, stream):\n        self._stream = stream\n        self._queue = Queue()\n        self._thread = None\n        self.closed = True\n\n    def start(self, push_callback=None):\n        def _worker():\n            while True:\n                line = self._stream.readline()\n                if line:\n                    if push_callback:\n                        push_callback(line)\n                    self._queue.put(line)\n                else:\n                    logger.debug(""the stream may be closed"")\n                    break\n            self.closed = True\n\n        self._thread = Thread(target=_worker)\n        self._thread.setDaemon(True)\n        self._thread.setName(""NonBlockingStreamReader of %s"" % repr(self._stream))\n        self.closed = False\n        self._thread.start()\n\n    def readline(self, timeout=None):\n        try:\n            return self._queue.get(block=timeout is not None, timeout=timeout)\n        except Empty:\n            return None\n'"
src/reversi_zero/lib/reversi_solver.py,0,"b'from time import time\n\nfrom logging import getLogger\n\nfrom reversi_zero.env.reversi_env import ReversiEnv, Player\nfrom reversi_zero.lib.bitboard import find_correct_moves\nimport numpy as np\n\n\nlogger = getLogger(__name__)\n\n\nclass Timeout(Exception):\n    pass\n\n\nclass ReversiSolver:\n    """"""calculate which is winner. Not estimation by NN!\n\n    this implementation runs very slow. (^^;\n    """"""\n    def __init__(self):\n        self.cache = {}\n        self.start_time = None\n        self.timeout = None\n        self.last_is_exactly = False\n\n    def solve(self, black, white, next_player, timeout=30, exactly=False):\n        self.timeout = timeout\n        self.start_time = time()\n        if not self.last_is_exactly and exactly:\n            self.cache = {}\n        self.last_is_exactly = exactly\n        \n        try:\n            # logger.debug(""start resolving"")\n            move, score = self.find_winning_move_and_score(ReversiEnv().update(black, white, next_player),\n                                                           exactly=exactly)\n            if next_player == Player.white:\n                score = -score\n            # logger.debug(f""solve answer=({move},{score})({time()-self.start_time:.3f} seconds)"")\n            return move, score\n        except Timeout:\n            return None, None\n\n    def find_winning_move_and_score(self, env: ReversiEnv, exactly=True):\n        if env.done:\n            b, w = env.board.number_of_black_and_white\n            return None, b - w\n        if time() - self.start_time > self.timeout:\n            logger.debug(""timeout!"")\n            raise Timeout()\n\n        turn = env.turn\n        key = black, white, next_player = env.board.black, env.board.white, env.next_player\n        if key in self.cache:\n            return self.cache[key]\n\n        if next_player == Player.black:\n            legal_moves = find_correct_moves(black, white)\n        else:\n            legal_moves = find_correct_moves(white, black)\n\n        action_list = [idx for idx in range(64) if legal_moves & (1 << idx)]\n        score_list = np.zeros(len(action_list), dtype=int)\n        for i, action in enumerate(action_list):\n            # env.update(black, white, next_player)\n            env.board.black = black\n            env.board.white = white\n            env.next_player = next_player\n            env.turn = turn\n            env.done = False\n            env.winner = None\n            #\n            env.step(action)\n            _, score = self.find_winning_move_and_score(env, exactly=exactly)\n            score_list[i] = score\n\n            if not exactly:\n                # do not need to find the best score move\n                if next_player == Player.black and score > 0:\n                    break\n                elif next_player == Player.white and score < 0:\n                    break\n\n        # print(list(zip(action_list, score_list)))\n\n        if next_player == Player.black:\n            best_action = action_list[int(np.argmax(score_list))]\n            best_score = np.max(score_list)\n        else:\n            best_action = action_list[int(np.argmin(score_list))]\n            best_score = np.min(score_list)\n\n        self.cache[key] = (best_action, best_score)\n        return best_action, best_score\n\n\nif __name__ == \'__main__\':\n    from reversi_zero.lib.util import parse_to_bitboards\n\n    def q1():\n        board = \'\'\'\n        ##########\n        #XXXX    #\n        #XOXX    #\n        #XOXXOOOO#\n        #XOXOXOOO#\n        #XOXXOXOO#\n        #OOOOXOXO#\n        # OOOOOOO#\n        #  XXXXXO#\n        ##########\'\'\'\n        b, w = parse_to_bitboards(board)\n        rr = ReversiSolver()\n        print(""correct is (57, +2)"")\n        print(rr.solve(b, w, Player.white, exactly=False))\n        print(len(rr.cache))\n\n    def q2():\n        board = \'\'\'\n        ##########\n        #XXXX    #\n        #XXXX X  #\n        #XXXXXXOO#\n        #XXXXXXOO#\n        #XXXXOXOO#\n        #OXOOXOXO#\n        # OOOOOOO#\n        #OOOOOOOO#\n        ##########\'\'\'\n        b, w = parse_to_bitboards(board)\n        rr = ReversiSolver()\n        print(""correct is (4 or 14, -2)"")\n        print(rr.solve(b, w, Player.black, exactly=False))\n        print(len(rr.cache))\n\n    def q3():  # O: black, X: white\n        board = \'\'\'\n        ##########\n        #  X OOO #\n        #X XOXO O#\n        #XXXXOXOO#\n        #XOXOOXXO#\n        #XOOOOXXO#\n        #XOOOXXXO#\n        # OOOOXX #\n        #  OOOOX #\n        ##########\'\'\'\n        b, w = parse_to_bitboards(board)\n        rr = ReversiSolver()\n        print(""correct is (3, +2)"")\n        print(rr.solve(b, w, Player.white, exactly=True))\n        print(len(rr.cache))\n\n    q3()\n\n'"
src/reversi_zero/lib/tensorboard_logger.py,2,"b'import tensorflow as tf\n\n\nclass TensorBoardLogger:\n    def __init__(self, log_dir, filename_suffix=None):\n        self.writer = tf.summary.FileWriter(log_dir, filename_suffix=filename_suffix)\n\n    def log_scaler(self, info: dict, step):\n        """"""\n\n        :param dict info: dict of {<tag>: <value>}\n        :param int step:\n        :return:\n        """"""\n        for tag, value in info.items():\n            summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n            self.writer.add_summary(summary, step)\n        self.writer.flush()\n\n'"
src/reversi_zero/lib/tensorboard_step_callback.py,2,"b'from keras.callbacks import Callback\nimport tensorflow as tf\n\n\nclass TensorBoardStepCallback(Callback):\n    """"""Tensorboard basic visualizations by step.\n\n    """"""\n\n    def __init__(self, log_dir, logging_per_steps=100, step=0):\n        super().__init__()\n        self.step = step\n        self.logging_per_steps = logging_per_steps\n        self.writer = tf.summary.FileWriter(log_dir)\n\n    def on_batch_end(self, batch, logs=None):\n        self.step += 1\n\n        if self.step % self.logging_per_steps > 0:\n            return\n\n        for name, value in logs.items():\n            if name in [\'batch\', \'size\']:\n                continue\n            summary = tf.Summary()\n            summary_value = summary.value.add()\n            summary_value.simple_value = value.item()\n            summary_value.tag = name\n            self.writer.add_summary(summary, self.step)\n        self.writer.flush()\n\n    def close(self):\n        self.writer.close()\n'"
src/reversi_zero/lib/tf_util.py,3,"b'def set_session_config(per_process_gpu_memory_fraction=None, allow_growth=None):\n    """"""\n\n    :param allow_growth: When necessary, reserve memory\n    :param float per_process_gpu_memory_fraction: specify GPU memory usage as 0 to 1\n\n    :return:\n    """"""\n    import tensorflow as tf\n    import keras.backend as K\n\n    config = tf.ConfigProto(\n        gpu_options=tf.GPUOptions(\n            per_process_gpu_memory_fraction=per_process_gpu_memory_fraction,\n            allow_growth=allow_growth,\n        )\n    )\n    sess = tf.Session(config=config)\n    K.set_session(sess)\n'"
src/reversi_zero/lib/util.py,0,"b'def parse_to_bitboards(string: str):\n    lines = string.strip().split(""\\n"")\n    black = 0\n    white = 0\n    y = 0\n\n    for line in [l.strip() for l in lines]:\n        if line[:2] == \'##\':\n            continue\n        for i, ch in enumerate(line[1:9]):\n            if ch == \'O\':\n                black |= 1 << (y*8+i)\n            elif ch == \'X\':\n                white |= 1 << (y*8+i)\n        y += 1\n\n    return black, white\n\n\ndef parse_ggf_board_to_bitboard(string: str):\n    white = black = 0\n    for i, ch in enumerate(string):\n        if ch == ""*"":\n            black |= 1 << i\n        elif ch == ""O"":\n            white |= 1 << i\n    return black, white\n'"
src/reversi_zero/play_game/__init__.py,0,b''
src/reversi_zero/play_game/common.py,0,"b'from reversi_zero.config import Config\nfrom reversi_zero.lib.model_helpler import reload_newest_next_generation_model_if_changed, load_best_model_weight\n\n\ndef load_model(config: Config):\n    from reversi_zero.agent.model import ReversiModel\n    model = ReversiModel(config)\n    if config.play.use_newest_next_generation_model:\n        loaded = reload_newest_next_generation_model_if_changed(model) or load_best_model_weight(model)\n    else:\n        loaded = load_best_model_weight(model) or reload_newest_next_generation_model_if_changed(model)\n    if not loaded:\n        raise RuntimeError(""No models found!"")\n    return model\n'"
src/reversi_zero/play_game/game_model.py,0,"b'import enum\nfrom logging import getLogger\n\nfrom reversi_zero.agent.player import HistoryItem\nfrom reversi_zero.agent.player import ReversiPlayer\nfrom reversi_zero.config import Config\nfrom reversi_zero.env.reversi_env import Player, ReversiEnv\nfrom reversi_zero.lib.bitboard import find_correct_moves\nfrom reversi_zero.lib.model_helpler import load_best_model_weight, reload_newest_next_generation_model_if_changed\nfrom reversi_zero.play_game.common import load_model\n\nlogger = getLogger(__name__)\n\nGameEvent = enum.Enum(""GameEvent"", ""update ai_move over pass"")\n\n\nclass PlayWithHuman:\n    def __init__(self, config: Config):\n        self.config = config\n        self.human_color = None\n        self.observers = []\n        self.env = ReversiEnv().reset()\n        self.model = self._load_model()\n        self.ai = None  # type: ReversiPlayer\n        self.last_evaluation = None\n        self.last_history = None  # type: HistoryItem\n\n    def add_observer(self, observer_func):\n        self.observers.append(observer_func)\n\n    def notify_all(self, event):\n        for ob_func in self.observers:\n            ob_func(event)\n\n    def start_game(self, human_is_black):\n        self.human_color = Player.black if human_is_black else Player.white\n        self.env = ReversiEnv().reset()\n        self.ai = ReversiPlayer(self.config, self.model)\n\n    def play_next_turn(self):\n        self.notify_all(GameEvent.update)\n\n        if self.over:\n            self.notify_all(GameEvent.over)\n            return\n\n        if self.next_player != self.human_color:\n            self.notify_all(GameEvent.ai_move)\n\n    @property\n    def over(self):\n        return self.env.done\n\n    @property\n    def next_player(self):\n        return self.env.next_player\n\n    def stone(self, px, py):\n        """"""left top=(0, 0), right bottom=(7,7)""""""\n        pos = int(py * 8 + px)\n        assert 0 <= pos < 64\n        bit = 1 << pos\n        if self.env.board.black & bit:\n            return Player.black\n        elif self.env.board.white & bit:\n            return Player.white\n        return None\n\n    @property\n    def number_of_black_and_white(self):\n        return self.env.observation.number_of_black_and_white\n\n    def available(self, px, py):\n        pos = int(py * 8 + px)\n        if pos < 0 or 64 <= pos:\n            return False\n        own, enemy = self.env.board.black, self.env.board.white\n        if self.human_color == Player.white:\n            own, enemy = enemy, own\n        legal_moves = find_correct_moves(own, enemy)\n        return legal_moves & (1 << pos)\n\n    def move(self, px, py):\n        pos = int(py * 8 + px)\n        assert 0 <= pos < 64\n\n        if self.next_player != self.human_color:\n            return False\n\n        self.env.step(pos)\n\n    def _load_model(self):\n        return load_model(self.config)\n\n    def move_by_ai(self):\n        if self.next_player == self.human_color:\n            return False\n\n        own, enemy = self.get_state_of_next_player()\n        action = self.ai.action(own, enemy)\n        self.env.step(action)\n\n        self.last_history = self.ai.ask_thought_about(own, enemy)\n        self.last_evaluation = self.last_history.values[self.last_history.action]\n        logger.debug(f""evaluation by ai={self.last_evaluation}"")\n\n    def get_state_of_next_player(self):\n        if self.next_player == Player.black:\n            own, enemy = self.env.board.black, self.env.board.white\n        else:\n            own, enemy = self.env.board.white, self.env.board.black\n        return own, enemy\n\n\n'"
src/reversi_zero/play_game/gui.py,0,"b'# many code from http://d.hatena.ne.jp/yatt/20100129/1264791420\n\nfrom logging import getLogger\n\nimport wx\nfrom wx.core import CommandEvent\n\nfrom reversi_zero.config import Config, GuiConfig, PlayWithHumanConfig\nfrom reversi_zero.env.reversi_env import Player\nfrom reversi_zero.play_game.game_model import PlayWithHuman, GameEvent\n\nlogger = getLogger(__name__)\n\n\ndef start(config: Config):\n    config.play_with_human.update_play_config(config.play)\n    reversi_model = PlayWithHuman(config)\n    app = wx.App()\n    Frame(reversi_model, config.gui).Show()\n    app.MainLoop()\n\n\ndef notify(caption, message):\n    dialog = wx.MessageDialog(None, message=message, caption=caption, style=wx.OK)\n    dialog.ShowModal()\n    dialog.Destroy()\n\n\nclass Frame(wx.Frame):\n    def __init__(self, model: PlayWithHuman, gui_config: GuiConfig):\n        self.model = model\n        self.gui_config = gui_config\n        self.is_flip_vertical = False\n        self.show_player_evaluation = True\n        wx.Frame.__init__(self, None, -1, self.gui_config.window_title, size=self.gui_config.window_size)\n        # panel\n        self.panel = wx.Panel(self)\n        self.panel.Bind(wx.EVT_LEFT_DOWN, self.try_move)\n        self.panel.Bind(wx.EVT_PAINT, self.refresh)\n\n        self.new_game(human_is_black=True)\n        # menu bar\n        menu = wx.Menu()\n        menu.Append(1, u""New Game(Black)"")\n        menu.Append(2, u""New Game(White)"")\n        menu.AppendSeparator()\n        menu.Append(5, u""Flip Vertical"")\n        menu.Append(6, u""Show/Hide Player evaluation"")\n        menu.AppendSeparator()\n        menu.Append(9, u""quit"")\n        menu_bar = wx.MenuBar()\n        menu_bar.Append(menu, u""menu"")\n        self.SetMenuBar(menu_bar)\n        self.Bind(wx.EVT_MENU, self.handle_new_game, id=1)\n        self.Bind(wx.EVT_MENU, self.handle_new_game, id=2)\n        self.Bind(wx.EVT_MENU, self.handle_flip_vertical, id=5)\n        self.Bind(wx.EVT_MENU, self.handle_show_hide_player_evaluation, id=6)\n        self.Bind(wx.EVT_MENU, self.handle_quit, id=9)\n\n        # status bar\n        self.CreateStatusBar()\n\n        self.model.add_observer(self.handle_game_event)\n\n    def handle_game_event(self, event):\n        if event == GameEvent.update:\n            self.panel.Refresh()\n            self.update_status_bar()\n            wx.Yield()\n        elif event == GameEvent.over:\n            self.game_over()\n        elif event == GameEvent.ai_move:\n            self.ai_move()\n\n    def handle_quit(self, event: CommandEvent):\n        self.Close()\n\n    def handle_new_game(self, event: CommandEvent):\n        self.new_game(human_is_black=event.GetId() == 1)\n\n    def handle_flip_vertical(self, event):\n        self.is_flip_vertical = not self.is_flip_vertical\n        self.panel.Refresh()\n\n    def handle_show_hide_player_evaluation(self, event):\n        self.show_player_evaluation = not self.show_player_evaluation\n        self.panel.Refresh()\n\n    def new_game(self, human_is_black):\n        self.model.start_game(human_is_black=human_is_black)\n        self.model.play_next_turn()\n\n    def ai_move(self):\n        self.panel.Refresh()\n        self.update_status_bar()\n        wx.Yield()\n        self.model.move_by_ai()\n        self.model.play_next_turn()\n\n    def try_move(self, event):\n        if self.model.over:\n            return\n        # calculate coordinate from window coordinate\n        event_x, event_y = event.GetX(), event.GetY()\n        w, h = self.panel.GetSize()\n        x = int(event_x / (w / 8))\n        y = int(event_y / (h / 8))\n\n        if self.is_flip_vertical:\n            y = 7-y\n\n        if not self.model.available(x, y):\n            return\n\n        self.model.move(x, y)\n        self.model.play_next_turn()\n\n    def game_over(self):\n        # if game is over then display dialog\n\n        black, white = self.model.number_of_black_and_white\n        mes = ""black: %d\\nwhite: %d\\n"" % (black, white)\n        if black == white:\n            mes += ""** draw **""\n        else:\n            mes += ""winner: %s"" % [""black"", ""white""][black < white]\n        notify(""game is over"", mes)\n        # elif self.reversi.passed != None:\n        #     notify(""passing turn"", ""pass"")\n\n    def update_status_bar(self):\n        msg = ""current player is "" + [""White"", ""Black""][self.model.next_player == Player.black]\n        if self.model.last_evaluation:\n            msg += f""|AI Confidence={self.model.last_evaluation:.4f}""\n        self.SetStatusText(msg)\n\n    def refresh(self, event):\n        dc = wx.PaintDC(self.panel)\n        self.update_status_bar()\n\n        w, h = self.panel.GetSize()\n        # background\n        dc.SetBrush(wx.Brush(""#228b22""))\n        dc.DrawRectangle(0, 0, w, h)\n        # grid\n        dc.SetBrush(wx.Brush(""black""))\n        px, py = w / 8, h / 8\n        for y in range(8):\n            dc.DrawLine(y * px, 0, y * px, h)\n            dc.DrawLine(0, y * py, w, y * py)\n        dc.DrawLine(w - 1, 0, w - 1, h - 1)\n        dc.DrawLine(0, h - 1, w - 1, h - 1)\n\n        # stones\n        brushes = {Player.white: wx.Brush(""white""), Player.black: wx.Brush(""black"")}\n        for y in range(8):\n            vy = 7-y if self.is_flip_vertical else y\n            for x in range(8):\n                c = self.model.stone(x, y)\n                if c is not None:\n                    dc.SetBrush(brushes[c])\n                    dc.DrawEllipse(x * px, vy * py, px, py)\n                if self.model.last_history:\n                    q_value = self.model.last_history.values[y*8+x]\n                    n_value = self.model.last_history.visit[y*8+x]\n                    enemy_q_value = - self.model.last_history.enemy_values[y*8+x]\n                    enemy_n_value = self.model.last_history.enemy_visit[y*8+x]\n\n                    dc.SetTextForeground(wx.Colour(""blue""))\n                    if n_value:\n                        dc.DrawText(f""{int(n_value):d}"", x*px+2, vy*py+2)\n                    if q_value:\n                        if q_value < 0:\n                            dc.SetTextForeground(wx.Colour(""red""))\n                        dc.DrawText(f""{int(q_value*100):d}"", x*px+2, (vy+1)*py-16)\n\n                    if self.show_player_evaluation:\n                        dc.SetTextForeground(wx.Colour(""purple""))\n                        if enemy_n_value:\n                            dc.DrawText(f""{int(enemy_n_value):2d}"", (x+1)*px-20, vy*py+2)\n                        if enemy_q_value:\n                            if enemy_q_value < 0:\n                                dc.SetTextForeground(wx.Colour(""orange""))\n                            dc.DrawText(f""{int(enemy_q_value*100):2d}"", (x+1)*px-24, (vy+1)*py-16)\n'"
src/reversi_zero/play_game/nboard.py,0,"b'import re\nimport sys\nfrom collections import namedtuple\n\nfrom logging import getLogger, StreamHandler, FileHandler\nfrom time import time\n\nfrom reversi_zero.agent.player import ReversiPlayer, CallbackInMCTS\nfrom reversi_zero.config import Config, PlayWithHumanConfig\nfrom reversi_zero.env.reversi_env import ReversiEnv, Player\nfrom reversi_zero.lib.ggf import parse_ggf, convert_to_bitboard_and_actions, convert_move_to_action, \\\n    convert_action_to_move\nfrom reversi_zero.lib.nonblocking_stream_reader import NonBlockingStreamReader\nfrom reversi_zero.play_game.common import load_model\n\nlogger = getLogger(__name__)\n\nGameState = namedtuple(""GameState"", ""black white actions player"")\nGoResponse = namedtuple(""GoResponse"", ""action eval time"")\nHintResponse = namedtuple(""HintResponse"", ""action value visit"")\n\n\ndef start(config: Config):\n    config.play_with_human.update_play_config(config.play)\n    root_logger = getLogger()\n    for h in root_logger.handlers:\n        if isinstance(h, StreamHandler) and not isinstance(h, FileHandler):\n            root_logger.removeHandler(h)\n    logger.info(f""config type={config.type}"")\n    NBoardEngine(config).start()\n    logger.info(""finish nboard"")\n\n\nclass NBoardEngine:\n    def __init__(self, config: Config):\n        self.config = config\n        self.reader = NonBlockingStreamReader(sys.stdin)\n        self.handler = NBoardProtocolVersion2(config, self)\n        self.running = False\n        self.nc = self.config.nboard  # shorcut\n        #\n        self.env = ReversiEnv().reset()\n        self.model = load_model(self.config)\n        self.play_config = self.config.play\n        self.player = self.create_player()\n        self.turn_of_nboard = None\n\n    def create_player(self):\n        logger.debug(""create new ReversiPlayer()"")\n        return ReversiPlayer(self.config, self.model, self.play_config, enable_resign=False)\n\n    def start(self):\n        self.running = True\n        self.reader.start(push_callback=self.push_callback)\n        while self.running and not self.reader.closed:\n            message = self.reader.readline(self.nc.read_stdin_timeout)\n            if message is None:\n                continue\n            message = message.strip()\n            logger.debug(f""> {message}"")\n            self.handler.handle_message(message)\n\n    def push_callback(self, message: str):\n        # note: called in another thread\n        if message.startswith(""ping""):  # interupt\n            self.stop_thinkng()\n\n    def stop(self):\n        self.running = False\n\n    def reply(self, message):\n        logger.debug(f""< {message}"")\n        sys.stdout.write(message + ""\\n"")\n        sys.stdout.flush()\n\n    def stop_thinkng(self):\n        self.player.stop_thinking()\n\n    def set_depth(self, n):\n        try:\n            n = int(n)\n            # self.play_config.simulation_num_per_move = n * self.nc.simulation_num_per_depth_about\n            self.play_config.required_visit_to_decide_action = n * self.nc.simulation_num_per_depth_about\n            self.play_config.thinking_loop = min(\n                30,\n                int(self.play_config.required_visit_to_decide_action * 5 / self.play_config.simulation_num_per_move)\n            )\n\n            logger.info(f""set required_visit_to_decide_action to {self.play_config.required_visit_to_decide_action}"")\n        except ValueError:\n            pass\n\n    def reset_state(self):\n        self.player = self.create_player()\n\n    def set_game(self, game_state: GameState):\n        self.env.reset()\n        self.env.update(game_state.black, game_state.white, game_state.player)\n        self.turn_of_nboard = game_state.player\n        for action in game_state.actions:\n            self._change_turn()\n            if action is not None:\n                self.env.step(action)\n\n    def _change_turn(self):\n        if self.turn_of_nboard:\n            self.turn_of_nboard = Player.black if self.turn_of_nboard == Player.white else Player.white\n\n    def move(self, action):\n        self._change_turn()\n        if action is not None:\n            self.env.step(action)\n\n    def go(self) -> GoResponse:\n        if self.env.next_player != self.turn_of_nboard:\n            return GoResponse(None, 0, 0)\n\n        board = self.env.board\n        if self.env.next_player == Player.black:\n            states = (board.black, board.white)\n        else:\n            states = (board.white, board.black)\n        start_time = time()\n        action = self.player.action(*states)\n        item = self.player.ask_thought_about(*states)\n        evaluation = item.values[action]\n        time_took = time() - start_time\n        return GoResponse(action, evaluation, time_took)\n\n    def hint(self, n_hint):\n        """"""\n\n        :param n_hint:\n        """"""\n        board = self.env.board\n        if self.env.next_player == Player.black:\n            states = (board.black, board.white)\n        else:\n            states = (board.white, board.black)\n\n        def hint_report_callback(values, visits):\n            hint_list = []\n            for action, visit in list(sorted(enumerate(visits), key=lambda x: -x[1]))[:n_hint]:\n                if visit > 0:\n                    hint_list.append(HintResponse(action, values[action], visit))\n            self.handler.report_hint(hint_list)\n\n        callback_info = CallbackInMCTS(self.config.nboard.hint_callback_per_sim, hint_report_callback)\n        self.player.action(*states, callback_in_mtcs=callback_info)\n        item = self.player.ask_thought_about(*states)\n        hint_report_callback(item.values, item.visit)\n\n\nclass NBoardProtocolVersion2:\n    def __init__(self, config: Config, engine: NBoardEngine):\n        self.config = config\n        self.engine = engine\n        self.handlers = [\n            (re.compile(r\'nboard ([0-9]+)\'), self.nboard),\n            (re.compile(r\'set depth ([0-9]+)\'), self.set_depth),\n            (re.compile(r\'set game (.+)\'), self.set_game),\n            (re.compile(r\'move ([^/]+)(/[^/]*)?(/[^/]*)?\'), self.move),\n            (re.compile(r\'hint ([0-9]+)\'), self.hint),\n            (re.compile(r\'go\'), self.go),\n            (re.compile(r\'ping ([0-9]+)\'), self.ping),\n            (re.compile(r\'learn\'), self.learn),\n            (re.compile(r\'analyze\'), self.analyze),\n        ]\n\n    def handle_message(self, message):\n        for regexp, func in self.handlers:\n            if self.scan(message, regexp, func):\n                return\n        logger.debug(f""ignore message: {message}"")\n\n    def scan(self, message, regexp, func):\n        match = regexp.match(message)\n        if match:\n            func(*match.groups())\n            return True\n        return False\n\n    def nboard(self, version):\n        if version != ""2"":\n            logger.warning(f""UNKNOWN NBoard Version {version}!!!"")\n        self.engine.reply(f""set myname {self.config.nboard.my_name}({self.config.type})"")\n        self.tell_status(""waiting"")\n\n    def set_depth(self, depth):\n        """"""Set engine midgame search depth.\n\n        Optional: Set midgame depth to {maxDepth}. Endgame depths are at the engine author\'s discretion.\n        :param depth:\n        """"""\n        self.engine.set_depth(depth)\n\n    def set_game(self, ggf_str):\n        """"""Tell the engine that all further commands relate to the position at the end of the given game, in GGF format.\n\n        Required:The engine must update its stored game state.\n        :param ggf_str: see https://skatgame.net/mburo/ggsa/ggf . important info are BO, B+, W+\n        """"""\n        ggf = parse_ggf(ggf_str)\n        black, white, actions = convert_to_bitboard_and_actions(ggf)\n        player = Player.black if ggf.BO.color == ""*"" else Player.white\n        self.engine.set_game(GameState(black, white, actions, player))\n\n        # if set_game at turn=1~2 is sent, reset engine state.\n        if len(actions) <= 1:\n            self.engine.reset_state()  # clear MCTS cache\n\n    def move(self, move, evaluation, time_sec):\n        """"""Tell the engine that all further commands relate to the position after the given move.\n        The move is 2 characters e.g. ""F5"". Eval is normally in centi-disks. Time is in seconds.\n        Eval and time may be omitted. If eval is omitted it is assumed to be ""unknown"";\n        if time is omitted it is assumed to be 0.\n\n        Required:Update the game state by making the move. No response required.\n        """"""\n        # logger.debug(f""[{move}] [{evaluation}] [{time_sec}]"")\n\n        action = convert_move_to_action(move)\n        self.engine.move(action)\n\n    def hint(self, n):\n        """"""Tell the engine to give evaluations for the given position. n tells how many moves to evaluate,\n        e.g. 2 means give evaluations for the top 2 positions. This is used when the user is analyzing a game.\n        With the ""hint"" command the engine is not CONSTRained by the time remaining in the game.\n\n        Required: The engine sends back an evaluation for at its top move\n\n        Best: The engine sends back an evaluation for approximately the top n moves.\n        If the engine searches using iterative deepening it should also send back evaluations during search,\n        which makes the GUI feel more responsive to the user.\n\n        Depending on whether the evalation came from book or a search, the engine sends back\n\n        search {pv: PV} {eval:Eval} 0 {depth:Depth} {freeform text}\n        or\n        book {pv: PV} {eval:Eval} {# games:long} {depth:Depth} {freeform text:string}\n\n        PV: The pv must begin with two characters representing the move considered (e.g. ""F5"" or ""PA"") and\n        must not contain any whitespace. ""F5d6C3"" and ""F5-D6-C3"" are valid PVs but ""F5 D6 C3"" will\n        consider D6 to be the eval.\n\n        Eval: The eval is from the point-of-view of the player to move and is a double.\n        At the engine\'s option it can also be an ordered pair of doubles separated by a comma:\n        {draw-to-black value}, {draw-to-white value}.\n\n        Depth: depth is the search depth. It must start with an integer but can end with other characters;\n        for instance ""100%W"" is a valid depth. The depth cannot contain spaces.\n\n        Two depth codes have special meaning to NBoard: ""100%W"" tells NBoard that the engine has solved\n        for a win/loss/draw and the sign of the eval matches the sign of the returned eval.\n        ""100%"" tells NBoard that the engine has done an exact solve.\n        The freeform text can be any other information that the engine wants to convey.\n        NBoard 1.1 and 2.0 do not display this information but later versions or other GUIs may.\n\n        :param n:\n        """"""\n        self.tell_status(""thinkng hint..."")\n        self.engine.hint(int(n))\n        self.tell_status(""waiting"")\n\n    def report_hint(self, hint_list):\n        for hint in reversed(hint_list):  # there is a rule that the last is best?\n            move = convert_action_to_move(hint.action)\n            self.engine.reply(f""search {move} {hint.value} 0 {int(hint.visit)}"")\n\n    def go(self):\n        """"""Tell the engine to decide what move it would play.\n\n        This is used when the engine is playing in a game.\n        With the ""go"" command the computer is limited by both the maximum search depth and\n        the time remaining in the game.\n\n        Required: The engine responds with ""=== {move}"" where move is e.g. ""F5""\n\n        Best: The engine responds with ""=== {move:String}/{eval:float}/{time:float}"".\n        Eval may be omitted if the move is forced. The engine also sends back thinking output\n        as in the ""hint"" command.\n\n        Important: The engine does not update the board with this move,\n        instead it waits for a ""move"" command from NBoard.\n        This is because the user may have modified the board while the engine was thinking.\n\n        Note: To make it easier for the engine author,\n        The NBoard gui sets the engine\'s status to """" when it receives the response.\n        The engine can override this behaviour by sending a ""status"" command immediately after the response.\n        """"""\n        self.tell_status(""thinking..."")\n        gr = self.engine.go()\n        move = convert_action_to_move(gr.action)\n        self.engine.reply(f""=== {move}/{gr.eval * 10}/{gr.time}"")\n        self.tell_status(""waiting"")\n\n    def ping(self, n):\n        """"""Ensure synchronization when the board position is about to change.\n\n        Required: Stop thinking and respond with ""pong n"".\n        If the engine is analyzing a position it must stop analyzing before sending ""pong n""\n        otherwise NBoard will think the analysis relates to the current position.\n        :param n:\n        :return:\n        """"""\n        # self.engine.stop_thinkng()  # not implemented\n        self.engine.reply(f""pong {n}"")\n\n    def learn(self):\n        """"""Learn the current game.\n        Required: Respond ""learned"".\n\n        Best: Add the current game to book.\n\n        Note: To make it easier for the engine author,\n        The NBoard gui sets the engine\'s status to """" when it receives the ""learned"" response.\n        The engine can override this behaviour by sending a ""status"" command immediately after the response.\n        """"""\n        self.engine.reply(""learned"")\n\n    def analyze(self):\n        """"""Perform a retrograde analysis of the current game.\n\n        Optional: Perform a retrograde analysis of the current game.\n        For each board position occurring in the game,\n        the engine sends back a line of the form analysis {movesMade:int} {eval:double}.\n        movesMade = 0 corresponds to the start position. Passes count towards movesMade,\n        so movesMade can go above 60.\n        """"""\n        pass\n\n    def tell_status(self, status):\n        self.engine.reply(f""status {status}"")\n\n\n'"
src/reversi_zero/worker/__init__.py,0,b''
src/reversi_zero/worker/evaluate.py,0,"b'import os\nfrom logging import getLogger\nfrom random import random\nfrom time import sleep\n\nfrom reversi_zero.agent.model import ReversiModel\nfrom reversi_zero.agent.player import ReversiPlayer\nfrom reversi_zero.config import Config\nfrom reversi_zero.env.reversi_env import ReversiEnv, Player, Winner\nfrom reversi_zero.lib import tf_util\nfrom reversi_zero.lib.data_helper import get_next_generation_model_dirs\nfrom reversi_zero.lib.model_helpler import save_as_best_model, load_best_model_weight\n\nlogger = getLogger(__name__)\n\n\ndef start(config: Config):\n    tf_util.set_session_config(per_process_gpu_memory_fraction=0.2)\n    return EvaluateWorker(config).start()\n\n\nclass EvaluateWorker:\n    def __init__(self, config: Config):\n        """"""\n\n        :param config:\n        """"""\n        self.config = config\n        self.best_model = None\n\n    def start(self):\n        self.best_model = self.load_best_model()\n\n        while True:\n            ng_model, model_dir = self.load_next_generation_model()\n            logger.debug(f""start evaluate model {model_dir}"")\n            ng_is_great = self.evaluate_model(ng_model)\n            if ng_is_great:\n                logger.debug(f""New Model become best model: {model_dir}"")\n                save_as_best_model(ng_model)\n                self.best_model = ng_model\n            self.remove_model(model_dir)\n\n    def evaluate_model(self, ng_model):\n        results = []\n        winning_rate = 0\n        for game_idx in range(self.config.eval.game_num):\n            # ng_win := if ng_model win -> 1, lose -> 0, draw -> None\n            ng_win, black_is_best, black_white = self.play_game(self.best_model, ng_model)\n            if ng_win is not None:\n                results.append(ng_win)\n                winning_rate = sum(results) / len(results)\n            logger.debug(f""game {game_idx}: ng_win={ng_win} black_is_best_model={black_is_best} score={black_white} ""\n                         f""winning rate {winning_rate*100:.1f}%"")\n            if results.count(0) >= self.config.eval.game_num * (1-self.config.eval.replace_rate):\n                logger.debug(f""lose count reach {results.count(0)} so give up challenge"")\n                break\n            if results.count(1) >= self.config.eval.game_num * self.config.eval.replace_rate:\n                logger.debug(f""win count reach {results.count(1)} so change best model"")\n                break\n\n        winning_rate = sum(results) / len(results)\n        logger.debug(f""winning rate {winning_rate*100:.1f}%"")\n        return winning_rate >= self.config.eval.replace_rate\n\n    def play_game(self, best_model, ng_model):\n        env = ReversiEnv().reset()\n\n        best_player = ReversiPlayer(self.config, best_model, play_config=self.config.eval.play_config)\n        ng_player = ReversiPlayer(self.config, ng_model, play_config=self.config.eval.play_config)\n        best_is_black = random() < 0.5\n        if best_is_black:\n            black, white = best_player, ng_player\n        else:\n            black, white = ng_player, best_player\n\n        observation = env.observation\n        while not env.done:\n            if env.next_player == Player.black:\n                action = black.action(observation.black, observation.white)\n            else:\n                action = white.action(observation.white, observation.black)\n            observation, info = env.step(action)\n\n        ng_win = None\n        if env.winner == Winner.black:\n            if best_is_black:\n                ng_win = 0\n            else:\n                ng_win = 1\n        elif env.winner == Winner.white:\n            if best_is_black:\n                ng_win = 1\n            else:\n                ng_win = 0\n        return ng_win, best_is_black, observation.number_of_black_and_white\n\n    def load_best_model(self):\n        model = ReversiModel(self.config)\n        load_best_model_weight(model)\n        return model\n\n    def load_next_generation_model(self):\n        rc = self.config.resource\n        while True:\n            dirs = get_next_generation_model_dirs(self.config.resource)\n            if dirs:\n                break\n            logger.info(f""There is no next generation model to evaluate"")\n            sleep(60)\n        model_dir = dirs[-1] if self.config.eval.evaluate_latest_first else dirs[0]\n        config_path = os.path.join(model_dir, rc.next_generation_model_config_filename)\n        weight_path = os.path.join(model_dir, rc.next_generation_model_weight_filename)\n        model = ReversiModel(self.config)\n        model.load(config_path, weight_path)\n        return model, model_dir\n\n    def remove_model(self, model_dir):\n        rc = self.config.resource\n        config_path = os.path.join(model_dir, rc.next_generation_model_config_filename)\n        weight_path = os.path.join(model_dir, rc.next_generation_model_weight_filename)\n        os.remove(config_path)\n        os.remove(weight_path)\n        os.rmdir(model_dir)\n'"
src/reversi_zero/worker/optimize.py,0,"b'import os\nfrom collections import Counter\nfrom datetime import datetime\nfrom logging import getLogger\nfrom time import sleep, time\n\nimport keras.backend as K\nimport numpy as np\nfrom keras.callbacks import Callback\nfrom keras.optimizers import SGD\n\nfrom reversi_zero.agent.model import ReversiModel, objective_function_for_policy, \\\n    objective_function_for_value\nfrom reversi_zero.config import Config\nfrom reversi_zero.lib import tf_util\nfrom reversi_zero.lib.bitboard import bit_to_array\nfrom reversi_zero.lib.data_helper import get_game_data_filenames, read_game_data_from_file, \\\n    get_next_generation_model_dirs\nfrom reversi_zero.lib.model_helpler import load_best_model_weight\nfrom reversi_zero.lib.tensorboard_step_callback import TensorBoardStepCallback\n\nlogger = getLogger(__name__)\n\n\ndef start(config: Config):\n    tf_util.set_session_config(per_process_gpu_memory_fraction=0.65)\n    return OptimizeWorker(config).start()\n\n\nclass OptimizeWorker:\n    def __init__(self, config: Config):\n        self.config = config\n        self.model = None  # type: ReversiModel\n        self.loaded_filenames = set()\n        self.loaded_data = {}\n        self.training_count_of_files = Counter()\n        self.dataset = None\n        self.optimizer = None\n\n    def start(self):\n        self.model = self.load_model()\n        self.training()\n\n    def training(self):\n        self.compile_model()\n        total_steps = self.config.trainer.start_total_steps\n        save_model_callback = PerStepCallback(self.config.trainer.save_model_steps, self.save_current_model,\n                                              self.config.trainer.wait_after_save_model_ratio)\n        callbacks = [save_model_callback]  # type: list[Callback]\n        tb_callback = None  # type: TensorBoardStepCallback\n\n        if self.config.trainer.use_tensorboard:\n            tb_callback = TensorBoardStepCallback(\n                log_dir=self.config.resource.tensorboard_log_dir,\n                logging_per_steps=self.config.trainer.logging_per_steps,\n                step=total_steps,\n            )\n            callbacks.append(tb_callback)\n\n        while True:\n            self.load_play_data()\n            if self.dataset_size < self.config.trainer.min_data_size_to_learn:\n                logger.info(f""dataset_size={self.dataset_size} is less than {self.config.trainer.min_data_size_to_learn}"")\n                sleep(10)\n                continue\n            self.update_learning_rate(total_steps)\n            total_steps += self.train_epoch(self.config.trainer.epoch_to_checkpoint, callbacks)\n            self.count_up_training_count_and_delete_self_play_data_files()\n\n        if tb_callback:  # This code is never reached. But potentially this is required.\n            tb_callback.close()\n\n    def train_epoch(self, epochs, callbacks):\n        tc = self.config.trainer\n        state_ary, policy_ary, z_ary = self.dataset\n        self.model.model.fit(state_ary, [policy_ary, z_ary],\n                             batch_size=tc.batch_size,\n                             callbacks=callbacks,\n                             epochs=epochs)\n        steps = (state_ary.shape[0] // tc.batch_size) * epochs\n        return steps\n\n    def compile_model(self):\n        self.optimizer = SGD(lr=1e-2, momentum=0.9)\n        losses = [objective_function_for_policy, objective_function_for_value]\n        self.model.model.compile(optimizer=self.optimizer, loss=losses)\n\n    def update_learning_rate(self, total_steps):\n        # The deepmind paper says\n        # ~400k: 1e-2\n        # 400k~600k: 1e-3\n        # 600k~: 1e-4\n\n        lr = self.decide_learning_rate(total_steps)\n        if lr:\n            K.set_value(self.optimizer.lr, lr)\n            logger.debug(f""total step={total_steps}, set learning rate to {lr}"")\n\n    def decide_learning_rate(self, total_steps):\n        ret = None\n\n        if os.path.exists(self.config.resource.force_learing_rate_file):\n            try:\n                with open(self.config.resource.force_learing_rate_file, ""rt"") as f:\n                    ret = float(str(f.read()).strip())\n                    if ret:\n                        logger.debug(f""loaded lr from force learning rate file: {ret}"")\n                        return ret\n            except ValueError:\n                pass\n\n        for step, lr in self.config.trainer.lr_schedules:\n            if total_steps >= step:\n                ret = lr\n        return ret\n\n    def save_current_model(self):\n        rc = self.config.resource\n        model_id = datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")\n        model_dir = os.path.join(rc.next_generation_model_dir, rc.next_generation_model_dirname_tmpl % model_id)\n        os.makedirs(model_dir, exist_ok=True)\n        config_path = os.path.join(model_dir, rc.next_generation_model_config_filename)\n        weight_path = os.path.join(model_dir, rc.next_generation_model_weight_filename)\n        self.model.save(config_path, weight_path)\n\n    def collect_all_loaded_data(self):\n        state_ary_list, policy_ary_list, z_ary_list = [], [], []\n        for s_ary, p_ary, z_ary_ in self.loaded_data.values():\n            state_ary_list.append(s_ary)\n            policy_ary_list.append(p_ary)\n            z_ary_list.append(z_ary_)\n\n        if state_ary_list:\n            state_ary = np.concatenate(state_ary_list)\n            policy_ary = np.concatenate(policy_ary_list)\n            z_ary = np.concatenate(z_ary_list)\n            return state_ary, policy_ary, z_ary\n        else:\n            return None\n\n    @property\n    def dataset_size(self):\n        if self.dataset is None:\n            return 0\n        return len(self.dataset[0])\n\n    def load_model(self):\n        from reversi_zero.agent.model import ReversiModel\n        model = ReversiModel(self.config)\n        rc = self.config.resource\n\n        dirs = get_next_generation_model_dirs(rc)\n        if not dirs:\n            logger.debug(f""loading best model"")\n            if not load_best_model_weight(model):\n                raise RuntimeError(f""Best model can not loaded!"")\n        else:\n            latest_dir = dirs[-1]\n            logger.debug(f""loading latest model"")\n            config_path = os.path.join(latest_dir, rc.next_generation_model_config_filename)\n            weight_path = os.path.join(latest_dir, rc.next_generation_model_weight_filename)\n            model.load(config_path, weight_path)\n        return model\n\n    def load_play_data(self):\n        filenames = get_game_data_filenames(self.config.resource)\n        updated = False\n        for filename in filenames:\n            if filename in self.loaded_filenames:\n                continue\n            self.load_data_from_file(filename)\n            updated = True\n\n        for filename in (self.loaded_filenames - set(filenames)):\n            self.unload_data_of_file(filename)\n            updated = True\n\n        if updated:\n            logger.debug(""updating training dataset"")\n            self.dataset = self.collect_all_loaded_data()\n\n    def load_data_from_file(self, filename):\n        try:\n            logger.debug(f""loading data from {filename}"")\n            data = read_game_data_from_file(filename)\n            self.loaded_data[filename] = self.convert_to_training_data(data)\n            self.loaded_filenames.add(filename)\n        except Exception as e:\n            logger.warning(str(e))\n\n    def unload_data_of_file(self, filename):\n        logger.debug(f""removing data about {filename} from training set"")\n        self.loaded_filenames.remove(filename)\n        if filename in self.loaded_data:\n            del self.loaded_data[filename]\n        if filename in self.training_count_of_files:\n            del self.training_count_of_files[filename]\n\n    def count_up_training_count_and_delete_self_play_data_files(self):\n        limit = self.config.trainer.delete_self_play_after_number_of_training\n        if not limit:\n            return\n\n        for filename in self.loaded_filenames:\n            self.training_count_of_files[filename] += 1\n            if self.training_count_of_files[filename] >= limit:\n                if os.path.exists(filename):\n                    try:\n                        logger.debug(f""remove {filename}"")\n                        os.remove(filename)\n                    except Exception as e:\n                        logger.warning(e)\n\n    @staticmethod\n    def convert_to_training_data(data):\n        """"""\n\n        :param data: format is SelfPlayWorker.buffer\n            list of [(own: bitboard, enemy: bitboard), [policy: float 64 items], z: number]\n        :return:\n        """"""\n        state_list = []\n        policy_list = []\n        z_list = []\n        for state, policy, z in data:\n            own, enemy = bit_to_array(state[0], 64).reshape((8, 8)), bit_to_array(state[1], 64).reshape((8, 8))\n            state_list.append([own, enemy])\n            policy_list.append(policy)\n            z_list.append(z)\n\n        return np.array(state_list), np.array(policy_list), np.array(z_list)\n\n\nclass PerStepCallback(Callback):\n    def __init__(self, per_step, callback, wait_after_save_model_ratio=None):\n        super().__init__()\n        self.per_step = per_step\n        self.step = 0\n        self.callback = callback\n        self.wait_after_save_model_ratio = wait_after_save_model_ratio\n        self.last_wait_time = time()\n\n    def on_batch_end(self, batch, logs=None):\n        self.step += 1\n        if self.step % self.per_step == 0:\n            self.callback()\n            self.wait()\n\n    def wait(self):\n        if self.wait_after_save_model_ratio:\n            time_spent = time() - self.last_wait_time\n            logger.debug(f""start sleeping {time_spent} seconds"")\n            sleep(time_spent * self.wait_after_save_model_ratio)\n            logger.debug(f""finish sleeping"")\n            self.last_wait_time = time()\n'"
src/reversi_zero/worker/self_play.py,0,"b'import cProfile\nimport os\nfrom concurrent.futures import ProcessPoolExecutor\nfrom datetime import datetime\nfrom logging import getLogger\nfrom random import random\nfrom time import time\nfrom traceback import print_stack\n\nimport numpy as np\nfrom multiprocessing import Manager, Lock\n\n\nfrom reversi_zero.agent.api import MultiProcessReversiModelAPIServer\nfrom reversi_zero.agent.player import ReversiPlayer\nfrom reversi_zero.config import Config\nfrom reversi_zero.env.reversi_env import Board, Winner\nfrom reversi_zero.env.reversi_env import ReversiEnv, Player\nfrom reversi_zero.lib import tf_util\nfrom reversi_zero.lib.data_helper import get_game_data_filenames, write_game_data_to_file\nfrom reversi_zero.lib.file_util import read_as_int\nfrom reversi_zero.lib.ggf import convert_action_to_move, make_ggf_string\nfrom reversi_zero.lib.tensorboard_logger import TensorBoardLogger\n\nlogger = getLogger(__name__)\n\n\ndef start(config: Config):\n    tf_util.set_session_config(per_process_gpu_memory_fraction=0.3)\n    api_server = MultiProcessReversiModelAPIServer(config)\n    process_num = config.play_data.multi_process_num\n    api_server.start_serve()\n\n    with Manager() as manager:\n        shared_var = SharedVar(manager, game_idx=read_as_int(config.resource.self_play_game_idx_file) or 0)\n        with ProcessPoolExecutor(max_workers=process_num) as executor:\n            futures = []\n            for i in range(process_num):\n                play_worker = SelfPlayWorker(config, env=ReversiEnv(), api=api_server.get_api_client(),\n                                             shared_var=shared_var, worker_index=i)\n                futures.append(executor.submit(play_worker.start))\n\n\nclass SharedVar:\n    def __init__(self, manager, game_idx: int):\n        """"""\n\n        :param Manager manager:\n        :param int game_idx:\n        """"""\n        self._lock = manager.Lock()\n        self._game_idx = manager.Value(\'i\', game_idx)  # type: multiprocessing.managers.ValueProxy\n\n    @property\n    def game_idx(self):\n        return self._game_idx.value\n\n    def incr_game_idx(self, n=1):\n        with self._lock:\n            self._game_idx.value += n\n            return self._game_idx.value\n\n\nclass SelfPlayWorker:\n    def __init__(self, config: Config, env, api, shared_var, worker_index=0):\n        """"""\n\n        :param config:\n        :param ReversiEnv|None env:\n        :param ReversiModelAPI|None api:\n        :param SharedVar shared_var:\n        :param int worker_index:\n        """"""\n        self.config = config\n        self.env = env\n        self.api = api\n        self.shared_var = shared_var\n        self.black = None  # type: ReversiPlayer\n        self.white = None  # type: ReversiPlayer\n        self.buffer = []\n        self.false_positive_count_of_resign = 0\n        self.resign_test_game_count = 0\n        self.worker_index = worker_index\n        self.tensor_board = None  # type: TensorBoardLogger\n        self.move_history = None  # type: MoveHistory\n        self.move_history_buffer = []  # type: list[MoveHistory]\n\n    def start(self):\n        try:\n            self._start()\n        except Exception as e:\n            print(repr(e))\n            print_stack()\n\n    def _start(self):\n        logger.debug(""SelfPlayWorker#start()"")\n        np.random.seed(None)\n        worker_name = f""worker{self.worker_index:03d}""\n        self.tensor_board = TensorBoardLogger(os.path.join(self.config.resource.self_play_log_dir, worker_name))\n\n        self.buffer = []\n        mtcs_info = None\n        local_idx = 0\n\n        while True:\n            np.random.seed(None)\n            local_idx += 1\n            game_idx = self.shared_var.game_idx\n\n            start_time = time()\n            if mtcs_info is None and self.config.play.share_mtcs_info_in_self_play:\n                mtcs_info = ReversiPlayer.create_mtcs_info()\n\n            # play game\n            env = self.start_game(local_idx, game_idx, mtcs_info)\n\n            game_idx = self.shared_var.incr_game_idx()\n            # just log\n            end_time = time()\n            time_spent = end_time - start_time\n            logger.debug(f""play game {game_idx} time={time_spent} sec, ""\n                         f""turn={env.turn}:{env.board.number_of_black_and_white}:{env.winner}"")\n\n            # log play info to tensor board\n            prefix = ""self""\n            log_info = {f""{prefix}/time"": time_spent, f""{prefix}/turn"": env.turn}\n            if mtcs_info:\n                log_info[f""{prefix}/mcts_buffer_size""] = len(mtcs_info.var_p)\n            self.tensor_board.log_scaler(log_info, game_idx)\n\n            # reset MCTS info per X games\n            if self.config.play.reset_mtcs_info_per_game and local_idx % self.config.play.reset_mtcs_info_per_game == 0:\n                logger.debug(""reset MCTS info"")\n                mtcs_info = None\n\n            with open(self.config.resource.self_play_game_idx_file, ""wt"") as f:\n                f.write(str(game_idx))\n\n    def start_game(self, local_idx, last_game_idx, mtcs_info):\n        # profiler = cProfile.Profile()\n        # profiler.enable()\n\n        self.env.reset()\n        enable_resign = self.config.play.disable_resignation_rate <= random()\n        self.config.play.simulation_num_per_move = self.decide_simulation_num_per_move(last_game_idx)\n        logger.debug(f""simulation_num_per_move = {self.config.play.simulation_num_per_move}"")\n        self.black = self.create_reversi_player(enable_resign=enable_resign, mtcs_info=mtcs_info)\n        self.white = self.create_reversi_player(enable_resign=enable_resign, mtcs_info=mtcs_info)\n        if not enable_resign:\n            logger.debug(""Resignation is disabled in the next game."")\n        observation = self.env.observation  # type: Board\n        self.move_history = MoveHistory()\n\n        # game loop\n        while not self.env.done:\n            # logger.debug(f""turn={self.env.turn}"")\n            if self.env.next_player == Player.black:\n                action = self.black.action_with_evaluation(observation.black, observation.white)\n            else:\n                action = self.white.action_with_evaluation(observation.white, observation.black)\n            self.move_history.move(self.env, action)\n            observation, info = self.env.step(action.action)\n\n        self.finish_game(resign_enabled=enable_resign)\n        self.save_play_data(write=local_idx % self.config.play_data.nb_game_in_file == 0)\n        self.remove_play_data()\n\n        if self.config.play_data.enable_ggf_data:\n            is_write = local_idx % self.config.play_data.nb_game_in_ggf_file == 0\n            is_write |= local_idx <= 5\n            self.save_ggf_data(write=is_write)\n\n        # profiler.disable()\n        # profiler.dump_stats(f""profile-worker-{self.worker_index}-{local_idx}"")\n        return self.env\n\n    def create_reversi_player(self, enable_resign=None, mtcs_info=None):\n        return ReversiPlayer(self.config, None, enable_resign=enable_resign, mtcs_info=mtcs_info, api=self.api)\n\n    def save_play_data(self, write=True):\n        # drop draw game by drop_draw_game_rate\n        if self.black.moves[0][-1] != 0 or self.config.play_data.drop_draw_game_rate <= np.random.random():\n            data = self.black.moves + self.white.moves\n            self.buffer += data\n\n        if not write or not self.buffer:\n            return\n\n        rc = self.config.resource\n        game_id = datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")\n        path = os.path.join(rc.play_data_dir, rc.play_data_filename_tmpl % game_id)\n        logger.info(f""save play data to {path}"")\n        write_game_data_to_file(path, self.buffer)\n        self.buffer = []\n\n    def save_ggf_data(self, write=True):\n        self.move_history_buffer.append(self.move_history)\n        if not write:\n            return\n\n        rc = self.config.resource\n        game_id = datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")\n        path = os.path.join(rc.self_play_ggf_data_dir, rc.ggf_filename_tmpl % game_id)\n        with open(path, ""wt"") as f:\n            for mh in self.move_history_buffer:\n                f.write(mh.make_ggf_string(""RAZ"", ""RAZ"") + ""\\n"")\n        self.move_history_buffer = []\n\n    def remove_play_data(self):\n        files = get_game_data_filenames(self.config.resource)\n        if len(files) < self.config.play_data.max_file_num:\n            return\n        try:\n            for i in range(len(files) - self.config.play_data.max_file_num):\n                os.remove(files[i])\n        except:\n            pass\n\n    def finish_game(self, resign_enabled=True):\n        if self.env.winner == Winner.black:\n            black_win = 1\n            false_positive_of_resign = self.black.resigned\n        elif self.env.winner == Winner.white:\n            black_win = -1\n            false_positive_of_resign = self.white.resigned\n        else:\n            black_win = 0\n            false_positive_of_resign = self.black.resigned or self.white.resigned\n\n        self.black.finish_game(black_win)\n        self.white.finish_game(-black_win)\n\n        if not resign_enabled:\n            self.resign_test_game_count += 1\n            if false_positive_of_resign:\n                self.false_positive_count_of_resign += 1\n                logger.debug(""false positive of resignation happened"")\n            self.check_and_update_resignation_threshold()\n\n    def reset_false_positive_count(self):\n        self.false_positive_count_of_resign = 0\n        self.resign_test_game_count = 0\n\n    @property\n    def false_positive_rate(self):\n        if self.resign_test_game_count == 0:\n            return 0\n        return self.false_positive_count_of_resign / self.resign_test_game_count\n\n    def check_and_update_resignation_threshold(self):\n        if self.resign_test_game_count < 100 or self.config.play.resign_threshold is None:\n            return\n\n        old_threshold = self.config.play.resign_threshold\n        if self.false_positive_rate >= self.config.play.false_positive_threshold:\n            self.config.play.resign_threshold -= self.config.play.resign_threshold_delta\n        else:\n            self.config.play.resign_threshold += self.config.play.resign_threshold_delta\n        logger.debug(f""update resign_threshold: {old_threshold} -> {self.config.play.resign_threshold}"")\n        self.reset_false_positive_count()\n\n    def decide_simulation_num_per_move(self, idx):\n        ret = read_as_int(self.config.resource.force_simulation_num_file)\n\n        if ret:\n            logger.debug(f""loaded simulation num from file: {ret}"")\n            return ret\n\n        for min_idx, num in self.config.play.schedule_of_simulation_num_per_move:\n            if idx >= min_idx:\n                ret = num\n        return ret\n\n\nclass MoveHistory:\n    def __init__(self):\n        self.moves = []\n\n    def move(self, env, action):\n        """"""\n\n        :param ReversiEnv env:\n        :param ActionWithEvaluation action:\n        :return:\n        """"""\n        if action.action is None:\n            return  # resigned\n\n        if len(self.moves) % 2 == 0:\n            if env.next_player == Player.white:\n                self.moves.append(convert_action_to_move(None))\n        else:\n            if env.next_player == Player.black:\n                self.moves.append(convert_action_to_move(None))\n        move = f""{convert_action_to_move(action.action)}/{action.q*10}/{action.n}""\n        self.moves.append(move)\n\n    def make_ggf_string(self, black_name=None, white_name=None):\n        return make_ggf_string(black_name=black_name, white_name=white_name, moves=self.moves)\n'"
src/reversi_zero/lib/alt/__init__.py,0,b''
src/reversi_zero/lib/alt/reversi_solver.py,0,b'import pyximport\npyximport.install()\n\nfrom .reversi_solver_cython import *\n'
