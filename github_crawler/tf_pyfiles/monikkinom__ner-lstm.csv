file_path,api_count,code
input.py,0,"b""from __future__ import print_function\nimport pickle\n\n\ndef get_train_data():\n    emb = pickle.load(open('embeddings/train_embed.pkl', 'rb'))\n    tag = pickle.load(open('embeddings/train_tag.pkl', 'rb'))\n    print('train data loaded')\n    return emb, tag\n\n\ndef get_test_a_data():\n    emb = pickle.load(open('embeddings/test_a_embed.pkl', 'rb'))\n    tag = pickle.load(open('embeddings/test_a_tag.pkl', 'rb'))\n    print('test_a data loaded')\n    return emb, tag\n\n\ndef get_test_b_data():\n    emb = pickle.load(open('embeddings/test_b_embed.pkl', 'rb'))\n    tag = pickle.load(open('embeddings/test_b_tag.pkl', 'rb'))\n    print('test_b data loaded')\n    return emb, tag\n"""
model.py,40,"b'from __future__ import print_function\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nfrom input import *\n\n\nclass Model:\n    def __init__(self, args):\n        self.args = args\n        self.input_data = tf.placeholder(tf.float32, [None, args.sentence_length, args.word_dim])\n        self.output_data = tf.placeholder(tf.float32, [None, args.sentence_length, args.class_size])\n        #fw_cell = tf.nn.rnn_cell.LSTMCell(args.rnn_size)\n        #fw_cell = tf.nn.rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=0.5)\n        #bw_cell = tf.nn.rnn_cell.LSTMCell(args.rnn_size)\n        #bw_cell = tf.nn.rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=0.5)\n        fw_cell = tf.nn.rnn_cell.MultiRNNCell([self.lstm_cell() for _ in range(args.num_layers)], state_is_tuple=True)\n        bw_cell = tf.nn.rnn_cell.MultiRNNCell([self.lstm_cell() for _ in range(args.num_layers)], state_is_tuple=True)\n        words_used_in_sent = tf.sign(tf.reduce_max(tf.abs(self.input_data), reduction_indices=2))\n        self.length = tf.cast(tf.reduce_sum(words_used_in_sent, reduction_indices=1), tf.int32)\n        print(self.input_data.shape)\n        check = tf.transpose(self.input_data, perm=[1, 0, 2])\n        print(check.shape)\n        #output, _, _ = tf.nn.bidirectional_rnn(fw_cell, bw_cell,\n        #                                       tf.unpack(tf.transpose(self.input_data, perm=[1, 0, 2])),\n        #                                       dtype=tf.float32, sequence_length=self.length)\n        (output, _) = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell,\n                                               self.input_data,\n                                               dtype=tf.float32, sequence_length=self.length)\n        out_fw, out_bw = output\n        print(out_fw.shape)\n        print(out_bw.shape)\n        output = tf.concat([out_fw, out_bw], axis=-1)\n        print(output.shape)\n        \n        weight, bias = self.weight_and_bias(2 * args.rnn_size, args.class_size)\n        print(weight.shape)\n        print(bias.shape)\n        #output = tf.reshape(tf.transpose(tf.pack(output), perm=[1, 0, 2]), [-1, 2 * args.rnn_size])\n        #output = tf.reshape(tf.transpose(tf.stack(output), perm=[1, 0, 2]), [-1, 2 * args.rnn_size])\n        output = tf.reshape(output, [-1, 2 * args.rnn_size])\n        prediction = tf.nn.softmax(tf.matmul(output, weight) + bias)\n        print(prediction.shape)\n        self.prediction = tf.reshape(prediction, [-1, args.sentence_length, args.class_size])\n        print(self.prediction.shape)\n        self.loss = self.cost()\n        optimizer = tf.train.AdamOptimizer(0.003)\n        tvars = tf.trainable_variables()\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), 10)\n        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n    def lstm_cell(self):\n        cell = tf.nn.rnn_cell.LSTMCell(self.args.rnn_size)\n        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.5)\n        return cell\n\n    def cost(self):\n        cross_entropy = self.output_data * tf.log(self.prediction)\n        cross_entropy = -tf.reduce_sum(cross_entropy, reduction_indices=2)\n        mask = tf.sign(tf.reduce_max(tf.abs(self.output_data), reduction_indices=2))\n        cross_entropy *= mask\n        cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1)\n        cross_entropy /= tf.cast(self.length, tf.float32)\n        return tf.reduce_mean(cross_entropy)\n\n    @staticmethod\n    def weight_and_bias(in_size, out_size):\n        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n        bias = tf.constant(0.1, shape=[out_size])\n        return tf.Variable(weight), tf.Variable(bias)\n\n\ndef f1(args, prediction, target, length):\n    tp = np.array([0] * (args.class_size + 1))\n    fp = np.array([0] * (args.class_size + 1))\n    fn = np.array([0] * (args.class_size + 1))\n    target = np.argmax(target, 2)\n    prediction = np.argmax(prediction, 2)\n    for i in range(len(target)):\n        for j in range(length[i]):\n            if target[i, j] == prediction[i, j]:\n                tp[target[i, j]] += 1\n            else:\n                fp[target[i, j]] += 1\n                fn[prediction[i, j]] += 1\n    unnamed_entity = args.class_size - 1\n    for i in range(args.class_size):\n        if i != unnamed_entity:\n            tp[args.class_size] += tp[i]\n            fp[args.class_size] += fp[i]\n            fn[args.class_size] += fn[i]\n    precision = []\n    recall = []\n    fscore = []\n    for i in range(args.class_size + 1):\n        precision.append(tp[i] * 1.0 / (tp[i] + fp[i]))\n        recall.append(tp[i] * 1.0 / (tp[i] + fn[i]))\n        fscore.append(2.0 * precision[i] * recall[i] / (precision[i] + recall[i]))\n    print(fscore)\n    return fscore[args.class_size]\n\n\ndef train(args):\n    train_inp, train_out = get_train_data()\n    print(np.array(train_inp[0]).shape)\n    print(np.array(train_out[0]).shape)\n    test_a_inp, test_a_out = get_test_a_data()\n    test_b_inp, test_b_out = get_test_b_data()\n    model = Model(args)\n    maximum = 0\n    with tf.Session() as sess:\n        #sess.run(tf.initialize_all_variables())\n        sess.run(tf.global_variables_initializer())\n        saver = tf.train.Saver()\n        if args.restore is not None:\n            saver.restore(sess, \'model.ckpt\')\n            print(""model restored"")\n        for e in range(args.epoch):\n            for ptr in range(0, len(train_inp), args.batch_size):\n                sess.run(model.train_op, {model.input_data: train_inp[ptr:ptr + args.batch_size],\n                                          model.output_data: train_out[ptr:ptr + args.batch_size]})\n            if e % 10 == 0:\n                save_path = saver.save(sess, ""output/model.ckpt"")\n                print(""model saved in file: %s"" % save_path)\n            pred, length = sess.run([model.prediction, model.length], {model.input_data: test_a_inp,\n                                                                       model.output_data: test_a_out})\n            print(""epoch %d:"" % e)\n            print(\'test_a score:\')\n            m = f1(args, pred, test_a_out, length)\n            if m > maximum:\n                maximum = m\n                save_path = saver.save(sess, ""output/model_max.ckpt"")\n                print(""max model saved in file: %s"" % save_path)\n                pred, length = sess.run([model.prediction, model.length], {model.input_data: test_b_inp,\n                                                                           model.output_data: test_b_out})\n                print(""test_b score:"")\n                f1(args, pred, test_b_out, length)\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--word_dim\', type=int, help=\'dimension of word vector\', required=True)\nparser.add_argument(\'--sentence_length\', type=int, help=\'max sentence length\', required=True)\nparser.add_argument(\'--class_size\', type=int, help=\'number of classes\', required=True)\nparser.add_argument(\'--rnn_size\', type=int, default=256, help=\'hidden dimension of rnn\')\nparser.add_argument(\'--num_layers\', type=int, default=2, help=\'number of layers in rnn\')\nparser.add_argument(\'--batch_size\', type=int, default=128, help=\'batch size of training\')\nparser.add_argument(\'--epoch\', type=int, default=50, help=\'number of epochs\')\nparser.add_argument(\'--restore\', type=str, default=None, help=""path of saved model"")\ntrain(parser.parse_args())\n'"
sample.py,2,"b'from model import Model\nimport argparse\nimport tensorflow as tf\nimport pickle as pkl\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--word_dim\', type=int, help=\'dimension of word vector\', required=True)\nparser.add_argument(\'--sentence_length\', type=int, help=\'max sentence length\', rquired=True)\nparser.add_argument(\'--class_size\', type=int, help=\'number of classes\', required=True)\nparser.add_argument(\'--rnn_size\', type=int, default=256, help=\'hidden dimension of rnn\')\nparser.add_argument(\'--num_layers\', type=int, default=2, help=\'number of layers in rnn\')\nparser.add_argument(\'--input_embed\', type=str, help=\'location of input pickle embedding\', required=True)\nparser.add_argument(\'--restore\', type=str, help=""path of saved model"", required=True)\nargs = parser.parse_args()\nmodel = Model(args)\ninp = pkl.load(open(args.input_embed, \'rb\'))\nsess = tf.Session()\nsaver = tf.train.Saver()\nsaver.restore(sess, args.restore)\npred = sess.run(model.prediction, {model.input_data: inp})\npkl.dump(pred, open(\'predictions.npy\', \'wb\'))\n'"
data/hindi_util.py,0,"b""from wxconv import WXC\nimport argparse\nimport random\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--format', type=str, help='format of the file text/ssf', required=True)\nparser.add_argument('--input', type=str, help='input file to be converted', required=True)\nparser.add_argument('--dist', type=int, nargs='+', default=[1, 0, 0], help='train:test_a:test_b')\nargs = parser.parse_args()\nwxc = WXC(order='utf2wx')\nif args.format == 'text':\n    open('hin.text', 'w').write(wxc.convert(open(args.input).read()))\nelif args.format == 'ssf':\n    assert len(args.dist) == 3\n\n\n    def tag_extract(string):\n        tag_list = ['PERSON', 'ORGANIZATION', 'LOCATION', 'ENTERTAINMENT', 'FACILITIES', 'ARTIFACT', 'LIVTHINGS',\n                    'LOCOMOTIVE', 'PLANTS', 'MATERIALS', 'DISEASE', 'O']\n        for tag in tag_list:\n            if tag in string:\n                return tag\n\n\n    def write_conll(sentences, output):\n        f = open(output, 'w')\n        for sentence in sentences:\n            for word in sentence:\n                f.write(word + '\\n')\n            f.write('\\n')\n        f.close()\n\n\n    sentences = []\n    sentence = []\n    ner_tag = 'O'\n    for line in open(args.input):\n        if line.startswith('<Sentence'):\n            sentence = []\n        elif line.startswith('</Sentence'):\n            sentences.append(sentence)\n        elif line.startswith('<ENAMEX'):\n            ner_tag = tag_extract(line)\n        else:\n            line = line.split()\n            if len(line) == 0:\n                continue\n            try:\n                index = float(line[0])\n                if index != int(index):\n                    sentence.append(wxc.convert(line[1]) + ' ' + line[2] + ' ' + '.' + ' ' + ner_tag)\n            except ValueError:\n                pass\n            ner_tag = 'O'\n    random.shuffle(sentences)\n    train = args.dist[0] * len(sentences) // sum(args.dist)\n    test_a = args.dist[1] * len(sentences) // sum(args.dist)\n    test_b = len(sentences) - train - test_a\n    train, test_a, test_b = sentences[0:train], sentences[train:train + test_a], sentences[train + test_a:]\n    write_conll(train, 'hin.train')\n    write_conll(test_a, 'hin.test_a')\n    write_conll(test_b, 'hin.test_b')\n"""
data/resize_input.py,0,"b'from __future__ import print_function\nimport argparse\nimport os\n\n\ndef remove_crap(input_file):\n    f = open(input_file)\n    lines = f.readlines()\n    l = list()\n    for line in lines:\n        if ""-DOCSTART-"" in line:\n            pass\n        else:\n            l.append(line)\n    ff = open(\'temp.txt\', \'w\')\n    ff.writelines(l)\n    ff.close()\n\n\ndef modify_data_size(output_file, trim):\n    final_list = list()\n    l = list()\n    temp_len = 0\n    count = 0\n    for line in open(\'temp.txt\', \'r\'):\n        if line in [\'\\n\', \'\\r\\n\']:\n            if temp_len == 0:\n                l = []\n            elif temp_len > trim:\n                count += 1\n                l = []\n                temp_len = 0\n            else:\n                l.append(line)\n                final_list.append(l)\n                l = []\n                temp_len = 0\n        else:\n            l.append(line)\n            temp_len += 1\n    f = open(output_file, \'w\')\n    for i in final_list:\n        f.writelines(i)\n    f.close()\n    print(\'%d sentences trimmed out of %d total sentences\' % (count, len(final_list)))\n    os.system(\'rm temp.txt\')\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--input\', type=str, help=\'input file location\', required=True)\nparser.add_argument(\'--output\', type=str, help=\'output file location\', required=True)\nparser.add_argument(\'--trim\', type=int, help=\'trimmed sentence length\', required=True)\nargs = parser.parse_args()\nremove_crap(args.input)\nmodify_data_size(args.output, args.trim)\n'"
embeddings/get_conll_embeddings.py,0,"b'from __future__ import print_function\nimport numpy as np\nimport pickle as pkl\nimport sys\nimport argparse\nfrom wordvec_model import WordVec\nfrom glove_model import GloveVec\nfrom rnnvec_model import RnnVec\n\n\ndef find_max_length(file_name):\n    temp_len = 0\n    max_length = 0\n    for line in open(file_name):\n        if line in [\'\\n\', \'\\r\\n\']:\n            if temp_len > max_length:\n                max_length = temp_len\n            temp_len = 0\n        else:\n            temp_len += 1\n    return max_length\n\n\ndef pos(tag):\n    one_hot = np.zeros(5)\n    if tag == \'NN\' or tag == \'NNS\':\n        one_hot[0] = 1\n    elif tag == \'FW\':\n        one_hot[1] = 1\n    elif tag == \'NNP\' or tag == \'NNPS\':\n        one_hot[2] = 1\n    elif \'VB\' in tag:\n        one_hot[3] = 1\n    else:\n        one_hot[4] = 1\n    return one_hot\n\n\ndef chunk(tag):\n    one_hot = np.zeros(5)\n    if \'NP\' in tag:\n        one_hot[0] = 1\n    elif \'VP\' in tag:\n        one_hot[1] = 1\n    elif \'PP\' in tag:\n        one_hot[2] = 1\n    elif tag == \'O\':\n        one_hot[3] = 1\n    else:\n        one_hot[4] = 1\n    return one_hot\n\n\ndef capital(word):\n    if ord(\'A\') <= ord(word[0]) <= ord(\'Z\'):\n        return np.array([1])\n    else:\n        return np.array([0])\n\n\ndef get_input(model, word_dim, input_file, output_embed, output_tag, sentence_length=-1):\n    print(\'processing %s\' % input_file)\n    word = []\n    tag = []\n    sentence = []\n    sentence_tag = []\n    if sentence_length == -1:\n        max_sentence_length = find_max_length(input_file)\n    else:\n        max_sentence_length = sentence_length\n    sentence_length = 0\n    print(""max sentence length is %d"" % max_sentence_length)\n    for line in open(input_file):\n        if line in [\'\\n\', \'\\r\\n\']:\n            for _ in range(max_sentence_length - sentence_length):\n                tag.append(np.array([0] * 5))\n                temp = np.array([0 for _ in range(word_dim + 11)])\n                word.append(temp)\n            sentence.append(word)\n            sentence_tag.append(np.array(tag))\n            sentence_length = 0\n            word = []\n            tag = []\n        else:\n            assert (len(line.split()) == 4)\n            sentence_length += 1\n            temp = model[line.split()[0]]\n            assert len(temp) == word_dim\n            temp = np.append(temp, pos(line.split()[1]))  # adding pos embeddings\n            temp = np.append(temp, chunk(line.split()[2]))  # adding chunk embeddings\n            temp = np.append(temp, capital(line.split()[0]))  # adding capital embedding\n            word.append(temp)\n            t = line.split()[3]\n            # Five classes 0-None,1-Person,2-Location,3-Organisation,4-Misc\n            if t.endswith(\'PER\'):\n                tag.append(np.array([1, 0, 0, 0, 0]))\n            elif t.endswith(\'LOC\'):\n                tag.append(np.array([0, 1, 0, 0, 0]))\n            elif t.endswith(\'ORG\'):\n                tag.append(np.array([0, 0, 1, 0, 0]))\n            elif t.endswith(\'MISC\'):\n                tag.append(np.array([0, 0, 0, 1, 0]))\n            elif t.endswith(\'O\'):\n                tag.append(np.array([0, 0, 0, 0, 1]))\n            else:\n                print(""error in input tag {%s}"" % t)\n                sys.exit(0)\n    assert (len(sentence) == len(sentence_tag))\n    pkl.dump(sentence, open(output_embed, \'wb\'))\n    pkl.dump(sentence_tag, open(output_tag, \'wb\'))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--train\', type=str, help=\'train file location\', required=True)\n    parser.add_argument(\'--test_a\', type=str, help=\'test_a file location\', required=True)\n    parser.add_argument(\'--test_b\', type=str, help=\'test_b location\', required=True)\n    parser.add_argument(\'--sentence_length\', type=int, default=-1, help=\'max sentence length\')\n    parser.add_argument(\'--use_model\', type=str, help=\'model location\', required=True)\n    parser.add_argument(\'--model_dim\', type=int, help=\'model dimension of words\', required=True)\n    args = parser.parse_args()\n    trained_model = pkl.load(open(args.use_model, \'rb\'))\n    get_input(trained_model, args.model_dim, args.train, \'train_embed.pkl\', \'train_tag.pkl\',\n              sentence_length=args.sentence_length)\n    get_input(trained_model, args.model_dim, args.test_a, \'test_a_embed.pkl\', \'test_a_tag.pkl\',\n              sentence_length=args.sentence_length)\n    get_input(trained_model, args.model_dim, args.test_b, \'test_b_embed.pkl\', \'test_b_tag.pkl\',\n              sentence_length=args.sentence_length)\n'"
embeddings/get_icon_embeddings.py,0,"b'from __future__ import print_function\nimport numpy as np\nimport pickle as pkl\nimport sys\nimport argparse\nfrom wordvec_model import WordVec\nfrom glove_model import GloveVec\nfrom rnnvec_model import RnnVec\n\n\ndef find_max_length(file_name):\n    temp_len = 0\n    max_length = 0\n    for line in open(file_name):\n        if line in [\'\\n\', \'\\r\\n\']:\n            if temp_len > max_length:\n                max_length = temp_len\n            temp_len = 0\n        else:\n            temp_len += 1\n    return max_length\n\n\ndef pos(tag):\n    one_hot = np.zeros(5)\n    if tag == \'NN\' or tag == \'NNS\':\n        one_hot[0] = 1\n    elif tag == \'FW\':\n        one_hot[1] = 1\n    elif tag == \'NNP\' or tag == \'NNPS\':\n        one_hot[2] = 1\n    elif \'VB\' in tag:\n        one_hot[3] = 1\n    else:\n        one_hot[4] = 1\n    return one_hot\n\n\ndef chunk(tag):\n    one_hot = np.zeros(5)\n    if \'NP\' in tag:\n        one_hot[0] = 1\n    elif \'VP\' in tag:\n        one_hot[1] = 1\n    elif \'PP\' in tag:\n        one_hot[2] = 1\n    elif tag == \'O\':\n        one_hot[3] = 1\n    else:\n        one_hot[4] = 1\n    return one_hot\n\n\ndef capital(word):\n    if ord(\'A\') <= ord(word[0]) <= ord(\'Z\'):\n        return np.array([1])\n    else:\n        return np.array([0])\n\n\ndef get_input(model, word_dim, input_file, output_embed, output_tag, sentence_length=-1):\n    print(\'processing %s\' % input_file)\n    word = []\n    tag = []\n    sentence = []\n    sentence_tag = []\n    if sentence_length == -1:\n        max_sentence_length = find_max_length(input_file)\n    else:\n        max_sentence_length = sentence_length\n    sentence_length = 0\n    print(""max sentence length is %d"" % max_sentence_length)\n    for line in open(input_file):\n        if line in [\'\\n\', \'\\r\\n\']:\n            for _ in range(max_sentence_length - sentence_length):\n                tag.append(np.array([0] * 12))\n                temp = np.array([0 for _ in range(word_dim + 11)])\n                word.append(temp)\n            sentence.append(word)\n            sentence_tag.append(np.array(tag))\n            sentence_length = 0\n            word = []\n            tag = []\n        else:\n            assert (len(line.split()) == 4)\n            sentence_length += 1\n            temp = model[line.split()[0]]\n            assert len(temp) == word_dim\n            temp = np.append(temp, pos(line.split()[1]))  # adding pos embeddings\n            temp = np.append(temp, chunk(line.split()[2]))  # adding chunk embeddings\n            temp = np.append(temp, capital(line.split()[0]))  # adding capital embedding\n            word.append(temp)\n            t = line.split()[3]\n            # Five classes 0-None,1-Person,2-Location,3-Organisation,4-Misc\n            if t == \'PERSON\':\n                tag.append(np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n            elif t == \'ORGANIZATION\':\n                tag.append(np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n            elif t == \'LOCATION\':\n                tag.append(np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n            elif t == \'ENTERTAINMENT\':\n                tag.append(np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]))\n            elif t == \'FACILITIES\':\n                tag.append(np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))\n            elif t == \'ARTIFACT\':\n                tag.append(np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))\n            elif t == \'LIVTHINGS\':\n                tag.append(np.array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]))\n            elif t == \'LOCOMOTIVE\':\n                tag.append(np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))\n            elif t == \'PLANTS\':\n                tag.append(np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))\n            elif t == \'MATERIALS\':\n                tag.append(np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]))\n            elif t == \'DISEASE\':\n                tag.append(np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))\n            elif t == \'O\':\n                tag.append(np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))\n            else:\n                print(t)\n                print(""error in input"")\n                sys.exit(0)\n    assert (len(sentence) == len(sentence_tag))\n    pkl.dump(sentence, open(output_embed, \'wb\'))\n    pkl.dump(sentence_tag, open(output_tag, \'wb\'))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--train\', type=str, help=\'train file location\', required=True)\n    parser.add_argument(\'--test_a\', type=str, help=\'test_a file location\', required=True)\n    parser.add_argument(\'--test_b\', type=str, help=\'test_b location\', required=True)\n    parser.add_argument(\'--sentence_length\', type=int, default=-1, help=\'max sentence length\')\n    parser.add_argument(\'--use_model\', type=str, help=\'model location\', required=True)\n    parser.add_argument(\'--model_dim\', type=int, help=\'model dimension of words\', required=True)\n    args = parser.parse_args()\n    trained_model = pkl.load(open(args.use_model, \'rb\'))\n    get_input(trained_model, args.model_dim, args.train, \'train_embed.pkl\', \'train_tag.pkl\',\n              sentence_length=args.sentence_length)\n    get_input(trained_model, args.model_dim, args.test_a, \'test_a_embed.pkl\', \'test_a_tag.pkl\',\n              sentence_length=args.sentence_length)\n    get_input(trained_model, args.model_dim, args.test_b, \'test_b_embed.pkl\', \'test_b_tag.pkl\',\n              sentence_length=args.sentence_length)\n'"
embeddings/glove_model.py,0,"b""from __future__ import print_function\nfrom random_vec import RandomVec\nimport numpy as np\nimport pickle as pkl\nimport argparse\nimport os\n\n\nclass GloveVec:\n    def __init__(self, args):\n        self.model = {}\n        self.rand_model = RandomVec(args.dimension)\n        if args.restore is None:\n            os.chdir(args.glove_path)\n            os.system('bash demo.sh ' + args.corpus + ' ' + str(args.dimension))\n            os.system('rm -r build *.bin vocab.txt')\n            path = 'vectors.txt'\n        else:\n            path = args.restore\n        invalid = 0\n        for line in open(path):\n            line = line.split()\n            word = line[0]\n            vector = np.array([float(val) for val in line[1:]])\n            if len(vector) != args.dimension:\n                invalid += 1\n                continue\n            self.model[word] = vector\n        print('invalid entries %d' % invalid)\n        os.system('rm ' + path)\n        os.chdir('..')\n\n    def __getitem__(self, word):\n        word = word.lower()\n        try:\n            return self.model[word]\n        except KeyError:\n            return self.rand_model[word]\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--corpus', type=str, help='corpus location', required=True)\n    parser.add_argument('--dimension', type=int, help='vector dimension', required=True)\n    parser.add_argument('--restore', type=str, default=None, help='pre-trained glove vectors.txt')\n    parser.add_argument('--glove_path', type=str, help='path to glove c code', required=True)\n    args = parser.parse_args()\n    model = GloveVec(args)\n    pkl.dump(model, open('glovevec_model_' + str(args.dimension) + '.pkl', 'wb'))\n"""
embeddings/random_vec.py,0,"b'from random import random\nimport numpy as np\n\n\nclass RandomVec:\n    def __init__(self, dim):\n        self.dim = dim\n        self.vocab = {}\n        self.vec = []\n\n    def __getitem__(self, word):\n        ind = self.vocab.get(word, -1)\n        if ind == -1:\n            new_vec = np.array([random() for i in range(self.dim)])\n            self.vocab[word] = len(self.vocab)\n            self.vec.append(new_vec)\n            return new_vec\n        else:\n            return self.vec[ind]\n'"
embeddings/rnnvec_model.py,21,"b'from __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nfrom random_vec import RandomVec\nfrom collections import Counter\nimport pickle as pkl\nimport argparse\n\n\nclass RnnVec:\n    def __init__(self, args):\n        self.rand_model = RandomVec(args.rnn_size)\n        corpus = open(args.corpus, \'r\').read().lower().split()\n        word_count = [[\'unk\', 0]]\n        word_count.extend(Counter(corpus).most_common(args.vocab_size - 1))\n        word_count.sort(key=lambda x: x[1], reverse=True)\n        self.vocab = {}\n        for word, freq in word_count:\n            self.vocab[word] = len(self.vocab)\n        print(\'vocabulary built\')\n        int_corpus = []\n        for word in corpus:\n            int_corpus.append(self.vocab.get(word, 0))\n        input_data = np.zeros(len(int_corpus))\n        output_data = np.zeros((len(int_corpus), 2 * args.window))\n        for i in range(len(int_corpus)):\n            input_data[i] = int_corpus[i]\n            past = int_corpus[i - args.window:i]\n            future = int_corpus[i + 1:i + 1 + args.window]\n            total = past + future\n            total.extend([0] * (2 * args.window - len(total)))\n            output_data[i] = total\n        print(\'data formed\')\n        del corpus\n        del word_count\n        del int_corpus\n        num_sentence = (len(input_data) + args.sentence_length - 1) // args.sentence_length\n        input_data = np.resize(input_data, (num_sentence, args.sentence_length))\n        output_data = np.resize(output_data, (num_sentence, args.sentence_length, 2 * args.window))\n        print(\'training\')\n        cell = tf.nn.rnn_cell.BasicLSTMCell(args.rnn_size, state_is_tuple=True)\n        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * args.num_layers, state_is_tuple=True)\n        input_tensor = tf.placeholder(tf.int32, [args.batch_size, args.sentence_length])\n        output_tensor = tf.placeholder(tf.int32, [args.batch_size, args.sentence_length, 2 * args.window])\n        initial_state = cell.zero_state(args.batch_size, tf.float32)\n        with tf.variable_scope(\'rnnlm\'):\n            weights = tf.get_variable(""softmax_w"", [args.vocab_size, args.rnn_size])\n            bias = tf.get_variable(""softmax_b"", [args.vocab_size])\n            with tf.device(""/cpu:0""):\n                embedding = tf.get_variable(""embedding"", [args.vocab_size, args.rnn_size])\n                inputs = tf.nn.embedding_lookup(embedding, input_tensor)\n        outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)\n        output = tf.reshape(outputs, [-1, args.rnn_size])\n        loss = tf.nn.sampled_softmax_loss(weights, bias, output, tf.reshape(output_tensor, [-1, 2 * args.window]), 128,\n                                          args.vocab_size, num_true=2 * args.window)\n        cost = tf.reduce_mean(loss)\n        lr = tf.Variable(0.0, trainable=False)\n        tvars = tf.trainable_variables()\n        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), 5)\n        optimizer = tf.train.AdamOptimizer(lr)\n        train_op = optimizer.apply_gradients(zip(grads, tvars))\n        sess = tf.Session()\n        sess.run(tf.initialize_all_variables())\n        for e in range(3):\n            state = sess.run(initial_state)\n            for i in range(0, input_data.shape[0], args.batch_size):\n                if i + args.batch_size > input_data.shape[0]:\n                    i = input_data.shape[0] - args.batch_size\n                _, error, state = sess.run([train_op, cost, last_state],\n                                           feed_dict={input_tensor: input_data[i:i + args.batch_size],\n                                                      output_tensor: output_data[i:i + args.batch_size],\n                                                      initial_state: state,\n                                                      lr: .01 * (.3 ** e)})\n                print(\'batch percentage = %f, cost = %f, epoch %d\' % (i * 100 / input_data.shape[0], error, e))\n        self.embeddings = sess.run(embedding)\n\n    def __getitem__(self, word):\n        word = word.lower()\n        try:\n            return self.embeddings[self.vocab[word]]\n        except KeyError:\n            return self.rand_model[word]\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--corpus\', type=str, help=\'corpus location\', required=True)\n    parser.add_argument(\'--window\', type=int, default=5, help=\'window size\')\n    parser.add_argument(\'--vocab_size\', type=int, default=50000, help=\'number of threads\')\n    parser.add_argument(\'--batch_size\', type=int, default=256, help=\'batch size of training\')\n    parser.add_argument(\'--sentence_length\', type=int, help=\'sentence length\', required=True)\n    parser.add_argument(\'--rnn_size\', type=int, default=128, help=\'hidden layer size of rnn\')\n    parser.add_argument(\'--num_layers\', type=int, default=2, help=\'number of layers in the rnn\')\n    args = parser.parse_args()\n    model = RnnVec(args)\n    pkl.dump(model, open(\'rnnvec_model_\' + str(args.rnn_size) + \'.pkl\', \'wb\'))\n'"
embeddings/wordvec_model.py,0,"b""from __future__ import print_function\nfrom gensim.models.word2vec import Word2Vec\nfrom random_vec import RandomVec\nimport pickle as pkl\nimport argparse\n\n\nclass WordVec:\n    def __init__(self, args):\n        print('processing corpus')\n        if args.restore is None:\n            corpus = open(args.corpus, 'r').read().lower().split()\n            sentences = []\n            sentence = []\n            length = 0\n            for word in corpus:\n                sentence.append(word)\n                length += 1\n                if length == args.sentence_length:\n                    sentences.append(sentence)\n                    sentence = []\n                    length = 0\n            if length != 0:\n                sentences.append(sentence)\n            print('training')\n            self.wvec_model = Word2Vec(sentences=sentences, size=args.dimension, window=args.window,\n                                       workers=args.workers,\n                                       sg=args.sg,\n                                       batch_words=args.batch_size, min_count=1, max_vocab_size=args.vocab_size)\n        else:\n            self.wvec_model = Word2Vec.load_word2vec_format(args.restore, binary=True)\n        self.rand_model = RandomVec(args.dimension)\n\n    def __getitem__(self, word):\n        word = word.lower()\n        try:\n            return self.wvec_model[word]\n        except KeyError:\n            return self.rand_model[word]\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--corpus', type=str, help='corpus location', required=True)\n    parser.add_argument('--dimension', type=int, help='vector dimension', required=True)\n    parser.add_argument('--window', type=int, default=5, help='window size')\n    parser.add_argument('--vocab_size', type=int, help='vocabulary size', required=True)\n    parser.add_argument('--workers', type=int, default=3, help='number of threads')\n    parser.add_argument('--sg', type=int, default=1, help='if skipgram 1 if cbow 0')\n    parser.add_argument('--batch_size', type=int, default=10000, help='batch size of training')\n    parser.add_argument('--sentence_length', type=int, help='sentence length', required=True)\n    parser.add_argument('--restore', type=str, default=None, help='word2vec format save')\n    args = parser.parse_args()\n    model = WordVec(args)\n    pkl.dump(model, open('wordvec_model_' + str(args.dimension) + '.pkl', 'wb'))\n"""
thirdparty/GloVe-1.2/eval/python/evaluate.py,0,"b'import argparse\nimport numpy as np\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--vocab_file\', default=\'vocab.txt\', type=str)\n    parser.add_argument(\'--vectors_file\', default=\'vectors.txt\', type=str)\n    args = parser.parse_args()\n\n    with open(args.vocab_file, \'r\') as f:\n        words = [x.rstrip().split(\' \')[0] for x in f.readlines()]\n    with open(args.vectors_file, \'r\') as f:\n        vectors = {}\n        for line in f:\n            vals = line.rstrip().split(\' \')\n            vectors[vals[0]] = map(float, vals[1:])\n\n    vocab_size = len(words)\n    vocab = {w: idx for idx, w in enumerate(words)}\n    ivocab = {idx: w for idx, w in enumerate(words)}\n\n    vector_dim = len(vectors[ivocab[0]])\n    W = np.zeros((vocab_size, vector_dim))\n    for word, v in vectors.iteritems():\n        if word == \'<unk>\':\n            continue\n        W[vocab[word], :] = v\n\n    # normalize each word vector to unit variance\n    W_norm = np.zeros(W.shape)\n    d = (np.sum(W ** 2, 1) ** (0.5))\n    W_norm = (W.T / d).T\n    evaluate_vectors(W_norm, vocab, ivocab)\n\ndef evaluate_vectors(W, vocab, ivocab):\n    """"""Evaluate the trained word vectors on a variety of tasks""""""\n\n    filenames = [\n        \'capital-common-countries.txt\', \'capital-world.txt\', \'currency.txt\',\n        \'city-in-state.txt\', \'family.txt\', \'gram1-adjective-to-adverb.txt\',\n        \'gram2-opposite.txt\', \'gram3-comparative.txt\', \'gram4-superlative.txt\',\n        \'gram5-present-participle.txt\', \'gram6-nationality-adjective.txt\',\n        \'gram7-past-tense.txt\', \'gram8-plural.txt\', \'gram9-plural-verbs.txt\',\n        ]\n    prefix = \'./eval/question-data/\'\n\n    # to avoid memory overflow, could be increased/decreased\n    # depending on system and vocab size\n    split_size = 100\n\n    correct_sem = 0; # count correct semantic questions\n    correct_syn = 0; # count correct syntactic questions\n    correct_tot = 0 # count correct questions\n    count_sem = 0; # count all semantic questions\n    count_syn = 0; # count all syntactic questions\n    count_tot = 0 # count all questions\n    full_count = 0 # count all questions, including those with unknown words\n\n    for i in xrange(len(filenames)):\n        with open(\'%s/%s\' % (prefix, filenames[i]), \'r\') as f:\n            full_data = [line.rstrip().split(\' \') for line in f]\n            full_count += len(full_data)\n            data = [x for x in full_data if all(word in vocab for word in x)]\n\n        indices = np.array([[vocab[word] for word in row] for row in data])\n        ind1, ind2, ind3, ind4 = indices.T\n\n        predictions = np.zeros((len(indices),))\n        num_iter = int(np.ceil(len(indices) / float(split_size)))\n        for j in xrange(num_iter):\n            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n\n            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n                +  W[ind3[subset], :])\n            #cosine similarity if input W has been normalized\n            dist = np.dot(W, pred_vec.T)\n\n            for k in xrange(len(subset)):\n                dist[ind1[subset[k]], k] = -np.Inf\n                dist[ind2[subset[k]], k] = -np.Inf\n                dist[ind3[subset[k]], k] = -np.Inf\n\n            # predicted word index\n            predictions[subset] = np.argmax(dist, 0).flatten()\n\n        val = (ind4 == predictions) # correct predictions\n        count_tot = count_tot + len(ind1)\n        correct_tot = correct_tot + sum(val)\n        if i < 5:\n            count_sem = count_sem + len(ind1)\n            correct_sem = correct_sem + sum(val)\n        else:\n            count_syn = count_syn + len(ind1)\n            correct_syn = correct_syn + sum(val)\n\n        print(""%s:"" % filenames[i])\n        print(\'ACCURACY TOP1: %.2f%% (%d/%d)\' %\n            (np.mean(val) * 100, np.sum(val), len(val)))\n\n    print(\'Questions seen/total: %.2f%% (%d/%d)\' %\n        (100 * count_tot / float(full_count), count_tot, full_count))\n    print(\'Semantic accuracy: %.2f%%  (%i/%i)\' %\n        (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n    print(\'Syntactic accuracy: %.2f%%  (%i/%i)\' %\n        (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n    print(\'Total accuracy: %.2f%%  (%i/%i)\' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
