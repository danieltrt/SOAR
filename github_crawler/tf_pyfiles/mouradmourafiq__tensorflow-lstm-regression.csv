file_path,api_count,code
__init__.py,0,b''
data_processing.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport pandas as pd\n\n\ndef x_sin(x):\n    return x * np.sin(x)\n\n\ndef sin_cos(x):\n    return pd.DataFrame(dict(a=np.sin(x), b=np.cos(x)), index=x)\n\n\ndef rnn_data(data, time_steps, labels=False):\n    """"""\n    creates new data frame based on previous observation\n      * example:\n        l = [1, 2, 3, 4, 5]\n        time_steps = 2\n        -> labels == False [[1, 2], [2, 3], [3, 4]]\n        -> labels == True [3, 4, 5]\n    """"""\n    rnn_df = []\n    for i in range(len(data) - time_steps):\n        if labels:\n            try:\n                rnn_df.append(data.iloc[i + time_steps].as_matrix())\n            except AttributeError:\n                rnn_df.append(data.iloc[i + time_steps])\n        else:\n            data_ = data.iloc[i: i + time_steps].as_matrix()\n            rnn_df.append(data_ if len(data_.shape) > 1 else [[i] for i in data_])\n\n    return np.array(rnn_df, dtype=np.float32)\n\n\ndef split_data(data, val_size=0.1, test_size=0.1):\n    """"""\n    splits data to training, validation and testing parts\n    """"""\n    ntest = int(round(len(data) * (1 - test_size)))\n    nval = int(round(len(data.iloc[:ntest]) * (1 - val_size)))\n\n    df_train, df_val, df_test = data.iloc[:nval], data.iloc[nval:ntest], data.iloc[ntest:]\n\n    return df_train, df_val, df_test\n\n\ndef prepare_data(data, time_steps, labels=False, val_size=0.1, test_size=0.1):\n    """"""\n    Given the number of `time_steps` and some data,\n    prepares training, validation and test data for an lstm cell.\n    """"""\n    df_train, df_val, df_test = split_data(data, val_size, test_size)\n    return (rnn_data(df_train, time_steps, labels=labels),\n            rnn_data(df_val, time_steps, labels=labels),\n            rnn_data(df_test, time_steps, labels=labels))\n\n\ndef load_csvdata(rawdata, time_steps, seperate=False):\n    data = rawdata\n    if not isinstance(data, pd.DataFrame):\n        data = pd.DataFrame(data)\n\n    train_x, val_x, test_x = prepare_data(data[\'a\'] if seperate else data, time_steps)\n    train_y, val_y, test_y = prepare_data(data[\'b\'] if seperate else data, time_steps, labels=True)\n    return dict(train=train_x, val=val_x, test=test_x), dict(train=train_y, val=val_y, test=test_y)\n\n\ndef generate_data(fct, x, time_steps, seperate=False):\n    """"""generates data with based on a function fct""""""\n    data = fct(x)\n    if not isinstance(data, pd.DataFrame):\n        data = pd.DataFrame(data)\n    train_x, val_x, test_x = prepare_data(data[\'a\'] if seperate else data, time_steps)\n    train_y, val_y, test_y = prepare_data(data[\'b\'] if seperate else data, time_steps, labels=True)\n    return dict(train=train_x, val=val_x, test=test_x), dict(train=train_y, val=val_y, test=test_y)\n'"
lstm.py,9,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.contrib import learn as tflearn\nfrom tensorflow.contrib import layers as tflayers\n\n\ndef lstm_model(num_units, rnn_layers, dense_layers=None, learning_rate=0.1, optimizer=\'Adagrad\'):\n    """"""\n    Creates a deep model based on:\n        * stacked lstm cells\n        * an optional dense layers\n    :param num_units: the size of the cells.\n    :param rnn_layers: list of int or dict\n                         * list of int: the steps used to instantiate the `BasicLSTMCell` cell\n                         * list of dict: [{steps: int, keep_prob: int}, ...]\n    :param dense_layers: list of nodes for each layer\n    :return: the model definition\n    """"""\n\n    def lstm_cells(layers):\n        if isinstance(layers[0], dict):\n            return [tf.contrib.rnn.DropoutWrapper(\n                tf.contrib.rnn.BasicLSTMCell(\n                    layer[\'num_units\'], state_is_tuple=True\n                ),\n                layer[\'keep_prob\']\n            ) if layer.get(\'keep_prob\') else tf.contrib.rnn.BasicLSTMCell(\n                    layer[\'num_units\'],\n                    state_is_tuple=True\n                ) for layer in layers\n            ]\n        return [tf.contrib.rnn.BasicLSTMCell(steps, state_is_tuple=True) for steps in layers]\n\n    def dnn_layers(input_layers, layers):\n        if layers and isinstance(layers, dict):\n            return tflayers.stack(input_layers, tflayers.fully_connected,\n                                  layers[\'layers\'],\n                                  activation=layers.get(\'activation\'),\n                                  dropout=layers.get(\'dropout\'))\n        elif layers:\n            return tflayers.stack(input_layers, tflayers.fully_connected, layers)\n        else:\n            return input_layers\n\n    def _lstm_model(X, y):\n        stacked_lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells(rnn_layers), state_is_tuple=True)\n        x_ = tf.unstack(X, axis=1, num=num_units)\n        output, layers = tf.contrib.rnn.static_rnn(stacked_lstm, x_, dtype=dtypes.float32)\n        output = dnn_layers(output[-1], dense_layers)\n        prediction, loss = tflearn.models.linear_regression(output, y)\n        train_op = tf.contrib.layers.optimize_loss(\n            loss, tf.contrib.framework.get_global_step(), optimizer=optimizer,\n            learning_rate=learning_rate)\n        return prediction, loss, train_op\n\n    return _lstm_model\n'"
polyaxon_experiement.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport polyaxon as plx\n\n\ndef create_experiment(output_dir, X, y, train_steps=1000, num_units=7, output_units=1, num_layers=1):\n    """"""Creates an experiment using LSTM architecture for timeseries regression problem.""""""\n\n    config = {\n        \'name\': \'time_series\',\n        \'output_dir\': output_dir,\n        \'eval_every_n_steps\': 100,\n        \'train_steps_per_iteration\': 100,\n        \'train_steps\': train_steps,\n        \'run_config\': {\'save_checkpoints_steps\': 100},\n        \'train_input_data_config\': {\n            \'input_type\': plx.configs.InputDataConfig.NUMPY,\n            \'pipeline_config\': {\'name\': \'train\', \'batch_size\': 64, \'num_epochs\': None,\n                                \'shuffle\': False},\n            \'x\': {\'x\': X[\'train\']},\n            \'y\': y[\'train\']\n        },\n        \'eval_input_data_config\': {\n            \'input_type\': plx.configs.InputDataConfig.NUMPY,\n            \'pipeline_config\': {\'name\': \'eval\', \'batch_size\': 32, \'num_epochs\': None,\n                                \'shuffle\': False},\n            \'x\': {\'x\': X[\'val\']},\n            \'y\': y[\'val\']\n        },\n        \'estimator_config\': {\'output_dir\': output_dir},\n        \'model_config\': {\n            \'module\': \'Regressor\',\n            \'loss_config\': {\'module\': \'mean_squared_error\'},\n            \'eval_metrics_config\': [{\'module\': \'streaming_root_mean_squared_error\'},\n                                    {\'module\': \'streaming_mean_absolute_error\'}],\n            \'optimizer_config\': {\'module\': \'adagrad\', \'learning_rate\': 0.1},\n            \'graph_config\': {\n                \'name\': \'regressor\',\n                \'features\': [\'x\'],\n                \'definition\': [\n                    (plx.layers.LSTM, {\'num_units\': num_units, \'num_layers\': num_layers}),\n                    (plx.layers.FullyConnected, {\'num_units\': output_units}),\n                ]\n            }\n        }\n    }\n    experiment_config = plx.configs.ExperimentConfig.read_configs(config)\n    return plx.experiments.create_experiment(experiment_config)\n'"
