file_path,api_count,code
DeepSpeech.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nif __name__ == '__main__':\n    try:\n        from deepspeech_training import train as ds_train\n    except ImportError:\n        print('Training package is not installed. See training documentation.')\n        raise\n\n    ds_train.run_script()\n"""
evaluate.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nif __name__ == '__main__':\n    try:\n        from deepspeech_training import evaluate as ds_evaluate\n    except ImportError:\n        print('Training package is not installed. See training documentation.')\n        raise\n\n    ds_evaluate.run_script()\n"""
evaluate_tflite.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport absl.app\nimport argparse\nimport numpy as np\nimport wave\nimport csv\nimport os\nimport sys\n\nfrom deepspeech import Model\nfrom deepspeech_training.util.evaluate_tools import calculate_and_print_report\nfrom deepspeech_training.util.flags import create_flags\nfrom functools import partial\nfrom multiprocessing import JoinableQueue, Process, cpu_count, Manager\nfrom six.moves import zip, range\n\nr\'\'\'\nThis module should be self-contained:\n  - build libdeepspeech.so with TFLite:\n    - bazel build [...] --define=runtime=tflite [...] //native_client:libdeepspeech.so\n  - make -C native_client/python/ TFDIR=... bindings\n  - setup a virtualenv\n  - pip install native_client/python/dist/deepspeech*.whl\n  - pip install -r requirements_eval_tflite.txt\n\nThen run with a TF Lite model, a scorer and a CSV test file\n\'\'\'\n\ndef tflite_worker(model, scorer, queue_in, queue_out, gpu_mask):\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(gpu_mask)\n    ds = Model(model)\n    ds.enableExternalScorer(scorer)\n\n    while True:\n        try:\n            msg = queue_in.get()\n\n            filename = msg[\'filename\']\n            fin = wave.open(filename, \'rb\')\n            audio = np.frombuffer(fin.readframes(fin.getnframes()), np.int16)\n            fin.close()\n\n            decoded = ds.stt(audio)\n\n            queue_out.put({\'wav\': filename, \'prediction\': decoded, \'ground_truth\': msg[\'transcript\']})\n        except FileNotFoundError as ex:\n            print(\'FileNotFoundError: \', ex)\n\n        print(queue_out.qsize(), end=\'\\r\') # Update the current progress\n        queue_in.task_done()\n\ndef main(args, _):\n    manager = Manager()\n    work_todo = JoinableQueue()   # this is where we are going to store input data\n    work_done = manager.Queue()  # this where we are gonna push them out\n\n    processes = []\n    for i in range(args.proc):\n        worker_process = Process(target=tflite_worker, args=(args.model, args.scorer, work_todo, work_done, i), daemon=True, name=\'tflite_process_{}\'.format(i))\n        worker_process.start()        # Launch reader() as a separate python process\n        processes.append(worker_process)\n\n    print([x.name for x in processes])\n\n    wavlist = []\n    ground_truths = []\n    predictions = []\n    losses = []\n    wav_filenames = []\n\n    with open(args.csv, \'r\') as csvfile:\n        csvreader = csv.DictReader(csvfile)\n        count = 0\n        for row in csvreader:\n            count += 1\n            # Relative paths are relative to the folder the CSV file is in\n            if not os.path.isabs(row[\'wav_filename\']):\n                row[\'wav_filename\'] = os.path.join(os.path.dirname(args.csv), row[\'wav_filename\'])\n            work_todo.put({\'filename\': row[\'wav_filename\'], \'transcript\': row[\'transcript\']})\n            wav_filenames.extend(row[\'wav_filename\'])\n\n    print(\'Totally %d wav entries found in csv\\n\' % count)\n    work_todo.join()\n    print(\'\\nTotally %d wav file transcripted\' % work_done.qsize())\n\n    while not work_done.empty():\n        msg = work_done.get()\n        losses.append(0.0)\n        ground_truths.append(msg[\'ground_truth\'])\n        predictions.append(msg[\'prediction\'])\n        wavlist.append(msg[\'wav\'])\n\n    # Print test summary\n    _ = calculate_and_print_report(wav_filenames, ground_truths, predictions, losses, args.csv)\n\n    if args.dump:\n        with open(args.dump + \'.txt\', \'w\') as ftxt, open(args.dump + \'.out\', \'w\') as fout:\n            for wav, txt, out in zip(wavlist, ground_truths, predictions):\n                ftxt.write(\'%s %s\\n\' % (wav, txt))\n                fout.write(\'%s %s\\n\' % (wav, out))\n            print(\'Reference texts dumped to %s.txt\' % args.dump)\n            print(\'Transcription   dumped to %s.out\' % args.dump)\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Computing TFLite accuracy\')\n    parser.add_argument(\'--model\', required=True,\n                        help=\'Path to the model (protocol buffer binary file)\')\n    parser.add_argument(\'--scorer\', required=True,\n                        help=\'Path to the external scorer file\')\n    parser.add_argument(\'--csv\', required=True,\n                        help=\'Path to the CSV source file\')\n    parser.add_argument(\'--proc\', required=False, default=cpu_count(), type=int,\n                        help=\'Number of processes to spawn, defaulting to number of CPUs\')\n    parser.add_argument(\'--dump\', required=False,\n                        help=\'Path to dump the results as text file, with one line for each wav: ""wav transcription"".\')\n    args, unknown = parser.parse_known_args()\n    # Reconstruct argv for absl.flags\n    sys.argv = [sys.argv[0]] + unknown\n    return args\n\nif __name__ == \'__main__\':\n    create_flags()\n    absl.app.run(partial(main, parse_args()))\n'"
lm_optimizer.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport absl.app\nimport optuna\nimport sys\nimport tensorflow.compat.v1 as tfv1\n\nfrom deepspeech_training.evaluate import evaluate\nfrom deepspeech_training.train import create_model\nfrom deepspeech_training.util.config import Config, initialize_globals\nfrom deepspeech_training.util.flags import create_flags, FLAGS\nfrom deepspeech_training.util.logging import log_error\nfrom deepspeech_training.util.evaluate_tools import wer_cer_batch\nfrom ds_ctcdecoder import Scorer\n\n\ndef character_based():\n    is_character_based = False\n    if FLAGS.scorer_path:\n        scorer = Scorer(FLAGS.lm_alpha, FLAGS.lm_beta, FLAGS.scorer_path, Config.alphabet)\n        is_character_based = scorer.is_utf8_mode()\n    return is_character_based\n\ndef objective(trial):\n    FLAGS.lm_alpha = trial.suggest_uniform(\'lm_alpha\', 0, FLAGS.lm_alpha_max)\n    FLAGS.lm_beta = trial.suggest_uniform(\'lm_beta\', 0, FLAGS.lm_beta_max)\n\n    is_character_based = trial.study.user_attrs[\'is_character_based\']\n\n    samples = []\n    for step, test_file in enumerate(FLAGS.test_files.split(\',\')):\n        tfv1.reset_default_graph()\n\n        current_samples = evaluate([test_file], create_model)\n        samples += current_samples\n\n        # Report intermediate objective value.\n        wer, cer = wer_cer_batch(current_samples)\n        trial.report(cer if is_character_based else wer, step)\n\n        # Handle pruning based on the intermediate value.\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n\n    wer, cer = wer_cer_batch(samples)\n    return cer if is_character_based else wer\n\ndef main(_):\n    initialize_globals()\n\n    if not FLAGS.test_files:\n        log_error(\'You need to specify what files to use for evaluation via \'\n                  \'the --test_files flag.\')\n        sys.exit(1)\n\n    is_character_based = character_based()\n\n    study = optuna.create_study()\n    study.set_user_attr(""is_character_based"", is_character_based)\n    study.optimize(objective, n_jobs=1, n_trials=FLAGS.n_trials)\n    print(\'Best params: lm_alpha={} and lm_beta={} with WER={}\'.format(study.best_params[\'lm_alpha\'],\n                                                                       study.best_params[\'lm_beta\'],\n                                                                       study.best_value))\n\n\nif __name__ == \'__main__\':\n    create_flags()\n    absl.app.run(main)\n'"
setup.py,0,"b'import os\nimport platform\nimport sys\nfrom pathlib import Path\n\nfrom pkg_resources import parse_version\nfrom setuptools import find_packages, setup\n\n\ndef get_tc_decoder_pkg_url(version, artifacts_root):\n    assert artifacts_root\n\n    ds_version = parse_version(version)\n    branch = ""v{}"".format(version)\n\n    plat = platform.system().lower()\n    arch = platform.machine().lower()\n\n    if plat == \'linux\' and arch == \'x86_64\':\n        plat = \'manylinux1\'\n\n    if plat == \'darwin\':\n        plat = \'macosx_10_10\'\n\n    if plat == \'windows\':\n        plat = \'win\'\n\n    # ABI does not contain m / mu anymore after Python 3.8\n    if sys.version_info.major == 3 and sys.version_info.minor >= 8:\n        m_or_mu = \'\'\n    else:\n        is_ucs2 = sys.maxunicode < 0x10ffff\n        m_or_mu = \'mu\' if is_ucs2 else \'m\'\n\n    pyver = \'\'.join(str(i) for i in sys.version_info[0:2])\n\n    return \'ds_ctcdecoder @ {artifacts_root}/ds_ctcdecoder-{ds_version}-cp{pyver}-cp{pyver}{m_or_mu}-{platform}_{arch}.whl\'.format(\n        artifacts_root=artifacts_root,\n        ds_version=ds_version,\n        pyver=pyver,\n        m_or_mu=m_or_mu,\n        platform=plat,\n        arch=arch,\n    )\n\n\ndef main():\n    version_file = Path(__file__).parent / \'VERSION\'\n    with open(str(version_file)) as fin:\n        version = fin.read().strip()\n\n    install_requires_base = [\n        \'tensorflow == 1.15.2\',\n        \'numpy\',\n        \'progressbar2\',\n        \'six\',\n        \'pyxdg\',\n        \'attrdict\',\n        \'absl-py\',\n        \'semver\',\n        \'opuslib == 2.0.0\',\n        \'optuna\',\n        \'sox\',\n        \'bs4\',\n        \'pandas\',\n        \'requests\',\n        \'numba == 0.47.0\', # ships py3.5 wheel\n        \'llvmlite == 0.31.0\', # for numba==0.47.0\n        \'librosa\',\n        \'soundfile\',\n    ]\n\n    decoder_pypi_dep = [\n        \'ds_ctcdecoder == {}\'.format(version)\n    ]\n\n    # Due to pip craziness environment variables are the only consistent way to\n    # get options into this script when doing `pip install`.\n    tc_decoder_artifacts_root = os.environ.get(\'DECODER_ARTIFACTS_ROOT\', \'\')\n    if tc_decoder_artifacts_root:\n        # We\'re running inside the TaskCluster environment, override the decoder\n        # package URL with the one we just built.\n        decoder_pkg_url = get_tc_decoder_pkg_url(version, tc_decoder_artifacts_root)\n        install_requires = install_requires_base + [decoder_pkg_url]\n    elif os.environ.get(\'DS_NODECODER\', \'\'):\n        install_requires = install_requires_base\n    else:\n        install_requires = install_requires_base + decoder_pypi_dep\n\n    setup(\n        name=\'deepspeech_training\',\n        version=version,\n        description=\'Training code for mozilla DeepSpeech\',\n        url=\'https://github.com/mozilla/DeepSpeech\',\n        author=\'Mozilla\',\n        license=\'MPL-2.0\',\n        # Classifiers help users find your project by categorizing it.\n        #\n        # For a list of valid classifiers, see https://pypi.org/classifiers/\n        classifiers=[\n            \'Development Status :: 3 - Alpha\',\n            \'Intended Audience :: Developers\',\n            \'Topic :: Multimedia :: Sound/Audio :: Speech\',\n            \'License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)\',\n            \'Programming Language :: Python :: 3\',\n        ],\n        package_dir={\'\': \'training\'},\n        packages=find_packages(where=\'training\'),\n        python_requires=\'>=3.5, <4\',\n        install_requires=install_requires,\n        # If there are data files included in your packages that need to be\n        # installed, specify them here.\n        package_data={\n            \'deepspeech_training\': [\n                \'VERSION\',\n                \'GRAPH_VERSION\',\n            ],\n        },\n    )\n\nif __name__ == \'__main__\':\n    main()\n'"
stats.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport functools\nimport pandas\n\nfrom deepspeech_training.util.helpers import secs_to_hours\nfrom pathlib import Path\n\n\ndef read_csvs(csv_files):\n    # Relative paths are relative to CSV location\n    def absolutify(csv, path):\n        path = Path(path)\n        if path.is_absolute():\n            return str(path)\n        return str(csv.parent / path)\n\n    sets = []\n    for csv in csv_files:\n        file = pandas.read_csv(csv, encoding=\'utf-8\', na_filter=False)\n        file[\'wav_filename\'] = file[\'wav_filename\'].apply(functools.partial(absolutify, csv))\n        sets.append(file)\n\n    # Concat all sets, drop any extra columns, re-index the final result as 0..N\n    return pandas.concat(sets, join=\'inner\', ignore_index=True)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(""-csv"", ""--csv-files"", help=""Str. Filenames as a comma separated list"", required=True)\n    parser.add_argument(""--sample-rate"", type=int, default=16000, required=False, help=""Audio sample rate"")\n    parser.add_argument(""--channels"", type=int, default=1, required=False, help=""Audio channels"")\n    parser.add_argument(""--bits-per-sample"", type=int, default=16, required=False, help=""Audio bits per sample"")\n    args = parser.parse_args()\n    in_files = [Path(i).absolute() for i in args.csv_files.split("","")]\n\n    csv_dataframe = read_csvs(in_files)\n    total_bytes = csv_dataframe[\'wav_filesize\'].sum()\n    total_files = len(csv_dataframe)\n    total_seconds = ((csv_dataframe[\'wav_filesize\'] - 44) / args.sample_rate / args.channels / (args.bits_per_sample // 8)).sum()\n\n    print(\'Total bytes:\', total_bytes)\n    print(\'Total files:\', total_files)\n    print(\'Total time:\', secs_to_hours(total_seconds))\n\nif __name__ == \'__main__\':\n    main()\n'"
transcribe.py,14,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport sys\nimport json\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\nimport tensorflow as tf\nimport tensorflow.compat.v1.logging as tflogging\ntflogging.set_verbosity(tflogging.ERROR)\nimport logging\nlogging.getLogger(\'sox\').setLevel(logging.ERROR)\nimport glob\n\nfrom deepspeech_training.util.audio import AudioFile\nfrom deepspeech_training.util.config import Config, initialize_globals\nfrom deepspeech_training.util.feeding import split_audio_file\nfrom deepspeech_training.util.flags import create_flags, FLAGS\nfrom deepspeech_training.util.logging import log_error, log_info, log_progress, create_progressbar\nfrom ds_ctcdecoder import ctc_beam_search_decoder_batch, Scorer\nfrom multiprocessing import Process, cpu_count\n\n\ndef fail(message, code=1):\n    log_error(message)\n    sys.exit(code)\n\n\ndef transcribe_file(audio_path, tlog_path):\n    from deepspeech_training.train import create_model  # pylint: disable=cyclic-import,import-outside-toplevel\n    from deepspeech_training.util.checkpoints import load_graph_for_evaluation\n    initialize_globals()\n    scorer = Scorer(FLAGS.lm_alpha, FLAGS.lm_beta, FLAGS.scorer_path, Config.alphabet)\n    try:\n        num_processes = cpu_count()\n    except NotImplementedError:\n        num_processes = 1\n    with AudioFile(audio_path, as_path=True) as wav_path:\n        data_set = split_audio_file(wav_path,\n                                    batch_size=FLAGS.batch_size,\n                                    aggressiveness=FLAGS.vad_aggressiveness,\n                                    outlier_duration_ms=FLAGS.outlier_duration_ms,\n                                    outlier_batch_size=FLAGS.outlier_batch_size)\n        iterator = tf.data.Iterator.from_structure(data_set.output_types, data_set.output_shapes,\n                                                   output_classes=data_set.output_classes)\n        batch_time_start, batch_time_end, batch_x, batch_x_len = iterator.get_next()\n        no_dropout = [None] * 6\n        logits, _ = create_model(batch_x=batch_x, seq_length=batch_x_len, dropout=no_dropout)\n        transposed = tf.nn.softmax(tf.transpose(logits, [1, 0, 2]))\n        tf.train.get_or_create_global_step()\n        with tf.Session(config=Config.session_config) as session:\n            load_graph_for_evaluation(session)\n            session.run(iterator.make_initializer(data_set))\n            transcripts = []\n            while True:\n                try:\n                    starts, ends, batch_logits, batch_lengths = \\\n                        session.run([batch_time_start, batch_time_end, transposed, batch_x_len])\n                except tf.errors.OutOfRangeError:\n                    break\n                decoded = ctc_beam_search_decoder_batch(batch_logits, batch_lengths, Config.alphabet, FLAGS.beam_width,\n                                                        num_processes=num_processes,\n                                                        scorer=scorer)\n                decoded = list(d[0][1] for d in decoded)\n                transcripts.extend(zip(starts, ends, decoded))\n            transcripts.sort(key=lambda t: t[0])\n            transcripts = [{\'start\': int(start),\n                            \'end\': int(end),\n                            \'transcript\': transcript} for start, end, transcript in transcripts]\n            with open(tlog_path, \'w\') as tlog_file:\n                json.dump(transcripts, tlog_file, default=float)\n\n\ndef transcribe_many(src_paths,dst_paths):\n    pbar = create_progressbar(prefix=\'Transcribing files | \', max_value=len(src_paths)).start()\n    for i in range(len(src_paths)):\n        p = Process(target=transcribe_file, args=(src_paths[i], dst_paths[i]))\n        p.start()\n        p.join()\n        log_progress(\'Transcribed file {} of {} from ""{}"" to ""{}""\'.format(i + 1, len(src_paths), src_paths[i], dst_paths[i]))\n        pbar.update(i)\n    pbar.finish()\n\n\ndef transcribe_one(src_path, dst_path):\n    transcribe_file(src_path, dst_path)\n    log_info(\'Transcribed file ""{}"" to ""{}""\'.format(src_path, dst_path))\n\n\ndef resolve(base_path, spec_path):\n    if spec_path is None:\n        return None\n    if not os.path.isabs(spec_path):\n        spec_path = os.path.join(base_path, spec_path)\n    return spec_path\n\n\ndef main(_):\n    if not FLAGS.src or not os.path.exists(FLAGS.src):\n        # path not given or non-existant\n        fail(\'You have to specify which file or catalog to transcribe via the --src flag.\')\n    else:\n        # path given and exists\n        src_path = os.path.abspath(FLAGS.src)\n        if os.path.isfile(src_path):\n            if src_path.endswith(\'.catalog\'):\n                # Transcribe batch of files via "".catalog"" file (from DSAlign)\n                if FLAGS.dst:\n                    fail(\'Parameter --dst not supported if --src points to a catalog\')\n                catalog_dir = os.path.dirname(src_path)\n                with open(src_path, \'r\') as catalog_file:\n                    catalog_entries = json.load(catalog_file)\n                catalog_entries = [(resolve(catalog_dir, e[\'audio\']), resolve(catalog_dir, e[\'tlog\'])) for e in catalog_entries]\n                if any(map(lambda e: not os.path.isfile(e[0]), catalog_entries)):\n                    fail(\'Missing source file(s) in catalog\')\n                if not FLAGS.force and any(map(lambda e: os.path.isfile(e[1]), catalog_entries)):\n                    fail(\'Destination file(s) from catalog already existing, use --force for overwriting\')\n                if any(map(lambda e: not os.path.isdir(os.path.dirname(e[1])), catalog_entries)):\n                    fail(\'Missing destination directory for at least one catalog entry\')\n                src_paths,dst_paths = zip(*paths)\n                transcribe_many(src_paths,dst_paths)\n            else:\n                # Transcribe one file\n                dst_path = os.path.abspath(FLAGS.dst) if FLAGS.dst else os.path.splitext(src_path)[0] + \'.tlog\'\n                if os.path.isfile(dst_path):\n                    if FLAGS.force:\n                        transcribe_one(src_path, dst_path)\n                    else:\n                        fail(\'Destination file ""{}"" already existing - use --force for overwriting\'.format(dst_path), code=0)\n                elif os.path.isdir(os.path.dirname(dst_path)):\n                    transcribe_one(src_path, dst_path)\n                else:\n                    fail(\'Missing destination directory\')\n        elif os.path.isdir(src_path):\n            # Transcribe all files in dir\n            print(""Transcribing all WAV files in --src"")\n            if FLAGS.dst:\n                fail(\'Destination file not supported for batch decoding jobs.\')\n            else:\n                if not FLAGS.recursive:\n                    print(""If you wish to recursively scan --src, then you must use --recursive"")\n                    wav_paths = glob.glob(src_path + ""/*.wav"")\n                else:\n                    wav_paths = glob.glob(src_path + ""/**/*.wav"")\n                dst_paths = [path.replace(\'.wav\',\'.tlog\') for path in wav_paths]\n                transcribe_many(wav_paths,dst_paths)\n\n\nif __name__ == \'__main__\':\n    create_flags()\n    tf.app.flags.DEFINE_string(\'src\', \'\', \'Source path to an audio file or directory or catalog file.\'\n                                          \'Catalog files should be formatted from DSAlign. A directory will\'\n                                          \'be recursively searched for audio. If --dst not set, transcription logs (.tlog) will be \'\n                                          \'written in-place using the source filenames with \'\n                                          \'suffix "".tlog"" instead of "".wav"".\')\n    tf.app.flags.DEFINE_string(\'dst\', \'\', \'path for writing the transcription log or logs (.tlog). \'\n                                          \'If --src is a directory, this one also has to be a directory \'\n                                          \'and the required sub-dir tree of --src will get replicated.\')\n    tf.app.flags.DEFINE_boolean(\'recursive\', False, \'scan dir of audio recursively\')\n    tf.app.flags.DEFINE_boolean(\'force\', False, \'Forces re-transcribing and overwriting of already existing \'\n                                                \'transcription logs (.tlog)\')\n    tf.app.flags.DEFINE_integer(\'vad_aggressiveness\', 3, \'How aggressive (0=lowest, 3=highest) the VAD should \'\n                                                         \'split audio\')\n    tf.app.flags.DEFINE_integer(\'batch_size\', 40, \'Default batch size\')\n    tf.app.flags.DEFINE_float(\'outlier_duration_ms\', 10000, \'Duration in ms after which samples are considered outliers\')\n    tf.app.flags.DEFINE_integer(\'outlier_batch_size\', 1, \'Batch size for duration outliers (defaults to 1)\')\n    tf.app.run(main)\n'"
bin/build_sdb.py,0,"b'#!/usr/bin/env python\n""""""\nTool for building Sample Databases (SDB files) from DeepSpeech CSV files and other SDB files\nUse ""python3 build_sdb.py -h"" for help\n""""""\nimport argparse\n\nimport progressbar\n\nfrom deepspeech_training.util.audio import (\n    AUDIO_TYPE_OPUS,\n    AUDIO_TYPE_WAV,\n    change_audio_types,\n)\nfrom deepspeech_training.util.downloader import SIMPLE_BAR\nfrom deepspeech_training.util.sample_collections import (\n    DirectSDBWriter,\n    samples_from_sources,\n)\n\nAUDIO_TYPE_LOOKUP = {""wav"": AUDIO_TYPE_WAV, ""opus"": AUDIO_TYPE_OPUS}\n\n\ndef build_sdb():\n    audio_type = AUDIO_TYPE_LOOKUP[CLI_ARGS.audio_type]\n    with DirectSDBWriter(\n        CLI_ARGS.target, audio_type=audio_type, labeled=not CLI_ARGS.unlabeled\n    ) as sdb_writer:\n        samples = samples_from_sources(CLI_ARGS.sources, labeled=not CLI_ARGS.unlabeled)\n        bar = progressbar.ProgressBar(max_value=len(samples), widgets=SIMPLE_BAR)\n        for sample in bar(\n            change_audio_types(samples, audio_type=audio_type, bitrate=CLI_ARGS.bitrate, processes=CLI_ARGS.workers)\n        ):\n            sdb_writer.add(sample)\n\n\ndef handle_args():\n    parser = argparse.ArgumentParser(\n        description=""Tool for building Sample Databases (SDB files) ""\n        ""from DeepSpeech CSV files and other SDB files""\n    )\n    parser.add_argument(\n        ""sources"",\n        nargs=""+"",\n        help=""Source CSV and/or SDB files - ""\n        ""Note: For getting a correctly ordered target SDB, source SDBs have to have their samples ""\n        ""already ordered from shortest to longest."",\n    )\n    parser.add_argument(""target"", help=""SDB file to create"")\n    parser.add_argument(\n        ""--audio-type"",\n        default=""opus"",\n        choices=AUDIO_TYPE_LOOKUP.keys(),\n        help=""Audio representation inside target SDB"",\n    )\n    parser.add_argument(\n        ""--bitrate"",\n        type=int,\n        help=""Bitrate for lossy compressed SDB samples like in case of --audio-type opus"",\n    )\n    parser.add_argument(\n        ""--workers"", type=int, default=None, help=""Number of encoding SDB workers""\n    )\n    parser.add_argument(\n        ""--unlabeled"",\n        action=""store_true"",\n        help=""If to build an SDB with unlabeled (audio only) samples - ""\n        ""typically used for building noise augmentation corpora"",\n    )\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    CLI_ARGS = handle_args()\n    build_sdb()\n'"
bin/compare_samples.py,0,"b'#!/usr/bin/env python\n""""""\nTool for comparing two wav samples\n""""""\nimport sys\nimport argparse\n\nfrom deepspeech_training.util.audio import AUDIO_TYPE_NP, mean_dbfs\nfrom deepspeech_training.util.sample_collections import load_sample\n\n\ndef fail(message):\n    print(message, file=sys.stderr, flush=True)\n    sys.exit(1)\n\n\ndef compare_samples():\n    sample1 = load_sample(CLI_ARGS.sample1)\n    sample2 = load_sample(CLI_ARGS.sample2)\n    if sample1.audio_format != sample2.audio_format:\n        fail(\'Samples differ on: audio-format ({} and {})\'.format(sample1.audio_format, sample2.audio_format))\n    if sample1.duration != sample2.duration:\n        fail(\'Samples differ on: duration ({} and {})\'.format(sample1.duration, sample2.duration))\n    sample1.change_audio_type(AUDIO_TYPE_NP)\n    sample2.change_audio_type(AUDIO_TYPE_NP)\n    audio_diff = sample1.audio - sample2.audio\n    diff_dbfs = mean_dbfs(audio_diff)\n    differ_msg = \'Samples differ on: sample data ({:0.2f} dB difference) \'.format(diff_dbfs)\n    equal_msg = \'Samples are considered equal ({:0.2f} dB difference)\'.format(diff_dbfs)\n    if CLI_ARGS.if_differ:\n        if diff_dbfs <= CLI_ARGS.threshold:\n            fail(equal_msg)\n        if not CLI_ARGS.no_success_output:\n            print(differ_msg, file=sys.stderr, flush=True)\n    else:\n        if diff_dbfs > CLI_ARGS.threshold:\n            fail(differ_msg)\n        if not CLI_ARGS.no_success_output:\n            print(equal_msg, file=sys.stderr, flush=True)\n\n\ndef handle_args():\n    parser = argparse.ArgumentParser(\n        description=""Tool for checking similarity of two samples""\n    )\n    parser.add_argument(""sample1"", help=""Filename of sample 1 to compare"")\n    parser.add_argument(""sample2"", help=""Filename of sample 2 to compare"")\n    parser.add_argument(""--threshold"", type=float, default=-60.0,\n                        help=""dB of sample deltas above which they are considered different"")\n    parser.add_argument(\n        ""--if-differ"",\n        action=""store_true"",\n        help=""If to succeed and return status code 0 on different signals and fail on equal ones (inverse check).""\n             ""This will still fail on different formats or durations."",\n    )\n    parser.add_argument(\n        ""--no-success-output"",\n        action=""store_true"",\n        help=""Stay silent on success (if samples are equal of - with --if-differ - samples are not equal)"",\n    )\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    CLI_ARGS = handle_args()\n    compare_samples()\n'"
bin/graphdef_binary_to_text.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\n\nimport tensorflow.compat.v1 as tfv1\nfrom google.protobuf import text_format\n\n\ndef main():\n    # Load and export as string\n    with tfv1.gfile.FastGFile(sys.argv[1], ""rb"") as fin:\n        graph_def = tfv1.GraphDef()\n        graph_def.ParseFromString(fin.read())\n\n        with tfv1.gfile.FastGFile(sys.argv[1] + ""txt"", ""w"") as fout:\n            fout.write(text_format.MessageToString(graph_def))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
bin/import_aidatatang.py,0,"b'#!/usr/bin/env python\nimport glob\nimport os\nimport tarfile\n\nimport pandas\n\nfrom deepspeech_training.util.importers import get_importers_parser\n\nCOLUMN_NAMES = [""wav_filename"", ""wav_filesize"", ""transcript""]\n\n\ndef extract(archive_path, target_dir):\n    print(""Extracting {} into {}..."".format(archive_path, target_dir))\n    with tarfile.open(archive_path) as tar:\n        tar.extractall(target_dir)\n\n\ndef preprocess_data(tgz_file, target_dir):\n    # First extract main archive and sub-archives\n    extract(tgz_file, target_dir)\n    main_folder = os.path.join(target_dir, ""aidatatang_200zh"")\n\n    for targz in glob.glob(os.path.join(main_folder, ""corpus"", ""*"", ""*.tar.gz"")):\n        extract(targz, os.path.dirname(targz))\n\n    # Folder structure is now:\n    # - aidatatang_200zh/\n    #   - transcript/aidatatang_200_zh_transcript.txt\n    #   - corpus/train/*.tar.gz\n    #   - corpus/train/*/*.{wav,txt,trn,metadata}\n    #   - corpus/dev/*.tar.gz\n    #   - corpus/dev/*/*.{wav,txt,trn,metadata}\n    #   - corpus/test/*.tar.gz\n    #   - corpus/test/*/*.{wav,txt,trn,metadata}\n\n    # Transcripts file has one line per WAV file, where each line consists of\n    # the WAV file name without extension followed by a single space followed\n    # by the transcript.\n\n    # Since the transcripts themselves can contain spaces, we split on space but\n    # only once, then build a mapping from file name to transcript\n    transcripts_path = os.path.join(\n        main_folder, ""transcript"", ""aidatatang_200_zh_transcript.txt""\n    )\n    with open(transcripts_path) as fin:\n        transcripts = dict((line.split("" "", maxsplit=1) for line in fin))\n\n    def load_set(glob_path):\n        set_files = []\n        for wav in glob.glob(glob_path):\n            try:\n                wav_filename = wav\n                wav_filesize = os.path.getsize(wav)\n                transcript_key = os.path.splitext(os.path.basename(wav))[0]\n                transcript = transcripts[transcript_key].strip(""\\n"")\n                set_files.append((wav_filename, wav_filesize, transcript))\n            except KeyError:\n                print(""Warning: Missing transcript for WAV file {}."".format(wav))\n        return set_files\n\n    for subset in (""train"", ""dev"", ""test""):\n        print(""Loading {} set samples..."".format(subset))\n        subset_files = load_set(\n            os.path.join(main_folder, ""corpus"", subset, ""*"", ""*.wav"")\n        )\n        df = pandas.DataFrame(data=subset_files, columns=COLUMN_NAMES)\n\n        # Trim train set to under 10s by removing the last couple hundred samples\n        if subset == ""train"":\n            durations = (df[""wav_filesize""] - 44) / 16000 / 2\n            df = df[durations <= 10.0]\n            print(""Trimming {} samples > 10 seconds"".format((durations > 10.0).sum()))\n\n        dest_csv = os.path.join(target_dir, ""aidatatang_{}.csv"".format(subset))\n        print(""Saving {} set into {}..."".format(subset, dest_csv))\n        df.to_csv(dest_csv, index=False)\n\n\ndef main():\n    # https://www.openslr.org/62/\n    parser = get_importers_parser(description=""Import aidatatang_200zh corpus"")\n    parser.add_argument(""tgz_file"", help=""Path to aidatatang_200zh.tgz"")\n    parser.add_argument(\n        ""--target_dir"",\n        default="""",\n        help=""Target folder to extract files into and put the resulting CSVs. Defaults to same folder as the main archive."",\n    )\n    params = parser.parse_args()\n\n    if not params.target_dir:\n        params.target_dir = os.path.dirname(params.tgz_file)\n\n    preprocess_data(params.tgz_file, params.target_dir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
bin/import_aishell.py,0,"b'#!/usr/bin/env python\nimport glob\nimport os\nimport tarfile\n\nimport pandas\n\nfrom deepspeech_training.util.importers import get_importers_parser\n\nCOLUMNNAMES = [""wav_filename"", ""wav_filesize"", ""transcript""]\n\n\ndef extract(archive_path, target_dir):\n    print(""Extracting {} into {}..."".format(archive_path, target_dir))\n    with tarfile.open(archive_path) as tar:\n        tar.extractall(target_dir)\n\n\ndef preprocess_data(tgz_file, target_dir):\n    # First extract main archive and sub-archives\n    extract(tgz_file, target_dir)\n    main_folder = os.path.join(target_dir, ""data_aishell"")\n\n    wav_archives_folder = os.path.join(main_folder, ""wav"")\n    for targz in glob.glob(os.path.join(wav_archives_folder, ""*.tar.gz"")):\n        extract(targz, main_folder)\n\n    # Folder structure is now:\n    # - data_aishell/\n    #   - train/S****/*.wav\n    #   - dev/S****/*.wav\n    #   - test/S****/*.wav\n    #   - wav/S****.tar.gz\n    #   - transcript/aishell_transcript_v0.8.txt\n\n    # Transcripts file has one line per WAV file, where each line consists of\n    # the WAV file name without extension followed by a single space followed\n    # by the transcript.\n\n    # Since the transcripts themselves can contain spaces, we split on space but\n    # only once, then build a mapping from file name to transcript\n    transcripts_path = os.path.join(\n        main_folder, ""transcript"", ""aishell_transcript_v0.8.txt""\n    )\n    with open(transcripts_path) as fin:\n        transcripts = dict((line.split("" "", maxsplit=1) for line in fin))\n\n    def load_set(glob_path):\n        set_files = []\n        for wav in glob.glob(glob_path):\n            try:\n                wav_filename = wav\n                wav_filesize = os.path.getsize(wav)\n                transcript_key = os.path.splitext(os.path.basename(wav))[0]\n                transcript = transcripts[transcript_key].strip(""\\n"")\n                set_files.append((wav_filename, wav_filesize, transcript))\n            except KeyError:\n                print(""Warning: Missing transcript for WAV file {}."".format(wav))\n        return set_files\n\n    for subset in (""train"", ""dev"", ""test""):\n        print(""Loading {} set samples..."".format(subset))\n        subset_files = load_set(os.path.join(main_folder, subset, ""S*"", ""*.wav""))\n        df = pandas.DataFrame(data=subset_files, columns=COLUMNNAMES)\n\n        # Trim train set to under 10s by removing the last couple hundred samples\n        if subset == ""train"":\n            durations = (df[""wav_filesize""] - 44) / 16000 / 2\n            df = df[durations <= 10.0]\n            print(""Trimming {} samples > 10 seconds"".format((durations > 10.0).sum()))\n\n        dest_csv = os.path.join(target_dir, ""aishell_{}.csv"".format(subset))\n        print(""Saving {} set into {}..."".format(subset, dest_csv))\n        df.to_csv(dest_csv, index=False)\n\n\ndef main():\n    # http://www.openslr.org/33/\n    parser = get_importers_parser(description=""Import AISHELL corpus"")\n    parser.add_argument(""aishell_tgz_file"", help=""Path to data_aishell.tgz"")\n    parser.add_argument(\n        ""--target_dir"",\n        default="""",\n        help=""Target folder to extract files into and put the resulting CSVs. Defaults to same folder as the main archive."",\n    )\n    params = parser.parse_args()\n\n    if not params.target_dir:\n        params.target_dir = os.path.dirname(params.aishell_tgz_file)\n\n    preprocess_data(params.aishell_tgz_file, params.target_dir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
bin/import_cv.py,0,"b'#!/usr/bin/env python\nimport csv\nimport os\nimport subprocess\nimport tarfile\nfrom glob import glob\nfrom multiprocessing import Pool\n\nimport progressbar\nimport sox\n\nfrom deepspeech_training.util.downloader import SIMPLE_BAR, maybe_download\nfrom deepspeech_training.util.importers import (\n    get_counter,\n    get_imported_samples,\n    print_import_report,\n)\nfrom deepspeech_training.util.importers import validate_label_eng as validate_label\n\nFIELDNAMES = [""wav_filename"", ""wav_filesize"", ""transcript""]\nSAMPLE_RATE = 16000\nMAX_SECS = 10\nARCHIVE_DIR_NAME = ""cv_corpus_v1""\nARCHIVE_NAME = ARCHIVE_DIR_NAME + "".tar.gz""\nARCHIVE_URL = (\n    ""https://s3.us-east-2.amazonaws.com/common-voice-data-download/"" + ARCHIVE_NAME\n)\n\n\ndef _download_and_preprocess_data(target_dir):\n    # Making path absolute\n    target_dir = os.path.abspath(target_dir)\n    # Conditionally download data\n    archive_path = maybe_download(ARCHIVE_NAME, target_dir, ARCHIVE_URL)\n    # Conditionally extract common voice data\n    _maybe_extract(target_dir, ARCHIVE_DIR_NAME, archive_path)\n    # Conditionally convert common voice CSV files and mp3 data to DeepSpeech CSVs and wav\n    _maybe_convert_sets(target_dir, ARCHIVE_DIR_NAME)\n\n\ndef _maybe_extract(target_dir, extracted_data, archive_path):\n    # If target_dir/extracted_data does not exist, extract archive in target_dir\n    extracted_path = os.join(target_dir, extracted_data)\n    if not os.path.exists(extracted_path):\n        print(\'No directory ""%s"" - extracting archive...\' % extracted_path)\n        with tarfile.open(archive_path) as tar:\n            tar.extractall(target_dir)\n    else:\n        print(\'Found directory ""%s"" - not extracting it from archive.\' % extracted_path)\n\n\ndef _maybe_convert_sets(target_dir, extracted_data):\n    extracted_dir = os.path.join(target_dir, extracted_data)\n    for source_csv in glob(os.path.join(extracted_dir, ""*.csv"")):\n        _maybe_convert_set(\n            extracted_dir,\n            source_csv,\n            os.path.join(target_dir, os.path.split(source_csv)[-1]),\n        )\n\n\ndef one_sample(sample):\n    mp3_filename = sample[0]\n    # Storing wav files next to the mp3 ones - just with a different suffix\n    wav_filename = path.splitext(mp3_filename)[0] + "".wav""\n    _maybe_convert_wav(mp3_filename, wav_filename)\n    frames = int(\n        subprocess.check_output([""soxi"", ""-s"", wav_filename], stderr=subprocess.STDOUT)\n    )\n    file_size = -1\n    if os.path.exists(wav_filename):\n        file_size = path.getsize(wav_filename)\n        frames = int(\n            subprocess.check_output(\n                [""soxi"", ""-s"", wav_filename], stderr=subprocess.STDOUT\n            )\n        )\n    label = validate_label(sample[1])\n    rows = []\n    counter = get_counter()\n    if file_size == -1:\n        # Excluding samples that failed upon conversion\n        counter[""failed""] += 1\n    elif label is None:\n        # Excluding samples that failed on label validation\n        counter[""invalid_label""] += 1\n    elif int(frames / SAMPLE_RATE * 1000 / 10 / 2) < len(str(label)):\n        # Excluding samples that are too short to fit the transcript\n        counter[""too_short""] += 1\n    elif frames / SAMPLE_RATE > MAX_SECS:\n        # Excluding very long samples to keep a reasonable batch-size\n        counter[""too_long""] += 1\n    else:\n        # This one is good - keep it for the target CSV\n        rows.append((wav_filename, file_size, label))\n        counter[""imported_time""] += frames\n    counter[""all""] += 1\n    counter[""total_time""] += frames\n    return (counter, rows)\n\n\ndef _maybe_convert_set(extracted_dir, source_csv, target_csv):\n    print()\n    if os.path.exists(target_csv):\n        print(\'Found CSV file ""%s"" - not importing ""%s"".\' % (target_csv, source_csv))\n        return\n    print(\'No CSV file ""%s"" - importing ""%s""...\' % (target_csv, source_csv))\n    samples = []\n    with open(source_csv) as source_csv_file:\n        reader = csv.DictReader(source_csv_file)\n        for row in reader:\n            samples.append((os.path.join(extracted_dir, row[""filename""]), row[""text""]))\n\n    # Mutable counters for the concurrent embedded routine\n    counter = get_counter()\n    num_samples = len(samples)\n    rows = []\n\n    print(""Importing mp3 files..."")\n    pool = Pool()\n    bar = progressbar.ProgressBar(max_value=num_samples, widgets=SIMPLE_BAR)\n    for i, processed in enumerate(pool.imap_unordered(one_sample, samples), start=1):\n        counter += processed[0]\n        rows += processed[1]\n        bar.update(i)\n    bar.update(num_samples)\n    pool.close()\n    pool.join()\n\n    print(\'Writing ""%s""...\' % target_csv)\n    with open(target_csv, ""w"", encoding=""utf-8"", newline="""") as target_csv_file:\n        writer = csv.DictWriter(target_csv_file, fieldnames=FIELDNAMES)\n        writer.writeheader()\n        bar = progressbar.ProgressBar(max_value=len(rows), widgets=SIMPLE_BAR)\n        for filename, file_size, transcript in bar(rows):\n            writer.writerow(\n                {\n                    ""wav_filename"": filename,\n                    ""wav_filesize"": file_size,\n                    ""transcript"": transcript,\n                }\n            )\n\n    imported_samples = get_imported_samples(counter)\n    assert counter[""all""] == num_samples\n    assert len(rows) == imported_samples\n\n    print_import_report(counter, SAMPLE_RATE, MAX_SECS)\n\n\ndef _maybe_convert_wav(mp3_filename, wav_filename):\n    if not os.path.exists(wav_filename):\n        transformer = sox.Transformer()\n        transformer.convert(samplerate=SAMPLE_RATE)\n        try:\n            transformer.build(mp3_filename, wav_filename)\n        except sox.core.SoxError:\n            pass\n\n\nif __name__ == ""__main__"":\n    _download_and_preprocess_data(sys.argv[1])\n'"
bin/import_cv2.py,0,"b'#!/usr/bin/env python\n""""""\nBroadly speaking, this script takes the audio downloaded from Common Voice\nfor a certain language, in addition to the *.tsv files output by CorporaCreator,\nand the script formats the data and transcripts to be in a state usable by\nDeepSpeech.py\nUse ""python3 import_cv2.py -h"" for help\n""""""\nimport csv\nimport itertools\nimport os\nimport subprocess\nimport unicodedata\nfrom multiprocessing import Pool\n\nimport progressbar\nimport sox\n\nfrom deepspeech_training.util.downloader import SIMPLE_BAR\nfrom deepspeech_training.util.importers import (\n    get_counter,\n    get_imported_samples,\n    get_importers_parser,\n    get_validate_label,\n    print_import_report,\n)\nfrom deepspeech_training.util.text import Alphabet\n\nFIELDNAMES = [""wav_filename"", ""wav_filesize"", ""transcript""]\nSAMPLE_RATE = 16000\nMAX_SECS = 10\n\n\ndef _preprocess_data(tsv_dir, audio_dir, filter_obj, space_after_every_character=False):\n    exclude = []\n    for dataset in [""test"", ""dev"", ""train"", ""validated"", ""other""]:\n        set_samples = _maybe_convert_set(dataset, tsv_dir, audio_dir, filter_obj, space_after_every_character)\n        if dataset in [""test"", ""dev""]:\n            exclude += set_samples\n        if dataset == ""validated"":\n            _maybe_convert_set(""train-all"", tsv_dir, audio_dir, filter_obj, space_after_every_character,\n                               rows=set_samples, exclude=exclude)\n\n\ndef one_sample(args):\n    """""" Take an audio file, and optionally convert it to 16kHz WAV """"""\n    sample, filter_obj = args\n    mp3_filename = sample[0]\n    if not os.path.splitext(mp3_filename.lower())[1] == "".mp3"":\n        mp3_filename += "".mp3""\n    # Storing wav files next to the mp3 ones - just with a different suffix\n    wav_filename = os.path.splitext(mp3_filename)[0] + "".wav""\n    _maybe_convert_wav(mp3_filename, wav_filename)\n    file_size = -1\n    frames = 0\n    if os.path.exists(wav_filename):\n        file_size = os.path.getsize(wav_filename)\n        frames = int(\n            subprocess.check_output(\n                [""soxi"", ""-s"", wav_filename], stderr=subprocess.STDOUT\n            )\n        )\n    label = filter_obj.filter(sample[1])\n    rows = []\n    counter = get_counter()\n    if file_size == -1:\n        # Excluding samples that failed upon conversion\n        counter[""failed""] += 1\n    elif label is None:\n        # Excluding samples that failed on label validation\n        counter[""invalid_label""] += 1\n    elif int(frames / SAMPLE_RATE * 1000 / 10 / 2) < len(str(label)):\n        # Excluding samples that are too short to fit the transcript\n        counter[""too_short""] += 1\n    elif frames / SAMPLE_RATE > MAX_SECS:\n        # Excluding very long samples to keep a reasonable batch-size\n        counter[""too_long""] += 1\n    else:\n        # This one is good - keep it for the target CSV\n        rows.append((os.path.split(wav_filename)[-1], file_size, label, sample[2]))\n        counter[""imported_time""] += frames\n    counter[""all""] += 1\n    counter[""total_time""] += frames\n\n    return (counter, rows)\n\n\ndef _maybe_convert_set(dataset, tsv_dir, audio_dir, filter_obj, space_after_every_character=None, rows=None, exclude=None):\n    exclude_transcripts = set()\n    exclude_speakers = set()\n    if exclude is not None:\n        for sample in exclude:\n            exclude_transcripts.add(sample[2])\n            exclude_speakers.add(sample[3])\n\n    if rows is None:\n        rows = []\n        input_tsv = os.path.join(os.path.abspath(tsv_dir), dataset + "".tsv"")\n        if not os.path.isfile(input_tsv):\n            return rows\n        print(""Loading TSV file: "", input_tsv)\n        # Get audiofile path and transcript for each sentence in tsv\n        samples = []\n        with open(input_tsv, encoding=""utf-8"") as input_tsv_file:\n            reader = csv.DictReader(input_tsv_file, delimiter=""\\t"")\n            for row in reader:\n                samples.append((os.path.join(audio_dir, row[""path""]), row[""sentence""], row[""client_id""]))\n\n        counter = get_counter()\n        num_samples = len(samples)\n\n        print(""Importing mp3 files..."")\n        pool = Pool()\n        bar = progressbar.ProgressBar(max_value=num_samples, widgets=SIMPLE_BAR)\n        samples_with_context = itertools.zip_longest(samples, [], fillvalue=filter_obj)\n        for i, processed in enumerate(pool.imap_unordered(one_sample, samples_with_context), start=1):\n            counter += processed[0]\n            rows += processed[1]\n            bar.update(i)\n        bar.update(num_samples)\n        pool.close()\n        pool.join()\n\n        imported_samples = get_imported_samples(counter)\n        assert counter[""all""] == num_samples\n        assert len(rows) == imported_samples\n        print_import_report(counter, SAMPLE_RATE, MAX_SECS)\n\n    output_csv = os.path.join(os.path.abspath(audio_dir), dataset + "".csv"")\n    print(""Saving new DeepSpeech-formatted CSV file to: "", output_csv)\n    with open(output_csv, ""w"", encoding=""utf-8"", newline="""") as output_csv_file:\n        print(""Writing CSV file for DeepSpeech.py as: "", output_csv)\n        writer = csv.DictWriter(output_csv_file, fieldnames=FIELDNAMES)\n        writer.writeheader()\n        bar = progressbar.ProgressBar(max_value=len(rows), widgets=SIMPLE_BAR)\n        for filename, file_size, transcript, speaker in bar(rows):\n            if transcript in exclude_transcripts or speaker in exclude_speakers:\n                continue\n            if space_after_every_character:\n                writer.writerow(\n                    {\n                        ""wav_filename"": filename,\n                        ""wav_filesize"": file_size,\n                        ""transcript"": "" "".join(transcript),\n                    }\n                )\n            else:\n                writer.writerow(\n                    {\n                        ""wav_filename"": filename,\n                        ""wav_filesize"": file_size,\n                        ""transcript"": transcript,\n                    }\n                )\n    return rows\n\n\ndef _maybe_convert_wav(mp3_filename, wav_filename):\n    if not os.path.exists(wav_filename):\n        transformer = sox.Transformer()\n        transformer.convert(samplerate=SAMPLE_RATE)\n        try:\n            transformer.build(mp3_filename, wav_filename)\n        except sox.core.SoxError:\n            pass\n\nclass LabelFilter:\n    def __init__(self, normalize, alphabet, validate_fun):\n        self.normalize = normalize\n        self.alphabet = alphabet\n        self.validate_fun = validate_fun\n\n    def filter(self, label):\n        if self.normalize:\n            label = (\n                unicodedata.normalize(""NFKD"", label.strip())\n                .encode(""ascii"", ""ignore"")\n                .decode(""ascii"", ""ignore"")\n            )\n        label = self.validate_fun(label)\n        if self.alphabet and label:\n            try:\n                self.alphabet.encode(label)\n            except KeyError:\n                label = None\n        return label\n\ndef main():\n    parser = get_importers_parser(description=""Import CommonVoice v2.0 corpora"")\n    parser.add_argument(""tsv_dir"", help=""Directory containing tsv files"")\n    parser.add_argument(\n        ""--audio_dir"",\n        help=\'Directory containing the audio clips - defaults to ""<tsv_dir>/clips""\',\n    )\n    parser.add_argument(\n        ""--filter_alphabet"",\n        help=""Exclude samples with characters not in provided alphabet"",\n    )\n    parser.add_argument(\n        ""--normalize"",\n        action=""store_true"",\n        help=""Converts diacritic characters to their base ones"",\n    )\n    parser.add_argument(\n        ""--space_after_every_character"",\n        action=""store_true"",\n        help=""To help transcript join by white space"",\n    )\n\n    params = parser.parse_args()\n    validate_label = get_validate_label(params)\n\n    audio_dir = (\n        params.audio_dir if params.audio_dir else os.path.join(params.tsv_dir, ""clips"")\n    )\n    alphabet = Alphabet(params.filter_alphabet) if params.filter_alphabet else None\n\n    filter_obj = LabelFilter(params.normalize, alphabet, validate_label)\n    _preprocess_data(params.tsv_dir, audio_dir, filter_obj,\n                     params.space_after_every_character)\n\nif __name__ == ""__main__"":\n    main()\n'"
bin/import_fisher.py,0,"b'#!/usr/bin/env python\nimport codecs\nimport fnmatch\nimport os\nimport subprocess\nimport sys\nimport unicodedata\n\nimport librosa\nimport pandas\nimport soundfile  # <= Has an external dependency on libsndfile\n\nfrom deepspeech_training.util.importers import validate_label_eng as validate_label\n\n# Prerequisite: Having the sph2pipe tool in your PATH:\n# https://www.ldc.upenn.edu/language-resources/tools/sphere-conversion-tools\n\n\ndef _download_and_preprocess_data(data_dir):\n    # Assume data_dir contains extracted LDC2004S13, LDC2004T19, LDC2005S13, LDC2005T19\n\n    # Conditionally convert Fisher sph data to wav\n    _maybe_convert_wav(data_dir, ""LDC2004S13"", ""fisher-2004-wav"")\n    _maybe_convert_wav(data_dir, ""LDC2005S13"", ""fisher-2005-wav"")\n\n    # Conditionally split Fisher wav data\n    all_2004 = _split_wav_and_sentences(\n        data_dir,\n        original_data=""fisher-2004-wav"",\n        converted_data=""fisher-2004-split-wav"",\n        trans_data=os.path.join(""LDC2004T19"", ""fe_03_p1_tran"", ""data"", ""trans""),\n    )\n    all_2005 = _split_wav_and_sentences(\n        data_dir,\n        original_data=""fisher-2005-wav"",\n        converted_data=""fisher-2005-split-wav"",\n        trans_data=os.path.join(""LDC2005T19"", ""fe_03_p2_tran"", ""data"", ""trans""),\n    )\n\n    # The following files have incorrect transcripts that are much longer than\n    # their audio source. The result is that we end up with more labels than time\n    # slices, which breaks CTC.\n    all_2004.loc[\n        all_2004[""wav_filename""].str.endswith(""fe_03_00265-33.53-33.81.wav""),\n        ""transcript"",\n    ] = ""correct""\n    all_2004.loc[\n        all_2004[""wav_filename""].str.endswith(""fe_03_00991-527.39-528.3.wav""),\n        ""transcript"",\n    ] = ""that\'s one of those""\n    all_2005.loc[\n        all_2005[""wav_filename""].str.endswith(""fe_03_10282-344.42-344.84.wav""),\n        ""transcript"",\n    ] = ""they don\'t want""\n    all_2005.loc[\n        all_2005[""wav_filename""].str.endswith(""fe_03_10677-101.04-106.41.wav""),\n        ""transcript"",\n    ] = ""uh my mine yeah the german shepherd pitbull mix he snores almost as loud as i do""\n\n    # The following file is just a short sound and not at all transcribed like provided.\n    # So we just exclude it.\n    all_2004 = all_2004[\n        ~all_2004[""wav_filename""].str.endswith(""fe_03_00027-393.8-394.05.wav"")\n    ]\n\n    # The following file is far too long and would ruin our training batch size.\n    # So we just exclude it.\n    all_2005 = all_2005[\n        ~all_2005[""wav_filename""].str.endswith(""fe_03_11487-31.09-234.06.wav"")\n    ]\n\n    # The following file is too large for its transcript, so we just exclude it.\n    all_2004 = all_2004[\n        ~all_2004[""wav_filename""].str.endswith(""fe_03_01326-307.42-307.93.wav"")\n    ]\n\n    # Conditionally split Fisher data into train/validation/test sets\n    train_2004, dev_2004, test_2004 = _split_sets(all_2004)\n    train_2005, dev_2005, test_2005 = _split_sets(all_2005)\n\n    # Join 2004 and 2005 data\n    train_files = train_2004.append(train_2005)\n    dev_files = dev_2004.append(dev_2005)\n    test_files = test_2004.append(test_2005)\n\n    # Write sets to disk as CSV files\n    train_files.to_csv(os.path.join(data_dir, ""fisher-train.csv""), index=False)\n    dev_files.to_csv(os.path.join(data_dir, ""fisher-dev.csv""), index=False)\n    test_files.to_csv(os.path.join(data_dir, ""fisher-test.csv""), index=False)\n\n\ndef _maybe_convert_wav(data_dir, original_data, converted_data):\n    source_dir = os.path.join(data_dir, original_data)\n    target_dir = os.path.join(data_dir, converted_data)\n\n    # Conditionally convert sph files to wav files\n    if os.path.exists(target_dir):\n        print(""skipping maybe_convert_wav"")\n        return\n\n    # Create target_dir\n    os.makedirs(target_dir)\n\n    # Loop over sph files in source_dir and convert each to 16-bit PCM wav\n    for root, dirnames, filenames in os.walk(source_dir):\n        for filename in fnmatch.filter(filenames, ""*.sph""):\n            sph_file = os.path.join(root, filename)\n            for channel in [""1"", ""2""]:\n                wav_filename = (\n                    os.path.splitext(os.path.basename(sph_file))[0]\n                    + ""_c""\n                    + channel\n                    + "".wav""\n                )\n                wav_file = os.path.join(target_dir, wav_filename)\n                print(""converting {} to {}"".format(sph_file, wav_file))\n                subprocess.check_call(\n                    [""sph2pipe"", ""-c"", channel, ""-p"", ""-f"", ""rif"", sph_file, wav_file]\n                )\n\n\ndef _parse_transcriptions(trans_file):\n    segments = []\n    with codecs.open(trans_file, ""r"", ""utf-8"") as fin:\n        for line in fin:\n            if line.startswith(""#"") or len(line) <= 1:\n                continue\n\n            tokens = line.split()\n            start_time = float(tokens[0])\n            stop_time = float(tokens[1])\n            speaker = tokens[2]\n            transcript = "" "".join(tokens[3:])\n\n            # We need to do the encode-decode dance here because encode\n            # returns a bytes() object on Python 3, and text_to_char_array\n            # expects a string.\n            transcript = (\n                unicodedata.normalize(""NFKD"", transcript)\n                .encode(""ascii"", ""ignore"")\n                .decode(""ascii"", ""ignore"")\n            )\n\n            segments.append(\n                {\n                    ""start_time"": start_time,\n                    ""stop_time"": stop_time,\n                    ""speaker"": speaker,\n                    ""transcript"": transcript,\n                }\n            )\n    return segments\n\n\ndef _split_wav_and_sentences(data_dir, trans_data, original_data, converted_data):\n    trans_dir = os.path.join(data_dir, trans_data)\n    source_dir = os.path.join(data_dir, original_data)\n    target_dir = os.path.join(data_dir, converted_data)\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    files = []\n\n    # Loop over transcription files and split corresponding wav\n    for root, dirnames, filenames in os.walk(trans_dir):\n        for filename in fnmatch.filter(filenames, ""*.txt""):\n            trans_file = os.path.join(root, filename)\n            segments = _parse_transcriptions(trans_file)\n\n            # Open wav corresponding to transcription file\n            wav_filenames = [\n                os.path.splitext(os.path.basename(trans_file))[0]\n                + ""_c""\n                + channel\n                + "".wav""\n                for channel in [""1"", ""2""]\n            ]\n            wav_files = [\n                os.path.join(source_dir, wav_filename) for wav_filename in wav_filenames\n            ]\n\n            print(""splitting {} according to {}"".format(wav_files, trans_file))\n\n            origAudios = [\n                librosa.load(wav_file, sr=16000, mono=False) for wav_file in wav_files\n            ]\n\n            # Loop over segments and split wav_file for each segment\n            for segment in segments:\n                # Create wav segment filename\n                start_time = segment[""start_time""]\n                stop_time = segment[""stop_time""]\n                new_wav_filename = (\n                    os.path.splitext(os.path.basename(trans_file))[0]\n                    + ""-""\n                    + str(start_time)\n                    + ""-""\n                    + str(stop_time)\n                    + "".wav""\n                )\n                new_wav_file = os.path.join(target_dir, new_wav_filename)\n\n                channel = 0 if segment[""speaker""] == ""A:"" else 1\n                _split_and_resample_wav(\n                    origAudios[channel], start_time, stop_time, new_wav_file\n                )\n\n                new_wav_filesize = os.path.getsize(new_wav_file)\n                transcript = validate_label(segment[""transcript""])\n                if transcript != None:\n                    files.append(\n                        (os.path.abspath(new_wav_file), new_wav_filesize, transcript)\n                    )\n\n    return pandas.DataFrame(\n        data=files, columns=[""wav_filename"", ""wav_filesize"", ""transcript""]\n    )\n\n\ndef _split_audio(origAudio, start_time, stop_time):\n    audioData, frameRate = origAudio\n    nChannels = len(audioData.shape)\n    startIndex = int(start_time * frameRate)\n    stopIndex = int(stop_time * frameRate)\n    return (\n        audioData[startIndex:stopIndex]\n        if 1 == nChannels\n        else audioData[:, startIndex:stopIndex]\n    )\n\n\ndef _split_and_resample_wav(origAudio, start_time, stop_time, new_wav_file):\n    frameRate = origAudio[1]\n    chunkData = _split_audio(origAudio, start_time, stop_time)\n    soundfile.write(new_wav_file, chunkData, frameRate, ""PCM_16"")\n\n\ndef _split_sets(filelist):\n    # We initially split the entire set into 80% train and 20% test, then\n    # split the train set into 80% train and 20% validation.\n    train_beg = 0\n    train_end = int(0.8 * len(filelist))\n\n    dev_beg = int(0.8 * train_end)\n    dev_end = train_end\n    train_end = dev_beg\n\n    test_beg = dev_end\n    test_end = len(filelist)\n\n    return (\n        filelist[train_beg:train_end],\n        filelist[dev_beg:dev_end],\n        filelist[test_beg:test_end],\n    )\n\n\nif __name__ == ""__main__"":\n    _download_and_preprocess_data(sys.argv[1])\n'"
bin/import_freestmandarin.py,0,"b'#!/usr/bin/env python\nimport glob\nimport os\nimport tarfile\n\nimport numpy as np\nimport pandas\n\nfrom deepspeech_training.util.importers import get_importers_parser\n\nCOLUMN_NAMES = [""wav_filename"", ""wav_filesize"", ""transcript""]\n\n\ndef extract(archive_path, target_dir):\n    print(""Extracting {} into {}..."".format(archive_path, target_dir))\n    with tarfile.open(archive_path) as tar:\n        tar.extractall(target_dir)\n\n\ndef preprocess_data(tgz_file, target_dir):\n    # First extract main archive and sub-archives\n    extract(tgz_file, target_dir)\n    main_folder = os.path.join(target_dir, ""ST-CMDS-20170001_1-OS"")\n\n    # Folder structure is now:\n    # - ST-CMDS-20170001_1-OS/\n    #   - *.wav\n    #   - *.txt\n    #   - *.metadata\n\n    def load_set(glob_path):\n        set_files = []\n        for wav in glob.glob(glob_path):\n            wav_filename = wav\n            wav_filesize = os.path.getsize(wav)\n            txt_filename = os.path.splitext(wav_filename)[0] + "".txt""\n            with open(txt_filename, ""r"") as fin:\n                transcript = fin.read()\n            set_files.append((wav_filename, wav_filesize, transcript))\n        return set_files\n\n    # Load all files, then deterministically split into train/dev/test sets\n    all_files = load_set(os.path.join(main_folder, ""*.wav""))\n    df = pandas.DataFrame(data=all_files, columns=COLUMN_NAMES)\n    df.sort_values(by=""wav_filename"", inplace=True)\n\n    indices = np.arange(0, len(df))\n    np.random.seed(12345)\n    np.random.shuffle(indices)\n\n    # Total corpus size: 102600 samples. 5000 samples gives us 99% confidence\n    # level with a margin of error of under 2%.\n    test_indices = indices[-5000:]\n    dev_indices = indices[-10000:-5000]\n    train_indices = indices[:-10000]\n\n    train_files = df.iloc[train_indices]\n    durations = (train_files[""wav_filesize""] - 44) / 16000 / 2\n    train_files = train_files[durations <= 10.0]\n    print(""Trimming {} samples > 10 seconds"".format((durations > 10.0).sum()))\n    dest_csv = os.path.join(target_dir, ""freestmandarin_train.csv"")\n    print(""Saving train set into {}..."".format(dest_csv))\n    train_files.to_csv(dest_csv, index=False)\n\n    dev_files = df.iloc[dev_indices]\n    dest_csv = os.path.join(target_dir, ""freestmandarin_dev.csv"")\n    print(""Saving dev set into {}..."".format(dest_csv))\n    dev_files.to_csv(dest_csv, index=False)\n\n    test_files = df.iloc[test_indices]\n    dest_csv = os.path.join(target_dir, ""freestmandarin_test.csv"")\n    print(""Saving test set into {}..."".format(dest_csv))\n    test_files.to_csv(dest_csv, index=False)\n\n\ndef main():\n    # https://www.openslr.org/38/\n    parser = get_importers_parser(description=""Import Free ST Chinese Mandarin corpus"")\n    parser.add_argument(""tgz_file"", help=""Path to ST-CMDS-20170001_1-OS.tar.gz"")\n    parser.add_argument(\n        ""--target_dir"",\n        default="""",\n        help=""Target folder to extract files into and put the resulting CSVs. Defaults to same folder as the main archive."",\n    )\n    params = parser.parse_args()\n\n    if not params.target_dir:\n        params.target_dir = os.path.dirname(params.tgz_file)\n\n    preprocess_data(params.tgz_file, params.target_dir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
bin/import_gram_vaani.py,0,"b'#!/usr/bin/env python\n\nimport csv\nimport logging\nimport math\nimport os\nimport subprocess\nimport urllib\nfrom pathlib import Path\n\nimport pandas as pd\nfrom sox import Transformer\n\nimport swifter\nfrom deepspeech_training.util.importers import get_importers_parser, get_validate_label\n\n__version__ = ""0.1.0""\n_logger = logging.getLogger(__name__)\n\n\nMAX_SECS = 10\nBITDEPTH = 16\nN_CHANNELS = 1\nSAMPLE_RATE = 16000\n\nDEV_PERCENTAGE = 0.10\nTRAIN_PERCENTAGE = 0.80\n\n\ndef parse_args(args):\n    """"""Parse command line parameters\n    Args:\n      args ([str]): Command line parameters as list of strings\n    Returns:\n      :obj:`argparse.Namespace`: command line parameters namespace\n    """"""\n    parser = get_importers_parser(description=""Imports GramVaani data for Deep Speech"")\n    parser.add_argument(\n        ""--version"",\n        action=""version"",\n        version=""GramVaaniImporter {ver}"".format(ver=__version__),\n    )\n    parser.add_argument(\n        ""-v"",\n        ""--verbose"",\n        action=""store_const"",\n        required=False,\n        help=""set loglevel to INFO"",\n        dest=""loglevel"",\n        const=logging.INFO,\n    )\n    parser.add_argument(\n        ""-vv"",\n        ""--very-verbose"",\n        action=""store_const"",\n        required=False,\n        help=""set loglevel to DEBUG"",\n        dest=""loglevel"",\n        const=logging.DEBUG,\n    )\n    parser.add_argument(\n        ""-c"",\n        ""--csv_filename"",\n        required=True,\n        help=""Path to the GramVaani csv"",\n        dest=""csv_filename"",\n    )\n    parser.add_argument(\n        ""-t"",\n        ""--target_dir"",\n        required=True,\n        help=""Directory in which to save the importer GramVaani data"",\n        dest=""target_dir"",\n    )\n    return parser.parse_args(args)\n\n\ndef setup_logging(level):\n    """"""Setup basic logging\n    Args:\n      level (int): minimum log level for emitting messages\n    """"""\n    format = ""[%(asctime)s] %(levelname)s:%(name)s:%(message)s""\n    logging.basicConfig(\n        level=level, stream=sys.stdout, format=format, datefmt=""%Y-%m-%d %H:%M:%S""\n    )\n\n\nclass GramVaaniCSV:\n    """"""GramVaaniCSV representing a GramVaani dataset.\n    Args:\n      csv_filename (str): Path to the GramVaani csv\n    Attributes:\n        data (:class:`pandas.DataFrame`): `pandas.DataFrame` Containing the GramVaani csv data\n    """"""\n\n    def __init__(self, csv_filename):\n        self.data = self._parse_csv(csv_filename)\n\n    def _parse_csv(self, csv_filename):\n        _logger.info(""Parsing csv file...%s"", os.path.abspath(csv_filename))\n        data = pd.read_csv(\n            os.path.abspath(csv_filename),\n            names=[\n                ""piece_id"",\n                ""audio_url"",\n                ""transcript_labelled"",\n                ""transcript"",\n                ""labels"",\n                ""content_filename"",\n                ""audio_length"",\n                ""user_id"",\n            ],\n            usecols=[""audio_url"", ""transcript"", ""audio_length""],\n            skiprows=[0],\n            engine=""python"",\n            encoding=""utf-8"",\n            quotechar=\'""\',\n            quoting=csv.QUOTE_ALL,\n        )\n        data.dropna(inplace=True)\n        _logger.info(""Parsed %d lines csv file."" % len(data))\n        return data\n\n\nclass GramVaaniDownloader:\n    """"""GramVaaniDownloader downloads a GramVaani dataset.\n    Args:\n      gram_vaani_csv (GramVaaniCSV): A GramVaaniCSV representing the data to download\n      target_dir (str): The path to download the data to\n    Attributes:\n        data (:class:`pandas.DataFrame`): `pandas.DataFrame` Containing the GramVaani csv data\n    """"""\n\n    def __init__(self, gram_vaani_csv, target_dir):\n        self.target_dir = target_dir\n        self.data = gram_vaani_csv.data\n\n    def download(self):\n        """"""Downloads the data associated with this instance\n        Return:\n          mp3_directory (os.path): The directory into which the associated mp3\'s were downloaded\n        """"""\n        mp3_directory = self._pre_download()\n        self.data.swifter.apply(\n            func=lambda arg: self._download(*arg, mp3_directory), axis=1, raw=True\n        )\n        return mp3_directory\n\n    def _pre_download(self):\n        mp3_directory = os.path.join(self.target_dir, ""mp3"")\n        if not os.path.exists(self.target_dir):\n            _logger.info(""Creating directory...%s"", self.target_dir)\n            os.mkdir(self.target_dir)\n        if not os.path.exists(mp3_directory):\n            _logger.info(""Creating directory...%s"", mp3_directory)\n            os.mkdir(mp3_directory)\n        return mp3_directory\n\n    def _download(self, audio_url, transcript, audio_length, mp3_directory):\n        if audio_url == ""audio_url"":\n            return\n        mp3_filename = os.path.join(mp3_directory, os.path.basename(audio_url))\n        if not os.path.exists(mp3_filename):\n            _logger.debug(""Downloading mp3 file...%s"", audio_url)\n            urllib.request.urlretrieve(audio_url, mp3_filename)\n        else:\n            _logger.debug(""Already downloaded mp3 file...%s"", audio_url)\n\n\nclass GramVaaniConverter:\n    """"""GramVaaniConverter converts the mp3\'s to wav\'s for a GramVaani dataset.\n    Args:\n      target_dir (str): The path to download the data from\n      mp3_directory (os.path): The path containing the GramVaani mp3\'s\n    Attributes:\n        target_dir (str): The target directory passed as a command line argument\n        mp3_directory (os.path): The path containing the GramVaani mp3\'s\n    """"""\n\n    def __init__(self, target_dir, mp3_directory):\n        self.target_dir = target_dir\n        self.mp3_directory = Path(mp3_directory)\n\n    def convert(self):\n        """"""Converts the mp3\'s associated with this instance to wav\'s\n        Return:\n          wav_directory (os.path): The directory into which the associated wav\'s were downloaded\n        """"""\n        wav_directory = self._pre_convert()\n        for mp3_filename in self.mp3_directory.glob(""**/*.mp3""):\n            wav_filename = os.path.join(\n                wav_directory,\n                os.path.splitext(os.path.basename(mp3_filename))[0] + "".wav"",\n            )\n            if not os.path.exists(wav_filename):\n                _logger.debug(\n                    ""Converting mp3 file %s to wav file %s""\n                    % (mp3_filename, wav_filename)\n                )\n                transformer = Transformer()\n                transformer.convert(\n                    samplerate=SAMPLE_RATE, n_channels=N_CHANNELS, bitdepth=BITDEPTH\n                )\n                transformer.build(str(mp3_filename), str(wav_filename))\n            else:\n                _logger.debug(\n                    ""Already converted mp3 file %s to wav file %s""\n                    % (mp3_filename, wav_filename)\n                )\n        return wav_directory\n\n    def _pre_convert(self):\n        wav_directory = os.path.join(self.target_dir, ""wav"")\n        if not os.path.exists(self.target_dir):\n            _logger.info(""Creating directory...%s"", self.target_dir)\n            os.mkdir(self.target_dir)\n        if not os.path.exists(wav_directory):\n            _logger.info(""Creating directory...%s"", wav_directory)\n            os.mkdir(wav_directory)\n        return wav_directory\n\n\nclass GramVaaniDataSets:\n    def __init__(self, target_dir, wav_directory, gram_vaani_csv):\n        self.target_dir = target_dir\n        self.wav_directory = wav_directory\n        self.csv_data = gram_vaani_csv.data\n        self.raw = pd.DataFrame(columns=[""wav_filename"", ""wav_filesize"", ""transcript""])\n        self.valid = pd.DataFrame(\n            columns=[""wav_filename"", ""wav_filesize"", ""transcript""]\n        )\n        self.train = pd.DataFrame(\n            columns=[""wav_filename"", ""wav_filesize"", ""transcript""]\n        )\n        self.dev = pd.DataFrame(columns=[""wav_filename"", ""wav_filesize"", ""transcript""])\n        self.test = pd.DataFrame(columns=[""wav_filename"", ""wav_filesize"", ""transcript""])\n\n    def create(self):\n        self._convert_csv_data_to_raw_data()\n        self.raw.index = range(len(self.raw.index))\n        self.valid = self.raw[self._is_valid_raw_rows()]\n        self.valid = self.valid.sample(frac=1).reset_index(drop=True)\n        train_size, dev_size, test_size = self._calculate_data_set_sizes()\n        self.train = self.valid.loc[0:train_size]\n        self.dev = self.valid.loc[train_size : train_size + dev_size]\n        self.test = self.valid.loc[\n            train_size + dev_size : train_size + dev_size + test_size\n        ]\n\n    def _convert_csv_data_to_raw_data(self):\n        self.raw[[""wav_filename"", ""wav_filesize"", ""transcript""]] = self.csv_data[\n            [""audio_url"", ""transcript"", ""audio_length""]\n        ].swifter.apply(\n            func=lambda arg: self._convert_csv_data_to_raw_data_impl(*arg),\n            axis=1,\n            raw=True,\n        )\n        self.raw.reset_index()\n\n    def _convert_csv_data_to_raw_data_impl(self, audio_url, transcript, audio_length):\n        if audio_url == ""audio_url"":\n            return pd.Series([""wav_filename"", ""wav_filesize"", ""transcript""])\n        mp3_filename = os.path.basename(audio_url)\n        wav_relative_filename = os.path.join(\n            ""wav"", os.path.splitext(os.path.basename(mp3_filename))[0] + "".wav""\n        )\n        wav_filesize = os.path.getsize(\n            os.path.join(self.target_dir, wav_relative_filename)\n        )\n        transcript = validate_label(transcript)\n        if None == transcript:\n            transcript = """"\n        return pd.Series([wav_relative_filename, wav_filesize, transcript])\n\n    def _is_valid_raw_rows(self):\n        is_valid_raw_transcripts = self._is_valid_raw_transcripts()\n        is_valid_raw_wav_frames = self._is_valid_raw_wav_frames()\n        is_valid_raw_row = [\n            (is_valid_raw_transcript & is_valid_raw_wav_frame)\n            for is_valid_raw_transcript, is_valid_raw_wav_frame in zip(\n                is_valid_raw_transcripts, is_valid_raw_wav_frames\n            )\n        ]\n        series = pd.Series(is_valid_raw_row)\n        return series\n\n    def _is_valid_raw_transcripts(self):\n        return pd.Series([bool(transcript) for transcript in self.raw.transcript])\n\n    def _is_valid_raw_wav_frames(self):\n        transcripts = [str(transcript) for transcript in self.raw.transcript]\n        wav_filepaths = [\n            os.path.join(self.target_dir, str(wav_filename))\n            for wav_filename in self.raw.wav_filename\n        ]\n        wav_frames = [\n            int(\n                subprocess.check_output(\n                    [""soxi"", ""-s"", wav_filepath], stderr=subprocess.STDOUT\n                )\n            )\n            for wav_filepath in wav_filepaths\n        ]\n        is_valid_raw_wav_frames = [\n            self._is_wav_frame_valid(wav_frame, transcript)\n            for wav_frame, transcript in zip(wav_frames, transcripts)\n        ]\n        return pd.Series(is_valid_raw_wav_frames)\n\n    def _is_wav_frame_valid(self, wav_frame, transcript):\n        is_wav_frame_valid = True\n        if int(wav_frame / SAMPLE_RATE * 1000 / 10 / 2) < len(str(transcript)):\n            is_wav_frame_valid = False\n        elif wav_frame / SAMPLE_RATE > MAX_SECS:\n            is_wav_frame_valid = False\n        return is_wav_frame_valid\n\n    def _calculate_data_set_sizes(self):\n        total_size = len(self.valid)\n        dev_size = math.floor(total_size * DEV_PERCENTAGE)\n        train_size = math.floor(total_size * TRAIN_PERCENTAGE)\n        test_size = total_size - (train_size + dev_size)\n        return (train_size, dev_size, test_size)\n\n    def save(self):\n        datasets = [""train"", ""dev"", ""test""]\n        for dataset in datasets:\n            self._save(dataset)\n\n    def _save(self, dataset):\n        dataset_path = os.path.join(self.target_dir, dataset + "".csv"")\n        dataframe = getattr(self, dataset)\n        dataframe.to_csv(\n            dataset_path,\n            index=False,\n            encoding=""utf-8"",\n            escapechar=""\\\\"",\n            quoting=csv.QUOTE_MINIMAL,\n        )\n\n\ndef main(args):\n    """"""Main entry point allowing external calls\n    Args:\n      args ([str]): command line parameter list\n    """"""\n    args = parse_args(args)\n    validate_label = get_validate_label(args)\n    setup_logging(args.loglevel)\n    _logger.info(""Starting GramVaani importer..."")\n    _logger.info(""Starting loading GramVaani csv..."")\n    csv = GramVaaniCSV(args.csv_filename)\n    _logger.info(""Starting downloading GramVaani mp3\'s..."")\n    downloader = GramVaaniDownloader(csv, args.target_dir)\n    mp3_directory = downloader.download()\n    _logger.info(""Starting converting GramVaani mp3\'s to wav\'s..."")\n    converter = GramVaaniConverter(args.target_dir, mp3_directory)\n    wav_directory = converter.convert()\n    datasets = GramVaaniDataSets(args.target_dir, wav_directory, csv)\n    datasets.create()\n    datasets.save()\n    _logger.info(""Finished GramVaani importer..."")\n\n\nmain(sys.argv[1:])\n'"
bin/import_ldc93s1.py,0,"b'#!/usr/bin/env python\nimport os\nimport sys\n\nimport pandas\n\nfrom deepspeech_training.util.downloader import maybe_download\n\n\ndef _download_and_preprocess_data(data_dir):\n    # Conditionally download data\n    LDC93S1_BASE = ""LDC93S1""\n    LDC93S1_BASE_URL = ""https://catalog.ldc.upenn.edu/desc/addenda/""\n    local_file = maybe_download(\n        LDC93S1_BASE + "".wav"", data_dir, LDC93S1_BASE_URL + LDC93S1_BASE + "".wav""\n    )\n    trans_file = maybe_download(\n        LDC93S1_BASE + "".txt"", data_dir, LDC93S1_BASE_URL + LDC93S1_BASE + "".txt""\n    )\n    with open(trans_file, ""r"") as fin:\n        transcript = "" "".join(fin.read().strip().lower().split("" "")[2:]).replace(\n            ""."", """"\n        )\n\n    df = pandas.DataFrame(\n        data=[(os.path.abspath(local_file), os.path.getsize(local_file), transcript)],\n        columns=[""wav_filename"", ""wav_filesize"", ""transcript""],\n    )\n    df.to_csv(os.path.join(data_dir, ""ldc93s1.csv""), index=False)\n\n\nif __name__ == ""__main__"":\n    _download_and_preprocess_data(sys.argv[1])\n'"
bin/import_librivox.py,0,"b'#!/usr/bin/env python\nimport codecs\nimport fnmatch\nimport os\nimport subprocess\nimport sys\nimport tarfile\nimport unicodedata\n\nimport pandas\nimport progressbar\nfrom sox import Transformer\nfrom tensorflow.python.platform import gfile\n\nfrom deepspeech_training.util.downloader import maybe_download\n\nSAMPLE_RATE = 16000\n\n\ndef _download_and_preprocess_data(data_dir):\n    # Conditionally download data to data_dir\n    print(\n        ""Downloading Librivox data set (55GB) into {} if not already present..."".format(\n            data_dir\n        )\n    )\n    with progressbar.ProgressBar(max_value=7, widget=progressbar.AdaptiveETA) as bar:\n        TRAIN_CLEAN_100_URL = (\n            ""http://www.openslr.org/resources/12/train-clean-100.tar.gz""\n        )\n        TRAIN_CLEAN_360_URL = (\n            ""http://www.openslr.org/resources/12/train-clean-360.tar.gz""\n        )\n        TRAIN_OTHER_500_URL = (\n            ""http://www.openslr.org/resources/12/train-other-500.tar.gz""\n        )\n\n        DEV_CLEAN_URL = ""http://www.openslr.org/resources/12/dev-clean.tar.gz""\n        DEV_OTHER_URL = ""http://www.openslr.org/resources/12/dev-other.tar.gz""\n\n        TEST_CLEAN_URL = ""http://www.openslr.org/resources/12/test-clean.tar.gz""\n        TEST_OTHER_URL = ""http://www.openslr.org/resources/12/test-other.tar.gz""\n\n        def filename_of(x):\n            return os.path.split(x)[1]\n\n        train_clean_100 = maybe_download(\n            filename_of(TRAIN_CLEAN_100_URL), data_dir, TRAIN_CLEAN_100_URL\n        )\n        bar.update(0)\n        train_clean_360 = maybe_download(\n            filename_of(TRAIN_CLEAN_360_URL), data_dir, TRAIN_CLEAN_360_URL\n        )\n        bar.update(1)\n        train_other_500 = maybe_download(\n            filename_of(TRAIN_OTHER_500_URL), data_dir, TRAIN_OTHER_500_URL\n        )\n        bar.update(2)\n\n        dev_clean = maybe_download(filename_of(DEV_CLEAN_URL), data_dir, DEV_CLEAN_URL)\n        bar.update(3)\n        dev_other = maybe_download(filename_of(DEV_OTHER_URL), data_dir, DEV_OTHER_URL)\n        bar.update(4)\n\n        test_clean = maybe_download(\n            filename_of(TEST_CLEAN_URL), data_dir, TEST_CLEAN_URL\n        )\n        bar.update(5)\n        test_other = maybe_download(\n            filename_of(TEST_OTHER_URL), data_dir, TEST_OTHER_URL\n        )\n        bar.update(6)\n\n    # Conditionally extract LibriSpeech data\n    # We extract each archive into data_dir, but test for existence in\n    # data_dir/LibriSpeech because the archives share that root.\n    print(""Extracting librivox data if not already extracted..."")\n    with progressbar.ProgressBar(max_value=7, widget=progressbar.AdaptiveETA) as bar:\n        LIBRIVOX_DIR = ""LibriSpeech""\n        work_dir = os.path.join(data_dir, LIBRIVOX_DIR)\n\n        _maybe_extract(\n            data_dir, os.path.join(LIBRIVOX_DIR, ""train-clean-100""), train_clean_100\n        )\n        bar.update(0)\n        _maybe_extract(\n            data_dir, os.path.join(LIBRIVOX_DIR, ""train-clean-360""), train_clean_360\n        )\n        bar.update(1)\n        _maybe_extract(\n            data_dir, os.path.join(LIBRIVOX_DIR, ""train-other-500""), train_other_500\n        )\n        bar.update(2)\n\n        _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, ""dev-clean""), dev_clean)\n        bar.update(3)\n        _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, ""dev-other""), dev_other)\n        bar.update(4)\n\n        _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, ""test-clean""), test_clean)\n        bar.update(5)\n        _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, ""test-other""), test_other)\n        bar.update(6)\n\n    # Convert FLAC data to wav, from:\n    #  data_dir/LibriSpeech/split/1/2/1-2-3.flac\n    # to:\n    #  data_dir/LibriSpeech/split-wav/1-2-3.wav\n    #\n    # And split LibriSpeech transcriptions, from:\n    #  data_dir/LibriSpeech/split/1/2/1-2.trans.txt\n    # to:\n    #  data_dir/LibriSpeech/split-wav/1-2-0.txt\n    #  data_dir/LibriSpeech/split-wav/1-2-1.txt\n    #  data_dir/LibriSpeech/split-wav/1-2-2.txt\n    #  ...\n    print(""Converting FLAC to WAV and splitting transcriptions..."")\n    with progressbar.ProgressBar(max_value=7, widget=progressbar.AdaptiveETA) as bar:\n        train_100 = _convert_audio_and_split_sentences(\n            work_dir, ""train-clean-100"", ""train-clean-100-wav""\n        )\n        bar.update(0)\n        train_360 = _convert_audio_and_split_sentences(\n            work_dir, ""train-clean-360"", ""train-clean-360-wav""\n        )\n        bar.update(1)\n        train_500 = _convert_audio_and_split_sentences(\n            work_dir, ""train-other-500"", ""train-other-500-wav""\n        )\n        bar.update(2)\n\n        dev_clean = _convert_audio_and_split_sentences(\n            work_dir, ""dev-clean"", ""dev-clean-wav""\n        )\n        bar.update(3)\n        dev_other = _convert_audio_and_split_sentences(\n            work_dir, ""dev-other"", ""dev-other-wav""\n        )\n        bar.update(4)\n\n        test_clean = _convert_audio_and_split_sentences(\n            work_dir, ""test-clean"", ""test-clean-wav""\n        )\n        bar.update(5)\n        test_other = _convert_audio_and_split_sentences(\n            work_dir, ""test-other"", ""test-other-wav""\n        )\n        bar.update(6)\n\n    # Write sets to disk as CSV files\n    train_100.to_csv(\n        os.path.join(data_dir, ""librivox-train-clean-100.csv""), index=False\n    )\n    train_360.to_csv(\n        os.path.join(data_dir, ""librivox-train-clean-360.csv""), index=False\n    )\n    train_500.to_csv(\n        os.path.join(data_dir, ""librivox-train-other-500.csv""), index=False\n    )\n\n    dev_clean.to_csv(os.path.join(data_dir, ""librivox-dev-clean.csv""), index=False)\n    dev_other.to_csv(os.path.join(data_dir, ""librivox-dev-other.csv""), index=False)\n\n    test_clean.to_csv(os.path.join(data_dir, ""librivox-test-clean.csv""), index=False)\n    test_other.to_csv(os.path.join(data_dir, ""librivox-test-other.csv""), index=False)\n\n\ndef _maybe_extract(data_dir, extracted_data, archive):\n    # If data_dir/extracted_data does not exist, extract archive in data_dir\n    if not gfile.Exists(os.path.join(data_dir, extracted_data)):\n        tar = tarfile.open(archive)\n        tar.extractall(data_dir)\n        tar.close()\n\n\ndef _convert_audio_and_split_sentences(extracted_dir, data_set, dest_dir):\n    source_dir = os.path.join(extracted_dir, data_set)\n    target_dir = os.path.join(extracted_dir, dest_dir)\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Loop over transcription files and split each one\n    #\n    # The format for each file 1-2.trans.txt is:\n    #  1-2-0 transcription of 1-2-0.flac\n    #  1-2-1 transcription of 1-2-1.flac\n    #  ...\n    #\n    # Each file is then split into several files:\n    #  1-2-0.txt (contains transcription of 1-2-0.flac)\n    #  1-2-1.txt (contains transcription of 1-2-1.flac)\n    #  ...\n    #\n    # We also convert the corresponding FLACs to WAV in the same pass\n    files = []\n    for root, dirnames, filenames in os.walk(source_dir):\n        for filename in fnmatch.filter(filenames, ""*.trans.txt""):\n            trans_filename = os.path.join(root, filename)\n            with codecs.open(trans_filename, ""r"", ""utf-8"") as fin:\n                for line in fin:\n                    # Parse each segment line\n                    first_space = line.find("" "")\n                    seqid, transcript = line[:first_space], line[first_space + 1 :]\n\n                    # We need to do the encode-decode dance here because encode\n                    # returns a bytes() object on Python 3, and text_to_char_array\n                    # expects a string.\n                    transcript = (\n                        unicodedata.normalize(""NFKD"", transcript)\n                        .encode(""ascii"", ""ignore"")\n                        .decode(""ascii"", ""ignore"")\n                    )\n\n                    transcript = transcript.lower().strip()\n\n                    # Convert corresponding FLAC to a WAV\n                    flac_file = os.path.join(root, seqid + "".flac"")\n                    wav_file = os.path.join(target_dir, seqid + "".wav"")\n                    if not os.path.exists(wav_file):\n                        tfm = Transformer()\n                        tfm.set_output_format(rate=SAMPLE_RATE)\n                        tfm.build(flac_file, wav_file)\n                    wav_filesize = os.path.getsize(wav_file)\n\n                    files.append((os.path.abspath(wav_file), wav_filesize, transcript))\n\n    return pandas.DataFrame(\n        data=files, columns=[""wav_filename"", ""wav_filesize"", ""transcript""]\n    )\n\n\nif __name__ == ""__main__"":\n    _download_and_preprocess_data(sys.argv[1])\n'"
bin/import_lingua_libre.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport csv\nimport os\nimport re\nimport subprocess\nimport unicodedata\nimport zipfile\nfrom glob import glob\nfrom multiprocessing import Pool\n\nimport progressbar\nimport sox\n\nfrom deepspeech_training.util.downloader import SIMPLE_BAR, maybe_download\nfrom deepspeech_training.util.importers import (\n    get_counter,\n    get_imported_samples,\n    get_importers_parser,\n    get_validate_label,\n    print_import_report,\n)\nfrom deepspeech_training.util.text import Alphabet\n\nFIELDNAMES = [""wav_filename"", ""wav_filesize"", ""transcript""]\nSAMPLE_RATE = 16000\nBITDEPTH = 16\nN_CHANNELS = 1\nMAX_SECS = 10\n\nARCHIVE_DIR_NAME = ""lingua_libre""\nARCHIVE_NAME = ""Q{qId}-{iso639_3}-{language_English_name}.zip""\nARCHIVE_URL = ""https://lingualibre.fr/datasets/"" + ARCHIVE_NAME\n\n\ndef _download_and_preprocess_data(target_dir):\n    # Making path absolute\n    target_dir = os.path.abspath(target_dir)\n    # Conditionally download data\n    archive_path = maybe_download(ARCHIVE_NAME, target_dir, ARCHIVE_URL)\n    # Conditionally extract data\n    _maybe_extract(target_dir, ARCHIVE_DIR_NAME, archive_path)\n    # Produce CSV files and convert ogg data to wav\n    _maybe_convert_sets(target_dir, ARCHIVE_DIR_NAME)\n\n\ndef _maybe_extract(target_dir, extracted_data, archive_path):\n    # If target_dir/extracted_data does not exist, extract archive in target_dir\n    extracted_path = os.path.join(target_dir, extracted_data)\n    if not os.path.exists(extracted_path):\n        print(\'No directory ""%s"" - extracting archive...\' % extracted_path)\n        if not os.path.isdir(extracted_path):\n            os.mkdir(extracted_path)\n        with zipfile.ZipFile(archive_path) as zip_f:\n            zip_f.extractall(extracted_path)\n    else:\n        print(\'Found directory ""%s"" - not extracting it from archive.\' % archive_path)\n\n\ndef one_sample(sample):\n    """""" Take a audio file, and optionally convert it to 16kHz WAV """"""\n    ogg_filename = sample[0]\n    # Storing wav files next to the ogg ones - just with a different suffix\n    wav_filename = os.path.splitext(ogg_filename)[0] + "".wav""\n    _maybe_convert_wav(ogg_filename, wav_filename)\n    file_size = -1\n    frames = 0\n    if os.path.exists(wav_filename):\n        file_size = os.path.getsize(wav_filename)\n        frames = int(\n            subprocess.check_output(\n                [""soxi"", ""-s"", wav_filename], stderr=subprocess.STDOUT\n            )\n        )\n    label = label_filter(sample[1])\n    rows = []\n    counter = get_counter()\n\n    if file_size == -1:\n        # Excluding samples that failed upon conversion\n        counter[""failed""] += 1\n    elif label is None:\n        # Excluding samples that failed on label validation\n        counter[""invalid_label""] += 1\n    elif int(frames / SAMPLE_RATE * 1000 / 10 / 2) < len(str(label)):\n        # Excluding samples that are too short to fit the transcript\n        counter[""too_short""] += 1\n    elif frames / SAMPLE_RATE > MAX_SECS:\n        # Excluding very long samples to keep a reasonable batch-size\n        counter[""too_long""] += 1\n    else:\n        # This one is good - keep it for the target CSV\n        rows.append((wav_filename, file_size, label))\n        counter[""imported_time""] += frames\n    counter[""all""] += 1\n    counter[""total_time""] += frames\n\n    return (counter, rows)\n\n\ndef _maybe_convert_sets(target_dir, extracted_data):\n    extracted_dir = os.path.join(target_dir, extracted_data)\n    # override existing CSV with normalized one\n    target_csv_template = os.path.join(\n        target_dir, ARCHIVE_DIR_NAME + ""_"" + ARCHIVE_NAME.replace("".zip"", ""_{}.csv"")\n    )\n    if os.path.isfile(target_csv_template):\n        return\n\n    ogg_root_dir = os.path.join(extracted_dir, ARCHIVE_NAME.replace("".zip"", """"))\n\n    # Get audiofile path and transcript for each sentence in tsv\n    samples = []\n    glob_dir = os.path.join(ogg_root_dir, ""**/*.ogg"")\n    for record in glob(glob_dir, recursive=True):\n        record_file = record.replace(ogg_root_dir + os.path.sep, """")\n        if record_filter(record_file):\n            samples.append(\n                (\n                    os.path.join(ogg_root_dir, record_file),\n                    os.path.splitext(os.path.basename(record_file))[0],\n                )\n            )\n\n    counter = get_counter()\n    num_samples = len(samples)\n    rows = []\n\n    print(""Importing ogg files..."")\n    pool = Pool()\n    bar = progressbar.ProgressBar(max_value=num_samples, widgets=SIMPLE_BAR)\n    for i, processed in enumerate(pool.imap_unordered(one_sample, samples), start=1):\n        counter += processed[0]\n        rows += processed[1]\n        bar.update(i)\n    bar.update(num_samples)\n    pool.close()\n    pool.join()\n\n    with open(target_csv_template.format(""train""), ""w"", encoding=""utf-8"", newline="""") as train_csv_file:  # 80%\n        with open(target_csv_template.format(""dev""), ""w"", encoding=""utf-8"", newline="""") as dev_csv_file:  # 10%\n            with open(target_csv_template.format(""test""), ""w"", encoding=""utf-8"", newline="""") as test_csv_file:  # 10%\n                train_writer = csv.DictWriter(train_csv_file, fieldnames=FIELDNAMES)\n                train_writer.writeheader()\n                dev_writer = csv.DictWriter(dev_csv_file, fieldnames=FIELDNAMES)\n                dev_writer.writeheader()\n                test_writer = csv.DictWriter(test_csv_file, fieldnames=FIELDNAMES)\n                test_writer.writeheader()\n\n                for i, item in enumerate(rows):\n                    transcript = validate_label(item[2])\n                    if not transcript:\n                        continue\n                    wav_filename = os.path.join(\n                        ogg_root_dir, item[0].replace("".ogg"", "".wav"")\n                    )\n                    i_mod = i % 10\n                    if i_mod == 0:\n                        writer = test_writer\n                    elif i_mod == 1:\n                        writer = dev_writer\n                    else:\n                        writer = train_writer\n                    writer.writerow(\n                        dict(\n                            wav_filename=wav_filename,\n                            wav_filesize=os.path.getsize(wav_filename),\n                            transcript=transcript,\n                        )\n                    )\n\n    imported_samples = get_imported_samples(counter)\n    assert counter[""all""] == num_samples\n    assert len(rows) == imported_samples\n\n    print_import_report(counter, SAMPLE_RATE, MAX_SECS)\n\n\ndef _maybe_convert_wav(ogg_filename, wav_filename):\n    if not os.path.exists(wav_filename):\n        transformer = sox.Transformer()\n        transformer.convert(samplerate=SAMPLE_RATE, n_channels=N_CHANNELS, bitdepth=BITDEPTH)\n        try:\n            transformer.build(ogg_filename, wav_filename)\n        except sox.core.SoxError as ex:\n            print(""SoX processing error"", ex, ogg_filename, wav_filename)\n\n\ndef handle_args():\n    parser = get_importers_parser(\n        description=""Importer for LinguaLibre dataset. Check https://lingualibre.fr/wiki/Help:Download_from_LinguaLibre for details.""\n    )\n    parser.add_argument(dest=""target_dir"")\n    parser.add_argument(\n        ""--qId"", type=int, required=True, help=""LinguaLibre language qId""\n    )\n    parser.add_argument(\n        ""--iso639-3"", type=str, required=True, help=""ISO639-3 language code""\n    )\n    parser.add_argument(\n        ""--english-name"", type=str, required=True, help=""Enligh name of the language""\n    )\n    parser.add_argument(\n        ""--filter_alphabet"",\n        help=""Exclude samples with characters not in provided alphabet"",\n    )\n    parser.add_argument(\n        ""--normalize"",\n        action=""store_true"",\n        help=""Converts diacritic characters to their base ones"",\n    )\n    parser.add_argument(\n        ""--bogus-records"",\n        type=argparse.FileType(""r""),\n        required=False,\n        help=""Text file listing well-known bogus record to skip from importing, from https://lingualibre.fr/wiki/LinguaLibre:Misleading_items"",\n    )\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    CLI_ARGS = handle_args()\n    ALPHABET = Alphabet(CLI_ARGS.filter_alphabet) if CLI_ARGS.filter_alphabet else None\n    validate_label = get_validate_label(CLI_ARGS)\n\n    bogus_regexes = []\n    if CLI_ARGS.bogus_records:\n        for line in CLI_ARGS.bogus_records:\n            bogus_regexes.append(re.compile(line.strip()))\n\n    def record_filter(path):\n        if any(regex.match(path) for regex in bogus_regexes):\n            print(""Reject"", path)\n            return False\n        return True\n\n    def label_filter(label):\n        if CLI_ARGS.normalize:\n            label = (\n                unicodedata.normalize(""NFKD"", label.strip())\n                .encode(""ascii"", ""ignore"")\n                .decode(""ascii"", ""ignore"")\n            )\n        label = validate_label(label)\n        if ALPHABET and label:\n            try:\n                ALPHABET.encode(label)\n            except KeyError:\n                label = None\n        return label\n\n    ARCHIVE_NAME = ARCHIVE_NAME.format(\n        qId=CLI_ARGS.qId,\n        iso639_3=CLI_ARGS.iso639_3,\n        language_English_name=CLI_ARGS.english_name,\n    )\n    ARCHIVE_URL = ARCHIVE_URL.format(\n        qId=CLI_ARGS.qId,\n        iso639_3=CLI_ARGS.iso639_3,\n        language_English_name=CLI_ARGS.english_name,\n    )\n    _download_and_preprocess_data(target_dir=CLI_ARGS.target_dir)\n'"
bin/import_m-ailabs.py,0,"b'#!/usr/bin/env python3\n# pylint: disable=invalid-name\nimport csv\nimport os\nimport subprocess\nimport tarfile\nimport unicodedata\nfrom glob import glob\nfrom multiprocessing import Pool\n\nimport progressbar\n\nfrom deepspeech_training.util.downloader import SIMPLE_BAR, maybe_download\nfrom deepspeech_training.util.importers import (\n    get_counter,\n    get_imported_samples,\n    get_importers_parser,\n    get_validate_label,\n    print_import_report,\n)\nfrom deepspeech_training.util.text import Alphabet\n\nFIELDNAMES = [""wav_filename"", ""wav_filesize"", ""transcript""]\nSAMPLE_RATE = 16000\nMAX_SECS = 15\n\nARCHIVE_DIR_NAME = ""{language}""\nARCHIVE_NAME = ""{language}.tgz""\nARCHIVE_URL = ""http://www.caito.de/data/Training/stt_tts/"" + ARCHIVE_NAME\n\n\ndef _download_and_preprocess_data(target_dir):\n    # Making path absolute\n    target_dir = os.path.abspath(target_dir)\n    # Conditionally download data\n    archive_path = maybe_download(ARCHIVE_NAME, target_dir, ARCHIVE_URL)\n    # Conditionally extract data\n    _maybe_extract(target_dir, ARCHIVE_DIR_NAME, archive_path)\n    # Produce CSV files\n    _maybe_convert_sets(target_dir, ARCHIVE_DIR_NAME)\n\n\ndef _maybe_extract(target_dir, extracted_data, archive_path):\n    # If target_dir/extracted_data does not exist, extract archive in target_dir\n    extracted_path = os.path.join(target_dir, extracted_data)\n    if not os.path.exists(extracted_path):\n        print(\'No directory ""%s"" - extracting archive...\' % extracted_path)\n        if not os.path.isdir(extracted_path):\n            os.mkdir(extracted_path)\n        tar = tarfile.open(archive_path)\n        tar.extractall(extracted_path)\n        tar.close()\n    else:\n        print(\'Found directory ""%s"" - not extracting it from archive.\' % archive_path)\n\n\ndef one_sample(sample):\n    """""" Take a audio file, and optionally convert it to 16kHz WAV """"""\n    wav_filename = sample[0]\n    file_size = -1\n    frames = 0\n    if os.path.exists(wav_filename):\n        tmp_filename = os.path.splitext(wav_filename)[0]+\'.tmp.wav\'\n        subprocess.check_call(\n            [\'sox\', wav_filename, \'-r\', str(SAMPLE_RATE), \'-c\', \'1\', \'-b\', \'16\', tmp_filename], stderr=subprocess.STDOUT\n        )\n        os.rename(tmp_filename, wav_filename)\n        file_size = os.path.getsize(wav_filename)\n        frames = int(\n            subprocess.check_output(\n                [""soxi"", ""-s"", wav_filename], stderr=subprocess.STDOUT\n            )\n        )\n    label = label_filter(sample[1])\n    counter = get_counter()\n    rows = []\n\n    if file_size == -1:\n        # Excluding samples that failed upon conversion\n        print(""conversion failure"", wav_filename)\n        counter[""failed""] += 1\n    elif label is None:\n        # Excluding samples that failed on label validation\n        counter[""invalid_label""] += 1\n    elif int(frames / SAMPLE_RATE * 1000 / 15 / 2) < len(str(label)):\n        # Excluding samples that are too short to fit the transcript\n        counter[""too_short""] += 1\n    elif frames / SAMPLE_RATE > MAX_SECS:\n        # Excluding very long samples to keep a reasonable batch-size\n        counter[""too_long""] += 1\n    else:\n        # This one is good - keep it for the target CSV\n        rows.append((wav_filename, file_size, label))\n        counter[""imported_time""] += frames\n    counter[""all""] += 1\n    counter[""total_time""] += frames\n    return (counter, rows)\n\n\ndef _maybe_convert_sets(target_dir, extracted_data):\n    extracted_dir = os.path.join(target_dir, extracted_data)\n    # override existing CSV with normalized one\n    target_csv_template = os.path.join(\n        target_dir, ARCHIVE_DIR_NAME, ARCHIVE_NAME.replace("".tgz"", ""_{}.csv"")\n    )\n    if os.path.isfile(target_csv_template):\n        return\n\n    wav_root_dir = os.path.join(extracted_dir)\n\n    # Get audiofile path and transcript for each sentence in tsv\n    samples = []\n    glob_dir = os.path.join(wav_root_dir, ""**/metadata.csv"")\n    for record in glob(glob_dir, recursive=True):\n        if any(\n            map(lambda sk: sk in record, SKIP_LIST)\n        ):  # pylint: disable=cell-var-from-loop\n            continue\n        with open(record, ""r"") as rec:\n            for re in rec.readlines():\n                re = re.strip().split(""|"")\n                audio = os.path.join(os.path.dirname(record), ""wavs"", re[0] + "".wav"")\n                transcript = re[2]\n                samples.append((audio, transcript))\n\n    counter = get_counter()\n    num_samples = len(samples)\n    rows = []\n\n    print(""Importing WAV files..."")\n    pool = Pool()\n    bar = progressbar.ProgressBar(max_value=num_samples, widgets=SIMPLE_BAR)\n    for i, processed in enumerate(pool.imap_unordered(one_sample, samples), start=1):\n        counter += processed[0]\n        rows += processed[1]\n        bar.update(i)\n    bar.update(num_samples)\n    pool.close()\n    pool.join()\n\n    with open(target_csv_template.format(""train""), ""w"", encoding=""utf-8"", newline="""") as train_csv_file:  # 80%\n        with open(target_csv_template.format(""dev""), ""w"", encoding=""utf-8"", newline="""") as dev_csv_file:  # 10%\n            with open(target_csv_template.format(""test""), ""w"", encoding=""utf-8"", newline="""") as test_csv_file:  # 10%\n                train_writer = csv.DictWriter(train_csv_file, fieldnames=FIELDNAMES)\n                train_writer.writeheader()\n                dev_writer = csv.DictWriter(dev_csv_file, fieldnames=FIELDNAMES)\n                dev_writer.writeheader()\n                test_writer = csv.DictWriter(test_csv_file, fieldnames=FIELDNAMES)\n                test_writer.writeheader()\n\n                for i, item in enumerate(rows):\n                    transcript = validate_label(item[2])\n                    if not transcript:\n                        continue\n                    wav_filename = item[0]\n                    i_mod = i % 10\n                    if i_mod == 0:\n                        writer = test_writer\n                    elif i_mod == 1:\n                        writer = dev_writer\n                    else:\n                        writer = train_writer\n                    writer.writerow(\n                        dict(\n                            wav_filename=os.path.relpath(wav_filename, extracted_dir),\n                            wav_filesize=os.path.getsize(wav_filename),\n                            transcript=transcript,\n                        )\n                    )\n\n    imported_samples = get_imported_samples(counter)\n    assert counter[""all""] == num_samples\n    assert len(rows) == imported_samples\n\n    print_import_report(counter, SAMPLE_RATE, MAX_SECS)\n\n\ndef handle_args():\n    parser = get_importers_parser(\n        description=""Importer for M-AILABS dataset. https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/.""\n    )\n    parser.add_argument(dest=""target_dir"")\n    parser.add_argument(\n        ""--filter_alphabet"",\n        help=""Exclude samples with characters not in provided alphabet"",\n    )\n    parser.add_argument(\n        ""--normalize"",\n        action=""store_true"",\n        help=""Converts diacritic characters to their base ones"",\n    )\n    parser.add_argument(\n        ""--skiplist"",\n        type=str,\n        default="""",\n        help=""Directories / books to skip, comma separated"",\n    )\n    parser.add_argument(\n        ""--language"", required=True, type=str, help=""Dataset language to use""\n    )\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    CLI_ARGS = handle_args()\n    ALPHABET = Alphabet(CLI_ARGS.filter_alphabet) if CLI_ARGS.filter_alphabet else None\n    SKIP_LIST = filter(None, CLI_ARGS.skiplist.split("",""))\n    validate_label = get_validate_label(CLI_ARGS)\n\n    def label_filter(label):\n        if CLI_ARGS.normalize:\n            label = (\n                unicodedata.normalize(""NFKD"", label.strip())\n                .encode(""ascii"", ""ignore"")\n                .decode(""ascii"", ""ignore"")\n            )\n        label = validate_label(label)\n        if ALPHABET and label:\n            try:\n                ALPHABET.encode(label)\n            except KeyError:\n                label = None\n        return label\n\n    ARCHIVE_DIR_NAME = ARCHIVE_DIR_NAME.format(language=CLI_ARGS.language)\n    ARCHIVE_NAME = ARCHIVE_NAME.format(language=CLI_ARGS.language)\n    ARCHIVE_URL = ARCHIVE_URL.format(language=CLI_ARGS.language)\n\n    _download_and_preprocess_data(target_dir=CLI_ARGS.target_dir)\n'"
bin/import_magicdata.py,0,"b'#!/usr/bin/env python\nimport glob\nimport os\nimport tarfile\nimport wave\n\nimport pandas\n\nfrom deepspeech_training.util.importers import get_importers_parser\n\nCOLUMN_NAMES = [""wav_filename"", ""wav_filesize"", ""transcript""]\n\n\ndef extract(archive_path, target_dir):\n    print(""Extracting {} into {}..."".format(archive_path, target_dir))\n    with tarfile.open(archive_path) as tar:\n        tar.extractall(target_dir)\n\n\ndef is_file_truncated(wav_filename, wav_filesize):\n    with wave.open(wav_filename, mode=""rb"") as fin:\n        assert fin.getframerate() == 16000\n        assert fin.getsampwidth() == 2\n        assert fin.getnchannels() == 1\n\n        header_duration = fin.getnframes() / fin.getframerate()\n        filesize_duration = (wav_filesize - 44) / 16000 / 2\n\n    return header_duration != filesize_duration\n\n\ndef preprocess_data(folder_with_archives, target_dir):\n    # First extract subset archives\n    for subset in (""train"", ""dev"", ""test""):\n        extract(\n            os.path.join(\n                folder_with_archives, ""magicdata_{}_set.tar.gz"".format(subset)\n            ),\n            target_dir,\n        )\n\n    # Folder structure is now:\n    # - magicdata_{train,dev,test}.tar.gz\n    # - magicdata/\n    #   - train/*.wav\n    #   - train/TRANS.txt\n    #   - dev/*.wav\n    #   - dev/TRANS.txt\n    #   - test/*.wav\n    #   - test/TRANS.txt\n\n    # The TRANS files are CSVs with three columns, one containing the WAV file\n    # name, one containing the speaker ID, and one containing the transcription\n\n    def load_set(set_path):\n        transcripts = pandas.read_csv(\n            os.path.join(set_path, ""TRANS.txt""), sep=""\\t"", index_col=0\n        )\n        glob_path = os.path.join(set_path, ""*"", ""*.wav"")\n        set_files = []\n        for wav in glob.glob(glob_path):\n            try:\n                wav_filename = wav\n                wav_filesize = os.path.getsize(wav)\n                transcript_key = os.path.basename(wav)\n                transcript = transcripts.loc[transcript_key, ""Transcription""]\n\n                # Some files in this dataset are truncated, the header duration\n                # doesn\'t match the file size. This causes errors at training\n                # time, so check here if things are fine before including a file\n                if is_file_truncated(wav_filename, wav_filesize):\n                    print(\n                        ""Warning: File {} is corrupted, header duration does ""\n                        ""not match file size. Ignoring."".format(wav_filename)\n                    )\n                    continue\n\n                set_files.append((wav_filename, wav_filesize, transcript))\n            except KeyError:\n                print(""Warning: Missing transcript for WAV file {}."".format(wav))\n        return set_files\n\n    for subset in (""train"", ""dev"", ""test""):\n        print(""Loading {} set samples..."".format(subset))\n        subset_files = load_set(os.path.join(target_dir, subset))\n        df = pandas.DataFrame(data=subset_files, columns=COLUMN_NAMES)\n\n        # Trim train set to under 10s\n        if subset == ""train"":\n            durations = (df[""wav_filesize""] - 44) / 16000 / 2\n            df = df[durations <= 10.0]\n            print(""Trimming {} samples > 10 seconds"".format((durations > 10.0).sum()))\n\n            with_noise = df[""transcript""].str.contains(r""\\[(FIL|SPK)\\]"")\n            df = df[~with_noise]\n            print(\n                ""Trimming {} samples with noise ([FIL] or [SPK])"".format(\n                    sum(with_noise)\n                )\n            )\n\n        dest_csv = os.path.join(target_dir, ""magicdata_{}.csv"".format(subset))\n        print(""Saving {} set into {}..."".format(subset, dest_csv))\n        df.to_csv(dest_csv, index=False)\n\n\ndef main():\n    # https://openslr.org/68/\n    parser = get_importers_parser(description=""Import MAGICDATA corpus"")\n    parser.add_argument(\n        ""folder_with_archives"",\n        help=""Path to folder containing magicdata_{train,dev,test}.tar.gz"",\n    )\n    parser.add_argument(\n        ""--target_dir"",\n        default="""",\n        help=""Target folder to extract files into and put the resulting CSVs. Defaults to a folder called magicdata next to the archives"",\n    )\n    params = parser.parse_args()\n\n    if not params.target_dir:\n        params.target_dir = os.path.join(params.folder_with_archives, ""magicdata"")\n\n    preprocess_data(params.folder_with_archives, params.target_dir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
bin/import_primewords.py,0,"b'#!/usr/bin/env python\nimport glob\nimport json\nimport os\nimport tarfile\n\nimport numpy as np\nimport pandas\n\nfrom deepspeech_training.util.importers import get_importers_parser\n\nCOLUMN_NAMES = [""wav_filename"", ""wav_filesize"", ""transcript""]\n\n\ndef extract(archive_path, target_dir):\n    print(""Extracting {} into {}..."".format(archive_path, target_dir))\n    with tarfile.open(archive_path) as tar:\n        tar.extractall(target_dir)\n\n\ndef preprocess_data(tgz_file, target_dir):\n    # First extract main archive and sub-archives\n    extract(tgz_file, target_dir)\n    main_folder = os.path.join(target_dir, ""primewords_md_2018_set1"")\n\n    # Folder structure is now:\n    # - primewords_md_2018_set1/\n    #   - audio_files/\n    #     - [0-f]/[00-0f]/*.wav\n    #   - set1_transcript.json\n\n    transcripts_path = os.path.join(main_folder, ""set1_transcript.json"")\n    with open(transcripts_path) as fin:\n        transcripts = json.load(fin)\n\n    transcripts = {entry[""file""]: entry[""text""] for entry in transcripts}\n\n    def load_set(glob_path):\n        set_files = []\n        for wav in glob.glob(glob_path):\n            try:\n                wav_filename = wav\n                wav_filesize = os.path.getsize(wav)\n                transcript_key = os.path.basename(wav)\n                transcript = transcripts[transcript_key]\n                set_files.append((wav_filename, wav_filesize, transcript))\n            except KeyError:\n                print(""Warning: Missing transcript for WAV file {}."".format(wav))\n        return set_files\n\n    # Load all files, then deterministically split into train/dev/test sets\n    all_files = load_set(os.path.join(main_folder, ""audio_files"", ""*"", ""*"", ""*.wav""))\n    df = pandas.DataFrame(data=all_files, columns=COLUMN_NAMES)\n    df.sort_values(by=""wav_filename"", inplace=True)\n\n    indices = np.arange(0, len(df))\n    np.random.seed(12345)\n    np.random.shuffle(indices)\n\n    # Total corpus size: 50287 samples. 5000 samples gives us 99% confidence\n    # level with a margin of error of under 2%.\n    test_indices = indices[-5000:]\n    dev_indices = indices[-10000:-5000]\n    train_indices = indices[:-10000]\n\n    train_files = df.iloc[train_indices]\n    durations = (train_files[""wav_filesize""] - 44) / 16000 / 2\n    train_files = train_files[durations <= 15.0]\n    print(""Trimming {} samples > 15 seconds"".format((durations > 15.0).sum()))\n    dest_csv = os.path.join(target_dir, ""primewords_train.csv"")\n    print(""Saving train set into {}..."".format(dest_csv))\n    train_files.to_csv(dest_csv, index=False)\n\n    dev_files = df.iloc[dev_indices]\n    dest_csv = os.path.join(target_dir, ""primewords_dev.csv"")\n    print(""Saving dev set into {}..."".format(dest_csv))\n    dev_files.to_csv(dest_csv, index=False)\n\n    test_files = df.iloc[test_indices]\n    dest_csv = os.path.join(target_dir, ""primewords_test.csv"")\n    print(""Saving test set into {}..."".format(dest_csv))\n    test_files.to_csv(dest_csv, index=False)\n\n\ndef main():\n    # https://www.openslr.org/47/\n    parser = get_importers_parser(description=""Import Primewords Chinese corpus set 1"")\n    parser.add_argument(""tgz_file"", help=""Path to primewords_md_2018_set1.tar.gz"")\n    parser.add_argument(\n        ""--target_dir"",\n        default="""",\n        help=""Target folder to extract files into and put the resulting CSVs. Defaults to same folder as the main archive."",\n    )\n    params = parser.parse_args()\n\n    if not params.target_dir:\n        params.target_dir = os.path.dirname(params.tgz_file)\n\n    preprocess_data(params.tgz_file, params.target_dir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
bin/import_slr57.py,0,"b'#!/usr/bin/env python3\nimport csv\nimport os\nimport re\nimport subprocess\nimport tarfile\nimport unicodedata\nimport zipfile\nfrom glob import glob\nfrom multiprocessing import Pool\n\nimport progressbar\nimport sox\n\nfrom deepspeech_training.util.downloader import SIMPLE_BAR, maybe_download\nfrom deepspeech_training.util.importers import (\n    get_counter,\n    get_imported_samples,\n    get_importers_parser,\n    get_validate_label,\n    print_import_report,\n)\nfrom deepspeech_training.util.text import Alphabet\n\nFIELDNAMES = [""wav_filename"", ""wav_filesize"", ""transcript""]\nSAMPLE_RATE = 16000\nMAX_SECS = 15\n\nARCHIVE_DIR_NAME = ""African_Accented_French""\nARCHIVE_NAME = ""African_Accented_French.tar.gz""\nARCHIVE_URL = ""http://www.openslr.org/resources/57/"" + ARCHIVE_NAME\n\n\ndef _download_and_preprocess_data(target_dir):\n    # Making path absolute\n    target_dir = os.path.abspath(target_dir)\n    # Conditionally download data\n    archive_path = maybe_download(ARCHIVE_NAME, target_dir, ARCHIVE_URL)\n    # Conditionally extract data\n    _maybe_extract(target_dir, ARCHIVE_DIR_NAME, archive_path)\n    # Produce CSV files\n    _maybe_convert_sets(target_dir, ARCHIVE_DIR_NAME)\n\n\ndef _maybe_extract(target_dir, extracted_data, archive_path):\n    # If target_dir/extracted_data does not exist, extract archive in target_dir\n    extracted_path = os.path.join(target_dir, extracted_data)\n    if not os.path.exists(extracted_path):\n        print(\'No directory ""%s"" - extracting archive...\' % extracted_path)\n        if not os.path.isdir(extracted_path):\n            os.mkdir(extracted_path)\n        tar = tarfile.open(archive_path)\n        tar.extractall(target_dir)\n        tar.close()\n    else:\n        print(\'Found directory ""%s"" - not extracting it from archive.\' % archive_path)\n\n\ndef one_sample(sample):\n    """""" Take a audio file, and optionally convert it to 16kHz WAV """"""\n    wav_filename = sample[0]\n    file_size = -1\n    frames = 0\n    if os.path.exists(wav_filename):\n        file_size = os.path.getsize(wav_filename)\n        frames = int(\n            subprocess.check_output(\n                [""soxi"", ""-s"", wav_filename], stderr=subprocess.STDOUT\n            )\n        )\n    label = label_filter(sample[1])\n    counter = get_counter()\n    rows = []\n    if file_size == -1:\n        # Excluding samples that failed upon conversion\n        counter[""failed""] += 1\n    elif label is None:\n        # Excluding samples that failed on label validation\n        counter[""invalid_label""] += 1\n    elif int(frames / SAMPLE_RATE * 1000 / 15 / 2) < len(str(label)):\n        # Excluding samples that are too short to fit the transcript\n        counter[""too_short""] += 1\n    elif frames / SAMPLE_RATE > MAX_SECS:\n        # Excluding very long samples to keep a reasonable batch-size\n        counter[""too_long""] += 1\n    else:\n        # This one is good - keep it for the target CSV\n        rows.append((wav_filename, file_size, label))\n        counter[""imported_time""] += frames\n    counter[""all""] += 1\n    counter[""total_time""] += frames\n\n    return (counter, rows)\n\n\ndef _maybe_convert_sets(target_dir, extracted_data):\n    extracted_dir = os.path.join(target_dir, extracted_data)\n    # override existing CSV with normalized one\n    target_csv_template = os.path.join(\n        target_dir, ARCHIVE_DIR_NAME, ARCHIVE_NAME.replace("".tar.gz"", ""_{}.csv"")\n    )\n    if os.path.isfile(target_csv_template):\n        return\n\n    wav_root_dir = os.path.join(extracted_dir)\n\n    all_files = [\n        ""transcripts/train/yaounde/fn_text.txt"",\n        ""transcripts/train/ca16_conv/transcripts.txt"",\n        ""transcripts/train/ca16_read/conditioned.txt"",\n        ""transcripts/dev/niger_west_african_fr/transcripts.txt"",\n        ""speech/dev/niger_west_african_fr/niger_wav_file_name_transcript.tsv"",\n        ""transcripts/devtest/ca16_read/conditioned.txt"",\n        ""transcripts/test/ca16/prompts.txt"",\n    ]\n\n    transcripts = {}\n    for tr in all_files:\n        with open(os.path.join(target_dir, ARCHIVE_DIR_NAME, tr), ""r"") as tr_source:\n            for line in tr_source.readlines():\n                line = line.strip()\n\n                if "".tsv"" in tr:\n                    sep = ""\t""\n                else:\n                    sep = "" ""\n\n                audio = os.path.basename(line.split(sep)[0])\n\n                if not ("".wav"" in audio):\n                    if "".tdf"" in audio:\n                        audio = audio.replace("".tdf"", "".wav"")\n                    else:\n                        audio += "".wav""\n\n                transcript = "" "".join(line.split(sep)[1:])\n                transcripts[audio] = transcript\n\n    # Get audiofile path and transcript for each sentence in tsv\n    samples = []\n    glob_dir = os.path.join(wav_root_dir, ""**/*.wav"")\n    for record in glob(glob_dir, recursive=True):\n        record_file = os.path.basename(record)\n        if record_file in transcripts:\n            samples.append((record, transcripts[record_file]))\n\n    # Keep track of how many samples are good vs. problematic\n    counter = get_counter()\n    num_samples = len(samples)\n    rows = []\n\n    print(""Importing WAV files..."")\n    pool = Pool()\n    bar = progressbar.ProgressBar(max_value=num_samples, widgets=SIMPLE_BAR)\n    for i, processed in enumerate(pool.imap_unordered(one_sample, samples), start=1):\n        counter += processed[0]\n        rows += processed[1]\n        bar.update(i)\n    bar.update(num_samples)\n    pool.close()\n    pool.join()\n\n    with open(target_csv_template.format(""train""), ""w"", encoding=""utf-8"", newline="""") as train_csv_file:  # 80%\n        with open(target_csv_template.format(""dev""), ""w"", encoding=""utf-8"", newline="""") as dev_csv_file:  # 10%\n            with open(target_csv_template.format(""test""), ""w"", encoding=""utf-8"", newline="""") as test_csv_file:  # 10%\n                train_writer = csv.DictWriter(train_csv_file, fieldnames=FIELDNAMES)\n                train_writer.writeheader()\n                dev_writer = csv.DictWriter(dev_csv_file, fieldnames=FIELDNAMES)\n                dev_writer.writeheader()\n                test_writer = csv.DictWriter(test_csv_file, fieldnames=FIELDNAMES)\n                test_writer.writeheader()\n\n                for i, item in enumerate(rows):\n                    transcript = validate_label(item[2])\n                    if not transcript:\n                        continue\n                    wav_filename = item[0]\n                    i_mod = i % 10\n                    if i_mod == 0:\n                        writer = test_writer\n                    elif i_mod == 1:\n                        writer = dev_writer\n                    else:\n                        writer = train_writer\n                    writer.writerow(\n                        dict(\n                            wav_filename=wav_filename,\n                            wav_filesize=os.path.getsize(wav_filename),\n                            transcript=transcript,\n                        )\n                    )\n\n    imported_samples = get_imported_samples(counter)\n    assert counter[""all""] == num_samples\n    assert len(rows) == imported_samples\n\n    print_import_report(counter, SAMPLE_RATE, MAX_SECS)\n\n\ndef handle_args():\n    parser = get_importers_parser(\n        description=""Importer for African Accented French dataset. More information on http://www.openslr.org/57/.""\n    )\n    parser.add_argument(dest=""target_dir"")\n    parser.add_argument(\n        ""--filter_alphabet"",\n        help=""Exclude samples with characters not in provided alphabet"",\n    )\n    parser.add_argument(\n        ""--normalize"",\n        action=""store_true"",\n        help=""Converts diacritic characters to their base ones"",\n    )\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    CLI_ARGS = handle_args()\n    ALPHABET = Alphabet(CLI_ARGS.filter_alphabet) if CLI_ARGS.filter_alphabet else None\n    validate_label = get_validate_label(CLI_ARGS)\n\n    def label_filter(label):\n        if CLI_ARGS.normalize:\n            label = (\n                unicodedata.normalize(""NFKD"", label.strip())\n                .encode(""ascii"", ""ignore"")\n                .decode(""ascii"", ""ignore"")\n            )\n        label = validate_label(label)\n        if ALPHABET and label:\n            try:\n                ALPHABET.encode(label)\n            except KeyError:\n                label = None\n        return label\n\n    _download_and_preprocess_data(target_dir=CLI_ARGS.target_dir)\n'"
bin/import_swb.py,0,"b'#!/usr/bin/env python\n# ensure that you have downloaded the LDC dataset LDC97S62 and tar exists in a folder e.g.\n# ./data/swb/swb1_LDC97S62.tgz\n# from the deepspeech directory run with: ./bin/import_swb.py ./data/swb/\nimport codecs\nimport fnmatch\nimport os\nimport subprocess\nimport sys\nimport tarfile\nimport unicodedata\nimport wave\n\nimport librosa\nimport pandas\nimport requests\nimport soundfile  # <= Has an external dependency on libsndfile\n\nfrom deepspeech_training.util.importers import validate_label_eng as validate_label\n\n# ARCHIVE_NAME refers to ISIP alignments from 01/29/03\nARCHIVE_NAME = ""switchboard_word_alignments.tar.gz""\nARCHIVE_URL = ""http://www.openslr.org/resources/5/""\nARCHIVE_DIR_NAME = ""LDC97S62""\nLDC_DATASET = ""swb1_LDC97S62.tgz""\n\n\ndef download_file(folder, url):\n    # https://stackoverflow.com/a/16696317/738515\n    local_filename = url.split(""/"")[-1]\n    full_filename = os.path.join(folder, local_filename)\n    r = requests.get(url, stream=True)\n    with open(full_filename, ""wb"") as f:\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n    return full_filename\n\n\ndef maybe_download(archive_url, target_dir, ldc_dataset):\n    # If archive file does not exist, download it...\n    archive_path = os.path.join(target_dir, ldc_dataset)\n    ldc_path = archive_url + ldc_dataset\n    if not os.path.exists(target_dir):\n        print(\'No path ""%s"" - creating ...\' % target_dir)\n        makedirs(target_dir)\n\n    if not os.path.exists(archive_path):\n        print(\'No archive ""%s"" - downloading...\' % archive_path)\n        download_file(target_dir, ldc_path)\n    else:\n        print(\'Found archive ""%s"" - not downloading.\' % archive_path)\n    return archive_path\n\n\ndef _download_and_preprocess_data(data_dir):\n    new_data_dir = os.path.join(data_dir, ARCHIVE_DIR_NAME)\n    target_dir = os.path.abspath(new_data_dir)\n    archive_path = os.path.abspath(os.path.join(data_dir, LDC_DATASET))\n\n    # Check swb1_LDC97S62.tgz then extract\n    assert os.path.isfile(archive_path)\n    _extract(target_dir, archive_path)\n\n    # Transcripts\n    transcripts_path = maybe_download(ARCHIVE_URL, target_dir, ARCHIVE_NAME)\n    _extract(target_dir, transcripts_path)\n\n    # Check swb1_d1/2/3/4/swb_ms98_transcriptions\n    expected_folders = [\n        ""swb1_d1"",\n        ""swb1_d2"",\n        ""swb1_d3"",\n        ""swb1_d4"",\n        ""swb_ms98_transcriptions"",\n    ]\n    assert all([os.path.isdir(os.path.join(target_dir, e)) for e in expected_folders])\n\n    # Conditionally convert swb sph data to wav\n    _maybe_convert_wav(target_dir, ""swb1_d1"", ""swb1_d1-wav"")\n    _maybe_convert_wav(target_dir, ""swb1_d2"", ""swb1_d2-wav"")\n    _maybe_convert_wav(target_dir, ""swb1_d3"", ""swb1_d3-wav"")\n    _maybe_convert_wav(target_dir, ""swb1_d4"", ""swb1_d4-wav"")\n\n    # Conditionally split wav data\n    d1 = _maybe_split_wav_and_sentences(\n        target_dir, ""swb_ms98_transcriptions"", ""swb1_d1-wav"", ""swb1_d1-split-wav""\n    )\n    d2 = _maybe_split_wav_and_sentences(\n        target_dir, ""swb_ms98_transcriptions"", ""swb1_d2-wav"", ""swb1_d2-split-wav""\n    )\n    d3 = _maybe_split_wav_and_sentences(\n        target_dir, ""swb_ms98_transcriptions"", ""swb1_d3-wav"", ""swb1_d3-split-wav""\n    )\n    d4 = _maybe_split_wav_and_sentences(\n        target_dir, ""swb_ms98_transcriptions"", ""swb1_d4-wav"", ""swb1_d4-split-wav""\n    )\n\n    swb_files = d1.append(d2).append(d3).append(d4)\n\n    train_files, dev_files, test_files = _split_sets(swb_files)\n\n    # Write sets to disk as CSV files\n    train_files.to_csv(os.path.join(target_dir, ""swb-train.csv""), index=False)\n    dev_files.to_csv(os.path.join(target_dir, ""swb-dev.csv""), index=False)\n    test_files.to_csv(os.path.join(target_dir, ""swb-test.csv""), index=False)\n\n\ndef _extract(target_dir, archive_path):\n    with tarfile.open(archive_path) as tar:\n        tar.extractall(target_dir)\n\n\ndef _maybe_convert_wav(data_dir, original_data, converted_data):\n    source_dir = os.path.join(data_dir, original_data)\n    target_dir = os.path.join(data_dir, converted_data)\n\n    # Conditionally convert sph files to wav files\n    if os.path.exists(target_dir):\n        print(""skipping maybe_convert_wav"")\n        return\n\n    # Create target_dir\n    os.makedirs(target_dir)\n\n    # Loop over sph files in source_dir and convert each to 16-bit PCM wav\n    for root, dirnames, filenames in os.walk(source_dir):\n        for filename in fnmatch.filter(filenames, ""*.sph""):\n            for channel in [""1"", ""2""]:\n                sph_file = os.path.join(root, filename)\n                wav_filename = (\n                    os.path.splitext(os.path.basename(sph_file))[0]\n                    + ""-""\n                    + channel\n                    + "".wav""\n                )\n                wav_file = os.path.join(target_dir, wav_filename)\n                temp_wav_filename = (\n                    os.path.splitext(os.path.basename(sph_file))[0]\n                    + ""-""\n                    + channel\n                    + ""-temp.wav""\n                )\n                temp_wav_file = os.path.join(target_dir, temp_wav_filename)\n                print(""converting {} to {}"".format(sph_file, temp_wav_file))\n                subprocess.check_call(\n                    [\n                        ""sph2pipe"",\n                        ""-c"",\n                        channel,\n                        ""-p"",\n                        ""-f"",\n                        ""rif"",\n                        sph_file,\n                        temp_wav_file,\n                    ]\n                )\n                print(""upsampling {} to {}"".format(temp_wav_file, wav_file))\n                audioData, frameRate = librosa.load(temp_wav_file, sr=16000, mono=True)\n                soundfile.write(wav_file, audioData, frameRate, ""PCM_16"")\n                os.remove(temp_wav_file)\n\n\ndef _parse_transcriptions(trans_file):\n    segments = []\n    with codecs.open(trans_file, ""r"", ""utf-8"") as fin:\n        for line in fin:\n            if line.startswith(""#"") or len(line) <= 1:\n                continue\n\n            tokens = line.split()\n            start_time = float(tokens[1])\n            stop_time = float(tokens[2])\n            transcript = validate_label("" "".join(tokens[3:]))\n\n            if transcript == None:\n                continue\n\n            # We need to do the encode-decode dance here because encode\n            # returns a bytes() object on Python 3, and text_to_char_array\n            # expects a string.\n            transcript = (\n                unicodedata.normalize(""NFKD"", transcript)\n                .encode(""ascii"", ""ignore"")\n                .decode(""ascii"", ""ignore"")\n            )\n\n            segments.append(\n                {\n                    ""start_time"": start_time,\n                    ""stop_time"": stop_time,\n                    ""transcript"": transcript,\n                }\n            )\n    return segments\n\n\ndef _maybe_split_wav_and_sentences(data_dir, trans_data, original_data, converted_data):\n    trans_dir = os.path.join(data_dir, trans_data)\n    source_dir = os.path.join(data_dir, original_data)\n    target_dir = os.path.join(data_dir, converted_data)\n    if os.path.exists(target_dir):\n        print(""skipping maybe_split_wav"")\n        return\n\n    os.makedirs(target_dir)\n\n    files = []\n\n    # Loop over transcription files and split corresponding wav\n    for root, dirnames, filenames in os.walk(trans_dir):\n        for filename in fnmatch.filter(filenames, ""*.text""):\n            if ""trans"" not in filename:\n                continue\n            trans_file = os.path.join(root, filename)\n            segments = _parse_transcriptions(trans_file)\n\n            # Open wav corresponding to transcription file\n            channel = (""2"", ""1"")[\n                (os.path.splitext(os.path.basename(trans_file))[0])[6] == ""A""\n            ]\n            wav_filename = (\n                ""sw0""\n                + (os.path.splitext(os.path.basename(trans_file))[0])[2:6]\n                + ""-""\n                + channel\n                + "".wav""\n            )\n            wav_file = os.path.join(source_dir, wav_filename)\n\n            print(""splitting {} according to {}"".format(wav_file, trans_file))\n\n            if not os.path.exists(wav_file):\n                print(""skipping. does not exist:"" + wav_file)\n                continue\n\n            origAudio = wave.open(wav_file, ""r"")\n\n            # Loop over segments and split wav_file for each segment\n            for segment in segments:\n                # Create wav segment filename\n                start_time = segment[""start_time""]\n                stop_time = segment[""stop_time""]\n                new_wav_filename = (\n                    os.path.splitext(os.path.basename(trans_file))[0]\n                    + ""-""\n                    + str(start_time)\n                    + ""-""\n                    + str(stop_time)\n                    + "".wav""\n                )\n                if _is_wav_too_short(new_wav_filename):\n                    continue\n                new_wav_file = os.path.join(target_dir, new_wav_filename)\n\n                _split_wav(origAudio, start_time, stop_time, new_wav_file)\n\n                new_wav_filesize = os.path.getsize(new_wav_file)\n                transcript = segment[""transcript""]\n                files.append(\n                    (os.path.abspath(new_wav_file), new_wav_filesize, transcript)\n                )\n\n            # Close origAudio\n            origAudio.close()\n\n    return pandas.DataFrame(\n        data=files, columns=[""wav_filename"", ""wav_filesize"", ""transcript""]\n    )\n\n\ndef _is_wav_too_short(wav_filename):\n    short_wav_filenames = [\n        ""sw2986A-ms98-a-trans-80.6385-83.358875.wav"",\n        ""sw2663A-ms98-a-trans-161.12025-164.213375.wav"",\n    ]\n    return wav_filename in short_wav_filenames\n\n\ndef _split_wav(origAudio, start_time, stop_time, new_wav_file):\n    frameRate = origAudio.getframerate()\n    origAudio.setpos(int(start_time * frameRate))\n    chunkData = origAudio.readframes(int((stop_time - start_time) * frameRate))\n    chunkAudio = wave.open(new_wav_file, ""w"")\n    chunkAudio.setnchannels(origAudio.getnchannels())\n    chunkAudio.setsampwidth(origAudio.getsampwidth())\n    chunkAudio.setframerate(frameRate)\n    chunkAudio.writeframes(chunkData)\n    chunkAudio.close()\n\n\ndef _split_sets(filelist):\n    # We initially split the entire set into 80% train and 20% test, then\n    # split the train set into 80% train and 20% validation.\n    train_beg = 0\n    train_end = int(0.8 * len(filelist))\n\n    dev_beg = int(0.8 * train_end)\n    dev_end = train_end\n    train_end = dev_beg\n\n    test_beg = dev_end\n    test_end = len(filelist)\n\n    return (\n        filelist[train_beg:train_end],\n        filelist[dev_beg:dev_end],\n        filelist[test_beg:test_end],\n    )\n\n\ndef _read_data_set(\n    filelist,\n    thread_count,\n    batch_size,\n    numcep,\n    numcontext,\n    stride=1,\n    offset=0,\n    next_index=lambda i: i + 1,\n    limit=0,\n):\n    # Optionally apply dataset size limit\n    if limit > 0:\n        filelist = filelist.iloc[:limit]\n\n    filelist = filelist[offset::stride]\n\n    # Return DataSet\n    return DataSet(\n        txt_files, thread_count, batch_size, numcep, numcontext, next_index=next_index\n    )\n\n\nif __name__ == ""__main__"":\n    _download_and_preprocess_data(sys.argv[1])\n'"
bin/import_swc.py,0,"b'#!/usr/bin/env python\n""""""\nDownloads and prepares (parts of) the ""Spoken Wikipedia Corpora"" for DeepSpeech.py\nUse ""python3 import_swc.py -h"" for help\n""""""\n\nimport argparse\nimport csv\nimport os\nimport random\nimport re\nimport shutil\nimport sys\nimport tarfile\nimport unicodedata\nimport wave\nimport xml.etree.cElementTree as ET\nfrom collections import Counter\nfrom glob import glob\nfrom multiprocessing.pool import ThreadPool\n\nimport progressbar\nimport sox\n\nfrom deepspeech_training.util.downloader import SIMPLE_BAR, maybe_download\nfrom deepspeech_training.util.importers import validate_label_eng as validate_label\nfrom deepspeech_training.util.text import Alphabet\n\nSWC_URL = ""https://www2.informatik.uni-hamburg.de/nats/pub/SWC/SWC_{language}.tar""\nSWC_ARCHIVE = ""SWC_{language}.tar""\nLANGUAGES = [""dutch"", ""english"", ""german""]\nFIELDNAMES = [""wav_filename"", ""wav_filesize"", ""transcript""]\nFIELDNAMES_EXT = FIELDNAMES + [""article"", ""speaker""]\nCHANNELS = 1\nSAMPLE_RATE = 16000\nUNKNOWN = ""<unknown>""\nAUDIO_PATTERN = ""audio*.ogg""\nWAV_NAME = ""audio.wav""\nALIGNED_NAME = ""aligned.swc""\n\nSUBSTITUTIONS = {\n    ""german"": [\n        (re.compile(r""\\$""), ""dollar""),\n        (re.compile(r""\xe2\x82\xac""), ""euro""),\n        (re.compile(r""\xc2\xa3""), ""pfund""),\n        (\n            re.compile(r""ein tausend ([^\\s]+) hundert ([^\\s]+) er( |$)""),\n            r""\\1zehnhundert \\2er "",\n        ),\n        (re.compile(r""ein tausend (acht|neun) hundert""), r""\\1zehnhundert""),\n        (\n            re.compile(\n                r""eins punkt null null null punkt null null null punkt null null null""\n            ),\n            ""eine milliarde"",\n        ),\n        (\n            re.compile(\n                r""punkt null null null punkt null null null punkt null null null""\n            ),\n            ""milliarden"",\n        ),\n        (re.compile(r""eins punkt null null null punkt null null null""), ""eine million""),\n        (re.compile(r""punkt null null null punkt null null null""), ""millionen""),\n        (re.compile(r""eins punkt null null null""), ""ein tausend""),\n        (re.compile(r""punkt null null null""), ""tausend""),\n        (re.compile(r""punkt null""), None),\n    ]\n}\n\nDONT_NORMALIZE = {""german"": ""\xc3\x84\xc3\x96\xc3\x9c\xc3\xa4\xc3\xb6\xc3\xbc\xc3\x9f""}\n\nPRE_FILTER = str.maketrans(dict.fromkeys(""/()[]{}<>:""))\n\n\nclass Sample:\n    def __init__(self, wav_path, start, end, text, article, speaker, sub_set=None):\n        self.wav_path = wav_path\n        self.start = start\n        self.end = end\n        self.text = text\n        self.article = article\n        self.speaker = speaker\n        self.sub_set = sub_set\n\n\ndef fail(message):\n    print(message)\n    sys.exit(1)\n\n\ndef group(lst, get_key):\n    groups = {}\n    for obj in lst:\n        key = get_key(obj)\n        if key in groups:\n            groups[key].append(obj)\n        else:\n            groups[key] = [obj]\n    return groups\n\n\ndef get_sample_size(population_size):\n    margin_of_error = 0.01\n    fraction_picking = 0.50\n    z_score = 2.58  # Corresponds to confidence level 99%\n    numerator = (z_score ** 2 * fraction_picking * (1 - fraction_picking)) / (\n        margin_of_error ** 2\n    )\n    sample_size = 0\n    for train_size in range(population_size, 0, -1):\n        denominator = 1 + (z_score ** 2 * fraction_picking * (1 - fraction_picking)) / (\n            margin_of_error ** 2 * train_size\n        )\n        sample_size = int(numerator / denominator)\n        if 2 * sample_size + train_size <= population_size:\n            break\n    return sample_size\n\n\ndef maybe_download_language(language):\n    lang_upper = language[0].upper() + language[1:]\n    return maybe_download(\n        SWC_ARCHIVE.format(language=lang_upper),\n        CLI_ARGS.base_dir,\n        SWC_URL.format(language=lang_upper),\n    )\n\n\ndef maybe_extract(data_dir, extracted_data, archive):\n    extracted = os.path.join(data_dir, extracted_data)\n    if os.path.isdir(extracted):\n        print(\'Found directory ""{}"" - not extracting.\'.format(extracted))\n    else:\n        print(\'Extracting ""{}""...\'.format(archive))\n        with tarfile.open(archive) as tar:\n            members = tar.getmembers()\n            bar = progressbar.ProgressBar(max_value=len(members), widgets=SIMPLE_BAR)\n            for member in bar(members):\n                tar.extract(member=member, path=extracted)\n    return extracted\n\n\ndef ignored(node):\n    if node is None:\n        return False\n    if node.tag == ""ignored"":\n        return True\n    return ignored(node.find(""..""))\n\n\ndef read_token(token):\n    texts, start, end = [], None, None\n    notes = token.findall(""n"")\n    if len(notes) > 0:\n        for note in notes:\n            attributes = note.attrib\n            if start is None and ""start"" in attributes:\n                start = int(attributes[""start""])\n            if ""end"" in attributes:\n                token_end = int(attributes[""end""])\n                if end is None or token_end > end:\n                    end = token_end\n            if ""pronunciation"" in attributes:\n                t = attributes[""pronunciation""]\n                texts.append(t)\n    elif ""text"" in token.attrib:\n        texts.append(token.attrib[""text""])\n    return start, end, "" "".join(texts)\n\n\ndef in_alphabet(alphabet, c):\n    return True if alphabet is None else alphabet.has_char(c)\n\n\nALPHABETS = {}\n\n\ndef get_alphabet(language):\n    if language in ALPHABETS:\n        return ALPHABETS[language]\n    alphabet_path = getattr(CLI_ARGS, language + ""_alphabet"")\n    alphabet = Alphabet(alphabet_path) if alphabet_path else None\n    ALPHABETS[language] = alphabet\n    return alphabet\n\n\ndef label_filter(label, language):\n    label = label.translate(PRE_FILTER)\n    label = validate_label(label)\n    if label is None:\n        return None, ""validation""\n    substitutions = SUBSTITUTIONS[language] if language in SUBSTITUTIONS else []\n    for pattern, replacement in substitutions:\n        if replacement is None:\n            if pattern.match(label):\n                return None, ""substitution rule""\n        else:\n            label = pattern.sub(replacement, label)\n    chars = []\n    dont_normalize = DONT_NORMALIZE[language] if language in DONT_NORMALIZE else """"\n    alphabet = get_alphabet(language)\n    for c in label:\n        if (\n            CLI_ARGS.normalize\n            and c not in dont_normalize\n            and not in_alphabet(alphabet, c)\n        ):\n            c = (\n                unicodedata.normalize(""NFKD"", c)\n                .encode(""ascii"", ""ignore"")\n                .decode(""ascii"", ""ignore"")\n            )\n        for sc in c:\n            if not in_alphabet(alphabet, sc):\n                return None, ""illegal character""\n            chars.append(sc)\n    label = """".join(chars)\n    label = validate_label(label)\n    return label, ""validation"" if label is None else None\n\n\ndef collect_samples(base_dir, language):\n    roots = []\n    for root, _, files in os.walk(base_dir):\n        if ALIGNED_NAME in files and WAV_NAME in files:\n            roots.append(root)\n    samples = []\n    reasons = Counter()\n\n    def add_sample(\n        p_wav_path, p_article, p_speaker, p_start, p_end, p_text, p_reason=""complete""\n    ):\n        if p_start is not None and p_end is not None and p_text is not None:\n            duration = p_end - p_start\n            text, filter_reason = label_filter(p_text, language)\n            skip = False\n            if filter_reason is not None:\n                skip = True\n                p_reason = filter_reason\n            elif CLI_ARGS.exclude_unknown_speakers and p_speaker == UNKNOWN:\n                skip = True\n                p_reason = ""unknown speaker""\n            elif CLI_ARGS.exclude_unknown_articles and p_article == UNKNOWN:\n                skip = True\n                p_reason = ""unknown article""\n            elif duration > CLI_ARGS.max_duration > 0 and CLI_ARGS.ignore_too_long:\n                skip = True\n                p_reason = ""exceeded duration""\n            elif int(duration / 30) < len(text):\n                skip = True\n                p_reason = ""too short to decode""\n            elif duration / len(text) < 10:\n                skip = True\n                p_reason = ""length duration ratio""\n            if skip:\n                reasons[p_reason] += 1\n            else:\n                samples.append(\n                    Sample(p_wav_path, p_start, p_end, text, p_article, p_speaker)\n                )\n        elif p_start is None or p_end is None:\n            reasons[""missing timestamps""] += 1\n        else:\n            reasons[""missing text""] += 1\n\n    print(""Collecting samples..."")\n    bar = progressbar.ProgressBar(max_value=len(roots), widgets=SIMPLE_BAR)\n    for root in bar(roots):\n        wav_path = os.path.join(root, WAV_NAME)\n        aligned = ET.parse(os.path.join(root, ALIGNED_NAME))\n        article = UNKNOWN\n        speaker = UNKNOWN\n        for prop in aligned.iter(""prop""):\n            attributes = prop.attrib\n            if ""key"" in attributes and ""value"" in attributes:\n                if attributes[""key""] == ""DC.identifier"":\n                    article = attributes[""value""]\n                elif attributes[""key""] == ""reader.name"":\n                    speaker = attributes[""value""]\n        for sentence in aligned.iter(""s""):\n            if ignored(sentence):\n                continue\n            split = False\n            tokens = list(map(read_token, sentence.findall(""t"")))\n            sample_start, sample_end, token_texts, sample_texts = None, None, [], []\n            for token_start, token_end, token_text in tokens:\n                if CLI_ARGS.exclude_numbers and any(c.isdigit() for c in token_text):\n                    add_sample(\n                        wav_path,\n                        article,\n                        speaker,\n                        sample_start,\n                        sample_end,\n                        "" "".join(sample_texts),\n                        p_reason=""has numbers"",\n                    )\n                    sample_start, sample_end, token_texts, sample_texts = (\n                        None,\n                        None,\n                        [],\n                        [],\n                    )\n                    continue\n                if sample_start is None:\n                    sample_start = token_start\n                if sample_start is None:\n                    continue\n                token_texts.append(token_text)\n                if token_end is not None:\n                    if (\n                        token_start != sample_start\n                        and token_end - sample_start > CLI_ARGS.max_duration > 0\n                    ):\n                        add_sample(\n                            wav_path,\n                            article,\n                            speaker,\n                            sample_start,\n                            sample_end,\n                            "" "".join(sample_texts),\n                            p_reason=""split"",\n                        )\n                        sample_start = sample_end\n                        sample_texts = []\n                        split = True\n                    sample_end = token_end\n                    sample_texts.extend(token_texts)\n                    token_texts = []\n            add_sample(\n                wav_path,\n                article,\n                speaker,\n                sample_start,\n                sample_end,\n                "" "".join(sample_texts),\n                p_reason=""split"" if split else ""complete"",\n            )\n    print(""Skipped samples:"")\n    for reason, n in reasons.most_common():\n        print("" - {}: {}"".format(reason, n))\n    return samples\n\n\ndef maybe_convert_one_to_wav(entry):\n    root, _, files = entry\n    transformer = sox.Transformer()\n    transformer.convert(samplerate=SAMPLE_RATE, n_channels=CHANNELS)\n    combiner = sox.Combiner()\n    combiner.convert(samplerate=SAMPLE_RATE, n_channels=CHANNELS)\n    output_wav = os.path.join(root, WAV_NAME)\n    if os.path.isfile(output_wav):\n        return\n    files = sorted(glob(os.path.join(root, AUDIO_PATTERN)))\n    try:\n        if len(files) == 1:\n            transformer.build(files[0], output_wav)\n        elif len(files) > 1:\n            wav_files = []\n            for i, file in enumerate(files):\n                wav_path = os.path.join(root, ""audio{}.wav"".format(i))\n                transformer.build(file, wav_path)\n                wav_files.append(wav_path)\n            combiner.set_input_format(file_type=[""wav""] * len(wav_files))\n            combiner.build(wav_files, output_wav, ""concatenate"")\n    except sox.core.SoxError:\n        return\n\n\ndef maybe_convert_to_wav(base_dir):\n    roots = list(os.walk(base_dir))\n    print(""Converting and joining source audio files..."")\n    bar = progressbar.ProgressBar(max_value=len(roots), widgets=SIMPLE_BAR)\n    tp = ThreadPool()\n    for _ in bar(tp.imap_unordered(maybe_convert_one_to_wav, roots)):\n        pass\n    tp.close()\n    tp.join()\n\n\ndef assign_sub_sets(samples):\n    sample_size = get_sample_size(len(samples))\n    speakers = group(samples, lambda sample: sample.speaker).values()\n    speakers = list(sorted(speakers, key=len))\n    sample_sets = [[], []]\n    while any(map(lambda s: len(s) < sample_size, sample_sets)) and len(speakers) > 0:\n        for sample_set in sample_sets:\n            if len(sample_set) < sample_size and len(speakers) > 0:\n                sample_set.extend(speakers.pop(0))\n    train_set = sum(speakers, [])\n    if len(train_set) == 0:\n        print(\n            ""WARNING: Unable to build dev and test sets without speaker bias as there is no speaker meta data""\n        )\n        random.seed(42)  # same source data == same output\n        random.shuffle(samples)\n        for index, sample in enumerate(samples):\n            if index < sample_size:\n                sample.sub_set = ""dev""\n            elif index < 2 * sample_size:\n                sample.sub_set = ""test""\n            else:\n                sample.sub_set = ""train""\n    else:\n        for sub_set, sub_set_samples in [\n            (""train"", train_set),\n            (""dev"", sample_sets[0]),\n            (""test"", sample_sets[1]),\n        ]:\n            for sample in sub_set_samples:\n                sample.sub_set = sub_set\n    for sub_set, sub_set_samples in group(samples, lambda s: s.sub_set).items():\n        t = sum(map(lambda s: s.end - s.start, sub_set_samples)) / (1000 * 60 * 60)\n        print(\n            \'Sub-set ""{}"" with {} samples (duration: {:.2f} h)\'.format(\n                sub_set, len(sub_set_samples), t\n            )\n        )\n\n\ndef create_sample_dirs(language):\n    print(""Creating sample directories..."")\n    for set_name in [""train"", ""dev"", ""test""]:\n        dir_path = os.path.join(CLI_ARGS.base_dir, language + ""-"" + set_name)\n        if not os.path.isdir(dir_path):\n            os.mkdir(dir_path)\n\n\ndef split_audio_files(samples, language):\n    print(""Splitting audio files..."")\n    sub_sets = Counter()\n    src_wav_files = group(samples, lambda s: s.wav_path).items()\n    bar = progressbar.ProgressBar(max_value=len(src_wav_files), widgets=SIMPLE_BAR)\n    for wav_path, file_samples in bar(src_wav_files):\n        file_samples = sorted(file_samples, key=lambda s: s.start)\n        with wave.open(wav_path, ""r"") as src_wav_file:\n            rate = src_wav_file.getframerate()\n            for sample in file_samples:\n                index = sub_sets[sample.sub_set]\n                sample_wav_path = os.path.join(\n                    CLI_ARGS.base_dir,\n                    language + ""-"" + sample.sub_set,\n                    ""sample-{0:06d}.wav"".format(index),\n                )\n                sample.wav_path = sample_wav_path\n                sub_sets[sample.sub_set] += 1\n                src_wav_file.setpos(int(sample.start * rate / 1000.0))\n                data = src_wav_file.readframes(\n                    int((sample.end - sample.start) * rate / 1000.0)\n                )\n                with wave.open(sample_wav_path, ""w"") as sample_wav_file:\n                    sample_wav_file.setnchannels(src_wav_file.getnchannels())\n                    sample_wav_file.setsampwidth(src_wav_file.getsampwidth())\n                    sample_wav_file.setframerate(rate)\n                    sample_wav_file.writeframes(data)\n\n\ndef write_csvs(samples, language):\n    for sub_set, set_samples in group(samples, lambda s: s.sub_set).items():\n        set_samples = sorted(set_samples, key=lambda s: s.wav_path)\n        base_dir = os.path.abspath(CLI_ARGS.base_dir)\n        csv_path = os.path.join(base_dir, language + ""-"" + sub_set + "".csv"")\n        print(\'Writing ""{}""...\'.format(csv_path))\n        with open(csv_path, ""w"", encoding=""utf-8"", newline="""") as csv_file:\n            writer = csv.DictWriter(\n                csv_file, fieldnames=FIELDNAMES_EXT if CLI_ARGS.add_meta else FIELDNAMES\n            )\n            writer.writeheader()\n            bar = progressbar.ProgressBar(\n                max_value=len(set_samples), widgets=SIMPLE_BAR\n            )\n            for sample in bar(set_samples):\n                row = {\n                    ""wav_filename"": os.path.relpath(sample.wav_path, base_dir),\n                    ""wav_filesize"": os.path.getsize(sample.wav_path),\n                    ""transcript"": sample.text,\n                }\n                if CLI_ARGS.add_meta:\n                    row[""article""] = sample.article\n                    row[""speaker""] = sample.speaker\n                writer.writerow(row)\n\n\ndef cleanup(archive, language):\n    if not CLI_ARGS.keep_archive:\n        print(\'Removing archive ""{}""...\'.format(archive))\n        os.remove(archive)\n    language_dir = os.path.join(CLI_ARGS.base_dir, language)\n    if not CLI_ARGS.keep_intermediate and os.path.isdir(language_dir):\n        print(\'Removing intermediate files in ""{}""...\'.format(language_dir))\n        shutil.rmtree(language_dir)\n\n\ndef prepare_language(language):\n    archive = maybe_download_language(language)\n    extracted = maybe_extract(CLI_ARGS.base_dir, language, archive)\n    maybe_convert_to_wav(extracted)\n    samples = collect_samples(extracted, language)\n    assign_sub_sets(samples)\n    create_sample_dirs(language)\n    split_audio_files(samples, language)\n    write_csvs(samples, language)\n    cleanup(archive, language)\n\n\ndef handle_args():\n    parser = argparse.ArgumentParser(description=""Import Spoken Wikipedia Corpora"")\n    parser.add_argument(""base_dir"", help=""Directory containing all data"")\n    parser.add_argument(\n        ""--language"", default=""all"", help=""One of (all|{})"".format(""|"".join(LANGUAGES))\n    )\n    parser.add_argument(\n        ""--exclude_numbers"",\n        type=bool,\n        default=True,\n        help=""If sequences with non-transliterated numbers should be excluded"",\n    )\n    parser.add_argument(\n        ""--max_duration"",\n        type=int,\n        default=10000,\n        help=""Maximum sample duration in milliseconds"",\n    )\n    parser.add_argument(\n        ""--ignore_too_long"",\n        type=bool,\n        default=False,\n        help=""If samples exceeding max_duration should be removed"",\n    )\n    parser.add_argument(\n        ""--normalize"",\n        action=""store_true"",\n        help=""Converts diacritic characters to their base ones"",\n    )\n    for language in LANGUAGES:\n        parser.add_argument(\n            ""--{}_alphabet"".format(language),\n            help=""Exclude {} samples with characters not in provided alphabet file"".format(\n                language\n            ),\n        )\n    parser.add_argument(\n        ""--add_meta"", action=""store_true"", help=""Adds article and speaker CSV columns""\n    )\n    parser.add_argument(\n        ""--exclude_unknown_speakers"",\n        action=""store_true"",\n        help=""Exclude unknown speakers"",\n    )\n    parser.add_argument(\n        ""--exclude_unknown_articles"",\n        action=""store_true"",\n        help=""Exclude unknown articles"",\n    )\n    parser.add_argument(\n        ""--keep_archive"",\n        type=bool,\n        default=True,\n        help=""If downloaded archives should be kept"",\n    )\n    parser.add_argument(\n        ""--keep_intermediate"",\n        type=bool,\n        default=False,\n        help=""If intermediate files should be kept"",\n    )\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    CLI_ARGS = handle_args()\n    if CLI_ARGS.language == ""all"":\n        for lang in LANGUAGES:\n            prepare_language(lang)\n    elif CLI_ARGS.language in LANGUAGES:\n        prepare_language(CLI_ARGS.language)\n    else:\n        fail(""Wrong language id"")\n'"
bin/import_ted.py,0,"b'#!/usr/bin/env python\nimport sys\nimport tarfile\nimport unicodedata\nimport wave\nfrom glob import glob\nfrom os import makedirs, path, remove, rmdir\n\nimport pandas\nfrom sox import Transformer\nfrom tensorflow.python.platform import gfile\n\nfrom deepspeech_training.util.downloader import maybe_download\nfrom deepspeech_training.util.stm import parse_stm_file\n\n\ndef _download_and_preprocess_data(data_dir):\n    # Conditionally download data\n    TED_DATA = ""TEDLIUM_release2.tar.gz""\n    TED_DATA_URL = ""http://www.openslr.org/resources/19/TEDLIUM_release2.tar.gz""\n    local_file = maybe_download(TED_DATA, data_dir, TED_DATA_URL)\n\n    # Conditionally extract TED data\n    TED_DIR = ""TEDLIUM_release2""\n    _maybe_extract(data_dir, TED_DIR, local_file)\n\n    # Conditionally convert TED sph data to wav\n    _maybe_convert_wav(data_dir, TED_DIR)\n\n    # Conditionally split TED wav and text data into sentences\n    train_files, dev_files, test_files = _maybe_split_sentences(data_dir, TED_DIR)\n\n    # Write sets to disk as CSV files\n    train_files.to_csv(path.join(data_dir, ""ted-train.csv""), index=False)\n    dev_files.to_csv(path.join(data_dir, ""ted-dev.csv""), index=False)\n    test_files.to_csv(path.join(data_dir, ""ted-test.csv""), index=False)\n\n\ndef _maybe_extract(data_dir, extracted_data, archive):\n    # If data_dir/extracted_data does not exist, extract archive in data_dir\n    if not gfile.Exists(path.join(data_dir, extracted_data)):\n        tar = tarfile.open(archive)\n        tar.extractall(data_dir)\n        tar.close()\n\n\ndef _maybe_convert_wav(data_dir, extracted_data):\n    # Create extracted_data dir\n    extracted_dir = path.join(data_dir, extracted_data)\n\n    # Conditionally convert dev sph to wav\n    _maybe_convert_wav_dataset(extracted_dir, ""dev"")\n\n    # Conditionally convert train sph to wav\n    _maybe_convert_wav_dataset(extracted_dir, ""train"")\n\n    # Conditionally convert test sph to wav\n    _maybe_convert_wav_dataset(extracted_dir, ""test"")\n\n\ndef _maybe_convert_wav_dataset(extracted_dir, data_set):\n    # Create source dir\n    source_dir = path.join(extracted_dir, data_set, ""sph"")\n\n    # Create target dir\n    target_dir = path.join(extracted_dir, data_set, ""wav"")\n\n    # Conditionally convert sph files to wav files\n    if not gfile.Exists(target_dir):\n        # Create target_dir\n        makedirs(target_dir)\n\n        # Loop over sph files in source_dir and convert each to wav\n        for sph_file in glob(path.join(source_dir, ""*.sph"")):\n            transformer = Transformer()\n            wav_filename = path.splitext(path.basename(sph_file))[0] + "".wav""\n            wav_file = path.join(target_dir, wav_filename)\n            transformer.build(sph_file, wav_file)\n            remove(sph_file)\n\n        # Remove source_dir\n        rmdir(source_dir)\n\n\ndef _maybe_split_sentences(data_dir, extracted_data):\n    # Create extracted_data dir\n    extracted_dir = path.join(data_dir, extracted_data)\n\n    # Conditionally split dev wav\n    dev_files = _maybe_split_dataset(extracted_dir, ""dev"")\n\n    # Conditionally split train wav\n    train_files = _maybe_split_dataset(extracted_dir, ""train"")\n\n    # Conditionally split test wav\n    test_files = _maybe_split_dataset(extracted_dir, ""test"")\n\n    return train_files, dev_files, test_files\n\n\ndef _maybe_split_dataset(extracted_dir, data_set):\n    # Create stm dir\n    stm_dir = path.join(extracted_dir, data_set, ""stm"")\n\n    # Create wav dir\n    wav_dir = path.join(extracted_dir, data_set, ""wav"")\n\n    files = []\n\n    # Loop over stm files and split corresponding wav\n    for stm_file in glob(path.join(stm_dir, ""*.stm"")):\n        # Parse stm file\n        stm_segments = parse_stm_file(stm_file)\n\n        # Open wav corresponding to stm_file\n        wav_filename = path.splitext(path.basename(stm_file))[0] + "".wav""\n        wav_file = path.join(wav_dir, wav_filename)\n        origAudio = wave.open(wav_file, ""r"")\n\n        # Loop over stm_segments and split wav_file for each segment\n        for stm_segment in stm_segments:\n            # Create wav segment filename\n            start_time = stm_segment.start_time\n            stop_time = stm_segment.stop_time\n            new_wav_filename = (\n                path.splitext(path.basename(stm_file))[0]\n                + ""-""\n                + str(start_time)\n                + ""-""\n                + str(stop_time)\n                + "".wav""\n            )\n            new_wav_file = path.join(wav_dir, new_wav_filename)\n\n            # If the wav segment filename does not exist create it\n            if not gfile.Exists(new_wav_file):\n                _split_wav(origAudio, start_time, stop_time, new_wav_file)\n\n            new_wav_filesize = path.getsize(new_wav_file)\n            files.append(\n                (path.abspath(new_wav_file), new_wav_filesize, stm_segment.transcript)\n            )\n\n        # Close origAudio\n        origAudio.close()\n\n    return pandas.DataFrame(\n        data=files, columns=[""wav_filename"", ""wav_filesize"", ""transcript""]\n    )\n\n\ndef _split_wav(origAudio, start_time, stop_time, new_wav_file):\n    frameRate = origAudio.getframerate()\n    origAudio.setpos(int(start_time * frameRate))\n    chunkData = origAudio.readframes(int((stop_time - start_time) * frameRate))\n    chunkAudio = wave.open(new_wav_file, ""w"")\n    chunkAudio.setnchannels(origAudio.getnchannels())\n    chunkAudio.setsampwidth(origAudio.getsampwidth())\n    chunkAudio.setframerate(frameRate)\n    chunkAudio.writeframes(chunkData)\n    chunkAudio.close()\n\n\nif __name__ == ""__main__"":\n    _download_and_preprocess_data(sys.argv[1])\n'"
bin/import_timit.py,0,"b'#!/usr/bin/env python\n\n""""""\n    NAME    : LDC TIMIT Dataset\n    URL     : https://catalog.ldc.upenn.edu/ldc93s1\n    HOURS   : 5\n    TYPE    : Read - English\n    AUTHORS : Garofolo, John, et al.\n    TYPE    : LDC Membership\n    LICENCE : LDC User Agreement\n""""""\n\nimport errno\nimport fnmatch\nimport os\nimport subprocess\nimport sys\nimport tarfile\nfrom os import path\n\nimport pandas as pd\n\n\ndef clean(word):\n    # LC ALL & strip punctuation which are not required\n    new = word.lower().replace(""."", """")\n    new = new.replace("","", """")\n    new = new.replace("";"", """")\n    new = new.replace(\'""\', """")\n    new = new.replace(""!"", """")\n    new = new.replace(""?"", """")\n    new = new.replace("":"", """")\n    new = new.replace(""-"", """")\n    return new\n\n\ndef _preprocess_data(args):\n\n    # Assume data is downloaded from LDC - https://catalog.ldc.upenn.edu/ldc93s1\n\n    # SA sentences are repeated throughout by each speaker therefore can be removed for ASR as they will affect WER\n    ignoreSASentences = True\n\n    if ignoreSASentences:\n        print(""Using recommended ignore SA sentences"")\n        print(\n            ""Ignoring SA sentences (2 x sentences which are repeated by all speakers)""\n        )\n    else:\n        print(""Using unrecommended setting to include SA sentences"")\n\n    datapath = args\n    target = path.join(datapath, ""TIMIT"")\n    print(\n        ""Checking to see if data has already been extracted in given argument: %s"",\n        target,\n    )\n\n    if not path.isdir(target):\n        print(\n            ""Could not find extracted data, trying to find: TIMIT-LDC93S1.tgz in: "",\n            datapath,\n        )\n        filepath = path.join(datapath, ""TIMIT-LDC93S1.tgz"")\n        if path.isfile(filepath):\n            print(""File found, extracting"")\n            tar = tarfile.open(filepath)\n            tar.extractall(target)\n            tar.close()\n        else:\n            print(""File should be downloaded from LDC and placed at:"", filepath)\n            strerror = ""File not found""\n            raise IOError(errno, strerror, filepath)\n\n    else:\n        # is path therefore continue\n        print(""Found extracted data in: "", target)\n\n    print(""Preprocessing data"")\n    # We convert the .WAV (NIST sphere format) into MSOFT .wav\n    # creates _rif.wav as the new .wav file\n    for root, dirnames, filenames in os.walk(target):\n        for filename in fnmatch.filter(filenames, ""*.WAV""):\n            sph_file = os.path.join(root, filename)\n            wav_file = os.path.join(root, filename)[:-4] + ""_rif.wav""\n            print(""converting {} to {}"".format(sph_file, wav_file))\n            subprocess.check_call([""sox"", sph_file, wav_file])\n\n    print(""Preprocessing Complete"")\n    print(""Building CSVs"")\n\n    # Lists to build CSV files\n    train_list_wavs, train_list_trans, train_list_size = [], [], []\n    test_list_wavs, test_list_trans, test_list_size = [], [], []\n\n    for root, dirnames, filenames in os.walk(target):\n        for filename in fnmatch.filter(filenames, ""*_rif.wav""):\n            full_wav = os.path.join(root, filename)\n            wav_filesize = path.getsize(full_wav)\n\n            # need to remove _rif.wav (8chars) then add .TXT\n            trans_file = full_wav[:-8] + "".TXT""\n            with open(trans_file, ""r"") as f:\n                for line in f:\n                    split = line.split()\n                    start = split[0]\n                    end = split[1]\n                    t_list = split[2:]\n                    trans = """"\n\n                    for t in t_list:\n                        trans = trans + "" "" + clean(t)\n\n            # if ignoreSAsentences we only want those without SA in the name\n            # OR\n            # if not ignoreSAsentences we want all to be added\n            if (ignoreSASentences and not (""SA"" in os.path.basename(full_wav))) or (\n                not ignoreSASentences\n            ):\n                if ""train"" in full_wav.lower():\n                    train_list_wavs.append(full_wav)\n                    train_list_trans.append(trans)\n                    train_list_size.append(wav_filesize)\n                elif ""test"" in full_wav.lower():\n                    test_list_wavs.append(full_wav)\n                    test_list_trans.append(trans)\n                    test_list_size.append(wav_filesize)\n                else:\n                    raise IOError\n\n    a = {\n        ""wav_filename"": train_list_wavs,\n        ""wav_filesize"": train_list_size,\n        ""transcript"": train_list_trans,\n    }\n\n    c = {\n        ""wav_filename"": test_list_wavs,\n        ""wav_filesize"": test_list_size,\n        ""transcript"": test_list_trans,\n    }\n\n    all = {\n        ""wav_filename"": train_list_wavs + test_list_wavs,\n        ""wav_filesize"": train_list_size + test_list_size,\n        ""transcript"": train_list_trans + test_list_trans,\n    }\n\n    df_all = pd.DataFrame(\n        all, columns=[""wav_filename"", ""wav_filesize"", ""transcript""], dtype=int\n    )\n    df_train = pd.DataFrame(\n        a, columns=[""wav_filename"", ""wav_filesize"", ""transcript""], dtype=int\n    )\n    df_test = pd.DataFrame(\n        c, columns=[""wav_filename"", ""wav_filesize"", ""transcript""], dtype=int\n    )\n\n    df_all.to_csv(\n        target + ""/timit_all.csv"", sep="","", header=True, index=False, encoding=""ascii""\n    )\n    df_train.to_csv(\n        target + ""/timit_train.csv"", sep="","", header=True, index=False, encoding=""ascii""\n    )\n    df_test.to_csv(\n        target + ""/timit_test.csv"", sep="","", header=True, index=False, encoding=""ascii""\n    )\n\n\nif __name__ == ""__main__"":\n    _preprocess_data(sys.argv[1])\n    print(""Completed"")\n'"
bin/import_ts.py,0,"b'#!/usr/bin/env python3\nimport csv\nimport os\nimport re\nimport subprocess\nimport zipfile\nfrom multiprocessing import Pool\n\nimport progressbar\nimport sox\n\nimport unidecode\nfrom deepspeech_training.util.downloader import SIMPLE_BAR, maybe_download\nfrom deepspeech_training.util.importers import (\n    get_counter,\n    get_imported_samples,\n    get_importers_parser,\n    get_validate_label,\n    print_import_report,\n)\n\nFIELDNAMES = [""wav_filename"", ""wav_filesize"", ""transcript""]\nSAMPLE_RATE = 16000\nMAX_SECS = 15\nARCHIVE_NAME = ""2019-04-11_fr_FR""\nARCHIVE_DIR_NAME = ""ts_"" + ARCHIVE_NAME\nARCHIVE_URL = (\n    ""https://deepspeech-storage-mirror.s3.fr-par.scw.cloud/"" + ARCHIVE_NAME + "".zip""\n)\n\n\ndef _download_and_preprocess_data(target_dir, english_compatible=False):\n    # Making path absolute\n    target_dir = os.path.abspath(target_dir)\n    # Conditionally download data\n    archive_path = maybe_download(\n        ""ts_"" + ARCHIVE_NAME + "".zip"", target_dir, ARCHIVE_URL\n    )\n    # Conditionally extract archive data\n    _maybe_extract(target_dir, ARCHIVE_DIR_NAME, archive_path)\n    # Conditionally convert TrainingSpeech data to DeepSpeech CSVs and wav\n    _maybe_convert_sets(\n        target_dir, ARCHIVE_DIR_NAME, english_compatible=english_compatible\n    )\n\n\ndef _maybe_extract(target_dir, extracted_data, archive_path):\n    # If target_dir/extracted_data does not exist, extract archive in target_dir\n    extracted_path = os.path.join(target_dir, extracted_data)\n    if not os.path.exists(extracted_path):\n        print(\'No directory ""%s"" - extracting archive...\' % extracted_path)\n        if not os.path.isdir(extracted_path):\n            os.mkdir(extracted_path)\n        with zipfile.ZipFile(archive_path) as zip_f:\n            zip_f.extractall(extracted_path)\n    else:\n        print(\'Found directory ""%s"" - not extracting it from archive.\' % archive_path)\n\n\ndef one_sample(sample):\n    """""" Take a audio file, and optionally convert it to 16kHz WAV """"""\n    orig_filename = sample[""path""]\n    # Storing wav files next to the wav ones - just with a different suffix\n    wav_filename = os.path.splitext(orig_filename)[0] + "".converted.wav""\n    _maybe_convert_wav(orig_filename, wav_filename)\n    file_size = -1\n    frames = 0\n    if os.path.exists(wav_filename):\n        file_size = os.path.getsize(wav_filename)\n        frames = int(\n            subprocess.check_output(\n                [""soxi"", ""-s"", wav_filename], stderr=subprocess.STDOUT\n            )\n        )\n    label = sample[""text""]\n\n    rows = []\n\n    # Keep track of how many samples are good vs. problematic\n    counter = get_counter()\n    if file_size == -1:\n        # Excluding samples that failed upon conversion\n        counter[""failed""] += 1\n    elif label is None:\n        # Excluding samples that failed on label validation\n        counter[""invalid_label""] += 1\n    elif int(frames / SAMPLE_RATE * 1000 / 10 / 2) < len(str(label)):\n        # Excluding samples that are too short to fit the transcript\n        counter[""too_short""] += 1\n    elif frames / SAMPLE_RATE > MAX_SECS:\n        # Excluding very long samples to keep a reasonable batch-size\n        counter[""too_long""] += 1\n    else:\n        # This one is good - keep it for the target CSV\n        rows.append((wav_filename, file_size, label))\n        counter[""imported_time""] += frames\n    counter[""all""] += 1\n    counter[""total_time""] += frames\n\n    return (counter, rows)\n\n\ndef _maybe_convert_sets(target_dir, extracted_data, english_compatible=False):\n    extracted_dir = os.path.join(target_dir, extracted_data)\n    # override existing CSV with normalized one\n    target_csv_template = os.path.join(target_dir, ""ts_"" + ARCHIVE_NAME + ""_{}.csv"")\n    if os.path.isfile(target_csv_template):\n        return\n    path_to_original_csv = os.path.join(extracted_dir, ""data.csv"")\n    with open(path_to_original_csv) as csv_f:\n        data = [\n            d\n            for d in csv.DictReader(csv_f, delimiter="","")\n            if float(d[""duration""]) <= MAX_SECS\n        ]\n\n    for line in data:\n        line[""path""] = os.path.join(extracted_dir, line[""path""])\n\n    num_samples = len(data)\n    rows = []\n    counter = get_counter()\n\n    print(""Importing {} wav files..."".format(num_samples))\n    pool = Pool()\n    bar = progressbar.ProgressBar(max_value=num_samples, widgets=SIMPLE_BAR)\n    for i, processed in enumerate(pool.imap_unordered(one_sample, data), start=1):\n        counter += processed[0]\n        rows += processed[1]\n        bar.update(i)\n    bar.update(num_samples)\n    pool.close()\n    pool.join()\n\n    with open(target_csv_template.format(""train""), ""w"", encoding=""utf-8"", newline="""") as train_csv_file:  # 80%\n        with open(target_csv_template.format(""dev""), ""w"", encoding=""utf-8"", newline="""") as dev_csv_file:  # 10%\n            with open(target_csv_template.format(""test""), ""w"", encoding=""utf-8"", newline="""") as test_csv_file:  # 10%\n                train_writer = csv.DictWriter(train_csv_file, fieldnames=FIELDNAMES)\n                train_writer.writeheader()\n                dev_writer = csv.DictWriter(dev_csv_file, fieldnames=FIELDNAMES)\n                dev_writer.writeheader()\n                test_writer = csv.DictWriter(test_csv_file, fieldnames=FIELDNAMES)\n                test_writer.writeheader()\n\n                for i, item in enumerate(rows):\n                    transcript = validate_label(\n                        cleanup_transcript(\n                            item[2], english_compatible=english_compatible\n                        )\n                    )\n                    if not transcript:\n                        continue\n                    wav_filename = os.path.join(target_dir, extracted_data, item[0])\n                    i_mod = i % 10\n                    if i_mod == 0:\n                        writer = test_writer\n                    elif i_mod == 1:\n                        writer = dev_writer\n                    else:\n                        writer = train_writer\n                    writer.writerow(\n                        dict(\n                            wav_filename=wav_filename,\n                            wav_filesize=os.path.getsize(wav_filename),\n                            transcript=transcript,\n                        )\n                    )\n\n    imported_samples = get_imported_samples(counter)\n    assert counter[""all""] == num_samples\n    assert len(rows) == imported_samples\n\n    print_import_report(counter, SAMPLE_RATE, MAX_SECS)\n\n\ndef _maybe_convert_wav(orig_filename, wav_filename):\n    if not os.path.exists(wav_filename):\n        transformer = sox.Transformer()\n        transformer.convert(samplerate=SAMPLE_RATE)\n        try:\n            transformer.build(orig_filename, wav_filename)\n        except sox.core.SoxError as ex:\n            print(""SoX processing error"", ex, orig_filename, wav_filename)\n\n\nPUNCTUATIONS_REG = re.compile(r""[\xc2\xb0\\-,;!?.()\\[\\]*\xe2\x80\xa6\xe2\x80\x94]"")\nMULTIPLE_SPACES_REG = re.compile(r""\\s{2,}"")\n\n\ndef cleanup_transcript(text, english_compatible=False):\n    text = text.replace(""\xe2\x80\x99"", ""\'"").replace(""\\u00A0"", "" "")\n    text = PUNCTUATIONS_REG.sub("" "", text)\n    text = MULTIPLE_SPACES_REG.sub("" "", text)\n    if english_compatible:\n        text = unidecode.unidecode(text)\n    return text.strip().lower()\n\n\ndef handle_args():\n    parser = get_importers_parser(description=""Importer for TrainingSpeech dataset."")\n    parser.add_argument(dest=""target_dir"")\n    parser.add_argument(\n        ""--english-compatible"",\n        action=""store_true"",\n        dest=""english_compatible"",\n        help=""Remove diactrics and other non-ascii chars."",\n    )\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    cli_args = handle_args()\n    validate_label = get_validate_label(cli_args)\n    _download_and_preprocess_data(cli_args.target_dir, cli_args.english_compatible)\n'"
bin/import_tuda.py,0,"b'#!/usr/bin/env python\n""""""\nDownloads and prepares (parts of) the ""German Distant Speech"" corpus (TUDA) for DeepSpeech.py\nUse ""python3 import_tuda.py -h"" for help\n""""""\nimport argparse\nimport csv\nimport os\nimport tarfile\nimport unicodedata\nimport wave\nimport xml.etree.cElementTree as ET\nfrom collections import Counter\n\nimport progressbar\n\nfrom deepspeech_training.util.downloader import SIMPLE_BAR, maybe_download\nfrom deepspeech_training.util.importers import validate_label_eng as validate_label\nfrom deepspeech_training.util.text import Alphabet\n\nTUDA_VERSION = ""v2""\nTUDA_PACKAGE = ""german-speechdata-package-{}"".format(TUDA_VERSION)\nTUDA_URL = ""http://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/{}.tar.gz"".format(\n    TUDA_PACKAGE\n)\nTUDA_ARCHIVE = ""{}.tar.gz"".format(TUDA_PACKAGE)\n\nCHANNELS = 1\nSAMPLE_WIDTH = 2\nSAMPLE_RATE = 16000\n\nFIELDNAMES = [""wav_filename"", ""wav_filesize"", ""transcript""]\n\n\ndef maybe_extract(archive):\n    extracted = os.path.join(CLI_ARGS.base_dir, TUDA_PACKAGE)\n    if os.path.isdir(extracted):\n        print(\'Found directory ""{}"" - not extracting.\'.format(extracted))\n    else:\n        print(\'Extracting ""{}""...\'.format(archive))\n        with tarfile.open(archive) as tar:\n            members = tar.getmembers()\n            bar = progressbar.ProgressBar(max_value=len(members), widgets=SIMPLE_BAR)\n            for member in bar(members):\n                tar.extract(member=member, path=CLI_ARGS.base_dir)\n    return extracted\n\n\ndef check_and_prepare_sentence(sentence):\n    sentence = sentence.lower().replace(""co2"", ""c o zwei"")\n    chars = []\n    for c in sentence:\n        if (\n            CLI_ARGS.normalize\n            and c not in ""\xc3\xa4\xc3\xb6\xc3\xbc\xc3\x9f""\n            and (ALPHABET is None or not ALPHABET.has_char(c))\n        ):\n            c = (\n                unicodedata.normalize(""NFKD"", c)\n                .encode(""ascii"", ""ignore"")\n                .decode(""ascii"", ""ignore"")\n            )\n        for sc in c:\n            if ALPHABET is not None and not ALPHABET.has_char(c):\n                return None\n            chars.append(sc)\n    return validate_label("""".join(chars))\n\n\ndef check_wav_file(wav_path, sentence):  # pylint: disable=too-many-return-statements\n    try:\n        with wave.open(wav_path, ""r"") as src_wav_file:\n            rate = src_wav_file.getframerate()\n            channels = src_wav_file.getnchannels()\n            sample_width = src_wav_file.getsampwidth()\n            milliseconds = int(src_wav_file.getnframes() * 1000 / rate)\n        if rate != SAMPLE_RATE:\n            return False, ""wrong sample rate""\n        if channels != CHANNELS:\n            return False, ""wrong number of channels""\n        if sample_width != SAMPLE_WIDTH:\n            return False, ""wrong sample width""\n        if milliseconds / len(sentence) < 30:\n            return False, ""too short""\n        if milliseconds > CLI_ARGS.max_duration > 0:\n            return False, ""too long""\n    except wave.Error:\n        return False, ""invalid wav file""\n    except EOFError:\n        return False, ""premature EOF""\n    return True, ""OK""\n\n\ndef write_csvs(extracted):\n    sample_counter = 0\n    reasons = Counter()\n    for sub_set in [""train"", ""dev"", ""test""]:\n        set_path = os.path.join(extracted, sub_set)\n        set_files = os.listdir(set_path)\n        recordings = {}\n        for file in set_files:\n            if file.endswith("".xml""):\n                recordings[file[:-4]] = []\n        for file in set_files:\n            if file.endswith("".wav"") and ""_"" in file:\n                prefix = file.split(""_"")[0]\n                if prefix in recordings:\n                    recordings[prefix].append(file)\n        recordings = recordings.items()\n        csv_path = os.path.join(\n            CLI_ARGS.base_dir, ""tuda-{}-{}.csv"".format(TUDA_VERSION, sub_set)\n        )\n        print(\'Writing ""{}""...\'.format(csv_path))\n        with open(csv_path, ""w"", encoding=""utf-8"", newline="""") as csv_file:\n            writer = csv.DictWriter(csv_file, fieldnames=FIELDNAMES)\n            writer.writeheader()\n            set_dir = os.path.join(extracted, sub_set)\n            bar = progressbar.ProgressBar(max_value=len(recordings), widgets=SIMPLE_BAR)\n            for prefix, wav_names in bar(recordings):\n                xml_path = os.path.join(set_dir, prefix + "".xml"")\n                meta = ET.parse(xml_path).getroot()\n                sentence = list(meta.iter(""cleaned_sentence""))[0].text\n                sentence = check_and_prepare_sentence(sentence)\n                if sentence is None:\n                    continue\n                for wav_name in wav_names:\n                    sample_counter += 1\n                    wav_path = os.path.join(set_path, wav_name)\n                    keep, reason = check_wav_file(wav_path, sentence)\n                    if keep:\n                        writer.writerow(\n                            {\n                                ""wav_filename"": os.path.relpath(\n                                    wav_path, CLI_ARGS.base_dir\n                                ),\n                                ""wav_filesize"": os.path.getsize(wav_path),\n                                ""transcript"": sentence.lower(),\n                            }\n                        )\n                    else:\n                        reasons[reason] += 1\n    if len(reasons.keys()) > 0:\n        print(""Excluded samples:"")\n        for reason, n in reasons.most_common():\n            print(\' - ""{}"": {} ({:.2f}%)\'.format(reason, n, n * 100 / sample_counter))\n\n\ndef cleanup(archive):\n    if not CLI_ARGS.keep_archive:\n        print(\'Removing archive ""{}""...\'.format(archive))\n        os.remove(archive)\n\n\ndef download_and_prepare():\n    archive = maybe_download(TUDA_ARCHIVE, CLI_ARGS.base_dir, TUDA_URL)\n    extracted = maybe_extract(archive)\n    write_csvs(extracted)\n    cleanup(archive)\n\n\ndef handle_args():\n    parser = argparse.ArgumentParser(description=""Import German Distant Speech (TUDA)"")\n    parser.add_argument(""base_dir"", help=""Directory containing all data"")\n    parser.add_argument(\n        ""--max_duration"",\n        type=int,\n        default=10000,\n        help=""Maximum sample duration in milliseconds"",\n    )\n    parser.add_argument(\n        ""--normalize"",\n        action=""store_true"",\n        help=""Converts diacritic characters to their base ones"",\n    )\n    parser.add_argument(\n        ""--alphabet"",\n        help=""Exclude samples with characters not in provided alphabet file"",\n    )\n    parser.add_argument(\n        ""--keep_archive"",\n        type=bool,\n        default=True,\n        help=""If downloaded archives should be kept"",\n    )\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    CLI_ARGS = handle_args()\n    ALPHABET = Alphabet(CLI_ARGS.alphabet) if CLI_ARGS.alphabet else None\n    download_and_prepare()\n'"
bin/import_vctk.py,0,"b'#!/usr/bin/env python\n# VCTK used in wavenet paper https://arxiv.org/pdf/1609.03499.pdf\n# Licenced under Open Data Commons Attribution License (ODC-By) v1.0.\n# as per https://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html\nimport os\nimport random\nimport re\nfrom multiprocessing import Pool\nfrom zipfile import ZipFile\n\nimport librosa\nimport progressbar\n\nfrom deepspeech_training.util.downloader import SIMPLE_BAR, maybe_download\nfrom deepspeech_training.util.importers import (\n    get_counter,\n    get_imported_samples,\n    print_import_report,\n)\n\nSAMPLE_RATE = 16000\nMAX_SECS = 10\nMIN_SECS = 1\nARCHIVE_DIR_NAME = ""VCTK-Corpus""\nARCHIVE_NAME = ""VCTK-Corpus.zip?sequence=2&isAllowed=y""\nARCHIVE_URL = (\n    ""https://datashare.is.ed.ac.uk/bitstream/handle/10283/2651/"" + ARCHIVE_NAME\n)\n\n\ndef _download_and_preprocess_data(target_dir):\n    # Making path absolute\n    target_dir = os.path.abspath(target_dir)\n    # Conditionally download data\n    archive_path = maybe_download(ARCHIVE_NAME, target_dir, ARCHIVE_URL)\n    # Conditionally extract common voice data\n    _maybe_extract(target_dir, ARCHIVE_DIR_NAME, archive_path)\n    # Conditionally convert common voice CSV files and mp3 data to DeepSpeech CSVs and wav\n    _maybe_convert_sets(target_dir, ARCHIVE_DIR_NAME)\n\n\ndef _maybe_extract(target_dir, extracted_data, archive_path):\n    # If target_dir/extracted_data does not exist, extract archive in target_dir\n    extracted_path = os.path.join(target_dir, extracted_data)\n    if not os.path.exists(extracted_path):\n        print(f""No directory {extracted_path} - extracting archive..."")\n        with ZipFile(archive_path, ""r"") as zipobj:\n            # Extract all the contents of zip file in current directory\n            zipobj.extractall(target_dir)\n    else:\n        print(f""Found directory {extracted_path} - not extracting it from archive."")\n\n\ndef _maybe_convert_sets(target_dir, extracted_data):\n    extracted_dir = os.path.join(target_dir, extracted_data, ""wav48"")\n    txt_dir = os.path.join(target_dir, extracted_data, ""txt"")\n\n    directory = os.path.expanduser(extracted_dir)\n    srtd = len(sorted(os.listdir(directory)))\n    all_samples = []\n\n    for target in sorted(os.listdir(directory)):\n        all_samples += _maybe_prepare_set(\n            path.join(extracted_dir, os.path.split(target)[-1])\n        )\n\n    num_samples = len(all_samples)\n    print(f""Converting wav files to {SAMPLE_RATE}hz..."")\n    pool = Pool()\n    bar = progressbar.ProgressBar(max_value=num_samples, widgets=SIMPLE_BAR)\n    for i, _ in enumerate(pool.imap_unordered(one_sample, all_samples), start=1):\n        bar.update(i)\n    bar.update(num_samples)\n    pool.close()\n    pool.join()\n\n    _write_csv(extracted_dir, txt_dir, target_dir)\n\n\ndef one_sample(sample):\n    if is_audio_file(sample):\n        y, sr = librosa.load(sample, sr=16000)\n\n        # Trim the beginning and ending silence\n        yt, index = librosa.effects.trim(y)  # pylint: disable=unused-variable\n\n        duration = librosa.get_duration(yt, sr)\n        if duration > MAX_SECS or duration < MIN_SECS:\n            os.remove(sample)\n        else:\n            librosa.output.write_wav(sample, yt, sr)\n\n\ndef _maybe_prepare_set(target_csv):\n    samples = sorted(os.listdir(target_csv))\n    new_samples = []\n    for s in samples:\n        new_samples.append(os.path.join(target_csv, s))\n    samples = new_samples\n    return samples\n\n\ndef _write_csv(extracted_dir, txt_dir, target_dir):\n    print(f""Writing CSV file"")\n    dset_abs_path = extracted_dir\n    dset_txt_abs_path = txt_dir\n\n    audios = make_manifest(dset_abs_path)\n    utterences = load_txts(dset_txt_abs_path)\n\n    csv = []\n\n    for file in audios:\n\n        st = os.stat(file)\n        file_size = st.st_size\n\n        # Seems to be one wav directory missing from txts - skip it\n        file_parts = file.split(os.sep)\n        file_subdir = file_parts[-2]\n        if file_subdir == ""p315"":\n            continue\n\n        file_name = file_parts[-1]\n        file_name_no_ext = file_name.split(""."")[0]\n\n        utterence = utterences[file_name_no_ext]\n        utterence_clean = re.sub(r""[^a-zA-Z\' ]+"", """", utterence).lower().strip()\n\n        csv_line = f""{file},{file_size},{utterence_clean}\\n""\n        csv.append(csv_line)\n\n    random.seed(1454)\n    random.shuffle(csv)\n\n    train_data = csv[:37000]\n    dev_data = csv[37000:40200]\n    test_data = csv[40200:]\n\n    with open(os.path.join(target_dir, ""vctk_full.csv""), ""w"") as fd:\n        fd.write(""wav_filename,wav_filesize,transcript\\n"")\n        for i in csv:\n            fd.write(i)\n    with open(os.path.join(target_dir, ""vctk_train.csv""), ""w"") as fd:\n        fd.write(""wav_filename,wav_filesize,transcript\\n"")\n        for i in train_data:\n            fd.write(i)\n    with open(os.path.join(target_dir, ""vctk_dev.csv""), ""w"") as fd:\n        fd.write(""wav_filename,wav_filesize,transcript\\n"")\n        for i in dev_data:\n            fd.write(i)\n    with open(os.path.join(target_dir, ""vctk_test.csv""), ""w"") as fd:\n        fd.write(""wav_filename,wav_filesize,transcript\\n"")\n        for i in test_data:\n            fd.write(i)\n\n    print(f""Wrote {len(csv)} entries"")\n\n\ndef make_manifest(directory):\n    audios = []\n    directory = os.path.expanduser(directory)\n    for target in sorted(os.listdir(directory)):\n        d = os.path.join(directory, target)\n        if not os.path.isdir(d):\n            continue\n\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in fnames:\n                new_path = os.path.join(root, fname)\n                item = new_path\n                audios.append(item)\n    return audios\n\n\ndef load_txts(directory):\n    utterences = dict()\n    directory = os.path.expanduser(directory)\n    for target in sorted(os.listdir(directory)):\n        d = os.path.join(directory, target)\n        if not os.path.isdir(d):\n            continue\n\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in fnames:\n                if fname.endswith("".txt""):\n                    with open(os.path.join(root, fname), ""r"") as f:\n                        fname_no_ext = os.path.basename(fname).rsplit(""."", 1)[0]\n                        utterences[fname_no_ext] = f.readline()\n    return utterences\n\n\nAUDIO_EXTENSIONS = ["".wav"", ""WAV""]\n\n\ndef is_audio_file(filepath):\n    return any(\n        os.path.basename(filepath).endswith(extension) for extension in AUDIO_EXTENSIONS\n    )\n\n\nif __name__ == ""__main__"":\n    _download_and_preprocess_data(sys.argv[1])\n'"
bin/import_voxforge.py,0,"b'#!/usr/bin/env python\nimport codecs\nimport os\nimport re\nimport tarfile\nimport threading\nimport unicodedata\nimport urllib\nfrom glob import glob\nfrom multiprocessing.pool import ThreadPool\nfrom os import makedirs, path\n\nimport pandas\nfrom bs4 import BeautifulSoup\nfrom tensorflow.python.platform import gfile\nfrom deepspeech_training.util.downloader import maybe_download\n\n""""""The number of jobs to run in parallel""""""\nNUM_PARALLEL = 8\n\n""""""Lambda function returns the filename of a path""""""\nfilename_of = lambda x: path.split(x)[1]\n\n\nclass AtomicCounter(object):\n    """"""A class that atomically increments a counter""""""\n\n    def __init__(self, start_count=0):\n        """"""Initialize the counter\n        :param start_count: the number to start counting at\n        """"""\n        self.__lock = threading.Lock()\n        self.__count = start_count\n\n    def increment(self, amount=1):\n        """"""Increments the counter by the given amount\n        :param amount: the amount to increment by (default 1)\n        :return:       the incremented value of the counter\n        """"""\n        self.__lock.acquire()\n        self.__count += amount\n        v = self.value()\n        self.__lock.release()\n        return v\n\n    def value(self):\n        """"""Returns the current value of the counter (not atomic)""""""\n        return self.__count\n\n\ndef _parallel_downloader(voxforge_url, archive_dir, total, counter):\n    """"""Generate a function to download a file based on given parameters\n    This works by currying the above given arguments into a closure\n    in the form of the following function.\n\n    :param voxforge_url: the base voxforge URL\n    :param archive_dir:  the location to store the downloaded file\n    :param total:        the total number of files to download\n    :param counter:      an atomic counter to keep track of # of downloaded files\n    :return:             a function that actually downloads a file given these params\n    """"""\n\n    def download(d):\n        """"""Binds voxforge_url, archive_dir, total, and counter into this scope\n        Downloads the given file\n        :param d: a tuple consisting of (index, file) where index is the index\n                  of the file to download and file is the name of the file to download\n        """"""\n        (i, file) = d\n        download_url = voxforge_url + ""/"" + file\n        c = counter.increment()\n        print(""Downloading file {} ({}/{})..."".format(i + 1, c, total))\n        maybe_download(filename_of(download_url), archive_dir, download_url)\n\n    return download\n\n\ndef _parallel_extracter(data_dir, number_of_test, number_of_dev, total, counter):\n    """"""Generate a function to extract a tar file based on given parameters\n    This works by currying the above given arguments into a closure\n    in the form of the following function.\n\n    :param data_dir:       the target directory to extract into\n    :param number_of_test: the number of files to keep as the test set\n    :param number_of_dev:  the number of files to keep as the dev set\n    :param total:          the total number of files to extract\n    :param counter:        an atomic counter to keep track of # of extracted files\n    :return:               a function that actually extracts a tar file given these params\n    """"""\n\n    def extract(d):\n        """"""Binds data_dir, number_of_test, number_of_dev, total, and counter into this scope\n        Extracts the given file\n        :param d: a tuple consisting of (index, file) where index is the index\n                  of the file to extract and file is the name of the file to extract\n        """"""\n        (i, archive) = d\n        if i < number_of_test:\n            dataset_dir = path.join(data_dir, ""test"")\n        elif i < number_of_test + number_of_dev:\n            dataset_dir = path.join(data_dir, ""dev"")\n        else:\n            dataset_dir = path.join(data_dir, ""train"")\n        if not gfile.Exists(\n            os.path.join(dataset_dir, ""."".join(filename_of(archive).split(""."")[:-1]))\n        ):\n            c = counter.increment()\n            print(""Extracting file {} ({}/{})..."".format(i + 1, c, total))\n            tar = tarfile.open(archive)\n            tar.extractall(dataset_dir)\n            tar.close()\n\n    return extract\n\n\ndef _download_and_preprocess_data(data_dir):\n    # Conditionally download data to data_dir\n    if not path.isdir(data_dir):\n        makedirs(data_dir)\n\n    archive_dir = data_dir + ""/archive""\n    if not path.isdir(archive_dir):\n        makedirs(archive_dir)\n\n    print(\n        ""Downloading Voxforge data set into {} if not already present..."".format(\n            archive_dir\n        )\n    )\n\n    voxforge_url = ""http://www.repository.voxforge1.org/downloads/SpeechCorpus/Trunk/Audio/Main/16kHz_16bit""\n    html_page = urllib.request.urlopen(voxforge_url)\n    soup = BeautifulSoup(html_page, ""html.parser"")\n\n    # list all links\n    refs = [l[""href""] for l in soup.find_all(""a"") if "".tgz"" in l[""href""]]\n\n    # download files in parallel\n    print(""{} files to download"".format(len(refs)))\n    downloader = _parallel_downloader(\n        voxforge_url, archive_dir, len(refs), AtomicCounter()\n    )\n    p = ThreadPool(NUM_PARALLEL)\n    p.map(downloader, enumerate(refs))\n\n    # Conditionally extract data to dataset_dir\n    if not path.isdir(os.path.join(data_dir, ""test"")):\n        makedirs(os.path.join(data_dir, ""test""))\n    if not path.isdir(os.path.join(data_dir, ""dev"")):\n        makedirs(os.path.join(data_dir, ""dev""))\n    if not path.isdir(os.path.join(data_dir, ""train"")):\n        makedirs(os.path.join(data_dir, ""train""))\n\n    tarfiles = glob(os.path.join(archive_dir, ""*.tgz""))\n    number_of_files = len(tarfiles)\n    number_of_test = number_of_files // 100\n    number_of_dev = number_of_files // 100\n\n    # extract tars in parallel\n    print(\n        ""Extracting Voxforge data set into {} if not already present..."".format(\n            data_dir\n        )\n    )\n    extracter = _parallel_extracter(\n        data_dir, number_of_test, number_of_dev, len(tarfiles), AtomicCounter()\n    )\n    p.map(extracter, enumerate(tarfiles))\n\n    # Generate data set\n    print(""Generating Voxforge data set into {}"".format(data_dir))\n    test_files = _generate_dataset(data_dir, ""test"")\n    dev_files = _generate_dataset(data_dir, ""dev"")\n    train_files = _generate_dataset(data_dir, ""train"")\n\n    # Write sets to disk as CSV files\n    train_files.to_csv(os.path.join(data_dir, ""voxforge-train.csv""), index=False)\n    dev_files.to_csv(os.path.join(data_dir, ""voxforge-dev.csv""), index=False)\n    test_files.to_csv(os.path.join(data_dir, ""voxforge-test.csv""), index=False)\n\n\ndef _generate_dataset(data_dir, data_set):\n    extracted_dir = path.join(data_dir, data_set)\n    files = []\n    for promts_file in glob(os.path.join(extracted_dir + ""/*/etc/"", ""PROMPTS"")):\n        if path.isdir(os.path.join(promts_file[:-11], ""wav"")):\n            with codecs.open(promts_file, ""r"", ""utf-8"") as f:\n                for line in f:\n                    id = line.split("" "")[0].split(""/"")[-1]\n                    sentence = "" "".join(line.split("" "")[1:])\n                    sentence = re.sub(""[^a-z\']"", "" "", sentence.strip().lower())\n                    transcript = """"\n                    for token in sentence.split("" ""):\n                        word = token.strip()\n                        if word != """" and word != "" "":\n                            transcript += word + "" ""\n                    transcript = (\n                        unicodedata.normalize(""NFKD"", transcript.strip())\n                        .encode(""ascii"", ""ignore"")\n                        .decode(""ascii"", ""ignore"")\n                    )\n                    wav_file = path.join(promts_file[:-11], ""wav/"" + id + "".wav"")\n                    if gfile.Exists(wav_file):\n                        wav_filesize = path.getsize(wav_file)\n                        # remove audios that are shorter than 0.5s and longer than 20s.\n                        # remove audios that are too short for transcript.\n                        if (\n                            (wav_filesize / 32000) > 0.5\n                            and (wav_filesize / 32000) < 20\n                            and transcript != """"\n                            and wav_filesize / len(transcript) > 1400\n                        ):\n                            files.append(\n                                (os.path.abspath(wav_file), wav_filesize, transcript)\n                            )\n\n    return pandas.DataFrame(\n        data=files, columns=[""wav_filename"", ""wav_filesize"", ""transcript""]\n    )\n\n\nif __name__ == ""__main__"":\n    _download_and_preprocess_data(sys.argv[1])\n'"
bin/ops_in_graph.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\n\nimport tensorflow.compat.v1 as tfv1\n\n\ndef main():\n    with tfv1.gfile.FastGFile(sys.argv[1], ""rb"") as fin:\n        graph_def = tfv1.GraphDef()\n        graph_def.ParseFromString(fin.read())\n\n        print(""\\n"".join(sorted(set(n.op for n in graph_def.node))))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
bin/play.py,0,"b'#!/usr/bin/env python\n""""""\nTool for playing (and augmenting) single samples or samples from Sample Databases (SDB files) and DeepSpeech CSV files\nUse ""python3 build_sdb.py -h"" for help\n""""""\n\nimport os\nimport sys\nimport random\nimport argparse\n\nfrom deepspeech_training.util.audio import LOADABLE_AUDIO_EXTENSIONS, AUDIO_TYPE_PCM, AUDIO_TYPE_WAV\nfrom deepspeech_training.util.sample_collections import SampleList, LabeledSample, samples_from_source, augment_samples\n\n\ndef get_samples_in_play_order():\n    ext = os.path.splitext(CLI_ARGS.source)[1].lower()\n    if ext in LOADABLE_AUDIO_EXTENSIONS:\n        samples = SampleList([(CLI_ARGS.source, 0)], labeled=False)\n    else:\n        samples = samples_from_source(CLI_ARGS.source, buffering=0)\n    played = 0\n    index = CLI_ARGS.start\n    while True:\n        if 0 <= CLI_ARGS.number <= played:\n            return\n        if CLI_ARGS.random:\n            yield samples[random.randint(0, len(samples) - 1)]\n        elif index < 0:\n            yield samples[len(samples) + index]\n        elif index >= len(samples):\n            print(""No sample with index {}"".format(CLI_ARGS.start))\n            sys.exit(1)\n        else:\n            yield samples[index]\n        played += 1\n        index = (index + 1) % len(samples)\n\n\ndef play_collection():\n    samples = get_samples_in_play_order()\n    samples = augment_samples(samples,\n                              audio_type=AUDIO_TYPE_PCM,\n                              augmentation_specs=CLI_ARGS.augment,\n                              process_ahead=0,\n                              fixed_clock=CLI_ARGS.clock)\n    for sample in samples:\n        if not CLI_ARGS.quiet:\n            print(\'Sample ""{}""\'.format(sample.sample_id), file=sys.stderr)\n            if isinstance(sample, LabeledSample):\n                print(\'  ""{}""\'.format(sample.transcript), file=sys.stderr)\n        if CLI_ARGS.pipe:\n            sample.change_audio_type(AUDIO_TYPE_WAV)\n            sys.stdout.buffer.write(sample.audio.getvalue())\n            return\n        wave_obj = simpleaudio.WaveObject(sample.audio,\n                                          sample.audio_format.channels,\n                                          sample.audio_format.width,\n                                          sample.audio_format.rate)\n        play_obj = wave_obj.play()\n        play_obj.wait_done()\n\n\ndef handle_args():\n    parser = argparse.ArgumentParser(\n        description=""Tool for playing (and augmenting) single samples or samples from Sample Databases (SDB files) ""\n        ""and DeepSpeech CSV files""\n    )\n    parser.add_argument(""source"", help=""Sample DB, CSV or WAV file to play samples from"")\n    parser.add_argument(\n        ""--start"",\n        type=int,\n        default=0,\n        help=""Sample index to start at (negative numbers are relative to the end of the collection)"",\n    )\n    parser.add_argument(\n        ""--number"",\n        type=int,\n        default=-1,\n        help=""Number of samples to play (-1 for endless)"",\n    )\n    parser.add_argument(\n        ""--random"",\n        action=""store_true"",\n        help=""If samples should be played in random order"",\n    )\n    parser.add_argument(\n        ""--augment"",\n        action=\'append\',\n        help=""Add an augmentation operation"",\n    )\n    parser.add_argument(\n        ""--clock"",\n        type=float,\n        default=0.5,\n        help=""Simulates clock value used for augmentations during training.""\n             ""Ranges from 0.0 (representing parameter start values) to""\n             ""1.0 (representing parameter end values)"",\n    )\n    parser.add_argument(\n        ""--pipe"",\n        action=""store_true"",\n        help=""Pipe first sample as wav file to stdout. Forces --number to 1."",\n    )\n    parser.add_argument(\n        ""--quiet"",\n        action=""store_true"",\n        help=""No info logging to console"",\n    )\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    CLI_ARGS = handle_args()\n    if not CLI_ARGS.pipe:\n        try:\n            import simpleaudio\n        except ModuleNotFoundError:\n            print(\'Unless using the --pipe flag, play.py requires Python package ""simpleaudio"" for playing samples\')\n            sys.exit(1)\n    try:\n        play_collection()\n    except KeyboardInterrupt:\n        print("" Stopped"")\n        sys.exit(0)\n'"
doc/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# DeepSpeech documentation build configuration file, created by\n# sphinx-quickstart on Thu Feb  2 21:20:39 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n\n# pylint: skip-file\n\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\'../\'))\n\nautodoc_mock_imports = [\'deepspeech\']\n\n# This is in fact only relevant on ReadTheDocs, but we want to run the same way\n# on our CI as in RTD to avoid regressions on RTD that we would not catch on\n# TaskCluster\nimport subprocess\nsubprocess.check_call(\'cd ../ && npm install typedoc@0.17.4 typescript@3.8.3 @types/node@13.9.x\', shell=True)\nsubprocess.check_call(\'cd ../ && doxygen doc/doxygen-c.conf\', shell=True)\nsubprocess.check_call(\'cd ../ && doxygen doc/doxygen-java.conf\', shell=True)\nsubprocess.check_call(\'cd ../ && doxygen doc/doxygen-dotnet.conf\', shell=True)\n\n# -- General configuration ------------------------------------------------\n\nimport semver\n\n# -- Project information -----------------------------------------------------\n\nproject = u\'DeepSpeech\'\ncopyright = \'2019-2020, Mozilla Corporation\'\nauthor = \'Mozilla Corporation\'\n\nwith open(\'../VERSION\', \'r\') as ver:\n    v = ver.read().strip()\nvv = semver.parse(v)\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n# The short X.Y version\nversion = \'{}.{}\'.format(vv[\'major\'], vv[\'minor\'])\n# The full version, including alpha/beta/rc tags\nrelease = v\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n  \'sphinx.ext.autodoc\',\n  \'sphinx.ext.extlinks\',\n  \'sphinx.ext.intersphinx\',\n  \'sphinx.ext.mathjax\',\n  \'sphinx.ext.viewcode\',\n  \'sphinx_rtd_theme\',\n  \'sphinx_js\',\n  \'breathe\'\n]\n\n\nbreathe_projects = {\n  ""deepspeech-c"": ""xml-c/"",\n  ""deepspeech-java"": ""xml-java/"",\n  ""deepspeech-dotnet"": ""xml-dotnet/"",\n}\n\njs_source_path = ""../native_client/javascript/index.ts""\njs_language = ""typescript""\njsdoc_config_path = ""../native_client/javascript/tsconfig.json""\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'.templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'.build\', \'Thumbs.db\', \'.DS_Store\', \'node_modules\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\nadd_module_names = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n  \'collapse_navigation\': False,\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'.static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'DeepSpeechdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'DeepSpeech.tex\', u\'DeepSpeech Documentation\',\n     u\'Mozilla Research\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'deepspeech\', u\'DeepSpeech Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'DeepSpeech\', u\'DeepSpeech Documentation\',\n     author, \'DeepSpeech\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\'https://docs.python.org/\': None}\n\nextlinks = {\'github\': (\'https://github.com/mozilla/DeepSpeech/blob/v{}/%s\'.format(release),\n                      \'%s\')}\n'"
tests/__init__.py,0,b''
tests/test_importers.py,0,"b'import unittest\n\nfrom argparse import Namespace\nfrom deepspeech_training.util.importers import validate_label_eng, get_validate_label\nfrom pathlib import Path\n\ndef from_here(path):\n    here = Path(__file__)\n    return here.parent / path\n\nclass TestValidateLabelEng(unittest.TestCase):\n    def test_numbers(self):\n        label = validate_label_eng(""this is a 1 2 3 test"")\n        self.assertEqual(label, None)\n\nclass TestGetValidateLabel(unittest.TestCase):\n\n    def test_no_validate_label_locale(self):\n        f = get_validate_label(Namespace())\n        self.assertEqual(f(\'toto\'), \'toto\')\n        self.assertEqual(f(\'toto1234\'), None)\n        self.assertEqual(f(\'toto1234[{[{[]\'), None)\n\n    def test_validate_label_locale_default(self):\n        f = get_validate_label(Namespace(validate_label_locale=None))\n        self.assertEqual(f(\'toto\'), \'toto\')\n        self.assertEqual(f(\'toto1234\'), None)\n        self.assertEqual(f(\'toto1234[{[{[]\'), None)\n\n    def test_get_validate_label_missing(self):\n        args = Namespace(validate_label_locale=from_here(\'test_data/validate_locale_ger.py\'))\n        f = get_validate_label(args)\n        self.assertEqual(f, None)\n\n    def test_get_validate_label(self):\n        args = Namespace(validate_label_locale=from_here(\'test_data/validate_locale_fra.py\'))\n        f = get_validate_label(args)\n        l = f(\'toto\')\n        self.assertEqual(l, \'toto\')\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_text.py,0,"b""import unittest\nimport os\n\nfrom deepspeech_training.util.text import Alphabet\n\nclass TestAlphabetParsing(unittest.TestCase):\n\n    def _ending_tester(self, file, expected):\n        alphabet = Alphabet(os.path.join(os.path.dirname(__file__), 'test_data', file))\n        label = ''\n        label_id = -1\n        for expected_label, expected_label_id in expected:\n            try:\n                label_id = alphabet.encode(expected_label)\n            except KeyError:\n                pass\n            self.assertEqual(label_id, [expected_label_id])\n            try:\n                label = alphabet.decode([expected_label_id])\n            except KeyError:\n                pass\n            self.assertEqual(label, expected_label)\n\n    def test_macos_ending(self):\n        self._ending_tester('alphabet_macos.txt', [('a', 0), ('b', 1), ('c', 2)])\n\n    def test_unix_ending(self):\n        self._ending_tester('alphabet_unix.txt', [('a', 0), ('b', 1), ('c', 2)])\n\n    def test_windows_ending(self):\n        self._ending_tester('alphabet_windows.txt', [('a', 0), ('b', 1), ('c', 2)])\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/test_value_range.py,0,"b""import unittest\n\nfrom deepspeech_training.util.helpers import ValueRange, get_value_range, pick_value_from_range\n\n\nclass TestValueRange(unittest.TestCase):\n\n    def _ending_tester(self, value, value_type, expected):\n        result = get_value_range(value, value_type)\n        self.assertEqual(result, expected)\n\n    def test_int_str_scalar(self):\n        self._ending_tester('1', int, ValueRange(1, 1, 0))\n\n    def test_int_str_scalar_radius(self):\n        self._ending_tester('1~3', int, ValueRange(1, 1, 3))\n\n    def test_int_str_range(self):\n        self._ending_tester('1:2', int, ValueRange(1, 2, 0))\n\n    def test_int_str_range_radius(self):\n        self._ending_tester('1:2~3', int, ValueRange(1, 2, 3))\n\n    def test_int_scalar(self):\n        self._ending_tester(1, int, ValueRange(1, 1, 0))\n\n    def test_int_2tuple(self):\n        self._ending_tester((1, 2), int, ValueRange(1, 2, 0))\n\n    def test_int_3tuple(self):\n        self._ending_tester((1, 2, 3), int, ValueRange(1, 2, 3))\n\n    def test_float_str_scalar(self):\n        self._ending_tester('1.0', float, ValueRange(1.0, 1.0, 0.0))\n\n    def test_float_str_scalar_radius(self):\n        self._ending_tester('1.0~3.0', float, ValueRange(1.0, 1.0, 3.0))\n\n    def test_float_str_range(self):\n        self._ending_tester('1.0:2.0', float, ValueRange(1.0, 2.0, 0.0))\n\n    def test_float_str_range_radius(self):\n        self._ending_tester('1.0:2.0~3.0', float, ValueRange(1.0, 2.0, 3.0))\n\n    def test_float_scalar(self):\n        self._ending_tester(1.0, float, ValueRange(1.0, 1.0, 0.0))\n\n    def test_float_2tuple(self):\n        self._ending_tester((1.0, 2.0), float, ValueRange(1.0, 2.0, 0.0))\n\n    def test_float_3tuple(self):\n        self._ending_tester((1.0, 2.0, 3.0), float, ValueRange(1.0, 2.0, 3.0))\n\n    def test_float_int_3tuple(self):\n        self._ending_tester((1, 2, 3), float, ValueRange(1.0, 2.0, 3.0))\n\n\nclass TestPickValueFromFixedRange(unittest.TestCase):\n\n    def _ending_tester(self, value_range, clock, expected):\n        is_int = isinstance(value_range.start, int)\n        result = pick_value_from_range(value_range, clock)\n        self.assertEqual(result, expected)\n        self.assertTrue(isinstance(result, int if is_int else float))\n\n    def test_int_0(self):\n        self._ending_tester(ValueRange(1, 3, 0), 0.0, 1)\n\n    def test_int_half(self):\n        self._ending_tester(ValueRange(1, 3, 0), 0.5, 2)\n\n    def test_int_1(self):\n        self._ending_tester(ValueRange(1, 3, 0), 1.0, 3)\n\n    def test_float_0(self):\n        self._ending_tester(ValueRange(1.0, 2.0, 0.0), 0.0, 1.0)\n\n    def test_float_half(self):\n        self._ending_tester(ValueRange(1.0, 2.0, 0.0), 0.5, 1.5)\n\n    def test_float_1(self):\n        self._ending_tester(ValueRange(1.0, 2.0, 0.0), 1.0, 2.0)\n\n\nclass TestPickValueFromRandomizedRange(unittest.TestCase):\n\n    def _ending_tester(self, value_range, clock, expected_min, expected_max):\n        is_int = isinstance(value_range.start, int)\n        results = list(map(lambda x: pick_value_from_range(value_range, clock), range(100)))\n        self.assertGreater(len(set(results)), 80)\n        self.assertTrue(all(map(lambda x: expected_min <= x <= expected_max, results)))\n        self.assertTrue(all(map(lambda x: isinstance(x, int if is_int else float), results)))\n\n    def test_int_0(self):\n        self._ending_tester(ValueRange(10000, 30000, 10000), 0.0, 0, 20000)\n\n    def test_int_half(self):\n        self._ending_tester(ValueRange(10000, 30000, 10000), 0.5, 10000, 30000)\n\n    def test_int_1(self):\n        self._ending_tester(ValueRange(10000, 30000, 10000), 1.0, 20000, 40000)\n\n    def test_float_0(self):\n        self._ending_tester(ValueRange(10000.0, 30000.0, 10000.0), 0.0, 0.0, 20000.0)\n\n    def test_float_half(self):\n        self._ending_tester(ValueRange(10000.0, 30000.0, 10000.0), 0.5, 10000.0, 30000.0)\n\n    def test_float_1(self):\n        self._ending_tester(ValueRange(10000.0, 30000.0, 10000.0), 1.0, 20000.0, 40000.0)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
util/taskcluster.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nif __name__ == '__main__':\n    try:\n        from deepspeech_training.util import taskcluster as dsu_taskcluster\n    except ImportError:\n        print('Training package is not installed. See training documentation.')\n        raise\n\n    dsu_taskcluster.main()\n"""
data/lm/generate_lm.py,0,"b'import argparse\nimport gzip\nimport io\nimport os\nimport subprocess\nfrom collections import Counter\n\nimport progressbar\n\n\ndef convert_and_filter_topk(args):\n    """""" Convert to lowercase, count word occurrences and save top-k words to a file """"""\n\n    counter = Counter()\n    data_lower = os.path.join(args.output_dir, ""lower.txt.gz"")\n\n    print(""\\nConverting to lowercase and counting word occurrences ..."")\n    with io.TextIOWrapper(\n        io.BufferedWriter(gzip.open(data_lower, ""w+"")), encoding=""utf-8""\n    ) as file_out:\n\n        # Open the input file either from input.txt or input.txt.gz\n        _, file_extension = os.path.splitext(args.input_txt)\n        if file_extension == "".gz"":\n            file_in = io.TextIOWrapper(\n                io.BufferedReader(gzip.open(args.input_txt)), encoding=""utf-8""\n            )\n        else:\n            file_in = open(args.input_txt, encoding=""utf-8"")\n\n        for line in progressbar.progressbar(file_in):\n            line_lower = line.lower()\n            counter.update(line_lower.split())\n            file_out.write(line_lower)\n\n        file_in.close()\n\n    # Save top-k words\n    print(""\\nSaving top {} words ..."".format(args.top_k))\n    top_counter = counter.most_common(args.top_k)\n    vocab_str = ""\\n"".join(word for word, count in top_counter)\n    vocab_path = ""vocab-{}.txt"".format(args.top_k)\n    vocab_path = os.path.join(args.output_dir, vocab_path)\n    with open(vocab_path, ""w+"") as file:\n        file.write(vocab_str)\n\n    print(""\\nCalculating word statistics ..."")\n    total_words = sum(counter.values())\n    print(""  Your text file has {} words in total"".format(total_words))\n    print(""  It has {} unique words"".format(len(counter)))\n    top_words_sum = sum(count for word, count in top_counter)\n    word_fraction = (top_words_sum / total_words) * 100\n    print(\n        ""  Your top-{} words are {:.4f} percent of all words"".format(\n            args.top_k, word_fraction\n        )\n    )\n    print(\'  Your most common word ""{}"" occurred {} times\'.format(*top_counter[0]))\n    last_word, last_count = top_counter[-1]\n    print(\n        \'  The least common word in your top-k is ""{}"" with {} times\'.format(\n            last_word, last_count\n        )\n    )\n    for i, (w, c) in enumerate(reversed(top_counter)):\n        if c > last_count:\n            print(\n                \'  The first word with {} occurrences is ""{}"" at place {}\'.format(\n                    c, w, len(top_counter) - 1 - i\n                )\n            )\n            break\n\n    return data_lower, vocab_str\n\n\ndef build_lm(args, data_lower, vocab_str):\n    print(""\\nCreating ARPA file ..."")\n    lm_path = os.path.join(args.output_dir, ""lm.arpa"")\n    subargs = [\n            os.path.join(args.kenlm_bins, ""lmplz""),\n            ""--order"",\n            str(args.arpa_order),\n            ""--temp_prefix"",\n            args.output_dir,\n            ""--memory"",\n            args.max_arpa_memory,\n            ""--text"",\n            data_lower,\n            ""--arpa"",\n            lm_path,\n            ""--prune"",\n            *args.arpa_prune.split(""|""),\n        ]\n    if args.discount_fallback:\n        subargs += [""--discount_fallback""]\n    subprocess.check_call(subargs)\n\n    # Filter LM using vocabulary of top-k words\n    print(""\\nFiltering ARPA file using vocabulary of top-k words ..."")\n    filtered_path = os.path.join(args.output_dir, ""lm_filtered.arpa"")\n    subprocess.run(\n        [\n            os.path.join(args.kenlm_bins, ""filter""),\n            ""single"",\n            ""model:{}"".format(lm_path),\n            filtered_path,\n        ],\n        input=vocab_str.encode(""utf-8""),\n        check=True,\n    )\n\n    # Quantize and produce trie binary.\n    print(""\\nBuilding lm.binary ..."")\n    binary_path = os.path.join(args.output_dir, ""lm.binary"")\n    subprocess.check_call(\n        [\n            os.path.join(args.kenlm_bins, ""build_binary""),\n            ""-a"",\n            str(args.binary_a_bits),\n            ""-q"",\n            str(args.binary_q_bits),\n            ""-v"",\n            args.binary_type,\n            filtered_path,\n            binary_path,\n        ]\n    )\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=""Generate lm.binary and top-k vocab for DeepSpeech.""\n    )\n    parser.add_argument(\n        ""--input_txt"",\n        help=""Path to a file.txt or file.txt.gz with sample sentences"",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        ""--output_dir"", help=""Directory path for the output"", type=str, required=True\n    )\n    parser.add_argument(\n        ""--top_k"",\n        help=""Use top_k most frequent words for the vocab.txt file. These will be used to filter the ARPA file."",\n        type=int,\n        required=True,\n    )\n    parser.add_argument(\n        ""--kenlm_bins"",\n        help=""File path to the KENLM binaries lmplz, filter and build_binary"",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        ""--arpa_order"",\n        help=""Order of k-grams in ARPA-file generation"",\n        type=int,\n        required=True,\n    )\n    parser.add_argument(\n        ""--max_arpa_memory"",\n        help=""Maximum allowed memory usage for ARPA-file generation"",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        ""--arpa_prune"",\n        help=""ARPA pruning parameters. Separate values with \'|\'"",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        ""--binary_a_bits"",\n        help=""Build binary quantization value a in bits"",\n        type=int,\n        required=True,\n    )\n    parser.add_argument(\n        ""--binary_q_bits"",\n        help=""Build binary quantization value q in bits"",\n        type=int,\n        required=True,\n    )\n    parser.add_argument(\n        ""--binary_type"",\n        help=""Build binary data structure type"",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        ""--discount_fallback"",\n        help=""To try when such message is returned by kenlm: \'Could not calculate Kneser-Ney discounts [...] rerun with --discount_fallback\'"",\n        action=""store_true"",\n    )\n\n    args = parser.parse_args()\n\n    data_lower, vocab_str = convert_and_filter_topk(args)\n    build_lm(args, data_lower, vocab_str)\n\n    # Delete intermediate files\n    os.remove(os.path.join(args.output_dir, ""lower.txt.gz""))\n    os.remove(os.path.join(args.output_dir, ""lm.arpa""))\n    os.remove(os.path.join(args.output_dir, ""lm_filtered.arpa""))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
data/lm/generate_package.py,0,"b'#!/usr/bin/env python\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport shutil\nimport sys\n\nimport ds_ctcdecoder\nfrom deepspeech_training.util.text import Alphabet, UTF8Alphabet\nfrom ds_ctcdecoder import Scorer, Alphabet as NativeAlphabet\n\n\ndef create_bundle(\n    alphabet_path,\n    lm_path,\n    vocab_path,\n    package_path,\n    force_utf8,\n    default_alpha,\n    default_beta,\n):\n    words = set()\n    vocab_looks_char_based = True\n    with open(vocab_path) as fin:\n        for line in fin:\n            for word in line.split():\n                words.add(word.encode(""utf-8""))\n                if len(word) > 1:\n                    vocab_looks_char_based = False\n    print(""{} unique words read from vocabulary file."".format(len(words)))\n\n    cbm = ""Looks"" if vocab_looks_char_based else ""Doesn\'t look""\n    print(""{} like a character based model."".format(cbm))\n\n    if force_utf8 != None:  # pylint: disable=singleton-comparison\n        use_utf8 = force_utf8.value\n    else:\n        use_utf8 = vocab_looks_char_based\n        print(""Using detected UTF-8 mode: {}"".format(use_utf8))\n\n    if use_utf8:\n        serialized_alphabet = UTF8Alphabet().serialize()\n    else:\n        if not alphabet_path:\n            raise RuntimeError(""No --alphabet path specified, can\'t continue."")\n        serialized_alphabet = Alphabet(alphabet_path).serialize()\n\n    alphabet = NativeAlphabet()\n    err = alphabet.deserialize(serialized_alphabet, len(serialized_alphabet))\n    if err != 0:\n        raise RuntimeError(""Error loading alphabet: {}"".format(err))\n\n    scorer = Scorer()\n    scorer.set_alphabet(alphabet)\n    scorer.set_utf8_mode(use_utf8)\n    scorer.reset_params(default_alpha, default_beta)\n    err = scorer.load_lm(lm_path)\n    if err != ds_ctcdecoder.DS_ERR_SCORER_NO_TRIE:\n        print(\'Error loading language model file: 0x{:X}.\'.format(err))\n        print(\'See the error codes section in https://deepspeech.readthedocs.io for a description.\')\n        sys.exit(1)\n    scorer.fill_dictionary(list(words))\n    shutil.copy(lm_path, package_path)\n    scorer.save_dictionary(package_path, True)  # append, not overwrite\n    print(""Package created in {}"".format(package_path))\n\n\nclass Tristate(object):\n    def __init__(self, value=None):\n        if any(value is v for v in (True, False, None)):\n            self.value = value\n        else:\n            raise ValueError(""Tristate value must be True, False, or None"")\n\n    def __eq__(self, other):\n        return (\n            self.value is other.value\n            if isinstance(other, Tristate)\n            else self.value is other\n        )\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __bool__(self):\n        raise TypeError(""Tristate object may not be used as a Boolean"")\n\n    def __str__(self):\n        return str(self.value)\n\n    def __repr__(self):\n        return ""Tristate(%s)"" % self.value\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=""Generate an external scorer package for DeepSpeech.""\n    )\n    parser.add_argument(\n        ""--alphabet"",\n        help=""Path of alphabet file to use for vocabulary construction. Words with characters not in the alphabet will not be included in the vocabulary. Optional if using UTF-8 mode."",\n    )\n    parser.add_argument(\n        ""--lm"",\n        required=True,\n        help=""Path of KenLM binary LM file. Must be built without including the vocabulary (use the -v flag). See generate_lm.py for how to create a binary LM."",\n    )\n    parser.add_argument(\n        ""--vocab"",\n        required=True,\n        help=""Path of vocabulary file. Must contain words separated by whitespace."",\n    )\n    parser.add_argument(""--package"", required=True, help=""Path to save scorer package."")\n    parser.add_argument(\n        ""--default_alpha"",\n        type=float,\n        required=True,\n        help=""Default value of alpha hyperparameter."",\n    )\n    parser.add_argument(\n        ""--default_beta"",\n        type=float,\n        required=True,\n        help=""Default value of beta hyperparameter."",\n    )\n    parser.add_argument(\n        ""--force_utf8"",\n        type=str,\n        default="""",\n        help=""Boolean flag, force set or unset UTF-8 mode in the scorer package. If not set, infers from the vocabulary. See <https://github.com/mozilla/DeepSpeech/blob/master/doc/Decoder.rst#utf-8-mode> for further explanation"",\n    )\n    args = parser.parse_args()\n\n    if args.force_utf8 in (""True"", ""1"", ""true"", ""yes"", ""y""):\n        force_utf8 = Tristate(True)\n    elif args.force_utf8 in (""False"", ""0"", ""false"", ""no"", ""n""):\n        force_utf8 = Tristate(False)\n    else:\n        force_utf8 = Tristate(None)\n\n    create_bundle(\n        args.alphabet,\n        args.lm,\n        args.vocab,\n        args.package,\n        force_utf8,\n        args.default_alpha,\n        args.default_beta,\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
native_client/ctcdecode/__init__.py,0,"b'from __future__ import absolute_import, division, print_function\n\nfrom . import swigwrapper # pylint: disable=import-self\nfrom .swigwrapper import Alphabet\n\n__version__ = swigwrapper.__version__\n\n# Hack: import error codes by matching on their names, as SWIG unfortunately\n# does not support binding enums to Python in a scoped manner yet.\nfor symbol in dir(swigwrapper):\n    if symbol.startswith(\'DS_ERR_\'):\n        globals()[symbol] = getattr(swigwrapper, symbol)\n\nclass Scorer(swigwrapper.Scorer):\n    """"""Wrapper for Scorer.\n\n    :param alpha: Language model weight.\n    :type alpha: float\n    :param beta: Word insertion bonus.\n    :type beta: float\n    :scorer_path: Path to load scorer from.\n    :alphabet: Alphabet\n    :type scorer_path: basestring\n    """"""\n    def __init__(self, alpha=None, beta=None, scorer_path=None, alphabet=None):\n        super(Scorer, self).__init__()\n        # Allow bare initialization\n        if alphabet:\n            assert alpha is not None, \'alpha parameter is required\'\n            assert beta is not None, \'beta parameter is required\'\n            assert scorer_path, \'scorer_path parameter is required\'\n\n            serialized = alphabet.serialize()\n            native_alphabet = swigwrapper.Alphabet()\n            err = native_alphabet.deserialize(serialized, len(serialized))\n            if err != 0:\n                raise ValueError(\'Error when deserializing alphabet.\')\n\n            err = self.init(scorer_path.encode(\'utf-8\'),\n                            native_alphabet)\n            if err != 0:\n                raise ValueError(\'Scorer initialization failed with error code {}\'.format(err))\n\n            self.reset_params(alpha, beta)\n\n    def load_lm(self, lm_path):\n        return super(Scorer, self).load_lm(lm_path.encode(\'utf-8\'))\n\n    def save_dictionary(self, save_path, *args, **kwargs):\n        super(Scorer, self).save_dictionary(save_path.encode(\'utf-8\'), *args, **kwargs)\n\n\ndef ctc_beam_search_decoder(probs_seq,\n                            alphabet,\n                            beam_size,\n                            cutoff_prob=1.0,\n                            cutoff_top_n=40,\n                            scorer=None):\n    """"""Wrapper for the CTC Beam Search Decoder.\n\n    :param probs_seq: 2-D list of probability distributions over each time\n                      step, with each element being a list of normalized\n                      probabilities over alphabet and blank.\n    :type probs_seq: 2-D list\n    :param alphabet: Alphabet\n    :param beam_size: Width for beam search.\n    :type beam_size: int\n    :param cutoff_prob: Cutoff probability in pruning,\n                        default 1.0, no pruning.\n    :type cutoff_prob: float\n    :param cutoff_top_n: Cutoff number in pruning, only top cutoff_top_n\n                         characters with highest probs in alphabet will be\n                         used in beam search, default 40.\n    :type cutoff_top_n: int\n    :param scorer: External scorer for partially decoded sentence, e.g. word\n                   count or language model.\n    :type scorer: Scorer\n    :return: List of tuples of confidence and sentence as decoding\n             results, in descending order of the confidence.\n    :rtype: list\n    """"""\n    serialized = alphabet.serialize()\n    native_alphabet = swigwrapper.Alphabet()\n    err = native_alphabet.deserialize(serialized, len(serialized))\n    if err != 0:\n        raise ValueError(""Error when deserializing alphabet."")\n    beam_results = swigwrapper.ctc_beam_search_decoder(\n        probs_seq, native_alphabet, beam_size, cutoff_prob, cutoff_top_n,\n        scorer)\n    beam_results = [(res.confidence, alphabet.decode(res.tokens)) for res in beam_results]\n    return beam_results\n\n\ndef ctc_beam_search_decoder_batch(probs_seq,\n                                  seq_lengths,\n                                  alphabet,\n                                  beam_size,\n                                  num_processes,\n                                  cutoff_prob=1.0,\n                                  cutoff_top_n=40,\n                                  scorer=None):\n    """"""Wrapper for the batched CTC beam search decoder.\n\n    :param probs_seq: 3-D list with each element as an instance of 2-D list\n                      of probabilities used by ctc_beam_search_decoder().\n    :type probs_seq: 3-D list\n    :param alphabet: alphabet list.\n    :alphabet: Alphabet\n    :param beam_size: Width for beam search.\n    :type beam_size: int\n    :param num_processes: Number of parallel processes.\n    :type num_processes: int\n    :param cutoff_prob: Cutoff probability in alphabet pruning,\n                        default 1.0, no pruning.\n    :type cutoff_prob: float\n    :param cutoff_top_n: Cutoff number in pruning, only top cutoff_top_n\n                         characters with highest probs in alphabet will be\n                         used in beam search, default 40.\n    :type cutoff_top_n: int\n    :param num_processes: Number of parallel processes.\n    :type num_processes: int\n    :param scorer: External scorer for partially decoded sentence, e.g. word\n                   count or language model.\n    :type scorer: Scorer\n    :return: List of tuples of confidence and sentence as decoding\n             results, in descending order of the confidence.\n    :rtype: list\n    """"""\n    serialized = alphabet.serialize()\n    native_alphabet = swigwrapper.Alphabet()\n    err = native_alphabet.deserialize(serialized, len(serialized))\n    if err != 0:\n        raise ValueError(""Error when deserializing alphabet."")\n    batch_beam_results = swigwrapper.ctc_beam_search_decoder_batch(probs_seq, seq_lengths, native_alphabet, beam_size, num_processes, cutoff_prob, cutoff_top_n, scorer)\n    batch_beam_results = [\n        [(res.confidence, alphabet.decode(res.tokens)) for res in beam_results]\n        for beam_results in batch_beam_results\n    ]\n    return batch_beam_results\n'"
native_client/ctcdecode/build_archive.py,0,"b'#!/usr/bin/env python\nfrom __future__ import absolute_import, division, print_function\n\nimport glob\nimport os\nimport shlex\nimport subprocess\nimport sys\n\nfrom multiprocessing.dummy import Pool\n\nif sys.platform.startswith(\'win\'):\n    ARGS = [\'/nologo\', \'/D KENLM_MAX_ORDER=6\', \'/EHsc\', \'/source-charset:utf-8\']\n    OPT_ARGS = [\'/O2\', \'/MT\', \'/D NDEBUG\']\n    DBG_ARGS = [\'/Od\', \'/MTd\', \'/Zi\', \'/U NDEBUG\', \'/D DEBUG\']\n    OPENFST_DIR = \'third_party/openfst-1.6.9-win\'\nelse:\n    ARGS = [\'-fPIC\', \'-DKENLM_MAX_ORDER=6\', \'-std=c++11\', \'-Wno-unused-local-typedefs\', \'-Wno-sign-compare\']\n    OPT_ARGS = [\'-O3\', \'-DNDEBUG\']\n    DBG_ARGS = [\'-O0\', \'-g\', \'-UNDEBUG\', \'-DDEBUG\']\n    OPENFST_DIR = \'third_party/openfst-1.6.7\'\n\n\n\nINCLUDES = [\n    \'..\',\n    \'../kenlm\',\n    OPENFST_DIR + \'/src/include\',\n    \'third_party/ThreadPool\'\n]\n\nKENLM_FILES = (glob.glob(\'../kenlm/util/*.cc\')\n                + glob.glob(\'../kenlm/lm/*.cc\')\n                + glob.glob(\'../kenlm/util/double-conversion/*.cc\'))\n\nKENLM_FILES += glob.glob(OPENFST_DIR + \'/src/lib/*.cc\')\n\nKENLM_FILES = [\n    fn for fn in KENLM_FILES\n    if not (fn.endswith(\'main.cc\') or fn.endswith(\'test.cc\') or fn.endswith(\n        \'unittest.cc\'))\n]\n\nCTC_DECODER_FILES = [\n    \'ctc_beam_search_decoder.cpp\',\n    \'scorer.cpp\',\n    \'path_trie.cpp\',\n    \'decoder_utils.cpp\',\n    \'workspace_status.cc\'\n]\n\ndef build_archive(srcs=[], out_name=\'\', build_dir=\'temp_build/temp_build\', debug=False, num_parallel=1):\n    compiler = os.environ.get(\'CXX\', \'g++\')\n    if sys.platform.startswith(\'win\'):\n        compiler = \'""{}""\'.format(compiler)\n    ar = os.environ.get(\'AR\', \'ar\')\n    libexe = os.environ.get(\'LIBEXE\', \'lib.exe\')\n    libtool = os.environ.get(\'LIBTOOL\', \'libtool\')\n    cflags = os.environ.get(\'CFLAGS\', \'\') + os.environ.get(\'CXXFLAGS\', \'\')\n    args = ARGS + (DBG_ARGS if debug else OPT_ARGS)\n\n    for file in srcs:\n        outfile = os.path.join(build_dir, os.path.splitext(file)[0] + \'.o\')\n        outdir = os.path.dirname(outfile)\n        if not os.path.exists(outdir):\n            print(\'mkdir\', outdir)\n            os.makedirs(outdir)\n\n    def build_one(file):\n        outfile = os.path.join(build_dir, os.path.splitext(file)[0] + \'.o\')\n        if os.path.exists(outfile):\n            return\n\n        if sys.platform.startswith(\'win\'):\n            file = \'""{}""\'.format(file.replace(\'\\\\\', \'/\'))\n            output = \'/Fo""{}""\'.format(outfile.replace(\'\\\\\', \'/\'))\n        else:\n            output = \'-o \' + outfile\n\n        cmd = \'{cc} -c {cflags} {args} {includes} {infile} {output}\'.format(\n            cc=compiler,\n            cflags=cflags,\n            args=\' \'.join(args),\n            includes=\' \'.join(\'-I\' + i for i in INCLUDES),\n            infile=file,\n            output=output,\n        )\n        print(cmd)\n        subprocess.check_call(shlex.split(cmd))\n        return outfile\n\n    pool = Pool(num_parallel)\n    obj_files = list(pool.imap_unordered(build_one, srcs))\n\n    if sys.platform.startswith(\'darwin\'):\n        cmd = \'{libtool} -static -o {outfile} {infiles}\'.format(\n            libtool=libtool,\n            outfile=out_name,\n            infiles=\' \'.join(obj_files),\n        )\n        print(cmd)\n        subprocess.check_call(shlex.split(cmd))\n    elif sys.platform.startswith(\'win\'):\n        cmd = \'""{libexe}"" /OUT:""{outfile}"" {infiles} /MACHINE:X64 /NOLOGO\'.format(\n            libexe=libexe,\n            outfile=out_name,\n            infiles=\' \'.join(obj_files))\n        cmd = cmd.replace(\'\\\\\', \'/\')\n        print(cmd)\n        subprocess.check_call(shlex.split(cmd))\n    else:\n        cmd = \'{ar} rcs {outfile} {infiles}\'.format(\n            ar=ar,\n            outfile=out_name,\n            infiles=\' \'.join(obj_files)\n        )\n        print(cmd)\n        subprocess.check_call(shlex.split(cmd))\n\nif __name__ == \'__main__\':\n    build_common()\n'"
native_client/ctcdecode/setup.py,0,"b'#!/usr/bin/env python\nfrom __future__ import absolute_import, division, print_function\n\nfrom distutils.command.build import build\nfrom setuptools import setup, Extension, distutils\n\nimport argparse\nimport multiprocessing.pool\nimport os\nimport platform\nimport sys\n\nfrom build_archive import *\n\ntry:\n    import numpy\n    try:\n        numpy_include = numpy.get_include()\n    except AttributeError:\n        numpy_include = numpy.get_numpy_include()\nexcept ImportError:\n    numpy_include = \'\'\n    assert \'NUMPY_INCLUDE\' in os.environ\n\nnumpy_include = os.getenv(\'NUMPY_INCLUDE\', numpy_include)\nnumpy_min_ver = os.getenv(\'NUMPY_DEP_VERSION\', \'\')\n\nparser = argparse.ArgumentParser(description=__doc__)\nparser.add_argument(\n    ""--num_processes"",\n    default=1,\n    type=int,\n    help=""Number of cpu processes to build package. (default: %(default)d)"")\nknown_args, unknown_args = parser.parse_known_args()\ndebug = \'--debug\' in unknown_args\n\n# reconstruct sys.argv to pass to setup below\nsys.argv = [sys.argv[0]] + unknown_args\n\ndef read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname)).read()\n\ndef maybe_rebuild(srcs, out_name, build_dir):\n    if not os.path.exists(out_name):\n        if not os.path.exists(build_dir):\n            os.makedirs(build_dir)\n\n        build_archive(srcs=srcs,\n                     out_name=out_name,\n                     build_dir=build_dir,\n                     num_parallel=known_args.num_processes,\n                     debug=debug)\n\nproject_version = read(\'../../training/deepspeech_training/VERSION\').strip()\n\nbuild_dir = \'temp_build/temp_build\'\n\nif sys.platform.startswith(\'win\'):\n    archive_ext = \'lib\'\nelse:\n    archive_ext = \'a\'\n\nthird_party_build = \'third_party.{}\'.format(archive_ext)\nctc_decoder_build = \'first_party.{}\'.format(archive_ext)\n\n\nmaybe_rebuild(KENLM_FILES, third_party_build, build_dir)\nmaybe_rebuild(CTC_DECODER_FILES, ctc_decoder_build, build_dir)\n\ndecoder_module = Extension(\n    name=\'ds_ctcdecoder._swigwrapper\',\n    sources=[\'swigwrapper.i\'],\n    swig_opts=[\'-c++\', \'-extranative\'],\n    language=\'c++\',\n    include_dirs=INCLUDES + [numpy_include],\n    extra_compile_args=ARGS + (DBG_ARGS if debug else OPT_ARGS),\n    extra_link_args=[ctc_decoder_build, third_party_build],\n)\n\nclass BuildExtFirst(build):\n    sub_commands = [(\'build_ext\', build.has_ext_modules),\n                    (\'build_py\', build.has_pure_modules),\n                    (\'build_clib\', build.has_c_libraries),\n                    (\'build_scripts\', build.has_scripts)]\n\nsetup(\n    name=\'ds_ctcdecoder\',\n    version=project_version,\n    description=""""""DS CTC decoder"""""",\n    cmdclass = {\'build\': BuildExtFirst},\n    ext_modules=[decoder_module],\n    package_dir = {\'ds_ctcdecoder\': \'.\'},\n    py_modules=[\'ds_ctcdecoder\', \'ds_ctcdecoder.swigwrapper\'],\n    install_requires = [\'numpy%s\' % numpy_min_ver],\n)\n'"
native_client/kenlm/setup.py,0,"b'from setuptools import setup, Extension\nimport glob\nimport platform\nimport os\nimport sys\nimport re\n\n#Does gcc compile with this header and library?\ndef compile_test(header, library):\n    dummy_path = os.path.join(os.path.dirname(__file__), ""dummy"")\n    command = ""bash -c \\""g++ -include "" + header + "" -l"" + library + "" -x c++ - <<<\'int main() {}\' -o "" + dummy_path + "" >/dev/null 2>/dev/null && rm "" + dummy_path + "" 2>/dev/null\\""""\n    return os.system(command) == 0\n\nmax_order = ""6""\nis_max_order = [s for s in sys.argv if ""--max_order"" in s]\nfor element in is_max_order:\n    max_order = re.split(\'[= ]\',element)[1]\n    sys.argv.remove(element)\n\nFILES = glob.glob(\'util/*.cc\') + glob.glob(\'lm/*.cc\') + glob.glob(\'util/double-conversion/*.cc\') + glob.glob(\'python/*.cc\')\nFILES = [fn for fn in FILES if not (fn.endswith(\'main.cc\') or fn.endswith(\'test.cc\'))]\n\nif platform.system() == \'Linux\':\n    LIBS = [\'stdc++\', \'rt\']\nelif platform.system() == \'Darwin\':\n    LIBS = [\'c++\']\nelse:\n    LIBS = []\n\n#We don\'t need -std=c++11 but python seems to be compiled with it now.  https://github.com/kpu/kenlm/issues/86\nARGS = [\'-O3\', \'-DNDEBUG\', \'-DKENLM_MAX_ORDER=\'+max_order, \'-std=c++11\']\n\n#Attempted fix to https://github.com/kpu/kenlm/issues/186 and https://github.com/kpu/kenlm/issues/197\nif platform.system() == \'Darwin\':\n    ARGS += [""-stdlib=libc++"", ""-mmacosx-version-min=10.7""]\n\nif compile_test(\'zlib.h\', \'z\'):\n    ARGS.append(\'-DHAVE_ZLIB\')\n    LIBS.append(\'z\')\n\nif compile_test(\'bzlib.h\', \'bz2\'):\n    ARGS.append(\'-DHAVE_BZLIB\')\n    LIBS.append(\'bz2\')\n\nif compile_test(\'lzma.h\', \'lzma\'):\n    ARGS.append(\'-DHAVE_XZLIB\')\n    LIBS.append(\'lzma\')\n\next_modules = [\n    Extension(name=\'kenlm\',\n        sources=FILES + [\'python/kenlm.cpp\'],\n        language=\'C++\', \n        include_dirs=[\'.\'],\n        libraries=LIBS, \n        extra_compile_args=ARGS)\n]\n\nsetup(\n    name=\'kenlm\',\n    ext_modules=ext_modules,\n    include_package_data=True,\n)\n'"
native_client/python/__init__.py,0,"b'import os\nimport platform\n\n#The API is not snake case which triggers linter errors\n#pylint: disable=invalid-name\n\nif platform.system().lower() == ""windows"":\n    dslib_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \'lib\')\n\n    # On Windows, we can\'t rely on RPATH being set to $ORIGIN/lib/ or on\n    # @loader_path/lib\n    if hasattr(os, \'add_dll_directory\'):\n        # Starting with Python 3.8 this properly handles the problem\n        os.add_dll_directory(dslib_path)\n    else:\n        # Before Pythin 3.8 we need to change the PATH to include the proper\n        # directory for the dynamic linker\n        os.environ[\'PATH\'] = dslib_path + \';\' + os.environ[\'PATH\']\n\nimport deepspeech\n\n# rename for backwards compatibility\nfrom deepspeech.impl import Version as version\n\nclass Model(object):\n    """"""\n    Class holding a DeepSpeech model\n\n    :param aModelPath: Path to model file to load\n    :type aModelPath: str\n    """"""\n    def __init__(self, model_path):\n        # make sure the attribute is there if CreateModel fails\n        self._impl = None\n\n        status, impl = deepspeech.impl.CreateModel(model_path)\n        if status != 0:\n            raise RuntimeError(""CreateModel failed with \'{}\' (0x{:X})"".format(deepspeech.impl.ErrorCodeToErrorMessage(status),status))\n        self._impl = impl\n\n    def __del__(self):\n        if self._impl:\n            deepspeech.impl.FreeModel(self._impl)\n            self._impl = None\n\n    def beamWidth(self):\n        """"""\n        Get beam width value used by the model. If setModelBeamWidth was not\n        called before, will return the default value loaded from the model file.\n\n        :return: Beam width value used by the model.\n        :type: int\n        """"""\n        return deepspeech.impl.GetModelBeamWidth(self._impl)\n\n    def setBeamWidth(self, beam_width):\n        """"""\n        Set beam width value used by the model.\n\n        :param beam_width: The beam width used by the model. A larger beam width value generates better results at the cost of decoding time.\n        :type beam_width: int\n\n        :return: Zero on success, non-zero on failure.\n        :type: int\n        """"""\n        return deepspeech.impl.SetModelBeamWidth(self._impl, beam_width)\n\n    def sampleRate(self):\n        """"""\n        Return the sample rate expected by the model.\n\n        :return: Sample rate.\n        :type: int\n        """"""\n        return deepspeech.impl.GetModelSampleRate(self._impl)\n\n    def enableExternalScorer(self, scorer_path):\n        """"""\n        Enable decoding using an external scorer.\n\n        :param scorer_path: The path to the external scorer file.\n        :type scorer_path: str\n\n        :throws: RuntimeError on error\n        """"""\n        status = deepspeech.impl.EnableExternalScorer(self._impl, scorer_path)\n        if status != 0:\n            raise RuntimeError(""EnableExternalScorer failed with \'{}\' (0x{:X})"".format(deepspeech.impl.ErrorCodeToErrorMessage(status),status))\n\n    def disableExternalScorer(self):\n        """"""\n        Disable decoding using an external scorer.\n\n        :return: Zero on success, non-zero on failure.\n        """"""\n        return deepspeech.impl.DisableExternalScorer(self._impl)\n\n    def setScorerAlphaBeta(self, alpha, beta):\n        """"""\n        Set hyperparameters alpha and beta of the external scorer.\n\n        :param alpha: The alpha hyperparameter of the decoder. Language model weight.\n        :type alpha: float\n\n        :param beta: The beta hyperparameter of the decoder. Word insertion weight.\n        :type beta: float\n\n        :return: Zero on success, non-zero on failure.\n        :type: int\n        """"""\n        return deepspeech.impl.SetScorerAlphaBeta(self._impl, alpha, beta)\n\n    def stt(self, audio_buffer):\n        """"""\n        Use the DeepSpeech model to perform Speech-To-Text.\n\n        :param audio_buffer: A 16-bit, mono raw audio signal at the appropriate sample rate (matching what the model was trained on).\n        :type audio_buffer: numpy.int16 array\n\n        :return: The STT result.\n        :type: str\n        """"""\n        return deepspeech.impl.SpeechToText(self._impl, audio_buffer)\n\n    def sttWithMetadata(self, audio_buffer, num_results=1):\n        """"""\n        Use the DeepSpeech model to perform Speech-To-Text and return results including metadata.\n\n        :param audio_buffer: A 16-bit, mono raw audio signal at the appropriate sample rate (matching what the model was trained on).\n        :type audio_buffer: numpy.int16 array\n\n        :param num_results: Maximum number of candidate transcripts to return. Returned list might be smaller than this.\n        :type num_results: int\n\n        :return: Metadata object containing multiple candidate transcripts. Each transcript has per-token metadata including timing information.\n        :type: :func:`Metadata`\n        """"""\n        return deepspeech.impl.SpeechToTextWithMetadata(self._impl, audio_buffer, num_results)\n\n    def createStream(self):\n        """"""\n        Create a new streaming inference state. The streaming state returned by\n        this function can then be passed to :func:`feedAudioContent()` and :func:`finishStream()`.\n\n        :return: Stream object representing the newly created stream\n        :type: :func:`Stream`\n\n        :throws: RuntimeError on error\n        """"""\n        status, ctx = deepspeech.impl.CreateStream(self._impl)\n        if status != 0:\n            raise RuntimeError(""CreateStream failed with \'{}\' (0x{:X})"".format(deepspeech.impl.ErrorCodeToErrorMessage(status),status))\n        return Stream(ctx)\n\n\nclass Stream(object):\n    """"""\n    Class wrapping a DeepSpeech stream. The constructor cannot be called directly.\n    Use :func:`Model.createStream()`\n    """"""\n    def __init__(self, native_stream):\n        self._impl = native_stream\n\n    def __del__(self):\n        if self._impl:\n            self.freeStream()\n\n    def feedAudioContent(self, audio_buffer):\n        """"""\n        Feed audio samples to an ongoing streaming inference.\n\n        :param audio_buffer: A 16-bit, mono raw audio signal at the appropriate sample rate (matching what the model was trained on).\n        :type audio_buffer: numpy.int16 array\n\n        :throws: RuntimeError if the stream object is not valid\n        """"""\n        if not self._impl:\n            raise RuntimeError(""Stream object is not valid. Trying to feed an already finished stream?"")\n        deepspeech.impl.FeedAudioContent(self._impl, audio_buffer)\n\n    def intermediateDecode(self):\n        """"""\n        Compute the intermediate decoding of an ongoing streaming inference.\n\n        :return: The STT intermediate result.\n        :type: str\n\n        :throws: RuntimeError if the stream object is not valid\n        """"""\n        if not self._impl:\n            raise RuntimeError(""Stream object is not valid. Trying to decode an already finished stream?"")\n        return deepspeech.impl.IntermediateDecode(self._impl)\n\n    def intermediateDecodeWithMetadata(self, num_results=1):\n        """"""\n        Compute the intermediate decoding of an ongoing streaming inference and return results including metadata.\n\n        :param num_results: Maximum number of candidate transcripts to return. Returned list might be smaller than this.\n        :type num_results: int\n\n        :return: Metadata object containing multiple candidate transcripts. Each transcript has per-token metadata including timing information.\n        :type: :func:`Metadata`\n\n        :throws: RuntimeError if the stream object is not valid\n        """"""\n        if not self._impl:\n            raise RuntimeError(""Stream object is not valid. Trying to decode an already finished stream?"")\n        return deepspeech.impl.IntermediateDecodeWithMetadata(self._impl, num_results)\n\n    def finishStream(self):\n        """"""\n        Compute the final decoding of an ongoing streaming inference and return\n        the result. Signals the end of an ongoing streaming inference. The underlying\n        stream object must not be used after this method is called.\n\n        :return: The STT result.\n        :type: str\n\n        :throws: RuntimeError if the stream object is not valid\n        """"""\n        if not self._impl:\n            raise RuntimeError(""Stream object is not valid. Trying to finish an already finished stream?"")\n        result = deepspeech.impl.FinishStream(self._impl)\n        self._impl = None\n        return result\n\n    def finishStreamWithMetadata(self, num_results=1):\n        """"""\n        Compute the final decoding of an ongoing streaming inference and return\n        results including metadata. Signals the end of an ongoing streaming\n        inference. The underlying stream object must not be used after this\n        method is called.\n\n        :param num_results: Maximum number of candidate transcripts to return. Returned list might be smaller than this.\n        :type num_results: int\n\n        :return: Metadata object containing multiple candidate transcripts. Each transcript has per-token metadata including timing information.\n        :type: :func:`Metadata`\n\n        :throws: RuntimeError if the stream object is not valid\n        """"""\n        if not self._impl:\n            raise RuntimeError(""Stream object is not valid. Trying to finish an already finished stream?"")\n        result = deepspeech.impl.FinishStreamWithMetadata(self._impl, num_results)\n        self._impl = None\n        return result\n\n    def freeStream(self):\n        """"""\n        Destroy a streaming state without decoding the computed logits. This can\n        be used if you no longer need the result of an ongoing streaming inference.\n\n        :throws: RuntimeError if the stream object is not valid\n        """"""\n        if not self._impl:\n            raise RuntimeError(""Stream object is not valid. Trying to free an already finished stream?"")\n        deepspeech.impl.FreeStream(self._impl)\n        self._impl = None\n\n\n# This is only for documentation purpose\n# Metadata, CandidateTranscript and TokenMetadata should be in sync with native_client/deepspeech.h\nclass TokenMetadata(object):\n    """"""\n    Stores each individual character, along with its timing information\n    """"""\n\n    def text(self):\n        """"""\n        The text for this token\n        """"""\n\n\n    def timestep(self):\n        """"""\n        Position of the token in units of 20ms\n        """"""\n\n\n    def start_time(self):\n        """"""\n        Position of the token in seconds\n        """"""\n\n\nclass CandidateTranscript(object):\n    """"""\n    Stores the entire CTC output as an array of character metadata objects\n    """"""\n    def tokens(self):\n        """"""\n        List of tokens\n\n        :return: A list of :func:`TokenMetadata` elements\n        :type: list\n        """"""\n\n\n    def confidence(self):\n        """"""\n        Approximated confidence value for this transcription. This is roughly the\n        sum of the acoustic model logit values for each timestep/character that\n        contributed to the creation of this transcription.\n        """"""\n\n\nclass Metadata(object):\n    def transcripts(self):\n        """"""\n        List of candidate transcripts\n\n        :return: A list of :func:`CandidateTranscript` objects\n        :type: list\n        """"""\n'"
native_client/python/client.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport numpy as np\nimport shlex\nimport subprocess\nimport sys\nimport wave\nimport json\n\nfrom deepspeech import Model, version\nfrom timeit import default_timer as timer\n\ntry:\n    from shhlex import quote\nexcept ImportError:\n    from pipes import quote\n\n\ndef convert_samplerate(audio_path, desired_sample_rate):\n    sox_cmd = \'sox {} --type raw --bits 16 --channels 1 --rate {} --encoding signed-integer --endian little --compression 0.0 --no-dither - \'.format(quote(audio_path), desired_sample_rate)\n    try:\n        output = subprocess.check_output(shlex.split(sox_cmd), stderr=subprocess.PIPE)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(\'SoX returned non-zero status: {}\'.format(e.stderr))\n    except OSError as e:\n        raise OSError(e.errno, \'SoX not found, use {}hz files or install it: {}\'.format(desired_sample_rate, e.strerror))\n\n    return desired_sample_rate, np.frombuffer(output, np.int16)\n\n\ndef metadata_to_string(metadata):\n    return \'\'.join(token.text for token in metadata.tokens)\n\n\ndef words_from_candidate_transcript(metadata):\n    word = """"\n    word_list = []\n    word_start_time = 0\n    # Loop through each character\n    for i, token in enumerate(metadata.tokens):\n        # Append character to word if it\'s not a space\n        if token.text != "" "":\n            if len(word) == 0:\n                # Log the start time of the new word\n                word_start_time = token.start_time\n\n            word = word + token.text\n        # Word boundary is either a space or the last character in the array\n        if token.text == "" "" or i == len(metadata.tokens) - 1:\n            word_duration = token.start_time - word_start_time\n\n            if word_duration < 0:\n                word_duration = 0\n\n            each_word = dict()\n            each_word[""word""] = word\n            each_word[""start_time ""] = round(word_start_time, 4)\n            each_word[""duration""] = round(word_duration, 4)\n\n            word_list.append(each_word)\n            # Reset\n            word = """"\n            word_start_time = 0\n\n    return word_list\n\n\ndef metadata_json_output(metadata):\n    json_result = dict()\n    json_result[""transcripts""] = [{\n        ""confidence"": transcript.confidence,\n        ""words"": words_from_candidate_transcript(transcript),\n    } for transcript in metadata.transcripts]\n    return json.dumps(json_result, indent=2)\n\n\n\nclass VersionAction(argparse.Action):\n    def __init__(self, *args, **kwargs):\n        super(VersionAction, self).__init__(nargs=0, *args, **kwargs)\n\n    def __call__(self, *args, **kwargs):\n        print(\'DeepSpeech \', version())\n        exit(0)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Running DeepSpeech inference.\')\n    parser.add_argument(\'--model\', required=True,\n                        help=\'Path to the model (protocol buffer binary file)\')\n    parser.add_argument(\'--scorer\', required=False,\n                        help=\'Path to the external scorer file\')\n    parser.add_argument(\'--audio\', required=True,\n                        help=\'Path to the audio file to run (WAV format)\')\n    parser.add_argument(\'--beam_width\', type=int,\n                        help=\'Beam width for the CTC decoder\')\n    parser.add_argument(\'--lm_alpha\', type=float,\n                        help=\'Language model weight (lm_alpha). If not specified, use default from the scorer package.\')\n    parser.add_argument(\'--lm_beta\', type=float,\n                        help=\'Word insertion bonus (lm_beta). If not specified, use default from the scorer package.\')\n    parser.add_argument(\'--version\', action=VersionAction,\n                        help=\'Print version and exits\')\n    parser.add_argument(\'--extended\', required=False, action=\'store_true\',\n                        help=\'Output string from extended metadata\')\n    parser.add_argument(\'--json\', required=False, action=\'store_true\',\n                        help=\'Output json from metadata with timestamp of each word\')\n    parser.add_argument(\'--candidate_transcripts\', type=int, default=3,\n                        help=\'Number of candidate transcripts to include in JSON output\')\n    args = parser.parse_args()\n\n    print(\'Loading model from file {}\'.format(args.model), file=sys.stderr)\n    model_load_start = timer()\n    # sphinx-doc: python_ref_model_start\n    ds = Model(args.model)\n    # sphinx-doc: python_ref_model_stop\n    model_load_end = timer() - model_load_start\n    print(\'Loaded model in {:.3}s.\'.format(model_load_end), file=sys.stderr)\n\n    if args.beam_width:\n        ds.setBeamWidth(args.beam_width)\n\n    desired_sample_rate = ds.sampleRate()\n\n    if args.scorer:\n        print(\'Loading scorer from files {}\'.format(args.scorer), file=sys.stderr)\n        scorer_load_start = timer()\n        ds.enableExternalScorer(args.scorer)\n        scorer_load_end = timer() - scorer_load_start\n        print(\'Loaded scorer in {:.3}s.\'.format(scorer_load_end), file=sys.stderr)\n\n        if args.lm_alpha and args.lm_beta:\n            ds.setScorerAlphaBeta(args.lm_alpha, args.lm_beta)\n\n    fin = wave.open(args.audio, \'rb\')\n    fs_orig = fin.getframerate()\n    if fs_orig != desired_sample_rate:\n        print(\'Warning: original sample rate ({}) is different than {}hz. Resampling might produce erratic speech recognition.\'.format(fs_orig, desired_sample_rate), file=sys.stderr)\n        fs_new, audio = convert_samplerate(args.audio, desired_sample_rate)\n    else:\n        audio = np.frombuffer(fin.readframes(fin.getnframes()), np.int16)\n\n    audio_length = fin.getnframes() * (1/fs_orig)\n    fin.close()\n\n    print(\'Running inference.\', file=sys.stderr)\n    inference_start = timer()\n    # sphinx-doc: python_ref_inference_start\n    if args.extended:\n        print(metadata_to_string(ds.sttWithMetadata(audio, 1).transcripts[0]))\n    elif args.json:\n        print(metadata_json_output(ds.sttWithMetadata(audio, args.candidate_transcripts)))\n    else:\n        print(ds.stt(audio))\n    # sphinx-doc: python_ref_inference_stop\n    inference_end = timer() - inference_start\n    print(\'Inference took %0.3fs for %0.3fs audio file.\' % (inference_end, audio_length), file=sys.stderr)\n\nif __name__ == \'__main__\':\n    main()\n'"
native_client/python/setup.py,0,"b""#! /usr/bin/env python\n\nfrom setuptools import setup, Extension\nfrom distutils.command.build import build\n\nimport os\nimport subprocess\nimport sys\n\ndef main():\n    try:\n        import numpy\n        try:\n            numpy_include = numpy.get_include()\n        except AttributeError:\n            numpy_include = numpy.get_numpy_include()\n    except ImportError:\n        numpy_include = ''\n        assert 'NUMPY_INCLUDE' in os.environ\n\n    def read(fname):\n        return open(os.path.join(os.path.dirname(__file__), fname)).read()\n\n    numpy_include = os.getenv('NUMPY_INCLUDE', numpy_include)\n    numpy_min_ver = os.getenv('NUMPY_DEP_VERSION', '')\n\n    project_name = 'deepspeech'\n    if '--project_name' in sys.argv:\n        project_name_idx = sys.argv.index('--project_name')\n        project_name = sys.argv[project_name_idx + 1]\n        sys.argv.remove('--project_name')\n        sys.argv.pop(project_name_idx)\n\n    with open('../../training/deepspeech_training/VERSION', 'r') as ver:\n        project_version = ver.read().strip()\n\n    class BuildExtFirst(build):\n        sub_commands = [('build_ext', build.has_ext_modules),\n                        ('build_py', build.has_pure_modules),\n                        ('build_clib', build.has_c_libraries),\n                        ('build_scripts', build.has_scripts)]\n\n    # Properly pass arguments for linking, setuptools will perform some checks\n    def lib_dirs_split(a):\n        if os.name == 'posix':\n            return a.split('-L')[1:]\n\n        if os.name == 'nt':\n            return []\n\n        raise AssertionError('os.name == java not expected')\n\n    def libs_split(a):\n        if os.name == 'posix':\n            return a.split('-l')[1:]\n\n        if os.name == 'nt':\n            return a.split('.lib')[0:1]\n\n        raise AssertionError('os.name == java not expected')\n\n    ds_ext = Extension(name='deepspeech._impl',\n                       sources=['impl.i'],\n                       include_dirs=[numpy_include, '../'],\n                       library_dirs=list(map(lambda x: x.strip(), lib_dirs_split(os.getenv('MODEL_LDFLAGS', '')))),\n                       libraries=list(map(lambda x: x.strip(), libs_split(os.getenv('MODEL_LIBS', '')))),\n                       swig_opts=['-c++', '-keyword'])\n\n    setup(name=project_name,\n          description='A library for running inference on a DeepSpeech model',\n          long_description=read('README.rst'),\n          long_description_content_type='text/x-rst; charset=UTF-8',\n          author='Mozilla',\n          version=project_version,\n          package_dir={'deepspeech': '.'},\n          cmdclass={'build': BuildExtFirst},\n          license='MPL-2.0',\n          url='https://github.com/mozilla/DeepSpeech',\n          project_urls={\n              'Documentation': 'https://github.com/mozilla/DeepSpeech/tree/v{}#project-deepspeech'.format(project_version),\n              'Tracker': 'https://github.com/mozilla/DeepSpeech/issues',\n              'Repository': 'https://github.com/mozilla/DeepSpeech/tree/v{}'.format(project_version),\n              'Discussions': 'https://discourse.mozilla.org/c/deep-speech',\n          },\n          ext_modules=[ds_ext],\n          py_modules=['deepspeech', 'deepspeech.client', 'deepspeech.impl'],\n          entry_points={'console_scripts':['deepspeech=deepspeech.client:main']},\n          install_requires=['numpy%s' % numpy_min_ver],\n          include_package_data=True,\n          classifiers=[\n              'Development Status :: 3 - Alpha',\n              'Environment :: Console',\n              'Intended Audience :: Developers',\n              'Intended Audience :: Science/Research',\n              'License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)',\n              'Programming Language :: Python :: 2.7',\n              'Programming Language :: Python :: 3.4',\n              'Programming Language :: Python :: 3.5',\n              'Programming Language :: Python :: 3.6',\n              'Topic :: Multimedia :: Sound/Audio :: Speech',\n              'Topic :: Scientific/Engineering :: Human Machine Interfaces',\n              'Topic :: Scientific/Engineering',\n              'Topic :: Utilities',\n          ])\n\nif __name__ == '__main__':\n    main()\n"""
native_client/test/concurrent_streams.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport numpy as np\nimport wave\n\nfrom deepspeech import Model\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Running DeepSpeech inference.')\n    parser.add_argument('--model', required=True,\n                        help='Path to the model (protocol buffer binary file)')\n    parser.add_argument('--scorer', nargs='?',\n                        help='Path to the external scorer file')\n    parser.add_argument('--audio1', required=True,\n                        help='First audio file to use in interleaved streams')\n    parser.add_argument('--audio2', required=True,\n                        help='Second audio file to use in interleaved streams')\n    args = parser.parse_args()\n\n    ds = Model(args.model)\n\n    if args.scorer:\n        ds.enableExternalScorer(args.scorer)\n\n    fin = wave.open(args.audio1, 'rb')\n    fs1 = fin.getframerate()\n    audio1 = np.frombuffer(fin.readframes(fin.getnframes()), np.int16)\n    fin.close()\n\n    fin = wave.open(args.audio2, 'rb')\n    fs2 = fin.getframerate()\n    audio2 = np.frombuffer(fin.readframes(fin.getnframes()), np.int16)\n    fin.close()\n\n    stream1 = ds.createStream()\n    stream2 = ds.createStream()\n\n    splits1 = np.array_split(audio1, 10)\n    splits2 = np.array_split(audio2, 10)\n\n    for part1, part2 in zip(splits1, splits2):\n        stream1.feedAudioContent(part1)\n        stream2.feedAudioContent(part2)\n\n    print(stream1.finishStream())\n    print(stream2.finishStream())\n\nif __name__ == '__main__':\n    main()\n"""
tests/test_data/validate_locale_fra.py,0,b'def validate_label(label):\n    return label\n'
training/deepspeech_training/__init__.py,0,b''
training/deepspeech_training/evaluate.py,3,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport sys\n\nfrom multiprocessing import cpu_count\n\nimport absl.app\nimport progressbar\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tfv1\n\nfrom ds_ctcdecoder import ctc_beam_search_decoder_batch, Scorer\nfrom six.moves import zip\n\nfrom .util.config import Config, initialize_globals\nfrom .util.checkpoints import load_graph_for_evaluation\nfrom .util.evaluate_tools import calculate_and_print_report, save_samples_json\nfrom .util.feeding import create_dataset\nfrom .util.flags import create_flags, FLAGS\nfrom .util.helpers import check_ctcdecoder_version\nfrom .util.logging import create_progressbar, log_error, log_progress\n\ncheck_ctcdecoder_version()\n\ndef sparse_tensor_value_to_texts(value, alphabet):\n    r""""""\n    Given a :class:`tf.SparseTensor` ``value``, return an array of Python strings\n    representing its values, converting tokens to strings using ``alphabet``.\n    """"""\n    return sparse_tuple_to_texts((value.indices, value.values, value.dense_shape), alphabet)\n\n\ndef sparse_tuple_to_texts(sp_tuple, alphabet):\n    indices = sp_tuple[0]\n    values = sp_tuple[1]\n    results = [[] for _ in range(sp_tuple[2][0])]\n    for i, index in enumerate(indices):\n        results[index[0]].append(values[i])\n    # List of strings\n    return [alphabet.decode(res) for res in results]\n\n\ndef evaluate(test_csvs, create_model):\n    if FLAGS.scorer_path:\n        scorer = Scorer(FLAGS.lm_alpha, FLAGS.lm_beta,\n                        FLAGS.scorer_path, Config.alphabet)\n    else:\n        scorer = None\n\n    test_sets = [create_dataset([csv], batch_size=FLAGS.test_batch_size, train_phase=False) for csv in test_csvs]\n    iterator = tfv1.data.Iterator.from_structure(tfv1.data.get_output_types(test_sets[0]),\n                                                 tfv1.data.get_output_shapes(test_sets[0]),\n                                                 output_classes=tfv1.data.get_output_classes(test_sets[0]))\n    test_init_ops = [iterator.make_initializer(test_set) for test_set in test_sets]\n\n    batch_wav_filename, (batch_x, batch_x_len), batch_y = iterator.get_next()\n\n    # One rate per layer\n    no_dropout = [None] * 6\n    logits, _ = create_model(batch_x=batch_x,\n                             seq_length=batch_x_len,\n                             dropout=no_dropout)\n\n    # Transpose to batch major and apply softmax for decoder\n    transposed = tf.nn.softmax(tf.transpose(a=logits, perm=[1, 0, 2]))\n\n    loss = tfv1.nn.ctc_loss(labels=batch_y,\n                            inputs=logits,\n                            sequence_length=batch_x_len)\n\n    tfv1.train.get_or_create_global_step()\n\n    # Get number of accessible CPU cores for this process\n    try:\n        num_processes = cpu_count()\n    except NotImplementedError:\n        num_processes = 1\n\n    with tfv1.Session(config=Config.session_config) as session:\n        load_graph_for_evaluation(session)\n\n        def run_test(init_op, dataset):\n            wav_filenames = []\n            losses = []\n            predictions = []\n            ground_truths = []\n\n            bar = create_progressbar(prefix=\'Test epoch | \',\n                                     widgets=[\'Steps: \', progressbar.Counter(), \' | \', progressbar.Timer()]).start()\n            log_progress(\'Test epoch...\')\n\n            step_count = 0\n\n            # Initialize iterator to the appropriate dataset\n            session.run(init_op)\n\n            # First pass, compute losses and transposed logits for decoding\n            while True:\n                try:\n                    batch_wav_filenames, batch_logits, batch_loss, batch_lengths, batch_transcripts = \\\n                        session.run([batch_wav_filename, transposed, loss, batch_x_len, batch_y])\n                except tf.errors.OutOfRangeError:\n                    break\n\n                decoded = ctc_beam_search_decoder_batch(batch_logits, batch_lengths, Config.alphabet, FLAGS.beam_width,\n                                                        num_processes=num_processes, scorer=scorer,\n                                                        cutoff_prob=FLAGS.cutoff_prob, cutoff_top_n=FLAGS.cutoff_top_n)\n                predictions.extend(d[0][1] for d in decoded)\n                ground_truths.extend(sparse_tensor_value_to_texts(batch_transcripts, Config.alphabet))\n                wav_filenames.extend(wav_filename.decode(\'UTF-8\') for wav_filename in batch_wav_filenames)\n                losses.extend(batch_loss)\n\n                step_count += 1\n                bar.update(step_count)\n\n            bar.finish()\n\n            # Print test summary\n            test_samples = calculate_and_print_report(wav_filenames, ground_truths, predictions, losses, dataset)\n            return test_samples\n\n        samples = []\n        for csv, init_op in zip(test_csvs, test_init_ops):\n            print(\'Testing model on {}\'.format(csv))\n            samples.extend(run_test(init_op, dataset=csv))\n        return samples\n\n\ndef main(_):\n    initialize_globals()\n\n    if not FLAGS.test_files:\n        log_error(\'You need to specify what files to use for evaluation via \'\n                  \'the --test_files flag.\')\n        sys.exit(1)\n\n    from .train import create_model # pylint: disable=cyclic-import,import-outside-toplevel\n    samples = evaluate(FLAGS.test_files.split(\',\'), create_model)\n\n    if FLAGS.test_output_file:\n        save_samples_json(samples, FLAGS.test_output_file)\n\n\ndef run_script():\n    create_flags()\n    absl.app.run(main)\n\nif __name__ == \'__main__\':\n    run_script()\n'"
training/deepspeech_training/train.py,70,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport sys\n\nLOG_LEVEL_INDEX = sys.argv.index(\'--log_level\') + 1 if \'--log_level\' in sys.argv else 0\nDESIRED_LOG_LEVEL = sys.argv[LOG_LEVEL_INDEX] if 0 < LOG_LEVEL_INDEX < len(sys.argv) else \'3\'\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = DESIRED_LOG_LEVEL\n\nimport absl.app\nimport json\nimport numpy as np\nimport progressbar\nimport shutil\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tfv1\nimport time\n\ntfv1.logging.set_verbosity({\n    \'0\': tfv1.logging.DEBUG,\n    \'1\': tfv1.logging.INFO,\n    \'2\': tfv1.logging.WARN,\n    \'3\': tfv1.logging.ERROR\n}.get(DESIRED_LOG_LEVEL))\n\nfrom datetime import datetime\nfrom ds_ctcdecoder import ctc_beam_search_decoder, Scorer\nfrom .evaluate import evaluate\nfrom six.moves import zip, range\nfrom .util.config import Config, initialize_globals\nfrom .util.checkpoints import load_or_init_graph_for_training, load_graph_for_evaluation\nfrom .util.evaluate_tools import save_samples_json\nfrom .util.feeding import create_dataset, samples_to_mfccs, audiofile_to_features\nfrom .util.flags import create_flags, FLAGS\nfrom .util.helpers import check_ctcdecoder_version, ExceptionBox\nfrom .util.logging import create_progressbar, log_debug, log_error, log_info, log_progress, log_warn\n\ncheck_ctcdecoder_version()\n\n# Graph Creation\n# ==============\n\ndef variable_on_cpu(name, shape, initializer):\n    r""""""\n    Next we concern ourselves with graph creation.\n    However, before we do so we must introduce a utility function ``variable_on_cpu()``\n    used to create a variable in CPU memory.\n    """"""\n    # Use the /cpu:0 device for scoped operations\n    with tf.device(Config.cpu_device):\n        # Create or get apropos variable\n        var = tfv1.get_variable(name=name, shape=shape, initializer=initializer)\n    return var\n\n\ndef create_overlapping_windows(batch_x):\n    batch_size = tf.shape(input=batch_x)[0]\n    window_width = 2 * Config.n_context + 1\n    num_channels = Config.n_input\n\n    # Create a constant convolution filter using an identity matrix, so that the\n    # convolution returns patches of the input tensor as is, and we can create\n    # overlapping windows over the MFCCs.\n    eye_filter = tf.constant(np.eye(window_width * num_channels)\n                               .reshape(window_width, num_channels, window_width * num_channels), tf.float32) # pylint: disable=bad-continuation\n\n    # Create overlapping windows\n    batch_x = tf.nn.conv1d(input=batch_x, filters=eye_filter, stride=1, padding=\'SAME\')\n\n    # Remove dummy depth dimension and reshape into [batch_size, n_windows, window_width, n_input]\n    batch_x = tf.reshape(batch_x, [batch_size, -1, window_width, num_channels])\n\n    return batch_x\n\n\ndef dense(name, x, units, dropout_rate=None, relu=True):\n    with tfv1.variable_scope(name):\n        bias = variable_on_cpu(\'bias\', [units], tfv1.zeros_initializer())\n        weights = variable_on_cpu(\'weights\', [x.shape[-1], units], tfv1.keras.initializers.VarianceScaling(scale=1.0, mode=""fan_avg"", distribution=""uniform""))\n\n    output = tf.nn.bias_add(tf.matmul(x, weights), bias)\n\n    if relu:\n        output = tf.minimum(tf.nn.relu(output), FLAGS.relu_clip)\n\n    if dropout_rate is not None:\n        output = tf.nn.dropout(output, rate=dropout_rate)\n\n    return output\n\n\ndef rnn_impl_lstmblockfusedcell(x, seq_length, previous_state, reuse):\n    with tfv1.variable_scope(\'cudnn_lstm/rnn/multi_rnn_cell/cell_0\'):\n        fw_cell = tf.contrib.rnn.LSTMBlockFusedCell(Config.n_cell_dim,\n                                                    forget_bias=0,\n                                                    reuse=reuse,\n                                                    name=\'cudnn_compatible_lstm_cell\')\n\n        output, output_state = fw_cell(inputs=x,\n                                       dtype=tf.float32,\n                                       sequence_length=seq_length,\n                                       initial_state=previous_state)\n\n    return output, output_state\n\n\ndef rnn_impl_cudnn_rnn(x, seq_length, previous_state, _):\n    assert previous_state is None # \'Passing previous state not supported with CuDNN backend\'\n\n    # Hack: CudnnLSTM works similarly to Keras layers in that when you instantiate\n    # the object it creates the variables, and then you just call it several times\n    # to enable variable re-use. Because all of our code is structure in an old\n    # school TensorFlow structure where you can just call tf.get_variable again with\n    # reuse=True to reuse variables, we can\'t easily make use of the object oriented\n    # way CudnnLSTM is implemented, so we save a singleton instance in the function,\n    # emulating a static function variable.\n    if not rnn_impl_cudnn_rnn.cell:\n        # Forward direction cell:\n        fw_cell = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers=1,\n                                                 num_units=Config.n_cell_dim,\n                                                 input_mode=\'linear_input\',\n                                                 direction=\'unidirectional\',\n                                                 dtype=tf.float32)\n        rnn_impl_cudnn_rnn.cell = fw_cell\n\n    output, output_state = rnn_impl_cudnn_rnn.cell(inputs=x,\n                                                   sequence_lengths=seq_length)\n\n    return output, output_state\n\nrnn_impl_cudnn_rnn.cell = None\n\n\ndef rnn_impl_static_rnn(x, seq_length, previous_state, reuse):\n    with tfv1.variable_scope(\'cudnn_lstm/rnn/multi_rnn_cell\'):\n        # Forward direction cell:\n        fw_cell = tfv1.nn.rnn_cell.LSTMCell(Config.n_cell_dim,\n                                            forget_bias=0,\n                                            reuse=reuse,\n                                            name=\'cudnn_compatible_lstm_cell\')\n\n        # Split rank N tensor into list of rank N-1 tensors\n        x = [x[l] for l in range(x.shape[0])]\n\n        output, output_state = tfv1.nn.static_rnn(cell=fw_cell,\n                                                  inputs=x,\n                                                  sequence_length=seq_length,\n                                                  initial_state=previous_state,\n                                                  dtype=tf.float32,\n                                                  scope=\'cell_0\')\n\n        output = tf.concat(output, 0)\n\n    return output, output_state\n\n\ndef create_model(batch_x, seq_length, dropout, reuse=False, batch_size=None, previous_state=None, overlap=True, rnn_impl=rnn_impl_lstmblockfusedcell):\n    layers = {}\n\n    # Input shape: [batch_size, n_steps, n_input + 2*n_input*n_context]\n    if not batch_size:\n        batch_size = tf.shape(input=batch_x)[0]\n\n    # Create overlapping feature windows if needed\n    if overlap:\n        batch_x = create_overlapping_windows(batch_x)\n\n    # Reshaping `batch_x` to a tensor with shape `[n_steps*batch_size, n_input + 2*n_input*n_context]`.\n    # This is done to prepare the batch for input into the first layer which expects a tensor of rank `2`.\n\n    # Permute n_steps and batch_size\n    batch_x = tf.transpose(a=batch_x, perm=[1, 0, 2, 3])\n    # Reshape to prepare input for first layer\n    batch_x = tf.reshape(batch_x, [-1, Config.n_input + 2*Config.n_input*Config.n_context]) # (n_steps*batch_size, n_input + 2*n_input*n_context)\n    layers[\'input_reshaped\'] = batch_x\n\n    # The next three blocks will pass `batch_x` through three hidden layers with\n    # clipped RELU activation and dropout.\n    layers[\'layer_1\'] = layer_1 = dense(\'layer_1\', batch_x, Config.n_hidden_1, dropout_rate=dropout[0])\n    layers[\'layer_2\'] = layer_2 = dense(\'layer_2\', layer_1, Config.n_hidden_2, dropout_rate=dropout[1])\n    layers[\'layer_3\'] = layer_3 = dense(\'layer_3\', layer_2, Config.n_hidden_3, dropout_rate=dropout[2])\n\n    # `layer_3` is now reshaped into `[n_steps, batch_size, 2*n_cell_dim]`,\n    # as the LSTM RNN expects its input to be of shape `[max_time, batch_size, input_size]`.\n    layer_3 = tf.reshape(layer_3, [-1, batch_size, Config.n_hidden_3])\n\n    # Run through parametrized RNN implementation, as we use different RNNs\n    # for training and inference\n    output, output_state = rnn_impl(layer_3, seq_length, previous_state, reuse)\n\n    # Reshape output from a tensor of shape [n_steps, batch_size, n_cell_dim]\n    # to a tensor of shape [n_steps*batch_size, n_cell_dim]\n    output = tf.reshape(output, [-1, Config.n_cell_dim])\n    layers[\'rnn_output\'] = output\n    layers[\'rnn_output_state\'] = output_state\n\n    # Now we feed `output` to the fifth hidden layer with clipped RELU activation\n    layers[\'layer_5\'] = layer_5 = dense(\'layer_5\', output, Config.n_hidden_5, dropout_rate=dropout[5])\n\n    # Now we apply a final linear layer creating `n_classes` dimensional vectors, the logits.\n    layers[\'layer_6\'] = layer_6 = dense(\'layer_6\', layer_5, Config.n_hidden_6, relu=False)\n\n    # Finally we reshape layer_6 from a tensor of shape [n_steps*batch_size, n_hidden_6]\n    # to the slightly more useful shape [n_steps, batch_size, n_hidden_6].\n    # Note, that this differs from the input in that it is time-major.\n    layer_6 = tf.reshape(layer_6, [-1, batch_size, Config.n_hidden_6], name=\'raw_logits\')\n    layers[\'raw_logits\'] = layer_6\n\n    # Output shape: [n_steps, batch_size, n_hidden_6]\n    return layer_6, layers\n\n\n# Accuracy and Loss\n# =================\n\n# In accord with \'Deep Speech: Scaling up end-to-end speech recognition\'\n# (http://arxiv.org/abs/1412.5567),\n# the loss function used by our network should be the CTC loss function\n# (http://www.cs.toronto.edu/~graves/preprint.pdf).\n# Conveniently, this loss function is implemented in TensorFlow.\n# Thus, we can simply make use of this implementation to define our loss.\n\ndef calculate_mean_edit_distance_and_loss(iterator, dropout, reuse):\n    r\'\'\'\n    This routine beam search decodes a mini-batch and calculates the loss and mean edit distance.\n    Next to total and average loss it returns the mean edit distance,\n    the decoded result and the batch\'s original Y.\n    \'\'\'\n    # Obtain the next batch of data\n    batch_filenames, (batch_x, batch_seq_len), batch_y = iterator.get_next()\n\n    if FLAGS.train_cudnn:\n        rnn_impl = rnn_impl_cudnn_rnn\n    else:\n        rnn_impl = rnn_impl_lstmblockfusedcell\n\n    # Calculate the logits of the batch\n    logits, _ = create_model(batch_x, batch_seq_len, dropout, reuse=reuse, rnn_impl=rnn_impl)\n\n    # Compute the CTC loss using TensorFlow\'s `ctc_loss`\n    total_loss = tfv1.nn.ctc_loss(labels=batch_y, inputs=logits, sequence_length=batch_seq_len)\n\n    # Check if any files lead to non finite loss\n    non_finite_files = tf.gather(batch_filenames, tfv1.where(~tf.math.is_finite(total_loss)))\n\n    # Calculate the average loss across the batch\n    avg_loss = tf.reduce_mean(input_tensor=total_loss)\n\n    # Finally we return the average loss\n    return avg_loss, non_finite_files\n\n\n# Adam Optimization\n# =================\n\n# In contrast to \'Deep Speech: Scaling up end-to-end speech recognition\'\n# (http://arxiv.org/abs/1412.5567),\n# in which \'Nesterov\'s Accelerated Gradient Descent\'\n# (www.cs.toronto.edu/~fritz/absps/momentum.pdf) was used,\n# we will use the Adam method for optimization (http://arxiv.org/abs/1412.6980),\n# because, generally, it requires less fine-tuning.\ndef create_optimizer(learning_rate_var):\n    optimizer = tfv1.train.AdamOptimizer(learning_rate=learning_rate_var,\n                                         beta1=FLAGS.beta1,\n                                         beta2=FLAGS.beta2,\n                                         epsilon=FLAGS.epsilon)\n    return optimizer\n\n\n# Towers\n# ======\n\n# In order to properly make use of multiple GPU\'s, one must introduce new abstractions,\n# not present when using a single GPU, that facilitate the multi-GPU use case.\n# In particular, one must introduce a means to isolate the inference and gradient\n# calculations on the various GPU\'s.\n# The abstraction we intoduce for this purpose is called a \'tower\'.\n# A tower is specified by two properties:\n# * **Scope** - A scope, as provided by `tf.name_scope()`,\n# is a means to isolate the operations within a tower.\n# For example, all operations within \'tower 0\' could have their name prefixed with `tower_0/`.\n# * **Device** - A hardware device, as provided by `tf.device()`,\n# on which all operations within the tower execute.\n# For example, all operations of \'tower 0\' could execute on the first GPU `tf.device(\'/gpu:0\')`.\n\ndef get_tower_results(iterator, optimizer, dropout_rates):\n    r\'\'\'\n    With this preliminary step out of the way, we can for each GPU introduce a\n    tower for which\'s batch we calculate and return the optimization gradients\n    and the average loss across towers.\n    \'\'\'\n    # To calculate the mean of the losses\n    tower_avg_losses = []\n\n    # Tower gradients to return\n    tower_gradients = []\n\n    # Aggregate any non finite files in the batches\n    tower_non_finite_files = []\n\n    with tfv1.variable_scope(tfv1.get_variable_scope()):\n        # Loop over available_devices\n        for i in range(len(Config.available_devices)):\n            # Execute operations of tower i on device i\n            device = Config.available_devices[i]\n            with tf.device(device):\n                # Create a scope for all operations of tower i\n                with tf.name_scope(\'tower_%d\' % i):\n                    # Calculate the avg_loss and mean_edit_distance and retrieve the decoded\n                    # batch along with the original batch\'s labels (Y) of this tower\n                    avg_loss, non_finite_files = calculate_mean_edit_distance_and_loss(iterator, dropout_rates, reuse=i > 0)\n\n                    # Allow for variables to be re-used by the next tower\n                    tfv1.get_variable_scope().reuse_variables()\n\n                    # Retain tower\'s avg losses\n                    tower_avg_losses.append(avg_loss)\n\n                    # Compute gradients for model parameters using tower\'s mini-batch\n                    gradients = optimizer.compute_gradients(avg_loss)\n\n                    # Retain tower\'s gradients\n                    tower_gradients.append(gradients)\n\n                    tower_non_finite_files.append(non_finite_files)\n\n    avg_loss_across_towers = tf.reduce_mean(input_tensor=tower_avg_losses, axis=0)\n    tfv1.summary.scalar(name=\'step_loss\', tensor=avg_loss_across_towers, collections=[\'step_summaries\'])\n\n    all_non_finite_files = tf.concat(tower_non_finite_files, axis=0)\n\n    # Return gradients and the average loss\n    return tower_gradients, avg_loss_across_towers, all_non_finite_files\n\n\ndef average_gradients(tower_gradients):\n    r\'\'\'\n    A routine for computing each variable\'s average of the gradients obtained from the GPUs.\n    Note also that this code acts as a synchronization point as it requires all\n    GPUs to be finished with their mini-batch before it can run to completion.\n    \'\'\'\n    # List of average gradients to return to the caller\n    average_grads = []\n\n    # Run this on cpu_device to conserve GPU memory\n    with tf.device(Config.cpu_device):\n        # Loop over gradient/variable pairs from all towers\n        for grad_and_vars in zip(*tower_gradients):\n            # Introduce grads to store the gradients for the current variable\n            grads = []\n\n            # Loop over the gradients for the current variable\n            for g, _ in grad_and_vars:\n                # Add 0 dimension to the gradients to represent the tower.\n                expanded_g = tf.expand_dims(g, 0)\n                # Append on a \'tower\' dimension which we will average over below.\n                grads.append(expanded_g)\n\n            # Average over the \'tower\' dimension\n            grad = tf.concat(grads, 0)\n            grad = tf.reduce_mean(input_tensor=grad, axis=0)\n\n            # Create a gradient/variable tuple for the current variable with its average gradient\n            grad_and_var = (grad, grad_and_vars[0][1])\n\n            # Add the current tuple to average_grads\n            average_grads.append(grad_and_var)\n\n    # Return result to caller\n    return average_grads\n\n\n\n# Logging\n# =======\n\ndef log_variable(variable, gradient=None):\n    r\'\'\'\n    We introduce a function for logging a tensor variable\'s current state.\n    It logs scalar values for the mean, standard deviation, minimum and maximum.\n    Furthermore it logs a histogram of its state and (if given) of an optimization gradient.\n    \'\'\'\n    name = variable.name.replace(\':\', \'_\')\n    mean = tf.reduce_mean(input_tensor=variable)\n    tfv1.summary.scalar(name=\'%s/mean\'   % name, tensor=mean)\n    tfv1.summary.scalar(name=\'%s/sttdev\' % name, tensor=tf.sqrt(tf.reduce_mean(input_tensor=tf.square(variable - mean))))\n    tfv1.summary.scalar(name=\'%s/max\'    % name, tensor=tf.reduce_max(input_tensor=variable))\n    tfv1.summary.scalar(name=\'%s/min\'    % name, tensor=tf.reduce_min(input_tensor=variable))\n    tfv1.summary.histogram(name=name, values=variable)\n    if gradient is not None:\n        if isinstance(gradient, tf.IndexedSlices):\n            grad_values = gradient.values\n        else:\n            grad_values = gradient\n        if grad_values is not None:\n            tfv1.summary.histogram(name=\'%s/gradients\' % name, values=grad_values)\n\n\ndef log_grads_and_vars(grads_and_vars):\n    r\'\'\'\n    Let\'s also introduce a helper function for logging collections of gradient/variable tuples.\n    \'\'\'\n    for gradient, variable in grads_and_vars:\n        log_variable(variable, gradient=gradient)\n\n\ndef train():\n    do_cache_dataset = True\n\n    # pylint: disable=too-many-boolean-expressions\n    if (FLAGS.data_aug_features_multiplicative > 0 or\n            FLAGS.data_aug_features_additive > 0 or\n            FLAGS.augmentation_spec_dropout_keeprate < 1 or\n            FLAGS.augmentation_freq_and_time_masking or\n            FLAGS.augmentation_pitch_and_tempo_scaling or\n            FLAGS.augmentation_speed_up_std > 0 or\n            FLAGS.augmentation_sparse_warp):\n        do_cache_dataset = False\n\n    exception_box = ExceptionBox()\n\n    # Create training and validation datasets\n    train_set = create_dataset(FLAGS.train_files.split(\',\'),\n                               batch_size=FLAGS.train_batch_size,\n                               repetitions=FLAGS.augmentations_per_epoch,\n                               augmentation_specs=FLAGS.augment,\n                               enable_cache=FLAGS.feature_cache and do_cache_dataset,\n                               cache_path=FLAGS.feature_cache,\n                               train_phase=True,\n                               exception_box=exception_box,\n                               process_ahead=len(Config.available_devices) * FLAGS.train_batch_size * 2,\n                               buffering=FLAGS.read_buffer)\n\n    iterator = tfv1.data.Iterator.from_structure(tfv1.data.get_output_types(train_set),\n                                                 tfv1.data.get_output_shapes(train_set),\n                                                 output_classes=tfv1.data.get_output_classes(train_set))\n\n    # Make initialization ops for switching between the two sets\n    train_init_op = iterator.make_initializer(train_set)\n\n    if FLAGS.dev_files:\n        dev_sources = FLAGS.dev_files.split(\',\')\n        dev_sets = [create_dataset([source],\n                                   batch_size=FLAGS.dev_batch_size,\n                                   train_phase=False,\n                                   exception_box=exception_box,\n                                   process_ahead=len(Config.available_devices) * FLAGS.dev_batch_size * 2,\n                                   buffering=FLAGS.read_buffer) for source in dev_sources]\n        dev_init_ops = [iterator.make_initializer(dev_set) for dev_set in dev_sets]\n\n    if FLAGS.metrics_files:\n        metrics_sources = FLAGS.metrics_files.split(\',\')\n        metrics_sets = [create_dataset([source],\n                                       batch_size=FLAGS.dev_batch_size,\n                                       train_phase=False,\n                                       exception_box=exception_box,\n                                       process_ahead=len(Config.available_devices) * FLAGS.dev_batch_size * 2,\n                                       buffering=FLAGS.read_buffer) for source in metrics_sources]\n        metrics_init_ops = [iterator.make_initializer(metrics_set) for metrics_set in metrics_sets]\n\n    # Dropout\n    dropout_rates = [tfv1.placeholder(tf.float32, name=\'dropout_{}\'.format(i)) for i in range(6)]\n    dropout_feed_dict = {\n        dropout_rates[0]: FLAGS.dropout_rate,\n        dropout_rates[1]: FLAGS.dropout_rate2,\n        dropout_rates[2]: FLAGS.dropout_rate3,\n        dropout_rates[3]: FLAGS.dropout_rate4,\n        dropout_rates[4]: FLAGS.dropout_rate5,\n        dropout_rates[5]: FLAGS.dropout_rate6,\n    }\n    no_dropout_feed_dict = {\n        rate: 0. for rate in dropout_rates\n    }\n\n    # Building the graph\n    learning_rate_var = tfv1.get_variable(\'learning_rate\', initializer=FLAGS.learning_rate, trainable=False)\n    reduce_learning_rate_op = learning_rate_var.assign(tf.multiply(learning_rate_var, FLAGS.plateau_reduction))\n    optimizer = create_optimizer(learning_rate_var)\n\n    # Enable mixed precision training\n    if FLAGS.automatic_mixed_precision:\n        log_info(\'Enabling automatic mixed precision training.\')\n        optimizer = tfv1.train.experimental.enable_mixed_precision_graph_rewrite(optimizer)\n\n    gradients, loss, non_finite_files = get_tower_results(iterator, optimizer, dropout_rates)\n\n    # Average tower gradients across GPUs\n    avg_tower_gradients = average_gradients(gradients)\n    log_grads_and_vars(avg_tower_gradients)\n\n    # global_step is automagically incremented by the optimizer\n    global_step = tfv1.train.get_or_create_global_step()\n    apply_gradient_op = optimizer.apply_gradients(avg_tower_gradients, global_step=global_step)\n\n    # Summaries\n    step_summaries_op = tfv1.summary.merge_all(\'step_summaries\')\n    step_summary_writers = {\n        \'train\': tfv1.summary.FileWriter(os.path.join(FLAGS.summary_dir, \'train\'), max_queue=120),\n        \'dev\': tfv1.summary.FileWriter(os.path.join(FLAGS.summary_dir, \'dev\'), max_queue=120),\n        \'metrics\': tfv1.summary.FileWriter(os.path.join(FLAGS.summary_dir, \'metrics\'), max_queue=120),\n    }\n\n    human_readable_set_names = {\n        \'train\': \'Training\',\n        \'dev\': \'Validation\',\n        \'metrics\': \'Metrics\',\n    }\n\n    # Checkpointing\n    checkpoint_saver = tfv1.train.Saver(max_to_keep=FLAGS.max_to_keep)\n    checkpoint_path = os.path.join(FLAGS.save_checkpoint_dir, \'train\')\n\n    best_dev_saver = tfv1.train.Saver(max_to_keep=1)\n    best_dev_path = os.path.join(FLAGS.save_checkpoint_dir, \'best_dev\')\n\n    # Save flags next to checkpoints\n    os.makedirs(FLAGS.save_checkpoint_dir, exist_ok=True)\n    flags_file = os.path.join(FLAGS.save_checkpoint_dir, \'flags.txt\')\n    with open(flags_file, \'w\') as fout:\n        fout.write(FLAGS.flags_into_string())\n\n    with tfv1.Session(config=Config.session_config) as session:\n        log_debug(\'Session opened.\')\n\n        # Prevent further graph changes\n        tfv1.get_default_graph().finalize()\n\n        # Load checkpoint or initialize variables\n        load_or_init_graph_for_training(session)\n\n        def run_set(set_name, epoch, init_op, dataset=None):\n            is_train = set_name == \'train\'\n            train_op = apply_gradient_op if is_train else []\n            feed_dict = dropout_feed_dict if is_train else no_dropout_feed_dict\n\n            total_loss = 0.0\n            step_count = 0\n\n            step_summary_writer = step_summary_writers.get(set_name)\n            checkpoint_time = time.time()\n\n            # Setup progress bar\n            class LossWidget(progressbar.widgets.FormatLabel):\n                def __init__(self):\n                    progressbar.widgets.FormatLabel.__init__(self, format=\'Loss: %(mean_loss)f\')\n\n                def __call__(self, progress, data, **kwargs):\n                    data[\'mean_loss\'] = total_loss / step_count if step_count else 0.0\n                    return progressbar.widgets.FormatLabel.__call__(self, progress, data, **kwargs)\n\n            prefix = \'Epoch {} | {:>10}\'.format(epoch, human_readable_set_names[set_name])\n            widgets = [\' | \', progressbar.widgets.Timer(),\n                       \' | Steps: \', progressbar.widgets.Counter(),\n                       \' | \', LossWidget()]\n            suffix = \' | Dataset: {}\'.format(dataset) if dataset else None\n            pbar = create_progressbar(prefix=prefix, widgets=widgets, suffix=suffix).start()\n\n            # Initialize iterator to the appropriate dataset\n            session.run(init_op)\n\n            # Batch loop\n            while True:\n                try:\n                    _, current_step, batch_loss, problem_files, step_summary = \\\n                        session.run([train_op, global_step, loss, non_finite_files, step_summaries_op],\n                                    feed_dict=feed_dict)\n                    exception_box.raise_if_set()\n                except tf.errors.InvalidArgumentError as err:\n                    if FLAGS.augmentation_sparse_warp:\n                        log_info(""Ignoring sparse warp error: {}"".format(err))\n                        continue\n                    raise\n                except tf.errors.OutOfRangeError:\n                    exception_box.raise_if_set()\n                    break\n\n                if problem_files.size > 0:\n                    problem_files = [f.decode(\'utf8\') for f in problem_files[..., 0]]\n                    log_error(\'The following files caused an infinite (or NaN) \'\n                              \'loss: {}\'.format(\',\'.join(problem_files)))\n\n                total_loss += batch_loss\n                step_count += 1\n\n                pbar.update(step_count)\n\n                step_summary_writer.add_summary(step_summary, current_step)\n\n                if is_train and FLAGS.checkpoint_secs > 0 and time.time() - checkpoint_time > FLAGS.checkpoint_secs:\n                    checkpoint_saver.save(session, checkpoint_path, global_step=current_step)\n                    checkpoint_time = time.time()\n\n            pbar.finish()\n            mean_loss = total_loss / step_count if step_count > 0 else 0.0\n            return mean_loss, step_count\n\n        log_info(\'STARTING Optimization\')\n        train_start_time = datetime.utcnow()\n        best_dev_loss = float(\'inf\')\n        dev_losses = []\n        epochs_without_improvement = 0\n        try:\n            for epoch in range(FLAGS.epochs):\n                # Training\n                log_progress(\'Training epoch %d...\' % epoch)\n                train_loss, _ = run_set(\'train\', epoch, train_init_op)\n                log_progress(\'Finished training epoch %d - loss: %f\' % (epoch, train_loss))\n                checkpoint_saver.save(session, checkpoint_path, global_step=global_step)\n\n                if FLAGS.dev_files:\n                    # Validation\n                    dev_loss = 0.0\n                    total_steps = 0\n                    for source, init_op in zip(dev_sources, dev_init_ops):\n                        log_progress(\'Validating epoch %d on %s...\' % (epoch, source))\n                        set_loss, steps = run_set(\'dev\', epoch, init_op, dataset=source)\n                        dev_loss += set_loss * steps\n                        total_steps += steps\n                        log_progress(\'Finished validating epoch %d on %s - loss: %f\' % (epoch, source, set_loss))\n\n                    dev_loss = dev_loss / total_steps\n                    dev_losses.append(dev_loss)\n\n                    # Count epochs without an improvement for early stopping and reduction of learning rate on a plateau\n                    # the improvement has to be greater than FLAGS.es_min_delta\n                    if dev_loss > best_dev_loss - FLAGS.es_min_delta:\n                        epochs_without_improvement += 1\n                    else:\n                        epochs_without_improvement = 0\n\n                    # Save new best model\n                    if dev_loss < best_dev_loss:\n                        best_dev_loss = dev_loss\n                        save_path = best_dev_saver.save(session, best_dev_path, global_step=global_step, latest_filename=\'best_dev_checkpoint\')\n                        log_info(""Saved new best validating model with loss %f to: %s"" % (best_dev_loss, save_path))\n\n                    # Early stopping\n                    if FLAGS.early_stop and epochs_without_improvement == FLAGS.es_epochs:\n                        log_info(\'Early stop triggered as the loss did not improve the last {} epochs\'.format(\n                            epochs_without_improvement))\n                        break\n\n                    # Reduce learning rate on plateau\n                    if (FLAGS.reduce_lr_on_plateau and\n                            epochs_without_improvement % FLAGS.plateau_epochs == 0 and epochs_without_improvement > 0):\n                        # If the learning rate was reduced and there is still no improvement\n                        # wait FLAGS.plateau_epochs before the learning rate is reduced again\n                        session.run(reduce_learning_rate_op)\n                        current_learning_rate = learning_rate_var.eval()\n                        log_info(\'Encountered a plateau, reducing learning rate to {}\'.format(\n                            current_learning_rate))\n\n                if FLAGS.metrics_files:\n                    # Read only metrics, not affecting best validation loss tracking\n                    for source, init_op in zip(metrics_sources, metrics_init_ops):\n                        log_progress(\'Metrics for epoch %d on %s...\' % (epoch, source))\n                        set_loss, _ = run_set(\'metrics\', epoch, init_op, dataset=source)\n                        log_progress(\'Metrics for epoch %d on %s - loss: %f\' % (epoch, source, set_loss))\n\n                print(\'-\' * 80)\n\n\n        except KeyboardInterrupt:\n            pass\n        log_info(\'FINISHED optimization in {}\'.format(datetime.utcnow() - train_start_time))\n    log_debug(\'Session closed.\')\n\n\ndef test():\n    samples = evaluate(FLAGS.test_files.split(\',\'), create_model)\n    if FLAGS.test_output_file:\n        save_samples_json(samples, FLAGS.test_output_file)\n\n\ndef create_inference_graph(batch_size=1, n_steps=16, tflite=False):\n    batch_size = batch_size if batch_size > 0 else None\n\n    # Create feature computation graph\n    input_samples = tfv1.placeholder(tf.float32, [Config.audio_window_samples], \'input_samples\')\n    samples = tf.expand_dims(input_samples, -1)\n    mfccs, _ = samples_to_mfccs(samples, FLAGS.audio_sample_rate)\n    mfccs = tf.identity(mfccs, name=\'mfccs\')\n\n    # Input tensor will be of shape [batch_size, n_steps, 2*n_context+1, n_input]\n    # This shape is read by the native_client in DS_CreateModel to know the\n    # value of n_steps, n_context and n_input. Make sure you update the code\n    # there if this shape is changed.\n    input_tensor = tfv1.placeholder(tf.float32, [batch_size, n_steps if n_steps > 0 else None, 2 * Config.n_context + 1, Config.n_input], name=\'input_node\')\n    seq_length = tfv1.placeholder(tf.int32, [batch_size], name=\'input_lengths\')\n\n    if batch_size <= 0:\n        # no state management since n_step is expected to be dynamic too (see below)\n        previous_state = None\n    else:\n        previous_state_c = tfv1.placeholder(tf.float32, [batch_size, Config.n_cell_dim], name=\'previous_state_c\')\n        previous_state_h = tfv1.placeholder(tf.float32, [batch_size, Config.n_cell_dim], name=\'previous_state_h\')\n\n        previous_state = tf.nn.rnn_cell.LSTMStateTuple(previous_state_c, previous_state_h)\n\n    # One rate per layer\n    no_dropout = [None] * 6\n\n    if tflite:\n        rnn_impl = rnn_impl_static_rnn\n    else:\n        rnn_impl = rnn_impl_lstmblockfusedcell\n\n    logits, layers = create_model(batch_x=input_tensor,\n                                  batch_size=batch_size,\n                                  seq_length=seq_length if not FLAGS.export_tflite else None,\n                                  dropout=no_dropout,\n                                  previous_state=previous_state,\n                                  overlap=False,\n                                  rnn_impl=rnn_impl)\n\n    # TF Lite runtime will check that input dimensions are 1, 2 or 4\n    # by default we get 3, the middle one being batch_size which is forced to\n    # one on inference graph, so remove that dimension\n    if tflite:\n        logits = tf.squeeze(logits, [1])\n\n    # Apply softmax for CTC decoder\n    logits = tf.nn.softmax(logits, name=\'logits\')\n\n    if batch_size <= 0:\n        if tflite:\n            raise NotImplementedError(\'dynamic batch_size does not support tflite nor streaming\')\n        if n_steps > 0:\n            raise NotImplementedError(\'dynamic batch_size expect n_steps to be dynamic too\')\n        return (\n            {\n                \'input\': input_tensor,\n                \'input_lengths\': seq_length,\n            },\n            {\n                \'outputs\': logits,\n            },\n            layers\n        )\n\n    new_state_c, new_state_h = layers[\'rnn_output_state\']\n    new_state_c = tf.identity(new_state_c, name=\'new_state_c\')\n    new_state_h = tf.identity(new_state_h, name=\'new_state_h\')\n\n    inputs = {\n        \'input\': input_tensor,\n        \'previous_state_c\': previous_state_c,\n        \'previous_state_h\': previous_state_h,\n        \'input_samples\': input_samples,\n    }\n\n    if not FLAGS.export_tflite:\n        inputs[\'input_lengths\'] = seq_length\n\n    outputs = {\n        \'outputs\': logits,\n        \'new_state_c\': new_state_c,\n        \'new_state_h\': new_state_h,\n        \'mfccs\': mfccs,\n    }\n\n    return inputs, outputs, layers\n\n\ndef file_relative_read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname)).read()\n\n\ndef export():\n    r\'\'\'\n    Restores the trained variables into a simpler graph that will be exported for serving.\n    \'\'\'\n    log_info(\'Exporting the model...\')\n\n    inputs, outputs, _ = create_inference_graph(batch_size=FLAGS.export_batch_size, n_steps=FLAGS.n_steps, tflite=FLAGS.export_tflite)\n\n    graph_version = int(file_relative_read(\'GRAPH_VERSION\').strip())\n    assert graph_version > 0\n\n    outputs[\'metadata_version\'] = tf.constant([graph_version], name=\'metadata_version\')\n    outputs[\'metadata_sample_rate\'] = tf.constant([FLAGS.audio_sample_rate], name=\'metadata_sample_rate\')\n    outputs[\'metadata_feature_win_len\'] = tf.constant([FLAGS.feature_win_len], name=\'metadata_feature_win_len\')\n    outputs[\'metadata_feature_win_step\'] = tf.constant([FLAGS.feature_win_step], name=\'metadata_feature_win_step\')\n    outputs[\'metadata_beam_width\'] = tf.constant([FLAGS.export_beam_width], name=\'metadata_beam_width\')\n    outputs[\'metadata_alphabet\'] = tf.constant([Config.alphabet.serialize()], name=\'metadata_alphabet\')\n\n    if FLAGS.export_language:\n        outputs[\'metadata_language\'] = tf.constant([FLAGS.export_language.encode(\'utf-8\')], name=\'metadata_language\')\n\n    # Prevent further graph changes\n    tfv1.get_default_graph().finalize()\n\n    output_names_tensors = [tensor.op.name for tensor in outputs.values() if isinstance(tensor, tf.Tensor)]\n    output_names_ops = [op.name for op in outputs.values() if isinstance(op, tf.Operation)]\n    output_names = output_names_tensors + output_names_ops\n\n    with tf.Session() as session:\n        # Restore variables from checkpoint\n        load_graph_for_evaluation(session)\n\n        output_filename = FLAGS.export_file_name + \'.pb\'\n        if FLAGS.remove_export:\n            if os.path.isdir(FLAGS.export_dir):\n                log_info(\'Removing old export\')\n                shutil.rmtree(FLAGS.export_dir)\n\n        output_graph_path = os.path.join(FLAGS.export_dir, output_filename)\n\n        if not os.path.isdir(FLAGS.export_dir):\n            os.makedirs(FLAGS.export_dir)\n\n        frozen_graph = tfv1.graph_util.convert_variables_to_constants(\n            sess=session,\n            input_graph_def=tfv1.get_default_graph().as_graph_def(),\n            output_node_names=output_names)\n\n        frozen_graph = tfv1.graph_util.extract_sub_graph(\n            graph_def=frozen_graph,\n            dest_nodes=output_names)\n\n        if not FLAGS.export_tflite:\n            with open(output_graph_path, \'wb\') as fout:\n                fout.write(frozen_graph.SerializeToString())\n        else:\n            output_tflite_path = os.path.join(FLAGS.export_dir, output_filename.replace(\'.pb\', \'.tflite\'))\n\n            converter = tf.lite.TFLiteConverter(frozen_graph, input_tensors=inputs.values(), output_tensors=outputs.values())\n            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n            # AudioSpectrogram and Mfcc ops are custom but have built-in kernels in TFLite\n            converter.allow_custom_ops = True\n            tflite_model = converter.convert()\n\n            with open(output_tflite_path, \'wb\') as fout:\n                fout.write(tflite_model)\n\n        log_info(\'Models exported at %s\' % (FLAGS.export_dir))\n\n    metadata_fname = os.path.join(FLAGS.export_dir, \'{}_{}_{}.md\'.format(\n        FLAGS.export_author_id,\n        FLAGS.export_model_name,\n        FLAGS.export_model_version))\n\n    model_runtime = \'tflite\' if FLAGS.export_tflite else \'tensorflow\'\n    with open(metadata_fname, \'w\') as f:\n        f.write(\'---\\n\')\n        f.write(\'author: {}\\n\'.format(FLAGS.export_author_id))\n        f.write(\'model_name: {}\\n\'.format(FLAGS.export_model_name))\n        f.write(\'model_version: {}\\n\'.format(FLAGS.export_model_version))\n        f.write(\'contact_info: {}\\n\'.format(FLAGS.export_contact_info))\n        f.write(\'license: {}\\n\'.format(FLAGS.export_license))\n        f.write(\'language: {}\\n\'.format(FLAGS.export_language))\n        f.write(\'runtime: {}\\n\'.format(model_runtime))\n        f.write(\'min_ds_version: {}\\n\'.format(FLAGS.export_min_ds_version))\n        f.write(\'max_ds_version: {}\\n\'.format(FLAGS.export_max_ds_version))\n        f.write(\'acoustic_model_url: <replace this with a publicly available URL of the acoustic model>\\n\')\n        f.write(\'scorer_url: <replace this with a publicly available URL of the scorer, if present>\\n\')\n        f.write(\'---\\n\')\n        f.write(\'{}\\n\'.format(FLAGS.export_description))\n\n    log_info(\'Model metadata file saved to {}. Before submitting the exported model for publishing make sure all information in the metadata file is correct, and complete the URL fields.\'.format(metadata_fname))\n\n\ndef package_zip():\n    # --export_dir path/to/export/LANG_CODE/ => path/to/export/LANG_CODE.zip\n    export_dir = os.path.join(os.path.abspath(FLAGS.export_dir), \'\') # Force ending \'/\'\n    zip_filename = os.path.dirname(export_dir)\n\n    shutil.copy(FLAGS.scorer_path, export_dir)\n\n    archive = shutil.make_archive(zip_filename, \'zip\', export_dir)\n    log_info(\'Exported packaged model {}\'.format(archive))\n\n\ndef do_single_file_inference(input_file_path):\n    with tfv1.Session(config=Config.session_config) as session:\n        inputs, outputs, _ = create_inference_graph(batch_size=1, n_steps=-1)\n\n        # Restore variables from training checkpoint\n        load_graph_for_evaluation(session)\n\n        features, features_len = audiofile_to_features(input_file_path)\n        previous_state_c = np.zeros([1, Config.n_cell_dim])\n        previous_state_h = np.zeros([1, Config.n_cell_dim])\n\n        # Add batch dimension\n        features = tf.expand_dims(features, 0)\n        features_len = tf.expand_dims(features_len, 0)\n\n        # Evaluate\n        features = create_overlapping_windows(features).eval(session=session)\n        features_len = features_len.eval(session=session)\n\n        logits = outputs[\'outputs\'].eval(feed_dict={\n            inputs[\'input\']: features,\n            inputs[\'input_lengths\']: features_len,\n            inputs[\'previous_state_c\']: previous_state_c,\n            inputs[\'previous_state_h\']: previous_state_h,\n        }, session=session)\n\n        logits = np.squeeze(logits)\n\n        if FLAGS.scorer_path:\n            scorer = Scorer(FLAGS.lm_alpha, FLAGS.lm_beta,\n                            FLAGS.scorer_path, Config.alphabet)\n        else:\n            scorer = None\n        decoded = ctc_beam_search_decoder(logits, Config.alphabet, FLAGS.beam_width,\n                                          scorer=scorer, cutoff_prob=FLAGS.cutoff_prob,\n                                          cutoff_top_n=FLAGS.cutoff_top_n)\n        # Print highest probability result\n        print(decoded[0][1])\n\n\ndef early_training_checks():\n    # Check for proper scorer early\n    if FLAGS.scorer_path:\n        scorer = Scorer(FLAGS.lm_alpha, FLAGS.lm_beta,\n                        FLAGS.scorer_path, Config.alphabet)\n        del scorer\n\n    if FLAGS.train_files and FLAGS.test_files and FLAGS.load_checkpoint_dir != FLAGS.save_checkpoint_dir:\n        log_warn(\'WARNING: You specified different values for --load_checkpoint_dir \'\n                 \'and --save_checkpoint_dir, but you are running training and testing \'\n                 \'in a single invocation. The testing step will respect --load_checkpoint_dir, \'\n                 \'and thus WILL NOT TEST THE CHECKPOINT CREATED BY THE TRAINING STEP. \'\n                 \'Train and test in two separate invocations, specifying the correct \'\n                 \'--load_checkpoint_dir in both cases, or use the same location \'\n                 \'for loading and saving.\')\n\n\ndef main(_):\n    initialize_globals()\n    early_training_checks()\n\n    if FLAGS.train_files:\n        tfv1.reset_default_graph()\n        tfv1.set_random_seed(FLAGS.random_seed)\n        train()\n\n    if FLAGS.test_files:\n        tfv1.reset_default_graph()\n        test()\n\n    if FLAGS.export_dir and not FLAGS.export_zip:\n        tfv1.reset_default_graph()\n        export()\n\n    if FLAGS.export_zip:\n        tfv1.reset_default_graph()\n        FLAGS.export_tflite = True\n\n        if os.listdir(FLAGS.export_dir):\n            log_error(\'Directory {} is not empty, please fix this.\'.format(FLAGS.export_dir))\n            sys.exit(1)\n\n        export()\n        package_zip()\n\n    if FLAGS.one_shot_infer:\n        tfv1.reset_default_graph()\n        do_single_file_inference(FLAGS.one_shot_infer)\n\n\ndef run_script():\n    create_flags()\n    absl.app.run(main)\n\nif __name__ == \'__main__\':\n    run_script()\n'"
training/deepspeech_training/util/__init__.py,0,b''
training/deepspeech_training/util/audio.py,0,"b'import os\nimport io\nimport wave\nimport math\nimport tempfile\nimport collections\nimport numpy as np\n\nfrom .helpers import LimitingPool\nfrom collections import namedtuple\n\nAudioFormat = namedtuple(\'AudioFormat\', \'rate channels width\')\n\nDEFAULT_RATE = 16000\nDEFAULT_CHANNELS = 1\nDEFAULT_WIDTH = 2\nDEFAULT_FORMAT = AudioFormat(DEFAULT_RATE, DEFAULT_CHANNELS, DEFAULT_WIDTH)\n\nAUDIO_TYPE_NP = \'application/vnd.mozilla.np\'\nAUDIO_TYPE_PCM = \'application/vnd.mozilla.pcm\'\nAUDIO_TYPE_WAV = \'audio/wav\'\nAUDIO_TYPE_OPUS = \'application/vnd.mozilla.opus\'\nSERIALIZABLE_AUDIO_TYPES = [AUDIO_TYPE_WAV, AUDIO_TYPE_OPUS]\nLOADABLE_AUDIO_EXTENSIONS = {\'.wav\': AUDIO_TYPE_WAV}\n\nOPUS_PCM_LEN_SIZE = 4\nOPUS_RATE_SIZE = 4\nOPUS_CHANNELS_SIZE = 1\nOPUS_WIDTH_SIZE = 1\nOPUS_CHUNK_LEN_SIZE = 2\n\n\nclass Sample:\n    """"""\n    Represents in-memory audio data of a certain (convertible) representation.\n\n    Attributes\n    ----------\n    audio_type : str\n        See `__init__`.\n    audio_format : util.audio.AudioFormat\n        See `__init__`.\n    audio : binary\n        Audio data represented as indicated by `audio_type`\n    duration : float\n        Audio duration of the sample in seconds\n    """"""\n    def __init__(self, audio_type, raw_data, audio_format=None, sample_id=None):\n        """"""\n        Parameters\n        ----------\n        audio_type : str\n            Audio data representation type\n            Supported types:\n                - util.audio.AUDIO_TYPE_OPUS: Memory file representation (BytesIO) of Opus encoded audio\n                    wrapped by a custom container format (used in SDBs)\n                - util.audio.AUDIO_TYPE_WAV: Memory file representation (BytesIO) of a Wave file\n                - util.audio.AUDIO_TYPE_PCM: Binary representation (bytearray) of PCM encoded audio data (Wave file without header)\n                - util.audio.AUDIO_TYPE_NP: NumPy representation of audio data (np.float32) - typically used for GPU feeding\n        raw_data : binary\n            Audio data in the form of the provided representation type (see audio_type).\n            For types util.audio.AUDIO_TYPE_OPUS or util.audio.AUDIO_TYPE_WAV data can also be passed as a bytearray.\n        audio_format : util.audio.AudioFormat\n            Required in case of audio_type = util.audio.AUDIO_TYPE_PCM or util.audio.AUDIO_TYPE_NP,\n            as this information cannot be derived from raw audio data.\n        sample_id : str\n            Tracking ID - should indicate sample\'s origin as precisely as possible\n        """"""\n        self.audio_type = audio_type\n        self.audio_format = audio_format\n        self.sample_id = sample_id\n        if audio_type in SERIALIZABLE_AUDIO_TYPES:\n            self.audio = raw_data if isinstance(raw_data, io.BytesIO) else io.BytesIO(raw_data)\n            self.duration = read_duration(audio_type, self.audio)\n        else:\n            self.audio = raw_data\n            if self.audio_format is None:\n                raise ValueError(\'For audio type ""{}"" parameter ""audio_format"" is mandatory\'.format(self.audio_type))\n            if audio_type == AUDIO_TYPE_PCM:\n                self.duration = get_pcm_duration(len(self.audio), self.audio_format)\n            elif audio_type == AUDIO_TYPE_NP:\n                self.duration = get_np_duration(len(self.audio), self.audio_format)\n            else:\n                raise ValueError(\'Unsupported audio type: {}\'.format(self.audio_type))\n\n    def change_audio_type(self, new_audio_type, bitrate=None):\n        """"""\n        In-place conversion of audio data into a different representation.\n\n        Parameters\n        ----------\n        new_audio_type : str\n            New audio-type - see `__init__`.\n        bitrate : int\n            Bitrate to use in case of converting to a lossy audio-type.\n        """"""\n        if self.audio_type == new_audio_type:\n            return\n        if new_audio_type == AUDIO_TYPE_PCM and self.audio_type in SERIALIZABLE_AUDIO_TYPES:\n            self.audio_format, audio = read_audio(self.audio_type, self.audio)\n            self.audio.close()\n            self.audio = audio\n        elif new_audio_type == AUDIO_TYPE_PCM and self.audio_type == AUDIO_TYPE_NP:\n            self.audio = np_to_pcm(self.audio, self.audio_format)\n        elif new_audio_type == AUDIO_TYPE_NP:\n            self.change_audio_type(AUDIO_TYPE_PCM)\n            self.audio = pcm_to_np(self.audio, self.audio_format)\n        elif new_audio_type in SERIALIZABLE_AUDIO_TYPES:\n            self.change_audio_type(AUDIO_TYPE_PCM)\n            audio_bytes = io.BytesIO()\n            write_audio(new_audio_type, audio_bytes, self.audio, audio_format=self.audio_format, bitrate=bitrate)\n            audio_bytes.seek(0)\n            self.audio = audio_bytes\n        else:\n            raise RuntimeError(\'Changing audio representation type from ""{}"" to ""{}"" not supported\'\n                               .format(self.audio_type, new_audio_type))\n        self.audio_type = new_audio_type\n\n\ndef _change_audio_type(sample_and_audio_type):\n    sample, audio_type, bitrate = sample_and_audio_type\n    sample.change_audio_type(audio_type, bitrate=bitrate)\n    return sample\n\n\ndef change_audio_types(samples, audio_type=AUDIO_TYPE_PCM, bitrate=None, processes=None, process_ahead=None):\n    with LimitingPool(processes=processes, process_ahead=process_ahead) as pool:\n        yield from pool.imap(_change_audio_type, map(lambda s: (s, audio_type, bitrate), samples))\n\n\ndef get_audio_type_from_extension(ext):\n    if ext in LOADABLE_AUDIO_EXTENSIONS:\n        return LOADABLE_AUDIO_EXTENSIONS[ext]\n    return None\n\n\ndef read_audio_format_from_wav_file(wav_file):\n    return AudioFormat(wav_file.getframerate(), wav_file.getnchannels(), wav_file.getsampwidth())\n\n\ndef get_num_samples(pcm_buffer_size, audio_format=DEFAULT_FORMAT):\n    return pcm_buffer_size // (audio_format.channels * audio_format.width)\n\n\ndef get_pcm_duration(pcm_buffer_size, audio_format=DEFAULT_FORMAT):\n    """"""Calculates duration in seconds of a binary PCM buffer (typically read from a WAV file)""""""\n    return get_num_samples(pcm_buffer_size, audio_format) / audio_format.rate\n\n\ndef get_np_duration(np_len, audio_format=DEFAULT_FORMAT):\n    """"""Calculates duration in seconds of NumPy audio data""""""\n    return np_len / audio_format.rate\n\n\ndef convert_audio(src_audio_path, dst_audio_path, file_type=None, audio_format=DEFAULT_FORMAT):\n    import sox\n    transformer = sox.Transformer()\n    transformer.set_output_format(file_type=file_type,\n                                  rate=audio_format.rate,\n                                  channels=audio_format.channels,\n                                  bits=audio_format.width * 8)\n    transformer.build(src_audio_path, dst_audio_path)\n\n\nclass AudioFile:\n    def __init__(self, audio_path, as_path=False, audio_format=DEFAULT_FORMAT):\n        self.audio_path = audio_path\n        self.audio_format = audio_format\n        self.as_path = as_path\n        self.open_file = None\n        self.tmp_file_path = None\n\n    def __enter__(self):\n        if self.audio_path.endswith(\'.wav\'):\n            self.open_file = wave.open(self.audio_path, \'r\')\n            if read_audio_format_from_wav_file(self.open_file) == self.audio_format:\n                if self.as_path:\n                    self.open_file.close()\n                    return self.audio_path\n                return self.open_file\n            self.open_file.close()\n        _, self.tmp_file_path = tempfile.mkstemp(suffix=\'.wav\')\n        convert_audio(self.audio_path, self.tmp_file_path, file_type=\'wav\', audio_format=self.audio_format)\n        if self.as_path:\n            return self.tmp_file_path\n        self.open_file = wave.open(self.tmp_file_path, \'r\')\n        return self.open_file\n\n    def __exit__(self, *args):\n        if not self.as_path:\n            self.open_file.close()\n        if self.tmp_file_path is not None:\n            os.remove(self.tmp_file_path)\n\n\ndef read_frames(wav_file, frame_duration_ms=30, yield_remainder=False):\n    audio_format = read_audio_format_from_wav_file(wav_file)\n    frame_size = int(audio_format.rate * (frame_duration_ms / 1000.0))\n    while True:\n        try:\n            data = wav_file.readframes(frame_size)\n            if not yield_remainder and get_pcm_duration(len(data), audio_format) * 1000 < frame_duration_ms:\n                break\n            yield data\n        except EOFError:\n            break\n\n\ndef read_frames_from_file(audio_path, audio_format=DEFAULT_FORMAT, frame_duration_ms=30, yield_remainder=False):\n    with AudioFile(audio_path, audio_format=audio_format) as wav_file:\n        for frame in read_frames(wav_file, frame_duration_ms=frame_duration_ms, yield_remainder=yield_remainder):\n            yield frame\n\n\ndef vad_split(audio_frames,\n              audio_format=DEFAULT_FORMAT,\n              num_padding_frames=10,\n              threshold=0.5,\n              aggressiveness=3):\n    from webrtcvad import Vad  # pylint: disable=import-outside-toplevel\n    if audio_format.channels != 1:\n        raise ValueError(\'VAD-splitting requires mono samples\')\n    if audio_format.width != 2:\n        raise ValueError(\'VAD-splitting requires 16 bit samples\')\n    if audio_format.rate not in [8000, 16000, 32000, 48000]:\n        raise ValueError(\'VAD-splitting only supported for sample rates 8000, 16000, 32000, or 48000\')\n    if aggressiveness not in [0, 1, 2, 3]:\n        raise ValueError(\'VAD-splitting aggressiveness mode has to be one of 0, 1, 2, or 3\')\n    ring_buffer = collections.deque(maxlen=num_padding_frames)\n    triggered = False\n    vad = Vad(int(aggressiveness))\n    voiced_frames = []\n    frame_duration_ms = 0\n    frame_index = 0\n    for frame_index, frame in enumerate(audio_frames):\n        frame_duration_ms = get_pcm_duration(len(frame), audio_format) * 1000\n        if int(frame_duration_ms) not in [10, 20, 30]:\n            raise ValueError(\'VAD-splitting only supported for frame durations 10, 20, or 30 ms\')\n        is_speech = vad.is_speech(frame, audio_format.rate)\n        if not triggered:\n            ring_buffer.append((frame, is_speech))\n            num_voiced = len([f for f, speech in ring_buffer if speech])\n            if num_voiced > threshold * ring_buffer.maxlen:\n                triggered = True\n                for f, s in ring_buffer:\n                    voiced_frames.append(f)\n                ring_buffer.clear()\n        else:\n            voiced_frames.append(frame)\n            ring_buffer.append((frame, is_speech))\n            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n            if num_unvoiced > threshold * ring_buffer.maxlen:\n                triggered = False\n                yield b\'\'.join(voiced_frames), \\\n                      frame_duration_ms * max(0, frame_index - len(voiced_frames)), \\\n                      frame_duration_ms * frame_index\n                ring_buffer.clear()\n                voiced_frames = []\n    if len(voiced_frames) > 0:\n        yield b\'\'.join(voiced_frames), \\\n              frame_duration_ms * (frame_index - len(voiced_frames)), \\\n              frame_duration_ms * (frame_index + 1)\n\n\ndef pack_number(n, num_bytes):\n    return n.to_bytes(num_bytes, \'big\', signed=False)\n\n\ndef unpack_number(data):\n    return int.from_bytes(data, \'big\', signed=False)\n\n\ndef get_opus_frame_size(rate):\n    return 60 * rate // 1000\n\n\ndef write_opus(opus_file, audio_data, audio_format=DEFAULT_FORMAT, bitrate=None):\n    frame_size = get_opus_frame_size(audio_format.rate)\n    import opuslib  # pylint: disable=import-outside-toplevel\n    encoder = opuslib.Encoder(audio_format.rate, audio_format.channels, \'audio\')\n    if bitrate is not None:\n        encoder.bitrate = bitrate\n    chunk_size = frame_size * audio_format.channels * audio_format.width\n    opus_file.write(pack_number(len(audio_data), OPUS_PCM_LEN_SIZE))\n    opus_file.write(pack_number(audio_format.rate, OPUS_RATE_SIZE))\n    opus_file.write(pack_number(audio_format.channels, OPUS_CHANNELS_SIZE))\n    opus_file.write(pack_number(audio_format.width, OPUS_WIDTH_SIZE))\n    for i in range(0, len(audio_data), chunk_size):\n        chunk = audio_data[i:i + chunk_size]\n        # Preventing non-deterministic encoding results from uninitialized remainder of the encoder buffer\n        if len(chunk) < chunk_size:\n            chunk = chunk + b\'\\0\' * (chunk_size - len(chunk))\n        encoded = encoder.encode(chunk, frame_size)\n        opus_file.write(pack_number(len(encoded), OPUS_CHUNK_LEN_SIZE))\n        opus_file.write(encoded)\n\n\ndef read_opus_header(opus_file):\n    opus_file.seek(0)\n    pcm_buffer_size = unpack_number(opus_file.read(OPUS_PCM_LEN_SIZE))\n    rate = unpack_number(opus_file.read(OPUS_RATE_SIZE))\n    channels = unpack_number(opus_file.read(OPUS_CHANNELS_SIZE))\n    width = unpack_number(opus_file.read(OPUS_WIDTH_SIZE))\n    return pcm_buffer_size, AudioFormat(rate, channels, width)\n\n\ndef read_opus(opus_file):\n    pcm_buffer_size, audio_format = read_opus_header(opus_file)\n    frame_size = get_opus_frame_size(audio_format.rate)\n    import opuslib  # pylint: disable=import-outside-toplevel\n    decoder = opuslib.Decoder(audio_format.rate, audio_format.channels)\n    audio_data = bytearray()\n    while len(audio_data) < pcm_buffer_size:\n        chunk_len = unpack_number(opus_file.read(OPUS_CHUNK_LEN_SIZE))\n        chunk = opus_file.read(chunk_len)\n        decoded = decoder.decode(chunk, frame_size)\n        audio_data.extend(decoded)\n    audio_data = audio_data[:pcm_buffer_size]\n    return audio_format, bytes(audio_data)\n\n\ndef write_wav(wav_file, pcm_data, audio_format=DEFAULT_FORMAT):\n    with wave.open(wav_file, \'wb\') as wav_file_writer:\n        wav_file_writer.setframerate(audio_format.rate)\n        wav_file_writer.setnchannels(audio_format.channels)\n        wav_file_writer.setsampwidth(audio_format.width)\n        wav_file_writer.writeframes(pcm_data)\n\n\ndef read_wav(wav_file):\n    wav_file.seek(0)\n    with wave.open(wav_file, \'rb\') as wav_file_reader:\n        audio_format = read_audio_format_from_wav_file(wav_file_reader)\n        pcm_data = wav_file_reader.readframes(wav_file_reader.getnframes())\n        return audio_format, pcm_data\n\n\ndef read_audio(audio_type, audio_file):\n    if audio_type == AUDIO_TYPE_WAV:\n        return read_wav(audio_file)\n    if audio_type == AUDIO_TYPE_OPUS:\n        return read_opus(audio_file)\n    raise ValueError(\'Unsupported audio type: {}\'.format(audio_type))\n\n\ndef write_audio(audio_type, audio_file, pcm_data, audio_format=DEFAULT_FORMAT, bitrate=None):\n    if audio_type == AUDIO_TYPE_WAV:\n        return write_wav(audio_file, pcm_data, audio_format=audio_format)\n    if audio_type == AUDIO_TYPE_OPUS:\n        return write_opus(audio_file, pcm_data, audio_format=audio_format, bitrate=bitrate)\n    raise ValueError(\'Unsupported audio type: {}\'.format(audio_type))\n\n\ndef read_wav_duration(wav_file):\n    wav_file.seek(0)\n    with wave.open(wav_file, \'rb\') as wav_file_reader:\n        return wav_file_reader.getnframes() / wav_file_reader.getframerate()\n\n\ndef read_opus_duration(opus_file):\n    pcm_buffer_size, audio_format = read_opus_header(opus_file)\n    return get_pcm_duration(pcm_buffer_size, audio_format)\n\n\ndef read_duration(audio_type, audio_file):\n    if audio_type == AUDIO_TYPE_WAV:\n        return read_wav_duration(audio_file)\n    if audio_type == AUDIO_TYPE_OPUS:\n        return read_opus_duration(audio_file)\n    raise ValueError(\'Unsupported audio type: {}\'.format(audio_type))\n\n\ndef get_dtype(audio_format):\n    if audio_format.width not in [1, 2, 4]:\n        raise ValueError(\'Unsupported sample width: {}\'.format(audio_format.width))\n    return [None, np.int8, np.int16, None, np.int32][audio_format.width]\n\n\ndef pcm_to_np(pcm_data, audio_format=DEFAULT_FORMAT):\n    assert audio_format.channels == 1  # only mono supported for now\n    dtype = get_dtype(audio_format)\n    samples = np.frombuffer(pcm_data, dtype=dtype)\n    samples = samples.astype(np.float32) / np.iinfo(dtype).max\n    return np.expand_dims(samples, axis=1)\n\n\ndef np_to_pcm(np_data, audio_format=DEFAULT_FORMAT):\n    assert audio_format.channels == 1  # only mono supported for now\n    dtype = get_dtype(audio_format)\n    np_data = np_data.squeeze()\n    np_data = np_data * np.iinfo(dtype).max\n    np_data = np_data.astype(dtype)\n    return np_data.tobytes()\n\n\ndef rms_to_dbfs(rms):\n    return 20.0 * math.log10(max(1e-16, rms)) + 3.0103\n\n\ndef max_dbfs(sample_data):\n    # Peak dBFS based on the maximum energy sample. Will prevent overdrive if used for normalization.\n    return rms_to_dbfs(max(abs(np.min(sample_data)), abs(np.max(sample_data))))\n\n\ndef mean_dbfs(sample_data):\n    return rms_to_dbfs(math.sqrt(np.mean(np.square(sample_data, dtype=np.float64))))\n\n\ndef gain_db_to_ratio(gain_db):\n    return math.pow(10.0, gain_db / 20.0)\n\n\ndef normalize_audio(sample_data, dbfs=3.0103):\n    return np.maximum(np.minimum(sample_data * gain_db_to_ratio(dbfs - max_dbfs(sample_data)), 1.0), -1.0)\n'"
training/deepspeech_training/util/check_characters.py,0,"b'""""""\nUsage: $ python3 check_characters.py ""INFILE""\n e.g.  $ python3 check_characters.py -csv /home/data/french.csv\n e.g.  $ python3 check_characters.py -csv ../train.csv,../test.csv\n e.g.  $ python3 check_characters.py -alpha -csv ../train.csv\n\nPoint this script to your transcripts, and it returns\nto the terminal the unique set of characters in those\nfiles (combined).\n\nThese files are assumed to be csv, with the transcript being the third field.\n\nThe script simply reads all the text from all the files,\nstoring a set of unique characters that were seen\nalong the way.\n""""""\nimport argparse\nimport csv\nimport os\nimport sys\nimport unicodedata\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(""-csv"", ""--csv-files"", help=""Str. Filenames as a comma separated list"", required=True)\n    parser.add_argument(""-alpha"", ""--alphabet-format"", help=""Bool. Print in format for alphabet.txt"", action=""store_true"")\n    parser.add_argument(""-unicode"", ""--disable-unicode-variants"", help=""Bool. DISABLE check for unicode consistency (use with --alphabet-format)"", action=""store_true"")\n    args = parser.parse_args()\n    in_files = [os.path.abspath(i) for i in args.csv_files.split("","")]\n\n    print(""### Reading in the following transcript files: ###"")\n    print(""### {} ###"".format(in_files))\n\n    all_text = set()\n    for in_file in in_files:\n        with open(in_file, ""r"") as csv_file:\n            reader = csv.reader(csv_file)\n            try:\n                next(reader, None)  # skip the file header (i.e. ""transcript"")\n                for row in reader:\n                    if not args.disable_unicode_variants:\n                        unicode_transcript = unicodedata.normalize(""NFKC"", row[2])\n                        if row[2] != unicode_transcript:\n                            print(""Your input file"", in_file, ""contains at least one transript with unicode chars on more than one code-point: \'{}\'. Consider using NFKC normalization: unicodedata.normalize(\'NFKC\', str)."".format(row[2]))\n                            sys.exit(-1)\n                    all_text |= set(row[2])\n            except IndexError:\n                print(""Your input file"", in_file, ""is not formatted properly. Check if there are 3 columns with the 3rd containing the transcript"")\n                sys.exit(-1)\n            finally:\n                csv_file.close()\n\n    print(""### The following unique characters were found in your transcripts: ###"")\n    if args.alphabet_format:\n        for char in list(all_text):\n            print(char)\n        print(""### ^^^ You can copy-paste these into data/alphabet.txt ###"")\n    else:\n        print(list(all_text))\n\nif __name__ == \'__main__\':\n    main()\n'"
training/deepspeech_training/util/checkpoints.py,0,"b""import sys\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tfv1\n\nfrom .flags import FLAGS\nfrom .logging import log_info, log_error, log_warn\n\n\ndef _load_checkpoint(session, checkpoint_path, allow_drop_layers):\n    # Load the checkpoint and put all variables into loading list\n    # we will exclude variables we do not wish to load and then\n    # we will initialize them instead\n    ckpt = tfv1.train.load_checkpoint(checkpoint_path)\n    vars_in_ckpt = frozenset(ckpt.get_variable_to_shape_map().keys())\n    load_vars = set(tfv1.global_variables())\n    init_vars = set()\n\n    # We explicitly allow the learning rate variable to be missing for backwards\n    # compatibility with older checkpoints.\n    lr_var = set(v for v in load_vars if v.op.name == 'learning_rate')\n    if lr_var and ('learning_rate' not in vars_in_ckpt or FLAGS.force_initialize_learning_rate):\n        assert len(lr_var) <= 1\n        load_vars -= lr_var\n        init_vars |= lr_var\n\n    if FLAGS.load_cudnn:\n        # Initialize training from a CuDNN RNN checkpoint\n        # Identify the variables which we cannot load, and set them\n        # for initialization\n        missing_vars = set()\n        for v in load_vars:\n            if v.op.name not in vars_in_ckpt:\n                log_warn('CUDNN variable not found: %s' % (v.op.name))\n                missing_vars.add(v)\n                init_vars.add(v)\n\n        load_vars -= init_vars\n\n        # Check that the only missing variables (i.e. those to be initialised)\n        # are the Adam moment tensors, if they aren't then we have an issue\n        missing_var_names = [v.op.name for v in missing_vars]\n        if any('Adam' not in v for v in missing_var_names):\n            log_error('Tried to load a CuDNN RNN checkpoint but there were '\n                      'more missing variables than just the Adam moment '\n                      'tensors. Missing variables: {}'.format(missing_var_names))\n            sys.exit(1)\n\n    if allow_drop_layers and FLAGS.drop_source_layers > 0:\n        # This transfer learning approach requires supplying\n        # the layers which we exclude from the source model.\n        # Say we want to exclude all layers except for the first one,\n        # then we are dropping five layers total, so: drop_source_layers=5\n        # If we want to use all layers from the source model except\n        # the last one, we use this: drop_source_layers=1\n        if FLAGS.drop_source_layers >= 6:\n            log_warn('The checkpoint only has 6 layers, but you are trying to drop '\n                     'all of them or more than all of them. Continuing and '\n                     'dropping only 5 layers.')\n            FLAGS.drop_source_layers = 5\n\n        dropped_layers = ['2', '3', 'lstm', '5', '6'][-1 * int(FLAGS.drop_source_layers):]\n        # Initialize all variables needed for DS, but not loaded from ckpt\n        for v in load_vars:\n            if any(layer in v.op.name for layer in dropped_layers):\n                init_vars.add(v)\n        load_vars -= init_vars\n\n    for v in sorted(load_vars, key=lambda v: v.op.name):\n        log_info('Loading variable from checkpoint: %s' % (v.op.name))\n        v.load(ckpt.get_tensor(v.op.name), session=session)\n\n    for v in sorted(init_vars, key=lambda v: v.op.name):\n        log_info('Initializing variable: %s' % (v.op.name))\n        session.run(v.initializer)\n\n\ndef _checkpoint_path_or_none(checkpoint_filename):\n    checkpoint = tfv1.train.get_checkpoint_state(FLAGS.load_checkpoint_dir, checkpoint_filename)\n    if not checkpoint:\n        return None\n    return checkpoint.model_checkpoint_path\n\n\ndef _initialize_all_variables(session):\n    init_vars = tfv1.global_variables()\n    for v in init_vars:\n        session.run(v.initializer)\n\n\ndef _load_or_init_impl(session, method_order, allow_drop_layers):\n    for method in method_order:\n        # Load best validating checkpoint, saved in checkpoint file 'best_dev_checkpoint'\n        if method == 'best':\n            ckpt_path = _checkpoint_path_or_none('best_dev_checkpoint')\n            if ckpt_path:\n                log_info('Loading best validating checkpoint from {}'.format(ckpt_path))\n                return _load_checkpoint(session, ckpt_path, allow_drop_layers)\n            log_info('Could not find best validating checkpoint.')\n\n        # Load most recent checkpoint, saved in checkpoint file 'checkpoint'\n        elif method == 'last':\n            ckpt_path = _checkpoint_path_or_none('checkpoint')\n            if ckpt_path:\n                log_info('Loading most recent checkpoint from {}'.format(ckpt_path))\n                return _load_checkpoint(session, ckpt_path, allow_drop_layers)\n            log_info('Could not find most recent checkpoint.')\n\n        # Initialize all variables\n        elif method == 'init':\n            log_info('Initializing all variables.')\n            return _initialize_all_variables(session)\n\n        else:\n            log_error('Unknown initialization method: {}'.format(method))\n            sys.exit(1)\n\n    log_error('All initialization methods failed ({}).'.format(method_order))\n    sys.exit(1)\n\n\ndef load_or_init_graph_for_training(session):\n    '''\n    Load variables from checkpoint or initialize variables. By default this will\n    try to load the best validating checkpoint, then try the last checkpoint,\n    and finally initialize the weights from scratch. This can be overriden with\n    the `--load_train` flag. See its documentation for more info.\n    '''\n    if FLAGS.load_train == 'auto':\n        methods = ['best', 'last', 'init']\n    else:\n        methods = [FLAGS.load_train]\n    _load_or_init_impl(session, methods, allow_drop_layers=True)\n\n\ndef load_graph_for_evaluation(session):\n    '''\n    Load variables from checkpoint. Initialization is not allowed. By default\n    this will try to load the best validating checkpoint, then try the last\n    checkpoint. This can be overriden with the `--load_evaluate` flag. See its\n    documentation for more info.\n    '''\n    if FLAGS.load_evaluate == 'auto':\n        methods = ['best', 'last']\n    else:\n        methods = [FLAGS.load_evaluate]\n    _load_or_init_impl(session, methods, allow_drop_layers=False)\n"""
training/deepspeech_training/util/config.py,0,"b'from __future__ import absolute_import, division, print_function\n\nimport os\nimport sys\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tfv1\n\nfrom attrdict import AttrDict\nfrom xdg import BaseDirectory as xdg\n\nfrom .flags import FLAGS\nfrom .gpu import get_available_gpus\nfrom .logging import log_error, log_warn\nfrom .text import Alphabet, UTF8Alphabet\nfrom .helpers import parse_file_size\n\nclass ConfigSingleton:\n    _config = None\n\n    def __getattr__(self, name):\n        if not ConfigSingleton._config:\n            raise RuntimeError(""Global configuration not yet initialized."")\n        if not hasattr(ConfigSingleton._config, name):\n            raise RuntimeError(""Configuration option {} not found in config."".format(name))\n        return ConfigSingleton._config[name]\n\n\nConfig = ConfigSingleton() # pylint: disable=invalid-name\n\ndef initialize_globals():\n    c = AttrDict()\n\n    # Read-buffer\n    FLAGS.read_buffer = parse_file_size(FLAGS.read_buffer)\n\n    # Set default dropout rates\n    if FLAGS.dropout_rate2 < 0:\n        FLAGS.dropout_rate2 = FLAGS.dropout_rate\n    if FLAGS.dropout_rate3 < 0:\n        FLAGS.dropout_rate3 = FLAGS.dropout_rate\n    if FLAGS.dropout_rate6 < 0:\n        FLAGS.dropout_rate6 = FLAGS.dropout_rate\n\n    # Set default checkpoint dir\n    if not FLAGS.checkpoint_dir:\n        FLAGS.checkpoint_dir = xdg.save_data_path(os.path.join(\'deepspeech\', \'checkpoints\'))\n\n    if FLAGS.load_train not in [\'last\', \'best\', \'init\', \'auto\']:\n        FLAGS.load_train = \'auto\'\n\n    if FLAGS.load_evaluate not in [\'last\', \'best\', \'auto\']:\n        FLAGS.load_evaluate = \'auto\'\n\n    # Set default summary dir\n    if not FLAGS.summary_dir:\n        FLAGS.summary_dir = xdg.save_data_path(os.path.join(\'deepspeech\', \'summaries\'))\n\n    # Standard session configuration that\'ll be used for all new sessions.\n    c.session_config = tfv1.ConfigProto(allow_soft_placement=True, log_device_placement=FLAGS.log_placement,\n                                        inter_op_parallelism_threads=FLAGS.inter_op_parallelism_threads,\n                                        intra_op_parallelism_threads=FLAGS.intra_op_parallelism_threads,\n                                        gpu_options=tfv1.GPUOptions(allow_growth=FLAGS.use_allow_growth))\n\n    # CPU device\n    c.cpu_device = \'/cpu:0\'\n\n    # Available GPU devices\n    c.available_devices = get_available_gpus(c.session_config)\n\n    # If there is no GPU available, we fall back to CPU based operation\n    if not c.available_devices:\n        c.available_devices = [c.cpu_device]\n\n    if FLAGS.utf8:\n        c.alphabet = UTF8Alphabet()\n    else:\n        c.alphabet = Alphabet(os.path.abspath(FLAGS.alphabet_config_path))\n\n    # Geometric Constants\n    # ===================\n\n    # For an explanation of the meaning of the geometric constants, please refer to\n    # doc/Geometry.md\n\n    # Number of MFCC features\n    c.n_input = 26 # TODO: Determine this programmatically from the sample rate\n\n    # The number of frames in the context\n    c.n_context = 9 # TODO: Determine the optimal value using a validation data set\n\n    # Number of units in hidden layers\n    c.n_hidden = FLAGS.n_hidden\n\n    c.n_hidden_1 = c.n_hidden\n\n    c.n_hidden_2 = c.n_hidden\n\n    c.n_hidden_5 = c.n_hidden\n\n    # LSTM cell state dimension\n    c.n_cell_dim = c.n_hidden\n\n    # The number of units in the third layer, which feeds in to the LSTM\n    c.n_hidden_3 = c.n_cell_dim\n\n    # Units in the sixth layer = number of characters in the target language plus one\n    c.n_hidden_6 = c.alphabet.size() + 1 # +1 for CTC blank label\n\n    # Size of audio window in samples\n    if (FLAGS.feature_win_len * FLAGS.audio_sample_rate) % 1000 != 0:\n        log_error(\'--feature_win_len value ({}) in milliseconds ({}) multiplied \'\n                  \'by --audio_sample_rate value ({}) must be an integer value. Adjust \'\n                  \'your --feature_win_len value or resample your audio accordingly.\'\n                  \'\'.format(FLAGS.feature_win_len, FLAGS.feature_win_len / 1000, FLAGS.audio_sample_rate))\n        sys.exit(1)\n\n    c.audio_window_samples = FLAGS.audio_sample_rate * (FLAGS.feature_win_len / 1000)\n\n    # Stride for feature computations in samples\n    if (FLAGS.feature_win_step * FLAGS.audio_sample_rate) % 1000 != 0:\n        log_error(\'--feature_win_step value ({}) in milliseconds ({}) multiplied \'\n                  \'by --audio_sample_rate value ({}) must be an integer value. Adjust \'\n                  \'your --feature_win_step value or resample your audio accordingly.\'\n                  \'\'.format(FLAGS.feature_win_step, FLAGS.feature_win_step / 1000, FLAGS.audio_sample_rate))\n        sys.exit(1)\n\n    c.audio_step_samples = FLAGS.audio_sample_rate * (FLAGS.feature_win_step / 1000)\n\n    if FLAGS.one_shot_infer:\n        if not os.path.exists(FLAGS.one_shot_infer):\n            log_error(\'Path specified in --one_shot_infer is not a valid file.\')\n            sys.exit(1)\n\n    if FLAGS.train_cudnn and FLAGS.load_cudnn:\n        log_error(\'Trying to use --train_cudnn, but --load_cudnn \'\n                  \'was also specified. The --load_cudnn flag is only \'\n                  \'needed when converting a CuDNN RNN checkpoint to \'\n                  \'a CPU-capable graph. If your system is capable of \'\n                  \'using CuDNN RNN, you can just specify the CuDNN RNN \'\n                  \'checkpoint normally with --save_checkpoint_dir.\')\n        sys.exit(1)\n\n    # If separate save and load flags were not specified, default to load and save\n    # from the same dir.\n    if not FLAGS.save_checkpoint_dir:\n        FLAGS.save_checkpoint_dir = FLAGS.checkpoint_dir\n\n    if not FLAGS.load_checkpoint_dir:\n        FLAGS.load_checkpoint_dir = FLAGS.checkpoint_dir\n\n    ConfigSingleton._config = c # pylint: disable=protected-access\n'"
training/deepspeech_training/util/downloader.py,0,"b'import requests\nimport progressbar\n\nfrom os import path, makedirs\n\nSIMPLE_BAR = [\'Progress \', progressbar.Bar(), \' \', progressbar.Percentage(), \' completed\']\n\ndef maybe_download(archive_name, target_dir, archive_url):\n    # If archive file does not exist, download it...\n    archive_path = path.join(target_dir, archive_name)\n\n    if not path.exists(target_dir):\n        print(\'No path ""%s"" - creating ...\' % target_dir)\n        makedirs(target_dir)\n\n    if not path.exists(archive_path):\n        print(\'No archive ""%s"" - downloading...\' % archive_path)\n        req = requests.get(archive_url, stream=True)\n        total_size = int(req.headers.get(\'content-length\', 0))\n        done = 0\n        with open(archive_path, \'wb\') as f:\n            bar = progressbar.ProgressBar(max_value=total_size, widgets=SIMPLE_BAR)\n            for data in req.iter_content(1024*1024):\n                done += len(data)\n                f.write(data)\n                bar.update(done)\n    else:\n        print(\'Found archive ""%s"" - not downloading.\' % archive_path)\n    return archive_path\n'"
training/deepspeech_training/util/evaluate_tools.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nfrom multiprocessing.dummy import Pool\n\nimport numpy as np\nfrom attrdict import AttrDict\n\nfrom .flags import FLAGS\nfrom .text import levenshtein\n\n\ndef pmap(fun, iterable):\n    pool = Pool()\n    results = pool.map(fun, iterable)\n    pool.close()\n    return results\n\n\ndef wer_cer_batch(samples):\n    r""""""\n    The WER is defined as the edit/Levenshtein distance on word level divided by\n    the amount of words in the original text.\n    In case of the original having more words (N) than the result and both\n    being totally different (all N words resulting in 1 edit operation each),\n    the WER will always be 1 (N / N = 1).\n    """"""\n    wer = sum(s.word_distance for s in samples) / sum(s.word_length for s in samples)\n    cer = sum(s.char_distance for s in samples) / sum(s.char_length for s in samples)\n\n    wer = min(wer, 1.0)\n    cer = min(cer, 1.0)\n\n    return wer, cer\n\n\ndef process_decode_result(item):\n    wav_filename, ground_truth, prediction, loss = item\n    char_distance = levenshtein(ground_truth, prediction)\n    char_length = len(ground_truth)\n    word_distance = levenshtein(ground_truth.split(), prediction.split())\n    word_length = len(ground_truth.split())\n    return AttrDict({\n        \'wav_filename\': wav_filename,\n        \'src\': ground_truth,\n        \'res\': prediction,\n        \'loss\': loss,\n        \'char_distance\': char_distance,\n        \'char_length\': char_length,\n        \'word_distance\': word_distance,\n        \'word_length\': word_length,\n        \'cer\': char_distance / char_length,\n        \'wer\': word_distance / word_length,\n    })\n\n\ndef calculate_and_print_report(wav_filenames, labels, decodings, losses, dataset_name):\n    r\'\'\'\n    This routine will calculate and print a WER report.\n    It\'ll compute the `mean` WER and create ``Sample`` objects of the ``report_count`` top lowest\n    loss items from the provided WER results tuple (only items with WER!=0 and ordered by their WER).\n    \'\'\'\n    samples = pmap(process_decode_result, zip(wav_filenames, labels, decodings, losses))\n\n    # Getting the WER and CER from the accumulated edit distances and lengths\n    samples_wer, samples_cer = wer_cer_batch(samples)\n\n    # Reversed because the worst WER with the best loss is to identify systemic issues, where the acoustic model is confident,\n    # yet the result is completely off the mark. This can point to transcription errors and stuff like that.\n    samples.sort(key=lambda s: s.loss, reverse=True)\n\n    # Then order by ascending WER/CER\n    if FLAGS.utf8:\n        samples.sort(key=lambda s: s.cer)\n    else:\n        samples.sort(key=lambda s: s.wer)\n\n    # Print the report\n    print_report(samples, losses, samples_wer, samples_cer, dataset_name)\n\n    return samples\n\n\ndef print_report(samples, losses, wer, cer, dataset_name):\n    """""" Print a report summary and samples of best, median and worst results """"""\n\n    # Print summary\n    mean_loss = np.mean(losses)\n    print(\'Test on %s - WER: %f, CER: %f, loss: %f\' % (dataset_name, wer, cer, mean_loss))\n    print(\'-\' * 80)\n\n    best_samples = samples[:FLAGS.report_count]\n    worst_samples = samples[-FLAGS.report_count:]\n    median_index = int(len(samples) / 2)\n    median_left = int(FLAGS.report_count / 2)\n    median_right = FLAGS.report_count - median_left\n    median_samples = samples[median_index - median_left:median_index + median_right]\n\n    def print_single_sample(sample):\n        print(\'WER: %f, CER: %f, loss: %f\' % (sample.wer, sample.cer, sample.loss))\n        print(\' - wav: file://%s\' % sample.wav_filename)\n        print(\' - src: ""%s""\' % sample.src)\n        print(\' - res: ""%s""\' % sample.res)\n        print(\'-\' * 80)\n\n    print(\'Best WER:\', \'\\n\' + \'-\' * 80)\n    for s in best_samples:\n        print_single_sample(s)\n\n    print(\'Median WER:\', \'\\n\' + \'-\' * 80)\n    for s in median_samples:\n        print_single_sample(s)\n\n    print(\'Worst WER:\', \'\\n\' + \'-\' * 80)\n    for s in worst_samples:\n        print_single_sample(s)\n\n\ndef save_samples_json(samples, output_path):\n    \'\'\' Save decoded tuples as JSON, converting NumPy floats to Python floats.\n\n        We set ensure_ascii=True to prevent json from escaping non-ASCII chars\n        in the texts.\n    \'\'\'\n    with open(output_path, \'w\') as fout:\n        json.dump(samples, fout, default=float, ensure_ascii=False, indent=2)\n'"
training/deepspeech_training/util/feeding.py,20,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nfrom functools import partial\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import gen_audio_ops as contrib_audio\n\nfrom .config import Config\nfrom .text import text_to_char_array\nfrom .flags import FLAGS\nfrom .spectrogram_augmentations import augment_freq_time_mask, augment_dropout, augment_pitch_and_tempo, augment_speed_up, augment_sparse_warp\nfrom .audio import read_frames_from_file, vad_split, pcm_to_np, DEFAULT_FORMAT\nfrom .sample_collections import samples_from_sources, augment_samples\nfrom .helpers import remember_exception, MEGABYTE\n\n\ndef samples_to_mfccs(samples, sample_rate, train_phase=False, sample_id=None):\n    if train_phase:\n        # We need the lambdas to make TensorFlow happy.\n        # pylint: disable=unnecessary-lambda\n        tf.cond(tf.math.not_equal(sample_rate, FLAGS.audio_sample_rate),\n                lambda: tf.print(\'WARNING: sample rate of sample\', sample_id, \'(\', sample_rate, \') \'\n                                 \'does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\'),\n                lambda: tf.no_op(),\n                name=\'matching_sample_rate\')\n\n    spectrogram = contrib_audio.audio_spectrogram(samples,\n                                                  window_size=Config.audio_window_samples,\n                                                  stride=Config.audio_step_samples,\n                                                  magnitude_squared=True)\n\n    # Data Augmentations\n    if train_phase:\n        if FLAGS.augmentation_spec_dropout_keeprate < 1:\n            spectrogram = augment_dropout(spectrogram,\n                                          keep_prob=FLAGS.augmentation_spec_dropout_keeprate)\n\n        # sparse warp must before freq/time masking\n        if FLAGS.augmentation_sparse_warp:\n            spectrogram = augment_sparse_warp(spectrogram,\n                                              time_warping_para=FLAGS.augmentation_sparse_warp_time_warping_para,\n                                              interpolation_order=FLAGS.augmentation_sparse_warp_interpolation_order,\n                                              regularization_weight=FLAGS.augmentation_sparse_warp_regularization_weight,\n                                              num_boundary_points=FLAGS.augmentation_sparse_warp_num_boundary_points,\n                                              num_control_points=FLAGS.augmentation_sparse_warp_num_control_points)\n\n        if FLAGS.augmentation_freq_and_time_masking:\n            spectrogram = augment_freq_time_mask(spectrogram,\n                                                 frequency_masking_para=FLAGS.augmentation_freq_and_time_masking_freq_mask_range,\n                                                 time_masking_para=FLAGS.augmentation_freq_and_time_masking_time_mask_range,\n                                                 frequency_mask_num=FLAGS.augmentation_freq_and_time_masking_number_freq_masks,\n                                                 time_mask_num=FLAGS.augmentation_freq_and_time_masking_number_time_masks)\n\n        if FLAGS.augmentation_pitch_and_tempo_scaling:\n            spectrogram = augment_pitch_and_tempo(spectrogram,\n                                                  max_tempo=FLAGS.augmentation_pitch_and_tempo_scaling_max_tempo,\n                                                  max_pitch=FLAGS.augmentation_pitch_and_tempo_scaling_max_pitch,\n                                                  min_pitch=FLAGS.augmentation_pitch_and_tempo_scaling_min_pitch)\n\n        if FLAGS.augmentation_speed_up_std > 0:\n            spectrogram = augment_speed_up(spectrogram, speed_std=FLAGS.augmentation_speed_up_std)\n\n    mfccs = contrib_audio.mfcc(spectrogram=spectrogram,\n                               sample_rate=sample_rate,\n                               dct_coefficient_count=Config.n_input,\n                               upper_frequency_limit=FLAGS.audio_sample_rate/2)\n    mfccs = tf.reshape(mfccs, [-1, Config.n_input])\n\n    return mfccs, tf.shape(input=mfccs)[0]\n\n\ndef audio_to_features(audio, sample_rate, train_phase=False, sample_id=None):\n    features, features_len = samples_to_mfccs(audio, sample_rate, train_phase=train_phase, sample_id=sample_id)\n\n    if train_phase:\n        if FLAGS.data_aug_features_multiplicative > 0:\n            features = features*tf.random.normal(mean=1, stddev=FLAGS.data_aug_features_multiplicative, shape=tf.shape(features))\n\n        if FLAGS.data_aug_features_additive > 0:\n            features = features+tf.random.normal(mean=0.0, stddev=FLAGS.data_aug_features_additive, shape=tf.shape(features))\n\n    return features, features_len\n\n\ndef audiofile_to_features(wav_filename, train_phase=False):\n    samples = tf.io.read_file(wav_filename)\n    decoded = contrib_audio.decode_wav(samples, desired_channels=1)\n    return audio_to_features(decoded.audio, decoded.sample_rate, train_phase=train_phase, sample_id=wav_filename)\n\n\ndef entry_to_features(sample_id, audio, sample_rate, transcript, train_phase=False):\n    # https://bugs.python.org/issue32117\n    features, features_len = audio_to_features(audio, sample_rate, train_phase=train_phase, sample_id=sample_id)\n    sparse_transcript = tf.SparseTensor(*transcript)\n    return sample_id, features, features_len, sparse_transcript\n\n\ndef to_sparse_tuple(sequence):\n    r""""""Creates a sparse representention of ``sequence``.\n        Returns a tuple with (indices, values, shape)\n    """"""\n    indices = np.asarray(list(zip([0]*len(sequence), range(len(sequence)))), dtype=np.int64)\n    shape = np.asarray([1, len(sequence)], dtype=np.int64)\n    return indices, sequence, shape\n\n\ndef create_dataset(sources,\n                   batch_size,\n                   repetitions=1,\n                   augmentation_specs=None,\n                   enable_cache=False,\n                   cache_path=None,\n                   train_phase=False,\n                   exception_box=None,\n                   process_ahead=None,\n                   buffering=1 * MEGABYTE):\n    def generate_values():\n        samples = samples_from_sources(sources, buffering=buffering, labeled=True)\n        samples = augment_samples(samples,\n                                  repetitions=repetitions,\n                                  augmentation_specs=augmentation_specs,\n                                  buffering=buffering,\n                                  process_ahead=2 * batch_size if process_ahead is None else process_ahead)\n        for sample in samples:\n            transcript = text_to_char_array(sample.transcript, Config.alphabet, context=sample.sample_id)\n            transcript = to_sparse_tuple(transcript)\n            yield sample.sample_id, sample.audio, sample.audio_format.rate, transcript\n\n    # Batching a dataset of 2D SparseTensors creates 3D batches, which fail\n    # when passed to tf.nn.ctc_loss, so we reshape them to remove the extra\n    # dimension here.\n    def sparse_reshape(sparse):\n        shape = sparse.dense_shape\n        return tf.sparse.reshape(sparse, [shape[0], shape[2]])\n\n    def batch_fn(sample_ids, features, features_len, transcripts):\n        features = tf.data.Dataset.zip((features, features_len))\n        features = features.padded_batch(batch_size, padded_shapes=([None, Config.n_input], []))\n        transcripts = transcripts.batch(batch_size).map(sparse_reshape)\n        sample_ids = sample_ids.batch(batch_size)\n        return tf.data.Dataset.zip((sample_ids, features, transcripts))\n\n    process_fn = partial(entry_to_features, train_phase=train_phase)\n\n    dataset = (tf.data.Dataset.from_generator(remember_exception(generate_values, exception_box),\n                                              output_types=(tf.string, tf.float32, tf.int32,\n                                                            (tf.int64, tf.int32, tf.int64)))\n                              .map(process_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE))\n    if enable_cache:\n        dataset = dataset.cache(cache_path)\n    dataset = (dataset.window(batch_size, drop_remainder=train_phase).flat_map(batch_fn)\n                      .prefetch(len(Config.available_devices)))\n    return dataset\n\n\ndef split_audio_file(audio_path,\n                     audio_format=DEFAULT_FORMAT,\n                     batch_size=1,\n                     aggressiveness=3,\n                     outlier_duration_ms=10000,\n                     outlier_batch_size=1,\n                     exception_box=None):\n    def generate_values():\n        frames = read_frames_from_file(audio_path)\n        segments = vad_split(frames, aggressiveness=aggressiveness)\n        for segment in segments:\n            segment_buffer, time_start, time_end = segment\n            samples = pcm_to_np(segment_buffer, audio_format)\n            yield time_start, time_end, samples\n\n    def to_mfccs(time_start, time_end, samples):\n        features, features_len = samples_to_mfccs(samples, audio_format.rate)\n        return time_start, time_end, features, features_len\n\n    def create_batch_set(bs, criteria):\n        return (tf.data.Dataset\n                .from_generator(remember_exception(generate_values, exception_box),\n                                output_types=(tf.int32, tf.int32, tf.float32))\n                .map(to_mfccs, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n                .filter(criteria)\n                .padded_batch(bs, padded_shapes=([], [], [None, Config.n_input], [])))\n\n    nds = create_batch_set(batch_size,\n                           lambda start, end, f, fl: end - start <= int(outlier_duration_ms))\n    ods = create_batch_set(outlier_batch_size,\n                           lambda start, end, f, fl: end - start > int(outlier_duration_ms))\n    dataset = nds.concatenate(ods)\n    dataset = dataset.prefetch(len(Config.available_devices))\n    return dataset\n'"
training/deepspeech_training/util/flags.py,2,"b'from __future__ import absolute_import, division, print_function\n\nimport os\nimport absl.flags\n\nFLAGS = absl.flags.FLAGS\n\n# sphinx-doc: training_ref_flags_start\ndef create_flags():\n    # Importer\n    # ========\n\n    f = absl.flags\n\n    f.DEFINE_string(\'train_files\', \'\', \'comma separated list of files specifying the dataset used for training. Multiple files will get merged. If empty, training will not be run.\')\n    f.DEFINE_string(\'dev_files\', \'\', \'comma separated list of files specifying the datasets used for validation. Multiple files will get reported separately. If empty, validation will not be run.\')\n    f.DEFINE_string(\'test_files\', \'\', \'comma separated list of files specifying the datasets used for testing. Multiple files will get reported separately. If empty, the model will not be tested.\')\n    f.DEFINE_string(\'metrics_files\', \'\', \'comma separated list of files specifying the datasets used for tracking of metrics (after validation step). Currently the only metric is the CTC loss but without affecting the tracking of best validation loss. Multiple files will get reported separately. If empty, metrics will not be computed.\')\n\n    f.DEFINE_string(\'read_buffer\', \'1MB\', \'buffer-size for reading samples from datasets (supports file-size suffixes KB, MB, GB, TB)\')\n    f.DEFINE_string(\'feature_cache\', \'\', \'cache MFCC features to disk to speed up future training runs on the same data. This flag specifies the path where cached features extracted from --train_files will be saved. If empty, or if online augmentation flags are enabled, caching will be disabled.\')\n\n    f.DEFINE_integer(\'feature_win_len\', 32, \'feature extraction audio window length in milliseconds\')\n    f.DEFINE_integer(\'feature_win_step\', 20, \'feature extraction window step length in milliseconds\')\n    f.DEFINE_integer(\'audio_sample_rate\', 16000, \'sample rate value expected by model\')\n\n    # Data Augmentation\n    # ================\n\n    f.DEFINE_multi_string(\'augment\', None, \'specifies an augmentation of the training samples. Format is ""--augment operation[param1=value1, ...]""\')\n    f.DEFINE_integer(\'augmentations_per_epoch\', 1, \'how often the train set should be repeated and re-augmented per epoch\')\n\n    f.DEFINE_float(\'data_aug_features_additive\', 0, \'std of the Gaussian additive noise\')\n    f.DEFINE_float(\'data_aug_features_multiplicative\', 0, \'std of normal distribution around 1 for multiplicative noise\')\n\n    f.DEFINE_float(\'augmentation_spec_dropout_keeprate\', 1, \'keep rate of dropout augmentation on spectrogram (if 1, no dropout will be performed on spectrogram)\')\n\n    f.DEFINE_boolean(\'augmentation_sparse_warp\', False, \'whether to use spectrogram sparse warp. USE OF THIS FLAG IS UNSUPPORTED, enable sparse warp will increase training time drastically, and the paper also mentioned that this is not a major factor to improve accuracy.\')\n    f.DEFINE_integer(\'augmentation_sparse_warp_num_control_points\', 1, \'specify number of control points\')\n    f.DEFINE_integer(\'augmentation_sparse_warp_time_warping_para\', 20, \'time_warping_para\')\n    f.DEFINE_integer(\'augmentation_sparse_warp_interpolation_order\', 2, \'sparse_warp_interpolation_order\')\n    f.DEFINE_float(\'augmentation_sparse_warp_regularization_weight\', 0.0, \'sparse_warp_regularization_weight\')\n    f.DEFINE_integer(\'augmentation_sparse_warp_num_boundary_points\', 1, \'sparse_warp_num_boundary_points\')\n\n    f.DEFINE_boolean(\'augmentation_freq_and_time_masking\', False, \'whether to use frequency and time masking augmentation\')\n    f.DEFINE_integer(\'augmentation_freq_and_time_masking_freq_mask_range\', 5, \'max range of masks in the frequency domain when performing freqtime-mask augmentation\')\n    f.DEFINE_integer(\'augmentation_freq_and_time_masking_number_freq_masks\', 3, \'number of masks in the frequency domain when performing freqtime-mask augmentation\')\n    f.DEFINE_integer(\'augmentation_freq_and_time_masking_time_mask_range\', 2, \'max range of masks in the time domain when performing freqtime-mask augmentation\')\n    f.DEFINE_integer(\'augmentation_freq_and_time_masking_number_time_masks\', 3, \'number of masks in the time domain when performing freqtime-mask augmentation\')\n\n    f.DEFINE_float(\'augmentation_speed_up_std\', 0, \'std for speeding-up tempo. If std is 0, this augmentation is not performed\')\n\n    f.DEFINE_boolean(\'augmentation_pitch_and_tempo_scaling\', False, \'whether to use spectrogram speed and tempo scaling\')\n    f.DEFINE_float(\'augmentation_pitch_and_tempo_scaling_min_pitch\', 0.95, \'min value of pitch scaling\')\n    f.DEFINE_float(\'augmentation_pitch_and_tempo_scaling_max_pitch\', 1.2, \'max value of pitch scaling\')\n    f.DEFINE_float(\'augmentation_pitch_and_tempo_scaling_max_tempo\', 1.2, \'max vlaue of tempo scaling\')\n\n    # Global Constants\n    # ================\n\n    f.DEFINE_integer(\'epochs\', 75, \'how many epochs (complete runs through the train files) to train for\')\n\n    f.DEFINE_float(\'dropout_rate\', 0.05, \'dropout rate for feedforward layers\')\n    f.DEFINE_float(\'dropout_rate2\', -1.0, \'dropout rate for layer 2 - defaults to dropout_rate\')\n    f.DEFINE_float(\'dropout_rate3\', -1.0, \'dropout rate for layer 3 - defaults to dropout_rate\')\n    f.DEFINE_float(\'dropout_rate4\', 0.0, \'dropout rate for layer 4 - defaults to 0.0\')\n    f.DEFINE_float(\'dropout_rate5\', 0.0, \'dropout rate for layer 5 - defaults to 0.0\')\n    f.DEFINE_float(\'dropout_rate6\', -1.0, \'dropout rate for layer 6 - defaults to dropout_rate\')\n\n    f.DEFINE_float(\'relu_clip\', 20.0, \'ReLU clipping value for non-recurrent layers\')\n\n    # Adam optimizer(http://arxiv.org/abs/1412.6980) parameters\n\n    f.DEFINE_float(\'beta1\', 0.9, \'beta 1 parameter of Adam optimizer\')\n    f.DEFINE_float(\'beta2\', 0.999, \'beta 2 parameter of Adam optimizer\')\n    f.DEFINE_float(\'epsilon\', 1e-8, \'epsilon parameter of Adam optimizer\')\n    f.DEFINE_float(\'learning_rate\', 0.001, \'learning rate of Adam optimizer\')\n\n    # Batch sizes\n\n    f.DEFINE_integer(\'train_batch_size\', 1, \'number of elements in a training batch\')\n    f.DEFINE_integer(\'dev_batch_size\', 1, \'number of elements in a validation batch\')\n    f.DEFINE_integer(\'test_batch_size\', 1, \'number of elements in a test batch\')\n\n    f.DEFINE_integer(\'export_batch_size\', 1, \'number of elements per batch on the exported graph\')\n\n    # Performance\n\n    f.DEFINE_integer(\'inter_op_parallelism_threads\', 0, \'number of inter-op parallelism threads - see tf.ConfigProto for more details. USE OF THIS FLAG IS UNSUPPORTED\')\n    f.DEFINE_integer(\'intra_op_parallelism_threads\', 0, \'number of intra-op parallelism threads - see tf.ConfigProto for more details. USE OF THIS FLAG IS UNSUPPORTED\')\n    f.DEFINE_boolean(\'use_allow_growth\', False, \'use Allow Growth flag which will allocate only required amount of GPU memory and prevent full allocation of available GPU memory\')\n    f.DEFINE_boolean(\'load_cudnn\', False, \'Specifying this flag allows one to convert a CuDNN RNN checkpoint to a checkpoint capable of running on a CPU graph.\')\n    f.DEFINE_boolean(\'train_cudnn\', False, \'use CuDNN RNN backend for training on GPU. Note that checkpoints created with this flag can only be used with CuDNN RNN, i.e. fine tuning on a CPU device will not work\')\n    f.DEFINE_boolean(\'automatic_mixed_precision\', False, \'whether to allow automatic mixed precision training. USE OF THIS FLAG IS UNSUPPORTED. Checkpoints created with automatic mixed precision training will not be usable without mixed precision.\')\n\n    # Sample limits\n\n    f.DEFINE_integer(\'limit_train\', 0, \'maximum number of elements to use from train set - 0 means no limit\')\n    f.DEFINE_integer(\'limit_dev\', 0, \'maximum number of elements to use from validation set- 0 means no limit\')\n    f.DEFINE_integer(\'limit_test\', 0, \'maximum number of elements to use from test set- 0 means no limit\')\n\n    # Checkpointing\n\n    f.DEFINE_string(\'checkpoint_dir\', \'\', \'directory from which checkpoints are loaded and to which they are saved - defaults to directory ""deepspeech/checkpoints"" within user\\\'s data home specified by the XDG Base Directory Specification\')\n    f.DEFINE_string(\'load_checkpoint_dir\', \'\', \'directory in which checkpoints are stored - defaults to directory ""deepspeech/checkpoints"" within user\\\'s data home specified by the XDG Base Directory Specification\')\n    f.DEFINE_string(\'save_checkpoint_dir\', \'\', \'directory to which checkpoints are saved - defaults to directory ""deepspeech/checkpoints"" within user\\\'s data home specified by the XDG Base Directory Specification\')\n    f.DEFINE_integer(\'checkpoint_secs\', 600, \'checkpoint saving interval in seconds\')\n    f.DEFINE_integer(\'max_to_keep\', 5, \'number of checkpoint files to keep - default value is 5\')\n    f.DEFINE_string(\'load_train\', \'auto\', \'what checkpoint to load before starting the training process. ""last"" for loading most recent epoch checkpoint, ""best"" for loading best validation loss checkpoint, ""init"" for initializing a new checkpoint, ""auto"" for trying several options.\')\n    f.DEFINE_string(\'load_evaluate\', \'auto\', \'what checkpoint to load for evaluation tasks (test epochs, model export, single file inference, etc). ""last"" for loading most recent epoch checkpoint, ""best"" for loading best validation loss checkpoint, ""auto"" for trying several options.\')\n\n    # Transfer Learning\n\n    f.DEFINE_integer(\'drop_source_layers\', 0, \'single integer for how many layers to drop from source model (to drop just output == 1, drop penultimate and output ==2, etc)\')\n\n    # Exporting\n\n    f.DEFINE_string(\'export_dir\', \'\', \'directory in which exported models are stored - if omitted, the model won\\\'t get exported\')\n    f.DEFINE_boolean(\'remove_export\', False, \'whether to remove old exported models\')\n    f.DEFINE_boolean(\'export_tflite\', False, \'export a graph ready for TF Lite engine\')\n    f.DEFINE_integer(\'n_steps\', 16, \'how many timesteps to process at once by the export graph, higher values mean more latency\')\n    f.DEFINE_boolean(\'export_zip\', False, \'export a TFLite model and package with LM and info.json\')\n    f.DEFINE_string(\'export_file_name\', \'output_graph\', \'name for the exported model file name\')\n    f.DEFINE_integer(\'export_beam_width\', 500, \'default beam width to embed into exported graph\')\n\n    # Model metadata\n\n    f.DEFINE_string(\'export_author_id\', \'author\', \'author of the exported model. GitHub user or organization name used to uniquely identify the author of this model\')\n    f.DEFINE_string(\'export_model_name\', \'model\', \'name of the exported model. Must not contain forward slashes.\')\n    f.DEFINE_string(\'export_model_version\', \'0.0.1\', \'semantic version of the exported model. See https://semver.org/. This is fully controlled by you as author of the model and has no required connection with DeepSpeech versions\')\n\n    def str_val_equals_help(name, val_desc):\n        f.DEFINE_string(name, \'<{}>\'.format(val_desc), val_desc)\n\n    str_val_equals_help(\'export_contact_info\', \'public contact information of the author. Can be an email address, or a link to a contact form, issue tracker, or discussion forum. Must provide a way to reach the model authors\')\n    str_val_equals_help(\'export_license\', \'SPDX identifier of the license of the exported model. See https://spdx.org/licenses/. If the license does not have an SPDX identifier, use the license name.\')\n    str_val_equals_help(\'export_language\', \'language the model was trained on - IETF BCP 47 language tag including at least language, script and region subtags. E.g. ""en-Latn-UK"" or ""de-Latn-DE"" or ""cmn-Hans-CN"". Include as much info as you can without loss of precision. For example, if a model is trained on Scottish English, include the variant subtag: ""en-Latn-GB-Scotland"".\')\n    str_val_equals_help(\'export_min_ds_version\', \'minimum DeepSpeech version (inclusive) the exported model is compatible with\')\n    str_val_equals_help(\'export_max_ds_version\', \'maximum DeepSpeech version (inclusive) the exported model is compatible with\')\n    str_val_equals_help(\'export_description\', \'Freeform description of the model being exported. Markdown accepted. You can also leave this flag unchanged and edit the generated .md file directly. Useful things to describe are demographic and acoustic characteristics of the data used to train the model, any architectural changes, names of public datasets that were used when applicable, hyperparameters used for training, evaluation results on standard benchmark datasets, etc.\')\n\n    # Reporting\n\n    f.DEFINE_integer(\'log_level\', 1, \'log level for console logs - 0: DEBUG, 1: INFO, 2: WARN, 3: ERROR\')\n    f.DEFINE_boolean(\'show_progressbar\', True, \'Show progress for training, validation and testing processes. Log level should be > 0.\')\n\n    f.DEFINE_boolean(\'log_placement\', False, \'whether to log device placement of the operators to the console\')\n    f.DEFINE_integer(\'report_count\', 5, \'number of phrases for each of best WER, median WER and worst WER to print out during a WER report\')\n\n    f.DEFINE_string(\'summary_dir\', \'\', \'target directory for TensorBoard summaries - defaults to directory ""deepspeech/summaries"" within user\\\'s data home specified by the XDG Base Directory Specification\')\n\n    f.DEFINE_string(\'test_output_file\', \'\', \'path to a file to save all src/decoded/distance/loss tuples generated during a test epoch\')\n\n    # Geometry\n\n    f.DEFINE_integer(\'n_hidden\', 2048, \'layer width to use when initialising layers\')\n\n    # Initialization\n\n    f.DEFINE_integer(\'random_seed\', 4568, \'default random seed that is used to initialize variables\')\n\n    # Early Stopping\n\n    f.DEFINE_boolean(\'early_stop\', False, \'Enable early stopping mechanism over validation dataset. If validation is not being run, early stopping is disabled.\')\n    f.DEFINE_integer(\'es_epochs\', 25, \'Number of epochs with no improvement after which training will be stopped. Loss is not stored in the checkpoint so when checkpoint is revived it starts the loss calculation from start at that point\')\n    f.DEFINE_float(\'es_min_delta\', 0.05, \'Minimum change in loss to qualify as an improvement. This value will also be used in Reduce learning rate on plateau\')\n\n    # Reduce learning rate on plateau\n\n    f.DEFINE_boolean(\'reduce_lr_on_plateau\', False, \'Enable reducing the learning rate if a plateau is reached. This is the case if the validation loss did not improve for some epochs.\')\n    f.DEFINE_integer(\'plateau_epochs\', 10, \'Number of epochs to consider for RLROP. Has to be smaller than es_epochs from early stopping\')\n    f.DEFINE_float(\'plateau_reduction\', 0.1, \'Multiplicative factor to apply to the current learning rate if a plateau has occurred.\')\n    f.DEFINE_boolean(\'force_initialize_learning_rate\', False, \'Force re-initialization of learning rate which was previously reduced.\')\n\n    # Decoder\n\n    f.DEFINE_boolean(\'utf8\', False, \'enable UTF-8 mode. When this is used the model outputs UTF-8 sequences directly rather than using an alphabet mapping.\')\n    f.DEFINE_string(\'alphabet_config_path\', \'data/alphabet.txt\', \'path to the configuration file specifying the alphabet used by the network. See the comment in data/alphabet.txt for a description of the format.\')\n    f.DEFINE_string(\'scorer_path\', \'data/lm/kenlm.scorer\', \'path to the external scorer file created with data/lm/generate_package.py\')\n    f.DEFINE_alias(\'scorer\', \'scorer_path\')\n    f.DEFINE_integer(\'beam_width\', 1024, \'beam width used in the CTC decoder when building candidate transcriptions\')\n    f.DEFINE_float(\'lm_alpha\', 0.931289039105002, \'the alpha hyperparameter of the CTC decoder. Language Model weight.\')\n    f.DEFINE_float(\'lm_beta\', 1.1834137581510284, \'the beta hyperparameter of the CTC decoder. Word insertion weight.\')\n    f.DEFINE_float(\'cutoff_prob\', 1.0, \'only consider characters until this probability mass is reached. 1.0 = disabled.\')\n    f.DEFINE_integer(\'cutoff_top_n\', 300, \'only process this number of characters sorted by probability mass for each time step. If bigger than alphabet size, disabled.\')\n\n    # Inference mode\n\n    f.DEFINE_string(\'one_shot_infer\', \'\', \'one-shot inference mode: specify a wav file and the script will load the checkpoint and perform inference on it.\')\n\n    # Optimizer mode\n\n    f.DEFINE_float(\'lm_alpha_max\', 5, \'the maximum of the alpha hyperparameter of the CTC decoder explored during hyperparameter optimization. Language Model weight.\')\n    f.DEFINE_float(\'lm_beta_max\', 5, \'the maximum beta hyperparameter of the CTC decoder explored during hyperparameter optimization. Word insertion weight.\')\n    f.DEFINE_integer(\'n_trials\', 2400, \'the number of trials to run during hyperparameter optimization.\')\n\n    # Register validators for paths which require a file to be specified\n\n    f.register_validator(\'alphabet_config_path\',\n                         os.path.isfile,\n                         message=\'The file pointed to by --alphabet_config_path must exist and be readable.\')\n\n    f.register_validator(\'one_shot_infer\',\n                         lambda value: not value or os.path.isfile(value),\n                         message=\'The file pointed to by --one_shot_infer must exist and be readable.\')\n\n# sphinx-doc: training_ref_flags_end\n'"
training/deepspeech_training/util/gpu.py,0,"b'from tensorflow.python.client import device_lib\n\n\ndef get_available_gpus(config):\n    r""""""\n    Returns the number of GPUs available on this system.\n    """"""\n    local_device_protos = device_lib.list_local_devices(session_config=config)\n    return [x.name for x in local_device_protos if x.device_type == \'GPU\']\n'"
training/deepspeech_training/util/helpers.py,0,"b'import os\nimport sys\nimport time\nimport heapq\nimport semver\nimport random\n\nfrom multiprocessing import Pool\nfrom collections import namedtuple\n\nKILO = 1024\nKILOBYTE = 1 * KILO\nMEGABYTE = KILO * KILOBYTE\nGIGABYTE = KILO * MEGABYTE\nTERABYTE = KILO * GIGABYTE\nSIZE_PREFIX_LOOKUP = {\'k\': KILOBYTE, \'m\': MEGABYTE, \'g\': GIGABYTE, \'t\': TERABYTE}\n\nValueRange = namedtuple(\'ValueRange\', \'start end r\')\n\n\ndef parse_file_size(file_size):\n    file_size = file_size.lower().strip()\n    if len(file_size) == 0:\n        return 0\n    n = int(keep_only_digits(file_size))\n    if file_size[-1] == \'b\':\n        file_size = file_size[:-1]\n    e = file_size[-1]\n    return SIZE_PREFIX_LOOKUP[e] * n if e in SIZE_PREFIX_LOOKUP else n\n\n\ndef keep_only_digits(txt):\n    return \'\'.join(filter(str.isdigit, txt))\n\n\ndef secs_to_hours(secs):\n    hours, remainder = divmod(secs, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    return \'%d:%02d:%02d\' % (hours, minutes, seconds)\n\n\ndef check_ctcdecoder_version():\n    ds_version_s = open(os.path.join(os.path.dirname(__file__), \'../VERSION\')).read().strip()\n\n    try:\n        # pylint: disable=import-outside-toplevel\n        from ds_ctcdecoder import __version__ as decoder_version\n    except ImportError as e:\n        if e.msg.find(\'__version__\') > 0:\n            print(""DeepSpeech version ({ds_version}) requires CTC decoder to expose __version__. ""\n                  ""Please upgrade the ds_ctcdecoder package to version {ds_version}"".format(ds_version=ds_version_s))\n            sys.exit(1)\n        raise e\n\n    decoder_version_s = decoder_version.decode()\n\n    rv = semver.compare(ds_version_s, decoder_version_s)\n    if rv != 0:\n        print(""DeepSpeech version ({}) and CTC decoder version ({}) do not match. ""\n              ""Please ensure matching versions are in use."".format(ds_version_s, decoder_version_s))\n        sys.exit(1)\n\n    return rv\n\n\nclass Interleaved:\n    """"""Collection that lazily combines sorted collections in an interleaving fashion.\n    During iteration the next smallest element from all the sorted collections is always picked.\n    The collections must support iter() and len().""""""\n    def __init__(self, *iterables, key=lambda obj: obj):\n        self.iterables = iterables\n        self.key = key\n        self.len = sum(map(len, iterables))\n\n    def __iter__(self):\n        return heapq.merge(*self.iterables, key=self.key)\n\n    def __len__(self):\n        return self.len\n\n\nclass LimitingPool:\n    """"""Limits unbound ahead-processing of multiprocessing.Pool\'s imap method\n    before items get consumed by the iteration caller.\n    This prevents OOM issues in situations where items represent larger memory allocations.""""""\n    def __init__(self, processes=None, initializer=None, initargs=None, process_ahead=None, sleeping_for=0.1):\n        self.process_ahead = os.cpu_count() if process_ahead is None else process_ahead\n        self.sleeping_for = sleeping_for\n        self.processed = 0\n        self.pool = Pool(processes=processes, initializer=initializer, initargs=initargs)\n\n    def __enter__(self):\n        return self\n\n    def _limit(self, it):\n        for obj in it:\n            while self.processed >= self.process_ahead:\n                time.sleep(self.sleeping_for)\n            self.processed += 1\n            yield obj\n\n    def imap(self, fun, it):\n        for obj in self.pool.imap(fun, self._limit(it)):\n            self.processed -= 1\n            yield obj\n\n    def terminate(self):\n        self.pool.terminate()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.pool.close()\n\n\nclass ExceptionBox:\n    """"""Helper class for passing-back and re-raising an exception from inside a TensorFlow dataset generator.\n    Used in conjunction with `remember_exception`.""""""\n    def __init__(self):\n        self.exception = None\n\n    def raise_if_set(self):\n        if self.exception is not None:\n            exception = self.exception\n            self.exception = None\n            raise exception  # pylint: disable = raising-bad-type\n\n\ndef remember_exception(iterable, exception_box=None):\n    """"""Wraps a TensorFlow dataset generator for catching its actual exceptions\n    that would otherwise just interrupt iteration w/o bubbling up.""""""\n    def do_iterate():\n        try:\n            yield from iterable()\n        except StopIteration:\n            return\n        except Exception as ex:  # pylint: disable = broad-except\n            exception_box.exception = ex\n    return iterable if exception_box is None else do_iterate\n\n\ndef get_value_range(value, target_type):\n    if isinstance(value, str):\n        r = target_type(0)\n        parts = value.split(\'~\')\n        if len(parts) == 2:\n            value = parts[0]\n            r = target_type(parts[1])\n        elif len(parts) > 2:\n            raise ValueError(\'Cannot parse value range\')\n        parts = value.split(\':\')\n        if len(parts) == 1:\n            parts.append(parts[0])\n        elif len(parts) > 2:\n            raise ValueError(\'Cannot parse value range\')\n        return ValueRange(target_type(parts[0]), target_type(parts[1]), r)\n    if isinstance(value, tuple):\n        if len(value) == 2:\n            return ValueRange(target_type(value[0]), target_type(value[1]), 0)\n        if len(value) == 3:\n            return ValueRange(target_type(value[0]), target_type(value[1]), target_type(value[2]))\n        raise ValueError(\'Cannot convert to ValueRange: Wrong tuple size\')\n    return ValueRange(target_type(value), target_type(value), 0)\n\n\ndef int_range(value):\n    return get_value_range(value, int)\n\n\ndef float_range(value):\n    return get_value_range(value, float)\n\n\ndef pick_value_from_range(value_range, clock=None):\n    clock = random.random() if clock is None else max(0.0, min(1.0, float(clock)))\n    value = value_range.start + clock * (value_range.end - value_range.start)\n    value = random.uniform(value - value_range.r, value + value_range.r)\n    return round(value) if isinstance(value_range.start, int) else value\n'"
training/deepspeech_training/util/importers.py,0,"b'import argparse\nimport importlib\nimport os\nimport re\nimport sys\n\nfrom .helpers import secs_to_hours\nfrom collections import Counter\n\ndef get_counter():\n    return Counter({\'all\': 0, \'failed\': 0, \'invalid_label\': 0, \'too_short\': 0, \'too_long\': 0, \'imported_time\': 0, \'total_time\': 0})\n\ndef get_imported_samples(counter):\n    return counter[\'all\'] - counter[\'failed\'] - counter[\'too_short\'] - counter[\'too_long\'] - counter[\'invalid_label\']\n\ndef print_import_report(counter, sample_rate, max_secs):\n    print(\'Imported %d samples.\' % (get_imported_samples(counter)))\n    if counter[\'failed\'] > 0:\n        print(\'Skipped %d samples that failed upon conversion.\' % counter[\'failed\'])\n    if counter[\'invalid_label\'] > 0:\n        print(\'Skipped %d samples that failed on transcript validation.\' % counter[\'invalid_label\'])\n    if counter[\'too_short\'] > 0:\n        print(\'Skipped %d samples that were too short to match the transcript.\' % counter[\'too_short\'])\n    if counter[\'too_long\'] > 0:\n        print(\'Skipped %d samples that were longer than %d seconds.\' % (counter[\'too_long\'], max_secs))\n    print(\'Final amount of imported audio: %s from %s.\' % (secs_to_hours(counter[\'imported_time\'] / sample_rate), secs_to_hours(counter[\'total_time\'] / sample_rate)))\n\ndef get_importers_parser(description):\n    parser = argparse.ArgumentParser(description=description)\n    parser.add_argument(\'--validate_label_locale\', help=\'Path to a Python file defining a |validate_label| function for your locale. WARNING: THIS WILL ADD THIS FILE\\\'s DIRECTORY INTO PYTHONPATH.\')\n    return parser\n\ndef get_validate_label(args):\n    """"""\n    Expects an argparse.Namespace argument to search for validate_label_locale parameter.\n    If found, this will modify Python\'s library search path and add the directory of the\n    file pointed by the validate_label_locale argument.\n\n    :param args: The importer\'s CLI argument object\n    :type args: argparse.Namespace\n\n    :return: The user-supplied validate_label function\n    :type: function\n    """"""\n    # Python 3.5 does not support passing a pathlib.Path to os.path.* methods\n    if \'validate_label_locale\' not in args or (args.validate_label_locale is None):\n        print(\'WARNING: No --validate_label_locale specified, your might end with inconsistent dataset.\')\n        return validate_label_eng\n    validate_label_locale = str(args.validate_label_locale)\n    if not os.path.exists(os.path.abspath(validate_label_locale)):\n        print(\'ERROR: Inexistent --validate_label_locale specified. Please check.\')\n        return None\n    module_dir = os.path.abspath(os.path.dirname(validate_label_locale))\n    sys.path.insert(1, module_dir)\n    fname = os.path.basename(validate_label_locale).replace(\'.py\', \'\')\n    locale_module = importlib.import_module(fname, package=None)\n    return locale_module.validate_label\n\n# Validate and normalize transcriptions. Returns a cleaned version of the label\n# or None if it\'s invalid.\ndef validate_label_eng(label):\n    # For now we can only handle [a-z \']\n    if re.search(r""[0-9]|[(<\\[\\]&*{]"", label) is not None:\n        return None\n\n    label = label.replace(""-"", "" "")\n    label = label.replace(""_"", "" "")\n    label = re.sub(""[ ]{2,}"", "" "", label)\n    label = label.replace(""."", """")\n    label = label.replace("","", """")\n    label = label.replace("";"", """")\n    label = label.replace(""?"", """")\n    label = label.replace(""!"", """")\n    label = label.replace("":"", """")\n    label = label.replace(""\\"""", """")\n    label = label.strip()\n    label = label.lower()\n\n    return label if label else None\n'"
training/deepspeech_training/util/logging.py,0,"b""from __future__ import print_function\n\nimport progressbar\nimport sys\n\nfrom .flags import FLAGS\n\n\n# Logging functions\n# =================\n\ndef prefix_print(prefix, message):\n    print(prefix + ('\\n' + prefix).join(message.split('\\n')))\n\n\ndef log_debug(message):\n    if FLAGS.log_level == 0:\n        prefix_print('D ', message)\n\n\ndef log_info(message):\n    if FLAGS.log_level <= 1:\n        prefix_print('I ', message)\n\n\ndef log_warn(message):\n    if FLAGS.log_level <= 2:\n        prefix_print('W ', message)\n\n\ndef log_error(message):\n    if FLAGS.log_level <= 3:\n        prefix_print('E ', message)\n\n\ndef create_progressbar(*args, **kwargs):\n    # Progress bars in stdout by default\n    if 'fd' not in kwargs:\n        kwargs['fd'] = sys.stdout\n\n    if FLAGS.show_progressbar:\n        return progressbar.ProgressBar(*args, **kwargs)\n\n    return progressbar.NullBar(*args, **kwargs)\n\n\ndef log_progress(message):\n    if not FLAGS.show_progressbar:\n        log_info(message)\n"""
training/deepspeech_training/util/sample_collections.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport csv\nimport json\nimport random\n\nfrom pathlib import Path\nfrom functools import partial\n\nfrom .signal_augmentations import parse_augmentation\nfrom .helpers import MEGABYTE, GIGABYTE, Interleaved, LimitingPool\nfrom .audio import Sample, DEFAULT_FORMAT, AUDIO_TYPE_OPUS, AUDIO_TYPE_NP, SERIALIZABLE_AUDIO_TYPES, get_audio_type_from_extension\n\nBIG_ENDIAN = \'big\'\nINT_SIZE = 4\nBIGINT_SIZE = 2 * INT_SIZE\nMAGIC = b\'SAMPLEDB\'\n\nBUFFER_SIZE = 1 * MEGABYTE\nCACHE_SIZE = 1 * GIGABYTE\n\nSCHEMA_KEY = \'schema\'\nCONTENT_KEY = \'content\'\nMIME_TYPE_KEY = \'mime-type\'\nMIME_TYPE_TEXT = \'text/plain\'\nCONTENT_TYPE_SPEECH = \'speech\'\nCONTENT_TYPE_TRANSCRIPT = \'transcript\'\n\n\nclass LabeledSample(Sample):\n    """"""In-memory labeled audio sample representing an utterance.\n    Derived from util.audio.Sample and used by sample collection readers and writers.""""""\n    def __init__(self, audio_type, raw_data, transcript, audio_format=DEFAULT_FORMAT, sample_id=None):\n        """"""\n        Parameters\n        ----------\n        audio_type : str\n            See util.audio.Sample.__init__ .\n        raw_data : binary\n            See util.audio.Sample.__init__ .\n        transcript : str\n            Transcript of the sample\'s utterance\n        audio_format : tuple\n            See util.audio.Sample.__init__ .\n        sample_id : str\n            Tracking ID - should indicate sample\'s origin as precisely as possible.\n            It is typically assigned by collection readers.\n        """"""\n        super().__init__(audio_type, raw_data, audio_format=audio_format, sample_id=sample_id)\n        self.transcript = transcript\n\n\ndef load_sample(filename, label=None):\n    """"""\n    Loads audio-file as a (labeled or unlabeled) sample\n\n    Parameters\n    ----------\n    filename : str\n        Filename of the audio-file to load as sample\n    label : str\n        Label (transcript) of the sample.\n        If None: return util.audio.Sample instance\n        Otherwise: return util.sample_collections.LabeledSample instance\n\n    Returns\n    -------\n    util.audio.Sample instance if label is None, else util.sample_collections.LabeledSample instance\n    """"""\n    ext = os.path.splitext(filename)[1].lower()\n    audio_type = get_audio_type_from_extension(ext)\n    if audio_type is None:\n        raise ValueError(\'Unknown audio type extension ""{}""\'.format(ext))\n    with open(filename, \'rb\') as audio_file:\n        if label is None:\n            return Sample(audio_type, audio_file.read(), sample_id=filename)\n        return LabeledSample(audio_type, audio_file.read(), label, sample_id=filename)\n\n\nclass DirectSDBWriter:\n    """"""Sample collection writer for creating a Sample DB (SDB) file""""""\n    def __init__(self,\n                 sdb_filename,\n                 buffering=BUFFER_SIZE,\n                 audio_type=AUDIO_TYPE_OPUS,\n                 bitrate=None,\n                 id_prefix=None,\n                 labeled=True):\n        """"""\n        Parameters\n        ----------\n        sdb_filename : str\n            Path to the SDB file to write\n        buffering : int\n            Write-buffer size to use while writing the SDB file\n        audio_type : str\n            See util.audio.Sample.__init__ .\n        bitrate : int\n            Bitrate for sample-compression in case of lossy audio_type (e.g. AUDIO_TYPE_OPUS)\n        id_prefix : str\n            Prefix for IDs of written samples - defaults to sdb_filename\n        labeled : bool or None\n            If True: Writes labeled samples (util.sample_collections.LabeledSample) only.\n            If False: Ignores transcripts (if available) and writes (unlabeled) util.audio.Sample instances.\n        """"""\n        self.sdb_filename = sdb_filename\n        self.id_prefix = sdb_filename if id_prefix is None else id_prefix\n        self.labeled = labeled\n        if audio_type not in SERIALIZABLE_AUDIO_TYPES:\n            raise ValueError(\'Audio type ""{}"" not supported\'.format(audio_type))\n        self.audio_type = audio_type\n        self.bitrate = bitrate\n        self.sdb_file = open(sdb_filename, \'wb\', buffering=buffering)\n        self.offsets = []\n        self.num_samples = 0\n\n        self.sdb_file.write(MAGIC)\n\n        schema_entries = [{CONTENT_KEY: CONTENT_TYPE_SPEECH, MIME_TYPE_KEY: audio_type}]\n        if self.labeled:\n            schema_entries.append({CONTENT_KEY: CONTENT_TYPE_TRANSCRIPT, MIME_TYPE_KEY: MIME_TYPE_TEXT})\n        meta_data = {SCHEMA_KEY: schema_entries}\n        meta_data = json.dumps(meta_data).encode()\n        self.write_big_int(len(meta_data))\n        self.sdb_file.write(meta_data)\n\n        self.offset_samples = self.sdb_file.tell()\n        self.sdb_file.seek(2 * BIGINT_SIZE, 1)\n\n    def write_int(self, n):\n        return self.sdb_file.write(n.to_bytes(INT_SIZE, BIG_ENDIAN))\n\n    def write_big_int(self, n):\n        return self.sdb_file.write(n.to_bytes(BIGINT_SIZE, BIG_ENDIAN))\n\n    def __enter__(self):\n        return self\n\n    def add(self, sample):\n        def to_bytes(n):\n            return n.to_bytes(INT_SIZE, BIG_ENDIAN)\n        sample.change_audio_type(self.audio_type, bitrate=self.bitrate)\n        opus = sample.audio.getbuffer()\n        opus_len = to_bytes(len(opus))\n        if self.labeled:\n            transcript = sample.transcript.encode()\n            transcript_len = to_bytes(len(transcript))\n            entry_len = to_bytes(len(opus_len) + len(opus) + len(transcript_len) + len(transcript))\n            buffer = b\'\'.join([entry_len, opus_len, opus, transcript_len, transcript])\n        else:\n            entry_len = to_bytes(len(opus_len) + len(opus))\n            buffer = b\'\'.join([entry_len, opus_len, opus])\n        self.offsets.append(self.sdb_file.tell())\n        self.sdb_file.write(buffer)\n        sample.sample_id = \'{}:{}\'.format(self.id_prefix, self.num_samples)\n        self.num_samples += 1\n        return sample.sample_id\n\n    def close(self):\n        if self.sdb_file is None:\n            return\n        offset_index = self.sdb_file.tell()\n        self.sdb_file.seek(self.offset_samples)\n        self.write_big_int(offset_index - self.offset_samples - BIGINT_SIZE)\n        self.write_big_int(self.num_samples)\n\n        self.sdb_file.seek(offset_index + BIGINT_SIZE)\n        self.write_big_int(self.num_samples)\n        for offset in self.offsets:\n            self.write_big_int(offset)\n        offset_end = self.sdb_file.tell()\n        self.sdb_file.seek(offset_index)\n        self.write_big_int(offset_end - offset_index - BIGINT_SIZE)\n        self.sdb_file.close()\n        self.sdb_file = None\n\n    def __len__(self):\n        return len(self.offsets)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n\nclass SDB:  # pylint: disable=too-many-instance-attributes\n    """"""Sample collection reader for reading a Sample DB (SDB) file""""""\n    def __init__(self, sdb_filename, buffering=BUFFER_SIZE, id_prefix=None, labeled=True):\n        """"""\n        Parameters\n        ----------\n        sdb_filename : str\n            Path to the SDB file to read samples from\n        buffering : int\n            Read-buffer size to use while reading the SDB file\n        id_prefix : str\n            Prefix for IDs of read samples - defaults to sdb_filename\n        labeled : bool or None\n            If True: Reads util.sample_collections.LabeledSample instances. Fails, if SDB file provides no transcripts.\n            If False: Ignores transcripts (if available) and reads (unlabeled) util.audio.Sample instances.\n            If None: Automatically determines if SDB schema has transcripts\n            (reading util.sample_collections.LabeledSample instances) or not (reading util.audio.Sample instances).\n        """"""\n        self.sdb_filename = sdb_filename\n        self.id_prefix = sdb_filename if id_prefix is None else id_prefix\n        self.sdb_file = open(sdb_filename, \'rb\', buffering=buffering)\n        self.offsets = []\n        if self.sdb_file.read(len(MAGIC)) != MAGIC:\n            raise RuntimeError(\'No Sample Database\')\n        meta_chunk_len = self.read_big_int()\n        self.meta = json.loads(self.sdb_file.read(meta_chunk_len).decode())\n        if SCHEMA_KEY not in self.meta:\n            raise RuntimeError(\'Missing schema\')\n        self.schema = self.meta[SCHEMA_KEY]\n\n        speech_columns = self.find_columns(content=CONTENT_TYPE_SPEECH, mime_type=SERIALIZABLE_AUDIO_TYPES)\n        if not speech_columns:\n            raise RuntimeError(\'No speech data (missing in schema)\')\n        self.speech_index = speech_columns[0]\n        self.audio_type = self.schema[self.speech_index][MIME_TYPE_KEY]\n\n        self.transcript_index = None\n        if labeled is not False:\n            transcript_columns = self.find_columns(content=CONTENT_TYPE_TRANSCRIPT, mime_type=MIME_TYPE_TEXT)\n            if transcript_columns:\n                self.transcript_index = transcript_columns[0]\n            else:\n                if labeled is True:\n                    raise RuntimeError(\'No transcript data (missing in schema)\')\n\n        sample_chunk_len = self.read_big_int()\n        self.sdb_file.seek(sample_chunk_len + BIGINT_SIZE, 1)\n        num_samples = self.read_big_int()\n        for _ in range(num_samples):\n            self.offsets.append(self.read_big_int())\n\n    def read_int(self):\n        return int.from_bytes(self.sdb_file.read(INT_SIZE), BIG_ENDIAN)\n\n    def read_big_int(self):\n        return int.from_bytes(self.sdb_file.read(BIGINT_SIZE), BIG_ENDIAN)\n\n    def find_columns(self, content=None, mime_type=None):\n        criteria = []\n        if content is not None:\n            criteria.append((CONTENT_KEY, content))\n        if mime_type is not None:\n            criteria.append((MIME_TYPE_KEY, mime_type))\n        if len(criteria) == 0:\n            raise ValueError(\'At least one of ""content"" or ""mime-type"" has to be provided\')\n        matches = []\n        for index, column in enumerate(self.schema):\n            matched = 0\n            for field, value in criteria:\n                if column[field] == value or (isinstance(value, list) and column[field] in value):\n                    matched += 1\n            if matched == len(criteria):\n                matches.append(index)\n        return matches\n\n    def read_row(self, row_index, *columns):\n        columns = list(columns)\n        column_data = [None] * len(columns)\n        found = 0\n        if not 0 <= row_index < len(self.offsets):\n            raise ValueError(\'Wrong sample index: {} - has to be between 0 and {}\'\n                             .format(row_index, len(self.offsets) - 1))\n        self.sdb_file.seek(self.offsets[row_index] + INT_SIZE)\n        for index in range(len(self.schema)):\n            chunk_len = self.read_int()\n            if index in columns:\n                column_data[columns.index(index)] = self.sdb_file.read(chunk_len)\n                found += 1\n                if found == len(columns):\n                    return tuple(column_data)\n            else:\n                self.sdb_file.seek(chunk_len, 1)\n        return tuple(column_data)\n\n    def __getitem__(self, i):\n        sample_id = \'{}:{}\'.format(self.id_prefix, i)\n        if self.transcript_index is None:\n            [audio_data] = self.read_row(i, self.speech_index)\n            return Sample(self.audio_type, audio_data, sample_id=sample_id)\n        audio_data, transcript = self.read_row(i, self.speech_index, self.transcript_index)\n        transcript = transcript.decode()\n        return LabeledSample(self.audio_type, audio_data, transcript, sample_id=sample_id)\n\n    def __iter__(self):\n        for i in range(len(self.offsets)):\n            yield self[i]\n\n    def __len__(self):\n        return len(self.offsets)\n\n    def close(self):\n        if self.sdb_file is not None:\n            self.sdb_file.close()\n\n    def __del__(self):\n        self.close()\n\n\nclass SampleList:\n    """"""Sample collection base class with samples loaded from a list of in-memory paths.""""""\n    def __init__(self, samples, labeled=True):\n        """"""\n        Parameters\n        ----------\n        samples : iterable of tuples of the form (sample_filename, filesize [, transcript])\n            File-size is used for ordering the samples; transcript has to be provided if labeled=True\n        labeled : bool or None\n            If True: Reads LabeledSample instances.\n            If False: Ignores transcripts (if available) and reads (unlabeled) util.audio.Sample instances.\n        """"""\n        self.labeled = labeled\n        self.samples = list(samples)\n        self.samples.sort(key=lambda r: r[1])\n\n    def __getitem__(self, i):\n        sample_spec = self.samples[i]\n        return load_sample(sample_spec[0], label=sample_spec[2] if self.labeled else None)\n\n    def __len__(self):\n        return len(self.samples)\n\n\nclass CSV(SampleList):\n    """"""Sample collection reader for reading a DeepSpeech CSV file\n    Automatically orders samples by CSV column wav_filesize (if available).""""""\n    def __init__(self, csv_filename, labeled=None):\n        """"""\n        Parameters\n        ----------\n        csv_filename : str\n            Path to the CSV file containing sample audio paths and transcripts\n        labeled : bool or None\n            If True: Reads LabeledSample instances. Fails, if CSV file has no transcript column.\n            If False: Ignores transcripts (if available) and reads (unlabeled) util.audio.Sample instances.\n            If None: Automatically determines if CSV file has a transcript column\n            (reading util.sample_collections.LabeledSample instances) or not (reading util.audio.Sample instances).\n        """"""\n        rows = []\n        csv_dir = Path(csv_filename).parent\n        with open(csv_filename, \'r\', encoding=\'utf8\') as csv_file:\n            reader = csv.DictReader(csv_file)\n            if \'transcript\' in reader.fieldnames:\n                if labeled is None:\n                    labeled = True\n            elif labeled:\n                raise RuntimeError(\'No transcript data (missing CSV column)\')\n            for row in reader:\n                wav_filename = Path(row[\'wav_filename\'])\n                if not wav_filename.is_absolute():\n                    wav_filename = csv_dir / wav_filename\n                wav_filename = str(wav_filename)\n                wav_filesize = int(row[\'wav_filesize\']) if \'wav_filesize\' in row else 0\n                if labeled:\n                    rows.append((wav_filename, wav_filesize, row[\'transcript\']))\n                else:\n                    rows.append((wav_filename, wav_filesize))\n        super(CSV, self).__init__(rows, labeled=labeled)\n\n\ndef samples_from_source(sample_source, buffering=BUFFER_SIZE, labeled=None):\n    """"""\n    Loads samples from a sample source file.\n\n    Parameters\n    ----------\n    sample_source : str\n        Path to the sample source file (SDB or CSV)\n    buffering : int\n        Read-buffer size to use while reading files\n    labeled : bool or None\n        If True: Reads LabeledSample instances. Fails, if source provides no transcripts.\n        If False: Ignores transcripts (if available) and reads (unlabeled) util.audio.Sample instances.\n        If None: Automatically determines if source provides transcripts\n        (reading util.sample_collections.LabeledSample instances) or not (reading util.audio.Sample instances).\n\n    Returns\n    -------\n    iterable of util.sample_collections.LabeledSample or util.audio.Sample instances supporting len.\n    """"""\n    ext = os.path.splitext(sample_source)[1].lower()\n    if ext == \'.sdb\':\n        return SDB(sample_source, buffering=buffering, labeled=labeled)\n    if ext == \'.csv\':\n        return CSV(sample_source, labeled=labeled)\n    raise ValueError(\'Unknown file type: ""{}""\'.format(ext))\n\n\ndef samples_from_sources(sample_sources, buffering=BUFFER_SIZE, labeled=None):\n    """"""\n    Loads and combines samples from a list of source files. Sources are combined in an interleaving way to\n    keep default sample order from shortest to longest.\n\n    Parameters\n    ----------\n    sample_sources : list of str\n        Paths to sample source files (SDBs or CSVs)\n    buffering : int\n        Read-buffer size to use while reading files\n    labeled : bool or None\n        If True: Reads LabeledSample instances. Fails, if not all sources provide transcripts.\n        If False: Ignores transcripts (if available) and always reads (unlabeled) util.audio.Sample instances.\n        If None: Reads util.sample_collections.LabeledSample instances from sources with transcripts and\n        util.audio.Sample instances from sources with no transcripts.\n\n    Returns\n    -------\n    iterable of util.sample_collections.LabeledSample (labeled=True) or util.audio.Sample (labeled=False) supporting len\n    """"""\n    sample_sources = list(sample_sources)\n    if len(sample_sources) == 0:\n        raise ValueError(\'No files\')\n    if len(sample_sources) == 1:\n        return samples_from_source(sample_sources[0], buffering=buffering, labeled=labeled)\n    cols = list(map(partial(samples_from_source, buffering=buffering, labeled=labeled), sample_sources))\n    return Interleaved(*cols, key=lambda s: s.duration)\n\n\nclass PreparationContext:\n    def __init__(self, target_audio_type, augmentations):\n        self.target_audio_type = target_audio_type\n        self.augmentations = augmentations\n\n\nAUGMENTATION_CONTEXT = None\n\n\ndef _init_augmentation_worker(preparation_context):\n    global AUGMENTATION_CONTEXT  # pylint: disable=global-statement\n    AUGMENTATION_CONTEXT = preparation_context\n\n\ndef _augment_sample(timed_sample, context=None):\n    context = AUGMENTATION_CONTEXT if context is None else context\n    sample, clock = timed_sample\n    for augmentation in context.augmentations:\n        if random.random() < augmentation.probability:\n            augmentation.apply(sample, clock)\n    sample.change_audio_type(new_audio_type=context.target_audio_type)\n    return sample\n\n\ndef augment_samples(samples,\n                    audio_type=AUDIO_TYPE_NP,\n                    augmentation_specs=None,\n                    buffering=BUFFER_SIZE,\n                    process_ahead=None,\n                    repetitions=1,\n                    fixed_clock=None):\n    """"""\n    Prepares samples for being used during training.\n    This includes parallel and buffered application of augmentations and a conversion to a specified audio-type.\n\n    Parameters\n    ----------\n    samples : Sample enumeration\n        Typically produced by samples_from_sources.\n    audio_type : str\n        Target audio-type to convert samples to. See util.audio.Sample.__init__ .\n    augmentation_specs : list of str\n        Augmentation specifications like [""reverb[delay=20.0,decay=-20]"", ""volume""]. See TRAINING.rst.\n    buffering : int\n        Read-buffer size to use while reading files.\n    process_ahead : int\n        Number of samples to pre-process ahead of time.\n    repetitions : int\n        How often the input sample enumeration should get repeated for being re-augmented.\n    fixed_clock : float\n        Sets the internal clock to a value between 0.0 (beginning of epoch) and 1.0 (end of epoch).\n        Setting this to a number is used for simulating augmentations at a certain epoch-time.\n        If kept at None (default), the internal clock will run regularly from 0.0 to 1.0,\n        hence preparing them for training.\n\n    Returns\n    -------\n    iterable of util.sample_collections.LabeledSample or util.audio.Sample\n    """"""\n    def timed_samples():\n        for repetition in range(repetitions):\n            for sample_index, sample in enumerate(samples):\n                if fixed_clock is None:\n                    yield sample, (repetition * len(samples) + sample_index) / (repetitions * len(samples))\n                else:\n                    yield sample, fixed_clock\n\n    augmentations = [] if augmentation_specs is None else list(map(parse_augmentation, augmentation_specs))\n    try:\n        for augmentation in augmentations:\n            augmentation.start(buffering=buffering)\n        context = PreparationContext(audio_type, augmentations)\n        if process_ahead == 0:\n            for timed_sample in timed_samples():\n                yield _augment_sample(timed_sample, context=context)\n        else:\n            with LimitingPool(process_ahead=process_ahead,\n                              initializer=_init_augmentation_worker,\n                              initargs=(context,)) as pool:\n                yield from pool.imap(_augment_sample, timed_samples())\n    finally:\n        for augmentation in augmentations:\n            augmentation.stop()\n'"
training/deepspeech_training/util/signal_augmentations.py,0,"b'\nimport os\nimport re\nimport math\nimport random\nimport numpy as np\n\nfrom multiprocessing import Queue, Process\nfrom .audio import gain_db_to_ratio, max_dbfs, normalize_audio, AUDIO_TYPE_NP, AUDIO_TYPE_PCM, AUDIO_TYPE_OPUS\nfrom .helpers import int_range, float_range, pick_value_from_range, MEGABYTE\n\nSPEC_PARSER = re.compile(r\'^(?P<cls>[a-z]+)(\\[(?P<params>.*)\\])?$\')\nBUFFER_SIZE = 1 * MEGABYTE\n\n\nclass Augmentation:\n    def __init__(self, p=1.0):\n        self.probability = float(p)\n\n    def start(self, buffering=BUFFER_SIZE):\n        pass\n\n    def apply(self, sample, clock):\n        raise NotImplementedError\n\n    def stop(self):\n        pass\n\n\ndef _enqueue_overlay_samples(sample_source, queue, buffering=BUFFER_SIZE):\n    """"""\n    As the central distribution point for overlay samples this function is supposed to run in one process only.\n    This ensures that samples are not used twice if not required.\n    It loads the (raw and still compressed) data and provides it to the actual augmentation workers.\n    These are then doing decompression, potential conversion and overlaying in parallel.\n    """"""\n    # preventing cyclic import problems\n    from .sample_collections import samples_from_source  # pylint: disable=import-outside-toplevel\n    samples = samples_from_source(sample_source, buffering=buffering, labeled=False)\n    while True:\n        for sample in samples:\n            queue.put(sample)\n\n\nclass Overlay(Augmentation):\n    """"""See ""Overlay augmentation"" in TRAINING.rst""""""\n    def __init__(self, source, p=1.0, snr=3.0, layers=1):\n        super(Overlay, self).__init__(p)\n        self.source = source\n        self.snr = float_range(snr)\n        self.layers = int_range(layers)\n        self.queue = Queue(max(1, math.floor(self.probability * self.layers[1] * os.cpu_count())))\n        self.current_sample = None\n        self.enqueue_process = None\n\n    def start(self, buffering=BUFFER_SIZE):\n        self.enqueue_process = Process(target=_enqueue_overlay_samples,\n                                       args=(self.source, self.queue),\n                                       kwargs={\'buffering\': buffering})\n        self.enqueue_process.start()\n\n    def apply(self, sample, clock):\n        sample.change_audio_type(new_audio_type=AUDIO_TYPE_NP)\n        n_layers = pick_value_from_range(self.layers, clock=clock)\n        audio = sample.audio\n        overlay_data = np.zeros_like(audio)\n        for _ in range(n_layers):\n            overlay_offset = 0\n            while overlay_offset < len(audio):\n                if self.current_sample is None:\n                    next_overlay_sample = self.queue.get()\n                    next_overlay_sample.change_audio_type(new_audio_type=AUDIO_TYPE_NP)\n                    self.current_sample = next_overlay_sample.audio\n                n_required = len(audio) - overlay_offset\n                n_current = len(self.current_sample)\n                if n_required >= n_current:  # take it completely\n                    overlay_data[overlay_offset:overlay_offset + n_current] += self.current_sample\n                    overlay_offset += n_current\n                    self.current_sample = None\n                else:  # take required slice from head and keep tail for next layer or sample\n                    overlay_data[overlay_offset:overlay_offset + n_required] += self.current_sample[0:n_required]\n                    overlay_offset += n_required\n                    self.current_sample = self.current_sample[n_required:]\n        snr_db = pick_value_from_range(self.snr, clock=clock)\n        orig_dbfs = max_dbfs(audio)\n        overlay_gain = orig_dbfs - max_dbfs(overlay_data) - snr_db\n        audio += overlay_data * gain_db_to_ratio(overlay_gain)\n        sample.audio = normalize_audio(audio, dbfs=orig_dbfs)\n\n    def stop(self):\n        if self.enqueue_process is not None:\n            self.enqueue_process.terminate()\n\n\nclass Reverb(Augmentation):\n    """"""See ""Reverb augmentation"" in TRAINING.rst""""""\n    def __init__(self, p=1.0, delay=20.0, decay=10.0):\n        super(Reverb, self).__init__(p)\n        self.delay = float_range(delay)\n        self.decay = float_range(decay)\n\n    def apply(self, sample, clock):\n        sample.change_audio_type(new_audio_type=AUDIO_TYPE_NP)\n        audio = np.array(sample.audio, dtype=np.float64)\n        orig_dbfs = max_dbfs(audio)\n        delay = pick_value_from_range(self.delay, clock=clock)\n        decay = pick_value_from_range(self.decay, clock=clock)\n        decay = gain_db_to_ratio(-decay)\n        result = np.copy(audio)\n        primes = [17, 19, 23, 29, 31]\n        for delay_prime in primes:  # primes to minimize comb filter interference\n            layer = np.copy(audio)\n            n_delay = math.floor(delay * (delay_prime / primes[0]) * sample.audio_format.rate / 1000.0)\n            n_delay = max(16, n_delay)  # 16 samples minimum to avoid performance trap and risk of division by zero\n            for w_index in range(0, math.floor(len(audio) / n_delay)):\n                w1 = w_index * n_delay\n                w2 = (w_index + 1) * n_delay\n                width = min(len(audio) - w2, n_delay)  # last window could be smaller\n                layer[w2:w2 + width] += decay * layer[w1:w1 + width]\n            result += layer\n        audio = normalize_audio(result, dbfs=orig_dbfs)\n        sample.audio = np.array(audio, dtype=np.float32)\n\n\nclass Resample(Augmentation):\n    """"""See ""Resample augmentation"" in TRAINING.rst""""""\n    def __init__(self, p=1.0, rate=8000):\n        super(Resample, self).__init__(p)\n        self.rate = int_range(rate)\n\n    def apply(self, sample, clock):\n        # late binding librosa and its dependencies\n        from librosa.core import resample  # pylint: disable=import-outside-toplevel\n        sample.change_audio_type(new_audio_type=AUDIO_TYPE_NP)\n        rate = pick_value_from_range(self.rate, clock=clock)\n        audio = sample.audio\n        orig_len = len(audio)\n        audio = np.swapaxes(audio, 0, 1)\n        audio = resample(audio, sample.audio_format.rate, rate)\n        audio = resample(audio, rate, sample.audio_format.rate)\n        audio = np.swapaxes(audio, 0, 1)[0:orig_len]\n        sample.audio = audio\n\n\nclass Codec(Augmentation):\n    """"""See ""Codec augmentation"" in TRAINING.rst""""""\n    def __init__(self, p=1.0, bitrate=3200):\n        super(Codec, self).__init__(p)\n        self.bitrate = int_range(bitrate)\n\n    def apply(self, sample, clock):\n        bitrate = pick_value_from_range(self.bitrate, clock=clock)\n        sample.change_audio_type(new_audio_type=AUDIO_TYPE_PCM)  # decoding to ensure it has to get encoded again\n        sample.change_audio_type(new_audio_type=AUDIO_TYPE_OPUS, bitrate=bitrate)  # will get decoded again downstream\n\n\nclass Gaps(Augmentation):\n    """"""See ""Gaps augmentation"" in TRAINING.rst""""""\n    def __init__(self, p=1.0, n=1, size=50.0):\n        super(Gaps, self).__init__(p)\n        self.n_gaps = int_range(n)\n        self.size = float_range(size)\n\n    def apply(self, sample, clock):\n        sample.change_audio_type(new_audio_type=AUDIO_TYPE_NP)\n        audio = sample.audio\n        n_gaps = pick_value_from_range(self.n_gaps, clock=clock)\n        for _ in range(n_gaps):\n            size = pick_value_from_range(self.size, clock=clock)\n            size = int(size * sample.audio_format.rate / 1000.0)\n            size = min(size, len(audio) // 10)  # a gap should never exceed 10 percent of the audio\n            offset = random.randint(0, max(0, len(audio) - size - 1))\n            audio[offset:offset + size] = 0\n        sample.audio = audio\n\n\nclass Volume(Augmentation):\n    """"""See ""Volume augmentation"" in TRAINING.rst""""""\n    def __init__(self, p=1.0, dbfs=3.0103):\n        super(Volume, self).__init__(p)\n        self.target_dbfs = float_range(dbfs)\n\n    def apply(self, sample, clock):\n        sample.change_audio_type(new_audio_type=AUDIO_TYPE_NP)\n        target_dbfs = pick_value_from_range(self.target_dbfs, clock=clock)\n        sample.audio = normalize_audio(sample.audio, dbfs=target_dbfs)\n\n\ndef parse_augmentation(augmentation_spec):\n    """"""\n    Parses an augmentation specification.\n\n    Parameters\n    ----------\n    augmentation_spec : str\n        Augmentation specification like ""reverb[delay=20.0,decay=-20]"".\n\n    Returns\n    -------\n    Instance of an augmentation class from util.signal_augmentations.*.\n    """"""\n    match = SPEC_PARSER.match(augmentation_spec)\n    if not match:\n        raise ValueError(\'Augmentation specification has wrong format\')\n    cls_name = match.group(\'cls\')\n    cls_name = cls_name[0].upper() + cls_name[1:]\n    augmentation_cls = globals()[cls_name] if cls_name in globals() else None\n    if not issubclass(augmentation_cls, Augmentation) or augmentation_cls == Augmentation:\n        raise ValueError(\'Unknown augmentation: {}\'.format(cls_name))\n    parameters = match.group(\'params\')\n    parameters = [] if parameters is None else parameters.split(\',\')\n    args = []\n    kwargs = {}\n    for parameter in parameters:\n        pair = tuple(list(map(str.strip, (parameter.split(\'=\')))))\n        if len(pair) == 1:\n            args.append(pair)\n        elif len(pair) == 2:\n            kwargs[pair[0]] = pair[1]\n        else:\n            raise ValueError(\'Unable to parse augmentation value assignment\')\n    return augmentation_cls(*args, **kwargs)\n'"
training/deepspeech_training/util/sparse_image_warp.py,37,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Image warping using sparse flow defined at control points.""""""\n\n# The following code is from: https://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/contrib/image/python/ops/sparse_image_warp.py\n# But refactored for dynamic tensor shape compatibility\n# The core idea is to replace every numpy implementation with tensorflow implementation\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tfv1\nfrom tensorflow.compat import dimension_value\nfrom tensorflow.contrib.image.python.ops import dense_image_warp\nfrom tensorflow.contrib.image.python.ops import interpolate_spline\n\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\n\ndef _to_float32(value):\n    return tf.cast(value, tf.float32)\n\ndef _to_int32(value):\n    return tf.cast(value, tf.int32)\n\ndef _get_grid_locations(image_height, image_width):\n    """"""Wrapper for np.meshgrid.""""""\n    tfv1.assert_type(image_height, tf.int32)\n    tfv1.assert_type(image_width, tf.int32)\n\n    y_range = tf.range(image_height)\n    x_range = tf.range(image_width)\n    y_grid, x_grid = tf.meshgrid(y_range, x_range, indexing=\'ij\')\n    return tf.stack((y_grid, x_grid), -1)\n\n\ndef _expand_to_minibatch(tensor, batch_size):\n    """"""Tile arbitrarily-sized np_array to include new batch dimension.""""""\n    ndim = tf.size(tf.shape(tensor))\n    ones = tf.ones((ndim,), tf.int32)\n\n    tiles = tf.concat(([batch_size], ones), 0)\n    return tf.tile(tf.expand_dims(tensor, 0), tiles)\n\n\ndef _get_boundary_locations(image_height, image_width, num_points_per_edge):\n    """"""Compute evenly-spaced indices along edge of image.""""""\n    image_height_end = _to_float32(tf.math.subtract(image_height, 1))\n    image_width_end = _to_float32(tf.math.subtract(image_width, 1))\n    y_range = tf.linspace(0.0, image_height_end, num_points_per_edge + 2)\n    x_range = tf.linspace(0.0, image_height_end, num_points_per_edge + 2)\n    ys, xs = tf.meshgrid(y_range, x_range, indexing=\'ij\')\n    is_boundary = tf.logical_or(\n        tf.logical_or(tf.equal(xs, 0.0), tf.equal(xs, image_width_end)),\n        tf.logical_or(tf.equal(ys, 0.0), tf.equal(ys, image_height_end)))\n    return tf.stack([tf.boolean_mask(ys, is_boundary), tf.boolean_mask(xs, is_boundary)], axis=-1)\n\n\ndef _add_zero_flow_controls_at_boundary(control_point_locations,\n                                        control_point_flows, image_height,\n                                        image_width, boundary_points_per_edge):\n    """"""Add control points for zero-flow boundary conditions.\n\n     Augment the set of control points with extra points on the\n     boundary of the image that have zero flow.\n\n    Args:\n      control_point_locations: input control points\n      control_point_flows: their flows\n      image_height: image height\n      image_width: image width\n      boundary_points_per_edge: number of points to add in the middle of each\n                             edge (not including the corners).\n                             The total number of points added is\n                             4 + 4*(boundary_points_per_edge).\n\n    Returns:\n      merged_control_point_locations: augmented set of control point locations\n      merged_control_point_flows: augmented set of control point flows\n    """"""\n\n    batch_size = dimension_value(tf.shape(control_point_locations)[0])\n\n    boundary_point_locations = _get_boundary_locations(image_height, image_width,\n                                                       boundary_points_per_edge)\n    boundary_point_shape = tf.shape(boundary_point_locations)\n    boundary_point_flows = tf.zeros([boundary_point_shape[0], 2])\n\n    minbatch_locations = _expand_to_minibatch(boundary_point_locations, batch_size)\n    type_to_use = control_point_locations.dtype\n    boundary_point_locations = tf.cast(minbatch_locations, type_to_use)\n\n    minbatch_flows = _expand_to_minibatch(boundary_point_flows, batch_size)\n\n    boundary_point_flows = tf.cast(minbatch_flows, type_to_use)\n\n    merged_control_point_locations = tf.concat(\n        [control_point_locations, boundary_point_locations], 1)\n\n    merged_control_point_flows = tf.concat(\n        [control_point_flows, boundary_point_flows], 1)\n\n    return merged_control_point_locations, merged_control_point_flows\n\n\ndef sparse_image_warp(image,\n                      source_control_point_locations,\n                      dest_control_point_locations,\n                      interpolation_order=2,\n                      regularization_weight=0.0,\n                      num_boundary_points=0,\n                      name=\'sparse_image_warp\'):\n    """"""Image warping using correspondences between sparse control points.\n\n    Apply a non-linear warp to the image, where the warp is specified by\n    the source and destination locations of a (potentially small) number of\n    control points. First, we use a polyharmonic spline\n    (`tf.contrib.image.interpolate_spline`) to interpolate the displacements\n    between the corresponding control points to a dense flow field.\n    Then, we warp the image using this dense flow field\n    (`tf.contrib.image.dense_image_warp`).\n\n    Let t index our control points. For regularization_weight=0, we have:\n    warped_image[b, dest_control_point_locations[b, t, 0],\n                    dest_control_point_locations[b, t, 1], :] =\n    image[b, source_control_point_locations[b, t, 0],\n             source_control_point_locations[b, t, 1], :].\n\n    For regularization_weight > 0, this condition is met approximately, since\n    regularized interpolation trades off smoothness of the interpolant vs.\n    reconstruction of the interpolant at the control points.\n    See `tf.contrib.image.interpolate_spline` for further documentation of the\n    interpolation_order and regularization_weight arguments.\n\n\n    Args:\n      image: `[batch, height, width, channels]` float `Tensor`\n      source_control_point_locations: `[batch, num_control_points, 2]` float\n        `Tensor`\n      dest_control_point_locations: `[batch, num_control_points, 2]` float\n        `Tensor`\n      interpolation_order: polynomial order used by the spline interpolation\n      regularization_weight: weight on smoothness regularizer in interpolation\n      num_boundary_points: How many zero-flow boundary points to include at\n        each image edge.Usage:\n          num_boundary_points=0: don\'t add zero-flow points\n          num_boundary_points=1: 4 corners of the image\n          num_boundary_points=2: 4 corners and one in the middle of each edge\n            (8 points total)\n          num_boundary_points=n: 4 corners and n-1 along each edge\n      name: A name for the operation (optional).\n\n      Note that image and offsets can be of type tf.half, tf.float32, or\n      tf.float64, and do not necessarily have to be the same type.\n\n    Returns:\n      warped_image: `[batch, height, width, channels]` float `Tensor` with same\n        type as input image.\n      flow_field: `[batch, height, width, 2]` float `Tensor` containing the dense\n        flow field produced by the interpolation.\n    """"""\n\n    image = ops.convert_to_tensor(image)\n    source_control_point_locations = ops.convert_to_tensor(\n        source_control_point_locations)\n    dest_control_point_locations = ops.convert_to_tensor(\n        dest_control_point_locations)\n\n    control_point_flows = (\n        dest_control_point_locations - source_control_point_locations)\n\n    clamp_boundaries = num_boundary_points > 0\n    boundary_points_per_edge = num_boundary_points - 1\n\n    with ops.name_scope(name):\n        image_shape = tf.shape(image)\n        batch_size, image_height, image_width = image_shape[0], image_shape[1], image_shape[2]\n\n        # This generates the dense locations where the interpolant\n        # will be evaluated.\n        grid_locations = _get_grid_locations(image_height, image_width)\n\n        flattened_grid_locations = tf.reshape(grid_locations,\n                                              [tf.multiply(image_height, image_width), 2])\n\n        # flattened_grid_locations = constant_op.constant(\n        #     _expand_to_minibatch(flattened_grid_locations, batch_size), image.dtype)\n        flattened_grid_locations = _expand_to_minibatch(flattened_grid_locations, batch_size)\n        flattened_grid_locations = tf.cast(flattened_grid_locations, dtype=image.dtype)\n\n        if clamp_boundaries:\n            (dest_control_point_locations,\n             control_point_flows) = _add_zero_flow_controls_at_boundary(\n                 dest_control_point_locations, control_point_flows, image_height,\n                 image_width, boundary_points_per_edge)\n\n        flattened_flows = interpolate_spline.interpolate_spline(\n            dest_control_point_locations, control_point_flows,\n            flattened_grid_locations, interpolation_order, regularization_weight)\n\n        dense_flows = array_ops.reshape(flattened_flows,\n                                        [batch_size, image_height, image_width, 2])\n\n        warped_image = dense_image_warp.dense_image_warp(image, dense_flows)\n\n        return warped_image, dense_flows\n'"
training/deepspeech_training/util/spectrogram_augmentations.py,42,"b'import tensorflow as tf\nimport tensorflow.compat.v1 as tfv1\n\nfrom .sparse_image_warp import sparse_image_warp\n\ndef augment_freq_time_mask(spectrogram,\n                           frequency_masking_para=30,\n                           time_masking_para=10,\n                           frequency_mask_num=3,\n                           time_mask_num=3):\n    time_max = tf.shape(spectrogram)[1]\n    freq_max = tf.shape(spectrogram)[2]\n    # Frequency masking\n    for _ in range(frequency_mask_num):\n        f = tf.random.uniform(shape=(), minval=0, maxval=frequency_masking_para, dtype=tf.dtypes.int32)\n        f0 = tf.random.uniform(shape=(), minval=0, maxval=freq_max - f, dtype=tf.dtypes.int32)\n        value_ones_freq_prev = tf.ones(shape=[1, time_max, f0])\n        value_zeros_freq = tf.zeros(shape=[1, time_max, f])\n        value_ones_freq_next = tf.ones(shape=[1, time_max, freq_max-(f0+f)])\n        freq_mask = tf.concat([value_ones_freq_prev, value_zeros_freq, value_ones_freq_next], axis=2)\n        # mel_spectrogram[:, f0:f0 + f, :] = 0 #can\'t assign to tensor\n        # mel_spectrogram[:, f0:f0 + f, :] = value_zeros_freq #can\'t assign to tensor\n        spectrogram = spectrogram*freq_mask\n\n    # Time masking\n    for _ in range(time_mask_num):\n        t = tf.random.uniform(shape=(), minval=0, maxval=time_masking_para, dtype=tf.dtypes.int32)\n        t0 = tf.random.uniform(shape=(), minval=0, maxval=time_max - t, dtype=tf.dtypes.int32)\n        value_zeros_time_prev = tf.ones(shape=[1, t0, freq_max])\n        value_zeros_time = tf.zeros(shape=[1, t, freq_max])\n        value_zeros_time_next = tf.ones(shape=[1, time_max-(t0+t), freq_max])\n        time_mask = tf.concat([value_zeros_time_prev, value_zeros_time, value_zeros_time_next], axis=1)\n        # mel_spectrogram[:, :, t0:t0 + t] = 0 #can\'t assign to tensor\n        # mel_spectrogram[:, :, t0:t0 + t] = value_zeros_time #can\'t assign to tensor\n        spectrogram = spectrogram*time_mask\n\n    return spectrogram\n\ndef augment_pitch_and_tempo(spectrogram,\n                            max_tempo=1.2,\n                            max_pitch=1.1,\n                            min_pitch=0.95):\n    original_shape = tf.shape(spectrogram)\n    choosen_pitch = tf.random.uniform(shape=(), minval=min_pitch, maxval=max_pitch)\n    choosen_tempo = tf.random.uniform(shape=(), minval=1, maxval=max_tempo)\n    new_freq_size = tf.cast(tf.cast(original_shape[2], tf.float32)*choosen_pitch, tf.int32)\n    new_time_size = tf.cast(tf.cast(original_shape[1], tf.float32)/(choosen_tempo), tf.int32)\n    spectrogram_aug = tf.image.resize_bilinear(tf.expand_dims(spectrogram, -1), [new_time_size, new_freq_size])\n    spectrogram_aug = tf.image.crop_to_bounding_box(spectrogram_aug, offset_height=0, offset_width=0, target_height=tf.shape(spectrogram_aug)[1], target_width=tf.minimum(original_shape[2], new_freq_size))\n    spectrogram_aug = tf.cond(choosen_pitch < 1,\n                              lambda: tf.image.pad_to_bounding_box(spectrogram_aug, offset_height=0, offset_width=0,\n                                                                   target_height=tf.shape(spectrogram_aug)[1], target_width=original_shape[2]),\n                              lambda: spectrogram_aug)\n    return spectrogram_aug[:, :, :, 0]\n\n\ndef augment_speed_up(spectrogram,\n                     speed_std=0.1):\n    original_shape = tf.shape(spectrogram)\n    choosen_speed = tf.math.abs(tf.random.normal(shape=(), stddev=speed_std)) # abs makes sure the augmention will only speed up\n    choosen_speed = 1 + choosen_speed\n    new_freq_size = tf.cast(tf.cast(original_shape[2], tf.float32), tf.int32)\n    new_time_size = tf.cast(tf.cast(original_shape[1], tf.float32)/(choosen_speed), tf.int32)\n    spectrogram_aug = tf.image.resize_bilinear(tf.expand_dims(spectrogram, -1), [new_time_size, new_freq_size])\n    return spectrogram_aug[:, :, :, 0]\n\ndef augment_dropout(spectrogram,\n                    keep_prob=0.95):\n    return tf.nn.dropout(spectrogram, rate=1-keep_prob)\n\n\ndef augment_sparse_warp(spectrogram, time_warping_para=20, interpolation_order=2, regularization_weight=0.0, num_boundary_points=1, num_control_points=1):\n    """"""Reference: https://arxiv.org/pdf/1904.08779.pdf\n    Args:\n        spectrogram: `[batch, time, frequency]` float `Tensor`\n        time_warping_para: \'W\' parameter in paper\n        interpolation_order: used to put into `sparse_image_warp`\n        regularization_weight: used to put into `sparse_image_warp`\n        num_boundary_points: used to put into `sparse_image_warp`,\n                            default=1 means boundary points on 4 corners of the image\n        num_control_points: number of control points\n    Returns:\n        warped_spectrogram: `[batch, time, frequency]` float `Tensor` with same\n            type as input image.\n    """"""\n    # reshape to fit `sparse_image_warp`\'s input shape\n    # (1, time steps, freq, 1), batch_size must be 1\n    spectrogram = tf.expand_dims(spectrogram, -1)\n\n    original_shape = tf.shape(spectrogram)\n    tau, freq_size = original_shape[1], original_shape[2]\n\n    # to protect short audio\n    time_warping_para = tf.math.minimum(\n        time_warping_para, tf.math.subtract(tf.math.floordiv(tau, 2), 1))\n\n    # don\'t choose boundary frequency\n    choosen_freqs = tf.random.shuffle(\n        tf.add(tf.range(freq_size - 3), 1))[0: num_control_points]\n\n    source_max = tau - time_warping_para\n    source_min = tf.math.minimum(source_max - num_control_points, time_warping_para)\n\n    choosen_times = tf.random.shuffle(tf.range(source_min, limit=source_max))[0: num_control_points]\n    dest_time_widths = tfv1.random_uniform([num_control_points], tf.negative(time_warping_para), time_warping_para, tf.int32)\n\n    sources = []\n    dests = []\n    for i in range(num_control_points):\n        # generate source points `t` of time axis between (W, tau-W)\n        rand_source_time = choosen_times[i]\n        rand_dest_time = rand_source_time + dest_time_widths[i]\n\n        choosen_freq = choosen_freqs[i]\n        sources.append([rand_source_time, choosen_freq])\n        dests.append([rand_dest_time, choosen_freq])\n\n    source_control_point_locations = tf.cast([sources], tf.float32)\n    dest_control_point_locations = tf.cast([dests], tf.float32)\n\n    warped_spectrogram, _ = sparse_image_warp(spectrogram,\n                                              source_control_point_locations=source_control_point_locations,\n                                              dest_control_point_locations=dest_control_point_locations,\n                                              interpolation_order=interpolation_order,\n                                              regularization_weight=regularization_weight,\n                                              num_boundary_points=num_boundary_points)\n    return tf.reshape(warped_spectrogram, shape=(1, -1, freq_size))\n'"
training/deepspeech_training/util/stm.py,0,"b'import codecs\nimport unicodedata\n\nclass STMSegment(object):\n    r""""""\n    Representation of an individual segment in an STM file.\n    """"""\n    def __init__(self, stm_line):\n        tokens = stm_line.split()\n        self._filename    = tokens[0]\n        self._channel     = tokens[1]\n        self._speaker_id  = tokens[2]\n        self._start_time  = float(tokens[3])\n        self._stop_time   = float(tokens[4])\n        self._labels      = tokens[5]\n        self._transcript  = """"\n        for token in tokens[6:]:\n          self._transcript += token + "" ""\n        # We need to do the encode-decode dance here because encode\n        # returns a bytes() object on Python 3, and text_to_char_array\n        # expects a string.\n        self._transcript = unicodedata.normalize(""NFKD"", self._transcript.strip())  \\\n                                      .encode(""ascii"", ""ignore"")                    \\\n                                      .decode(""ascii"", ""ignore"")\n\n    @property\n    def filename(self):\n        return self._filename\n\n    @property\n    def channel(self):\n        return self._channel\n\n    @property\n    def speaker_id(self):\n        return self._speaker_id\n\n    @property\n    def start_time(self):\n        return self._start_time\n\n    @property\n    def stop_time(self):\n        return self._stop_time\n\n    @property\n    def labels(self):\n        return self._labels\n\n    @property\n    def transcript(self):\n        return self._transcript\n\ndef parse_stm_file(stm_file):\n    r""""""\n    Parses an STM file at ``stm_file`` into a list of :class:`STMSegment`.\n    """"""\n    stm_segments = []\n    with codecs.open(stm_file, encoding=""utf-8"") as stm_lines:\n        for stm_line in stm_lines:\n            stmSegment = STMSegment(stm_line)\n            if not ""ignore_time_segment_in_scoring"" == stmSegment.transcript:\n                stm_segments.append(stmSegment)\n    return stm_segments\n'"
training/deepspeech_training/util/taskcluster.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import print_function, absolute_import, division\n\nimport argparse\nimport errno\nimport gzip\nimport os\nimport platform\nimport six.moves.urllib as urllib\nimport stat\nimport subprocess\nimport sys\n\nfrom pkg_resources import parse_version\n\n\nDEFAULT_SCHEMES = {\n    \'deepspeech\': \'https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.deepspeech.native_client.%(branch_name)s.%(arch_string)s/artifacts/public/%(artifact_name)s\',\n    \'tensorflow\': \'https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.tensorflow.pip.%(branch_name)s.%(arch_string)s/artifacts/public/%(artifact_name)s\'\n}\n\nTASKCLUSTER_SCHEME = os.getenv(\'TASKCLUSTER_SCHEME\', DEFAULT_SCHEMES[\'deepspeech\'])\n\ndef get_tc_url(arch_string, artifact_name=\'native_client.tar.xz\', branch_name=\'master\'):\n    assert arch_string is not None\n    assert artifact_name is not None\n    assert artifact_name\n    assert branch_name is not None\n    assert branch_name\n\n    return TASKCLUSTER_SCHEME % {\'arch_string\': arch_string, \'artifact_name\': artifact_name, \'branch_name\': branch_name}\n\ndef maybe_download_tc(target_dir, tc_url, progress=True):\n    def report_progress(count, block_size, total_size):\n        percent = (count * block_size * 100) // total_size\n        sys.stdout.write(""\\rDownloading: %d%%"" % percent)\n        sys.stdout.flush()\n\n        if percent >= 100:\n            print(\'\\n\')\n\n    assert target_dir is not None\n\n    target_dir = os.path.abspath(target_dir)\n    try:\n        os.makedirs(target_dir)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise e\n    assert os.path.isdir(os.path.dirname(target_dir))\n\n    tc_filename = os.path.basename(tc_url)\n    target_file = os.path.join(target_dir, tc_filename)\n    is_gzip = False\n    if not os.path.isfile(target_file):\n        print(\'Downloading %s ...\' % tc_url)\n        _, headers = urllib.request.urlretrieve(tc_url, target_file, reporthook=(report_progress if progress else None))\n        is_gzip = headers.get(\'Content-Encoding\') == \'gzip\'\n    else:\n        print(\'File already exists: %s\' % target_file)\n\n    if is_gzip:\n        with open(target_file, ""r+b"") as frw:\n            decompressed = gzip.decompress(frw.read())\n            frw.seek(0)\n            frw.write(decompressed)\n            frw.truncate()\n\n    return target_file\n\ndef maybe_download_tc_bin(**kwargs):\n    final_file = maybe_download_tc(kwargs[\'target_dir\'], kwargs[\'tc_url\'], kwargs[\'progress\'])\n    final_stat = os.stat(final_file)\n    os.chmod(final_file, final_stat.st_mode | stat.S_IEXEC)\n\ndef read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname)).read()\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Tooling to ease downloading of components from TaskCluster.\')\n    parser.add_argument(\'--target\', required=False,\n                        help=\'Where to put the native client binary files\')\n    parser.add_argument(\'--arch\', required=False,\n                        help=\'Which architecture to download binaries for. ""arm"" for ARM 7 (32-bit), ""arm64"" for ARM64, ""gpu"" for CUDA enabled x86_64 binaries, ""cpu"" for CPU-only x86_64 binaries, ""osx"" for CPU-only x86_64 OSX binaries. Optional (""cpu"" by default)\')\n    parser.add_argument(\'--artifact\', required=False,\n                        default=\'native_client.tar.xz\',\n                        help=\'Name of the artifact to download. Defaults to ""native_client.tar.xz""\')\n    parser.add_argument(\'--source\', required=False, default=None,\n                        help=\'Name of the TaskCluster scheme to use.\')\n    parser.add_argument(\'--branch\', required=False,\n                        help=\'Branch name to use. Defaulting to current content of VERSION file.\')\n\n    args = parser.parse_args()\n\n    if not args.target and not args.decoder:\n        print(\'Pass either --target or --decoder.\')\n        sys.exit(1)\n\n    is_arm = \'arm\' in platform.machine()\n    is_mac = \'darwin\' in sys.platform\n    is_64bit = sys.maxsize > (2**31 - 1)\n    is_ucs2 = sys.maxunicode < 0x10ffff\n\n    if not args.arch:\n        if is_arm:\n            args.arch = \'arm64\' if is_64bit else \'arm\'\n        elif is_mac:\n            args.arch = \'osx\'\n        else:\n            args.arch = \'cpu\'\n\n    if not args.branch:\n        version_string = read(\'../VERSION\').strip()\n        ds_version = parse_version(version_string)\n        args.branch = ""v{}"".format(version_string)\n    else:\n        ds_version = parse_version(args.branch)\n\n    if args.source is not None:\n        if args.source in DEFAULT_SCHEMES:\n            global TASKCLUSTER_SCHEME\n            TASKCLUSTER_SCHEME = DEFAULT_SCHEMES[args.source]\n        else:\n            print(\'No such scheme: %s\' % args.source)\n            sys.exit(1)\n\n    maybe_download_tc(target_dir=args.target, tc_url=get_tc_url(args.arch, args.artifact, args.branch))\n\n    if args.artifact == ""convert_graphdef_memmapped_format"":\n        convert_graph_file = os.path.join(args.target, args.artifact)\n        final_stat = os.stat(convert_graph_file)\n        os.chmod(convert_graph_file, final_stat.st_mode | stat.S_IEXEC)\n\n    if \'.tar.\' in args.artifact:\n        subprocess.check_call([\'tar\', \'xvf\', os.path.join(args.target, args.artifact), \'-C\', args.target])\n\nif __name__ == \'__main__\':\n    main()\n'"
training/deepspeech_training/util/text.py,0,"b'from __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport struct\n\nfrom six.moves import range\n\nclass Alphabet(object):\n    def __init__(self, config_file):\n        self._config_file = config_file\n        self._label_to_str = {}\n        self._str_to_label = {}\n        self._size = 0\n        if config_file:\n            with open(config_file, \'r\', encoding=\'utf-8\') as fin:\n                for line in fin:\n                    if line[0:2] == \'\\\\#\':\n                        line = \'#\\n\'\n                    elif line[0] == \'#\':\n                        continue\n                    self._label_to_str[self._size] = line[:-1] # remove the line ending\n                    self._str_to_label[line[:-1]] = self._size\n                    self._size += 1\n\n    def _string_from_label(self, label):\n        return self._label_to_str[label]\n\n    def _label_from_string(self, string):\n        try:\n            return self._str_to_label[string]\n        except KeyError as e:\n            raise KeyError(\n                \'ERROR: Your transcripts contain characters (e.g. \\\'{}\\\') which do not occur in \\\'{}\\\'! Use \' \\\n                \'util/check_characters.py to see what characters are in your [train,dev,test].csv transcripts, and \' \\\n                \'then add all these to \\\'{}\\\'.\'.format(string, self._config_file, self._config_file)\n            ).with_traceback(e.__traceback__)\n\n    def has_char(self, char):\n        return char in self._str_to_label\n\n    def encode(self, string):\n        res = []\n        for char in string:\n            res.append(self._label_from_string(char))\n        return res\n\n    def decode(self, labels):\n        res = \'\'\n        for label in labels:\n            res += self._string_from_label(label)\n        return res\n\n    def serialize(self):\n        # Serialization format is a sequence of (key, value) pairs, where key is\n        # a uint16_t and value is a uint16_t length followed by `length` UTF-8\n        # encoded bytes with the label.\n        res = bytearray()\n\n        # We start by writing the number of pairs in the buffer as uint16_t.\n        res += struct.pack(\'<H\', self._size)\n        for key, value in self._label_to_str.items():\n            value = value.encode(\'utf-8\')\n            # struct.pack only takes fixed length strings/buffers, so we have to\n            # construct the correct format string with the length of the encoded\n            # label.\n            res += struct.pack(\'<HH{}s\'.format(len(value)), key, len(value), value)\n        return bytes(res)\n\n    def size(self):\n        return self._size\n\n    def config_file(self):\n        return self._config_file\n\n\nclass UTF8Alphabet(object):\n    @staticmethod\n    def _string_from_label(_):\n        assert False\n\n    @staticmethod\n    def _label_from_string(_):\n        assert False\n\n    @staticmethod\n    def encode(string):\n        # 0 never happens in the data, so we can shift values by one, use 255 for\n        # the CTC blank, and keep the alphabet size = 256\n        return np.frombuffer(string.encode(\'utf-8\'), np.uint8).astype(np.int32) - 1\n\n    @staticmethod\n    def decode(labels):\n        # And here we need to shift back up\n        return bytes(np.asarray(labels, np.uint8) + 1).decode(\'utf-8\', errors=\'replace\')\n\n    @staticmethod\n    def size():\n        return 255\n\n    @staticmethod\n    def serialize():\n        res = bytearray()\n        res += struct.pack(\'<h\', 255)\n        for i in range(255):\n            # Note that we also shift back up in the mapping constructed here\n            # so that the native client sees the correct byte values when decoding.\n            res += struct.pack(\'<hh1s\', i, 1, bytes([i+1]))\n        return bytes(res)\n\n    @staticmethod\n    def deserialize(buf):\n        size = struct.unpack(\'<I\', buf)[0]\n        assert size == 255\n        return UTF8Alphabet()\n\n    @staticmethod\n    def config_file():\n        return \'\'\n\n\ndef text_to_char_array(transcript, alphabet, context=\'\'):\n    r""""""\n    Given a transcript string, map characters to\n    integers and return a numpy array representing the processed string.\n    Use a string in `context` for adding text to raised exceptions.\n    """"""\n    try:\n        transcript = alphabet.encode(transcript)\n        if len(transcript) == 0:\n            raise ValueError(\'While processing {}: Found an empty transcript! \'\n                             \'You must include a transcript for all training data.\'\n                             .format(context))\n        return transcript\n    except KeyError as e:\n        # Provide the row context (especially wav_filename) for alphabet errors\n        raise ValueError(\'While processing: {}\\n{}\'.format(context, e))\n\n\n# The following code is from: http://hetland.org/coding/python/levenshtein.py\n\n# This is a straightforward implementation of a well-known algorithm, and thus\n# probably shouldn\'t be covered by copyright to begin with. But in case it is,\n# the author (Magnus Lie Hetland) has, to the extent possible under law,\n# dedicated all copyright and related and neighboring rights to this software\n# to the public domain worldwide, by distributing it under the CC0 license,\n# version 1.0. This software is distributed without any warranty. For more\n# information, see <http://creativecommons.org/publicdomain/zero/1.0>\n\ndef levenshtein(a, b):\n    ""Calculates the Levenshtein distance between a and b.""\n    n, m = len(a), len(b)\n    if n > m:\n        # Make sure n <= m, to use O(min(n,m)) space\n        a, b = b, a\n        n, m = m, n\n\n    current = list(range(n+1))\n    for i in range(1, m+1):\n        previous, current = current, [i]+[0]*n\n        for j in range(1, n+1):\n            add, delete = previous[j]+1, current[j-1]+1\n            change = previous[j-1]\n            if a[j-1] != b[i-1]:\n                change = change + 1\n            current[j] = min(add, delete, change)\n\n    return current[n]\n'"
