file_path,api_count,code
setup.py,0,"b'import setuptools\nimport smartnlp\n\nwith open(""smartnlp/README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nREQUIRED_PACKAGES = [\n    \'h5py\', \'requests\'\n]\n\nsetuptools.setup(\n    name=""smartnlp"",\n    version=smartnlp.version,\n    author=""msgi(\xe6\x85\xa2\xe6\x97\xb6\xe5\x85\x89)"",\n    author_email=""mayuan120226@sina.cn"",\n    description=""Easy-to-use and Extendable package of deep learning based nlp (Natural Language Processing) ""\n                ""tools with tensorflow 2.x ."",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/msgi/nlp-journey.git"",\n    # download_url=\'https://github.com/msgi/nlp-journey/tags\',\n    packages=setuptools.find_packages(\n        exclude=[""tests""]),\n    python_requires="">=3.6"",  # \'>=3.4\',  # 3.4.6\n    install_requires=REQUIRED_PACKAGES,\n    extras_require={\n        ""cpu"": [""tensorflow>=2.0.0""],\n        ""gpu"": [""tensorflow-gpu>=2.0.0""],\n    },\n    entry_points={\n    },\n    classifiers=(\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Operating System :: OS Independent"",\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ),\n    license=""Apache-2.0"",\n    keywords=[\'nlp\', \'natural language processing\', ""nlu"", ""natural language understanding""\n                                                           \'deep learning\', \'tensorflow\', \'keras\'],\n)\n'"
examples/cls_base_demo.py,0,"b""from smartnlp.classfication.deep_classifier import BasicTextClassifier\n\nif __name__ == '__main__':\n    base_classifier = BasicTextClassifier(model_path='./model/base/',\n                                          config_path='./model/base/config.pkl',\n                                          train=True,\n                                          vector_path='/data/GoogleNews-vectors-negative300.bin.gz')\n    out = base_classifier.predict(\n        ['this is very good movie , i want to watch it again!', 'this is very bad movie , i hate it!'])\n    out2 = base_classifier.predict('this is very good movie , i want to watch it again!')\n    print(out)\n    print(out2)\n"""
examples/cls_cnn_demo.py,0,"b""from smartnlp.classfication.deep_classifier import TextCnnClassifier\n\nif __name__ == '__main__':\n    base_classifier = TextCnnClassifier(model_path='./model/cnn/',\n                                        config_path='./model/cnn/config.pkl',\n                                        train=True,\n                                        vector_path='/data/GoogleNews-vectors-negative300.bin.gz')\n    out = base_classifier.predict(\n        ['this is very good movie , i want to watch it again!', 'this is very bad movie , i hate it!'])\n    out2 = base_classifier.predict('this is very good movie , i want to watch it again!')\n    print(out)\n    print(out2)\n"""
examples/cls_fasttext_demo.py,0,"b""from smartnlp.classfication.fasttext_classifier import FastTextClassifier\n\nif __name__ == '__main__':\n    train_path = '../tutorials/03.poem_generation/data/imdb/'\n    model = FastTextClassifier('./model/fasttext/classifier',\n                               train=True,\n                               file_path=train_path)\n    model.predict(\n        'this is the weepy that beaches never was as much as i wanted to love beaches it always seemed '\n        'too hurried for me to feel for it its soundtrack is one of my favorite albums though stella on the other hand '\n        'moves at a slower and occasionally too slow pace '\n        'and though it s somewhat manipulative in its tears inducing tale about a self sacrificial mother it works '\n        'because bette and the rest of the cast turn in great performances')\n"""
examples/cls_han_demo.py,0,"b""from smartnlp.classfication.deep_classifier import TextHanClassifier\n\nif __name__ == '__main__':\n    base_classifier = TextHanClassifier(model_path='./model/han/',\n                                        config_path='./model/han/config.pkl',\n                                        train=True,\n                                        vector_path='/data/GoogleNews-vectors-negative300.bin.gz')\n    out = base_classifier.predict(\n        ['this is very good movie , i want to watch it again!', 'this is very bad movie , i hate it!'])\n    out2 = base_classifier.predict('this is very good movie , i want to watch it again!')\n    print(out)\n    print(out2)\n"""
examples/cls_rcnn_demo.py,0,"b""from smartnlp.classfication.deep_classifier import TextRCNNClassifier\n\nif __name__ == '__main__':\n    base_classifier = TextRCNNClassifier(model_path='./model/rcnn/',\n                                         config_path='./model/rcnn/config.pkl',\n                                         train=True,\n                                         vector_path='/data/GoogleNews-vectors-negative300.bin.gz')\n    out = base_classifier.predict(\n        ['this is very good movie , i want to watch it again!', 'this is very bad movie , i hate it!'])\n    out2 = base_classifier.predict('this is very good movie , i want to watch it again!')\n    print(out)\n    print(out2)\n"""
examples/cls_rnn_attention_demo.py,0,"b""from smartnlp.classfication.deep_classifier import TextRNNAttentionClassifier\n\nif __name__ == '__main__':\n    base_classifier = TextRNNAttentionClassifier(model_path='./model/rnn_att/',\n                                                 config_path='./model/rnn_att/config.pkl',\n                                                 train=True,\n                                                 vector_path='/data/GoogleNews-vectors-negative300.bin.gz')\n    out = base_classifier.predict(\n        ['this is very good movie , i want to watch it again!', 'this is very bad movie , i hate it!'])\n    out2 = base_classifier.predict('this is very good movie , i want to watch it again!')\n    print(out)\n    print(out2)\n"""
examples/cls_rnn_demo.py,0,"b""from smartnlp.classfication.deep_classifier import TextRnnClassifier\n\nif __name__ == '__main__':\n    base_classifier = TextRnnClassifier(model_path='./model/rnn/',\n                                        config_path='./model/rnn/config.pkl',\n                                        train=True,\n                                        vector_path='/data/GoogleNews-vectors-negative300.bin.gz')\n    out = base_classifier.predict(\n        ['this is very good movie , i want to watch it again!', 'this is very bad movie , i hate it!'])\n    out2 = base_classifier.predict('this is very good movie , i want to watch it again!')\n    print(out)\n    print(out2)\n"""
examples/cls_svm_demo.py,0,"b""from smartnlp.classfication.svm_classifier import SVMClassifier\n\nif __name__ == '__main__':\n    svm_model = SVMClassifier('model/svm/model.pkl',\n                              './data/imdb/aclImdb.txt',\n                              train=True)\n    # svm_model = SVMClassifier('model/svm/model.pkl')\n    svm_model.predict(['i like it ! its very interesting', 'I don\\'t like it, it\\'s boring'])\n"""
examples/eda_demo.py,0,"b'# coding=utf-8\n# created by msgi on 2020/5/18\n\nfrom smartnlp.augmentation.eda import EDA\n\nif __name__ == ""__main__"":\n    eda = EDA(""./data/stopwords/stopwords.txt"")\n    aug_sentences = eda.fit_transform(""\xe9\xb1\xbc\xe9\xa6\x99\xe8\x82\x89\xe4\xb8\x9d\xe5\xa5\xbd\xe5\x90\x83\xe7\x9a\x84\xe5\xbe\x88\xef\xbc\x8c\xe4\xbd\xa0\xe8\xa6\x81\xe4\xb8\x8d\xe8\xa6\x81\xe6\x9d\xa5\xe5\xb0\x9d\xe4\xb8\x80\xe5\xb0\x9d"")\n    print(aug_sentences)\n'"
examples/emb_fasttext_demo.py,0,"b""from smartnlp.embedding.fasttext_model import FastTextModel\n\nif __name__ == '__main__':\n    # cbow \xe6\xa8\xa1\xe5\x9e\x8b\n    model = FastTextModel('data/tianlong_seg.txt', 'model/fasttext/model.vec', model_type='cbow')\n    # skipgram \xe6\xa8\xa1\xe5\x9e\x8b\n    # model = FastTextModel('data/tianlong_seg.txt', 'model/fasttext/model')\n    print(model.get_nearest_neighbors('\xe6\xae\xb5\xe8\xaa\x89', 10))\n"""
examples/emb_word2vec_demo.py,0,"b""from smartnlp.embedding.word2vec import GensimWord2VecModel\n\nif __name__ == '__main__':\n    word_vec_model = GensimWord2VecModel('data/tianlong.txt', 'model/gensim/model.txt')\n\n    print(word_vec_model.similar('\xe6\xae\xb5\xe8\xaa\x89'))\n    print('**************************************************')\n"""
examples/ner_bilstm_crf_demo.py,0,"b""from smartnlp.ner.bilstm_crf import BiLSTMCRFNamedEntityRecognition\n\nif __name__ == '__main__':\n    ner = BiLSTMCRFNamedEntityRecognition('model/ner/',\n                                          'model/ner/config.pkl',\n                                          train=True,\n                                          file_path='data/ner')\n    ner.predict('\xe4\xb8\xad\xe5\x8d\x8e\xe4\xba\xba\xe6\xb0\x91\xe5\x85\xb1\xe5\x92\x8c\xe5\x9b\xbd\xe5\x9b\xbd\xe5\x8a\xa1\xe9\x99\xa2\xe6\x80\xbb\xe7\x90\x86\xe5\x91\xa8\xe6\x81\xa9\xe6\x9d\xa5\xe5\x9c\xa8\xe5\xa4\x96\xe4\xba\xa4\xe9\x83\xa8\xe9\x95\xbf\xe9\x99\x88\xe6\xaf\x85\xe7\x9a\x84\xe9\x99\xaa\xe5\x90\x8c\xe4\xb8\x8b\xef\xbc\x8c\xe8\xbf\x9e\xe7\xbb\xad\xe8\xae\xbf\xe9\x97\xae\xe4\xba\x86\xe5\x9f\x83\xe5\xa1\x9e\xe4\xbf\x84\xe6\xaf\x94\xe4\xba\x9a\xe7\xad\x89\xe9\x9d\x9e\xe6\xb4\xb210\xe5\x9b\xbd\xe4\xbb\xa5\xe5\x8f\x8a\xe9\x98\xbf\xe5\xb0\x94\xe5\xb7\xb4\xe5\xb0\xbc\xe4\xba\x9a')\n"""
examples/nmt_transformer_demo.py,1,"b'# coding=utf-8\n# created by msgi on 2020/4/28 4:37 \xe4\xb8\x8b\xe5\x8d\x88\nimport os\nimport tensorflow_datasets as tfds\n\nimport tensorflow as tf\nfrom datetime import datetime\nfrom smartnlp.nmt.training import Trainer\nfrom smartnlp.custom.learning_rate.learning_rate import CustomSchedule\nfrom smartnlp.custom.model.transformer import Transformer\nfrom smartnlp.nmt.data_process import DataProcessor\n\n# \xe8\xae\xbe\xe7\xbd\xae\xe4\xbd\xbf\xe7\x94\xa8GPU\n# os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1""\n\nexamples, metadata = tfds.load(\'ted_hrlr_translate/pt_to_en\', with_info=True,\n                               as_supervised=True)\ntrain_examples, val_examples = examples[\'train\'], examples[\'validation\']\n\ndata_processor = DataProcessor()\ntrain_dataset, test_dataset, tokenizer_feat, tokenizer_tar = data_processor.init_preprocess(train_examples,\n                                                                                            val_examples)\n\nEPOCHS = 100\nnum_layers = 6\nd_model = 128\ndff = 512\nnum_heads = 8\ninput_vocab_size = tokenizer_feat.vocab_size + 2\ntarget_vocab_size = tokenizer_tar.vocab_size + 2\ndropout_rate = 0.1\n\n# Custom Scheduler\nlearning_rate = CustomSchedule(d_model)\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n\n# Transformer\ntransformer = Transformer(d_model=d_model, num_heads=num_heads, num_layers=num_layers,\n                          target_vocab_size=target_vocab_size, input_vocab_size=input_vocab_size,\n                          dff=dff, rate=dropout_rate)\n\n# Trainer\nprint(f\'\\n\\nBeginning training for {EPOCHS} epochs @ {datetime.now()}...\\n\')\ntrainer = Trainer(train_dataset=train_dataset,\n                  test_dataset=test_dataset,\n                  learning_rate=learning_rate,\n                  optimizer=optimizer,\n                  transformer=transformer,\n                  epochs=EPOCHS,\n                  checkpoint_path=\'./models/checkpoints/\',\n                  tb_log_dir=\'./logs/gradient_tape/\'\n\n                  )\n\nloss_hist, acc_hist = trainer.train()\n'"
examples/sim_siamese_demo.py,0,"b""from smartnlp.simililarity.siamese_similarity import SiameseSimilarity\n\nif __name__ == '__main__':\n    siamese = SiameseSimilarity('model/sim/',\n                                'model/sim/config.pkl',\n                                train=True,\n                                data_path='./data/similarity',\n                                embedding_file='/data/GoogleNews-vectors-negative300.bin.gz')\n    print(siamese.predict('Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?',\n                          'I m a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?'))\n"""
examples/util_pre_process_demo.py,0,"b'from smartnlp.utils.pre_process import seg_to_file\n\nif __name__ == \'__main__\':\n    seg_to_file(\'data/tianlong.txt\', ""data/tianlong_seg.txt"")\n'"
examples/web_sanic_demo.py,0,"b'from sanic import Sanic\nfrom sanic.response import json\n\n# \xe5\x90\xaf\xe5\x8a\xa8web\napp = Sanic()\nmodel = None\n\n\n@app.route(""/predict"", methods=[\'POST\'])\nasync def predict(request):\n    """"""\n    \xe9\x87\x87\xe7\x94\xa8restful\xe6\x8e\xa5\xe5\x8f\xa3\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f,\xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\x86\xe7\xb1\xbb\xe7\xbb\x93\xe6\x9e\x9c\n    :param request: {\n                        ""sentence"": ""\xe5\xbe\x85\xe6\x8e\xa8\xe6\xb5\x8b\xe6\x96\x87\xe6\x9c\xac""\n                    }\n    :return:\n    """"""\n    nlp = request.json.get(\'sentence\')\n    answer = model.predict(nlp)\n\n    ans = answer[0]\n\n    return json({\'category\': ans})\n\n\nif __name__ == \'__main__\':\n    app.run(host=""127.0.0.1"", port=8000)\n'"
smartnlp/__init__.py,0,"b'version = ""0.0.3""\n'"
smartnlp/augmentation/__init__.py,0,b''
smartnlp/augmentation/eda.py,0,"b'import random\nfrom random import shuffle\nimport jieba\nimport synonyms\n\nrandom.seed(2030)\n\n\nclass EDA:\n    def __init__(self, stop_path):\n        self.stopwords = self._load_stopwords(stop_path)\n\n    def fit_transform(self, sentence,\n                      alpha_sr=0.1,\n                      alpha_ri=0.1,\n                      alpha_rs=0.1,\n                      p_rd=0.1,\n                      num_aug=9):\n        """"""\n        \xe7\xbb\x9f\xe4\xb8\x80\xe6\x9b\xbf\xe6\x8d\xa2\n        """"""\n        seg_list = jieba.cut(sentence)\n        seg_list = "" "".join(seg_list)\n        words = list(seg_list.split())\n        num_words = len(words)\n\n        augmented_sentences = []\n        num_new_per_technique = int(num_aug / 4) + 1\n        n_sr = max(1, int(alpha_sr * num_words))\n        n_ri = max(1, int(alpha_ri * num_words))\n        n_rs = max(1, int(alpha_rs * num_words))\n\n        # \xe5\x90\x8c\xe4\xb9\x89\xe8\xaf\x8d\xe6\x9b\xbf\xe6\x8d\xa2sr\n        for _ in range(num_new_per_technique):\n            a_words = self._synonym_replacement(words, self.stopwords, n_sr)\n            augmented_sentences.append(\' \'.join(a_words))\n\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe6\x8f\x92\xe5\x85\xa5ri\n        for _ in range(num_new_per_technique):\n            a_words = self._random_insertion(words, n_ri)\n            augmented_sentences.append(\' \'.join(a_words))\n\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe4\xba\xa4\xe6\x8d\xa2rs\n        for _ in range(num_new_per_technique):\n            a_words = self._random_swap(words, n_rs)\n            augmented_sentences.append(\' \'.join(a_words))\n\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\xa0\xe9\x99\xa4rd\n        for _ in range(num_new_per_technique):\n            a_words = self._random_deletion(words, p_rd)\n            augmented_sentences.append(\' \'.join(a_words))\n\n        shuffle(augmented_sentences)\n\n        if num_aug >= 1:\n            augmented_sentences = augmented_sentences[:num_aug]\n        else:\n            keep_prob = num_aug / len(augmented_sentences)\n            augmented_sentences = [\n                s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n\n        augmented_sentences.append(seg_list)\n\n        return augmented_sentences\n\n    @staticmethod\n    def _load_stopwords(stop_path):\n        """"""\n        \xe5\x8a\xa0\xe8\xbd\xbd\xe5\x81\x9c\xe7\x94\xa8\xe8\xaf\x8d\xe8\xa1\xa8\n        """"""\n        with open(stop_path, \'r\', encoding=\'utf-8\') as f:\n            stop_words = f.readlines()\n            stop_words = [stop_word.strip() for stop_word in stop_words]\n        return stop_words\n\n    @staticmethod\n    def _get_synonyms(word):\n        """"""\n        \xe8\x8e\xb7\xe5\xbe\x97\xe8\xaf\x8d\xe7\x9a\x84\xe5\x90\x8c\xe4\xb9\x89\xe8\xaf\x8d\n        """"""\n        return synonyms.nearby(word)[0]\n\n    def _add_word(self, new_words):\n        """"""\n        \xe6\xb7\xbb\xe5\x8a\xa0\xe6\x96\xb0\xe8\xaf\x8d\n        """"""\n        synonyms_ = []\n        counter = 0\n        while len(synonyms_) < 1:\n            random_word = new_words[random.randint(0, len(new_words) - 1)]\n            synonyms_ = self._get_synonyms(random_word)\n            counter += 1\n            if counter >= 10:\n                return\n        random_synonym = random.choice(synonyms_)\n        random_idx = random.randint(0, len(new_words) - 1)\n        new_words.insert(random_idx, random_synonym)\n\n    @staticmethod\n    def _swap_word(new_words):\n        """"""\n        \xe8\xb0\x83\xe6\x8d\xa2\xe4\xbd\x8d\xe7\xbd\xae\n        """"""\n        random_idx_1 = random.randint(0, len(new_words) - 1)\n        random_idx_2 = random_idx_1\n        counter = 0\n        while random_idx_2 == random_idx_1:\n            random_idx_2 = random.randint(0, len(new_words) - 1)\n            counter += 1\n            if counter > 3:\n                return new_words\n        new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n        return new_words\n\n    @staticmethod\n    def _random_deletion(words, p):\n        """"""\n        \xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\xa0\xe9\x99\xa4\n        """"""\n        if len(words) == 1:\n            return words\n\n        new_words = []\n        for word in words:\n            r = random.uniform(0, 1)\n            if r > p:\n                new_words.append(word)\n\n        if len(new_words) == 0:\n            rand_int = random.randint(0, len(words) - 1)\n            return [words[rand_int]]\n\n        return new_words\n\n    def _random_swap(self, words, n):\n        """"""\n        \xe9\x9a\x8f\xe6\x9c\xba\xe6\x9b\xbf\xe6\x8d\xa2\n        """"""\n        new_words = words.copy()\n        for _ in range(n):\n            new_words = self._swap_word(new_words)\n        return new_words\n\n    def _random_insertion(self, words, n):\n        """"""\n        \xe9\x9a\x8f\xe6\x9c\xba\xe6\xb7\xbb\xe5\x8a\xa0\n        """"""\n        new_words = words.copy()\n        for _ in range(n):\n            self._add_word(new_words)\n        return new_words\n\n    def _synonym_replacement(self, words, stopwords, n):\n        """"""\n        \xe5\x90\x8c\xe4\xb9\x89\xe8\xaf\x8d\xe6\x9b\xbf\xe6\x8d\xa2\n        :param words: \xe8\xa6\x81\xe6\x9b\xbf\xe6\x8d\xa2\xe7\x9a\x84\xe8\xaf\x8d\n        :param stopwords: \xe5\x81\x9c\xe7\x94\xa8\xe8\xaf\x8d\n        """"""\n        new_words = words.copy()\n        random_word_list = list(set([word for word in words if word not in stopwords]))\n        random.shuffle(random_word_list)\n        num_replaced = 0\n        for random_word in random_word_list:\n            synonyms_ = self._get_synonyms(random_word)\n            if len(synonyms_) >= 1:\n                synonym = random.choice(synonyms_)\n                new_words = [synonym if word == random_word else word for word in new_words]\n                num_replaced += 1\n            if num_replaced >= n:\n                break\n\n        sentence = \' \'.join(new_words)\n        new_words = sentence.split(\' \')\n\n        return new_words'"
smartnlp/classfication/__init__.py,0,b''
smartnlp/classfication/deep_classifier.py,2,"b'import os\nfrom collections import Counter\n\nimport numpy as np\nimport tensorflow as tf\nfrom keras_preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\n\nfrom smartnlp.custom.layer.attention import VanillaRNNAttention\nfrom smartnlp.utils.loader import load_bin_word2vec\nfrom smartnlp.utils.loader import load_en_stopwords, save_config, load_config, load_model, save_model\nfrom smartnlp.utils.plot_model_history import plot\n\n\nclass BasicTextClassifier:\n    """"""\n    A basic text classifier.\n    Argument:\n        model_path: The model path: if you dont have a model, after the training,\n        this will be the path to save your model.\n        config_path: The path to save or load some configurations to speed up a little bit.\n        train: Whether you want to train the model or just load model from the disk.\n        train_file_path: If train is True, the file path must not be None.\n        vector_path: If you want to use a trained word2vec model, just set this.\n    """"""\n\n    def __init__(self, model_path,\n                 config_path,\n                 train=False,\n                 train_file_path=None,\n                 vector_path=None):\n        self.model_path = model_path\n        self.config_path = config_path\n        if not train:\n            assert config_path is not None, \'The config path cannot be None.\'\n            config = load_config(self.config_path)\n            if not config:\n                (self.word_index, self.max_len, self.embeddings) = config\n                self.model = load_model(self.model_path, self.build_model())\n            if not self.model:\n                print(\'The model cannot be loaded\xef\xbc\x9a\', self.model_path)\n        else:\n            self.vector_path = vector_path\n            self.train_file_path = train_file_path\n            self.x_train, self.y_train, self.x_test, self.y_test, self.word_index, self.max_index = self.load_data()\n            self.max_len = self.x_train.shape[1]\n            config = load_config(self.config_path)\n            if not config:\n                self.embeddings = load_bin_word2vec(self.word_index, self.vector_path, self.max_index)\n                save_config((self.word_index, self.max_len, self.embeddings), self.config_path)\n            else:\n                (_, _, self.embeddings) = config\n            self.model = self.train()\n            save_model(self.model, self.model_path)\n\n    # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c, \xe4\xbb\x85\xe7\x94\xa8\xe6\x9d\xa5\xe4\xbd\x9c\xe4\xb8\xba\xe5\x9f\xba\xe7\xb1\xbb\xe6\xb5\x8b\xe8\xaf\x95\xe4\xbb\xa3\xe7\xa0\x81\xe9\x80\x9a\xe8\xbf\x87\xef\xbc\x8c\xe9\x80\x9f\xe5\xba\xa6\xe5\xbf\xab, \xe4\xbd\x86\xe6\x98\xaf\xe5\x88\x86\xe7\xb1\xbb\xe6\x95\x88\xe6\x9e\x9c\xe7\x89\xb9\xe5\x88\xab\xe5\xb7\xae\n    def build_model(self):\n        inputs = Input(shape=(self.max_len,))\n\n        x = Embedding(len(self.embeddings),\n                      300,\n                      weights=[self.embeddings],\n                      trainable=False)(inputs)\n\n        x = Lambda(lambda t: tf.reduce_mean(t, axis=1))(x)\n        x = Dense(128, activation=\'relu\')(x)\n        x = Dense(64, activation=\'relu\')(x)\n        x = Dense(16, activation=\'relu\')(x)\n        predictions = Dense(1, activation=\'sigmoid\')(x)\n        model = Model(inputs=inputs, outputs=predictions)\n        model.compile(optimizer=\'adam\',\n                      loss=\'binary_crossentropy\',\n                      metrics=[\'accuracy\'])\n        model.summary()\n        return model\n\n    def train(self, batch_size=512, epochs=20):\n        model = self.build_model()\n        # early_stop\xe9\x85\x8d\xe5\x90\x88checkpoint\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xbe\x97\xe5\x88\xb0val_loss\xe6\x9c\x80\xe5\xb0\x8f\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        early_stop = EarlyStopping(patience=3, verbose=1)\n        checkpoint = ModelCheckpoint(os.path.join(self.model_path, \'weights.{epoch:03d}-{val_loss:.3f}.h5\'),\n                                     verbose=1,\n                                     monitor=\'val_loss\',\n                                     save_best_only=True)\n        history = model.fit(self.x_train,\n                            self.y_train,\n                            batch_size=batch_size,\n                            epochs=epochs,\n                            verbose=1,\n                            callbacks=[checkpoint, early_stop],\n                            validation_data=(self.x_test, self.y_test))\n        plot(history)\n        return model\n\n    def predict(self, text):\n        indices = None\n        if isinstance(text, str):\n            indices = [[self.word_index[t] if t in self.word_index.keys() else 0 for t in text.split()]]\n        elif isinstance(text, list):\n            indices = [[self.word_index[t] if t in self.word_index.keys() else 0 for t in tx.split()] for tx in text]\n        if indices:\n            indices = pad_sequences(indices, 500)\n            return self.model.predict(indices)\n        else:\n            return []\n\n    # \xe9\xbb\x98\xe8\xae\xa4\xe9\x80\x89\xe7\x94\xa8keras\xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84\xe5\xa4\x84\xe7\x90\x86\xe5\xa5\xbd\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x9d\xa5\xe5\x81\x9a\xe6\xa8\xa1\xe6\x8b\x9f\xe5\x88\x86\xe7\xb1\xbb\n    def load_data(self):\n        return self.load_data_from_keras()\n\n    @staticmethod\n    def load_data_from_keras(max_len=500):\n        (x_train, y_train), (x_test, y_test) = imdb.load_data()\n\n        word_index = imdb.get_word_index()\n\n        x_train = pad_sequences(x_train, maxlen=max_len)\n        x_test = pad_sequences(x_test, maxlen=x_train.shape[1])\n        y_train = np.asarray(y_train).astype(\'float32\')\n        y_test = np.asarray(y_test).astype(\'float32\')\n\n        max_index = max([max(x) for x in x_train])\n\n        return x_train, y_train, x_test, y_test, word_index, max_index\n\n    # \xe7\x94\xa8\xe8\x87\xaa\xe5\xb7\xb1\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x81\x9a\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x88\xe6\xa0\xbc\xe5\xbc\x8f\xef\xbc\x9a\xe5\x88\x86\xe5\xa5\xbd\xe8\xaf\x8d\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90##\xe6\xa0\x87\xe7\xad\xbe\xef\xbc\x8c\xe5\xa6\x82\xef\xbc\x9a\xe6\x88\x91 \xe5\xbe\x88 \xe5\x96\x9c\xe6\xac\xa2 \xe8\xbf\x99\xe9\x83\xa8 \xe7\x94\xb5\xe5\xbd\xb1#pos\xef\xbc\x89\n    def load_data_from_scratch(self, test_size=0.2, max_len=100):\n        assert self.train_file_path is not None, \'file must not be none \'\n        stopwords = load_en_stopwords()\n        with open(self.train_file_path, \'r\', encoding=\'utf-8\') as file:\n            lines = file.readlines()\n        lines = [line.strip() for line in lines]\n        lines = [line.split(\'##\') for line in lines]\n        x = [line[0] for line in lines]\n        x = [line.split() for line in x]\n        data = [word for xx in x for word in xx]\n        y = [line[0] for line in lines]\n\n        counter = Counter(data)\n        vocab = [k for k, v in counter.items() if v >= 5]\n\n        word_index = {k: v for v, k in enumerate(vocab)}\n\n        max_sentence_length = max([len(words) for words in x])\n        max_len = max_len if max_sentence_length > max_len else max_sentence_length\n\n        x_data = [[word_index[word] for word in words if word in word_index.keys() and word not in stopwords] for words\n                  in x]\n        x_data = pad_sequences(x_data, maxlen=max_len)\n\n        y_data = to_categorical(y)\n\n        x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_size)\n        return x_train, y_train, x_test, y_test, word_index\n\n\n# cnn\nclass TextCnnClassifier(BasicTextClassifier):\n\n    def __init__(self, model_path,\n                 config_path,\n                 train=False,\n                 vector_path=None,\n                 filter_sizes=None,\n                 num_filters=256,\n                 drop=0.5):\n        if filter_sizes is None:\n            filter_sizes = [3, 4, 5, 6]\n        self.filter_sizes = filter_sizes\n        self.num_filters = num_filters\n        self.drop = drop\n        super(TextCnnClassifier, self).__init__(model_path=model_path,\n                                                config_path=config_path,\n                                                train=train,\n                                                vector_path=vector_path)\n\n    def build_model(self, input_shape=(500,)):\n        inputs = Input(shape=input_shape, dtype=\'int32\')\n        embedding = Embedding(self.max_index + 1,\n                              300,\n                              weights=[self.embeddings],\n                              trainable=False)(inputs)\n        filter_results = []\n        for i, filter_size in enumerate(self.filter_sizes):\n            c = Conv1D(self.num_filters,\n                       kernel_size=filter_size,\n                       padding=\'valid\',\n                       activation=\'relu\',\n                       kernel_regularizer=tf.keras.regularizers.l2(0.001),\n                       name=\'conv-\' + str(i + 1))(embedding)\n            max_pool = GlobalMaxPooling1D(name=\'max-pool-\' + str(i + 1))(c)\n            filter_results.append(max_pool)\n        concat = Concatenate()(filter_results)\n        dropout = Dropout(self.drop)(concat)\n        output = Dense(units=1,\n                       activation=\'sigmoid\',\n                       name=\'dense\')(dropout)\n        model = Model(inputs=inputs, outputs=output)\n        model.compile(optimizer=\'adam\',\n                      loss=\'binary_crossentropy\',\n                      metrics=[\'accuracy\'])\n        model.summary()\n        return model\n\n\n# han: Hierarchical Attention Networks\nclass TextHanClassifier(BasicTextClassifier):\n    # \xe5\xaf\xb9\xe9\x95\xbf\xe6\x96\x87\xe6\x9c\xac\xe6\xaf\x94\xe8\xbe\x83\xe5\xa5\xbd, \xe5\x8f\xaf\xe4\xbb\xa5\xe5\x9c\xa8\xe9\x95\xbf\xe6\x96\x87\xe6\x9c\xac\xe4\xb8\xad\xe6\x88\xaa\xe6\x96\xad\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe6\x8a\x8a\xe4\xb8\x80\xe6\xae\xb5\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaasentence\n    def build_model(self):\n        # word part\n        input_word = Input(shape=(int(self.max_len / 5),))\n        x_word = Embedding(len(self.embeddings),\n                           300,\n                           weights=[self.embeddings],\n                           trainable=False)(input_word)\n        x_word = Bidirectional(LSTM(128, return_sequences=True))(x_word)\n        x_word = VanillaRNNAttention(256)(x_word)\n        model_word = Model(input_word, x_word)\n\n        # Sentence part\n        inputs = Input(shape=(self.max_len,))  # (5, self.max_len) \xef\xbc\x9a(\xe7\xaf\x87\xe7\xab\xa0\xe6\x9c\x80\xe5\xa4\x9a\xe5\x8c\x85\xe5\x90\xab\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe6\xaf\x8f\xe5\x8f\xa5\xe5\x8c\x85\xe5\x90\xab\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe8\xaf\x8d\xe6\x95\xb0)\n        reshape = Reshape((5, int(self.max_len / 5)))(inputs)\n        x_sentence = TimeDistributed(model_word)(reshape)\n        x_sentence = Bidirectional(LSTM(128, return_sequences=True))(x_sentence)\n        x_sentence = VanillaRNNAttention(256)(x_sentence)\n\n        output = Dense(1, activation=\'sigmoid\')(x_sentence)\n        model = Model(inputs=inputs, outputs=output)\n        model.compile(\'adam\', \'binary_crossentropy\', metrics=[\'accuracy\'])\n        return model\n\n    def train(self, batch_size=128, epochs=2):\n        # \xe6\xaf\x94\xe8\xbe\x83\xe8\x80\x97\xe8\xb4\xb9\xe8\xb5\x84\xe6\xba\x90\xef\xbc\x8c\xe7\xac\x94\xe8\xae\xb0\xe6\x9c\xacGPU\xe8\xb7\x91\xe4\xb8\x8d\xe5\x8a\xa8\xef\xbc\x8c\xe5\x8f\xaa\xe5\xa5\xbd\xe5\x87\x8f\xe5\xb0\x8fbatch_size\n        return super(TextHanClassifier, self).train(batch_size=batch_size, epochs=epochs)\n\n\n# rnn + cnn\nclass TextRCNNClassifier(BasicTextClassifier):\n\n    def build_model(self):\n        inputs = Input((self.max_len,))\n        embedding = Embedding(len(self.embeddings),\n                              300,\n                              weights=[self.embeddings],\n                              trainable=False)(inputs)\n        x_context = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n        x = Concatenate()([embedding, x_context])\n        cs = []\n        for kernel_size in range(1, 5):\n            c = Conv1D(128, kernel_size, activation=\'relu\')(x)\n            cs.append(c)\n        pools = [GlobalAveragePooling1D()(c) for c in cs] + [GlobalMaxPooling1D()(c) for c in cs]\n        x = Concatenate()(pools)\n        output = Dense(1, activation=\'sigmoid\')(x)\n        model = Model(inputs=inputs, outputs=output)\n        model.compile(\'adam\', \'binary_crossentropy\', metrics=[\'accuracy\'])\n        return model\n\n    def train(self, batch_size=128, epochs=2):\n        super(TextRCNNClassifier, self).train(batch_size=batch_size, epochs=epochs)\n\n\nclass TextRnnClassifier(BasicTextClassifier):\n    """"""\n    rnn\n    """"""\n\n    def __init__(self, model_path, config_path, train, vector_path):\n        super(TextRnnClassifier, self).__init__(model_path=model_path,\n                                                config_path=config_path,\n                                                train=train,\n                                                vector_path=vector_path)\n\n    def build_model(self):\n        inputs = Input(shape=(self.max_len,))\n        x = Embedding(len(self.embeddings),\n                      300,\n                      weights=[self.embeddings],\n                      trainable=False)(inputs)\n        x = Bidirectional(LSTM(150))(x)\n        x = BatchNormalization()(x)\n        x = Dense(128, activation=""relu"")(x)\n        x = Dropout(0.25)(x)\n        y = Dense(1, activation=""sigmoid"")(x)\n        model = Model(inputs=inputs, outputs=y)\n        model.compile(loss=\'binary_crossentropy\',\n                      optimizer=\'adam\',\n                      metrics=[\'accuracy\'])\n        return model\n\n\nclass TextRNNAttentionClassifier(BasicTextClassifier):\n    """"""\n    rnn + attention\n    """"""\n\n    def build_model(self):\n        inputs = Input(shape=(self.max_len,))\n        output = Embedding(len(self.embeddings),\n                           300,\n                           weights=[self.embeddings],\n                           trainable=False)(inputs)\n        output = Bidirectional(LSTM(150,\n                                    return_sequences=True,\n                                    dropout=0.25,\n                                    recurrent_dropout=0.25))(output)\n        output = VanillaRNNAttention(300)(output)\n        output = Dense(128, activation=""relu"")(output)\n        output = Dropout(0.25)(output)\n        output = Dense(1, activation=""sigmoid"")(output)\n        model = Model(inputs=inputs, outputs=output)\n        model.compile(loss=\'binary_crossentropy\',\n                      optimizer=\'adam\',\n                      metrics=[\'accuracy\'])\n        model.summary()\n        return model\n\n    def train(self, batch_size=128, epochs=5):\n        super(TextRNNAttentionClassifier, self).train(batch_size=batch_size, epochs=epochs)\n'"
smartnlp/classfication/fasttext_classifier.py,0,"b'# coding:utf-8\n\nimport fasttext\nfrom smartnlp.utils.clean_text import clean_zh_text, clean_en_text\nimport os\nfrom smartnlp.utils.basic_log import Log\nimport logging\n\nlog = Log(logging.INFO)\n\n\nclass FastTextClassifier:\n    """"""\n    \xe9\x9c\x80\xe8\xa6\x81\xe5\xae\x89\xe8\xa3\x85\xe5\x8c\x85\xef\xbc\x9apip install fasttext\n    \xe5\x88\xa9\xe7\x94\xa8fasttext\xe6\x9d\xa5\xe5\xaf\xb9\xe6\x96\x87\xe6\x9c\xac\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\n    """"""\n\n    def __init__(self, model_path,\n                 train=False,\n                 file_path=None):\n        """"""\n        \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        :param file_path: \xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe8\xb7\xaf\xe5\xbe\x84\n        :param model_path: \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\n        """"""\n        self.model_path = model_path\n        if not train:\n            self.model = self.load(self.model_path)\n            assert self.model is not None, \'\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x97\xa0\xe6\xb3\x95\xe8\x8e\xb7\xe5\x8f\x96\'\n        else:\n            assert file_path is not None, \'\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6, file_path\xe4\xb8\x8d\xe8\x83\xbd\xe4\xb8\xbaNone\'\n            self.train_path = os.path.join(file_path, \'train.txt\')\n            self.test_path = os.path.join(file_path, \'test.txt\')\n            self.model = self.train()\n\n    def train(self):\n        """"""\n        \xe8\xae\xad\xe7\xbb\x83:\xe5\x8f\x82\xe6\x95\xb0\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x92\x88\xe5\xaf\xb9\xe6\x80\xa7\xe4\xbf\xae\xe6\x94\xb9,\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xb0\x83\xe4\xbc\x98\n        """"""\n        model = fasttext.supervised(self.train_path,\n                                    self.model_path,\n                                    label_prefix=""__label__"",\n                                    epoch=100,\n                                    dim=256,\n                                    silent=False,\n                                    lr=0.01)\n\n        test_result = model.test(self.test_path)\n        print(\'\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87: \', test_result.precision)\n        return model\n\n    def predict(self, text):\n        """"""\n        \xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\x80\xe6\x9d\xa1\xe6\x95\xb0\xe6\x8d\xae,\xe7\x94\xb1\xe4\xba\x8efasttext\xe8\x8e\xb7\xe5\x8f\x96\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xe5\x88\x97\xe8\xa1\xa8,\xe5\xa6\x82\xe6\x9e\x9c\xe5\x8f\xaa\xe6\x98\xaf\xe7\xae\x80\xe5\x8d\x95\xe8\xbe\x93\xe5\x85\xa5\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2,\xe4\xbc\x9a\xe5\xb0\x86\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe6\x8c\x89\xe7\xa9\xba\xe6\xa0\xbc\xe6\x8b\x86\xe5\x88\x86\xe7\xbb\x84\xe6\x88\x90\xe5\x88\x97\xe8\xa1\xa8\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8e\xa8\xe7\x90\x86\n        :param text: \xe5\xbe\x85\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n        :return: \xe5\x88\x86\xe7\xb1\xbb\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n        """"""\n        if isinstance(text, list):\n            output = self.model.predict(text, )\n        else:\n            output = self.model.predict([text], )\n        print(\'predict:\', output)\n        return output\n\n    def load(self, model_path):\n        """"""\n        \xe5\x8a\xa0\xe8\xbd\xbd\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        :param model_path: \xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb7\xaf\xe5\xbe\x84\n        :return:\n        """"""\n        if os.path.exists(self.model_path + \'.bin\'):\n            return fasttext.load_model(model_path + \'.bin\')\n        else:\n            return None\n\n\ndef clean(file_path):\n    """"""\n    \xe6\xb8\x85\xe7\x90\x86\xe6\x96\x87\xe6\x9c\xac, \xe7\x84\xb6\xe5\x90\x8e\xe5\x88\xa9\xe7\x94\xa8\xe6\xb8\x85\xe7\x90\x86\xe5\x90\x8e\xe7\x9a\x84\xe6\x96\x87\xe6\x9c\xac\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\n    """"""\n    with open(file_path, \'r\', encoding=\'utf-8\') as f:\n        lines = f.readlines()\n        lines_clean = []\n        for line in lines:\n            line_list = line.split(\'__label__\')\n            lines_clean.append(clean_en_text(line_list[0]) + \' __label__\' + line_list[1])\n\n    with open(file_path, \'w\', encoding=\'utf-8\') as f:\n        f.writelines(lines_clean)\n'"
smartnlp/classfication/svm_classifier.py,0,"b'# coding:utf-8\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nimport pickle\nimport pandas as pd\nfrom smartnlp.utils.clean_text import clean_en_text\nfrom smartnlp.utils.basic_log import Log\nimport logging\n\nlog = Log(logging.INFO)\n\n\nclass SVMClassifier(object):\n    """"""\n    \xe8\xbf\x99\xe4\xb8\xaa\xe7\xb1\xbb,\xe6\x98\xaf\xe7\x94\xa8svm\xe5\xaf\xb9\xe6\x96\x87\xe6\x9c\xac\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb.\n    1. \xe7\x94\xa8TF-IDF\xe8\xae\xa1\xe7\xae\x97\xe6\x9d\x83\xe9\x87\x8d\xe5\x80\xbc\n    2. \xe7\x94\xa8\xe5\x8d\xa1\xe6\x96\xb9\xe6\xa3\x80\xe9\xaa\x8c\xe8\x8e\xb7\xe5\x8f\x96\xe7\x89\xb9\xe5\xbe\x81\n    3. \xe7\x94\xa8SVM\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xe8\xae\xad\xe7\xbb\x83\n    """"""\n\n    def __init__(self, model_file,\n                 train_path=None,\n                 train=False):\n        """"""\n        \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x82\xe6\x95\xb0\n        :param train_path: \xe8\xae\xad\xe7\xbb\x83\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x9a\xe6\x95\xb0\xe6\x8d\xae\xe4\xbb\xa5\xe2\x80\x9cx##y\xe2\x80\x9d\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\xe5\x88\x86\xe9\x9a\x94\xef\xbc\x8cx\xe4\xb8\xba\xe5\x88\x86\xe5\xa5\xbd\xe8\xaf\x8d\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8cy\xe4\xb8\xba\xe6\x95\xb0\xe6\x8d\xae\xe6\xa0\x87\xe7\xad\xbe\n        :param model_file: \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\n        """"""\n        self.model_file = model_file\n        # \xe5\x85\x88\xe8\xaf\xbb\xe5\x8f\x96\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84model,\xe5\xa6\x82\xe6\x9e\x9c\xe8\xaf\xbb\xe5\x8f\x96\xe4\xb8\x8d\xe5\x88\xb0,\xe5\x88\x99\xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xad\xe7\xbb\x83\n        if not train:\n            self.tf_idf_model, self.chi_model, self.clf_model = self.read_model()\n            assert self.tf_idf_model is not None, \'\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8,\xe8\xaf\xb7\xe7\xa1\xae\xe8\xae\xa4\xe5\x90\x8e\xe5\x86\x8d\xe8\xaf\x95\'\n        else:\n            assert train_path is not None, \'\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\xe4\xb8\x8b, \xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8d\xe8\x83\xbd\xe4\xb8\xbaNone\'\n            self.train_path = train_path\n            self.tf_idf_model, self.chi_model, self.clf_model = self.train_model()\n            self.save_model()\n\n    def predict(self, texts):\n        """"""\n        \xe6\xa0\xb9\xe6\x8d\xae\xe6\xa8\xa1\xe5\x9e\x8b\xe9\xa2\x84\xe6\xb5\x8b\xe6\x9f\x90\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\n        :param texts: \xe8\xa6\x81\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe6\x96\x87\xe6\x9c\xac\n        :return: \xe8\xbf\x94\xe5\x9b\x9e\xe5\x88\x86\xe7\xb1\xbb\n        """"""\n        texts = [clean_en_text(t) for t in texts]\n        tf_vector = self.tf_idf_model.transform(texts)\n        chi_vector = self.chi_model.transform(tf_vector)\n        out = self.clf_model.predict(chi_vector)\n        print(\'\xe6\x8e\xa8\xe7\x90\x86\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x9a\', out)\n        return out\n\n    def read_model(self):\n        try:\n            with open(self.model_file, \'rb\') as f:\n                (tf_idf_model, chi_model, clf_model) = pickle.load(f)\n        except FileNotFoundError:\n            log.error(\'\xe6\xb2\xa1\xe6\x9c\x89\xe6\x89\xbe\xe5\x88\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x96\x87\xe4\xbb\xb6\')\n            tf_idf_model = None\n            chi_model = None\n            clf_model = None\n        return tf_idf_model, chi_model, clf_model\n\n    def train_model(self, test_size=0.2):\n        """"""\n        \xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b,\xe7\xae\x80\xe5\x8d\x95\xe5\x9c\xb0\xe5\xb0\x86\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84TF-IDF\xe6\x95\xb0\xe6\x8d\xae,chi\xe6\x8f\x90\xe5\x8f\x96\xe5\x90\x8e\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81,\xe4\xbb\xa5\xe5\x8f\x8asvm\xe7\xae\x97\xe6\xb3\x95\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x86\x99\xe5\x85\xa5\xe5\x88\xb0\xe4\xba\x86\xe7\xa3\x81\xe7\x9b\x98\xe4\xb8\xad\n        :return: \xe8\xbf\x94\xe5\x9b\x9e\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        """"""\n        data_set = pd.read_csv(self.train_path,\n                               sep=\'##\',\n                               encoding=\'utf-8\',\n                               header=None,\n                               engine=\'python\')\n        data_set = data_set.dropna()\n        chi_features, tf_idf_model, chi_model = self.__select_features(data_set)\n        x_train, x_test, y_train, y_test = train_test_split(chi_features,\n                                                            data_set[1],\n                                                            test_size=test_size,\n                                                            random_state=42)\n        # \xe8\xbf\x99\xe9\x87\x8c\xe9\x87\x87\xe7\x94\xa8\xe7\x9a\x84\xe6\x98\xaf\xe7\xba\xbf\xe6\x80\xa7\xe5\x88\x86\xe7\xb1\xbb\xe6\xa8\xa1\xe5\x9e\x8b,\xe5\xa6\x82\xe6\x9e\x9c\xe9\x87\x87\xe7\x94\xa8rbf\xe5\xbe\x84\xe5\x90\x91\xe5\x9f\xba\xe6\xa8\xa1\xe5\x9e\x8b,\xe9\x80\x9f\xe5\xba\xa6\xe4\xbc\x9a\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x85\xa2.\n        clf_model = svm.SVC(kernel=\'linear\', verbose=True)\n        print(clf_model)\n        clf_model.fit(x_train, y_train)\n        score = clf_model.score(x_test, y_test)\n        print(\'\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87:\', score)\n        return tf_idf_model, chi_model, clf_model\n\n    @staticmethod\n    def __select_features(data_set):\n        dataset = [clean_en_text(data) for data in data_set[0]]\n        tf_idf_model = TfidfVectorizer(ngram_range=(1, 1),\n                                       binary=True, \n                                       sublinear_tf=True)\n        tf_vectors = tf_idf_model.fit_transform(dataset)\n\n        # \xe9\x80\x89\xe5\x87\xba\xe5\x89\x8d1/5\xe7\x9a\x84\xe8\xaf\x8d\xe7\x94\xa8\xe6\x9d\xa5\xe5\x81\x9a\xe7\x89\xb9\xe5\xbe\x81\n        k = int(tf_vectors.shape[1] / 6)\n        chi_model = SelectKBest(chi2, k=k)\n        chi_features = chi_model.fit_transform(tf_vectors, data_set[1])\n        print(\'tf-idf:\\t\\t\' + str(tf_vectors.shape[1]))\n        print(\'chi:\\t\\t\' + str(chi_features.shape[1]))\n\n        return chi_features, tf_idf_model, chi_model\n\n    def save_model(self):\n        with open(self.model_file, ""wb"") as file:\n            pickle.dump((self.tf_idf_model, self.chi_model, self.clf_model), file)\n        if file:\n            file.close()\n'"
smartnlp/custom/__init__.py,0,b'# coding=utf-8\n# created by msgi on 2020/4/1 7:21 \xe4\xb8\x8b\xe5\x8d\x88\n\n'
smartnlp/embedding/__init__.py,0,b''
smartnlp/embedding/embedding.py,1,"b'# coding=utf-8\n# created by msgi on 2020/4/26 6:54 \xe4\xb8\x8b\xe5\x8d\x88\n\nimport io\nimport os\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom smartnlp.utils.plot_model_history import plot\nfrom smartnlp.utils.loader import *\n\n\nclass VanillaEmbeddingModel:\n    def __init__(self, model_path, embedding_dim, file_path=None, training=False):\n        self.embedding_dim = embedding_dim\n        self.file_path = file_path\n        self.model_path = model_path\n        self.train_batches, self.test_batches, self.encoder = self.preprocess()\n\n        if training:\n            self.model = self.train_model()\n            save_model(self.model, self.model_path)\n            self.save_embeddings_to_file()\n        else:\n            self.model = load_model(self.model_path, self._build_model())\n\n    def _build_model(self):\n        model = keras.Sequential([\n            layers.Embedding(self.encoder.vocab_size, self.embedding_dim),\n            layers.GlobalAveragePooling1D(),\n            layers.Dense(16, activation=\'relu\'),\n            layers.Dense(1)\n        ])\n        model.compile(optimizer=\'adam\',\n                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                      metrics=[\'accuracy\'])\n        model.summary()\n        return model\n\n    def train_model(self):\n        model = self._build_model()\n        history = model.fit(\n            self.train_batches,\n            epochs=10,\n            validation_data=self.test_batches, validation_steps=20)\n        plot(history)\n        return model\n\n    def save_embeddings_to_file(self):\n        e = self.model.layers[0]\n        weights = e.get_weights()[0]\n        out_v = io.open(\'vecs.tsv\', \'w\', encoding=\'utf-8\')\n        out_m = io.open(\'meta.tsv\', \'w\', encoding=\'utf-8\')\n\n        for num, word in enumerate(self.encoder.subwords):\n            vec = weights[num + 1]  # skip 0, it\'s padding.\n            out_m.write(word + ""\\n"")\n            out_v.write(\'\\t\'.join([str(x) for x in vec]) + ""\\n"")\n        out_v.close()\n        out_m.close()\n\n    @staticmethod\n    def preprocess():\n        (train_data, test_data), info = tfds.load(\'imdb_reviews/subwords8k\',\n                                                  split=(tfds.Split.TRAIN, tfds.Split.TEST),\n                                                  with_info=True, as_supervised=True)\n        train_batches = train_data.shuffle(1000).padded_batch(10, padded_shapes=([None], []))\n        test_batches = test_data.shuffle(1000).padded_batch(10, padded_shapes=([None], []))\n\n        encoder = info.features[\'text\'].encoder\n        return train_batches, test_batches, encoder\n\n\nif __name__ == \'__main__\':\n    model = VanillaEmbeddingModel(\'model\', 16, training=True)\n    model.save_embeddings_to_file()\n'"
smartnlp/embedding/fasttext_model.py,0,"b'# _*_ encoding: utf-8 _*_\nimport os\nimport fasttext\n\n\nclass FastTextModel:\n\n    def __init__(self, train_file,\n                 model_path,\n                 model_type=\'skipgram\'):\n        """"""\n        \xe7\x94\xa8facebook\xe7\x9a\x84fasttext\xe8\xae\xad\xe7\xbb\x83\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x88\xe9\xbb\x98\xe8\xae\xa4skipgram\xe6\x96\xb9\xe5\xbc\x8f, \xe5\xa6\x82\xe6\x9e\x9c\xe9\x87\x87\xe7\x94\xa8cbow\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8cmodel_type\xe8\xae\xbe\xe4\xb8\xba\'cbow\'\xef\xbc\x89\n        :param train_file: \xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\x96\x87\xe6\x9c\xac\xef\xbc\x8c\xe6\x96\x87\xe4\xbb\xb6\xe5\x86\x85\xe5\xae\xb9\xe6\x98\xaf\xe5\x88\x86\xe5\xa5\xbd\xe8\xaf\x8d\xe7\x9a\x84\n        :param model_path: \xe8\xa6\x81\xe5\xad\x98\xe5\x82\xa8\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb7\xaf\xe5\xbe\x84\n        """"""\n        self.train_file = train_file\n        self.model_path = model_path\n        self.model_type = model_type\n        self.model = self.load()\n        if not self.model:\n            self.model = self.train()\n\n    # \xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\n    def train(self):\n        model = fasttext.train_unsupervised(input=self.train_file, model=self.model_type)\n        model.save_model(self.model_path)\n        print(model.words)\n        return model\n\n    # \xe8\xbf\x94\xe5\x9b\x9e\xe8\xaf\x8d\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\n    def vector(self, word):\n        return self.model[word]\n\n    def get_nearest_neighbors(self, word, k):\n        return self.model.get_nearest_neighbors(word, k)\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n    def load(self):\n        if os.path.exists(self.model_path):\n            return fasttext.load_model(self.model_path)\n        else:\n            return None\n'"
smartnlp/embedding/word2vec.py,0,"b'# coding: utf-8\nfrom gensim.models import word2vec, KeyedVectors\nfrom smartnlp.utils.pre_process import process_data\n\n\nclass GensimWord2VecModel:\n\n    def __init__(self, train_file,\n                 model_path,\n                 embed_size=100,\n                 vocab_path=None):\n        """"""\n        \xe7\x94\xa8gensim word2vec \xe8\xae\xad\xe7\xbb\x83\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\n        :param train_file: \xe5\x88\x86\xe5\xa5\xbd\xe8\xaf\x8d\xe7\x9a\x84\xe6\x96\x87\xe6\x9c\xac\n        :param model_path: \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe8\xb7\xaf\xe5\x8a\xb2\n        """"""\n        self.train_file = train_file\n        self.model_path = model_path\n        self.vocab_path = vocab_path\n        self.embed_size = embed_size\n        self.model = self.load_text()\n        if not self.model:\n            self.model = self.train()\n            # self.save()\n            self.save_text()\n\n    def train(self):\n        sentences = process_data(self.train_file)\n        model = word2vec.Word2Vec(sentences, min_count=2, window=3, size=self.embed_size, workers=4)\n        return model\n\n    def vector(self, word):\n        return self.model.wv.get_vector(word)\n\n    def similar(self, word):\n        return self.model.wv.similar_by_word(word, topn=10)\n\n    def save(self):\n        self.model.save(self.model_path)\n\n    def save_text(self):\n        self.model.wv.save_word2vec_format(self.model_path, self.vocab_path, False)\n\n    def load(self):\n        # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x96\x87\xe4\xbb\xb6\n        try:\n            model = word2vec.Word2Vec.load(self.model_path)\n        except FileNotFoundError:\n            model = None\n        return model\n\n    def load_text(self):\n        try:\n            model = KeyedVectors.load_word2vec_format(self.model_path, self.vocab_path, binary=False)\n        except FileNotFoundError:\n            model = None\n        return model\n'"
smartnlp/ner/__init__.py,0,b''
smartnlp/ner/bilstm_crf.py,22,"b'import logging\nimport os\nfrom collections import Counter\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as ta\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom smartnlp.utils.basic_log import Log\nfrom smartnlp.utils.loader import load_model, load_config, save_model, save_config\n\nlog = Log(logging.INFO)\n\n\nclass BiLSTMCRFModel(tf.keras.Model):\n    def __init__(self, hidden_units, vocab_size, label_size, embedding_size):\n        super(BiLSTMCRFModel, self).__init__()\n        self.num_hidden = hidden_units\n        self.vocab_size = vocab_size\n        self.label_size = label_size\n        self.transition_params = None\n\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n        self.biLSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_units, return_sequences=True))\n        self.dense = tf.keras.layers.Dense(label_size)\n\n        self.transition_params = tf.Variable(tf.random.uniform(shape=(label_size, label_size)),\n                                             trainable=False)\n        self.dropout = tf.keras.layers.Dropout(0.5)\n\n    def call(self, text, labels=None, training=None):\n        text_lens = tf.math.reduce_sum(tf.cast(tf.math.not_equal(text, 0), dtype=tf.int32), axis=-1)\n        inputs = self.embedding(text)\n        inputs = self.dropout(inputs, training)\n        logits = self.dense(self.biLSTM(inputs))\n\n        if labels is not None:\n            label_sequences = tf.convert_to_tensor(labels, dtype=tf.int32)\n            log_likelihood, self.transition_params = ta.text.crf_log_likelihood(logits, label_sequences, text_lens)\n            self.transition_params = tf.Variable(self.transition_params, trainable=False)\n            return logits, text_lens, log_likelihood\n        else:\n            return logits, text_lens\n\n\nclass BiLSTMCRFNamedEntityRecognition:\n    def __init__(self,\n                 model_path,\n                 config_path,\n                 embed_dim=200,\n                 rnn_units=200,\n                 epochs=1,\n                 batch_size=64,\n                 train=False,\n                 file_path=None):\n        self.model_path = model_path\n        self.config_path = config_path\n        self.embed_dim = embed_dim\n        self.rnn_units = rnn_units\n        self.file_path = file_path\n        self.batch_size = batch_size\n        # \xe8\xaf\x8d\xe6\x80\xa7tag\n        self.tags = [\'O\', \'B-PER\', \'I-PER\', \'B-LOC\', \'I-LOC\', ""B-ORG"", ""I-ORG""]\n\n        # \xe9\x9d\x9e\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\n        if not train:\n            self.word2idx = load_config(self.config_path)\n            self.model = load_model(self.model_path, self._build_model())\n            assert self.model is not None, \'\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x97\xa0\xe6\xb3\x95\xe8\x8e\xb7\xe5\x8f\x96\'\n        else:\n            (self.train_x, self.train_y), (self.test_x, self.test_y), self.word2idx = self._load_data()\n            self.epochs = epochs\n            self.model = self.train()\n        save_model(self.model, self.model_path)\n        save_config(self.word2idx, self.config_path)\n\n    # \xe8\xae\xad\xe7\xbb\x83\n    def train(self):\n        model = self._build_model()\n\n        optimizer = tf.keras.optimizers.Adam(0.01)\n\n        ckpt = tf.train.Checkpoint(optimizer=optimizer, model=model)\n        ckpt.restore(tf.train.latest_checkpoint(self.model_path))\n        ckpt_manager = tf.train.CheckpointManager(ckpt,\n                                                  self.model_path,\n                                                  checkpoint_name=\'model.ckpt\',\n                                                  max_to_keep=3)\n        best_acc = 0\n        step = 0\n        train_dataset = tf.data.Dataset.from_tensor_slices((self.train_x, self.train_y))\n        train_dataset = train_dataset.shuffle(len(self.train_x)).batch(self.batch_size, drop_remainder=True)\n\n        for epoch in range(self.epochs):\n            for _, (text_batch, labels_batch) in enumerate(train_dataset):\n                step = step + 1\n                loss, logits, text_lens = self.train_one_step(model, optimizer, text_batch, labels_batch)\n                if step % 20 == 0:\n                    accuracy = self.get_acc_one_step(model, logits, text_lens, labels_batch)\n                    log.info(\'epoch %d, step %d, loss %.4f , accuracy %.4f\' % (epoch, step, loss, accuracy))\n                    if accuracy > best_acc:\n                        best_acc = accuracy\n                        ckpt_manager.save()\n                        log.info(""model saved"")\n\n        return model\n\n    @staticmethod\n    def train_one_step(model, optimizer, text_batch, labels_batch):\n        with tf.GradientTape() as tape:\n            logits, text_lens, log_likelihood = model(text_batch, labels_batch, training=True)\n            loss = - tf.reduce_mean(log_likelihood)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        return loss, logits, text_lens\n\n    @staticmethod\n    def get_acc_one_step(model, logits, text_lens, labels_batch):\n        paths = []\n        accuracy = 0\n        for logit, text_len, labels in zip(logits, text_lens, labels_batch):\n            viterbi_path, _ = ta.text.viterbi_decode(logit[:text_len], model.transition_params)\n            paths.append(viterbi_path)\n            correct_prediction = tf.equal(\n                tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences([viterbi_path], padding=\'post\'),\n                                     dtype=tf.int32),\n                tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences([labels[:text_len]], padding=\'post\'),\n                                     dtype=tf.int32)\n            )\n            accuracy = accuracy + tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n        accuracy = accuracy / len(paths)\n        return accuracy\n\n    # \xe8\xaf\x86\xe5\x88\xab\xe5\x8f\xa5\xe5\xad\x90\xe4\xb8\xad\xe7\x9a\x84\xe5\xae\x9e\xe4\xbd\x93\n    def predict(self, predict_text):\n        # predict_text = \'\'\n        sent, length = self._preprocess_data(predict_text)\n        raw = self.model.predict(sent)[0][-length:]\n        result = np.argmax(raw, axis=1)\n        result_tags = [self.tags[i] for i in result]\n\n        per, loc, org = \'\', \'\', \'\'\n        for s, t in zip(predict_text, result_tags):\n            if t in (\'B-PER\', \'I-PER\'):\n                per += \' \' + s if (t == \'B-PER\') else s\n            if t in (\'B-ORG\', \'I-ORG\'):\n                org += \' \' + s if (t == \'B-ORG\') else s\n            if t in (\'B-LOC\', \'I-LOC\'):\n                loc += \' \' + s if (t == \'B-LOC\') else s\n        results = [\'person:\' + per, \'location:\' + loc, \'organization:\' + org]\n        print(results)\n        return results\n\n    # \xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\x84\xe7\x90\x86\xe8\xbd\xac\xe6\x8d\xa2\n    def _preprocess_data(self, data, max_len=100):\n        x = [self.word2idx.get(w[0].lower(), 1) for w in data]\n        length = len(x)\n        x = pad_sequences([x], max_len)\n        return x, length\n\n    # \xe6\x9e\x84\xe9\x80\xa0\xe6\xa8\xa1\xe5\x9e\x8b\n    def _build_model(self):\n        model = BiLSTMCRFModel(self.rnn_units, len(self.word2idx), len(self.tags), self.embed_dim)\n        return model\n\n    # \xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n    def _load_data(self):\n        train = self._parse_data(os.path.join(self.file_path, \'train.data\'))\n        test = self._parse_data(os.path.join(self.file_path, \'test.data\'))\n\n        # \xe7\xbb\x9f\xe8\xae\xa1\xe6\xaf\x8f\xe4\xb8\xaa\xe5\xad\x97\xe5\x87\xba\xe7\x8e\xb0\xe7\x9a\x84\xe9\xa2\x91\xe6\xac\xa1\n        word_counts = Counter(row[0].lower() for sample in train for row in sample)\n        vocab = [w for w, f in iter(word_counts.items()) if f >= 2]\n        word2idx = dict((w, i) for i, w in enumerate(vocab))\n\n        train = self._process_data(train, word2idx, self.tags)\n        test = self._process_data(test, word2idx, self.tags)\n        return train, test, word2idx\n\n    @staticmethod\n    def _process_data(data, word2idx, chunk_tags, max_len=None):\n        if max_len is None:\n            max_len = max(len(s) for s in data)\n        x = [[word2idx.get(w[0].lower(), 1) for w in s] for s in data]\n        y_chunk = [[chunk_tags.index(w[1]) for w in s] for s in data]\n\n        x = pad_sequences(x, max_len, padding=\'post\')\n        y_chunk = pad_sequences(y_chunk, max_len, padding=\'post\')\n\n        return x, y_chunk\n\n    @staticmethod\n    def _parse_data(file_path):\n        with open(file_path, \'rb\') as f:\n            string = f.read().decode(\'utf-8\')\n            data = [[row.split() for row in sample.split(\'\\n\')] for sample in\n                    string.strip().split(\'\\n\' + \'\\n\')]\n        return data\n\n\nif __name__ == \'__main__\':\n    biLSTM = BiLSTMCRFNamedEntityRecognition(\'model/ner/\',\n                                             \'model/ner/config.pkl\',\n                                             train=True,\n                                             file_path=\'/Users/msgi/workspace/pythons/nlp-journey/examples/data/ner\')\n'"
smartnlp/nmt/__init__.py,0,b'# coding=utf-8\n# created by msgi on 2020/4/28 4:37 \xe4\xb8\x8b\xe5\x8d\x88\n\n'
smartnlp/nmt/data_process.py,8,"b""# coding=utf-8\n# created by msgi on 2020/4/29 4:11 \xe4\xb8\x8b\xe5\x8d\x88\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n\nclass DataProcessor:\n\n    def __init__(self, csv_path=None,\n                 test_size=0.1,\n                 max_length=40,\n                 feature_col='',\n                 target_col='',\n                 buffer_size=20000,\n                 batch_size=64):\n        self.test_size = test_size\n        self.csv_path = csv_path\n        self.feature_col = feature_col\n        self.target_col = target_col\n        self.max_length = max_length\n        self.buffer_size = buffer_size\n        self.batch_size = batch_size\n\n    def load_data(self):\n        return pd.read_csv(self.csv_path).dropna(inplace=True)\n\n    def train_test_split(self, df):\n        test_rows = int(len(df) * self.test_size)\n        test = df.sample(test_rows)\n        train = df[~df.isin(test)]\n\n        train.dropna(inplace=True)\n        test.dropna(inplace=True)\n\n        return train, test\n\n    def tokenizer(self, train, vocab_size=2 ** 13):\n        if self.feature_col is None:\n            feature_col = train.columns[0]\n        else:\n            feature_col = self.feature_col\n\n        if self.target_col is None:\n            target_col = train.columns[1]\n        else:\n            target_col = self.target_col\n        global tokenizer_feat\n        global tokenizer_tar\n        tokenizer_tar = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n            (line for line in train[target_col].values), target_vocab_size=vocab_size\n        )\n\n        tokenizer_feat = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n            (line for line in train[feature_col].values), target_vocab_size=vocab_size\n        )\n\n        return tokenizer_feat, tokenizer_tar\n\n    @staticmethod\n    def encode(lang1, lang2):\n        lang1 = [tokenizer_feat.vocab_size] + tokenizer_feat.encode(\n            lang1.numpy()) + [tokenizer_feat.vocab_size + 1]\n\n        lang2 = [tokenizer_tar.vocab_size] + tokenizer_tar.encode(\n            lang2.numpy()) + [tokenizer_tar.vocab_size + 1]\n\n        return lang1, lang2\n\n    def filter_max_length(self, x, y):\n        # \xe4\xb8\xba\xe4\xba\x86\xe4\xbd\xbf\xe6\x9c\xac\xe7\xa4\xba\xe4\xbe\x8b\xe8\xbe\x83\xe5\xb0\x8f\xe4\xb8\x94\xe7\x9b\xb8\xe5\xaf\xb9\xe8\xbe\x83\xe5\xbf\xab\xef\xbc\x8c\xe5\x88\xa0\xe9\x99\xa4\xe9\x95\xbf\xe5\xba\xa6\xe5\xa4\xa7\xe4\xba\x8e max_length \xe4\xb8\xaa\xe6\xa0\x87\xe8\xae\xb0\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe3\x80\x82\n        return tf.logical_and(tf.size(x) <= self.max_length,\n                              tf.size(y) <= self.max_length)\n\n    def tf_encode(self, feature, target):\n        return tf.py_function(self.encode, [feature, target], [tf.int64, tf.int64])\n\n    def to_tensor_dataset(self, data):\n        return tf.data.Dataset.from_tensor_slices(\n            (\n                tf.cast(data[self.feature_col].values, tf.string),\n                tf.cast(data[self.target_col].values, tf.string)\n            )\n        )\n\n    def init_preprocess(self, train_examples, val_examples):\n        global tokenizer_tar\n        global tokenizer_feat\n        tokenizer_tar = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n            (en.numpy() for pt, en in train_examples), target_vocab_size=2 ** 13)\n\n        tokenizer_feat = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n            (pt.numpy() for pt, en in train_examples), target_vocab_size=2 ** 13)\n\n        train_dataset = train_examples.map(self.tf_encode)\n        train_dataset = train_dataset.filter(self.filter_max_length)\n        # \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\xbc\x93\xe5\xad\x98\xe5\x88\xb0\xe5\x86\x85\xe5\xad\x98\xe4\xb8\xad\xe4\xbb\xa5\xe5\x8a\xa0\xe5\xbf\xab\xe8\xaf\xbb\xe5\x8f\x96\xe9\x80\x9f\xe5\xba\xa6\xe3\x80\x82\n        train_dataset = train_dataset.cache()\n        train_dataset = train_dataset.shuffle(self.buffer_size).padded_batch(self.batch_size,\n                                                                             padded_shapes=([-1], [-1]))\n        train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n        val_dataset = val_examples.map(self.tf_encode)\n        val_dataset = val_dataset.filter(self.filter_max_length).padded_batch(self.batch_size,\n                                                                              padded_shapes=([-1], [-1]))\n        return train_dataset, val_dataset, tokenizer_feat, tokenizer_tar\n\n    def preprocess(self, train: pd.DataFrame, test: pd.DataFrame):\n        train_data = self.to_tensor_dataset(train)\n        test_data = self.to_tensor_dataset(test)\n\n        train_dataset = train_data.map(self.tf_encode)\n        train_dataset = train_dataset.filter(self.filter_max_length)\n        train_dataset = train_dataset.cache()\n        train_dataset = train_dataset.shuffle(self.buffer_size).padded_batch(self.batch_size,\n                                                                             padded_shapes=([-1], [-1]))\n        train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n        test_dataset = test_data.map(self.tf_encode)\n        test_dataset = test_dataset.filter(self.filter_max_length).padded_batch(self.batch_size,\n                                                                                padded_shapes=([-1], [-1]))\n\n        return train_dataset, test_dataset\n"""
smartnlp/nmt/evaluator.py,8,"b'# coding=utf-8\n# created by msgi on 2020/4/29 4:36 \xe4\xb8\x8b\xe5\x8d\x88\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n\nclass Evaluator(object):\n    def __init__(self, tokenizer_feat, tokenizer_tar, max_length, mskr, transformer):\n        self.tokenizer_feat = tokenizer_feat\n        self.tokenizer_tar = tokenizer_tar\n        self.max_length = max_length\n        self.mskr = mskr\n        self.transformer = transformer\n\n    def evaluate(self, inp_sentence):\n        """"""\n        This allows the ability to score the models effectiveness at predicting each output sequence\n        :return: The model output as well as the attention weights from the calculation\n        """"""\n\n        start_token = [self.tokenizer_feat.vocab_size]\n        end_token = [self.tokenizer_feat.vocab_size + 1]\n\n        # inp sentence is portuguese, hence adding the start and end token\n        inp_sentence = start_token + self.tokenizer_feat.encode(inp_sentence) + end_token\n        encoder_input = tf.expand_dims(inp_sentence, 0)\n\n        # as the target is english, the first word to the transformer should be the\n        # english start token.\n        decoder_input = [self.tokenizer_tar.vocab_size]\n        output = tf.expand_dims(decoder_input, 0)\n\n        for i in range(self.max_length):\n            enc_padding_mask, combined_mask, dec_padding_mask = self.mskr.create_masks(\n                encoder_input, output)\n\n            # predictions.shape == (batch_size, seq_len, vocab_size)\n            predictions, attention_weights = self.transformer(encoder_input,\n                                                              output,\n                                                              False,\n                                                              enc_padding_mask,\n                                                              combined_mask,\n                                                              dec_padding_mask)\n\n            # select the last word from the seq_len dimension\n            predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n\n            predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n\n            # return the result if the predicted_id is equal to the end token\n            if tf.equal(predicted_id, self.tokenizer_tar.vocab_size + 1):\n                return tf.squeeze(output, axis=0), attention_weights\n\n            # concatenate the predicted_id to the output which is given to the decoder\n            # as its input.\n            output = tf.concat([output, predicted_id], axis=-1)\n\n        return tf.squeeze(output, axis=0), attention_weights\n\n    def plot_attention_weights(self, attention, sentence, result, layer):\n        """"""\n        A handy function to plot the attention weights.\n        """"""\n\n        fig = plt.figure(figsize=(16, 8))\n\n        sentence = self.tokenizer_feat.encode(sentence)\n\n        attention = tf.squeeze(attention[layer], axis=0)\n\n        for head in range(attention.shape[0]):\n            ax = fig.add_subplot(2, 4, head + 1)\n\n            # plot the attention weights\n            ax.matshow(attention[head][:-1, :], cmap=\'viridis\')\n\n            fontdict = {\'fontsize\': 10}\n\n            ax.set_xticks(range(len(sentence) + 2))\n            ax.set_yticks(range(len(result)))\n\n            ax.set_ylim(len(result) - 1.5, -0.5)\n\n            ax.set_xticklabels(\n                [\'<start>\'] + [self.tokenizer_feat.decode([i]) for i in sentence] + [\'<end>\'],\n                fontdict=fontdict, rotation=90)\n\n            ax.set_yticklabels([self.tokenizer_tar.decode([i]) for i in result\n                                if i < self.tokenizer_tar.vocab_size],\n                               fontdict=fontdict)\n\n            ax.set_xlabel(\'Head {}\'.format(head + 1))\n\n        plt.tight_layout()\n        plt.show()\n\n    def translate(self, sentence, plot=\'\'):\n        """"""\n        :param sentence: input sentence you wish to receive a response to\n        :param plot: Whether you want to plot the attention weighting\n        :return: The response given the input text.\n        """"""\n\n        result, attention_weights = self.evaluate(sentence)\n\n        predicted_sentence = self.tokenizer_tar.decode([i for i in result\n                                                        if i < self.tokenizer_tar.vocab_size])\n\n        print(\'Input: {}\'.format(sentence))\n        print(\'Predicted translation: {}\'.format(predicted_sentence))\n\n        if plot:\n            self.plot_attention_weights(attention_weights, sentence, result, plot)\n'"
smartnlp/nmt/training.py,25,"b'# coding=utf-8\n# created by msgi on 2020/4/29 4:33 \xe4\xb8\x8b\xe5\x8d\x88\n\nimport time\nfrom datetime import datetime\n\nimport tensorflow as tf\n\nfrom smartnlp.custom.encoding.mask_encoder import MaskEncoder\n\n\nclass Trainer(object):\n    def __init__(self, transformer, learning_rate, optimizer, epochs, train_dataset, test_dataset,\n                 load_ckpt=True,\n                 loss_object=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\'none\'),\n                 train_loss=tf.keras.metrics.Mean(name=\'train_loss\'),\n                 train_accuracy=tf.keras.metrics.SparseCategoricalCrossentropy(name=\'train_accuracy\'),\n                 checkpoint_path=\'./models/checkpoints/\',\n                 max_to_keep=5, test_loss=tf.keras.metrics.Mean(name=\'test_loss\'),\n                 test_accuracy=tf.keras.metrics.SparseCategoricalCrossentropy(name=\'test_accuracy\'),\n                 use_tensorboard=True,\n                 tb_log_dir=\'./logs/gradient_tape/\'\n                 ):\n        self.train_dataset = train_dataset\n        self.test_dataset = test_dataset\n\n        self.loss_object = loss_object\n        self.train_loss = train_loss\n        self.train_accuracy = train_accuracy\n        self.checkpoint_path = checkpoint_path\n        self.mask_encoder = MaskEncoder()\n\n        self.transformer = transformer\n        self.learning_rate = learning_rate\n        self.optimizer = optimizer\n\n        self.epochs = epochs\n        self.ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n        self.max_to_keep = max_to_keep\n        self.ckpt_manager = tf.train.CheckpointManager(self.ckpt, self.checkpoint_path, self.max_to_keep)\n\n        self.test_loss = test_loss\n        self.test_accuracy = test_accuracy\n\n        self.use_tensorboard = use_tensorboard\n        current_time = datetime.now().strftime(""%Y%m%d-%H%M%S"")\n\n        if use_tensorboard:\n            self.train_log_dir = tb_log_dir + current_time + \'/train\'\n            self.test_log_dir = tb_log_dir + current_time + \'/test\'\n\n            self.train_summary_writer = tf.summary.create_file_writer(self.train_log_dir)\n            self.test_summary_writer = tf.summary.create_file_writer(self.test_log_dir)\n\n        if load_ckpt:\n            print(\'Attempting to load latest checkpoint...\')\n            if self.ckpt_manager.latest_checkpoint:\n                self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n                print(\'Latest Checkpoint Restored!!!\')\n            else:\n                print(\'No checkpoint found...\')\n                print(\'Training from scratch\')\n                pass\n        else:\n            pass\n\n    def loss_function(self, real, pred):\n        mask = tf.math.logical_not(tf.math.equal(real, 0))\n        loss_ = self.loss_object(real, pred)\n\n        mask = tf.cast(mask, dtype=loss_.dtype)\n        loss_ *= mask\n\n        return tf.reduce_mean(loss_)\n\n    def create_masks(self, inp, tar):\n        # Encoder padding mask\n        enc_padding_mask = self.mask_encoder.create_padding_mask(inp)\n\n        # Used in the 2nd attention block in the decoder.\n        # This padding mask is used to mask the encoder outputs.\n        dec_padding_mask = self.mask_encoder.create_padding_mask(inp)\n\n        # Used in the 1st attention block in the decoder.\n        # It is used to pad and mask future tokens in the input received by\n        # the decoder.\n        look_ahead_mask = self.mask_encoder.create_look_ahead_mask(tf.shape(tar)[1])\n        dec_target_padding_mask = self.mask_encoder.create_padding_mask(tar)\n        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n\n        return enc_padding_mask, combined_mask, dec_padding_mask\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n    ])\n    def train_step(self, inp, tar):\n        tar_inp = tar[:, :-1]\n        tar_real = tar[:, 1:]\n\n        enc_padding_mask, combined_mask, dec_padding_mask = self.mask_encoder.create_masks(inp, tar_inp)\n\n        with tf.GradientTape() as tape:\n            predictions, _ = self.transformer(inp, tar_inp,\n                                              True,\n                                              enc_padding_mask,\n                                              combined_mask,\n                                              dec_padding_mask)\n            loss = self.loss_function(tar_real, predictions)\n\n        gradients = tape.gradient(loss, self.transformer.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.transformer.trainable_variables))\n\n        self.train_loss(loss)\n        self.train_accuracy(tar_real, predictions)\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n    ])\n    def test_step(self, inp, tar):\n        tar_inp = tar[:, :-1]\n        tar_real = tar[:, 1:]\n\n        enc_padding_mask, combined_mask, dec_padding_mask = self.mask_encoder.create_masks(inp, tar_inp)\n\n        predictions, _ = self.transformer(inp, tar_inp,\n                                          True,\n                                          enc_padding_mask,\n                                          combined_mask,\n                                          dec_padding_mask)\n        loss = self.loss_function(tar_real, predictions)\n\n        self.test_loss(loss)\n        self.test_accuracy(tar_real, predictions)\n\n    def train(self):\n        loss_hist = {}\n        acc_hist = {}\n\n        test_loss_hist = {}\n        test_acc_hist = {}\n\n        for epoch in range(self.epochs):\n            start = time.time()\n\n            self.train_loss.reset_states()\n            self.train_accuracy.reset_states()\n\n            # inp -> client, tar -> qltm\n            for (batch, (inp, tar)) in enumerate(self.train_dataset):\n                self.train_step(inp, tar)\n            if self.use_tensorboard:\n                with self.train_summary_writer.as_default():\n                    tf.summary.scalar(\'loss\', self.train_loss.result(), step=epoch)\n                    tf.summary.scalar(\'accuracy\', self.train_accuracy.result(), step=epoch)\n\n                \'\'\'\n                if batch % 50 == 0:\n                    print(\'Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\'.format(\n                        epoch + 1, batch, self.train_loss.result(), self.train_accuracy.result()))\n                \'\'\'\n\n            # inp -> client, tar -> qltm\n            for (batch, (inp, tar)) in enumerate(self.test_dataset):\n                self.test_step(inp, tar)\n            if self.use_tensorboard:\n                with self.test_summary_writer.as_default():\n                    tf.summary.scalar(\'test_loss\', self.test_loss.result(), step=epoch)\n                    tf.summary.scalar(\'test_accuracy\', self.test_accuracy.result(), step=epoch)\n\n            \'\'\'\n            if (epoch + 1) % 5 == 0:\n                ckpt_save_path = self.ckpt_manager.save()\n                print(\'Saving checkpoint for epoch {} at {}\'.format(epoch + 1,\n                                                                    ckpt_save_path))\n            print(\'Epoch {} Loss {:.4f} Accuracy {:.4f}\'.format(epoch + 1,\n                                                                self.train_loss.result(),\n                                                                self.train_accuracy.result()))\n            \'\'\'\n            loss_hist[epoch] = self.train_loss.result()\n            acc_hist[epoch] = self.train_accuracy.result()\n            test_loss_hist[epoch] = self.test_loss.result()\n            test_acc_hist[epoch] = self.test_accuracy.result()\n\n            template = f\'Epoch {epoch + 1} | Loss: {self.train_loss.result()} | Accuracy: {self.train_accuracy.result() * 100}\'\n            template += f\'\\nTest Loss: {self.test_loss.result()} | Test Accuracy: {self.test_accuracy.result() * 100}\'\n            template += f\'\\nTime taken: {time.time() - start}\'\n            print(template)\n        self.train_loss.reset_states()\n        self.test_loss.reset_states()\n        self.train_accuracy.reset_states()\n        self.test_accuracy.reset_states()\n\n        return loss_hist, acc_hist, test_loss_hist, test_acc_hist\n'"
smartnlp/simililarity/__init__.py,0,b''
smartnlp/simililarity/siamese_similarity.py,0,"b'import itertools\nimport logging\nimport os\nimport pickle\nimport time\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, concatenate, BatchNormalization, \\\n    Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom smartnlp.utils.basic_log import Log\nfrom smartnlp.utils.clean_text import clean_to_list\nfrom smartnlp.utils.loader import load_bin_word2vec\nfrom smartnlp.utils.plot_model_history import plot\nfrom smartnlp.utils.stopwords import get_en_stopwords\n\nlog = Log(logging.INFO)\n\n\nclass SiameseSimilarity:\n    def __init__(self, model_path,\n                 config_path,\n                 data_path=None,\n                 embedding_file=None,\n                 n_hidden=128,\n                 batch_size=64,\n                 epochs=10,\n                 embedding_dim=300,\n                 train=False):\n        """"""\n        \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        :param model_path: \xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe6\x88\x96\xe8\x80\x85\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb7\xaf\xe5\xbe\x84\n        :param config_path: \xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe6\x88\x96\xe8\x80\x85\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n        :param data_path: \xe5\xad\x98\xe6\x94\xbe\xe4\xba\x86train.csv\xe5\x92\x8ctest.csv\xe7\x9a\x84\xe7\x9b\xae\xe5\xbd\x95\n        :param embedding_file: \xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe6\x96\x87\xe4\xbb\xb6\n        :param n_hidden: lstm\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xbb\xb4\xe5\xba\xa6\n        :param batch_size: \xe6\xaf\x8f\xe6\x89\xb9\xe6\x95\xb0\xe7\x9b\xae\xe5\xa4\xa7\xe5\xb0\x8f\n        :param epochs: \n        :param train: \xe6\x98\xaf\xe5\x90\xa6\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x8c\xe5\x88\x99\xe5\xbf\x85\xe9\xa1\xbb\xe6\x8f\x90\xe4\xbe\x9bdata_path\n        """"""\n\n        self.model_path = model_path\n        self.config_path = config_path\n        self.embedding_dim = embedding_dim\n        self.n_hidden = n_hidden\n        self.max_length = 500\n\n        # \xe5\x8a\xa0\xe8\xbd\xbd\xe5\x81\x9c\xe7\x94\xa8\xe8\xaf\x8d\n        self.stops = get_en_stopwords()\n        if not train:\n            self.embeddings, self.word_index, self.max_length = self._load_config()\n            self.model = self._load_model()\n        else:\n            assert data_path is not None, \'\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x8c\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe5\xbf\x85\xe9\xa1\xbb\xef\xbc\x81\'\n            assert embedding_file is not None, \'\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x8c\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe6\x95\xb0\xe6\x8d\xae\xe5\xbf\x85\xe9\xa1\xbb\xef\xbc\x81\'\n            self.data_path = data_path\n            self.batch_size = batch_size\n            self.epochs = epochs\n            self.embedding_file = embedding_file\n            self.x_train, self.y_train, self.x_val, self.y_val, self.word_index, self.max_length = self._load_data()\n            self.embeddings = load_bin_word2vec(self.word_index, self.embedding_file)\n            self.model = self.train(call_back=True)\n\n    def _build_model(self):\n        left_input = Input(shape=(self.max_length,), dtype=\'int32\')\n        right_input = Input(shape=(self.max_length,), dtype=\'int32\')\n        embedding_layer = Embedding(len(self.embeddings),\n                                    self.embedding_dim,\n                                    weights=[self.embeddings],\n                                    input_length=self.max_length,\n                                    trainable=False)\n        # Embedding\n        encoded_left = embedding_layer(left_input)\n        encoded_right = embedding_layer(right_input)\n        # \xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84lstm\xe7\xbd\x91\xe7\xbb\x9c\n        shared_lstm = Bidirectional(LSTM(self.n_hidden // 2, return_sequences=True))\n        shared_lstm4 = Bidirectional(LSTM(self.n_hidden // 2))\n\n        for _ in range(3):\n            encoded_left = shared_lstm(encoded_left)\n        left_output = shared_lstm4(encoded_left)\n\n        for _ in range(3):\n            encoded_right = shared_lstm(encoded_right)\n        right_output = shared_lstm4(encoded_right)\n\n        # \xe5\x90\x88\xe5\xb9\xb6\xe5\x90\x8e\xe8\xae\xa1\xe7\xae\x97\n        merged = concatenate([left_output, right_output])\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.5)(merged)\n        merged = Dense(32, activation=\'relu\')(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(0.5)(merged)\n        output = Dense(1, activation=\'sigmoid\')(merged)\n        # \xe6\x9e\x84\xe9\x80\xa0\xe6\xa8\xa1\xe5\x9e\x8b\n        model = Model([left_input, right_input], [output])\n        # Adam \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n        model.compile(loss=\'binary_crossentropy\',\n                      optimizer=\'adam\',\n                      metrics=[\'accuracy\'])\n        model.summary()\n        return model\n\n    def train(self, weights_only=True, call_back=False):\n        model = self._build_model()\n\n        if call_back:\n            early_stopping = EarlyStopping(monitor=\'val_loss\', patience=30)\n            stamp = \'lstm_%d\' % self.n_hidden\n            checkpoint_dir = os.path.join(\n                self.model_path, \'checkpoints/\' + str(int(time.time())) + \'/\')\n            if not os.path.exists(checkpoint_dir):\n                os.makedirs(checkpoint_dir)\n\n            bst_model_path = checkpoint_dir + stamp + \'.h5\'\n            if weights_only:\n                model_checkpoint = ModelCheckpoint(\n                    bst_model_path, save_best_only=True, save_weights_only=True)\n            else:\n                model_checkpoint = ModelCheckpoint(\n                    bst_model_path, save_best_only=True)\n            tensor_board = TensorBoard(\n                log_dir=checkpoint_dir + ""logs/{}"".format(time.time()))\n            callbacks = [early_stopping, model_checkpoint, tensor_board]\n        else:\n            callbacks = None\n        model_trained = model.fit([self.x_train[\'left\'], self.x_train[\'right\']],\n                                  self.y_train,\n                                  batch_size=self.batch_size,\n                                  epochs=self.epochs,\n                                  validation_data=([self.x_val[\'left\'], self.x_val[\'right\']], self.y_val),\n                                  verbose=1,\n                                  callbacks=callbacks)\n        if weights_only and not call_back:\n            model.save_weights(os.path.join(self.model_path, \'weights_only.h5\'))\n        elif not weights_only and not call_back:\n            model.save(os.path.join(self.model_path, \'model.h5\'))\n        self._save_config()\n        plot(model_trained)\n        return model\n\n    def _save_config(self):\n        with open(self.config_path, \'wb\') as out:\n            pickle.dump((self.embeddings, self.word_index, self.max_length), out)\n        if out:\n            out.close()\n\n    # \xe6\x8e\xa8\xe7\x90\x86\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x96\x87\xe6\x9c\xac\xe7\x9a\x84\xe7\x9b\xb8\xe4\xbc\xbc\xe5\xba\xa6\xef\xbc\x8c\xe5\xa4\xa7\xe4\xba\x8e0.5\xe5\x88\x99\xe7\x9b\xb8\xe4\xbc\xbc\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe4\xb8\x8d\xe7\x9b\xb8\xe4\xbc\xbc\n    def predict(self, text1, text2):\n        x1 = self._process_data(text1)\n        x2 = self._process_data(text2)\n\n        # \xe8\xbd\xac\xe4\xb8\xba\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\n        return self.model.predict([x1, x2])\n\n    def _process_data(self, text):\n        t = [[self.word_index.get(word, 0) for word in clean_to_list(\n            tex)] for tex in text]\n        t = pad_sequences(t, maxlen=self.max_length)\n        return t\n\n    # \xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\xe4\xb8\x8e\xe5\x8a\xa0\xe8\xbd\xbd\xe8\xb7\xaf\xe5\xbe\x84\xe7\x9b\xb8\xe5\x90\x8c\n    def _load_model(self, weights_only=True):\n        return self._load_model_by_path(self.model_path, weights_only)\n\n    # \xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe5\x8a\xa0\xe8\xbd\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb7\xaf\xe5\xbe\x84\n    def _load_model_by_path(self, model_path, weights_only=True):\n        try:\n            if weights_only:\n                model = self._build_model()\n                model.load_weights(model_path)\n            else:\n                model = load_model(model_path)\n        except FileNotFoundError:\n            model = None\n        return model\n\n    def _load_config(self):\n        log.info(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x88\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe5\x92\x8c\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x89\')\n        with open(self.config_path, \'rb\') as config:\n            embeddings, vocabulary, max_seq_length = pickle.load(config)\n        if config:\n            config.close()\n        return embeddings, vocabulary, max_seq_length\n\n    def _load_data(self, test_size=0.2):\n        log.info(\'\xe6\x95\xb0\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86...\')\n        # word:index\xe5\x92\x8cindex:word\n        word_index = dict()\n        index_word = [\'<unk>\']\n        questions_cols = [\'question1\', \'question2\']\n\n        log.info(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86...\')\n        train_data = os.path.join(self.data_path, \'train.csv\')\n        test_data = os.path.join(self.data_path, \'test.csv\')\n\n        train_df = pd.read_csv(train_data)\n        test_df = pd.read_csv(test_data)\n\n        # \xe6\x89\xbe\xe5\x88\xb0\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\n        sentences = [df[col].str.split(\' \') for df in [train_df, test_df] for col in questions_cols]\n        max_length = max([len(s) for ss in sentences for s in ss if isinstance(s, list)])\n        # \xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86(\xe7\xbb\x9f\xe8\xae\xa1\xe5\xb9\xb6\xe5\xb0\x86\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe7\xb4\xa2\xe5\xbc\x95)\n        for dataset in [train_df, test_df]:\n            for index, row in dataset.iterrows():\n                for question_col in questions_cols:\n                    question_indexes = []\n                    for word in clean_to_list(row[question_col]):\n                        if word in self.stops:\n                            continue\n                        if word not in word_index:\n                            word_index[word] = len(index_word)\n                            question_indexes.append(len(index_word))\n                            index_word.append(word)\n                        else:\n                            question_indexes.append(word_index[word])\n                    dataset._set_value(index, question_col, question_indexes)\n\n        x = train_df[questions_cols]\n        y = train_df[\'is_duplicate\']\n        x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=test_size)\n\n        x_train = {\'left\': x_train.question1, \'right\': x_train.question2}\n        x_val = {\'left\': x_val.question1, \'right\': x_val.question2}\n\n        y_train = y_train.values\n        y_val = y_val.values\n\n        for dataset, side in itertools.product([x_train, x_val], [\'left\', \'right\']):\n            dataset[side] = pad_sequences(dataset[side], maxlen=max_length)\n\n        # \xe6\xa0\xa1\xe9\xaa\x8c\xe9\x97\xae\xe9\xa2\x98\xe5\xaf\xb9\xe5\x90\x84\xe8\x87\xaa\xe6\x95\xb0\xe7\x9b\xae\xe6\x98\xaf\xe5\x90\xa6\xe6\xad\xa3\xe7\xa1\xae\n        assert x_train[\'left\'].shape == x_train[\'right\'].shape\n        assert len(x_train[\'left\']) == len(y_train)\n        return x_train, y_train, x_val, y_val, word_index, max_length\n'"
smartnlp/topic/__init__.py,0,b''
smartnlp/topic/lda_topic.py,0,"b""import pickle\n\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel, TfidfModel\nimport jieba\nimport os\n\n\nclass LdaTopicModel(object):\n\n    def __init__(self, model_path,\n                 config_path,\n                 train=False,\n                 file_path=None):\n        self.model_path = model_path\n        self.config_path = config_path\n        if not train:\n            self.dictionary, self.tf_idf = self.load_config()\n            self.model = self.load_model()\n        else:\n            self.file_path = file_path\n            self.dictionary, self.tf_idf, self.model = self.train()\n\n    def train(self):\n        corpus = self.preprocess()\n        dictionary = Dictionary(corpus)\n        doc2bow = [dictionary.doc2bow(text) for text in corpus]\n\n        tf_idf = TfidfModel(doc2bow)\n        corpus_tf_idf = tf_idf[doc2bow]\n\n        model = LdaModel(corpus_tf_idf, num_topics=2)\n        return dictionary, tf_idf, model\n\n    def save_model(self):\n        self.model.save(self.model_path)\n\n    def load_model(self):\n        try:\n            model = LdaModel.load(self.model_path)\n        except FileNotFoundError:\n            model = None\n        return model\n\n    def predict(self, text):\n        line_cut = list(jieba.cut(text))\n        doc2bow = self.dictionary.doc2bow(line_cut)\n        corpus_tf_idf = self.tf_idf[doc2bow]\n        return self.model[corpus_tf_idf]\n\n    def save_config(self):\n        with open(self.config_path, 'wb') as file:\n            pickle.dump((self.dictionary, self.tf_idf), file)\n\n    def load_config(self):\n        with open(self.config_path, 'rb') as file:\n            dictionary, tf_idf = pickle.load(file)\n        return dictionary, tf_idf\n\n    def preprocess(self):\n        # \xe8\xaf\xbb\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x8b\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\n        files = os.listdir(self.file_path)\n        corpus = []\n        for file in files:\n            dir_ = os.path.join(self.file_path, file)\n            with open(dir_, 'r', encoding='utf-8') as file_:\n                line = file_.read()\n                line_cut = list(jieba.cut(line))\n                corpus.append(line_cut)\n        return corpus\n"""
smartnlp/utils/__init__.py,0,b''
smartnlp/utils/basic_log.py,0,"b""import logging as log\n\n\nclass Log:\n    def __init__(self, level):\n        self.level = level\n        log.basicConfig(format='%(asctime)s - %(pathname)s[line:%(lineno)d] - %(levelname)s: %(message)s',\n                        level=level)\n        self.log = log\n\n    def info(self, msg):\n        self.log.info(msg)\n\n    def debug(self, msg):\n        self.log.debug(msg)\n\n    def warn(self, msg):\n        self.log.warn(msg)\n\n    def error(self, msg):\n        self.log.error(msg)\n"""
smartnlp/utils/clean_text.py,0,"b'# coding:utf-8\n\nimport re\n\n\ndef clean_en_text(text):\n    """"""\n    \xe6\xb8\x85\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae,\xe6\xad\xa3\xe5\x88\x99\xe6\x96\xb9\xe5\xbc\x8f,\xe5\x8e\xbb\xe9\x99\xa4\xe6\xa0\x87\xe7\x82\xb9\xe7\xac\xa6\xe5\x8f\xb7\xe7\xad\x89\n    :param text:\n    :return:\n    """"""\n    text = re.sub(r""[^A-Za-z0-9(),!?\\\'`]"", "" "", text)\n    text = re.sub(r""\\\'s"", "" \\\'s"", text)\n    text = re.sub(r""\\\'ve"", "" \\\'ve"", text)\n    text = re.sub(r""n\\\'t"", "" n\\\'t"", text)\n    text = re.sub(r""\\\'re"", "" \\\'re"", text)\n    text = re.sub(r""\\\'d"", "" \\\'d"", text)\n    text = re.sub(r""\\\'ll"", "" \\\'ll"", text)\n    text = re.sub(r"","", "" , "", text)\n    text = re.sub(r""!"", "" ! "", text)\n    text = re.sub(r""\\("", "" \\( "", text)\n    text = re.sub(r""\\)"", "" \\) "", text)\n    text = re.sub(r""\\?"", "" \\? "", text)\n    text = re.sub(r""\\s{2,}"", "" "", text)\n    return text.strip().lower()\n\n\ndef clean_zh_text(text):\n    text = re.sub(r\'[""\\\'` ?!\xe3\x80\x90\xe3\x80\x91\\[\\]./%\xef\xbc\x9a:&()=\xef\xbc\x8c,<>+_\xef\xbc\x9b;\\-*]+\', "" "", text)\n    return text\n\n\ndef clean_to_list(text):\n    text = str(text)\n    text = text.lower()\n    # \xe6\xb8\x85\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\n    text = re.sub(r""[^A-Za-z0-9^,!.\\/\'+-=]"", "" "", text)\n    text = re.sub(r""what\'s"", ""what is "", text)\n    text = re.sub(r""\\\'s"", "" "", text)\n    text = re.sub(r""\\\'ve"", "" have "", text)\n    text = re.sub(r""can\'t"", ""cannot "", text)\n    text = re.sub(r""n\'t"", "" not "", text)\n    text = re.sub(r""i\'m"", ""i am "", text)\n    text = re.sub(r""\\\'re"", "" are "", text)\n    text = re.sub(r""\\\'d"", "" would "", text)\n    text = re.sub(r""\\\'ll"", "" will "", text)\n    text = re.sub(r"","", "" "", text)\n    text = re.sub(r""\\."", "" "", text)\n    text = re.sub(r""!"", "" ! "", text)\n    text = re.sub(r""/"", "" "", text)\n    text = re.sub(r""\\^"", "" ^ "", text)\n    text = re.sub(r""\\+"", "" + "", text)\n    text = re.sub(r""-"", "" - "", text)\n    text = re.sub(r""="", "" = "", text)\n    text = re.sub(r""\'"", "" "", text)\n    text = re.sub(r""(\\d+)(k)"", r""\\g<1>000"", text)\n    text = re.sub(r"":"", "" : "", text)\n    text = re.sub(r"" e g "", "" eg "", text)\n    text = re.sub(r"" b g "", "" bg "", text)\n    text = re.sub(r"" u s "", "" american "", text)\n    text = re.sub(r""\\0s"", ""0"", text)\n    text = re.sub(r"" 9 11 "", ""911"", text)\n    text = re.sub(r""e - mail"", ""email"", text)\n    text = re.sub(r""j k"", ""jk"", text)\n    text = re.sub(r""\\s{2,}"", "" "", text)\n    text = text.split()\n    return text\n'"
smartnlp/utils/loader.py,0,"b""# coding=utf-8\n# created by msgi on 2020/4/25 10:01 \xe4\xb8\x8a\xe5\x8d\x88\nimport os\n\nimport numpy as np\nimport logging\nimport pickle\nfrom gensim.models import KeyedVectors\nfrom nltk.corpus import stopwords\nfrom smartnlp.utils.basic_log import Log\n\nlog = Log(logging.INFO)\n\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f(\xe8\x8e\xb7\xe5\x8f\x96\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f: \xe8\xb0\xb7\xe6\xad\x8c\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f)\ndef load_bin_word2vec(word_index, word2vec_path, max_index):\n    log.info('Begin load word vector from bin...')\n    word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n    embeddings = _select_vectors(word2vec, word_index, max_index)\n    log.info('End load word vector from bin...')\n    return embeddings\n\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f(\xe8\x8e\xb7\xe5\x8f\x96\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x9a\xe8\x87\xaa\xe5\xb7\xb1\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\x96\x87\xe6\x9c\xac\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f)\ndef load_text_vector(word_index, word2vec_path, max_index):\n    log.info('Begin load word vector from text...')\n    word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=False)\n    embeddings = _select_vectors(word2vec, word_index, max_index)\n    log.info('End load word vector from text...')\n    return embeddings\n\n\ndef _select_vectors(word2vec, word_index, max_index):\n    embedding_dim = word2vec.vector_size\n    embeddings = 1 * np.random.randn(max_index + 1, embedding_dim)\n    embeddings[0] = 0\n\n    for word, index in word_index.items():\n        if word in word2vec.vocab:\n            embeddings[index] = word2vec.word_vec(word)\n    return embeddings\n\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe8\x8b\xb1\xe6\x96\x87\xe5\x81\x9c\xe7\x94\xa8\xe8\xaf\x8d\ndef load_en_stopwords(file_path=None):\n    return stopwords.words('english')\n\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe4\xb8\xad\xe6\x96\x87\xe5\x81\x9c\xe7\x94\xa8\xe8\xaf\x8d\ndef load_zh_stopwords(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n        lines = [line.strip() for line in lines]\n    return lines\n\n\n# \xe5\x86\x99\xe5\x85\xa5\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\ndef save_config(config, config_path):\n    with open(config_path, 'wb') as f:\n        pickle.dump(config, f)\n\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\ndef load_config(config_path):\n    try:\n        with open(config_path, 'rb') as f:\n            config = pickle.load(f)\n    except FileNotFoundError:\n        config = None\n    return config\n\n\n# \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\ndef save_model(model, model_path, weights_only=True):\n    if model:\n        if weights_only:\n            model.save_weights(os.path.join(model_path, 'weights.h5'))\n        else:\n            model.save(os.path.join(model_path, 'model.h5'))\n\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\ndef load_model(model_path, model=None, weights_only=True):\n    try:\n        if weights_only and model:\n            model.load_weights(os.path.join(model_path, 'weights.h5'))\n        else:\n            model = load_model(os.path.join(model_path, 'model.h5'))\n    except FileNotFoundError:\n        model = None\n    return model\n"""
smartnlp/utils/plot_model_history.py,0,"b""import matplotlib.pyplot as plt\n\n\ndef plot(model_trained):\n    # Plot accuracy\n    plt.plot(model_trained.history['accuracy'])\n    plt.plot(model_trained.history['val_accuracy'])\n    plt.title('Model Accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n\n    # Plot loss\n    plt.plot(model_trained.history['loss'])\n    plt.plot(model_trained.history['val_loss'])\n    plt.title('Model Loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper right')\n    plt.show()\n"""
smartnlp/utils/pre_process.py,0,"b""import jieba\n\n\n# \xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x88\x86\xe8\xaf\x8d\xe5\xb9\xb6\xe5\x8e\xbb\xe9\x99\xa4\xe5\x81\x9c\xe7\x94\xa8\xe8\xaf\x8d\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x86\x99\xe5\x9b\x9e\xe6\x96\x87\xe4\xbb\xb6\ndef seg_to_file(file, out_file, user_dict=None, stop_dict=None):\n    sentences = process_data(file, user_dict, stop_dict)\n    sentences = [' '.join([l for l in line]) + '\\n' for line in sentences]\n\n    with open(out_file, 'w', encoding='utf-8') as o:\n        o.writelines(sentences)\n\n\ndef process_data(train_file, user_dict=None, stop_dict=None):\n    # \xe7\xbb\x93\xe5\xb7\xb4\xe5\x88\x86\xe8\xaf\x8d\xe5\x8a\xa0\xe8\xbd\xbd\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe8\xaf\x8d\xe5\x85\xb8(\xe8\xa6\x81\xe7\xac\xa6\xe5\x90\x88jieba\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe8\xaf\x8d\xe5\x85\xb8\xe8\xa7\x84\xe8\x8c\x83)\n    if user_dict:\n        jieba.load_userdict(user_dict)\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe5\x81\x9c\xe7\x94\xa8\xe8\xaf\x8d\xe8\xa1\xa8(\xe6\xaf\x8f\xe8\xa1\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe5\x81\x9c\xe7\x94\xa8\xe8\xaf\x8d)\n    stop_words = []\n    if stop_dict:\n        with open(stop_dict, 'r', encoding='utf-8') as file:\n            stop_words = [stop_word.strip() for stop_word in file.readlines()]\n\n    # \xe8\xaf\xbb\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6\xe5\x86\x85\xe5\xae\xb9\xe5\xb9\xb6\xe5\x88\x86\xe8\xaf\x8d, \xe5\x8e\xbb\xe6\x8e\x89\xe5\x81\x9c\xe7\x94\xa8\xe8\xaf\x8d\n    with open(train_file, 'r', encoding='utf-8') as file:\n        sentences = file.readlines()\n        sentences = [jieba.lcut(sentence.strip()) for sentence in sentences]\n        sentences = [[s for s in sentence if s not in stop_words and s.strip() != ''] for sentence in sentences]\n\n    return sentences\n"""
smartnlp/utils/stopwords.py,0,"b""from nltk.corpus import stopwords\n\n\n# \xe8\x8b\xb1\xe6\x96\x87\xe5\x81\x9c\xe7\x94\xa8\xe8\xaf\x8d\xe7\x9b\xb4\xe6\x8e\xa5\xe5\xbc\x95\xe7\x94\xa8nltk\xe5\x8c\x85\xe9\x87\x8c\xe7\x9a\x84\xe5\x8d\xb3\xe5\x8f\xaf\ndef get_en_stopwords():\n    return set(stopwords.words('english'))\n\n\n# \xe4\xb8\xad\xe6\x96\x87\xe5\x81\x9c\xe7\x94\xa8\xe8\xaf\x8d\xe8\x87\xaa\xe5\xb7\xb1\xe5\xae\x9e\xe7\x8e\xb0\ndef get_zh_stopwords():\n    raise NotImplementedError\n"""
tutorials/01.game_sales_predict/01.preprocess_data.py,0,"b'import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\ntraining_data_df = pd.read_csv(""dataset/sales_data_training.csv"")\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\ntest_data_df = pd.read_csv(""dataset/sales_data_testing.csv"")\n\n# \xe6\x95\xb0\xe6\x8d\xae\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\nscaler = MinMaxScaler(feature_range=(0, 1))\n\n# \xe5\xaf\xb9\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x92\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe9\x83\xbd\xe5\x81\x9a\xe5\x90\x8c\xe6\xa0\xb7\xe7\x9a\x84\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x9a\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe9\x83\xbd\xe5\x81\x9a\xe4\xba\x86\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\nscaled_training = scaler.fit_transform(training_data_df)\nscaled_testing = scaler.transform(test_data_df)\n\n# multiplying by 0.0000036968 and adding -0.115913\nprint(""Scaled by multiplying by {:.10f} and adding {:.6f}"".format(scaler.scale_[8], scaler.min_[8]))\n\n# \xe5\x88\x9b\xe5\xbb\xba\xe6\x96\xb0\xe7\x9a\x84data frame\nscaled_training_df = pd.DataFrame(scaled_training, columns=training_data_df.columns.values)\nscaled_testing_df = pd.DataFrame(scaled_testing, columns=test_data_df.columns.values)\n\n# \xe4\xbf\x9d\xe5\xad\x98\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x90\x8e\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\nscaled_training_df.to_csv(""dataset/sales_data_training_scaled.csv"", index=False)\nscaled_testing_df.to_csv(""dataset/sales_data_testing_scaled.csv"", index=False)\n'"
tutorials/01.game_sales_predict/02.create_model.py,0,"b'from tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=9, activation=\'relu\'))\nmodel.add(Dense(100, activation=\'relu\'))\nmodel.add(Dense(50, activation=\'relu\'))\nmodel.add(Dense(1, activation=\'linear\'))\nmodel.compile(loss=""mean_squared_error"", optimizer=""adam"")\n'"
tutorials/01.game_sales_predict/03.train_model.py,0,"b'import pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\n\ntraining_data_df = pd.read_csv(""dataset/sales_data_training_scaled.csv"")\n\nX = training_data_df.drop(\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\', axis=1).values\nY = training_data_df[[\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\']].values\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x9a\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe7\xbd\x91\xe7\xbb\x9c\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=9, activation=\'relu\'))\nmodel.add(Dense(100, activation=\'relu\'))\nmodel.add(Dense(50, activation=\'relu\'))\nmodel.add(Dense(1, activation=\'linear\'))\nmodel.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\n\n# \xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\nmodel.fit(\n    X,\n    Y,\n    epochs=50,\n    shuffle=True,\n    verbose=2\n)\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\ntest_data_df = pd.read_csv(""dataset/sales_data_testing_scaled.csv"")\n\nX_test = test_data_df.drop(\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\', axis=1).values\nY_test = test_data_df[[\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\']].values\n\ntest_error_rate = model.evaluate(X_test, Y_test, verbose=0)\nprint(""The mean squared error (MSE) for the test data set is: {}"".format(test_error_rate))\n'"
tutorials/01.game_sales_predict/04.predict.py,0,"b'import pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\n\ntraining_data_df = pd.read_csv(""dataset/sales_data_training_scaled.csv"")\n\nX = training_data_df.drop(\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\', axis=1).values\nY = training_data_df[[\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\']].values\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x9a\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe7\xbd\x91\xe7\xbb\x9c\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=9, activation=\'relu\'))\nmodel.add(Dense(100, activation=\'relu\'))\nmodel.add(Dense(50, activation=\'relu\'))\nmodel.add(Dense(1, activation=\'linear\'))\nmodel.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\n\n# \xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\nmodel.fit(\n    X,\n    Y,\n    epochs=50,\n    shuffle=True,\n    verbose=2\n)\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\ntest_data_df = pd.read_csv(""dataset/sales_data_testing_scaled.csv"")\n\nX_test = test_data_df.drop(\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\', axis=1).values\nY_test = test_data_df[[\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\']].values\n\ntest_error_rate = model.evaluate(X_test, Y_test, verbose=0)\nprint(""The mean squared error (MSE) for the test data set is: {}"".format(test_error_rate))\n\nX = pd.read_csv(""dataset/proposed_new_product.csv"").values\nprediction = model.predict(X)\n\n# \xe5\xbe\x85\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\nprediction = prediction[0][0]\n\n# \xe9\x87\x8d\xe6\x96\xb0\xe6\x81\xa2\xe5\xa4\x8d\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\nprediction = prediction + 0.1159\nprediction = prediction / 0.0000036968\n\nprint(""Earnings Prediction for Proposed Product : ${}"".format(prediction))\n'"
tutorials/01.game_sales_predict/05.save_trained_model.py,0,"b'import pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\n\ntraining_data_df = pd.read_csv(""dataset/sales_data_training_scaled.csv"")\n\nX = training_data_df.drop(\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\', axis=1).values\nY = training_data_df[[\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\']].values\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x9a\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe7\xbd\x91\xe7\xbb\x9c\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=9, activation=\'relu\'))\nmodel.add(Dense(100, activation=\'relu\'))\nmodel.add(Dense(50, activation=\'relu\'))\nmodel.add(Dense(1, activation=\'linear\'))\nmodel.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\n\n# \xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\nmodel.fit(\n    X,\n    Y,\n    epochs=50,\n    shuffle=True,\n    verbose=2\n)\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\ntest_data_df = pd.read_csv(""dataset/sales_data_testing_scaled.csv"")\n\nX_test = test_data_df.drop(\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\', axis=1).values\nY_test = test_data_df[[\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\']].values\n\ntest_error_rate = model.evaluate(X_test, Y_test, verbose=0)\nprint(""The mean squared error (MSE) for the test data set is: {}"".format(test_error_rate))\n\n# \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n# Save the model to disk\nmodel.save(""./models/trained_model.h5"")\nprint(""Model saved to disk."")\n'"
tutorials/01.game_sales_predict/06.load_saved_model.py,0,"b'import pandas as pd\nfrom tensorflow.keras.models import load_model\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\nmodel = load_model(\'./models/trained_model.h5\')\n\nX = pd.read_csv(""dataset/proposed_new_product.csv"").values\nprediction = model.predict(X, )\n\n# \xe5\xbe\x85\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\nprediction = prediction[0][0]\n\n# \xe9\x87\x8d\xe6\x96\xb0\xe6\x81\xa2\xe5\xa4\x8d\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\nprediction = prediction + 0.1159\nprediction = prediction / 0.0000036968\n\nprint(""Earnings Prediction for Proposed Product : ${}"".format(prediction))\n'"
tutorials/01.game_sales_predict/07.model_logging.py,0,"b'import pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.callbacks import *\n\ntraining_data_df = pd.read_csv(""dataset/sales_data_training_scaled.csv"")\n\nX = training_data_df.drop(\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\', axis=1).values\nY = training_data_df[[\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\']].values\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=9, activation=\'relu\', name=\'layer_1\'))\nmodel.add(Dense(100, activation=\'relu\', name=\'layer_2\'))\nmodel.add(Dense(50, activation=\'relu\', name=\'layer_3\'))\nmodel.add(Dense(1, activation=\'linear\', name=\'output_layer\'))\nmodel.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\n\n# \xe6\x9e\x84\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\x97\xa5\xe5\xbf\x97\xe6\x89\x93\xe5\x8d\xb0\xe7\xbb\x84\xe4\xbb\xb6\nlogger = TensorBoard(\n    log_dir=\'logs\',\n    write_graph=True,\n    histogram_freq=5\n)\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\ntest_data_df = pd.read_csv(""dataset/sales_data_testing_scaled.csv"")\n\nX_test = test_data_df.drop(\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\', axis=1).values\nY_test = test_data_df[[\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\']].values\n\n# \xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\nmodel.fit(\n    X,\n    Y,\n    epochs=50,\n    shuffle=True,\n    verbose=2,\n    callbacks=[logger],\n    validation_data=[X_test, Y_test]\n)\n\ntest_error_rate = model.evaluate(X_test, Y_test, verbose=0)\nprint(""The mean squared error (MSE) for the test data set is: {}"".format(test_error_rate))\n'"
tutorials/01.game_sales_predict/08.visualize_training.py,0,"b'import pandas as pd\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\n\n""""""\n\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xa4\x9a\xe6\xac\xa1\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x9c\xa8TensorBoard\xe4\xb8\xad\xe6\x9f\xa5\xe7\x9c\x8b\xe5\x9b\xa0\xe4\xb8\xba\xe5\x8f\x82\xe6\x95\xb0\xe5\x8f\x98\xe5\x8a\xa8\xe5\xaf\xbc\xe8\x87\xb4\xe7\x9a\x84\xe5\x8f\x98\xe5\x8a\xa8\n""""""\n\nRUN_NAME = ""run 3 with 500 nodes""  # \xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbf\xae\xe6\x94\xb9\n\ntraining_data_df = pd.read_csv(""dataset/sales_data_training_scaled.csv"")\n\nX = training_data_df.drop(\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\', axis=1).values\nY = training_data_df[[\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\']].values\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\nmodel = Sequential()\nmodel.add(Dense(500, input_dim=9, activation=\'relu\', name=\'layer_1\'))\nmodel.add(Dense(100, activation=\'relu\', name=\'layer_2\'))\nmodel.add(Dense(50, activation=\'relu\', name=\'layer_3\'))\nmodel.add(Dense(1, activation=\'linear\', name=\'output_layer\'))\nmodel.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\n\n# \xe6\x9e\x84\xe5\xbb\xbaTensorBoard\xe6\x97\xa5\xe5\xbf\x97\xe7\xbb\x84\xe4\xbb\xb6\nlogger = TensorBoard(\n    log_dir=\'logs/{}\'.format(RUN_NAME),\n    histogram_freq=5,\n    write_graph=True\n)\n\ntest_data_df = pd.read_csv(""dataset/sales_data_testing_scaled.csv"")\n\nX_test = test_data_df.drop(\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\', axis=1).values\nY_test = test_data_df[[\'\xe9\x94\x80\xe5\x94\xae\xe6\x80\xbb\xe9\xa2\x9d\']].values\n\n# \xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\nmodel.fit(\n    X,\n    Y,\n    epochs=50,\n    shuffle=True,\n    verbose=2,\n    callbacks=[logger],\n    validation_data=[X_test, Y_test]\n)\n\ntest_error_rate = model.evaluate(X_test, Y_test, verbose=0)\nprint(""The mean squared error (MSE) for the test data set is: {}"".format(test_error_rate))\n'"
tutorials/02.bert-sentiment-classification/app.py,0,"b'import time\n\nimport flask\nimport torch\nimport torch.nn as nn\nfrom flask import Flask, request\n\nimport config\nfrom model import BERTBaseUncased\n\napp = Flask(__name__)\n\nMODEL = None\n\n\ndef sentence_prediction(sentence, model):\n    tokenizer = config.TOKENIZER\n    max_length = config.MAX_LEN\n    review = str(sentence)\n\n    inputs = tokenizer.encode_plus(\n        review,\n        None,\n        add_special_tokens=True,\n        max_length=max_length\n    )\n\n    ids = inputs[""input_ids""]\n    mask = inputs[""attention_mask""]\n    token_type_ids = inputs[""token_type_ids""]\n\n    padding_length = max_length - len(ids)\n    ids = ids + ([0] * padding_length)\n    mask = mask + ([0] * padding_length)\n    token_type_ids = token_type_ids + ([0] * padding_length)\n\n    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0)\n\n    ids = ids.to(config.DEVICE, dtype=torch.long)\n    token_type_ids = token_type_ids.to(config.DEVICE, dtype=torch.long)\n    mask = mask.to(config.DEVICE, dtype=torch.long)\n\n    outputs = model(\n        ids=ids,\n        mask=mask,\n        token_type_ids=token_type_ids\n    )\n\n    outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n    return outputs[0][0]\n\n\n@app.route(""/predict"")\ndef predict():\n    sentence = request.args.get(""sentence"")\n    start_time = time.time()\n    positive_prediction = sentence_prediction(sentence, model=MODEL)\n    negative_prediction = 1 - positive_prediction\n    response = {}\n    response[""response""] = {\n        \'positive\': str(positive_prediction),\n        \'negative\': str(negative_prediction),\n        \'sentence\': str(sentence),\n        \'time_taken\': str(time.time() - start_time)\n    }\n    return flask.jsonify(response)\n\n\nif __name__ == ""__main__"":\n    MODEL = BERTBaseUncased()\n    MODEL = nn.DataParallel(MODEL)\n    MODEL.load_state_dict(torch.load(config.MODEL_PATH, map_location=torch.device(config.DEVICE)))\n    MODEL.to(config.DEVICE)\n    MODEL.eval()\n\n    app.run()\n'"
tutorials/02.bert-sentiment-classification/config.py,0,"b'import transformers\nimport torch\n\nMAX_LEN = 512\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nEPOCHS = 10\nBERT_PATH = ""./input/bert_base_chinese/""\nMODEL_PATH = ""model.bin""\nTRAINING_FILE = ""./input/sentiment.csv""\nTOKENIZER = transformers.BertTokenizer.from_pretrained(\n    BERT_PATH,\n    do_lower_case=True\n)\nDEVICE = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")\n\n'"
tutorials/02.bert-sentiment-classification/dataset.py,0,"b'import config\nimport torch\n\n\nclass BERTDataset:\n    def __init__(self, review, target):\n        self.review = review\n        self.target = target\n        self.tokenizer = config.TOKENIZER\n        self.max_len = config.MAX_LEN\n\n    def __len__(self):\n        return len(self.review)\n\n    def __getitem__(self, item):\n        review = str(self.review[item])\n\n        inputs = self.tokenizer.encode_plus(\n            review,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True\n        )\n\n        ids = inputs[""input_ids""]\n        mask = inputs[""attention_mask""]\n        token_type_ids = inputs[""token_type_ids""]\n\n        return {\n            \'ids\': torch.tensor(ids, dtype=torch.long),\n            \'mask\': torch.tensor(mask, dtype=torch.long),\n            \'token_type_ids\': torch.tensor(token_type_ids, dtype=torch.long),\n            \'targets\': torch.tensor(self.target[item], dtype=torch.float)\n        }\n'"
tutorials/02.bert-sentiment-classification/engine.py,0,"b'from tqdm import tqdm\nimport torch\nimport torch.nn as nn\n\n\ndef loss_fn(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n\n\ndef train_fn(data_loader, model, optimizer, scheduler, device):\n    model.train()\n    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n        ids = d[""ids""]\n        mask = d[""mask""]\n        token_type_ids = d[""token_type_ids""]\n        targets = d[""targets""]\n\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n\n        optimizer.zero_grad()\n        outputs = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n\n    final_targets = []\n    final_outputs = []\n\n    with torch.no_grad():\n        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n            ids = d[""ids""]\n            mask = d[""mask""]\n            token_type_ids = d[""token_type_ids""]\n            targets = d[""targets""]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            outputs = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            final_targets.extend(targets.cpu().detach().numpy().tolist())\n            final_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return final_outputs, final_targets\n'"
tutorials/02.bert-sentiment-classification/model.py,0,"b'import config\nimport transformers\nimport torch.nn as nn\n\n\nclass BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH)\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 1)\n\n    def forward(self, ids, mask, token_type_ids):\n        _, cls_out = self.bert(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n        bert_output = self.bert_drop(cls_out)\n        output = self.out(bert_output)\n        return output\n'"
tutorials/02.bert-sentiment-classification/train.py,0,"b'import config\nimport torch\nimport torch.nn as nn\nimport dataset\nimport engine\nimport pandas as pd\nimport numpy as np\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom model import BERTBaseUncased\n\n\ndef run():\n    dfx = pd.read_csv(config.TRAINING_FILE).fillna(""NONE"")\n\n    df_train, df_valid = model_selection.train_test_split(\n        dfx,\n        test_size=0.1,\n        random_state=42,\n        stratify=dfx.sentiment.values\n    )\n\n    df_train = df_train.reset_index(drop=True)\n    df_valid = df_valid.reset_index(drop=True)\n\n    train_dataset = dataset.BERTDataset(\n        review=df_train.review.values,\n        target=df_train.sentiment.values\n    )\n\n    valid_dataset = dataset.BERTDataset(\n        review=df_valid.review.values,\n        target=df_valid.sentiment.values\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        num_workers=4\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        num_workers=1\n    )\n\n    model = BERTBaseUncased()\n    model.to(config.DEVICE)\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = [""bias"", ""LayerNorm.bias"", ""LayerNorm.weight""]\n    optimizer_parameters = [\n        {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], ""weight_decay"": 0.001},\n        {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], ""weight_decay"": 0.0}\n    ]\n\n    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n\n    model = nn.DataParallel(model)\n\n    best_accuracy = 0\n    for epoch in range(config.EPOCHS):\n        engine.train_fn(train_data_loader, model, optimizer, scheduler, config.DEVICE)\n        outputs, targets = engine.eval_fn(valid_data_loader, model, config.DEVICE)\n        outputs = np.array(outputs) >= 0.5\n        accuracy = metrics.accuracy_score(targets, outputs)\n        print(f""Accuracy Score={accuracy}"")\n\n        if accuracy > best_accuracy:\n            torch.save(model.state_dict(), config.MODEL_PATH)\n            best_accuracy = accuracy\n\n\nif __name__ == ""__main__"":\n    run()\n'"
smartnlp/custom/encoding/__init__.py,0,b'# coding=utf-8\n# created by msgi on 2020/4/28 7:38 \xe4\xb8\x8b\xe5\x8d\x88\n\n'
smartnlp/custom/encoding/mask_encoder.py,7,"b""# coding=utf-8\n# created by msgi on 2020/4/28 7:43 \xe4\xb8\x8b\xe5\x8d\x88\nimport tensorflow as tf\n\n\nclass MaskEncoder:\n    @staticmethod\n    def create_padding_mask(seq):\n        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n        # \xe6\xb7\xbb\xe5\x8a\xa0\xe9\xa2\x9d\xe5\xa4\x96\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe6\x9d\xa5\xe5\xb0\x86\xe5\xa1\xab\xe5\x85\x85\xe5\x8a\xa0\xe5\x88\xb0\xe6\xb3\xa8\xe6\x84\x8f\xe5\x8a\x9b\xe5\xaf\xb9\xe6\x95\xb0\xef\xbc\x88logits\xef\xbc\x89\n        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n\n    @staticmethod\n    def create_look_ahead_mask(size):\n        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n        return mask  # (seq_len, seq_len)\n\n    def create_masks(self, inp, tar):\n        # \xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8\xe5\xa1\xab\xe5\x85\x85\xe9\x81\xae\xe6\x8c\xa1\n        enc_padding_mask = self.create_padding_mask(inp)\n\n        # \xe5\x9c\xa8\xe8\xa7\xa3\xe7\xa0\x81\xe5\x99\xa8\xe7\x9a\x84\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe6\xb3\xa8\xe6\x84\x8f\xe5\x8a\x9b\xe6\xa8\xa1\xe5\x9d\x97\xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n        # \xe8\xaf\xa5\xe5\xa1\xab\xe5\x85\x85\xe9\x81\xae\xe6\x8c\xa1\xe7\x94\xa8\xe4\xba\x8e\xe9\x81\xae\xe6\x8c\xa1\xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe3\x80\x82\n        dec_padding_mask = self.create_padding_mask(inp)\n\n        # \xe5\x9c\xa8\xe8\xa7\xa3\xe7\xa0\x81\xe5\x99\xa8\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\xb3\xa8\xe6\x84\x8f\xe5\x8a\x9b\xe6\xa8\xa1\xe5\x9d\x97\xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n        # \xe7\x94\xa8\xe4\xba\x8e\xe5\xa1\xab\xe5\x85\x85\xef\xbc\x88pad\xef\xbc\x89\xe5\x92\x8c\xe9\x81\xae\xe6\x8c\xa1\xef\xbc\x88mask\xef\xbc\x89\xe8\xa7\xa3\xe7\xa0\x81\xe5\x99\xa8\xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\xb0\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x90\x8e\xe7\xbb\xad\xe6\xa0\x87\xe8\xae\xb0\xef\xbc\x88future tokens\xef\xbc\x89\xe3\x80\x82\n        look_ahead_mask = self.create_look_ahead_mask(tf.shape(tar)[1])\n        dec_target_padding_mask = self.create_padding_mask(tar)\n        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n\n        return enc_padding_mask, combined_mask, dec_padding_mask\n\n\nif __name__ == '__main__':\n    x = tf.random.uniform((1, 8))\n    print(x)\n    temp = MaskEncoder().create_look_ahead_mask(x.shape[1])\n    print(temp)\n\n    x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n    print(MaskEncoder().create_padding_mask(x))\n"""
smartnlp/custom/encoding/position_encoder.py,1,"b'# coding=utf-8\n# created by msgi on 2020/4/28 7:24 \xe4\xb8\x8b\xe5\x8d\x88\nimport numpy as np\nimport tensorflow as tf\n\n\nclass PositionalEncoder:\n\n    def __init__(self, d_model=512):\n        """"""\n        This allows to grade positional importance for each word relative to each sequence it belongs to relative to\n        the entire corpus.\n        :param d_model: Dimensionality of the encoding.\n        """"""\n        self.d_model = d_model\n\n    def get_angles(self, position, i, d_model=None):\n        if d_model is None:\n            d_model = self.d_model\n        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n        return position * angle_rates\n\n    def positional_encoding(self, position, d_model=None):\n        if d_model is None:\n            d_model = self.d_model\n        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n                                     np.arange(d_model)[np.newaxis, :],\n                                     d_model)\n\n        # apply sin to even indices in the array; 2i\n        sines = np.sin(angle_rads[:, 0::2])\n\n        # apply cos to odd indices in the array; 2i+1\n        cosines = np.cos(angle_rads[:, 1::2])\n\n        pos_encoding = np.concatenate([sines, cosines], axis=-1)\n\n        pos_encoding = pos_encoding[np.newaxis, ...]\n\n        return tf.cast(pos_encoding, dtype=tf.float32)\n\n\nif __name__ == \'__main__\':\n    pos_encoding = PositionalEncoder().positional_encoding(50, 512)\n    print(pos_encoding.shape)\n'"
smartnlp/custom/layer/__init__.py,0,b'# coding=utf-8\n# created by msgi on 2020/4/1 7:21 \xe4\xb8\x8b\xe5\x8d\x88\n\nfrom .decoder import Decoder\nfrom .decoder_layer import DecoderLayer\nfrom .encoder_layer import EncoderLayer\nfrom .encoder import Encoder\nfrom .attention import *'
smartnlp/custom/layer/attention.py,40,"b'# coding=utf-8\n# created by msgi on 2020/4/1 7:23 \xe4\xb8\x8b\xe5\x8d\x88\nimport tensorflow as tf\n\n\n# simple attention mechanism\nclass BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, query, values):\n        # query shape: (batch_size, hidden_size)\n        # values shape: (batch_size, max_len, hidden_size)\n        # hidden_with_time_axis shape: (batch_size\xef\xbc\x8c1\xef\xbc\x8chidden_size)\n        hidden_with_time_axis = tf.expand_dims(query, 1)\n        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n\n        # attention_weights shape: (batch_size\xef\xbc\x8cmax_len\xef\xbc\x8c1)\n        attention_weights = tf.nn.softmax(score, axis=1)\n\n        # context vector shape: (batch_size, hidden_size)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        return context_vector, attention_weights\n\n\nclass LuongAttention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(LuongAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n\n    def call(self, query, values):\n        # query shape: (batch_size, hidden_size)\n        # values shape: (batch_size, max_len, hidden_size)\n        # score shape: (batch_size, 1, max_len)\n        # hidden_with_time_axis shape: (batch_size, 1, hidden_size)\n        hidden_with_time_axis = tf.expand_dims(query, 1)\n        score = tf.matmul(hidden_with_time_axis, self.W1(values), transpose_b=True)\n        attention_weights = tf.nn.softmax(score, axis=2)\n        context_vector = tf.matmul(attention_weights, values)\n        return context_vector, attention_weights\n\n\nclass VanillaRNNAttention(tf.keras.layers.Layer):\n    def __init__(self, attention_size):\n        self.attention_size = attention_size\n        self.W = tf.keras.layers.Dense(attention_size, activation=\'tanh\')\n        self.U = tf.keras.layers.Dense(1)\n        super(VanillaRNNAttention, self).__init__()\n\n    def call(self, x, mask=None):\n        # et shape: (batch_size, max_len, attention_size)\n        et = self.W(x)\n        # at shape: (batch_size, max_len)\n        at = tf.nn.softmax(tf.squeeze(self.U(et), axis=-1))\n        if mask is not None:\n            at *= tf.cast(mask, tf.float32)\n        # atx shape: (batch_size, max_len, 1)\n        atx = tf.expand_dims(at, -1)\n\n        # sum result shape: (batch_size, attention_size)\n        sum_result = tf.reduce_sum(atx * x, axis=1)\n        return sum_result\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]\n\n\n# \xe5\xa4\x9a\xe5\xa4\xb4\xe8\x87\xaa\xe6\xb3\xa8\xe6\x84\x8f\xe5\x8a\x9b\xe6\x9c\xba\xe5\x88\xb6\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    @staticmethod\n    def scaled_dot_product_attention(q, k, v, mask):\n        """"""\xe8\xae\xa1\xe7\xae\x97\xe6\xb3\xa8\xe6\x84\x8f\xe5\x8a\x9b\xe6\x9d\x83\xe9\x87\x8d\xe3\x80\x82\n          q, k, v \xe5\xbf\x85\xe9\xa1\xbb\xe5\x85\xb7\xe6\x9c\x89\xe5\x8c\xb9\xe9\x85\x8d\xe7\x9a\x84\xe5\x89\x8d\xe7\xbd\xae\xe7\xbb\xb4\xe5\xba\xa6\xe3\x80\x82\n          k, v \xe5\xbf\x85\xe9\xa1\xbb\xe6\x9c\x89\xe5\x8c\xb9\xe9\x85\x8d\xe7\x9a\x84\xe5\x80\x92\xe6\x95\xb0\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x8c\xe4\xbe\x8b\xe5\xa6\x82\xef\xbc\x9aseq_len_k = seq_len_v\xe3\x80\x82\n          \xe8\x99\xbd\xe7\x84\xb6 mask \xe6\xa0\xb9\xe6\x8d\xae\xe5\x85\xb6\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x88\xe5\xa1\xab\xe5\x85\x85\xe6\x88\x96\xe5\x89\x8d\xe7\x9e\xbb\xef\xbc\x89\xe6\x9c\x89\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x8c\n          \xe4\xbd\x86\xe6\x98\xaf mask \xe5\xbf\x85\xe9\xa1\xbb\xe8\x83\xbd\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xb9\xbf\xe6\x92\xad\xe8\xbd\xac\xe6\x8d\xa2\xe4\xbb\xa5\xe4\xbe\xbf\xe6\xb1\x82\xe5\x92\x8c\xe3\x80\x82\n          \xe5\x8f\x82\xe6\x95\xb0:\n            q: \xe8\xaf\xb7\xe6\xb1\x82\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6 == (..., seq_len_q, depth)\n            k: \xe4\xb8\xbb\xe9\x94\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6 == (..., seq_len_k, depth)\n            v: \xe6\x95\xb0\xe5\x80\xbc\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6 == (..., seq_len_v, depth_v)\n            mask: Float \xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8c\xe5\x85\xb6\xe5\xbd\xa2\xe7\x8a\xb6\xe8\x83\xbd\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\n                  (..., seq_len_q, seq_len_k)\xe3\x80\x82\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xbaNone\xe3\x80\x82\n          \xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc:\n            \xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe6\xb3\xa8\xe6\x84\x8f\xe5\x8a\x9b\xe6\x9d\x83\xe9\x87\x8d\n        """"""\n        matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n        # \xe7\xbc\xa9\xe6\x94\xbe matmul_qk\n        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n        # \xe5\xb0\x86 mask \xe5\x8a\xa0\xe5\x85\xa5\xe5\x88\xb0\xe7\xbc\xa9\xe6\x94\xbe\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\x8a\xe3\x80\x82\n        if mask is not None:\n            scaled_attention_logits += (mask * -1e9)\n\n        # softmax \xe5\x9c\xa8\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbd\xb4\xef\xbc\x88seq_len_k\xef\xbc\x89\xe4\xb8\x8a\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe5\x88\x86\xe6\x95\xb0\xe7\x9b\xb8\xe5\x8a\xa0\xe7\xad\x89\xe4\xba\x8e1\xe3\x80\x82\n        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n        output = tf.matmul(attention_weights, v)\n        return output, attention_weights\n\n    def split_heads(self, x, batch_size):\n        """"""\n        \xe5\x88\x86\xe6\x8b\x86\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\xe5\x88\xb0 (num_heads, depth).\n        \xe8\xbd\xac\xe7\xbd\xae\xe7\xbb\x93\xe6\x9e\x9c\xe4\xbd\xbf\xe5\xbe\x97\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba (batch_size, num_heads, seq_len, depth)\n        """"""\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n\n        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n        output = self.dense(concat_attention)\n\n        return output, attention_weights\n\n    @staticmethod\n    def point_wise_feed_forward_network(d_model, dff):\n        """"""\n        :param d_model:\n        :param dff:\n        :return: A feed forward nn to be stacked after each attention layer.\n        """"""\n        return tf.keras.Sequential([\n            tf.keras.layers.Dense(dff, activation=\'relu\'),  # (batch_size, seq_len, dff)\n            tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n        ])\n\n\nif __name__ == \'__main__\':\n    temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n    y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n    out, attn = temp_mha(y, k=y, q=y, mask=None)\n    print(out.shape, attn.shape)\n'"
smartnlp/custom/layer/decoder.py,7,"b""# coding=utf-8\n# created by msgi on 2020/4/26 3:15 \xe4\xb8\x8b\xe5\x8d\x88\nimport tensorflow as tf\nfrom smartnlp.custom.layer.decoder_layer import DecoderLayer\nfrom smartnlp.custom.layer.encoder_layer import EncoderLayer\nfrom smartnlp.custom.encoding.position_encoder import PositionalEncoder\n\n\nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoder = PositionalEncoder(d_model=d_model)\n        self.pos_encoding = self.pos_encoder.positional_encoding(target_vocab_size, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,\n             look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                                   look_ahead_mask, padding_mask)\n\n            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n\n        # x.shape == (batch_size, target_seq_len, d_model)\n        return x, attention_weights\n\n\nif __name__ == '__main__':\n    sample_encoder_layer = EncoderLayer(512, 8, 2048)\n    sample_encoder_layer_output = sample_encoder_layer(\n        tf.random.uniform((64, 43, 512)),\n        False,\n        None\n    )\n    print(sample_encoder_layer_output.shape)  # (batch_size, input_seq_len, d_model)\n\n    sample_decoder_layer = DecoderLayer(512, 8, 2048)\n\n    sample_decoder_layer_output, _, _ = sample_decoder_layer(\n        tf.random.uniform((64, 50, 512)),\n        sample_encoder_layer_output,\n        False,\n        None,\n        None\n    )\n\n    print(sample_decoder_layer_output.shape)  # (batch_size, target_seq_len, d_model)\n"""
smartnlp/custom/layer/decoder_layer.py,7,"b'# coding=utf-8\n# created by msgi on 2020/4/28 7:26 \xe4\xb8\x8b\xe5\x8d\x88\nimport tensorflow as tf\nfrom smartnlp.custom.layer.attention import MultiHeadAttention\n\n\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = MultiHeadAttention.point_wise_feed_forward_network(d_model, dff)\n\n        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layer_norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,\n             look_ahead_mask, padding_mask):\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layer_norm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(\n            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layer_norm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layer_norm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n\n        return out3, attn_weights_block1, attn_weights_block2\n'"
smartnlp/custom/layer/encoder.py,5,"b'# coding=utf-8\n# created by msgi on 2020/4/28 7:28 \xe4\xb8\x8b\xe5\x8d\x88\nimport tensorflow as tf\nfrom smartnlp.custom.layer.encoder_layer import EncoderLayer\nfrom smartnlp.custom.encoding.position_encoder import PositionalEncoder\n\n\nclass Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.pos_encoder = PositionalEncoder(d_model=d_model)\n        self.pos_encoding = self.pos_encoder.positional_encoding(input_vocab_size, d_model)\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n                           for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n\n        # \xe5\xb0\x86\xe5\xb5\x8c\xe5\x85\xa5\xe5\x92\x8c\xe4\xbd\x8d\xe7\xbd\xae\xe7\xbc\x96\xe7\xa0\x81\xe7\x9b\xb8\xe5\x8a\xa0\xe3\x80\x82\n        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)\n'"
smartnlp/custom/layer/encoder_layer.py,5,"b'# coding=utf-8\n# created by msgi on 2020/4/28 7:26 \xe4\xb8\x8b\xe5\x8d\x88\nimport tensorflow as tf\nfrom smartnlp.custom.layer.attention import MultiHeadAttention\n\n\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = MultiHeadAttention.point_wise_feed_forward_network(d_model, dff)\n\n        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layer_norm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layer_norm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2\n'"
smartnlp/custom/learning_rate/__init__.py,0,b'# coding=utf-8\n# created by msgi on 2020/4/30 6:56 \xe4\xb8\x8a\xe5\x8d\x88\n\n'
smartnlp/custom/learning_rate/learning_rate.py,6,"b'# coding=utf-8\n# created by msgi on 2020/4/30 6:57 \xe4\xb8\x8a\xe5\x8d\x88\nimport tensorflow as tf\n\n\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n\n    def __init__(self, d_model, warm_up_steps=4000):\n        """"""\n        :param d_model: Dimensionality of the Transformer model\n        :param warm_up_steps: Number of train steps to alter the learning rate over\n        """"""\n        super(CustomSchedule, self).__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warm_up_steps = warm_up_steps\n\n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warm_up_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n\n    def get_config(self):\n        return {\n            ""d_model"": self.d_model,\n            ""warm_up_steps"": self.warm_up_steps\n        }\n\n\nif __name__ == \'__main__\':\n    import matplotlib.pyplot as plt\n\n    d_model = 512\n    learning_rate = CustomSchedule(d_model)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                         epsilon=1e-9)\n\n    temp_learning_rate_schedule = CustomSchedule(d_model)\n\n    plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n    plt.ylabel(""Learning Rate"")\n    plt.xlabel(""Train Step"")\n\n    plt.show()\n'"
smartnlp/custom/model/__init__.py,0,b'# coding=utf-8\n# created by msgi on 2020/4/1 7:21 \xe4\xb8\x8b\xe5\x8d\x88\n\n'
smartnlp/custom/model/classification.py,4,"b""# coding=utf-8\n# created by msgi on 2020/5/27\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\n\n\nclass VanillaClassificationModel(tf.keras.Model):\n    def __init__(self, vocab_size, emb_dim=300):\n        super(VanillaClassificationModel, self).__init__()\n        self.emb = Embedding(vocab_size, emb_dim)\n\n        self.lamb = Lambda(lambda t: tf.reduce_mean(t, axis=1))\n        self.dense1 = Dense(128, activation='relu')\n        self.dense2 = Dense(64, activation='relu')\n        self.dense3 = Dense(16, activation='relu')\n        self.classifier = Dense(1, activation='sigmoid')\n\n    def call(self, inputs):\n        x = self.emb(inputs)\n        x = self.lamb(x)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        x = self.dense3(x)\n        return self.classifier(x)\n\n\nclass CNNClassificationModel(tf.keras.Model):\n    def __init__(self, vocab_size, filter_sizes, emb_dim=300, num_filters=256, drop=0.5):\n        super(CNNClassificationModel, self).__init__()\n        self.filter_sizes = filter_sizes\n        self.num_filters = num_filters\n        self.drop = drop\n        self.emb = Embedding(vocab_size, emb_dim)\n        self.convolutions = [Conv1D(self.num_filters,\n                                    kernel_size=filter_size,\n                                    activation='relu',\n                                    kernel_regularizer=tf.keras.regularizers.l2(0.001))\n                             for filter_size in self.filter_sizes]\n        self.max_pool = GlobalMaxPooling1D()\n        self.concat = Concatenate()\n        self.dropout = Dropout(self.drop)\n        self.classifier = Dense(units=1,\n                                activation='sigmoid',\n                                name='dense')\n\n    def call(self, inputs):\n        x = self.emb(inputs)\n        filter_results = []\n        for convolution in self.convolutions:\n            x = convolution(x)\n            max_pool = self.max_pool(x)\n            filter_results.append(max_pool)\n        concat = self.concat(filter_results)\n        dropout = self.dropout(concat)\n        return self.classifier(dropout)"""
smartnlp/custom/model/seq2seq.py,10,"b""# coding=utf-8\n# created by msgi on 2020/4/28 7:30 \xe4\xb8\x8b\xe5\x8d\x88\n\nimport tensorflow as tf\nfrom smartnlp.custom.layer.attention import BahdanauAttention\n\n\nclass Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n        super(Encoder, self).__init__()\n        self.batch_size = batch_size\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.enc_units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state=hidden)\n        return output, state\n\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_size, self.enc_units))\n\n\nclass Decoder(tf.keras.models.Model):\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n        super(Decoder, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.dec_units = dec_units\n        self.batch_size = batch_size\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(dec_units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n        self.attention = BahdanauAttention(self.dec_units)\n        self.fc = tf.keras.layers.Dense(vocab_size)\n\n    def call(self, x, hidden, enc_output):\n        # \xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x9aenc_output(batch_size, max_length, hidden_size)\n        context_vector, attention_weights = self.attention(hidden, enc_output)\n        # x \xe9\x80\x9a\xe8\xbf\x87embedding\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba x(batch_size, 1, embedding_dim)\n        x = self.embedding(x)\n        # x \xe5\x9c\xa8\xe6\x8b\xbc\xe6\x8e\xa5 \xef\xbc\x88concatenation\xef\xbc\x89 \xe5\x90\x8e\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6 == \xef\xbc\x88\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x8c1\xef\xbc\x8c\xe5\xb5\x8c\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6 + \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x89\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n        output, state = self.gru(x)\n        output = tf.reshape(output, (-1, output.shape[2]))\n        x = self.fc(output)\n        return x, state, attention_weights\n\n\nclass Seq2seq:\n    def __init__(self):\n        pass\n\n    def train_model(self):\n        pass\n\n    def _build_model(self):\n        pass\n"""
smartnlp/custom/model/transformer.py,4,"b""# coding=utf-8\n# created by msgi on 2020/4/26 3:02 \xe4\xb8\x8b\xe5\x8d\x88\nimport tensorflow as tf\nfrom smartnlp.custom.layer.encoder import Encoder\nfrom smartnlp.custom.layer.decoder import Decoder\n\n\nclass Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n                 target_vocab_size, rate=0.1):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n                               input_vocab_size, rate)\n\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n                               target_vocab_size, rate)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n    def call(self, inp, tar, training, enc_padding_mask,\n             look_ahead_mask, dec_padding_mask):\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n\n        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n        dec_output, attention_weights = self.decoder(\n            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n\n        return final_output, attention_weights\n\n\nif __name__ == '__main__':\n    sample_transformer = Transformer(\n        num_layers=2, d_model=512, num_heads=8, dff=2048,\n        input_vocab_size=8500, target_vocab_size=8000)\n\n    temp_input = tf.random.uniform((64, 62))\n    temp_target = tf.random.uniform((64, 26))\n\n    fn_out, _ = sample_transformer(temp_input, temp_target, training=False,\n                                   enc_padding_mask=None,\n                                   look_ahead_mask=None,\n                                   dec_padding_mask=None)\n\n    print(fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size)\n"""
