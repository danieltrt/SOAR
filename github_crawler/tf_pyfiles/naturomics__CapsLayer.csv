file_path,api_count,code
setup.py,0,"b'from setuptools import setup\nfrom setuptools import find_packages\n\ninstall_requires = [\n    \'numpy\',\n    \'scipy\',\n    \'tqdm\'\n]\n\nsetup(\n    name=""capslayer"",\n    version=""0.1.5"",\n    packages=find_packages(),\n    include_package_data=True,\n    author=""Naturomics Liao"",\n    author_email=""naturomics.liao@gmail.com"",\n    url=""https://github.com/naturomics/CapsLayer"",\n    license=""Apache-2.0"",\n    install_requires=install_requires,\n    description=""CapsLayer: An advanced library for capsule theory"",\n    keywords=""capsule, capsNet, deep learning, tensorflow"",\n    platforms=[\'any\']\n)\n'"
capslayer/__init__.py,0,"b'from __future__ import absolute_import\n\nfrom capslayer import layers\nfrom capslayer import data\nfrom capslayer.ops import losses\nfrom capslayer import summary\nfrom capslayer.ops.losses import losses\n\nfrom capslayer.ops.nn_ops import space_to_batch_nd_v1 as space_to_batch_nd\nfrom capslayer.ops.nn_ops import batch_to_space_nd\nfrom capslayer.ops.nn_ops import softmax\n\nfrom capslayer.ops.math_ops import matmul_v2 as matmul\nfrom capslayer.ops.math_ops import norm\nfrom capslayer.ops.math_ops import reduce_sum\nfrom capslayer.ops.math_ops import divide\nfrom capslayer.ops.math_ops import log\n\nfrom capslayer.ops.ops import shape\n\n_allowed_symbols = [\n    \'layers\',\n    \'data\',\n    \'distributions\',\n    \'losses\',\n    \'summary\',\n    \'space_to_batch_nd\',\n    \'batch_to_space_nd\',\n    \'softmax\',\n    \'matmul\',\n    \'norm\',\n    \'divide\',\n    \'log\',\n    \'shape\'\n]\n\n__version__ = ""0.1.5""\n\n__all__ = [s for s in _allowed_symbols]\n'"
models/__init__.py,0,b''
models/baseline.py,22,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport capslayer as cl\nimport tensorflow as tf\n\nfrom config import cfg\n\n\nclass Model(object):\n    def __init__(self, height, width, channels, num_label):\n        self.height = height\n        self.width = width\n        self.channels = channels\n        self.num_label = num_label\n\n    def create_network(self, inputs, labels):\n        self.labels = labels\n        self.y = tf.one_hot(labels, depth=self.num_label, axis=-1, dtype=tf.float32)\n        inputs = tf.reshape(inputs, shape=[-1, self.height, self.width, self.channels])\n        conv1 = tf.layers.conv2d(inputs, filters=256, kernel_size=5, activation=tf.nn.relu)\n        pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding=""SAME"")\n\n        conv2 = tf.layers.conv2d(pool1, filters=256, kernel_size=5, activation=tf.nn.relu)\n        pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding=""SAME"")\n\n        conv3 = tf.layers.conv2d(pool2, filters=128, kernel_size=5, activation=tf.nn.relu)\n        pool3 = tf.nn.max_pool(conv3, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding=""SAME"")\n\n        input = tf.reshape(pool3, shape=(-1, np.prod(pool3.get_shape()[1:])))\n        fc1 = tf.layers.dense(input, units=328)\n        fc2 = tf.layers.dense(fc1, units=192)\n        out = tf.layers.dense(fc2, units=self.num_label, activation=None)\n        self.y_pred = out\n        self.probs = tf.nn.softmax(self.y_pred, axis=1)\n\n        with tf.variable_scope(\'accuracy\'):\n            logits_idx = tf.to_int32(tf.argmax(self.probs, axis=1))\n            correct_prediction = tf.equal(tf.to_int32(self.labels), logits_idx)\n            correct = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n            self.accuracy = tf.reduce_mean(correct / tf.cast(tf.shape(self.probs)[0], tf.float32))\n            cl.summary.scalar(\'accuracy\', self.accuracy, verbose=cfg.summary_verbose)\n\n    def _loss(self):\n        return tf.losses.softmax_cross_entropy(self.y, self.y_pred)\n\n    def train(self, optimizer, num_gpus=1):\n        self.global_step = tf.Variable(1, name=\'global_step\', trainable=False)\n        total_loss = self._loss()\n        optimizer = tf.train.AdamOptimizer()\n        train_ops = optimizer.minimize(total_loss, global_step=self.global_step)\n        summary_ops = tf.summary.merge_all()\n\n        return(total_loss, train_ops, summary_ops)\n'"
models/config.py,3,"b'import os\nimport tensorflow as tf\n\nflags = tf.app.flags\n\n\n############################\n#    hyper parameters      #\n############################\n\n# For margin loss\nflags.DEFINE_float(\'m_plus\', 0.9, \'the parameter of m plus\')\nflags.DEFINE_float(\'m_minus\', 0.1, \'the parameter of m minus\')\n\n# For spread loss\nflags.DEFINE_float(\'m_scheduler\', 1, \'.\')\nflags.DEFINE_float(\'lambda_val\', 0.5, \'down weight of the loss for absent digit classes\')\n\n# for training\nflags.DEFINE_integer(\'batch_size\', 128, \'batch size\')\nflags.DEFINE_integer(\'num_steps\', 50000, \'The number of training steps, default: 50,000\')\nflags.DEFINE_integer(\'iter_routing\', 3, \'number of iterations in routing algorithm\')\nflags.DEFINE_integer(\'train_sum_every\', 200, \'the frequency of saving train summary(step)\')\nflags.DEFINE_integer(\'val_sum_every\', 500, \'the frequency of saving valuation summary(step)\')\nflags.DEFINE_integer(\'save_ckpt_every\', 1000, \'the frequency of saving model(step)\')\n\nflags.DEFINE_float(\'regularization_scale\', 0.392, \'regularization coefficient for reconstruction loss, default to 0.0005*784=0.392\')\n\n\n############################\n#   environment setting    #\n############################\nflags.DEFINE_string(\'model\', \'vectorCapsNet\',\n                    \'The model to use. Default: vectorCapsNet\')\n\nsupported_datasets = [""mnist"", ""fashion_mnist"", ""cifar10"", ""cifar100"", ""norb"", ""celeba""]\nflags.DEFINE_string(\'dataset\', \'mnist\',\n                    \'The name of dataset, one of [\' + "", "".join(supported_datasets) + \']. Default: mnist\')\n\ndata_dir = os.path.join(""data"", ""mnist"")\nresults_dir = os.path.join(\'models\', \'results\')\nlogdir = os.path.join(results_dir, \'logdir\')\n\nflags.DEFINE_string(\'data_dir\', data_dir,\n                    \'The directory containing dataset. Default: \' + data_dir)\nflags.DEFINE_string(\'results_dir\', results_dir,\n                    \'The directory to save all results. Default: \' + results_dir)\nflags.DEFINE_string(\'logdir\', logdir,\n                    \'Logs directory for saving checkpoint. Default: \' + logdir)\nflags.DEFINE_string(\'splitting\', ""TVT"",\n                    \'One of ""TVT"" or ""TT"" (case-insensitive). \\\n                     ""TVT"" for training-validation-test data splitting, and ""TT"" for training-test splitting. \\\n                     Default: TVT\')\nflags.DEFINE_boolean(\'is_training\', True,\n                     \'Boolean, train or predict mode. Default: True\')\nflags.DEFINE_integer(\'num_works\', 8,\n                     \'The number of works for processing data\')\nflags.DEFINE_boolean(\'summary_verbose\', True, \'Use tensorflow summary\')\n\n############################\n#   distributed setting    #\n############################\nflags.DEFINE_integer(\'num_gpus\', 1, \'The number of gpus for distributed training\')\nflags.DEFINE_integer(\'batch_size_per_gpu\', 128, \'batch size on 1 gpu\')\nflags.DEFINE_integer(\'thread_per_gpu\', 8, \'Number of preprocessing threads per tower.\')\n\ncfg = tf.app.flags.FLAGS\n\n# Uncomment this line to run in debug mode\n# tf.logging.set_verbosity(tf.logging.INFO)\n'"
models/main.py,16,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.client import timeline\nfrom importlib import import_module\nfrom capslayer.plotlib import plot_activation\n\nfrom config import cfg\n\n\ndef save_to(is_training):\n    os.makedirs(os.path.join(cfg.results_dir, ""activations""), exist_ok=True)\n    os.makedirs(os.path.join(cfg.results_dir, ""timelines""), exist_ok=True)\n\n    if is_training:\n        loss = os.path.join(cfg.results_dir, \'loss.csv\')\n        train_acc = os.path.join(cfg.results_dir, \'train_acc.csv\')\n        val_acc = os.path.join(cfg.results_dir, \'val_acc.csv\')\n\n        if os.path.exists(val_acc):\n            os.remove(val_acc)\n        if os.path.exists(loss):\n            os.remove(loss)\n        if os.path.exists(train_acc):\n            os.remove(train_acc)\n\n        fd_train_acc = open(train_acc, \'w\')\n        fd_train_acc.write(\'step,train_acc\\n\')\n        fd_loss = open(loss, \'w\')\n        fd_loss.write(\'step,loss\\n\')\n        fd_val_acc = open(val_acc, \'w\')\n        fd_val_acc.write(\'step,val_acc\\n\')\n        fd = {""train_acc"": fd_train_acc,\n              ""loss"": fd_loss,\n              ""val_acc"": fd_val_acc}\n    else:\n        test_acc = os.path.jion(cfg.results_dir, \'test_acc.csv\')\n        if os.path.exists(test_acc):\n            os.remove(test_acc)\n        fd_test_acc = open(test_acc, \'w\')\n        fd_test_acc.write(\'test_acc\\n\')\n        fd = {""test_acc"": fd_test_acc}\n\n    return(fd)\n\n\ndef train(model, data_loader):\n    # Setting up model\n    training_iterator = data_loader(cfg.batch_size, mode=""train"")\n    validation_iterator = data_loader(cfg.batch_size, mode=""eval"")\n    inputs = data_loader.next_element[""images""]\n    labels = data_loader.next_element[""labels""]\n    model.create_network(inputs, labels)\n\n    loss, train_ops, summary_ops = model.train(cfg.num_gpus)\n\n    # Creating files, saver and summary writer to save training results\n    fd = save_to(is_training=True)\n    summary_writer = tf.summary.FileWriter(cfg.logdir)\n    summary_writer.add_graph(tf.get_default_graph())\n    saver = tf.train.Saver(var_list=tf.trainable_variables(), max_to_keep=10)\n\n    # Setting up training session\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    run_metadata = tf.RunMetadata()\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        training_handle = sess.run(training_iterator.string_handle())\n        validation_handle = sess.run(validation_iterator.string_handle())\n        init_op = tf.global_variables_initializer()\n        sess.run(init_op)\n        print(""\\nNote: all of results will be saved to directory: "" + cfg.results_dir)\n        for step in range(1, cfg.num_steps):\n            start_time = time.time()\n            if step % cfg.train_sum_every == 0:\n                _, loss_val, train_acc, summary_str = sess.run([train_ops,\n                                                               loss,\n                                                               model.accuracy,\n                                                               summary_ops],\n                                                               feed_dict={data_loader.handle: training_handle})\n                tl = timeline.Timeline(run_metadata.step_stats)\n                ctf = tl.generate_chrome_trace_format()\n                out_path = os.path.join(cfg.results_dir, ""timelines/timeline_%d.json"" % step)\n                with open(out_path, ""w"") as f:\n                    f.write(ctf)\n                summary_writer.add_summary(summary_str, step)\n                fd[""loss""].write(""{:d},{:.4f}\\n"".format(step, loss_val))\n                fd[""loss""].flush()\n                fd[""train_acc""].write(""{:d},{:.4f}\\n"".format(step, train_acc))\n                fd[""train_acc""].flush()\n            else:\n                _, loss_val = sess.run([train_ops, loss], feed_dict={data_loader.handle: training_handle})\n                # assert not np.isnan(loss_val), \'Something wrong! loss is nan...\'\n\n            if step % cfg.val_sum_every == 0:\n                print(""evaluating, it will take a while..."")\n                sess.run(validation_iterator.initializer)\n                probs = []\n                targets = []\n                total_acc = 0\n                n = 0\n                while True:\n                    try:\n                        val_acc, prob, label = sess.run([model.accuracy, model.probs, labels], feed_dict={data_loader.handle: validation_handle})\n                        probs.append(prob)\n                        targets.append(label)\n                        total_acc += val_acc\n                        n += 1\n                    except tf.errors.OutOfRangeError:\n                        break\n                probs = np.concatenate(probs, axis=0)\n                targets = np.concatenate(targets, axis=0).reshape((-1, 1))\n                avg_acc = total_acc / n\n                path = os.path.join(os.path.join(cfg.results_dir, ""activations""))\n                plot_activation(np.hstack((probs, targets)), step=step, save_to=path)\n                fd[""val_acc""].write(""{:d},{:.4f}\\n"".format(step, avg_acc))\n                fd[""val_acc""].flush()\n            if step % cfg.save_ckpt_every == 0:\n                saver.save(sess,\n                           save_path=os.path.join(cfg.logdir, \'model.ckpt\'),\n                           global_step=step)\n\n            duration = time.time() - start_time\n            log_str = \' step: {:d}, loss: {:.3f}, time: {:.3f} sec/step\' \\\n                      .format(step, loss_val, duration)\n            print(log_str)\n\n\ndef evaluate(model, data_loader):\n    # Setting up model\n    test_iterator = data_loader(cfg.batch_size, mode=""test"")\n    inputs = data_loader.next_element[""images""]\n    labels = data_loader.next_element[""labels""]\n    model.create_network(inputs, labels)\n\n    # Create files to save evaluating results\n    fd = save_to(is_training=False)\n    saver = tf.train.Saver()\n\n    config = tf.ConfigProto(allow_soft_placement=True)\n    config.gpu_options.allow_growth = True\n    with tf.Session(config=config) as sess:\n        test_handle = sess.run(test_iterator.string_handle())\n        saver.restore(sess, tf.train.latest_checkpoint(cfg.logdir))\n        tf.logging.info(\'Model restored!\')\n\n        probs = []\n        targets = []\n        total_acc = 0\n        n = 0\n        while True:\n            try:\n                test_acc, prob, label = sess.run([model.accuracy, model.probs, labels], feed_dict={data_loader.handle: test_handle})\n                probs.append(prob)\n                targets.append(label)\n                total_acc += test_acc\n                n += 1\n            except tf.errors.OutOfRangeError:\n                break\n        probs = np.concatenate(probs, axis=0)\n        targets = np.concatenate(targets, axis=0).reshape((-1, 1))\n        avg_acc = total_acc / n\n        out_path = os.path.join(cfg.results_dir, \'prob_test.txt\')\n        np.savetxt(out_path, np.hstack((probs, targets)), fmt=\'%1.2f\')\n        print(\'Classification probability for each category has been saved to \' + out_path)\n        fd[""test_acc""].write(str(avg_acc))\n        fd[""test_acc""].close()\n        out_path = os.path.join(cfg.results_dir, \'test_accuracy.txt\')\n        print(\'Test accuracy has been saved to \' + out_path)\n\n\ndef main(_):\n    model_list = [\'baseline\', \'vectorCapsNet\', \'matrixCapsNet\', \'convCapsNet\']\n\n    # Deciding which model to use\n    if cfg.model == \'baseline\':\n        model = import_module(cfg.model).Model\n    elif cfg.model in model_list:\n        model = import_module(cfg.model).CapsNet\n    else:\n        raise ValueError(\'Unsupported model, please check the name of model:\', cfg.model)\n\n    # Deciding which dataset to use\n    if cfg.dataset == \'mnist\' or cfg.dataset == \'fashion-mnist\':\n        height = 28\n        width = 28\n        channels = 1\n        num_label = 10\n    elif cfg.dataset == \'smallNORB\':\n        num_label = 5\n        height = 32\n        width = 32\n        channels = 1\n\n    # Initializing model and data loader\n    net = model(height=height, width=width, channels=channels, num_label=num_label)\n    dataset = ""capslayer.data.datasets."" + cfg.dataset\n    data_loader = import_module(dataset).DataLoader(path=cfg.data_dir,\n                                                    splitting=cfg.splitting,\n                                                    num_works=cfg.num_works)\n\n    # Deciding to train or evaluate model\n    if cfg.is_training:\n        train(net, data_loader)\n    else:\n        evaluate(net, data_loader)\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
models/matrixCapsNet.py,34,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport capslayer as cl\nimport tensorflow as tf\n\nfrom config import cfg\n\n\nclass CapsNet(object):\n    def __init__(self, height=28, width=28, channels=1, num_label=10):\n        \'\'\'\n        Args:\n            height: Integer, the height of inputs.\n            width: Integer, the width of inputs.\n            channels: Integer, the channels of inputs.\n            num_label: Integer, the category number.\n        \'\'\'\n        self.height = height\n        self.width = width\n        self.channels = channels\n        self.num_label = num_label\n\n    def create_network(self, inputs, labels):\n        """""" Setup capsule network.\n        Args:\n            inputs: Tensor or array with shape [batch_size, height, width, channels] or [batch_size, height * width * channels].\n            labels: Tensor or array with shape [batch_size].\n\n        Returns:\n            poses: Tensor with shape [batch_size, num_label, 16, 1].\n            probs: Tensor with shape [batch_size, num_label], the probability of entity presence.\n        """"""\n        self.raw_imgs = inputs\n        self.labels = labels\n        probs = []\n        inputs = tf.reshape(inputs, shape=[-1, self.height, self.width, self.channels])\n        conv1 = tf.layers.conv2d(inputs,\n                                 filters=32,\n                                 kernel_size=5,\n                                 strides=2,\n                                 padding=\'VALID\',\n                                 activation=tf.nn.relu,\n                                 name=""Conv1_layer"")\n\n        convCaps, activation = cl.layers.primaryCaps(conv1,\n                                                     filters=32,\n                                                     kernel_size=1,\n                                                     strides=1,\n                                                     out_caps_dims=[4, 4],\n                                                     method=""logistic"",\n                                                     name=""PrimaryCaps_layer"")\n\n        probs.append(tf.reduce_mean(activation))\n        routing_method = \'EMRouting\'\n        convCaps, activation = cl.layers.conv2d(convCaps,\n                                                activation,\n                                                filters=32,\n                                                out_caps_dims=[4, 4],\n                                                kernel_size=(3, 3),\n                                                strides=(2, 2),\n                                                routing_method=routing_method,\n                                                name=""ConvCaps1_layer"")\n\n        probs.append(tf.reduce_mean(activation))\n        convCaps, activation = cl.layers.conv2d(convCaps,\n                                                activation,\n                                                filters=32,\n                                                out_caps_dims=[4, 4],\n                                                kernel_size=(3, 3),\n                                                strides=(1, 1),\n                                                routing_method=routing_method,\n                                                name=""ConvCaps2_layer"")\n\n        probs.append(tf.reduce_mean(activation))\n        self.poses, self.probs = cl.layers.dense(convCaps,\n                                                 activation,\n                                                 num_outputs=self.num_label,\n                                                 out_caps_dims=[4, 4],\n                                                 routing_method=routing_method,\n                                                 coordinate_addition=True,\n                                                 name=""ClassCaps_layer"")\n        probs.append(tf.reduce_mean(self.probs))\n        tf.summary.scalar(""probs"", tf.reduce_mean(probs))\n\n        # Decoder structure\n        # Reconstructe the inputs with 3 FC layers\n        # [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n        with tf.variable_scope(\'Decoder\'):\n            labels = tf.one_hot(self.labels, depth=self.num_label, axis=-1, dtype=tf.float32)\n            self.labels_one_hoted = tf.reshape(labels, (-1, self.num_label, 1, 1))\n            masked_caps = tf.multiply(self.poses, self.labels_one_hoted)\n            num_inputs = np.prod(masked_caps.get_shape().as_list()[1:])\n            active_caps = tf.reshape(masked_caps, shape=(-1, num_inputs))\n            fc1 = tf.layers.dense(active_caps, units=512, activation=tf.nn.relu)\n            fc2 = tf.layers.dense(fc1, units=1024, activation=tf.nn.relu)\n            num_outputs = self.height * self.width * self.channels\n            self.recon_imgs = tf.layers.dense(fc2,\n                                              units=num_outputs,\n                                              activation=tf.sigmoid)\n            recon_imgs = tf.reshape(self.recon_imgs, shape=[-1, self.height, self.width, self.channels])\n            cl.summary.image(\'reconstruction_img\', recon_imgs, verbose=cfg.summary_verbose)\n\n        with tf.variable_scope(\'accuracy\'):\n            cl.summary.histogram(\'activation\', tf.nn.softmax(self.probs, 1), verbose=cfg.summary_verbose)\n            logits_idx = tf.to_int32(tf.argmax(cl.softmax(self.probs, axis=1), axis=1))\n            correct_prediction = tf.equal(tf.to_int32(self.labels), logits_idx)\n            correct = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n            self.accuracy = tf.reduce_mean(correct / tf.cast(tf.shape(self.probs)[0], tf.float32))\n            cl.summary.scalar(\'accuracy\', self.accuracy, verbose=cfg.summary_verbose)\n\n        return self.poses, self.probs\n\n    def _loss(self):\n        with tf.variable_scope(""loss""):\n            # The reconstruction loss\n            orgin = tf.reshape(self.raw_imgs, shape=(-1, self.height * self.width * self.channels))\n            squared = tf.square(self.recon_imgs - orgin)\n            reconstruction_err = tf.reduce_mean(squared)\n            cl.summary.scalar(\'reconstruction_loss\', reconstruction_err, verbose=cfg.summary_verbose)\n\n            # Spread loss\n            initial_margin = 0.2\n            max_margin = 0.9\n            interstep = 8000\n            margin = (self.global_step / interstep) * 0.1 + initial_margin\n            margin = tf.cast(tf.minimum(margin, max_margin), tf.float32)\n            cl.summary.scalar(\'margin\', tf.reduce_mean(margin), verbose=cfg.summary_verbose)\n            spread_loss = cl.losses.spread_loss(logits=self.probs,\n                                                labels=tf.squeeze(self.labels_one_hoted, axis=(2, 3)),\n                                                margin=margin)\n            cl.summary.scalar(\'spread_loss\', spread_loss, verbose=cfg.summary_verbose)\n\n            # Total loss\n            total_loss = spread_loss + cfg.regularization_scale * reconstruction_err\n            cl.summary.scalar(\'total_loss\', total_loss, verbose=cfg.summary_verbose)\n            return total_loss\n\n    def train(self, optimizer, num_gpus=1):\n        self.global_step = tf.Variable(1, name=\'global_step\', trainable=False)\n        total_loss = self._loss()\n        optimizer = tf.train.AdamOptimizer()\n        train_ops = optimizer.minimize(total_loss, global_step=self.global_step)\n        summary_ops = tf.summary.merge_all()\n\n        return(total_loss, train_ops, summary_ops)\n'"
models/vectorCapsNet.py,31,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport capslayer as cl\nimport tensorflow as tf\n\nfrom config import cfg\n\n\nclass CapsNet(object):\n    def __init__(self, height=28, width=28, channels=1, num_label=10):\n        \'\'\'\n\n        Args:\n            height: Integer, the height of inputs.\n            width: Integer, the width of inputs.\n            channels: Integer, the channels of inputs.\n            num_label: Integer, the category number.\n        \'\'\'\n        self.height = height\n        self.width = width\n        self.channels = channels\n        self.num_label = num_label\n\n    def create_network(self, inputs, labels):\n        """""" Setup capsule network.\n\n        Args:\n            inputs: Tensor or array with shape [batch_size, height, width, channels] or [batch_size, height * width * channels].\n            labels: Tensor or array with shape [batch_size].\n\n        Returns:\n            poses: [batch_size, num_label, 16, 1].\n            probs: Tensor with shape [batch_size, num_label], the probability of entity presence.\n        """"""\n        self.raw_imgs = inputs\n        self.labels = labels\n        with tf.variable_scope(\'Conv1_layer\'):\n            # Conv1, return with shape [batch_size, 20, 20, 256]\n            inputs = tf.reshape(self.raw_imgs, shape=[-1, self.height, self.width, self.channels])\n            conv1 = tf.layers.conv2d(inputs,\n                                     filters=256,\n                                     kernel_size=9,\n                                     strides=1,\n                                     padding=\'VALID\',\n                                     activation=tf.nn.relu)\n\n        with tf.variable_scope(\'PrimaryCaps_layer\'):\n            primaryCaps, activation = cl.layers.primaryCaps(conv1,\n                                                            filters=32,\n                                                            kernel_size=9,\n                                                            strides=2,\n                                                            out_caps_dims=[8, 1],\n                                                            method=""norm"")\n\n        with tf.variable_scope(\'DigitCaps_layer\'):\n            routing_method = ""EMRouting""\n            num_inputs = np.prod(cl.shape(primaryCaps)[1:4])\n            primaryCaps = tf.reshape(primaryCaps, shape=[-1, num_inputs, 8, 1])\n            activation = tf.reshape(activation, shape=[-1, num_inputs])\n            self.poses, self.probs = cl.layers.dense(primaryCaps,\n                                                     activation,\n                                                     num_outputs=self.num_label,\n                                                     out_caps_dims=[16, 1],\n                                                     routing_method=routing_method)\n            cl.summary.histogram(\'activation\', self.probs, verbose=cfg.summary_verbose)\n\n        # Decoder structure\n        # Reconstructe the inputs with 3 FC layers\n        with tf.variable_scope(\'Decoder\'):\n            labels = tf.one_hot(self.labels, depth=self.num_label, axis=-1, dtype=tf.float32)\n            self.labels_one_hoted = tf.reshape(labels, (-1, self.num_label, 1, 1))\n            masked_caps = tf.multiply(self.poses, self.labels_one_hoted)\n            num_inputs = np.prod(masked_caps.get_shape().as_list()[1:])\n            active_caps = tf.reshape(masked_caps, shape=(-1, num_inputs))\n            fc1 = tf.layers.dense(active_caps, units=512, activation=tf.nn.relu)\n            fc2 = tf.layers.dense(fc1, units=1024, activation=tf.nn.relu)\n            num_outputs = self.height * self.width * self.channels\n            self.recon_imgs = tf.layers.dense(fc2,\n                                              units=num_outputs,\n                                              activation=tf.sigmoid)\n            recon_imgs = tf.reshape(self.recon_imgs, shape=[-1, self.height, self.width, self.channels])\n            cl.summary.image(\'reconstruction_img\', recon_imgs, verbose=cfg.summary_verbose)\n\n        with tf.variable_scope(\'accuracy\'):\n            logits_idx = tf.to_int32(tf.argmax(cl.softmax(self.probs, axis=1), axis=1))\n            correct_prediction = tf.equal(tf.to_int32(self.labels), logits_idx)\n            correct = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n            self.accuracy = tf.reduce_mean(correct / tf.cast(tf.shape(self.probs)[0], tf.float32))\n            cl.summary.scalar(\'accuracy\', self.accuracy, verbose=cfg.summary_verbose)\n\n        return self.poses, self.probs\n\n    def _loss(self):\n        with tf.variable_scope(""loss""):\n            # 1. Margin loss\n            margin_loss = cl.losses.margin_loss(logits=self.probs,\n                                                labels=tf.squeeze(self.labels_one_hoted, axis=(2, 3)))\n            cl.summary.scalar(\'margin_loss\', margin_loss, verbose=cfg.summary_verbose)\n\n            # 2. The reconstruction loss\n            orgin = tf.reshape(self.raw_imgs, shape=(-1, self.height * self.width * self.channels))\n            squared = tf.square(self.recon_imgs - orgin)\n            reconstruction_err = tf.reduce_mean(squared)\n            cl.summary.scalar(\'reconstruction_loss\', reconstruction_err, verbose=cfg.summary_verbose)\n\n            # 3. Total loss\n            # The paper uses sum of squared error as reconstruction error, but we\n            # have used reduce_mean in `# 2 The reconstruction loss` to calculate\n            # mean squared error. In order to keep in line with the paper,the\n            # regularization scale should be 0.0005*784=0.392\n            total_loss = margin_loss + cfg.regularization_scale * reconstruction_err\n\n            cl.summary.scalar(\'total_loss\', total_loss, verbose=cfg.summary_verbose)\n            return total_loss\n\n    def train(self, optimizer, num_gpus=1):\n        self.global_step = tf.Variable(1, name=\'global_step\', trainable=False)\n        total_loss = self._loss()\n        optimizer = tf.train.AdamOptimizer()\n        train_ops = optimizer.minimize(total_loss, global_step=self.global_step)\n        summary_ops = tf.summary.merge_all()\n\n        return(total_loss, train_ops, summary_ops)\n'"
capslayer/core/__init__.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom capslayer.core.routing import routing\nfrom capslayer.core.transformation import transforming\n\n__all__ = [""routing"", ""transforming""]\n'"
capslayer/core/routing.py,48,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport capslayer as cl\nimport tensorflow as tf\n\n\ndef routing(votes,\n            activation=None,\n            method=\'EMRouting\',\n            num_iter=3,\n            name=None):\n    """""" Routing-by-agreement algorithm.\n\n    Args:\n        votes: A 5-D or 7-D tensor, the transformed matrices of the layer below, which shape should be [batch_size, ..., in_channels/num_inputs, num_outputs] + out_caps_dims and the routing will be performed on the last four dimensions.\n        activation: A 2-D or 4-D tensor, [batch_size, num_inputs] or [batch_size, out_height, out_width, in_channels].\n        method: One of \'EMRouting\' or \'DynamicRouting\', the method for updating coupling coefficients between votes and pose.\n        num_iter: Integer, the number of routing iterations.\n        name: String, a name for the operation.\n\n    Returns:\n        pose: A 4-D or 6-D tensor, [batch_size, num_outputs] + out_caps_dims or [batch_size, out_height, out_width, out_channels] + out_caps_dims.\n        activation: A 2-D or 4-D tensor, [batch_size, num_outputs] or [batch_size, out_height, out_width, out_channels].\n    """"""\n    vote_rank = len(votes.shape)\n    activation_rank = len(activation.shape)\n    if vote_rank != 5 and vote_rank != 7:\n        raise ValueError(\'Votes to ""routing"" should have rank 5 or 7 but it is rank\', str(vote_rank))\n    if activation_rank != 2 and activation_rank != 4:\n        raise ValueError(\'Activation to ""routing"" should have rank 2 or 4 but it is rank\', str(activation_rank))\n\n    name = ""routing"" if name is None else name\n    with tf.variable_scope(name):\n        if method == \'EMRouting\':\n            pose, activation = EMRouting(votes, activation, num_iter)\n        elif method == \'DynamicRouting\':\n            if not votes.shape[-1].value == 1:\n                raise ValueError(\'""DynamicRouting"" method only works for vetocr, please set the ""out_caps_dims"" like [n, 1]\')\n            pose, activation = dynamicRouting(votes, num_iter, leaky=True)\n            # pose, activation = dynamicRouting_v1(votes, num_iter, leaky=True)\n        else:\n            raise Exception(\'Invalid routing method!\', method)\n\n    activation = tf.clip_by_value(activation, 1e-30, 1. - 1e-30)\n    assert len(pose.shape) == 4 or len(pose.shape) == 6\n    assert len(activation.shape) == 2 or len(activation.shape) == 4\n    return(pose, activation)\n\n\ndef dynamicRouting(votes,\n                   num_routing=3,\n                   use_bias=True,\n                   leaky=True):\n    """""" Dynamic routing algorithm.\n\n    See [Sabour et al., 2017](https://arxiv.org/abs/1710.09829).\n\n    Args:\n        votes: A 5-D or 7-D tensor with shape [batch_size, ..., in_channels/num_inputs, num_outputs] + out_caps_dims.\n        num_routing: Integer, number of routing iterations.\n        use_bias: Boolean, whether the layer uses a bias.\n        leaky: Boolean, whether the algorithm uses leaky routing.\n\n    Returns:\n        poses: A 4-D or 6-D tensor.\n        probs: A 2-D or 4-D tensor.\n    """"""\n\n    vote_shape = cl.shape(votes)\n    logit_shape = vote_shape[:-2] + [1, 1]\n    logits = tf.fill(logit_shape, 0.0)\n    squash_on = -2 if vote_shape[-1] == 1 else [-2, -1]\n    if use_bias:\n        bias_shape = [1 for i in range(len(vote_shape) - 3)] + vote_shape[-3:]\n        biases = tf.get_variable(""biases"",\n                                 bias_shape,\n                                 initializer=tf.constant_initializer(0.1),\n                                 dtype=tf.float32)\n\n    def _body(i, logits, poses):\n        if leaky:\n            route = _leaky_routing(logits)\n        else:\n            route = tf.nn.softmax(logits, axis=-3)\n\n        if use_bias:\n            preactivate = cl.reduce_sum(route * votes, axis=-4, keepdims=True) + biases\n        else:\n            preactivate = cl.reduce_sum(route * votes, axis=-4, keepdims=True)\n\n        pose = cl.ops.squash(preactivate, axis=squash_on)\n        poses = poses.write(i, pose)\n\n        if vote_shape[-1] == 1:\n            distances = cl.matmul(votes, pose, transpose_a=True)\n        else:\n            diff = votes - pose\n            distances = tf.trace(cl.matmul(diff, diff))[..., tf.newaxis, tf.newaxis]\n        logits += distances\n\n        return(i + 1, logits, poses)\n\n    poses = tf.TensorArray(dtype=tf.float32,\n                           size=num_routing,\n                           clear_after_read=False)\n    i = tf.constant(0, dtype=tf.int32)\n    _, logits, poses = tf.while_loop(lambda i, logits, poses: i < num_routing,\n                                     _body,\n                                     loop_vars=[i, logits, poses],\n                                     swap_memory=True)\n    poses = tf.squeeze(poses.read(num_routing - 1), axis=-4)\n    probs = tf.norm(poses, axis=[-2, -1])\n\n    return poses, probs\n\n\ndef _leaky_routing(logits):\n    leak_shape = cl.shape(logits)\n    leak = tf.zeros(leak_shape[:-3] + [1, 1, 1])\n    leaky_logits = tf.concat([leak, logits], axis=-3)\n    leaky_routing = cl.softmax(leaky_logits, axis=-3)\n    return tf.split(leaky_routing, [1, leak_shape[-3]], axis=-3)[1]\n\n\ndef dynamicRouting_v1(votes,\n                      num_routing=3,\n                      use_bias=True,\n                      leaky=True):\n    """""" Dynamic routing algorithm.\n\n    See [Sabour et al., 2017](https://arxiv.org/abs/1710.09829).\n\n    Args:\n        votes: A 5-D or 7-D tensor with shape [batch_size, ..., in_channels/num_inputs, num_outputs] + out_caps_dims.\n        num_routing: Integer, number of routing iterations.\n        use_bias: Boolean, whether the layer uses a bias.\n        leaky: Boolean, whether the algorithm uses leaky routing.\n\n    Returns:\n        poses: A 4-D or 6-D tensor.\n        probs: A 2-D or 4-D tensor.\n    """"""\n    vote_shape = cl.shape(votes)\n    logit_shape = vote_shape[:-2] + [1, 1]\n    logits = tf.fill(logit_shape, 0.0)\n    squash_on = -2 if vote_shape[-1] == 1 else [-2, -1]\n    if use_bias:\n        bias_shape = [1 for i in range(len(vote_shape) - 3)] + vote_shape[-3:]\n        biases = tf.get_variable(""biases"",\n                                 bias_shape,\n                                 initializer=tf.constant_initializer(0.1),\n                                 dtype=tf.float32)\n\n    vote_stopped = tf.stop_gradient(votes, name=""stop_gradient"")\n    for i in range(num_routing):\n        with tf.variable_scope(""iter_"" + str(i)):\n            if leaky:\n                route = _leaky_routing(logits)\n            else:\n                route = cl.softmax(logits, axis=-3)\n            if i == num_routing - 1:\n                if use_bias:\n                    preactivate = cl.reduce_sum(tf.multiply(route, votes), axis=-4, keepdims=True) + biases\n                else:\n                    preactivate = cl.reduce_sum(tf.multiply(route, votes), axis=-4, keepdims=True)\n                poses = cl.ops.squash(preactivate, axis=squash_on)\n            else:\n                if use_bias:\n                    preactivate = cl.reduce_sum(tf.multiply(route, vote_stopped), axis=1, keepdims=True) + biases\n                else:\n                    preactivate = cl.reduce_sum(tf.multiply(route, vote_stopped), axis=1, keepdims=True)\n                poses = cl.ops.squash(preactivate, axis=squash_on)\n                logits += cl.reduce_sum(vote_stopped * poses, axis=-4, keepdims=True)\n\n    poses = tf.squeeze(poses, axis=-4)\n    probs = tf.norm(poses, axis=(-2, -1))\n    return(poses, probs)\n\n\ndef EMRouting(votes,\n              activation,\n              num_routing,\n              use_bias=True):\n    """"""\n    Args:\n        votes: [batch_size, ..., in_channels/num_inputs, num_outputs] + out_caps_dims.\n        activation: [batch_size, ..., in_channels/num_inputs]\n\n    Returns:\n        pose: [batch_size, num_outputs] + out_caps_dims\n        activation: [batch_size, num_outputs]\n    """"""\n    vote_shape = cl.shape(votes)\n    num_outputs = vote_shape[-3]\n    out_caps_dims = vote_shape[-2:]\n\n    shape = vote_shape[:-2] + [np.prod(out_caps_dims)]\n    votes = tf.reshape(votes, shape=shape)\n    activation = activation[..., tf.newaxis, tf.newaxis]\n    log_activation = tf.log(activation)\n\n    log_R = tf.log(tf.fill(vote_shape[:-2] + [1], 1.) / num_outputs)\n\n    lambda_min = 0.001\n    lambda_max = 0.006\n    for t_iter in range(num_routing):\n        # increase inverse temperature linearly: lambda = k * t_iter + lambda_min\n        #  let k = (lambda_max - lambda_min) / num_routing\n        # TODO: search for the better lambda_min and lambda_max\n        inverse_temperature = lambda_min + (lambda_max - lambda_min) * t_iter / max(1.0, num_routing)\n        with tf.variable_scope(\'M-STEP\') as scope:\n            if t_iter > 0:\n                scope.reuse_variables()\n            pose, log_var, log_activation_prime = M_step(log_R, log_activation, votes, lambda_val=inverse_temperature)\n            # It\'s no need to do the `E-STEP` in the last iteration\n            if t_iter == num_routing - 1:\n                break\n        with tf.variable_scope(\'E-STEP\'):\n            log_R = E_step(pose, log_var, log_activation_prime, votes)\n    pose = tf.reshape(pose, shape=vote_shape[:-4] + [num_outputs] + out_caps_dims)\n    activation = tf.reshape(tf.exp(log_activation_prime), shape=vote_shape[:-4] + [num_outputs])\n    return pose, activation\n\n\ndef M_step(log_R, log_activation, vote, lambda_val=0.01):\n    R_shape = tf.shape(log_R)\n    log_R = log_R + log_activation\n\n    R_sum_i = cl.reduce_sum(tf.exp(log_R), axis=-3, keepdims=True)\n    log_normalized_R = log_R - tf.reduce_logsumexp(log_R, axis=-3, keepdims=True)\n\n    pose = cl.reduce_sum(vote * tf.exp(log_normalized_R), axis=-3, keepdims=True)\n    log_var = tf.reduce_logsumexp(log_normalized_R + cl.log(tf.square(vote - pose)), axis=-3, keepdims=True)\n\n    beta_v = tf.get_variable(\'beta_v\',\n                             shape=[1 for i in range(len(pose.shape) - 2)] + [pose.shape[-2], 1],\n                             initializer=tf.truncated_normal_initializer(mean=15., stddev=3.))\n    cost = R_sum_i * (beta_v + 0.5 * log_var)\n\n    beta_a = tf.get_variable(\'beta_a\',\n                             shape=[1 for i in range(len(pose.shape) - 2)] + [pose.shape[-2], 1],\n                             initializer=tf.truncated_normal_initializer(mean=100.0, stddev=10))\n    cost_sum_h = cl.reduce_sum(cost, axis=-1, keepdims=True)\n    logit = lambda_val * (beta_a - cost_sum_h)\n    log_activation = tf.log_sigmoid(logit)\n\n    return(pose, log_var, log_activation)\n\n\ndef E_step(pose, log_var, log_activation, vote):\n    normalized_vote = cl.divide(tf.square(vote - pose), 2 * tf.exp(log_var))\n    log_probs = normalized_vote + cl.log(2 * np.pi) + log_var\n    log_probs = -0.5 * cl.reduce_sum(log_probs, axis=-1, keepdims=True)\n    log_activation_logit = log_activation + log_probs\n    log_activation_logit = log_probs\n    log_R = log_activation_logit - tf.reduce_logsumexp(log_activation_logit, axis=-2, keepdims=True)\n    return log_R\n'"
capslayer/core/transformation.py,6,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport capslayer as cl\n\n\ndef transforming(inputs, num_outputs, out_caps_dims, name=None):\n    """"""\n    Args:\n        inputs: A 4-D or 6-D tensor, [batch_size, num_inputs] + in_caps_dims or [batch_size, height, width, channels] + in_caps_dims.\n        num_outputs: Integer, the number of output capsules.\n        out_caps_dims: A list of 2 integers. The dimensions of output capsule, e.g. out_caps_dims=[4, 4].\n        name: String, a name for this operation.\n\n    Returns:\n        votes: A 5-D or 7-D tensor, [batch_size, num_inputs, num_outputs] + out_caps_dims or [batch_size, height, width, channels, num_outputs] + out_caps_dims.\n    """"""\n    name = ""transforming"" if name is None else name\n    with tf.variable_scope(name) as scope:\n        input_shape = cl.shape(inputs)\n        prefix_shape = [1 for i in range(len(input_shape) - 3)] + input_shape[-3:-2] + [num_outputs]\n        in_caps_dims = input_shape[-2:]\n        if in_caps_dims[0] == out_caps_dims[1]:\n            shape = prefix_shape + [out_caps_dims[0], 1, in_caps_dims[1]]\n            expand_axis = -3\n            reduce_sum_axis = -1\n        elif in_caps_dims[1] == out_caps_dims[0]:\n            shape = prefix_shape + [in_caps_dims[0], 1, out_caps_dims[1]]\n            expand_axis = -1\n            reduce_sum_axis = -3\n        elif in_caps_dims[0] == out_caps_dims[0]:\n            shape = prefix_shape + [1, out_caps_dims[1], in_caps_dims[1]]\n            expand_axis = -2\n            reduce_sum_axis = -1\n        elif in_caps_dims[1] == out_caps_dims[1]:\n            shape = prefix_shape + [in_caps_dims[0], out_caps_dims[0], 1]\n            expand_axis = -2\n            reduce_sum_axis = -3\n        else:\n            raise TypeError(""out_caps_dims must have at least one value being the same with the in_caps_dims"")\n        in_pose = tf.expand_dims(inputs, axis=-3)\n        ones = tf.ones(shape=prefix_shape + [1, 1])\n        in_pose = tf.expand_dims(in_pose * ones, axis=expand_axis)\n        transform_mat = tf.get_variable(""transformation_matrix"", shape=shape)\n        votes = tf.reduce_sum(in_pose * transform_mat, axis=reduce_sum_axis)\n\n        return votes\n'"
capslayer/data/__init__.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom capslayer.data import datasets\n'"
capslayer/layers/__init__.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom capslayer.layers.convolutional import conv1d\nfrom capslayer.layers.convolutional import conv2d\nfrom capslayer.layers.convolutional import conv3d\nfrom capslayer.layers.layers import primaryCaps\nfrom capslayer.layers.layers import dense\nfrom capslayer.layers.layers import dense as fully_connected\n\n__all__ = [\'conv1d\', \'conv2d\', \'conv3d\', \'primaryCaps\', \'dense\', \'fully_connected\']\n'"
capslayer/layers/convolutional.py,7,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport capslayer as cl\nimport tensorflow as tf\n\nfrom capslayer.core import routing\nfrom capslayer.core import transforming\n\n\ndef conv2d(inputs,\n           activation,\n           filters,\n           out_caps_dims,\n           kernel_size,\n           strides,\n           padding=""valid"",\n           routing_method=""EMRouting"",\n           name=None,\n           reuse=None):\n    """"""A 2D convolutional capsule layer.\n\n    Args:\n        inputs: A 6-D tensor with shape [batch_size, in_height, in_width, in_channels] + in_caps_dims.\n        activation: A 4-D tensor with shape [batch_size, in_height, in_width, in_channels].\n        filters: Integer, the dimensionality of the output space (i.e. the number of filters in the convolution).\n        out_caps_dims: A tuple/list of 2 integers, specifying the dimensions of output capsule, e.g. out_caps_dims=[4, 4] representing that each output capsule has shape [4, 4].\n        kernel_size:  An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n        strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions.\n        padding: One of ""valid"" or ""same"" (case-insensitive), now only support ""valid"".\n        routing_method: One of ""EMRouting"" or ""DynamicRouting"", the method of routing-by-agreement algorithm.\n        name: A string, the name of the layer.\n        reuse: Boolean, whether to reuse the weights of a previous layer by the same name.\n\n    Returns:\n        pose: A 6-D tensor with shape [batch_size, out_height, out_width, out_channesl] + out_caps_dims.\n        activation: A 4-D tensor with shape [batch_size, out_height, out_width, out_channels].\n    """"""\n\n    name = ""conv2d"" if name is None else name\n    with tf.variable_scope(name) as scope:\n        if reuse:\n            scope.reuse_variables()\n\n        input_shape = cl.shape(inputs)\n        input_rank = len(input_shape)\n        activation_rank = len(activation.shape)\n        if not input_rank == 6:\n            raise ValueError(\'Inputs to `conv2d` should have rank 6. Received inputs rank:\', str(input_rank))\n        if not activation_rank == 4:\n            raise ValueError(\'Activation to `conv2d` should have rank 4. Received activation rank:\', str(activation_rank))\n\n        if isinstance(kernel_size, int):\n            kernel_size = [kernel_size, kernel_size, input_shape[3]]\n        elif isinstance(kernel_size, (list, tuple)) and len(kernel_size) == 2:\n            kernel_size = [kernel_size[0], kernel_size[1], input_shape[3]]\n        else:\n            raise ValueError(\'""kernel_size"" should be an integer or tuple/list of 2 integers. Received:\', str(kernel_size))\n\n        if isinstance(strides, int):\n            strides = [strides, strides, 1]\n        elif isinstance(strides, (list, tuple)) and len(strides) == 2:\n            strides = [strides[0], strides[1], 1]\n        else:\n            raise ValueError(\'""strides"" should be an integer or tuple/list of 2 integers. Received:\', str(kernel_size))\n\n        if not isinstance(out_caps_dims, (list, tuple)) or len(out_caps_dims) != 2:\n            raise ValueError(\'""out_caps_dims"" should be a tuple/list of 2 integers. Received:\', str(out_caps_dims))\n        elif isinstance(out_caps_dims, tuple):\n            out_caps_dims = list(out_caps_dims)\n\n        # 1. space to batch\n        # patching everything into [batch_size, out_height, out_width, in_channels] + in_caps_dims (batched)\n        # and [batch_size, out_height, out_width, in_channels] (activation).\n        batched = cl.space_to_batch_nd(inputs, kernel_size, strides)\n        activation = cl.space_to_batch_nd(activation, kernel_size, strides)\n\n        # 2. transforming\n        # transforming to [batch_size, out_height, out_width, in_channels, out_channels/filters] + out_caps_dims\n        vote = transforming(batched,\n                            num_outputs=filters,\n                            out_caps_dims=out_caps_dims)\n\n        # 3. routing\n        pose, activation = routing(vote, activation, method=routing_method)\n\n        return pose, activation\n\n\ndef conv3d(inputs,\n           activation,\n           filters,\n           out_caps_dims,\n           kernel_size,\n           strides,\n           padding=""valid"",\n           routing_method=""EMRouting"",\n           name=None,\n           reuse=None):\n    """"""A 3D convolutional capsule layer.\n\n    Args:\n        inputs: A 7-D tensor with shape [batch_size, in_depth, in_height, in_width, in_channels] + in_caps_dims.\n        activation: A 5-D tensor with shape [batch_size, in_depth, in_height, in_width, in_channels].\n        filters: Integer, the dimensionality of the output space (i.e. the number of filters in the convolution).\n        out_caps_dims: A tuple/list of 2 integers, specifying the dimensions of output capsule, e.g. out_caps_dims=[4, 4] representing that each output capsule has shape [4, 4].\n        kernel_size:  An integer or tuple/list of 3 integers, specifying the height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n        strides: An integer or tuple/list of 3 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions.\n        padding: One of ""valid"" or ""same"" (case-insensitive), now only support ""valid"".\n        routing_method: One of ""EMRouting"" or ""DynamicRouting"", the method of routing-by-agreement algorithm.\n        name: String, a name for the operation (optional).\n        reuse: Boolean, whether to reuse the weights of a previous layer by the same name.\n\n    Returns:\n        pose: A 7-D tensor with shape [batch_size, out_depth, out_height, out_width, out_channesl] + out_caps_dims.\n        activation: A 5-D tensor with shape [batch_size, out_depth, out_height, out_width, out_channels].\n    """"""\n\n    name = ""conv1d"" if name is None else name\n    with tf.name_scope(name):\n        input_shape = cl.shape(inputs)\n        input_rank = len(input_shape)\n        activation_rank = len(activation.shape)\n        if input_rank != 7:\n            raise ValueError(\'Inputs to `conv3d` should have rank 7. Received input rank:\', str(input_rank))\n        if activation_rank != 5:\n            raise ValueError(\'Activation to `conv3d` should have rank 5. Received input shape:\', str(activation_rank))\n\n        if isinstance(kernel_size, int):\n            kernel_size = [kernel_size, kernel_size, kernel_size]\n        elif isinstance(kernel_size, (list, tuple)) and len(kernel_size) == 3:\n            kernel_size = kernel_size\n        else:\n            raise ValueError(\'""kernel_size"" should be an integer or tuple/list of 3 integers. Received:\', str(kernel_size))\n\n        if isinstance(strides, int):\n            strides = [strides, strides, strides]\n        elif isinstance(strides, (list, tuple)) and len(strides) == 3:\n            strides = strides\n        else:\n            raise ValueError(\'""strides"" should be an integer or tuple/list of 3 integers. Received:\', str(strides))\n\n        if not isinstance(out_caps_dims, (list, tuple)) or len(out_caps_dims) != 2:\n            raise ValueError(\'""out_caps_dims"" should be a tuple/list of 2 integers. Received:\', str(out_caps_dims))\n        elif isinstance(out_caps_dims, tuple):\n            out_caps_dims = list(out_caps_dims)\n\n        # 1. space to batch\n        batched = cl.space_to_batch_nd(inputs, kernel_size, strides)\n        activation = cl.space_to_batch_nd(activation, kernel_size, strides)\n\n        # 2. transforming\n        vote = transforming(batched,\n                            num_outputs=filters,\n                            out_caps_dims=out_caps_dims)\n\n        # 3. routing\n        pose, activation = routing(vote, activation, method=routing_method)\n\n        return pose, activation\n\n\ndef conv1d(inputs,\n           activation,\n           filters,\n           out_caps_dims,\n           kernel_size,\n           stride,\n           padding=""valid"",\n           routing_method=""EMRouting"",\n           name=None,\n           reuse=None):\n    """"""A 1D convolutional capsule layer (e.g. temporal convolution).\n\n    Args:\n        inputs: A 5-D tensor with shape [batch_size, in_width, in_channels] + in_caps_dims.\n        activation: A 3-D tensor with shape [batch_size, in_width, in_channels].\n        kernel_size: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n        strides: An integer or tuple/list of a single integer, specifying the stride length of the convolution.\n\n    Returns:\n        pose: A 5-D tensor with shape [batch_size, out_width, out_channesl] + out_caps_dims.\n        activation: A 3-D tensor with shape [batch_size, out_width, out_channels].\n\n    """"""\n\n    name = ""conv1d"" if name is None else name\n    with tf.variable_scope(name):\n        input_shape = cl.shape(inputs)\n        input_rank = len(input_shape)\n        activation_rank = len(activation.shape)\n        if input_rank != 5:\n            raise ValueError(\'Inputs to `conv1d` should have rank 5. Received input rank:\', str(input_rank))\n        if activation_rank != 3:\n            raise ValueError(\'Activation to `conv1d` should have rank 3. Received input shape:\', str(activation_rank))\n\n        if isinstance(kernel_size, int):\n            kernel_size = [1, kernel_size]\n        elif isinstance(kernel_size, (list, tuple)) and len(kernel_size) == 1:\n            kernel_size = [1, kernel_size[0]]\n        else:\n            raise ValueError(\'""kernel_size"" should be an integer or tuple/list of 2 integers. Received:\', str(kernel_size))\n\n        if isinstance(stride, int):\n            strides = [1, stride]\n        elif isinstance(stride, (list, tuple)) and len(stride) == 1:\n            strides = [1, stride[0]]\n        else:\n            raise ValueError(\'""stride"" should be an integer or tuple/list of a single integer. Received:\', str(stride))\n\n        if not isinstance(out_caps_dims, (list, tuple)) or len(out_caps_dims) != 2:\n            raise ValueError(\'""out_caps_dims"" should be a tuple/list of 2 integers. Received:\', str(out_caps_dims))\n        elif isinstance(out_caps_dims, tuple):\n            out_caps_dims = list(out_caps_dims)\n\n        inputs = tf.expand_dims(inputs, axis=1)\n        activation = tf.expand_dims(activation, axis=1)\n        pose, activation = conv2d(inputs,\n                                  activation,\n                                  filters=filters,\n                                  out_caps_dims=out_caps_dims,\n                                  kernel_size=kernel_size,\n                                  strides=strides,\n                                  padding=padding,\n                                  routing_method=routing_method,\n                                  name=""convolution"",\n                                  reuse=reuse)\n        pose = tf.squeeze(pose, axis=1)\n        activation = tf.squeeze(activation, axis=1)\n\n    return pose, activation\n'"
capslayer/layers/convolutional_test.py,9,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport capslayer as cl\nimport tensorflow as tf\n\n\ndef testConv2d():\n    shape = [1, 9, 9, 9, 1, 1]\n    input_tensor = tf.random_normal(shape=shape, stddev=5, seed=2018)\n    activation = tf.random_normal(shape=shape[:4], mean=0.5, stddev=0.5, seed=2018)\n    activation = tf.clip_by_value(activation, 0., 0.999)\n    conv_cl, activation = cl.layers.conv2d(input_tensor,\n                                           activation,\n                                           filters=1,\n                                           kernel_size=3,\n                                           strides=1,\n                                           out_caps_dims=[1, 1])\n    input_tensor = tf.squeeze(input_tensor, axis=(-2, -1))\n    conv_tf = tf.layers.conv2d(input_tensor, filters=1, kernel_size=3, strides=1, use_bias=False)\n\n    return tf.squeeze(conv_cl, axis=(-2, -1)), conv_tf\n\n\nif __name__ == ""__main__"":\n    conv_cl, conv_tf = testConv2d()\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    init = tf.global_variables_initializer()\n    sess = tf.Session(config=config)\n    sess.run(init)\n    conv_cl = sess.run(conv_cl)\n    conv_tf = sess.run(conv_tf)\n    print(conv_cl)\n    # print(conv_tf)\n'"
capslayer/layers/layers.py,12,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\n""""""\nThis module provides a set of high-level capsule networks layers.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport capslayer as cl\nimport tensorflow as tf\n\nfrom capslayer.core import routing\nfrom capslayer.core import transforming\n\n\ndef dense(inputs, activation,\n          num_outputs,\n          out_caps_dims,\n          routing_method=\'EMRouting\',\n          coordinate_addition=False,\n          reuse=None,\n          name=None):\n    """"""A fully connected capsule layer.\n\n    Args:\n        inputs: A 4-D tensor with shape [batch_size, num_inputs] + in_caps_dims or [batch_size, in_height, in_width, in_channels] + in_caps_dims\n        activation: [batch_size, num_inputs] or [batch_size, in_height, in_width, in_channels]\n        num_outputs: Integer, the number of output capsules in the layer.\n        out_caps_dims: A list with two elements, pose shape of output capsules.\n\n    Returns:\n        pose: A 4-D tensor with shape [batch_size, num_outputs] + out_caps_dims\n        activation: [batch_size, num_outputs]\n    """"""\n    name = ""dense"" if name is None else name\n    with tf.variable_scope(name) as scope:\n        if reuse:\n            scope.reuse()\n        if coordinate_addition and len(inputs.shape) == 6 and len(activation.shape) == 4:\n            vote = transforming(inputs, num_outputs=num_outputs, out_caps_dims=out_caps_dims)\n            with tf.name_scope(""coodinate_addition""):\n                batch_size, in_height, in_width, in_channels, _, out_caps_height, out_caps_width = cl.shape(vote)\n                num_inputs = in_height * in_width * in_channels\n\n                zeros = np.zeros((in_height, out_caps_width - 1))\n                coord_offset_h = ((np.arange(in_height) + 0.5) / in_height).reshape([in_height, 1])\n                coord_offset_h = np.concatenate([zeros, coord_offset_h], axis=-1)\n                zeros = np.zeros((out_caps_height - 1, out_caps_width))\n                coord_offset_h = np.stack([np.concatenate([coord_offset_h[i:(i + 1), :], zeros], axis=0) for i in range(in_height)], axis=0)\n                coord_offset_h = coord_offset_h.reshape((1, in_height, 1, 1, 1, out_caps_height, out_caps_width))\n\n                zeros = np.zeros((1, in_width))\n                coord_offset_w = ((np.arange(in_width) + 0.5) / in_width).reshape([1, in_width])\n                coord_offset_w = np.concatenate([zeros, coord_offset_w, zeros, zeros], axis=0)\n                zeros = np.zeros((out_caps_height, out_caps_width - 1))\n                coord_offset_w = np.stack([np.concatenate([zeros, coord_offset_w[:, i:(i + 1)]], axis=1) for i in range(in_width)], axis=0)\n                coord_offset_w = coord_offset_w.reshape((1, 1, in_width, 1, 1, out_caps_height, out_caps_width))\n\n                vote = vote + tf.constant(coord_offset_h + coord_offset_w, dtype=tf.float32)\n\n                vote = tf.reshape(vote, shape=[batch_size, num_inputs, num_outputs] + out_caps_dims)\n                activation = tf.reshape(activation, shape=[batch_size, num_inputs])\n\n        elif len(inputs.shape) == 4 and len(activation.shape) == 2:\n            vote = transforming(inputs, num_outputs=num_outputs, out_caps_dims=out_caps_dims)\n\n        else:\n            raise TypeError(""Wrong rank for inputs or activation"")\n\n        pose, activation = routing(vote, activation, routing_method)\n        # pose, activation = cl.core.gluing(vote, activation)\n        assert len(pose.shape) == 4\n        assert len(activation.shape) == 2\n\n    return(pose, activation)\n\n\ndef primaryCaps(inputs, filters,\n                kernel_size,\n                strides,\n                out_caps_dims,\n                method=None,\n                name=None):\n    \'\'\'Primary capsule layer.\n\n    Args:\n        inputs: [batch_size, in_height, in_width, in_channels].\n        filters: Integer, the dimensionality of the output space.\n        kernel_size: kernel_size\n        strides: strides\n        out_caps_dims: A list of 2 integers.\n        method: the method of calculating probability of entity existence(logistic, norm, None)\n\n    Returns:\n        pose: A 6-D tensor, [batch_size, out_height, out_width, filters] + out_caps_dims\n        activation: A 4-D tensor, [batch_size, out_height, out_width, filters]\n    \'\'\'\n\n    name = ""primary_capsule"" if name is None else name\n    with tf.variable_scope(name):\n        channels = filters * np.prod(out_caps_dims)\n        channels = channels + filters if method == ""logistic"" else channels\n        pose = tf.layers.conv2d(inputs, channels,\n                                kernel_size=kernel_size,\n                                strides=strides, activation=None)\n        shape = cl.shape(pose, name=""get_pose_shape"")\n        batch_size = shape[0]\n        height = shape[1]\n        width = shape[2]\n        shape = [batch_size, height, width, filters] + out_caps_dims\n\n        if method == \'logistic\':\n            # logistic activation unit\n            pose, activation_logit = tf.split(pose, [channels - filters, filters], axis=-1)\n            pose = tf.reshape(pose, shape=shape)\n            activation = tf.sigmoid(activation_logit)\n        elif method == \'norm\' or method is None:\n            pose = tf.reshape(pose, shape=shape)\n            squash_on = -2 if out_caps_dims[-1] == 1 else [-2, -1]\n            pose = cl.ops.squash(pose, axis=squash_on)\n            activation = cl.norm(pose, axis=(-2, -1))\n        activation = tf.clip_by_value(activation, 1e-20, 1. - 1e-20)\n\n        return(pose, activation)\n'"
capslayer/ops/__init__.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .losses import losses\nfrom .ops import squash\n\n__all__ = [\'losses\', \'squash\']\n'"
capslayer/ops/math_ops.py,37,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport capslayer as cl\nimport tensorflow as tf\n\n\ndef matmul(a, b,\n           transpose_a=False,\n           transpose_b=False,\n           name=None):\n    name = ""matmul"" if name is None else name\n    with tf.name_scope(name):\n        if transpose_a:\n            rank_a = len(a.shape)\n            if rank_a < 2:\n                raise TypeError(""Rank must be greater than 2"")\n            perm = [i for i in range(rank_a - 2)]\n            perm = perm + [rank_a - 1, rank_a - 2]\n            a = tf.transpose(a, perm=perm)\n        if transpose_b:\n            rank_b = len(b.shape)\n            if rank_b < 2:\n                raise TypeError(""Rank must be greater than 2"")\n            perm = [i for i in range(rank_b - 2)]\n            perm = perm + [rank_b - 1, rank_b - 2]\n            b = tf.transpose(b, perm=perm)\n        a = tf.expand_dims(a, axis=-1)\n        b = tf.expand_dims(b, axis=-3)\n        c = tf.reduce_sum(a * b, axis=-2)\n        return c\n\n\n# Memory consumed!\ndef matmul_v1(a, b,\n              transpose_a=False,\n              transpose_b=False,\n              name=None):\n    name = ""matmul"" if name is None else name\n    with tf.name_scope(name):\n        rank_a = len(a.shape)\n        rank_b = len(b.shape)\n        if rank_a < 2 or rank_b < 2:\n            raise TypeError(""Rank must be greater than 2"")\n        perm_a = [i for i in range(rank_a - 2)]\n        perm_b = [i for i in range(rank_b - 2)]\n\n        if transpose_a:\n            perm = [rank_a - 1] + perm_a + [rank_a - 2]\n        else:\n            perm = [rank_a - 2] + perm_a + [rank_a - 1]\n        a = tf.transpose(a, perm=perm)\n\n        if transpose_b:\n            perm = [rank_b - 2] + perm_b + [rank_b - 1]\n        else:\n            perm = [rank_b - 1] + perm_b + [rank_b - 2]\n        b = tf.transpose(b, perm=perm)\n\n        C = []\n        for i in range(a.get_shape()[0].value):\n            B = []\n            for j in range(b.get_shape()[0].value):\n                k = tf.reduce_sum(a[i] * b[j], axis=-1, keepdims=True)\n                B.append(k)\n            C.append(tf.expand_dims(tf.concat(B, axis=-1), axis=-2))\n        C = tf.concat(C, axis=-2)\n        return(C)\n\n\ndef matmul_v2(a, b,\n              transpose_a=False,\n              transpose_b=False,\n              name=None):\n    name = ""matmul"" if name is None else name\n    with tf.name_scope(name):\n        rank_a = len(a.shape)\n        rank_b = len(b.shape)\n        if rank_a < 2 or rank_b < 2:\n            raise TypeError(""Rank must be greater than 2"")\n        if transpose_a:\n            perm = [i for i in range(rank_a - 2)]\n            perm = perm + [rank_a - 1, rank_a - 2]\n            a = tf.transpose(a, perm=perm)\n        if transpose_b:\n            perm = [i for i in range(rank_b - 2)]\n            perm = perm + [rank_b - 1, rank_b - 2]\n            b = tf.transpose(b, perm=perm)\n\n        b = tf.tile(b, [1 for i in range(rank_b - 2)] + [cl.shape(a)[-2], 1])\n        shape = cl.shape(a)[:-2] + [np.prod(cl.shape(a)[-2:]), 1]\n        a_prime = tf.reshape(a, shape=shape)\n        c = a_prime * b\n        shape = cl.shape(a) + cl.shape(b)[-1:]\n        c = tf.reshape(c, shape=shape)\n        c = tf.reduce_sum(c, axis=-2)\n\n    return c\n\n\ndef norm(tensor, ord=\'euclidean\', axis=None, keepdims=None, name=None):\n    try:\n        return tf.norm(tensor, ord=ord, axis=axis, keepdims=keepdims, name=name)\n    except:\n        return tf.norm(tensor, ord=ord, axis=axis, keep_dims=keepdims, name=name)\n\n\ndef reduce_sum(input_tensor,\n               axis=None,\n               keepdims=None,\n               name=None):\n    try:\n        return tf.reduce_sum(input_tensor, axis=axis, keepdims=keepdims, name=name)\n    except:\n        return tf.reduce_sum(input_tensor, axis=axis, keep_dims=keepdims, name=name)\n\n\ndef divide(x, y, safe_mode=True, epsilon=None, name=None):\n    """""" A wrapper of `tf.divide`, computes Python style division of x by y but extends safe divide support.\n        If safe_mode is `True` or epsilon is given(a small float number), the absolute value of denominator\n        in the division will be clip to make sure it\'s bigger than epsilon(default is 1e-13).\n\n    Args:\n        safe_mode: Use safe divide mode.\n        epsilon: Float number. Default is `1e-13`.\n    """"""\n    if not safe_mode and epsilon is None:\n        return tf.divide(x, y, name=name)\n    else:\n        epsilon = 1e-20 if epsilon is None else epsilon\n        name = ""safe_divide"" if name is None else name\n        with tf.name_scope(name):\n            y = tf.where(tf.greater(tf.abs(y), epsilon), y, y + tf.sign(y) * epsilon)\n            return tf.divide(x, y)\n\n\ndef log(x, epsilon=1e-20, name=None):\n    """""" A wrapper of `tf.log`, computes natural logarithm of x element-wise but extends safe log support.\n        If epsilon is given as a positive float, x will be clipped to bigger than epsilon before doing computing.\n    """"""\n    if isinstance(epsilon, float) and epsilon > 0:\n        return tf.log(tf.maximum(x, epsilon), name=name)\n    else:\n        return tf.log(x, name=name)\n\n\nif __name__ == ""__main__"":\n    a = tf.constant([[[1, 2], [3, 4], [5, 6]]], dtype=tf.float32)\n    b = tf.constant([[[7, 8, 9], [10, 11, 12], [13, 14, 15]]],\n                    dtype=tf.float32)\n    c1 = tf.matmul(a, b, transpose_a=True, transpose_b=True)\n    c2 = matmul_v2(a, b, transpose_a=True, transpose_b=True)\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n    print(sess.run(c1))\n    print(sess.run(c2))\n'"
capslayer/ops/nn_ops.py,17,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport capslayer as cl\nimport numpy as np\n\n\ndef space_to_batch_nd(input, kernel_size, strides, name=None):\n    """""" Space to batch with strides. Different to tf.space_to_batch_nd.\n        for convCapsNet model: memory 4729M, speed 0.165 sec/step, similiar to space_to_batch_nd_v1\n\n    Args:\n        input: A Tensor. N-D with shape input_shape = [batch] + spatial_shape + remaining_shape, where spatial_shape has M dimensions.\n        kernel_size: A sequence of len(spatial_shape)-D positive integers specifying the spatial dimensions of the filters.\n        strides: A sequence of len(spatial_shape)-D positive integers specifying the stride at which to compute output.\n\n    Returns:\n        A Tensor.\n    """"""\n    assert len(kernel_size) == 3\n    assert len(strides) == 3\n    name = ""space_to_batch_nd"" if name is None else name\n    with tf.name_scope(name):\n        input_shape = cl.shape(input)\n        h_steps = int((input_shape[1] - kernel_size[0]) / strides[0] + 1)\n        w_steps = int((input_shape[2] - kernel_size[1]) / strides[1] + 1)\n        d_steps = int((input_shape[3] - kernel_size[2]) / strides[2] + 1)\n        blocks = []  # each element with shape [batch, h_kernel_size * w_kernel_size * d_kernel_size] + remaining_shape\n        for d in range(d_steps):\n            d_s = d * strides[2]\n            d_e = d_s + kernel_size[2]\n            h_blocks = []\n            for h in range(h_steps):\n                h_s = h * strides[0]\n                h_e = h_s + kernel_size[0]\n                w_blocks = []\n                for w in range(w_steps):\n                    w_s = w * strides[1]\n                    w_e = w_s + kernel_size[1]\n                    block = input[:, h_s:h_e, w_s:w_e, d_s:d_e]\n                    # block = tf.reshape(block, shape=[tf.shape(input)[0], np.prod(kernel_size)] + input_shape[4:])\n                    w_blocks.append(block)\n                h_blocks.append(tf.concat(w_blocks, axis=2))\n            blocks.append(tf.concat(h_blocks, axis=1))\n        return tf.concat(blocks, axis=0)\n\n\ndef space_to_batch_nd_v1(inputs, kernel_size, strides, name=None):\n    """""" for convCapsNet model: memory 4719M, speed 0.169 sec/step\n    """"""\n    name = ""space_to_batch_nd"" if name is None else name\n    with tf.name_scope(name):\n        height, width, depth = cl.shape(inputs)[1:4]\n        h_offsets = [[(h + k) for k in range(0, kernel_size[0])] for h in range(0, height + 1 - kernel_size[0], strides[0])]\n        w_offsets = [[(w + k) for k in range(0, kernel_size[1])] for w in range(0, width + 1 - kernel_size[1], strides[1])]\n        d_offsets = [[(d + k) for k in range(0, kernel_size[2])] for d in range(0, depth + 1 - kernel_size[2], strides[2])]\n        patched = tf.gather(inputs, h_offsets, axis=1)\n        patched = tf.gather(patched, w_offsets, axis=3)\n        patched = tf.gather(patched, d_offsets, axis=5)\n\n        if len(patched.shape) == 7:\n            perm = [0, 1, 3, 5, 2, 4, 6]\n        else:\n            perm = [0, 1, 3, 5, 2, 4, 6, 7, 8]\n\n        patched = tf.transpose(patched, perm=perm)\n        shape = cl.shape(patched)\n\n        if depth == kernel_size[2]:   # for conv2d\n            shape = shape[:3] + [np.prod(shape[3:-2])] + shape[-2:] if len(patched.shape) == 9 else shape[:3] + [np.prod(shape[3:])]\n        else:                         # for conv3d\n            shape = shape[:4] + [np.prod(shape[4:-2])] + shape[-2:] if len(patched.shape) == 9 else shape[:4] + [np.prod(shape[4:])]\n\n        patched = tf.reshape(patched, shape=shape)\n    return patched\n\n\ndef batch_to_space_nd(input, spatial_shape, name=None):\n    name = ""batch_to_space_nd"" if name is None else name\n    with tf.name_scope(name):\n        input_shape = cl.shape(input)\n        shape = [-1] + spatial_shape + input_shape[1:]\n        return tf.reshape(input, shape=shape)\n\n\ndef softmax(logits, axis=None, name=None):\n    name = ""Softmax"" if name is None else name\n    with tf.name_scope(name):\n        if axis < 0:\n            axis = len(logits.shape) + axis\n        try:\n            return tf.nn.softmax(logits, axis=axis)\n        except:\n            return tf.nn.softmax(logits, dim=axis)\n'"
capslayer/ops/nn_ops_test.py,5,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nn_ops import space_to_batch_nd_v1\nfrom nn_ops import space_to_batch_nd\nfrom nn_ops import batch_to_space_nd\n\n\ndef testSpaceToBatch():\n    shape = [1, 9, 9, 9]\n    input_tensor = tf.reshape(tf.range(np.prod(shape), dtype=tf.float32), shape=shape)\n    return space_to_batch_nd(input_tensor, kernel_size=(3, 3, 3), strides=(1, 1, 1))\n\n\ndef testSpaceToBatch_v1():\n    shape = [1, 9, 9, 9]\n    input_tensor = tf.reshape(tf.range(np.prod(shape), dtype=tf.float32), shape=shape)\n    return space_to_batch_nd_v1(input_tensor, kernel_size=(3, 3, 3), strides=(1, 1, 1))\n\n\ndef testBatchToSpace(input):\n    return batch_to_space_nd(input, spatial_shape=[7, 7, 7])\n\nif __name__ == ""__main__"":\n    transfered = testSpaceToBatch()\n    transfered_v1 = testSpaceToBatch_v1()\n    out = testBatchToSpace(tf.reduce_mean(transfered, axis=-1))\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session()\n    out1 = sess.run(transfered)\n    out2 = sess.run(transfered_v1)\n    # out2 = sess.run(out)\n    print(out1[2, :])\n    print(out1[2, :])\n    # print(out1[1, 18:])\n    # print(out2[0])\n    # print(np.mean(out1[1]))\n    # print(out2)\n    # print(out)\n'"
capslayer/ops/ops.py,4,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport capslayer as cl\nimport tensorflow as tf\n\n\ndef squash(inputs, axis=-2, ord=""euclidean"", name=None):\n    """"""Squashing function.\n\n    Args:\n        inputs: A tensor with shape [batch_size, 1, num_caps, vec_len, 1] or [batch_size, num_caps, vec_len, 1]\n        ord: Order of the norm. Supported values are \'fro\', \'euclidean\', 1, 2, np.inf and any positive real number yielding the corresponding p-norm. Default is \'euclidean\' which is equivalent to Frobenius norm if tensor is a matrix and equivalent to 2-norm for vectors.\n\n    Returns:\n        A tensor with the same shape as inputs but squashed in `axis` dimension.\n    """"""\n\n    name = ""squashing"" if name is None else name\n    with tf.name_scope(name):\n        norm = cl.norm(inputs, ord=ord, axis=axis, keepdims=True)\n        norm_squared = tf.square(norm)\n        scalar_factor = norm_squared / (1 + norm_squared)\n        return scalar_factor * (inputs / norm)\n\n\ndef shape(inputs, name=None):\n    name = ""shape"" if name is None else name\n    with tf.name_scope(name):\n        static_shape = inputs.get_shape().as_list()\n        dynamic_shape = tf.shape(inputs)\n        shape = []\n        for i, dim in enumerate(static_shape):\n            dim = dim if dim is not None else dynamic_shape[i]\n            shape.append(dim)\n        return(shape)\n'"
capslayer/plotlib/__init__.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .figure import plot_activation\n'"
capslayer/plotlib/figure.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom matplotlib import pyplot as plt\nfrom matplotlib import ticker\n\n\ndef plot_activation(matrix, step, save_to=None):\n    save_to = os.path.join(""."", ""activations"") if save_to is None else save_to\n    os.makedirs(save_to, exist_ok=True)\n    if len(matrix.shape) != 2:\n        raise ValueError(\'Input ""matrix"" should have 2 rank, but it is\',str(len(matrix.shape)))\n    num_label = matrix.shape[1] - 1\n    matrix = matrix[matrix[:, num_label].argsort()]\n    fig, axes = plt.subplots(ncols=1, nrows=num_label, figsize=(15,12))\n    fig.suptitle(""The probability of entity presence (step %s)""%str(step), fontsize=20)\n    fig.tight_layout()\n    for i, ax in enumerate(axes.flatten()):\n        idx = num_label - (i + 1)\n        ax.spines[\'top\'].set_color(\'none\')\n        ax.spines[\'bottom\'].set_color(\'none\')\n        ax.set_ylim(0, 1.05)\n        ax.set_ylabel(""Capsule "" + str(idx))\n        ax.yaxis.set_major_locator(ticker.NullLocator())\n        if idx > 0:\n            ax.xaxis.set_major_locator(ticker.NullLocator())\n        else:\n            ax.xaxis.set_major_locator(ticker.IndexLocator(base=500,offset=0))\n            ax.set_xlabel(""Sample index "")\n        ax.plot(matrix[:,idx])\n        ax_prime = ax.twinx()\n        ax_prime.spines[\'top\'].set_color(\'none\')\n        ax_prime.spines[\'bottom\'].set_color(\'none\')\n    plt.subplots_adjust(hspace=0.2, left=0.05, right=0.95, bottom=0.05, top=.95)\n    plt.savefig(os.path.join(save_to, ""activation_%s.png"" % str(step)))\n    plt.close()\n'"
capslayer/summary/__init__.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .summary import image\nfrom .summary import scalar\nfrom .summary import histogram\nfrom .summary import tensor_stats\n\n\n__all__ = [""image"", ""scalar"", ""histogram"", ""tensor_stats""]\n'"
capslayer/summary/summary.py,12,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef image(name,\n          tensor,\n          verbose=True,\n          max_outputs=3,\n          collections=None,\n          family=None):\n    if verbose:\n        tf.summary.image(name, tensor, max_outputs, collections, family)\n    else:\n        pass\n\n\ndef scalar(name, tensor, verbose=False, collections=None, family=None):\n    if verbose:\n        tf.summary.scalar(name, tensor, collections, family)\n    else:\n        pass\n\n\ndef histogram(name, values, verbose=False, collections=None, family=None):\n    if verbose:\n        tf.summary.histogram(name, values, collections, family)\n    else:\n        pass\n\n\ndef tensor_stats(name, tensor, verbose=True, collections=None, family=None):\n    """"""\n    Args:\n        tensor: A non-scalar tensor.\n    """"""\n    if verbose:\n        with tf.name_scope(name):\n            mean = tf.reduce_mean(tensor)\n            tf.summary.scalar(\'mean\', mean, collections=collections, family=family)\n\n            with tf.name_scope(\'stddev\'):\n                stddev = tf.sqrt(tf.reduce_mean(tf.square(tensor - mean)))\n            tf.summary.scalar(\'stddev\', stddev, collections=collections, family=family)\n            tf.summary.scalar(\'max\', tf.reduce_max(tensor), collections=collections, family=family)\n            tf.summary.scalar(\'min\', tf.reduce_min(tensor), collections=collections, family=family)\n            tf.summary.histogram(\'histogram\', tensor, collections=collections, family=family)\n    else:\n        pass\n'"
models/tools/download_data.py,0,"b'import os\nimport sys\nimport gzip\nimport shutil\nfrom six.moves import urllib\n\n# mnist dataset\nHOMEPAGE = ""http://yann.lecun.com/exdb/mnist/""\nMNIST_TRAIN_IMGS_URL = HOMEPAGE + ""train-images-idx3-ubyte.gz""\nMNIST_TRAIN_LABELS_URL = HOMEPAGE + ""train-labels-idx1-ubyte.gz""\nMNIST_TEST_IMGS_URL = HOMEPAGE + ""t10k-images-idx3-ubyte.gz""\nMNIST_TEST_LABELS_URL = HOMEPAGE + ""t10k-labels-idx1-ubyte.gz""\n\n# fashion-mnist dataset\nHOMEPAGE = ""http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/""\nFASHION_MNIST_TRAIN_IMGS_URL = HOMEPAGE + ""train-images-idx3-ubyte.gz""\nFASHION_MNIST_TRAIN_LABELS_URL = HOMEPAGE + ""train-labels-idx1-ubyte.gz""\nFASHION_MNIST_TEST_IMGS_URL = HOMEPAGE + ""t10k-images-idx3-ubyte.gz""\nFASHION_MNIST_TEST_LABELS_URL = HOMEPAGE + ""t10k-labels-idx1-ubyte.gz""\n\n# smallNORB dataset\nHOMEPAGE = ""https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/""\nSMALLNORB_TRAIN_DAT_URL = HOMEPAGE + ""smallnorb-5x46789x9x18x6x2x96x96-training-dat.mat.gz""\nSMALLNORB_TRAIN_CAT_URL = HOMEPAGE + ""smallnorb-5x46789x9x18x6x2x96x96-training-cat.mat.gz""\nSMALLNORB_TRAIN_INFO_URL = HOMEPAGE + ""smallnorb-5x46789x9x18x6x2x96x96-training-info.mat.gz""\nSMALLNORB_TEST_DAT_URL = HOMEPAGE + ""smallnorb-5x01235x9x18x6x2x96x96-testing-dat.mat.gz""\nSMALLNORB_TEST_CAT_URL = HOMEPAGE + ""smallnorb-5x01235x9x18x6x2x96x96-testing-cat.mat.gz""\nSMALLNORB_TEST_INFO_URL = HOMEPAGE + ""smallnorb-5x01235x9x18x6x2x96x96-testing-info.mat.gz""\n\n\ndef download_and_uncompress_zip(URL, dataset_dir, force=False):\n    \'\'\'\n    Args:\n        URL: the download links for data\n        dataset_dir: the path to save data\n        force: redownload data\n    \'\'\'\n    filename = URL.split(\'/\')[-1]\n    filepath = os.path.join(dataset_dir, filename)\n    if not os.path.exists(dataset_dir):\n        os.mkdir(dataset_dir)\n    extract_to = os.path.splitext(filepath)[0]\n\n    def download_progress(count, block_size, total_size):\n        sys.stdout.write(""\\r>> Downloading %s %.1f%%"" % (filename, float(count * block_size) / float(total_size) * 100.))\n        sys.stdout.flush()\n\n    if not force and os.path.exists(filepath):\n        print(""file %s already exist"" % (filename))\n    else:\n        filepath, _ = urllib.request.urlretrieve(URL, filepath, download_progress)\n        print()\n        print(\'Successfully Downloaded\', filename)\n\n    # with zipfile.ZipFile(filepath) as fd:\n    with gzip.open(filepath, \'rb\') as f_in, open(extract_to, \'wb\') as f_out:\n        print(\'Extracting \', filename)\n        shutil.copyfileobj(f_in, f_out)\n        print(\'Successfully extracted\')\n        print()\n\n\ndef start_download(dataset, save_to, force):\n    if dataset == \'mnist\':\n        print(""Start downloading dataset MNIST:"")\n        download_and_uncompress_zip(MNIST_TRAIN_IMGS_URL, save_to, force)\n        download_and_uncompress_zip(MNIST_TRAIN_LABELS_URL, save_to, force)\n        download_and_uncompress_zip(MNIST_TEST_IMGS_URL, save_to, force)\n        download_and_uncompress_zip(MNIST_TEST_LABELS_URL, save_to, force)\n    elif dataset == \'fashion-mnist\':\n        print(""Start downloading dataset Fashion MNIST:"")\n        download_and_uncompress_zip(FASHION_MNIST_TRAIN_IMGS_URL, save_to, force)\n        download_and_uncompress_zip(FASHION_MNIST_TRAIN_LABELS_URL, save_to, force)\n        download_and_uncompress_zip(FASHION_MNIST_TEST_IMGS_URL, save_to, force)\n        download_and_uncompress_zip(FASHION_MNIST_TEST_LABELS_URL, save_to, force)\n    elif dataset == \'smallNORB\':\n        print(""Start downloading dataset small NORB:"")\n        download_and_uncompress_zip(SMALLNORB_TRAIN_DAT_URL, save_to, force)\n        download_and_uncompress_zip(SMALLNORB_TRAIN_CAT_URL, save_to, force)\n        download_and_uncompress_zip(SMALLNORB_TRAIN_INFO_URL, save_to, force)\n        download_and_uncompress_zip(SMALLNORB_TEST_DAT_URL, save_to, force)\n        download_and_uncompress_zip(SMALLNORB_TEST_CAT_URL, save_to, force)\n        download_and_uncompress_zip(SMALLNORB_TEST_INFO_URL, save_to, force)\n    else:\n        raise Exception(""Invalid dataset name! please check it: "", dataset)\n\nif __name__ == \'__main__\':\n    import argparse\n    parser = argparse.ArgumentParser(\'Script for automatically downloading datasets\')\n    parser.add_argument(""--dataset"", default=\'mnist\', choices=[\'mnist\', \'fashion-mnist\', \'smallNORB\'])\n    parser.add_argument(""--save_to"", default=\'models/data/mnist\')\n    parser.add_argument(""--force"", default=False, type=bool)\n    args = parser.parse_args()\n    start_download(args.dataset, args.save_to, args.force)\n'"
capslayer/data/datasets/__init__.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom . import mnist\nfrom . import fashion_mnist\nfrom . import cifar10\nfrom . import cifar100\n\n\n__all__ = [\'mnist\', \'fashion_mnist\', \'cifar10\', \'cifar100\']\n'"
capslayer/data/utils/TFRecordHelper.py,2,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef int64_feature(value):\n    """"""Casts value to a TensorFlow int64 feature list.""""""\n    if not isinstance(value, (tuple, list)):\n        value = [value]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef bytes_feature(value):\n    """"""Casts value to a TensorFlow bytes feature list.""""""\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n'"
capslayer/data/utils/__init__.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n'"
capslayer/data/utils/download_utils.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport gzip\nimport shutil\nfrom six.moves import urllib\n\n# mnist dataset\nHOMEPAGE = ""http://yann.lecun.com/exdb/mnist/""\nMNIST_TRAIN_IMGS_URL = HOMEPAGE + ""train-images-idx3-ubyte.gz""\nMNIST_TRAIN_LABELS_URL = HOMEPAGE + ""train-labels-idx1-ubyte.gz""\nMNIST_TEST_IMGS_URL = HOMEPAGE + ""t10k-images-idx3-ubyte.gz""\nMNIST_TEST_LABELS_URL = HOMEPAGE + ""t10k-labels-idx1-ubyte.gz""\n\n# fashion-mnist dataset\nHOMEPAGE = ""http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/""\nFASHION_MNIST_TRAIN_IMGS_URL = HOMEPAGE + ""train-images-idx3-ubyte.gz""\nFASHION_MNIST_TRAIN_LABELS_URL = HOMEPAGE + ""train-labels-idx1-ubyte.gz""\nFASHION_MNIST_TEST_IMGS_URL = HOMEPAGE + ""t10k-images-idx3-ubyte.gz""\nFASHION_MNIST_TEST_LABELS_URL = HOMEPAGE + ""t10k-labels-idx1-ubyte.gz""\n\n# smallNORB dataset\nHOMEPAGE = ""https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/""\nSMALLNORB_TRAIN_DAT_URL = HOMEPAGE + ""smallnorb-5x46789x9x18x6x2x96x96-training-dat.mat.gz""\nSMALLNORB_TRAIN_CAT_URL = HOMEPAGE + ""smallnorb-5x46789x9x18x6x2x96x96-training-cat.mat.gz""\nSMALLNORB_TRAIN_INFO_URL = HOMEPAGE + ""smallnorb-5x46789x9x18x6x2x96x96-training-info.mat.gz""\nSMALLNORB_TEST_DAT_URL = HOMEPAGE + ""smallnorb-5x01235x9x18x6x2x96x96-testing-dat.mat.gz""\nSMALLNORB_TEST_CAT_URL = HOMEPAGE + ""smallnorb-5x01235x9x18x6x2x96x96-testing-cat.mat.gz""\nSMALLNORB_TEST_INFO_URL = HOMEPAGE + ""smallnorb-5x01235x9x18x6x2x96x96-testing-info.mat.gz""\n\n# CIFAR-10 dataset\nHOMEPAGE = ""http://www.cs.toronto.edu/~kriz/""\nCIFAR_10_URL = ""http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz""\nCIFAR_10_BIN_URL = ""http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz""\n\n# CIFAR-100 dataset\nHOMEPAGE = ""http://www.cs.toronto.edu/~kriz/""\nCIFAR_100_URL = ""http://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz""\nCIFAR_100_BIN_URL = ""http://www.cs.toronto.edu/~kriz/cifar-100-binary.tar.gz""\n\n\ndef download_and_uncompress_zip(URL, dataset_dir, force=False):\n    \'\'\'\n    Args:\n        URL: the download links for data\n        dataset_dir: the path to save data\n        force: redownload data\n    \'\'\'\n    filename = URL.split(\'/\')[-1]\n    filepath = os.path.join(dataset_dir, filename)\n    if not os.path.exists(dataset_dir):\n        os.makedirs(dataset_dir)\n    extract_to = os.path.splitext(filepath)[0]\n\n    def download_progress(count, block_size, total_size):\n        sys.stdout.write(""\\r>> Downloading %s %.1f%%"" % (filename, float(count * block_size) / float(total_size) * 100.))\n        sys.stdout.flush()\n\n    if not force and os.path.exists(filepath):\n        print(""file %s already exist"" % (filename))\n        return 0\n    else:\n        filepath, _ = urllib.request.urlretrieve(URL, filepath, download_progress)\n        print()\n        print(\'Successfully Downloaded\', filename)\n\n    # with zipfile.ZipFile(filepath) as fd:\n    with gzip.open(filepath, \'rb\') as f_in, open(extract_to, \'wb\') as f_out:\n        print(\'Extracting \', filename)\n        shutil.copyfileobj(f_in, f_out)\n        print(\'Successfully extracted\')\n        print()\n\n\ndef maybe_download_and_extract(dataset, save_to, force=False):\n    if dataset == \'mnist\':\n        print(""Start downloading dataset MNIST:"")\n        download_and_uncompress_zip(MNIST_TRAIN_IMGS_URL, save_to, force)\n        download_and_uncompress_zip(MNIST_TRAIN_LABELS_URL, save_to, force)\n        download_and_uncompress_zip(MNIST_TEST_IMGS_URL, save_to, force)\n        download_and_uncompress_zip(MNIST_TEST_LABELS_URL, save_to, force)\n    elif dataset == \'fashion-mnist\' or dataset == \'fashion_mnist\':\n        print(""Start downloading dataset Fashion MNIST:"")\n        download_and_uncompress_zip(FASHION_MNIST_TRAIN_IMGS_URL, save_to, force)\n        download_and_uncompress_zip(FASHION_MNIST_TRAIN_LABELS_URL, save_to, force)\n        download_and_uncompress_zip(FASHION_MNIST_TEST_IMGS_URL, save_to, force)\n        download_and_uncompress_zip(FASHION_MNIST_TEST_LABELS_URL, save_to, force)\n    elif dataset == \'smallNORB\':\n        print(""Start downloading dataset small NORB:"")\n        download_and_uncompress_zip(SMALLNORB_TRAIN_DAT_URL, save_to, force)\n        download_and_uncompress_zip(SMALLNORB_TRAIN_CAT_URL, save_to, force)\n        download_and_uncompress_zip(SMALLNORB_TRAIN_INFO_URL, save_to, force)\n        download_and_uncompress_zip(SMALLNORB_TEST_DAT_URL, save_to, force)\n        download_and_uncompress_zip(SMALLNORB_TEST_CAT_URL, save_to, force)\n        download_and_uncompress_zip(SMALLNORB_TEST_INFO_URL, save_to, force)\n    elif dataset == \'cifar-10\' or dataset == \'cifar10\' or dataset == ""cifar_10"":\n        download_and_uncompress_zip(CIFAR_10_BIN_URL, save_to, force)\n    elif dataset == \'cifar-100\' or dataset == \'cifar100\' or dataset == ""cifar_100"":\n        download_and_uncompress_zip(CIFAR_100_URL, save_to, force)\n    else:\n        raise Exception(""Invalid dataset name! please check it: "", dataset)\n\n\nif __name__ == \'__main__\':\n    import argparse\n    parser = argparse.ArgumentParser(\'Script for automatically downloading datasets\')\n    parser.add_argument(""--dataset"", default=\'mnist\', choices=[\'mnist\', \'fashion-mnist\', \'smallNORB\', \'cifar-10\', \'cifar-100\'])\n    parser.add_argument(""--save_to"", default=\'models/data/mnist\')\n    parser.add_argument(""--force"", default=False, type=bool)\n    args = parser.parse_args()\n    maybe_download_and_extract(args.dataset, args.save_to, args.force)\n'"
capslayer/ops/losses/__init__.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# from capslayer.ops.losses.losses import margin_loss\nfrom capslayer.ops.losses.losses import *\n\n__all__ =[\'margin_loss\', \'spread_loss\', \'cross_entropy\', \'fcm_loss\']\n# __all__ =[\'margin_loss\', \'losses\']\n'"
capslayer/ops/losses/losses.py,19,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport capslayer as cl\nimport tensorflow as tf\n\n\ndef spread_loss(labels, logits, margin, regularizer=None):\n    """"""\n    Args:\n        labels: [batch_size, num_label].\n        logits: [batch_size, num_label].\n        margin: Integer or 1-D Tensor.\n        regularizer: use regularization.\n\n    Returns:\n        loss: Spread loss.\n    """"""\n    a_target = cl.reduce_sum(labels * logits, axis=1, keepdims=True)\n    dist = (1 - labels) * margin - (a_target - logits)\n    dist = tf.pow(tf.maximum(0., dist), 2)\n    loss = tf.reduce_mean(tf.reduce_sum(dist, axis=-1))\n    if regularizer is not None:\n        regularizer = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n        loss += tf.reduce_mean(regularizer)\n    return(loss)\n\n\ndef margin_loss(labels,\n                logits,\n                upper_margin=0.9,\n                bottom_margin=0.1,\n                downweight=0.5):\n    """"""\n    Args:\n        labels: [batch_size, num_label].\n        logits: [batch_size, num_label].\n    """"""\n    positive_selctor = tf.cast(tf.less(logits, upper_margin), tf.float32)\n    positive_cost = positive_selctor * labels * tf.pow(logits - upper_margin, 2)\n\n    negative_selctor = tf.cast(tf.greater(logits, bottom_margin), tf.float32)\n    negative_cost = negative_selctor * (1 - labels) * tf.pow(logits - bottom_margin, 2)\n    loss = 0.5 * positive_cost + 0.5 * downweight * negative_cost\n    return tf.reduce_mean(tf.reduce_sum(loss, axis=-1))\n\n\ndef cross_entropy(labels, logits, regularizer=None):\n    \'\'\'\n    Args:\n        ...\n\n    Returns:\n        ...\n    \'\'\'\n    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n    if regularizer is not None:\n        regularizer = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n        loss += tf.reduce_mean(regularizer)\n    return(loss)\n\n\ndef fcm_loss(inputs, probs, num_iters=5, m=2):\n    """"""\n    Args:\n        inputs: [num_samples, 1, x_dims, 1]\n        probs: [num_samples, num_clusters, 1, 1].\n    """"""\n    # centers: [1, num_clusters, x_dims]\n    k = 1\n    b = 0\n    weights = []\n    delta_probs = []\n    for i in range(num_iters):\n        probs_m = tf.pow(probs, m)\n        centers = cl.reduce_sum(probs_m * inputs, axis=0, keepdims=True) / cl.reduce_sum(probs_m, axis=0, keepdims=True)\n        # distance matrix with shape [num_samples, num_clusters, 1, 1]\n        distance_matrix = cl.norm(inputs - centers, axis=(2, 3), keepdims=True)\n        distance_matrix = tf.pow(distance_matrix, 2 / (m - 1))\n        probs_plus = 1 / (distance_matrix / cl.reduce_sum(distance_matrix, axis=1, keepdims=True))\n        delta_probs.append(tf.norm(probs_plus - probs))\n        weights.append(tf.exp(tf.cast(k * i + b, tf.float32)))\n        probs = probs_plus\n\n    weights = tf.stack(weights, axis=0)\n    delta_probs = tf.stack(delta_probs, axis=0)\n    loss = tf.reduce_sum(weights * delta_probs) / tf.reduce_sum(weights)\n    return loss\n'"
capslayer/data/datasets/cifar10/__init__.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .reader import DataLoader\n\n__all__ = [""DataLoader""]\n'"
capslayer/data/datasets/cifar10/reader.py,11,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\n""""""Input utility functions for reading Cifar10 dataset.\n\nHandles reading from Cifar10 dataset saved in binary original format. Scales and\nnormalizes the images as the preprocessing step. It can distort the images by\nrandom cropping and contrast adjusting.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom capslayer.data.datasets.cifar10.writer import tfrecord_runner\n\n\ndef parse_fun(serialized_example):\n    """""" Data parsing function.\n    """"""\n    features = tf.parse_single_example(serialized_example,\n                                       features={\'image\': tf.FixedLenFeature([], tf.string),\n                                                 \'label\': tf.FixedLenFeature([], tf.int64)})\n    image = tf.decode_raw(features[\'image\'], tf.float32)\n    image = tf.reshape(image, shape=[32 * 32 * 3])\n    image.set_shape([32 * 32 * 3])\n    image = tf.cast(image, tf.float32) / 255. # * (2. / 255) - 1.\n    label = tf.cast(features[\'label\'], tf.int32)\n    features = {\'images\': image, \'labels\': label}\n    return(features)\n\n\nclass DataLoader(object):\n    """""" Data Loader.\n    """"""\n    def __init__(self, path=None,\n                 num_works=1,\n                 splitting=""TVT"",\n                 one_hot=False,\n                 name=""create_inputs""):\n        if path is None or not os.path.exists(path):\n            tfrecord_runner()\n            path = os.path.join(os.path.expanduser(\'~\'), "".capslayer"", ""datasets"", ""cifar10"")\n\n        self.handle = tf.placeholder(tf.string, shape=[])\n        self.next_element = None\n        self.path = path\n        self.name = name\n\n    def __call__(self, batch_size, mode):\n        """"""\n        Args:\n            batch_size: Integer.\n            mode: Running phase, one of ""train"", ""test"" or ""eval""(only if splitting=\'TVT\').\n        """"""\n        with tf.name_scope(self.name):\n            mode = mode.lower()\n            modes = [""train"", ""test"", ""eval""]\n            filenames = [os.path.join(self.path, \'%s_cifar10.tfrecord\' % mode)]\n\n            dataset = tf.data.TFRecordDataset(filenames)\n            dataset = dataset.map(parse_fun)\n            dataset = dataset.batch(batch_size)\n\n            if mode == ""train"":\n                dataset = dataset.shuffle(buffer_size=50000)\n                dataset = dataset.repeat()\n                iterator = dataset.make_one_shot_iterator()\n            elif mode == ""eval"":\n                dataset = dataset.repeat(1)\n                iterator = dataset.make_initializable_iterator()\n            elif mode == ""test"":\n                dataset = dataset.repeat(1)\n                iterator = dataset.make_one_shot_iterator()\n\n            if self.next_element is None:\n                self.next_element = tf.data.Iterator.from_string_handle(self.handle, iterator.output_types).get_next()\n\n            return(iterator)\n'"
capslayer/data/datasets/cifar10/writer.py,2,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.keras.utils.data_utils import get_file\nfrom tensorflow.python.keras.datasets.cifar import load_batch\n\nfrom capslayer.data.utils.TFRecordHelper import int64_feature, bytes_feature\n\n\nURL = ""http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz""\nmd5sum = \'c58f30108f718f92721af3b95e74349a\'\n\n\ndef load_cifar10(split, path=None):\n    if path is None:\n        cache_path = os.path.join(os.path.expanduser(\'~\'), "".capslayer"")\n        path = get_file(\'cifar-10-batches-py\', cache_dir=cache_path, file_hash=md5sum, origin=URL, untar=True)\n\n    split = split.lower()\n    if split == \'test\':\n        fpath = os.path.join(path, \'test_batch\')\n        images, labels = load_batch(fpath)\n    else:\n        num_samples = 50000\n        images = np.empty((num_samples, 3, 32, 32), dtype=\'uint8\')\n        labels = np.empty((num_samples,), dtype=\'uint8\')\n\n        for i in range(1, 6):\n            fpath = os.path.join(path, \'data_batch_\' + str(i))\n            (images[(i - 1) * 10000:i * 10000, :, :, :],\n             labels[(i - 1) * 10000:i * 10000]) = load_batch(fpath)\n\n        idx = np.arange(len(images))\n        np.random.seed(201808)\n        np.random.shuffle(idx)\n\n        images = images[idx[:45000]] if split == ""train"" else images[idx[45000:]]\n        labels = labels[idx[:45000]] if split == ""train"" else labels[idx[45000:]]\n    images = np.reshape(images.transpose(0, 2, 3, 1), (-1, 3072)).astype(np.float32)\n    labels = np.reshape(labels, (-1, )).astype(np.int32)\n\n    return(zip(images, labels))\n\n\ndef encode_and_write(dataset, filename):\n    with tf.python_io.TFRecordWriter(filename) as writer:\n        for image, label in dataset:\n            image_raw = image.tostring()\n            example = tf.train.Example(features=tf.train.Features(\n                                       feature={\'image\': bytes_feature(image_raw),\n                                                \'label\': int64_feature(label)}))\n            writer.write(example.SerializeToString())\n\n\ndef tfrecord_runner(path=None, force=True):\n    train_set = load_cifar10(path=path, split=\'train\')\n    eval_set = load_cifar10(path=path, split=\'eval\')\n    test_set = load_cifar10(path=path, split=\'test\')\n\n    if path is None:\n        path = os.path.join(os.path.expanduser(\'~\'), "".capslayer"", ""datasets"", ""cifar10"")\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    train_set_outpath = os.path.join(path, ""train_cifar10.tfrecord"")\n    eval_set_outpath = os.path.join(path, ""eval_cifar10.tfrecord"")\n    test_set_outpath = os.path.join(path, ""test_cifar10.tfrecord"")\n\n    if not os.path.exists(train_set_outpath) or force:\n        encode_and_write(train_set, train_set_outpath)\n    if not os.path.exists(eval_set_outpath) or force:\n        encode_and_write(eval_set, eval_set_outpath)\n    if not os.path.exists(test_set_outpath) or force:\n        encode_and_write(test_set, test_set_outpath)\n'"
capslayer/data/datasets/cifar100/__init__.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .reader import DataLoader\n\n__all__ = [""DataLoader""]\n'"
capslayer/data/datasets/cifar100/reader.py,11,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\n""""""Input utility functions for reading Cifar10 dataset.\n\nHandles reading from Cifar10 dataset saved in binary original format. Scales and\nnormalizes the images as the preprocessing step. It can distort the images by\nrandom cropping and contrast adjusting.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom capslayer.data.datasets.cifar100.writer import tfrecord_runner\n\n\ndef parse_fun(serialized_example):\n    """""" Data parsing function.\n    """"""\n    features = tf.parse_single_example(serialized_example,\n                                       features={\'image\': tf.FixedLenFeature([], tf.string),\n                                                 \'label\': tf.FixedLenFeature([], tf.int64)})\n    image = tf.decode_raw(features[\'image\'], tf.float32)\n    image = tf.reshape(image, shape=[32 * 32 * 3])\n    image.set_shape([32 * 32 * 3])\n    image = tf.cast(image, tf.float32) * (1. / 255)\n    label = tf.cast(features[\'label\'], tf.int32)\n    features = {\'images\': image, \'labels\': label}\n    return(features)\n\n\nclass DataLoader(object):\n    """""" Data Loader.\n    """"""\n    def __init__(self, path=None,\n                 num_works=1,\n                 splitting=""TVT"",\n                 one_hot=False,\n                 name=""create_inputs""):\n        if path is None or not os.path.exists(path):\n            tfrecord_runner()\n\n        self.handle = tf.placeholder(tf.string, shape=[])\n        self.next_element = None\n        self.path = path\n        self.name = name\n\n    def __call__(self, batch_size, mode):\n        """"""\n        Args:\n            batch_size: Integer.\n            mode: Running phase, one of ""train"", ""test"" or ""eval""(only if splitting=\'TVT\').\n        """"""\n        with tf.name_scope(self.name):\n            mode = mode.lower()\n            modes = [""train"", ""test"", ""eval""]\n            filenames = [os.path.join(self.path, \'%s_cifar100.tfrecord\' % mode)]\n\n            dataset = tf.data.TFRecordDataset(filenames)\n            dataset = dataset.map(parse_fun)\n            dataset = dataset.batch(batch_size)\n\n            if mode == ""train"":\n                dataset = dataset.shuffle(buffer_size=50000)\n                dataset = dataset.repeat()\n                iterator = dataset.make_one_shot_iterator()\n            elif mode == ""eval"":\n                dataset = dataset.repeat(1)\n                iterator = dataset.make_initializable_iterator()\n            elif mode == ""test"":\n                dataset = dataset.repeat(1)\n                iterator = dataset.make_one_shot_iterator()\n\n            if self.next_element is None:\n                self.next_element = tf.data.Iterator.from_string_handle(self.handle, iterator.output_types).get_next()\n\n            return(iterator)\n'"
capslayer/data/datasets/cifar100/writer.py,2,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.keras.utils.data_utils import get_file\nfrom tensorflow.python.keras.datasets.cifar import load_batch\n\nfrom capslayer.data.utils.TFRecordHelper import int64_feature, bytes_feature\n\n\nURL = ""https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz""\nmd5sum = \'eb9058c3a382ffc7106e4002c42a8d85\'\n\n\ndef load_cifar100(split, path=None):\n    if path is None:\n        cache_path = os.path.join(os.path.expanduser(\'~\'), "".capslayer"")\n        path = get_file(\'cifar-100-python\', cache_dir=cache_path, file_hash=md5sum, origin=URL, untar=True)\n\n    split = split.lower()\n    if split == \'test\':\n        fpath = os.path.join(path, \'test\')\n        images, labels = load_batch(fpath, label_key=\'fine_labels\')\n    else:\n        fpath = os.path.join(path, \'train\')\n        images, labels = load_batch(fpath, label_key=\'fine_labels\')\n\n        idx = np.arange(len(images))\n        np.random.seed(201808)\n        np.random.shuffle(idx)\n\n        labels = np.reshape(labels, (-1, ))\n        images = images[idx[:45000]] if split == ""train"" else images[idx[45000:]]\n        labels = labels[idx[:45000]] if split == ""train"" else labels[idx[45000:]]\n    images = np.reshape(images.transpose(0, 2, 3, 1), (-1, 3072)).astype(np.float32)\n    labels = np.reshape(labels, (-1, )).astype(np.int32)\n\n    return(zip(images, labels))\n\n\ndef encode_and_write(dataset, filename):\n    with tf.python_io.TFRecordWriter(filename) as writer:\n        for image, label in dataset:\n            print(image.shape)\n            exit()\n            image_raw = image.tostring()\n            example = tf.train.Example(features=tf.train.Features(\n                                       feature={\'image\': bytes_feature(image_raw),\n                                                \'label\': int64_feature(label)}))\n            writer.write(example.SerializeToString())\n\n\ndef tfrecord_runner(path=None, force=True):\n    train_set = load_cifar100(path=path, split=\'train\')\n    eval_set = load_cifar100(path=path, split=\'eval\')\n    test_set = load_cifar100(path=path, split=\'test\')\n\n    if path is None:\n        path = os.path.join(os.path.expanduser(\'~\'), "".capslayer"", ""datasets"", ""cifar100"")\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    train_set_outpath = os.path.join(path, ""train_cifar100.tfrecord"")\n    eval_set_outpath = os.path.join(path, ""eval_cifar100.tfrecord"")\n    test_set_outpath = os.path.join(path, ""test_cifar100.tfrecord"")\n\n    if not os.path.exists(train_set_outpath) or force:\n        encode_and_write(train_set, train_set_outpath)\n    if not os.path.exists(eval_set_outpath) or force:\n        encode_and_write(eval_set, eval_set_outpath)\n    if not os.path.exists(test_set_outpath) or force:\n        encode_and_write(test_set, test_set_outpath)\n\n\nif __name__ == ""__main__"":\n    data = load_cifar100(split=\'train\')\n    print(data)\n'"
capslayer/data/datasets/fashion_mnist/__init__.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .reader import DataLoader\n\n__all__ = [""DataLoader""]\n'"
capslayer/data/datasets/fashion_mnist/reader.py,17,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\nfrom capslayer.data.utils.download_utils import maybe_download_and_extract\nfrom capslayer.data.datasets.fashion_mnist.writer import tfrecord_runner\n\n\ndef parse_fun(serialized_example):\n    """""" Data parsing function.\n    """"""\n    features = tf.parse_single_example(serialized_example,\n                                       features={\'image\': tf.FixedLenFeature([], tf.string),\n                                                 \'label\': tf.FixedLenFeature([], tf.int64),\n                                                 \'height\': tf.FixedLenFeature([], tf.int64),\n                                                 \'width\': tf.FixedLenFeature([], tf.int64),\n                                                 \'depth\': tf.FixedLenFeature([], tf.int64)})\n    height = tf.cast(features[\'height\'], tf.int32)\n    width = tf.cast(features[\'width\'], tf.int32)\n    depth = tf.cast(features[\'depth\'], tf.int32)\n    image = tf.decode_raw(features[\'image\'], tf.float32)\n    image = tf.reshape(image, shape=[height * width * depth])\n    image.set_shape([28 * 28 * 1])\n    image = tf.cast(image, tf.float32) * (1. / 255)\n    label = tf.cast(features[\'label\'], tf.int32)\n    features = {\'images\': image, \'labels\': label}\n    return(features)\n\n\nclass DataLoader(object):\n    """""" Data Loader.\n    """"""\n    def __init__(self, path=None,\n                 num_works=1,\n                 splitting=""TVT"",\n                 one_hot=False,\n                 name=""create_inputs""):\n        """"""\n        Args:\n            path: Path to store data.\n            name: Name for the operations.\n        """"""\n\n        # path exists and is writable?\n        if path is None:\n            path = os.path.join(os.environ[""HOME""], "".cache"", ""capslayer"", ""datasets"", ""fashion_mnist"")\n            os.makedirs(path, exist_ok=True)\n        elif os.access(path, os.F_OK):\n            path = path if os.path.basename(path) == ""fashion_mnist"" else os.path.join(path, ""fashion_mnist"")\n            os.makedirs(path, exist_ok=True)\n        elif os.access(path, os.W_OK):\n            raise IOError(""Permission denied! Path %s is not writable."" % (str(path)))\n\n        # data downloaded and data extracted?\n        maybe_download_and_extract(""fashion-mnist"", path)\n        # data tfrecorded?\n        tfrecord_runner(path, force=False)\n        self.handle = tf.placeholder(tf.string, shape=[])\n        self.next_element = None\n        self.path = path\n        self.name = name\n\n    def __call__(self, batch_size, mode):\n        """"""\n        Args:\n            batch_size: Integer.\n            mode: Running phase, one of ""train"", ""test"" or ""eval""(only if splitting=\'TVT\').\n        """"""\n        with tf.name_scope(self.name):\n            mode = mode.lower()\n            modes = [""train"", ""test"", ""eval""]\n            if mode not in modes:\n                raise ""mode not found! supported modes are "" + modes\n            filenames = [os.path.join(self.path, ""%s_fashion_mnist.tfrecord"" % mode)]\n            dataset = tf.data.TFRecordDataset(filenames)\n            dataset = dataset.map(parse_fun)\n            dataset = dataset.batch(batch_size)\n\n            if mode == ""train"":\n                dataset = dataset.shuffle(buffer_size=50000)\n                dataset = dataset.repeat()\n                iterator = dataset.make_one_shot_iterator()\n            elif mode == ""eval"":\n                dataset = dataset.repeat(1)\n                iterator = dataset.make_initializable_iterator()\n            elif mode == ""test"":\n                dataset = dataset.repeat(1)\n                iterator = dataset.make_one_shot_iterator()\n\n            if self.next_element is None:\n                self.next_element = tf.data.Iterator.from_string_handle(self.handle, iterator.output_types).get_next()\n\n            return(iterator)\n'"
capslayer/data/datasets/fashion_mnist/writer.py,2,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom capslayer.data.utils.TFRecordHelper import int64_feature, bytes_feature\n\n\nFASHION_MNIST_FILES = {\n    \'train\': (\'train-images-idx3-ubyte\', \'train-labels-idx1-ubyte\'),\n    \'eval\': (\'train-images-idx3-ubyte\', \'train-labels-idx1-ubyte\'),\n    \'test\': (\'t10k-images-idx3-ubyte\', \'t10k-labels-idx1-ubyte\')\n}\n\n\ndef load_fashion_mnist(path, split):\n    split = split.lower()\n    image_file, label_file = [os.path.join(path, file_name) for file_name in MNIST_FILES[split]]\n\n    with open(image_file) as fd:\n        images = np.fromfile(file=fd, dtype=np.uint8)\n        images = images[16:].reshape(-1, 784).astype(np.float32)\n        if split == ""train"":\n            images = images[:55000]\n        elif split == ""eval"":\n            images = images[55000:]\n    with open(label_file) as fd:\n        labels = np.fromfile(file=fd, dtype=np.uint8)\n        labels = labels[8:].astype(np.int32)\n        if split == ""train"":\n            labels = labels[:55000]\n        elif split == ""eval"":\n            labels = labels[55000:]\n    return(zip(images, labels))\n\n\ndef encode_and_write(dataset, filename):\n    with tf.python_io.TFRecordWriter(filename) as writer:\n        for image, label in dataset:\n            image_raw = image.tostring()\n            example = tf.train.Example(features=tf.train.Features(\n                                       feature={\'image\': bytes_feature(image_raw),\n                                                \'label\': int64_feature(label),\n                                                \'height\': int64_feature(28),\n                                                \'width\': int64_feature(28),\n                                                \'depth\': int64_feature(1)}))\n            writer.write(example.SerializeToString())\n\n\ndef tfrecord_runner(path, force=True):\n    train_set = load_fashion_mnist(path, \'train\')\n    eval_set = load_fashion_mnist(path, \'eval\')\n    test_set = load_fashion_mnist(path, \'test\')\n\n    train_set_outpath = os.path.join(path, ""train_fashion_mnist.tfrecord"")\n    eval_set_outpath = os.path.join(path, ""eval_fashion_mnist.tfrecord"")\n    test_set_outpath = os.path.join(path, ""test_fashion_mnist.tfrecord"")\n\n    if not os.path.exists(train_set_outpath) or force:\n        encode_and_write(train_set, train_set_outpath)\n    if not os.path.exists(eval_set_outpath) or force:\n        encode_and_write(eval_set, eval_set_outpath)\n    if not os.path.exists(test_set_outpath) or force:\n        encode_and_write(test_set, test_set_outpath)\n\n\nif __name__ == \'__main__\':\n    path = ""models/data/fashion_mnist""\n    tfrecord_runner(path)\n'"
capslayer/data/datasets/mnist/__init__.py,0,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .reader import DataLoader\n\n__all__ = [""DataLoader""]\n'"
capslayer/data/datasets/mnist/reader.py,17,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\nfrom capslayer.data.utils.download_utils import maybe_download_and_extract\nfrom capslayer.data.datasets.mnist.writer import tfrecord_runner\n\n\ndef parse_fun(serialized_example):\n    """""" Data parsing function.\n    """"""\n    features = tf.parse_single_example(serialized_example,\n                                       features={\'image\': tf.FixedLenFeature([], tf.string),\n                                                 \'label\': tf.FixedLenFeature([], tf.int64),\n                                                 \'height\': tf.FixedLenFeature([], tf.int64),\n                                                 \'width\': tf.FixedLenFeature([], tf.int64),\n                                                 \'depth\': tf.FixedLenFeature([], tf.int64)})\n    height = tf.cast(features[\'height\'], tf.int32)\n    width = tf.cast(features[\'width\'], tf.int32)\n    depth = tf.cast(features[\'depth\'], tf.int32)\n    image = tf.decode_raw(features[\'image\'], tf.float32)\n    image = tf.reshape(image, shape=[height * width * depth])\n    image.set_shape([28 * 28 * 1])\n    image = tf.cast(image, tf.float32) * (1. / 255)\n    label = tf.cast(features[\'label\'], tf.int32)\n    features = {\'images\': image, \'labels\': label}\n    return(features)\n\n\nclass DataLoader(object):\n    """""" Data Loader.\n    """"""\n    def __init__(self, path=None,\n                 num_works=1,\n                 splitting=""TVT"",\n                 one_hot=False,\n                 name=""create_inputs""):\n        """"""\n        Args:\n            path: Path to store data.\n            name: Name for the operations.\n        """"""\n\n        # path exists and is writable?\n        if path is None:\n            path = os.path.join(os.environ[""HOME""], "".cache"", ""capslayer"", ""datasets"", ""mnist"")\n            os.makedirs(path, exist_ok=True)\n        elif os.access(path, os.F_OK):\n            path = path if os.path.basename(path) == ""mnist"" else os.path.join(path, ""mnist"")\n            os.makedirs(path, exist_ok=True)\n        elif os.access(path, os.W_OK):\n            raise IOError(""Permission denied! Path %s is not writable."" % (str(path)))\n\n        # data downloaded and data extracted?\n        maybe_download_and_extract(""mnist"", path)\n        # data tfrecorded?\n        tfrecord_runner(path, force=False)\n        self.handle = tf.placeholder(tf.string, shape=[])\n        self.next_element = None\n        self.path = path\n        self.name = name\n\n    def __call__(self, batch_size, mode):\n        """"""\n        Args:\n            batch_size: Integer.\n            mode: Running phase, one of ""train"", ""test"" or ""eval""(only if splitting=\'TVT\').\n        """"""\n        with tf.name_scope(self.name):\n            mode = mode.lower()\n            modes = [""train"", ""test"", ""eval""]\n            if mode not in modes:\n                raise ""mode not found! supported modes are "" + modes\n            filenames = [os.path.join(self.path, ""%s_mnist.tfrecord"" % mode)]\n            dataset = tf.data.TFRecordDataset(filenames)\n            dataset = dataset.map(parse_fun)\n            dataset = dataset.batch(batch_size)\n\n            if mode == ""train"":\n                dataset = dataset.shuffle(buffer_size=50000)\n                dataset = dataset.repeat()\n                iterator = dataset.make_one_shot_iterator()\n            elif mode == ""eval"":\n                dataset = dataset.repeat(1)\n                iterator = dataset.make_initializable_iterator()\n            elif mode == ""test"":\n                dataset = dataset.repeat(1)\n                iterator = dataset.make_one_shot_iterator()\n\n            if self.next_element is None:\n                self.next_element = tf.data.Iterator.from_string_handle(self.handle, iterator.output_types).get_next()\n\n            return(iterator)\n'"
capslayer/data/datasets/mnist/writer.py,2,"b'# Copyright 2018 The CapsLayer Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==========================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom capslayer.data.utils.TFRecordHelper import int64_feature, bytes_feature\n\n\nMNIST_FILES = {\n    \'train\': (\'train-images-idx3-ubyte\', \'train-labels-idx1-ubyte\'),\n    \'eval\': (\'train-images-idx3-ubyte\', \'train-labels-idx1-ubyte\'),\n    \'test\': (\'t10k-images-idx3-ubyte\', \'t10k-labels-idx1-ubyte\')\n}\n\n\ndef load_mnist(path, split):\n    split = split.lower()\n    image_file, label_file = [os.path.join(path, file_name) for file_name in MNIST_FILES[split]]\n\n    with open(image_file) as fd:\n        images = np.fromfile(file=fd, dtype=np.uint8)\n        images = images[16:].reshape(-1, 784).astype(np.float32)\n        if split == ""train"":\n            images = images[:55000]\n        elif split == ""eval"":\n            images = images[55000:]\n    with open(label_file) as fd:\n        labels = np.fromfile(file=fd, dtype=np.uint8)\n        labels = labels[8:].astype(np.int32)\n        if split == ""train"":\n            labels = labels[:55000]\n        elif split == ""eval"":\n            labels = labels[55000:]\n    return(zip(images, labels))\n\n\ndef encode_and_write(dataset, filename):\n    with tf.python_io.TFRecordWriter(filename) as writer:\n        for image, label in dataset:\n            image_raw = image.tostring()\n            example = tf.train.Example(features=tf.train.Features(\n                                       feature={\'image\': bytes_feature(image_raw),\n                                                \'label\': int64_feature(label),\n                                                \'height\': int64_feature(28),\n                                                \'width\': int64_feature(28),\n                                                \'depth\': int64_feature(1)}))\n            writer.write(example.SerializeToString())\n\n\ndef tfrecord_runner(path, force=True):\n    train_set = load_mnist(path, \'train\')\n    eval_set = load_mnist(path, \'eval\')\n    test_set = load_mnist(path, \'test\')\n\n    train_set_outpath = os.path.join(path, ""train_mnist.tfrecord"")\n    eval_set_outpath = os.path.join(path, ""eval_mnist.tfrecord"")\n    test_set_outpath = os.path.join(path, ""test_mnist.tfrecord"")\n\n    if not os.path.exists(train_set_outpath) or force:\n        encode_and_write(train_set, train_set_outpath)\n    if not os.path.exists(eval_set_outpath) or force:\n        encode_and_write(eval_set, eval_set_outpath)\n    if not os.path.exists(test_set_outpath) or force:\n        encode_and_write(test_set, test_set_outpath)\n\n\nif __name__ == \'__main__\':\n    path = ""models/data/mnist""\n    tfrecord_runner(path)\n'"
