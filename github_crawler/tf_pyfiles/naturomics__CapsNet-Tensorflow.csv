file_path,api_count,code
capsLayer.py,22,"b'""""""\nLicense: Apache-2.0\nAuthor: Huadong Liao\nE-mail: naturomics.liao@gmail.com\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom config import cfg\nfrom utils import reduce_sum\nfrom utils import softmax\nfrom utils import get_shape\n\n\nepsilon = 1e-9\n\n\nclass CapsLayer(object):\n    \'\'\' Capsule layer.\n    Args:\n        input: A 4-D tensor.\n        num_outputs: the number of capsule in this layer.\n        vec_len: integer, the length of the output vector of a capsule.\n        layer_type: string, one of \'FC\' or ""CONV"", the type of this layer,\n            fully connected or convolution, for the future expansion capability\n        with_routing: boolean, this capsule is routing with the\n                      lower-level layer capsule.\n\n    Returns:\n        A 4-D tensor.\n    \'\'\'\n    def __init__(self, num_outputs, vec_len, with_routing=True, layer_type=\'FC\'):\n        self.num_outputs = num_outputs\n        self.vec_len = vec_len\n        self.with_routing = with_routing\n        self.layer_type = layer_type\n\n    def __call__(self, input, kernel_size=None, stride=None):\n        \'\'\'\n        The parameters \'kernel_size\' and \'stride\' will be used while \'layer_type\' equal \'CONV\'\n        \'\'\'\n        if self.layer_type == \'CONV\':\n            self.kernel_size = kernel_size\n            self.stride = stride\n\n            if not self.with_routing:\n                # the PrimaryCaps layer, a convolutional layer\n                # input: [batch_size, 20, 20, 256]\n                # assert input.get_shape() == [cfg.batch_size, 20, 20, 256]\n\n                # NOTE: I can\'t find out any words from the paper whether the\n                # PrimaryCap convolution does a ReLU activation or not before\n                # squashing function, but experiment show that using ReLU get a\n                # higher test accuracy. So, which one to use will be your choice\n                capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n                                                    self.kernel_size, self.stride, padding=""VALID"",\n                                                    activation_fn=tf.nn.relu)\n                # capsules = tf.contrib.layers.conv2d(input, self.num_outputs * self.vec_len,\n                #                                    self.kernel_size, self.stride,padding=""VALID"",\n                #                                    activation_fn=None)\n                capsules = tf.reshape(capsules, (cfg.batch_size, -1, self.vec_len, 1))\n\n                # return tensor with shape [batch_size, 1152, 8, 1]\n                capsules = squash(capsules)\n                return(capsules)\n\n        if self.layer_type == \'FC\':\n            if self.with_routing:\n                # the DigitCaps layer, a fully connected layer\n                # Reshape the input into [batch_size, 1152, 1, 8, 1]\n                self.input = tf.reshape(input, shape=(cfg.batch_size, -1, 1, input.shape[-2].value, 1))\n\n                with tf.variable_scope(\'routing\'):\n                    # b_IJ: [batch_size, num_caps_l, num_caps_l_plus_1, 1, 1],\n                    # about the reason of using \'batch_size\', see issue #21\n                    b_IJ = tf.constant(np.zeros([cfg.batch_size, input.shape[1].value, self.num_outputs, 1, 1], dtype=np.float32))\n                    capsules = routing(self.input, b_IJ, num_outputs=self.num_outputs, num_dims=self.vec_len)\n                    capsules = tf.squeeze(capsules, axis=1)\n\n            return(capsules)\n\n\ndef routing(input, b_IJ, num_outputs=10, num_dims=16):\n    \'\'\' The routing algorithm.\n\n    Args:\n        input: A Tensor with [batch_size, num_caps_l=1152, 1, length(u_i)=8, 1]\n               shape, num_caps_l meaning the number of capsule in the layer l.\n        num_outputs: the number of output capsules.\n        num_dims: the number of dimensions for output capsule.\n    Returns:\n        A Tensor of shape [batch_size, num_caps_l_plus_1, length(v_j)=16, 1]\n        representing the vector output `v_j` in the layer l+1\n    Notes:\n        u_i represents the vector output of capsule i in the layer l, and\n        v_j the vector output of capsule j in the layer l+1.\n     \'\'\'\n\n    # W: [1, num_caps_i, num_caps_j * len_v_j, len_u_j, 1]\n    input_shape = get_shape(input)\n    W = tf.get_variable(\'Weight\', shape=[1, input_shape[1], num_dims * num_outputs] + input_shape[-2:],\n                        dtype=tf.float32, initializer=tf.random_normal_initializer(stddev=cfg.stddev))\n    biases = tf.get_variable(\'bias\', shape=(1, 1, num_outputs, num_dims, 1))\n\n    # Eq.2, calc u_hat\n    # Since tf.matmul is a time-consuming op,\n    # A better solution is using element-wise multiply, reduce_sum and reshape\n    # ops instead. Matmul [a, b] x [b, c] is equal to a series ops as\n    # element-wise multiply [a*c, b] * [a*c, b], reduce_sum at axis=1 and\n    # reshape to [a, c]\n    input = tf.tile(input, [1, 1, num_dims * num_outputs, 1, 1])\n    # assert input.get_shape() == [cfg.batch_size, 1152, 160, 8, 1]\n\n    u_hat = reduce_sum(W * input, axis=3, keepdims=True)\n    u_hat = tf.reshape(u_hat, shape=[-1, input_shape[1], num_outputs, num_dims, 1])\n    # assert u_hat.get_shape() == [cfg.batch_size, 1152, 10, 16, 1]\n\n    # In forward, u_hat_stopped = u_hat; in backward, no gradient passed back from u_hat_stopped to u_hat\n    u_hat_stopped = tf.stop_gradient(u_hat, name=\'stop_gradient\')\n\n    # line 3,for r iterations do\n    for r_iter in range(cfg.iter_routing):\n        with tf.variable_scope(\'iter_\' + str(r_iter)):\n            # line 4:\n            # => [batch_size, 1152, 10, 1, 1]\n            c_IJ = softmax(b_IJ, axis=2)\n\n            # At last iteration, use `u_hat` in order to receive gradients from the following graph\n            if r_iter == cfg.iter_routing - 1:\n                # line 5:\n                # weighting u_hat with c_IJ, element-wise in the last two dims\n                # => [batch_size, 1152, 10, 16, 1]\n                s_J = tf.multiply(c_IJ, u_hat)\n                # then sum in the second dim, resulting in [batch_size, 1, 10, 16, 1]\n                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n                # assert s_J.get_shape() == [cfg.batch_size, 1, num_outputs, num_dims, 1]\n\n                # line 6:\n                # squash using Eq.1,\n                v_J = squash(s_J)\n                # assert v_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n            elif r_iter < cfg.iter_routing - 1:  # Inner iterations, do not apply backpropagation\n                s_J = tf.multiply(c_IJ, u_hat_stopped)\n                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n                v_J = squash(s_J)\n\n                # line 7:\n                # reshape & tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1152, 10, 16, 1]\n                # then matmul in the last tow dim: [16, 1].T x [16, 1] => [1, 1], reduce mean in the\n                # batch_size dim, resulting in [1, 1152, 10, 1, 1]\n                v_J_tiled = tf.tile(v_J, [1, input_shape[1], 1, 1, 1])\n                u_produce_v = reduce_sum(u_hat_stopped * v_J_tiled, axis=3, keepdims=True)\n                # assert u_produce_v.get_shape() == [cfg.batch_size, 1152, 10, 1, 1]\n\n                # b_IJ += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n                b_IJ += u_produce_v\n\n    return(v_J)\n\n\ndef squash(vector):\n    \'\'\'Squashing function corresponding to Eq. 1\n    Args:\n        vector: A tensor with shape [batch_size, 1, num_caps, vec_len, 1] or [batch_size, num_caps, vec_len, 1].\n    Returns:\n        A tensor with the same shape as vector but squashed in \'vec_len\' dimension.\n    \'\'\'\n    vec_squared_norm = reduce_sum(tf.square(vector), -2, keepdims=True)\n    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + epsilon)\n    vec_squashed = scalar_factor * vector  # element-wise\n    return(vec_squashed)\n'"
capsNet.py,43,"b'""""""\nLicense: Apache-2.0\nAuthor: Huadong Liao\nE-mail: naturomics.liao@gmail.com\n""""""\n\nimport tensorflow as tf\n\nfrom config import cfg\nfrom utils import get_batch_data\nfrom utils import softmax\nfrom utils import reduce_sum\nfrom capsLayer import CapsLayer\n\n\nepsilon = 1e-9\n\n\nclass CapsNet(object):\n    def __init__(self, is_training=True, height=28, width=28, channels=1, num_label=10):\n        """"""\n        Args:\n            height: Integer, the height of inputs.\n            width: Integer, the width of inputs.\n            channels: Integer, the channels of inputs.\n            num_label: Integer, the category number.\n        """"""\n        self.height = height\n        self.width = width\n        self.channels = channels\n        self.num_label = num_label\n\n        self.graph = tf.Graph()\n\n        with self.graph.as_default():\n            if is_training:\n                self.X, self.labels = get_batch_data(cfg.dataset, cfg.batch_size, cfg.num_threads)\n                self.Y = tf.one_hot(self.labels, depth=self.num_label, axis=1, dtype=tf.float32)\n\n                self.build_arch()\n                self.loss()\n                self._summary()\n\n                # t_vars = tf.trainable_variables()\n                self.global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n                self.optimizer = tf.train.AdamOptimizer()\n                self.train_op = self.optimizer.minimize(self.total_loss, global_step=self.global_step)\n            else:\n                self.X = tf.placeholder(tf.float32, shape=(cfg.batch_size, self.height, self.width, self.channels))\n                self.labels = tf.placeholder(tf.int32, shape=(cfg.batch_size, ))\n                self.Y = tf.reshape(self.labels, shape=(cfg.batch_size, self.num_label, 1))\n                self.build_arch()\n\n        tf.logging.info(\'Seting up the main structure\')\n\n    def build_arch(self):\n        with tf.variable_scope(\'Conv1_layer\'):\n            # Conv1, return tensor with shape [batch_size, 20, 20, 256]\n            conv1 = tf.contrib.layers.conv2d(self.X, num_outputs=256,\n                                             kernel_size=9, stride=1,\n                                             padding=\'VALID\')\n\n        # Primary Capsules layer, return tensor with shape [batch_size, 1152, 8, 1]\n        with tf.variable_scope(\'PrimaryCaps_layer\'):\n            primaryCaps = CapsLayer(num_outputs=32, vec_len=8, with_routing=False, layer_type=\'CONV\')\n            caps1 = primaryCaps(conv1, kernel_size=9, stride=2)\n\n        # DigitCaps layer, return shape [batch_size, 10, 16, 1]\n        with tf.variable_scope(\'DigitCaps_layer\'):\n            digitCaps = CapsLayer(num_outputs=self.num_label, vec_len=16, with_routing=True, layer_type=\'FC\')\n            self.caps2 = digitCaps(caps1)\n\n        # Decoder structure in Fig. 2\n        # 1. Do masking, how:\n        with tf.variable_scope(\'Masking\'):\n            # a). calc ||v_c||, then do softmax(||v_c||)\n            # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n            self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2),\n                                               axis=2, keepdims=True) + epsilon)\n            self.softmax_v = softmax(self.v_length, axis=1)\n            # assert self.softmax_v.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n\n            # b). pick out the index of max softmax val of the 10 caps\n            # [batch_size, 10, 1, 1] => [batch_size] (index)\n            self.argmax_idx = tf.to_int32(tf.argmax(self.softmax_v, axis=1))\n            # assert self.argmax_idx.get_shape() == [cfg.batch_size, 1, 1]\n            self.argmax_idx = tf.reshape(self.argmax_idx, shape=(cfg.batch_size, ))\n\n            # Method 1.\n            if not cfg.mask_with_y:\n                # c). indexing\n                # It\'s not easy to understand the indexing process with argmax_idx\n                # as we are 3-dim animal\n                masked_v = []\n                for batch_size in range(cfg.batch_size):\n                    v = self.caps2[batch_size][self.argmax_idx[batch_size], :]\n                    masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n\n                self.masked_v = tf.concat(masked_v, axis=0)\n                assert self.masked_v.get_shape() == [cfg.batch_size, 1, 16, 1]\n            # Method 2. masking with true label, default mode\n            else:\n                self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, self.num_label, 1)))\n                self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon)\n\n        # 2. Reconstructe the MNIST images with 3 FC layers\n        # [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n        with tf.variable_scope(\'Decoder\'):\n            vector_j = tf.reshape(self.masked_v, shape=(cfg.batch_size, -1))\n            fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n            fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n            self.decoded = tf.contrib.layers.fully_connected(fc2,\n                                                             num_outputs=self.height * self.width * self.channels,\n                                                             activation_fn=tf.sigmoid)\n\n    def loss(self):\n        # 1. The margin loss\n\n        # [batch_size, 10, 1, 1]\n        # max_l = max(0, m_plus-||v_c||)^2\n        max_l = tf.square(tf.maximum(0., cfg.m_plus - self.v_length))\n        # max_r = max(0, ||v_c||-m_minus)^2\n        max_r = tf.square(tf.maximum(0., self.v_length - cfg.m_minus))\n        assert max_l.get_shape() == [cfg.batch_size, self.num_label, 1, 1]\n\n        # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n        max_l = tf.reshape(max_l, shape=(cfg.batch_size, -1))\n        max_r = tf.reshape(max_r, shape=(cfg.batch_size, -1))\n\n        # calc T_c: [batch_size, 10]\n        # T_c = Y, is my understanding correct? Try it.\n        T_c = self.Y\n        # [batch_size, 10], element-wise multiply\n        L_c = T_c * max_l + cfg.lambda_val * (1 - T_c) * max_r\n\n        self.margin_loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n\n        # 2. The reconstruction loss\n        orgin = tf.reshape(self.X, shape=(cfg.batch_size, -1))\n        squared = tf.square(self.decoded - orgin)\n        self.reconstruction_err = tf.reduce_mean(squared)\n\n        # 3. Total loss\n        # The paper uses sum of squared error as reconstruction error, but we\n        # have used reduce_mean in `# 2 The reconstruction loss` to calculate\n        # mean squared error. In order to keep in line with the paper,the\n        # regularization scale should be 0.0005*784=0.392\n        self.total_loss = self.margin_loss + cfg.regularization_scale * self.reconstruction_err\n\n    # Summary\n    def _summary(self):\n        train_summary = []\n        train_summary.append(tf.summary.scalar(\'train/margin_loss\', self.margin_loss))\n        train_summary.append(tf.summary.scalar(\'train/reconstruction_loss\', self.reconstruction_err))\n        train_summary.append(tf.summary.scalar(\'train/total_loss\', self.total_loss))\n        recon_img = tf.reshape(self.decoded, shape=(cfg.batch_size, self.height, self.width, self.channels))\n        train_summary.append(tf.summary.image(\'reconstruction_img\', recon_img))\n        self.train_summary = tf.summary.merge(train_summary)\n\n        correct_prediction = tf.equal(tf.to_int32(self.labels), self.argmax_idx)\n        self.accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n'"
config.py,3,"b""import tensorflow as tf\n\nflags = tf.app.flags\n\n\n############################\n#    hyper parameters      #\n############################\n\n# For separate margin loss\nflags.DEFINE_float('m_plus', 0.9, 'the parameter of m plus')\nflags.DEFINE_float('m_minus', 0.1, 'the parameter of m minus')\nflags.DEFINE_float('lambda_val', 0.5, 'down weight of the loss for absent digit classes')\n\n# for training\nflags.DEFINE_integer('batch_size', 128, 'batch size')\nflags.DEFINE_integer('epoch', 50, 'epoch')\nflags.DEFINE_integer('iter_routing', 3, 'number of iterations in routing algorithm')\nflags.DEFINE_boolean('mask_with_y', True, 'use the true label to mask out target capsule or not')\n\nflags.DEFINE_float('stddev', 0.01, 'stddev for W initializer')\nflags.DEFINE_float('regularization_scale', 0.392, 'regularization coefficient for reconstruction loss, default to 0.0005*784=0.392')\n\n\n############################\n#   environment setting    #\n############################\nflags.DEFINE_string('dataset', 'mnist', 'The name of dataset [mnist, fashion-mnist')\nflags.DEFINE_boolean('is_training', True, 'train or predict phase')\nflags.DEFINE_integer('num_threads', 8, 'number of threads of enqueueing examples')\nflags.DEFINE_string('logdir', 'logdir', 'logs directory')\nflags.DEFINE_integer('train_sum_freq', 100, 'the frequency of saving train summary(step)')\nflags.DEFINE_integer('val_sum_freq', 500, 'the frequency of saving valuation summary(step)')\nflags.DEFINE_integer('save_freq', 3, 'the frequency of saving model(epoch)')\nflags.DEFINE_string('results', 'results', 'path for saving results')\n\n############################\n#   distributed setting    #\n############################\nflags.DEFINE_integer('num_gpu', 2, 'number of gpus for distributed training')\nflags.DEFINE_integer('batch_size_per_gpu', 128, 'batch size on 1 gpu')\nflags.DEFINE_integer('thread_per_gpu', 4, 'Number of preprocessing threads per tower.')\n\ncfg = tf.app.flags.FLAGS\n# tf.logging.set_verbosity(tf.logging.INFO)\n"""
download_data.py,0,"b'import os\nimport sys\nimport gzip\nimport shutil\nfrom six.moves import urllib\n\n# mnist dataset\nHOMEPAGE = ""http://yann.lecun.com/exdb/mnist/""\nMNIST_TRAIN_IMGS_URL = HOMEPAGE + ""train-images-idx3-ubyte.gz""\nMNIST_TRAIN_LABELS_URL = HOMEPAGE + ""train-labels-idx1-ubyte.gz""\nMNIST_TEST_IMGS_URL = HOMEPAGE + ""t10k-images-idx3-ubyte.gz""\nMNIST_TEST_LABELS_URL = HOMEPAGE + ""t10k-labels-idx1-ubyte.gz""\n\n# fashion-mnist dataset\nHOMEPAGE = ""http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/""\nFASHION_MNIST_TRAIN_IMGS_URL = HOMEPAGE + ""train-images-idx3-ubyte.gz""\nFASHION_MNIST_TRAIN_LABELS_URL = HOMEPAGE + ""train-labels-idx1-ubyte.gz""\nFASHION_MNIST_TEST_IMGS_URL = HOMEPAGE + ""t10k-images-idx3-ubyte.gz""\nFASHION_MNIST_TEST_LABELS_URL = HOMEPAGE + ""t10k-labels-idx1-ubyte.gz""\n\n\ndef download_and_uncompress_zip(URL, dataset_dir, force=False):\n    \'\'\'\n    Args:\n        URL: the download links for data\n        dataset_dir: the path to save data\n        force: re-download data\n    \'\'\'\n    filename = URL.split(\'/\')[-1]\n    filepath = os.path.join(dataset_dir, filename)\n    if not os.path.exists(dataset_dir):\n        os.mkdir(dataset_dir)\n    extract_to = os.path.splitext(filepath)[0]\n\n    def download_progress(count, block_size, total_size):\n        sys.stdout.write(""\\r>> Downloading %s %.1f%%"" % (filename, float(count * block_size) / float(total_size) * 100.))\n        sys.stdout.flush()\n\n    if not force and os.path.exists(filepath):\n        print(""file %s already exist"" % (filename))\n    else:\n        filepath, _ = urllib.request.urlretrieve(URL, filepath, download_progress)\n        print()\n        print(\'Successfully Downloaded\', filename)\n\n    # with zipfile.ZipFile(filepath) as fd:\n    with gzip.open(filepath, \'rb\') as f_in, open(extract_to, \'wb\') as f_out:\n        print(\'Extracting \', filename)\n        shutil.copyfileobj(f_in, f_out)\n        print(\'Successfully extracted\')\n        print()\n\n\ndef start_download(dataset, save_to, force):\n    if not os.path.exists(save_to):\n        os.makedirs(save_to)\n    if dataset == \'mnist\':\n        download_and_uncompress_zip(MNIST_TRAIN_IMGS_URL, save_to, force)\n        download_and_uncompress_zip(MNIST_TRAIN_LABELS_URL, save_to, force)\n        download_and_uncompress_zip(MNIST_TEST_IMGS_URL, save_to, force)\n        download_and_uncompress_zip(MNIST_TEST_LABELS_URL, save_to, force)\n    elif dataset == \'fashion-mnist\':\n        download_and_uncompress_zip(FASHION_MNIST_TRAIN_IMGS_URL, save_to, force)\n        download_and_uncompress_zip(FASHION_MNIST_TRAIN_LABELS_URL, save_to, force)\n        download_and_uncompress_zip(FASHION_MNIST_TEST_IMGS_URL, save_to, force)\n        download_and_uncompress_zip(FASHION_MNIST_TEST_LABELS_URL, save_to, force)\n    else:\n        raise Exception(""Invalid dataset name! please check it: "", dataset)\n\nif __name__ == \'__main__\':\n    import argparse\n    parser = argparse.ArgumentParser(\'Script for automatically downloading datasets\')\n    parser.add_argument(""--dataset"", default=\'mnist\', choices=[\'mnist\', \'fashion-mnist\', \'smallNORB\'])\n    save_to = os.path.join(\'data\', \'mnist\')\n    parser.add_argument(""--save_to"", default=save_to)\n    parser.add_argument(""--force"", default=False, type=bool)\n    args = parser.parse_args()\n    start_download(args.dataset, args.save_to, args.force)\n'"
main.py,10,"b'import os\nimport sys\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom config import cfg\nfrom utils import load_data\nfrom capsNet import CapsNet\n\n\ndef save_to():\n    if not os.path.exists(cfg.results):\n        os.mkdir(cfg.results)\n    if cfg.is_training:\n        loss = cfg.results + \'/loss.csv\'\n        train_acc = cfg.results + \'/train_acc.csv\'\n        val_acc = cfg.results + \'/val_acc.csv\'\n\n        if os.path.exists(val_acc):\n            os.remove(val_acc)\n        if os.path.exists(loss):\n            os.remove(loss)\n        if os.path.exists(train_acc):\n            os.remove(train_acc)\n\n        fd_train_acc = open(train_acc, \'w\')\n        fd_train_acc.write(\'step,train_acc\\n\')\n        fd_loss = open(loss, \'w\')\n        fd_loss.write(\'step,loss\\n\')\n        fd_val_acc = open(val_acc, \'w\')\n        fd_val_acc.write(\'step,val_acc\\n\')\n        return(fd_train_acc, fd_loss, fd_val_acc)\n    else:\n        test_acc = cfg.results + \'/test_acc.csv\'\n        if os.path.exists(test_acc):\n            os.remove(test_acc)\n        fd_test_acc = open(test_acc, \'w\')\n        fd_test_acc.write(\'test_acc\\n\')\n        return(fd_test_acc)\n\n\ndef train(model, supervisor, num_label):\n    trX, trY, num_tr_batch, valX, valY, num_val_batch = load_data(cfg.dataset, cfg.batch_size, is_training=True)\n    Y = valY[:num_val_batch * cfg.batch_size].reshape((-1, 1))\n\n    fd_train_acc, fd_loss, fd_val_acc = save_to()\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    with supervisor.managed_session(config=config) as sess:\n        print(""\\nNote: all of results will be saved to directory: "" + cfg.results)\n        for epoch in range(cfg.epoch):\n            print(""Training for epoch %d/%d:"" % (epoch, cfg.epoch))\n            if supervisor.should_stop():\n                print(\'supervisor stoped!\')\n                break\n            for step in tqdm(range(num_tr_batch), total=num_tr_batch, ncols=70, leave=False, unit=\'b\'):\n                start = step * cfg.batch_size\n                end = start + cfg.batch_size\n                global_step = epoch * num_tr_batch + step\n\n                if global_step % cfg.train_sum_freq == 0:\n                    _, loss, train_acc, summary_str = sess.run([model.train_op, model.total_loss, model.accuracy, model.train_summary])\n                    assert not np.isnan(loss), \'Something wrong! loss is nan...\'\n                    supervisor.summary_writer.add_summary(summary_str, global_step)\n\n                    fd_loss.write(str(global_step) + \',\' + str(loss) + ""\\n"")\n                    fd_loss.flush()\n                    fd_train_acc.write(str(global_step) + \',\' + str(train_acc / cfg.batch_size) + ""\\n"")\n                    fd_train_acc.flush()\n                else:\n                    sess.run(model.train_op)\n\n                if cfg.val_sum_freq != 0 and (global_step) % cfg.val_sum_freq == 0:\n                    val_acc = 0\n                    for i in range(num_val_batch):\n                        start = i * cfg.batch_size\n                        end = start + cfg.batch_size\n                        acc = sess.run(model.accuracy, {model.X: valX[start:end], model.labels: valY[start:end]})\n                        val_acc += acc\n                    val_acc = val_acc / (cfg.batch_size * num_val_batch)\n                    fd_val_acc.write(str(global_step) + \',\' + str(val_acc) + \'\\n\')\n                    fd_val_acc.flush()\n\n            if (epoch + 1) % cfg.save_freq == 0:\n                supervisor.saver.save(sess, cfg.logdir + \'/model_epoch_%04d_step_%02d\' % (epoch, global_step))\n\n        fd_val_acc.close()\n        fd_train_acc.close()\n        fd_loss.close()\n\n\ndef evaluation(model, supervisor, num_label):\n    teX, teY, num_te_batch = load_data(cfg.dataset, cfg.batch_size, is_training=False)\n    fd_test_acc = save_to()\n    with supervisor.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n        supervisor.saver.restore(sess, tf.train.latest_checkpoint(cfg.logdir))\n        tf.logging.info(\'Model restored!\')\n\n        test_acc = 0\n        for i in tqdm(range(num_te_batch), total=num_te_batch, ncols=70, leave=False, unit=\'b\'):\n            start = i * cfg.batch_size\n            end = start + cfg.batch_size\n            acc = sess.run(model.accuracy, {model.X: teX[start:end], model.labels: teY[start:end]})\n            test_acc += acc\n        test_acc = test_acc / (cfg.batch_size * num_te_batch)\n        fd_test_acc.write(str(test_acc))\n        fd_test_acc.close()\n        print(\'Test accuracy has been saved to \' + cfg.results + \'/test_acc.csv\')\n\n\ndef main(_):\n    tf.logging.info(\' Loading Graph...\')\n    num_label = 10\n    model = CapsNet()\n    tf.logging.info(\' Graph loaded\')\n\n    sv = tf.train.Supervisor(graph=model.graph, logdir=cfg.logdir, save_model_secs=0)\n\n    if cfg.is_training:\n        tf.logging.info(\' Start training...\')\n        train(model, sv, num_label)\n        tf.logging.info(\'Training done\')\n    else:\n        evaluation(model, sv, num_label)\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
utils.py,8,"b'import os\nimport scipy\nimport numpy as np\nimport tensorflow as tf\n\n\ndef load_mnist(batch_size, is_training=True):\n    path = os.path.join(\'data\', \'mnist\')\n    if is_training:\n        fd = open(os.path.join(path, \'train-images-idx3-ubyte\'))\n        loaded = np.fromfile(file=fd, dtype=np.uint8)\n        trainX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float32)\n\n        fd = open(os.path.join(path, \'train-labels-idx1-ubyte\'))\n        loaded = np.fromfile(file=fd, dtype=np.uint8)\n        trainY = loaded[8:].reshape((60000)).astype(np.int32)\n\n        trX = trainX[:55000] / 255.\n        trY = trainY[:55000]\n\n        valX = trainX[55000:, ] / 255.\n        valY = trainY[55000:]\n\n        num_tr_batch = 55000 // batch_size\n        num_val_batch = 5000 // batch_size\n\n        return trX, trY, num_tr_batch, valX, valY, num_val_batch\n    else:\n        fd = open(os.path.join(path, \'t10k-images-idx3-ubyte\'))\n        loaded = np.fromfile(file=fd, dtype=np.uint8)\n        teX = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n\n        fd = open(os.path.join(path, \'t10k-labels-idx1-ubyte\'))\n        loaded = np.fromfile(file=fd, dtype=np.uint8)\n        teY = loaded[8:].reshape((10000)).astype(np.int32)\n\n        num_te_batch = 10000 // batch_size\n        return teX / 255., teY, num_te_batch\n\n\ndef load_fashion_mnist(batch_size, is_training=True):\n    path = os.path.join(\'data\', \'fashion-mnist\')\n    if is_training:\n        fd = open(os.path.join(path, \'train-images-idx3-ubyte\'))\n        loaded = np.fromfile(file=fd, dtype=np.uint8)\n        trainX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float32)\n\n        fd = open(os.path.join(path, \'train-labels-idx1-ubyte\'))\n        loaded = np.fromfile(file=fd, dtype=np.uint8)\n        trainY = loaded[8:].reshape((60000)).astype(np.int32)\n\n        trX = trainX[:55000] / 255.\n        trY = trainY[:55000]\n\n        valX = trainX[55000:, ] / 255.\n        valY = trainY[55000:]\n\n        num_tr_batch = 55000 // batch_size\n        num_val_batch = 5000 // batch_size\n\n        return trX, trY, num_tr_batch, valX, valY, num_val_batch\n    else:\n        fd = open(os.path.join(path, \'t10k-images-idx3-ubyte\'))\n        loaded = np.fromfile(file=fd, dtype=np.uint8)\n        teX = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n\n        fd = open(os.path.join(path, \'t10k-labels-idx1-ubyte\'))\n        loaded = np.fromfile(file=fd, dtype=np.uint8)\n        teY = loaded[8:].reshape((10000)).astype(np.int32)\n\n        num_te_batch = 10000 // batch_size\n        return teX / 255., teY, num_te_batch\n\n\ndef load_data(dataset, batch_size, is_training=True, one_hot=False):\n    if dataset == \'mnist\':\n        return load_mnist(batch_size, is_training)\n    elif dataset == \'fashion-mnist\':\n        return load_fashion_mnist(batch_size, is_training)\n    else:\n        raise Exception(\'Invalid dataset, please check the name of dataset:\', dataset)\n\n\ndef get_batch_data(dataset, batch_size, num_threads):\n    if dataset == \'mnist\':\n        trX, trY, num_tr_batch, valX, valY, num_val_batch = load_mnist(batch_size, is_training=True)\n    elif dataset == \'fashion-mnist\':\n        trX, trY, num_tr_batch, valX, valY, num_val_batch = load_fashion_mnist(batch_size, is_training=True)\n    data_queues = tf.train.slice_input_producer([trX, trY])\n    X, Y = tf.train.shuffle_batch(data_queues, num_threads=num_threads,\n                                  batch_size=batch_size,\n                                  capacity=batch_size * 64,\n                                  min_after_dequeue=batch_size * 32,\n                                  allow_smaller_final_batch=False)\n\n    return(X, Y)\n\n\ndef save_images(imgs, size, path):\n    \'\'\'\n    Args:\n        imgs: [batch_size, image_height, image_width]\n        size: a list with tow int elements, [image_height, image_width]\n        path: the path to save images\n    \'\'\'\n    imgs = (imgs + 1.) / 2  # inverse_transform\n    return(scipy.misc.imsave(path, mergeImgs(imgs, size)))\n\n\ndef mergeImgs(images, size):\n    h, w = images.shape[1], images.shape[2]\n    imgs = np.zeros((h * size[0], w * size[1], 3))\n    for idx, image in enumerate(images):\n        i = idx % size[1]\n        j = idx // size[1]\n        imgs[j * h:j * h + h, i * w:i * w + w, :] = image\n\n    return imgs\n\n\n# For version compatibility\ndef reduce_sum(input_tensor, axis=None, keepdims=False):\n    try:\n        return tf.reduce_sum(input_tensor, axis=axis, keepdims=keepdims)\n    except:\n        return tf.reduce_sum(input_tensor, axis=axis, keep_dims=keepdims)\n\n\n# For version compatibility\ndef softmax(logits, axis=None):\n    try:\n        return tf.nn.softmax(logits, axis=axis)\n    except:\n        return tf.nn.softmax(logits, dim=axis)\n\n\ndef get_shape(inputs, name=None):\n    name = ""shape"" if name is None else name\n    with tf.name_scope(name):\n        static_shape = inputs.get_shape().as_list()\n        dynamic_shape = tf.shape(inputs)\n        shape = []\n        for i, dim in enumerate(static_shape):\n            dim = dim if dim is not None else dynamic_shape[i]\n            shape.append(dim)\n        return(shape)\n'"
dist_version/capsnet_slim.py,36,"b""import tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom config import cfg\nimport numpy as np\n\ndef build_arch(input, y, is_train=False):\n    initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n    biasInitializer = tf.constant_initializer(0.0)\n\n    with slim.arg_scope([slim.conv2d], trainable=is_train, weights_initializer=initializer, biases_initializer=biasInitializer):\n        with tf.variable_scope('Conv1_layer') as scope:\n            output = slim.conv2d(input, num_outputs=256, kernel_size=[9, 9], stride=1, padding='VALID', scope=scope)\n            assert output.get_shape() == [cfg.batch_size_per_gpu, 20, 20, 256]\n\n        with tf.variable_scope('PrimaryCaps_layer') as scope:\n            output = slim.conv2d(output, num_outputs=32*8, kernel_size=[9, 9], stride=2, padding='VALID', scope=scope, activation_fn=None)\n            output = tf.reshape(output, [cfg.batch_size_per_gpu, -1, 1, 8])\n            assert output.get_shape() == [cfg.batch_size_per_gpu, 1152, 1, 8]\n\n        with tf.variable_scope('DigitCaps_layer') as scope:\n            u_hats = []\n            input_groups = tf.split(axis=1, num_or_size_splits=1152, value=output)\n            for i in range(1152):\n                u_hat = slim.conv2d(input_groups[i], num_outputs=16*10, kernel_size=[1, 1], stride=1, padding='VALID', scope='DigitCaps_layer_w_'+str(i), activation_fn=None)\n                u_hat = tf.reshape(u_hat, [cfg.batch_size_per_gpu, 1, 10, 16])\n                u_hats.append(u_hat)\n\n            output = tf.concat(u_hats, axis=1)\n            assert output.get_shape() == [cfg.batch_size_per_gpu, 1152, 10, 16]\n\n            b_ijs = tf.constant(np.zeros([1152, 10], dtype=np.float32))\n            v_js = []\n            for r_iter in range(cfg.iter_routing):\n                with tf.variable_scope('iter_'+str(r_iter)):\n                    c_ijs = tf.nn.softmax(b_ijs, dim=1)\n\n                    c_ij_groups = tf.split(axis=1, num_or_size_splits=10, value=c_ijs)\n                    b_ij_groups = tf.split(axis=1, num_or_size_splits=10, value=b_ijs)\n                    input_groups = tf.split(axis=2, num_or_size_splits=10, value=output)\n\n                    s_js = []\n\n                    for i in range(10):\n                        c_ij = tf.reshape(tf.tile(c_ij_groups[i], [1, 16]), [1152, 1, 16, 1])\n                        s_j = tf.nn.depthwise_conv2d(input_groups[i], c_ij, strides=[1, 1, 1, 1], padding='VALID')\n                        assert s_j.get_shape() == [cfg.batch_size_per_gpu, 1, 1, 16]\n\n                        s_j = tf.reshape(s_j, [cfg.batch_size_per_gpu, 16])\n                        s_j_norm_square = tf.reduce_mean(tf.square(s_j), axis=1, keep_dims=True)\n                        v_j = s_j_norm_square*s_j/((1+s_j_norm_square)*tf.sqrt(s_j_norm_square+1e-9))\n                        assert v_j.get_shape() == [cfg.batch_size_per_gpu, 16]\n\n                        b_ij_groups[i] = b_ij_groups[i]+tf.reduce_sum(tf.matmul(tf.reshape(input_groups[i], [cfg.batch_size_per_gpu, 1152, 16]), tf.reshape(v_j, [cfg.batch_size, 16, 1])), axis=0)\n\n                        if r_iter == cfg.iter_routing-1:\n                            v_js.append(tf.reshape(v_j, [cfg.batch_size_per_gpu, 1, 16]))\n\n                    b_ijs = tf.concat(b_ij_groups, axis=1)\n\n            output = tf.concat(v_js, axis=1)\n\n            with tf.variable_scope('Masking'):\n                v_len = tf.norm(output, axis=2)\n\n            if is_train:\n                masked_v = tf.matmul(output, tf.reshape(y, [-1, 10, 1]), transpose_a=True)\n                masked_v = tf.reshape(masked_v, [-1, 16])\n\n                with tf.variable_scope('Decoder'):\n                    output = slim.fully_connected(masked_v, 512, trainable=is_train)\n                    output = slim.fully_connected(output, 1024, trainable=is_train)\n                    output = slim.fully_connected(output, 784, trainable=is_train, activation_fn=tf.sigmoid)\n\n    return v_len, output\n\ndef loss(v_len, output, x, y):\n    max_l = tf.square(tf.maximum(0., cfg.m_plus-v_len))\n    max_r = tf.square(tf.maximum(0., v_len - cfg.m_minus))\n\n    l_c = y*max_l+cfg.lambda_val * (1 - y) * max_r\n\n    margin_loss = tf.reduce_mean(tf.reduce_sum(l_c, axis=1))\n\n    origin = tf.reshape(x, shape=[cfg.batch_size, -1])\n    reconstruction_err = tf.reduce_mean(tf.square(output-origin))\n\n    total_loss = margin_loss+0.0005*reconstruction_err\n\n    tf.losses.add_loss(total_loss)\n\n    return total_loss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
dist_version/distributed_train.py,27,"b'import sys\n\nsys.path.append(\'.\')\n\nimport tensorflow as tf\nfrom config import cfg\nfrom utils import load_mnist\nimport dist_version.capsnet_slim as net\nimport time\nimport tensorflow.contrib.slim as slim\nimport re\nimport copy\nimport numpy as np\nimport os\n\ndef create_inputs():\n    trX, trY = load_mnist(cfg.dataset, cfg.is_training)\n\n    num_pre_threads = cfg.thread_per_gpu*cfg.num_gpu\n    data_queue = tf.train.slice_input_producer([trX, trY], capacity=64*num_pre_threads)\n    X, Y = tf.train.shuffle_batch(data_queue, num_threads=num_pre_threads,\n                                  batch_size=cfg.batch_size_per_gpu*cfg.num_gpu,\n                                  capacity=cfg.batch_size_per_gpu*cfg.num_gpu * 64,\n                                  min_after_dequeue=cfg.batch_size_per_gpu*cfg.num_gpu * 32,\n                                  allow_smaller_final_batch=False)\n\n    return (X, Y)\n\ndef tower_loss(x, y, scope, reuse_variables=None):\n    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):\n        v_len, output = net.build_arch(x, y, is_train=True)\n\n    net.loss(v_len, output, x, y)\n\n    loss = tf.get_collection(tf.GraphKeys.LOSSES, scope)[0]\n    loss_name = re.sub(\'%s_[0-9]*/\' % \'tower_\', \'\', loss.op.name)\n    tf.summary.scalar(loss_name, loss)\n\n    return loss\n\ndef average_gradients(tower_grads):\n  """"""Calculate the average gradient for each shared variable across all towers.\n\n  Note that this function provides a synchronization point across all towers.\n\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over individual gradients. The inner list is over the gradient\n      calculation for each tower.\n  Returns:\n     List of pairs of (gradient, variable) where the gradient has been averaged\n     across all towers.\n  """"""\n  average_grads = []\n  for grad_and_vars in zip(*tower_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n    grads = []\n    for g, _ in grad_and_vars:\n      # Add 0 dimension to the gradients to represent the tower.\n      expanded_g = tf.expand_dims(g, 0)\n\n      # Append on a \'tower\' dimension which we will average over below.\n      grads.append(expanded_g)\n\n    # Average over the \'tower\' dimension.\n    grad = tf.concat(axis=0, values=grads)\n    grad = tf.reduce_mean(grad, 0)\n\n    # Keep in mind that the Variables are redundant because they are shared\n    # across towers. So .. we will just return the first tower\'s pointer to\n    # the Variable.\n    v = grad_and_vars[0][1]\n    grad_and_var = (grad, v)\n    average_grads.append(grad_and_var)\n  return average_grads\n\ndef main(_):\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n        global_step = tf.get_variable(\'global_step\', [],\n                                  initializer=tf.constant_initializer(0),\n                                  trainable=False)\n\n        num_batches_per_epoch = int(60000/(cfg.batch_size_per_gpu*cfg.num_gpu))\n\n        opt = tf.train.AdamOptimizer()\n\n        batch_x, batch_labels = create_inputs()\n        batch_y = tf.one_hot(batch_labels, depth=10, axis=1, dtype=tf.float32)\n        input_summaries = copy.copy(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n        x_splits = tf.split(axis=0, num_or_size_splits=cfg.num_gpu, value=batch_x)\n        y_splits = tf.split(axis=0, num_or_size_splits=cfg.num_gpu, value=batch_y)\n\n        tower_grads = []\n        reuse_variables = None\n        for i in range(cfg.num_gpu):\n            with tf.device(\'/gpu:%d\' % i):\n                with tf.name_scope(\'%s_%d\' % (\'tower_\', i)) as scope:\n                    with slim.arg_scope([slim.variable], device=\'/cpu:0\'):\n                        loss = tower_loss(x_splits[i], y_splits[i], scope, reuse_variables)\n\n                    reuse_variables = True\n\n                    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n\n                    grads = opt.compute_gradients(loss)\n                    tower_grads.append(grads)\n\n        grad = average_gradients(tower_grads)\n\n        summaries.extend(input_summaries)\n\n        train_op = opt.apply_gradients(grad, global_step=global_step)\n\n        config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n        config.gpu_options.allow_growth = True\n        sess = tf.Session(config=config)\n        sess.run(tf.global_variables_initializer())\n\n        saver = tf.train.Saver(tf.global_variables(), max_to_keep=cfg.epoch)\n        summary_op = tf.summary.merge(summaries)\n        tf.train.start_queue_runners(sess=sess)\n\n        summary_writer = tf.summary.FileWriter(\n            cfg.logdir,\n            graph=sess.graph)\n\n        for step in range(cfg.epoch*num_batches_per_epoch):\n            tic = time.time()\n            _, loss_value = sess.run([train_op, loss])\n            print(str(time.time()-tic)+\' \'+str(step))\n\n            assert not np.isnan(loss_value)\n\n            if step % 10 == 0:\n                summary_str = sess.run(summary_op)\n                summary_writer.add_summary(summary_str, step)\n\n            if step % num_batches_per_epoch == 0 or (step+1) == cfg.epoch*num_batches_per_epoch:\n                ckpt_path = os.path.join(cfg.logdir, \'model.ckpt\')\n                saver.save(sess, ckpt_path, global_step=step)\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
