file_path,api_count,code
duplicate_questions/__init__.py,0,b''
tests/__init__.py,0,b''
duplicate_questions/data/__init__.py,0,b''
duplicate_questions/data/data_indexer.py,0,"b'from collections import Counter, defaultdict\nimport logging\n\nfrom .dataset import Dataset\n\nimport tqdm\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\nclass DataIndexer:\n    """"""\n    A DataIndexer maps strings to integers, allowing for strings to be mapped\n    to an out-of-vocabulary token.\n\n    DataIndexers are fit to a particular dataset, which we use to decide which\n    words are in-vocabulary.\n    """"""\n    def __init__(self):\n        # Typically all input words to this code are lower-cased, so we could\n        # simply use ""PADDING"" for this. But doing it this way, with special\n        # characters, future-proofs the code in case it is used later in a\n        # setting where not all input is lowercase.\n        self._padding_token = ""@@PADDING@@""\n        self._oov_token = ""@@UNKOWN@@""\n        self.word_indices = defaultdict(self._default_namespace_word_indices_dict)\n        self.is_fit = False\n        self.reverse_word_indices = defaultdict(\n            self._default_namespace_reverse_word_indices_dict)\n\n    def _default_namespace_word_indices_dict(self):\n        return {self._padding_token: 0, self._oov_token: 1}\n\n    def _default_namespace_reverse_word_indices_dict(self):\n        return {0: self._padding_token, 1: self._oov_token}\n\n    def fit_word_dictionary(self, dataset, min_count=1):\n        """"""\n        Given a Dataset, this method decides which words (which could be words\n        or characters) are given an index, and which ones are mapped to an OOV\n        token (in this case ""@@UNKNOWN@@"").\n\n        This method must be called before any dataset is indexed with this\n        DataIndexer. If you don\'t first fit the word dictionary, you\'ll\n        basically map every token to the OOV token. We call instance.words()\n        for each instance in the dataset, and then keep all words that appear\n        at least min_count times.\n\n        Parameters\n        ----------\n        dataset: Dataset\n            The dataset to index.\n\n        min_count: int\n            The minimum number of times a word must occur in order\n            to be indexed.\n        """"""\n        if not isinstance(dataset, Dataset):\n            raise ValueError(""Expected dataset to be type ""\n                             ""Dataset, found {} of type ""\n                             ""{}"".format(dataset, type(dataset)))\n        if not isinstance(min_count, int):\n            raise ValueError(""Expected min_count to be type ""\n                             ""int, found {} of type ""\n                             ""{}"".format(min_count, type(min_count)))\n\n        logger.info(""Fitting word dictionary with min count of %d"", min_count)\n        namespace_word_counts = defaultdict(Counter)\n        for instance in tqdm.tqdm(dataset.instances):\n            # dictionary with keys as namespace names, and values asarray\n            # the words for that namespace.\n            namespace_dict = instance.words()\n            for namespace in namespace_dict:\n                for word in namespace_dict[namespace]:\n                    namespace_word_counts[namespace][word] += 1\n        # Index the dataset, sorted by order of decreasing frequency, and then\n        # alphabetically for ties.\n        for namespace, word_counts in namespace_word_counts.items():\n            sorted_word_counts = sorted(word_counts.items(),\n                                        key=lambda pair: (-pair[1],\n                                                          pair[0]))\n            for word, count in sorted_word_counts:\n                if count >= min_count:\n                    self.add_word_to_index(word, namespace)\n        self.is_fit = True\n\n    def add_word_to_index(self, word, namespace=""words""):\n        """"""\n        Adds `word` to the index, if it is not already present. Either way, we\n        return the index of the word.\n\n        Parameters\n        ----------\n        word: str\n            A string to be added to the indexer.\n\n        namespace: str\n            The string namespace to index the word under.\n\n        Returns\n        -------\n        index: int\n            The index of the input word in the namespace.\n        """"""\n        if not isinstance(word, str):\n            raise ValueError(""Expected word to be type ""\n                             ""str, found {} of type ""\n                             ""{}"".format(word, type(word)))\n        if word not in self.word_indices[namespace]:\n            index = len(self.word_indices[namespace])\n            self.word_indices[namespace][word] = index\n            self.reverse_word_indices[namespace][index] = word\n            return index\n        else:\n            return self.word_indices[namespace][word]\n\n    def words_in_index(self, namespace=""words""):\n        """"""\n        Returns a list of the words in the index for a\n        given namespace.\n\n        Parameters\n        ----------\n        namespace: str, optional (default=""words"")\n            The string namespace to return the list of words\n            in.\n\n        Returns\n        -------\n        word_list: List of str\n            A list of the words added to this DataIndexer.\n        """"""\n        return self.word_indices[namespace].keys()\n\n    def get_word_index(self, word, namespace=""words""):\n        """"""\n        Get the index of a word.\n\n        Parameters\n        ----------\n        word: str\n            A string to return the index of.\n\n        namespace: str, optional (default=""words"")\n            The string namespace to return the list of words\n            in.\n\n        Returns\n        -------\n        index: int\n            The index of the input word if it is in the index, or the index\n            corresponding to the OOV token if it is not.\n        """"""\n        if not isinstance(word, str):\n            raise ValueError(""Expected word to be type ""\n                             ""str, found {} of type ""\n                             ""{}"".format(word, type(word)))\n        if word in self.word_indices[namespace]:\n            return self.word_indices[namespace][word]\n        else:\n            return self.word_indices[namespace][self._oov_token]\n\n    def get_word_from_index(self, index, namespace=""words""):\n        """"""\n        Get the word corresponding to an input index, for a\n        given namespace.\n\n        Parameters\n        ----------\n        index: int\n            The int index to retrieve the word from.\n\n        namespace: str, optional (default=""words"")\n            The string namespace to return the list of words\n            in.\n\n        Returns\n        -------\n        word: str\n            The string word occupying the input index.\n        """"""\n        if not isinstance(index, int):\n            raise ValueError(""Expected index to be type ""\n                             ""int, found {} of type ""\n                             ""{}"".format(index, type(index)))\n        return self.reverse_word_indices[namespace][index]\n\n    def get_vocab_size(self, namespace=""words""):\n        """"""\n        Get the number of words in a namespace.\n\n        Parameters\n        ----------\n        namespace: str, optional (default=""words"")\n            The string namespace to return the list of words\n            in.\n\n        Returns\n        -------\n        vocab_size: int\n            The number of words added to this DataIndexer.\n        """"""\n        return len(self.word_indices[namespace])\n'"
duplicate_questions/data/data_manager.py,0,"b'import logging\nimport numpy as np\nfrom itertools import islice\n\nfrom .data_indexer import DataIndexer\nfrom .dataset import TextDataset\n\nlogger = logging.getLogger(__name__)\n\n\nclass DataManager():\n    """"""\n    The goal of this class is to act as a centralized place\n    to do high-level operations on your data (i.e. loading them from filename\n    to NumPy arrays).\n    """"""\n\n    def __init__(self, instance_type):\n        self.data_indexer = DataIndexer()\n        self.instance_type = instance_type\n        self.data_indexer_fitted = False\n        self.training_data_max_lengths = {}\n\n    @staticmethod\n    def get_batch_generator(get_instance_generator, batch_size):\n        """"""\n        Convenience function that, when called, produces a generator that yields\n        individual instances as numpy arrays into a generator\n        that yields batches of instances.\n\n        Parameters\n        ----------\n        instance_generator: numpy array generator\n            The instance_generator should be an infinite generator that outputs\n            individual training instances (as numpy arrays in this codebase,\n            but any iterable works). The expected format is:\n            ((input0, input1,...), (target0, target1, ...))\n\n        batch_size: int, optional\n            The size of each batch. Depending on how many\n            instances there are in the dataset, the last batch\n            may have less instances.\n\n        Returns\n        -------\n        output: returns a tuple of 2 tuples\n            The expected return schema is:\n            ((input0, input1, ...), (target0, target1, ...),\n            where each of ""input*"" and ""target*"" are numpy arrays.\n            The number of rows in each input and target numpy array\n            should be the same as the batch size.\n        """"""\n\n        # batched_instances is a list of batch_size instances, where each\n        # instance is a tuple ((inputs), targets)\n        instance_generator = get_instance_generator()\n        batched_instances = list(islice(instance_generator, batch_size))\n        while batched_instances:\n            # Take the batched instances and create a batch from it.\n            # The batch is a tuple ((inputs), targets), where (inputs)\n            # can be (inputs0, inputs1, etc...). each of ""inputs*"" and\n            # ""targets"" are numpy arrays.\n            flattened = ([ins[0] for ins in batched_instances],\n                         [ins[1] for ins in batched_instances])\n            flattened_inputs, flattened_targets = flattened\n            batch_inputs = tuple(map(np.array, tuple(zip(*flattened_inputs))))\n            batch_targets = tuple(map(np.array, tuple(zip(*flattened_targets))))\n            yield batch_inputs, batch_targets\n            batched_instances = list(islice(instance_generator, batch_size))\n\n    def get_train_data_from_file(self, filenames, min_count=1,\n                                 max_instances=None,\n                                 max_lengths=None, pad=True, mode=""word""):\n        """"""\n        Given a filename or list of filenames, return a generator for producing\n        individual instances of data ready for use in a model read from those\n        file(s).\n\n        Given a string path to a file in the format accepted by the instance,\n        we fit the data_indexer word dictionary on it. Next, we use this\n        DataIndexer to convert the instance into IndexedInstances (replacing\n        words with integer indices).\n\n        This function returns a function to construct generators that take\n        these IndexedInstances, pads them to the appropriate lengths (either the\n        maximum lengths in the dataset, or lengths specified in the constructor),\n        and then converts them to NumPy arrays suitable for training with\n        instance.as_training_data. The generator yields one instance at a time,\n        represented as tuples of (inputs, labels).\n\n        Parameters\n        ----------\n        filenames: List[str]\n            A collection of filenames to read the specific self.instance_type\n            from, line by line.\n\n        min_count: int, default=1\n            The minimum number of times a word must occur in order\n            to be indexed.\n\n        max_instances: int, default=None\n            If not None, the maximum number of instances to produce as\n            training data. If necessary, we will truncate the dataset.\n            Useful for debugging and making sure things work with small\n            amounts of data.\n\n        max_lengths: dict from str to int, default=None\n            If not None, the max length of a sequence in a given dimension.\n            The keys for this dict must be in the same format as\n            the instances\' get_lengths() function. These are the lengths\n            that the instances are padded or truncated to.\n\n        pad: boolean, default=True\n            If True, pads or truncates the instances to either the input\n            max_lengths or max_lengths across the train filenames. If False,\n            no padding or truncation is applied.\n\n        mode: str, optional (default=""word"")\n            String describing whether to return the word-level representations,\n            character-level representations, or both. One of ""word"",\n            ""character"", or ""word+character""\n\n        Returns\n        -------\n        output: returns a function to construct a train data generator\n            This returns a function that can be called to produce a tuple of\n            (instance generator, train_set_size). The instance generator\n            outputs instances as generated by the as_training_data function\n            of the underlying instance class. The train_set_size is the number\n            of instances in the train set, which can be used to initialize a\n            progress bar.\n        """"""\n        if self.data_indexer_fitted:\n            raise ValueError(""You have already called get_train_data from ""\n                             ""this DataManager, so you cannnot do it again. ""\n                             ""If you want to train on multiple datasets, pass ""\n                             ""in a list of files."")\n        logger.info(""Getting training data from {}"".format(filenames))\n        training_dataset = TextDataset.read_from_file(filenames,\n                                                      self.instance_type)\n        if max_instances:\n            logger.info(""Truncating the training dataset ""\n                        ""to {} instances"".format(max_instances))\n            training_dataset = training_dataset.truncate(max_instances)\n\n        training_dataset_size = len(training_dataset.instances)\n\n        # Since this is data for training, we fit the data indexer\n        logger.info(""Fitting data indexer word ""\n                    ""dictionary, min_count is {}."".format(min_count))\n        self.data_indexer.fit_word_dictionary(training_dataset,\n                                              min_count=min_count)\n        self.data_indexer_fitted = True\n\n        # With our fitted data indexer, we convert the dataset\n        # from string tokens to numeric int indices.\n        logger.info(""Indexing dataset"")\n        indexed_training_dataset = training_dataset.to_indexed_dataset(\n            self.data_indexer)\n\n        # We now need to check if the user specified max_lengths for\n        # the instance, and accordingly truncate or pad if applicable. If\n        # max_lengths is None for a given string key, we assume that no\n        # truncation is to be done and the max lengths should be read from the\n        # instances.\n        if not pad and max_lengths:\n            raise ValueError(""Passed in max_lengths {}, but set pad to false. ""\n                             ""Did you mean to do this?"".format(max_lengths))\n\n        # Get max lengths from the dataset\n        dataset_max_lengths = indexed_training_dataset.max_lengths()\n        logger.info(""Instance max lengths {}"".format(dataset_max_lengths))\n        max_lengths_to_use = dataset_max_lengths\n        if pad:\n            # If the user set max lengths, iterate over the\n            # dictionary provided and verify that they did not\n            # pass any keys to truncate that are not in the instance.\n            if max_lengths is not None:\n                for input_dimension, length in max_lengths.items():\n                    if input_dimension in dataset_max_lengths:\n                        max_lengths_to_use[input_dimension] = length\n                    else:\n                        raise ValueError(""Passed a value for the max_lengths ""\n                                         ""that does not exist in the ""\n                                         ""instance. Improper input length ""\n                                         ""dimension (key) we found was {}, ""\n                                         ""lengths dimensions in the instance ""\n                                         ""are {}"".format(\n                                             input_dimension,\n                                             dataset_max_lengths.keys()))\n            logger.info(""Padding lengths to ""\n                        ""length: {}"".format(str(max_lengths_to_use)))\n        self.training_data_max_lengths = max_lengths_to_use\n\n        # This is a hack to get the function to run the code above immediately,\n        # instead of doing the standard python generator lazy-ish evaluation.\n        # This is necessary to set the class variables ASAP.\n        def _get_train_data_generator():\n            for indexed_instance in indexed_training_dataset.instances:\n                # For each instance, we want to pad or truncate if applicable\n                if pad:\n                    indexed_instance.pad(max_lengths_to_use)\n                # Now, we want to take the instance and convert it into\n                # NumPy arrays suitable for training.\n                inputs, labels = indexed_instance.as_training_data(mode=mode)\n                yield inputs, labels\n        return _get_train_data_generator, training_dataset_size\n\n    def get_validation_data_from_file(self, filenames, max_instances=None,\n                                      max_lengths=None, pad=True, mode=""word""):\n        """"""\n        Given a filename or list of filenames, return a generator for producing\n        individual instances of data ready for use as validation data in a\n        model read from those file(s).\n\n        Given a string path to a file in the format accepted by the instance,\n        we use a data_indexer previously fitted on train data. Next, we use\n        this DataIndexer to convert the instance into IndexedInstances\n        (replacing words with integer indices).\n\n        This function returns a function to construct generators that take\n        these IndexedInstances, pads them to the appropriate lengths (either the\n        maximum lengths in the dataset, or lengths specified in the constructor),\n        and then converts them to NumPy arrays suitable for training with\n        instance.as_validation_data. The generator yields one instance at a time,\n        represented as tuples of (inputs, labels).\n\n        Parameters\n        ----------\n        filenames: List[str]\n            A collection of filenames to read the specific self.instance_type\n            from, line by line.\n\n        max_instances: int, default=None\n            If not None, the maximum number of instances to produce as\n            training data. If necessary, we will truncate the dataset.\n            Useful for debugging and making sure things work with small\n            amounts of data.\n\n        max_lengths: dict from str to int, default=None\n            If not None, the max length of a sequence in a given dimension.\n            The keys for this dict must be in the same format as\n            the instances\' get_lengths() function. These are the lengths\n            that the instances are padded or truncated to.\n\n        pad: boolean, default=True\n            If True, pads or truncates the instances to either the input\n            max_lengths or max_lengths used on the train filenames. If False,\n            no padding or truncation is applied.\n\n        mode: str, optional (default=""word"")\n            String describing whether to return the word-level representations,\n            character-level representations, or both. One of ""word"",\n            ""character"", or ""word+character""\n\n        Returns\n        -------\n        output: returns a function to construct a validation data generator\n            This returns a function that can be called to produce a tuple of\n            (instance generator, validation_set_size). The instance generator\n            outputs instances as generated by the as_validation_data function\n            of the underlying instance class. The validation_set_size is the number\n            of instances in the validation set, which can be used to initialize a\n            progress bar.\n        """"""\n        logger.info(""Getting validation data from {}"".format(filenames))\n        validation_dataset = TextDataset.read_from_file(filenames,\n                                                        self.instance_type)\n        if max_instances:\n            logger.info(""Truncating the validation dataset ""\n                        ""to {} instances"".format(max_instances))\n            validation_dataset = validation_dataset.truncate(max_instances)\n\n        validation_dataset_size = len(validation_dataset.instances)\n\n        # With our fitted data indexer, we we convert the dataset\n        # from string tokens to numeric int indices.\n        logger.info(""Indexing validation dataset with ""\n                    ""DataIndexer fit on train data."")\n        indexed_validation_dataset = validation_dataset.to_indexed_dataset(\n            self.data_indexer)\n\n        # We now need to check if the user specified max_lengths for\n        # the instance, and accordingly truncate or pad if applicable. If\n        # max_lengths is None for a given string key, we assume that no\n        # truncation is to be done and the max lengths should be taken from\n        # the train dataset.\n        if not pad and max_lengths:\n            raise ValueError(""Passed in max_lengths {}, but set pad to false. ""\n                             ""Did you mean to do this?"".format(max_lengths))\n        if pad:\n            # Get max lengths from the train dataset\n            training_data_max_lengths = self.training_data_max_lengths\n            logger.info(""Max lengths in training ""\n                        ""data: {}"".format(training_data_max_lengths))\n\n            max_lengths_to_use = training_data_max_lengths\n            # If the user set max lengths, iterate over the\n            # dictionary provided and verify that they did not\n            # pass any keys to truncate that are not in the instance.\n            if max_lengths is not None:\n                for input_dimension, length in max_lengths.items():\n                    if input_dimension in training_data_max_lengths:\n                        max_lengths_to_use[input_dimension] = length\n                    else:\n                        raise ValueError(""Passed a value for the max_lengths ""\n                                         ""that does not exist in the ""\n                                         ""instance. Improper input length ""\n                                         ""dimension (key) we found was {}, ""\n                                         ""lengths dimensions in the instance ""\n                                         ""are {}"".format(\n                                             input_dimension,\n                                             training_data_max_lengths.keys()))\n            logger.info(""Padding lengths to ""\n                        ""length: {}"".format(str(max_lengths_to_use)))\n\n        # This is a hack to get the function to run the code above immediately,\n        # instead of doing the standard python generator lazy-ish evaluation.\n        # This is necessary to set the class variables ASAP.\n        def _get_validation_data_generator():\n            for indexed_val_instance in indexed_validation_dataset.instances:\n                # For each instance, we want to pad or truncate if applicable\n                if pad:\n                    indexed_val_instance.pad(max_lengths_to_use)\n                # Now, we want to take the instance and convert it into\n                # NumPy arrays suitable for validation.\n                inputs, labels = indexed_val_instance.as_training_data(mode=mode)\n\n                yield inputs, labels\n        return _get_validation_data_generator, validation_dataset_size\n\n    def get_test_data_from_file(self, filenames, max_instances=None,\n                                max_lengths=None, pad=True, mode=""word""):\n        """"""\n        Given a filename or list of filenames, return a generator for producing\n        individual instances of data ready for use as model test data.\n\n        Given a string path to a file in the format accepted by the instance,\n        we use a data_indexer previously fitted on train data. Next, we use\n        this DataIndexer to convert the instance into IndexedInstances\n        (replacing words with integer indices).\n\n        This function returns a function to construct generators that take\n        these IndexedInstances, pads them to the appropriate lengths (either the\n        maximum lengths in the dataset, or lengths specified in the constructor),\n        and then converts them to NumPy arrays suitable for training with\n        instance.as_testinging_data. The generator yields one instance at a time,\n        represented as tuples of (inputs, labels).\n\n        Parameters\n        ----------\n        filenames: List[str]\n            A collection of filenames to read the specific self.instance_type\n            from, line by line.\n\n        max_instances: int, default=None\n            If not None, the maximum number of instances to produce as\n            training data. If necessary, we will truncate the dataset.\n            Useful for debugging and making sure things work with small\n            amounts of data.\n\n        max_lengths: dict from str to int, default=None\n            If not None, the max length of a sequence in a given dimension.\n            The keys for this dict must be in the same format as\n            the instances\' get_lengths() function. These are the lengths\n            that the instances are padded or truncated to.\n\n        pad: boolean, default=True\n            If True, pads or truncates the instances to either the input\n            max_lengths or max_lengths used on the train filenames. If False,\n            no padding or truncation is applied.\n\n        mode: str, optional (default=""word"")\n            String describing whether to return the word-level representations,\n            character-level representations, or both. One of ""word"",\n            ""character"", or ""word+character""\n\n        Returns\n        -------\n        output: returns a function to construct a test data generator\n            This returns a function that can be called to produce a tuple of\n            (instance generator, test_set_size). The instance generator\n            outputs instances as generated by the as_testing_data function\n            of the underlying instance class. The test_set_size is the number\n            of instances in the test set, which can be used to initialize a\n            progress bar.\n        """"""\n        logger.info(""Getting test data from {}"".format(filenames))\n        test_dataset = TextDataset.read_from_file(filenames,\n                                                  self.instance_type)\n        if max_instances:\n            logger.info(""Truncating the test dataset ""\n                        ""to {} instances"".format(max_instances))\n            test_dataset = test_dataset.truncate(max_instances)\n\n        test_dataset_size = len(test_dataset.instances)\n\n        # With our fitted data indexer, we we convert the dataset\n        # from string tokens to numeric int indices.\n        logger.info(""Indexing test dataset with DataIndexer ""\n                    ""fit on train data."")\n        indexed_test_dataset = test_dataset.to_indexed_dataset(\n            self.data_indexer)\n\n        # We now need to check if the user specified max_lengths for\n        # the instance, and accordingly truncate or pad if applicable. If\n        # max_lengths is None for a given string key, we assume that no\n        # truncation is to be done and the max lengths should be taken from\n        # the train dataset.\n        if not pad and max_lengths:\n            raise ValueError(""Passed in max_lengths {}, but set pad to false. ""\n                             ""Did you mean to do this?"".format(max_lengths))\n        if pad:\n            # Get max lengths from the train dataset\n            training_data_max_lengths = self.training_data_max_lengths\n            logger.info(""Max lengths in training ""\n                        ""data: {}"".format(training_data_max_lengths))\n\n            max_lengths_to_use = training_data_max_lengths\n            # If the user set max lengths, iterate over the\n            # dictionary provided and verify that they did not\n            # pass any keys to truncate that are not in the instance.\n            if max_lengths is not None:\n                for input_dimension, length in max_lengths.items():\n                    if input_dimension in training_data_max_lengths:\n                        max_lengths_to_use[input_dimension] = length\n                    else:\n                        raise ValueError(""Passed a value for the max_lengths ""\n                                         ""that does not exist in the ""\n                                         ""instance. Improper input length ""\n                                         ""dimension (key) we found was {}, ""\n                                         ""lengths dimensions in the instance ""\n                                         ""are {}"".format(\n                                             input_dimension,\n                                             training_data_max_lengths.keys()))\n            logger.info(""Padding lengths to ""\n                        ""length: {}"".format(str(max_lengths_to_use)))\n\n        # This is a hack to get the function to run the code above immediately,\n        # instead of doing the standard python generator lazy-ish evaluation.\n        # This is necessary to set the class variables ASAP.\n        def _get_test_data_generator():\n            for indexed_test_instance in indexed_test_dataset.instances:\n                # For each instance, we want to pad or truncate if applicable\n                if pad:\n                    indexed_test_instance.pad(max_lengths_to_use)\n                # Now, we want to take the instance and convert it into\n                # NumPy arrays suitable for validation.\n                inputs = indexed_test_instance.as_testing_data(mode=mode)\n                yield inputs\n\n        return _get_test_data_generator, test_dataset_size\n'"
duplicate_questions/data/dataset.py,0,"b'import codecs\nimport itertools\nimport logging\n\nfrom tqdm import tqdm\n\nfrom .instances.instance import Instance\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\nclass Dataset:\n    """"""\n    A collection of Instances. This base class has general methods that apply\n    to all collections of Instances. That basically is just methods that\n    operate on sets, like merging and truncating.\n\n    """"""\n    def __init__(self, instances):\n        """"""\n        Construct a dataset from a List of Instances.\n\n        Parameters\n        ----------\n        instances: List of Instance\n            The list of instances to build a Dataset from.\n        """"""\n        if not isinstance(instances, list):\n            raise ValueError(""Expected instances to be type ""\n                             ""List, found {} of type ""\n                             ""{}"".format(instances, type(instances)))\n        if not isinstance(instances[0], Instance):\n            raise ValueError(""Expected instances to be a List ""\n                             ""of Instances, but the first element ""\n                             ""of the input list was {} of type ""\n                             ""{}"".format(instances[0], type(instances[0])))\n        self.instances = instances\n\n    def merge(self, other):\n        """"""\n        Combine two datasets. If you call try to merge two Datasets of the same\n        subtype, you will end up with a Dataset of the same type (i.e., calling\n        IndexedDataset.merge() with another IndexedDataset will return an\n        IndexedDataset). If the types differ, this method currently raises an\n        error, because the underlying Instance objects are not currently type\n        compatible.\n        """"""\n        if type(self) is type(other):\n            return self.__class__(self.instances + other.instances)\n        else:\n            raise ValueError(""Cannot merge datasets with different types"")\n\n    def truncate(self, max_instances):\n        """"""\n        Truncate the dataset to a fixed size.\n\n        Parameters\n        ----------\n        max_instances: int\n            The maximum amount of instances allowed in this dataset. If there\n            are more instances than `max_instances` in this dataset, we\n            return a new dataset with a random subset of size `max_instances`.\n            If there are fewer than `max_instances` already, we just return\n            self.\n        """"""\n        if not isinstance(max_instances, int):\n            raise ValueError(""Expected max_instances to be type ""\n                             ""int, found {} of type ""\n                             ""{}"".format(max_instances, type(max_instances)))\n        if max_instances < 1:\n            raise ValueError(""max_instances must be at least 1""\n                             "", found {}"".format(max_instances))\n        if len(self.instances) <= max_instances:\n            return self\n        new_instances = [i for i in self.instances]\n        return self.__class__(new_instances[:max_instances])\n\n\nclass TextDataset(Dataset):\n    """"""\n    A Dataset of TextInstances, with a few helper methods. TextInstances aren\'t\n    useful for much until they\'ve been indexed. So this class just has methods\n    to read in data from a file and converting it into other kinds of Datasets.\n    """"""\n    def __init__(self, instances):\n        """"""\n        Construct a new TextDataset\n\n        Parameters\n        ----------\n        instances: List of TextInstance\n            A list of TextInstances to construct\n            the TextDataset from.\n        """"""\n        super(TextDataset, self).__init__(instances)\n\n    def to_indexed_dataset(self, data_indexer):\n        """"""\n        Converts the Dataset into an IndexedDataset, given a DataIndexer.\n\n        Parameters\n        ----------\n        data_indexer: DataIndexer\n            The DataIndexer to use in converting words to indices.\n        """"""\n        indexed_instances = [instance.to_indexed_instance(data_indexer) for\n                             instance in tqdm(self.instances)]\n        return IndexedDataset(indexed_instances)\n\n    @staticmethod\n    def read_from_file(filenames, instance_class):\n        """"""\n        Read a dataset (basically a list of Instances) from\n        a data file.\n\n        Parameters\n        ----------\n        filenames: str or List of str\n            The string filename from which to read the instances, or a List of\n            strings repesenting the files to pull instances from. If a string\n            is passed in, it is automatically converted to a single-element\n            list.\n\n        instance_class: Instance\n            The Instance class to create from these lines.\n\n        Returns\n        -------\n        text_dataset: TextDataset\n           A new TextDataset with the instances read from the file.\n        """"""\n        if isinstance(filenames, str):\n            filenames = [filenames]\n        # If filenames is not a list, throw an error. If it is a list,\n        # but the first element isn\'t a string, also throw an error.\n        if not isinstance(filenames, list) or not isinstance(filenames[0],\n                                                             str):\n            raise ValueError(""Expected filename to be a List of strings ""\n                             ""but was {} of type ""\n                             ""{}"".format(filenames, type(filenames)))\n        logger.info(""Reading files {} to a list of lines."".format(filenames))\n        lines = [x.strip() for filename in filenames\n                 for x in tqdm(codecs.open(filename,\n                                           ""r"", ""utf-8"").readlines())]\n        return TextDataset.read_from_lines(lines, instance_class)\n\n    @staticmethod\n    def read_from_lines(lines, instance_class):\n        """"""\n        Read a dataset (basically a list of Instances) from\n        a data file.\n\n        Parameters\n        ----------\n        lines: List of str\n            A list containing string representations of each\n            line in the file.\n\n        instance_class: Instance\n            The Instance class to create from these lines.\n\n        Returns\n        -------\n        text_dataset: TextDataset\n           A new TextDataset with the instances read from the list.\n        """"""\n        if not isinstance(lines, list):\n            raise ValueError(""Expected lines to be a list, ""\n                             ""but was {} of type ""\n                             ""{}"".format(lines, type(lines)))\n        if not isinstance(lines[0], str):\n            raise ValueError(""Expected lines to be a list of strings, ""\n                             ""but the first element of the list was {} ""\n                             ""of type {}"".format(lines[0], type(lines[0])))\n        logger.info(""Creating list of {} instances from ""\n                    ""list of lines."".format(instance_class))\n        instances = [instance_class.read_from_line(line) for line in tqdm(lines)]\n        labels = [(x.label, x) for x in instances]\n        labels.sort(key=lambda x: str(x[0]))\n        label_counts = [(label, len([x for x in group]))\n                        for label, group\n                        in itertools.groupby(labels, lambda x: x[0])]\n        label_count_str = str(label_counts)\n        if len(label_count_str) > 100:\n            label_count_str = label_count_str[:100] + \'...\'\n        logger.info(""Finished reading dataset; label counts: %s"",\n                    label_count_str)\n        return TextDataset(instances)\n\n\nclass IndexedDataset(Dataset):\n    """"""\n    A Dataset of IndexedInstances, with some helper methods.\n\n    IndexedInstances have text sequences replaced with lists of word indices,\n    and are thus able to be padded to consistent lengths and converted to\n    training inputs.\n    """"""\n    def __init__(self, instances):\n        super(IndexedDataset, self).__init__(instances)\n\n    def max_lengths(self):\n        max_lengths = {}\n        lengths = [instance.get_lengths() for instance in self.instances]\n        if not lengths:\n            return max_lengths\n        for key in lengths[0]:\n            max_lengths[key] = max(x[key] if key in x else 0 for x in lengths)\n        return max_lengths\n\n    def pad_instances(self, max_lengths=None):\n        """"""\n        Make all of the IndexedInstances in the dataset have the same length\n        by padding them (in the front) with zeros.\n\n        If max_length is given for a particular dimension, we will pad all\n        instances to that length (including left-truncating instances if\n        necessary). If not, we will find the longest instance and pad all\n        instances to that length. Note that max_lengths is a _List_, not an int\n        - there could be several dimensions on which we need to pad, depending\n        on what kind of instance we are dealing with.\n\n        This method _modifies_ the current object, it does not return a new\n        IndexedDataset.\n        """"""\n        # First we need to decide _how much_ to pad. To do that, we find the\n        # max length for all relevant padding decisions from the instances\n        # themselves. Then we check whether we were given a max length for a\n        # particular dimension. If we were, we use that instead of the\n        # instance-based one.\n        logger.info(""Getting max lengths from instances"")\n        instance_max_lengths = self.max_lengths()\n        logger.info(""Instance max lengths: %s"", str(instance_max_lengths))\n        lengths_to_use = {}\n        for key in instance_max_lengths:\n            if max_lengths and max_lengths[key] is not None:\n                lengths_to_use[key] = max_lengths[key]\n            else:\n                lengths_to_use[key] = instance_max_lengths[key]\n\n        logger.info(""Now actually padding instances to length: %s"",\n                    str(lengths_to_use))\n        for instance in tqdm(self.instances):\n            instance.pad(lengths_to_use)\n\n    def as_training_data(self, mode=""word""):\n        """"""\n        Takes each IndexedInstance and converts it into (inputs, labels),\n        according to the Instance\'s as_training_data() method. Note that\n        you might need to call numpy.asarray() on the results of this; we\n        don\'t do that for you, because the inputs might be complicated.\n\n        Parameters\n        ----------\n        mode: str, optional (default=""word"")\n            String describing whether to return the word-level representations,\n            character-level representations, or both. One of ""word"",\n            ""character"", or ""word+character""\n        """"""\n        inputs = []\n        labels = []\n        instances = self.instances\n        for instance in instances:\n            instance_inputs, label = instance.as_training_data(mode=mode)\n            inputs.append(instance_inputs)\n            labels.append(label)\n        return inputs, labels\n\n    def as_testing_data(self, mode=""word""):\n        """"""\n        Takes each IndexedInstance and converts it into inputs,\n        according to the Instance\'s as_testing_data() method. Note that\n        you might need to call numpy.asarray() on the results of this; we\n        don\'t do that for you, because the inputs might be complicated.\n\n        Parameters\n        ----------\n        mode: str, optional (default=""word"")\n            String describing whether to return the word-level representations,\n            character-level representations, or both. One of ""word"",\n            ""character"", or ""word+character""\n        """"""\n        inputs = []\n        instances = self.instances\n        for instance in instances:\n            instance_inputs, _ = instance.as_testing_data(mode=mode)\n            inputs.append(instance_inputs)\n        return inputs, []\n\n    def sort(self, reverse=True):\n        """"""\n        Sorts the list of IndexedInstances, in either ascending or descending order,\n        if the instances are IndexedSTSInstances\n\n        Parameters\n        ----------\n        reverse: boolean, optional (default=True)\n            Boolean which detrmines what reverse parameter is used in the\n            sorting function.\n        """"""\n        self.instances.sort(reverse=reverse)\n'"
duplicate_questions/data/embedding_manager.py,0,"b'import logging\n\nimport numpy as np\nfrom tqdm import tqdm\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\nclass EmbeddingManager():\n    """"""\n    An EmbeddingManager takes a DataIndexer fit on a train dataset,\n    and produces an embedding matrix with pretrained embedding files.\n    """"""\n    def __init__(self, data_indexer):\n        if not data_indexer.is_fit:\n            raise ValueError(""Input DataIndexer to EmbeddingManager ""\n                             ""must first be fit on input data."")\n        self.data_indexer = data_indexer\n\n    @staticmethod\n    def initialize_random_matrix(shape, scale=0.05, seed=0):\n        if len(shape) != 2:\n            raise ValueError(""Shape of embedding matrix must be 2D, ""\n                             ""got shape {}"".format(shape))\n        numpy_rng = np.random.RandomState(seed)\n        return numpy_rng.uniform(low=-scale, high=scale, size=shape)\n\n    def get_embedding_matrix(self, embedding_dim,\n                             pretrained_embeddings_file_path=None,\n                             pretrained_embeddings_dict=None,\n                             namespace=""words""):\n        """"""\n        Given an int embedding_dim, initialize an embedding matrix for each\n        index in the data_indexer. If a pretrained embeddings file is\n        provided, words in the data_indexer are assigned the vectors in the\n        file. If a pretrained embeddings dictionary (of word to vector) is\n        provided, words in the data_indexer are assigned to the vectors in\n        this dictionary. Else, the vectors are randomly initialized.\n\n        If pretrained_embeddings_file_path is provided, all rows must have\n        the same number of dimensions. If pretrained_embeddings_dict is\n        provided, all vectors must have the same number of dimensions.\n\n        If both pretrained_embeddings_file_path and\n        pretrained_embeddings_dict are provided, we will first check for\n        words in the pretrained_embeddings_dict, then in the\n        pretrained_embeddings_file, then randomly initialize if it is not\n        found in either.\n\n        Parameters\n        ----------\n        embedding_dim: int\n            The length of each word embedding (row in the matrix).\n\n        pretrained_embeddings_file_path: str, default=None\n            Path to text file, with tokens and their vectors.\n            The file should be formatted as [word] [dim 1] [dim 2] ...,\n            i.e. the word and each dimension should be separated by\n            a space.\n\n        pretrained_embeddings_dict: dictionary of str:ndarray, default=None\n            A dictionary of words and their vectors. Each word key should\n            be a string, and each vector value should be a NumPy array.\n\n        namespace: str, optional (default=""words"")\n            A string indicating the DataIndexer namespace to get the maximum\n            vocab size from.\n\n        Returns\n        -------\n        embedding_matrix: NumPy array\n            A NumPy array embedding_matrix of shape\n            (num_indices, embedding_dim) where embedding_matrix[i]\n            indicates the word vector for index i in the input DataIndexer.\n        """"""\n        if not isinstance(embedding_dim, int):\n            raise ValueError(""Expected input embedding_dim to be of ""\n                             ""type int, found {} of type {} ""\n                             ""instead."".format(embedding_dim,\n                                               type(embedding_dim)))\n        if (pretrained_embeddings_file_path and\n                not isinstance(pretrained_embeddings_file_path, str)):\n            raise ValueError(""Expected input ""\n                             ""pretrained_embeddings_file_path ""\n                             ""to be of type str, found {} of type ""\n                             ""{}"".format(\n                                 pretrained_embeddings_file_path,\n                                 type(pretrained_embeddings_file_path)))\n        if (pretrained_embeddings_dict and\n                not isinstance(pretrained_embeddings_dict, dict)):\n            raise ValueError(""Expected input pretrained_embeddings_dict ""\n                             ""to be of type dict, found {} of type ""\n                             ""{}"".format(\n                                 pretrained_embeddings_dict,\n                                 type(pretrained_embeddings_dict)))\n\n        embeddings_from_file = {}\n        if pretrained_embeddings_file_path:\n            logger.info(""Reading pretrained ""\n                        ""embeddings from {}"".format(\n                            pretrained_embeddings_file_path))\n            with open(pretrained_embeddings_file_path) as embedding_file:\n                for line in tqdm(embedding_file):\n                    fields = line.strip().split("" "")\n                    if len(fields) - 1 <= 1:\n                        raise ValueError(""Found embedding size of 1; ""\n                                         ""do you have a header?"")\n                    if embedding_dim != len(fields) - 1:\n                        raise ValueError(""Provided embedding_dim of {}, but ""\n                                         ""file at pretrained_embeddings_""\n                                         ""file_path has embeddings of ""\n                                         ""size {}"".format(embedding_dim,\n                                                          len(fields) - 1))\n                    word = fields[0]\n                    vector = np.array(fields[1:], dtype=\'float32\')\n                    embeddings_from_file[word] = vector\n\n        if pretrained_embeddings_dict:\n            # Check the all the values in the dictionary have the same\n            # length, and check that that length is the same as\n            # embedding_dim\n            embeddings_dict_dim = 0\n            for word, vector in pretrained_embeddings_dict.items():\n                if not embeddings_dict_dim:\n                    embeddings_dict_dim = len(vector)\n                if embeddings_dict_dim != len(vector):\n                    raise ValueError(""Found vectors of different lengths in ""\n                                     ""the pretrained_embeddings_dict."")\n            if embeddings_dict_dim != embedding_dim:\n                raise ValueError(""Provided embedding_dim of {}, but ""\n                                 ""pretrained_embeddings_dict has embeddings ""\n                                 ""of size {}"".format(embedding_dim,\n                                                     embeddings_dict_dim))\n\n        vocab_size = self.data_indexer.get_vocab_size(namespace=namespace)\n        # Build the embedding matrix\n        embedding_matrix = self.initialize_random_matrix((vocab_size,\n                                                          embedding_dim))\n        # The 2 here because there is no point in setting vectors\n        # for 0 (padding token) and 1 (OOV token)\n        for i in range(2, vocab_size):\n            # Get the word corresponding to the index\n            word = self.data_indexer.get_word_from_index(i)\n            # If we don\'t have a pre-trained vector for this word, just\n            # leave this row alone so the word has a random initialization.\n            if (pretrained_embeddings_dict and\n                    word in pretrained_embeddings_dict):\n                embedding_matrix[i] = pretrained_embeddings_dict[word]\n            else:\n                if embeddings_from_file and word in embeddings_from_file:\n                    embedding_matrix[i] = embeddings_from_file[word]\n        return embedding_matrix\n'"
duplicate_questions/models/__init__.py,0,b''
duplicate_questions/models/base_tf_model.py,18,"b'import logging\nimport math\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom ..data.data_manager import DataManager\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseTFModel:\n    """"""\n    This class is a base model class for Tensorflow that other Tensorflow\n    models should inherit from. It defines a unifying API for training and\n    prediction.\n\n    Parameters\n    ----------\n    mode: str\n        One of [train|predict], to indicate what you want the model to do.\n    """"""\n    def __init__(self, mode):\n        self.mode = mode\n        self.global_step = tf.get_variable(name=""global_step"",\n                                           shape=[],\n                                           dtype=\'int32\',\n                                           initializer=tf.constant_initializer(0),\n                                           trainable=False)\n\n        # Outputs from the model\n        self.y_pred = None\n        self.loss = None\n        self.accuracy = None\n\n        self.training_op = None\n        self.summary_op = None\n\n    def _create_placeholders(self):\n        raise NotImplementedError\n\n    def _build_forward(self):\n        raise NotImplementedError\n\n    def build_graph(self, seed=0):\n        """"""\n        Build the graph, ostensibly by setting up the placeholders and then\n        creating the forward pass.\n\n        Parameters\n        ----------\n        seed: int, optional (default=0)\n             The graph-level seed to use when building the graph.\n        """"""\n        logger.info(""Building graph..."")\n        tf.set_random_seed(seed)\n        self._create_placeholders()\n        self._build_forward()\n\n    def _get_train_feed_dict(self, batch):\n        """"""\n        Given a train batch from a batch generator,\n        return the appropriate feed_dict to pass to the\n        model during training.\n\n        Parameters\n        ----------\n        batch: tuple of NumPy arrays\n            A tuple of NumPy arrays containing the data necessary\n            to train.\n        """"""\n        raise NotImplementedError\n\n    def _get_validation_feed_dict(self, batch):\n        """"""\n        Given a validation batch from a batch generator,\n        return the appropriate feed_dict to pass to the\n        model during validation.\n\n        Parameters\n        ----------\n        batch: tuple of NumPy arrays\n            A tuple of NumPy arrays containing the data necessary\n            to validate.\n        """"""\n        raise NotImplementedError\n\n    def _get_test_feed_dict(self, batch):\n        """"""\n        Given a test batch from a batch generator,\n        return the appropriate feed_dict to pass to the\n        model during prediction.\n\n        Parameters\n        ----------\n        batch: tuple of NumPy arrays\n            A tuple of NumPy arrays containing the data necessary\n            to predict.\n        """"""\n        raise NotImplementedError\n\n    def train(self,\n              get_train_instance_generator, get_val_instance_generator,\n              batch_size, num_train_steps_per_epoch, num_epochs,\n              num_val_steps, save_path, log_path,\n              val_period=250, log_period=10, save_period=250,\n              max_ckpts_to_keep=10, patience=0):\n        """"""\n        Train the model.\n\n        Parameters\n        ----------\n        get_train_instance_generator: Function returning generator\n            This function should return a finite generator that produces\n            instances for use in training.\n\n        get_val_instance_generator: Function returning generator\n            This function should return a finite generator that produces\n            instances for use in validation.\n\n        batch_size: int\n            The number of instances per batch produced by the generator.\n\n        num_train_steps_per_epoch: int\n            The number of training steps after which an epoch has passed.\n\n        num_epochs: int\n            The number of epochs to train for.\n\n        num_val_steps: int\n            The number of batches generated by the validation batch generator.\n\n        save_path: str\n            The input path to the tensorflow Saver responsible for\n            checkpointing.\n\n        log_path: str\n            The input path to the tensorflow SummaryWriter responsible for\n            logging the progress.\n\n        val_period: int, optional (default=250)\n            Number of steps between each evaluation of performance on the\n            held-out validation set.\n\n        log_period: int, optional (default=10)\n            Number of steps between each summary op evaluation.\n\n        save_period: int, optional (default=250)\n            Number of steps between each model checkpoint.\n\n        max_ckpts_to_keep: int, optional (default=10)\n            The maximum number of model to checkpoints to keep.\n\n        patience: int, optional (default=0)\n            The number of epochs with no improvement in validation loss\n            after which training will be stopped.\n        """"""\n\n        global_step = 0\n        init_op = tf.global_variables_initializer()\n\n        gpu_options = tf.GPUOptions(allow_growth=True)\n        sess_config = tf.ConfigProto(gpu_options=gpu_options)\n        with tf.Session(config=sess_config) as sess:\n            sess.run(init_op)\n            # Set up the classes for logging to Tensorboard.\n            train_writer = tf.summary.FileWriter(log_path + ""/train"",\n                                                 sess.graph)\n            val_writer = tf.summary.FileWriter(log_path + ""/val"",\n                                               sess.graph)\n            # Set up a Saver for periodically serializing the model.\n            saver = tf.train.Saver(max_to_keep=max_ckpts_to_keep)\n\n            epoch_validation_losses = []\n            # Iterate over a generator that returns batches.\n            for epoch in tqdm(range(num_epochs), desc=""Epochs Completed""):\n                # Get a generator of train batches\n                train_batch_gen = DataManager.get_batch_generator(\n                    get_train_instance_generator, batch_size)\n                # Iterate over the generated batches\n                for train_batch in tqdm(train_batch_gen,\n                                        total=num_train_steps_per_epoch,\n                                        desc=""Train Batches Completed"",\n                                        leave=False):\n                    global_step = sess.run(self.global_step) + 1\n\n                    inputs, targets = train_batch\n                    feed_dict = self._get_train_feed_dict(train_batch)\n\n                    # Do a gradient update, and log results to Tensorboard\n                    # if necessary.\n                    if global_step % log_period == 0:\n                        # Record summary with gradient update\n                        train_loss, _, train_summary = sess.run(\n                            [self.loss, self.training_op, self.summary_op],\n                            feed_dict=feed_dict)\n                        train_writer.add_summary(train_summary, global_step)\n                    else:\n                        # Do a gradient update without recording anything.\n                        train_loss, _ = sess.run(\n                            [self.loss, self.training_op],\n                            feed_dict=feed_dict)\n\n                    if global_step % val_period == 0:\n                        # Evaluate on validation data\n                        val_acc, val_loss, val_summary = self._evaluate_on_validation(\n                            get_val_instance_generator=get_val_instance_generator,\n                            batch_size=batch_size,\n                            num_val_steps=num_val_steps,\n                            session=sess)\n                        val_writer.add_summary(val_summary, global_step)\n                    # Write a model checkpoint if necessary.\n                    if global_step % save_period == 0:\n                        saver.save(sess, save_path, global_step=global_step)\n\n                # End of the epoch, so save the model and check validation loss,\n                # stopping if applicable.\n                saver.save(sess, save_path, global_step=global_step)\n                val_acc, val_loss, val_summary = self._evaluate_on_validation(\n                    get_val_instance_generator=get_val_instance_generator,\n                    batch_size=batch_size,\n                    num_val_steps=num_val_steps,\n                    session=sess)\n                val_writer.add_summary(val_summary, global_step)\n\n                epoch_validation_losses.append(val_loss)\n\n                # Get the lowest validation loss, with regards to the patience\n                # threshold.\n                patience_val_losses = epoch_validation_losses[:-(patience + 1)]\n                if patience_val_losses:\n                    min_patience_val_loss = min(patience_val_losses)\n                else:\n                    min_patience_val_loss = math.inf\n                if min_patience_val_loss <= val_loss:\n                    # past loss was lower, so stop\n                    logger.info(""Validation loss of {} in last {} ""\n                                ""epochs, which is lower than current ""\n                                ""epoch validation loss of {}; stopping ""\n                                ""early."".format(min_patience_val_loss,\n                                                patience,\n                                                val_loss))\n                    break\n\n        # Done training!\n        logger.info(""Finished {} epochs!"".format(epoch + 1))\n\n    def predict(self, get_test_instance_generator, model_load_dir, batch_size,\n                num_test_steps=None):\n        """"""\n        Load a serialized model and use it for prediction on a test\n        set (from a finite generator).\n\n        Parameters\n        ----------\n        get_test_instance_generator: Function returning generator\n            This function should return a finite generator that produces instances\n            for use in training.\n\n        model_load_dir: str\n            Path to a directory with serialized tensorflow checkpoints for the\n            model to be run. The most recent checkpoint will be loaded and used\n            for prediction.\n\n        batch_size: int\n            The number of instances per batch produced by the generator.\n\n        num_test_steps: int\n            The number of steps (calculated by ceil(total # test examples / batch_size))\n            in testing. This does not have any effect on how much of the test data\n            is read; inference keeps going until the generator is exhausted. It\n            is used to set a total for the progress bar.\n        """"""\n        if num_test_steps is None:\n            logger.info(""num_test_steps is not set, pass in a value ""\n                        ""to show a progress bar."")\n\n        gpu_options = tf.GPUOptions(allow_growth=True)\n        sess_config = tf.ConfigProto(gpu_options=gpu_options)\n        with tf.Session(config=sess_config) as sess:\n            saver = tf.train.Saver()\n            logger.info(""Getting latest checkpoint in {}"".format(model_load_dir))\n            last_checkpoint = tf.train.latest_checkpoint(model_load_dir)\n            logger.info(""Attempting to load checkpoint at {}"".format(last_checkpoint))\n            saver.restore(sess, last_checkpoint)\n            logger.info(""Successfully loaded {}!"".format(last_checkpoint))\n\n            # Get a generator of test batches\n            test_batch_gen = DataManager.get_batch_generator(\n                get_test_instance_generator, batch_size)\n\n            y_pred = []\n            for batch in tqdm(test_batch_gen,\n                              total=num_test_steps,\n                              desc=""Test Batches Completed""):\n                feed_dict = self._get_test_feed_dict(batch)\n                y_pred_batch = sess.run(self.y_pred, feed_dict=feed_dict)\n                y_pred.append(y_pred_batch)\n            y_pred_flat = np.concatenate(y_pred, axis=0)\n        return y_pred_flat\n\n    def _evaluate_on_validation(self, get_val_instance_generator,\n                                batch_size,\n                                num_val_steps,\n                                session):\n        val_batch_gen = DataManager.get_batch_generator(\n            get_val_instance_generator, batch_size)\n        # Calculate the mean of the validation metrics\n        # over the validation set.\n        val_accuracies = []\n        val_losses = []\n        for val_batch in tqdm(val_batch_gen,\n                              total=num_val_steps,\n                              desc=""Validation Batches Completed"",\n                              leave=False):\n            feed_dict = self._get_validation_feed_dict(val_batch)\n            val_batch_acc, val_batch_loss = session.run(\n                [self.accuracy, self.loss],\n                feed_dict=feed_dict)\n\n            val_accuracies.append(val_batch_acc)\n            val_losses.append(val_batch_loss)\n\n        # Take the mean of the accuracies and losses.\n        # TODO/FIXME this assumes each batch is same shape, which\n        # is not necessarily true.\n        mean_val_accuracy = np.mean(val_accuracies)\n        mean_val_loss = np.mean(val_losses)\n\n        # Create a new Summary object with mean_val accuracy\n        # and mean_val_loss and add it to Tensorboard.\n        val_summary = tf.Summary(value=[\n            tf.Summary.Value(tag=""val_summaries/loss"",\n                             simple_value=mean_val_loss),\n            tf.Summary.Value(tag=""val_summaries/accuracy"",\n                             simple_value=mean_val_accuracy)])\n        return mean_val_accuracy, mean_val_loss, val_summary\n'"
duplicate_questions/util/pooling.py,4,"b'import tensorflow as tf\n\n\ndef mean_pool(input_tensor, sequence_length=None):\n    """"""\n    Given an input tensor (e.g., the outputs of a LSTM), do mean pooling\n    over the last dimension of the input.\n\n    For example, if the input was the output of a LSTM of shape\n    (batch_size, sequence length, hidden_dim), this would\n    calculate a mean pooling over the last dimension (taking the padding\n    into account, if provided) to output a tensor of shape\n    (batch_size, hidden_dim).\n\n    Parameters\n    ----------\n    input_tensor: Tensor\n        An input tensor, preferably the output of a tensorflow RNN.\n        The mean-pooled representation of this output will be calculated\n        over the last dimension.\n\n    sequence_length: Tensor, optional (default=None)\n        A tensor of dimension (batch_size, ) indicating the length\n        of the sequences before padding was applied.\n\n    Returns\n    -------\n    mean_pooled_output: Tensor\n        A tensor of one less dimension than the input, with the size of the\n        last dimension equal to the hidden dimension state size.\n    """"""\n    with tf.name_scope(""mean_pool""):\n        # shape (batch_size, sequence_length)\n        input_tensor_sum = tf.reduce_sum(input_tensor, axis=-2)\n\n        # If sequence_length is None, divide by the sequence length\n        # as indicated by the input tensor.\n        if sequence_length is None:\n            sequence_length = tf.shape(input_tensor)[-2]\n\n        # Expand sequence length from shape (batch_size,) to\n        # (batch_size, 1) for broadcasting to work.\n        expanded_sequence_length = tf.cast(tf.expand_dims(sequence_length, -1),\n                                           ""float32"") + 1e-08\n\n        # Now, divide by the length of each sequence.\n        # shape (batch_size, sequence_length)\n        mean_pooled_input = (input_tensor_sum /\n                             expanded_sequence_length)\n        return mean_pooled_input\n'"
duplicate_questions/util/rnn.py,6,"b'import tensorflow as tf\n\n\ndef last_relevant_output(output, sequence_length):\n    """"""\n    Given the outputs of a LSTM, get the last relevant output that\n    is not padding. We assume that the last 2 dimensions of the input\n    represent (sequence_length, hidden_size).\n\n    Parameters\n    ----------\n    output: Tensor\n        A tensor, generally the output of a tensorflow RNN.\n        The tensor index sequence_lengths+1 is selected for each\n        instance in the output.\n\n    sequence_length: Tensor\n        A tensor of dimension (batch_size, ) indicating the length\n        of the sequences before padding was applied.\n\n    Returns\n    -------\n    last_relevant_output: Tensor\n        The last relevant output (last element of the sequence), as retrieved\n        by the output Tensor and indicated by the sequence_length Tensor.\n    """"""\n    with tf.name_scope(""last_relevant_output""):\n        batch_size = tf.shape(output)[0]\n        max_length = tf.shape(output)[-2]\n        out_size = int(output.get_shape()[-1])\n        index = tf.range(0, batch_size) * max_length + (sequence_length - 1)\n        flat = tf.reshape(output, [-1, out_size])\n        relevant = tf.gather(flat, index)\n        return relevant\n'"
duplicate_questions/util/switchable_dropout_wrapper.py,4,"b'import tensorflow as tf\nfrom tensorflow.contrib.rnn import DropoutWrapper\n\n\nclass SwitchableDropoutWrapper(DropoutWrapper):\n    """"""\n    A wrapper of tensorflow.contrib.rnn.DropoutWrapper that does not apply\n    dropout if is_train is not True (dropout only in training).\n    """"""\n    def __init__(self, cell, is_train, input_keep_prob=1.0,\n                 output_keep_prob=1.0, seed=None):\n        super(SwitchableDropoutWrapper, self).__init__(\n            cell,\n            input_keep_prob=input_keep_prob,\n            output_keep_prob=output_keep_prob,\n            seed=seed)\n        self.is_train = is_train\n\n    def __call__(self, inputs, state, scope=None):\n        # Get the dropped-out outputs and state\n        outputs_do, new_state_do = super(SwitchableDropoutWrapper,\n                                         self).__call__(\n                                             inputs, state, scope=scope)\n        tf.get_variable_scope().reuse_variables()\n        # Get the un-dropped-out outputs and state\n        outputs, new_state = self._cell(inputs, state, scope)\n\n        # Set the outputs and state to be the dropped out version if we are\n        # training, and no dropout if we are not training.\n        outputs = tf.cond(self.is_train, lambda: outputs_do,\n                          lambda: outputs * (self._output_keep_prob))\n        if isinstance(state, tuple):\n            new_state = state.__class__(\n                *[tf.cond(self.is_train, lambda: new_state_do_i,\n                          lambda: new_state_i)\n                  for new_state_do_i, new_state_i in\n                  zip(new_state_do, new_state)])\n        else:\n            new_state = tf.cond(self.is_train, lambda: new_state_do,\n                                lambda: new_state)\n        return outputs, new_state\n'"
scripts/run_model/run_bimpm.py,0,"b'import argparse\nimport sys\nimport logging\nimport math\nimport numpy as np\nimport os\nimport pandas as pd\nimport pickle\nimport json\n\nsys.path.append(os.path.join(os.path.dirname(__file__), ""../../""))\nfrom duplicate_questions.data.data_manager import DataManager\nfrom duplicate_questions.data.embedding_manager import EmbeddingManager\nfrom duplicate_questions.data.instances.sts_instance import STSInstance\nfrom duplicate_questions.models.bimpm.bimpm import BiMPM\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    project_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir)\n\n    # Parse config arguments\n    argparser = argparse.ArgumentParser(\n        description=(""Run the Bilateral Multi-Perspective ""\n                     ""Matching (biMPM) model on the paraphrase ""\n                     ""identification task.""))\n    argparser.add_argument(""mode"", type=str,\n                           choices=[""train"", ""predict""],\n                           help=(""One of {train|predict}, to ""\n                                 ""indicate what you want the model to do. ""\n                                 ""If you pick \\""predict\\"", then you must also ""\n                                 ""supply the path to a pretrained model and ""\n                                 ""DataIndexer to load.""))\n    argparser.add_argument(""--model_load_dir"", type=str,\n                           help=(""The path to a directory with checkpoints to ""\n                                 ""load for evaluation or prediction. The ""\n                                 ""latest checkpoint will be loaded.""))\n    argparser.add_argument(""--dataindexer_load_path"", type=str,\n                           help=(""The path to the dataindexer fit on the ""\n                                 ""train data, so we can properly index the ""\n                                 ""test data for evaluation or prediction.""))\n    argparser.add_argument(""--train_file"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""data/processed/quora/""\n                                                ""train_cleaned_train_split.csv""),\n                           help=""Path to a file to train on."")\n    argparser.add_argument(""--val_file"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""data/processed/quora/""\n                                                ""train_cleaned_val_split.csv""),\n                           help=""Path to a file to monitor validation acc. on."")\n    argparser.add_argument(""--test_file"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""data/processed/quora/""\n                                                ""test_final.csv""))\n    argparser.add_argument(""--batch_size"", type=int, default=64,\n                           help=""Number of instances per batch."")\n    argparser.add_argument(""--num_epochs"", type=int, default=10,\n                           help=(""Number of epochs to perform in ""\n                                 ""training.""))\n    argparser.add_argument(""--early_stopping_patience"", type=int, default=0,\n                           help=(""number of epochs with no validation ""\n                                 ""accuracy improvement after which training ""\n                                 ""will be stopped""))\n    argparser.add_argument(""--num_sentence_words"", type=int, default=100,\n                           help=(""The maximum length of a sentence. Longer ""\n                                 ""sentences will be truncated, and shorter ""\n                                 ""ones will be padded.""))\n    argparser.add_argument(""--num_word_characters"", type=int, default=10,\n                           help=(""The maximum length of a word. Longer ""\n                                 ""words will be truncated, and shorter ""\n                                 ""ones will be padded.""))\n    argparser.add_argument(""--word_embedding_dim"", type=int, default=300,\n                           help=""Dimensionality of the word embedding layer"")\n    argparser.add_argument(""--pretrained_word_embeddings_file_path"", type=str,\n                           help=""Path to a file with pretrained word embeddings."",\n                           default=os.path.join(project_dir,\n                                                ""data/external/"",\n                                                ""glove.6B.300d.txt""))\n    argparser.add_argument(""--char_embedding_dim"", type=int, default=20,\n                           help=""Dimensionality of the char embedding layer"")\n    argparser.add_argument(""--fine_tune_embeddings"", action=""store_true"",\n                           help=(""Whether to train the embedding layer ""\n                                 ""(if True), or keep it fixed (False).""))\n    argparser.add_argument(""--char_rnn_hidden_size"", type=int, default=50,\n                           help=(""The output dimension of the character ""\n                                 ""encoder RNN.""))\n    argparser.add_argument(""--context_rnn_hidden_size"", type=int, default=100,\n                           help=(""The output dimension of the context ""\n                                 ""encoding RNN.""))\n    argparser.add_argument(""--aggregation_rnn_hidden_size"", type=int, default=100,\n                           help=(""The output dimension of the aggregation ""\n                                 ""encoding RNN.""))\n    argparser.add_argument(""--dropout_ratio"", type=float, default=0.1,\n                           help=(""The proportion of RNN outputs to ""\n                                 ""drop out.""))\n    argparser.add_argument(""--log_period"", type=int, default=10,\n                           help=(""Number of steps between each summary ""\n                                 ""op evaluation.""))\n    argparser.add_argument(""--val_period"", type=int, default=2500,\n                           help=(""Number of steps between each evaluation of ""\n                                 ""validation performance.""))\n    argparser.add_argument(""--log_dir"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""logs/""),\n                           help=(""Directory to save logs to.""))\n    argparser.add_argument(""--save_period"", type=int, default=2500,\n                           help=(""Number of steps between each ""\n                                 ""model checkpoint""))\n    argparser.add_argument(""--save_dir"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""models/""),\n                           help=(""Directory to save model checkpoints to.""))\n    argparser.add_argument(""--run_id"", type=str, required=True,\n                           help=(""Identifying run ID for this run. If ""\n                                 ""predicting, you probably want this ""\n                                 ""to be the same as the train run_id""))\n    argparser.add_argument(""--model_name"", type=str, required=True,\n                           help=(""Identifying model name for this run. If""\n                                 ""predicting, you probably want this ""\n                                 ""to be the same as the train run_id""))\n    argparser.add_argument(""--reweight_predictions_for_kaggle"", action=""store_true"",\n                           help=(""Only relevant when predicting. Whether to ""\n                                 ""reweight the prediction probabilities to ""\n                                 ""account for class proportion discrepancy ""\n                                 ""between train and test.""))\n\n    config = argparser.parse_args()\n\n    model_name = config.model_name\n    run_id = config.run_id\n    mode = config.mode\n\n    # Get the data.\n    batch_size = config.batch_size\n    if mode == ""train"":\n        # Read the train data from a file, and use it to index the\n        # validation data\n        data_manager = DataManager(STSInstance)\n        num_sentence_words = config.num_sentence_words\n        num_word_characters = config.num_word_characters\n        get_train_data_gen, train_data_size = data_manager.get_train_data_from_file(\n            [config.train_file],\n            max_lengths={""num_sentence_words"": num_sentence_words,\n                         ""num_word_characters"": num_word_characters},\n            mode=""word+character"")\n        get_val_data_gen, val_data_size = data_manager.get_validation_data_from_file(\n            [config.val_file],\n            max_lengths={""num_sentence_words"": num_sentence_words,\n                         ""num_word_characters"": num_word_characters},\n            mode=""word+character"")\n    else:\n        # Load the fitted DataManager, and use it to index the test data\n        logger.info(""Loading pickled DataManager from {}"".format(\n            config.dataindexer_load_path))\n        data_manager = pickle.load(open(config.dataindexer_load_path, ""rb""))\n        get_test_data_gen, test_data_size = data_manager.get_test_data_from_file(\n            [config.test_file],\n            mode=""word+character"")\n\n    vars(config)[""word_vocab_size""] = data_manager.data_indexer.get_vocab_size()\n    vars(config)[""char_vocab_size""] = data_manager.data_indexer.get_vocab_size(\n        namespace=""characters"")\n\n    # Log the run parameters.\n    log_dir = config.log_dir\n    log_path = os.path.join(log_dir, model_name, run_id.zfill(2))\n    logger.info(""Writing logs to {}"".format(log_path))\n    if not os.path.exists(log_path):\n        logger.info(""log path {} does not exist, ""\n                    ""creating it"".format(log_path))\n        os.makedirs(log_path)\n    params_path = os.path.join(log_path, mode + ""params.json"")\n    logger.info(""Writing params to {}"".format(params_path))\n    with open(params_path, \'w\') as params_file:\n        json.dump(vars(config), params_file, indent=4)\n\n    # Get the embeddings.\n    embedding_manager = EmbeddingManager(data_manager.data_indexer)\n    word_embedding_matrix = embedding_manager.get_embedding_matrix(\n        config.word_embedding_dim,\n        config.pretrained_word_embeddings_file_path)\n    vars(config)[""word_embedding_matrix""] = word_embedding_matrix\n    char_embedding_matrix = embedding_manager.get_embedding_matrix(\n        config.char_embedding_dim, namespace=""characters"")\n    vars(config)[""char_embedding_matrix""] = char_embedding_matrix\n\n    # Initialize the model.\n    model = BiMPM(vars(config))\n    model.build_graph()\n\n    if mode == ""train"":\n        # Train the model.\n        num_epochs = config.num_epochs\n        num_train_steps_per_epoch = int(math.ceil(train_data_size / batch_size))\n        num_val_steps = int(math.ceil(val_data_size / batch_size))\n        log_period = config.log_period\n        val_period = config.val_period\n\n        save_period = config.save_period\n        save_dir = os.path.join(config.save_dir, model_name, run_id.zfill(2) + ""/"")\n        save_path = os.path.join(save_dir, model_name + ""-"" + run_id.zfill(2))\n\n        logger.info(""Checkpoints will be written to {}"".format(save_dir))\n        if not os.path.exists(save_dir):\n            logger.info(""save path {} does not exist, ""\n                        ""creating it"".format(save_dir))\n            os.makedirs(save_dir)\n\n        logger.info(""Saving fitted DataManager to {}"".format(save_dir))\n        data_manager_pickle_name = ""{}-{}-DataManager.pkl"".format(model_name,\n                                                                  run_id.zfill(2))\n        pickle.dump(data_manager,\n                    open(os.path.join(save_dir, data_manager_pickle_name), ""wb""))\n        patience = config.early_stopping_patience\n        model.train(get_train_instance_generator=get_train_data_gen,\n                    get_val_instance_generator=get_val_data_gen,\n                    batch_size=batch_size,\n                    num_train_steps_per_epoch=num_train_steps_per_epoch,\n                    num_epochs=num_epochs,\n                    num_val_steps=num_val_steps,\n                    save_path=save_path,\n                    log_path=log_path,\n                    log_period=log_period,\n                    val_period=val_period,\n                    save_period=save_period,\n                    patience=patience)\n    else:\n        # Predict with the model\n        model_load_dir = config.model_load_dir\n        num_test_steps = int(math.ceil(test_data_size / batch_size))\n        # Numpy array of shape (num_test_examples, 2)\n        raw_predictions = model.predict(get_test_instance_generator=get_test_data_gen,\n                                        model_load_dir=model_load_dir,\n                                        batch_size=batch_size,\n                                        num_test_steps=num_test_steps)\n        # Remove the first column, so we\'re left with just the probabilities\n        # that a question is a duplicate.\n        is_duplicate_probabilities = np.delete(raw_predictions, 0, 1)\n\n        # The class balance between kaggle train and test seems different.\n        # This edits prediction probability to account for the discrepancy.\n        # See: https://www.kaggle.com/c/quora-question-pairs/discussion/31179\n        if config.reweight_predictions_for_kaggle:\n            positive_weight = 0.165 / 0.37\n            negative_weight = (1 - 0.165) / (1 - 0.37)\n            is_duplicate_probabilities = ((positive_weight * is_duplicate_probabilities) /\n                                          (positive_weight * is_duplicate_probabilities +\n                                           negative_weight *\n                                           (1 - is_duplicate_probabilities)))\n\n        # Write the predictions to an output submission file\n        output_predictions_path = os.path.join(log_path, model_name + ""-"" +\n                                               run_id.zfill(2) +\n                                               ""-output_predictions.csv"")\n        logger.info(""Writing predictions to {}"".format(output_predictions_path))\n        is_duplicate_df = pd.DataFrame(is_duplicate_probabilities)\n        is_duplicate_df.to_csv(output_predictions_path, index_label=""test_id"",\n                               header=[""is_duplicate""])\n\n\nif __name__ == ""__main__"":\n    logging.basicConfig(format=""%(asctime)s - %(levelname)s ""\n                        ""- %(name)s - %(message)s"",\n                        level=logging.INFO)\n    main()\n'"
scripts/run_model/run_siamese.py,0,"b'import argparse\nimport sys\nimport logging\nimport math\nimport numpy as np\nimport os\nimport pandas as pd\nimport pickle\nimport json\n\nsys.path.append(os.path.join(os.path.dirname(__file__), ""../../""))\nfrom duplicate_questions.data.data_manager import DataManager\nfrom duplicate_questions.data.embedding_manager import EmbeddingManager\nfrom duplicate_questions.data.instances.sts_instance import STSInstance\nfrom duplicate_questions.models.siamese_bilstm.siamese_bilstm import SiameseBiLSTM\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    project_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir)\n\n    # Parse config arguments\n    argparser = argparse.ArgumentParser(\n        description=(""Run a baseline Siamese BiLSTM model ""\n                     ""for paraphrase identification.""))\n    argparser.add_argument(""mode"", type=str,\n                           choices=[""train"", ""predict""],\n                           help=(""One of {train|predict}, to ""\n                                 ""indicate what you want the model to do. ""\n                                 ""If you pick \\""predict\\"", then you must also ""\n                                 ""supply the path to a pretrained model and ""\n                                 ""DataIndexer to load.""))\n    argparser.add_argument(""--model_load_dir"", type=str,\n                           help=(""The path to a directory with checkpoints to ""\n                                 ""load for evaluation or prediction. The ""\n                                 ""latest checkpoint will be loaded.""))\n    argparser.add_argument(""--dataindexer_load_path"", type=str,\n                           help=(""The path to the dataindexer fit on the ""\n                                 ""train data, so we can properly index the ""\n                                 ""test data for evaluation or prediction.""))\n    argparser.add_argument(""--train_file"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""data/processed/quora/""\n                                                ""train_cleaned_train_split.csv""),\n                           help=""Path to a file to train on."")\n    argparser.add_argument(""--val_file"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""data/processed/quora/""\n                                                ""train_cleaned_val_split.csv""),\n                           help=""Path to a file to monitor validation acc. on."")\n    argparser.add_argument(""--test_file"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""data/processed/quora/""\n                                                ""test_final.csv""))\n    argparser.add_argument(""--batch_size"", type=int, default=128,\n                           help=""Number of instances per batch."")\n    argparser.add_argument(""--num_epochs"", type=int, default=10,\n                           help=(""Number of epochs to perform in ""\n                                 ""training.""))\n    argparser.add_argument(""--early_stopping_patience"", type=int, default=0,\n                           help=(""number of epochs with no validation ""\n                                 ""accuracy improvement after which training ""\n                                 ""will be stopped""))\n    argparser.add_argument(""--num_sentence_words"", type=int, default=30,\n                           help=(""The maximum length of a sentence. Longer ""\n                                 ""sentences will be truncated, and shorter ""\n                                 ""ones will be padded.""))\n    argparser.add_argument(""--word_embedding_dim"", type=int, default=300,\n                           help=""Dimensionality of the word embedding layer"")\n    argparser.add_argument(""--pretrained_embeddings_file_path"", type=str,\n                           help=""Path to a file with pretrained embeddings."",\n                           default=os.path.join(project_dir,\n                                                ""data/external/"",\n                                                ""glove.6B.300d.txt""))\n    argparser.add_argument(""--fine_tune_embeddings"", action=""store_true"",\n                           help=(""Whether to train the embedding layer ""\n                                 ""(if True), or keep it fixed (False).""))\n    argparser.add_argument(""--rnn_hidden_size"", type=int, default=256,\n                           help=(""The output dimension of the RNN.""))\n    argparser.add_argument(""--share_encoder_weights"", action=""store_true"",\n                           help=(""Whether to use the same encoder on both ""\n                                 ""input sentences (thus sharing weights), ""\n                                 ""or a different one for each sentence""))\n    argparser.add_argument(""--rnn_output_mode"", type=str, default=""last"",\n                           choices=[""mean_pool"", ""last""],\n                           help=(""How to calculate the final sentence ""\n                                 ""representation from the RNN outputs. ""\n                                 ""\\""mean_pool\\"" indicates that the outputs ""\n                                 ""will be averaged (with respect to padding), ""\n                                 ""and \\""last\\"" indicates that the last ""\n                                 ""relevant output will be used as the ""\n                                 ""sentence representation.""))\n    argparser.add_argument(""--output_keep_prob"", type=float, default=1.0,\n                           help=(""The proportion of RNN outputs to keep, ""\n                                 ""where the rest are dropped out.""))\n    argparser.add_argument(""--log_period"", type=int, default=10,\n                           help=(""Number of steps between each summary ""\n                                 ""op evaluation.""))\n    argparser.add_argument(""--val_period"", type=int, default=250,\n                           help=(""Number of steps between each evaluation of ""\n                                 ""validation performance.""))\n    argparser.add_argument(""--log_dir"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""logs/""),\n                           help=(""Directory to save logs to.""))\n    argparser.add_argument(""--save_period"", type=int, default=250,\n                           help=(""Number of steps between each ""\n                                 ""model checkpoint""))\n    argparser.add_argument(""--save_dir"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""models/""),\n                           help=(""Directory to save model checkpoints to.""))\n    argparser.add_argument(""--run_id"", type=str, required=True,\n                           help=(""Identifying run ID for this run. If ""\n                                 ""predicting, you probably want this ""\n                                 ""to be the same as the train run_id""))\n    argparser.add_argument(""--model_name"", type=str, required=True,\n                           help=(""Identifying model name for this run. If""\n                                 ""predicting, you probably want this ""\n                                 ""to be the same as the train run_id""))\n    argparser.add_argument(""--reweight_predictions_for_kaggle"", action=""store_true"",\n                           help=(""Only relevant when predicting. Whether to ""\n                                 ""reweight the prediction probabilities to ""\n                                 ""account for class proportion discrepancy ""\n                                 ""between train and test.""))\n\n    config = argparser.parse_args()\n\n    model_name = config.model_name\n    run_id = config.run_id\n    mode = config.mode\n\n    # Get the data.\n    batch_size = config.batch_size\n    if mode == ""train"":\n        # Read the train data from a file, and use it to index the validation data\n        data_manager = DataManager(STSInstance)\n        num_sentence_words = config.num_sentence_words\n        get_train_data_gen, train_data_size = data_manager.get_train_data_from_file(\n            [config.train_file],\n            max_lengths={""num_sentence_words"": num_sentence_words})\n        get_val_data_gen, val_data_size = data_manager.get_validation_data_from_file(\n            [config.val_file], max_lengths={""num_sentence_words"": num_sentence_words})\n    else:\n        # Load the fitted DataManager, and use it to index the test data\n        logger.info(""Loading pickled DataManager ""\n                    ""from {}"".format(config.dataindexer_load_path))\n        data_manager = pickle.load(open(config.dataindexer_load_path, ""rb""))\n        test_data_gen, test_data_size = data_manager.get_test_data_from_file(\n            [config.test_file])\n\n    vars(config)[""word_vocab_size""] = data_manager.data_indexer.get_vocab_size()\n\n    # Log the run parameters.\n    log_dir = config.log_dir\n    log_path = os.path.join(log_dir, model_name, run_id.zfill(2))\n    logger.info(""Writing logs to {}"".format(log_path))\n    if not os.path.exists(log_path):\n        logger.info(""log path {} does not exist, ""\n                    ""creating it"".format(log_path))\n        os.makedirs(log_path)\n    params_path = os.path.join(log_path, mode + ""params.json"")\n    logger.info(""Writing params to {}"".format(params_path))\n    with open(params_path, \'w\') as params_file:\n        json.dump(vars(config), params_file, indent=4)\n\n    # Get the embeddings.\n    embedding_manager = EmbeddingManager(data_manager.data_indexer)\n    embedding_matrix = embedding_manager.get_embedding_matrix(\n        config.word_embedding_dim,\n        config.pretrained_embeddings_file_path)\n    vars(config)[""word_embedding_matrix""] = embedding_matrix\n\n    # Initialize the model.\n    model = SiameseBiLSTM(vars(config))\n    model.build_graph()\n\n    if mode == ""train"":\n        # Train the model.\n        num_epochs = config.num_epochs\n        num_train_steps_per_epoch = int(math.ceil(train_data_size / batch_size))\n        num_val_steps = int(math.ceil(val_data_size / batch_size))\n        log_period = config.log_period\n        val_period = config.val_period\n\n        save_period = config.save_period\n        save_dir = os.path.join(config.save_dir, model_name, run_id.zfill(2) + ""/"")\n        save_path = os.path.join(save_dir, model_name + ""-"" + run_id.zfill(2))\n\n        logger.info(""Checkpoints will be written to {}"".format(save_dir))\n        if not os.path.exists(save_dir):\n            logger.info(""save path {} does not exist, ""\n                        ""creating it"".format(save_dir))\n            os.makedirs(save_dir)\n\n        logger.info(""Saving fitted DataManager to {}"".format(save_dir))\n        data_manager_pickle_name = ""{}-{}-DataManager.pkl"".format(model_name,\n                                                                  run_id.zfill(2))\n        pickle.dump(data_manager,\n                    open(os.path.join(save_dir, data_manager_pickle_name), ""wb""))\n\n        patience = config.early_stopping_patience\n        model.train(get_train_instance_generator=get_train_data_gen,\n                    get_val_instance_generator=get_val_data_gen,\n                    batch_size=batch_size,\n                    num_train_steps_per_epoch=num_train_steps_per_epoch,\n                    num_epochs=num_epochs,\n                    num_val_steps=num_val_steps,\n                    save_path=save_path,\n                    log_path=log_path,\n                    log_period=log_period,\n                    val_period=val_period,\n                    save_period=save_period,\n                    patience=patience)\n    else:\n        # Predict with the model\n        model_load_dir = config.model_load_dir\n        num_test_steps = int(math.ceil(test_data_size / batch_size))\n        # Numpy array of shape (num_test_examples, 2)\n        raw_predictions = model.predict(get_test_instance_generator=test_data_gen,\n                                        model_load_dir=model_load_dir,\n                                        batch_size=batch_size,\n                                        num_test_steps=num_test_steps)\n        # Remove the first column, so we\'re left with just the probabilities\n        # that a question is a duplicate.\n        is_duplicate_probabilities = np.delete(raw_predictions, 0, 1)\n\n        # The class balance between kaggle train and test seems different.\n        # This edits prediction probability to account for the discrepancy.\n        # See: https://www.kaggle.com/c/quora-question-pairs/discussion/31179\n        if config.reweight_predictions_for_kaggle:\n            positive_weight = 0.165 / 0.37\n            negative_weight = (1 - 0.165) / (1 - 0.37)\n            is_duplicate_probabilities = ((positive_weight * is_duplicate_probabilities) /\n                                          (positive_weight * is_duplicate_probabilities +\n                                           negative_weight *\n                                           (1 - is_duplicate_probabilities)))\n\n        # Write the predictions to an output submission file\n        output_predictions_path = os.path.join(log_path, model_name + ""-"" +\n                                               run_id.zfill(2) +\n                                               ""-output_predictions.csv"")\n        logger.info(""Writing predictions to {}"".format(output_predictions_path))\n        is_duplicate_df = pd.DataFrame(is_duplicate_probabilities)\n        is_duplicate_df.to_csv(output_predictions_path, index_label=""test_id"",\n                               header=[""is_duplicate""])\n\n\nif __name__ == ""__main__"":\n    logging.basicConfig(format=""%(asctime)s - %(levelname)s ""\n                        ""- %(name)s - %(message)s"",\n                        level=logging.INFO)\n    main()\n'"
scripts/run_model/run_siamese_matching_bilstm.py,0,"b'import argparse\nimport sys\nimport logging\nimport math\nimport numpy as np\nimport os\nimport pandas as pd\nimport pickle\nimport json\n\nsys.path.append(os.path.join(os.path.dirname(__file__), ""../../""))\nfrom duplicate_questions.data.data_manager import DataManager\nfrom duplicate_questions.data.embedding_manager import EmbeddingManager\nfrom duplicate_questions.data.instances.sts_instance import STSInstance\nfrom duplicate_questions.models.siamese_bilstm.siamese_matching_bilstm import (\n    SiameseMatchingBiLSTM\n)\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    project_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir)\n\n    # Parse config arguments\n    argparser = argparse.ArgumentParser(\n        description=(""Run the Siamese BiLSTM model with an added ""\n                     ""matching layer for paraphase detection.""))\n    argparser.add_argument(""mode"", type=str,\n                           choices=[""train"", ""predict""],\n                           help=(""One of {train|predict}, to ""\n                                 ""indicate what you want the model to do. ""\n                                 ""If you pick \\""predict\\"", then you must also ""\n                                 ""supply the path to a pretrained model and ""\n                                 ""DataIndexer to load.""))\n    argparser.add_argument(""--model_load_dir"", type=str,\n                           help=(""The path to a directory with checkpoints to ""\n                                 ""load for evaluation or prediction. The ""\n                                 ""latest checkpoint will be loaded.""))\n    argparser.add_argument(""--dataindexer_load_path"", type=str,\n                           help=(""The path to the DataIndexer fit on the ""\n                                 ""train data, so we can properly index the ""\n                                 ""test data for evaluation or prediction.""))\n    argparser.add_argument(""--train_file"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""data/processed/quora/""\n                                                ""train_cleaned_train_split.csv""),\n                           help=""Path to a file to train on."")\n    argparser.add_argument(""--val_file"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""data/processed/quora/""\n                                                ""train_cleaned_val_split.csv""),\n                           help=""Path to a file to monitor validation acc. on."")\n    argparser.add_argument(""--test_file"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""data/processed/quora/""\n                                                ""test_final.csv""))\n    argparser.add_argument(""--batch_size"", type=int, default=128,\n                           help=""Number of instances per batch."")\n    argparser.add_argument(""--num_epochs"", type=int, default=10,\n                           help=(""Number of epochs to perform in ""\n                                 ""training.""))\n    argparser.add_argument(""--early_stopping_patience"", type=int, default=0,\n                           help=(""number of epochs with no validation ""\n                                 ""accuracy improvement after which training ""\n                                 ""will be stopped""))\n    argparser.add_argument(""--num_sentence_words"", type=int, default=30,\n                           help=(""The maximum length of a sentence. Longer ""\n                                 ""sentences will be truncated, and shorter ""\n                                 ""ones will be padded.""))\n    argparser.add_argument(""--word_embedding_dim"", type=int, default=300,\n                           help=""Dimensionality of the word embedding layer"")\n    argparser.add_argument(""--pretrained_embeddings_file_path"", type=str,\n                           help=""Path to a file with pretrained embeddings."",\n                           default=os.path.join(project_dir,\n                                                ""data/external/"",\n                                                ""glove.6B.300d.txt""))\n    argparser.add_argument(""--fine_tune_embeddings"", action=""store_true"",\n                           help=(""Whether to train the embedding layer ""\n                                 ""(if True), or keep it fixed (False).""))\n    argparser.add_argument(""--rnn_hidden_size"", type=int, default=256,\n                           help=(""The output dimension of the RNN.""))\n    argparser.add_argument(""--share_encoder_weights"", action=""store_true"",\n                           help=(""Whether to use the same encoder on both ""\n                                 ""input sentences (thus sharing weights), ""\n                                 ""or a different one for each sentence""))\n    argparser.add_argument(""--output_keep_prob"", type=float, default=1.0,\n                           help=(""The proportion of RNN outputs to keep, ""\n                                 ""where the rest are dropped out.""))\n    argparser.add_argument(""--log_period"", type=int, default=10,\n                           help=(""Number of steps between each summary ""\n                                 ""op evaluation.""))\n    argparser.add_argument(""--val_period"", type=int, default=250,\n                           help=(""Number of steps between each evaluation of ""\n                                 ""validation performance.""))\n    argparser.add_argument(""--log_dir"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""logs/""),\n                           help=(""Directory to save logs to.""))\n    argparser.add_argument(""--save_period"", type=int, default=250,\n                           help=(""Number of steps between each ""\n                                 ""model checkpoint""))\n    argparser.add_argument(""--save_dir"", type=str,\n                           default=os.path.join(project_dir,\n                                                ""models/""),\n                           help=(""Directory to save model checkpoints to.""))\n    argparser.add_argument(""--run_id"", type=str, required=True,\n                           help=(""Identifying run ID for this run. If ""\n                                 ""predicting, you probably want this ""\n                                 ""to be the same as the train run_id""))\n    argparser.add_argument(""--model_name"", type=str, required=True,\n                           help=(""Identifying model name for this run. If""\n                                 ""predicting, you probably want this ""\n                                 ""to be the same as the train run_id""))\n    argparser.add_argument(""--reweight_predictions_for_kaggle"", action=""store_true"",\n                           help=(""Only relevant when predicting. Whether to ""\n                                 ""reweight the prediction probabilities to ""\n                                 ""account for class proportion discrepancy ""\n                                 ""between train and test.""))\n\n    config = argparser.parse_args()\n\n    model_name = config.model_name\n    run_id = config.run_id\n    mode = config.mode\n\n    # Get the data.\n    batch_size = config.batch_size\n    if mode == ""train"":\n        # Read the train data from a file, and use it to index the\n        # validation data\n        data_manager = DataManager(STSInstance)\n        num_sentence_words = config.num_sentence_words\n        get_train_data_gen, train_data_size = data_manager.get_train_data_from_file(\n            [config.train_file], max_lengths={""num_sentence_words"": num_sentence_words})\n        get_val_data_gen, val_data_size = data_manager.get_validation_data_from_file(\n            [config.val_file], max_lengths={""num_sentence_words"": num_sentence_words})\n    else:\n        # Load the fitted DataManager, and use it to index the test data\n        logger.info(""Loading pickled DataManager from {}"".format(\n            config.dataindexer_load_path))\n        data_manager = pickle.load(open(config.dataindexer_load_path, ""rb""))\n        get_test_data_gen, test_data_size = data_manager.get_test_data_from_file(\n            [config.test_file])\n\n    vars(config)[""word_vocab_size""] = data_manager.data_indexer.get_vocab_size()\n\n    # Log the run parameters.\n    log_dir = config.log_dir\n    log_path = os.path.join(log_dir, model_name, run_id.zfill(2))\n    logger.info(""Writing logs to {}"".format(log_path))\n    if not os.path.exists(log_path):\n        logger.info(""log path {} does not exist, ""\n                    ""creating it"".format(log_path))\n        os.makedirs(log_path)\n    params_path = os.path.join(log_path, mode + ""params.json"")\n    logger.info(""Writing params to {}"".format(params_path))\n    with open(params_path, \'w\') as params_file:\n        json.dump(vars(config), params_file, indent=4)\n\n    # Get the embeddings.\n    embedding_manager = EmbeddingManager(data_manager.data_indexer)\n    embedding_matrix = embedding_manager.get_embedding_matrix(\n        config.word_embedding_dim,\n        config.pretrained_embeddings_file_path)\n    vars(config)[""word_embedding_matrix""] = embedding_matrix\n\n    # Initialize the model.\n    model = SiameseMatchingBiLSTM(vars(config))\n    model.build_graph()\n\n    if mode == ""train"":\n        # Train the model.\n        num_epochs = config.num_epochs\n        num_train_steps_per_epoch = int(math.ceil(train_data_size / batch_size))\n        num_val_steps = int(math.ceil(val_data_size / batch_size))\n        log_period = config.log_period\n        val_period = config.val_period\n\n        save_period = config.save_period\n        save_dir = os.path.join(config.save_dir, model_name, run_id.zfill(2) + ""/"")\n        save_path = os.path.join(save_dir, model_name + ""-"" + run_id.zfill(2))\n\n        logger.info(""Checkpoints will be written to {}"".format(save_dir))\n        if not os.path.exists(save_dir):\n            logger.info(""save path {} does not exist, ""\n                        ""creating it"".format(save_dir))\n            os.makedirs(save_dir)\n\n        logger.info(""Saving fitted DataManager to {}"".format(save_dir))\n        data_manager_pickle_name = ""{}-{}-DataManager.pkl"".format(model_name,\n                                                                  run_id.zfill(2))\n        pickle.dump(data_manager,\n                    open(os.path.join(save_dir, data_manager_pickle_name), ""wb""))\n\n        patience = config.early_stopping_patience\n        model.train(get_train_instance_generator=get_train_data_gen,\n                    get_val_instance_generator=get_val_data_gen,\n                    batch_size=batch_size,\n                    num_train_steps_per_epoch=num_train_steps_per_epoch,\n                    num_epochs=num_epochs,\n                    num_val_steps=num_val_steps,\n                    save_path=save_path,\n                    log_path=log_path,\n                    log_period=log_period,\n                    val_period=val_period,\n                    save_period=save_period,\n                    patience=patience)\n    else:\n        # Predict with the model\n        model_load_dir = config.model_load_dir\n        num_test_steps = int(math.ceil(test_data_size / batch_size))\n        # Numpy array of shape (num_test_examples, 2)\n        raw_predictions = model.predict(get_test_instance_generator=get_test_data_gen,\n                                        model_load_dir=model_load_dir,\n                                        batch_size=batch_size,\n                                        num_test_steps=num_test_steps)\n\n        # Remove the first column, so we\'re left with just the probabilities\n        # that a question is a duplicate.\n        is_duplicate_probabilities = np.delete(raw_predictions, 0, 1)\n\n        # The class balance between kaggle train and test seems different.\n        # This edits prediction probability to account for the discrepancy.\n        # See: https://www.kaggle.com/c/quora-question-pairs/discussion/31179\n        if config.reweight_predictions_for_kaggle:\n            positive_weight = 0.165 / 0.37\n            negative_weight = (1 - 0.165) / (1 - 0.37)\n            is_duplicate_probabilities = ((positive_weight * is_duplicate_probabilities) /\n                                          (positive_weight * is_duplicate_probabilities +\n                                           negative_weight *\n                                           (1 - is_duplicate_probabilities)))\n\n        # Write the predictions to an output submission file\n        output_predictions_path = os.path.join(log_path, model_name + ""-"" +\n                                               run_id.zfill(2) +\n                                               ""-output_predictions.csv"")\n        logger.info(""Writing predictions to {}"".format(output_predictions_path))\n        is_duplicate_df = pd.DataFrame(is_duplicate_probabilities)\n        is_duplicate_df.to_csv(output_predictions_path, index_label=""test_id"",\n                               header=[""is_duplicate""])\n\n\nif __name__ == ""__main__"":\n    logging.basicConfig(format=""%(asctime)s - %(levelname)s ""\n                        ""- %(name)s - %(message)s"",\n                        level=logging.INFO)\n    main()\n'"
tests/common/__init__.py,0,b''
tests/common/test_case.py,1,"b'# pylint: disable=invalid-name,protected-access\nfrom unittest import TestCase\nimport codecs\nimport logging\nimport os\nimport shutil\nimport tensorflow as tf\n\n\nclass DuplicateTestCase(TestCase):\n    TEST_DIR = \'./TMP_TEST/\'\n    TRAIN_FILE = TEST_DIR + \'train_file\'\n    VALIDATION_FILE = TEST_DIR + \'validation_file\'\n    TEST_FILE = TEST_DIR + \'test_file\'\n    VECTORS_FILE = TEST_DIR + \'vectors_file\'\n\n    def setUp(self):\n        logging.basicConfig(format=(\'%(asctime)s - %(levelname)s - \'\n                                    \'%(name)s - %(message)s\'),\n                            level=logging.INFO)\n        os.makedirs(self.TEST_DIR, exist_ok=True)\n\n    def tearDown(self):\n        tf.reset_default_graph()\n        shutil.rmtree(self.TEST_DIR)\n\n    def write_duplicate_questions_train_file(self):\n        with codecs.open(self.TRAIN_FILE, \'w\', \'utf-8\') as dupe_train_file:\n            dupe_train_file.write(""\\""1\\"",\\""1\\"",\\""2\\"",\\""question1\\"",""\n                                  ""\\""question2 question3pad\\"",\\""0\\""\\n"")\n            dupe_train_file.write(""\\""2\\"",\\""3\\"",\\""4\\"",\\""question4\\"",""\n                                  ""\\""question5\\"",\\""1\\""\\n"")\n            dupe_train_file.write(""\\""3\\"",\\""5\\"",\\""6\\"",\\""question6\\"",""\n                                  ""\\""question7\\"",\\""0\\""\\n"")\n\n    def write_duplicate_questions_validation_file(self):\n        with codecs.open(self.VALIDATION_FILE, \'w\',\n                         \'utf-8\') as dupe_val_file:\n            dupe_val_file.write(""\\""1\\"",\\""7\\"",\\""8\\"",\\""question1\\"",""\n                                ""\\""question2 question8\\"",\\""0\\""\\n"")\n            dupe_val_file.write(""\\""2\\"",\\""9\\"",\\""10\\"",\\""question9\\"",""\n                                ""\\""question10\\"",\\""1\\""\\n"")\n            dupe_val_file.write(""\\""3\\"",\\""11\\"",\\""12\\"",\\""question6\\"",""\n                                ""\\""question7 question11 question12\\"",""\n                                ""\\""0\\""\\n"")\n\n    def write_duplicate_questions_test_file(self):\n        with codecs.open(self.TEST_FILE, \'w\', \'utf-8\') as dupe_test_file:\n            dupe_test_file.write(""\\""1\\"",\\""question1 questionunk1 question1\\"",""\n                                 ""\\""questionunk2\\""\\n"")\n            dupe_test_file.write(""\\""2\\"",\\""question3pad\\"",""\n                                 ""\\""question4 questionunk3\\""\\n"")\n            dupe_test_file.write(""\\""3\\"",\\""question5\\"",\\""question6\\""\\n"")\n\n    def write_vector_file(self):\n        with codecs.open(self.VECTORS_FILE, \'w\', \'utf-8\') as vectors_file:\n            vectors_file.write(""word1 0.0 1.1 0.2\\n"")\n            vectors_file.write(""word2 0.1 0.4 -4.0\\n"")\n'"
tests/data/__init__.py,0,b''
tests/data/test_data_indexer.py,0,"b'from duplicate_questions.data.data_indexer import DataIndexer\nfrom duplicate_questions.data.dataset import TextDataset\nfrom duplicate_questions.data.instances.sts_instance import STSInstance\n\nfrom ..common.test_case import DuplicateTestCase\n\n\nclass TestDataIndexer(DuplicateTestCase):\n    def test_fit_word_dictionary_respects_min_count(self):\n        instance = STSInstance(""a a a a b"", ""b c c c"", 1)\n        dataset = TextDataset([instance])\n        data_indexer = DataIndexer()\n        data_indexer.fit_word_dictionary(dataset, min_count=4)\n        assert \'a\' in data_indexer.words_in_index()\n        assert \'b\' not in data_indexer.words_in_index()\n        assert \'c\' not in data_indexer.words_in_index()\n\n        data_indexer = DataIndexer()\n        data_indexer.fit_word_dictionary(dataset, min_count=1)\n        assert \'a\' in data_indexer.words_in_index()\n        assert \'b\' in data_indexer.words_in_index()\n        assert \'c\' in data_indexer.words_in_index()\n\n    def test_add_word_to_index_gives_consistent_results(self):\n        data_indexer = DataIndexer()\n        initial_vocab_size = data_indexer.get_vocab_size()\n        word_index = data_indexer.add_word_to_index(""word"")\n        assert ""word"" in data_indexer.words_in_index()\n        assert data_indexer.get_word_index(""word"") == word_index\n        assert data_indexer.get_word_from_index(word_index) == ""word""\n        assert data_indexer.get_vocab_size() == initial_vocab_size + 1\n\n        # Now add it again, and make sure nothing changes.\n        data_indexer.add_word_to_index(""word"")\n        assert ""word"" in data_indexer.words_in_index()\n        assert data_indexer.get_word_index(""word"") == word_index\n        assert data_indexer.get_word_from_index(word_index) == ""word""\n        assert data_indexer.get_vocab_size() == initial_vocab_size + 1\n\n    def test_exceptions(self):\n        data_indexer = DataIndexer()\n        instance = STSInstance(""a a a a b"", ""b c c c"", 1)\n        dataset = TextDataset([instance])\n        with self.assertRaises(ValueError):\n            data_indexer.fit_word_dictionary(dataset, ""3"")\n        with self.assertRaises(ValueError):\n            data_indexer.fit_word_dictionary(""not a dataset"", 3)\n        with self.assertRaises(ValueError):\n            data_indexer.add_word_to_index(3)\n        with self.assertRaises(ValueError):\n            data_indexer.get_word_index(3)\n        with self.assertRaises(ValueError):\n            data_indexer.get_word_from_index(""3"")\n'"
tests/data/test_data_manager.py,0,"b'from numpy.testing import assert_allclose\nimport numpy as np\nfrom overrides import overrides\n\nfrom duplicate_questions.data.data_manager import DataManager\nfrom duplicate_questions.data.instances.sts_instance import STSInstance\n\nfrom ..common.test_case import DuplicateTestCase\n\n\nclass TestDataManagerTrain(DuplicateTestCase):\n    @overrides\n    def setUp(self):\n        super(TestDataManagerTrain, self).setUp()\n        self.write_duplicate_questions_train_file()\n        self.data_manager = DataManager(STSInstance)\n\n    def test_get_train_data_default(self):\n        get_train_gen, train_size = self.data_manager.get_train_data_from_file(\n            [self.TRAIN_FILE])\n        assert train_size == 3\n        train_gen = get_train_gen()\n        inputs1, labels1 = train_gen.__next__()\n        assert_allclose(inputs1[0], np.array([2, 0]))\n        assert_allclose(inputs1[1], np.array([3, 4]))\n        assert_allclose(labels1[0], np.array([1, 0]))\n\n        inputs2, labels2 = train_gen.__next__()\n        assert_allclose(inputs2[0], np.array([5, 0]))\n        assert_allclose(inputs2[1], np.array([6, 0]))\n        assert_allclose(labels2[0], np.array([0, 1]))\n\n        inputs3, labels3 = train_gen.__next__()\n        assert_allclose(inputs3[0], np.array([7, 0]))\n        assert_allclose(inputs3[1], np.array([8, 0]))\n        assert_allclose(labels3[0], np.array([1, 0]))\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            train_gen.__next__()\n\n        # Test that we can make a new train generator\n        new_train_gen = get_train_gen()\n        # Verify that the new and old generator are not the same object\n        assert new_train_gen != train_gen\n        new_inputs1, new_labels1 = new_train_gen.__next__()\n        assert_allclose(new_inputs1, inputs1)\n        assert_allclose(new_labels1, labels1)\n        new_inputs2, new_labels2 = new_train_gen.__next__()\n        assert_allclose(new_inputs2, inputs2)\n        assert_allclose(new_labels2, labels2)\n        new_inputs3, new_labels3 = new_train_gen.__next__()\n        assert_allclose(new_inputs3, inputs3)\n        assert_allclose(new_labels3, labels3)\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            new_train_gen.__next__()\n\n    def test_get_train_data_default_character(self):\n        get_train_gen, train_size = self.data_manager.get_train_data_from_file(\n            [self.TRAIN_FILE], mode=""character"")\n        train_gen = get_train_gen()\n        assert train_size == 3\n        inputs1, labels1 = train_gen.__next__()\n        assert_allclose(inputs1[0], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 10, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs1[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 11, 0, 0, 0],\n                                              [6, 9, 2, 7, 8, 3, 5, 4, 12, 19, 17, 18]]))\n        assert_allclose(labels1[0], np.array([1, 0]))\n\n        inputs2, labels2 = train_gen.__next__()\n        assert_allclose(inputs2[0], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 13, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs2[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 14, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(labels2[0], np.array([0, 1]))\n\n        inputs3, labels3 = train_gen.__next__()\n        assert_allclose(inputs3[0], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 15, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs3[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 16, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(labels3[0], np.array([1, 0]))\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            train_gen.__next__()\n\n    def test_get_train_data_default_word_and_character(self):\n        get_train_gen, train_size = self.data_manager.get_train_data_from_file(\n            [self.TRAIN_FILE], mode=""word+character"")\n        train_gen = get_train_gen()\n        assert train_size == 3\n        inputs1, labels1 = train_gen.__next__()\n        assert_allclose(inputs1[0], np.array([2, 0]))\n        assert_allclose(inputs1[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 10, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs1[2], np.array([3, 4]))\n        assert_allclose(inputs1[3], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 11, 0, 0, 0],\n                                              [6, 9, 2, 7, 8, 3, 5, 4, 12, 19, 17, 18]]))\n        assert_allclose(labels1[0], np.array([1, 0]))\n\n        inputs2, labels2 = train_gen.__next__()\n        assert_allclose(inputs2[0], np.array([5, 0]))\n        assert_allclose(inputs2[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 13, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs2[2], np.array([6, 0]))\n        assert_allclose(inputs2[3], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 14, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(labels2[0], np.array([0, 1]))\n\n        inputs3, labels3 = train_gen.__next__()\n        assert_allclose(inputs3[0], np.array([7, 0]))\n        assert_allclose(inputs3[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 15, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs3[2], np.array([8, 0]))\n        assert_allclose(inputs3[3], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 16, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(labels3[0], np.array([1, 0]))\n\n        # Should cycle back to the start\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            train_gen.__next__()\n\n    def test_get_train_data_pad_with_max_lens(self):\n        get_train_gen, train_size = self.data_manager.get_train_data_from_file(\n            [self.TRAIN_FILE],\n            max_lengths={""num_sentence_words"": 1})\n        train_gen = get_train_gen()\n        assert train_size == 3\n        inputs1, labels1 = train_gen.__next__()\n        assert_allclose(inputs1[0], np.array([2]))\n        assert_allclose(inputs1[1], np.array([3]))\n        assert_allclose(labels1[0], np.array([1, 0]))\n\n        inputs2, labels2 = train_gen.__next__()\n        assert_allclose(inputs2[0], np.array([5]))\n        assert_allclose(inputs2[1], np.array([6]))\n        assert_allclose(labels2[0], np.array([0, 1]))\n\n        inputs3, labels3 = train_gen.__next__()\n        assert_allclose(inputs3[0], np.array([7]))\n        assert_allclose(inputs3[1], np.array([8]))\n        assert_allclose(labels3[0], np.array([1, 0]))\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            train_gen.__next__()\n\n    def test_get_train_data_with_max_instances(self):\n        get_train_gen, train_size = self.data_manager.get_train_data_from_file(\n            [self.TRAIN_FILE],\n            max_instances=2)\n        train_gen = get_train_gen()\n        assert train_size == 2\n        inputs1, labels1 = train_gen.__next__()\n        assert_allclose(inputs1[0], np.array([2, 0]))\n        assert_allclose(inputs1[1], np.array([3, 4]))\n        assert_allclose(labels1[0], np.array([1, 0]))\n\n        inputs2, labels2 = train_gen.__next__()\n        assert_allclose(inputs2[0], np.array([5, 0]))\n        assert_allclose(inputs2[1], np.array([6, 0]))\n        assert_allclose(labels2[0], np.array([0, 1]))\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            train_gen.__next__()\n\n    def test_get_train_data_errors(self):\n        with self.assertRaises(ValueError):\n            self.data_manager.get_train_data_from_file(\n                [self.TRAIN_FILE],\n                max_lengths={""num_sentence_words"": 1},\n                pad=False)\n        with self.assertRaises(ValueError):\n            self.data_manager.get_train_data_from_file(\n                [self.TRAIN_FILE],\n                max_lengths={""some wrong key"": 1})\n\n    def test_get_train_data_no_pad(self):\n        get_train_gen, train_size = self.data_manager.get_train_data_from_file(\n            [self.TRAIN_FILE],\n            pad=False)\n        train_gen = get_train_gen()\n        assert train_size == 3\n        inputs1, labels1 = train_gen.__next__()\n        assert_allclose(inputs1[0], np.array([2]))\n        assert_allclose(inputs1[1], np.array([3, 4]))\n        assert_allclose(labels1[0], np.array([1, 0]))\n\n        inputs2, labels2 = train_gen.__next__()\n        assert_allclose(inputs2[0], np.array([5]))\n        assert_allclose(inputs2[1], np.array([6]))\n        assert_allclose(labels2[0], np.array([0, 1]))\n\n        inputs3, labels3 = train_gen.__next__()\n        assert_allclose(inputs3[0], np.array([7]))\n        assert_allclose(inputs3[1], np.array([8]))\n        assert_allclose(labels3[0], np.array([1, 0]))\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            train_gen.__next__()\n\n    def test_generate_train_batches(self):\n        get_train_gen, train_size = self.data_manager.get_train_data_from_file(\n            [self.TRAIN_FILE])\n        batch_gen = DataManager.get_batch_generator(get_train_gen, 2)\n        new_batch_gen = DataManager.get_batch_generator(get_train_gen, 2)\n\n        # Assert that the new generator is a different object\n        # than the old generator.\n        assert new_batch_gen != batch_gen\n        assert train_size == 3\n\n        first_batch = batch_gen.__next__()\n        new_first_batch = new_batch_gen.__next__()\n        inputs, labels = first_batch\n        new_inputs, new_labels = new_first_batch\n        assert len(inputs) == len(new_inputs) == 2\n        assert len(labels) == len(new_labels) == 1\n\n        # Ensure output matches ground truth\n        assert_allclose(inputs[0], np.array([[2, 0], [5, 0]]))\n        assert_allclose(inputs[1], np.array([[3, 4], [6, 0]]))\n        assert_allclose(labels[0], np.array([[1, 0], [0, 1]]))\n        # Ensure both generators produce same results.\n        assert_allclose(inputs[0], new_inputs[0])\n        assert_allclose(inputs[1], new_inputs[1])\n        assert_allclose(labels[0], labels[0])\n\n        second_batch = batch_gen.__next__()\n        new_second_batch = new_batch_gen.__next__()\n        inputs, labels = second_batch\n        new_inputs, new_labels = new_second_batch\n        assert len(inputs) == len(new_inputs) == 2\n        assert len(labels) == len(new_labels) == 1\n\n        # Ensure output matches ground truth\n        assert_allclose(inputs[0], np.array([[7, 0]]))\n        assert_allclose(inputs[1], np.array([[8, 0]]))\n        assert_allclose(labels[0], np.array([[1, 0]]))\n        # Ensure both generators produce same results.\n        assert_allclose(inputs[0], new_inputs[0])\n        assert_allclose(inputs[1], new_inputs[1])\n        assert_allclose(labels[0], labels[0])\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            batch_gen.__next__()\n            new_batch_gen.__next__()\n\n\nclass TestDataManagerValidation(DuplicateTestCase):\n    @overrides\n    def setUp(self):\n        super(TestDataManagerValidation, self).setUp()\n        self.write_duplicate_questions_train_file()\n        self.write_duplicate_questions_validation_file()\n        self.data_manager = DataManager(STSInstance)\n        self.data_manager.get_train_data_from_file([self.TRAIN_FILE])\n\n    def test_get_validation_data_default(self):\n        get_val_gen, val_size = self.data_manager.get_validation_data_from_file(\n            [self.VALIDATION_FILE])\n        assert val_size == 3\n        val_gen = get_val_gen()\n        inputs1, labels1 = val_gen.__next__()\n        assert_allclose(inputs1[0], np.array([2, 0]))\n        assert_allclose(inputs1[1], np.array([3, 1]))\n        assert_allclose(labels1[0], np.array([1, 0]))\n\n        inputs2, labels2 = val_gen.__next__()\n        assert_allclose(inputs2[0], np.array([1, 0]))\n        assert_allclose(inputs2[1], np.array([1, 0]))\n        assert_allclose(labels2[0], np.array([0, 1]))\n\n        inputs3, labels3 = val_gen.__next__()\n        assert_allclose(inputs3[0], np.array([7, 0]))\n        assert_allclose(inputs3[1], np.array([8, 1]))\n        assert_allclose(labels3[0], np.array([1, 0]))\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            val_gen.__next__()\n\n        # Test that we can make a new val generator\n        new_val_gen = get_val_gen()\n        # Verify that the new and old generator are not the same object\n        assert new_val_gen != val_gen\n        new_inputs1, new_labels1 = new_val_gen.__next__()\n        assert_allclose(new_inputs1, inputs1)\n        assert_allclose(new_labels1, labels1)\n        new_inputs2, new_labels2 = new_val_gen.__next__()\n        assert_allclose(new_inputs2, inputs2)\n        assert_allclose(new_labels2, labels2)\n        new_inputs3, new_labels3 = new_val_gen.__next__()\n        assert_allclose(new_inputs3, inputs3)\n        assert_allclose(new_labels3, labels3)\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            new_val_gen.__next__()\n\n    def test_get_validation_data_default_character(self):\n        get_val_gen, val_size = self.data_manager.get_validation_data_from_file(\n            [self.VALIDATION_FILE], mode=""character"")\n        assert val_size == 3\n        val_gen = get_val_gen()\n        inputs1, labels1 = val_gen.__next__()\n        assert_allclose(inputs1[0], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 10, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs1[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 11, 0, 0, 0],\n                                              [6, 9, 2, 7, 8, 3, 5, 4, 1, 0, 0, 0]]))\n        assert_allclose(labels1[0], np.array([1, 0]))\n\n        inputs2, labels2 = val_gen.__next__()\n        assert_allclose(inputs2[0], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 1, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs2[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 10, 1, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(labels2[0], np.array([0, 1]))\n\n        inputs3, labels3 = val_gen.__next__()\n        assert_allclose(inputs3[0], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 15, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs3[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 16, 0, 0, 0],\n                                              [6, 9, 2, 7, 8, 3, 5, 4, 10, 10, 0, 0]]))\n        assert_allclose(labels3[0], np.array([1, 0]))\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            val_gen.__next__()\n\n    def test_get_validation_data_default_word_and_character(self):\n        get_val_gen, val_size = self.data_manager.get_validation_data_from_file(\n            [self.VALIDATION_FILE], mode=""word+character"")\n        val_gen = get_val_gen()\n        assert val_size == 3\n        inputs1, labels1 = val_gen.__next__()\n        assert_allclose(inputs1[0], np.array([2, 0]))\n        assert_allclose(inputs1[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 10, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs1[2], np.array([3, 1]))\n        assert_allclose(inputs1[3], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 11, 0, 0, 0],\n                                              [6, 9, 2, 7, 8, 3, 5, 4, 1, 0, 0, 0]]))\n        assert_allclose(labels1[0], np.array([1, 0]))\n\n        inputs2, labels2 = val_gen.__next__()\n        assert_allclose(inputs2[0], np.array([1, 0]))\n        assert_allclose(inputs2[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 1, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs2[2], np.array([1, 0]))\n        assert_allclose(inputs2[3], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 10, 1, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(labels2[0], np.array([0, 1]))\n\n        inputs3, labels3 = val_gen.__next__()\n        assert_allclose(inputs3[0], np.array([7, 0]))\n        assert_allclose(inputs3[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 15, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs3[2], np.array([8, 1]))\n        assert_allclose(inputs3[3], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 16, 0, 0, 0],\n                                              [6, 9, 2, 7, 8, 3, 5, 4, 10, 10, 0, 0]]))\n        assert_allclose(labels3[0], np.array([1, 0]))\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            val_gen.__next__()\n\n    def test_get_validation_data_pad_with_max_lens(self):\n        get_val_gen, val_size = self.data_manager.get_validation_data_from_file(\n            [self.VALIDATION_FILE],\n            max_lengths={""num_sentence_words"": 1})\n        val_gen = get_val_gen()\n        assert val_size == 3\n        inputs1, labels1 = val_gen.__next__()\n        assert_allclose(inputs1[0], np.array([2]))\n        assert_allclose(inputs1[1], np.array([3]))\n        assert_allclose(labels1[0], np.array([1, 0]))\n\n        inputs2, labels2 = val_gen.__next__()\n        assert_allclose(inputs2[0], np.array([1]))\n        assert_allclose(inputs2[1], np.array([1]))\n        assert_allclose(labels2[0], np.array([0, 1]))\n\n        inputs3, labels3 = val_gen.__next__()\n        assert_allclose(inputs3[0], np.array([7]))\n        assert_allclose(inputs3[1], np.array([8]))\n        assert_allclose(labels3[0], np.array([1, 0]))\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            val_gen.__next__()\n\n    def test_get_validation_data_with_max_instances(self):\n        get_val_gen, val_size = self.data_manager.get_validation_data_from_file(\n            [self.VALIDATION_FILE],\n            max_instances=2)\n        val_size == 2\n        val_gen = get_val_gen()\n        inputs1, labels1 = val_gen.__next__()\n        assert_allclose(inputs1[0], np.array([2, 0]))\n        assert_allclose(inputs1[1], np.array([3, 1]))\n        assert_allclose(labels1[0], np.array([1, 0]))\n\n        inputs2, labels2 = val_gen.__next__()\n        assert_allclose(inputs2[0], np.array([1, 0]))\n        assert_allclose(inputs2[1], np.array([1, 0]))\n        assert_allclose(labels2[0], np.array([0, 1]))\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            val_gen.__next__()\n\n    def test_get_validation_data_errors(self):\n        with self.assertRaises(ValueError):\n            self.data_manager.get_validation_data_from_file(\n                [self.VALIDATION_FILE],\n                max_lengths={""num_sentence_words"": 1},\n                pad=False)\n        with self.assertRaises(ValueError):\n            self.data_manager.get_validation_data_from_file(\n                [self.VALIDATION_FILE],\n                max_lengths={""some wrong key"": 1})\n\n    def test_get_validation_data_no_pad(self):\n        get_val_gen, val_size = self.data_manager.get_validation_data_from_file(\n            [self.VALIDATION_FILE],\n            pad=False)\n        assert val_size == 3\n        val_gen = get_val_gen()\n        inputs1, labels1 = val_gen.__next__()\n        assert_allclose(inputs1[0], np.array([2]))\n        assert_allclose(inputs1[1], np.array([3, 1]))\n        assert_allclose(labels1[0], np.array([1, 0]))\n\n        inputs2, labels2 = val_gen.__next__()\n        assert_allclose(inputs2[0], np.array([1]))\n        assert_allclose(inputs2[1], np.array([1]))\n        assert_allclose(labels2[0], np.array([0, 1]))\n\n        inputs3, labels3 = val_gen.__next__()\n        assert_allclose(inputs3[0], np.array([7]))\n        assert_allclose(inputs3[1], np.array([8, 1, 1]))\n        assert_allclose(labels3[0], np.array([1, 0]))\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            val_gen.__next__()\n\n    def test_generate_validation_batches(self):\n        get_val_gen, val_size = self.data_manager.get_validation_data_from_file(\n            [self.VALIDATION_FILE])\n        batch_gen = self.data_manager.get_batch_generator(get_val_gen, 2)\n        new_batch_gen = DataManager.get_batch_generator(get_val_gen, 2)\n        assert val_size == 3\n\n        # Assert that the new generator is a different object\n        # than the old generator.\n        assert new_batch_gen != batch_gen\n\n        first_batch = batch_gen.__next__()\n        new_first_batch = new_batch_gen.__next__()\n        inputs, labels = first_batch\n        new_inputs, new_labels = new_first_batch\n        assert len(inputs) == len(new_inputs) == 2\n        assert len(labels) == len(new_labels) == 1\n\n        # Ensure output matches ground truth.\n        assert_allclose(inputs[0], np.array([[2, 0], [1, 0]]))\n        assert_allclose(inputs[1], np.array([[3, 1], [1, 0]]))\n        assert_allclose(labels[0], np.array([[1, 0], [0, 1]]))\n        # Ensure both generators produce same results.\n        assert_allclose(inputs[0], new_inputs[0])\n        assert_allclose(inputs[1], new_inputs[1])\n        assert_allclose(labels[0], labels[0])\n\n        second_batch = batch_gen.__next__()\n        new_second_batch = new_batch_gen.__next__()\n        inputs, labels = second_batch\n        new_inputs, new_labels = new_second_batch\n        assert len(inputs) == len(new_inputs) == 2\n        assert len(labels) == len(new_labels) == 1\n\n        # Ensure output matches ground truth.\n        assert_allclose(inputs[0], np.array([[7, 0]]))\n        assert_allclose(inputs[1], np.array([[8, 1]]))\n        assert_allclose(labels[0], np.array([[1, 0]]))\n        # Ensure both generators produce same results.\n        assert_allclose(inputs[0], new_inputs[0])\n        assert_allclose(inputs[1], new_inputs[1])\n        assert_allclose(labels[0], labels[0])\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            batch_gen.__next__()\n            new_batch_gen.__next__()\n\n\nclass TestDataManagerTest(DuplicateTestCase):\n    @overrides\n    def setUp(self):\n        super(TestDataManagerTest, self).setUp()\n        self.write_duplicate_questions_train_file()\n        self.write_duplicate_questions_test_file()\n        self.data_manager = DataManager(STSInstance)\n        self.data_manager.get_train_data_from_file([self.TRAIN_FILE])\n\n    def test_get_test_data_default(self):\n        get_test_gen, test_size = self.data_manager.get_test_data_from_file(\n            [self.TEST_FILE])\n        assert test_size == 3\n        test_gen = get_test_gen()\n        inputs1, labels1 = test_gen.__next__()\n        assert_allclose(inputs1[0], np.array([2, 1]))\n        assert_allclose(inputs1[1], np.array([1, 0]))\n\n        inputs2, labels2 = test_gen.__next__()\n        assert_allclose(inputs2[0], np.array([4, 0]))\n        assert_allclose(inputs2[1], np.array([5, 1]))\n\n        inputs3, labels3 = test_gen.__next__()\n        assert_allclose(inputs3[0], np.array([6, 0]))\n        assert_allclose(inputs3[1], np.array([7, 0]))\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            test_gen.__next__()\n\n        # Test that we can make a new test generator\n        new_test_gen = get_test_gen()\n        # Verify that the new and old generator are not the same object\n        assert new_test_gen != test_gen\n        new_inputs1, new_labels1 = new_test_gen.__next__()\n        assert_allclose(new_inputs1, inputs1)\n        assert_allclose(new_labels1, labels1)\n        new_inputs2, new_labels2 = new_test_gen.__next__()\n        assert_allclose(new_inputs2, inputs2)\n        assert_allclose(new_labels2, labels2)\n        new_inputs3, new_labels3 = new_test_gen.__next__()\n        assert_allclose(new_inputs3, inputs3)\n        assert_allclose(new_labels3, labels3)\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            new_test_gen.__next__()\n\n    def test_get_test_data_default_character(self):\n        get_test_gen, test_size = self.data_manager.get_test_data_from_file(\n            [self.TEST_FILE], mode=""character"")\n        test_gen = get_test_gen()\n        assert test_size == 3\n        inputs1, labels = test_gen.__next__()\n        assert_allclose(inputs1[0], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 10, 0, 0, 0],\n                                              [6, 9, 2, 7, 8, 3, 5, 4, 9, 4, 1, 10]]))\n        assert_allclose(inputs1[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 9, 4, 1, 11],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert len(labels) == 0\n\n        inputs2, labels = test_gen.__next__()\n        assert_allclose(inputs2[0], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 12, 19, 17, 18],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs2[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 13, 0, 0, 0],\n                                              [6, 9, 2, 7, 8, 3, 5, 4, 9, 4, 1, 12]]))\n        assert len(labels) == 0\n\n        inputs3, labels = test_gen.__next__()\n        assert_allclose(inputs3[0], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 14, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs3[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 15, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert len(labels) == 0\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            test_gen.__next__()\n\n    def test_get_test_data_default_word_and_character(self):\n        get_test_gen, test_size = self.data_manager.get_test_data_from_file(\n            [self.TEST_FILE], mode=""word+character"")\n        test_gen = get_test_gen()\n        assert test_size == 3\n        inputs1, labels = test_gen.__next__()\n        assert_allclose(inputs1[0], np.array([2, 1]))\n        assert_allclose(inputs1[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 10, 0, 0, 0],\n                                              [6, 9, 2, 7, 8, 3, 5, 4, 9, 4, 1, 10]]))\n        assert_allclose(inputs1[2], np.array([1, 0]))\n        assert_allclose(inputs1[3], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 9, 4, 1, 11],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert len(labels) == 0\n\n        inputs2, labels = test_gen.__next__()\n        assert_allclose(inputs2[0], np.array([4, 0]))\n        assert_allclose(inputs2[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 12, 19, 17, 18],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs2[2], np.array([5, 1]))\n        assert_allclose(inputs2[3], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 13, 0, 0, 0],\n                                              [6, 9, 2, 7, 8, 3, 5, 4, 9, 4, 1, 12]]))\n        assert len(labels) == 0\n\n        inputs3, labels = test_gen.__next__()\n        assert_allclose(inputs3[0], np.array([6, 0]))\n        assert_allclose(inputs3[1], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 14, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert_allclose(inputs3[2], np.array([7, 0]))\n        assert_allclose(inputs3[3], np.array([[6, 9, 2, 7, 8, 3, 5, 4, 15, 0, 0, 0],\n                                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n        assert len(labels) == 0\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            test_gen.__next__()\n\n    def test_get_test_data_pad_with_max_lens(self):\n        get_test_gen, test_size = self.data_manager.get_test_data_from_file(\n            [self.TEST_FILE],\n            max_lengths={""num_sentence_words"": 1})\n        test_gen = get_test_gen()\n        assert test_size == 3\n\n        inputs, labels = test_gen.__next__()\n        assert_allclose(inputs[0], np.array([2]))\n        assert_allclose(inputs[1], np.array([1]))\n        assert len(labels) == 0\n\n        inputs, labels = test_gen.__next__()\n        assert_allclose(inputs[0], np.array([4]))\n        assert_allclose(inputs[1], np.array([5]))\n        assert len(labels) == 0\n\n        inputs, labels = test_gen.__next__()\n        assert_allclose(inputs[0], np.array([6]))\n        assert_allclose(inputs[1], np.array([7]))\n        assert len(labels) == 0\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            test_gen.__next__()\n\n    def test_get_test_data_with_max_instances(self):\n        get_test_gen, test_size = self.data_manager.get_test_data_from_file(\n            [self.TEST_FILE],\n            max_instances=2)\n        test_gen = get_test_gen()\n        assert test_size == 2\n\n        inputs, labels = test_gen.__next__()\n        assert_allclose(inputs[0], np.array([2, 1]))\n        assert_allclose(inputs[1], np.array([1, 0]))\n        assert len(labels) == 0\n\n        inputs, labels = test_gen.__next__()\n        assert_allclose(inputs[0], np.array([4, 0]))\n        assert_allclose(inputs[1], np.array([5, 1]))\n        assert len(labels) == 0\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            test_gen.__next__()\n\n    def test_get_test_data_errors(self):\n        with self.assertRaises(ValueError):\n            self.data_manager.get_test_data_from_file(\n                [self.TEST_FILE],\n                max_lengths={""num_sentence_words"": 1},\n                pad=False)\n        with self.assertRaises(ValueError):\n            self.data_manager.get_test_data_from_file(\n                [self.TEST_FILE],\n                max_lengths={""some wrong key"": 1})\n\n    def test_get_test_data_no_pad(self):\n        get_test_gen, test_size = self.data_manager.get_test_data_from_file(\n            [self.TEST_FILE],\n            pad=False)\n        test_gen = get_test_gen()\n        assert test_size == 3\n\n        inputs, labels = test_gen.__next__()\n        assert_allclose(inputs[0], np.array([2, 1, 2]))\n        assert_allclose(inputs[1], np.array([1]))\n        assert len(labels) == 0\n\n        inputs, labels = test_gen.__next__()\n        assert_allclose(inputs[0], np.array([4]))\n        assert_allclose(inputs[1], np.array([5, 1]))\n        assert len(labels) == 0\n\n        inputs, labels = test_gen.__next__()\n        assert_allclose(inputs[0], np.array([6]))\n        assert_allclose(inputs[1], np.array([7]))\n        assert len(labels) == 0\n\n        # Should raise a StopIteration\n        with self.assertRaises(StopIteration):\n            test_gen.__next__()\n\n    def test_generate_test_batches(self):\n        get_test_gen, test_size = self.data_manager.get_test_data_from_file(\n            [self.TEST_FILE])\n        batch_gen = self.data_manager.get_batch_generator(get_test_gen, 2)\n        new_batch_gen = DataManager.get_batch_generator(get_test_gen, 2)\n\n        # Assert that the new generator is a different object\n        # than the old generator.\n        assert new_batch_gen != batch_gen\n        assert test_size == 3\n\n        first_batch = batch_gen.__next__()\n        new_first_batch = new_batch_gen.__next__()\n        inputs, labels = first_batch\n        new_inputs, new_labels = new_first_batch\n        assert len(inputs) == 2\n        assert len(labels) == 0\n\n        # Ensure output matches ground truth\n        assert_allclose(inputs[0], np.array([[2, 1], [4, 0]]))\n        assert_allclose(inputs[1], np.array([[1, 0], [5, 1]]))\n        # Ensure both generators produce same results.\n        assert_allclose(inputs[0], new_inputs[0])\n        assert_allclose(inputs[1], new_inputs[1])\n\n        second_batch = batch_gen.__next__()\n        new_second_batch = new_batch_gen.__next__()\n        inputs, labels = second_batch\n        new_inputs, new_labels = new_second_batch\n        assert len(inputs) == 2\n        assert len(labels) == 0\n\n        # Ensure output matches ground truth\n        assert_allclose(inputs[0], np.array([[6, 0]]))\n        assert_allclose(inputs[1], np.array([[7, 0]]))\n        # Ensure both generators produce same results.\n        assert_allclose(inputs[0], new_inputs[0])\n        assert_allclose(inputs[1], new_inputs[1])\n\n        with self.assertRaises(StopIteration):\n            batch_gen.__next__()\n            new_batch_gen.__next__()\n'"
tests/data/test_dataset.py,0,"b'from overrides import overrides\nfrom numpy.testing import assert_allclose\nimport numpy as np\n\nfrom duplicate_questions.data.data_indexer import DataIndexer\nfrom duplicate_questions.data.dataset import Dataset\nfrom duplicate_questions.data.dataset import IndexedDataset\nfrom duplicate_questions.data.dataset import TextDataset\nfrom duplicate_questions.data.instances.instance_word import IndexedInstanceWord\nfrom duplicate_questions.data.instances.sts_instance import IndexedSTSInstance\nfrom duplicate_questions.data.instances.sts_instance import STSInstance\n\nfrom ..common.test_case import DuplicateTestCase\n\n\nclass TestDataset(DuplicateTestCase):\n    def test_truncate(self):\n        instances = [STSInstance(""testing1"", ""test1"", None),\n                     STSInstance(""testing2"", ""test2"", None)]\n        dataset = Dataset(instances)\n        truncated = dataset.truncate(1)\n        assert len(truncated.instances) == 1\n        with self.assertRaises(ValueError):\n            truncated = dataset.truncate(""1"")\n        with self.assertRaises(ValueError):\n            truncated = dataset.truncate(0)\n\n    def test_merge(self):\n        instances = [STSInstance(""testing1"", ""test1"", None),\n                     STSInstance(""testing2"", ""test2"", None)]\n        dataset1 = Dataset(instances[:1])\n        dataset2 = Dataset(instances[1:])\n        merged = dataset1.merge(dataset2)\n        assert merged.instances == instances\n        with self.assertRaises(ValueError):\n            merged = dataset1.merge(instances)\n\n    def test_exceptions(self):\n        instance = STSInstance(""testing1"", ""test1"", 0)\n        with self.assertRaises(ValueError):\n            Dataset(instance)\n        with self.assertRaises(ValueError):\n            Dataset([""not an instance""])\n\n\nclass TestTextDataset(DuplicateTestCase):\n    def test_read_from_train_file(self):\n        self.write_duplicate_questions_train_file()\n        dataset = TextDataset.read_from_file(self.TRAIN_FILE, STSInstance)\n        assert len(dataset.instances) == 3\n        instance = dataset.instances[0]\n        assert instance.first_sentence_str == ""question1""\n        assert instance.second_sentence_str == ""question2 question3pad""\n        assert instance.label == 0\n        instance = dataset.instances[1]\n        assert instance.first_sentence_str == ""question4""\n        assert instance.second_sentence_str == ""question5""\n        assert instance.label == 1\n        instance = dataset.instances[2]\n        assert instance.first_sentence_str == ""question6""\n        assert instance.second_sentence_str == ""question7""\n        assert instance.label == 0\n        with self.assertRaises(ValueError):\n            TextDataset.read_from_file(3, STSInstance)\n        with self.assertRaises(ValueError):\n            TextDataset.read_from_file([3], STSInstance)\n\n    def test_read_from_lines(self):\n        self.write_duplicate_questions_train_file()\n        lines = [""\\""1\\"",\\""2\\"",\\""3\\"",\\""question1\\"",\\""question2\\"",\\""0\\""\\n"",\n                 ""\\""4\\"",\\""5\\"",\\""6\\"",\\""question3\\"",\\""question4\\"",\\""1\\""\\n"",\n                 ""\\""7\\"",\\""8\\"",\\""9\\"",\\""question5\\"",\\""question6\\"",\\""0\\""\\n""]\n        dataset = TextDataset.read_from_lines(lines, STSInstance)\n        assert len(dataset.instances) == 3\n        instance = dataset.instances[0]\n        assert instance.first_sentence_str == ""question1""\n        assert instance.second_sentence_str == ""question2""\n        assert instance.label == 0\n        instance = dataset.instances[1]\n        assert instance.first_sentence_str == ""question3""\n        assert instance.second_sentence_str == ""question4""\n        assert instance.label == 1\n        instance = dataset.instances[2]\n        assert instance.first_sentence_str == ""question5""\n        assert instance.second_sentence_str == ""question6""\n        assert instance.label == 0\n        with self.assertRaises(ValueError):\n            TextDataset.read_from_lines(""some line"", STSInstance)\n        with self.assertRaises(ValueError):\n            TextDataset.read_from_lines([3], ""STSInstance"")\n\n    def test_read_from_test_file(self):\n        self.write_duplicate_questions_test_file()\n        dataset = TextDataset.read_from_file(self.TEST_FILE, STSInstance)\n        assert len(dataset.instances) == 3\n        instance = dataset.instances[0]\n        assert instance.first_sentence_str == ""question1 questionunk1 question1""\n        assert instance.second_sentence_str == ""questionunk2""\n        assert instance.label is None\n        instance = dataset.instances[1]\n        assert instance.first_sentence_str == ""question3pad""\n        assert instance.second_sentence_str == ""question4 questionunk3""\n        assert instance.label is None\n        instance = dataset.instances[2]\n        assert instance.first_sentence_str == ""question5""\n        assert instance.second_sentence_str == ""question6""\n        assert instance.label is None\n        with self.assertRaises(ValueError):\n            TextDataset.read_from_file(3, STSInstance)\n\n    def test_to_indexed_dataset(self):\n        instances = [STSInstance(""testing1 test1"", ""test1"", None),\n                     STSInstance(""testing2"", ""test2 testing1"", None)]\n        data_indexer = DataIndexer()\n        testing1_index = data_indexer.add_word_to_index(""testing1"")\n        test1_index = data_indexer.add_word_to_index(""test1"")\n        testing2_index = data_indexer.add_word_to_index(""testing2"")\n        test2_index = data_indexer.add_word_to_index(""test2"")\n        dataset = TextDataset(instances)\n        indexed_dataset = dataset.to_indexed_dataset(data_indexer)\n\n        indexed_instance = indexed_dataset.instances[0]\n        first_sent_idxs, second_sent_idxs = indexed_instance.get_int_word_indices()\n        assert first_sent_idxs == [testing1_index,\n                                   test1_index]\n        assert second_sent_idxs == [test1_index]\n\n        indexed_instance = indexed_dataset.instances[1]\n        first_sent_idxs, second_sent_idxs = indexed_instance.get_int_word_indices()\n        assert first_sent_idxs == [testing2_index]\n        assert second_sent_idxs == [test2_index,\n                                    testing1_index]\n\n\nclass TestIndexedDataset(DuplicateTestCase):\n    @overrides\n    def setUp(self):\n        super(TestIndexedDataset, self).setUp()\n        self.instances = [IndexedSTSInstance([IndexedInstanceWord(1, [1, 5]),\n                                              IndexedInstanceWord(2, [2, 1]),\n                                              IndexedInstanceWord(3, [1, 4, 1])],\n                                             [IndexedInstanceWord(2, [2, 1]),\n                                              IndexedInstanceWord(3, [1, 4, 1])],\n                                             [0, 1]),\n                          IndexedSTSInstance([IndexedInstanceWord(3, [1, 4, 1]),\n                                              IndexedInstanceWord(1, [1, 5])],\n                                             [IndexedInstanceWord(3, [1, 4, 1]),\n                                              IndexedInstanceWord(1, [1, 5]),\n                                              IndexedInstanceWord(3, [1, 4, 1]),\n                                              IndexedInstanceWord(2, [2, 1])],\n                                             [1, 0])]\n        self.indexed_dataset = IndexedDataset(self.instances)\n\n    def test_max_lengths(self):\n        max_lengths = self.indexed_dataset.max_lengths()\n        assert max_lengths == {""num_sentence_words"": 4, ""num_word_characters"": 3}\n\n    def test_pad_adds_zeroes(self):\n        self.indexed_dataset.pad_instances({""num_sentence_words"": 4,\n                                            ""num_word_characters"": 3})\n        instance = self.indexed_dataset.instances[0]\n        first_sent_word_idxs, second_sent_word_idxs = instance.get_int_word_indices()\n        first_sent_char_idxs, second_sent_char_idxs = instance.get_int_char_indices()\n        assert first_sent_word_idxs == [1, 2, 3, 0]\n        assert second_sent_word_idxs == [2, 3, 0, 0]\n        assert first_sent_char_idxs == [[1, 5, 0], [2, 1, 0],\n                                        [1, 4, 1], [0, 0, 0]]\n        assert second_sent_char_idxs == [[2, 1, 0], [1, 4, 1],\n                                         [0, 0, 0], [0, 0, 0]]\n        assert instance.label == [0, 1]\n\n        instance = self.indexed_dataset.instances[1]\n        first_sent_word_idxs, second_sent_word_idxs = instance.get_int_word_indices()\n        first_sent_char_idxs, second_sent_char_idxs = instance.get_int_char_indices()\n        assert first_sent_word_idxs == [3, 1, 0, 0]\n        assert second_sent_word_idxs == [3, 1, 3, 2]\n        assert first_sent_char_idxs == [[1, 4, 1], [1, 5, 0],\n                                        [0, 0, 0], [0, 0, 0]]\n        assert second_sent_char_idxs == [[1, 4, 1], [1, 5, 0],\n                                         [1, 4, 1], [2, 1, 0]]\n        assert instance.label == [1, 0]\n\n    def test_pad_truncates(self):\n        self.indexed_dataset.pad_instances({""num_sentence_words"": 2,\n                                            ""num_word_characters"": 1})\n        instance = self.indexed_dataset.instances[0]\n        first_sent_word_idxs, second_sent_word_idxs = instance.get_int_word_indices()\n        first_sent_char_idxs, second_sent_char_idxs = instance.get_int_char_indices()\n        assert first_sent_word_idxs == [1, 2]\n        assert second_sent_word_idxs == [2, 3]\n        assert first_sent_char_idxs == [[1], [2]]\n        assert second_sent_char_idxs == [[2], [1]]\n        assert instance.label == [0, 1]\n\n        instance = self.indexed_dataset.instances[1]\n        first_sent_word_idxs, second_sent_word_idxs = instance.get_int_word_indices()\n        first_sent_char_idxs, second_sent_char_idxs = instance.get_int_char_indices()\n        assert first_sent_word_idxs == [3, 1]\n        assert second_sent_word_idxs == [3, 1]\n        assert first_sent_char_idxs == [[1], [1]]\n        assert second_sent_char_idxs == [[1], [1]]\n        assert instance.label == [1, 0]\n\n    def test_as_training_data(self):\n        self.indexed_dataset.pad_instances(self.indexed_dataset.max_lengths())\n        inputs, labels = self.indexed_dataset.as_training_data()\n\n        first_sentence, second_sentence = inputs[0]\n        label = labels[0]\n        assert_allclose(first_sentence, np.array([1, 2, 3, 0]))\n        assert_allclose(second_sentence, np.array([2, 3, 0, 0]))\n        assert_allclose(label[0], np.array([0, 1]))\n\n        first_sentence, second_sentence = inputs[1]\n        label = labels[1]\n        assert_allclose(first_sentence, np.array([3, 1, 0, 0]))\n        assert_allclose(second_sentence, np.array([3, 1, 3, 2]))\n        assert_allclose(label[0], np.array([1, 0]))\n\n        inputs, labels = self.indexed_dataset.as_training_data(mode=""character"")\n        first_sentence, second_sentence = inputs[0]\n        label = labels[0]\n        assert_allclose(first_sentence, np.array([[1, 5, 0], [2, 1, 0],\n                                                  [1, 4, 1], [0, 0, 0]]))\n        assert_allclose(second_sentence, np.array([[2, 1, 0], [1, 4, 1],\n                                                   [0, 0, 0], [0, 0, 0]]))\n        assert_allclose(label[0], np.array([0, 1]))\n\n        first_sentence, second_sentence = inputs[1]\n        label = labels[1]\n        assert_allclose(first_sentence, np.array([[1, 4, 1], [1, 5, 0],\n                                                  [0, 0, 0], [0, 0, 0]]))\n        assert_allclose(second_sentence, np.array([[1, 4, 1], [1, 5, 0],\n                                                   [1, 4, 1], [2, 1, 0]]))\n\n        inputs, labels = self.indexed_dataset.as_training_data(mode=""word+character"")\n        (first_sentence_words, first_sentence_characters,\n         second_sentence_words, second_sentence_characters) = inputs[0]\n        label = labels[0]\n        assert_allclose(first_sentence_words, np.array([1, 2, 3, 0]))\n        assert_allclose(second_sentence_words, np.array([2, 3, 0, 0]))\n        assert_allclose(first_sentence_characters, np.array([[1, 5, 0], [2, 1, 0],\n                                                             [1, 4, 1], [0, 0, 0]]))\n        assert_allclose(second_sentence_characters, np.array([[2, 1, 0], [1, 4, 1],\n                                                              [0, 0, 0], [0, 0, 0]]))\n        assert_allclose(label[0], np.array([0, 1]))\n\n        (first_sentence_words, first_sentence_characters,\n         second_sentence_words, second_sentence_characters) = inputs[1]\n        label = labels[1]\n        assert_allclose(first_sentence_words, np.array([3, 1, 0, 0]))\n        assert_allclose(second_sentence_words, np.array([3, 1, 3, 2]))\n        assert_allclose(first_sentence_characters, np.array([[1, 4, 1], [1, 5, 0],\n                                                             [0, 0, 0], [0, 0, 0]]))\n        assert_allclose(second_sentence_characters, np.array([[1, 4, 1], [1, 5, 0],\n                                                              [1, 4, 1], [2, 1, 0]]))\n\n    def test_as_testing_data(self):\n        instances = [IndexedSTSInstance([IndexedInstanceWord(1, [1, 4, 4]),\n                                         IndexedInstanceWord(2, [2, 3]),\n                                         IndexedInstanceWord(3, [5, 1])],\n                                        [IndexedInstanceWord(2, [2, 3]),\n                                         IndexedInstanceWord(3, [5, 1])],\n                                        None),\n                     IndexedSTSInstance([IndexedInstanceWord(3, [5, 1]),\n                                         IndexedInstanceWord(1, [1, 4, 4])],\n                                        [IndexedInstanceWord(3, [5, 1]),\n                                         IndexedInstanceWord(1, [1, 4, 4]),\n                                         IndexedInstanceWord(3, [5, 1]),\n                                         IndexedInstanceWord(2, [2, 3])],\n                                        None)]\n        indexed_dataset = IndexedDataset(instances)\n        indexed_dataset.pad_instances(indexed_dataset.max_lengths())\n        inputs, labels = indexed_dataset.as_testing_data()\n        assert len(labels) == 0\n\n        first_sentence, second_sentence = inputs[0]\n        assert_allclose(first_sentence, np.array([1, 2, 3, 0]))\n        assert_allclose(second_sentence, np.array([2, 3, 0, 0]))\n\n        first_sentence, second_sentence = inputs[1]\n        assert_allclose(first_sentence, np.array([3, 1, 0, 0]))\n        assert_allclose(second_sentence, np.array([3, 1, 3, 2]))\n\n        inputs, labels = indexed_dataset.as_testing_data(mode=""character"")\n        assert len(labels) == 0\n\n        first_sentence, second_sentence = inputs[0]\n        assert_allclose(first_sentence, np.array([[1, 4, 4], [2, 3, 0],\n                                                  [5, 1, 0], [0, 0, 0]]))\n        assert_allclose(second_sentence, np.array([[2, 3, 0], [5, 1, 0],\n                                                   [0, 0, 0], [0, 0, 0]]))\n\n        first_sentence, second_sentence = inputs[1]\n        assert_allclose(first_sentence, np.array([[5, 1, 0], [1, 4, 4],\n                                                  [0, 0, 0], [0, 0, 0]]))\n        assert_allclose(second_sentence, np.array([[5, 1, 0], [1, 4, 4],\n                                                   [5, 1, 0], [2, 3, 0]]))\n\n        inputs, labels = indexed_dataset.as_testing_data(mode=""word+character"")\n        assert len(labels) == 0\n\n        (first_sentence_words, first_sentence_characters,\n         second_sentence_words, second_sentence_characters) = inputs[0]\n        assert_allclose(first_sentence_words, np.array([1, 2, 3, 0]))\n        assert_allclose(second_sentence_words, np.array([2, 3, 0, 0]))\n        assert_allclose(first_sentence_characters, np.array([[1, 4, 4], [2, 3, 0],\n                                                             [5, 1, 0], [0, 0, 0]]))\n        assert_allclose(second_sentence_characters, np.array([[2, 3, 0], [5, 1, 0],\n                                                              [0, 0, 0], [0, 0, 0]]))\n\n        (first_sentence_words, first_sentence_characters,\n         second_sentence_words, second_sentence_characters) = inputs[1]\n        assert_allclose(first_sentence_words, np.array([3, 1, 0, 0]))\n        assert_allclose(second_sentence_words, np.array([3, 1, 3, 2]))\n        assert_allclose(first_sentence_characters, np.array([[5, 1, 0], [1, 4, 4],\n                                                             [0, 0, 0], [0, 0, 0]]))\n        assert_allclose(second_sentence_characters, np.array([[5, 1, 0], [1, 4, 4],\n                                                              [5, 1, 0], [2, 3, 0]]))\n        with self.assertRaises(ValueError):\n            indexed_dataset.as_testing_data(mode=""char"")\n\n    def test_sort(self):\n        # lengths: 3, 4, 1, 2, 2\n        sorted_instances = [IndexedSTSInstance([IndexedInstanceWord(3, [1, 4, 1]),\n                                                IndexedInstanceWord(1, [1, 5])],\n                                               [IndexedInstanceWord(3, [1, 4, 1]),\n                                                IndexedInstanceWord(1, [1, 5]),\n                                                IndexedInstanceWord(3, [1, 4, 1]),\n                                                IndexedInstanceWord(2, [2, 1])],\n                                               [1, 0]),\n                            IndexedSTSInstance([IndexedInstanceWord(1, [1, 5]),\n                                                IndexedInstanceWord(2, [2, 1]),\n                                                IndexedInstanceWord(3, [1, 4, 1])],\n                                               [IndexedInstanceWord(2, [2, 1]),\n                                                IndexedInstanceWord(3, [1, 4, 1])],\n                                               [0, 1])]\n        self.assertNotEqual(sorted_instances, self.indexed_dataset.instances)\n        self.indexed_dataset.sort()\n        self.assertEquals(sorted_instances, self.indexed_dataset.instances)\n'"
tests/data/test_embedding_manager.py,0,"b'import codecs\nfrom numpy.testing import assert_allclose\nimport numpy as np\nfrom overrides import overrides\n\nfrom duplicate_questions.data.embedding_manager import EmbeddingManager\nfrom duplicate_questions.data.data_indexer import DataIndexer\n\nfrom ..common.test_case import DuplicateTestCase\n\n\nclass TestEmbeddingManager(DuplicateTestCase):\n    @overrides\n    def setUp(self):\n        super(TestEmbeddingManager, self).setUp()\n        self.write_vector_file()\n        self.data_indexer = DataIndexer()\n        self.data_indexer.add_word_to_index(""word1"")\n        self.data_indexer.add_word_to_index(""word2"")\n        self.data_indexer.is_fit = True\n        self.embedding_dict = {""word1"": np.array([5.1, 7.2, -0.2]),\n                               ""word2"": np.array([0.8, 0.1, 0.9])}\n        self.embedding_manager = EmbeddingManager(self.data_indexer)\n\n    def test_get_embedding_matrix_reads_data_file(self):\n        embed_mat = self.embedding_manager.get_embedding_matrix(\n            3,\n            pretrained_embeddings_file_path=self.VECTORS_FILE)\n        assert_allclose(embed_mat[2], np.array([0.0, 1.1, 0.2]))\n        assert_allclose(embed_mat[3], np.array([0.1, 0.4, -4.0]))\n\n    def test_get_embedding_matrix_reads_dict(self):\n        embed_mat = self.embedding_manager.get_embedding_matrix(\n            3,\n            pretrained_embeddings_dict=self.embedding_dict)\n        assert_allclose(embed_mat[2], np.array([5.1, 7.2, -0.2]))\n        assert_allclose(embed_mat[3], np.array([0.8, 0.1, 0.9]))\n\n    def test_get_embedding_matrix_dict_overrides_file(self):\n        embed_mat = self.embedding_manager.get_embedding_matrix(\n            3,\n            pretrained_embeddings_file_path=self.VECTORS_FILE,\n            pretrained_embeddings_dict=self.embedding_dict)\n        assert_allclose(embed_mat[2], np.array([5.1, 7.2, -0.2]))\n        assert_allclose(embed_mat[3], np.array([0.8, 0.1, 0.9]))\n\n    def test_get_embedding_matrix_reproducible(self):\n        embed_mat_1_random = self.embedding_manager.get_embedding_matrix(100)\n        embed_mat_2_random = self.embedding_manager.get_embedding_matrix(100)\n        assert_allclose(embed_mat_1_random, embed_mat_2_random)\n\n        embed_mat_1_file = self.embedding_manager.get_embedding_matrix(\n            3,\n            pretrained_embeddings_file_path=self.VECTORS_FILE)\n        embed_mat_2_file = self.embedding_manager.get_embedding_matrix(\n            3,\n            pretrained_embeddings_file_path=self.VECTORS_FILE)\n        assert_allclose(embed_mat_1_file, embed_mat_2_file)\n\n        embed_mat_1_dict = self.embedding_manager.get_embedding_matrix(\n            3,\n            pretrained_embeddings_dict=self.embedding_dict)\n        embed_mat_2_dict = self.embedding_manager.get_embedding_matrix(\n            3,\n            pretrained_embeddings_dict=self.embedding_dict)\n        assert_allclose(embed_mat_1_dict, embed_mat_2_dict)\n\n    def test_embedding_manager_errors(self):\n        with self.assertRaises(ValueError):\n            unfitted_data_indexer = DataIndexer()\n            EmbeddingManager(unfitted_data_indexer)\n        with self.assertRaises(ValueError):\n            EmbeddingManager.initialize_random_matrix((19,))\n        with self.assertRaises(ValueError):\n            EmbeddingManager.initialize_random_matrix((19, 100, 100))\n        with self.assertRaises(ValueError):\n            self.embedding_manager.get_embedding_matrix(5.0)\n        with self.assertRaises(ValueError):\n            self.embedding_manager.get_embedding_matrix(""5"")\n        with self.assertRaises(ValueError):\n            self.embedding_manager.get_embedding_matrix(\n                5,\n                pretrained_embeddings_file_path=[""some_path""])\n        with self.assertRaises(ValueError):\n            self.embedding_manager.get_embedding_matrix(\n                5,\n                pretrained_embeddings_dict=[""list"", [0.1, 0.2]])\n        with self.assertRaises(ValueError):\n            self.embedding_manager.get_embedding_matrix(\n                5,\n                pretrained_embeddings_file_path=self.VECTORS_FILE)\n        with self.assertRaises(ValueError):\n            self.embedding_manager.get_embedding_matrix(\n                5,\n                pretrained_embeddings_dict=self.embedding_dict)\n        with self.assertRaises(ValueError):\n            bad_dict = {""word1"": np.array([0.1, 0.2]),\n                        ""word2"": np.array([0.3, 0.4, 0.5])}\n            self.embedding_manager.get_embedding_matrix(\n                5,\n                pretrained_embeddings_dict=bad_dict)\n        with self.assertRaises(ValueError):\n            bad_vectors_path = self.TEST_DIR + \'bad_vectors_file\'\n            with codecs.open(bad_vectors_path, \'w\', \'utf-8\') as vectors_file:\n                vectors_file.write(""word1 0.0 1.1 0.2\\n"")\n                vectors_file.write(""word2 0.1 0.4\\n"")\n            self.embedding_manager.get_embedding_matrix(\n                3,\n                pretrained_embeddings_file_path=bad_vectors_path)\n        with self.assertRaises(ValueError):\n            bad_vectors_path = self.TEST_DIR + \'bad_vectors_file\'\n            with codecs.open(bad_vectors_path, \'w\', \'utf-8\') as vectors_file:\n                vectors_file.write(""word0 0.0\\n"")\n                vectors_file.write(""word1 0.0 1.1 0.2\\n"")\n                vectors_file.write(""word2 0.1 0.4\\n"")\n            self.embedding_manager.get_embedding_matrix(\n                3,\n                pretrained_embeddings_file_path=bad_vectors_path)\n'"
tests/models/__init__.py,0,b''
tests/util/__init__.py,0,b''
tests/util/test_pooling.py,5,"b'import tensorflow as tf\nfrom numpy.testing import assert_allclose\nimport numpy as np\n\nfrom duplicate_questions.util.pooling import mean_pool\n\nfrom ..common.test_case import DuplicateTestCase\n\n\nclass TestUtilsPooling(DuplicateTestCase):\n    def test_mean_pool_with_sequence_length(self):\n        with tf.Session():\n            lstm_output = tf.constant(\n                np.asarray([[[0.1, 0.2], [0.8, 0.9], [0.0, 0.0]],\n                            [[1.1, 1.2], [0.0, 0.0], [0.0, 0.0]],\n                            [[2.1, 2.2], [2.8, 2.9], [2.3, 2.9]]]),\n                dtype=""float32"")\n            lstm_sequence_lengths = tf.constant(np.asarray([2, 1, 3]),\n                                                dtype=""int32"")\n            mean_pooled_outputs = mean_pool(lstm_output,\n                                            lstm_sequence_lengths)\n            assert_allclose(mean_pooled_outputs.eval(),\n                            np.asarray([[0.45, 0.55],\n                                        [1.1, 1.2],\n                                        [2.4, 8 / 3]]))\n\n    def test_mean_pool_without_sequence_length(self):\n        with tf.Session():\n            lstm_output = tf.constant(\n                np.asarray([[[0.1, 0.2], [0.8, 0.9], [0.0, 0.0]],\n                            [[1.1, 1.2], [0.0, 0.0], [0.0, 0.0]],\n                            [[2.1, 2.2], [2.8, 2.9], [2.3, 2.9]]]),\n                dtype=""float32"")\n            mean_pooled_outputs = mean_pool(lstm_output)\n            assert_allclose(mean_pooled_outputs.eval(),\n                            np.asarray([[0.9 / 3, 1.1 / 3],\n                                        [1.1 / 3, 1.2 / 3],\n                                        [7.2 / 3, 8 / 3]]))\n'"
tests/util/test_rnn.py,3,"b'import tensorflow as tf\nfrom numpy.testing import assert_allclose\nimport numpy as np\n\nfrom duplicate_questions.util.rnn import last_relevant_output\n\nfrom ..common.test_case import DuplicateTestCase\n\n\nclass TestUtilsRNN(DuplicateTestCase):\n    def test_get_last_relevant_output(self):\n        with tf.Session():\n            lstm_output = tf.constant(\n                np.asarray([[[0.1, 0.2], [0.8, 0.9], [0.0, 0.0]],\n                            [[1.1, 1.2], [0.0, 0.0], [0.0, 0.0]],\n                            [[2.1, 2.2], [2.8, 2.9], [2.3, 2.9]]]),\n                dtype=""float32"")\n            lstm_sequence_lengths = tf.constant(np.asarray([2, 1, 3]),\n                                                dtype=""int32"")\n            last_relevant_outputs = last_relevant_output(lstm_output,\n                                                         lstm_sequence_lengths)\n            assert_allclose(last_relevant_outputs.eval(),\n                            np.asarray([[0.8, 0.9],\n                                        [1.1, 1.2],\n                                        [2.3, 2.9]]))\n'"
tests/util/test_switchable_dropout_wrapper.py,14,"b'from flaky import flaky\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import LSTMCell\nfrom numpy.testing import assert_allclose\nimport numpy as np\n\nfrom duplicate_questions.util.switchable_dropout_wrapper import SwitchableDropoutWrapper\nfrom ..common.test_case import DuplicateTestCase\n\n\nclass TestUtilsSwitchableDropoutWrapper(DuplicateTestCase):\n    @flaky\n    def test_switchable_dropout_wrapper_state_is_tuple(self):\n        tf.set_random_seed(0)\n        batch_size = 3\n        sequence_len = 3\n        word_embedding_dim = 5\n        lstm_input = tf.random_normal([batch_size, sequence_len,\n                                       word_embedding_dim])\n        sequence_length = tf.constant(np.array([2, 1, 3]), dtype=""int32"")\n\n        is_train = tf.placeholder(\'bool\', [])\n        rnn_hidden_size = 3\n        output_keep_prob = 0.75\n\n        rnn_cell = LSTMCell(rnn_hidden_size, state_is_tuple=True)\n        d_rnn_cell = SwitchableDropoutWrapper(rnn_cell,\n                                              is_train,\n                                              output_keep_prob=output_keep_prob)\n        rnn_output, (rnn_c_state, rnn_m_state) = tf.nn.dynamic_rnn(\n            cell=d_rnn_cell,\n            dtype=""float"",\n            sequence_length=sequence_length,\n            inputs=lstm_input)\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            output_no_train = rnn_output.eval(feed_dict={is_train: False})\n            expected_output_no_train = np.array([[[0.10523333, -0.03578992, 0.16407447],\n                                                  [-0.07642615, -0.1346959, 0.07218226],\n                                                  [0.0, 0.0, 0.0]],\n                                                 [[-0.31979755, -0.12604457, -0.24436688],\n                                                  [0.0, 0.0, 0.0],\n                                                  [0.0, 0.0, 0.0]],\n                                                 [[0.27140033, -0.01063369, 0.11808267],\n                                                  [0.15138564, -0.10808259, 0.13118345],\n                                                  [0.20397078, -0.06317351, 0.21408504]]])\n            assert_allclose(output_no_train,\n                            expected_output_no_train * d_rnn_cell._output_keep_prob,\n                            rtol=1e-06)\n\n            output_train = rnn_output.eval(feed_dict={is_train: True})\n            expected_output_train = np.array([[[-0.0, -0.21935862, -0.11160457],\n                                               [-0.0, -0.0, 0.09479073],\n                                               [0.0, 0.0, 0.0]],\n                                              [[0.02565068, 0.21709232, -0.0],\n                                               [0.0, 0.0, 0.0],\n                                               [0.0, 0.0, 0.0]],\n                                              [[0.0, 0.0, 0.07740743],\n                                               [0.04682902, -0.14770079, 0.14597748],\n                                               [0.0, 0.09399685, 0.0]]])\n            # low precision test, this one seems flaky\n            assert_allclose(output_train, expected_output_train, rtol=1e-06)\n\n    @flaky\n    def test_switchable_dropout_wrapper_state_is_not_tuple(self):\n        tf.set_random_seed(0)\n        batch_size = 3\n        sequence_len = 3\n        word_embedding_dim = 5\n        lstm_input = tf.random_normal([batch_size, sequence_len,\n                                       word_embedding_dim])\n        sequence_length = tf.constant(np.array([2, 1, 3]), dtype=""int32"")\n\n        is_train = tf.placeholder(\'bool\', [])\n        rnn_hidden_size = 3\n        output_keep_prob = 0.75\n\n        rnn_cell = LSTMCell(rnn_hidden_size, state_is_tuple=False)\n        d_rnn_cell = SwitchableDropoutWrapper(rnn_cell,\n                                              is_train,\n                                              output_keep_prob=output_keep_prob)\n        rnn_output, rnn_state = tf.nn.dynamic_rnn(\n            cell=d_rnn_cell,\n            dtype=""float"",\n            sequence_length=sequence_length,\n            inputs=lstm_input)\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            output_no_train = rnn_output.eval(feed_dict={is_train: False})\n            expected_output_no_train = np.array(\n                [[[-0.10366952, -0.01751264, -0.02237115],\n                  [-0.07636562, 0.06660741, 0.02946584],\n                  [0.0, 0.0, 0.0]],\n                 [[-0.09134783, 0.15928121, 0.05786164],\n                  [0.0, 0.0, 0.0],\n                  [0.0, 0.0, 0.0]],\n                 [[-0.00575439, -0.22505699, -0.27295753],\n                  [-0.12970942, -0.16395324, -0.06502352],\n                  [-0.16302694, -0.27601245, -0.20045257]]])\n            assert_allclose(output_no_train,\n                            expected_output_no_train * d_rnn_cell._output_keep_prob,\n                            rtol=1e-06)\n            output_train = rnn_output.eval(feed_dict={is_train: True})\n            expected_output_train = np.array([[[-0.0, 0.13120674, -0.02568678],\n                                               [-0.0, 0.0, -0.20105337],\n                                               [0.0, 0.0, 0.0]],\n                                              [[-0.02063255, 0.25306353, 0.0],\n                                               [0.0, 0.0, 0.0],\n                                               [0.0, 0.0, 0.0]],\n                                              [[0.0, -0.0, -0.0595048],\n                                               [0.03207482, -0.07930075, -0.09382694],\n                                               [0.0, -0.00405498, -0.0]]])\n            assert_allclose(output_train, expected_output_train, rtol=1e-04)\n'"
duplicate_questions/data/instances/__init__.py,0,b''
duplicate_questions/data/instances/instance.py,0,"b'""""""\nThis module contains the base ``Instance`` classes that concrete classes\ninherit from.\n\nSpecifically, there are three classes:\n1. ``Instance``, that just exists as a base type with no functionality\n\n2. ``TextInstance``, which adds a ``words()`` method and a method to convert\n   strings to indices using a DataIndexer.\n\n3. ``IndexedInstance``, which is a ``TextInstance`` that has had all of its\n   strings converted into indices.\n\nThis class has methods to deal with padding (so that sequences all have the\nsame length) and converting an ``Instance`` into a set of Numpy arrays suitable\nfor use with TensorFlow.\n""""""\nfrom ..tokenizers.word_tokenizers import NLTKWordTokenizer\n\n\nclass Instance:\n    """"""\n    A data instance, used either for training a neural network or for\n    testing one.\n\n    Parameters\n    ----------\n    label : boolean or index\n        The label encodes the ground truth label of the Instance.\n        This encoding varies across tasks and instances. If we are\n        making predictions on an unlabeled test set, the label is None.\n    """"""\n    def __init__(self, label=None):\n        self.label = label\n\n\nclass TextInstance(Instance):\n    """"""\n    An ``Instance`` that has some attached text, typically either a sentence\n    or a logical form.\n\n    This is called a ``TextInstance`` because the individual tokens here are\n    encoded as strings, and we can get a list of strings out when we ask what\n    words show up in the instance.\n\n    We use these kinds of instances to fit a ``DataIndexer`` (i.e., deciding\n    which words should be mapped to an unknown token); to use them in training\n    or testing, we need to first convert them into ``IndexedInstances``.\n\n    In order to actually convert text into some kind of indexed sequence, we\n    rely on a ``Tokenizer``.\n    """"""\n\n    def __init__(self, label=None, tokenizer=None):\n        if not tokenizer:\n            self.tokenizer = NLTKWordTokenizer()\n        else:\n            self.tokenizer = tokenizer()\n        super(TextInstance, self).__init__(label)\n\n    def _words_from_text(self, text):\n        """"""\n        This function uses a Tokenizer to output a\n        list of the words in the input string.\n\n        Parameters\n        ----------\n        text: str\n            The label encodes the ground truth label of the Instance.\n            This encoding varies across tasks and instances.\n\n        Returns\n        -------\n        word_list: List[str]\n           A list of the words, as tokenized by the\n           TextInstance\'s tokenizer.\n        """"""\n        return self.tokenizer.get_words_for_indexer(text)\n\n    def _index_text(self, text, data_indexer):\n        """"""\n        This function uses a Tokenizer and an input DataIndexer to convert a\n        string into a list of integers representing the word indices according\n        to the DataIndexer.\n\n        Parameters\n        ----------\n        text: str\n            The label encodes the ground truth label of the Instance.\n            This encoding varies across tasks and instances.\n\n        Returns\n        -------\n        index_list: List[int]\n           A list of the words converted to indices, as tokenized by the\n           TextInstance\'s tokenizer and indexed by the DataIndexer.\n\n        """"""\n        return self.tokenizer.index_text(text, data_indexer)\n\n    def words(self):\n        """"""\n        Returns a list of all of the words in this instance, contained in a\n        namespace dictionary.\n\n        This is mainly used for computing word counts when fitting a word\n        vocabulary on a dataset. The namespace dictionary allows you to have\n        several embedding matrices with different vocab sizes, e.g., for words\n        and for characters (in fact, words and characters are the only use\n        cases I can think of for now, but this allows you to do other more\n        crazy things if you want). You can call the namespaces whatever you\n        want, but if you want the ``DataIndexer`` to work correctly without\n        namespace arguments, you should use the key \'words\' to represent word\n        tokens.\n\n        Returns\n        -------\n        namespace : Dictionary of {str: List[str]}\n            The ``str`` key refers to vocabularies, and the ``List[str]``\n            should contain the tokens in that vocabulary. For example, you\n            should use the key ``words`` to represent word tokens, and the\n            corresponding value in the dictionary would be a list of all the\n            words in the instance.\n        """"""\n        raise NotImplementedError\n\n    def to_indexed_instance(self, data_indexer):\n        """"""\n        Converts the words in this ``Instance`` into indices using the\n        ``DataIndexer``.\n\n        Parameters\n        ----------\n        data_indexer : DataIndexer\n            ``DataIndexer`` to use in converting the ``Instance`` to\n            an ``IndexedInstance``.\n\n        Returns\n        -------\n        indexed_instance : IndexedInstance\n            A ``TextInstance`` that has had all of its strings converted into\n            indices.\n        """"""\n        raise NotImplementedError\n\n    @classmethod\n    def read_from_line(cls, line):\n        """"""\n        Reads an instance of this type from a line.\n\n        Parameters\n        ----------\n        line: str\n            A line from a data file.\n\n        Returns\n        -------\n        indexed_instance: IndexedInstance\n            A ``TextInstance`` that has had all of its strings converted into\n            indices.\n\n        Notes\n        -----\n        We throw a ``RuntimeError`` here instead of a ``NotImplementedError``,\n        because it\'s not expected that all subclasses will implement this.\n        """"""\n        raise RuntimeError(""%s instances can\'t be read ""\n                           ""from a line!"" % str(cls))\n\n\nclass IndexedInstance(Instance):\n    """"""\n    An indexed data instance has all word tokens replaced with word indices,\n    along with some kind of label, suitable for input to a model. An\n    ``IndexedInstance`` is created from an ``Instance`` using a\n    ``DataIndexer``, and the indices here have no recoverable meaning without\n    the ``DataIndexer``.\n\n    For example, we might have the following ``Instance``:\n\n    - ``TrueFalseInstance(\'Jamie is nice, Holly is mean\', True, 25)``\n\n    After being converted into an ``IndexedInstance``, we might have\n    the following:\n    - ``IndexedTrueFalseInstance([1, 6, 7, 1, 6, 8], True, 25)``\n\n    This would mean that ``""Jamie""`` and ``""Holly""`` were OOV to the\n    ``DataIndexer``, and the other words were given indices.\n    """"""\n    @classmethod\n    def empty_instance(cls):\n        """"""\n        Returns an empty, unpadded instance of this class. Necessary for option\n        padding in multiple choice instances.\n        """"""\n        raise NotImplementedError\n\n    def get_lengths(self):\n        """"""\n        Returns the length of this instance in all dimensions that\n        require padding.\n\n        Different kinds of instances have different fields that are padded,\n        such as sentence length, number of background sentences, number of\n        options, etc.\n\n        Returns\n        -------\n        lengths: {str:int}\n            A dict from string to integers, where the value at each string\n            key is sthe max length to pad that dimension.\n        """"""\n        raise NotImplementedError\n\n    def pad(self, max_lengths):\n        """"""\n        Add zero-padding to make each data example of equal length for use\n        in the neural network.\n\n        This modifies the current object.\n\n        Parameters\n        ----------\n        max_lengths: Dictionary of {str:int}\n            In this dictionary, each ``str`` refers to a type of token\n            (e.g. ``max_words_question``), and the corresponding ``int`` is\n            the value. This dictionary must have the same dimension as was\n            returned by ``get_lengths()``. We will use these lengths to pad the\n            instance in all of the necessary dimensions to the given leangths.\n        """"""\n        raise NotImplementedError\n\n    def as_training_data(self):\n        """"""\n        Convert this ``IndexedInstance`` to NumPy arrays suitable for use as\n        training data to models.\n\n        Returns\n        -------\n        train_data : (inputs, label)\n            The ``IndexedInstance`` as NumPy arrays to be used in the model.\n            Note that ``inputs`` might itself be a complex tuple, depending\n            on the ``Instance`` type.\n        """"""\n        raise NotImplementedError\n\n    def as_testing_data(self):\n        """"""\n        Convert this ``IndexedInstance`` to NumPy arrays suitable for use as\n        testing data for models.\n\n        Returns\n        -------\n        test_data : inputs\n            The ``IndexedInstance`` as NumPy arrays to be used in getting\n            predictions from the model. Note that ``inputs`` might itself\n            be a complex tuple, depending on the ``Instance`` type.\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def pad_word_sequence(word_sequence,\n                          sequence_length,\n                          truncate_from_right=True):\n        """"""\n        Take a list of indices and pads them.\n\n        Parameters\n        ----------\n        word_sequence : List of int\n            A list of word indices.\n\n        sequence_length : int\n            The length to pad all the input sequence to.\n\n        truncate_from_right : bool, default=True\n            If truncating the indices is necessary, this parameter dictates\n            whether we do so on the left or right. Truncating from the right\n            means that when we truncate, we remove the end indices first.\n\n        Returns\n        -------\n        padded_word_sequence : List of int\n            A padded list of word indices.\n\n        Notes\n        -----\n        The reason we truncate from the right by default for\n        questions is because the core question is generally at the start, and\n        we at least want to get the core query encoded even if it means that we\n        lose some of the details that are provided at the end. If you want to\n        truncate from the other direction, you can.\n        """"""\n        def default_pad_value():\n            return 0\n\n        padded_word_sequence = IndexedInstance.pad_sequence_to_length(\n            word_sequence, sequence_length,\n            default_pad_value, truncate_from_right)\n        return padded_word_sequence\n\n    @staticmethod\n    def pad_sequence_to_length(sequence,\n                               desired_length,\n                               default_value=lambda: 0,\n                               truncate_from_right=True):\n        """"""\n        Take a list of indices and pads them to the desired length.\n\n        Parameters\n        ----------\n        word_sequence : List of int\n            A list of word indices.\n\n        desired_length : int\n            Maximum length of each sequence. Longer sequences\n            are truncated to this length, and shorter ones are padded to it.\n\n        default_value: int, default=lambda: 0\n            Callable that outputs a default value (of any type) to use as\n            padding values.\n\n        truncate_from_right : bool, default=True\n            If truncating the indices is necessary, this parameter dictates\n            whether we do so on the left or right.\n\n        Returns\n        -------\n        padded_word_sequence : List of int\n            A padded or truncated list of word indices.\n\n        Notes\n        -----\n        The reason we truncate from the right by default is for\n        cases that are questions, with long set ups. We at least want to get\n        the question encoded, which is always at the end, even if we\'ve lost\n        much of the question set up. If you want to truncate from the other\n        direction, you can.\n        """"""\n        if truncate_from_right:\n            truncated = sequence[:desired_length]\n        else:\n            truncated = sequence[-desired_length:]\n        if len(truncated) < desired_length:\n            # If the length of the truncated sequence is less than the desired\n            # length, we need to pad.\n            padding_sequence = [default_value()] * (desired_length -\n                                                    len(truncated))\n            if truncate_from_right:\n                # When we truncate from the right, we add zeroes to the end.\n                truncated.extend(padding_sequence)\n                return truncated\n            else:\n                # When we do not truncate from the right, we add zeroes to the\n                # front.\n                padding_sequence.extend(truncated)\n                return padding_sequence\n        return truncated\n'"
duplicate_questions/data/instances/instance_word.py,0,"b'class IndexedInstanceWord():\n    """"""\n    An InstanceWord represents one word in an IndexedInstance, and stores its\n    int word index and character-level indices (a list of ints).\n\n    This is mostly a convenience class for doing padding on.\n    """"""\n    def __init__(self, word_index, char_indices):\n        """"""\n        Parameters\n        ----------\n        word_index: int\n            The int index representing the word.\n\n        char_indices: List[str]\n            A List of indices corresponding to the characters representing\n            the word.\n        """"""\n        self.word_index = word_index\n        self.char_indices = char_indices\n\n    @classmethod\n    def padding_instance_word(cls):\n        return IndexedInstanceWord(0, [0])\n'"
duplicate_questions/data/instances/sts_instance.py,0,"b'import csv\nfrom copy import deepcopy\nimport itertools\nimport numpy as np\nfrom overrides import overrides\n\nfrom .instance import TextInstance, IndexedInstance\nfrom .instance_word import IndexedInstanceWord\n\n\nclass STSInstance(TextInstance):\n    """"""\n    STSInstance contains a labeled pair of sentences and one binary label.\n\n    You could have the label represent whatever you want, in this repo the\n    label indicates whether or not the sentences are duplicate questions in the\n    Kaggle Quora dataset. 0 indicates that they are not duplicates, 1 indicates\n    that they are.\n\n    Parameters\n    ----------\n    first_sentence: str\n        A string of the first sentence in this training instance.\n\n    second_sentence: str\n        A string of the second sentence in this training instance.\n\n    label: int\n        An int, where 0 indicates that the two sentences are not\n        duplicate questions and 1 indicates that they are.\n\n    """"""\n\n    label_mapping = {0: [1, 0], 1: [0, 1], None: None}\n\n    def __init__(self, first_sentence, second_sentence, label):\n        super(STSInstance, self).__init__(label)\n        self.first_sentence_str = first_sentence\n        self.second_sentence_str = second_sentence\n        # Tokenize the string representations of the first\n        # and second sentence into words and characters\n\n        first_sentence_words = self.tokenizer.tokenize(first_sentence)\n        self.first_sentence_tokenized = {\n            ""words"": first_sentence_words,\n            ""characters"": list(map(list, first_sentence_words))}\n        second_sentence_words = self.tokenizer.tokenize(second_sentence)\n        self.second_sentence_tokenized = {\n            ""words"": second_sentence_words,\n            ""characters"": list(map(list, second_sentence_words))}\n\n    def __str__(self):\n        return (\'STSInstance(\' + self.first_sentence_str + \', \' +\n                self.second_sentence_str + \', \' + str(self.label) + \')\')\n\n    @overrides\n    def words(self):\n        words = deepcopy(self.first_sentence_tokenized)\n        second_sentence_words = deepcopy(self.second_sentence_tokenized)\n\n        # Flatten the list of lists of characters\n        words[""characters""] = list(itertools.chain.from_iterable(words[""characters""]))\n        second_sentence_words[""characters""] = list(itertools.chain.from_iterable(\n            second_sentence_words[""characters""]))\n        for namespace in words:\n            words[namespace].extend(second_sentence_words[namespace])\n        return words\n\n    @overrides\n    def to_indexed_instance(self, data_indexer):\n        indexed_first_words, indexed_first_chars = self._index_text(\n            self.first_sentence_tokenized,\n            data_indexer)\n        indexed_second_words, indexed_second_chars = self._index_text(\n            self.second_sentence_tokenized,\n            data_indexer)\n        # These are lists of IndexedInstanceWords\n        indexed_first_sentence = [IndexedInstanceWord(word, word_characters) for\n                                  word, word_characters in zip(indexed_first_words,\n                                                               indexed_first_chars)]\n        indexed_second_sentence = [IndexedInstanceWord(word, word_characters) for\n                                   word, word_characters in zip(indexed_second_words,\n                                                                indexed_second_chars)]\n\n        return IndexedSTSInstance(indexed_first_sentence,\n                                  indexed_second_sentence,\n                                  self.label_mapping[self.label])\n\n    @classmethod\n    def read_from_line(cls, line):\n        """"""\n        Given a string line from the dataset, construct an STSInstance from it.\n\n        Parameters\n        ----------\n        line: str\n            The line from the dataset from which to construct an STSInstance\n            from. Expected line format for training data:\n            (1) [id],[qid1],[qid2],[question1],[question2],[is_duplicate]\n            Or, in the case of the test set:\n            (2) [id],[question1],[question2]\n\n        Returns\n        -------\n        instance: STSInstance\n            An instance constructed from the data in the line of the dataset.\n        """"""\n        fields = list(csv.reader([line]))[0]\n        if len(fields) == 6:\n            # training set instance\n            _, _, _, first_sentence, second_sentence, label = fields\n            label = int(label)\n        elif len(fields) == 3:\n            # test set instance\n            _, first_sentence, second_sentence = fields\n            label = None\n        else:\n            raise RuntimeError(""Unrecognized line format: "" + line)\n        return cls(first_sentence, second_sentence, label)\n\n\nclass IndexedSTSInstance(IndexedInstance):\n    """"""\n    This is an indexed instance that is commonly used for sentence\n    pairs with a label. In this repo, we are using it to indicate\n    the indices of two question sentences, and the label is a one-hot\n    vector indicating whether the two sentences are duplicates.\n\n    Parameters\n    ----------\n    first_sentence_indices: List of IndexedInstanceWord\n        A list of IndexedInstanceWord representing the word and character\n        indices of the first sentence.\n\n    second_sentence_indices: List of IndexedInstanceWord\n        A list of IndexedInstanceWord representing the word and character\n        indices of the second sentence.\n\n    label: List of int\n        A list of integers representing the label. If the two sentences\n        are not duplicates, the indexed label is [1, 0]. If the two sentences\n        are duplicates, the indexed label is [0, 1].\n    """"""\n    def __init__(self, first_sentence_indices, second_sentence_indices, label):\n        super(IndexedSTSInstance, self).__init__(label)\n        self.first_sentence_indices = first_sentence_indices\n        self.second_sentence_indices = second_sentence_indices\n\n    def get_int_word_indices(self):\n        """"""\n        The method extracts the indices corresponding to the words in this\n        instance.\n\n        Returns\n        -------\n        word_indices: tuple of List[int]\n            A tuple of List[int], where the first list refers to the indices of the words\n            in the first sentence and the second list refers to the indices of the words\n            in the second sentence.\n        """"""\n        first_sentence_word_indices = [idxd_word.word_index for idxd_word in\n                                       self.first_sentence_indices]\n        second_sentence_word_indices = [idxd_word.word_index for idxd_word in\n                                        self.second_sentence_indices]\n        return (first_sentence_word_indices, second_sentence_word_indices)\n\n    def get_int_char_indices(self):\n        """"""\n        The method extracts the indices corresponding to the characters for\n        each word in this instance.\n\n        Returns\n        -------\n        char_indices: tuple of List[List[int]]\n            A tuple of List[int], where the first list refers to the indices of\n            the characters of the words in the first sentence. Each inner list\n            returned contains the character indices, and the length of the list\n            returned corresponds to the number of words in the sentence. The\n            second list refers to the indices of the characters of\n            the words in the second sentence.\n        """"""\n        first_sentence_char_indices = [idxd_word.char_indices for idxd_word in\n                                       self.first_sentence_indices]\n        second_sentence_char_indices = [idxd_word.char_indices for idxd_word in\n                                        self.second_sentence_indices]\n        return (first_sentence_char_indices, second_sentence_char_indices)\n\n    @classmethod\n    @overrides\n    def empty_instance(cls):\n        return IndexedSTSInstance([], [], label=None)\n\n    @overrides\n    def get_lengths(self):\n        """"""\n        Returns the maximum length of the two\n        sentences as a dictionary.\n\n        Returns\n        -------\n        lengths_dict: Dictionary of string to int\n            The ""num_sentence_words"" and ""num_word_characters"" keys are\n            hard-coded to have the length to pad to. This is kind\n            of a messy API, but I\'ve not yet thought of\n            a cleaner way to do it.\n        """"""\n        # Length of longest sentence, as measured in # words.\n        first_sentence_word_len = len(self.first_sentence_indices)\n        second_sentence_word_len = len(self.second_sentence_indices)\n        # Length of longest word, as measured in # characters\n        # The length of the list can be 0, so we have to take some\n        # precautions with the max.\n        first_sentence_chars = [len(idxd_word.char_indices) for\n                                idxd_word in self.first_sentence_indices]\n        if first_sentence_chars:\n            first_sentence_chars_len = max(first_sentence_chars)\n        else:\n            first_sentence_chars_len = 0\n        second_sentence_chars = [len(idxd_word.char_indices) for\n                                 idxd_word in self.second_sentence_indices]\n        if second_sentence_chars:\n            second_sentence_chars_len = max(second_sentence_chars)\n        else:\n            second_sentence_chars_len = 0\n        lengths = {""num_sentence_words"": max(first_sentence_word_len,\n                                             second_sentence_word_len),\n                   ""num_word_characters"": max(first_sentence_chars_len,\n                                              second_sentence_chars_len)}\n        return lengths\n\n    @overrides\n    def pad(self, max_lengths):\n        """"""\n        Pads or truncates each of the sentences, according to the input lengths\n        dictionary. This dictionary is usually acquired from get_lengths.\n\n        Parameters\n        ----------\n        max_lengths: Dictionary of string to int\n            The dictionary holding the lengths to pad the sequences to.\n            In this case, we pad both to the value of the\n            ""num_sentence_words"" key.\n        """"""\n        num_sentence_words = max_lengths[""num_sentence_words""]\n        num_word_characters = max_lengths[""num_word_characters""]\n        # Pad at the word-level, adding empty IndexedInstanceWords\n        self.first_sentence_indices = self.pad_sequence_to_length(\n            self.first_sentence_indices,\n            num_sentence_words,\n            default_value=IndexedInstanceWord.padding_instance_word)\n        self.second_sentence_indices = self.pad_sequence_to_length(\n            self.second_sentence_indices,\n            num_sentence_words,\n            default_value=IndexedInstanceWord.padding_instance_word)\n\n        # Pad at the character-level, adding 0 padding to character list\n        for indexed_instance_word in self.first_sentence_indices:\n            indexed_instance_word.char_indices = self.pad_sequence_to_length(\n                indexed_instance_word.char_indices,\n                num_word_characters)\n\n        for indexed_instance_word in self.second_sentence_indices:\n            indexed_instance_word.char_indices = self.pad_sequence_to_length(\n                indexed_instance_word.char_indices,\n                num_word_characters)\n\n    @overrides\n    def as_training_data(self, mode=""word""):\n        """"""\n        Transforms the instance into a collection of NumPy\n        arrays suitable for use as training data in the model.\n\n        Parameters\n        ----------\n        mode: str, optional (default=""word"")\n            String describing whether to return the word-level representations,\n            character-level representations, or both. One of ""word"",\n            ""character"", or ""word+character""\n\n        Returns\n        -------\n        data_tuple: tuple\n            The outer tuple has two elements.\n            The first element of this outer tuple is another tuple, with the\n            ""training data"". In this case, this is the NumPy arrays of the\n            first and second sentence. The second element of the outer tuple is\n            a NumPy array with the label.\n        """"""\n        if self.label is None:\n            raise ValueError(""self.label is None so this is a test example. ""\n                             ""You cannot call as_training_data on it."")\n        if mode not in set([""word"", ""character"", ""word+character""]):\n            raise ValueError(""Input mode was {}, expected \\""word\\"",""\n                             ""\\""character\\"", or \\""word+character\\"""")\n        if mode == ""word"" or mode == ""word+character"":\n            first_sentence_word_array = np.asarray([word.word_index for word\n                                                    in self.first_sentence_indices],\n                                                   dtype=""int32"")\n            second_sentence_word_array = np.asarray([word.word_index for word\n                                                     in self.second_sentence_indices],\n                                                    dtype=""int32"")\n        if mode == ""character"" or mode == ""word+character"":\n            first_sentence_char_matrix = np.asarray([word.char_indices for word\n                                                     in self.first_sentence_indices],\n                                                    dtype=""int32"")\n            second_sentence_char_matrix = np.asarray([word.char_indices for word\n                                                      in self.second_sentence_indices],\n                                                     dtype=""int32"")\n        if mode == ""character"":\n            return ((first_sentence_char_matrix, second_sentence_char_matrix),\n                    (np.asarray(self.label),))\n        if mode == ""word"":\n            return ((first_sentence_word_array, second_sentence_word_array),\n                    (np.asarray(self.label),))\n        if mode == ""word+character"":\n            return ((first_sentence_word_array, first_sentence_char_matrix,\n                     second_sentence_word_array, second_sentence_char_matrix),\n                    (np.asarray(self.label),))\n\n    @overrides\n    def as_testing_data(self, mode=""word""):\n        """"""\n        Transforms the instance into a collection of NumPy\n        arrays suitable for use as testing data in the model.\n\n        Parameters\n        ----------\n        mode: str, optional (default=""word"")\n            String describing whether to return the word-level representations,\n            character-level representations, or both. One of ""word"",\n            ""character"", or ""word+character"".\n\n        Returns\n        -------\n        data_tuple: tuple\n            The first element of this tuple has the NumPy array\n            of the first sentence, and the second element has the\n            NumPy array of the second sentence.\n        """"""\n        if mode not in set([""word"", ""character"", ""word+character""]):\n            raise ValueError(""Input mode was {}, expected \\""word\\"",""\n                             ""\\""character\\"", or \\""word+character\\"""")\n        if mode == ""word"" or mode == ""word+character"":\n            first_sentence_word_array = np.asarray([word.word_index for word\n                                                    in self.first_sentence_indices],\n                                                   dtype=""int32"")\n            second_sentence_word_array = np.asarray([word.word_index for word\n                                                     in self.second_sentence_indices],\n                                                    dtype=""int32"")\n        if mode == ""character"" or mode == ""word+character"":\n            first_sentence_char_matrix = np.asarray([word.char_indices for word\n                                                     in self.first_sentence_indices],\n                                                    dtype=""int32"")\n            second_sentence_char_matrix = np.asarray([word.char_indices for word\n                                                      in self.second_sentence_indices],\n                                                     dtype=""int32"")\n        if mode == ""character"":\n            return ((first_sentence_char_matrix, second_sentence_char_matrix),\n                    ())\n        if mode == ""word"":\n            return ((first_sentence_word_array, second_sentence_word_array),\n                    ())\n        if mode == ""word+character"":\n            return ((first_sentence_word_array, first_sentence_char_matrix,\n                     second_sentence_word_array, second_sentence_char_matrix),\n                    ())\n\n    @overrides\n    def __eq__(self, other):\n        """"""\n        Checks for equality between this instance and another instance.\n        Two IndexedSTSInstance objects are equal when they have the same\n        sentence lengths and the same word indices for each sentence.\n\n        Parameters\n        ----------\n        other: IndexedSTSInstance\n            The IndexedSTSInstance this instance is being compared against.\n\n        Returns\n        -------\n        equality: boolean\n            Returns whether or not the two instances are equal.\n        """"""\n\n        if not isinstance(other, self.__class__):\n            return False\n\n        this_length = self.get_lengths()[""num_sentence_words""]\n        other_length = other.get_lengths()[""num_sentence_words""]\n        if this_length == other_length:\n            sentence_1, sentence_2 = self.get_int_word_indices()\n            other_sentence_1, other_sentence_2 = other.get_int_word_indices()\n            for word_instance_1, word_instance_2 in zip(sentence_1,\n                                                        other_sentence_1):\n                if word_instance_1 != word_instance_2:\n                    return False\n            for word_instance_1, word_instance_2 in zip(sentence_2,\n                                                        other_sentence_2):\n                if word_instance_1 != word_instance_2:\n                    return False\n            return True\n        else:\n            return False\n\n    @overrides\n    def __lt__(self, other):\n        """"""\n        Checks for the less than relationship between this instance\n        and another instance. if the maximum length of the two sentences in this instance\n        is less than the maximum length of the two sentences in the other instance.\n\n        Parameters\n        ----------\n        other: IndexedSTSInstance\n            The IndexedSTSInstance this instance is being compared against.\n\n        Returns\n        -------\n        lt: boolean\n            Returns whether or not the this instance is less than the other.\n        """"""\n        if not isinstance(other, self.__class__):\n            return False\n\n        this_length = self.get_lengths()[""num_sentence_words""]\n        other_length = other.get_lengths()[""num_sentence_words""]\n        if this_length == other_length:\n            sentence_1, sentence_2 = self.get_int_word_indices()\n            other_sentence_1, other_sentence_2 = other.get_int_word_indices()\n            for word_instance_1, word_instance_2 in zip(sentence_1,\n                                                        other_sentence_1):\n                if word_instance_1 != word_instance_2:\n                    return word_instance_1 < word_instance_2\n            for word_instance_1, word_instance_2 in zip(sentence_2,\n                                                        other_sentence_2):\n                if word_instance_1 != word_instance_2:\n                    return word_instance_1 < word_instance_2\n            return False\n        else:\n            return this_length < other_length\n'"
duplicate_questions/data/tokenizers/__init__.py,0,b''
duplicate_questions/data/tokenizers/word_tokenizers.py,0,"b'import nltk\n\n\nclass NLTKWordTokenizer():\n    """"""\n    A NLTKWordTokenizer splits strings into word tokens with NLTK.\n    """"""\n    def tokenize(self, sentence):\n        """"""\n        Given a string, tokenize it into words (with the conventional notion\n        of word).\n\n        Parameters\n        ----------\n        sentence: str\n            The string to tokenize.\n\n        Returns\n        -------\n        tokenized_sentence: List[str]\n            The tokenized representation of the string, as a list of tokens.\n        """"""\n        return nltk.word_tokenize(sentence.lower())\n\n    def get_words_for_indexer(self, text):\n        """"""\n        Given a string, get the tokens in it that should be run\n        through a DataIndexer. In this class, this involves tokenizing\n        it into words (with the conventional notion of word), and then\n        returning a namespace dictionary suitable for the DataIndexer.\n\n        Parameters\n        ----------\n        text: str\n            The string to tokenize.\n\n        Returns\n        -------\n        namespace_dict: Dict of {str:List[str]}\n            Return a dictionary, where the key is the name of the namespace\n            (""words"") for this tokenizer, and the value is the tokenized\n            representation of the string, as a list of tokens.\n        """"""\n        words = self.tokenize(text)\n        characters = [char for word in words for char in word]\n        return {""words"": words, ""characters"": characters}\n\n    def index_text(self, tokenized_sentence, data_indexer):\n        """"""\n        Given a namespace dictionary with ""words"" and ""characters"" keys, as\n        well as a DataIndexer, assign indices to each of the ""words"" and\n        ""characters"" tokens based on the DataIndexer.\n\n        Parameters\n        ----------\n        tokenized_setence: Dict {""words"": List[str], ""characters"": List[List[str]]}\n            A dictionary representing the string as tokenized at the word and\n            character level, as generated by ``self.get_words_for_indexer``.\n\n        data_indexer: DataIndexer\n            A DataIndexer to use in deciding what index to assign each word.\n            This should be a pre-fit DataIndexer.\n\n        Returns\n        -------\n        indexed_text: (List[int], List[List[int]])\n            A tuple with two parts. The first element is a list of int, where\n            each index of the list represents a word-level token of\n            the original string that was then assigned an index by the\n            DataIndexer. The second element is a list of list of ints, where\n            each inner list is a word tokenized into characters, and the\n            contents of each inner list are the indices assigned to each\n            character.\n        """"""\n        word_indexed_text = [data_indexer.get_word_index(word, namespace=""words"")\n                             for word in tokenized_sentence[""words""]]\n        character_indexed_text = []\n        for word in tokenized_sentence[""characters""]:\n            character_indexed_word = []\n            for character in word:\n                character_index = data_indexer.get_word_index(character,\n                                                              namespace=""characters"")\n                character_indexed_word.append(character_index)\n            character_indexed_text.append(character_indexed_word)\n        return (word_indexed_text, character_indexed_text)\n'"
duplicate_questions/models/bimpm/__init__.py,0,b''
duplicate_questions/models/bimpm/bimpm.py,89,"b'from copy import deepcopy\nimport logging\nfrom overrides import overrides\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import LSTMCell\n\nfrom .matching import bilateral_matching\nfrom ..base_tf_model import BaseTFModel\nfrom ...util.rnn import last_relevant_output\n\nlogger = logging.getLogger(__name__)\n\n\nclass BiMPM(BaseTFModel):\n    """"""\n    Parameters\n    ----------\n    mode: str\n        One of [train|predict], to indicate what you want the model to do.\n        If you pick ""predict"", then you must also supply the path to a\n        pretrained model and DataIndexer to load to the ``predict`` method.\n\n    word_vocab_size: int\n        The number of unique tokens in the dataset, plus the UNK and padding\n        tokens. Alternatively, the highest index assigned to any word, +1.\n        This is used by the model to figure out the dimensionality of the\n        embedding matrix.\n\n    word_embedding_dim: int\n        The length of a word embedding. This is used by\n        the model to figure out the dimensionality of the word\n        embedding matrix.\n\n    word_embedding_matrix: numpy array, optional if predicting\n        A numpy array of shape (word_vocab_size, word_emb_dim).\n        word_embedding_matrix[index] should represent the word vector for\n        that particular word index. This is used to initialize the\n        word embedding matrix in the model, and is optional if predicting\n        since we assume that the word embeddings variable will be loaded\n        with the model.\n\n    char_vocab_size: int\n        The number of unique character tokens in the dataset, plus the UNK\n        and padding tokens. Alternatively, the highest index assigned to any\n        word, +1. This is used by the model to figure out the dimensionality\n        of the embedding matrix.\n\n    char_embedding_dim: int\n        The length of a character embedding. This is used by\n        the model to figure out the dimensionality of the character\n        embedding matrix.\n\n    char_embedding_matrix: numpy array, optional if predicting\n        A numpy array of shape (char_vocab_size, char_emb_dim).\n        char_embedding_matrix[index] should represent the char vector for\n        that particular char index. This is used to initialize the\n        char embedding matrix in the model, and is optional if predicting\n        since we assume that the char embeddings variable will be loaded\n        with the model.\n\n    char_rnn_hidden_size: int\n        The number of hidden units in the LSTM used to encode the character\n        vectors into a single word representation.\n\n    fine_tune_embeddings: boolean\n        If true, sets the embeddings to be trainable.\n\n    context_rnn_hidden_size: int\n        The output dimension of the RNN encoder in the context representation layer\n        (to get a vector for each sentence). Note that this model uses a\n        bidirectional LSTM, so there will be two sentence vectors with this\n        dimensionality.\n\n    aggregation_rnn_hidden_size: int\n        The output dimension of the RNN encoder in the aggregation layer\n        (to compose the matching vectors into a vector). Note that this model uses a\n        bidirectional LSTM, so there will be two aggregation vectors with this\n        dimensionality.\n\n    dropout_ratio: float\n        The dropout ratio applied after every layer of the model.\n    """"""\n\n    @overrides\n    def __init__(self, config_dict):\n        config_dict = deepcopy(config_dict)\n        mode = config_dict.pop(""mode"")\n        super(BiMPM, self).__init__(mode=mode)\n\n        self.word_vocab_size = config_dict.pop(""word_vocab_size"")\n        self.word_embedding_dim = config_dict.pop(""word_embedding_dim"")\n        self.word_embedding_matrix = config_dict.pop(""word_embedding_matrix"", None)\n        self.char_vocab_size = config_dict.pop(""char_vocab_size"")\n        self.char_embedding_dim = config_dict.pop(""char_embedding_dim"")\n        self.char_embedding_matrix = config_dict.pop(""char_embedding_matrix"", None)\n        self.char_rnn_hidden_size = config_dict.pop(""char_rnn_hidden_size"")\n        self.fine_tune_embeddings = config_dict.pop(""fine_tune_embeddings"")\n        self.context_rnn_hidden_size = config_dict.pop(""context_rnn_hidden_size"")\n        self.aggregation_rnn_hidden_size = config_dict.pop(""aggregation_rnn_hidden_size"")\n        self.dropout_ratio = config_dict.pop(""dropout_ratio"")\n\n        if config_dict:\n            logger.warning(""UNUSED VALUES IN CONFIG DICT: {}"".format(config_dict))\n\n    @overrides\n    def _create_placeholders(self):\n        """"""\n        Create the placeholders for use in the model.\n        """"""\n        # Define the inputs here\n        # Shape: (batch_size, num_sentence_words)\n        # The first input sentence, indexed by word.\n        self.sentence_one_word = tf.placeholder(""int32"",\n                                                [None, None],\n                                                name=""sentence_one_word"")\n\n        # Shape: (batch_size, num_sentence_words, num_word_characters)\n        # The first input sentence, indexed by character.\n        self.sentence_one_char = tf.placeholder(""int32"",\n                                                [None, None, None],\n                                                name=""sentence_one_char"")\n        # Shape: (batch_size, num_sentence_words)\n        # The second input sentence, indexed by word.\n        self.sentence_two_word = tf.placeholder(""int32"",\n                                                [None, None],\n                                                name=""sentence_two_word"")\n        # Shape: (batch_size, num_sentence_words, num_word_characters)\n        # The second input sentence, indexed by character.\n        self.sentence_two_char = tf.placeholder(""int32"",\n                                                [None, None, None],\n                                                name=""sentence_two_char"")\n\n        # Shape: (batch_size, 2)\n        # The true labels, encoded as a one-hot vector. So\n        # [1, 0] indicates not duplicate, [0, 1] indicates duplicate.\n        self.y_true = tf.placeholder(""int32"",\n                                     [None, 2],\n                                     name=""true_labels"")\n\n        # A boolean that encodes whether we are training or evaluating\n        self.is_train = tf.placeholder(\'bool\', [], name=\'is_train\')\n\n    @overrides\n    def _build_forward(self):\n        """"""\n        Using the values in the config passed to the BiMPM object\n        on creation, build the forward pass of the computation graph.\n        """"""\n        with tf.name_scope(""helper_lengths""):\n            batch_size = tf.shape(self.sentence_one_word)[0]\n            # The number of words in a sentence.\n            num_sentence_words = tf.shape(self.sentence_one_word)[1]\n            # The number of characters in a word\n            num_word_characters = tf.shape(self.sentence_one_char)[2]\n\n            # A mask over the word indices in the sentence, indicating\n            # which indices are padding and which are words.\n            # Shape: (batch_size, num_sentence_words)\n            sentence_one_wordlevel_mask = tf.sign(self.sentence_one_word,\n                                                  name=""sentence_one_word_mask"")\n            sentence_two_wordlevel_mask = tf.sign(self.sentence_two_word,\n                                                  name=""sentence_two_word_mask"")\n\n            # A mask over the char indices in the char indexed sentence, indicating\n            # which indices are padding and which are chars.\n            # Shape: (batch_size, num_sentence_words, num_word_characters)\n            sentence_one_charlevel_mask = tf.sign(self.sentence_one_char,\n                                                  name=""sentence_one_char_mask"")\n            sentence_two_charlevel_mask = tf.sign(self.sentence_two_char,\n                                                  name=""sentence_two_char_mask"")\n\n            # The unpadded word lengths of sentence one and sentence two\n            # Shape: (batch_size,)\n            sentence_one_len = tf.reduce_sum(sentence_one_wordlevel_mask, 1)\n            sentence_two_len = tf.reduce_sum(sentence_two_wordlevel_mask, 1)\n\n            # The unpadded character lengths of each of the words in sentence one\n            # and sentence two.\n            # Shape: (batch_size, num_sentence_words)\n            sentence_one_words_len = tf.reduce_sum(sentence_one_charlevel_mask, 2)\n            sentence_two_words_len = tf.reduce_sum(sentence_two_charlevel_mask, 2)\n\n        with tf.variable_scope(""embeddings""):\n            # Embedding variables\n            word_vocab_size = self.word_vocab_size\n            word_embedding_dim = self.word_embedding_dim\n            word_embedding_matrix = self.word_embedding_matrix\n            char_vocab_size = self.char_vocab_size\n            char_embedding_dim = self.char_embedding_dim\n            char_embedding_matrix = self.char_embedding_matrix\n            fine_tune_embeddings = self.fine_tune_embeddings\n\n            with tf.variable_scope(""embedding_var""):\n                if self.mode == ""train"":\n                    # Load the word embedding matrix that was passed in\n                    # to the configuration dict since we are training\n                    word_emb_mat = tf.get_variable(\n                        ""word_emb_mat"",\n                        dtype=""float"",\n                        shape=[word_vocab_size,\n                               word_embedding_dim],\n                        initializer=tf.constant_initializer(\n                            word_embedding_matrix),\n                        trainable=fine_tune_embeddings)\n                    char_emb_mat = tf.get_variable(\n                        ""char_emb_mat"",\n                        dtype=""float"",\n                        shape=[char_vocab_size,\n                               char_embedding_dim],\n                        initializer=tf.constant_initializer(\n                            char_embedding_matrix))\n                else:\n                    # We are not training, so a model should have been\n                    # loaded with the embedding matrices already there.\n                    word_emb_mat = tf.get_variable(""word_emb_mat"",\n                                                   shape=[word_vocab_size,\n                                                          word_embedding_dim],\n                                                   dtype=""float"",\n                                                   trainable=fine_tune_embeddings)\n                    char_emb_mat = tf.get_variable(""char_emb_mat"",\n                                                   shape=[char_vocab_size,\n                                                          char_embedding_dim],\n                                                   dtype=""float"")\n\n            # Retrieve the word embeddings for the sentence.\n            with tf.name_scope(""word_embeddings""):\n                # Shape: (batch_size, num_sentence_words, word_embed_dim)\n                word_embedded_sentence_one = tf.nn.embedding_lookup(\n                    word_emb_mat,\n                    self.sentence_one_word)\n                # Shape: (batch_size, num_sentence_words, word_embed_dim)\n                word_embedded_sentence_two = tf.nn.embedding_lookup(\n                    word_emb_mat,\n                    self.sentence_two_word)\n\n            # Construct the character embedding for each word in the sentence.\n            with tf.name_scope(""char_embeddings""):\n                # Shapes: (batch_size, num_sentence_words,\n                #          num_word_characters, char_embed_dim)\n                char_embedded_sentence_one = tf.nn.embedding_lookup(\n                    char_emb_mat,\n                    self.sentence_one_char)\n                char_embedded_sentence_two = tf.nn.embedding_lookup(\n                    char_emb_mat,\n                    self.sentence_two_char)\n                # Need to flatten it for the shape to be compatible with the RNN.\n                # Shapes: (batch_size*num_sentence_words,\n                #          num_word_characters, char_embed_dim)\n                flat_char_embedded_sentence_one = tf.reshape(\n                    char_embedded_sentence_one,\n                    [batch_size * num_sentence_words,\n                     num_word_characters, char_embedding_dim])\n                flat_char_embedded_sentence_two = tf.reshape(\n                    char_embedded_sentence_two,\n                    [batch_size * num_sentence_words, num_word_characters,\n                     char_embedding_dim])\n                # Shapes: (batch_size*num_sentence_words,)\n                flat_sentence_one_words_len = tf.reshape(\n                    sentence_one_words_len,\n                    [batch_size * num_sentence_words])\n                flat_sentence_two_words_len = tf.reshape(\n                    sentence_two_words_len,\n                    [batch_size * num_sentence_words])\n\n                # Encode the character vectors into a word vector.\n                with tf.variable_scope(""char_lstm""):\n                    char_rnn_hidden_size = self.char_rnn_hidden_size\n                    dropout_ratio = self.dropout_ratio\n                    char_lstm_cell = LSTMCell(char_rnn_hidden_size)\n                    sentence_one_char_output, _ = tf.nn.dynamic_rnn(\n                        char_lstm_cell,\n                        dtype=""float"",\n                        sequence_length=flat_sentence_one_words_len,\n                        inputs=flat_char_embedded_sentence_one)\n                    d_sentence_one_char_output = tf.layers.dropout(\n                        sentence_one_char_output,\n                        rate=dropout_ratio,\n                        training=self.is_train,\n                        name=""sentence_one_char_lstm_dropout"")\n                    tf.get_variable_scope().reuse_variables()\n                    sentence_two_char_output, _ = tf.nn.dynamic_rnn(\n                        char_lstm_cell,\n                        dtype=""float"",\n                        sequence_length=flat_sentence_two_words_len,\n                        inputs=flat_char_embedded_sentence_two)\n                    d_sentence_two_char_output = tf.layers.dropout(\n                        sentence_two_char_output,\n                        rate=dropout_ratio,\n                        training=self.is_train,\n                        name=""sentence_two_char_lstm_dropout"")\n                    # Get the last relevant output of the LSTM with respect\n                    # to sequence length.\n                    # Shapes: (batch_size*num_sentence_words, char_rnn_hidden_size)\n                    flat_sentence_one_char_repr = last_relevant_output(\n                        d_sentence_one_char_output,\n                        flat_sentence_one_words_len)\n                    flat_sentence_two_char_repr = last_relevant_output(\n                        d_sentence_two_char_output,\n                        flat_sentence_two_words_len)\n                    # Take the RNN output of the flat representation and transform it back\n                    # into the original shape.\n                    # Shapes: (batch_size, num_sentence_words, char_rnn_hidden_size)\n                    sentence_one_char_repr = tf.reshape(\n                        flat_sentence_one_char_repr,\n                        [batch_size, num_sentence_words, char_rnn_hidden_size])\n                    sentence_two_char_repr = tf.reshape(\n                        flat_sentence_two_char_repr,\n                        [batch_size, num_sentence_words, char_rnn_hidden_size])\n\n            # Combine the word-level and character-level representations.\n            # Shapes: (batch_size, num_sentence_words,\n            #          word_embed_dim+char_rnn_hidden_size)\n            embedded_sentence_one = tf.concat([word_embedded_sentence_one,\n                                               sentence_one_char_repr], 2)\n            embedded_sentence_two = tf.concat([word_embedded_sentence_two,\n                                               sentence_two_char_repr], 2)\n\n            # Apply dropout to the embeddings, but only if we are training.\n            # Shapes: (batch_size, num_sentence_words,\n            #          word_embed_dim+char_rnn_hidden_size)\n            embedded_sentence_one = tf.layers.dropout(\n                embedded_sentence_one,\n                rate=dropout_ratio,\n                training=self.is_train,\n                name=""sentence_one_embedding_dropout"")\n            embedded_sentence_two = tf.layers.dropout(\n                embedded_sentence_two,\n                rate=dropout_ratio,\n                training=self.is_train,\n                name=""sentence_two_embedding_dropout"")\n\n        # Encode the embedded sentences with a BiLSTM to get two vectors\n        # for each sentence (one from forward LSTM and one from backward LSTM).\n        with tf.variable_scope(""context_representation_layer""):\n            context_rnn_hidden_size = self.context_rnn_hidden_size\n            sentence_enc_fw = LSTMCell(context_rnn_hidden_size,\n                                       state_is_tuple=True)\n            sentence_enc_bw = LSTMCell(context_rnn_hidden_size,\n                                       state_is_tuple=True)\n\n            # Encode sentence one.\n            # Shapes: (batch_size, num_sentence_words, context_rnn_hidden_size)\n            (sentence_one_fw_representation,\n             sentence_one_bw_representation), _ = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=sentence_enc_fw,\n                cell_bw=sentence_enc_bw,\n                dtype=""float"",\n                sequence_length=sentence_one_len,\n                inputs=embedded_sentence_one,\n                scope=""encoded_sentence_one"")\n            d_sentence_one_fw_representation = tf.layers.dropout(\n                sentence_one_fw_representation,\n                rate=dropout_ratio,\n                training=self.is_train,\n                name=""sentence_one_fw_representation_dropout"")\n            d_sentence_one_bw_representation = tf.layers.dropout(\n                sentence_one_bw_representation,\n                rate=dropout_ratio,\n                training=self.is_train,\n                name=""sentence_one_bw_representation_dropout"")\n\n            tf.get_variable_scope().reuse_variables()\n            # Encode sentence two with the same biLSTM as sentence one.\n            # Shapes: (batch_size, num_sentence_words, context_rnn_hidden_size)\n            (sentence_two_fw_representation,\n             sentence_two_bw_representation), _ = tf.nn.bidirectional_dynamic_rnn(\n                 cell_fw=sentence_enc_fw,\n                 cell_bw=sentence_enc_bw,\n                 dtype=""float"",\n                 sequence_length=sentence_two_len,\n                 inputs=embedded_sentence_two,\n                 scope=""encoded_sentence_one"")\n            d_sentence_two_fw_representation = tf.layers.dropout(\n                sentence_two_fw_representation,\n                rate=dropout_ratio,\n                training=self.is_train,\n                name=""sentence_two_fw_representation_dropout"")\n            d_sentence_two_bw_representation = tf.layers.dropout(\n                sentence_two_bw_representation,\n                rate=dropout_ratio,\n                training=self.is_train,\n                name=""sentence_two_bw_representation_dropout"")\n\n        # Apply the bilateral matching function to the embedded sentence\n        # one and the embedded sentence two.\n        with tf.variable_scope(""matching_layer""):\n            # Shapes: (batch_size, num_sentence_words, 8*multiperspective_dims)\n            match_one_to_two_out, match_two_to_one_out = bilateral_matching(\n                d_sentence_one_fw_representation, d_sentence_one_bw_representation,\n                d_sentence_two_fw_representation, d_sentence_two_bw_representation,\n                sentence_one_wordlevel_mask, sentence_two_wordlevel_mask, self.is_train,\n                dropout_ratio)\n\n        # Aggregate the representations from the matching\n        # functions.\n        with tf.variable_scope(""aggregation_layer""):\n            aggregated_representations = []\n            sentence_one_aggregation_input = match_one_to_two_out\n            sentence_two_aggregation_input = match_two_to_one_out\n            aggregation_rnn_hidden_size = self.aggregation_rnn_hidden_size\n\n            with tf.variable_scope(""aggregate_sentence_one""):\n                aggregation_lstm_fw = LSTMCell(aggregation_rnn_hidden_size)\n\n                aggregation_lstm_bw = LSTMCell(aggregation_rnn_hidden_size)\n                # Encode the matching output into a fixed size vector.\n                # Shapes: (batch_size, num_sentence_words, aggregation_rnn_hidden_size)\n                (fw_agg_outputs, bw_agg_outputs), _ = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw=aggregation_lstm_fw,\n                    cell_bw=aggregation_lstm_bw,\n                    dtype=""float"",\n                    sequence_length=sentence_one_len,\n                    inputs=sentence_one_aggregation_input,\n                    scope=""encode_sentence_one_matching_vectors"")\n                d_fw_agg_outputs = tf.layers.dropout(\n                    fw_agg_outputs,\n                    rate=dropout_ratio,\n                    training=self.is_train,\n                    name=""sentence_one_fw_agg_outputs_dropout"")\n                d_bw_agg_outputs = tf.layers.dropout(\n                    bw_agg_outputs,\n                    rate=dropout_ratio,\n                    training=self.is_train,\n                    name=""sentence_one_bw_agg_outputs_dropout"")\n\n                # Get the last output (wrt padding) of the LSTM.\n                # Shapes: (batch_size, aggregation_rnn_hidden_size)\n                last_fw_agg_output = last_relevant_output(d_fw_agg_outputs,\n                                                          sentence_one_len)\n                last_bw_agg_output = last_relevant_output(d_bw_agg_outputs,\n                                                          sentence_one_len)\n                aggregated_representations.append(last_fw_agg_output)\n                aggregated_representations.append(last_bw_agg_output)\n\n            with tf.variable_scope(""aggregate_sentence_two""):\n                aggregation_lstm_fw = LSTMCell(aggregation_rnn_hidden_size)\n                aggregation_lstm_bw = LSTMCell(aggregation_rnn_hidden_size)\n                # Encode the matching output into a fixed size vector.\n                # Shapes: (batch_size, num_sentence_words, aggregation_rnn_hidden_size)\n                (fw_agg_outputs, bw_agg_outputs), _ = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw=aggregation_lstm_fw,\n                    cell_bw=aggregation_lstm_bw,\n                    dtype=""float"",\n                    sequence_length=sentence_two_len,\n                    inputs=sentence_two_aggregation_input,\n                    scope=""encode_sentence_two_matching_vectors"")\n                d_fw_agg_outputs = tf.layers.dropout(\n                    fw_agg_outputs,\n                    rate=dropout_ratio,\n                    training=self.is_train,\n                    name=""sentence_two_fw_agg_outputs_dropout"")\n                d_bw_agg_outputs = tf.layers.dropout(\n                    bw_agg_outputs,\n                    rate=dropout_ratio,\n                    training=self.is_train,\n                    name=""sentence_two_bw_agg_outputs_dropout"")\n\n                # Get the last output (wrt padding) of the LSTM.\n                # Shapes: (batch_size, aggregation_rnn_hidden_size)\n                last_fw_agg_output = last_relevant_output(d_fw_agg_outputs,\n                                                          sentence_two_len)\n                last_bw_agg_output = last_relevant_output(d_bw_agg_outputs,\n                                                          sentence_two_len)\n                aggregated_representations.append(last_fw_agg_output)\n                aggregated_representations.append(last_bw_agg_output)\n            # Combine the 4 aggregated representations (fw a to b, bw a to b,\n            # fw b to a, bw b to a)\n            # Shape: (batch_size, 4*aggregation_rnn_hidden_size)\n            combined_aggregated_representation = tf.concat(aggregated_representations, 1)\n\n        with tf.variable_scope(""prediction_layer""):\n            # Now, we pass the combined aggregated representation\n            # through a 2-layer feed forward NN.\n            predict_layer_one_out = tf.layers.dense(\n                combined_aggregated_representation,\n                combined_aggregated_representation.get_shape().as_list()[1],\n                activation=tf.nn.tanh,\n                name=""prediction_layer_one"")\n            d_predict_layer_one_out = tf.layers.dropout(\n                predict_layer_one_out,\n                rate=dropout_ratio,\n                training=self.is_train,\n                name=""prediction_layer_dropout"")\n            predict_layer_two_logits = tf.layers.dense(\n                d_predict_layer_one_out,\n                units=2,\n                name=""prediction_layer_two"")\n\n        with tf.name_scope(""loss""):\n            # get the predicted class probabilities\n            # Shape: (batch_size, 2)\n            self.y_pred = tf.nn.softmax(predict_layer_two_logits,\n                                        name=""softmax_probabilities"")\n            # Use softmax_cross_entropy_with_logits to calculate xentropy\n            self.loss = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(\n                    labels=self.y_true,\n                    logits=predict_layer_two_logits,\n                    name=""xentropy_loss""))\n\n        with tf.name_scope(""accuracy""):\n            # Get the correct predictions.\n            # Shape: (batch_size,) of bool\n            correct_predictions = tf.equal(\n                tf.argmax(self.y_pred, 1),\n                tf.argmax(self.y_true, 1))\n\n            # Cast to float, and take the mean to get accuracy\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions,\n                                                   ""float""))\n        with tf.name_scope(""train""):\n            optimizer = tf.train.AdamOptimizer()\n            # Gradient clipping\n            clip_norm = 50\n            trainable_vars = tf.trainable_variables()\n            gradients, _ = tf.clip_by_global_norm(tf.gradients(self.loss,\n                                                               trainable_vars),\n                                                  clip_norm)\n            self.training_op = optimizer.apply_gradients(zip(gradients,\n                                                             trainable_vars),\n                                                         global_step=self.global_step)\n        with tf.name_scope(""train_summaries""):\n            # Add the loss and the accuracy to the tensorboard summary\n            tf.summary.scalar(""loss"", self.loss)\n            tf.summary.scalar(""accuracy"", self.accuracy)\n            self.summary_op = tf.summary.merge_all()\n\n    @overrides\n    def _get_train_feed_dict(self, batch):\n        inputs, targets = batch\n        feed_dict = {self.sentence_one_word: inputs[0],\n                     self.sentence_one_char: inputs[1],\n                     self.sentence_two_word: inputs[2],\n                     self.sentence_two_char: inputs[3],\n                     self.y_true: targets[0],\n                     self.is_train: True}\n        return feed_dict\n\n    @overrides\n    def _get_validation_feed_dict(self, batch):\n        inputs, targets = batch\n        feed_dict = {self.sentence_one_word: inputs[0],\n                     self.sentence_one_char: inputs[1],\n                     self.sentence_two_word: inputs[2],\n                     self.sentence_two_char: inputs[3],\n                     self.y_true: targets[0],\n                     self.is_train: False}\n        return feed_dict\n\n    @overrides\n    def _get_test_feed_dict(self, batch):\n        inputs, _ = batch\n        feed_dict = {self.sentence_one_word: inputs[0],\n                     self.sentence_one_char: inputs[1],\n                     self.sentence_two_word: inputs[2],\n                     self.sentence_two_char: inputs[3],\n                     self.is_train: False}\n        return feed_dict\n'"
duplicate_questions/models/bimpm/matching.py,59,"b'import tensorflow as tf\nfrom ...util.rnn import last_relevant_output\nEPSILON = 1e-6\n\n\ndef bilateral_matching(sentence_one_fw_representation, sentence_one_bw_representation,\n                       sentence_two_fw_representation, sentence_two_bw_representation,\n                       sentence_one_mask, sentence_two_mask,\n                       is_train, dropout_rate, multiperspective_dims=20,\n                       with_full_match=True, with_pool_match=True,\n                       with_attentive_match=True, with_max_attentive_match=True):\n    """"""\n    Given the representations of a sentence from a BiLSTM, apply four bilateral\n    matching functions between sentence_one and sentence_two in both directions\n    (sentence_one to sentence_two, and sentence_two to sentence_one).\n\n    Parameters\n    ----------\n    sentence_one_fw_representation: Tensor\n        Tensor of shape (batch_size, num_sentence_words, context_rnn_hidden size)\n        representing sentence_one as encoded by the forward layer of a BiLSTM.\n\n    sentence_one_bw_representation: Tensor\n        Tensor of shape (batch_size, num_sentence_words, context_rnn_hidden size)\n        representing sentence_one as encoded by the backward layer of a BiLSTM.\n\n    sentence_two_fw_representation: Tensor\n        Tensor of shape (batch_size, num_sentence_words, context_rnn_hidden size)\n        representing sentence_two as encoded by the forward layer of a BiLSTM.\n\n    sentence_two_bw_representation: Tensor\n        Tensor of shape (batch_size, num_sentence_words, context_rnn_hidden size)\n        representing sentence_two as encoded by the backward layer of a BiLSTM.\n\nn    sentence_one_mask: Tensor\n        Binary Tensor of shape (batch_size, num_sentence_words), indicating which\n        positions in sentence one are padding (0) and which are not (1).\n\n    sentence_two_mask: Tensor\n        Binary Tensor of shape (batch_size, num_sentence_words), indicating which\n        positions in sentence two are padding (0) and which are not (1).\n\n    is_train: Tensor\n        Boolean tensor indicating whether the model is performing training\n        or inference.\n\n    dropout_rate: float\n        The proportion of the Tensor to dropout after each layer.\n\n    multiperspective_dims: int, optional (default=20)\n        The ""number of perspectives"", referring to the dimensionality\n        of the output of the cosine matching function.\n\n    with_full_match: boolean, optional (default=True)\n        Whether or not to apply the full matching function.\n\n    with_pool_match: boolean, optional (default=True)\n        Whether or not to apply the pooling matching function.\n\n    with_attentive_match: boolean, optional (default=True)\n        Whether or not to apply the attentive matching function.\n\n    with_max_attentive_match: boolean, optional (default=True)\n        Whether or not to apply the max attentive matching function.\n    """"""\n    # Match each word of sentence one to the entirety of sentence two.\n    with tf.variable_scope(""match_one_to_two""):\n        match_one_to_two_output = match_sequences(\n            sentence_one_fw_representation,\n            sentence_one_bw_representation,\n            sentence_two_fw_representation,\n            sentence_two_bw_representation,\n            sentence_one_mask,\n            sentence_two_mask,\n            multiperspective_dims=multiperspective_dims,\n            with_full_match=with_full_match,\n            with_pool_match=with_pool_match,\n            with_attentive_match=with_attentive_match,\n            with_max_attentive_match=with_max_attentive_match)\n\n    # Match each word of sentence two to the entirety of sentence one.\n    with tf.variable_scope(""match_two_to_one""):\n        match_two_to_one_output = match_sequences(\n            sentence_two_fw_representation,\n            sentence_two_bw_representation,\n            sentence_one_fw_representation,\n            sentence_one_bw_representation,\n            sentence_two_mask,\n            sentence_one_mask,\n            multiperspective_dims=multiperspective_dims,\n            with_full_match=with_full_match,\n            with_pool_match=with_pool_match,\n            with_attentive_match=with_attentive_match,\n            with_max_attentive_match=with_max_attentive_match)\n\n    # Shapes: (batch_size, num_sentence_words, 13*multiperspective_dims)\n    match_one_to_two_representations = tf.concat(\n        match_one_to_two_output, 2)\n    match_two_to_one_representations = tf.concat(\n        match_two_to_one_output, 2)\n\n    # Apply dropout to the matched representations.\n    # Shapes: (batch_size, num_sentence_words, 13*multiperspective_dims)\n    match_one_to_two_representations = tf.layers.dropout(\n        match_one_to_two_representations,\n        rate=dropout_rate,\n        training=is_train,\n        name=""match_one_to_two_dropout"")\n    match_two_to_one_representations = tf.layers.dropout(\n        match_two_to_one_representations,\n        rate=dropout_rate,\n        training=is_train,\n        name=""match_two_to_one_dropout"")\n\n    # Shapes: (batch_size, num_sentence_words, 8*multiperspective_dims)\n    return match_one_to_two_representations, match_two_to_one_representations\n\n\ndef match_sequences(sentence_a_fw, sentence_a_bw, sentence_b_fw, sentence_b_bw,\n                    sentence_a_mask, sentence_b_mask, multiperspective_dims,\n                    with_full_match, with_pool_match, with_attentive_match,\n                    with_max_attentive_match):\n    """"""\n    Given the representations of a sentence from a BiLSTM, apply four bilateral\n    matching functions from sentence_a to sentence_b (so each time step of sentence_a is\n    matched with the the entirety of sentence_b).\n\n    Parameters\n    ----------\n    sentence_a_fw: Tensor\n        Tensor of shape (batch_size, num_sentence_words, context_rnn_hidden size)\n        representing sentence_one as encoded by the forward layer of a BiLSTM.\n\n    sentence_a_bw: Tensor\n        Tensor of shape (batch_size, num_sentence_words, context_rnn_hidden size)\n        representing sentence_one as encoded by the backward layer of a BiLSTM.\n\n    sentence_b_fw: Tensor\n        Tensor of shape (batch_size, num_sentence_words, context_rnn_hidden size)\n        representing sentence_two as encoded by the forward layer of a BiLSTM.\n\n    sentence_b_bw: Tensor\n        Tensor of shape (batch_size, num_sentence_words, context_rnn_hidden size)\n        representing sentence_two as encoded by the backward layer of a BiLSTM.\n\n    sentence_a_mask: Tensor\n        Binary Tensor of shape (batch_size, num_sentence_words), indicating which\n        positions in a sentence are padding (0) and which are not (1).\n\n    sentence_b_mask: Tensor\n        Binary Tensor of shape (batch_size, num_sentence_words), indicating which\n        positions in a sentence are padding (0) and which are not (1).\n\n    multiperspective_dims: int\n        The ""number of perspectives"", referring to the dimensionality\n        of the output of the cosine matching function.\n\n    with_full_match: boolean\n        Whether or not to apply the full matching function.\n\n    with_pool_match: boolean\n        Whether or not to apply the pooling matching function.\n\n    with_attentive_match: boolean\n        Whether or not to apply the attentive matching function.\n\n    with_max_attentive_match: boolean\n        Whether or not to apply the max attentive matching function.\n    """"""\n    matched_representations = []\n\n    # The unpadded lengths of sentence_b\n    # Shape: (batch_size,)\n    sentence_b_len = tf.reduce_sum(sentence_b_mask, 1)\n\n    # The context rnn hidden size.\n    sentence_encoding_dim = sentence_a_fw.get_shape().as_list()[2]\n\n    # Calculate the cosine similarity matrices for\n    # fw and bw representations, used in the attention-based matching\n    # functions.\n    # Shapes: (batch_size, num_sentence_words, num_sentence_words)\n    fw_similarity_matrix = calculate_cosine_similarity_matrix(sentence_b_fw,\n                                                              sentence_a_fw)\n    fw_similarity_matrix = mask_similarity_matrix(fw_similarity_matrix,\n                                                  sentence_b_mask,\n                                                  sentence_a_mask)\n    bw_similarity_matrix = calculate_cosine_similarity_matrix(sentence_b_bw,\n                                                              sentence_a_bw)\n    bw_similarity_matrix = mask_similarity_matrix(bw_similarity_matrix,\n                                                  sentence_b_mask,\n                                                  sentence_a_mask)\n    # Apply the multiperspective matching functions.\n    if multiperspective_dims > 0:\n        # Apply forward and backward full matching\n        if with_full_match:\n            # Forward full matching: each timestep of sentence_a_fw vs last\n            # output of sentence_b_fw.\n            with tf.variable_scope(""forward_full_matching""):\n                # Shape: (batch_size, rnn_hidden_size)\n                last_output_sentence_b_fw = last_relevant_output(\n                    sentence_b_fw, sentence_b_len)\n                # The weights for the matching function.\n                fw_full_match_params = tf.get_variable(\n                    ""forward_full_matching_params"",\n                    shape=[multiperspective_dims, sentence_encoding_dim],\n                    dtype=""float"")\n                # Shape: (batch_size, num_passage_words, multiperspective_dims)\n                fw_full_match_output = full_matching(\n                    sentence_a_fw,\n                    last_output_sentence_b_fw,\n                    fw_full_match_params)\n            matched_representations.append(fw_full_match_output)\n            # Backward full matching: each timestep of sentence_a_bw vs last\n            # output of sentence_b_bw.\n            with tf.variable_scope(""backward_full_matching""):\n                # Shape: (batch_size, rnn_hidden_size)\n                last_output_sentence_b_bw = last_relevant_output(\n                    sentence_b_bw, sentence_b_len)\n                # The weights for the matching function.\n                bw_full_match_params = tf.get_variable(\n                    ""backward_full_matching_params"",\n                    shape=[multiperspective_dims, sentence_encoding_dim],\n                    dtype=""float"")\n                # Shape: (batch_size, num_passage_words, multiperspective_dims)\n                bw_full_match_output = full_matching(\n                    sentence_a_bw,\n                    last_output_sentence_b_bw,\n                    bw_full_match_params)\n            matched_representations.append(bw_full_match_output)\n\n        # Apply forward and backward pool matching.\n        if with_pool_match:\n            # Forward Pooling-Matching: each timestep of sentence_a_fw vs.\n            # each element of sentence_b_fw, then taking the elementwise mean.\n            with tf.variable_scope(""forward_pooling_matching""):\n                # The weights for the matching function.\n                fw_pooling_params = tf.get_variable(\n                    ""forward_pooling_matching_params"",\n                    shape=[multiperspective_dims, sentence_encoding_dim],\n                    dtype=""float"")\n                # Shape: (batch_size, num_sentence_words, multiperspective_dims)\n                fw_pooling_match_output = pooling_matching(\n                    sentence_a_fw,\n                    sentence_b_fw,\n                    fw_pooling_params)\n                matched_representations.append(fw_pooling_match_output)\n            # Backward Pooling-Matching: each timestep of sentence_a_bw vs.\n            # each element of sentence_b_bw, then taking the elementwise mean.\n            with tf.variable_scope(""backward_pooling_matching""):\n                # The weights for the matching function\n                bw_pooling_params = tf.get_variable(\n                    ""backward_pooling_matching_params"",\n                    shape=[multiperspective_dims, sentence_encoding_dim],\n                    dtype=""float"")\n                # Shape: (batch_size, num_sentence_words, multiperspective_dims)\n                bw_pooling_match_output = pooling_matching(\n                    sentence_a_bw,\n                    sentence_b_bw,\n                    bw_pooling_params)\n                matched_representations.append(bw_pooling_match_output)\n\n        # Apply forward and backward attentive matching.\n        # Using the cosine distances between the sentence\n        # representations from the LSTM, we use a weighted\n        # sum across the entire sentence to generate an attention vector.\n        if with_attentive_match:\n            # Forward Attentive Matching\n            with tf.variable_scope(""forward_attentive_matching""):\n                # Shape: (batch_size, num_sentence_words, rnn_hidden_dim)\n                sentence_b_fw_att = weight_sentence_by_similarity(\n                    sentence_b_fw,\n                    fw_similarity_matrix)\n                # The weights for the matching function.\n                fw_attentive_params = tf.get_variable(\n                    ""forward_attentive_matching_params"",\n                    shape=[multiperspective_dims, sentence_encoding_dim],\n                    dtype=""float"")\n                # Shape: (batch_size, num_sentence_words, multiperspective_dim)\n                fw_attentive_matching_output = attentive_matching(\n                    sentence_a_fw,\n                    sentence_b_fw_att,\n                    fw_attentive_params)\n                matched_representations.append(fw_attentive_matching_output)\n            # Backward Attentive Matching\n            with tf.variable_scope(""backward_attentive_matching""):\n                sentence_b_bw_att = weight_sentence_by_similarity(\n                    sentence_b_bw,\n                    bw_similarity_matrix)\n                bw_attentive_params = tf.get_variable(\n                    ""backward_attentive_matching_params"",\n                    shape=[multiperspective_dims, sentence_encoding_dim],\n                    dtype=""float"")\n                # Shape: (batch_size, num_sentence_words, multiperspective_dim)\n                bw_attentive_matching_output = attentive_matching(\n                    sentence_a_bw,\n                    sentence_b_bw_att,\n                    bw_attentive_params)\n                matched_representations.append(bw_attentive_matching_output)\n\n        # Apply forward and backward max attentive matching.\n        # Use the time step of the sentence_b with the highest cosine similarity\n        # to cosine b as an attention vector.\n        if with_max_attentive_match:\n            # Forward max attentive-matching\n            with tf.variable_scope(""forward_attentive_matching""):\n                # Shape: (batch_size, num_sentence_words, rnn_hidden_dim)\n                sentence_b_fw_max_att = max_sentence_similarity(\n                    sentence_b_fw,\n                    fw_similarity_matrix)\n                # The weights for the matching function.\n                fw_max_attentive_params = tf.get_variable(\n                    ""fw_max_attentive_params"",\n                    shape=[multiperspective_dims,\n                           sentence_encoding_dim],\n                    dtype=""float"")\n                # Shape: (batch_size, num_sentence_words, multiperspective_dim)\n                fw_max_attentive_matching_output = attentive_matching(\n                    sentence_a_fw,\n                    sentence_b_fw_max_att,\n                    fw_max_attentive_params)\n                matched_representations.append(fw_max_attentive_matching_output)\n            # Backward max attentive-matching\n            with tf.variable_scope(""backward_attentive_matching""):\n                # Shape: (batch_size, num_sentence_words, rnn_hidden_dim)\n                sentence_b_bw_max_att = max_sentence_similarity(\n                    sentence_b_bw,\n                    bw_similarity_matrix)\n                # The weights for the matching function.\n                bw_max_attentive_params = tf.get_variable(\n                    ""bw_max_attentive_params"",\n                    shape=[multiperspective_dims,\n                           sentence_encoding_dim],\n                    dtype=""float"")\n                # Shape: (batch_size, num_sentence_words, multiperspective_dim)\n                bw_max_attentive_matching_output = attentive_matching(\n                    sentence_a_bw,\n                    sentence_b_bw_max_att,\n                    bw_max_attentive_params)\n                matched_representations.append(bw_max_attentive_matching_output)\n\n    return matched_representations\n\n\ndef calculate_cosine_similarity_matrix(v1, v2):\n    """"""\n    Calculate the cosine similarity matrix between two\n    sentences.\n\n    Parameters\n    ----------\n    v1: Tensor\n        Tensor of shape (batch_size, num_sentence_words,\n        context_rnn_hidden_size), representing the output of running\n        a sentence through a BiLSTM.\n\n    v2: Tensor\n        Tensor of shape (batch_size, num_sentence_words,\n        context_rnn_hidden_size), representing the output of running\n        another sentences through a BiLSTM.\n    """"""\n    # Shape: (batch_size, 1, num_sentence_words, rnn_hidden_size)\n    expanded_v1 = tf.expand_dims(v1, 1)\n    # Shape: (batch_size, num_sentence_words, 1, rnn_hidden_size)\n    expanded_v2 = tf.expand_dims(v2, 2)\n    # Shape: (batch_size, num_sentence_words, num_sentence_words)\n    cosine_relevancy_matrix = cosine_distance(expanded_v1,\n                                              expanded_v2)\n    return cosine_relevancy_matrix\n\n\ndef cosine_distance(v1, v2):\n    """"""\n    Calculate the cosine distance between the representations of the\n    words of the two sentences.\n\n    Parameters\n    ----------\n    v1: Tensor\n        Tensor of shape (batch_size, 1, num_sentence_words, context_rnn_hidden_size)\n        representing the first sentence to take the cosine similarity with.\n\n    v2: Tensor\n        Tensor of shape (batch_size, num_sentence_words, 1, context_rnn_hidden_size)\n        representing the second sentence to take the cosine similarity with.\n    """"""\n    # The product of the two vectors is shape\n    # (batch_size, num_sentence_words, num_sentence_words, rnn_hidden_size)\n    # Taking the sum over the last axis reesults in shape:\n    # (batch_size, num_sentence_words, num_sentence_words)\n    cosine_numerator = tf.reduce_sum(tf.multiply(v1, v2), axis=-1)\n    # Shape: (batch_size, 1, num_sentence_words)\n    v1_norm = tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(v1), axis=-1),\n                                 EPSILON))\n    # Shape: (batch_size, num_sentence_words, 1)\n    v2_norm = tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(v2), axis=-1),\n                                 EPSILON))\n    # Shape: (batch_size, num_sentence_words, num_sentence_words)\n    return cosine_numerator / v1_norm / v2_norm\n\n\ndef mask_similarity_matrix(similarity_matrix, mask_a, mask_b):\n    """"""\n    Given the mask of the two sentences, apply the mask to the similarity\n    matrix.\n\n    Parameters\n    ----------\n    similarity_matrix: Tensor\n        Tensor of shape (batch_size, num_sentence_words, num_sentence_words).\n\n    mask_a: Tensor\n        Tensor of shape (batch_size, num_sentence_words). This mask should\n        correspond to the first vector (v1) used to calculate the similarity\n        matrix.\n\n    mask_b: Tensor\n        Tensor of shape (batch_size, num_sentence_words). This mask should\n        correspond to the second vector (v2) used to calculate the similarity\n        matrix.\n    """"""\n    similarity_matrix = tf.multiply(similarity_matrix,\n                                    tf.expand_dims(tf.cast(mask_a, ""float""), 1))\n    similarity_matrix = tf.multiply(similarity_matrix,\n                                    tf.expand_dims(tf.cast(mask_b, ""float""), 2))\n    return similarity_matrix\n\n\ndef max_sentence_similarity(sentence_input, similarity_matrix):\n    """"""\n    Parameters\n    ----------\n    sentence_input: Tensor\n        Tensor of shape (batch_size, num_sentence_words, rnn_hidden_dim).\n\n    similarity_matrix: Tensor\n        Tensor of shape (batch_size, num_sentence_words, num_sentence_words).\n    """"""\n    # Shape: (batch_size, passage_len)\n    def single_instance(inputs):\n        single_sentence = inputs[0]\n        argmax_index = inputs[1]\n        # Shape: (num_sentence_words, rnn_hidden_dim)\n        return tf.gather(single_sentence, argmax_index)\n\n    question_index = tf.arg_max(similarity_matrix, 2)\n    elems = (sentence_input, question_index)\n    # Shape: (batch_size, num_sentence_words, rnn_hidden_dim)\n    return tf.map_fn(single_instance, elems, dtype=""float"")\n\n\ndef full_matching(sentence_a_representation,\n                  sentence_b_last_output,\n                  weights):\n    """"""\n    Match each time step of sentence_a with the last output of sentence_b\n    by passing them both through the multiperspective matching function.\n\n    Parameters\n    ----------\n    sentence_a_representation: Tensor\n        Tensor of shape (batch_size, num_sentence_words, rnn_hidden_dim)\n\n    sentence_b_last_output: Tensor\n        Tensor of shape (batch_size, rnn_hidden_dim)\n\n    weights: Tensor\n        Tensor of shape (multiperspective_dims, rnn_hidden_dim)\n\n    Returns\n    -------\n    full_match_output: Tensor\n        Tensor of shape (batch_size, num_passage_words, multiperspective_dims).\n    """"""\n    def single_instance(inputs):\n        # Shape: (num_passage_words, rnn_hidden_dim)\n        sentence_a_representation_single = inputs[0]\n        # Shape: (rnn_hidden_dim)\n        sentence_b_last_output_single = inputs[1]\n        # Shape: (num_sentence_words, multiperspective_dims, rnn_hidden_dim)\n        sentence_a_single_expanded = multi_perspective_expand_for_2D(\n            sentence_a_representation_single,\n            weights)\n        # Shape: (multiperspective_dims, rnn_hidden_dim)\n        sentence_b_last_output_expanded = multi_perspective_expand_for_1D(\n            sentence_b_last_output_single,\n            weights)\n\n        # Shape: (1, multiperspective_dims, rnn_hidden_dim)\n        sentence_b_last_output_expanded = tf.expand_dims(\n            sentence_b_last_output_expanded, 0)\n        # Shape: (num_passage_words, multiperspective_dims)\n        return cosine_distance(sentence_a_single_expanded,\n                               sentence_b_last_output_expanded)\n\n    elems = (sentence_a_representation, sentence_b_last_output)\n    # Shape: (batch_size, num_passage_words, multiperspective_dims)\n    return tf.map_fn(single_instance, elems, dtype=""float"")\n\n\ndef pooling_matching(sentence_a_representation,\n                     sentence_b_representation, weights):\n    """"""\n    Parameters\n    ----------\n    sentence_a_representation: Tensor\n        Tensor of shape (batch_size, num_sentence_words, rnn_hidden_dim)\n\n    sentence_b_representation: Tensor\n        Tensor of shape (batch_size, num_sentence_words, rnn_hidden_dim)\n\n    weights: Tensor\n        Tensor of shape (multiperspective_dims, rnn_hidden_dim)\n    """"""\n    def single_instance(inputs):\n        # Shape: (passage_len, rnn_hidden_dim)\n        sentence_a_representation_single = inputs[0]\n        # Shape: (passage_len, rnn_hidden_dim)\n        sentence_b_representation_single = inputs[1]\n        # Shape: (num_sentence_words, multiperspective_dims, rnn_hidden_dim)\n        sentence_a_expanded = multi_perspective_expand_for_2D(\n            sentence_a_representation_single, weights)\n        # Shape: (num_sentence_words, multiperspective_dims, rnn_hidden_dim)\n        sentence_b_expanded = multi_perspective_expand_for_2D(\n            sentence_b_representation_single, weights)\n        # Shape: (num_sentence_words, 1, multiperspective_dims,\n        #         rnn_hidden_dim)\n        sentence_a_expanded = tf.expand_dims(sentence_a_expanded, 1)\n\n        # Shape: (1, num_sentence_words, multiperspective_dims,\n        #         rnn_hidden_dim)\n        sentence_b_expanded = tf.expand_dims(sentence_b_expanded, 0)\n        # Shape: (num_sentence_words, multiperspective_dims)\n        return cosine_distance(sentence_a_expanded,\n                               sentence_b_expanded)\n\n    elems = (sentence_a_representation, sentence_b_representation)\n    # Shape: (batch_size, num_sentence_words, num_sentence_words,\n    #         multiperspective_dims)\n    matching_matrix = tf.map_fn(single_instance, elems, dtype=""float"")\n    # Take the max and mean pool of the matching matrix.\n    # Shape: (batch_size, num_sentence_words, multiperspective_dims)\n    return tf.reduce_mean(matching_matrix, axis=2)\n\n\ndef attentive_matching(input_sentence, att_matrix, weights):\n    """"""\n    Parameters\n    ----------\n    input_sentence: Tensor\n        Tensor of shape (batch_size, num_sentence_words, rnn_hidden_dim)\n\n    att_matrix: Tensor\n        Tensor of shape (batch_size, num_sentence_words, rnn_hidden_dim)\n    """"""\n    def single_instance(inputs):\n        # Shapes: (num_sentence_words, rnn_hidden_dim)\n        sentence_a_single = inputs[0]\n        sentence_b_single_att = inputs[1]\n\n        # Shapes: (num_sentence_words, multiperspective_dims, rnn_hidden_dim)\n        expanded_sentence_a_single = multi_perspective_expand_for_2D(\n            sentence_a_single, weights)\n        expanded_sentence_b_single_att = multi_perspective_expand_for_2D(\n            sentence_b_single_att, weights)\n        # Shape: (num_sentence_words, multiperspective_dims)\n        return cosine_distance(expanded_sentence_a_single,\n                               expanded_sentence_b_single_att)\n\n    elems = (input_sentence, att_matrix)\n    # Shape: (batch_size, num_sentence_words, multiperspective_dims)\n    return tf.map_fn(single_instance, elems, dtype=""float"")\n\n\ndef weight_sentence_by_similarity(input_sentence, cosine_matrix,\n                                  normalize=False):\n    """"""\n    Parameters\n    ----------\n    input_sentence: Tensor\n        Tensor of shape (batch_size, num_sentence_words, rnn_hidden_dim)\n\n    cosine_matrix: Tensor\n        Tensor of shape (batch_size, num_sentence_words, num_sentence_words)\n    """"""\n    if normalize:\n        cosine_matrix = tf.nn.softmax(cosine_matrix)\n    # Shape: (batch_size, num_sentence_words, num_sentence_words, 1)\n    expanded_cosine_matrix = tf.expand_dims(cosine_matrix, axis=-1)\n    # Shape: (batch_size, 1, num_sentence_words, rnn_hidden_dim)\n    weighted_question_words = tf.expand_dims(input_sentence, axis=1)\n    # Shape: (batch_size, num_sentence_words, rnn_hidden_dim)\n    weighted_question_words = tf.reduce_sum(\n        tf.multiply(weighted_question_words, expanded_cosine_matrix), axis=2)\n    if not normalize:\n        weighted_question_words = tf.div(\n            weighted_question_words,\n            tf.expand_dims(\n                tf.add(tf.reduce_sum(cosine_matrix, axis=-1),\n                       EPSILON),\n                axis=-1))\n    # Shape: (batch_size, num_sentence_words, rnn_hidden_dim)\n    return weighted_question_words\n\n\ndef multi_perspective_expand_for_3D(in_tensor, weights):\n    # Shape: (batch_size, num_passage_words, 1, rnn_hidden_dim)\n    in_tensor_expanded = tf.expand_dims(in_tensor, axis=2)\n    # Shape: (1, 1, multiperspective_dims, rnn_hidden_dim)\n    weights_expanded = tf.expand_dims(\n        tf.expand_dims(weights, axis=0),\n        axis=0)\n    # Shape: (batch_size, num_passage_words, multiperspective_dims,\n    #         rnn_hidden_dim)\n    return tf.multiply(in_tensor_expanded, weights_expanded)\n\n\ndef multi_perspective_expand_for_2D(in_tensor, weights):\n    """"""\n    Given a 2D input tensor and weights of the appropriate shape,\n    weight the input tensor by the weights by multiplying them\n    together.\n\n    Parameters\n    ----------\n    in_tensor:\n        Tensor of shape (x_1, x_2) to be weighted. In this case,\n        x_1 might represent num_passage_words and x_2 might be\n        the rnn_hidden_dim.\n\n    weights:\n        Tensor of shape (y, x) to multiply the input tensor by. In this\n        case, y is the number of perspectives and x is the rnn_hidden_dim.\n\n    Returns\n    -------\n    weighted_input:\n        Tensor of shape (y, x), representing the weighted input\n        across multiple perspectives.\n    """"""\n    # Shape: (num_sentence_words, 1, rnn_hidden_dim)\n    in_tensor_expanded = tf.expand_dims(in_tensor, axis=1)\n    # Shape: (1, multiperspective_dims, rnn_hidden_dim)\n    weights_expanded = tf.expand_dims(weights, axis=0)\n    # Shape: (num_sentence_words, multiperspective_dims, rnn_hidden_dim)\n    return tf.multiply(in_tensor_expanded, weights_expanded)\n\n\ndef multi_perspective_expand_for_1D(in_tensor, weights):\n    """"""\n    Given a 1D input tensor and weights of the appropriate shape,\n    weight the input tensor by the weights by multiplying them\n    together.\n\n    Parameters\n    ----------\n    in_tensor:\n        Tensor of shape (x,) to be weighted.\n\n    weights:\n        Tensor of shape (y, x) to multiply the input tensor by. In this\n        case, y is the number of perspectives.\n\n    Returns\n    -------\n    weighted_input:\n        Tensor of shape (y, x), representing the weighted input\n        across multiple perspectives.\n    """"""\n    # Shape: (1, rnn_hidden_dim)\n    in_tensor_expanded = tf.expand_dims(in_tensor, axis=0)\n    # Shape: (multiperspective_dims, rnn_hidden_dim)\n    return tf.multiply(in_tensor_expanded, weights)\n'"
duplicate_questions/models/siamese_bilstm/__init__.py,0,b''
duplicate_questions/models/siamese_bilstm/siamese_bilstm.py,46,"b'from copy import deepcopy\nimport logging\nfrom overrides import overrides\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import LSTMCell\n\nfrom ..base_tf_model import BaseTFModel\nfrom ...util.switchable_dropout_wrapper import SwitchableDropoutWrapper\nfrom ...util.pooling import mean_pool\nfrom ...util.rnn import last_relevant_output\n\nlogger = logging.getLogger(__name__)\n\n\nclass SiameseBiLSTM(BaseTFModel):\n    """"""\n    Create a model based off of ""Siamese Recurrent Architectures for Learning\n    Sentence Similarity"" at AAAI \'16. The model is super simple: just encode\n    both sentences with a LSTM, and then use the function\n    exp(-||sentence_one - sentence_two||) to get a probability that the\n    two sentences are semantically identical.\n\n    Parameters\n    ----------\n    mode: str\n        One of [train|predict], to indicate what you want the model to do.\n        If you pick ""predict"", then you must also supply the path to a\n        pretrained model and DataIndexer to load to the ``predict`` method.\n\n    word_vocab_size: int\n        The number of unique tokens in the dataset, plus the UNK and padding\n        tokens. Alternatively, the highest index assigned to any word, +1.\n        This is used by the model to figure out the dimensionality of the\n        embedding matrix.\n\n    word_embedding_dim: int\n        The length of a word embedding. This is used by\n        the model to figure out the dimensionality of the embedding matrix.\n\n    word_embedding_matrix: numpy array, optional if predicting\n        A numpy array of shape (word_vocab_size, word_emb_dim).\n        word_embedding_matrix[index] should represent the word vector for\n        that particular word index. This is used to initialize the\n        word embedding matrix in the model, and is optional if predicting\n        since we assume that the word embeddings variable will be loaded\n        with the model.\n\n    fine_tune_embeddings: boolean\n        If true, sets the embeddings to be trainable.\n\n    rnn_hidden_size: int\n        The output dimension of the RNN encoder. Note that this model uses a\n        bidirectional LSTM, so the actual sentence vectors will be\n        of length 2*rnn_hidden_size.\n\n    share_encoder_weights: boolean\n        Whether to use the same encoder on both input sentnces (thus\n        sharing weights), or a different one for each sentence.\n\n    rnn_output_mode: str\n        How to calculate the final sentence representation from the RNN\n        outputs. mean pool"" indicates that the outputs will be averaged (with\n        respect to padding), and ""last"" indicates that the last\n        relevant output will be used as the sentence representation.\n\n    output_keep_prob: float\n        The probability of keeping an RNN outputs to keep, as opposed\n        to dropping it out.\n    """"""\n\n    @overrides\n    def __init__(self, config_dict):\n        config_dict = deepcopy(config_dict)\n        mode = config_dict.pop(""mode"")\n        super(SiameseBiLSTM, self).__init__(mode=mode)\n\n        self.word_vocab_size = config_dict.pop(""word_vocab_size"")\n        self.word_embedding_dim = config_dict.pop(""word_embedding_dim"")\n        self.word_embedding_matrix = config_dict.pop(""word_embedding_matrix"", None)\n        self.fine_tune_embeddings = config_dict.pop(""fine_tune_embeddings"")\n        self.rnn_hidden_size = config_dict.pop(""rnn_hidden_size"")\n        self.share_encoder_weights = config_dict.pop(""share_encoder_weights"")\n        self.rnn_output_mode = config_dict.pop(""rnn_output_mode"")\n        self.output_keep_prob = config_dict.pop(""output_keep_prob"")\n\n        if config_dict:\n            logger.warning(""UNUSED VALUES IN CONFIG DICT: {}"".format(config_dict))\n\n    @overrides\n    def _create_placeholders(self):\n        """"""\n        Create the placeholders for use in the model.\n        """"""\n        # Define the inputs here\n        # Shape: (batch_size, num_sentence_words)\n        # The first input sentence.\n        self.sentence_one = tf.placeholder(""int32"",\n                                           [None, None],\n                                           name=""sentence_one"")\n\n        # Shape: (batch_size, num_sentence_words)\n        # The second input sentence.\n        self.sentence_two = tf.placeholder(""int32"",\n                                           [None, None],\n                                           name=""sentence_two"")\n\n        # Shape: (batch_size, 2)\n        # The true labels, encoded as a one-hot vector. So\n        # [1, 0] indicates not duplicate, [0, 1] indicates duplicate.\n        self.y_true = tf.placeholder(""int32"",\n                                     [None, 2],\n                                     name=""true_labels"")\n\n        # A boolean that encodes whether we are training or evaluating\n        self.is_train = tf.placeholder(\'bool\', [], name=\'is_train\')\n\n    @overrides\n    def _build_forward(self):\n        """"""\n        Using the values in the config passed to the SiameseBiLSTM object\n        on creation, build the forward pass of the computation graph.\n        """"""\n        # A mask over the word indices in the sentence, indicating\n        # which indices are padding and which are words.\n        # Shape: (batch_size, num_sentence_words)\n        sentence_one_mask = tf.sign(self.sentence_one,\n                                    name=""sentence_one_masking"")\n        sentence_two_mask = tf.sign(self.sentence_two,\n                                    name=""sentence_two_masking"")\n\n        # The unpadded lengths of sentence one and sentence two\n        # Shape: (batch_size,)\n        sentence_one_len = tf.reduce_sum(sentence_one_mask, 1)\n        sentence_two_len = tf.reduce_sum(sentence_two_mask, 1)\n\n        word_vocab_size = self.word_vocab_size\n        word_embedding_dim = self.word_embedding_dim\n        word_embedding_matrix = self.word_embedding_matrix\n        fine_tune_embeddings = self.fine_tune_embeddings\n\n        with tf.variable_scope(""embeddings""):\n            with tf.variable_scope(""embedding_var""), tf.device(""/cpu:0""):\n                if self.mode == ""train"":\n                    # Load the word embedding matrix that was passed in\n                    # since we are training\n                    word_emb_mat = tf.get_variable(\n                        ""word_emb_mat"",\n                        dtype=""float"",\n                        shape=[word_vocab_size,\n                               word_embedding_dim],\n                        initializer=tf.constant_initializer(\n                            word_embedding_matrix),\n                        trainable=fine_tune_embeddings)\n                else:\n                    # We are not training, so a model should have been\n                    # loaded with the embedding matrix already there.\n                    word_emb_mat = tf.get_variable(""word_emb_mat"",\n                                                   shape=[word_vocab_size,\n                                                          word_embedding_dim],\n                                                   dtype=""float"",\n                                                   trainable=fine_tune_embeddings)\n\n            with tf.variable_scope(""word_embeddings""):\n                # Shape: (batch_size, num_sentence_words, embedding_dim)\n                word_embedded_sentence_one = tf.nn.embedding_lookup(\n                    word_emb_mat,\n                    self.sentence_one)\n                # Shape: (batch_size, num_sentence_words, embedding_dim)\n                word_embedded_sentence_two = tf.nn.embedding_lookup(\n                    word_emb_mat,\n                    self.sentence_two)\n\n        rnn_hidden_size = self.rnn_hidden_size\n        rnn_output_mode = self.rnn_output_mode\n        output_keep_prob = self.output_keep_prob\n        rnn_cell_fw_one = LSTMCell(rnn_hidden_size, state_is_tuple=True)\n        d_rnn_cell_fw_one = SwitchableDropoutWrapper(rnn_cell_fw_one,\n                                                     self.is_train,\n                                                     output_keep_prob=output_keep_prob)\n        rnn_cell_bw_one = LSTMCell(rnn_hidden_size, state_is_tuple=True)\n        d_rnn_cell_bw_one = SwitchableDropoutWrapper(rnn_cell_bw_one,\n                                                     self.is_train,\n                                                     output_keep_prob=output_keep_prob)\n        with tf.variable_scope(""encode_sentences""):\n            # Encode the first sentence.\n            (fw_output_one, bw_output_one), _ = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=d_rnn_cell_fw_one,\n                cell_bw=d_rnn_cell_bw_one,\n                dtype=""float"",\n                sequence_length=sentence_one_len,\n                inputs=word_embedded_sentence_one,\n                scope=""encoded_sentence_one"")\n            if self.share_encoder_weights:\n                # Encode the second sentence, using the same RNN weights.\n                tf.get_variable_scope().reuse_variables()\n                (fw_output_two, bw_output_two), _ = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw=d_rnn_cell_fw_one,\n                    cell_bw=d_rnn_cell_bw_one,\n                    dtype=""float"",\n                    sequence_length=sentence_two_len,\n                    inputs=word_embedded_sentence_two,\n                    scope=""encoded_sentence_one"")\n            else:\n                # Encode the second sentence with a different RNN\n                rnn_cell_fw_two = LSTMCell(rnn_hidden_size, state_is_tuple=True)\n                d_rnn_cell_fw_two = SwitchableDropoutWrapper(\n                    rnn_cell_fw_two,\n                    self.is_train,\n                    output_keep_prob=output_keep_prob)\n                rnn_cell_bw_two = LSTMCell(rnn_hidden_size, state_is_tuple=True)\n                d_rnn_cell_bw_two = SwitchableDropoutWrapper(\n                    rnn_cell_bw_two,\n                    self.is_train,\n                    output_keep_prob=output_keep_prob)\n                (fw_output_two, bw_output_two), _ = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw=d_rnn_cell_fw_two,\n                    cell_bw=d_rnn_cell_bw_two,\n                    dtype=""float"",\n                    sequence_length=sentence_two_len,\n                    inputs=word_embedded_sentence_two,\n                    scope=""encoded_sentence_two"")\n\n            # Now, combine the fw_output and bw_output for the\n            # first and second sentence LSTM outputs\n            if rnn_output_mode == ""mean_pool"":\n                # Mean pool the forward and backward RNN outputs\n                pooled_fw_output_one = mean_pool(fw_output_one,\n                                                 sentence_one_len)\n                pooled_bw_output_one = mean_pool(bw_output_one,\n                                                 sentence_one_len)\n                pooled_fw_output_two = mean_pool(fw_output_two,\n                                                 sentence_two_len)\n                pooled_bw_output_two = mean_pool(bw_output_two,\n                                                 sentence_two_len)\n                # Shape: (batch_size, 2*rnn_hidden_size)\n                encoded_sentence_one = tf.concat([pooled_fw_output_one,\n                                                  pooled_bw_output_one], 1)\n                encoded_sentence_two = tf.concat([pooled_fw_output_two,\n                                                  pooled_bw_output_two], 1)\n            elif rnn_output_mode == ""last"":\n                # Get the last unmasked output from the RNN\n                last_fw_output_one = last_relevant_output(fw_output_one,\n                                                          sentence_one_len)\n                last_bw_output_one = last_relevant_output(bw_output_one,\n                                                          sentence_one_len)\n                last_fw_output_two = last_relevant_output(fw_output_two,\n                                                          sentence_two_len)\n                last_bw_output_two = last_relevant_output(bw_output_two,\n                                                          sentence_two_len)\n                # Shape: (batch_size, 2*rnn_hidden_size)\n                encoded_sentence_one = tf.concat([last_fw_output_one,\n                                                  last_bw_output_one], 1)\n                encoded_sentence_two = tf.concat([last_fw_output_two,\n                                                  last_bw_output_two], 1)\n            else:\n                raise ValueError(""Got an unexpected value {} for ""\n                                 ""rnn_output_mode, expected one of ""\n                                 ""[mean_pool, last]"")\n\n        with tf.name_scope(""loss""):\n            # Use the exponential of the negative L1 distance\n            # between the two encoded sentences to get an output\n            # distribution over labels.\n            # Shape: (batch_size, 2)\n            self.y_pred = self._l1_similarity(encoded_sentence_one,\n                                              encoded_sentence_two)\n            # Manually calculating cross-entropy, since we output\n            # probabilities and can\'t use softmax_cross_entropy_with_logits\n            # Add epsilon to the probabilities in order to prevent log(0)\n            self.loss = tf.reduce_mean(\n                -tf.reduce_sum(tf.cast(self.y_true, ""float"") *\n                               tf.log(self.y_pred),\n                               axis=1))\n\n        with tf.name_scope(""accuracy""):\n            # Get the correct predictions.\n            # Shape: (batch_size,) of bool\n            correct_predictions = tf.equal(\n                tf.argmax(self.y_pred, 1),\n                tf.argmax(self.y_true, 1))\n\n            # Cast to float, and take the mean to get accuracy\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions,\n                                                   ""float""))\n\n        with tf.name_scope(""train""):\n            optimizer = tf.train.AdamOptimizer()\n            self.training_op = optimizer.minimize(self.loss,\n                                                  global_step=self.global_step)\n\n        with tf.name_scope(""train_summaries""):\n            # Add the loss and the accuracy to the tensorboard summary\n            tf.summary.scalar(""loss"", self.loss)\n            tf.summary.scalar(""accuracy"", self.accuracy)\n            self.summary_op = tf.summary.merge_all()\n\n    @overrides\n    def _get_train_feed_dict(self, batch):\n        inputs, targets = batch\n        feed_dict = {self.sentence_one: inputs[0],\n                     self.sentence_two: inputs[1],\n                     self.y_true: targets[0],\n                     self.is_train: True}\n        return feed_dict\n\n    @overrides\n    def _get_validation_feed_dict(self, batch):\n        inputs, targets = batch\n        feed_dict = {self.sentence_one: inputs[0],\n                     self.sentence_two: inputs[1],\n                     self.y_true: targets[0],\n                     self.is_train: False}\n        return feed_dict\n\n    @overrides\n    def _get_test_feed_dict(self, batch):\n        inputs, _ = batch\n        feed_dict = {self.sentence_one: inputs[0],\n                     self.sentence_two: inputs[1],\n                     self.is_train: False}\n        return feed_dict\n\n    def _l1_similarity(self, sentence_one, sentence_two):\n        """"""\n        Given a pair of encoded sentences (vectors), return a probability\n        distribution on whether they are duplicates are not with:\n        exp(-||sentence_one - sentence_two||)\n\n        Parameters\n        ----------\n        sentence_one: Tensor\n            A tensor of shape (batch_size, 2*rnn_hidden_size) representing\n            the encoded sentence_ones to use in the probability calculation.\n\n        sentence_one: Tensor\n            A tensor of shape (batch_size, 2*rnn_hidden_size) representing\n            the encoded sentence_twos to use in the probability calculation.\n\n        Returns\n        -------\n        class_probabilities: Tensor\n            A tensor of shape (batch_size, 2), represnting the probability\n            that a pair of sentences are duplicates as\n            [is_not_duplicate, is_duplicate].\n        """"""\n        with tf.name_scope(""l1_similarity""):\n            # Take the L1 norm of the two vectors.\n            # Shape: (batch_size, 2*rnn_hidden_size)\n            l1_distance = tf.abs(sentence_one - sentence_two)\n\n            # Take the sum for each sentence pair\n            # Shape: (batch_size, 1)\n            summed_l1_distance = tf.reduce_sum(l1_distance, axis=1,\n                                               keep_dims=True)\n\n            # Exponentiate the negative summed L1 distance to get the\n            # positive-class probability.\n            # Shape: (batch_size, 1)\n            positive_class_probs = tf.exp(-summed_l1_distance)\n\n            # Get the negative class probabilities by subtracting\n            # the positive class probabilities from 1.\n            # Shape: (batch_size, 1)\n            negative_class_probs = 1 - positive_class_probs\n\n            # Concatenate the positive and negative class probabilities\n            # Shape: (batch_size, 2)\n            class_probabilities = tf.concat([negative_class_probs,\n                                             positive_class_probs], 1)\n\n            # if class_probabilities has 0\'s, then taking the log of it\n            # (e.g. for cross-entropy loss) will cause NaNs. So we add\n            # epsilon and renormalize by the sum of the vector.\n            safe_class_probabilities = class_probabilities + 1e-08\n            safe_class_probabilities /= tf.reduce_sum(safe_class_probabilities,\n                                                      axis=1,\n                                                      keep_dims=True)\n            return safe_class_probabilities\n'"
duplicate_questions/models/siamese_bilstm/siamese_matching_bilstm.py,42,"b'from copy import deepcopy\nimport logging\nfrom overrides import overrides\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import LSTMCell\n\nfrom ..base_tf_model import BaseTFModel\nfrom ...util.switchable_dropout_wrapper import SwitchableDropoutWrapper\nfrom ...util.pooling import mean_pool\n\nlogger = logging.getLogger(__name__)\n\n\nclass SiameseMatchingBiLSTM(BaseTFModel):\n    """"""\n    Create a model based off of the baseline (no inner-attention) in\n    ""Learning Natural Language Inference using Bidirectional LSTM model\n    and Inner-Attention"" (https://arxiv.org/abs/1605.09090).\n\n    The model is super simple: just encode\n    both sentences with a LSTM, and take the mean pool over the timesteps\n    as the sentence representation. Then, create a vector with the\n    by concatenating (||) the following:\n    sentence1|sentence1-sentence2|sentence1*sentence2|sentence2\n\n    Lastly, run this vector through a dense layer to (relu activation)\n    to get the logits, which are then softmaxed to get a probability\n    distribution [is_not_duplicate, is_duplicate].\n\n    The input config is an argarse Namespace storing a variety of configuration\n    values that are necessary to build the graph. The keys we expect\n    in this Namespace are outlined below.\n\n    Parameters\n    ----------\n    mode: str\n        One of {train|predict}, to indicate what you want the model to do.\n        If you pick ""predict"", then you must also supply the path to a\n        pretrained model and DataIndexer to load.\n\n    word_vocab_size: int\n        The number of unique tokens in the dataset, plus the UNK and padding\n        tokens. Alternatively, the highest index assigned to any word, +1.\n        This is used by the model to figure out the dimensionality of the\n        embedding matrix.\n\n    word_embedding_dim: int\n        The length of a word embedding. This is used by\n        the model to figure out the dimensionality of the embedding matrix.\n\n    word_embedding_matrix: numpy array, optional if predicting\n        A numpy array of shape (word_vocab_size, word_emb_dim).\n        word_embedding_matrix[index] should represent the word vector for\n        that particular word index. This is used to initialize the\n        word embedding matrix in the model, and is optional if predicting\n        since we assume that the word embeddings variable will be loaded\n        with the model.\n\n    fine_tune_embeddings: boolean\n        If true, sets the embeddings to be trainable.\n\n    rnn_hidden_size: int\n        The output dimension of the RNN encoder. Note that this model uses a\n        bidirectional LSTM, so the actual sentence vectors will be\n        of length 2*rnn_hidden_size.\n\n    share_encoder_weights: boolean\n        Whether to use the same encoder on both input sentnces (thus\n        sharing weights), or a different one for each sentence.\n\n    output_keep_prob: float\n        The probability of keeping an RNN outputs to keep, as opposed\n        to dropping it out.\n    """"""\n    def __init__(self, config_dict):\n        config_dict = deepcopy(config_dict)\n        mode = config_dict.pop(""mode"")\n        super(SiameseMatchingBiLSTM, self).__init__(mode=mode)\n\n        self.word_vocab_size = config_dict.pop(""word_vocab_size"")\n        self.word_embedding_dim = config_dict.pop(""word_embedding_dim"")\n        self.word_embedding_matrix = config_dict.pop(""word_embedding_matrix"", None)\n        self.fine_tune_embeddings = config_dict.pop(""fine_tune_embeddings"")\n        self.rnn_hidden_size = config_dict.pop(""rnn_hidden_size"")\n        self.share_encoder_weights = config_dict.pop(""share_encoder_weights"")\n        self.output_keep_prob = config_dict.pop(""output_keep_prob"")\n\n        if config_dict:\n            logger.warning(""UNUSED VALUES IN CONFIG DICT: {}"".format(config_dict))\n\n    def _create_placeholders(self):\n        """"""\n        Create the placeholders for use in the model.\n        """"""\n        # Define the inputs here\n        # Shape: (batch_size, num_sentence_words)\n        # The first input sentence.\n        self.sentence_one = tf.placeholder(""int32"",\n                                           [None, None],\n                                           name=""sentence_one"")\n\n        # Shape: (batch_size, num_sentence_words)\n        # The second input sentence.\n        self.sentence_two = tf.placeholder(""int32"",\n                                           [None, None],\n                                           name=""sentence_two"")\n        # Shape: (batch_size, 2)\n        # The true labels, encoded as a one-hot vector. So\n        # [1, 0] indicates not duplicate, [0, 1] indicates duplicate.\n        self.y_true = tf.placeholder(""int32"",\n                                     [None, 2],\n                                     name=""true_labels"")\n\n        # A boolean that encodes whether we are training or evaluating\n        self.is_train = tf.placeholder(\'bool\', [], name=\'is_train\')\n\n    def _build_forward(self):\n        """"""\n        Using the config passed to the SiameseMatchingBiLSTM object on\n        creation, build the forward pass of the computation graph.\n        """"""\n        # A mask over the word indices in the sentence, indicating\n        # which indices are padding and which are words.\n        # Shape: (batch_size, num_sentence_words)\n        sentence_one_mask = tf.sign(self.sentence_one,\n                                    name=""sentence_one_masking"")\n        sentence_two_mask = tf.sign(self.sentence_two,\n                                    name=""sentence_two_masking"")\n\n        # The unpadded lengths of sentence one and sentence two\n        # Shape: (batch_size,)\n        sentence_one_len = tf.reduce_sum(sentence_one_mask, 1)\n        sentence_two_len = tf.reduce_sum(sentence_two_mask, 1)\n\n        word_vocab_size = self.word_vocab_size\n        word_embedding_dim = self.word_embedding_dim\n        word_embedding_matrix = self.word_embedding_matrix\n        fine_tune_embeddings = self.fine_tune_embeddings\n\n        with tf.variable_scope(""embeddings""):\n            with tf.variable_scope(""embedding_var""), tf.device(""/cpu:0""):\n                if self.mode == ""train"":\n                    # Load the word embedding matrix from the config,\n                    # since we are training\n                    word_emb_mat = tf.get_variable(\n                        ""word_emb_mat"",\n                        dtype=""float"",\n                        shape=[word_vocab_size,\n                               word_embedding_dim],\n                        initializer=tf.constant_initializer(\n                            word_embedding_matrix),\n                        trainable=fine_tune_embeddings)\n                else:\n                    # We are not training, so a model should have been\n                    # loaded with the embedding matrix already there.\n                    word_emb_mat = tf.get_variable(""word_emb_mat"",\n                                                   shape=[word_vocab_size,\n                                                          word_embedding_dim],\n                                                   dtype=""float"",\n                                                   trainable=fine_tune_embeddings)\n\n            with tf.variable_scope(""word_embeddings""):\n                # Shape: (batch_size, num_sentence_words, embedding_dim)\n                word_embedded_sentence_one = tf.nn.embedding_lookup(\n                    word_emb_mat,\n                    self.sentence_one)\n                # Shape: (batch_size, num_sentence_words, embedding_dim)\n                word_embedded_sentence_two = tf.nn.embedding_lookup(\n                    word_emb_mat,\n                    self.sentence_two)\n\n        rnn_hidden_size = self.rnn_hidden_size\n        output_keep_prob = self.output_keep_prob\n        rnn_cell_fw_one = LSTMCell(rnn_hidden_size, state_is_tuple=True)\n        d_rnn_cell_fw_one = SwitchableDropoutWrapper(rnn_cell_fw_one,\n                                                     self.is_train,\n                                                     output_keep_prob=output_keep_prob)\n        rnn_cell_bw_one = LSTMCell(rnn_hidden_size, state_is_tuple=True)\n        d_rnn_cell_bw_one = SwitchableDropoutWrapper(rnn_cell_bw_one,\n                                                     self.is_train,\n                                                     output_keep_prob=output_keep_prob)\n        with tf.variable_scope(""encode_sentences""):\n            # Encode the first sentence.\n            (fw_output_one, bw_output_one), _ = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=d_rnn_cell_fw_one,\n                cell_bw=d_rnn_cell_bw_one,\n                dtype=""float"",\n                sequence_length=sentence_one_len,\n                inputs=word_embedded_sentence_one,\n                scope=""encoded_sentence_one"")\n            if self.share_encoder_weights:\n                # Encode the second sentence, using the same RNN weights.\n                tf.get_variable_scope().reuse_variables()\n                (fw_output_two, bw_output_two), _ = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw=d_rnn_cell_fw_one,\n                    cell_bw=d_rnn_cell_bw_one,\n                    dtype=""float"",\n                    sequence_length=sentence_two_len,\n                    inputs=word_embedded_sentence_two,\n                    scope=""encoded_sentence_one"")\n            else:\n                # Encode the second sentence with a different RNN\n                rnn_cell_fw_two = LSTMCell(rnn_hidden_size, state_is_tuple=True)\n                d_rnn_cell_fw_two = SwitchableDropoutWrapper(\n                    rnn_cell_fw_two,\n                    self.is_train,\n                    output_keep_prob=output_keep_prob)\n                rnn_cell_bw_two = LSTMCell(rnn_hidden_size, state_is_tuple=True)\n                d_rnn_cell_bw_two = SwitchableDropoutWrapper(\n                    rnn_cell_bw_two,\n                    self.is_train,\n                    output_keep_prob=output_keep_prob)\n                (fw_output_two, bw_output_two), _ = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw=d_rnn_cell_fw_two,\n                    cell_bw=d_rnn_cell_bw_two,\n                    dtype=""float"",\n                    sequence_length=sentence_two_len,\n                    inputs=word_embedded_sentence_two,\n                    scope=""encoded_sentence_two"")\n\n            # Now, combine the fw_output and bw_output for the\n            # first and second sentence LSTM outputs by mean pooling\n            pooled_fw_output_one = mean_pool(fw_output_one,\n                                             sentence_one_len)\n            pooled_bw_output_one = mean_pool(bw_output_one,\n                                             sentence_one_len)\n            pooled_fw_output_two = mean_pool(fw_output_two,\n                                             sentence_two_len)\n            pooled_bw_output_two = mean_pool(bw_output_two,\n                                             sentence_two_len)\n            # Shape: (batch_size, 2*rnn_hidden_size)\n            encoded_sentence_one = tf.concat([pooled_fw_output_one,\n                                              pooled_bw_output_one], 1)\n            encoded_sentence_two = tf.concat([pooled_fw_output_two,\n                                              pooled_bw_output_two], 1)\n\n        # Sentence matching layer\n        with tf.name_scope(""match_sentences""):\n            sentence_difference = encoded_sentence_one - encoded_sentence_two\n            sentence_product = encoded_sentence_one * encoded_sentence_two\n            # Shape: (batch_size, 4 * 2*rnn_hidden_size)\n            matching_vector = tf.concat([encoded_sentence_one, sentence_product,\n                                         sentence_difference, encoded_sentence_two], 1)\n        # Nonlinear projection to 2 dimensional class probabilities\n        with tf.variable_scope(""project_matching_vector""):\n            # Shape: (batch_size, 2)\n            projection = tf.layers.dense(matching_vector, 2, tf.nn.relu,\n                                         name=""matching_vector_projection"")\n\n        with tf.name_scope(""loss""):\n            # Get the predicted class probabilities\n            # Shape: (batch_size, 2)\n            self.y_pred = tf.nn.softmax(projection, name=""softmax_probabilities"")\n            # Use softmax_cross_entropy_with_logits to calculate xentropy.\n            # It\'s unideal to do the softmax twice, but I prefer the numerical\n            # stability of the tf function.\n            self.loss = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(labels=self.y_true,\n                                                        logits=projection,\n                                                        name=""xentropy_loss""))\n\n        with tf.name_scope(""accuracy""):\n            # Get the correct predictions.\n            # Shape: (batch_size,) of bool\n            correct_predictions = tf.equal(\n                tf.argmax(self.y_pred, 1),\n                tf.argmax(self.y_true, 1))\n\n            # Cast to float, and take the mean to get accuracy\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions,\n                                                   ""float""))\n\n        with tf.name_scope(""train""):\n            optimizer = tf.train.AdamOptimizer()\n            self.training_op = optimizer.minimize(self.loss,\n                                                  global_step=self.global_step)\n\n        with tf.name_scope(""train_summaries""):\n            # Add the loss and the accuracy to the tensorboard summary\n            tf.summary.scalar(""loss"", self.loss)\n            tf.summary.scalar(""accuracy"", self.accuracy)\n            self.summary_op = tf.summary.merge_all()\n\n    @overrides\n    def _get_train_feed_dict(self, batch):\n        inputs, targets = batch\n        feed_dict = {self.sentence_one: inputs[0],\n                     self.sentence_two: inputs[1],\n                     self.y_true: targets[0],\n                     self.is_train: True}\n        return feed_dict\n\n    @overrides\n    def _get_validation_feed_dict(self, batch):\n        inputs, targets = batch\n        feed_dict = {self.sentence_one: inputs[0],\n                     self.sentence_two: inputs[1],\n                     self.y_true: targets[0],\n                     self.is_train: False}\n        return feed_dict\n\n    @overrides\n    def _get_test_feed_dict(self, batch):\n        inputs, _ = batch\n        feed_dict = {self.sentence_one: inputs[0],\n                     self.sentence_two: inputs[1],\n                     self.is_train: False}\n        return feed_dict\n'"
scripts/data/quora/clean_quora_dataset.py,0,"b'import argparse\nimport csv\nimport logging\nimport re\nimport os\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    argparser = argparse.ArgumentParser(description=(""Clean the Quora dataset ""\n                                                     ""by removing newlines in ""\n                                                     ""the data.""))\n    argparser.add_argument(""dataset_input_path"", type=str,\n                           help=(""The path to the raw Quora ""\n                                 ""dataset to clean.""))\n    argparser.add_argument(""dataset_output_path"", type=str,\n                           help=(""The *folder* to write the ""\n                                 ""cleaned file to. The name will just have ""\n                                 ""_cleaned appended to it, before the ""\n                                 ""extension""))\n    config = argparser.parse_args()\n\n    # Get the data\n    logger.info(""Reading csv at {}"".format(config.dataset_input_path))\n\n    # Iterate through the CSV, removing anomalous whitespace\n    # and making a list of lists the clean csv.\n    logger.info(""Cleaning csv"")\n    clean_rows = []\n    with open(config.dataset_input_path) as f:\n        reader = csv.reader(f)\n        # skip the header\n        reader.__next__()\n        for row in reader:\n            clean_row = []\n            for item in row:\n                # normalize whitespace in each string in each row\n                item_no_newlines = re.sub(r""\\n"", "" "", item)\n                clean_item = re.sub(r""\\s+"", "" "", item_no_newlines)\n                clean_row.append(clean_item)\n            clean_rows.append(clean_row)\n\n    input_filename_full = os.path.basename(config.dataset_input_path)\n    input_filename, input_ext = os.path.splitext(input_filename_full)\n    out_path = os.path.join(config.dataset_output_path,\n                            input_filename + ""_cleaned"" + input_ext)\n\n    logger.info(""Writing output to {}"".format(out_path))\n    with open(out_path, ""w"") as f:\n        writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n        writer.writerows(clean_rows)\n\n\nif __name__ == ""__main__"":\n    logging.basicConfig(format=(""%(asctime)s - %(levelname)s - ""\n                                ""%(name)s - %(message)s""),\n                        level=logging.INFO)\n    main()\n'"
scripts/data/quora/split_quora_file.py,0,"b'import argparse\nimport csv\nimport logging\nimport os\nimport random\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    argparser = argparse.ArgumentParser(\n        description=(""Split a file from the Kaggle Quora dataset ""\n                     ""into train and validation files, given a validation ""\n                     ""proportion""))\n    argparser.add_argument(""validation_proportion"", type=float,\n                           help=(""Proportion of data in the input file ""\n                                 ""to randomly split into a separate ""\n                                 ""validation file.""))\n    argparser.add_argument(""dataset_input_path"", type=str,\n                           help=(""The path to the cleaned Quora ""\n                                 ""dataset file to split.""))\n    argparser.add_argument(""dataset_output_path"", type=str,\n                           help=(""The *folder* to write the ""\n                                 ""split files to. The name will just have ""\n                                 ""_{split}_split appended to it, before ""\n                                 ""the extension""))\n    config = argparser.parse_args()\n\n    # Get the data\n    logger.info(""Reading csv at {}"".format(config.dataset_input_path))\n    with open(config.dataset_input_path) as f:\n        reader = csv.reader(f)\n        csv_rows = list(reader)\n\n    logger.info(""Shuffling input csv."")\n    # For reproducibility\n    random.seed(0)\n    # Shuffle csv_rows deterministically in place\n    random.shuffle(csv_rows)\n\n    num_validation_lines = int(len(csv_rows) * config.validation_proportion)\n\n    input_filename_full = os.path.basename(config.dataset_input_path)\n    input_filename, input_ext = os.path.splitext(input_filename_full)\n    train_out_path = os.path.join(config.dataset_output_path,\n                                  input_filename + ""_train_split"" + input_ext)\n    val_out_path = os.path.join(config.dataset_output_path,\n                                input_filename + ""_val_split"" + input_ext)\n\n    logger.info(""Writing train split output to {}"".format(train_out_path))\n    with open(train_out_path, ""w"") as f:\n        writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n        writer.writerows(csv_rows[num_validation_lines:])\n\n    logger.info(""Writing validation split output to {}"".format(val_out_path))\n    with open(val_out_path, ""w"") as f:\n        writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n        writer.writerows(csv_rows[:num_validation_lines])\n\n\nif __name__ == ""__main__"":\n    logging.basicConfig(format=(""%(asctime)s - %(levelname)s - ""\n                                ""%(name)s - %(message)s""),\n                        level=logging.INFO)\n    main()\n'"
tests/data/instances/__init__.py,0,b''
tests/data/instances/test_instance.py,0,"b'# pylint: disable=no-self-use,invalid-name\nfrom duplicate_questions.data.data_indexer import DataIndexer\nfrom duplicate_questions.data.instances.instance import TextInstance\nfrom duplicate_questions.data.tokenizers.word_tokenizers import NLTKWordTokenizer\nfrom duplicate_questions.data.instances.instance import IndexedInstance\nfrom duplicate_questions.data.instances.sts_instance import STSInstance\n\nfrom ...common.test_case import DuplicateTestCase\n\n\nclass TestTextInstance(DuplicateTestCase):\n    """"""\n    The point of this test class is to test the tokenizer used by the\n    TextInstance, to be sure that we get what we expect.\n    """"""\n    def test_word_tokenizer_tokenizes_the_sentence_correctly(self):\n        instance = STSInstance(""One sentence."",\n                               ""A two sentence."", NLTKWordTokenizer)\n        assert instance.words() == {""words"": [""one"", ""sentence"",\n                                              ""."", ""a"", ""two"", ""sentence"", "".""],\n                                    ""characters"": [\'o\', \'n\', \'e\', \'s\', \'e\', \'n\',\n                                                   \'t\', \'e\', \'n\', \'c\', \'e\', \'.\',\n                                                   \'a\', \'t\', \'w\', \'o\', \'s\', \'e\',\n                                                   \'n\', \'t\', \'e\', \'n\', \'c\', \'e\', \'.\']}\n\n    def test_exceptions(self):\n        instance = TextInstance()\n        data_indexer = DataIndexer()\n        with self.assertRaises(NotImplementedError):\n            instance.words()\n        with self.assertRaises(NotImplementedError):\n            instance.to_indexed_instance(data_indexer)\n        with self.assertRaises(RuntimeError):\n            instance.read_from_line(""some line"")\n        with self.assertRaises(NotImplementedError):\n            instance.words()\n\n\nclass TestIndexedInstance(DuplicateTestCase):\n    def test_exceptions(self):\n        instance = IndexedInstance()\n        with self.assertRaises(NotImplementedError):\n            instance.empty_instance()\n        with self.assertRaises(NotImplementedError):\n            instance.get_lengths()\n        with self.assertRaises(NotImplementedError):\n            instance.pad({})\n        with self.assertRaises(NotImplementedError):\n            instance.as_training_data()\n        with self.assertRaises(NotImplementedError):\n            instance.as_testing_data()\n'"
tests/data/instances/test_sts_instance.py,0,"b'from numpy.testing import assert_allclose\nimport numpy as np\nfrom duplicate_questions.data.data_indexer import DataIndexer\nfrom duplicate_questions.data.instances.instance_word import IndexedInstanceWord\nfrom duplicate_questions.data.instances.sts_instance import IndexedSTSInstance\nfrom duplicate_questions.data.instances.sts_instance import STSInstance\n\nfrom ...common.test_case import DuplicateTestCase\n\n\nclass TestSTSInstance(DuplicateTestCase):\n    @staticmethod\n    def instance_to_line(id, question1, question2,\n                         is_duplicate=None, qid1=None, qid2=None):\n        if qid1 is None and qid2 is None and is_duplicate is None:\n            # test set example\n            line = ""\\""{}\\"",\\""{}\\"",\\""{}\\"""".format(id, question1,\n                                                 question2)\n        else:\n            line = (""\\""{}\\"",\\""{}\\"",\\""{}\\"",""\n                    ""\\""{}\\"",\\""{}\\"",\\""{}\\"""").format(id, qid1, qid2, question1,\n                                                   question2, is_duplicate)\n        return line\n\n    def test_read_from_line_handles_test_example(self):\n        question1 = ""Does he enjoy playing soccer in the rain?""\n        question2 = ""Does he enjoy coding in the rain?""\n        id = 0\n        line = self.instance_to_line(id, question1, question2)\n        instance = STSInstance.read_from_line(line)\n        assert instance.first_sentence_str == question1\n        expected_first_sentence_words = [""does"", ""he"", ""enjoy"", ""playing"",\n                                         ""soccer"", ""in"", ""the"", ""rain"", ""?""]\n        expected_first_sentence_chars = list(map(list, expected_first_sentence_words))\n        assert instance.first_sentence_tokenized == {\n            ""words"": expected_first_sentence_words,\n            ""characters"": expected_first_sentence_chars\n        }\n        expected_second_sentence_words = [""does"", ""he"", ""enjoy"", ""coding"",\n                                          ""in"", ""the"", ""rain"", ""?""]\n        expected_second_sentence_chars = list(map(list, expected_second_sentence_words))\n        assert instance.second_sentence_tokenized == {\n            ""words"": expected_second_sentence_words,\n            ""characters"": expected_second_sentence_chars\n        }\n        assert instance.second_sentence_str == question2\n        assert instance.label is None\n\n    def test_read_from_line_handles_train_example(self):\n        question1 = ""Does he enjoy playing soccer in the rain?""\n        question2 = ""Does he enjoy coding in the rain?""\n        id = 0\n        qid1 = 0\n        qid2 = 1\n        label = 0\n        line = self.instance_to_line(id, question1, question2,\n                                     label, qid1, qid2)\n        instance = STSInstance.read_from_line(line)\n        assert instance.first_sentence_str == question1\n        expected_first_sentence_words = [""does"", ""he"", ""enjoy"", ""playing"",\n                                         ""soccer"", ""in"", ""the"", ""rain"", ""?""]\n        expected_first_sentence_chars = list(map(list, expected_first_sentence_words))\n        assert instance.first_sentence_tokenized == {\n            ""words"": expected_first_sentence_words,\n            ""characters"": expected_first_sentence_chars\n        }\n        expected_second_sentence_words = [""does"", ""he"", ""enjoy"", ""coding"",\n                                          ""in"", ""the"", ""rain"", ""?""]\n        expected_second_sentence_chars = list(map(list, expected_second_sentence_words))\n        assert instance.second_sentence_tokenized == {\n            ""words"": expected_second_sentence_words,\n            ""characters"": expected_second_sentence_chars\n        }\n        assert instance.second_sentence_str == question2\n        assert instance.label == 0\n        with self.assertRaises(RuntimeError):\n            STSInstance.read_from_line(""This is not a proper line."")\n\n    def test_to_indexed_instance_converts_correctly(self):\n        instance = STSInstance(""What do dogs eat?"",\n                               ""What do cats eat, play with, or enjoy?"",\n                               0)\n        data_indexer = DataIndexer()\n        what_index = data_indexer.add_word_to_index(""what"")\n        do_index = data_indexer.add_word_to_index(""do"")\n        dogs_index = data_indexer.add_word_to_index(""dogs"")\n        eat_index = data_indexer.add_word_to_index(""eat"")\n        cats_index = data_indexer.add_word_to_index(""cats"")\n        question_index = data_indexer.add_word_to_index(""?"")\n        comma_index = data_indexer.add_word_to_index("","")\n        play_index = data_indexer.add_word_to_index(""play"")\n        with_index = data_indexer.add_word_to_index(""with"")\n        or_index = data_indexer.add_word_to_index(""or"")\n        enjoy_index = data_indexer.add_word_to_index(""enjoy"")\n        idxd_instance = instance.to_indexed_instance(data_indexer)\n        first_sent_word_idxs, second_sent_word_idxs = idxd_instance.get_int_word_indices()\n        assert first_sent_word_idxs == [what_index,\n                                        do_index,\n                                        dogs_index,\n                                        eat_index,\n                                        question_index]\n        assert second_sent_word_idxs == [what_index,\n                                         do_index,\n                                         cats_index,\n                                         eat_index,\n                                         comma_index,\n                                         play_index,\n                                         with_index,\n                                         comma_index,\n                                         or_index,\n                                         enjoy_index,\n                                         question_index]\n        assert idxd_instance.label == [1, 0]\n\n    def test_words_tokenizes_the_sentence_correctly(self):\n        sts_instance = STSInstance(""A sentence."",\n                                   ""Another sentence."",\n                                   0)\n        expected_words = [""a"", ""sentence"", ""."",\n                          ""another"", ""sentence"", "".""]\n        expected_characters = [\'a\', \'s\', \'e\', \'n\', \'t\', \'e\', \'n\', \'c\', \'e\',\n                               \'.\', \'a\', \'n\', \'o\', \'t\', \'h\', \'e\', \'r\', \'s\', \'e\',\n                               \'n\', \'t\', \'e\', \'n\', \'c\', \'e\', \'.\']\n        assert sts_instance.words() == {""words"": expected_words,\n                                        ""characters"": expected_characters}\n\n\nclass TestIndexedSTSInstance(DuplicateTestCase):\n    def setUp(self):\n        super(TestIndexedSTSInstance, self).setUp()\n        self.instance = IndexedSTSInstance([IndexedInstanceWord(1, [1, 2]),\n                                            IndexedInstanceWord(2, [3, 4]),\n                                            IndexedInstanceWord(3, [5]),\n                                            IndexedInstanceWord(5, [1, 4, 1]),\n                                            IndexedInstanceWord(4, [1, 2, 6])],\n                                           [IndexedInstanceWord(1, [1, 2]),\n                                            IndexedInstanceWord(8, [3, 1, 2, 1]),\n                                            IndexedInstanceWord(2, [3, 4]),\n                                            IndexedInstanceWord(3, [5])],\n                                           [0, 1])\n\n    def test_get_lengths(self):\n        assert self.instance.get_lengths() == {""num_sentence_words"": 5,\n                                               \'num_word_characters\': 4}\n\n    def test_pad_adds_padding_words(self):\n        self.instance.pad({""num_sentence_words"": 6,\n                           \'num_word_characters\': 5})\n        first_sent_word_idxs, second_sent_word_idxs = self.instance.get_int_word_indices()\n        first_sent_char_idxs, second_sent_char_idxs = self.instance.get_int_char_indices()\n\n        assert first_sent_word_idxs == [1, 2, 3, 5, 4, 0]\n        assert second_sent_word_idxs == [1, 8, 2, 3, 0, 0]\n        assert first_sent_char_idxs == [[1, 2, 0, 0, 0], [3, 4, 0, 0, 0],\n                                        [5, 0, 0, 0, 0], [1, 4, 1, 0, 0],\n                                        [1, 2, 6, 0, 0], [0, 0, 0, 0, 0]]\n        assert second_sent_char_idxs == [[1, 2, 0, 0, 0], [3, 1, 2, 1, 0],\n                                         [3, 4, 0, 0, 0], [5, 0, 0, 0, 0],\n                                         [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n        assert self.instance.label == [0, 1]\n\n    def test_pad_truncates(self):\n        self.instance.pad({""num_sentence_words"": 2,\n                           \'num_word_characters\': 3})\n        first_sent_word_idxs, second_sent_word_idxs = self.instance.get_int_word_indices()\n        first_sent_char_idxs, second_sent_char_idxs = self.instance.get_int_char_indices()\n\n        assert first_sent_word_idxs == [1, 2]\n        assert second_sent_word_idxs == [1, 8]\n        assert first_sent_char_idxs == [[1, 2, 0], [3, 4, 0]]\n        assert second_sent_char_idxs == [[1, 2, 0], [3, 1, 2]]\n        assert self.instance.label == [0, 1]\n\n    def test_pad_general(self):\n        self.instance.pad(self.instance.get_lengths())\n        first_sent_word_idxs, second_sent_word_idxs = self.instance.get_int_word_indices()\n        first_sent_char_idxs, second_sent_char_idxs = self.instance.get_int_char_indices()\n\n        assert first_sent_word_idxs == [1, 2, 3, 5, 4]\n        assert second_sent_word_idxs == [1, 8, 2, 3, 0]\n        assert first_sent_char_idxs == [[1, 2, 0, 0], [3, 4, 0, 0],\n                                        [5, 0, 0, 0], [1, 4, 1, 0],\n                                        [1, 2, 6, 0]]\n        assert second_sent_char_idxs == [[1, 2, 0, 0], [3, 1, 2, 1],\n                                         [3, 4, 0, 0], [5, 0, 0, 0],\n                                         [0, 0, 0, 0]]\n        assert self.instance.label == [0, 1]\n\n    def test_as_training_data_produces_correct_numpy_arrays(self):\n        self.instance.pad({\'num_sentence_words\': 3, \'num_word_characters\': 2})\n        inputs, label = self.instance.as_training_data()\n        assert_allclose(label[0], np.asarray([0, 1]))\n        assert len(inputs) == 2\n        assert_allclose(inputs[0], np.asarray([1, 2, 3]))\n        assert_allclose(inputs[1], np.asarray([1, 8, 2]))\n\n        inputs, label = self.instance.as_training_data(mode=""character"")\n        assert_allclose(label[0], np.asarray([0, 1]))\n        assert len(inputs) == 2\n        assert_allclose(inputs[0], np.asarray([[1, 2], [3, 4], [5, 0]]))\n        assert_allclose(inputs[1], np.asarray([[1, 2], [3, 1], [3, 4]]))\n\n        inputs, label = self.instance.as_training_data(mode=""word+character"")\n        assert_allclose(label[0], np.asarray([0, 1]))\n        assert len(inputs) == 4\n        assert_allclose(inputs[0], np.asarray([1, 2, 3]))\n        assert_allclose(inputs[1], np.asarray([[1, 2], [3, 4], [5, 0]]))\n        assert_allclose(inputs[2], np.asarray([1, 8, 2]))\n        assert_allclose(inputs[3], np.asarray([[1, 2], [3, 1], [3, 4]]))\n\n    def test_as_training_data_error(self):\n        with self.assertRaises(ValueError):\n            instance = IndexedSTSInstance([IndexedInstanceWord(1, [1, 2]),\n                                           IndexedInstanceWord(4, [1, 2, 6])],\n                                          [IndexedInstanceWord(1, [1, 2]),\n                                           IndexedInstanceWord(3, [5])],\n                                          None)\n            instance.as_training_data()\n        with self.assertRaises(ValueError):\n            self.instance.as_training_data(mode=""words+character"")\n\n    def test_as_testing_data_produces_correct_numpy_arrays(self):\n        self.instance.pad({\'num_sentence_words\': 4, \'num_word_characters\': 2})\n        inputs, labels = self.instance.as_testing_data()\n        assert len(labels) == 0\n        assert len(inputs) == 2\n        assert_allclose(inputs[0], np.asarray([1, 2, 3, 5]))\n        assert_allclose(inputs[1], np.asarray([1, 8, 2, 3]))\n\n        inputs, label = self.instance.as_training_data(mode=""character"")\n        assert len(labels) == 0\n        assert len(inputs) == 2\n        assert_allclose(inputs[0], np.asarray([[1, 2], [3, 4], [5, 0], [1, 4]]))\n        assert_allclose(inputs[1], np.asarray([[1, 2], [3, 1], [3, 4], [5, 0]]))\n\n        inputs, label = self.instance.as_training_data(mode=""word+character"")\n        assert len(labels) == 0\n        assert len(inputs) == 4\n        assert_allclose(inputs[0], np.asarray([1, 2, 3, 5]))\n        assert_allclose(inputs[1], np.asarray([[1, 2], [3, 4], [5, 0], [1, 4]]))\n        assert_allclose(inputs[2], np.asarray([1, 8, 2, 3]))\n        assert_allclose(inputs[3], np.asarray([[1, 2], [3, 1], [3, 4], [5, 0]]))\n\n    def test_as_testing_data_error(self):\n        with self.assertRaises(ValueError):\n            self.instance.as_testing_data(mode=""words+character"")\n\n    def test_equals(self):\n        instance_1 = IndexedSTSInstance([IndexedInstanceWord(1, [1, 2]),\n                                         IndexedInstanceWord(4, [1, 2, 6])],\n                                        [IndexedInstanceWord(1, [1, 2]),\n                                         IndexedInstanceWord(3, [5])],\n                                        None)\n\n        instance_2 = IndexedSTSInstance([IndexedInstanceWord(1, [1, 2]),\n                                         IndexedInstanceWord(4, [1, 2, 6])],\n                                        [IndexedInstanceWord(1, [1, 2]),\n                                         IndexedInstanceWord(3, [5])],\n                                        None)\n\n        instance_3 = IndexedSTSInstance([IndexedInstanceWord(1, [1, 2]),\n                                         IndexedInstanceWord(1, [2, 2])],\n                                        [IndexedInstanceWord(1, [1, 2]),\n                                         IndexedInstanceWord(3, [5])],\n                                        None)\n        instance_4 = IndexedSTSInstance([IndexedInstanceWord(1, [1, 2])],\n                                        [IndexedInstanceWord(1, [2, 2])],\n                                        None)\n        self.assertNotEquals(instance_1, instance_4)\n        self.assertNotEquals(instance_1, instance_3)\n        self.assertFalse(instance_1.__eq__(0))\n        self.assertEquals(instance_1, instance_2)\n\n    def test_less_than(self):\n        instance_1 = IndexedSTSInstance([IndexedInstanceWord(1, [1, 2]),\n                                         IndexedInstanceWord(4, [1, 2, 6])],\n                                        [IndexedInstanceWord(1, [1, 2]),\n                                         IndexedInstanceWord(3, [5])],\n                                        None)\n\n        instance_2 = IndexedSTSInstance([IndexedInstanceWord(1, [1, 2]),\n                                         IndexedInstanceWord(4, [1, 2, 6])],\n                                        [IndexedInstanceWord(2, [2, 2]),\n                                         IndexedInstanceWord(3, [5])],\n                                        None)\n\n        instance_3 = IndexedSTSInstance([IndexedInstanceWord(1, [1, 2])],\n                                        [IndexedInstanceWord(1, [2, 2])],\n                                        None)\n        self.assertFalse(instance_1.__lt__(0))\n        self.assertFalse(instance_2.__lt__(instance_1))\n        self.assertLess(instance_1, instance_2)\n        self.assertLess(instance_3, instance_2)\n'"
tests/models/bimpm/__init__.py,0,b''
tests/models/bimpm/test_bimpm.py,1,"b'from overrides import overrides\nimport math\nimport tensorflow as tf\n\nfrom duplicate_questions.data.data_manager import DataManager\nfrom duplicate_questions.data.embedding_manager import EmbeddingManager\nfrom duplicate_questions.models.bimpm.bimpm import BiMPM\nfrom duplicate_questions.data.instances.sts_instance import STSInstance\n\nfrom ...common.test_case import DuplicateTestCase\n\n\nclass TestBiMPM(DuplicateTestCase):\n    @overrides\n    def setUp(self):\n        super(TestBiMPM, self).setUp()\n        self.write_duplicate_questions_train_file()\n        self.write_duplicate_questions_validation_file()\n        self.write_duplicate_questions_test_file()\n        self.data_manager = DataManager(STSInstance)\n        self.batch_size = 3\n        self.get_train_gen, self.train_size = self.data_manager.get_train_data_from_file(\n            [self.TRAIN_FILE],\n            mode=""word+character"")\n        self.get_val_gen, self.val_size = self.data_manager.get_validation_data_from_file(\n            [self.VALIDATION_FILE],\n            mode=""word+character"")\n        self.get_test_gen, self.test_size = self.data_manager.get_test_data_from_file(\n            [self.TEST_FILE],\n            mode=""word+character"")\n\n        self.embedding_manager = EmbeddingManager(self.data_manager.data_indexer)\n        self.word_embedding_dim = 5\n        self.word_embedding_matrix = self.embedding_manager.get_embedding_matrix(\n            self.word_embedding_dim)\n        self.char_embedding_dim = 2\n        self.char_embedding_matrix = self.embedding_manager.get_embedding_matrix(\n            self.char_embedding_dim)\n        self.char_rnn_hidden_size = 6\n        self.context_rnn_hidden_size = 3\n        self.aggregation_rnn_hidden_size = 4\n        self.dropout_ratio = 0.1\n        self.config_dict = {\n            ""mode"": ""train"",\n            ""word_vocab_size"": self.data_manager.data_indexer.get_vocab_size(),\n            ""word_embedding_dim"": self.word_embedding_dim,\n            ""word_embedding_matrix"": self.word_embedding_matrix,\n            ""char_vocab_size"": self.data_manager.data_indexer.get_vocab_size(\n                namespace=""characters""),\n            ""char_embedding_dim"": self.char_embedding_dim,\n            ""char_embedding_matrix"": self.char_embedding_matrix,\n            ""char_rnn_hidden_size"": self.char_rnn_hidden_size,\n            ""fine_tune_embeddings"": False,\n            ""context_rnn_hidden_size"": self.context_rnn_hidden_size,\n            ""aggregation_rnn_hidden_size"": self.aggregation_rnn_hidden_size,\n            ""dropout_ratio"": self.dropout_ratio\n        }\n\n        self.num_train_steps_per_epoch = int(math.ceil(self.train_size / self.batch_size))\n        self.num_val_steps = int(math.ceil(self.val_size / self.batch_size))\n        self.num_test_steps = int(math.ceil(self.test_size / self.batch_size))\n\n    def test_default_does_not_crash(self):\n        # Initialize the model\n        model = BiMPM(self.config_dict)\n        model.build_graph()\n        # Train the model\n        model.train(get_train_instance_generator=self.get_train_gen,\n                    get_val_instance_generator=self.get_val_gen,\n                    batch_size=self.batch_size,\n                    num_train_steps_per_epoch=self.num_train_steps_per_epoch,\n                    num_epochs=2,\n                    num_val_steps=self.num_val_steps,\n                    save_path=self.TEST_DIR,\n                    log_path=self.TEST_DIR,\n                    log_period=2,\n                    val_period=2,\n                    save_period=2,\n                    patience=0)\n\n        tf.reset_default_graph()\n        # Load and predict with the model\n        self.config_dict[""mode""] = ""test""\n        del self.config_dict[""word_embedding_matrix""]\n        del self.config_dict[""char_embedding_matrix""]\n        loaded_model = BiMPM(self.config_dict)\n        loaded_model.build_graph()\n        loaded_model.predict(get_test_instance_generator=self.get_test_gen,\n                             model_load_dir=self.TEST_DIR,\n                             batch_size=self.batch_size,\n                             num_test_steps=self.num_test_steps)\n'"
tests/models/siamese_bilstm/__init__.py,0,b''
tests/models/siamese_bilstm/test_siamese_bilstm.py,3,"b'from overrides import overrides\nimport math\nimport tensorflow as tf\n\nfrom duplicate_questions.data.data_manager import DataManager\nfrom duplicate_questions.data.embedding_manager import EmbeddingManager\nfrom duplicate_questions.models.siamese_bilstm.siamese_bilstm import SiameseBiLSTM\nfrom duplicate_questions.data.instances.sts_instance import STSInstance\n\nfrom ...common.test_case import DuplicateTestCase\n\n\nclass TestSiameseBiLSTM(DuplicateTestCase):\n    @overrides\n    def setUp(self):\n        super(TestSiameseBiLSTM, self).setUp()\n        self.write_duplicate_questions_train_file()\n        self.write_duplicate_questions_validation_file()\n        self.write_duplicate_questions_test_file()\n        self.data_manager = DataManager(STSInstance)\n        self.batch_size = 2\n        self.get_train_gen, self.train_size = self.data_manager.get_train_data_from_file(\n            [self.TRAIN_FILE])\n        self.get_val_gen, self.val_size = self.data_manager.get_validation_data_from_file(\n            [self.VALIDATION_FILE])\n        self.get_test_gen, self.test_size = self.data_manager.get_test_data_from_file(\n            [self.TEST_FILE])\n\n        self.embedding_manager = EmbeddingManager(self.data_manager.data_indexer)\n        self.word_embedding_dim = 5\n        self.embedding_matrix = self.embedding_manager.get_embedding_matrix(\n            self.word_embedding_dim)\n        self.rnn_hidden_size = 6\n        self.rnn_output_mode = ""last""\n        self.output_keep_prob = 1.0\n        self.share_encoder_weights = True\n        self.config_dict = {\n            ""mode"": ""train"",\n            ""word_vocab_size"": self.data_manager.data_indexer.get_vocab_size(),\n            ""word_embedding_dim"": self.word_embedding_dim,\n            ""fine_tune_embeddings"": False,\n            ""word_embedding_matrix"": self.embedding_matrix,\n            ""rnn_hidden_size"": self.rnn_hidden_size,\n            ""rnn_output_mode"": self.rnn_output_mode,\n            ""output_keep_prob"": self.output_keep_prob,\n            ""share_encoder_weights"": self.share_encoder_weights\n        }\n        self.num_train_steps_per_epoch = int(math.ceil(self.train_size / self.batch_size))\n        self.num_val_steps = int(math.ceil(self.val_size / self.batch_size))\n        self.num_test_steps = int(math.ceil(self.test_size / self.batch_size))\n\n    def test_default_does_not_crash(self):\n        # Initialize the model\n        model = SiameseBiLSTM(self.config_dict)\n        model.build_graph()\n        # Train the model\n        model.train(get_train_instance_generator=self.get_train_gen,\n                    get_val_instance_generator=self.get_val_gen,\n                    batch_size=self.batch_size,\n                    num_train_steps_per_epoch=self.num_train_steps_per_epoch,\n                    num_epochs=2,\n                    num_val_steps=self.num_val_steps,\n                    save_path=self.TEST_DIR,\n                    log_path=self.TEST_DIR,\n                    log_period=2,\n                    val_period=2,\n                    save_period=2,\n                    patience=0)\n\n        tf.reset_default_graph()\n        # Load and predict with the model\n        self.config_dict[""mode""] = ""test""\n        del self.config_dict[""word_embedding_matrix""]\n        loaded_model = SiameseBiLSTM(self.config_dict)\n        loaded_model.build_graph()\n        loaded_model.predict(get_test_instance_generator=self.get_test_gen,\n                             model_load_dir=self.TEST_DIR,\n                             batch_size=self.batch_size,\n                             num_test_steps=self.num_test_steps)\n\n    def test_mean_pool_does_not_crash(self):\n        # Initialize the model\n        self.config_dict[""rnn_output_mode""] = ""mean_pool""\n        model = SiameseBiLSTM(self.config_dict)\n        model.build_graph()\n        # Train the model\n        model.train(get_train_instance_generator=self.get_train_gen,\n                    get_val_instance_generator=self.get_val_gen,\n                    batch_size=self.batch_size,\n                    num_train_steps_per_epoch=self.num_train_steps_per_epoch,\n                    num_epochs=2,\n                    num_val_steps=self.num_val_steps,\n                    save_path=self.TEST_DIR,\n                    log_path=self.TEST_DIR,\n                    log_period=2,\n                    val_period=2,\n                    save_period=2,\n                    patience=0)\n\n        tf.reset_default_graph()\n        # Load and predict with the model\n        self.config_dict[""mode""] = ""test""\n        del self.config_dict[""word_embedding_matrix""]\n        loaded_model = SiameseBiLSTM(self.config_dict)\n        loaded_model.build_graph()\n        loaded_model.predict(get_test_instance_generator=self.get_test_gen,\n                             model_load_dir=self.TEST_DIR,\n                             batch_size=self.batch_size,\n                             num_test_steps=self.num_test_steps)\n\n    def test_non_sharing_encoders_does_not_crash(self):\n        # Initialize the model\n        self.config_dict[""share_encoder_weights""] = False\n        model = SiameseBiLSTM(self.config_dict)\n        model.build_graph()\n        # Train the model\n        model.train(get_train_instance_generator=self.get_train_gen,\n                    get_val_instance_generator=self.get_val_gen,\n                    batch_size=self.batch_size,\n                    num_train_steps_per_epoch=self.num_train_steps_per_epoch,\n                    num_epochs=2,\n                    num_val_steps=self.num_val_steps,\n                    save_path=self.TEST_DIR,\n                    log_path=self.TEST_DIR,\n                    log_period=2,\n                    val_period=2,\n                    save_period=2,\n                    patience=0)\n\n        tf.reset_default_graph()\n        # Load and predict with the model\n        self.config_dict[""mode""] = ""test""\n        del self.config_dict[""word_embedding_matrix""]\n        loaded_model = SiameseBiLSTM(self.config_dict)\n        loaded_model.build_graph()\n        loaded_model.predict(get_test_instance_generator=self.get_test_gen,\n                             model_load_dir=self.TEST_DIR,\n                             batch_size=self.batch_size,\n                             num_test_steps=self.num_test_steps)\n'"
tests/models/siamese_bilstm/test_siamese_matching_bilstm.py,2,"b'from overrides import overrides\nimport math\nimport tensorflow as tf\n\nfrom duplicate_questions.data.data_manager import DataManager\nfrom duplicate_questions.data.embedding_manager import EmbeddingManager\nfrom duplicate_questions.models.siamese_bilstm.siamese_matching_bilstm import (\n    SiameseMatchingBiLSTM\n)\nfrom duplicate_questions.data.instances.sts_instance import STSInstance\n\nfrom ...common.test_case import DuplicateTestCase\n\n\nclass TestSiameseMatchingBiLSTM(DuplicateTestCase):\n    @overrides\n    def setUp(self):\n        super(TestSiameseMatchingBiLSTM, self).setUp()\n        self.write_duplicate_questions_train_file()\n        self.write_duplicate_questions_validation_file()\n        self.write_duplicate_questions_test_file()\n        self.data_manager = DataManager(STSInstance)\n        self.batch_size = 2\n        self.get_train_gen, self.train_size = self.data_manager.get_train_data_from_file(\n            [self.TRAIN_FILE])\n        self.get_val_gen, self.val_size = self.data_manager.get_validation_data_from_file(\n            [self.VALIDATION_FILE])\n        self.get_test_gen, self.test_size = self.data_manager.get_test_data_from_file(\n            [self.TEST_FILE])\n\n        self.embedding_manager = EmbeddingManager(self.data_manager.data_indexer)\n        self.word_embedding_dim = 5\n        self.embedding_matrix = self.embedding_manager.get_embedding_matrix(\n            self.word_embedding_dim)\n        self.rnn_hidden_size = 6\n        self.output_keep_prob = 1.0\n        self.share_encoder_weights = True\n        self.config_dict = {\n            ""mode"": ""train"",\n            ""word_vocab_size"": self.data_manager.data_indexer.get_vocab_size(),\n            ""word_embedding_dim"": self.word_embedding_dim,\n            ""fine_tune_embeddings"": False,\n            ""word_embedding_matrix"": self.embedding_matrix,\n            ""rnn_hidden_size"": self.rnn_hidden_size,\n            ""output_keep_prob"": self.output_keep_prob,\n            ""share_encoder_weights"": self.share_encoder_weights\n        }\n        self.num_train_steps_per_epoch = int(math.ceil(self.train_size / self.batch_size))\n        self.num_val_steps = int(math.ceil(self.val_size / self.batch_size))\n        self.num_test_steps = int(math.ceil(self.test_size / self.batch_size))\n\n    def test_default_does_not_crash(self):\n        # Initialize the model\n        model = SiameseMatchingBiLSTM(self.config_dict)\n        model.build_graph()\n        # Train the model\n        model.train(get_train_instance_generator=self.get_train_gen,\n                    get_val_instance_generator=self.get_val_gen,\n                    batch_size=self.batch_size,\n                    num_train_steps_per_epoch=self.num_train_steps_per_epoch,\n                    num_epochs=2,\n                    num_val_steps=self.num_val_steps,\n                    save_path=self.TEST_DIR,\n                    log_path=self.TEST_DIR,\n                    log_period=2,\n                    val_period=2,\n                    save_period=2,\n                    patience=0)\n\n        tf.reset_default_graph()\n        # Load and predict with the model\n        self.config_dict[""mode""] = ""test""\n        del self.config_dict[""word_embedding_matrix""]\n        loaded_model = SiameseMatchingBiLSTM(self.config_dict)\n        loaded_model.build_graph()\n        loaded_model.predict(get_test_instance_generator=self.get_test_gen,\n                             model_load_dir=self.TEST_DIR,\n                             batch_size=self.batch_size,\n                             num_test_steps=self.num_test_steps)\n\n    def test_non_sharing_encoders_does_not_crash(self):\n        # Initialize the model\n        self.config_dict[""share_encoder_weights""] = False\n        model = SiameseMatchingBiLSTM(self.config_dict)\n        model.build_graph()\n        # Train the model\n        model.train(get_train_instance_generator=self.get_train_gen,\n                    get_val_instance_generator=self.get_val_gen,\n                    batch_size=self.batch_size,\n                    num_train_steps_per_epoch=self.num_train_steps_per_epoch,\n                    num_epochs=2,\n                    num_val_steps=self.num_val_steps,\n                    save_path=self.TEST_DIR,\n                    log_path=self.TEST_DIR,\n                    log_period=2,\n                    val_period=2,\n                    save_period=2,\n                    patience=0)\n\n        tf.reset_default_graph()\n        # Load and predict with the model\n        self.config_dict[""mode""] = ""test""\n        del self.config_dict[""word_embedding_matrix""]\n        loaded_model = SiameseMatchingBiLSTM(self.config_dict)\n        loaded_model.build_graph()\n        loaded_model.predict(get_test_instance_generator=self.get_test_gen,\n                             model_load_dir=self.TEST_DIR,\n                             batch_size=self.batch_size,\n                             num_test_steps=self.num_test_steps)\n'"
